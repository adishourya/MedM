  0%|                                                                                                                                   | 0/10 [00:00<?, ?it/s]
[0;31m---------------------------------------------------------------------------[0m
[0;31mOutOfMemoryError[0m                          Traceback (most recent call last)
File [0;32m~/code/um/MedM/experiments/scratchpad/sample_trainer.py:71[0m
[1;32m     48[0m training_args [38;5;241m=[39m Seq2SeqTrainingArguments(
[1;32m     49[0m     output_dir[38;5;241m=[39m[38;5;124m"[39m[38;5;124m./results[39m[38;5;124m"[39m,
[1;32m     50[0m     evaluation_strategy[38;5;241m=[39m[38;5;124m"[39m[38;5;124mepoch[39m[38;5;124m"[39m,
[0;32m   (...)[0m
[1;32m     58[0m     predict_with_generate[38;5;241m=[39m[38;5;28;01mTrue[39;00m
[1;32m     59[0m )
[1;32m     61[0m trainer [38;5;241m=[39m Seq2SeqTrainer(
[1;32m     62[0m     model[38;5;241m=[39mmodel,
[1;32m     63[0m     args[38;5;241m=[39mtraining_args,
[0;32m   (...)[0m
[1;32m     68[0m     compute_metrics[38;5;241m=[39mcompute_metrics
[1;32m     69[0m )
[0;32m---> 71[0m trainer[38;5;241m.[39mtrain()

File [0;32m~/anaconda3/lib/python3.12/site-packages/transformers/trainer.py:1938[0m, in [0;36mTrainer.train[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)[0m
[1;32m   1936[0m         hf_hub_utils[38;5;241m.[39menable_progress_bars()
[1;32m   1937[0m [38;5;28;01melse[39;00m:
[0;32m-> 1938[0m     [38;5;28;01mreturn[39;00m inner_training_loop(
[1;32m   1939[0m         args[38;5;241m=[39margs,
[1;32m   1940[0m         resume_from_checkpoint[38;5;241m=[39mresume_from_checkpoint,
[1;32m   1941[0m         trial[38;5;241m=[39mtrial,
[1;32m   1942[0m         ignore_keys_for_eval[38;5;241m=[39mignore_keys_for_eval,
[1;32m   1943[0m     )

File [0;32m~/anaconda3/lib/python3.12/site-packages/transformers/trainer.py:2279[0m, in [0;36mTrainer._inner_training_loop[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)[0m
[1;32m   2276[0m     [38;5;28mself[39m[38;5;241m.[39mcontrol [38;5;241m=[39m [38;5;28mself[39m[38;5;241m.[39mcallback_handler[38;5;241m.[39mon_step_begin(args, [38;5;28mself[39m[38;5;241m.[39mstate, [38;5;28mself[39m[38;5;241m.[39mcontrol)
[1;32m   2278[0m [38;5;28;01mwith[39;00m [38;5;28mself[39m[38;5;241m.[39maccelerator[38;5;241m.[39maccumulate(model):
[0;32m-> 2279[0m     tr_loss_step [38;5;241m=[39m [38;5;28mself[39m[38;5;241m.[39mtraining_step(model, inputs)
[1;32m   2281[0m [38;5;28;01mif[39;00m (
[1;32m   2282[0m     args[38;5;241m.[39mlogging_nan_inf_filter
[1;32m   2283[0m     [38;5;129;01mand[39;00m [38;5;129;01mnot[39;00m is_torch_xla_available()
[1;32m   2284[0m     [38;5;129;01mand[39;00m (torch[38;5;241m.[39misnan(tr_loss_step) [38;5;129;01mor[39;00m torch[38;5;241m.[39misinf(tr_loss_step))
[1;32m   2285[0m ):
[1;32m   2286[0m     [38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses[39;00m
[1;32m   2287[0m     tr_loss [38;5;241m+[39m[38;5;241m=[39m tr_loss [38;5;241m/[39m ([38;5;241m1[39m [38;5;241m+[39m [38;5;28mself[39m[38;5;241m.[39mstate[38;5;241m.[39mglobal_step [38;5;241m-[39m [38;5;28mself[39m[38;5;241m.[39m_globalstep_last_logged)

File [0;32m~/anaconda3/lib/python3.12/site-packages/transformers/trainer.py:3318[0m, in [0;36mTrainer.training_step[0;34m(self, model, inputs)[0m
[1;32m   3315[0m     [38;5;28;01mreturn[39;00m loss_mb[38;5;241m.[39mreduce_mean()[38;5;241m.[39mdetach()[38;5;241m.[39mto([38;5;28mself[39m[38;5;241m.[39margs[38;5;241m.[39mdevice)
[1;32m   3317[0m [38;5;28;01mwith[39;00m [38;5;28mself[39m[38;5;241m.[39mcompute_loss_context_manager():
[0;32m-> 3318[0m     loss [38;5;241m=[39m [38;5;28mself[39m[38;5;241m.[39mcompute_loss(model, inputs)
[1;32m   3320[0m [38;5;28;01mdel[39;00m inputs
[1;32m   3321[0m [38;5;28;01mif[39;00m (
[1;32m   3322[0m     [38;5;28mself[39m[38;5;241m.[39margs[38;5;241m.[39mtorch_empty_cache_steps [38;5;129;01mis[39;00m [38;5;129;01mnot[39;00m [38;5;28;01mNone[39;00m
[1;32m   3323[0m     [38;5;129;01mand[39;00m [38;5;28mself[39m[38;5;241m.[39mstate[38;5;241m.[39mglobal_step [38;5;241m%[39m [38;5;28mself[39m[38;5;241m.[39margs[38;5;241m.[39mtorch_empty_cache_steps [38;5;241m==[39m [38;5;241m0[39m
[1;32m   3324[0m ):

File [0;32m~/anaconda3/lib/python3.12/site-packages/transformers/trainer.py:3363[0m, in [0;36mTrainer.compute_loss[0;34m(self, model, inputs, return_outputs)[0m
[1;32m   3361[0m [38;5;28;01melse[39;00m:
[1;32m   3362[0m     labels [38;5;241m=[39m [38;5;28;01mNone[39;00m
[0;32m-> 3363[0m outputs [38;5;241m=[39m model([38;5;241m*[39m[38;5;241m*[39minputs)
[1;32m   3364[0m [38;5;66;03m# Save past state if it exists[39;00m
[1;32m   3365[0m [38;5;66;03m# TODO: this needs to be fixed and made cleaner later.[39;00m
[1;32m   3366[0m [38;5;28;01mif[39;00m [38;5;28mself[39m[38;5;241m.[39margs[38;5;241m.[39mpast_index [38;5;241m>[39m[38;5;241m=[39m [38;5;241m0[39m:

File [0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553[0m, in [0;36mModule._wrapped_call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1551[0m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m[38;5;241m.[39m_compiled_call_impl([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)  [38;5;66;03m# type: ignore[misc][39;00m
[1;32m   1552[0m [38;5;28;01melse[39;00m:
[0;32m-> 1553[0m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m[38;5;241m.[39m_call_impl([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)

File [0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562[0m, in [0;36mModule._call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1557[0m [38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in[39;00m
[1;32m   1558[0m [38;5;66;03m# this function, and just call forward.[39;00m
[1;32m   1559[0m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m ([38;5;28mself[39m[38;5;241m.[39m_backward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_backward_pre_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_pre_hooks
[1;32m   1560[0m         [38;5;129;01mor[39;00m _global_backward_pre_hooks [38;5;129;01mor[39;00m _global_backward_hooks
[1;32m   1561[0m         [38;5;129;01mor[39;00m _global_forward_hooks [38;5;129;01mor[39;00m _global_forward_pre_hooks):
[0;32m-> 1562[0m     [38;5;28;01mreturn[39;00m forward_call([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)
[1;32m   1564[0m [38;5;28;01mtry[39;00m:
[1;32m   1565[0m     result [38;5;241m=[39m [38;5;28;01mNone[39;00m

File [0;32m~/anaconda3/lib/python3.12/site-packages/accelerate/utils/operations.py:820[0m, in [0;36mconvert_outputs_to_fp32.<locals>.forward[0;34m(*args, **kwargs)[0m
[1;32m    819[0m [38;5;28;01mdef[39;00m [38;5;21mforward[39m([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs):
[0;32m--> 820[0m     [38;5;28;01mreturn[39;00m model_forward([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)

File [0;32m~/anaconda3/lib/python3.12/site-packages/accelerate/utils/operations.py:808[0m, in [0;36mConvertOutputsToFp32.__call__[0;34m(self, *args, **kwargs)[0m
[1;32m    807[0m [38;5;28;01mdef[39;00m [38;5;21m__call__[39m([38;5;28mself[39m, [38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs):
[0;32m--> 808[0m     [38;5;28;01mreturn[39;00m convert_to_fp32([38;5;28mself[39m[38;5;241m.[39mmodel_forward([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs))

File [0;32m~/anaconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:43[0m, in [0;36mautocast_decorator.<locals>.decorate_autocast[0;34m(*args, **kwargs)[0m
[1;32m     40[0m [38;5;129m@functools[39m[38;5;241m.[39mwraps(func)
[1;32m     41[0m [38;5;28;01mdef[39;00m [38;5;21mdecorate_autocast[39m([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs):
[1;32m     42[0m     [38;5;28;01mwith[39;00m autocast_instance:
[0;32m---> 43[0m         [38;5;28;01mreturn[39;00m func([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)

File [0;32m~/anaconda3/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:1702[0m, in [0;36mT5ForConditionalGeneration.forward[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)[0m
[1;32m   1699[0m [38;5;66;03m# Encode if needed (training, first prediction pass)[39;00m
[1;32m   1700[0m [38;5;28;01mif[39;00m encoder_outputs [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m:
[1;32m   1701[0m     [38;5;66;03m# Convert encoder inputs in embeddings if needed[39;00m
[0;32m-> 1702[0m     encoder_outputs [38;5;241m=[39m [38;5;28mself[39m[38;5;241m.[39mencoder(
[1;32m   1703[0m         input_ids[38;5;241m=[39minput_ids,
[1;32m   1704[0m         attention_mask[38;5;241m=[39mattention_mask,
[1;32m   1705[0m         inputs_embeds[38;5;241m=[39minputs_embeds,
[1;32m   1706[0m         head_mask[38;5;241m=[39mhead_mask,
[1;32m   1707[0m         output_attentions[38;5;241m=[39moutput_attentions,
[1;32m   1708[0m         output_hidden_states[38;5;241m=[39moutput_hidden_states,
[1;32m   1709[0m         return_dict[38;5;241m=[39mreturn_dict,
[1;32m   1710[0m     )
[1;32m   1711[0m [38;5;28;01melif[39;00m return_dict [38;5;129;01mand[39;00m [38;5;129;01mnot[39;00m [38;5;28misinstance[39m(encoder_outputs, BaseModelOutput):
[1;32m   1712[0m     encoder_outputs [38;5;241m=[39m BaseModelOutput(
[1;32m   1713[0m         last_hidden_state[38;5;241m=[39mencoder_outputs[[38;5;241m0[39m],
[1;32m   1714[0m         hidden_states[38;5;241m=[39mencoder_outputs[[38;5;241m1[39m] [38;5;28;01mif[39;00m [38;5;28mlen[39m(encoder_outputs) [38;5;241m>[39m [38;5;241m1[39m [38;5;28;01melse[39;00m [38;5;28;01mNone[39;00m,
[1;32m   1715[0m         attentions[38;5;241m=[39mencoder_outputs[[38;5;241m2[39m] [38;5;28;01mif[39;00m [38;5;28mlen[39m(encoder_outputs) [38;5;241m>[39m [38;5;241m2[39m [38;5;28;01melse[39;00m [38;5;28;01mNone[39;00m,
[1;32m   1716[0m     )

File [0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553[0m, in [0;36mModule._wrapped_call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1551[0m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m[38;5;241m.[39m_compiled_call_impl([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)  [38;5;66;03m# type: ignore[misc][39;00m
[1;32m   1552[0m [38;5;28;01melse[39;00m:
[0;32m-> 1553[0m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m[38;5;241m.[39m_call_impl([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)

File [0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562[0m, in [0;36mModule._call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1557[0m [38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in[39;00m
[1;32m   1558[0m [38;5;66;03m# this function, and just call forward.[39;00m
[1;32m   1559[0m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m ([38;5;28mself[39m[38;5;241m.[39m_backward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_backward_pre_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_pre_hooks
[1;32m   1560[0m         [38;5;129;01mor[39;00m _global_backward_pre_hooks [38;5;129;01mor[39;00m _global_backward_hooks
[1;32m   1561[0m         [38;5;129;01mor[39;00m _global_forward_hooks [38;5;129;01mor[39;00m _global_forward_pre_hooks):
[0;32m-> 1562[0m     [38;5;28;01mreturn[39;00m forward_call([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)
[1;32m   1564[0m [38;5;28;01mtry[39;00m:
[1;32m   1565[0m     result [38;5;241m=[39m [38;5;28;01mNone[39;00m

File [0;32m~/anaconda3/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:1106[0m, in [0;36mT5Stack.forward[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)[0m
[1;32m   1091[0m     layer_outputs [38;5;241m=[39m [38;5;28mself[39m[38;5;241m.[39m_gradient_checkpointing_func(
[1;32m   1092[0m         layer_module[38;5;241m.[39mforward,
[1;32m   1093[0m         hidden_states,
[0;32m   (...)[0m
[1;32m   1103[0m         output_attentions,
[1;32m   1104[0m     )
[1;32m   1105[0m [38;5;28;01melse[39;00m:
[0;32m-> 1106[0m     layer_outputs [38;5;241m=[39m layer_module(
[1;32m   1107[0m         hidden_states,
[1;32m   1108[0m         attention_mask[38;5;241m=[39mextended_attention_mask,
[1;32m   1109[0m         position_bias[38;5;241m=[39mposition_bias,
[1;32m   1110[0m         encoder_hidden_states[38;5;241m=[39mencoder_hidden_states,
[1;32m   1111[0m         encoder_attention_mask[38;5;241m=[39mencoder_extended_attention_mask,
[1;32m   1112[0m         encoder_decoder_position_bias[38;5;241m=[39mencoder_decoder_position_bias,
[1;32m   1113[0m         layer_head_mask[38;5;241m=[39mlayer_head_mask,
[1;32m   1114[0m         cross_attn_layer_head_mask[38;5;241m=[39mcross_attn_layer_head_mask,
[1;32m   1115[0m         past_key_value[38;5;241m=[39mpast_key_value,
[1;32m   1116[0m         use_cache[38;5;241m=[39muse_cache,
[1;32m   1117[0m         output_attentions[38;5;241m=[39moutput_attentions,
[1;32m   1118[0m     )
[1;32m   1120[0m [38;5;66;03m# layer_outputs is a tuple with:[39;00m
[1;32m   1121[0m [38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)[39;00m
[1;32m   1122[0m [38;5;28;01mif[39;00m use_cache [38;5;129;01mis[39;00m [38;5;28;01mFalse[39;00m:

File [0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553[0m, in [0;36mModule._wrapped_call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1551[0m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m[38;5;241m.[39m_compiled_call_impl([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)  [38;5;66;03m# type: ignore[misc][39;00m
[1;32m   1552[0m [38;5;28;01melse[39;00m:
[0;32m-> 1553[0m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m[38;5;241m.[39m_call_impl([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)

File [0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562[0m, in [0;36mModule._call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1557[0m [38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in[39;00m
[1;32m   1558[0m [38;5;66;03m# this function, and just call forward.[39;00m
[1;32m   1559[0m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m ([38;5;28mself[39m[38;5;241m.[39m_backward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_backward_pre_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_pre_hooks
[1;32m   1560[0m         [38;5;129;01mor[39;00m _global_backward_pre_hooks [38;5;129;01mor[39;00m _global_backward_hooks
[1;32m   1561[0m         [38;5;129;01mor[39;00m _global_forward_hooks [38;5;129;01mor[39;00m _global_forward_pre_hooks):
[0;32m-> 1562[0m     [38;5;28;01mreturn[39;00m forward_call([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)
[1;32m   1564[0m [38;5;28;01mtry[39;00m:
[1;32m   1565[0m     result [38;5;241m=[39m [38;5;28;01mNone[39;00m

File [0;32m~/anaconda3/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:686[0m, in [0;36mT5Block.forward[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)[0m
[1;32m    683[0m [38;5;28;01melse[39;00m:
[1;32m    684[0m     self_attn_past_key_value, cross_attn_past_key_value [38;5;241m=[39m [38;5;28;01mNone[39;00m, [38;5;28;01mNone[39;00m
[0;32m--> 686[0m self_attention_outputs [38;5;241m=[39m [38;5;28mself[39m[38;5;241m.[39mlayer[[38;5;241m0[39m](
[1;32m    687[0m     hidden_states,
[1;32m    688[0m     attention_mask[38;5;241m=[39mattention_mask,
[1;32m    689[0m     position_bias[38;5;241m=[39mposition_bias,
[1;32m    690[0m     layer_head_mask[38;5;241m=[39mlayer_head_mask,
[1;32m    691[0m     past_key_value[38;5;241m=[39mself_attn_past_key_value,
[1;32m    692[0m     use_cache[38;5;241m=[39muse_cache,
[1;32m    693[0m     output_attentions[38;5;241m=[39moutput_attentions,
[1;32m    694[0m )
[1;32m    695[0m hidden_states, present_key_value_state [38;5;241m=[39m self_attention_outputs[:[38;5;241m2[39m]
[1;32m    696[0m attention_outputs [38;5;241m=[39m self_attention_outputs[[38;5;241m2[39m:]  [38;5;66;03m# Keep self-attention outputs and relative position weights[39;00m

File [0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553[0m, in [0;36mModule._wrapped_call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1551[0m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m[38;5;241m.[39m_compiled_call_impl([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)  [38;5;66;03m# type: ignore[misc][39;00m
[1;32m   1552[0m [38;5;28;01melse[39;00m:
[0;32m-> 1553[0m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m[38;5;241m.[39m_call_impl([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)

File [0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562[0m, in [0;36mModule._call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1557[0m [38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in[39;00m
[1;32m   1558[0m [38;5;66;03m# this function, and just call forward.[39;00m
[1;32m   1559[0m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m ([38;5;28mself[39m[38;5;241m.[39m_backward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_backward_pre_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_pre_hooks
[1;32m   1560[0m         [38;5;129;01mor[39;00m _global_backward_pre_hooks [38;5;129;01mor[39;00m _global_backward_hooks
[1;32m   1561[0m         [38;5;129;01mor[39;00m _global_forward_hooks [38;5;129;01mor[39;00m _global_forward_pre_hooks):
[0;32m-> 1562[0m     [38;5;28;01mreturn[39;00m forward_call([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)
[1;32m   1564[0m [38;5;28;01mtry[39;00m:
[1;32m   1565[0m     result [38;5;241m=[39m [38;5;28;01mNone[39;00m

File [0;32m~/anaconda3/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:593[0m, in [0;36mT5LayerSelfAttention.forward[0;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions)[0m
[1;32m    582[0m [38;5;28;01mdef[39;00m [38;5;21mforward[39m(
[1;32m    583[0m     [38;5;28mself[39m,
[1;32m    584[0m     hidden_states,
[0;32m   (...)[0m
[1;32m    590[0m     output_attentions[38;5;241m=[39m[38;5;28;01mFalse[39;00m,
[1;32m    591[0m ):
[1;32m    592[0m     normed_hidden_states [38;5;241m=[39m [38;5;28mself[39m[38;5;241m.[39mlayer_norm(hidden_states)
[0;32m--> 593[0m     attention_output [38;5;241m=[39m [38;5;28mself[39m[38;5;241m.[39mSelfAttention(
[1;32m    594[0m         normed_hidden_states,
[1;32m    595[0m         mask[38;5;241m=[39mattention_mask,
[1;32m    596[0m         position_bias[38;5;241m=[39mposition_bias,
[1;32m    597[0m         layer_head_mask[38;5;241m=[39mlayer_head_mask,
[1;32m    598[0m         past_key_value[38;5;241m=[39mpast_key_value,
[1;32m    599[0m         use_cache[38;5;241m=[39muse_cache,
[1;32m    600[0m         output_attentions[38;5;241m=[39moutput_attentions,
[1;32m    601[0m     )
[1;32m    602[0m     hidden_states [38;5;241m=[39m hidden_states [38;5;241m+[39m [38;5;28mself[39m[38;5;241m.[39mdropout(attention_output[[38;5;241m0[39m])
[1;32m    603[0m     outputs [38;5;241m=[39m (hidden_states,) [38;5;241m+[39m attention_output[[38;5;241m1[39m:]  [38;5;66;03m# add attentions if we output them[39;00m

File [0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553[0m, in [0;36mModule._wrapped_call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1551[0m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m[38;5;241m.[39m_compiled_call_impl([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)  [38;5;66;03m# type: ignore[misc][39;00m
[1;32m   1552[0m [38;5;28;01melse[39;00m:
[0;32m-> 1553[0m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m[38;5;241m.[39m_call_impl([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)

File [0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562[0m, in [0;36mModule._call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1557[0m [38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in[39;00m
[1;32m   1558[0m [38;5;66;03m# this function, and just call forward.[39;00m
[1;32m   1559[0m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m ([38;5;28mself[39m[38;5;241m.[39m_backward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_backward_pre_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_pre_hooks
[1;32m   1560[0m         [38;5;129;01mor[39;00m _global_backward_pre_hooks [38;5;129;01mor[39;00m _global_backward_hooks
[1;32m   1561[0m         [38;5;129;01mor[39;00m _global_forward_hooks [38;5;129;01mor[39;00m _global_forward_pre_hooks):
[0;32m-> 1562[0m     [38;5;28;01mreturn[39;00m forward_call([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)
[1;32m   1564[0m [38;5;28;01mtry[39;00m:
[1;32m   1565[0m     result [38;5;241m=[39m [38;5;28;01mNone[39;00m

File [0;32m~/anaconda3/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:553[0m, in [0;36mT5Attention.forward[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)[0m
[1;32m    550[0m     position_bias_masked [38;5;241m=[39m position_bias
[1;32m    552[0m scores [38;5;241m+[39m[38;5;241m=[39m position_bias_masked
[0;32m--> 553[0m attn_weights [38;5;241m=[39m nn[38;5;241m.[39mfunctional[38;5;241m.[39msoftmax(scores[38;5;241m.[39mfloat(), dim[38;5;241m=[39m[38;5;241m-[39m[38;5;241m1[39m)[38;5;241m.[39mtype_as(
[1;32m    554[0m     scores
[1;32m    555[0m )  [38;5;66;03m# (batch_size, n_heads, seq_length, key_length)[39;00m
[1;32m    556[0m attn_weights [38;5;241m=[39m nn[38;5;241m.[39mfunctional[38;5;241m.[39mdropout(
[1;32m    557[0m     attn_weights, p[38;5;241m=[39m[38;5;28mself[39m[38;5;241m.[39mdropout, training[38;5;241m=[39m[38;5;28mself[39m[38;5;241m.[39mtraining
[1;32m    558[0m )  [38;5;66;03m# (batch_size, n_heads, seq_length, key_length)[39;00m
[1;32m    560[0m [38;5;66;03m# Mask heads if we want to[39;00m

[0;31mOutOfMemoryError[0m: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 0 has a total capacity of 7.63 GiB of which 210.00 MiB is free. Including non-PyTorch memory, this process has 6.76 GiB memory in use. Of the allocated memory 6.46 GiB is allocated by PyTorch, and 142.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 686.95 examples/s]
/home/adi/anaconda3/lib/python3.12/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/adi/anaconda3/lib/python3.12/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.scaler = torch.cuda.amp.GradScaler(**kwargs)

  0%|                                                                                                                                   | 0/10 [00:00<?, ?it/s]

[0;31m---------------------------------------------------------------------------[0m
[0;31mOutOfMemoryError[0m                          Traceback (most recent call last)
File [0;32m~/code/um/MedM/experiments/scratchpad/sample_trainer.py:71[0m
[1;32m     48[0m training_args [38;5;241m=[39m Seq2SeqTrainingArguments(
[1;32m     49[0m     output_dir[38;5;241m=[39m[38;5;124m"[39m[38;5;124m./results[39m[38;5;124m"[39m,
[1;32m     50[0m     evaluation_strategy[38;5;241m=[39m[38;5;124m"[39m[38;5;124mepoch[39m[38;5;124m"[39m,
[0;32m   (...)[0m
[1;32m     58[0m     predict_with_generate[38;5;241m=[39m[38;5;28;01mTrue[39;00m
[1;32m     59[0m )
[1;32m     61[0m trainer [38;5;241m=[39m Seq2SeqTrainer(
[1;32m     62[0m     model[38;5;241m=[39mmodel,
[1;32m     63[0m     args[38;5;241m=[39mtraining_args,
[0;32m   (...)[0m
[1;32m     68[0m     compute_metrics[38;5;241m=[39mcompute_metrics
[1;32m     69[0m )
[0;32m---> 71[0m trainer[38;5;241m.[39mtrain()

File [0;32m~/anaconda3/lib/python3.12/site-packages/transformers/trainer.py:1938[0m, in [0;36mTrainer.train[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)[0m
[1;32m   1936[0m         hf_hub_utils[38;5;241m.[39menable_progress_bars()
[1;32m   1937[0m [38;5;28;01melse[39;00m:
[0;32m-> 1938[0m     [38;5;28;01mreturn[39;00m inner_training_loop(
[1;32m   1939[0m         args[38;5;241m=[39margs,
[1;32m   1940[0m         resume_from_checkpoint[38;5;241m=[39mresume_from_checkpoint,
[1;32m   1941[0m         trial[38;5;241m=[39mtrial,
[1;32m   1942[0m         ignore_keys_for_eval[38;5;241m=[39mignore_keys_for_eval,
[1;32m   1943[0m     )

File [0;32m~/anaconda3/lib/python3.12/site-packages/transformers/trainer.py:2279[0m, in [0;36mTrainer._inner_training_loop[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)[0m
[1;32m   2276[0m     [38;5;28mself[39m[38;5;241m.[39mcontrol [38;5;241m=[39m [38;5;28mself[39m[38;5;241m.[39mcallback_handler[38;5;241m.[39mon_step_begin(args, [38;5;28mself[39m[38;5;241m.[39mstate, [38;5;28mself[39m[38;5;241m.[39mcontrol)
[1;32m   2278[0m [38;5;28;01mwith[39;00m [38;5;28mself[39m[38;5;241m.[39maccelerator[38;5;241m.[39maccumulate(model):
[0;32m-> 2279[0m     tr_loss_step [38;5;241m=[39m [38;5;28mself[39m[38;5;241m.[39mtraining_step(model, inputs)
[1;32m   2281[0m [38;5;28;01mif[39;00m (
[1;32m   2282[0m     args[38;5;241m.[39mlogging_nan_inf_filter
[1;32m   2283[0m     [38;5;129;01mand[39;00m [38;5;129;01mnot[39;00m is_torch_xla_available()
[1;32m   2284[0m     [38;5;129;01mand[39;00m (torch[38;5;241m.[39misnan(tr_loss_step) [38;5;129;01mor[39;00m torch[38;5;241m.[39misinf(tr_loss_step))
[1;32m   2285[0m ):
[1;32m   2286[0m     [38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses[39;00m
[1;32m   2287[0m     tr_loss [38;5;241m+[39m[38;5;241m=[39m tr_loss [38;5;241m/[39m ([38;5;241m1[39m [38;5;241m+[39m [38;5;28mself[39m[38;5;241m.[39mstate[38;5;241m.[39mglobal_step [38;5;241m-[39m [38;5;28mself[39m[38;5;241m.[39m_globalstep_last_logged)

File [0;32m~/anaconda3/lib/python3.12/site-packages/transformers/trainer.py:3318[0m, in [0;36mTrainer.training_step[0;34m(self, model, inputs)[0m
[1;32m   3315[0m     [38;5;28;01mreturn[39;00m loss_mb[38;5;241m.[39mreduce_mean()[38;5;241m.[39mdetach()[38;5;241m.[39mto([38;5;28mself[39m[38;5;241m.[39margs[38;5;241m.[39mdevice)
[1;32m   3317[0m [38;5;28;01mwith[39;00m [38;5;28mself[39m[38;5;241m.[39mcompute_loss_context_manager():
[0;32m-> 3318[0m     loss [38;5;241m=[39m [38;5;28mself[39m[38;5;241m.[39mcompute_loss(model, inputs)
[1;32m   3320[0m [38;5;28;01mdel[39;00m inputs
[1;32m   3321[0m [38;5;28;01mif[39;00m (
[1;32m   3322[0m     [38;5;28mself[39m[38;5;241m.[39margs[38;5;241m.[39mtorch_empty_cache_steps [38;5;129;01mis[39;00m [38;5;129;01mnot[39;00m [38;5;28;01mNone[39;00m
[1;32m   3323[0m     [38;5;129;01mand[39;00m [38;5;28mself[39m[38;5;241m.[39mstate[38;5;241m.[39mglobal_step [38;5;241m%[39m [38;5;28mself[39m[38;5;241m.[39margs[38;5;241m.[39mtorch_empty_cache_steps [38;5;241m==[39m [38;5;241m0[39m
[1;32m   3324[0m ):

File [0;32m~/anaconda3/lib/python3.12/site-packages/transformers/trainer.py:3363[0m, in [0;36mTrainer.compute_loss[0;34m(self, model, inputs, return_outputs)[0m
[1;32m   3361[0m [38;5;28;01melse[39;00m:
[1;32m   3362[0m     labels [38;5;241m=[39m [38;5;28;01mNone[39;00m
[0;32m-> 3363[0m outputs [38;5;241m=[39m model([38;5;241m*[39m[38;5;241m*[39minputs)
[1;32m   3364[0m [38;5;66;03m# Save past state if it exists[39;00m
[1;32m   3365[0m [38;5;66;03m# TODO: this needs to be fixed and made cleaner later.[39;00m
[1;32m   3366[0m [38;5;28;01mif[39;00m [38;5;28mself[39m[38;5;241m.[39margs[38;5;241m.[39mpast_index [38;5;241m>[39m[38;5;241m=[39m [38;5;241m0[39m:

File [0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553[0m, in [0;36mModule._wrapped_call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1551[0m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m[38;5;241m.[39m_compiled_call_impl([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)  [38;5;66;03m# type: ignore[misc][39;00m
[1;32m   1552[0m [38;5;28;01melse[39;00m:
[0;32m-> 1553[0m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m[38;5;241m.[39m_call_impl([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)

File [0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562[0m, in [0;36mModule._call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1557[0m [38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in[39;00m
[1;32m   1558[0m [38;5;66;03m# this function, and just call forward.[39;00m
[1;32m   1559[0m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m ([38;5;28mself[39m[38;5;241m.[39m_backward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_backward_pre_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_pre_hooks
[1;32m   1560[0m         [38;5;129;01mor[39;00m _global_backward_pre_hooks [38;5;129;01mor[39;00m _global_backward_hooks
[1;32m   1561[0m         [38;5;129;01mor[39;00m _global_forward_hooks [38;5;129;01mor[39;00m _global_forward_pre_hooks):
[0;32m-> 1562[0m     [38;5;28;01mreturn[39;00m forward_call([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)
[1;32m   1564[0m [38;5;28;01mtry[39;00m:
[1;32m   1565[0m     result [38;5;241m=[39m [38;5;28;01mNone[39;00m

File [0;32m~/anaconda3/lib/python3.12/site-packages/accelerate/utils/operations.py:820[0m, in [0;36mconvert_outputs_to_fp32.<locals>.forward[0;34m(*args, **kwargs)[0m
[1;32m    819[0m [38;5;28;01mdef[39;00m [38;5;21mforward[39m([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs):
[0;32m--> 820[0m     [38;5;28;01mreturn[39;00m model_forward([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)

File [0;32m~/anaconda3/lib/python3.12/site-packages/accelerate/utils/operations.py:808[0m, in [0;36mConvertOutputsToFp32.__call__[0;34m(self, *args, **kwargs)[0m
[1;32m    807[0m [38;5;28;01mdef[39;00m [38;5;21m__call__[39m([38;5;28mself[39m, [38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs):
[0;32m--> 808[0m     [38;5;28;01mreturn[39;00m convert_to_fp32([38;5;28mself[39m[38;5;241m.[39mmodel_forward([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs))

File [0;32m~/anaconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:43[0m, in [0;36mautocast_decorator.<locals>.decorate_autocast[0;34m(*args, **kwargs)[0m
[1;32m     40[0m [38;5;129m@functools[39m[38;5;241m.[39mwraps(func)
[1;32m     41[0m [38;5;28;01mdef[39;00m [38;5;21mdecorate_autocast[39m([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs):
[1;32m     42[0m     [38;5;28;01mwith[39;00m autocast_instance:
[0;32m---> 43[0m         [38;5;28;01mreturn[39;00m func([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)

File [0;32m~/anaconda3/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:1702[0m, in [0;36mT5ForConditionalGeneration.forward[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)[0m
[1;32m   1699[0m [38;5;66;03m# Encode if needed (training, first prediction pass)[39;00m
[1;32m   1700[0m [38;5;28;01mif[39;00m encoder_outputs [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m:
[1;32m   1701[0m     [38;5;66;03m# Convert encoder inputs in embeddings if needed[39;00m
[0;32m-> 1702[0m     encoder_outputs [38;5;241m=[39m [38;5;28mself[39m[38;5;241m.[39mencoder(
[1;32m   1703[0m         input_ids[38;5;241m=[39minput_ids,
[1;32m   1704[0m         attention_mask[38;5;241m=[39mattention_mask,
[1;32m   1705[0m         inputs_embeds[38;5;241m=[39minputs_embeds,
[1;32m   1706[0m         head_mask[38;5;241m=[39mhead_mask,
[1;32m   1707[0m         output_attentions[38;5;241m=[39moutput_attentions,
[1;32m   1708[0m         output_hidden_states[38;5;241m=[39moutput_hidden_states,
[1;32m   1709[0m         return_dict[38;5;241m=[39mreturn_dict,
[1;32m   1710[0m     )
[1;32m   1711[0m [38;5;28;01melif[39;00m return_dict [38;5;129;01mand[39;00m [38;5;129;01mnot[39;00m [38;5;28misinstance[39m(encoder_outputs, BaseModelOutput):
[1;32m   1712[0m     encoder_outputs [38;5;241m=[39m BaseModelOutput(
[1;32m   1713[0m         last_hidden_state[38;5;241m=[39mencoder_outputs[[38;5;241m0[39m],
[1;32m   1714[0m         hidden_states[38;5;241m=[39mencoder_outputs[[38;5;241m1[39m] [38;5;28;01mif[39;00m [38;5;28mlen[39m(encoder_outputs) [38;5;241m>[39m [38;5;241m1[39m [38;5;28;01melse[39;00m [38;5;28;01mNone[39;00m,
[1;32m   1715[0m         attentions[38;5;241m=[39mencoder_outputs[[38;5;241m2[39m] [38;5;28;01mif[39;00m [38;5;28mlen[39m(encoder_outputs) [38;5;241m>[39m [38;5;241m2[39m [38;5;28;01melse[39;00m [38;5;28;01mNone[39;00m,
[1;32m   1716[0m     )

File [0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553[0m, in [0;36mModule._wrapped_call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1551[0m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m[38;5;241m.[39m_compiled_call_impl([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)  [38;5;66;03m# type: ignore[misc][39;00m
[1;32m   1552[0m [38;5;28;01melse[39;00m:
[0;32m-> 1553[0m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m[38;5;241m.[39m_call_impl([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)

File [0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562[0m, in [0;36mModule._call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1557[0m [38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in[39;00m
[1;32m   1558[0m [38;5;66;03m# this function, and just call forward.[39;00m
[1;32m   1559[0m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m ([38;5;28mself[39m[38;5;241m.[39m_backward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_backward_pre_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_pre_hooks
[1;32m   1560[0m         [38;5;129;01mor[39;00m _global_backward_pre_hooks [38;5;129;01mor[39;00m _global_backward_hooks
[1;32m   1561[0m         [38;5;129;01mor[39;00m _global_forward_hooks [38;5;129;01mor[39;00m _global_forward_pre_hooks):
[0;32m-> 1562[0m     [38;5;28;01mreturn[39;00m forward_call([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)
[1;32m   1564[0m [38;5;28;01mtry[39;00m:
[1;32m   1565[0m     result [38;5;241m=[39m [38;5;28;01mNone[39;00m

File [0;32m~/anaconda3/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:1106[0m, in [0;36mT5Stack.forward[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)[0m
[1;32m   1091[0m     layer_outputs [38;5;241m=[39m [38;5;28mself[39m[38;5;241m.[39m_gradient_checkpointing_func(
[1;32m   1092[0m         layer_module[38;5;241m.[39mforward,
[1;32m   1093[0m         hidden_states,
[0;32m   (...)[0m
[1;32m   1103[0m         output_attentions,
[1;32m   1104[0m     )
[1;32m   1105[0m [38;5;28;01melse[39;00m:
[0;32m-> 1106[0m     layer_outputs [38;5;241m=[39m layer_module(
[1;32m   1107[0m         hidden_states,
[1;32m   1108[0m         attention_mask[38;5;241m=[39mextended_attention_mask,
[1;32m   1109[0m         position_bias[38;5;241m=[39mposition_bias,
[1;32m   1110[0m         encoder_hidden_states[38;5;241m=[39mencoder_hidden_states,
[1;32m   1111[0m         encoder_attention_mask[38;5;241m=[39mencoder_extended_attention_mask,
[1;32m   1112[0m         encoder_decoder_position_bias[38;5;241m=[39mencoder_decoder_position_bias,
[1;32m   1113[0m         layer_head_mask[38;5;241m=[39mlayer_head_mask,
[1;32m   1114[0m         cross_attn_layer_head_mask[38;5;241m=[39mcross_attn_layer_head_mask,
[1;32m   1115[0m         past_key_value[38;5;241m=[39mpast_key_value,
[1;32m   1116[0m         use_cache[38;5;241m=[39muse_cache,
[1;32m   1117[0m         output_attentions[38;5;241m=[39moutput_attentions,
[1;32m   1118[0m     )
[1;32m   1120[0m [38;5;66;03m# layer_outputs is a tuple with:[39;00m
[1;32m   1121[0m [38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)[39;00m
[1;32m   1122[0m [38;5;28;01mif[39;00m use_cache [38;5;129;01mis[39;00m [38;5;28;01mFalse[39;00m:

File [0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553[0m, in [0;36mModule._wrapped_call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1551[0m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m[38;5;241m.[39m_compiled_call_impl([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)  [38;5;66;03m# type: ignore[misc][39;00m
[1;32m   1552[0m [38;5;28;01melse[39;00m:
[0;32m-> 1553[0m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m[38;5;241m.[39m_call_impl([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)

File [0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562[0m, in [0;36mModule._call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1557[0m [38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in[39;00m
[1;32m   1558[0m [38;5;66;03m# this function, and just call forward.[39;00m
[1;32m   1559[0m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m ([38;5;28mself[39m[38;5;241m.[39m_backward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_backward_pre_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_pre_hooks
[1;32m   1560[0m         [38;5;129;01mor[39;00m _global_backward_pre_hooks [38;5;129;01mor[39;00m _global_backward_hooks
[1;32m   1561[0m         [38;5;129;01mor[39;00m _global_forward_hooks [38;5;129;01mor[39;00m _global_forward_pre_hooks):
[0;32m-> 1562[0m     [38;5;28;01mreturn[39;00m forward_call([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)
[1;32m   1564[0m [38;5;28;01mtry[39;00m:
[1;32m   1565[0m     result [38;5;241m=[39m [38;5;28;01mNone[39;00m

File [0;32m~/anaconda3/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:686[0m, in [0;36mT5Block.forward[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)[0m
[1;32m    683[0m [38;5;28;01melse[39;00m:
[1;32m    684[0m     self_attn_past_key_value, cross_attn_past_key_value [38;5;241m=[39m [38;5;28;01mNone[39;00m, [38;5;28;01mNone[39;00m
[0;32m--> 686[0m self_attention_outputs [38;5;241m=[39m [38;5;28mself[39m[38;5;241m.[39mlayer[[38;5;241m0[39m](
[1;32m    687[0m     hidden_states,
[1;32m    688[0m     attention_mask[38;5;241m=[39mattention_mask,
[1;32m    689[0m     position_bias[38;5;241m=[39mposition_bias,
[1;32m    690[0m     layer_head_mask[38;5;241m=[39mlayer_head_mask,
[1;32m    691[0m     past_key_value[38;5;241m=[39mself_attn_past_key_value,
[1;32m    692[0m     use_cache[38;5;241m=[39muse_cache,
[1;32m    693[0m     output_attentions[38;5;241m=[39moutput_attentions,
[1;32m    694[0m )
[1;32m    695[0m hidden_states, present_key_value_state [38;5;241m=[39m self_attention_outputs[:[38;5;241m2[39m]
[1;32m    696[0m attention_outputs [38;5;241m=[39m self_attention_outputs[[38;5;241m2[39m:]  [38;5;66;03m# Keep self-attention outputs and relative position weights[39;00m

File [0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553[0m, in [0;36mModule._wrapped_call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1551[0m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m[38;5;241m.[39m_compiled_call_impl([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)  [38;5;66;03m# type: ignore[misc][39;00m
[1;32m   1552[0m [38;5;28;01melse[39;00m:
[0;32m-> 1553[0m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m[38;5;241m.[39m_call_impl([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)

File [0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562[0m, in [0;36mModule._call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1557[0m [38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in[39;00m
[1;32m   1558[0m [38;5;66;03m# this function, and just call forward.[39;00m
[1;32m   1559[0m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m ([38;5;28mself[39m[38;5;241m.[39m_backward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_backward_pre_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_pre_hooks
[1;32m   1560[0m         [38;5;129;01mor[39;00m _global_backward_pre_hooks [38;5;129;01mor[39;00m _global_backward_hooks
[1;32m   1561[0m         [38;5;129;01mor[39;00m _global_forward_hooks [38;5;129;01mor[39;00m _global_forward_pre_hooks):
[0;32m-> 1562[0m     [38;5;28;01mreturn[39;00m forward_call([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)
[1;32m   1564[0m [38;5;28;01mtry[39;00m:
[1;32m   1565[0m     result [38;5;241m=[39m [38;5;28;01mNone[39;00m

File [0;32m~/anaconda3/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:592[0m, in [0;36mT5LayerSelfAttention.forward[0;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions)[0m
[1;32m    582[0m [38;5;28;01mdef[39;00m [38;5;21mforward[39m(
[1;32m    583[0m     [38;5;28mself[39m,
[1;32m    584[0m     hidden_states,
[0;32m   (...)[0m
[1;32m    590[0m     output_attentions[38;5;241m=[39m[38;5;28;01mFalse[39;00m,
[1;32m    591[0m ):
[0;32m--> 592[0m     normed_hidden_states [38;5;241m=[39m [38;5;28mself[39m[38;5;241m.[39mlayer_norm(hidden_states)
[1;32m    593[0m     attention_output [38;5;241m=[39m [38;5;28mself[39m[38;5;241m.[39mSelfAttention(
[1;32m    594[0m         normed_hidden_states,
[1;32m    595[0m         mask[38;5;241m=[39mattention_mask,
[0;32m   (...)[0m
[1;32m    600[0m         output_attentions[38;5;241m=[39moutput_attentions,
[1;32m    601[0m     )
[1;32m    602[0m     hidden_states [38;5;241m=[39m hidden_states [38;5;241m+[39m [38;5;28mself[39m[38;5;241m.[39mdropout(attention_output[[38;5;241m0[39m])

File [0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553[0m, in [0;36mModule._wrapped_call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1551[0m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m[38;5;241m.[39m_compiled_call_impl([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)  [38;5;66;03m# type: ignore[misc][39;00m
[1;32m   1552[0m [38;5;28;01melse[39;00m:
[0;32m-> 1553[0m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m[38;5;241m.[39m_call_impl([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)

File [0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562[0m, in [0;36mModule._call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1557[0m [38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in[39;00m
[1;32m   1558[0m [38;5;66;03m# this function, and just call forward.[39;00m
[1;32m   1559[0m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m ([38;5;28mself[39m[38;5;241m.[39m_backward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_backward_pre_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_pre_hooks
[1;32m   1560[0m         [38;5;129;01mor[39;00m _global_backward_pre_hooks [38;5;129;01mor[39;00m _global_backward_hooks
[1;32m   1561[0m         [38;5;129;01mor[39;00m _global_forward_hooks [38;5;129;01mor[39;00m _global_forward_pre_hooks):
[0;32m-> 1562[0m     [38;5;28;01mreturn[39;00m forward_call([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)
[1;32m   1564[0m [38;5;28;01mtry[39;00m:
[1;32m   1565[0m     result [38;5;241m=[39m [38;5;28;01mNone[39;00m

File [0;32m~/anaconda3/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:245[0m, in [0;36mT5LayerNorm.forward[0;34m(self, hidden_states)[0m
[1;32m    239[0m [38;5;28;01mdef[39;00m [38;5;21mforward[39m([38;5;28mself[39m, hidden_states):
[1;32m    240[0m     [38;5;66;03m# T5 uses a layer_norm which only scales and doesn't shift, which is also known as Root Mean[39;00m
[1;32m    241[0m     [38;5;66;03m# Square Layer Normalization https://arxiv.org/abs/1910.07467 thus varience is calculated[39;00m
[1;32m    242[0m     [38;5;66;03m# w/o mean and there is no bias. Additionally we want to make sure that the accumulation for[39;00m
[1;32m    243[0m     [38;5;66;03m# half-precision inputs is done in fp32[39;00m
[0;32m--> 245[0m     variance [38;5;241m=[39m hidden_states[38;5;241m.[39mto(torch[38;5;241m.[39mfloat32)[38;5;241m.[39mpow([38;5;241m2[39m)[38;5;241m.[39mmean([38;5;241m-[39m[38;5;241m1[39m, keepdim[38;5;241m=[39m[38;5;28;01mTrue[39;00m)
[1;32m    246[0m     hidden_states [38;5;241m=[39m hidden_states [38;5;241m*[39m torch[38;5;241m.[39mrsqrt(variance [38;5;241m+[39m [38;5;28mself[39m[38;5;241m.[39mvariance_epsilon)
[1;32m    248[0m     [38;5;66;03m# convert into half-precision if necessary[39;00m

[0;31mOutOfMemoryError[0m: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 7.63 GiB of which 34.00 MiB is free. Including non-PyTorch memory, this process has 6.93 GiB memory in use. Of the allocated memory 6.76 GiB is allocated by PyTorch, and 15.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[23;0t
