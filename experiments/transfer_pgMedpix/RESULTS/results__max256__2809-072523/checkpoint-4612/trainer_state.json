{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.9998915989159891,
  "eval_steps": 6000,
  "global_step": 4612,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0,
      "step": 0,
      "training_loss": 11.458562850952148
    },
    {
      "epoch": 0,
      "step": 0,
      "training_loss": 12.426315307617188
    },
    {
      "epoch": 0,
      "step": 0,
      "training_loss": 36.26213073730469
    },
    {
      "epoch": 0,
      "step": 0,
      "training_loss": 12.55823040008545
    },
    {
      "epoch": 0.00021680216802168022,
      "step": 1,
      "training_loss": 11.656610488891602
    },
    {
      "epoch": 0.00021680216802168022,
      "step": 1,
      "training_loss": 12.587981224060059
    },
    {
      "epoch": 0.00021680216802168022,
      "step": 1,
      "training_loss": 11.465662956237793
    },
    {
      "epoch": 0.00021680216802168022,
      "step": 1,
      "training_loss": 11.929010391235352
    },
    {
      "epoch": 0.00043360433604336043,
      "step": 2,
      "training_loss": 12.198442459106445
    },
    {
      "epoch": 0.00043360433604336043,
      "step": 2,
      "training_loss": 12.258001327514648
    },
    {
      "epoch": 0.00043360433604336043,
      "step": 2,
      "training_loss": 13.903679847717285
    },
    {
      "epoch": 0.00043360433604336043,
      "step": 2,
      "training_loss": 11.844098091125488
    },
    {
      "epoch": 0.0006504065040650406,
      "step": 3,
      "training_loss": 11.819459915161133
    },
    {
      "epoch": 0.0006504065040650406,
      "step": 3,
      "training_loss": 12.934650421142578
    },
    {
      "epoch": 0.0006504065040650406,
      "step": 3,
      "training_loss": 11.953208923339844
    },
    {
      "epoch": 0.0006504065040650406,
      "step": 3,
      "training_loss": 12.883639335632324
    },
    {
      "epoch": 0.0008672086720867209,
      "grad_norm": NaN,
      "learning_rate": 1e-05,
      "loss": 13.7587,
      "step": 4
    },
    {
      "epoch": 0.0008672086720867209,
      "step": 4,
      "training_loss": 12.585869789123535
    },
    {
      "epoch": 0.0008672086720867209,
      "step": 4,
      "training_loss": 11.81513500213623
    },
    {
      "epoch": 0.0008672086720867209,
      "step": 4,
      "training_loss": 12.895086288452148
    },
    {
      "epoch": 0.0008672086720867209,
      "step": 4,
      "training_loss": 11.210583686828613
    },
    {
      "epoch": 0.001084010840108401,
      "step": 5,
      "training_loss": 12.213593482971191
    },
    {
      "epoch": 0.001084010840108401,
      "step": 5,
      "training_loss": 13.09145736694336
    },
    {
      "epoch": 0.001084010840108401,
      "step": 5,
      "training_loss": 12.80006217956543
    },
    {
      "epoch": 0.001084010840108401,
      "step": 5,
      "training_loss": 12.888026237487793
    },
    {
      "epoch": 0.0013008130081300813,
      "step": 6,
      "training_loss": 16.55288314819336
    },
    {
      "epoch": 0.0013008130081300813,
      "step": 6,
      "training_loss": 11.808995246887207
    },
    {
      "epoch": 0.0013008130081300813,
      "step": 6,
      "training_loss": 12.034102439880371
    },
    {
      "epoch": 0.0013008130081300813,
      "step": 6,
      "training_loss": 10.378800392150879
    },
    {
      "epoch": 0.0015176151761517615,
      "step": 7,
      "training_loss": 12.813860893249512
    },
    {
      "epoch": 0.0015176151761517615,
      "step": 7,
      "training_loss": 11.279294967651367
    },
    {
      "epoch": 0.0015176151761517615,
      "step": 7,
      "training_loss": 11.913098335266113
    },
    {
      "epoch": 0.0015176151761517615,
      "step": 7,
      "training_loss": 12.295784950256348
    },
    {
      "epoch": 0.0017344173441734417,
      "grad_norm": 257.8070068359375,
      "learning_rate": 1e-05,
      "loss": 12.411,
      "step": 8
    },
    {
      "epoch": 0.0017344173441734417,
      "step": 8,
      "training_loss": 10.768017768859863
    },
    {
      "epoch": 0.0017344173441734417,
      "step": 8,
      "training_loss": 11.806264877319336
    },
    {
      "epoch": 0.0017344173441734417,
      "step": 8,
      "training_loss": 12.632651329040527
    },
    {
      "epoch": 0.0017344173441734417,
      "step": 8,
      "training_loss": 12.18397045135498
    },
    {
      "epoch": 0.001951219512195122,
      "step": 9,
      "training_loss": 12.846508979797363
    },
    {
      "epoch": 0.001951219512195122,
      "step": 9,
      "training_loss": 12.808541297912598
    },
    {
      "epoch": 0.001951219512195122,
      "step": 9,
      "training_loss": 13.617380142211914
    },
    {
      "epoch": 0.001951219512195122,
      "step": 9,
      "training_loss": 12.031545639038086
    },
    {
      "epoch": 0.002168021680216802,
      "step": 10,
      "training_loss": 10.946611404418945
    },
    {
      "epoch": 0.002168021680216802,
      "step": 10,
      "training_loss": 10.543678283691406
    },
    {
      "epoch": 0.002168021680216802,
      "step": 10,
      "training_loss": 13.284244537353516
    },
    {
      "epoch": 0.002168021680216802,
      "step": 10,
      "training_loss": 12.849996566772461
    },
    {
      "epoch": 0.0023848238482384824,
      "step": 11,
      "training_loss": 13.316246032714844
    },
    {
      "epoch": 0.0023848238482384824,
      "step": 11,
      "training_loss": 13.209609985351562
    },
    {
      "epoch": 0.0023848238482384824,
      "step": 11,
      "training_loss": 13.090795516967773
    },
    {
      "epoch": 0.0023848238482384824,
      "step": 11,
      "training_loss": 13.122804641723633
    },
    {
      "epoch": 0.0026016260162601626,
      "grad_norm": 956.4659423828125,
      "learning_rate": 1e-05,
      "loss": 12.4412,
      "step": 12
    },
    {
      "epoch": 0.0026016260162601626,
      "step": 12,
      "training_loss": 12.116061210632324
    },
    {
      "epoch": 0.0026016260162601626,
      "step": 12,
      "training_loss": 13.250920295715332
    },
    {
      "epoch": 0.0026016260162601626,
      "step": 12,
      "training_loss": 11.611281394958496
    },
    {
      "epoch": 0.0026016260162601626,
      "step": 12,
      "training_loss": 11.796618461608887
    },
    {
      "epoch": 0.002818428184281843,
      "step": 13,
      "training_loss": 12.753524780273438
    },
    {
      "epoch": 0.002818428184281843,
      "step": 13,
      "training_loss": 11.657181739807129
    },
    {
      "epoch": 0.002818428184281843,
      "step": 13,
      "training_loss": 12.836631774902344
    },
    {
      "epoch": 0.002818428184281843,
      "step": 13,
      "training_loss": 12.481934547424316
    },
    {
      "epoch": 0.003035230352303523,
      "step": 14,
      "training_loss": 12.52478313446045
    },
    {
      "epoch": 0.003035230352303523,
      "step": 14,
      "training_loss": 11.856612205505371
    },
    {
      "epoch": 0.003035230352303523,
      "step": 14,
      "training_loss": 11.432311058044434
    },
    {
      "epoch": 0.003035230352303523,
      "step": 14,
      "training_loss": 12.387460708618164
    },
    {
      "epoch": 0.0032520325203252032,
      "step": 15,
      "training_loss": 13.829143524169922
    },
    {
      "epoch": 0.0032520325203252032,
      "step": 15,
      "training_loss": 13.410964965820312
    },
    {
      "epoch": 0.0032520325203252032,
      "step": 15,
      "training_loss": 13.880173683166504
    },
    {
      "epoch": 0.0032520325203252032,
      "step": 15,
      "training_loss": 13.729913711547852
    },
    {
      "epoch": 0.0034688346883468835,
      "grad_norm": 133.61614990234375,
      "learning_rate": 1e-05,
      "loss": 12.5972,
      "step": 16
    },
    {
      "epoch": 0.0034688346883468835,
      "step": 16,
      "training_loss": 12.596051216125488
    },
    {
      "epoch": 0.0034688346883468835,
      "step": 16,
      "training_loss": 12.39728832244873
    },
    {
      "epoch": 0.0034688346883468835,
      "step": 16,
      "training_loss": 11.214624404907227
    },
    {
      "epoch": 0.0034688346883468835,
      "step": 16,
      "training_loss": 12.123148918151855
    },
    {
      "epoch": 0.0036856368563685637,
      "step": 17,
      "training_loss": 11.41363525390625
    },
    {
      "epoch": 0.0036856368563685637,
      "step": 17,
      "training_loss": 11.665146827697754
    },
    {
      "epoch": 0.0036856368563685637,
      "step": 17,
      "training_loss": 10.549247741699219
    },
    {
      "epoch": 0.0036856368563685637,
      "step": 17,
      "training_loss": 12.210662841796875
    },
    {
      "epoch": 0.003902439024390244,
      "step": 18,
      "training_loss": 12.880047798156738
    },
    {
      "epoch": 0.003902439024390244,
      "step": 18,
      "training_loss": 11.917532920837402
    },
    {
      "epoch": 0.003902439024390244,
      "step": 18,
      "training_loss": 12.226015090942383
    },
    {
      "epoch": 0.003902439024390244,
      "step": 18,
      "training_loss": 11.495735168457031
    },
    {
      "epoch": 0.004119241192411924,
      "step": 19,
      "training_loss": 13.040058135986328
    },
    {
      "epoch": 0.004119241192411924,
      "step": 19,
      "training_loss": 10.931778907775879
    },
    {
      "epoch": 0.004119241192411924,
      "step": 19,
      "training_loss": 10.977572441101074
    },
    {
      "epoch": 0.004119241192411924,
      "step": 19,
      "training_loss": 11.097881317138672
    },
    {
      "epoch": 0.004336043360433604,
      "grad_norm": 318.05322265625,
      "learning_rate": 1e-05,
      "loss": 11.796,
      "step": 20
    },
    {
      "epoch": 0.004336043360433604,
      "step": 20,
      "training_loss": 14.248425483703613
    },
    {
      "epoch": 0.004336043360433604,
      "step": 20,
      "training_loss": 11.918573379516602
    },
    {
      "epoch": 0.004336043360433604,
      "step": 20,
      "training_loss": 10.004987716674805
    },
    {
      "epoch": 0.004336043360433604,
      "step": 20,
      "training_loss": 9.58729076385498
    },
    {
      "epoch": 0.0045528455284552845,
      "step": 21,
      "training_loss": 10.976823806762695
    },
    {
      "epoch": 0.0045528455284552845,
      "step": 21,
      "training_loss": 11.752579689025879
    },
    {
      "epoch": 0.0045528455284552845,
      "step": 21,
      "training_loss": 11.930525779724121
    },
    {
      "epoch": 0.0045528455284552845,
      "step": 21,
      "training_loss": 11.060833930969238
    },
    {
      "epoch": 0.004769647696476965,
      "step": 22,
      "training_loss": 12.360039710998535
    },
    {
      "epoch": 0.004769647696476965,
      "step": 22,
      "training_loss": 14.095599174499512
    },
    {
      "epoch": 0.004769647696476965,
      "step": 22,
      "training_loss": 12.477606773376465
    },
    {
      "epoch": 0.004769647696476965,
      "step": 22,
      "training_loss": 11.406213760375977
    },
    {
      "epoch": 0.004986449864498645,
      "step": 23,
      "training_loss": 13.206520080566406
    },
    {
      "epoch": 0.004986449864498645,
      "step": 23,
      "training_loss": 11.280794143676758
    },
    {
      "epoch": 0.004986449864498645,
      "step": 23,
      "training_loss": 11.421768188476562
    },
    {
      "epoch": 0.004986449864498645,
      "step": 23,
      "training_loss": 12.152509689331055
    },
    {
      "epoch": 0.005203252032520325,
      "grad_norm": 470.3500671386719,
      "learning_rate": 1e-05,
      "loss": 11.8676,
      "step": 24
    },
    {
      "epoch": 0.005203252032520325,
      "step": 24,
      "training_loss": 11.348692893981934
    },
    {
      "epoch": 0.005203252032520325,
      "step": 24,
      "training_loss": 11.165254592895508
    },
    {
      "epoch": 0.005203252032520325,
      "step": 24,
      "training_loss": 12.693574905395508
    },
    {
      "epoch": 0.005203252032520325,
      "step": 24,
      "training_loss": 12.271294593811035
    },
    {
      "epoch": 0.005420054200542005,
      "step": 25,
      "training_loss": 11.527099609375
    },
    {
      "epoch": 0.005420054200542005,
      "step": 25,
      "training_loss": 12.428112983703613
    },
    {
      "epoch": 0.005420054200542005,
      "step": 25,
      "training_loss": 12.619234085083008
    },
    {
      "epoch": 0.005420054200542005,
      "step": 25,
      "training_loss": 11.719436645507812
    },
    {
      "epoch": 0.005636856368563686,
      "step": 26,
      "training_loss": 11.13652229309082
    },
    {
      "epoch": 0.005636856368563686,
      "step": 26,
      "training_loss": 12.756791114807129
    },
    {
      "epoch": 0.005636856368563686,
      "step": 26,
      "training_loss": 11.158803939819336
    },
    {
      "epoch": 0.005636856368563686,
      "step": 26,
      "training_loss": 11.134981155395508
    },
    {
      "epoch": 0.005853658536585366,
      "step": 27,
      "training_loss": 11.977128982543945
    },
    {
      "epoch": 0.005853658536585366,
      "step": 27,
      "training_loss": 11.345928192138672
    },
    {
      "epoch": 0.005853658536585366,
      "step": 27,
      "training_loss": 11.230192184448242
    },
    {
      "epoch": 0.005853658536585366,
      "step": 27,
      "training_loss": 13.210217475891113
    },
    {
      "epoch": 0.006070460704607046,
      "grad_norm": NaN,
      "learning_rate": 1e-05,
      "loss": 11.8577,
      "step": 28
    },
    {
      "epoch": 0.006070460704607046,
      "step": 28,
      "training_loss": 11.101593017578125
    },
    {
      "epoch": 0.006070460704607046,
      "step": 28,
      "training_loss": 11.57370662689209
    },
    {
      "epoch": 0.006070460704607046,
      "step": 28,
      "training_loss": 11.728682518005371
    },
    {
      "epoch": 0.006070460704607046,
      "step": 28,
      "training_loss": 12.92201042175293
    },
    {
      "epoch": 0.006287262872628726,
      "step": 29,
      "training_loss": 11.965493202209473
    },
    {
      "epoch": 0.006287262872628726,
      "step": 29,
      "training_loss": 12.637529373168945
    },
    {
      "epoch": 0.006287262872628726,
      "step": 29,
      "training_loss": 12.182782173156738
    },
    {
      "epoch": 0.006287262872628726,
      "step": 29,
      "training_loss": 20.807106018066406
    },
    {
      "epoch": 0.0065040650406504065,
      "step": 30,
      "training_loss": 11.840847969055176
    },
    {
      "epoch": 0.0065040650406504065,
      "step": 30,
      "training_loss": 11.28408432006836
    },
    {
      "epoch": 0.0065040650406504065,
      "step": 30,
      "training_loss": 12.534539222717285
    },
    {
      "epoch": 0.0065040650406504065,
      "step": 30,
      "training_loss": 12.56774616241455
    },
    {
      "epoch": 0.006720867208672087,
      "step": 31,
      "training_loss": 11.782092094421387
    },
    {
      "epoch": 0.006720867208672087,
      "step": 31,
      "training_loss": 11.017529487609863
    },
    {
      "epoch": 0.006720867208672087,
      "step": 31,
      "training_loss": 12.201333999633789
    },
    {
      "epoch": 0.006720867208672087,
      "step": 31,
      "training_loss": 12.540549278259277
    },
    {
      "epoch": 0.006937669376693767,
      "grad_norm": 185.11605834960938,
      "learning_rate": 1e-05,
      "loss": 12.543,
      "step": 32
    },
    {
      "epoch": 0.006937669376693767,
      "step": 32,
      "training_loss": 10.813491821289062
    },
    {
      "epoch": 0.006937669376693767,
      "step": 32,
      "training_loss": 11.080595970153809
    },
    {
      "epoch": 0.006937669376693767,
      "step": 32,
      "training_loss": 11.527863502502441
    },
    {
      "epoch": 0.006937669376693767,
      "step": 32,
      "training_loss": 12.290899276733398
    },
    {
      "epoch": 0.007154471544715447,
      "step": 33,
      "training_loss": 12.052085876464844
    },
    {
      "epoch": 0.007154471544715447,
      "step": 33,
      "training_loss": 11.44066333770752
    },
    {
      "epoch": 0.007154471544715447,
      "step": 33,
      "training_loss": 12.268414497375488
    },
    {
      "epoch": 0.007154471544715447,
      "step": 33,
      "training_loss": 12.823366165161133
    },
    {
      "epoch": 0.007371273712737127,
      "step": 34,
      "training_loss": 11.311810493469238
    },
    {
      "epoch": 0.007371273712737127,
      "step": 34,
      "training_loss": 11.654996871948242
    },
    {
      "epoch": 0.007371273712737127,
      "step": 34,
      "training_loss": 11.701005935668945
    },
    {
      "epoch": 0.007371273712737127,
      "step": 34,
      "training_loss": 14.377203941345215
    },
    {
      "epoch": 0.007588075880758808,
      "step": 35,
      "training_loss": 12.680988311767578
    },
    {
      "epoch": 0.007588075880758808,
      "step": 35,
      "training_loss": 10.079978942871094
    },
    {
      "epoch": 0.007588075880758808,
      "step": 35,
      "training_loss": 11.637107849121094
    },
    {
      "epoch": 0.007588075880758808,
      "step": 35,
      "training_loss": 10.579911231994629
    },
    {
      "epoch": 0.007804878048780488,
      "grad_norm": 38.89251708984375,
      "learning_rate": 1e-05,
      "loss": 11.77,
      "step": 36
    },
    {
      "epoch": 0.007804878048780488,
      "step": 36,
      "training_loss": 12.673018455505371
    },
    {
      "epoch": 0.007804878048780488,
      "step": 36,
      "training_loss": 11.657429695129395
    },
    {
      "epoch": 0.007804878048780488,
      "step": 36,
      "training_loss": 13.10355281829834
    },
    {
      "epoch": 0.007804878048780488,
      "step": 36,
      "training_loss": 12.299590110778809
    },
    {
      "epoch": 0.008021680216802168,
      "step": 37,
      "training_loss": 11.515899658203125
    },
    {
      "epoch": 0.008021680216802168,
      "step": 37,
      "training_loss": 11.195992469787598
    },
    {
      "epoch": 0.008021680216802168,
      "step": 37,
      "training_loss": 10.281453132629395
    },
    {
      "epoch": 0.008021680216802168,
      "step": 37,
      "training_loss": 11.13774299621582
    },
    {
      "epoch": 0.008238482384823848,
      "step": 38,
      "training_loss": 11.469161987304688
    },
    {
      "epoch": 0.008238482384823848,
      "step": 38,
      "training_loss": 11.431034088134766
    },
    {
      "epoch": 0.008238482384823848,
      "step": 38,
      "training_loss": 11.21944808959961
    },
    {
      "epoch": 0.008238482384823848,
      "step": 38,
      "training_loss": 11.589136123657227
    },
    {
      "epoch": 0.008455284552845528,
      "step": 39,
      "training_loss": 11.486983299255371
    },
    {
      "epoch": 0.008455284552845528,
      "step": 39,
      "training_loss": 13.28955364227295
    },
    {
      "epoch": 0.008455284552845528,
      "step": 39,
      "training_loss": 10.069796562194824
    },
    {
      "epoch": 0.008455284552845528,
      "step": 39,
      "training_loss": 12.492613792419434
    },
    {
      "epoch": 0.008672086720867209,
      "grad_norm": 143.4229736328125,
      "learning_rate": 1e-05,
      "loss": 11.682,
      "step": 40
    },
    {
      "epoch": 0.008672086720867209,
      "step": 40,
      "training_loss": 10.247964859008789
    },
    {
      "epoch": 0.008672086720867209,
      "step": 40,
      "training_loss": 12.831314086914062
    },
    {
      "epoch": 0.008672086720867209,
      "step": 40,
      "training_loss": 10.425542831420898
    },
    {
      "epoch": 0.008672086720867209,
      "step": 40,
      "training_loss": 11.055216789245605
    },
    {
      "epoch": 0.008888888888888889,
      "step": 41,
      "training_loss": 10.284911155700684
    },
    {
      "epoch": 0.008888888888888889,
      "step": 41,
      "training_loss": 12.762950897216797
    },
    {
      "epoch": 0.008888888888888889,
      "step": 41,
      "training_loss": 9.793540000915527
    },
    {
      "epoch": 0.008888888888888889,
      "step": 41,
      "training_loss": 12.060232162475586
    },
    {
      "epoch": 0.009105691056910569,
      "step": 42,
      "training_loss": 10.720221519470215
    },
    {
      "epoch": 0.009105691056910569,
      "step": 42,
      "training_loss": 11.575395584106445
    },
    {
      "epoch": 0.009105691056910569,
      "step": 42,
      "training_loss": 12.746465682983398
    },
    {
      "epoch": 0.009105691056910569,
      "step": 42,
      "training_loss": 10.40291976928711
    },
    {
      "epoch": 0.00932249322493225,
      "step": 43,
      "training_loss": 10.387868881225586
    },
    {
      "epoch": 0.00932249322493225,
      "step": 43,
      "training_loss": 10.89380931854248
    },
    {
      "epoch": 0.00932249322493225,
      "step": 43,
      "training_loss": 10.732917785644531
    },
    {
      "epoch": 0.00932249322493225,
      "step": 43,
      "training_loss": 11.078319549560547
    },
    {
      "epoch": 0.00953929539295393,
      "grad_norm": 243.88197326660156,
      "learning_rate": 1e-05,
      "loss": 11.125,
      "step": 44
    },
    {
      "epoch": 0.00953929539295393,
      "step": 44,
      "training_loss": 11.367700576782227
    },
    {
      "epoch": 0.00953929539295393,
      "step": 44,
      "training_loss": 11.777183532714844
    },
    {
      "epoch": 0.00953929539295393,
      "step": 44,
      "training_loss": 11.378599166870117
    },
    {
      "epoch": 0.00953929539295393,
      "step": 44,
      "training_loss": 9.97325325012207
    },
    {
      "epoch": 0.00975609756097561,
      "step": 45,
      "training_loss": 11.435903549194336
    },
    {
      "epoch": 0.00975609756097561,
      "step": 45,
      "training_loss": 11.516525268554688
    },
    {
      "epoch": 0.00975609756097561,
      "step": 45,
      "training_loss": 10.107525825500488
    },
    {
      "epoch": 0.00975609756097561,
      "step": 45,
      "training_loss": 12.031611442565918
    },
    {
      "epoch": 0.00997289972899729,
      "step": 46,
      "training_loss": 11.244182586669922
    },
    {
      "epoch": 0.00997289972899729,
      "step": 46,
      "training_loss": 10.283971786499023
    },
    {
      "epoch": 0.00997289972899729,
      "step": 46,
      "training_loss": 11.653179168701172
    },
    {
      "epoch": 0.00997289972899729,
      "step": 46,
      "training_loss": 10.125810623168945
    },
    {
      "epoch": 0.01018970189701897,
      "step": 47,
      "training_loss": 11.46253776550293
    },
    {
      "epoch": 0.01018970189701897,
      "step": 47,
      "training_loss": 11.507570266723633
    },
    {
      "epoch": 0.01018970189701897,
      "step": 47,
      "training_loss": 11.993964195251465
    },
    {
      "epoch": 0.01018970189701897,
      "step": 47,
      "training_loss": 9.98692798614502
    },
    {
      "epoch": 0.01040650406504065,
      "grad_norm": 267.4221496582031,
      "learning_rate": 1e-05,
      "loss": 11.1154,
      "step": 48
    },
    {
      "epoch": 0.01040650406504065,
      "step": 48,
      "training_loss": 10.763617515563965
    },
    {
      "epoch": 0.01040650406504065,
      "step": 48,
      "training_loss": 9.30883502960205
    },
    {
      "epoch": 0.01040650406504065,
      "step": 48,
      "training_loss": 10.06734561920166
    },
    {
      "epoch": 0.01040650406504065,
      "step": 48,
      "training_loss": 11.876011848449707
    },
    {
      "epoch": 0.01062330623306233,
      "step": 49,
      "training_loss": 10.91184139251709
    },
    {
      "epoch": 0.01062330623306233,
      "step": 49,
      "training_loss": 12.1062593460083
    },
    {
      "epoch": 0.01062330623306233,
      "step": 49,
      "training_loss": 11.304647445678711
    },
    {
      "epoch": 0.01062330623306233,
      "step": 49,
      "training_loss": 11.14669418334961
    },
    {
      "epoch": 0.01084010840108401,
      "step": 50,
      "training_loss": 9.770421028137207
    },
    {
      "epoch": 0.01084010840108401,
      "step": 50,
      "training_loss": 11.837535858154297
    },
    {
      "epoch": 0.01084010840108401,
      "step": 50,
      "training_loss": 10.42093276977539
    },
    {
      "epoch": 0.01084010840108401,
      "step": 50,
      "training_loss": 11.764927864074707
    },
    {
      "epoch": 0.011056910569105691,
      "step": 51,
      "training_loss": 11.52365779876709
    },
    {
      "epoch": 0.011056910569105691,
      "step": 51,
      "training_loss": 11.139102935791016
    },
    {
      "epoch": 0.011056910569105691,
      "step": 51,
      "training_loss": 11.225062370300293
    },
    {
      "epoch": 0.011056910569105691,
      "step": 51,
      "training_loss": 11.246814727783203
    },
    {
      "epoch": 0.011273712737127371,
      "grad_norm": 81.33429718017578,
      "learning_rate": 1e-05,
      "loss": 11.0259,
      "step": 52
    },
    {
      "epoch": 0.011273712737127371,
      "step": 52,
      "training_loss": 11.23912525177002
    },
    {
      "epoch": 0.011273712737127371,
      "step": 52,
      "training_loss": 11.146463394165039
    },
    {
      "epoch": 0.011273712737127371,
      "step": 52,
      "training_loss": 12.071565628051758
    },
    {
      "epoch": 0.011273712737127371,
      "step": 52,
      "training_loss": 10.719017028808594
    },
    {
      "epoch": 0.011490514905149051,
      "step": 53,
      "training_loss": 11.790292739868164
    },
    {
      "epoch": 0.011490514905149051,
      "step": 53,
      "training_loss": 11.080885887145996
    },
    {
      "epoch": 0.011490514905149051,
      "step": 53,
      "training_loss": 10.251631736755371
    },
    {
      "epoch": 0.011490514905149051,
      "step": 53,
      "training_loss": 10.500884056091309
    },
    {
      "epoch": 0.011707317073170732,
      "step": 54,
      "training_loss": 11.465171813964844
    },
    {
      "epoch": 0.011707317073170732,
      "step": 54,
      "training_loss": 10.435234069824219
    },
    {
      "epoch": 0.011707317073170732,
      "step": 54,
      "training_loss": 10.8902587890625
    },
    {
      "epoch": 0.011707317073170732,
      "step": 54,
      "training_loss": 12.174555778503418
    },
    {
      "epoch": 0.011924119241192412,
      "step": 55,
      "training_loss": 11.59957504272461
    },
    {
      "epoch": 0.011924119241192412,
      "step": 55,
      "training_loss": 11.856620788574219
    },
    {
      "epoch": 0.011924119241192412,
      "step": 55,
      "training_loss": 12.579904556274414
    },
    {
      "epoch": 0.011924119241192412,
      "step": 55,
      "training_loss": 11.086438179016113
    },
    {
      "epoch": 0.012140921409214092,
      "grad_norm": 621.2954711914062,
      "learning_rate": 1e-05,
      "loss": 11.3055,
      "step": 56
    },
    {
      "epoch": 0.012140921409214092,
      "step": 56,
      "training_loss": 11.824420928955078
    },
    {
      "epoch": 0.012140921409214092,
      "step": 56,
      "training_loss": 10.228438377380371
    },
    {
      "epoch": 0.012140921409214092,
      "step": 56,
      "training_loss": 11.565823554992676
    },
    {
      "epoch": 0.012140921409214092,
      "step": 56,
      "training_loss": 11.046697616577148
    },
    {
      "epoch": 0.012357723577235772,
      "step": 57,
      "training_loss": 10.061279296875
    },
    {
      "epoch": 0.012357723577235772,
      "step": 57,
      "training_loss": 11.302141189575195
    },
    {
      "epoch": 0.012357723577235772,
      "step": 57,
      "training_loss": 11.934012413024902
    },
    {
      "epoch": 0.012357723577235772,
      "step": 57,
      "training_loss": 11.343156814575195
    },
    {
      "epoch": 0.012574525745257453,
      "step": 58,
      "training_loss": 11.168499946594238
    },
    {
      "epoch": 0.012574525745257453,
      "step": 58,
      "training_loss": 10.884211540222168
    },
    {
      "epoch": 0.012574525745257453,
      "step": 58,
      "training_loss": 11.311549186706543
    },
    {
      "epoch": 0.012574525745257453,
      "step": 58,
      "training_loss": 10.321839332580566
    },
    {
      "epoch": 0.012791327913279133,
      "step": 59,
      "training_loss": 10.512557029724121
    },
    {
      "epoch": 0.012791327913279133,
      "step": 59,
      "training_loss": 10.58855152130127
    },
    {
      "epoch": 0.012791327913279133,
      "step": 59,
      "training_loss": 10.821465492248535
    },
    {
      "epoch": 0.012791327913279133,
      "step": 59,
      "training_loss": 10.936254501342773
    },
    {
      "epoch": 0.013008130081300813,
      "grad_norm": 330.0079040527344,
      "learning_rate": 1e-05,
      "loss": 10.9907,
      "step": 60
    },
    {
      "epoch": 0.013008130081300813,
      "step": 60,
      "training_loss": 11.823135375976562
    },
    {
      "epoch": 0.013008130081300813,
      "step": 60,
      "training_loss": 10.342425346374512
    },
    {
      "epoch": 0.013008130081300813,
      "step": 60,
      "training_loss": 9.467570304870605
    },
    {
      "epoch": 0.013008130081300813,
      "step": 60,
      "training_loss": 12.161638259887695
    },
    {
      "epoch": 0.013224932249322493,
      "step": 61,
      "training_loss": 9.72585391998291
    },
    {
      "epoch": 0.013224932249322493,
      "step": 61,
      "training_loss": 11.907783508300781
    },
    {
      "epoch": 0.013224932249322493,
      "step": 61,
      "training_loss": 11.217903137207031
    },
    {
      "epoch": 0.013224932249322493,
      "step": 61,
      "training_loss": 20.978042602539062
    },
    {
      "epoch": 0.013441734417344173,
      "step": 62,
      "training_loss": 13.619941711425781
    },
    {
      "epoch": 0.013441734417344173,
      "step": 62,
      "training_loss": 9.62725830078125
    },
    {
      "epoch": 0.013441734417344173,
      "step": 62,
      "training_loss": 10.585098266601562
    },
    {
      "epoch": 0.013441734417344173,
      "step": 62,
      "training_loss": 11.026774406433105
    },
    {
      "epoch": 0.013658536585365854,
      "step": 63,
      "training_loss": 11.254293441772461
    },
    {
      "epoch": 0.013658536585365854,
      "step": 63,
      "training_loss": 10.396044731140137
    },
    {
      "epoch": 0.013658536585365854,
      "step": 63,
      "training_loss": 10.354084968566895
    },
    {
      "epoch": 0.013658536585365854,
      "step": 63,
      "training_loss": 10.629480361938477
    },
    {
      "epoch": 0.013875338753387534,
      "grad_norm": 88.22277069091797,
      "learning_rate": 1e-05,
      "loss": 11.5698,
      "step": 64
    },
    {
      "epoch": 0.013875338753387534,
      "step": 64,
      "training_loss": 10.171649932861328
    },
    {
      "epoch": 0.013875338753387534,
      "step": 64,
      "training_loss": 10.431427001953125
    },
    {
      "epoch": 0.013875338753387534,
      "step": 64,
      "training_loss": 11.576193809509277
    },
    {
      "epoch": 0.013875338753387534,
      "step": 64,
      "training_loss": 11.547978401184082
    },
    {
      "epoch": 0.014092140921409214,
      "step": 65,
      "training_loss": 11.583837509155273
    },
    {
      "epoch": 0.014092140921409214,
      "step": 65,
      "training_loss": 9.577176094055176
    },
    {
      "epoch": 0.014092140921409214,
      "step": 65,
      "training_loss": 11.011433601379395
    },
    {
      "epoch": 0.014092140921409214,
      "step": 65,
      "training_loss": 10.622544288635254
    },
    {
      "epoch": 0.014308943089430894,
      "step": 66,
      "training_loss": 10.439275741577148
    },
    {
      "epoch": 0.014308943089430894,
      "step": 66,
      "training_loss": 10.610671997070312
    },
    {
      "epoch": 0.014308943089430894,
      "step": 66,
      "training_loss": 11.316889762878418
    },
    {
      "epoch": 0.014308943089430894,
      "step": 66,
      "training_loss": 10.380965232849121
    },
    {
      "epoch": 0.014525745257452575,
      "step": 67,
      "training_loss": 11.336755752563477
    },
    {
      "epoch": 0.014525745257452575,
      "step": 67,
      "training_loss": 11.77556037902832
    },
    {
      "epoch": 0.014525745257452575,
      "step": 67,
      "training_loss": 9.664860725402832
    },
    {
      "epoch": 0.014525745257452575,
      "step": 67,
      "training_loss": 13.215471267700195
    },
    {
      "epoch": 0.014742547425474255,
      "grad_norm": 286.317626953125,
      "learning_rate": 1e-05,
      "loss": 10.9539,
      "step": 68
    },
    {
      "epoch": 0.014742547425474255,
      "step": 68,
      "training_loss": 9.921239852905273
    },
    {
      "epoch": 0.014742547425474255,
      "step": 68,
      "training_loss": 9.50959587097168
    },
    {
      "epoch": 0.014742547425474255,
      "step": 68,
      "training_loss": 10.835391998291016
    },
    {
      "epoch": 0.014742547425474255,
      "step": 68,
      "training_loss": 9.54834270477295
    },
    {
      "epoch": 0.014959349593495935,
      "step": 69,
      "training_loss": 10.128743171691895
    },
    {
      "epoch": 0.014959349593495935,
      "step": 69,
      "training_loss": 10.616089820861816
    },
    {
      "epoch": 0.014959349593495935,
      "step": 69,
      "training_loss": 11.064868927001953
    },
    {
      "epoch": 0.014959349593495935,
      "step": 69,
      "training_loss": 9.496380805969238
    },
    {
      "epoch": 0.015176151761517615,
      "step": 70,
      "training_loss": 9.156525611877441
    },
    {
      "epoch": 0.015176151761517615,
      "step": 70,
      "training_loss": 11.66843318939209
    },
    {
      "epoch": 0.015176151761517615,
      "step": 70,
      "training_loss": 9.826478004455566
    },
    {
      "epoch": 0.015176151761517615,
      "step": 70,
      "training_loss": 11.330495834350586
    },
    {
      "epoch": 0.015392953929539295,
      "step": 71,
      "training_loss": 9.503997802734375
    },
    {
      "epoch": 0.015392953929539295,
      "step": 71,
      "training_loss": 9.493706703186035
    },
    {
      "epoch": 0.015392953929539295,
      "step": 71,
      "training_loss": 11.369006156921387
    },
    {
      "epoch": 0.015392953929539295,
      "step": 71,
      "training_loss": 11.755043983459473
    },
    {
      "epoch": 0.015609756097560976,
      "grad_norm": 135.3744659423828,
      "learning_rate": 1e-05,
      "loss": 10.3265,
      "step": 72
    },
    {
      "epoch": 0.015609756097560976,
      "step": 72,
      "training_loss": 9.87833023071289
    },
    {
      "epoch": 0.015609756097560976,
      "step": 72,
      "training_loss": 10.049241065979004
    },
    {
      "epoch": 0.015609756097560976,
      "step": 72,
      "training_loss": 12.046591758728027
    },
    {
      "epoch": 0.015609756097560976,
      "step": 72,
      "training_loss": 11.270051002502441
    },
    {
      "epoch": 0.015826558265582658,
      "step": 73,
      "training_loss": 11.732701301574707
    },
    {
      "epoch": 0.015826558265582658,
      "step": 73,
      "training_loss": 10.45803451538086
    },
    {
      "epoch": 0.015826558265582658,
      "step": 73,
      "training_loss": 11.10019588470459
    },
    {
      "epoch": 0.015826558265582658,
      "step": 73,
      "training_loss": 10.703932762145996
    },
    {
      "epoch": 0.016043360433604336,
      "step": 74,
      "training_loss": 10.87665843963623
    },
    {
      "epoch": 0.016043360433604336,
      "step": 74,
      "training_loss": 12.043797492980957
    },
    {
      "epoch": 0.016043360433604336,
      "step": 74,
      "training_loss": 10.488093376159668
    },
    {
      "epoch": 0.016043360433604336,
      "step": 74,
      "training_loss": 11.387258529663086
    },
    {
      "epoch": 0.016260162601626018,
      "step": 75,
      "training_loss": 10.694602012634277
    },
    {
      "epoch": 0.016260162601626018,
      "step": 75,
      "training_loss": 8.98235034942627
    },
    {
      "epoch": 0.016260162601626018,
      "step": 75,
      "training_loss": 11.121962547302246
    },
    {
      "epoch": 0.016260162601626018,
      "step": 75,
      "training_loss": 9.940589904785156
    },
    {
      "epoch": 0.016476964769647696,
      "grad_norm": 132.09774780273438,
      "learning_rate": 1e-05,
      "loss": 10.7984,
      "step": 76
    },
    {
      "epoch": 0.016476964769647696,
      "step": 76,
      "training_loss": 10.336990356445312
    },
    {
      "epoch": 0.016476964769647696,
      "step": 76,
      "training_loss": 11.226700782775879
    },
    {
      "epoch": 0.016476964769647696,
      "step": 76,
      "training_loss": 9.787009239196777
    },
    {
      "epoch": 0.016476964769647696,
      "step": 76,
      "training_loss": 10.619097709655762
    },
    {
      "epoch": 0.01669376693766938,
      "step": 77,
      "training_loss": 10.575088500976562
    },
    {
      "epoch": 0.01669376693766938,
      "step": 77,
      "training_loss": 11.31038761138916
    },
    {
      "epoch": 0.01669376693766938,
      "step": 77,
      "training_loss": 9.97350788116455
    },
    {
      "epoch": 0.01669376693766938,
      "step": 77,
      "training_loss": 10.154258728027344
    },
    {
      "epoch": 0.016910569105691057,
      "step": 78,
      "training_loss": 11.030391693115234
    },
    {
      "epoch": 0.016910569105691057,
      "step": 78,
      "training_loss": 11.308675765991211
    },
    {
      "epoch": 0.016910569105691057,
      "step": 78,
      "training_loss": 11.482189178466797
    },
    {
      "epoch": 0.016910569105691057,
      "step": 78,
      "training_loss": 9.84171199798584
    },
    {
      "epoch": 0.01712737127371274,
      "step": 79,
      "training_loss": 10.810867309570312
    },
    {
      "epoch": 0.01712737127371274,
      "step": 79,
      "training_loss": 12.6842041015625
    },
    {
      "epoch": 0.01712737127371274,
      "step": 79,
      "training_loss": 9.595470428466797
    },
    {
      "epoch": 0.01712737127371274,
      "step": 79,
      "training_loss": 11.732863426208496
    },
    {
      "epoch": 0.017344173441734417,
      "grad_norm": 96.53912353515625,
      "learning_rate": 1e-05,
      "loss": 10.7793,
      "step": 80
    },
    {
      "epoch": 0.017344173441734417,
      "step": 80,
      "training_loss": 9.808753967285156
    },
    {
      "epoch": 0.017344173441734417,
      "step": 80,
      "training_loss": 11.16174602508545
    },
    {
      "epoch": 0.017344173441734417,
      "step": 80,
      "training_loss": 9.847762107849121
    },
    {
      "epoch": 0.017344173441734417,
      "step": 80,
      "training_loss": 10.527948379516602
    },
    {
      "epoch": 0.0175609756097561,
      "step": 81,
      "training_loss": 10.009452819824219
    },
    {
      "epoch": 0.0175609756097561,
      "step": 81,
      "training_loss": 10.475266456604004
    },
    {
      "epoch": 0.0175609756097561,
      "step": 81,
      "training_loss": 10.340861320495605
    },
    {
      "epoch": 0.0175609756097561,
      "step": 81,
      "training_loss": 8.278257369995117
    },
    {
      "epoch": 0.017777777777777778,
      "step": 82,
      "training_loss": 11.225159645080566
    },
    {
      "epoch": 0.017777777777777778,
      "step": 82,
      "training_loss": 9.806085586547852
    },
    {
      "epoch": 0.017777777777777778,
      "step": 82,
      "training_loss": 10.034290313720703
    },
    {
      "epoch": 0.017777777777777778,
      "step": 82,
      "training_loss": 10.642143249511719
    },
    {
      "epoch": 0.01799457994579946,
      "step": 83,
      "training_loss": 9.514667510986328
    },
    {
      "epoch": 0.01799457994579946,
      "step": 83,
      "training_loss": 9.843507766723633
    },
    {
      "epoch": 0.01799457994579946,
      "step": 83,
      "training_loss": 9.639962196350098
    },
    {
      "epoch": 0.01799457994579946,
      "step": 83,
      "training_loss": 10.463105201721191
    },
    {
      "epoch": 0.018211382113821138,
      "grad_norm": 138.573486328125,
      "learning_rate": 1e-05,
      "loss": 10.1012,
      "step": 84
    },
    {
      "epoch": 0.018211382113821138,
      "step": 84,
      "training_loss": 10.120891571044922
    },
    {
      "epoch": 0.018211382113821138,
      "step": 84,
      "training_loss": 10.02445125579834
    },
    {
      "epoch": 0.018211382113821138,
      "step": 84,
      "training_loss": 9.85688304901123
    },
    {
      "epoch": 0.018211382113821138,
      "step": 84,
      "training_loss": 11.204394340515137
    },
    {
      "epoch": 0.01842818428184282,
      "step": 85,
      "training_loss": 10.262032508850098
    },
    {
      "epoch": 0.01842818428184282,
      "step": 85,
      "training_loss": 10.497461318969727
    },
    {
      "epoch": 0.01842818428184282,
      "step": 85,
      "training_loss": 9.956238746643066
    },
    {
      "epoch": 0.01842818428184282,
      "step": 85,
      "training_loss": 9.379246711730957
    },
    {
      "epoch": 0.0186449864498645,
      "step": 86,
      "training_loss": 9.875662803649902
    },
    {
      "epoch": 0.0186449864498645,
      "step": 86,
      "training_loss": 10.488238334655762
    },
    {
      "epoch": 0.0186449864498645,
      "step": 86,
      "training_loss": 9.284514427185059
    },
    {
      "epoch": 0.0186449864498645,
      "step": 86,
      "training_loss": 9.612297058105469
    },
    {
      "epoch": 0.01886178861788618,
      "step": 87,
      "training_loss": 11.152883529663086
    },
    {
      "epoch": 0.01886178861788618,
      "step": 87,
      "training_loss": 10.54931926727295
    },
    {
      "epoch": 0.01886178861788618,
      "step": 87,
      "training_loss": 11.574110984802246
    },
    {
      "epoch": 0.01886178861788618,
      "step": 87,
      "training_loss": 11.979876518249512
    },
    {
      "epoch": 0.01907859078590786,
      "grad_norm": 1241.30517578125,
      "learning_rate": 1e-05,
      "loss": 10.3637,
      "step": 88
    },
    {
      "epoch": 0.01907859078590786,
      "step": 88,
      "training_loss": 11.338078498840332
    },
    {
      "epoch": 0.01907859078590786,
      "step": 88,
      "training_loss": 9.853370666503906
    },
    {
      "epoch": 0.01907859078590786,
      "step": 88,
      "training_loss": 9.269926071166992
    },
    {
      "epoch": 0.01907859078590786,
      "step": 88,
      "training_loss": 10.583924293518066
    },
    {
      "epoch": 0.01929539295392954,
      "step": 89,
      "training_loss": 10.321105003356934
    },
    {
      "epoch": 0.01929539295392954,
      "step": 89,
      "training_loss": 10.537999153137207
    },
    {
      "epoch": 0.01929539295392954,
      "step": 89,
      "training_loss": 8.79469108581543
    },
    {
      "epoch": 0.01929539295392954,
      "step": 89,
      "training_loss": 9.390617370605469
    },
    {
      "epoch": 0.01951219512195122,
      "step": 90,
      "training_loss": 9.711448669433594
    },
    {
      "epoch": 0.01951219512195122,
      "step": 90,
      "training_loss": 10.660801887512207
    },
    {
      "epoch": 0.01951219512195122,
      "step": 90,
      "training_loss": 12.22561264038086
    },
    {
      "epoch": 0.01951219512195122,
      "step": 90,
      "training_loss": 10.619099617004395
    },
    {
      "epoch": 0.0197289972899729,
      "step": 91,
      "training_loss": 10.029189109802246
    },
    {
      "epoch": 0.0197289972899729,
      "step": 91,
      "training_loss": 9.857510566711426
    },
    {
      "epoch": 0.0197289972899729,
      "step": 91,
      "training_loss": 10.798392295837402
    },
    {
      "epoch": 0.0197289972899729,
      "step": 91,
      "training_loss": 10.454123497009277
    },
    {
      "epoch": 0.01994579945799458,
      "grad_norm": 778.8587646484375,
      "learning_rate": 1e-05,
      "loss": 10.2779,
      "step": 92
    },
    {
      "epoch": 0.01994579945799458,
      "step": 92,
      "training_loss": 10.13498592376709
    },
    {
      "epoch": 0.01994579945799458,
      "step": 92,
      "training_loss": 9.57377815246582
    },
    {
      "epoch": 0.01994579945799458,
      "step": 92,
      "training_loss": 10.984047889709473
    },
    {
      "epoch": 0.01994579945799458,
      "step": 92,
      "training_loss": 9.518506050109863
    },
    {
      "epoch": 0.020162601626016262,
      "step": 93,
      "training_loss": 10.877557754516602
    },
    {
      "epoch": 0.020162601626016262,
      "step": 93,
      "training_loss": 10.449996948242188
    },
    {
      "epoch": 0.020162601626016262,
      "step": 93,
      "training_loss": 10.526564598083496
    },
    {
      "epoch": 0.020162601626016262,
      "step": 93,
      "training_loss": 10.152436256408691
    },
    {
      "epoch": 0.02037940379403794,
      "step": 94,
      "training_loss": 9.912399291992188
    },
    {
      "epoch": 0.02037940379403794,
      "step": 94,
      "training_loss": 11.51640510559082
    },
    {
      "epoch": 0.02037940379403794,
      "step": 94,
      "training_loss": 11.036502838134766
    },
    {
      "epoch": 0.02037940379403794,
      "step": 94,
      "training_loss": 12.359025001525879
    },
    {
      "epoch": 0.020596205962059622,
      "step": 95,
      "training_loss": 9.667274475097656
    },
    {
      "epoch": 0.020596205962059622,
      "step": 95,
      "training_loss": 9.078266143798828
    },
    {
      "epoch": 0.020596205962059622,
      "step": 95,
      "training_loss": 9.388545989990234
    },
    {
      "epoch": 0.020596205962059622,
      "step": 95,
      "training_loss": 10.893657684326172
    },
    {
      "epoch": 0.0208130081300813,
      "grad_norm": 51.14881134033203,
      "learning_rate": 1e-05,
      "loss": 10.3794,
      "step": 96
    },
    {
      "epoch": 0.0208130081300813,
      "step": 96,
      "training_loss": 8.308914184570312
    },
    {
      "epoch": 0.0208130081300813,
      "step": 96,
      "training_loss": 10.232254028320312
    },
    {
      "epoch": 0.0208130081300813,
      "step": 96,
      "training_loss": 10.471602439880371
    },
    {
      "epoch": 0.0208130081300813,
      "step": 96,
      "training_loss": 10.28966999053955
    },
    {
      "epoch": 0.021029810298102983,
      "step": 97,
      "training_loss": 10.467583656311035
    },
    {
      "epoch": 0.021029810298102983,
      "step": 97,
      "training_loss": 10.242535591125488
    },
    {
      "epoch": 0.021029810298102983,
      "step": 97,
      "training_loss": 8.617980003356934
    },
    {
      "epoch": 0.021029810298102983,
      "step": 97,
      "training_loss": 10.285823822021484
    },
    {
      "epoch": 0.02124661246612466,
      "step": 98,
      "training_loss": 9.881416320800781
    },
    {
      "epoch": 0.02124661246612466,
      "step": 98,
      "training_loss": 9.68205451965332
    },
    {
      "epoch": 0.02124661246612466,
      "step": 98,
      "training_loss": 10.18307876586914
    },
    {
      "epoch": 0.02124661246612466,
      "step": 98,
      "training_loss": 9.690686225891113
    },
    {
      "epoch": 0.021463414634146343,
      "step": 99,
      "training_loss": 8.017963409423828
    },
    {
      "epoch": 0.021463414634146343,
      "step": 99,
      "training_loss": 10.45832347869873
    },
    {
      "epoch": 0.021463414634146343,
      "step": 99,
      "training_loss": 10.786794662475586
    },
    {
      "epoch": 0.021463414634146343,
      "step": 99,
      "training_loss": 10.24976921081543
    },
    {
      "epoch": 0.02168021680216802,
      "grad_norm": 73.55876922607422,
      "learning_rate": 1e-05,
      "loss": 9.8667,
      "step": 100
    },
    {
      "epoch": 0.02168021680216802,
      "step": 100,
      "training_loss": 10.799559593200684
    },
    {
      "epoch": 0.02168021680216802,
      "step": 100,
      "training_loss": 10.926652908325195
    },
    {
      "epoch": 0.02168021680216802,
      "step": 100,
      "training_loss": 12.198471069335938
    },
    {
      "epoch": 0.02168021680216802,
      "step": 100,
      "training_loss": 10.003201484680176
    },
    {
      "epoch": 0.021897018970189704,
      "step": 101,
      "training_loss": 8.612322807312012
    },
    {
      "epoch": 0.021897018970189704,
      "step": 101,
      "training_loss": 9.903634071350098
    },
    {
      "epoch": 0.021897018970189704,
      "step": 101,
      "training_loss": 10.099663734436035
    },
    {
      "epoch": 0.021897018970189704,
      "step": 101,
      "training_loss": 10.423314094543457
    },
    {
      "epoch": 0.022113821138211382,
      "step": 102,
      "training_loss": 9.66181468963623
    },
    {
      "epoch": 0.022113821138211382,
      "step": 102,
      "training_loss": 9.920584678649902
    },
    {
      "epoch": 0.022113821138211382,
      "step": 102,
      "training_loss": 10.334123611450195
    },
    {
      "epoch": 0.022113821138211382,
      "step": 102,
      "training_loss": 11.147148132324219
    },
    {
      "epoch": 0.022330623306233064,
      "step": 103,
      "training_loss": 9.658746719360352
    },
    {
      "epoch": 0.022330623306233064,
      "step": 103,
      "training_loss": 10.008766174316406
    },
    {
      "epoch": 0.022330623306233064,
      "step": 103,
      "training_loss": 10.047891616821289
    },
    {
      "epoch": 0.022330623306233064,
      "step": 103,
      "training_loss": 10.006131172180176
    },
    {
      "epoch": 0.022547425474254743,
      "grad_norm": 143.89724731445312,
      "learning_rate": 1e-05,
      "loss": 10.2345,
      "step": 104
    },
    {
      "epoch": 0.022547425474254743,
      "step": 104,
      "training_loss": 9.484115600585938
    },
    {
      "epoch": 0.022547425474254743,
      "step": 104,
      "training_loss": 8.987526893615723
    },
    {
      "epoch": 0.022547425474254743,
      "step": 104,
      "training_loss": 8.817723274230957
    },
    {
      "epoch": 0.022547425474254743,
      "step": 104,
      "training_loss": 10.957877159118652
    },
    {
      "epoch": 0.022764227642276424,
      "step": 105,
      "training_loss": 9.51769733428955
    },
    {
      "epoch": 0.022764227642276424,
      "step": 105,
      "training_loss": 9.533219337463379
    },
    {
      "epoch": 0.022764227642276424,
      "step": 105,
      "training_loss": 9.770508766174316
    },
    {
      "epoch": 0.022764227642276424,
      "step": 105,
      "training_loss": 9.396286964416504
    },
    {
      "epoch": 0.022981029810298103,
      "step": 106,
      "training_loss": 10.985240936279297
    },
    {
      "epoch": 0.022981029810298103,
      "step": 106,
      "training_loss": 9.15552806854248
    },
    {
      "epoch": 0.022981029810298103,
      "step": 106,
      "training_loss": 10.988699913024902
    },
    {
      "epoch": 0.022981029810298103,
      "step": 106,
      "training_loss": 11.730093002319336
    },
    {
      "epoch": 0.023197831978319785,
      "step": 107,
      "training_loss": 10.841992378234863
    },
    {
      "epoch": 0.023197831978319785,
      "step": 107,
      "training_loss": 9.02997875213623
    },
    {
      "epoch": 0.023197831978319785,
      "step": 107,
      "training_loss": 9.853361129760742
    },
    {
      "epoch": 0.023197831978319785,
      "step": 107,
      "training_loss": 10.116843223571777
    },
    {
      "epoch": 0.023414634146341463,
      "grad_norm": 79.63467407226562,
      "learning_rate": 1e-05,
      "loss": 9.9479,
      "step": 108
    },
    {
      "epoch": 0.023414634146341463,
      "step": 108,
      "training_loss": 10.854742050170898
    },
    {
      "epoch": 0.023414634146341463,
      "step": 108,
      "training_loss": 10.825624465942383
    },
    {
      "epoch": 0.023414634146341463,
      "step": 108,
      "training_loss": 9.662630081176758
    },
    {
      "epoch": 0.023414634146341463,
      "step": 108,
      "training_loss": 10.88685417175293
    },
    {
      "epoch": 0.023631436314363145,
      "step": 109,
      "training_loss": 8.459156036376953
    },
    {
      "epoch": 0.023631436314363145,
      "step": 109,
      "training_loss": 8.810479164123535
    },
    {
      "epoch": 0.023631436314363145,
      "step": 109,
      "training_loss": 9.776252746582031
    },
    {
      "epoch": 0.023631436314363145,
      "step": 109,
      "training_loss": 10.202486038208008
    },
    {
      "epoch": 0.023848238482384824,
      "step": 110,
      "training_loss": 8.979019165039062
    },
    {
      "epoch": 0.023848238482384824,
      "step": 110,
      "training_loss": 10.179869651794434
    },
    {
      "epoch": 0.023848238482384824,
      "step": 110,
      "training_loss": 9.834428787231445
    },
    {
      "epoch": 0.023848238482384824,
      "step": 110,
      "training_loss": 10.86985969543457
    },
    {
      "epoch": 0.024065040650406506,
      "step": 111,
      "training_loss": 9.44255542755127
    },
    {
      "epoch": 0.024065040650406506,
      "step": 111,
      "training_loss": 9.180475234985352
    },
    {
      "epoch": 0.024065040650406506,
      "step": 111,
      "training_loss": 10.458887100219727
    },
    {
      "epoch": 0.024065040650406506,
      "step": 111,
      "training_loss": 9.127165794372559
    },
    {
      "epoch": 0.024281842818428184,
      "grad_norm": 64.77804565429688,
      "learning_rate": 1e-05,
      "loss": 9.8469,
      "step": 112
    },
    {
      "epoch": 0.024281842818428184,
      "step": 112,
      "training_loss": 9.855704307556152
    },
    {
      "epoch": 0.024281842818428184,
      "step": 112,
      "training_loss": 10.468626976013184
    },
    {
      "epoch": 0.024281842818428184,
      "step": 112,
      "training_loss": 7.941848278045654
    },
    {
      "epoch": 0.024281842818428184,
      "step": 112,
      "training_loss": 8.921780586242676
    },
    {
      "epoch": 0.024498644986449866,
      "step": 113,
      "training_loss": 10.39553451538086
    },
    {
      "epoch": 0.024498644986449866,
      "step": 113,
      "training_loss": 10.175721168518066
    },
    {
      "epoch": 0.024498644986449866,
      "step": 113,
      "training_loss": 11.009395599365234
    },
    {
      "epoch": 0.024498644986449866,
      "step": 113,
      "training_loss": 10.037847518920898
    },
    {
      "epoch": 0.024715447154471545,
      "step": 114,
      "training_loss": 10.640485763549805
    },
    {
      "epoch": 0.024715447154471545,
      "step": 114,
      "training_loss": 9.058655738830566
    },
    {
      "epoch": 0.024715447154471545,
      "step": 114,
      "training_loss": 9.739526748657227
    },
    {
      "epoch": 0.024715447154471545,
      "step": 114,
      "training_loss": 10.7631196975708
    },
    {
      "epoch": 0.024932249322493227,
      "step": 115,
      "training_loss": 9.016117095947266
    },
    {
      "epoch": 0.024932249322493227,
      "step": 115,
      "training_loss": 9.184043884277344
    },
    {
      "epoch": 0.024932249322493227,
      "step": 115,
      "training_loss": 9.059178352355957
    },
    {
      "epoch": 0.024932249322493227,
      "step": 115,
      "training_loss": 9.909873962402344
    },
    {
      "epoch": 0.025149051490514905,
      "grad_norm": 101.30384063720703,
      "learning_rate": 1e-05,
      "loss": 9.7611,
      "step": 116
    },
    {
      "epoch": 0.025149051490514905,
      "step": 116,
      "training_loss": 10.498079299926758
    },
    {
      "epoch": 0.025149051490514905,
      "step": 116,
      "training_loss": 11.358905792236328
    },
    {
      "epoch": 0.025149051490514905,
      "step": 116,
      "training_loss": 9.333642959594727
    },
    {
      "epoch": 0.025149051490514905,
      "step": 116,
      "training_loss": 10.10902214050293
    },
    {
      "epoch": 0.025365853658536587,
      "step": 117,
      "training_loss": 9.510281562805176
    },
    {
      "epoch": 0.025365853658536587,
      "step": 117,
      "training_loss": 9.541401863098145
    },
    {
      "epoch": 0.025365853658536587,
      "step": 117,
      "training_loss": 10.544628143310547
    },
    {
      "epoch": 0.025365853658536587,
      "step": 117,
      "training_loss": 9.496354103088379
    },
    {
      "epoch": 0.025582655826558266,
      "step": 118,
      "training_loss": 11.515974044799805
    },
    {
      "epoch": 0.025582655826558266,
      "step": 118,
      "training_loss": 9.587512016296387
    },
    {
      "epoch": 0.025582655826558266,
      "step": 118,
      "training_loss": 12.04337215423584
    },
    {
      "epoch": 0.025582655826558266,
      "step": 118,
      "training_loss": 9.909585952758789
    },
    {
      "epoch": 0.025799457994579948,
      "step": 119,
      "training_loss": 10.505685806274414
    },
    {
      "epoch": 0.025799457994579948,
      "step": 119,
      "training_loss": 8.377866744995117
    },
    {
      "epoch": 0.025799457994579948,
      "step": 119,
      "training_loss": 9.58510684967041
    },
    {
      "epoch": 0.025799457994579948,
      "step": 119,
      "training_loss": 8.914850234985352
    },
    {
      "epoch": 0.026016260162601626,
      "grad_norm": 43.6111946105957,
      "learning_rate": 1e-05,
      "loss": 10.052,
      "step": 120
    },
    {
      "epoch": 0.026016260162601626,
      "step": 120,
      "training_loss": 8.352078437805176
    },
    {
      "epoch": 0.026016260162601626,
      "step": 120,
      "training_loss": 9.556110382080078
    },
    {
      "epoch": 0.026016260162601626,
      "step": 120,
      "training_loss": 8.872753143310547
    },
    {
      "epoch": 0.026016260162601626,
      "step": 120,
      "training_loss": 10.47005558013916
    },
    {
      "epoch": 0.026233062330623308,
      "step": 121,
      "training_loss": 8.481276512145996
    },
    {
      "epoch": 0.026233062330623308,
      "step": 121,
      "training_loss": 10.006538391113281
    },
    {
      "epoch": 0.026233062330623308,
      "step": 121,
      "training_loss": 9.140453338623047
    },
    {
      "epoch": 0.026233062330623308,
      "step": 121,
      "training_loss": 9.6499662399292
    },
    {
      "epoch": 0.026449864498644986,
      "step": 122,
      "training_loss": 11.106840133666992
    },
    {
      "epoch": 0.026449864498644986,
      "step": 122,
      "training_loss": 10.878007888793945
    },
    {
      "epoch": 0.026449864498644986,
      "step": 122,
      "training_loss": 9.846151351928711
    },
    {
      "epoch": 0.026449864498644986,
      "step": 122,
      "training_loss": 8.273571014404297
    },
    {
      "epoch": 0.02666666666666667,
      "step": 123,
      "training_loss": 11.379265785217285
    },
    {
      "epoch": 0.02666666666666667,
      "step": 123,
      "training_loss": 8.327776908874512
    },
    {
      "epoch": 0.02666666666666667,
      "step": 123,
      "training_loss": 9.679412841796875
    },
    {
      "epoch": 0.02666666666666667,
      "step": 123,
      "training_loss": 9.317057609558105
    },
    {
      "epoch": 0.026883468834688347,
      "grad_norm": 52.33649826049805,
      "learning_rate": 1e-05,
      "loss": 9.5836,
      "step": 124
    },
    {
      "epoch": 0.026883468834688347,
      "step": 124,
      "training_loss": 8.477231979370117
    },
    {
      "epoch": 0.026883468834688347,
      "step": 124,
      "training_loss": 9.79317855834961
    },
    {
      "epoch": 0.026883468834688347,
      "step": 124,
      "training_loss": 9.28866195678711
    },
    {
      "epoch": 0.026883468834688347,
      "step": 124,
      "training_loss": 10.491571426391602
    },
    {
      "epoch": 0.02710027100271003,
      "step": 125,
      "training_loss": 10.690743446350098
    },
    {
      "epoch": 0.02710027100271003,
      "step": 125,
      "training_loss": 11.925535202026367
    },
    {
      "epoch": 0.02710027100271003,
      "step": 125,
      "training_loss": 8.641976356506348
    },
    {
      "epoch": 0.02710027100271003,
      "step": 125,
      "training_loss": 8.78683090209961
    },
    {
      "epoch": 0.027317073170731707,
      "step": 126,
      "training_loss": 10.224790573120117
    },
    {
      "epoch": 0.027317073170731707,
      "step": 126,
      "training_loss": 9.317902565002441
    },
    {
      "epoch": 0.027317073170731707,
      "step": 126,
      "training_loss": 12.265427589416504
    },
    {
      "epoch": 0.027317073170731707,
      "step": 126,
      "training_loss": 9.460977554321289
    },
    {
      "epoch": 0.02753387533875339,
      "step": 127,
      "training_loss": 10.495759010314941
    },
    {
      "epoch": 0.02753387533875339,
      "step": 127,
      "training_loss": 9.017297744750977
    },
    {
      "epoch": 0.02753387533875339,
      "step": 127,
      "training_loss": 9.605910301208496
    },
    {
      "epoch": 0.02753387533875339,
      "step": 127,
      "training_loss": 8.680270195007324
    },
    {
      "epoch": 0.027750677506775068,
      "grad_norm": 79.31343078613281,
      "learning_rate": 1e-05,
      "loss": 9.8228,
      "step": 128
    },
    {
      "epoch": 0.027750677506775068,
      "step": 128,
      "training_loss": 10.55966854095459
    },
    {
      "epoch": 0.027750677506775068,
      "step": 128,
      "training_loss": 8.899353981018066
    },
    {
      "epoch": 0.027750677506775068,
      "step": 128,
      "training_loss": 10.649454116821289
    },
    {
      "epoch": 0.027750677506775068,
      "step": 128,
      "training_loss": 8.6652250289917
    },
    {
      "epoch": 0.02796747967479675,
      "step": 129,
      "training_loss": 9.365446090698242
    },
    {
      "epoch": 0.02796747967479675,
      "step": 129,
      "training_loss": 11.216536521911621
    },
    {
      "epoch": 0.02796747967479675,
      "step": 129,
      "training_loss": 9.470932006835938
    },
    {
      "epoch": 0.02796747967479675,
      "step": 129,
      "training_loss": 8.338984489440918
    },
    {
      "epoch": 0.028184281842818428,
      "step": 130,
      "training_loss": 8.567583084106445
    },
    {
      "epoch": 0.028184281842818428,
      "step": 130,
      "training_loss": 8.68410873413086
    },
    {
      "epoch": 0.028184281842818428,
      "step": 130,
      "training_loss": 9.917405128479004
    },
    {
      "epoch": 0.028184281842818428,
      "step": 130,
      "training_loss": 10.18718433380127
    },
    {
      "epoch": 0.02840108401084011,
      "step": 131,
      "training_loss": 9.696126937866211
    },
    {
      "epoch": 0.02840108401084011,
      "step": 131,
      "training_loss": 9.566000938415527
    },
    {
      "epoch": 0.02840108401084011,
      "step": 131,
      "training_loss": 9.44117259979248
    },
    {
      "epoch": 0.02840108401084011,
      "step": 131,
      "training_loss": 8.731123924255371
    },
    {
      "epoch": 0.02861788617886179,
      "grad_norm": 49.079872131347656,
      "learning_rate": 1e-05,
      "loss": 9.4973,
      "step": 132
    },
    {
      "epoch": 0.02861788617886179,
      "step": 132,
      "training_loss": 9.200819969177246
    },
    {
      "epoch": 0.02861788617886179,
      "step": 132,
      "training_loss": 10.775903701782227
    },
    {
      "epoch": 0.02861788617886179,
      "step": 132,
      "training_loss": 10.023784637451172
    },
    {
      "epoch": 0.02861788617886179,
      "step": 132,
      "training_loss": 8.778909683227539
    },
    {
      "epoch": 0.02883468834688347,
      "step": 133,
      "training_loss": 10.857417106628418
    },
    {
      "epoch": 0.02883468834688347,
      "step": 133,
      "training_loss": 8.813789367675781
    },
    {
      "epoch": 0.02883468834688347,
      "step": 133,
      "training_loss": 10.767064094543457
    },
    {
      "epoch": 0.02883468834688347,
      "step": 133,
      "training_loss": 9.028725624084473
    },
    {
      "epoch": 0.02905149051490515,
      "step": 134,
      "training_loss": 8.922635078430176
    },
    {
      "epoch": 0.02905149051490515,
      "step": 134,
      "training_loss": 8.995953559875488
    },
    {
      "epoch": 0.02905149051490515,
      "step": 134,
      "training_loss": 9.742178916931152
    },
    {
      "epoch": 0.02905149051490515,
      "step": 134,
      "training_loss": 8.33629322052002
    },
    {
      "epoch": 0.02926829268292683,
      "step": 135,
      "training_loss": 8.576773643493652
    },
    {
      "epoch": 0.02926829268292683,
      "step": 135,
      "training_loss": 10.740087509155273
    },
    {
      "epoch": 0.02926829268292683,
      "step": 135,
      "training_loss": 8.001723289489746
    },
    {
      "epoch": 0.02926829268292683,
      "step": 135,
      "training_loss": 9.308122634887695
    },
    {
      "epoch": 0.02948509485094851,
      "grad_norm": 31.64908790588379,
      "learning_rate": 1e-05,
      "loss": 9.4294,
      "step": 136
    },
    {
      "epoch": 0.02948509485094851,
      "step": 136,
      "training_loss": 8.0272798538208
    },
    {
      "epoch": 0.02948509485094851,
      "step": 136,
      "training_loss": 9.802240371704102
    },
    {
      "epoch": 0.02948509485094851,
      "step": 136,
      "training_loss": 9.12115478515625
    },
    {
      "epoch": 0.02948509485094851,
      "step": 136,
      "training_loss": 9.057506561279297
    },
    {
      "epoch": 0.02970189701897019,
      "step": 137,
      "training_loss": 9.011663436889648
    },
    {
      "epoch": 0.02970189701897019,
      "step": 137,
      "training_loss": 9.171136856079102
    },
    {
      "epoch": 0.02970189701897019,
      "step": 137,
      "training_loss": 9.635442733764648
    },
    {
      "epoch": 0.02970189701897019,
      "step": 137,
      "training_loss": 10.168187141418457
    },
    {
      "epoch": 0.02991869918699187,
      "step": 138,
      "training_loss": 8.368490219116211
    },
    {
      "epoch": 0.02991869918699187,
      "step": 138,
      "training_loss": 8.767744064331055
    },
    {
      "epoch": 0.02991869918699187,
      "step": 138,
      "training_loss": 9.689095497131348
    },
    {
      "epoch": 0.02991869918699187,
      "step": 138,
      "training_loss": 7.829265594482422
    },
    {
      "epoch": 0.030135501355013552,
      "step": 139,
      "training_loss": 9.194008827209473
    },
    {
      "epoch": 0.030135501355013552,
      "step": 139,
      "training_loss": 9.334831237792969
    },
    {
      "epoch": 0.030135501355013552,
      "step": 139,
      "training_loss": 8.966822624206543
    },
    {
      "epoch": 0.030135501355013552,
      "step": 139,
      "training_loss": 8.230215072631836
    },
    {
      "epoch": 0.03035230352303523,
      "grad_norm": 50.46195602416992,
      "learning_rate": 1e-05,
      "loss": 9.0234,
      "step": 140
    },
    {
      "epoch": 0.03035230352303523,
      "step": 140,
      "training_loss": 8.742035865783691
    },
    {
      "epoch": 0.03035230352303523,
      "step": 140,
      "training_loss": 10.841475486755371
    },
    {
      "epoch": 0.03035230352303523,
      "step": 140,
      "training_loss": 10.213281631469727
    },
    {
      "epoch": 0.03035230352303523,
      "step": 140,
      "training_loss": 9.184085845947266
    },
    {
      "epoch": 0.030569105691056912,
      "step": 141,
      "training_loss": 7.570423603057861
    },
    {
      "epoch": 0.030569105691056912,
      "step": 141,
      "training_loss": 8.893404960632324
    },
    {
      "epoch": 0.030569105691056912,
      "step": 141,
      "training_loss": 8.932044982910156
    },
    {
      "epoch": 0.030569105691056912,
      "step": 141,
      "training_loss": 10.285871505737305
    },
    {
      "epoch": 0.03078590785907859,
      "step": 142,
      "training_loss": 8.2794189453125
    },
    {
      "epoch": 0.03078590785907859,
      "step": 142,
      "training_loss": 8.788124084472656
    },
    {
      "epoch": 0.03078590785907859,
      "step": 142,
      "training_loss": 9.482942581176758
    },
    {
      "epoch": 0.03078590785907859,
      "step": 142,
      "training_loss": 6.887938022613525
    },
    {
      "epoch": 0.031002710027100273,
      "step": 143,
      "training_loss": 8.308817863464355
    },
    {
      "epoch": 0.031002710027100273,
      "step": 143,
      "training_loss": 8.871071815490723
    },
    {
      "epoch": 0.031002710027100273,
      "step": 143,
      "training_loss": 9.6576509475708
    },
    {
      "epoch": 0.031002710027100273,
      "step": 143,
      "training_loss": 9.599544525146484
    },
    {
      "epoch": 0.03121951219512195,
      "grad_norm": 35.07929229736328,
      "learning_rate": 1e-05,
      "loss": 9.0336,
      "step": 144
    },
    {
      "epoch": 0.03121951219512195,
      "step": 144,
      "training_loss": 8.810026168823242
    },
    {
      "epoch": 0.03121951219512195,
      "step": 144,
      "training_loss": 8.785268783569336
    },
    {
      "epoch": 0.03121951219512195,
      "step": 144,
      "training_loss": 9.08885383605957
    },
    {
      "epoch": 0.03121951219512195,
      "step": 144,
      "training_loss": 9.377460479736328
    },
    {
      "epoch": 0.03143631436314363,
      "step": 145,
      "training_loss": 11.996742248535156
    },
    {
      "epoch": 0.03143631436314363,
      "step": 145,
      "training_loss": 10.810502052307129
    },
    {
      "epoch": 0.03143631436314363,
      "step": 145,
      "training_loss": 8.863670349121094
    },
    {
      "epoch": 0.03143631436314363,
      "step": 145,
      "training_loss": 7.4691009521484375
    },
    {
      "epoch": 0.031653116531165315,
      "step": 146,
      "training_loss": 9.552370071411133
    },
    {
      "epoch": 0.031653116531165315,
      "step": 146,
      "training_loss": 7.274857997894287
    },
    {
      "epoch": 0.031653116531165315,
      "step": 146,
      "training_loss": 9.805930137634277
    },
    {
      "epoch": 0.031653116531165315,
      "step": 146,
      "training_loss": 8.199610710144043
    },
    {
      "epoch": 0.03186991869918699,
      "step": 147,
      "training_loss": 8.722465515136719
    },
    {
      "epoch": 0.03186991869918699,
      "step": 147,
      "training_loss": 9.119549751281738
    },
    {
      "epoch": 0.03186991869918699,
      "step": 147,
      "training_loss": 9.304970741271973
    },
    {
      "epoch": 0.03186991869918699,
      "step": 147,
      "training_loss": 9.341635704040527
    },
    {
      "epoch": 0.03208672086720867,
      "grad_norm": 83.25457763671875,
      "learning_rate": 1e-05,
      "loss": 9.1577,
      "step": 148
    },
    {
      "epoch": 0.03208672086720867,
      "step": 148,
      "training_loss": 10.311639785766602
    },
    {
      "epoch": 0.03208672086720867,
      "step": 148,
      "training_loss": 9.36246109008789
    },
    {
      "epoch": 0.03208672086720867,
      "step": 148,
      "training_loss": 10.107453346252441
    },
    {
      "epoch": 0.03208672086720867,
      "step": 148,
      "training_loss": 9.67961311340332
    },
    {
      "epoch": 0.032303523035230354,
      "step": 149,
      "training_loss": 9.517083168029785
    },
    {
      "epoch": 0.032303523035230354,
      "step": 149,
      "training_loss": 9.651283264160156
    },
    {
      "epoch": 0.032303523035230354,
      "step": 149,
      "training_loss": 10.783088684082031
    },
    {
      "epoch": 0.032303523035230354,
      "step": 149,
      "training_loss": 8.740230560302734
    },
    {
      "epoch": 0.032520325203252036,
      "step": 150,
      "training_loss": 8.474102973937988
    },
    {
      "epoch": 0.032520325203252036,
      "step": 150,
      "training_loss": 10.377204895019531
    },
    {
      "epoch": 0.032520325203252036,
      "step": 150,
      "training_loss": 8.892976760864258
    },
    {
      "epoch": 0.032520325203252036,
      "step": 150,
      "training_loss": 9.570219993591309
    },
    {
      "epoch": 0.03273712737127371,
      "step": 151,
      "training_loss": 8.85846996307373
    },
    {
      "epoch": 0.03273712737127371,
      "step": 151,
      "training_loss": 8.898930549621582
    },
    {
      "epoch": 0.03273712737127371,
      "step": 151,
      "training_loss": 8.135441780090332
    },
    {
      "epoch": 0.03273712737127371,
      "step": 151,
      "training_loss": 8.091106414794922
    },
    {
      "epoch": 0.03295392953929539,
      "grad_norm": 31.79426383972168,
      "learning_rate": 1e-05,
      "loss": 9.3407,
      "step": 152
    },
    {
      "epoch": 0.03295392953929539,
      "step": 152,
      "training_loss": 9.3147554397583
    },
    {
      "epoch": 0.03295392953929539,
      "step": 152,
      "training_loss": 9.760736465454102
    },
    {
      "epoch": 0.03295392953929539,
      "step": 152,
      "training_loss": 9.214138984680176
    },
    {
      "epoch": 0.03295392953929539,
      "step": 152,
      "training_loss": 8.246806144714355
    },
    {
      "epoch": 0.033170731707317075,
      "step": 153,
      "training_loss": 9.55620002746582
    },
    {
      "epoch": 0.033170731707317075,
      "step": 153,
      "training_loss": 8.736401557922363
    },
    {
      "epoch": 0.033170731707317075,
      "step": 153,
      "training_loss": 9.278958320617676
    },
    {
      "epoch": 0.033170731707317075,
      "step": 153,
      "training_loss": 10.736263275146484
    },
    {
      "epoch": 0.03338753387533876,
      "step": 154,
      "training_loss": 11.434940338134766
    },
    {
      "epoch": 0.03338753387533876,
      "step": 154,
      "training_loss": 10.110197067260742
    },
    {
      "epoch": 0.03338753387533876,
      "step": 154,
      "training_loss": 10.372203826904297
    },
    {
      "epoch": 0.03338753387533876,
      "step": 154,
      "training_loss": 8.190546989440918
    },
    {
      "epoch": 0.03360433604336043,
      "step": 155,
      "training_loss": 8.662943840026855
    },
    {
      "epoch": 0.03360433604336043,
      "step": 155,
      "training_loss": 9.689093589782715
    },
    {
      "epoch": 0.03360433604336043,
      "step": 155,
      "training_loss": 8.526435852050781
    },
    {
      "epoch": 0.03360433604336043,
      "step": 155,
      "training_loss": 8.810677528381348
    },
    {
      "epoch": 0.033821138211382114,
      "grad_norm": 35.98637771606445,
      "learning_rate": 1e-05,
      "loss": 9.4151,
      "step": 156
    },
    {
      "epoch": 0.033821138211382114,
      "step": 156,
      "training_loss": 7.906284332275391
    },
    {
      "epoch": 0.033821138211382114,
      "step": 156,
      "training_loss": 9.379060745239258
    },
    {
      "epoch": 0.033821138211382114,
      "step": 156,
      "training_loss": 8.856088638305664
    },
    {
      "epoch": 0.033821138211382114,
      "step": 156,
      "training_loss": 9.063264846801758
    },
    {
      "epoch": 0.034037940379403796,
      "step": 157,
      "training_loss": 9.082563400268555
    },
    {
      "epoch": 0.034037940379403796,
      "step": 157,
      "training_loss": 9.511397361755371
    },
    {
      "epoch": 0.034037940379403796,
      "step": 157,
      "training_loss": 9.212658882141113
    },
    {
      "epoch": 0.034037940379403796,
      "step": 157,
      "training_loss": 9.475129127502441
    },
    {
      "epoch": 0.03425474254742548,
      "step": 158,
      "training_loss": 9.524382591247559
    },
    {
      "epoch": 0.03425474254742548,
      "step": 158,
      "training_loss": 8.3201265335083
    },
    {
      "epoch": 0.03425474254742548,
      "step": 158,
      "training_loss": 8.714247703552246
    },
    {
      "epoch": 0.03425474254742548,
      "step": 158,
      "training_loss": 9.885592460632324
    },
    {
      "epoch": 0.03447154471544715,
      "step": 159,
      "training_loss": 10.072463035583496
    },
    {
      "epoch": 0.03447154471544715,
      "step": 159,
      "training_loss": 8.202421188354492
    },
    {
      "epoch": 0.03447154471544715,
      "step": 159,
      "training_loss": 9.207663536071777
    },
    {
      "epoch": 0.03447154471544715,
      "step": 159,
      "training_loss": 9.395827293395996
    },
    {
      "epoch": 0.034688346883468835,
      "grad_norm": 28.816497802734375,
      "learning_rate": 1e-05,
      "loss": 9.1131,
      "step": 160
    },
    {
      "epoch": 0.034688346883468835,
      "step": 160,
      "training_loss": 8.536709785461426
    },
    {
      "epoch": 0.034688346883468835,
      "step": 160,
      "training_loss": 8.882932662963867
    },
    {
      "epoch": 0.034688346883468835,
      "step": 160,
      "training_loss": 8.119142532348633
    },
    {
      "epoch": 0.034688346883468835,
      "step": 160,
      "training_loss": 9.059551239013672
    },
    {
      "epoch": 0.03490514905149052,
      "step": 161,
      "training_loss": 7.959689140319824
    },
    {
      "epoch": 0.03490514905149052,
      "step": 161,
      "training_loss": 8.734158515930176
    },
    {
      "epoch": 0.03490514905149052,
      "step": 161,
      "training_loss": 8.455190658569336
    },
    {
      "epoch": 0.03490514905149052,
      "step": 161,
      "training_loss": 7.655040740966797
    },
    {
      "epoch": 0.0351219512195122,
      "step": 162,
      "training_loss": 10.639800071716309
    },
    {
      "epoch": 0.0351219512195122,
      "step": 162,
      "training_loss": 8.389206886291504
    },
    {
      "epoch": 0.0351219512195122,
      "step": 162,
      "training_loss": 8.661149024963379
    },
    {
      "epoch": 0.0351219512195122,
      "step": 162,
      "training_loss": 5.73712158203125
    },
    {
      "epoch": 0.035338753387533874,
      "step": 163,
      "training_loss": 8.711015701293945
    },
    {
      "epoch": 0.035338753387533874,
      "step": 163,
      "training_loss": 10.579140663146973
    },
    {
      "epoch": 0.035338753387533874,
      "step": 163,
      "training_loss": 8.373502731323242
    },
    {
      "epoch": 0.035338753387533874,
      "step": 163,
      "training_loss": 8.312232971191406
    },
    {
      "epoch": 0.035555555555555556,
      "grad_norm": 60.62960433959961,
      "learning_rate": 1e-05,
      "loss": 8.5503,
      "step": 164
    },
    {
      "epoch": 0.035555555555555556,
      "step": 164,
      "training_loss": 7.390329837799072
    },
    {
      "epoch": 0.035555555555555556,
      "step": 164,
      "training_loss": 9.023292541503906
    },
    {
      "epoch": 0.035555555555555556,
      "step": 164,
      "training_loss": 8.48611831665039
    },
    {
      "epoch": 0.035555555555555556,
      "step": 164,
      "training_loss": 7.57403564453125
    },
    {
      "epoch": 0.03577235772357724,
      "step": 165,
      "training_loss": 8.608896255493164
    },
    {
      "epoch": 0.03577235772357724,
      "step": 165,
      "training_loss": 8.863012313842773
    },
    {
      "epoch": 0.03577235772357724,
      "step": 165,
      "training_loss": 8.088973045349121
    },
    {
      "epoch": 0.03577235772357724,
      "step": 165,
      "training_loss": 7.167816162109375
    },
    {
      "epoch": 0.03598915989159892,
      "step": 166,
      "training_loss": 7.6144795417785645
    },
    {
      "epoch": 0.03598915989159892,
      "step": 166,
      "training_loss": 8.936484336853027
    },
    {
      "epoch": 0.03598915989159892,
      "step": 166,
      "training_loss": 8.742755889892578
    },
    {
      "epoch": 0.03598915989159892,
      "step": 166,
      "training_loss": 8.641460418701172
    },
    {
      "epoch": 0.036205962059620594,
      "step": 167,
      "training_loss": 8.576921463012695
    },
    {
      "epoch": 0.036205962059620594,
      "step": 167,
      "training_loss": 9.91187572479248
    },
    {
      "epoch": 0.036205962059620594,
      "step": 167,
      "training_loss": 8.925762176513672
    },
    {
      "epoch": 0.036205962059620594,
      "step": 167,
      "training_loss": 10.060413360595703
    },
    {
      "epoch": 0.036422764227642276,
      "grad_norm": 62.11560821533203,
      "learning_rate": 1e-05,
      "loss": 8.5383,
      "step": 168
    },
    {
      "epoch": 0.036422764227642276,
      "step": 168,
      "training_loss": 8.259425163269043
    },
    {
      "epoch": 0.036422764227642276,
      "step": 168,
      "training_loss": 9.275432586669922
    },
    {
      "epoch": 0.036422764227642276,
      "step": 168,
      "training_loss": 8.133602142333984
    },
    {
      "epoch": 0.036422764227642276,
      "step": 168,
      "training_loss": 7.654846668243408
    },
    {
      "epoch": 0.03663956639566396,
      "step": 169,
      "training_loss": 8.471658706665039
    },
    {
      "epoch": 0.03663956639566396,
      "step": 169,
      "training_loss": 8.556511878967285
    },
    {
      "epoch": 0.03663956639566396,
      "step": 169,
      "training_loss": 9.500248908996582
    },
    {
      "epoch": 0.03663956639566396,
      "step": 169,
      "training_loss": 9.866628646850586
    },
    {
      "epoch": 0.03685636856368564,
      "step": 170,
      "training_loss": 7.408977031707764
    },
    {
      "epoch": 0.03685636856368564,
      "step": 170,
      "training_loss": 8.865582466125488
    },
    {
      "epoch": 0.03685636856368564,
      "step": 170,
      "training_loss": 9.459897994995117
    },
    {
      "epoch": 0.03685636856368564,
      "step": 170,
      "training_loss": 9.143144607543945
    },
    {
      "epoch": 0.037073170731707315,
      "step": 171,
      "training_loss": 10.816558837890625
    },
    {
      "epoch": 0.037073170731707315,
      "step": 171,
      "training_loss": 10.215564727783203
    },
    {
      "epoch": 0.037073170731707315,
      "step": 171,
      "training_loss": 9.181129455566406
    },
    {
      "epoch": 0.037073170731707315,
      "step": 171,
      "training_loss": 8.083414077758789
    },
    {
      "epoch": 0.037289972899729,
      "grad_norm": 20.97196388244629,
      "learning_rate": 1e-05,
      "loss": 8.9308,
      "step": 172
    },
    {
      "epoch": 0.037289972899729,
      "step": 172,
      "training_loss": 7.296741485595703
    },
    {
      "epoch": 0.037289972899729,
      "step": 172,
      "training_loss": 8.469498634338379
    },
    {
      "epoch": 0.037289972899729,
      "step": 172,
      "training_loss": 8.711353302001953
    },
    {
      "epoch": 0.037289972899729,
      "step": 172,
      "training_loss": 8.047564506530762
    },
    {
      "epoch": 0.03750677506775068,
      "step": 173,
      "training_loss": 8.868247032165527
    },
    {
      "epoch": 0.03750677506775068,
      "step": 173,
      "training_loss": 9.513591766357422
    },
    {
      "epoch": 0.03750677506775068,
      "step": 173,
      "training_loss": 8.654505729675293
    },
    {
      "epoch": 0.03750677506775068,
      "step": 173,
      "training_loss": 9.258834838867188
    },
    {
      "epoch": 0.03772357723577236,
      "step": 174,
      "training_loss": 8.055747032165527
    },
    {
      "epoch": 0.03772357723577236,
      "step": 174,
      "training_loss": 7.915574073791504
    },
    {
      "epoch": 0.03772357723577236,
      "step": 174,
      "training_loss": 8.46495246887207
    },
    {
      "epoch": 0.03772357723577236,
      "step": 174,
      "training_loss": 7.163816928863525
    },
    {
      "epoch": 0.037940379403794036,
      "step": 175,
      "training_loss": 8.589943885803223
    },
    {
      "epoch": 0.037940379403794036,
      "step": 175,
      "training_loss": 8.547208786010742
    },
    {
      "epoch": 0.037940379403794036,
      "step": 175,
      "training_loss": 9.093790054321289
    },
    {
      "epoch": 0.037940379403794036,
      "step": 175,
      "training_loss": 7.667160511016846
    },
    {
      "epoch": 0.03815718157181572,
      "grad_norm": 56.327720642089844,
      "learning_rate": 1e-05,
      "loss": 8.3949,
      "step": 176
    },
    {
      "epoch": 0.03815718157181572,
      "step": 176,
      "training_loss": 8.788019180297852
    },
    {
      "epoch": 0.03815718157181572,
      "step": 176,
      "training_loss": 8.332951545715332
    },
    {
      "epoch": 0.03815718157181572,
      "step": 176,
      "training_loss": 8.647704124450684
    },
    {
      "epoch": 0.03815718157181572,
      "step": 176,
      "training_loss": 9.427600860595703
    },
    {
      "epoch": 0.0383739837398374,
      "step": 177,
      "training_loss": 9.240089416503906
    },
    {
      "epoch": 0.0383739837398374,
      "step": 177,
      "training_loss": 8.732690811157227
    },
    {
      "epoch": 0.0383739837398374,
      "step": 177,
      "training_loss": 8.682411193847656
    },
    {
      "epoch": 0.0383739837398374,
      "step": 177,
      "training_loss": 7.852941513061523
    },
    {
      "epoch": 0.03859078590785908,
      "step": 178,
      "training_loss": 7.7562971115112305
    },
    {
      "epoch": 0.03859078590785908,
      "step": 178,
      "training_loss": 7.730259418487549
    },
    {
      "epoch": 0.03859078590785908,
      "step": 178,
      "training_loss": 8.091879844665527
    },
    {
      "epoch": 0.03859078590785908,
      "step": 178,
      "training_loss": 8.22940731048584
    },
    {
      "epoch": 0.03880758807588076,
      "step": 179,
      "training_loss": 7.22964334487915
    },
    {
      "epoch": 0.03880758807588076,
      "step": 179,
      "training_loss": 8.83569622039795
    },
    {
      "epoch": 0.03880758807588076,
      "step": 179,
      "training_loss": 8.042238235473633
    },
    {
      "epoch": 0.03880758807588076,
      "step": 179,
      "training_loss": 6.864123344421387
    },
    {
      "epoch": 0.03902439024390244,
      "grad_norm": 18.238889694213867,
      "learning_rate": 1e-05,
      "loss": 8.2802,
      "step": 180
    },
    {
      "epoch": 0.03902439024390244,
      "step": 180,
      "training_loss": 9.9585542678833
    },
    {
      "epoch": 0.03902439024390244,
      "step": 180,
      "training_loss": 9.26081371307373
    },
    {
      "epoch": 0.03902439024390244,
      "step": 180,
      "training_loss": 7.836890697479248
    },
    {
      "epoch": 0.03902439024390244,
      "step": 180,
      "training_loss": 7.836526393890381
    },
    {
      "epoch": 0.03924119241192412,
      "step": 181,
      "training_loss": 7.564270496368408
    },
    {
      "epoch": 0.03924119241192412,
      "step": 181,
      "training_loss": 6.98668909072876
    },
    {
      "epoch": 0.03924119241192412,
      "step": 181,
      "training_loss": 8.647092819213867
    },
    {
      "epoch": 0.03924119241192412,
      "step": 181,
      "training_loss": 8.836336135864258
    },
    {
      "epoch": 0.0394579945799458,
      "step": 182,
      "training_loss": 9.124706268310547
    },
    {
      "epoch": 0.0394579945799458,
      "step": 182,
      "training_loss": 10.153733253479004
    },
    {
      "epoch": 0.0394579945799458,
      "step": 182,
      "training_loss": 8.236933708190918
    },
    {
      "epoch": 0.0394579945799458,
      "step": 182,
      "training_loss": 9.469133377075195
    },
    {
      "epoch": 0.03967479674796748,
      "step": 183,
      "training_loss": 8.319482803344727
    },
    {
      "epoch": 0.03967479674796748,
      "step": 183,
      "training_loss": 9.379897117614746
    },
    {
      "epoch": 0.03967479674796748,
      "step": 183,
      "training_loss": 10.24155330657959
    },
    {
      "epoch": 0.03967479674796748,
      "step": 183,
      "training_loss": 6.987492561340332
    },
    {
      "epoch": 0.03989159891598916,
      "grad_norm": 31.418651580810547,
      "learning_rate": 1e-05,
      "loss": 8.6775,
      "step": 184
    },
    {
      "epoch": 0.03989159891598916,
      "step": 184,
      "training_loss": 9.051019668579102
    },
    {
      "epoch": 0.03989159891598916,
      "step": 184,
      "training_loss": 10.01270866394043
    },
    {
      "epoch": 0.03989159891598916,
      "step": 184,
      "training_loss": 9.33643913269043
    },
    {
      "epoch": 0.03989159891598916,
      "step": 184,
      "training_loss": 7.88789701461792
    },
    {
      "epoch": 0.04010840108401084,
      "step": 185,
      "training_loss": 8.683876991271973
    },
    {
      "epoch": 0.04010840108401084,
      "step": 185,
      "training_loss": 8.728982925415039
    },
    {
      "epoch": 0.04010840108401084,
      "step": 185,
      "training_loss": 8.188596725463867
    },
    {
      "epoch": 0.04010840108401084,
      "step": 185,
      "training_loss": 8.408267974853516
    },
    {
      "epoch": 0.040325203252032524,
      "step": 186,
      "training_loss": 8.219948768615723
    },
    {
      "epoch": 0.040325203252032524,
      "step": 186,
      "training_loss": 7.1108832359313965
    },
    {
      "epoch": 0.040325203252032524,
      "step": 186,
      "training_loss": 9.590967178344727
    },
    {
      "epoch": 0.040325203252032524,
      "step": 186,
      "training_loss": 7.155070781707764
    },
    {
      "epoch": 0.0405420054200542,
      "step": 187,
      "training_loss": 8.027523040771484
    },
    {
      "epoch": 0.0405420054200542,
      "step": 187,
      "training_loss": 8.421154022216797
    },
    {
      "epoch": 0.0405420054200542,
      "step": 187,
      "training_loss": 8.876898765563965
    },
    {
      "epoch": 0.0405420054200542,
      "step": 187,
      "training_loss": 7.312948703765869
    },
    {
      "epoch": 0.04075880758807588,
      "grad_norm": 57.727149963378906,
      "learning_rate": 1e-05,
      "loss": 8.4383,
      "step": 188
    },
    {
      "epoch": 0.04075880758807588,
      "step": 188,
      "training_loss": 9.087652206420898
    },
    {
      "epoch": 0.04075880758807588,
      "step": 188,
      "training_loss": 8.564638137817383
    },
    {
      "epoch": 0.04075880758807588,
      "step": 188,
      "training_loss": 7.775477409362793
    },
    {
      "epoch": 0.04075880758807588,
      "step": 188,
      "training_loss": 7.907656669616699
    },
    {
      "epoch": 0.04097560975609756,
      "step": 189,
      "training_loss": 7.730147838592529
    },
    {
      "epoch": 0.04097560975609756,
      "step": 189,
      "training_loss": 6.563903331756592
    },
    {
      "epoch": 0.04097560975609756,
      "step": 189,
      "training_loss": 8.54993724822998
    },
    {
      "epoch": 0.04097560975609756,
      "step": 189,
      "training_loss": 8.412701606750488
    },
    {
      "epoch": 0.041192411924119245,
      "step": 190,
      "training_loss": 7.840000629425049
    },
    {
      "epoch": 0.041192411924119245,
      "step": 190,
      "training_loss": 8.23355770111084
    },
    {
      "epoch": 0.041192411924119245,
      "step": 190,
      "training_loss": 8.613897323608398
    },
    {
      "epoch": 0.041192411924119245,
      "step": 190,
      "training_loss": 8.864795684814453
    },
    {
      "epoch": 0.04140921409214092,
      "step": 191,
      "training_loss": 9.523693084716797
    },
    {
      "epoch": 0.04140921409214092,
      "step": 191,
      "training_loss": 8.001965522766113
    },
    {
      "epoch": 0.04140921409214092,
      "step": 191,
      "training_loss": 7.710859775543213
    },
    {
      "epoch": 0.04140921409214092,
      "step": 191,
      "training_loss": 7.890651702880859
    },
    {
      "epoch": 0.0416260162601626,
      "grad_norm": 52.21328353881836,
      "learning_rate": 1e-05,
      "loss": 8.2045,
      "step": 192
    },
    {
      "epoch": 0.0416260162601626,
      "step": 192,
      "training_loss": 7.315098285675049
    },
    {
      "epoch": 0.0416260162601626,
      "step": 192,
      "training_loss": 7.967780113220215
    },
    {
      "epoch": 0.0416260162601626,
      "step": 192,
      "training_loss": 8.478693008422852
    },
    {
      "epoch": 0.0416260162601626,
      "step": 192,
      "training_loss": 7.755116939544678
    },
    {
      "epoch": 0.041842818428184284,
      "step": 193,
      "training_loss": 7.3687424659729
    },
    {
      "epoch": 0.041842818428184284,
      "step": 193,
      "training_loss": 9.729471206665039
    },
    {
      "epoch": 0.041842818428184284,
      "step": 193,
      "training_loss": 8.316875457763672
    },
    {
      "epoch": 0.041842818428184284,
      "step": 193,
      "training_loss": 8.125467300415039
    },
    {
      "epoch": 0.042059620596205965,
      "step": 194,
      "training_loss": 8.202564239501953
    },
    {
      "epoch": 0.042059620596205965,
      "step": 194,
      "training_loss": 8.249571800231934
    },
    {
      "epoch": 0.042059620596205965,
      "step": 194,
      "training_loss": 8.180390357971191
    },
    {
      "epoch": 0.042059620596205965,
      "step": 194,
      "training_loss": 8.48539924621582
    },
    {
      "epoch": 0.04227642276422764,
      "step": 195,
      "training_loss": 7.395503997802734
    },
    {
      "epoch": 0.04227642276422764,
      "step": 195,
      "training_loss": 8.297281265258789
    },
    {
      "epoch": 0.04227642276422764,
      "step": 195,
      "training_loss": 7.333622932434082
    },
    {
      "epoch": 0.04227642276422764,
      "step": 195,
      "training_loss": 7.924859046936035
    },
    {
      "epoch": 0.04249322493224932,
      "grad_norm": 49.23585891723633,
      "learning_rate": 1e-05,
      "loss": 8.0704,
      "step": 196
    },
    {
      "epoch": 0.04249322493224932,
      "step": 196,
      "training_loss": 7.0040388107299805
    },
    {
      "epoch": 0.04249322493224932,
      "step": 196,
      "training_loss": 6.66881799697876
    },
    {
      "epoch": 0.04249322493224932,
      "step": 196,
      "training_loss": 8.309988021850586
    },
    {
      "epoch": 0.04249322493224932,
      "step": 196,
      "training_loss": 8.243276596069336
    },
    {
      "epoch": 0.042710027100271004,
      "step": 197,
      "training_loss": 7.562500953674316
    },
    {
      "epoch": 0.042710027100271004,
      "step": 197,
      "training_loss": 6.715220928192139
    },
    {
      "epoch": 0.042710027100271004,
      "step": 197,
      "training_loss": 7.91696834564209
    },
    {
      "epoch": 0.042710027100271004,
      "step": 197,
      "training_loss": 10.578045845031738
    },
    {
      "epoch": 0.042926829268292686,
      "step": 198,
      "training_loss": 7.919389724731445
    },
    {
      "epoch": 0.042926829268292686,
      "step": 198,
      "training_loss": 8.576778411865234
    },
    {
      "epoch": 0.042926829268292686,
      "step": 198,
      "training_loss": 8.608426094055176
    },
    {
      "epoch": 0.042926829268292686,
      "step": 198,
      "training_loss": 8.12193775177002
    },
    {
      "epoch": 0.04314363143631436,
      "step": 199,
      "training_loss": 8.059076309204102
    },
    {
      "epoch": 0.04314363143631436,
      "step": 199,
      "training_loss": 8.79384708404541
    },
    {
      "epoch": 0.04314363143631436,
      "step": 199,
      "training_loss": 7.230030059814453
    },
    {
      "epoch": 0.04314363143631436,
      "step": 199,
      "training_loss": 9.651134490966797
    },
    {
      "epoch": 0.04336043360433604,
      "grad_norm": 19.55570411682129,
      "learning_rate": 1e-05,
      "loss": 8.1225,
      "step": 200
    },
    {
      "epoch": 0.04336043360433604,
      "step": 200,
      "training_loss": 9.052474975585938
    },
    {
      "epoch": 0.04336043360433604,
      "step": 200,
      "training_loss": 7.754600524902344
    },
    {
      "epoch": 0.04336043360433604,
      "step": 200,
      "training_loss": 9.408693313598633
    },
    {
      "epoch": 0.04336043360433604,
      "step": 200,
      "training_loss": 6.877816677093506
    },
    {
      "epoch": 0.043577235772357725,
      "step": 201,
      "training_loss": 7.203100681304932
    },
    {
      "epoch": 0.043577235772357725,
      "step": 201,
      "training_loss": 9.065375328063965
    },
    {
      "epoch": 0.043577235772357725,
      "step": 201,
      "training_loss": 9.716856956481934
    },
    {
      "epoch": 0.043577235772357725,
      "step": 201,
      "training_loss": 8.268153190612793
    },
    {
      "epoch": 0.04379403794037941,
      "step": 202,
      "training_loss": 7.287124156951904
    },
    {
      "epoch": 0.04379403794037941,
      "step": 202,
      "training_loss": 9.3938570022583
    },
    {
      "epoch": 0.04379403794037941,
      "step": 202,
      "training_loss": 7.906777381896973
    },
    {
      "epoch": 0.04379403794037941,
      "step": 202,
      "training_loss": 8.653108596801758
    },
    {
      "epoch": 0.04401084010840108,
      "step": 203,
      "training_loss": 7.819037437438965
    },
    {
      "epoch": 0.04401084010840108,
      "step": 203,
      "training_loss": 8.264877319335938
    },
    {
      "epoch": 0.04401084010840108,
      "step": 203,
      "training_loss": 7.377601146697998
    },
    {
      "epoch": 0.04401084010840108,
      "step": 203,
      "training_loss": 6.1667022705078125
    },
    {
      "epoch": 0.044227642276422764,
      "grad_norm": 10.758712768554688,
      "learning_rate": 1e-05,
      "loss": 8.1385,
      "step": 204
    },
    {
      "epoch": 0.044227642276422764,
      "step": 204,
      "training_loss": 7.2198872566223145
    },
    {
      "epoch": 0.044227642276422764,
      "step": 204,
      "training_loss": 7.649533271789551
    },
    {
      "epoch": 0.044227642276422764,
      "step": 204,
      "training_loss": 8.436734199523926
    },
    {
      "epoch": 0.044227642276422764,
      "step": 204,
      "training_loss": 7.8632378578186035
    },
    {
      "epoch": 0.044444444444444446,
      "step": 205,
      "training_loss": 7.554119110107422
    },
    {
      "epoch": 0.044444444444444446,
      "step": 205,
      "training_loss": 8.256949424743652
    },
    {
      "epoch": 0.044444444444444446,
      "step": 205,
      "training_loss": 7.484814167022705
    },
    {
      "epoch": 0.044444444444444446,
      "step": 205,
      "training_loss": 7.251282215118408
    },
    {
      "epoch": 0.04466124661246613,
      "step": 206,
      "training_loss": 7.4292683601379395
    },
    {
      "epoch": 0.04466124661246613,
      "step": 206,
      "training_loss": 6.3924102783203125
    },
    {
      "epoch": 0.04466124661246613,
      "step": 206,
      "training_loss": 8.469098091125488
    },
    {
      "epoch": 0.04466124661246613,
      "step": 206,
      "training_loss": 7.990666389465332
    },
    {
      "epoch": 0.0448780487804878,
      "step": 207,
      "training_loss": 8.00766658782959
    },
    {
      "epoch": 0.0448780487804878,
      "step": 207,
      "training_loss": 7.979650497436523
    },
    {
      "epoch": 0.0448780487804878,
      "step": 207,
      "training_loss": 7.956889629364014
    },
    {
      "epoch": 0.0448780487804878,
      "step": 207,
      "training_loss": 8.124571800231934
    },
    {
      "epoch": 0.045094850948509485,
      "grad_norm": 19.488603591918945,
      "learning_rate": 1e-05,
      "loss": 7.7542,
      "step": 208
    },
    {
      "epoch": 0.045094850948509485,
      "step": 208,
      "training_loss": 8.203861236572266
    },
    {
      "epoch": 0.045094850948509485,
      "step": 208,
      "training_loss": 9.992316246032715
    },
    {
      "epoch": 0.045094850948509485,
      "step": 208,
      "training_loss": 8.732843399047852
    },
    {
      "epoch": 0.045094850948509485,
      "step": 208,
      "training_loss": 9.632978439331055
    },
    {
      "epoch": 0.04531165311653117,
      "step": 209,
      "training_loss": 8.836840629577637
    },
    {
      "epoch": 0.04531165311653117,
      "step": 209,
      "training_loss": 8.115530967712402
    },
    {
      "epoch": 0.04531165311653117,
      "step": 209,
      "training_loss": 9.070371627807617
    },
    {
      "epoch": 0.04531165311653117,
      "step": 209,
      "training_loss": 8.134398460388184
    },
    {
      "epoch": 0.04552845528455285,
      "step": 210,
      "training_loss": 7.688992500305176
    },
    {
      "epoch": 0.04552845528455285,
      "step": 210,
      "training_loss": 7.120285987854004
    },
    {
      "epoch": 0.04552845528455285,
      "step": 210,
      "training_loss": 7.360812664031982
    },
    {
      "epoch": 0.04552845528455285,
      "step": 210,
      "training_loss": 6.998596668243408
    },
    {
      "epoch": 0.045745257452574524,
      "step": 211,
      "training_loss": 8.088738441467285
    },
    {
      "epoch": 0.045745257452574524,
      "step": 211,
      "training_loss": 9.284534454345703
    },
    {
      "epoch": 0.045745257452574524,
      "step": 211,
      "training_loss": 7.835326194763184
    },
    {
      "epoch": 0.045745257452574524,
      "step": 211,
      "training_loss": 9.237963676452637
    },
    {
      "epoch": 0.045962059620596206,
      "grad_norm": 16.1412353515625,
      "learning_rate": 1e-05,
      "loss": 8.3959,
      "step": 212
    },
    {
      "epoch": 0.045962059620596206,
      "step": 212,
      "training_loss": 8.235233306884766
    },
    {
      "epoch": 0.045962059620596206,
      "step": 212,
      "training_loss": 9.009893417358398
    },
    {
      "epoch": 0.045962059620596206,
      "step": 212,
      "training_loss": 7.966220855712891
    },
    {
      "epoch": 0.045962059620596206,
      "step": 212,
      "training_loss": 8.355354309082031
    },
    {
      "epoch": 0.04617886178861789,
      "step": 213,
      "training_loss": 8.40869426727295
    },
    {
      "epoch": 0.04617886178861789,
      "step": 213,
      "training_loss": 5.930446147918701
    },
    {
      "epoch": 0.04617886178861789,
      "step": 213,
      "training_loss": 7.005673408508301
    },
    {
      "epoch": 0.04617886178861789,
      "step": 213,
      "training_loss": 7.668601036071777
    },
    {
      "epoch": 0.04639566395663957,
      "step": 214,
      "training_loss": 7.942913055419922
    },
    {
      "epoch": 0.04639566395663957,
      "step": 214,
      "training_loss": 8.050304412841797
    },
    {
      "epoch": 0.04639566395663957,
      "step": 214,
      "training_loss": 10.574621200561523
    },
    {
      "epoch": 0.04639566395663957,
      "step": 214,
      "training_loss": 8.318849563598633
    },
    {
      "epoch": 0.046612466124661245,
      "step": 215,
      "training_loss": 7.527249813079834
    },
    {
      "epoch": 0.046612466124661245,
      "step": 215,
      "training_loss": 8.245226860046387
    },
    {
      "epoch": 0.046612466124661245,
      "step": 215,
      "training_loss": 8.469914436340332
    },
    {
      "epoch": 0.046612466124661245,
      "step": 215,
      "training_loss": 10.580761909484863
    },
    {
      "epoch": 0.04682926829268293,
      "grad_norm": 20.49294662475586,
      "learning_rate": 1e-05,
      "loss": 8.2681,
      "step": 216
    },
    {
      "epoch": 0.04682926829268293,
      "step": 216,
      "training_loss": 8.027024269104004
    },
    {
      "epoch": 0.04682926829268293,
      "step": 216,
      "training_loss": 8.288110733032227
    },
    {
      "epoch": 0.04682926829268293,
      "step": 216,
      "training_loss": 7.063637733459473
    },
    {
      "epoch": 0.04682926829268293,
      "step": 216,
      "training_loss": 6.82849645614624
    },
    {
      "epoch": 0.04704607046070461,
      "step": 217,
      "training_loss": 9.026834487915039
    },
    {
      "epoch": 0.04704607046070461,
      "step": 217,
      "training_loss": 7.99335241317749
    },
    {
      "epoch": 0.04704607046070461,
      "step": 217,
      "training_loss": 8.495424270629883
    },
    {
      "epoch": 0.04704607046070461,
      "step": 217,
      "training_loss": 7.043656826019287
    },
    {
      "epoch": 0.04726287262872629,
      "step": 218,
      "training_loss": 8.060832977294922
    },
    {
      "epoch": 0.04726287262872629,
      "step": 218,
      "training_loss": 8.849339485168457
    },
    {
      "epoch": 0.04726287262872629,
      "step": 218,
      "training_loss": 7.896482944488525
    },
    {
      "epoch": 0.04726287262872629,
      "step": 218,
      "training_loss": 7.5275044441223145
    },
    {
      "epoch": 0.047479674796747966,
      "step": 219,
      "training_loss": 8.3085298538208
    },
    {
      "epoch": 0.047479674796747966,
      "step": 219,
      "training_loss": 8.003089904785156
    },
    {
      "epoch": 0.047479674796747966,
      "step": 219,
      "training_loss": 7.575748920440674
    },
    {
      "epoch": 0.047479674796747966,
      "step": 219,
      "training_loss": 7.611477375030518
    },
    {
      "epoch": 0.04769647696476965,
      "grad_norm": 17.54635238647461,
      "learning_rate": 1e-05,
      "loss": 7.9125,
      "step": 220
    },
    {
      "epoch": 0.04769647696476965,
      "step": 220,
      "training_loss": 7.069464206695557
    },
    {
      "epoch": 0.04769647696476965,
      "step": 220,
      "training_loss": 9.408080101013184
    },
    {
      "epoch": 0.04769647696476965,
      "step": 220,
      "training_loss": 7.127811908721924
    },
    {
      "epoch": 0.04769647696476965,
      "step": 220,
      "training_loss": 6.7129340171813965
    },
    {
      "epoch": 0.04791327913279133,
      "step": 221,
      "training_loss": 8.840887069702148
    },
    {
      "epoch": 0.04791327913279133,
      "step": 221,
      "training_loss": 8.074884414672852
    },
    {
      "epoch": 0.04791327913279133,
      "step": 221,
      "training_loss": 6.759345054626465
    },
    {
      "epoch": 0.04791327913279133,
      "step": 221,
      "training_loss": 8.115578651428223
    },
    {
      "epoch": 0.04813008130081301,
      "step": 222,
      "training_loss": 7.659296989440918
    },
    {
      "epoch": 0.04813008130081301,
      "step": 222,
      "training_loss": 7.717864036560059
    },
    {
      "epoch": 0.04813008130081301,
      "step": 222,
      "training_loss": 7.840294361114502
    },
    {
      "epoch": 0.04813008130081301,
      "step": 222,
      "training_loss": 8.144059181213379
    },
    {
      "epoch": 0.04834688346883469,
      "step": 223,
      "training_loss": 8.195585250854492
    },
    {
      "epoch": 0.04834688346883469,
      "step": 223,
      "training_loss": 7.7703986167907715
    },
    {
      "epoch": 0.04834688346883469,
      "step": 223,
      "training_loss": 7.920498371124268
    },
    {
      "epoch": 0.04834688346883469,
      "step": 223,
      "training_loss": 9.07197380065918
    },
    {
      "epoch": 0.04856368563685637,
      "grad_norm": 15.439579963684082,
      "learning_rate": 1e-05,
      "loss": 7.9018,
      "step": 224
    },
    {
      "epoch": 0.04856368563685637,
      "step": 224,
      "training_loss": 7.98779821395874
    },
    {
      "epoch": 0.04856368563685637,
      "step": 224,
      "training_loss": 8.138240814208984
    },
    {
      "epoch": 0.04856368563685637,
      "step": 224,
      "training_loss": 7.680708885192871
    },
    {
      "epoch": 0.04856368563685637,
      "step": 224,
      "training_loss": 8.101638793945312
    },
    {
      "epoch": 0.04878048780487805,
      "step": 225,
      "training_loss": 6.8900465965271
    },
    {
      "epoch": 0.04878048780487805,
      "step": 225,
      "training_loss": 8.03573989868164
    },
    {
      "epoch": 0.04878048780487805,
      "step": 225,
      "training_loss": 7.009772777557373
    },
    {
      "epoch": 0.04878048780487805,
      "step": 225,
      "training_loss": 7.5413312911987305
    },
    {
      "epoch": 0.04899728997289973,
      "step": 226,
      "training_loss": 7.86841344833374
    },
    {
      "epoch": 0.04899728997289973,
      "step": 226,
      "training_loss": 7.849111557006836
    },
    {
      "epoch": 0.04899728997289973,
      "step": 226,
      "training_loss": 8.689005851745605
    },
    {
      "epoch": 0.04899728997289973,
      "step": 226,
      "training_loss": 8.10628890991211
    },
    {
      "epoch": 0.04921409214092141,
      "step": 227,
      "training_loss": 6.788441181182861
    },
    {
      "epoch": 0.04921409214092141,
      "step": 227,
      "training_loss": 8.27027702331543
    },
    {
      "epoch": 0.04921409214092141,
      "step": 227,
      "training_loss": 6.665724277496338
    },
    {
      "epoch": 0.04921409214092141,
      "step": 227,
      "training_loss": 7.36285924911499
    },
    {
      "epoch": 0.04943089430894309,
      "grad_norm": 18.863550186157227,
      "learning_rate": 1e-05,
      "loss": 7.6866,
      "step": 228
    },
    {
      "epoch": 0.04943089430894309,
      "step": 228,
      "training_loss": 8.103017807006836
    },
    {
      "epoch": 0.04943089430894309,
      "step": 228,
      "training_loss": 8.0956449508667
    },
    {
      "epoch": 0.04943089430894309,
      "step": 228,
      "training_loss": 8.007570266723633
    },
    {
      "epoch": 0.04943089430894309,
      "step": 228,
      "training_loss": 8.976572036743164
    },
    {
      "epoch": 0.04964769647696477,
      "step": 229,
      "training_loss": 8.647848129272461
    },
    {
      "epoch": 0.04964769647696477,
      "step": 229,
      "training_loss": 7.683704376220703
    },
    {
      "epoch": 0.04964769647696477,
      "step": 229,
      "training_loss": 7.535953998565674
    },
    {
      "epoch": 0.04964769647696477,
      "step": 229,
      "training_loss": 8.121408462524414
    },
    {
      "epoch": 0.04986449864498645,
      "step": 230,
      "training_loss": 8.443603515625
    },
    {
      "epoch": 0.04986449864498645,
      "step": 230,
      "training_loss": 8.622092247009277
    },
    {
      "epoch": 0.04986449864498645,
      "step": 230,
      "training_loss": 10.283246040344238
    },
    {
      "epoch": 0.04986449864498645,
      "step": 230,
      "training_loss": 8.364385604858398
    },
    {
      "epoch": 0.05008130081300813,
      "step": 231,
      "training_loss": 7.448822975158691
    },
    {
      "epoch": 0.05008130081300813,
      "step": 231,
      "training_loss": 7.617153644561768
    },
    {
      "epoch": 0.05008130081300813,
      "step": 231,
      "training_loss": 7.818017959594727
    },
    {
      "epoch": 0.05008130081300813,
      "step": 231,
      "training_loss": 7.894349098205566
    },
    {
      "epoch": 0.05029810298102981,
      "grad_norm": 5.930635929107666,
      "learning_rate": 1e-05,
      "loss": 8.229,
      "step": 232
    },
    {
      "epoch": 0.05029810298102981,
      "step": 232,
      "training_loss": 6.56287145614624
    },
    {
      "epoch": 0.05029810298102981,
      "step": 232,
      "training_loss": 8.018044471740723
    },
    {
      "epoch": 0.05029810298102981,
      "step": 232,
      "training_loss": 8.923282623291016
    },
    {
      "epoch": 0.05029810298102981,
      "step": 232,
      "training_loss": 6.824768543243408
    },
    {
      "epoch": 0.05051490514905149,
      "step": 233,
      "training_loss": 7.355700969696045
    },
    {
      "epoch": 0.05051490514905149,
      "step": 233,
      "training_loss": 7.573115825653076
    },
    {
      "epoch": 0.05051490514905149,
      "step": 233,
      "training_loss": 7.927455425262451
    },
    {
      "epoch": 0.05051490514905149,
      "step": 233,
      "training_loss": 7.714363098144531
    },
    {
      "epoch": 0.050731707317073174,
      "step": 234,
      "training_loss": 5.440317630767822
    },
    {
      "epoch": 0.050731707317073174,
      "step": 234,
      "training_loss": 8.242471694946289
    },
    {
      "epoch": 0.050731707317073174,
      "step": 234,
      "training_loss": 7.456821918487549
    },
    {
      "epoch": 0.050731707317073174,
      "step": 234,
      "training_loss": 7.892816543579102
    },
    {
      "epoch": 0.05094850948509485,
      "step": 235,
      "training_loss": 7.622716426849365
    },
    {
      "epoch": 0.05094850948509485,
      "step": 235,
      "training_loss": 7.665501117706299
    },
    {
      "epoch": 0.05094850948509485,
      "step": 235,
      "training_loss": 6.975071907043457
    },
    {
      "epoch": 0.05094850948509485,
      "step": 235,
      "training_loss": 8.437215805053711
    },
    {
      "epoch": 0.05116531165311653,
      "grad_norm": 16.298011779785156,
      "learning_rate": 1e-05,
      "loss": 7.5395,
      "step": 236
    },
    {
      "epoch": 0.05116531165311653,
      "step": 236,
      "training_loss": 6.7090277671813965
    },
    {
      "epoch": 0.05116531165311653,
      "step": 236,
      "training_loss": 8.124167442321777
    },
    {
      "epoch": 0.05116531165311653,
      "step": 236,
      "training_loss": 7.766275405883789
    },
    {
      "epoch": 0.05116531165311653,
      "step": 236,
      "training_loss": 8.778416633605957
    },
    {
      "epoch": 0.05138211382113821,
      "step": 237,
      "training_loss": 7.654317855834961
    },
    {
      "epoch": 0.05138211382113821,
      "step": 237,
      "training_loss": 7.368546009063721
    },
    {
      "epoch": 0.05138211382113821,
      "step": 237,
      "training_loss": 7.5954742431640625
    },
    {
      "epoch": 0.05138211382113821,
      "step": 237,
      "training_loss": 7.610722064971924
    },
    {
      "epoch": 0.051598915989159895,
      "step": 238,
      "training_loss": 7.359185218811035
    },
    {
      "epoch": 0.051598915989159895,
      "step": 238,
      "training_loss": 9.26130199432373
    },
    {
      "epoch": 0.051598915989159895,
      "step": 238,
      "training_loss": 8.194207191467285
    },
    {
      "epoch": 0.051598915989159895,
      "step": 238,
      "training_loss": 8.36383056640625
    },
    {
      "epoch": 0.05181571815718157,
      "step": 239,
      "training_loss": 7.924783229827881
    },
    {
      "epoch": 0.05181571815718157,
      "step": 239,
      "training_loss": 7.031407833099365
    },
    {
      "epoch": 0.05181571815718157,
      "step": 239,
      "training_loss": 6.8286333084106445
    },
    {
      "epoch": 0.05181571815718157,
      "step": 239,
      "training_loss": 7.490898609161377
    },
    {
      "epoch": 0.05203252032520325,
      "grad_norm": 25.200780868530273,
      "learning_rate": 1e-05,
      "loss": 7.7538,
      "step": 240
    },
    {
      "epoch": 0.05203252032520325,
      "step": 240,
      "training_loss": 7.719397068023682
    },
    {
      "epoch": 0.05203252032520325,
      "step": 240,
      "training_loss": 7.078297138214111
    },
    {
      "epoch": 0.05203252032520325,
      "step": 240,
      "training_loss": 7.363676071166992
    },
    {
      "epoch": 0.05203252032520325,
      "step": 240,
      "training_loss": 7.02138614654541
    },
    {
      "epoch": 0.052249322493224934,
      "step": 241,
      "training_loss": 7.078479766845703
    },
    {
      "epoch": 0.052249322493224934,
      "step": 241,
      "training_loss": 6.4569878578186035
    },
    {
      "epoch": 0.052249322493224934,
      "step": 241,
      "training_loss": 7.968657493591309
    },
    {
      "epoch": 0.052249322493224934,
      "step": 241,
      "training_loss": 8.390281677246094
    },
    {
      "epoch": 0.052466124661246616,
      "step": 242,
      "training_loss": 7.57361364364624
    },
    {
      "epoch": 0.052466124661246616,
      "step": 242,
      "training_loss": 6.268011569976807
    },
    {
      "epoch": 0.052466124661246616,
      "step": 242,
      "training_loss": 8.081500053405762
    },
    {
      "epoch": 0.052466124661246616,
      "step": 242,
      "training_loss": 7.490175724029541
    },
    {
      "epoch": 0.05268292682926829,
      "step": 243,
      "training_loss": 6.827514171600342
    },
    {
      "epoch": 0.05268292682926829,
      "step": 243,
      "training_loss": 6.40953254699707
    },
    {
      "epoch": 0.05268292682926829,
      "step": 243,
      "training_loss": 6.150390625
    },
    {
      "epoch": 0.05268292682926829,
      "step": 243,
      "training_loss": 7.2240705490112305
    },
    {
      "epoch": 0.05289972899728997,
      "grad_norm": 8.995244979858398,
      "learning_rate": 1e-05,
      "loss": 7.1939,
      "step": 244
    },
    {
      "epoch": 0.05289972899728997,
      "step": 244,
      "training_loss": 9.017387390136719
    },
    {
      "epoch": 0.05289972899728997,
      "step": 244,
      "training_loss": 7.219889163970947
    },
    {
      "epoch": 0.05289972899728997,
      "step": 244,
      "training_loss": 7.482367038726807
    },
    {
      "epoch": 0.05289972899728997,
      "step": 244,
      "training_loss": 7.744876384735107
    },
    {
      "epoch": 0.053116531165311655,
      "step": 245,
      "training_loss": 9.436880111694336
    },
    {
      "epoch": 0.053116531165311655,
      "step": 245,
      "training_loss": 6.923351764678955
    },
    {
      "epoch": 0.053116531165311655,
      "step": 245,
      "training_loss": 6.683416843414307
    },
    {
      "epoch": 0.053116531165311655,
      "step": 245,
      "training_loss": 8.77727222442627
    },
    {
      "epoch": 0.05333333333333334,
      "step": 246,
      "training_loss": 6.751479625701904
    },
    {
      "epoch": 0.05333333333333334,
      "step": 246,
      "training_loss": 6.576840400695801
    },
    {
      "epoch": 0.05333333333333334,
      "step": 246,
      "training_loss": 7.705849647521973
    },
    {
      "epoch": 0.05333333333333334,
      "step": 246,
      "training_loss": 8.029927253723145
    },
    {
      "epoch": 0.05355013550135501,
      "step": 247,
      "training_loss": 6.638267517089844
    },
    {
      "epoch": 0.05355013550135501,
      "step": 247,
      "training_loss": 6.989480018615723
    },
    {
      "epoch": 0.05355013550135501,
      "step": 247,
      "training_loss": 7.312686443328857
    },
    {
      "epoch": 0.05355013550135501,
      "step": 247,
      "training_loss": 8.443486213684082
    },
    {
      "epoch": 0.053766937669376694,
      "grad_norm": 11.638315200805664,
      "learning_rate": 1e-05,
      "loss": 7.6083,
      "step": 248
    },
    {
      "epoch": 0.053766937669376694,
      "step": 248,
      "training_loss": 7.707231044769287
    },
    {
      "epoch": 0.053766937669376694,
      "step": 248,
      "training_loss": 8.948485374450684
    },
    {
      "epoch": 0.053766937669376694,
      "step": 248,
      "training_loss": 7.690384864807129
    },
    {
      "epoch": 0.053766937669376694,
      "step": 248,
      "training_loss": 7.986595153808594
    },
    {
      "epoch": 0.053983739837398376,
      "step": 249,
      "training_loss": 8.61886978149414
    },
    {
      "epoch": 0.053983739837398376,
      "step": 249,
      "training_loss": 7.345028400421143
    },
    {
      "epoch": 0.053983739837398376,
      "step": 249,
      "training_loss": 6.00013542175293
    },
    {
      "epoch": 0.053983739837398376,
      "step": 249,
      "training_loss": 8.420014381408691
    },
    {
      "epoch": 0.05420054200542006,
      "step": 250,
      "training_loss": 7.732717990875244
    },
    {
      "epoch": 0.05420054200542006,
      "step": 250,
      "training_loss": 8.679167747497559
    },
    {
      "epoch": 0.05420054200542006,
      "step": 250,
      "training_loss": 7.288818836212158
    },
    {
      "epoch": 0.05420054200542006,
      "step": 250,
      "training_loss": 7.421443462371826
    },
    {
      "epoch": 0.05441734417344173,
      "step": 251,
      "training_loss": 6.870491981506348
    },
    {
      "epoch": 0.05441734417344173,
      "step": 251,
      "training_loss": 7.203847408294678
    },
    {
      "epoch": 0.05441734417344173,
      "step": 251,
      "training_loss": 6.894846439361572
    },
    {
      "epoch": 0.05441734417344173,
      "step": 251,
      "training_loss": 9.814221382141113
    },
    {
      "epoch": 0.054634146341463415,
      "grad_norm": 10.182159423828125,
      "learning_rate": 1e-05,
      "loss": 7.7889,
      "step": 252
    },
    {
      "epoch": 0.054634146341463415,
      "step": 252,
      "training_loss": 5.032736778259277
    },
    {
      "epoch": 0.054634146341463415,
      "step": 252,
      "training_loss": 6.673186779022217
    },
    {
      "epoch": 0.054634146341463415,
      "step": 252,
      "training_loss": 7.381402969360352
    },
    {
      "epoch": 0.054634146341463415,
      "step": 252,
      "training_loss": 7.337798118591309
    },
    {
      "epoch": 0.0548509485094851,
      "step": 253,
      "training_loss": 8.418834686279297
    },
    {
      "epoch": 0.0548509485094851,
      "step": 253,
      "training_loss": 7.7570085525512695
    },
    {
      "epoch": 0.0548509485094851,
      "step": 253,
      "training_loss": 7.686895847320557
    },
    {
      "epoch": 0.0548509485094851,
      "step": 253,
      "training_loss": 6.722657680511475
    },
    {
      "epoch": 0.05506775067750678,
      "step": 254,
      "training_loss": 7.650125503540039
    },
    {
      "epoch": 0.05506775067750678,
      "step": 254,
      "training_loss": 8.152322769165039
    },
    {
      "epoch": 0.05506775067750678,
      "step": 254,
      "training_loss": 6.329427719116211
    },
    {
      "epoch": 0.05506775067750678,
      "step": 254,
      "training_loss": 8.512348175048828
    },
    {
      "epoch": 0.055284552845528454,
      "step": 255,
      "training_loss": 7.781507968902588
    },
    {
      "epoch": 0.055284552845528454,
      "step": 255,
      "training_loss": 8.325518608093262
    },
    {
      "epoch": 0.055284552845528454,
      "step": 255,
      "training_loss": 7.997587203979492
    },
    {
      "epoch": 0.055284552845528454,
      "step": 255,
      "training_loss": 7.630438804626465
    },
    {
      "epoch": 0.055501355013550135,
      "grad_norm": 12.246054649353027,
      "learning_rate": 1e-05,
      "loss": 7.4619,
      "step": 256
    },
    {
      "epoch": 0.055501355013550135,
      "step": 256,
      "training_loss": 7.2658843994140625
    },
    {
      "epoch": 0.055501355013550135,
      "step": 256,
      "training_loss": 6.518746852874756
    },
    {
      "epoch": 0.055501355013550135,
      "step": 256,
      "training_loss": 8.17115592956543
    },
    {
      "epoch": 0.055501355013550135,
      "step": 256,
      "training_loss": 7.836607456207275
    },
    {
      "epoch": 0.05571815718157182,
      "step": 257,
      "training_loss": 8.451658248901367
    },
    {
      "epoch": 0.05571815718157182,
      "step": 257,
      "training_loss": 8.910909652709961
    },
    {
      "epoch": 0.05571815718157182,
      "step": 257,
      "training_loss": 7.891700744628906
    },
    {
      "epoch": 0.05571815718157182,
      "step": 257,
      "training_loss": 6.446791648864746
    },
    {
      "epoch": 0.0559349593495935,
      "step": 258,
      "training_loss": 7.4651618003845215
    },
    {
      "epoch": 0.0559349593495935,
      "step": 258,
      "training_loss": 7.306513786315918
    },
    {
      "epoch": 0.0559349593495935,
      "step": 258,
      "training_loss": 7.2560505867004395
    },
    {
      "epoch": 0.0559349593495935,
      "step": 258,
      "training_loss": 7.278066635131836
    },
    {
      "epoch": 0.056151761517615174,
      "step": 259,
      "training_loss": 6.152781963348389
    },
    {
      "epoch": 0.056151761517615174,
      "step": 259,
      "training_loss": 6.5592193603515625
    },
    {
      "epoch": 0.056151761517615174,
      "step": 259,
      "training_loss": 6.347538471221924
    },
    {
      "epoch": 0.056151761517615174,
      "step": 259,
      "training_loss": 7.770118236541748
    },
    {
      "epoch": 0.056368563685636856,
      "grad_norm": 9.765273094177246,
      "learning_rate": 1e-05,
      "loss": 7.3518,
      "step": 260
    },
    {
      "epoch": 0.056368563685636856,
      "step": 260,
      "training_loss": 7.208245277404785
    },
    {
      "epoch": 0.056368563685636856,
      "step": 260,
      "training_loss": 6.7911224365234375
    },
    {
      "epoch": 0.056368563685636856,
      "step": 260,
      "training_loss": 8.62703800201416
    },
    {
      "epoch": 0.056368563685636856,
      "step": 260,
      "training_loss": 6.446178913116455
    },
    {
      "epoch": 0.05658536585365854,
      "step": 261,
      "training_loss": 8.51399040222168
    },
    {
      "epoch": 0.05658536585365854,
      "step": 261,
      "training_loss": 6.838447570800781
    },
    {
      "epoch": 0.05658536585365854,
      "step": 261,
      "training_loss": 7.878900527954102
    },
    {
      "epoch": 0.05658536585365854,
      "step": 261,
      "training_loss": 6.135529518127441
    },
    {
      "epoch": 0.05680216802168022,
      "step": 262,
      "training_loss": 6.9341230392456055
    },
    {
      "epoch": 0.05680216802168022,
      "step": 262,
      "training_loss": 8.404580116271973
    },
    {
      "epoch": 0.05680216802168022,
      "step": 262,
      "training_loss": 6.314973831176758
    },
    {
      "epoch": 0.05680216802168022,
      "step": 262,
      "training_loss": 7.638430118560791
    },
    {
      "epoch": 0.057018970189701895,
      "step": 263,
      "training_loss": 9.185648918151855
    },
    {
      "epoch": 0.057018970189701895,
      "step": 263,
      "training_loss": 6.763179779052734
    },
    {
      "epoch": 0.057018970189701895,
      "step": 263,
      "training_loss": 6.9485344886779785
    },
    {
      "epoch": 0.057018970189701895,
      "step": 263,
      "training_loss": 9.215547561645508
    },
    {
      "epoch": 0.05723577235772358,
      "grad_norm": 15.574507713317871,
      "learning_rate": 1e-05,
      "loss": 7.4903,
      "step": 264
    },
    {
      "epoch": 0.05723577235772358,
      "step": 264,
      "training_loss": 8.551088333129883
    },
    {
      "epoch": 0.05723577235772358,
      "step": 264,
      "training_loss": 7.588894367218018
    },
    {
      "epoch": 0.05723577235772358,
      "step": 264,
      "training_loss": 7.4824137687683105
    },
    {
      "epoch": 0.05723577235772358,
      "step": 264,
      "training_loss": 9.056781768798828
    },
    {
      "epoch": 0.05745257452574526,
      "step": 265,
      "training_loss": 7.522091865539551
    },
    {
      "epoch": 0.05745257452574526,
      "step": 265,
      "training_loss": 7.817105293273926
    },
    {
      "epoch": 0.05745257452574526,
      "step": 265,
      "training_loss": 7.278335094451904
    },
    {
      "epoch": 0.05745257452574526,
      "step": 265,
      "training_loss": 7.548736572265625
    },
    {
      "epoch": 0.05766937669376694,
      "step": 266,
      "training_loss": 7.045229434967041
    },
    {
      "epoch": 0.05766937669376694,
      "step": 266,
      "training_loss": 5.815404891967773
    },
    {
      "epoch": 0.05766937669376694,
      "step": 266,
      "training_loss": 6.713987827301025
    },
    {
      "epoch": 0.05766937669376694,
      "step": 266,
      "training_loss": 9.410653114318848
    },
    {
      "epoch": 0.057886178861788616,
      "step": 267,
      "training_loss": 8.059284210205078
    },
    {
      "epoch": 0.057886178861788616,
      "step": 267,
      "training_loss": 8.13495922088623
    },
    {
      "epoch": 0.057886178861788616,
      "step": 267,
      "training_loss": 7.741898536682129
    },
    {
      "epoch": 0.057886178861788616,
      "step": 267,
      "training_loss": 7.717564105987549
    },
    {
      "epoch": 0.0581029810298103,
      "grad_norm": 10.976743698120117,
      "learning_rate": 1e-05,
      "loss": 7.7178,
      "step": 268
    },
    {
      "epoch": 0.0581029810298103,
      "step": 268,
      "training_loss": 7.551599502563477
    },
    {
      "epoch": 0.0581029810298103,
      "step": 268,
      "training_loss": 8.411321640014648
    },
    {
      "epoch": 0.0581029810298103,
      "step": 268,
      "training_loss": 7.502470016479492
    },
    {
      "epoch": 0.0581029810298103,
      "step": 268,
      "training_loss": 7.674718856811523
    },
    {
      "epoch": 0.05831978319783198,
      "step": 269,
      "training_loss": 7.458215236663818
    },
    {
      "epoch": 0.05831978319783198,
      "step": 269,
      "training_loss": 8.759163856506348
    },
    {
      "epoch": 0.05831978319783198,
      "step": 269,
      "training_loss": 8.42300033569336
    },
    {
      "epoch": 0.05831978319783198,
      "step": 269,
      "training_loss": 6.368552207946777
    },
    {
      "epoch": 0.05853658536585366,
      "step": 270,
      "training_loss": 8.22667407989502
    },
    {
      "epoch": 0.05853658536585366,
      "step": 270,
      "training_loss": 6.813636779785156
    },
    {
      "epoch": 0.05853658536585366,
      "step": 270,
      "training_loss": 6.135164737701416
    },
    {
      "epoch": 0.05853658536585366,
      "step": 270,
      "training_loss": 9.148365020751953
    },
    {
      "epoch": 0.05875338753387534,
      "step": 271,
      "training_loss": 8.226652145385742
    },
    {
      "epoch": 0.05875338753387534,
      "step": 271,
      "training_loss": 7.856372833251953
    },
    {
      "epoch": 0.05875338753387534,
      "step": 271,
      "training_loss": 7.694685459136963
    },
    {
      "epoch": 0.05875338753387534,
      "step": 271,
      "training_loss": 8.500811576843262
    },
    {
      "epoch": 0.05897018970189702,
      "grad_norm": 8.287850379943848,
      "learning_rate": 1e-05,
      "loss": 7.797,
      "step": 272
    },
    {
      "epoch": 0.05897018970189702,
      "step": 272,
      "training_loss": 7.402929782867432
    },
    {
      "epoch": 0.05897018970189702,
      "step": 272,
      "training_loss": 7.51662540435791
    },
    {
      "epoch": 0.05897018970189702,
      "step": 272,
      "training_loss": 8.046456336975098
    },
    {
      "epoch": 0.05897018970189702,
      "step": 272,
      "training_loss": 6.997222900390625
    },
    {
      "epoch": 0.0591869918699187,
      "step": 273,
      "training_loss": 8.842316627502441
    },
    {
      "epoch": 0.0591869918699187,
      "step": 273,
      "training_loss": 7.397320747375488
    },
    {
      "epoch": 0.0591869918699187,
      "step": 273,
      "training_loss": 7.941710472106934
    },
    {
      "epoch": 0.0591869918699187,
      "step": 273,
      "training_loss": 6.930459022521973
    },
    {
      "epoch": 0.05940379403794038,
      "step": 274,
      "training_loss": 8.531883239746094
    },
    {
      "epoch": 0.05940379403794038,
      "step": 274,
      "training_loss": 7.62583065032959
    },
    {
      "epoch": 0.05940379403794038,
      "step": 274,
      "training_loss": 7.3276801109313965
    },
    {
      "epoch": 0.05940379403794038,
      "step": 274,
      "training_loss": 7.330495834350586
    },
    {
      "epoch": 0.05962059620596206,
      "step": 275,
      "training_loss": 7.40502405166626
    },
    {
      "epoch": 0.05962059620596206,
      "step": 275,
      "training_loss": 5.925942897796631
    },
    {
      "epoch": 0.05962059620596206,
      "step": 275,
      "training_loss": 7.5161542892456055
    },
    {
      "epoch": 0.05962059620596206,
      "step": 275,
      "training_loss": 6.444459915161133
    },
    {
      "epoch": 0.05983739837398374,
      "grad_norm": 12.212151527404785,
      "learning_rate": 1e-05,
      "loss": 7.4489,
      "step": 276
    },
    {
      "epoch": 0.05983739837398374,
      "step": 276,
      "training_loss": 7.115035057067871
    },
    {
      "epoch": 0.05983739837398374,
      "step": 276,
      "training_loss": 7.696692943572998
    },
    {
      "epoch": 0.05983739837398374,
      "step": 276,
      "training_loss": 6.945154190063477
    },
    {
      "epoch": 0.05983739837398374,
      "step": 276,
      "training_loss": 6.678189754486084
    },
    {
      "epoch": 0.06005420054200542,
      "step": 277,
      "training_loss": 8.031144142150879
    },
    {
      "epoch": 0.06005420054200542,
      "step": 277,
      "training_loss": 5.940830707550049
    },
    {
      "epoch": 0.06005420054200542,
      "step": 277,
      "training_loss": 7.923683166503906
    },
    {
      "epoch": 0.06005420054200542,
      "step": 277,
      "training_loss": 7.568713188171387
    },
    {
      "epoch": 0.060271002710027104,
      "step": 278,
      "training_loss": 7.814483642578125
    },
    {
      "epoch": 0.060271002710027104,
      "step": 278,
      "training_loss": 5.4415154457092285
    },
    {
      "epoch": 0.060271002710027104,
      "step": 278,
      "training_loss": 8.377715110778809
    },
    {
      "epoch": 0.060271002710027104,
      "step": 278,
      "training_loss": 6.937535285949707
    },
    {
      "epoch": 0.06048780487804878,
      "step": 279,
      "training_loss": 6.365692138671875
    },
    {
      "epoch": 0.06048780487804878,
      "step": 279,
      "training_loss": 7.972522258758545
    },
    {
      "epoch": 0.06048780487804878,
      "step": 279,
      "training_loss": 7.187962055206299
    },
    {
      "epoch": 0.06048780487804878,
      "step": 279,
      "training_loss": 6.944251537322998
    },
    {
      "epoch": 0.06070460704607046,
      "grad_norm": 8.441193580627441,
      "learning_rate": 1e-05,
      "loss": 7.1838,
      "step": 280
    },
    {
      "epoch": 0.06070460704607046,
      "step": 280,
      "training_loss": 6.531884670257568
    },
    {
      "epoch": 0.06070460704607046,
      "step": 280,
      "training_loss": 7.821432113647461
    },
    {
      "epoch": 0.06070460704607046,
      "step": 280,
      "training_loss": 7.641506671905518
    },
    {
      "epoch": 0.06070460704607046,
      "step": 280,
      "training_loss": 7.209323883056641
    },
    {
      "epoch": 0.06092140921409214,
      "step": 281,
      "training_loss": 6.58898401260376
    },
    {
      "epoch": 0.06092140921409214,
      "step": 281,
      "training_loss": 6.6504364013671875
    },
    {
      "epoch": 0.06092140921409214,
      "step": 281,
      "training_loss": 7.452279567718506
    },
    {
      "epoch": 0.06092140921409214,
      "step": 281,
      "training_loss": 9.540038108825684
    },
    {
      "epoch": 0.061138211382113825,
      "step": 282,
      "training_loss": 5.545039176940918
    },
    {
      "epoch": 0.061138211382113825,
      "step": 282,
      "training_loss": 7.083472728729248
    },
    {
      "epoch": 0.061138211382113825,
      "step": 282,
      "training_loss": 7.202460765838623
    },
    {
      "epoch": 0.061138211382113825,
      "step": 282,
      "training_loss": 7.488897800445557
    },
    {
      "epoch": 0.0613550135501355,
      "step": 283,
      "training_loss": 6.620791435241699
    },
    {
      "epoch": 0.0613550135501355,
      "step": 283,
      "training_loss": 8.742483139038086
    },
    {
      "epoch": 0.0613550135501355,
      "step": 283,
      "training_loss": 8.043445587158203
    },
    {
      "epoch": 0.0613550135501355,
      "step": 283,
      "training_loss": 8.167091369628906
    },
    {
      "epoch": 0.06157181571815718,
      "grad_norm": 10.43885326385498,
      "learning_rate": 1e-05,
      "loss": 7.3956,
      "step": 284
    },
    {
      "epoch": 0.06157181571815718,
      "step": 284,
      "training_loss": 7.428296089172363
    },
    {
      "epoch": 0.06157181571815718,
      "step": 284,
      "training_loss": 5.713442802429199
    },
    {
      "epoch": 0.06157181571815718,
      "step": 284,
      "training_loss": 6.633415222167969
    },
    {
      "epoch": 0.06157181571815718,
      "step": 284,
      "training_loss": 6.73024845123291
    },
    {
      "epoch": 0.061788617886178863,
      "step": 285,
      "training_loss": 7.768450736999512
    },
    {
      "epoch": 0.061788617886178863,
      "step": 285,
      "training_loss": 6.644658088684082
    },
    {
      "epoch": 0.061788617886178863,
      "step": 285,
      "training_loss": 7.590202808380127
    },
    {
      "epoch": 0.061788617886178863,
      "step": 285,
      "training_loss": 8.54789924621582
    },
    {
      "epoch": 0.062005420054200545,
      "step": 286,
      "training_loss": 8.121613502502441
    },
    {
      "epoch": 0.062005420054200545,
      "step": 286,
      "training_loss": 7.609255313873291
    },
    {
      "epoch": 0.062005420054200545,
      "step": 286,
      "training_loss": 8.010976791381836
    },
    {
      "epoch": 0.062005420054200545,
      "step": 286,
      "training_loss": 8.23397445678711
    },
    {
      "epoch": 0.06222222222222222,
      "step": 287,
      "training_loss": 6.914229393005371
    },
    {
      "epoch": 0.06222222222222222,
      "step": 287,
      "training_loss": 6.324631214141846
    },
    {
      "epoch": 0.06222222222222222,
      "step": 287,
      "training_loss": 8.040966033935547
    },
    {
      "epoch": 0.06222222222222222,
      "step": 287,
      "training_loss": 6.609036922454834
    },
    {
      "epoch": 0.0624390243902439,
      "grad_norm": 9.317283630371094,
      "learning_rate": 1e-05,
      "loss": 7.3076,
      "step": 288
    },
    {
      "epoch": 0.0624390243902439,
      "step": 288,
      "training_loss": 5.167140483856201
    },
    {
      "epoch": 0.0624390243902439,
      "step": 288,
      "training_loss": 7.806210994720459
    },
    {
      "epoch": 0.0624390243902439,
      "step": 288,
      "training_loss": 6.772307395935059
    },
    {
      "epoch": 0.0624390243902439,
      "step": 288,
      "training_loss": 7.444108009338379
    },
    {
      "epoch": 0.06265582655826558,
      "step": 289,
      "training_loss": 6.939181804656982
    },
    {
      "epoch": 0.06265582655826558,
      "step": 289,
      "training_loss": 7.465320587158203
    },
    {
      "epoch": 0.06265582655826558,
      "step": 289,
      "training_loss": 6.801382064819336
    },
    {
      "epoch": 0.06265582655826558,
      "step": 289,
      "training_loss": 6.137953758239746
    },
    {
      "epoch": 0.06287262872628727,
      "step": 290,
      "training_loss": 7.018416881561279
    },
    {
      "epoch": 0.06287262872628727,
      "step": 290,
      "training_loss": 7.238819599151611
    },
    {
      "epoch": 0.06287262872628727,
      "step": 290,
      "training_loss": 9.524394989013672
    },
    {
      "epoch": 0.06287262872628727,
      "step": 290,
      "training_loss": 7.785585880279541
    },
    {
      "epoch": 0.06308943089430895,
      "step": 291,
      "training_loss": 6.349150657653809
    },
    {
      "epoch": 0.06308943089430895,
      "step": 291,
      "training_loss": 7.361972808837891
    },
    {
      "epoch": 0.06308943089430895,
      "step": 291,
      "training_loss": 7.6063456535339355
    },
    {
      "epoch": 0.06308943089430895,
      "step": 291,
      "training_loss": 7.522221565246582
    },
    {
      "epoch": 0.06330623306233063,
      "grad_norm": 5.975465297698975,
      "learning_rate": 1e-05,
      "loss": 7.1838,
      "step": 292
    },
    {
      "epoch": 0.06330623306233063,
      "step": 292,
      "training_loss": 7.509018898010254
    },
    {
      "epoch": 0.06330623306233063,
      "step": 292,
      "training_loss": 7.728145599365234
    },
    {
      "epoch": 0.06330623306233063,
      "step": 292,
      "training_loss": 6.897945880889893
    },
    {
      "epoch": 0.06330623306233063,
      "step": 292,
      "training_loss": 7.814251899719238
    },
    {
      "epoch": 0.0635230352303523,
      "step": 293,
      "training_loss": 7.165831565856934
    },
    {
      "epoch": 0.0635230352303523,
      "step": 293,
      "training_loss": 7.534028053283691
    },
    {
      "epoch": 0.0635230352303523,
      "step": 293,
      "training_loss": 8.005590438842773
    },
    {
      "epoch": 0.0635230352303523,
      "step": 293,
      "training_loss": 7.049530982971191
    },
    {
      "epoch": 0.06373983739837398,
      "step": 294,
      "training_loss": 6.846338748931885
    },
    {
      "epoch": 0.06373983739837398,
      "step": 294,
      "training_loss": 7.0392985343933105
    },
    {
      "epoch": 0.06373983739837398,
      "step": 294,
      "training_loss": 7.271032333374023
    },
    {
      "epoch": 0.06373983739837398,
      "step": 294,
      "training_loss": 7.609474182128906
    },
    {
      "epoch": 0.06395663956639566,
      "step": 295,
      "training_loss": 7.2320709228515625
    },
    {
      "epoch": 0.06395663956639566,
      "step": 295,
      "training_loss": 8.146509170532227
    },
    {
      "epoch": 0.06395663956639566,
      "step": 295,
      "training_loss": 7.763802528381348
    },
    {
      "epoch": 0.06395663956639566,
      "step": 295,
      "training_loss": 8.810722351074219
    },
    {
      "epoch": 0.06417344173441734,
      "grad_norm": 56.03611373901367,
      "learning_rate": 1e-05,
      "loss": 7.5265,
      "step": 296
    },
    {
      "epoch": 0.06417344173441734,
      "step": 296,
      "training_loss": 6.436797142028809
    },
    {
      "epoch": 0.06417344173441734,
      "step": 296,
      "training_loss": 7.899248123168945
    },
    {
      "epoch": 0.06417344173441734,
      "step": 296,
      "training_loss": 6.316470623016357
    },
    {
      "epoch": 0.06417344173441734,
      "step": 296,
      "training_loss": 7.032528400421143
    },
    {
      "epoch": 0.06439024390243903,
      "step": 297,
      "training_loss": 7.717388153076172
    },
    {
      "epoch": 0.06439024390243903,
      "step": 297,
      "training_loss": 7.552374362945557
    },
    {
      "epoch": 0.06439024390243903,
      "step": 297,
      "training_loss": 7.801682472229004
    },
    {
      "epoch": 0.06439024390243903,
      "step": 297,
      "training_loss": 7.795514106750488
    },
    {
      "epoch": 0.06460704607046071,
      "step": 298,
      "training_loss": 6.031283855438232
    },
    {
      "epoch": 0.06460704607046071,
      "step": 298,
      "training_loss": 8.294975280761719
    },
    {
      "epoch": 0.06460704607046071,
      "step": 298,
      "training_loss": 7.60531759262085
    },
    {
      "epoch": 0.06460704607046071,
      "step": 298,
      "training_loss": 5.892339706420898
    },
    {
      "epoch": 0.06482384823848239,
      "step": 299,
      "training_loss": 8.084742546081543
    },
    {
      "epoch": 0.06482384823848239,
      "step": 299,
      "training_loss": 8.01467514038086
    },
    {
      "epoch": 0.06482384823848239,
      "step": 299,
      "training_loss": 8.711590766906738
    },
    {
      "epoch": 0.06482384823848239,
      "step": 299,
      "training_loss": 7.159682273864746
    },
    {
      "epoch": 0.06504065040650407,
      "grad_norm": 9.543771743774414,
      "learning_rate": 1e-05,
      "loss": 7.3967,
      "step": 300
    },
    {
      "epoch": 0.06504065040650407,
      "step": 300,
      "training_loss": 7.839555263519287
    },
    {
      "epoch": 0.06504065040650407,
      "step": 300,
      "training_loss": 7.038215160369873
    },
    {
      "epoch": 0.06504065040650407,
      "step": 300,
      "training_loss": 7.712975978851318
    },
    {
      "epoch": 0.06504065040650407,
      "step": 300,
      "training_loss": 6.8494367599487305
    },
    {
      "epoch": 0.06525745257452574,
      "step": 301,
      "training_loss": 6.865564346313477
    },
    {
      "epoch": 0.06525745257452574,
      "step": 301,
      "training_loss": 6.867547035217285
    },
    {
      "epoch": 0.06525745257452574,
      "step": 301,
      "training_loss": 7.404945373535156
    },
    {
      "epoch": 0.06525745257452574,
      "step": 301,
      "training_loss": 6.061622142791748
    },
    {
      "epoch": 0.06547425474254742,
      "step": 302,
      "training_loss": 8.142127990722656
    },
    {
      "epoch": 0.06547425474254742,
      "step": 302,
      "training_loss": 7.771823406219482
    },
    {
      "epoch": 0.06547425474254742,
      "step": 302,
      "training_loss": 7.816885471343994
    },
    {
      "epoch": 0.06547425474254742,
      "step": 302,
      "training_loss": 5.942605495452881
    },
    {
      "epoch": 0.0656910569105691,
      "step": 303,
      "training_loss": 6.965455055236816
    },
    {
      "epoch": 0.0656910569105691,
      "step": 303,
      "training_loss": 7.227146148681641
    },
    {
      "epoch": 0.0656910569105691,
      "step": 303,
      "training_loss": 6.795644283294678
    },
    {
      "epoch": 0.0656910569105691,
      "step": 303,
      "training_loss": 7.092919826507568
    },
    {
      "epoch": 0.06590785907859079,
      "grad_norm": 10.276991844177246,
      "learning_rate": 1e-05,
      "loss": 7.1497,
      "step": 304
    },
    {
      "epoch": 0.06590785907859079,
      "step": 304,
      "training_loss": 8.083745002746582
    },
    {
      "epoch": 0.06590785907859079,
      "step": 304,
      "training_loss": 7.2875237464904785
    },
    {
      "epoch": 0.06590785907859079,
      "step": 304,
      "training_loss": 7.603734016418457
    },
    {
      "epoch": 0.06590785907859079,
      "step": 304,
      "training_loss": 7.432066440582275
    },
    {
      "epoch": 0.06612466124661247,
      "step": 305,
      "training_loss": 7.1283650398254395
    },
    {
      "epoch": 0.06612466124661247,
      "step": 305,
      "training_loss": 7.08873176574707
    },
    {
      "epoch": 0.06612466124661247,
      "step": 305,
      "training_loss": 7.871223449707031
    },
    {
      "epoch": 0.06612466124661247,
      "step": 305,
      "training_loss": 6.223638534545898
    },
    {
      "epoch": 0.06634146341463415,
      "step": 306,
      "training_loss": 6.883688926696777
    },
    {
      "epoch": 0.06634146341463415,
      "step": 306,
      "training_loss": 6.845622539520264
    },
    {
      "epoch": 0.06634146341463415,
      "step": 306,
      "training_loss": 7.874059200286865
    },
    {
      "epoch": 0.06634146341463415,
      "step": 306,
      "training_loss": 7.876143932342529
    },
    {
      "epoch": 0.06655826558265583,
      "step": 307,
      "training_loss": 8.262666702270508
    },
    {
      "epoch": 0.06655826558265583,
      "step": 307,
      "training_loss": 8.113189697265625
    },
    {
      "epoch": 0.06655826558265583,
      "step": 307,
      "training_loss": 7.269906997680664
    },
    {
      "epoch": 0.06655826558265583,
      "step": 307,
      "training_loss": 6.125573635101318
    },
    {
      "epoch": 0.06677506775067751,
      "grad_norm": 8.355751037597656,
      "learning_rate": 1e-05,
      "loss": 7.3731,
      "step": 308
    },
    {
      "epoch": 0.06677506775067751,
      "step": 308,
      "training_loss": 8.863662719726562
    },
    {
      "epoch": 0.06677506775067751,
      "step": 308,
      "training_loss": 7.726521015167236
    },
    {
      "epoch": 0.06677506775067751,
      "step": 308,
      "training_loss": 7.0894365310668945
    },
    {
      "epoch": 0.06677506775067751,
      "step": 308,
      "training_loss": 7.735821723937988
    },
    {
      "epoch": 0.06699186991869918,
      "step": 309,
      "training_loss": 8.143513679504395
    },
    {
      "epoch": 0.06699186991869918,
      "step": 309,
      "training_loss": 8.029550552368164
    },
    {
      "epoch": 0.06699186991869918,
      "step": 309,
      "training_loss": 7.439038276672363
    },
    {
      "epoch": 0.06699186991869918,
      "step": 309,
      "training_loss": 7.7243170738220215
    },
    {
      "epoch": 0.06720867208672086,
      "step": 310,
      "training_loss": 8.088890075683594
    },
    {
      "epoch": 0.06720867208672086,
      "step": 310,
      "training_loss": 7.7645392417907715
    },
    {
      "epoch": 0.06720867208672086,
      "step": 310,
      "training_loss": 8.255669593811035
    },
    {
      "epoch": 0.06720867208672086,
      "step": 310,
      "training_loss": 7.835088729858398
    },
    {
      "epoch": 0.06742547425474255,
      "step": 311,
      "training_loss": 6.575963020324707
    },
    {
      "epoch": 0.06742547425474255,
      "step": 311,
      "training_loss": 6.178025722503662
    },
    {
      "epoch": 0.06742547425474255,
      "step": 311,
      "training_loss": 7.395183086395264
    },
    {
      "epoch": 0.06742547425474255,
      "step": 311,
      "training_loss": 6.829033851623535
    },
    {
      "epoch": 0.06764227642276423,
      "grad_norm": 11.962940216064453,
      "learning_rate": 1e-05,
      "loss": 7.6046,
      "step": 312
    },
    {
      "epoch": 0.06764227642276423,
      "step": 312,
      "training_loss": 7.338956832885742
    },
    {
      "epoch": 0.06764227642276423,
      "step": 312,
      "training_loss": 7.377199649810791
    },
    {
      "epoch": 0.06764227642276423,
      "step": 312,
      "training_loss": 6.880629539489746
    },
    {
      "epoch": 0.06764227642276423,
      "step": 312,
      "training_loss": 6.415348529815674
    },
    {
      "epoch": 0.06785907859078591,
      "step": 313,
      "training_loss": 7.275302410125732
    },
    {
      "epoch": 0.06785907859078591,
      "step": 313,
      "training_loss": 7.527695178985596
    },
    {
      "epoch": 0.06785907859078591,
      "step": 313,
      "training_loss": 7.971768856048584
    },
    {
      "epoch": 0.06785907859078591,
      "step": 313,
      "training_loss": 7.390110015869141
    },
    {
      "epoch": 0.06807588075880759,
      "step": 314,
      "training_loss": 7.80546760559082
    },
    {
      "epoch": 0.06807588075880759,
      "step": 314,
      "training_loss": 6.31403923034668
    },
    {
      "epoch": 0.06807588075880759,
      "step": 314,
      "training_loss": 7.4315996170043945
    },
    {
      "epoch": 0.06807588075880759,
      "step": 314,
      "training_loss": 6.5695481300354
    },
    {
      "epoch": 0.06829268292682927,
      "step": 315,
      "training_loss": 7.272418975830078
    },
    {
      "epoch": 0.06829268292682927,
      "step": 315,
      "training_loss": 6.970995903015137
    },
    {
      "epoch": 0.06829268292682927,
      "step": 315,
      "training_loss": 9.198315620422363
    },
    {
      "epoch": 0.06829268292682927,
      "step": 315,
      "training_loss": 6.978814601898193
    },
    {
      "epoch": 0.06850948509485096,
      "grad_norm": 9.559276580810547,
      "learning_rate": 1e-05,
      "loss": 7.2949,
      "step": 316
    },
    {
      "epoch": 0.06850948509485096,
      "step": 316,
      "training_loss": 6.927078723907471
    },
    {
      "epoch": 0.06850948509485096,
      "step": 316,
      "training_loss": 5.927759170532227
    },
    {
      "epoch": 0.06850948509485096,
      "step": 316,
      "training_loss": 8.120406150817871
    },
    {
      "epoch": 0.06850948509485096,
      "step": 316,
      "training_loss": 7.1461591720581055
    },
    {
      "epoch": 0.06872628726287262,
      "step": 317,
      "training_loss": 6.9077935218811035
    },
    {
      "epoch": 0.06872628726287262,
      "step": 317,
      "training_loss": 7.032877445220947
    },
    {
      "epoch": 0.06872628726287262,
      "step": 317,
      "training_loss": 8.482177734375
    },
    {
      "epoch": 0.06872628726287262,
      "step": 317,
      "training_loss": 7.727051734924316
    },
    {
      "epoch": 0.0689430894308943,
      "step": 318,
      "training_loss": 7.700786590576172
    },
    {
      "epoch": 0.0689430894308943,
      "step": 318,
      "training_loss": 7.493459224700928
    },
    {
      "epoch": 0.0689430894308943,
      "step": 318,
      "training_loss": 6.400937080383301
    },
    {
      "epoch": 0.0689430894308943,
      "step": 318,
      "training_loss": 8.13266658782959
    },
    {
      "epoch": 0.06915989159891599,
      "step": 319,
      "training_loss": 6.807592391967773
    },
    {
      "epoch": 0.06915989159891599,
      "step": 319,
      "training_loss": 8.367398262023926
    },
    {
      "epoch": 0.06915989159891599,
      "step": 319,
      "training_loss": 7.8605475425720215
    },
    {
      "epoch": 0.06915989159891599,
      "step": 319,
      "training_loss": 9.381656646728516
    },
    {
      "epoch": 0.06937669376693767,
      "grad_norm": 10.704842567443848,
      "learning_rate": 1e-05,
      "loss": 7.526,
      "step": 320
    },
    {
      "epoch": 0.06937669376693767,
      "step": 320,
      "training_loss": 6.277553558349609
    },
    {
      "epoch": 0.06937669376693767,
      "step": 320,
      "training_loss": 6.829296588897705
    },
    {
      "epoch": 0.06937669376693767,
      "step": 320,
      "training_loss": 8.379470825195312
    },
    {
      "epoch": 0.06937669376693767,
      "step": 320,
      "training_loss": 6.441191673278809
    },
    {
      "epoch": 0.06959349593495935,
      "step": 321,
      "training_loss": 7.261468887329102
    },
    {
      "epoch": 0.06959349593495935,
      "step": 321,
      "training_loss": 8.130764961242676
    },
    {
      "epoch": 0.06959349593495935,
      "step": 321,
      "training_loss": 6.823978900909424
    },
    {
      "epoch": 0.06959349593495935,
      "step": 321,
      "training_loss": 8.112521171569824
    },
    {
      "epoch": 0.06981029810298103,
      "step": 322,
      "training_loss": 6.674448013305664
    },
    {
      "epoch": 0.06981029810298103,
      "step": 322,
      "training_loss": 7.484942436218262
    },
    {
      "epoch": 0.06981029810298103,
      "step": 322,
      "training_loss": 6.096072196960449
    },
    {
      "epoch": 0.06981029810298103,
      "step": 322,
      "training_loss": 7.587853908538818
    },
    {
      "epoch": 0.07002710027100272,
      "step": 323,
      "training_loss": 7.464957237243652
    },
    {
      "epoch": 0.07002710027100272,
      "step": 323,
      "training_loss": 7.148139476776123
    },
    {
      "epoch": 0.07002710027100272,
      "step": 323,
      "training_loss": 6.566998481750488
    },
    {
      "epoch": 0.07002710027100272,
      "step": 323,
      "training_loss": 7.631389617919922
    },
    {
      "epoch": 0.0702439024390244,
      "grad_norm": 7.544561862945557,
      "learning_rate": 1e-05,
      "loss": 7.1819,
      "step": 324
    },
    {
      "epoch": 0.0702439024390244,
      "step": 324,
      "training_loss": 7.933328151702881
    },
    {
      "epoch": 0.0702439024390244,
      "step": 324,
      "training_loss": 5.647134304046631
    },
    {
      "epoch": 0.0702439024390244,
      "step": 324,
      "training_loss": 7.597581386566162
    },
    {
      "epoch": 0.0702439024390244,
      "step": 324,
      "training_loss": 7.622373104095459
    },
    {
      "epoch": 0.07046070460704607,
      "step": 325,
      "training_loss": 5.748319149017334
    },
    {
      "epoch": 0.07046070460704607,
      "step": 325,
      "training_loss": 7.965821266174316
    },
    {
      "epoch": 0.07046070460704607,
      "step": 325,
      "training_loss": 8.154885292053223
    },
    {
      "epoch": 0.07046070460704607,
      "step": 325,
      "training_loss": 6.284898281097412
    },
    {
      "epoch": 0.07067750677506775,
      "step": 326,
      "training_loss": 6.996857643127441
    },
    {
      "epoch": 0.07067750677506775,
      "step": 326,
      "training_loss": 6.723649024963379
    },
    {
      "epoch": 0.07067750677506775,
      "step": 326,
      "training_loss": 7.957627773284912
    },
    {
      "epoch": 0.07067750677506775,
      "step": 326,
      "training_loss": 7.523959159851074
    },
    {
      "epoch": 0.07089430894308943,
      "step": 327,
      "training_loss": 6.0324578285217285
    },
    {
      "epoch": 0.07089430894308943,
      "step": 327,
      "training_loss": 7.94990348815918
    },
    {
      "epoch": 0.07089430894308943,
      "step": 327,
      "training_loss": 8.056228637695312
    },
    {
      "epoch": 0.07089430894308943,
      "step": 327,
      "training_loss": 6.650707721710205
    },
    {
      "epoch": 0.07111111111111111,
      "grad_norm": 7.279305934906006,
      "learning_rate": 1e-05,
      "loss": 7.1779,
      "step": 328
    },
    {
      "epoch": 0.07111111111111111,
      "step": 328,
      "training_loss": 6.465012073516846
    },
    {
      "epoch": 0.07111111111111111,
      "step": 328,
      "training_loss": 5.709517478942871
    },
    {
      "epoch": 0.07111111111111111,
      "step": 328,
      "training_loss": 7.919474124908447
    },
    {
      "epoch": 0.07111111111111111,
      "step": 328,
      "training_loss": 7.9756550788879395
    },
    {
      "epoch": 0.07132791327913279,
      "step": 329,
      "training_loss": 6.646673679351807
    },
    {
      "epoch": 0.07132791327913279,
      "step": 329,
      "training_loss": 5.979430675506592
    },
    {
      "epoch": 0.07132791327913279,
      "step": 329,
      "training_loss": 7.25238037109375
    },
    {
      "epoch": 0.07132791327913279,
      "step": 329,
      "training_loss": 8.945930480957031
    },
    {
      "epoch": 0.07154471544715447,
      "step": 330,
      "training_loss": 8.81856918334961
    },
    {
      "epoch": 0.07154471544715447,
      "step": 330,
      "training_loss": 7.6974968910217285
    },
    {
      "epoch": 0.07154471544715447,
      "step": 330,
      "training_loss": 7.587303638458252
    },
    {
      "epoch": 0.07154471544715447,
      "step": 330,
      "training_loss": 6.504336357116699
    },
    {
      "epoch": 0.07176151761517616,
      "step": 331,
      "training_loss": 6.5138325691223145
    },
    {
      "epoch": 0.07176151761517616,
      "step": 331,
      "training_loss": 6.965378284454346
    },
    {
      "epoch": 0.07176151761517616,
      "step": 331,
      "training_loss": 7.95586633682251
    },
    {
      "epoch": 0.07176151761517616,
      "step": 331,
      "training_loss": 6.549798965454102
    },
    {
      "epoch": 0.07197831978319784,
      "grad_norm": 11.076000213623047,
      "learning_rate": 1e-05,
      "loss": 7.2179,
      "step": 332
    },
    {
      "epoch": 0.07197831978319784,
      "step": 332,
      "training_loss": 8.027012825012207
    },
    {
      "epoch": 0.07197831978319784,
      "step": 332,
      "training_loss": 7.600356101989746
    },
    {
      "epoch": 0.07197831978319784,
      "step": 332,
      "training_loss": 6.849877834320068
    },
    {
      "epoch": 0.07197831978319784,
      "step": 332,
      "training_loss": 6.829782485961914
    },
    {
      "epoch": 0.0721951219512195,
      "step": 333,
      "training_loss": 7.0495100021362305
    },
    {
      "epoch": 0.0721951219512195,
      "step": 333,
      "training_loss": 7.784453868865967
    },
    {
      "epoch": 0.0721951219512195,
      "step": 333,
      "training_loss": 7.907826900482178
    },
    {
      "epoch": 0.0721951219512195,
      "step": 333,
      "training_loss": 8.009651184082031
    },
    {
      "epoch": 0.07241192411924119,
      "step": 334,
      "training_loss": 7.184807777404785
    },
    {
      "epoch": 0.07241192411924119,
      "step": 334,
      "training_loss": 6.359482288360596
    },
    {
      "epoch": 0.07241192411924119,
      "step": 334,
      "training_loss": 7.404332160949707
    },
    {
      "epoch": 0.07241192411924119,
      "step": 334,
      "training_loss": 7.633327007293701
    },
    {
      "epoch": 0.07262872628726287,
      "step": 335,
      "training_loss": 6.86152458190918
    },
    {
      "epoch": 0.07262872628726287,
      "step": 335,
      "training_loss": 6.538652420043945
    },
    {
      "epoch": 0.07262872628726287,
      "step": 335,
      "training_loss": 7.140222549438477
    },
    {
      "epoch": 0.07262872628726287,
      "step": 335,
      "training_loss": 7.401059627532959
    },
    {
      "epoch": 0.07284552845528455,
      "grad_norm": 7.512730121612549,
      "learning_rate": 1e-05,
      "loss": 7.2864,
      "step": 336
    },
    {
      "epoch": 0.07284552845528455,
      "step": 336,
      "training_loss": 8.223283767700195
    },
    {
      "epoch": 0.07284552845528455,
      "step": 336,
      "training_loss": 6.235193729400635
    },
    {
      "epoch": 0.07284552845528455,
      "step": 336,
      "training_loss": 6.165507793426514
    },
    {
      "epoch": 0.07284552845528455,
      "step": 336,
      "training_loss": 7.752190589904785
    },
    {
      "epoch": 0.07306233062330623,
      "step": 337,
      "training_loss": 6.1988067626953125
    },
    {
      "epoch": 0.07306233062330623,
      "step": 337,
      "training_loss": 7.652230262756348
    },
    {
      "epoch": 0.07306233062330623,
      "step": 337,
      "training_loss": 6.717169284820557
    },
    {
      "epoch": 0.07306233062330623,
      "step": 337,
      "training_loss": 4.879029750823975
    },
    {
      "epoch": 0.07327913279132792,
      "step": 338,
      "training_loss": 8.237714767456055
    },
    {
      "epoch": 0.07327913279132792,
      "step": 338,
      "training_loss": 7.51923942565918
    },
    {
      "epoch": 0.07327913279132792,
      "step": 338,
      "training_loss": 7.831136226654053
    },
    {
      "epoch": 0.07327913279132792,
      "step": 338,
      "training_loss": 7.573692321777344
    },
    {
      "epoch": 0.0734959349593496,
      "step": 339,
      "training_loss": 6.103276252746582
    },
    {
      "epoch": 0.0734959349593496,
      "step": 339,
      "training_loss": 7.979603290557861
    },
    {
      "epoch": 0.0734959349593496,
      "step": 339,
      "training_loss": 7.362600803375244
    },
    {
      "epoch": 0.0734959349593496,
      "step": 339,
      "training_loss": 7.023906707763672
    },
    {
      "epoch": 0.07371273712737128,
      "grad_norm": 9.330965995788574,
      "learning_rate": 1e-05,
      "loss": 7.0909,
      "step": 340
    },
    {
      "epoch": 0.07371273712737128,
      "step": 340,
      "training_loss": 7.566070079803467
    },
    {
      "epoch": 0.07371273712737128,
      "step": 340,
      "training_loss": 6.062989711761475
    },
    {
      "epoch": 0.07371273712737128,
      "step": 340,
      "training_loss": 6.768592834472656
    },
    {
      "epoch": 0.07371273712737128,
      "step": 340,
      "training_loss": 7.505809783935547
    },
    {
      "epoch": 0.07392953929539295,
      "step": 341,
      "training_loss": 6.150308609008789
    },
    {
      "epoch": 0.07392953929539295,
      "step": 341,
      "training_loss": 6.691350936889648
    },
    {
      "epoch": 0.07392953929539295,
      "step": 341,
      "training_loss": 7.4589524269104
    },
    {
      "epoch": 0.07392953929539295,
      "step": 341,
      "training_loss": 7.936442852020264
    },
    {
      "epoch": 0.07414634146341463,
      "step": 342,
      "training_loss": 6.731500625610352
    },
    {
      "epoch": 0.07414634146341463,
      "step": 342,
      "training_loss": 5.757558822631836
    },
    {
      "epoch": 0.07414634146341463,
      "step": 342,
      "training_loss": 8.540909767150879
    },
    {
      "epoch": 0.07414634146341463,
      "step": 342,
      "training_loss": 7.036434173583984
    },
    {
      "epoch": 0.07436314363143631,
      "step": 343,
      "training_loss": 6.663383960723877
    },
    {
      "epoch": 0.07436314363143631,
      "step": 343,
      "training_loss": 8.366472244262695
    },
    {
      "epoch": 0.07436314363143631,
      "step": 343,
      "training_loss": 6.218027114868164
    },
    {
      "epoch": 0.07436314363143631,
      "step": 343,
      "training_loss": 8.335893630981445
    },
    {
      "epoch": 0.074579945799458,
      "grad_norm": 16.463542938232422,
      "learning_rate": 1e-05,
      "loss": 7.1119,
      "step": 344
    },
    {
      "epoch": 0.074579945799458,
      "step": 344,
      "training_loss": 7.858180046081543
    },
    {
      "epoch": 0.074579945799458,
      "step": 344,
      "training_loss": 7.273468494415283
    },
    {
      "epoch": 0.074579945799458,
      "step": 344,
      "training_loss": 6.125883102416992
    },
    {
      "epoch": 0.074579945799458,
      "step": 344,
      "training_loss": 7.250621795654297
    },
    {
      "epoch": 0.07479674796747968,
      "step": 345,
      "training_loss": 7.118679523468018
    },
    {
      "epoch": 0.07479674796747968,
      "step": 345,
      "training_loss": 7.186753273010254
    },
    {
      "epoch": 0.07479674796747968,
      "step": 345,
      "training_loss": 7.353147983551025
    },
    {
      "epoch": 0.07479674796747968,
      "step": 345,
      "training_loss": 7.426127910614014
    },
    {
      "epoch": 0.07501355013550136,
      "step": 346,
      "training_loss": 6.800285339355469
    },
    {
      "epoch": 0.07501355013550136,
      "step": 346,
      "training_loss": 7.593306064605713
    },
    {
      "epoch": 0.07501355013550136,
      "step": 346,
      "training_loss": 8.310588836669922
    },
    {
      "epoch": 0.07501355013550136,
      "step": 346,
      "training_loss": 6.610690116882324
    },
    {
      "epoch": 0.07523035230352304,
      "step": 347,
      "training_loss": 6.360987663269043
    },
    {
      "epoch": 0.07523035230352304,
      "step": 347,
      "training_loss": 6.869555950164795
    },
    {
      "epoch": 0.07523035230352304,
      "step": 347,
      "training_loss": 7.66251277923584
    },
    {
      "epoch": 0.07523035230352304,
      "step": 347,
      "training_loss": 6.518960475921631
    },
    {
      "epoch": 0.07544715447154472,
      "grad_norm": 8.343344688415527,
      "learning_rate": 1e-05,
      "loss": 7.145,
      "step": 348
    },
    {
      "epoch": 0.07544715447154472,
      "step": 348,
      "training_loss": 9.585491180419922
    },
    {
      "epoch": 0.07544715447154472,
      "step": 348,
      "training_loss": 7.547425746917725
    },
    {
      "epoch": 0.07544715447154472,
      "step": 348,
      "training_loss": 7.4212565422058105
    },
    {
      "epoch": 0.07544715447154472,
      "step": 348,
      "training_loss": 7.38618278503418
    },
    {
      "epoch": 0.07566395663956639,
      "step": 349,
      "training_loss": 6.765326499938965
    },
    {
      "epoch": 0.07566395663956639,
      "step": 349,
      "training_loss": 7.491415977478027
    },
    {
      "epoch": 0.07566395663956639,
      "step": 349,
      "training_loss": 7.408094882965088
    },
    {
      "epoch": 0.07566395663956639,
      "step": 349,
      "training_loss": 7.827916622161865
    },
    {
      "epoch": 0.07588075880758807,
      "step": 350,
      "training_loss": 8.016393661499023
    },
    {
      "epoch": 0.07588075880758807,
      "step": 350,
      "training_loss": 7.03807258605957
    },
    {
      "epoch": 0.07588075880758807,
      "step": 350,
      "training_loss": 7.91988468170166
    },
    {
      "epoch": 0.07588075880758807,
      "step": 350,
      "training_loss": 5.300983905792236
    },
    {
      "epoch": 0.07609756097560975,
      "step": 351,
      "training_loss": 7.618810176849365
    },
    {
      "epoch": 0.07609756097560975,
      "step": 351,
      "training_loss": 8.14640998840332
    },
    {
      "epoch": 0.07609756097560975,
      "step": 351,
      "training_loss": 7.794924259185791
    },
    {
      "epoch": 0.07609756097560975,
      "step": 351,
      "training_loss": 6.412093639373779
    },
    {
      "epoch": 0.07631436314363144,
      "grad_norm": 9.882720947265625,
      "learning_rate": 1e-05,
      "loss": 7.48,
      "step": 352
    },
    {
      "epoch": 0.07631436314363144,
      "step": 352,
      "training_loss": 7.178801536560059
    },
    {
      "epoch": 0.07631436314363144,
      "step": 352,
      "training_loss": 9.023365020751953
    },
    {
      "epoch": 0.07631436314363144,
      "step": 352,
      "training_loss": 8.069089889526367
    },
    {
      "epoch": 0.07631436314363144,
      "step": 352,
      "training_loss": 6.252120494842529
    },
    {
      "epoch": 0.07653116531165312,
      "step": 353,
      "training_loss": 5.3750481605529785
    },
    {
      "epoch": 0.07653116531165312,
      "step": 353,
      "training_loss": 7.642358779907227
    },
    {
      "epoch": 0.07653116531165312,
      "step": 353,
      "training_loss": 6.456449508666992
    },
    {
      "epoch": 0.07653116531165312,
      "step": 353,
      "training_loss": 6.934029579162598
    },
    {
      "epoch": 0.0767479674796748,
      "step": 354,
      "training_loss": 7.456325054168701
    },
    {
      "epoch": 0.0767479674796748,
      "step": 354,
      "training_loss": 7.57826042175293
    },
    {
      "epoch": 0.0767479674796748,
      "step": 354,
      "training_loss": 6.6568803787231445
    },
    {
      "epoch": 0.0767479674796748,
      "step": 354,
      "training_loss": 7.363404750823975
    },
    {
      "epoch": 0.07696476964769648,
      "step": 355,
      "training_loss": 7.305367946624756
    },
    {
      "epoch": 0.07696476964769648,
      "step": 355,
      "training_loss": 8.12037181854248
    },
    {
      "epoch": 0.07696476964769648,
      "step": 355,
      "training_loss": 7.485754489898682
    },
    {
      "epoch": 0.07696476964769648,
      "step": 355,
      "training_loss": 7.718730449676514
    },
    {
      "epoch": 0.07718157181571816,
      "grad_norm": 8.002769470214844,
      "learning_rate": 1e-05,
      "loss": 7.2885,
      "step": 356
    },
    {
      "epoch": 0.07718157181571816,
      "step": 356,
      "training_loss": 6.903692722320557
    },
    {
      "epoch": 0.07718157181571816,
      "step": 356,
      "training_loss": 8.136487007141113
    },
    {
      "epoch": 0.07718157181571816,
      "step": 356,
      "training_loss": 5.385498523712158
    },
    {
      "epoch": 0.07718157181571816,
      "step": 356,
      "training_loss": 7.619228839874268
    },
    {
      "epoch": 0.07739837398373983,
      "step": 357,
      "training_loss": 7.989485740661621
    },
    {
      "epoch": 0.07739837398373983,
      "step": 357,
      "training_loss": 5.884688377380371
    },
    {
      "epoch": 0.07739837398373983,
      "step": 357,
      "training_loss": 6.539104461669922
    },
    {
      "epoch": 0.07739837398373983,
      "step": 357,
      "training_loss": 6.643735408782959
    },
    {
      "epoch": 0.07761517615176151,
      "step": 358,
      "training_loss": 7.402616500854492
    },
    {
      "epoch": 0.07761517615176151,
      "step": 358,
      "training_loss": 6.765473365783691
    },
    {
      "epoch": 0.07761517615176151,
      "step": 358,
      "training_loss": 6.543114185333252
    },
    {
      "epoch": 0.07761517615176151,
      "step": 358,
      "training_loss": 6.534614086151123
    },
    {
      "epoch": 0.0778319783197832,
      "step": 359,
      "training_loss": 7.911755084991455
    },
    {
      "epoch": 0.0778319783197832,
      "step": 359,
      "training_loss": 7.505893707275391
    },
    {
      "epoch": 0.0778319783197832,
      "step": 359,
      "training_loss": 5.764211177825928
    },
    {
      "epoch": 0.0778319783197832,
      "step": 359,
      "training_loss": 7.297797679901123
    },
    {
      "epoch": 0.07804878048780488,
      "grad_norm": 9.449657440185547,
      "learning_rate": 1e-05,
      "loss": 6.9267,
      "step": 360
    },
    {
      "epoch": 0.07804878048780488,
      "step": 360,
      "training_loss": 7.558891296386719
    },
    {
      "epoch": 0.07804878048780488,
      "step": 360,
      "training_loss": 7.270266056060791
    },
    {
      "epoch": 0.07804878048780488,
      "step": 360,
      "training_loss": 7.123398303985596
    },
    {
      "epoch": 0.07804878048780488,
      "step": 360,
      "training_loss": 7.304138660430908
    },
    {
      "epoch": 0.07826558265582656,
      "step": 361,
      "training_loss": 6.845207691192627
    },
    {
      "epoch": 0.07826558265582656,
      "step": 361,
      "training_loss": 7.288515090942383
    },
    {
      "epoch": 0.07826558265582656,
      "step": 361,
      "training_loss": 6.759293079376221
    },
    {
      "epoch": 0.07826558265582656,
      "step": 361,
      "training_loss": 7.592654705047607
    },
    {
      "epoch": 0.07848238482384824,
      "step": 362,
      "training_loss": 7.403152942657471
    },
    {
      "epoch": 0.07848238482384824,
      "step": 362,
      "training_loss": 9.003768920898438
    },
    {
      "epoch": 0.07848238482384824,
      "step": 362,
      "training_loss": 6.995492458343506
    },
    {
      "epoch": 0.07848238482384824,
      "step": 362,
      "training_loss": 7.953708171844482
    },
    {
      "epoch": 0.07869918699186992,
      "step": 363,
      "training_loss": 7.178027153015137
    },
    {
      "epoch": 0.07869918699186992,
      "step": 363,
      "training_loss": 7.987707614898682
    },
    {
      "epoch": 0.07869918699186992,
      "step": 363,
      "training_loss": 7.585031032562256
    },
    {
      "epoch": 0.07869918699186992,
      "step": 363,
      "training_loss": 5.466658115386963
    },
    {
      "epoch": 0.0789159891598916,
      "grad_norm": 10.402522087097168,
      "learning_rate": 1e-05,
      "loss": 7.3322,
      "step": 364
    },
    {
      "epoch": 0.0789159891598916,
      "step": 364,
      "training_loss": 7.02157735824585
    },
    {
      "epoch": 0.0789159891598916,
      "step": 364,
      "training_loss": 7.223348140716553
    },
    {
      "epoch": 0.0789159891598916,
      "step": 364,
      "training_loss": 7.370894908905029
    },
    {
      "epoch": 0.0789159891598916,
      "step": 364,
      "training_loss": 6.016990661621094
    },
    {
      "epoch": 0.07913279132791327,
      "step": 365,
      "training_loss": 8.110618591308594
    },
    {
      "epoch": 0.07913279132791327,
      "step": 365,
      "training_loss": 6.330266952514648
    },
    {
      "epoch": 0.07913279132791327,
      "step": 365,
      "training_loss": 7.559991836547852
    },
    {
      "epoch": 0.07913279132791327,
      "step": 365,
      "training_loss": 7.8614068031311035
    },
    {
      "epoch": 0.07934959349593496,
      "step": 366,
      "training_loss": 7.617776393890381
    },
    {
      "epoch": 0.07934959349593496,
      "step": 366,
      "training_loss": 7.475768089294434
    },
    {
      "epoch": 0.07934959349593496,
      "step": 366,
      "training_loss": 7.811481952667236
    },
    {
      "epoch": 0.07934959349593496,
      "step": 366,
      "training_loss": 8.414051055908203
    },
    {
      "epoch": 0.07956639566395664,
      "step": 367,
      "training_loss": 5.9374213218688965
    },
    {
      "epoch": 0.07956639566395664,
      "step": 367,
      "training_loss": 7.116849899291992
    },
    {
      "epoch": 0.07956639566395664,
      "step": 367,
      "training_loss": 10.125070571899414
    },
    {
      "epoch": 0.07956639566395664,
      "step": 367,
      "training_loss": 5.995860576629639
    },
    {
      "epoch": 0.07978319783197832,
      "grad_norm": 11.139396667480469,
      "learning_rate": 1e-05,
      "loss": 7.3743,
      "step": 368
    },
    {
      "epoch": 0.07978319783197832,
      "step": 368,
      "training_loss": 7.308587551116943
    },
    {
      "epoch": 0.07978319783197832,
      "step": 368,
      "training_loss": 9.323271751403809
    },
    {
      "epoch": 0.07978319783197832,
      "step": 368,
      "training_loss": 5.400503158569336
    },
    {
      "epoch": 0.07978319783197832,
      "step": 368,
      "training_loss": 6.012446403503418
    },
    {
      "epoch": 0.08,
      "step": 369,
      "training_loss": 8.16865348815918
    },
    {
      "epoch": 0.08,
      "step": 369,
      "training_loss": 7.613523483276367
    },
    {
      "epoch": 0.08,
      "step": 369,
      "training_loss": 4.117346286773682
    },
    {
      "epoch": 0.08,
      "step": 369,
      "training_loss": 8.095791816711426
    },
    {
      "epoch": 0.08021680216802168,
      "step": 370,
      "training_loss": 7.863114833831787
    },
    {
      "epoch": 0.08021680216802168,
      "step": 370,
      "training_loss": 7.085750102996826
    },
    {
      "epoch": 0.08021680216802168,
      "step": 370,
      "training_loss": 6.98181676864624
    },
    {
      "epoch": 0.08021680216802168,
      "step": 370,
      "training_loss": 6.328423023223877
    },
    {
      "epoch": 0.08043360433604337,
      "step": 371,
      "training_loss": 5.605462551116943
    },
    {
      "epoch": 0.08043360433604337,
      "step": 371,
      "training_loss": 6.910205364227295
    },
    {
      "epoch": 0.08043360433604337,
      "step": 371,
      "training_loss": 6.637826919555664
    },
    {
      "epoch": 0.08043360433604337,
      "step": 371,
      "training_loss": 7.923057556152344
    },
    {
      "epoch": 0.08065040650406505,
      "grad_norm": 10.303519248962402,
      "learning_rate": 1e-05,
      "loss": 6.961,
      "step": 372
    },
    {
      "epoch": 0.08065040650406505,
      "step": 372,
      "training_loss": 5.156768798828125
    },
    {
      "epoch": 0.08065040650406505,
      "step": 372,
      "training_loss": 7.06074857711792
    },
    {
      "epoch": 0.08065040650406505,
      "step": 372,
      "training_loss": 7.404016494750977
    },
    {
      "epoch": 0.08065040650406505,
      "step": 372,
      "training_loss": 7.1184401512146
    },
    {
      "epoch": 0.08086720867208672,
      "step": 373,
      "training_loss": 8.159137725830078
    },
    {
      "epoch": 0.08086720867208672,
      "step": 373,
      "training_loss": 6.815533638000488
    },
    {
      "epoch": 0.08086720867208672,
      "step": 373,
      "training_loss": 6.110157012939453
    },
    {
      "epoch": 0.08086720867208672,
      "step": 373,
      "training_loss": 8.777236938476562
    },
    {
      "epoch": 0.0810840108401084,
      "step": 374,
      "training_loss": 7.38739013671875
    },
    {
      "epoch": 0.0810840108401084,
      "step": 374,
      "training_loss": 7.053267955780029
    },
    {
      "epoch": 0.0810840108401084,
      "step": 374,
      "training_loss": 7.216965675354004
    },
    {
      "epoch": 0.0810840108401084,
      "step": 374,
      "training_loss": 6.407527923583984
    },
    {
      "epoch": 0.08130081300813008,
      "step": 375,
      "training_loss": 7.535796642303467
    },
    {
      "epoch": 0.08130081300813008,
      "step": 375,
      "training_loss": 7.279159069061279
    },
    {
      "epoch": 0.08130081300813008,
      "step": 375,
      "training_loss": 7.463853359222412
    },
    {
      "epoch": 0.08130081300813008,
      "step": 375,
      "training_loss": 5.794834136962891
    },
    {
      "epoch": 0.08151761517615176,
      "grad_norm": 12.274685859680176,
      "learning_rate": 1e-05,
      "loss": 7.0463,
      "step": 376
    },
    {
      "epoch": 0.08151761517615176,
      "step": 376,
      "training_loss": 7.385788440704346
    },
    {
      "epoch": 0.08151761517615176,
      "step": 376,
      "training_loss": 7.628615856170654
    },
    {
      "epoch": 0.08151761517615176,
      "step": 376,
      "training_loss": 6.708714008331299
    },
    {
      "epoch": 0.08151761517615176,
      "step": 376,
      "training_loss": 6.241640090942383
    },
    {
      "epoch": 0.08173441734417344,
      "step": 377,
      "training_loss": 7.429653644561768
    },
    {
      "epoch": 0.08173441734417344,
      "step": 377,
      "training_loss": 6.468784332275391
    },
    {
      "epoch": 0.08173441734417344,
      "step": 377,
      "training_loss": 6.6034159660339355
    },
    {
      "epoch": 0.08173441734417344,
      "step": 377,
      "training_loss": 7.303355693817139
    },
    {
      "epoch": 0.08195121951219513,
      "step": 378,
      "training_loss": 7.871899604797363
    },
    {
      "epoch": 0.08195121951219513,
      "step": 378,
      "training_loss": 7.224704265594482
    },
    {
      "epoch": 0.08195121951219513,
      "step": 378,
      "training_loss": 6.852969169616699
    },
    {
      "epoch": 0.08195121951219513,
      "step": 378,
      "training_loss": 7.831522464752197
    },
    {
      "epoch": 0.08216802168021681,
      "step": 379,
      "training_loss": 7.40233039855957
    },
    {
      "epoch": 0.08216802168021681,
      "step": 379,
      "training_loss": 6.583349227905273
    },
    {
      "epoch": 0.08216802168021681,
      "step": 379,
      "training_loss": 7.289618968963623
    },
    {
      "epoch": 0.08216802168021681,
      "step": 379,
      "training_loss": 7.204078197479248
    },
    {
      "epoch": 0.08238482384823849,
      "grad_norm": 12.744876861572266,
      "learning_rate": 1e-05,
      "loss": 7.1269,
      "step": 380
    },
    {
      "epoch": 0.08238482384823849,
      "step": 380,
      "training_loss": 6.9077324867248535
    },
    {
      "epoch": 0.08238482384823849,
      "step": 380,
      "training_loss": 6.784528732299805
    },
    {
      "epoch": 0.08238482384823849,
      "step": 380,
      "training_loss": 6.540785312652588
    },
    {
      "epoch": 0.08238482384823849,
      "step": 380,
      "training_loss": 8.424079895019531
    },
    {
      "epoch": 0.08260162601626016,
      "step": 381,
      "training_loss": 6.828361988067627
    },
    {
      "epoch": 0.08260162601626016,
      "step": 381,
      "training_loss": 7.461328029632568
    },
    {
      "epoch": 0.08260162601626016,
      "step": 381,
      "training_loss": 8.543180465698242
    },
    {
      "epoch": 0.08260162601626016,
      "step": 381,
      "training_loss": 7.298242092132568
    },
    {
      "epoch": 0.08281842818428184,
      "step": 382,
      "training_loss": 7.973941802978516
    },
    {
      "epoch": 0.08281842818428184,
      "step": 382,
      "training_loss": 6.51055908203125
    },
    {
      "epoch": 0.08281842818428184,
      "step": 382,
      "training_loss": 7.97018575668335
    },
    {
      "epoch": 0.08281842818428184,
      "step": 382,
      "training_loss": 7.623456001281738
    },
    {
      "epoch": 0.08303523035230352,
      "step": 383,
      "training_loss": 7.495517730712891
    },
    {
      "epoch": 0.08303523035230352,
      "step": 383,
      "training_loss": 7.374047756195068
    },
    {
      "epoch": 0.08303523035230352,
      "step": 383,
      "training_loss": 8.402389526367188
    },
    {
      "epoch": 0.08303523035230352,
      "step": 383,
      "training_loss": 6.927697658538818
    },
    {
      "epoch": 0.0832520325203252,
      "grad_norm": 8.63561725616455,
      "learning_rate": 1e-05,
      "loss": 7.4416,
      "step": 384
    },
    {
      "epoch": 0.0832520325203252,
      "step": 384,
      "training_loss": 7.163276672363281
    },
    {
      "epoch": 0.0832520325203252,
      "step": 384,
      "training_loss": 7.643392562866211
    },
    {
      "epoch": 0.0832520325203252,
      "step": 384,
      "training_loss": 6.9671711921691895
    },
    {
      "epoch": 0.0832520325203252,
      "step": 384,
      "training_loss": 7.089645862579346
    },
    {
      "epoch": 0.08346883468834689,
      "step": 385,
      "training_loss": 7.810018062591553
    },
    {
      "epoch": 0.08346883468834689,
      "step": 385,
      "training_loss": 6.141658306121826
    },
    {
      "epoch": 0.08346883468834689,
      "step": 385,
      "training_loss": 6.637906074523926
    },
    {
      "epoch": 0.08346883468834689,
      "step": 385,
      "training_loss": 6.977285385131836
    },
    {
      "epoch": 0.08368563685636857,
      "step": 386,
      "training_loss": 7.162291526794434
    },
    {
      "epoch": 0.08368563685636857,
      "step": 386,
      "training_loss": 6.877755165100098
    },
    {
      "epoch": 0.08368563685636857,
      "step": 386,
      "training_loss": 6.65018892288208
    },
    {
      "epoch": 0.08368563685636857,
      "step": 386,
      "training_loss": 7.695532321929932
    },
    {
      "epoch": 0.08390243902439025,
      "step": 387,
      "training_loss": 7.404749393463135
    },
    {
      "epoch": 0.08390243902439025,
      "step": 387,
      "training_loss": 7.043624401092529
    },
    {
      "epoch": 0.08390243902439025,
      "step": 387,
      "training_loss": 7.71258020401001
    },
    {
      "epoch": 0.08390243902439025,
      "step": 387,
      "training_loss": 7.268475532531738
    },
    {
      "epoch": 0.08411924119241193,
      "grad_norm": 7.335081577301025,
      "learning_rate": 1e-05,
      "loss": 7.1403,
      "step": 388
    },
    {
      "epoch": 0.08411924119241193,
      "step": 388,
      "training_loss": 5.794533729553223
    },
    {
      "epoch": 0.08411924119241193,
      "step": 388,
      "training_loss": 6.428040981292725
    },
    {
      "epoch": 0.08411924119241193,
      "step": 388,
      "training_loss": 9.524544715881348
    },
    {
      "epoch": 0.08411924119241193,
      "step": 388,
      "training_loss": 7.065751075744629
    },
    {
      "epoch": 0.0843360433604336,
      "step": 389,
      "training_loss": 7.183258056640625
    },
    {
      "epoch": 0.0843360433604336,
      "step": 389,
      "training_loss": 7.866400241851807
    },
    {
      "epoch": 0.0843360433604336,
      "step": 389,
      "training_loss": 7.167806625366211
    },
    {
      "epoch": 0.0843360433604336,
      "step": 389,
      "training_loss": 6.6145710945129395
    },
    {
      "epoch": 0.08455284552845528,
      "step": 390,
      "training_loss": 7.862417697906494
    },
    {
      "epoch": 0.08455284552845528,
      "step": 390,
      "training_loss": 6.912542343139648
    },
    {
      "epoch": 0.08455284552845528,
      "step": 390,
      "training_loss": 5.274046897888184
    },
    {
      "epoch": 0.08455284552845528,
      "step": 390,
      "training_loss": 7.676604747772217
    },
    {
      "epoch": 0.08476964769647696,
      "step": 391,
      "training_loss": 7.891050815582275
    },
    {
      "epoch": 0.08476964769647696,
      "step": 391,
      "training_loss": 6.820775508880615
    },
    {
      "epoch": 0.08476964769647696,
      "step": 391,
      "training_loss": 6.857896327972412
    },
    {
      "epoch": 0.08476964769647696,
      "step": 391,
      "training_loss": 7.068439483642578
    },
    {
      "epoch": 0.08498644986449864,
      "grad_norm": 8.664656639099121,
      "learning_rate": 1e-05,
      "loss": 7.1255,
      "step": 392
    },
    {
      "epoch": 0.08498644986449864,
      "step": 392,
      "training_loss": 7.367777347564697
    },
    {
      "epoch": 0.08498644986449864,
      "step": 392,
      "training_loss": 7.187390327453613
    },
    {
      "epoch": 0.08498644986449864,
      "step": 392,
      "training_loss": 7.055963516235352
    },
    {
      "epoch": 0.08498644986449864,
      "step": 392,
      "training_loss": 6.446442604064941
    },
    {
      "epoch": 0.08520325203252033,
      "step": 393,
      "training_loss": 8.003338813781738
    },
    {
      "epoch": 0.08520325203252033,
      "step": 393,
      "training_loss": 7.19921350479126
    },
    {
      "epoch": 0.08520325203252033,
      "step": 393,
      "training_loss": 7.427685260772705
    },
    {
      "epoch": 0.08520325203252033,
      "step": 393,
      "training_loss": 8.516560554504395
    },
    {
      "epoch": 0.08542005420054201,
      "step": 394,
      "training_loss": 6.40228271484375
    },
    {
      "epoch": 0.08542005420054201,
      "step": 394,
      "training_loss": 7.743587493896484
    },
    {
      "epoch": 0.08542005420054201,
      "step": 394,
      "training_loss": 8.421610832214355
    },
    {
      "epoch": 0.08542005420054201,
      "step": 394,
      "training_loss": 7.214401721954346
    },
    {
      "epoch": 0.08563685636856369,
      "step": 395,
      "training_loss": 6.747849464416504
    },
    {
      "epoch": 0.08563685636856369,
      "step": 395,
      "training_loss": 7.388584613800049
    },
    {
      "epoch": 0.08563685636856369,
      "step": 395,
      "training_loss": 7.668516635894775
    },
    {
      "epoch": 0.08563685636856369,
      "step": 395,
      "training_loss": 7.532722473144531
    },
    {
      "epoch": 0.08585365853658537,
      "grad_norm": 7.473388195037842,
      "learning_rate": 1e-05,
      "loss": 7.3952,
      "step": 396
    },
    {
      "epoch": 0.08585365853658537,
      "step": 396,
      "training_loss": 6.177483558654785
    },
    {
      "epoch": 0.08585365853658537,
      "step": 396,
      "training_loss": 7.26150369644165
    },
    {
      "epoch": 0.08585365853658537,
      "step": 396,
      "training_loss": 8.243261337280273
    },
    {
      "epoch": 0.08585365853658537,
      "step": 396,
      "training_loss": 6.591599941253662
    },
    {
      "epoch": 0.08607046070460704,
      "step": 397,
      "training_loss": 7.42371940612793
    },
    {
      "epoch": 0.08607046070460704,
      "step": 397,
      "training_loss": 7.569942951202393
    },
    {
      "epoch": 0.08607046070460704,
      "step": 397,
      "training_loss": 7.080404281616211
    },
    {
      "epoch": 0.08607046070460704,
      "step": 397,
      "training_loss": 8.157369613647461
    },
    {
      "epoch": 0.08628726287262872,
      "step": 398,
      "training_loss": 7.922757148742676
    },
    {
      "epoch": 0.08628726287262872,
      "step": 398,
      "training_loss": 7.8807902336120605
    },
    {
      "epoch": 0.08628726287262872,
      "step": 398,
      "training_loss": 7.3692755699157715
    },
    {
      "epoch": 0.08628726287262872,
      "step": 398,
      "training_loss": 6.353941440582275
    },
    {
      "epoch": 0.0865040650406504,
      "step": 399,
      "training_loss": 8.028409957885742
    },
    {
      "epoch": 0.0865040650406504,
      "step": 399,
      "training_loss": 7.3256611824035645
    },
    {
      "epoch": 0.0865040650406504,
      "step": 399,
      "training_loss": 6.877035140991211
    },
    {
      "epoch": 0.0865040650406504,
      "step": 399,
      "training_loss": 7.5966877937316895
    },
    {
      "epoch": 0.08672086720867209,
      "grad_norm": 9.650616645812988,
      "learning_rate": 1e-05,
      "loss": 7.3662,
      "step": 400
    },
    {
      "epoch": 0.08672086720867209,
      "step": 400,
      "training_loss": 7.70344352722168
    },
    {
      "epoch": 0.08672086720867209,
      "step": 400,
      "training_loss": 8.21861457824707
    },
    {
      "epoch": 0.08672086720867209,
      "step": 400,
      "training_loss": 6.932497024536133
    },
    {
      "epoch": 0.08672086720867209,
      "step": 400,
      "training_loss": 7.794618606567383
    },
    {
      "epoch": 0.08693766937669377,
      "step": 401,
      "training_loss": 6.534200668334961
    },
    {
      "epoch": 0.08693766937669377,
      "step": 401,
      "training_loss": 7.717850685119629
    },
    {
      "epoch": 0.08693766937669377,
      "step": 401,
      "training_loss": 6.534238815307617
    },
    {
      "epoch": 0.08693766937669377,
      "step": 401,
      "training_loss": 7.505532264709473
    },
    {
      "epoch": 0.08715447154471545,
      "step": 402,
      "training_loss": 8.005050659179688
    },
    {
      "epoch": 0.08715447154471545,
      "step": 402,
      "training_loss": 7.441718578338623
    },
    {
      "epoch": 0.08715447154471545,
      "step": 402,
      "training_loss": 7.4852471351623535
    },
    {
      "epoch": 0.08715447154471545,
      "step": 402,
      "training_loss": 7.3363776206970215
    },
    {
      "epoch": 0.08737127371273713,
      "step": 403,
      "training_loss": 7.499264717102051
    },
    {
      "epoch": 0.08737127371273713,
      "step": 403,
      "training_loss": 6.763901710510254
    },
    {
      "epoch": 0.08737127371273713,
      "step": 403,
      "training_loss": 8.006119728088379
    },
    {
      "epoch": 0.08737127371273713,
      "step": 403,
      "training_loss": 6.830820083618164
    },
    {
      "epoch": 0.08758807588075881,
      "grad_norm": 10.56161117553711,
      "learning_rate": 1e-05,
      "loss": 7.3943,
      "step": 404
    },
    {
      "epoch": 0.08758807588075881,
      "step": 404,
      "training_loss": 7.273719787597656
    },
    {
      "epoch": 0.08758807588075881,
      "step": 404,
      "training_loss": 7.840233325958252
    },
    {
      "epoch": 0.08758807588075881,
      "step": 404,
      "training_loss": 9.88465404510498
    },
    {
      "epoch": 0.08758807588075881,
      "step": 404,
      "training_loss": 7.183591365814209
    },
    {
      "epoch": 0.08780487804878048,
      "step": 405,
      "training_loss": 7.3659772872924805
    },
    {
      "epoch": 0.08780487804878048,
      "step": 405,
      "training_loss": 8.240560531616211
    },
    {
      "epoch": 0.08780487804878048,
      "step": 405,
      "training_loss": 7.54634428024292
    },
    {
      "epoch": 0.08780487804878048,
      "step": 405,
      "training_loss": 7.447433948516846
    },
    {
      "epoch": 0.08802168021680216,
      "step": 406,
      "training_loss": 7.103166103363037
    },
    {
      "epoch": 0.08802168021680216,
      "step": 406,
      "training_loss": 7.141509532928467
    },
    {
      "epoch": 0.08802168021680216,
      "step": 406,
      "training_loss": 8.011829376220703
    },
    {
      "epoch": 0.08802168021680216,
      "step": 406,
      "training_loss": 7.123205184936523
    },
    {
      "epoch": 0.08823848238482385,
      "step": 407,
      "training_loss": 7.355606555938721
    },
    {
      "epoch": 0.08823848238482385,
      "step": 407,
      "training_loss": 7.112099647521973
    },
    {
      "epoch": 0.08823848238482385,
      "step": 407,
      "training_loss": 7.792913913726807
    },
    {
      "epoch": 0.08823848238482385,
      "step": 407,
      "training_loss": 6.760565757751465
    },
    {
      "epoch": 0.08845528455284553,
      "grad_norm": 6.503026008605957,
      "learning_rate": 1e-05,
      "loss": 7.574,
      "step": 408
    },
    {
      "epoch": 0.08845528455284553,
      "step": 408,
      "training_loss": 6.859428882598877
    },
    {
      "epoch": 0.08845528455284553,
      "step": 408,
      "training_loss": 7.046127796173096
    },
    {
      "epoch": 0.08845528455284553,
      "step": 408,
      "training_loss": 6.359787940979004
    },
    {
      "epoch": 0.08845528455284553,
      "step": 408,
      "training_loss": 6.881577014923096
    },
    {
      "epoch": 0.08867208672086721,
      "step": 409,
      "training_loss": 8.661126136779785
    },
    {
      "epoch": 0.08867208672086721,
      "step": 409,
      "training_loss": 6.310565948486328
    },
    {
      "epoch": 0.08867208672086721,
      "step": 409,
      "training_loss": 6.0485053062438965
    },
    {
      "epoch": 0.08867208672086721,
      "step": 409,
      "training_loss": 7.922336101531982
    },
    {
      "epoch": 0.08888888888888889,
      "step": 410,
      "training_loss": 8.179341316223145
    },
    {
      "epoch": 0.08888888888888889,
      "step": 410,
      "training_loss": 7.25736665725708
    },
    {
      "epoch": 0.08888888888888889,
      "step": 410,
      "training_loss": 7.632358551025391
    },
    {
      "epoch": 0.08888888888888889,
      "step": 410,
      "training_loss": 6.472314834594727
    },
    {
      "epoch": 0.08910569105691057,
      "step": 411,
      "training_loss": 5.613000392913818
    },
    {
      "epoch": 0.08910569105691057,
      "step": 411,
      "training_loss": 6.926004409790039
    },
    {
      "epoch": 0.08910569105691057,
      "step": 411,
      "training_loss": 6.984107971191406
    },
    {
      "epoch": 0.08910569105691057,
      "step": 411,
      "training_loss": 7.4379119873046875
    },
    {
      "epoch": 0.08932249322493226,
      "grad_norm": 11.863326072692871,
      "learning_rate": 1e-05,
      "loss": 7.037,
      "step": 412
    },
    {
      "epoch": 0.08932249322493226,
      "step": 412,
      "training_loss": 7.215590476989746
    },
    {
      "epoch": 0.08932249322493226,
      "step": 412,
      "training_loss": 7.725694179534912
    },
    {
      "epoch": 0.08932249322493226,
      "step": 412,
      "training_loss": 7.408360481262207
    },
    {
      "epoch": 0.08932249322493226,
      "step": 412,
      "training_loss": 6.798262596130371
    },
    {
      "epoch": 0.08953929539295392,
      "step": 413,
      "training_loss": 7.097841739654541
    },
    {
      "epoch": 0.08953929539295392,
      "step": 413,
      "training_loss": 7.034102916717529
    },
    {
      "epoch": 0.08953929539295392,
      "step": 413,
      "training_loss": 7.977014541625977
    },
    {
      "epoch": 0.08953929539295392,
      "step": 413,
      "training_loss": 7.7113938331604
    },
    {
      "epoch": 0.0897560975609756,
      "step": 414,
      "training_loss": 7.111988067626953
    },
    {
      "epoch": 0.0897560975609756,
      "step": 414,
      "training_loss": 7.272708415985107
    },
    {
      "epoch": 0.0897560975609756,
      "step": 414,
      "training_loss": 6.830965995788574
    },
    {
      "epoch": 0.0897560975609756,
      "step": 414,
      "training_loss": 6.871196746826172
    },
    {
      "epoch": 0.08997289972899729,
      "step": 415,
      "training_loss": 8.87951946258545
    },
    {
      "epoch": 0.08997289972899729,
      "step": 415,
      "training_loss": 7.416462421417236
    },
    {
      "epoch": 0.08997289972899729,
      "step": 415,
      "training_loss": 6.066652297973633
    },
    {
      "epoch": 0.08997289972899729,
      "step": 415,
      "training_loss": 6.858325004577637
    },
    {
      "epoch": 0.09018970189701897,
      "grad_norm": 12.304323196411133,
      "learning_rate": 1e-05,
      "loss": 7.2673,
      "step": 416
    },
    {
      "epoch": 0.09018970189701897,
      "step": 416,
      "training_loss": 8.372096061706543
    },
    {
      "epoch": 0.09018970189701897,
      "step": 416,
      "training_loss": 6.958247661590576
    },
    {
      "epoch": 0.09018970189701897,
      "step": 416,
      "training_loss": 7.6775898933410645
    },
    {
      "epoch": 0.09018970189701897,
      "step": 416,
      "training_loss": 6.876744747161865
    },
    {
      "epoch": 0.09040650406504065,
      "step": 417,
      "training_loss": 5.82085657119751
    },
    {
      "epoch": 0.09040650406504065,
      "step": 417,
      "training_loss": 7.883205890655518
    },
    {
      "epoch": 0.09040650406504065,
      "step": 417,
      "training_loss": 6.913529396057129
    },
    {
      "epoch": 0.09040650406504065,
      "step": 417,
      "training_loss": 8.117959976196289
    },
    {
      "epoch": 0.09062330623306233,
      "step": 418,
      "training_loss": 7.738210201263428
    },
    {
      "epoch": 0.09062330623306233,
      "step": 418,
      "training_loss": 7.213300704956055
    },
    {
      "epoch": 0.09062330623306233,
      "step": 418,
      "training_loss": 7.378101348876953
    },
    {
      "epoch": 0.09062330623306233,
      "step": 418,
      "training_loss": 6.984978199005127
    },
    {
      "epoch": 0.09084010840108402,
      "step": 419,
      "training_loss": 7.484378337860107
    },
    {
      "epoch": 0.09084010840108402,
      "step": 419,
      "training_loss": 7.8264241218566895
    },
    {
      "epoch": 0.09084010840108402,
      "step": 419,
      "training_loss": 6.440570831298828
    },
    {
      "epoch": 0.09084010840108402,
      "step": 419,
      "training_loss": 6.999765872955322
    },
    {
      "epoch": 0.0910569105691057,
      "grad_norm": 8.62780475616455,
      "learning_rate": 1e-05,
      "loss": 7.2929,
      "step": 420
    },
    {
      "epoch": 0.0910569105691057,
      "step": 420,
      "training_loss": 7.176281452178955
    },
    {
      "epoch": 0.0910569105691057,
      "step": 420,
      "training_loss": 7.82358980178833
    },
    {
      "epoch": 0.0910569105691057,
      "step": 420,
      "training_loss": 7.527884006500244
    },
    {
      "epoch": 0.0910569105691057,
      "step": 420,
      "training_loss": 7.590724945068359
    },
    {
      "epoch": 0.09127371273712737,
      "step": 421,
      "training_loss": 8.2521333694458
    },
    {
      "epoch": 0.09127371273712737,
      "step": 421,
      "training_loss": 7.979620933532715
    },
    {
      "epoch": 0.09127371273712737,
      "step": 421,
      "training_loss": 7.23245906829834
    },
    {
      "epoch": 0.09127371273712737,
      "step": 421,
      "training_loss": 7.384654998779297
    },
    {
      "epoch": 0.09149051490514905,
      "step": 422,
      "training_loss": 7.213919639587402
    },
    {
      "epoch": 0.09149051490514905,
      "step": 422,
      "training_loss": 6.0251078605651855
    },
    {
      "epoch": 0.09149051490514905,
      "step": 422,
      "training_loss": 7.540555953979492
    },
    {
      "epoch": 0.09149051490514905,
      "step": 422,
      "training_loss": 7.550441741943359
    },
    {
      "epoch": 0.09170731707317073,
      "step": 423,
      "training_loss": 7.320291996002197
    },
    {
      "epoch": 0.09170731707317073,
      "step": 423,
      "training_loss": 7.410642147064209
    },
    {
      "epoch": 0.09170731707317073,
      "step": 423,
      "training_loss": 4.426968574523926
    },
    {
      "epoch": 0.09170731707317073,
      "step": 423,
      "training_loss": 6.931756973266602
    },
    {
      "epoch": 0.09192411924119241,
      "grad_norm": 9.459228515625,
      "learning_rate": 1e-05,
      "loss": 7.2117,
      "step": 424
    },
    {
      "epoch": 0.09192411924119241,
      "step": 424,
      "training_loss": 5.53497314453125
    },
    {
      "epoch": 0.09192411924119241,
      "step": 424,
      "training_loss": 6.889934062957764
    },
    {
      "epoch": 0.09192411924119241,
      "step": 424,
      "training_loss": 6.542405128479004
    },
    {
      "epoch": 0.09192411924119241,
      "step": 424,
      "training_loss": 5.654237270355225
    },
    {
      "epoch": 0.0921409214092141,
      "step": 425,
      "training_loss": 7.924257278442383
    },
    {
      "epoch": 0.0921409214092141,
      "step": 425,
      "training_loss": 8.490142822265625
    },
    {
      "epoch": 0.0921409214092141,
      "step": 425,
      "training_loss": 6.022305011749268
    },
    {
      "epoch": 0.0921409214092141,
      "step": 425,
      "training_loss": 6.127028942108154
    },
    {
      "epoch": 0.09235772357723578,
      "step": 426,
      "training_loss": 7.6120285987854
    },
    {
      "epoch": 0.09235772357723578,
      "step": 426,
      "training_loss": 7.316117763519287
    },
    {
      "epoch": 0.09235772357723578,
      "step": 426,
      "training_loss": 6.699737071990967
    },
    {
      "epoch": 0.09235772357723578,
      "step": 426,
      "training_loss": 7.890315532684326
    },
    {
      "epoch": 0.09257452574525746,
      "step": 427,
      "training_loss": 8.0033597946167
    },
    {
      "epoch": 0.09257452574525746,
      "step": 427,
      "training_loss": 7.4165449142456055
    },
    {
      "epoch": 0.09257452574525746,
      "step": 427,
      "training_loss": 7.9355292320251465
    },
    {
      "epoch": 0.09257452574525746,
      "step": 427,
      "training_loss": 6.137234687805176
    },
    {
      "epoch": 0.09279132791327914,
      "grad_norm": 10.840630531311035,
      "learning_rate": 1e-05,
      "loss": 7.0123,
      "step": 428
    },
    {
      "epoch": 0.09279132791327914,
      "step": 428,
      "training_loss": 7.218887805938721
    },
    {
      "epoch": 0.09279132791327914,
      "step": 428,
      "training_loss": 7.463106155395508
    },
    {
      "epoch": 0.09279132791327914,
      "step": 428,
      "training_loss": 5.911827564239502
    },
    {
      "epoch": 0.09279132791327914,
      "step": 428,
      "training_loss": 5.682816505432129
    },
    {
      "epoch": 0.09300813008130081,
      "step": 429,
      "training_loss": 7.2510294914245605
    },
    {
      "epoch": 0.09300813008130081,
      "step": 429,
      "training_loss": 7.360248565673828
    },
    {
      "epoch": 0.09300813008130081,
      "step": 429,
      "training_loss": 7.713620662689209
    },
    {
      "epoch": 0.09300813008130081,
      "step": 429,
      "training_loss": 6.707645893096924
    },
    {
      "epoch": 0.09322493224932249,
      "step": 430,
      "training_loss": 8.496352195739746
    },
    {
      "epoch": 0.09322493224932249,
      "step": 430,
      "training_loss": 7.382816314697266
    },
    {
      "epoch": 0.09322493224932249,
      "step": 430,
      "training_loss": 7.397069454193115
    },
    {
      "epoch": 0.09322493224932249,
      "step": 430,
      "training_loss": 7.262023448944092
    },
    {
      "epoch": 0.09344173441734417,
      "step": 431,
      "training_loss": 6.356154918670654
    },
    {
      "epoch": 0.09344173441734417,
      "step": 431,
      "training_loss": 7.87091588973999
    },
    {
      "epoch": 0.09344173441734417,
      "step": 431,
      "training_loss": 4.7270894050598145
    },
    {
      "epoch": 0.09344173441734417,
      "step": 431,
      "training_loss": 6.86943244934082
    },
    {
      "epoch": 0.09365853658536585,
      "grad_norm": 8.146764755249023,
      "learning_rate": 1e-05,
      "loss": 6.9794,
      "step": 432
    },
    {
      "epoch": 0.09365853658536585,
      "step": 432,
      "training_loss": 7.429526329040527
    },
    {
      "epoch": 0.09365853658536585,
      "step": 432,
      "training_loss": 6.83367919921875
    },
    {
      "epoch": 0.09365853658536585,
      "step": 432,
      "training_loss": 7.87603235244751
    },
    {
      "epoch": 0.09365853658536585,
      "step": 432,
      "training_loss": 7.228231906890869
    },
    {
      "epoch": 0.09387533875338754,
      "step": 433,
      "training_loss": 6.416215896606445
    },
    {
      "epoch": 0.09387533875338754,
      "step": 433,
      "training_loss": 6.737086772918701
    },
    {
      "epoch": 0.09387533875338754,
      "step": 433,
      "training_loss": 5.729211330413818
    },
    {
      "epoch": 0.09387533875338754,
      "step": 433,
      "training_loss": 6.709980487823486
    },
    {
      "epoch": 0.09409214092140922,
      "step": 434,
      "training_loss": 8.234701156616211
    },
    {
      "epoch": 0.09409214092140922,
      "step": 434,
      "training_loss": 7.3559136390686035
    },
    {
      "epoch": 0.09409214092140922,
      "step": 434,
      "training_loss": 6.757698059082031
    },
    {
      "epoch": 0.09409214092140922,
      "step": 434,
      "training_loss": 7.327489376068115
    },
    {
      "epoch": 0.0943089430894309,
      "step": 435,
      "training_loss": 6.5024871826171875
    },
    {
      "epoch": 0.0943089430894309,
      "step": 435,
      "training_loss": 7.624791622161865
    },
    {
      "epoch": 0.0943089430894309,
      "step": 435,
      "training_loss": 7.6231913566589355
    },
    {
      "epoch": 0.0943089430894309,
      "step": 435,
      "training_loss": 7.5357441902160645
    },
    {
      "epoch": 0.09452574525745258,
      "grad_norm": 7.908862590789795,
      "learning_rate": 1e-05,
      "loss": 7.1201,
      "step": 436
    },
    {
      "epoch": 0.09452574525745258,
      "step": 436,
      "training_loss": 6.858415126800537
    },
    {
      "epoch": 0.09452574525745258,
      "step": 436,
      "training_loss": 7.5475335121154785
    },
    {
      "epoch": 0.09452574525745258,
      "step": 436,
      "training_loss": 6.753315448760986
    },
    {
      "epoch": 0.09452574525745258,
      "step": 436,
      "training_loss": 7.1569719314575195
    },
    {
      "epoch": 0.09474254742547425,
      "step": 437,
      "training_loss": 6.678298473358154
    },
    {
      "epoch": 0.09474254742547425,
      "step": 437,
      "training_loss": 6.795024871826172
    },
    {
      "epoch": 0.09474254742547425,
      "step": 437,
      "training_loss": 7.039063453674316
    },
    {
      "epoch": 0.09474254742547425,
      "step": 437,
      "training_loss": 6.73647928237915
    },
    {
      "epoch": 0.09495934959349593,
      "step": 438,
      "training_loss": 5.553271770477295
    },
    {
      "epoch": 0.09495934959349593,
      "step": 438,
      "training_loss": 7.218787670135498
    },
    {
      "epoch": 0.09495934959349593,
      "step": 438,
      "training_loss": 7.544483184814453
    },
    {
      "epoch": 0.09495934959349593,
      "step": 438,
      "training_loss": 7.030589580535889
    },
    {
      "epoch": 0.09517615176151761,
      "step": 439,
      "training_loss": 7.321338176727295
    },
    {
      "epoch": 0.09517615176151761,
      "step": 439,
      "training_loss": 6.247363567352295
    },
    {
      "epoch": 0.09517615176151761,
      "step": 439,
      "training_loss": 8.717790603637695
    },
    {
      "epoch": 0.09517615176151761,
      "step": 439,
      "training_loss": 6.882702827453613
    },
    {
      "epoch": 0.0953929539295393,
      "grad_norm": 13.17747688293457,
      "learning_rate": 1e-05,
      "loss": 7.0051,
      "step": 440
    },
    {
      "epoch": 0.0953929539295393,
      "step": 440,
      "training_loss": 5.934942722320557
    },
    {
      "epoch": 0.0953929539295393,
      "step": 440,
      "training_loss": 7.092322826385498
    },
    {
      "epoch": 0.0953929539295393,
      "step": 440,
      "training_loss": 6.982938289642334
    },
    {
      "epoch": 0.0953929539295393,
      "step": 440,
      "training_loss": 6.4976091384887695
    },
    {
      "epoch": 0.09560975609756098,
      "step": 441,
      "training_loss": 7.7944841384887695
    },
    {
      "epoch": 0.09560975609756098,
      "step": 441,
      "training_loss": 6.152702808380127
    },
    {
      "epoch": 0.09560975609756098,
      "step": 441,
      "training_loss": 6.2052001953125
    },
    {
      "epoch": 0.09560975609756098,
      "step": 441,
      "training_loss": 7.073969841003418
    },
    {
      "epoch": 0.09582655826558266,
      "step": 442,
      "training_loss": 6.221671104431152
    },
    {
      "epoch": 0.09582655826558266,
      "step": 442,
      "training_loss": 6.226191997528076
    },
    {
      "epoch": 0.09582655826558266,
      "step": 442,
      "training_loss": 8.009552001953125
    },
    {
      "epoch": 0.09582655826558266,
      "step": 442,
      "training_loss": 8.643951416015625
    },
    {
      "epoch": 0.09604336043360434,
      "step": 443,
      "training_loss": 5.817463397979736
    },
    {
      "epoch": 0.09604336043360434,
      "step": 443,
      "training_loss": 5.743379592895508
    },
    {
      "epoch": 0.09604336043360434,
      "step": 443,
      "training_loss": 7.251195430755615
    },
    {
      "epoch": 0.09604336043360434,
      "step": 443,
      "training_loss": 7.175926685333252
    },
    {
      "epoch": 0.09626016260162602,
      "grad_norm": 12.17806625366211,
      "learning_rate": 1e-05,
      "loss": 6.8015,
      "step": 444
    },
    {
      "epoch": 0.09626016260162602,
      "step": 444,
      "training_loss": 6.810486316680908
    },
    {
      "epoch": 0.09626016260162602,
      "step": 444,
      "training_loss": 7.036202430725098
    },
    {
      "epoch": 0.09626016260162602,
      "step": 444,
      "training_loss": 6.245882034301758
    },
    {
      "epoch": 0.09626016260162602,
      "step": 444,
      "training_loss": 7.8505635261535645
    },
    {
      "epoch": 0.09647696476964769,
      "step": 445,
      "training_loss": 6.490572452545166
    },
    {
      "epoch": 0.09647696476964769,
      "step": 445,
      "training_loss": 6.343875408172607
    },
    {
      "epoch": 0.09647696476964769,
      "step": 445,
      "training_loss": 4.814797878265381
    },
    {
      "epoch": 0.09647696476964769,
      "step": 445,
      "training_loss": 5.777690887451172
    },
    {
      "epoch": 0.09669376693766937,
      "step": 446,
      "training_loss": 6.847370147705078
    },
    {
      "epoch": 0.09669376693766937,
      "step": 446,
      "training_loss": 6.5334272384643555
    },
    {
      "epoch": 0.09669376693766937,
      "step": 446,
      "training_loss": 7.217382907867432
    },
    {
      "epoch": 0.09669376693766937,
      "step": 446,
      "training_loss": 7.697766304016113
    },
    {
      "epoch": 0.09691056910569106,
      "step": 447,
      "training_loss": 7.530991554260254
    },
    {
      "epoch": 0.09691056910569106,
      "step": 447,
      "training_loss": 7.341429710388184
    },
    {
      "epoch": 0.09691056910569106,
      "step": 447,
      "training_loss": 8.07534408569336
    },
    {
      "epoch": 0.09691056910569106,
      "step": 447,
      "training_loss": 7.243075370788574
    },
    {
      "epoch": 0.09712737127371274,
      "grad_norm": 7.4808349609375,
      "learning_rate": 1e-05,
      "loss": 6.8661,
      "step": 448
    },
    {
      "epoch": 0.09712737127371274,
      "step": 448,
      "training_loss": 6.840117454528809
    },
    {
      "epoch": 0.09712737127371274,
      "step": 448,
      "training_loss": 7.598024368286133
    },
    {
      "epoch": 0.09712737127371274,
      "step": 448,
      "training_loss": 8.978449821472168
    },
    {
      "epoch": 0.09712737127371274,
      "step": 448,
      "training_loss": 7.878036022186279
    },
    {
      "epoch": 0.09734417344173442,
      "step": 449,
      "training_loss": 5.933165550231934
    },
    {
      "epoch": 0.09734417344173442,
      "step": 449,
      "training_loss": 6.433210849761963
    },
    {
      "epoch": 0.09734417344173442,
      "step": 449,
      "training_loss": 7.384937763214111
    },
    {
      "epoch": 0.09734417344173442,
      "step": 449,
      "training_loss": 6.665628910064697
    },
    {
      "epoch": 0.0975609756097561,
      "step": 450,
      "training_loss": 6.245662212371826
    },
    {
      "epoch": 0.0975609756097561,
      "step": 450,
      "training_loss": 6.921314239501953
    },
    {
      "epoch": 0.0975609756097561,
      "step": 450,
      "training_loss": 6.69264554977417
    },
    {
      "epoch": 0.0975609756097561,
      "step": 450,
      "training_loss": 6.923404693603516
    },
    {
      "epoch": 0.09777777777777778,
      "step": 451,
      "training_loss": 7.332199573516846
    },
    {
      "epoch": 0.09777777777777778,
      "step": 451,
      "training_loss": 6.99437952041626
    },
    {
      "epoch": 0.09777777777777778,
      "step": 451,
      "training_loss": 5.885593891143799
    },
    {
      "epoch": 0.09777777777777778,
      "step": 451,
      "training_loss": 7.488059997558594
    },
    {
      "epoch": 0.09799457994579946,
      "grad_norm": 7.560835361480713,
      "learning_rate": 1e-05,
      "loss": 7.0122,
      "step": 452
    },
    {
      "epoch": 0.09799457994579946,
      "step": 452,
      "training_loss": 7.919029235839844
    },
    {
      "epoch": 0.09799457994579946,
      "step": 452,
      "training_loss": 7.329712867736816
    },
    {
      "epoch": 0.09799457994579946,
      "step": 452,
      "training_loss": 6.521658420562744
    },
    {
      "epoch": 0.09799457994579946,
      "step": 452,
      "training_loss": 6.763828754425049
    },
    {
      "epoch": 0.09821138211382113,
      "step": 453,
      "training_loss": 6.610156059265137
    },
    {
      "epoch": 0.09821138211382113,
      "step": 453,
      "training_loss": 6.725164890289307
    },
    {
      "epoch": 0.09821138211382113,
      "step": 453,
      "training_loss": 7.495176792144775
    },
    {
      "epoch": 0.09821138211382113,
      "step": 453,
      "training_loss": 5.957719326019287
    },
    {
      "epoch": 0.09842818428184281,
      "step": 454,
      "training_loss": 6.106435298919678
    },
    {
      "epoch": 0.09842818428184281,
      "step": 454,
      "training_loss": 6.825964450836182
    },
    {
      "epoch": 0.09842818428184281,
      "step": 454,
      "training_loss": 6.976534366607666
    },
    {
      "epoch": 0.09842818428184281,
      "step": 454,
      "training_loss": 7.687772750854492
    },
    {
      "epoch": 0.0986449864498645,
      "step": 455,
      "training_loss": 6.650537014007568
    },
    {
      "epoch": 0.0986449864498645,
      "step": 455,
      "training_loss": 6.807676792144775
    },
    {
      "epoch": 0.0986449864498645,
      "step": 455,
      "training_loss": 6.488529682159424
    },
    {
      "epoch": 0.0986449864498645,
      "step": 455,
      "training_loss": 6.742185592651367
    },
    {
      "epoch": 0.09886178861788618,
      "grad_norm": 16.621143341064453,
      "learning_rate": 1e-05,
      "loss": 6.8505,
      "step": 456
    },
    {
      "epoch": 0.09886178861788618,
      "step": 456,
      "training_loss": 6.631616115570068
    },
    {
      "epoch": 0.09886178861788618,
      "step": 456,
      "training_loss": 6.254372596740723
    },
    {
      "epoch": 0.09886178861788618,
      "step": 456,
      "training_loss": 5.843878746032715
    },
    {
      "epoch": 0.09886178861788618,
      "step": 456,
      "training_loss": 7.2369232177734375
    },
    {
      "epoch": 0.09907859078590786,
      "step": 457,
      "training_loss": 7.477080821990967
    },
    {
      "epoch": 0.09907859078590786,
      "step": 457,
      "training_loss": 7.203558921813965
    },
    {
      "epoch": 0.09907859078590786,
      "step": 457,
      "training_loss": 7.272771835327148
    },
    {
      "epoch": 0.09907859078590786,
      "step": 457,
      "training_loss": 6.669443130493164
    },
    {
      "epoch": 0.09929539295392954,
      "step": 458,
      "training_loss": 6.867480278015137
    },
    {
      "epoch": 0.09929539295392954,
      "step": 458,
      "training_loss": 6.737813472747803
    },
    {
      "epoch": 0.09929539295392954,
      "step": 458,
      "training_loss": 7.53336238861084
    },
    {
      "epoch": 0.09929539295392954,
      "step": 458,
      "training_loss": 7.471886157989502
    },
    {
      "epoch": 0.09951219512195122,
      "step": 459,
      "training_loss": 7.161625862121582
    },
    {
      "epoch": 0.09951219512195122,
      "step": 459,
      "training_loss": 5.741668224334717
    },
    {
      "epoch": 0.09951219512195122,
      "step": 459,
      "training_loss": 6.853603363037109
    },
    {
      "epoch": 0.09951219512195122,
      "step": 459,
      "training_loss": 6.012610912322998
    },
    {
      "epoch": 0.0997289972899729,
      "grad_norm": 12.049869537353516,
      "learning_rate": 1e-05,
      "loss": 6.8106,
      "step": 460
    },
    {
      "epoch": 0.0997289972899729,
      "step": 460,
      "training_loss": 6.799405574798584
    },
    {
      "epoch": 0.0997289972899729,
      "step": 460,
      "training_loss": 7.8464179039001465
    },
    {
      "epoch": 0.0997289972899729,
      "step": 460,
      "training_loss": 7.986469268798828
    },
    {
      "epoch": 0.0997289972899729,
      "step": 460,
      "training_loss": 6.974750995635986
    },
    {
      "epoch": 0.09994579945799457,
      "step": 461,
      "training_loss": 6.509397983551025
    },
    {
      "epoch": 0.09994579945799457,
      "step": 461,
      "training_loss": 6.560853958129883
    },
    {
      "epoch": 0.09994579945799457,
      "step": 461,
      "training_loss": 7.005667686462402
    },
    {
      "epoch": 0.09994579945799457,
      "step": 461,
      "training_loss": 5.31218957901001
    },
    {
      "epoch": 0.10016260162601626,
      "step": 462,
      "training_loss": 7.532958984375
    },
    {
      "epoch": 0.10016260162601626,
      "step": 462,
      "training_loss": 8.268312454223633
    },
    {
      "epoch": 0.10016260162601626,
      "step": 462,
      "training_loss": 7.5533294677734375
    },
    {
      "epoch": 0.10016260162601626,
      "step": 462,
      "training_loss": 7.127960681915283
    },
    {
      "epoch": 0.10037940379403794,
      "step": 463,
      "training_loss": 7.304149150848389
    },
    {
      "epoch": 0.10037940379403794,
      "step": 463,
      "training_loss": 7.292895317077637
    },
    {
      "epoch": 0.10037940379403794,
      "step": 463,
      "training_loss": 6.603727340698242
    },
    {
      "epoch": 0.10037940379403794,
      "step": 463,
      "training_loss": 7.568155765533447
    },
    {
      "epoch": 0.10059620596205962,
      "grad_norm": 8.582197189331055,
      "learning_rate": 1e-05,
      "loss": 7.1404,
      "step": 464
    },
    {
      "epoch": 0.10059620596205962,
      "step": 464,
      "training_loss": 6.500561237335205
    },
    {
      "epoch": 0.10059620596205962,
      "step": 464,
      "training_loss": 6.345970153808594
    },
    {
      "epoch": 0.10059620596205962,
      "step": 464,
      "training_loss": 6.81966495513916
    },
    {
      "epoch": 0.10059620596205962,
      "step": 464,
      "training_loss": 7.7142252922058105
    },
    {
      "epoch": 0.1008130081300813,
      "step": 465,
      "training_loss": 5.936171054840088
    },
    {
      "epoch": 0.1008130081300813,
      "step": 465,
      "training_loss": 7.598244667053223
    },
    {
      "epoch": 0.1008130081300813,
      "step": 465,
      "training_loss": 6.479454517364502
    },
    {
      "epoch": 0.1008130081300813,
      "step": 465,
      "training_loss": 6.285173416137695
    },
    {
      "epoch": 0.10102981029810298,
      "step": 466,
      "training_loss": 7.154382705688477
    },
    {
      "epoch": 0.10102981029810298,
      "step": 466,
      "training_loss": 7.370335578918457
    },
    {
      "epoch": 0.10102981029810298,
      "step": 466,
      "training_loss": 7.423366546630859
    },
    {
      "epoch": 0.10102981029810298,
      "step": 466,
      "training_loss": 7.516659259796143
    },
    {
      "epoch": 0.10124661246612467,
      "step": 467,
      "training_loss": 8.073773384094238
    },
    {
      "epoch": 0.10124661246612467,
      "step": 467,
      "training_loss": 6.350748062133789
    },
    {
      "epoch": 0.10124661246612467,
      "step": 467,
      "training_loss": 7.157135963439941
    },
    {
      "epoch": 0.10124661246612467,
      "step": 467,
      "training_loss": 6.924979209899902
    },
    {
      "epoch": 0.10146341463414635,
      "grad_norm": 15.541024208068848,
      "learning_rate": 1e-05,
      "loss": 6.9782,
      "step": 468
    },
    {
      "epoch": 0.10146341463414635,
      "step": 468,
      "training_loss": 6.908101558685303
    },
    {
      "epoch": 0.10146341463414635,
      "step": 468,
      "training_loss": 6.239418029785156
    },
    {
      "epoch": 0.10146341463414635,
      "step": 468,
      "training_loss": 7.0934739112854
    },
    {
      "epoch": 0.10146341463414635,
      "step": 468,
      "training_loss": 7.211306571960449
    },
    {
      "epoch": 0.10168021680216802,
      "step": 469,
      "training_loss": 7.945801258087158
    },
    {
      "epoch": 0.10168021680216802,
      "step": 469,
      "training_loss": 6.553814888000488
    },
    {
      "epoch": 0.10168021680216802,
      "step": 469,
      "training_loss": 8.108688354492188
    },
    {
      "epoch": 0.10168021680216802,
      "step": 469,
      "training_loss": 7.054125785827637
    },
    {
      "epoch": 0.1018970189701897,
      "step": 470,
      "training_loss": 7.678314685821533
    },
    {
      "epoch": 0.1018970189701897,
      "step": 470,
      "training_loss": 7.45640230178833
    },
    {
      "epoch": 0.1018970189701897,
      "step": 470,
      "training_loss": 7.3450608253479
    },
    {
      "epoch": 0.1018970189701897,
      "step": 470,
      "training_loss": 6.309415340423584
    },
    {
      "epoch": 0.10211382113821138,
      "step": 471,
      "training_loss": 6.601892948150635
    },
    {
      "epoch": 0.10211382113821138,
      "step": 471,
      "training_loss": 6.085987091064453
    },
    {
      "epoch": 0.10211382113821138,
      "step": 471,
      "training_loss": 6.538783073425293
    },
    {
      "epoch": 0.10211382113821138,
      "step": 471,
      "training_loss": 7.381738662719727
    },
    {
      "epoch": 0.10233062330623306,
      "grad_norm": 11.659954071044922,
      "learning_rate": 1e-05,
      "loss": 7.032,
      "step": 472
    },
    {
      "epoch": 0.10233062330623306,
      "step": 472,
      "training_loss": 8.664532661437988
    },
    {
      "epoch": 0.10233062330623306,
      "step": 472,
      "training_loss": 7.105472564697266
    },
    {
      "epoch": 0.10233062330623306,
      "step": 472,
      "training_loss": 6.205920696258545
    },
    {
      "epoch": 0.10233062330623306,
      "step": 472,
      "training_loss": 7.530663967132568
    },
    {
      "epoch": 0.10254742547425474,
      "step": 473,
      "training_loss": 7.675816535949707
    },
    {
      "epoch": 0.10254742547425474,
      "step": 473,
      "training_loss": 6.844049453735352
    },
    {
      "epoch": 0.10254742547425474,
      "step": 473,
      "training_loss": 7.533459186553955
    },
    {
      "epoch": 0.10254742547425474,
      "step": 473,
      "training_loss": 6.204203128814697
    },
    {
      "epoch": 0.10276422764227643,
      "step": 474,
      "training_loss": 6.509629249572754
    },
    {
      "epoch": 0.10276422764227643,
      "step": 474,
      "training_loss": 7.7107954025268555
    },
    {
      "epoch": 0.10276422764227643,
      "step": 474,
      "training_loss": 6.68780517578125
    },
    {
      "epoch": 0.10276422764227643,
      "step": 474,
      "training_loss": 6.618644714355469
    },
    {
      "epoch": 0.10298102981029811,
      "step": 475,
      "training_loss": 6.275461196899414
    },
    {
      "epoch": 0.10298102981029811,
      "step": 475,
      "training_loss": 7.437265872955322
    },
    {
      "epoch": 0.10298102981029811,
      "step": 475,
      "training_loss": 7.353854656219482
    },
    {
      "epoch": 0.10298102981029811,
      "step": 475,
      "training_loss": 6.125638484954834
    },
    {
      "epoch": 0.10319783197831979,
      "grad_norm": 11.813575744628906,
      "learning_rate": 1e-05,
      "loss": 7.0302,
      "step": 476
    },
    {
      "epoch": 0.10319783197831979,
      "step": 476,
      "training_loss": 7.131283283233643
    },
    {
      "epoch": 0.10319783197831979,
      "step": 476,
      "training_loss": 5.224076747894287
    },
    {
      "epoch": 0.10319783197831979,
      "step": 476,
      "training_loss": 6.942650318145752
    },
    {
      "epoch": 0.10319783197831979,
      "step": 476,
      "training_loss": 7.258881092071533
    },
    {
      "epoch": 0.10341463414634146,
      "step": 477,
      "training_loss": 7.877115726470947
    },
    {
      "epoch": 0.10341463414634146,
      "step": 477,
      "training_loss": 7.1476216316223145
    },
    {
      "epoch": 0.10341463414634146,
      "step": 477,
      "training_loss": 7.095988750457764
    },
    {
      "epoch": 0.10341463414634146,
      "step": 477,
      "training_loss": 5.718789577484131
    },
    {
      "epoch": 0.10363143631436314,
      "step": 478,
      "training_loss": 6.435654640197754
    },
    {
      "epoch": 0.10363143631436314,
      "step": 478,
      "training_loss": 5.979014873504639
    },
    {
      "epoch": 0.10363143631436314,
      "step": 478,
      "training_loss": 5.302471160888672
    },
    {
      "epoch": 0.10363143631436314,
      "step": 478,
      "training_loss": 7.793633937835693
    },
    {
      "epoch": 0.10384823848238482,
      "step": 479,
      "training_loss": 7.639026165008545
    },
    {
      "epoch": 0.10384823848238482,
      "step": 479,
      "training_loss": 7.5793232917785645
    },
    {
      "epoch": 0.10384823848238482,
      "step": 479,
      "training_loss": 7.870100021362305
    },
    {
      "epoch": 0.10384823848238482,
      "step": 479,
      "training_loss": 7.079348564147949
    },
    {
      "epoch": 0.1040650406504065,
      "grad_norm": 10.575132369995117,
      "learning_rate": 1e-05,
      "loss": 6.8797,
      "step": 480
    },
    {
      "epoch": 0.1040650406504065,
      "step": 480,
      "training_loss": 7.448462963104248
    },
    {
      "epoch": 0.1040650406504065,
      "step": 480,
      "training_loss": 6.103111743927002
    },
    {
      "epoch": 0.1040650406504065,
      "step": 480,
      "training_loss": 5.886322498321533
    },
    {
      "epoch": 0.1040650406504065,
      "step": 480,
      "training_loss": 5.818662166595459
    },
    {
      "epoch": 0.10428184281842819,
      "step": 481,
      "training_loss": 11.649673461914062
    },
    {
      "epoch": 0.10428184281842819,
      "step": 481,
      "training_loss": 5.571672439575195
    },
    {
      "epoch": 0.10428184281842819,
      "step": 481,
      "training_loss": 5.436093330383301
    },
    {
      "epoch": 0.10428184281842819,
      "step": 481,
      "training_loss": 5.9413251876831055
    },
    {
      "epoch": 0.10449864498644987,
      "step": 482,
      "training_loss": 7.857540130615234
    },
    {
      "epoch": 0.10449864498644987,
      "step": 482,
      "training_loss": 7.180466175079346
    },
    {
      "epoch": 0.10449864498644987,
      "step": 482,
      "training_loss": 6.324224948883057
    },
    {
      "epoch": 0.10449864498644987,
      "step": 482,
      "training_loss": 6.4803547859191895
    },
    {
      "epoch": 0.10471544715447155,
      "step": 483,
      "training_loss": 7.325991630554199
    },
    {
      "epoch": 0.10471544715447155,
      "step": 483,
      "training_loss": 7.161638259887695
    },
    {
      "epoch": 0.10471544715447155,
      "step": 483,
      "training_loss": 7.944698810577393
    },
    {
      "epoch": 0.10471544715447155,
      "step": 483,
      "training_loss": 8.160787582397461
    },
    {
      "epoch": 0.10493224932249323,
      "grad_norm": 11.310632705688477,
      "learning_rate": 1e-05,
      "loss": 7.0182,
      "step": 484
    },
    {
      "epoch": 0.10493224932249323,
      "step": 484,
      "training_loss": 8.243651390075684
    },
    {
      "epoch": 0.10493224932249323,
      "step": 484,
      "training_loss": 7.25445556640625
    },
    {
      "epoch": 0.10493224932249323,
      "step": 484,
      "training_loss": 7.2320966720581055
    },
    {
      "epoch": 0.10493224932249323,
      "step": 484,
      "training_loss": 5.9284820556640625
    },
    {
      "epoch": 0.1051490514905149,
      "step": 485,
      "training_loss": 7.987606048583984
    },
    {
      "epoch": 0.1051490514905149,
      "step": 485,
      "training_loss": 7.399197101593018
    },
    {
      "epoch": 0.1051490514905149,
      "step": 485,
      "training_loss": 7.428399562835693
    },
    {
      "epoch": 0.1051490514905149,
      "step": 485,
      "training_loss": 5.69093656539917
    },
    {
      "epoch": 0.10536585365853658,
      "step": 486,
      "training_loss": 8.017127990722656
    },
    {
      "epoch": 0.10536585365853658,
      "step": 486,
      "training_loss": 6.876014709472656
    },
    {
      "epoch": 0.10536585365853658,
      "step": 486,
      "training_loss": 7.239037036895752
    },
    {
      "epoch": 0.10536585365853658,
      "step": 486,
      "training_loss": 6.0742974281311035
    },
    {
      "epoch": 0.10558265582655826,
      "step": 487,
      "training_loss": 5.985689163208008
    },
    {
      "epoch": 0.10558265582655826,
      "step": 487,
      "training_loss": 6.994046211242676
    },
    {
      "epoch": 0.10558265582655826,
      "step": 487,
      "training_loss": 7.625695705413818
    },
    {
      "epoch": 0.10558265582655826,
      "step": 487,
      "training_loss": 7.124585151672363
    },
    {
      "epoch": 0.10579945799457995,
      "grad_norm": 9.295541763305664,
      "learning_rate": 1e-05,
      "loss": 7.0688,
      "step": 488
    },
    {
      "epoch": 0.10579945799457995,
      "step": 488,
      "training_loss": 7.420576095581055
    },
    {
      "epoch": 0.10579945799457995,
      "step": 488,
      "training_loss": 7.517667770385742
    },
    {
      "epoch": 0.10579945799457995,
      "step": 488,
      "training_loss": 7.219398498535156
    },
    {
      "epoch": 0.10579945799457995,
      "step": 488,
      "training_loss": 7.517569541931152
    },
    {
      "epoch": 0.10601626016260163,
      "step": 489,
      "training_loss": 6.999921798706055
    },
    {
      "epoch": 0.10601626016260163,
      "step": 489,
      "training_loss": 6.376694679260254
    },
    {
      "epoch": 0.10601626016260163,
      "step": 489,
      "training_loss": 6.081268787384033
    },
    {
      "epoch": 0.10601626016260163,
      "step": 489,
      "training_loss": 8.019230842590332
    },
    {
      "epoch": 0.10623306233062331,
      "step": 490,
      "training_loss": 7.67453145980835
    },
    {
      "epoch": 0.10623306233062331,
      "step": 490,
      "training_loss": 5.423788070678711
    },
    {
      "epoch": 0.10623306233062331,
      "step": 490,
      "training_loss": 7.451131343841553
    },
    {
      "epoch": 0.10623306233062331,
      "step": 490,
      "training_loss": 6.981405735015869
    },
    {
      "epoch": 0.10644986449864499,
      "step": 491,
      "training_loss": 7.145960330963135
    },
    {
      "epoch": 0.10644986449864499,
      "step": 491,
      "training_loss": 7.816411018371582
    },
    {
      "epoch": 0.10644986449864499,
      "step": 491,
      "training_loss": 6.9794697761535645
    },
    {
      "epoch": 0.10644986449864499,
      "step": 491,
      "training_loss": 7.753975868225098
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 8.668519973754883,
      "learning_rate": 1e-05,
      "loss": 7.1487,
      "step": 492
    },
    {
      "epoch": 0.10666666666666667,
      "step": 492,
      "training_loss": 6.8671793937683105
    },
    {
      "epoch": 0.10666666666666667,
      "step": 492,
      "training_loss": 7.373325824737549
    },
    {
      "epoch": 0.10666666666666667,
      "step": 492,
      "training_loss": 6.922890663146973
    },
    {
      "epoch": 0.10666666666666667,
      "step": 492,
      "training_loss": 8.230195045471191
    },
    {
      "epoch": 0.10688346883468834,
      "step": 493,
      "training_loss": 6.484180450439453
    },
    {
      "epoch": 0.10688346883468834,
      "step": 493,
      "training_loss": 6.7708635330200195
    },
    {
      "epoch": 0.10688346883468834,
      "step": 493,
      "training_loss": 7.514718532562256
    },
    {
      "epoch": 0.10688346883468834,
      "step": 493,
      "training_loss": 6.83115816116333
    },
    {
      "epoch": 0.10710027100271002,
      "step": 494,
      "training_loss": 6.650847911834717
    },
    {
      "epoch": 0.10710027100271002,
      "step": 494,
      "training_loss": 6.992758274078369
    },
    {
      "epoch": 0.10710027100271002,
      "step": 494,
      "training_loss": 5.685719013214111
    },
    {
      "epoch": 0.10710027100271002,
      "step": 494,
      "training_loss": 6.4325971603393555
    },
    {
      "epoch": 0.1073170731707317,
      "step": 495,
      "training_loss": 7.1235198974609375
    },
    {
      "epoch": 0.1073170731707317,
      "step": 495,
      "training_loss": 6.939486980438232
    },
    {
      "epoch": 0.1073170731707317,
      "step": 495,
      "training_loss": 6.666158199310303
    },
    {
      "epoch": 0.1073170731707317,
      "step": 495,
      "training_loss": 7.181458473205566
    },
    {
      "epoch": 0.10753387533875339,
      "grad_norm": 8.906322479248047,
      "learning_rate": 1e-05,
      "loss": 6.9167,
      "step": 496
    },
    {
      "epoch": 0.10753387533875339,
      "step": 496,
      "training_loss": 9.15221118927002
    },
    {
      "epoch": 0.10753387533875339,
      "step": 496,
      "training_loss": 7.525659084320068
    },
    {
      "epoch": 0.10753387533875339,
      "step": 496,
      "training_loss": 7.385865688323975
    },
    {
      "epoch": 0.10753387533875339,
      "step": 496,
      "training_loss": 8.039556503295898
    },
    {
      "epoch": 0.10775067750677507,
      "step": 497,
      "training_loss": 7.408957481384277
    },
    {
      "epoch": 0.10775067750677507,
      "step": 497,
      "training_loss": 6.8404459953308105
    },
    {
      "epoch": 0.10775067750677507,
      "step": 497,
      "training_loss": 6.061496257781982
    },
    {
      "epoch": 0.10775067750677507,
      "step": 497,
      "training_loss": 7.189970970153809
    },
    {
      "epoch": 0.10796747967479675,
      "step": 498,
      "training_loss": 7.195947170257568
    },
    {
      "epoch": 0.10796747967479675,
      "step": 498,
      "training_loss": 9.051958084106445
    },
    {
      "epoch": 0.10796747967479675,
      "step": 498,
      "training_loss": 7.247398853302002
    },
    {
      "epoch": 0.10796747967479675,
      "step": 498,
      "training_loss": 7.275570869445801
    },
    {
      "epoch": 0.10818428184281843,
      "step": 499,
      "training_loss": 7.025731086730957
    },
    {
      "epoch": 0.10818428184281843,
      "step": 499,
      "training_loss": 7.3068766593933105
    },
    {
      "epoch": 0.10818428184281843,
      "step": 499,
      "training_loss": 7.070556640625
    },
    {
      "epoch": 0.10818428184281843,
      "step": 499,
      "training_loss": 8.03092098236084
    },
    {
      "epoch": 0.10840108401084012,
      "grad_norm": 8.426773071289062,
      "learning_rate": 1e-05,
      "loss": 7.4881,
      "step": 500
    },
    {
      "epoch": 0.10840108401084012,
      "step": 500,
      "training_loss": 5.465490341186523
    },
    {
      "epoch": 0.10840108401084012,
      "step": 500,
      "training_loss": 6.9370222091674805
    },
    {
      "epoch": 0.10840108401084012,
      "step": 500,
      "training_loss": 6.727759838104248
    },
    {
      "epoch": 0.10840108401084012,
      "step": 500,
      "training_loss": 7.455414295196533
    },
    {
      "epoch": 0.10861788617886178,
      "step": 501,
      "training_loss": 7.719005584716797
    },
    {
      "epoch": 0.10861788617886178,
      "step": 501,
      "training_loss": 7.130109786987305
    },
    {
      "epoch": 0.10861788617886178,
      "step": 501,
      "training_loss": 6.326452255249023
    },
    {
      "epoch": 0.10861788617886178,
      "step": 501,
      "training_loss": 4.973262786865234
    },
    {
      "epoch": 0.10883468834688347,
      "step": 502,
      "training_loss": 6.700676441192627
    },
    {
      "epoch": 0.10883468834688347,
      "step": 502,
      "training_loss": 7.007712364196777
    },
    {
      "epoch": 0.10883468834688347,
      "step": 502,
      "training_loss": 7.190728664398193
    },
    {
      "epoch": 0.10883468834688347,
      "step": 502,
      "training_loss": 8.236494064331055
    },
    {
      "epoch": 0.10905149051490515,
      "step": 503,
      "training_loss": 7.513542652130127
    },
    {
      "epoch": 0.10905149051490515,
      "step": 503,
      "training_loss": 5.8239312171936035
    },
    {
      "epoch": 0.10905149051490515,
      "step": 503,
      "training_loss": 6.439432621002197
    },
    {
      "epoch": 0.10905149051490515,
      "step": 503,
      "training_loss": 5.0997395515441895
    },
    {
      "epoch": 0.10926829268292683,
      "grad_norm": 8.942358016967773,
      "learning_rate": 1e-05,
      "loss": 6.6717,
      "step": 504
    },
    {
      "epoch": 0.10926829268292683,
      "step": 504,
      "training_loss": 5.266537189483643
    },
    {
      "epoch": 0.10926829268292683,
      "step": 504,
      "training_loss": 4.635751724243164
    },
    {
      "epoch": 0.10926829268292683,
      "step": 504,
      "training_loss": 6.984493732452393
    },
    {
      "epoch": 0.10926829268292683,
      "step": 504,
      "training_loss": 7.662283420562744
    },
    {
      "epoch": 0.10948509485094851,
      "step": 505,
      "training_loss": 6.902346134185791
    },
    {
      "epoch": 0.10948509485094851,
      "step": 505,
      "training_loss": 7.022753715515137
    },
    {
      "epoch": 0.10948509485094851,
      "step": 505,
      "training_loss": 6.747014045715332
    },
    {
      "epoch": 0.10948509485094851,
      "step": 505,
      "training_loss": 8.134099960327148
    },
    {
      "epoch": 0.1097018970189702,
      "step": 506,
      "training_loss": 6.717401027679443
    },
    {
      "epoch": 0.1097018970189702,
      "step": 506,
      "training_loss": 7.039280414581299
    },
    {
      "epoch": 0.1097018970189702,
      "step": 506,
      "training_loss": 6.582849502563477
    },
    {
      "epoch": 0.1097018970189702,
      "step": 506,
      "training_loss": 7.726086616516113
    },
    {
      "epoch": 0.10991869918699188,
      "step": 507,
      "training_loss": 7.250476837158203
    },
    {
      "epoch": 0.10991869918699188,
      "step": 507,
      "training_loss": 5.225200176239014
    },
    {
      "epoch": 0.10991869918699188,
      "step": 507,
      "training_loss": 6.8804755210876465
    },
    {
      "epoch": 0.10991869918699188,
      "step": 507,
      "training_loss": 9.746779441833496
    },
    {
      "epoch": 0.11013550135501356,
      "grad_norm": 9.551268577575684,
      "learning_rate": 1e-05,
      "loss": 6.9077,
      "step": 508
    },
    {
      "epoch": 0.11013550135501356,
      "step": 508,
      "training_loss": 7.152677059173584
    },
    {
      "epoch": 0.11013550135501356,
      "step": 508,
      "training_loss": 6.334667682647705
    },
    {
      "epoch": 0.11013550135501356,
      "step": 508,
      "training_loss": 5.645060062408447
    },
    {
      "epoch": 0.11013550135501356,
      "step": 508,
      "training_loss": 7.546749114990234
    },
    {
      "epoch": 0.11035230352303523,
      "step": 509,
      "training_loss": 7.7975335121154785
    },
    {
      "epoch": 0.11035230352303523,
      "step": 509,
      "training_loss": 6.730597972869873
    },
    {
      "epoch": 0.11035230352303523,
      "step": 509,
      "training_loss": 6.588309288024902
    },
    {
      "epoch": 0.11035230352303523,
      "step": 509,
      "training_loss": 7.307759761810303
    },
    {
      "epoch": 0.11056910569105691,
      "step": 510,
      "training_loss": 7.238173484802246
    },
    {
      "epoch": 0.11056910569105691,
      "step": 510,
      "training_loss": 6.155001640319824
    },
    {
      "epoch": 0.11056910569105691,
      "step": 510,
      "training_loss": 7.715841770172119
    },
    {
      "epoch": 0.11056910569105691,
      "step": 510,
      "training_loss": 6.73594331741333
    },
    {
      "epoch": 0.11078590785907859,
      "step": 511,
      "training_loss": 6.60457706451416
    },
    {
      "epoch": 0.11078590785907859,
      "step": 511,
      "training_loss": 5.629286289215088
    },
    {
      "epoch": 0.11078590785907859,
      "step": 511,
      "training_loss": 6.555238246917725
    },
    {
      "epoch": 0.11078590785907859,
      "step": 511,
      "training_loss": 6.065543174743652
    },
    {
      "epoch": 0.11100271002710027,
      "grad_norm": 9.172842979431152,
      "learning_rate": 1e-05,
      "loss": 6.7377,
      "step": 512
    },
    {
      "epoch": 0.11100271002710027,
      "step": 512,
      "training_loss": 7.198293209075928
    },
    {
      "epoch": 0.11100271002710027,
      "step": 512,
      "training_loss": 5.9102582931518555
    },
    {
      "epoch": 0.11100271002710027,
      "step": 512,
      "training_loss": 7.914356708526611
    },
    {
      "epoch": 0.11100271002710027,
      "step": 512,
      "training_loss": 7.39050817489624
    },
    {
      "epoch": 0.11121951219512195,
      "step": 513,
      "training_loss": 5.945545196533203
    },
    {
      "epoch": 0.11121951219512195,
      "step": 513,
      "training_loss": 6.734013080596924
    },
    {
      "epoch": 0.11121951219512195,
      "step": 513,
      "training_loss": 7.005516052246094
    },
    {
      "epoch": 0.11121951219512195,
      "step": 513,
      "training_loss": 7.825024604797363
    },
    {
      "epoch": 0.11143631436314363,
      "step": 514,
      "training_loss": 6.5179877281188965
    },
    {
      "epoch": 0.11143631436314363,
      "step": 514,
      "training_loss": 8.289884567260742
    },
    {
      "epoch": 0.11143631436314363,
      "step": 514,
      "training_loss": 6.9430155754089355
    },
    {
      "epoch": 0.11143631436314363,
      "step": 514,
      "training_loss": 5.980615615844727
    },
    {
      "epoch": 0.11165311653116532,
      "step": 515,
      "training_loss": 7.96221399307251
    },
    {
      "epoch": 0.11165311653116532,
      "step": 515,
      "training_loss": 6.560301780700684
    },
    {
      "epoch": 0.11165311653116532,
      "step": 515,
      "training_loss": 7.546723365783691
    },
    {
      "epoch": 0.11165311653116532,
      "step": 515,
      "training_loss": 5.537167549133301
    },
    {
      "epoch": 0.111869918699187,
      "grad_norm": 6.992321014404297,
      "learning_rate": 1e-05,
      "loss": 6.9538,
      "step": 516
    },
    {
      "epoch": 0.111869918699187,
      "step": 516,
      "training_loss": 5.307744979858398
    },
    {
      "epoch": 0.111869918699187,
      "step": 516,
      "training_loss": 7.56760835647583
    },
    {
      "epoch": 0.111869918699187,
      "step": 516,
      "training_loss": 8.449039459228516
    },
    {
      "epoch": 0.111869918699187,
      "step": 516,
      "training_loss": 6.763646125793457
    },
    {
      "epoch": 0.11208672086720867,
      "step": 517,
      "training_loss": 6.406866550445557
    },
    {
      "epoch": 0.11208672086720867,
      "step": 517,
      "training_loss": 6.934685230255127
    },
    {
      "epoch": 0.11208672086720867,
      "step": 517,
      "training_loss": 6.954341888427734
    },
    {
      "epoch": 0.11208672086720867,
      "step": 517,
      "training_loss": 7.479280948638916
    },
    {
      "epoch": 0.11230352303523035,
      "step": 518,
      "training_loss": 6.787776947021484
    },
    {
      "epoch": 0.11230352303523035,
      "step": 518,
      "training_loss": 5.8803935050964355
    },
    {
      "epoch": 0.11230352303523035,
      "step": 518,
      "training_loss": 6.996118068695068
    },
    {
      "epoch": 0.11230352303523035,
      "step": 518,
      "training_loss": 7.276904106140137
    },
    {
      "epoch": 0.11252032520325203,
      "step": 519,
      "training_loss": 6.980951309204102
    },
    {
      "epoch": 0.11252032520325203,
      "step": 519,
      "training_loss": 7.541431903839111
    },
    {
      "epoch": 0.11252032520325203,
      "step": 519,
      "training_loss": 6.0147552490234375
    },
    {
      "epoch": 0.11252032520325203,
      "step": 519,
      "training_loss": 6.838963985443115
    },
    {
      "epoch": 0.11273712737127371,
      "grad_norm": 12.666415214538574,
      "learning_rate": 1e-05,
      "loss": 6.8863,
      "step": 520
    },
    {
      "epoch": 0.11273712737127371,
      "step": 520,
      "training_loss": 6.404928207397461
    },
    {
      "epoch": 0.11273712737127371,
      "step": 520,
      "training_loss": 6.851916790008545
    },
    {
      "epoch": 0.11273712737127371,
      "step": 520,
      "training_loss": 7.494287490844727
    },
    {
      "epoch": 0.11273712737127371,
      "step": 520,
      "training_loss": 6.582523345947266
    },
    {
      "epoch": 0.1129539295392954,
      "step": 521,
      "training_loss": 8.047616004943848
    },
    {
      "epoch": 0.1129539295392954,
      "step": 521,
      "training_loss": 6.58046293258667
    },
    {
      "epoch": 0.1129539295392954,
      "step": 521,
      "training_loss": 7.644490718841553
    },
    {
      "epoch": 0.1129539295392954,
      "step": 521,
      "training_loss": 7.375294208526611
    },
    {
      "epoch": 0.11317073170731708,
      "step": 522,
      "training_loss": 6.80375862121582
    },
    {
      "epoch": 0.11317073170731708,
      "step": 522,
      "training_loss": 7.478518962860107
    },
    {
      "epoch": 0.11317073170731708,
      "step": 522,
      "training_loss": 6.261988639831543
    },
    {
      "epoch": 0.11317073170731708,
      "step": 522,
      "training_loss": 6.604442119598389
    },
    {
      "epoch": 0.11338753387533876,
      "step": 523,
      "training_loss": 5.189835071563721
    },
    {
      "epoch": 0.11338753387533876,
      "step": 523,
      "training_loss": 7.397442817687988
    },
    {
      "epoch": 0.11338753387533876,
      "step": 523,
      "training_loss": 6.911155700683594
    },
    {
      "epoch": 0.11338753387533876,
      "step": 523,
      "training_loss": 7.49144172668457
    },
    {
      "epoch": 0.11360433604336044,
      "grad_norm": 15.394043922424316,
      "learning_rate": 1e-05,
      "loss": 6.945,
      "step": 524
    },
    {
      "epoch": 0.11360433604336044,
      "step": 524,
      "training_loss": 7.661531448364258
    },
    {
      "epoch": 0.11360433604336044,
      "step": 524,
      "training_loss": 6.84170389175415
    },
    {
      "epoch": 0.11360433604336044,
      "step": 524,
      "training_loss": 4.525063514709473
    },
    {
      "epoch": 0.11360433604336044,
      "step": 524,
      "training_loss": 7.053206443786621
    },
    {
      "epoch": 0.11382113821138211,
      "step": 525,
      "training_loss": 6.778448104858398
    },
    {
      "epoch": 0.11382113821138211,
      "step": 525,
      "training_loss": 7.126004219055176
    },
    {
      "epoch": 0.11382113821138211,
      "step": 525,
      "training_loss": 7.22273588180542
    },
    {
      "epoch": 0.11382113821138211,
      "step": 525,
      "training_loss": 7.115231990814209
    },
    {
      "epoch": 0.11403794037940379,
      "step": 526,
      "training_loss": 7.283390522003174
    },
    {
      "epoch": 0.11403794037940379,
      "step": 526,
      "training_loss": 6.308622360229492
    },
    {
      "epoch": 0.11403794037940379,
      "step": 526,
      "training_loss": 6.922770023345947
    },
    {
      "epoch": 0.11403794037940379,
      "step": 526,
      "training_loss": 6.658687591552734
    },
    {
      "epoch": 0.11425474254742547,
      "step": 527,
      "training_loss": 7.072789192199707
    },
    {
      "epoch": 0.11425474254742547,
      "step": 527,
      "training_loss": 6.518877029418945
    },
    {
      "epoch": 0.11425474254742547,
      "step": 527,
      "training_loss": 6.346286296844482
    },
    {
      "epoch": 0.11425474254742547,
      "step": 527,
      "training_loss": 7.204113483428955
    },
    {
      "epoch": 0.11447154471544715,
      "grad_norm": 10.88736343383789,
      "learning_rate": 1e-05,
      "loss": 6.79,
      "step": 528
    },
    {
      "epoch": 0.11447154471544715,
      "step": 528,
      "training_loss": 7.377616882324219
    },
    {
      "epoch": 0.11447154471544715,
      "step": 528,
      "training_loss": 7.171948432922363
    },
    {
      "epoch": 0.11447154471544715,
      "step": 528,
      "training_loss": 7.204850196838379
    },
    {
      "epoch": 0.11447154471544715,
      "step": 528,
      "training_loss": 6.991467475891113
    },
    {
      "epoch": 0.11468834688346884,
      "step": 529,
      "training_loss": 8.012614250183105
    },
    {
      "epoch": 0.11468834688346884,
      "step": 529,
      "training_loss": 6.0401482582092285
    },
    {
      "epoch": 0.11468834688346884,
      "step": 529,
      "training_loss": 7.532251834869385
    },
    {
      "epoch": 0.11468834688346884,
      "step": 529,
      "training_loss": 7.540384769439697
    },
    {
      "epoch": 0.11490514905149052,
      "step": 530,
      "training_loss": 7.105078220367432
    },
    {
      "epoch": 0.11490514905149052,
      "step": 530,
      "training_loss": 6.562617301940918
    },
    {
      "epoch": 0.11490514905149052,
      "step": 530,
      "training_loss": 6.4047136306762695
    },
    {
      "epoch": 0.11490514905149052,
      "step": 530,
      "training_loss": 6.836984634399414
    },
    {
      "epoch": 0.1151219512195122,
      "step": 531,
      "training_loss": 7.503129005432129
    },
    {
      "epoch": 0.1151219512195122,
      "step": 531,
      "training_loss": 5.563414573669434
    },
    {
      "epoch": 0.1151219512195122,
      "step": 531,
      "training_loss": 6.042747497558594
    },
    {
      "epoch": 0.1151219512195122,
      "step": 531,
      "training_loss": 5.953836441040039
    },
    {
      "epoch": 0.11533875338753388,
      "grad_norm": 7.253567218780518,
      "learning_rate": 1e-05,
      "loss": 6.8652,
      "step": 532
    },
    {
      "epoch": 0.11533875338753388,
      "step": 532,
      "training_loss": 7.250985145568848
    },
    {
      "epoch": 0.11533875338753388,
      "step": 532,
      "training_loss": 6.394416332244873
    },
    {
      "epoch": 0.11533875338753388,
      "step": 532,
      "training_loss": 7.000106334686279
    },
    {
      "epoch": 0.11533875338753388,
      "step": 532,
      "training_loss": 7.228240489959717
    },
    {
      "epoch": 0.11555555555555555,
      "step": 533,
      "training_loss": 6.216087818145752
    },
    {
      "epoch": 0.11555555555555555,
      "step": 533,
      "training_loss": 7.571450710296631
    },
    {
      "epoch": 0.11555555555555555,
      "step": 533,
      "training_loss": 6.260665416717529
    },
    {
      "epoch": 0.11555555555555555,
      "step": 533,
      "training_loss": 6.578571796417236
    },
    {
      "epoch": 0.11577235772357723,
      "step": 534,
      "training_loss": 5.589195728302002
    },
    {
      "epoch": 0.11577235772357723,
      "step": 534,
      "training_loss": 7.469272136688232
    },
    {
      "epoch": 0.11577235772357723,
      "step": 534,
      "training_loss": 6.049039363861084
    },
    {
      "epoch": 0.11577235772357723,
      "step": 534,
      "training_loss": 6.340561866760254
    },
    {
      "epoch": 0.11598915989159891,
      "step": 535,
      "training_loss": 7.286792755126953
    },
    {
      "epoch": 0.11598915989159891,
      "step": 535,
      "training_loss": 7.716400146484375
    },
    {
      "epoch": 0.11598915989159891,
      "step": 535,
      "training_loss": 6.640924453735352
    },
    {
      "epoch": 0.11598915989159891,
      "step": 535,
      "training_loss": 6.682662010192871
    },
    {
      "epoch": 0.1162059620596206,
      "grad_norm": 7.999037742614746,
      "learning_rate": 1e-05,
      "loss": 6.7672,
      "step": 536
    },
    {
      "epoch": 0.1162059620596206,
      "step": 536,
      "training_loss": 5.93546724319458
    },
    {
      "epoch": 0.1162059620596206,
      "step": 536,
      "training_loss": 6.764415264129639
    },
    {
      "epoch": 0.1162059620596206,
      "step": 536,
      "training_loss": 6.188488483428955
    },
    {
      "epoch": 0.1162059620596206,
      "step": 536,
      "training_loss": 7.74083137512207
    },
    {
      "epoch": 0.11642276422764228,
      "step": 537,
      "training_loss": 7.0899338722229
    },
    {
      "epoch": 0.11642276422764228,
      "step": 537,
      "training_loss": 6.561156272888184
    },
    {
      "epoch": 0.11642276422764228,
      "step": 537,
      "training_loss": 6.483990669250488
    },
    {
      "epoch": 0.11642276422764228,
      "step": 537,
      "training_loss": 7.770092964172363
    },
    {
      "epoch": 0.11663956639566396,
      "step": 538,
      "training_loss": 8.007804870605469
    },
    {
      "epoch": 0.11663956639566396,
      "step": 538,
      "training_loss": 6.5787739753723145
    },
    {
      "epoch": 0.11663956639566396,
      "step": 538,
      "training_loss": 5.149840831756592
    },
    {
      "epoch": 0.11663956639566396,
      "step": 538,
      "training_loss": 7.806624412536621
    },
    {
      "epoch": 0.11685636856368564,
      "step": 539,
      "training_loss": 7.347342014312744
    },
    {
      "epoch": 0.11685636856368564,
      "step": 539,
      "training_loss": 5.466555595397949
    },
    {
      "epoch": 0.11685636856368564,
      "step": 539,
      "training_loss": 5.896438121795654
    },
    {
      "epoch": 0.11685636856368564,
      "step": 539,
      "training_loss": 6.5997748374938965
    },
    {
      "epoch": 0.11707317073170732,
      "grad_norm": 10.892132759094238,
      "learning_rate": 1e-05,
      "loss": 6.7117,
      "step": 540
    },
    {
      "epoch": 0.11707317073170732,
      "step": 540,
      "training_loss": 8.03382396697998
    },
    {
      "epoch": 0.11707317073170732,
      "step": 540,
      "training_loss": 7.544435024261475
    },
    {
      "epoch": 0.11707317073170732,
      "step": 540,
      "training_loss": 5.899033546447754
    },
    {
      "epoch": 0.11707317073170732,
      "step": 540,
      "training_loss": 7.042945861816406
    },
    {
      "epoch": 0.11728997289972899,
      "step": 541,
      "training_loss": 7.291200637817383
    },
    {
      "epoch": 0.11728997289972899,
      "step": 541,
      "training_loss": 6.630680084228516
    },
    {
      "epoch": 0.11728997289972899,
      "step": 541,
      "training_loss": 7.045807361602783
    },
    {
      "epoch": 0.11728997289972899,
      "step": 541,
      "training_loss": 7.4017157554626465
    },
    {
      "epoch": 0.11750677506775067,
      "step": 542,
      "training_loss": 7.486234664916992
    },
    {
      "epoch": 0.11750677506775067,
      "step": 542,
      "training_loss": 7.783194541931152
    },
    {
      "epoch": 0.11750677506775067,
      "step": 542,
      "training_loss": 7.375181198120117
    },
    {
      "epoch": 0.11750677506775067,
      "step": 542,
      "training_loss": 7.690895080566406
    },
    {
      "epoch": 0.11772357723577236,
      "step": 543,
      "training_loss": 7.044957637786865
    },
    {
      "epoch": 0.11772357723577236,
      "step": 543,
      "training_loss": 7.299696445465088
    },
    {
      "epoch": 0.11772357723577236,
      "step": 543,
      "training_loss": 7.375855445861816
    },
    {
      "epoch": 0.11772357723577236,
      "step": 543,
      "training_loss": 7.9427032470703125
    },
    {
      "epoch": 0.11794037940379404,
      "grad_norm": 12.310446739196777,
      "learning_rate": 1e-05,
      "loss": 7.3055,
      "step": 544
    },
    {
      "epoch": 0.11794037940379404,
      "step": 544,
      "training_loss": 7.469574928283691
    },
    {
      "epoch": 0.11794037940379404,
      "step": 544,
      "training_loss": 5.49615478515625
    },
    {
      "epoch": 0.11794037940379404,
      "step": 544,
      "training_loss": 7.340818881988525
    },
    {
      "epoch": 0.11794037940379404,
      "step": 544,
      "training_loss": 7.105639457702637
    },
    {
      "epoch": 0.11815718157181572,
      "step": 545,
      "training_loss": 5.87653923034668
    },
    {
      "epoch": 0.11815718157181572,
      "step": 545,
      "training_loss": 7.421051979064941
    },
    {
      "epoch": 0.11815718157181572,
      "step": 545,
      "training_loss": 7.123177528381348
    },
    {
      "epoch": 0.11815718157181572,
      "step": 545,
      "training_loss": 6.526625156402588
    },
    {
      "epoch": 0.1183739837398374,
      "step": 546,
      "training_loss": 7.643099784851074
    },
    {
      "epoch": 0.1183739837398374,
      "step": 546,
      "training_loss": 7.755581378936768
    },
    {
      "epoch": 0.1183739837398374,
      "step": 546,
      "training_loss": 6.88006067276001
    },
    {
      "epoch": 0.1183739837398374,
      "step": 546,
      "training_loss": 5.934032440185547
    },
    {
      "epoch": 0.11859078590785908,
      "step": 547,
      "training_loss": 7.500781059265137
    },
    {
      "epoch": 0.11859078590785908,
      "step": 547,
      "training_loss": 7.756667137145996
    },
    {
      "epoch": 0.11859078590785908,
      "step": 547,
      "training_loss": 5.995570659637451
    },
    {
      "epoch": 0.11859078590785908,
      "step": 547,
      "training_loss": 7.873323440551758
    },
    {
      "epoch": 0.11880758807588077,
      "grad_norm": 8.711071014404297,
      "learning_rate": 1e-05,
      "loss": 6.9812,
      "step": 548
    },
    {
      "epoch": 0.11880758807588077,
      "step": 548,
      "training_loss": 6.676638603210449
    },
    {
      "epoch": 0.11880758807588077,
      "step": 548,
      "training_loss": 7.537689208984375
    },
    {
      "epoch": 0.11880758807588077,
      "step": 548,
      "training_loss": 7.418217182159424
    },
    {
      "epoch": 0.11880758807588077,
      "step": 548,
      "training_loss": 7.577217102050781
    },
    {
      "epoch": 0.11902439024390243,
      "step": 549,
      "training_loss": 7.659740924835205
    },
    {
      "epoch": 0.11902439024390243,
      "step": 549,
      "training_loss": 6.7751641273498535
    },
    {
      "epoch": 0.11902439024390243,
      "step": 549,
      "training_loss": 7.3840203285217285
    },
    {
      "epoch": 0.11902439024390243,
      "step": 549,
      "training_loss": 6.598138332366943
    },
    {
      "epoch": 0.11924119241192412,
      "step": 550,
      "training_loss": 7.5731706619262695
    },
    {
      "epoch": 0.11924119241192412,
      "step": 550,
      "training_loss": 4.5220746994018555
    },
    {
      "epoch": 0.11924119241192412,
      "step": 550,
      "training_loss": 6.138619899749756
    },
    {
      "epoch": 0.11924119241192412,
      "step": 550,
      "training_loss": 6.02822732925415
    },
    {
      "epoch": 0.1194579945799458,
      "step": 551,
      "training_loss": 6.677309989929199
    },
    {
      "epoch": 0.1194579945799458,
      "step": 551,
      "training_loss": 7.458592891693115
    },
    {
      "epoch": 0.1194579945799458,
      "step": 551,
      "training_loss": 8.117851257324219
    },
    {
      "epoch": 0.1194579945799458,
      "step": 551,
      "training_loss": 7.949036121368408
    },
    {
      "epoch": 0.11967479674796748,
      "grad_norm": 11.31176471710205,
      "learning_rate": 1e-05,
      "loss": 7.0057,
      "step": 552
    },
    {
      "epoch": 0.11967479674796748,
      "step": 552,
      "training_loss": 7.305986404418945
    },
    {
      "epoch": 0.11967479674796748,
      "step": 552,
      "training_loss": 6.387953758239746
    },
    {
      "epoch": 0.11967479674796748,
      "step": 552,
      "training_loss": 6.281477451324463
    },
    {
      "epoch": 0.11967479674796748,
      "step": 552,
      "training_loss": 7.062189102172852
    },
    {
      "epoch": 0.11989159891598916,
      "step": 553,
      "training_loss": 7.404554843902588
    },
    {
      "epoch": 0.11989159891598916,
      "step": 553,
      "training_loss": 6.849514961242676
    },
    {
      "epoch": 0.11989159891598916,
      "step": 553,
      "training_loss": 6.851048946380615
    },
    {
      "epoch": 0.11989159891598916,
      "step": 553,
      "training_loss": 7.743557453155518
    },
    {
      "epoch": 0.12010840108401084,
      "step": 554,
      "training_loss": 6.957724571228027
    },
    {
      "epoch": 0.12010840108401084,
      "step": 554,
      "training_loss": 7.039496421813965
    },
    {
      "epoch": 0.12010840108401084,
      "step": 554,
      "training_loss": 7.708283424377441
    },
    {
      "epoch": 0.12010840108401084,
      "step": 554,
      "training_loss": 6.36073112487793
    },
    {
      "epoch": 0.12032520325203253,
      "step": 555,
      "training_loss": 7.818177223205566
    },
    {
      "epoch": 0.12032520325203253,
      "step": 555,
      "training_loss": 7.273388862609863
    },
    {
      "epoch": 0.12032520325203253,
      "step": 555,
      "training_loss": 6.452199935913086
    },
    {
      "epoch": 0.12032520325203253,
      "step": 555,
      "training_loss": 6.71828031539917
    },
    {
      "epoch": 0.12054200542005421,
      "grad_norm": 11.149691581726074,
      "learning_rate": 1e-05,
      "loss": 7.0134,
      "step": 556
    },
    {
      "epoch": 0.12054200542005421,
      "step": 556,
      "training_loss": 6.202076435089111
    },
    {
      "epoch": 0.12054200542005421,
      "step": 556,
      "training_loss": 7.862170219421387
    },
    {
      "epoch": 0.12054200542005421,
      "step": 556,
      "training_loss": 7.403846263885498
    },
    {
      "epoch": 0.12054200542005421,
      "step": 556,
      "training_loss": 5.958739757537842
    },
    {
      "epoch": 0.12075880758807588,
      "step": 557,
      "training_loss": 9.286341667175293
    },
    {
      "epoch": 0.12075880758807588,
      "step": 557,
      "training_loss": 6.479926586151123
    },
    {
      "epoch": 0.12075880758807588,
      "step": 557,
      "training_loss": 4.9818644523620605
    },
    {
      "epoch": 0.12075880758807588,
      "step": 557,
      "training_loss": 7.585785865783691
    },
    {
      "epoch": 0.12097560975609756,
      "step": 558,
      "training_loss": 7.225111484527588
    },
    {
      "epoch": 0.12097560975609756,
      "step": 558,
      "training_loss": 7.7049455642700195
    },
    {
      "epoch": 0.12097560975609756,
      "step": 558,
      "training_loss": 7.908955097198486
    },
    {
      "epoch": 0.12097560975609756,
      "step": 558,
      "training_loss": 7.191324234008789
    },
    {
      "epoch": 0.12119241192411924,
      "step": 559,
      "training_loss": 7.4377875328063965
    },
    {
      "epoch": 0.12119241192411924,
      "step": 559,
      "training_loss": 6.59072208404541
    },
    {
      "epoch": 0.12119241192411924,
      "step": 559,
      "training_loss": 8.517611503601074
    },
    {
      "epoch": 0.12119241192411924,
      "step": 559,
      "training_loss": 5.96497106552124
    },
    {
      "epoch": 0.12140921409214092,
      "grad_norm": 14.456183433532715,
      "learning_rate": 1e-05,
      "loss": 7.1439,
      "step": 560
    },
    {
      "epoch": 0.12140921409214092,
      "step": 560,
      "training_loss": 7.914245128631592
    },
    {
      "epoch": 0.12140921409214092,
      "step": 560,
      "training_loss": 6.920324802398682
    },
    {
      "epoch": 0.12140921409214092,
      "step": 560,
      "training_loss": 6.350444316864014
    },
    {
      "epoch": 0.12140921409214092,
      "step": 560,
      "training_loss": 6.799843788146973
    },
    {
      "epoch": 0.1216260162601626,
      "step": 561,
      "training_loss": 7.52509880065918
    },
    {
      "epoch": 0.1216260162601626,
      "step": 561,
      "training_loss": 7.375654697418213
    },
    {
      "epoch": 0.1216260162601626,
      "step": 561,
      "training_loss": 5.154085159301758
    },
    {
      "epoch": 0.1216260162601626,
      "step": 561,
      "training_loss": 6.310026168823242
    },
    {
      "epoch": 0.12184281842818429,
      "step": 562,
      "training_loss": 6.243751049041748
    },
    {
      "epoch": 0.12184281842818429,
      "step": 562,
      "training_loss": 6.264582633972168
    },
    {
      "epoch": 0.12184281842818429,
      "step": 562,
      "training_loss": 7.7641472816467285
    },
    {
      "epoch": 0.12184281842818429,
      "step": 562,
      "training_loss": 6.810905933380127
    },
    {
      "epoch": 0.12205962059620597,
      "step": 563,
      "training_loss": 7.347211837768555
    },
    {
      "epoch": 0.12205962059620597,
      "step": 563,
      "training_loss": 7.06948709487915
    },
    {
      "epoch": 0.12205962059620597,
      "step": 563,
      "training_loss": 5.413048267364502
    },
    {
      "epoch": 0.12205962059620597,
      "step": 563,
      "training_loss": 6.986719608306885
    },
    {
      "epoch": 0.12227642276422765,
      "grad_norm": 9.517666816711426,
      "learning_rate": 1e-05,
      "loss": 6.7656,
      "step": 564
    },
    {
      "epoch": 0.12227642276422765,
      "step": 564,
      "training_loss": 6.5262451171875
    },
    {
      "epoch": 0.12227642276422765,
      "step": 564,
      "training_loss": 7.390644550323486
    },
    {
      "epoch": 0.12227642276422765,
      "step": 564,
      "training_loss": 5.585036277770996
    },
    {
      "epoch": 0.12227642276422765,
      "step": 564,
      "training_loss": 6.854288101196289
    },
    {
      "epoch": 0.12249322493224932,
      "step": 565,
      "training_loss": 5.8213958740234375
    },
    {
      "epoch": 0.12249322493224932,
      "step": 565,
      "training_loss": 7.640475749969482
    },
    {
      "epoch": 0.12249322493224932,
      "step": 565,
      "training_loss": 5.871447563171387
    },
    {
      "epoch": 0.12249322493224932,
      "step": 565,
      "training_loss": 6.197997093200684
    },
    {
      "epoch": 0.122710027100271,
      "step": 566,
      "training_loss": 6.008236885070801
    },
    {
      "epoch": 0.122710027100271,
      "step": 566,
      "training_loss": 6.016468524932861
    },
    {
      "epoch": 0.122710027100271,
      "step": 566,
      "training_loss": 7.751806735992432
    },
    {
      "epoch": 0.122710027100271,
      "step": 566,
      "training_loss": 6.549020767211914
    },
    {
      "epoch": 0.12292682926829268,
      "step": 567,
      "training_loss": 11.12137508392334
    },
    {
      "epoch": 0.12292682926829268,
      "step": 567,
      "training_loss": 9.075461387634277
    },
    {
      "epoch": 0.12292682926829268,
      "step": 567,
      "training_loss": 7.026027202606201
    },
    {
      "epoch": 0.12292682926829268,
      "step": 567,
      "training_loss": 7.952629566192627
    },
    {
      "epoch": 0.12314363143631436,
      "grad_norm": 18.711345672607422,
      "learning_rate": 1e-05,
      "loss": 7.0868,
      "step": 568
    },
    {
      "epoch": 0.12314363143631436,
      "step": 568,
      "training_loss": 6.591861724853516
    },
    {
      "epoch": 0.12314363143631436,
      "step": 568,
      "training_loss": 7.581920623779297
    },
    {
      "epoch": 0.12314363143631436,
      "step": 568,
      "training_loss": 6.483835697174072
    },
    {
      "epoch": 0.12314363143631436,
      "step": 568,
      "training_loss": 7.565531253814697
    },
    {
      "epoch": 0.12336043360433604,
      "step": 569,
      "training_loss": 7.423863887786865
    },
    {
      "epoch": 0.12336043360433604,
      "step": 569,
      "training_loss": 7.429831027984619
    },
    {
      "epoch": 0.12336043360433604,
      "step": 569,
      "training_loss": 6.279879093170166
    },
    {
      "epoch": 0.12336043360433604,
      "step": 569,
      "training_loss": 7.670935153961182
    },
    {
      "epoch": 0.12357723577235773,
      "step": 570,
      "training_loss": 6.433023929595947
    },
    {
      "epoch": 0.12357723577235773,
      "step": 570,
      "training_loss": 7.198883533477783
    },
    {
      "epoch": 0.12357723577235773,
      "step": 570,
      "training_loss": 8.01701831817627
    },
    {
      "epoch": 0.12357723577235773,
      "step": 570,
      "training_loss": 6.420291423797607
    },
    {
      "epoch": 0.12379403794037941,
      "step": 571,
      "training_loss": 4.6607794761657715
    },
    {
      "epoch": 0.12379403794037941,
      "step": 571,
      "training_loss": 6.727743148803711
    },
    {
      "epoch": 0.12379403794037941,
      "step": 571,
      "training_loss": 7.559453010559082
    },
    {
      "epoch": 0.12379403794037941,
      "step": 571,
      "training_loss": 5.17920446395874
    },
    {
      "epoch": 0.12401084010840109,
      "grad_norm": 10.805449485778809,
      "learning_rate": 1e-05,
      "loss": 6.8265,
      "step": 572
    },
    {
      "epoch": 0.12401084010840109,
      "step": 572,
      "training_loss": 7.775519847869873
    },
    {
      "epoch": 0.12401084010840109,
      "step": 572,
      "training_loss": 7.1318769454956055
    },
    {
      "epoch": 0.12401084010840109,
      "step": 572,
      "training_loss": 7.509617328643799
    },
    {
      "epoch": 0.12401084010840109,
      "step": 572,
      "training_loss": 7.167300224304199
    },
    {
      "epoch": 0.12422764227642276,
      "step": 573,
      "training_loss": 8.179383277893066
    },
    {
      "epoch": 0.12422764227642276,
      "step": 573,
      "training_loss": 6.722033977508545
    },
    {
      "epoch": 0.12422764227642276,
      "step": 573,
      "training_loss": 5.64176607131958
    },
    {
      "epoch": 0.12422764227642276,
      "step": 573,
      "training_loss": 7.866800308227539
    },
    {
      "epoch": 0.12444444444444444,
      "step": 574,
      "training_loss": 7.26054573059082
    },
    {
      "epoch": 0.12444444444444444,
      "step": 574,
      "training_loss": 7.11468505859375
    },
    {
      "epoch": 0.12444444444444444,
      "step": 574,
      "training_loss": 7.0710015296936035
    },
    {
      "epoch": 0.12444444444444444,
      "step": 574,
      "training_loss": 8.152270317077637
    },
    {
      "epoch": 0.12466124661246612,
      "step": 575,
      "training_loss": 5.126063823699951
    },
    {
      "epoch": 0.12466124661246612,
      "step": 575,
      "training_loss": 4.955852508544922
    },
    {
      "epoch": 0.12466124661246612,
      "step": 575,
      "training_loss": 7.498713970184326
    },
    {
      "epoch": 0.12466124661246612,
      "step": 575,
      "training_loss": 5.582648754119873
    },
    {
      "epoch": 0.1248780487804878,
      "grad_norm": 9.206042289733887,
      "learning_rate": 1e-05,
      "loss": 6.9223,
      "step": 576
    },
    {
      "epoch": 0.1248780487804878,
      "step": 576,
      "training_loss": 7.902830123901367
    },
    {
      "epoch": 0.1248780487804878,
      "step": 576,
      "training_loss": 6.19246244430542
    },
    {
      "epoch": 0.1248780487804878,
      "step": 576,
      "training_loss": 6.815939903259277
    },
    {
      "epoch": 0.1248780487804878,
      "step": 576,
      "training_loss": 7.511099815368652
    },
    {
      "epoch": 0.12509485094850947,
      "step": 577,
      "training_loss": 5.842238903045654
    },
    {
      "epoch": 0.12509485094850947,
      "step": 577,
      "training_loss": 6.407593250274658
    },
    {
      "epoch": 0.12509485094850947,
      "step": 577,
      "training_loss": 7.740795612335205
    },
    {
      "epoch": 0.12509485094850947,
      "step": 577,
      "training_loss": 6.782012939453125
    },
    {
      "epoch": 0.12531165311653117,
      "step": 578,
      "training_loss": 5.846649646759033
    },
    {
      "epoch": 0.12531165311653117,
      "step": 578,
      "training_loss": 6.610843658447266
    },
    {
      "epoch": 0.12531165311653117,
      "step": 578,
      "training_loss": 7.467822551727295
    },
    {
      "epoch": 0.12531165311653117,
      "step": 578,
      "training_loss": 6.050813674926758
    },
    {
      "epoch": 0.12552845528455284,
      "step": 579,
      "training_loss": 6.6383056640625
    },
    {
      "epoch": 0.12552845528455284,
      "step": 579,
      "training_loss": 7.043087482452393
    },
    {
      "epoch": 0.12552845528455284,
      "step": 579,
      "training_loss": 5.7954020500183105
    },
    {
      "epoch": 0.12552845528455284,
      "step": 579,
      "training_loss": 6.98846960067749
    },
    {
      "epoch": 0.12574525745257453,
      "grad_norm": 11.590147018432617,
      "learning_rate": 1e-05,
      "loss": 6.7273,
      "step": 580
    },
    {
      "epoch": 0.12574525745257453,
      "step": 580,
      "training_loss": 7.834303855895996
    },
    {
      "epoch": 0.12574525745257453,
      "step": 580,
      "training_loss": 7.690550327301025
    },
    {
      "epoch": 0.12574525745257453,
      "step": 580,
      "training_loss": 7.341750621795654
    },
    {
      "epoch": 0.12574525745257453,
      "step": 580,
      "training_loss": 6.8297834396362305
    },
    {
      "epoch": 0.1259620596205962,
      "step": 581,
      "training_loss": 7.489314556121826
    },
    {
      "epoch": 0.1259620596205962,
      "step": 581,
      "training_loss": 6.807191848754883
    },
    {
      "epoch": 0.1259620596205962,
      "step": 581,
      "training_loss": 7.840348720550537
    },
    {
      "epoch": 0.1259620596205962,
      "step": 581,
      "training_loss": 7.338271141052246
    },
    {
      "epoch": 0.1261788617886179,
      "step": 582,
      "training_loss": 7.354270935058594
    },
    {
      "epoch": 0.1261788617886179,
      "step": 582,
      "training_loss": 4.839841365814209
    },
    {
      "epoch": 0.1261788617886179,
      "step": 582,
      "training_loss": 8.534412384033203
    },
    {
      "epoch": 0.1261788617886179,
      "step": 582,
      "training_loss": 8.264760971069336
    },
    {
      "epoch": 0.12639566395663956,
      "step": 583,
      "training_loss": 7.561810493469238
    },
    {
      "epoch": 0.12639566395663956,
      "step": 583,
      "training_loss": 7.712203502655029
    },
    {
      "epoch": 0.12639566395663956,
      "step": 583,
      "training_loss": 5.5580153465271
    },
    {
      "epoch": 0.12639566395663956,
      "step": 583,
      "training_loss": 5.681931018829346
    },
    {
      "epoch": 0.12661246612466126,
      "grad_norm": 11.70468807220459,
      "learning_rate": 1e-05,
      "loss": 7.1674,
      "step": 584
    },
    {
      "epoch": 0.12661246612466126,
      "step": 584,
      "training_loss": 6.924960136413574
    },
    {
      "epoch": 0.12661246612466126,
      "step": 584,
      "training_loss": 7.5945658683776855
    },
    {
      "epoch": 0.12661246612466126,
      "step": 584,
      "training_loss": 5.599420070648193
    },
    {
      "epoch": 0.12661246612466126,
      "step": 584,
      "training_loss": 7.702493667602539
    },
    {
      "epoch": 0.12682926829268293,
      "step": 585,
      "training_loss": 6.4614996910095215
    },
    {
      "epoch": 0.12682926829268293,
      "step": 585,
      "training_loss": 7.728447437286377
    },
    {
      "epoch": 0.12682926829268293,
      "step": 585,
      "training_loss": 7.746238708496094
    },
    {
      "epoch": 0.12682926829268293,
      "step": 585,
      "training_loss": 7.200351715087891
    },
    {
      "epoch": 0.1270460704607046,
      "step": 586,
      "training_loss": 7.807740211486816
    },
    {
      "epoch": 0.1270460704607046,
      "step": 586,
      "training_loss": 6.976062297821045
    },
    {
      "epoch": 0.1270460704607046,
      "step": 586,
      "training_loss": 6.177127838134766
    },
    {
      "epoch": 0.1270460704607046,
      "step": 586,
      "training_loss": 5.5950093269348145
    },
    {
      "epoch": 0.1272628726287263,
      "step": 587,
      "training_loss": 5.877548694610596
    },
    {
      "epoch": 0.1272628726287263,
      "step": 587,
      "training_loss": 7.216992378234863
    },
    {
      "epoch": 0.1272628726287263,
      "step": 587,
      "training_loss": 7.7187042236328125
    },
    {
      "epoch": 0.1272628726287263,
      "step": 587,
      "training_loss": 6.478651523590088
    },
    {
      "epoch": 0.12747967479674796,
      "grad_norm": 13.597759246826172,
      "learning_rate": 1e-05,
      "loss": 6.9254,
      "step": 588
    },
    {
      "epoch": 0.12747967479674796,
      "step": 588,
      "training_loss": 7.191518783569336
    },
    {
      "epoch": 0.12747967479674796,
      "step": 588,
      "training_loss": 7.69394588470459
    },
    {
      "epoch": 0.12747967479674796,
      "step": 588,
      "training_loss": 6.9928364753723145
    },
    {
      "epoch": 0.12747967479674796,
      "step": 588,
      "training_loss": 7.010197639465332
    },
    {
      "epoch": 0.12769647696476966,
      "step": 589,
      "training_loss": 6.552066326141357
    },
    {
      "epoch": 0.12769647696476966,
      "step": 589,
      "training_loss": 6.985489368438721
    },
    {
      "epoch": 0.12769647696476966,
      "step": 589,
      "training_loss": 10.09351634979248
    },
    {
      "epoch": 0.12769647696476966,
      "step": 589,
      "training_loss": 7.247641563415527
    },
    {
      "epoch": 0.12791327913279132,
      "step": 590,
      "training_loss": 6.263129234313965
    },
    {
      "epoch": 0.12791327913279132,
      "step": 590,
      "training_loss": 8.449261665344238
    },
    {
      "epoch": 0.12791327913279132,
      "step": 590,
      "training_loss": 7.822937488555908
    },
    {
      "epoch": 0.12791327913279132,
      "step": 590,
      "training_loss": 7.246096611022949
    },
    {
      "epoch": 0.12813008130081302,
      "step": 591,
      "training_loss": 8.164037704467773
    },
    {
      "epoch": 0.12813008130081302,
      "step": 591,
      "training_loss": 7.707070350646973
    },
    {
      "epoch": 0.12813008130081302,
      "step": 591,
      "training_loss": 6.028268814086914
    },
    {
      "epoch": 0.12813008130081302,
      "step": 591,
      "training_loss": 6.738068103790283
    },
    {
      "epoch": 0.1283468834688347,
      "grad_norm": 10.991086959838867,
      "learning_rate": 1e-05,
      "loss": 7.3866,
      "step": 592
    },
    {
      "epoch": 0.1283468834688347,
      "step": 592,
      "training_loss": 7.553479194641113
    },
    {
      "epoch": 0.1283468834688347,
      "step": 592,
      "training_loss": 6.837444305419922
    },
    {
      "epoch": 0.1283468834688347,
      "step": 592,
      "training_loss": 6.743144989013672
    },
    {
      "epoch": 0.1283468834688347,
      "step": 592,
      "training_loss": 5.997982501983643
    },
    {
      "epoch": 0.12856368563685636,
      "step": 593,
      "training_loss": 6.526514530181885
    },
    {
      "epoch": 0.12856368563685636,
      "step": 593,
      "training_loss": 7.261288166046143
    },
    {
      "epoch": 0.12856368563685636,
      "step": 593,
      "training_loss": 6.561809062957764
    },
    {
      "epoch": 0.12856368563685636,
      "step": 593,
      "training_loss": 7.605901718139648
    },
    {
      "epoch": 0.12878048780487805,
      "step": 594,
      "training_loss": 6.474710464477539
    },
    {
      "epoch": 0.12878048780487805,
      "step": 594,
      "training_loss": 6.337009429931641
    },
    {
      "epoch": 0.12878048780487805,
      "step": 594,
      "training_loss": 6.831905364990234
    },
    {
      "epoch": 0.12878048780487805,
      "step": 594,
      "training_loss": 6.0926408767700195
    },
    {
      "epoch": 0.12899728997289972,
      "step": 595,
      "training_loss": 7.632419109344482
    },
    {
      "epoch": 0.12899728997289972,
      "step": 595,
      "training_loss": 6.214293479919434
    },
    {
      "epoch": 0.12899728997289972,
      "step": 595,
      "training_loss": 6.84074068069458
    },
    {
      "epoch": 0.12899728997289972,
      "step": 595,
      "training_loss": 6.879133224487305
    },
    {
      "epoch": 0.12921409214092142,
      "grad_norm": 9.097855567932129,
      "learning_rate": 1e-05,
      "loss": 6.7744,
      "step": 596
    },
    {
      "epoch": 0.12921409214092142,
      "step": 596,
      "training_loss": 6.627327919006348
    },
    {
      "epoch": 0.12921409214092142,
      "step": 596,
      "training_loss": 7.488576889038086
    },
    {
      "epoch": 0.12921409214092142,
      "step": 596,
      "training_loss": 6.402317047119141
    },
    {
      "epoch": 0.12921409214092142,
      "step": 596,
      "training_loss": 7.141048431396484
    },
    {
      "epoch": 0.12943089430894308,
      "step": 597,
      "training_loss": 9.05880069732666
    },
    {
      "epoch": 0.12943089430894308,
      "step": 597,
      "training_loss": 7.305906772613525
    },
    {
      "epoch": 0.12943089430894308,
      "step": 597,
      "training_loss": 7.232239723205566
    },
    {
      "epoch": 0.12943089430894308,
      "step": 597,
      "training_loss": 5.493894577026367
    },
    {
      "epoch": 0.12964769647696478,
      "step": 598,
      "training_loss": 7.252207279205322
    },
    {
      "epoch": 0.12964769647696478,
      "step": 598,
      "training_loss": 5.577996730804443
    },
    {
      "epoch": 0.12964769647696478,
      "step": 598,
      "training_loss": 6.958985805511475
    },
    {
      "epoch": 0.12964769647696478,
      "step": 598,
      "training_loss": 7.04778528213501
    },
    {
      "epoch": 0.12986449864498645,
      "step": 599,
      "training_loss": 5.761180400848389
    },
    {
      "epoch": 0.12986449864498645,
      "step": 599,
      "training_loss": 4.729254245758057
    },
    {
      "epoch": 0.12986449864498645,
      "step": 599,
      "training_loss": 8.28231143951416
    },
    {
      "epoch": 0.12986449864498645,
      "step": 599,
      "training_loss": 6.682663917541504
    },
    {
      "epoch": 0.13008130081300814,
      "grad_norm": 15.986223220825195,
      "learning_rate": 1e-05,
      "loss": 6.8152,
      "step": 600
    },
    {
      "epoch": 0.13008130081300814,
      "step": 600,
      "training_loss": 7.362457275390625
    },
    {
      "epoch": 0.13008130081300814,
      "step": 600,
      "training_loss": 8.44530963897705
    },
    {
      "epoch": 0.13008130081300814,
      "step": 600,
      "training_loss": 6.269057273864746
    },
    {
      "epoch": 0.13008130081300814,
      "step": 600,
      "training_loss": 7.459283828735352
    },
    {
      "epoch": 0.1302981029810298,
      "step": 601,
      "training_loss": 6.496366500854492
    },
    {
      "epoch": 0.1302981029810298,
      "step": 601,
      "training_loss": 7.6259870529174805
    },
    {
      "epoch": 0.1302981029810298,
      "step": 601,
      "training_loss": 7.222630023956299
    },
    {
      "epoch": 0.1302981029810298,
      "step": 601,
      "training_loss": 4.595189094543457
    },
    {
      "epoch": 0.13051490514905148,
      "step": 602,
      "training_loss": 6.459635257720947
    },
    {
      "epoch": 0.13051490514905148,
      "step": 602,
      "training_loss": 7.300271987915039
    },
    {
      "epoch": 0.13051490514905148,
      "step": 602,
      "training_loss": 7.938019752502441
    },
    {
      "epoch": 0.13051490514905148,
      "step": 602,
      "training_loss": 7.450700283050537
    },
    {
      "epoch": 0.13073170731707318,
      "step": 603,
      "training_loss": 6.986081600189209
    },
    {
      "epoch": 0.13073170731707318,
      "step": 603,
      "training_loss": 6.876092433929443
    },
    {
      "epoch": 0.13073170731707318,
      "step": 603,
      "training_loss": 6.689446926116943
    },
    {
      "epoch": 0.13073170731707318,
      "step": 603,
      "training_loss": 6.550333023071289
    },
    {
      "epoch": 0.13094850948509484,
      "grad_norm": 11.35714340209961,
      "learning_rate": 1e-05,
      "loss": 6.9829,
      "step": 604
    },
    {
      "epoch": 0.13094850948509484,
      "step": 604,
      "training_loss": 8.482391357421875
    },
    {
      "epoch": 0.13094850948509484,
      "step": 604,
      "training_loss": 7.659276485443115
    },
    {
      "epoch": 0.13094850948509484,
      "step": 604,
      "training_loss": 6.602341651916504
    },
    {
      "epoch": 0.13094850948509484,
      "step": 604,
      "training_loss": 6.459796905517578
    },
    {
      "epoch": 0.13116531165311654,
      "step": 605,
      "training_loss": 6.957017421722412
    },
    {
      "epoch": 0.13116531165311654,
      "step": 605,
      "training_loss": 7.690162658691406
    },
    {
      "epoch": 0.13116531165311654,
      "step": 605,
      "training_loss": 6.408506870269775
    },
    {
      "epoch": 0.13116531165311654,
      "step": 605,
      "training_loss": 6.86550760269165
    },
    {
      "epoch": 0.1313821138211382,
      "step": 606,
      "training_loss": 6.713503837585449
    },
    {
      "epoch": 0.1313821138211382,
      "step": 606,
      "training_loss": 7.024291038513184
    },
    {
      "epoch": 0.1313821138211382,
      "step": 606,
      "training_loss": 7.087837219238281
    },
    {
      "epoch": 0.1313821138211382,
      "step": 606,
      "training_loss": 7.447344779968262
    },
    {
      "epoch": 0.1315989159891599,
      "step": 607,
      "training_loss": 7.4968976974487305
    },
    {
      "epoch": 0.1315989159891599,
      "step": 607,
      "training_loss": 5.823748588562012
    },
    {
      "epoch": 0.1315989159891599,
      "step": 607,
      "training_loss": 7.083538055419922
    },
    {
      "epoch": 0.1315989159891599,
      "step": 607,
      "training_loss": 7.291022777557373
    },
    {
      "epoch": 0.13181571815718157,
      "grad_norm": 9.522836685180664,
      "learning_rate": 1e-05,
      "loss": 7.0683,
      "step": 608
    },
    {
      "epoch": 0.13181571815718157,
      "step": 608,
      "training_loss": 5.065051078796387
    },
    {
      "epoch": 0.13181571815718157,
      "step": 608,
      "training_loss": 6.76629114151001
    },
    {
      "epoch": 0.13181571815718157,
      "step": 608,
      "training_loss": 6.796115875244141
    },
    {
      "epoch": 0.13181571815718157,
      "step": 608,
      "training_loss": 6.7855730056762695
    },
    {
      "epoch": 0.13203252032520324,
      "step": 609,
      "training_loss": 6.422372341156006
    },
    {
      "epoch": 0.13203252032520324,
      "step": 609,
      "training_loss": 6.726914882659912
    },
    {
      "epoch": 0.13203252032520324,
      "step": 609,
      "training_loss": 6.759197235107422
    },
    {
      "epoch": 0.13203252032520324,
      "step": 609,
      "training_loss": 6.070675849914551
    },
    {
      "epoch": 0.13224932249322494,
      "step": 610,
      "training_loss": 7.484208583831787
    },
    {
      "epoch": 0.13224932249322494,
      "step": 610,
      "training_loss": 7.3500895500183105
    },
    {
      "epoch": 0.13224932249322494,
      "step": 610,
      "training_loss": 9.198661804199219
    },
    {
      "epoch": 0.13224932249322494,
      "step": 610,
      "training_loss": 6.797843933105469
    },
    {
      "epoch": 0.1324661246612466,
      "step": 611,
      "training_loss": 7.661280632019043
    },
    {
      "epoch": 0.1324661246612466,
      "step": 611,
      "training_loss": 5.6993279457092285
    },
    {
      "epoch": 0.1324661246612466,
      "step": 611,
      "training_loss": 6.861638069152832
    },
    {
      "epoch": 0.1324661246612466,
      "step": 611,
      "training_loss": 6.979366779327393
    },
    {
      "epoch": 0.1326829268292683,
      "grad_norm": 10.889287948608398,
      "learning_rate": 1e-05,
      "loss": 6.839,
      "step": 612
    },
    {
      "epoch": 0.1326829268292683,
      "step": 612,
      "training_loss": 5.14585542678833
    },
    {
      "epoch": 0.1326829268292683,
      "step": 612,
      "training_loss": 7.222496509552002
    },
    {
      "epoch": 0.1326829268292683,
      "step": 612,
      "training_loss": 7.431121826171875
    },
    {
      "epoch": 0.1326829268292683,
      "step": 612,
      "training_loss": 6.594180583953857
    },
    {
      "epoch": 0.13289972899728997,
      "step": 613,
      "training_loss": 7.241272449493408
    },
    {
      "epoch": 0.13289972899728997,
      "step": 613,
      "training_loss": 6.408395767211914
    },
    {
      "epoch": 0.13289972899728997,
      "step": 613,
      "training_loss": 6.470536708831787
    },
    {
      "epoch": 0.13289972899728997,
      "step": 613,
      "training_loss": 6.869794845581055
    },
    {
      "epoch": 0.13311653116531166,
      "step": 614,
      "training_loss": 7.142079830169678
    },
    {
      "epoch": 0.13311653116531166,
      "step": 614,
      "training_loss": 4.778338432312012
    },
    {
      "epoch": 0.13311653116531166,
      "step": 614,
      "training_loss": 6.839430809020996
    },
    {
      "epoch": 0.13311653116531166,
      "step": 614,
      "training_loss": 7.591554164886475
    },
    {
      "epoch": 0.13333333333333333,
      "step": 615,
      "training_loss": 7.867498397827148
    },
    {
      "epoch": 0.13333333333333333,
      "step": 615,
      "training_loss": 7.643041610717773
    },
    {
      "epoch": 0.13333333333333333,
      "step": 615,
      "training_loss": 7.132882118225098
    },
    {
      "epoch": 0.13333333333333333,
      "step": 615,
      "training_loss": 6.760000705718994
    },
    {
      "epoch": 0.13355013550135503,
      "grad_norm": 10.478150367736816,
      "learning_rate": 1e-05,
      "loss": 6.8212,
      "step": 616
    },
    {
      "epoch": 0.13355013550135503,
      "step": 616,
      "training_loss": 5.865601539611816
    },
    {
      "epoch": 0.13355013550135503,
      "step": 616,
      "training_loss": 5.234901428222656
    },
    {
      "epoch": 0.13355013550135503,
      "step": 616,
      "training_loss": 5.943489074707031
    },
    {
      "epoch": 0.13355013550135503,
      "step": 616,
      "training_loss": 6.689693927764893
    },
    {
      "epoch": 0.1337669376693767,
      "step": 617,
      "training_loss": 7.263117790222168
    },
    {
      "epoch": 0.1337669376693767,
      "step": 617,
      "training_loss": 6.942456245422363
    },
    {
      "epoch": 0.1337669376693767,
      "step": 617,
      "training_loss": 7.431679725646973
    },
    {
      "epoch": 0.1337669376693767,
      "step": 617,
      "training_loss": 6.933259010314941
    },
    {
      "epoch": 0.13398373983739836,
      "step": 618,
      "training_loss": 7.739584922790527
    },
    {
      "epoch": 0.13398373983739836,
      "step": 618,
      "training_loss": 5.971092224121094
    },
    {
      "epoch": 0.13398373983739836,
      "step": 618,
      "training_loss": 6.856098175048828
    },
    {
      "epoch": 0.13398373983739836,
      "step": 618,
      "training_loss": 6.874795913696289
    },
    {
      "epoch": 0.13420054200542006,
      "step": 619,
      "training_loss": 8.106931686401367
    },
    {
      "epoch": 0.13420054200542006,
      "step": 619,
      "training_loss": 5.883533477783203
    },
    {
      "epoch": 0.13420054200542006,
      "step": 619,
      "training_loss": 7.702856540679932
    },
    {
      "epoch": 0.13420054200542006,
      "step": 619,
      "training_loss": 7.8888630867004395
    },
    {
      "epoch": 0.13441734417344173,
      "grad_norm": 13.515213966369629,
      "learning_rate": 1e-05,
      "loss": 6.833,
      "step": 620
    },
    {
      "epoch": 0.13441734417344173,
      "step": 620,
      "training_loss": 7.493080139160156
    },
    {
      "epoch": 0.13441734417344173,
      "step": 620,
      "training_loss": 8.029559135437012
    },
    {
      "epoch": 0.13441734417344173,
      "step": 620,
      "training_loss": 5.644326686859131
    },
    {
      "epoch": 0.13441734417344173,
      "step": 620,
      "training_loss": 7.40468168258667
    },
    {
      "epoch": 0.13463414634146342,
      "step": 621,
      "training_loss": 6.475581645965576
    },
    {
      "epoch": 0.13463414634146342,
      "step": 621,
      "training_loss": 6.589951515197754
    },
    {
      "epoch": 0.13463414634146342,
      "step": 621,
      "training_loss": 7.41038179397583
    },
    {
      "epoch": 0.13463414634146342,
      "step": 621,
      "training_loss": 5.194584369659424
    },
    {
      "epoch": 0.1348509485094851,
      "step": 622,
      "training_loss": 7.108349323272705
    },
    {
      "epoch": 0.1348509485094851,
      "step": 622,
      "training_loss": 7.190074443817139
    },
    {
      "epoch": 0.1348509485094851,
      "step": 622,
      "training_loss": 7.34209680557251
    },
    {
      "epoch": 0.1348509485094851,
      "step": 622,
      "training_loss": 7.3079705238342285
    },
    {
      "epoch": 0.1350677506775068,
      "step": 623,
      "training_loss": 7.0197553634643555
    },
    {
      "epoch": 0.1350677506775068,
      "step": 623,
      "training_loss": 7.253497123718262
    },
    {
      "epoch": 0.1350677506775068,
      "step": 623,
      "training_loss": 7.7808451652526855
    },
    {
      "epoch": 0.1350677506775068,
      "step": 623,
      "training_loss": 6.265605449676514
    },
    {
      "epoch": 0.13528455284552846,
      "grad_norm": 9.88961124420166,
      "learning_rate": 1e-05,
      "loss": 6.9694,
      "step": 624
    },
    {
      "epoch": 0.13528455284552846,
      "step": 624,
      "training_loss": 4.638489246368408
    },
    {
      "epoch": 0.13528455284552846,
      "step": 624,
      "training_loss": 7.167359352111816
    },
    {
      "epoch": 0.13528455284552846,
      "step": 624,
      "training_loss": 6.786139965057373
    },
    {
      "epoch": 0.13528455284552846,
      "step": 624,
      "training_loss": 6.291347980499268
    },
    {
      "epoch": 0.13550135501355012,
      "step": 625,
      "training_loss": 6.150607109069824
    },
    {
      "epoch": 0.13550135501355012,
      "step": 625,
      "training_loss": 7.256443500518799
    },
    {
      "epoch": 0.13550135501355012,
      "step": 625,
      "training_loss": 6.008245468139648
    },
    {
      "epoch": 0.13550135501355012,
      "step": 625,
      "training_loss": 7.202314376831055
    },
    {
      "epoch": 0.13571815718157182,
      "step": 626,
      "training_loss": 7.429245471954346
    },
    {
      "epoch": 0.13571815718157182,
      "step": 626,
      "training_loss": 5.385565757751465
    },
    {
      "epoch": 0.13571815718157182,
      "step": 626,
      "training_loss": 6.701971530914307
    },
    {
      "epoch": 0.13571815718157182,
      "step": 626,
      "training_loss": 6.3721394538879395
    },
    {
      "epoch": 0.1359349593495935,
      "step": 627,
      "training_loss": 6.661854267120361
    },
    {
      "epoch": 0.1359349593495935,
      "step": 627,
      "training_loss": 6.763976097106934
    },
    {
      "epoch": 0.1359349593495935,
      "step": 627,
      "training_loss": 7.7186737060546875
    },
    {
      "epoch": 0.1359349593495935,
      "step": 627,
      "training_loss": 6.819558620452881
    },
    {
      "epoch": 0.13615176151761518,
      "grad_norm": 12.620915412902832,
      "learning_rate": 1e-05,
      "loss": 6.5846,
      "step": 628
    },
    {
      "epoch": 0.13615176151761518,
      "step": 628,
      "training_loss": 4.91827392578125
    },
    {
      "epoch": 0.13615176151761518,
      "step": 628,
      "training_loss": 6.906396389007568
    },
    {
      "epoch": 0.13615176151761518,
      "step": 628,
      "training_loss": 7.8069562911987305
    },
    {
      "epoch": 0.13615176151761518,
      "step": 628,
      "training_loss": 6.830732822418213
    },
    {
      "epoch": 0.13636856368563685,
      "step": 629,
      "training_loss": 5.920076847076416
    },
    {
      "epoch": 0.13636856368563685,
      "step": 629,
      "training_loss": 7.752476692199707
    },
    {
      "epoch": 0.13636856368563685,
      "step": 629,
      "training_loss": 4.913300514221191
    },
    {
      "epoch": 0.13636856368563685,
      "step": 629,
      "training_loss": 6.019907474517822
    },
    {
      "epoch": 0.13658536585365855,
      "step": 630,
      "training_loss": 7.16345739364624
    },
    {
      "epoch": 0.13658536585365855,
      "step": 630,
      "training_loss": 7.476114749908447
    },
    {
      "epoch": 0.13658536585365855,
      "step": 630,
      "training_loss": 6.561827182769775
    },
    {
      "epoch": 0.13658536585365855,
      "step": 630,
      "training_loss": 6.481699466705322
    },
    {
      "epoch": 0.13680216802168021,
      "step": 631,
      "training_loss": 7.164887428283691
    },
    {
      "epoch": 0.13680216802168021,
      "step": 631,
      "training_loss": 8.485069274902344
    },
    {
      "epoch": 0.13680216802168021,
      "step": 631,
      "training_loss": 6.864321231842041
    },
    {
      "epoch": 0.13680216802168021,
      "step": 631,
      "training_loss": 7.502161979675293
    },
    {
      "epoch": 0.1370189701897019,
      "grad_norm": 14.266402244567871,
      "learning_rate": 1e-05,
      "loss": 6.798,
      "step": 632
    },
    {
      "epoch": 0.1370189701897019,
      "step": 632,
      "training_loss": 7.018377780914307
    },
    {
      "epoch": 0.1370189701897019,
      "step": 632,
      "training_loss": 8.07746410369873
    },
    {
      "epoch": 0.1370189701897019,
      "step": 632,
      "training_loss": 6.626797676086426
    },
    {
      "epoch": 0.1370189701897019,
      "step": 632,
      "training_loss": 6.568144798278809
    },
    {
      "epoch": 0.13723577235772358,
      "step": 633,
      "training_loss": 6.90077543258667
    },
    {
      "epoch": 0.13723577235772358,
      "step": 633,
      "training_loss": 7.4468865394592285
    },
    {
      "epoch": 0.13723577235772358,
      "step": 633,
      "training_loss": 8.528946876525879
    },
    {
      "epoch": 0.13723577235772358,
      "step": 633,
      "training_loss": 7.638576507568359
    },
    {
      "epoch": 0.13745257452574525,
      "step": 634,
      "training_loss": 6.612978458404541
    },
    {
      "epoch": 0.13745257452574525,
      "step": 634,
      "training_loss": 5.4723896980285645
    },
    {
      "epoch": 0.13745257452574525,
      "step": 634,
      "training_loss": 7.036005973815918
    },
    {
      "epoch": 0.13745257452574525,
      "step": 634,
      "training_loss": 7.685053825378418
    },
    {
      "epoch": 0.13766937669376694,
      "step": 635,
      "training_loss": 8.794412612915039
    },
    {
      "epoch": 0.13766937669376694,
      "step": 635,
      "training_loss": 7.155514717102051
    },
    {
      "epoch": 0.13766937669376694,
      "step": 635,
      "training_loss": 7.770578384399414
    },
    {
      "epoch": 0.13766937669376694,
      "step": 635,
      "training_loss": 7.056313514709473
    },
    {
      "epoch": 0.1378861788617886,
      "grad_norm": 15.15108871459961,
      "learning_rate": 1e-05,
      "loss": 7.2743,
      "step": 636
    },
    {
      "epoch": 0.1378861788617886,
      "step": 636,
      "training_loss": 7.903223991394043
    },
    {
      "epoch": 0.1378861788617886,
      "step": 636,
      "training_loss": 7.082657337188721
    },
    {
      "epoch": 0.1378861788617886,
      "step": 636,
      "training_loss": 6.381917476654053
    },
    {
      "epoch": 0.1378861788617886,
      "step": 636,
      "training_loss": 7.036535263061523
    },
    {
      "epoch": 0.1381029810298103,
      "step": 637,
      "training_loss": 6.320520401000977
    },
    {
      "epoch": 0.1381029810298103,
      "step": 637,
      "training_loss": 7.925241470336914
    },
    {
      "epoch": 0.1381029810298103,
      "step": 637,
      "training_loss": 7.63012170791626
    },
    {
      "epoch": 0.1381029810298103,
      "step": 637,
      "training_loss": 5.48276424407959
    },
    {
      "epoch": 0.13831978319783197,
      "step": 638,
      "training_loss": 4.854954242706299
    },
    {
      "epoch": 0.13831978319783197,
      "step": 638,
      "training_loss": 7.658401012420654
    },
    {
      "epoch": 0.13831978319783197,
      "step": 638,
      "training_loss": 7.201416015625
    },
    {
      "epoch": 0.13831978319783197,
      "step": 638,
      "training_loss": 7.080235481262207
    },
    {
      "epoch": 0.13853658536585367,
      "step": 639,
      "training_loss": 4.609606742858887
    },
    {
      "epoch": 0.13853658536585367,
      "step": 639,
      "training_loss": 4.459155082702637
    },
    {
      "epoch": 0.13853658536585367,
      "step": 639,
      "training_loss": 5.72221565246582
    },
    {
      "epoch": 0.13853658536585367,
      "step": 639,
      "training_loss": 5.549524307250977
    },
    {
      "epoch": 0.13875338753387534,
      "grad_norm": 17.132287979125977,
      "learning_rate": 1e-05,
      "loss": 6.4312,
      "step": 640
    },
    {
      "epoch": 0.13875338753387534,
      "step": 640,
      "training_loss": 6.863297462463379
    },
    {
      "epoch": 0.13875338753387534,
      "step": 640,
      "training_loss": 6.90893030166626
    },
    {
      "epoch": 0.13875338753387534,
      "step": 640,
      "training_loss": 6.900781154632568
    },
    {
      "epoch": 0.13875338753387534,
      "step": 640,
      "training_loss": 6.639620304107666
    },
    {
      "epoch": 0.138970189701897,
      "step": 641,
      "training_loss": 6.384894847869873
    },
    {
      "epoch": 0.138970189701897,
      "step": 641,
      "training_loss": 7.583239555358887
    },
    {
      "epoch": 0.138970189701897,
      "step": 641,
      "training_loss": 6.463252544403076
    },
    {
      "epoch": 0.138970189701897,
      "step": 641,
      "training_loss": 6.084088325500488
    },
    {
      "epoch": 0.1391869918699187,
      "step": 642,
      "training_loss": 7.597775936126709
    },
    {
      "epoch": 0.1391869918699187,
      "step": 642,
      "training_loss": 4.784132480621338
    },
    {
      "epoch": 0.1391869918699187,
      "step": 642,
      "training_loss": 7.400675296783447
    },
    {
      "epoch": 0.1391869918699187,
      "step": 642,
      "training_loss": 7.469593524932861
    },
    {
      "epoch": 0.13940379403794037,
      "step": 643,
      "training_loss": 7.616232395172119
    },
    {
      "epoch": 0.13940379403794037,
      "step": 643,
      "training_loss": 7.690462112426758
    },
    {
      "epoch": 0.13940379403794037,
      "step": 643,
      "training_loss": 8.170612335205078
    },
    {
      "epoch": 0.13940379403794037,
      "step": 643,
      "training_loss": 7.260654449462891
    },
    {
      "epoch": 0.13962059620596207,
      "grad_norm": 12.660036087036133,
      "learning_rate": 1e-05,
      "loss": 6.9886,
      "step": 644
    },
    {
      "epoch": 0.13962059620596207,
      "step": 644,
      "training_loss": 6.185309886932373
    },
    {
      "epoch": 0.13962059620596207,
      "step": 644,
      "training_loss": 8.054224967956543
    },
    {
      "epoch": 0.13962059620596207,
      "step": 644,
      "training_loss": 6.452949047088623
    },
    {
      "epoch": 0.13962059620596207,
      "step": 644,
      "training_loss": 7.184423446655273
    },
    {
      "epoch": 0.13983739837398373,
      "step": 645,
      "training_loss": 8.298617362976074
    },
    {
      "epoch": 0.13983739837398373,
      "step": 645,
      "training_loss": 7.043267250061035
    },
    {
      "epoch": 0.13983739837398373,
      "step": 645,
      "training_loss": 6.420306205749512
    },
    {
      "epoch": 0.13983739837398373,
      "step": 645,
      "training_loss": 7.517209053039551
    },
    {
      "epoch": 0.14005420054200543,
      "step": 646,
      "training_loss": 8.075942039489746
    },
    {
      "epoch": 0.14005420054200543,
      "step": 646,
      "training_loss": 6.853187084197998
    },
    {
      "epoch": 0.14005420054200543,
      "step": 646,
      "training_loss": 7.691099166870117
    },
    {
      "epoch": 0.14005420054200543,
      "step": 646,
      "training_loss": 9.097163200378418
    },
    {
      "epoch": 0.1402710027100271,
      "step": 647,
      "training_loss": 7.277780532836914
    },
    {
      "epoch": 0.1402710027100271,
      "step": 647,
      "training_loss": 7.5752058029174805
    },
    {
      "epoch": 0.1402710027100271,
      "step": 647,
      "training_loss": 4.775140762329102
    },
    {
      "epoch": 0.1402710027100271,
      "step": 647,
      "training_loss": 5.002701759338379
    },
    {
      "epoch": 0.1404878048780488,
      "grad_norm": 12.54897403717041,
      "learning_rate": 1e-05,
      "loss": 7.094,
      "step": 648
    },
    {
      "epoch": 0.1404878048780488,
      "step": 648,
      "training_loss": 7.231775283813477
    },
    {
      "epoch": 0.1404878048780488,
      "step": 648,
      "training_loss": 6.930691242218018
    },
    {
      "epoch": 0.1404878048780488,
      "step": 648,
      "training_loss": 7.501760959625244
    },
    {
      "epoch": 0.1404878048780488,
      "step": 648,
      "training_loss": 6.999504089355469
    },
    {
      "epoch": 0.14070460704607046,
      "step": 649,
      "training_loss": 6.22592306137085
    },
    {
      "epoch": 0.14070460704607046,
      "step": 649,
      "training_loss": 7.711659908294678
    },
    {
      "epoch": 0.14070460704607046,
      "step": 649,
      "training_loss": 6.867582321166992
    },
    {
      "epoch": 0.14070460704607046,
      "step": 649,
      "training_loss": 8.116166114807129
    },
    {
      "epoch": 0.14092140921409213,
      "step": 650,
      "training_loss": 6.12068510055542
    },
    {
      "epoch": 0.14092140921409213,
      "step": 650,
      "training_loss": 11.324902534484863
    },
    {
      "epoch": 0.14092140921409213,
      "step": 650,
      "training_loss": 6.078447341918945
    },
    {
      "epoch": 0.14092140921409213,
      "step": 650,
      "training_loss": 5.808701038360596
    },
    {
      "epoch": 0.14113821138211383,
      "step": 651,
      "training_loss": 6.522376537322998
    },
    {
      "epoch": 0.14113821138211383,
      "step": 651,
      "training_loss": 6.1648640632629395
    },
    {
      "epoch": 0.14113821138211383,
      "step": 651,
      "training_loss": 8.723386764526367
    },
    {
      "epoch": 0.14113821138211383,
      "step": 651,
      "training_loss": 6.361204147338867
    },
    {
      "epoch": 0.1413550135501355,
      "grad_norm": 13.213872909545898,
      "learning_rate": 1e-05,
      "loss": 7.1681,
      "step": 652
    },
    {
      "epoch": 0.1413550135501355,
      "step": 652,
      "training_loss": 4.724933624267578
    },
    {
      "epoch": 0.1413550135501355,
      "step": 652,
      "training_loss": 6.150960922241211
    },
    {
      "epoch": 0.1413550135501355,
      "step": 652,
      "training_loss": 7.410021781921387
    },
    {
      "epoch": 0.1413550135501355,
      "step": 652,
      "training_loss": 6.949731826782227
    },
    {
      "epoch": 0.1415718157181572,
      "step": 653,
      "training_loss": 7.432483673095703
    },
    {
      "epoch": 0.1415718157181572,
      "step": 653,
      "training_loss": 6.997818470001221
    },
    {
      "epoch": 0.1415718157181572,
      "step": 653,
      "training_loss": 6.808399677276611
    },
    {
      "epoch": 0.1415718157181572,
      "step": 653,
      "training_loss": 6.378239631652832
    },
    {
      "epoch": 0.14178861788617886,
      "step": 654,
      "training_loss": 7.6558685302734375
    },
    {
      "epoch": 0.14178861788617886,
      "step": 654,
      "training_loss": 7.429409503936768
    },
    {
      "epoch": 0.14178861788617886,
      "step": 654,
      "training_loss": 8.769356727600098
    },
    {
      "epoch": 0.14178861788617886,
      "step": 654,
      "training_loss": 11.217374801635742
    },
    {
      "epoch": 0.14200542005420055,
      "step": 655,
      "training_loss": 6.3385233879089355
    },
    {
      "epoch": 0.14200542005420055,
      "step": 655,
      "training_loss": 4.452700614929199
    },
    {
      "epoch": 0.14200542005420055,
      "step": 655,
      "training_loss": 6.270654678344727
    },
    {
      "epoch": 0.14200542005420055,
      "step": 655,
      "training_loss": 7.462324619293213
    },
    {
      "epoch": 0.14222222222222222,
      "grad_norm": 10.430987358093262,
      "learning_rate": 1e-05,
      "loss": 7.028,
      "step": 656
    },
    {
      "epoch": 0.14222222222222222,
      "step": 656,
      "training_loss": 7.445666790008545
    },
    {
      "epoch": 0.14222222222222222,
      "step": 656,
      "training_loss": 6.580256938934326
    },
    {
      "epoch": 0.14222222222222222,
      "step": 656,
      "training_loss": 6.522145748138428
    },
    {
      "epoch": 0.14222222222222222,
      "step": 656,
      "training_loss": 7.427690505981445
    },
    {
      "epoch": 0.1424390243902439,
      "step": 657,
      "training_loss": 6.947932720184326
    },
    {
      "epoch": 0.1424390243902439,
      "step": 657,
      "training_loss": 4.8682379722595215
    },
    {
      "epoch": 0.1424390243902439,
      "step": 657,
      "training_loss": 7.524588584899902
    },
    {
      "epoch": 0.1424390243902439,
      "step": 657,
      "training_loss": 5.330218315124512
    },
    {
      "epoch": 0.14265582655826559,
      "step": 658,
      "training_loss": 4.045657157897949
    },
    {
      "epoch": 0.14265582655826559,
      "step": 658,
      "training_loss": 7.713953495025635
    },
    {
      "epoch": 0.14265582655826559,
      "step": 658,
      "training_loss": 6.4903717041015625
    },
    {
      "epoch": 0.14265582655826559,
      "step": 658,
      "training_loss": 7.746514797210693
    },
    {
      "epoch": 0.14287262872628725,
      "step": 659,
      "training_loss": 7.028337478637695
    },
    {
      "epoch": 0.14287262872628725,
      "step": 659,
      "training_loss": 6.393481254577637
    },
    {
      "epoch": 0.14287262872628725,
      "step": 659,
      "training_loss": 7.127564907073975
    },
    {
      "epoch": 0.14287262872628725,
      "step": 659,
      "training_loss": 7.679161071777344
    },
    {
      "epoch": 0.14308943089430895,
      "grad_norm": 12.273449897766113,
      "learning_rate": 1e-05,
      "loss": 6.6795,
      "step": 660
    },
    {
      "epoch": 0.14308943089430895,
      "step": 660,
      "training_loss": 7.345773696899414
    },
    {
      "epoch": 0.14308943089430895,
      "step": 660,
      "training_loss": 7.563815593719482
    },
    {
      "epoch": 0.14308943089430895,
      "step": 660,
      "training_loss": 7.473140716552734
    },
    {
      "epoch": 0.14308943089430895,
      "step": 660,
      "training_loss": 7.125513553619385
    },
    {
      "epoch": 0.14330623306233062,
      "step": 661,
      "training_loss": 7.440817356109619
    },
    {
      "epoch": 0.14330623306233062,
      "step": 661,
      "training_loss": 6.790949821472168
    },
    {
      "epoch": 0.14330623306233062,
      "step": 661,
      "training_loss": 6.805210590362549
    },
    {
      "epoch": 0.14330623306233062,
      "step": 661,
      "training_loss": 4.915271759033203
    },
    {
      "epoch": 0.1435230352303523,
      "step": 662,
      "training_loss": 7.309330940246582
    },
    {
      "epoch": 0.1435230352303523,
      "step": 662,
      "training_loss": 6.515159606933594
    },
    {
      "epoch": 0.1435230352303523,
      "step": 662,
      "training_loss": 7.353512763977051
    },
    {
      "epoch": 0.1435230352303523,
      "step": 662,
      "training_loss": 6.481134414672852
    },
    {
      "epoch": 0.14373983739837398,
      "step": 663,
      "training_loss": 6.901337146759033
    },
    {
      "epoch": 0.14373983739837398,
      "step": 663,
      "training_loss": 5.565469264984131
    },
    {
      "epoch": 0.14373983739837398,
      "step": 663,
      "training_loss": 6.900045871734619
    },
    {
      "epoch": 0.14373983739837398,
      "step": 663,
      "training_loss": 7.12789249420166
    },
    {
      "epoch": 0.14395663956639568,
      "grad_norm": 10.922796249389648,
      "learning_rate": 1e-05,
      "loss": 6.8509,
      "step": 664
    },
    {
      "epoch": 0.14395663956639568,
      "step": 664,
      "training_loss": 6.702150344848633
    },
    {
      "epoch": 0.14395663956639568,
      "step": 664,
      "training_loss": 7.345467567443848
    },
    {
      "epoch": 0.14395663956639568,
      "step": 664,
      "training_loss": 7.212191581726074
    },
    {
      "epoch": 0.14395663956639568,
      "step": 664,
      "training_loss": 6.3989763259887695
    },
    {
      "epoch": 0.14417344173441735,
      "step": 665,
      "training_loss": 7.330689430236816
    },
    {
      "epoch": 0.14417344173441735,
      "step": 665,
      "training_loss": 6.918613910675049
    },
    {
      "epoch": 0.14417344173441735,
      "step": 665,
      "training_loss": 7.94482946395874
    },
    {
      "epoch": 0.14417344173441735,
      "step": 665,
      "training_loss": 6.269953727722168
    },
    {
      "epoch": 0.144390243902439,
      "step": 666,
      "training_loss": 6.565276145935059
    },
    {
      "epoch": 0.144390243902439,
      "step": 666,
      "training_loss": 5.912130355834961
    },
    {
      "epoch": 0.144390243902439,
      "step": 666,
      "training_loss": 7.11903190612793
    },
    {
      "epoch": 0.144390243902439,
      "step": 666,
      "training_loss": 7.980705261230469
    },
    {
      "epoch": 0.1446070460704607,
      "step": 667,
      "training_loss": 7.238470554351807
    },
    {
      "epoch": 0.1446070460704607,
      "step": 667,
      "training_loss": 6.733363151550293
    },
    {
      "epoch": 0.1446070460704607,
      "step": 667,
      "training_loss": 6.019313335418701
    },
    {
      "epoch": 0.1446070460704607,
      "step": 667,
      "training_loss": 6.65116548538208
    },
    {
      "epoch": 0.14482384823848238,
      "grad_norm": 11.85120964050293,
      "learning_rate": 1e-05,
      "loss": 6.8964,
      "step": 668
    },
    {
      "epoch": 0.14482384823848238,
      "step": 668,
      "training_loss": 5.111878395080566
    },
    {
      "epoch": 0.14482384823848238,
      "step": 668,
      "training_loss": 7.459550380706787
    },
    {
      "epoch": 0.14482384823848238,
      "step": 668,
      "training_loss": 6.787946701049805
    },
    {
      "epoch": 0.14482384823848238,
      "step": 668,
      "training_loss": 7.4111104011535645
    },
    {
      "epoch": 0.14504065040650407,
      "step": 669,
      "training_loss": 4.9582133293151855
    },
    {
      "epoch": 0.14504065040650407,
      "step": 669,
      "training_loss": 7.1794352531433105
    },
    {
      "epoch": 0.14504065040650407,
      "step": 669,
      "training_loss": 7.38616418838501
    },
    {
      "epoch": 0.14504065040650407,
      "step": 669,
      "training_loss": 5.536322593688965
    },
    {
      "epoch": 0.14525745257452574,
      "step": 670,
      "training_loss": 6.034509181976318
    },
    {
      "epoch": 0.14525745257452574,
      "step": 670,
      "training_loss": 7.221184253692627
    },
    {
      "epoch": 0.14525745257452574,
      "step": 670,
      "training_loss": 7.422224521636963
    },
    {
      "epoch": 0.14525745257452574,
      "step": 670,
      "training_loss": 7.95207405090332
    },
    {
      "epoch": 0.14547425474254744,
      "step": 671,
      "training_loss": 7.011399269104004
    },
    {
      "epoch": 0.14547425474254744,
      "step": 671,
      "training_loss": 5.906545162200928
    },
    {
      "epoch": 0.14547425474254744,
      "step": 671,
      "training_loss": 7.007485389709473
    },
    {
      "epoch": 0.14547425474254744,
      "step": 671,
      "training_loss": 7.882785797119141
    },
    {
      "epoch": 0.1456910569105691,
      "grad_norm": 9.791339874267578,
      "learning_rate": 1e-05,
      "loss": 6.7668,
      "step": 672
    },
    {
      "epoch": 0.1456910569105691,
      "step": 672,
      "training_loss": 6.192446708679199
    },
    {
      "epoch": 0.1456910569105691,
      "step": 672,
      "training_loss": 7.413188457489014
    },
    {
      "epoch": 0.1456910569105691,
      "step": 672,
      "training_loss": 5.272552967071533
    },
    {
      "epoch": 0.1456910569105691,
      "step": 672,
      "training_loss": 7.405897617340088
    },
    {
      "epoch": 0.14590785907859077,
      "step": 673,
      "training_loss": 6.416623115539551
    },
    {
      "epoch": 0.14590785907859077,
      "step": 673,
      "training_loss": 7.022449016571045
    },
    {
      "epoch": 0.14590785907859077,
      "step": 673,
      "training_loss": 6.8308844566345215
    },
    {
      "epoch": 0.14590785907859077,
      "step": 673,
      "training_loss": 6.69262170791626
    },
    {
      "epoch": 0.14612466124661247,
      "step": 674,
      "training_loss": 7.570789813995361
    },
    {
      "epoch": 0.14612466124661247,
      "step": 674,
      "training_loss": 6.494484901428223
    },
    {
      "epoch": 0.14612466124661247,
      "step": 674,
      "training_loss": 6.957488059997559
    },
    {
      "epoch": 0.14612466124661247,
      "step": 674,
      "training_loss": 7.297667980194092
    },
    {
      "epoch": 0.14634146341463414,
      "step": 675,
      "training_loss": 5.6827826499938965
    },
    {
      "epoch": 0.14634146341463414,
      "step": 675,
      "training_loss": 6.802709579467773
    },
    {
      "epoch": 0.14634146341463414,
      "step": 675,
      "training_loss": 7.2079057693481445
    },
    {
      "epoch": 0.14634146341463414,
      "step": 675,
      "training_loss": 7.643395900726318
    },
    {
      "epoch": 0.14655826558265583,
      "grad_norm": 10.182442665100098,
      "learning_rate": 1e-05,
      "loss": 6.8065,
      "step": 676
    },
    {
      "epoch": 0.14655826558265583,
      "step": 676,
      "training_loss": 6.130342960357666
    },
    {
      "epoch": 0.14655826558265583,
      "step": 676,
      "training_loss": 7.231035232543945
    },
    {
      "epoch": 0.14655826558265583,
      "step": 676,
      "training_loss": 6.800984859466553
    },
    {
      "epoch": 0.14655826558265583,
      "step": 676,
      "training_loss": 7.864853858947754
    },
    {
      "epoch": 0.1467750677506775,
      "step": 677,
      "training_loss": 7.323034763336182
    },
    {
      "epoch": 0.1467750677506775,
      "step": 677,
      "training_loss": 6.8287034034729
    },
    {
      "epoch": 0.1467750677506775,
      "step": 677,
      "training_loss": 6.710616588592529
    },
    {
      "epoch": 0.1467750677506775,
      "step": 677,
      "training_loss": 5.549870491027832
    },
    {
      "epoch": 0.1469918699186992,
      "step": 678,
      "training_loss": 7.338355541229248
    },
    {
      "epoch": 0.1469918699186992,
      "step": 678,
      "training_loss": 7.421346664428711
    },
    {
      "epoch": 0.1469918699186992,
      "step": 678,
      "training_loss": 6.921695709228516
    },
    {
      "epoch": 0.1469918699186992,
      "step": 678,
      "training_loss": 7.262184143066406
    },
    {
      "epoch": 0.14720867208672087,
      "step": 679,
      "training_loss": 4.681844234466553
    },
    {
      "epoch": 0.14720867208672087,
      "step": 679,
      "training_loss": 7.301377773284912
    },
    {
      "epoch": 0.14720867208672087,
      "step": 679,
      "training_loss": 7.193334579467773
    },
    {
      "epoch": 0.14720867208672087,
      "step": 679,
      "training_loss": 9.608847618103027
    },
    {
      "epoch": 0.14742547425474256,
      "grad_norm": 9.68793773651123,
      "learning_rate": 1e-05,
      "loss": 7.0105,
      "step": 680
    },
    {
      "epoch": 0.14742547425474256,
      "step": 680,
      "training_loss": 7.386789798736572
    },
    {
      "epoch": 0.14742547425474256,
      "step": 680,
      "training_loss": 7.660863399505615
    },
    {
      "epoch": 0.14742547425474256,
      "step": 680,
      "training_loss": 7.378231048583984
    },
    {
      "epoch": 0.14742547425474256,
      "step": 680,
      "training_loss": 7.028947353363037
    },
    {
      "epoch": 0.14764227642276423,
      "step": 681,
      "training_loss": 6.795446872711182
    },
    {
      "epoch": 0.14764227642276423,
      "step": 681,
      "training_loss": 6.483525276184082
    },
    {
      "epoch": 0.14764227642276423,
      "step": 681,
      "training_loss": 6.213660717010498
    },
    {
      "epoch": 0.14764227642276423,
      "step": 681,
      "training_loss": 6.619238376617432
    },
    {
      "epoch": 0.1478590785907859,
      "step": 682,
      "training_loss": 4.581815719604492
    },
    {
      "epoch": 0.1478590785907859,
      "step": 682,
      "training_loss": 7.505136966705322
    },
    {
      "epoch": 0.1478590785907859,
      "step": 682,
      "training_loss": 7.747395992279053
    },
    {
      "epoch": 0.1478590785907859,
      "step": 682,
      "training_loss": 7.623763084411621
    },
    {
      "epoch": 0.1480758807588076,
      "step": 683,
      "training_loss": 9.297009468078613
    },
    {
      "epoch": 0.1480758807588076,
      "step": 683,
      "training_loss": 6.862890720367432
    },
    {
      "epoch": 0.1480758807588076,
      "step": 683,
      "training_loss": 7.828448295593262
    },
    {
      "epoch": 0.1480758807588076,
      "step": 683,
      "training_loss": 7.37946081161499
    },
    {
      "epoch": 0.14829268292682926,
      "grad_norm": 13.35322093963623,
      "learning_rate": 1e-05,
      "loss": 7.1495,
      "step": 684
    },
    {
      "epoch": 0.14829268292682926,
      "step": 684,
      "training_loss": 7.082295894622803
    },
    {
      "epoch": 0.14829268292682926,
      "step": 684,
      "training_loss": 6.725580215454102
    },
    {
      "epoch": 0.14829268292682926,
      "step": 684,
      "training_loss": 7.323364734649658
    },
    {
      "epoch": 0.14829268292682926,
      "step": 684,
      "training_loss": 7.149343013763428
    },
    {
      "epoch": 0.14850948509485096,
      "step": 685,
      "training_loss": 6.939088344573975
    },
    {
      "epoch": 0.14850948509485096,
      "step": 685,
      "training_loss": 6.04080057144165
    },
    {
      "epoch": 0.14850948509485096,
      "step": 685,
      "training_loss": 6.969913482666016
    },
    {
      "epoch": 0.14850948509485096,
      "step": 685,
      "training_loss": 4.669867992401123
    },
    {
      "epoch": 0.14872628726287263,
      "step": 686,
      "training_loss": 6.879676818847656
    },
    {
      "epoch": 0.14872628726287263,
      "step": 686,
      "training_loss": 7.637465000152588
    },
    {
      "epoch": 0.14872628726287263,
      "step": 686,
      "training_loss": 5.665716648101807
    },
    {
      "epoch": 0.14872628726287263,
      "step": 686,
      "training_loss": 6.17932653427124
    },
    {
      "epoch": 0.14894308943089432,
      "step": 687,
      "training_loss": 6.719881057739258
    },
    {
      "epoch": 0.14894308943089432,
      "step": 687,
      "training_loss": 7.207765579223633
    },
    {
      "epoch": 0.14894308943089432,
      "step": 687,
      "training_loss": 6.085336208343506
    },
    {
      "epoch": 0.14894308943089432,
      "step": 687,
      "training_loss": 6.693056106567383
    },
    {
      "epoch": 0.149159891598916,
      "grad_norm": 47.10881042480469,
      "learning_rate": 1e-05,
      "loss": 6.623,
      "step": 688
    },
    {
      "epoch": 0.149159891598916,
      "step": 688,
      "training_loss": 7.273576736450195
    },
    {
      "epoch": 0.149159891598916,
      "step": 688,
      "training_loss": 7.412854194641113
    },
    {
      "epoch": 0.149159891598916,
      "step": 688,
      "training_loss": 5.836642265319824
    },
    {
      "epoch": 0.149159891598916,
      "step": 688,
      "training_loss": 7.699892997741699
    },
    {
      "epoch": 0.14937669376693766,
      "step": 689,
      "training_loss": 6.812214374542236
    },
    {
      "epoch": 0.14937669376693766,
      "step": 689,
      "training_loss": 7.0383524894714355
    },
    {
      "epoch": 0.14937669376693766,
      "step": 689,
      "training_loss": 7.066812038421631
    },
    {
      "epoch": 0.14937669376693766,
      "step": 689,
      "training_loss": 7.734971046447754
    },
    {
      "epoch": 0.14959349593495935,
      "step": 690,
      "training_loss": 6.206684112548828
    },
    {
      "epoch": 0.14959349593495935,
      "step": 690,
      "training_loss": 4.662224769592285
    },
    {
      "epoch": 0.14959349593495935,
      "step": 690,
      "training_loss": 7.320678234100342
    },
    {
      "epoch": 0.14959349593495935,
      "step": 690,
      "training_loss": 7.25432825088501
    },
    {
      "epoch": 0.14981029810298102,
      "step": 691,
      "training_loss": 8.22890853881836
    },
    {
      "epoch": 0.14981029810298102,
      "step": 691,
      "training_loss": 6.3457465171813965
    },
    {
      "epoch": 0.14981029810298102,
      "step": 691,
      "training_loss": 7.627476215362549
    },
    {
      "epoch": 0.14981029810298102,
      "step": 691,
      "training_loss": 7.765125274658203
    },
    {
      "epoch": 0.15002710027100272,
      "grad_norm": 11.443584442138672,
      "learning_rate": 1e-05,
      "loss": 7.0179,
      "step": 692
    },
    {
      "epoch": 0.15002710027100272,
      "step": 692,
      "training_loss": 6.135953903198242
    },
    {
      "epoch": 0.15002710027100272,
      "step": 692,
      "training_loss": 7.6174540519714355
    },
    {
      "epoch": 0.15002710027100272,
      "step": 692,
      "training_loss": 7.55051851272583
    },
    {
      "epoch": 0.15002710027100272,
      "step": 692,
      "training_loss": 6.668249607086182
    },
    {
      "epoch": 0.15024390243902438,
      "step": 693,
      "training_loss": 6.767697811126709
    },
    {
      "epoch": 0.15024390243902438,
      "step": 693,
      "training_loss": 7.569443225860596
    },
    {
      "epoch": 0.15024390243902438,
      "step": 693,
      "training_loss": 5.333428859710693
    },
    {
      "epoch": 0.15024390243902438,
      "step": 693,
      "training_loss": 7.5420660972595215
    },
    {
      "epoch": 0.15046070460704608,
      "step": 694,
      "training_loss": 7.743038177490234
    },
    {
      "epoch": 0.15046070460704608,
      "step": 694,
      "training_loss": 6.292490005493164
    },
    {
      "epoch": 0.15046070460704608,
      "step": 694,
      "training_loss": 5.295091152191162
    },
    {
      "epoch": 0.15046070460704608,
      "step": 694,
      "training_loss": 7.8812408447265625
    },
    {
      "epoch": 0.15067750677506775,
      "step": 695,
      "training_loss": 6.2260284423828125
    },
    {
      "epoch": 0.15067750677506775,
      "step": 695,
      "training_loss": 7.4584245681762695
    },
    {
      "epoch": 0.15067750677506775,
      "step": 695,
      "training_loss": 7.598738193511963
    },
    {
      "epoch": 0.15067750677506775,
      "step": 695,
      "training_loss": 8.94509220123291
    },
    {
      "epoch": 0.15089430894308944,
      "grad_norm": 14.75003719329834,
      "learning_rate": 1e-05,
      "loss": 7.0391,
      "step": 696
    },
    {
      "epoch": 0.15089430894308944,
      "step": 696,
      "training_loss": 6.451839447021484
    },
    {
      "epoch": 0.15089430894308944,
      "step": 696,
      "training_loss": 7.83774471282959
    },
    {
      "epoch": 0.15089430894308944,
      "step": 696,
      "training_loss": 8.326704025268555
    },
    {
      "epoch": 0.15089430894308944,
      "step": 696,
      "training_loss": 5.6114325523376465
    },
    {
      "epoch": 0.1511111111111111,
      "step": 697,
      "training_loss": 6.593347072601318
    },
    {
      "epoch": 0.1511111111111111,
      "step": 697,
      "training_loss": 5.485723495483398
    },
    {
      "epoch": 0.1511111111111111,
      "step": 697,
      "training_loss": 5.698417663574219
    },
    {
      "epoch": 0.1511111111111111,
      "step": 697,
      "training_loss": 6.619605541229248
    },
    {
      "epoch": 0.15132791327913278,
      "step": 698,
      "training_loss": 6.923966407775879
    },
    {
      "epoch": 0.15132791327913278,
      "step": 698,
      "training_loss": 7.020344257354736
    },
    {
      "epoch": 0.15132791327913278,
      "step": 698,
      "training_loss": 6.259270668029785
    },
    {
      "epoch": 0.15132791327913278,
      "step": 698,
      "training_loss": 5.433775424957275
    },
    {
      "epoch": 0.15154471544715448,
      "step": 699,
      "training_loss": 6.841888904571533
    },
    {
      "epoch": 0.15154471544715448,
      "step": 699,
      "training_loss": 7.371780872344971
    },
    {
      "epoch": 0.15154471544715448,
      "step": 699,
      "training_loss": 6.694293022155762
    },
    {
      "epoch": 0.15154471544715448,
      "step": 699,
      "training_loss": 7.537586688995361
    },
    {
      "epoch": 0.15176151761517614,
      "grad_norm": 8.89802360534668,
      "learning_rate": 1e-05,
      "loss": 6.6692,
      "step": 700
    },
    {
      "epoch": 0.15176151761517614,
      "step": 700,
      "training_loss": 5.912881374359131
    },
    {
      "epoch": 0.15176151761517614,
      "step": 700,
      "training_loss": 7.185011863708496
    },
    {
      "epoch": 0.15176151761517614,
      "step": 700,
      "training_loss": 8.225882530212402
    },
    {
      "epoch": 0.15176151761517614,
      "step": 700,
      "training_loss": 5.907773971557617
    },
    {
      "epoch": 0.15197831978319784,
      "step": 701,
      "training_loss": 6.386635780334473
    },
    {
      "epoch": 0.15197831978319784,
      "step": 701,
      "training_loss": 6.59149694442749
    },
    {
      "epoch": 0.15197831978319784,
      "step": 701,
      "training_loss": 7.075887203216553
    },
    {
      "epoch": 0.15197831978319784,
      "step": 701,
      "training_loss": 8.084000587463379
    },
    {
      "epoch": 0.1521951219512195,
      "step": 702,
      "training_loss": 7.643258571624756
    },
    {
      "epoch": 0.1521951219512195,
      "step": 702,
      "training_loss": 8.075459480285645
    },
    {
      "epoch": 0.1521951219512195,
      "step": 702,
      "training_loss": 6.150611400604248
    },
    {
      "epoch": 0.1521951219512195,
      "step": 702,
      "training_loss": 6.423486232757568
    },
    {
      "epoch": 0.1524119241192412,
      "step": 703,
      "training_loss": 5.234172344207764
    },
    {
      "epoch": 0.1524119241192412,
      "step": 703,
      "training_loss": 5.3656110763549805
    },
    {
      "epoch": 0.1524119241192412,
      "step": 703,
      "training_loss": 6.543145179748535
    },
    {
      "epoch": 0.1524119241192412,
      "step": 703,
      "training_loss": 6.141666889190674
    },
    {
      "epoch": 0.15262872628726287,
      "grad_norm": 13.051966667175293,
      "learning_rate": 1e-05,
      "loss": 6.6842,
      "step": 704
    },
    {
      "epoch": 0.15262872628726287,
      "step": 704,
      "training_loss": 7.4162468910217285
    },
    {
      "epoch": 0.15262872628726287,
      "step": 704,
      "training_loss": 5.326960563659668
    },
    {
      "epoch": 0.15262872628726287,
      "step": 704,
      "training_loss": 6.70935583114624
    },
    {
      "epoch": 0.15262872628726287,
      "step": 704,
      "training_loss": 6.99198055267334
    },
    {
      "epoch": 0.15284552845528454,
      "step": 705,
      "training_loss": 7.306004524230957
    },
    {
      "epoch": 0.15284552845528454,
      "step": 705,
      "training_loss": 6.814359664916992
    },
    {
      "epoch": 0.15284552845528454,
      "step": 705,
      "training_loss": 6.972079753875732
    },
    {
      "epoch": 0.15284552845528454,
      "step": 705,
      "training_loss": 6.60076904296875
    },
    {
      "epoch": 0.15306233062330624,
      "step": 706,
      "training_loss": 8.02338981628418
    },
    {
      "epoch": 0.15306233062330624,
      "step": 706,
      "training_loss": 7.417742729187012
    },
    {
      "epoch": 0.15306233062330624,
      "step": 706,
      "training_loss": 5.789424419403076
    },
    {
      "epoch": 0.15306233062330624,
      "step": 706,
      "training_loss": 7.638497352600098
    },
    {
      "epoch": 0.1532791327913279,
      "step": 707,
      "training_loss": 6.643157482147217
    },
    {
      "epoch": 0.1532791327913279,
      "step": 707,
      "training_loss": 7.853382110595703
    },
    {
      "epoch": 0.1532791327913279,
      "step": 707,
      "training_loss": 7.903920650482178
    },
    {
      "epoch": 0.1532791327913279,
      "step": 707,
      "training_loss": 6.351140022277832
    },
    {
      "epoch": 0.1534959349593496,
      "grad_norm": 10.142842292785645,
      "learning_rate": 1e-05,
      "loss": 6.9849,
      "step": 708
    },
    {
      "epoch": 0.1534959349593496,
      "step": 708,
      "training_loss": 7.660791873931885
    },
    {
      "epoch": 0.1534959349593496,
      "step": 708,
      "training_loss": 6.972989082336426
    },
    {
      "epoch": 0.1534959349593496,
      "step": 708,
      "training_loss": 6.195489406585693
    },
    {
      "epoch": 0.1534959349593496,
      "step": 708,
      "training_loss": 6.857264518737793
    },
    {
      "epoch": 0.15371273712737127,
      "step": 709,
      "training_loss": 7.275468349456787
    },
    {
      "epoch": 0.15371273712737127,
      "step": 709,
      "training_loss": 6.959506511688232
    },
    {
      "epoch": 0.15371273712737127,
      "step": 709,
      "training_loss": 7.000723838806152
    },
    {
      "epoch": 0.15371273712737127,
      "step": 709,
      "training_loss": 5.948845386505127
    },
    {
      "epoch": 0.15392953929539296,
      "step": 710,
      "training_loss": 6.873884201049805
    },
    {
      "epoch": 0.15392953929539296,
      "step": 710,
      "training_loss": 7.353370189666748
    },
    {
      "epoch": 0.15392953929539296,
      "step": 710,
      "training_loss": 7.147772789001465
    },
    {
      "epoch": 0.15392953929539296,
      "step": 710,
      "training_loss": 6.290768146514893
    },
    {
      "epoch": 0.15414634146341463,
      "step": 711,
      "training_loss": 7.228436470031738
    },
    {
      "epoch": 0.15414634146341463,
      "step": 711,
      "training_loss": 6.879688739776611
    },
    {
      "epoch": 0.15414634146341463,
      "step": 711,
      "training_loss": 5.497870445251465
    },
    {
      "epoch": 0.15414634146341463,
      "step": 711,
      "training_loss": 8.037400245666504
    },
    {
      "epoch": 0.15436314363143633,
      "grad_norm": 17.339763641357422,
      "learning_rate": 1e-05,
      "loss": 6.8863,
      "step": 712
    },
    {
      "epoch": 0.15436314363143633,
      "step": 712,
      "training_loss": 6.5455780029296875
    },
    {
      "epoch": 0.15436314363143633,
      "step": 712,
      "training_loss": 7.300222396850586
    },
    {
      "epoch": 0.15436314363143633,
      "step": 712,
      "training_loss": 6.128946781158447
    },
    {
      "epoch": 0.15436314363143633,
      "step": 712,
      "training_loss": 8.202308654785156
    },
    {
      "epoch": 0.154579945799458,
      "step": 713,
      "training_loss": 6.79846715927124
    },
    {
      "epoch": 0.154579945799458,
      "step": 713,
      "training_loss": 7.028829097747803
    },
    {
      "epoch": 0.154579945799458,
      "step": 713,
      "training_loss": 7.628298759460449
    },
    {
      "epoch": 0.154579945799458,
      "step": 713,
      "training_loss": 6.488275527954102
    },
    {
      "epoch": 0.15479674796747966,
      "step": 714,
      "training_loss": 7.762573719024658
    },
    {
      "epoch": 0.15479674796747966,
      "step": 714,
      "training_loss": 6.995081901550293
    },
    {
      "epoch": 0.15479674796747966,
      "step": 714,
      "training_loss": 6.385814189910889
    },
    {
      "epoch": 0.15479674796747966,
      "step": 714,
      "training_loss": 6.4013895988464355
    },
    {
      "epoch": 0.15501355013550136,
      "step": 715,
      "training_loss": 6.958819389343262
    },
    {
      "epoch": 0.15501355013550136,
      "step": 715,
      "training_loss": 6.091724872589111
    },
    {
      "epoch": 0.15501355013550136,
      "step": 715,
      "training_loss": 6.955604076385498
    },
    {
      "epoch": 0.15501355013550136,
      "step": 715,
      "training_loss": 6.561294078826904
    },
    {
      "epoch": 0.15523035230352303,
      "grad_norm": 9.738065719604492,
      "learning_rate": 1e-05,
      "loss": 6.8896,
      "step": 716
    },
    {
      "epoch": 0.15523035230352303,
      "step": 716,
      "training_loss": 7.077799320220947
    },
    {
      "epoch": 0.15523035230352303,
      "step": 716,
      "training_loss": 7.096444606781006
    },
    {
      "epoch": 0.15523035230352303,
      "step": 716,
      "training_loss": 7.360388278961182
    },
    {
      "epoch": 0.15523035230352303,
      "step": 716,
      "training_loss": 6.522125720977783
    },
    {
      "epoch": 0.15544715447154472,
      "step": 717,
      "training_loss": 6.299814224243164
    },
    {
      "epoch": 0.15544715447154472,
      "step": 717,
      "training_loss": 6.4812517166137695
    },
    {
      "epoch": 0.15544715447154472,
      "step": 717,
      "training_loss": 6.45324182510376
    },
    {
      "epoch": 0.15544715447154472,
      "step": 717,
      "training_loss": 6.862557888031006
    },
    {
      "epoch": 0.1556639566395664,
      "step": 718,
      "training_loss": 7.374262809753418
    },
    {
      "epoch": 0.1556639566395664,
      "step": 718,
      "training_loss": 5.969536781311035
    },
    {
      "epoch": 0.1556639566395664,
      "step": 718,
      "training_loss": 6.6483659744262695
    },
    {
      "epoch": 0.1556639566395664,
      "step": 718,
      "training_loss": 7.494802474975586
    },
    {
      "epoch": 0.1558807588075881,
      "step": 719,
      "training_loss": 5.979668617248535
    },
    {
      "epoch": 0.1558807588075881,
      "step": 719,
      "training_loss": 4.546652793884277
    },
    {
      "epoch": 0.1558807588075881,
      "step": 719,
      "training_loss": 9.714264869689941
    },
    {
      "epoch": 0.1558807588075881,
      "step": 719,
      "training_loss": 6.747882843017578
    },
    {
      "epoch": 0.15609756097560976,
      "grad_norm": 11.062800407409668,
      "learning_rate": 1e-05,
      "loss": 6.7893,
      "step": 720
    },
    {
      "epoch": 0.15609756097560976,
      "step": 720,
      "training_loss": 6.5013556480407715
    },
    {
      "epoch": 0.15609756097560976,
      "step": 720,
      "training_loss": 7.806737899780273
    },
    {
      "epoch": 0.15609756097560976,
      "step": 720,
      "training_loss": 7.346515655517578
    },
    {
      "epoch": 0.15609756097560976,
      "step": 720,
      "training_loss": 5.654427528381348
    },
    {
      "epoch": 0.15631436314363142,
      "step": 721,
      "training_loss": 6.602355003356934
    },
    {
      "epoch": 0.15631436314363142,
      "step": 721,
      "training_loss": 6.058169841766357
    },
    {
      "epoch": 0.15631436314363142,
      "step": 721,
      "training_loss": 7.056898593902588
    },
    {
      "epoch": 0.15631436314363142,
      "step": 721,
      "training_loss": 6.886296272277832
    },
    {
      "epoch": 0.15653116531165312,
      "step": 722,
      "training_loss": 7.917225360870361
    },
    {
      "epoch": 0.15653116531165312,
      "step": 722,
      "training_loss": 4.0642595291137695
    },
    {
      "epoch": 0.15653116531165312,
      "step": 722,
      "training_loss": 6.922091484069824
    },
    {
      "epoch": 0.15653116531165312,
      "step": 722,
      "training_loss": 6.291625022888184
    },
    {
      "epoch": 0.1567479674796748,
      "step": 723,
      "training_loss": 7.902331352233887
    },
    {
      "epoch": 0.1567479674796748,
      "step": 723,
      "training_loss": 6.631643772125244
    },
    {
      "epoch": 0.1567479674796748,
      "step": 723,
      "training_loss": 6.603214263916016
    },
    {
      "epoch": 0.1567479674796748,
      "step": 723,
      "training_loss": 6.186952114105225
    },
    {
      "epoch": 0.15696476964769648,
      "grad_norm": 12.47742748260498,
      "learning_rate": 1e-05,
      "loss": 6.652,
      "step": 724
    },
    {
      "epoch": 0.15696476964769648,
      "step": 724,
      "training_loss": 5.618281364440918
    },
    {
      "epoch": 0.15696476964769648,
      "step": 724,
      "training_loss": 6.427001476287842
    },
    {
      "epoch": 0.15696476964769648,
      "step": 724,
      "training_loss": 7.374691963195801
    },
    {
      "epoch": 0.15696476964769648,
      "step": 724,
      "training_loss": 6.590901851654053
    },
    {
      "epoch": 0.15718157181571815,
      "step": 725,
      "training_loss": 7.017581939697266
    },
    {
      "epoch": 0.15718157181571815,
      "step": 725,
      "training_loss": 7.556588172912598
    },
    {
      "epoch": 0.15718157181571815,
      "step": 725,
      "training_loss": 5.257073879241943
    },
    {
      "epoch": 0.15718157181571815,
      "step": 725,
      "training_loss": 6.67094612121582
    },
    {
      "epoch": 0.15739837398373985,
      "step": 726,
      "training_loss": 6.672422885894775
    },
    {
      "epoch": 0.15739837398373985,
      "step": 726,
      "training_loss": 6.868688583374023
    },
    {
      "epoch": 0.15739837398373985,
      "step": 726,
      "training_loss": 7.268359184265137
    },
    {
      "epoch": 0.15739837398373985,
      "step": 726,
      "training_loss": 5.6907958984375
    },
    {
      "epoch": 0.15761517615176152,
      "step": 727,
      "training_loss": 8.723210334777832
    },
    {
      "epoch": 0.15761517615176152,
      "step": 727,
      "training_loss": 7.500955581665039
    },
    {
      "epoch": 0.15761517615176152,
      "step": 727,
      "training_loss": 7.3448100090026855
    },
    {
      "epoch": 0.15761517615176152,
      "step": 727,
      "training_loss": 6.8215460777282715
    },
    {
      "epoch": 0.1578319783197832,
      "grad_norm": 16.675098419189453,
      "learning_rate": 1e-05,
      "loss": 6.8377,
      "step": 728
    },
    {
      "epoch": 0.1578319783197832,
      "step": 728,
      "training_loss": 6.603810787200928
    },
    {
      "epoch": 0.1578319783197832,
      "step": 728,
      "training_loss": 7.170165538787842
    },
    {
      "epoch": 0.1578319783197832,
      "step": 728,
      "training_loss": 7.366392612457275
    },
    {
      "epoch": 0.1578319783197832,
      "step": 728,
      "training_loss": 6.825237274169922
    },
    {
      "epoch": 0.15804878048780488,
      "step": 729,
      "training_loss": 8.645187377929688
    },
    {
      "epoch": 0.15804878048780488,
      "step": 729,
      "training_loss": 7.556667804718018
    },
    {
      "epoch": 0.15804878048780488,
      "step": 729,
      "training_loss": 6.695419788360596
    },
    {
      "epoch": 0.15804878048780488,
      "step": 729,
      "training_loss": 7.773565292358398
    },
    {
      "epoch": 0.15826558265582655,
      "step": 730,
      "training_loss": 7.057063579559326
    },
    {
      "epoch": 0.15826558265582655,
      "step": 730,
      "training_loss": 5.960569858551025
    },
    {
      "epoch": 0.15826558265582655,
      "step": 730,
      "training_loss": 6.786688804626465
    },
    {
      "epoch": 0.15826558265582655,
      "step": 730,
      "training_loss": 6.808221340179443
    },
    {
      "epoch": 0.15848238482384824,
      "step": 731,
      "training_loss": 5.929818153381348
    },
    {
      "epoch": 0.15848238482384824,
      "step": 731,
      "training_loss": 7.123842716217041
    },
    {
      "epoch": 0.15848238482384824,
      "step": 731,
      "training_loss": 5.950440883636475
    },
    {
      "epoch": 0.15848238482384824,
      "step": 731,
      "training_loss": 6.7391533851623535
    },
    {
      "epoch": 0.1586991869918699,
      "grad_norm": 11.440272331237793,
      "learning_rate": 1e-05,
      "loss": 6.937,
      "step": 732
    },
    {
      "epoch": 0.1586991869918699,
      "step": 732,
      "training_loss": 7.718202114105225
    },
    {
      "epoch": 0.1586991869918699,
      "step": 732,
      "training_loss": 6.61843204498291
    },
    {
      "epoch": 0.1586991869918699,
      "step": 732,
      "training_loss": 5.345557689666748
    },
    {
      "epoch": 0.1586991869918699,
      "step": 732,
      "training_loss": 6.556666851043701
    },
    {
      "epoch": 0.1589159891598916,
      "step": 733,
      "training_loss": 8.090052604675293
    },
    {
      "epoch": 0.1589159891598916,
      "step": 733,
      "training_loss": 7.543970584869385
    },
    {
      "epoch": 0.1589159891598916,
      "step": 733,
      "training_loss": 6.4152116775512695
    },
    {
      "epoch": 0.1589159891598916,
      "step": 733,
      "training_loss": 8.222785949707031
    },
    {
      "epoch": 0.15913279132791328,
      "step": 734,
      "training_loss": 9.469035148620605
    },
    {
      "epoch": 0.15913279132791328,
      "step": 734,
      "training_loss": 7.447685241699219
    },
    {
      "epoch": 0.15913279132791328,
      "step": 734,
      "training_loss": 7.068941593170166
    },
    {
      "epoch": 0.15913279132791328,
      "step": 734,
      "training_loss": 6.767817974090576
    },
    {
      "epoch": 0.15934959349593497,
      "step": 735,
      "training_loss": 6.447711944580078
    },
    {
      "epoch": 0.15934959349593497,
      "step": 735,
      "training_loss": 4.998560428619385
    },
    {
      "epoch": 0.15934959349593497,
      "step": 735,
      "training_loss": 6.037599563598633
    },
    {
      "epoch": 0.15934959349593497,
      "step": 735,
      "training_loss": 7.206298351287842
    },
    {
      "epoch": 0.15956639566395664,
      "grad_norm": 17.754024505615234,
      "learning_rate": 1e-05,
      "loss": 6.9972,
      "step": 736
    },
    {
      "epoch": 0.15956639566395664,
      "step": 736,
      "training_loss": 8.319594383239746
    },
    {
      "epoch": 0.15956639566395664,
      "step": 736,
      "training_loss": 8.27192211151123
    },
    {
      "epoch": 0.15956639566395664,
      "step": 736,
      "training_loss": 7.16057825088501
    },
    {
      "epoch": 0.15956639566395664,
      "step": 736,
      "training_loss": 7.275721073150635
    },
    {
      "epoch": 0.1597831978319783,
      "step": 737,
      "training_loss": 6.05057954788208
    },
    {
      "epoch": 0.1597831978319783,
      "step": 737,
      "training_loss": 5.671435832977295
    },
    {
      "epoch": 0.1597831978319783,
      "step": 737,
      "training_loss": 7.594963550567627
    },
    {
      "epoch": 0.1597831978319783,
      "step": 737,
      "training_loss": 6.504634857177734
    },
    {
      "epoch": 0.16,
      "step": 738,
      "training_loss": 8.288406372070312
    },
    {
      "epoch": 0.16,
      "step": 738,
      "training_loss": 6.194538116455078
    },
    {
      "epoch": 0.16,
      "step": 738,
      "training_loss": 5.577808380126953
    },
    {
      "epoch": 0.16,
      "step": 738,
      "training_loss": 6.292362689971924
    },
    {
      "epoch": 0.16021680216802167,
      "step": 739,
      "training_loss": 7.386529445648193
    },
    {
      "epoch": 0.16021680216802167,
      "step": 739,
      "training_loss": 8.732232093811035
    },
    {
      "epoch": 0.16021680216802167,
      "step": 739,
      "training_loss": 7.743754863739014
    },
    {
      "epoch": 0.16021680216802167,
      "step": 739,
      "training_loss": 6.07351541519165
    },
    {
      "epoch": 0.16043360433604337,
      "grad_norm": 13.69426155090332,
      "learning_rate": 1e-05,
      "loss": 7.0712,
      "step": 740
    },
    {
      "epoch": 0.16043360433604337,
      "step": 740,
      "training_loss": 5.201635360717773
    },
    {
      "epoch": 0.16043360433604337,
      "step": 740,
      "training_loss": 7.21342658996582
    },
    {
      "epoch": 0.16043360433604337,
      "step": 740,
      "training_loss": 6.378462314605713
    },
    {
      "epoch": 0.16043360433604337,
      "step": 740,
      "training_loss": 6.7083740234375
    },
    {
      "epoch": 0.16065040650406504,
      "step": 741,
      "training_loss": 6.4136061668396
    },
    {
      "epoch": 0.16065040650406504,
      "step": 741,
      "training_loss": 7.7001142501831055
    },
    {
      "epoch": 0.16065040650406504,
      "step": 741,
      "training_loss": 6.666906833648682
    },
    {
      "epoch": 0.16065040650406504,
      "step": 741,
      "training_loss": 7.420842170715332
    },
    {
      "epoch": 0.16086720867208673,
      "step": 742,
      "training_loss": 7.377943992614746
    },
    {
      "epoch": 0.16086720867208673,
      "step": 742,
      "training_loss": 7.0058207511901855
    },
    {
      "epoch": 0.16086720867208673,
      "step": 742,
      "training_loss": 7.531010150909424
    },
    {
      "epoch": 0.16086720867208673,
      "step": 742,
      "training_loss": 6.086958885192871
    },
    {
      "epoch": 0.1610840108401084,
      "step": 743,
      "training_loss": 6.988089084625244
    },
    {
      "epoch": 0.1610840108401084,
      "step": 743,
      "training_loss": 7.106507301330566
    },
    {
      "epoch": 0.1610840108401084,
      "step": 743,
      "training_loss": 7.949800968170166
    },
    {
      "epoch": 0.1610840108401084,
      "step": 743,
      "training_loss": 7.908069610595703
    },
    {
      "epoch": 0.1613008130081301,
      "grad_norm": 10.80022144317627,
      "learning_rate": 1e-05,
      "loss": 6.9786,
      "step": 744
    },
    {
      "epoch": 0.1613008130081301,
      "step": 744,
      "training_loss": 5.185824394226074
    },
    {
      "epoch": 0.1613008130081301,
      "step": 744,
      "training_loss": 7.279666423797607
    },
    {
      "epoch": 0.1613008130081301,
      "step": 744,
      "training_loss": 6.719738960266113
    },
    {
      "epoch": 0.1613008130081301,
      "step": 744,
      "training_loss": 6.97107458114624
    },
    {
      "epoch": 0.16151761517615176,
      "step": 745,
      "training_loss": 7.603496074676514
    },
    {
      "epoch": 0.16151761517615176,
      "step": 745,
      "training_loss": 7.496981143951416
    },
    {
      "epoch": 0.16151761517615176,
      "step": 745,
      "training_loss": 7.641183853149414
    },
    {
      "epoch": 0.16151761517615176,
      "step": 745,
      "training_loss": 6.09628963470459
    },
    {
      "epoch": 0.16173441734417343,
      "step": 746,
      "training_loss": 5.736917972564697
    },
    {
      "epoch": 0.16173441734417343,
      "step": 746,
      "training_loss": 5.826615810394287
    },
    {
      "epoch": 0.16173441734417343,
      "step": 746,
      "training_loss": 7.0610175132751465
    },
    {
      "epoch": 0.16173441734417343,
      "step": 746,
      "training_loss": 7.836568355560303
    },
    {
      "epoch": 0.16195121951219513,
      "step": 747,
      "training_loss": 7.132564544677734
    },
    {
      "epoch": 0.16195121951219513,
      "step": 747,
      "training_loss": 4.875388145446777
    },
    {
      "epoch": 0.16195121951219513,
      "step": 747,
      "training_loss": 7.108310222625732
    },
    {
      "epoch": 0.16195121951219513,
      "step": 747,
      "training_loss": 7.6811604499816895
    },
    {
      "epoch": 0.1621680216802168,
      "grad_norm": 9.108129501342773,
      "learning_rate": 1e-05,
      "loss": 6.7658,
      "step": 748
    },
    {
      "epoch": 0.1621680216802168,
      "step": 748,
      "training_loss": 7.281104564666748
    },
    {
      "epoch": 0.1621680216802168,
      "step": 748,
      "training_loss": 8.000932693481445
    },
    {
      "epoch": 0.1621680216802168,
      "step": 748,
      "training_loss": 7.181814670562744
    },
    {
      "epoch": 0.1621680216802168,
      "step": 748,
      "training_loss": 6.530331611633301
    },
    {
      "epoch": 0.1623848238482385,
      "step": 749,
      "training_loss": 6.085318088531494
    },
    {
      "epoch": 0.1623848238482385,
      "step": 749,
      "training_loss": 6.217570781707764
    },
    {
      "epoch": 0.1623848238482385,
      "step": 749,
      "training_loss": 7.1552581787109375
    },
    {
      "epoch": 0.1623848238482385,
      "step": 749,
      "training_loss": 6.238675117492676
    },
    {
      "epoch": 0.16260162601626016,
      "step": 750,
      "training_loss": 5.963615417480469
    },
    {
      "epoch": 0.16260162601626016,
      "step": 750,
      "training_loss": 6.436392784118652
    },
    {
      "epoch": 0.16260162601626016,
      "step": 750,
      "training_loss": 5.403265953063965
    },
    {
      "epoch": 0.16260162601626016,
      "step": 750,
      "training_loss": 4.859384059906006
    },
    {
      "epoch": 0.16281842818428185,
      "step": 751,
      "training_loss": 7.1855549812316895
    },
    {
      "epoch": 0.16281842818428185,
      "step": 751,
      "training_loss": 6.072152137756348
    },
    {
      "epoch": 0.16281842818428185,
      "step": 751,
      "training_loss": 7.6776604652404785
    },
    {
      "epoch": 0.16281842818428185,
      "step": 751,
      "training_loss": 7.59786319732666
    },
    {
      "epoch": 0.16303523035230352,
      "grad_norm": 9.292708396911621,
      "learning_rate": 1e-05,
      "loss": 6.6179,
      "step": 752
    },
    {
      "epoch": 0.16303523035230352,
      "step": 752,
      "training_loss": 6.456784725189209
    },
    {
      "epoch": 0.16303523035230352,
      "step": 752,
      "training_loss": 8.206975936889648
    },
    {
      "epoch": 0.16303523035230352,
      "step": 752,
      "training_loss": 7.1364521980285645
    },
    {
      "epoch": 0.16303523035230352,
      "step": 752,
      "training_loss": 7.217615604400635
    },
    {
      "epoch": 0.1632520325203252,
      "step": 753,
      "training_loss": 6.520120143890381
    },
    {
      "epoch": 0.1632520325203252,
      "step": 753,
      "training_loss": 6.302980899810791
    },
    {
      "epoch": 0.1632520325203252,
      "step": 753,
      "training_loss": 5.575801849365234
    },
    {
      "epoch": 0.1632520325203252,
      "step": 753,
      "training_loss": 5.988945960998535
    },
    {
      "epoch": 0.1634688346883469,
      "step": 754,
      "training_loss": 6.773738384246826
    },
    {
      "epoch": 0.1634688346883469,
      "step": 754,
      "training_loss": 7.145087718963623
    },
    {
      "epoch": 0.1634688346883469,
      "step": 754,
      "training_loss": 6.998406887054443
    },
    {
      "epoch": 0.1634688346883469,
      "step": 754,
      "training_loss": 6.96101713180542
    },
    {
      "epoch": 0.16368563685636855,
      "step": 755,
      "training_loss": 7.494884014129639
    },
    {
      "epoch": 0.16368563685636855,
      "step": 755,
      "training_loss": 5.82757568359375
    },
    {
      "epoch": 0.16368563685636855,
      "step": 755,
      "training_loss": 6.995184421539307
    },
    {
      "epoch": 0.16368563685636855,
      "step": 755,
      "training_loss": 7.510707855224609
    },
    {
      "epoch": 0.16390243902439025,
      "grad_norm": 6.698128700256348,
      "learning_rate": 1e-05,
      "loss": 6.8195,
      "step": 756
    },
    {
      "epoch": 0.16390243902439025,
      "step": 756,
      "training_loss": 6.8647990226745605
    },
    {
      "epoch": 0.16390243902439025,
      "step": 756,
      "training_loss": 7.993642807006836
    },
    {
      "epoch": 0.16390243902439025,
      "step": 756,
      "training_loss": 6.656938076019287
    },
    {
      "epoch": 0.16390243902439025,
      "step": 756,
      "training_loss": 7.766968727111816
    },
    {
      "epoch": 0.16411924119241192,
      "step": 757,
      "training_loss": 7.319908142089844
    },
    {
      "epoch": 0.16411924119241192,
      "step": 757,
      "training_loss": 6.9297966957092285
    },
    {
      "epoch": 0.16411924119241192,
      "step": 757,
      "training_loss": 7.628816604614258
    },
    {
      "epoch": 0.16411924119241192,
      "step": 757,
      "training_loss": 6.620446681976318
    },
    {
      "epoch": 0.16433604336043361,
      "step": 758,
      "training_loss": 6.196506977081299
    },
    {
      "epoch": 0.16433604336043361,
      "step": 758,
      "training_loss": 7.406291961669922
    },
    {
      "epoch": 0.16433604336043361,
      "step": 758,
      "training_loss": 6.186891078948975
    },
    {
      "epoch": 0.16433604336043361,
      "step": 758,
      "training_loss": 7.983853340148926
    },
    {
      "epoch": 0.16455284552845528,
      "step": 759,
      "training_loss": 7.4763922691345215
    },
    {
      "epoch": 0.16455284552845528,
      "step": 759,
      "training_loss": 7.45306921005249
    },
    {
      "epoch": 0.16455284552845528,
      "step": 759,
      "training_loss": 6.373473644256592
    },
    {
      "epoch": 0.16455284552845528,
      "step": 759,
      "training_loss": 7.081961631774902
    },
    {
      "epoch": 0.16476964769647698,
      "grad_norm": 12.09217357635498,
      "learning_rate": 1e-05,
      "loss": 7.1212,
      "step": 760
    },
    {
      "epoch": 0.16476964769647698,
      "step": 760,
      "training_loss": 6.4387664794921875
    },
    {
      "epoch": 0.16476964769647698,
      "step": 760,
      "training_loss": 6.19108772277832
    },
    {
      "epoch": 0.16476964769647698,
      "step": 760,
      "training_loss": 7.613958835601807
    },
    {
      "epoch": 0.16476964769647698,
      "step": 760,
      "training_loss": 6.296429634094238
    },
    {
      "epoch": 0.16498644986449865,
      "step": 761,
      "training_loss": 6.568427562713623
    },
    {
      "epoch": 0.16498644986449865,
      "step": 761,
      "training_loss": 7.757421016693115
    },
    {
      "epoch": 0.16498644986449865,
      "step": 761,
      "training_loss": 5.9018144607543945
    },
    {
      "epoch": 0.16498644986449865,
      "step": 761,
      "training_loss": 7.737530708312988
    },
    {
      "epoch": 0.16520325203252031,
      "step": 762,
      "training_loss": 6.485808372497559
    },
    {
      "epoch": 0.16520325203252031,
      "step": 762,
      "training_loss": 4.516681671142578
    },
    {
      "epoch": 0.16520325203252031,
      "step": 762,
      "training_loss": 7.595330715179443
    },
    {
      "epoch": 0.16520325203252031,
      "step": 762,
      "training_loss": 7.400606155395508
    },
    {
      "epoch": 0.165420054200542,
      "step": 763,
      "training_loss": 7.774190425872803
    },
    {
      "epoch": 0.165420054200542,
      "step": 763,
      "training_loss": 7.133455753326416
    },
    {
      "epoch": 0.165420054200542,
      "step": 763,
      "training_loss": 7.157826900482178
    },
    {
      "epoch": 0.165420054200542,
      "step": 763,
      "training_loss": 7.642549514770508
    },
    {
      "epoch": 0.16563685636856368,
      "grad_norm": 10.697566032409668,
      "learning_rate": 1e-05,
      "loss": 6.8882,
      "step": 764
    },
    {
      "epoch": 0.16563685636856368,
      "step": 764,
      "training_loss": 7.237533092498779
    },
    {
      "epoch": 0.16563685636856368,
      "step": 764,
      "training_loss": 10.282132148742676
    },
    {
      "epoch": 0.16563685636856368,
      "step": 764,
      "training_loss": 7.622227191925049
    },
    {
      "epoch": 0.16563685636856368,
      "step": 764,
      "training_loss": 8.557581901550293
    },
    {
      "epoch": 0.16585365853658537,
      "step": 765,
      "training_loss": 6.595579624176025
    },
    {
      "epoch": 0.16585365853658537,
      "step": 765,
      "training_loss": 7.4077253341674805
    },
    {
      "epoch": 0.16585365853658537,
      "step": 765,
      "training_loss": 6.168924808502197
    },
    {
      "epoch": 0.16585365853658537,
      "step": 765,
      "training_loss": 6.319938659667969
    },
    {
      "epoch": 0.16607046070460704,
      "step": 766,
      "training_loss": 6.5923566818237305
    },
    {
      "epoch": 0.16607046070460704,
      "step": 766,
      "training_loss": 6.346169948577881
    },
    {
      "epoch": 0.16607046070460704,
      "step": 766,
      "training_loss": 6.2533345222473145
    },
    {
      "epoch": 0.16607046070460704,
      "step": 766,
      "training_loss": 7.07008695602417
    },
    {
      "epoch": 0.16628726287262874,
      "step": 767,
      "training_loss": 7.064291000366211
    },
    {
      "epoch": 0.16628726287262874,
      "step": 767,
      "training_loss": 6.936184883117676
    },
    {
      "epoch": 0.16628726287262874,
      "step": 767,
      "training_loss": 7.579033374786377
    },
    {
      "epoch": 0.16628726287262874,
      "step": 767,
      "training_loss": 8.525003433227539
    },
    {
      "epoch": 0.1665040650406504,
      "grad_norm": 7.779500484466553,
      "learning_rate": 1e-05,
      "loss": 7.2849,
      "step": 768
    },
    {
      "epoch": 0.1665040650406504,
      "step": 768,
      "training_loss": 7.689243793487549
    },
    {
      "epoch": 0.1665040650406504,
      "step": 768,
      "training_loss": 4.487087726593018
    },
    {
      "epoch": 0.1665040650406504,
      "step": 768,
      "training_loss": 6.71958589553833
    },
    {
      "epoch": 0.1665040650406504,
      "step": 768,
      "training_loss": 7.517817974090576
    },
    {
      "epoch": 0.16672086720867207,
      "step": 769,
      "training_loss": 6.907090663909912
    },
    {
      "epoch": 0.16672086720867207,
      "step": 769,
      "training_loss": 7.366576194763184
    },
    {
      "epoch": 0.16672086720867207,
      "step": 769,
      "training_loss": 5.708889484405518
    },
    {
      "epoch": 0.16672086720867207,
      "step": 769,
      "training_loss": 6.082359313964844
    },
    {
      "epoch": 0.16693766937669377,
      "step": 770,
      "training_loss": 7.122894763946533
    },
    {
      "epoch": 0.16693766937669377,
      "step": 770,
      "training_loss": 7.556682586669922
    },
    {
      "epoch": 0.16693766937669377,
      "step": 770,
      "training_loss": 8.218693733215332
    },
    {
      "epoch": 0.16693766937669377,
      "step": 770,
      "training_loss": 7.006978511810303
    },
    {
      "epoch": 0.16715447154471544,
      "step": 771,
      "training_loss": 6.48651123046875
    },
    {
      "epoch": 0.16715447154471544,
      "step": 771,
      "training_loss": 5.822145938873291
    },
    {
      "epoch": 0.16715447154471544,
      "step": 771,
      "training_loss": 5.642151832580566
    },
    {
      "epoch": 0.16715447154471544,
      "step": 771,
      "training_loss": 4.830153942108154
    },
    {
      "epoch": 0.16737127371273713,
      "grad_norm": 11.098732948303223,
      "learning_rate": 1e-05,
      "loss": 6.5728,
      "step": 772
    },
    {
      "epoch": 0.16737127371273713,
      "step": 772,
      "training_loss": 6.8327460289001465
    },
    {
      "epoch": 0.16737127371273713,
      "step": 772,
      "training_loss": 5.04060173034668
    },
    {
      "epoch": 0.16737127371273713,
      "step": 772,
      "training_loss": 8.175342559814453
    },
    {
      "epoch": 0.16737127371273713,
      "step": 772,
      "training_loss": 6.157895565032959
    },
    {
      "epoch": 0.1675880758807588,
      "step": 773,
      "training_loss": 7.0729899406433105
    },
    {
      "epoch": 0.1675880758807588,
      "step": 773,
      "training_loss": 6.336261749267578
    },
    {
      "epoch": 0.1675880758807588,
      "step": 773,
      "training_loss": 6.3633198738098145
    },
    {
      "epoch": 0.1675880758807588,
      "step": 773,
      "training_loss": 7.958156585693359
    },
    {
      "epoch": 0.1678048780487805,
      "step": 774,
      "training_loss": 7.5772480964660645
    },
    {
      "epoch": 0.1678048780487805,
      "step": 774,
      "training_loss": 7.092553615570068
    },
    {
      "epoch": 0.1678048780487805,
      "step": 774,
      "training_loss": 7.58123779296875
    },
    {
      "epoch": 0.1678048780487805,
      "step": 774,
      "training_loss": 7.923181056976318
    },
    {
      "epoch": 0.16802168021680217,
      "step": 775,
      "training_loss": 6.382556915283203
    },
    {
      "epoch": 0.16802168021680217,
      "step": 775,
      "training_loss": 7.184630870819092
    },
    {
      "epoch": 0.16802168021680217,
      "step": 775,
      "training_loss": 5.448554992675781
    },
    {
      "epoch": 0.16802168021680217,
      "step": 775,
      "training_loss": 5.889243125915527
    },
    {
      "epoch": 0.16823848238482386,
      "grad_norm": 11.701077461242676,
      "learning_rate": 1e-05,
      "loss": 6.8135,
      "step": 776
    },
    {
      "epoch": 0.16823848238482386,
      "step": 776,
      "training_loss": 6.794582843780518
    },
    {
      "epoch": 0.16823848238482386,
      "step": 776,
      "training_loss": 7.426935195922852
    },
    {
      "epoch": 0.16823848238482386,
      "step": 776,
      "training_loss": 7.369106769561768
    },
    {
      "epoch": 0.16823848238482386,
      "step": 776,
      "training_loss": 6.747346878051758
    },
    {
      "epoch": 0.16845528455284553,
      "step": 777,
      "training_loss": 7.802756309509277
    },
    {
      "epoch": 0.16845528455284553,
      "step": 777,
      "training_loss": 7.177994728088379
    },
    {
      "epoch": 0.16845528455284553,
      "step": 777,
      "training_loss": 7.427372455596924
    },
    {
      "epoch": 0.16845528455284553,
      "step": 777,
      "training_loss": 7.1714019775390625
    },
    {
      "epoch": 0.1686720867208672,
      "step": 778,
      "training_loss": 6.836748123168945
    },
    {
      "epoch": 0.1686720867208672,
      "step": 778,
      "training_loss": 7.1127238273620605
    },
    {
      "epoch": 0.1686720867208672,
      "step": 778,
      "training_loss": 7.337116241455078
    },
    {
      "epoch": 0.1686720867208672,
      "step": 778,
      "training_loss": 5.171647548675537
    },
    {
      "epoch": 0.1688888888888889,
      "step": 779,
      "training_loss": 6.110335350036621
    },
    {
      "epoch": 0.1688888888888889,
      "step": 779,
      "training_loss": 7.114761829376221
    },
    {
      "epoch": 0.1688888888888889,
      "step": 779,
      "training_loss": 7.077260494232178
    },
    {
      "epoch": 0.1688888888888889,
      "step": 779,
      "training_loss": 5.728100776672363
    },
    {
      "epoch": 0.16910569105691056,
      "grad_norm": 14.815192222595215,
      "learning_rate": 1e-05,
      "loss": 6.9004,
      "step": 780
    },
    {
      "epoch": 0.16910569105691056,
      "step": 780,
      "training_loss": 6.3157639503479
    },
    {
      "epoch": 0.16910569105691056,
      "step": 780,
      "training_loss": 7.472921848297119
    },
    {
      "epoch": 0.16910569105691056,
      "step": 780,
      "training_loss": 7.324349403381348
    },
    {
      "epoch": 0.16910569105691056,
      "step": 780,
      "training_loss": 7.8752360343933105
    },
    {
      "epoch": 0.16932249322493226,
      "step": 781,
      "training_loss": 6.99735164642334
    },
    {
      "epoch": 0.16932249322493226,
      "step": 781,
      "training_loss": 7.744672775268555
    },
    {
      "epoch": 0.16932249322493226,
      "step": 781,
      "training_loss": 4.359806060791016
    },
    {
      "epoch": 0.16932249322493226,
      "step": 781,
      "training_loss": 6.467365741729736
    },
    {
      "epoch": 0.16953929539295393,
      "step": 782,
      "training_loss": 5.457208633422852
    },
    {
      "epoch": 0.16953929539295393,
      "step": 782,
      "training_loss": 6.778303623199463
    },
    {
      "epoch": 0.16953929539295393,
      "step": 782,
      "training_loss": 7.575918197631836
    },
    {
      "epoch": 0.16953929539295393,
      "step": 782,
      "training_loss": 6.7361836433410645
    },
    {
      "epoch": 0.16975609756097562,
      "step": 783,
      "training_loss": 6.5033745765686035
    },
    {
      "epoch": 0.16975609756097562,
      "step": 783,
      "training_loss": 6.501191139221191
    },
    {
      "epoch": 0.16975609756097562,
      "step": 783,
      "training_loss": 6.593221187591553
    },
    {
      "epoch": 0.16975609756097562,
      "step": 783,
      "training_loss": 7.493416786193848
    },
    {
      "epoch": 0.1699728997289973,
      "grad_norm": 11.44506549835205,
      "learning_rate": 1e-05,
      "loss": 6.7623,
      "step": 784
    },
    {
      "epoch": 0.1699728997289973,
      "step": 784,
      "training_loss": 7.087179183959961
    },
    {
      "epoch": 0.1699728997289973,
      "step": 784,
      "training_loss": 7.008302211761475
    },
    {
      "epoch": 0.1699728997289973,
      "step": 784,
      "training_loss": 6.6568603515625
    },
    {
      "epoch": 0.1699728997289973,
      "step": 784,
      "training_loss": 5.167121887207031
    },
    {
      "epoch": 0.17018970189701896,
      "step": 785,
      "training_loss": 7.7731523513793945
    },
    {
      "epoch": 0.17018970189701896,
      "step": 785,
      "training_loss": 7.032493591308594
    },
    {
      "epoch": 0.17018970189701896,
      "step": 785,
      "training_loss": 6.516013145446777
    },
    {
      "epoch": 0.17018970189701896,
      "step": 785,
      "training_loss": 7.9322075843811035
    },
    {
      "epoch": 0.17040650406504065,
      "step": 786,
      "training_loss": 7.055087566375732
    },
    {
      "epoch": 0.17040650406504065,
      "step": 786,
      "training_loss": 5.6387786865234375
    },
    {
      "epoch": 0.17040650406504065,
      "step": 786,
      "training_loss": 7.066140174865723
    },
    {
      "epoch": 0.17040650406504065,
      "step": 786,
      "training_loss": 8.344456672668457
    },
    {
      "epoch": 0.17062330623306232,
      "step": 787,
      "training_loss": 7.075167655944824
    },
    {
      "epoch": 0.17062330623306232,
      "step": 787,
      "training_loss": 6.786641597747803
    },
    {
      "epoch": 0.17062330623306232,
      "step": 787,
      "training_loss": 6.1951422691345215
    },
    {
      "epoch": 0.17062330623306232,
      "step": 787,
      "training_loss": 7.641579627990723
    },
    {
      "epoch": 0.17084010840108402,
      "grad_norm": 10.400982856750488,
      "learning_rate": 1e-05,
      "loss": 6.936,
      "step": 788
    },
    {
      "epoch": 0.17084010840108402,
      "step": 788,
      "training_loss": 5.8062920570373535
    },
    {
      "epoch": 0.17084010840108402,
      "step": 788,
      "training_loss": 6.245115756988525
    },
    {
      "epoch": 0.17084010840108402,
      "step": 788,
      "training_loss": 4.980648040771484
    },
    {
      "epoch": 0.17084010840108402,
      "step": 788,
      "training_loss": 7.123615264892578
    },
    {
      "epoch": 0.17105691056910569,
      "step": 789,
      "training_loss": 7.517327785491943
    },
    {
      "epoch": 0.17105691056910569,
      "step": 789,
      "training_loss": 4.356700897216797
    },
    {
      "epoch": 0.17105691056910569,
      "step": 789,
      "training_loss": 6.852986812591553
    },
    {
      "epoch": 0.17105691056910569,
      "step": 789,
      "training_loss": 7.712071418762207
    },
    {
      "epoch": 0.17127371273712738,
      "step": 790,
      "training_loss": 4.320352077484131
    },
    {
      "epoch": 0.17127371273712738,
      "step": 790,
      "training_loss": 6.2018961906433105
    },
    {
      "epoch": 0.17127371273712738,
      "step": 790,
      "training_loss": 6.317287921905518
    },
    {
      "epoch": 0.17127371273712738,
      "step": 790,
      "training_loss": 6.801633358001709
    },
    {
      "epoch": 0.17149051490514905,
      "step": 791,
      "training_loss": 5.944613456726074
    },
    {
      "epoch": 0.17149051490514905,
      "step": 791,
      "training_loss": 7.222477912902832
    },
    {
      "epoch": 0.17149051490514905,
      "step": 791,
      "training_loss": 7.618494033813477
    },
    {
      "epoch": 0.17149051490514905,
      "step": 791,
      "training_loss": 6.287044525146484
    },
    {
      "epoch": 0.17170731707317075,
      "grad_norm": 10.564165115356445,
      "learning_rate": 1e-05,
      "loss": 6.3318,
      "step": 792
    },
    {
      "epoch": 0.17170731707317075,
      "step": 792,
      "training_loss": 7.063281059265137
    },
    {
      "epoch": 0.17170731707317075,
      "step": 792,
      "training_loss": 5.467628002166748
    },
    {
      "epoch": 0.17170731707317075,
      "step": 792,
      "training_loss": 7.6177191734313965
    },
    {
      "epoch": 0.17170731707317075,
      "step": 792,
      "training_loss": 5.923655986785889
    },
    {
      "epoch": 0.1719241192411924,
      "step": 793,
      "training_loss": 6.763174057006836
    },
    {
      "epoch": 0.1719241192411924,
      "step": 793,
      "training_loss": 6.821995258331299
    },
    {
      "epoch": 0.1719241192411924,
      "step": 793,
      "training_loss": 4.957284927368164
    },
    {
      "epoch": 0.1719241192411924,
      "step": 793,
      "training_loss": 7.67620325088501
    },
    {
      "epoch": 0.17214092140921408,
      "step": 794,
      "training_loss": 7.391430377960205
    },
    {
      "epoch": 0.17214092140921408,
      "step": 794,
      "training_loss": 6.606963157653809
    },
    {
      "epoch": 0.17214092140921408,
      "step": 794,
      "training_loss": 7.843196392059326
    },
    {
      "epoch": 0.17214092140921408,
      "step": 794,
      "training_loss": 8.622920989990234
    },
    {
      "epoch": 0.17235772357723578,
      "step": 795,
      "training_loss": 7.551662445068359
    },
    {
      "epoch": 0.17235772357723578,
      "step": 795,
      "training_loss": 5.807357311248779
    },
    {
      "epoch": 0.17235772357723578,
      "step": 795,
      "training_loss": 6.185600280761719
    },
    {
      "epoch": 0.17235772357723578,
      "step": 795,
      "training_loss": 6.349977493286133
    },
    {
      "epoch": 0.17257452574525745,
      "grad_norm": 12.182005882263184,
      "learning_rate": 1e-05,
      "loss": 6.7906,
      "step": 796
    },
    {
      "epoch": 0.17257452574525745,
      "step": 796,
      "training_loss": 7.576204776763916
    },
    {
      "epoch": 0.17257452574525745,
      "step": 796,
      "training_loss": 5.481775760650635
    },
    {
      "epoch": 0.17257452574525745,
      "step": 796,
      "training_loss": 8.116432189941406
    },
    {
      "epoch": 0.17257452574525745,
      "step": 796,
      "training_loss": 5.683465003967285
    },
    {
      "epoch": 0.17279132791327914,
      "step": 797,
      "training_loss": 4.499591827392578
    },
    {
      "epoch": 0.17279132791327914,
      "step": 797,
      "training_loss": 7.104742050170898
    },
    {
      "epoch": 0.17279132791327914,
      "step": 797,
      "training_loss": 6.3323445320129395
    },
    {
      "epoch": 0.17279132791327914,
      "step": 797,
      "training_loss": 6.3186821937561035
    },
    {
      "epoch": 0.1730081300813008,
      "step": 798,
      "training_loss": 6.466822624206543
    },
    {
      "epoch": 0.1730081300813008,
      "step": 798,
      "training_loss": 6.8523783683776855
    },
    {
      "epoch": 0.1730081300813008,
      "step": 798,
      "training_loss": 5.594944000244141
    },
    {
      "epoch": 0.1730081300813008,
      "step": 798,
      "training_loss": 7.609063148498535
    },
    {
      "epoch": 0.1732249322493225,
      "step": 799,
      "training_loss": 10.267594337463379
    },
    {
      "epoch": 0.1732249322493225,
      "step": 799,
      "training_loss": 7.0936198234558105
    },
    {
      "epoch": 0.1732249322493225,
      "step": 799,
      "training_loss": 6.514724254608154
    },
    {
      "epoch": 0.1732249322493225,
      "step": 799,
      "training_loss": 6.4710469245910645
    },
    {
      "epoch": 0.17344173441734417,
      "grad_norm": 23.31578826904297,
      "learning_rate": 1e-05,
      "loss": 6.749,
      "step": 800
    },
    {
      "epoch": 0.17344173441734417,
      "step": 800,
      "training_loss": 6.842626571655273
    },
    {
      "epoch": 0.17344173441734417,
      "step": 800,
      "training_loss": 6.628035545349121
    },
    {
      "epoch": 0.17344173441734417,
      "step": 800,
      "training_loss": 6.131931781768799
    },
    {
      "epoch": 0.17344173441734417,
      "step": 800,
      "training_loss": 6.916749000549316
    },
    {
      "epoch": 0.17365853658536584,
      "step": 801,
      "training_loss": 6.296395301818848
    },
    {
      "epoch": 0.17365853658536584,
      "step": 801,
      "training_loss": 5.860847473144531
    },
    {
      "epoch": 0.17365853658536584,
      "step": 801,
      "training_loss": 7.819273948669434
    },
    {
      "epoch": 0.17365853658536584,
      "step": 801,
      "training_loss": 6.251369953155518
    },
    {
      "epoch": 0.17387533875338754,
      "step": 802,
      "training_loss": 5.96967077255249
    },
    {
      "epoch": 0.17387533875338754,
      "step": 802,
      "training_loss": 7.150206089019775
    },
    {
      "epoch": 0.17387533875338754,
      "step": 802,
      "training_loss": 6.836250305175781
    },
    {
      "epoch": 0.17387533875338754,
      "step": 802,
      "training_loss": 6.565985202789307
    },
    {
      "epoch": 0.1740921409214092,
      "step": 803,
      "training_loss": 5.712703227996826
    },
    {
      "epoch": 0.1740921409214092,
      "step": 803,
      "training_loss": 7.198022842407227
    },
    {
      "epoch": 0.1740921409214092,
      "step": 803,
      "training_loss": 7.193417072296143
    },
    {
      "epoch": 0.1740921409214092,
      "step": 803,
      "training_loss": 7.814006805419922
    },
    {
      "epoch": 0.1743089430894309,
      "grad_norm": 9.046106338500977,
      "learning_rate": 1e-05,
      "loss": 6.6992,
      "step": 804
    },
    {
      "epoch": 0.1743089430894309,
      "step": 804,
      "training_loss": 6.178389072418213
    },
    {
      "epoch": 0.1743089430894309,
      "step": 804,
      "training_loss": 6.273148536682129
    },
    {
      "epoch": 0.1743089430894309,
      "step": 804,
      "training_loss": 6.936293125152588
    },
    {
      "epoch": 0.1743089430894309,
      "step": 804,
      "training_loss": 7.434547424316406
    },
    {
      "epoch": 0.17452574525745257,
      "step": 805,
      "training_loss": 7.289594650268555
    },
    {
      "epoch": 0.17452574525745257,
      "step": 805,
      "training_loss": 7.647544860839844
    },
    {
      "epoch": 0.17452574525745257,
      "step": 805,
      "training_loss": 6.3909125328063965
    },
    {
      "epoch": 0.17452574525745257,
      "step": 805,
      "training_loss": 6.283392429351807
    },
    {
      "epoch": 0.17474254742547426,
      "step": 806,
      "training_loss": 6.263301372528076
    },
    {
      "epoch": 0.17474254742547426,
      "step": 806,
      "training_loss": 7.253014087677002
    },
    {
      "epoch": 0.17474254742547426,
      "step": 806,
      "training_loss": 7.89410924911499
    },
    {
      "epoch": 0.17474254742547426,
      "step": 806,
      "training_loss": 6.972642421722412
    },
    {
      "epoch": 0.17495934959349593,
      "step": 807,
      "training_loss": 5.806665420532227
    },
    {
      "epoch": 0.17495934959349593,
      "step": 807,
      "training_loss": 6.913209915161133
    },
    {
      "epoch": 0.17495934959349593,
      "step": 807,
      "training_loss": 6.943569660186768
    },
    {
      "epoch": 0.17495934959349593,
      "step": 807,
      "training_loss": 6.323540687561035
    },
    {
      "epoch": 0.17517615176151763,
      "grad_norm": 13.573442459106445,
      "learning_rate": 1e-05,
      "loss": 6.8002,
      "step": 808
    },
    {
      "epoch": 0.17517615176151763,
      "step": 808,
      "training_loss": 6.413756370544434
    },
    {
      "epoch": 0.17517615176151763,
      "step": 808,
      "training_loss": 6.986155033111572
    },
    {
      "epoch": 0.17517615176151763,
      "step": 808,
      "training_loss": 7.3499755859375
    },
    {
      "epoch": 0.17517615176151763,
      "step": 808,
      "training_loss": 7.277876853942871
    },
    {
      "epoch": 0.1753929539295393,
      "step": 809,
      "training_loss": 7.543823719024658
    },
    {
      "epoch": 0.1753929539295393,
      "step": 809,
      "training_loss": 5.698249340057373
    },
    {
      "epoch": 0.1753929539295393,
      "step": 809,
      "training_loss": 6.311072826385498
    },
    {
      "epoch": 0.1753929539295393,
      "step": 809,
      "training_loss": 5.739467620849609
    },
    {
      "epoch": 0.17560975609756097,
      "step": 810,
      "training_loss": 6.705162048339844
    },
    {
      "epoch": 0.17560975609756097,
      "step": 810,
      "training_loss": 6.165484428405762
    },
    {
      "epoch": 0.17560975609756097,
      "step": 810,
      "training_loss": 6.355681419372559
    },
    {
      "epoch": 0.17560975609756097,
      "step": 810,
      "training_loss": 8.491955757141113
    },
    {
      "epoch": 0.17582655826558266,
      "step": 811,
      "training_loss": 6.118951797485352
    },
    {
      "epoch": 0.17582655826558266,
      "step": 811,
      "training_loss": 7.739119529724121
    },
    {
      "epoch": 0.17582655826558266,
      "step": 811,
      "training_loss": 5.774665832519531
    },
    {
      "epoch": 0.17582655826558266,
      "step": 811,
      "training_loss": 7.3380842208862305
    },
    {
      "epoch": 0.17604336043360433,
      "grad_norm": 9.504616737365723,
      "learning_rate": 1e-05,
      "loss": 6.7506,
      "step": 812
    },
    {
      "epoch": 0.17604336043360433,
      "step": 812,
      "training_loss": 7.887509346008301
    },
    {
      "epoch": 0.17604336043360433,
      "step": 812,
      "training_loss": 7.253437042236328
    },
    {
      "epoch": 0.17604336043360433,
      "step": 812,
      "training_loss": 4.444128513336182
    },
    {
      "epoch": 0.17604336043360433,
      "step": 812,
      "training_loss": 6.5769805908203125
    },
    {
      "epoch": 0.17626016260162602,
      "step": 813,
      "training_loss": 4.918615818023682
    },
    {
      "epoch": 0.17626016260162602,
      "step": 813,
      "training_loss": 7.349145889282227
    },
    {
      "epoch": 0.17626016260162602,
      "step": 813,
      "training_loss": 6.29245662689209
    },
    {
      "epoch": 0.17626016260162602,
      "step": 813,
      "training_loss": 7.291077136993408
    },
    {
      "epoch": 0.1764769647696477,
      "step": 814,
      "training_loss": 5.251978874206543
    },
    {
      "epoch": 0.1764769647696477,
      "step": 814,
      "training_loss": 6.808468818664551
    },
    {
      "epoch": 0.1764769647696477,
      "step": 814,
      "training_loss": 7.010383605957031
    },
    {
      "epoch": 0.1764769647696477,
      "step": 814,
      "training_loss": 7.392532825469971
    },
    {
      "epoch": 0.1766937669376694,
      "step": 815,
      "training_loss": 4.064798831939697
    },
    {
      "epoch": 0.1766937669376694,
      "step": 815,
      "training_loss": 7.352591514587402
    },
    {
      "epoch": 0.1766937669376694,
      "step": 815,
      "training_loss": 7.676690101623535
    },
    {
      "epoch": 0.1766937669376694,
      "step": 815,
      "training_loss": 7.705703258514404
    },
    {
      "epoch": 0.17691056910569106,
      "grad_norm": 8.580677032470703,
      "learning_rate": 1e-05,
      "loss": 6.5798,
      "step": 816
    },
    {
      "epoch": 0.17691056910569106,
      "step": 816,
      "training_loss": 6.655157089233398
    },
    {
      "epoch": 0.17691056910569106,
      "step": 816,
      "training_loss": 4.971314430236816
    },
    {
      "epoch": 0.17691056910569106,
      "step": 816,
      "training_loss": 7.006643772125244
    },
    {
      "epoch": 0.17691056910569106,
      "step": 816,
      "training_loss": 6.155038356781006
    },
    {
      "epoch": 0.17712737127371272,
      "step": 817,
      "training_loss": 5.023613452911377
    },
    {
      "epoch": 0.17712737127371272,
      "step": 817,
      "training_loss": 7.8280158042907715
    },
    {
      "epoch": 0.17712737127371272,
      "step": 817,
      "training_loss": 7.360953330993652
    },
    {
      "epoch": 0.17712737127371272,
      "step": 817,
      "training_loss": 5.747304439544678
    },
    {
      "epoch": 0.17734417344173442,
      "step": 818,
      "training_loss": 7.506671905517578
    },
    {
      "epoch": 0.17734417344173442,
      "step": 818,
      "training_loss": 7.124176025390625
    },
    {
      "epoch": 0.17734417344173442,
      "step": 818,
      "training_loss": 7.056155204772949
    },
    {
      "epoch": 0.17734417344173442,
      "step": 818,
      "training_loss": 7.829583644866943
    },
    {
      "epoch": 0.1775609756097561,
      "step": 819,
      "training_loss": 6.50831937789917
    },
    {
      "epoch": 0.1775609756097561,
      "step": 819,
      "training_loss": 7.410495758056641
    },
    {
      "epoch": 0.1775609756097561,
      "step": 819,
      "training_loss": 7.2520751953125
    },
    {
      "epoch": 0.1775609756097561,
      "step": 819,
      "training_loss": 7.311925888061523
    },
    {
      "epoch": 0.17777777777777778,
      "grad_norm": 8.74510383605957,
      "learning_rate": 1e-05,
      "loss": 6.7967,
      "step": 820
    },
    {
      "epoch": 0.17777777777777778,
      "step": 820,
      "training_loss": 6.823246002197266
    },
    {
      "epoch": 0.17777777777777778,
      "step": 820,
      "training_loss": 6.887869358062744
    },
    {
      "epoch": 0.17777777777777778,
      "step": 820,
      "training_loss": 7.367611885070801
    },
    {
      "epoch": 0.17777777777777778,
      "step": 820,
      "training_loss": 4.73960018157959
    },
    {
      "epoch": 0.17799457994579945,
      "step": 821,
      "training_loss": 7.002971172332764
    },
    {
      "epoch": 0.17799457994579945,
      "step": 821,
      "training_loss": 6.842121601104736
    },
    {
      "epoch": 0.17799457994579945,
      "step": 821,
      "training_loss": 6.412945747375488
    },
    {
      "epoch": 0.17799457994579945,
      "step": 821,
      "training_loss": 6.978895664215088
    },
    {
      "epoch": 0.17821138211382115,
      "step": 822,
      "training_loss": 6.607158184051514
    },
    {
      "epoch": 0.17821138211382115,
      "step": 822,
      "training_loss": 7.1181254386901855
    },
    {
      "epoch": 0.17821138211382115,
      "step": 822,
      "training_loss": 6.393167018890381
    },
    {
      "epoch": 0.17821138211382115,
      "step": 822,
      "training_loss": 6.474328517913818
    },
    {
      "epoch": 0.17842818428184282,
      "step": 823,
      "training_loss": 6.891255855560303
    },
    {
      "epoch": 0.17842818428184282,
      "step": 823,
      "training_loss": 7.0755934715271
    },
    {
      "epoch": 0.17842818428184282,
      "step": 823,
      "training_loss": 8.416621208190918
    },
    {
      "epoch": 0.17842818428184282,
      "step": 823,
      "training_loss": 5.076500415802002
    },
    {
      "epoch": 0.1786449864498645,
      "grad_norm": 12.756467819213867,
      "learning_rate": 1e-05,
      "loss": 6.6943,
      "step": 824
    },
    {
      "epoch": 0.1786449864498645,
      "step": 824,
      "training_loss": 6.7672929763793945
    },
    {
      "epoch": 0.1786449864498645,
      "step": 824,
      "training_loss": 7.320947170257568
    },
    {
      "epoch": 0.1786449864498645,
      "step": 824,
      "training_loss": 7.779383659362793
    },
    {
      "epoch": 0.1786449864498645,
      "step": 824,
      "training_loss": 5.462066173553467
    },
    {
      "epoch": 0.17886178861788618,
      "step": 825,
      "training_loss": 5.909123420715332
    },
    {
      "epoch": 0.17886178861788618,
      "step": 825,
      "training_loss": 8.51172924041748
    },
    {
      "epoch": 0.17886178861788618,
      "step": 825,
      "training_loss": 7.351067543029785
    },
    {
      "epoch": 0.17886178861788618,
      "step": 825,
      "training_loss": 6.586723327636719
    },
    {
      "epoch": 0.17907859078590785,
      "step": 826,
      "training_loss": 6.1647419929504395
    },
    {
      "epoch": 0.17907859078590785,
      "step": 826,
      "training_loss": 7.053569793701172
    },
    {
      "epoch": 0.17907859078590785,
      "step": 826,
      "training_loss": 7.514837741851807
    },
    {
      "epoch": 0.17907859078590785,
      "step": 826,
      "training_loss": 6.599621772766113
    },
    {
      "epoch": 0.17929539295392954,
      "step": 827,
      "training_loss": 6.8073811531066895
    },
    {
      "epoch": 0.17929539295392954,
      "step": 827,
      "training_loss": 7.93460750579834
    },
    {
      "epoch": 0.17929539295392954,
      "step": 827,
      "training_loss": 6.717708110809326
    },
    {
      "epoch": 0.17929539295392954,
      "step": 827,
      "training_loss": 7.390326023101807
    },
    {
      "epoch": 0.1795121951219512,
      "grad_norm": 10.108262062072754,
      "learning_rate": 1e-05,
      "loss": 6.9919,
      "step": 828
    },
    {
      "epoch": 0.1795121951219512,
      "step": 828,
      "training_loss": 7.309143543243408
    },
    {
      "epoch": 0.1795121951219512,
      "step": 828,
      "training_loss": 7.718020439147949
    },
    {
      "epoch": 0.1795121951219512,
      "step": 828,
      "training_loss": 7.489171028137207
    },
    {
      "epoch": 0.1795121951219512,
      "step": 828,
      "training_loss": 7.701061725616455
    },
    {
      "epoch": 0.1797289972899729,
      "step": 829,
      "training_loss": 6.567974090576172
    },
    {
      "epoch": 0.1797289972899729,
      "step": 829,
      "training_loss": 6.788607120513916
    },
    {
      "epoch": 0.1797289972899729,
      "step": 829,
      "training_loss": 7.724737644195557
    },
    {
      "epoch": 0.1797289972899729,
      "step": 829,
      "training_loss": 6.814694404602051
    },
    {
      "epoch": 0.17994579945799458,
      "step": 830,
      "training_loss": 7.438003063201904
    },
    {
      "epoch": 0.17994579945799458,
      "step": 830,
      "training_loss": 6.586265563964844
    },
    {
      "epoch": 0.17994579945799458,
      "step": 830,
      "training_loss": 5.979654788970947
    },
    {
      "epoch": 0.17994579945799458,
      "step": 830,
      "training_loss": 6.976528644561768
    },
    {
      "epoch": 0.18016260162601627,
      "step": 831,
      "training_loss": 6.393564224243164
    },
    {
      "epoch": 0.18016260162601627,
      "step": 831,
      "training_loss": 6.223677158355713
    },
    {
      "epoch": 0.18016260162601627,
      "step": 831,
      "training_loss": 7.262542724609375
    },
    {
      "epoch": 0.18016260162601627,
      "step": 831,
      "training_loss": 7.563178062438965
    },
    {
      "epoch": 0.18037940379403794,
      "grad_norm": 13.597881317138672,
      "learning_rate": 1e-05,
      "loss": 7.0336,
      "step": 832
    },
    {
      "epoch": 0.18037940379403794,
      "step": 832,
      "training_loss": 6.7817888259887695
    },
    {
      "epoch": 0.18037940379403794,
      "step": 832,
      "training_loss": 8.237425804138184
    },
    {
      "epoch": 0.18037940379403794,
      "step": 832,
      "training_loss": 6.203253269195557
    },
    {
      "epoch": 0.18037940379403794,
      "step": 832,
      "training_loss": 6.972982406616211
    },
    {
      "epoch": 0.1805962059620596,
      "step": 833,
      "training_loss": 7.2518205642700195
    },
    {
      "epoch": 0.1805962059620596,
      "step": 833,
      "training_loss": 6.606057643890381
    },
    {
      "epoch": 0.1805962059620596,
      "step": 833,
      "training_loss": 7.012537956237793
    },
    {
      "epoch": 0.1805962059620596,
      "step": 833,
      "training_loss": 7.553338527679443
    },
    {
      "epoch": 0.1808130081300813,
      "step": 834,
      "training_loss": 6.905998706817627
    },
    {
      "epoch": 0.1808130081300813,
      "step": 834,
      "training_loss": 7.240993976593018
    },
    {
      "epoch": 0.1808130081300813,
      "step": 834,
      "training_loss": 7.159289836883545
    },
    {
      "epoch": 0.1808130081300813,
      "step": 834,
      "training_loss": 7.524410724639893
    },
    {
      "epoch": 0.18102981029810297,
      "step": 835,
      "training_loss": 7.66998815536499
    },
    {
      "epoch": 0.18102981029810297,
      "step": 835,
      "training_loss": 6.85635232925415
    },
    {
      "epoch": 0.18102981029810297,
      "step": 835,
      "training_loss": 5.6821608543396
    },
    {
      "epoch": 0.18102981029810297,
      "step": 835,
      "training_loss": 4.937942028045654
    },
    {
      "epoch": 0.18124661246612467,
      "grad_norm": 12.629129409790039,
      "learning_rate": 1e-05,
      "loss": 6.9123,
      "step": 836
    },
    {
      "epoch": 0.18124661246612467,
      "step": 836,
      "training_loss": 7.5404953956604
    },
    {
      "epoch": 0.18124661246612467,
      "step": 836,
      "training_loss": 5.916575908660889
    },
    {
      "epoch": 0.18124661246612467,
      "step": 836,
      "training_loss": 6.707595348358154
    },
    {
      "epoch": 0.18124661246612467,
      "step": 836,
      "training_loss": 7.4909539222717285
    },
    {
      "epoch": 0.18146341463414634,
      "step": 837,
      "training_loss": 6.1932053565979
    },
    {
      "epoch": 0.18146341463414634,
      "step": 837,
      "training_loss": 6.093774795532227
    },
    {
      "epoch": 0.18146341463414634,
      "step": 837,
      "training_loss": 7.947668552398682
    },
    {
      "epoch": 0.18146341463414634,
      "step": 837,
      "training_loss": 5.2429046630859375
    },
    {
      "epoch": 0.18168021680216803,
      "step": 838,
      "training_loss": 5.393590927124023
    },
    {
      "epoch": 0.18168021680216803,
      "step": 838,
      "training_loss": 4.946475505828857
    },
    {
      "epoch": 0.18168021680216803,
      "step": 838,
      "training_loss": 6.8962531089782715
    },
    {
      "epoch": 0.18168021680216803,
      "step": 838,
      "training_loss": 7.28692626953125
    },
    {
      "epoch": 0.1818970189701897,
      "step": 839,
      "training_loss": 5.154906749725342
    },
    {
      "epoch": 0.1818970189701897,
      "step": 839,
      "training_loss": 7.705446243286133
    },
    {
      "epoch": 0.1818970189701897,
      "step": 839,
      "training_loss": 6.369869709014893
    },
    {
      "epoch": 0.1818970189701897,
      "step": 839,
      "training_loss": 6.697632789611816
    },
    {
      "epoch": 0.1821138211382114,
      "grad_norm": 14.39673900604248,
      "learning_rate": 1e-05,
      "loss": 6.474,
      "step": 840
    },
    {
      "epoch": 0.1821138211382114,
      "step": 840,
      "training_loss": 6.90183687210083
    },
    {
      "epoch": 0.1821138211382114,
      "step": 840,
      "training_loss": 7.169308662414551
    },
    {
      "epoch": 0.1821138211382114,
      "step": 840,
      "training_loss": 5.5402326583862305
    },
    {
      "epoch": 0.1821138211382114,
      "step": 840,
      "training_loss": 6.4155802726745605
    },
    {
      "epoch": 0.18233062330623306,
      "step": 841,
      "training_loss": 7.426743030548096
    },
    {
      "epoch": 0.18233062330623306,
      "step": 841,
      "training_loss": 7.110913276672363
    },
    {
      "epoch": 0.18233062330623306,
      "step": 841,
      "training_loss": 7.205390930175781
    },
    {
      "epoch": 0.18233062330623306,
      "step": 841,
      "training_loss": 6.680217266082764
    },
    {
      "epoch": 0.18254742547425473,
      "step": 842,
      "training_loss": 7.2677083015441895
    },
    {
      "epoch": 0.18254742547425473,
      "step": 842,
      "training_loss": 7.971159934997559
    },
    {
      "epoch": 0.18254742547425473,
      "step": 842,
      "training_loss": 7.432767868041992
    },
    {
      "epoch": 0.18254742547425473,
      "step": 842,
      "training_loss": 6.515979766845703
    },
    {
      "epoch": 0.18276422764227643,
      "step": 843,
      "training_loss": 7.491003513336182
    },
    {
      "epoch": 0.18276422764227643,
      "step": 843,
      "training_loss": 5.977386474609375
    },
    {
      "epoch": 0.18276422764227643,
      "step": 843,
      "training_loss": 7.9490156173706055
    },
    {
      "epoch": 0.18276422764227643,
      "step": 843,
      "training_loss": 6.521909236907959
    },
    {
      "epoch": 0.1829810298102981,
      "grad_norm": 17.504440307617188,
      "learning_rate": 1e-05,
      "loss": 6.9736,
      "step": 844
    },
    {
      "epoch": 0.1829810298102981,
      "step": 844,
      "training_loss": 6.7727556228637695
    },
    {
      "epoch": 0.1829810298102981,
      "step": 844,
      "training_loss": 6.432783603668213
    },
    {
      "epoch": 0.1829810298102981,
      "step": 844,
      "training_loss": 7.659539699554443
    },
    {
      "epoch": 0.1829810298102981,
      "step": 844,
      "training_loss": 5.5645856857299805
    },
    {
      "epoch": 0.1831978319783198,
      "step": 845,
      "training_loss": 5.010157585144043
    },
    {
      "epoch": 0.1831978319783198,
      "step": 845,
      "training_loss": 6.755201816558838
    },
    {
      "epoch": 0.1831978319783198,
      "step": 845,
      "training_loss": 6.578129291534424
    },
    {
      "epoch": 0.1831978319783198,
      "step": 845,
      "training_loss": 6.703677654266357
    },
    {
      "epoch": 0.18341463414634146,
      "step": 846,
      "training_loss": 5.231194019317627
    },
    {
      "epoch": 0.18341463414634146,
      "step": 846,
      "training_loss": 6.383975982666016
    },
    {
      "epoch": 0.18341463414634146,
      "step": 846,
      "training_loss": 7.423555850982666
    },
    {
      "epoch": 0.18341463414634146,
      "step": 846,
      "training_loss": 7.2944464683532715
    },
    {
      "epoch": 0.18363143631436316,
      "step": 847,
      "training_loss": 7.279870510101318
    },
    {
      "epoch": 0.18363143631436316,
      "step": 847,
      "training_loss": 7.575031280517578
    },
    {
      "epoch": 0.18363143631436316,
      "step": 847,
      "training_loss": 8.917243003845215
    },
    {
      "epoch": 0.18363143631436316,
      "step": 847,
      "training_loss": 7.6421942710876465
    },
    {
      "epoch": 0.18384823848238482,
      "grad_norm": 8.172648429870605,
      "learning_rate": 1e-05,
      "loss": 6.8265,
      "step": 848
    },
    {
      "epoch": 0.18384823848238482,
      "step": 848,
      "training_loss": 8.216119766235352
    },
    {
      "epoch": 0.18384823848238482,
      "step": 848,
      "training_loss": 6.314955234527588
    },
    {
      "epoch": 0.18384823848238482,
      "step": 848,
      "training_loss": 6.6658854484558105
    },
    {
      "epoch": 0.18384823848238482,
      "step": 848,
      "training_loss": 7.205784320831299
    },
    {
      "epoch": 0.1840650406504065,
      "step": 849,
      "training_loss": 5.900261402130127
    },
    {
      "epoch": 0.1840650406504065,
      "step": 849,
      "training_loss": 7.503483772277832
    },
    {
      "epoch": 0.1840650406504065,
      "step": 849,
      "training_loss": 7.549423694610596
    },
    {
      "epoch": 0.1840650406504065,
      "step": 849,
      "training_loss": 7.368478298187256
    },
    {
      "epoch": 0.1842818428184282,
      "step": 850,
      "training_loss": 6.0584588050842285
    },
    {
      "epoch": 0.1842818428184282,
      "step": 850,
      "training_loss": 6.999340057373047
    },
    {
      "epoch": 0.1842818428184282,
      "step": 850,
      "training_loss": 8.188440322875977
    },
    {
      "epoch": 0.1842818428184282,
      "step": 850,
      "training_loss": 6.6479692459106445
    },
    {
      "epoch": 0.18449864498644986,
      "step": 851,
      "training_loss": 6.525893211364746
    },
    {
      "epoch": 0.18449864498644986,
      "step": 851,
      "training_loss": 7.247757911682129
    },
    {
      "epoch": 0.18449864498644986,
      "step": 851,
      "training_loss": 7.33519983291626
    },
    {
      "epoch": 0.18449864498644986,
      "step": 851,
      "training_loss": 5.459049701690674
    },
    {
      "epoch": 0.18471544715447155,
      "grad_norm": 9.958404541015625,
      "learning_rate": 1e-05,
      "loss": 6.9492,
      "step": 852
    },
    {
      "epoch": 0.18471544715447155,
      "step": 852,
      "training_loss": 6.413839340209961
    },
    {
      "epoch": 0.18471544715447155,
      "step": 852,
      "training_loss": 7.40408992767334
    },
    {
      "epoch": 0.18471544715447155,
      "step": 852,
      "training_loss": 6.134465217590332
    },
    {
      "epoch": 0.18471544715447155,
      "step": 852,
      "training_loss": 6.4495344161987305
    },
    {
      "epoch": 0.18493224932249322,
      "step": 853,
      "training_loss": 7.2241339683532715
    },
    {
      "epoch": 0.18493224932249322,
      "step": 853,
      "training_loss": 6.02701473236084
    },
    {
      "epoch": 0.18493224932249322,
      "step": 853,
      "training_loss": 6.885545253753662
    },
    {
      "epoch": 0.18493224932249322,
      "step": 853,
      "training_loss": 7.164748191833496
    },
    {
      "epoch": 0.18514905149051492,
      "step": 854,
      "training_loss": 7.8013916015625
    },
    {
      "epoch": 0.18514905149051492,
      "step": 854,
      "training_loss": 7.321120738983154
    },
    {
      "epoch": 0.18514905149051492,
      "step": 854,
      "training_loss": 6.674760341644287
    },
    {
      "epoch": 0.18514905149051492,
      "step": 854,
      "training_loss": 5.9792304039001465
    },
    {
      "epoch": 0.18536585365853658,
      "step": 855,
      "training_loss": 7.321018218994141
    },
    {
      "epoch": 0.18536585365853658,
      "step": 855,
      "training_loss": 7.071277141571045
    },
    {
      "epoch": 0.18536585365853658,
      "step": 855,
      "training_loss": 5.598282337188721
    },
    {
      "epoch": 0.18536585365853658,
      "step": 855,
      "training_loss": 5.885578632354736
    },
    {
      "epoch": 0.18558265582655828,
      "grad_norm": 14.163718223571777,
      "learning_rate": 1e-05,
      "loss": 6.7098,
      "step": 856
    },
    {
      "epoch": 0.18558265582655828,
      "step": 856,
      "training_loss": 7.056341648101807
    },
    {
      "epoch": 0.18558265582655828,
      "step": 856,
      "training_loss": 7.126513481140137
    },
    {
      "epoch": 0.18558265582655828,
      "step": 856,
      "training_loss": 7.486110687255859
    },
    {
      "epoch": 0.18558265582655828,
      "step": 856,
      "training_loss": 6.898015975952148
    },
    {
      "epoch": 0.18579945799457995,
      "step": 857,
      "training_loss": 7.2575860023498535
    },
    {
      "epoch": 0.18579945799457995,
      "step": 857,
      "training_loss": 7.266690731048584
    },
    {
      "epoch": 0.18579945799457995,
      "step": 857,
      "training_loss": 6.275269031524658
    },
    {
      "epoch": 0.18579945799457995,
      "step": 857,
      "training_loss": 7.598546504974365
    },
    {
      "epoch": 0.18601626016260162,
      "step": 858,
      "training_loss": 6.289386749267578
    },
    {
      "epoch": 0.18601626016260162,
      "step": 858,
      "training_loss": 7.044407367706299
    },
    {
      "epoch": 0.18601626016260162,
      "step": 858,
      "training_loss": 4.322335720062256
    },
    {
      "epoch": 0.18601626016260162,
      "step": 858,
      "training_loss": 5.229772090911865
    },
    {
      "epoch": 0.1862330623306233,
      "step": 859,
      "training_loss": 5.896420955657959
    },
    {
      "epoch": 0.1862330623306233,
      "step": 859,
      "training_loss": 7.295280933380127
    },
    {
      "epoch": 0.1862330623306233,
      "step": 859,
      "training_loss": 6.049184799194336
    },
    {
      "epoch": 0.1862330623306233,
      "step": 859,
      "training_loss": 3.998030424118042
    },
    {
      "epoch": 0.18644986449864498,
      "grad_norm": 10.171514511108398,
      "learning_rate": 1e-05,
      "loss": 6.4431,
      "step": 860
    },
    {
      "epoch": 0.18644986449864498,
      "step": 860,
      "training_loss": 7.557105541229248
    },
    {
      "epoch": 0.18644986449864498,
      "step": 860,
      "training_loss": 4.5104851722717285
    },
    {
      "epoch": 0.18644986449864498,
      "step": 860,
      "training_loss": 7.549286365509033
    },
    {
      "epoch": 0.18644986449864498,
      "step": 860,
      "training_loss": 5.938323974609375
    },
    {
      "epoch": 0.18666666666666668,
      "step": 861,
      "training_loss": 6.265542984008789
    },
    {
      "epoch": 0.18666666666666668,
      "step": 861,
      "training_loss": 7.069749355316162
    },
    {
      "epoch": 0.18666666666666668,
      "step": 861,
      "training_loss": 5.865260124206543
    },
    {
      "epoch": 0.18666666666666668,
      "step": 861,
      "training_loss": 4.416109561920166
    },
    {
      "epoch": 0.18688346883468834,
      "step": 862,
      "training_loss": 5.968348503112793
    },
    {
      "epoch": 0.18688346883468834,
      "step": 862,
      "training_loss": 7.509198188781738
    },
    {
      "epoch": 0.18688346883468834,
      "step": 862,
      "training_loss": 5.830752849578857
    },
    {
      "epoch": 0.18688346883468834,
      "step": 862,
      "training_loss": 8.36498737335205
    },
    {
      "epoch": 0.18710027100271004,
      "step": 863,
      "training_loss": 6.835749626159668
    },
    {
      "epoch": 0.18710027100271004,
      "step": 863,
      "training_loss": 5.867084980010986
    },
    {
      "epoch": 0.18710027100271004,
      "step": 863,
      "training_loss": 8.382264137268066
    },
    {
      "epoch": 0.18710027100271004,
      "step": 863,
      "training_loss": 7.758155822753906
    },
    {
      "epoch": 0.1873170731707317,
      "grad_norm": 11.010170936584473,
      "learning_rate": 1e-05,
      "loss": 6.6055,
      "step": 864
    },
    {
      "epoch": 0.1873170731707317,
      "step": 864,
      "training_loss": 7.097879886627197
    },
    {
      "epoch": 0.1873170731707317,
      "step": 864,
      "training_loss": 5.855429649353027
    },
    {
      "epoch": 0.1873170731707317,
      "step": 864,
      "training_loss": 7.380894660949707
    },
    {
      "epoch": 0.1873170731707317,
      "step": 864,
      "training_loss": 6.582264423370361
    },
    {
      "epoch": 0.18753387533875338,
      "step": 865,
      "training_loss": 6.381596088409424
    },
    {
      "epoch": 0.18753387533875338,
      "step": 865,
      "training_loss": 4.684199333190918
    },
    {
      "epoch": 0.18753387533875338,
      "step": 865,
      "training_loss": 7.576179504394531
    },
    {
      "epoch": 0.18753387533875338,
      "step": 865,
      "training_loss": 7.32344388961792
    },
    {
      "epoch": 0.18775067750677507,
      "step": 866,
      "training_loss": 6.4203410148620605
    },
    {
      "epoch": 0.18775067750677507,
      "step": 866,
      "training_loss": 7.317115306854248
    },
    {
      "epoch": 0.18775067750677507,
      "step": 866,
      "training_loss": 6.0423903465271
    },
    {
      "epoch": 0.18775067750677507,
      "step": 866,
      "training_loss": 7.418962478637695
    },
    {
      "epoch": 0.18796747967479674,
      "step": 867,
      "training_loss": 6.857257843017578
    },
    {
      "epoch": 0.18796747967479674,
      "step": 867,
      "training_loss": 3.691380500793457
    },
    {
      "epoch": 0.18796747967479674,
      "step": 867,
      "training_loss": 7.333705902099609
    },
    {
      "epoch": 0.18796747967479674,
      "step": 867,
      "training_loss": 7.729398250579834
    },
    {
      "epoch": 0.18818428184281843,
      "grad_norm": 7.588561534881592,
      "learning_rate": 1e-05,
      "loss": 6.6058,
      "step": 868
    },
    {
      "epoch": 0.18818428184281843,
      "step": 868,
      "training_loss": 7.813611030578613
    },
    {
      "epoch": 0.18818428184281843,
      "step": 868,
      "training_loss": 9.395195960998535
    },
    {
      "epoch": 0.18818428184281843,
      "step": 868,
      "training_loss": 6.550797462463379
    },
    {
      "epoch": 0.18818428184281843,
      "step": 868,
      "training_loss": 6.9231696128845215
    },
    {
      "epoch": 0.1884010840108401,
      "step": 869,
      "training_loss": 6.8957014083862305
    },
    {
      "epoch": 0.1884010840108401,
      "step": 869,
      "training_loss": 6.576935768127441
    },
    {
      "epoch": 0.1884010840108401,
      "step": 869,
      "training_loss": 5.809262275695801
    },
    {
      "epoch": 0.1884010840108401,
      "step": 869,
      "training_loss": 4.723826885223389
    },
    {
      "epoch": 0.1886178861788618,
      "step": 870,
      "training_loss": 7.174185752868652
    },
    {
      "epoch": 0.1886178861788618,
      "step": 870,
      "training_loss": 7.6922526359558105
    },
    {
      "epoch": 0.1886178861788618,
      "step": 870,
      "training_loss": 7.708037376403809
    },
    {
      "epoch": 0.1886178861788618,
      "step": 870,
      "training_loss": 6.401156425476074
    },
    {
      "epoch": 0.18883468834688347,
      "step": 871,
      "training_loss": 7.015880584716797
    },
    {
      "epoch": 0.18883468834688347,
      "step": 871,
      "training_loss": 7.7388482093811035
    },
    {
      "epoch": 0.18883468834688347,
      "step": 871,
      "training_loss": 7.7787394523620605
    },
    {
      "epoch": 0.18883468834688347,
      "step": 871,
      "training_loss": 7.463954448699951
    },
    {
      "epoch": 0.18905149051490516,
      "grad_norm": 13.404902458190918,
      "learning_rate": 1e-05,
      "loss": 7.1038,
      "step": 872
    },
    {
      "epoch": 0.18905149051490516,
      "step": 872,
      "training_loss": 5.593206405639648
    },
    {
      "epoch": 0.18905149051490516,
      "step": 872,
      "training_loss": 7.768279552459717
    },
    {
      "epoch": 0.18905149051490516,
      "step": 872,
      "training_loss": 7.968555450439453
    },
    {
      "epoch": 0.18905149051490516,
      "step": 872,
      "training_loss": 6.655534744262695
    },
    {
      "epoch": 0.18926829268292683,
      "step": 873,
      "training_loss": 6.649226188659668
    },
    {
      "epoch": 0.18926829268292683,
      "step": 873,
      "training_loss": 4.882646083831787
    },
    {
      "epoch": 0.18926829268292683,
      "step": 873,
      "training_loss": 4.800568580627441
    },
    {
      "epoch": 0.18926829268292683,
      "step": 873,
      "training_loss": 5.427021026611328
    },
    {
      "epoch": 0.1894850948509485,
      "step": 874,
      "training_loss": 5.894060134887695
    },
    {
      "epoch": 0.1894850948509485,
      "step": 874,
      "training_loss": 8.81352424621582
    },
    {
      "epoch": 0.1894850948509485,
      "step": 874,
      "training_loss": 7.2147908210754395
    },
    {
      "epoch": 0.1894850948509485,
      "step": 874,
      "training_loss": 6.944873809814453
    },
    {
      "epoch": 0.1897018970189702,
      "step": 875,
      "training_loss": 7.318897247314453
    },
    {
      "epoch": 0.1897018970189702,
      "step": 875,
      "training_loss": 7.796590328216553
    },
    {
      "epoch": 0.1897018970189702,
      "step": 875,
      "training_loss": 7.497662544250488
    },
    {
      "epoch": 0.1897018970189702,
      "step": 875,
      "training_loss": 6.488674163818359
    },
    {
      "epoch": 0.18991869918699186,
      "grad_norm": 11.741094589233398,
      "learning_rate": 1e-05,
      "loss": 6.7321,
      "step": 876
    },
    {
      "epoch": 0.18991869918699186,
      "step": 876,
      "training_loss": 5.678836345672607
    },
    {
      "epoch": 0.18991869918699186,
      "step": 876,
      "training_loss": 7.270770072937012
    },
    {
      "epoch": 0.18991869918699186,
      "step": 876,
      "training_loss": 6.147757053375244
    },
    {
      "epoch": 0.18991869918699186,
      "step": 876,
      "training_loss": 6.961951732635498
    },
    {
      "epoch": 0.19013550135501356,
      "step": 877,
      "training_loss": 6.609966278076172
    },
    {
      "epoch": 0.19013550135501356,
      "step": 877,
      "training_loss": 7.200104236602783
    },
    {
      "epoch": 0.19013550135501356,
      "step": 877,
      "training_loss": 5.959210395812988
    },
    {
      "epoch": 0.19013550135501356,
      "step": 877,
      "training_loss": 7.082489013671875
    },
    {
      "epoch": 0.19035230352303523,
      "step": 878,
      "training_loss": 5.835183143615723
    },
    {
      "epoch": 0.19035230352303523,
      "step": 878,
      "training_loss": 7.513627052307129
    },
    {
      "epoch": 0.19035230352303523,
      "step": 878,
      "training_loss": 6.959214210510254
    },
    {
      "epoch": 0.19035230352303523,
      "step": 878,
      "training_loss": 7.970824241638184
    },
    {
      "epoch": 0.19056910569105692,
      "step": 879,
      "training_loss": 7.172862529754639
    },
    {
      "epoch": 0.19056910569105692,
      "step": 879,
      "training_loss": 5.841548919677734
    },
    {
      "epoch": 0.19056910569105692,
      "step": 879,
      "training_loss": 7.2539753913879395
    },
    {
      "epoch": 0.19056910569105692,
      "step": 879,
      "training_loss": 7.880335807800293
    },
    {
      "epoch": 0.1907859078590786,
      "grad_norm": 11.798023223876953,
      "learning_rate": 1e-05,
      "loss": 6.8337,
      "step": 880
    },
    {
      "epoch": 0.1907859078590786,
      "step": 880,
      "training_loss": 7.706845283508301
    },
    {
      "epoch": 0.1907859078590786,
      "step": 880,
      "training_loss": 6.127853870391846
    },
    {
      "epoch": 0.1907859078590786,
      "step": 880,
      "training_loss": 7.794128894805908
    },
    {
      "epoch": 0.1907859078590786,
      "step": 880,
      "training_loss": 7.023628234863281
    },
    {
      "epoch": 0.19100271002710026,
      "step": 881,
      "training_loss": 5.28890323638916
    },
    {
      "epoch": 0.19100271002710026,
      "step": 881,
      "training_loss": 6.132719993591309
    },
    {
      "epoch": 0.19100271002710026,
      "step": 881,
      "training_loss": 7.366426467895508
    },
    {
      "epoch": 0.19100271002710026,
      "step": 881,
      "training_loss": 5.632505416870117
    },
    {
      "epoch": 0.19121951219512195,
      "step": 882,
      "training_loss": 7.402988433837891
    },
    {
      "epoch": 0.19121951219512195,
      "step": 882,
      "training_loss": 6.841047763824463
    },
    {
      "epoch": 0.19121951219512195,
      "step": 882,
      "training_loss": 8.267152786254883
    },
    {
      "epoch": 0.19121951219512195,
      "step": 882,
      "training_loss": 5.261898994445801
    },
    {
      "epoch": 0.19143631436314362,
      "step": 883,
      "training_loss": 5.987696170806885
    },
    {
      "epoch": 0.19143631436314362,
      "step": 883,
      "training_loss": 7.093550205230713
    },
    {
      "epoch": 0.19143631436314362,
      "step": 883,
      "training_loss": 6.67935037612915
    },
    {
      "epoch": 0.19143631436314362,
      "step": 883,
      "training_loss": 7.287263870239258
    },
    {
      "epoch": 0.19165311653116532,
      "grad_norm": 13.004971504211426,
      "learning_rate": 1e-05,
      "loss": 6.7434,
      "step": 884
    },
    {
      "epoch": 0.19165311653116532,
      "step": 884,
      "training_loss": 6.512149810791016
    },
    {
      "epoch": 0.19165311653116532,
      "step": 884,
      "training_loss": 7.252744197845459
    },
    {
      "epoch": 0.19165311653116532,
      "step": 884,
      "training_loss": 6.969258785247803
    },
    {
      "epoch": 0.19165311653116532,
      "step": 884,
      "training_loss": 7.6100850105285645
    },
    {
      "epoch": 0.191869918699187,
      "step": 885,
      "training_loss": 6.8116865158081055
    },
    {
      "epoch": 0.191869918699187,
      "step": 885,
      "training_loss": 6.189436435699463
    },
    {
      "epoch": 0.191869918699187,
      "step": 885,
      "training_loss": 5.435823917388916
    },
    {
      "epoch": 0.191869918699187,
      "step": 885,
      "training_loss": 6.75324010848999
    },
    {
      "epoch": 0.19208672086720868,
      "step": 886,
      "training_loss": 7.5351667404174805
    },
    {
      "epoch": 0.19208672086720868,
      "step": 886,
      "training_loss": 8.291223526000977
    },
    {
      "epoch": 0.19208672086720868,
      "step": 886,
      "training_loss": 7.155385971069336
    },
    {
      "epoch": 0.19208672086720868,
      "step": 886,
      "training_loss": 5.734325885772705
    },
    {
      "epoch": 0.19230352303523035,
      "step": 887,
      "training_loss": 4.3684868812561035
    },
    {
      "epoch": 0.19230352303523035,
      "step": 887,
      "training_loss": 4.835896015167236
    },
    {
      "epoch": 0.19230352303523035,
      "step": 887,
      "training_loss": 9.3375244140625
    },
    {
      "epoch": 0.19230352303523035,
      "step": 887,
      "training_loss": 6.891692161560059
    },
    {
      "epoch": 0.19252032520325205,
      "grad_norm": 16.737871170043945,
      "learning_rate": 1e-05,
      "loss": 6.7303,
      "step": 888
    },
    {
      "epoch": 0.19252032520325205,
      "step": 888,
      "training_loss": 6.081811904907227
    },
    {
      "epoch": 0.19252032520325205,
      "step": 888,
      "training_loss": 6.769283771514893
    },
    {
      "epoch": 0.19252032520325205,
      "step": 888,
      "training_loss": 7.356983661651611
    },
    {
      "epoch": 0.19252032520325205,
      "step": 888,
      "training_loss": 7.537240982055664
    },
    {
      "epoch": 0.19273712737127371,
      "step": 889,
      "training_loss": 7.444024562835693
    },
    {
      "epoch": 0.19273712737127371,
      "step": 889,
      "training_loss": 6.897499084472656
    },
    {
      "epoch": 0.19273712737127371,
      "step": 889,
      "training_loss": 7.683300018310547
    },
    {
      "epoch": 0.19273712737127371,
      "step": 889,
      "training_loss": 6.656833648681641
    },
    {
      "epoch": 0.19295392953929538,
      "step": 890,
      "training_loss": 6.7633442878723145
    },
    {
      "epoch": 0.19295392953929538,
      "step": 890,
      "training_loss": 7.274376392364502
    },
    {
      "epoch": 0.19295392953929538,
      "step": 890,
      "training_loss": 6.984272480010986
    },
    {
      "epoch": 0.19295392953929538,
      "step": 890,
      "training_loss": 7.535528659820557
    },
    {
      "epoch": 0.19317073170731708,
      "step": 891,
      "training_loss": 5.931468963623047
    },
    {
      "epoch": 0.19317073170731708,
      "step": 891,
      "training_loss": 6.711596488952637
    },
    {
      "epoch": 0.19317073170731708,
      "step": 891,
      "training_loss": 7.222375392913818
    },
    {
      "epoch": 0.19317073170731708,
      "step": 891,
      "training_loss": 6.264718055725098
    },
    {
      "epoch": 0.19338753387533875,
      "grad_norm": 10.747960090637207,
      "learning_rate": 1e-05,
      "loss": 6.9447,
      "step": 892
    },
    {
      "epoch": 0.19338753387533875,
      "step": 892,
      "training_loss": 6.927699565887451
    },
    {
      "epoch": 0.19338753387533875,
      "step": 892,
      "training_loss": 7.274895191192627
    },
    {
      "epoch": 0.19338753387533875,
      "step": 892,
      "training_loss": 4.448272228240967
    },
    {
      "epoch": 0.19338753387533875,
      "step": 892,
      "training_loss": 6.777163505554199
    },
    {
      "epoch": 0.19360433604336044,
      "step": 893,
      "training_loss": 6.918709754943848
    },
    {
      "epoch": 0.19360433604336044,
      "step": 893,
      "training_loss": 7.049936771392822
    },
    {
      "epoch": 0.19360433604336044,
      "step": 893,
      "training_loss": 7.563156604766846
    },
    {
      "epoch": 0.19360433604336044,
      "step": 893,
      "training_loss": 6.774939060211182
    },
    {
      "epoch": 0.1938211382113821,
      "step": 894,
      "training_loss": 6.5242180824279785
    },
    {
      "epoch": 0.1938211382113821,
      "step": 894,
      "training_loss": 7.711068630218506
    },
    {
      "epoch": 0.1938211382113821,
      "step": 894,
      "training_loss": 5.563199996948242
    },
    {
      "epoch": 0.1938211382113821,
      "step": 894,
      "training_loss": 8.09560489654541
    },
    {
      "epoch": 0.1940379403794038,
      "step": 895,
      "training_loss": 6.951185703277588
    },
    {
      "epoch": 0.1940379403794038,
      "step": 895,
      "training_loss": 6.1013503074646
    },
    {
      "epoch": 0.1940379403794038,
      "step": 895,
      "training_loss": 6.9076714515686035
    },
    {
      "epoch": 0.1940379403794038,
      "step": 895,
      "training_loss": 7.58807897567749
    },
    {
      "epoch": 0.19425474254742547,
      "grad_norm": 10.413105010986328,
      "learning_rate": 1e-05,
      "loss": 6.8236,
      "step": 896
    },
    {
      "epoch": 0.19425474254742547,
      "step": 896,
      "training_loss": 6.7587127685546875
    },
    {
      "epoch": 0.19425474254742547,
      "step": 896,
      "training_loss": 6.438232898712158
    },
    {
      "epoch": 0.19425474254742547,
      "step": 896,
      "training_loss": 7.774062156677246
    },
    {
      "epoch": 0.19425474254742547,
      "step": 896,
      "training_loss": 7.252418041229248
    },
    {
      "epoch": 0.19447154471544714,
      "step": 897,
      "training_loss": 7.453073978424072
    },
    {
      "epoch": 0.19447154471544714,
      "step": 897,
      "training_loss": 7.176039695739746
    },
    {
      "epoch": 0.19447154471544714,
      "step": 897,
      "training_loss": 7.356082916259766
    },
    {
      "epoch": 0.19447154471544714,
      "step": 897,
      "training_loss": 7.129809856414795
    },
    {
      "epoch": 0.19468834688346884,
      "step": 898,
      "training_loss": 7.187875747680664
    },
    {
      "epoch": 0.19468834688346884,
      "step": 898,
      "training_loss": 7.835042953491211
    },
    {
      "epoch": 0.19468834688346884,
      "step": 898,
      "training_loss": 7.095414161682129
    },
    {
      "epoch": 0.19468834688346884,
      "step": 898,
      "training_loss": 7.633195400238037
    },
    {
      "epoch": 0.1949051490514905,
      "step": 899,
      "training_loss": 6.6078267097473145
    },
    {
      "epoch": 0.1949051490514905,
      "step": 899,
      "training_loss": 7.3050432205200195
    },
    {
      "epoch": 0.1949051490514905,
      "step": 899,
      "training_loss": 6.792914867401123
    },
    {
      "epoch": 0.1949051490514905,
      "step": 899,
      "training_loss": 5.02008056640625
    },
    {
      "epoch": 0.1951219512195122,
      "grad_norm": 10.252851486206055,
      "learning_rate": 1e-05,
      "loss": 7.051,
      "step": 900
    },
    {
      "epoch": 0.1951219512195122,
      "step": 900,
      "training_loss": 6.410019874572754
    },
    {
      "epoch": 0.1951219512195122,
      "step": 900,
      "training_loss": 6.038712501525879
    },
    {
      "epoch": 0.1951219512195122,
      "step": 900,
      "training_loss": 6.785032749176025
    },
    {
      "epoch": 0.1951219512195122,
      "step": 900,
      "training_loss": 6.592451095581055
    },
    {
      "epoch": 0.19533875338753387,
      "step": 901,
      "training_loss": 6.880007743835449
    },
    {
      "epoch": 0.19533875338753387,
      "step": 901,
      "training_loss": 6.147342205047607
    },
    {
      "epoch": 0.19533875338753387,
      "step": 901,
      "training_loss": 6.640106678009033
    },
    {
      "epoch": 0.19533875338753387,
      "step": 901,
      "training_loss": 5.976489543914795
    },
    {
      "epoch": 0.19555555555555557,
      "step": 902,
      "training_loss": 6.095851421356201
    },
    {
      "epoch": 0.19555555555555557,
      "step": 902,
      "training_loss": 6.873294830322266
    },
    {
      "epoch": 0.19555555555555557,
      "step": 902,
      "training_loss": 5.825009822845459
    },
    {
      "epoch": 0.19555555555555557,
      "step": 902,
      "training_loss": 7.392847537994385
    },
    {
      "epoch": 0.19577235772357723,
      "step": 903,
      "training_loss": 7.449864387512207
    },
    {
      "epoch": 0.19577235772357723,
      "step": 903,
      "training_loss": 6.889398574829102
    },
    {
      "epoch": 0.19577235772357723,
      "step": 903,
      "training_loss": 6.561769008636475
    },
    {
      "epoch": 0.19577235772357723,
      "step": 903,
      "training_loss": 5.595492362976074
    },
    {
      "epoch": 0.19598915989159893,
      "grad_norm": 13.169519424438477,
      "learning_rate": 1e-05,
      "loss": 6.5096,
      "step": 904
    },
    {
      "epoch": 0.19598915989159893,
      "step": 904,
      "training_loss": 7.004337787628174
    },
    {
      "epoch": 0.19598915989159893,
      "step": 904,
      "training_loss": 6.1486287117004395
    },
    {
      "epoch": 0.19598915989159893,
      "step": 904,
      "training_loss": 7.599747657775879
    },
    {
      "epoch": 0.19598915989159893,
      "step": 904,
      "training_loss": 7.126088619232178
    },
    {
      "epoch": 0.1962059620596206,
      "step": 905,
      "training_loss": 6.913734436035156
    },
    {
      "epoch": 0.1962059620596206,
      "step": 905,
      "training_loss": 6.168493747711182
    },
    {
      "epoch": 0.1962059620596206,
      "step": 905,
      "training_loss": 6.2487359046936035
    },
    {
      "epoch": 0.1962059620596206,
      "step": 905,
      "training_loss": 6.851497173309326
    },
    {
      "epoch": 0.19642276422764227,
      "step": 906,
      "training_loss": 6.622215270996094
    },
    {
      "epoch": 0.19642276422764227,
      "step": 906,
      "training_loss": 6.3171844482421875
    },
    {
      "epoch": 0.19642276422764227,
      "step": 906,
      "training_loss": 7.257202625274658
    },
    {
      "epoch": 0.19642276422764227,
      "step": 906,
      "training_loss": 6.970135688781738
    },
    {
      "epoch": 0.19663956639566396,
      "step": 907,
      "training_loss": 6.828000545501709
    },
    {
      "epoch": 0.19663956639566396,
      "step": 907,
      "training_loss": 5.923429012298584
    },
    {
      "epoch": 0.19663956639566396,
      "step": 907,
      "training_loss": 6.6707563400268555
    },
    {
      "epoch": 0.19663956639566396,
      "step": 907,
      "training_loss": 5.8778557777404785
    },
    {
      "epoch": 0.19685636856368563,
      "grad_norm": 11.108222007751465,
      "learning_rate": 1e-05,
      "loss": 6.658,
      "step": 908
    },
    {
      "epoch": 0.19685636856368563,
      "step": 908,
      "training_loss": 6.703888416290283
    },
    {
      "epoch": 0.19685636856368563,
      "step": 908,
      "training_loss": 6.622559070587158
    },
    {
      "epoch": 0.19685636856368563,
      "step": 908,
      "training_loss": 6.2015604972839355
    },
    {
      "epoch": 0.19685636856368563,
      "step": 908,
      "training_loss": 7.254855155944824
    },
    {
      "epoch": 0.19707317073170733,
      "step": 909,
      "training_loss": 6.315056800842285
    },
    {
      "epoch": 0.19707317073170733,
      "step": 909,
      "training_loss": 6.060405254364014
    },
    {
      "epoch": 0.19707317073170733,
      "step": 909,
      "training_loss": 7.219874858856201
    },
    {
      "epoch": 0.19707317073170733,
      "step": 909,
      "training_loss": 6.711683750152588
    },
    {
      "epoch": 0.197289972899729,
      "step": 910,
      "training_loss": 5.392670154571533
    },
    {
      "epoch": 0.197289972899729,
      "step": 910,
      "training_loss": 7.865575313568115
    },
    {
      "epoch": 0.197289972899729,
      "step": 910,
      "training_loss": 6.806331157684326
    },
    {
      "epoch": 0.197289972899729,
      "step": 910,
      "training_loss": 7.038043975830078
    },
    {
      "epoch": 0.1975067750677507,
      "step": 911,
      "training_loss": 6.718445301055908
    },
    {
      "epoch": 0.1975067750677507,
      "step": 911,
      "training_loss": 8.014277458190918
    },
    {
      "epoch": 0.1975067750677507,
      "step": 911,
      "training_loss": 7.998960018157959
    },
    {
      "epoch": 0.1975067750677507,
      "step": 911,
      "training_loss": 6.624818801879883
    },
    {
      "epoch": 0.19772357723577236,
      "grad_norm": 16.3929500579834,
      "learning_rate": 1e-05,
      "loss": 6.8468,
      "step": 912
    },
    {
      "epoch": 0.19772357723577236,
      "step": 912,
      "training_loss": 7.7473344802856445
    },
    {
      "epoch": 0.19772357723577236,
      "step": 912,
      "training_loss": 7.644293308258057
    },
    {
      "epoch": 0.19772357723577236,
      "step": 912,
      "training_loss": 5.784482479095459
    },
    {
      "epoch": 0.19772357723577236,
      "step": 912,
      "training_loss": 7.049864292144775
    },
    {
      "epoch": 0.19794037940379403,
      "step": 913,
      "training_loss": 7.969338893890381
    },
    {
      "epoch": 0.19794037940379403,
      "step": 913,
      "training_loss": 6.1306915283203125
    },
    {
      "epoch": 0.19794037940379403,
      "step": 913,
      "training_loss": 7.969943523406982
    },
    {
      "epoch": 0.19794037940379403,
      "step": 913,
      "training_loss": 7.627865314483643
    },
    {
      "epoch": 0.19815718157181572,
      "step": 914,
      "training_loss": 6.9404296875
    },
    {
      "epoch": 0.19815718157181572,
      "step": 914,
      "training_loss": 7.00363826751709
    },
    {
      "epoch": 0.19815718157181572,
      "step": 914,
      "training_loss": 6.1134490966796875
    },
    {
      "epoch": 0.19815718157181572,
      "step": 914,
      "training_loss": 6.489457130432129
    },
    {
      "epoch": 0.1983739837398374,
      "step": 915,
      "training_loss": 6.935311794281006
    },
    {
      "epoch": 0.1983739837398374,
      "step": 915,
      "training_loss": 5.469728946685791
    },
    {
      "epoch": 0.1983739837398374,
      "step": 915,
      "training_loss": 7.977860450744629
    },
    {
      "epoch": 0.1983739837398374,
      "step": 915,
      "training_loss": 7.372036933898926
    },
    {
      "epoch": 0.19859078590785909,
      "grad_norm": 14.723663330078125,
      "learning_rate": 1e-05,
      "loss": 7.0141,
      "step": 916
    },
    {
      "epoch": 0.19859078590785909,
      "step": 916,
      "training_loss": 7.212057590484619
    },
    {
      "epoch": 0.19859078590785909,
      "step": 916,
      "training_loss": 6.210007190704346
    },
    {
      "epoch": 0.19859078590785909,
      "step": 916,
      "training_loss": 5.921400547027588
    },
    {
      "epoch": 0.19859078590785909,
      "step": 916,
      "training_loss": 5.230468273162842
    },
    {
      "epoch": 0.19880758807588075,
      "step": 917,
      "training_loss": 7.004193305969238
    },
    {
      "epoch": 0.19880758807588075,
      "step": 917,
      "training_loss": 7.184668064117432
    },
    {
      "epoch": 0.19880758807588075,
      "step": 917,
      "training_loss": 7.625781536102295
    },
    {
      "epoch": 0.19880758807588075,
      "step": 917,
      "training_loss": 6.287769794464111
    },
    {
      "epoch": 0.19902439024390245,
      "step": 918,
      "training_loss": 7.494413375854492
    },
    {
      "epoch": 0.19902439024390245,
      "step": 918,
      "training_loss": 9.108219146728516
    },
    {
      "epoch": 0.19902439024390245,
      "step": 918,
      "training_loss": 7.580753803253174
    },
    {
      "epoch": 0.19902439024390245,
      "step": 918,
      "training_loss": 7.217353820800781
    },
    {
      "epoch": 0.19924119241192412,
      "step": 919,
      "training_loss": 5.652766704559326
    },
    {
      "epoch": 0.19924119241192412,
      "step": 919,
      "training_loss": 5.4445929527282715
    },
    {
      "epoch": 0.19924119241192412,
      "step": 919,
      "training_loss": 5.5415940284729
    },
    {
      "epoch": 0.19924119241192412,
      "step": 919,
      "training_loss": 6.673961639404297
    },
    {
      "epoch": 0.1994579945799458,
      "grad_norm": 10.08931827545166,
      "learning_rate": 1e-05,
      "loss": 6.7119,
      "step": 920
    },
    {
      "epoch": 0.1994579945799458,
      "step": 920,
      "training_loss": 6.8289666175842285
    },
    {
      "epoch": 0.1994579945799458,
      "step": 920,
      "training_loss": 7.502258777618408
    },
    {
      "epoch": 0.1994579945799458,
      "step": 920,
      "training_loss": 6.40430212020874
    },
    {
      "epoch": 0.1994579945799458,
      "step": 920,
      "training_loss": 6.534623622894287
    },
    {
      "epoch": 0.19967479674796748,
      "step": 921,
      "training_loss": 7.402346611022949
    },
    {
      "epoch": 0.19967479674796748,
      "step": 921,
      "training_loss": 6.809055805206299
    },
    {
      "epoch": 0.19967479674796748,
      "step": 921,
      "training_loss": 6.738516807556152
    },
    {
      "epoch": 0.19967479674796748,
      "step": 921,
      "training_loss": 6.204294204711914
    },
    {
      "epoch": 0.19989159891598915,
      "step": 922,
      "training_loss": 5.865514755249023
    },
    {
      "epoch": 0.19989159891598915,
      "step": 922,
      "training_loss": 5.560064792633057
    },
    {
      "epoch": 0.19989159891598915,
      "step": 922,
      "training_loss": 7.084851264953613
    },
    {
      "epoch": 0.19989159891598915,
      "step": 922,
      "training_loss": 6.094139099121094
    },
    {
      "epoch": 0.20010840108401085,
      "step": 923,
      "training_loss": 6.950689315795898
    },
    {
      "epoch": 0.20010840108401085,
      "step": 923,
      "training_loss": 5.526797294616699
    },
    {
      "epoch": 0.20010840108401085,
      "step": 923,
      "training_loss": 8.325019836425781
    },
    {
      "epoch": 0.20010840108401085,
      "step": 923,
      "training_loss": 7.061394214630127
    },
    {
      "epoch": 0.2003252032520325,
      "grad_norm": 11.42160701751709,
      "learning_rate": 1e-05,
      "loss": 6.6808,
      "step": 924
    },
    {
      "epoch": 0.2003252032520325,
      "step": 924,
      "training_loss": 6.396037578582764
    },
    {
      "epoch": 0.2003252032520325,
      "step": 924,
      "training_loss": 7.350115776062012
    },
    {
      "epoch": 0.2003252032520325,
      "step": 924,
      "training_loss": 7.531520366668701
    },
    {
      "epoch": 0.2003252032520325,
      "step": 924,
      "training_loss": 7.342408180236816
    },
    {
      "epoch": 0.2005420054200542,
      "step": 925,
      "training_loss": 7.064502239227295
    },
    {
      "epoch": 0.2005420054200542,
      "step": 925,
      "training_loss": 7.454601287841797
    },
    {
      "epoch": 0.2005420054200542,
      "step": 925,
      "training_loss": 6.004039764404297
    },
    {
      "epoch": 0.2005420054200542,
      "step": 925,
      "training_loss": 8.559237480163574
    },
    {
      "epoch": 0.20075880758807588,
      "step": 926,
      "training_loss": 4.063633918762207
    },
    {
      "epoch": 0.20075880758807588,
      "step": 926,
      "training_loss": 6.877433776855469
    },
    {
      "epoch": 0.20075880758807588,
      "step": 926,
      "training_loss": 6.6381072998046875
    },
    {
      "epoch": 0.20075880758807588,
      "step": 926,
      "training_loss": 6.971539497375488
    },
    {
      "epoch": 0.20097560975609757,
      "step": 927,
      "training_loss": 7.247645378112793
    },
    {
      "epoch": 0.20097560975609757,
      "step": 927,
      "training_loss": 6.8095808029174805
    },
    {
      "epoch": 0.20097560975609757,
      "step": 927,
      "training_loss": 7.975776672363281
    },
    {
      "epoch": 0.20097560975609757,
      "step": 927,
      "training_loss": 7.549464225769043
    },
    {
      "epoch": 0.20119241192411924,
      "grad_norm": 14.797106742858887,
      "learning_rate": 1e-05,
      "loss": 6.9897,
      "step": 928
    },
    {
      "epoch": 0.20119241192411924,
      "step": 928,
      "training_loss": 7.628403186798096
    },
    {
      "epoch": 0.20119241192411924,
      "step": 928,
      "training_loss": 7.525078773498535
    },
    {
      "epoch": 0.20119241192411924,
      "step": 928,
      "training_loss": 7.4803361892700195
    },
    {
      "epoch": 0.20119241192411924,
      "step": 928,
      "training_loss": 7.0455217361450195
    },
    {
      "epoch": 0.2014092140921409,
      "step": 929,
      "training_loss": 7.338402271270752
    },
    {
      "epoch": 0.2014092140921409,
      "step": 929,
      "training_loss": 5.926655292510986
    },
    {
      "epoch": 0.2014092140921409,
      "step": 929,
      "training_loss": 7.177343845367432
    },
    {
      "epoch": 0.2014092140921409,
      "step": 929,
      "training_loss": 7.2205634117126465
    },
    {
      "epoch": 0.2016260162601626,
      "step": 930,
      "training_loss": 6.7669477462768555
    },
    {
      "epoch": 0.2016260162601626,
      "step": 930,
      "training_loss": 7.702770709991455
    },
    {
      "epoch": 0.2016260162601626,
      "step": 930,
      "training_loss": 8.154397010803223
    },
    {
      "epoch": 0.2016260162601626,
      "step": 930,
      "training_loss": 7.192293167114258
    },
    {
      "epoch": 0.20184281842818427,
      "step": 931,
      "training_loss": 6.382828712463379
    },
    {
      "epoch": 0.20184281842818427,
      "step": 931,
      "training_loss": 6.868321418762207
    },
    {
      "epoch": 0.20184281842818427,
      "step": 931,
      "training_loss": 6.627385139465332
    },
    {
      "epoch": 0.20184281842818427,
      "step": 931,
      "training_loss": 6.163031578063965
    },
    {
      "epoch": 0.20205962059620597,
      "grad_norm": 9.797733306884766,
      "learning_rate": 1e-05,
      "loss": 7.075,
      "step": 932
    },
    {
      "epoch": 0.20205962059620597,
      "step": 932,
      "training_loss": 6.548210144042969
    },
    {
      "epoch": 0.20205962059620597,
      "step": 932,
      "training_loss": 4.846714496612549
    },
    {
      "epoch": 0.20205962059620597,
      "step": 932,
      "training_loss": 6.056587219238281
    },
    {
      "epoch": 0.20205962059620597,
      "step": 932,
      "training_loss": 7.2180657386779785
    },
    {
      "epoch": 0.20227642276422764,
      "step": 933,
      "training_loss": 7.382997035980225
    },
    {
      "epoch": 0.20227642276422764,
      "step": 933,
      "training_loss": 7.573495388031006
    },
    {
      "epoch": 0.20227642276422764,
      "step": 933,
      "training_loss": 7.326600551605225
    },
    {
      "epoch": 0.20227642276422764,
      "step": 933,
      "training_loss": 7.335122108459473
    },
    {
      "epoch": 0.20249322493224933,
      "step": 934,
      "training_loss": 5.113866329193115
    },
    {
      "epoch": 0.20249322493224933,
      "step": 934,
      "training_loss": 6.846713066101074
    },
    {
      "epoch": 0.20249322493224933,
      "step": 934,
      "training_loss": 6.9236531257629395
    },
    {
      "epoch": 0.20249322493224933,
      "step": 934,
      "training_loss": 6.967811107635498
    },
    {
      "epoch": 0.202710027100271,
      "step": 935,
      "training_loss": 7.268775939941406
    },
    {
      "epoch": 0.202710027100271,
      "step": 935,
      "training_loss": 5.642610549926758
    },
    {
      "epoch": 0.202710027100271,
      "step": 935,
      "training_loss": 6.690211296081543
    },
    {
      "epoch": 0.202710027100271,
      "step": 935,
      "training_loss": 6.991703033447266
    },
    {
      "epoch": 0.2029268292682927,
      "grad_norm": 9.862648963928223,
      "learning_rate": 1e-05,
      "loss": 6.6708,
      "step": 936
    },
    {
      "epoch": 0.2029268292682927,
      "step": 936,
      "training_loss": 6.7761993408203125
    },
    {
      "epoch": 0.2029268292682927,
      "step": 936,
      "training_loss": 7.198761940002441
    },
    {
      "epoch": 0.2029268292682927,
      "step": 936,
      "training_loss": 7.68937349319458
    },
    {
      "epoch": 0.2029268292682927,
      "step": 936,
      "training_loss": 6.495145320892334
    },
    {
      "epoch": 0.20314363143631436,
      "step": 937,
      "training_loss": 6.531006336212158
    },
    {
      "epoch": 0.20314363143631436,
      "step": 937,
      "training_loss": 7.324926376342773
    },
    {
      "epoch": 0.20314363143631436,
      "step": 937,
      "training_loss": 7.519677639007568
    },
    {
      "epoch": 0.20314363143631436,
      "step": 937,
      "training_loss": 6.3707451820373535
    },
    {
      "epoch": 0.20336043360433603,
      "step": 938,
      "training_loss": 5.952656269073486
    },
    {
      "epoch": 0.20336043360433603,
      "step": 938,
      "training_loss": 7.111839294433594
    },
    {
      "epoch": 0.20336043360433603,
      "step": 938,
      "training_loss": 6.318502902984619
    },
    {
      "epoch": 0.20336043360433603,
      "step": 938,
      "training_loss": 5.602982044219971
    },
    {
      "epoch": 0.20357723577235773,
      "step": 939,
      "training_loss": 6.560694217681885
    },
    {
      "epoch": 0.20357723577235773,
      "step": 939,
      "training_loss": 6.922123908996582
    },
    {
      "epoch": 0.20357723577235773,
      "step": 939,
      "training_loss": 6.449138641357422
    },
    {
      "epoch": 0.20357723577235773,
      "step": 939,
      "training_loss": 7.114769458770752
    },
    {
      "epoch": 0.2037940379403794,
      "grad_norm": 12.055709838867188,
      "learning_rate": 1e-05,
      "loss": 6.7462,
      "step": 940
    },
    {
      "epoch": 0.2037940379403794,
      "step": 940,
      "training_loss": 5.71608304977417
    },
    {
      "epoch": 0.2037940379403794,
      "step": 940,
      "training_loss": 5.586877822875977
    },
    {
      "epoch": 0.2037940379403794,
      "step": 940,
      "training_loss": 5.774004936218262
    },
    {
      "epoch": 0.2037940379403794,
      "step": 940,
      "training_loss": 8.879987716674805
    },
    {
      "epoch": 0.2040108401084011,
      "step": 941,
      "training_loss": 7.838369846343994
    },
    {
      "epoch": 0.2040108401084011,
      "step": 941,
      "training_loss": 6.731426239013672
    },
    {
      "epoch": 0.2040108401084011,
      "step": 941,
      "training_loss": 7.410549163818359
    },
    {
      "epoch": 0.2040108401084011,
      "step": 941,
      "training_loss": 7.513400077819824
    },
    {
      "epoch": 0.20422764227642276,
      "step": 942,
      "training_loss": 6.408084869384766
    },
    {
      "epoch": 0.20422764227642276,
      "step": 942,
      "training_loss": 7.906741142272949
    },
    {
      "epoch": 0.20422764227642276,
      "step": 942,
      "training_loss": 6.433056831359863
    },
    {
      "epoch": 0.20422764227642276,
      "step": 942,
      "training_loss": 7.656757831573486
    },
    {
      "epoch": 0.20444444444444446,
      "step": 943,
      "training_loss": 7.454322814941406
    },
    {
      "epoch": 0.20444444444444446,
      "step": 943,
      "training_loss": 5.339118480682373
    },
    {
      "epoch": 0.20444444444444446,
      "step": 943,
      "training_loss": 7.137185096740723
    },
    {
      "epoch": 0.20444444444444446,
      "step": 943,
      "training_loss": 7.740235328674316
    },
    {
      "epoch": 0.20466124661246612,
      "grad_norm": 11.289227485656738,
      "learning_rate": 1e-05,
      "loss": 6.9704,
      "step": 944
    },
    {
      "epoch": 0.20466124661246612,
      "step": 944,
      "training_loss": 4.619752883911133
    },
    {
      "epoch": 0.20466124661246612,
      "step": 944,
      "training_loss": 7.070964813232422
    },
    {
      "epoch": 0.20466124661246612,
      "step": 944,
      "training_loss": 5.570656776428223
    },
    {
      "epoch": 0.20466124661246612,
      "step": 944,
      "training_loss": 7.34799861907959
    },
    {
      "epoch": 0.2048780487804878,
      "step": 945,
      "training_loss": 7.2077226638793945
    },
    {
      "epoch": 0.2048780487804878,
      "step": 945,
      "training_loss": 7.121876239776611
    },
    {
      "epoch": 0.2048780487804878,
      "step": 945,
      "training_loss": 6.625929832458496
    },
    {
      "epoch": 0.2048780487804878,
      "step": 945,
      "training_loss": 6.620075225830078
    },
    {
      "epoch": 0.2050948509485095,
      "step": 946,
      "training_loss": 6.47477388381958
    },
    {
      "epoch": 0.2050948509485095,
      "step": 946,
      "training_loss": 5.183115005493164
    },
    {
      "epoch": 0.2050948509485095,
      "step": 946,
      "training_loss": 5.518153667449951
    },
    {
      "epoch": 0.2050948509485095,
      "step": 946,
      "training_loss": 6.08314847946167
    },
    {
      "epoch": 0.20531165311653116,
      "step": 947,
      "training_loss": 6.8234381675720215
    },
    {
      "epoch": 0.20531165311653116,
      "step": 947,
      "training_loss": 6.417984485626221
    },
    {
      "epoch": 0.20531165311653116,
      "step": 947,
      "training_loss": 6.066508769989014
    },
    {
      "epoch": 0.20531165311653116,
      "step": 947,
      "training_loss": 6.697681427001953
    },
    {
      "epoch": 0.20552845528455285,
      "grad_norm": 11.40645694732666,
      "learning_rate": 1e-05,
      "loss": 6.3406,
      "step": 948
    },
    {
      "epoch": 0.20552845528455285,
      "step": 948,
      "training_loss": 4.964573860168457
    },
    {
      "epoch": 0.20552845528455285,
      "step": 948,
      "training_loss": 8.498941421508789
    },
    {
      "epoch": 0.20552845528455285,
      "step": 948,
      "training_loss": 7.193306922912598
    },
    {
      "epoch": 0.20552845528455285,
      "step": 948,
      "training_loss": 7.200658798217773
    },
    {
      "epoch": 0.20574525745257452,
      "step": 949,
      "training_loss": 6.908924579620361
    },
    {
      "epoch": 0.20574525745257452,
      "step": 949,
      "training_loss": 7.482834815979004
    },
    {
      "epoch": 0.20574525745257452,
      "step": 949,
      "training_loss": 7.433303356170654
    },
    {
      "epoch": 0.20574525745257452,
      "step": 949,
      "training_loss": 4.5105109214782715
    },
    {
      "epoch": 0.20596205962059622,
      "step": 950,
      "training_loss": 6.718617916107178
    },
    {
      "epoch": 0.20596205962059622,
      "step": 950,
      "training_loss": 9.7091646194458
    },
    {
      "epoch": 0.20596205962059622,
      "step": 950,
      "training_loss": 6.097064971923828
    },
    {
      "epoch": 0.20596205962059622,
      "step": 950,
      "training_loss": 6.416415691375732
    },
    {
      "epoch": 0.20617886178861788,
      "step": 951,
      "training_loss": 6.61126184463501
    },
    {
      "epoch": 0.20617886178861788,
      "step": 951,
      "training_loss": 8.457383155822754
    },
    {
      "epoch": 0.20617886178861788,
      "step": 951,
      "training_loss": 7.232575416564941
    },
    {
      "epoch": 0.20617886178861788,
      "step": 951,
      "training_loss": 8.291563034057617
    },
    {
      "epoch": 0.20639566395663958,
      "grad_norm": 14.895604133605957,
      "learning_rate": 1e-05,
      "loss": 7.1079,
      "step": 952
    },
    {
      "epoch": 0.20639566395663958,
      "step": 952,
      "training_loss": 7.280658721923828
    },
    {
      "epoch": 0.20639566395663958,
      "step": 952,
      "training_loss": 7.597265720367432
    },
    {
      "epoch": 0.20639566395663958,
      "step": 952,
      "training_loss": 3.9241111278533936
    },
    {
      "epoch": 0.20639566395663958,
      "step": 952,
      "training_loss": 5.438540458679199
    },
    {
      "epoch": 0.20661246612466125,
      "step": 953,
      "training_loss": 6.605566024780273
    },
    {
      "epoch": 0.20661246612466125,
      "step": 953,
      "training_loss": 6.0606465339660645
    },
    {
      "epoch": 0.20661246612466125,
      "step": 953,
      "training_loss": 6.953158855438232
    },
    {
      "epoch": 0.20661246612466125,
      "step": 953,
      "training_loss": 7.660801887512207
    },
    {
      "epoch": 0.20682926829268292,
      "step": 954,
      "training_loss": 6.734930515289307
    },
    {
      "epoch": 0.20682926829268292,
      "step": 954,
      "training_loss": 7.091240406036377
    },
    {
      "epoch": 0.20682926829268292,
      "step": 954,
      "training_loss": 6.9032769203186035
    },
    {
      "epoch": 0.20682926829268292,
      "step": 954,
      "training_loss": 6.8353705406188965
    },
    {
      "epoch": 0.2070460704607046,
      "step": 955,
      "training_loss": 7.350099563598633
    },
    {
      "epoch": 0.2070460704607046,
      "step": 955,
      "training_loss": 5.543229103088379
    },
    {
      "epoch": 0.2070460704607046,
      "step": 955,
      "training_loss": 6.937189102172852
    },
    {
      "epoch": 0.2070460704607046,
      "step": 955,
      "training_loss": 7.310955047607422
    },
    {
      "epoch": 0.20726287262872628,
      "grad_norm": 10.218572616577148,
      "learning_rate": 1e-05,
      "loss": 6.6392,
      "step": 956
    },
    {
      "epoch": 0.20726287262872628,
      "step": 956,
      "training_loss": 6.859731197357178
    },
    {
      "epoch": 0.20726287262872628,
      "step": 956,
      "training_loss": 7.008071422576904
    },
    {
      "epoch": 0.20726287262872628,
      "step": 956,
      "training_loss": 8.011085510253906
    },
    {
      "epoch": 0.20726287262872628,
      "step": 956,
      "training_loss": 7.731377601623535
    },
    {
      "epoch": 0.20747967479674798,
      "step": 957,
      "training_loss": 7.272496700286865
    },
    {
      "epoch": 0.20747967479674798,
      "step": 957,
      "training_loss": 4.171160697937012
    },
    {
      "epoch": 0.20747967479674798,
      "step": 957,
      "training_loss": 7.257751941680908
    },
    {
      "epoch": 0.20747967479674798,
      "step": 957,
      "training_loss": 6.82198429107666
    },
    {
      "epoch": 0.20769647696476964,
      "step": 958,
      "training_loss": 6.681884288787842
    },
    {
      "epoch": 0.20769647696476964,
      "step": 958,
      "training_loss": 6.663392066955566
    },
    {
      "epoch": 0.20769647696476964,
      "step": 958,
      "training_loss": 4.145452976226807
    },
    {
      "epoch": 0.20769647696476964,
      "step": 958,
      "training_loss": 6.850218772888184
    },
    {
      "epoch": 0.20791327913279134,
      "step": 959,
      "training_loss": 6.668527603149414
    },
    {
      "epoch": 0.20791327913279134,
      "step": 959,
      "training_loss": 8.024252891540527
    },
    {
      "epoch": 0.20791327913279134,
      "step": 959,
      "training_loss": 5.841175556182861
    },
    {
      "epoch": 0.20791327913279134,
      "step": 959,
      "training_loss": 6.653477668762207
    },
    {
      "epoch": 0.208130081300813,
      "grad_norm": 11.750377655029297,
      "learning_rate": 1e-05,
      "loss": 6.6664,
      "step": 960
    },
    {
      "epoch": 0.208130081300813,
      "step": 960,
      "training_loss": 5.928070068359375
    },
    {
      "epoch": 0.208130081300813,
      "step": 960,
      "training_loss": 6.183848857879639
    },
    {
      "epoch": 0.208130081300813,
      "step": 960,
      "training_loss": 7.669580936431885
    },
    {
      "epoch": 0.208130081300813,
      "step": 960,
      "training_loss": 7.122302055358887
    },
    {
      "epoch": 0.20834688346883468,
      "step": 961,
      "training_loss": 6.993180751800537
    },
    {
      "epoch": 0.20834688346883468,
      "step": 961,
      "training_loss": 6.364787578582764
    },
    {
      "epoch": 0.20834688346883468,
      "step": 961,
      "training_loss": 6.935750961303711
    },
    {
      "epoch": 0.20834688346883468,
      "step": 961,
      "training_loss": 7.145972728729248
    },
    {
      "epoch": 0.20856368563685637,
      "step": 962,
      "training_loss": 7.2729034423828125
    },
    {
      "epoch": 0.20856368563685637,
      "step": 962,
      "training_loss": 7.396655082702637
    },
    {
      "epoch": 0.20856368563685637,
      "step": 962,
      "training_loss": 7.3157548904418945
    },
    {
      "epoch": 0.20856368563685637,
      "step": 962,
      "training_loss": 7.089907646179199
    },
    {
      "epoch": 0.20878048780487804,
      "step": 963,
      "training_loss": 4.227067947387695
    },
    {
      "epoch": 0.20878048780487804,
      "step": 963,
      "training_loss": 7.812112808227539
    },
    {
      "epoch": 0.20878048780487804,
      "step": 963,
      "training_loss": 6.902698993682861
    },
    {
      "epoch": 0.20878048780487804,
      "step": 963,
      "training_loss": 5.8673882484436035
    },
    {
      "epoch": 0.20899728997289974,
      "grad_norm": 13.375202178955078,
      "learning_rate": 1e-05,
      "loss": 6.7642,
      "step": 964
    },
    {
      "epoch": 0.20899728997289974,
      "step": 964,
      "training_loss": 7.1429877281188965
    },
    {
      "epoch": 0.20899728997289974,
      "step": 964,
      "training_loss": 7.591956615447998
    },
    {
      "epoch": 0.20899728997289974,
      "step": 964,
      "training_loss": 6.87971830368042
    },
    {
      "epoch": 0.20899728997289974,
      "step": 964,
      "training_loss": 5.7360687255859375
    },
    {
      "epoch": 0.2092140921409214,
      "step": 965,
      "training_loss": 7.714488506317139
    },
    {
      "epoch": 0.2092140921409214,
      "step": 965,
      "training_loss": 5.534103870391846
    },
    {
      "epoch": 0.2092140921409214,
      "step": 965,
      "training_loss": 7.8012471199035645
    },
    {
      "epoch": 0.2092140921409214,
      "step": 965,
      "training_loss": 7.341259479522705
    },
    {
      "epoch": 0.2094308943089431,
      "step": 966,
      "training_loss": 6.915891647338867
    },
    {
      "epoch": 0.2094308943089431,
      "step": 966,
      "training_loss": 7.036248207092285
    },
    {
      "epoch": 0.2094308943089431,
      "step": 966,
      "training_loss": 5.558864593505859
    },
    {
      "epoch": 0.2094308943089431,
      "step": 966,
      "training_loss": 6.712002277374268
    },
    {
      "epoch": 0.20964769647696477,
      "step": 967,
      "training_loss": 4.33690071105957
    },
    {
      "epoch": 0.20964769647696477,
      "step": 967,
      "training_loss": 6.06450891494751
    },
    {
      "epoch": 0.20964769647696477,
      "step": 967,
      "training_loss": 7.515952110290527
    },
    {
      "epoch": 0.20964769647696477,
      "step": 967,
      "training_loss": 6.011727333068848
    },
    {
      "epoch": 0.20986449864498646,
      "grad_norm": 8.393977165222168,
      "learning_rate": 1e-05,
      "loss": 6.6184,
      "step": 968
    },
    {
      "epoch": 0.20986449864498646,
      "step": 968,
      "training_loss": 5.434031009674072
    },
    {
      "epoch": 0.20986449864498646,
      "step": 968,
      "training_loss": 6.809906482696533
    },
    {
      "epoch": 0.20986449864498646,
      "step": 968,
      "training_loss": 6.582987308502197
    },
    {
      "epoch": 0.20986449864498646,
      "step": 968,
      "training_loss": 6.084590435028076
    },
    {
      "epoch": 0.21008130081300813,
      "step": 969,
      "training_loss": 7.442358016967773
    },
    {
      "epoch": 0.21008130081300813,
      "step": 969,
      "training_loss": 7.324625015258789
    },
    {
      "epoch": 0.21008130081300813,
      "step": 969,
      "training_loss": 6.68674373626709
    },
    {
      "epoch": 0.21008130081300813,
      "step": 969,
      "training_loss": 7.118534564971924
    },
    {
      "epoch": 0.2102981029810298,
      "step": 970,
      "training_loss": 6.443439960479736
    },
    {
      "epoch": 0.2102981029810298,
      "step": 970,
      "training_loss": 8.034974098205566
    },
    {
      "epoch": 0.2102981029810298,
      "step": 970,
      "training_loss": 5.8592400550842285
    },
    {
      "epoch": 0.2102981029810298,
      "step": 970,
      "training_loss": 7.294824600219727
    },
    {
      "epoch": 0.2105149051490515,
      "step": 971,
      "training_loss": 6.728196620941162
    },
    {
      "epoch": 0.2105149051490515,
      "step": 971,
      "training_loss": 6.960277557373047
    },
    {
      "epoch": 0.2105149051490515,
      "step": 971,
      "training_loss": 7.293741226196289
    },
    {
      "epoch": 0.2105149051490515,
      "step": 971,
      "training_loss": 7.7975969314575195
    },
    {
      "epoch": 0.21073170731707316,
      "grad_norm": 11.602764129638672,
      "learning_rate": 1e-05,
      "loss": 6.8685,
      "step": 972
    },
    {
      "epoch": 0.21073170731707316,
      "step": 972,
      "training_loss": 7.880792617797852
    },
    {
      "epoch": 0.21073170731707316,
      "step": 972,
      "training_loss": 6.991096496582031
    },
    {
      "epoch": 0.21073170731707316,
      "step": 972,
      "training_loss": 5.3505167961120605
    },
    {
      "epoch": 0.21073170731707316,
      "step": 972,
      "training_loss": 5.665745735168457
    },
    {
      "epoch": 0.21094850948509486,
      "step": 973,
      "training_loss": 6.479724407196045
    },
    {
      "epoch": 0.21094850948509486,
      "step": 973,
      "training_loss": 7.527515888214111
    },
    {
      "epoch": 0.21094850948509486,
      "step": 973,
      "training_loss": 8.127091407775879
    },
    {
      "epoch": 0.21094850948509486,
      "step": 973,
      "training_loss": 5.210731506347656
    },
    {
      "epoch": 0.21116531165311653,
      "step": 974,
      "training_loss": 7.011293411254883
    },
    {
      "epoch": 0.21116531165311653,
      "step": 974,
      "training_loss": 5.115030288696289
    },
    {
      "epoch": 0.21116531165311653,
      "step": 974,
      "training_loss": 5.272374629974365
    },
    {
      "epoch": 0.21116531165311653,
      "step": 974,
      "training_loss": 4.724429607391357
    },
    {
      "epoch": 0.21138211382113822,
      "step": 975,
      "training_loss": 6.958291530609131
    },
    {
      "epoch": 0.21138211382113822,
      "step": 975,
      "training_loss": 5.46183443069458
    },
    {
      "epoch": 0.21138211382113822,
      "step": 975,
      "training_loss": 6.61613655090332
    },
    {
      "epoch": 0.21138211382113822,
      "step": 975,
      "training_loss": 6.955977916717529
    },
    {
      "epoch": 0.2115989159891599,
      "grad_norm": 15.488658905029297,
      "learning_rate": 1e-05,
      "loss": 6.3343,
      "step": 976
    },
    {
      "epoch": 0.2115989159891599,
      "step": 976,
      "training_loss": 7.047172546386719
    },
    {
      "epoch": 0.2115989159891599,
      "step": 976,
      "training_loss": 6.778889179229736
    },
    {
      "epoch": 0.2115989159891599,
      "step": 976,
      "training_loss": 6.468046188354492
    },
    {
      "epoch": 0.2115989159891599,
      "step": 976,
      "training_loss": 7.206860542297363
    },
    {
      "epoch": 0.21181571815718156,
      "step": 977,
      "training_loss": 6.296598434448242
    },
    {
      "epoch": 0.21181571815718156,
      "step": 977,
      "training_loss": 7.198758125305176
    },
    {
      "epoch": 0.21181571815718156,
      "step": 977,
      "training_loss": 6.118924617767334
    },
    {
      "epoch": 0.21181571815718156,
      "step": 977,
      "training_loss": 5.543763637542725
    },
    {
      "epoch": 0.21203252032520326,
      "step": 978,
      "training_loss": 7.297406196594238
    },
    {
      "epoch": 0.21203252032520326,
      "step": 978,
      "training_loss": 6.680685043334961
    },
    {
      "epoch": 0.21203252032520326,
      "step": 978,
      "training_loss": 6.074613094329834
    },
    {
      "epoch": 0.21203252032520326,
      "step": 978,
      "training_loss": 6.890076637268066
    },
    {
      "epoch": 0.21224932249322492,
      "step": 979,
      "training_loss": 5.41463565826416
    },
    {
      "epoch": 0.21224932249322492,
      "step": 979,
      "training_loss": 6.281480312347412
    },
    {
      "epoch": 0.21224932249322492,
      "step": 979,
      "training_loss": 6.189355850219727
    },
    {
      "epoch": 0.21224932249322492,
      "step": 979,
      "training_loss": 6.584610462188721
    },
    {
      "epoch": 0.21246612466124662,
      "grad_norm": 17.52227020263672,
      "learning_rate": 1e-05,
      "loss": 6.5045,
      "step": 980
    },
    {
      "epoch": 0.21246612466124662,
      "step": 980,
      "training_loss": 6.260865688323975
    },
    {
      "epoch": 0.21246612466124662,
      "step": 980,
      "training_loss": 6.454609394073486
    },
    {
      "epoch": 0.21246612466124662,
      "step": 980,
      "training_loss": 7.354898452758789
    },
    {
      "epoch": 0.21246612466124662,
      "step": 980,
      "training_loss": 5.818613529205322
    },
    {
      "epoch": 0.2126829268292683,
      "step": 981,
      "training_loss": 4.908157825469971
    },
    {
      "epoch": 0.2126829268292683,
      "step": 981,
      "training_loss": 6.3401594161987305
    },
    {
      "epoch": 0.2126829268292683,
      "step": 981,
      "training_loss": 3.796949863433838
    },
    {
      "epoch": 0.2126829268292683,
      "step": 981,
      "training_loss": 6.0842695236206055
    },
    {
      "epoch": 0.21289972899728998,
      "step": 982,
      "training_loss": 7.516922950744629
    },
    {
      "epoch": 0.21289972899728998,
      "step": 982,
      "training_loss": 7.958730220794678
    },
    {
      "epoch": 0.21289972899728998,
      "step": 982,
      "training_loss": 7.8896164894104
    },
    {
      "epoch": 0.21289972899728998,
      "step": 982,
      "training_loss": 7.688656806945801
    },
    {
      "epoch": 0.21311653116531165,
      "step": 983,
      "training_loss": 5.963533401489258
    },
    {
      "epoch": 0.21311653116531165,
      "step": 983,
      "training_loss": 6.82605504989624
    },
    {
      "epoch": 0.21311653116531165,
      "step": 983,
      "training_loss": 5.2335004806518555
    },
    {
      "epoch": 0.21311653116531165,
      "step": 983,
      "training_loss": 7.436433792114258
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 11.881196975708008,
      "learning_rate": 1e-05,
      "loss": 6.4707,
      "step": 984
    },
    {
      "epoch": 0.21333333333333335,
      "step": 984,
      "training_loss": 5.313714027404785
    },
    {
      "epoch": 0.21333333333333335,
      "step": 984,
      "training_loss": 7.381152629852295
    },
    {
      "epoch": 0.21333333333333335,
      "step": 984,
      "training_loss": 6.588947772979736
    },
    {
      "epoch": 0.21333333333333335,
      "step": 984,
      "training_loss": 7.0042948722839355
    },
    {
      "epoch": 0.21355013550135502,
      "step": 985,
      "training_loss": 7.878608226776123
    },
    {
      "epoch": 0.21355013550135502,
      "step": 985,
      "training_loss": 6.504297733306885
    },
    {
      "epoch": 0.21355013550135502,
      "step": 985,
      "training_loss": 7.596579074859619
    },
    {
      "epoch": 0.21355013550135502,
      "step": 985,
      "training_loss": 7.427313804626465
    },
    {
      "epoch": 0.21376693766937668,
      "step": 986,
      "training_loss": 5.0690693855285645
    },
    {
      "epoch": 0.21376693766937668,
      "step": 986,
      "training_loss": 6.625874042510986
    },
    {
      "epoch": 0.21376693766937668,
      "step": 986,
      "training_loss": 7.112913131713867
    },
    {
      "epoch": 0.21376693766937668,
      "step": 986,
      "training_loss": 7.187612533569336
    },
    {
      "epoch": 0.21398373983739838,
      "step": 987,
      "training_loss": 7.487321376800537
    },
    {
      "epoch": 0.21398373983739838,
      "step": 987,
      "training_loss": 8.240203857421875
    },
    {
      "epoch": 0.21398373983739838,
      "step": 987,
      "training_loss": 7.323732376098633
    },
    {
      "epoch": 0.21398373983739838,
      "step": 987,
      "training_loss": 6.808518886566162
    },
    {
      "epoch": 0.21420054200542005,
      "grad_norm": 10.379949569702148,
      "learning_rate": 1e-05,
      "loss": 6.9719,
      "step": 988
    },
    {
      "epoch": 0.21420054200542005,
      "step": 988,
      "training_loss": 8.247991561889648
    },
    {
      "epoch": 0.21420054200542005,
      "step": 988,
      "training_loss": 7.555893421173096
    },
    {
      "epoch": 0.21420054200542005,
      "step": 988,
      "training_loss": 6.266180515289307
    },
    {
      "epoch": 0.21420054200542005,
      "step": 988,
      "training_loss": 5.70490837097168
    },
    {
      "epoch": 0.21441734417344174,
      "step": 989,
      "training_loss": 7.771803379058838
    },
    {
      "epoch": 0.21441734417344174,
      "step": 989,
      "training_loss": 5.691685676574707
    },
    {
      "epoch": 0.21441734417344174,
      "step": 989,
      "training_loss": 7.960183143615723
    },
    {
      "epoch": 0.21441734417344174,
      "step": 989,
      "training_loss": 7.01726770401001
    },
    {
      "epoch": 0.2146341463414634,
      "step": 990,
      "training_loss": 6.429697036743164
    },
    {
      "epoch": 0.2146341463414634,
      "step": 990,
      "training_loss": 4.593037128448486
    },
    {
      "epoch": 0.2146341463414634,
      "step": 990,
      "training_loss": 7.2281036376953125
    },
    {
      "epoch": 0.2146341463414634,
      "step": 990,
      "training_loss": 6.588313102722168
    },
    {
      "epoch": 0.2148509485094851,
      "step": 991,
      "training_loss": 6.999197959899902
    },
    {
      "epoch": 0.2148509485094851,
      "step": 991,
      "training_loss": 5.793789386749268
    },
    {
      "epoch": 0.2148509485094851,
      "step": 991,
      "training_loss": 6.844557285308838
    },
    {
      "epoch": 0.2148509485094851,
      "step": 991,
      "training_loss": 11.2730073928833
    },
    {
      "epoch": 0.21506775067750677,
      "grad_norm": 13.400458335876465,
      "learning_rate": 1e-05,
      "loss": 6.9979,
      "step": 992
    },
    {
      "epoch": 0.21506775067750677,
      "step": 992,
      "training_loss": 6.594588279724121
    },
    {
      "epoch": 0.21506775067750677,
      "step": 992,
      "training_loss": 6.738476276397705
    },
    {
      "epoch": 0.21506775067750677,
      "step": 992,
      "training_loss": 7.529223918914795
    },
    {
      "epoch": 0.21506775067750677,
      "step": 992,
      "training_loss": 7.42839241027832
    },
    {
      "epoch": 0.21528455284552844,
      "step": 993,
      "training_loss": 6.831856727600098
    },
    {
      "epoch": 0.21528455284552844,
      "step": 993,
      "training_loss": 7.2448201179504395
    },
    {
      "epoch": 0.21528455284552844,
      "step": 993,
      "training_loss": 7.656555652618408
    },
    {
      "epoch": 0.21528455284552844,
      "step": 993,
      "training_loss": 7.788643836975098
    },
    {
      "epoch": 0.21550135501355014,
      "step": 994,
      "training_loss": 8.180100440979004
    },
    {
      "epoch": 0.21550135501355014,
      "step": 994,
      "training_loss": 7.893927097320557
    },
    {
      "epoch": 0.21550135501355014,
      "step": 994,
      "training_loss": 5.6071696281433105
    },
    {
      "epoch": 0.21550135501355014,
      "step": 994,
      "training_loss": 7.460078716278076
    },
    {
      "epoch": 0.2157181571815718,
      "step": 995,
      "training_loss": 5.867816925048828
    },
    {
      "epoch": 0.2157181571815718,
      "step": 995,
      "training_loss": 7.226640224456787
    },
    {
      "epoch": 0.2157181571815718,
      "step": 995,
      "training_loss": 7.113525867462158
    },
    {
      "epoch": 0.2157181571815718,
      "step": 995,
      "training_loss": 7.069512367248535
    },
    {
      "epoch": 0.2159349593495935,
      "grad_norm": 11.355611801147461,
      "learning_rate": 1e-05,
      "loss": 7.1395,
      "step": 996
    },
    {
      "epoch": 0.2159349593495935,
      "step": 996,
      "training_loss": 6.504165172576904
    },
    {
      "epoch": 0.2159349593495935,
      "step": 996,
      "training_loss": 4.114959239959717
    },
    {
      "epoch": 0.2159349593495935,
      "step": 996,
      "training_loss": 6.504206657409668
    },
    {
      "epoch": 0.2159349593495935,
      "step": 996,
      "training_loss": 8.179671287536621
    },
    {
      "epoch": 0.21615176151761517,
      "step": 997,
      "training_loss": 7.374756813049316
    },
    {
      "epoch": 0.21615176151761517,
      "step": 997,
      "training_loss": 7.024382591247559
    },
    {
      "epoch": 0.21615176151761517,
      "step": 997,
      "training_loss": 6.6649699211120605
    },
    {
      "epoch": 0.21615176151761517,
      "step": 997,
      "training_loss": 7.246527671813965
    },
    {
      "epoch": 0.21636856368563687,
      "step": 998,
      "training_loss": 7.6864333152771
    },
    {
      "epoch": 0.21636856368563687,
      "step": 998,
      "training_loss": 7.485249042510986
    },
    {
      "epoch": 0.21636856368563687,
      "step": 998,
      "training_loss": 4.892784118652344
    },
    {
      "epoch": 0.21636856368563687,
      "step": 998,
      "training_loss": 7.325279712677002
    },
    {
      "epoch": 0.21658536585365853,
      "step": 999,
      "training_loss": 6.876127243041992
    },
    {
      "epoch": 0.21658536585365853,
      "step": 999,
      "training_loss": 6.783810615539551
    },
    {
      "epoch": 0.21658536585365853,
      "step": 999,
      "training_loss": 5.897454261779785
    },
    {
      "epoch": 0.21658536585365853,
      "step": 999,
      "training_loss": 6.970232009887695
    },
    {
      "epoch": 0.21680216802168023,
      "grad_norm": 14.76280689239502,
      "learning_rate": 1e-05,
      "loss": 6.7207,
      "step": 1000
    },
    {
      "epoch": 0.21680216802168023,
      "step": 1000,
      "training_loss": 4.639580726623535
    },
    {
      "epoch": 0.21680216802168023,
      "step": 1000,
      "training_loss": 6.425708770751953
    },
    {
      "epoch": 0.21680216802168023,
      "step": 1000,
      "training_loss": 7.189006805419922
    },
    {
      "epoch": 0.21680216802168023,
      "step": 1000,
      "training_loss": 5.230274677276611
    },
    {
      "epoch": 0.2170189701897019,
      "step": 1001,
      "training_loss": 7.535024642944336
    },
    {
      "epoch": 0.2170189701897019,
      "step": 1001,
      "training_loss": 6.388510704040527
    },
    {
      "epoch": 0.2170189701897019,
      "step": 1001,
      "training_loss": 4.518159866333008
    },
    {
      "epoch": 0.2170189701897019,
      "step": 1001,
      "training_loss": 5.931512355804443
    },
    {
      "epoch": 0.21723577235772357,
      "step": 1002,
      "training_loss": 7.352877616882324
    },
    {
      "epoch": 0.21723577235772357,
      "step": 1002,
      "training_loss": 6.563340187072754
    },
    {
      "epoch": 0.21723577235772357,
      "step": 1002,
      "training_loss": 6.997580051422119
    },
    {
      "epoch": 0.21723577235772357,
      "step": 1002,
      "training_loss": 6.002429485321045
    },
    {
      "epoch": 0.21745257452574526,
      "step": 1003,
      "training_loss": 7.025942325592041
    },
    {
      "epoch": 0.21745257452574526,
      "step": 1003,
      "training_loss": 7.38901948928833
    },
    {
      "epoch": 0.21745257452574526,
      "step": 1003,
      "training_loss": 7.099486351013184
    },
    {
      "epoch": 0.21745257452574526,
      "step": 1003,
      "training_loss": 7.771266937255859
    },
    {
      "epoch": 0.21766937669376693,
      "grad_norm": 11.792217254638672,
      "learning_rate": 1e-05,
      "loss": 6.5037,
      "step": 1004
    },
    {
      "epoch": 0.21766937669376693,
      "step": 1004,
      "training_loss": 6.696483612060547
    },
    {
      "epoch": 0.21766937669376693,
      "step": 1004,
      "training_loss": 5.489889144897461
    },
    {
      "epoch": 0.21766937669376693,
      "step": 1004,
      "training_loss": 7.720519065856934
    },
    {
      "epoch": 0.21766937669376693,
      "step": 1004,
      "training_loss": 7.827459335327148
    },
    {
      "epoch": 0.21788617886178863,
      "step": 1005,
      "training_loss": 6.147801876068115
    },
    {
      "epoch": 0.21788617886178863,
      "step": 1005,
      "training_loss": 5.431487560272217
    },
    {
      "epoch": 0.21788617886178863,
      "step": 1005,
      "training_loss": 6.925555229187012
    },
    {
      "epoch": 0.21788617886178863,
      "step": 1005,
      "training_loss": 6.783900737762451
    },
    {
      "epoch": 0.2181029810298103,
      "step": 1006,
      "training_loss": 7.8654985427856445
    },
    {
      "epoch": 0.2181029810298103,
      "step": 1006,
      "training_loss": 7.391892433166504
    },
    {
      "epoch": 0.2181029810298103,
      "step": 1006,
      "training_loss": 7.5456695556640625
    },
    {
      "epoch": 0.2181029810298103,
      "step": 1006,
      "training_loss": 6.309708595275879
    },
    {
      "epoch": 0.218319783197832,
      "step": 1007,
      "training_loss": 6.2393412590026855
    },
    {
      "epoch": 0.218319783197832,
      "step": 1007,
      "training_loss": 7.130129814147949
    },
    {
      "epoch": 0.218319783197832,
      "step": 1007,
      "training_loss": 7.403193473815918
    },
    {
      "epoch": 0.218319783197832,
      "step": 1007,
      "training_loss": 7.47944974899292
    },
    {
      "epoch": 0.21853658536585366,
      "grad_norm": 14.302849769592285,
      "learning_rate": 1e-05,
      "loss": 6.8992,
      "step": 1008
    },
    {
      "epoch": 0.21853658536585366,
      "step": 1008,
      "training_loss": 6.376709461212158
    },
    {
      "epoch": 0.21853658536585366,
      "step": 1008,
      "training_loss": 5.948699474334717
    },
    {
      "epoch": 0.21853658536585366,
      "step": 1008,
      "training_loss": 6.08711576461792
    },
    {
      "epoch": 0.21853658536585366,
      "step": 1008,
      "training_loss": 7.14471435546875
    },
    {
      "epoch": 0.21875338753387533,
      "step": 1009,
      "training_loss": 7.20417594909668
    },
    {
      "epoch": 0.21875338753387533,
      "step": 1009,
      "training_loss": 7.240353107452393
    },
    {
      "epoch": 0.21875338753387533,
      "step": 1009,
      "training_loss": 3.5370066165924072
    },
    {
      "epoch": 0.21875338753387533,
      "step": 1009,
      "training_loss": 6.713709831237793
    },
    {
      "epoch": 0.21897018970189702,
      "step": 1010,
      "training_loss": 6.020421504974365
    },
    {
      "epoch": 0.21897018970189702,
      "step": 1010,
      "training_loss": 7.789578914642334
    },
    {
      "epoch": 0.21897018970189702,
      "step": 1010,
      "training_loss": 7.992901802062988
    },
    {
      "epoch": 0.21897018970189702,
      "step": 1010,
      "training_loss": 7.0849223136901855
    },
    {
      "epoch": 0.2191869918699187,
      "step": 1011,
      "training_loss": 6.093425750732422
    },
    {
      "epoch": 0.2191869918699187,
      "step": 1011,
      "training_loss": 6.896345615386963
    },
    {
      "epoch": 0.2191869918699187,
      "step": 1011,
      "training_loss": 6.944741725921631
    },
    {
      "epoch": 0.2191869918699187,
      "step": 1011,
      "training_loss": 6.509200096130371
    },
    {
      "epoch": 0.2194037940379404,
      "grad_norm": 15.387384414672852,
      "learning_rate": 1e-05,
      "loss": 6.599,
      "step": 1012
    },
    {
      "epoch": 0.2194037940379404,
      "step": 1012,
      "training_loss": 5.55635929107666
    },
    {
      "epoch": 0.2194037940379404,
      "step": 1012,
      "training_loss": 5.291263103485107
    },
    {
      "epoch": 0.2194037940379404,
      "step": 1012,
      "training_loss": 6.423808574676514
    },
    {
      "epoch": 0.2194037940379404,
      "step": 1012,
      "training_loss": 6.886758804321289
    },
    {
      "epoch": 0.21962059620596205,
      "step": 1013,
      "training_loss": 4.289411544799805
    },
    {
      "epoch": 0.21962059620596205,
      "step": 1013,
      "training_loss": 6.896795749664307
    },
    {
      "epoch": 0.21962059620596205,
      "step": 1013,
      "training_loss": 6.50909948348999
    },
    {
      "epoch": 0.21962059620596205,
      "step": 1013,
      "training_loss": 6.867037296295166
    },
    {
      "epoch": 0.21983739837398375,
      "step": 1014,
      "training_loss": 6.864386081695557
    },
    {
      "epoch": 0.21983739837398375,
      "step": 1014,
      "training_loss": 6.560307502746582
    },
    {
      "epoch": 0.21983739837398375,
      "step": 1014,
      "training_loss": 6.960869312286377
    },
    {
      "epoch": 0.21983739837398375,
      "step": 1014,
      "training_loss": 6.860586166381836
    },
    {
      "epoch": 0.22005420054200542,
      "step": 1015,
      "training_loss": 7.025875091552734
    },
    {
      "epoch": 0.22005420054200542,
      "step": 1015,
      "training_loss": 6.436751365661621
    },
    {
      "epoch": 0.22005420054200542,
      "step": 1015,
      "training_loss": 7.859745025634766
    },
    {
      "epoch": 0.22005420054200542,
      "step": 1015,
      "training_loss": 7.191151142120361
    },
    {
      "epoch": 0.22027100271002711,
      "grad_norm": 16.734689712524414,
      "learning_rate": 1e-05,
      "loss": 6.53,
      "step": 1016
    },
    {
      "epoch": 0.22027100271002711,
      "step": 1016,
      "training_loss": 6.968222618103027
    },
    {
      "epoch": 0.22027100271002711,
      "step": 1016,
      "training_loss": 7.908885478973389
    },
    {
      "epoch": 0.22027100271002711,
      "step": 1016,
      "training_loss": 6.979062080383301
    },
    {
      "epoch": 0.22027100271002711,
      "step": 1016,
      "training_loss": 7.1229939460754395
    },
    {
      "epoch": 0.22048780487804878,
      "step": 1017,
      "training_loss": 8.451624870300293
    },
    {
      "epoch": 0.22048780487804878,
      "step": 1017,
      "training_loss": 7.616310119628906
    },
    {
      "epoch": 0.22048780487804878,
      "step": 1017,
      "training_loss": 6.166813850402832
    },
    {
      "epoch": 0.22048780487804878,
      "step": 1017,
      "training_loss": 8.092019081115723
    },
    {
      "epoch": 0.22070460704607045,
      "step": 1018,
      "training_loss": 7.419759750366211
    },
    {
      "epoch": 0.22070460704607045,
      "step": 1018,
      "training_loss": 7.232724189758301
    },
    {
      "epoch": 0.22070460704607045,
      "step": 1018,
      "training_loss": 4.947032928466797
    },
    {
      "epoch": 0.22070460704607045,
      "step": 1018,
      "training_loss": 6.692845344543457
    },
    {
      "epoch": 0.22092140921409215,
      "step": 1019,
      "training_loss": 7.370490550994873
    },
    {
      "epoch": 0.22092140921409215,
      "step": 1019,
      "training_loss": 5.569819450378418
    },
    {
      "epoch": 0.22092140921409215,
      "step": 1019,
      "training_loss": 7.966639041900635
    },
    {
      "epoch": 0.22092140921409215,
      "step": 1019,
      "training_loss": 6.453360557556152
    },
    {
      "epoch": 0.22113821138211381,
      "grad_norm": 10.769204139709473,
      "learning_rate": 1e-05,
      "loss": 7.0599,
      "step": 1020
    },
    {
      "epoch": 0.22113821138211381,
      "step": 1020,
      "training_loss": 7.00270938873291
    },
    {
      "epoch": 0.22113821138211381,
      "step": 1020,
      "training_loss": 8.054293632507324
    },
    {
      "epoch": 0.22113821138211381,
      "step": 1020,
      "training_loss": 7.217811107635498
    },
    {
      "epoch": 0.22113821138211381,
      "step": 1020,
      "training_loss": 6.568742275238037
    },
    {
      "epoch": 0.2213550135501355,
      "step": 1021,
      "training_loss": 7.8451337814331055
    },
    {
      "epoch": 0.2213550135501355,
      "step": 1021,
      "training_loss": 7.552143096923828
    },
    {
      "epoch": 0.2213550135501355,
      "step": 1021,
      "training_loss": 7.4252824783325195
    },
    {
      "epoch": 0.2213550135501355,
      "step": 1021,
      "training_loss": 4.477390766143799
    },
    {
      "epoch": 0.22157181571815718,
      "step": 1022,
      "training_loss": 7.280358791351318
    },
    {
      "epoch": 0.22157181571815718,
      "step": 1022,
      "training_loss": 5.269411087036133
    },
    {
      "epoch": 0.22157181571815718,
      "step": 1022,
      "training_loss": 7.797352313995361
    },
    {
      "epoch": 0.22157181571815718,
      "step": 1022,
      "training_loss": 7.513314723968506
    },
    {
      "epoch": 0.22178861788617887,
      "step": 1023,
      "training_loss": 6.780852794647217
    },
    {
      "epoch": 0.22178861788617887,
      "step": 1023,
      "training_loss": 7.7640910148620605
    },
    {
      "epoch": 0.22178861788617887,
      "step": 1023,
      "training_loss": 6.018237590789795
    },
    {
      "epoch": 0.22178861788617887,
      "step": 1023,
      "training_loss": 7.315698623657227
    },
    {
      "epoch": 0.22200542005420054,
      "grad_norm": 14.323233604431152,
      "learning_rate": 1e-05,
      "loss": 6.9927,
      "step": 1024
    },
    {
      "epoch": 0.22200542005420054,
      "step": 1024,
      "training_loss": 6.72427225112915
    },
    {
      "epoch": 0.22200542005420054,
      "step": 1024,
      "training_loss": 5.750377178192139
    },
    {
      "epoch": 0.22200542005420054,
      "step": 1024,
      "training_loss": 7.06068229675293
    },
    {
      "epoch": 0.22200542005420054,
      "step": 1024,
      "training_loss": 5.703455924987793
    },
    {
      "epoch": 0.2222222222222222,
      "step": 1025,
      "training_loss": 6.876993179321289
    },
    {
      "epoch": 0.2222222222222222,
      "step": 1025,
      "training_loss": 7.575263023376465
    },
    {
      "epoch": 0.2222222222222222,
      "step": 1025,
      "training_loss": 5.659398555755615
    },
    {
      "epoch": 0.2222222222222222,
      "step": 1025,
      "training_loss": 7.382283687591553
    },
    {
      "epoch": 0.2224390243902439,
      "step": 1026,
      "training_loss": 3.9098026752471924
    },
    {
      "epoch": 0.2224390243902439,
      "step": 1026,
      "training_loss": 7.766237258911133
    },
    {
      "epoch": 0.2224390243902439,
      "step": 1026,
      "training_loss": 6.788718223571777
    },
    {
      "epoch": 0.2224390243902439,
      "step": 1026,
      "training_loss": 7.7653093338012695
    },
    {
      "epoch": 0.22265582655826557,
      "step": 1027,
      "training_loss": 6.985245704650879
    },
    {
      "epoch": 0.22265582655826557,
      "step": 1027,
      "training_loss": 6.558282852172852
    },
    {
      "epoch": 0.22265582655826557,
      "step": 1027,
      "training_loss": 6.98795223236084
    },
    {
      "epoch": 0.22265582655826557,
      "step": 1027,
      "training_loss": 7.036118984222412
    },
    {
      "epoch": 0.22287262872628727,
      "grad_norm": 9.762934684753418,
      "learning_rate": 1e-05,
      "loss": 6.6581,
      "step": 1028
    },
    {
      "epoch": 0.22287262872628727,
      "step": 1028,
      "training_loss": 6.677422523498535
    },
    {
      "epoch": 0.22287262872628727,
      "step": 1028,
      "training_loss": 6.778899192810059
    },
    {
      "epoch": 0.22287262872628727,
      "step": 1028,
      "training_loss": 6.543402194976807
    },
    {
      "epoch": 0.22287262872628727,
      "step": 1028,
      "training_loss": 6.13249397277832
    },
    {
      "epoch": 0.22308943089430894,
      "step": 1029,
      "training_loss": 7.02332067489624
    },
    {
      "epoch": 0.22308943089430894,
      "step": 1029,
      "training_loss": 7.4352707862854
    },
    {
      "epoch": 0.22308943089430894,
      "step": 1029,
      "training_loss": 6.233916282653809
    },
    {
      "epoch": 0.22308943089430894,
      "step": 1029,
      "training_loss": 6.021007537841797
    },
    {
      "epoch": 0.22330623306233063,
      "step": 1030,
      "training_loss": 5.861043453216553
    },
    {
      "epoch": 0.22330623306233063,
      "step": 1030,
      "training_loss": 6.937435626983643
    },
    {
      "epoch": 0.22330623306233063,
      "step": 1030,
      "training_loss": 7.431464672088623
    },
    {
      "epoch": 0.22330623306233063,
      "step": 1030,
      "training_loss": 7.599756717681885
    },
    {
      "epoch": 0.2235230352303523,
      "step": 1031,
      "training_loss": 6.637553691864014
    },
    {
      "epoch": 0.2235230352303523,
      "step": 1031,
      "training_loss": 6.833459854125977
    },
    {
      "epoch": 0.2235230352303523,
      "step": 1031,
      "training_loss": 6.434797763824463
    },
    {
      "epoch": 0.2235230352303523,
      "step": 1031,
      "training_loss": 7.169924259185791
    },
    {
      "epoch": 0.223739837398374,
      "grad_norm": 11.284574508666992,
      "learning_rate": 1e-05,
      "loss": 6.7344,
      "step": 1032
    },
    {
      "epoch": 0.223739837398374,
      "step": 1032,
      "training_loss": 6.6616621017456055
    },
    {
      "epoch": 0.223739837398374,
      "step": 1032,
      "training_loss": 6.417482376098633
    },
    {
      "epoch": 0.223739837398374,
      "step": 1032,
      "training_loss": 6.257296085357666
    },
    {
      "epoch": 0.223739837398374,
      "step": 1032,
      "training_loss": 6.578361988067627
    },
    {
      "epoch": 0.22395663956639567,
      "step": 1033,
      "training_loss": 8.113287925720215
    },
    {
      "epoch": 0.22395663956639567,
      "step": 1033,
      "training_loss": 4.942067623138428
    },
    {
      "epoch": 0.22395663956639567,
      "step": 1033,
      "training_loss": 6.794788837432861
    },
    {
      "epoch": 0.22395663956639567,
      "step": 1033,
      "training_loss": 7.104677677154541
    },
    {
      "epoch": 0.22417344173441733,
      "step": 1034,
      "training_loss": 7.640349388122559
    },
    {
      "epoch": 0.22417344173441733,
      "step": 1034,
      "training_loss": 7.090174674987793
    },
    {
      "epoch": 0.22417344173441733,
      "step": 1034,
      "training_loss": 6.982470512390137
    },
    {
      "epoch": 0.22417344173441733,
      "step": 1034,
      "training_loss": 4.884188175201416
    },
    {
      "epoch": 0.22439024390243903,
      "step": 1035,
      "training_loss": 6.498918533325195
    },
    {
      "epoch": 0.22439024390243903,
      "step": 1035,
      "training_loss": 7.2637457847595215
    },
    {
      "epoch": 0.22439024390243903,
      "step": 1035,
      "training_loss": 7.026363849639893
    },
    {
      "epoch": 0.22439024390243903,
      "step": 1035,
      "training_loss": 7.559170246124268
    },
    {
      "epoch": 0.2246070460704607,
      "grad_norm": 14.572357177734375,
      "learning_rate": 1e-05,
      "loss": 6.7384,
      "step": 1036
    },
    {
      "epoch": 0.2246070460704607,
      "step": 1036,
      "training_loss": 7.041715621948242
    },
    {
      "epoch": 0.2246070460704607,
      "step": 1036,
      "training_loss": 7.1195454597473145
    },
    {
      "epoch": 0.2246070460704607,
      "step": 1036,
      "training_loss": 4.780422210693359
    },
    {
      "epoch": 0.2246070460704607,
      "step": 1036,
      "training_loss": 5.863949775695801
    },
    {
      "epoch": 0.2248238482384824,
      "step": 1037,
      "training_loss": 7.278903961181641
    },
    {
      "epoch": 0.2248238482384824,
      "step": 1037,
      "training_loss": 6.844699382781982
    },
    {
      "epoch": 0.2248238482384824,
      "step": 1037,
      "training_loss": 4.66014289855957
    },
    {
      "epoch": 0.2248238482384824,
      "step": 1037,
      "training_loss": 6.4880266189575195
    },
    {
      "epoch": 0.22504065040650406,
      "step": 1038,
      "training_loss": 7.661743640899658
    },
    {
      "epoch": 0.22504065040650406,
      "step": 1038,
      "training_loss": 6.437438488006592
    },
    {
      "epoch": 0.22504065040650406,
      "step": 1038,
      "training_loss": 6.425034999847412
    },
    {
      "epoch": 0.22504065040650406,
      "step": 1038,
      "training_loss": 4.752225875854492
    },
    {
      "epoch": 0.22525745257452576,
      "step": 1039,
      "training_loss": 7.095402240753174
    },
    {
      "epoch": 0.22525745257452576,
      "step": 1039,
      "training_loss": 6.311557292938232
    },
    {
      "epoch": 0.22525745257452576,
      "step": 1039,
      "training_loss": 6.4936676025390625
    },
    {
      "epoch": 0.22525745257452576,
      "step": 1039,
      "training_loss": 7.6771368980407715
    },
    {
      "epoch": 0.22547425474254743,
      "grad_norm": 12.016794204711914,
      "learning_rate": 1e-05,
      "loss": 6.4332,
      "step": 1040
    },
    {
      "epoch": 0.22547425474254743,
      "step": 1040,
      "training_loss": 8.211875915527344
    },
    {
      "epoch": 0.22547425474254743,
      "step": 1040,
      "training_loss": 6.397835731506348
    },
    {
      "epoch": 0.22547425474254743,
      "step": 1040,
      "training_loss": 6.305916786193848
    },
    {
      "epoch": 0.22547425474254743,
      "step": 1040,
      "training_loss": 5.886252403259277
    },
    {
      "epoch": 0.2256910569105691,
      "step": 1041,
      "training_loss": 7.407899856567383
    },
    {
      "epoch": 0.2256910569105691,
      "step": 1041,
      "training_loss": 7.501275539398193
    },
    {
      "epoch": 0.2256910569105691,
      "step": 1041,
      "training_loss": 7.347240447998047
    },
    {
      "epoch": 0.2256910569105691,
      "step": 1041,
      "training_loss": 7.449887752532959
    },
    {
      "epoch": 0.2259078590785908,
      "step": 1042,
      "training_loss": 7.677231788635254
    },
    {
      "epoch": 0.2259078590785908,
      "step": 1042,
      "training_loss": 6.2534027099609375
    },
    {
      "epoch": 0.2259078590785908,
      "step": 1042,
      "training_loss": 7.916149139404297
    },
    {
      "epoch": 0.2259078590785908,
      "step": 1042,
      "training_loss": 6.22450065612793
    },
    {
      "epoch": 0.22612466124661246,
      "step": 1043,
      "training_loss": 3.644096612930298
    },
    {
      "epoch": 0.22612466124661246,
      "step": 1043,
      "training_loss": 6.656096935272217
    },
    {
      "epoch": 0.22612466124661246,
      "step": 1043,
      "training_loss": 7.119507312774658
    },
    {
      "epoch": 0.22612466124661246,
      "step": 1043,
      "training_loss": 7.007090091705322
    },
    {
      "epoch": 0.22634146341463415,
      "grad_norm": 13.889800071716309,
      "learning_rate": 1e-05,
      "loss": 6.8129,
      "step": 1044
    },
    {
      "epoch": 0.22634146341463415,
      "step": 1044,
      "training_loss": 7.075532913208008
    },
    {
      "epoch": 0.22634146341463415,
      "step": 1044,
      "training_loss": 5.171424388885498
    },
    {
      "epoch": 0.22634146341463415,
      "step": 1044,
      "training_loss": 5.855630874633789
    },
    {
      "epoch": 0.22634146341463415,
      "step": 1044,
      "training_loss": 6.9970855712890625
    },
    {
      "epoch": 0.22655826558265582,
      "step": 1045,
      "training_loss": 7.583987236022949
    },
    {
      "epoch": 0.22655826558265582,
      "step": 1045,
      "training_loss": 7.184595584869385
    },
    {
      "epoch": 0.22655826558265582,
      "step": 1045,
      "training_loss": 4.901026725769043
    },
    {
      "epoch": 0.22655826558265582,
      "step": 1045,
      "training_loss": 9.438127517700195
    },
    {
      "epoch": 0.22677506775067752,
      "step": 1046,
      "training_loss": 6.462440013885498
    },
    {
      "epoch": 0.22677506775067752,
      "step": 1046,
      "training_loss": 6.949387073516846
    },
    {
      "epoch": 0.22677506775067752,
      "step": 1046,
      "training_loss": 6.058046340942383
    },
    {
      "epoch": 0.22677506775067752,
      "step": 1046,
      "training_loss": 5.616639137268066
    },
    {
      "epoch": 0.22699186991869919,
      "step": 1047,
      "training_loss": 5.860885143280029
    },
    {
      "epoch": 0.22699186991869919,
      "step": 1047,
      "training_loss": 6.784962177276611
    },
    {
      "epoch": 0.22699186991869919,
      "step": 1047,
      "training_loss": 6.003159999847412
    },
    {
      "epoch": 0.22699186991869919,
      "step": 1047,
      "training_loss": 5.765993595123291
    },
    {
      "epoch": 0.22720867208672088,
      "grad_norm": 14.101436614990234,
      "learning_rate": 1e-05,
      "loss": 6.4818,
      "step": 1048
    },
    {
      "epoch": 0.22720867208672088,
      "step": 1048,
      "training_loss": 5.631011962890625
    },
    {
      "epoch": 0.22720867208672088,
      "step": 1048,
      "training_loss": 7.5760273933410645
    },
    {
      "epoch": 0.22720867208672088,
      "step": 1048,
      "training_loss": 6.82792854309082
    },
    {
      "epoch": 0.22720867208672088,
      "step": 1048,
      "training_loss": 7.728386402130127
    },
    {
      "epoch": 0.22742547425474255,
      "step": 1049,
      "training_loss": 5.986099720001221
    },
    {
      "epoch": 0.22742547425474255,
      "step": 1049,
      "training_loss": 6.55707311630249
    },
    {
      "epoch": 0.22742547425474255,
      "step": 1049,
      "training_loss": 7.245800018310547
    },
    {
      "epoch": 0.22742547425474255,
      "step": 1049,
      "training_loss": 7.082322597503662
    },
    {
      "epoch": 0.22764227642276422,
      "step": 1050,
      "training_loss": 7.245607852935791
    },
    {
      "epoch": 0.22764227642276422,
      "step": 1050,
      "training_loss": 6.825151443481445
    },
    {
      "epoch": 0.22764227642276422,
      "step": 1050,
      "training_loss": 6.922244071960449
    },
    {
      "epoch": 0.22764227642276422,
      "step": 1050,
      "training_loss": 6.530670166015625
    },
    {
      "epoch": 0.2278590785907859,
      "step": 1051,
      "training_loss": 5.490631580352783
    },
    {
      "epoch": 0.2278590785907859,
      "step": 1051,
      "training_loss": 5.302844047546387
    },
    {
      "epoch": 0.2278590785907859,
      "step": 1051,
      "training_loss": 7.467681884765625
    },
    {
      "epoch": 0.2278590785907859,
      "step": 1051,
      "training_loss": 6.033429145812988
    },
    {
      "epoch": 0.22807588075880758,
      "grad_norm": 8.992950439453125,
      "learning_rate": 1e-05,
      "loss": 6.6533,
      "step": 1052
    },
    {
      "epoch": 0.22807588075880758,
      "step": 1052,
      "training_loss": 6.733585357666016
    },
    {
      "epoch": 0.22807588075880758,
      "step": 1052,
      "training_loss": 6.631549835205078
    },
    {
      "epoch": 0.22807588075880758,
      "step": 1052,
      "training_loss": 5.7733154296875
    },
    {
      "epoch": 0.22807588075880758,
      "step": 1052,
      "training_loss": 7.249697208404541
    },
    {
      "epoch": 0.22829268292682928,
      "step": 1053,
      "training_loss": 6.741957187652588
    },
    {
      "epoch": 0.22829268292682928,
      "step": 1053,
      "training_loss": 7.920290946960449
    },
    {
      "epoch": 0.22829268292682928,
      "step": 1053,
      "training_loss": 6.654849052429199
    },
    {
      "epoch": 0.22829268292682928,
      "step": 1053,
      "training_loss": 7.1262311935424805
    },
    {
      "epoch": 0.22850948509485094,
      "step": 1054,
      "training_loss": 8.083357810974121
    },
    {
      "epoch": 0.22850948509485094,
      "step": 1054,
      "training_loss": 7.4241623878479
    },
    {
      "epoch": 0.22850948509485094,
      "step": 1054,
      "training_loss": 6.618217945098877
    },
    {
      "epoch": 0.22850948509485094,
      "step": 1054,
      "training_loss": 6.912909030914307
    },
    {
      "epoch": 0.22872628726287264,
      "step": 1055,
      "training_loss": 6.864279270172119
    },
    {
      "epoch": 0.22872628726287264,
      "step": 1055,
      "training_loss": 7.680325984954834
    },
    {
      "epoch": 0.22872628726287264,
      "step": 1055,
      "training_loss": 7.086394309997559
    },
    {
      "epoch": 0.22872628726287264,
      "step": 1055,
      "training_loss": 6.868878364562988
    },
    {
      "epoch": 0.2289430894308943,
      "grad_norm": 11.18864631652832,
      "learning_rate": 1e-05,
      "loss": 7.0231,
      "step": 1056
    },
    {
      "epoch": 0.2289430894308943,
      "step": 1056,
      "training_loss": 6.243940353393555
    },
    {
      "epoch": 0.2289430894308943,
      "step": 1056,
      "training_loss": 7.615036487579346
    },
    {
      "epoch": 0.2289430894308943,
      "step": 1056,
      "training_loss": 6.350350379943848
    },
    {
      "epoch": 0.2289430894308943,
      "step": 1056,
      "training_loss": 7.022852897644043
    },
    {
      "epoch": 0.22915989159891598,
      "step": 1057,
      "training_loss": 7.793158531188965
    },
    {
      "epoch": 0.22915989159891598,
      "step": 1057,
      "training_loss": 7.1466875076293945
    },
    {
      "epoch": 0.22915989159891598,
      "step": 1057,
      "training_loss": 6.227070331573486
    },
    {
      "epoch": 0.22915989159891598,
      "step": 1057,
      "training_loss": 7.640254974365234
    },
    {
      "epoch": 0.22937669376693767,
      "step": 1058,
      "training_loss": 5.787395000457764
    },
    {
      "epoch": 0.22937669376693767,
      "step": 1058,
      "training_loss": 5.664153099060059
    },
    {
      "epoch": 0.22937669376693767,
      "step": 1058,
      "training_loss": 6.6917548179626465
    },
    {
      "epoch": 0.22937669376693767,
      "step": 1058,
      "training_loss": 7.588839054107666
    },
    {
      "epoch": 0.22959349593495934,
      "step": 1059,
      "training_loss": 6.4175872802734375
    },
    {
      "epoch": 0.22959349593495934,
      "step": 1059,
      "training_loss": 5.636266708374023
    },
    {
      "epoch": 0.22959349593495934,
      "step": 1059,
      "training_loss": 8.434836387634277
    },
    {
      "epoch": 0.22959349593495934,
      "step": 1059,
      "training_loss": 5.364205360412598
    },
    {
      "epoch": 0.22981029810298104,
      "grad_norm": 15.038848876953125,
      "learning_rate": 1e-05,
      "loss": 6.7265,
      "step": 1060
    },
    {
      "epoch": 0.22981029810298104,
      "step": 1060,
      "training_loss": 6.4395751953125
    },
    {
      "epoch": 0.22981029810298104,
      "step": 1060,
      "training_loss": 7.500603199005127
    },
    {
      "epoch": 0.22981029810298104,
      "step": 1060,
      "training_loss": 6.393589496612549
    },
    {
      "epoch": 0.22981029810298104,
      "step": 1060,
      "training_loss": 7.106594085693359
    },
    {
      "epoch": 0.2300271002710027,
      "step": 1061,
      "training_loss": 7.135009288787842
    },
    {
      "epoch": 0.2300271002710027,
      "step": 1061,
      "training_loss": 7.443182945251465
    },
    {
      "epoch": 0.2300271002710027,
      "step": 1061,
      "training_loss": 5.828924655914307
    },
    {
      "epoch": 0.2300271002710027,
      "step": 1061,
      "training_loss": 8.046316146850586
    },
    {
      "epoch": 0.2302439024390244,
      "step": 1062,
      "training_loss": 5.668956756591797
    },
    {
      "epoch": 0.2302439024390244,
      "step": 1062,
      "training_loss": 6.3680338859558105
    },
    {
      "epoch": 0.2302439024390244,
      "step": 1062,
      "training_loss": 6.8733038902282715
    },
    {
      "epoch": 0.2302439024390244,
      "step": 1062,
      "training_loss": 7.081657409667969
    },
    {
      "epoch": 0.23046070460704607,
      "step": 1063,
      "training_loss": 7.246517181396484
    },
    {
      "epoch": 0.23046070460704607,
      "step": 1063,
      "training_loss": 7.374749660491943
    },
    {
      "epoch": 0.23046070460704607,
      "step": 1063,
      "training_loss": 6.1526336669921875
    },
    {
      "epoch": 0.23046070460704607,
      "step": 1063,
      "training_loss": 7.928071975708008
    },
    {
      "epoch": 0.23067750677506776,
      "grad_norm": 8.911870956420898,
      "learning_rate": 1e-05,
      "loss": 6.9117,
      "step": 1064
    },
    {
      "epoch": 0.23067750677506776,
      "step": 1064,
      "training_loss": 6.93082332611084
    },
    {
      "epoch": 0.23067750677506776,
      "step": 1064,
      "training_loss": 6.866483211517334
    },
    {
      "epoch": 0.23067750677506776,
      "step": 1064,
      "training_loss": 7.373427391052246
    },
    {
      "epoch": 0.23067750677506776,
      "step": 1064,
      "training_loss": 6.272457122802734
    },
    {
      "epoch": 0.23089430894308943,
      "step": 1065,
      "training_loss": 7.172759056091309
    },
    {
      "epoch": 0.23089430894308943,
      "step": 1065,
      "training_loss": 9.21273422241211
    },
    {
      "epoch": 0.23089430894308943,
      "step": 1065,
      "training_loss": 6.949163436889648
    },
    {
      "epoch": 0.23089430894308943,
      "step": 1065,
      "training_loss": 7.808767318725586
    },
    {
      "epoch": 0.2311111111111111,
      "step": 1066,
      "training_loss": 6.548428058624268
    },
    {
      "epoch": 0.2311111111111111,
      "step": 1066,
      "training_loss": 6.589507579803467
    },
    {
      "epoch": 0.2311111111111111,
      "step": 1066,
      "training_loss": 4.512495994567871
    },
    {
      "epoch": 0.2311111111111111,
      "step": 1066,
      "training_loss": 6.690192699432373
    },
    {
      "epoch": 0.2313279132791328,
      "step": 1067,
      "training_loss": 4.883358955383301
    },
    {
      "epoch": 0.2313279132791328,
      "step": 1067,
      "training_loss": 6.714084625244141
    },
    {
      "epoch": 0.2313279132791328,
      "step": 1067,
      "training_loss": 6.7182698249816895
    },
    {
      "epoch": 0.2313279132791328,
      "step": 1067,
      "training_loss": 7.15146017074585
    },
    {
      "epoch": 0.23154471544715446,
      "grad_norm": 13.151580810546875,
      "learning_rate": 1e-05,
      "loss": 6.7747,
      "step": 1068
    },
    {
      "epoch": 0.23154471544715446,
      "step": 1068,
      "training_loss": 7.624346733093262
    },
    {
      "epoch": 0.23154471544715446,
      "step": 1068,
      "training_loss": 6.373107433319092
    },
    {
      "epoch": 0.23154471544715446,
      "step": 1068,
      "training_loss": 8.196789741516113
    },
    {
      "epoch": 0.23154471544715446,
      "step": 1068,
      "training_loss": 6.381317615509033
    },
    {
      "epoch": 0.23176151761517616,
      "step": 1069,
      "training_loss": 7.235537052154541
    },
    {
      "epoch": 0.23176151761517616,
      "step": 1069,
      "training_loss": 6.45232629776001
    },
    {
      "epoch": 0.23176151761517616,
      "step": 1069,
      "training_loss": 5.529358386993408
    },
    {
      "epoch": 0.23176151761517616,
      "step": 1069,
      "training_loss": 5.7105793952941895
    },
    {
      "epoch": 0.23197831978319783,
      "step": 1070,
      "training_loss": 6.027144432067871
    },
    {
      "epoch": 0.23197831978319783,
      "step": 1070,
      "training_loss": 6.605613708496094
    },
    {
      "epoch": 0.23197831978319783,
      "step": 1070,
      "training_loss": 3.2536590099334717
    },
    {
      "epoch": 0.23197831978319783,
      "step": 1070,
      "training_loss": 8.338205337524414
    },
    {
      "epoch": 0.23219512195121952,
      "step": 1071,
      "training_loss": 6.891297817230225
    },
    {
      "epoch": 0.23219512195121952,
      "step": 1071,
      "training_loss": 7.591212272644043
    },
    {
      "epoch": 0.23219512195121952,
      "step": 1071,
      "training_loss": 7.490809440612793
    },
    {
      "epoch": 0.23219512195121952,
      "step": 1071,
      "training_loss": 6.924367427825928
    },
    {
      "epoch": 0.2324119241192412,
      "grad_norm": 13.446345329284668,
      "learning_rate": 1e-05,
      "loss": 6.6641,
      "step": 1072
    },
    {
      "epoch": 0.2324119241192412,
      "step": 1072,
      "training_loss": 5.116069316864014
    },
    {
      "epoch": 0.2324119241192412,
      "step": 1072,
      "training_loss": 7.639873027801514
    },
    {
      "epoch": 0.2324119241192412,
      "step": 1072,
      "training_loss": 7.146246910095215
    },
    {
      "epoch": 0.2324119241192412,
      "step": 1072,
      "training_loss": 6.4087371826171875
    },
    {
      "epoch": 0.23262872628726286,
      "step": 1073,
      "training_loss": 5.816607475280762
    },
    {
      "epoch": 0.23262872628726286,
      "step": 1073,
      "training_loss": 7.301397323608398
    },
    {
      "epoch": 0.23262872628726286,
      "step": 1073,
      "training_loss": 6.9758477210998535
    },
    {
      "epoch": 0.23262872628726286,
      "step": 1073,
      "training_loss": 6.122770309448242
    },
    {
      "epoch": 0.23284552845528456,
      "step": 1074,
      "training_loss": 8.225970268249512
    },
    {
      "epoch": 0.23284552845528456,
      "step": 1074,
      "training_loss": 7.103411674499512
    },
    {
      "epoch": 0.23284552845528456,
      "step": 1074,
      "training_loss": 3.813974618911743
    },
    {
      "epoch": 0.23284552845528456,
      "step": 1074,
      "training_loss": 7.029326438903809
    },
    {
      "epoch": 0.23306233062330622,
      "step": 1075,
      "training_loss": 7.0813822746276855
    },
    {
      "epoch": 0.23306233062330622,
      "step": 1075,
      "training_loss": 6.161448955535889
    },
    {
      "epoch": 0.23306233062330622,
      "step": 1075,
      "training_loss": 6.828871726989746
    },
    {
      "epoch": 0.23306233062330622,
      "step": 1075,
      "training_loss": 5.613533020019531
    },
    {
      "epoch": 0.23327913279132792,
      "grad_norm": 13.814159393310547,
      "learning_rate": 1e-05,
      "loss": 6.5241,
      "step": 1076
    },
    {
      "epoch": 0.23327913279132792,
      "step": 1076,
      "training_loss": 7.261520862579346
    },
    {
      "epoch": 0.23327913279132792,
      "step": 1076,
      "training_loss": 6.800203323364258
    },
    {
      "epoch": 0.23327913279132792,
      "step": 1076,
      "training_loss": 7.433238506317139
    },
    {
      "epoch": 0.23327913279132792,
      "step": 1076,
      "training_loss": 7.25602912902832
    },
    {
      "epoch": 0.2334959349593496,
      "step": 1077,
      "training_loss": 7.270481109619141
    },
    {
      "epoch": 0.2334959349593496,
      "step": 1077,
      "training_loss": 7.606809139251709
    },
    {
      "epoch": 0.2334959349593496,
      "step": 1077,
      "training_loss": 7.296766757965088
    },
    {
      "epoch": 0.2334959349593496,
      "step": 1077,
      "training_loss": 8.032093048095703
    },
    {
      "epoch": 0.23371273712737128,
      "step": 1078,
      "training_loss": 5.084588050842285
    },
    {
      "epoch": 0.23371273712737128,
      "step": 1078,
      "training_loss": 6.500659465789795
    },
    {
      "epoch": 0.23371273712737128,
      "step": 1078,
      "training_loss": 6.896267414093018
    },
    {
      "epoch": 0.23371273712737128,
      "step": 1078,
      "training_loss": 6.933813095092773
    },
    {
      "epoch": 0.23392953929539295,
      "step": 1079,
      "training_loss": 7.1632232666015625
    },
    {
      "epoch": 0.23392953929539295,
      "step": 1079,
      "training_loss": 5.806965351104736
    },
    {
      "epoch": 0.23392953929539295,
      "step": 1079,
      "training_loss": 5.646686553955078
    },
    {
      "epoch": 0.23392953929539295,
      "step": 1079,
      "training_loss": 6.422418594360352
    },
    {
      "epoch": 0.23414634146341465,
      "grad_norm": 10.846923828125,
      "learning_rate": 1e-05,
      "loss": 6.8382,
      "step": 1080
    },
    {
      "epoch": 0.23414634146341465,
      "step": 1080,
      "training_loss": 6.229769229888916
    },
    {
      "epoch": 0.23414634146341465,
      "step": 1080,
      "training_loss": 6.181774616241455
    },
    {
      "epoch": 0.23414634146341465,
      "step": 1080,
      "training_loss": 7.228447914123535
    },
    {
      "epoch": 0.23414634146341465,
      "step": 1080,
      "training_loss": 6.964474678039551
    },
    {
      "epoch": 0.23436314363143632,
      "step": 1081,
      "training_loss": 6.401610851287842
    },
    {
      "epoch": 0.23436314363143632,
      "step": 1081,
      "training_loss": 7.3080878257751465
    },
    {
      "epoch": 0.23436314363143632,
      "step": 1081,
      "training_loss": 5.679243564605713
    },
    {
      "epoch": 0.23436314363143632,
      "step": 1081,
      "training_loss": 7.1819682121276855
    },
    {
      "epoch": 0.23457994579945798,
      "step": 1082,
      "training_loss": 6.6538472175598145
    },
    {
      "epoch": 0.23457994579945798,
      "step": 1082,
      "training_loss": 7.234033107757568
    },
    {
      "epoch": 0.23457994579945798,
      "step": 1082,
      "training_loss": 7.9976091384887695
    },
    {
      "epoch": 0.23457994579945798,
      "step": 1082,
      "training_loss": 6.593204021453857
    },
    {
      "epoch": 0.23479674796747968,
      "step": 1083,
      "training_loss": 6.7143096923828125
    },
    {
      "epoch": 0.23479674796747968,
      "step": 1083,
      "training_loss": 6.72665548324585
    },
    {
      "epoch": 0.23479674796747968,
      "step": 1083,
      "training_loss": 7.8307061195373535
    },
    {
      "epoch": 0.23479674796747968,
      "step": 1083,
      "training_loss": 6.547508716583252
    },
    {
      "epoch": 0.23501355013550135,
      "grad_norm": 8.547525405883789,
      "learning_rate": 1e-05,
      "loss": 6.8421,
      "step": 1084
    },
    {
      "epoch": 0.23501355013550135,
      "step": 1084,
      "training_loss": 6.029293060302734
    },
    {
      "epoch": 0.23501355013550135,
      "step": 1084,
      "training_loss": 7.0691819190979
    },
    {
      "epoch": 0.23501355013550135,
      "step": 1084,
      "training_loss": 6.738419532775879
    },
    {
      "epoch": 0.23501355013550135,
      "step": 1084,
      "training_loss": 3.955206871032715
    },
    {
      "epoch": 0.23523035230352304,
      "step": 1085,
      "training_loss": 7.784605979919434
    },
    {
      "epoch": 0.23523035230352304,
      "step": 1085,
      "training_loss": 7.748814582824707
    },
    {
      "epoch": 0.23523035230352304,
      "step": 1085,
      "training_loss": 6.57808256149292
    },
    {
      "epoch": 0.23523035230352304,
      "step": 1085,
      "training_loss": 4.439982891082764
    },
    {
      "epoch": 0.2354471544715447,
      "step": 1086,
      "training_loss": 7.983285427093506
    },
    {
      "epoch": 0.2354471544715447,
      "step": 1086,
      "training_loss": 6.406927585601807
    },
    {
      "epoch": 0.2354471544715447,
      "step": 1086,
      "training_loss": 5.954535484313965
    },
    {
      "epoch": 0.2354471544715447,
      "step": 1086,
      "training_loss": 7.420893669128418
    },
    {
      "epoch": 0.2356639566395664,
      "step": 1087,
      "training_loss": 7.192331790924072
    },
    {
      "epoch": 0.2356639566395664,
      "step": 1087,
      "training_loss": 6.077193260192871
    },
    {
      "epoch": 0.2356639566395664,
      "step": 1087,
      "training_loss": 7.576261043548584
    },
    {
      "epoch": 0.2356639566395664,
      "step": 1087,
      "training_loss": 5.176839351654053
    },
    {
      "epoch": 0.23588075880758808,
      "grad_norm": 18.350086212158203,
      "learning_rate": 1e-05,
      "loss": 6.5082,
      "step": 1088
    },
    {
      "epoch": 0.23588075880758808,
      "step": 1088,
      "training_loss": 7.863913059234619
    },
    {
      "epoch": 0.23588075880758808,
      "step": 1088,
      "training_loss": 7.450620651245117
    },
    {
      "epoch": 0.23588075880758808,
      "step": 1088,
      "training_loss": 7.637813568115234
    },
    {
      "epoch": 0.23588075880758808,
      "step": 1088,
      "training_loss": 5.704506874084473
    },
    {
      "epoch": 0.23609756097560974,
      "step": 1089,
      "training_loss": 6.248264789581299
    },
    {
      "epoch": 0.23609756097560974,
      "step": 1089,
      "training_loss": 6.344691276550293
    },
    {
      "epoch": 0.23609756097560974,
      "step": 1089,
      "training_loss": 8.041847229003906
    },
    {
      "epoch": 0.23609756097560974,
      "step": 1089,
      "training_loss": 6.934622287750244
    },
    {
      "epoch": 0.23631436314363144,
      "step": 1090,
      "training_loss": 6.014935493469238
    },
    {
      "epoch": 0.23631436314363144,
      "step": 1090,
      "training_loss": 5.454517364501953
    },
    {
      "epoch": 0.23631436314363144,
      "step": 1090,
      "training_loss": 7.674498081207275
    },
    {
      "epoch": 0.23631436314363144,
      "step": 1090,
      "training_loss": 5.882176399230957
    },
    {
      "epoch": 0.2365311653116531,
      "step": 1091,
      "training_loss": 6.57256555557251
    },
    {
      "epoch": 0.2365311653116531,
      "step": 1091,
      "training_loss": 6.0220046043396
    },
    {
      "epoch": 0.2365311653116531,
      "step": 1091,
      "training_loss": 6.513139724731445
    },
    {
      "epoch": 0.2365311653116531,
      "step": 1091,
      "training_loss": 6.754984378814697
    },
    {
      "epoch": 0.2367479674796748,
      "grad_norm": 11.96371078491211,
      "learning_rate": 1e-05,
      "loss": 6.6947,
      "step": 1092
    },
    {
      "epoch": 0.2367479674796748,
      "step": 1092,
      "training_loss": 7.733052730560303
    },
    {
      "epoch": 0.2367479674796748,
      "step": 1092,
      "training_loss": 7.645979404449463
    },
    {
      "epoch": 0.2367479674796748,
      "step": 1092,
      "training_loss": 6.594357013702393
    },
    {
      "epoch": 0.2367479674796748,
      "step": 1092,
      "training_loss": 6.602394104003906
    },
    {
      "epoch": 0.23696476964769647,
      "step": 1093,
      "training_loss": 7.044422626495361
    },
    {
      "epoch": 0.23696476964769647,
      "step": 1093,
      "training_loss": 5.556058883666992
    },
    {
      "epoch": 0.23696476964769647,
      "step": 1093,
      "training_loss": 7.43888521194458
    },
    {
      "epoch": 0.23696476964769647,
      "step": 1093,
      "training_loss": 5.510531425476074
    },
    {
      "epoch": 0.23718157181571817,
      "step": 1094,
      "training_loss": 6.054412841796875
    },
    {
      "epoch": 0.23718157181571817,
      "step": 1094,
      "training_loss": 6.400736331939697
    },
    {
      "epoch": 0.23718157181571817,
      "step": 1094,
      "training_loss": 6.721119403839111
    },
    {
      "epoch": 0.23718157181571817,
      "step": 1094,
      "training_loss": 5.7966790199279785
    },
    {
      "epoch": 0.23739837398373984,
      "step": 1095,
      "training_loss": 5.851257801055908
    },
    {
      "epoch": 0.23739837398373984,
      "step": 1095,
      "training_loss": 5.822197437286377
    },
    {
      "epoch": 0.23739837398373984,
      "step": 1095,
      "training_loss": 8.570965766906738
    },
    {
      "epoch": 0.23739837398373984,
      "step": 1095,
      "training_loss": 4.748308181762695
    },
    {
      "epoch": 0.23761517615176153,
      "grad_norm": 14.464205741882324,
      "learning_rate": 1e-05,
      "loss": 6.5057,
      "step": 1096
    },
    {
      "epoch": 0.23761517615176153,
      "step": 1096,
      "training_loss": 6.576345443725586
    },
    {
      "epoch": 0.23761517615176153,
      "step": 1096,
      "training_loss": 7.367132186889648
    },
    {
      "epoch": 0.23761517615176153,
      "step": 1096,
      "training_loss": 7.136712551116943
    },
    {
      "epoch": 0.23761517615176153,
      "step": 1096,
      "training_loss": 7.1565985679626465
    },
    {
      "epoch": 0.2378319783197832,
      "step": 1097,
      "training_loss": 6.29902458190918
    },
    {
      "epoch": 0.2378319783197832,
      "step": 1097,
      "training_loss": 7.445870876312256
    },
    {
      "epoch": 0.2378319783197832,
      "step": 1097,
      "training_loss": 7.656853199005127
    },
    {
      "epoch": 0.2378319783197832,
      "step": 1097,
      "training_loss": 6.493443965911865
    },
    {
      "epoch": 0.23804878048780487,
      "step": 1098,
      "training_loss": 7.18951416015625
    },
    {
      "epoch": 0.23804878048780487,
      "step": 1098,
      "training_loss": 5.923993110656738
    },
    {
      "epoch": 0.23804878048780487,
      "step": 1098,
      "training_loss": 7.770020961761475
    },
    {
      "epoch": 0.23804878048780487,
      "step": 1098,
      "training_loss": 7.399391174316406
    },
    {
      "epoch": 0.23826558265582656,
      "step": 1099,
      "training_loss": 6.866240978240967
    },
    {
      "epoch": 0.23826558265582656,
      "step": 1099,
      "training_loss": 6.717777729034424
    },
    {
      "epoch": 0.23826558265582656,
      "step": 1099,
      "training_loss": 7.285275459289551
    },
    {
      "epoch": 0.23826558265582656,
      "step": 1099,
      "training_loss": 6.460835933685303
    },
    {
      "epoch": 0.23848238482384823,
      "grad_norm": 11.864595413208008,
      "learning_rate": 1e-05,
      "loss": 6.9841,
      "step": 1100
    },
    {
      "epoch": 0.23848238482384823,
      "step": 1100,
      "training_loss": 7.161358833312988
    },
    {
      "epoch": 0.23848238482384823,
      "step": 1100,
      "training_loss": 7.30934476852417
    },
    {
      "epoch": 0.23848238482384823,
      "step": 1100,
      "training_loss": 7.332605361938477
    },
    {
      "epoch": 0.23848238482384823,
      "step": 1100,
      "training_loss": 6.278385162353516
    },
    {
      "epoch": 0.23869918699186993,
      "step": 1101,
      "training_loss": 7.358206748962402
    },
    {
      "epoch": 0.23869918699186993,
      "step": 1101,
      "training_loss": 8.091056823730469
    },
    {
      "epoch": 0.23869918699186993,
      "step": 1101,
      "training_loss": 5.900748252868652
    },
    {
      "epoch": 0.23869918699186993,
      "step": 1101,
      "training_loss": 6.673649787902832
    },
    {
      "epoch": 0.2389159891598916,
      "step": 1102,
      "training_loss": 6.601121425628662
    },
    {
      "epoch": 0.2389159891598916,
      "step": 1102,
      "training_loss": 6.943404197692871
    },
    {
      "epoch": 0.2389159891598916,
      "step": 1102,
      "training_loss": 5.926120758056641
    },
    {
      "epoch": 0.2389159891598916,
      "step": 1102,
      "training_loss": 5.459903717041016
    },
    {
      "epoch": 0.2391327913279133,
      "step": 1103,
      "training_loss": 6.672754764556885
    },
    {
      "epoch": 0.2391327913279133,
      "step": 1103,
      "training_loss": 6.855106830596924
    },
    {
      "epoch": 0.2391327913279133,
      "step": 1103,
      "training_loss": 6.787961483001709
    },
    {
      "epoch": 0.2391327913279133,
      "step": 1103,
      "training_loss": 7.324593544006348
    },
    {
      "epoch": 0.23934959349593496,
      "grad_norm": 11.978684425354004,
      "learning_rate": 1e-05,
      "loss": 6.7923,
      "step": 1104
    },
    {
      "epoch": 0.23934959349593496,
      "step": 1104,
      "training_loss": 6.739798069000244
    },
    {
      "epoch": 0.23934959349593496,
      "step": 1104,
      "training_loss": 6.767233371734619
    },
    {
      "epoch": 0.23934959349593496,
      "step": 1104,
      "training_loss": 7.826473712921143
    },
    {
      "epoch": 0.23934959349593496,
      "step": 1104,
      "training_loss": 7.587876319885254
    },
    {
      "epoch": 0.23956639566395663,
      "step": 1105,
      "training_loss": 8.443629264831543
    },
    {
      "epoch": 0.23956639566395663,
      "step": 1105,
      "training_loss": 6.806957721710205
    },
    {
      "epoch": 0.23956639566395663,
      "step": 1105,
      "training_loss": 4.6849045753479
    },
    {
      "epoch": 0.23956639566395663,
      "step": 1105,
      "training_loss": 6.767367362976074
    },
    {
      "epoch": 0.23978319783197832,
      "step": 1106,
      "training_loss": 7.1670613288879395
    },
    {
      "epoch": 0.23978319783197832,
      "step": 1106,
      "training_loss": 6.610885143280029
    },
    {
      "epoch": 0.23978319783197832,
      "step": 1106,
      "training_loss": 7.02003288269043
    },
    {
      "epoch": 0.23978319783197832,
      "step": 1106,
      "training_loss": 6.583588123321533
    },
    {
      "epoch": 0.24,
      "step": 1107,
      "training_loss": 6.090479850769043
    },
    {
      "epoch": 0.24,
      "step": 1107,
      "training_loss": 6.666590690612793
    },
    {
      "epoch": 0.24,
      "step": 1107,
      "training_loss": 6.950150966644287
    },
    {
      "epoch": 0.24,
      "step": 1107,
      "training_loss": 5.166680335998535
    },
    {
      "epoch": 0.2402168021680217,
      "grad_norm": 14.023415565490723,
      "learning_rate": 1e-05,
      "loss": 6.7425,
      "step": 1108
    },
    {
      "epoch": 0.2402168021680217,
      "step": 1108,
      "training_loss": 6.367795944213867
    },
    {
      "epoch": 0.2402168021680217,
      "step": 1108,
      "training_loss": 8.039997100830078
    },
    {
      "epoch": 0.2402168021680217,
      "step": 1108,
      "training_loss": 6.383030891418457
    },
    {
      "epoch": 0.2402168021680217,
      "step": 1108,
      "training_loss": 6.881935119628906
    },
    {
      "epoch": 0.24043360433604336,
      "step": 1109,
      "training_loss": 6.219118118286133
    },
    {
      "epoch": 0.24043360433604336,
      "step": 1109,
      "training_loss": 6.989243507385254
    },
    {
      "epoch": 0.24043360433604336,
      "step": 1109,
      "training_loss": 6.938195705413818
    },
    {
      "epoch": 0.24043360433604336,
      "step": 1109,
      "training_loss": 6.041487216949463
    },
    {
      "epoch": 0.24065040650406505,
      "step": 1110,
      "training_loss": 6.767630577087402
    },
    {
      "epoch": 0.24065040650406505,
      "step": 1110,
      "training_loss": 7.262453556060791
    },
    {
      "epoch": 0.24065040650406505,
      "step": 1110,
      "training_loss": 7.607872486114502
    },
    {
      "epoch": 0.24065040650406505,
      "step": 1110,
      "training_loss": 7.497188091278076
    },
    {
      "epoch": 0.24086720867208672,
      "step": 1111,
      "training_loss": 5.540069580078125
    },
    {
      "epoch": 0.24086720867208672,
      "step": 1111,
      "training_loss": 8.157478332519531
    },
    {
      "epoch": 0.24086720867208672,
      "step": 1111,
      "training_loss": 7.470256328582764
    },
    {
      "epoch": 0.24086720867208672,
      "step": 1111,
      "training_loss": 6.847378253936768
    },
    {
      "epoch": 0.24108401084010841,
      "grad_norm": 18.964859008789062,
      "learning_rate": 1e-05,
      "loss": 6.9382,
      "step": 1112
    },
    {
      "epoch": 0.24108401084010841,
      "step": 1112,
      "training_loss": 8.235461235046387
    },
    {
      "epoch": 0.24108401084010841,
      "step": 1112,
      "training_loss": 6.403416633605957
    },
    {
      "epoch": 0.24108401084010841,
      "step": 1112,
      "training_loss": 6.7979536056518555
    },
    {
      "epoch": 0.24108401084010841,
      "step": 1112,
      "training_loss": 6.266350746154785
    },
    {
      "epoch": 0.24130081300813008,
      "step": 1113,
      "training_loss": 7.423403739929199
    },
    {
      "epoch": 0.24130081300813008,
      "step": 1113,
      "training_loss": 5.854977607727051
    },
    {
      "epoch": 0.24130081300813008,
      "step": 1113,
      "training_loss": 6.291191577911377
    },
    {
      "epoch": 0.24130081300813008,
      "step": 1113,
      "training_loss": 5.6787285804748535
    },
    {
      "epoch": 0.24151761517615175,
      "step": 1114,
      "training_loss": 7.712707042694092
    },
    {
      "epoch": 0.24151761517615175,
      "step": 1114,
      "training_loss": 7.187505722045898
    },
    {
      "epoch": 0.24151761517615175,
      "step": 1114,
      "training_loss": 6.537240505218506
    },
    {
      "epoch": 0.24151761517615175,
      "step": 1114,
      "training_loss": 6.515848159790039
    },
    {
      "epoch": 0.24173441734417345,
      "step": 1115,
      "training_loss": 7.184597969055176
    },
    {
      "epoch": 0.24173441734417345,
      "step": 1115,
      "training_loss": 6.382758617401123
    },
    {
      "epoch": 0.24173441734417345,
      "step": 1115,
      "training_loss": 5.971217632293701
    },
    {
      "epoch": 0.24173441734417345,
      "step": 1115,
      "training_loss": 7.174916744232178
    },
    {
      "epoch": 0.24195121951219511,
      "grad_norm": 10.910989761352539,
      "learning_rate": 1e-05,
      "loss": 6.7261,
      "step": 1116
    },
    {
      "epoch": 0.24195121951219511,
      "step": 1116,
      "training_loss": 6.9546217918396
    },
    {
      "epoch": 0.24195121951219511,
      "step": 1116,
      "training_loss": 5.70611047744751
    },
    {
      "epoch": 0.24195121951219511,
      "step": 1116,
      "training_loss": 6.18554162979126
    },
    {
      "epoch": 0.24195121951219511,
      "step": 1116,
      "training_loss": 6.62649393081665
    },
    {
      "epoch": 0.2421680216802168,
      "step": 1117,
      "training_loss": 7.1819047927856445
    },
    {
      "epoch": 0.2421680216802168,
      "step": 1117,
      "training_loss": 6.542003631591797
    },
    {
      "epoch": 0.2421680216802168,
      "step": 1117,
      "training_loss": 5.9712300300598145
    },
    {
      "epoch": 0.2421680216802168,
      "step": 1117,
      "training_loss": 6.329830646514893
    },
    {
      "epoch": 0.24238482384823848,
      "step": 1118,
      "training_loss": 8.459558486938477
    },
    {
      "epoch": 0.24238482384823848,
      "step": 1118,
      "training_loss": 6.904819488525391
    },
    {
      "epoch": 0.24238482384823848,
      "step": 1118,
      "training_loss": 7.0729804039001465
    },
    {
      "epoch": 0.24238482384823848,
      "step": 1118,
      "training_loss": 8.298432350158691
    },
    {
      "epoch": 0.24260162601626017,
      "step": 1119,
      "training_loss": 7.473409652709961
    },
    {
      "epoch": 0.24260162601626017,
      "step": 1119,
      "training_loss": 7.563960552215576
    },
    {
      "epoch": 0.24260162601626017,
      "step": 1119,
      "training_loss": 7.637893199920654
    },
    {
      "epoch": 0.24260162601626017,
      "step": 1119,
      "training_loss": 5.848658084869385
    },
    {
      "epoch": 0.24281842818428184,
      "grad_norm": 12.311969757080078,
      "learning_rate": 1e-05,
      "loss": 6.9223,
      "step": 1120
    },
    {
      "epoch": 0.24281842818428184,
      "step": 1120,
      "training_loss": 6.274725437164307
    },
    {
      "epoch": 0.24281842818428184,
      "step": 1120,
      "training_loss": 6.973522663116455
    },
    {
      "epoch": 0.24281842818428184,
      "step": 1120,
      "training_loss": 7.446173191070557
    },
    {
      "epoch": 0.24281842818428184,
      "step": 1120,
      "training_loss": 6.304863929748535
    },
    {
      "epoch": 0.2430352303523035,
      "step": 1121,
      "training_loss": 6.211068153381348
    },
    {
      "epoch": 0.2430352303523035,
      "step": 1121,
      "training_loss": 6.478945255279541
    },
    {
      "epoch": 0.2430352303523035,
      "step": 1121,
      "training_loss": 6.672622203826904
    },
    {
      "epoch": 0.2430352303523035,
      "step": 1121,
      "training_loss": 5.311831474304199
    },
    {
      "epoch": 0.2432520325203252,
      "step": 1122,
      "training_loss": 5.750637054443359
    },
    {
      "epoch": 0.2432520325203252,
      "step": 1122,
      "training_loss": 6.910355567932129
    },
    {
      "epoch": 0.2432520325203252,
      "step": 1122,
      "training_loss": 6.6453166007995605
    },
    {
      "epoch": 0.2432520325203252,
      "step": 1122,
      "training_loss": 4.989336967468262
    },
    {
      "epoch": 0.24346883468834687,
      "step": 1123,
      "training_loss": 6.769257068634033
    },
    {
      "epoch": 0.24346883468834687,
      "step": 1123,
      "training_loss": 6.271596431732178
    },
    {
      "epoch": 0.24346883468834687,
      "step": 1123,
      "training_loss": 7.641395568847656
    },
    {
      "epoch": 0.24346883468834687,
      "step": 1123,
      "training_loss": 5.111581802368164
    },
    {
      "epoch": 0.24368563685636857,
      "grad_norm": 11.26518440246582,
      "learning_rate": 1e-05,
      "loss": 6.3602,
      "step": 1124
    },
    {
      "epoch": 0.24368563685636857,
      "step": 1124,
      "training_loss": 7.0605082511901855
    },
    {
      "epoch": 0.24368563685636857,
      "step": 1124,
      "training_loss": 7.343125820159912
    },
    {
      "epoch": 0.24368563685636857,
      "step": 1124,
      "training_loss": 7.056169509887695
    },
    {
      "epoch": 0.24368563685636857,
      "step": 1124,
      "training_loss": 7.9607110023498535
    },
    {
      "epoch": 0.24390243902439024,
      "step": 1125,
      "training_loss": 6.609813213348389
    },
    {
      "epoch": 0.24390243902439024,
      "step": 1125,
      "training_loss": 6.8451104164123535
    },
    {
      "epoch": 0.24390243902439024,
      "step": 1125,
      "training_loss": 7.275009632110596
    },
    {
      "epoch": 0.24390243902439024,
      "step": 1125,
      "training_loss": 7.6653337478637695
    },
    {
      "epoch": 0.24411924119241193,
      "step": 1126,
      "training_loss": 6.173382759094238
    },
    {
      "epoch": 0.24411924119241193,
      "step": 1126,
      "training_loss": 7.892482280731201
    },
    {
      "epoch": 0.24411924119241193,
      "step": 1126,
      "training_loss": 6.928306579589844
    },
    {
      "epoch": 0.24411924119241193,
      "step": 1126,
      "training_loss": 7.499162197113037
    },
    {
      "epoch": 0.2443360433604336,
      "step": 1127,
      "training_loss": 7.1507887840271
    },
    {
      "epoch": 0.2443360433604336,
      "step": 1127,
      "training_loss": 6.518571853637695
    },
    {
      "epoch": 0.2443360433604336,
      "step": 1127,
      "training_loss": 6.149475574493408
    },
    {
      "epoch": 0.2443360433604336,
      "step": 1127,
      "training_loss": 6.195147514343262
    },
    {
      "epoch": 0.2445528455284553,
      "grad_norm": 13.313433647155762,
      "learning_rate": 1e-05,
      "loss": 7.0202,
      "step": 1128
    },
    {
      "epoch": 0.2445528455284553,
      "step": 1128,
      "training_loss": 7.84121561050415
    },
    {
      "epoch": 0.2445528455284553,
      "step": 1128,
      "training_loss": 5.892972469329834
    },
    {
      "epoch": 0.2445528455284553,
      "step": 1128,
      "training_loss": 7.452784061431885
    },
    {
      "epoch": 0.2445528455284553,
      "step": 1128,
      "training_loss": 7.956925392150879
    },
    {
      "epoch": 0.24476964769647697,
      "step": 1129,
      "training_loss": 7.128426551818848
    },
    {
      "epoch": 0.24476964769647697,
      "step": 1129,
      "training_loss": 5.962085723876953
    },
    {
      "epoch": 0.24476964769647697,
      "step": 1129,
      "training_loss": 5.913417816162109
    },
    {
      "epoch": 0.24476964769647697,
      "step": 1129,
      "training_loss": 6.93565559387207
    },
    {
      "epoch": 0.24498644986449863,
      "step": 1130,
      "training_loss": 7.186883926391602
    },
    {
      "epoch": 0.24498644986449863,
      "step": 1130,
      "training_loss": 6.577425479888916
    },
    {
      "epoch": 0.24498644986449863,
      "step": 1130,
      "training_loss": 7.668450355529785
    },
    {
      "epoch": 0.24498644986449863,
      "step": 1130,
      "training_loss": 5.801393508911133
    },
    {
      "epoch": 0.24520325203252033,
      "step": 1131,
      "training_loss": 6.719900608062744
    },
    {
      "epoch": 0.24520325203252033,
      "step": 1131,
      "training_loss": 6.497982978820801
    },
    {
      "epoch": 0.24520325203252033,
      "step": 1131,
      "training_loss": 5.761976718902588
    },
    {
      "epoch": 0.24520325203252033,
      "step": 1131,
      "training_loss": 8.522490501403809
    },
    {
      "epoch": 0.245420054200542,
      "grad_norm": 21.58127212524414,
      "learning_rate": 1e-05,
      "loss": 6.8637,
      "step": 1132
    },
    {
      "epoch": 0.245420054200542,
      "step": 1132,
      "training_loss": 5.566511154174805
    },
    {
      "epoch": 0.245420054200542,
      "step": 1132,
      "training_loss": 6.53839111328125
    },
    {
      "epoch": 0.245420054200542,
      "step": 1132,
      "training_loss": 5.692047119140625
    },
    {
      "epoch": 0.245420054200542,
      "step": 1132,
      "training_loss": 5.282847881317139
    },
    {
      "epoch": 0.2456368563685637,
      "step": 1133,
      "training_loss": 7.162802696228027
    },
    {
      "epoch": 0.2456368563685637,
      "step": 1133,
      "training_loss": 7.217354774475098
    },
    {
      "epoch": 0.2456368563685637,
      "step": 1133,
      "training_loss": 6.975909233093262
    },
    {
      "epoch": 0.2456368563685637,
      "step": 1133,
      "training_loss": 7.302096843719482
    },
    {
      "epoch": 0.24585365853658536,
      "step": 1134,
      "training_loss": 6.1207404136657715
    },
    {
      "epoch": 0.24585365853658536,
      "step": 1134,
      "training_loss": 7.52048397064209
    },
    {
      "epoch": 0.24585365853658536,
      "step": 1134,
      "training_loss": 7.491270542144775
    },
    {
      "epoch": 0.24585365853658536,
      "step": 1134,
      "training_loss": 7.421907901763916
    },
    {
      "epoch": 0.24607046070460706,
      "step": 1135,
      "training_loss": 6.316450595855713
    },
    {
      "epoch": 0.24607046070460706,
      "step": 1135,
      "training_loss": 7.0273356437683105
    },
    {
      "epoch": 0.24607046070460706,
      "step": 1135,
      "training_loss": 6.421547889709473
    },
    {
      "epoch": 0.24607046070460706,
      "step": 1135,
      "training_loss": 5.242700576782227
    },
    {
      "epoch": 0.24628726287262873,
      "grad_norm": 11.578035354614258,
      "learning_rate": 1e-05,
      "loss": 6.5813,
      "step": 1136
    },
    {
      "epoch": 0.24628726287262873,
      "step": 1136,
      "training_loss": 5.202764511108398
    },
    {
      "epoch": 0.24628726287262873,
      "step": 1136,
      "training_loss": 5.207369804382324
    },
    {
      "epoch": 0.24628726287262873,
      "step": 1136,
      "training_loss": 5.983776092529297
    },
    {
      "epoch": 0.24628726287262873,
      "step": 1136,
      "training_loss": 7.077033996582031
    },
    {
      "epoch": 0.2465040650406504,
      "step": 1137,
      "training_loss": 7.001257419586182
    },
    {
      "epoch": 0.2465040650406504,
      "step": 1137,
      "training_loss": 5.2486677169799805
    },
    {
      "epoch": 0.2465040650406504,
      "step": 1137,
      "training_loss": 5.7543439865112305
    },
    {
      "epoch": 0.2465040650406504,
      "step": 1137,
      "training_loss": 8.078916549682617
    },
    {
      "epoch": 0.2467208672086721,
      "step": 1138,
      "training_loss": 6.076120376586914
    },
    {
      "epoch": 0.2467208672086721,
      "step": 1138,
      "training_loss": 6.227116584777832
    },
    {
      "epoch": 0.2467208672086721,
      "step": 1138,
      "training_loss": 6.541199684143066
    },
    {
      "epoch": 0.2467208672086721,
      "step": 1138,
      "training_loss": 5.686223030090332
    },
    {
      "epoch": 0.24693766937669376,
      "step": 1139,
      "training_loss": 6.295095443725586
    },
    {
      "epoch": 0.24693766937669376,
      "step": 1139,
      "training_loss": 6.584990501403809
    },
    {
      "epoch": 0.24693766937669376,
      "step": 1139,
      "training_loss": 7.658348083496094
    },
    {
      "epoch": 0.24693766937669376,
      "step": 1139,
      "training_loss": 6.805656909942627
    },
    {
      "epoch": 0.24715447154471545,
      "grad_norm": 11.4686861038208,
      "learning_rate": 1e-05,
      "loss": 6.3393,
      "step": 1140
    },
    {
      "epoch": 0.24715447154471545,
      "step": 1140,
      "training_loss": 7.202479362487793
    },
    {
      "epoch": 0.24715447154471545,
      "step": 1140,
      "training_loss": 8.128504753112793
    },
    {
      "epoch": 0.24715447154471545,
      "step": 1140,
      "training_loss": 6.6202168464660645
    },
    {
      "epoch": 0.24715447154471545,
      "step": 1140,
      "training_loss": 5.747938632965088
    },
    {
      "epoch": 0.24737127371273712,
      "step": 1141,
      "training_loss": 7.132670879364014
    },
    {
      "epoch": 0.24737127371273712,
      "step": 1141,
      "training_loss": 7.701309680938721
    },
    {
      "epoch": 0.24737127371273712,
      "step": 1141,
      "training_loss": 6.80160665512085
    },
    {
      "epoch": 0.24737127371273712,
      "step": 1141,
      "training_loss": 7.434997081756592
    },
    {
      "epoch": 0.24758807588075882,
      "step": 1142,
      "training_loss": 8.226162910461426
    },
    {
      "epoch": 0.24758807588075882,
      "step": 1142,
      "training_loss": 6.720249176025391
    },
    {
      "epoch": 0.24758807588075882,
      "step": 1142,
      "training_loss": 6.96533203125
    },
    {
      "epoch": 0.24758807588075882,
      "step": 1142,
      "training_loss": 6.704659461975098
    },
    {
      "epoch": 0.24780487804878049,
      "step": 1143,
      "training_loss": 6.125611782073975
    },
    {
      "epoch": 0.24780487804878049,
      "step": 1143,
      "training_loss": 6.356522560119629
    },
    {
      "epoch": 0.24780487804878049,
      "step": 1143,
      "training_loss": 7.195744514465332
    },
    {
      "epoch": 0.24780487804878049,
      "step": 1143,
      "training_loss": 6.174285888671875
    },
    {
      "epoch": 0.24802168021680218,
      "grad_norm": 11.534890174865723,
      "learning_rate": 1e-05,
      "loss": 6.9524,
      "step": 1144
    },
    {
      "epoch": 0.24802168021680218,
      "step": 1144,
      "training_loss": 7.47705078125
    },
    {
      "epoch": 0.24802168021680218,
      "step": 1144,
      "training_loss": 7.005580902099609
    },
    {
      "epoch": 0.24802168021680218,
      "step": 1144,
      "training_loss": 7.220067501068115
    },
    {
      "epoch": 0.24802168021680218,
      "step": 1144,
      "training_loss": 8.269893646240234
    },
    {
      "epoch": 0.24823848238482385,
      "step": 1145,
      "training_loss": 7.229643821716309
    },
    {
      "epoch": 0.24823848238482385,
      "step": 1145,
      "training_loss": 6.627819538116455
    },
    {
      "epoch": 0.24823848238482385,
      "step": 1145,
      "training_loss": 6.940098762512207
    },
    {
      "epoch": 0.24823848238482385,
      "step": 1145,
      "training_loss": 7.644664287567139
    },
    {
      "epoch": 0.24845528455284552,
      "step": 1146,
      "training_loss": 6.553979873657227
    },
    {
      "epoch": 0.24845528455284552,
      "step": 1146,
      "training_loss": 4.658374309539795
    },
    {
      "epoch": 0.24845528455284552,
      "step": 1146,
      "training_loss": 7.181001663208008
    },
    {
      "epoch": 0.24845528455284552,
      "step": 1146,
      "training_loss": 6.2952561378479
    },
    {
      "epoch": 0.2486720867208672,
      "step": 1147,
      "training_loss": 5.835972309112549
    },
    {
      "epoch": 0.2486720867208672,
      "step": 1147,
      "training_loss": 7.61261510848999
    },
    {
      "epoch": 0.2486720867208672,
      "step": 1147,
      "training_loss": 6.065277099609375
    },
    {
      "epoch": 0.2486720867208672,
      "step": 1147,
      "training_loss": 6.173361301422119
    },
    {
      "epoch": 0.24888888888888888,
      "grad_norm": 11.462265968322754,
      "learning_rate": 1e-05,
      "loss": 6.7994,
      "step": 1148
    },
    {
      "epoch": 0.24888888888888888,
      "step": 1148,
      "training_loss": 6.46340274810791
    },
    {
      "epoch": 0.24888888888888888,
      "step": 1148,
      "training_loss": 6.545014381408691
    },
    {
      "epoch": 0.24888888888888888,
      "step": 1148,
      "training_loss": 7.208134174346924
    },
    {
      "epoch": 0.24888888888888888,
      "step": 1148,
      "training_loss": 5.128902435302734
    },
    {
      "epoch": 0.24910569105691058,
      "step": 1149,
      "training_loss": 6.717348098754883
    },
    {
      "epoch": 0.24910569105691058,
      "step": 1149,
      "training_loss": 6.563532829284668
    },
    {
      "epoch": 0.24910569105691058,
      "step": 1149,
      "training_loss": 7.098618507385254
    },
    {
      "epoch": 0.24910569105691058,
      "step": 1149,
      "training_loss": 7.516281604766846
    },
    {
      "epoch": 0.24932249322493225,
      "step": 1150,
      "training_loss": 7.197251319885254
    },
    {
      "epoch": 0.24932249322493225,
      "step": 1150,
      "training_loss": 6.488426208496094
    },
    {
      "epoch": 0.24932249322493225,
      "step": 1150,
      "training_loss": 6.8837361335754395
    },
    {
      "epoch": 0.24932249322493225,
      "step": 1150,
      "training_loss": 6.360343933105469
    },
    {
      "epoch": 0.24953929539295394,
      "step": 1151,
      "training_loss": 6.881474494934082
    },
    {
      "epoch": 0.24953929539295394,
      "step": 1151,
      "training_loss": 7.480414390563965
    },
    {
      "epoch": 0.24953929539295394,
      "step": 1151,
      "training_loss": 6.903372287750244
    },
    {
      "epoch": 0.24953929539295394,
      "step": 1151,
      "training_loss": 7.123238563537598
    },
    {
      "epoch": 0.2497560975609756,
      "grad_norm": 10.801066398620605,
      "learning_rate": 1e-05,
      "loss": 6.785,
      "step": 1152
    },
    {
      "epoch": 0.2497560975609756,
      "step": 1152,
      "training_loss": 6.980281829833984
    },
    {
      "epoch": 0.2497560975609756,
      "step": 1152,
      "training_loss": 6.938034534454346
    },
    {
      "epoch": 0.2497560975609756,
      "step": 1152,
      "training_loss": 6.7788872718811035
    },
    {
      "epoch": 0.2497560975609756,
      "step": 1152,
      "training_loss": 7.362497806549072
    },
    {
      "epoch": 0.24997289972899728,
      "step": 1153,
      "training_loss": 6.296611309051514
    },
    {
      "epoch": 0.24997289972899728,
      "step": 1153,
      "training_loss": 5.759487628936768
    },
    {
      "epoch": 0.24997289972899728,
      "step": 1153,
      "training_loss": 7.830583572387695
    },
    {
      "epoch": 0.24997289972899728,
      "step": 1153,
      "training_loss": 6.416811943054199
    },
    {
      "epoch": 0.25018970189701895,
      "step": 1154,
      "training_loss": 6.519064426422119
    },
    {
      "epoch": 0.25018970189701895,
      "step": 1154,
      "training_loss": 5.83140230178833
    },
    {
      "epoch": 0.25018970189701895,
      "step": 1154,
      "training_loss": 4.979236602783203
    },
    {
      "epoch": 0.25018970189701895,
      "step": 1154,
      "training_loss": 7.3240885734558105
    },
    {
      "epoch": 0.25040650406504067,
      "step": 1155,
      "training_loss": 5.30153226852417
    },
    {
      "epoch": 0.25040650406504067,
      "step": 1155,
      "training_loss": 7.419341564178467
    },
    {
      "epoch": 0.25040650406504067,
      "step": 1155,
      "training_loss": 6.532401084899902
    },
    {
      "epoch": 0.25040650406504067,
      "step": 1155,
      "training_loss": 7.2406325340271
    },
    {
      "epoch": 0.25062330623306234,
      "grad_norm": 15.477190971374512,
      "learning_rate": 1e-05,
      "loss": 6.5944,
      "step": 1156
    },
    {
      "epoch": 0.25062330623306234,
      "step": 1156,
      "training_loss": 8.410856246948242
    },
    {
      "epoch": 0.25062330623306234,
      "step": 1156,
      "training_loss": 6.589561939239502
    },
    {
      "epoch": 0.25062330623306234,
      "step": 1156,
      "training_loss": 4.203344821929932
    },
    {
      "epoch": 0.25062330623306234,
      "step": 1156,
      "training_loss": 6.521162509918213
    },
    {
      "epoch": 0.250840108401084,
      "step": 1157,
      "training_loss": 7.971388816833496
    },
    {
      "epoch": 0.250840108401084,
      "step": 1157,
      "training_loss": 6.540341854095459
    },
    {
      "epoch": 0.250840108401084,
      "step": 1157,
      "training_loss": 6.629518032073975
    },
    {
      "epoch": 0.250840108401084,
      "step": 1157,
      "training_loss": 7.965607166290283
    },
    {
      "epoch": 0.2510569105691057,
      "step": 1158,
      "training_loss": 5.978099346160889
    },
    {
      "epoch": 0.2510569105691057,
      "step": 1158,
      "training_loss": 6.973516464233398
    },
    {
      "epoch": 0.2510569105691057,
      "step": 1158,
      "training_loss": 7.249026775360107
    },
    {
      "epoch": 0.2510569105691057,
      "step": 1158,
      "training_loss": 8.065348625183105
    },
    {
      "epoch": 0.2512737127371274,
      "step": 1159,
      "training_loss": 6.714842319488525
    },
    {
      "epoch": 0.2512737127371274,
      "step": 1159,
      "training_loss": 7.241458415985107
    },
    {
      "epoch": 0.2512737127371274,
      "step": 1159,
      "training_loss": 6.791820049285889
    },
    {
      "epoch": 0.2512737127371274,
      "step": 1159,
      "training_loss": 6.812960147857666
    },
    {
      "epoch": 0.25149051490514907,
      "grad_norm": 15.861539840698242,
      "learning_rate": 1e-05,
      "loss": 6.9162,
      "step": 1160
    },
    {
      "epoch": 0.25149051490514907,
      "step": 1160,
      "training_loss": 6.675248622894287
    },
    {
      "epoch": 0.25149051490514907,
      "step": 1160,
      "training_loss": 6.6852850914001465
    },
    {
      "epoch": 0.25149051490514907,
      "step": 1160,
      "training_loss": 6.698747634887695
    },
    {
      "epoch": 0.25149051490514907,
      "step": 1160,
      "training_loss": 6.647373676300049
    },
    {
      "epoch": 0.25170731707317073,
      "step": 1161,
      "training_loss": 4.561410427093506
    },
    {
      "epoch": 0.25170731707317073,
      "step": 1161,
      "training_loss": 7.1724443435668945
    },
    {
      "epoch": 0.25170731707317073,
      "step": 1161,
      "training_loss": 6.530140399932861
    },
    {
      "epoch": 0.25170731707317073,
      "step": 1161,
      "training_loss": 6.454102993011475
    },
    {
      "epoch": 0.2519241192411924,
      "step": 1162,
      "training_loss": 7.704244136810303
    },
    {
      "epoch": 0.2519241192411924,
      "step": 1162,
      "training_loss": 6.429324626922607
    },
    {
      "epoch": 0.2519241192411924,
      "step": 1162,
      "training_loss": 7.6708478927612305
    },
    {
      "epoch": 0.2519241192411924,
      "step": 1162,
      "training_loss": 6.044451713562012
    },
    {
      "epoch": 0.25214092140921407,
      "step": 1163,
      "training_loss": 7.674633502960205
    },
    {
      "epoch": 0.25214092140921407,
      "step": 1163,
      "training_loss": 6.7578511238098145
    },
    {
      "epoch": 0.25214092140921407,
      "step": 1163,
      "training_loss": 7.924099445343018
    },
    {
      "epoch": 0.25214092140921407,
      "step": 1163,
      "training_loss": 6.130955219268799
    },
    {
      "epoch": 0.2523577235772358,
      "grad_norm": 14.67178726196289,
      "learning_rate": 1e-05,
      "loss": 6.7351,
      "step": 1164
    },
    {
      "epoch": 0.2523577235772358,
      "step": 1164,
      "training_loss": 7.146847724914551
    },
    {
      "epoch": 0.2523577235772358,
      "step": 1164,
      "training_loss": 6.2336745262146
    },
    {
      "epoch": 0.2523577235772358,
      "step": 1164,
      "training_loss": 5.868919849395752
    },
    {
      "epoch": 0.2523577235772358,
      "step": 1164,
      "training_loss": 4.899631500244141
    },
    {
      "epoch": 0.25257452574525746,
      "step": 1165,
      "training_loss": 4.212094306945801
    },
    {
      "epoch": 0.25257452574525746,
      "step": 1165,
      "training_loss": 6.165221214294434
    },
    {
      "epoch": 0.25257452574525746,
      "step": 1165,
      "training_loss": 5.940024375915527
    },
    {
      "epoch": 0.25257452574525746,
      "step": 1165,
      "training_loss": 7.497688293457031
    },
    {
      "epoch": 0.25279132791327913,
      "step": 1166,
      "training_loss": 7.500664710998535
    },
    {
      "epoch": 0.25279132791327913,
      "step": 1166,
      "training_loss": 8.238633155822754
    },
    {
      "epoch": 0.25279132791327913,
      "step": 1166,
      "training_loss": 4.315001487731934
    },
    {
      "epoch": 0.25279132791327913,
      "step": 1166,
      "training_loss": 5.908618450164795
    },
    {
      "epoch": 0.2530081300813008,
      "step": 1167,
      "training_loss": 9.086199760437012
    },
    {
      "epoch": 0.2530081300813008,
      "step": 1167,
      "training_loss": 6.736177921295166
    },
    {
      "epoch": 0.2530081300813008,
      "step": 1167,
      "training_loss": 7.08922815322876
    },
    {
      "epoch": 0.2530081300813008,
      "step": 1167,
      "training_loss": 7.406327724456787
    },
    {
      "epoch": 0.2532249322493225,
      "grad_norm": 18.677000045776367,
      "learning_rate": 1e-05,
      "loss": 6.5153,
      "step": 1168
    },
    {
      "epoch": 0.2532249322493225,
      "step": 1168,
      "training_loss": 4.726930141448975
    },
    {
      "epoch": 0.2532249322493225,
      "step": 1168,
      "training_loss": 6.032874584197998
    },
    {
      "epoch": 0.2532249322493225,
      "step": 1168,
      "training_loss": 7.194314002990723
    },
    {
      "epoch": 0.2532249322493225,
      "step": 1168,
      "training_loss": 7.1793622970581055
    },
    {
      "epoch": 0.2534417344173442,
      "step": 1169,
      "training_loss": 7.445631980895996
    },
    {
      "epoch": 0.2534417344173442,
      "step": 1169,
      "training_loss": 6.611545562744141
    },
    {
      "epoch": 0.2534417344173442,
      "step": 1169,
      "training_loss": 6.706212520599365
    },
    {
      "epoch": 0.2534417344173442,
      "step": 1169,
      "training_loss": 7.580338001251221
    },
    {
      "epoch": 0.25365853658536586,
      "step": 1170,
      "training_loss": 6.704108715057373
    },
    {
      "epoch": 0.25365853658536586,
      "step": 1170,
      "training_loss": 7.242380619049072
    },
    {
      "epoch": 0.25365853658536586,
      "step": 1170,
      "training_loss": 8.62434196472168
    },
    {
      "epoch": 0.25365853658536586,
      "step": 1170,
      "training_loss": 7.924494743347168
    },
    {
      "epoch": 0.2538753387533875,
      "step": 1171,
      "training_loss": 6.583343982696533
    },
    {
      "epoch": 0.2538753387533875,
      "step": 1171,
      "training_loss": 7.163374900817871
    },
    {
      "epoch": 0.2538753387533875,
      "step": 1171,
      "training_loss": 4.00091028213501
    },
    {
      "epoch": 0.2538753387533875,
      "step": 1171,
      "training_loss": 7.767458438873291
    },
    {
      "epoch": 0.2540921409214092,
      "grad_norm": 12.1524019241333,
      "learning_rate": 1e-05,
      "loss": 6.843,
      "step": 1172
    },
    {
      "epoch": 0.2540921409214092,
      "step": 1172,
      "training_loss": 7.054852485656738
    },
    {
      "epoch": 0.2540921409214092,
      "step": 1172,
      "training_loss": 5.545114040374756
    },
    {
      "epoch": 0.2540921409214092,
      "step": 1172,
      "training_loss": 7.092120170593262
    },
    {
      "epoch": 0.2540921409214092,
      "step": 1172,
      "training_loss": 6.469597339630127
    },
    {
      "epoch": 0.2543089430894309,
      "step": 1173,
      "training_loss": 6.733973026275635
    },
    {
      "epoch": 0.2543089430894309,
      "step": 1173,
      "training_loss": 6.341973304748535
    },
    {
      "epoch": 0.2543089430894309,
      "step": 1173,
      "training_loss": 6.907252788543701
    },
    {
      "epoch": 0.2543089430894309,
      "step": 1173,
      "training_loss": 4.071538925170898
    },
    {
      "epoch": 0.2545257452574526,
      "step": 1174,
      "training_loss": 5.632241725921631
    },
    {
      "epoch": 0.2545257452574526,
      "step": 1174,
      "training_loss": 6.824992656707764
    },
    {
      "epoch": 0.2545257452574526,
      "step": 1174,
      "training_loss": 7.028667449951172
    },
    {
      "epoch": 0.2545257452574526,
      "step": 1174,
      "training_loss": 5.660060405731201
    },
    {
      "epoch": 0.25474254742547425,
      "step": 1175,
      "training_loss": 6.966358661651611
    },
    {
      "epoch": 0.25474254742547425,
      "step": 1175,
      "training_loss": 7.363021373748779
    },
    {
      "epoch": 0.25474254742547425,
      "step": 1175,
      "training_loss": 7.4585747718811035
    },
    {
      "epoch": 0.25474254742547425,
      "step": 1175,
      "training_loss": 6.734447002410889
    },
    {
      "epoch": 0.2549593495934959,
      "grad_norm": 14.69416332244873,
      "learning_rate": 1e-05,
      "loss": 6.4928,
      "step": 1176
    },
    {
      "epoch": 0.2549593495934959,
      "step": 1176,
      "training_loss": 7.159759044647217
    },
    {
      "epoch": 0.2549593495934959,
      "step": 1176,
      "training_loss": 7.03575325012207
    },
    {
      "epoch": 0.2549593495934959,
      "step": 1176,
      "training_loss": 6.417213439941406
    },
    {
      "epoch": 0.2549593495934959,
      "step": 1176,
      "training_loss": 6.04651403427124
    },
    {
      "epoch": 0.2551761517615176,
      "step": 1177,
      "training_loss": 7.056924343109131
    },
    {
      "epoch": 0.2551761517615176,
      "step": 1177,
      "training_loss": 7.757740497589111
    },
    {
      "epoch": 0.2551761517615176,
      "step": 1177,
      "training_loss": 6.944985866546631
    },
    {
      "epoch": 0.2551761517615176,
      "step": 1177,
      "training_loss": 8.001652717590332
    },
    {
      "epoch": 0.2553929539295393,
      "step": 1178,
      "training_loss": 7.051724433898926
    },
    {
      "epoch": 0.2553929539295393,
      "step": 1178,
      "training_loss": 7.814977169036865
    },
    {
      "epoch": 0.2553929539295393,
      "step": 1178,
      "training_loss": 6.936789035797119
    },
    {
      "epoch": 0.2553929539295393,
      "step": 1178,
      "training_loss": 6.677354335784912
    },
    {
      "epoch": 0.255609756097561,
      "step": 1179,
      "training_loss": 6.040410995483398
    },
    {
      "epoch": 0.255609756097561,
      "step": 1179,
      "training_loss": 6.485989093780518
    },
    {
      "epoch": 0.255609756097561,
      "step": 1179,
      "training_loss": 7.184370040893555
    },
    {
      "epoch": 0.255609756097561,
      "step": 1179,
      "training_loss": 6.554378032684326
    },
    {
      "epoch": 0.25582655826558265,
      "grad_norm": 13.932809829711914,
      "learning_rate": 1e-05,
      "loss": 6.9479,
      "step": 1180
    },
    {
      "epoch": 0.25582655826558265,
      "step": 1180,
      "training_loss": 7.471374034881592
    },
    {
      "epoch": 0.25582655826558265,
      "step": 1180,
      "training_loss": 6.902505874633789
    },
    {
      "epoch": 0.25582655826558265,
      "step": 1180,
      "training_loss": 5.826280117034912
    },
    {
      "epoch": 0.25582655826558265,
      "step": 1180,
      "training_loss": 6.691870212554932
    },
    {
      "epoch": 0.2560433604336043,
      "step": 1181,
      "training_loss": 7.706649303436279
    },
    {
      "epoch": 0.2560433604336043,
      "step": 1181,
      "training_loss": 6.482424259185791
    },
    {
      "epoch": 0.2560433604336043,
      "step": 1181,
      "training_loss": 5.468364238739014
    },
    {
      "epoch": 0.2560433604336043,
      "step": 1181,
      "training_loss": 7.858631610870361
    },
    {
      "epoch": 0.25626016260162604,
      "step": 1182,
      "training_loss": 6.922835826873779
    },
    {
      "epoch": 0.25626016260162604,
      "step": 1182,
      "training_loss": 6.202931880950928
    },
    {
      "epoch": 0.25626016260162604,
      "step": 1182,
      "training_loss": 7.135705947875977
    },
    {
      "epoch": 0.25626016260162604,
      "step": 1182,
      "training_loss": 6.579414367675781
    },
    {
      "epoch": 0.2564769647696477,
      "step": 1183,
      "training_loss": 7.222825050354004
    },
    {
      "epoch": 0.2564769647696477,
      "step": 1183,
      "training_loss": 7.577293395996094
    },
    {
      "epoch": 0.2564769647696477,
      "step": 1183,
      "training_loss": 6.9796247482299805
    },
    {
      "epoch": 0.2564769647696477,
      "step": 1183,
      "training_loss": 7.500078201293945
    },
    {
      "epoch": 0.2566937669376694,
      "grad_norm": 13.865222930908203,
      "learning_rate": 1e-05,
      "loss": 6.9081,
      "step": 1184
    },
    {
      "epoch": 0.2566937669376694,
      "step": 1184,
      "training_loss": 6.924456596374512
    },
    {
      "epoch": 0.2566937669376694,
      "step": 1184,
      "training_loss": 5.67438268661499
    },
    {
      "epoch": 0.2566937669376694,
      "step": 1184,
      "training_loss": 7.557185173034668
    },
    {
      "epoch": 0.2566937669376694,
      "step": 1184,
      "training_loss": 7.0910491943359375
    },
    {
      "epoch": 0.25691056910569104,
      "step": 1185,
      "training_loss": 6.842487812042236
    },
    {
      "epoch": 0.25691056910569104,
      "step": 1185,
      "training_loss": 8.405190467834473
    },
    {
      "epoch": 0.25691056910569104,
      "step": 1185,
      "training_loss": 6.937725067138672
    },
    {
      "epoch": 0.25691056910569104,
      "step": 1185,
      "training_loss": 7.053127288818359
    },
    {
      "epoch": 0.2571273712737127,
      "step": 1186,
      "training_loss": 7.316253662109375
    },
    {
      "epoch": 0.2571273712737127,
      "step": 1186,
      "training_loss": 7.547175884246826
    },
    {
      "epoch": 0.2571273712737127,
      "step": 1186,
      "training_loss": 7.48198127746582
    },
    {
      "epoch": 0.2571273712737127,
      "step": 1186,
      "training_loss": 5.992469310760498
    },
    {
      "epoch": 0.25734417344173444,
      "step": 1187,
      "training_loss": 7.419839382171631
    },
    {
      "epoch": 0.25734417344173444,
      "step": 1187,
      "training_loss": 7.432235240936279
    },
    {
      "epoch": 0.25734417344173444,
      "step": 1187,
      "training_loss": 6.150386333465576
    },
    {
      "epoch": 0.25734417344173444,
      "step": 1187,
      "training_loss": 7.7722554206848145
    },
    {
      "epoch": 0.2575609756097561,
      "grad_norm": 14.726133346557617,
      "learning_rate": 1e-05,
      "loss": 7.0999,
      "step": 1188
    },
    {
      "epoch": 0.2575609756097561,
      "step": 1188,
      "training_loss": 6.602014064788818
    },
    {
      "epoch": 0.2575609756097561,
      "step": 1188,
      "training_loss": 5.643073558807373
    },
    {
      "epoch": 0.2575609756097561,
      "step": 1188,
      "training_loss": 6.938040733337402
    },
    {
      "epoch": 0.2575609756097561,
      "step": 1188,
      "training_loss": 6.556100845336914
    },
    {
      "epoch": 0.2577777777777778,
      "step": 1189,
      "training_loss": 4.943605899810791
    },
    {
      "epoch": 0.2577777777777778,
      "step": 1189,
      "training_loss": 7.56016731262207
    },
    {
      "epoch": 0.2577777777777778,
      "step": 1189,
      "training_loss": 7.5567626953125
    },
    {
      "epoch": 0.2577777777777778,
      "step": 1189,
      "training_loss": 7.670636177062988
    },
    {
      "epoch": 0.25799457994579944,
      "step": 1190,
      "training_loss": 7.119740009307861
    },
    {
      "epoch": 0.25799457994579944,
      "step": 1190,
      "training_loss": 4.935723304748535
    },
    {
      "epoch": 0.25799457994579944,
      "step": 1190,
      "training_loss": 7.065262794494629
    },
    {
      "epoch": 0.25799457994579944,
      "step": 1190,
      "training_loss": 7.437734603881836
    },
    {
      "epoch": 0.25821138211382116,
      "step": 1191,
      "training_loss": 6.988476753234863
    },
    {
      "epoch": 0.25821138211382116,
      "step": 1191,
      "training_loss": 6.751095294952393
    },
    {
      "epoch": 0.25821138211382116,
      "step": 1191,
      "training_loss": 7.358656883239746
    },
    {
      "epoch": 0.25821138211382116,
      "step": 1191,
      "training_loss": 7.226243495941162
    },
    {
      "epoch": 0.25842818428184283,
      "grad_norm": 11.575762748718262,
      "learning_rate": 1e-05,
      "loss": 6.7721,
      "step": 1192
    },
    {
      "epoch": 0.25842818428184283,
      "step": 1192,
      "training_loss": 5.419506549835205
    },
    {
      "epoch": 0.25842818428184283,
      "step": 1192,
      "training_loss": 6.646270751953125
    },
    {
      "epoch": 0.25842818428184283,
      "step": 1192,
      "training_loss": 7.003760814666748
    },
    {
      "epoch": 0.25842818428184283,
      "step": 1192,
      "training_loss": 6.191370010375977
    },
    {
      "epoch": 0.2586449864498645,
      "step": 1193,
      "training_loss": 6.934004783630371
    },
    {
      "epoch": 0.2586449864498645,
      "step": 1193,
      "training_loss": 7.506249904632568
    },
    {
      "epoch": 0.2586449864498645,
      "step": 1193,
      "training_loss": 7.33634614944458
    },
    {
      "epoch": 0.2586449864498645,
      "step": 1193,
      "training_loss": 6.829737663269043
    },
    {
      "epoch": 0.25886178861788617,
      "step": 1194,
      "training_loss": 7.466970443725586
    },
    {
      "epoch": 0.25886178861788617,
      "step": 1194,
      "training_loss": 7.595094203948975
    },
    {
      "epoch": 0.25886178861788617,
      "step": 1194,
      "training_loss": 6.794471263885498
    },
    {
      "epoch": 0.25886178861788617,
      "step": 1194,
      "training_loss": 7.242392063140869
    },
    {
      "epoch": 0.25907859078590784,
      "step": 1195,
      "training_loss": 6.615776538848877
    },
    {
      "epoch": 0.25907859078590784,
      "step": 1195,
      "training_loss": 6.817997932434082
    },
    {
      "epoch": 0.25907859078590784,
      "step": 1195,
      "training_loss": 6.380482196807861
    },
    {
      "epoch": 0.25907859078590784,
      "step": 1195,
      "training_loss": 6.651588439941406
    },
    {
      "epoch": 0.25929539295392956,
      "grad_norm": 11.727580070495605,
      "learning_rate": 1e-05,
      "loss": 6.8395,
      "step": 1196
    },
    {
      "epoch": 0.25929539295392956,
      "step": 1196,
      "training_loss": 8.686407089233398
    },
    {
      "epoch": 0.25929539295392956,
      "step": 1196,
      "training_loss": 7.611452102661133
    },
    {
      "epoch": 0.25929539295392956,
      "step": 1196,
      "training_loss": 4.3867878913879395
    },
    {
      "epoch": 0.25929539295392956,
      "step": 1196,
      "training_loss": 4.2822370529174805
    },
    {
      "epoch": 0.25951219512195123,
      "step": 1197,
      "training_loss": 6.905032634735107
    },
    {
      "epoch": 0.25951219512195123,
      "step": 1197,
      "training_loss": 7.099164962768555
    },
    {
      "epoch": 0.25951219512195123,
      "step": 1197,
      "training_loss": 6.472329616546631
    },
    {
      "epoch": 0.25951219512195123,
      "step": 1197,
      "training_loss": 7.516144275665283
    },
    {
      "epoch": 0.2597289972899729,
      "step": 1198,
      "training_loss": 6.736466884613037
    },
    {
      "epoch": 0.2597289972899729,
      "step": 1198,
      "training_loss": 7.757477760314941
    },
    {
      "epoch": 0.2597289972899729,
      "step": 1198,
      "training_loss": 6.466638565063477
    },
    {
      "epoch": 0.2597289972899729,
      "step": 1198,
      "training_loss": 6.3189873695373535
    },
    {
      "epoch": 0.25994579945799456,
      "step": 1199,
      "training_loss": 7.021762371063232
    },
    {
      "epoch": 0.25994579945799456,
      "step": 1199,
      "training_loss": 6.9184250831604
    },
    {
      "epoch": 0.25994579945799456,
      "step": 1199,
      "training_loss": 6.493801593780518
    },
    {
      "epoch": 0.25994579945799456,
      "step": 1199,
      "training_loss": 7.569875717163086
    },
    {
      "epoch": 0.2601626016260163,
      "grad_norm": 13.377778053283691,
      "learning_rate": 1e-05,
      "loss": 6.7652,
      "step": 1200
    },
    {
      "epoch": 0.2601626016260163,
      "step": 1200,
      "training_loss": 7.4428606033325195
    },
    {
      "epoch": 0.2601626016260163,
      "step": 1200,
      "training_loss": 5.8878092765808105
    },
    {
      "epoch": 0.2601626016260163,
      "step": 1200,
      "training_loss": 8.119656562805176
    },
    {
      "epoch": 0.2601626016260163,
      "step": 1200,
      "training_loss": 3.967881202697754
    },
    {
      "epoch": 0.26037940379403796,
      "step": 1201,
      "training_loss": 7.541084289550781
    },
    {
      "epoch": 0.26037940379403796,
      "step": 1201,
      "training_loss": 5.624430179595947
    },
    {
      "epoch": 0.26037940379403796,
      "step": 1201,
      "training_loss": 7.389548301696777
    },
    {
      "epoch": 0.26037940379403796,
      "step": 1201,
      "training_loss": 7.376512050628662
    },
    {
      "epoch": 0.2605962059620596,
      "step": 1202,
      "training_loss": 5.205961227416992
    },
    {
      "epoch": 0.2605962059620596,
      "step": 1202,
      "training_loss": 6.8987956047058105
    },
    {
      "epoch": 0.2605962059620596,
      "step": 1202,
      "training_loss": 6.966306686401367
    },
    {
      "epoch": 0.2605962059620596,
      "step": 1202,
      "training_loss": 6.646025657653809
    },
    {
      "epoch": 0.2608130081300813,
      "step": 1203,
      "training_loss": 7.315791606903076
    },
    {
      "epoch": 0.2608130081300813,
      "step": 1203,
      "training_loss": 5.956434726715088
    },
    {
      "epoch": 0.2608130081300813,
      "step": 1203,
      "training_loss": 7.155303001403809
    },
    {
      "epoch": 0.2608130081300813,
      "step": 1203,
      "training_loss": 7.530378341674805
    },
    {
      "epoch": 0.26102981029810296,
      "grad_norm": 15.126099586486816,
      "learning_rate": 1e-05,
      "loss": 6.689,
      "step": 1204
    },
    {
      "epoch": 0.26102981029810296,
      "step": 1204,
      "training_loss": 4.492587089538574
    },
    {
      "epoch": 0.26102981029810296,
      "step": 1204,
      "training_loss": 6.7146711349487305
    },
    {
      "epoch": 0.26102981029810296,
      "step": 1204,
      "training_loss": 5.276289939880371
    },
    {
      "epoch": 0.26102981029810296,
      "step": 1204,
      "training_loss": 6.263497829437256
    },
    {
      "epoch": 0.2612466124661247,
      "step": 1205,
      "training_loss": 6.233273506164551
    },
    {
      "epoch": 0.2612466124661247,
      "step": 1205,
      "training_loss": 6.800948619842529
    },
    {
      "epoch": 0.2612466124661247,
      "step": 1205,
      "training_loss": 8.37907600402832
    },
    {
      "epoch": 0.2612466124661247,
      "step": 1205,
      "training_loss": 4.333276271820068
    },
    {
      "epoch": 0.26146341463414635,
      "step": 1206,
      "training_loss": 7.184767246246338
    },
    {
      "epoch": 0.26146341463414635,
      "step": 1206,
      "training_loss": 7.582998752593994
    },
    {
      "epoch": 0.26146341463414635,
      "step": 1206,
      "training_loss": 7.391961097717285
    },
    {
      "epoch": 0.26146341463414635,
      "step": 1206,
      "training_loss": 7.299191474914551
    },
    {
      "epoch": 0.261680216802168,
      "step": 1207,
      "training_loss": 6.912837982177734
    },
    {
      "epoch": 0.261680216802168,
      "step": 1207,
      "training_loss": 6.236178398132324
    },
    {
      "epoch": 0.261680216802168,
      "step": 1207,
      "training_loss": 6.641618251800537
    },
    {
      "epoch": 0.261680216802168,
      "step": 1207,
      "training_loss": 7.155877113342285
    },
    {
      "epoch": 0.2618970189701897,
      "grad_norm": 12.372371673583984,
      "learning_rate": 1e-05,
      "loss": 6.5562,
      "step": 1208
    },
    {
      "epoch": 0.2618970189701897,
      "step": 1208,
      "training_loss": 5.187840938568115
    },
    {
      "epoch": 0.2618970189701897,
      "step": 1208,
      "training_loss": 4.205121994018555
    },
    {
      "epoch": 0.2618970189701897,
      "step": 1208,
      "training_loss": 7.2435688972473145
    },
    {
      "epoch": 0.2618970189701897,
      "step": 1208,
      "training_loss": 7.03528356552124
    },
    {
      "epoch": 0.26211382113821136,
      "step": 1209,
      "training_loss": 6.03248929977417
    },
    {
      "epoch": 0.26211382113821136,
      "step": 1209,
      "training_loss": 7.1277594566345215
    },
    {
      "epoch": 0.26211382113821136,
      "step": 1209,
      "training_loss": 7.960866451263428
    },
    {
      "epoch": 0.26211382113821136,
      "step": 1209,
      "training_loss": 4.070746421813965
    },
    {
      "epoch": 0.2623306233062331,
      "step": 1210,
      "training_loss": 6.172758102416992
    },
    {
      "epoch": 0.2623306233062331,
      "step": 1210,
      "training_loss": 7.202614784240723
    },
    {
      "epoch": 0.2623306233062331,
      "step": 1210,
      "training_loss": 5.895495891571045
    },
    {
      "epoch": 0.2623306233062331,
      "step": 1210,
      "training_loss": 6.679699897766113
    },
    {
      "epoch": 0.26254742547425475,
      "step": 1211,
      "training_loss": 7.056246757507324
    },
    {
      "epoch": 0.26254742547425475,
      "step": 1211,
      "training_loss": 6.9135637283325195
    },
    {
      "epoch": 0.26254742547425475,
      "step": 1211,
      "training_loss": 6.606335163116455
    },
    {
      "epoch": 0.26254742547425475,
      "step": 1211,
      "training_loss": 6.152341842651367
    },
    {
      "epoch": 0.2627642276422764,
      "grad_norm": 13.589262008666992,
      "learning_rate": 1e-05,
      "loss": 6.3464,
      "step": 1212
    },
    {
      "epoch": 0.2627642276422764,
      "step": 1212,
      "training_loss": 7.749239444732666
    },
    {
      "epoch": 0.2627642276422764,
      "step": 1212,
      "training_loss": 7.101838111877441
    },
    {
      "epoch": 0.2627642276422764,
      "step": 1212,
      "training_loss": 6.97429084777832
    },
    {
      "epoch": 0.2627642276422764,
      "step": 1212,
      "training_loss": 7.298833847045898
    },
    {
      "epoch": 0.2629810298102981,
      "step": 1213,
      "training_loss": 5.936355113983154
    },
    {
      "epoch": 0.2629810298102981,
      "step": 1213,
      "training_loss": 7.028907775878906
    },
    {
      "epoch": 0.2629810298102981,
      "step": 1213,
      "training_loss": 5.7746453285217285
    },
    {
      "epoch": 0.2629810298102981,
      "step": 1213,
      "training_loss": 7.28926420211792
    },
    {
      "epoch": 0.2631978319783198,
      "step": 1214,
      "training_loss": 5.95646858215332
    },
    {
      "epoch": 0.2631978319783198,
      "step": 1214,
      "training_loss": 8.702369689941406
    },
    {
      "epoch": 0.2631978319783198,
      "step": 1214,
      "training_loss": 6.882928371429443
    },
    {
      "epoch": 0.2631978319783198,
      "step": 1214,
      "training_loss": 7.972075939178467
    },
    {
      "epoch": 0.2634146341463415,
      "step": 1215,
      "training_loss": 7.837807655334473
    },
    {
      "epoch": 0.2634146341463415,
      "step": 1215,
      "training_loss": 6.056807041168213
    },
    {
      "epoch": 0.2634146341463415,
      "step": 1215,
      "training_loss": 7.026334762573242
    },
    {
      "epoch": 0.2634146341463415,
      "step": 1215,
      "training_loss": 8.269257545471191
    },
    {
      "epoch": 0.26363143631436314,
      "grad_norm": 10.940226554870605,
      "learning_rate": 1e-05,
      "loss": 7.1161,
      "step": 1216
    },
    {
      "epoch": 0.26363143631436314,
      "step": 1216,
      "training_loss": 6.957222938537598
    },
    {
      "epoch": 0.26363143631436314,
      "step": 1216,
      "training_loss": 7.309741497039795
    },
    {
      "epoch": 0.26363143631436314,
      "step": 1216,
      "training_loss": 7.079995632171631
    },
    {
      "epoch": 0.26363143631436314,
      "step": 1216,
      "training_loss": 5.512380599975586
    },
    {
      "epoch": 0.2638482384823848,
      "step": 1217,
      "training_loss": 7.693929672241211
    },
    {
      "epoch": 0.2638482384823848,
      "step": 1217,
      "training_loss": 5.748963356018066
    },
    {
      "epoch": 0.2638482384823848,
      "step": 1217,
      "training_loss": 6.087589263916016
    },
    {
      "epoch": 0.2638482384823848,
      "step": 1217,
      "training_loss": 7.087571144104004
    },
    {
      "epoch": 0.2640650406504065,
      "step": 1218,
      "training_loss": 7.380123615264893
    },
    {
      "epoch": 0.2640650406504065,
      "step": 1218,
      "training_loss": 6.042848110198975
    },
    {
      "epoch": 0.2640650406504065,
      "step": 1218,
      "training_loss": 8.37425708770752
    },
    {
      "epoch": 0.2640650406504065,
      "step": 1218,
      "training_loss": 5.721967697143555
    },
    {
      "epoch": 0.2642818428184282,
      "step": 1219,
      "training_loss": 6.291070938110352
    },
    {
      "epoch": 0.2642818428184282,
      "step": 1219,
      "training_loss": 7.177716255187988
    },
    {
      "epoch": 0.2642818428184282,
      "step": 1219,
      "training_loss": 6.895393371582031
    },
    {
      "epoch": 0.2642818428184282,
      "step": 1219,
      "training_loss": 7.061700820922852
    },
    {
      "epoch": 0.26449864498644987,
      "grad_norm": 12.569005966186523,
      "learning_rate": 1e-05,
      "loss": 6.7764,
      "step": 1220
    },
    {
      "epoch": 0.26449864498644987,
      "step": 1220,
      "training_loss": 6.950433731079102
    },
    {
      "epoch": 0.26449864498644987,
      "step": 1220,
      "training_loss": 7.2677903175354
    },
    {
      "epoch": 0.26449864498644987,
      "step": 1220,
      "training_loss": 4.338557243347168
    },
    {
      "epoch": 0.26449864498644987,
      "step": 1220,
      "training_loss": 7.9460062980651855
    },
    {
      "epoch": 0.26471544715447154,
      "step": 1221,
      "training_loss": 5.558574676513672
    },
    {
      "epoch": 0.26471544715447154,
      "step": 1221,
      "training_loss": 6.7215704917907715
    },
    {
      "epoch": 0.26471544715447154,
      "step": 1221,
      "training_loss": 8.8281831741333
    },
    {
      "epoch": 0.26471544715447154,
      "step": 1221,
      "training_loss": 7.287763595581055
    },
    {
      "epoch": 0.2649322493224932,
      "step": 1222,
      "training_loss": 7.790215015411377
    },
    {
      "epoch": 0.2649322493224932,
      "step": 1222,
      "training_loss": 6.676602840423584
    },
    {
      "epoch": 0.2649322493224932,
      "step": 1222,
      "training_loss": 6.854658603668213
    },
    {
      "epoch": 0.2649322493224932,
      "step": 1222,
      "training_loss": 5.65119743347168
    },
    {
      "epoch": 0.26514905149051493,
      "step": 1223,
      "training_loss": 7.144409656524658
    },
    {
      "epoch": 0.26514905149051493,
      "step": 1223,
      "training_loss": 7.640016078948975
    },
    {
      "epoch": 0.26514905149051493,
      "step": 1223,
      "training_loss": 7.353480339050293
    },
    {
      "epoch": 0.26514905149051493,
      "step": 1223,
      "training_loss": 6.22286319732666
    },
    {
      "epoch": 0.2653658536585366,
      "grad_norm": 11.064715385437012,
      "learning_rate": 1e-05,
      "loss": 6.8895,
      "step": 1224
    },
    {
      "epoch": 0.2653658536585366,
      "step": 1224,
      "training_loss": 6.379822254180908
    },
    {
      "epoch": 0.2653658536585366,
      "step": 1224,
      "training_loss": 6.3986358642578125
    },
    {
      "epoch": 0.2653658536585366,
      "step": 1224,
      "training_loss": 7.023858547210693
    },
    {
      "epoch": 0.2653658536585366,
      "step": 1224,
      "training_loss": 5.976156711578369
    },
    {
      "epoch": 0.26558265582655827,
      "step": 1225,
      "training_loss": 7.160346984863281
    },
    {
      "epoch": 0.26558265582655827,
      "step": 1225,
      "training_loss": 5.87714958190918
    },
    {
      "epoch": 0.26558265582655827,
      "step": 1225,
      "training_loss": 6.29430627822876
    },
    {
      "epoch": 0.26558265582655827,
      "step": 1225,
      "training_loss": 7.304287433624268
    },
    {
      "epoch": 0.26579945799457994,
      "step": 1226,
      "training_loss": 6.537153244018555
    },
    {
      "epoch": 0.26579945799457994,
      "step": 1226,
      "training_loss": 7.346737384796143
    },
    {
      "epoch": 0.26579945799457994,
      "step": 1226,
      "training_loss": 7.197120666503906
    },
    {
      "epoch": 0.26579945799457994,
      "step": 1226,
      "training_loss": 6.558924674987793
    },
    {
      "epoch": 0.2660162601626016,
      "step": 1227,
      "training_loss": 7.496476650238037
    },
    {
      "epoch": 0.2660162601626016,
      "step": 1227,
      "training_loss": 5.7357611656188965
    },
    {
      "epoch": 0.2660162601626016,
      "step": 1227,
      "training_loss": 6.90187406539917
    },
    {
      "epoch": 0.2660162601626016,
      "step": 1227,
      "training_loss": 7.771990776062012
    },
    {
      "epoch": 0.2662330623306233,
      "grad_norm": 9.483983993530273,
      "learning_rate": 1e-05,
      "loss": 6.7475,
      "step": 1228
    },
    {
      "epoch": 0.2662330623306233,
      "step": 1228,
      "training_loss": 8.99083423614502
    },
    {
      "epoch": 0.2662330623306233,
      "step": 1228,
      "training_loss": 8.088475227355957
    },
    {
      "epoch": 0.2662330623306233,
      "step": 1228,
      "training_loss": 7.045226097106934
    },
    {
      "epoch": 0.2662330623306233,
      "step": 1228,
      "training_loss": 4.859979629516602
    },
    {
      "epoch": 0.266449864498645,
      "step": 1229,
      "training_loss": 7.207235336303711
    },
    {
      "epoch": 0.266449864498645,
      "step": 1229,
      "training_loss": 7.199958801269531
    },
    {
      "epoch": 0.266449864498645,
      "step": 1229,
      "training_loss": 6.6183319091796875
    },
    {
      "epoch": 0.266449864498645,
      "step": 1229,
      "training_loss": 6.559317111968994
    },
    {
      "epoch": 0.26666666666666666,
      "step": 1230,
      "training_loss": 6.025253772735596
    },
    {
      "epoch": 0.26666666666666666,
      "step": 1230,
      "training_loss": 6.862485408782959
    },
    {
      "epoch": 0.26666666666666666,
      "step": 1230,
      "training_loss": 5.981011867523193
    },
    {
      "epoch": 0.26666666666666666,
      "step": 1230,
      "training_loss": 6.849032878875732
    },
    {
      "epoch": 0.26688346883468833,
      "step": 1231,
      "training_loss": 7.4531707763671875
    },
    {
      "epoch": 0.26688346883468833,
      "step": 1231,
      "training_loss": 7.699061393737793
    },
    {
      "epoch": 0.26688346883468833,
      "step": 1231,
      "training_loss": 6.589794158935547
    },
    {
      "epoch": 0.26688346883468833,
      "step": 1231,
      "training_loss": 6.605055809020996
    },
    {
      "epoch": 0.26710027100271005,
      "grad_norm": 8.856049537658691,
      "learning_rate": 1e-05,
      "loss": 6.9146,
      "step": 1232
    },
    {
      "epoch": 0.26710027100271005,
      "step": 1232,
      "training_loss": 3.9665610790252686
    },
    {
      "epoch": 0.26710027100271005,
      "step": 1232,
      "training_loss": 6.650651454925537
    },
    {
      "epoch": 0.26710027100271005,
      "step": 1232,
      "training_loss": 7.252195835113525
    },
    {
      "epoch": 0.26710027100271005,
      "step": 1232,
      "training_loss": 6.767505645751953
    },
    {
      "epoch": 0.2673170731707317,
      "step": 1233,
      "training_loss": 7.424722194671631
    },
    {
      "epoch": 0.2673170731707317,
      "step": 1233,
      "training_loss": 7.169427394866943
    },
    {
      "epoch": 0.2673170731707317,
      "step": 1233,
      "training_loss": 5.887815952301025
    },
    {
      "epoch": 0.2673170731707317,
      "step": 1233,
      "training_loss": 4.630364894866943
    },
    {
      "epoch": 0.2675338753387534,
      "step": 1234,
      "training_loss": 6.819945812225342
    },
    {
      "epoch": 0.2675338753387534,
      "step": 1234,
      "training_loss": 8.589256286621094
    },
    {
      "epoch": 0.2675338753387534,
      "step": 1234,
      "training_loss": 6.866786003112793
    },
    {
      "epoch": 0.2675338753387534,
      "step": 1234,
      "training_loss": 4.584987640380859
    },
    {
      "epoch": 0.26775067750677506,
      "step": 1235,
      "training_loss": 6.365230560302734
    },
    {
      "epoch": 0.26775067750677506,
      "step": 1235,
      "training_loss": 6.260791778564453
    },
    {
      "epoch": 0.26775067750677506,
      "step": 1235,
      "training_loss": 7.038957118988037
    },
    {
      "epoch": 0.26775067750677506,
      "step": 1235,
      "training_loss": 7.460729598999023
    },
    {
      "epoch": 0.2679674796747967,
      "grad_norm": 9.476186752319336,
      "learning_rate": 1e-05,
      "loss": 6.4835,
      "step": 1236
    },
    {
      "epoch": 0.2679674796747967,
      "step": 1236,
      "training_loss": 7.111823081970215
    },
    {
      "epoch": 0.2679674796747967,
      "step": 1236,
      "training_loss": 7.421720504760742
    },
    {
      "epoch": 0.2679674796747967,
      "step": 1236,
      "training_loss": 7.059980392456055
    },
    {
      "epoch": 0.2679674796747967,
      "step": 1236,
      "training_loss": 6.674509048461914
    },
    {
      "epoch": 0.26818428184281845,
      "step": 1237,
      "training_loss": 9.60118579864502
    },
    {
      "epoch": 0.26818428184281845,
      "step": 1237,
      "training_loss": 7.213562965393066
    },
    {
      "epoch": 0.26818428184281845,
      "step": 1237,
      "training_loss": 7.455472469329834
    },
    {
      "epoch": 0.26818428184281845,
      "step": 1237,
      "training_loss": 7.300440311431885
    },
    {
      "epoch": 0.2684010840108401,
      "step": 1238,
      "training_loss": 7.037896633148193
    },
    {
      "epoch": 0.2684010840108401,
      "step": 1238,
      "training_loss": 7.128944396972656
    },
    {
      "epoch": 0.2684010840108401,
      "step": 1238,
      "training_loss": 8.010171890258789
    },
    {
      "epoch": 0.2684010840108401,
      "step": 1238,
      "training_loss": 5.88821268081665
    },
    {
      "epoch": 0.2686178861788618,
      "step": 1239,
      "training_loss": 7.644579887390137
    },
    {
      "epoch": 0.2686178861788618,
      "step": 1239,
      "training_loss": 3.9285805225372314
    },
    {
      "epoch": 0.2686178861788618,
      "step": 1239,
      "training_loss": 7.942766189575195
    },
    {
      "epoch": 0.2686178861788618,
      "step": 1239,
      "training_loss": 5.755613327026367
    },
    {
      "epoch": 0.26883468834688345,
      "grad_norm": 12.54114818572998,
      "learning_rate": 1e-05,
      "loss": 7.0735,
      "step": 1240
    },
    {
      "epoch": 0.26883468834688345,
      "step": 1240,
      "training_loss": 6.531989097595215
    },
    {
      "epoch": 0.26883468834688345,
      "step": 1240,
      "training_loss": 5.999692440032959
    },
    {
      "epoch": 0.26883468834688345,
      "step": 1240,
      "training_loss": 7.8127923011779785
    },
    {
      "epoch": 0.26883468834688345,
      "step": 1240,
      "training_loss": 8.456238746643066
    },
    {
      "epoch": 0.2690514905149051,
      "step": 1241,
      "training_loss": 8.587170600891113
    },
    {
      "epoch": 0.2690514905149051,
      "step": 1241,
      "training_loss": 7.032614231109619
    },
    {
      "epoch": 0.2690514905149051,
      "step": 1241,
      "training_loss": 6.901297569274902
    },
    {
      "epoch": 0.2690514905149051,
      "step": 1241,
      "training_loss": 7.494546413421631
    },
    {
      "epoch": 0.26926829268292685,
      "step": 1242,
      "training_loss": 7.85774040222168
    },
    {
      "epoch": 0.26926829268292685,
      "step": 1242,
      "training_loss": 7.514543533325195
    },
    {
      "epoch": 0.26926829268292685,
      "step": 1242,
      "training_loss": 6.614981174468994
    },
    {
      "epoch": 0.26926829268292685,
      "step": 1242,
      "training_loss": 5.504301071166992
    },
    {
      "epoch": 0.2694850948509485,
      "step": 1243,
      "training_loss": 7.344936847686768
    },
    {
      "epoch": 0.2694850948509485,
      "step": 1243,
      "training_loss": 6.631008148193359
    },
    {
      "epoch": 0.2694850948509485,
      "step": 1243,
      "training_loss": 6.33540153503418
    },
    {
      "epoch": 0.2694850948509485,
      "step": 1243,
      "training_loss": 5.936873912811279
    },
    {
      "epoch": 0.2697018970189702,
      "grad_norm": 15.30392074584961,
      "learning_rate": 1e-05,
      "loss": 7.0348,
      "step": 1244
    },
    {
      "epoch": 0.2697018970189702,
      "step": 1244,
      "training_loss": 4.174477577209473
    },
    {
      "epoch": 0.2697018970189702,
      "step": 1244,
      "training_loss": 7.248812198638916
    },
    {
      "epoch": 0.2697018970189702,
      "step": 1244,
      "training_loss": 5.565361022949219
    },
    {
      "epoch": 0.2697018970189702,
      "step": 1244,
      "training_loss": 8.27523422241211
    },
    {
      "epoch": 0.26991869918699185,
      "step": 1245,
      "training_loss": 7.236711025238037
    },
    {
      "epoch": 0.26991869918699185,
      "step": 1245,
      "training_loss": 7.788871765136719
    },
    {
      "epoch": 0.26991869918699185,
      "step": 1245,
      "training_loss": 6.00822639465332
    },
    {
      "epoch": 0.26991869918699185,
      "step": 1245,
      "training_loss": 7.085115909576416
    },
    {
      "epoch": 0.2701355013550136,
      "step": 1246,
      "training_loss": 6.1818952560424805
    },
    {
      "epoch": 0.2701355013550136,
      "step": 1246,
      "training_loss": 7.257464408874512
    },
    {
      "epoch": 0.2701355013550136,
      "step": 1246,
      "training_loss": 7.500173568725586
    },
    {
      "epoch": 0.2701355013550136,
      "step": 1246,
      "training_loss": 7.029336929321289
    },
    {
      "epoch": 0.27035230352303524,
      "step": 1247,
      "training_loss": 5.466380596160889
    },
    {
      "epoch": 0.27035230352303524,
      "step": 1247,
      "training_loss": 6.454343318939209
    },
    {
      "epoch": 0.27035230352303524,
      "step": 1247,
      "training_loss": 6.514760494232178
    },
    {
      "epoch": 0.27035230352303524,
      "step": 1247,
      "training_loss": 4.396745681762695
    },
    {
      "epoch": 0.2705691056910569,
      "grad_norm": 16.609325408935547,
      "learning_rate": 1e-05,
      "loss": 6.5115,
      "step": 1248
    },
    {
      "epoch": 0.2705691056910569,
      "step": 1248,
      "training_loss": 4.806519031524658
    },
    {
      "epoch": 0.2705691056910569,
      "step": 1248,
      "training_loss": 6.2596435546875
    },
    {
      "epoch": 0.2705691056910569,
      "step": 1248,
      "training_loss": 6.526283264160156
    },
    {
      "epoch": 0.2705691056910569,
      "step": 1248,
      "training_loss": 7.295706272125244
    },
    {
      "epoch": 0.2707859078590786,
      "step": 1249,
      "training_loss": 7.5635480880737305
    },
    {
      "epoch": 0.2707859078590786,
      "step": 1249,
      "training_loss": 6.952712059020996
    },
    {
      "epoch": 0.2707859078590786,
      "step": 1249,
      "training_loss": 6.926498889923096
    },
    {
      "epoch": 0.2707859078590786,
      "step": 1249,
      "training_loss": 5.081357002258301
    },
    {
      "epoch": 0.27100271002710025,
      "step": 1250,
      "training_loss": 7.903067111968994
    },
    {
      "epoch": 0.27100271002710025,
      "step": 1250,
      "training_loss": 6.024219989776611
    },
    {
      "epoch": 0.27100271002710025,
      "step": 1250,
      "training_loss": 7.0809407234191895
    },
    {
      "epoch": 0.27100271002710025,
      "step": 1250,
      "training_loss": 7.439990520477295
    },
    {
      "epoch": 0.27121951219512197,
      "step": 1251,
      "training_loss": 6.6538496017456055
    },
    {
      "epoch": 0.27121951219512197,
      "step": 1251,
      "training_loss": 7.156719207763672
    },
    {
      "epoch": 0.27121951219512197,
      "step": 1251,
      "training_loss": 4.334589958190918
    },
    {
      "epoch": 0.27121951219512197,
      "step": 1251,
      "training_loss": 6.612874507904053
    },
    {
      "epoch": 0.27143631436314364,
      "grad_norm": 12.545469284057617,
      "learning_rate": 1e-05,
      "loss": 6.5387,
      "step": 1252
    },
    {
      "epoch": 0.27143631436314364,
      "step": 1252,
      "training_loss": 6.803836345672607
    },
    {
      "epoch": 0.27143631436314364,
      "step": 1252,
      "training_loss": 7.715218544006348
    },
    {
      "epoch": 0.27143631436314364,
      "step": 1252,
      "training_loss": 7.55362606048584
    },
    {
      "epoch": 0.27143631436314364,
      "step": 1252,
      "training_loss": 6.14442777633667
    },
    {
      "epoch": 0.2716531165311653,
      "step": 1253,
      "training_loss": 5.824310779571533
    },
    {
      "epoch": 0.2716531165311653,
      "step": 1253,
      "training_loss": 7.465627670288086
    },
    {
      "epoch": 0.2716531165311653,
      "step": 1253,
      "training_loss": 6.858346462249756
    },
    {
      "epoch": 0.2716531165311653,
      "step": 1253,
      "training_loss": 7.882258892059326
    },
    {
      "epoch": 0.271869918699187,
      "step": 1254,
      "training_loss": 7.410949230194092
    },
    {
      "epoch": 0.271869918699187,
      "step": 1254,
      "training_loss": 7.778713226318359
    },
    {
      "epoch": 0.271869918699187,
      "step": 1254,
      "training_loss": 6.73942232131958
    },
    {
      "epoch": 0.271869918699187,
      "step": 1254,
      "training_loss": 6.114382266998291
    },
    {
      "epoch": 0.2720867208672087,
      "step": 1255,
      "training_loss": 7.496072769165039
    },
    {
      "epoch": 0.2720867208672087,
      "step": 1255,
      "training_loss": 8.036945343017578
    },
    {
      "epoch": 0.2720867208672087,
      "step": 1255,
      "training_loss": 6.101250171661377
    },
    {
      "epoch": 0.2720867208672087,
      "step": 1255,
      "training_loss": 5.79591178894043
    },
    {
      "epoch": 0.27230352303523037,
      "grad_norm": 12.710761070251465,
      "learning_rate": 1e-05,
      "loss": 6.9826,
      "step": 1256
    },
    {
      "epoch": 0.27230352303523037,
      "step": 1256,
      "training_loss": 6.3155107498168945
    },
    {
      "epoch": 0.27230352303523037,
      "step": 1256,
      "training_loss": 7.140223979949951
    },
    {
      "epoch": 0.27230352303523037,
      "step": 1256,
      "training_loss": 6.563331127166748
    },
    {
      "epoch": 0.27230352303523037,
      "step": 1256,
      "training_loss": 7.720266342163086
    },
    {
      "epoch": 0.27252032520325203,
      "step": 1257,
      "training_loss": 7.688114643096924
    },
    {
      "epoch": 0.27252032520325203,
      "step": 1257,
      "training_loss": 6.5444793701171875
    },
    {
      "epoch": 0.27252032520325203,
      "step": 1257,
      "training_loss": 6.198914051055908
    },
    {
      "epoch": 0.27252032520325203,
      "step": 1257,
      "training_loss": 6.990573406219482
    },
    {
      "epoch": 0.2727371273712737,
      "step": 1258,
      "training_loss": 8.356542587280273
    },
    {
      "epoch": 0.2727371273712737,
      "step": 1258,
      "training_loss": 6.370977401733398
    },
    {
      "epoch": 0.2727371273712737,
      "step": 1258,
      "training_loss": 7.433780193328857
    },
    {
      "epoch": 0.2727371273712737,
      "step": 1258,
      "training_loss": 6.287263870239258
    },
    {
      "epoch": 0.27295392953929537,
      "step": 1259,
      "training_loss": 7.311290264129639
    },
    {
      "epoch": 0.27295392953929537,
      "step": 1259,
      "training_loss": 6.963810920715332
    },
    {
      "epoch": 0.27295392953929537,
      "step": 1259,
      "training_loss": 7.832189083099365
    },
    {
      "epoch": 0.27295392953929537,
      "step": 1259,
      "training_loss": 6.73722505569458
    },
    {
      "epoch": 0.2731707317073171,
      "grad_norm": 13.264354705810547,
      "learning_rate": 1e-05,
      "loss": 7.0284,
      "step": 1260
    },
    {
      "epoch": 0.2731707317073171,
      "step": 1260,
      "training_loss": 6.913094520568848
    },
    {
      "epoch": 0.2731707317073171,
      "step": 1260,
      "training_loss": 6.784658908843994
    },
    {
      "epoch": 0.2731707317073171,
      "step": 1260,
      "training_loss": 4.4845194816589355
    },
    {
      "epoch": 0.2731707317073171,
      "step": 1260,
      "training_loss": 6.6337890625
    },
    {
      "epoch": 0.27338753387533876,
      "step": 1261,
      "training_loss": 7.600133895874023
    },
    {
      "epoch": 0.27338753387533876,
      "step": 1261,
      "training_loss": 5.285205841064453
    },
    {
      "epoch": 0.27338753387533876,
      "step": 1261,
      "training_loss": 6.997411251068115
    },
    {
      "epoch": 0.27338753387533876,
      "step": 1261,
      "training_loss": 7.422525405883789
    },
    {
      "epoch": 0.27360433604336043,
      "step": 1262,
      "training_loss": 6.767841339111328
    },
    {
      "epoch": 0.27360433604336043,
      "step": 1262,
      "training_loss": 6.450355052947998
    },
    {
      "epoch": 0.27360433604336043,
      "step": 1262,
      "training_loss": 7.610127925872803
    },
    {
      "epoch": 0.27360433604336043,
      "step": 1262,
      "training_loss": 5.935582160949707
    },
    {
      "epoch": 0.2738211382113821,
      "step": 1263,
      "training_loss": 3.6849172115325928
    },
    {
      "epoch": 0.2738211382113821,
      "step": 1263,
      "training_loss": 6.376523017883301
    },
    {
      "epoch": 0.2738211382113821,
      "step": 1263,
      "training_loss": 6.1759138107299805
    },
    {
      "epoch": 0.2738211382113821,
      "step": 1263,
      "training_loss": 7.216353893280029
    },
    {
      "epoch": 0.2740379403794038,
      "grad_norm": 11.018821716308594,
      "learning_rate": 1e-05,
      "loss": 6.3962,
      "step": 1264
    },
    {
      "epoch": 0.2740379403794038,
      "step": 1264,
      "training_loss": 4.8451008796691895
    },
    {
      "epoch": 0.2740379403794038,
      "step": 1264,
      "training_loss": 6.810481548309326
    },
    {
      "epoch": 0.2740379403794038,
      "step": 1264,
      "training_loss": 6.702264308929443
    },
    {
      "epoch": 0.2740379403794038,
      "step": 1264,
      "training_loss": 6.6773247718811035
    },
    {
      "epoch": 0.2742547425474255,
      "step": 1265,
      "training_loss": 5.954233646392822
    },
    {
      "epoch": 0.2742547425474255,
      "step": 1265,
      "training_loss": 6.733474254608154
    },
    {
      "epoch": 0.2742547425474255,
      "step": 1265,
      "training_loss": 6.364413738250732
    },
    {
      "epoch": 0.2742547425474255,
      "step": 1265,
      "training_loss": 6.757667541503906
    },
    {
      "epoch": 0.27447154471544716,
      "step": 1266,
      "training_loss": 4.385682582855225
    },
    {
      "epoch": 0.27447154471544716,
      "step": 1266,
      "training_loss": 7.063321113586426
    },
    {
      "epoch": 0.27447154471544716,
      "step": 1266,
      "training_loss": 8.39970588684082
    },
    {
      "epoch": 0.27447154471544716,
      "step": 1266,
      "training_loss": 7.806135177612305
    },
    {
      "epoch": 0.2746883468834688,
      "step": 1267,
      "training_loss": 5.3560967445373535
    },
    {
      "epoch": 0.2746883468834688,
      "step": 1267,
      "training_loss": 5.975706100463867
    },
    {
      "epoch": 0.2746883468834688,
      "step": 1267,
      "training_loss": 7.193729400634766
    },
    {
      "epoch": 0.2746883468834688,
      "step": 1267,
      "training_loss": 6.921851634979248
    },
    {
      "epoch": 0.2749051490514905,
      "grad_norm": 12.487045288085938,
      "learning_rate": 1e-05,
      "loss": 6.4967,
      "step": 1268
    },
    {
      "epoch": 0.2749051490514905,
      "step": 1268,
      "training_loss": 6.679834365844727
    },
    {
      "epoch": 0.2749051490514905,
      "step": 1268,
      "training_loss": 7.957015037536621
    },
    {
      "epoch": 0.2749051490514905,
      "step": 1268,
      "training_loss": 6.891253471374512
    },
    {
      "epoch": 0.2749051490514905,
      "step": 1268,
      "training_loss": 6.236278533935547
    },
    {
      "epoch": 0.2751219512195122,
      "step": 1269,
      "training_loss": 6.649633407592773
    },
    {
      "epoch": 0.2751219512195122,
      "step": 1269,
      "training_loss": 7.999024391174316
    },
    {
      "epoch": 0.2751219512195122,
      "step": 1269,
      "training_loss": 5.330374717712402
    },
    {
      "epoch": 0.2751219512195122,
      "step": 1269,
      "training_loss": 5.989555358886719
    },
    {
      "epoch": 0.2753387533875339,
      "step": 1270,
      "training_loss": 7.411678314208984
    },
    {
      "epoch": 0.2753387533875339,
      "step": 1270,
      "training_loss": 5.217106342315674
    },
    {
      "epoch": 0.2753387533875339,
      "step": 1270,
      "training_loss": 7.38040018081665
    },
    {
      "epoch": 0.2753387533875339,
      "step": 1270,
      "training_loss": 4.962962627410889
    },
    {
      "epoch": 0.27555555555555555,
      "step": 1271,
      "training_loss": 7.327955722808838
    },
    {
      "epoch": 0.27555555555555555,
      "step": 1271,
      "training_loss": 6.0134148597717285
    },
    {
      "epoch": 0.27555555555555555,
      "step": 1271,
      "training_loss": 8.717658996582031
    },
    {
      "epoch": 0.27555555555555555,
      "step": 1271,
      "training_loss": 6.498566627502441
    },
    {
      "epoch": 0.2757723577235772,
      "grad_norm": 14.610479354858398,
      "learning_rate": 1e-05,
      "loss": 6.7039,
      "step": 1272
    },
    {
      "epoch": 0.2757723577235772,
      "step": 1272,
      "training_loss": 7.159061431884766
    },
    {
      "epoch": 0.2757723577235772,
      "step": 1272,
      "training_loss": 6.938431739807129
    },
    {
      "epoch": 0.2757723577235772,
      "step": 1272,
      "training_loss": 4.104351043701172
    },
    {
      "epoch": 0.2757723577235772,
      "step": 1272,
      "training_loss": 7.3705153465271
    },
    {
      "epoch": 0.2759891598915989,
      "step": 1273,
      "training_loss": 7.528500080108643
    },
    {
      "epoch": 0.2759891598915989,
      "step": 1273,
      "training_loss": 5.522803783416748
    },
    {
      "epoch": 0.2759891598915989,
      "step": 1273,
      "training_loss": 5.967084884643555
    },
    {
      "epoch": 0.2759891598915989,
      "step": 1273,
      "training_loss": 6.411779880523682
    },
    {
      "epoch": 0.2762059620596206,
      "step": 1274,
      "training_loss": 7.105755805969238
    },
    {
      "epoch": 0.2762059620596206,
      "step": 1274,
      "training_loss": 6.8590779304504395
    },
    {
      "epoch": 0.2762059620596206,
      "step": 1274,
      "training_loss": 5.931763648986816
    },
    {
      "epoch": 0.2762059620596206,
      "step": 1274,
      "training_loss": 6.526415824890137
    },
    {
      "epoch": 0.2764227642276423,
      "step": 1275,
      "training_loss": 7.332970142364502
    },
    {
      "epoch": 0.2764227642276423,
      "step": 1275,
      "training_loss": 6.926102161407471
    },
    {
      "epoch": 0.2764227642276423,
      "step": 1275,
      "training_loss": 6.771237850189209
    },
    {
      "epoch": 0.2764227642276423,
      "step": 1275,
      "training_loss": 6.917026519775391
    },
    {
      "epoch": 0.27663956639566395,
      "grad_norm": 10.285144805908203,
      "learning_rate": 1e-05,
      "loss": 6.5858,
      "step": 1276
    },
    {
      "epoch": 0.27663956639566395,
      "step": 1276,
      "training_loss": 7.7801361083984375
    },
    {
      "epoch": 0.27663956639566395,
      "step": 1276,
      "training_loss": 7.241379737854004
    },
    {
      "epoch": 0.27663956639566395,
      "step": 1276,
      "training_loss": 6.563289642333984
    },
    {
      "epoch": 0.27663956639566395,
      "step": 1276,
      "training_loss": 6.540427207946777
    },
    {
      "epoch": 0.2768563685636856,
      "step": 1277,
      "training_loss": 6.554546356201172
    },
    {
      "epoch": 0.2768563685636856,
      "step": 1277,
      "training_loss": 4.462031841278076
    },
    {
      "epoch": 0.2768563685636856,
      "step": 1277,
      "training_loss": 7.630399227142334
    },
    {
      "epoch": 0.2768563685636856,
      "step": 1277,
      "training_loss": 7.78497838973999
    },
    {
      "epoch": 0.27707317073170734,
      "step": 1278,
      "training_loss": 7.2824482917785645
    },
    {
      "epoch": 0.27707317073170734,
      "step": 1278,
      "training_loss": 7.179204940795898
    },
    {
      "epoch": 0.27707317073170734,
      "step": 1278,
      "training_loss": 6.607677459716797
    },
    {
      "epoch": 0.27707317073170734,
      "step": 1278,
      "training_loss": 5.450827598571777
    },
    {
      "epoch": 0.277289972899729,
      "step": 1279,
      "training_loss": 6.470785140991211
    },
    {
      "epoch": 0.277289972899729,
      "step": 1279,
      "training_loss": 4.275631427764893
    },
    {
      "epoch": 0.277289972899729,
      "step": 1279,
      "training_loss": 6.308078289031982
    },
    {
      "epoch": 0.277289972899729,
      "step": 1279,
      "training_loss": 5.557147979736328
    },
    {
      "epoch": 0.2775067750677507,
      "grad_norm": 13.287923812866211,
      "learning_rate": 1e-05,
      "loss": 6.4806,
      "step": 1280
    },
    {
      "epoch": 0.2775067750677507,
      "step": 1280,
      "training_loss": 5.849812030792236
    },
    {
      "epoch": 0.2775067750677507,
      "step": 1280,
      "training_loss": 7.31864070892334
    },
    {
      "epoch": 0.2775067750677507,
      "step": 1280,
      "training_loss": 6.343311786651611
    },
    {
      "epoch": 0.2775067750677507,
      "step": 1280,
      "training_loss": 7.688888072967529
    },
    {
      "epoch": 0.27772357723577235,
      "step": 1281,
      "training_loss": 7.080204010009766
    },
    {
      "epoch": 0.27772357723577235,
      "step": 1281,
      "training_loss": 6.875766277313232
    },
    {
      "epoch": 0.27772357723577235,
      "step": 1281,
      "training_loss": 5.933875560760498
    },
    {
      "epoch": 0.27772357723577235,
      "step": 1281,
      "training_loss": 7.142638206481934
    },
    {
      "epoch": 0.277940379403794,
      "step": 1282,
      "training_loss": 6.085839748382568
    },
    {
      "epoch": 0.277940379403794,
      "step": 1282,
      "training_loss": 6.84250545501709
    },
    {
      "epoch": 0.277940379403794,
      "step": 1282,
      "training_loss": 7.118863105773926
    },
    {
      "epoch": 0.277940379403794,
      "step": 1282,
      "training_loss": 4.980191707611084
    },
    {
      "epoch": 0.27815718157181574,
      "step": 1283,
      "training_loss": 6.817904472351074
    },
    {
      "epoch": 0.27815718157181574,
      "step": 1283,
      "training_loss": 4.585943698883057
    },
    {
      "epoch": 0.27815718157181574,
      "step": 1283,
      "training_loss": 5.95550537109375
    },
    {
      "epoch": 0.27815718157181574,
      "step": 1283,
      "training_loss": 7.9481024742126465
    },
    {
      "epoch": 0.2783739837398374,
      "grad_norm": 10.83644962310791,
      "learning_rate": 1e-05,
      "loss": 6.5355,
      "step": 1284
    },
    {
      "epoch": 0.2783739837398374,
      "step": 1284,
      "training_loss": 7.532538890838623
    },
    {
      "epoch": 0.2783739837398374,
      "step": 1284,
      "training_loss": 5.362637996673584
    },
    {
      "epoch": 0.2783739837398374,
      "step": 1284,
      "training_loss": 6.685410022735596
    },
    {
      "epoch": 0.2783739837398374,
      "step": 1284,
      "training_loss": 7.520078182220459
    },
    {
      "epoch": 0.2785907859078591,
      "step": 1285,
      "training_loss": 6.729510307312012
    },
    {
      "epoch": 0.2785907859078591,
      "step": 1285,
      "training_loss": 6.63940954208374
    },
    {
      "epoch": 0.2785907859078591,
      "step": 1285,
      "training_loss": 7.454707622528076
    },
    {
      "epoch": 0.2785907859078591,
      "step": 1285,
      "training_loss": 7.402034282684326
    },
    {
      "epoch": 0.27880758807588074,
      "step": 1286,
      "training_loss": 6.961462497711182
    },
    {
      "epoch": 0.27880758807588074,
      "step": 1286,
      "training_loss": 7.433558464050293
    },
    {
      "epoch": 0.27880758807588074,
      "step": 1286,
      "training_loss": 6.528378963470459
    },
    {
      "epoch": 0.27880758807588074,
      "step": 1286,
      "training_loss": 6.272356986999512
    },
    {
      "epoch": 0.27902439024390246,
      "step": 1287,
      "training_loss": 6.312164783477783
    },
    {
      "epoch": 0.27902439024390246,
      "step": 1287,
      "training_loss": 7.691428184509277
    },
    {
      "epoch": 0.27902439024390246,
      "step": 1287,
      "training_loss": 7.295969009399414
    },
    {
      "epoch": 0.27902439024390246,
      "step": 1287,
      "training_loss": 7.012453079223633
    },
    {
      "epoch": 0.27924119241192413,
      "grad_norm": 13.215592384338379,
      "learning_rate": 1e-05,
      "loss": 6.9271,
      "step": 1288
    },
    {
      "epoch": 0.27924119241192413,
      "step": 1288,
      "training_loss": 6.225880146026611
    },
    {
      "epoch": 0.27924119241192413,
      "step": 1288,
      "training_loss": 7.020412445068359
    },
    {
      "epoch": 0.27924119241192413,
      "step": 1288,
      "training_loss": 6.89329195022583
    },
    {
      "epoch": 0.27924119241192413,
      "step": 1288,
      "training_loss": 5.961359024047852
    },
    {
      "epoch": 0.2794579945799458,
      "step": 1289,
      "training_loss": 7.71359395980835
    },
    {
      "epoch": 0.2794579945799458,
      "step": 1289,
      "training_loss": 6.539363861083984
    },
    {
      "epoch": 0.2794579945799458,
      "step": 1289,
      "training_loss": 5.992184162139893
    },
    {
      "epoch": 0.2794579945799458,
      "step": 1289,
      "training_loss": 5.481046676635742
    },
    {
      "epoch": 0.27967479674796747,
      "step": 1290,
      "training_loss": 7.577216148376465
    },
    {
      "epoch": 0.27967479674796747,
      "step": 1290,
      "training_loss": 7.32822322845459
    },
    {
      "epoch": 0.27967479674796747,
      "step": 1290,
      "training_loss": 6.547091960906982
    },
    {
      "epoch": 0.27967479674796747,
      "step": 1290,
      "training_loss": 4.127869606018066
    },
    {
      "epoch": 0.27989159891598914,
      "step": 1291,
      "training_loss": 7.164432525634766
    },
    {
      "epoch": 0.27989159891598914,
      "step": 1291,
      "training_loss": 7.5483527183532715
    },
    {
      "epoch": 0.27989159891598914,
      "step": 1291,
      "training_loss": 6.275447368621826
    },
    {
      "epoch": 0.27989159891598914,
      "step": 1291,
      "training_loss": 7.641259670257568
    },
    {
      "epoch": 0.28010840108401086,
      "grad_norm": 12.771008491516113,
      "learning_rate": 1e-05,
      "loss": 6.6273,
      "step": 1292
    },
    {
      "epoch": 0.28010840108401086,
      "step": 1292,
      "training_loss": 5.889508247375488
    },
    {
      "epoch": 0.28010840108401086,
      "step": 1292,
      "training_loss": 6.310127258300781
    },
    {
      "epoch": 0.28010840108401086,
      "step": 1292,
      "training_loss": 6.32009744644165
    },
    {
      "epoch": 0.28010840108401086,
      "step": 1292,
      "training_loss": 4.761760234832764
    },
    {
      "epoch": 0.28032520325203253,
      "step": 1293,
      "training_loss": 7.661700248718262
    },
    {
      "epoch": 0.28032520325203253,
      "step": 1293,
      "training_loss": 8.369171142578125
    },
    {
      "epoch": 0.28032520325203253,
      "step": 1293,
      "training_loss": 5.837307453155518
    },
    {
      "epoch": 0.28032520325203253,
      "step": 1293,
      "training_loss": 6.595383167266846
    },
    {
      "epoch": 0.2805420054200542,
      "step": 1294,
      "training_loss": 7.742365837097168
    },
    {
      "epoch": 0.2805420054200542,
      "step": 1294,
      "training_loss": 5.538017749786377
    },
    {
      "epoch": 0.2805420054200542,
      "step": 1294,
      "training_loss": 6.7292070388793945
    },
    {
      "epoch": 0.2805420054200542,
      "step": 1294,
      "training_loss": 5.573309898376465
    },
    {
      "epoch": 0.28075880758807586,
      "step": 1295,
      "training_loss": 4.880760669708252
    },
    {
      "epoch": 0.28075880758807586,
      "step": 1295,
      "training_loss": 6.391879558563232
    },
    {
      "epoch": 0.28075880758807586,
      "step": 1295,
      "training_loss": 7.527067184448242
    },
    {
      "epoch": 0.28075880758807586,
      "step": 1295,
      "training_loss": 7.490439414978027
    },
    {
      "epoch": 0.2809756097560976,
      "grad_norm": 15.907304763793945,
      "learning_rate": 1e-05,
      "loss": 6.4761,
      "step": 1296
    },
    {
      "epoch": 0.2809756097560976,
      "step": 1296,
      "training_loss": 5.537409782409668
    },
    {
      "epoch": 0.2809756097560976,
      "step": 1296,
      "training_loss": 8.031411170959473
    },
    {
      "epoch": 0.2809756097560976,
      "step": 1296,
      "training_loss": 7.83876895904541
    },
    {
      "epoch": 0.2809756097560976,
      "step": 1296,
      "training_loss": 6.5710368156433105
    },
    {
      "epoch": 0.28119241192411926,
      "step": 1297,
      "training_loss": 6.964560508728027
    },
    {
      "epoch": 0.28119241192411926,
      "step": 1297,
      "training_loss": 7.619194507598877
    },
    {
      "epoch": 0.28119241192411926,
      "step": 1297,
      "training_loss": 8.741976737976074
    },
    {
      "epoch": 0.28119241192411926,
      "step": 1297,
      "training_loss": 6.626954078674316
    },
    {
      "epoch": 0.2814092140921409,
      "step": 1298,
      "training_loss": 6.219152450561523
    },
    {
      "epoch": 0.2814092140921409,
      "step": 1298,
      "training_loss": 6.248525142669678
    },
    {
      "epoch": 0.2814092140921409,
      "step": 1298,
      "training_loss": 5.930513381958008
    },
    {
      "epoch": 0.2814092140921409,
      "step": 1298,
      "training_loss": 6.893763065338135
    },
    {
      "epoch": 0.2816260162601626,
      "step": 1299,
      "training_loss": 4.492568016052246
    },
    {
      "epoch": 0.2816260162601626,
      "step": 1299,
      "training_loss": 6.856692314147949
    },
    {
      "epoch": 0.2816260162601626,
      "step": 1299,
      "training_loss": 4.212995529174805
    },
    {
      "epoch": 0.2816260162601626,
      "step": 1299,
      "training_loss": 7.260330677032471
    },
    {
      "epoch": 0.28184281842818426,
      "grad_norm": 10.069007873535156,
      "learning_rate": 1e-05,
      "loss": 6.6279,
      "step": 1300
    },
    {
      "epoch": 0.28184281842818426,
      "step": 1300,
      "training_loss": 6.810020923614502
    },
    {
      "epoch": 0.28184281842818426,
      "step": 1300,
      "training_loss": 6.287102222442627
    },
    {
      "epoch": 0.28184281842818426,
      "step": 1300,
      "training_loss": 7.4291768074035645
    },
    {
      "epoch": 0.28184281842818426,
      "step": 1300,
      "training_loss": 4.5124382972717285
    },
    {
      "epoch": 0.282059620596206,
      "step": 1301,
      "training_loss": 6.601881504058838
    },
    {
      "epoch": 0.282059620596206,
      "step": 1301,
      "training_loss": 6.641538143157959
    },
    {
      "epoch": 0.282059620596206,
      "step": 1301,
      "training_loss": 7.555900573730469
    },
    {
      "epoch": 0.282059620596206,
      "step": 1301,
      "training_loss": 6.941967010498047
    },
    {
      "epoch": 0.28227642276422765,
      "step": 1302,
      "training_loss": 7.77180290222168
    },
    {
      "epoch": 0.28227642276422765,
      "step": 1302,
      "training_loss": 7.82992696762085
    },
    {
      "epoch": 0.28227642276422765,
      "step": 1302,
      "training_loss": 7.090352535247803
    },
    {
      "epoch": 0.28227642276422765,
      "step": 1302,
      "training_loss": 6.244860649108887
    },
    {
      "epoch": 0.2824932249322493,
      "step": 1303,
      "training_loss": 6.649808883666992
    },
    {
      "epoch": 0.2824932249322493,
      "step": 1303,
      "training_loss": 6.5104756355285645
    },
    {
      "epoch": 0.2824932249322493,
      "step": 1303,
      "training_loss": 8.048812866210938
    },
    {
      "epoch": 0.2824932249322493,
      "step": 1303,
      "training_loss": 6.0672831535339355
    },
    {
      "epoch": 0.282710027100271,
      "grad_norm": 12.914384841918945,
      "learning_rate": 1e-05,
      "loss": 6.8121,
      "step": 1304
    },
    {
      "epoch": 0.282710027100271,
      "step": 1304,
      "training_loss": 6.860449314117432
    },
    {
      "epoch": 0.282710027100271,
      "step": 1304,
      "training_loss": 6.489392280578613
    },
    {
      "epoch": 0.282710027100271,
      "step": 1304,
      "training_loss": 7.143007278442383
    },
    {
      "epoch": 0.282710027100271,
      "step": 1304,
      "training_loss": 7.393828392028809
    },
    {
      "epoch": 0.28292682926829266,
      "step": 1305,
      "training_loss": 7.495584011077881
    },
    {
      "epoch": 0.28292682926829266,
      "step": 1305,
      "training_loss": 6.816179275512695
    },
    {
      "epoch": 0.28292682926829266,
      "step": 1305,
      "training_loss": 7.430291175842285
    },
    {
      "epoch": 0.28292682926829266,
      "step": 1305,
      "training_loss": 6.689067840576172
    },
    {
      "epoch": 0.2831436314363144,
      "step": 1306,
      "training_loss": 7.1120405197143555
    },
    {
      "epoch": 0.2831436314363144,
      "step": 1306,
      "training_loss": 6.709163665771484
    },
    {
      "epoch": 0.2831436314363144,
      "step": 1306,
      "training_loss": 7.87064266204834
    },
    {
      "epoch": 0.2831436314363144,
      "step": 1306,
      "training_loss": 8.052338600158691
    },
    {
      "epoch": 0.28336043360433605,
      "step": 1307,
      "training_loss": 5.898859024047852
    },
    {
      "epoch": 0.28336043360433605,
      "step": 1307,
      "training_loss": 7.5115742683410645
    },
    {
      "epoch": 0.28336043360433605,
      "step": 1307,
      "training_loss": 7.419686317443848
    },
    {
      "epoch": 0.28336043360433605,
      "step": 1307,
      "training_loss": 6.865229606628418
    },
    {
      "epoch": 0.2835772357723577,
      "grad_norm": 12.744242668151855,
      "learning_rate": 1e-05,
      "loss": 7.1098,
      "step": 1308
    },
    {
      "epoch": 0.2835772357723577,
      "step": 1308,
      "training_loss": 7.084264278411865
    },
    {
      "epoch": 0.2835772357723577,
      "step": 1308,
      "training_loss": 6.619237422943115
    },
    {
      "epoch": 0.2835772357723577,
      "step": 1308,
      "training_loss": 6.681921005249023
    },
    {
      "epoch": 0.2835772357723577,
      "step": 1308,
      "training_loss": 7.3623833656311035
    },
    {
      "epoch": 0.2837940379403794,
      "step": 1309,
      "training_loss": 7.801114082336426
    },
    {
      "epoch": 0.2837940379403794,
      "step": 1309,
      "training_loss": 5.978297233581543
    },
    {
      "epoch": 0.2837940379403794,
      "step": 1309,
      "training_loss": 7.6631975173950195
    },
    {
      "epoch": 0.2837940379403794,
      "step": 1309,
      "training_loss": 6.438363552093506
    },
    {
      "epoch": 0.2840108401084011,
      "step": 1310,
      "training_loss": 6.381532192230225
    },
    {
      "epoch": 0.2840108401084011,
      "step": 1310,
      "training_loss": 6.854647159576416
    },
    {
      "epoch": 0.2840108401084011,
      "step": 1310,
      "training_loss": 7.294785022735596
    },
    {
      "epoch": 0.2840108401084011,
      "step": 1310,
      "training_loss": 7.431067943572998
    },
    {
      "epoch": 0.2842276422764228,
      "step": 1311,
      "training_loss": 6.64064884185791
    },
    {
      "epoch": 0.2842276422764228,
      "step": 1311,
      "training_loss": 7.625575542449951
    },
    {
      "epoch": 0.2842276422764228,
      "step": 1311,
      "training_loss": 7.586635589599609
    },
    {
      "epoch": 0.2842276422764228,
      "step": 1311,
      "training_loss": 6.59635066986084
    },
    {
      "epoch": 0.28444444444444444,
      "grad_norm": 13.343762397766113,
      "learning_rate": 1e-05,
      "loss": 7.0025,
      "step": 1312
    },
    {
      "epoch": 0.28444444444444444,
      "step": 1312,
      "training_loss": 7.176857948303223
    },
    {
      "epoch": 0.28444444444444444,
      "step": 1312,
      "training_loss": 6.841585636138916
    },
    {
      "epoch": 0.28444444444444444,
      "step": 1312,
      "training_loss": 7.251543045043945
    },
    {
      "epoch": 0.28444444444444444,
      "step": 1312,
      "training_loss": 5.256594181060791
    },
    {
      "epoch": 0.2846612466124661,
      "step": 1313,
      "training_loss": 7.273621559143066
    },
    {
      "epoch": 0.2846612466124661,
      "step": 1313,
      "training_loss": 6.934322834014893
    },
    {
      "epoch": 0.2846612466124661,
      "step": 1313,
      "training_loss": 7.602904796600342
    },
    {
      "epoch": 0.2846612466124661,
      "step": 1313,
      "training_loss": 4.049524307250977
    },
    {
      "epoch": 0.2848780487804878,
      "step": 1314,
      "training_loss": 6.790857315063477
    },
    {
      "epoch": 0.2848780487804878,
      "step": 1314,
      "training_loss": 5.231410503387451
    },
    {
      "epoch": 0.2848780487804878,
      "step": 1314,
      "training_loss": 6.421335697174072
    },
    {
      "epoch": 0.2848780487804878,
      "step": 1314,
      "training_loss": 7.017825603485107
    },
    {
      "epoch": 0.2850948509485095,
      "step": 1315,
      "training_loss": 4.5132951736450195
    },
    {
      "epoch": 0.2850948509485095,
      "step": 1315,
      "training_loss": 6.415019989013672
    },
    {
      "epoch": 0.2850948509485095,
      "step": 1315,
      "training_loss": 6.467182159423828
    },
    {
      "epoch": 0.2850948509485095,
      "step": 1315,
      "training_loss": 6.840564727783203
    },
    {
      "epoch": 0.28531165311653117,
      "grad_norm": 10.910167694091797,
      "learning_rate": 1e-05,
      "loss": 6.3803,
      "step": 1316
    },
    {
      "epoch": 0.28531165311653117,
      "step": 1316,
      "training_loss": 6.710961818695068
    },
    {
      "epoch": 0.28531165311653117,
      "step": 1316,
      "training_loss": 6.013721942901611
    },
    {
      "epoch": 0.28531165311653117,
      "step": 1316,
      "training_loss": 5.7275919914245605
    },
    {
      "epoch": 0.28531165311653117,
      "step": 1316,
      "training_loss": 7.062868118286133
    },
    {
      "epoch": 0.28552845528455284,
      "step": 1317,
      "training_loss": 7.249147891998291
    },
    {
      "epoch": 0.28552845528455284,
      "step": 1317,
      "training_loss": 7.168698787689209
    },
    {
      "epoch": 0.28552845528455284,
      "step": 1317,
      "training_loss": 7.543551921844482
    },
    {
      "epoch": 0.28552845528455284,
      "step": 1317,
      "training_loss": 6.060503005981445
    },
    {
      "epoch": 0.2857452574525745,
      "step": 1318,
      "training_loss": 7.304805278778076
    },
    {
      "epoch": 0.2857452574525745,
      "step": 1318,
      "training_loss": 7.347201824188232
    },
    {
      "epoch": 0.2857452574525745,
      "step": 1318,
      "training_loss": 6.7665252685546875
    },
    {
      "epoch": 0.2857452574525745,
      "step": 1318,
      "training_loss": 8.126893997192383
    },
    {
      "epoch": 0.28596205962059623,
      "step": 1319,
      "training_loss": 8.67489242553711
    },
    {
      "epoch": 0.28596205962059623,
      "step": 1319,
      "training_loss": 5.647912979125977
    },
    {
      "epoch": 0.28596205962059623,
      "step": 1319,
      "training_loss": 6.531076908111572
    },
    {
      "epoch": 0.28596205962059623,
      "step": 1319,
      "training_loss": 7.209208965301514
    },
    {
      "epoch": 0.2861788617886179,
      "grad_norm": 17.766172409057617,
      "learning_rate": 1e-05,
      "loss": 6.9466,
      "step": 1320
    },
    {
      "epoch": 0.2861788617886179,
      "step": 1320,
      "training_loss": 7.052462100982666
    },
    {
      "epoch": 0.2861788617886179,
      "step": 1320,
      "training_loss": 7.583687782287598
    },
    {
      "epoch": 0.2861788617886179,
      "step": 1320,
      "training_loss": 5.466414928436279
    },
    {
      "epoch": 0.2861788617886179,
      "step": 1320,
      "training_loss": 7.309842586517334
    },
    {
      "epoch": 0.28639566395663957,
      "step": 1321,
      "training_loss": 6.6845269203186035
    },
    {
      "epoch": 0.28639566395663957,
      "step": 1321,
      "training_loss": 6.915716648101807
    },
    {
      "epoch": 0.28639566395663957,
      "step": 1321,
      "training_loss": 6.828310012817383
    },
    {
      "epoch": 0.28639566395663957,
      "step": 1321,
      "training_loss": 5.900152206420898
    },
    {
      "epoch": 0.28661246612466124,
      "step": 1322,
      "training_loss": 7.000010967254639
    },
    {
      "epoch": 0.28661246612466124,
      "step": 1322,
      "training_loss": 7.063095569610596
    },
    {
      "epoch": 0.28661246612466124,
      "step": 1322,
      "training_loss": 9.290716171264648
    },
    {
      "epoch": 0.28661246612466124,
      "step": 1322,
      "training_loss": 6.113304615020752
    },
    {
      "epoch": 0.2868292682926829,
      "step": 1323,
      "training_loss": 5.761320114135742
    },
    {
      "epoch": 0.2868292682926829,
      "step": 1323,
      "training_loss": 7.495623588562012
    },
    {
      "epoch": 0.2868292682926829,
      "step": 1323,
      "training_loss": 5.320895195007324
    },
    {
      "epoch": 0.2868292682926829,
      "step": 1323,
      "training_loss": 9.803515434265137
    },
    {
      "epoch": 0.2870460704607046,
      "grad_norm": 19.252111434936523,
      "learning_rate": 1e-05,
      "loss": 6.9743,
      "step": 1324
    },
    {
      "epoch": 0.2870460704607046,
      "step": 1324,
      "training_loss": 3.7485694885253906
    },
    {
      "epoch": 0.2870460704607046,
      "step": 1324,
      "training_loss": 7.408312797546387
    },
    {
      "epoch": 0.2870460704607046,
      "step": 1324,
      "training_loss": 4.499955177307129
    },
    {
      "epoch": 0.2870460704607046,
      "step": 1324,
      "training_loss": 7.333242893218994
    },
    {
      "epoch": 0.2872628726287263,
      "step": 1325,
      "training_loss": 6.398241996765137
    },
    {
      "epoch": 0.2872628726287263,
      "step": 1325,
      "training_loss": 6.787384510040283
    },
    {
      "epoch": 0.2872628726287263,
      "step": 1325,
      "training_loss": 6.884654998779297
    },
    {
      "epoch": 0.2872628726287263,
      "step": 1325,
      "training_loss": 7.567488670349121
    },
    {
      "epoch": 0.28747967479674796,
      "step": 1326,
      "training_loss": 6.9464216232299805
    },
    {
      "epoch": 0.28747967479674796,
      "step": 1326,
      "training_loss": 4.963953971862793
    },
    {
      "epoch": 0.28747967479674796,
      "step": 1326,
      "training_loss": 7.643954277038574
    },
    {
      "epoch": 0.28747967479674796,
      "step": 1326,
      "training_loss": 6.479794979095459
    },
    {
      "epoch": 0.28769647696476963,
      "step": 1327,
      "training_loss": 7.6734232902526855
    },
    {
      "epoch": 0.28769647696476963,
      "step": 1327,
      "training_loss": 4.788295745849609
    },
    {
      "epoch": 0.28769647696476963,
      "step": 1327,
      "training_loss": 7.498501300811768
    },
    {
      "epoch": 0.28769647696476963,
      "step": 1327,
      "training_loss": 6.1728057861328125
    },
    {
      "epoch": 0.28791327913279136,
      "grad_norm": 15.898442268371582,
      "learning_rate": 1e-05,
      "loss": 6.4247,
      "step": 1328
    },
    {
      "epoch": 0.28791327913279136,
      "step": 1328,
      "training_loss": 8.158706665039062
    },
    {
      "epoch": 0.28791327913279136,
      "step": 1328,
      "training_loss": 6.375985622406006
    },
    {
      "epoch": 0.28791327913279136,
      "step": 1328,
      "training_loss": 6.917510986328125
    },
    {
      "epoch": 0.28791327913279136,
      "step": 1328,
      "training_loss": 7.286960124969482
    },
    {
      "epoch": 0.288130081300813,
      "step": 1329,
      "training_loss": 6.832374095916748
    },
    {
      "epoch": 0.288130081300813,
      "step": 1329,
      "training_loss": 6.322106838226318
    },
    {
      "epoch": 0.288130081300813,
      "step": 1329,
      "training_loss": 7.052063941955566
    },
    {
      "epoch": 0.288130081300813,
      "step": 1329,
      "training_loss": 7.206673622131348
    },
    {
      "epoch": 0.2883468834688347,
      "step": 1330,
      "training_loss": 7.0062479972839355
    },
    {
      "epoch": 0.2883468834688347,
      "step": 1330,
      "training_loss": 7.379530906677246
    },
    {
      "epoch": 0.2883468834688347,
      "step": 1330,
      "training_loss": 6.349975109100342
    },
    {
      "epoch": 0.2883468834688347,
      "step": 1330,
      "training_loss": 7.116785526275635
    },
    {
      "epoch": 0.28856368563685636,
      "step": 1331,
      "training_loss": 6.872553825378418
    },
    {
      "epoch": 0.28856368563685636,
      "step": 1331,
      "training_loss": 7.122921466827393
    },
    {
      "epoch": 0.28856368563685636,
      "step": 1331,
      "training_loss": 6.093643665313721
    },
    {
      "epoch": 0.28856368563685636,
      "step": 1331,
      "training_loss": 5.345187664031982
    },
    {
      "epoch": 0.288780487804878,
      "grad_norm": 12.652533531188965,
      "learning_rate": 1e-05,
      "loss": 6.84,
      "step": 1332
    },
    {
      "epoch": 0.288780487804878,
      "step": 1332,
      "training_loss": 7.913863658905029
    },
    {
      "epoch": 0.288780487804878,
      "step": 1332,
      "training_loss": 5.852348327636719
    },
    {
      "epoch": 0.288780487804878,
      "step": 1332,
      "training_loss": 7.133604049682617
    },
    {
      "epoch": 0.288780487804878,
      "step": 1332,
      "training_loss": 6.583501815795898
    },
    {
      "epoch": 0.28899728997289975,
      "step": 1333,
      "training_loss": 4.635079383850098
    },
    {
      "epoch": 0.28899728997289975,
      "step": 1333,
      "training_loss": 7.15163516998291
    },
    {
      "epoch": 0.28899728997289975,
      "step": 1333,
      "training_loss": 6.933563232421875
    },
    {
      "epoch": 0.28899728997289975,
      "step": 1333,
      "training_loss": 6.492157459259033
    },
    {
      "epoch": 0.2892140921409214,
      "step": 1334,
      "training_loss": 4.605567455291748
    },
    {
      "epoch": 0.2892140921409214,
      "step": 1334,
      "training_loss": 8.078642845153809
    },
    {
      "epoch": 0.2892140921409214,
      "step": 1334,
      "training_loss": 6.643608570098877
    },
    {
      "epoch": 0.2892140921409214,
      "step": 1334,
      "training_loss": 6.3281331062316895
    },
    {
      "epoch": 0.2894308943089431,
      "step": 1335,
      "training_loss": 6.398940086364746
    },
    {
      "epoch": 0.2894308943089431,
      "step": 1335,
      "training_loss": 7.002805233001709
    },
    {
      "epoch": 0.2894308943089431,
      "step": 1335,
      "training_loss": 6.2113165855407715
    },
    {
      "epoch": 0.2894308943089431,
      "step": 1335,
      "training_loss": 7.427282810211182
    },
    {
      "epoch": 0.28964769647696476,
      "grad_norm": 12.468378067016602,
      "learning_rate": 1e-05,
      "loss": 6.587,
      "step": 1336
    },
    {
      "epoch": 0.28964769647696476,
      "step": 1336,
      "training_loss": 7.218626976013184
    },
    {
      "epoch": 0.28964769647696476,
      "step": 1336,
      "training_loss": 5.854464530944824
    },
    {
      "epoch": 0.28964769647696476,
      "step": 1336,
      "training_loss": 6.468228340148926
    },
    {
      "epoch": 0.28964769647696476,
      "step": 1336,
      "training_loss": 6.336306571960449
    },
    {
      "epoch": 0.2898644986449864,
      "step": 1337,
      "training_loss": 5.646677494049072
    },
    {
      "epoch": 0.2898644986449864,
      "step": 1337,
      "training_loss": 7.740891933441162
    },
    {
      "epoch": 0.2898644986449864,
      "step": 1337,
      "training_loss": 6.389126777648926
    },
    {
      "epoch": 0.2898644986449864,
      "step": 1337,
      "training_loss": 6.332053184509277
    },
    {
      "epoch": 0.29008130081300815,
      "step": 1338,
      "training_loss": 5.205928325653076
    },
    {
      "epoch": 0.29008130081300815,
      "step": 1338,
      "training_loss": 6.989593505859375
    },
    {
      "epoch": 0.29008130081300815,
      "step": 1338,
      "training_loss": 4.442984104156494
    },
    {
      "epoch": 0.29008130081300815,
      "step": 1338,
      "training_loss": 6.606970310211182
    },
    {
      "epoch": 0.2902981029810298,
      "step": 1339,
      "training_loss": 6.308664321899414
    },
    {
      "epoch": 0.2902981029810298,
      "step": 1339,
      "training_loss": 6.909087181091309
    },
    {
      "epoch": 0.2902981029810298,
      "step": 1339,
      "training_loss": 7.114184379577637
    },
    {
      "epoch": 0.2902981029810298,
      "step": 1339,
      "training_loss": 8.113797187805176
    },
    {
      "epoch": 0.2905149051490515,
      "grad_norm": 22.632280349731445,
      "learning_rate": 1e-05,
      "loss": 6.4798,
      "step": 1340
    },
    {
      "epoch": 0.2905149051490515,
      "step": 1340,
      "training_loss": 7.56404972076416
    },
    {
      "epoch": 0.2905149051490515,
      "step": 1340,
      "training_loss": 6.889871120452881
    },
    {
      "epoch": 0.2905149051490515,
      "step": 1340,
      "training_loss": 7.012172698974609
    },
    {
      "epoch": 0.2905149051490515,
      "step": 1340,
      "training_loss": 7.553373336791992
    },
    {
      "epoch": 0.29073170731707315,
      "step": 1341,
      "training_loss": 6.363046646118164
    },
    {
      "epoch": 0.29073170731707315,
      "step": 1341,
      "training_loss": 6.575982570648193
    },
    {
      "epoch": 0.29073170731707315,
      "step": 1341,
      "training_loss": 7.32199239730835
    },
    {
      "epoch": 0.29073170731707315,
      "step": 1341,
      "training_loss": 8.021598815917969
    },
    {
      "epoch": 0.2909485094850949,
      "step": 1342,
      "training_loss": 5.834506511688232
    },
    {
      "epoch": 0.2909485094850949,
      "step": 1342,
      "training_loss": 6.943058967590332
    },
    {
      "epoch": 0.2909485094850949,
      "step": 1342,
      "training_loss": 4.225324630737305
    },
    {
      "epoch": 0.2909485094850949,
      "step": 1342,
      "training_loss": 6.97036600112915
    },
    {
      "epoch": 0.29116531165311654,
      "step": 1343,
      "training_loss": 8.24795913696289
    },
    {
      "epoch": 0.29116531165311654,
      "step": 1343,
      "training_loss": 5.125082492828369
    },
    {
      "epoch": 0.29116531165311654,
      "step": 1343,
      "training_loss": 6.559038162231445
    },
    {
      "epoch": 0.29116531165311654,
      "step": 1343,
      "training_loss": 6.266921520233154
    },
    {
      "epoch": 0.2913821138211382,
      "grad_norm": 15.972419738769531,
      "learning_rate": 1e-05,
      "loss": 6.7171,
      "step": 1344
    },
    {
      "epoch": 0.2913821138211382,
      "step": 1344,
      "training_loss": 6.776487827301025
    },
    {
      "epoch": 0.2913821138211382,
      "step": 1344,
      "training_loss": 6.2809672355651855
    },
    {
      "epoch": 0.2913821138211382,
      "step": 1344,
      "training_loss": 7.254016399383545
    },
    {
      "epoch": 0.2913821138211382,
      "step": 1344,
      "training_loss": 7.173825740814209
    },
    {
      "epoch": 0.2915989159891599,
      "step": 1345,
      "training_loss": 7.074932098388672
    },
    {
      "epoch": 0.2915989159891599,
      "step": 1345,
      "training_loss": 6.4856953620910645
    },
    {
      "epoch": 0.2915989159891599,
      "step": 1345,
      "training_loss": 6.5350422859191895
    },
    {
      "epoch": 0.2915989159891599,
      "step": 1345,
      "training_loss": 6.978299617767334
    },
    {
      "epoch": 0.29181571815718155,
      "step": 1346,
      "training_loss": 4.018786430358887
    },
    {
      "epoch": 0.29181571815718155,
      "step": 1346,
      "training_loss": 7.543076992034912
    },
    {
      "epoch": 0.29181571815718155,
      "step": 1346,
      "training_loss": 6.575643539428711
    },
    {
      "epoch": 0.29181571815718155,
      "step": 1346,
      "training_loss": 6.172153472900391
    },
    {
      "epoch": 0.29203252032520327,
      "step": 1347,
      "training_loss": 6.568784713745117
    },
    {
      "epoch": 0.29203252032520327,
      "step": 1347,
      "training_loss": 7.0988311767578125
    },
    {
      "epoch": 0.29203252032520327,
      "step": 1347,
      "training_loss": 8.150613784790039
    },
    {
      "epoch": 0.29203252032520327,
      "step": 1347,
      "training_loss": 6.080081939697266
    },
    {
      "epoch": 0.29224932249322494,
      "grad_norm": 16.27137565612793,
      "learning_rate": 1e-05,
      "loss": 6.673,
      "step": 1348
    },
    {
      "epoch": 0.29224932249322494,
      "step": 1348,
      "training_loss": 6.176191329956055
    },
    {
      "epoch": 0.29224932249322494,
      "step": 1348,
      "training_loss": 5.973109245300293
    },
    {
      "epoch": 0.29224932249322494,
      "step": 1348,
      "training_loss": 7.147101402282715
    },
    {
      "epoch": 0.29224932249322494,
      "step": 1348,
      "training_loss": 7.903606414794922
    },
    {
      "epoch": 0.2924661246612466,
      "step": 1349,
      "training_loss": 7.163229942321777
    },
    {
      "epoch": 0.2924661246612466,
      "step": 1349,
      "training_loss": 6.734236717224121
    },
    {
      "epoch": 0.2924661246612466,
      "step": 1349,
      "training_loss": 8.407845497131348
    },
    {
      "epoch": 0.2924661246612466,
      "step": 1349,
      "training_loss": 6.8465576171875
    },
    {
      "epoch": 0.2926829268292683,
      "step": 1350,
      "training_loss": 7.946640491485596
    },
    {
      "epoch": 0.2926829268292683,
      "step": 1350,
      "training_loss": 7.112903118133545
    },
    {
      "epoch": 0.2926829268292683,
      "step": 1350,
      "training_loss": 7.407663822174072
    },
    {
      "epoch": 0.2926829268292683,
      "step": 1350,
      "training_loss": 7.102404594421387
    },
    {
      "epoch": 0.29289972899729,
      "step": 1351,
      "training_loss": 6.74347448348999
    },
    {
      "epoch": 0.29289972899729,
      "step": 1351,
      "training_loss": 6.797824382781982
    },
    {
      "epoch": 0.29289972899729,
      "step": 1351,
      "training_loss": 7.307351589202881
    },
    {
      "epoch": 0.29289972899729,
      "step": 1351,
      "training_loss": 7.631748199462891
    },
    {
      "epoch": 0.29311653116531167,
      "grad_norm": 11.434033393859863,
      "learning_rate": 1e-05,
      "loss": 7.1501,
      "step": 1352
    },
    {
      "epoch": 0.29311653116531167,
      "step": 1352,
      "training_loss": 6.735259532928467
    },
    {
      "epoch": 0.29311653116531167,
      "step": 1352,
      "training_loss": 6.997584819793701
    },
    {
      "epoch": 0.29311653116531167,
      "step": 1352,
      "training_loss": 8.096525192260742
    },
    {
      "epoch": 0.29311653116531167,
      "step": 1352,
      "training_loss": 5.748788833618164
    },
    {
      "epoch": 0.29333333333333333,
      "step": 1353,
      "training_loss": 6.717619895935059
    },
    {
      "epoch": 0.29333333333333333,
      "step": 1353,
      "training_loss": 6.615087032318115
    },
    {
      "epoch": 0.29333333333333333,
      "step": 1353,
      "training_loss": 6.550461292266846
    },
    {
      "epoch": 0.29333333333333333,
      "step": 1353,
      "training_loss": 6.666436195373535
    },
    {
      "epoch": 0.293550135501355,
      "step": 1354,
      "training_loss": 6.461333751678467
    },
    {
      "epoch": 0.293550135501355,
      "step": 1354,
      "training_loss": 7.438094615936279
    },
    {
      "epoch": 0.293550135501355,
      "step": 1354,
      "training_loss": 7.2101826667785645
    },
    {
      "epoch": 0.293550135501355,
      "step": 1354,
      "training_loss": 6.458803176879883
    },
    {
      "epoch": 0.29376693766937667,
      "step": 1355,
      "training_loss": 7.770211696624756
    },
    {
      "epoch": 0.29376693766937667,
      "step": 1355,
      "training_loss": 6.825734615325928
    },
    {
      "epoch": 0.29376693766937667,
      "step": 1355,
      "training_loss": 6.544833183288574
    },
    {
      "epoch": 0.29376693766937667,
      "step": 1355,
      "training_loss": 7.1489176750183105
    },
    {
      "epoch": 0.2939837398373984,
      "grad_norm": 13.230466842651367,
      "learning_rate": 1e-05,
      "loss": 6.8741,
      "step": 1356
    },
    {
      "epoch": 0.2939837398373984,
      "step": 1356,
      "training_loss": 6.358774662017822
    },
    {
      "epoch": 0.2939837398373984,
      "step": 1356,
      "training_loss": 8.793094635009766
    },
    {
      "epoch": 0.2939837398373984,
      "step": 1356,
      "training_loss": 5.566204071044922
    },
    {
      "epoch": 0.2939837398373984,
      "step": 1356,
      "training_loss": 5.790104866027832
    },
    {
      "epoch": 0.29420054200542006,
      "step": 1357,
      "training_loss": 6.946720123291016
    },
    {
      "epoch": 0.29420054200542006,
      "step": 1357,
      "training_loss": 6.237910270690918
    },
    {
      "epoch": 0.29420054200542006,
      "step": 1357,
      "training_loss": 7.329329967498779
    },
    {
      "epoch": 0.29420054200542006,
      "step": 1357,
      "training_loss": 6.627526760101318
    },
    {
      "epoch": 0.29441734417344173,
      "step": 1358,
      "training_loss": 5.157344341278076
    },
    {
      "epoch": 0.29441734417344173,
      "step": 1358,
      "training_loss": 6.096437931060791
    },
    {
      "epoch": 0.29441734417344173,
      "step": 1358,
      "training_loss": 5.649619102478027
    },
    {
      "epoch": 0.29441734417344173,
      "step": 1358,
      "training_loss": 6.718867301940918
    },
    {
      "epoch": 0.2946341463414634,
      "step": 1359,
      "training_loss": 5.156196117401123
    },
    {
      "epoch": 0.2946341463414634,
      "step": 1359,
      "training_loss": 6.9245171546936035
    },
    {
      "epoch": 0.2946341463414634,
      "step": 1359,
      "training_loss": 7.4839701652526855
    },
    {
      "epoch": 0.2946341463414634,
      "step": 1359,
      "training_loss": 6.334133148193359
    },
    {
      "epoch": 0.2948509485094851,
      "grad_norm": 9.734341621398926,
      "learning_rate": 1e-05,
      "loss": 6.4482,
      "step": 1360
    },
    {
      "epoch": 0.2948509485094851,
      "step": 1360,
      "training_loss": 7.0204877853393555
    },
    {
      "epoch": 0.2948509485094851,
      "step": 1360,
      "training_loss": 6.714045524597168
    },
    {
      "epoch": 0.2948509485094851,
      "step": 1360,
      "training_loss": 6.529388904571533
    },
    {
      "epoch": 0.2948509485094851,
      "step": 1360,
      "training_loss": 5.032479286193848
    },
    {
      "epoch": 0.2950677506775068,
      "step": 1361,
      "training_loss": 5.095959186553955
    },
    {
      "epoch": 0.2950677506775068,
      "step": 1361,
      "training_loss": 6.79853630065918
    },
    {
      "epoch": 0.2950677506775068,
      "step": 1361,
      "training_loss": 7.926906108856201
    },
    {
      "epoch": 0.2950677506775068,
      "step": 1361,
      "training_loss": 6.743923664093018
    },
    {
      "epoch": 0.29528455284552846,
      "step": 1362,
      "training_loss": 6.301528453826904
    },
    {
      "epoch": 0.29528455284552846,
      "step": 1362,
      "training_loss": 6.253498554229736
    },
    {
      "epoch": 0.29528455284552846,
      "step": 1362,
      "training_loss": 6.314336776733398
    },
    {
      "epoch": 0.29528455284552846,
      "step": 1362,
      "training_loss": 5.693105220794678
    },
    {
      "epoch": 0.2955013550135501,
      "step": 1363,
      "training_loss": 7.708775043487549
    },
    {
      "epoch": 0.2955013550135501,
      "step": 1363,
      "training_loss": 7.094983100891113
    },
    {
      "epoch": 0.2955013550135501,
      "step": 1363,
      "training_loss": 6.7445197105407715
    },
    {
      "epoch": 0.2955013550135501,
      "step": 1363,
      "training_loss": 8.545433044433594
    },
    {
      "epoch": 0.2957181571815718,
      "grad_norm": 15.22334098815918,
      "learning_rate": 1e-05,
      "loss": 6.6574,
      "step": 1364
    },
    {
      "epoch": 0.2957181571815718,
      "step": 1364,
      "training_loss": 7.840020656585693
    },
    {
      "epoch": 0.2957181571815718,
      "step": 1364,
      "training_loss": 6.955127716064453
    },
    {
      "epoch": 0.2957181571815718,
      "step": 1364,
      "training_loss": 6.932441234588623
    },
    {
      "epoch": 0.2957181571815718,
      "step": 1364,
      "training_loss": 6.170704364776611
    },
    {
      "epoch": 0.2959349593495935,
      "step": 1365,
      "training_loss": 7.776655197143555
    },
    {
      "epoch": 0.2959349593495935,
      "step": 1365,
      "training_loss": 6.400313377380371
    },
    {
      "epoch": 0.2959349593495935,
      "step": 1365,
      "training_loss": 7.161675453186035
    },
    {
      "epoch": 0.2959349593495935,
      "step": 1365,
      "training_loss": 7.061697006225586
    },
    {
      "epoch": 0.2961517615176152,
      "step": 1366,
      "training_loss": 7.991856575012207
    },
    {
      "epoch": 0.2961517615176152,
      "step": 1366,
      "training_loss": 6.194230079650879
    },
    {
      "epoch": 0.2961517615176152,
      "step": 1366,
      "training_loss": 5.489253520965576
    },
    {
      "epoch": 0.2961517615176152,
      "step": 1366,
      "training_loss": 7.799817085266113
    },
    {
      "epoch": 0.29636856368563685,
      "step": 1367,
      "training_loss": 6.640335559844971
    },
    {
      "epoch": 0.29636856368563685,
      "step": 1367,
      "training_loss": 7.158557891845703
    },
    {
      "epoch": 0.29636856368563685,
      "step": 1367,
      "training_loss": 5.0199689865112305
    },
    {
      "epoch": 0.29636856368563685,
      "step": 1367,
      "training_loss": 5.949731826782227
    },
    {
      "epoch": 0.2965853658536585,
      "grad_norm": 10.781343460083008,
      "learning_rate": 1e-05,
      "loss": 6.7839,
      "step": 1368
    },
    {
      "epoch": 0.2965853658536585,
      "step": 1368,
      "training_loss": 6.570436954498291
    },
    {
      "epoch": 0.2965853658536585,
      "step": 1368,
      "training_loss": 7.966790199279785
    },
    {
      "epoch": 0.2965853658536585,
      "step": 1368,
      "training_loss": 6.855046272277832
    },
    {
      "epoch": 0.2965853658536585,
      "step": 1368,
      "training_loss": 6.588707447052002
    },
    {
      "epoch": 0.2968021680216802,
      "step": 1369,
      "training_loss": 4.329687595367432
    },
    {
      "epoch": 0.2968021680216802,
      "step": 1369,
      "training_loss": 7.929257392883301
    },
    {
      "epoch": 0.2968021680216802,
      "step": 1369,
      "training_loss": 7.264288425445557
    },
    {
      "epoch": 0.2968021680216802,
      "step": 1369,
      "training_loss": 6.225968360900879
    },
    {
      "epoch": 0.2970189701897019,
      "step": 1370,
      "training_loss": 6.434063911437988
    },
    {
      "epoch": 0.2970189701897019,
      "step": 1370,
      "training_loss": 6.78171968460083
    },
    {
      "epoch": 0.2970189701897019,
      "step": 1370,
      "training_loss": 6.1632561683654785
    },
    {
      "epoch": 0.2970189701897019,
      "step": 1370,
      "training_loss": 5.806246280670166
    },
    {
      "epoch": 0.2972357723577236,
      "step": 1371,
      "training_loss": 5.707716464996338
    },
    {
      "epoch": 0.2972357723577236,
      "step": 1371,
      "training_loss": 7.128995895385742
    },
    {
      "epoch": 0.2972357723577236,
      "step": 1371,
      "training_loss": 5.848001956939697
    },
    {
      "epoch": 0.2972357723577236,
      "step": 1371,
      "training_loss": 6.528724670410156
    },
    {
      "epoch": 0.29745257452574525,
      "grad_norm": 23.219783782958984,
      "learning_rate": 1e-05,
      "loss": 6.5081,
      "step": 1372
    },
    {
      "epoch": 0.29745257452574525,
      "step": 1372,
      "training_loss": 6.6113810539245605
    },
    {
      "epoch": 0.29745257452574525,
      "step": 1372,
      "training_loss": 8.030694961547852
    },
    {
      "epoch": 0.29745257452574525,
      "step": 1372,
      "training_loss": 7.01724910736084
    },
    {
      "epoch": 0.29745257452574525,
      "step": 1372,
      "training_loss": 7.28882360458374
    },
    {
      "epoch": 0.2976693766937669,
      "step": 1373,
      "training_loss": 8.073230743408203
    },
    {
      "epoch": 0.2976693766937669,
      "step": 1373,
      "training_loss": 6.550226211547852
    },
    {
      "epoch": 0.2976693766937669,
      "step": 1373,
      "training_loss": 6.210726261138916
    },
    {
      "epoch": 0.2976693766937669,
      "step": 1373,
      "training_loss": 4.3593034744262695
    },
    {
      "epoch": 0.29788617886178864,
      "step": 1374,
      "training_loss": 7.601169586181641
    },
    {
      "epoch": 0.29788617886178864,
      "step": 1374,
      "training_loss": 5.08790922164917
    },
    {
      "epoch": 0.29788617886178864,
      "step": 1374,
      "training_loss": 7.525094985961914
    },
    {
      "epoch": 0.29788617886178864,
      "step": 1374,
      "training_loss": 7.070566177368164
    },
    {
      "epoch": 0.2981029810298103,
      "step": 1375,
      "training_loss": 6.912687301635742
    },
    {
      "epoch": 0.2981029810298103,
      "step": 1375,
      "training_loss": 5.301664352416992
    },
    {
      "epoch": 0.2981029810298103,
      "step": 1375,
      "training_loss": 8.055009841918945
    },
    {
      "epoch": 0.2981029810298103,
      "step": 1375,
      "training_loss": 7.701381683349609
    },
    {
      "epoch": 0.298319783197832,
      "grad_norm": 17.30449104309082,
      "learning_rate": 1e-05,
      "loss": 6.8373,
      "step": 1376
    },
    {
      "epoch": 0.298319783197832,
      "step": 1376,
      "training_loss": 7.607091903686523
    },
    {
      "epoch": 0.298319783197832,
      "step": 1376,
      "training_loss": 6.186398506164551
    },
    {
      "epoch": 0.298319783197832,
      "step": 1376,
      "training_loss": 6.814085006713867
    },
    {
      "epoch": 0.298319783197832,
      "step": 1376,
      "training_loss": 7.8689703941345215
    },
    {
      "epoch": 0.29853658536585365,
      "step": 1377,
      "training_loss": 7.054162502288818
    },
    {
      "epoch": 0.29853658536585365,
      "step": 1377,
      "training_loss": 6.450982570648193
    },
    {
      "epoch": 0.29853658536585365,
      "step": 1377,
      "training_loss": 8.162421226501465
    },
    {
      "epoch": 0.29853658536585365,
      "step": 1377,
      "training_loss": 6.834281921386719
    },
    {
      "epoch": 0.2987533875338753,
      "step": 1378,
      "training_loss": 7.102719306945801
    },
    {
      "epoch": 0.2987533875338753,
      "step": 1378,
      "training_loss": 6.12769889831543
    },
    {
      "epoch": 0.2987533875338753,
      "step": 1378,
      "training_loss": 7.900758743286133
    },
    {
      "epoch": 0.2987533875338753,
      "step": 1378,
      "training_loss": 6.899157524108887
    },
    {
      "epoch": 0.29897018970189704,
      "step": 1379,
      "training_loss": 6.733062744140625
    },
    {
      "epoch": 0.29897018970189704,
      "step": 1379,
      "training_loss": 8.02580451965332
    },
    {
      "epoch": 0.29897018970189704,
      "step": 1379,
      "training_loss": 6.931155681610107
    },
    {
      "epoch": 0.29897018970189704,
      "step": 1379,
      "training_loss": 6.79383659362793
    },
    {
      "epoch": 0.2991869918699187,
      "grad_norm": 19.166292190551758,
      "learning_rate": 1e-05,
      "loss": 7.0933,
      "step": 1380
    },
    {
      "epoch": 0.2991869918699187,
      "step": 1380,
      "training_loss": 5.62145471572876
    },
    {
      "epoch": 0.2991869918699187,
      "step": 1380,
      "training_loss": 6.4542083740234375
    },
    {
      "epoch": 0.2991869918699187,
      "step": 1380,
      "training_loss": 7.387967586517334
    },
    {
      "epoch": 0.2991869918699187,
      "step": 1380,
      "training_loss": 7.77352237701416
    },
    {
      "epoch": 0.2994037940379404,
      "step": 1381,
      "training_loss": 4.0244646072387695
    },
    {
      "epoch": 0.2994037940379404,
      "step": 1381,
      "training_loss": 5.945379257202148
    },
    {
      "epoch": 0.2994037940379404,
      "step": 1381,
      "training_loss": 5.747041702270508
    },
    {
      "epoch": 0.2994037940379404,
      "step": 1381,
      "training_loss": 4.22377872467041
    },
    {
      "epoch": 0.29962059620596204,
      "step": 1382,
      "training_loss": 4.295629501342773
    },
    {
      "epoch": 0.29962059620596204,
      "step": 1382,
      "training_loss": 6.608102321624756
    },
    {
      "epoch": 0.29962059620596204,
      "step": 1382,
      "training_loss": 6.400540351867676
    },
    {
      "epoch": 0.29962059620596204,
      "step": 1382,
      "training_loss": 4.254399299621582
    },
    {
      "epoch": 0.29983739837398377,
      "step": 1383,
      "training_loss": 7.271580696105957
    },
    {
      "epoch": 0.29983739837398377,
      "step": 1383,
      "training_loss": 7.398327350616455
    },
    {
      "epoch": 0.29983739837398377,
      "step": 1383,
      "training_loss": 6.912118911743164
    },
    {
      "epoch": 0.29983739837398377,
      "step": 1383,
      "training_loss": 7.0075788497924805
    },
    {
      "epoch": 0.30005420054200543,
      "grad_norm": 14.955411911010742,
      "learning_rate": 1e-05,
      "loss": 6.0829,
      "step": 1384
    },
    {
      "epoch": 0.30005420054200543,
      "step": 1384,
      "training_loss": 8.173809051513672
    },
    {
      "epoch": 0.30005420054200543,
      "step": 1384,
      "training_loss": 6.321713924407959
    },
    {
      "epoch": 0.30005420054200543,
      "step": 1384,
      "training_loss": 6.750280857086182
    },
    {
      "epoch": 0.30005420054200543,
      "step": 1384,
      "training_loss": 6.942405700683594
    },
    {
      "epoch": 0.3002710027100271,
      "step": 1385,
      "training_loss": 7.687934398651123
    },
    {
      "epoch": 0.3002710027100271,
      "step": 1385,
      "training_loss": 5.567225456237793
    },
    {
      "epoch": 0.3002710027100271,
      "step": 1385,
      "training_loss": 7.557606220245361
    },
    {
      "epoch": 0.3002710027100271,
      "step": 1385,
      "training_loss": 6.623787879943848
    },
    {
      "epoch": 0.30048780487804877,
      "step": 1386,
      "training_loss": 6.571475028991699
    },
    {
      "epoch": 0.30048780487804877,
      "step": 1386,
      "training_loss": 7.439786911010742
    },
    {
      "epoch": 0.30048780487804877,
      "step": 1386,
      "training_loss": 7.618531227111816
    },
    {
      "epoch": 0.30048780487804877,
      "step": 1386,
      "training_loss": 5.1164469718933105
    },
    {
      "epoch": 0.30070460704607044,
      "step": 1387,
      "training_loss": 7.840679168701172
    },
    {
      "epoch": 0.30070460704607044,
      "step": 1387,
      "training_loss": 7.409266948699951
    },
    {
      "epoch": 0.30070460704607044,
      "step": 1387,
      "training_loss": 6.7878193855285645
    },
    {
      "epoch": 0.30070460704607044,
      "step": 1387,
      "training_loss": 6.518970966339111
    },
    {
      "epoch": 0.30092140921409216,
      "grad_norm": 16.241790771484375,
      "learning_rate": 1e-05,
      "loss": 6.933,
      "step": 1388
    },
    {
      "epoch": 0.30092140921409216,
      "step": 1388,
      "training_loss": 7.992922306060791
    },
    {
      "epoch": 0.30092140921409216,
      "step": 1388,
      "training_loss": 6.303822040557861
    },
    {
      "epoch": 0.30092140921409216,
      "step": 1388,
      "training_loss": 7.529316425323486
    },
    {
      "epoch": 0.30092140921409216,
      "step": 1388,
      "training_loss": 8.282881736755371
    },
    {
      "epoch": 0.30113821138211383,
      "step": 1389,
      "training_loss": 5.543176174163818
    },
    {
      "epoch": 0.30113821138211383,
      "step": 1389,
      "training_loss": 7.70306921005249
    },
    {
      "epoch": 0.30113821138211383,
      "step": 1389,
      "training_loss": 6.397167682647705
    },
    {
      "epoch": 0.30113821138211383,
      "step": 1389,
      "training_loss": 7.600156784057617
    },
    {
      "epoch": 0.3013550135501355,
      "step": 1390,
      "training_loss": 6.689845561981201
    },
    {
      "epoch": 0.3013550135501355,
      "step": 1390,
      "training_loss": 6.3251543045043945
    },
    {
      "epoch": 0.3013550135501355,
      "step": 1390,
      "training_loss": 6.178611755371094
    },
    {
      "epoch": 0.3013550135501355,
      "step": 1390,
      "training_loss": 5.996553421020508
    },
    {
      "epoch": 0.30157181571815717,
      "step": 1391,
      "training_loss": 6.790215492248535
    },
    {
      "epoch": 0.30157181571815717,
      "step": 1391,
      "training_loss": 6.367956638336182
    },
    {
      "epoch": 0.30157181571815717,
      "step": 1391,
      "training_loss": 6.907278537750244
    },
    {
      "epoch": 0.30157181571815717,
      "step": 1391,
      "training_loss": 7.271768093109131
    },
    {
      "epoch": 0.3017886178861789,
      "grad_norm": 14.544967651367188,
      "learning_rate": 1e-05,
      "loss": 6.8675,
      "step": 1392
    },
    {
      "epoch": 0.3017886178861789,
      "step": 1392,
      "training_loss": 6.74839448928833
    },
    {
      "epoch": 0.3017886178861789,
      "step": 1392,
      "training_loss": 8.356846809387207
    },
    {
      "epoch": 0.3017886178861789,
      "step": 1392,
      "training_loss": 3.859017848968506
    },
    {
      "epoch": 0.3017886178861789,
      "step": 1392,
      "training_loss": 5.903558254241943
    },
    {
      "epoch": 0.30200542005420056,
      "step": 1393,
      "training_loss": 7.547022342681885
    },
    {
      "epoch": 0.30200542005420056,
      "step": 1393,
      "training_loss": 6.819164276123047
    },
    {
      "epoch": 0.30200542005420056,
      "step": 1393,
      "training_loss": 8.208586692810059
    },
    {
      "epoch": 0.30200542005420056,
      "step": 1393,
      "training_loss": 5.017237663269043
    },
    {
      "epoch": 0.3022222222222222,
      "step": 1394,
      "training_loss": 7.509252071380615
    },
    {
      "epoch": 0.3022222222222222,
      "step": 1394,
      "training_loss": 7.156900882720947
    },
    {
      "epoch": 0.3022222222222222,
      "step": 1394,
      "training_loss": 5.734721660614014
    },
    {
      "epoch": 0.3022222222222222,
      "step": 1394,
      "training_loss": 5.736717700958252
    },
    {
      "epoch": 0.3024390243902439,
      "step": 1395,
      "training_loss": 6.512749671936035
    },
    {
      "epoch": 0.3024390243902439,
      "step": 1395,
      "training_loss": 6.814656734466553
    },
    {
      "epoch": 0.3024390243902439,
      "step": 1395,
      "training_loss": 6.110393524169922
    },
    {
      "epoch": 0.3024390243902439,
      "step": 1395,
      "training_loss": 5.901780605316162
    },
    {
      "epoch": 0.30265582655826556,
      "grad_norm": 11.778761863708496,
      "learning_rate": 1e-05,
      "loss": 6.4961,
      "step": 1396
    },
    {
      "epoch": 0.30265582655826556,
      "step": 1396,
      "training_loss": 6.324875354766846
    },
    {
      "epoch": 0.30265582655826556,
      "step": 1396,
      "training_loss": 5.679746627807617
    },
    {
      "epoch": 0.30265582655826556,
      "step": 1396,
      "training_loss": 7.530043125152588
    },
    {
      "epoch": 0.30265582655826556,
      "step": 1396,
      "training_loss": 7.216066360473633
    },
    {
      "epoch": 0.3028726287262873,
      "step": 1397,
      "training_loss": 4.423112869262695
    },
    {
      "epoch": 0.3028726287262873,
      "step": 1397,
      "training_loss": 7.547185897827148
    },
    {
      "epoch": 0.3028726287262873,
      "step": 1397,
      "training_loss": 6.438981533050537
    },
    {
      "epoch": 0.3028726287262873,
      "step": 1397,
      "training_loss": 5.686913013458252
    },
    {
      "epoch": 0.30308943089430895,
      "step": 1398,
      "training_loss": 7.788201808929443
    },
    {
      "epoch": 0.30308943089430895,
      "step": 1398,
      "training_loss": 7.52447509765625
    },
    {
      "epoch": 0.30308943089430895,
      "step": 1398,
      "training_loss": 6.838524341583252
    },
    {
      "epoch": 0.30308943089430895,
      "step": 1398,
      "training_loss": 7.318325042724609
    },
    {
      "epoch": 0.3033062330623306,
      "step": 1399,
      "training_loss": 7.068358421325684
    },
    {
      "epoch": 0.3033062330623306,
      "step": 1399,
      "training_loss": 7.057887554168701
    },
    {
      "epoch": 0.3033062330623306,
      "step": 1399,
      "training_loss": 5.0706634521484375
    },
    {
      "epoch": 0.3033062330623306,
      "step": 1399,
      "training_loss": 7.59103536605835
    },
    {
      "epoch": 0.3035230352303523,
      "grad_norm": 11.384747505187988,
      "learning_rate": 1e-05,
      "loss": 6.694,
      "step": 1400
    },
    {
      "epoch": 0.3035230352303523,
      "step": 1400,
      "training_loss": 7.5120439529418945
    },
    {
      "epoch": 0.3035230352303523,
      "step": 1400,
      "training_loss": 6.493667125701904
    },
    {
      "epoch": 0.3035230352303523,
      "step": 1400,
      "training_loss": 6.994469165802002
    },
    {
      "epoch": 0.3035230352303523,
      "step": 1400,
      "training_loss": 6.203680992126465
    },
    {
      "epoch": 0.30373983739837396,
      "step": 1401,
      "training_loss": 7.238953113555908
    },
    {
      "epoch": 0.30373983739837396,
      "step": 1401,
      "training_loss": 5.7728800773620605
    },
    {
      "epoch": 0.30373983739837396,
      "step": 1401,
      "training_loss": 6.932456970214844
    },
    {
      "epoch": 0.30373983739837396,
      "step": 1401,
      "training_loss": 6.551589012145996
    },
    {
      "epoch": 0.3039566395663957,
      "step": 1402,
      "training_loss": 7.167412757873535
    },
    {
      "epoch": 0.3039566395663957,
      "step": 1402,
      "training_loss": 6.847579479217529
    },
    {
      "epoch": 0.3039566395663957,
      "step": 1402,
      "training_loss": 7.0991621017456055
    },
    {
      "epoch": 0.3039566395663957,
      "step": 1402,
      "training_loss": 8.228328704833984
    },
    {
      "epoch": 0.30417344173441735,
      "step": 1403,
      "training_loss": 6.599272727966309
    },
    {
      "epoch": 0.30417344173441735,
      "step": 1403,
      "training_loss": 5.763125896453857
    },
    {
      "epoch": 0.30417344173441735,
      "step": 1403,
      "training_loss": 6.558004856109619
    },
    {
      "epoch": 0.30417344173441735,
      "step": 1403,
      "training_loss": 5.017269611358643
    },
    {
      "epoch": 0.304390243902439,
      "grad_norm": 13.606943130493164,
      "learning_rate": 1e-05,
      "loss": 6.6862,
      "step": 1404
    },
    {
      "epoch": 0.304390243902439,
      "step": 1404,
      "training_loss": 5.721384525299072
    },
    {
      "epoch": 0.304390243902439,
      "step": 1404,
      "training_loss": 7.802256107330322
    },
    {
      "epoch": 0.304390243902439,
      "step": 1404,
      "training_loss": 6.329334259033203
    },
    {
      "epoch": 0.304390243902439,
      "step": 1404,
      "training_loss": 6.824406147003174
    },
    {
      "epoch": 0.3046070460704607,
      "step": 1405,
      "training_loss": 6.643347263336182
    },
    {
      "epoch": 0.3046070460704607,
      "step": 1405,
      "training_loss": 7.0000481605529785
    },
    {
      "epoch": 0.3046070460704607,
      "step": 1405,
      "training_loss": 7.3759660720825195
    },
    {
      "epoch": 0.3046070460704607,
      "step": 1405,
      "training_loss": 6.584873676300049
    },
    {
      "epoch": 0.3048238482384824,
      "step": 1406,
      "training_loss": 4.858124732971191
    },
    {
      "epoch": 0.3048238482384824,
      "step": 1406,
      "training_loss": 6.7580952644348145
    },
    {
      "epoch": 0.3048238482384824,
      "step": 1406,
      "training_loss": 5.129665374755859
    },
    {
      "epoch": 0.3048238482384824,
      "step": 1406,
      "training_loss": 7.7255048751831055
    },
    {
      "epoch": 0.3050406504065041,
      "step": 1407,
      "training_loss": 5.872826099395752
    },
    {
      "epoch": 0.3050406504065041,
      "step": 1407,
      "training_loss": 7.254803657531738
    },
    {
      "epoch": 0.3050406504065041,
      "step": 1407,
      "training_loss": 5.7286224365234375
    },
    {
      "epoch": 0.3050406504065041,
      "step": 1407,
      "training_loss": 6.380197048187256
    },
    {
      "epoch": 0.30525745257452574,
      "grad_norm": 11.069196701049805,
      "learning_rate": 1e-05,
      "loss": 6.4993,
      "step": 1408
    },
    {
      "epoch": 0.30525745257452574,
      "step": 1408,
      "training_loss": 3.9975147247314453
    },
    {
      "epoch": 0.30525745257452574,
      "step": 1408,
      "training_loss": 6.643405914306641
    },
    {
      "epoch": 0.30525745257452574,
      "step": 1408,
      "training_loss": 7.446573257446289
    },
    {
      "epoch": 0.30525745257452574,
      "step": 1408,
      "training_loss": 5.405411243438721
    },
    {
      "epoch": 0.3054742547425474,
      "step": 1409,
      "training_loss": 8.090225219726562
    },
    {
      "epoch": 0.3054742547425474,
      "step": 1409,
      "training_loss": 7.355663299560547
    },
    {
      "epoch": 0.3054742547425474,
      "step": 1409,
      "training_loss": 7.107804298400879
    },
    {
      "epoch": 0.3054742547425474,
      "step": 1409,
      "training_loss": 7.256630897521973
    },
    {
      "epoch": 0.3056910569105691,
      "step": 1410,
      "training_loss": 6.13923978805542
    },
    {
      "epoch": 0.3056910569105691,
      "step": 1410,
      "training_loss": 6.951998233795166
    },
    {
      "epoch": 0.3056910569105691,
      "step": 1410,
      "training_loss": 5.39179801940918
    },
    {
      "epoch": 0.3056910569105691,
      "step": 1410,
      "training_loss": 7.311962604522705
    },
    {
      "epoch": 0.3059078590785908,
      "step": 1411,
      "training_loss": 6.2878546714782715
    },
    {
      "epoch": 0.3059078590785908,
      "step": 1411,
      "training_loss": 5.828041076660156
    },
    {
      "epoch": 0.3059078590785908,
      "step": 1411,
      "training_loss": 4.685575008392334
    },
    {
      "epoch": 0.3059078590785908,
      "step": 1411,
      "training_loss": 6.351187229156494
    },
    {
      "epoch": 0.3061246612466125,
      "grad_norm": 13.426641464233398,
      "learning_rate": 1e-05,
      "loss": 6.3907,
      "step": 1412
    },
    {
      "epoch": 0.3061246612466125,
      "step": 1412,
      "training_loss": 7.139742851257324
    },
    {
      "epoch": 0.3061246612466125,
      "step": 1412,
      "training_loss": 6.324019432067871
    },
    {
      "epoch": 0.3061246612466125,
      "step": 1412,
      "training_loss": 6.641673564910889
    },
    {
      "epoch": 0.3061246612466125,
      "step": 1412,
      "training_loss": 7.608128070831299
    },
    {
      "epoch": 0.30634146341463414,
      "step": 1413,
      "training_loss": 6.379562854766846
    },
    {
      "epoch": 0.30634146341463414,
      "step": 1413,
      "training_loss": 8.357694625854492
    },
    {
      "epoch": 0.30634146341463414,
      "step": 1413,
      "training_loss": 5.626741409301758
    },
    {
      "epoch": 0.30634146341463414,
      "step": 1413,
      "training_loss": 7.1899518966674805
    },
    {
      "epoch": 0.3065582655826558,
      "step": 1414,
      "training_loss": 6.519652366638184
    },
    {
      "epoch": 0.3065582655826558,
      "step": 1414,
      "training_loss": 5.660458564758301
    },
    {
      "epoch": 0.3065582655826558,
      "step": 1414,
      "training_loss": 4.153300762176514
    },
    {
      "epoch": 0.3065582655826558,
      "step": 1414,
      "training_loss": 7.625481128692627
    },
    {
      "epoch": 0.30677506775067753,
      "step": 1415,
      "training_loss": 6.749198913574219
    },
    {
      "epoch": 0.30677506775067753,
      "step": 1415,
      "training_loss": 6.594371318817139
    },
    {
      "epoch": 0.30677506775067753,
      "step": 1415,
      "training_loss": 7.189671993255615
    },
    {
      "epoch": 0.30677506775067753,
      "step": 1415,
      "training_loss": 7.53172492980957
    },
    {
      "epoch": 0.3069918699186992,
      "grad_norm": 17.496196746826172,
      "learning_rate": 1e-05,
      "loss": 6.7057,
      "step": 1416
    },
    {
      "epoch": 0.3069918699186992,
      "step": 1416,
      "training_loss": 6.756158828735352
    },
    {
      "epoch": 0.3069918699186992,
      "step": 1416,
      "training_loss": 6.5746283531188965
    },
    {
      "epoch": 0.3069918699186992,
      "step": 1416,
      "training_loss": 7.243355751037598
    },
    {
      "epoch": 0.3069918699186992,
      "step": 1416,
      "training_loss": 5.688704490661621
    },
    {
      "epoch": 0.30720867208672087,
      "step": 1417,
      "training_loss": 5.758035182952881
    },
    {
      "epoch": 0.30720867208672087,
      "step": 1417,
      "training_loss": 6.098149299621582
    },
    {
      "epoch": 0.30720867208672087,
      "step": 1417,
      "training_loss": 5.63273286819458
    },
    {
      "epoch": 0.30720867208672087,
      "step": 1417,
      "training_loss": 6.919812202453613
    },
    {
      "epoch": 0.30742547425474254,
      "step": 1418,
      "training_loss": 7.76881742477417
    },
    {
      "epoch": 0.30742547425474254,
      "step": 1418,
      "training_loss": 7.027225017547607
    },
    {
      "epoch": 0.30742547425474254,
      "step": 1418,
      "training_loss": 5.7784600257873535
    },
    {
      "epoch": 0.30742547425474254,
      "step": 1418,
      "training_loss": 6.398581504821777
    },
    {
      "epoch": 0.3076422764227642,
      "step": 1419,
      "training_loss": 6.887998104095459
    },
    {
      "epoch": 0.3076422764227642,
      "step": 1419,
      "training_loss": 8.083507537841797
    },
    {
      "epoch": 0.3076422764227642,
      "step": 1419,
      "training_loss": 6.750874042510986
    },
    {
      "epoch": 0.3076422764227642,
      "step": 1419,
      "training_loss": 6.558993339538574
    },
    {
      "epoch": 0.30785907859078593,
      "grad_norm": 9.982659339904785,
      "learning_rate": 1e-05,
      "loss": 6.6204,
      "step": 1420
    },
    {
      "epoch": 0.30785907859078593,
      "step": 1420,
      "training_loss": 6.24371862411499
    },
    {
      "epoch": 0.30785907859078593,
      "step": 1420,
      "training_loss": 6.936748027801514
    },
    {
      "epoch": 0.30785907859078593,
      "step": 1420,
      "training_loss": 7.92408561706543
    },
    {
      "epoch": 0.30785907859078593,
      "step": 1420,
      "training_loss": 6.369785785675049
    },
    {
      "epoch": 0.3080758807588076,
      "step": 1421,
      "training_loss": 4.971831798553467
    },
    {
      "epoch": 0.3080758807588076,
      "step": 1421,
      "training_loss": 7.557741165161133
    },
    {
      "epoch": 0.3080758807588076,
      "step": 1421,
      "training_loss": 7.252252578735352
    },
    {
      "epoch": 0.3080758807588076,
      "step": 1421,
      "training_loss": 5.538578987121582
    },
    {
      "epoch": 0.30829268292682926,
      "step": 1422,
      "training_loss": 7.280585765838623
    },
    {
      "epoch": 0.30829268292682926,
      "step": 1422,
      "training_loss": 5.9848175048828125
    },
    {
      "epoch": 0.30829268292682926,
      "step": 1422,
      "training_loss": 6.887289524078369
    },
    {
      "epoch": 0.30829268292682926,
      "step": 1422,
      "training_loss": 7.6440653800964355
    },
    {
      "epoch": 0.30850948509485093,
      "step": 1423,
      "training_loss": 7.80511474609375
    },
    {
      "epoch": 0.30850948509485093,
      "step": 1423,
      "training_loss": 5.022353172302246
    },
    {
      "epoch": 0.30850948509485093,
      "step": 1423,
      "training_loss": 7.1986212730407715
    },
    {
      "epoch": 0.30850948509485093,
      "step": 1423,
      "training_loss": 6.671414852142334
    },
    {
      "epoch": 0.30872628726287266,
      "grad_norm": 14.82640552520752,
      "learning_rate": 1e-05,
      "loss": 6.7056,
      "step": 1424
    },
    {
      "epoch": 0.30872628726287266,
      "step": 1424,
      "training_loss": 6.831900596618652
    },
    {
      "epoch": 0.30872628726287266,
      "step": 1424,
      "training_loss": 6.336429119110107
    },
    {
      "epoch": 0.30872628726287266,
      "step": 1424,
      "training_loss": 6.355194091796875
    },
    {
      "epoch": 0.30872628726287266,
      "step": 1424,
      "training_loss": 6.861011981964111
    },
    {
      "epoch": 0.3089430894308943,
      "step": 1425,
      "training_loss": 6.800725936889648
    },
    {
      "epoch": 0.3089430894308943,
      "step": 1425,
      "training_loss": 7.213233947753906
    },
    {
      "epoch": 0.3089430894308943,
      "step": 1425,
      "training_loss": 6.96196174621582
    },
    {
      "epoch": 0.3089430894308943,
      "step": 1425,
      "training_loss": 7.1363325119018555
    },
    {
      "epoch": 0.309159891598916,
      "step": 1426,
      "training_loss": 6.275664329528809
    },
    {
      "epoch": 0.309159891598916,
      "step": 1426,
      "training_loss": 6.64984655380249
    },
    {
      "epoch": 0.309159891598916,
      "step": 1426,
      "training_loss": 6.213592529296875
    },
    {
      "epoch": 0.309159891598916,
      "step": 1426,
      "training_loss": 4.635868549346924
    },
    {
      "epoch": 0.30937669376693766,
      "step": 1427,
      "training_loss": 6.593475341796875
    },
    {
      "epoch": 0.30937669376693766,
      "step": 1427,
      "training_loss": 7.014228820800781
    },
    {
      "epoch": 0.30937669376693766,
      "step": 1427,
      "training_loss": 4.252357482910156
    },
    {
      "epoch": 0.30937669376693766,
      "step": 1427,
      "training_loss": 7.108349323272705
    },
    {
      "epoch": 0.30959349593495933,
      "grad_norm": 11.514863967895508,
      "learning_rate": 1e-05,
      "loss": 6.4525,
      "step": 1428
    },
    {
      "epoch": 0.30959349593495933,
      "step": 1428,
      "training_loss": 8.227124214172363
    },
    {
      "epoch": 0.30959349593495933,
      "step": 1428,
      "training_loss": 7.225296497344971
    },
    {
      "epoch": 0.30959349593495933,
      "step": 1428,
      "training_loss": 6.652248382568359
    },
    {
      "epoch": 0.30959349593495933,
      "step": 1428,
      "training_loss": 6.233054161071777
    },
    {
      "epoch": 0.30981029810298105,
      "step": 1429,
      "training_loss": 6.977043151855469
    },
    {
      "epoch": 0.30981029810298105,
      "step": 1429,
      "training_loss": 6.070866107940674
    },
    {
      "epoch": 0.30981029810298105,
      "step": 1429,
      "training_loss": 7.096938610076904
    },
    {
      "epoch": 0.30981029810298105,
      "step": 1429,
      "training_loss": 7.717878818511963
    },
    {
      "epoch": 0.3100271002710027,
      "step": 1430,
      "training_loss": 6.4437575340271
    },
    {
      "epoch": 0.3100271002710027,
      "step": 1430,
      "training_loss": 7.648641586303711
    },
    {
      "epoch": 0.3100271002710027,
      "step": 1430,
      "training_loss": 6.072420597076416
    },
    {
      "epoch": 0.3100271002710027,
      "step": 1430,
      "training_loss": 7.40692138671875
    },
    {
      "epoch": 0.3102439024390244,
      "step": 1431,
      "training_loss": 7.164690017700195
    },
    {
      "epoch": 0.3102439024390244,
      "step": 1431,
      "training_loss": 5.806889057159424
    },
    {
      "epoch": 0.3102439024390244,
      "step": 1431,
      "training_loss": 5.452540874481201
    },
    {
      "epoch": 0.3102439024390244,
      "step": 1431,
      "training_loss": 6.679403781890869
    },
    {
      "epoch": 0.31046070460704606,
      "grad_norm": 15.1935453414917,
      "learning_rate": 1e-05,
      "loss": 6.8047,
      "step": 1432
    },
    {
      "epoch": 0.31046070460704606,
      "step": 1432,
      "training_loss": 6.560759544372559
    },
    {
      "epoch": 0.31046070460704606,
      "step": 1432,
      "training_loss": 6.266528606414795
    },
    {
      "epoch": 0.31046070460704606,
      "step": 1432,
      "training_loss": 6.95404577255249
    },
    {
      "epoch": 0.31046070460704606,
      "step": 1432,
      "training_loss": 5.444431781768799
    },
    {
      "epoch": 0.3106775067750677,
      "step": 1433,
      "training_loss": 5.630626678466797
    },
    {
      "epoch": 0.3106775067750677,
      "step": 1433,
      "training_loss": 6.900425910949707
    },
    {
      "epoch": 0.3106775067750677,
      "step": 1433,
      "training_loss": 6.299784183502197
    },
    {
      "epoch": 0.3106775067750677,
      "step": 1433,
      "training_loss": 8.368404388427734
    },
    {
      "epoch": 0.31089430894308945,
      "step": 1434,
      "training_loss": 7.284867286682129
    },
    {
      "epoch": 0.31089430894308945,
      "step": 1434,
      "training_loss": 7.058640956878662
    },
    {
      "epoch": 0.31089430894308945,
      "step": 1434,
      "training_loss": 7.167344570159912
    },
    {
      "epoch": 0.31089430894308945,
      "step": 1434,
      "training_loss": 8.03536319732666
    },
    {
      "epoch": 0.3111111111111111,
      "step": 1435,
      "training_loss": 6.181209564208984
    },
    {
      "epoch": 0.3111111111111111,
      "step": 1435,
      "training_loss": 7.023962020874023
    },
    {
      "epoch": 0.3111111111111111,
      "step": 1435,
      "training_loss": 6.830103397369385
    },
    {
      "epoch": 0.3111111111111111,
      "step": 1435,
      "training_loss": 6.344310760498047
    },
    {
      "epoch": 0.3113279132791328,
      "grad_norm": 15.400979995727539,
      "learning_rate": 1e-05,
      "loss": 6.7719,
      "step": 1436
    },
    {
      "epoch": 0.3113279132791328,
      "step": 1436,
      "training_loss": 4.7397074699401855
    },
    {
      "epoch": 0.3113279132791328,
      "step": 1436,
      "training_loss": 6.900457859039307
    },
    {
      "epoch": 0.3113279132791328,
      "step": 1436,
      "training_loss": 6.829129219055176
    },
    {
      "epoch": 0.3113279132791328,
      "step": 1436,
      "training_loss": 3.955441474914551
    },
    {
      "epoch": 0.31154471544715445,
      "step": 1437,
      "training_loss": 7.452698230743408
    },
    {
      "epoch": 0.31154471544715445,
      "step": 1437,
      "training_loss": 6.647624969482422
    },
    {
      "epoch": 0.31154471544715445,
      "step": 1437,
      "training_loss": 6.532986164093018
    },
    {
      "epoch": 0.31154471544715445,
      "step": 1437,
      "training_loss": 6.250934600830078
    },
    {
      "epoch": 0.3117615176151762,
      "step": 1438,
      "training_loss": 6.740455150604248
    },
    {
      "epoch": 0.3117615176151762,
      "step": 1438,
      "training_loss": 7.697369575500488
    },
    {
      "epoch": 0.3117615176151762,
      "step": 1438,
      "training_loss": 6.502850532531738
    },
    {
      "epoch": 0.3117615176151762,
      "step": 1438,
      "training_loss": 5.700214385986328
    },
    {
      "epoch": 0.31197831978319784,
      "step": 1439,
      "training_loss": 7.3946614265441895
    },
    {
      "epoch": 0.31197831978319784,
      "step": 1439,
      "training_loss": 6.360816478729248
    },
    {
      "epoch": 0.31197831978319784,
      "step": 1439,
      "training_loss": 7.1335601806640625
    },
    {
      "epoch": 0.31197831978319784,
      "step": 1439,
      "training_loss": 6.826016426086426
    },
    {
      "epoch": 0.3121951219512195,
      "grad_norm": 15.166536331176758,
      "learning_rate": 1e-05,
      "loss": 6.4791,
      "step": 1440
    },
    {
      "epoch": 0.3121951219512195,
      "step": 1440,
      "training_loss": 6.5235795974731445
    },
    {
      "epoch": 0.3121951219512195,
      "step": 1440,
      "training_loss": 7.4461212158203125
    },
    {
      "epoch": 0.3121951219512195,
      "step": 1440,
      "training_loss": 6.563883304595947
    },
    {
      "epoch": 0.3121951219512195,
      "step": 1440,
      "training_loss": 5.22013521194458
    },
    {
      "epoch": 0.3124119241192412,
      "step": 1441,
      "training_loss": 6.182435035705566
    },
    {
      "epoch": 0.3124119241192412,
      "step": 1441,
      "training_loss": 7.518096923828125
    },
    {
      "epoch": 0.3124119241192412,
      "step": 1441,
      "training_loss": 6.492757797241211
    },
    {
      "epoch": 0.3124119241192412,
      "step": 1441,
      "training_loss": 6.098438739776611
    },
    {
      "epoch": 0.31262872628726285,
      "step": 1442,
      "training_loss": 6.740138530731201
    },
    {
      "epoch": 0.31262872628726285,
      "step": 1442,
      "training_loss": 7.238559246063232
    },
    {
      "epoch": 0.31262872628726285,
      "step": 1442,
      "training_loss": 5.4126434326171875
    },
    {
      "epoch": 0.31262872628726285,
      "step": 1442,
      "training_loss": 6.610109806060791
    },
    {
      "epoch": 0.31284552845528457,
      "step": 1443,
      "training_loss": 7.036413669586182
    },
    {
      "epoch": 0.31284552845528457,
      "step": 1443,
      "training_loss": 7.170127868652344
    },
    {
      "epoch": 0.31284552845528457,
      "step": 1443,
      "training_loss": 6.773323059082031
    },
    {
      "epoch": 0.31284552845528457,
      "step": 1443,
      "training_loss": 6.181596279144287
    },
    {
      "epoch": 0.31306233062330624,
      "grad_norm": 12.90324878692627,
      "learning_rate": 1e-05,
      "loss": 6.5755,
      "step": 1444
    },
    {
      "epoch": 0.31306233062330624,
      "step": 1444,
      "training_loss": 7.299563407897949
    },
    {
      "epoch": 0.31306233062330624,
      "step": 1444,
      "training_loss": 7.652458667755127
    },
    {
      "epoch": 0.31306233062330624,
      "step": 1444,
      "training_loss": 6.505842208862305
    },
    {
      "epoch": 0.31306233062330624,
      "step": 1444,
      "training_loss": 7.537594795227051
    },
    {
      "epoch": 0.3132791327913279,
      "step": 1445,
      "training_loss": 6.086413383483887
    },
    {
      "epoch": 0.3132791327913279,
      "step": 1445,
      "training_loss": 7.0533528327941895
    },
    {
      "epoch": 0.3132791327913279,
      "step": 1445,
      "training_loss": 6.754629135131836
    },
    {
      "epoch": 0.3132791327913279,
      "step": 1445,
      "training_loss": 5.223629951477051
    },
    {
      "epoch": 0.3134959349593496,
      "step": 1446,
      "training_loss": 4.784209251403809
    },
    {
      "epoch": 0.3134959349593496,
      "step": 1446,
      "training_loss": 6.491424560546875
    },
    {
      "epoch": 0.3134959349593496,
      "step": 1446,
      "training_loss": 6.042160511016846
    },
    {
      "epoch": 0.3134959349593496,
      "step": 1446,
      "training_loss": 7.134817123413086
    },
    {
      "epoch": 0.3137127371273713,
      "step": 1447,
      "training_loss": 6.817974090576172
    },
    {
      "epoch": 0.3137127371273713,
      "step": 1447,
      "training_loss": 7.0386505126953125
    },
    {
      "epoch": 0.3137127371273713,
      "step": 1447,
      "training_loss": 6.4588117599487305
    },
    {
      "epoch": 0.3137127371273713,
      "step": 1447,
      "training_loss": 6.45604944229126
    },
    {
      "epoch": 0.31392953929539297,
      "grad_norm": 14.766861915588379,
      "learning_rate": 1e-05,
      "loss": 6.5836,
      "step": 1448
    },
    {
      "epoch": 0.31392953929539297,
      "step": 1448,
      "training_loss": 5.981817722320557
    },
    {
      "epoch": 0.31392953929539297,
      "step": 1448,
      "training_loss": 6.692948341369629
    },
    {
      "epoch": 0.31392953929539297,
      "step": 1448,
      "training_loss": 6.320825099945068
    },
    {
      "epoch": 0.31392953929539297,
      "step": 1448,
      "training_loss": 5.567397117614746
    },
    {
      "epoch": 0.31414634146341464,
      "step": 1449,
      "training_loss": 7.064596652984619
    },
    {
      "epoch": 0.31414634146341464,
      "step": 1449,
      "training_loss": 7.262058258056641
    },
    {
      "epoch": 0.31414634146341464,
      "step": 1449,
      "training_loss": 6.617007732391357
    },
    {
      "epoch": 0.31414634146341464,
      "step": 1449,
      "training_loss": 8.244988441467285
    },
    {
      "epoch": 0.3143631436314363,
      "step": 1450,
      "training_loss": 7.086119174957275
    },
    {
      "epoch": 0.3143631436314363,
      "step": 1450,
      "training_loss": 7.450374603271484
    },
    {
      "epoch": 0.3143631436314363,
      "step": 1450,
      "training_loss": 7.170804023742676
    },
    {
      "epoch": 0.3143631436314363,
      "step": 1450,
      "training_loss": 6.571603775024414
    },
    {
      "epoch": 0.31457994579945797,
      "step": 1451,
      "training_loss": 6.358726501464844
    },
    {
      "epoch": 0.31457994579945797,
      "step": 1451,
      "training_loss": 4.4185638427734375
    },
    {
      "epoch": 0.31457994579945797,
      "step": 1451,
      "training_loss": 7.298398494720459
    },
    {
      "epoch": 0.31457994579945797,
      "step": 1451,
      "training_loss": 7.203731060028076
    },
    {
      "epoch": 0.3147967479674797,
      "grad_norm": 11.403297424316406,
      "learning_rate": 1e-05,
      "loss": 6.7069,
      "step": 1452
    },
    {
      "epoch": 0.3147967479674797,
      "step": 1452,
      "training_loss": 7.747441291809082
    },
    {
      "epoch": 0.3147967479674797,
      "step": 1452,
      "training_loss": 7.816789627075195
    },
    {
      "epoch": 0.3147967479674797,
      "step": 1452,
      "training_loss": 5.535353183746338
    },
    {
      "epoch": 0.3147967479674797,
      "step": 1452,
      "training_loss": 7.418148040771484
    },
    {
      "epoch": 0.31501355013550136,
      "step": 1453,
      "training_loss": 5.968069553375244
    },
    {
      "epoch": 0.31501355013550136,
      "step": 1453,
      "training_loss": 6.002097129821777
    },
    {
      "epoch": 0.31501355013550136,
      "step": 1453,
      "training_loss": 7.954354286193848
    },
    {
      "epoch": 0.31501355013550136,
      "step": 1453,
      "training_loss": 7.8796563148498535
    },
    {
      "epoch": 0.31523035230352303,
      "step": 1454,
      "training_loss": 7.373528003692627
    },
    {
      "epoch": 0.31523035230352303,
      "step": 1454,
      "training_loss": 6.429934978485107
    },
    {
      "epoch": 0.31523035230352303,
      "step": 1454,
      "training_loss": 4.51657247543335
    },
    {
      "epoch": 0.31523035230352303,
      "step": 1454,
      "training_loss": 6.353317737579346
    },
    {
      "epoch": 0.3154471544715447,
      "step": 1455,
      "training_loss": 7.143143653869629
    },
    {
      "epoch": 0.3154471544715447,
      "step": 1455,
      "training_loss": 7.484302997589111
    },
    {
      "epoch": 0.3154471544715447,
      "step": 1455,
      "training_loss": 6.980496406555176
    },
    {
      "epoch": 0.3154471544715447,
      "step": 1455,
      "training_loss": 5.182796001434326
    },
    {
      "epoch": 0.3156639566395664,
      "grad_norm": 10.090580940246582,
      "learning_rate": 1e-05,
      "loss": 6.7366,
      "step": 1456
    },
    {
      "epoch": 0.3156639566395664,
      "step": 1456,
      "training_loss": 6.512559413909912
    },
    {
      "epoch": 0.3156639566395664,
      "step": 1456,
      "training_loss": 6.053907871246338
    },
    {
      "epoch": 0.3156639566395664,
      "step": 1456,
      "training_loss": 7.330726623535156
    },
    {
      "epoch": 0.3156639566395664,
      "step": 1456,
      "training_loss": 6.809665679931641
    },
    {
      "epoch": 0.3158807588075881,
      "step": 1457,
      "training_loss": 7.786816596984863
    },
    {
      "epoch": 0.3158807588075881,
      "step": 1457,
      "training_loss": 5.4143452644348145
    },
    {
      "epoch": 0.3158807588075881,
      "step": 1457,
      "training_loss": 6.861936569213867
    },
    {
      "epoch": 0.3158807588075881,
      "step": 1457,
      "training_loss": 7.109205722808838
    },
    {
      "epoch": 0.31609756097560976,
      "step": 1458,
      "training_loss": 7.1605000495910645
    },
    {
      "epoch": 0.31609756097560976,
      "step": 1458,
      "training_loss": 6.248264789581299
    },
    {
      "epoch": 0.31609756097560976,
      "step": 1458,
      "training_loss": 5.209222793579102
    },
    {
      "epoch": 0.31609756097560976,
      "step": 1458,
      "training_loss": 7.138776779174805
    },
    {
      "epoch": 0.3163143631436314,
      "step": 1459,
      "training_loss": 5.972806453704834
    },
    {
      "epoch": 0.3163143631436314,
      "step": 1459,
      "training_loss": 7.638842582702637
    },
    {
      "epoch": 0.3163143631436314,
      "step": 1459,
      "training_loss": 6.113433837890625
    },
    {
      "epoch": 0.3163143631436314,
      "step": 1459,
      "training_loss": 5.4908905029296875
    },
    {
      "epoch": 0.3165311653116531,
      "grad_norm": 13.058316230773926,
      "learning_rate": 1e-05,
      "loss": 6.5532,
      "step": 1460
    },
    {
      "epoch": 0.3165311653116531,
      "step": 1460,
      "training_loss": 6.892864227294922
    },
    {
      "epoch": 0.3165311653116531,
      "step": 1460,
      "training_loss": 7.812435626983643
    },
    {
      "epoch": 0.3165311653116531,
      "step": 1460,
      "training_loss": 6.573893070220947
    },
    {
      "epoch": 0.3165311653116531,
      "step": 1460,
      "training_loss": 6.603824138641357
    },
    {
      "epoch": 0.3167479674796748,
      "step": 1461,
      "training_loss": 7.823136806488037
    },
    {
      "epoch": 0.3167479674796748,
      "step": 1461,
      "training_loss": 7.217936038970947
    },
    {
      "epoch": 0.3167479674796748,
      "step": 1461,
      "training_loss": 6.736776351928711
    },
    {
      "epoch": 0.3167479674796748,
      "step": 1461,
      "training_loss": 6.0287089347839355
    },
    {
      "epoch": 0.3169647696476965,
      "step": 1462,
      "training_loss": 7.588181972503662
    },
    {
      "epoch": 0.3169647696476965,
      "step": 1462,
      "training_loss": 5.1027116775512695
    },
    {
      "epoch": 0.3169647696476965,
      "step": 1462,
      "training_loss": 6.071611404418945
    },
    {
      "epoch": 0.3169647696476965,
      "step": 1462,
      "training_loss": 5.464017868041992
    },
    {
      "epoch": 0.31718157181571816,
      "step": 1463,
      "training_loss": 6.915159702301025
    },
    {
      "epoch": 0.31718157181571816,
      "step": 1463,
      "training_loss": 7.163126468658447
    },
    {
      "epoch": 0.31718157181571816,
      "step": 1463,
      "training_loss": 5.624140739440918
    },
    {
      "epoch": 0.31718157181571816,
      "step": 1463,
      "training_loss": 6.887110710144043
    },
    {
      "epoch": 0.3173983739837398,
      "grad_norm": 16.12411117553711,
      "learning_rate": 1e-05,
      "loss": 6.6566,
      "step": 1464
    },
    {
      "epoch": 0.3173983739837398,
      "step": 1464,
      "training_loss": 7.7419891357421875
    },
    {
      "epoch": 0.3173983739837398,
      "step": 1464,
      "training_loss": 8.02475357055664
    },
    {
      "epoch": 0.3173983739837398,
      "step": 1464,
      "training_loss": 6.260840892791748
    },
    {
      "epoch": 0.3173983739837398,
      "step": 1464,
      "training_loss": 7.166885852813721
    },
    {
      "epoch": 0.3176151761517615,
      "step": 1465,
      "training_loss": 6.507572650909424
    },
    {
      "epoch": 0.3176151761517615,
      "step": 1465,
      "training_loss": 5.525765419006348
    },
    {
      "epoch": 0.3176151761517615,
      "step": 1465,
      "training_loss": 5.946476459503174
    },
    {
      "epoch": 0.3176151761517615,
      "step": 1465,
      "training_loss": 9.034109115600586
    },
    {
      "epoch": 0.3178319783197832,
      "step": 1466,
      "training_loss": 6.519838809967041
    },
    {
      "epoch": 0.3178319783197832,
      "step": 1466,
      "training_loss": 8.175443649291992
    },
    {
      "epoch": 0.3178319783197832,
      "step": 1466,
      "training_loss": 7.709340572357178
    },
    {
      "epoch": 0.3178319783197832,
      "step": 1466,
      "training_loss": 5.281872749328613
    },
    {
      "epoch": 0.3180487804878049,
      "step": 1467,
      "training_loss": 7.303961277008057
    },
    {
      "epoch": 0.3180487804878049,
      "step": 1467,
      "training_loss": 7.143678665161133
    },
    {
      "epoch": 0.3180487804878049,
      "step": 1467,
      "training_loss": 6.962413311004639
    },
    {
      "epoch": 0.3180487804878049,
      "step": 1467,
      "training_loss": 7.269455909729004
    },
    {
      "epoch": 0.31826558265582655,
      "grad_norm": 12.591389656066895,
      "learning_rate": 1e-05,
      "loss": 7.0359,
      "step": 1468
    },
    {
      "epoch": 0.31826558265582655,
      "step": 1468,
      "training_loss": 7.472229957580566
    },
    {
      "epoch": 0.31826558265582655,
      "step": 1468,
      "training_loss": 6.685512542724609
    },
    {
      "epoch": 0.31826558265582655,
      "step": 1468,
      "training_loss": 7.070766925811768
    },
    {
      "epoch": 0.31826558265582655,
      "step": 1468,
      "training_loss": 7.404540538787842
    },
    {
      "epoch": 0.3184823848238482,
      "step": 1469,
      "training_loss": 7.2875823974609375
    },
    {
      "epoch": 0.3184823848238482,
      "step": 1469,
      "training_loss": 7.195608615875244
    },
    {
      "epoch": 0.3184823848238482,
      "step": 1469,
      "training_loss": 4.284704208374023
    },
    {
      "epoch": 0.3184823848238482,
      "step": 1469,
      "training_loss": 7.514369010925293
    },
    {
      "epoch": 0.31869918699186994,
      "step": 1470,
      "training_loss": 6.9478654861450195
    },
    {
      "epoch": 0.31869918699186994,
      "step": 1470,
      "training_loss": 5.4694504737854
    },
    {
      "epoch": 0.31869918699186994,
      "step": 1470,
      "training_loss": 6.5871052742004395
    },
    {
      "epoch": 0.31869918699186994,
      "step": 1470,
      "training_loss": 6.798605442047119
    },
    {
      "epoch": 0.3189159891598916,
      "step": 1471,
      "training_loss": 5.9544548988342285
    },
    {
      "epoch": 0.3189159891598916,
      "step": 1471,
      "training_loss": 7.090847969055176
    },
    {
      "epoch": 0.3189159891598916,
      "step": 1471,
      "training_loss": 4.9712677001953125
    },
    {
      "epoch": 0.3189159891598916,
      "step": 1471,
      "training_loss": 4.67686128616333
    },
    {
      "epoch": 0.3191327913279133,
      "grad_norm": 12.28015422821045,
      "learning_rate": 1e-05,
      "loss": 6.4632,
      "step": 1472
    },
    {
      "epoch": 0.3191327913279133,
      "step": 1472,
      "training_loss": 3.135718822479248
    },
    {
      "epoch": 0.3191327913279133,
      "step": 1472,
      "training_loss": 6.621153831481934
    },
    {
      "epoch": 0.3191327913279133,
      "step": 1472,
      "training_loss": 7.307872772216797
    },
    {
      "epoch": 0.3191327913279133,
      "step": 1472,
      "training_loss": 7.062936305999756
    },
    {
      "epoch": 0.31934959349593495,
      "step": 1473,
      "training_loss": 6.5947346687316895
    },
    {
      "epoch": 0.31934959349593495,
      "step": 1473,
      "training_loss": 8.264187812805176
    },
    {
      "epoch": 0.31934959349593495,
      "step": 1473,
      "training_loss": 5.659360408782959
    },
    {
      "epoch": 0.31934959349593495,
      "step": 1473,
      "training_loss": 7.270788192749023
    },
    {
      "epoch": 0.3195663956639566,
      "step": 1474,
      "training_loss": 7.490208148956299
    },
    {
      "epoch": 0.3195663956639566,
      "step": 1474,
      "training_loss": 5.738902568817139
    },
    {
      "epoch": 0.3195663956639566,
      "step": 1474,
      "training_loss": 6.617491722106934
    },
    {
      "epoch": 0.3195663956639566,
      "step": 1474,
      "training_loss": 6.96946382522583
    },
    {
      "epoch": 0.31978319783197834,
      "step": 1475,
      "training_loss": 7.545963764190674
    },
    {
      "epoch": 0.31978319783197834,
      "step": 1475,
      "training_loss": 7.181844711303711
    },
    {
      "epoch": 0.31978319783197834,
      "step": 1475,
      "training_loss": 7.104644775390625
    },
    {
      "epoch": 0.31978319783197834,
      "step": 1475,
      "training_loss": 3.491584062576294
    },
    {
      "epoch": 0.32,
      "grad_norm": 13.726191520690918,
      "learning_rate": 1e-05,
      "loss": 6.5036,
      "step": 1476
    },
    {
      "epoch": 0.32,
      "step": 1476,
      "training_loss": 6.370233058929443
    },
    {
      "epoch": 0.32,
      "step": 1476,
      "training_loss": 3.3331167697906494
    },
    {
      "epoch": 0.32,
      "step": 1476,
      "training_loss": 6.55009126663208
    },
    {
      "epoch": 0.32,
      "step": 1476,
      "training_loss": 7.23117208480835
    },
    {
      "epoch": 0.3202168021680217,
      "step": 1477,
      "training_loss": 8.11544132232666
    },
    {
      "epoch": 0.3202168021680217,
      "step": 1477,
      "training_loss": 7.432767391204834
    },
    {
      "epoch": 0.3202168021680217,
      "step": 1477,
      "training_loss": 5.206063747406006
    },
    {
      "epoch": 0.3202168021680217,
      "step": 1477,
      "training_loss": 7.091216087341309
    },
    {
      "epoch": 0.32043360433604334,
      "step": 1478,
      "training_loss": 7.357374668121338
    },
    {
      "epoch": 0.32043360433604334,
      "step": 1478,
      "training_loss": 6.944231033325195
    },
    {
      "epoch": 0.32043360433604334,
      "step": 1478,
      "training_loss": 6.735477924346924
    },
    {
      "epoch": 0.32043360433604334,
      "step": 1478,
      "training_loss": 6.502369403839111
    },
    {
      "epoch": 0.32065040650406507,
      "step": 1479,
      "training_loss": 5.553070068359375
    },
    {
      "epoch": 0.32065040650406507,
      "step": 1479,
      "training_loss": 7.377788543701172
    },
    {
      "epoch": 0.32065040650406507,
      "step": 1479,
      "training_loss": 7.420177459716797
    },
    {
      "epoch": 0.32065040650406507,
      "step": 1479,
      "training_loss": 5.2167205810546875
    },
    {
      "epoch": 0.32086720867208673,
      "grad_norm": 12.496674537658691,
      "learning_rate": 1e-05,
      "loss": 6.5273,
      "step": 1480
    },
    {
      "epoch": 0.32086720867208673,
      "step": 1480,
      "training_loss": 6.575604438781738
    },
    {
      "epoch": 0.32086720867208673,
      "step": 1480,
      "training_loss": 5.246714115142822
    },
    {
      "epoch": 0.32086720867208673,
      "step": 1480,
      "training_loss": 6.420111179351807
    },
    {
      "epoch": 0.32086720867208673,
      "step": 1480,
      "training_loss": 5.2061662673950195
    },
    {
      "epoch": 0.3210840108401084,
      "step": 1481,
      "training_loss": 6.968156814575195
    },
    {
      "epoch": 0.3210840108401084,
      "step": 1481,
      "training_loss": 6.785482406616211
    },
    {
      "epoch": 0.3210840108401084,
      "step": 1481,
      "training_loss": 4.482162952423096
    },
    {
      "epoch": 0.3210840108401084,
      "step": 1481,
      "training_loss": 7.03845739364624
    },
    {
      "epoch": 0.32130081300813007,
      "step": 1482,
      "training_loss": 4.250467300415039
    },
    {
      "epoch": 0.32130081300813007,
      "step": 1482,
      "training_loss": 6.234080791473389
    },
    {
      "epoch": 0.32130081300813007,
      "step": 1482,
      "training_loss": 7.48617696762085
    },
    {
      "epoch": 0.32130081300813007,
      "step": 1482,
      "training_loss": 4.809839725494385
    },
    {
      "epoch": 0.32151761517615174,
      "step": 1483,
      "training_loss": 7.105777740478516
    },
    {
      "epoch": 0.32151761517615174,
      "step": 1483,
      "training_loss": 6.891172885894775
    },
    {
      "epoch": 0.32151761517615174,
      "step": 1483,
      "training_loss": 7.188080787658691
    },
    {
      "epoch": 0.32151761517615174,
      "step": 1483,
      "training_loss": 6.220148086547852
    },
    {
      "epoch": 0.32173441734417346,
      "grad_norm": 14.054383277893066,
      "learning_rate": 1e-05,
      "loss": 6.1818,
      "step": 1484
    },
    {
      "epoch": 0.32173441734417346,
      "step": 1484,
      "training_loss": 6.229039192199707
    },
    {
      "epoch": 0.32173441734417346,
      "step": 1484,
      "training_loss": 7.60812520980835
    },
    {
      "epoch": 0.32173441734417346,
      "step": 1484,
      "training_loss": 7.293359279632568
    },
    {
      "epoch": 0.32173441734417346,
      "step": 1484,
      "training_loss": 6.491476535797119
    },
    {
      "epoch": 0.32195121951219513,
      "step": 1485,
      "training_loss": 5.537410259246826
    },
    {
      "epoch": 0.32195121951219513,
      "step": 1485,
      "training_loss": 5.984094142913818
    },
    {
      "epoch": 0.32195121951219513,
      "step": 1485,
      "training_loss": 7.789951324462891
    },
    {
      "epoch": 0.32195121951219513,
      "step": 1485,
      "training_loss": 6.481825351715088
    },
    {
      "epoch": 0.3221680216802168,
      "step": 1486,
      "training_loss": 8.896051406860352
    },
    {
      "epoch": 0.3221680216802168,
      "step": 1486,
      "training_loss": 6.560357570648193
    },
    {
      "epoch": 0.3221680216802168,
      "step": 1486,
      "training_loss": 5.660211086273193
    },
    {
      "epoch": 0.3221680216802168,
      "step": 1486,
      "training_loss": 6.75779390335083
    },
    {
      "epoch": 0.32238482384823847,
      "step": 1487,
      "training_loss": 7.158713340759277
    },
    {
      "epoch": 0.32238482384823847,
      "step": 1487,
      "training_loss": 7.353158473968506
    },
    {
      "epoch": 0.32238482384823847,
      "step": 1487,
      "training_loss": 5.981078147888184
    },
    {
      "epoch": 0.32238482384823847,
      "step": 1487,
      "training_loss": 6.780857563018799
    },
    {
      "epoch": 0.3226016260162602,
      "grad_norm": 15.493189811706543,
      "learning_rate": 1e-05,
      "loss": 6.7852,
      "step": 1488
    },
    {
      "epoch": 0.3226016260162602,
      "step": 1488,
      "training_loss": 4.9066901206970215
    },
    {
      "epoch": 0.3226016260162602,
      "step": 1488,
      "training_loss": 5.871562480926514
    },
    {
      "epoch": 0.3226016260162602,
      "step": 1488,
      "training_loss": 5.731579303741455
    },
    {
      "epoch": 0.3226016260162602,
      "step": 1488,
      "training_loss": 4.151111602783203
    },
    {
      "epoch": 0.32281842818428186,
      "step": 1489,
      "training_loss": 7.804083347320557
    },
    {
      "epoch": 0.32281842818428186,
      "step": 1489,
      "training_loss": 5.408797740936279
    },
    {
      "epoch": 0.32281842818428186,
      "step": 1489,
      "training_loss": 6.926033973693848
    },
    {
      "epoch": 0.32281842818428186,
      "step": 1489,
      "training_loss": 6.926212787628174
    },
    {
      "epoch": 0.3230352303523035,
      "step": 1490,
      "training_loss": 6.7422966957092285
    },
    {
      "epoch": 0.3230352303523035,
      "step": 1490,
      "training_loss": 7.221024990081787
    },
    {
      "epoch": 0.3230352303523035,
      "step": 1490,
      "training_loss": 6.733432769775391
    },
    {
      "epoch": 0.3230352303523035,
      "step": 1490,
      "training_loss": 6.697493076324463
    },
    {
      "epoch": 0.3232520325203252,
      "step": 1491,
      "training_loss": 6.384793281555176
    },
    {
      "epoch": 0.3232520325203252,
      "step": 1491,
      "training_loss": 5.728231430053711
    },
    {
      "epoch": 0.3232520325203252,
      "step": 1491,
      "training_loss": 8.01009464263916
    },
    {
      "epoch": 0.3232520325203252,
      "step": 1491,
      "training_loss": 5.404452800750732
    },
    {
      "epoch": 0.32346883468834686,
      "grad_norm": 26.30378532409668,
      "learning_rate": 1e-05,
      "loss": 6.2905,
      "step": 1492
    },
    {
      "epoch": 0.32346883468834686,
      "step": 1492,
      "training_loss": 7.72133207321167
    },
    {
      "epoch": 0.32346883468834686,
      "step": 1492,
      "training_loss": 4.9789042472839355
    },
    {
      "epoch": 0.32346883468834686,
      "step": 1492,
      "training_loss": 6.4911932945251465
    },
    {
      "epoch": 0.32346883468834686,
      "step": 1492,
      "training_loss": 5.946134090423584
    },
    {
      "epoch": 0.3236856368563686,
      "step": 1493,
      "training_loss": 7.550821781158447
    },
    {
      "epoch": 0.3236856368563686,
      "step": 1493,
      "training_loss": 6.750415802001953
    },
    {
      "epoch": 0.3236856368563686,
      "step": 1493,
      "training_loss": 6.469501972198486
    },
    {
      "epoch": 0.3236856368563686,
      "step": 1493,
      "training_loss": 6.768886566162109
    },
    {
      "epoch": 0.32390243902439025,
      "step": 1494,
      "training_loss": 4.163028717041016
    },
    {
      "epoch": 0.32390243902439025,
      "step": 1494,
      "training_loss": 6.754677772521973
    },
    {
      "epoch": 0.32390243902439025,
      "step": 1494,
      "training_loss": 7.660046100616455
    },
    {
      "epoch": 0.32390243902439025,
      "step": 1494,
      "training_loss": 4.643220901489258
    },
    {
      "epoch": 0.3241192411924119,
      "step": 1495,
      "training_loss": 7.296154499053955
    },
    {
      "epoch": 0.3241192411924119,
      "step": 1495,
      "training_loss": 7.132160186767578
    },
    {
      "epoch": 0.3241192411924119,
      "step": 1495,
      "training_loss": 5.984346389770508
    },
    {
      "epoch": 0.3241192411924119,
      "step": 1495,
      "training_loss": 8.009449005126953
    },
    {
      "epoch": 0.3243360433604336,
      "grad_norm": 14.649179458618164,
      "learning_rate": 1e-05,
      "loss": 6.52,
      "step": 1496
    },
    {
      "epoch": 0.3243360433604336,
      "step": 1496,
      "training_loss": 7.812135696411133
    },
    {
      "epoch": 0.3243360433604336,
      "step": 1496,
      "training_loss": 6.839138984680176
    },
    {
      "epoch": 0.3243360433604336,
      "step": 1496,
      "training_loss": 4.857730388641357
    },
    {
      "epoch": 0.3243360433604336,
      "step": 1496,
      "training_loss": 6.711650371551514
    },
    {
      "epoch": 0.32455284552845526,
      "step": 1497,
      "training_loss": 4.095984935760498
    },
    {
      "epoch": 0.32455284552845526,
      "step": 1497,
      "training_loss": 6.544941425323486
    },
    {
      "epoch": 0.32455284552845526,
      "step": 1497,
      "training_loss": 7.956439018249512
    },
    {
      "epoch": 0.32455284552845526,
      "step": 1497,
      "training_loss": 6.6632609367370605
    },
    {
      "epoch": 0.324769647696477,
      "step": 1498,
      "training_loss": 6.2543487548828125
    },
    {
      "epoch": 0.324769647696477,
      "step": 1498,
      "training_loss": 5.7205328941345215
    },
    {
      "epoch": 0.324769647696477,
      "step": 1498,
      "training_loss": 7.442310333251953
    },
    {
      "epoch": 0.324769647696477,
      "step": 1498,
      "training_loss": 6.914670467376709
    },
    {
      "epoch": 0.32498644986449865,
      "step": 1499,
      "training_loss": 5.4024248123168945
    },
    {
      "epoch": 0.32498644986449865,
      "step": 1499,
      "training_loss": 5.312464237213135
    },
    {
      "epoch": 0.32498644986449865,
      "step": 1499,
      "training_loss": 6.539517879486084
    },
    {
      "epoch": 0.32498644986449865,
      "step": 1499,
      "training_loss": 7.306922912597656
    },
    {
      "epoch": 0.3252032520325203,
      "grad_norm": 10.849408149719238,
      "learning_rate": 1e-05,
      "loss": 6.3984,
      "step": 1500
    },
    {
      "epoch": 0.3252032520325203,
      "step": 1500,
      "training_loss": 6.705276012420654
    },
    {
      "epoch": 0.3252032520325203,
      "step": 1500,
      "training_loss": 6.666539669036865
    },
    {
      "epoch": 0.3252032520325203,
      "step": 1500,
      "training_loss": 7.378455638885498
    },
    {
      "epoch": 0.3252032520325203,
      "step": 1500,
      "training_loss": 6.83196496963501
    },
    {
      "epoch": 0.325420054200542,
      "step": 1501,
      "training_loss": 6.594230651855469
    },
    {
      "epoch": 0.325420054200542,
      "step": 1501,
      "training_loss": 6.804318904876709
    },
    {
      "epoch": 0.325420054200542,
      "step": 1501,
      "training_loss": 6.382983684539795
    },
    {
      "epoch": 0.325420054200542,
      "step": 1501,
      "training_loss": 7.19168758392334
    },
    {
      "epoch": 0.3256368563685637,
      "step": 1502,
      "training_loss": 8.019336700439453
    },
    {
      "epoch": 0.3256368563685637,
      "step": 1502,
      "training_loss": 7.262879371643066
    },
    {
      "epoch": 0.3256368563685637,
      "step": 1502,
      "training_loss": 4.067432880401611
    },
    {
      "epoch": 0.3256368563685637,
      "step": 1502,
      "training_loss": 6.993190765380859
    },
    {
      "epoch": 0.3258536585365854,
      "step": 1503,
      "training_loss": 5.730326175689697
    },
    {
      "epoch": 0.3258536585365854,
      "step": 1503,
      "training_loss": 6.716270446777344
    },
    {
      "epoch": 0.3258536585365854,
      "step": 1503,
      "training_loss": 9.302720069885254
    },
    {
      "epoch": 0.3258536585365854,
      "step": 1503,
      "training_loss": 7.967455863952637
    },
    {
      "epoch": 0.32607046070460705,
      "grad_norm": 16.995559692382812,
      "learning_rate": 1e-05,
      "loss": 6.9134,
      "step": 1504
    },
    {
      "epoch": 0.32607046070460705,
      "step": 1504,
      "training_loss": 7.0252180099487305
    },
    {
      "epoch": 0.32607046070460705,
      "step": 1504,
      "training_loss": 7.691977500915527
    },
    {
      "epoch": 0.32607046070460705,
      "step": 1504,
      "training_loss": 5.674160480499268
    },
    {
      "epoch": 0.32607046070460705,
      "step": 1504,
      "training_loss": 7.553293228149414
    },
    {
      "epoch": 0.3262872628726287,
      "step": 1505,
      "training_loss": 7.6336283683776855
    },
    {
      "epoch": 0.3262872628726287,
      "step": 1505,
      "training_loss": 7.044039249420166
    },
    {
      "epoch": 0.3262872628726287,
      "step": 1505,
      "training_loss": 8.126368522644043
    },
    {
      "epoch": 0.3262872628726287,
      "step": 1505,
      "training_loss": 7.23314094543457
    },
    {
      "epoch": 0.3265040650406504,
      "step": 1506,
      "training_loss": 6.679322719573975
    },
    {
      "epoch": 0.3265040650406504,
      "step": 1506,
      "training_loss": 7.115769386291504
    },
    {
      "epoch": 0.3265040650406504,
      "step": 1506,
      "training_loss": 6.755956172943115
    },
    {
      "epoch": 0.3265040650406504,
      "step": 1506,
      "training_loss": 7.338284015655518
    },
    {
      "epoch": 0.3267208672086721,
      "step": 1507,
      "training_loss": 4.960415363311768
    },
    {
      "epoch": 0.3267208672086721,
      "step": 1507,
      "training_loss": 5.962301254272461
    },
    {
      "epoch": 0.3267208672086721,
      "step": 1507,
      "training_loss": 6.359477996826172
    },
    {
      "epoch": 0.3267208672086721,
      "step": 1507,
      "training_loss": 7.28242826461792
    },
    {
      "epoch": 0.3269376693766938,
      "grad_norm": 15.929144859313965,
      "learning_rate": 1e-05,
      "loss": 6.9022,
      "step": 1508
    },
    {
      "epoch": 0.3269376693766938,
      "step": 1508,
      "training_loss": 7.2293195724487305
    },
    {
      "epoch": 0.3269376693766938,
      "step": 1508,
      "training_loss": 8.249763488769531
    },
    {
      "epoch": 0.3269376693766938,
      "step": 1508,
      "training_loss": 6.96155309677124
    },
    {
      "epoch": 0.3269376693766938,
      "step": 1508,
      "training_loss": 6.707022190093994
    },
    {
      "epoch": 0.32715447154471544,
      "step": 1509,
      "training_loss": 6.4891839027404785
    },
    {
      "epoch": 0.32715447154471544,
      "step": 1509,
      "training_loss": 7.1932053565979
    },
    {
      "epoch": 0.32715447154471544,
      "step": 1509,
      "training_loss": 7.567944526672363
    },
    {
      "epoch": 0.32715447154471544,
      "step": 1509,
      "training_loss": 7.33799409866333
    },
    {
      "epoch": 0.3273712737127371,
      "step": 1510,
      "training_loss": 4.898310661315918
    },
    {
      "epoch": 0.3273712737127371,
      "step": 1510,
      "training_loss": 7.016164302825928
    },
    {
      "epoch": 0.3273712737127371,
      "step": 1510,
      "training_loss": 6.29205322265625
    },
    {
      "epoch": 0.3273712737127371,
      "step": 1510,
      "training_loss": 7.090784549713135
    },
    {
      "epoch": 0.32758807588075883,
      "step": 1511,
      "training_loss": 7.3730316162109375
    },
    {
      "epoch": 0.32758807588075883,
      "step": 1511,
      "training_loss": 7.39955997467041
    },
    {
      "epoch": 0.32758807588075883,
      "step": 1511,
      "training_loss": 6.746002674102783
    },
    {
      "epoch": 0.32758807588075883,
      "step": 1511,
      "training_loss": 5.185479164123535
    },
    {
      "epoch": 0.3278048780487805,
      "grad_norm": 14.627893447875977,
      "learning_rate": 1e-05,
      "loss": 6.8586,
      "step": 1512
    },
    {
      "epoch": 0.3278048780487805,
      "step": 1512,
      "training_loss": 5.043091773986816
    },
    {
      "epoch": 0.3278048780487805,
      "step": 1512,
      "training_loss": 7.875112056732178
    },
    {
      "epoch": 0.3278048780487805,
      "step": 1512,
      "training_loss": 4.3977370262146
    },
    {
      "epoch": 0.3278048780487805,
      "step": 1512,
      "training_loss": 5.413111209869385
    },
    {
      "epoch": 0.32802168021680217,
      "step": 1513,
      "training_loss": 5.885917663574219
    },
    {
      "epoch": 0.32802168021680217,
      "step": 1513,
      "training_loss": 7.273772239685059
    },
    {
      "epoch": 0.32802168021680217,
      "step": 1513,
      "training_loss": 7.061177730560303
    },
    {
      "epoch": 0.32802168021680217,
      "step": 1513,
      "training_loss": 5.915901184082031
    },
    {
      "epoch": 0.32823848238482384,
      "step": 1514,
      "training_loss": 6.408389568328857
    },
    {
      "epoch": 0.32823848238482384,
      "step": 1514,
      "training_loss": 7.523591995239258
    },
    {
      "epoch": 0.32823848238482384,
      "step": 1514,
      "training_loss": 6.339154243469238
    },
    {
      "epoch": 0.32823848238482384,
      "step": 1514,
      "training_loss": 7.0104217529296875
    },
    {
      "epoch": 0.3284552845528455,
      "step": 1515,
      "training_loss": 7.607236385345459
    },
    {
      "epoch": 0.3284552845528455,
      "step": 1515,
      "training_loss": 6.007636547088623
    },
    {
      "epoch": 0.3284552845528455,
      "step": 1515,
      "training_loss": 6.994683742523193
    },
    {
      "epoch": 0.3284552845528455,
      "step": 1515,
      "training_loss": 7.125250816345215
    },
    {
      "epoch": 0.32867208672086723,
      "grad_norm": 11.715949058532715,
      "learning_rate": 1e-05,
      "loss": 6.4926,
      "step": 1516
    },
    {
      "epoch": 0.32867208672086723,
      "step": 1516,
      "training_loss": 5.890319347381592
    },
    {
      "epoch": 0.32867208672086723,
      "step": 1516,
      "training_loss": 6.007751941680908
    },
    {
      "epoch": 0.32867208672086723,
      "step": 1516,
      "training_loss": 6.697409629821777
    },
    {
      "epoch": 0.32867208672086723,
      "step": 1516,
      "training_loss": 6.5266499519348145
    },
    {
      "epoch": 0.3288888888888889,
      "step": 1517,
      "training_loss": 7.295187950134277
    },
    {
      "epoch": 0.3288888888888889,
      "step": 1517,
      "training_loss": 7.451169967651367
    },
    {
      "epoch": 0.3288888888888889,
      "step": 1517,
      "training_loss": 6.561813831329346
    },
    {
      "epoch": 0.3288888888888889,
      "step": 1517,
      "training_loss": 8.734918594360352
    },
    {
      "epoch": 0.32910569105691057,
      "step": 1518,
      "training_loss": 6.9277167320251465
    },
    {
      "epoch": 0.32910569105691057,
      "step": 1518,
      "training_loss": 7.259658336639404
    },
    {
      "epoch": 0.32910569105691057,
      "step": 1518,
      "training_loss": 5.929673671722412
    },
    {
      "epoch": 0.32910569105691057,
      "step": 1518,
      "training_loss": 6.699129581451416
    },
    {
      "epoch": 0.32932249322493223,
      "step": 1519,
      "training_loss": 6.467686176300049
    },
    {
      "epoch": 0.32932249322493223,
      "step": 1519,
      "training_loss": 6.981428623199463
    },
    {
      "epoch": 0.32932249322493223,
      "step": 1519,
      "training_loss": 5.34085750579834
    },
    {
      "epoch": 0.32932249322493223,
      "step": 1519,
      "training_loss": 4.611717224121094
    },
    {
      "epoch": 0.32953929539295396,
      "grad_norm": 18.153444290161133,
      "learning_rate": 1e-05,
      "loss": 6.5864,
      "step": 1520
    },
    {
      "epoch": 0.32953929539295396,
      "step": 1520,
      "training_loss": 6.889308929443359
    },
    {
      "epoch": 0.32953929539295396,
      "step": 1520,
      "training_loss": 7.59757661819458
    },
    {
      "epoch": 0.32953929539295396,
      "step": 1520,
      "training_loss": 7.469997406005859
    },
    {
      "epoch": 0.32953929539295396,
      "step": 1520,
      "training_loss": 7.181765079498291
    },
    {
      "epoch": 0.3297560975609756,
      "step": 1521,
      "training_loss": 6.749255180358887
    },
    {
      "epoch": 0.3297560975609756,
      "step": 1521,
      "training_loss": 6.765903472900391
    },
    {
      "epoch": 0.3297560975609756,
      "step": 1521,
      "training_loss": 6.741714000701904
    },
    {
      "epoch": 0.3297560975609756,
      "step": 1521,
      "training_loss": 5.6029276847839355
    },
    {
      "epoch": 0.3299728997289973,
      "step": 1522,
      "training_loss": 5.730589389801025
    },
    {
      "epoch": 0.3299728997289973,
      "step": 1522,
      "training_loss": 7.213761329650879
    },
    {
      "epoch": 0.3299728997289973,
      "step": 1522,
      "training_loss": 6.608771324157715
    },
    {
      "epoch": 0.3299728997289973,
      "step": 1522,
      "training_loss": 5.874973297119141
    },
    {
      "epoch": 0.33018970189701896,
      "step": 1523,
      "training_loss": 7.049635887145996
    },
    {
      "epoch": 0.33018970189701896,
      "step": 1523,
      "training_loss": 7.928752899169922
    },
    {
      "epoch": 0.33018970189701896,
      "step": 1523,
      "training_loss": 6.261691570281982
    },
    {
      "epoch": 0.33018970189701896,
      "step": 1523,
      "training_loss": 7.526402473449707
    },
    {
      "epoch": 0.33040650406504063,
      "grad_norm": 10.097023010253906,
      "learning_rate": 1e-05,
      "loss": 6.8246,
      "step": 1524
    },
    {
      "epoch": 0.33040650406504063,
      "step": 1524,
      "training_loss": 7.568277835845947
    },
    {
      "epoch": 0.33040650406504063,
      "step": 1524,
      "training_loss": 5.41591215133667
    },
    {
      "epoch": 0.33040650406504063,
      "step": 1524,
      "training_loss": 7.258732795715332
    },
    {
      "epoch": 0.33040650406504063,
      "step": 1524,
      "training_loss": 5.090357303619385
    },
    {
      "epoch": 0.33062330623306235,
      "step": 1525,
      "training_loss": 4.0417585372924805
    },
    {
      "epoch": 0.33062330623306235,
      "step": 1525,
      "training_loss": 7.053063869476318
    },
    {
      "epoch": 0.33062330623306235,
      "step": 1525,
      "training_loss": 6.339320659637451
    },
    {
      "epoch": 0.33062330623306235,
      "step": 1525,
      "training_loss": 8.145771980285645
    },
    {
      "epoch": 0.330840108401084,
      "step": 1526,
      "training_loss": 5.349514484405518
    },
    {
      "epoch": 0.330840108401084,
      "step": 1526,
      "training_loss": 6.0651021003723145
    },
    {
      "epoch": 0.330840108401084,
      "step": 1526,
      "training_loss": 7.015992164611816
    },
    {
      "epoch": 0.330840108401084,
      "step": 1526,
      "training_loss": 5.985637664794922
    },
    {
      "epoch": 0.3310569105691057,
      "step": 1527,
      "training_loss": 7.3345136642456055
    },
    {
      "epoch": 0.3310569105691057,
      "step": 1527,
      "training_loss": 5.582236289978027
    },
    {
      "epoch": 0.3310569105691057,
      "step": 1527,
      "training_loss": 6.895026206970215
    },
    {
      "epoch": 0.3310569105691057,
      "step": 1527,
      "training_loss": 6.674848556518555
    },
    {
      "epoch": 0.33127371273712736,
      "grad_norm": 17.044631958007812,
      "learning_rate": 1e-05,
      "loss": 6.3635,
      "step": 1528
    },
    {
      "epoch": 0.33127371273712736,
      "step": 1528,
      "training_loss": 6.049299240112305
    },
    {
      "epoch": 0.33127371273712736,
      "step": 1528,
      "training_loss": 6.271190643310547
    },
    {
      "epoch": 0.33127371273712736,
      "step": 1528,
      "training_loss": 6.427387714385986
    },
    {
      "epoch": 0.33127371273712736,
      "step": 1528,
      "training_loss": 7.384654521942139
    },
    {
      "epoch": 0.331490514905149,
      "step": 1529,
      "training_loss": 6.359354496002197
    },
    {
      "epoch": 0.331490514905149,
      "step": 1529,
      "training_loss": 7.330630302429199
    },
    {
      "epoch": 0.331490514905149,
      "step": 1529,
      "training_loss": 6.900172233581543
    },
    {
      "epoch": 0.331490514905149,
      "step": 1529,
      "training_loss": 5.5472002029418945
    },
    {
      "epoch": 0.33170731707317075,
      "step": 1530,
      "training_loss": 5.829732894897461
    },
    {
      "epoch": 0.33170731707317075,
      "step": 1530,
      "training_loss": 6.706548690795898
    },
    {
      "epoch": 0.33170731707317075,
      "step": 1530,
      "training_loss": 7.363600254058838
    },
    {
      "epoch": 0.33170731707317075,
      "step": 1530,
      "training_loss": 6.71193265914917
    },
    {
      "epoch": 0.3319241192411924,
      "step": 1531,
      "training_loss": 6.024480819702148
    },
    {
      "epoch": 0.3319241192411924,
      "step": 1531,
      "training_loss": 6.785518169403076
    },
    {
      "epoch": 0.3319241192411924,
      "step": 1531,
      "training_loss": 7.57243013381958
    },
    {
      "epoch": 0.3319241192411924,
      "step": 1531,
      "training_loss": 5.131168842315674
    },
    {
      "epoch": 0.3321409214092141,
      "grad_norm": 11.604549407958984,
      "learning_rate": 1e-05,
      "loss": 6.5247,
      "step": 1532
    },
    {
      "epoch": 0.3321409214092141,
      "step": 1532,
      "training_loss": 4.573686599731445
    },
    {
      "epoch": 0.3321409214092141,
      "step": 1532,
      "training_loss": 6.5967326164245605
    },
    {
      "epoch": 0.3321409214092141,
      "step": 1532,
      "training_loss": 8.018189430236816
    },
    {
      "epoch": 0.3321409214092141,
      "step": 1532,
      "training_loss": 6.4728474617004395
    },
    {
      "epoch": 0.33235772357723575,
      "step": 1533,
      "training_loss": 7.31658935546875
    },
    {
      "epoch": 0.33235772357723575,
      "step": 1533,
      "training_loss": 5.995903968811035
    },
    {
      "epoch": 0.33235772357723575,
      "step": 1533,
      "training_loss": 7.737021446228027
    },
    {
      "epoch": 0.33235772357723575,
      "step": 1533,
      "training_loss": 6.767191410064697
    },
    {
      "epoch": 0.3325745257452575,
      "step": 1534,
      "training_loss": 5.079423904418945
    },
    {
      "epoch": 0.3325745257452575,
      "step": 1534,
      "training_loss": 7.013541221618652
    },
    {
      "epoch": 0.3325745257452575,
      "step": 1534,
      "training_loss": 6.578226089477539
    },
    {
      "epoch": 0.3325745257452575,
      "step": 1534,
      "training_loss": 7.63280725479126
    },
    {
      "epoch": 0.33279132791327914,
      "step": 1535,
      "training_loss": 5.5845746994018555
    },
    {
      "epoch": 0.33279132791327914,
      "step": 1535,
      "training_loss": 7.074036121368408
    },
    {
      "epoch": 0.33279132791327914,
      "step": 1535,
      "training_loss": 5.908144474029541
    },
    {
      "epoch": 0.33279132791327914,
      "step": 1535,
      "training_loss": 5.428346157073975
    },
    {
      "epoch": 0.3330081300813008,
      "grad_norm": 14.001886367797852,
      "learning_rate": 1e-05,
      "loss": 6.4861,
      "step": 1536
    },
    {
      "epoch": 0.3330081300813008,
      "step": 1536,
      "training_loss": 5.239570140838623
    },
    {
      "epoch": 0.3330081300813008,
      "step": 1536,
      "training_loss": 6.858803749084473
    },
    {
      "epoch": 0.3330081300813008,
      "step": 1536,
      "training_loss": 8.021035194396973
    },
    {
      "epoch": 0.3330081300813008,
      "step": 1536,
      "training_loss": 6.9576826095581055
    },
    {
      "epoch": 0.3332249322493225,
      "step": 1537,
      "training_loss": 7.42371940612793
    },
    {
      "epoch": 0.3332249322493225,
      "step": 1537,
      "training_loss": 5.6319379806518555
    },
    {
      "epoch": 0.3332249322493225,
      "step": 1537,
      "training_loss": 7.900148868560791
    },
    {
      "epoch": 0.3332249322493225,
      "step": 1537,
      "training_loss": 6.823555946350098
    },
    {
      "epoch": 0.33344173441734415,
      "step": 1538,
      "training_loss": 6.859564781188965
    },
    {
      "epoch": 0.33344173441734415,
      "step": 1538,
      "training_loss": 7.449663162231445
    },
    {
      "epoch": 0.33344173441734415,
      "step": 1538,
      "training_loss": 5.450275897979736
    },
    {
      "epoch": 0.33344173441734415,
      "step": 1538,
      "training_loss": 6.190432548522949
    },
    {
      "epoch": 0.3336585365853659,
      "step": 1539,
      "training_loss": 7.390794277191162
    },
    {
      "epoch": 0.3336585365853659,
      "step": 1539,
      "training_loss": 6.963996887207031
    },
    {
      "epoch": 0.3336585365853659,
      "step": 1539,
      "training_loss": 6.23102331161499
    },
    {
      "epoch": 0.3336585365853659,
      "step": 1539,
      "training_loss": 4.805397987365723
    },
    {
      "epoch": 0.33387533875338754,
      "grad_norm": 13.090044975280762,
      "learning_rate": 1e-05,
      "loss": 6.6374,
      "step": 1540
    },
    {
      "epoch": 0.33387533875338754,
      "step": 1540,
      "training_loss": 6.362005233764648
    },
    {
      "epoch": 0.33387533875338754,
      "step": 1540,
      "training_loss": 6.780503749847412
    },
    {
      "epoch": 0.33387533875338754,
      "step": 1540,
      "training_loss": 5.751678943634033
    },
    {
      "epoch": 0.33387533875338754,
      "step": 1540,
      "training_loss": 6.963526725769043
    },
    {
      "epoch": 0.3340921409214092,
      "step": 1541,
      "training_loss": 6.624166488647461
    },
    {
      "epoch": 0.3340921409214092,
      "step": 1541,
      "training_loss": 6.036715984344482
    },
    {
      "epoch": 0.3340921409214092,
      "step": 1541,
      "training_loss": 7.647428512573242
    },
    {
      "epoch": 0.3340921409214092,
      "step": 1541,
      "training_loss": 8.09119987487793
    },
    {
      "epoch": 0.3343089430894309,
      "step": 1542,
      "training_loss": 7.103879451751709
    },
    {
      "epoch": 0.3343089430894309,
      "step": 1542,
      "training_loss": 6.784028053283691
    },
    {
      "epoch": 0.3343089430894309,
      "step": 1542,
      "training_loss": 7.801358222961426
    },
    {
      "epoch": 0.3343089430894309,
      "step": 1542,
      "training_loss": 6.7457146644592285
    },
    {
      "epoch": 0.3345257452574526,
      "step": 1543,
      "training_loss": 6.205838203430176
    },
    {
      "epoch": 0.3345257452574526,
      "step": 1543,
      "training_loss": 7.396963596343994
    },
    {
      "epoch": 0.3345257452574526,
      "step": 1543,
      "training_loss": 7.80596923828125
    },
    {
      "epoch": 0.3345257452574526,
      "step": 1543,
      "training_loss": 7.401353359222412
    },
    {
      "epoch": 0.33474254742547427,
      "grad_norm": 12.181587219238281,
      "learning_rate": 1e-05,
      "loss": 6.9689,
      "step": 1544
    },
    {
      "epoch": 0.33474254742547427,
      "step": 1544,
      "training_loss": 8.737383842468262
    },
    {
      "epoch": 0.33474254742547427,
      "step": 1544,
      "training_loss": 7.794888019561768
    },
    {
      "epoch": 0.33474254742547427,
      "step": 1544,
      "training_loss": 6.8642778396606445
    },
    {
      "epoch": 0.33474254742547427,
      "step": 1544,
      "training_loss": 7.038503646850586
    },
    {
      "epoch": 0.33495934959349594,
      "step": 1545,
      "training_loss": 6.790425777435303
    },
    {
      "epoch": 0.33495934959349594,
      "step": 1545,
      "training_loss": 6.959658622741699
    },
    {
      "epoch": 0.33495934959349594,
      "step": 1545,
      "training_loss": 5.379873752593994
    },
    {
      "epoch": 0.33495934959349594,
      "step": 1545,
      "training_loss": 5.944204330444336
    },
    {
      "epoch": 0.3351761517615176,
      "step": 1546,
      "training_loss": 6.137747764587402
    },
    {
      "epoch": 0.3351761517615176,
      "step": 1546,
      "training_loss": 4.185013294219971
    },
    {
      "epoch": 0.3351761517615176,
      "step": 1546,
      "training_loss": 4.87550687789917
    },
    {
      "epoch": 0.3351761517615176,
      "step": 1546,
      "training_loss": 6.870575904846191
    },
    {
      "epoch": 0.3353929539295393,
      "step": 1547,
      "training_loss": 6.043983459472656
    },
    {
      "epoch": 0.3353929539295393,
      "step": 1547,
      "training_loss": 6.586652755737305
    },
    {
      "epoch": 0.3353929539295393,
      "step": 1547,
      "training_loss": 7.825027942657471
    },
    {
      "epoch": 0.3353929539295393,
      "step": 1547,
      "training_loss": 3.848487377166748
    },
    {
      "epoch": 0.335609756097561,
      "grad_norm": 16.79572105407715,
      "learning_rate": 1e-05,
      "loss": 6.3676,
      "step": 1548
    },
    {
      "epoch": 0.335609756097561,
      "step": 1548,
      "training_loss": 6.681548595428467
    },
    {
      "epoch": 0.335609756097561,
      "step": 1548,
      "training_loss": 6.1092915534973145
    },
    {
      "epoch": 0.335609756097561,
      "step": 1548,
      "training_loss": 7.693299770355225
    },
    {
      "epoch": 0.335609756097561,
      "step": 1548,
      "training_loss": 6.842706680297852
    },
    {
      "epoch": 0.33582655826558266,
      "step": 1549,
      "training_loss": 6.619765758514404
    },
    {
      "epoch": 0.33582655826558266,
      "step": 1549,
      "training_loss": 6.4904398918151855
    },
    {
      "epoch": 0.33582655826558266,
      "step": 1549,
      "training_loss": 7.274086952209473
    },
    {
      "epoch": 0.33582655826558266,
      "step": 1549,
      "training_loss": 6.7858171463012695
    },
    {
      "epoch": 0.33604336043360433,
      "step": 1550,
      "training_loss": 5.51044225692749
    },
    {
      "epoch": 0.33604336043360433,
      "step": 1550,
      "training_loss": 6.683871269226074
    },
    {
      "epoch": 0.33604336043360433,
      "step": 1550,
      "training_loss": 6.8982439041137695
    },
    {
      "epoch": 0.33604336043360433,
      "step": 1550,
      "training_loss": 7.274135112762451
    },
    {
      "epoch": 0.336260162601626,
      "step": 1551,
      "training_loss": 5.9798173904418945
    },
    {
      "epoch": 0.336260162601626,
      "step": 1551,
      "training_loss": 6.448479175567627
    },
    {
      "epoch": 0.336260162601626,
      "step": 1551,
      "training_loss": 6.2800397872924805
    },
    {
      "epoch": 0.336260162601626,
      "step": 1551,
      "training_loss": 5.68648624420166
    },
    {
      "epoch": 0.3364769647696477,
      "grad_norm": 12.480437278747559,
      "learning_rate": 1e-05,
      "loss": 6.5787,
      "step": 1552
    },
    {
      "epoch": 0.3364769647696477,
      "step": 1552,
      "training_loss": 7.993825912475586
    },
    {
      "epoch": 0.3364769647696477,
      "step": 1552,
      "training_loss": 8.509242057800293
    },
    {
      "epoch": 0.3364769647696477,
      "step": 1552,
      "training_loss": 6.6246657371521
    },
    {
      "epoch": 0.3364769647696477,
      "step": 1552,
      "training_loss": 7.029420375823975
    },
    {
      "epoch": 0.3366937669376694,
      "step": 1553,
      "training_loss": 7.586154460906982
    },
    {
      "epoch": 0.3366937669376694,
      "step": 1553,
      "training_loss": 5.757575511932373
    },
    {
      "epoch": 0.3366937669376694,
      "step": 1553,
      "training_loss": 6.1786370277404785
    },
    {
      "epoch": 0.3366937669376694,
      "step": 1553,
      "training_loss": 5.285924434661865
    },
    {
      "epoch": 0.33691056910569106,
      "step": 1554,
      "training_loss": 6.544537544250488
    },
    {
      "epoch": 0.33691056910569106,
      "step": 1554,
      "training_loss": 6.036379814147949
    },
    {
      "epoch": 0.33691056910569106,
      "step": 1554,
      "training_loss": 6.964249134063721
    },
    {
      "epoch": 0.33691056910569106,
      "step": 1554,
      "training_loss": 6.1889801025390625
    },
    {
      "epoch": 0.33712737127371273,
      "step": 1555,
      "training_loss": 7.137011528015137
    },
    {
      "epoch": 0.33712737127371273,
      "step": 1555,
      "training_loss": 6.261411666870117
    },
    {
      "epoch": 0.33712737127371273,
      "step": 1555,
      "training_loss": 4.753962516784668
    },
    {
      "epoch": 0.33712737127371273,
      "step": 1555,
      "training_loss": 8.178893089294434
    },
    {
      "epoch": 0.3373441734417344,
      "grad_norm": 14.752138137817383,
      "learning_rate": 1e-05,
      "loss": 6.6894,
      "step": 1556
    },
    {
      "epoch": 0.3373441734417344,
      "step": 1556,
      "training_loss": 6.365461349487305
    },
    {
      "epoch": 0.3373441734417344,
      "step": 1556,
      "training_loss": 5.842243194580078
    },
    {
      "epoch": 0.3373441734417344,
      "step": 1556,
      "training_loss": 7.053430557250977
    },
    {
      "epoch": 0.3373441734417344,
      "step": 1556,
      "training_loss": 6.088119029998779
    },
    {
      "epoch": 0.3375609756097561,
      "step": 1557,
      "training_loss": 5.296848773956299
    },
    {
      "epoch": 0.3375609756097561,
      "step": 1557,
      "training_loss": 4.821465015411377
    },
    {
      "epoch": 0.3375609756097561,
      "step": 1557,
      "training_loss": 4.555234432220459
    },
    {
      "epoch": 0.3375609756097561,
      "step": 1557,
      "training_loss": 7.2260050773620605
    },
    {
      "epoch": 0.3377777777777778,
      "step": 1558,
      "training_loss": 6.175177097320557
    },
    {
      "epoch": 0.3377777777777778,
      "step": 1558,
      "training_loss": 7.528772830963135
    },
    {
      "epoch": 0.3377777777777778,
      "step": 1558,
      "training_loss": 7.548313617706299
    },
    {
      "epoch": 0.3377777777777778,
      "step": 1558,
      "training_loss": 6.621988296508789
    },
    {
      "epoch": 0.33799457994579946,
      "step": 1559,
      "training_loss": 4.372387409210205
    },
    {
      "epoch": 0.33799457994579946,
      "step": 1559,
      "training_loss": 7.147729873657227
    },
    {
      "epoch": 0.33799457994579946,
      "step": 1559,
      "training_loss": 7.737164497375488
    },
    {
      "epoch": 0.33799457994579946,
      "step": 1559,
      "training_loss": 7.565054893493652
    },
    {
      "epoch": 0.3382113821138211,
      "grad_norm": 11.770475387573242,
      "learning_rate": 1e-05,
      "loss": 6.3716,
      "step": 1560
    },
    {
      "epoch": 0.3382113821138211,
      "step": 1560,
      "training_loss": 7.9378581047058105
    },
    {
      "epoch": 0.3382113821138211,
      "step": 1560,
      "training_loss": 4.708425045013428
    },
    {
      "epoch": 0.3382113821138211,
      "step": 1560,
      "training_loss": 6.954696178436279
    },
    {
      "epoch": 0.3382113821138211,
      "step": 1560,
      "training_loss": 6.95074987411499
    },
    {
      "epoch": 0.3384281842818428,
      "step": 1561,
      "training_loss": 6.559878826141357
    },
    {
      "epoch": 0.3384281842818428,
      "step": 1561,
      "training_loss": 6.473276138305664
    },
    {
      "epoch": 0.3384281842818428,
      "step": 1561,
      "training_loss": 6.3430867195129395
    },
    {
      "epoch": 0.3384281842818428,
      "step": 1561,
      "training_loss": 5.908609867095947
    },
    {
      "epoch": 0.3386449864498645,
      "step": 1562,
      "training_loss": 6.968374252319336
    },
    {
      "epoch": 0.3386449864498645,
      "step": 1562,
      "training_loss": 4.478886127471924
    },
    {
      "epoch": 0.3386449864498645,
      "step": 1562,
      "training_loss": 7.083196640014648
    },
    {
      "epoch": 0.3386449864498645,
      "step": 1562,
      "training_loss": 6.846506118774414
    },
    {
      "epoch": 0.3388617886178862,
      "step": 1563,
      "training_loss": 4.947370529174805
    },
    {
      "epoch": 0.3388617886178862,
      "step": 1563,
      "training_loss": 7.924668788909912
    },
    {
      "epoch": 0.3388617886178862,
      "step": 1563,
      "training_loss": 5.908615589141846
    },
    {
      "epoch": 0.3388617886178862,
      "step": 1563,
      "training_loss": 7.290425777435303
    },
    {
      "epoch": 0.33907859078590785,
      "grad_norm": 10.583182334899902,
      "learning_rate": 1e-05,
      "loss": 6.4553,
      "step": 1564
    },
    {
      "epoch": 0.33907859078590785,
      "step": 1564,
      "training_loss": 6.112092018127441
    },
    {
      "epoch": 0.33907859078590785,
      "step": 1564,
      "training_loss": 7.607795238494873
    },
    {
      "epoch": 0.33907859078590785,
      "step": 1564,
      "training_loss": 6.424147605895996
    },
    {
      "epoch": 0.33907859078590785,
      "step": 1564,
      "training_loss": 6.179421424865723
    },
    {
      "epoch": 0.3392953929539295,
      "step": 1565,
      "training_loss": 7.3046441078186035
    },
    {
      "epoch": 0.3392953929539295,
      "step": 1565,
      "training_loss": 5.900970935821533
    },
    {
      "epoch": 0.3392953929539295,
      "step": 1565,
      "training_loss": 6.710708141326904
    },
    {
      "epoch": 0.3392953929539295,
      "step": 1565,
      "training_loss": 4.509260177612305
    },
    {
      "epoch": 0.33951219512195124,
      "step": 1566,
      "training_loss": 8.103618621826172
    },
    {
      "epoch": 0.33951219512195124,
      "step": 1566,
      "training_loss": 6.260141372680664
    },
    {
      "epoch": 0.33951219512195124,
      "step": 1566,
      "training_loss": 6.985652923583984
    },
    {
      "epoch": 0.33951219512195124,
      "step": 1566,
      "training_loss": 6.638857364654541
    },
    {
      "epoch": 0.3397289972899729,
      "step": 1567,
      "training_loss": 7.372692584991455
    },
    {
      "epoch": 0.3397289972899729,
      "step": 1567,
      "training_loss": 6.3709635734558105
    },
    {
      "epoch": 0.3397289972899729,
      "step": 1567,
      "training_loss": 7.218706130981445
    },
    {
      "epoch": 0.3397289972899729,
      "step": 1567,
      "training_loss": 7.2147698402404785
    },
    {
      "epoch": 0.3399457994579946,
      "grad_norm": 15.141343116760254,
      "learning_rate": 1e-05,
      "loss": 6.6822,
      "step": 1568
    },
    {
      "epoch": 0.3399457994579946,
      "step": 1568,
      "training_loss": 7.032965183258057
    },
    {
      "epoch": 0.3399457994579946,
      "step": 1568,
      "training_loss": 7.942735195159912
    },
    {
      "epoch": 0.3399457994579946,
      "step": 1568,
      "training_loss": 5.598586559295654
    },
    {
      "epoch": 0.3399457994579946,
      "step": 1568,
      "training_loss": 7.003057956695557
    },
    {
      "epoch": 0.34016260162601625,
      "step": 1569,
      "training_loss": 5.554831504821777
    },
    {
      "epoch": 0.34016260162601625,
      "step": 1569,
      "training_loss": 4.611821174621582
    },
    {
      "epoch": 0.34016260162601625,
      "step": 1569,
      "training_loss": 6.0928826332092285
    },
    {
      "epoch": 0.34016260162601625,
      "step": 1569,
      "training_loss": 6.9401469230651855
    },
    {
      "epoch": 0.3403794037940379,
      "step": 1570,
      "training_loss": 6.887782096862793
    },
    {
      "epoch": 0.3403794037940379,
      "step": 1570,
      "training_loss": 6.245619773864746
    },
    {
      "epoch": 0.3403794037940379,
      "step": 1570,
      "training_loss": 6.637809753417969
    },
    {
      "epoch": 0.3403794037940379,
      "step": 1570,
      "training_loss": 6.9249444007873535
    },
    {
      "epoch": 0.34059620596205964,
      "step": 1571,
      "training_loss": 7.962652683258057
    },
    {
      "epoch": 0.34059620596205964,
      "step": 1571,
      "training_loss": 7.815670013427734
    },
    {
      "epoch": 0.34059620596205964,
      "step": 1571,
      "training_loss": 7.063454627990723
    },
    {
      "epoch": 0.34059620596205964,
      "step": 1571,
      "training_loss": 5.311654090881348
    },
    {
      "epoch": 0.3408130081300813,
      "grad_norm": 18.106460571289062,
      "learning_rate": 1e-05,
      "loss": 6.6017,
      "step": 1572
    },
    {
      "epoch": 0.3408130081300813,
      "step": 1572,
      "training_loss": 3.9884846210479736
    },
    {
      "epoch": 0.3408130081300813,
      "step": 1572,
      "training_loss": 7.389801979064941
    },
    {
      "epoch": 0.3408130081300813,
      "step": 1572,
      "training_loss": 5.667388439178467
    },
    {
      "epoch": 0.3408130081300813,
      "step": 1572,
      "training_loss": 8.296339988708496
    },
    {
      "epoch": 0.341029810298103,
      "step": 1573,
      "training_loss": 7.160274982452393
    },
    {
      "epoch": 0.341029810298103,
      "step": 1573,
      "training_loss": 7.889925956726074
    },
    {
      "epoch": 0.341029810298103,
      "step": 1573,
      "training_loss": 7.556766033172607
    },
    {
      "epoch": 0.341029810298103,
      "step": 1573,
      "training_loss": 6.698592185974121
    },
    {
      "epoch": 0.34124661246612464,
      "step": 1574,
      "training_loss": 6.940601348876953
    },
    {
      "epoch": 0.34124661246612464,
      "step": 1574,
      "training_loss": 5.602400779724121
    },
    {
      "epoch": 0.34124661246612464,
      "step": 1574,
      "training_loss": 7.081831455230713
    },
    {
      "epoch": 0.34124661246612464,
      "step": 1574,
      "training_loss": 6.14012336730957
    },
    {
      "epoch": 0.34146341463414637,
      "step": 1575,
      "training_loss": 6.196491241455078
    },
    {
      "epoch": 0.34146341463414637,
      "step": 1575,
      "training_loss": 7.3405375480651855
    },
    {
      "epoch": 0.34146341463414637,
      "step": 1575,
      "training_loss": 6.454802513122559
    },
    {
      "epoch": 0.34146341463414637,
      "step": 1575,
      "training_loss": 8.30794620513916
    },
    {
      "epoch": 0.34168021680216804,
      "grad_norm": 13.832036972045898,
      "learning_rate": 1e-05,
      "loss": 6.7945,
      "step": 1576
    },
    {
      "epoch": 0.34168021680216804,
      "step": 1576,
      "training_loss": 6.959426403045654
    },
    {
      "epoch": 0.34168021680216804,
      "step": 1576,
      "training_loss": 6.614569664001465
    },
    {
      "epoch": 0.34168021680216804,
      "step": 1576,
      "training_loss": 6.329202175140381
    },
    {
      "epoch": 0.34168021680216804,
      "step": 1576,
      "training_loss": 4.795740127563477
    },
    {
      "epoch": 0.3418970189701897,
      "step": 1577,
      "training_loss": 6.196523189544678
    },
    {
      "epoch": 0.3418970189701897,
      "step": 1577,
      "training_loss": 6.418375492095947
    },
    {
      "epoch": 0.3418970189701897,
      "step": 1577,
      "training_loss": 5.620077133178711
    },
    {
      "epoch": 0.3418970189701897,
      "step": 1577,
      "training_loss": 6.935630798339844
    },
    {
      "epoch": 0.34211382113821137,
      "step": 1578,
      "training_loss": 7.450989246368408
    },
    {
      "epoch": 0.34211382113821137,
      "step": 1578,
      "training_loss": 6.7229533195495605
    },
    {
      "epoch": 0.34211382113821137,
      "step": 1578,
      "training_loss": 7.039632320404053
    },
    {
      "epoch": 0.34211382113821137,
      "step": 1578,
      "training_loss": 6.255396366119385
    },
    {
      "epoch": 0.34233062330623304,
      "step": 1579,
      "training_loss": 6.700588226318359
    },
    {
      "epoch": 0.34233062330623304,
      "step": 1579,
      "training_loss": 6.900949954986572
    },
    {
      "epoch": 0.34233062330623304,
      "step": 1579,
      "training_loss": 3.8765337467193604
    },
    {
      "epoch": 0.34233062330623304,
      "step": 1579,
      "training_loss": 7.417760848999023
    },
    {
      "epoch": 0.34254742547425476,
      "grad_norm": 16.289417266845703,
      "learning_rate": 1e-05,
      "loss": 6.3896,
      "step": 1580
    },
    {
      "epoch": 0.34254742547425476,
      "step": 1580,
      "training_loss": 6.2459211349487305
    },
    {
      "epoch": 0.34254742547425476,
      "step": 1580,
      "training_loss": 7.013138294219971
    },
    {
      "epoch": 0.34254742547425476,
      "step": 1580,
      "training_loss": 3.966331720352173
    },
    {
      "epoch": 0.34254742547425476,
      "step": 1580,
      "training_loss": 6.2811126708984375
    },
    {
      "epoch": 0.34276422764227643,
      "step": 1581,
      "training_loss": 4.171061992645264
    },
    {
      "epoch": 0.34276422764227643,
      "step": 1581,
      "training_loss": 6.58604097366333
    },
    {
      "epoch": 0.34276422764227643,
      "step": 1581,
      "training_loss": 6.415837287902832
    },
    {
      "epoch": 0.34276422764227643,
      "step": 1581,
      "training_loss": 5.4933671951293945
    },
    {
      "epoch": 0.3429810298102981,
      "step": 1582,
      "training_loss": 8.195806503295898
    },
    {
      "epoch": 0.3429810298102981,
      "step": 1582,
      "training_loss": 7.196476459503174
    },
    {
      "epoch": 0.3429810298102981,
      "step": 1582,
      "training_loss": 7.806089878082275
    },
    {
      "epoch": 0.3429810298102981,
      "step": 1582,
      "training_loss": 6.375326156616211
    },
    {
      "epoch": 0.34319783197831977,
      "step": 1583,
      "training_loss": 5.410503387451172
    },
    {
      "epoch": 0.34319783197831977,
      "step": 1583,
      "training_loss": 6.123374938964844
    },
    {
      "epoch": 0.34319783197831977,
      "step": 1583,
      "training_loss": 5.673414707183838
    },
    {
      "epoch": 0.34319783197831977,
      "step": 1583,
      "training_loss": 7.123295783996582
    },
    {
      "epoch": 0.3434146341463415,
      "grad_norm": 12.739148139953613,
      "learning_rate": 1e-05,
      "loss": 6.2548,
      "step": 1584
    },
    {
      "epoch": 0.3434146341463415,
      "step": 1584,
      "training_loss": 6.410386085510254
    },
    {
      "epoch": 0.3434146341463415,
      "step": 1584,
      "training_loss": 7.113259315490723
    },
    {
      "epoch": 0.3434146341463415,
      "step": 1584,
      "training_loss": 5.655806064605713
    },
    {
      "epoch": 0.3434146341463415,
      "step": 1584,
      "training_loss": 7.258056163787842
    },
    {
      "epoch": 0.34363143631436316,
      "step": 1585,
      "training_loss": 7.360811233520508
    },
    {
      "epoch": 0.34363143631436316,
      "step": 1585,
      "training_loss": 6.010619640350342
    },
    {
      "epoch": 0.34363143631436316,
      "step": 1585,
      "training_loss": 6.515361309051514
    },
    {
      "epoch": 0.34363143631436316,
      "step": 1585,
      "training_loss": 6.956752777099609
    },
    {
      "epoch": 0.3438482384823848,
      "step": 1586,
      "training_loss": 7.08087158203125
    },
    {
      "epoch": 0.3438482384823848,
      "step": 1586,
      "training_loss": 4.52373743057251
    },
    {
      "epoch": 0.3438482384823848,
      "step": 1586,
      "training_loss": 7.490567684173584
    },
    {
      "epoch": 0.3438482384823848,
      "step": 1586,
      "training_loss": 6.7763543128967285
    },
    {
      "epoch": 0.3440650406504065,
      "step": 1587,
      "training_loss": 6.74934196472168
    },
    {
      "epoch": 0.3440650406504065,
      "step": 1587,
      "training_loss": 6.450066089630127
    },
    {
      "epoch": 0.3440650406504065,
      "step": 1587,
      "training_loss": 5.997970104217529
    },
    {
      "epoch": 0.3440650406504065,
      "step": 1587,
      "training_loss": 4.728060245513916
    },
    {
      "epoch": 0.34428184281842816,
      "grad_norm": 12.134984016418457,
      "learning_rate": 1e-05,
      "loss": 6.4424,
      "step": 1588
    },
    {
      "epoch": 0.34428184281842816,
      "step": 1588,
      "training_loss": 6.457262992858887
    },
    {
      "epoch": 0.34428184281842816,
      "step": 1588,
      "training_loss": 7.583941459655762
    },
    {
      "epoch": 0.34428184281842816,
      "step": 1588,
      "training_loss": 6.761821269989014
    },
    {
      "epoch": 0.34428184281842816,
      "step": 1588,
      "training_loss": 6.682778835296631
    },
    {
      "epoch": 0.3444986449864499,
      "step": 1589,
      "training_loss": 7.3685431480407715
    },
    {
      "epoch": 0.3444986449864499,
      "step": 1589,
      "training_loss": 5.066795349121094
    },
    {
      "epoch": 0.3444986449864499,
      "step": 1589,
      "training_loss": 6.636072635650635
    },
    {
      "epoch": 0.3444986449864499,
      "step": 1589,
      "training_loss": 7.358443260192871
    },
    {
      "epoch": 0.34471544715447155,
      "step": 1590,
      "training_loss": 6.304995536804199
    },
    {
      "epoch": 0.34471544715447155,
      "step": 1590,
      "training_loss": 5.3067827224731445
    },
    {
      "epoch": 0.34471544715447155,
      "step": 1590,
      "training_loss": 7.19725227355957
    },
    {
      "epoch": 0.34471544715447155,
      "step": 1590,
      "training_loss": 6.714027404785156
    },
    {
      "epoch": 0.3449322493224932,
      "step": 1591,
      "training_loss": 5.180523872375488
    },
    {
      "epoch": 0.3449322493224932,
      "step": 1591,
      "training_loss": 8.639949798583984
    },
    {
      "epoch": 0.3449322493224932,
      "step": 1591,
      "training_loss": 6.744867324829102
    },
    {
      "epoch": 0.3449322493224932,
      "step": 1591,
      "training_loss": 6.714144706726074
    },
    {
      "epoch": 0.3451490514905149,
      "grad_norm": 20.514183044433594,
      "learning_rate": 1e-05,
      "loss": 6.6699,
      "step": 1592
    },
    {
      "epoch": 0.3451490514905149,
      "step": 1592,
      "training_loss": 7.619663715362549
    },
    {
      "epoch": 0.3451490514905149,
      "step": 1592,
      "training_loss": 6.545093059539795
    },
    {
      "epoch": 0.3451490514905149,
      "step": 1592,
      "training_loss": 7.600837707519531
    },
    {
      "epoch": 0.3451490514905149,
      "step": 1592,
      "training_loss": 5.348443984985352
    },
    {
      "epoch": 0.34536585365853656,
      "step": 1593,
      "training_loss": 5.495766639709473
    },
    {
      "epoch": 0.34536585365853656,
      "step": 1593,
      "training_loss": 7.388479709625244
    },
    {
      "epoch": 0.34536585365853656,
      "step": 1593,
      "training_loss": 6.792330741882324
    },
    {
      "epoch": 0.34536585365853656,
      "step": 1593,
      "training_loss": 7.215632438659668
    },
    {
      "epoch": 0.3455826558265583,
      "step": 1594,
      "training_loss": 6.623170375823975
    },
    {
      "epoch": 0.3455826558265583,
      "step": 1594,
      "training_loss": 6.707062244415283
    },
    {
      "epoch": 0.3455826558265583,
      "step": 1594,
      "training_loss": 6.723503112792969
    },
    {
      "epoch": 0.3455826558265583,
      "step": 1594,
      "training_loss": 7.187746047973633
    },
    {
      "epoch": 0.34579945799457995,
      "step": 1595,
      "training_loss": 6.8619184494018555
    },
    {
      "epoch": 0.34579945799457995,
      "step": 1595,
      "training_loss": 3.6950886249542236
    },
    {
      "epoch": 0.34579945799457995,
      "step": 1595,
      "training_loss": 7.165131092071533
    },
    {
      "epoch": 0.34579945799457995,
      "step": 1595,
      "training_loss": 7.291921138763428
    },
    {
      "epoch": 0.3460162601626016,
      "grad_norm": 10.846385955810547,
      "learning_rate": 1e-05,
      "loss": 6.6414,
      "step": 1596
    },
    {
      "epoch": 0.3460162601626016,
      "step": 1596,
      "training_loss": 5.683964252471924
    },
    {
      "epoch": 0.3460162601626016,
      "step": 1596,
      "training_loss": 6.702027797698975
    },
    {
      "epoch": 0.3460162601626016,
      "step": 1596,
      "training_loss": 7.29515266418457
    },
    {
      "epoch": 0.3460162601626016,
      "step": 1596,
      "training_loss": 7.213603496551514
    },
    {
      "epoch": 0.3462330623306233,
      "step": 1597,
      "training_loss": 6.723990440368652
    },
    {
      "epoch": 0.3462330623306233,
      "step": 1597,
      "training_loss": 6.748919486999512
    },
    {
      "epoch": 0.3462330623306233,
      "step": 1597,
      "training_loss": 7.477642059326172
    },
    {
      "epoch": 0.3462330623306233,
      "step": 1597,
      "training_loss": 6.775512218475342
    },
    {
      "epoch": 0.346449864498645,
      "step": 1598,
      "training_loss": 7.293554306030273
    },
    {
      "epoch": 0.346449864498645,
      "step": 1598,
      "training_loss": 5.622381687164307
    },
    {
      "epoch": 0.346449864498645,
      "step": 1598,
      "training_loss": 6.202781677246094
    },
    {
      "epoch": 0.346449864498645,
      "step": 1598,
      "training_loss": 5.947842597961426
    },
    {
      "epoch": 0.3466666666666667,
      "step": 1599,
      "training_loss": 6.499563694000244
    },
    {
      "epoch": 0.3466666666666667,
      "step": 1599,
      "training_loss": 6.652442932128906
    },
    {
      "epoch": 0.3466666666666667,
      "step": 1599,
      "training_loss": 6.736520290374756
    },
    {
      "epoch": 0.3466666666666667,
      "step": 1599,
      "training_loss": 5.803678512573242
    },
    {
      "epoch": 0.34688346883468835,
      "grad_norm": 12.655332565307617,
      "learning_rate": 1e-05,
      "loss": 6.5862,
      "step": 1600
    },
    {
      "epoch": 0.34688346883468835,
      "step": 1600,
      "training_loss": 4.76314640045166
    },
    {
      "epoch": 0.34688346883468835,
      "step": 1600,
      "training_loss": 6.5984416007995605
    },
    {
      "epoch": 0.34688346883468835,
      "step": 1600,
      "training_loss": 5.2590250968933105
    },
    {
      "epoch": 0.34688346883468835,
      "step": 1600,
      "training_loss": 5.225205898284912
    },
    {
      "epoch": 0.34710027100271,
      "step": 1601,
      "training_loss": 7.159638404846191
    },
    {
      "epoch": 0.34710027100271,
      "step": 1601,
      "training_loss": 6.199925422668457
    },
    {
      "epoch": 0.34710027100271,
      "step": 1601,
      "training_loss": 6.210999011993408
    },
    {
      "epoch": 0.34710027100271,
      "step": 1601,
      "training_loss": 6.583770751953125
    },
    {
      "epoch": 0.3473170731707317,
      "step": 1602,
      "training_loss": 7.298152923583984
    },
    {
      "epoch": 0.3473170731707317,
      "step": 1602,
      "training_loss": 6.487784385681152
    },
    {
      "epoch": 0.3473170731707317,
      "step": 1602,
      "training_loss": 6.998312950134277
    },
    {
      "epoch": 0.3473170731707317,
      "step": 1602,
      "training_loss": 6.9437456130981445
    },
    {
      "epoch": 0.3475338753387534,
      "step": 1603,
      "training_loss": 6.502740859985352
    },
    {
      "epoch": 0.3475338753387534,
      "step": 1603,
      "training_loss": 5.12459659576416
    },
    {
      "epoch": 0.3475338753387534,
      "step": 1603,
      "training_loss": 6.924617767333984
    },
    {
      "epoch": 0.3475338753387534,
      "step": 1603,
      "training_loss": 6.2058210372924805
    },
    {
      "epoch": 0.3477506775067751,
      "grad_norm": 13.09716796875,
      "learning_rate": 1e-05,
      "loss": 6.2804,
      "step": 1604
    },
    {
      "epoch": 0.3477506775067751,
      "step": 1604,
      "training_loss": 5.662106037139893
    },
    {
      "epoch": 0.3477506775067751,
      "step": 1604,
      "training_loss": 6.719144344329834
    },
    {
      "epoch": 0.3477506775067751,
      "step": 1604,
      "training_loss": 7.385202884674072
    },
    {
      "epoch": 0.3477506775067751,
      "step": 1604,
      "training_loss": 7.265718936920166
    },
    {
      "epoch": 0.34796747967479674,
      "step": 1605,
      "training_loss": 6.27492618560791
    },
    {
      "epoch": 0.34796747967479674,
      "step": 1605,
      "training_loss": 7.396439075469971
    },
    {
      "epoch": 0.34796747967479674,
      "step": 1605,
      "training_loss": 6.813732147216797
    },
    {
      "epoch": 0.34796747967479674,
      "step": 1605,
      "training_loss": 7.203444480895996
    },
    {
      "epoch": 0.3481842818428184,
      "step": 1606,
      "training_loss": 5.032772064208984
    },
    {
      "epoch": 0.3481842818428184,
      "step": 1606,
      "training_loss": 3.4819812774658203
    },
    {
      "epoch": 0.3481842818428184,
      "step": 1606,
      "training_loss": 5.626957893371582
    },
    {
      "epoch": 0.3481842818428184,
      "step": 1606,
      "training_loss": 5.741591453552246
    },
    {
      "epoch": 0.34840108401084013,
      "step": 1607,
      "training_loss": 6.876443386077881
    },
    {
      "epoch": 0.34840108401084013,
      "step": 1607,
      "training_loss": 6.674443244934082
    },
    {
      "epoch": 0.34840108401084013,
      "step": 1607,
      "training_loss": 5.275676727294922
    },
    {
      "epoch": 0.34840108401084013,
      "step": 1607,
      "training_loss": 6.556909561157227
    },
    {
      "epoch": 0.3486178861788618,
      "grad_norm": 17.058027267456055,
      "learning_rate": 1e-05,
      "loss": 6.2492,
      "step": 1608
    },
    {
      "epoch": 0.3486178861788618,
      "step": 1608,
      "training_loss": 5.8179216384887695
    },
    {
      "epoch": 0.3486178861788618,
      "step": 1608,
      "training_loss": 6.023440361022949
    },
    {
      "epoch": 0.3486178861788618,
      "step": 1608,
      "training_loss": 7.6393914222717285
    },
    {
      "epoch": 0.3486178861788618,
      "step": 1608,
      "training_loss": 6.967545986175537
    },
    {
      "epoch": 0.34883468834688347,
      "step": 1609,
      "training_loss": 5.560833930969238
    },
    {
      "epoch": 0.34883468834688347,
      "step": 1609,
      "training_loss": 6.733032703399658
    },
    {
      "epoch": 0.34883468834688347,
      "step": 1609,
      "training_loss": 7.028883934020996
    },
    {
      "epoch": 0.34883468834688347,
      "step": 1609,
      "training_loss": 7.282248020172119
    },
    {
      "epoch": 0.34905149051490514,
      "step": 1610,
      "training_loss": 7.711682319641113
    },
    {
      "epoch": 0.34905149051490514,
      "step": 1610,
      "training_loss": 7.296599864959717
    },
    {
      "epoch": 0.34905149051490514,
      "step": 1610,
      "training_loss": 5.773132801055908
    },
    {
      "epoch": 0.34905149051490514,
      "step": 1610,
      "training_loss": 6.705759525299072
    },
    {
      "epoch": 0.3492682926829268,
      "step": 1611,
      "training_loss": 3.353623151779175
    },
    {
      "epoch": 0.3492682926829268,
      "step": 1611,
      "training_loss": 5.482333183288574
    },
    {
      "epoch": 0.3492682926829268,
      "step": 1611,
      "training_loss": 6.706671237945557
    },
    {
      "epoch": 0.3492682926829268,
      "step": 1611,
      "training_loss": 6.317897796630859
    },
    {
      "epoch": 0.34948509485094853,
      "grad_norm": 17.80296516418457,
      "learning_rate": 1e-05,
      "loss": 6.4001,
      "step": 1612
    },
    {
      "epoch": 0.34948509485094853,
      "step": 1612,
      "training_loss": 7.157780647277832
    },
    {
      "epoch": 0.34948509485094853,
      "step": 1612,
      "training_loss": 5.99738073348999
    },
    {
      "epoch": 0.34948509485094853,
      "step": 1612,
      "training_loss": 6.363231182098389
    },
    {
      "epoch": 0.34948509485094853,
      "step": 1612,
      "training_loss": 6.807590007781982
    },
    {
      "epoch": 0.3497018970189702,
      "step": 1613,
      "training_loss": 6.958680629730225
    },
    {
      "epoch": 0.3497018970189702,
      "step": 1613,
      "training_loss": 6.66729211807251
    },
    {
      "epoch": 0.3497018970189702,
      "step": 1613,
      "training_loss": 7.656929969787598
    },
    {
      "epoch": 0.3497018970189702,
      "step": 1613,
      "training_loss": 6.717577934265137
    },
    {
      "epoch": 0.34991869918699187,
      "step": 1614,
      "training_loss": 6.762634754180908
    },
    {
      "epoch": 0.34991869918699187,
      "step": 1614,
      "training_loss": 7.2265143394470215
    },
    {
      "epoch": 0.34991869918699187,
      "step": 1614,
      "training_loss": 6.573046684265137
    },
    {
      "epoch": 0.34991869918699187,
      "step": 1614,
      "training_loss": 6.679012775421143
    },
    {
      "epoch": 0.35013550135501353,
      "step": 1615,
      "training_loss": 7.808555603027344
    },
    {
      "epoch": 0.35013550135501353,
      "step": 1615,
      "training_loss": 7.158659934997559
    },
    {
      "epoch": 0.35013550135501353,
      "step": 1615,
      "training_loss": 7.666773319244385
    },
    {
      "epoch": 0.35013550135501353,
      "step": 1615,
      "training_loss": 6.492377281188965
    },
    {
      "epoch": 0.35035230352303526,
      "grad_norm": 13.7992525100708,
      "learning_rate": 1e-05,
      "loss": 6.9184,
      "step": 1616
    },
    {
      "epoch": 0.35035230352303526,
      "step": 1616,
      "training_loss": 7.133089065551758
    },
    {
      "epoch": 0.35035230352303526,
      "step": 1616,
      "training_loss": 6.336052894592285
    },
    {
      "epoch": 0.35035230352303526,
      "step": 1616,
      "training_loss": 7.254441261291504
    },
    {
      "epoch": 0.35035230352303526,
      "step": 1616,
      "training_loss": 7.717905044555664
    },
    {
      "epoch": 0.3505691056910569,
      "step": 1617,
      "training_loss": 7.782539367675781
    },
    {
      "epoch": 0.3505691056910569,
      "step": 1617,
      "training_loss": 5.673479080200195
    },
    {
      "epoch": 0.3505691056910569,
      "step": 1617,
      "training_loss": 6.530593395233154
    },
    {
      "epoch": 0.3505691056910569,
      "step": 1617,
      "training_loss": 6.517182350158691
    },
    {
      "epoch": 0.3507859078590786,
      "step": 1618,
      "training_loss": 6.863574028015137
    },
    {
      "epoch": 0.3507859078590786,
      "step": 1618,
      "training_loss": 6.976365566253662
    },
    {
      "epoch": 0.3507859078590786,
      "step": 1618,
      "training_loss": 6.537945747375488
    },
    {
      "epoch": 0.3507859078590786,
      "step": 1618,
      "training_loss": 5.8195366859436035
    },
    {
      "epoch": 0.35100271002710026,
      "step": 1619,
      "training_loss": 6.760677337646484
    },
    {
      "epoch": 0.35100271002710026,
      "step": 1619,
      "training_loss": 8.065534591674805
    },
    {
      "epoch": 0.35100271002710026,
      "step": 1619,
      "training_loss": 7.052891731262207
    },
    {
      "epoch": 0.35100271002710026,
      "step": 1619,
      "training_loss": 6.473545074462891
    },
    {
      "epoch": 0.35121951219512193,
      "grad_norm": 15.112712860107422,
      "learning_rate": 1e-05,
      "loss": 6.8435,
      "step": 1620
    },
    {
      "epoch": 0.35121951219512193,
      "step": 1620,
      "training_loss": 7.163512706756592
    },
    {
      "epoch": 0.35121951219512193,
      "step": 1620,
      "training_loss": 7.168973445892334
    },
    {
      "epoch": 0.35121951219512193,
      "step": 1620,
      "training_loss": 7.372042179107666
    },
    {
      "epoch": 0.35121951219512193,
      "step": 1620,
      "training_loss": 6.271060943603516
    },
    {
      "epoch": 0.35143631436314365,
      "step": 1621,
      "training_loss": 6.09128475189209
    },
    {
      "epoch": 0.35143631436314365,
      "step": 1621,
      "training_loss": 6.4914703369140625
    },
    {
      "epoch": 0.35143631436314365,
      "step": 1621,
      "training_loss": 6.806426048278809
    },
    {
      "epoch": 0.35143631436314365,
      "step": 1621,
      "training_loss": 7.241113662719727
    },
    {
      "epoch": 0.3516531165311653,
      "step": 1622,
      "training_loss": 7.057372093200684
    },
    {
      "epoch": 0.3516531165311653,
      "step": 1622,
      "training_loss": 6.663821697235107
    },
    {
      "epoch": 0.3516531165311653,
      "step": 1622,
      "training_loss": 6.861939907073975
    },
    {
      "epoch": 0.3516531165311653,
      "step": 1622,
      "training_loss": 6.883613109588623
    },
    {
      "epoch": 0.351869918699187,
      "step": 1623,
      "training_loss": 7.181551933288574
    },
    {
      "epoch": 0.351869918699187,
      "step": 1623,
      "training_loss": 6.2936015129089355
    },
    {
      "epoch": 0.351869918699187,
      "step": 1623,
      "training_loss": 6.211673259735107
    },
    {
      "epoch": 0.351869918699187,
      "step": 1623,
      "training_loss": 7.552959442138672
    },
    {
      "epoch": 0.35208672086720866,
      "grad_norm": 11.423932075500488,
      "learning_rate": 1e-05,
      "loss": 6.832,
      "step": 1624
    },
    {
      "epoch": 0.35208672086720866,
      "step": 1624,
      "training_loss": 5.2986955642700195
    },
    {
      "epoch": 0.35208672086720866,
      "step": 1624,
      "training_loss": 7.289077281951904
    },
    {
      "epoch": 0.35208672086720866,
      "step": 1624,
      "training_loss": 8.50186538696289
    },
    {
      "epoch": 0.35208672086720866,
      "step": 1624,
      "training_loss": 8.049757957458496
    },
    {
      "epoch": 0.3523035230352303,
      "step": 1625,
      "training_loss": 6.927133560180664
    },
    {
      "epoch": 0.3523035230352303,
      "step": 1625,
      "training_loss": 4.129000663757324
    },
    {
      "epoch": 0.3523035230352303,
      "step": 1625,
      "training_loss": 5.962741374969482
    },
    {
      "epoch": 0.3523035230352303,
      "step": 1625,
      "training_loss": 7.207662105560303
    },
    {
      "epoch": 0.35252032520325205,
      "step": 1626,
      "training_loss": 7.162098407745361
    },
    {
      "epoch": 0.35252032520325205,
      "step": 1626,
      "training_loss": 3.2174129486083984
    },
    {
      "epoch": 0.35252032520325205,
      "step": 1626,
      "training_loss": 6.929558753967285
    },
    {
      "epoch": 0.35252032520325205,
      "step": 1626,
      "training_loss": 5.447125434875488
    },
    {
      "epoch": 0.3527371273712737,
      "step": 1627,
      "training_loss": 6.766557693481445
    },
    {
      "epoch": 0.3527371273712737,
      "step": 1627,
      "training_loss": 5.2397637367248535
    },
    {
      "epoch": 0.3527371273712737,
      "step": 1627,
      "training_loss": 7.444577693939209
    },
    {
      "epoch": 0.3527371273712737,
      "step": 1627,
      "training_loss": 4.163211345672607
    },
    {
      "epoch": 0.3529539295392954,
      "grad_norm": 15.306652069091797,
      "learning_rate": 1e-05,
      "loss": 6.2335,
      "step": 1628
    },
    {
      "epoch": 0.3529539295392954,
      "step": 1628,
      "training_loss": 6.810220241546631
    },
    {
      "epoch": 0.3529539295392954,
      "step": 1628,
      "training_loss": 7.011592388153076
    },
    {
      "epoch": 0.3529539295392954,
      "step": 1628,
      "training_loss": 6.4905877113342285
    },
    {
      "epoch": 0.3529539295392954,
      "step": 1628,
      "training_loss": 4.031383514404297
    },
    {
      "epoch": 0.35317073170731705,
      "step": 1629,
      "training_loss": 6.196506023406982
    },
    {
      "epoch": 0.35317073170731705,
      "step": 1629,
      "training_loss": 7.4286909103393555
    },
    {
      "epoch": 0.35317073170731705,
      "step": 1629,
      "training_loss": 6.997620582580566
    },
    {
      "epoch": 0.35317073170731705,
      "step": 1629,
      "training_loss": 6.978851318359375
    },
    {
      "epoch": 0.3533875338753388,
      "step": 1630,
      "training_loss": 8.087267875671387
    },
    {
      "epoch": 0.3533875338753388,
      "step": 1630,
      "training_loss": 6.460810661315918
    },
    {
      "epoch": 0.3533875338753388,
      "step": 1630,
      "training_loss": 7.732973575592041
    },
    {
      "epoch": 0.3533875338753388,
      "step": 1630,
      "training_loss": 5.630054950714111
    },
    {
      "epoch": 0.35360433604336045,
      "step": 1631,
      "training_loss": 7.085742950439453
    },
    {
      "epoch": 0.35360433604336045,
      "step": 1631,
      "training_loss": 6.970870018005371
    },
    {
      "epoch": 0.35360433604336045,
      "step": 1631,
      "training_loss": 7.41502571105957
    },
    {
      "epoch": 0.35360433604336045,
      "step": 1631,
      "training_loss": 7.1527628898620605
    },
    {
      "epoch": 0.3538211382113821,
      "grad_norm": 10.088996887207031,
      "learning_rate": 1e-05,
      "loss": 6.7801,
      "step": 1632
    },
    {
      "epoch": 0.3538211382113821,
      "step": 1632,
      "training_loss": 6.150372505187988
    },
    {
      "epoch": 0.3538211382113821,
      "step": 1632,
      "training_loss": 5.989902019500732
    },
    {
      "epoch": 0.3538211382113821,
      "step": 1632,
      "training_loss": 4.162961006164551
    },
    {
      "epoch": 0.3538211382113821,
      "step": 1632,
      "training_loss": 6.0378851890563965
    },
    {
      "epoch": 0.3540379403794038,
      "step": 1633,
      "training_loss": 7.2712202072143555
    },
    {
      "epoch": 0.3540379403794038,
      "step": 1633,
      "training_loss": 5.651459217071533
    },
    {
      "epoch": 0.3540379403794038,
      "step": 1633,
      "training_loss": 5.444139003753662
    },
    {
      "epoch": 0.3540379403794038,
      "step": 1633,
      "training_loss": 7.19904088973999
    },
    {
      "epoch": 0.35425474254742545,
      "step": 1634,
      "training_loss": 6.94573974609375
    },
    {
      "epoch": 0.35425474254742545,
      "step": 1634,
      "training_loss": 7.719788074493408
    },
    {
      "epoch": 0.35425474254742545,
      "step": 1634,
      "training_loss": 6.915071964263916
    },
    {
      "epoch": 0.35425474254742545,
      "step": 1634,
      "training_loss": 7.032058238983154
    },
    {
      "epoch": 0.3544715447154472,
      "step": 1635,
      "training_loss": 5.678132057189941
    },
    {
      "epoch": 0.3544715447154472,
      "step": 1635,
      "training_loss": 5.456352233886719
    },
    {
      "epoch": 0.3544715447154472,
      "step": 1635,
      "training_loss": 7.25885009765625
    },
    {
      "epoch": 0.3544715447154472,
      "step": 1635,
      "training_loss": 7.969820499420166
    },
    {
      "epoch": 0.35468834688346884,
      "grad_norm": 17.455669403076172,
      "learning_rate": 1e-05,
      "loss": 6.4302,
      "step": 1636
    },
    {
      "epoch": 0.35468834688346884,
      "step": 1636,
      "training_loss": 6.484288215637207
    },
    {
      "epoch": 0.35468834688346884,
      "step": 1636,
      "training_loss": 6.383040904998779
    },
    {
      "epoch": 0.35468834688346884,
      "step": 1636,
      "training_loss": 6.9619269371032715
    },
    {
      "epoch": 0.35468834688346884,
      "step": 1636,
      "training_loss": 7.30967378616333
    },
    {
      "epoch": 0.3549051490514905,
      "step": 1637,
      "training_loss": 5.7120161056518555
    },
    {
      "epoch": 0.3549051490514905,
      "step": 1637,
      "training_loss": 7.132643222808838
    },
    {
      "epoch": 0.3549051490514905,
      "step": 1637,
      "training_loss": 6.65674352645874
    },
    {
      "epoch": 0.3549051490514905,
      "step": 1637,
      "training_loss": 4.141076564788818
    },
    {
      "epoch": 0.3551219512195122,
      "step": 1638,
      "training_loss": 6.448458671569824
    },
    {
      "epoch": 0.3551219512195122,
      "step": 1638,
      "training_loss": 7.046011447906494
    },
    {
      "epoch": 0.3551219512195122,
      "step": 1638,
      "training_loss": 5.813723564147949
    },
    {
      "epoch": 0.3551219512195122,
      "step": 1638,
      "training_loss": 8.046070098876953
    },
    {
      "epoch": 0.3553387533875339,
      "step": 1639,
      "training_loss": 7.329338550567627
    },
    {
      "epoch": 0.3553387533875339,
      "step": 1639,
      "training_loss": 6.73862886428833
    },
    {
      "epoch": 0.3553387533875339,
      "step": 1639,
      "training_loss": 6.245546817779541
    },
    {
      "epoch": 0.3553387533875339,
      "step": 1639,
      "training_loss": 5.138453960418701
    },
    {
      "epoch": 0.35555555555555557,
      "grad_norm": 16.801349639892578,
      "learning_rate": 1e-05,
      "loss": 6.4742,
      "step": 1640
    },
    {
      "epoch": 0.35555555555555557,
      "step": 1640,
      "training_loss": 7.151556015014648
    },
    {
      "epoch": 0.35555555555555557,
      "step": 1640,
      "training_loss": 8.366013526916504
    },
    {
      "epoch": 0.35555555555555557,
      "step": 1640,
      "training_loss": 3.653970241546631
    },
    {
      "epoch": 0.35555555555555557,
      "step": 1640,
      "training_loss": 7.412871360778809
    },
    {
      "epoch": 0.35577235772357724,
      "step": 1641,
      "training_loss": 7.611032009124756
    },
    {
      "epoch": 0.35577235772357724,
      "step": 1641,
      "training_loss": 6.810533046722412
    },
    {
      "epoch": 0.35577235772357724,
      "step": 1641,
      "training_loss": 6.246467590332031
    },
    {
      "epoch": 0.35577235772357724,
      "step": 1641,
      "training_loss": 7.0267791748046875
    },
    {
      "epoch": 0.3559891598915989,
      "step": 1642,
      "training_loss": 7.3711981773376465
    },
    {
      "epoch": 0.3559891598915989,
      "step": 1642,
      "training_loss": 6.960945129394531
    },
    {
      "epoch": 0.3559891598915989,
      "step": 1642,
      "training_loss": 7.324847221374512
    },
    {
      "epoch": 0.3559891598915989,
      "step": 1642,
      "training_loss": 5.66038179397583
    },
    {
      "epoch": 0.3562059620596206,
      "step": 1643,
      "training_loss": 8.40167236328125
    },
    {
      "epoch": 0.3562059620596206,
      "step": 1643,
      "training_loss": 6.429330348968506
    },
    {
      "epoch": 0.3562059620596206,
      "step": 1643,
      "training_loss": 6.856107234954834
    },
    {
      "epoch": 0.3562059620596206,
      "step": 1643,
      "training_loss": 7.559565544128418
    },
    {
      "epoch": 0.3564227642276423,
      "grad_norm": 11.973102569580078,
      "learning_rate": 1e-05,
      "loss": 6.9277,
      "step": 1644
    },
    {
      "epoch": 0.3564227642276423,
      "step": 1644,
      "training_loss": 3.9530446529388428
    },
    {
      "epoch": 0.3564227642276423,
      "step": 1644,
      "training_loss": 5.902032375335693
    },
    {
      "epoch": 0.3564227642276423,
      "step": 1644,
      "training_loss": 7.0733819007873535
    },
    {
      "epoch": 0.3564227642276423,
      "step": 1644,
      "training_loss": 7.583714962005615
    },
    {
      "epoch": 0.35663956639566397,
      "step": 1645,
      "training_loss": 8.796030044555664
    },
    {
      "epoch": 0.35663956639566397,
      "step": 1645,
      "training_loss": 3.69950270652771
    },
    {
      "epoch": 0.35663956639566397,
      "step": 1645,
      "training_loss": 9.609399795532227
    },
    {
      "epoch": 0.35663956639566397,
      "step": 1645,
      "training_loss": 6.094902515411377
    },
    {
      "epoch": 0.35685636856368563,
      "step": 1646,
      "training_loss": 7.433208465576172
    },
    {
      "epoch": 0.35685636856368563,
      "step": 1646,
      "training_loss": 4.186668395996094
    },
    {
      "epoch": 0.35685636856368563,
      "step": 1646,
      "training_loss": 6.107023239135742
    },
    {
      "epoch": 0.35685636856368563,
      "step": 1646,
      "training_loss": 6.084893226623535
    },
    {
      "epoch": 0.3570731707317073,
      "step": 1647,
      "training_loss": 7.443645477294922
    },
    {
      "epoch": 0.3570731707317073,
      "step": 1647,
      "training_loss": 7.968600273132324
    },
    {
      "epoch": 0.3570731707317073,
      "step": 1647,
      "training_loss": 5.287439823150635
    },
    {
      "epoch": 0.3570731707317073,
      "step": 1647,
      "training_loss": 5.722078323364258
    },
    {
      "epoch": 0.357289972899729,
      "grad_norm": 15.19559097290039,
      "learning_rate": 1e-05,
      "loss": 6.4341,
      "step": 1648
    },
    {
      "epoch": 0.357289972899729,
      "step": 1648,
      "training_loss": 3.955249071121216
    },
    {
      "epoch": 0.357289972899729,
      "step": 1648,
      "training_loss": 5.844376564025879
    },
    {
      "epoch": 0.357289972899729,
      "step": 1648,
      "training_loss": 5.956125736236572
    },
    {
      "epoch": 0.357289972899729,
      "step": 1648,
      "training_loss": 7.389045715332031
    },
    {
      "epoch": 0.3575067750677507,
      "step": 1649,
      "training_loss": 6.179080486297607
    },
    {
      "epoch": 0.3575067750677507,
      "step": 1649,
      "training_loss": 6.524203300476074
    },
    {
      "epoch": 0.3575067750677507,
      "step": 1649,
      "training_loss": 8.023191452026367
    },
    {
      "epoch": 0.3575067750677507,
      "step": 1649,
      "training_loss": 6.078415393829346
    },
    {
      "epoch": 0.35772357723577236,
      "step": 1650,
      "training_loss": 5.331316947937012
    },
    {
      "epoch": 0.35772357723577236,
      "step": 1650,
      "training_loss": 6.08027982711792
    },
    {
      "epoch": 0.35772357723577236,
      "step": 1650,
      "training_loss": 7.21787691116333
    },
    {
      "epoch": 0.35772357723577236,
      "step": 1650,
      "training_loss": 5.356323719024658
    },
    {
      "epoch": 0.35794037940379403,
      "step": 1651,
      "training_loss": 5.511902809143066
    },
    {
      "epoch": 0.35794037940379403,
      "step": 1651,
      "training_loss": 5.622997283935547
    },
    {
      "epoch": 0.35794037940379403,
      "step": 1651,
      "training_loss": 7.139556884765625
    },
    {
      "epoch": 0.35794037940379403,
      "step": 1651,
      "training_loss": 7.500982284545898
    },
    {
      "epoch": 0.3581571815718157,
      "grad_norm": 11.65412712097168,
      "learning_rate": 1e-05,
      "loss": 6.2319,
      "step": 1652
    },
    {
      "epoch": 0.3581571815718157,
      "step": 1652,
      "training_loss": 6.006218910217285
    },
    {
      "epoch": 0.3581571815718157,
      "step": 1652,
      "training_loss": 6.273341655731201
    },
    {
      "epoch": 0.3581571815718157,
      "step": 1652,
      "training_loss": 6.149144649505615
    },
    {
      "epoch": 0.3581571815718157,
      "step": 1652,
      "training_loss": 4.552186489105225
    },
    {
      "epoch": 0.3583739837398374,
      "step": 1653,
      "training_loss": 6.494911193847656
    },
    {
      "epoch": 0.3583739837398374,
      "step": 1653,
      "training_loss": 5.470125198364258
    },
    {
      "epoch": 0.3583739837398374,
      "step": 1653,
      "training_loss": 8.259756088256836
    },
    {
      "epoch": 0.3583739837398374,
      "step": 1653,
      "training_loss": 7.053183078765869
    },
    {
      "epoch": 0.3585907859078591,
      "step": 1654,
      "training_loss": 6.054500579833984
    },
    {
      "epoch": 0.3585907859078591,
      "step": 1654,
      "training_loss": 3.9174046516418457
    },
    {
      "epoch": 0.3585907859078591,
      "step": 1654,
      "training_loss": 6.927293300628662
    },
    {
      "epoch": 0.3585907859078591,
      "step": 1654,
      "training_loss": 5.237828254699707
    },
    {
      "epoch": 0.35880758807588076,
      "step": 1655,
      "training_loss": 7.208029747009277
    },
    {
      "epoch": 0.35880758807588076,
      "step": 1655,
      "training_loss": 6.704776287078857
    },
    {
      "epoch": 0.35880758807588076,
      "step": 1655,
      "training_loss": 6.130715847015381
    },
    {
      "epoch": 0.35880758807588076,
      "step": 1655,
      "training_loss": 6.482089996337891
    },
    {
      "epoch": 0.3590243902439024,
      "grad_norm": 8.412117004394531,
      "learning_rate": 1e-05,
      "loss": 6.1826,
      "step": 1656
    },
    {
      "epoch": 0.3590243902439024,
      "step": 1656,
      "training_loss": 6.305828094482422
    },
    {
      "epoch": 0.3590243902439024,
      "step": 1656,
      "training_loss": 6.507976055145264
    },
    {
      "epoch": 0.3590243902439024,
      "step": 1656,
      "training_loss": 7.579514026641846
    },
    {
      "epoch": 0.3590243902439024,
      "step": 1656,
      "training_loss": 4.697028636932373
    },
    {
      "epoch": 0.3592411924119241,
      "step": 1657,
      "training_loss": 7.831515312194824
    },
    {
      "epoch": 0.3592411924119241,
      "step": 1657,
      "training_loss": 7.602161884307861
    },
    {
      "epoch": 0.3592411924119241,
      "step": 1657,
      "training_loss": 6.8776960372924805
    },
    {
      "epoch": 0.3592411924119241,
      "step": 1657,
      "training_loss": 6.170588493347168
    },
    {
      "epoch": 0.3594579945799458,
      "step": 1658,
      "training_loss": 5.888300895690918
    },
    {
      "epoch": 0.3594579945799458,
      "step": 1658,
      "training_loss": 6.794041633605957
    },
    {
      "epoch": 0.3594579945799458,
      "step": 1658,
      "training_loss": 7.166503429412842
    },
    {
      "epoch": 0.3594579945799458,
      "step": 1658,
      "training_loss": 6.595521450042725
    },
    {
      "epoch": 0.3596747967479675,
      "step": 1659,
      "training_loss": 5.694127082824707
    },
    {
      "epoch": 0.3596747967479675,
      "step": 1659,
      "training_loss": 6.420421123504639
    },
    {
      "epoch": 0.3596747967479675,
      "step": 1659,
      "training_loss": 7.311382293701172
    },
    {
      "epoch": 0.3596747967479675,
      "step": 1659,
      "training_loss": 8.142448425292969
    },
    {
      "epoch": 0.35989159891598915,
      "grad_norm": 17.29397964477539,
      "learning_rate": 1e-05,
      "loss": 6.7241,
      "step": 1660
    },
    {
      "epoch": 0.35989159891598915,
      "step": 1660,
      "training_loss": 5.450674057006836
    },
    {
      "epoch": 0.35989159891598915,
      "step": 1660,
      "training_loss": 6.981081962585449
    },
    {
      "epoch": 0.35989159891598915,
      "step": 1660,
      "training_loss": 7.4567790031433105
    },
    {
      "epoch": 0.35989159891598915,
      "step": 1660,
      "training_loss": 6.966866493225098
    },
    {
      "epoch": 0.3601084010840108,
      "step": 1661,
      "training_loss": 7.329753875732422
    },
    {
      "epoch": 0.3601084010840108,
      "step": 1661,
      "training_loss": 6.881504535675049
    },
    {
      "epoch": 0.3601084010840108,
      "step": 1661,
      "training_loss": 7.166102886199951
    },
    {
      "epoch": 0.3601084010840108,
      "step": 1661,
      "training_loss": 6.348181247711182
    },
    {
      "epoch": 0.36032520325203254,
      "step": 1662,
      "training_loss": 7.875239849090576
    },
    {
      "epoch": 0.36032520325203254,
      "step": 1662,
      "training_loss": 7.597071647644043
    },
    {
      "epoch": 0.36032520325203254,
      "step": 1662,
      "training_loss": 6.3410563468933105
    },
    {
      "epoch": 0.36032520325203254,
      "step": 1662,
      "training_loss": 7.224948883056641
    },
    {
      "epoch": 0.3605420054200542,
      "step": 1663,
      "training_loss": 6.683969974517822
    },
    {
      "epoch": 0.3605420054200542,
      "step": 1663,
      "training_loss": 7.184784889221191
    },
    {
      "epoch": 0.3605420054200542,
      "step": 1663,
      "training_loss": 5.921461582183838
    },
    {
      "epoch": 0.3605420054200542,
      "step": 1663,
      "training_loss": 7.751577854156494
    },
    {
      "epoch": 0.3607588075880759,
      "grad_norm": 13.811254501342773,
      "learning_rate": 1e-05,
      "loss": 6.9476,
      "step": 1664
    },
    {
      "epoch": 0.3607588075880759,
      "step": 1664,
      "training_loss": 8.26358699798584
    },
    {
      "epoch": 0.3607588075880759,
      "step": 1664,
      "training_loss": 5.81224250793457
    },
    {
      "epoch": 0.3607588075880759,
      "step": 1664,
      "training_loss": 7.072081565856934
    },
    {
      "epoch": 0.3607588075880759,
      "step": 1664,
      "training_loss": 7.177028179168701
    },
    {
      "epoch": 0.36097560975609755,
      "step": 1665,
      "training_loss": 6.941091537475586
    },
    {
      "epoch": 0.36097560975609755,
      "step": 1665,
      "training_loss": 6.928192138671875
    },
    {
      "epoch": 0.36097560975609755,
      "step": 1665,
      "training_loss": 4.553476333618164
    },
    {
      "epoch": 0.36097560975609755,
      "step": 1665,
      "training_loss": 7.673747539520264
    },
    {
      "epoch": 0.3611924119241192,
      "step": 1666,
      "training_loss": 7.050045013427734
    },
    {
      "epoch": 0.3611924119241192,
      "step": 1666,
      "training_loss": 6.447944164276123
    },
    {
      "epoch": 0.3611924119241192,
      "step": 1666,
      "training_loss": 6.10116720199585
    },
    {
      "epoch": 0.3611924119241192,
      "step": 1666,
      "training_loss": 5.899255275726318
    },
    {
      "epoch": 0.36140921409214094,
      "step": 1667,
      "training_loss": 5.703483581542969
    },
    {
      "epoch": 0.36140921409214094,
      "step": 1667,
      "training_loss": 8.408913612365723
    },
    {
      "epoch": 0.36140921409214094,
      "step": 1667,
      "training_loss": 6.369351863861084
    },
    {
      "epoch": 0.36140921409214094,
      "step": 1667,
      "training_loss": 6.775967121124268
    },
    {
      "epoch": 0.3616260162601626,
      "grad_norm": 19.17467498779297,
      "learning_rate": 1e-05,
      "loss": 6.6986,
      "step": 1668
    },
    {
      "epoch": 0.3616260162601626,
      "step": 1668,
      "training_loss": 6.670784950256348
    },
    {
      "epoch": 0.3616260162601626,
      "step": 1668,
      "training_loss": 6.092491149902344
    },
    {
      "epoch": 0.3616260162601626,
      "step": 1668,
      "training_loss": 6.091737747192383
    },
    {
      "epoch": 0.3616260162601626,
      "step": 1668,
      "training_loss": 5.4049530029296875
    },
    {
      "epoch": 0.3618428184281843,
      "step": 1669,
      "training_loss": 6.669806957244873
    },
    {
      "epoch": 0.3618428184281843,
      "step": 1669,
      "training_loss": 6.435513019561768
    },
    {
      "epoch": 0.3618428184281843,
      "step": 1669,
      "training_loss": 6.26580286026001
    },
    {
      "epoch": 0.3618428184281843,
      "step": 1669,
      "training_loss": 7.313808917999268
    },
    {
      "epoch": 0.36205962059620594,
      "step": 1670,
      "training_loss": 6.770855903625488
    },
    {
      "epoch": 0.36205962059620594,
      "step": 1670,
      "training_loss": 7.151103973388672
    },
    {
      "epoch": 0.36205962059620594,
      "step": 1670,
      "training_loss": 5.0936431884765625
    },
    {
      "epoch": 0.36205962059620594,
      "step": 1670,
      "training_loss": 7.277982711791992
    },
    {
      "epoch": 0.36227642276422767,
      "step": 1671,
      "training_loss": 6.598198890686035
    },
    {
      "epoch": 0.36227642276422767,
      "step": 1671,
      "training_loss": 7.1664910316467285
    },
    {
      "epoch": 0.36227642276422767,
      "step": 1671,
      "training_loss": 3.6708950996398926
    },
    {
      "epoch": 0.36227642276422767,
      "step": 1671,
      "training_loss": 6.417576313018799
    },
    {
      "epoch": 0.36249322493224934,
      "grad_norm": 13.040349006652832,
      "learning_rate": 1e-05,
      "loss": 6.3182,
      "step": 1672
    },
    {
      "epoch": 0.36249322493224934,
      "step": 1672,
      "training_loss": 7.824430465698242
    },
    {
      "epoch": 0.36249322493224934,
      "step": 1672,
      "training_loss": 6.7814130783081055
    },
    {
      "epoch": 0.36249322493224934,
      "step": 1672,
      "training_loss": 8.006861686706543
    },
    {
      "epoch": 0.36249322493224934,
      "step": 1672,
      "training_loss": 5.640092849731445
    },
    {
      "epoch": 0.362710027100271,
      "step": 1673,
      "training_loss": 6.825302600860596
    },
    {
      "epoch": 0.362710027100271,
      "step": 1673,
      "training_loss": 7.017777442932129
    },
    {
      "epoch": 0.362710027100271,
      "step": 1673,
      "training_loss": 6.749617099761963
    },
    {
      "epoch": 0.362710027100271,
      "step": 1673,
      "training_loss": 6.183072566986084
    },
    {
      "epoch": 0.36292682926829267,
      "step": 1674,
      "training_loss": 7.426080226898193
    },
    {
      "epoch": 0.36292682926829267,
      "step": 1674,
      "training_loss": 6.003681659698486
    },
    {
      "epoch": 0.36292682926829267,
      "step": 1674,
      "training_loss": 6.893812656402588
    },
    {
      "epoch": 0.36292682926829267,
      "step": 1674,
      "training_loss": 7.900667667388916
    },
    {
      "epoch": 0.36314363143631434,
      "step": 1675,
      "training_loss": 7.558669090270996
    },
    {
      "epoch": 0.36314363143631434,
      "step": 1675,
      "training_loss": 6.8383708000183105
    },
    {
      "epoch": 0.36314363143631434,
      "step": 1675,
      "training_loss": 7.552821159362793
    },
    {
      "epoch": 0.36314363143631434,
      "step": 1675,
      "training_loss": 6.307906150817871
    },
    {
      "epoch": 0.36336043360433606,
      "grad_norm": 18.380369186401367,
      "learning_rate": 1e-05,
      "loss": 6.9694,
      "step": 1676
    },
    {
      "epoch": 0.36336043360433606,
      "step": 1676,
      "training_loss": 5.626914978027344
    },
    {
      "epoch": 0.36336043360433606,
      "step": 1676,
      "training_loss": 5.7131757736206055
    },
    {
      "epoch": 0.36336043360433606,
      "step": 1676,
      "training_loss": 6.938427925109863
    },
    {
      "epoch": 0.36336043360433606,
      "step": 1676,
      "training_loss": 7.646144390106201
    },
    {
      "epoch": 0.36357723577235773,
      "step": 1677,
      "training_loss": 6.022496223449707
    },
    {
      "epoch": 0.36357723577235773,
      "step": 1677,
      "training_loss": 6.857547760009766
    },
    {
      "epoch": 0.36357723577235773,
      "step": 1677,
      "training_loss": 6.448421478271484
    },
    {
      "epoch": 0.36357723577235773,
      "step": 1677,
      "training_loss": 6.635365009307861
    },
    {
      "epoch": 0.3637940379403794,
      "step": 1678,
      "training_loss": 6.727620601654053
    },
    {
      "epoch": 0.3637940379403794,
      "step": 1678,
      "training_loss": 6.605408191680908
    },
    {
      "epoch": 0.3637940379403794,
      "step": 1678,
      "training_loss": 8.028505325317383
    },
    {
      "epoch": 0.3637940379403794,
      "step": 1678,
      "training_loss": 7.551763534545898
    },
    {
      "epoch": 0.36401084010840107,
      "step": 1679,
      "training_loss": 4.353610038757324
    },
    {
      "epoch": 0.36401084010840107,
      "step": 1679,
      "training_loss": 6.121728420257568
    },
    {
      "epoch": 0.36401084010840107,
      "step": 1679,
      "training_loss": 6.808550834655762
    },
    {
      "epoch": 0.36401084010840107,
      "step": 1679,
      "training_loss": 6.261069297790527
    },
    {
      "epoch": 0.3642276422764228,
      "grad_norm": 14.541967391967773,
      "learning_rate": 1e-05,
      "loss": 6.5217,
      "step": 1680
    },
    {
      "epoch": 0.3642276422764228,
      "step": 1680,
      "training_loss": 6.553010940551758
    },
    {
      "epoch": 0.3642276422764228,
      "step": 1680,
      "training_loss": 7.238440036773682
    },
    {
      "epoch": 0.3642276422764228,
      "step": 1680,
      "training_loss": 6.394287109375
    },
    {
      "epoch": 0.3642276422764228,
      "step": 1680,
      "training_loss": 8.533357620239258
    },
    {
      "epoch": 0.36444444444444446,
      "step": 1681,
      "training_loss": 6.60529899597168
    },
    {
      "epoch": 0.36444444444444446,
      "step": 1681,
      "training_loss": 6.040898323059082
    },
    {
      "epoch": 0.36444444444444446,
      "step": 1681,
      "training_loss": 7.375764846801758
    },
    {
      "epoch": 0.36444444444444446,
      "step": 1681,
      "training_loss": 5.5486555099487305
    },
    {
      "epoch": 0.36466124661246613,
      "step": 1682,
      "training_loss": 7.132476329803467
    },
    {
      "epoch": 0.36466124661246613,
      "step": 1682,
      "training_loss": 6.042782783508301
    },
    {
      "epoch": 0.36466124661246613,
      "step": 1682,
      "training_loss": 7.515349864959717
    },
    {
      "epoch": 0.36466124661246613,
      "step": 1682,
      "training_loss": 6.939192771911621
    },
    {
      "epoch": 0.3648780487804878,
      "step": 1683,
      "training_loss": 6.294944763183594
    },
    {
      "epoch": 0.3648780487804878,
      "step": 1683,
      "training_loss": 7.303005695343018
    },
    {
      "epoch": 0.3648780487804878,
      "step": 1683,
      "training_loss": 7.767099380493164
    },
    {
      "epoch": 0.3648780487804878,
      "step": 1683,
      "training_loss": 6.041337013244629
    },
    {
      "epoch": 0.36509485094850946,
      "grad_norm": 16.41422462463379,
      "learning_rate": 1e-05,
      "loss": 6.8329,
      "step": 1684
    },
    {
      "epoch": 0.36509485094850946,
      "step": 1684,
      "training_loss": 7.268341541290283
    },
    {
      "epoch": 0.36509485094850946,
      "step": 1684,
      "training_loss": 5.74672794342041
    },
    {
      "epoch": 0.36509485094850946,
      "step": 1684,
      "training_loss": 6.886046886444092
    },
    {
      "epoch": 0.36509485094850946,
      "step": 1684,
      "training_loss": 7.638936519622803
    },
    {
      "epoch": 0.3653116531165312,
      "step": 1685,
      "training_loss": 6.105523109436035
    },
    {
      "epoch": 0.3653116531165312,
      "step": 1685,
      "training_loss": 9.135634422302246
    },
    {
      "epoch": 0.3653116531165312,
      "step": 1685,
      "training_loss": 7.10502815246582
    },
    {
      "epoch": 0.3653116531165312,
      "step": 1685,
      "training_loss": 5.717620372772217
    },
    {
      "epoch": 0.36552845528455286,
      "step": 1686,
      "training_loss": 6.607359886169434
    },
    {
      "epoch": 0.36552845528455286,
      "step": 1686,
      "training_loss": 6.288303852081299
    },
    {
      "epoch": 0.36552845528455286,
      "step": 1686,
      "training_loss": 4.478786945343018
    },
    {
      "epoch": 0.36552845528455286,
      "step": 1686,
      "training_loss": 7.2589802742004395
    },
    {
      "epoch": 0.3657452574525745,
      "step": 1687,
      "training_loss": 6.486997604370117
    },
    {
      "epoch": 0.3657452574525745,
      "step": 1687,
      "training_loss": 6.800708293914795
    },
    {
      "epoch": 0.3657452574525745,
      "step": 1687,
      "training_loss": 6.346968650817871
    },
    {
      "epoch": 0.3657452574525745,
      "step": 1687,
      "training_loss": 6.912776470184326
    },
    {
      "epoch": 0.3659620596205962,
      "grad_norm": 11.819332122802734,
      "learning_rate": 1e-05,
      "loss": 6.674,
      "step": 1688
    },
    {
      "epoch": 0.3659620596205962,
      "step": 1688,
      "training_loss": 6.807143211364746
    },
    {
      "epoch": 0.3659620596205962,
      "step": 1688,
      "training_loss": 6.190027236938477
    },
    {
      "epoch": 0.3659620596205962,
      "step": 1688,
      "training_loss": 5.0939555168151855
    },
    {
      "epoch": 0.3659620596205962,
      "step": 1688,
      "training_loss": 6.651963233947754
    },
    {
      "epoch": 0.36617886178861786,
      "step": 1689,
      "training_loss": 6.105930328369141
    },
    {
      "epoch": 0.36617886178861786,
      "step": 1689,
      "training_loss": 6.937415599822998
    },
    {
      "epoch": 0.36617886178861786,
      "step": 1689,
      "training_loss": 6.160190105438232
    },
    {
      "epoch": 0.36617886178861786,
      "step": 1689,
      "training_loss": 5.990450859069824
    },
    {
      "epoch": 0.3663956639566396,
      "step": 1690,
      "training_loss": 7.367300033569336
    },
    {
      "epoch": 0.3663956639566396,
      "step": 1690,
      "training_loss": 7.605810165405273
    },
    {
      "epoch": 0.3663956639566396,
      "step": 1690,
      "training_loss": 8.160783767700195
    },
    {
      "epoch": 0.3663956639566396,
      "step": 1690,
      "training_loss": 6.491050720214844
    },
    {
      "epoch": 0.36661246612466125,
      "step": 1691,
      "training_loss": 6.996105670928955
    },
    {
      "epoch": 0.36661246612466125,
      "step": 1691,
      "training_loss": 6.472946643829346
    },
    {
      "epoch": 0.36661246612466125,
      "step": 1691,
      "training_loss": 6.898610591888428
    },
    {
      "epoch": 0.36661246612466125,
      "step": 1691,
      "training_loss": 6.458858013153076
    },
    {
      "epoch": 0.3668292682926829,
      "grad_norm": 13.553308486938477,
      "learning_rate": 1e-05,
      "loss": 6.6493,
      "step": 1692
    },
    {
      "epoch": 0.3668292682926829,
      "step": 1692,
      "training_loss": 4.7806901931762695
    },
    {
      "epoch": 0.3668292682926829,
      "step": 1692,
      "training_loss": 6.347671985626221
    },
    {
      "epoch": 0.3668292682926829,
      "step": 1692,
      "training_loss": 4.801697731018066
    },
    {
      "epoch": 0.3668292682926829,
      "step": 1692,
      "training_loss": 7.319849014282227
    },
    {
      "epoch": 0.3670460704607046,
      "step": 1693,
      "training_loss": 7.174954891204834
    },
    {
      "epoch": 0.3670460704607046,
      "step": 1693,
      "training_loss": 6.619477272033691
    },
    {
      "epoch": 0.3670460704607046,
      "step": 1693,
      "training_loss": 6.408192157745361
    },
    {
      "epoch": 0.3670460704607046,
      "step": 1693,
      "training_loss": 7.760904788970947
    },
    {
      "epoch": 0.3672628726287263,
      "step": 1694,
      "training_loss": 7.052290439605713
    },
    {
      "epoch": 0.3672628726287263,
      "step": 1694,
      "training_loss": 3.5483059883117676
    },
    {
      "epoch": 0.3672628726287263,
      "step": 1694,
      "training_loss": 6.595254898071289
    },
    {
      "epoch": 0.3672628726287263,
      "step": 1694,
      "training_loss": 6.252882480621338
    },
    {
      "epoch": 0.367479674796748,
      "step": 1695,
      "training_loss": 8.343596458435059
    },
    {
      "epoch": 0.367479674796748,
      "step": 1695,
      "training_loss": 4.739311218261719
    },
    {
      "epoch": 0.367479674796748,
      "step": 1695,
      "training_loss": 7.42041015625
    },
    {
      "epoch": 0.367479674796748,
      "step": 1695,
      "training_loss": 6.363532543182373
    },
    {
      "epoch": 0.36769647696476965,
      "grad_norm": 13.095319747924805,
      "learning_rate": 1e-05,
      "loss": 6.3456,
      "step": 1696
    },
    {
      "epoch": 0.36769647696476965,
      "step": 1696,
      "training_loss": 8.461319923400879
    },
    {
      "epoch": 0.36769647696476965,
      "step": 1696,
      "training_loss": 6.172823905944824
    },
    {
      "epoch": 0.36769647696476965,
      "step": 1696,
      "training_loss": 7.1682024002075195
    },
    {
      "epoch": 0.36769647696476965,
      "step": 1696,
      "training_loss": 6.5796003341674805
    },
    {
      "epoch": 0.3679132791327913,
      "step": 1697,
      "training_loss": 5.998425483703613
    },
    {
      "epoch": 0.3679132791327913,
      "step": 1697,
      "training_loss": 6.81593132019043
    },
    {
      "epoch": 0.3679132791327913,
      "step": 1697,
      "training_loss": 7.169168949127197
    },
    {
      "epoch": 0.3679132791327913,
      "step": 1697,
      "training_loss": 6.7439188957214355
    },
    {
      "epoch": 0.368130081300813,
      "step": 1698,
      "training_loss": 5.932453632354736
    },
    {
      "epoch": 0.368130081300813,
      "step": 1698,
      "training_loss": 8.373819351196289
    },
    {
      "epoch": 0.368130081300813,
      "step": 1698,
      "training_loss": 6.489739418029785
    },
    {
      "epoch": 0.368130081300813,
      "step": 1698,
      "training_loss": 3.772644281387329
    },
    {
      "epoch": 0.3683468834688347,
      "step": 1699,
      "training_loss": 6.210227012634277
    },
    {
      "epoch": 0.3683468834688347,
      "step": 1699,
      "training_loss": 8.430508613586426
    },
    {
      "epoch": 0.3683468834688347,
      "step": 1699,
      "training_loss": 6.866250514984131
    },
    {
      "epoch": 0.3683468834688347,
      "step": 1699,
      "training_loss": 7.171442985534668
    },
    {
      "epoch": 0.3685636856368564,
      "grad_norm": 13.91926383972168,
      "learning_rate": 1e-05,
      "loss": 6.7723,
      "step": 1700
    },
    {
      "epoch": 0.3685636856368564,
      "step": 1700,
      "training_loss": 6.844139099121094
    },
    {
      "epoch": 0.3685636856368564,
      "step": 1700,
      "training_loss": 7.766231536865234
    },
    {
      "epoch": 0.3685636856368564,
      "step": 1700,
      "training_loss": 6.005216121673584
    },
    {
      "epoch": 0.3685636856368564,
      "step": 1700,
      "training_loss": 5.876101016998291
    },
    {
      "epoch": 0.36878048780487804,
      "step": 1701,
      "training_loss": 6.199299335479736
    },
    {
      "epoch": 0.36878048780487804,
      "step": 1701,
      "training_loss": 5.930431365966797
    },
    {
      "epoch": 0.36878048780487804,
      "step": 1701,
      "training_loss": 3.399385690689087
    },
    {
      "epoch": 0.36878048780487804,
      "step": 1701,
      "training_loss": 7.296216011047363
    },
    {
      "epoch": 0.3689972899728997,
      "step": 1702,
      "training_loss": 5.42437219619751
    },
    {
      "epoch": 0.3689972899728997,
      "step": 1702,
      "training_loss": 6.236000061035156
    },
    {
      "epoch": 0.3689972899728997,
      "step": 1702,
      "training_loss": 6.668696403503418
    },
    {
      "epoch": 0.3689972899728997,
      "step": 1702,
      "training_loss": 5.2215895652771
    },
    {
      "epoch": 0.36921409214092143,
      "step": 1703,
      "training_loss": 6.133433818817139
    },
    {
      "epoch": 0.36921409214092143,
      "step": 1703,
      "training_loss": 7.301944732666016
    },
    {
      "epoch": 0.36921409214092143,
      "step": 1703,
      "training_loss": 9.187355995178223
    },
    {
      "epoch": 0.36921409214092143,
      "step": 1703,
      "training_loss": 6.176915645599365
    },
    {
      "epoch": 0.3694308943089431,
      "grad_norm": 23.259113311767578,
      "learning_rate": 1e-05,
      "loss": 6.3542,
      "step": 1704
    },
    {
      "epoch": 0.3694308943089431,
      "step": 1704,
      "training_loss": 6.864190578460693
    },
    {
      "epoch": 0.3694308943089431,
      "step": 1704,
      "training_loss": 6.552988529205322
    },
    {
      "epoch": 0.3694308943089431,
      "step": 1704,
      "training_loss": 6.748276233673096
    },
    {
      "epoch": 0.3694308943089431,
      "step": 1704,
      "training_loss": 7.252100944519043
    },
    {
      "epoch": 0.36964769647696477,
      "step": 1705,
      "training_loss": 7.113383769989014
    },
    {
      "epoch": 0.36964769647696477,
      "step": 1705,
      "training_loss": 6.834072589874268
    },
    {
      "epoch": 0.36964769647696477,
      "step": 1705,
      "training_loss": 5.503541469573975
    },
    {
      "epoch": 0.36964769647696477,
      "step": 1705,
      "training_loss": 6.496250629425049
    },
    {
      "epoch": 0.36986449864498644,
      "step": 1706,
      "training_loss": 7.178794860839844
    },
    {
      "epoch": 0.36986449864498644,
      "step": 1706,
      "training_loss": 5.583669185638428
    },
    {
      "epoch": 0.36986449864498644,
      "step": 1706,
      "training_loss": 6.028750896453857
    },
    {
      "epoch": 0.36986449864498644,
      "step": 1706,
      "training_loss": 6.152188777923584
    },
    {
      "epoch": 0.3700813008130081,
      "step": 1707,
      "training_loss": 6.902048110961914
    },
    {
      "epoch": 0.3700813008130081,
      "step": 1707,
      "training_loss": 6.993825435638428
    },
    {
      "epoch": 0.3700813008130081,
      "step": 1707,
      "training_loss": 7.073884010314941
    },
    {
      "epoch": 0.3700813008130081,
      "step": 1707,
      "training_loss": 6.469703197479248
    },
    {
      "epoch": 0.37029810298102983,
      "grad_norm": 12.652093887329102,
      "learning_rate": 1e-05,
      "loss": 6.6092,
      "step": 1708
    },
    {
      "epoch": 0.37029810298102983,
      "step": 1708,
      "training_loss": 4.466176986694336
    },
    {
      "epoch": 0.37029810298102983,
      "step": 1708,
      "training_loss": 5.820239067077637
    },
    {
      "epoch": 0.37029810298102983,
      "step": 1708,
      "training_loss": 5.653407573699951
    },
    {
      "epoch": 0.37029810298102983,
      "step": 1708,
      "training_loss": 7.014603614807129
    },
    {
      "epoch": 0.3705149051490515,
      "step": 1709,
      "training_loss": 6.554520606994629
    },
    {
      "epoch": 0.3705149051490515,
      "step": 1709,
      "training_loss": 6.670101165771484
    },
    {
      "epoch": 0.3705149051490515,
      "step": 1709,
      "training_loss": 6.462254524230957
    },
    {
      "epoch": 0.3705149051490515,
      "step": 1709,
      "training_loss": 7.0573272705078125
    },
    {
      "epoch": 0.37073170731707317,
      "step": 1710,
      "training_loss": 6.8772873878479
    },
    {
      "epoch": 0.37073170731707317,
      "step": 1710,
      "training_loss": 6.873913288116455
    },
    {
      "epoch": 0.37073170731707317,
      "step": 1710,
      "training_loss": 6.969480991363525
    },
    {
      "epoch": 0.37073170731707317,
      "step": 1710,
      "training_loss": 6.832742691040039
    },
    {
      "epoch": 0.37094850948509484,
      "step": 1711,
      "training_loss": 6.797446250915527
    },
    {
      "epoch": 0.37094850948509484,
      "step": 1711,
      "training_loss": 6.545215606689453
    },
    {
      "epoch": 0.37094850948509484,
      "step": 1711,
      "training_loss": 7.6814045906066895
    },
    {
      "epoch": 0.37094850948509484,
      "step": 1711,
      "training_loss": 7.570361137390137
    },
    {
      "epoch": 0.37116531165311656,
      "grad_norm": 16.15659523010254,
      "learning_rate": 1e-05,
      "loss": 6.6154,
      "step": 1712
    },
    {
      "epoch": 0.37116531165311656,
      "step": 1712,
      "training_loss": 3.276787042617798
    },
    {
      "epoch": 0.37116531165311656,
      "step": 1712,
      "training_loss": 6.521420001983643
    },
    {
      "epoch": 0.37116531165311656,
      "step": 1712,
      "training_loss": 6.655952453613281
    },
    {
      "epoch": 0.37116531165311656,
      "step": 1712,
      "training_loss": 7.029144763946533
    },
    {
      "epoch": 0.3713821138211382,
      "step": 1713,
      "training_loss": 7.181362628936768
    },
    {
      "epoch": 0.3713821138211382,
      "step": 1713,
      "training_loss": 6.8262152671813965
    },
    {
      "epoch": 0.3713821138211382,
      "step": 1713,
      "training_loss": 6.70470666885376
    },
    {
      "epoch": 0.3713821138211382,
      "step": 1713,
      "training_loss": 6.533786296844482
    },
    {
      "epoch": 0.3715989159891599,
      "step": 1714,
      "training_loss": 6.699630260467529
    },
    {
      "epoch": 0.3715989159891599,
      "step": 1714,
      "training_loss": 6.747594833374023
    },
    {
      "epoch": 0.3715989159891599,
      "step": 1714,
      "training_loss": 7.374433994293213
    },
    {
      "epoch": 0.3715989159891599,
      "step": 1714,
      "training_loss": 7.420838356018066
    },
    {
      "epoch": 0.37181571815718156,
      "step": 1715,
      "training_loss": 7.117231845855713
    },
    {
      "epoch": 0.37181571815718156,
      "step": 1715,
      "training_loss": 7.522427082061768
    },
    {
      "epoch": 0.37181571815718156,
      "step": 1715,
      "training_loss": 7.398263931274414
    },
    {
      "epoch": 0.37181571815718156,
      "step": 1715,
      "training_loss": 7.333362102508545
    },
    {
      "epoch": 0.37203252032520323,
      "grad_norm": 16.165889739990234,
      "learning_rate": 1e-05,
      "loss": 6.7714,
      "step": 1716
    },
    {
      "epoch": 0.37203252032520323,
      "step": 1716,
      "training_loss": 6.160750865936279
    },
    {
      "epoch": 0.37203252032520323,
      "step": 1716,
      "training_loss": 5.140091896057129
    },
    {
      "epoch": 0.37203252032520323,
      "step": 1716,
      "training_loss": 3.6515841484069824
    },
    {
      "epoch": 0.37203252032520323,
      "step": 1716,
      "training_loss": 6.796846866607666
    },
    {
      "epoch": 0.37224932249322495,
      "step": 1717,
      "training_loss": 5.608113765716553
    },
    {
      "epoch": 0.37224932249322495,
      "step": 1717,
      "training_loss": 6.672998428344727
    },
    {
      "epoch": 0.37224932249322495,
      "step": 1717,
      "training_loss": 6.546082019805908
    },
    {
      "epoch": 0.37224932249322495,
      "step": 1717,
      "training_loss": 6.693213939666748
    },
    {
      "epoch": 0.3724661246612466,
      "step": 1718,
      "training_loss": 6.272661209106445
    },
    {
      "epoch": 0.3724661246612466,
      "step": 1718,
      "training_loss": 3.301213264465332
    },
    {
      "epoch": 0.3724661246612466,
      "step": 1718,
      "training_loss": 6.453814506530762
    },
    {
      "epoch": 0.3724661246612466,
      "step": 1718,
      "training_loss": 7.150961875915527
    },
    {
      "epoch": 0.3726829268292683,
      "step": 1719,
      "training_loss": 6.960754871368408
    },
    {
      "epoch": 0.3726829268292683,
      "step": 1719,
      "training_loss": 8.028923034667969
    },
    {
      "epoch": 0.3726829268292683,
      "step": 1719,
      "training_loss": 8.0631742477417
    },
    {
      "epoch": 0.3726829268292683,
      "step": 1719,
      "training_loss": 6.62954044342041
    },
    {
      "epoch": 0.37289972899728996,
      "grad_norm": 19.718482971191406,
      "learning_rate": 1e-05,
      "loss": 6.2582,
      "step": 1720
    },
    {
      "epoch": 0.37289972899728996,
      "step": 1720,
      "training_loss": 8.069450378417969
    },
    {
      "epoch": 0.37289972899728996,
      "step": 1720,
      "training_loss": 5.622835636138916
    },
    {
      "epoch": 0.37289972899728996,
      "step": 1720,
      "training_loss": 7.800941467285156
    },
    {
      "epoch": 0.37289972899728996,
      "step": 1720,
      "training_loss": 5.14333438873291
    },
    {
      "epoch": 0.3731165311653116,
      "step": 1721,
      "training_loss": 6.377940654754639
    },
    {
      "epoch": 0.3731165311653116,
      "step": 1721,
      "training_loss": 5.874131202697754
    },
    {
      "epoch": 0.3731165311653116,
      "step": 1721,
      "training_loss": 6.5710978507995605
    },
    {
      "epoch": 0.3731165311653116,
      "step": 1721,
      "training_loss": 6.576568126678467
    },
    {
      "epoch": 0.37333333333333335,
      "step": 1722,
      "training_loss": 5.577570915222168
    },
    {
      "epoch": 0.37333333333333335,
      "step": 1722,
      "training_loss": 6.812070369720459
    },
    {
      "epoch": 0.37333333333333335,
      "step": 1722,
      "training_loss": 5.0146684646606445
    },
    {
      "epoch": 0.37333333333333335,
      "step": 1722,
      "training_loss": 6.843521595001221
    },
    {
      "epoch": 0.373550135501355,
      "step": 1723,
      "training_loss": 5.350793838500977
    },
    {
      "epoch": 0.373550135501355,
      "step": 1723,
      "training_loss": 5.935794353485107
    },
    {
      "epoch": 0.373550135501355,
      "step": 1723,
      "training_loss": 6.692995548248291
    },
    {
      "epoch": 0.373550135501355,
      "step": 1723,
      "training_loss": 3.969986915588379
    },
    {
      "epoch": 0.3737669376693767,
      "grad_norm": 12.88603687286377,
      "learning_rate": 1e-05,
      "loss": 6.1396,
      "step": 1724
    },
    {
      "epoch": 0.3737669376693767,
      "step": 1724,
      "training_loss": 7.712812423706055
    },
    {
      "epoch": 0.3737669376693767,
      "step": 1724,
      "training_loss": 6.926775932312012
    },
    {
      "epoch": 0.3737669376693767,
      "step": 1724,
      "training_loss": 6.819137096405029
    },
    {
      "epoch": 0.3737669376693767,
      "step": 1724,
      "training_loss": 5.654843330383301
    },
    {
      "epoch": 0.37398373983739835,
      "step": 1725,
      "training_loss": 3.4586658477783203
    },
    {
      "epoch": 0.37398373983739835,
      "step": 1725,
      "training_loss": 7.073276519775391
    },
    {
      "epoch": 0.37398373983739835,
      "step": 1725,
      "training_loss": 6.171370506286621
    },
    {
      "epoch": 0.37398373983739835,
      "step": 1725,
      "training_loss": 5.698232173919678
    },
    {
      "epoch": 0.3742005420054201,
      "step": 1726,
      "training_loss": 4.365842819213867
    },
    {
      "epoch": 0.3742005420054201,
      "step": 1726,
      "training_loss": 7.154234886169434
    },
    {
      "epoch": 0.3742005420054201,
      "step": 1726,
      "training_loss": 5.672484874725342
    },
    {
      "epoch": 0.3742005420054201,
      "step": 1726,
      "training_loss": 6.513164043426514
    },
    {
      "epoch": 0.37441734417344175,
      "step": 1727,
      "training_loss": 7.782552242279053
    },
    {
      "epoch": 0.37441734417344175,
      "step": 1727,
      "training_loss": 5.305847644805908
    },
    {
      "epoch": 0.37441734417344175,
      "step": 1727,
      "training_loss": 7.492905616760254
    },
    {
      "epoch": 0.37441734417344175,
      "step": 1727,
      "training_loss": 6.403732776641846
    },
    {
      "epoch": 0.3746341463414634,
      "grad_norm": 18.462575912475586,
      "learning_rate": 1e-05,
      "loss": 6.2629,
      "step": 1728
    },
    {
      "epoch": 0.3746341463414634,
      "step": 1728,
      "training_loss": 5.852160930633545
    },
    {
      "epoch": 0.3746341463414634,
      "step": 1728,
      "training_loss": 6.510371685028076
    },
    {
      "epoch": 0.3746341463414634,
      "step": 1728,
      "training_loss": 6.957988739013672
    },
    {
      "epoch": 0.3746341463414634,
      "step": 1728,
      "training_loss": 6.811238765716553
    },
    {
      "epoch": 0.3748509485094851,
      "step": 1729,
      "training_loss": 4.73267126083374
    },
    {
      "epoch": 0.3748509485094851,
      "step": 1729,
      "training_loss": 5.778157711029053
    },
    {
      "epoch": 0.3748509485094851,
      "step": 1729,
      "training_loss": 7.558765411376953
    },
    {
      "epoch": 0.3748509485094851,
      "step": 1729,
      "training_loss": 6.832138538360596
    },
    {
      "epoch": 0.37506775067750675,
      "step": 1730,
      "training_loss": 7.083914756774902
    },
    {
      "epoch": 0.37506775067750675,
      "step": 1730,
      "training_loss": 6.911636829376221
    },
    {
      "epoch": 0.37506775067750675,
      "step": 1730,
      "training_loss": 6.755860805511475
    },
    {
      "epoch": 0.37506775067750675,
      "step": 1730,
      "training_loss": 3.60337495803833
    },
    {
      "epoch": 0.3752845528455285,
      "step": 1731,
      "training_loss": 7.417530059814453
    },
    {
      "epoch": 0.3752845528455285,
      "step": 1731,
      "training_loss": 6.1452155113220215
    },
    {
      "epoch": 0.3752845528455285,
      "step": 1731,
      "training_loss": 7.378807544708252
    },
    {
      "epoch": 0.3752845528455285,
      "step": 1731,
      "training_loss": 7.493928909301758
    },
    {
      "epoch": 0.37550135501355014,
      "grad_norm": 17.919069290161133,
      "learning_rate": 1e-05,
      "loss": 6.489,
      "step": 1732
    },
    {
      "epoch": 0.37550135501355014,
      "step": 1732,
      "training_loss": 4.769728183746338
    },
    {
      "epoch": 0.37550135501355014,
      "step": 1732,
      "training_loss": 6.854294300079346
    },
    {
      "epoch": 0.37550135501355014,
      "step": 1732,
      "training_loss": 7.335245132446289
    },
    {
      "epoch": 0.37550135501355014,
      "step": 1732,
      "training_loss": 3.4069159030914307
    },
    {
      "epoch": 0.3757181571815718,
      "step": 1733,
      "training_loss": 7.568099498748779
    },
    {
      "epoch": 0.3757181571815718,
      "step": 1733,
      "training_loss": 7.136455535888672
    },
    {
      "epoch": 0.3757181571815718,
      "step": 1733,
      "training_loss": 5.283815383911133
    },
    {
      "epoch": 0.3757181571815718,
      "step": 1733,
      "training_loss": 7.194512367248535
    },
    {
      "epoch": 0.3759349593495935,
      "step": 1734,
      "training_loss": 5.955873966217041
    },
    {
      "epoch": 0.3759349593495935,
      "step": 1734,
      "training_loss": 6.4051833152771
    },
    {
      "epoch": 0.3759349593495935,
      "step": 1734,
      "training_loss": 6.851061820983887
    },
    {
      "epoch": 0.3759349593495935,
      "step": 1734,
      "training_loss": 6.7146453857421875
    },
    {
      "epoch": 0.3761517615176152,
      "step": 1735,
      "training_loss": 6.714278697967529
    },
    {
      "epoch": 0.3761517615176152,
      "step": 1735,
      "training_loss": 6.98076057434082
    },
    {
      "epoch": 0.3761517615176152,
      "step": 1735,
      "training_loss": 6.778404712677002
    },
    {
      "epoch": 0.3761517615176152,
      "step": 1735,
      "training_loss": 6.7513346672058105
    },
    {
      "epoch": 0.37636856368563687,
      "grad_norm": 12.385355949401855,
      "learning_rate": 1e-05,
      "loss": 6.4188,
      "step": 1736
    },
    {
      "epoch": 0.37636856368563687,
      "step": 1736,
      "training_loss": 7.628412246704102
    },
    {
      "epoch": 0.37636856368563687,
      "step": 1736,
      "training_loss": 6.861084938049316
    },
    {
      "epoch": 0.37636856368563687,
      "step": 1736,
      "training_loss": 5.665472507476807
    },
    {
      "epoch": 0.37636856368563687,
      "step": 1736,
      "training_loss": 7.179157733917236
    },
    {
      "epoch": 0.37658536585365854,
      "step": 1737,
      "training_loss": 7.19644832611084
    },
    {
      "epoch": 0.37658536585365854,
      "step": 1737,
      "training_loss": 7.82399320602417
    },
    {
      "epoch": 0.37658536585365854,
      "step": 1737,
      "training_loss": 7.427751541137695
    },
    {
      "epoch": 0.37658536585365854,
      "step": 1737,
      "training_loss": 6.597367763519287
    },
    {
      "epoch": 0.3768021680216802,
      "step": 1738,
      "training_loss": 7.434717178344727
    },
    {
      "epoch": 0.3768021680216802,
      "step": 1738,
      "training_loss": 7.2659783363342285
    },
    {
      "epoch": 0.3768021680216802,
      "step": 1738,
      "training_loss": 6.414393424987793
    },
    {
      "epoch": 0.3768021680216802,
      "step": 1738,
      "training_loss": 6.098838806152344
    },
    {
      "epoch": 0.3770189701897019,
      "step": 1739,
      "training_loss": 6.737016201019287
    },
    {
      "epoch": 0.3770189701897019,
      "step": 1739,
      "training_loss": 4.783403396606445
    },
    {
      "epoch": 0.3770189701897019,
      "step": 1739,
      "training_loss": 6.566399097442627
    },
    {
      "epoch": 0.3770189701897019,
      "step": 1739,
      "training_loss": 7.152409553527832
    },
    {
      "epoch": 0.3772357723577236,
      "grad_norm": 13.803152084350586,
      "learning_rate": 1e-05,
      "loss": 6.8021,
      "step": 1740
    },
    {
      "epoch": 0.3772357723577236,
      "step": 1740,
      "training_loss": 6.311511516571045
    },
    {
      "epoch": 0.3772357723577236,
      "step": 1740,
      "training_loss": 6.5859479904174805
    },
    {
      "epoch": 0.3772357723577236,
      "step": 1740,
      "training_loss": 6.664177894592285
    },
    {
      "epoch": 0.3772357723577236,
      "step": 1740,
      "training_loss": 7.087583065032959
    },
    {
      "epoch": 0.37745257452574527,
      "step": 1741,
      "training_loss": 6.3552069664001465
    },
    {
      "epoch": 0.37745257452574527,
      "step": 1741,
      "training_loss": 6.58021354675293
    },
    {
      "epoch": 0.37745257452574527,
      "step": 1741,
      "training_loss": 4.7682671546936035
    },
    {
      "epoch": 0.37745257452574527,
      "step": 1741,
      "training_loss": 6.227646827697754
    },
    {
      "epoch": 0.37766937669376693,
      "step": 1742,
      "training_loss": 7.976476669311523
    },
    {
      "epoch": 0.37766937669376693,
      "step": 1742,
      "training_loss": 6.535946369171143
    },
    {
      "epoch": 0.37766937669376693,
      "step": 1742,
      "training_loss": 5.692946434020996
    },
    {
      "epoch": 0.37766937669376693,
      "step": 1742,
      "training_loss": 6.590587139129639
    },
    {
      "epoch": 0.3778861788617886,
      "step": 1743,
      "training_loss": 6.805186748504639
    },
    {
      "epoch": 0.3778861788617886,
      "step": 1743,
      "training_loss": 6.7840728759765625
    },
    {
      "epoch": 0.3778861788617886,
      "step": 1743,
      "training_loss": 6.393389701843262
    },
    {
      "epoch": 0.3778861788617886,
      "step": 1743,
      "training_loss": 5.582275867462158
    },
    {
      "epoch": 0.3781029810298103,
      "grad_norm": 13.062435150146484,
      "learning_rate": 1e-05,
      "loss": 6.4338,
      "step": 1744
    },
    {
      "epoch": 0.3781029810298103,
      "step": 1744,
      "training_loss": 6.9510321617126465
    },
    {
      "epoch": 0.3781029810298103,
      "step": 1744,
      "training_loss": 6.382911682128906
    },
    {
      "epoch": 0.3781029810298103,
      "step": 1744,
      "training_loss": 6.5107645988464355
    },
    {
      "epoch": 0.3781029810298103,
      "step": 1744,
      "training_loss": 7.815828323364258
    },
    {
      "epoch": 0.378319783197832,
      "step": 1745,
      "training_loss": 6.174991607666016
    },
    {
      "epoch": 0.378319783197832,
      "step": 1745,
      "training_loss": 6.373593330383301
    },
    {
      "epoch": 0.378319783197832,
      "step": 1745,
      "training_loss": 5.95464563369751
    },
    {
      "epoch": 0.378319783197832,
      "step": 1745,
      "training_loss": 7.680259704589844
    },
    {
      "epoch": 0.37853658536585366,
      "step": 1746,
      "training_loss": 5.492003917694092
    },
    {
      "epoch": 0.37853658536585366,
      "step": 1746,
      "training_loss": 6.913475036621094
    },
    {
      "epoch": 0.37853658536585366,
      "step": 1746,
      "training_loss": 8.339681625366211
    },
    {
      "epoch": 0.37853658536585366,
      "step": 1746,
      "training_loss": 6.941518306732178
    },
    {
      "epoch": 0.37875338753387533,
      "step": 1747,
      "training_loss": 5.464134216308594
    },
    {
      "epoch": 0.37875338753387533,
      "step": 1747,
      "training_loss": 7.005738735198975
    },
    {
      "epoch": 0.37875338753387533,
      "step": 1747,
      "training_loss": 7.217505931854248
    },
    {
      "epoch": 0.37875338753387533,
      "step": 1747,
      "training_loss": 6.692370891571045
    },
    {
      "epoch": 0.378970189701897,
      "grad_norm": 12.25314998626709,
      "learning_rate": 1e-05,
      "loss": 6.7444,
      "step": 1748
    },
    {
      "epoch": 0.378970189701897,
      "step": 1748,
      "training_loss": 7.9393229484558105
    },
    {
      "epoch": 0.378970189701897,
      "step": 1748,
      "training_loss": 6.655029773712158
    },
    {
      "epoch": 0.378970189701897,
      "step": 1748,
      "training_loss": 6.8448405265808105
    },
    {
      "epoch": 0.378970189701897,
      "step": 1748,
      "training_loss": 6.729875087738037
    },
    {
      "epoch": 0.3791869918699187,
      "step": 1749,
      "training_loss": 7.441734313964844
    },
    {
      "epoch": 0.3791869918699187,
      "step": 1749,
      "training_loss": 6.009603023529053
    },
    {
      "epoch": 0.3791869918699187,
      "step": 1749,
      "training_loss": 7.785199165344238
    },
    {
      "epoch": 0.3791869918699187,
      "step": 1749,
      "training_loss": 6.434377193450928
    },
    {
      "epoch": 0.3794037940379404,
      "step": 1750,
      "training_loss": 7.164779186248779
    },
    {
      "epoch": 0.3794037940379404,
      "step": 1750,
      "training_loss": 5.212656021118164
    },
    {
      "epoch": 0.3794037940379404,
      "step": 1750,
      "training_loss": 5.975437164306641
    },
    {
      "epoch": 0.3794037940379404,
      "step": 1750,
      "training_loss": 7.4065961837768555
    },
    {
      "epoch": 0.37962059620596206,
      "step": 1751,
      "training_loss": 6.74610710144043
    },
    {
      "epoch": 0.37962059620596206,
      "step": 1751,
      "training_loss": 6.057751178741455
    },
    {
      "epoch": 0.37962059620596206,
      "step": 1751,
      "training_loss": 6.700127124786377
    },
    {
      "epoch": 0.37962059620596206,
      "step": 1751,
      "training_loss": 3.8862435817718506
    },
    {
      "epoch": 0.3798373983739837,
      "grad_norm": 15.136088371276855,
      "learning_rate": 1e-05,
      "loss": 6.5619,
      "step": 1752
    },
    {
      "epoch": 0.3798373983739837,
      "step": 1752,
      "training_loss": 7.037611484527588
    },
    {
      "epoch": 0.3798373983739837,
      "step": 1752,
      "training_loss": 6.338307857513428
    },
    {
      "epoch": 0.3798373983739837,
      "step": 1752,
      "training_loss": 7.320683002471924
    },
    {
      "epoch": 0.3798373983739837,
      "step": 1752,
      "training_loss": 6.890900611877441
    },
    {
      "epoch": 0.3800542005420054,
      "step": 1753,
      "training_loss": 7.282172203063965
    },
    {
      "epoch": 0.3800542005420054,
      "step": 1753,
      "training_loss": 8.993781089782715
    },
    {
      "epoch": 0.3800542005420054,
      "step": 1753,
      "training_loss": 7.311824321746826
    },
    {
      "epoch": 0.3800542005420054,
      "step": 1753,
      "training_loss": 4.250057697296143
    },
    {
      "epoch": 0.3802710027100271,
      "step": 1754,
      "training_loss": 7.051910400390625
    },
    {
      "epoch": 0.3802710027100271,
      "step": 1754,
      "training_loss": 8.000374794006348
    },
    {
      "epoch": 0.3802710027100271,
      "step": 1754,
      "training_loss": 6.7492828369140625
    },
    {
      "epoch": 0.3802710027100271,
      "step": 1754,
      "training_loss": 6.785904884338379
    },
    {
      "epoch": 0.3804878048780488,
      "step": 1755,
      "training_loss": 7.439687252044678
    },
    {
      "epoch": 0.3804878048780488,
      "step": 1755,
      "training_loss": 6.432446002960205
    },
    {
      "epoch": 0.3804878048780488,
      "step": 1755,
      "training_loss": 6.887086868286133
    },
    {
      "epoch": 0.3804878048780488,
      "step": 1755,
      "training_loss": 7.631419658660889
    },
    {
      "epoch": 0.38070460704607045,
      "grad_norm": 11.164848327636719,
      "learning_rate": 1e-05,
      "loss": 7.0252,
      "step": 1756
    },
    {
      "epoch": 0.38070460704607045,
      "step": 1756,
      "training_loss": 6.5658769607543945
    },
    {
      "epoch": 0.38070460704607045,
      "step": 1756,
      "training_loss": 5.749858856201172
    },
    {
      "epoch": 0.38070460704607045,
      "step": 1756,
      "training_loss": 6.597489356994629
    },
    {
      "epoch": 0.38070460704607045,
      "step": 1756,
      "training_loss": 6.605267524719238
    },
    {
      "epoch": 0.3809214092140921,
      "step": 1757,
      "training_loss": 5.896512985229492
    },
    {
      "epoch": 0.3809214092140921,
      "step": 1757,
      "training_loss": 7.013137340545654
    },
    {
      "epoch": 0.3809214092140921,
      "step": 1757,
      "training_loss": 5.88669490814209
    },
    {
      "epoch": 0.3809214092140921,
      "step": 1757,
      "training_loss": 5.763142108917236
    },
    {
      "epoch": 0.38113821138211385,
      "step": 1758,
      "training_loss": 4.521124362945557
    },
    {
      "epoch": 0.38113821138211385,
      "step": 1758,
      "training_loss": 7.0174665451049805
    },
    {
      "epoch": 0.38113821138211385,
      "step": 1758,
      "training_loss": 5.479283809661865
    },
    {
      "epoch": 0.38113821138211385,
      "step": 1758,
      "training_loss": 8.570219039916992
    },
    {
      "epoch": 0.3813550135501355,
      "step": 1759,
      "training_loss": 7.334090232849121
    },
    {
      "epoch": 0.3813550135501355,
      "step": 1759,
      "training_loss": 6.468979358673096
    },
    {
      "epoch": 0.3813550135501355,
      "step": 1759,
      "training_loss": 6.972683429718018
    },
    {
      "epoch": 0.3813550135501355,
      "step": 1759,
      "training_loss": 5.737932205200195
    },
    {
      "epoch": 0.3815718157181572,
      "grad_norm": 17.27590560913086,
      "learning_rate": 1e-05,
      "loss": 6.3862,
      "step": 1760
    },
    {
      "epoch": 0.3815718157181572,
      "step": 1760,
      "training_loss": 6.148475646972656
    },
    {
      "epoch": 0.3815718157181572,
      "step": 1760,
      "training_loss": 8.050204277038574
    },
    {
      "epoch": 0.3815718157181572,
      "step": 1760,
      "training_loss": 4.572479724884033
    },
    {
      "epoch": 0.3815718157181572,
      "step": 1760,
      "training_loss": 7.633092403411865
    },
    {
      "epoch": 0.38178861788617885,
      "step": 1761,
      "training_loss": 6.673309803009033
    },
    {
      "epoch": 0.38178861788617885,
      "step": 1761,
      "training_loss": 5.185641765594482
    },
    {
      "epoch": 0.38178861788617885,
      "step": 1761,
      "training_loss": 6.645052909851074
    },
    {
      "epoch": 0.38178861788617885,
      "step": 1761,
      "training_loss": 6.564115047454834
    },
    {
      "epoch": 0.3820054200542005,
      "step": 1762,
      "training_loss": 7.656989574432373
    },
    {
      "epoch": 0.3820054200542005,
      "step": 1762,
      "training_loss": 5.056845664978027
    },
    {
      "epoch": 0.3820054200542005,
      "step": 1762,
      "training_loss": 7.637176036834717
    },
    {
      "epoch": 0.3820054200542005,
      "step": 1762,
      "training_loss": 7.087973117828369
    },
    {
      "epoch": 0.38222222222222224,
      "step": 1763,
      "training_loss": 6.82175350189209
    },
    {
      "epoch": 0.38222222222222224,
      "step": 1763,
      "training_loss": 5.545058727264404
    },
    {
      "epoch": 0.38222222222222224,
      "step": 1763,
      "training_loss": 6.224174499511719
    },
    {
      "epoch": 0.38222222222222224,
      "step": 1763,
      "training_loss": 6.279879093170166
    },
    {
      "epoch": 0.3824390243902439,
      "grad_norm": 11.305389404296875,
      "learning_rate": 1e-05,
      "loss": 6.4864,
      "step": 1764
    },
    {
      "epoch": 0.3824390243902439,
      "step": 1764,
      "training_loss": 7.546764373779297
    },
    {
      "epoch": 0.3824390243902439,
      "step": 1764,
      "training_loss": 7.528502941131592
    },
    {
      "epoch": 0.3824390243902439,
      "step": 1764,
      "training_loss": 7.330273628234863
    },
    {
      "epoch": 0.3824390243902439,
      "step": 1764,
      "training_loss": 7.003708839416504
    },
    {
      "epoch": 0.3826558265582656,
      "step": 1765,
      "training_loss": 7.349177360534668
    },
    {
      "epoch": 0.3826558265582656,
      "step": 1765,
      "training_loss": 7.137682914733887
    },
    {
      "epoch": 0.3826558265582656,
      "step": 1765,
      "training_loss": 6.415215492248535
    },
    {
      "epoch": 0.3826558265582656,
      "step": 1765,
      "training_loss": 6.677982330322266
    },
    {
      "epoch": 0.38287262872628725,
      "step": 1766,
      "training_loss": 7.392834663391113
    },
    {
      "epoch": 0.38287262872628725,
      "step": 1766,
      "training_loss": 6.861910820007324
    },
    {
      "epoch": 0.38287262872628725,
      "step": 1766,
      "training_loss": 7.0934529304504395
    },
    {
      "epoch": 0.38287262872628725,
      "step": 1766,
      "training_loss": 4.1234869956970215
    },
    {
      "epoch": 0.38308943089430897,
      "step": 1767,
      "training_loss": 6.640324115753174
    },
    {
      "epoch": 0.38308943089430897,
      "step": 1767,
      "training_loss": 6.0058088302612305
    },
    {
      "epoch": 0.38308943089430897,
      "step": 1767,
      "training_loss": 7.416507720947266
    },
    {
      "epoch": 0.38308943089430897,
      "step": 1767,
      "training_loss": 5.825589656829834
    },
    {
      "epoch": 0.38330623306233064,
      "grad_norm": 14.602264404296875,
      "learning_rate": 1e-05,
      "loss": 6.7718,
      "step": 1768
    },
    {
      "epoch": 0.38330623306233064,
      "step": 1768,
      "training_loss": 6.6722564697265625
    },
    {
      "epoch": 0.38330623306233064,
      "step": 1768,
      "training_loss": 7.129804611206055
    },
    {
      "epoch": 0.38330623306233064,
      "step": 1768,
      "training_loss": 6.859873294830322
    },
    {
      "epoch": 0.38330623306233064,
      "step": 1768,
      "training_loss": 6.106348037719727
    },
    {
      "epoch": 0.3835230352303523,
      "step": 1769,
      "training_loss": 7.166662216186523
    },
    {
      "epoch": 0.3835230352303523,
      "step": 1769,
      "training_loss": 6.208059310913086
    },
    {
      "epoch": 0.3835230352303523,
      "step": 1769,
      "training_loss": 7.5078349113464355
    },
    {
      "epoch": 0.3835230352303523,
      "step": 1769,
      "training_loss": 5.926022052764893
    },
    {
      "epoch": 0.383739837398374,
      "step": 1770,
      "training_loss": 6.988088607788086
    },
    {
      "epoch": 0.383739837398374,
      "step": 1770,
      "training_loss": 5.318775653839111
    },
    {
      "epoch": 0.383739837398374,
      "step": 1770,
      "training_loss": 5.801455020904541
    },
    {
      "epoch": 0.383739837398374,
      "step": 1770,
      "training_loss": 5.368905544281006
    },
    {
      "epoch": 0.38395663956639564,
      "step": 1771,
      "training_loss": 7.775381565093994
    },
    {
      "epoch": 0.38395663956639564,
      "step": 1771,
      "training_loss": 8.478241920471191
    },
    {
      "epoch": 0.38395663956639564,
      "step": 1771,
      "training_loss": 5.236979007720947
    },
    {
      "epoch": 0.38395663956639564,
      "step": 1771,
      "training_loss": 6.652606964111328
    },
    {
      "epoch": 0.38417344173441736,
      "grad_norm": 11.11736011505127,
      "learning_rate": 1e-05,
      "loss": 6.5748,
      "step": 1772
    },
    {
      "epoch": 0.38417344173441736,
      "step": 1772,
      "training_loss": 5.6274261474609375
    },
    {
      "epoch": 0.38417344173441736,
      "step": 1772,
      "training_loss": 3.5081512928009033
    },
    {
      "epoch": 0.38417344173441736,
      "step": 1772,
      "training_loss": 6.5550055503845215
    },
    {
      "epoch": 0.38417344173441736,
      "step": 1772,
      "training_loss": 7.101219177246094
    },
    {
      "epoch": 0.38439024390243903,
      "step": 1773,
      "training_loss": 6.960559368133545
    },
    {
      "epoch": 0.38439024390243903,
      "step": 1773,
      "training_loss": 6.233962059020996
    },
    {
      "epoch": 0.38439024390243903,
      "step": 1773,
      "training_loss": 6.2935380935668945
    },
    {
      "epoch": 0.38439024390243903,
      "step": 1773,
      "training_loss": 6.677430629730225
    },
    {
      "epoch": 0.3846070460704607,
      "step": 1774,
      "training_loss": 7.099595546722412
    },
    {
      "epoch": 0.3846070460704607,
      "step": 1774,
      "training_loss": 2.660768747329712
    },
    {
      "epoch": 0.3846070460704607,
      "step": 1774,
      "training_loss": 5.42655611038208
    },
    {
      "epoch": 0.3846070460704607,
      "step": 1774,
      "training_loss": 4.183479309082031
    },
    {
      "epoch": 0.38482384823848237,
      "step": 1775,
      "training_loss": 5.189230918884277
    },
    {
      "epoch": 0.38482384823848237,
      "step": 1775,
      "training_loss": 9.11377239227295
    },
    {
      "epoch": 0.38482384823848237,
      "step": 1775,
      "training_loss": 7.676698207855225
    },
    {
      "epoch": 0.38482384823848237,
      "step": 1775,
      "training_loss": 6.602959156036377
    },
    {
      "epoch": 0.3850406504065041,
      "grad_norm": 14.71165943145752,
      "learning_rate": 1e-05,
      "loss": 6.0569,
      "step": 1776
    },
    {
      "epoch": 0.3850406504065041,
      "step": 1776,
      "training_loss": 7.847937107086182
    },
    {
      "epoch": 0.3850406504065041,
      "step": 1776,
      "training_loss": 6.018947601318359
    },
    {
      "epoch": 0.3850406504065041,
      "step": 1776,
      "training_loss": 7.024130344390869
    },
    {
      "epoch": 0.3850406504065041,
      "step": 1776,
      "training_loss": 6.490348815917969
    },
    {
      "epoch": 0.38525745257452576,
      "step": 1777,
      "training_loss": 5.961583614349365
    },
    {
      "epoch": 0.38525745257452576,
      "step": 1777,
      "training_loss": 6.203819274902344
    },
    {
      "epoch": 0.38525745257452576,
      "step": 1777,
      "training_loss": 6.197389602661133
    },
    {
      "epoch": 0.38525745257452576,
      "step": 1777,
      "training_loss": 7.588528633117676
    },
    {
      "epoch": 0.38547425474254743,
      "step": 1778,
      "training_loss": 7.200709819793701
    },
    {
      "epoch": 0.38547425474254743,
      "step": 1778,
      "training_loss": 5.197630882263184
    },
    {
      "epoch": 0.38547425474254743,
      "step": 1778,
      "training_loss": 6.7646403312683105
    },
    {
      "epoch": 0.38547425474254743,
      "step": 1778,
      "training_loss": 6.39954137802124
    },
    {
      "epoch": 0.3856910569105691,
      "step": 1779,
      "training_loss": 5.786717891693115
    },
    {
      "epoch": 0.3856910569105691,
      "step": 1779,
      "training_loss": 6.231077194213867
    },
    {
      "epoch": 0.3856910569105691,
      "step": 1779,
      "training_loss": 7.39390754699707
    },
    {
      "epoch": 0.3856910569105691,
      "step": 1779,
      "training_loss": 6.973910331726074
    },
    {
      "epoch": 0.38590785907859076,
      "grad_norm": 16.723987579345703,
      "learning_rate": 1e-05,
      "loss": 6.5801,
      "step": 1780
    },
    {
      "epoch": 0.38590785907859076,
      "step": 1780,
      "training_loss": 6.567380428314209
    },
    {
      "epoch": 0.38590785907859076,
      "step": 1780,
      "training_loss": 6.517270565032959
    },
    {
      "epoch": 0.38590785907859076,
      "step": 1780,
      "training_loss": 7.61931037902832
    },
    {
      "epoch": 0.38590785907859076,
      "step": 1780,
      "training_loss": 6.876010894775391
    },
    {
      "epoch": 0.3861246612466125,
      "step": 1781,
      "training_loss": 7.750917434692383
    },
    {
      "epoch": 0.3861246612466125,
      "step": 1781,
      "training_loss": 6.822659969329834
    },
    {
      "epoch": 0.3861246612466125,
      "step": 1781,
      "training_loss": 4.903820514678955
    },
    {
      "epoch": 0.3861246612466125,
      "step": 1781,
      "training_loss": 7.008294105529785
    },
    {
      "epoch": 0.38634146341463416,
      "step": 1782,
      "training_loss": 6.238530158996582
    },
    {
      "epoch": 0.38634146341463416,
      "step": 1782,
      "training_loss": 7.070303916931152
    },
    {
      "epoch": 0.38634146341463416,
      "step": 1782,
      "training_loss": 6.114336967468262
    },
    {
      "epoch": 0.38634146341463416,
      "step": 1782,
      "training_loss": 6.105322360992432
    },
    {
      "epoch": 0.3865582655826558,
      "step": 1783,
      "training_loss": 3.6869492530822754
    },
    {
      "epoch": 0.3865582655826558,
      "step": 1783,
      "training_loss": 6.355197906494141
    },
    {
      "epoch": 0.3865582655826558,
      "step": 1783,
      "training_loss": 8.262357711791992
    },
    {
      "epoch": 0.3865582655826558,
      "step": 1783,
      "training_loss": 6.379878044128418
    },
    {
      "epoch": 0.3867750677506775,
      "grad_norm": 16.37649154663086,
      "learning_rate": 1e-05,
      "loss": 6.5174,
      "step": 1784
    },
    {
      "epoch": 0.3867750677506775,
      "step": 1784,
      "training_loss": 5.265366077423096
    },
    {
      "epoch": 0.3867750677506775,
      "step": 1784,
      "training_loss": 4.494567394256592
    },
    {
      "epoch": 0.3867750677506775,
      "step": 1784,
      "training_loss": 5.529315948486328
    },
    {
      "epoch": 0.3867750677506775,
      "step": 1784,
      "training_loss": 7.216907501220703
    },
    {
      "epoch": 0.38699186991869916,
      "step": 1785,
      "training_loss": 5.684736251831055
    },
    {
      "epoch": 0.38699186991869916,
      "step": 1785,
      "training_loss": 5.986059188842773
    },
    {
      "epoch": 0.38699186991869916,
      "step": 1785,
      "training_loss": 5.590049743652344
    },
    {
      "epoch": 0.38699186991869916,
      "step": 1785,
      "training_loss": 5.756143569946289
    },
    {
      "epoch": 0.3872086720867209,
      "step": 1786,
      "training_loss": 4.922818183898926
    },
    {
      "epoch": 0.3872086720867209,
      "step": 1786,
      "training_loss": 6.3343706130981445
    },
    {
      "epoch": 0.3872086720867209,
      "step": 1786,
      "training_loss": 6.946874618530273
    },
    {
      "epoch": 0.3872086720867209,
      "step": 1786,
      "training_loss": 8.474852561950684
    },
    {
      "epoch": 0.38742547425474255,
      "step": 1787,
      "training_loss": 7.7261810302734375
    },
    {
      "epoch": 0.38742547425474255,
      "step": 1787,
      "training_loss": 6.074691295623779
    },
    {
      "epoch": 0.38742547425474255,
      "step": 1787,
      "training_loss": 6.874046802520752
    },
    {
      "epoch": 0.38742547425474255,
      "step": 1787,
      "training_loss": 7.04022741317749
    },
    {
      "epoch": 0.3876422764227642,
      "grad_norm": 20.521392822265625,
      "learning_rate": 1e-05,
      "loss": 6.2448,
      "step": 1788
    },
    {
      "epoch": 0.3876422764227642,
      "step": 1788,
      "training_loss": 7.092209339141846
    },
    {
      "epoch": 0.3876422764227642,
      "step": 1788,
      "training_loss": 6.605749607086182
    },
    {
      "epoch": 0.3876422764227642,
      "step": 1788,
      "training_loss": 6.6614203453063965
    },
    {
      "epoch": 0.3876422764227642,
      "step": 1788,
      "training_loss": 6.469749927520752
    },
    {
      "epoch": 0.3878590785907859,
      "step": 1789,
      "training_loss": 7.375415802001953
    },
    {
      "epoch": 0.3878590785907859,
      "step": 1789,
      "training_loss": 4.091580867767334
    },
    {
      "epoch": 0.3878590785907859,
      "step": 1789,
      "training_loss": 6.872415065765381
    },
    {
      "epoch": 0.3878590785907859,
      "step": 1789,
      "training_loss": 6.419606685638428
    },
    {
      "epoch": 0.3880758807588076,
      "step": 1790,
      "training_loss": 6.813599109649658
    },
    {
      "epoch": 0.3880758807588076,
      "step": 1790,
      "training_loss": 6.266330718994141
    },
    {
      "epoch": 0.3880758807588076,
      "step": 1790,
      "training_loss": 6.893244743347168
    },
    {
      "epoch": 0.3880758807588076,
      "step": 1790,
      "training_loss": 5.887783527374268
    },
    {
      "epoch": 0.3882926829268293,
      "step": 1791,
      "training_loss": 7.663549423217773
    },
    {
      "epoch": 0.3882926829268293,
      "step": 1791,
      "training_loss": 5.823553085327148
    },
    {
      "epoch": 0.3882926829268293,
      "step": 1791,
      "training_loss": 7.864938259124756
    },
    {
      "epoch": 0.3882926829268293,
      "step": 1791,
      "training_loss": 6.171109676361084
    },
    {
      "epoch": 0.38850948509485095,
      "grad_norm": 11.751055717468262,
      "learning_rate": 1e-05,
      "loss": 6.5608,
      "step": 1792
    },
    {
      "epoch": 0.38850948509485095,
      "step": 1792,
      "training_loss": 7.507495880126953
    },
    {
      "epoch": 0.38850948509485095,
      "step": 1792,
      "training_loss": 6.130850791931152
    },
    {
      "epoch": 0.38850948509485095,
      "step": 1792,
      "training_loss": 8.488225936889648
    },
    {
      "epoch": 0.38850948509485095,
      "step": 1792,
      "training_loss": 7.4557108879089355
    },
    {
      "epoch": 0.3887262872628726,
      "step": 1793,
      "training_loss": 6.920445442199707
    },
    {
      "epoch": 0.3887262872628726,
      "step": 1793,
      "training_loss": 7.2603759765625
    },
    {
      "epoch": 0.3887262872628726,
      "step": 1793,
      "training_loss": 7.290107727050781
    },
    {
      "epoch": 0.3887262872628726,
      "step": 1793,
      "training_loss": 6.2911553382873535
    },
    {
      "epoch": 0.3889430894308943,
      "step": 1794,
      "training_loss": 5.774352550506592
    },
    {
      "epoch": 0.3889430894308943,
      "step": 1794,
      "training_loss": 6.822420597076416
    },
    {
      "epoch": 0.3889430894308943,
      "step": 1794,
      "training_loss": 7.854981422424316
    },
    {
      "epoch": 0.3889430894308943,
      "step": 1794,
      "training_loss": 6.688186168670654
    },
    {
      "epoch": 0.389159891598916,
      "step": 1795,
      "training_loss": 5.435826778411865
    },
    {
      "epoch": 0.389159891598916,
      "step": 1795,
      "training_loss": 6.6586456298828125
    },
    {
      "epoch": 0.389159891598916,
      "step": 1795,
      "training_loss": 7.297823905944824
    },
    {
      "epoch": 0.389159891598916,
      "step": 1795,
      "training_loss": 7.247853755950928
    },
    {
      "epoch": 0.3893766937669377,
      "grad_norm": 13.199332237243652,
      "learning_rate": 1e-05,
      "loss": 6.9453,
      "step": 1796
    },
    {
      "epoch": 0.3893766937669377,
      "step": 1796,
      "training_loss": 7.558833599090576
    },
    {
      "epoch": 0.3893766937669377,
      "step": 1796,
      "training_loss": 7.195366859436035
    },
    {
      "epoch": 0.3893766937669377,
      "step": 1796,
      "training_loss": 6.385056018829346
    },
    {
      "epoch": 0.3893766937669377,
      "step": 1796,
      "training_loss": 6.004980564117432
    },
    {
      "epoch": 0.38959349593495934,
      "step": 1797,
      "training_loss": 7.661192417144775
    },
    {
      "epoch": 0.38959349593495934,
      "step": 1797,
      "training_loss": 5.9799370765686035
    },
    {
      "epoch": 0.38959349593495934,
      "step": 1797,
      "training_loss": 5.828657627105713
    },
    {
      "epoch": 0.38959349593495934,
      "step": 1797,
      "training_loss": 5.469747066497803
    },
    {
      "epoch": 0.389810298102981,
      "step": 1798,
      "training_loss": 6.65309476852417
    },
    {
      "epoch": 0.389810298102981,
      "step": 1798,
      "training_loss": 6.502811908721924
    },
    {
      "epoch": 0.389810298102981,
      "step": 1798,
      "training_loss": 7.4949727058410645
    },
    {
      "epoch": 0.389810298102981,
      "step": 1798,
      "training_loss": 6.540848255157471
    },
    {
      "epoch": 0.39002710027100274,
      "step": 1799,
      "training_loss": 6.709367752075195
    },
    {
      "epoch": 0.39002710027100274,
      "step": 1799,
      "training_loss": 6.890290260314941
    },
    {
      "epoch": 0.39002710027100274,
      "step": 1799,
      "training_loss": 5.150182247161865
    },
    {
      "epoch": 0.39002710027100274,
      "step": 1799,
      "training_loss": 5.318838119506836
    },
    {
      "epoch": 0.3902439024390244,
      "grad_norm": 12.858855247497559,
      "learning_rate": 1e-05,
      "loss": 6.459,
      "step": 1800
    },
    {
      "epoch": 0.3902439024390244,
      "step": 1800,
      "training_loss": 7.239455699920654
    },
    {
      "epoch": 0.3902439024390244,
      "step": 1800,
      "training_loss": 7.917703151702881
    },
    {
      "epoch": 0.3902439024390244,
      "step": 1800,
      "training_loss": 7.012182712554932
    },
    {
      "epoch": 0.3902439024390244,
      "step": 1800,
      "training_loss": 6.754292011260986
    },
    {
      "epoch": 0.39046070460704607,
      "step": 1801,
      "training_loss": 7.353269577026367
    },
    {
      "epoch": 0.39046070460704607,
      "step": 1801,
      "training_loss": 4.7667670249938965
    },
    {
      "epoch": 0.39046070460704607,
      "step": 1801,
      "training_loss": 5.307288646697998
    },
    {
      "epoch": 0.39046070460704607,
      "step": 1801,
      "training_loss": 7.835556507110596
    },
    {
      "epoch": 0.39067750677506774,
      "step": 1802,
      "training_loss": 6.529562950134277
    },
    {
      "epoch": 0.39067750677506774,
      "step": 1802,
      "training_loss": 5.8632073402404785
    },
    {
      "epoch": 0.39067750677506774,
      "step": 1802,
      "training_loss": 7.8527398109436035
    },
    {
      "epoch": 0.39067750677506774,
      "step": 1802,
      "training_loss": 7.077398777008057
    },
    {
      "epoch": 0.3908943089430894,
      "step": 1803,
      "training_loss": 7.428258895874023
    },
    {
      "epoch": 0.3908943089430894,
      "step": 1803,
      "training_loss": 5.672504901885986
    },
    {
      "epoch": 0.3908943089430894,
      "step": 1803,
      "training_loss": 7.589352130889893
    },
    {
      "epoch": 0.3908943089430894,
      "step": 1803,
      "training_loss": 6.57212495803833
    },
    {
      "epoch": 0.39111111111111113,
      "grad_norm": 13.18481159210205,
      "learning_rate": 1e-05,
      "loss": 6.7982,
      "step": 1804
    },
    {
      "epoch": 0.39111111111111113,
      "step": 1804,
      "training_loss": 7.018025875091553
    },
    {
      "epoch": 0.39111111111111113,
      "step": 1804,
      "training_loss": 6.514638900756836
    },
    {
      "epoch": 0.39111111111111113,
      "step": 1804,
      "training_loss": 7.128602504730225
    },
    {
      "epoch": 0.39111111111111113,
      "step": 1804,
      "training_loss": 8.354936599731445
    },
    {
      "epoch": 0.3913279132791328,
      "step": 1805,
      "training_loss": 5.030317783355713
    },
    {
      "epoch": 0.3913279132791328,
      "step": 1805,
      "training_loss": 6.609666347503662
    },
    {
      "epoch": 0.3913279132791328,
      "step": 1805,
      "training_loss": 6.363211154937744
    },
    {
      "epoch": 0.3913279132791328,
      "step": 1805,
      "training_loss": 6.494276523590088
    },
    {
      "epoch": 0.39154471544715447,
      "step": 1806,
      "training_loss": 7.58437442779541
    },
    {
      "epoch": 0.39154471544715447,
      "step": 1806,
      "training_loss": 5.6416778564453125
    },
    {
      "epoch": 0.39154471544715447,
      "step": 1806,
      "training_loss": 5.194583415985107
    },
    {
      "epoch": 0.39154471544715447,
      "step": 1806,
      "training_loss": 7.757874488830566
    },
    {
      "epoch": 0.39176151761517614,
      "step": 1807,
      "training_loss": 6.453566074371338
    },
    {
      "epoch": 0.39176151761517614,
      "step": 1807,
      "training_loss": 6.971127033233643
    },
    {
      "epoch": 0.39176151761517614,
      "step": 1807,
      "training_loss": 5.581414222717285
    },
    {
      "epoch": 0.39176151761517614,
      "step": 1807,
      "training_loss": 5.028590679168701
    },
    {
      "epoch": 0.39197831978319786,
      "grad_norm": 14.261855125427246,
      "learning_rate": 1e-05,
      "loss": 6.4829,
      "step": 1808
    },
    {
      "epoch": 0.39197831978319786,
      "step": 1808,
      "training_loss": 6.782247066497803
    },
    {
      "epoch": 0.39197831978319786,
      "step": 1808,
      "training_loss": 7.310409069061279
    },
    {
      "epoch": 0.39197831978319786,
      "step": 1808,
      "training_loss": 5.989667892456055
    },
    {
      "epoch": 0.39197831978319786,
      "step": 1808,
      "training_loss": 8.754888534545898
    },
    {
      "epoch": 0.3921951219512195,
      "step": 1809,
      "training_loss": 6.310330867767334
    },
    {
      "epoch": 0.3921951219512195,
      "step": 1809,
      "training_loss": 5.0917816162109375
    },
    {
      "epoch": 0.3921951219512195,
      "step": 1809,
      "training_loss": 6.484167098999023
    },
    {
      "epoch": 0.3921951219512195,
      "step": 1809,
      "training_loss": 6.524777889251709
    },
    {
      "epoch": 0.3924119241192412,
      "step": 1810,
      "training_loss": 6.269641876220703
    },
    {
      "epoch": 0.3924119241192412,
      "step": 1810,
      "training_loss": 6.878357410430908
    },
    {
      "epoch": 0.3924119241192412,
      "step": 1810,
      "training_loss": 6.721930503845215
    },
    {
      "epoch": 0.3924119241192412,
      "step": 1810,
      "training_loss": 7.091719150543213
    },
    {
      "epoch": 0.39262872628726286,
      "step": 1811,
      "training_loss": 6.93998908996582
    },
    {
      "epoch": 0.39262872628726286,
      "step": 1811,
      "training_loss": 7.297280311584473
    },
    {
      "epoch": 0.39262872628726286,
      "step": 1811,
      "training_loss": 5.975552558898926
    },
    {
      "epoch": 0.39262872628726286,
      "step": 1811,
      "training_loss": 7.09022855758667
    },
    {
      "epoch": 0.39284552845528453,
      "grad_norm": 13.327008247375488,
      "learning_rate": 1e-05,
      "loss": 6.7196,
      "step": 1812
    },
    {
      "epoch": 0.39284552845528453,
      "step": 1812,
      "training_loss": 3.64066743850708
    },
    {
      "epoch": 0.39284552845528453,
      "step": 1812,
      "training_loss": 6.756862640380859
    },
    {
      "epoch": 0.39284552845528453,
      "step": 1812,
      "training_loss": 4.426907062530518
    },
    {
      "epoch": 0.39284552845528453,
      "step": 1812,
      "training_loss": 6.593325614929199
    },
    {
      "epoch": 0.39306233062330626,
      "step": 1813,
      "training_loss": 6.589440822601318
    },
    {
      "epoch": 0.39306233062330626,
      "step": 1813,
      "training_loss": 6.944395542144775
    },
    {
      "epoch": 0.39306233062330626,
      "step": 1813,
      "training_loss": 7.047418117523193
    },
    {
      "epoch": 0.39306233062330626,
      "step": 1813,
      "training_loss": 5.955801010131836
    },
    {
      "epoch": 0.3932791327913279,
      "step": 1814,
      "training_loss": 6.9422807693481445
    },
    {
      "epoch": 0.3932791327913279,
      "step": 1814,
      "training_loss": 6.59575891494751
    },
    {
      "epoch": 0.3932791327913279,
      "step": 1814,
      "training_loss": 7.383248805999756
    },
    {
      "epoch": 0.3932791327913279,
      "step": 1814,
      "training_loss": 8.048722267150879
    },
    {
      "epoch": 0.3934959349593496,
      "step": 1815,
      "training_loss": 6.778234958648682
    },
    {
      "epoch": 0.3934959349593496,
      "step": 1815,
      "training_loss": 7.651379585266113
    },
    {
      "epoch": 0.3934959349593496,
      "step": 1815,
      "training_loss": 7.291290283203125
    },
    {
      "epoch": 0.3934959349593496,
      "step": 1815,
      "training_loss": 6.978601455688477
    },
    {
      "epoch": 0.39371273712737126,
      "grad_norm": 14.93437671661377,
      "learning_rate": 1e-05,
      "loss": 6.6015,
      "step": 1816
    },
    {
      "epoch": 0.39371273712737126,
      "step": 1816,
      "training_loss": 7.371711730957031
    },
    {
      "epoch": 0.39371273712737126,
      "step": 1816,
      "training_loss": 4.598648548126221
    },
    {
      "epoch": 0.39371273712737126,
      "step": 1816,
      "training_loss": 5.396660327911377
    },
    {
      "epoch": 0.39371273712737126,
      "step": 1816,
      "training_loss": 7.030468463897705
    },
    {
      "epoch": 0.3939295392953929,
      "step": 1817,
      "training_loss": 7.2017107009887695
    },
    {
      "epoch": 0.3939295392953929,
      "step": 1817,
      "training_loss": 6.471428871154785
    },
    {
      "epoch": 0.3939295392953929,
      "step": 1817,
      "training_loss": 6.290039539337158
    },
    {
      "epoch": 0.3939295392953929,
      "step": 1817,
      "training_loss": 7.95171594619751
    },
    {
      "epoch": 0.39414634146341465,
      "step": 1818,
      "training_loss": 5.326849460601807
    },
    {
      "epoch": 0.39414634146341465,
      "step": 1818,
      "training_loss": 7.194995880126953
    },
    {
      "epoch": 0.39414634146341465,
      "step": 1818,
      "training_loss": 5.40168571472168
    },
    {
      "epoch": 0.39414634146341465,
      "step": 1818,
      "training_loss": 6.689990043640137
    },
    {
      "epoch": 0.3943631436314363,
      "step": 1819,
      "training_loss": 5.502078533172607
    },
    {
      "epoch": 0.3943631436314363,
      "step": 1819,
      "training_loss": 5.561434745788574
    },
    {
      "epoch": 0.3943631436314363,
      "step": 1819,
      "training_loss": 3.3739309310913086
    },
    {
      "epoch": 0.3943631436314363,
      "step": 1819,
      "training_loss": 5.702375888824463
    },
    {
      "epoch": 0.394579945799458,
      "grad_norm": 25.127975463867188,
      "learning_rate": 1e-05,
      "loss": 6.0666,
      "step": 1820
    },
    {
      "epoch": 0.394579945799458,
      "step": 1820,
      "training_loss": 6.638058185577393
    },
    {
      "epoch": 0.394579945799458,
      "step": 1820,
      "training_loss": 6.460450172424316
    },
    {
      "epoch": 0.394579945799458,
      "step": 1820,
      "training_loss": 7.534379959106445
    },
    {
      "epoch": 0.394579945799458,
      "step": 1820,
      "training_loss": 4.850107192993164
    },
    {
      "epoch": 0.39479674796747966,
      "step": 1821,
      "training_loss": 5.766262531280518
    },
    {
      "epoch": 0.39479674796747966,
      "step": 1821,
      "training_loss": 5.39863395690918
    },
    {
      "epoch": 0.39479674796747966,
      "step": 1821,
      "training_loss": 7.063547134399414
    },
    {
      "epoch": 0.39479674796747966,
      "step": 1821,
      "training_loss": 8.373688697814941
    },
    {
      "epoch": 0.3950135501355014,
      "step": 1822,
      "training_loss": 7.1577372550964355
    },
    {
      "epoch": 0.3950135501355014,
      "step": 1822,
      "training_loss": 7.018717288970947
    },
    {
      "epoch": 0.3950135501355014,
      "step": 1822,
      "training_loss": 6.6324262619018555
    },
    {
      "epoch": 0.3950135501355014,
      "step": 1822,
      "training_loss": 6.163194179534912
    },
    {
      "epoch": 0.39523035230352305,
      "step": 1823,
      "training_loss": 6.251163482666016
    },
    {
      "epoch": 0.39523035230352305,
      "step": 1823,
      "training_loss": 7.223939895629883
    },
    {
      "epoch": 0.39523035230352305,
      "step": 1823,
      "training_loss": 5.38124418258667
    },
    {
      "epoch": 0.39523035230352305,
      "step": 1823,
      "training_loss": 8.122901916503906
    },
    {
      "epoch": 0.3954471544715447,
      "grad_norm": 15.726658821105957,
      "learning_rate": 1e-05,
      "loss": 6.6273,
      "step": 1824
    },
    {
      "epoch": 0.3954471544715447,
      "step": 1824,
      "training_loss": 6.901730060577393
    },
    {
      "epoch": 0.3954471544715447,
      "step": 1824,
      "training_loss": 4.584878921508789
    },
    {
      "epoch": 0.3954471544715447,
      "step": 1824,
      "training_loss": 6.910524368286133
    },
    {
      "epoch": 0.3954471544715447,
      "step": 1824,
      "training_loss": 7.071211814880371
    },
    {
      "epoch": 0.3956639566395664,
      "step": 1825,
      "training_loss": 6.284287452697754
    },
    {
      "epoch": 0.3956639566395664,
      "step": 1825,
      "training_loss": 7.462472438812256
    },
    {
      "epoch": 0.3956639566395664,
      "step": 1825,
      "training_loss": 6.743473529815674
    },
    {
      "epoch": 0.3956639566395664,
      "step": 1825,
      "training_loss": 6.358077526092529
    },
    {
      "epoch": 0.39588075880758805,
      "step": 1826,
      "training_loss": 5.394658088684082
    },
    {
      "epoch": 0.39588075880758805,
      "step": 1826,
      "training_loss": 7.5623979568481445
    },
    {
      "epoch": 0.39588075880758805,
      "step": 1826,
      "training_loss": 6.8381452560424805
    },
    {
      "epoch": 0.39588075880758805,
      "step": 1826,
      "training_loss": 6.664172172546387
    },
    {
      "epoch": 0.3960975609756098,
      "step": 1827,
      "training_loss": 6.472211837768555
    },
    {
      "epoch": 0.3960975609756098,
      "step": 1827,
      "training_loss": 5.895226001739502
    },
    {
      "epoch": 0.3960975609756098,
      "step": 1827,
      "training_loss": 6.081202030181885
    },
    {
      "epoch": 0.3960975609756098,
      "step": 1827,
      "training_loss": 7.6257123947143555
    },
    {
      "epoch": 0.39631436314363144,
      "grad_norm": 11.953033447265625,
      "learning_rate": 1e-05,
      "loss": 6.5531,
      "step": 1828
    },
    {
      "epoch": 0.39631436314363144,
      "step": 1828,
      "training_loss": 7.467174530029297
    },
    {
      "epoch": 0.39631436314363144,
      "step": 1828,
      "training_loss": 6.607654571533203
    },
    {
      "epoch": 0.39631436314363144,
      "step": 1828,
      "training_loss": 7.20292329788208
    },
    {
      "epoch": 0.39631436314363144,
      "step": 1828,
      "training_loss": 7.618633270263672
    },
    {
      "epoch": 0.3965311653116531,
      "step": 1829,
      "training_loss": 4.675410270690918
    },
    {
      "epoch": 0.3965311653116531,
      "step": 1829,
      "training_loss": 6.063150405883789
    },
    {
      "epoch": 0.3965311653116531,
      "step": 1829,
      "training_loss": 5.391350746154785
    },
    {
      "epoch": 0.3965311653116531,
      "step": 1829,
      "training_loss": 7.119162559509277
    },
    {
      "epoch": 0.3967479674796748,
      "step": 1830,
      "training_loss": 5.0405120849609375
    },
    {
      "epoch": 0.3967479674796748,
      "step": 1830,
      "training_loss": 7.36552095413208
    },
    {
      "epoch": 0.3967479674796748,
      "step": 1830,
      "training_loss": 4.596900939941406
    },
    {
      "epoch": 0.3967479674796748,
      "step": 1830,
      "training_loss": 9.01729679107666
    },
    {
      "epoch": 0.3969647696476965,
      "step": 1831,
      "training_loss": 6.951169967651367
    },
    {
      "epoch": 0.3969647696476965,
      "step": 1831,
      "training_loss": 7.507321357727051
    },
    {
      "epoch": 0.3969647696476965,
      "step": 1831,
      "training_loss": 6.540144920349121
    },
    {
      "epoch": 0.3969647696476965,
      "step": 1831,
      "training_loss": 7.324876308441162
    },
    {
      "epoch": 0.39718157181571817,
      "grad_norm": 17.096481323242188,
      "learning_rate": 1e-05,
      "loss": 6.6556,
      "step": 1832
    },
    {
      "epoch": 0.39718157181571817,
      "step": 1832,
      "training_loss": 7.297571659088135
    },
    {
      "epoch": 0.39718157181571817,
      "step": 1832,
      "training_loss": 6.769868850708008
    },
    {
      "epoch": 0.39718157181571817,
      "step": 1832,
      "training_loss": 7.285360336303711
    },
    {
      "epoch": 0.39718157181571817,
      "step": 1832,
      "training_loss": 5.826105117797852
    },
    {
      "epoch": 0.39739837398373984,
      "step": 1833,
      "training_loss": 6.714789390563965
    },
    {
      "epoch": 0.39739837398373984,
      "step": 1833,
      "training_loss": 6.9469313621521
    },
    {
      "epoch": 0.39739837398373984,
      "step": 1833,
      "training_loss": 7.183802127838135
    },
    {
      "epoch": 0.39739837398373984,
      "step": 1833,
      "training_loss": 6.60377836227417
    },
    {
      "epoch": 0.3976151761517615,
      "step": 1834,
      "training_loss": 7.201557159423828
    },
    {
      "epoch": 0.3976151761517615,
      "step": 1834,
      "training_loss": 6.935940742492676
    },
    {
      "epoch": 0.3976151761517615,
      "step": 1834,
      "training_loss": 5.5462965965271
    },
    {
      "epoch": 0.3976151761517615,
      "step": 1834,
      "training_loss": 4.998410701751709
    },
    {
      "epoch": 0.3978319783197832,
      "step": 1835,
      "training_loss": 6.262472152709961
    },
    {
      "epoch": 0.3978319783197832,
      "step": 1835,
      "training_loss": 7.597689151763916
    },
    {
      "epoch": 0.3978319783197832,
      "step": 1835,
      "training_loss": 4.313800811767578
    },
    {
      "epoch": 0.3978319783197832,
      "step": 1835,
      "training_loss": 6.68502950668335
    },
    {
      "epoch": 0.3980487804878049,
      "grad_norm": 13.77673053741455,
      "learning_rate": 1e-05,
      "loss": 6.5106,
      "step": 1836
    },
    {
      "epoch": 0.3980487804878049,
      "step": 1836,
      "training_loss": 3.3323731422424316
    },
    {
      "epoch": 0.3980487804878049,
      "step": 1836,
      "training_loss": 6.753296852111816
    },
    {
      "epoch": 0.3980487804878049,
      "step": 1836,
      "training_loss": 6.2193450927734375
    },
    {
      "epoch": 0.3980487804878049,
      "step": 1836,
      "training_loss": 7.160686016082764
    },
    {
      "epoch": 0.39826558265582657,
      "step": 1837,
      "training_loss": 5.3751349449157715
    },
    {
      "epoch": 0.39826558265582657,
      "step": 1837,
      "training_loss": 7.878276348114014
    },
    {
      "epoch": 0.39826558265582657,
      "step": 1837,
      "training_loss": 7.368752479553223
    },
    {
      "epoch": 0.39826558265582657,
      "step": 1837,
      "training_loss": 7.219639301300049
    },
    {
      "epoch": 0.39848238482384823,
      "step": 1838,
      "training_loss": 7.170558929443359
    },
    {
      "epoch": 0.39848238482384823,
      "step": 1838,
      "training_loss": 7.3620924949646
    },
    {
      "epoch": 0.39848238482384823,
      "step": 1838,
      "training_loss": 5.2764201164245605
    },
    {
      "epoch": 0.39848238482384823,
      "step": 1838,
      "training_loss": 5.938638687133789
    },
    {
      "epoch": 0.3986991869918699,
      "step": 1839,
      "training_loss": 6.237970352172852
    },
    {
      "epoch": 0.3986991869918699,
      "step": 1839,
      "training_loss": 7.565756320953369
    },
    {
      "epoch": 0.3986991869918699,
      "step": 1839,
      "training_loss": 6.819091796875
    },
    {
      "epoch": 0.3986991869918699,
      "step": 1839,
      "training_loss": 6.838465213775635
    },
    {
      "epoch": 0.3989159891598916,
      "grad_norm": 12.34493637084961,
      "learning_rate": 1e-05,
      "loss": 6.5323,
      "step": 1840
    },
    {
      "epoch": 0.3989159891598916,
      "step": 1840,
      "training_loss": 6.045246601104736
    },
    {
      "epoch": 0.3989159891598916,
      "step": 1840,
      "training_loss": 6.686836242675781
    },
    {
      "epoch": 0.3989159891598916,
      "step": 1840,
      "training_loss": 5.391768932342529
    },
    {
      "epoch": 0.3989159891598916,
      "step": 1840,
      "training_loss": 7.331491947174072
    },
    {
      "epoch": 0.3991327913279133,
      "step": 1841,
      "training_loss": 5.698660850524902
    },
    {
      "epoch": 0.3991327913279133,
      "step": 1841,
      "training_loss": 5.669996738433838
    },
    {
      "epoch": 0.3991327913279133,
      "step": 1841,
      "training_loss": 6.306488990783691
    },
    {
      "epoch": 0.3991327913279133,
      "step": 1841,
      "training_loss": 5.654125213623047
    },
    {
      "epoch": 0.39934959349593496,
      "step": 1842,
      "training_loss": 5.6138410568237305
    },
    {
      "epoch": 0.39934959349593496,
      "step": 1842,
      "training_loss": 6.195460796356201
    },
    {
      "epoch": 0.39934959349593496,
      "step": 1842,
      "training_loss": 5.8447394371032715
    },
    {
      "epoch": 0.39934959349593496,
      "step": 1842,
      "training_loss": 6.488795280456543
    },
    {
      "epoch": 0.39956639566395663,
      "step": 1843,
      "training_loss": 7.364006996154785
    },
    {
      "epoch": 0.39956639566395663,
      "step": 1843,
      "training_loss": 5.193319320678711
    },
    {
      "epoch": 0.39956639566395663,
      "step": 1843,
      "training_loss": 6.341142654418945
    },
    {
      "epoch": 0.39956639566395663,
      "step": 1843,
      "training_loss": 6.183879852294922
    },
    {
      "epoch": 0.3997831978319783,
      "grad_norm": 16.812732696533203,
      "learning_rate": 1e-05,
      "loss": 6.1256,
      "step": 1844
    },
    {
      "epoch": 0.3997831978319783,
      "step": 1844,
      "training_loss": 7.154664516448975
    },
    {
      "epoch": 0.3997831978319783,
      "step": 1844,
      "training_loss": 6.5580153465271
    },
    {
      "epoch": 0.3997831978319783,
      "step": 1844,
      "training_loss": 3.47829270362854
    },
    {
      "epoch": 0.3997831978319783,
      "step": 1844,
      "training_loss": 7.393864631652832
    },
    {
      "epoch": 0.4,
      "step": 1845,
      "training_loss": 5.922027587890625
    },
    {
      "epoch": 0.4,
      "step": 1845,
      "training_loss": 8.613734245300293
    },
    {
      "epoch": 0.4,
      "step": 1845,
      "training_loss": 7.599584102630615
    },
    {
      "epoch": 0.4,
      "step": 1845,
      "training_loss": 6.467301845550537
    },
    {
      "epoch": 0.4002168021680217,
      "step": 1846,
      "training_loss": 6.244213581085205
    },
    {
      "epoch": 0.4002168021680217,
      "step": 1846,
      "training_loss": 5.931941032409668
    },
    {
      "epoch": 0.4002168021680217,
      "step": 1846,
      "training_loss": 7.676751613616943
    },
    {
      "epoch": 0.4002168021680217,
      "step": 1846,
      "training_loss": 6.5366106033325195
    },
    {
      "epoch": 0.40043360433604336,
      "step": 1847,
      "training_loss": 6.185329914093018
    },
    {
      "epoch": 0.40043360433604336,
      "step": 1847,
      "training_loss": 4.221979141235352
    },
    {
      "epoch": 0.40043360433604336,
      "step": 1847,
      "training_loss": 6.658681392669678
    },
    {
      "epoch": 0.40043360433604336,
      "step": 1847,
      "training_loss": 6.248517036437988
    },
    {
      "epoch": 0.400650406504065,
      "grad_norm": 15.378260612487793,
      "learning_rate": 1e-05,
      "loss": 6.4307,
      "step": 1848
    },
    {
      "epoch": 0.400650406504065,
      "step": 1848,
      "training_loss": 3.813234567642212
    },
    {
      "epoch": 0.400650406504065,
      "step": 1848,
      "training_loss": 4.4440131187438965
    },
    {
      "epoch": 0.400650406504065,
      "step": 1848,
      "training_loss": 6.846251010894775
    },
    {
      "epoch": 0.400650406504065,
      "step": 1848,
      "training_loss": 7.11522912979126
    },
    {
      "epoch": 0.4008672086720867,
      "step": 1849,
      "training_loss": 7.5301432609558105
    },
    {
      "epoch": 0.4008672086720867,
      "step": 1849,
      "training_loss": 7.186920642852783
    },
    {
      "epoch": 0.4008672086720867,
      "step": 1849,
      "training_loss": 5.239124774932861
    },
    {
      "epoch": 0.4008672086720867,
      "step": 1849,
      "training_loss": 6.900386810302734
    },
    {
      "epoch": 0.4010840108401084,
      "step": 1850,
      "training_loss": 3.5744166374206543
    },
    {
      "epoch": 0.4010840108401084,
      "step": 1850,
      "training_loss": 4.8971381187438965
    },
    {
      "epoch": 0.4010840108401084,
      "step": 1850,
      "training_loss": 7.257935047149658
    },
    {
      "epoch": 0.4010840108401084,
      "step": 1850,
      "training_loss": 6.103186130523682
    },
    {
      "epoch": 0.4013008130081301,
      "step": 1851,
      "training_loss": 7.569950103759766
    },
    {
      "epoch": 0.4013008130081301,
      "step": 1851,
      "training_loss": 7.135965824127197
    },
    {
      "epoch": 0.4013008130081301,
      "step": 1851,
      "training_loss": 7.203205585479736
    },
    {
      "epoch": 0.4013008130081301,
      "step": 1851,
      "training_loss": 6.87995719909668
    },
    {
      "epoch": 0.40151761517615175,
      "grad_norm": 13.596660614013672,
      "learning_rate": 1e-05,
      "loss": 6.2311,
      "step": 1852
    },
    {
      "epoch": 0.40151761517615175,
      "step": 1852,
      "training_loss": 3.6622800827026367
    },
    {
      "epoch": 0.40151761517615175,
      "step": 1852,
      "training_loss": 6.503032207489014
    },
    {
      "epoch": 0.40151761517615175,
      "step": 1852,
      "training_loss": 6.761368751525879
    },
    {
      "epoch": 0.40151761517615175,
      "step": 1852,
      "training_loss": 7.342020034790039
    },
    {
      "epoch": 0.4017344173441734,
      "step": 1853,
      "training_loss": 6.785214900970459
    },
    {
      "epoch": 0.4017344173441734,
      "step": 1853,
      "training_loss": 6.681735038757324
    },
    {
      "epoch": 0.4017344173441734,
      "step": 1853,
      "training_loss": 7.456065654754639
    },
    {
      "epoch": 0.4017344173441734,
      "step": 1853,
      "training_loss": 7.079651355743408
    },
    {
      "epoch": 0.40195121951219515,
      "step": 1854,
      "training_loss": 6.5498738288879395
    },
    {
      "epoch": 0.40195121951219515,
      "step": 1854,
      "training_loss": 6.705897331237793
    },
    {
      "epoch": 0.40195121951219515,
      "step": 1854,
      "training_loss": 7.4334282875061035
    },
    {
      "epoch": 0.40195121951219515,
      "step": 1854,
      "training_loss": 6.809157848358154
    },
    {
      "epoch": 0.4021680216802168,
      "step": 1855,
      "training_loss": 5.801132678985596
    },
    {
      "epoch": 0.4021680216802168,
      "step": 1855,
      "training_loss": 7.5386762619018555
    },
    {
      "epoch": 0.4021680216802168,
      "step": 1855,
      "training_loss": 6.772559642791748
    },
    {
      "epoch": 0.4021680216802168,
      "step": 1855,
      "training_loss": 6.346911430358887
    },
    {
      "epoch": 0.4023848238482385,
      "grad_norm": 18.78920555114746,
      "learning_rate": 1e-05,
      "loss": 6.6393,
      "step": 1856
    },
    {
      "epoch": 0.4023848238482385,
      "step": 1856,
      "training_loss": 5.76571798324585
    },
    {
      "epoch": 0.4023848238482385,
      "step": 1856,
      "training_loss": 7.788021564483643
    },
    {
      "epoch": 0.4023848238482385,
      "step": 1856,
      "training_loss": 6.0220866203308105
    },
    {
      "epoch": 0.4023848238482385,
      "step": 1856,
      "training_loss": 7.239981651306152
    },
    {
      "epoch": 0.40260162601626015,
      "step": 1857,
      "training_loss": 6.772984504699707
    },
    {
      "epoch": 0.40260162601626015,
      "step": 1857,
      "training_loss": 7.315937042236328
    },
    {
      "epoch": 0.40260162601626015,
      "step": 1857,
      "training_loss": 6.967238426208496
    },
    {
      "epoch": 0.40260162601626015,
      "step": 1857,
      "training_loss": 5.7186970710754395
    },
    {
      "epoch": 0.4028184281842818,
      "step": 1858,
      "training_loss": 5.399300575256348
    },
    {
      "epoch": 0.4028184281842818,
      "step": 1858,
      "training_loss": 6.253477096557617
    },
    {
      "epoch": 0.4028184281842818,
      "step": 1858,
      "training_loss": 6.227306842803955
    },
    {
      "epoch": 0.4028184281842818,
      "step": 1858,
      "training_loss": 6.507107734680176
    },
    {
      "epoch": 0.40303523035230354,
      "step": 1859,
      "training_loss": 5.3873209953308105
    },
    {
      "epoch": 0.40303523035230354,
      "step": 1859,
      "training_loss": 7.38709831237793
    },
    {
      "epoch": 0.40303523035230354,
      "step": 1859,
      "training_loss": 6.16787576675415
    },
    {
      "epoch": 0.40303523035230354,
      "step": 1859,
      "training_loss": 6.38165807723999
    },
    {
      "epoch": 0.4032520325203252,
      "grad_norm": 17.281696319580078,
      "learning_rate": 1e-05,
      "loss": 6.4564,
      "step": 1860
    },
    {
      "epoch": 0.4032520325203252,
      "step": 1860,
      "training_loss": 6.003698825836182
    },
    {
      "epoch": 0.4032520325203252,
      "step": 1860,
      "training_loss": 4.020540714263916
    },
    {
      "epoch": 0.4032520325203252,
      "step": 1860,
      "training_loss": 6.489699840545654
    },
    {
      "epoch": 0.4032520325203252,
      "step": 1860,
      "training_loss": 6.180394172668457
    },
    {
      "epoch": 0.4034688346883469,
      "step": 1861,
      "training_loss": 7.470664978027344
    },
    {
      "epoch": 0.4034688346883469,
      "step": 1861,
      "training_loss": 6.270418643951416
    },
    {
      "epoch": 0.4034688346883469,
      "step": 1861,
      "training_loss": 5.145856857299805
    },
    {
      "epoch": 0.4034688346883469,
      "step": 1861,
      "training_loss": 6.869450092315674
    },
    {
      "epoch": 0.40368563685636855,
      "step": 1862,
      "training_loss": 6.730738162994385
    },
    {
      "epoch": 0.40368563685636855,
      "step": 1862,
      "training_loss": 6.5919575691223145
    },
    {
      "epoch": 0.40368563685636855,
      "step": 1862,
      "training_loss": 7.901322364807129
    },
    {
      "epoch": 0.40368563685636855,
      "step": 1862,
      "training_loss": 5.6797990798950195
    },
    {
      "epoch": 0.40390243902439027,
      "step": 1863,
      "training_loss": 6.4247822761535645
    },
    {
      "epoch": 0.40390243902439027,
      "step": 1863,
      "training_loss": 5.097153663635254
    },
    {
      "epoch": 0.40390243902439027,
      "step": 1863,
      "training_loss": 7.2169647216796875
    },
    {
      "epoch": 0.40390243902439027,
      "step": 1863,
      "training_loss": 6.0102152824401855
    },
    {
      "epoch": 0.40411924119241194,
      "grad_norm": 21.67249870300293,
      "learning_rate": 1e-05,
      "loss": 6.2565,
      "step": 1864
    },
    {
      "epoch": 0.40411924119241194,
      "step": 1864,
      "training_loss": 6.871197700500488
    },
    {
      "epoch": 0.40411924119241194,
      "step": 1864,
      "training_loss": 6.765739917755127
    },
    {
      "epoch": 0.40411924119241194,
      "step": 1864,
      "training_loss": 7.170307636260986
    },
    {
      "epoch": 0.40411924119241194,
      "step": 1864,
      "training_loss": 6.720277786254883
    },
    {
      "epoch": 0.4043360433604336,
      "step": 1865,
      "training_loss": 5.745275020599365
    },
    {
      "epoch": 0.4043360433604336,
      "step": 1865,
      "training_loss": 7.258533477783203
    },
    {
      "epoch": 0.4043360433604336,
      "step": 1865,
      "training_loss": 5.876693248748779
    },
    {
      "epoch": 0.4043360433604336,
      "step": 1865,
      "training_loss": 7.696162700653076
    },
    {
      "epoch": 0.4045528455284553,
      "step": 1866,
      "training_loss": 7.52539587020874
    },
    {
      "epoch": 0.4045528455284553,
      "step": 1866,
      "training_loss": 6.815370559692383
    },
    {
      "epoch": 0.4045528455284553,
      "step": 1866,
      "training_loss": 7.287437915802002
    },
    {
      "epoch": 0.4045528455284553,
      "step": 1866,
      "training_loss": 6.409738540649414
    },
    {
      "epoch": 0.40476964769647694,
      "step": 1867,
      "training_loss": 10.687601089477539
    },
    {
      "epoch": 0.40476964769647694,
      "step": 1867,
      "training_loss": 6.661602973937988
    },
    {
      "epoch": 0.40476964769647694,
      "step": 1867,
      "training_loss": 6.817859649658203
    },
    {
      "epoch": 0.40476964769647694,
      "step": 1867,
      "training_loss": 5.97905969619751
    },
    {
      "epoch": 0.40498644986449867,
      "grad_norm": 21.524457931518555,
      "learning_rate": 1e-05,
      "loss": 7.018,
      "step": 1868
    },
    {
      "epoch": 0.40498644986449867,
      "step": 1868,
      "training_loss": 6.233287811279297
    },
    {
      "epoch": 0.40498644986449867,
      "step": 1868,
      "training_loss": 6.726119041442871
    },
    {
      "epoch": 0.40498644986449867,
      "step": 1868,
      "training_loss": 5.897986888885498
    },
    {
      "epoch": 0.40498644986449867,
      "step": 1868,
      "training_loss": 6.1259541511535645
    },
    {
      "epoch": 0.40520325203252033,
      "step": 1869,
      "training_loss": 5.244901180267334
    },
    {
      "epoch": 0.40520325203252033,
      "step": 1869,
      "training_loss": 8.135306358337402
    },
    {
      "epoch": 0.40520325203252033,
      "step": 1869,
      "training_loss": 4.731372356414795
    },
    {
      "epoch": 0.40520325203252033,
      "step": 1869,
      "training_loss": 7.571338176727295
    },
    {
      "epoch": 0.405420054200542,
      "step": 1870,
      "training_loss": 6.799116134643555
    },
    {
      "epoch": 0.405420054200542,
      "step": 1870,
      "training_loss": 6.605714321136475
    },
    {
      "epoch": 0.405420054200542,
      "step": 1870,
      "training_loss": 5.013128757476807
    },
    {
      "epoch": 0.405420054200542,
      "step": 1870,
      "training_loss": 6.532676696777344
    },
    {
      "epoch": 0.40563685636856367,
      "step": 1871,
      "training_loss": 7.7609734535217285
    },
    {
      "epoch": 0.40563685636856367,
      "step": 1871,
      "training_loss": 5.946559906005859
    },
    {
      "epoch": 0.40563685636856367,
      "step": 1871,
      "training_loss": 5.793696880340576
    },
    {
      "epoch": 0.40563685636856367,
      "step": 1871,
      "training_loss": 6.580784320831299
    },
    {
      "epoch": 0.4058536585365854,
      "grad_norm": 17.780170440673828,
      "learning_rate": 1e-05,
      "loss": 6.3562,
      "step": 1872
    },
    {
      "epoch": 0.4058536585365854,
      "step": 1872,
      "training_loss": 6.190147876739502
    },
    {
      "epoch": 0.4058536585365854,
      "step": 1872,
      "training_loss": 7.1649065017700195
    },
    {
      "epoch": 0.4058536585365854,
      "step": 1872,
      "training_loss": 5.308456897735596
    },
    {
      "epoch": 0.4058536585365854,
      "step": 1872,
      "training_loss": 6.204809188842773
    },
    {
      "epoch": 0.40607046070460706,
      "step": 1873,
      "training_loss": 6.861056804656982
    },
    {
      "epoch": 0.40607046070460706,
      "step": 1873,
      "training_loss": 5.980719089508057
    },
    {
      "epoch": 0.40607046070460706,
      "step": 1873,
      "training_loss": 7.6779866218566895
    },
    {
      "epoch": 0.40607046070460706,
      "step": 1873,
      "training_loss": 7.016140937805176
    },
    {
      "epoch": 0.40628726287262873,
      "step": 1874,
      "training_loss": 6.2549309730529785
    },
    {
      "epoch": 0.40628726287262873,
      "step": 1874,
      "training_loss": 3.5693860054016113
    },
    {
      "epoch": 0.40628726287262873,
      "step": 1874,
      "training_loss": 7.356621265411377
    },
    {
      "epoch": 0.40628726287262873,
      "step": 1874,
      "training_loss": 7.005140781402588
    },
    {
      "epoch": 0.4065040650406504,
      "step": 1875,
      "training_loss": 8.318432807922363
    },
    {
      "epoch": 0.4065040650406504,
      "step": 1875,
      "training_loss": 7.369244575500488
    },
    {
      "epoch": 0.4065040650406504,
      "step": 1875,
      "training_loss": 6.669164657592773
    },
    {
      "epoch": 0.4065040650406504,
      "step": 1875,
      "training_loss": 6.820835113525391
    },
    {
      "epoch": 0.40672086720867207,
      "grad_norm": 20.665708541870117,
      "learning_rate": 1e-05,
      "loss": 6.6105,
      "step": 1876
    },
    {
      "epoch": 0.40672086720867207,
      "step": 1876,
      "training_loss": 6.416834354400635
    },
    {
      "epoch": 0.40672086720867207,
      "step": 1876,
      "training_loss": 5.6337809562683105
    },
    {
      "epoch": 0.40672086720867207,
      "step": 1876,
      "training_loss": 7.0164103507995605
    },
    {
      "epoch": 0.40672086720867207,
      "step": 1876,
      "training_loss": 7.652088642120361
    },
    {
      "epoch": 0.4069376693766938,
      "step": 1877,
      "training_loss": 8.57795524597168
    },
    {
      "epoch": 0.4069376693766938,
      "step": 1877,
      "training_loss": 6.560370922088623
    },
    {
      "epoch": 0.4069376693766938,
      "step": 1877,
      "training_loss": 7.591281414031982
    },
    {
      "epoch": 0.4069376693766938,
      "step": 1877,
      "training_loss": 5.459208011627197
    },
    {
      "epoch": 0.40715447154471546,
      "step": 1878,
      "training_loss": 5.951707363128662
    },
    {
      "epoch": 0.40715447154471546,
      "step": 1878,
      "training_loss": 6.566199779510498
    },
    {
      "epoch": 0.40715447154471546,
      "step": 1878,
      "training_loss": 7.3766188621521
    },
    {
      "epoch": 0.40715447154471546,
      "step": 1878,
      "training_loss": 7.409692287445068
    },
    {
      "epoch": 0.4073712737127371,
      "step": 1879,
      "training_loss": 6.3880205154418945
    },
    {
      "epoch": 0.4073712737127371,
      "step": 1879,
      "training_loss": 7.366631984710693
    },
    {
      "epoch": 0.4073712737127371,
      "step": 1879,
      "training_loss": 4.675314426422119
    },
    {
      "epoch": 0.4073712737127371,
      "step": 1879,
      "training_loss": 7.029503345489502
    },
    {
      "epoch": 0.4075880758807588,
      "grad_norm": 16.82312774658203,
      "learning_rate": 1e-05,
      "loss": 6.7295,
      "step": 1880
    },
    {
      "epoch": 0.4075880758807588,
      "step": 1880,
      "training_loss": 6.931262016296387
    },
    {
      "epoch": 0.4075880758807588,
      "step": 1880,
      "training_loss": 7.379494667053223
    },
    {
      "epoch": 0.4075880758807588,
      "step": 1880,
      "training_loss": 7.2394208908081055
    },
    {
      "epoch": 0.4075880758807588,
      "step": 1880,
      "training_loss": 5.8978705406188965
    },
    {
      "epoch": 0.40780487804878046,
      "step": 1881,
      "training_loss": 7.003448963165283
    },
    {
      "epoch": 0.40780487804878046,
      "step": 1881,
      "training_loss": 6.452674865722656
    },
    {
      "epoch": 0.40780487804878046,
      "step": 1881,
      "training_loss": 7.382771968841553
    },
    {
      "epoch": 0.40780487804878046,
      "step": 1881,
      "training_loss": 5.953222274780273
    },
    {
      "epoch": 0.4080216802168022,
      "step": 1882,
      "training_loss": 6.717197418212891
    },
    {
      "epoch": 0.4080216802168022,
      "step": 1882,
      "training_loss": 6.979671955108643
    },
    {
      "epoch": 0.4080216802168022,
      "step": 1882,
      "training_loss": 5.626338005065918
    },
    {
      "epoch": 0.4080216802168022,
      "step": 1882,
      "training_loss": 5.3171610832214355
    },
    {
      "epoch": 0.40823848238482385,
      "step": 1883,
      "training_loss": 4.691711902618408
    },
    {
      "epoch": 0.40823848238482385,
      "step": 1883,
      "training_loss": 10.696297645568848
    },
    {
      "epoch": 0.40823848238482385,
      "step": 1883,
      "training_loss": 5.484442710876465
    },
    {
      "epoch": 0.40823848238482385,
      "step": 1883,
      "training_loss": 7.473940849304199
    },
    {
      "epoch": 0.4084552845528455,
      "grad_norm": 22.5004825592041,
      "learning_rate": 1e-05,
      "loss": 6.7017,
      "step": 1884
    },
    {
      "epoch": 0.4084552845528455,
      "step": 1884,
      "training_loss": 6.352770805358887
    },
    {
      "epoch": 0.4084552845528455,
      "step": 1884,
      "training_loss": 6.9017133712768555
    },
    {
      "epoch": 0.4084552845528455,
      "step": 1884,
      "training_loss": 6.746150493621826
    },
    {
      "epoch": 0.4084552845528455,
      "step": 1884,
      "training_loss": 5.227016925811768
    },
    {
      "epoch": 0.4086720867208672,
      "step": 1885,
      "training_loss": 7.759888172149658
    },
    {
      "epoch": 0.4086720867208672,
      "step": 1885,
      "training_loss": 7.135725975036621
    },
    {
      "epoch": 0.4086720867208672,
      "step": 1885,
      "training_loss": 8.670476913452148
    },
    {
      "epoch": 0.4086720867208672,
      "step": 1885,
      "training_loss": 4.320190906524658
    },
    {
      "epoch": 0.4088888888888889,
      "step": 1886,
      "training_loss": 5.478879928588867
    },
    {
      "epoch": 0.4088888888888889,
      "step": 1886,
      "training_loss": 6.924107074737549
    },
    {
      "epoch": 0.4088888888888889,
      "step": 1886,
      "training_loss": 6.228591442108154
    },
    {
      "epoch": 0.4088888888888889,
      "step": 1886,
      "training_loss": 6.637307643890381
    },
    {
      "epoch": 0.4091056910569106,
      "step": 1887,
      "training_loss": 6.911525249481201
    },
    {
      "epoch": 0.4091056910569106,
      "step": 1887,
      "training_loss": 7.312859535217285
    },
    {
      "epoch": 0.4091056910569106,
      "step": 1887,
      "training_loss": 6.333946228027344
    },
    {
      "epoch": 0.4091056910569106,
      "step": 1887,
      "training_loss": 6.4138946533203125
    },
    {
      "epoch": 0.40932249322493225,
      "grad_norm": 12.248376846313477,
      "learning_rate": 1e-05,
      "loss": 6.5847,
      "step": 1888
    },
    {
      "epoch": 0.40932249322493225,
      "step": 1888,
      "training_loss": 5.431532382965088
    },
    {
      "epoch": 0.40932249322493225,
      "step": 1888,
      "training_loss": 6.369750022888184
    },
    {
      "epoch": 0.40932249322493225,
      "step": 1888,
      "training_loss": 6.85179328918457
    },
    {
      "epoch": 0.40932249322493225,
      "step": 1888,
      "training_loss": 6.882585525512695
    },
    {
      "epoch": 0.4095392953929539,
      "step": 1889,
      "training_loss": 7.033573627471924
    },
    {
      "epoch": 0.4095392953929539,
      "step": 1889,
      "training_loss": 7.5205817222595215
    },
    {
      "epoch": 0.4095392953929539,
      "step": 1889,
      "training_loss": 6.6327643394470215
    },
    {
      "epoch": 0.4095392953929539,
      "step": 1889,
      "training_loss": 7.576594829559326
    },
    {
      "epoch": 0.4097560975609756,
      "step": 1890,
      "training_loss": 6.142634391784668
    },
    {
      "epoch": 0.4097560975609756,
      "step": 1890,
      "training_loss": 6.619161605834961
    },
    {
      "epoch": 0.4097560975609756,
      "step": 1890,
      "training_loss": 3.827387571334839
    },
    {
      "epoch": 0.4097560975609756,
      "step": 1890,
      "training_loss": 6.942764759063721
    },
    {
      "epoch": 0.4099728997289973,
      "step": 1891,
      "training_loss": 5.69010591506958
    },
    {
      "epoch": 0.4099728997289973,
      "step": 1891,
      "training_loss": 6.935648441314697
    },
    {
      "epoch": 0.4099728997289973,
      "step": 1891,
      "training_loss": 6.436203479766846
    },
    {
      "epoch": 0.4099728997289973,
      "step": 1891,
      "training_loss": 5.4283037185668945
    },
    {
      "epoch": 0.410189701897019,
      "grad_norm": 15.506071090698242,
      "learning_rate": 1e-05,
      "loss": 6.3951,
      "step": 1892
    },
    {
      "epoch": 0.410189701897019,
      "step": 1892,
      "training_loss": 6.487832069396973
    },
    {
      "epoch": 0.410189701897019,
      "step": 1892,
      "training_loss": 7.098579406738281
    },
    {
      "epoch": 0.410189701897019,
      "step": 1892,
      "training_loss": 6.53529167175293
    },
    {
      "epoch": 0.410189701897019,
      "step": 1892,
      "training_loss": 6.5880231857299805
    },
    {
      "epoch": 0.41040650406504064,
      "step": 1893,
      "training_loss": 5.939143180847168
    },
    {
      "epoch": 0.41040650406504064,
      "step": 1893,
      "training_loss": 6.175016403198242
    },
    {
      "epoch": 0.41040650406504064,
      "step": 1893,
      "training_loss": 7.343886852264404
    },
    {
      "epoch": 0.41040650406504064,
      "step": 1893,
      "training_loss": 6.850743293762207
    },
    {
      "epoch": 0.4106233062330623,
      "step": 1894,
      "training_loss": 6.9289231300354
    },
    {
      "epoch": 0.4106233062330623,
      "step": 1894,
      "training_loss": 7.069553375244141
    },
    {
      "epoch": 0.4106233062330623,
      "step": 1894,
      "training_loss": 5.856866836547852
    },
    {
      "epoch": 0.4106233062330623,
      "step": 1894,
      "training_loss": 7.127680778503418
    },
    {
      "epoch": 0.41084010840108404,
      "step": 1895,
      "training_loss": 6.526439189910889
    },
    {
      "epoch": 0.41084010840108404,
      "step": 1895,
      "training_loss": 6.550628662109375
    },
    {
      "epoch": 0.41084010840108404,
      "step": 1895,
      "training_loss": 4.745205402374268
    },
    {
      "epoch": 0.41084010840108404,
      "step": 1895,
      "training_loss": 8.8599271774292
    },
    {
      "epoch": 0.4110569105691057,
      "grad_norm": 17.227519989013672,
      "learning_rate": 1e-05,
      "loss": 6.6677,
      "step": 1896
    },
    {
      "epoch": 0.4110569105691057,
      "step": 1896,
      "training_loss": 5.970170974731445
    },
    {
      "epoch": 0.4110569105691057,
      "step": 1896,
      "training_loss": 6.269354820251465
    },
    {
      "epoch": 0.4110569105691057,
      "step": 1896,
      "training_loss": 7.04443883895874
    },
    {
      "epoch": 0.4110569105691057,
      "step": 1896,
      "training_loss": 7.052167892456055
    },
    {
      "epoch": 0.4112737127371274,
      "step": 1897,
      "training_loss": 5.986697673797607
    },
    {
      "epoch": 0.4112737127371274,
      "step": 1897,
      "training_loss": 7.747893333435059
    },
    {
      "epoch": 0.4112737127371274,
      "step": 1897,
      "training_loss": 7.427491188049316
    },
    {
      "epoch": 0.4112737127371274,
      "step": 1897,
      "training_loss": 6.74328088760376
    },
    {
      "epoch": 0.41149051490514904,
      "step": 1898,
      "training_loss": 4.345280170440674
    },
    {
      "epoch": 0.41149051490514904,
      "step": 1898,
      "training_loss": 7.102515697479248
    },
    {
      "epoch": 0.41149051490514904,
      "step": 1898,
      "training_loss": 7.054249286651611
    },
    {
      "epoch": 0.41149051490514904,
      "step": 1898,
      "training_loss": 5.519176483154297
    },
    {
      "epoch": 0.4117073170731707,
      "step": 1899,
      "training_loss": 6.29099178314209
    },
    {
      "epoch": 0.4117073170731707,
      "step": 1899,
      "training_loss": 5.374403953552246
    },
    {
      "epoch": 0.4117073170731707,
      "step": 1899,
      "training_loss": 4.2166218757629395
    },
    {
      "epoch": 0.4117073170731707,
      "step": 1899,
      "training_loss": 5.409414291381836
    },
    {
      "epoch": 0.41192411924119243,
      "grad_norm": 14.2677583694458,
      "learning_rate": 1e-05,
      "loss": 6.2221,
      "step": 1900
    },
    {
      "epoch": 0.41192411924119243,
      "step": 1900,
      "training_loss": 7.179126739501953
    },
    {
      "epoch": 0.41192411924119243,
      "step": 1900,
      "training_loss": 5.007016658782959
    },
    {
      "epoch": 0.41192411924119243,
      "step": 1900,
      "training_loss": 6.238134384155273
    },
    {
      "epoch": 0.41192411924119243,
      "step": 1900,
      "training_loss": 6.2163567543029785
    },
    {
      "epoch": 0.4121409214092141,
      "step": 1901,
      "training_loss": 5.875698089599609
    },
    {
      "epoch": 0.4121409214092141,
      "step": 1901,
      "training_loss": 6.415410041809082
    },
    {
      "epoch": 0.4121409214092141,
      "step": 1901,
      "training_loss": 6.738513946533203
    },
    {
      "epoch": 0.4121409214092141,
      "step": 1901,
      "training_loss": 6.627484321594238
    },
    {
      "epoch": 0.41235772357723577,
      "step": 1902,
      "training_loss": 3.7812342643737793
    },
    {
      "epoch": 0.41235772357723577,
      "step": 1902,
      "training_loss": 5.914610385894775
    },
    {
      "epoch": 0.41235772357723577,
      "step": 1902,
      "training_loss": 7.705478191375732
    },
    {
      "epoch": 0.41235772357723577,
      "step": 1902,
      "training_loss": 6.359334468841553
    },
    {
      "epoch": 0.41257452574525744,
      "step": 1903,
      "training_loss": 2.991119146347046
    },
    {
      "epoch": 0.41257452574525744,
      "step": 1903,
      "training_loss": 7.184608459472656
    },
    {
      "epoch": 0.41257452574525744,
      "step": 1903,
      "training_loss": 4.243656158447266
    },
    {
      "epoch": 0.41257452574525744,
      "step": 1903,
      "training_loss": 6.201735019683838
    },
    {
      "epoch": 0.41279132791327916,
      "grad_norm": 15.86628532409668,
      "learning_rate": 1e-05,
      "loss": 5.9175,
      "step": 1904
    },
    {
      "epoch": 0.41279132791327916,
      "step": 1904,
      "training_loss": 6.595329761505127
    },
    {
      "epoch": 0.41279132791327916,
      "step": 1904,
      "training_loss": 6.542766571044922
    },
    {
      "epoch": 0.41279132791327916,
      "step": 1904,
      "training_loss": 6.4353837966918945
    },
    {
      "epoch": 0.41279132791327916,
      "step": 1904,
      "training_loss": 6.687745571136475
    },
    {
      "epoch": 0.41300813008130083,
      "step": 1905,
      "training_loss": 6.9228196144104
    },
    {
      "epoch": 0.41300813008130083,
      "step": 1905,
      "training_loss": 6.515002727508545
    },
    {
      "epoch": 0.41300813008130083,
      "step": 1905,
      "training_loss": 6.225407123565674
    },
    {
      "epoch": 0.41300813008130083,
      "step": 1905,
      "training_loss": 2.9247775077819824
    },
    {
      "epoch": 0.4132249322493225,
      "step": 1906,
      "training_loss": 7.578939914703369
    },
    {
      "epoch": 0.4132249322493225,
      "step": 1906,
      "training_loss": 4.9982805252075195
    },
    {
      "epoch": 0.4132249322493225,
      "step": 1906,
      "training_loss": 6.626768112182617
    },
    {
      "epoch": 0.4132249322493225,
      "step": 1906,
      "training_loss": 6.4777445793151855
    },
    {
      "epoch": 0.41344173441734416,
      "step": 1907,
      "training_loss": 7.828850746154785
    },
    {
      "epoch": 0.41344173441734416,
      "step": 1907,
      "training_loss": 6.952213287353516
    },
    {
      "epoch": 0.41344173441734416,
      "step": 1907,
      "training_loss": 7.033118724822998
    },
    {
      "epoch": 0.41344173441734416,
      "step": 1907,
      "training_loss": 6.330508232116699
    },
    {
      "epoch": 0.41365853658536583,
      "grad_norm": 14.877819061279297,
      "learning_rate": 1e-05,
      "loss": 6.4172,
      "step": 1908
    },
    {
      "epoch": 0.41365853658536583,
      "step": 1908,
      "training_loss": 7.918198108673096
    },
    {
      "epoch": 0.41365853658536583,
      "step": 1908,
      "training_loss": 5.634116172790527
    },
    {
      "epoch": 0.41365853658536583,
      "step": 1908,
      "training_loss": 7.263555526733398
    },
    {
      "epoch": 0.41365853658536583,
      "step": 1908,
      "training_loss": 7.501521110534668
    },
    {
      "epoch": 0.41387533875338756,
      "step": 1909,
      "training_loss": 6.846982002258301
    },
    {
      "epoch": 0.41387533875338756,
      "step": 1909,
      "training_loss": 4.658488750457764
    },
    {
      "epoch": 0.41387533875338756,
      "step": 1909,
      "training_loss": 5.303377151489258
    },
    {
      "epoch": 0.41387533875338756,
      "step": 1909,
      "training_loss": 7.251320838928223
    },
    {
      "epoch": 0.4140921409214092,
      "step": 1910,
      "training_loss": 6.426847457885742
    },
    {
      "epoch": 0.4140921409214092,
      "step": 1910,
      "training_loss": 7.724176406860352
    },
    {
      "epoch": 0.4140921409214092,
      "step": 1910,
      "training_loss": 6.204939842224121
    },
    {
      "epoch": 0.4140921409214092,
      "step": 1910,
      "training_loss": 7.360024452209473
    },
    {
      "epoch": 0.4143089430894309,
      "step": 1911,
      "training_loss": 4.800931453704834
    },
    {
      "epoch": 0.4143089430894309,
      "step": 1911,
      "training_loss": 6.581536293029785
    },
    {
      "epoch": 0.4143089430894309,
      "step": 1911,
      "training_loss": 4.535346508026123
    },
    {
      "epoch": 0.4143089430894309,
      "step": 1911,
      "training_loss": 7.603006362915039
    },
    {
      "epoch": 0.41452574525745256,
      "grad_norm": 11.48713493347168,
      "learning_rate": 1e-05,
      "loss": 6.4759,
      "step": 1912
    },
    {
      "epoch": 0.41452574525745256,
      "step": 1912,
      "training_loss": 3.6928694248199463
    },
    {
      "epoch": 0.41452574525745256,
      "step": 1912,
      "training_loss": 5.933510780334473
    },
    {
      "epoch": 0.41452574525745256,
      "step": 1912,
      "training_loss": 4.993372917175293
    },
    {
      "epoch": 0.41452574525745256,
      "step": 1912,
      "training_loss": 4.072874069213867
    },
    {
      "epoch": 0.41474254742547423,
      "step": 1913,
      "training_loss": 7.627976894378662
    },
    {
      "epoch": 0.41474254742547423,
      "step": 1913,
      "training_loss": 5.135328769683838
    },
    {
      "epoch": 0.41474254742547423,
      "step": 1913,
      "training_loss": 7.214280605316162
    },
    {
      "epoch": 0.41474254742547423,
      "step": 1913,
      "training_loss": 5.6542649269104
    },
    {
      "epoch": 0.41495934959349595,
      "step": 1914,
      "training_loss": 5.926912307739258
    },
    {
      "epoch": 0.41495934959349595,
      "step": 1914,
      "training_loss": 6.495694637298584
    },
    {
      "epoch": 0.41495934959349595,
      "step": 1914,
      "training_loss": 7.195808410644531
    },
    {
      "epoch": 0.41495934959349595,
      "step": 1914,
      "training_loss": 7.094879627227783
    },
    {
      "epoch": 0.4151761517615176,
      "step": 1915,
      "training_loss": 6.78726863861084
    },
    {
      "epoch": 0.4151761517615176,
      "step": 1915,
      "training_loss": 3.5857090950012207
    },
    {
      "epoch": 0.4151761517615176,
      "step": 1915,
      "training_loss": 4.581235885620117
    },
    {
      "epoch": 0.4151761517615176,
      "step": 1915,
      "training_loss": 6.586261749267578
    },
    {
      "epoch": 0.4153929539295393,
      "grad_norm": 19.122825622558594,
      "learning_rate": 1e-05,
      "loss": 5.7861,
      "step": 1916
    },
    {
      "epoch": 0.4153929539295393,
      "step": 1916,
      "training_loss": 7.2874226570129395
    },
    {
      "epoch": 0.4153929539295393,
      "step": 1916,
      "training_loss": 3.7157466411590576
    },
    {
      "epoch": 0.4153929539295393,
      "step": 1916,
      "training_loss": 7.390779495239258
    },
    {
      "epoch": 0.4153929539295393,
      "step": 1916,
      "training_loss": 5.417410373687744
    },
    {
      "epoch": 0.41560975609756096,
      "step": 1917,
      "training_loss": 6.662814140319824
    },
    {
      "epoch": 0.41560975609756096,
      "step": 1917,
      "training_loss": 6.061402797698975
    },
    {
      "epoch": 0.41560975609756096,
      "step": 1917,
      "training_loss": 6.778809547424316
    },
    {
      "epoch": 0.41560975609756096,
      "step": 1917,
      "training_loss": 6.388246536254883
    },
    {
      "epoch": 0.4158265582655827,
      "step": 1918,
      "training_loss": 7.04603910446167
    },
    {
      "epoch": 0.4158265582655827,
      "step": 1918,
      "training_loss": 5.550232410430908
    },
    {
      "epoch": 0.4158265582655827,
      "step": 1918,
      "training_loss": 5.713818073272705
    },
    {
      "epoch": 0.4158265582655827,
      "step": 1918,
      "training_loss": 6.174120903015137
    },
    {
      "epoch": 0.41604336043360435,
      "step": 1919,
      "training_loss": 6.049472808837891
    },
    {
      "epoch": 0.41604336043360435,
      "step": 1919,
      "training_loss": 6.646623134613037
    },
    {
      "epoch": 0.41604336043360435,
      "step": 1919,
      "training_loss": 6.865466594696045
    },
    {
      "epoch": 0.41604336043360435,
      "step": 1919,
      "training_loss": 8.061714172363281
    },
    {
      "epoch": 0.416260162601626,
      "grad_norm": 13.72866153717041,
      "learning_rate": 1e-05,
      "loss": 6.3631,
      "step": 1920
    },
    {
      "epoch": 0.416260162601626,
      "step": 1920,
      "training_loss": 5.462740421295166
    },
    {
      "epoch": 0.416260162601626,
      "step": 1920,
      "training_loss": 4.569949150085449
    },
    {
      "epoch": 0.416260162601626,
      "step": 1920,
      "training_loss": 7.433914661407471
    },
    {
      "epoch": 0.416260162601626,
      "step": 1920,
      "training_loss": 6.924218654632568
    },
    {
      "epoch": 0.4164769647696477,
      "step": 1921,
      "training_loss": 6.625162601470947
    },
    {
      "epoch": 0.4164769647696477,
      "step": 1921,
      "training_loss": 7.551168918609619
    },
    {
      "epoch": 0.4164769647696477,
      "step": 1921,
      "training_loss": 6.9455790519714355
    },
    {
      "epoch": 0.4164769647696477,
      "step": 1921,
      "training_loss": 7.727940082550049
    },
    {
      "epoch": 0.41669376693766935,
      "step": 1922,
      "training_loss": 5.71296501159668
    },
    {
      "epoch": 0.41669376693766935,
      "step": 1922,
      "training_loss": 6.053689002990723
    },
    {
      "epoch": 0.41669376693766935,
      "step": 1922,
      "training_loss": 7.120579719543457
    },
    {
      "epoch": 0.41669376693766935,
      "step": 1922,
      "training_loss": 6.152449607849121
    },
    {
      "epoch": 0.4169105691056911,
      "step": 1923,
      "training_loss": 4.529555320739746
    },
    {
      "epoch": 0.4169105691056911,
      "step": 1923,
      "training_loss": 5.401223659515381
    },
    {
      "epoch": 0.4169105691056911,
      "step": 1923,
      "training_loss": 5.226245403289795
    },
    {
      "epoch": 0.4169105691056911,
      "step": 1923,
      "training_loss": 6.483837604522705
    },
    {
      "epoch": 0.41712737127371274,
      "grad_norm": 16.00236701965332,
      "learning_rate": 1e-05,
      "loss": 6.2451,
      "step": 1924
    },
    {
      "epoch": 0.41712737127371274,
      "step": 1924,
      "training_loss": 6.339420318603516
    },
    {
      "epoch": 0.41712737127371274,
      "step": 1924,
      "training_loss": 6.590285301208496
    },
    {
      "epoch": 0.41712737127371274,
      "step": 1924,
      "training_loss": 4.4133100509643555
    },
    {
      "epoch": 0.41712737127371274,
      "step": 1924,
      "training_loss": 6.7745680809021
    },
    {
      "epoch": 0.4173441734417344,
      "step": 1925,
      "training_loss": 5.538273811340332
    },
    {
      "epoch": 0.4173441734417344,
      "step": 1925,
      "training_loss": 7.4234700202941895
    },
    {
      "epoch": 0.4173441734417344,
      "step": 1925,
      "training_loss": 6.689407825469971
    },
    {
      "epoch": 0.4173441734417344,
      "step": 1925,
      "training_loss": 6.74006462097168
    },
    {
      "epoch": 0.4175609756097561,
      "step": 1926,
      "training_loss": 5.480340480804443
    },
    {
      "epoch": 0.4175609756097561,
      "step": 1926,
      "training_loss": 6.600086212158203
    },
    {
      "epoch": 0.4175609756097561,
      "step": 1926,
      "training_loss": 5.744663238525391
    },
    {
      "epoch": 0.4175609756097561,
      "step": 1926,
      "training_loss": 6.495875358581543
    },
    {
      "epoch": 0.4177777777777778,
      "step": 1927,
      "training_loss": 8.14091968536377
    },
    {
      "epoch": 0.4177777777777778,
      "step": 1927,
      "training_loss": 7.059469699859619
    },
    {
      "epoch": 0.4177777777777778,
      "step": 1927,
      "training_loss": 6.91501522064209
    },
    {
      "epoch": 0.4177777777777778,
      "step": 1927,
      "training_loss": 6.790468692779541
    },
    {
      "epoch": 0.41799457994579947,
      "grad_norm": 15.990968704223633,
      "learning_rate": 1e-05,
      "loss": 6.4835,
      "step": 1928
    },
    {
      "epoch": 0.41799457994579947,
      "step": 1928,
      "training_loss": 4.320781707763672
    },
    {
      "epoch": 0.41799457994579947,
      "step": 1928,
      "training_loss": 7.1291985511779785
    },
    {
      "epoch": 0.41799457994579947,
      "step": 1928,
      "training_loss": 7.320425033569336
    },
    {
      "epoch": 0.41799457994579947,
      "step": 1928,
      "training_loss": 4.5460205078125
    },
    {
      "epoch": 0.41821138211382114,
      "step": 1929,
      "training_loss": 7.096149444580078
    },
    {
      "epoch": 0.41821138211382114,
      "step": 1929,
      "training_loss": 7.203803539276123
    },
    {
      "epoch": 0.41821138211382114,
      "step": 1929,
      "training_loss": 6.173168659210205
    },
    {
      "epoch": 0.41821138211382114,
      "step": 1929,
      "training_loss": 5.108775615692139
    },
    {
      "epoch": 0.4184281842818428,
      "step": 1930,
      "training_loss": 5.6264872550964355
    },
    {
      "epoch": 0.4184281842818428,
      "step": 1930,
      "training_loss": 6.903293609619141
    },
    {
      "epoch": 0.4184281842818428,
      "step": 1930,
      "training_loss": 6.987724781036377
    },
    {
      "epoch": 0.4184281842818428,
      "step": 1930,
      "training_loss": 7.4498443603515625
    },
    {
      "epoch": 0.4186449864498645,
      "step": 1931,
      "training_loss": 9.152097702026367
    },
    {
      "epoch": 0.4186449864498645,
      "step": 1931,
      "training_loss": 7.235228061676025
    },
    {
      "epoch": 0.4186449864498645,
      "step": 1931,
      "training_loss": 6.4201860427856445
    },
    {
      "epoch": 0.4186449864498645,
      "step": 1931,
      "training_loss": 4.6673264503479
    },
    {
      "epoch": 0.4188617886178862,
      "grad_norm": 19.249807357788086,
      "learning_rate": 1e-05,
      "loss": 6.4588,
      "step": 1932
    },
    {
      "epoch": 0.4188617886178862,
      "step": 1932,
      "training_loss": 5.8642096519470215
    },
    {
      "epoch": 0.4188617886178862,
      "step": 1932,
      "training_loss": 7.4649763107299805
    },
    {
      "epoch": 0.4188617886178862,
      "step": 1932,
      "training_loss": 7.198094844818115
    },
    {
      "epoch": 0.4188617886178862,
      "step": 1932,
      "training_loss": 5.641026020050049
    },
    {
      "epoch": 0.41907859078590787,
      "step": 1933,
      "training_loss": 4.78489351272583
    },
    {
      "epoch": 0.41907859078590787,
      "step": 1933,
      "training_loss": 6.629158020019531
    },
    {
      "epoch": 0.41907859078590787,
      "step": 1933,
      "training_loss": 3.6082873344421387
    },
    {
      "epoch": 0.41907859078590787,
      "step": 1933,
      "training_loss": 5.402839183807373
    },
    {
      "epoch": 0.41929539295392954,
      "step": 1934,
      "training_loss": 6.582144260406494
    },
    {
      "epoch": 0.41929539295392954,
      "step": 1934,
      "training_loss": 6.798700332641602
    },
    {
      "epoch": 0.41929539295392954,
      "step": 1934,
      "training_loss": 7.016340255737305
    },
    {
      "epoch": 0.41929539295392954,
      "step": 1934,
      "training_loss": 6.967323303222656
    },
    {
      "epoch": 0.4195121951219512,
      "step": 1935,
      "training_loss": 7.011240482330322
    },
    {
      "epoch": 0.4195121951219512,
      "step": 1935,
      "training_loss": 5.385530471801758
    },
    {
      "epoch": 0.4195121951219512,
      "step": 1935,
      "training_loss": 5.903026580810547
    },
    {
      "epoch": 0.4195121951219512,
      "step": 1935,
      "training_loss": 7.647223949432373
    },
    {
      "epoch": 0.4197289972899729,
      "grad_norm": 21.717267990112305,
      "learning_rate": 1e-05,
      "loss": 6.2441,
      "step": 1936
    },
    {
      "epoch": 0.4197289972899729,
      "step": 1936,
      "training_loss": 6.514617919921875
    },
    {
      "epoch": 0.4197289972899729,
      "step": 1936,
      "training_loss": 5.9985175132751465
    },
    {
      "epoch": 0.4197289972899729,
      "step": 1936,
      "training_loss": 7.462266445159912
    },
    {
      "epoch": 0.4197289972899729,
      "step": 1936,
      "training_loss": 6.4354248046875
    },
    {
      "epoch": 0.4199457994579946,
      "step": 1937,
      "training_loss": 7.105074882507324
    },
    {
      "epoch": 0.4199457994579946,
      "step": 1937,
      "training_loss": 5.28849458694458
    },
    {
      "epoch": 0.4199457994579946,
      "step": 1937,
      "training_loss": 5.935733795166016
    },
    {
      "epoch": 0.4199457994579946,
      "step": 1937,
      "training_loss": 6.176095962524414
    },
    {
      "epoch": 0.42016260162601626,
      "step": 1938,
      "training_loss": 6.336986541748047
    },
    {
      "epoch": 0.42016260162601626,
      "step": 1938,
      "training_loss": 7.340609073638916
    },
    {
      "epoch": 0.42016260162601626,
      "step": 1938,
      "training_loss": 7.326084136962891
    },
    {
      "epoch": 0.42016260162601626,
      "step": 1938,
      "training_loss": 7.490988254547119
    },
    {
      "epoch": 0.42037940379403793,
      "step": 1939,
      "training_loss": 3.4644198417663574
    },
    {
      "epoch": 0.42037940379403793,
      "step": 1939,
      "training_loss": 9.043461799621582
    },
    {
      "epoch": 0.42037940379403793,
      "step": 1939,
      "training_loss": 7.08278226852417
    },
    {
      "epoch": 0.42037940379403793,
      "step": 1939,
      "training_loss": 7.322987079620361
    },
    {
      "epoch": 0.4205962059620596,
      "grad_norm": 18.822376251220703,
      "learning_rate": 1e-05,
      "loss": 6.6453,
      "step": 1940
    },
    {
      "epoch": 0.4205962059620596,
      "step": 1940,
      "training_loss": 6.724848747253418
    },
    {
      "epoch": 0.4205962059620596,
      "step": 1940,
      "training_loss": 11.536581039428711
    },
    {
      "epoch": 0.4205962059620596,
      "step": 1940,
      "training_loss": 7.092392444610596
    },
    {
      "epoch": 0.4205962059620596,
      "step": 1940,
      "training_loss": 4.389467239379883
    },
    {
      "epoch": 0.4208130081300813,
      "step": 1941,
      "training_loss": 6.564652919769287
    },
    {
      "epoch": 0.4208130081300813,
      "step": 1941,
      "training_loss": 7.809585094451904
    },
    {
      "epoch": 0.4208130081300813,
      "step": 1941,
      "training_loss": 5.6182379722595215
    },
    {
      "epoch": 0.4208130081300813,
      "step": 1941,
      "training_loss": 6.705471038818359
    },
    {
      "epoch": 0.421029810298103,
      "step": 1942,
      "training_loss": 5.87739896774292
    },
    {
      "epoch": 0.421029810298103,
      "step": 1942,
      "training_loss": 7.138388156890869
    },
    {
      "epoch": 0.421029810298103,
      "step": 1942,
      "training_loss": 6.764799118041992
    },
    {
      "epoch": 0.421029810298103,
      "step": 1942,
      "training_loss": 5.941048622131348
    },
    {
      "epoch": 0.42124661246612466,
      "step": 1943,
      "training_loss": 6.280146598815918
    },
    {
      "epoch": 0.42124661246612466,
      "step": 1943,
      "training_loss": 6.332985877990723
    },
    {
      "epoch": 0.42124661246612466,
      "step": 1943,
      "training_loss": 6.7055230140686035
    },
    {
      "epoch": 0.42124661246612466,
      "step": 1943,
      "training_loss": 5.922226428985596
    },
    {
      "epoch": 0.4214634146341463,
      "grad_norm": 17.37688446044922,
      "learning_rate": 1e-05,
      "loss": 6.7127,
      "step": 1944
    },
    {
      "epoch": 0.4214634146341463,
      "step": 1944,
      "training_loss": 8.407937049865723
    },
    {
      "epoch": 0.4214634146341463,
      "step": 1944,
      "training_loss": 4.852139472961426
    },
    {
      "epoch": 0.4214634146341463,
      "step": 1944,
      "training_loss": 7.0811848640441895
    },
    {
      "epoch": 0.4214634146341463,
      "step": 1944,
      "training_loss": 7.314359664916992
    },
    {
      "epoch": 0.421680216802168,
      "step": 1945,
      "training_loss": 4.692690372467041
    },
    {
      "epoch": 0.421680216802168,
      "step": 1945,
      "training_loss": 6.077812194824219
    },
    {
      "epoch": 0.421680216802168,
      "step": 1945,
      "training_loss": 7.008935928344727
    },
    {
      "epoch": 0.421680216802168,
      "step": 1945,
      "training_loss": 5.971729755401611
    },
    {
      "epoch": 0.4218970189701897,
      "step": 1946,
      "training_loss": 5.714325904846191
    },
    {
      "epoch": 0.4218970189701897,
      "step": 1946,
      "training_loss": 6.823634624481201
    },
    {
      "epoch": 0.4218970189701897,
      "step": 1946,
      "training_loss": 7.644558906555176
    },
    {
      "epoch": 0.4218970189701897,
      "step": 1946,
      "training_loss": 4.855965614318848
    },
    {
      "epoch": 0.4221138211382114,
      "step": 1947,
      "training_loss": 6.121892929077148
    },
    {
      "epoch": 0.4221138211382114,
      "step": 1947,
      "training_loss": 5.778757095336914
    },
    {
      "epoch": 0.4221138211382114,
      "step": 1947,
      "training_loss": 5.400357723236084
    },
    {
      "epoch": 0.4221138211382114,
      "step": 1947,
      "training_loss": 6.085691452026367
    },
    {
      "epoch": 0.42233062330623306,
      "grad_norm": 15.029021263122559,
      "learning_rate": 1e-05,
      "loss": 6.2395,
      "step": 1948
    },
    {
      "epoch": 0.42233062330623306,
      "step": 1948,
      "training_loss": 7.27654504776001
    },
    {
      "epoch": 0.42233062330623306,
      "step": 1948,
      "training_loss": 8.10555362701416
    },
    {
      "epoch": 0.42233062330623306,
      "step": 1948,
      "training_loss": 6.675846576690674
    },
    {
      "epoch": 0.42233062330623306,
      "step": 1948,
      "training_loss": 7.166060447692871
    },
    {
      "epoch": 0.4225474254742547,
      "step": 1949,
      "training_loss": 7.8402533531188965
    },
    {
      "epoch": 0.4225474254742547,
      "step": 1949,
      "training_loss": 7.01231050491333
    },
    {
      "epoch": 0.4225474254742547,
      "step": 1949,
      "training_loss": 6.148483753204346
    },
    {
      "epoch": 0.4225474254742547,
      "step": 1949,
      "training_loss": 5.560249328613281
    },
    {
      "epoch": 0.42276422764227645,
      "step": 1950,
      "training_loss": 5.230250835418701
    },
    {
      "epoch": 0.42276422764227645,
      "step": 1950,
      "training_loss": 6.614917755126953
    },
    {
      "epoch": 0.42276422764227645,
      "step": 1950,
      "training_loss": 6.431891918182373
    },
    {
      "epoch": 0.42276422764227645,
      "step": 1950,
      "training_loss": 6.472931861877441
    },
    {
      "epoch": 0.4229810298102981,
      "step": 1951,
      "training_loss": 6.634889602661133
    },
    {
      "epoch": 0.4229810298102981,
      "step": 1951,
      "training_loss": 7.331399917602539
    },
    {
      "epoch": 0.4229810298102981,
      "step": 1951,
      "training_loss": 6.498500823974609
    },
    {
      "epoch": 0.4229810298102981,
      "step": 1951,
      "training_loss": 6.182773590087891
    },
    {
      "epoch": 0.4231978319783198,
      "grad_norm": 12.37807559967041,
      "learning_rate": 1e-05,
      "loss": 6.6989,
      "step": 1952
    },
    {
      "epoch": 0.4231978319783198,
      "step": 1952,
      "training_loss": 7.256240367889404
    },
    {
      "epoch": 0.4231978319783198,
      "step": 1952,
      "training_loss": 7.3637590408325195
    },
    {
      "epoch": 0.4231978319783198,
      "step": 1952,
      "training_loss": 9.320858001708984
    },
    {
      "epoch": 0.4231978319783198,
      "step": 1952,
      "training_loss": 3.2409069538116455
    },
    {
      "epoch": 0.42341463414634145,
      "step": 1953,
      "training_loss": 8.146780967712402
    },
    {
      "epoch": 0.42341463414634145,
      "step": 1953,
      "training_loss": 6.672832012176514
    },
    {
      "epoch": 0.42341463414634145,
      "step": 1953,
      "training_loss": 6.613819599151611
    },
    {
      "epoch": 0.42341463414634145,
      "step": 1953,
      "training_loss": 7.4319658279418945
    },
    {
      "epoch": 0.4236314363143631,
      "step": 1954,
      "training_loss": 7.27736759185791
    },
    {
      "epoch": 0.4236314363143631,
      "step": 1954,
      "training_loss": 7.537203788757324
    },
    {
      "epoch": 0.4236314363143631,
      "step": 1954,
      "training_loss": 6.173745155334473
    },
    {
      "epoch": 0.4236314363143631,
      "step": 1954,
      "training_loss": 3.8710646629333496
    },
    {
      "epoch": 0.42384823848238484,
      "step": 1955,
      "training_loss": 4.803616523742676
    },
    {
      "epoch": 0.42384823848238484,
      "step": 1955,
      "training_loss": 7.337883472442627
    },
    {
      "epoch": 0.42384823848238484,
      "step": 1955,
      "training_loss": 6.904659748077393
    },
    {
      "epoch": 0.42384823848238484,
      "step": 1955,
      "training_loss": 5.7524800300598145
    },
    {
      "epoch": 0.4240650406504065,
      "grad_norm": 12.378597259521484,
      "learning_rate": 1e-05,
      "loss": 6.6066,
      "step": 1956
    },
    {
      "epoch": 0.4240650406504065,
      "step": 1956,
      "training_loss": 6.824871063232422
    },
    {
      "epoch": 0.4240650406504065,
      "step": 1956,
      "training_loss": 3.9460041522979736
    },
    {
      "epoch": 0.4240650406504065,
      "step": 1956,
      "training_loss": 5.537207126617432
    },
    {
      "epoch": 0.4240650406504065,
      "step": 1956,
      "training_loss": 7.309979438781738
    },
    {
      "epoch": 0.4242818428184282,
      "step": 1957,
      "training_loss": 6.657268047332764
    },
    {
      "epoch": 0.4242818428184282,
      "step": 1957,
      "training_loss": 4.642824172973633
    },
    {
      "epoch": 0.4242818428184282,
      "step": 1957,
      "training_loss": 6.325021266937256
    },
    {
      "epoch": 0.4242818428184282,
      "step": 1957,
      "training_loss": 6.565391540527344
    },
    {
      "epoch": 0.42449864498644985,
      "step": 1958,
      "training_loss": 4.58551549911499
    },
    {
      "epoch": 0.42449864498644985,
      "step": 1958,
      "training_loss": 5.770686626434326
    },
    {
      "epoch": 0.42449864498644985,
      "step": 1958,
      "training_loss": 6.639513969421387
    },
    {
      "epoch": 0.42449864498644985,
      "step": 1958,
      "training_loss": 7.19669246673584
    },
    {
      "epoch": 0.42471544715447157,
      "step": 1959,
      "training_loss": 6.279959201812744
    },
    {
      "epoch": 0.42471544715447157,
      "step": 1959,
      "training_loss": 6.555194854736328
    },
    {
      "epoch": 0.42471544715447157,
      "step": 1959,
      "training_loss": 5.855679988861084
    },
    {
      "epoch": 0.42471544715447157,
      "step": 1959,
      "training_loss": 7.577493667602539
    },
    {
      "epoch": 0.42493224932249324,
      "grad_norm": 16.885805130004883,
      "learning_rate": 1e-05,
      "loss": 6.1418,
      "step": 1960
    },
    {
      "epoch": 0.42493224932249324,
      "step": 1960,
      "training_loss": 5.782657146453857
    },
    {
      "epoch": 0.42493224932249324,
      "step": 1960,
      "training_loss": 6.152173042297363
    },
    {
      "epoch": 0.42493224932249324,
      "step": 1960,
      "training_loss": 5.916916370391846
    },
    {
      "epoch": 0.42493224932249324,
      "step": 1960,
      "training_loss": 5.991964340209961
    },
    {
      "epoch": 0.4251490514905149,
      "step": 1961,
      "training_loss": 4.301333427429199
    },
    {
      "epoch": 0.4251490514905149,
      "step": 1961,
      "training_loss": 6.263515472412109
    },
    {
      "epoch": 0.4251490514905149,
      "step": 1961,
      "training_loss": 6.867367744445801
    },
    {
      "epoch": 0.4251490514905149,
      "step": 1961,
      "training_loss": 9.138154029846191
    },
    {
      "epoch": 0.4253658536585366,
      "step": 1962,
      "training_loss": 6.3052825927734375
    },
    {
      "epoch": 0.4253658536585366,
      "step": 1962,
      "training_loss": 6.612076282501221
    },
    {
      "epoch": 0.4253658536585366,
      "step": 1962,
      "training_loss": 6.660966873168945
    },
    {
      "epoch": 0.4253658536585366,
      "step": 1962,
      "training_loss": 7.4342827796936035
    },
    {
      "epoch": 0.42558265582655824,
      "step": 1963,
      "training_loss": 4.342331886291504
    },
    {
      "epoch": 0.42558265582655824,
      "step": 1963,
      "training_loss": 6.943949222564697
    },
    {
      "epoch": 0.42558265582655824,
      "step": 1963,
      "training_loss": 6.996601104736328
    },
    {
      "epoch": 0.42558265582655824,
      "step": 1963,
      "training_loss": 4.929022789001465
    },
    {
      "epoch": 0.42579945799457997,
      "grad_norm": 15.291763305664062,
      "learning_rate": 1e-05,
      "loss": 6.2899,
      "step": 1964
    },
    {
      "epoch": 0.42579945799457997,
      "step": 1964,
      "training_loss": 5.432398796081543
    },
    {
      "epoch": 0.42579945799457997,
      "step": 1964,
      "training_loss": 4.837802886962891
    },
    {
      "epoch": 0.42579945799457997,
      "step": 1964,
      "training_loss": 7.472695350646973
    },
    {
      "epoch": 0.42579945799457997,
      "step": 1964,
      "training_loss": 6.869626045227051
    },
    {
      "epoch": 0.42601626016260163,
      "step": 1965,
      "training_loss": 6.394474983215332
    },
    {
      "epoch": 0.42601626016260163,
      "step": 1965,
      "training_loss": 4.779491424560547
    },
    {
      "epoch": 0.42601626016260163,
      "step": 1965,
      "training_loss": 6.334591388702393
    },
    {
      "epoch": 0.42601626016260163,
      "step": 1965,
      "training_loss": 7.307385444641113
    },
    {
      "epoch": 0.4262330623306233,
      "step": 1966,
      "training_loss": 6.646592617034912
    },
    {
      "epoch": 0.4262330623306233,
      "step": 1966,
      "training_loss": 4.617787837982178
    },
    {
      "epoch": 0.4262330623306233,
      "step": 1966,
      "training_loss": 6.378553867340088
    },
    {
      "epoch": 0.4262330623306233,
      "step": 1966,
      "training_loss": 7.837917327880859
    },
    {
      "epoch": 0.42644986449864497,
      "step": 1967,
      "training_loss": 6.384069919586182
    },
    {
      "epoch": 0.42644986449864497,
      "step": 1967,
      "training_loss": 5.928412437438965
    },
    {
      "epoch": 0.42644986449864497,
      "step": 1967,
      "training_loss": 6.7939558029174805
    },
    {
      "epoch": 0.42644986449864497,
      "step": 1967,
      "training_loss": 7.712777614593506
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 12.748549461364746,
      "learning_rate": 1e-05,
      "loss": 6.358,
      "step": 1968
    },
    {
      "epoch": 0.4266666666666667,
      "step": 1968,
      "training_loss": 7.894888401031494
    },
    {
      "epoch": 0.4266666666666667,
      "step": 1968,
      "training_loss": 7.354206085205078
    },
    {
      "epoch": 0.4266666666666667,
      "step": 1968,
      "training_loss": 5.751587390899658
    },
    {
      "epoch": 0.4266666666666667,
      "step": 1968,
      "training_loss": 3.4808273315429688
    },
    {
      "epoch": 0.42688346883468836,
      "step": 1969,
      "training_loss": 6.993139743804932
    },
    {
      "epoch": 0.42688346883468836,
      "step": 1969,
      "training_loss": 6.9302825927734375
    },
    {
      "epoch": 0.42688346883468836,
      "step": 1969,
      "training_loss": 6.6046142578125
    },
    {
      "epoch": 0.42688346883468836,
      "step": 1969,
      "training_loss": 5.936968803405762
    },
    {
      "epoch": 0.42710027100271003,
      "step": 1970,
      "training_loss": 4.688375473022461
    },
    {
      "epoch": 0.42710027100271003,
      "step": 1970,
      "training_loss": 6.691744804382324
    },
    {
      "epoch": 0.42710027100271003,
      "step": 1970,
      "training_loss": 6.599416732788086
    },
    {
      "epoch": 0.42710027100271003,
      "step": 1970,
      "training_loss": 6.772994518280029
    },
    {
      "epoch": 0.4273170731707317,
      "step": 1971,
      "training_loss": 4.97035026550293
    },
    {
      "epoch": 0.4273170731707317,
      "step": 1971,
      "training_loss": 4.836235046386719
    },
    {
      "epoch": 0.4273170731707317,
      "step": 1971,
      "training_loss": 5.8309125900268555
    },
    {
      "epoch": 0.4273170731707317,
      "step": 1971,
      "training_loss": 6.999247074127197
    },
    {
      "epoch": 0.42753387533875337,
      "grad_norm": 23.948205947875977,
      "learning_rate": 1e-05,
      "loss": 6.146,
      "step": 1972
    },
    {
      "epoch": 0.42753387533875337,
      "step": 1972,
      "training_loss": 7.407402515411377
    },
    {
      "epoch": 0.42753387533875337,
      "step": 1972,
      "training_loss": 7.521298885345459
    },
    {
      "epoch": 0.42753387533875337,
      "step": 1972,
      "training_loss": 4.454235076904297
    },
    {
      "epoch": 0.42753387533875337,
      "step": 1972,
      "training_loss": 3.561450242996216
    },
    {
      "epoch": 0.4277506775067751,
      "step": 1973,
      "training_loss": 6.479554653167725
    },
    {
      "epoch": 0.4277506775067751,
      "step": 1973,
      "training_loss": 6.9445905685424805
    },
    {
      "epoch": 0.4277506775067751,
      "step": 1973,
      "training_loss": 7.201164722442627
    },
    {
      "epoch": 0.4277506775067751,
      "step": 1973,
      "training_loss": 5.8318586349487305
    },
    {
      "epoch": 0.42796747967479676,
      "step": 1974,
      "training_loss": 7.483212947845459
    },
    {
      "epoch": 0.42796747967479676,
      "step": 1974,
      "training_loss": 6.234999179840088
    },
    {
      "epoch": 0.42796747967479676,
      "step": 1974,
      "training_loss": 6.0608296394348145
    },
    {
      "epoch": 0.42796747967479676,
      "step": 1974,
      "training_loss": 6.320125102996826
    },
    {
      "epoch": 0.4281842818428184,
      "step": 1975,
      "training_loss": 7.421614170074463
    },
    {
      "epoch": 0.4281842818428184,
      "step": 1975,
      "training_loss": 6.764812469482422
    },
    {
      "epoch": 0.4281842818428184,
      "step": 1975,
      "training_loss": 6.869360446929932
    },
    {
      "epoch": 0.4281842818428184,
      "step": 1975,
      "training_loss": 6.20742130279541
    },
    {
      "epoch": 0.4284010840108401,
      "grad_norm": 17.5069522857666,
      "learning_rate": 1e-05,
      "loss": 6.4227,
      "step": 1976
    },
    {
      "epoch": 0.4284010840108401,
      "step": 1976,
      "training_loss": 7.04453706741333
    },
    {
      "epoch": 0.4284010840108401,
      "step": 1976,
      "training_loss": 6.990075588226318
    },
    {
      "epoch": 0.4284010840108401,
      "step": 1976,
      "training_loss": 5.101840496063232
    },
    {
      "epoch": 0.4284010840108401,
      "step": 1976,
      "training_loss": 5.449841499328613
    },
    {
      "epoch": 0.42861788617886176,
      "step": 1977,
      "training_loss": 6.660983085632324
    },
    {
      "epoch": 0.42861788617886176,
      "step": 1977,
      "training_loss": 7.277291297912598
    },
    {
      "epoch": 0.42861788617886176,
      "step": 1977,
      "training_loss": 7.829600811004639
    },
    {
      "epoch": 0.42861788617886176,
      "step": 1977,
      "training_loss": 4.266043186187744
    },
    {
      "epoch": 0.4288346883468835,
      "step": 1978,
      "training_loss": 7.103231906890869
    },
    {
      "epoch": 0.4288346883468835,
      "step": 1978,
      "training_loss": 7.4383440017700195
    },
    {
      "epoch": 0.4288346883468835,
      "step": 1978,
      "training_loss": 6.214207649230957
    },
    {
      "epoch": 0.4288346883468835,
      "step": 1978,
      "training_loss": 5.008162975311279
    },
    {
      "epoch": 0.42905149051490515,
      "step": 1979,
      "training_loss": 6.82331657409668
    },
    {
      "epoch": 0.42905149051490515,
      "step": 1979,
      "training_loss": 6.689362525939941
    },
    {
      "epoch": 0.42905149051490515,
      "step": 1979,
      "training_loss": 7.263024806976318
    },
    {
      "epoch": 0.42905149051490515,
      "step": 1979,
      "training_loss": 5.467463970184326
    },
    {
      "epoch": 0.4292682926829268,
      "grad_norm": 16.0744571685791,
      "learning_rate": 1e-05,
      "loss": 6.4142,
      "step": 1980
    },
    {
      "epoch": 0.4292682926829268,
      "step": 1980,
      "training_loss": 7.185141563415527
    },
    {
      "epoch": 0.4292682926829268,
      "step": 1980,
      "training_loss": 7.8654608726501465
    },
    {
      "epoch": 0.4292682926829268,
      "step": 1980,
      "training_loss": 6.913125991821289
    },
    {
      "epoch": 0.4292682926829268,
      "step": 1980,
      "training_loss": 4.135911464691162
    },
    {
      "epoch": 0.4294850948509485,
      "step": 1981,
      "training_loss": 6.287231922149658
    },
    {
      "epoch": 0.4294850948509485,
      "step": 1981,
      "training_loss": 6.5320143699646
    },
    {
      "epoch": 0.4294850948509485,
      "step": 1981,
      "training_loss": 6.23534631729126
    },
    {
      "epoch": 0.4294850948509485,
      "step": 1981,
      "training_loss": 7.685181140899658
    },
    {
      "epoch": 0.4297018970189702,
      "step": 1982,
      "training_loss": 5.904992580413818
    },
    {
      "epoch": 0.4297018970189702,
      "step": 1982,
      "training_loss": 7.103508472442627
    },
    {
      "epoch": 0.4297018970189702,
      "step": 1982,
      "training_loss": 5.650876998901367
    },
    {
      "epoch": 0.4297018970189702,
      "step": 1982,
      "training_loss": 7.529867649078369
    },
    {
      "epoch": 0.4299186991869919,
      "step": 1983,
      "training_loss": 6.2740936279296875
    },
    {
      "epoch": 0.4299186991869919,
      "step": 1983,
      "training_loss": 7.483299255371094
    },
    {
      "epoch": 0.4299186991869919,
      "step": 1983,
      "training_loss": 6.765726089477539
    },
    {
      "epoch": 0.4299186991869919,
      "step": 1983,
      "training_loss": 6.724885940551758
    },
    {
      "epoch": 0.43013550135501355,
      "grad_norm": 16.370803833007812,
      "learning_rate": 1e-05,
      "loss": 6.6423,
      "step": 1984
    },
    {
      "epoch": 0.43013550135501355,
      "step": 1984,
      "training_loss": 7.326430320739746
    },
    {
      "epoch": 0.43013550135501355,
      "step": 1984,
      "training_loss": 7.672976493835449
    },
    {
      "epoch": 0.43013550135501355,
      "step": 1984,
      "training_loss": 5.9467973709106445
    },
    {
      "epoch": 0.43013550135501355,
      "step": 1984,
      "training_loss": 7.160945892333984
    },
    {
      "epoch": 0.4303523035230352,
      "step": 1985,
      "training_loss": 6.686909198760986
    },
    {
      "epoch": 0.4303523035230352,
      "step": 1985,
      "training_loss": 5.214825630187988
    },
    {
      "epoch": 0.4303523035230352,
      "step": 1985,
      "training_loss": 7.625732421875
    },
    {
      "epoch": 0.4303523035230352,
      "step": 1985,
      "training_loss": 6.814837455749512
    },
    {
      "epoch": 0.4305691056910569,
      "step": 1986,
      "training_loss": 6.409984588623047
    },
    {
      "epoch": 0.4305691056910569,
      "step": 1986,
      "training_loss": 5.263878345489502
    },
    {
      "epoch": 0.4305691056910569,
      "step": 1986,
      "training_loss": 5.464653491973877
    },
    {
      "epoch": 0.4305691056910569,
      "step": 1986,
      "training_loss": 5.621829509735107
    },
    {
      "epoch": 0.4307859078590786,
      "step": 1987,
      "training_loss": 6.860688209533691
    },
    {
      "epoch": 0.4307859078590786,
      "step": 1987,
      "training_loss": 6.5796895027160645
    },
    {
      "epoch": 0.4307859078590786,
      "step": 1987,
      "training_loss": 7.116315841674805
    },
    {
      "epoch": 0.4307859078590786,
      "step": 1987,
      "training_loss": 7.158393383026123
    },
    {
      "epoch": 0.4310027100271003,
      "grad_norm": 11.155780792236328,
      "learning_rate": 1e-05,
      "loss": 6.5578,
      "step": 1988
    },
    {
      "epoch": 0.4310027100271003,
      "step": 1988,
      "training_loss": 6.538125038146973
    },
    {
      "epoch": 0.4310027100271003,
      "step": 1988,
      "training_loss": 5.751912593841553
    },
    {
      "epoch": 0.4310027100271003,
      "step": 1988,
      "training_loss": 7.939618110656738
    },
    {
      "epoch": 0.4310027100271003,
      "step": 1988,
      "training_loss": 7.331798076629639
    },
    {
      "epoch": 0.43121951219512195,
      "step": 1989,
      "training_loss": 8.832826614379883
    },
    {
      "epoch": 0.43121951219512195,
      "step": 1989,
      "training_loss": 8.473052024841309
    },
    {
      "epoch": 0.43121951219512195,
      "step": 1989,
      "training_loss": 6.156797885894775
    },
    {
      "epoch": 0.43121951219512195,
      "step": 1989,
      "training_loss": 5.646363735198975
    },
    {
      "epoch": 0.4314363143631436,
      "step": 1990,
      "training_loss": 7.1154890060424805
    },
    {
      "epoch": 0.4314363143631436,
      "step": 1990,
      "training_loss": 5.673190116882324
    },
    {
      "epoch": 0.4314363143631436,
      "step": 1990,
      "training_loss": 6.161099433898926
    },
    {
      "epoch": 0.4314363143631436,
      "step": 1990,
      "training_loss": 4.793049335479736
    },
    {
      "epoch": 0.43165311653116534,
      "step": 1991,
      "training_loss": 7.044740200042725
    },
    {
      "epoch": 0.43165311653116534,
      "step": 1991,
      "training_loss": 5.889859199523926
    },
    {
      "epoch": 0.43165311653116534,
      "step": 1991,
      "training_loss": 6.816483974456787
    },
    {
      "epoch": 0.43165311653116534,
      "step": 1991,
      "training_loss": 5.590874195098877
    },
    {
      "epoch": 0.431869918699187,
      "grad_norm": 26.718626022338867,
      "learning_rate": 1e-05,
      "loss": 6.6097,
      "step": 1992
    },
    {
      "epoch": 0.431869918699187,
      "step": 1992,
      "training_loss": 4.922537326812744
    },
    {
      "epoch": 0.431869918699187,
      "step": 1992,
      "training_loss": 7.058611869812012
    },
    {
      "epoch": 0.431869918699187,
      "step": 1992,
      "training_loss": 6.2175984382629395
    },
    {
      "epoch": 0.431869918699187,
      "step": 1992,
      "training_loss": 6.579796314239502
    },
    {
      "epoch": 0.4320867208672087,
      "step": 1993,
      "training_loss": 5.812890529632568
    },
    {
      "epoch": 0.4320867208672087,
      "step": 1993,
      "training_loss": 5.561485290527344
    },
    {
      "epoch": 0.4320867208672087,
      "step": 1993,
      "training_loss": 7.858956336975098
    },
    {
      "epoch": 0.4320867208672087,
      "step": 1993,
      "training_loss": 4.441977024078369
    },
    {
      "epoch": 0.43230352303523034,
      "step": 1994,
      "training_loss": 6.910668849945068
    },
    {
      "epoch": 0.43230352303523034,
      "step": 1994,
      "training_loss": 6.730808734893799
    },
    {
      "epoch": 0.43230352303523034,
      "step": 1994,
      "training_loss": 5.412820816040039
    },
    {
      "epoch": 0.43230352303523034,
      "step": 1994,
      "training_loss": 3.5125961303710938
    },
    {
      "epoch": 0.432520325203252,
      "step": 1995,
      "training_loss": 6.734462738037109
    },
    {
      "epoch": 0.432520325203252,
      "step": 1995,
      "training_loss": 2.96783447265625
    },
    {
      "epoch": 0.432520325203252,
      "step": 1995,
      "training_loss": 6.888129234313965
    },
    {
      "epoch": 0.432520325203252,
      "step": 1995,
      "training_loss": 6.180241107940674
    },
    {
      "epoch": 0.43273712737127373,
      "grad_norm": 16.2698917388916,
      "learning_rate": 1e-05,
      "loss": 5.862,
      "step": 1996
    },
    {
      "epoch": 0.43273712737127373,
      "step": 1996,
      "training_loss": 7.639695644378662
    },
    {
      "epoch": 0.43273712737127373,
      "step": 1996,
      "training_loss": 7.573399066925049
    },
    {
      "epoch": 0.43273712737127373,
      "step": 1996,
      "training_loss": 7.505317211151123
    },
    {
      "epoch": 0.43273712737127373,
      "step": 1996,
      "training_loss": 7.087536811828613
    },
    {
      "epoch": 0.4329539295392954,
      "step": 1997,
      "training_loss": 7.375400543212891
    },
    {
      "epoch": 0.4329539295392954,
      "step": 1997,
      "training_loss": 6.345117092132568
    },
    {
      "epoch": 0.4329539295392954,
      "step": 1997,
      "training_loss": 5.6860175132751465
    },
    {
      "epoch": 0.4329539295392954,
      "step": 1997,
      "training_loss": 7.131241798400879
    },
    {
      "epoch": 0.43317073170731707,
      "step": 1998,
      "training_loss": 7.395174980163574
    },
    {
      "epoch": 0.43317073170731707,
      "step": 1998,
      "training_loss": 7.540004730224609
    },
    {
      "epoch": 0.43317073170731707,
      "step": 1998,
      "training_loss": 4.728414058685303
    },
    {
      "epoch": 0.43317073170731707,
      "step": 1998,
      "training_loss": 6.532900333404541
    },
    {
      "epoch": 0.43338753387533874,
      "step": 1999,
      "training_loss": 9.057669639587402
    },
    {
      "epoch": 0.43338753387533874,
      "step": 1999,
      "training_loss": 7.304220199584961
    },
    {
      "epoch": 0.43338753387533874,
      "step": 1999,
      "training_loss": 7.485139846801758
    },
    {
      "epoch": 0.43338753387533874,
      "step": 1999,
      "training_loss": 5.018863677978516
    },
    {
      "epoch": 0.43360433604336046,
      "grad_norm": 27.609649658203125,
      "learning_rate": 1e-05,
      "loss": 6.9629,
      "step": 2000
    },
    {
      "epoch": 0.43360433604336046,
      "step": 2000,
      "training_loss": 5.081393241882324
    },
    {
      "epoch": 0.43360433604336046,
      "step": 2000,
      "training_loss": 6.119755268096924
    },
    {
      "epoch": 0.43360433604336046,
      "step": 2000,
      "training_loss": 5.554696559906006
    },
    {
      "epoch": 0.43360433604336046,
      "step": 2000,
      "training_loss": 6.929450511932373
    },
    {
      "epoch": 0.43382113821138213,
      "step": 2001,
      "training_loss": 6.786854267120361
    },
    {
      "epoch": 0.43382113821138213,
      "step": 2001,
      "training_loss": 8.319547653198242
    },
    {
      "epoch": 0.43382113821138213,
      "step": 2001,
      "training_loss": 5.717026233673096
    },
    {
      "epoch": 0.43382113821138213,
      "step": 2001,
      "training_loss": 5.312775611877441
    },
    {
      "epoch": 0.4340379403794038,
      "step": 2002,
      "training_loss": 6.2617506980896
    },
    {
      "epoch": 0.4340379403794038,
      "step": 2002,
      "training_loss": 6.186056137084961
    },
    {
      "epoch": 0.4340379403794038,
      "step": 2002,
      "training_loss": 6.547546863555908
    },
    {
      "epoch": 0.4340379403794038,
      "step": 2002,
      "training_loss": 7.167089939117432
    },
    {
      "epoch": 0.43425474254742547,
      "step": 2003,
      "training_loss": 6.965847015380859
    },
    {
      "epoch": 0.43425474254742547,
      "step": 2003,
      "training_loss": 6.342546463012695
    },
    {
      "epoch": 0.43425474254742547,
      "step": 2003,
      "training_loss": 6.642926216125488
    },
    {
      "epoch": 0.43425474254742547,
      "step": 2003,
      "training_loss": 6.734688758850098
    },
    {
      "epoch": 0.43447154471544713,
      "grad_norm": 17.22683334350586,
      "learning_rate": 1e-05,
      "loss": 6.4169,
      "step": 2004
    },
    {
      "epoch": 0.43447154471544713,
      "step": 2004,
      "training_loss": 7.043473720550537
    },
    {
      "epoch": 0.43447154471544713,
      "step": 2004,
      "training_loss": 6.541811943054199
    },
    {
      "epoch": 0.43447154471544713,
      "step": 2004,
      "training_loss": 7.279214859008789
    },
    {
      "epoch": 0.43447154471544713,
      "step": 2004,
      "training_loss": 8.363178253173828
    },
    {
      "epoch": 0.43468834688346886,
      "step": 2005,
      "training_loss": 7.502565383911133
    },
    {
      "epoch": 0.43468834688346886,
      "step": 2005,
      "training_loss": 3.902390241622925
    },
    {
      "epoch": 0.43468834688346886,
      "step": 2005,
      "training_loss": 6.440542697906494
    },
    {
      "epoch": 0.43468834688346886,
      "step": 2005,
      "training_loss": 6.444117069244385
    },
    {
      "epoch": 0.4349051490514905,
      "step": 2006,
      "training_loss": 4.701027870178223
    },
    {
      "epoch": 0.4349051490514905,
      "step": 2006,
      "training_loss": 3.793062686920166
    },
    {
      "epoch": 0.4349051490514905,
      "step": 2006,
      "training_loss": 7.3200297355651855
    },
    {
      "epoch": 0.4349051490514905,
      "step": 2006,
      "training_loss": 5.99482536315918
    },
    {
      "epoch": 0.4351219512195122,
      "step": 2007,
      "training_loss": 7.1640706062316895
    },
    {
      "epoch": 0.4351219512195122,
      "step": 2007,
      "training_loss": 7.079514503479004
    },
    {
      "epoch": 0.4351219512195122,
      "step": 2007,
      "training_loss": 5.275717735290527
    },
    {
      "epoch": 0.4351219512195122,
      "step": 2007,
      "training_loss": 7.163722515106201
    },
    {
      "epoch": 0.43533875338753386,
      "grad_norm": 20.348031997680664,
      "learning_rate": 1e-05,
      "loss": 6.3756,
      "step": 2008
    },
    {
      "epoch": 0.43533875338753386,
      "step": 2008,
      "training_loss": 7.459742069244385
    },
    {
      "epoch": 0.43533875338753386,
      "step": 2008,
      "training_loss": 4.453056812286377
    },
    {
      "epoch": 0.43533875338753386,
      "step": 2008,
      "training_loss": 6.021087646484375
    },
    {
      "epoch": 0.43533875338753386,
      "step": 2008,
      "training_loss": 7.707250118255615
    },
    {
      "epoch": 0.43555555555555553,
      "step": 2009,
      "training_loss": 6.985107421875
    },
    {
      "epoch": 0.43555555555555553,
      "step": 2009,
      "training_loss": 5.279170036315918
    },
    {
      "epoch": 0.43555555555555553,
      "step": 2009,
      "training_loss": 8.18753433227539
    },
    {
      "epoch": 0.43555555555555553,
      "step": 2009,
      "training_loss": 5.705732822418213
    },
    {
      "epoch": 0.43577235772357725,
      "step": 2010,
      "training_loss": 7.268345832824707
    },
    {
      "epoch": 0.43577235772357725,
      "step": 2010,
      "training_loss": 7.367259979248047
    },
    {
      "epoch": 0.43577235772357725,
      "step": 2010,
      "training_loss": 6.387740135192871
    },
    {
      "epoch": 0.43577235772357725,
      "step": 2010,
      "training_loss": 6.45619535446167
    },
    {
      "epoch": 0.4359891598915989,
      "step": 2011,
      "training_loss": 7.828014850616455
    },
    {
      "epoch": 0.4359891598915989,
      "step": 2011,
      "training_loss": 6.4102463722229
    },
    {
      "epoch": 0.4359891598915989,
      "step": 2011,
      "training_loss": 5.412553787231445
    },
    {
      "epoch": 0.4359891598915989,
      "step": 2011,
      "training_loss": 5.192622184753418
    },
    {
      "epoch": 0.4362059620596206,
      "grad_norm": 16.713260650634766,
      "learning_rate": 1e-05,
      "loss": 6.5076,
      "step": 2012
    },
    {
      "epoch": 0.4362059620596206,
      "step": 2012,
      "training_loss": 6.589184761047363
    },
    {
      "epoch": 0.4362059620596206,
      "step": 2012,
      "training_loss": 7.0273919105529785
    },
    {
      "epoch": 0.4362059620596206,
      "step": 2012,
      "training_loss": 6.369409084320068
    },
    {
      "epoch": 0.4362059620596206,
      "step": 2012,
      "training_loss": 5.4986491203308105
    },
    {
      "epoch": 0.43642276422764226,
      "step": 2013,
      "training_loss": 6.925865173339844
    },
    {
      "epoch": 0.43642276422764226,
      "step": 2013,
      "training_loss": 6.274716854095459
    },
    {
      "epoch": 0.43642276422764226,
      "step": 2013,
      "training_loss": 6.69683837890625
    },
    {
      "epoch": 0.43642276422764226,
      "step": 2013,
      "training_loss": 5.682301998138428
    },
    {
      "epoch": 0.436639566395664,
      "step": 2014,
      "training_loss": 6.092219829559326
    },
    {
      "epoch": 0.436639566395664,
      "step": 2014,
      "training_loss": 7.172915458679199
    },
    {
      "epoch": 0.436639566395664,
      "step": 2014,
      "training_loss": 7.678661823272705
    },
    {
      "epoch": 0.436639566395664,
      "step": 2014,
      "training_loss": 6.971216201782227
    },
    {
      "epoch": 0.43685636856368565,
      "step": 2015,
      "training_loss": 3.616525888442993
    },
    {
      "epoch": 0.43685636856368565,
      "step": 2015,
      "training_loss": 6.748923301696777
    },
    {
      "epoch": 0.43685636856368565,
      "step": 2015,
      "training_loss": 5.856109619140625
    },
    {
      "epoch": 0.43685636856368565,
      "step": 2015,
      "training_loss": 6.8771071434021
    },
    {
      "epoch": 0.4370731707317073,
      "grad_norm": 12.225285530090332,
      "learning_rate": 1e-05,
      "loss": 6.3799,
      "step": 2016
    },
    {
      "epoch": 0.4370731707317073,
      "step": 2016,
      "training_loss": 4.849241256713867
    },
    {
      "epoch": 0.4370731707317073,
      "step": 2016,
      "training_loss": 7.209533214569092
    },
    {
      "epoch": 0.4370731707317073,
      "step": 2016,
      "training_loss": 6.651932239532471
    },
    {
      "epoch": 0.4370731707317073,
      "step": 2016,
      "training_loss": 6.890330791473389
    },
    {
      "epoch": 0.437289972899729,
      "step": 2017,
      "training_loss": 5.331846237182617
    },
    {
      "epoch": 0.437289972899729,
      "step": 2017,
      "training_loss": 4.690274715423584
    },
    {
      "epoch": 0.437289972899729,
      "step": 2017,
      "training_loss": 6.6522955894470215
    },
    {
      "epoch": 0.437289972899729,
      "step": 2017,
      "training_loss": 6.994718551635742
    },
    {
      "epoch": 0.43750677506775065,
      "step": 2018,
      "training_loss": 7.365340709686279
    },
    {
      "epoch": 0.43750677506775065,
      "step": 2018,
      "training_loss": 6.121066570281982
    },
    {
      "epoch": 0.43750677506775065,
      "step": 2018,
      "training_loss": 4.641022205352783
    },
    {
      "epoch": 0.43750677506775065,
      "step": 2018,
      "training_loss": 8.344267845153809
    },
    {
      "epoch": 0.4377235772357724,
      "step": 2019,
      "training_loss": 4.453423976898193
    },
    {
      "epoch": 0.4377235772357724,
      "step": 2019,
      "training_loss": 5.879279613494873
    },
    {
      "epoch": 0.4377235772357724,
      "step": 2019,
      "training_loss": 7.133804798126221
    },
    {
      "epoch": 0.4377235772357724,
      "step": 2019,
      "training_loss": 6.120062351226807
    },
    {
      "epoch": 0.43794037940379404,
      "grad_norm": 14.968165397644043,
      "learning_rate": 1e-05,
      "loss": 6.208,
      "step": 2020
    },
    {
      "epoch": 0.43794037940379404,
      "step": 2020,
      "training_loss": 4.958009719848633
    },
    {
      "epoch": 0.43794037940379404,
      "step": 2020,
      "training_loss": 6.130365371704102
    },
    {
      "epoch": 0.43794037940379404,
      "step": 2020,
      "training_loss": 6.16943883895874
    },
    {
      "epoch": 0.43794037940379404,
      "step": 2020,
      "training_loss": 6.894900798797607
    },
    {
      "epoch": 0.4381571815718157,
      "step": 2021,
      "training_loss": 6.71715784072876
    },
    {
      "epoch": 0.4381571815718157,
      "step": 2021,
      "training_loss": 7.392475128173828
    },
    {
      "epoch": 0.4381571815718157,
      "step": 2021,
      "training_loss": 7.218275547027588
    },
    {
      "epoch": 0.4381571815718157,
      "step": 2021,
      "training_loss": 7.902514457702637
    },
    {
      "epoch": 0.4383739837398374,
      "step": 2022,
      "training_loss": 4.291882038116455
    },
    {
      "epoch": 0.4383739837398374,
      "step": 2022,
      "training_loss": 7.331747055053711
    },
    {
      "epoch": 0.4383739837398374,
      "step": 2022,
      "training_loss": 6.543973445892334
    },
    {
      "epoch": 0.4383739837398374,
      "step": 2022,
      "training_loss": 6.9086127281188965
    },
    {
      "epoch": 0.4385907859078591,
      "step": 2023,
      "training_loss": 7.350157260894775
    },
    {
      "epoch": 0.4385907859078591,
      "step": 2023,
      "training_loss": 5.898954391479492
    },
    {
      "epoch": 0.4385907859078591,
      "step": 2023,
      "training_loss": 6.269144535064697
    },
    {
      "epoch": 0.4385907859078591,
      "step": 2023,
      "training_loss": 4.318796634674072
    },
    {
      "epoch": 0.4388075880758808,
      "grad_norm": 16.16013526916504,
      "learning_rate": 1e-05,
      "loss": 6.3935,
      "step": 2024
    },
    {
      "epoch": 0.4388075880758808,
      "step": 2024,
      "training_loss": 7.384621620178223
    },
    {
      "epoch": 0.4388075880758808,
      "step": 2024,
      "training_loss": 5.271022319793701
    },
    {
      "epoch": 0.4388075880758808,
      "step": 2024,
      "training_loss": 7.08871603012085
    },
    {
      "epoch": 0.4388075880758808,
      "step": 2024,
      "training_loss": 8.240740776062012
    },
    {
      "epoch": 0.43902439024390244,
      "step": 2025,
      "training_loss": 5.098527431488037
    },
    {
      "epoch": 0.43902439024390244,
      "step": 2025,
      "training_loss": 6.618521690368652
    },
    {
      "epoch": 0.43902439024390244,
      "step": 2025,
      "training_loss": 8.036516189575195
    },
    {
      "epoch": 0.43902439024390244,
      "step": 2025,
      "training_loss": 6.475197792053223
    },
    {
      "epoch": 0.4392411924119241,
      "step": 2026,
      "training_loss": 6.524682521820068
    },
    {
      "epoch": 0.4392411924119241,
      "step": 2026,
      "training_loss": 7.058011054992676
    },
    {
      "epoch": 0.4392411924119241,
      "step": 2026,
      "training_loss": 8.173125267028809
    },
    {
      "epoch": 0.4392411924119241,
      "step": 2026,
      "training_loss": 6.307259559631348
    },
    {
      "epoch": 0.4394579945799458,
      "step": 2027,
      "training_loss": 7.760855197906494
    },
    {
      "epoch": 0.4394579945799458,
      "step": 2027,
      "training_loss": 5.166954517364502
    },
    {
      "epoch": 0.4394579945799458,
      "step": 2027,
      "training_loss": 6.569044589996338
    },
    {
      "epoch": 0.4394579945799458,
      "step": 2027,
      "training_loss": 5.31549596786499
    },
    {
      "epoch": 0.4396747967479675,
      "grad_norm": 17.236154556274414,
      "learning_rate": 1e-05,
      "loss": 6.6931,
      "step": 2028
    },
    {
      "epoch": 0.4396747967479675,
      "step": 2028,
      "training_loss": 5.80032205581665
    },
    {
      "epoch": 0.4396747967479675,
      "step": 2028,
      "training_loss": 6.855379581451416
    },
    {
      "epoch": 0.4396747967479675,
      "step": 2028,
      "training_loss": 7.0244855880737305
    },
    {
      "epoch": 0.4396747967479675,
      "step": 2028,
      "training_loss": 6.587911128997803
    },
    {
      "epoch": 0.43989159891598917,
      "step": 2029,
      "training_loss": 4.085419654846191
    },
    {
      "epoch": 0.43989159891598917,
      "step": 2029,
      "training_loss": 5.981442451477051
    },
    {
      "epoch": 0.43989159891598917,
      "step": 2029,
      "training_loss": 6.365292072296143
    },
    {
      "epoch": 0.43989159891598917,
      "step": 2029,
      "training_loss": 7.453306198120117
    },
    {
      "epoch": 0.44010840108401084,
      "step": 2030,
      "training_loss": 6.295116424560547
    },
    {
      "epoch": 0.44010840108401084,
      "step": 2030,
      "training_loss": 6.242604732513428
    },
    {
      "epoch": 0.44010840108401084,
      "step": 2030,
      "training_loss": 7.106522083282471
    },
    {
      "epoch": 0.44010840108401084,
      "step": 2030,
      "training_loss": 6.689035415649414
    },
    {
      "epoch": 0.4403252032520325,
      "step": 2031,
      "training_loss": 7.260329246520996
    },
    {
      "epoch": 0.4403252032520325,
      "step": 2031,
      "training_loss": 4.592020511627197
    },
    {
      "epoch": 0.4403252032520325,
      "step": 2031,
      "training_loss": 6.890520095825195
    },
    {
      "epoch": 0.4403252032520325,
      "step": 2031,
      "training_loss": 7.8283371925354
    },
    {
      "epoch": 0.44054200542005423,
      "grad_norm": 15.195242881774902,
      "learning_rate": 1e-05,
      "loss": 6.4411,
      "step": 2032
    },
    {
      "epoch": 0.44054200542005423,
      "step": 2032,
      "training_loss": 5.678666591644287
    },
    {
      "epoch": 0.44054200542005423,
      "step": 2032,
      "training_loss": 6.972352027893066
    },
    {
      "epoch": 0.44054200542005423,
      "step": 2032,
      "training_loss": 5.853158950805664
    },
    {
      "epoch": 0.44054200542005423,
      "step": 2032,
      "training_loss": 6.8642754554748535
    },
    {
      "epoch": 0.4407588075880759,
      "step": 2033,
      "training_loss": 6.552287578582764
    },
    {
      "epoch": 0.4407588075880759,
      "step": 2033,
      "training_loss": 5.692944526672363
    },
    {
      "epoch": 0.4407588075880759,
      "step": 2033,
      "training_loss": 6.886984348297119
    },
    {
      "epoch": 0.4407588075880759,
      "step": 2033,
      "training_loss": 5.5157575607299805
    },
    {
      "epoch": 0.44097560975609756,
      "step": 2034,
      "training_loss": 5.974370956420898
    },
    {
      "epoch": 0.44097560975609756,
      "step": 2034,
      "training_loss": 7.0623555183410645
    },
    {
      "epoch": 0.44097560975609756,
      "step": 2034,
      "training_loss": 6.256461143493652
    },
    {
      "epoch": 0.44097560975609756,
      "step": 2034,
      "training_loss": 4.576663970947266
    },
    {
      "epoch": 0.44119241192411923,
      "step": 2035,
      "training_loss": 3.6792595386505127
    },
    {
      "epoch": 0.44119241192411923,
      "step": 2035,
      "training_loss": 6.982473850250244
    },
    {
      "epoch": 0.44119241192411923,
      "step": 2035,
      "training_loss": 5.985476970672607
    },
    {
      "epoch": 0.44119241192411923,
      "step": 2035,
      "training_loss": 5.770137310028076
    },
    {
      "epoch": 0.4414092140921409,
      "grad_norm": 17.538585662841797,
      "learning_rate": 1e-05,
      "loss": 6.019,
      "step": 2036
    },
    {
      "epoch": 0.4414092140921409,
      "step": 2036,
      "training_loss": 8.122406959533691
    },
    {
      "epoch": 0.4414092140921409,
      "step": 2036,
      "training_loss": 7.409092426300049
    },
    {
      "epoch": 0.4414092140921409,
      "step": 2036,
      "training_loss": 5.488576412200928
    },
    {
      "epoch": 0.4414092140921409,
      "step": 2036,
      "training_loss": 6.793255805969238
    },
    {
      "epoch": 0.4416260162601626,
      "step": 2037,
      "training_loss": 6.5019354820251465
    },
    {
      "epoch": 0.4416260162601626,
      "step": 2037,
      "training_loss": 5.614657402038574
    },
    {
      "epoch": 0.4416260162601626,
      "step": 2037,
      "training_loss": 7.355722427368164
    },
    {
      "epoch": 0.4416260162601626,
      "step": 2037,
      "training_loss": 7.8906569480896
    },
    {
      "epoch": 0.4418428184281843,
      "step": 2038,
      "training_loss": 5.595144271850586
    },
    {
      "epoch": 0.4418428184281843,
      "step": 2038,
      "training_loss": 7.456447124481201
    },
    {
      "epoch": 0.4418428184281843,
      "step": 2038,
      "training_loss": 6.085555553436279
    },
    {
      "epoch": 0.4418428184281843,
      "step": 2038,
      "training_loss": 8.60753059387207
    },
    {
      "epoch": 0.44205962059620596,
      "step": 2039,
      "training_loss": 6.838062286376953
    },
    {
      "epoch": 0.44205962059620596,
      "step": 2039,
      "training_loss": 7.049501895904541
    },
    {
      "epoch": 0.44205962059620596,
      "step": 2039,
      "training_loss": 7.8916826248168945
    },
    {
      "epoch": 0.44205962059620596,
      "step": 2039,
      "training_loss": 6.680219650268555
    },
    {
      "epoch": 0.44227642276422763,
      "grad_norm": 9.871322631835938,
      "learning_rate": 1e-05,
      "loss": 6.9613,
      "step": 2040
    },
    {
      "epoch": 0.44227642276422763,
      "step": 2040,
      "training_loss": 6.211812496185303
    },
    {
      "epoch": 0.44227642276422763,
      "step": 2040,
      "training_loss": 7.009774208068848
    },
    {
      "epoch": 0.44227642276422763,
      "step": 2040,
      "training_loss": 7.1748223304748535
    },
    {
      "epoch": 0.44227642276422763,
      "step": 2040,
      "training_loss": 2.5087687969207764
    },
    {
      "epoch": 0.4424932249322493,
      "step": 2041,
      "training_loss": 6.647746562957764
    },
    {
      "epoch": 0.4424932249322493,
      "step": 2041,
      "training_loss": 6.500943183898926
    },
    {
      "epoch": 0.4424932249322493,
      "step": 2041,
      "training_loss": 6.814785003662109
    },
    {
      "epoch": 0.4424932249322493,
      "step": 2041,
      "training_loss": 7.23216438293457
    },
    {
      "epoch": 0.442710027100271,
      "step": 2042,
      "training_loss": 5.64063024520874
    },
    {
      "epoch": 0.442710027100271,
      "step": 2042,
      "training_loss": 7.255361557006836
    },
    {
      "epoch": 0.442710027100271,
      "step": 2042,
      "training_loss": 7.0401811599731445
    },
    {
      "epoch": 0.442710027100271,
      "step": 2042,
      "training_loss": 7.743254661560059
    },
    {
      "epoch": 0.4429268292682927,
      "step": 2043,
      "training_loss": 6.7646164894104
    },
    {
      "epoch": 0.4429268292682927,
      "step": 2043,
      "training_loss": 6.58003044128418
    },
    {
      "epoch": 0.4429268292682927,
      "step": 2043,
      "training_loss": 6.803021430969238
    },
    {
      "epoch": 0.4429268292682927,
      "step": 2043,
      "training_loss": 7.691004276275635
    },
    {
      "epoch": 0.44314363143631436,
      "grad_norm": 13.653626441955566,
      "learning_rate": 1e-05,
      "loss": 6.6012,
      "step": 2044
    },
    {
      "epoch": 0.44314363143631436,
      "step": 2044,
      "training_loss": 5.601287841796875
    },
    {
      "epoch": 0.44314363143631436,
      "step": 2044,
      "training_loss": 6.201940059661865
    },
    {
      "epoch": 0.44314363143631436,
      "step": 2044,
      "training_loss": 6.099611759185791
    },
    {
      "epoch": 0.44314363143631436,
      "step": 2044,
      "training_loss": 7.389634132385254
    },
    {
      "epoch": 0.443360433604336,
      "step": 2045,
      "training_loss": 5.717138290405273
    },
    {
      "epoch": 0.443360433604336,
      "step": 2045,
      "training_loss": 6.499878406524658
    },
    {
      "epoch": 0.443360433604336,
      "step": 2045,
      "training_loss": 7.121482849121094
    },
    {
      "epoch": 0.443360433604336,
      "step": 2045,
      "training_loss": 7.033870697021484
    },
    {
      "epoch": 0.44357723577235775,
      "step": 2046,
      "training_loss": 7.8169355392456055
    },
    {
      "epoch": 0.44357723577235775,
      "step": 2046,
      "training_loss": 6.513749122619629
    },
    {
      "epoch": 0.44357723577235775,
      "step": 2046,
      "training_loss": 6.516968727111816
    },
    {
      "epoch": 0.44357723577235775,
      "step": 2046,
      "training_loss": 3.7468841075897217
    },
    {
      "epoch": 0.4437940379403794,
      "step": 2047,
      "training_loss": 6.831134796142578
    },
    {
      "epoch": 0.4437940379403794,
      "step": 2047,
      "training_loss": 7.204257965087891
    },
    {
      "epoch": 0.4437940379403794,
      "step": 2047,
      "training_loss": 6.177770137786865
    },
    {
      "epoch": 0.4437940379403794,
      "step": 2047,
      "training_loss": 6.221826076507568
    },
    {
      "epoch": 0.4440108401084011,
      "grad_norm": 11.448875427246094,
      "learning_rate": 1e-05,
      "loss": 6.4184,
      "step": 2048
    },
    {
      "epoch": 0.4440108401084011,
      "step": 2048,
      "training_loss": 7.968963623046875
    },
    {
      "epoch": 0.4440108401084011,
      "step": 2048,
      "training_loss": 7.092385292053223
    },
    {
      "epoch": 0.4440108401084011,
      "step": 2048,
      "training_loss": 4.190641403198242
    },
    {
      "epoch": 0.4440108401084011,
      "step": 2048,
      "training_loss": 8.397860527038574
    },
    {
      "epoch": 0.44422764227642275,
      "step": 2049,
      "training_loss": 5.581312656402588
    },
    {
      "epoch": 0.44422764227642275,
      "step": 2049,
      "training_loss": 3.7956695556640625
    },
    {
      "epoch": 0.44422764227642275,
      "step": 2049,
      "training_loss": 8.043159484863281
    },
    {
      "epoch": 0.44422764227642275,
      "step": 2049,
      "training_loss": 4.923234462738037
    },
    {
      "epoch": 0.4444444444444444,
      "step": 2050,
      "training_loss": 6.629024028778076
    },
    {
      "epoch": 0.4444444444444444,
      "step": 2050,
      "training_loss": 6.802781581878662
    },
    {
      "epoch": 0.4444444444444444,
      "step": 2050,
      "training_loss": 6.985507965087891
    },
    {
      "epoch": 0.4444444444444444,
      "step": 2050,
      "training_loss": 4.092069625854492
    },
    {
      "epoch": 0.44466124661246614,
      "step": 2051,
      "training_loss": 7.0827178955078125
    },
    {
      "epoch": 0.44466124661246614,
      "step": 2051,
      "training_loss": 7.790135383605957
    },
    {
      "epoch": 0.44466124661246614,
      "step": 2051,
      "training_loss": 2.838564395904541
    },
    {
      "epoch": 0.44466124661246614,
      "step": 2051,
      "training_loss": 7.404294013977051
    },
    {
      "epoch": 0.4448780487804878,
      "grad_norm": 15.846996307373047,
      "learning_rate": 1e-05,
      "loss": 6.2261,
      "step": 2052
    },
    {
      "epoch": 0.4448780487804878,
      "step": 2052,
      "training_loss": 8.18232536315918
    },
    {
      "epoch": 0.4448780487804878,
      "step": 2052,
      "training_loss": 7.815439701080322
    },
    {
      "epoch": 0.4448780487804878,
      "step": 2052,
      "training_loss": 4.709377288818359
    },
    {
      "epoch": 0.4448780487804878,
      "step": 2052,
      "training_loss": 7.77296781539917
    },
    {
      "epoch": 0.4450948509485095,
      "step": 2053,
      "training_loss": 7.345144271850586
    },
    {
      "epoch": 0.4450948509485095,
      "step": 2053,
      "training_loss": 6.677487373352051
    },
    {
      "epoch": 0.4450948509485095,
      "step": 2053,
      "training_loss": 6.2565178871154785
    },
    {
      "epoch": 0.4450948509485095,
      "step": 2053,
      "training_loss": 7.561803340911865
    },
    {
      "epoch": 0.44531165311653115,
      "step": 2054,
      "training_loss": 7.003300189971924
    },
    {
      "epoch": 0.44531165311653115,
      "step": 2054,
      "training_loss": 6.695736408233643
    },
    {
      "epoch": 0.44531165311653115,
      "step": 2054,
      "training_loss": 6.106277942657471
    },
    {
      "epoch": 0.44531165311653115,
      "step": 2054,
      "training_loss": 7.423306941986084
    },
    {
      "epoch": 0.44552845528455287,
      "step": 2055,
      "training_loss": 5.324882984161377
    },
    {
      "epoch": 0.44552845528455287,
      "step": 2055,
      "training_loss": 6.846159934997559
    },
    {
      "epoch": 0.44552845528455287,
      "step": 2055,
      "training_loss": 6.4696784019470215
    },
    {
      "epoch": 0.44552845528455287,
      "step": 2055,
      "training_loss": 5.601804256439209
    },
    {
      "epoch": 0.44574525745257454,
      "grad_norm": 16.544408798217773,
      "learning_rate": 1e-05,
      "loss": 6.737,
      "step": 2056
    },
    {
      "epoch": 0.44574525745257454,
      "step": 2056,
      "training_loss": 3.307152032852173
    },
    {
      "epoch": 0.44574525745257454,
      "step": 2056,
      "training_loss": 5.796286582946777
    },
    {
      "epoch": 0.44574525745257454,
      "step": 2056,
      "training_loss": 6.897727966308594
    },
    {
      "epoch": 0.44574525745257454,
      "step": 2056,
      "training_loss": 7.399073600769043
    },
    {
      "epoch": 0.4459620596205962,
      "step": 2057,
      "training_loss": 7.5649614334106445
    },
    {
      "epoch": 0.4459620596205962,
      "step": 2057,
      "training_loss": 2.8475863933563232
    },
    {
      "epoch": 0.4459620596205962,
      "step": 2057,
      "training_loss": 7.374698638916016
    },
    {
      "epoch": 0.4459620596205962,
      "step": 2057,
      "training_loss": 7.152003765106201
    },
    {
      "epoch": 0.4461788617886179,
      "step": 2058,
      "training_loss": 7.252493858337402
    },
    {
      "epoch": 0.4461788617886179,
      "step": 2058,
      "training_loss": 7.208215713500977
    },
    {
      "epoch": 0.4461788617886179,
      "step": 2058,
      "training_loss": 6.221757411956787
    },
    {
      "epoch": 0.4461788617886179,
      "step": 2058,
      "training_loss": 6.0019636154174805
    },
    {
      "epoch": 0.44639566395663954,
      "step": 2059,
      "training_loss": 7.043006896972656
    },
    {
      "epoch": 0.44639566395663954,
      "step": 2059,
      "training_loss": 5.664480209350586
    },
    {
      "epoch": 0.44639566395663954,
      "step": 2059,
      "training_loss": 4.85049295425415
    },
    {
      "epoch": 0.44639566395663954,
      "step": 2059,
      "training_loss": 4.100913047790527
    },
    {
      "epoch": 0.44661246612466127,
      "grad_norm": 10.70873737335205,
      "learning_rate": 1e-05,
      "loss": 6.0427,
      "step": 2060
    },
    {
      "epoch": 0.44661246612466127,
      "step": 2060,
      "training_loss": 7.5409321784973145
    },
    {
      "epoch": 0.44661246612466127,
      "step": 2060,
      "training_loss": 7.583621025085449
    },
    {
      "epoch": 0.44661246612466127,
      "step": 2060,
      "training_loss": 6.974122524261475
    },
    {
      "epoch": 0.44661246612466127,
      "step": 2060,
      "training_loss": 6.396778583526611
    },
    {
      "epoch": 0.44682926829268294,
      "step": 2061,
      "training_loss": 7.371278285980225
    },
    {
      "epoch": 0.44682926829268294,
      "step": 2061,
      "training_loss": 7.951347351074219
    },
    {
      "epoch": 0.44682926829268294,
      "step": 2061,
      "training_loss": 7.781543731689453
    },
    {
      "epoch": 0.44682926829268294,
      "step": 2061,
      "training_loss": 6.326108455657959
    },
    {
      "epoch": 0.4470460704607046,
      "step": 2062,
      "training_loss": 5.5253071784973145
    },
    {
      "epoch": 0.4470460704607046,
      "step": 2062,
      "training_loss": 6.311065673828125
    },
    {
      "epoch": 0.4470460704607046,
      "step": 2062,
      "training_loss": 6.450033664703369
    },
    {
      "epoch": 0.4470460704607046,
      "step": 2062,
      "training_loss": 6.787680625915527
    },
    {
      "epoch": 0.44726287262872627,
      "step": 2063,
      "training_loss": 6.957620143890381
    },
    {
      "epoch": 0.44726287262872627,
      "step": 2063,
      "training_loss": 4.345839977264404
    },
    {
      "epoch": 0.44726287262872627,
      "step": 2063,
      "training_loss": 6.324061393737793
    },
    {
      "epoch": 0.44726287262872627,
      "step": 2063,
      "training_loss": 7.8696699142456055
    },
    {
      "epoch": 0.447479674796748,
      "grad_norm": 15.665571212768555,
      "learning_rate": 1e-05,
      "loss": 6.7811,
      "step": 2064
    },
    {
      "epoch": 0.447479674796748,
      "step": 2064,
      "training_loss": 7.699272632598877
    },
    {
      "epoch": 0.447479674796748,
      "step": 2064,
      "training_loss": 7.019704341888428
    },
    {
      "epoch": 0.447479674796748,
      "step": 2064,
      "training_loss": 6.803782939910889
    },
    {
      "epoch": 0.447479674796748,
      "step": 2064,
      "training_loss": 6.665699005126953
    },
    {
      "epoch": 0.44769647696476966,
      "step": 2065,
      "training_loss": 5.381814479827881
    },
    {
      "epoch": 0.44769647696476966,
      "step": 2065,
      "training_loss": 6.554110527038574
    },
    {
      "epoch": 0.44769647696476966,
      "step": 2065,
      "training_loss": 6.852965831756592
    },
    {
      "epoch": 0.44769647696476966,
      "step": 2065,
      "training_loss": 5.541650295257568
    },
    {
      "epoch": 0.44791327913279133,
      "step": 2066,
      "training_loss": 7.044126510620117
    },
    {
      "epoch": 0.44791327913279133,
      "step": 2066,
      "training_loss": 6.825882434844971
    },
    {
      "epoch": 0.44791327913279133,
      "step": 2066,
      "training_loss": 3.6832640171051025
    },
    {
      "epoch": 0.44791327913279133,
      "step": 2066,
      "training_loss": 7.040489196777344
    },
    {
      "epoch": 0.448130081300813,
      "step": 2067,
      "training_loss": 7.162769317626953
    },
    {
      "epoch": 0.448130081300813,
      "step": 2067,
      "training_loss": 5.879113674163818
    },
    {
      "epoch": 0.448130081300813,
      "step": 2067,
      "training_loss": 6.9689249992370605
    },
    {
      "epoch": 0.448130081300813,
      "step": 2067,
      "training_loss": 6.453776836395264
    },
    {
      "epoch": 0.44834688346883467,
      "grad_norm": 18.970243453979492,
      "learning_rate": 1e-05,
      "loss": 6.4736,
      "step": 2068
    },
    {
      "epoch": 0.44834688346883467,
      "step": 2068,
      "training_loss": 7.332179069519043
    },
    {
      "epoch": 0.44834688346883467,
      "step": 2068,
      "training_loss": 4.725334644317627
    },
    {
      "epoch": 0.44834688346883467,
      "step": 2068,
      "training_loss": 6.150492191314697
    },
    {
      "epoch": 0.44834688346883467,
      "step": 2068,
      "training_loss": 7.304481029510498
    },
    {
      "epoch": 0.4485636856368564,
      "step": 2069,
      "training_loss": 7.283230304718018
    },
    {
      "epoch": 0.4485636856368564,
      "step": 2069,
      "training_loss": 6.709894180297852
    },
    {
      "epoch": 0.4485636856368564,
      "step": 2069,
      "training_loss": 7.066160202026367
    },
    {
      "epoch": 0.4485636856368564,
      "step": 2069,
      "training_loss": 6.904211521148682
    },
    {
      "epoch": 0.44878048780487806,
      "step": 2070,
      "training_loss": 6.343351364135742
    },
    {
      "epoch": 0.44878048780487806,
      "step": 2070,
      "training_loss": 5.607315540313721
    },
    {
      "epoch": 0.44878048780487806,
      "step": 2070,
      "training_loss": 4.0988688468933105
    },
    {
      "epoch": 0.44878048780487806,
      "step": 2070,
      "training_loss": 7.075281143188477
    },
    {
      "epoch": 0.4489972899728997,
      "step": 2071,
      "training_loss": 6.937881946563721
    },
    {
      "epoch": 0.4489972899728997,
      "step": 2071,
      "training_loss": 5.770401954650879
    },
    {
      "epoch": 0.4489972899728997,
      "step": 2071,
      "training_loss": 5.681878566741943
    },
    {
      "epoch": 0.4489972899728997,
      "step": 2071,
      "training_loss": 6.764472484588623
    },
    {
      "epoch": 0.4492140921409214,
      "grad_norm": 14.799311637878418,
      "learning_rate": 1e-05,
      "loss": 6.3597,
      "step": 2072
    },
    {
      "epoch": 0.4492140921409214,
      "step": 2072,
      "training_loss": 6.301612377166748
    },
    {
      "epoch": 0.4492140921409214,
      "step": 2072,
      "training_loss": 6.241903781890869
    },
    {
      "epoch": 0.4492140921409214,
      "step": 2072,
      "training_loss": 6.159777641296387
    },
    {
      "epoch": 0.4492140921409214,
      "step": 2072,
      "training_loss": 6.760429859161377
    },
    {
      "epoch": 0.44943089430894306,
      "step": 2073,
      "training_loss": 5.374443531036377
    },
    {
      "epoch": 0.44943089430894306,
      "step": 2073,
      "training_loss": 6.595839500427246
    },
    {
      "epoch": 0.44943089430894306,
      "step": 2073,
      "training_loss": 7.518540859222412
    },
    {
      "epoch": 0.44943089430894306,
      "step": 2073,
      "training_loss": 6.88358736038208
    },
    {
      "epoch": 0.4496476964769648,
      "step": 2074,
      "training_loss": 7.384160995483398
    },
    {
      "epoch": 0.4496476964769648,
      "step": 2074,
      "training_loss": 5.938047409057617
    },
    {
      "epoch": 0.4496476964769648,
      "step": 2074,
      "training_loss": 5.896021366119385
    },
    {
      "epoch": 0.4496476964769648,
      "step": 2074,
      "training_loss": 6.684077739715576
    },
    {
      "epoch": 0.44986449864498645,
      "step": 2075,
      "training_loss": 6.645907878875732
    },
    {
      "epoch": 0.44986449864498645,
      "step": 2075,
      "training_loss": 7.4561262130737305
    },
    {
      "epoch": 0.44986449864498645,
      "step": 2075,
      "training_loss": 7.355605602264404
    },
    {
      "epoch": 0.44986449864498645,
      "step": 2075,
      "training_loss": 4.143777847290039
    },
    {
      "epoch": 0.4500813008130081,
      "grad_norm": 11.789360046386719,
      "learning_rate": 1e-05,
      "loss": 6.4587,
      "step": 2076
    },
    {
      "epoch": 0.4500813008130081,
      "step": 2076,
      "training_loss": 6.579794883728027
    },
    {
      "epoch": 0.4500813008130081,
      "step": 2076,
      "training_loss": 7.567779541015625
    },
    {
      "epoch": 0.4500813008130081,
      "step": 2076,
      "training_loss": 7.525764465332031
    },
    {
      "epoch": 0.4500813008130081,
      "step": 2076,
      "training_loss": 5.177160739898682
    },
    {
      "epoch": 0.4502981029810298,
      "step": 2077,
      "training_loss": 6.692760467529297
    },
    {
      "epoch": 0.4502981029810298,
      "step": 2077,
      "training_loss": 7.189553737640381
    },
    {
      "epoch": 0.4502981029810298,
      "step": 2077,
      "training_loss": 6.539421558380127
    },
    {
      "epoch": 0.4502981029810298,
      "step": 2077,
      "training_loss": 7.623059272766113
    },
    {
      "epoch": 0.4505149051490515,
      "step": 2078,
      "training_loss": 6.324746608734131
    },
    {
      "epoch": 0.4505149051490515,
      "step": 2078,
      "training_loss": 7.0167388916015625
    },
    {
      "epoch": 0.4505149051490515,
      "step": 2078,
      "training_loss": 5.767227649688721
    },
    {
      "epoch": 0.4505149051490515,
      "step": 2078,
      "training_loss": 6.636127471923828
    },
    {
      "epoch": 0.4507317073170732,
      "step": 2079,
      "training_loss": 3.229489803314209
    },
    {
      "epoch": 0.4507317073170732,
      "step": 2079,
      "training_loss": 6.663600921630859
    },
    {
      "epoch": 0.4507317073170732,
      "step": 2079,
      "training_loss": 5.646756172180176
    },
    {
      "epoch": 0.4507317073170732,
      "step": 2079,
      "training_loss": 7.7188239097595215
    },
    {
      "epoch": 0.45094850948509485,
      "grad_norm": 13.03200912475586,
      "learning_rate": 1e-05,
      "loss": 6.4937,
      "step": 2080
    },
    {
      "epoch": 0.45094850948509485,
      "step": 2080,
      "training_loss": 5.643526077270508
    },
    {
      "epoch": 0.45094850948509485,
      "step": 2080,
      "training_loss": 7.102139472961426
    },
    {
      "epoch": 0.45094850948509485,
      "step": 2080,
      "training_loss": 6.829875469207764
    },
    {
      "epoch": 0.45094850948509485,
      "step": 2080,
      "training_loss": 6.66597843170166
    },
    {
      "epoch": 0.4511653116531165,
      "step": 2081,
      "training_loss": 6.509886264801025
    },
    {
      "epoch": 0.4511653116531165,
      "step": 2081,
      "training_loss": 7.294065952301025
    },
    {
      "epoch": 0.4511653116531165,
      "step": 2081,
      "training_loss": 5.140560150146484
    },
    {
      "epoch": 0.4511653116531165,
      "step": 2081,
      "training_loss": 6.336845874786377
    },
    {
      "epoch": 0.4513821138211382,
      "step": 2082,
      "training_loss": 8.180628776550293
    },
    {
      "epoch": 0.4513821138211382,
      "step": 2082,
      "training_loss": 3.4393370151519775
    },
    {
      "epoch": 0.4513821138211382,
      "step": 2082,
      "training_loss": 8.42745304107666
    },
    {
      "epoch": 0.4513821138211382,
      "step": 2082,
      "training_loss": 6.5019850730896
    },
    {
      "epoch": 0.4515989159891599,
      "step": 2083,
      "training_loss": 6.171756744384766
    },
    {
      "epoch": 0.4515989159891599,
      "step": 2083,
      "training_loss": 6.868814945220947
    },
    {
      "epoch": 0.4515989159891599,
      "step": 2083,
      "training_loss": 7.223052501678467
    },
    {
      "epoch": 0.4515989159891599,
      "step": 2083,
      "training_loss": 4.828691482543945
    },
    {
      "epoch": 0.4518157181571816,
      "grad_norm": 16.250762939453125,
      "learning_rate": 1e-05,
      "loss": 6.4478,
      "step": 2084
    },
    {
      "epoch": 0.4518157181571816,
      "step": 2084,
      "training_loss": 7.4413228034973145
    },
    {
      "epoch": 0.4518157181571816,
      "step": 2084,
      "training_loss": 6.636673450469971
    },
    {
      "epoch": 0.4518157181571816,
      "step": 2084,
      "training_loss": 7.582569599151611
    },
    {
      "epoch": 0.4518157181571816,
      "step": 2084,
      "training_loss": 7.022532939910889
    },
    {
      "epoch": 0.45203252032520325,
      "step": 2085,
      "training_loss": 6.90371036529541
    },
    {
      "epoch": 0.45203252032520325,
      "step": 2085,
      "training_loss": 6.03193998336792
    },
    {
      "epoch": 0.45203252032520325,
      "step": 2085,
      "training_loss": 4.80377721786499
    },
    {
      "epoch": 0.45203252032520325,
      "step": 2085,
      "training_loss": 5.719159126281738
    },
    {
      "epoch": 0.4522493224932249,
      "step": 2086,
      "training_loss": 6.823736190795898
    },
    {
      "epoch": 0.4522493224932249,
      "step": 2086,
      "training_loss": 6.520171165466309
    },
    {
      "epoch": 0.4522493224932249,
      "step": 2086,
      "training_loss": 7.381077766418457
    },
    {
      "epoch": 0.4522493224932249,
      "step": 2086,
      "training_loss": 6.883133411407471
    },
    {
      "epoch": 0.45246612466124664,
      "step": 2087,
      "training_loss": 6.589162826538086
    },
    {
      "epoch": 0.45246612466124664,
      "step": 2087,
      "training_loss": 4.863190650939941
    },
    {
      "epoch": 0.45246612466124664,
      "step": 2087,
      "training_loss": 6.9155964851379395
    },
    {
      "epoch": 0.45246612466124664,
      "step": 2087,
      "training_loss": 6.37506628036499
    },
    {
      "epoch": 0.4526829268292683,
      "grad_norm": 15.055546760559082,
      "learning_rate": 1e-05,
      "loss": 6.5308,
      "step": 2088
    },
    {
      "epoch": 0.4526829268292683,
      "step": 2088,
      "training_loss": 5.391080379486084
    },
    {
      "epoch": 0.4526829268292683,
      "step": 2088,
      "training_loss": 8.242897033691406
    },
    {
      "epoch": 0.4526829268292683,
      "step": 2088,
      "training_loss": 6.105942726135254
    },
    {
      "epoch": 0.4526829268292683,
      "step": 2088,
      "training_loss": 6.264145374298096
    },
    {
      "epoch": 0.45289972899729,
      "step": 2089,
      "training_loss": 6.79325532913208
    },
    {
      "epoch": 0.45289972899729,
      "step": 2089,
      "training_loss": 6.676513195037842
    },
    {
      "epoch": 0.45289972899729,
      "step": 2089,
      "training_loss": 6.5196003913879395
    },
    {
      "epoch": 0.45289972899729,
      "step": 2089,
      "training_loss": 7.157112121582031
    },
    {
      "epoch": 0.45311653116531164,
      "step": 2090,
      "training_loss": 8.038003921508789
    },
    {
      "epoch": 0.45311653116531164,
      "step": 2090,
      "training_loss": 6.685025215148926
    },
    {
      "epoch": 0.45311653116531164,
      "step": 2090,
      "training_loss": 7.744133949279785
    },
    {
      "epoch": 0.45311653116531164,
      "step": 2090,
      "training_loss": 5.618145942687988
    },
    {
      "epoch": 0.4533333333333333,
      "step": 2091,
      "training_loss": 6.661777973175049
    },
    {
      "epoch": 0.4533333333333333,
      "step": 2091,
      "training_loss": 7.318182945251465
    },
    {
      "epoch": 0.4533333333333333,
      "step": 2091,
      "training_loss": 7.144920825958252
    },
    {
      "epoch": 0.4533333333333333,
      "step": 2091,
      "training_loss": 5.327620506286621
    },
    {
      "epoch": 0.45355013550135503,
      "grad_norm": 14.555061340332031,
      "learning_rate": 1e-05,
      "loss": 6.7305,
      "step": 2092
    },
    {
      "epoch": 0.45355013550135503,
      "step": 2092,
      "training_loss": 6.514381408691406
    },
    {
      "epoch": 0.45355013550135503,
      "step": 2092,
      "training_loss": 7.267556667327881
    },
    {
      "epoch": 0.45355013550135503,
      "step": 2092,
      "training_loss": 5.935172080993652
    },
    {
      "epoch": 0.45355013550135503,
      "step": 2092,
      "training_loss": 7.126157283782959
    },
    {
      "epoch": 0.4537669376693767,
      "step": 2093,
      "training_loss": 6.364727973937988
    },
    {
      "epoch": 0.4537669376693767,
      "step": 2093,
      "training_loss": 5.687475204467773
    },
    {
      "epoch": 0.4537669376693767,
      "step": 2093,
      "training_loss": 6.707324028015137
    },
    {
      "epoch": 0.4537669376693767,
      "step": 2093,
      "training_loss": 6.884333610534668
    },
    {
      "epoch": 0.45398373983739837,
      "step": 2094,
      "training_loss": 6.847233772277832
    },
    {
      "epoch": 0.45398373983739837,
      "step": 2094,
      "training_loss": 5.156157970428467
    },
    {
      "epoch": 0.45398373983739837,
      "step": 2094,
      "training_loss": 6.883083820343018
    },
    {
      "epoch": 0.45398373983739837,
      "step": 2094,
      "training_loss": 5.617855548858643
    },
    {
      "epoch": 0.45420054200542004,
      "step": 2095,
      "training_loss": 6.52628755569458
    },
    {
      "epoch": 0.45420054200542004,
      "step": 2095,
      "training_loss": 7.179812431335449
    },
    {
      "epoch": 0.45420054200542004,
      "step": 2095,
      "training_loss": 7.527339935302734
    },
    {
      "epoch": 0.45420054200542004,
      "step": 2095,
      "training_loss": 6.840482711791992
    },
    {
      "epoch": 0.45441734417344176,
      "grad_norm": 11.801612854003906,
      "learning_rate": 1e-05,
      "loss": 6.5666,
      "step": 2096
    },
    {
      "epoch": 0.45441734417344176,
      "step": 2096,
      "training_loss": 7.280111789703369
    },
    {
      "epoch": 0.45441734417344176,
      "step": 2096,
      "training_loss": 6.502974987030029
    },
    {
      "epoch": 0.45441734417344176,
      "step": 2096,
      "training_loss": 6.7220611572265625
    },
    {
      "epoch": 0.45441734417344176,
      "step": 2096,
      "training_loss": 6.23155403137207
    },
    {
      "epoch": 0.45463414634146343,
      "step": 2097,
      "training_loss": 7.015724182128906
    },
    {
      "epoch": 0.45463414634146343,
      "step": 2097,
      "training_loss": 6.542254447937012
    },
    {
      "epoch": 0.45463414634146343,
      "step": 2097,
      "training_loss": 7.065730571746826
    },
    {
      "epoch": 0.45463414634146343,
      "step": 2097,
      "training_loss": 4.671038627624512
    },
    {
      "epoch": 0.4548509485094851,
      "step": 2098,
      "training_loss": 7.471324443817139
    },
    {
      "epoch": 0.4548509485094851,
      "step": 2098,
      "training_loss": 7.08803653717041
    },
    {
      "epoch": 0.4548509485094851,
      "step": 2098,
      "training_loss": 5.170973300933838
    },
    {
      "epoch": 0.4548509485094851,
      "step": 2098,
      "training_loss": 6.731497764587402
    },
    {
      "epoch": 0.45506775067750677,
      "step": 2099,
      "training_loss": 6.9128737449646
    },
    {
      "epoch": 0.45506775067750677,
      "step": 2099,
      "training_loss": 6.070019721984863
    },
    {
      "epoch": 0.45506775067750677,
      "step": 2099,
      "training_loss": 6.6396708488464355
    },
    {
      "epoch": 0.45506775067750677,
      "step": 2099,
      "training_loss": 8.263602256774902
    },
    {
      "epoch": 0.45528455284552843,
      "grad_norm": 21.245840072631836,
      "learning_rate": 1e-05,
      "loss": 6.6487,
      "step": 2100
    },
    {
      "epoch": 0.45528455284552843,
      "step": 2100,
      "training_loss": 6.76364803314209
    },
    {
      "epoch": 0.45528455284552843,
      "step": 2100,
      "training_loss": 3.7715444564819336
    },
    {
      "epoch": 0.45528455284552843,
      "step": 2100,
      "training_loss": 6.991842269897461
    },
    {
      "epoch": 0.45528455284552843,
      "step": 2100,
      "training_loss": 6.451719284057617
    },
    {
      "epoch": 0.45550135501355016,
      "step": 2101,
      "training_loss": 6.294018268585205
    },
    {
      "epoch": 0.45550135501355016,
      "step": 2101,
      "training_loss": 3.1600944995880127
    },
    {
      "epoch": 0.45550135501355016,
      "step": 2101,
      "training_loss": 5.912521839141846
    },
    {
      "epoch": 0.45550135501355016,
      "step": 2101,
      "training_loss": 7.46928596496582
    },
    {
      "epoch": 0.4557181571815718,
      "step": 2102,
      "training_loss": 6.986003875732422
    },
    {
      "epoch": 0.4557181571815718,
      "step": 2102,
      "training_loss": 5.094414710998535
    },
    {
      "epoch": 0.4557181571815718,
      "step": 2102,
      "training_loss": 6.942512512207031
    },
    {
      "epoch": 0.4557181571815718,
      "step": 2102,
      "training_loss": 6.575966835021973
    },
    {
      "epoch": 0.4559349593495935,
      "step": 2103,
      "training_loss": 6.14949369430542
    },
    {
      "epoch": 0.4559349593495935,
      "step": 2103,
      "training_loss": 3.274925708770752
    },
    {
      "epoch": 0.4559349593495935,
      "step": 2103,
      "training_loss": 6.532011032104492
    },
    {
      "epoch": 0.4559349593495935,
      "step": 2103,
      "training_loss": 6.707386016845703
    },
    {
      "epoch": 0.45615176151761516,
      "grad_norm": 17.031585693359375,
      "learning_rate": 1e-05,
      "loss": 5.9423,
      "step": 2104
    },
    {
      "epoch": 0.45615176151761516,
      "step": 2104,
      "training_loss": 6.868809223175049
    },
    {
      "epoch": 0.45615176151761516,
      "step": 2104,
      "training_loss": 5.583892345428467
    },
    {
      "epoch": 0.45615176151761516,
      "step": 2104,
      "training_loss": 7.099117755889893
    },
    {
      "epoch": 0.45615176151761516,
      "step": 2104,
      "training_loss": 6.716892719268799
    },
    {
      "epoch": 0.45636856368563683,
      "step": 2105,
      "training_loss": 6.751729488372803
    },
    {
      "epoch": 0.45636856368563683,
      "step": 2105,
      "training_loss": 6.778799057006836
    },
    {
      "epoch": 0.45636856368563683,
      "step": 2105,
      "training_loss": 6.984837055206299
    },
    {
      "epoch": 0.45636856368563683,
      "step": 2105,
      "training_loss": 6.063658714294434
    },
    {
      "epoch": 0.45658536585365855,
      "step": 2106,
      "training_loss": 6.33999490737915
    },
    {
      "epoch": 0.45658536585365855,
      "step": 2106,
      "training_loss": 5.477983474731445
    },
    {
      "epoch": 0.45658536585365855,
      "step": 2106,
      "training_loss": 6.7181243896484375
    },
    {
      "epoch": 0.45658536585365855,
      "step": 2106,
      "training_loss": 5.950603485107422
    },
    {
      "epoch": 0.4568021680216802,
      "step": 2107,
      "training_loss": 7.292842388153076
    },
    {
      "epoch": 0.4568021680216802,
      "step": 2107,
      "training_loss": 5.082295894622803
    },
    {
      "epoch": 0.4568021680216802,
      "step": 2107,
      "training_loss": 6.487939357757568
    },
    {
      "epoch": 0.4568021680216802,
      "step": 2107,
      "training_loss": 6.565787315368652
    },
    {
      "epoch": 0.4570189701897019,
      "grad_norm": 15.513898849487305,
      "learning_rate": 1e-05,
      "loss": 6.4227,
      "step": 2108
    },
    {
      "epoch": 0.4570189701897019,
      "step": 2108,
      "training_loss": 7.073505878448486
    },
    {
      "epoch": 0.4570189701897019,
      "step": 2108,
      "training_loss": 7.47481107711792
    },
    {
      "epoch": 0.4570189701897019,
      "step": 2108,
      "training_loss": 5.891777038574219
    },
    {
      "epoch": 0.4570189701897019,
      "step": 2108,
      "training_loss": 5.946340560913086
    },
    {
      "epoch": 0.45723577235772356,
      "step": 2109,
      "training_loss": 7.436795234680176
    },
    {
      "epoch": 0.45723577235772356,
      "step": 2109,
      "training_loss": 4.613576412200928
    },
    {
      "epoch": 0.45723577235772356,
      "step": 2109,
      "training_loss": 7.309027671813965
    },
    {
      "epoch": 0.45723577235772356,
      "step": 2109,
      "training_loss": 6.615939140319824
    },
    {
      "epoch": 0.4574525745257453,
      "step": 2110,
      "training_loss": 6.925155162811279
    },
    {
      "epoch": 0.4574525745257453,
      "step": 2110,
      "training_loss": 7.206752300262451
    },
    {
      "epoch": 0.4574525745257453,
      "step": 2110,
      "training_loss": 6.709136962890625
    },
    {
      "epoch": 0.4574525745257453,
      "step": 2110,
      "training_loss": 7.083523750305176
    },
    {
      "epoch": 0.45766937669376695,
      "step": 2111,
      "training_loss": 7.036396503448486
    },
    {
      "epoch": 0.45766937669376695,
      "step": 2111,
      "training_loss": 6.444700717926025
    },
    {
      "epoch": 0.45766937669376695,
      "step": 2111,
      "training_loss": 5.647190570831299
    },
    {
      "epoch": 0.45766937669376695,
      "step": 2111,
      "training_loss": 6.6610107421875
    },
    {
      "epoch": 0.4578861788617886,
      "grad_norm": 18.914779663085938,
      "learning_rate": 1e-05,
      "loss": 6.6297,
      "step": 2112
    },
    {
      "epoch": 0.4578861788617886,
      "step": 2112,
      "training_loss": 5.345175266265869
    },
    {
      "epoch": 0.4578861788617886,
      "step": 2112,
      "training_loss": 4.3859758377075195
    },
    {
      "epoch": 0.4578861788617886,
      "step": 2112,
      "training_loss": 7.243250370025635
    },
    {
      "epoch": 0.4578861788617886,
      "step": 2112,
      "training_loss": 7.157676696777344
    },
    {
      "epoch": 0.4581029810298103,
      "step": 2113,
      "training_loss": 6.834316253662109
    },
    {
      "epoch": 0.4581029810298103,
      "step": 2113,
      "training_loss": 7.67244291305542
    },
    {
      "epoch": 0.4581029810298103,
      "step": 2113,
      "training_loss": 5.920709609985352
    },
    {
      "epoch": 0.4581029810298103,
      "step": 2113,
      "training_loss": 6.893989086151123
    },
    {
      "epoch": 0.45831978319783195,
      "step": 2114,
      "training_loss": 7.868747711181641
    },
    {
      "epoch": 0.45831978319783195,
      "step": 2114,
      "training_loss": 7.121347904205322
    },
    {
      "epoch": 0.45831978319783195,
      "step": 2114,
      "training_loss": 7.0871076583862305
    },
    {
      "epoch": 0.45831978319783195,
      "step": 2114,
      "training_loss": 6.328047275543213
    },
    {
      "epoch": 0.4585365853658537,
      "step": 2115,
      "training_loss": 5.894570827484131
    },
    {
      "epoch": 0.4585365853658537,
      "step": 2115,
      "training_loss": 6.758544921875
    },
    {
      "epoch": 0.4585365853658537,
      "step": 2115,
      "training_loss": 7.451720237731934
    },
    {
      "epoch": 0.4585365853658537,
      "step": 2115,
      "training_loss": 7.764753818511963
    },
    {
      "epoch": 0.45875338753387535,
      "grad_norm": 18.23150062561035,
      "learning_rate": 1e-05,
      "loss": 6.733,
      "step": 2116
    },
    {
      "epoch": 0.45875338753387535,
      "step": 2116,
      "training_loss": 5.709657669067383
    },
    {
      "epoch": 0.45875338753387535,
      "step": 2116,
      "training_loss": 5.307326316833496
    },
    {
      "epoch": 0.45875338753387535,
      "step": 2116,
      "training_loss": 7.452775478363037
    },
    {
      "epoch": 0.45875338753387535,
      "step": 2116,
      "training_loss": 6.353480339050293
    },
    {
      "epoch": 0.458970189701897,
      "step": 2117,
      "training_loss": 6.595958709716797
    },
    {
      "epoch": 0.458970189701897,
      "step": 2117,
      "training_loss": 6.256389617919922
    },
    {
      "epoch": 0.458970189701897,
      "step": 2117,
      "training_loss": 4.841310501098633
    },
    {
      "epoch": 0.458970189701897,
      "step": 2117,
      "training_loss": 6.856168746948242
    },
    {
      "epoch": 0.4591869918699187,
      "step": 2118,
      "training_loss": 6.065671920776367
    },
    {
      "epoch": 0.4591869918699187,
      "step": 2118,
      "training_loss": 5.770637035369873
    },
    {
      "epoch": 0.4591869918699187,
      "step": 2118,
      "training_loss": 7.765876770019531
    },
    {
      "epoch": 0.4591869918699187,
      "step": 2118,
      "training_loss": 6.88516092300415
    },
    {
      "epoch": 0.4594037940379404,
      "step": 2119,
      "training_loss": 6.501502990722656
    },
    {
      "epoch": 0.4594037940379404,
      "step": 2119,
      "training_loss": 4.684773921966553
    },
    {
      "epoch": 0.4594037940379404,
      "step": 2119,
      "training_loss": 7.49937629699707
    },
    {
      "epoch": 0.4594037940379404,
      "step": 2119,
      "training_loss": 6.203625202178955
    },
    {
      "epoch": 0.4596205962059621,
      "grad_norm": 12.47254753112793,
      "learning_rate": 1e-05,
      "loss": 6.2969,
      "step": 2120
    },
    {
      "epoch": 0.4596205962059621,
      "step": 2120,
      "training_loss": 6.179569244384766
    },
    {
      "epoch": 0.4596205962059621,
      "step": 2120,
      "training_loss": 6.058091163635254
    },
    {
      "epoch": 0.4596205962059621,
      "step": 2120,
      "training_loss": 6.247961044311523
    },
    {
      "epoch": 0.4596205962059621,
      "step": 2120,
      "training_loss": 6.43390417098999
    },
    {
      "epoch": 0.45983739837398374,
      "step": 2121,
      "training_loss": 5.073761940002441
    },
    {
      "epoch": 0.45983739837398374,
      "step": 2121,
      "training_loss": 7.579983234405518
    },
    {
      "epoch": 0.45983739837398374,
      "step": 2121,
      "training_loss": 8.03962516784668
    },
    {
      "epoch": 0.45983739837398374,
      "step": 2121,
      "training_loss": 7.986019134521484
    },
    {
      "epoch": 0.4600542005420054,
      "step": 2122,
      "training_loss": 5.857583045959473
    },
    {
      "epoch": 0.4600542005420054,
      "step": 2122,
      "training_loss": 6.509790897369385
    },
    {
      "epoch": 0.4600542005420054,
      "step": 2122,
      "training_loss": 5.963714599609375
    },
    {
      "epoch": 0.4600542005420054,
      "step": 2122,
      "training_loss": 6.209902763366699
    },
    {
      "epoch": 0.4602710027100271,
      "step": 2123,
      "training_loss": 6.511111736297607
    },
    {
      "epoch": 0.4602710027100271,
      "step": 2123,
      "training_loss": 7.80030632019043
    },
    {
      "epoch": 0.4602710027100271,
      "step": 2123,
      "training_loss": 6.800665855407715
    },
    {
      "epoch": 0.4602710027100271,
      "step": 2123,
      "training_loss": 8.264066696166992
    },
    {
      "epoch": 0.4604878048780488,
      "grad_norm": 18.295452117919922,
      "learning_rate": 1e-05,
      "loss": 6.7198,
      "step": 2124
    },
    {
      "epoch": 0.4604878048780488,
      "step": 2124,
      "training_loss": 3.222419500350952
    },
    {
      "epoch": 0.4604878048780488,
      "step": 2124,
      "training_loss": 7.59289026260376
    },
    {
      "epoch": 0.4604878048780488,
      "step": 2124,
      "training_loss": 3.6100313663482666
    },
    {
      "epoch": 0.4604878048780488,
      "step": 2124,
      "training_loss": 6.834036350250244
    },
    {
      "epoch": 0.46070460704607047,
      "step": 2125,
      "training_loss": 5.788042068481445
    },
    {
      "epoch": 0.46070460704607047,
      "step": 2125,
      "training_loss": 6.663455963134766
    },
    {
      "epoch": 0.46070460704607047,
      "step": 2125,
      "training_loss": 6.593062877655029
    },
    {
      "epoch": 0.46070460704607047,
      "step": 2125,
      "training_loss": 5.601696014404297
    },
    {
      "epoch": 0.46092140921409214,
      "step": 2126,
      "training_loss": 7.337451457977295
    },
    {
      "epoch": 0.46092140921409214,
      "step": 2126,
      "training_loss": 6.756643295288086
    },
    {
      "epoch": 0.46092140921409214,
      "step": 2126,
      "training_loss": 7.008646488189697
    },
    {
      "epoch": 0.46092140921409214,
      "step": 2126,
      "training_loss": 8.509310722351074
    },
    {
      "epoch": 0.4611382113821138,
      "step": 2127,
      "training_loss": 5.444506645202637
    },
    {
      "epoch": 0.4611382113821138,
      "step": 2127,
      "training_loss": 3.603778839111328
    },
    {
      "epoch": 0.4611382113821138,
      "step": 2127,
      "training_loss": 4.983400821685791
    },
    {
      "epoch": 0.4611382113821138,
      "step": 2127,
      "training_loss": 7.0288214683532715
    },
    {
      "epoch": 0.46135501355013553,
      "grad_norm": 19.621736526489258,
      "learning_rate": 1e-05,
      "loss": 6.0361,
      "step": 2128
    },
    {
      "epoch": 0.46135501355013553,
      "step": 2128,
      "training_loss": 7.048816204071045
    },
    {
      "epoch": 0.46135501355013553,
      "step": 2128,
      "training_loss": 5.744085311889648
    },
    {
      "epoch": 0.46135501355013553,
      "step": 2128,
      "training_loss": 5.312225818634033
    },
    {
      "epoch": 0.46135501355013553,
      "step": 2128,
      "training_loss": 5.920431613922119
    },
    {
      "epoch": 0.4615718157181572,
      "step": 2129,
      "training_loss": 7.781729221343994
    },
    {
      "epoch": 0.4615718157181572,
      "step": 2129,
      "training_loss": 6.852774620056152
    },
    {
      "epoch": 0.4615718157181572,
      "step": 2129,
      "training_loss": 4.861595630645752
    },
    {
      "epoch": 0.4615718157181572,
      "step": 2129,
      "training_loss": 6.278682708740234
    },
    {
      "epoch": 0.46178861788617886,
      "step": 2130,
      "training_loss": 6.92918062210083
    },
    {
      "epoch": 0.46178861788617886,
      "step": 2130,
      "training_loss": 6.664018154144287
    },
    {
      "epoch": 0.46178861788617886,
      "step": 2130,
      "training_loss": 6.6227545738220215
    },
    {
      "epoch": 0.46178861788617886,
      "step": 2130,
      "training_loss": 7.591928958892822
    },
    {
      "epoch": 0.46200542005420053,
      "step": 2131,
      "training_loss": 6.884358882904053
    },
    {
      "epoch": 0.46200542005420053,
      "step": 2131,
      "training_loss": 7.059392929077148
    },
    {
      "epoch": 0.46200542005420053,
      "step": 2131,
      "training_loss": 5.466757297515869
    },
    {
      "epoch": 0.46200542005420053,
      "step": 2131,
      "training_loss": 6.944385528564453
    },
    {
      "epoch": 0.4622222222222222,
      "grad_norm": 16.18429946899414,
      "learning_rate": 1e-05,
      "loss": 6.4977,
      "step": 2132
    },
    {
      "epoch": 0.4622222222222222,
      "step": 2132,
      "training_loss": 5.205353736877441
    },
    {
      "epoch": 0.4622222222222222,
      "step": 2132,
      "training_loss": 6.273002624511719
    },
    {
      "epoch": 0.4622222222222222,
      "step": 2132,
      "training_loss": 6.5916547775268555
    },
    {
      "epoch": 0.4622222222222222,
      "step": 2132,
      "training_loss": 7.498631000518799
    },
    {
      "epoch": 0.4624390243902439,
      "step": 2133,
      "training_loss": 6.150447845458984
    },
    {
      "epoch": 0.4624390243902439,
      "step": 2133,
      "training_loss": 5.781065940856934
    },
    {
      "epoch": 0.4624390243902439,
      "step": 2133,
      "training_loss": 7.49324893951416
    },
    {
      "epoch": 0.4624390243902439,
      "step": 2133,
      "training_loss": 6.7618207931518555
    },
    {
      "epoch": 0.4626558265582656,
      "step": 2134,
      "training_loss": 7.314725875854492
    },
    {
      "epoch": 0.4626558265582656,
      "step": 2134,
      "training_loss": 7.241941452026367
    },
    {
      "epoch": 0.4626558265582656,
      "step": 2134,
      "training_loss": 8.350750923156738
    },
    {
      "epoch": 0.4626558265582656,
      "step": 2134,
      "training_loss": 8.035093307495117
    },
    {
      "epoch": 0.46287262872628726,
      "step": 2135,
      "training_loss": 6.094269752502441
    },
    {
      "epoch": 0.46287262872628726,
      "step": 2135,
      "training_loss": 5.342335224151611
    },
    {
      "epoch": 0.46287262872628726,
      "step": 2135,
      "training_loss": 6.046194553375244
    },
    {
      "epoch": 0.46287262872628726,
      "step": 2135,
      "training_loss": 6.81700325012207
    },
    {
      "epoch": 0.46308943089430893,
      "grad_norm": 16.145999908447266,
      "learning_rate": 1e-05,
      "loss": 6.6873,
      "step": 2136
    },
    {
      "epoch": 0.46308943089430893,
      "step": 2136,
      "training_loss": 8.161980628967285
    },
    {
      "epoch": 0.46308943089430893,
      "step": 2136,
      "training_loss": 4.535226345062256
    },
    {
      "epoch": 0.46308943089430893,
      "step": 2136,
      "training_loss": 6.82766056060791
    },
    {
      "epoch": 0.46308943089430893,
      "step": 2136,
      "training_loss": 7.792242527008057
    },
    {
      "epoch": 0.4633062330623306,
      "step": 2137,
      "training_loss": 7.485360622406006
    },
    {
      "epoch": 0.4633062330623306,
      "step": 2137,
      "training_loss": 6.9743876457214355
    },
    {
      "epoch": 0.4633062330623306,
      "step": 2137,
      "training_loss": 7.216591835021973
    },
    {
      "epoch": 0.4633062330623306,
      "step": 2137,
      "training_loss": 8.283891677856445
    },
    {
      "epoch": 0.4635230352303523,
      "step": 2138,
      "training_loss": 7.243531703948975
    },
    {
      "epoch": 0.4635230352303523,
      "step": 2138,
      "training_loss": 7.247643947601318
    },
    {
      "epoch": 0.4635230352303523,
      "step": 2138,
      "training_loss": 6.266782283782959
    },
    {
      "epoch": 0.4635230352303523,
      "step": 2138,
      "training_loss": 7.271057605743408
    },
    {
      "epoch": 0.463739837398374,
      "step": 2139,
      "training_loss": 6.352009296417236
    },
    {
      "epoch": 0.463739837398374,
      "step": 2139,
      "training_loss": 7.98709774017334
    },
    {
      "epoch": 0.463739837398374,
      "step": 2139,
      "training_loss": 3.683865547180176
    },
    {
      "epoch": 0.463739837398374,
      "step": 2139,
      "training_loss": 7.299898624420166
    },
    {
      "epoch": 0.46395663956639566,
      "grad_norm": 25.41069793701172,
      "learning_rate": 1e-05,
      "loss": 6.9143,
      "step": 2140
    },
    {
      "epoch": 0.46395663956639566,
      "step": 2140,
      "training_loss": 5.986573696136475
    },
    {
      "epoch": 0.46395663956639566,
      "step": 2140,
      "training_loss": 6.920000076293945
    },
    {
      "epoch": 0.46395663956639566,
      "step": 2140,
      "training_loss": 6.33341646194458
    },
    {
      "epoch": 0.46395663956639566,
      "step": 2140,
      "training_loss": 6.245964527130127
    },
    {
      "epoch": 0.4641734417344173,
      "step": 2141,
      "training_loss": 7.00506591796875
    },
    {
      "epoch": 0.4641734417344173,
      "step": 2141,
      "training_loss": 6.80034065246582
    },
    {
      "epoch": 0.4641734417344173,
      "step": 2141,
      "training_loss": 6.651463031768799
    },
    {
      "epoch": 0.4641734417344173,
      "step": 2141,
      "training_loss": 6.91184663772583
    },
    {
      "epoch": 0.46439024390243905,
      "step": 2142,
      "training_loss": 4.95501184463501
    },
    {
      "epoch": 0.46439024390243905,
      "step": 2142,
      "training_loss": 6.484686851501465
    },
    {
      "epoch": 0.46439024390243905,
      "step": 2142,
      "training_loss": 6.943003177642822
    },
    {
      "epoch": 0.46439024390243905,
      "step": 2142,
      "training_loss": 6.0545477867126465
    },
    {
      "epoch": 0.4646070460704607,
      "step": 2143,
      "training_loss": 6.236856460571289
    },
    {
      "epoch": 0.4646070460704607,
      "step": 2143,
      "training_loss": 3.533766269683838
    },
    {
      "epoch": 0.4646070460704607,
      "step": 2143,
      "training_loss": 6.796433925628662
    },
    {
      "epoch": 0.4646070460704607,
      "step": 2143,
      "training_loss": 7.514190196990967
    },
    {
      "epoch": 0.4648238482384824,
      "grad_norm": 18.47726058959961,
      "learning_rate": 1e-05,
      "loss": 6.3358,
      "step": 2144
    },
    {
      "epoch": 0.4648238482384824,
      "step": 2144,
      "training_loss": 6.672085762023926
    },
    {
      "epoch": 0.4648238482384824,
      "step": 2144,
      "training_loss": 7.161729335784912
    },
    {
      "epoch": 0.4648238482384824,
      "step": 2144,
      "training_loss": 7.072742938995361
    },
    {
      "epoch": 0.4648238482384824,
      "step": 2144,
      "training_loss": 6.203130722045898
    },
    {
      "epoch": 0.46504065040650405,
      "step": 2145,
      "training_loss": 7.443233966827393
    },
    {
      "epoch": 0.46504065040650405,
      "step": 2145,
      "training_loss": 6.921122074127197
    },
    {
      "epoch": 0.46504065040650405,
      "step": 2145,
      "training_loss": 6.472810745239258
    },
    {
      "epoch": 0.46504065040650405,
      "step": 2145,
      "training_loss": 6.909966945648193
    },
    {
      "epoch": 0.4652574525745257,
      "step": 2146,
      "training_loss": 6.75391149520874
    },
    {
      "epoch": 0.4652574525745257,
      "step": 2146,
      "training_loss": 6.610095024108887
    },
    {
      "epoch": 0.4652574525745257,
      "step": 2146,
      "training_loss": 6.664633274078369
    },
    {
      "epoch": 0.4652574525745257,
      "step": 2146,
      "training_loss": 7.48475980758667
    },
    {
      "epoch": 0.46547425474254744,
      "step": 2147,
      "training_loss": 7.239928722381592
    },
    {
      "epoch": 0.46547425474254744,
      "step": 2147,
      "training_loss": 3.659918785095215
    },
    {
      "epoch": 0.46547425474254744,
      "step": 2147,
      "training_loss": 6.987091064453125
    },
    {
      "epoch": 0.46547425474254744,
      "step": 2147,
      "training_loss": 4.461755275726318
    },
    {
      "epoch": 0.4656910569105691,
      "grad_norm": 18.19490623474121,
      "learning_rate": 1e-05,
      "loss": 6.5449,
      "step": 2148
    },
    {
      "epoch": 0.4656910569105691,
      "step": 2148,
      "training_loss": 7.173914432525635
    },
    {
      "epoch": 0.4656910569105691,
      "step": 2148,
      "training_loss": 6.328085422515869
    },
    {
      "epoch": 0.4656910569105691,
      "step": 2148,
      "training_loss": 4.208965301513672
    },
    {
      "epoch": 0.4656910569105691,
      "step": 2148,
      "training_loss": 4.911406517028809
    },
    {
      "epoch": 0.4659078590785908,
      "step": 2149,
      "training_loss": 7.342855453491211
    },
    {
      "epoch": 0.4659078590785908,
      "step": 2149,
      "training_loss": 6.61185359954834
    },
    {
      "epoch": 0.4659078590785908,
      "step": 2149,
      "training_loss": 4.894566535949707
    },
    {
      "epoch": 0.4659078590785908,
      "step": 2149,
      "training_loss": 7.635653018951416
    },
    {
      "epoch": 0.46612466124661245,
      "step": 2150,
      "training_loss": 9.291913986206055
    },
    {
      "epoch": 0.46612466124661245,
      "step": 2150,
      "training_loss": 6.767385005950928
    },
    {
      "epoch": 0.46612466124661245,
      "step": 2150,
      "training_loss": 7.217367172241211
    },
    {
      "epoch": 0.46612466124661245,
      "step": 2150,
      "training_loss": 6.396139144897461
    },
    {
      "epoch": 0.46634146341463417,
      "step": 2151,
      "training_loss": 6.6465959548950195
    },
    {
      "epoch": 0.46634146341463417,
      "step": 2151,
      "training_loss": 7.22846794128418
    },
    {
      "epoch": 0.46634146341463417,
      "step": 2151,
      "training_loss": 6.687197685241699
    },
    {
      "epoch": 0.46634146341463417,
      "step": 2151,
      "training_loss": 6.501733779907227
    },
    {
      "epoch": 0.46655826558265584,
      "grad_norm": 13.376550674438477,
      "learning_rate": 1e-05,
      "loss": 6.6153,
      "step": 2152
    },
    {
      "epoch": 0.46655826558265584,
      "step": 2152,
      "training_loss": 6.692609786987305
    },
    {
      "epoch": 0.46655826558265584,
      "step": 2152,
      "training_loss": 6.250265121459961
    },
    {
      "epoch": 0.46655826558265584,
      "step": 2152,
      "training_loss": 5.817099094390869
    },
    {
      "epoch": 0.46655826558265584,
      "step": 2152,
      "training_loss": 8.161811828613281
    },
    {
      "epoch": 0.4667750677506775,
      "step": 2153,
      "training_loss": 7.355982303619385
    },
    {
      "epoch": 0.4667750677506775,
      "step": 2153,
      "training_loss": 8.067769050598145
    },
    {
      "epoch": 0.4667750677506775,
      "step": 2153,
      "training_loss": 6.010601043701172
    },
    {
      "epoch": 0.4667750677506775,
      "step": 2153,
      "training_loss": 5.6292643547058105
    },
    {
      "epoch": 0.4669918699186992,
      "step": 2154,
      "training_loss": 7.415978908538818
    },
    {
      "epoch": 0.4669918699186992,
      "step": 2154,
      "training_loss": 5.727646827697754
    },
    {
      "epoch": 0.4669918699186992,
      "step": 2154,
      "training_loss": 5.758569717407227
    },
    {
      "epoch": 0.4669918699186992,
      "step": 2154,
      "training_loss": 5.182658672332764
    },
    {
      "epoch": 0.46720867208672084,
      "step": 2155,
      "training_loss": 5.144122123718262
    },
    {
      "epoch": 0.46720867208672084,
      "step": 2155,
      "training_loss": 6.176691055297852
    },
    {
      "epoch": 0.46720867208672084,
      "step": 2155,
      "training_loss": 7.124899864196777
    },
    {
      "epoch": 0.46720867208672084,
      "step": 2155,
      "training_loss": 8.700170516967773
    },
    {
      "epoch": 0.46742547425474257,
      "grad_norm": 14.180140495300293,
      "learning_rate": 1e-05,
      "loss": 6.576,
      "step": 2156
    },
    {
      "epoch": 0.46742547425474257,
      "step": 2156,
      "training_loss": 5.26520299911499
    },
    {
      "epoch": 0.46742547425474257,
      "step": 2156,
      "training_loss": 2.9493067264556885
    },
    {
      "epoch": 0.46742547425474257,
      "step": 2156,
      "training_loss": 6.957203388214111
    },
    {
      "epoch": 0.46742547425474257,
      "step": 2156,
      "training_loss": 7.516763210296631
    },
    {
      "epoch": 0.46764227642276424,
      "step": 2157,
      "training_loss": 6.5939860343933105
    },
    {
      "epoch": 0.46764227642276424,
      "step": 2157,
      "training_loss": 6.3711161613464355
    },
    {
      "epoch": 0.46764227642276424,
      "step": 2157,
      "training_loss": 3.0672271251678467
    },
    {
      "epoch": 0.46764227642276424,
      "step": 2157,
      "training_loss": 6.728316783905029
    },
    {
      "epoch": 0.4678590785907859,
      "step": 2158,
      "training_loss": 6.539564609527588
    },
    {
      "epoch": 0.4678590785907859,
      "step": 2158,
      "training_loss": 7.623879909515381
    },
    {
      "epoch": 0.4678590785907859,
      "step": 2158,
      "training_loss": 6.918992042541504
    },
    {
      "epoch": 0.4678590785907859,
      "step": 2158,
      "training_loss": 6.343848705291748
    },
    {
      "epoch": 0.46807588075880757,
      "step": 2159,
      "training_loss": 8.003928184509277
    },
    {
      "epoch": 0.46807588075880757,
      "step": 2159,
      "training_loss": 6.397050380706787
    },
    {
      "epoch": 0.46807588075880757,
      "step": 2159,
      "training_loss": 6.911990165710449
    },
    {
      "epoch": 0.46807588075880757,
      "step": 2159,
      "training_loss": 6.780984878540039
    },
    {
      "epoch": 0.4682926829268293,
      "grad_norm": 14.177202224731445,
      "learning_rate": 1e-05,
      "loss": 6.3106,
      "step": 2160
    },
    {
      "epoch": 0.4682926829268293,
      "step": 2160,
      "training_loss": 6.368317127227783
    },
    {
      "epoch": 0.4682926829268293,
      "step": 2160,
      "training_loss": 7.7912211418151855
    },
    {
      "epoch": 0.4682926829268293,
      "step": 2160,
      "training_loss": 6.948295593261719
    },
    {
      "epoch": 0.4682926829268293,
      "step": 2160,
      "training_loss": 7.109647274017334
    },
    {
      "epoch": 0.46850948509485096,
      "step": 2161,
      "training_loss": 5.593397617340088
    },
    {
      "epoch": 0.46850948509485096,
      "step": 2161,
      "training_loss": 7.112016677856445
    },
    {
      "epoch": 0.46850948509485096,
      "step": 2161,
      "training_loss": 7.088801383972168
    },
    {
      "epoch": 0.46850948509485096,
      "step": 2161,
      "training_loss": 5.906341552734375
    },
    {
      "epoch": 0.46872628726287263,
      "step": 2162,
      "training_loss": 7.398970603942871
    },
    {
      "epoch": 0.46872628726287263,
      "step": 2162,
      "training_loss": 4.586979389190674
    },
    {
      "epoch": 0.46872628726287263,
      "step": 2162,
      "training_loss": 3.6058425903320312
    },
    {
      "epoch": 0.46872628726287263,
      "step": 2162,
      "training_loss": 7.135986328125
    },
    {
      "epoch": 0.4689430894308943,
      "step": 2163,
      "training_loss": 7.209352016448975
    },
    {
      "epoch": 0.4689430894308943,
      "step": 2163,
      "training_loss": 6.689374923706055
    },
    {
      "epoch": 0.4689430894308943,
      "step": 2163,
      "training_loss": 7.2705488204956055
    },
    {
      "epoch": 0.4689430894308943,
      "step": 2163,
      "training_loss": 5.519228935241699
    },
    {
      "epoch": 0.46915989159891597,
      "grad_norm": 13.335283279418945,
      "learning_rate": 1e-05,
      "loss": 6.4584,
      "step": 2164
    },
    {
      "epoch": 0.46915989159891597,
      "step": 2164,
      "training_loss": 5.190263271331787
    },
    {
      "epoch": 0.46915989159891597,
      "step": 2164,
      "training_loss": 6.886358261108398
    },
    {
      "epoch": 0.46915989159891597,
      "step": 2164,
      "training_loss": 6.675905227661133
    },
    {
      "epoch": 0.46915989159891597,
      "step": 2164,
      "training_loss": 7.618011474609375
    },
    {
      "epoch": 0.4693766937669377,
      "step": 2165,
      "training_loss": 7.002774715423584
    },
    {
      "epoch": 0.4693766937669377,
      "step": 2165,
      "training_loss": 7.00547456741333
    },
    {
      "epoch": 0.4693766937669377,
      "step": 2165,
      "training_loss": 6.021491527557373
    },
    {
      "epoch": 0.4693766937669377,
      "step": 2165,
      "training_loss": 5.773613452911377
    },
    {
      "epoch": 0.46959349593495936,
      "step": 2166,
      "training_loss": 7.1015305519104
    },
    {
      "epoch": 0.46959349593495936,
      "step": 2166,
      "training_loss": 8.16207218170166
    },
    {
      "epoch": 0.46959349593495936,
      "step": 2166,
      "training_loss": 4.797689914703369
    },
    {
      "epoch": 0.46959349593495936,
      "step": 2166,
      "training_loss": 6.672578811645508
    },
    {
      "epoch": 0.469810298102981,
      "step": 2167,
      "training_loss": 7.564338684082031
    },
    {
      "epoch": 0.469810298102981,
      "step": 2167,
      "training_loss": 6.602536678314209
    },
    {
      "epoch": 0.469810298102981,
      "step": 2167,
      "training_loss": 4.929025173187256
    },
    {
      "epoch": 0.469810298102981,
      "step": 2167,
      "training_loss": 4.348989009857178
    },
    {
      "epoch": 0.4700271002710027,
      "grad_norm": 15.36961841583252,
      "learning_rate": 1e-05,
      "loss": 6.397,
      "step": 2168
    },
    {
      "epoch": 0.4700271002710027,
      "step": 2168,
      "training_loss": 6.292178630828857
    },
    {
      "epoch": 0.4700271002710027,
      "step": 2168,
      "training_loss": 7.018649578094482
    },
    {
      "epoch": 0.4700271002710027,
      "step": 2168,
      "training_loss": 5.775058269500732
    },
    {
      "epoch": 0.4700271002710027,
      "step": 2168,
      "training_loss": 6.597701072692871
    },
    {
      "epoch": 0.47024390243902436,
      "step": 2169,
      "training_loss": 5.949103832244873
    },
    {
      "epoch": 0.47024390243902436,
      "step": 2169,
      "training_loss": 4.789332866668701
    },
    {
      "epoch": 0.47024390243902436,
      "step": 2169,
      "training_loss": 8.058086395263672
    },
    {
      "epoch": 0.47024390243902436,
      "step": 2169,
      "training_loss": 6.399341106414795
    },
    {
      "epoch": 0.4704607046070461,
      "step": 2170,
      "training_loss": 5.6982831954956055
    },
    {
      "epoch": 0.4704607046070461,
      "step": 2170,
      "training_loss": 5.906915664672852
    },
    {
      "epoch": 0.4704607046070461,
      "step": 2170,
      "training_loss": 6.714604377746582
    },
    {
      "epoch": 0.4704607046070461,
      "step": 2170,
      "training_loss": 6.260424613952637
    },
    {
      "epoch": 0.47067750677506776,
      "step": 2171,
      "training_loss": 6.966450214385986
    },
    {
      "epoch": 0.47067750677506776,
      "step": 2171,
      "training_loss": 7.477149486541748
    },
    {
      "epoch": 0.47067750677506776,
      "step": 2171,
      "training_loss": 6.880398750305176
    },
    {
      "epoch": 0.47067750677506776,
      "step": 2171,
      "training_loss": 5.77354621887207
    },
    {
      "epoch": 0.4708943089430894,
      "grad_norm": 17.43536949157715,
      "learning_rate": 1e-05,
      "loss": 6.4098,
      "step": 2172
    },
    {
      "epoch": 0.4708943089430894,
      "step": 2172,
      "training_loss": 7.01482629776001
    },
    {
      "epoch": 0.4708943089430894,
      "step": 2172,
      "training_loss": 7.220313549041748
    },
    {
      "epoch": 0.4708943089430894,
      "step": 2172,
      "training_loss": 4.948552131652832
    },
    {
      "epoch": 0.4708943089430894,
      "step": 2172,
      "training_loss": 4.76727819442749
    },
    {
      "epoch": 0.4711111111111111,
      "step": 2173,
      "training_loss": 6.6633687019348145
    },
    {
      "epoch": 0.4711111111111111,
      "step": 2173,
      "training_loss": 7.140592098236084
    },
    {
      "epoch": 0.4711111111111111,
      "step": 2173,
      "training_loss": 6.493740558624268
    },
    {
      "epoch": 0.4711111111111111,
      "step": 2173,
      "training_loss": 7.504447937011719
    },
    {
      "epoch": 0.4713279132791328,
      "step": 2174,
      "training_loss": 7.390979766845703
    },
    {
      "epoch": 0.4713279132791328,
      "step": 2174,
      "training_loss": 4.196343421936035
    },
    {
      "epoch": 0.4713279132791328,
      "step": 2174,
      "training_loss": 5.993884086608887
    },
    {
      "epoch": 0.4713279132791328,
      "step": 2174,
      "training_loss": 8.778621673583984
    },
    {
      "epoch": 0.4715447154471545,
      "step": 2175,
      "training_loss": 4.755537986755371
    },
    {
      "epoch": 0.4715447154471545,
      "step": 2175,
      "training_loss": 6.395003318786621
    },
    {
      "epoch": 0.4715447154471545,
      "step": 2175,
      "training_loss": 6.377959728240967
    },
    {
      "epoch": 0.4715447154471545,
      "step": 2175,
      "training_loss": 7.024585247039795
    },
    {
      "epoch": 0.47176151761517615,
      "grad_norm": 17.293527603149414,
      "learning_rate": 1e-05,
      "loss": 6.4166,
      "step": 2176
    },
    {
      "epoch": 0.47176151761517615,
      "step": 2176,
      "training_loss": 7.003530025482178
    },
    {
      "epoch": 0.47176151761517615,
      "step": 2176,
      "training_loss": 5.390473365783691
    },
    {
      "epoch": 0.47176151761517615,
      "step": 2176,
      "training_loss": 5.731177806854248
    },
    {
      "epoch": 0.47176151761517615,
      "step": 2176,
      "training_loss": 6.444443702697754
    },
    {
      "epoch": 0.4719783197831978,
      "step": 2177,
      "training_loss": 6.621644496917725
    },
    {
      "epoch": 0.4719783197831978,
      "step": 2177,
      "training_loss": 7.285804271697998
    },
    {
      "epoch": 0.4719783197831978,
      "step": 2177,
      "training_loss": 4.502440929412842
    },
    {
      "epoch": 0.4719783197831978,
      "step": 2177,
      "training_loss": 7.11485481262207
    },
    {
      "epoch": 0.4721951219512195,
      "step": 2178,
      "training_loss": 6.933273792266846
    },
    {
      "epoch": 0.4721951219512195,
      "step": 2178,
      "training_loss": 7.765735626220703
    },
    {
      "epoch": 0.4721951219512195,
      "step": 2178,
      "training_loss": 6.070462703704834
    },
    {
      "epoch": 0.4721951219512195,
      "step": 2178,
      "training_loss": 6.451959133148193
    },
    {
      "epoch": 0.4724119241192412,
      "step": 2179,
      "training_loss": 6.641873836517334
    },
    {
      "epoch": 0.4724119241192412,
      "step": 2179,
      "training_loss": 6.851344585418701
    },
    {
      "epoch": 0.4724119241192412,
      "step": 2179,
      "training_loss": 6.666748046875
    },
    {
      "epoch": 0.4724119241192412,
      "step": 2179,
      "training_loss": 7.2606120109558105
    },
    {
      "epoch": 0.4726287262872629,
      "grad_norm": 12.003836631774902,
      "learning_rate": 1e-05,
      "loss": 6.546,
      "step": 2180
    },
    {
      "epoch": 0.4726287262872629,
      "step": 2180,
      "training_loss": 5.0567708015441895
    },
    {
      "epoch": 0.4726287262872629,
      "step": 2180,
      "training_loss": 5.955570220947266
    },
    {
      "epoch": 0.4726287262872629,
      "step": 2180,
      "training_loss": 6.974357604980469
    },
    {
      "epoch": 0.4726287262872629,
      "step": 2180,
      "training_loss": 6.987174034118652
    },
    {
      "epoch": 0.47284552845528455,
      "step": 2181,
      "training_loss": 5.697610378265381
    },
    {
      "epoch": 0.47284552845528455,
      "step": 2181,
      "training_loss": 6.236212253570557
    },
    {
      "epoch": 0.47284552845528455,
      "step": 2181,
      "training_loss": 6.338718891143799
    },
    {
      "epoch": 0.47284552845528455,
      "step": 2181,
      "training_loss": 6.49526834487915
    },
    {
      "epoch": 0.4730623306233062,
      "step": 2182,
      "training_loss": 6.662814140319824
    },
    {
      "epoch": 0.4730623306233062,
      "step": 2182,
      "training_loss": 6.769042015075684
    },
    {
      "epoch": 0.4730623306233062,
      "step": 2182,
      "training_loss": 7.399129867553711
    },
    {
      "epoch": 0.4730623306233062,
      "step": 2182,
      "training_loss": 5.551855564117432
    },
    {
      "epoch": 0.47327913279132794,
      "step": 2183,
      "training_loss": 5.690812587738037
    },
    {
      "epoch": 0.47327913279132794,
      "step": 2183,
      "training_loss": 9.392617225646973
    },
    {
      "epoch": 0.47327913279132794,
      "step": 2183,
      "training_loss": 4.966442108154297
    },
    {
      "epoch": 0.47327913279132794,
      "step": 2183,
      "training_loss": 8.030160903930664
    },
    {
      "epoch": 0.4734959349593496,
      "grad_norm": 20.582319259643555,
      "learning_rate": 1e-05,
      "loss": 6.5128,
      "step": 2184
    },
    {
      "epoch": 0.4734959349593496,
      "step": 2184,
      "training_loss": 6.72837495803833
    },
    {
      "epoch": 0.4734959349593496,
      "step": 2184,
      "training_loss": 7.170691013336182
    },
    {
      "epoch": 0.4734959349593496,
      "step": 2184,
      "training_loss": 6.698775768280029
    },
    {
      "epoch": 0.4734959349593496,
      "step": 2184,
      "training_loss": 7.629039287567139
    },
    {
      "epoch": 0.4737127371273713,
      "step": 2185,
      "training_loss": 5.9119672775268555
    },
    {
      "epoch": 0.4737127371273713,
      "step": 2185,
      "training_loss": 6.78621244430542
    },
    {
      "epoch": 0.4737127371273713,
      "step": 2185,
      "training_loss": 5.769906044006348
    },
    {
      "epoch": 0.4737127371273713,
      "step": 2185,
      "training_loss": 6.013615608215332
    },
    {
      "epoch": 0.47392953929539294,
      "step": 2186,
      "training_loss": 6.817533016204834
    },
    {
      "epoch": 0.47392953929539294,
      "step": 2186,
      "training_loss": 4.958930492401123
    },
    {
      "epoch": 0.47392953929539294,
      "step": 2186,
      "training_loss": 6.865540027618408
    },
    {
      "epoch": 0.47392953929539294,
      "step": 2186,
      "training_loss": 5.743076324462891
    },
    {
      "epoch": 0.4741463414634146,
      "step": 2187,
      "training_loss": 3.7636396884918213
    },
    {
      "epoch": 0.4741463414634146,
      "step": 2187,
      "training_loss": 6.266308784484863
    },
    {
      "epoch": 0.4741463414634146,
      "step": 2187,
      "training_loss": 3.3094472885131836
    },
    {
      "epoch": 0.4741463414634146,
      "step": 2187,
      "training_loss": 5.059552192687988
    },
    {
      "epoch": 0.47436314363143633,
      "grad_norm": 21.060997009277344,
      "learning_rate": 1e-05,
      "loss": 5.9683,
      "step": 2188
    },
    {
      "epoch": 0.47436314363143633,
      "step": 2188,
      "training_loss": 5.362034797668457
    },
    {
      "epoch": 0.47436314363143633,
      "step": 2188,
      "training_loss": 5.876223087310791
    },
    {
      "epoch": 0.47436314363143633,
      "step": 2188,
      "training_loss": 9.663310050964355
    },
    {
      "epoch": 0.47436314363143633,
      "step": 2188,
      "training_loss": 7.203699111938477
    },
    {
      "epoch": 0.474579945799458,
      "step": 2189,
      "training_loss": 5.539096832275391
    },
    {
      "epoch": 0.474579945799458,
      "step": 2189,
      "training_loss": 7.609835624694824
    },
    {
      "epoch": 0.474579945799458,
      "step": 2189,
      "training_loss": 6.975536823272705
    },
    {
      "epoch": 0.474579945799458,
      "step": 2189,
      "training_loss": 5.8605546951293945
    },
    {
      "epoch": 0.47479674796747967,
      "step": 2190,
      "training_loss": 6.6065521240234375
    },
    {
      "epoch": 0.47479674796747967,
      "step": 2190,
      "training_loss": 6.623866081237793
    },
    {
      "epoch": 0.47479674796747967,
      "step": 2190,
      "training_loss": 3.5206847190856934
    },
    {
      "epoch": 0.47479674796747967,
      "step": 2190,
      "training_loss": 5.5112738609313965
    },
    {
      "epoch": 0.47501355013550134,
      "step": 2191,
      "training_loss": 4.085875034332275
    },
    {
      "epoch": 0.47501355013550134,
      "step": 2191,
      "training_loss": 4.486404895782471
    },
    {
      "epoch": 0.47501355013550134,
      "step": 2191,
      "training_loss": 6.724985599517822
    },
    {
      "epoch": 0.47501355013550134,
      "step": 2191,
      "training_loss": 5.8256120681762695
    },
    {
      "epoch": 0.47523035230352306,
      "grad_norm": 17.874080657958984,
      "learning_rate": 1e-05,
      "loss": 6.0922,
      "step": 2192
    },
    {
      "epoch": 0.47523035230352306,
      "step": 2192,
      "training_loss": 7.453946113586426
    },
    {
      "epoch": 0.47523035230352306,
      "step": 2192,
      "training_loss": 7.612694263458252
    },
    {
      "epoch": 0.47523035230352306,
      "step": 2192,
      "training_loss": 6.603061199188232
    },
    {
      "epoch": 0.47523035230352306,
      "step": 2192,
      "training_loss": 6.89505672454834
    },
    {
      "epoch": 0.47544715447154473,
      "step": 2193,
      "training_loss": 6.464311122894287
    },
    {
      "epoch": 0.47544715447154473,
      "step": 2193,
      "training_loss": 6.282665252685547
    },
    {
      "epoch": 0.47544715447154473,
      "step": 2193,
      "training_loss": 7.341866970062256
    },
    {
      "epoch": 0.47544715447154473,
      "step": 2193,
      "training_loss": 6.912332057952881
    },
    {
      "epoch": 0.4756639566395664,
      "step": 2194,
      "training_loss": 6.721458911895752
    },
    {
      "epoch": 0.4756639566395664,
      "step": 2194,
      "training_loss": 7.627198696136475
    },
    {
      "epoch": 0.4756639566395664,
      "step": 2194,
      "training_loss": 7.666481971740723
    },
    {
      "epoch": 0.4756639566395664,
      "step": 2194,
      "training_loss": 7.600129127502441
    },
    {
      "epoch": 0.47588075880758807,
      "step": 2195,
      "training_loss": 6.352466106414795
    },
    {
      "epoch": 0.47588075880758807,
      "step": 2195,
      "training_loss": 6.754783630371094
    },
    {
      "epoch": 0.47588075880758807,
      "step": 2195,
      "training_loss": 6.395277976989746
    },
    {
      "epoch": 0.47588075880758807,
      "step": 2195,
      "training_loss": 7.8265509605407715
    },
    {
      "epoch": 0.47609756097560973,
      "grad_norm": 20.58833122253418,
      "learning_rate": 1e-05,
      "loss": 7.0319,
      "step": 2196
    },
    {
      "epoch": 0.47609756097560973,
      "step": 2196,
      "training_loss": 6.6424126625061035
    },
    {
      "epoch": 0.47609756097560973,
      "step": 2196,
      "training_loss": 6.300290584564209
    },
    {
      "epoch": 0.47609756097560973,
      "step": 2196,
      "training_loss": 6.872021198272705
    },
    {
      "epoch": 0.47609756097560973,
      "step": 2196,
      "training_loss": 5.639275074005127
    },
    {
      "epoch": 0.47631436314363146,
      "step": 2197,
      "training_loss": 6.231966495513916
    },
    {
      "epoch": 0.47631436314363146,
      "step": 2197,
      "training_loss": 7.134660720825195
    },
    {
      "epoch": 0.47631436314363146,
      "step": 2197,
      "training_loss": 6.831531047821045
    },
    {
      "epoch": 0.47631436314363146,
      "step": 2197,
      "training_loss": 6.256978511810303
    },
    {
      "epoch": 0.4765311653116531,
      "step": 2198,
      "training_loss": 7.2689409255981445
    },
    {
      "epoch": 0.4765311653116531,
      "step": 2198,
      "training_loss": 7.273171424865723
    },
    {
      "epoch": 0.4765311653116531,
      "step": 2198,
      "training_loss": 5.997986316680908
    },
    {
      "epoch": 0.4765311653116531,
      "step": 2198,
      "training_loss": 6.176175594329834
    },
    {
      "epoch": 0.4767479674796748,
      "step": 2199,
      "training_loss": 7.287072658538818
    },
    {
      "epoch": 0.4767479674796748,
      "step": 2199,
      "training_loss": 7.841551780700684
    },
    {
      "epoch": 0.4767479674796748,
      "step": 2199,
      "training_loss": 4.901468276977539
    },
    {
      "epoch": 0.4767479674796748,
      "step": 2199,
      "training_loss": 6.968730926513672
    },
    {
      "epoch": 0.47696476964769646,
      "grad_norm": 11.9918212890625,
      "learning_rate": 1e-05,
      "loss": 6.6015,
      "step": 2200
    },
    {
      "epoch": 0.47696476964769646,
      "step": 2200,
      "training_loss": 5.711770534515381
    },
    {
      "epoch": 0.47696476964769646,
      "step": 2200,
      "training_loss": 3.8615944385528564
    },
    {
      "epoch": 0.47696476964769646,
      "step": 2200,
      "training_loss": 7.603726863861084
    },
    {
      "epoch": 0.47696476964769646,
      "step": 2200,
      "training_loss": 4.545048713684082
    },
    {
      "epoch": 0.47718157181571813,
      "step": 2201,
      "training_loss": 5.379811763763428
    },
    {
      "epoch": 0.47718157181571813,
      "step": 2201,
      "training_loss": 7.566549301147461
    },
    {
      "epoch": 0.47718157181571813,
      "step": 2201,
      "training_loss": 6.333011627197266
    },
    {
      "epoch": 0.47718157181571813,
      "step": 2201,
      "training_loss": 6.679482936859131
    },
    {
      "epoch": 0.47739837398373985,
      "step": 2202,
      "training_loss": 7.888160705566406
    },
    {
      "epoch": 0.47739837398373985,
      "step": 2202,
      "training_loss": 4.916202545166016
    },
    {
      "epoch": 0.47739837398373985,
      "step": 2202,
      "training_loss": 5.805851459503174
    },
    {
      "epoch": 0.47739837398373985,
      "step": 2202,
      "training_loss": 6.334747314453125
    },
    {
      "epoch": 0.4776151761517615,
      "step": 2203,
      "training_loss": 5.61367130279541
    },
    {
      "epoch": 0.4776151761517615,
      "step": 2203,
      "training_loss": 6.36469841003418
    },
    {
      "epoch": 0.4776151761517615,
      "step": 2203,
      "training_loss": 7.035604476928711
    },
    {
      "epoch": 0.4776151761517615,
      "step": 2203,
      "training_loss": 6.665302753448486
    },
    {
      "epoch": 0.4778319783197832,
      "grad_norm": 19.725509643554688,
      "learning_rate": 1e-05,
      "loss": 6.1441,
      "step": 2204
    },
    {
      "epoch": 0.4778319783197832,
      "step": 2204,
      "training_loss": 6.411885738372803
    },
    {
      "epoch": 0.4778319783197832,
      "step": 2204,
      "training_loss": 7.605099678039551
    },
    {
      "epoch": 0.4778319783197832,
      "step": 2204,
      "training_loss": 7.791572570800781
    },
    {
      "epoch": 0.4778319783197832,
      "step": 2204,
      "training_loss": 5.8139801025390625
    },
    {
      "epoch": 0.47804878048780486,
      "step": 2205,
      "training_loss": 7.3807692527771
    },
    {
      "epoch": 0.47804878048780486,
      "step": 2205,
      "training_loss": 7.203433036804199
    },
    {
      "epoch": 0.47804878048780486,
      "step": 2205,
      "training_loss": 6.277950763702393
    },
    {
      "epoch": 0.47804878048780486,
      "step": 2205,
      "training_loss": 6.114619731903076
    },
    {
      "epoch": 0.4782655826558266,
      "step": 2206,
      "training_loss": 6.568579196929932
    },
    {
      "epoch": 0.4782655826558266,
      "step": 2206,
      "training_loss": 6.210142135620117
    },
    {
      "epoch": 0.4782655826558266,
      "step": 2206,
      "training_loss": 7.225123882293701
    },
    {
      "epoch": 0.4782655826558266,
      "step": 2206,
      "training_loss": 6.0068278312683105
    },
    {
      "epoch": 0.47848238482384825,
      "step": 2207,
      "training_loss": 4.836642265319824
    },
    {
      "epoch": 0.47848238482384825,
      "step": 2207,
      "training_loss": 4.224442958831787
    },
    {
      "epoch": 0.47848238482384825,
      "step": 2207,
      "training_loss": 3.58708119392395
    },
    {
      "epoch": 0.47848238482384825,
      "step": 2207,
      "training_loss": 7.597511291503906
    },
    {
      "epoch": 0.4786991869918699,
      "grad_norm": 28.649456024169922,
      "learning_rate": 1e-05,
      "loss": 6.3035,
      "step": 2208
    },
    {
      "epoch": 0.4786991869918699,
      "step": 2208,
      "training_loss": 6.89539098739624
    },
    {
      "epoch": 0.4786991869918699,
      "step": 2208,
      "training_loss": 6.694952487945557
    },
    {
      "epoch": 0.4786991869918699,
      "step": 2208,
      "training_loss": 5.668001651763916
    },
    {
      "epoch": 0.4786991869918699,
      "step": 2208,
      "training_loss": 4.3346848487854
    },
    {
      "epoch": 0.4789159891598916,
      "step": 2209,
      "training_loss": 7.654621124267578
    },
    {
      "epoch": 0.4789159891598916,
      "step": 2209,
      "training_loss": 6.527248382568359
    },
    {
      "epoch": 0.4789159891598916,
      "step": 2209,
      "training_loss": 6.8034467697143555
    },
    {
      "epoch": 0.4789159891598916,
      "step": 2209,
      "training_loss": 8.148858070373535
    },
    {
      "epoch": 0.47913279132791325,
      "step": 2210,
      "training_loss": 6.716458320617676
    },
    {
      "epoch": 0.47913279132791325,
      "step": 2210,
      "training_loss": 7.848501682281494
    },
    {
      "epoch": 0.47913279132791325,
      "step": 2210,
      "training_loss": 7.73091459274292
    },
    {
      "epoch": 0.47913279132791325,
      "step": 2210,
      "training_loss": 7.067748546600342
    },
    {
      "epoch": 0.479349593495935,
      "step": 2211,
      "training_loss": 4.28751802444458
    },
    {
      "epoch": 0.479349593495935,
      "step": 2211,
      "training_loss": 7.090115547180176
    },
    {
      "epoch": 0.479349593495935,
      "step": 2211,
      "training_loss": 5.184407711029053
    },
    {
      "epoch": 0.479349593495935,
      "step": 2211,
      "training_loss": 4.5266265869140625
    },
    {
      "epoch": 0.47956639566395665,
      "grad_norm": 17.202896118164062,
      "learning_rate": 1e-05,
      "loss": 6.4487,
      "step": 2212
    },
    {
      "epoch": 0.47956639566395665,
      "step": 2212,
      "training_loss": 5.714822769165039
    },
    {
      "epoch": 0.47956639566395665,
      "step": 2212,
      "training_loss": 5.558197021484375
    },
    {
      "epoch": 0.47956639566395665,
      "step": 2212,
      "training_loss": 6.555974006652832
    },
    {
      "epoch": 0.47956639566395665,
      "step": 2212,
      "training_loss": 6.984312057495117
    },
    {
      "epoch": 0.4797831978319783,
      "step": 2213,
      "training_loss": 4.520336627960205
    },
    {
      "epoch": 0.4797831978319783,
      "step": 2213,
      "training_loss": 6.516114711761475
    },
    {
      "epoch": 0.4797831978319783,
      "step": 2213,
      "training_loss": 7.558360576629639
    },
    {
      "epoch": 0.4797831978319783,
      "step": 2213,
      "training_loss": 5.916435241699219
    },
    {
      "epoch": 0.48,
      "step": 2214,
      "training_loss": 6.447150707244873
    },
    {
      "epoch": 0.48,
      "step": 2214,
      "training_loss": 6.38441276550293
    },
    {
      "epoch": 0.48,
      "step": 2214,
      "training_loss": 6.514179229736328
    },
    {
      "epoch": 0.48,
      "step": 2214,
      "training_loss": 6.350352764129639
    },
    {
      "epoch": 0.4802168021680217,
      "step": 2215,
      "training_loss": 5.7538042068481445
    },
    {
      "epoch": 0.4802168021680217,
      "step": 2215,
      "training_loss": 6.863561153411865
    },
    {
      "epoch": 0.4802168021680217,
      "step": 2215,
      "training_loss": 4.562564849853516
    },
    {
      "epoch": 0.4802168021680217,
      "step": 2215,
      "training_loss": 5.3545942306518555
    },
    {
      "epoch": 0.4804336043360434,
      "grad_norm": 16.213993072509766,
      "learning_rate": 1e-05,
      "loss": 6.0972,
      "step": 2216
    },
    {
      "epoch": 0.4804336043360434,
      "step": 2216,
      "training_loss": 6.847814083099365
    },
    {
      "epoch": 0.4804336043360434,
      "step": 2216,
      "training_loss": 7.093061447143555
    },
    {
      "epoch": 0.4804336043360434,
      "step": 2216,
      "training_loss": 3.9542646408081055
    },
    {
      "epoch": 0.4804336043360434,
      "step": 2216,
      "training_loss": 6.2798848152160645
    },
    {
      "epoch": 0.48065040650406504,
      "step": 2217,
      "training_loss": 6.775182723999023
    },
    {
      "epoch": 0.48065040650406504,
      "step": 2217,
      "training_loss": 6.471825122833252
    },
    {
      "epoch": 0.48065040650406504,
      "step": 2217,
      "training_loss": 6.428801536560059
    },
    {
      "epoch": 0.48065040650406504,
      "step": 2217,
      "training_loss": 6.603602886199951
    },
    {
      "epoch": 0.4808672086720867,
      "step": 2218,
      "training_loss": 5.748806476593018
    },
    {
      "epoch": 0.4808672086720867,
      "step": 2218,
      "training_loss": 6.4040422439575195
    },
    {
      "epoch": 0.4808672086720867,
      "step": 2218,
      "training_loss": 7.623016834259033
    },
    {
      "epoch": 0.4808672086720867,
      "step": 2218,
      "training_loss": 5.794885158538818
    },
    {
      "epoch": 0.4810840108401084,
      "step": 2219,
      "training_loss": 7.071025848388672
    },
    {
      "epoch": 0.4810840108401084,
      "step": 2219,
      "training_loss": 6.677340984344482
    },
    {
      "epoch": 0.4810840108401084,
      "step": 2219,
      "training_loss": 6.729739189147949
    },
    {
      "epoch": 0.4810840108401084,
      "step": 2219,
      "training_loss": 6.771897315979004
    },
    {
      "epoch": 0.4813008130081301,
      "grad_norm": 21.96796417236328,
      "learning_rate": 1e-05,
      "loss": 6.4547,
      "step": 2220
    },
    {
      "epoch": 0.4813008130081301,
      "step": 2220,
      "training_loss": 6.046440601348877
    },
    {
      "epoch": 0.4813008130081301,
      "step": 2220,
      "training_loss": 7.800911903381348
    },
    {
      "epoch": 0.4813008130081301,
      "step": 2220,
      "training_loss": 4.691683769226074
    },
    {
      "epoch": 0.4813008130081301,
      "step": 2220,
      "training_loss": 6.650667190551758
    },
    {
      "epoch": 0.48151761517615177,
      "step": 2221,
      "training_loss": 7.044699668884277
    },
    {
      "epoch": 0.48151761517615177,
      "step": 2221,
      "training_loss": 8.067849159240723
    },
    {
      "epoch": 0.48151761517615177,
      "step": 2221,
      "training_loss": 6.533266544342041
    },
    {
      "epoch": 0.48151761517615177,
      "step": 2221,
      "training_loss": 6.217283248901367
    },
    {
      "epoch": 0.48173441734417344,
      "step": 2222,
      "training_loss": 6.689116477966309
    },
    {
      "epoch": 0.48173441734417344,
      "step": 2222,
      "training_loss": 5.850881099700928
    },
    {
      "epoch": 0.48173441734417344,
      "step": 2222,
      "training_loss": 6.895906925201416
    },
    {
      "epoch": 0.48173441734417344,
      "step": 2222,
      "training_loss": 7.501873970031738
    },
    {
      "epoch": 0.4819512195121951,
      "step": 2223,
      "training_loss": 3.6660468578338623
    },
    {
      "epoch": 0.4819512195121951,
      "step": 2223,
      "training_loss": 6.973357677459717
    },
    {
      "epoch": 0.4819512195121951,
      "step": 2223,
      "training_loss": 6.374269008636475
    },
    {
      "epoch": 0.4819512195121951,
      "step": 2223,
      "training_loss": 5.231747627258301
    },
    {
      "epoch": 0.48216802168021683,
      "grad_norm": 15.084522247314453,
      "learning_rate": 1e-05,
      "loss": 6.3898,
      "step": 2224
    },
    {
      "epoch": 0.48216802168021683,
      "step": 2224,
      "training_loss": 7.363863468170166
    },
    {
      "epoch": 0.48216802168021683,
      "step": 2224,
      "training_loss": 6.659788131713867
    },
    {
      "epoch": 0.48216802168021683,
      "step": 2224,
      "training_loss": 3.518080472946167
    },
    {
      "epoch": 0.48216802168021683,
      "step": 2224,
      "training_loss": 7.708459377288818
    },
    {
      "epoch": 0.4823848238482385,
      "step": 2225,
      "training_loss": 5.5179667472839355
    },
    {
      "epoch": 0.4823848238482385,
      "step": 2225,
      "training_loss": 6.712329387664795
    },
    {
      "epoch": 0.4823848238482385,
      "step": 2225,
      "training_loss": 5.675415515899658
    },
    {
      "epoch": 0.4823848238482385,
      "step": 2225,
      "training_loss": 6.371942043304443
    },
    {
      "epoch": 0.48260162601626017,
      "step": 2226,
      "training_loss": 6.178604602813721
    },
    {
      "epoch": 0.48260162601626017,
      "step": 2226,
      "training_loss": 7.300015926361084
    },
    {
      "epoch": 0.48260162601626017,
      "step": 2226,
      "training_loss": 7.558740615844727
    },
    {
      "epoch": 0.48260162601626017,
      "step": 2226,
      "training_loss": 7.302578449249268
    },
    {
      "epoch": 0.48281842818428183,
      "step": 2227,
      "training_loss": 6.5699462890625
    },
    {
      "epoch": 0.48281842818428183,
      "step": 2227,
      "training_loss": 6.025918960571289
    },
    {
      "epoch": 0.48281842818428183,
      "step": 2227,
      "training_loss": 6.902434825897217
    },
    {
      "epoch": 0.48281842818428183,
      "step": 2227,
      "training_loss": 5.0756402015686035
    },
    {
      "epoch": 0.4830352303523035,
      "grad_norm": 15.240760803222656,
      "learning_rate": 1e-05,
      "loss": 6.4026,
      "step": 2228
    },
    {
      "epoch": 0.4830352303523035,
      "step": 2228,
      "training_loss": 6.5376996994018555
    },
    {
      "epoch": 0.4830352303523035,
      "step": 2228,
      "training_loss": 7.276147365570068
    },
    {
      "epoch": 0.4830352303523035,
      "step": 2228,
      "training_loss": 6.894802093505859
    },
    {
      "epoch": 0.4830352303523035,
      "step": 2228,
      "training_loss": 6.7445855140686035
    },
    {
      "epoch": 0.4832520325203252,
      "step": 2229,
      "training_loss": 6.057715892791748
    },
    {
      "epoch": 0.4832520325203252,
      "step": 2229,
      "training_loss": 7.061668395996094
    },
    {
      "epoch": 0.4832520325203252,
      "step": 2229,
      "training_loss": 7.507906913757324
    },
    {
      "epoch": 0.4832520325203252,
      "step": 2229,
      "training_loss": 7.063949108123779
    },
    {
      "epoch": 0.4834688346883469,
      "step": 2230,
      "training_loss": 6.247913837432861
    },
    {
      "epoch": 0.4834688346883469,
      "step": 2230,
      "training_loss": 5.215302467346191
    },
    {
      "epoch": 0.4834688346883469,
      "step": 2230,
      "training_loss": 6.931148052215576
    },
    {
      "epoch": 0.4834688346883469,
      "step": 2230,
      "training_loss": 6.6376633644104
    },
    {
      "epoch": 0.48368563685636856,
      "step": 2231,
      "training_loss": 6.093391418457031
    },
    {
      "epoch": 0.48368563685636856,
      "step": 2231,
      "training_loss": 7.228242874145508
    },
    {
      "epoch": 0.48368563685636856,
      "step": 2231,
      "training_loss": 5.5787458419799805
    },
    {
      "epoch": 0.48368563685636856,
      "step": 2231,
      "training_loss": 6.634793281555176
    },
    {
      "epoch": 0.48390243902439023,
      "grad_norm": 15.879256248474121,
      "learning_rate": 1e-05,
      "loss": 6.607,
      "step": 2232
    },
    {
      "epoch": 0.48390243902439023,
      "step": 2232,
      "training_loss": 8.262906074523926
    },
    {
      "epoch": 0.48390243902439023,
      "step": 2232,
      "training_loss": 6.766924858093262
    },
    {
      "epoch": 0.48390243902439023,
      "step": 2232,
      "training_loss": 6.254645824432373
    },
    {
      "epoch": 0.48390243902439023,
      "step": 2232,
      "training_loss": 6.942315101623535
    },
    {
      "epoch": 0.4841192411924119,
      "step": 2233,
      "training_loss": 7.989124298095703
    },
    {
      "epoch": 0.4841192411924119,
      "step": 2233,
      "training_loss": 4.524018287658691
    },
    {
      "epoch": 0.4841192411924119,
      "step": 2233,
      "training_loss": 6.585621356964111
    },
    {
      "epoch": 0.4841192411924119,
      "step": 2233,
      "training_loss": 2.2442526817321777
    },
    {
      "epoch": 0.4843360433604336,
      "step": 2234,
      "training_loss": 4.866176605224609
    },
    {
      "epoch": 0.4843360433604336,
      "step": 2234,
      "training_loss": 6.106842517852783
    },
    {
      "epoch": 0.4843360433604336,
      "step": 2234,
      "training_loss": 5.517646789550781
    },
    {
      "epoch": 0.4843360433604336,
      "step": 2234,
      "training_loss": 7.0800957679748535
    },
    {
      "epoch": 0.4845528455284553,
      "step": 2235,
      "training_loss": 8.504114151000977
    },
    {
      "epoch": 0.4845528455284553,
      "step": 2235,
      "training_loss": 6.81158971786499
    },
    {
      "epoch": 0.4845528455284553,
      "step": 2235,
      "training_loss": 5.034693241119385
    },
    {
      "epoch": 0.4845528455284553,
      "step": 2235,
      "training_loss": 7.0736799240112305
    },
    {
      "epoch": 0.48476964769647696,
      "grad_norm": 17.149242401123047,
      "learning_rate": 1e-05,
      "loss": 6.2853,
      "step": 2236
    },
    {
      "epoch": 0.48476964769647696,
      "step": 2236,
      "training_loss": 7.276619911193848
    },
    {
      "epoch": 0.48476964769647696,
      "step": 2236,
      "training_loss": 6.0815019607543945
    },
    {
      "epoch": 0.48476964769647696,
      "step": 2236,
      "training_loss": 5.508242607116699
    },
    {
      "epoch": 0.48476964769647696,
      "step": 2236,
      "training_loss": 6.49615478515625
    },
    {
      "epoch": 0.4849864498644986,
      "step": 2237,
      "training_loss": 7.315033912658691
    },
    {
      "epoch": 0.4849864498644986,
      "step": 2237,
      "training_loss": 7.984452247619629
    },
    {
      "epoch": 0.4849864498644986,
      "step": 2237,
      "training_loss": 6.743205547332764
    },
    {
      "epoch": 0.4849864498644986,
      "step": 2237,
      "training_loss": 9.656922340393066
    },
    {
      "epoch": 0.48520325203252035,
      "step": 2238,
      "training_loss": 6.700328350067139
    },
    {
      "epoch": 0.48520325203252035,
      "step": 2238,
      "training_loss": 7.09772253036499
    },
    {
      "epoch": 0.48520325203252035,
      "step": 2238,
      "training_loss": 6.787154197692871
    },
    {
      "epoch": 0.48520325203252035,
      "step": 2238,
      "training_loss": 6.940838813781738
    },
    {
      "epoch": 0.485420054200542,
      "step": 2239,
      "training_loss": 8.408295631408691
    },
    {
      "epoch": 0.485420054200542,
      "step": 2239,
      "training_loss": 3.5971341133117676
    },
    {
      "epoch": 0.485420054200542,
      "step": 2239,
      "training_loss": 7.5379862785339355
    },
    {
      "epoch": 0.485420054200542,
      "step": 2239,
      "training_loss": 6.79797887802124
    },
    {
      "epoch": 0.4856368563685637,
      "grad_norm": 17.206987380981445,
      "learning_rate": 1e-05,
      "loss": 6.9331,
      "step": 2240
    },
    {
      "epoch": 0.4856368563685637,
      "step": 2240,
      "training_loss": 4.543095588684082
    },
    {
      "epoch": 0.4856368563685637,
      "step": 2240,
      "training_loss": 7.948297500610352
    },
    {
      "epoch": 0.4856368563685637,
      "step": 2240,
      "training_loss": 5.617002487182617
    },
    {
      "epoch": 0.4856368563685637,
      "step": 2240,
      "training_loss": 7.659337520599365
    },
    {
      "epoch": 0.48585365853658535,
      "step": 2241,
      "training_loss": 6.286795139312744
    },
    {
      "epoch": 0.48585365853658535,
      "step": 2241,
      "training_loss": 6.484565734863281
    },
    {
      "epoch": 0.48585365853658535,
      "step": 2241,
      "training_loss": 4.607598304748535
    },
    {
      "epoch": 0.48585365853658535,
      "step": 2241,
      "training_loss": 7.159765243530273
    },
    {
      "epoch": 0.486070460704607,
      "step": 2242,
      "training_loss": 6.594743251800537
    },
    {
      "epoch": 0.486070460704607,
      "step": 2242,
      "training_loss": 6.858438491821289
    },
    {
      "epoch": 0.486070460704607,
      "step": 2242,
      "training_loss": 7.667047500610352
    },
    {
      "epoch": 0.486070460704607,
      "step": 2242,
      "training_loss": 8.023086547851562
    },
    {
      "epoch": 0.48628726287262874,
      "step": 2243,
      "training_loss": 5.039214611053467
    },
    {
      "epoch": 0.48628726287262874,
      "step": 2243,
      "training_loss": 6.107456684112549
    },
    {
      "epoch": 0.48628726287262874,
      "step": 2243,
      "training_loss": 8.531391143798828
    },
    {
      "epoch": 0.48628726287262874,
      "step": 2243,
      "training_loss": 6.809931755065918
    },
    {
      "epoch": 0.4865040650406504,
      "grad_norm": 14.544271469116211,
      "learning_rate": 1e-05,
      "loss": 6.6211,
      "step": 2244
    },
    {
      "epoch": 0.4865040650406504,
      "step": 2244,
      "training_loss": 6.274964332580566
    },
    {
      "epoch": 0.4865040650406504,
      "step": 2244,
      "training_loss": 7.455224514007568
    },
    {
      "epoch": 0.4865040650406504,
      "step": 2244,
      "training_loss": 6.333042621612549
    },
    {
      "epoch": 0.4865040650406504,
      "step": 2244,
      "training_loss": 6.393498420715332
    },
    {
      "epoch": 0.4867208672086721,
      "step": 2245,
      "training_loss": 7.245482444763184
    },
    {
      "epoch": 0.4867208672086721,
      "step": 2245,
      "training_loss": 3.871520757675171
    },
    {
      "epoch": 0.4867208672086721,
      "step": 2245,
      "training_loss": 6.352355003356934
    },
    {
      "epoch": 0.4867208672086721,
      "step": 2245,
      "training_loss": 7.575072765350342
    },
    {
      "epoch": 0.48693766937669375,
      "step": 2246,
      "training_loss": 6.576502799987793
    },
    {
      "epoch": 0.48693766937669375,
      "step": 2246,
      "training_loss": 7.179531097412109
    },
    {
      "epoch": 0.48693766937669375,
      "step": 2246,
      "training_loss": 6.847975730895996
    },
    {
      "epoch": 0.48693766937669375,
      "step": 2246,
      "training_loss": 6.954992771148682
    },
    {
      "epoch": 0.4871544715447155,
      "step": 2247,
      "training_loss": 6.385715007781982
    },
    {
      "epoch": 0.4871544715447155,
      "step": 2247,
      "training_loss": 7.046294212341309
    },
    {
      "epoch": 0.4871544715447155,
      "step": 2247,
      "training_loss": 7.397654056549072
    },
    {
      "epoch": 0.4871544715447155,
      "step": 2247,
      "training_loss": 6.813465118408203
    },
    {
      "epoch": 0.48737127371273714,
      "grad_norm": 12.666391372680664,
      "learning_rate": 1e-05,
      "loss": 6.669,
      "step": 2248
    },
    {
      "epoch": 0.48737127371273714,
      "step": 2248,
      "training_loss": 6.898871898651123
    },
    {
      "epoch": 0.48737127371273714,
      "step": 2248,
      "training_loss": 6.477575778961182
    },
    {
      "epoch": 0.48737127371273714,
      "step": 2248,
      "training_loss": 5.708544731140137
    },
    {
      "epoch": 0.48737127371273714,
      "step": 2248,
      "training_loss": 7.430423259735107
    },
    {
      "epoch": 0.4875880758807588,
      "step": 2249,
      "training_loss": 7.218667507171631
    },
    {
      "epoch": 0.4875880758807588,
      "step": 2249,
      "training_loss": 6.843708038330078
    },
    {
      "epoch": 0.4875880758807588,
      "step": 2249,
      "training_loss": 4.491041660308838
    },
    {
      "epoch": 0.4875880758807588,
      "step": 2249,
      "training_loss": 7.4831671714782715
    },
    {
      "epoch": 0.4878048780487805,
      "step": 2250,
      "training_loss": 6.563544273376465
    },
    {
      "epoch": 0.4878048780487805,
      "step": 2250,
      "training_loss": 7.285464763641357
    },
    {
      "epoch": 0.4878048780487805,
      "step": 2250,
      "training_loss": 6.93589973449707
    },
    {
      "epoch": 0.4878048780487805,
      "step": 2250,
      "training_loss": 6.618803024291992
    },
    {
      "epoch": 0.48802168021680215,
      "step": 2251,
      "training_loss": 6.963047981262207
    },
    {
      "epoch": 0.48802168021680215,
      "step": 2251,
      "training_loss": 7.153036594390869
    },
    {
      "epoch": 0.48802168021680215,
      "step": 2251,
      "training_loss": 5.7214813232421875
    },
    {
      "epoch": 0.48802168021680215,
      "step": 2251,
      "training_loss": 6.813528060913086
    },
    {
      "epoch": 0.48823848238482387,
      "grad_norm": 14.425642013549805,
      "learning_rate": 1e-05,
      "loss": 6.6629,
      "step": 2252
    },
    {
      "epoch": 0.48823848238482387,
      "step": 2252,
      "training_loss": 5.518604755401611
    },
    {
      "epoch": 0.48823848238482387,
      "step": 2252,
      "training_loss": 5.232369899749756
    },
    {
      "epoch": 0.48823848238482387,
      "step": 2252,
      "training_loss": 7.515347957611084
    },
    {
      "epoch": 0.48823848238482387,
      "step": 2252,
      "training_loss": 6.883315086364746
    },
    {
      "epoch": 0.48845528455284554,
      "step": 2253,
      "training_loss": 6.990324020385742
    },
    {
      "epoch": 0.48845528455284554,
      "step": 2253,
      "training_loss": 6.448753356933594
    },
    {
      "epoch": 0.48845528455284554,
      "step": 2253,
      "training_loss": 7.511164665222168
    },
    {
      "epoch": 0.48845528455284554,
      "step": 2253,
      "training_loss": 7.369927883148193
    },
    {
      "epoch": 0.4886720867208672,
      "step": 2254,
      "training_loss": 7.317133903503418
    },
    {
      "epoch": 0.4886720867208672,
      "step": 2254,
      "training_loss": 3.6577045917510986
    },
    {
      "epoch": 0.4886720867208672,
      "step": 2254,
      "training_loss": 6.630378246307373
    },
    {
      "epoch": 0.4886720867208672,
      "step": 2254,
      "training_loss": 7.554955005645752
    },
    {
      "epoch": 0.4888888888888889,
      "step": 2255,
      "training_loss": 5.779975414276123
    },
    {
      "epoch": 0.4888888888888889,
      "step": 2255,
      "training_loss": 5.30153751373291
    },
    {
      "epoch": 0.4888888888888889,
      "step": 2255,
      "training_loss": 6.736604690551758
    },
    {
      "epoch": 0.4888888888888889,
      "step": 2255,
      "training_loss": 6.2204155921936035
    },
    {
      "epoch": 0.4891056910569106,
      "grad_norm": 17.90928840637207,
      "learning_rate": 1e-05,
      "loss": 6.4168,
      "step": 2256
    },
    {
      "epoch": 0.4891056910569106,
      "step": 2256,
      "training_loss": 6.708611488342285
    },
    {
      "epoch": 0.4891056910569106,
      "step": 2256,
      "training_loss": 6.291678428649902
    },
    {
      "epoch": 0.4891056910569106,
      "step": 2256,
      "training_loss": 5.61836051940918
    },
    {
      "epoch": 0.4891056910569106,
      "step": 2256,
      "training_loss": 5.571348667144775
    },
    {
      "epoch": 0.48932249322493226,
      "step": 2257,
      "training_loss": 7.646553993225098
    },
    {
      "epoch": 0.48932249322493226,
      "step": 2257,
      "training_loss": 6.989979267120361
    },
    {
      "epoch": 0.48932249322493226,
      "step": 2257,
      "training_loss": 6.81678581237793
    },
    {
      "epoch": 0.48932249322493226,
      "step": 2257,
      "training_loss": 7.68934440612793
    },
    {
      "epoch": 0.48953929539295393,
      "step": 2258,
      "training_loss": 6.3247389793396
    },
    {
      "epoch": 0.48953929539295393,
      "step": 2258,
      "training_loss": 6.775909900665283
    },
    {
      "epoch": 0.48953929539295393,
      "step": 2258,
      "training_loss": 5.663026809692383
    },
    {
      "epoch": 0.48953929539295393,
      "step": 2258,
      "training_loss": 6.977346897125244
    },
    {
      "epoch": 0.4897560975609756,
      "step": 2259,
      "training_loss": 6.290027618408203
    },
    {
      "epoch": 0.4897560975609756,
      "step": 2259,
      "training_loss": 6.470606803894043
    },
    {
      "epoch": 0.4897560975609756,
      "step": 2259,
      "training_loss": 5.307836055755615
    },
    {
      "epoch": 0.4897560975609756,
      "step": 2259,
      "training_loss": 9.709000587463379
    },
    {
      "epoch": 0.48997289972899727,
      "grad_norm": 23.28272819519043,
      "learning_rate": 1e-05,
      "loss": 6.6782,
      "step": 2260
    },
    {
      "epoch": 0.48997289972899727,
      "step": 2260,
      "training_loss": 8.089439392089844
    },
    {
      "epoch": 0.48997289972899727,
      "step": 2260,
      "training_loss": 6.60306453704834
    },
    {
      "epoch": 0.48997289972899727,
      "step": 2260,
      "training_loss": 7.176336288452148
    },
    {
      "epoch": 0.48997289972899727,
      "step": 2260,
      "training_loss": 7.608471393585205
    },
    {
      "epoch": 0.490189701897019,
      "step": 2261,
      "training_loss": 6.591311454772949
    },
    {
      "epoch": 0.490189701897019,
      "step": 2261,
      "training_loss": 7.3173828125
    },
    {
      "epoch": 0.490189701897019,
      "step": 2261,
      "training_loss": 7.2108473777771
    },
    {
      "epoch": 0.490189701897019,
      "step": 2261,
      "training_loss": 6.207812309265137
    },
    {
      "epoch": 0.49040650406504066,
      "step": 2262,
      "training_loss": 7.635803699493408
    },
    {
      "epoch": 0.49040650406504066,
      "step": 2262,
      "training_loss": 6.963307857513428
    },
    {
      "epoch": 0.49040650406504066,
      "step": 2262,
      "training_loss": 6.977299690246582
    },
    {
      "epoch": 0.49040650406504066,
      "step": 2262,
      "training_loss": 6.888594627380371
    },
    {
      "epoch": 0.49062330623306233,
      "step": 2263,
      "training_loss": 6.7086029052734375
    },
    {
      "epoch": 0.49062330623306233,
      "step": 2263,
      "training_loss": 6.467238903045654
    },
    {
      "epoch": 0.49062330623306233,
      "step": 2263,
      "training_loss": 5.785419940948486
    },
    {
      "epoch": 0.49062330623306233,
      "step": 2263,
      "training_loss": 6.995140075683594
    },
    {
      "epoch": 0.490840108401084,
      "grad_norm": 17.885528564453125,
      "learning_rate": 1e-05,
      "loss": 6.9516,
      "step": 2264
    },
    {
      "epoch": 0.490840108401084,
      "step": 2264,
      "training_loss": 6.912203788757324
    },
    {
      "epoch": 0.490840108401084,
      "step": 2264,
      "training_loss": 8.283883094787598
    },
    {
      "epoch": 0.490840108401084,
      "step": 2264,
      "training_loss": 8.828248977661133
    },
    {
      "epoch": 0.490840108401084,
      "step": 2264,
      "training_loss": 6.967674255371094
    },
    {
      "epoch": 0.49105691056910566,
      "step": 2265,
      "training_loss": 7.2417755126953125
    },
    {
      "epoch": 0.49105691056910566,
      "step": 2265,
      "training_loss": 5.7398600578308105
    },
    {
      "epoch": 0.49105691056910566,
      "step": 2265,
      "training_loss": 6.588339805603027
    },
    {
      "epoch": 0.49105691056910566,
      "step": 2265,
      "training_loss": 6.610048770904541
    },
    {
      "epoch": 0.4912737127371274,
      "step": 2266,
      "training_loss": 5.987502098083496
    },
    {
      "epoch": 0.4912737127371274,
      "step": 2266,
      "training_loss": 6.583536148071289
    },
    {
      "epoch": 0.4912737127371274,
      "step": 2266,
      "training_loss": 6.8283185958862305
    },
    {
      "epoch": 0.4912737127371274,
      "step": 2266,
      "training_loss": 7.05972957611084
    },
    {
      "epoch": 0.49149051490514906,
      "step": 2267,
      "training_loss": 6.593754768371582
    },
    {
      "epoch": 0.49149051490514906,
      "step": 2267,
      "training_loss": 6.898104190826416
    },
    {
      "epoch": 0.49149051490514906,
      "step": 2267,
      "training_loss": 6.087527751922607
    },
    {
      "epoch": 0.49149051490514906,
      "step": 2267,
      "training_loss": 6.791353702545166
    },
    {
      "epoch": 0.4917073170731707,
      "grad_norm": 14.27170181274414,
      "learning_rate": 1e-05,
      "loss": 6.8751,
      "step": 2268
    },
    {
      "epoch": 0.4917073170731707,
      "step": 2268,
      "training_loss": 6.571778297424316
    },
    {
      "epoch": 0.4917073170731707,
      "step": 2268,
      "training_loss": 4.619285583496094
    },
    {
      "epoch": 0.4917073170731707,
      "step": 2268,
      "training_loss": 6.187070846557617
    },
    {
      "epoch": 0.4917073170731707,
      "step": 2268,
      "training_loss": 5.249757289886475
    },
    {
      "epoch": 0.4919241192411924,
      "step": 2269,
      "training_loss": 5.724222660064697
    },
    {
      "epoch": 0.4919241192411924,
      "step": 2269,
      "training_loss": 4.857265472412109
    },
    {
      "epoch": 0.4919241192411924,
      "step": 2269,
      "training_loss": 7.3162078857421875
    },
    {
      "epoch": 0.4919241192411924,
      "step": 2269,
      "training_loss": 6.95605993270874
    },
    {
      "epoch": 0.4921409214092141,
      "step": 2270,
      "training_loss": 7.3238301277160645
    },
    {
      "epoch": 0.4921409214092141,
      "step": 2270,
      "training_loss": 4.467048168182373
    },
    {
      "epoch": 0.4921409214092141,
      "step": 2270,
      "training_loss": 6.836942672729492
    },
    {
      "epoch": 0.4921409214092141,
      "step": 2270,
      "training_loss": 7.405154228210449
    },
    {
      "epoch": 0.4923577235772358,
      "step": 2271,
      "training_loss": 6.0446248054504395
    },
    {
      "epoch": 0.4923577235772358,
      "step": 2271,
      "training_loss": 5.803816318511963
    },
    {
      "epoch": 0.4923577235772358,
      "step": 2271,
      "training_loss": 6.400216102600098
    },
    {
      "epoch": 0.4923577235772358,
      "step": 2271,
      "training_loss": 7.720488548278809
    },
    {
      "epoch": 0.49257452574525745,
      "grad_norm": 14.54371166229248,
      "learning_rate": 1e-05,
      "loss": 6.2177,
      "step": 2272
    },
    {
      "epoch": 0.49257452574525745,
      "step": 2272,
      "training_loss": 5.513917446136475
    },
    {
      "epoch": 0.49257452574525745,
      "step": 2272,
      "training_loss": 6.673393726348877
    },
    {
      "epoch": 0.49257452574525745,
      "step": 2272,
      "training_loss": 7.152255058288574
    },
    {
      "epoch": 0.49257452574525745,
      "step": 2272,
      "training_loss": 4.632690906524658
    },
    {
      "epoch": 0.4927913279132791,
      "step": 2273,
      "training_loss": 7.252148628234863
    },
    {
      "epoch": 0.4927913279132791,
      "step": 2273,
      "training_loss": 6.872279167175293
    },
    {
      "epoch": 0.4927913279132791,
      "step": 2273,
      "training_loss": 5.566939830780029
    },
    {
      "epoch": 0.4927913279132791,
      "step": 2273,
      "training_loss": 6.594775676727295
    },
    {
      "epoch": 0.4930081300813008,
      "step": 2274,
      "training_loss": 6.783437728881836
    },
    {
      "epoch": 0.4930081300813008,
      "step": 2274,
      "training_loss": 7.18431282043457
    },
    {
      "epoch": 0.4930081300813008,
      "step": 2274,
      "training_loss": 7.576658248901367
    },
    {
      "epoch": 0.4930081300813008,
      "step": 2274,
      "training_loss": 6.5811896324157715
    },
    {
      "epoch": 0.4932249322493225,
      "step": 2275,
      "training_loss": 6.3369364738464355
    },
    {
      "epoch": 0.4932249322493225,
      "step": 2275,
      "training_loss": 8.376961708068848
    },
    {
      "epoch": 0.4932249322493225,
      "step": 2275,
      "training_loss": 6.734744548797607
    },
    {
      "epoch": 0.4932249322493225,
      "step": 2275,
      "training_loss": 7.459652423858643
    },
    {
      "epoch": 0.4934417344173442,
      "grad_norm": 13.216108322143555,
      "learning_rate": 1e-05,
      "loss": 6.7058,
      "step": 2276
    },
    {
      "epoch": 0.4934417344173442,
      "step": 2276,
      "training_loss": 6.532788276672363
    },
    {
      "epoch": 0.4934417344173442,
      "step": 2276,
      "training_loss": 6.260715007781982
    },
    {
      "epoch": 0.4934417344173442,
      "step": 2276,
      "training_loss": 7.1604695320129395
    },
    {
      "epoch": 0.4934417344173442,
      "step": 2276,
      "training_loss": 4.9336018562316895
    },
    {
      "epoch": 0.49365853658536585,
      "step": 2277,
      "training_loss": 7.500217914581299
    },
    {
      "epoch": 0.49365853658536585,
      "step": 2277,
      "training_loss": 6.277507781982422
    },
    {
      "epoch": 0.49365853658536585,
      "step": 2277,
      "training_loss": 7.569002151489258
    },
    {
      "epoch": 0.49365853658536585,
      "step": 2277,
      "training_loss": 7.505438804626465
    },
    {
      "epoch": 0.4938753387533875,
      "step": 2278,
      "training_loss": 7.297670841217041
    },
    {
      "epoch": 0.4938753387533875,
      "step": 2278,
      "training_loss": 3.7011048793792725
    },
    {
      "epoch": 0.4938753387533875,
      "step": 2278,
      "training_loss": 6.834007263183594
    },
    {
      "epoch": 0.4938753387533875,
      "step": 2278,
      "training_loss": 5.481611251831055
    },
    {
      "epoch": 0.49409214092140924,
      "step": 2279,
      "training_loss": 6.116610527038574
    },
    {
      "epoch": 0.49409214092140924,
      "step": 2279,
      "training_loss": 5.290969371795654
    },
    {
      "epoch": 0.49409214092140924,
      "step": 2279,
      "training_loss": 6.760402202606201
    },
    {
      "epoch": 0.49409214092140924,
      "step": 2279,
      "training_loss": 7.183291435241699
    },
    {
      "epoch": 0.4943089430894309,
      "grad_norm": 16.940889358520508,
      "learning_rate": 1e-05,
      "loss": 6.4003,
      "step": 2280
    },
    {
      "epoch": 0.4943089430894309,
      "step": 2280,
      "training_loss": 6.171318054199219
    },
    {
      "epoch": 0.4943089430894309,
      "step": 2280,
      "training_loss": 6.3671112060546875
    },
    {
      "epoch": 0.4943089430894309,
      "step": 2280,
      "training_loss": 6.180475234985352
    },
    {
      "epoch": 0.4943089430894309,
      "step": 2280,
      "training_loss": 7.028667449951172
    },
    {
      "epoch": 0.4945257452574526,
      "step": 2281,
      "training_loss": 7.276673793792725
    },
    {
      "epoch": 0.4945257452574526,
      "step": 2281,
      "training_loss": 6.38537073135376
    },
    {
      "epoch": 0.4945257452574526,
      "step": 2281,
      "training_loss": 7.6404709815979
    },
    {
      "epoch": 0.4945257452574526,
      "step": 2281,
      "training_loss": 8.18095588684082
    },
    {
      "epoch": 0.49474254742547424,
      "step": 2282,
      "training_loss": 5.521751403808594
    },
    {
      "epoch": 0.49474254742547424,
      "step": 2282,
      "training_loss": 6.656435012817383
    },
    {
      "epoch": 0.49474254742547424,
      "step": 2282,
      "training_loss": 5.437346935272217
    },
    {
      "epoch": 0.49474254742547424,
      "step": 2282,
      "training_loss": 6.348243236541748
    },
    {
      "epoch": 0.4949593495934959,
      "step": 2283,
      "training_loss": 4.882627487182617
    },
    {
      "epoch": 0.4949593495934959,
      "step": 2283,
      "training_loss": 8.726917266845703
    },
    {
      "epoch": 0.4949593495934959,
      "step": 2283,
      "training_loss": 7.069417953491211
    },
    {
      "epoch": 0.4949593495934959,
      "step": 2283,
      "training_loss": 6.040559768676758
    },
    {
      "epoch": 0.49517615176151764,
      "grad_norm": 16.656518936157227,
      "learning_rate": 1e-05,
      "loss": 6.6196,
      "step": 2284
    },
    {
      "epoch": 0.49517615176151764,
      "step": 2284,
      "training_loss": 6.920773029327393
    },
    {
      "epoch": 0.49517615176151764,
      "step": 2284,
      "training_loss": 5.9638166427612305
    },
    {
      "epoch": 0.49517615176151764,
      "step": 2284,
      "training_loss": 5.574865818023682
    },
    {
      "epoch": 0.49517615176151764,
      "step": 2284,
      "training_loss": 6.875422477722168
    },
    {
      "epoch": 0.4953929539295393,
      "step": 2285,
      "training_loss": 6.503091335296631
    },
    {
      "epoch": 0.4953929539295393,
      "step": 2285,
      "training_loss": 6.052694797515869
    },
    {
      "epoch": 0.4953929539295393,
      "step": 2285,
      "training_loss": 3.2642171382904053
    },
    {
      "epoch": 0.4953929539295393,
      "step": 2285,
      "training_loss": 5.712581634521484
    },
    {
      "epoch": 0.49560975609756097,
      "step": 2286,
      "training_loss": 6.456309795379639
    },
    {
      "epoch": 0.49560975609756097,
      "step": 2286,
      "training_loss": 6.549953460693359
    },
    {
      "epoch": 0.49560975609756097,
      "step": 2286,
      "training_loss": 6.908746719360352
    },
    {
      "epoch": 0.49560975609756097,
      "step": 2286,
      "training_loss": 7.399792194366455
    },
    {
      "epoch": 0.49582655826558264,
      "step": 2287,
      "training_loss": 6.543045520782471
    },
    {
      "epoch": 0.49582655826558264,
      "step": 2287,
      "training_loss": 6.3952860832214355
    },
    {
      "epoch": 0.49582655826558264,
      "step": 2287,
      "training_loss": 6.172933101654053
    },
    {
      "epoch": 0.49582655826558264,
      "step": 2287,
      "training_loss": 7.060174942016602
    },
    {
      "epoch": 0.49604336043360436,
      "grad_norm": 13.09240436553955,
      "learning_rate": 1e-05,
      "loss": 6.2721,
      "step": 2288
    },
    {
      "epoch": 0.49604336043360436,
      "step": 2288,
      "training_loss": 4.77156925201416
    },
    {
      "epoch": 0.49604336043360436,
      "step": 2288,
      "training_loss": 5.607234001159668
    },
    {
      "epoch": 0.49604336043360436,
      "step": 2288,
      "training_loss": 5.864597320556641
    },
    {
      "epoch": 0.49604336043360436,
      "step": 2288,
      "training_loss": 6.293326377868652
    },
    {
      "epoch": 0.49626016260162603,
      "step": 2289,
      "training_loss": 7.461140155792236
    },
    {
      "epoch": 0.49626016260162603,
      "step": 2289,
      "training_loss": 8.10965633392334
    },
    {
      "epoch": 0.49626016260162603,
      "step": 2289,
      "training_loss": 7.206532001495361
    },
    {
      "epoch": 0.49626016260162603,
      "step": 2289,
      "training_loss": 7.693783283233643
    },
    {
      "epoch": 0.4964769647696477,
      "step": 2290,
      "training_loss": 8.305106163024902
    },
    {
      "epoch": 0.4964769647696477,
      "step": 2290,
      "training_loss": 6.232902526855469
    },
    {
      "epoch": 0.4964769647696477,
      "step": 2290,
      "training_loss": 6.95328426361084
    },
    {
      "epoch": 0.4964769647696477,
      "step": 2290,
      "training_loss": 4.909837245941162
    },
    {
      "epoch": 0.49669376693766937,
      "step": 2291,
      "training_loss": 6.836230754852295
    },
    {
      "epoch": 0.49669376693766937,
      "step": 2291,
      "training_loss": 7.89178991317749
    },
    {
      "epoch": 0.49669376693766937,
      "step": 2291,
      "training_loss": 8.0681734085083
    },
    {
      "epoch": 0.49669376693766937,
      "step": 2291,
      "training_loss": 6.398261070251465
    },
    {
      "epoch": 0.49691056910569104,
      "grad_norm": 11.718266487121582,
      "learning_rate": 1e-05,
      "loss": 6.7877,
      "step": 2292
    },
    {
      "epoch": 0.49691056910569104,
      "step": 2292,
      "training_loss": 6.149984836578369
    },
    {
      "epoch": 0.49691056910569104,
      "step": 2292,
      "training_loss": 6.961551189422607
    },
    {
      "epoch": 0.49691056910569104,
      "step": 2292,
      "training_loss": 6.812316417694092
    },
    {
      "epoch": 0.49691056910569104,
      "step": 2292,
      "training_loss": 6.470302581787109
    },
    {
      "epoch": 0.49712737127371276,
      "step": 2293,
      "training_loss": 7.7810187339782715
    },
    {
      "epoch": 0.49712737127371276,
      "step": 2293,
      "training_loss": 5.888268947601318
    },
    {
      "epoch": 0.49712737127371276,
      "step": 2293,
      "training_loss": 6.635817527770996
    },
    {
      "epoch": 0.49712737127371276,
      "step": 2293,
      "training_loss": 7.891584873199463
    },
    {
      "epoch": 0.4973441734417344,
      "step": 2294,
      "training_loss": 6.805781841278076
    },
    {
      "epoch": 0.4973441734417344,
      "step": 2294,
      "training_loss": 5.38008451461792
    },
    {
      "epoch": 0.4973441734417344,
      "step": 2294,
      "training_loss": 6.556868076324463
    },
    {
      "epoch": 0.4973441734417344,
      "step": 2294,
      "training_loss": 6.609765529632568
    },
    {
      "epoch": 0.4975609756097561,
      "step": 2295,
      "training_loss": 6.815309047698975
    },
    {
      "epoch": 0.4975609756097561,
      "step": 2295,
      "training_loss": 6.524869918823242
    },
    {
      "epoch": 0.4975609756097561,
      "step": 2295,
      "training_loss": 5.986109733581543
    },
    {
      "epoch": 0.4975609756097561,
      "step": 2295,
      "training_loss": 6.524245738983154
    },
    {
      "epoch": 0.49777777777777776,
      "grad_norm": 15.037014961242676,
      "learning_rate": 1e-05,
      "loss": 6.6121,
      "step": 2296
    },
    {
      "epoch": 0.49777777777777776,
      "step": 2296,
      "training_loss": 7.130940914154053
    },
    {
      "epoch": 0.49777777777777776,
      "step": 2296,
      "training_loss": 6.807818412780762
    },
    {
      "epoch": 0.49777777777777776,
      "step": 2296,
      "training_loss": 7.045895099639893
    },
    {
      "epoch": 0.49777777777777776,
      "step": 2296,
      "training_loss": 6.4748616218566895
    },
    {
      "epoch": 0.49799457994579943,
      "step": 2297,
      "training_loss": 6.58022403717041
    },
    {
      "epoch": 0.49799457994579943,
      "step": 2297,
      "training_loss": 6.566415309906006
    },
    {
      "epoch": 0.49799457994579943,
      "step": 2297,
      "training_loss": 6.110406398773193
    },
    {
      "epoch": 0.49799457994579943,
      "step": 2297,
      "training_loss": 6.752718448638916
    },
    {
      "epoch": 0.49821138211382116,
      "step": 2298,
      "training_loss": 7.166794300079346
    },
    {
      "epoch": 0.49821138211382116,
      "step": 2298,
      "training_loss": 6.813425540924072
    },
    {
      "epoch": 0.49821138211382116,
      "step": 2298,
      "training_loss": 6.480301380157471
    },
    {
      "epoch": 0.49821138211382116,
      "step": 2298,
      "training_loss": 7.192168235778809
    },
    {
      "epoch": 0.4984281842818428,
      "step": 2299,
      "training_loss": 7.833720684051514
    },
    {
      "epoch": 0.4984281842818428,
      "step": 2299,
      "training_loss": 7.61022424697876
    },
    {
      "epoch": 0.4984281842818428,
      "step": 2299,
      "training_loss": 7.553614139556885
    },
    {
      "epoch": 0.4984281842818428,
      "step": 2299,
      "training_loss": 6.611395835876465
    },
    {
      "epoch": 0.4986449864498645,
      "grad_norm": 15.487854957580566,
      "learning_rate": 1e-05,
      "loss": 6.9207,
      "step": 2300
    },
    {
      "epoch": 0.4986449864498645,
      "step": 2300,
      "training_loss": 7.08809757232666
    },
    {
      "epoch": 0.4986449864498645,
      "step": 2300,
      "training_loss": 7.570145606994629
    },
    {
      "epoch": 0.4986449864498645,
      "step": 2300,
      "training_loss": 7.573685646057129
    },
    {
      "epoch": 0.4986449864498645,
      "step": 2300,
      "training_loss": 5.640288829803467
    },
    {
      "epoch": 0.49886178861788616,
      "step": 2301,
      "training_loss": 6.735799312591553
    },
    {
      "epoch": 0.49886178861788616,
      "step": 2301,
      "training_loss": 4.594700813293457
    },
    {
      "epoch": 0.49886178861788616,
      "step": 2301,
      "training_loss": 4.246939182281494
    },
    {
      "epoch": 0.49886178861788616,
      "step": 2301,
      "training_loss": 5.1955180168151855
    },
    {
      "epoch": 0.4990785907859079,
      "step": 2302,
      "training_loss": 6.123836517333984
    },
    {
      "epoch": 0.4990785907859079,
      "step": 2302,
      "training_loss": 7.235651016235352
    },
    {
      "epoch": 0.4990785907859079,
      "step": 2302,
      "training_loss": 7.3732428550720215
    },
    {
      "epoch": 0.4990785907859079,
      "step": 2302,
      "training_loss": 7.2332892417907715
    },
    {
      "epoch": 0.49929539295392955,
      "step": 2303,
      "training_loss": 5.476169586181641
    },
    {
      "epoch": 0.49929539295392955,
      "step": 2303,
      "training_loss": 6.503127574920654
    },
    {
      "epoch": 0.49929539295392955,
      "step": 2303,
      "training_loss": 6.732337474822998
    },
    {
      "epoch": 0.49929539295392955,
      "step": 2303,
      "training_loss": 7.090467929840088
    },
    {
      "epoch": 0.4995121951219512,
      "grad_norm": 14.521723747253418,
      "learning_rate": 1e-05,
      "loss": 6.4008,
      "step": 2304
    },
    {
      "epoch": 0.4995121951219512,
      "step": 2304,
      "training_loss": 6.800454616546631
    },
    {
      "epoch": 0.4995121951219512,
      "step": 2304,
      "training_loss": 6.6285719871521
    },
    {
      "epoch": 0.4995121951219512,
      "step": 2304,
      "training_loss": 5.989418983459473
    },
    {
      "epoch": 0.4995121951219512,
      "step": 2304,
      "training_loss": 6.268693923950195
    },
    {
      "epoch": 0.4997289972899729,
      "step": 2305,
      "training_loss": 7.281105995178223
    },
    {
      "epoch": 0.4997289972899729,
      "step": 2305,
      "training_loss": 6.827078819274902
    },
    {
      "epoch": 0.4997289972899729,
      "step": 2305,
      "training_loss": 6.870919704437256
    },
    {
      "epoch": 0.4997289972899729,
      "step": 2305,
      "training_loss": 7.0040364265441895
    },
    {
      "epoch": 0.49994579945799456,
      "step": 2306,
      "training_loss": 6.198539733886719
    },
    {
      "epoch": 0.49994579945799456,
      "step": 2306,
      "training_loss": 6.306135177612305
    },
    {
      "epoch": 0.49994579945799456,
      "step": 2306,
      "training_loss": 5.527005195617676
    },
    {
      "epoch": 0.49994579945799456,
      "step": 2306,
      "training_loss": 5.850572109222412
    },
    {
      "epoch": 0.5001626016260162,
      "step": 2307,
      "training_loss": 5.724177837371826
    },
    {
      "epoch": 0.5001626016260162,
      "step": 2307,
      "training_loss": 7.339812278747559
    },
    {
      "epoch": 0.5001626016260162,
      "step": 2307,
      "training_loss": 7.148808002471924
    },
    {
      "epoch": 0.5001626016260162,
      "step": 2307,
      "training_loss": 5.954618453979492
    },
    {
      "epoch": 0.5003794037940379,
      "grad_norm": 14.230688095092773,
      "learning_rate": 1e-05,
      "loss": 6.4825,
      "step": 2308
    },
    {
      "epoch": 0.5003794037940379,
      "step": 2308,
      "training_loss": 6.649636745452881
    },
    {
      "epoch": 0.5003794037940379,
      "step": 2308,
      "training_loss": 5.36012077331543
    },
    {
      "epoch": 0.5003794037940379,
      "step": 2308,
      "training_loss": 7.164190769195557
    },
    {
      "epoch": 0.5003794037940379,
      "step": 2308,
      "training_loss": 7.836670398712158
    },
    {
      "epoch": 0.5005962059620597,
      "step": 2309,
      "training_loss": 6.6268839836120605
    },
    {
      "epoch": 0.5005962059620597,
      "step": 2309,
      "training_loss": 5.520700454711914
    },
    {
      "epoch": 0.5005962059620597,
      "step": 2309,
      "training_loss": 4.859544277191162
    },
    {
      "epoch": 0.5005962059620597,
      "step": 2309,
      "training_loss": 5.350275039672852
    },
    {
      "epoch": 0.5008130081300813,
      "step": 2310,
      "training_loss": 6.711439609527588
    },
    {
      "epoch": 0.5008130081300813,
      "step": 2310,
      "training_loss": 7.333809852600098
    },
    {
      "epoch": 0.5008130081300813,
      "step": 2310,
      "training_loss": 6.970867156982422
    },
    {
      "epoch": 0.5008130081300813,
      "step": 2310,
      "training_loss": 6.143728733062744
    },
    {
      "epoch": 0.501029810298103,
      "step": 2311,
      "training_loss": 4.263411045074463
    },
    {
      "epoch": 0.501029810298103,
      "step": 2311,
      "training_loss": 7.5800065994262695
    },
    {
      "epoch": 0.501029810298103,
      "step": 2311,
      "training_loss": 6.320492744445801
    },
    {
      "epoch": 0.501029810298103,
      "step": 2311,
      "training_loss": 7.587225914001465
    },
    {
      "epoch": 0.5012466124661247,
      "grad_norm": 12.005955696105957,
      "learning_rate": 1e-05,
      "loss": 6.3924,
      "step": 2312
    },
    {
      "epoch": 0.5012466124661247,
      "step": 2312,
      "training_loss": 7.166525363922119
    },
    {
      "epoch": 0.5012466124661247,
      "step": 2312,
      "training_loss": 6.495523452758789
    },
    {
      "epoch": 0.5012466124661247,
      "step": 2312,
      "training_loss": 6.879731178283691
    },
    {
      "epoch": 0.5012466124661247,
      "step": 2312,
      "training_loss": 7.769222736358643
    },
    {
      "epoch": 0.5014634146341463,
      "step": 2313,
      "training_loss": 6.146838188171387
    },
    {
      "epoch": 0.5014634146341463,
      "step": 2313,
      "training_loss": 7.328890800476074
    },
    {
      "epoch": 0.5014634146341463,
      "step": 2313,
      "training_loss": 6.960702896118164
    },
    {
      "epoch": 0.5014634146341463,
      "step": 2313,
      "training_loss": 4.525028705596924
    },
    {
      "epoch": 0.501680216802168,
      "step": 2314,
      "training_loss": 7.112952709197998
    },
    {
      "epoch": 0.501680216802168,
      "step": 2314,
      "training_loss": 6.776698589324951
    },
    {
      "epoch": 0.501680216802168,
      "step": 2314,
      "training_loss": 6.653652191162109
    },
    {
      "epoch": 0.501680216802168,
      "step": 2314,
      "training_loss": 7.58056116104126
    },
    {
      "epoch": 0.5018970189701897,
      "step": 2315,
      "training_loss": 6.704820156097412
    },
    {
      "epoch": 0.5018970189701897,
      "step": 2315,
      "training_loss": 5.392839431762695
    },
    {
      "epoch": 0.5018970189701897,
      "step": 2315,
      "training_loss": 6.058606147766113
    },
    {
      "epoch": 0.5018970189701897,
      "step": 2315,
      "training_loss": 6.890044689178467
    },
    {
      "epoch": 0.5021138211382113,
      "grad_norm": 18.075986862182617,
      "learning_rate": 1e-05,
      "loss": 6.6527,
      "step": 2316
    },
    {
      "epoch": 0.5021138211382113,
      "step": 2316,
      "training_loss": 7.047220230102539
    },
    {
      "epoch": 0.5021138211382113,
      "step": 2316,
      "training_loss": 5.5269775390625
    },
    {
      "epoch": 0.5021138211382113,
      "step": 2316,
      "training_loss": 7.8499579429626465
    },
    {
      "epoch": 0.5021138211382113,
      "step": 2316,
      "training_loss": 7.117180347442627
    },
    {
      "epoch": 0.502330623306233,
      "step": 2317,
      "training_loss": 5.473954200744629
    },
    {
      "epoch": 0.502330623306233,
      "step": 2317,
      "training_loss": 6.162586212158203
    },
    {
      "epoch": 0.502330623306233,
      "step": 2317,
      "training_loss": 4.02107572555542
    },
    {
      "epoch": 0.502330623306233,
      "step": 2317,
      "training_loss": 6.845750331878662
    },
    {
      "epoch": 0.5025474254742548,
      "step": 2318,
      "training_loss": 5.2535624504089355
    },
    {
      "epoch": 0.5025474254742548,
      "step": 2318,
      "training_loss": 3.7026255130767822
    },
    {
      "epoch": 0.5025474254742548,
      "step": 2318,
      "training_loss": 5.490440368652344
    },
    {
      "epoch": 0.5025474254742548,
      "step": 2318,
      "training_loss": 7.593945026397705
    },
    {
      "epoch": 0.5027642276422765,
      "step": 2319,
      "training_loss": 5.594484329223633
    },
    {
      "epoch": 0.5027642276422765,
      "step": 2319,
      "training_loss": 6.924307823181152
    },
    {
      "epoch": 0.5027642276422765,
      "step": 2319,
      "training_loss": 4.550597667694092
    },
    {
      "epoch": 0.5027642276422765,
      "step": 2319,
      "training_loss": 4.931589126586914
    },
    {
      "epoch": 0.5029810298102981,
      "grad_norm": 20.61473274230957,
      "learning_rate": 1e-05,
      "loss": 5.8804,
      "step": 2320
    },
    {
      "epoch": 0.5029810298102981,
      "step": 2320,
      "training_loss": 7.196708679199219
    },
    {
      "epoch": 0.5029810298102981,
      "step": 2320,
      "training_loss": 7.308984279632568
    },
    {
      "epoch": 0.5029810298102981,
      "step": 2320,
      "training_loss": 5.777096748352051
    },
    {
      "epoch": 0.5029810298102981,
      "step": 2320,
      "training_loss": 6.485611915588379
    },
    {
      "epoch": 0.5031978319783198,
      "step": 2321,
      "training_loss": 7.4424896240234375
    },
    {
      "epoch": 0.5031978319783198,
      "step": 2321,
      "training_loss": 7.4006452560424805
    },
    {
      "epoch": 0.5031978319783198,
      "step": 2321,
      "training_loss": 6.3006510734558105
    },
    {
      "epoch": 0.5031978319783198,
      "step": 2321,
      "training_loss": 6.660951614379883
    },
    {
      "epoch": 0.5034146341463415,
      "step": 2322,
      "training_loss": 6.855754852294922
    },
    {
      "epoch": 0.5034146341463415,
      "step": 2322,
      "training_loss": 9.49417781829834
    },
    {
      "epoch": 0.5034146341463415,
      "step": 2322,
      "training_loss": 6.8270416259765625
    },
    {
      "epoch": 0.5034146341463415,
      "step": 2322,
      "training_loss": 7.197372913360596
    },
    {
      "epoch": 0.5036314363143631,
      "step": 2323,
      "training_loss": 6.079602241516113
    },
    {
      "epoch": 0.5036314363143631,
      "step": 2323,
      "training_loss": 6.798710823059082
    },
    {
      "epoch": 0.5036314363143631,
      "step": 2323,
      "training_loss": 6.182694435119629
    },
    {
      "epoch": 0.5036314363143631,
      "step": 2323,
      "training_loss": 7.226060390472412
    },
    {
      "epoch": 0.5038482384823848,
      "grad_norm": 13.156218528747559,
      "learning_rate": 1e-05,
      "loss": 6.9522,
      "step": 2324
    },
    {
      "epoch": 0.5038482384823848,
      "step": 2324,
      "training_loss": 6.790022850036621
    },
    {
      "epoch": 0.5038482384823848,
      "step": 2324,
      "training_loss": 7.451523303985596
    },
    {
      "epoch": 0.5038482384823848,
      "step": 2324,
      "training_loss": 6.761021614074707
    },
    {
      "epoch": 0.5038482384823848,
      "step": 2324,
      "training_loss": 6.446505069732666
    },
    {
      "epoch": 0.5040650406504065,
      "step": 2325,
      "training_loss": 6.3965067863464355
    },
    {
      "epoch": 0.5040650406504065,
      "step": 2325,
      "training_loss": 6.6483635902404785
    },
    {
      "epoch": 0.5040650406504065,
      "step": 2325,
      "training_loss": 6.443824291229248
    },
    {
      "epoch": 0.5040650406504065,
      "step": 2325,
      "training_loss": 7.708133697509766
    },
    {
      "epoch": 0.5042818428184281,
      "step": 2326,
      "training_loss": 6.081239700317383
    },
    {
      "epoch": 0.5042818428184281,
      "step": 2326,
      "training_loss": 7.121221542358398
    },
    {
      "epoch": 0.5042818428184281,
      "step": 2326,
      "training_loss": 7.326480865478516
    },
    {
      "epoch": 0.5042818428184281,
      "step": 2326,
      "training_loss": 7.607732772827148
    },
    {
      "epoch": 0.5044986449864499,
      "step": 2327,
      "training_loss": 7.287807941436768
    },
    {
      "epoch": 0.5044986449864499,
      "step": 2327,
      "training_loss": 5.690673828125
    },
    {
      "epoch": 0.5044986449864499,
      "step": 2327,
      "training_loss": 6.712669372558594
    },
    {
      "epoch": 0.5044986449864499,
      "step": 2327,
      "training_loss": 6.689038276672363
    },
    {
      "epoch": 0.5047154471544716,
      "grad_norm": 25.376514434814453,
      "learning_rate": 1e-05,
      "loss": 6.8227,
      "step": 2328
    },
    {
      "epoch": 0.5047154471544716,
      "step": 2328,
      "training_loss": 5.67425537109375
    },
    {
      "epoch": 0.5047154471544716,
      "step": 2328,
      "training_loss": 7.31190824508667
    },
    {
      "epoch": 0.5047154471544716,
      "step": 2328,
      "training_loss": 5.332035064697266
    },
    {
      "epoch": 0.5047154471544716,
      "step": 2328,
      "training_loss": 5.782227993011475
    },
    {
      "epoch": 0.5049322493224933,
      "step": 2329,
      "training_loss": 7.117888450622559
    },
    {
      "epoch": 0.5049322493224933,
      "step": 2329,
      "training_loss": 6.973986625671387
    },
    {
      "epoch": 0.5049322493224933,
      "step": 2329,
      "training_loss": 6.424309253692627
    },
    {
      "epoch": 0.5049322493224933,
      "step": 2329,
      "training_loss": 6.336664199829102
    },
    {
      "epoch": 0.5051490514905149,
      "step": 2330,
      "training_loss": 7.521005153656006
    },
    {
      "epoch": 0.5051490514905149,
      "step": 2330,
      "training_loss": 7.3960442543029785
    },
    {
      "epoch": 0.5051490514905149,
      "step": 2330,
      "training_loss": 3.4474120140075684
    },
    {
      "epoch": 0.5051490514905149,
      "step": 2330,
      "training_loss": 7.270944118499756
    },
    {
      "epoch": 0.5053658536585366,
      "step": 2331,
      "training_loss": 3.2929680347442627
    },
    {
      "epoch": 0.5053658536585366,
      "step": 2331,
      "training_loss": 7.386048793792725
    },
    {
      "epoch": 0.5053658536585366,
      "step": 2331,
      "training_loss": 5.201605796813965
    },
    {
      "epoch": 0.5053658536585366,
      "step": 2331,
      "training_loss": 4.305361747741699
    },
    {
      "epoch": 0.5055826558265583,
      "grad_norm": 14.858325958251953,
      "learning_rate": 1e-05,
      "loss": 6.0484,
      "step": 2332
    },
    {
      "epoch": 0.5055826558265583,
      "step": 2332,
      "training_loss": 5.983449935913086
    },
    {
      "epoch": 0.5055826558265583,
      "step": 2332,
      "training_loss": 6.544518947601318
    },
    {
      "epoch": 0.5055826558265583,
      "step": 2332,
      "training_loss": 6.098283767700195
    },
    {
      "epoch": 0.5055826558265583,
      "step": 2332,
      "training_loss": 6.557058334350586
    },
    {
      "epoch": 0.5057994579945799,
      "step": 2333,
      "training_loss": 7.848110675811768
    },
    {
      "epoch": 0.5057994579945799,
      "step": 2333,
      "training_loss": 6.314572334289551
    },
    {
      "epoch": 0.5057994579945799,
      "step": 2333,
      "training_loss": 6.3547468185424805
    },
    {
      "epoch": 0.5057994579945799,
      "step": 2333,
      "training_loss": 6.580237865447998
    },
    {
      "epoch": 0.5060162601626016,
      "step": 2334,
      "training_loss": 7.6991286277771
    },
    {
      "epoch": 0.5060162601626016,
      "step": 2334,
      "training_loss": 4.355227470397949
    },
    {
      "epoch": 0.5060162601626016,
      "step": 2334,
      "training_loss": 6.432346820831299
    },
    {
      "epoch": 0.5060162601626016,
      "step": 2334,
      "training_loss": 5.395847320556641
    },
    {
      "epoch": 0.5062330623306233,
      "step": 2335,
      "training_loss": 6.448874473571777
    },
    {
      "epoch": 0.5062330623306233,
      "step": 2335,
      "training_loss": 6.814423084259033
    },
    {
      "epoch": 0.5062330623306233,
      "step": 2335,
      "training_loss": 5.980385780334473
    },
    {
      "epoch": 0.5062330623306233,
      "step": 2335,
      "training_loss": 5.94791841506958
    },
    {
      "epoch": 0.506449864498645,
      "grad_norm": 14.270444869995117,
      "learning_rate": 1e-05,
      "loss": 6.3347,
      "step": 2336
    },
    {
      "epoch": 0.506449864498645,
      "step": 2336,
      "training_loss": 7.064577102661133
    },
    {
      "epoch": 0.506449864498645,
      "step": 2336,
      "training_loss": 6.434606075286865
    },
    {
      "epoch": 0.506449864498645,
      "step": 2336,
      "training_loss": 7.376354694366455
    },
    {
      "epoch": 0.506449864498645,
      "step": 2336,
      "training_loss": 6.7208333015441895
    },
    {
      "epoch": 0.5066666666666667,
      "step": 2337,
      "training_loss": 7.195209503173828
    },
    {
      "epoch": 0.5066666666666667,
      "step": 2337,
      "training_loss": 6.43189001083374
    },
    {
      "epoch": 0.5066666666666667,
      "step": 2337,
      "training_loss": 6.109916687011719
    },
    {
      "epoch": 0.5066666666666667,
      "step": 2337,
      "training_loss": 7.566097259521484
    },
    {
      "epoch": 0.5068834688346884,
      "step": 2338,
      "training_loss": 6.265636444091797
    },
    {
      "epoch": 0.5068834688346884,
      "step": 2338,
      "training_loss": 6.175160884857178
    },
    {
      "epoch": 0.5068834688346884,
      "step": 2338,
      "training_loss": 6.510811805725098
    },
    {
      "epoch": 0.5068834688346884,
      "step": 2338,
      "training_loss": 6.785695552825928
    },
    {
      "epoch": 0.50710027100271,
      "step": 2339,
      "training_loss": 5.959217071533203
    },
    {
      "epoch": 0.50710027100271,
      "step": 2339,
      "training_loss": 7.332436561584473
    },
    {
      "epoch": 0.50710027100271,
      "step": 2339,
      "training_loss": 6.613724708557129
    },
    {
      "epoch": 0.50710027100271,
      "step": 2339,
      "training_loss": 6.8851542472839355
    },
    {
      "epoch": 0.5073170731707317,
      "grad_norm": 12.032958030700684,
      "learning_rate": 1e-05,
      "loss": 6.7142,
      "step": 2340
    },
    {
      "epoch": 0.5073170731707317,
      "step": 2340,
      "training_loss": 7.224239349365234
    },
    {
      "epoch": 0.5073170731707317,
      "step": 2340,
      "training_loss": 5.831626892089844
    },
    {
      "epoch": 0.5073170731707317,
      "step": 2340,
      "training_loss": 7.7585530281066895
    },
    {
      "epoch": 0.5073170731707317,
      "step": 2340,
      "training_loss": 5.7725372314453125
    },
    {
      "epoch": 0.5075338753387534,
      "step": 2341,
      "training_loss": 6.058807373046875
    },
    {
      "epoch": 0.5075338753387534,
      "step": 2341,
      "training_loss": 6.723361968994141
    },
    {
      "epoch": 0.5075338753387534,
      "step": 2341,
      "training_loss": 6.606945514678955
    },
    {
      "epoch": 0.5075338753387534,
      "step": 2341,
      "training_loss": 6.800061225891113
    },
    {
      "epoch": 0.507750677506775,
      "step": 2342,
      "training_loss": 8.48737907409668
    },
    {
      "epoch": 0.507750677506775,
      "step": 2342,
      "training_loss": 7.486123085021973
    },
    {
      "epoch": 0.507750677506775,
      "step": 2342,
      "training_loss": 6.500843048095703
    },
    {
      "epoch": 0.507750677506775,
      "step": 2342,
      "training_loss": 4.07264518737793
    },
    {
      "epoch": 0.5079674796747967,
      "step": 2343,
      "training_loss": 6.834682464599609
    },
    {
      "epoch": 0.5079674796747967,
      "step": 2343,
      "training_loss": 3.143655300140381
    },
    {
      "epoch": 0.5079674796747967,
      "step": 2343,
      "training_loss": 7.907983303070068
    },
    {
      "epoch": 0.5079674796747967,
      "step": 2343,
      "training_loss": 6.705578804016113
    },
    {
      "epoch": 0.5081842818428184,
      "grad_norm": 23.24655532836914,
      "learning_rate": 1e-05,
      "loss": 6.4947,
      "step": 2344
    },
    {
      "epoch": 0.5081842818428184,
      "step": 2344,
      "training_loss": 6.762721061706543
    },
    {
      "epoch": 0.5081842818428184,
      "step": 2344,
      "training_loss": 6.131280422210693
    },
    {
      "epoch": 0.5081842818428184,
      "step": 2344,
      "training_loss": 7.238441467285156
    },
    {
      "epoch": 0.5081842818428184,
      "step": 2344,
      "training_loss": 5.552186965942383
    },
    {
      "epoch": 0.50840108401084,
      "step": 2345,
      "training_loss": 6.976241111755371
    },
    {
      "epoch": 0.50840108401084,
      "step": 2345,
      "training_loss": 7.787827014923096
    },
    {
      "epoch": 0.50840108401084,
      "step": 2345,
      "training_loss": 4.5573530197143555
    },
    {
      "epoch": 0.50840108401084,
      "step": 2345,
      "training_loss": 5.982508659362793
    },
    {
      "epoch": 0.5086178861788618,
      "step": 2346,
      "training_loss": 8.444780349731445
    },
    {
      "epoch": 0.5086178861788618,
      "step": 2346,
      "training_loss": 5.7829270362854
    },
    {
      "epoch": 0.5086178861788618,
      "step": 2346,
      "training_loss": 6.908775329589844
    },
    {
      "epoch": 0.5086178861788618,
      "step": 2346,
      "training_loss": 7.157374382019043
    },
    {
      "epoch": 0.5088346883468835,
      "step": 2347,
      "training_loss": 6.6280951499938965
    },
    {
      "epoch": 0.5088346883468835,
      "step": 2347,
      "training_loss": 6.042138576507568
    },
    {
      "epoch": 0.5088346883468835,
      "step": 2347,
      "training_loss": 7.089029788970947
    },
    {
      "epoch": 0.5088346883468835,
      "step": 2347,
      "training_loss": 7.140690326690674
    },
    {
      "epoch": 0.5090514905149052,
      "grad_norm": 14.062902450561523,
      "learning_rate": 1e-05,
      "loss": 6.6364,
      "step": 2348
    },
    {
      "epoch": 0.5090514905149052,
      "step": 2348,
      "training_loss": 6.3093743324279785
    },
    {
      "epoch": 0.5090514905149052,
      "step": 2348,
      "training_loss": 6.301064491271973
    },
    {
      "epoch": 0.5090514905149052,
      "step": 2348,
      "training_loss": 7.138744831085205
    },
    {
      "epoch": 0.5090514905149052,
      "step": 2348,
      "training_loss": 7.552587985992432
    },
    {
      "epoch": 0.5092682926829268,
      "step": 2349,
      "training_loss": 6.916320323944092
    },
    {
      "epoch": 0.5092682926829268,
      "step": 2349,
      "training_loss": 6.676942825317383
    },
    {
      "epoch": 0.5092682926829268,
      "step": 2349,
      "training_loss": 8.024588584899902
    },
    {
      "epoch": 0.5092682926829268,
      "step": 2349,
      "training_loss": 7.389951229095459
    },
    {
      "epoch": 0.5094850948509485,
      "step": 2350,
      "training_loss": 7.243598461151123
    },
    {
      "epoch": 0.5094850948509485,
      "step": 2350,
      "training_loss": 8.19719409942627
    },
    {
      "epoch": 0.5094850948509485,
      "step": 2350,
      "training_loss": 8.646770477294922
    },
    {
      "epoch": 0.5094850948509485,
      "step": 2350,
      "training_loss": 6.685921669006348
    },
    {
      "epoch": 0.5097018970189702,
      "step": 2351,
      "training_loss": 6.448298931121826
    },
    {
      "epoch": 0.5097018970189702,
      "step": 2351,
      "training_loss": 6.779923915863037
    },
    {
      "epoch": 0.5097018970189702,
      "step": 2351,
      "training_loss": 7.161980628967285
    },
    {
      "epoch": 0.5097018970189702,
      "step": 2351,
      "training_loss": 5.7742462158203125
    },
    {
      "epoch": 0.5099186991869918,
      "grad_norm": 20.384952545166016,
      "learning_rate": 1e-05,
      "loss": 7.078,
      "step": 2352
    },
    {
      "epoch": 0.5099186991869918,
      "step": 2352,
      "training_loss": 6.877835273742676
    },
    {
      "epoch": 0.5099186991869918,
      "step": 2352,
      "training_loss": 7.167118549346924
    },
    {
      "epoch": 0.5099186991869918,
      "step": 2352,
      "training_loss": 5.751292705535889
    },
    {
      "epoch": 0.5099186991869918,
      "step": 2352,
      "training_loss": 6.141964435577393
    },
    {
      "epoch": 0.5101355013550135,
      "step": 2353,
      "training_loss": 6.778838634490967
    },
    {
      "epoch": 0.5101355013550135,
      "step": 2353,
      "training_loss": 7.144766807556152
    },
    {
      "epoch": 0.5101355013550135,
      "step": 2353,
      "training_loss": 4.766576766967773
    },
    {
      "epoch": 0.5101355013550135,
      "step": 2353,
      "training_loss": 5.208286762237549
    },
    {
      "epoch": 0.5103523035230352,
      "step": 2354,
      "training_loss": 6.976776599884033
    },
    {
      "epoch": 0.5103523035230352,
      "step": 2354,
      "training_loss": 4.629872798919678
    },
    {
      "epoch": 0.5103523035230352,
      "step": 2354,
      "training_loss": 3.113860607147217
    },
    {
      "epoch": 0.5103523035230352,
      "step": 2354,
      "training_loss": 5.989633083343506
    },
    {
      "epoch": 0.510569105691057,
      "step": 2355,
      "training_loss": 3.8709070682525635
    },
    {
      "epoch": 0.510569105691057,
      "step": 2355,
      "training_loss": 7.265503883361816
    },
    {
      "epoch": 0.510569105691057,
      "step": 2355,
      "training_loss": 6.493905544281006
    },
    {
      "epoch": 0.510569105691057,
      "step": 2355,
      "training_loss": 6.788244247436523
    },
    {
      "epoch": 0.5107859078590786,
      "grad_norm": 12.602807998657227,
      "learning_rate": 1e-05,
      "loss": 5.9353,
      "step": 2356
    },
    {
      "epoch": 0.5107859078590786,
      "step": 2356,
      "training_loss": 7.373525619506836
    },
    {
      "epoch": 0.5107859078590786,
      "step": 2356,
      "training_loss": 7.551251411437988
    },
    {
      "epoch": 0.5107859078590786,
      "step": 2356,
      "training_loss": 7.516381740570068
    },
    {
      "epoch": 0.5107859078590786,
      "step": 2356,
      "training_loss": 6.286590576171875
    },
    {
      "epoch": 0.5110027100271003,
      "step": 2357,
      "training_loss": 5.464533805847168
    },
    {
      "epoch": 0.5110027100271003,
      "step": 2357,
      "training_loss": 6.5122833251953125
    },
    {
      "epoch": 0.5110027100271003,
      "step": 2357,
      "training_loss": 6.978663921356201
    },
    {
      "epoch": 0.5110027100271003,
      "step": 2357,
      "training_loss": 6.561833381652832
    },
    {
      "epoch": 0.511219512195122,
      "step": 2358,
      "training_loss": 4.125509262084961
    },
    {
      "epoch": 0.511219512195122,
      "step": 2358,
      "training_loss": 6.660172939300537
    },
    {
      "epoch": 0.511219512195122,
      "step": 2358,
      "training_loss": 4.416624546051025
    },
    {
      "epoch": 0.511219512195122,
      "step": 2358,
      "training_loss": 6.307042598724365
    },
    {
      "epoch": 0.5114363143631436,
      "step": 2359,
      "training_loss": 5.42171049118042
    },
    {
      "epoch": 0.5114363143631436,
      "step": 2359,
      "training_loss": 7.154238700866699
    },
    {
      "epoch": 0.5114363143631436,
      "step": 2359,
      "training_loss": 9.44100284576416
    },
    {
      "epoch": 0.5114363143631436,
      "step": 2359,
      "training_loss": 5.552272796630859
    },
    {
      "epoch": 0.5116531165311653,
      "grad_norm": 21.081357955932617,
      "learning_rate": 1e-05,
      "loss": 6.4577,
      "step": 2360
    },
    {
      "epoch": 0.5116531165311653,
      "step": 2360,
      "training_loss": 6.4866204261779785
    },
    {
      "epoch": 0.5116531165311653,
      "step": 2360,
      "training_loss": 6.00513219833374
    },
    {
      "epoch": 0.5116531165311653,
      "step": 2360,
      "training_loss": 6.0276336669921875
    },
    {
      "epoch": 0.5116531165311653,
      "step": 2360,
      "training_loss": 6.580811500549316
    },
    {
      "epoch": 0.511869918699187,
      "step": 2361,
      "training_loss": 6.91546106338501
    },
    {
      "epoch": 0.511869918699187,
      "step": 2361,
      "training_loss": 6.241514682769775
    },
    {
      "epoch": 0.511869918699187,
      "step": 2361,
      "training_loss": 7.071887016296387
    },
    {
      "epoch": 0.511869918699187,
      "step": 2361,
      "training_loss": 6.145822048187256
    },
    {
      "epoch": 0.5120867208672086,
      "step": 2362,
      "training_loss": 4.349887371063232
    },
    {
      "epoch": 0.5120867208672086,
      "step": 2362,
      "training_loss": 6.83452033996582
    },
    {
      "epoch": 0.5120867208672086,
      "step": 2362,
      "training_loss": 5.34480094909668
    },
    {
      "epoch": 0.5120867208672086,
      "step": 2362,
      "training_loss": 6.109014987945557
    },
    {
      "epoch": 0.5123035230352303,
      "step": 2363,
      "training_loss": 7.877971172332764
    },
    {
      "epoch": 0.5123035230352303,
      "step": 2363,
      "training_loss": 4.035579204559326
    },
    {
      "epoch": 0.5123035230352303,
      "step": 2363,
      "training_loss": 6.597835063934326
    },
    {
      "epoch": 0.5123035230352303,
      "step": 2363,
      "training_loss": 7.11734676361084
    },
    {
      "epoch": 0.5125203252032521,
      "grad_norm": 14.352062225341797,
      "learning_rate": 1e-05,
      "loss": 6.2339,
      "step": 2364
    },
    {
      "epoch": 0.5125203252032521,
      "step": 2364,
      "training_loss": 5.932178974151611
    },
    {
      "epoch": 0.5125203252032521,
      "step": 2364,
      "training_loss": 6.970138072967529
    },
    {
      "epoch": 0.5125203252032521,
      "step": 2364,
      "training_loss": 6.304646968841553
    },
    {
      "epoch": 0.5125203252032521,
      "step": 2364,
      "training_loss": 5.873966693878174
    },
    {
      "epoch": 0.5127371273712737,
      "step": 2365,
      "training_loss": 6.162412643432617
    },
    {
      "epoch": 0.5127371273712737,
      "step": 2365,
      "training_loss": 7.191769599914551
    },
    {
      "epoch": 0.5127371273712737,
      "step": 2365,
      "training_loss": 7.154949188232422
    },
    {
      "epoch": 0.5127371273712737,
      "step": 2365,
      "training_loss": 6.835062026977539
    },
    {
      "epoch": 0.5129539295392954,
      "step": 2366,
      "training_loss": 6.265170574188232
    },
    {
      "epoch": 0.5129539295392954,
      "step": 2366,
      "training_loss": 6.3921732902526855
    },
    {
      "epoch": 0.5129539295392954,
      "step": 2366,
      "training_loss": 7.442927837371826
    },
    {
      "epoch": 0.5129539295392954,
      "step": 2366,
      "training_loss": 7.286828517913818
    },
    {
      "epoch": 0.5131707317073171,
      "step": 2367,
      "training_loss": 7.2293477058410645
    },
    {
      "epoch": 0.5131707317073171,
      "step": 2367,
      "training_loss": 5.885888576507568
    },
    {
      "epoch": 0.5131707317073171,
      "step": 2367,
      "training_loss": 6.604710102081299
    },
    {
      "epoch": 0.5131707317073171,
      "step": 2367,
      "training_loss": 6.291301727294922
    },
    {
      "epoch": 0.5133875338753388,
      "grad_norm": 20.732471466064453,
      "learning_rate": 1e-05,
      "loss": 6.614,
      "step": 2368
    },
    {
      "epoch": 0.5133875338753388,
      "step": 2368,
      "training_loss": 9.43854808807373
    },
    {
      "epoch": 0.5133875338753388,
      "step": 2368,
      "training_loss": 6.826614856719971
    },
    {
      "epoch": 0.5133875338753388,
      "step": 2368,
      "training_loss": 7.130184173583984
    },
    {
      "epoch": 0.5133875338753388,
      "step": 2368,
      "training_loss": 7.365995407104492
    },
    {
      "epoch": 0.5136043360433604,
      "step": 2369,
      "training_loss": 6.264277935028076
    },
    {
      "epoch": 0.5136043360433604,
      "step": 2369,
      "training_loss": 7.550822734832764
    },
    {
      "epoch": 0.5136043360433604,
      "step": 2369,
      "training_loss": 6.882964134216309
    },
    {
      "epoch": 0.5136043360433604,
      "step": 2369,
      "training_loss": 4.718937397003174
    },
    {
      "epoch": 0.5138211382113821,
      "step": 2370,
      "training_loss": 6.269169807434082
    },
    {
      "epoch": 0.5138211382113821,
      "step": 2370,
      "training_loss": 7.586174011230469
    },
    {
      "epoch": 0.5138211382113821,
      "step": 2370,
      "training_loss": 6.928350925445557
    },
    {
      "epoch": 0.5138211382113821,
      "step": 2370,
      "training_loss": 7.59138298034668
    },
    {
      "epoch": 0.5140379403794038,
      "step": 2371,
      "training_loss": 5.02876091003418
    },
    {
      "epoch": 0.5140379403794038,
      "step": 2371,
      "training_loss": 6.128235816955566
    },
    {
      "epoch": 0.5140379403794038,
      "step": 2371,
      "training_loss": 3.090728998184204
    },
    {
      "epoch": 0.5140379403794038,
      "step": 2371,
      "training_loss": 6.849605560302734
    },
    {
      "epoch": 0.5142547425474254,
      "grad_norm": 19.291194915771484,
      "learning_rate": 1e-05,
      "loss": 6.6032,
      "step": 2372
    },
    {
      "epoch": 0.5142547425474254,
      "step": 2372,
      "training_loss": 6.58570671081543
    },
    {
      "epoch": 0.5142547425474254,
      "step": 2372,
      "training_loss": 6.997025489807129
    },
    {
      "epoch": 0.5142547425474254,
      "step": 2372,
      "training_loss": 7.204369068145752
    },
    {
      "epoch": 0.5142547425474254,
      "step": 2372,
      "training_loss": 6.375815391540527
    },
    {
      "epoch": 0.5144715447154472,
      "step": 2373,
      "training_loss": 7.396059036254883
    },
    {
      "epoch": 0.5144715447154472,
      "step": 2373,
      "training_loss": 6.375583648681641
    },
    {
      "epoch": 0.5144715447154472,
      "step": 2373,
      "training_loss": 7.2342963218688965
    },
    {
      "epoch": 0.5144715447154472,
      "step": 2373,
      "training_loss": 7.806643486022949
    },
    {
      "epoch": 0.5146883468834689,
      "step": 2374,
      "training_loss": 5.334779739379883
    },
    {
      "epoch": 0.5146883468834689,
      "step": 2374,
      "training_loss": 6.938691139221191
    },
    {
      "epoch": 0.5146883468834689,
      "step": 2374,
      "training_loss": 6.593213081359863
    },
    {
      "epoch": 0.5146883468834689,
      "step": 2374,
      "training_loss": 7.085400104522705
    },
    {
      "epoch": 0.5149051490514905,
      "step": 2375,
      "training_loss": 7.134848117828369
    },
    {
      "epoch": 0.5149051490514905,
      "step": 2375,
      "training_loss": 7.487231254577637
    },
    {
      "epoch": 0.5149051490514905,
      "step": 2375,
      "training_loss": 6.459853172302246
    },
    {
      "epoch": 0.5149051490514905,
      "step": 2375,
      "training_loss": 6.648524284362793
    },
    {
      "epoch": 0.5151219512195122,
      "grad_norm": 15.678905487060547,
      "learning_rate": 1e-05,
      "loss": 6.8536,
      "step": 2376
    },
    {
      "epoch": 0.5151219512195122,
      "step": 2376,
      "training_loss": 7.161771297454834
    },
    {
      "epoch": 0.5151219512195122,
      "step": 2376,
      "training_loss": 5.433896064758301
    },
    {
      "epoch": 0.5151219512195122,
      "step": 2376,
      "training_loss": 6.5833024978637695
    },
    {
      "epoch": 0.5151219512195122,
      "step": 2376,
      "training_loss": 4.064512729644775
    },
    {
      "epoch": 0.5153387533875339,
      "step": 2377,
      "training_loss": 5.37383508682251
    },
    {
      "epoch": 0.5153387533875339,
      "step": 2377,
      "training_loss": 7.043949127197266
    },
    {
      "epoch": 0.5153387533875339,
      "step": 2377,
      "training_loss": 6.077631950378418
    },
    {
      "epoch": 0.5153387533875339,
      "step": 2377,
      "training_loss": 7.183979511260986
    },
    {
      "epoch": 0.5155555555555555,
      "step": 2378,
      "training_loss": 6.211454391479492
    },
    {
      "epoch": 0.5155555555555555,
      "step": 2378,
      "training_loss": 7.090415954589844
    },
    {
      "epoch": 0.5155555555555555,
      "step": 2378,
      "training_loss": 7.585879325866699
    },
    {
      "epoch": 0.5155555555555555,
      "step": 2378,
      "training_loss": 7.562459945678711
    },
    {
      "epoch": 0.5157723577235772,
      "step": 2379,
      "training_loss": 2.936126708984375
    },
    {
      "epoch": 0.5157723577235772,
      "step": 2379,
      "training_loss": 7.775929927825928
    },
    {
      "epoch": 0.5157723577235772,
      "step": 2379,
      "training_loss": 6.760436058044434
    },
    {
      "epoch": 0.5157723577235772,
      "step": 2379,
      "training_loss": 7.529215335845947
    },
    {
      "epoch": 0.5159891598915989,
      "grad_norm": 15.489825248718262,
      "learning_rate": 1e-05,
      "loss": 6.3984,
      "step": 2380
    },
    {
      "epoch": 0.5159891598915989,
      "step": 2380,
      "training_loss": 5.242364406585693
    },
    {
      "epoch": 0.5159891598915989,
      "step": 2380,
      "training_loss": 7.200607776641846
    },
    {
      "epoch": 0.5159891598915989,
      "step": 2380,
      "training_loss": 7.13999605178833
    },
    {
      "epoch": 0.5159891598915989,
      "step": 2380,
      "training_loss": 6.9101362228393555
    },
    {
      "epoch": 0.5162059620596205,
      "step": 2381,
      "training_loss": 6.328207492828369
    },
    {
      "epoch": 0.5162059620596205,
      "step": 2381,
      "training_loss": 6.937072277069092
    },
    {
      "epoch": 0.5162059620596205,
      "step": 2381,
      "training_loss": 5.8295769691467285
    },
    {
      "epoch": 0.5162059620596205,
      "step": 2381,
      "training_loss": 5.715638160705566
    },
    {
      "epoch": 0.5164227642276423,
      "step": 2382,
      "training_loss": 7.506569862365723
    },
    {
      "epoch": 0.5164227642276423,
      "step": 2382,
      "training_loss": 7.60508918762207
    },
    {
      "epoch": 0.5164227642276423,
      "step": 2382,
      "training_loss": 6.484744548797607
    },
    {
      "epoch": 0.5164227642276423,
      "step": 2382,
      "training_loss": 5.850379467010498
    },
    {
      "epoch": 0.516639566395664,
      "step": 2383,
      "training_loss": 6.043768405914307
    },
    {
      "epoch": 0.516639566395664,
      "step": 2383,
      "training_loss": 7.8951802253723145
    },
    {
      "epoch": 0.516639566395664,
      "step": 2383,
      "training_loss": 6.228146076202393
    },
    {
      "epoch": 0.516639566395664,
      "step": 2383,
      "training_loss": 5.858817100524902
    },
    {
      "epoch": 0.5168563685636857,
      "grad_norm": 18.740333557128906,
      "learning_rate": 1e-05,
      "loss": 6.5485,
      "step": 2384
    },
    {
      "epoch": 0.5168563685636857,
      "step": 2384,
      "training_loss": 6.132294178009033
    },
    {
      "epoch": 0.5168563685636857,
      "step": 2384,
      "training_loss": 6.245058059692383
    },
    {
      "epoch": 0.5168563685636857,
      "step": 2384,
      "training_loss": 6.0421247482299805
    },
    {
      "epoch": 0.5168563685636857,
      "step": 2384,
      "training_loss": 7.201645851135254
    },
    {
      "epoch": 0.5170731707317073,
      "step": 2385,
      "training_loss": 7.052400588989258
    },
    {
      "epoch": 0.5170731707317073,
      "step": 2385,
      "training_loss": 7.432990550994873
    },
    {
      "epoch": 0.5170731707317073,
      "step": 2385,
      "training_loss": 5.8644609451293945
    },
    {
      "epoch": 0.5170731707317073,
      "step": 2385,
      "training_loss": 4.740488052368164
    },
    {
      "epoch": 0.517289972899729,
      "step": 2386,
      "training_loss": 5.969810962677002
    },
    {
      "epoch": 0.517289972899729,
      "step": 2386,
      "training_loss": 6.077859878540039
    },
    {
      "epoch": 0.517289972899729,
      "step": 2386,
      "training_loss": 7.307539463043213
    },
    {
      "epoch": 0.517289972899729,
      "step": 2386,
      "training_loss": 6.48006010055542
    },
    {
      "epoch": 0.5175067750677507,
      "step": 2387,
      "training_loss": 5.962575435638428
    },
    {
      "epoch": 0.5175067750677507,
      "step": 2387,
      "training_loss": 7.213592529296875
    },
    {
      "epoch": 0.5175067750677507,
      "step": 2387,
      "training_loss": 6.588761329650879
    },
    {
      "epoch": 0.5175067750677507,
      "step": 2387,
      "training_loss": 5.9868364334106445
    },
    {
      "epoch": 0.5177235772357723,
      "grad_norm": 14.262962341308594,
      "learning_rate": 1e-05,
      "loss": 6.3937,
      "step": 2388
    },
    {
      "epoch": 0.5177235772357723,
      "step": 2388,
      "training_loss": 7.883549690246582
    },
    {
      "epoch": 0.5177235772357723,
      "step": 2388,
      "training_loss": 4.00848913192749
    },
    {
      "epoch": 0.5177235772357723,
      "step": 2388,
      "training_loss": 7.4463348388671875
    },
    {
      "epoch": 0.5177235772357723,
      "step": 2388,
      "training_loss": 6.973423480987549
    },
    {
      "epoch": 0.517940379403794,
      "step": 2389,
      "training_loss": 6.110344409942627
    },
    {
      "epoch": 0.517940379403794,
      "step": 2389,
      "training_loss": 6.668839931488037
    },
    {
      "epoch": 0.517940379403794,
      "step": 2389,
      "training_loss": 7.158790111541748
    },
    {
      "epoch": 0.517940379403794,
      "step": 2389,
      "training_loss": 9.146906852722168
    },
    {
      "epoch": 0.5181571815718157,
      "step": 2390,
      "training_loss": 6.585315704345703
    },
    {
      "epoch": 0.5181571815718157,
      "step": 2390,
      "training_loss": 6.755043029785156
    },
    {
      "epoch": 0.5181571815718157,
      "step": 2390,
      "training_loss": 6.466401100158691
    },
    {
      "epoch": 0.5181571815718157,
      "step": 2390,
      "training_loss": 7.164722919464111
    },
    {
      "epoch": 0.5183739837398375,
      "step": 2391,
      "training_loss": 7.357823371887207
    },
    {
      "epoch": 0.5183739837398375,
      "step": 2391,
      "training_loss": 7.699583530426025
    },
    {
      "epoch": 0.5183739837398375,
      "step": 2391,
      "training_loss": 7.396401882171631
    },
    {
      "epoch": 0.5183739837398375,
      "step": 2391,
      "training_loss": 3.6778855323791504
    },
    {
      "epoch": 0.5185907859078591,
      "grad_norm": 16.973087310791016,
      "learning_rate": 1e-05,
      "loss": 6.7812,
      "step": 2392
    },
    {
      "epoch": 0.5185907859078591,
      "step": 2392,
      "training_loss": 8.480976104736328
    },
    {
      "epoch": 0.5185907859078591,
      "step": 2392,
      "training_loss": 5.619192600250244
    },
    {
      "epoch": 0.5185907859078591,
      "step": 2392,
      "training_loss": 7.405496597290039
    },
    {
      "epoch": 0.5185907859078591,
      "step": 2392,
      "training_loss": 6.109685897827148
    },
    {
      "epoch": 0.5188075880758808,
      "step": 2393,
      "training_loss": 6.013442039489746
    },
    {
      "epoch": 0.5188075880758808,
      "step": 2393,
      "training_loss": 6.115921497344971
    },
    {
      "epoch": 0.5188075880758808,
      "step": 2393,
      "training_loss": 6.187787055969238
    },
    {
      "epoch": 0.5188075880758808,
      "step": 2393,
      "training_loss": 4.978632926940918
    },
    {
      "epoch": 0.5190243902439025,
      "step": 2394,
      "training_loss": 6.651200294494629
    },
    {
      "epoch": 0.5190243902439025,
      "step": 2394,
      "training_loss": 6.912487030029297
    },
    {
      "epoch": 0.5190243902439025,
      "step": 2394,
      "training_loss": 6.489582538604736
    },
    {
      "epoch": 0.5190243902439025,
      "step": 2394,
      "training_loss": 7.013796806335449
    },
    {
      "epoch": 0.5192411924119241,
      "step": 2395,
      "training_loss": 6.394256114959717
    },
    {
      "epoch": 0.5192411924119241,
      "step": 2395,
      "training_loss": 6.121811866760254
    },
    {
      "epoch": 0.5192411924119241,
      "step": 2395,
      "training_loss": 6.391934394836426
    },
    {
      "epoch": 0.5192411924119241,
      "step": 2395,
      "training_loss": 7.196892738342285
    },
    {
      "epoch": 0.5194579945799458,
      "grad_norm": 19.473854064941406,
      "learning_rate": 1e-05,
      "loss": 6.5052,
      "step": 2396
    },
    {
      "epoch": 0.5194579945799458,
      "step": 2396,
      "training_loss": 5.32023811340332
    },
    {
      "epoch": 0.5194579945799458,
      "step": 2396,
      "training_loss": 7.426912784576416
    },
    {
      "epoch": 0.5194579945799458,
      "step": 2396,
      "training_loss": 7.075499534606934
    },
    {
      "epoch": 0.5194579945799458,
      "step": 2396,
      "training_loss": 6.528901100158691
    },
    {
      "epoch": 0.5196747967479675,
      "step": 2397,
      "training_loss": 7.463620662689209
    },
    {
      "epoch": 0.5196747967479675,
      "step": 2397,
      "training_loss": 7.189627647399902
    },
    {
      "epoch": 0.5196747967479675,
      "step": 2397,
      "training_loss": 6.3818278312683105
    },
    {
      "epoch": 0.5196747967479675,
      "step": 2397,
      "training_loss": 8.065093994140625
    },
    {
      "epoch": 0.5198915989159891,
      "step": 2398,
      "training_loss": 5.166183948516846
    },
    {
      "epoch": 0.5198915989159891,
      "step": 2398,
      "training_loss": 6.742673873901367
    },
    {
      "epoch": 0.5198915989159891,
      "step": 2398,
      "training_loss": 5.232017993927002
    },
    {
      "epoch": 0.5198915989159891,
      "step": 2398,
      "training_loss": 7.651475429534912
    },
    {
      "epoch": 0.5201084010840108,
      "step": 2399,
      "training_loss": 7.4912567138671875
    },
    {
      "epoch": 0.5201084010840108,
      "step": 2399,
      "training_loss": 6.063572883605957
    },
    {
      "epoch": 0.5201084010840108,
      "step": 2399,
      "training_loss": 7.2842559814453125
    },
    {
      "epoch": 0.5201084010840108,
      "step": 2399,
      "training_loss": 4.195000171661377
    },
    {
      "epoch": 0.5203252032520326,
      "grad_norm": 12.270742416381836,
      "learning_rate": 1e-05,
      "loss": 6.5799,
      "step": 2400
    },
    {
      "epoch": 0.5203252032520326,
      "step": 2400,
      "training_loss": 6.299344062805176
    },
    {
      "epoch": 0.5203252032520326,
      "step": 2400,
      "training_loss": 5.949203968048096
    },
    {
      "epoch": 0.5203252032520326,
      "step": 2400,
      "training_loss": 7.246049404144287
    },
    {
      "epoch": 0.5203252032520326,
      "step": 2400,
      "training_loss": 5.681582927703857
    },
    {
      "epoch": 0.5205420054200542,
      "step": 2401,
      "training_loss": 7.3962626457214355
    },
    {
      "epoch": 0.5205420054200542,
      "step": 2401,
      "training_loss": 3.202211380004883
    },
    {
      "epoch": 0.5205420054200542,
      "step": 2401,
      "training_loss": 3.3478074073791504
    },
    {
      "epoch": 0.5205420054200542,
      "step": 2401,
      "training_loss": 5.6323418617248535
    },
    {
      "epoch": 0.5207588075880759,
      "step": 2402,
      "training_loss": 6.627694606781006
    },
    {
      "epoch": 0.5207588075880759,
      "step": 2402,
      "training_loss": 3.475208044052124
    },
    {
      "epoch": 0.5207588075880759,
      "step": 2402,
      "training_loss": 5.961630344390869
    },
    {
      "epoch": 0.5207588075880759,
      "step": 2402,
      "training_loss": 7.928671836853027
    },
    {
      "epoch": 0.5209756097560976,
      "step": 2403,
      "training_loss": 6.758298397064209
    },
    {
      "epoch": 0.5209756097560976,
      "step": 2403,
      "training_loss": 7.89196252822876
    },
    {
      "epoch": 0.5209756097560976,
      "step": 2403,
      "training_loss": 7.205312252044678
    },
    {
      "epoch": 0.5209756097560976,
      "step": 2403,
      "training_loss": 4.559304714202881
    },
    {
      "epoch": 0.5211924119241192,
      "grad_norm": 17.6209659576416,
      "learning_rate": 1e-05,
      "loss": 5.9477,
      "step": 2404
    },
    {
      "epoch": 0.5211924119241192,
      "step": 2404,
      "training_loss": 6.736948490142822
    },
    {
      "epoch": 0.5211924119241192,
      "step": 2404,
      "training_loss": 6.040290355682373
    },
    {
      "epoch": 0.5211924119241192,
      "step": 2404,
      "training_loss": 7.110931396484375
    },
    {
      "epoch": 0.5211924119241192,
      "step": 2404,
      "training_loss": 7.005595684051514
    },
    {
      "epoch": 0.5214092140921409,
      "step": 2405,
      "training_loss": 7.462515354156494
    },
    {
      "epoch": 0.5214092140921409,
      "step": 2405,
      "training_loss": 6.540576457977295
    },
    {
      "epoch": 0.5214092140921409,
      "step": 2405,
      "training_loss": 6.710507392883301
    },
    {
      "epoch": 0.5214092140921409,
      "step": 2405,
      "training_loss": 6.071437358856201
    },
    {
      "epoch": 0.5216260162601626,
      "step": 2406,
      "training_loss": 6.703143119812012
    },
    {
      "epoch": 0.5216260162601626,
      "step": 2406,
      "training_loss": 6.588635444641113
    },
    {
      "epoch": 0.5216260162601626,
      "step": 2406,
      "training_loss": 6.193332672119141
    },
    {
      "epoch": 0.5216260162601626,
      "step": 2406,
      "training_loss": 6.491842269897461
    },
    {
      "epoch": 0.5218428184281843,
      "step": 2407,
      "training_loss": 7.063631057739258
    },
    {
      "epoch": 0.5218428184281843,
      "step": 2407,
      "training_loss": 6.414661884307861
    },
    {
      "epoch": 0.5218428184281843,
      "step": 2407,
      "training_loss": 7.044308185577393
    },
    {
      "epoch": 0.5218428184281843,
      "step": 2407,
      "training_loss": 6.338961124420166
    },
    {
      "epoch": 0.5220596205962059,
      "grad_norm": 18.131080627441406,
      "learning_rate": 1e-05,
      "loss": 6.6573,
      "step": 2408
    },
    {
      "epoch": 0.5220596205962059,
      "step": 2408,
      "training_loss": 7.731466293334961
    },
    {
      "epoch": 0.5220596205962059,
      "step": 2408,
      "training_loss": 6.217297554016113
    },
    {
      "epoch": 0.5220596205962059,
      "step": 2408,
      "training_loss": 6.022777557373047
    },
    {
      "epoch": 0.5220596205962059,
      "step": 2408,
      "training_loss": 4.893216133117676
    },
    {
      "epoch": 0.5222764227642276,
      "step": 2409,
      "training_loss": 7.676685333251953
    },
    {
      "epoch": 0.5222764227642276,
      "step": 2409,
      "training_loss": 5.262519359588623
    },
    {
      "epoch": 0.5222764227642276,
      "step": 2409,
      "training_loss": 7.2499775886535645
    },
    {
      "epoch": 0.5222764227642276,
      "step": 2409,
      "training_loss": 7.096548557281494
    },
    {
      "epoch": 0.5224932249322494,
      "step": 2410,
      "training_loss": 7.562557220458984
    },
    {
      "epoch": 0.5224932249322494,
      "step": 2410,
      "training_loss": 7.242324352264404
    },
    {
      "epoch": 0.5224932249322494,
      "step": 2410,
      "training_loss": 7.4709367752075195
    },
    {
      "epoch": 0.5224932249322494,
      "step": 2410,
      "training_loss": 6.347438335418701
    },
    {
      "epoch": 0.522710027100271,
      "step": 2411,
      "training_loss": 7.065202236175537
    },
    {
      "epoch": 0.522710027100271,
      "step": 2411,
      "training_loss": 6.523248195648193
    },
    {
      "epoch": 0.522710027100271,
      "step": 2411,
      "training_loss": 6.4192023277282715
    },
    {
      "epoch": 0.522710027100271,
      "step": 2411,
      "training_loss": 6.341074466705322
    },
    {
      "epoch": 0.5229268292682927,
      "grad_norm": 21.78373908996582,
      "learning_rate": 1e-05,
      "loss": 6.6952,
      "step": 2412
    },
    {
      "epoch": 0.5229268292682927,
      "step": 2412,
      "training_loss": 4.518552780151367
    },
    {
      "epoch": 0.5229268292682927,
      "step": 2412,
      "training_loss": 6.680163383483887
    },
    {
      "epoch": 0.5229268292682927,
      "step": 2412,
      "training_loss": 6.558603763580322
    },
    {
      "epoch": 0.5229268292682927,
      "step": 2412,
      "training_loss": 7.230609893798828
    },
    {
      "epoch": 0.5231436314363144,
      "step": 2413,
      "training_loss": 7.188659191131592
    },
    {
      "epoch": 0.5231436314363144,
      "step": 2413,
      "training_loss": 6.468571662902832
    },
    {
      "epoch": 0.5231436314363144,
      "step": 2413,
      "training_loss": 5.356505393981934
    },
    {
      "epoch": 0.5231436314363144,
      "step": 2413,
      "training_loss": 7.724391460418701
    },
    {
      "epoch": 0.523360433604336,
      "step": 2414,
      "training_loss": 6.7024712562561035
    },
    {
      "epoch": 0.523360433604336,
      "step": 2414,
      "training_loss": 2.8567216396331787
    },
    {
      "epoch": 0.523360433604336,
      "step": 2414,
      "training_loss": 6.72475528717041
    },
    {
      "epoch": 0.523360433604336,
      "step": 2414,
      "training_loss": 6.420018672943115
    },
    {
      "epoch": 0.5235772357723577,
      "step": 2415,
      "training_loss": 6.494259834289551
    },
    {
      "epoch": 0.5235772357723577,
      "step": 2415,
      "training_loss": 7.027062892913818
    },
    {
      "epoch": 0.5235772357723577,
      "step": 2415,
      "training_loss": 7.196139812469482
    },
    {
      "epoch": 0.5235772357723577,
      "step": 2415,
      "training_loss": 5.434688568115234
    },
    {
      "epoch": 0.5237940379403794,
      "grad_norm": 16.47624397277832,
      "learning_rate": 1e-05,
      "loss": 6.2864,
      "step": 2416
    },
    {
      "epoch": 0.5237940379403794,
      "step": 2416,
      "training_loss": 5.032101154327393
    },
    {
      "epoch": 0.5237940379403794,
      "step": 2416,
      "training_loss": 7.715733528137207
    },
    {
      "epoch": 0.5237940379403794,
      "step": 2416,
      "training_loss": 7.495599746704102
    },
    {
      "epoch": 0.5237940379403794,
      "step": 2416,
      "training_loss": 8.877182960510254
    },
    {
      "epoch": 0.524010840108401,
      "step": 2417,
      "training_loss": 7.8645219802856445
    },
    {
      "epoch": 0.524010840108401,
      "step": 2417,
      "training_loss": 6.676119804382324
    },
    {
      "epoch": 0.524010840108401,
      "step": 2417,
      "training_loss": 7.579245090484619
    },
    {
      "epoch": 0.524010840108401,
      "step": 2417,
      "training_loss": 6.455589294433594
    },
    {
      "epoch": 0.5242276422764227,
      "step": 2418,
      "training_loss": 4.8895697593688965
    },
    {
      "epoch": 0.5242276422764227,
      "step": 2418,
      "training_loss": 6.565418720245361
    },
    {
      "epoch": 0.5242276422764227,
      "step": 2418,
      "training_loss": 5.629371643066406
    },
    {
      "epoch": 0.5242276422764227,
      "step": 2418,
      "training_loss": 6.70966100692749
    },
    {
      "epoch": 0.5244444444444445,
      "step": 2419,
      "training_loss": 7.740799427032471
    },
    {
      "epoch": 0.5244444444444445,
      "step": 2419,
      "training_loss": 7.146761417388916
    },
    {
      "epoch": 0.5244444444444445,
      "step": 2419,
      "training_loss": 7.505447864532471
    },
    {
      "epoch": 0.5244444444444445,
      "step": 2419,
      "training_loss": 6.099588394165039
    },
    {
      "epoch": 0.5246612466124662,
      "grad_norm": 10.590203285217285,
      "learning_rate": 1e-05,
      "loss": 6.8739,
      "step": 2420
    },
    {
      "epoch": 0.5246612466124662,
      "step": 2420,
      "training_loss": 7.507143020629883
    },
    {
      "epoch": 0.5246612466124662,
      "step": 2420,
      "training_loss": 6.9800519943237305
    },
    {
      "epoch": 0.5246612466124662,
      "step": 2420,
      "training_loss": 5.194834232330322
    },
    {
      "epoch": 0.5246612466124662,
      "step": 2420,
      "training_loss": 6.6878437995910645
    },
    {
      "epoch": 0.5248780487804878,
      "step": 2421,
      "training_loss": 7.3719377517700195
    },
    {
      "epoch": 0.5248780487804878,
      "step": 2421,
      "training_loss": 7.929591178894043
    },
    {
      "epoch": 0.5248780487804878,
      "step": 2421,
      "training_loss": 6.798288345336914
    },
    {
      "epoch": 0.5248780487804878,
      "step": 2421,
      "training_loss": 6.955341815948486
    },
    {
      "epoch": 0.5250948509485095,
      "step": 2422,
      "training_loss": 7.510252952575684
    },
    {
      "epoch": 0.5250948509485095,
      "step": 2422,
      "training_loss": 6.860400676727295
    },
    {
      "epoch": 0.5250948509485095,
      "step": 2422,
      "training_loss": 6.7456769943237305
    },
    {
      "epoch": 0.5250948509485095,
      "step": 2422,
      "training_loss": 7.699413776397705
    },
    {
      "epoch": 0.5253116531165312,
      "step": 2423,
      "training_loss": 6.769697666168213
    },
    {
      "epoch": 0.5253116531165312,
      "step": 2423,
      "training_loss": 7.09545373916626
    },
    {
      "epoch": 0.5253116531165312,
      "step": 2423,
      "training_loss": 8.063552856445312
    },
    {
      "epoch": 0.5253116531165312,
      "step": 2423,
      "training_loss": 3.5272979736328125
    },
    {
      "epoch": 0.5255284552845528,
      "grad_norm": 14.173861503601074,
      "learning_rate": 1e-05,
      "loss": 6.856,
      "step": 2424
    },
    {
      "epoch": 0.5255284552845528,
      "step": 2424,
      "training_loss": 7.161866664886475
    },
    {
      "epoch": 0.5255284552845528,
      "step": 2424,
      "training_loss": 4.761056423187256
    },
    {
      "epoch": 0.5255284552845528,
      "step": 2424,
      "training_loss": 8.100838661193848
    },
    {
      "epoch": 0.5255284552845528,
      "step": 2424,
      "training_loss": 6.712644577026367
    },
    {
      "epoch": 0.5257452574525745,
      "step": 2425,
      "training_loss": 7.321484565734863
    },
    {
      "epoch": 0.5257452574525745,
      "step": 2425,
      "training_loss": 5.948812961578369
    },
    {
      "epoch": 0.5257452574525745,
      "step": 2425,
      "training_loss": 6.803844928741455
    },
    {
      "epoch": 0.5257452574525745,
      "step": 2425,
      "training_loss": 6.449286460876465
    },
    {
      "epoch": 0.5259620596205962,
      "step": 2426,
      "training_loss": 7.220708847045898
    },
    {
      "epoch": 0.5259620596205962,
      "step": 2426,
      "training_loss": 6.804266452789307
    },
    {
      "epoch": 0.5259620596205962,
      "step": 2426,
      "training_loss": 6.324527740478516
    },
    {
      "epoch": 0.5259620596205962,
      "step": 2426,
      "training_loss": 7.340290069580078
    },
    {
      "epoch": 0.5261788617886178,
      "step": 2427,
      "training_loss": 7.5715742111206055
    },
    {
      "epoch": 0.5261788617886178,
      "step": 2427,
      "training_loss": 7.011226177215576
    },
    {
      "epoch": 0.5261788617886178,
      "step": 2427,
      "training_loss": 5.972070693969727
    },
    {
      "epoch": 0.5261788617886178,
      "step": 2427,
      "training_loss": 6.66139030456543
    },
    {
      "epoch": 0.5263956639566396,
      "grad_norm": 10.267008781433105,
      "learning_rate": 1e-05,
      "loss": 6.7604,
      "step": 2428
    },
    {
      "epoch": 0.5263956639566396,
      "step": 2428,
      "training_loss": 6.395530700683594
    },
    {
      "epoch": 0.5263956639566396,
      "step": 2428,
      "training_loss": 6.778732776641846
    },
    {
      "epoch": 0.5263956639566396,
      "step": 2428,
      "training_loss": 6.465048789978027
    },
    {
      "epoch": 0.5263956639566396,
      "step": 2428,
      "training_loss": 7.069808006286621
    },
    {
      "epoch": 0.5266124661246613,
      "step": 2429,
      "training_loss": 6.9910783767700195
    },
    {
      "epoch": 0.5266124661246613,
      "step": 2429,
      "training_loss": 5.597981929779053
    },
    {
      "epoch": 0.5266124661246613,
      "step": 2429,
      "training_loss": 6.6986494064331055
    },
    {
      "epoch": 0.5266124661246613,
      "step": 2429,
      "training_loss": 6.16846227645874
    },
    {
      "epoch": 0.526829268292683,
      "step": 2430,
      "training_loss": 7.316659927368164
    },
    {
      "epoch": 0.526829268292683,
      "step": 2430,
      "training_loss": 4.907319068908691
    },
    {
      "epoch": 0.526829268292683,
      "step": 2430,
      "training_loss": 6.254692077636719
    },
    {
      "epoch": 0.526829268292683,
      "step": 2430,
      "training_loss": 7.2052001953125
    },
    {
      "epoch": 0.5270460704607046,
      "step": 2431,
      "training_loss": 6.577319145202637
    },
    {
      "epoch": 0.5270460704607046,
      "step": 2431,
      "training_loss": 3.707186222076416
    },
    {
      "epoch": 0.5270460704607046,
      "step": 2431,
      "training_loss": 6.087215900421143
    },
    {
      "epoch": 0.5270460704607046,
      "step": 2431,
      "training_loss": 5.981840133666992
    },
    {
      "epoch": 0.5272628726287263,
      "grad_norm": 15.810965538024902,
      "learning_rate": 1e-05,
      "loss": 6.2627,
      "step": 2432
    },
    {
      "epoch": 0.5272628726287263,
      "step": 2432,
      "training_loss": 3.822713613510132
    },
    {
      "epoch": 0.5272628726287263,
      "step": 2432,
      "training_loss": 5.5761260986328125
    },
    {
      "epoch": 0.5272628726287263,
      "step": 2432,
      "training_loss": 5.242640018463135
    },
    {
      "epoch": 0.5272628726287263,
      "step": 2432,
      "training_loss": 4.323057174682617
    },
    {
      "epoch": 0.527479674796748,
      "step": 2433,
      "training_loss": 7.156421184539795
    },
    {
      "epoch": 0.527479674796748,
      "step": 2433,
      "training_loss": 6.857062339782715
    },
    {
      "epoch": 0.527479674796748,
      "step": 2433,
      "training_loss": 6.021094799041748
    },
    {
      "epoch": 0.527479674796748,
      "step": 2433,
      "training_loss": 5.740505695343018
    },
    {
      "epoch": 0.5276964769647696,
      "step": 2434,
      "training_loss": 7.350259780883789
    },
    {
      "epoch": 0.5276964769647696,
      "step": 2434,
      "training_loss": 6.685749530792236
    },
    {
      "epoch": 0.5276964769647696,
      "step": 2434,
      "training_loss": 8.351984024047852
    },
    {
      "epoch": 0.5276964769647696,
      "step": 2434,
      "training_loss": 8.308066368103027
    },
    {
      "epoch": 0.5279132791327913,
      "step": 2435,
      "training_loss": 6.499783039093018
    },
    {
      "epoch": 0.5279132791327913,
      "step": 2435,
      "training_loss": 6.971601486206055
    },
    {
      "epoch": 0.5279132791327913,
      "step": 2435,
      "training_loss": 6.665708541870117
    },
    {
      "epoch": 0.5279132791327913,
      "step": 2435,
      "training_loss": 6.892879962921143
    },
    {
      "epoch": 0.528130081300813,
      "grad_norm": 14.245368957519531,
      "learning_rate": 1e-05,
      "loss": 6.4041,
      "step": 2436
    },
    {
      "epoch": 0.528130081300813,
      "step": 2436,
      "training_loss": 7.422351837158203
    },
    {
      "epoch": 0.528130081300813,
      "step": 2436,
      "training_loss": 4.9914140701293945
    },
    {
      "epoch": 0.528130081300813,
      "step": 2436,
      "training_loss": 6.619962215423584
    },
    {
      "epoch": 0.528130081300813,
      "step": 2436,
      "training_loss": 7.574504852294922
    },
    {
      "epoch": 0.5283468834688347,
      "step": 2437,
      "training_loss": 6.546422481536865
    },
    {
      "epoch": 0.5283468834688347,
      "step": 2437,
      "training_loss": 6.706356048583984
    },
    {
      "epoch": 0.5283468834688347,
      "step": 2437,
      "training_loss": 6.619331359863281
    },
    {
      "epoch": 0.5283468834688347,
      "step": 2437,
      "training_loss": 7.270297050476074
    },
    {
      "epoch": 0.5285636856368564,
      "step": 2438,
      "training_loss": 6.094545364379883
    },
    {
      "epoch": 0.5285636856368564,
      "step": 2438,
      "training_loss": 6.5639119148254395
    },
    {
      "epoch": 0.5285636856368564,
      "step": 2438,
      "training_loss": 7.352822780609131
    },
    {
      "epoch": 0.5285636856368564,
      "step": 2438,
      "training_loss": 7.250540733337402
    },
    {
      "epoch": 0.5287804878048781,
      "step": 2439,
      "training_loss": 7.013932228088379
    },
    {
      "epoch": 0.5287804878048781,
      "step": 2439,
      "training_loss": 7.633987903594971
    },
    {
      "epoch": 0.5287804878048781,
      "step": 2439,
      "training_loss": 8.696724891662598
    },
    {
      "epoch": 0.5287804878048781,
      "step": 2439,
      "training_loss": 6.742166519165039
    },
    {
      "epoch": 0.5289972899728997,
      "grad_norm": 16.098573684692383,
      "learning_rate": 1e-05,
      "loss": 6.9437,
      "step": 2440
    },
    {
      "epoch": 0.5289972899728997,
      "step": 2440,
      "training_loss": 8.273197174072266
    },
    {
      "epoch": 0.5289972899728997,
      "step": 2440,
      "training_loss": 6.426138401031494
    },
    {
      "epoch": 0.5289972899728997,
      "step": 2440,
      "training_loss": 6.768258571624756
    },
    {
      "epoch": 0.5289972899728997,
      "step": 2440,
      "training_loss": 6.132777214050293
    },
    {
      "epoch": 0.5292140921409214,
      "step": 2441,
      "training_loss": 5.698165416717529
    },
    {
      "epoch": 0.5292140921409214,
      "step": 2441,
      "training_loss": 6.403690814971924
    },
    {
      "epoch": 0.5292140921409214,
      "step": 2441,
      "training_loss": 6.744760513305664
    },
    {
      "epoch": 0.5292140921409214,
      "step": 2441,
      "training_loss": 6.941975116729736
    },
    {
      "epoch": 0.5294308943089431,
      "step": 2442,
      "training_loss": 4.169602870941162
    },
    {
      "epoch": 0.5294308943089431,
      "step": 2442,
      "training_loss": 7.461727619171143
    },
    {
      "epoch": 0.5294308943089431,
      "step": 2442,
      "training_loss": 4.646934986114502
    },
    {
      "epoch": 0.5294308943089431,
      "step": 2442,
      "training_loss": 6.7928056716918945
    },
    {
      "epoch": 0.5296476964769647,
      "step": 2443,
      "training_loss": 6.97585916519165
    },
    {
      "epoch": 0.5296476964769647,
      "step": 2443,
      "training_loss": 5.3385491371154785
    },
    {
      "epoch": 0.5296476964769647,
      "step": 2443,
      "training_loss": 4.652644157409668
    },
    {
      "epoch": 0.5296476964769647,
      "step": 2443,
      "training_loss": 7.336531639099121
    },
    {
      "epoch": 0.5298644986449864,
      "grad_norm": 17.369707107543945,
      "learning_rate": 1e-05,
      "loss": 6.2977,
      "step": 2444
    },
    {
      "epoch": 0.5298644986449864,
      "step": 2444,
      "training_loss": 7.001979351043701
    },
    {
      "epoch": 0.5298644986449864,
      "step": 2444,
      "training_loss": 6.821654796600342
    },
    {
      "epoch": 0.5298644986449864,
      "step": 2444,
      "training_loss": 6.395912170410156
    },
    {
      "epoch": 0.5298644986449864,
      "step": 2444,
      "training_loss": 9.406956672668457
    },
    {
      "epoch": 0.5300813008130081,
      "step": 2445,
      "training_loss": 6.856802940368652
    },
    {
      "epoch": 0.5300813008130081,
      "step": 2445,
      "training_loss": 7.90081262588501
    },
    {
      "epoch": 0.5300813008130081,
      "step": 2445,
      "training_loss": 6.779975891113281
    },
    {
      "epoch": 0.5300813008130081,
      "step": 2445,
      "training_loss": 6.806485652923584
    },
    {
      "epoch": 0.5302981029810299,
      "step": 2446,
      "training_loss": 7.275225639343262
    },
    {
      "epoch": 0.5302981029810299,
      "step": 2446,
      "training_loss": 5.684250831604004
    },
    {
      "epoch": 0.5302981029810299,
      "step": 2446,
      "training_loss": 6.900007247924805
    },
    {
      "epoch": 0.5302981029810299,
      "step": 2446,
      "training_loss": 7.400073051452637
    },
    {
      "epoch": 0.5305149051490515,
      "step": 2447,
      "training_loss": 7.131288051605225
    },
    {
      "epoch": 0.5305149051490515,
      "step": 2447,
      "training_loss": 6.952505588531494
    },
    {
      "epoch": 0.5305149051490515,
      "step": 2447,
      "training_loss": 4.335644721984863
    },
    {
      "epoch": 0.5305149051490515,
      "step": 2447,
      "training_loss": 5.477715969085693
    },
    {
      "epoch": 0.5307317073170732,
      "grad_norm": 13.219069480895996,
      "learning_rate": 1e-05,
      "loss": 6.8205,
      "step": 2448
    },
    {
      "epoch": 0.5307317073170732,
      "step": 2448,
      "training_loss": 6.651214122772217
    },
    {
      "epoch": 0.5307317073170732,
      "step": 2448,
      "training_loss": 5.808026313781738
    },
    {
      "epoch": 0.5307317073170732,
      "step": 2448,
      "training_loss": 6.2275285720825195
    },
    {
      "epoch": 0.5307317073170732,
      "step": 2448,
      "training_loss": 6.188040256500244
    },
    {
      "epoch": 0.5309485094850949,
      "step": 2449,
      "training_loss": 7.253605842590332
    },
    {
      "epoch": 0.5309485094850949,
      "step": 2449,
      "training_loss": 6.49428653717041
    },
    {
      "epoch": 0.5309485094850949,
      "step": 2449,
      "training_loss": 7.064958572387695
    },
    {
      "epoch": 0.5309485094850949,
      "step": 2449,
      "training_loss": 5.846770763397217
    },
    {
      "epoch": 0.5311653116531165,
      "step": 2450,
      "training_loss": 6.7003607749938965
    },
    {
      "epoch": 0.5311653116531165,
      "step": 2450,
      "training_loss": 7.702734470367432
    },
    {
      "epoch": 0.5311653116531165,
      "step": 2450,
      "training_loss": 5.554547309875488
    },
    {
      "epoch": 0.5311653116531165,
      "step": 2450,
      "training_loss": 4.580681324005127
    },
    {
      "epoch": 0.5313821138211382,
      "step": 2451,
      "training_loss": 6.478452205657959
    },
    {
      "epoch": 0.5313821138211382,
      "step": 2451,
      "training_loss": 5.938268184661865
    },
    {
      "epoch": 0.5313821138211382,
      "step": 2451,
      "training_loss": 7.263428211212158
    },
    {
      "epoch": 0.5313821138211382,
      "step": 2451,
      "training_loss": 8.12537670135498
    },
    {
      "epoch": 0.5315989159891599,
      "grad_norm": 14.169733047485352,
      "learning_rate": 1e-05,
      "loss": 6.4924,
      "step": 2452
    },
    {
      "epoch": 0.5315989159891599,
      "step": 2452,
      "training_loss": 4.13156795501709
    },
    {
      "epoch": 0.5315989159891599,
      "step": 2452,
      "training_loss": 7.210097312927246
    },
    {
      "epoch": 0.5315989159891599,
      "step": 2452,
      "training_loss": 4.321993350982666
    },
    {
      "epoch": 0.5315989159891599,
      "step": 2452,
      "training_loss": 6.795792102813721
    },
    {
      "epoch": 0.5318157181571815,
      "step": 2453,
      "training_loss": 6.339617729187012
    },
    {
      "epoch": 0.5318157181571815,
      "step": 2453,
      "training_loss": 6.934017658233643
    },
    {
      "epoch": 0.5318157181571815,
      "step": 2453,
      "training_loss": 7.2469024658203125
    },
    {
      "epoch": 0.5318157181571815,
      "step": 2453,
      "training_loss": 6.422988414764404
    },
    {
      "epoch": 0.5320325203252032,
      "step": 2454,
      "training_loss": 7.5610551834106445
    },
    {
      "epoch": 0.5320325203252032,
      "step": 2454,
      "training_loss": 7.348449230194092
    },
    {
      "epoch": 0.5320325203252032,
      "step": 2454,
      "training_loss": 5.983982563018799
    },
    {
      "epoch": 0.5320325203252032,
      "step": 2454,
      "training_loss": 6.811269760131836
    },
    {
      "epoch": 0.532249322493225,
      "step": 2455,
      "training_loss": 7.221358776092529
    },
    {
      "epoch": 0.532249322493225,
      "step": 2455,
      "training_loss": 4.8126935958862305
    },
    {
      "epoch": 0.532249322493225,
      "step": 2455,
      "training_loss": 6.452228546142578
    },
    {
      "epoch": 0.532249322493225,
      "step": 2455,
      "training_loss": 6.38827657699585
    },
    {
      "epoch": 0.5324661246612467,
      "grad_norm": 18.189546585083008,
      "learning_rate": 1e-05,
      "loss": 6.3739,
      "step": 2456
    },
    {
      "epoch": 0.5324661246612467,
      "step": 2456,
      "training_loss": 6.956736087799072
    },
    {
      "epoch": 0.5324661246612467,
      "step": 2456,
      "training_loss": 6.327145099639893
    },
    {
      "epoch": 0.5324661246612467,
      "step": 2456,
      "training_loss": 6.124685287475586
    },
    {
      "epoch": 0.5324661246612467,
      "step": 2456,
      "training_loss": 3.846693992614746
    },
    {
      "epoch": 0.5326829268292683,
      "step": 2457,
      "training_loss": 7.047221660614014
    },
    {
      "epoch": 0.5326829268292683,
      "step": 2457,
      "training_loss": 5.943306922912598
    },
    {
      "epoch": 0.5326829268292683,
      "step": 2457,
      "training_loss": 6.89940071105957
    },
    {
      "epoch": 0.5326829268292683,
      "step": 2457,
      "training_loss": 4.595205307006836
    },
    {
      "epoch": 0.53289972899729,
      "step": 2458,
      "training_loss": 8.445108413696289
    },
    {
      "epoch": 0.53289972899729,
      "step": 2458,
      "training_loss": 6.420080661773682
    },
    {
      "epoch": 0.53289972899729,
      "step": 2458,
      "training_loss": 5.2590107917785645
    },
    {
      "epoch": 0.53289972899729,
      "step": 2458,
      "training_loss": 6.7825446128845215
    },
    {
      "epoch": 0.5331165311653117,
      "step": 2459,
      "training_loss": 7.339679718017578
    },
    {
      "epoch": 0.5331165311653117,
      "step": 2459,
      "training_loss": 7.446835994720459
    },
    {
      "epoch": 0.5331165311653117,
      "step": 2459,
      "training_loss": 6.455388069152832
    },
    {
      "epoch": 0.5331165311653117,
      "step": 2459,
      "training_loss": 6.925746917724609
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 14.456413269042969,
      "learning_rate": 1e-05,
      "loss": 6.4259,
      "step": 2460
    },
    {
      "epoch": 0.5333333333333333,
      "step": 2460,
      "training_loss": 7.269227981567383
    },
    {
      "epoch": 0.5333333333333333,
      "step": 2460,
      "training_loss": 6.677590370178223
    },
    {
      "epoch": 0.5333333333333333,
      "step": 2460,
      "training_loss": 5.294984340667725
    },
    {
      "epoch": 0.5333333333333333,
      "step": 2460,
      "training_loss": 5.070756435394287
    },
    {
      "epoch": 0.533550135501355,
      "step": 2461,
      "training_loss": 7.573281764984131
    },
    {
      "epoch": 0.533550135501355,
      "step": 2461,
      "training_loss": 6.76980447769165
    },
    {
      "epoch": 0.533550135501355,
      "step": 2461,
      "training_loss": 7.445694446563721
    },
    {
      "epoch": 0.533550135501355,
      "step": 2461,
      "training_loss": 6.862390041351318
    },
    {
      "epoch": 0.5337669376693767,
      "step": 2462,
      "training_loss": 6.213740825653076
    },
    {
      "epoch": 0.5337669376693767,
      "step": 2462,
      "training_loss": 4.264669418334961
    },
    {
      "epoch": 0.5337669376693767,
      "step": 2462,
      "training_loss": 6.501062393188477
    },
    {
      "epoch": 0.5337669376693767,
      "step": 2462,
      "training_loss": 6.242552757263184
    },
    {
      "epoch": 0.5339837398373983,
      "step": 2463,
      "training_loss": 6.637704372406006
    },
    {
      "epoch": 0.5339837398373983,
      "step": 2463,
      "training_loss": 4.5213236808776855
    },
    {
      "epoch": 0.5339837398373983,
      "step": 2463,
      "training_loss": 7.513641357421875
    },
    {
      "epoch": 0.5339837398373983,
      "step": 2463,
      "training_loss": 7.067529678344727
    },
    {
      "epoch": 0.5342005420054201,
      "grad_norm": 11.079200744628906,
      "learning_rate": 1e-05,
      "loss": 6.3704,
      "step": 2464
    },
    {
      "epoch": 0.5342005420054201,
      "step": 2464,
      "training_loss": 6.414031505584717
    },
    {
      "epoch": 0.5342005420054201,
      "step": 2464,
      "training_loss": 6.334391117095947
    },
    {
      "epoch": 0.5342005420054201,
      "step": 2464,
      "training_loss": 4.626256942749023
    },
    {
      "epoch": 0.5342005420054201,
      "step": 2464,
      "training_loss": 7.1412506103515625
    },
    {
      "epoch": 0.5344173441734418,
      "step": 2465,
      "training_loss": 7.240773677825928
    },
    {
      "epoch": 0.5344173441734418,
      "step": 2465,
      "training_loss": 6.703868865966797
    },
    {
      "epoch": 0.5344173441734418,
      "step": 2465,
      "training_loss": 5.079730033874512
    },
    {
      "epoch": 0.5344173441734418,
      "step": 2465,
      "training_loss": 2.832484483718872
    },
    {
      "epoch": 0.5346341463414634,
      "step": 2466,
      "training_loss": 7.193412780761719
    },
    {
      "epoch": 0.5346341463414634,
      "step": 2466,
      "training_loss": 6.238700866699219
    },
    {
      "epoch": 0.5346341463414634,
      "step": 2466,
      "training_loss": 7.132015705108643
    },
    {
      "epoch": 0.5346341463414634,
      "step": 2466,
      "training_loss": 7.9337992668151855
    },
    {
      "epoch": 0.5348509485094851,
      "step": 2467,
      "training_loss": 6.906501770019531
    },
    {
      "epoch": 0.5348509485094851,
      "step": 2467,
      "training_loss": 7.814456939697266
    },
    {
      "epoch": 0.5348509485094851,
      "step": 2467,
      "training_loss": 4.45345401763916
    },
    {
      "epoch": 0.5348509485094851,
      "step": 2467,
      "training_loss": 6.310622692108154
    },
    {
      "epoch": 0.5350677506775068,
      "grad_norm": 21.03512954711914,
      "learning_rate": 1e-05,
      "loss": 6.2722,
      "step": 2468
    },
    {
      "epoch": 0.5350677506775068,
      "step": 2468,
      "training_loss": 6.580535888671875
    },
    {
      "epoch": 0.5350677506775068,
      "step": 2468,
      "training_loss": 6.21829080581665
    },
    {
      "epoch": 0.5350677506775068,
      "step": 2468,
      "training_loss": 6.803530216217041
    },
    {
      "epoch": 0.5350677506775068,
      "step": 2468,
      "training_loss": 7.596822261810303
    },
    {
      "epoch": 0.5352845528455284,
      "step": 2469,
      "training_loss": 4.849029541015625
    },
    {
      "epoch": 0.5352845528455284,
      "step": 2469,
      "training_loss": 4.4683051109313965
    },
    {
      "epoch": 0.5352845528455284,
      "step": 2469,
      "training_loss": 6.680416107177734
    },
    {
      "epoch": 0.5352845528455284,
      "step": 2469,
      "training_loss": 6.586596965789795
    },
    {
      "epoch": 0.5355013550135501,
      "step": 2470,
      "training_loss": 6.164550304412842
    },
    {
      "epoch": 0.5355013550135501,
      "step": 2470,
      "training_loss": 4.116822719573975
    },
    {
      "epoch": 0.5355013550135501,
      "step": 2470,
      "training_loss": 5.2173237800598145
    },
    {
      "epoch": 0.5355013550135501,
      "step": 2470,
      "training_loss": 7.231840133666992
    },
    {
      "epoch": 0.5357181571815718,
      "step": 2471,
      "training_loss": 6.625894069671631
    },
    {
      "epoch": 0.5357181571815718,
      "step": 2471,
      "training_loss": 4.950755596160889
    },
    {
      "epoch": 0.5357181571815718,
      "step": 2471,
      "training_loss": 6.363455295562744
    },
    {
      "epoch": 0.5357181571815718,
      "step": 2471,
      "training_loss": 7.366366863250732
    },
    {
      "epoch": 0.5359349593495935,
      "grad_norm": 10.19441032409668,
      "learning_rate": 1e-05,
      "loss": 6.1138,
      "step": 2472
    },
    {
      "epoch": 0.5359349593495935,
      "step": 2472,
      "training_loss": 7.354667663574219
    },
    {
      "epoch": 0.5359349593495935,
      "step": 2472,
      "training_loss": 5.175702095031738
    },
    {
      "epoch": 0.5359349593495935,
      "step": 2472,
      "training_loss": 6.478110313415527
    },
    {
      "epoch": 0.5359349593495935,
      "step": 2472,
      "training_loss": 7.578778266906738
    },
    {
      "epoch": 0.5361517615176151,
      "step": 2473,
      "training_loss": 7.4611101150512695
    },
    {
      "epoch": 0.5361517615176151,
      "step": 2473,
      "training_loss": 6.311798572540283
    },
    {
      "epoch": 0.5361517615176151,
      "step": 2473,
      "training_loss": 7.479963302612305
    },
    {
      "epoch": 0.5361517615176151,
      "step": 2473,
      "training_loss": 6.315715312957764
    },
    {
      "epoch": 0.5363685636856369,
      "step": 2474,
      "training_loss": 7.1909589767456055
    },
    {
      "epoch": 0.5363685636856369,
      "step": 2474,
      "training_loss": 5.2595720291137695
    },
    {
      "epoch": 0.5363685636856369,
      "step": 2474,
      "training_loss": 5.22700309753418
    },
    {
      "epoch": 0.5363685636856369,
      "step": 2474,
      "training_loss": 6.485245704650879
    },
    {
      "epoch": 0.5365853658536586,
      "step": 2475,
      "training_loss": 5.867590427398682
    },
    {
      "epoch": 0.5365853658536586,
      "step": 2475,
      "training_loss": 5.685427665710449
    },
    {
      "epoch": 0.5365853658536586,
      "step": 2475,
      "training_loss": 6.518599987030029
    },
    {
      "epoch": 0.5365853658536586,
      "step": 2475,
      "training_loss": 7.156356334686279
    },
    {
      "epoch": 0.5368021680216802,
      "grad_norm": 20.896257400512695,
      "learning_rate": 1e-05,
      "loss": 6.4717,
      "step": 2476
    },
    {
      "epoch": 0.5368021680216802,
      "step": 2476,
      "training_loss": 7.543315410614014
    },
    {
      "epoch": 0.5368021680216802,
      "step": 2476,
      "training_loss": 7.5136799812316895
    },
    {
      "epoch": 0.5368021680216802,
      "step": 2476,
      "training_loss": 3.6603074073791504
    },
    {
      "epoch": 0.5368021680216802,
      "step": 2476,
      "training_loss": 7.1801605224609375
    },
    {
      "epoch": 0.5370189701897019,
      "step": 2477,
      "training_loss": 6.577567100524902
    },
    {
      "epoch": 0.5370189701897019,
      "step": 2477,
      "training_loss": 6.580782413482666
    },
    {
      "epoch": 0.5370189701897019,
      "step": 2477,
      "training_loss": 6.782264232635498
    },
    {
      "epoch": 0.5370189701897019,
      "step": 2477,
      "training_loss": 7.0723042488098145
    },
    {
      "epoch": 0.5372357723577236,
      "step": 2478,
      "training_loss": 5.784104824066162
    },
    {
      "epoch": 0.5372357723577236,
      "step": 2478,
      "training_loss": 5.482385635375977
    },
    {
      "epoch": 0.5372357723577236,
      "step": 2478,
      "training_loss": 5.740139007568359
    },
    {
      "epoch": 0.5372357723577236,
      "step": 2478,
      "training_loss": 5.618990421295166
    },
    {
      "epoch": 0.5374525745257452,
      "step": 2479,
      "training_loss": 7.320046901702881
    },
    {
      "epoch": 0.5374525745257452,
      "step": 2479,
      "training_loss": 5.947285175323486
    },
    {
      "epoch": 0.5374525745257452,
      "step": 2479,
      "training_loss": 7.072249412536621
    },
    {
      "epoch": 0.5374525745257452,
      "step": 2479,
      "training_loss": 6.520090579986572
    },
    {
      "epoch": 0.5376693766937669,
      "grad_norm": 17.64494514465332,
      "learning_rate": 1e-05,
      "loss": 6.3997,
      "step": 2480
    },
    {
      "epoch": 0.5376693766937669,
      "step": 2480,
      "training_loss": 6.014915466308594
    },
    {
      "epoch": 0.5376693766937669,
      "step": 2480,
      "training_loss": 7.498510360717773
    },
    {
      "epoch": 0.5376693766937669,
      "step": 2480,
      "training_loss": 5.814742565155029
    },
    {
      "epoch": 0.5376693766937669,
      "step": 2480,
      "training_loss": 4.539919376373291
    },
    {
      "epoch": 0.5378861788617886,
      "step": 2481,
      "training_loss": 7.760427474975586
    },
    {
      "epoch": 0.5378861788617886,
      "step": 2481,
      "training_loss": 7.5252909660339355
    },
    {
      "epoch": 0.5378861788617886,
      "step": 2481,
      "training_loss": 5.344486236572266
    },
    {
      "epoch": 0.5378861788617886,
      "step": 2481,
      "training_loss": 6.279211521148682
    },
    {
      "epoch": 0.5381029810298102,
      "step": 2482,
      "training_loss": 7.200552940368652
    },
    {
      "epoch": 0.5381029810298102,
      "step": 2482,
      "training_loss": 7.416916847229004
    },
    {
      "epoch": 0.5381029810298102,
      "step": 2482,
      "training_loss": 5.413933277130127
    },
    {
      "epoch": 0.5381029810298102,
      "step": 2482,
      "training_loss": 7.799568176269531
    },
    {
      "epoch": 0.538319783197832,
      "step": 2483,
      "training_loss": 5.111942768096924
    },
    {
      "epoch": 0.538319783197832,
      "step": 2483,
      "training_loss": 6.309566020965576
    },
    {
      "epoch": 0.538319783197832,
      "step": 2483,
      "training_loss": 5.092698574066162
    },
    {
      "epoch": 0.538319783197832,
      "step": 2483,
      "training_loss": 7.145285129547119
    },
    {
      "epoch": 0.5385365853658537,
      "grad_norm": 16.5080623626709,
      "learning_rate": 1e-05,
      "loss": 6.3917,
      "step": 2484
    },
    {
      "epoch": 0.5385365853658537,
      "step": 2484,
      "training_loss": 5.286018371582031
    },
    {
      "epoch": 0.5385365853658537,
      "step": 2484,
      "training_loss": 6.277685165405273
    },
    {
      "epoch": 0.5385365853658537,
      "step": 2484,
      "training_loss": 7.247642517089844
    },
    {
      "epoch": 0.5385365853658537,
      "step": 2484,
      "training_loss": 6.0896315574646
    },
    {
      "epoch": 0.5387533875338754,
      "step": 2485,
      "training_loss": 6.350979804992676
    },
    {
      "epoch": 0.5387533875338754,
      "step": 2485,
      "training_loss": 6.459243297576904
    },
    {
      "epoch": 0.5387533875338754,
      "step": 2485,
      "training_loss": 5.723128795623779
    },
    {
      "epoch": 0.5387533875338754,
      "step": 2485,
      "training_loss": 6.444842338562012
    },
    {
      "epoch": 0.538970189701897,
      "step": 2486,
      "training_loss": 5.521676063537598
    },
    {
      "epoch": 0.538970189701897,
      "step": 2486,
      "training_loss": 3.984861135482788
    },
    {
      "epoch": 0.538970189701897,
      "step": 2486,
      "training_loss": 7.2607502937316895
    },
    {
      "epoch": 0.538970189701897,
      "step": 2486,
      "training_loss": 4.171811580657959
    },
    {
      "epoch": 0.5391869918699187,
      "step": 2487,
      "training_loss": 7.234951496124268
    },
    {
      "epoch": 0.5391869918699187,
      "step": 2487,
      "training_loss": 6.1048502922058105
    },
    {
      "epoch": 0.5391869918699187,
      "step": 2487,
      "training_loss": 7.018829345703125
    },
    {
      "epoch": 0.5391869918699187,
      "step": 2487,
      "training_loss": 6.408084392547607
    },
    {
      "epoch": 0.5394037940379404,
      "grad_norm": 14.595316886901855,
      "learning_rate": 1e-05,
      "loss": 6.0991,
      "step": 2488
    },
    {
      "epoch": 0.5394037940379404,
      "step": 2488,
      "training_loss": 6.874500274658203
    },
    {
      "epoch": 0.5394037940379404,
      "step": 2488,
      "training_loss": 6.921065807342529
    },
    {
      "epoch": 0.5394037940379404,
      "step": 2488,
      "training_loss": 5.259487628936768
    },
    {
      "epoch": 0.5394037940379404,
      "step": 2488,
      "training_loss": 6.1823201179504395
    },
    {
      "epoch": 0.539620596205962,
      "step": 2489,
      "training_loss": 7.42519998550415
    },
    {
      "epoch": 0.539620596205962,
      "step": 2489,
      "training_loss": 6.582845687866211
    },
    {
      "epoch": 0.539620596205962,
      "step": 2489,
      "training_loss": 6.4158101081848145
    },
    {
      "epoch": 0.539620596205962,
      "step": 2489,
      "training_loss": 6.297733783721924
    },
    {
      "epoch": 0.5398373983739837,
      "step": 2490,
      "training_loss": 6.377482891082764
    },
    {
      "epoch": 0.5398373983739837,
      "step": 2490,
      "training_loss": 7.716909408569336
    },
    {
      "epoch": 0.5398373983739837,
      "step": 2490,
      "training_loss": 3.3809783458709717
    },
    {
      "epoch": 0.5398373983739837,
      "step": 2490,
      "training_loss": 6.121816158294678
    },
    {
      "epoch": 0.5400542005420054,
      "step": 2491,
      "training_loss": 6.52606725692749
    },
    {
      "epoch": 0.5400542005420054,
      "step": 2491,
      "training_loss": 7.075088024139404
    },
    {
      "epoch": 0.5400542005420054,
      "step": 2491,
      "training_loss": 7.423009872436523
    },
    {
      "epoch": 0.5400542005420054,
      "step": 2491,
      "training_loss": 7.942169189453125
    },
    {
      "epoch": 0.5402710027100271,
      "grad_norm": 11.56108283996582,
      "learning_rate": 1e-05,
      "loss": 6.5327,
      "step": 2492
    },
    {
      "epoch": 0.5402710027100271,
      "step": 2492,
      "training_loss": 7.0847883224487305
    },
    {
      "epoch": 0.5402710027100271,
      "step": 2492,
      "training_loss": 7.1945977210998535
    },
    {
      "epoch": 0.5402710027100271,
      "step": 2492,
      "training_loss": 6.092644691467285
    },
    {
      "epoch": 0.5402710027100271,
      "step": 2492,
      "training_loss": 5.285275459289551
    },
    {
      "epoch": 0.5404878048780488,
      "step": 2493,
      "training_loss": 5.201463222503662
    },
    {
      "epoch": 0.5404878048780488,
      "step": 2493,
      "training_loss": 6.486555099487305
    },
    {
      "epoch": 0.5404878048780488,
      "step": 2493,
      "training_loss": 6.589409828186035
    },
    {
      "epoch": 0.5404878048780488,
      "step": 2493,
      "training_loss": 6.49940299987793
    },
    {
      "epoch": 0.5407046070460705,
      "step": 2494,
      "training_loss": 8.039742469787598
    },
    {
      "epoch": 0.5407046070460705,
      "step": 2494,
      "training_loss": 7.700832366943359
    },
    {
      "epoch": 0.5407046070460705,
      "step": 2494,
      "training_loss": 6.836652755737305
    },
    {
      "epoch": 0.5407046070460705,
      "step": 2494,
      "training_loss": 5.376169681549072
    },
    {
      "epoch": 0.5409214092140922,
      "step": 2495,
      "training_loss": 6.714088439941406
    },
    {
      "epoch": 0.5409214092140922,
      "step": 2495,
      "training_loss": 5.054221153259277
    },
    {
      "epoch": 0.5409214092140922,
      "step": 2495,
      "training_loss": 7.47214412689209
    },
    {
      "epoch": 0.5409214092140922,
      "step": 2495,
      "training_loss": 5.263039588928223
    },
    {
      "epoch": 0.5411382113821138,
      "grad_norm": 14.537464141845703,
      "learning_rate": 1e-05,
      "loss": 6.4307,
      "step": 2496
    },
    {
      "epoch": 0.5411382113821138,
      "step": 2496,
      "training_loss": 4.6641950607299805
    },
    {
      "epoch": 0.5411382113821138,
      "step": 2496,
      "training_loss": 6.509882926940918
    },
    {
      "epoch": 0.5411382113821138,
      "step": 2496,
      "training_loss": 7.507375717163086
    },
    {
      "epoch": 0.5411382113821138,
      "step": 2496,
      "training_loss": 7.238286018371582
    },
    {
      "epoch": 0.5413550135501355,
      "step": 2497,
      "training_loss": 6.42516565322876
    },
    {
      "epoch": 0.5413550135501355,
      "step": 2497,
      "training_loss": 7.274042129516602
    },
    {
      "epoch": 0.5413550135501355,
      "step": 2497,
      "training_loss": 7.199034690856934
    },
    {
      "epoch": 0.5413550135501355,
      "step": 2497,
      "training_loss": 7.933669090270996
    },
    {
      "epoch": 0.5415718157181572,
      "step": 2498,
      "training_loss": 6.522284030914307
    },
    {
      "epoch": 0.5415718157181572,
      "step": 2498,
      "training_loss": 7.830334663391113
    },
    {
      "epoch": 0.5415718157181572,
      "step": 2498,
      "training_loss": 7.505261421203613
    },
    {
      "epoch": 0.5415718157181572,
      "step": 2498,
      "training_loss": 6.753303527832031
    },
    {
      "epoch": 0.5417886178861788,
      "step": 2499,
      "training_loss": 3.3519678115844727
    },
    {
      "epoch": 0.5417886178861788,
      "step": 2499,
      "training_loss": 6.272243499755859
    },
    {
      "epoch": 0.5417886178861788,
      "step": 2499,
      "training_loss": 3.2234623432159424
    },
    {
      "epoch": 0.5417886178861788,
      "step": 2499,
      "training_loss": 6.12546443939209
    },
    {
      "epoch": 0.5420054200542005,
      "grad_norm": 16.244226455688477,
      "learning_rate": 1e-05,
      "loss": 6.396,
      "step": 2500
    },
    {
      "epoch": 0.5420054200542005,
      "step": 2500,
      "training_loss": 4.076314926147461
    },
    {
      "epoch": 0.5420054200542005,
      "step": 2500,
      "training_loss": 5.9368815422058105
    },
    {
      "epoch": 0.5420054200542005,
      "step": 2500,
      "training_loss": 6.998067378997803
    },
    {
      "epoch": 0.5420054200542005,
      "step": 2500,
      "training_loss": 6.21694803237915
    },
    {
      "epoch": 0.5422222222222223,
      "step": 2501,
      "training_loss": 6.893513202667236
    },
    {
      "epoch": 0.5422222222222223,
      "step": 2501,
      "training_loss": 6.650972366333008
    },
    {
      "epoch": 0.5422222222222223,
      "step": 2501,
      "training_loss": 5.512124538421631
    },
    {
      "epoch": 0.5422222222222223,
      "step": 2501,
      "training_loss": 6.818501949310303
    },
    {
      "epoch": 0.5424390243902439,
      "step": 2502,
      "training_loss": 6.752748012542725
    },
    {
      "epoch": 0.5424390243902439,
      "step": 2502,
      "training_loss": 6.12888240814209
    },
    {
      "epoch": 0.5424390243902439,
      "step": 2502,
      "training_loss": 6.651960849761963
    },
    {
      "epoch": 0.5424390243902439,
      "step": 2502,
      "training_loss": 6.749946117401123
    },
    {
      "epoch": 0.5426558265582656,
      "step": 2503,
      "training_loss": 3.9851043224334717
    },
    {
      "epoch": 0.5426558265582656,
      "step": 2503,
      "training_loss": 7.082226276397705
    },
    {
      "epoch": 0.5426558265582656,
      "step": 2503,
      "training_loss": 5.2186102867126465
    },
    {
      "epoch": 0.5426558265582656,
      "step": 2503,
      "training_loss": 8.114476203918457
    },
    {
      "epoch": 0.5428726287262873,
      "grad_norm": 12.762441635131836,
      "learning_rate": 1e-05,
      "loss": 6.2367,
      "step": 2504
    },
    {
      "epoch": 0.5428726287262873,
      "step": 2504,
      "training_loss": 7.361797332763672
    },
    {
      "epoch": 0.5428726287262873,
      "step": 2504,
      "training_loss": 6.39083194732666
    },
    {
      "epoch": 0.5428726287262873,
      "step": 2504,
      "training_loss": 6.015937805175781
    },
    {
      "epoch": 0.5428726287262873,
      "step": 2504,
      "training_loss": 7.236480712890625
    },
    {
      "epoch": 0.5430894308943089,
      "step": 2505,
      "training_loss": 6.448078632354736
    },
    {
      "epoch": 0.5430894308943089,
      "step": 2505,
      "training_loss": 6.629841327667236
    },
    {
      "epoch": 0.5430894308943089,
      "step": 2505,
      "training_loss": 6.2277421951293945
    },
    {
      "epoch": 0.5430894308943089,
      "step": 2505,
      "training_loss": 7.836231708526611
    },
    {
      "epoch": 0.5433062330623306,
      "step": 2506,
      "training_loss": 6.841184139251709
    },
    {
      "epoch": 0.5433062330623306,
      "step": 2506,
      "training_loss": 9.464484214782715
    },
    {
      "epoch": 0.5433062330623306,
      "step": 2506,
      "training_loss": 7.3729248046875
    },
    {
      "epoch": 0.5433062330623306,
      "step": 2506,
      "training_loss": 6.974954128265381
    },
    {
      "epoch": 0.5435230352303523,
      "step": 2507,
      "training_loss": 5.805340766906738
    },
    {
      "epoch": 0.5435230352303523,
      "step": 2507,
      "training_loss": 7.07988977432251
    },
    {
      "epoch": 0.5435230352303523,
      "step": 2507,
      "training_loss": 7.3986101150512695
    },
    {
      "epoch": 0.5435230352303523,
      "step": 2507,
      "training_loss": 3.682676076889038
    },
    {
      "epoch": 0.543739837398374,
      "grad_norm": 21.03561019897461,
      "learning_rate": 1e-05,
      "loss": 6.7979,
      "step": 2508
    },
    {
      "epoch": 0.543739837398374,
      "step": 2508,
      "training_loss": 4.918684005737305
    },
    {
      "epoch": 0.543739837398374,
      "step": 2508,
      "training_loss": 6.091369152069092
    },
    {
      "epoch": 0.543739837398374,
      "step": 2508,
      "training_loss": 7.320098876953125
    },
    {
      "epoch": 0.543739837398374,
      "step": 2508,
      "training_loss": 6.522042274475098
    },
    {
      "epoch": 0.5439566395663956,
      "step": 2509,
      "training_loss": 5.630570888519287
    },
    {
      "epoch": 0.5439566395663956,
      "step": 2509,
      "training_loss": 5.2744460105896
    },
    {
      "epoch": 0.5439566395663956,
      "step": 2509,
      "training_loss": 7.751403331756592
    },
    {
      "epoch": 0.5439566395663956,
      "step": 2509,
      "training_loss": 6.228184223175049
    },
    {
      "epoch": 0.5441734417344174,
      "step": 2510,
      "training_loss": 6.123007297515869
    },
    {
      "epoch": 0.5441734417344174,
      "step": 2510,
      "training_loss": 6.222514629364014
    },
    {
      "epoch": 0.5441734417344174,
      "step": 2510,
      "training_loss": 6.340274810791016
    },
    {
      "epoch": 0.5441734417344174,
      "step": 2510,
      "training_loss": 5.3051323890686035
    },
    {
      "epoch": 0.5443902439024391,
      "step": 2511,
      "training_loss": 6.545347690582275
    },
    {
      "epoch": 0.5443902439024391,
      "step": 2511,
      "training_loss": 3.9768102169036865
    },
    {
      "epoch": 0.5443902439024391,
      "step": 2511,
      "training_loss": 7.594257354736328
    },
    {
      "epoch": 0.5443902439024391,
      "step": 2511,
      "training_loss": 6.824220657348633
    },
    {
      "epoch": 0.5446070460704607,
      "grad_norm": 13.883456230163574,
      "learning_rate": 1e-05,
      "loss": 6.1668,
      "step": 2512
    },
    {
      "epoch": 0.5446070460704607,
      "step": 2512,
      "training_loss": 3.778076410293579
    },
    {
      "epoch": 0.5446070460704607,
      "step": 2512,
      "training_loss": 5.1683735847473145
    },
    {
      "epoch": 0.5446070460704607,
      "step": 2512,
      "training_loss": 6.1307053565979
    },
    {
      "epoch": 0.5446070460704607,
      "step": 2512,
      "training_loss": 6.396402359008789
    },
    {
      "epoch": 0.5448238482384824,
      "step": 2513,
      "training_loss": 6.93131685256958
    },
    {
      "epoch": 0.5448238482384824,
      "step": 2513,
      "training_loss": 4.9454827308654785
    },
    {
      "epoch": 0.5448238482384824,
      "step": 2513,
      "training_loss": 7.209160804748535
    },
    {
      "epoch": 0.5448238482384824,
      "step": 2513,
      "training_loss": 7.2132673263549805
    },
    {
      "epoch": 0.5450406504065041,
      "step": 2514,
      "training_loss": 7.3104963302612305
    },
    {
      "epoch": 0.5450406504065041,
      "step": 2514,
      "training_loss": 7.367983341217041
    },
    {
      "epoch": 0.5450406504065041,
      "step": 2514,
      "training_loss": 5.482503890991211
    },
    {
      "epoch": 0.5450406504065041,
      "step": 2514,
      "training_loss": 6.353931903839111
    },
    {
      "epoch": 0.5452574525745257,
      "step": 2515,
      "training_loss": 5.8187761306762695
    },
    {
      "epoch": 0.5452574525745257,
      "step": 2515,
      "training_loss": 5.483060836791992
    },
    {
      "epoch": 0.5452574525745257,
      "step": 2515,
      "training_loss": 4.671998500823975
    },
    {
      "epoch": 0.5452574525745257,
      "step": 2515,
      "training_loss": 6.51259183883667
    },
    {
      "epoch": 0.5454742547425474,
      "grad_norm": 26.55058479309082,
      "learning_rate": 1e-05,
      "loss": 6.0484,
      "step": 2516
    },
    {
      "epoch": 0.5454742547425474,
      "step": 2516,
      "training_loss": 5.546483039855957
    },
    {
      "epoch": 0.5454742547425474,
      "step": 2516,
      "training_loss": 6.092845439910889
    },
    {
      "epoch": 0.5454742547425474,
      "step": 2516,
      "training_loss": 6.349659442901611
    },
    {
      "epoch": 0.5454742547425474,
      "step": 2516,
      "training_loss": 6.9648027420043945
    },
    {
      "epoch": 0.5456910569105691,
      "step": 2517,
      "training_loss": 4.83146333694458
    },
    {
      "epoch": 0.5456910569105691,
      "step": 2517,
      "training_loss": 6.668747901916504
    },
    {
      "epoch": 0.5456910569105691,
      "step": 2517,
      "training_loss": 7.0908942222595215
    },
    {
      "epoch": 0.5456910569105691,
      "step": 2517,
      "training_loss": 7.404158115386963
    },
    {
      "epoch": 0.5459078590785907,
      "step": 2518,
      "training_loss": 6.129542350769043
    },
    {
      "epoch": 0.5459078590785907,
      "step": 2518,
      "training_loss": 7.695522308349609
    },
    {
      "epoch": 0.5459078590785907,
      "step": 2518,
      "training_loss": 7.287656784057617
    },
    {
      "epoch": 0.5459078590785907,
      "step": 2518,
      "training_loss": 7.535760402679443
    },
    {
      "epoch": 0.5461246612466125,
      "step": 2519,
      "training_loss": 4.295282363891602
    },
    {
      "epoch": 0.5461246612466125,
      "step": 2519,
      "training_loss": 6.907879829406738
    },
    {
      "epoch": 0.5461246612466125,
      "step": 2519,
      "training_loss": 7.720829963684082
    },
    {
      "epoch": 0.5461246612466125,
      "step": 2519,
      "training_loss": 7.210655212402344
    },
    {
      "epoch": 0.5463414634146342,
      "grad_norm": 12.751442909240723,
      "learning_rate": 1e-05,
      "loss": 6.6083,
      "step": 2520
    },
    {
      "epoch": 0.5463414634146342,
      "step": 2520,
      "training_loss": 7.746955871582031
    },
    {
      "epoch": 0.5463414634146342,
      "step": 2520,
      "training_loss": 6.947404861450195
    },
    {
      "epoch": 0.5463414634146342,
      "step": 2520,
      "training_loss": 6.55989408493042
    },
    {
      "epoch": 0.5463414634146342,
      "step": 2520,
      "training_loss": 6.0251641273498535
    },
    {
      "epoch": 0.5465582655826559,
      "step": 2521,
      "training_loss": 4.840612411499023
    },
    {
      "epoch": 0.5465582655826559,
      "step": 2521,
      "training_loss": 6.5383453369140625
    },
    {
      "epoch": 0.5465582655826559,
      "step": 2521,
      "training_loss": 3.7734198570251465
    },
    {
      "epoch": 0.5465582655826559,
      "step": 2521,
      "training_loss": 6.2645721435546875
    },
    {
      "epoch": 0.5467750677506775,
      "step": 2522,
      "training_loss": 3.485203981399536
    },
    {
      "epoch": 0.5467750677506775,
      "step": 2522,
      "training_loss": 2.9757494926452637
    },
    {
      "epoch": 0.5467750677506775,
      "step": 2522,
      "training_loss": 7.120232582092285
    },
    {
      "epoch": 0.5467750677506775,
      "step": 2522,
      "training_loss": 7.39810037612915
    },
    {
      "epoch": 0.5469918699186992,
      "step": 2523,
      "training_loss": 7.970934867858887
    },
    {
      "epoch": 0.5469918699186992,
      "step": 2523,
      "training_loss": 7.3508076667785645
    },
    {
      "epoch": 0.5469918699186992,
      "step": 2523,
      "training_loss": 6.944597244262695
    },
    {
      "epoch": 0.5469918699186992,
      "step": 2523,
      "training_loss": 7.1911725997924805
    },
    {
      "epoch": 0.5472086720867209,
      "grad_norm": 17.283618927001953,
      "learning_rate": 1e-05,
      "loss": 6.1958,
      "step": 2524
    },
    {
      "epoch": 0.5472086720867209,
      "step": 2524,
      "training_loss": 6.322994232177734
    },
    {
      "epoch": 0.5472086720867209,
      "step": 2524,
      "training_loss": 5.960742950439453
    },
    {
      "epoch": 0.5472086720867209,
      "step": 2524,
      "training_loss": 6.849505424499512
    },
    {
      "epoch": 0.5472086720867209,
      "step": 2524,
      "training_loss": 7.725927829742432
    },
    {
      "epoch": 0.5474254742547425,
      "step": 2525,
      "training_loss": 5.567351341247559
    },
    {
      "epoch": 0.5474254742547425,
      "step": 2525,
      "training_loss": 7.058272361755371
    },
    {
      "epoch": 0.5474254742547425,
      "step": 2525,
      "training_loss": 7.610011577606201
    },
    {
      "epoch": 0.5474254742547425,
      "step": 2525,
      "training_loss": 6.936662197113037
    },
    {
      "epoch": 0.5476422764227642,
      "step": 2526,
      "training_loss": 5.174569606781006
    },
    {
      "epoch": 0.5476422764227642,
      "step": 2526,
      "training_loss": 6.672020435333252
    },
    {
      "epoch": 0.5476422764227642,
      "step": 2526,
      "training_loss": 8.41797924041748
    },
    {
      "epoch": 0.5476422764227642,
      "step": 2526,
      "training_loss": 6.444887161254883
    },
    {
      "epoch": 0.5478590785907859,
      "step": 2527,
      "training_loss": 4.285762310028076
    },
    {
      "epoch": 0.5478590785907859,
      "step": 2527,
      "training_loss": 7.500000953674316
    },
    {
      "epoch": 0.5478590785907859,
      "step": 2527,
      "training_loss": 6.897902965545654
    },
    {
      "epoch": 0.5478590785907859,
      "step": 2527,
      "training_loss": 6.654128551483154
    },
    {
      "epoch": 0.5480758807588076,
      "grad_norm": 25.342363357543945,
      "learning_rate": 1e-05,
      "loss": 6.6299,
      "step": 2528
    },
    {
      "epoch": 0.5480758807588076,
      "step": 2528,
      "training_loss": 6.639497756958008
    },
    {
      "epoch": 0.5480758807588076,
      "step": 2528,
      "training_loss": 7.051565647125244
    },
    {
      "epoch": 0.5480758807588076,
      "step": 2528,
      "training_loss": 6.487843036651611
    },
    {
      "epoch": 0.5480758807588076,
      "step": 2528,
      "training_loss": 7.344702243804932
    },
    {
      "epoch": 0.5482926829268293,
      "step": 2529,
      "training_loss": 6.529736518859863
    },
    {
      "epoch": 0.5482926829268293,
      "step": 2529,
      "training_loss": 6.560117244720459
    },
    {
      "epoch": 0.5482926829268293,
      "step": 2529,
      "training_loss": 5.499782085418701
    },
    {
      "epoch": 0.5482926829268293,
      "step": 2529,
      "training_loss": 4.9972243309021
    },
    {
      "epoch": 0.548509485094851,
      "step": 2530,
      "training_loss": 6.716109752655029
    },
    {
      "epoch": 0.548509485094851,
      "step": 2530,
      "training_loss": 7.128757476806641
    },
    {
      "epoch": 0.548509485094851,
      "step": 2530,
      "training_loss": 8.53891372680664
    },
    {
      "epoch": 0.548509485094851,
      "step": 2530,
      "training_loss": 7.112151145935059
    },
    {
      "epoch": 0.5487262872628726,
      "step": 2531,
      "training_loss": 7.040281295776367
    },
    {
      "epoch": 0.5487262872628726,
      "step": 2531,
      "training_loss": 7.147529602050781
    },
    {
      "epoch": 0.5487262872628726,
      "step": 2531,
      "training_loss": 4.202611446380615
    },
    {
      "epoch": 0.5487262872628726,
      "step": 2531,
      "training_loss": 6.922846794128418
    },
    {
      "epoch": 0.5489430894308943,
      "grad_norm": 14.150399208068848,
      "learning_rate": 1e-05,
      "loss": 6.62,
      "step": 2532
    },
    {
      "epoch": 0.5489430894308943,
      "step": 2532,
      "training_loss": 7.448727130889893
    },
    {
      "epoch": 0.5489430894308943,
      "step": 2532,
      "training_loss": 6.108600616455078
    },
    {
      "epoch": 0.5489430894308943,
      "step": 2532,
      "training_loss": 6.042893409729004
    },
    {
      "epoch": 0.5489430894308943,
      "step": 2532,
      "training_loss": 5.940213680267334
    },
    {
      "epoch": 0.549159891598916,
      "step": 2533,
      "training_loss": 7.371002197265625
    },
    {
      "epoch": 0.549159891598916,
      "step": 2533,
      "training_loss": 6.396240234375
    },
    {
      "epoch": 0.549159891598916,
      "step": 2533,
      "training_loss": 6.924909591674805
    },
    {
      "epoch": 0.549159891598916,
      "step": 2533,
      "training_loss": 3.682738780975342
    },
    {
      "epoch": 0.5493766937669377,
      "step": 2534,
      "training_loss": 4.950084686279297
    },
    {
      "epoch": 0.5493766937669377,
      "step": 2534,
      "training_loss": 4.493112564086914
    },
    {
      "epoch": 0.5493766937669377,
      "step": 2534,
      "training_loss": 7.360086441040039
    },
    {
      "epoch": 0.5493766937669377,
      "step": 2534,
      "training_loss": 7.299361705780029
    },
    {
      "epoch": 0.5495934959349593,
      "step": 2535,
      "training_loss": 6.806545257568359
    },
    {
      "epoch": 0.5495934959349593,
      "step": 2535,
      "training_loss": 4.689713001251221
    },
    {
      "epoch": 0.5495934959349593,
      "step": 2535,
      "training_loss": 7.65380859375
    },
    {
      "epoch": 0.5495934959349593,
      "step": 2535,
      "training_loss": 6.058578968048096
    },
    {
      "epoch": 0.549810298102981,
      "grad_norm": 15.094913482666016,
      "learning_rate": 1e-05,
      "loss": 6.2017,
      "step": 2536
    },
    {
      "epoch": 0.549810298102981,
      "step": 2536,
      "training_loss": 7.586519718170166
    },
    {
      "epoch": 0.549810298102981,
      "step": 2536,
      "training_loss": 6.411066055297852
    },
    {
      "epoch": 0.549810298102981,
      "step": 2536,
      "training_loss": 3.687717914581299
    },
    {
      "epoch": 0.549810298102981,
      "step": 2536,
      "training_loss": 8.325358390808105
    },
    {
      "epoch": 0.5500271002710027,
      "step": 2537,
      "training_loss": 6.982021331787109
    },
    {
      "epoch": 0.5500271002710027,
      "step": 2537,
      "training_loss": 7.910254001617432
    },
    {
      "epoch": 0.5500271002710027,
      "step": 2537,
      "training_loss": 5.89176082611084
    },
    {
      "epoch": 0.5500271002710027,
      "step": 2537,
      "training_loss": 7.361481189727783
    },
    {
      "epoch": 0.5502439024390244,
      "step": 2538,
      "training_loss": 6.808901309967041
    },
    {
      "epoch": 0.5502439024390244,
      "step": 2538,
      "training_loss": 5.200265407562256
    },
    {
      "epoch": 0.5502439024390244,
      "step": 2538,
      "training_loss": 5.890785217285156
    },
    {
      "epoch": 0.5502439024390244,
      "step": 2538,
      "training_loss": 6.271361827850342
    },
    {
      "epoch": 0.5504607046070461,
      "step": 2539,
      "training_loss": 5.46000337600708
    },
    {
      "epoch": 0.5504607046070461,
      "step": 2539,
      "training_loss": 6.8169097900390625
    },
    {
      "epoch": 0.5504607046070461,
      "step": 2539,
      "training_loss": 5.519993305206299
    },
    {
      "epoch": 0.5504607046070461,
      "step": 2539,
      "training_loss": 6.294821262359619
    },
    {
      "epoch": 0.5506775067750678,
      "grad_norm": 14.732060432434082,
      "learning_rate": 1e-05,
      "loss": 6.4012,
      "step": 2540
    },
    {
      "epoch": 0.5506775067750678,
      "step": 2540,
      "training_loss": 5.15681791305542
    },
    {
      "epoch": 0.5506775067750678,
      "step": 2540,
      "training_loss": 6.889340877532959
    },
    {
      "epoch": 0.5506775067750678,
      "step": 2540,
      "training_loss": 9.50167465209961
    },
    {
      "epoch": 0.5506775067750678,
      "step": 2540,
      "training_loss": 6.322630405426025
    },
    {
      "epoch": 0.5508943089430894,
      "step": 2541,
      "training_loss": 6.83708381652832
    },
    {
      "epoch": 0.5508943089430894,
      "step": 2541,
      "training_loss": 7.797492504119873
    },
    {
      "epoch": 0.5508943089430894,
      "step": 2541,
      "training_loss": 6.51728630065918
    },
    {
      "epoch": 0.5508943089430894,
      "step": 2541,
      "training_loss": 8.016220092773438
    },
    {
      "epoch": 0.5511111111111111,
      "step": 2542,
      "training_loss": 3.1419262886047363
    },
    {
      "epoch": 0.5511111111111111,
      "step": 2542,
      "training_loss": 6.934761047363281
    },
    {
      "epoch": 0.5511111111111111,
      "step": 2542,
      "training_loss": 4.967769145965576
    },
    {
      "epoch": 0.5511111111111111,
      "step": 2542,
      "training_loss": 6.7796454429626465
    },
    {
      "epoch": 0.5513279132791328,
      "step": 2543,
      "training_loss": 7.090670585632324
    },
    {
      "epoch": 0.5513279132791328,
      "step": 2543,
      "training_loss": 7.70293664932251
    },
    {
      "epoch": 0.5513279132791328,
      "step": 2543,
      "training_loss": 6.9362874031066895
    },
    {
      "epoch": 0.5513279132791328,
      "step": 2543,
      "training_loss": 7.600725173950195
    },
    {
      "epoch": 0.5515447154471544,
      "grad_norm": 16.40243911743164,
      "learning_rate": 1e-05,
      "loss": 6.7621,
      "step": 2544
    },
    {
      "epoch": 0.5515447154471544,
      "step": 2544,
      "training_loss": 6.01045036315918
    },
    {
      "epoch": 0.5515447154471544,
      "step": 2544,
      "training_loss": 6.970233917236328
    },
    {
      "epoch": 0.5515447154471544,
      "step": 2544,
      "training_loss": 5.160686492919922
    },
    {
      "epoch": 0.5515447154471544,
      "step": 2544,
      "training_loss": 5.81648588180542
    },
    {
      "epoch": 0.5517615176151761,
      "step": 2545,
      "training_loss": 6.5986104011535645
    },
    {
      "epoch": 0.5517615176151761,
      "step": 2545,
      "training_loss": 6.867216110229492
    },
    {
      "epoch": 0.5517615176151761,
      "step": 2545,
      "training_loss": 7.327585697174072
    },
    {
      "epoch": 0.5517615176151761,
      "step": 2545,
      "training_loss": 7.3801727294921875
    },
    {
      "epoch": 0.5519783197831978,
      "step": 2546,
      "training_loss": 3.6904032230377197
    },
    {
      "epoch": 0.5519783197831978,
      "step": 2546,
      "training_loss": 6.079598426818848
    },
    {
      "epoch": 0.5519783197831978,
      "step": 2546,
      "training_loss": 5.917437553405762
    },
    {
      "epoch": 0.5519783197831978,
      "step": 2546,
      "training_loss": 6.8465986251831055
    },
    {
      "epoch": 0.5521951219512196,
      "step": 2547,
      "training_loss": 5.966251373291016
    },
    {
      "epoch": 0.5521951219512196,
      "step": 2547,
      "training_loss": 4.1603312492370605
    },
    {
      "epoch": 0.5521951219512196,
      "step": 2547,
      "training_loss": 6.733676910400391
    },
    {
      "epoch": 0.5521951219512196,
      "step": 2547,
      "training_loss": 6.539024353027344
    },
    {
      "epoch": 0.5524119241192412,
      "grad_norm": 16.061269760131836,
      "learning_rate": 1e-05,
      "loss": 6.129,
      "step": 2548
    },
    {
      "epoch": 0.5524119241192412,
      "step": 2548,
      "training_loss": 5.613349437713623
    },
    {
      "epoch": 0.5524119241192412,
      "step": 2548,
      "training_loss": 4.468242645263672
    },
    {
      "epoch": 0.5524119241192412,
      "step": 2548,
      "training_loss": 5.338440895080566
    },
    {
      "epoch": 0.5524119241192412,
      "step": 2548,
      "training_loss": 6.730532646179199
    },
    {
      "epoch": 0.5526287262872629,
      "step": 2549,
      "training_loss": 2.617008686065674
    },
    {
      "epoch": 0.5526287262872629,
      "step": 2549,
      "training_loss": 6.032001972198486
    },
    {
      "epoch": 0.5526287262872629,
      "step": 2549,
      "training_loss": 6.8914475440979
    },
    {
      "epoch": 0.5526287262872629,
      "step": 2549,
      "training_loss": 7.085444927215576
    },
    {
      "epoch": 0.5528455284552846,
      "step": 2550,
      "training_loss": 6.594655990600586
    },
    {
      "epoch": 0.5528455284552846,
      "step": 2550,
      "training_loss": 7.423366546630859
    },
    {
      "epoch": 0.5528455284552846,
      "step": 2550,
      "training_loss": 7.926161289215088
    },
    {
      "epoch": 0.5528455284552846,
      "step": 2550,
      "training_loss": 8.160561561584473
    },
    {
      "epoch": 0.5530623306233062,
      "step": 2551,
      "training_loss": 5.994962692260742
    },
    {
      "epoch": 0.5530623306233062,
      "step": 2551,
      "training_loss": 7.588842391967773
    },
    {
      "epoch": 0.5530623306233062,
      "step": 2551,
      "training_loss": 7.100142955780029
    },
    {
      "epoch": 0.5530623306233062,
      "step": 2551,
      "training_loss": 6.967940807342529
    },
    {
      "epoch": 0.5532791327913279,
      "grad_norm": 15.695758819580078,
      "learning_rate": 1e-05,
      "loss": 6.4083,
      "step": 2552
    },
    {
      "epoch": 0.5532791327913279,
      "step": 2552,
      "training_loss": 6.2090864181518555
    },
    {
      "epoch": 0.5532791327913279,
      "step": 2552,
      "training_loss": 8.4729585647583
    },
    {
      "epoch": 0.5532791327913279,
      "step": 2552,
      "training_loss": 7.161160945892334
    },
    {
      "epoch": 0.5532791327913279,
      "step": 2552,
      "training_loss": 6.9376935958862305
    },
    {
      "epoch": 0.5534959349593496,
      "step": 2553,
      "training_loss": 4.986240863800049
    },
    {
      "epoch": 0.5534959349593496,
      "step": 2553,
      "training_loss": 7.018120288848877
    },
    {
      "epoch": 0.5534959349593496,
      "step": 2553,
      "training_loss": 7.018892288208008
    },
    {
      "epoch": 0.5534959349593496,
      "step": 2553,
      "training_loss": 7.4701995849609375
    },
    {
      "epoch": 0.5537127371273712,
      "step": 2554,
      "training_loss": 7.14199161529541
    },
    {
      "epoch": 0.5537127371273712,
      "step": 2554,
      "training_loss": 7.433594226837158
    },
    {
      "epoch": 0.5537127371273712,
      "step": 2554,
      "training_loss": 7.210184097290039
    },
    {
      "epoch": 0.5537127371273712,
      "step": 2554,
      "training_loss": 8.948748588562012
    },
    {
      "epoch": 0.5539295392953929,
      "step": 2555,
      "training_loss": 7.051878929138184
    },
    {
      "epoch": 0.5539295392953929,
      "step": 2555,
      "training_loss": 5.22941255569458
    },
    {
      "epoch": 0.5539295392953929,
      "step": 2555,
      "training_loss": 7.499120235443115
    },
    {
      "epoch": 0.5539295392953929,
      "step": 2555,
      "training_loss": 3.5086817741394043
    },
    {
      "epoch": 0.5541463414634147,
      "grad_norm": 15.217447280883789,
      "learning_rate": 1e-05,
      "loss": 6.8311,
      "step": 2556
    },
    {
      "epoch": 0.5541463414634147,
      "step": 2556,
      "training_loss": 6.199566841125488
    },
    {
      "epoch": 0.5541463414634147,
      "step": 2556,
      "training_loss": 5.835661888122559
    },
    {
      "epoch": 0.5541463414634147,
      "step": 2556,
      "training_loss": 7.439484119415283
    },
    {
      "epoch": 0.5541463414634147,
      "step": 2556,
      "training_loss": 3.5267698764801025
    },
    {
      "epoch": 0.5543631436314364,
      "step": 2557,
      "training_loss": 5.86145544052124
    },
    {
      "epoch": 0.5543631436314364,
      "step": 2557,
      "training_loss": 5.839535236358643
    },
    {
      "epoch": 0.5543631436314364,
      "step": 2557,
      "training_loss": 6.350608825683594
    },
    {
      "epoch": 0.5543631436314364,
      "step": 2557,
      "training_loss": 6.683664798736572
    },
    {
      "epoch": 0.554579945799458,
      "step": 2558,
      "training_loss": 6.608724117279053
    },
    {
      "epoch": 0.554579945799458,
      "step": 2558,
      "training_loss": 7.2890706062316895
    },
    {
      "epoch": 0.554579945799458,
      "step": 2558,
      "training_loss": 5.785562992095947
    },
    {
      "epoch": 0.554579945799458,
      "step": 2558,
      "training_loss": 8.2659273147583
    },
    {
      "epoch": 0.5547967479674797,
      "step": 2559,
      "training_loss": 6.500280857086182
    },
    {
      "epoch": 0.5547967479674797,
      "step": 2559,
      "training_loss": 6.253571033477783
    },
    {
      "epoch": 0.5547967479674797,
      "step": 2559,
      "training_loss": 6.724275588989258
    },
    {
      "epoch": 0.5547967479674797,
      "step": 2559,
      "training_loss": 7.513230323791504
    },
    {
      "epoch": 0.5550135501355014,
      "grad_norm": 25.392154693603516,
      "learning_rate": 1e-05,
      "loss": 6.4173,
      "step": 2560
    },
    {
      "epoch": 0.5550135501355014,
      "step": 2560,
      "training_loss": 5.785467624664307
    },
    {
      "epoch": 0.5550135501355014,
      "step": 2560,
      "training_loss": 6.725908279418945
    },
    {
      "epoch": 0.5550135501355014,
      "step": 2560,
      "training_loss": 6.616162300109863
    },
    {
      "epoch": 0.5550135501355014,
      "step": 2560,
      "training_loss": 6.855156898498535
    },
    {
      "epoch": 0.555230352303523,
      "step": 2561,
      "training_loss": 4.374938011169434
    },
    {
      "epoch": 0.555230352303523,
      "step": 2561,
      "training_loss": 6.227578163146973
    },
    {
      "epoch": 0.555230352303523,
      "step": 2561,
      "training_loss": 5.984839916229248
    },
    {
      "epoch": 0.555230352303523,
      "step": 2561,
      "training_loss": 6.762260437011719
    },
    {
      "epoch": 0.5554471544715447,
      "step": 2562,
      "training_loss": 6.397688388824463
    },
    {
      "epoch": 0.5554471544715447,
      "step": 2562,
      "training_loss": 7.7978620529174805
    },
    {
      "epoch": 0.5554471544715447,
      "step": 2562,
      "training_loss": 6.998424530029297
    },
    {
      "epoch": 0.5554471544715447,
      "step": 2562,
      "training_loss": 6.918903827667236
    },
    {
      "epoch": 0.5556639566395664,
      "step": 2563,
      "training_loss": 6.061904430389404
    },
    {
      "epoch": 0.5556639566395664,
      "step": 2563,
      "training_loss": 4.784556865692139
    },
    {
      "epoch": 0.5556639566395664,
      "step": 2563,
      "training_loss": 7.942354202270508
    },
    {
      "epoch": 0.5556639566395664,
      "step": 2563,
      "training_loss": 6.835310935974121
    },
    {
      "epoch": 0.555880758807588,
      "grad_norm": 17.982677459716797,
      "learning_rate": 1e-05,
      "loss": 6.4418,
      "step": 2564
    },
    {
      "epoch": 0.555880758807588,
      "step": 2564,
      "training_loss": 5.684828758239746
    },
    {
      "epoch": 0.555880758807588,
      "step": 2564,
      "training_loss": 8.88469123840332
    },
    {
      "epoch": 0.555880758807588,
      "step": 2564,
      "training_loss": 8.321150779724121
    },
    {
      "epoch": 0.555880758807588,
      "step": 2564,
      "training_loss": 6.735837459564209
    },
    {
      "epoch": 0.5560975609756098,
      "step": 2565,
      "training_loss": 7.4168500900268555
    },
    {
      "epoch": 0.5560975609756098,
      "step": 2565,
      "training_loss": 5.671976089477539
    },
    {
      "epoch": 0.5560975609756098,
      "step": 2565,
      "training_loss": 7.172384262084961
    },
    {
      "epoch": 0.5560975609756098,
      "step": 2565,
      "training_loss": 6.43936824798584
    },
    {
      "epoch": 0.5563143631436315,
      "step": 2566,
      "training_loss": 6.1595139503479
    },
    {
      "epoch": 0.5563143631436315,
      "step": 2566,
      "training_loss": 5.678288459777832
    },
    {
      "epoch": 0.5563143631436315,
      "step": 2566,
      "training_loss": 3.8668084144592285
    },
    {
      "epoch": 0.5563143631436315,
      "step": 2566,
      "training_loss": 6.728504180908203
    },
    {
      "epoch": 0.5565311653116531,
      "step": 2567,
      "training_loss": 7.418990612030029
    },
    {
      "epoch": 0.5565311653116531,
      "step": 2567,
      "training_loss": 6.551823616027832
    },
    {
      "epoch": 0.5565311653116531,
      "step": 2567,
      "training_loss": 6.201547622680664
    },
    {
      "epoch": 0.5565311653116531,
      "step": 2567,
      "training_loss": 6.031917095184326
    },
    {
      "epoch": 0.5567479674796748,
      "grad_norm": 14.911092758178711,
      "learning_rate": 1e-05,
      "loss": 6.5603,
      "step": 2568
    },
    {
      "epoch": 0.5567479674796748,
      "step": 2568,
      "training_loss": 3.1166152954101562
    },
    {
      "epoch": 0.5567479674796748,
      "step": 2568,
      "training_loss": 6.37530517578125
    },
    {
      "epoch": 0.5567479674796748,
      "step": 2568,
      "training_loss": 6.883086204528809
    },
    {
      "epoch": 0.5567479674796748,
      "step": 2568,
      "training_loss": 7.387569427490234
    },
    {
      "epoch": 0.5569647696476965,
      "step": 2569,
      "training_loss": 3.680964708328247
    },
    {
      "epoch": 0.5569647696476965,
      "step": 2569,
      "training_loss": 7.049574851989746
    },
    {
      "epoch": 0.5569647696476965,
      "step": 2569,
      "training_loss": 7.268857002258301
    },
    {
      "epoch": 0.5569647696476965,
      "step": 2569,
      "training_loss": 5.909608840942383
    },
    {
      "epoch": 0.5571815718157181,
      "step": 2570,
      "training_loss": 7.447896957397461
    },
    {
      "epoch": 0.5571815718157181,
      "step": 2570,
      "training_loss": 6.668586730957031
    },
    {
      "epoch": 0.5571815718157181,
      "step": 2570,
      "training_loss": 6.3912200927734375
    },
    {
      "epoch": 0.5571815718157181,
      "step": 2570,
      "training_loss": 2.850865364074707
    },
    {
      "epoch": 0.5573983739837398,
      "step": 2571,
      "training_loss": 6.33847188949585
    },
    {
      "epoch": 0.5573983739837398,
      "step": 2571,
      "training_loss": 6.968652248382568
    },
    {
      "epoch": 0.5573983739837398,
      "step": 2571,
      "training_loss": 7.5693511962890625
    },
    {
      "epoch": 0.5573983739837398,
      "step": 2571,
      "training_loss": 6.597320556640625
    },
    {
      "epoch": 0.5576151761517615,
      "grad_norm": 17.71022605895996,
      "learning_rate": 1e-05,
      "loss": 6.1565,
      "step": 2572
    },
    {
      "epoch": 0.5576151761517615,
      "step": 2572,
      "training_loss": 7.34094762802124
    },
    {
      "epoch": 0.5576151761517615,
      "step": 2572,
      "training_loss": 6.436581134796143
    },
    {
      "epoch": 0.5576151761517615,
      "step": 2572,
      "training_loss": 6.647030353546143
    },
    {
      "epoch": 0.5576151761517615,
      "step": 2572,
      "training_loss": 6.8591814041137695
    },
    {
      "epoch": 0.5578319783197832,
      "step": 2573,
      "training_loss": 3.420228958129883
    },
    {
      "epoch": 0.5578319783197832,
      "step": 2573,
      "training_loss": 7.018100261688232
    },
    {
      "epoch": 0.5578319783197832,
      "step": 2573,
      "training_loss": 6.106253147125244
    },
    {
      "epoch": 0.5578319783197832,
      "step": 2573,
      "training_loss": 7.6293840408325195
    },
    {
      "epoch": 0.5580487804878049,
      "step": 2574,
      "training_loss": 7.046761989593506
    },
    {
      "epoch": 0.5580487804878049,
      "step": 2574,
      "training_loss": 6.991233825683594
    },
    {
      "epoch": 0.5580487804878049,
      "step": 2574,
      "training_loss": 7.279351234436035
    },
    {
      "epoch": 0.5580487804878049,
      "step": 2574,
      "training_loss": 6.7289557456970215
    },
    {
      "epoch": 0.5582655826558266,
      "step": 2575,
      "training_loss": 6.780384540557861
    },
    {
      "epoch": 0.5582655826558266,
      "step": 2575,
      "training_loss": 6.828345775604248
    },
    {
      "epoch": 0.5582655826558266,
      "step": 2575,
      "training_loss": 3.2371022701263428
    },
    {
      "epoch": 0.5582655826558266,
      "step": 2575,
      "training_loss": 7.308523654937744
    },
    {
      "epoch": 0.5584823848238483,
      "grad_norm": 14.574889183044434,
      "learning_rate": 1e-05,
      "loss": 6.4786,
      "step": 2576
    },
    {
      "epoch": 0.5584823848238483,
      "step": 2576,
      "training_loss": 6.035153865814209
    },
    {
      "epoch": 0.5584823848238483,
      "step": 2576,
      "training_loss": 5.013163089752197
    },
    {
      "epoch": 0.5584823848238483,
      "step": 2576,
      "training_loss": 5.762763023376465
    },
    {
      "epoch": 0.5584823848238483,
      "step": 2576,
      "training_loss": 5.4939284324646
    },
    {
      "epoch": 0.5586991869918699,
      "step": 2577,
      "training_loss": 5.564453125
    },
    {
      "epoch": 0.5586991869918699,
      "step": 2577,
      "training_loss": 5.6298699378967285
    },
    {
      "epoch": 0.5586991869918699,
      "step": 2577,
      "training_loss": 3.014322280883789
    },
    {
      "epoch": 0.5586991869918699,
      "step": 2577,
      "training_loss": 4.34470796585083
    },
    {
      "epoch": 0.5589159891598916,
      "step": 2578,
      "training_loss": 6.744747161865234
    },
    {
      "epoch": 0.5589159891598916,
      "step": 2578,
      "training_loss": 6.1029953956604
    },
    {
      "epoch": 0.5589159891598916,
      "step": 2578,
      "training_loss": 7.982704162597656
    },
    {
      "epoch": 0.5589159891598916,
      "step": 2578,
      "training_loss": 5.7925896644592285
    },
    {
      "epoch": 0.5591327913279133,
      "step": 2579,
      "training_loss": 4.192674160003662
    },
    {
      "epoch": 0.5591327913279133,
      "step": 2579,
      "training_loss": 4.14279842376709
    },
    {
      "epoch": 0.5591327913279133,
      "step": 2579,
      "training_loss": 7.04097843170166
    },
    {
      "epoch": 0.5591327913279133,
      "step": 2579,
      "training_loss": 6.696979999542236
    },
    {
      "epoch": 0.5593495934959349,
      "grad_norm": 21.840503692626953,
      "learning_rate": 1e-05,
      "loss": 5.5972,
      "step": 2580
    },
    {
      "epoch": 0.5593495934959349,
      "step": 2580,
      "training_loss": 5.558094501495361
    },
    {
      "epoch": 0.5593495934959349,
      "step": 2580,
      "training_loss": 5.998606204986572
    },
    {
      "epoch": 0.5593495934959349,
      "step": 2580,
      "training_loss": 8.126319885253906
    },
    {
      "epoch": 0.5593495934959349,
      "step": 2580,
      "training_loss": 6.903086185455322
    },
    {
      "epoch": 0.5595663956639566,
      "step": 2581,
      "training_loss": 6.527928829193115
    },
    {
      "epoch": 0.5595663956639566,
      "step": 2581,
      "training_loss": 7.0729780197143555
    },
    {
      "epoch": 0.5595663956639566,
      "step": 2581,
      "training_loss": 6.555258274078369
    },
    {
      "epoch": 0.5595663956639566,
      "step": 2581,
      "training_loss": 8.358262062072754
    },
    {
      "epoch": 0.5597831978319783,
      "step": 2582,
      "training_loss": 7.07504940032959
    },
    {
      "epoch": 0.5597831978319783,
      "step": 2582,
      "training_loss": 5.972132682800293
    },
    {
      "epoch": 0.5597831978319783,
      "step": 2582,
      "training_loss": 4.420587062835693
    },
    {
      "epoch": 0.5597831978319783,
      "step": 2582,
      "training_loss": 6.401770114898682
    },
    {
      "epoch": 0.56,
      "step": 2583,
      "training_loss": 6.839682579040527
    },
    {
      "epoch": 0.56,
      "step": 2583,
      "training_loss": 6.094386100769043
    },
    {
      "epoch": 0.56,
      "step": 2583,
      "training_loss": 7.599260330200195
    },
    {
      "epoch": 0.56,
      "step": 2583,
      "training_loss": 6.691769123077393
    },
    {
      "epoch": 0.5602168021680217,
      "grad_norm": 14.925887107849121,
      "learning_rate": 1e-05,
      "loss": 6.6372,
      "step": 2584
    },
    {
      "epoch": 0.5602168021680217,
      "step": 2584,
      "training_loss": 6.5830769538879395
    },
    {
      "epoch": 0.5602168021680217,
      "step": 2584,
      "training_loss": 7.086052894592285
    },
    {
      "epoch": 0.5602168021680217,
      "step": 2584,
      "training_loss": 7.146007061004639
    },
    {
      "epoch": 0.5602168021680217,
      "step": 2584,
      "training_loss": 5.046255588531494
    },
    {
      "epoch": 0.5604336043360434,
      "step": 2585,
      "training_loss": 5.487553596496582
    },
    {
      "epoch": 0.5604336043360434,
      "step": 2585,
      "training_loss": 7.0410051345825195
    },
    {
      "epoch": 0.5604336043360434,
      "step": 2585,
      "training_loss": 4.575414657592773
    },
    {
      "epoch": 0.5604336043360434,
      "step": 2585,
      "training_loss": 6.722179412841797
    },
    {
      "epoch": 0.5606504065040651,
      "step": 2586,
      "training_loss": 5.947164058685303
    },
    {
      "epoch": 0.5606504065040651,
      "step": 2586,
      "training_loss": 6.7980265617370605
    },
    {
      "epoch": 0.5606504065040651,
      "step": 2586,
      "training_loss": 7.23063850402832
    },
    {
      "epoch": 0.5606504065040651,
      "step": 2586,
      "training_loss": 5.768487453460693
    },
    {
      "epoch": 0.5608672086720867,
      "step": 2587,
      "training_loss": 7.485344886779785
    },
    {
      "epoch": 0.5608672086720867,
      "step": 2587,
      "training_loss": 6.9646453857421875
    },
    {
      "epoch": 0.5608672086720867,
      "step": 2587,
      "training_loss": 6.176029682159424
    },
    {
      "epoch": 0.5608672086720867,
      "step": 2587,
      "training_loss": 6.345858097076416
    },
    {
      "epoch": 0.5610840108401084,
      "grad_norm": 18.98261070251465,
      "learning_rate": 1e-05,
      "loss": 6.4002,
      "step": 2588
    },
    {
      "epoch": 0.5610840108401084,
      "step": 2588,
      "training_loss": 6.257181644439697
    },
    {
      "epoch": 0.5610840108401084,
      "step": 2588,
      "training_loss": 7.85270357131958
    },
    {
      "epoch": 0.5610840108401084,
      "step": 2588,
      "training_loss": 6.474363803863525
    },
    {
      "epoch": 0.5610840108401084,
      "step": 2588,
      "training_loss": 5.616198539733887
    },
    {
      "epoch": 0.5613008130081301,
      "step": 2589,
      "training_loss": 7.652891159057617
    },
    {
      "epoch": 0.5613008130081301,
      "step": 2589,
      "training_loss": 6.602069854736328
    },
    {
      "epoch": 0.5613008130081301,
      "step": 2589,
      "training_loss": 2.9573874473571777
    },
    {
      "epoch": 0.5613008130081301,
      "step": 2589,
      "training_loss": 4.293407440185547
    },
    {
      "epoch": 0.5615176151761517,
      "step": 2590,
      "training_loss": 6.729897499084473
    },
    {
      "epoch": 0.5615176151761517,
      "step": 2590,
      "training_loss": 7.220694065093994
    },
    {
      "epoch": 0.5615176151761517,
      "step": 2590,
      "training_loss": 5.762573719024658
    },
    {
      "epoch": 0.5615176151761517,
      "step": 2590,
      "training_loss": 8.403282165527344
    },
    {
      "epoch": 0.5617344173441734,
      "step": 2591,
      "training_loss": 5.003929138183594
    },
    {
      "epoch": 0.5617344173441734,
      "step": 2591,
      "training_loss": 6.957021236419678
    },
    {
      "epoch": 0.5617344173441734,
      "step": 2591,
      "training_loss": 5.571249485015869
    },
    {
      "epoch": 0.5617344173441734,
      "step": 2591,
      "training_loss": 5.775808811187744
    },
    {
      "epoch": 0.5619512195121952,
      "grad_norm": 19.48194694519043,
      "learning_rate": 1e-05,
      "loss": 6.1957,
      "step": 2592
    },
    {
      "epoch": 0.5619512195121952,
      "step": 2592,
      "training_loss": 5.264016628265381
    },
    {
      "epoch": 0.5619512195121952,
      "step": 2592,
      "training_loss": 5.577434539794922
    },
    {
      "epoch": 0.5619512195121952,
      "step": 2592,
      "training_loss": 6.113466262817383
    },
    {
      "epoch": 0.5619512195121952,
      "step": 2592,
      "training_loss": 7.644315719604492
    },
    {
      "epoch": 0.5621680216802168,
      "step": 2593,
      "training_loss": 7.500106334686279
    },
    {
      "epoch": 0.5621680216802168,
      "step": 2593,
      "training_loss": 7.035146713256836
    },
    {
      "epoch": 0.5621680216802168,
      "step": 2593,
      "training_loss": 5.414887428283691
    },
    {
      "epoch": 0.5621680216802168,
      "step": 2593,
      "training_loss": 6.711733818054199
    },
    {
      "epoch": 0.5623848238482385,
      "step": 2594,
      "training_loss": 7.822516918182373
    },
    {
      "epoch": 0.5623848238482385,
      "step": 2594,
      "training_loss": 6.608190536499023
    },
    {
      "epoch": 0.5623848238482385,
      "step": 2594,
      "training_loss": 7.467470645904541
    },
    {
      "epoch": 0.5623848238482385,
      "step": 2594,
      "training_loss": 7.538632869720459
    },
    {
      "epoch": 0.5626016260162602,
      "step": 2595,
      "training_loss": 6.655369758605957
    },
    {
      "epoch": 0.5626016260162602,
      "step": 2595,
      "training_loss": 6.733868598937988
    },
    {
      "epoch": 0.5626016260162602,
      "step": 2595,
      "training_loss": 7.054904937744141
    },
    {
      "epoch": 0.5626016260162602,
      "step": 2595,
      "training_loss": 6.241740703582764
    },
    {
      "epoch": 0.5628184281842818,
      "grad_norm": 14.082287788391113,
      "learning_rate": 1e-05,
      "loss": 6.7115,
      "step": 2596
    },
    {
      "epoch": 0.5628184281842818,
      "step": 2596,
      "training_loss": 6.730666637420654
    },
    {
      "epoch": 0.5628184281842818,
      "step": 2596,
      "training_loss": 3.7790377140045166
    },
    {
      "epoch": 0.5628184281842818,
      "step": 2596,
      "training_loss": 5.850079536437988
    },
    {
      "epoch": 0.5628184281842818,
      "step": 2596,
      "training_loss": 8.600493431091309
    },
    {
      "epoch": 0.5630352303523035,
      "step": 2597,
      "training_loss": 7.236923694610596
    },
    {
      "epoch": 0.5630352303523035,
      "step": 2597,
      "training_loss": 5.596160888671875
    },
    {
      "epoch": 0.5630352303523035,
      "step": 2597,
      "training_loss": 7.341140270233154
    },
    {
      "epoch": 0.5630352303523035,
      "step": 2597,
      "training_loss": 5.89387321472168
    },
    {
      "epoch": 0.5632520325203252,
      "step": 2598,
      "training_loss": 5.945102691650391
    },
    {
      "epoch": 0.5632520325203252,
      "step": 2598,
      "training_loss": 7.193885803222656
    },
    {
      "epoch": 0.5632520325203252,
      "step": 2598,
      "training_loss": 7.7497453689575195
    },
    {
      "epoch": 0.5632520325203252,
      "step": 2598,
      "training_loss": 7.128785133361816
    },
    {
      "epoch": 0.5634688346883469,
      "step": 2599,
      "training_loss": 7.91383171081543
    },
    {
      "epoch": 0.5634688346883469,
      "step": 2599,
      "training_loss": 7.294795036315918
    },
    {
      "epoch": 0.5634688346883469,
      "step": 2599,
      "training_loss": 6.886012554168701
    },
    {
      "epoch": 0.5634688346883469,
      "step": 2599,
      "training_loss": 6.597518444061279
    },
    {
      "epoch": 0.5636856368563685,
      "grad_norm": 15.703070640563965,
      "learning_rate": 1e-05,
      "loss": 6.7336,
      "step": 2600
    },
    {
      "epoch": 0.5636856368563685,
      "step": 2600,
      "training_loss": 6.919743537902832
    },
    {
      "epoch": 0.5636856368563685,
      "step": 2600,
      "training_loss": 7.938614845275879
    },
    {
      "epoch": 0.5636856368563685,
      "step": 2600,
      "training_loss": 5.498165607452393
    },
    {
      "epoch": 0.5636856368563685,
      "step": 2600,
      "training_loss": 4.944146156311035
    },
    {
      "epoch": 0.5639024390243902,
      "step": 2601,
      "training_loss": 5.44325590133667
    },
    {
      "epoch": 0.5639024390243902,
      "step": 2601,
      "training_loss": 6.2551984786987305
    },
    {
      "epoch": 0.5639024390243902,
      "step": 2601,
      "training_loss": 5.303683757781982
    },
    {
      "epoch": 0.5639024390243902,
      "step": 2601,
      "training_loss": 7.064992427825928
    },
    {
      "epoch": 0.564119241192412,
      "step": 2602,
      "training_loss": 7.649977207183838
    },
    {
      "epoch": 0.564119241192412,
      "step": 2602,
      "training_loss": 6.572169780731201
    },
    {
      "epoch": 0.564119241192412,
      "step": 2602,
      "training_loss": 6.0404510498046875
    },
    {
      "epoch": 0.564119241192412,
      "step": 2602,
      "training_loss": 4.0389838218688965
    },
    {
      "epoch": 0.5643360433604336,
      "step": 2603,
      "training_loss": 7.392821311950684
    },
    {
      "epoch": 0.5643360433604336,
      "step": 2603,
      "training_loss": 6.391176700592041
    },
    {
      "epoch": 0.5643360433604336,
      "step": 2603,
      "training_loss": 6.153670787811279
    },
    {
      "epoch": 0.5643360433604336,
      "step": 2603,
      "training_loss": 7.178976058959961
    },
    {
      "epoch": 0.5645528455284553,
      "grad_norm": 18.094234466552734,
      "learning_rate": 1e-05,
      "loss": 6.2991,
      "step": 2604
    },
    {
      "epoch": 0.5645528455284553,
      "step": 2604,
      "training_loss": 6.28533935546875
    },
    {
      "epoch": 0.5645528455284553,
      "step": 2604,
      "training_loss": 7.647822856903076
    },
    {
      "epoch": 0.5645528455284553,
      "step": 2604,
      "training_loss": 6.924354553222656
    },
    {
      "epoch": 0.5645528455284553,
      "step": 2604,
      "training_loss": 6.011704444885254
    },
    {
      "epoch": 0.564769647696477,
      "step": 2605,
      "training_loss": 8.007116317749023
    },
    {
      "epoch": 0.564769647696477,
      "step": 2605,
      "training_loss": 5.831610679626465
    },
    {
      "epoch": 0.564769647696477,
      "step": 2605,
      "training_loss": 6.19478702545166
    },
    {
      "epoch": 0.564769647696477,
      "step": 2605,
      "training_loss": 6.456510543823242
    },
    {
      "epoch": 0.5649864498644986,
      "step": 2606,
      "training_loss": 6.339197635650635
    },
    {
      "epoch": 0.5649864498644986,
      "step": 2606,
      "training_loss": 7.715978145599365
    },
    {
      "epoch": 0.5649864498644986,
      "step": 2606,
      "training_loss": 5.8798675537109375
    },
    {
      "epoch": 0.5649864498644986,
      "step": 2606,
      "training_loss": 5.004926681518555
    },
    {
      "epoch": 0.5652032520325203,
      "step": 2607,
      "training_loss": 6.742251396179199
    },
    {
      "epoch": 0.5652032520325203,
      "step": 2607,
      "training_loss": 5.938776016235352
    },
    {
      "epoch": 0.5652032520325203,
      "step": 2607,
      "training_loss": 6.362614154815674
    },
    {
      "epoch": 0.5652032520325203,
      "step": 2607,
      "training_loss": 7.040128231048584
    },
    {
      "epoch": 0.565420054200542,
      "grad_norm": 14.95046329498291,
      "learning_rate": 1e-05,
      "loss": 6.5239,
      "step": 2608
    },
    {
      "epoch": 0.565420054200542,
      "step": 2608,
      "training_loss": 5.852326393127441
    },
    {
      "epoch": 0.565420054200542,
      "step": 2608,
      "training_loss": 7.278061389923096
    },
    {
      "epoch": 0.565420054200542,
      "step": 2608,
      "training_loss": 5.470865249633789
    },
    {
      "epoch": 0.565420054200542,
      "step": 2608,
      "training_loss": 6.864805221557617
    },
    {
      "epoch": 0.5656368563685636,
      "step": 2609,
      "training_loss": 5.698114395141602
    },
    {
      "epoch": 0.5656368563685636,
      "step": 2609,
      "training_loss": 7.786603927612305
    },
    {
      "epoch": 0.5656368563685636,
      "step": 2609,
      "training_loss": 7.65687370300293
    },
    {
      "epoch": 0.5656368563685636,
      "step": 2609,
      "training_loss": 4.568220615386963
    },
    {
      "epoch": 0.5658536585365853,
      "step": 2610,
      "training_loss": 7.203141689300537
    },
    {
      "epoch": 0.5658536585365853,
      "step": 2610,
      "training_loss": 6.993677616119385
    },
    {
      "epoch": 0.5658536585365853,
      "step": 2610,
      "training_loss": 6.447088718414307
    },
    {
      "epoch": 0.5658536585365853,
      "step": 2610,
      "training_loss": 7.041943550109863
    },
    {
      "epoch": 0.5660704607046071,
      "step": 2611,
      "training_loss": 6.846140384674072
    },
    {
      "epoch": 0.5660704607046071,
      "step": 2611,
      "training_loss": 7.551522254943848
    },
    {
      "epoch": 0.5660704607046071,
      "step": 2611,
      "training_loss": 3.777355909347534
    },
    {
      "epoch": 0.5660704607046071,
      "step": 2611,
      "training_loss": 6.334948539733887
    },
    {
      "epoch": 0.5662872628726288,
      "grad_norm": 14.045537948608398,
      "learning_rate": 1e-05,
      "loss": 6.4607,
      "step": 2612
    },
    {
      "epoch": 0.5662872628726288,
      "step": 2612,
      "training_loss": 6.649482727050781
    },
    {
      "epoch": 0.5662872628726288,
      "step": 2612,
      "training_loss": 6.887265682220459
    },
    {
      "epoch": 0.5662872628726288,
      "step": 2612,
      "training_loss": 7.1687822341918945
    },
    {
      "epoch": 0.5662872628726288,
      "step": 2612,
      "training_loss": 7.248359203338623
    },
    {
      "epoch": 0.5665040650406504,
      "step": 2613,
      "training_loss": 5.892861366271973
    },
    {
      "epoch": 0.5665040650406504,
      "step": 2613,
      "training_loss": 6.487371921539307
    },
    {
      "epoch": 0.5665040650406504,
      "step": 2613,
      "training_loss": 6.442334175109863
    },
    {
      "epoch": 0.5665040650406504,
      "step": 2613,
      "training_loss": 5.329174518585205
    },
    {
      "epoch": 0.5667208672086721,
      "step": 2614,
      "training_loss": 4.0857157707214355
    },
    {
      "epoch": 0.5667208672086721,
      "step": 2614,
      "training_loss": 6.308612823486328
    },
    {
      "epoch": 0.5667208672086721,
      "step": 2614,
      "training_loss": 6.506727695465088
    },
    {
      "epoch": 0.5667208672086721,
      "step": 2614,
      "training_loss": 4.517606735229492
    },
    {
      "epoch": 0.5669376693766938,
      "step": 2615,
      "training_loss": 6.229551792144775
    },
    {
      "epoch": 0.5669376693766938,
      "step": 2615,
      "training_loss": 6.314691543579102
    },
    {
      "epoch": 0.5669376693766938,
      "step": 2615,
      "training_loss": 6.314326286315918
    },
    {
      "epoch": 0.5669376693766938,
      "step": 2615,
      "training_loss": 3.9994258880615234
    },
    {
      "epoch": 0.5671544715447154,
      "grad_norm": 21.222318649291992,
      "learning_rate": 1e-05,
      "loss": 6.0239,
      "step": 2616
    },
    {
      "epoch": 0.5671544715447154,
      "step": 2616,
      "training_loss": 7.269783020019531
    },
    {
      "epoch": 0.5671544715447154,
      "step": 2616,
      "training_loss": 6.535256385803223
    },
    {
      "epoch": 0.5671544715447154,
      "step": 2616,
      "training_loss": 6.209634304046631
    },
    {
      "epoch": 0.5671544715447154,
      "step": 2616,
      "training_loss": 4.779271125793457
    },
    {
      "epoch": 0.5673712737127371,
      "step": 2617,
      "training_loss": 7.148577690124512
    },
    {
      "epoch": 0.5673712737127371,
      "step": 2617,
      "training_loss": 6.036046981811523
    },
    {
      "epoch": 0.5673712737127371,
      "step": 2617,
      "training_loss": 7.840699672698975
    },
    {
      "epoch": 0.5673712737127371,
      "step": 2617,
      "training_loss": 3.9139490127563477
    },
    {
      "epoch": 0.5675880758807588,
      "step": 2618,
      "training_loss": 7.359324932098389
    },
    {
      "epoch": 0.5675880758807588,
      "step": 2618,
      "training_loss": 7.262717247009277
    },
    {
      "epoch": 0.5675880758807588,
      "step": 2618,
      "training_loss": 6.212900638580322
    },
    {
      "epoch": 0.5675880758807588,
      "step": 2618,
      "training_loss": 6.998017311096191
    },
    {
      "epoch": 0.5678048780487804,
      "step": 2619,
      "training_loss": 6.398411273956299
    },
    {
      "epoch": 0.5678048780487804,
      "step": 2619,
      "training_loss": 7.782374858856201
    },
    {
      "epoch": 0.5678048780487804,
      "step": 2619,
      "training_loss": 6.593265533447266
    },
    {
      "epoch": 0.5678048780487804,
      "step": 2619,
      "training_loss": 6.7364583015441895
    },
    {
      "epoch": 0.5680216802168022,
      "grad_norm": 18.65138053894043,
      "learning_rate": 1e-05,
      "loss": 6.5673,
      "step": 2620
    },
    {
      "epoch": 0.5680216802168022,
      "step": 2620,
      "training_loss": 6.398926258087158
    },
    {
      "epoch": 0.5680216802168022,
      "step": 2620,
      "training_loss": 7.0770440101623535
    },
    {
      "epoch": 0.5680216802168022,
      "step": 2620,
      "training_loss": 5.409252166748047
    },
    {
      "epoch": 0.5680216802168022,
      "step": 2620,
      "training_loss": 5.559369087219238
    },
    {
      "epoch": 0.5682384823848239,
      "step": 2621,
      "training_loss": 7.563636779785156
    },
    {
      "epoch": 0.5682384823848239,
      "step": 2621,
      "training_loss": 4.418321132659912
    },
    {
      "epoch": 0.5682384823848239,
      "step": 2621,
      "training_loss": 6.4590888023376465
    },
    {
      "epoch": 0.5682384823848239,
      "step": 2621,
      "training_loss": 6.676955699920654
    },
    {
      "epoch": 0.5684552845528456,
      "step": 2622,
      "training_loss": 6.395349025726318
    },
    {
      "epoch": 0.5684552845528456,
      "step": 2622,
      "training_loss": 7.020936965942383
    },
    {
      "epoch": 0.5684552845528456,
      "step": 2622,
      "training_loss": 5.748892307281494
    },
    {
      "epoch": 0.5684552845528456,
      "step": 2622,
      "training_loss": 6.210855960845947
    },
    {
      "epoch": 0.5686720867208672,
      "step": 2623,
      "training_loss": 5.698708534240723
    },
    {
      "epoch": 0.5686720867208672,
      "step": 2623,
      "training_loss": 7.644737720489502
    },
    {
      "epoch": 0.5686720867208672,
      "step": 2623,
      "training_loss": 5.882069110870361
    },
    {
      "epoch": 0.5686720867208672,
      "step": 2623,
      "training_loss": 7.821272850036621
    },
    {
      "epoch": 0.5688888888888889,
      "grad_norm": 19.071996688842773,
      "learning_rate": 1e-05,
      "loss": 6.3741,
      "step": 2624
    },
    {
      "epoch": 0.5688888888888889,
      "step": 2624,
      "training_loss": 7.0065016746521
    },
    {
      "epoch": 0.5688888888888889,
      "step": 2624,
      "training_loss": 6.140547275543213
    },
    {
      "epoch": 0.5688888888888889,
      "step": 2624,
      "training_loss": 7.8825249671936035
    },
    {
      "epoch": 0.5688888888888889,
      "step": 2624,
      "training_loss": 5.50047492980957
    },
    {
      "epoch": 0.5691056910569106,
      "step": 2625,
      "training_loss": 4.207467079162598
    },
    {
      "epoch": 0.5691056910569106,
      "step": 2625,
      "training_loss": 8.08125114440918
    },
    {
      "epoch": 0.5691056910569106,
      "step": 2625,
      "training_loss": 6.5504679679870605
    },
    {
      "epoch": 0.5691056910569106,
      "step": 2625,
      "training_loss": 4.879356384277344
    },
    {
      "epoch": 0.5693224932249322,
      "step": 2626,
      "training_loss": 6.268684387207031
    },
    {
      "epoch": 0.5693224932249322,
      "step": 2626,
      "training_loss": 7.310995101928711
    },
    {
      "epoch": 0.5693224932249322,
      "step": 2626,
      "training_loss": 5.762856483459473
    },
    {
      "epoch": 0.5693224932249322,
      "step": 2626,
      "training_loss": 5.233447074890137
    },
    {
      "epoch": 0.5695392953929539,
      "step": 2627,
      "training_loss": 6.309884548187256
    },
    {
      "epoch": 0.5695392953929539,
      "step": 2627,
      "training_loss": 7.591203212738037
    },
    {
      "epoch": 0.5695392953929539,
      "step": 2627,
      "training_loss": 3.573317050933838
    },
    {
      "epoch": 0.5695392953929539,
      "step": 2627,
      "training_loss": 5.085602760314941
    },
    {
      "epoch": 0.5697560975609756,
      "grad_norm": 12.488219261169434,
      "learning_rate": 1e-05,
      "loss": 6.0865,
      "step": 2628
    },
    {
      "epoch": 0.5697560975609756,
      "step": 2628,
      "training_loss": 7.449314594268799
    },
    {
      "epoch": 0.5697560975609756,
      "step": 2628,
      "training_loss": 3.8877694606781006
    },
    {
      "epoch": 0.5697560975609756,
      "step": 2628,
      "training_loss": 5.71687126159668
    },
    {
      "epoch": 0.5697560975609756,
      "step": 2628,
      "training_loss": 6.661914825439453
    },
    {
      "epoch": 0.5699728997289973,
      "step": 2629,
      "training_loss": 6.629414081573486
    },
    {
      "epoch": 0.5699728997289973,
      "step": 2629,
      "training_loss": 4.659841537475586
    },
    {
      "epoch": 0.5699728997289973,
      "step": 2629,
      "training_loss": 5.45673131942749
    },
    {
      "epoch": 0.5699728997289973,
      "step": 2629,
      "training_loss": 6.949097633361816
    },
    {
      "epoch": 0.570189701897019,
      "step": 2630,
      "training_loss": 7.762393951416016
    },
    {
      "epoch": 0.570189701897019,
      "step": 2630,
      "training_loss": 6.511780261993408
    },
    {
      "epoch": 0.570189701897019,
      "step": 2630,
      "training_loss": 7.424244403839111
    },
    {
      "epoch": 0.570189701897019,
      "step": 2630,
      "training_loss": 6.645749568939209
    },
    {
      "epoch": 0.5704065040650407,
      "step": 2631,
      "training_loss": 6.122754096984863
    },
    {
      "epoch": 0.5704065040650407,
      "step": 2631,
      "training_loss": 5.298575401306152
    },
    {
      "epoch": 0.5704065040650407,
      "step": 2631,
      "training_loss": 6.806596755981445
    },
    {
      "epoch": 0.5704065040650407,
      "step": 2631,
      "training_loss": 6.011623859405518
    },
    {
      "epoch": 0.5706233062330623,
      "grad_norm": 20.323875427246094,
      "learning_rate": 1e-05,
      "loss": 6.2497,
      "step": 2632
    },
    {
      "epoch": 0.5706233062330623,
      "step": 2632,
      "training_loss": 7.686742782592773
    },
    {
      "epoch": 0.5706233062330623,
      "step": 2632,
      "training_loss": 6.256803035736084
    },
    {
      "epoch": 0.5706233062330623,
      "step": 2632,
      "training_loss": 7.705338954925537
    },
    {
      "epoch": 0.5706233062330623,
      "step": 2632,
      "training_loss": 6.5197272300720215
    },
    {
      "epoch": 0.570840108401084,
      "step": 2633,
      "training_loss": 6.835154056549072
    },
    {
      "epoch": 0.570840108401084,
      "step": 2633,
      "training_loss": 7.5704450607299805
    },
    {
      "epoch": 0.570840108401084,
      "step": 2633,
      "training_loss": 6.018203258514404
    },
    {
      "epoch": 0.570840108401084,
      "step": 2633,
      "training_loss": 6.190608024597168
    },
    {
      "epoch": 0.5710569105691057,
      "step": 2634,
      "training_loss": 6.923860549926758
    },
    {
      "epoch": 0.5710569105691057,
      "step": 2634,
      "training_loss": 5.69824743270874
    },
    {
      "epoch": 0.5710569105691057,
      "step": 2634,
      "training_loss": 7.139735221862793
    },
    {
      "epoch": 0.5710569105691057,
      "step": 2634,
      "training_loss": 5.617193698883057
    },
    {
      "epoch": 0.5712737127371273,
      "step": 2635,
      "training_loss": 6.835862159729004
    },
    {
      "epoch": 0.5712737127371273,
      "step": 2635,
      "training_loss": 5.914817810058594
    },
    {
      "epoch": 0.5712737127371273,
      "step": 2635,
      "training_loss": 6.931019306182861
    },
    {
      "epoch": 0.5712737127371273,
      "step": 2635,
      "training_loss": 6.911503791809082
    },
    {
      "epoch": 0.571490514905149,
      "grad_norm": 17.76961898803711,
      "learning_rate": 1e-05,
      "loss": 6.6722,
      "step": 2636
    },
    {
      "epoch": 0.571490514905149,
      "step": 2636,
      "training_loss": 7.051142692565918
    },
    {
      "epoch": 0.571490514905149,
      "step": 2636,
      "training_loss": 6.177562236785889
    },
    {
      "epoch": 0.571490514905149,
      "step": 2636,
      "training_loss": 6.678800582885742
    },
    {
      "epoch": 0.571490514905149,
      "step": 2636,
      "training_loss": 6.6971845626831055
    },
    {
      "epoch": 0.5717073170731707,
      "step": 2637,
      "training_loss": 7.122015476226807
    },
    {
      "epoch": 0.5717073170731707,
      "step": 2637,
      "training_loss": 3.406085729598999
    },
    {
      "epoch": 0.5717073170731707,
      "step": 2637,
      "training_loss": 3.106335401535034
    },
    {
      "epoch": 0.5717073170731707,
      "step": 2637,
      "training_loss": 7.232946872711182
    },
    {
      "epoch": 0.5719241192411925,
      "step": 2638,
      "training_loss": 7.233365535736084
    },
    {
      "epoch": 0.5719241192411925,
      "step": 2638,
      "training_loss": 5.919357776641846
    },
    {
      "epoch": 0.5719241192411925,
      "step": 2638,
      "training_loss": 5.826121807098389
    },
    {
      "epoch": 0.5719241192411925,
      "step": 2638,
      "training_loss": 5.3595967292785645
    },
    {
      "epoch": 0.5721409214092141,
      "step": 2639,
      "training_loss": 3.320685386657715
    },
    {
      "epoch": 0.5721409214092141,
      "step": 2639,
      "training_loss": 8.330663681030273
    },
    {
      "epoch": 0.5721409214092141,
      "step": 2639,
      "training_loss": 4.750133514404297
    },
    {
      "epoch": 0.5721409214092141,
      "step": 2639,
      "training_loss": 7.046133995056152
    },
    {
      "epoch": 0.5723577235772358,
      "grad_norm": 27.406909942626953,
      "learning_rate": 1e-05,
      "loss": 5.9536,
      "step": 2640
    },
    {
      "epoch": 0.5723577235772358,
      "step": 2640,
      "training_loss": 3.250506639480591
    },
    {
      "epoch": 0.5723577235772358,
      "step": 2640,
      "training_loss": 6.391453742980957
    },
    {
      "epoch": 0.5723577235772358,
      "step": 2640,
      "training_loss": 5.650979995727539
    },
    {
      "epoch": 0.5723577235772358,
      "step": 2640,
      "training_loss": 6.5861968994140625
    },
    {
      "epoch": 0.5725745257452575,
      "step": 2641,
      "training_loss": 7.086578369140625
    },
    {
      "epoch": 0.5725745257452575,
      "step": 2641,
      "training_loss": 6.810123920440674
    },
    {
      "epoch": 0.5725745257452575,
      "step": 2641,
      "training_loss": 6.052248954772949
    },
    {
      "epoch": 0.5725745257452575,
      "step": 2641,
      "training_loss": 5.9423651695251465
    },
    {
      "epoch": 0.5727913279132791,
      "step": 2642,
      "training_loss": 6.815279483795166
    },
    {
      "epoch": 0.5727913279132791,
      "step": 2642,
      "training_loss": 6.0379815101623535
    },
    {
      "epoch": 0.5727913279132791,
      "step": 2642,
      "training_loss": 6.453384876251221
    },
    {
      "epoch": 0.5727913279132791,
      "step": 2642,
      "training_loss": 6.3622612953186035
    },
    {
      "epoch": 0.5730081300813008,
      "step": 2643,
      "training_loss": 7.684617042541504
    },
    {
      "epoch": 0.5730081300813008,
      "step": 2643,
      "training_loss": 6.0382256507873535
    },
    {
      "epoch": 0.5730081300813008,
      "step": 2643,
      "training_loss": 5.276885032653809
    },
    {
      "epoch": 0.5730081300813008,
      "step": 2643,
      "training_loss": 6.824182033538818
    },
    {
      "epoch": 0.5732249322493225,
      "grad_norm": 13.374192237854004,
      "learning_rate": 1e-05,
      "loss": 6.204,
      "step": 2644
    },
    {
      "epoch": 0.5732249322493225,
      "step": 2644,
      "training_loss": 5.807837009429932
    },
    {
      "epoch": 0.5732249322493225,
      "step": 2644,
      "training_loss": 7.042821407318115
    },
    {
      "epoch": 0.5732249322493225,
      "step": 2644,
      "training_loss": 7.2465410232543945
    },
    {
      "epoch": 0.5732249322493225,
      "step": 2644,
      "training_loss": 5.958529472351074
    },
    {
      "epoch": 0.5734417344173441,
      "step": 2645,
      "training_loss": 6.978947162628174
    },
    {
      "epoch": 0.5734417344173441,
      "step": 2645,
      "training_loss": 6.488363742828369
    },
    {
      "epoch": 0.5734417344173441,
      "step": 2645,
      "training_loss": 6.438764572143555
    },
    {
      "epoch": 0.5734417344173441,
      "step": 2645,
      "training_loss": 6.508270263671875
    },
    {
      "epoch": 0.5736585365853658,
      "step": 2646,
      "training_loss": 6.77876615524292
    },
    {
      "epoch": 0.5736585365853658,
      "step": 2646,
      "training_loss": 6.971530437469482
    },
    {
      "epoch": 0.5736585365853658,
      "step": 2646,
      "training_loss": 6.8884100914001465
    },
    {
      "epoch": 0.5736585365853658,
      "step": 2646,
      "training_loss": 7.315671443939209
    },
    {
      "epoch": 0.5738753387533876,
      "step": 2647,
      "training_loss": 6.451901912689209
    },
    {
      "epoch": 0.5738753387533876,
      "step": 2647,
      "training_loss": 6.607058525085449
    },
    {
      "epoch": 0.5738753387533876,
      "step": 2647,
      "training_loss": 6.77137565612793
    },
    {
      "epoch": 0.5738753387533876,
      "step": 2647,
      "training_loss": 7.338466167449951
    },
    {
      "epoch": 0.5740921409214093,
      "grad_norm": 19.344284057617188,
      "learning_rate": 1e-05,
      "loss": 6.7246,
      "step": 2648
    },
    {
      "epoch": 0.5740921409214093,
      "step": 2648,
      "training_loss": 8.792492866516113
    },
    {
      "epoch": 0.5740921409214093,
      "step": 2648,
      "training_loss": 6.779781341552734
    },
    {
      "epoch": 0.5740921409214093,
      "step": 2648,
      "training_loss": 5.768321990966797
    },
    {
      "epoch": 0.5740921409214093,
      "step": 2648,
      "training_loss": 7.020366191864014
    },
    {
      "epoch": 0.5743089430894309,
      "step": 2649,
      "training_loss": 7.3916096687316895
    },
    {
      "epoch": 0.5743089430894309,
      "step": 2649,
      "training_loss": 8.032724380493164
    },
    {
      "epoch": 0.5743089430894309,
      "step": 2649,
      "training_loss": 6.993922233581543
    },
    {
      "epoch": 0.5743089430894309,
      "step": 2649,
      "training_loss": 5.157944679260254
    },
    {
      "epoch": 0.5745257452574526,
      "step": 2650,
      "training_loss": 6.022284030914307
    },
    {
      "epoch": 0.5745257452574526,
      "step": 2650,
      "training_loss": 7.443838119506836
    },
    {
      "epoch": 0.5745257452574526,
      "step": 2650,
      "training_loss": 3.500800132751465
    },
    {
      "epoch": 0.5745257452574526,
      "step": 2650,
      "training_loss": 6.719367504119873
    },
    {
      "epoch": 0.5747425474254743,
      "step": 2651,
      "training_loss": 7.274847507476807
    },
    {
      "epoch": 0.5747425474254743,
      "step": 2651,
      "training_loss": 5.8366804122924805
    },
    {
      "epoch": 0.5747425474254743,
      "step": 2651,
      "training_loss": 5.878364086151123
    },
    {
      "epoch": 0.5747425474254743,
      "step": 2651,
      "training_loss": 7.576340198516846
    },
    {
      "epoch": 0.5749593495934959,
      "grad_norm": 18.04963493347168,
      "learning_rate": 1e-05,
      "loss": 6.6369,
      "step": 2652
    },
    {
      "epoch": 0.5749593495934959,
      "step": 2652,
      "training_loss": 7.584312438964844
    },
    {
      "epoch": 0.5749593495934959,
      "step": 2652,
      "training_loss": 7.028631687164307
    },
    {
      "epoch": 0.5749593495934959,
      "step": 2652,
      "training_loss": 7.05689001083374
    },
    {
      "epoch": 0.5749593495934959,
      "step": 2652,
      "training_loss": 8.045620918273926
    },
    {
      "epoch": 0.5751761517615176,
      "step": 2653,
      "training_loss": 6.033814430236816
    },
    {
      "epoch": 0.5751761517615176,
      "step": 2653,
      "training_loss": 6.9331440925598145
    },
    {
      "epoch": 0.5751761517615176,
      "step": 2653,
      "training_loss": 6.997954368591309
    },
    {
      "epoch": 0.5751761517615176,
      "step": 2653,
      "training_loss": 6.833166122436523
    },
    {
      "epoch": 0.5753929539295393,
      "step": 2654,
      "training_loss": 7.786479949951172
    },
    {
      "epoch": 0.5753929539295393,
      "step": 2654,
      "training_loss": 6.913765907287598
    },
    {
      "epoch": 0.5753929539295393,
      "step": 2654,
      "training_loss": 6.368531703948975
    },
    {
      "epoch": 0.5753929539295393,
      "step": 2654,
      "training_loss": 7.277358531951904
    },
    {
      "epoch": 0.5756097560975609,
      "step": 2655,
      "training_loss": 7.203462600708008
    },
    {
      "epoch": 0.5756097560975609,
      "step": 2655,
      "training_loss": 2.801666021347046
    },
    {
      "epoch": 0.5756097560975609,
      "step": 2655,
      "training_loss": 6.9140305519104
    },
    {
      "epoch": 0.5756097560975609,
      "step": 2655,
      "training_loss": 7.1837897300720215
    },
    {
      "epoch": 0.5758265582655827,
      "grad_norm": 19.83638572692871,
      "learning_rate": 1e-05,
      "loss": 6.8102,
      "step": 2656
    },
    {
      "epoch": 0.5758265582655827,
      "step": 2656,
      "training_loss": 5.543341636657715
    },
    {
      "epoch": 0.5758265582655827,
      "step": 2656,
      "training_loss": 6.309175491333008
    },
    {
      "epoch": 0.5758265582655827,
      "step": 2656,
      "training_loss": 7.772983551025391
    },
    {
      "epoch": 0.5758265582655827,
      "step": 2656,
      "training_loss": 5.76633358001709
    },
    {
      "epoch": 0.5760433604336044,
      "step": 2657,
      "training_loss": 5.935696601867676
    },
    {
      "epoch": 0.5760433604336044,
      "step": 2657,
      "training_loss": 5.514331340789795
    },
    {
      "epoch": 0.5760433604336044,
      "step": 2657,
      "training_loss": 6.649552822113037
    },
    {
      "epoch": 0.5760433604336044,
      "step": 2657,
      "training_loss": 6.688464641571045
    },
    {
      "epoch": 0.576260162601626,
      "step": 2658,
      "training_loss": 6.124192237854004
    },
    {
      "epoch": 0.576260162601626,
      "step": 2658,
      "training_loss": 7.23651647567749
    },
    {
      "epoch": 0.576260162601626,
      "step": 2658,
      "training_loss": 3.2512712478637695
    },
    {
      "epoch": 0.576260162601626,
      "step": 2658,
      "training_loss": 7.577655792236328
    },
    {
      "epoch": 0.5764769647696477,
      "step": 2659,
      "training_loss": 7.197659969329834
    },
    {
      "epoch": 0.5764769647696477,
      "step": 2659,
      "training_loss": 6.994356155395508
    },
    {
      "epoch": 0.5764769647696477,
      "step": 2659,
      "training_loss": 5.5616230964660645
    },
    {
      "epoch": 0.5764769647696477,
      "step": 2659,
      "training_loss": 7.980215549468994
    },
    {
      "epoch": 0.5766937669376694,
      "grad_norm": 18.06340980529785,
      "learning_rate": 1e-05,
      "loss": 6.3815,
      "step": 2660
    },
    {
      "epoch": 0.5766937669376694,
      "step": 2660,
      "training_loss": 7.820719242095947
    },
    {
      "epoch": 0.5766937669376694,
      "step": 2660,
      "training_loss": 4.551113128662109
    },
    {
      "epoch": 0.5766937669376694,
      "step": 2660,
      "training_loss": 7.280802249908447
    },
    {
      "epoch": 0.5766937669376694,
      "step": 2660,
      "training_loss": 5.641833782196045
    },
    {
      "epoch": 0.576910569105691,
      "step": 2661,
      "training_loss": 6.095165252685547
    },
    {
      "epoch": 0.576910569105691,
      "step": 2661,
      "training_loss": 7.287567615509033
    },
    {
      "epoch": 0.576910569105691,
      "step": 2661,
      "training_loss": 6.028266906738281
    },
    {
      "epoch": 0.576910569105691,
      "step": 2661,
      "training_loss": 7.612832069396973
    },
    {
      "epoch": 0.5771273712737127,
      "step": 2662,
      "training_loss": 5.827835559844971
    },
    {
      "epoch": 0.5771273712737127,
      "step": 2662,
      "training_loss": 7.847415447235107
    },
    {
      "epoch": 0.5771273712737127,
      "step": 2662,
      "training_loss": 6.235975742340088
    },
    {
      "epoch": 0.5771273712737127,
      "step": 2662,
      "training_loss": 6.746288776397705
    },
    {
      "epoch": 0.5773441734417344,
      "step": 2663,
      "training_loss": 6.361015796661377
    },
    {
      "epoch": 0.5773441734417344,
      "step": 2663,
      "training_loss": 4.9192891120910645
    },
    {
      "epoch": 0.5773441734417344,
      "step": 2663,
      "training_loss": 7.4645843505859375
    },
    {
      "epoch": 0.5773441734417344,
      "step": 2663,
      "training_loss": 5.412003993988037
    },
    {
      "epoch": 0.577560975609756,
      "grad_norm": 15.413000106811523,
      "learning_rate": 1e-05,
      "loss": 6.4458,
      "step": 2664
    },
    {
      "epoch": 0.577560975609756,
      "step": 2664,
      "training_loss": 6.6932573318481445
    },
    {
      "epoch": 0.577560975609756,
      "step": 2664,
      "training_loss": 7.188467025756836
    },
    {
      "epoch": 0.577560975609756,
      "step": 2664,
      "training_loss": 6.695914268493652
    },
    {
      "epoch": 0.577560975609756,
      "step": 2664,
      "training_loss": 4.375969409942627
    },
    {
      "epoch": 0.5777777777777777,
      "step": 2665,
      "training_loss": 6.363908767700195
    },
    {
      "epoch": 0.5777777777777777,
      "step": 2665,
      "training_loss": 6.151350498199463
    },
    {
      "epoch": 0.5777777777777777,
      "step": 2665,
      "training_loss": 7.5596604347229
    },
    {
      "epoch": 0.5777777777777777,
      "step": 2665,
      "training_loss": 7.071356296539307
    },
    {
      "epoch": 0.5779945799457995,
      "step": 2666,
      "training_loss": 6.796299457550049
    },
    {
      "epoch": 0.5779945799457995,
      "step": 2666,
      "training_loss": 2.984816551208496
    },
    {
      "epoch": 0.5779945799457995,
      "step": 2666,
      "training_loss": 7.122110843658447
    },
    {
      "epoch": 0.5779945799457995,
      "step": 2666,
      "training_loss": 5.085062503814697
    },
    {
      "epoch": 0.5782113821138212,
      "step": 2667,
      "training_loss": 6.471224784851074
    },
    {
      "epoch": 0.5782113821138212,
      "step": 2667,
      "training_loss": 6.402495861053467
    },
    {
      "epoch": 0.5782113821138212,
      "step": 2667,
      "training_loss": 6.727668762207031
    },
    {
      "epoch": 0.5782113821138212,
      "step": 2667,
      "training_loss": 5.229415416717529
    },
    {
      "epoch": 0.5784281842818428,
      "grad_norm": 18.394155502319336,
      "learning_rate": 1e-05,
      "loss": 6.1824,
      "step": 2668
    },
    {
      "epoch": 0.5784281842818428,
      "step": 2668,
      "training_loss": 5.3870439529418945
    },
    {
      "epoch": 0.5784281842818428,
      "step": 2668,
      "training_loss": 6.422094821929932
    },
    {
      "epoch": 0.5784281842818428,
      "step": 2668,
      "training_loss": 7.003296375274658
    },
    {
      "epoch": 0.5784281842818428,
      "step": 2668,
      "training_loss": 8.205072402954102
    },
    {
      "epoch": 0.5786449864498645,
      "step": 2669,
      "training_loss": 8.225384712219238
    },
    {
      "epoch": 0.5786449864498645,
      "step": 2669,
      "training_loss": 4.536655902862549
    },
    {
      "epoch": 0.5786449864498645,
      "step": 2669,
      "training_loss": 7.1531453132629395
    },
    {
      "epoch": 0.5786449864498645,
      "step": 2669,
      "training_loss": 7.090021133422852
    },
    {
      "epoch": 0.5788617886178862,
      "step": 2670,
      "training_loss": 6.995759963989258
    },
    {
      "epoch": 0.5788617886178862,
      "step": 2670,
      "training_loss": 7.4897260665893555
    },
    {
      "epoch": 0.5788617886178862,
      "step": 2670,
      "training_loss": 6.301825523376465
    },
    {
      "epoch": 0.5788617886178862,
      "step": 2670,
      "training_loss": 5.788689613342285
    },
    {
      "epoch": 0.5790785907859078,
      "step": 2671,
      "training_loss": 7.958305835723877
    },
    {
      "epoch": 0.5790785907859078,
      "step": 2671,
      "training_loss": 3.7008609771728516
    },
    {
      "epoch": 0.5790785907859078,
      "step": 2671,
      "training_loss": 6.373214244842529
    },
    {
      "epoch": 0.5790785907859078,
      "step": 2671,
      "training_loss": 7.7028374671936035
    },
    {
      "epoch": 0.5792953929539295,
      "grad_norm": 11.208457946777344,
      "learning_rate": 1e-05,
      "loss": 6.6459,
      "step": 2672
    },
    {
      "epoch": 0.5792953929539295,
      "step": 2672,
      "training_loss": 6.717261791229248
    },
    {
      "epoch": 0.5792953929539295,
      "step": 2672,
      "training_loss": 7.057109355926514
    },
    {
      "epoch": 0.5792953929539295,
      "step": 2672,
      "training_loss": 7.479628562927246
    },
    {
      "epoch": 0.5792953929539295,
      "step": 2672,
      "training_loss": 6.497630596160889
    },
    {
      "epoch": 0.5795121951219512,
      "step": 2673,
      "training_loss": 5.942981243133545
    },
    {
      "epoch": 0.5795121951219512,
      "step": 2673,
      "training_loss": 6.7602338790893555
    },
    {
      "epoch": 0.5795121951219512,
      "step": 2673,
      "training_loss": 7.824137210845947
    },
    {
      "epoch": 0.5795121951219512,
      "step": 2673,
      "training_loss": 5.727173328399658
    },
    {
      "epoch": 0.5797289972899728,
      "step": 2674,
      "training_loss": 6.47006893157959
    },
    {
      "epoch": 0.5797289972899728,
      "step": 2674,
      "training_loss": 6.843393802642822
    },
    {
      "epoch": 0.5797289972899728,
      "step": 2674,
      "training_loss": 7.7832746505737305
    },
    {
      "epoch": 0.5797289972899728,
      "step": 2674,
      "training_loss": 8.239956855773926
    },
    {
      "epoch": 0.5799457994579946,
      "step": 2675,
      "training_loss": 7.781033039093018
    },
    {
      "epoch": 0.5799457994579946,
      "step": 2675,
      "training_loss": 8.032397270202637
    },
    {
      "epoch": 0.5799457994579946,
      "step": 2675,
      "training_loss": 6.263706207275391
    },
    {
      "epoch": 0.5799457994579946,
      "step": 2675,
      "training_loss": 7.52081823348999
    },
    {
      "epoch": 0.5801626016260163,
      "grad_norm": 16.085893630981445,
      "learning_rate": 1e-05,
      "loss": 7.0588,
      "step": 2676
    },
    {
      "epoch": 0.5801626016260163,
      "step": 2676,
      "training_loss": 7.257136821746826
    },
    {
      "epoch": 0.5801626016260163,
      "step": 2676,
      "training_loss": 6.719925403594971
    },
    {
      "epoch": 0.5801626016260163,
      "step": 2676,
      "training_loss": 6.833230972290039
    },
    {
      "epoch": 0.5801626016260163,
      "step": 2676,
      "training_loss": 6.236032962799072
    },
    {
      "epoch": 0.580379403794038,
      "step": 2677,
      "training_loss": 5.8459272384643555
    },
    {
      "epoch": 0.580379403794038,
      "step": 2677,
      "training_loss": 6.958591461181641
    },
    {
      "epoch": 0.580379403794038,
      "step": 2677,
      "training_loss": 6.148103713989258
    },
    {
      "epoch": 0.580379403794038,
      "step": 2677,
      "training_loss": 6.645815849304199
    },
    {
      "epoch": 0.5805962059620596,
      "step": 2678,
      "training_loss": 7.276404857635498
    },
    {
      "epoch": 0.5805962059620596,
      "step": 2678,
      "training_loss": 5.885428428649902
    },
    {
      "epoch": 0.5805962059620596,
      "step": 2678,
      "training_loss": 7.2980875968933105
    },
    {
      "epoch": 0.5805962059620596,
      "step": 2678,
      "training_loss": 4.571692943572998
    },
    {
      "epoch": 0.5808130081300813,
      "step": 2679,
      "training_loss": 6.250574588775635
    },
    {
      "epoch": 0.5808130081300813,
      "step": 2679,
      "training_loss": 6.418405532836914
    },
    {
      "epoch": 0.5808130081300813,
      "step": 2679,
      "training_loss": 7.339233875274658
    },
    {
      "epoch": 0.5808130081300813,
      "step": 2679,
      "training_loss": 4.980531692504883
    },
    {
      "epoch": 0.581029810298103,
      "grad_norm": 11.131819725036621,
      "learning_rate": 1e-05,
      "loss": 6.4166,
      "step": 2680
    },
    {
      "epoch": 0.581029810298103,
      "step": 2680,
      "training_loss": 6.573138236999512
    },
    {
      "epoch": 0.581029810298103,
      "step": 2680,
      "training_loss": 6.807049751281738
    },
    {
      "epoch": 0.581029810298103,
      "step": 2680,
      "training_loss": 7.403043746948242
    },
    {
      "epoch": 0.581029810298103,
      "step": 2680,
      "training_loss": 5.586968421936035
    },
    {
      "epoch": 0.5812466124661246,
      "step": 2681,
      "training_loss": 6.868546009063721
    },
    {
      "epoch": 0.5812466124661246,
      "step": 2681,
      "training_loss": 6.658998489379883
    },
    {
      "epoch": 0.5812466124661246,
      "step": 2681,
      "training_loss": 3.324495315551758
    },
    {
      "epoch": 0.5812466124661246,
      "step": 2681,
      "training_loss": 7.187157154083252
    },
    {
      "epoch": 0.5814634146341463,
      "step": 2682,
      "training_loss": 7.415140628814697
    },
    {
      "epoch": 0.5814634146341463,
      "step": 2682,
      "training_loss": 6.282016754150391
    },
    {
      "epoch": 0.5814634146341463,
      "step": 2682,
      "training_loss": 6.718768119812012
    },
    {
      "epoch": 0.5814634146341463,
      "step": 2682,
      "training_loss": 5.381428241729736
    },
    {
      "epoch": 0.581680216802168,
      "step": 2683,
      "training_loss": 6.4292826652526855
    },
    {
      "epoch": 0.581680216802168,
      "step": 2683,
      "training_loss": 5.096242427825928
    },
    {
      "epoch": 0.581680216802168,
      "step": 2683,
      "training_loss": 7.333625316619873
    },
    {
      "epoch": 0.581680216802168,
      "step": 2683,
      "training_loss": 7.139440059661865
    },
    {
      "epoch": 0.5818970189701897,
      "grad_norm": 20.533079147338867,
      "learning_rate": 1e-05,
      "loss": 6.3878,
      "step": 2684
    },
    {
      "epoch": 0.5818970189701897,
      "step": 2684,
      "training_loss": 5.929365634918213
    },
    {
      "epoch": 0.5818970189701897,
      "step": 2684,
      "training_loss": 6.9438300132751465
    },
    {
      "epoch": 0.5818970189701897,
      "step": 2684,
      "training_loss": 7.131780624389648
    },
    {
      "epoch": 0.5818970189701897,
      "step": 2684,
      "training_loss": 6.663854122161865
    },
    {
      "epoch": 0.5821138211382114,
      "step": 2685,
      "training_loss": 8.11488151550293
    },
    {
      "epoch": 0.5821138211382114,
      "step": 2685,
      "training_loss": 6.509451389312744
    },
    {
      "epoch": 0.5821138211382114,
      "step": 2685,
      "training_loss": 6.7143778800964355
    },
    {
      "epoch": 0.5821138211382114,
      "step": 2685,
      "training_loss": 6.128890514373779
    },
    {
      "epoch": 0.5823306233062331,
      "step": 2686,
      "training_loss": 6.761941909790039
    },
    {
      "epoch": 0.5823306233062331,
      "step": 2686,
      "training_loss": 5.710328578948975
    },
    {
      "epoch": 0.5823306233062331,
      "step": 2686,
      "training_loss": 7.401494979858398
    },
    {
      "epoch": 0.5823306233062331,
      "step": 2686,
      "training_loss": 6.304293632507324
    },
    {
      "epoch": 0.5825474254742548,
      "step": 2687,
      "training_loss": 6.710402011871338
    },
    {
      "epoch": 0.5825474254742548,
      "step": 2687,
      "training_loss": 6.650065898895264
    },
    {
      "epoch": 0.5825474254742548,
      "step": 2687,
      "training_loss": 5.522500038146973
    },
    {
      "epoch": 0.5825474254742548,
      "step": 2687,
      "training_loss": 7.057706832885742
    },
    {
      "epoch": 0.5827642276422764,
      "grad_norm": 14.817559242248535,
      "learning_rate": 1e-05,
      "loss": 6.6409,
      "step": 2688
    },
    {
      "epoch": 0.5827642276422764,
      "step": 2688,
      "training_loss": 5.65629768371582
    },
    {
      "epoch": 0.5827642276422764,
      "step": 2688,
      "training_loss": 6.628586292266846
    },
    {
      "epoch": 0.5827642276422764,
      "step": 2688,
      "training_loss": 6.206844806671143
    },
    {
      "epoch": 0.5827642276422764,
      "step": 2688,
      "training_loss": 6.089916706085205
    },
    {
      "epoch": 0.5829810298102981,
      "step": 2689,
      "training_loss": 3.23919939994812
    },
    {
      "epoch": 0.5829810298102981,
      "step": 2689,
      "training_loss": 4.839719772338867
    },
    {
      "epoch": 0.5829810298102981,
      "step": 2689,
      "training_loss": 6.924220085144043
    },
    {
      "epoch": 0.5829810298102981,
      "step": 2689,
      "training_loss": 7.469638824462891
    },
    {
      "epoch": 0.5831978319783198,
      "step": 2690,
      "training_loss": 6.538662433624268
    },
    {
      "epoch": 0.5831978319783198,
      "step": 2690,
      "training_loss": 6.3420515060424805
    },
    {
      "epoch": 0.5831978319783198,
      "step": 2690,
      "training_loss": 7.573094367980957
    },
    {
      "epoch": 0.5831978319783198,
      "step": 2690,
      "training_loss": 7.339871406555176
    },
    {
      "epoch": 0.5834146341463414,
      "step": 2691,
      "training_loss": 6.1557440757751465
    },
    {
      "epoch": 0.5834146341463414,
      "step": 2691,
      "training_loss": 6.81672477722168
    },
    {
      "epoch": 0.5834146341463414,
      "step": 2691,
      "training_loss": 7.7630205154418945
    },
    {
      "epoch": 0.5834146341463414,
      "step": 2691,
      "training_loss": 6.422767639160156
    },
    {
      "epoch": 0.5836314363143631,
      "grad_norm": 20.79371452331543,
      "learning_rate": 1e-05,
      "loss": 6.3754,
      "step": 2692
    },
    {
      "epoch": 0.5836314363143631,
      "step": 2692,
      "training_loss": 7.13110876083374
    },
    {
      "epoch": 0.5836314363143631,
      "step": 2692,
      "training_loss": 7.7401652336120605
    },
    {
      "epoch": 0.5836314363143631,
      "step": 2692,
      "training_loss": 7.118607044219971
    },
    {
      "epoch": 0.5836314363143631,
      "step": 2692,
      "training_loss": 6.857287883758545
    },
    {
      "epoch": 0.5838482384823849,
      "step": 2693,
      "training_loss": 6.895869731903076
    },
    {
      "epoch": 0.5838482384823849,
      "step": 2693,
      "training_loss": 5.627552509307861
    },
    {
      "epoch": 0.5838482384823849,
      "step": 2693,
      "training_loss": 5.962884902954102
    },
    {
      "epoch": 0.5838482384823849,
      "step": 2693,
      "training_loss": 6.433167457580566
    },
    {
      "epoch": 0.5840650406504065,
      "step": 2694,
      "training_loss": 6.348060607910156
    },
    {
      "epoch": 0.5840650406504065,
      "step": 2694,
      "training_loss": 6.236115455627441
    },
    {
      "epoch": 0.5840650406504065,
      "step": 2694,
      "training_loss": 6.466188907623291
    },
    {
      "epoch": 0.5840650406504065,
      "step": 2694,
      "training_loss": 5.823849678039551
    },
    {
      "epoch": 0.5842818428184282,
      "step": 2695,
      "training_loss": 8.189425468444824
    },
    {
      "epoch": 0.5842818428184282,
      "step": 2695,
      "training_loss": 6.742187976837158
    },
    {
      "epoch": 0.5842818428184282,
      "step": 2695,
      "training_loss": 4.684694766998291
    },
    {
      "epoch": 0.5842818428184282,
      "step": 2695,
      "training_loss": 7.140030384063721
    },
    {
      "epoch": 0.5844986449864499,
      "grad_norm": 14.769548416137695,
      "learning_rate": 1e-05,
      "loss": 6.5873,
      "step": 2696
    },
    {
      "epoch": 0.5844986449864499,
      "step": 2696,
      "training_loss": 6.541481018066406
    },
    {
      "epoch": 0.5844986449864499,
      "step": 2696,
      "training_loss": 6.887616157531738
    },
    {
      "epoch": 0.5844986449864499,
      "step": 2696,
      "training_loss": 6.585508346557617
    },
    {
      "epoch": 0.5844986449864499,
      "step": 2696,
      "training_loss": 7.082430839538574
    },
    {
      "epoch": 0.5847154471544715,
      "step": 2697,
      "training_loss": 5.886728763580322
    },
    {
      "epoch": 0.5847154471544715,
      "step": 2697,
      "training_loss": 6.355445384979248
    },
    {
      "epoch": 0.5847154471544715,
      "step": 2697,
      "training_loss": 6.104366302490234
    },
    {
      "epoch": 0.5847154471544715,
      "step": 2697,
      "training_loss": 6.115945339202881
    },
    {
      "epoch": 0.5849322493224932,
      "step": 2698,
      "training_loss": 6.231215953826904
    },
    {
      "epoch": 0.5849322493224932,
      "step": 2698,
      "training_loss": 6.279306888580322
    },
    {
      "epoch": 0.5849322493224932,
      "step": 2698,
      "training_loss": 6.559741497039795
    },
    {
      "epoch": 0.5849322493224932,
      "step": 2698,
      "training_loss": 6.783067226409912
    },
    {
      "epoch": 0.5851490514905149,
      "step": 2699,
      "training_loss": 5.654139518737793
    },
    {
      "epoch": 0.5851490514905149,
      "step": 2699,
      "training_loss": 7.120527744293213
    },
    {
      "epoch": 0.5851490514905149,
      "step": 2699,
      "training_loss": 4.227206230163574
    },
    {
      "epoch": 0.5851490514905149,
      "step": 2699,
      "training_loss": 6.714807987213135
    },
    {
      "epoch": 0.5853658536585366,
      "grad_norm": 18.29522705078125,
      "learning_rate": 1e-05,
      "loss": 6.3206,
      "step": 2700
    },
    {
      "epoch": 0.5853658536585366,
      "step": 2700,
      "training_loss": 6.297961711883545
    },
    {
      "epoch": 0.5853658536585366,
      "step": 2700,
      "training_loss": 7.276073932647705
    },
    {
      "epoch": 0.5853658536585366,
      "step": 2700,
      "training_loss": 6.665871620178223
    },
    {
      "epoch": 0.5853658536585366,
      "step": 2700,
      "training_loss": 5.610506534576416
    },
    {
      "epoch": 0.5855826558265582,
      "step": 2701,
      "training_loss": 6.938572406768799
    },
    {
      "epoch": 0.5855826558265582,
      "step": 2701,
      "training_loss": 6.485645294189453
    },
    {
      "epoch": 0.5855826558265582,
      "step": 2701,
      "training_loss": 7.277976989746094
    },
    {
      "epoch": 0.5855826558265582,
      "step": 2701,
      "training_loss": 7.409331798553467
    },
    {
      "epoch": 0.58579945799458,
      "step": 2702,
      "training_loss": 6.761415004730225
    },
    {
      "epoch": 0.58579945799458,
      "step": 2702,
      "training_loss": 5.584156513214111
    },
    {
      "epoch": 0.58579945799458,
      "step": 2702,
      "training_loss": 6.574334144592285
    },
    {
      "epoch": 0.58579945799458,
      "step": 2702,
      "training_loss": 6.325213432312012
    },
    {
      "epoch": 0.5860162601626017,
      "step": 2703,
      "training_loss": 6.040217399597168
    },
    {
      "epoch": 0.5860162601626017,
      "step": 2703,
      "training_loss": 7.663877487182617
    },
    {
      "epoch": 0.5860162601626017,
      "step": 2703,
      "training_loss": 6.863554000854492
    },
    {
      "epoch": 0.5860162601626017,
      "step": 2703,
      "training_loss": 2.5496935844421387
    },
    {
      "epoch": 0.5862330623306233,
      "grad_norm": 26.518577575683594,
      "learning_rate": 1e-05,
      "loss": 6.3953,
      "step": 2704
    },
    {
      "epoch": 0.5862330623306233,
      "step": 2704,
      "training_loss": 6.9386677742004395
    },
    {
      "epoch": 0.5862330623306233,
      "step": 2704,
      "training_loss": 6.143523693084717
    },
    {
      "epoch": 0.5862330623306233,
      "step": 2704,
      "training_loss": 5.426534652709961
    },
    {
      "epoch": 0.5862330623306233,
      "step": 2704,
      "training_loss": 7.5074782371521
    },
    {
      "epoch": 0.586449864498645,
      "step": 2705,
      "training_loss": 6.8719940185546875
    },
    {
      "epoch": 0.586449864498645,
      "step": 2705,
      "training_loss": 6.458357810974121
    },
    {
      "epoch": 0.586449864498645,
      "step": 2705,
      "training_loss": 4.953414440155029
    },
    {
      "epoch": 0.586449864498645,
      "step": 2705,
      "training_loss": 4.1371750831604
    },
    {
      "epoch": 0.5866666666666667,
      "step": 2706,
      "training_loss": 7.307363986968994
    },
    {
      "epoch": 0.5866666666666667,
      "step": 2706,
      "training_loss": 4.390413761138916
    },
    {
      "epoch": 0.5866666666666667,
      "step": 2706,
      "training_loss": 7.014679431915283
    },
    {
      "epoch": 0.5866666666666667,
      "step": 2706,
      "training_loss": 3.092841386795044
    },
    {
      "epoch": 0.5868834688346883,
      "step": 2707,
      "training_loss": 6.970283031463623
    },
    {
      "epoch": 0.5868834688346883,
      "step": 2707,
      "training_loss": 6.422332286834717
    },
    {
      "epoch": 0.5868834688346883,
      "step": 2707,
      "training_loss": 5.303930759429932
    },
    {
      "epoch": 0.5868834688346883,
      "step": 2707,
      "training_loss": 6.486588001251221
    },
    {
      "epoch": 0.58710027100271,
      "grad_norm": 20.820566177368164,
      "learning_rate": 1e-05,
      "loss": 5.9641,
      "step": 2708
    },
    {
      "epoch": 0.58710027100271,
      "step": 2708,
      "training_loss": 6.761013507843018
    },
    {
      "epoch": 0.58710027100271,
      "step": 2708,
      "training_loss": 2.852637529373169
    },
    {
      "epoch": 0.58710027100271,
      "step": 2708,
      "training_loss": 6.357198715209961
    },
    {
      "epoch": 0.58710027100271,
      "step": 2708,
      "training_loss": 8.854774475097656
    },
    {
      "epoch": 0.5873170731707317,
      "step": 2709,
      "training_loss": 5.917990684509277
    },
    {
      "epoch": 0.5873170731707317,
      "step": 2709,
      "training_loss": 6.687182426452637
    },
    {
      "epoch": 0.5873170731707317,
      "step": 2709,
      "training_loss": 6.558792591094971
    },
    {
      "epoch": 0.5873170731707317,
      "step": 2709,
      "training_loss": 6.778199195861816
    },
    {
      "epoch": 0.5875338753387533,
      "step": 2710,
      "training_loss": 6.083982467651367
    },
    {
      "epoch": 0.5875338753387533,
      "step": 2710,
      "training_loss": 6.824868202209473
    },
    {
      "epoch": 0.5875338753387533,
      "step": 2710,
      "training_loss": 3.4394805431365967
    },
    {
      "epoch": 0.5875338753387533,
      "step": 2710,
      "training_loss": 2.728182792663574
    },
    {
      "epoch": 0.5877506775067751,
      "step": 2711,
      "training_loss": 7.209906578063965
    },
    {
      "epoch": 0.5877506775067751,
      "step": 2711,
      "training_loss": 7.910269260406494
    },
    {
      "epoch": 0.5877506775067751,
      "step": 2711,
      "training_loss": 7.575190544128418
    },
    {
      "epoch": 0.5877506775067751,
      "step": 2711,
      "training_loss": 6.338122844696045
    },
    {
      "epoch": 0.5879674796747968,
      "grad_norm": 11.036789894104004,
      "learning_rate": 1e-05,
      "loss": 6.1799,
      "step": 2712
    },
    {
      "epoch": 0.5879674796747968,
      "step": 2712,
      "training_loss": 7.232746124267578
    },
    {
      "epoch": 0.5879674796747968,
      "step": 2712,
      "training_loss": 6.482568740844727
    },
    {
      "epoch": 0.5879674796747968,
      "step": 2712,
      "training_loss": 7.133713722229004
    },
    {
      "epoch": 0.5879674796747968,
      "step": 2712,
      "training_loss": 7.4184041023254395
    },
    {
      "epoch": 0.5881842818428185,
      "step": 2713,
      "training_loss": 5.911022663116455
    },
    {
      "epoch": 0.5881842818428185,
      "step": 2713,
      "training_loss": 6.800307750701904
    },
    {
      "epoch": 0.5881842818428185,
      "step": 2713,
      "training_loss": 7.5591535568237305
    },
    {
      "epoch": 0.5881842818428185,
      "step": 2713,
      "training_loss": 7.442419052124023
    },
    {
      "epoch": 0.5884010840108401,
      "step": 2714,
      "training_loss": 6.581457614898682
    },
    {
      "epoch": 0.5884010840108401,
      "step": 2714,
      "training_loss": 4.86220121383667
    },
    {
      "epoch": 0.5884010840108401,
      "step": 2714,
      "training_loss": 4.543856143951416
    },
    {
      "epoch": 0.5884010840108401,
      "step": 2714,
      "training_loss": 8.006501197814941
    },
    {
      "epoch": 0.5886178861788618,
      "step": 2715,
      "training_loss": 5.993831157684326
    },
    {
      "epoch": 0.5886178861788618,
      "step": 2715,
      "training_loss": 7.659352779388428
    },
    {
      "epoch": 0.5886178861788618,
      "step": 2715,
      "training_loss": 7.115477085113525
    },
    {
      "epoch": 0.5886178861788618,
      "step": 2715,
      "training_loss": 6.638656139373779
    },
    {
      "epoch": 0.5888346883468835,
      "grad_norm": 15.931151390075684,
      "learning_rate": 1e-05,
      "loss": 6.7114,
      "step": 2716
    },
    {
      "epoch": 0.5888346883468835,
      "step": 2716,
      "training_loss": 7.1100754737854
    },
    {
      "epoch": 0.5888346883468835,
      "step": 2716,
      "training_loss": 5.018559455871582
    },
    {
      "epoch": 0.5888346883468835,
      "step": 2716,
      "training_loss": 6.945071220397949
    },
    {
      "epoch": 0.5888346883468835,
      "step": 2716,
      "training_loss": 4.01919412612915
    },
    {
      "epoch": 0.5890514905149051,
      "step": 2717,
      "training_loss": 5.860443592071533
    },
    {
      "epoch": 0.5890514905149051,
      "step": 2717,
      "training_loss": 6.535722732543945
    },
    {
      "epoch": 0.5890514905149051,
      "step": 2717,
      "training_loss": 7.226404666900635
    },
    {
      "epoch": 0.5890514905149051,
      "step": 2717,
      "training_loss": 5.668542385101318
    },
    {
      "epoch": 0.5892682926829268,
      "step": 2718,
      "training_loss": 6.566120624542236
    },
    {
      "epoch": 0.5892682926829268,
      "step": 2718,
      "training_loss": 6.627230167388916
    },
    {
      "epoch": 0.5892682926829268,
      "step": 2718,
      "training_loss": 5.421711444854736
    },
    {
      "epoch": 0.5892682926829268,
      "step": 2718,
      "training_loss": 6.667007923126221
    },
    {
      "epoch": 0.5894850948509485,
      "step": 2719,
      "training_loss": 5.266401290893555
    },
    {
      "epoch": 0.5894850948509485,
      "step": 2719,
      "training_loss": 7.663308143615723
    },
    {
      "epoch": 0.5894850948509485,
      "step": 2719,
      "training_loss": 5.950849533081055
    },
    {
      "epoch": 0.5894850948509485,
      "step": 2719,
      "training_loss": 4.811007976531982
    },
    {
      "epoch": 0.5897018970189702,
      "grad_norm": 22.61493492126465,
      "learning_rate": 1e-05,
      "loss": 6.0849,
      "step": 2720
    },
    {
      "epoch": 0.5897018970189702,
      "step": 2720,
      "training_loss": 7.826498508453369
    },
    {
      "epoch": 0.5897018970189702,
      "step": 2720,
      "training_loss": 7.012753009796143
    },
    {
      "epoch": 0.5897018970189702,
      "step": 2720,
      "training_loss": 4.158384323120117
    },
    {
      "epoch": 0.5897018970189702,
      "step": 2720,
      "training_loss": 7.41513729095459
    },
    {
      "epoch": 0.5899186991869919,
      "step": 2721,
      "training_loss": 5.203448295593262
    },
    {
      "epoch": 0.5899186991869919,
      "step": 2721,
      "training_loss": 6.059823513031006
    },
    {
      "epoch": 0.5899186991869919,
      "step": 2721,
      "training_loss": 6.764104843139648
    },
    {
      "epoch": 0.5899186991869919,
      "step": 2721,
      "training_loss": 8.675203323364258
    },
    {
      "epoch": 0.5901355013550136,
      "step": 2722,
      "training_loss": 5.181117534637451
    },
    {
      "epoch": 0.5901355013550136,
      "step": 2722,
      "training_loss": 6.411192417144775
    },
    {
      "epoch": 0.5901355013550136,
      "step": 2722,
      "training_loss": 6.568974494934082
    },
    {
      "epoch": 0.5901355013550136,
      "step": 2722,
      "training_loss": 4.8352861404418945
    },
    {
      "epoch": 0.5903523035230352,
      "step": 2723,
      "training_loss": 5.832823276519775
    },
    {
      "epoch": 0.5903523035230352,
      "step": 2723,
      "training_loss": 6.497929096221924
    },
    {
      "epoch": 0.5903523035230352,
      "step": 2723,
      "training_loss": 5.5884528160095215
    },
    {
      "epoch": 0.5903523035230352,
      "step": 2723,
      "training_loss": 7.0096755027771
    },
    {
      "epoch": 0.5905691056910569,
      "grad_norm": 17.67550277709961,
      "learning_rate": 1e-05,
      "loss": 6.3151,
      "step": 2724
    },
    {
      "epoch": 0.5905691056910569,
      "step": 2724,
      "training_loss": 6.946890354156494
    },
    {
      "epoch": 0.5905691056910569,
      "step": 2724,
      "training_loss": 8.002166748046875
    },
    {
      "epoch": 0.5905691056910569,
      "step": 2724,
      "training_loss": 6.48240327835083
    },
    {
      "epoch": 0.5905691056910569,
      "step": 2724,
      "training_loss": 7.597399711608887
    },
    {
      "epoch": 0.5907859078590786,
      "step": 2725,
      "training_loss": 7.259469032287598
    },
    {
      "epoch": 0.5907859078590786,
      "step": 2725,
      "training_loss": 6.3176045417785645
    },
    {
      "epoch": 0.5907859078590786,
      "step": 2725,
      "training_loss": 6.3366289138793945
    },
    {
      "epoch": 0.5907859078590786,
      "step": 2725,
      "training_loss": 5.008062839508057
    },
    {
      "epoch": 0.5910027100271003,
      "step": 2726,
      "training_loss": 3.76816725730896
    },
    {
      "epoch": 0.5910027100271003,
      "step": 2726,
      "training_loss": 6.665367603302002
    },
    {
      "epoch": 0.5910027100271003,
      "step": 2726,
      "training_loss": 6.693228721618652
    },
    {
      "epoch": 0.5910027100271003,
      "step": 2726,
      "training_loss": 6.771188735961914
    },
    {
      "epoch": 0.5912195121951219,
      "step": 2727,
      "training_loss": 7.691413879394531
    },
    {
      "epoch": 0.5912195121951219,
      "step": 2727,
      "training_loss": 3.9009745121002197
    },
    {
      "epoch": 0.5912195121951219,
      "step": 2727,
      "training_loss": 4.784773349761963
    },
    {
      "epoch": 0.5912195121951219,
      "step": 2727,
      "training_loss": 7.154343128204346
    },
    {
      "epoch": 0.5914363143631436,
      "grad_norm": 13.83951473236084,
      "learning_rate": 1e-05,
      "loss": 6.3363,
      "step": 2728
    },
    {
      "epoch": 0.5914363143631436,
      "step": 2728,
      "training_loss": 6.319948196411133
    },
    {
      "epoch": 0.5914363143631436,
      "step": 2728,
      "training_loss": 4.936993598937988
    },
    {
      "epoch": 0.5914363143631436,
      "step": 2728,
      "training_loss": 6.091623306274414
    },
    {
      "epoch": 0.5914363143631436,
      "step": 2728,
      "training_loss": 6.604922294616699
    },
    {
      "epoch": 0.5916531165311653,
      "step": 2729,
      "training_loss": 7.242741584777832
    },
    {
      "epoch": 0.5916531165311653,
      "step": 2729,
      "training_loss": 7.099618911743164
    },
    {
      "epoch": 0.5916531165311653,
      "step": 2729,
      "training_loss": 4.861433982849121
    },
    {
      "epoch": 0.5916531165311653,
      "step": 2729,
      "training_loss": 7.667146682739258
    },
    {
      "epoch": 0.591869918699187,
      "step": 2730,
      "training_loss": 5.761856555938721
    },
    {
      "epoch": 0.591869918699187,
      "step": 2730,
      "training_loss": 7.848232746124268
    },
    {
      "epoch": 0.591869918699187,
      "step": 2730,
      "training_loss": 5.809031009674072
    },
    {
      "epoch": 0.591869918699187,
      "step": 2730,
      "training_loss": 5.7191081047058105
    },
    {
      "epoch": 0.5920867208672087,
      "step": 2731,
      "training_loss": 7.083829879760742
    },
    {
      "epoch": 0.5920867208672087,
      "step": 2731,
      "training_loss": 7.085912704467773
    },
    {
      "epoch": 0.5920867208672087,
      "step": 2731,
      "training_loss": 5.331265449523926
    },
    {
      "epoch": 0.5920867208672087,
      "step": 2731,
      "training_loss": 6.834555149078369
    },
    {
      "epoch": 0.5923035230352304,
      "grad_norm": 17.144393920898438,
      "learning_rate": 1e-05,
      "loss": 6.3936,
      "step": 2732
    },
    {
      "epoch": 0.5923035230352304,
      "step": 2732,
      "training_loss": 5.886948585510254
    },
    {
      "epoch": 0.5923035230352304,
      "step": 2732,
      "training_loss": 6.788646697998047
    },
    {
      "epoch": 0.5923035230352304,
      "step": 2732,
      "training_loss": 6.974705219268799
    },
    {
      "epoch": 0.5923035230352304,
      "step": 2732,
      "training_loss": 5.694043159484863
    },
    {
      "epoch": 0.592520325203252,
      "step": 2733,
      "training_loss": 7.030104637145996
    },
    {
      "epoch": 0.592520325203252,
      "step": 2733,
      "training_loss": 5.434916019439697
    },
    {
      "epoch": 0.592520325203252,
      "step": 2733,
      "training_loss": 5.432092189788818
    },
    {
      "epoch": 0.592520325203252,
      "step": 2733,
      "training_loss": 7.1973795890808105
    },
    {
      "epoch": 0.5927371273712737,
      "step": 2734,
      "training_loss": 3.1681089401245117
    },
    {
      "epoch": 0.5927371273712737,
      "step": 2734,
      "training_loss": 5.790192604064941
    },
    {
      "epoch": 0.5927371273712737,
      "step": 2734,
      "training_loss": 5.336668968200684
    },
    {
      "epoch": 0.5927371273712737,
      "step": 2734,
      "training_loss": 6.665383338928223
    },
    {
      "epoch": 0.5929539295392954,
      "step": 2735,
      "training_loss": 7.095895767211914
    },
    {
      "epoch": 0.5929539295392954,
      "step": 2735,
      "training_loss": 3.864109754562378
    },
    {
      "epoch": 0.5929539295392954,
      "step": 2735,
      "training_loss": 7.222198009490967
    },
    {
      "epoch": 0.5929539295392954,
      "step": 2735,
      "training_loss": 6.522245407104492
    },
    {
      "epoch": 0.593170731707317,
      "grad_norm": 15.482233047485352,
      "learning_rate": 1e-05,
      "loss": 6.0065,
      "step": 2736
    },
    {
      "epoch": 0.593170731707317,
      "step": 2736,
      "training_loss": 6.243443489074707
    },
    {
      "epoch": 0.593170731707317,
      "step": 2736,
      "training_loss": 3.7134206295013428
    },
    {
      "epoch": 0.593170731707317,
      "step": 2736,
      "training_loss": 7.007534027099609
    },
    {
      "epoch": 0.593170731707317,
      "step": 2736,
      "training_loss": 7.3084330558776855
    },
    {
      "epoch": 0.5933875338753387,
      "step": 2737,
      "training_loss": 6.109156131744385
    },
    {
      "epoch": 0.5933875338753387,
      "step": 2737,
      "training_loss": 6.095766067504883
    },
    {
      "epoch": 0.5933875338753387,
      "step": 2737,
      "training_loss": 7.636348247528076
    },
    {
      "epoch": 0.5933875338753387,
      "step": 2737,
      "training_loss": 5.413355350494385
    },
    {
      "epoch": 0.5936043360433604,
      "step": 2738,
      "training_loss": 6.994324207305908
    },
    {
      "epoch": 0.5936043360433604,
      "step": 2738,
      "training_loss": 6.081186294555664
    },
    {
      "epoch": 0.5936043360433604,
      "step": 2738,
      "training_loss": 5.022775650024414
    },
    {
      "epoch": 0.5936043360433604,
      "step": 2738,
      "training_loss": 5.887353897094727
    },
    {
      "epoch": 0.5938211382113822,
      "step": 2739,
      "training_loss": 7.439205646514893
    },
    {
      "epoch": 0.5938211382113822,
      "step": 2739,
      "training_loss": 7.520177364349365
    },
    {
      "epoch": 0.5938211382113822,
      "step": 2739,
      "training_loss": 5.773524761199951
    },
    {
      "epoch": 0.5938211382113822,
      "step": 2739,
      "training_loss": 5.581987380981445
    },
    {
      "epoch": 0.5940379403794038,
      "grad_norm": 13.810271263122559,
      "learning_rate": 1e-05,
      "loss": 6.2392,
      "step": 2740
    },
    {
      "epoch": 0.5940379403794038,
      "step": 2740,
      "training_loss": 5.304317474365234
    },
    {
      "epoch": 0.5940379403794038,
      "step": 2740,
      "training_loss": 5.7006683349609375
    },
    {
      "epoch": 0.5940379403794038,
      "step": 2740,
      "training_loss": 6.371645450592041
    },
    {
      "epoch": 0.5940379403794038,
      "step": 2740,
      "training_loss": 6.862676620483398
    },
    {
      "epoch": 0.5942547425474255,
      "step": 2741,
      "training_loss": 7.859313011169434
    },
    {
      "epoch": 0.5942547425474255,
      "step": 2741,
      "training_loss": 5.3874287605285645
    },
    {
      "epoch": 0.5942547425474255,
      "step": 2741,
      "training_loss": 7.346534729003906
    },
    {
      "epoch": 0.5942547425474255,
      "step": 2741,
      "training_loss": 6.560756206512451
    },
    {
      "epoch": 0.5944715447154472,
      "step": 2742,
      "training_loss": 5.1799821853637695
    },
    {
      "epoch": 0.5944715447154472,
      "step": 2742,
      "training_loss": 6.823819637298584
    },
    {
      "epoch": 0.5944715447154472,
      "step": 2742,
      "training_loss": 7.113615989685059
    },
    {
      "epoch": 0.5944715447154472,
      "step": 2742,
      "training_loss": 6.099239349365234
    },
    {
      "epoch": 0.5946883468834688,
      "step": 2743,
      "training_loss": 6.424228191375732
    },
    {
      "epoch": 0.5946883468834688,
      "step": 2743,
      "training_loss": 6.856986045837402
    },
    {
      "epoch": 0.5946883468834688,
      "step": 2743,
      "training_loss": 2.5089855194091797
    },
    {
      "epoch": 0.5946883468834688,
      "step": 2743,
      "training_loss": 5.9453911781311035
    },
    {
      "epoch": 0.5949051490514905,
      "grad_norm": 17.45271873474121,
      "learning_rate": 1e-05,
      "loss": 6.1466,
      "step": 2744
    },
    {
      "epoch": 0.5949051490514905,
      "step": 2744,
      "training_loss": 6.7521772384643555
    },
    {
      "epoch": 0.5949051490514905,
      "step": 2744,
      "training_loss": 5.871559143066406
    },
    {
      "epoch": 0.5949051490514905,
      "step": 2744,
      "training_loss": 6.560451030731201
    },
    {
      "epoch": 0.5949051490514905,
      "step": 2744,
      "training_loss": 6.566733360290527
    },
    {
      "epoch": 0.5951219512195122,
      "step": 2745,
      "training_loss": 7.966165065765381
    },
    {
      "epoch": 0.5951219512195122,
      "step": 2745,
      "training_loss": 4.5819902420043945
    },
    {
      "epoch": 0.5951219512195122,
      "step": 2745,
      "training_loss": 6.907084941864014
    },
    {
      "epoch": 0.5951219512195122,
      "step": 2745,
      "training_loss": 7.5385565757751465
    },
    {
      "epoch": 0.5953387533875338,
      "step": 2746,
      "training_loss": 5.845903396606445
    },
    {
      "epoch": 0.5953387533875338,
      "step": 2746,
      "training_loss": 4.956496238708496
    },
    {
      "epoch": 0.5953387533875338,
      "step": 2746,
      "training_loss": 6.9800519943237305
    },
    {
      "epoch": 0.5953387533875338,
      "step": 2746,
      "training_loss": 6.618675231933594
    },
    {
      "epoch": 0.5955555555555555,
      "step": 2747,
      "training_loss": 5.916710376739502
    },
    {
      "epoch": 0.5955555555555555,
      "step": 2747,
      "training_loss": 7.517396450042725
    },
    {
      "epoch": 0.5955555555555555,
      "step": 2747,
      "training_loss": 5.727034091949463
    },
    {
      "epoch": 0.5955555555555555,
      "step": 2747,
      "training_loss": 6.622466564178467
    },
    {
      "epoch": 0.5957723577235773,
      "grad_norm": 16.097261428833008,
      "learning_rate": 1e-05,
      "loss": 6.4331,
      "step": 2748
    },
    {
      "epoch": 0.5957723577235773,
      "step": 2748,
      "training_loss": 6.340458869934082
    },
    {
      "epoch": 0.5957723577235773,
      "step": 2748,
      "training_loss": 6.7369513511657715
    },
    {
      "epoch": 0.5957723577235773,
      "step": 2748,
      "training_loss": 7.261006832122803
    },
    {
      "epoch": 0.5957723577235773,
      "step": 2748,
      "training_loss": 5.5830488204956055
    },
    {
      "epoch": 0.595989159891599,
      "step": 2749,
      "training_loss": 5.992495059967041
    },
    {
      "epoch": 0.595989159891599,
      "step": 2749,
      "training_loss": 7.9794535636901855
    },
    {
      "epoch": 0.595989159891599,
      "step": 2749,
      "training_loss": 7.291533946990967
    },
    {
      "epoch": 0.595989159891599,
      "step": 2749,
      "training_loss": 7.667444705963135
    },
    {
      "epoch": 0.5962059620596206,
      "step": 2750,
      "training_loss": 4.144006729125977
    },
    {
      "epoch": 0.5962059620596206,
      "step": 2750,
      "training_loss": 7.132148265838623
    },
    {
      "epoch": 0.5962059620596206,
      "step": 2750,
      "training_loss": 5.080297946929932
    },
    {
      "epoch": 0.5962059620596206,
      "step": 2750,
      "training_loss": 7.0819091796875
    },
    {
      "epoch": 0.5964227642276423,
      "step": 2751,
      "training_loss": 6.961334228515625
    },
    {
      "epoch": 0.5964227642276423,
      "step": 2751,
      "training_loss": 7.350495338439941
    },
    {
      "epoch": 0.5964227642276423,
      "step": 2751,
      "training_loss": 6.240579605102539
    },
    {
      "epoch": 0.5964227642276423,
      "step": 2751,
      "training_loss": 6.248558521270752
    },
    {
      "epoch": 0.596639566395664,
      "grad_norm": 18.93467903137207,
      "learning_rate": 1e-05,
      "loss": 6.5682,
      "step": 2752
    },
    {
      "epoch": 0.596639566395664,
      "step": 2752,
      "training_loss": 6.985297203063965
    },
    {
      "epoch": 0.596639566395664,
      "step": 2752,
      "training_loss": 6.315428256988525
    },
    {
      "epoch": 0.596639566395664,
      "step": 2752,
      "training_loss": 7.856712818145752
    },
    {
      "epoch": 0.596639566395664,
      "step": 2752,
      "training_loss": 6.094390392303467
    },
    {
      "epoch": 0.5968563685636856,
      "step": 2753,
      "training_loss": 6.444395065307617
    },
    {
      "epoch": 0.5968563685636856,
      "step": 2753,
      "training_loss": 7.815804481506348
    },
    {
      "epoch": 0.5968563685636856,
      "step": 2753,
      "training_loss": 6.487578868865967
    },
    {
      "epoch": 0.5968563685636856,
      "step": 2753,
      "training_loss": 7.801241397857666
    },
    {
      "epoch": 0.5970731707317073,
      "step": 2754,
      "training_loss": 6.653624534606934
    },
    {
      "epoch": 0.5970731707317073,
      "step": 2754,
      "training_loss": 7.123312473297119
    },
    {
      "epoch": 0.5970731707317073,
      "step": 2754,
      "training_loss": 6.6977715492248535
    },
    {
      "epoch": 0.5970731707317073,
      "step": 2754,
      "training_loss": 5.481893062591553
    },
    {
      "epoch": 0.597289972899729,
      "step": 2755,
      "training_loss": 6.410212516784668
    },
    {
      "epoch": 0.597289972899729,
      "step": 2755,
      "training_loss": 7.789801120758057
    },
    {
      "epoch": 0.597289972899729,
      "step": 2755,
      "training_loss": 6.742080211639404
    },
    {
      "epoch": 0.597289972899729,
      "step": 2755,
      "training_loss": 5.272171974182129
    },
    {
      "epoch": 0.5975067750677506,
      "grad_norm": 21.35333251953125,
      "learning_rate": 1e-05,
      "loss": 6.7482,
      "step": 2756
    },
    {
      "epoch": 0.5975067750677506,
      "step": 2756,
      "training_loss": 6.9318389892578125
    },
    {
      "epoch": 0.5975067750677506,
      "step": 2756,
      "training_loss": 5.746348857879639
    },
    {
      "epoch": 0.5975067750677506,
      "step": 2756,
      "training_loss": 7.302089691162109
    },
    {
      "epoch": 0.5975067750677506,
      "step": 2756,
      "training_loss": 5.112739562988281
    },
    {
      "epoch": 0.5977235772357724,
      "step": 2757,
      "training_loss": 7.393099308013916
    },
    {
      "epoch": 0.5977235772357724,
      "step": 2757,
      "training_loss": 7.620197296142578
    },
    {
      "epoch": 0.5977235772357724,
      "step": 2757,
      "training_loss": 8.352753639221191
    },
    {
      "epoch": 0.5977235772357724,
      "step": 2757,
      "training_loss": 6.531960964202881
    },
    {
      "epoch": 0.5979403794037941,
      "step": 2758,
      "training_loss": 7.164605617523193
    },
    {
      "epoch": 0.5979403794037941,
      "step": 2758,
      "training_loss": 5.517752170562744
    },
    {
      "epoch": 0.5979403794037941,
      "step": 2758,
      "training_loss": 7.588893890380859
    },
    {
      "epoch": 0.5979403794037941,
      "step": 2758,
      "training_loss": 5.099393844604492
    },
    {
      "epoch": 0.5981571815718157,
      "step": 2759,
      "training_loss": 4.544083595275879
    },
    {
      "epoch": 0.5981571815718157,
      "step": 2759,
      "training_loss": 7.587290287017822
    },
    {
      "epoch": 0.5981571815718157,
      "step": 2759,
      "training_loss": 4.710443019866943
    },
    {
      "epoch": 0.5981571815718157,
      "step": 2759,
      "training_loss": 5.752597332000732
    },
    {
      "epoch": 0.5983739837398374,
      "grad_norm": 17.750041961669922,
      "learning_rate": 1e-05,
      "loss": 6.4348,
      "step": 2760
    },
    {
      "epoch": 0.5983739837398374,
      "step": 2760,
      "training_loss": 6.662752151489258
    },
    {
      "epoch": 0.5983739837398374,
      "step": 2760,
      "training_loss": 5.803261756896973
    },
    {
      "epoch": 0.5983739837398374,
      "step": 2760,
      "training_loss": 7.096839427947998
    },
    {
      "epoch": 0.5983739837398374,
      "step": 2760,
      "training_loss": 6.552204608917236
    },
    {
      "epoch": 0.5985907859078591,
      "step": 2761,
      "training_loss": 2.8160295486450195
    },
    {
      "epoch": 0.5985907859078591,
      "step": 2761,
      "training_loss": 5.376847743988037
    },
    {
      "epoch": 0.5985907859078591,
      "step": 2761,
      "training_loss": 6.014267921447754
    },
    {
      "epoch": 0.5985907859078591,
      "step": 2761,
      "training_loss": 4.774110317230225
    },
    {
      "epoch": 0.5988075880758807,
      "step": 2762,
      "training_loss": 4.591787815093994
    },
    {
      "epoch": 0.5988075880758807,
      "step": 2762,
      "training_loss": 7.0217204093933105
    },
    {
      "epoch": 0.5988075880758807,
      "step": 2762,
      "training_loss": 3.510051965713501
    },
    {
      "epoch": 0.5988075880758807,
      "step": 2762,
      "training_loss": 5.7621541023254395
    },
    {
      "epoch": 0.5990243902439024,
      "step": 2763,
      "training_loss": 7.4998860359191895
    },
    {
      "epoch": 0.5990243902439024,
      "step": 2763,
      "training_loss": 5.507411956787109
    },
    {
      "epoch": 0.5990243902439024,
      "step": 2763,
      "training_loss": 6.143927097320557
    },
    {
      "epoch": 0.5990243902439024,
      "step": 2763,
      "training_loss": 7.91873836517334
    },
    {
      "epoch": 0.5992411924119241,
      "grad_norm": 18.5846004486084,
      "learning_rate": 1e-05,
      "loss": 5.8157,
      "step": 2764
    },
    {
      "epoch": 0.5992411924119241,
      "step": 2764,
      "training_loss": 8.030214309692383
    },
    {
      "epoch": 0.5992411924119241,
      "step": 2764,
      "training_loss": 5.930222988128662
    },
    {
      "epoch": 0.5992411924119241,
      "step": 2764,
      "training_loss": 6.858376502990723
    },
    {
      "epoch": 0.5992411924119241,
      "step": 2764,
      "training_loss": 7.048579216003418
    },
    {
      "epoch": 0.5994579945799458,
      "step": 2765,
      "training_loss": 7.047027111053467
    },
    {
      "epoch": 0.5994579945799458,
      "step": 2765,
      "training_loss": 5.7277936935424805
    },
    {
      "epoch": 0.5994579945799458,
      "step": 2765,
      "training_loss": 7.636358261108398
    },
    {
      "epoch": 0.5994579945799458,
      "step": 2765,
      "training_loss": 7.198220729827881
    },
    {
      "epoch": 0.5996747967479675,
      "step": 2766,
      "training_loss": 4.491641998291016
    },
    {
      "epoch": 0.5996747967479675,
      "step": 2766,
      "training_loss": 4.019896507263184
    },
    {
      "epoch": 0.5996747967479675,
      "step": 2766,
      "training_loss": 5.9521894454956055
    },
    {
      "epoch": 0.5996747967479675,
      "step": 2766,
      "training_loss": 5.652591705322266
    },
    {
      "epoch": 0.5998915989159892,
      "step": 2767,
      "training_loss": 6.493082523345947
    },
    {
      "epoch": 0.5998915989159892,
      "step": 2767,
      "training_loss": 6.557321548461914
    },
    {
      "epoch": 0.5998915989159892,
      "step": 2767,
      "training_loss": 6.026212692260742
    },
    {
      "epoch": 0.5998915989159892,
      "step": 2767,
      "training_loss": 7.5484395027160645
    },
    {
      "epoch": 0.6001084010840109,
      "grad_norm": 15.244182586669922,
      "learning_rate": 1e-05,
      "loss": 6.3886,
      "step": 2768
    },
    {
      "epoch": 0.6001084010840109,
      "step": 2768,
      "training_loss": 6.774989128112793
    },
    {
      "epoch": 0.6001084010840109,
      "step": 2768,
      "training_loss": 4.010853290557861
    },
    {
      "epoch": 0.6001084010840109,
      "step": 2768,
      "training_loss": 3.680453062057495
    },
    {
      "epoch": 0.6001084010840109,
      "step": 2768,
      "training_loss": 6.2762322425842285
    },
    {
      "epoch": 0.6003252032520325,
      "step": 2769,
      "training_loss": 5.2467451095581055
    },
    {
      "epoch": 0.6003252032520325,
      "step": 2769,
      "training_loss": 5.904796600341797
    },
    {
      "epoch": 0.6003252032520325,
      "step": 2769,
      "training_loss": 6.914771556854248
    },
    {
      "epoch": 0.6003252032520325,
      "step": 2769,
      "training_loss": 6.181262016296387
    },
    {
      "epoch": 0.6005420054200542,
      "step": 2770,
      "training_loss": 6.5224127769470215
    },
    {
      "epoch": 0.6005420054200542,
      "step": 2770,
      "training_loss": 5.716351509094238
    },
    {
      "epoch": 0.6005420054200542,
      "step": 2770,
      "training_loss": 7.323633670806885
    },
    {
      "epoch": 0.6005420054200542,
      "step": 2770,
      "training_loss": 6.948382377624512
    },
    {
      "epoch": 0.6007588075880759,
      "step": 2771,
      "training_loss": 5.339753150939941
    },
    {
      "epoch": 0.6007588075880759,
      "step": 2771,
      "training_loss": 6.428044319152832
    },
    {
      "epoch": 0.6007588075880759,
      "step": 2771,
      "training_loss": 6.15005350112915
    },
    {
      "epoch": 0.6007588075880759,
      "step": 2771,
      "training_loss": 5.124970436096191
    },
    {
      "epoch": 0.6009756097560975,
      "grad_norm": 17.68801498413086,
      "learning_rate": 1e-05,
      "loss": 5.909,
      "step": 2772
    },
    {
      "epoch": 0.6009756097560975,
      "step": 2772,
      "training_loss": 6.991570949554443
    },
    {
      "epoch": 0.6009756097560975,
      "step": 2772,
      "training_loss": 7.792695999145508
    },
    {
      "epoch": 0.6009756097560975,
      "step": 2772,
      "training_loss": 6.299283981323242
    },
    {
      "epoch": 0.6009756097560975,
      "step": 2772,
      "training_loss": 3.8758792877197266
    },
    {
      "epoch": 0.6011924119241192,
      "step": 2773,
      "training_loss": 7.7802910804748535
    },
    {
      "epoch": 0.6011924119241192,
      "step": 2773,
      "training_loss": 5.506344795227051
    },
    {
      "epoch": 0.6011924119241192,
      "step": 2773,
      "training_loss": 7.638118267059326
    },
    {
      "epoch": 0.6011924119241192,
      "step": 2773,
      "training_loss": 5.917829990386963
    },
    {
      "epoch": 0.6014092140921409,
      "step": 2774,
      "training_loss": 6.701627254486084
    },
    {
      "epoch": 0.6014092140921409,
      "step": 2774,
      "training_loss": 6.746257781982422
    },
    {
      "epoch": 0.6014092140921409,
      "step": 2774,
      "training_loss": 5.469915866851807
    },
    {
      "epoch": 0.6014092140921409,
      "step": 2774,
      "training_loss": 6.765180587768555
    },
    {
      "epoch": 0.6016260162601627,
      "step": 2775,
      "training_loss": 6.855172157287598
    },
    {
      "epoch": 0.6016260162601627,
      "step": 2775,
      "training_loss": 6.7212395668029785
    },
    {
      "epoch": 0.6016260162601627,
      "step": 2775,
      "training_loss": 6.5861101150512695
    },
    {
      "epoch": 0.6016260162601627,
      "step": 2775,
      "training_loss": 6.424534797668457
    },
    {
      "epoch": 0.6018428184281843,
      "grad_norm": 21.589588165283203,
      "learning_rate": 1e-05,
      "loss": 6.5045,
      "step": 2776
    },
    {
      "epoch": 0.6018428184281843,
      "step": 2776,
      "training_loss": 3.852041721343994
    },
    {
      "epoch": 0.6018428184281843,
      "step": 2776,
      "training_loss": 6.689737319946289
    },
    {
      "epoch": 0.6018428184281843,
      "step": 2776,
      "training_loss": 5.640395641326904
    },
    {
      "epoch": 0.6018428184281843,
      "step": 2776,
      "training_loss": 3.0370047092437744
    },
    {
      "epoch": 0.602059620596206,
      "step": 2777,
      "training_loss": 7.214827060699463
    },
    {
      "epoch": 0.602059620596206,
      "step": 2777,
      "training_loss": 7.5704569816589355
    },
    {
      "epoch": 0.602059620596206,
      "step": 2777,
      "training_loss": 6.976441860198975
    },
    {
      "epoch": 0.602059620596206,
      "step": 2777,
      "training_loss": 5.6992692947387695
    },
    {
      "epoch": 0.6022764227642277,
      "step": 2778,
      "training_loss": 6.582396984100342
    },
    {
      "epoch": 0.6022764227642277,
      "step": 2778,
      "training_loss": 8.216732025146484
    },
    {
      "epoch": 0.6022764227642277,
      "step": 2778,
      "training_loss": 6.689492225646973
    },
    {
      "epoch": 0.6022764227642277,
      "step": 2778,
      "training_loss": 8.235272407531738
    },
    {
      "epoch": 0.6024932249322493,
      "step": 2779,
      "training_loss": 7.4475626945495605
    },
    {
      "epoch": 0.6024932249322493,
      "step": 2779,
      "training_loss": 6.203479766845703
    },
    {
      "epoch": 0.6024932249322493,
      "step": 2779,
      "training_loss": 7.420290470123291
    },
    {
      "epoch": 0.6024932249322493,
      "step": 2779,
      "training_loss": 6.549888610839844
    },
    {
      "epoch": 0.602710027100271,
      "grad_norm": 18.546464920043945,
      "learning_rate": 1e-05,
      "loss": 6.5016,
      "step": 2780
    },
    {
      "epoch": 0.602710027100271,
      "step": 2780,
      "training_loss": 7.029770374298096
    },
    {
      "epoch": 0.602710027100271,
      "step": 2780,
      "training_loss": 6.0716400146484375
    },
    {
      "epoch": 0.602710027100271,
      "step": 2780,
      "training_loss": 5.022325038909912
    },
    {
      "epoch": 0.602710027100271,
      "step": 2780,
      "training_loss": 7.04400110244751
    },
    {
      "epoch": 0.6029268292682927,
      "step": 2781,
      "training_loss": 6.564574241638184
    },
    {
      "epoch": 0.6029268292682927,
      "step": 2781,
      "training_loss": 5.5920257568359375
    },
    {
      "epoch": 0.6029268292682927,
      "step": 2781,
      "training_loss": 5.966004371643066
    },
    {
      "epoch": 0.6029268292682927,
      "step": 2781,
      "training_loss": 4.827768325805664
    },
    {
      "epoch": 0.6031436314363143,
      "step": 2782,
      "training_loss": 6.586357593536377
    },
    {
      "epoch": 0.6031436314363143,
      "step": 2782,
      "training_loss": 4.875800132751465
    },
    {
      "epoch": 0.6031436314363143,
      "step": 2782,
      "training_loss": 7.12686014175415
    },
    {
      "epoch": 0.6031436314363143,
      "step": 2782,
      "training_loss": 5.183610439300537
    },
    {
      "epoch": 0.603360433604336,
      "step": 2783,
      "training_loss": 5.949901103973389
    },
    {
      "epoch": 0.603360433604336,
      "step": 2783,
      "training_loss": 6.729403018951416
    },
    {
      "epoch": 0.603360433604336,
      "step": 2783,
      "training_loss": 6.534152030944824
    },
    {
      "epoch": 0.603360433604336,
      "step": 2783,
      "training_loss": 3.949636697769165
    },
    {
      "epoch": 0.6035772357723578,
      "grad_norm": 15.222099304199219,
      "learning_rate": 1e-05,
      "loss": 5.9409,
      "step": 2784
    },
    {
      "epoch": 0.6035772357723578,
      "step": 2784,
      "training_loss": 7.421056747436523
    },
    {
      "epoch": 0.6035772357723578,
      "step": 2784,
      "training_loss": 6.908806324005127
    },
    {
      "epoch": 0.6035772357723578,
      "step": 2784,
      "training_loss": 5.85634708404541
    },
    {
      "epoch": 0.6035772357723578,
      "step": 2784,
      "training_loss": 4.7851643562316895
    },
    {
      "epoch": 0.6037940379403794,
      "step": 2785,
      "training_loss": 6.426653861999512
    },
    {
      "epoch": 0.6037940379403794,
      "step": 2785,
      "training_loss": 8.368183135986328
    },
    {
      "epoch": 0.6037940379403794,
      "step": 2785,
      "training_loss": 5.426822185516357
    },
    {
      "epoch": 0.6037940379403794,
      "step": 2785,
      "training_loss": 5.023995876312256
    },
    {
      "epoch": 0.6040108401084011,
      "step": 2786,
      "training_loss": 6.124948501586914
    },
    {
      "epoch": 0.6040108401084011,
      "step": 2786,
      "training_loss": 7.747219085693359
    },
    {
      "epoch": 0.6040108401084011,
      "step": 2786,
      "training_loss": 6.864233493804932
    },
    {
      "epoch": 0.6040108401084011,
      "step": 2786,
      "training_loss": 7.43124532699585
    },
    {
      "epoch": 0.6042276422764228,
      "step": 2787,
      "training_loss": 7.6556830406188965
    },
    {
      "epoch": 0.6042276422764228,
      "step": 2787,
      "training_loss": 7.509650230407715
    },
    {
      "epoch": 0.6042276422764228,
      "step": 2787,
      "training_loss": 7.006453514099121
    },
    {
      "epoch": 0.6042276422764228,
      "step": 2787,
      "training_loss": 6.729947090148926
    },
    {
      "epoch": 0.6044444444444445,
      "grad_norm": 15.297574043273926,
      "learning_rate": 1e-05,
      "loss": 6.7054,
      "step": 2788
    },
    {
      "epoch": 0.6044444444444445,
      "step": 2788,
      "training_loss": 7.0044050216674805
    },
    {
      "epoch": 0.6044444444444445,
      "step": 2788,
      "training_loss": 7.23573112487793
    },
    {
      "epoch": 0.6044444444444445,
      "step": 2788,
      "training_loss": 7.750192642211914
    },
    {
      "epoch": 0.6044444444444445,
      "step": 2788,
      "training_loss": 7.500483512878418
    },
    {
      "epoch": 0.6046612466124661,
      "step": 2789,
      "training_loss": 5.879216194152832
    },
    {
      "epoch": 0.6046612466124661,
      "step": 2789,
      "training_loss": 5.616287708282471
    },
    {
      "epoch": 0.6046612466124661,
      "step": 2789,
      "training_loss": 4.529012203216553
    },
    {
      "epoch": 0.6046612466124661,
      "step": 2789,
      "training_loss": 7.264862537384033
    },
    {
      "epoch": 0.6048780487804878,
      "step": 2790,
      "training_loss": 6.652908802032471
    },
    {
      "epoch": 0.6048780487804878,
      "step": 2790,
      "training_loss": 6.657390117645264
    },
    {
      "epoch": 0.6048780487804878,
      "step": 2790,
      "training_loss": 3.758747100830078
    },
    {
      "epoch": 0.6048780487804878,
      "step": 2790,
      "training_loss": 6.3161234855651855
    },
    {
      "epoch": 0.6050948509485095,
      "step": 2791,
      "training_loss": 5.862765312194824
    },
    {
      "epoch": 0.6050948509485095,
      "step": 2791,
      "training_loss": 6.908825874328613
    },
    {
      "epoch": 0.6050948509485095,
      "step": 2791,
      "training_loss": 11.65422248840332
    },
    {
      "epoch": 0.6050948509485095,
      "step": 2791,
      "training_loss": 8.335577964782715
    },
    {
      "epoch": 0.6053116531165311,
      "grad_norm": 30.475910186767578,
      "learning_rate": 1e-05,
      "loss": 6.8079,
      "step": 2792
    },
    {
      "epoch": 0.6053116531165311,
      "step": 2792,
      "training_loss": 7.896120548248291
    },
    {
      "epoch": 0.6053116531165311,
      "step": 2792,
      "training_loss": 5.740972518920898
    },
    {
      "epoch": 0.6053116531165311,
      "step": 2792,
      "training_loss": 5.648293972015381
    },
    {
      "epoch": 0.6053116531165311,
      "step": 2792,
      "training_loss": 8.12709903717041
    },
    {
      "epoch": 0.6055284552845528,
      "step": 2793,
      "training_loss": 6.821600914001465
    },
    {
      "epoch": 0.6055284552845528,
      "step": 2793,
      "training_loss": 7.491270542144775
    },
    {
      "epoch": 0.6055284552845528,
      "step": 2793,
      "training_loss": 7.156002998352051
    },
    {
      "epoch": 0.6055284552845528,
      "step": 2793,
      "training_loss": 6.3069281578063965
    },
    {
      "epoch": 0.6057452574525746,
      "step": 2794,
      "training_loss": 6.360194683074951
    },
    {
      "epoch": 0.6057452574525746,
      "step": 2794,
      "training_loss": 9.412256240844727
    },
    {
      "epoch": 0.6057452574525746,
      "step": 2794,
      "training_loss": 6.565701961517334
    },
    {
      "epoch": 0.6057452574525746,
      "step": 2794,
      "training_loss": 6.1024675369262695
    },
    {
      "epoch": 0.6059620596205962,
      "step": 2795,
      "training_loss": 7.110507965087891
    },
    {
      "epoch": 0.6059620596205962,
      "step": 2795,
      "training_loss": 6.83245849609375
    },
    {
      "epoch": 0.6059620596205962,
      "step": 2795,
      "training_loss": 6.119938373565674
    },
    {
      "epoch": 0.6059620596205962,
      "step": 2795,
      "training_loss": 6.936929702758789
    },
    {
      "epoch": 0.6061788617886179,
      "grad_norm": 21.540462493896484,
      "learning_rate": 1e-05,
      "loss": 6.9143,
      "step": 2796
    },
    {
      "epoch": 0.6061788617886179,
      "step": 2796,
      "training_loss": 6.3450469970703125
    },
    {
      "epoch": 0.6061788617886179,
      "step": 2796,
      "training_loss": 7.723696231842041
    },
    {
      "epoch": 0.6061788617886179,
      "step": 2796,
      "training_loss": 6.702536106109619
    },
    {
      "epoch": 0.6061788617886179,
      "step": 2796,
      "training_loss": 5.524682521820068
    },
    {
      "epoch": 0.6063956639566396,
      "step": 2797,
      "training_loss": 6.029508113861084
    },
    {
      "epoch": 0.6063956639566396,
      "step": 2797,
      "training_loss": 4.579981327056885
    },
    {
      "epoch": 0.6063956639566396,
      "step": 2797,
      "training_loss": 7.436054706573486
    },
    {
      "epoch": 0.6063956639566396,
      "step": 2797,
      "training_loss": 6.700028419494629
    },
    {
      "epoch": 0.6066124661246612,
      "step": 2798,
      "training_loss": 6.405941963195801
    },
    {
      "epoch": 0.6066124661246612,
      "step": 2798,
      "training_loss": 8.464940071105957
    },
    {
      "epoch": 0.6066124661246612,
      "step": 2798,
      "training_loss": 3.286858081817627
    },
    {
      "epoch": 0.6066124661246612,
      "step": 2798,
      "training_loss": 6.280151844024658
    },
    {
      "epoch": 0.6068292682926829,
      "step": 2799,
      "training_loss": 5.721951961517334
    },
    {
      "epoch": 0.6068292682926829,
      "step": 2799,
      "training_loss": 3.158691167831421
    },
    {
      "epoch": 0.6068292682926829,
      "step": 2799,
      "training_loss": 6.473313808441162
    },
    {
      "epoch": 0.6068292682926829,
      "step": 2799,
      "training_loss": 6.667579650878906
    },
    {
      "epoch": 0.6070460704607046,
      "grad_norm": 15.603066444396973,
      "learning_rate": 1e-05,
      "loss": 6.0938,
      "step": 2800
    },
    {
      "epoch": 0.6070460704607046,
      "step": 2800,
      "training_loss": 5.74724006652832
    },
    {
      "epoch": 0.6070460704607046,
      "step": 2800,
      "training_loss": 8.164216041564941
    },
    {
      "epoch": 0.6070460704607046,
      "step": 2800,
      "training_loss": 7.627338886260986
    },
    {
      "epoch": 0.6070460704607046,
      "step": 2800,
      "training_loss": 7.02204704284668
    },
    {
      "epoch": 0.6072628726287262,
      "step": 2801,
      "training_loss": 7.8678789138793945
    },
    {
      "epoch": 0.6072628726287262,
      "step": 2801,
      "training_loss": 6.944875717163086
    },
    {
      "epoch": 0.6072628726287262,
      "step": 2801,
      "training_loss": 6.446737289428711
    },
    {
      "epoch": 0.6072628726287262,
      "step": 2801,
      "training_loss": 7.989346504211426
    },
    {
      "epoch": 0.6074796747967479,
      "step": 2802,
      "training_loss": 3.277249813079834
    },
    {
      "epoch": 0.6074796747967479,
      "step": 2802,
      "training_loss": 5.983201026916504
    },
    {
      "epoch": 0.6074796747967479,
      "step": 2802,
      "training_loss": 6.0367350578308105
    },
    {
      "epoch": 0.6074796747967479,
      "step": 2802,
      "training_loss": 7.003937244415283
    },
    {
      "epoch": 0.6076964769647697,
      "step": 2803,
      "training_loss": 7.7091169357299805
    },
    {
      "epoch": 0.6076964769647697,
      "step": 2803,
      "training_loss": 7.084234714508057
    },
    {
      "epoch": 0.6076964769647697,
      "step": 2803,
      "training_loss": 7.143091678619385
    },
    {
      "epoch": 0.6076964769647697,
      "step": 2803,
      "training_loss": 5.363015651702881
    },
    {
      "epoch": 0.6079132791327914,
      "grad_norm": 14.570892333984375,
      "learning_rate": 1e-05,
      "loss": 6.7131,
      "step": 2804
    },
    {
      "epoch": 0.6079132791327914,
      "step": 2804,
      "training_loss": 6.8527021408081055
    },
    {
      "epoch": 0.6079132791327914,
      "step": 2804,
      "training_loss": 6.632475852966309
    },
    {
      "epoch": 0.6079132791327914,
      "step": 2804,
      "training_loss": 2.8477299213409424
    },
    {
      "epoch": 0.6079132791327914,
      "step": 2804,
      "training_loss": 6.21360445022583
    },
    {
      "epoch": 0.608130081300813,
      "step": 2805,
      "training_loss": 7.7179436683654785
    },
    {
      "epoch": 0.608130081300813,
      "step": 2805,
      "training_loss": 4.985904693603516
    },
    {
      "epoch": 0.608130081300813,
      "step": 2805,
      "training_loss": 6.585126876831055
    },
    {
      "epoch": 0.608130081300813,
      "step": 2805,
      "training_loss": 6.538205146789551
    },
    {
      "epoch": 0.6083468834688347,
      "step": 2806,
      "training_loss": 5.34828519821167
    },
    {
      "epoch": 0.6083468834688347,
      "step": 2806,
      "training_loss": 4.9803361892700195
    },
    {
      "epoch": 0.6083468834688347,
      "step": 2806,
      "training_loss": 6.010170936584473
    },
    {
      "epoch": 0.6083468834688347,
      "step": 2806,
      "training_loss": 7.3588032722473145
    },
    {
      "epoch": 0.6085636856368564,
      "step": 2807,
      "training_loss": 6.86852502822876
    },
    {
      "epoch": 0.6085636856368564,
      "step": 2807,
      "training_loss": 6.0992889404296875
    },
    {
      "epoch": 0.6085636856368564,
      "step": 2807,
      "training_loss": 6.037990093231201
    },
    {
      "epoch": 0.6085636856368564,
      "step": 2807,
      "training_loss": 6.151299953460693
    },
    {
      "epoch": 0.608780487804878,
      "grad_norm": 15.608377456665039,
      "learning_rate": 1e-05,
      "loss": 6.0768,
      "step": 2808
    },
    {
      "epoch": 0.608780487804878,
      "step": 2808,
      "training_loss": 5.782911777496338
    },
    {
      "epoch": 0.608780487804878,
      "step": 2808,
      "training_loss": 7.027900218963623
    },
    {
      "epoch": 0.608780487804878,
      "step": 2808,
      "training_loss": 6.6548261642456055
    },
    {
      "epoch": 0.608780487804878,
      "step": 2808,
      "training_loss": 6.946772575378418
    },
    {
      "epoch": 0.6089972899728997,
      "step": 2809,
      "training_loss": 7.273226737976074
    },
    {
      "epoch": 0.6089972899728997,
      "step": 2809,
      "training_loss": 7.983987808227539
    },
    {
      "epoch": 0.6089972899728997,
      "step": 2809,
      "training_loss": 6.535123348236084
    },
    {
      "epoch": 0.6089972899728997,
      "step": 2809,
      "training_loss": 2.7465832233428955
    },
    {
      "epoch": 0.6092140921409214,
      "step": 2810,
      "training_loss": 7.9462571144104
    },
    {
      "epoch": 0.6092140921409214,
      "step": 2810,
      "training_loss": 7.3672614097595215
    },
    {
      "epoch": 0.6092140921409214,
      "step": 2810,
      "training_loss": 7.053872108459473
    },
    {
      "epoch": 0.6092140921409214,
      "step": 2810,
      "training_loss": 7.328551292419434
    },
    {
      "epoch": 0.609430894308943,
      "step": 2811,
      "training_loss": 4.897119045257568
    },
    {
      "epoch": 0.609430894308943,
      "step": 2811,
      "training_loss": 3.9558005332946777
    },
    {
      "epoch": 0.609430894308943,
      "step": 2811,
      "training_loss": 7.114680290222168
    },
    {
      "epoch": 0.609430894308943,
      "step": 2811,
      "training_loss": 7.251180171966553
    },
    {
      "epoch": 0.6096476964769648,
      "grad_norm": 23.118059158325195,
      "learning_rate": 1e-05,
      "loss": 6.4916,
      "step": 2812
    },
    {
      "epoch": 0.6096476964769648,
      "step": 2812,
      "training_loss": 5.396290302276611
    },
    {
      "epoch": 0.6096476964769648,
      "step": 2812,
      "training_loss": 7.682957172393799
    },
    {
      "epoch": 0.6096476964769648,
      "step": 2812,
      "training_loss": 6.194910049438477
    },
    {
      "epoch": 0.6096476964769648,
      "step": 2812,
      "training_loss": 7.253896236419678
    },
    {
      "epoch": 0.6098644986449865,
      "step": 2813,
      "training_loss": 5.385228157043457
    },
    {
      "epoch": 0.6098644986449865,
      "step": 2813,
      "training_loss": 6.599537372589111
    },
    {
      "epoch": 0.6098644986449865,
      "step": 2813,
      "training_loss": 7.095130443572998
    },
    {
      "epoch": 0.6098644986449865,
      "step": 2813,
      "training_loss": 5.816115856170654
    },
    {
      "epoch": 0.6100813008130082,
      "step": 2814,
      "training_loss": 6.819711685180664
    },
    {
      "epoch": 0.6100813008130082,
      "step": 2814,
      "training_loss": 7.118364334106445
    },
    {
      "epoch": 0.6100813008130082,
      "step": 2814,
      "training_loss": 6.278953552246094
    },
    {
      "epoch": 0.6100813008130082,
      "step": 2814,
      "training_loss": 3.119603395462036
    },
    {
      "epoch": 0.6102981029810298,
      "step": 2815,
      "training_loss": 5.21828031539917
    },
    {
      "epoch": 0.6102981029810298,
      "step": 2815,
      "training_loss": 6.010195732116699
    },
    {
      "epoch": 0.6102981029810298,
      "step": 2815,
      "training_loss": 6.3864054679870605
    },
    {
      "epoch": 0.6102981029810298,
      "step": 2815,
      "training_loss": 6.357226371765137
    },
    {
      "epoch": 0.6105149051490515,
      "grad_norm": 25.26158332824707,
      "learning_rate": 1e-05,
      "loss": 6.1708,
      "step": 2816
    },
    {
      "epoch": 0.6105149051490515,
      "step": 2816,
      "training_loss": 6.117700576782227
    },
    {
      "epoch": 0.6105149051490515,
      "step": 2816,
      "training_loss": 6.773470401763916
    },
    {
      "epoch": 0.6105149051490515,
      "step": 2816,
      "training_loss": 6.602909564971924
    },
    {
      "epoch": 0.6105149051490515,
      "step": 2816,
      "training_loss": 7.1121649742126465
    },
    {
      "epoch": 0.6107317073170732,
      "step": 2817,
      "training_loss": 5.603762626647949
    },
    {
      "epoch": 0.6107317073170732,
      "step": 2817,
      "training_loss": 4.74941873550415
    },
    {
      "epoch": 0.6107317073170732,
      "step": 2817,
      "training_loss": 6.63707971572876
    },
    {
      "epoch": 0.6107317073170732,
      "step": 2817,
      "training_loss": 7.269254207611084
    },
    {
      "epoch": 0.6109485094850948,
      "step": 2818,
      "training_loss": 5.977382183074951
    },
    {
      "epoch": 0.6109485094850948,
      "step": 2818,
      "training_loss": 4.744716644287109
    },
    {
      "epoch": 0.6109485094850948,
      "step": 2818,
      "training_loss": 6.785640716552734
    },
    {
      "epoch": 0.6109485094850948,
      "step": 2818,
      "training_loss": 7.2890849113464355
    },
    {
      "epoch": 0.6111653116531165,
      "step": 2819,
      "training_loss": 6.332137584686279
    },
    {
      "epoch": 0.6111653116531165,
      "step": 2819,
      "training_loss": 6.2337822914123535
    },
    {
      "epoch": 0.6111653116531165,
      "step": 2819,
      "training_loss": 7.115598201751709
    },
    {
      "epoch": 0.6111653116531165,
      "step": 2819,
      "training_loss": 5.784185409545898
    },
    {
      "epoch": 0.6113821138211382,
      "grad_norm": 20.50267791748047,
      "learning_rate": 1e-05,
      "loss": 6.3205,
      "step": 2820
    },
    {
      "epoch": 0.6113821138211382,
      "step": 2820,
      "training_loss": 6.706496715545654
    },
    {
      "epoch": 0.6113821138211382,
      "step": 2820,
      "training_loss": 3.4862067699432373
    },
    {
      "epoch": 0.6113821138211382,
      "step": 2820,
      "training_loss": 7.265129089355469
    },
    {
      "epoch": 0.6113821138211382,
      "step": 2820,
      "training_loss": 6.526439666748047
    },
    {
      "epoch": 0.6115989159891599,
      "step": 2821,
      "training_loss": 7.502368927001953
    },
    {
      "epoch": 0.6115989159891599,
      "step": 2821,
      "training_loss": 6.434082508087158
    },
    {
      "epoch": 0.6115989159891599,
      "step": 2821,
      "training_loss": 4.559354305267334
    },
    {
      "epoch": 0.6115989159891599,
      "step": 2821,
      "training_loss": 5.870641231536865
    },
    {
      "epoch": 0.6118157181571816,
      "step": 2822,
      "training_loss": 5.627711296081543
    },
    {
      "epoch": 0.6118157181571816,
      "step": 2822,
      "training_loss": 7.115940570831299
    },
    {
      "epoch": 0.6118157181571816,
      "step": 2822,
      "training_loss": 5.913417816162109
    },
    {
      "epoch": 0.6118157181571816,
      "step": 2822,
      "training_loss": 6.463216781616211
    },
    {
      "epoch": 0.6120325203252033,
      "step": 2823,
      "training_loss": 5.6782145500183105
    },
    {
      "epoch": 0.6120325203252033,
      "step": 2823,
      "training_loss": 6.3347320556640625
    },
    {
      "epoch": 0.6120325203252033,
      "step": 2823,
      "training_loss": 6.529919624328613
    },
    {
      "epoch": 0.6120325203252033,
      "step": 2823,
      "training_loss": 6.561432361602783
    },
    {
      "epoch": 0.612249322493225,
      "grad_norm": 28.868741989135742,
      "learning_rate": 1e-05,
      "loss": 6.161,
      "step": 2824
    },
    {
      "epoch": 0.612249322493225,
      "step": 2824,
      "training_loss": 7.603122234344482
    },
    {
      "epoch": 0.612249322493225,
      "step": 2824,
      "training_loss": 6.765291690826416
    },
    {
      "epoch": 0.612249322493225,
      "step": 2824,
      "training_loss": 5.7118330001831055
    },
    {
      "epoch": 0.612249322493225,
      "step": 2824,
      "training_loss": 5.283571720123291
    },
    {
      "epoch": 0.6124661246612466,
      "step": 2825,
      "training_loss": 7.233827114105225
    },
    {
      "epoch": 0.6124661246612466,
      "step": 2825,
      "training_loss": 3.292057752609253
    },
    {
      "epoch": 0.6124661246612466,
      "step": 2825,
      "training_loss": 7.4165472984313965
    },
    {
      "epoch": 0.6124661246612466,
      "step": 2825,
      "training_loss": 4.584956645965576
    },
    {
      "epoch": 0.6126829268292683,
      "step": 2826,
      "training_loss": 6.294156074523926
    },
    {
      "epoch": 0.6126829268292683,
      "step": 2826,
      "training_loss": 7.397929668426514
    },
    {
      "epoch": 0.6126829268292683,
      "step": 2826,
      "training_loss": 6.620914936065674
    },
    {
      "epoch": 0.6126829268292683,
      "step": 2826,
      "training_loss": 5.838803768157959
    },
    {
      "epoch": 0.61289972899729,
      "step": 2827,
      "training_loss": 7.089359283447266
    },
    {
      "epoch": 0.61289972899729,
      "step": 2827,
      "training_loss": 5.466089725494385
    },
    {
      "epoch": 0.61289972899729,
      "step": 2827,
      "training_loss": 6.323578834533691
    },
    {
      "epoch": 0.61289972899729,
      "step": 2827,
      "training_loss": 5.5513529777526855
    },
    {
      "epoch": 0.6131165311653116,
      "grad_norm": 15.541616439819336,
      "learning_rate": 1e-05,
      "loss": 6.1546,
      "step": 2828
    },
    {
      "epoch": 0.6131165311653116,
      "step": 2828,
      "training_loss": 9.575730323791504
    },
    {
      "epoch": 0.6131165311653116,
      "step": 2828,
      "training_loss": 6.145612716674805
    },
    {
      "epoch": 0.6131165311653116,
      "step": 2828,
      "training_loss": 6.617954730987549
    },
    {
      "epoch": 0.6131165311653116,
      "step": 2828,
      "training_loss": 6.205503463745117
    },
    {
      "epoch": 0.6133333333333333,
      "step": 2829,
      "training_loss": 7.012654781341553
    },
    {
      "epoch": 0.6133333333333333,
      "step": 2829,
      "training_loss": 5.364075183868408
    },
    {
      "epoch": 0.6133333333333333,
      "step": 2829,
      "training_loss": 7.192417144775391
    },
    {
      "epoch": 0.6133333333333333,
      "step": 2829,
      "training_loss": 6.049344062805176
    },
    {
      "epoch": 0.6135501355013551,
      "step": 2830,
      "training_loss": 7.256688594818115
    },
    {
      "epoch": 0.6135501355013551,
      "step": 2830,
      "training_loss": 4.109737873077393
    },
    {
      "epoch": 0.6135501355013551,
      "step": 2830,
      "training_loss": 6.492913246154785
    },
    {
      "epoch": 0.6135501355013551,
      "step": 2830,
      "training_loss": 7.569063663482666
    },
    {
      "epoch": 0.6137669376693767,
      "step": 2831,
      "training_loss": 7.462811470031738
    },
    {
      "epoch": 0.6137669376693767,
      "step": 2831,
      "training_loss": 6.541084289550781
    },
    {
      "epoch": 0.6137669376693767,
      "step": 2831,
      "training_loss": 5.5816731452941895
    },
    {
      "epoch": 0.6137669376693767,
      "step": 2831,
      "training_loss": 6.399957656860352
    },
    {
      "epoch": 0.6139837398373984,
      "grad_norm": 13.242732048034668,
      "learning_rate": 1e-05,
      "loss": 6.5986,
      "step": 2832
    },
    {
      "epoch": 0.6139837398373984,
      "step": 2832,
      "training_loss": 3.9337968826293945
    },
    {
      "epoch": 0.6139837398373984,
      "step": 2832,
      "training_loss": 7.696165084838867
    },
    {
      "epoch": 0.6139837398373984,
      "step": 2832,
      "training_loss": 6.542984962463379
    },
    {
      "epoch": 0.6139837398373984,
      "step": 2832,
      "training_loss": 3.9962780475616455
    },
    {
      "epoch": 0.6142005420054201,
      "step": 2833,
      "training_loss": 6.572658061981201
    },
    {
      "epoch": 0.6142005420054201,
      "step": 2833,
      "training_loss": 5.543704986572266
    },
    {
      "epoch": 0.6142005420054201,
      "step": 2833,
      "training_loss": 6.191928386688232
    },
    {
      "epoch": 0.6142005420054201,
      "step": 2833,
      "training_loss": 7.560118675231934
    },
    {
      "epoch": 0.6144173441734417,
      "step": 2834,
      "training_loss": 6.864032745361328
    },
    {
      "epoch": 0.6144173441734417,
      "step": 2834,
      "training_loss": 6.790335178375244
    },
    {
      "epoch": 0.6144173441734417,
      "step": 2834,
      "training_loss": 6.80965518951416
    },
    {
      "epoch": 0.6144173441734417,
      "step": 2834,
      "training_loss": 5.185157299041748
    },
    {
      "epoch": 0.6146341463414634,
      "step": 2835,
      "training_loss": 4.640106678009033
    },
    {
      "epoch": 0.6146341463414634,
      "step": 2835,
      "training_loss": 6.745565891265869
    },
    {
      "epoch": 0.6146341463414634,
      "step": 2835,
      "training_loss": 7.1867899894714355
    },
    {
      "epoch": 0.6146341463414634,
      "step": 2835,
      "training_loss": 6.811888694763184
    },
    {
      "epoch": 0.6148509485094851,
      "grad_norm": 17.764942169189453,
      "learning_rate": 1e-05,
      "loss": 6.1919,
      "step": 2836
    },
    {
      "epoch": 0.6148509485094851,
      "step": 2836,
      "training_loss": 6.1518754959106445
    },
    {
      "epoch": 0.6148509485094851,
      "step": 2836,
      "training_loss": 7.0495076179504395
    },
    {
      "epoch": 0.6148509485094851,
      "step": 2836,
      "training_loss": 7.382203578948975
    },
    {
      "epoch": 0.6148509485094851,
      "step": 2836,
      "training_loss": 4.998604774475098
    },
    {
      "epoch": 0.6150677506775067,
      "step": 2837,
      "training_loss": 7.089890003204346
    },
    {
      "epoch": 0.6150677506775067,
      "step": 2837,
      "training_loss": 5.683357238769531
    },
    {
      "epoch": 0.6150677506775067,
      "step": 2837,
      "training_loss": 6.946008205413818
    },
    {
      "epoch": 0.6150677506775067,
      "step": 2837,
      "training_loss": 6.6155219078063965
    },
    {
      "epoch": 0.6152845528455284,
      "step": 2838,
      "training_loss": 5.613877773284912
    },
    {
      "epoch": 0.6152845528455284,
      "step": 2838,
      "training_loss": 5.099559783935547
    },
    {
      "epoch": 0.6152845528455284,
      "step": 2838,
      "training_loss": 5.981268405914307
    },
    {
      "epoch": 0.6152845528455284,
      "step": 2838,
      "training_loss": 5.680643081665039
    },
    {
      "epoch": 0.6155013550135502,
      "step": 2839,
      "training_loss": 6.238219738006592
    },
    {
      "epoch": 0.6155013550135502,
      "step": 2839,
      "training_loss": 7.218203544616699
    },
    {
      "epoch": 0.6155013550135502,
      "step": 2839,
      "training_loss": 6.886377811431885
    },
    {
      "epoch": 0.6155013550135502,
      "step": 2839,
      "training_loss": 6.596998691558838
    },
    {
      "epoch": 0.6157181571815719,
      "grad_norm": 21.139009475708008,
      "learning_rate": 1e-05,
      "loss": 6.327,
      "step": 2840
    },
    {
      "epoch": 0.6157181571815719,
      "step": 2840,
      "training_loss": 6.412291049957275
    },
    {
      "epoch": 0.6157181571815719,
      "step": 2840,
      "training_loss": 3.258681297302246
    },
    {
      "epoch": 0.6157181571815719,
      "step": 2840,
      "training_loss": 7.380415439605713
    },
    {
      "epoch": 0.6157181571815719,
      "step": 2840,
      "training_loss": 6.132427215576172
    },
    {
      "epoch": 0.6159349593495935,
      "step": 2841,
      "training_loss": 4.7371602058410645
    },
    {
      "epoch": 0.6159349593495935,
      "step": 2841,
      "training_loss": 5.83276891708374
    },
    {
      "epoch": 0.6159349593495935,
      "step": 2841,
      "training_loss": 7.621274948120117
    },
    {
      "epoch": 0.6159349593495935,
      "step": 2841,
      "training_loss": 7.340392112731934
    },
    {
      "epoch": 0.6161517615176152,
      "step": 2842,
      "training_loss": 5.56791877746582
    },
    {
      "epoch": 0.6161517615176152,
      "step": 2842,
      "training_loss": 5.951103687286377
    },
    {
      "epoch": 0.6161517615176152,
      "step": 2842,
      "training_loss": 6.587831497192383
    },
    {
      "epoch": 0.6161517615176152,
      "step": 2842,
      "training_loss": 5.341156005859375
    },
    {
      "epoch": 0.6163685636856369,
      "step": 2843,
      "training_loss": 6.249363422393799
    },
    {
      "epoch": 0.6163685636856369,
      "step": 2843,
      "training_loss": 7.422819137573242
    },
    {
      "epoch": 0.6163685636856369,
      "step": 2843,
      "training_loss": 6.698388576507568
    },
    {
      "epoch": 0.6163685636856369,
      "step": 2843,
      "training_loss": 6.412133693695068
    },
    {
      "epoch": 0.6165853658536585,
      "grad_norm": 12.13101577758789,
      "learning_rate": 1e-05,
      "loss": 6.1841,
      "step": 2844
    },
    {
      "epoch": 0.6165853658536585,
      "step": 2844,
      "training_loss": 6.418849468231201
    },
    {
      "epoch": 0.6165853658536585,
      "step": 2844,
      "training_loss": 6.955382347106934
    },
    {
      "epoch": 0.6165853658536585,
      "step": 2844,
      "training_loss": 6.389172554016113
    },
    {
      "epoch": 0.6165853658536585,
      "step": 2844,
      "training_loss": 6.877741813659668
    },
    {
      "epoch": 0.6168021680216802,
      "step": 2845,
      "training_loss": 7.950345516204834
    },
    {
      "epoch": 0.6168021680216802,
      "step": 2845,
      "training_loss": 3.088062047958374
    },
    {
      "epoch": 0.6168021680216802,
      "step": 2845,
      "training_loss": 6.314653396606445
    },
    {
      "epoch": 0.6168021680216802,
      "step": 2845,
      "training_loss": 5.489130020141602
    },
    {
      "epoch": 0.6170189701897019,
      "step": 2846,
      "training_loss": 5.7710418701171875
    },
    {
      "epoch": 0.6170189701897019,
      "step": 2846,
      "training_loss": 6.604369163513184
    },
    {
      "epoch": 0.6170189701897019,
      "step": 2846,
      "training_loss": 4.893487930297852
    },
    {
      "epoch": 0.6170189701897019,
      "step": 2846,
      "training_loss": 6.35668420791626
    },
    {
      "epoch": 0.6172357723577235,
      "step": 2847,
      "training_loss": 7.427881717681885
    },
    {
      "epoch": 0.6172357723577235,
      "step": 2847,
      "training_loss": 6.048182487487793
    },
    {
      "epoch": 0.6172357723577235,
      "step": 2847,
      "training_loss": 6.394896030426025
    },
    {
      "epoch": 0.6172357723577235,
      "step": 2847,
      "training_loss": 7.230280876159668
    },
    {
      "epoch": 0.6174525745257453,
      "grad_norm": 18.62519645690918,
      "learning_rate": 1e-05,
      "loss": 6.2631,
      "step": 2848
    },
    {
      "epoch": 0.6174525745257453,
      "step": 2848,
      "training_loss": 6.35117769241333
    },
    {
      "epoch": 0.6174525745257453,
      "step": 2848,
      "training_loss": 6.273317337036133
    },
    {
      "epoch": 0.6174525745257453,
      "step": 2848,
      "training_loss": 3.494931936264038
    },
    {
      "epoch": 0.6174525745257453,
      "step": 2848,
      "training_loss": 6.940867900848389
    },
    {
      "epoch": 0.617669376693767,
      "step": 2849,
      "training_loss": 5.537911891937256
    },
    {
      "epoch": 0.617669376693767,
      "step": 2849,
      "training_loss": 7.980046272277832
    },
    {
      "epoch": 0.617669376693767,
      "step": 2849,
      "training_loss": 4.706040382385254
    },
    {
      "epoch": 0.617669376693767,
      "step": 2849,
      "training_loss": 4.458642482757568
    },
    {
      "epoch": 0.6178861788617886,
      "step": 2850,
      "training_loss": 5.798556804656982
    },
    {
      "epoch": 0.6178861788617886,
      "step": 2850,
      "training_loss": 6.029727458953857
    },
    {
      "epoch": 0.6178861788617886,
      "step": 2850,
      "training_loss": 2.5670006275177
    },
    {
      "epoch": 0.6178861788617886,
      "step": 2850,
      "training_loss": 5.60797643661499
    },
    {
      "epoch": 0.6181029810298103,
      "step": 2851,
      "training_loss": 6.92606258392334
    },
    {
      "epoch": 0.6181029810298103,
      "step": 2851,
      "training_loss": 5.531458854675293
    },
    {
      "epoch": 0.6181029810298103,
      "step": 2851,
      "training_loss": 6.428940773010254
    },
    {
      "epoch": 0.6181029810298103,
      "step": 2851,
      "training_loss": 6.67852783203125
    },
    {
      "epoch": 0.618319783197832,
      "grad_norm": 21.801132202148438,
      "learning_rate": 1e-05,
      "loss": 5.7069,
      "step": 2852
    },
    {
      "epoch": 0.618319783197832,
      "step": 2852,
      "training_loss": 7.0814008712768555
    },
    {
      "epoch": 0.618319783197832,
      "step": 2852,
      "training_loss": 5.754230499267578
    },
    {
      "epoch": 0.618319783197832,
      "step": 2852,
      "training_loss": 6.873329162597656
    },
    {
      "epoch": 0.618319783197832,
      "step": 2852,
      "training_loss": 8.041839599609375
    },
    {
      "epoch": 0.6185365853658537,
      "step": 2853,
      "training_loss": 5.3595499992370605
    },
    {
      "epoch": 0.6185365853658537,
      "step": 2853,
      "training_loss": 7.810647964477539
    },
    {
      "epoch": 0.6185365853658537,
      "step": 2853,
      "training_loss": 5.496052265167236
    },
    {
      "epoch": 0.6185365853658537,
      "step": 2853,
      "training_loss": 7.761608600616455
    },
    {
      "epoch": 0.6187533875338753,
      "step": 2854,
      "training_loss": 5.9633684158325195
    },
    {
      "epoch": 0.6187533875338753,
      "step": 2854,
      "training_loss": 4.838857650756836
    },
    {
      "epoch": 0.6187533875338753,
      "step": 2854,
      "training_loss": 6.296942710876465
    },
    {
      "epoch": 0.6187533875338753,
      "step": 2854,
      "training_loss": 6.659501075744629
    },
    {
      "epoch": 0.618970189701897,
      "step": 2855,
      "training_loss": 6.9582600593566895
    },
    {
      "epoch": 0.618970189701897,
      "step": 2855,
      "training_loss": 8.228670120239258
    },
    {
      "epoch": 0.618970189701897,
      "step": 2855,
      "training_loss": 7.176152229309082
    },
    {
      "epoch": 0.618970189701897,
      "step": 2855,
      "training_loss": 7.593319416046143
    },
    {
      "epoch": 0.6191869918699187,
      "grad_norm": 19.363101959228516,
      "learning_rate": 1e-05,
      "loss": 6.7434,
      "step": 2856
    },
    {
      "epoch": 0.6191869918699187,
      "step": 2856,
      "training_loss": 6.636119842529297
    },
    {
      "epoch": 0.6191869918699187,
      "step": 2856,
      "training_loss": 7.036931991577148
    },
    {
      "epoch": 0.6191869918699187,
      "step": 2856,
      "training_loss": 3.222675085067749
    },
    {
      "epoch": 0.6191869918699187,
      "step": 2856,
      "training_loss": 7.746746063232422
    },
    {
      "epoch": 0.6194037940379403,
      "step": 2857,
      "training_loss": 4.052536964416504
    },
    {
      "epoch": 0.6194037940379403,
      "step": 2857,
      "training_loss": 7.905055999755859
    },
    {
      "epoch": 0.6194037940379403,
      "step": 2857,
      "training_loss": 6.63625955581665
    },
    {
      "epoch": 0.6194037940379403,
      "step": 2857,
      "training_loss": 6.602204322814941
    },
    {
      "epoch": 0.6196205962059621,
      "step": 2858,
      "training_loss": 6.9252400398254395
    },
    {
      "epoch": 0.6196205962059621,
      "step": 2858,
      "training_loss": 4.467634677886963
    },
    {
      "epoch": 0.6196205962059621,
      "step": 2858,
      "training_loss": 5.838080883026123
    },
    {
      "epoch": 0.6196205962059621,
      "step": 2858,
      "training_loss": 4.807123184204102
    },
    {
      "epoch": 0.6198373983739838,
      "step": 2859,
      "training_loss": 6.734661102294922
    },
    {
      "epoch": 0.6198373983739838,
      "step": 2859,
      "training_loss": 5.350198268890381
    },
    {
      "epoch": 0.6198373983739838,
      "step": 2859,
      "training_loss": 6.527663230895996
    },
    {
      "epoch": 0.6198373983739838,
      "step": 2859,
      "training_loss": 6.1041669845581055
    },
    {
      "epoch": 0.6200542005420054,
      "grad_norm": 13.827751159667969,
      "learning_rate": 1e-05,
      "loss": 6.0371,
      "step": 2860
    },
    {
      "epoch": 0.6200542005420054,
      "step": 2860,
      "training_loss": 5.079111099243164
    },
    {
      "epoch": 0.6200542005420054,
      "step": 2860,
      "training_loss": 6.437007904052734
    },
    {
      "epoch": 0.6200542005420054,
      "step": 2860,
      "training_loss": 5.747529983520508
    },
    {
      "epoch": 0.6200542005420054,
      "step": 2860,
      "training_loss": 8.450109481811523
    },
    {
      "epoch": 0.6202710027100271,
      "step": 2861,
      "training_loss": 5.126181602478027
    },
    {
      "epoch": 0.6202710027100271,
      "step": 2861,
      "training_loss": 7.214170932769775
    },
    {
      "epoch": 0.6202710027100271,
      "step": 2861,
      "training_loss": 7.295165061950684
    },
    {
      "epoch": 0.6202710027100271,
      "step": 2861,
      "training_loss": 6.509631156921387
    },
    {
      "epoch": 0.6204878048780488,
      "step": 2862,
      "training_loss": 5.363228797912598
    },
    {
      "epoch": 0.6204878048780488,
      "step": 2862,
      "training_loss": 7.140798091888428
    },
    {
      "epoch": 0.6204878048780488,
      "step": 2862,
      "training_loss": 5.79364013671875
    },
    {
      "epoch": 0.6204878048780488,
      "step": 2862,
      "training_loss": 5.631202697753906
    },
    {
      "epoch": 0.6207046070460704,
      "step": 2863,
      "training_loss": 6.342695236206055
    },
    {
      "epoch": 0.6207046070460704,
      "step": 2863,
      "training_loss": 5.232080459594727
    },
    {
      "epoch": 0.6207046070460704,
      "step": 2863,
      "training_loss": 6.216503143310547
    },
    {
      "epoch": 0.6207046070460704,
      "step": 2863,
      "training_loss": 7.483750820159912
    },
    {
      "epoch": 0.6209214092140921,
      "grad_norm": 16.087509155273438,
      "learning_rate": 1e-05,
      "loss": 6.3164,
      "step": 2864
    },
    {
      "epoch": 0.6209214092140921,
      "step": 2864,
      "training_loss": 6.217682361602783
    },
    {
      "epoch": 0.6209214092140921,
      "step": 2864,
      "training_loss": 7.210452556610107
    },
    {
      "epoch": 0.6209214092140921,
      "step": 2864,
      "training_loss": 6.966888904571533
    },
    {
      "epoch": 0.6209214092140921,
      "step": 2864,
      "training_loss": 5.700200080871582
    },
    {
      "epoch": 0.6211382113821138,
      "step": 2865,
      "training_loss": 7.3050642013549805
    },
    {
      "epoch": 0.6211382113821138,
      "step": 2865,
      "training_loss": 6.527063846588135
    },
    {
      "epoch": 0.6211382113821138,
      "step": 2865,
      "training_loss": 3.870100498199463
    },
    {
      "epoch": 0.6211382113821138,
      "step": 2865,
      "training_loss": 7.120697975158691
    },
    {
      "epoch": 0.6213550135501354,
      "step": 2866,
      "training_loss": 6.929024696350098
    },
    {
      "epoch": 0.6213550135501354,
      "step": 2866,
      "training_loss": 3.803962469100952
    },
    {
      "epoch": 0.6213550135501354,
      "step": 2866,
      "training_loss": 8.88178539276123
    },
    {
      "epoch": 0.6213550135501354,
      "step": 2866,
      "training_loss": 6.8624091148376465
    },
    {
      "epoch": 0.6215718157181572,
      "step": 2867,
      "training_loss": 5.665393829345703
    },
    {
      "epoch": 0.6215718157181572,
      "step": 2867,
      "training_loss": 8.269733428955078
    },
    {
      "epoch": 0.6215718157181572,
      "step": 2867,
      "training_loss": 6.805893421173096
    },
    {
      "epoch": 0.6215718157181572,
      "step": 2867,
      "training_loss": 5.193666934967041
    },
    {
      "epoch": 0.6217886178861789,
      "grad_norm": 44.749874114990234,
      "learning_rate": 1e-05,
      "loss": 6.4581,
      "step": 2868
    },
    {
      "epoch": 0.6217886178861789,
      "step": 2868,
      "training_loss": 7.449403762817383
    },
    {
      "epoch": 0.6217886178861789,
      "step": 2868,
      "training_loss": 6.715780735015869
    },
    {
      "epoch": 0.6217886178861789,
      "step": 2868,
      "training_loss": 5.968185901641846
    },
    {
      "epoch": 0.6217886178861789,
      "step": 2868,
      "training_loss": 7.662889003753662
    },
    {
      "epoch": 0.6220054200542006,
      "step": 2869,
      "training_loss": 6.260402202606201
    },
    {
      "epoch": 0.6220054200542006,
      "step": 2869,
      "training_loss": 6.905296802520752
    },
    {
      "epoch": 0.6220054200542006,
      "step": 2869,
      "training_loss": 6.226536273956299
    },
    {
      "epoch": 0.6220054200542006,
      "step": 2869,
      "training_loss": 7.923243045806885
    },
    {
      "epoch": 0.6222222222222222,
      "step": 2870,
      "training_loss": 5.9080634117126465
    },
    {
      "epoch": 0.6222222222222222,
      "step": 2870,
      "training_loss": 6.3835296630859375
    },
    {
      "epoch": 0.6222222222222222,
      "step": 2870,
      "training_loss": 5.671490669250488
    },
    {
      "epoch": 0.6222222222222222,
      "step": 2870,
      "training_loss": 6.90078067779541
    },
    {
      "epoch": 0.6224390243902439,
      "step": 2871,
      "training_loss": 5.9849982261657715
    },
    {
      "epoch": 0.6224390243902439,
      "step": 2871,
      "training_loss": 5.651019096374512
    },
    {
      "epoch": 0.6224390243902439,
      "step": 2871,
      "training_loss": 3.101867914199829
    },
    {
      "epoch": 0.6224390243902439,
      "step": 2871,
      "training_loss": 5.559950351715088
    },
    {
      "epoch": 0.6226558265582656,
      "grad_norm": 24.81173324584961,
      "learning_rate": 1e-05,
      "loss": 6.2671,
      "step": 2872
    },
    {
      "epoch": 0.6226558265582656,
      "step": 2872,
      "training_loss": 6.284886837005615
    },
    {
      "epoch": 0.6226558265582656,
      "step": 2872,
      "training_loss": 6.091301441192627
    },
    {
      "epoch": 0.6226558265582656,
      "step": 2872,
      "training_loss": 4.913340091705322
    },
    {
      "epoch": 0.6226558265582656,
      "step": 2872,
      "training_loss": 3.5229134559631348
    },
    {
      "epoch": 0.6228726287262872,
      "step": 2873,
      "training_loss": 4.5474958419799805
    },
    {
      "epoch": 0.6228726287262872,
      "step": 2873,
      "training_loss": 7.158149242401123
    },
    {
      "epoch": 0.6228726287262872,
      "step": 2873,
      "training_loss": 5.840888977050781
    },
    {
      "epoch": 0.6228726287262872,
      "step": 2873,
      "training_loss": 6.567196369171143
    },
    {
      "epoch": 0.6230894308943089,
      "step": 2874,
      "training_loss": 7.4904255867004395
    },
    {
      "epoch": 0.6230894308943089,
      "step": 2874,
      "training_loss": 6.211112022399902
    },
    {
      "epoch": 0.6230894308943089,
      "step": 2874,
      "training_loss": 6.401231288909912
    },
    {
      "epoch": 0.6230894308943089,
      "step": 2874,
      "training_loss": 7.130781173706055
    },
    {
      "epoch": 0.6233062330623306,
      "step": 2875,
      "training_loss": 6.890207767486572
    },
    {
      "epoch": 0.6233062330623306,
      "step": 2875,
      "training_loss": 7.37096643447876
    },
    {
      "epoch": 0.6233062330623306,
      "step": 2875,
      "training_loss": 6.618775367736816
    },
    {
      "epoch": 0.6233062330623306,
      "step": 2875,
      "training_loss": 7.100456714630127
    },
    {
      "epoch": 0.6235230352303524,
      "grad_norm": 19.023639678955078,
      "learning_rate": 1e-05,
      "loss": 6.2588,
      "step": 2876
    },
    {
      "epoch": 0.6235230352303524,
      "step": 2876,
      "training_loss": 6.176700115203857
    },
    {
      "epoch": 0.6235230352303524,
      "step": 2876,
      "training_loss": 7.766533851623535
    },
    {
      "epoch": 0.6235230352303524,
      "step": 2876,
      "training_loss": 6.963561058044434
    },
    {
      "epoch": 0.6235230352303524,
      "step": 2876,
      "training_loss": 5.722313404083252
    },
    {
      "epoch": 0.623739837398374,
      "step": 2877,
      "training_loss": 5.657782077789307
    },
    {
      "epoch": 0.623739837398374,
      "step": 2877,
      "training_loss": 6.192440986633301
    },
    {
      "epoch": 0.623739837398374,
      "step": 2877,
      "training_loss": 4.525056838989258
    },
    {
      "epoch": 0.623739837398374,
      "step": 2877,
      "training_loss": 7.989416122436523
    },
    {
      "epoch": 0.6239566395663957,
      "step": 2878,
      "training_loss": 5.228648662567139
    },
    {
      "epoch": 0.6239566395663957,
      "step": 2878,
      "training_loss": 4.915276050567627
    },
    {
      "epoch": 0.6239566395663957,
      "step": 2878,
      "training_loss": 5.379417896270752
    },
    {
      "epoch": 0.6239566395663957,
      "step": 2878,
      "training_loss": 7.495884895324707
    },
    {
      "epoch": 0.6241734417344174,
      "step": 2879,
      "training_loss": 7.037163734436035
    },
    {
      "epoch": 0.6241734417344174,
      "step": 2879,
      "training_loss": 3.512223958969116
    },
    {
      "epoch": 0.6241734417344174,
      "step": 2879,
      "training_loss": 6.1016011238098145
    },
    {
      "epoch": 0.6241734417344174,
      "step": 2879,
      "training_loss": 5.293792724609375
    },
    {
      "epoch": 0.624390243902439,
      "grad_norm": 16.81956672668457,
      "learning_rate": 1e-05,
      "loss": 5.9974,
      "step": 2880
    },
    {
      "epoch": 0.624390243902439,
      "step": 2880,
      "training_loss": 5.893326759338379
    },
    {
      "epoch": 0.624390243902439,
      "step": 2880,
      "training_loss": 6.157253742218018
    },
    {
      "epoch": 0.624390243902439,
      "step": 2880,
      "training_loss": 6.447628021240234
    },
    {
      "epoch": 0.624390243902439,
      "step": 2880,
      "training_loss": 6.030176162719727
    },
    {
      "epoch": 0.6246070460704607,
      "step": 2881,
      "training_loss": 6.527561664581299
    },
    {
      "epoch": 0.6246070460704607,
      "step": 2881,
      "training_loss": 4.7587995529174805
    },
    {
      "epoch": 0.6246070460704607,
      "step": 2881,
      "training_loss": 5.592088222503662
    },
    {
      "epoch": 0.6246070460704607,
      "step": 2881,
      "training_loss": 5.201045989990234
    },
    {
      "epoch": 0.6248238482384824,
      "step": 2882,
      "training_loss": 4.198521614074707
    },
    {
      "epoch": 0.6248238482384824,
      "step": 2882,
      "training_loss": 6.156016826629639
    },
    {
      "epoch": 0.6248238482384824,
      "step": 2882,
      "training_loss": 5.939449310302734
    },
    {
      "epoch": 0.6248238482384824,
      "step": 2882,
      "training_loss": 6.934446334838867
    },
    {
      "epoch": 0.625040650406504,
      "step": 2883,
      "training_loss": 6.485993385314941
    },
    {
      "epoch": 0.625040650406504,
      "step": 2883,
      "training_loss": 7.160107135772705
    },
    {
      "epoch": 0.625040650406504,
      "step": 2883,
      "training_loss": 7.408198833465576
    },
    {
      "epoch": 0.625040650406504,
      "step": 2883,
      "training_loss": 6.668200969696045
    },
    {
      "epoch": 0.6252574525745257,
      "grad_norm": 17.786724090576172,
      "learning_rate": 1e-05,
      "loss": 6.0974,
      "step": 2884
    },
    {
      "epoch": 0.6252574525745257,
      "step": 2884,
      "training_loss": 6.749675750732422
    },
    {
      "epoch": 0.6252574525745257,
      "step": 2884,
      "training_loss": 6.5310869216918945
    },
    {
      "epoch": 0.6252574525745257,
      "step": 2884,
      "training_loss": 5.602478504180908
    },
    {
      "epoch": 0.6252574525745257,
      "step": 2884,
      "training_loss": 6.560290813446045
    },
    {
      "epoch": 0.6254742547425475,
      "step": 2885,
      "training_loss": 5.557805061340332
    },
    {
      "epoch": 0.6254742547425475,
      "step": 2885,
      "training_loss": 6.766905784606934
    },
    {
      "epoch": 0.6254742547425475,
      "step": 2885,
      "training_loss": 6.921717166900635
    },
    {
      "epoch": 0.6254742547425475,
      "step": 2885,
      "training_loss": 6.802130222320557
    },
    {
      "epoch": 0.6256910569105691,
      "step": 2886,
      "training_loss": 6.820505142211914
    },
    {
      "epoch": 0.6256910569105691,
      "step": 2886,
      "training_loss": 7.292292594909668
    },
    {
      "epoch": 0.6256910569105691,
      "step": 2886,
      "training_loss": 7.5660858154296875
    },
    {
      "epoch": 0.6256910569105691,
      "step": 2886,
      "training_loss": 6.127462863922119
    },
    {
      "epoch": 0.6259078590785908,
      "step": 2887,
      "training_loss": 6.600973606109619
    },
    {
      "epoch": 0.6259078590785908,
      "step": 2887,
      "training_loss": 6.452929496765137
    },
    {
      "epoch": 0.6259078590785908,
      "step": 2887,
      "training_loss": 5.695951461791992
    },
    {
      "epoch": 0.6259078590785908,
      "step": 2887,
      "training_loss": 5.715297222137451
    },
    {
      "epoch": 0.6261246612466125,
      "grad_norm": 16.29714584350586,
      "learning_rate": 1e-05,
      "loss": 6.4852,
      "step": 2888
    },
    {
      "epoch": 0.6261246612466125,
      "step": 2888,
      "training_loss": 7.678834915161133
    },
    {
      "epoch": 0.6261246612466125,
      "step": 2888,
      "training_loss": 6.861722946166992
    },
    {
      "epoch": 0.6261246612466125,
      "step": 2888,
      "training_loss": 6.173074245452881
    },
    {
      "epoch": 0.6261246612466125,
      "step": 2888,
      "training_loss": 7.433661460876465
    },
    {
      "epoch": 0.6263414634146341,
      "step": 2889,
      "training_loss": 6.809010982513428
    },
    {
      "epoch": 0.6263414634146341,
      "step": 2889,
      "training_loss": 6.4140305519104
    },
    {
      "epoch": 0.6263414634146341,
      "step": 2889,
      "training_loss": 6.8406982421875
    },
    {
      "epoch": 0.6263414634146341,
      "step": 2889,
      "training_loss": 6.940550804138184
    },
    {
      "epoch": 0.6265582655826558,
      "step": 2890,
      "training_loss": 5.159628391265869
    },
    {
      "epoch": 0.6265582655826558,
      "step": 2890,
      "training_loss": 2.870225667953491
    },
    {
      "epoch": 0.6265582655826558,
      "step": 2890,
      "training_loss": 5.7379255294799805
    },
    {
      "epoch": 0.6265582655826558,
      "step": 2890,
      "training_loss": 6.907956123352051
    },
    {
      "epoch": 0.6267750677506775,
      "step": 2891,
      "training_loss": 4.30143404006958
    },
    {
      "epoch": 0.6267750677506775,
      "step": 2891,
      "training_loss": 6.363926410675049
    },
    {
      "epoch": 0.6267750677506775,
      "step": 2891,
      "training_loss": 7.239030361175537
    },
    {
      "epoch": 0.6267750677506775,
      "step": 2891,
      "training_loss": 6.583351135253906
    },
    {
      "epoch": 0.6269918699186992,
      "grad_norm": 23.945037841796875,
      "learning_rate": 1e-05,
      "loss": 6.2697,
      "step": 2892
    },
    {
      "epoch": 0.6269918699186992,
      "step": 2892,
      "training_loss": 4.985729217529297
    },
    {
      "epoch": 0.6269918699186992,
      "step": 2892,
      "training_loss": 7.120950698852539
    },
    {
      "epoch": 0.6269918699186992,
      "step": 2892,
      "training_loss": 5.255458831787109
    },
    {
      "epoch": 0.6269918699186992,
      "step": 2892,
      "training_loss": 3.9679787158966064
    },
    {
      "epoch": 0.6272086720867208,
      "step": 2893,
      "training_loss": 5.355107307434082
    },
    {
      "epoch": 0.6272086720867208,
      "step": 2893,
      "training_loss": 4.971026420593262
    },
    {
      "epoch": 0.6272086720867208,
      "step": 2893,
      "training_loss": 7.014443874359131
    },
    {
      "epoch": 0.6272086720867208,
      "step": 2893,
      "training_loss": 5.8949055671691895
    },
    {
      "epoch": 0.6274254742547426,
      "step": 2894,
      "training_loss": 5.5044732093811035
    },
    {
      "epoch": 0.6274254742547426,
      "step": 2894,
      "training_loss": 3.79264497756958
    },
    {
      "epoch": 0.6274254742547426,
      "step": 2894,
      "training_loss": 2.765805959701538
    },
    {
      "epoch": 0.6274254742547426,
      "step": 2894,
      "training_loss": 2.8142194747924805
    },
    {
      "epoch": 0.6276422764227643,
      "step": 2895,
      "training_loss": 6.144412040710449
    },
    {
      "epoch": 0.6276422764227643,
      "step": 2895,
      "training_loss": 4.881195068359375
    },
    {
      "epoch": 0.6276422764227643,
      "step": 2895,
      "training_loss": 7.3846049308776855
    },
    {
      "epoch": 0.6276422764227643,
      "step": 2895,
      "training_loss": 2.9977822303771973
    },
    {
      "epoch": 0.6278590785907859,
      "grad_norm": 19.191715240478516,
      "learning_rate": 1e-05,
      "loss": 5.0532,
      "step": 2896
    },
    {
      "epoch": 0.6278590785907859,
      "step": 2896,
      "training_loss": 6.1121625900268555
    },
    {
      "epoch": 0.6278590785907859,
      "step": 2896,
      "training_loss": 6.6238112449646
    },
    {
      "epoch": 0.6278590785907859,
      "step": 2896,
      "training_loss": 5.087429523468018
    },
    {
      "epoch": 0.6278590785907859,
      "step": 2896,
      "training_loss": 7.497767448425293
    },
    {
      "epoch": 0.6280758807588076,
      "step": 2897,
      "training_loss": 6.585119724273682
    },
    {
      "epoch": 0.6280758807588076,
      "step": 2897,
      "training_loss": 6.03813362121582
    },
    {
      "epoch": 0.6280758807588076,
      "step": 2897,
      "training_loss": 4.915590286254883
    },
    {
      "epoch": 0.6280758807588076,
      "step": 2897,
      "training_loss": 5.880384922027588
    },
    {
      "epoch": 0.6282926829268293,
      "step": 2898,
      "training_loss": 6.040811538696289
    },
    {
      "epoch": 0.6282926829268293,
      "step": 2898,
      "training_loss": 7.58247184753418
    },
    {
      "epoch": 0.6282926829268293,
      "step": 2898,
      "training_loss": 4.364272594451904
    },
    {
      "epoch": 0.6282926829268293,
      "step": 2898,
      "training_loss": 5.641605854034424
    },
    {
      "epoch": 0.6285094850948509,
      "step": 2899,
      "training_loss": 6.841941833496094
    },
    {
      "epoch": 0.6285094850948509,
      "step": 2899,
      "training_loss": 7.387862682342529
    },
    {
      "epoch": 0.6285094850948509,
      "step": 2899,
      "training_loss": 7.807045936584473
    },
    {
      "epoch": 0.6285094850948509,
      "step": 2899,
      "training_loss": 6.496367931365967
    },
    {
      "epoch": 0.6287262872628726,
      "grad_norm": 18.66037940979004,
      "learning_rate": 1e-05,
      "loss": 6.3064,
      "step": 2900
    },
    {
      "epoch": 0.6287262872628726,
      "step": 2900,
      "training_loss": 5.958780288696289
    },
    {
      "epoch": 0.6287262872628726,
      "step": 2900,
      "training_loss": 7.0783891677856445
    },
    {
      "epoch": 0.6287262872628726,
      "step": 2900,
      "training_loss": 7.3025898933410645
    },
    {
      "epoch": 0.6287262872628726,
      "step": 2900,
      "training_loss": 6.211440563201904
    },
    {
      "epoch": 0.6289430894308943,
      "step": 2901,
      "training_loss": 7.086621284484863
    },
    {
      "epoch": 0.6289430894308943,
      "step": 2901,
      "training_loss": 4.7322187423706055
    },
    {
      "epoch": 0.6289430894308943,
      "step": 2901,
      "training_loss": 6.164611339569092
    },
    {
      "epoch": 0.6289430894308943,
      "step": 2901,
      "training_loss": 6.7135009765625
    },
    {
      "epoch": 0.6291598915989159,
      "step": 2902,
      "training_loss": 7.97857141494751
    },
    {
      "epoch": 0.6291598915989159,
      "step": 2902,
      "training_loss": 8.075933456420898
    },
    {
      "epoch": 0.6291598915989159,
      "step": 2902,
      "training_loss": 5.082414627075195
    },
    {
      "epoch": 0.6291598915989159,
      "step": 2902,
      "training_loss": 6.746668815612793
    },
    {
      "epoch": 0.6293766937669377,
      "step": 2903,
      "training_loss": 6.942654609680176
    },
    {
      "epoch": 0.6293766937669377,
      "step": 2903,
      "training_loss": 7.9709577560424805
    },
    {
      "epoch": 0.6293766937669377,
      "step": 2903,
      "training_loss": 5.638827323913574
    },
    {
      "epoch": 0.6293766937669377,
      "step": 2903,
      "training_loss": 7.361645698547363
    },
    {
      "epoch": 0.6295934959349594,
      "grad_norm": 23.0717716217041,
      "learning_rate": 1e-05,
      "loss": 6.6904,
      "step": 2904
    },
    {
      "epoch": 0.6295934959349594,
      "step": 2904,
      "training_loss": 6.033601760864258
    },
    {
      "epoch": 0.6295934959349594,
      "step": 2904,
      "training_loss": 7.739494323730469
    },
    {
      "epoch": 0.6295934959349594,
      "step": 2904,
      "training_loss": 2.8215131759643555
    },
    {
      "epoch": 0.6295934959349594,
      "step": 2904,
      "training_loss": 7.606616973876953
    },
    {
      "epoch": 0.6298102981029811,
      "step": 2905,
      "training_loss": 5.9704270362854
    },
    {
      "epoch": 0.6298102981029811,
      "step": 2905,
      "training_loss": 5.5269622802734375
    },
    {
      "epoch": 0.6298102981029811,
      "step": 2905,
      "training_loss": 6.912005424499512
    },
    {
      "epoch": 0.6298102981029811,
      "step": 2905,
      "training_loss": 8.020601272583008
    },
    {
      "epoch": 0.6300271002710027,
      "step": 2906,
      "training_loss": 6.7676262855529785
    },
    {
      "epoch": 0.6300271002710027,
      "step": 2906,
      "training_loss": 5.349185466766357
    },
    {
      "epoch": 0.6300271002710027,
      "step": 2906,
      "training_loss": 6.441771507263184
    },
    {
      "epoch": 0.6300271002710027,
      "step": 2906,
      "training_loss": 8.18780517578125
    },
    {
      "epoch": 0.6302439024390244,
      "step": 2907,
      "training_loss": 5.5563273429870605
    },
    {
      "epoch": 0.6302439024390244,
      "step": 2907,
      "training_loss": 5.77569580078125
    },
    {
      "epoch": 0.6302439024390244,
      "step": 2907,
      "training_loss": 5.489073276519775
    },
    {
      "epoch": 0.6302439024390244,
      "step": 2907,
      "training_loss": 5.928873062133789
    },
    {
      "epoch": 0.6304607046070461,
      "grad_norm": 22.5993709564209,
      "learning_rate": 1e-05,
      "loss": 6.258,
      "step": 2908
    },
    {
      "epoch": 0.6304607046070461,
      "step": 2908,
      "training_loss": 5.414287567138672
    },
    {
      "epoch": 0.6304607046070461,
      "step": 2908,
      "training_loss": 6.982552528381348
    },
    {
      "epoch": 0.6304607046070461,
      "step": 2908,
      "training_loss": 7.242225646972656
    },
    {
      "epoch": 0.6304607046070461,
      "step": 2908,
      "training_loss": 6.820844650268555
    },
    {
      "epoch": 0.6306775067750677,
      "step": 2909,
      "training_loss": 5.658997535705566
    },
    {
      "epoch": 0.6306775067750677,
      "step": 2909,
      "training_loss": 6.135530948638916
    },
    {
      "epoch": 0.6306775067750677,
      "step": 2909,
      "training_loss": 4.971889019012451
    },
    {
      "epoch": 0.6306775067750677,
      "step": 2909,
      "training_loss": 7.611413478851318
    },
    {
      "epoch": 0.6308943089430894,
      "step": 2910,
      "training_loss": 7.565998554229736
    },
    {
      "epoch": 0.6308943089430894,
      "step": 2910,
      "training_loss": 3.870396137237549
    },
    {
      "epoch": 0.6308943089430894,
      "step": 2910,
      "training_loss": 4.2067036628723145
    },
    {
      "epoch": 0.6308943089430894,
      "step": 2910,
      "training_loss": 7.975281715393066
    },
    {
      "epoch": 0.6311111111111111,
      "step": 2911,
      "training_loss": 6.725350379943848
    },
    {
      "epoch": 0.6311111111111111,
      "step": 2911,
      "training_loss": 6.7677001953125
    },
    {
      "epoch": 0.6311111111111111,
      "step": 2911,
      "training_loss": 5.6565117835998535
    },
    {
      "epoch": 0.6311111111111111,
      "step": 2911,
      "training_loss": 5.057735443115234
    },
    {
      "epoch": 0.6313279132791328,
      "grad_norm": 23.079326629638672,
      "learning_rate": 1e-05,
      "loss": 6.1665,
      "step": 2912
    },
    {
      "epoch": 0.6313279132791328,
      "step": 2912,
      "training_loss": 7.706373691558838
    },
    {
      "epoch": 0.6313279132791328,
      "step": 2912,
      "training_loss": 7.077545642852783
    },
    {
      "epoch": 0.6313279132791328,
      "step": 2912,
      "training_loss": 5.51162576675415
    },
    {
      "epoch": 0.6313279132791328,
      "step": 2912,
      "training_loss": 7.088498592376709
    },
    {
      "epoch": 0.6315447154471545,
      "step": 2913,
      "training_loss": 6.87275505065918
    },
    {
      "epoch": 0.6315447154471545,
      "step": 2913,
      "training_loss": 7.143683433532715
    },
    {
      "epoch": 0.6315447154471545,
      "step": 2913,
      "training_loss": 7.824699878692627
    },
    {
      "epoch": 0.6315447154471545,
      "step": 2913,
      "training_loss": 7.3421807289123535
    },
    {
      "epoch": 0.6317615176151762,
      "step": 2914,
      "training_loss": 5.703547477722168
    },
    {
      "epoch": 0.6317615176151762,
      "step": 2914,
      "training_loss": 7.628082752227783
    },
    {
      "epoch": 0.6317615176151762,
      "step": 2914,
      "training_loss": 2.624150514602661
    },
    {
      "epoch": 0.6317615176151762,
      "step": 2914,
      "training_loss": 5.802358150482178
    },
    {
      "epoch": 0.6319783197831979,
      "step": 2915,
      "training_loss": 6.712438106536865
    },
    {
      "epoch": 0.6319783197831979,
      "step": 2915,
      "training_loss": 6.983288764953613
    },
    {
      "epoch": 0.6319783197831979,
      "step": 2915,
      "training_loss": 6.975686550140381
    },
    {
      "epoch": 0.6319783197831979,
      "step": 2915,
      "training_loss": 5.658446788787842
    },
    {
      "epoch": 0.6321951219512195,
      "grad_norm": 16.340717315673828,
      "learning_rate": 1e-05,
      "loss": 6.541,
      "step": 2916
    },
    {
      "epoch": 0.6321951219512195,
      "step": 2916,
      "training_loss": 5.538699626922607
    },
    {
      "epoch": 0.6321951219512195,
      "step": 2916,
      "training_loss": 6.425118923187256
    },
    {
      "epoch": 0.6321951219512195,
      "step": 2916,
      "training_loss": 6.8072686195373535
    },
    {
      "epoch": 0.6321951219512195,
      "step": 2916,
      "training_loss": 6.688924312591553
    },
    {
      "epoch": 0.6324119241192412,
      "step": 2917,
      "training_loss": 6.653026580810547
    },
    {
      "epoch": 0.6324119241192412,
      "step": 2917,
      "training_loss": 6.041666507720947
    },
    {
      "epoch": 0.6324119241192412,
      "step": 2917,
      "training_loss": 6.532144069671631
    },
    {
      "epoch": 0.6324119241192412,
      "step": 2917,
      "training_loss": 6.019175052642822
    },
    {
      "epoch": 0.6326287262872629,
      "step": 2918,
      "training_loss": 6.569599628448486
    },
    {
      "epoch": 0.6326287262872629,
      "step": 2918,
      "training_loss": 6.646225452423096
    },
    {
      "epoch": 0.6326287262872629,
      "step": 2918,
      "training_loss": 5.958017349243164
    },
    {
      "epoch": 0.6326287262872629,
      "step": 2918,
      "training_loss": 5.91330099105835
    },
    {
      "epoch": 0.6328455284552845,
      "step": 2919,
      "training_loss": 5.233337879180908
    },
    {
      "epoch": 0.6328455284552845,
      "step": 2919,
      "training_loss": 7.228723049163818
    },
    {
      "epoch": 0.6328455284552845,
      "step": 2919,
      "training_loss": 7.66829776763916
    },
    {
      "epoch": 0.6328455284552845,
      "step": 2919,
      "training_loss": 6.983428478240967
    },
    {
      "epoch": 0.6330623306233062,
      "grad_norm": 18.92718505859375,
      "learning_rate": 1e-05,
      "loss": 6.4317,
      "step": 2920
    },
    {
      "epoch": 0.6330623306233062,
      "step": 2920,
      "training_loss": 6.610400676727295
    },
    {
      "epoch": 0.6330623306233062,
      "step": 2920,
      "training_loss": 6.496281147003174
    },
    {
      "epoch": 0.6330623306233062,
      "step": 2920,
      "training_loss": 6.592073917388916
    },
    {
      "epoch": 0.6330623306233062,
      "step": 2920,
      "training_loss": 5.9421515464782715
    },
    {
      "epoch": 0.6332791327913279,
      "step": 2921,
      "training_loss": 7.2021660804748535
    },
    {
      "epoch": 0.6332791327913279,
      "step": 2921,
      "training_loss": 6.971181392669678
    },
    {
      "epoch": 0.6332791327913279,
      "step": 2921,
      "training_loss": 6.938494682312012
    },
    {
      "epoch": 0.6332791327913279,
      "step": 2921,
      "training_loss": 7.810605525970459
    },
    {
      "epoch": 0.6334959349593496,
      "step": 2922,
      "training_loss": 5.87534761428833
    },
    {
      "epoch": 0.6334959349593496,
      "step": 2922,
      "training_loss": 6.078146457672119
    },
    {
      "epoch": 0.6334959349593496,
      "step": 2922,
      "training_loss": 6.5107421875
    },
    {
      "epoch": 0.6334959349593496,
      "step": 2922,
      "training_loss": 6.32320499420166
    },
    {
      "epoch": 0.6337127371273713,
      "step": 2923,
      "training_loss": 6.445146083831787
    },
    {
      "epoch": 0.6337127371273713,
      "step": 2923,
      "training_loss": 6.902917861938477
    },
    {
      "epoch": 0.6337127371273713,
      "step": 2923,
      "training_loss": 6.436288833618164
    },
    {
      "epoch": 0.6337127371273713,
      "step": 2923,
      "training_loss": 6.119619369506836
    },
    {
      "epoch": 0.633929539295393,
      "grad_norm": 24.894121170043945,
      "learning_rate": 1e-05,
      "loss": 6.5784,
      "step": 2924
    },
    {
      "epoch": 0.633929539295393,
      "step": 2924,
      "training_loss": 6.351058006286621
    },
    {
      "epoch": 0.633929539295393,
      "step": 2924,
      "training_loss": 7.712339401245117
    },
    {
      "epoch": 0.633929539295393,
      "step": 2924,
      "training_loss": 7.126832485198975
    },
    {
      "epoch": 0.633929539295393,
      "step": 2924,
      "training_loss": 6.596732139587402
    },
    {
      "epoch": 0.6341463414634146,
      "step": 2925,
      "training_loss": 6.6304931640625
    },
    {
      "epoch": 0.6341463414634146,
      "step": 2925,
      "training_loss": 5.708050727844238
    },
    {
      "epoch": 0.6341463414634146,
      "step": 2925,
      "training_loss": 5.557625770568848
    },
    {
      "epoch": 0.6341463414634146,
      "step": 2925,
      "training_loss": 5.694121837615967
    },
    {
      "epoch": 0.6343631436314363,
      "step": 2926,
      "training_loss": 6.325394630432129
    },
    {
      "epoch": 0.6343631436314363,
      "step": 2926,
      "training_loss": 5.343477725982666
    },
    {
      "epoch": 0.6343631436314363,
      "step": 2926,
      "training_loss": 6.229902267456055
    },
    {
      "epoch": 0.6343631436314363,
      "step": 2926,
      "training_loss": 4.0036301612854
    },
    {
      "epoch": 0.634579945799458,
      "step": 2927,
      "training_loss": 7.0013747215271
    },
    {
      "epoch": 0.634579945799458,
      "step": 2927,
      "training_loss": 7.322548866271973
    },
    {
      "epoch": 0.634579945799458,
      "step": 2927,
      "training_loss": 5.617305278778076
    },
    {
      "epoch": 0.634579945799458,
      "step": 2927,
      "training_loss": 5.314678192138672
    },
    {
      "epoch": 0.6347967479674796,
      "grad_norm": 18.05109977722168,
      "learning_rate": 1e-05,
      "loss": 6.1585,
      "step": 2928
    },
    {
      "epoch": 0.6347967479674796,
      "step": 2928,
      "training_loss": 6.964271545410156
    },
    {
      "epoch": 0.6347967479674796,
      "step": 2928,
      "training_loss": 6.723797798156738
    },
    {
      "epoch": 0.6347967479674796,
      "step": 2928,
      "training_loss": 6.508656024932861
    },
    {
      "epoch": 0.6347967479674796,
      "step": 2928,
      "training_loss": 5.839844226837158
    },
    {
      "epoch": 0.6350135501355013,
      "step": 2929,
      "training_loss": 6.750341892242432
    },
    {
      "epoch": 0.6350135501355013,
      "step": 2929,
      "training_loss": 3.386207103729248
    },
    {
      "epoch": 0.6350135501355013,
      "step": 2929,
      "training_loss": 7.444664001464844
    },
    {
      "epoch": 0.6350135501355013,
      "step": 2929,
      "training_loss": 6.521852970123291
    },
    {
      "epoch": 0.635230352303523,
      "step": 2930,
      "training_loss": 7.859847545623779
    },
    {
      "epoch": 0.635230352303523,
      "step": 2930,
      "training_loss": 5.552567005157471
    },
    {
      "epoch": 0.635230352303523,
      "step": 2930,
      "training_loss": 7.8026442527771
    },
    {
      "epoch": 0.635230352303523,
      "step": 2930,
      "training_loss": 4.239081382751465
    },
    {
      "epoch": 0.6354471544715448,
      "step": 2931,
      "training_loss": 7.602919101715088
    },
    {
      "epoch": 0.6354471544715448,
      "step": 2931,
      "training_loss": 6.125172138214111
    },
    {
      "epoch": 0.6354471544715448,
      "step": 2931,
      "training_loss": 3.6474051475524902
    },
    {
      "epoch": 0.6354471544715448,
      "step": 2931,
      "training_loss": 7.170099258422852
    },
    {
      "epoch": 0.6356639566395664,
      "grad_norm": 17.028274536132812,
      "learning_rate": 1e-05,
      "loss": 6.2587,
      "step": 2932
    },
    {
      "epoch": 0.6356639566395664,
      "step": 2932,
      "training_loss": 7.689007759094238
    },
    {
      "epoch": 0.6356639566395664,
      "step": 2932,
      "training_loss": 6.48775053024292
    },
    {
      "epoch": 0.6356639566395664,
      "step": 2932,
      "training_loss": 5.815971374511719
    },
    {
      "epoch": 0.6356639566395664,
      "step": 2932,
      "training_loss": 6.808747291564941
    },
    {
      "epoch": 0.6358807588075881,
      "step": 2933,
      "training_loss": 5.400535583496094
    },
    {
      "epoch": 0.6358807588075881,
      "step": 2933,
      "training_loss": 6.994101047515869
    },
    {
      "epoch": 0.6358807588075881,
      "step": 2933,
      "training_loss": 6.9898176193237305
    },
    {
      "epoch": 0.6358807588075881,
      "step": 2933,
      "training_loss": 6.687999248504639
    },
    {
      "epoch": 0.6360975609756098,
      "step": 2934,
      "training_loss": 7.640671253204346
    },
    {
      "epoch": 0.6360975609756098,
      "step": 2934,
      "training_loss": 7.422623157501221
    },
    {
      "epoch": 0.6360975609756098,
      "step": 2934,
      "training_loss": 5.367832660675049
    },
    {
      "epoch": 0.6360975609756098,
      "step": 2934,
      "training_loss": 6.388542175292969
    },
    {
      "epoch": 0.6363143631436314,
      "step": 2935,
      "training_loss": 6.112839221954346
    },
    {
      "epoch": 0.6363143631436314,
      "step": 2935,
      "training_loss": 5.475098609924316
    },
    {
      "epoch": 0.6363143631436314,
      "step": 2935,
      "training_loss": 6.908072471618652
    },
    {
      "epoch": 0.6363143631436314,
      "step": 2935,
      "training_loss": 7.027828216552734
    },
    {
      "epoch": 0.6365311653116531,
      "grad_norm": 12.877867698669434,
      "learning_rate": 1e-05,
      "loss": 6.5761,
      "step": 2936
    },
    {
      "epoch": 0.6365311653116531,
      "step": 2936,
      "training_loss": 7.125344753265381
    },
    {
      "epoch": 0.6365311653116531,
      "step": 2936,
      "training_loss": 6.031019687652588
    },
    {
      "epoch": 0.6365311653116531,
      "step": 2936,
      "training_loss": 6.530470371246338
    },
    {
      "epoch": 0.6365311653116531,
      "step": 2936,
      "training_loss": 5.41710090637207
    },
    {
      "epoch": 0.6367479674796748,
      "step": 2937,
      "training_loss": 8.000307083129883
    },
    {
      "epoch": 0.6367479674796748,
      "step": 2937,
      "training_loss": 7.3286004066467285
    },
    {
      "epoch": 0.6367479674796748,
      "step": 2937,
      "training_loss": 6.446837425231934
    },
    {
      "epoch": 0.6367479674796748,
      "step": 2937,
      "training_loss": 6.808508396148682
    },
    {
      "epoch": 0.6369647696476964,
      "step": 2938,
      "training_loss": 5.917144298553467
    },
    {
      "epoch": 0.6369647696476964,
      "step": 2938,
      "training_loss": 6.370368003845215
    },
    {
      "epoch": 0.6369647696476964,
      "step": 2938,
      "training_loss": 6.549449443817139
    },
    {
      "epoch": 0.6369647696476964,
      "step": 2938,
      "training_loss": 7.689570903778076
    },
    {
      "epoch": 0.6371815718157181,
      "step": 2939,
      "training_loss": 4.457327365875244
    },
    {
      "epoch": 0.6371815718157181,
      "step": 2939,
      "training_loss": 7.287039756774902
    },
    {
      "epoch": 0.6371815718157181,
      "step": 2939,
      "training_loss": 6.576504230499268
    },
    {
      "epoch": 0.6371815718157181,
      "step": 2939,
      "training_loss": 7.969961643218994
    },
    {
      "epoch": 0.6373983739837399,
      "grad_norm": 13.721250534057617,
      "learning_rate": 1e-05,
      "loss": 6.6566,
      "step": 2940
    },
    {
      "epoch": 0.6373983739837399,
      "step": 2940,
      "training_loss": 6.445932388305664
    },
    {
      "epoch": 0.6373983739837399,
      "step": 2940,
      "training_loss": 6.833749294281006
    },
    {
      "epoch": 0.6373983739837399,
      "step": 2940,
      "training_loss": 5.976172924041748
    },
    {
      "epoch": 0.6373983739837399,
      "step": 2940,
      "training_loss": 7.561134338378906
    },
    {
      "epoch": 0.6376151761517616,
      "step": 2941,
      "training_loss": 6.743344306945801
    },
    {
      "epoch": 0.6376151761517616,
      "step": 2941,
      "training_loss": 6.134218692779541
    },
    {
      "epoch": 0.6376151761517616,
      "step": 2941,
      "training_loss": 5.823951244354248
    },
    {
      "epoch": 0.6376151761517616,
      "step": 2941,
      "training_loss": 7.044159889221191
    },
    {
      "epoch": 0.6378319783197832,
      "step": 2942,
      "training_loss": 7.247734546661377
    },
    {
      "epoch": 0.6378319783197832,
      "step": 2942,
      "training_loss": 5.410998821258545
    },
    {
      "epoch": 0.6378319783197832,
      "step": 2942,
      "training_loss": 7.302109718322754
    },
    {
      "epoch": 0.6378319783197832,
      "step": 2942,
      "training_loss": 5.761487007141113
    },
    {
      "epoch": 0.6380487804878049,
      "step": 2943,
      "training_loss": 6.749342441558838
    },
    {
      "epoch": 0.6380487804878049,
      "step": 2943,
      "training_loss": 6.879702568054199
    },
    {
      "epoch": 0.6380487804878049,
      "step": 2943,
      "training_loss": 6.786126136779785
    },
    {
      "epoch": 0.6380487804878049,
      "step": 2943,
      "training_loss": 6.322742938995361
    },
    {
      "epoch": 0.6382655826558266,
      "grad_norm": 15.825056076049805,
      "learning_rate": 1e-05,
      "loss": 6.5639,
      "step": 2944
    },
    {
      "epoch": 0.6382655826558266,
      "step": 2944,
      "training_loss": 7.506967067718506
    },
    {
      "epoch": 0.6382655826558266,
      "step": 2944,
      "training_loss": 6.5781779289245605
    },
    {
      "epoch": 0.6382655826558266,
      "step": 2944,
      "training_loss": 7.734519004821777
    },
    {
      "epoch": 0.6382655826558266,
      "step": 2944,
      "training_loss": 3.862509250640869
    },
    {
      "epoch": 0.6384823848238482,
      "step": 2945,
      "training_loss": 6.679740905761719
    },
    {
      "epoch": 0.6384823848238482,
      "step": 2945,
      "training_loss": 6.922744274139404
    },
    {
      "epoch": 0.6384823848238482,
      "step": 2945,
      "training_loss": 6.596187114715576
    },
    {
      "epoch": 0.6384823848238482,
      "step": 2945,
      "training_loss": 5.676691055297852
    },
    {
      "epoch": 0.6386991869918699,
      "step": 2946,
      "training_loss": 4.901008605957031
    },
    {
      "epoch": 0.6386991869918699,
      "step": 2946,
      "training_loss": 6.602413177490234
    },
    {
      "epoch": 0.6386991869918699,
      "step": 2946,
      "training_loss": 7.121659755706787
    },
    {
      "epoch": 0.6386991869918699,
      "step": 2946,
      "training_loss": 5.898359775543213
    },
    {
      "epoch": 0.6389159891598916,
      "step": 2947,
      "training_loss": 6.723512172698975
    },
    {
      "epoch": 0.6389159891598916,
      "step": 2947,
      "training_loss": 8.52680492401123
    },
    {
      "epoch": 0.6389159891598916,
      "step": 2947,
      "training_loss": 5.836618423461914
    },
    {
      "epoch": 0.6389159891598916,
      "step": 2947,
      "training_loss": 4.83508825302124
    },
    {
      "epoch": 0.6391327913279132,
      "grad_norm": 25.712724685668945,
      "learning_rate": 1e-05,
      "loss": 6.3752,
      "step": 2948
    },
    {
      "epoch": 0.6391327913279132,
      "step": 2948,
      "training_loss": 6.95639705657959
    },
    {
      "epoch": 0.6391327913279132,
      "step": 2948,
      "training_loss": 6.3059916496276855
    },
    {
      "epoch": 0.6391327913279132,
      "step": 2948,
      "training_loss": 6.018343925476074
    },
    {
      "epoch": 0.6391327913279132,
      "step": 2948,
      "training_loss": 7.560683727264404
    },
    {
      "epoch": 0.639349593495935,
      "step": 2949,
      "training_loss": 5.2446675300598145
    },
    {
      "epoch": 0.639349593495935,
      "step": 2949,
      "training_loss": 5.770417213439941
    },
    {
      "epoch": 0.639349593495935,
      "step": 2949,
      "training_loss": 7.237889766693115
    },
    {
      "epoch": 0.639349593495935,
      "step": 2949,
      "training_loss": 6.836135387420654
    },
    {
      "epoch": 0.6395663956639567,
      "step": 2950,
      "training_loss": 7.381134033203125
    },
    {
      "epoch": 0.6395663956639567,
      "step": 2950,
      "training_loss": 6.403172969818115
    },
    {
      "epoch": 0.6395663956639567,
      "step": 2950,
      "training_loss": 4.5708723068237305
    },
    {
      "epoch": 0.6395663956639567,
      "step": 2950,
      "training_loss": 5.054107189178467
    },
    {
      "epoch": 0.6397831978319783,
      "step": 2951,
      "training_loss": 6.6110520362854
    },
    {
      "epoch": 0.6397831978319783,
      "step": 2951,
      "training_loss": 6.524509429931641
    },
    {
      "epoch": 0.6397831978319783,
      "step": 2951,
      "training_loss": 7.093017101287842
    },
    {
      "epoch": 0.6397831978319783,
      "step": 2951,
      "training_loss": 6.63677978515625
    },
    {
      "epoch": 0.64,
      "grad_norm": 16.112241744995117,
      "learning_rate": 1e-05,
      "loss": 6.3878,
      "step": 2952
    },
    {
      "epoch": 0.64,
      "step": 2952,
      "training_loss": 8.16569995880127
    },
    {
      "epoch": 0.64,
      "step": 2952,
      "training_loss": 7.217514991760254
    },
    {
      "epoch": 0.64,
      "step": 2952,
      "training_loss": 7.437832832336426
    },
    {
      "epoch": 0.64,
      "step": 2952,
      "training_loss": 6.119577884674072
    },
    {
      "epoch": 0.6402168021680217,
      "step": 2953,
      "training_loss": 5.915567398071289
    },
    {
      "epoch": 0.6402168021680217,
      "step": 2953,
      "training_loss": 6.747239112854004
    },
    {
      "epoch": 0.6402168021680217,
      "step": 2953,
      "training_loss": 6.289026260375977
    },
    {
      "epoch": 0.6402168021680217,
      "step": 2953,
      "training_loss": 3.6252267360687256
    },
    {
      "epoch": 0.6404336043360433,
      "step": 2954,
      "training_loss": 6.247077941894531
    },
    {
      "epoch": 0.6404336043360433,
      "step": 2954,
      "training_loss": 6.648996353149414
    },
    {
      "epoch": 0.6404336043360433,
      "step": 2954,
      "training_loss": 6.937320232391357
    },
    {
      "epoch": 0.6404336043360433,
      "step": 2954,
      "training_loss": 5.848123073577881
    },
    {
      "epoch": 0.640650406504065,
      "step": 2955,
      "training_loss": 6.822360038757324
    },
    {
      "epoch": 0.640650406504065,
      "step": 2955,
      "training_loss": 7.368888854980469
    },
    {
      "epoch": 0.640650406504065,
      "step": 2955,
      "training_loss": 7.488203048706055
    },
    {
      "epoch": 0.640650406504065,
      "step": 2955,
      "training_loss": 7.788796424865723
    },
    {
      "epoch": 0.6408672086720867,
      "grad_norm": 13.26746654510498,
      "learning_rate": 1e-05,
      "loss": 6.6667,
      "step": 2956
    },
    {
      "epoch": 0.6408672086720867,
      "step": 2956,
      "training_loss": 5.5506391525268555
    },
    {
      "epoch": 0.6408672086720867,
      "step": 2956,
      "training_loss": 7.646702289581299
    },
    {
      "epoch": 0.6408672086720867,
      "step": 2956,
      "training_loss": 7.325810432434082
    },
    {
      "epoch": 0.6408672086720867,
      "step": 2956,
      "training_loss": 7.548769474029541
    },
    {
      "epoch": 0.6410840108401084,
      "step": 2957,
      "training_loss": 5.054037570953369
    },
    {
      "epoch": 0.6410840108401084,
      "step": 2957,
      "training_loss": 7.2784104347229
    },
    {
      "epoch": 0.6410840108401084,
      "step": 2957,
      "training_loss": 6.755257606506348
    },
    {
      "epoch": 0.6410840108401084,
      "step": 2957,
      "training_loss": 7.21071195602417
    },
    {
      "epoch": 0.6413008130081301,
      "step": 2958,
      "training_loss": 5.11491060256958
    },
    {
      "epoch": 0.6413008130081301,
      "step": 2958,
      "training_loss": 6.998175621032715
    },
    {
      "epoch": 0.6413008130081301,
      "step": 2958,
      "training_loss": 3.4532341957092285
    },
    {
      "epoch": 0.6413008130081301,
      "step": 2958,
      "training_loss": 7.227960109710693
    },
    {
      "epoch": 0.6415176151761518,
      "step": 2959,
      "training_loss": 5.295475959777832
    },
    {
      "epoch": 0.6415176151761518,
      "step": 2959,
      "training_loss": 6.860273361206055
    },
    {
      "epoch": 0.6415176151761518,
      "step": 2959,
      "training_loss": 6.454913139343262
    },
    {
      "epoch": 0.6415176151761518,
      "step": 2959,
      "training_loss": 7.117838382720947
    },
    {
      "epoch": 0.6417344173441735,
      "grad_norm": 19.71292495727539,
      "learning_rate": 1e-05,
      "loss": 6.4308,
      "step": 2960
    },
    {
      "epoch": 0.6417344173441735,
      "step": 2960,
      "training_loss": 8.215243339538574
    },
    {
      "epoch": 0.6417344173441735,
      "step": 2960,
      "training_loss": 4.688297271728516
    },
    {
      "epoch": 0.6417344173441735,
      "step": 2960,
      "training_loss": 6.2228474617004395
    },
    {
      "epoch": 0.6417344173441735,
      "step": 2960,
      "training_loss": 6.682746410369873
    },
    {
      "epoch": 0.6419512195121951,
      "step": 2961,
      "training_loss": 5.119903564453125
    },
    {
      "epoch": 0.6419512195121951,
      "step": 2961,
      "training_loss": 7.335343360900879
    },
    {
      "epoch": 0.6419512195121951,
      "step": 2961,
      "training_loss": 6.901266574859619
    },
    {
      "epoch": 0.6419512195121951,
      "step": 2961,
      "training_loss": 6.46732759475708
    },
    {
      "epoch": 0.6421680216802168,
      "step": 2962,
      "training_loss": 7.318586826324463
    },
    {
      "epoch": 0.6421680216802168,
      "step": 2962,
      "training_loss": 7.539892196655273
    },
    {
      "epoch": 0.6421680216802168,
      "step": 2962,
      "training_loss": 7.303290843963623
    },
    {
      "epoch": 0.6421680216802168,
      "step": 2962,
      "training_loss": 7.114803314208984
    },
    {
      "epoch": 0.6423848238482385,
      "step": 2963,
      "training_loss": 6.634554862976074
    },
    {
      "epoch": 0.6423848238482385,
      "step": 2963,
      "training_loss": 6.484856128692627
    },
    {
      "epoch": 0.6423848238482385,
      "step": 2963,
      "training_loss": 8.002130508422852
    },
    {
      "epoch": 0.6423848238482385,
      "step": 2963,
      "training_loss": 7.348787307739258
    },
    {
      "epoch": 0.6426016260162601,
      "grad_norm": 14.709403038024902,
      "learning_rate": 1e-05,
      "loss": 6.8362,
      "step": 2964
    },
    {
      "epoch": 0.6426016260162601,
      "step": 2964,
      "training_loss": 7.297512531280518
    },
    {
      "epoch": 0.6426016260162601,
      "step": 2964,
      "training_loss": 6.200264930725098
    },
    {
      "epoch": 0.6426016260162601,
      "step": 2964,
      "training_loss": 6.781301975250244
    },
    {
      "epoch": 0.6426016260162601,
      "step": 2964,
      "training_loss": 7.0603508949279785
    },
    {
      "epoch": 0.6428184281842818,
      "step": 2965,
      "training_loss": 6.1903181076049805
    },
    {
      "epoch": 0.6428184281842818,
      "step": 2965,
      "training_loss": 4.510080337524414
    },
    {
      "epoch": 0.6428184281842818,
      "step": 2965,
      "training_loss": 5.237318515777588
    },
    {
      "epoch": 0.6428184281842818,
      "step": 2965,
      "training_loss": 6.186036586761475
    },
    {
      "epoch": 0.6430352303523035,
      "step": 2966,
      "training_loss": 3.166106939315796
    },
    {
      "epoch": 0.6430352303523035,
      "step": 2966,
      "training_loss": 6.854689598083496
    },
    {
      "epoch": 0.6430352303523035,
      "step": 2966,
      "training_loss": 5.378941535949707
    },
    {
      "epoch": 0.6430352303523035,
      "step": 2966,
      "training_loss": 4.729444980621338
    },
    {
      "epoch": 0.6432520325203253,
      "step": 2967,
      "training_loss": 5.095744609832764
    },
    {
      "epoch": 0.6432520325203253,
      "step": 2967,
      "training_loss": 5.043872356414795
    },
    {
      "epoch": 0.6432520325203253,
      "step": 2967,
      "training_loss": 6.9637298583984375
    },
    {
      "epoch": 0.6432520325203253,
      "step": 2967,
      "training_loss": 6.659193992614746
    },
    {
      "epoch": 0.6434688346883469,
      "grad_norm": 15.98802375793457,
      "learning_rate": 1e-05,
      "loss": 5.8347,
      "step": 2968
    },
    {
      "epoch": 0.6434688346883469,
      "step": 2968,
      "training_loss": 7.1678147315979
    },
    {
      "epoch": 0.6434688346883469,
      "step": 2968,
      "training_loss": 6.328614234924316
    },
    {
      "epoch": 0.6434688346883469,
      "step": 2968,
      "training_loss": 6.861028671264648
    },
    {
      "epoch": 0.6434688346883469,
      "step": 2968,
      "training_loss": 6.598777770996094
    },
    {
      "epoch": 0.6436856368563686,
      "step": 2969,
      "training_loss": 6.697284698486328
    },
    {
      "epoch": 0.6436856368563686,
      "step": 2969,
      "training_loss": 5.724514484405518
    },
    {
      "epoch": 0.6436856368563686,
      "step": 2969,
      "training_loss": 5.952655792236328
    },
    {
      "epoch": 0.6436856368563686,
      "step": 2969,
      "training_loss": 7.364111423492432
    },
    {
      "epoch": 0.6439024390243903,
      "step": 2970,
      "training_loss": 6.163289546966553
    },
    {
      "epoch": 0.6439024390243903,
      "step": 2970,
      "training_loss": 6.107797145843506
    },
    {
      "epoch": 0.6439024390243903,
      "step": 2970,
      "training_loss": 5.503024101257324
    },
    {
      "epoch": 0.6439024390243903,
      "step": 2970,
      "training_loss": 6.532912731170654
    },
    {
      "epoch": 0.6441192411924119,
      "step": 2971,
      "training_loss": 6.984471797943115
    },
    {
      "epoch": 0.6441192411924119,
      "step": 2971,
      "training_loss": 4.94534969329834
    },
    {
      "epoch": 0.6441192411924119,
      "step": 2971,
      "training_loss": 7.007145881652832
    },
    {
      "epoch": 0.6441192411924119,
      "step": 2971,
      "training_loss": 4.832137584686279
    },
    {
      "epoch": 0.6443360433604336,
      "grad_norm": 12.363391876220703,
      "learning_rate": 1e-05,
      "loss": 6.2982,
      "step": 2972
    },
    {
      "epoch": 0.6443360433604336,
      "step": 2972,
      "training_loss": 7.312636375427246
    },
    {
      "epoch": 0.6443360433604336,
      "step": 2972,
      "training_loss": 7.010058403015137
    },
    {
      "epoch": 0.6443360433604336,
      "step": 2972,
      "training_loss": 7.062302112579346
    },
    {
      "epoch": 0.6443360433604336,
      "step": 2972,
      "training_loss": 6.212494373321533
    },
    {
      "epoch": 0.6445528455284553,
      "step": 2973,
      "training_loss": 7.268987655639648
    },
    {
      "epoch": 0.6445528455284553,
      "step": 2973,
      "training_loss": 6.035056114196777
    },
    {
      "epoch": 0.6445528455284553,
      "step": 2973,
      "training_loss": 7.1869940757751465
    },
    {
      "epoch": 0.6445528455284553,
      "step": 2973,
      "training_loss": 7.054584980010986
    },
    {
      "epoch": 0.6447696476964769,
      "step": 2974,
      "training_loss": 7.194075584411621
    },
    {
      "epoch": 0.6447696476964769,
      "step": 2974,
      "training_loss": 6.242133617401123
    },
    {
      "epoch": 0.6447696476964769,
      "step": 2974,
      "training_loss": 6.839897632598877
    },
    {
      "epoch": 0.6447696476964769,
      "step": 2974,
      "training_loss": 6.95350980758667
    },
    {
      "epoch": 0.6449864498644986,
      "step": 2975,
      "training_loss": 4.594267845153809
    },
    {
      "epoch": 0.6449864498644986,
      "step": 2975,
      "training_loss": 7.084036827087402
    },
    {
      "epoch": 0.6449864498644986,
      "step": 2975,
      "training_loss": 4.9770612716674805
    },
    {
      "epoch": 0.6449864498644986,
      "step": 2975,
      "training_loss": 6.56723165512085
    },
    {
      "epoch": 0.6452032520325204,
      "grad_norm": 23.949077606201172,
      "learning_rate": 1e-05,
      "loss": 6.5997,
      "step": 2976
    },
    {
      "epoch": 0.6452032520325204,
      "step": 2976,
      "training_loss": 5.85967493057251
    },
    {
      "epoch": 0.6452032520325204,
      "step": 2976,
      "training_loss": 5.527753829956055
    },
    {
      "epoch": 0.6452032520325204,
      "step": 2976,
      "training_loss": 6.0330305099487305
    },
    {
      "epoch": 0.6452032520325204,
      "step": 2976,
      "training_loss": 6.3146562576293945
    },
    {
      "epoch": 0.645420054200542,
      "step": 2977,
      "training_loss": 5.487530708312988
    },
    {
      "epoch": 0.645420054200542,
      "step": 2977,
      "training_loss": 7.281744003295898
    },
    {
      "epoch": 0.645420054200542,
      "step": 2977,
      "training_loss": 3.408554792404175
    },
    {
      "epoch": 0.645420054200542,
      "step": 2977,
      "training_loss": 7.4385223388671875
    },
    {
      "epoch": 0.6456368563685637,
      "step": 2978,
      "training_loss": 6.172573089599609
    },
    {
      "epoch": 0.6456368563685637,
      "step": 2978,
      "training_loss": 6.300562381744385
    },
    {
      "epoch": 0.6456368563685637,
      "step": 2978,
      "training_loss": 6.957822322845459
    },
    {
      "epoch": 0.6456368563685637,
      "step": 2978,
      "training_loss": 6.08630895614624
    },
    {
      "epoch": 0.6458536585365854,
      "step": 2979,
      "training_loss": 6.6460466384887695
    },
    {
      "epoch": 0.6458536585365854,
      "step": 2979,
      "training_loss": 6.4819841384887695
    },
    {
      "epoch": 0.6458536585365854,
      "step": 2979,
      "training_loss": 7.439332008361816
    },
    {
      "epoch": 0.6458536585365854,
      "step": 2979,
      "training_loss": 7.285676956176758
    },
    {
      "epoch": 0.646070460704607,
      "grad_norm": 14.326224327087402,
      "learning_rate": 1e-05,
      "loss": 6.2951,
      "step": 2980
    },
    {
      "epoch": 0.646070460704607,
      "step": 2980,
      "training_loss": 6.3279595375061035
    },
    {
      "epoch": 0.646070460704607,
      "step": 2980,
      "training_loss": 6.977603435516357
    },
    {
      "epoch": 0.646070460704607,
      "step": 2980,
      "training_loss": 6.31977653503418
    },
    {
      "epoch": 0.646070460704607,
      "step": 2980,
      "training_loss": 6.02086877822876
    },
    {
      "epoch": 0.6462872628726287,
      "step": 2981,
      "training_loss": 6.082681179046631
    },
    {
      "epoch": 0.6462872628726287,
      "step": 2981,
      "training_loss": 7.025583744049072
    },
    {
      "epoch": 0.6462872628726287,
      "step": 2981,
      "training_loss": 6.931943416595459
    },
    {
      "epoch": 0.6462872628726287,
      "step": 2981,
      "training_loss": 6.560685157775879
    },
    {
      "epoch": 0.6465040650406504,
      "step": 2982,
      "training_loss": 6.785715579986572
    },
    {
      "epoch": 0.6465040650406504,
      "step": 2982,
      "training_loss": 6.336276054382324
    },
    {
      "epoch": 0.6465040650406504,
      "step": 2982,
      "training_loss": 5.675999164581299
    },
    {
      "epoch": 0.6465040650406504,
      "step": 2982,
      "training_loss": 6.678404331207275
    },
    {
      "epoch": 0.6467208672086721,
      "step": 2983,
      "training_loss": 3.347585678100586
    },
    {
      "epoch": 0.6467208672086721,
      "step": 2983,
      "training_loss": 7.265972137451172
    },
    {
      "epoch": 0.6467208672086721,
      "step": 2983,
      "training_loss": 7.185873031616211
    },
    {
      "epoch": 0.6467208672086721,
      "step": 2983,
      "training_loss": 7.00302267074585
    },
    {
      "epoch": 0.6469376693766937,
      "grad_norm": 13.346704483032227,
      "learning_rate": 1e-05,
      "loss": 6.4079,
      "step": 2984
    },
    {
      "epoch": 0.6469376693766937,
      "step": 2984,
      "training_loss": 6.611523628234863
    },
    {
      "epoch": 0.6469376693766937,
      "step": 2984,
      "training_loss": 6.639874458312988
    },
    {
      "epoch": 0.6469376693766937,
      "step": 2984,
      "training_loss": 5.6517333984375
    },
    {
      "epoch": 0.6469376693766937,
      "step": 2984,
      "training_loss": 6.299589157104492
    },
    {
      "epoch": 0.6471544715447154,
      "step": 2985,
      "training_loss": 5.07590389251709
    },
    {
      "epoch": 0.6471544715447154,
      "step": 2985,
      "training_loss": 6.422281265258789
    },
    {
      "epoch": 0.6471544715447154,
      "step": 2985,
      "training_loss": 6.7114033699035645
    },
    {
      "epoch": 0.6471544715447154,
      "step": 2985,
      "training_loss": 5.711257457733154
    },
    {
      "epoch": 0.6473712737127372,
      "step": 2986,
      "training_loss": 7.293112754821777
    },
    {
      "epoch": 0.6473712737127372,
      "step": 2986,
      "training_loss": 7.386899471282959
    },
    {
      "epoch": 0.6473712737127372,
      "step": 2986,
      "training_loss": 7.780621528625488
    },
    {
      "epoch": 0.6473712737127372,
      "step": 2986,
      "training_loss": 6.563243865966797
    },
    {
      "epoch": 0.6475880758807588,
      "step": 2987,
      "training_loss": 7.505171775817871
    },
    {
      "epoch": 0.6475880758807588,
      "step": 2987,
      "training_loss": 7.73202657699585
    },
    {
      "epoch": 0.6475880758807588,
      "step": 2987,
      "training_loss": 7.291394233703613
    },
    {
      "epoch": 0.6475880758807588,
      "step": 2987,
      "training_loss": 6.602537155151367
    },
    {
      "epoch": 0.6478048780487805,
      "grad_norm": 11.227060317993164,
      "learning_rate": 1e-05,
      "loss": 6.7049,
      "step": 2988
    },
    {
      "epoch": 0.6478048780487805,
      "step": 2988,
      "training_loss": 6.173480987548828
    },
    {
      "epoch": 0.6478048780487805,
      "step": 2988,
      "training_loss": 3.9029276371002197
    },
    {
      "epoch": 0.6478048780487805,
      "step": 2988,
      "training_loss": 7.092785835266113
    },
    {
      "epoch": 0.6478048780487805,
      "step": 2988,
      "training_loss": 7.089054584503174
    },
    {
      "epoch": 0.6480216802168022,
      "step": 2989,
      "training_loss": 5.565672874450684
    },
    {
      "epoch": 0.6480216802168022,
      "step": 2989,
      "training_loss": 6.413599967956543
    },
    {
      "epoch": 0.6480216802168022,
      "step": 2989,
      "training_loss": 6.365666389465332
    },
    {
      "epoch": 0.6480216802168022,
      "step": 2989,
      "training_loss": 7.030977249145508
    },
    {
      "epoch": 0.6482384823848238,
      "step": 2990,
      "training_loss": 6.631789207458496
    },
    {
      "epoch": 0.6482384823848238,
      "step": 2990,
      "training_loss": 6.701085567474365
    },
    {
      "epoch": 0.6482384823848238,
      "step": 2990,
      "training_loss": 6.521625518798828
    },
    {
      "epoch": 0.6482384823848238,
      "step": 2990,
      "training_loss": 4.024477481842041
    },
    {
      "epoch": 0.6484552845528455,
      "step": 2991,
      "training_loss": 6.383371353149414
    },
    {
      "epoch": 0.6484552845528455,
      "step": 2991,
      "training_loss": 8.644996643066406
    },
    {
      "epoch": 0.6484552845528455,
      "step": 2991,
      "training_loss": 6.185415267944336
    },
    {
      "epoch": 0.6484552845528455,
      "step": 2991,
      "training_loss": 6.903547763824463
    },
    {
      "epoch": 0.6486720867208672,
      "grad_norm": 21.51416015625,
      "learning_rate": 1e-05,
      "loss": 6.3519,
      "step": 2992
    },
    {
      "epoch": 0.6486720867208672,
      "step": 2992,
      "training_loss": 7.3586602210998535
    },
    {
      "epoch": 0.6486720867208672,
      "step": 2992,
      "training_loss": 5.1100592613220215
    },
    {
      "epoch": 0.6486720867208672,
      "step": 2992,
      "training_loss": 6.851975917816162
    },
    {
      "epoch": 0.6486720867208672,
      "step": 2992,
      "training_loss": 6.973142147064209
    },
    {
      "epoch": 0.6488888888888888,
      "step": 2993,
      "training_loss": 7.208835124969482
    },
    {
      "epoch": 0.6488888888888888,
      "step": 2993,
      "training_loss": 6.8017988204956055
    },
    {
      "epoch": 0.6488888888888888,
      "step": 2993,
      "training_loss": 5.3912177085876465
    },
    {
      "epoch": 0.6488888888888888,
      "step": 2993,
      "training_loss": 6.290971279144287
    },
    {
      "epoch": 0.6491056910569105,
      "step": 2994,
      "training_loss": 6.7772932052612305
    },
    {
      "epoch": 0.6491056910569105,
      "step": 2994,
      "training_loss": 7.10155725479126
    },
    {
      "epoch": 0.6491056910569105,
      "step": 2994,
      "training_loss": 6.412596702575684
    },
    {
      "epoch": 0.6491056910569105,
      "step": 2994,
      "training_loss": 7.535925388336182
    },
    {
      "epoch": 0.6493224932249323,
      "step": 2995,
      "training_loss": 6.896428108215332
    },
    {
      "epoch": 0.6493224932249323,
      "step": 2995,
      "training_loss": 6.839941024780273
    },
    {
      "epoch": 0.6493224932249323,
      "step": 2995,
      "training_loss": 7.1343584060668945
    },
    {
      "epoch": 0.6493224932249323,
      "step": 2995,
      "training_loss": 6.779609203338623
    },
    {
      "epoch": 0.649539295392954,
      "grad_norm": 24.228927612304688,
      "learning_rate": 1e-05,
      "loss": 6.7165,
      "step": 2996
    },
    {
      "epoch": 0.649539295392954,
      "step": 2996,
      "training_loss": 5.510139465332031
    },
    {
      "epoch": 0.649539295392954,
      "step": 2996,
      "training_loss": 7.23932409286499
    },
    {
      "epoch": 0.649539295392954,
      "step": 2996,
      "training_loss": 4.935522079467773
    },
    {
      "epoch": 0.649539295392954,
      "step": 2996,
      "training_loss": 5.934908390045166
    },
    {
      "epoch": 0.6497560975609756,
      "step": 2997,
      "training_loss": 6.594488620758057
    },
    {
      "epoch": 0.6497560975609756,
      "step": 2997,
      "training_loss": 5.714993476867676
    },
    {
      "epoch": 0.6497560975609756,
      "step": 2997,
      "training_loss": 5.948685169219971
    },
    {
      "epoch": 0.6497560975609756,
      "step": 2997,
      "training_loss": 7.272757530212402
    },
    {
      "epoch": 0.6499728997289973,
      "step": 2998,
      "training_loss": 7.2798380851745605
    },
    {
      "epoch": 0.6499728997289973,
      "step": 2998,
      "training_loss": 3.832261323928833
    },
    {
      "epoch": 0.6499728997289973,
      "step": 2998,
      "training_loss": 6.113603115081787
    },
    {
      "epoch": 0.6499728997289973,
      "step": 2998,
      "training_loss": 6.61508321762085
    },
    {
      "epoch": 0.650189701897019,
      "step": 2999,
      "training_loss": 7.249152660369873
    },
    {
      "epoch": 0.650189701897019,
      "step": 2999,
      "training_loss": 6.819398880004883
    },
    {
      "epoch": 0.650189701897019,
      "step": 2999,
      "training_loss": 7.21191930770874
    },
    {
      "epoch": 0.650189701897019,
      "step": 2999,
      "training_loss": 5.207740783691406
    },
    {
      "epoch": 0.6504065040650406,
      "grad_norm": 17.576223373413086,
      "learning_rate": 1e-05,
      "loss": 6.2175,
      "step": 3000
    },
    {
      "epoch": 0.6504065040650406,
      "step": 3000,
      "training_loss": 5.170666217803955
    },
    {
      "epoch": 0.6504065040650406,
      "step": 3000,
      "training_loss": 5.5478034019470215
    },
    {
      "epoch": 0.6504065040650406,
      "step": 3000,
      "training_loss": 6.657370090484619
    },
    {
      "epoch": 0.6504065040650406,
      "step": 3000,
      "training_loss": 7.566187381744385
    },
    {
      "epoch": 0.6506233062330623,
      "step": 3001,
      "training_loss": 7.835216045379639
    },
    {
      "epoch": 0.6506233062330623,
      "step": 3001,
      "training_loss": 6.7108988761901855
    },
    {
      "epoch": 0.6506233062330623,
      "step": 3001,
      "training_loss": 5.066608428955078
    },
    {
      "epoch": 0.6506233062330623,
      "step": 3001,
      "training_loss": 7.045816421508789
    },
    {
      "epoch": 0.650840108401084,
      "step": 3002,
      "training_loss": 5.907420635223389
    },
    {
      "epoch": 0.650840108401084,
      "step": 3002,
      "training_loss": 6.160843849182129
    },
    {
      "epoch": 0.650840108401084,
      "step": 3002,
      "training_loss": 4.8929972648620605
    },
    {
      "epoch": 0.650840108401084,
      "step": 3002,
      "training_loss": 5.4508867263793945
    },
    {
      "epoch": 0.6510569105691056,
      "step": 3003,
      "training_loss": 7.407691478729248
    },
    {
      "epoch": 0.6510569105691056,
      "step": 3003,
      "training_loss": 5.136410236358643
    },
    {
      "epoch": 0.6510569105691056,
      "step": 3003,
      "training_loss": 6.518839359283447
    },
    {
      "epoch": 0.6510569105691056,
      "step": 3003,
      "training_loss": 3.301492214202881
    },
    {
      "epoch": 0.6512737127371274,
      "grad_norm": 15.608786582946777,
      "learning_rate": 1e-05,
      "loss": 6.0236,
      "step": 3004
    },
    {
      "epoch": 0.6512737127371274,
      "step": 3004,
      "training_loss": 7.020443916320801
    },
    {
      "epoch": 0.6512737127371274,
      "step": 3004,
      "training_loss": 6.745098114013672
    },
    {
      "epoch": 0.6512737127371274,
      "step": 3004,
      "training_loss": 4.893732070922852
    },
    {
      "epoch": 0.6512737127371274,
      "step": 3004,
      "training_loss": 6.650904655456543
    },
    {
      "epoch": 0.6514905149051491,
      "step": 3005,
      "training_loss": 7.953120708465576
    },
    {
      "epoch": 0.6514905149051491,
      "step": 3005,
      "training_loss": 6.6113739013671875
    },
    {
      "epoch": 0.6514905149051491,
      "step": 3005,
      "training_loss": 7.003744125366211
    },
    {
      "epoch": 0.6514905149051491,
      "step": 3005,
      "training_loss": 7.317943572998047
    },
    {
      "epoch": 0.6517073170731708,
      "step": 3006,
      "training_loss": 6.0988993644714355
    },
    {
      "epoch": 0.6517073170731708,
      "step": 3006,
      "training_loss": 5.893454074859619
    },
    {
      "epoch": 0.6517073170731708,
      "step": 3006,
      "training_loss": 6.4475603103637695
    },
    {
      "epoch": 0.6517073170731708,
      "step": 3006,
      "training_loss": 6.78548526763916
    },
    {
      "epoch": 0.6519241192411924,
      "step": 3007,
      "training_loss": 6.824014663696289
    },
    {
      "epoch": 0.6519241192411924,
      "step": 3007,
      "training_loss": 5.607119560241699
    },
    {
      "epoch": 0.6519241192411924,
      "step": 3007,
      "training_loss": 5.674120903015137
    },
    {
      "epoch": 0.6519241192411924,
      "step": 3007,
      "training_loss": 5.847325801849365
    },
    {
      "epoch": 0.6521409214092141,
      "grad_norm": 18.696718215942383,
      "learning_rate": 1e-05,
      "loss": 6.4609,
      "step": 3008
    },
    {
      "epoch": 0.6521409214092141,
      "step": 3008,
      "training_loss": 6.732697486877441
    },
    {
      "epoch": 0.6521409214092141,
      "step": 3008,
      "training_loss": 5.5007195472717285
    },
    {
      "epoch": 0.6521409214092141,
      "step": 3008,
      "training_loss": 5.9118547439575195
    },
    {
      "epoch": 0.6521409214092141,
      "step": 3008,
      "training_loss": 7.145073890686035
    },
    {
      "epoch": 0.6523577235772358,
      "step": 3009,
      "training_loss": 7.578065872192383
    },
    {
      "epoch": 0.6523577235772358,
      "step": 3009,
      "training_loss": 6.991331100463867
    },
    {
      "epoch": 0.6523577235772358,
      "step": 3009,
      "training_loss": 6.171445369720459
    },
    {
      "epoch": 0.6523577235772358,
      "step": 3009,
      "training_loss": 8.00057601928711
    },
    {
      "epoch": 0.6525745257452574,
      "step": 3010,
      "training_loss": 7.55910587310791
    },
    {
      "epoch": 0.6525745257452574,
      "step": 3010,
      "training_loss": 5.138366222381592
    },
    {
      "epoch": 0.6525745257452574,
      "step": 3010,
      "training_loss": 7.180563926696777
    },
    {
      "epoch": 0.6525745257452574,
      "step": 3010,
      "training_loss": 5.999307632446289
    },
    {
      "epoch": 0.6527913279132791,
      "step": 3011,
      "training_loss": 6.555107116699219
    },
    {
      "epoch": 0.6527913279132791,
      "step": 3011,
      "training_loss": 6.648440361022949
    },
    {
      "epoch": 0.6527913279132791,
      "step": 3011,
      "training_loss": 7.750362873077393
    },
    {
      "epoch": 0.6527913279132791,
      "step": 3011,
      "training_loss": 6.711635589599609
    },
    {
      "epoch": 0.6530081300813008,
      "grad_norm": 16.798091888427734,
      "learning_rate": 1e-05,
      "loss": 6.7234,
      "step": 3012
    },
    {
      "epoch": 0.6530081300813008,
      "step": 3012,
      "training_loss": 7.244147777557373
    },
    {
      "epoch": 0.6530081300813008,
      "step": 3012,
      "training_loss": 6.991601943969727
    },
    {
      "epoch": 0.6530081300813008,
      "step": 3012,
      "training_loss": 6.079976558685303
    },
    {
      "epoch": 0.6530081300813008,
      "step": 3012,
      "training_loss": 5.910924434661865
    },
    {
      "epoch": 0.6532249322493225,
      "step": 3013,
      "training_loss": 7.583967685699463
    },
    {
      "epoch": 0.6532249322493225,
      "step": 3013,
      "training_loss": 6.452356815338135
    },
    {
      "epoch": 0.6532249322493225,
      "step": 3013,
      "training_loss": 6.7886881828308105
    },
    {
      "epoch": 0.6532249322493225,
      "step": 3013,
      "training_loss": 6.410477638244629
    },
    {
      "epoch": 0.6534417344173442,
      "step": 3014,
      "training_loss": 5.152214527130127
    },
    {
      "epoch": 0.6534417344173442,
      "step": 3014,
      "training_loss": 5.333211421966553
    },
    {
      "epoch": 0.6534417344173442,
      "step": 3014,
      "training_loss": 6.004225730895996
    },
    {
      "epoch": 0.6534417344173442,
      "step": 3014,
      "training_loss": 5.744269371032715
    },
    {
      "epoch": 0.6536585365853659,
      "step": 3015,
      "training_loss": 6.182551860809326
    },
    {
      "epoch": 0.6536585365853659,
      "step": 3015,
      "training_loss": 7.894675254821777
    },
    {
      "epoch": 0.6536585365853659,
      "step": 3015,
      "training_loss": 6.007721424102783
    },
    {
      "epoch": 0.6536585365853659,
      "step": 3015,
      "training_loss": 7.381656169891357
    },
    {
      "epoch": 0.6538753387533875,
      "grad_norm": 12.322042465209961,
      "learning_rate": 1e-05,
      "loss": 6.4477,
      "step": 3016
    },
    {
      "epoch": 0.6538753387533875,
      "step": 3016,
      "training_loss": 4.389985084533691
    },
    {
      "epoch": 0.6538753387533875,
      "step": 3016,
      "training_loss": 6.604442119598389
    },
    {
      "epoch": 0.6538753387533875,
      "step": 3016,
      "training_loss": 5.792053699493408
    },
    {
      "epoch": 0.6538753387533875,
      "step": 3016,
      "training_loss": 13.018512725830078
    },
    {
      "epoch": 0.6540921409214092,
      "step": 3017,
      "training_loss": 6.218430519104004
    },
    {
      "epoch": 0.6540921409214092,
      "step": 3017,
      "training_loss": 6.7166032791137695
    },
    {
      "epoch": 0.6540921409214092,
      "step": 3017,
      "training_loss": 7.014947891235352
    },
    {
      "epoch": 0.6540921409214092,
      "step": 3017,
      "training_loss": 6.6499247550964355
    },
    {
      "epoch": 0.6543089430894309,
      "step": 3018,
      "training_loss": 7.086763858795166
    },
    {
      "epoch": 0.6543089430894309,
      "step": 3018,
      "training_loss": 7.060124397277832
    },
    {
      "epoch": 0.6543089430894309,
      "step": 3018,
      "training_loss": 6.46303129196167
    },
    {
      "epoch": 0.6543089430894309,
      "step": 3018,
      "training_loss": 6.23376989364624
    },
    {
      "epoch": 0.6545257452574526,
      "step": 3019,
      "training_loss": 5.531481742858887
    },
    {
      "epoch": 0.6545257452574526,
      "step": 3019,
      "training_loss": 6.405773162841797
    },
    {
      "epoch": 0.6545257452574526,
      "step": 3019,
      "training_loss": 8.463310241699219
    },
    {
      "epoch": 0.6545257452574526,
      "step": 3019,
      "training_loss": 4.307888984680176
    },
    {
      "epoch": 0.6547425474254742,
      "grad_norm": 15.991605758666992,
      "learning_rate": 1e-05,
      "loss": 6.7473,
      "step": 3020
    },
    {
      "epoch": 0.6547425474254742,
      "step": 3020,
      "training_loss": 7.210850238800049
    },
    {
      "epoch": 0.6547425474254742,
      "step": 3020,
      "training_loss": 5.102674961090088
    },
    {
      "epoch": 0.6547425474254742,
      "step": 3020,
      "training_loss": 6.860337734222412
    },
    {
      "epoch": 0.6547425474254742,
      "step": 3020,
      "training_loss": 6.007930278778076
    },
    {
      "epoch": 0.6549593495934959,
      "step": 3021,
      "training_loss": 6.427367210388184
    },
    {
      "epoch": 0.6549593495934959,
      "step": 3021,
      "training_loss": 6.624284267425537
    },
    {
      "epoch": 0.6549593495934959,
      "step": 3021,
      "training_loss": 7.200308799743652
    },
    {
      "epoch": 0.6549593495934959,
      "step": 3021,
      "training_loss": 3.1055047512054443
    },
    {
      "epoch": 0.6551761517615177,
      "step": 3022,
      "training_loss": 6.979405879974365
    },
    {
      "epoch": 0.6551761517615177,
      "step": 3022,
      "training_loss": 7.504162311553955
    },
    {
      "epoch": 0.6551761517615177,
      "step": 3022,
      "training_loss": 7.226171493530273
    },
    {
      "epoch": 0.6551761517615177,
      "step": 3022,
      "training_loss": 5.933666229248047
    },
    {
      "epoch": 0.6553929539295393,
      "step": 3023,
      "training_loss": 6.27004861831665
    },
    {
      "epoch": 0.6553929539295393,
      "step": 3023,
      "training_loss": 5.92722225189209
    },
    {
      "epoch": 0.6553929539295393,
      "step": 3023,
      "training_loss": 4.553609371185303
    },
    {
      "epoch": 0.6553929539295393,
      "step": 3023,
      "training_loss": 5.231029510498047
    },
    {
      "epoch": 0.655609756097561,
      "grad_norm": 15.55599308013916,
      "learning_rate": 1e-05,
      "loss": 6.1353,
      "step": 3024
    },
    {
      "epoch": 0.655609756097561,
      "step": 3024,
      "training_loss": 7.180522918701172
    },
    {
      "epoch": 0.655609756097561,
      "step": 3024,
      "training_loss": 7.817686080932617
    },
    {
      "epoch": 0.655609756097561,
      "step": 3024,
      "training_loss": 5.899052619934082
    },
    {
      "epoch": 0.655609756097561,
      "step": 3024,
      "training_loss": 7.735614776611328
    },
    {
      "epoch": 0.6558265582655827,
      "step": 3025,
      "training_loss": 2.730067253112793
    },
    {
      "epoch": 0.6558265582655827,
      "step": 3025,
      "training_loss": 6.572949409484863
    },
    {
      "epoch": 0.6558265582655827,
      "step": 3025,
      "training_loss": 6.4735798835754395
    },
    {
      "epoch": 0.6558265582655827,
      "step": 3025,
      "training_loss": 8.167656898498535
    },
    {
      "epoch": 0.6560433604336043,
      "step": 3026,
      "training_loss": 7.284472465515137
    },
    {
      "epoch": 0.6560433604336043,
      "step": 3026,
      "training_loss": 6.606989860534668
    },
    {
      "epoch": 0.6560433604336043,
      "step": 3026,
      "training_loss": 6.0207953453063965
    },
    {
      "epoch": 0.6560433604336043,
      "step": 3026,
      "training_loss": 6.3372673988342285
    },
    {
      "epoch": 0.656260162601626,
      "step": 3027,
      "training_loss": 6.309138774871826
    },
    {
      "epoch": 0.656260162601626,
      "step": 3027,
      "training_loss": 8.065269470214844
    },
    {
      "epoch": 0.656260162601626,
      "step": 3027,
      "training_loss": 6.141351222991943
    },
    {
      "epoch": 0.656260162601626,
      "step": 3027,
      "training_loss": 7.722650527954102
    },
    {
      "epoch": 0.6564769647696477,
      "grad_norm": 24.95505142211914,
      "learning_rate": 1e-05,
      "loss": 6.6916,
      "step": 3028
    },
    {
      "epoch": 0.6564769647696477,
      "step": 3028,
      "training_loss": 5.501828193664551
    },
    {
      "epoch": 0.6564769647696477,
      "step": 3028,
      "training_loss": 6.501479625701904
    },
    {
      "epoch": 0.6564769647696477,
      "step": 3028,
      "training_loss": 7.4019455909729
    },
    {
      "epoch": 0.6564769647696477,
      "step": 3028,
      "training_loss": 5.69528865814209
    },
    {
      "epoch": 0.6566937669376693,
      "step": 3029,
      "training_loss": 6.725202560424805
    },
    {
      "epoch": 0.6566937669376693,
      "step": 3029,
      "training_loss": 7.907650470733643
    },
    {
      "epoch": 0.6566937669376693,
      "step": 3029,
      "training_loss": 6.2671332359313965
    },
    {
      "epoch": 0.6566937669376693,
      "step": 3029,
      "training_loss": 6.814000606536865
    },
    {
      "epoch": 0.656910569105691,
      "step": 3030,
      "training_loss": 7.667164325714111
    },
    {
      "epoch": 0.656910569105691,
      "step": 3030,
      "training_loss": 6.411252021789551
    },
    {
      "epoch": 0.656910569105691,
      "step": 3030,
      "training_loss": 7.189510822296143
    },
    {
      "epoch": 0.656910569105691,
      "step": 3030,
      "training_loss": 4.990166664123535
    },
    {
      "epoch": 0.6571273712737128,
      "step": 3031,
      "training_loss": 6.478189468383789
    },
    {
      "epoch": 0.6571273712737128,
      "step": 3031,
      "training_loss": 4.05568265914917
    },
    {
      "epoch": 0.6571273712737128,
      "step": 3031,
      "training_loss": 6.463354110717773
    },
    {
      "epoch": 0.6571273712737128,
      "step": 3031,
      "training_loss": 6.8415913581848145
    },
    {
      "epoch": 0.6573441734417345,
      "grad_norm": 14.81156063079834,
      "learning_rate": 1e-05,
      "loss": 6.432,
      "step": 3032
    },
    {
      "epoch": 0.6573441734417345,
      "step": 3032,
      "training_loss": 7.501109600067139
    },
    {
      "epoch": 0.6573441734417345,
      "step": 3032,
      "training_loss": 7.295449733734131
    },
    {
      "epoch": 0.6573441734417345,
      "step": 3032,
      "training_loss": 3.873216152191162
    },
    {
      "epoch": 0.6573441734417345,
      "step": 3032,
      "training_loss": 6.518867492675781
    },
    {
      "epoch": 0.6575609756097561,
      "step": 3033,
      "training_loss": 7.130352973937988
    },
    {
      "epoch": 0.6575609756097561,
      "step": 3033,
      "training_loss": 7.161118030548096
    },
    {
      "epoch": 0.6575609756097561,
      "step": 3033,
      "training_loss": 6.690077781677246
    },
    {
      "epoch": 0.6575609756097561,
      "step": 3033,
      "training_loss": 7.038991451263428
    },
    {
      "epoch": 0.6577777777777778,
      "step": 3034,
      "training_loss": 5.877679347991943
    },
    {
      "epoch": 0.6577777777777778,
      "step": 3034,
      "training_loss": 4.945153713226318
    },
    {
      "epoch": 0.6577777777777778,
      "step": 3034,
      "training_loss": 6.252211570739746
    },
    {
      "epoch": 0.6577777777777778,
      "step": 3034,
      "training_loss": 6.717662334442139
    },
    {
      "epoch": 0.6579945799457995,
      "step": 3035,
      "training_loss": 5.419222354888916
    },
    {
      "epoch": 0.6579945799457995,
      "step": 3035,
      "training_loss": 5.571978569030762
    },
    {
      "epoch": 0.6579945799457995,
      "step": 3035,
      "training_loss": 3.324704647064209
    },
    {
      "epoch": 0.6579945799457995,
      "step": 3035,
      "training_loss": 7.3650617599487305
    },
    {
      "epoch": 0.6582113821138211,
      "grad_norm": 22.40999984741211,
      "learning_rate": 1e-05,
      "loss": 6.1677,
      "step": 3036
    },
    {
      "epoch": 0.6582113821138211,
      "step": 3036,
      "training_loss": 5.787130832672119
    },
    {
      "epoch": 0.6582113821138211,
      "step": 3036,
      "training_loss": 4.676188945770264
    },
    {
      "epoch": 0.6582113821138211,
      "step": 3036,
      "training_loss": 5.680145263671875
    },
    {
      "epoch": 0.6582113821138211,
      "step": 3036,
      "training_loss": 3.5446557998657227
    },
    {
      "epoch": 0.6584281842818428,
      "step": 3037,
      "training_loss": 6.962116718292236
    },
    {
      "epoch": 0.6584281842818428,
      "step": 3037,
      "training_loss": 6.0952534675598145
    },
    {
      "epoch": 0.6584281842818428,
      "step": 3037,
      "training_loss": 6.476308822631836
    },
    {
      "epoch": 0.6584281842818428,
      "step": 3037,
      "training_loss": 7.4048943519592285
    },
    {
      "epoch": 0.6586449864498645,
      "step": 3038,
      "training_loss": 6.4670000076293945
    },
    {
      "epoch": 0.6586449864498645,
      "step": 3038,
      "training_loss": 7.157164096832275
    },
    {
      "epoch": 0.6586449864498645,
      "step": 3038,
      "training_loss": 5.348988056182861
    },
    {
      "epoch": 0.6586449864498645,
      "step": 3038,
      "training_loss": 7.430254936218262
    },
    {
      "epoch": 0.6588617886178861,
      "step": 3039,
      "training_loss": 5.139035224914551
    },
    {
      "epoch": 0.6588617886178861,
      "step": 3039,
      "training_loss": 6.533289432525635
    },
    {
      "epoch": 0.6588617886178861,
      "step": 3039,
      "training_loss": 7.142675399780273
    },
    {
      "epoch": 0.6588617886178861,
      "step": 3039,
      "training_loss": 8.417349815368652
    },
    {
      "epoch": 0.6590785907859079,
      "grad_norm": 30.284067153930664,
      "learning_rate": 1e-05,
      "loss": 6.2664,
      "step": 3040
    },
    {
      "epoch": 0.6590785907859079,
      "step": 3040,
      "training_loss": 6.305653095245361
    },
    {
      "epoch": 0.6590785907859079,
      "step": 3040,
      "training_loss": 7.364871501922607
    },
    {
      "epoch": 0.6590785907859079,
      "step": 3040,
      "training_loss": 5.037727355957031
    },
    {
      "epoch": 0.6590785907859079,
      "step": 3040,
      "training_loss": 6.650162220001221
    },
    {
      "epoch": 0.6592953929539296,
      "step": 3041,
      "training_loss": 4.1110615730285645
    },
    {
      "epoch": 0.6592953929539296,
      "step": 3041,
      "training_loss": 6.4716572761535645
    },
    {
      "epoch": 0.6592953929539296,
      "step": 3041,
      "training_loss": 7.31071662902832
    },
    {
      "epoch": 0.6592953929539296,
      "step": 3041,
      "training_loss": 5.602645397186279
    },
    {
      "epoch": 0.6595121951219513,
      "step": 3042,
      "training_loss": 5.985018253326416
    },
    {
      "epoch": 0.6595121951219513,
      "step": 3042,
      "training_loss": 6.602878570556641
    },
    {
      "epoch": 0.6595121951219513,
      "step": 3042,
      "training_loss": 5.1256422996521
    },
    {
      "epoch": 0.6595121951219513,
      "step": 3042,
      "training_loss": 5.576542377471924
    },
    {
      "epoch": 0.6597289972899729,
      "step": 3043,
      "training_loss": 7.006711959838867
    },
    {
      "epoch": 0.6597289972899729,
      "step": 3043,
      "training_loss": 6.661919116973877
    },
    {
      "epoch": 0.6597289972899729,
      "step": 3043,
      "training_loss": 7.426711082458496
    },
    {
      "epoch": 0.6597289972899729,
      "step": 3043,
      "training_loss": 5.873046398162842
    },
    {
      "epoch": 0.6599457994579946,
      "grad_norm": 16.590139389038086,
      "learning_rate": 1e-05,
      "loss": 6.1946,
      "step": 3044
    },
    {
      "epoch": 0.6599457994579946,
      "step": 3044,
      "training_loss": 7.086181640625
    },
    {
      "epoch": 0.6599457994579946,
      "step": 3044,
      "training_loss": 7.480157852172852
    },
    {
      "epoch": 0.6599457994579946,
      "step": 3044,
      "training_loss": 7.110982894897461
    },
    {
      "epoch": 0.6599457994579946,
      "step": 3044,
      "training_loss": 5.75262975692749
    },
    {
      "epoch": 0.6601626016260163,
      "step": 3045,
      "training_loss": 7.527695178985596
    },
    {
      "epoch": 0.6601626016260163,
      "step": 3045,
      "training_loss": 7.80496883392334
    },
    {
      "epoch": 0.6601626016260163,
      "step": 3045,
      "training_loss": 6.606473445892334
    },
    {
      "epoch": 0.6601626016260163,
      "step": 3045,
      "training_loss": 6.907124996185303
    },
    {
      "epoch": 0.6603794037940379,
      "step": 3046,
      "training_loss": 6.8968071937561035
    },
    {
      "epoch": 0.6603794037940379,
      "step": 3046,
      "training_loss": 5.647282123565674
    },
    {
      "epoch": 0.6603794037940379,
      "step": 3046,
      "training_loss": 7.993224620819092
    },
    {
      "epoch": 0.6603794037940379,
      "step": 3046,
      "training_loss": 7.770073413848877
    },
    {
      "epoch": 0.6605962059620596,
      "step": 3047,
      "training_loss": 5.92983865737915
    },
    {
      "epoch": 0.6605962059620596,
      "step": 3047,
      "training_loss": 5.916171073913574
    },
    {
      "epoch": 0.6605962059620596,
      "step": 3047,
      "training_loss": 3.897420644760132
    },
    {
      "epoch": 0.6605962059620596,
      "step": 3047,
      "training_loss": 8.129981994628906
    },
    {
      "epoch": 0.6608130081300813,
      "grad_norm": 18.340423583984375,
      "learning_rate": 1e-05,
      "loss": 6.7786,
      "step": 3048
    },
    {
      "epoch": 0.6608130081300813,
      "step": 3048,
      "training_loss": 6.656429290771484
    },
    {
      "epoch": 0.6608130081300813,
      "step": 3048,
      "training_loss": 6.094876289367676
    },
    {
      "epoch": 0.6608130081300813,
      "step": 3048,
      "training_loss": 5.970006465911865
    },
    {
      "epoch": 0.6608130081300813,
      "step": 3048,
      "training_loss": 6.441375732421875
    },
    {
      "epoch": 0.6610298102981029,
      "step": 3049,
      "training_loss": 7.298421859741211
    },
    {
      "epoch": 0.6610298102981029,
      "step": 3049,
      "training_loss": 4.587036609649658
    },
    {
      "epoch": 0.6610298102981029,
      "step": 3049,
      "training_loss": 8.362290382385254
    },
    {
      "epoch": 0.6610298102981029,
      "step": 3049,
      "training_loss": 7.117729663848877
    },
    {
      "epoch": 0.6612466124661247,
      "step": 3050,
      "training_loss": 5.433831691741943
    },
    {
      "epoch": 0.6612466124661247,
      "step": 3050,
      "training_loss": 5.5794172286987305
    },
    {
      "epoch": 0.6612466124661247,
      "step": 3050,
      "training_loss": 5.438690662384033
    },
    {
      "epoch": 0.6612466124661247,
      "step": 3050,
      "training_loss": 6.968613624572754
    },
    {
      "epoch": 0.6614634146341464,
      "step": 3051,
      "training_loss": 6.912538528442383
    },
    {
      "epoch": 0.6614634146341464,
      "step": 3051,
      "training_loss": 6.969812393188477
    },
    {
      "epoch": 0.6614634146341464,
      "step": 3051,
      "training_loss": 6.420115947723389
    },
    {
      "epoch": 0.6614634146341464,
      "step": 3051,
      "training_loss": 4.39913272857666
    },
    {
      "epoch": 0.661680216802168,
      "grad_norm": 22.119770050048828,
      "learning_rate": 1e-05,
      "loss": 6.2906,
      "step": 3052
    },
    {
      "epoch": 0.661680216802168,
      "step": 3052,
      "training_loss": 3.2639567852020264
    },
    {
      "epoch": 0.661680216802168,
      "step": 3052,
      "training_loss": 5.020625114440918
    },
    {
      "epoch": 0.661680216802168,
      "step": 3052,
      "training_loss": 6.439609050750732
    },
    {
      "epoch": 0.661680216802168,
      "step": 3052,
      "training_loss": 6.78069543838501
    },
    {
      "epoch": 0.6618970189701897,
      "step": 3053,
      "training_loss": 7.071934700012207
    },
    {
      "epoch": 0.6618970189701897,
      "step": 3053,
      "training_loss": 6.706558704376221
    },
    {
      "epoch": 0.6618970189701897,
      "step": 3053,
      "training_loss": 6.594707012176514
    },
    {
      "epoch": 0.6618970189701897,
      "step": 3053,
      "training_loss": 6.245283603668213
    },
    {
      "epoch": 0.6621138211382114,
      "step": 3054,
      "training_loss": 6.6062188148498535
    },
    {
      "epoch": 0.6621138211382114,
      "step": 3054,
      "training_loss": 7.911581516265869
    },
    {
      "epoch": 0.6621138211382114,
      "step": 3054,
      "training_loss": 5.691437244415283
    },
    {
      "epoch": 0.6621138211382114,
      "step": 3054,
      "training_loss": 7.581221580505371
    },
    {
      "epoch": 0.662330623306233,
      "step": 3055,
      "training_loss": 5.61422061920166
    },
    {
      "epoch": 0.662330623306233,
      "step": 3055,
      "training_loss": 5.653739929199219
    },
    {
      "epoch": 0.662330623306233,
      "step": 3055,
      "training_loss": 7.799352645874023
    },
    {
      "epoch": 0.662330623306233,
      "step": 3055,
      "training_loss": 7.372954368591309
    },
    {
      "epoch": 0.6625474254742547,
      "grad_norm": 15.692193031311035,
      "learning_rate": 1e-05,
      "loss": 6.3971,
      "step": 3056
    },
    {
      "epoch": 0.6625474254742547,
      "step": 3056,
      "training_loss": 7.152854919433594
    },
    {
      "epoch": 0.6625474254742547,
      "step": 3056,
      "training_loss": 5.760486602783203
    },
    {
      "epoch": 0.6625474254742547,
      "step": 3056,
      "training_loss": 7.202735424041748
    },
    {
      "epoch": 0.6625474254742547,
      "step": 3056,
      "training_loss": 4.691071510314941
    },
    {
      "epoch": 0.6627642276422764,
      "step": 3057,
      "training_loss": 7.458422660827637
    },
    {
      "epoch": 0.6627642276422764,
      "step": 3057,
      "training_loss": 6.613559246063232
    },
    {
      "epoch": 0.6627642276422764,
      "step": 3057,
      "training_loss": 7.11899471282959
    },
    {
      "epoch": 0.6627642276422764,
      "step": 3057,
      "training_loss": 6.280550956726074
    },
    {
      "epoch": 0.662981029810298,
      "step": 3058,
      "training_loss": 6.884413242340088
    },
    {
      "epoch": 0.662981029810298,
      "step": 3058,
      "training_loss": 6.295223712921143
    },
    {
      "epoch": 0.662981029810298,
      "step": 3058,
      "training_loss": 5.369668960571289
    },
    {
      "epoch": 0.662981029810298,
      "step": 3058,
      "training_loss": 6.004455089569092
    },
    {
      "epoch": 0.6631978319783198,
      "step": 3059,
      "training_loss": 6.330954551696777
    },
    {
      "epoch": 0.6631978319783198,
      "step": 3059,
      "training_loss": 7.70904541015625
    },
    {
      "epoch": 0.6631978319783198,
      "step": 3059,
      "training_loss": 6.7134270668029785
    },
    {
      "epoch": 0.6631978319783198,
      "step": 3059,
      "training_loss": 6.539250373840332
    },
    {
      "epoch": 0.6634146341463415,
      "grad_norm": 18.100204467773438,
      "learning_rate": 1e-05,
      "loss": 6.5078,
      "step": 3060
    },
    {
      "epoch": 0.6634146341463415,
      "step": 3060,
      "training_loss": 6.07169246673584
    },
    {
      "epoch": 0.6634146341463415,
      "step": 3060,
      "training_loss": 4.3169264793396
    },
    {
      "epoch": 0.6634146341463415,
      "step": 3060,
      "training_loss": 4.6980881690979
    },
    {
      "epoch": 0.6634146341463415,
      "step": 3060,
      "training_loss": 3.12509822845459
    },
    {
      "epoch": 0.6636314363143632,
      "step": 3061,
      "training_loss": 5.943670749664307
    },
    {
      "epoch": 0.6636314363143632,
      "step": 3061,
      "training_loss": 5.922646999359131
    },
    {
      "epoch": 0.6636314363143632,
      "step": 3061,
      "training_loss": 6.257373809814453
    },
    {
      "epoch": 0.6636314363143632,
      "step": 3061,
      "training_loss": 6.107021331787109
    },
    {
      "epoch": 0.6638482384823848,
      "step": 3062,
      "training_loss": 6.3671488761901855
    },
    {
      "epoch": 0.6638482384823848,
      "step": 3062,
      "training_loss": 5.889621734619141
    },
    {
      "epoch": 0.6638482384823848,
      "step": 3062,
      "training_loss": 7.36791467666626
    },
    {
      "epoch": 0.6638482384823848,
      "step": 3062,
      "training_loss": 5.422130584716797
    },
    {
      "epoch": 0.6640650406504065,
      "step": 3063,
      "training_loss": 6.4670515060424805
    },
    {
      "epoch": 0.6640650406504065,
      "step": 3063,
      "training_loss": 7.381059169769287
    },
    {
      "epoch": 0.6640650406504065,
      "step": 3063,
      "training_loss": 6.130509376525879
    },
    {
      "epoch": 0.6640650406504065,
      "step": 3063,
      "training_loss": 3.0967276096343994
    },
    {
      "epoch": 0.6642818428184282,
      "grad_norm": 16.490497589111328,
      "learning_rate": 1e-05,
      "loss": 5.6603,
      "step": 3064
    },
    {
      "epoch": 0.6642818428184282,
      "step": 3064,
      "training_loss": 7.5165863037109375
    },
    {
      "epoch": 0.6642818428184282,
      "step": 3064,
      "training_loss": 4.258570194244385
    },
    {
      "epoch": 0.6642818428184282,
      "step": 3064,
      "training_loss": 4.012436866760254
    },
    {
      "epoch": 0.6642818428184282,
      "step": 3064,
      "training_loss": 7.240360260009766
    },
    {
      "epoch": 0.6644986449864498,
      "step": 3065,
      "training_loss": 6.406182289123535
    },
    {
      "epoch": 0.6644986449864498,
      "step": 3065,
      "training_loss": 6.557181358337402
    },
    {
      "epoch": 0.6644986449864498,
      "step": 3065,
      "training_loss": 5.83883810043335
    },
    {
      "epoch": 0.6644986449864498,
      "step": 3065,
      "training_loss": 7.11829948425293
    },
    {
      "epoch": 0.6647154471544715,
      "step": 3066,
      "training_loss": 6.906980037689209
    },
    {
      "epoch": 0.6647154471544715,
      "step": 3066,
      "training_loss": 7.539577484130859
    },
    {
      "epoch": 0.6647154471544715,
      "step": 3066,
      "training_loss": 6.470064163208008
    },
    {
      "epoch": 0.6647154471544715,
      "step": 3066,
      "training_loss": 5.1550774574279785
    },
    {
      "epoch": 0.6649322493224932,
      "step": 3067,
      "training_loss": 5.269437313079834
    },
    {
      "epoch": 0.6649322493224932,
      "step": 3067,
      "training_loss": 4.411823749542236
    },
    {
      "epoch": 0.6649322493224932,
      "step": 3067,
      "training_loss": 7.209843635559082
    },
    {
      "epoch": 0.6649322493224932,
      "step": 3067,
      "training_loss": 6.161257743835449
    },
    {
      "epoch": 0.665149051490515,
      "grad_norm": 20.63860321044922,
      "learning_rate": 1e-05,
      "loss": 6.1295,
      "step": 3068
    },
    {
      "epoch": 0.665149051490515,
      "step": 3068,
      "training_loss": 6.047149658203125
    },
    {
      "epoch": 0.665149051490515,
      "step": 3068,
      "training_loss": 7.389678478240967
    },
    {
      "epoch": 0.665149051490515,
      "step": 3068,
      "training_loss": 6.664374828338623
    },
    {
      "epoch": 0.665149051490515,
      "step": 3068,
      "training_loss": 7.342790126800537
    },
    {
      "epoch": 0.6653658536585366,
      "step": 3069,
      "training_loss": 5.952967643737793
    },
    {
      "epoch": 0.6653658536585366,
      "step": 3069,
      "training_loss": 6.880914211273193
    },
    {
      "epoch": 0.6653658536585366,
      "step": 3069,
      "training_loss": 7.166331768035889
    },
    {
      "epoch": 0.6653658536585366,
      "step": 3069,
      "training_loss": 7.37089204788208
    },
    {
      "epoch": 0.6655826558265583,
      "step": 3070,
      "training_loss": 6.6851806640625
    },
    {
      "epoch": 0.6655826558265583,
      "step": 3070,
      "training_loss": 4.677013874053955
    },
    {
      "epoch": 0.6655826558265583,
      "step": 3070,
      "training_loss": 7.803410530090332
    },
    {
      "epoch": 0.6655826558265583,
      "step": 3070,
      "training_loss": 6.557270526885986
    },
    {
      "epoch": 0.66579945799458,
      "step": 3071,
      "training_loss": 6.738914966583252
    },
    {
      "epoch": 0.66579945799458,
      "step": 3071,
      "training_loss": 5.963598251342773
    },
    {
      "epoch": 0.66579945799458,
      "step": 3071,
      "training_loss": 7.833603382110596
    },
    {
      "epoch": 0.66579945799458,
      "step": 3071,
      "training_loss": 7.334104061126709
    },
    {
      "epoch": 0.6660162601626016,
      "grad_norm": 15.278876304626465,
      "learning_rate": 1e-05,
      "loss": 6.7755,
      "step": 3072
    },
    {
      "epoch": 0.6660162601626016,
      "step": 3072,
      "training_loss": 6.664846420288086
    },
    {
      "epoch": 0.6660162601626016,
      "step": 3072,
      "training_loss": 5.937321186065674
    },
    {
      "epoch": 0.6660162601626016,
      "step": 3072,
      "training_loss": 7.089103698730469
    },
    {
      "epoch": 0.6660162601626016,
      "step": 3072,
      "training_loss": 7.315672397613525
    },
    {
      "epoch": 0.6662330623306233,
      "step": 3073,
      "training_loss": 7.798016548156738
    },
    {
      "epoch": 0.6662330623306233,
      "step": 3073,
      "training_loss": 4.113489627838135
    },
    {
      "epoch": 0.6662330623306233,
      "step": 3073,
      "training_loss": 6.073569297790527
    },
    {
      "epoch": 0.6662330623306233,
      "step": 3073,
      "training_loss": 5.623211860656738
    },
    {
      "epoch": 0.666449864498645,
      "step": 3074,
      "training_loss": 4.30534553527832
    },
    {
      "epoch": 0.666449864498645,
      "step": 3074,
      "training_loss": 7.179567337036133
    },
    {
      "epoch": 0.666449864498645,
      "step": 3074,
      "training_loss": 4.433235168457031
    },
    {
      "epoch": 0.666449864498645,
      "step": 3074,
      "training_loss": 7.310610294342041
    },
    {
      "epoch": 0.6666666666666666,
      "step": 3075,
      "training_loss": 7.375588893890381
    },
    {
      "epoch": 0.6666666666666666,
      "step": 3075,
      "training_loss": 6.148331642150879
    },
    {
      "epoch": 0.6666666666666666,
      "step": 3075,
      "training_loss": 3.998650074005127
    },
    {
      "epoch": 0.6666666666666666,
      "step": 3075,
      "training_loss": 7.903236389160156
    },
    {
      "epoch": 0.6668834688346883,
      "grad_norm": 11.792498588562012,
      "learning_rate": 1e-05,
      "loss": 6.2044,
      "step": 3076
    },
    {
      "epoch": 0.6668834688346883,
      "step": 3076,
      "training_loss": 6.356472969055176
    },
    {
      "epoch": 0.6668834688346883,
      "step": 3076,
      "training_loss": 4.468757629394531
    },
    {
      "epoch": 0.6668834688346883,
      "step": 3076,
      "training_loss": 5.565701007843018
    },
    {
      "epoch": 0.6668834688346883,
      "step": 3076,
      "training_loss": 6.412494659423828
    },
    {
      "epoch": 0.6671002710027101,
      "step": 3077,
      "training_loss": 7.111991882324219
    },
    {
      "epoch": 0.6671002710027101,
      "step": 3077,
      "training_loss": 5.756130695343018
    },
    {
      "epoch": 0.6671002710027101,
      "step": 3077,
      "training_loss": 6.713816165924072
    },
    {
      "epoch": 0.6671002710027101,
      "step": 3077,
      "training_loss": 5.380738258361816
    },
    {
      "epoch": 0.6673170731707317,
      "step": 3078,
      "training_loss": 6.661794185638428
    },
    {
      "epoch": 0.6673170731707317,
      "step": 3078,
      "training_loss": 6.683243274688721
    },
    {
      "epoch": 0.6673170731707317,
      "step": 3078,
      "training_loss": 6.7097086906433105
    },
    {
      "epoch": 0.6673170731707317,
      "step": 3078,
      "training_loss": 4.090598106384277
    },
    {
      "epoch": 0.6675338753387534,
      "step": 3079,
      "training_loss": 6.399028301239014
    },
    {
      "epoch": 0.6675338753387534,
      "step": 3079,
      "training_loss": 6.950779914855957
    },
    {
      "epoch": 0.6675338753387534,
      "step": 3079,
      "training_loss": 4.686816215515137
    },
    {
      "epoch": 0.6675338753387534,
      "step": 3079,
      "training_loss": 7.5450615882873535
    },
    {
      "epoch": 0.6677506775067751,
      "grad_norm": 13.610796928405762,
      "learning_rate": 1e-05,
      "loss": 6.0933,
      "step": 3080
    },
    {
      "epoch": 0.6677506775067751,
      "step": 3080,
      "training_loss": 6.42907190322876
    },
    {
      "epoch": 0.6677506775067751,
      "step": 3080,
      "training_loss": 5.2902302742004395
    },
    {
      "epoch": 0.6677506775067751,
      "step": 3080,
      "training_loss": 5.626175403594971
    },
    {
      "epoch": 0.6677506775067751,
      "step": 3080,
      "training_loss": 7.1587677001953125
    },
    {
      "epoch": 0.6679674796747967,
      "step": 3081,
      "training_loss": 4.080526351928711
    },
    {
      "epoch": 0.6679674796747967,
      "step": 3081,
      "training_loss": 3.6274683475494385
    },
    {
      "epoch": 0.6679674796747967,
      "step": 3081,
      "training_loss": 6.508290767669678
    },
    {
      "epoch": 0.6679674796747967,
      "step": 3081,
      "training_loss": 5.371099472045898
    },
    {
      "epoch": 0.6681842818428184,
      "step": 3082,
      "training_loss": 6.501878261566162
    },
    {
      "epoch": 0.6681842818428184,
      "step": 3082,
      "training_loss": 6.842319965362549
    },
    {
      "epoch": 0.6681842818428184,
      "step": 3082,
      "training_loss": 5.580739498138428
    },
    {
      "epoch": 0.6681842818428184,
      "step": 3082,
      "training_loss": 6.53012228012085
    },
    {
      "epoch": 0.6684010840108401,
      "step": 3083,
      "training_loss": 6.791974067687988
    },
    {
      "epoch": 0.6684010840108401,
      "step": 3083,
      "training_loss": 7.2890305519104
    },
    {
      "epoch": 0.6684010840108401,
      "step": 3083,
      "training_loss": 6.22427225112915
    },
    {
      "epoch": 0.6684010840108401,
      "step": 3083,
      "training_loss": 5.43830680847168
    },
    {
      "epoch": 0.6686178861788618,
      "grad_norm": 17.966962814331055,
      "learning_rate": 1e-05,
      "loss": 5.9556,
      "step": 3084
    },
    {
      "epoch": 0.6686178861788618,
      "step": 3084,
      "training_loss": 3.0047340393066406
    },
    {
      "epoch": 0.6686178861788618,
      "step": 3084,
      "training_loss": 6.907637119293213
    },
    {
      "epoch": 0.6686178861788618,
      "step": 3084,
      "training_loss": 4.857019901275635
    },
    {
      "epoch": 0.6686178861788618,
      "step": 3084,
      "training_loss": 4.18945837020874
    },
    {
      "epoch": 0.6688346883468834,
      "step": 3085,
      "training_loss": 8.311240196228027
    },
    {
      "epoch": 0.6688346883468834,
      "step": 3085,
      "training_loss": 7.039810657501221
    },
    {
      "epoch": 0.6688346883468834,
      "step": 3085,
      "training_loss": 6.830885887145996
    },
    {
      "epoch": 0.6688346883468834,
      "step": 3085,
      "training_loss": 5.996768474578857
    },
    {
      "epoch": 0.6690514905149052,
      "step": 3086,
      "training_loss": 7.984892845153809
    },
    {
      "epoch": 0.6690514905149052,
      "step": 3086,
      "training_loss": 6.8430352210998535
    },
    {
      "epoch": 0.6690514905149052,
      "step": 3086,
      "training_loss": 6.9573798179626465
    },
    {
      "epoch": 0.6690514905149052,
      "step": 3086,
      "training_loss": 6.032603740692139
    },
    {
      "epoch": 0.6692682926829269,
      "step": 3087,
      "training_loss": 6.722532749176025
    },
    {
      "epoch": 0.6692682926829269,
      "step": 3087,
      "training_loss": 7.168262004852295
    },
    {
      "epoch": 0.6692682926829269,
      "step": 3087,
      "training_loss": 5.908613204956055
    },
    {
      "epoch": 0.6692682926829269,
      "step": 3087,
      "training_loss": 6.535340785980225
    },
    {
      "epoch": 0.6694850948509485,
      "grad_norm": 13.169787406921387,
      "learning_rate": 1e-05,
      "loss": 6.3306,
      "step": 3088
    },
    {
      "epoch": 0.6694850948509485,
      "step": 3088,
      "training_loss": 4.598085403442383
    },
    {
      "epoch": 0.6694850948509485,
      "step": 3088,
      "training_loss": 6.414288520812988
    },
    {
      "epoch": 0.6694850948509485,
      "step": 3088,
      "training_loss": 5.416852951049805
    },
    {
      "epoch": 0.6694850948509485,
      "step": 3088,
      "training_loss": 6.451944828033447
    },
    {
      "epoch": 0.6697018970189702,
      "step": 3089,
      "training_loss": 4.918238162994385
    },
    {
      "epoch": 0.6697018970189702,
      "step": 3089,
      "training_loss": 6.634623050689697
    },
    {
      "epoch": 0.6697018970189702,
      "step": 3089,
      "training_loss": 6.492696285247803
    },
    {
      "epoch": 0.6697018970189702,
      "step": 3089,
      "training_loss": 6.223952293395996
    },
    {
      "epoch": 0.6699186991869919,
      "step": 3090,
      "training_loss": 6.245282173156738
    },
    {
      "epoch": 0.6699186991869919,
      "step": 3090,
      "training_loss": 6.495223522186279
    },
    {
      "epoch": 0.6699186991869919,
      "step": 3090,
      "training_loss": 7.681828498840332
    },
    {
      "epoch": 0.6699186991869919,
      "step": 3090,
      "training_loss": 6.338657855987549
    },
    {
      "epoch": 0.6701355013550135,
      "step": 3091,
      "training_loss": 7.817529678344727
    },
    {
      "epoch": 0.6701355013550135,
      "step": 3091,
      "training_loss": 7.483732223510742
    },
    {
      "epoch": 0.6701355013550135,
      "step": 3091,
      "training_loss": 7.27288818359375
    },
    {
      "epoch": 0.6701355013550135,
      "step": 3091,
      "training_loss": 5.614839553833008
    },
    {
      "epoch": 0.6703523035230352,
      "grad_norm": 19.906944274902344,
      "learning_rate": 1e-05,
      "loss": 6.3813,
      "step": 3092
    },
    {
      "epoch": 0.6703523035230352,
      "step": 3092,
      "training_loss": 6.611439228057861
    },
    {
      "epoch": 0.6703523035230352,
      "step": 3092,
      "training_loss": 7.145713806152344
    },
    {
      "epoch": 0.6703523035230352,
      "step": 3092,
      "training_loss": 6.21562385559082
    },
    {
      "epoch": 0.6703523035230352,
      "step": 3092,
      "training_loss": 8.349774360656738
    },
    {
      "epoch": 0.6705691056910569,
      "step": 3093,
      "training_loss": 7.459904193878174
    },
    {
      "epoch": 0.6705691056910569,
      "step": 3093,
      "training_loss": 7.37667179107666
    },
    {
      "epoch": 0.6705691056910569,
      "step": 3093,
      "training_loss": 5.591455459594727
    },
    {
      "epoch": 0.6705691056910569,
      "step": 3093,
      "training_loss": 6.002462863922119
    },
    {
      "epoch": 0.6707859078590785,
      "step": 3094,
      "training_loss": 6.435764312744141
    },
    {
      "epoch": 0.6707859078590785,
      "step": 3094,
      "training_loss": 6.684426307678223
    },
    {
      "epoch": 0.6707859078590785,
      "step": 3094,
      "training_loss": 4.208645343780518
    },
    {
      "epoch": 0.6707859078590785,
      "step": 3094,
      "training_loss": 6.67254114151001
    },
    {
      "epoch": 0.6710027100271003,
      "step": 3095,
      "training_loss": 6.77530574798584
    },
    {
      "epoch": 0.6710027100271003,
      "step": 3095,
      "training_loss": 5.212102890014648
    },
    {
      "epoch": 0.6710027100271003,
      "step": 3095,
      "training_loss": 6.826341152191162
    },
    {
      "epoch": 0.6710027100271003,
      "step": 3095,
      "training_loss": 6.463475227355957
    },
    {
      "epoch": 0.671219512195122,
      "grad_norm": 15.440410614013672,
      "learning_rate": 1e-05,
      "loss": 6.502,
      "step": 3096
    },
    {
      "epoch": 0.671219512195122,
      "step": 3096,
      "training_loss": 8.58960247039795
    },
    {
      "epoch": 0.671219512195122,
      "step": 3096,
      "training_loss": 5.711355686187744
    },
    {
      "epoch": 0.671219512195122,
      "step": 3096,
      "training_loss": 5.8202738761901855
    },
    {
      "epoch": 0.671219512195122,
      "step": 3096,
      "training_loss": 6.695796489715576
    },
    {
      "epoch": 0.6714363143631437,
      "step": 3097,
      "training_loss": 6.940942287445068
    },
    {
      "epoch": 0.6714363143631437,
      "step": 3097,
      "training_loss": 6.576801300048828
    },
    {
      "epoch": 0.6714363143631437,
      "step": 3097,
      "training_loss": 7.266114234924316
    },
    {
      "epoch": 0.6714363143631437,
      "step": 3097,
      "training_loss": 6.661402702331543
    },
    {
      "epoch": 0.6716531165311653,
      "step": 3098,
      "training_loss": 7.500024795532227
    },
    {
      "epoch": 0.6716531165311653,
      "step": 3098,
      "training_loss": 4.95050048828125
    },
    {
      "epoch": 0.6716531165311653,
      "step": 3098,
      "training_loss": 7.304647922515869
    },
    {
      "epoch": 0.6716531165311653,
      "step": 3098,
      "training_loss": 5.7148003578186035
    },
    {
      "epoch": 0.671869918699187,
      "step": 3099,
      "training_loss": 6.2310872077941895
    },
    {
      "epoch": 0.671869918699187,
      "step": 3099,
      "training_loss": 6.600953578948975
    },
    {
      "epoch": 0.671869918699187,
      "step": 3099,
      "training_loss": 5.500154972076416
    },
    {
      "epoch": 0.671869918699187,
      "step": 3099,
      "training_loss": 7.4157867431640625
    },
    {
      "epoch": 0.6720867208672087,
      "grad_norm": 15.589550971984863,
      "learning_rate": 1e-05,
      "loss": 6.5925,
      "step": 3100
    },
    {
      "epoch": 0.6720867208672087,
      "step": 3100,
      "training_loss": 6.196331977844238
    },
    {
      "epoch": 0.6720867208672087,
      "step": 3100,
      "training_loss": 4.600197792053223
    },
    {
      "epoch": 0.6720867208672087,
      "step": 3100,
      "training_loss": 7.007596969604492
    },
    {
      "epoch": 0.6720867208672087,
      "step": 3100,
      "training_loss": 5.536837100982666
    },
    {
      "epoch": 0.6723035230352303,
      "step": 3101,
      "training_loss": 7.552234649658203
    },
    {
      "epoch": 0.6723035230352303,
      "step": 3101,
      "training_loss": 6.239105224609375
    },
    {
      "epoch": 0.6723035230352303,
      "step": 3101,
      "training_loss": 6.101812839508057
    },
    {
      "epoch": 0.6723035230352303,
      "step": 3101,
      "training_loss": 7.360145092010498
    },
    {
      "epoch": 0.672520325203252,
      "step": 3102,
      "training_loss": 8.228301048278809
    },
    {
      "epoch": 0.672520325203252,
      "step": 3102,
      "training_loss": 6.732165336608887
    },
    {
      "epoch": 0.672520325203252,
      "step": 3102,
      "training_loss": 6.613422870635986
    },
    {
      "epoch": 0.672520325203252,
      "step": 3102,
      "training_loss": 5.170454978942871
    },
    {
      "epoch": 0.6727371273712737,
      "step": 3103,
      "training_loss": 6.980696201324463
    },
    {
      "epoch": 0.6727371273712737,
      "step": 3103,
      "training_loss": 6.356090068817139
    },
    {
      "epoch": 0.6727371273712737,
      "step": 3103,
      "training_loss": 7.242621421813965
    },
    {
      "epoch": 0.6727371273712737,
      "step": 3103,
      "training_loss": 6.933463096618652
    },
    {
      "epoch": 0.6729539295392954,
      "grad_norm": 16.512649536132812,
      "learning_rate": 1e-05,
      "loss": 6.5532,
      "step": 3104
    },
    {
      "epoch": 0.6729539295392954,
      "step": 3104,
      "training_loss": 7.786803245544434
    },
    {
      "epoch": 0.6729539295392954,
      "step": 3104,
      "training_loss": 8.049779891967773
    },
    {
      "epoch": 0.6729539295392954,
      "step": 3104,
      "training_loss": 6.243863582611084
    },
    {
      "epoch": 0.6729539295392954,
      "step": 3104,
      "training_loss": 8.312129020690918
    },
    {
      "epoch": 0.6731707317073171,
      "step": 3105,
      "training_loss": 7.339915752410889
    },
    {
      "epoch": 0.6731707317073171,
      "step": 3105,
      "training_loss": 3.50651216506958
    },
    {
      "epoch": 0.6731707317073171,
      "step": 3105,
      "training_loss": 3.0690736770629883
    },
    {
      "epoch": 0.6731707317073171,
      "step": 3105,
      "training_loss": 6.676494121551514
    },
    {
      "epoch": 0.6733875338753388,
      "step": 3106,
      "training_loss": 4.286571502685547
    },
    {
      "epoch": 0.6733875338753388,
      "step": 3106,
      "training_loss": 5.2948689460754395
    },
    {
      "epoch": 0.6733875338753388,
      "step": 3106,
      "training_loss": 6.9824748039245605
    },
    {
      "epoch": 0.6733875338753388,
      "step": 3106,
      "training_loss": 6.811720848083496
    },
    {
      "epoch": 0.6736043360433605,
      "step": 3107,
      "training_loss": 5.420570373535156
    },
    {
      "epoch": 0.6736043360433605,
      "step": 3107,
      "training_loss": 5.468125343322754
    },
    {
      "epoch": 0.6736043360433605,
      "step": 3107,
      "training_loss": 7.959873199462891
    },
    {
      "epoch": 0.6736043360433605,
      "step": 3107,
      "training_loss": 6.212426662445068
    },
    {
      "epoch": 0.6738211382113821,
      "grad_norm": 15.847004890441895,
      "learning_rate": 1e-05,
      "loss": 6.2138,
      "step": 3108
    },
    {
      "epoch": 0.6738211382113821,
      "step": 3108,
      "training_loss": 8.350115776062012
    },
    {
      "epoch": 0.6738211382113821,
      "step": 3108,
      "training_loss": 6.262861728668213
    },
    {
      "epoch": 0.6738211382113821,
      "step": 3108,
      "training_loss": 6.805118560791016
    },
    {
      "epoch": 0.6738211382113821,
      "step": 3108,
      "training_loss": 6.073855876922607
    },
    {
      "epoch": 0.6740379403794038,
      "step": 3109,
      "training_loss": 6.713118076324463
    },
    {
      "epoch": 0.6740379403794038,
      "step": 3109,
      "training_loss": 6.8804426193237305
    },
    {
      "epoch": 0.6740379403794038,
      "step": 3109,
      "training_loss": 7.978744983673096
    },
    {
      "epoch": 0.6740379403794038,
      "step": 3109,
      "training_loss": 7.483536243438721
    },
    {
      "epoch": 0.6742547425474255,
      "step": 3110,
      "training_loss": 4.77001428604126
    },
    {
      "epoch": 0.6742547425474255,
      "step": 3110,
      "training_loss": 5.6529035568237305
    },
    {
      "epoch": 0.6742547425474255,
      "step": 3110,
      "training_loss": 6.989267826080322
    },
    {
      "epoch": 0.6742547425474255,
      "step": 3110,
      "training_loss": 2.895698070526123
    },
    {
      "epoch": 0.6744715447154471,
      "step": 3111,
      "training_loss": 7.173943996429443
    },
    {
      "epoch": 0.6744715447154471,
      "step": 3111,
      "training_loss": 6.899792194366455
    },
    {
      "epoch": 0.6744715447154471,
      "step": 3111,
      "training_loss": 4.426736831665039
    },
    {
      "epoch": 0.6744715447154471,
      "step": 3111,
      "training_loss": 4.385528087615967
    },
    {
      "epoch": 0.6746883468834688,
      "grad_norm": 15.669575691223145,
      "learning_rate": 1e-05,
      "loss": 6.2339,
      "step": 3112
    },
    {
      "epoch": 0.6746883468834688,
      "step": 3112,
      "training_loss": 6.483199596405029
    },
    {
      "epoch": 0.6746883468834688,
      "step": 3112,
      "training_loss": 7.328729629516602
    },
    {
      "epoch": 0.6746883468834688,
      "step": 3112,
      "training_loss": 5.76930046081543
    },
    {
      "epoch": 0.6746883468834688,
      "step": 3112,
      "training_loss": 7.084805965423584
    },
    {
      "epoch": 0.6749051490514905,
      "step": 3113,
      "training_loss": 7.014610290527344
    },
    {
      "epoch": 0.6749051490514905,
      "step": 3113,
      "training_loss": 5.962004661560059
    },
    {
      "epoch": 0.6749051490514905,
      "step": 3113,
      "training_loss": 6.337619304656982
    },
    {
      "epoch": 0.6749051490514905,
      "step": 3113,
      "training_loss": 4.5312395095825195
    },
    {
      "epoch": 0.6751219512195122,
      "step": 3114,
      "training_loss": 6.751674175262451
    },
    {
      "epoch": 0.6751219512195122,
      "step": 3114,
      "training_loss": 5.443600654602051
    },
    {
      "epoch": 0.6751219512195122,
      "step": 3114,
      "training_loss": 7.395492076873779
    },
    {
      "epoch": 0.6751219512195122,
      "step": 3114,
      "training_loss": 2.952622652053833
    },
    {
      "epoch": 0.6753387533875339,
      "step": 3115,
      "training_loss": 6.287081241607666
    },
    {
      "epoch": 0.6753387533875339,
      "step": 3115,
      "training_loss": 6.700987339019775
    },
    {
      "epoch": 0.6753387533875339,
      "step": 3115,
      "training_loss": 7.302450656890869
    },
    {
      "epoch": 0.6753387533875339,
      "step": 3115,
      "training_loss": 6.400540351867676
    },
    {
      "epoch": 0.6755555555555556,
      "grad_norm": 19.558155059814453,
      "learning_rate": 1e-05,
      "loss": 6.2341,
      "step": 3116
    },
    {
      "epoch": 0.6755555555555556,
      "step": 3116,
      "training_loss": 6.760823726654053
    },
    {
      "epoch": 0.6755555555555556,
      "step": 3116,
      "training_loss": 7.404651165008545
    },
    {
      "epoch": 0.6755555555555556,
      "step": 3116,
      "training_loss": 5.148487567901611
    },
    {
      "epoch": 0.6755555555555556,
      "step": 3116,
      "training_loss": 6.979905128479004
    },
    {
      "epoch": 0.6757723577235772,
      "step": 3117,
      "training_loss": 7.778029441833496
    },
    {
      "epoch": 0.6757723577235772,
      "step": 3117,
      "training_loss": 7.770237445831299
    },
    {
      "epoch": 0.6757723577235772,
      "step": 3117,
      "training_loss": 6.992986679077148
    },
    {
      "epoch": 0.6757723577235772,
      "step": 3117,
      "training_loss": 6.27738094329834
    },
    {
      "epoch": 0.6759891598915989,
      "step": 3118,
      "training_loss": 6.964068412780762
    },
    {
      "epoch": 0.6759891598915989,
      "step": 3118,
      "training_loss": 6.010751724243164
    },
    {
      "epoch": 0.6759891598915989,
      "step": 3118,
      "training_loss": 7.283485412597656
    },
    {
      "epoch": 0.6759891598915989,
      "step": 3118,
      "training_loss": 8.659682273864746
    },
    {
      "epoch": 0.6762059620596206,
      "step": 3119,
      "training_loss": 6.124867916107178
    },
    {
      "epoch": 0.6762059620596206,
      "step": 3119,
      "training_loss": 4.081178665161133
    },
    {
      "epoch": 0.6762059620596206,
      "step": 3119,
      "training_loss": 7.344035625457764
    },
    {
      "epoch": 0.6762059620596206,
      "step": 3119,
      "training_loss": 6.842785835266113
    },
    {
      "epoch": 0.6764227642276422,
      "grad_norm": 24.89723014831543,
      "learning_rate": 1e-05,
      "loss": 6.7765,
      "step": 3120
    },
    {
      "epoch": 0.6764227642276422,
      "step": 3120,
      "training_loss": 6.185363292694092
    },
    {
      "epoch": 0.6764227642276422,
      "step": 3120,
      "training_loss": 6.885583877563477
    },
    {
      "epoch": 0.6764227642276422,
      "step": 3120,
      "training_loss": 6.537623405456543
    },
    {
      "epoch": 0.6764227642276422,
      "step": 3120,
      "training_loss": 9.306884765625
    },
    {
      "epoch": 0.6766395663956639,
      "step": 3121,
      "training_loss": 6.5051960945129395
    },
    {
      "epoch": 0.6766395663956639,
      "step": 3121,
      "training_loss": 6.45242166519165
    },
    {
      "epoch": 0.6766395663956639,
      "step": 3121,
      "training_loss": 6.572022438049316
    },
    {
      "epoch": 0.6766395663956639,
      "step": 3121,
      "training_loss": 6.143655776977539
    },
    {
      "epoch": 0.6768563685636856,
      "step": 3122,
      "training_loss": 5.599301338195801
    },
    {
      "epoch": 0.6768563685636856,
      "step": 3122,
      "training_loss": 5.316016674041748
    },
    {
      "epoch": 0.6768563685636856,
      "step": 3122,
      "training_loss": 6.875136375427246
    },
    {
      "epoch": 0.6768563685636856,
      "step": 3122,
      "training_loss": 7.650694847106934
    },
    {
      "epoch": 0.6770731707317074,
      "step": 3123,
      "training_loss": 6.859747886657715
    },
    {
      "epoch": 0.6770731707317074,
      "step": 3123,
      "training_loss": 6.885382652282715
    },
    {
      "epoch": 0.6770731707317074,
      "step": 3123,
      "training_loss": 6.625313758850098
    },
    {
      "epoch": 0.6770731707317074,
      "step": 3123,
      "training_loss": 6.2502336502075195
    },
    {
      "epoch": 0.677289972899729,
      "grad_norm": 20.303375244140625,
      "learning_rate": 1e-05,
      "loss": 6.6657,
      "step": 3124
    },
    {
      "epoch": 0.677289972899729,
      "step": 3124,
      "training_loss": 7.439310550689697
    },
    {
      "epoch": 0.677289972899729,
      "step": 3124,
      "training_loss": 6.0379228591918945
    },
    {
      "epoch": 0.677289972899729,
      "step": 3124,
      "training_loss": 5.222084045410156
    },
    {
      "epoch": 0.677289972899729,
      "step": 3124,
      "training_loss": 7.056572914123535
    },
    {
      "epoch": 0.6775067750677507,
      "step": 3125,
      "training_loss": 7.141515254974365
    },
    {
      "epoch": 0.6775067750677507,
      "step": 3125,
      "training_loss": 6.433114051818848
    },
    {
      "epoch": 0.6775067750677507,
      "step": 3125,
      "training_loss": 7.1587910652160645
    },
    {
      "epoch": 0.6775067750677507,
      "step": 3125,
      "training_loss": 6.930803298950195
    },
    {
      "epoch": 0.6777235772357724,
      "step": 3126,
      "training_loss": 7.01749849319458
    },
    {
      "epoch": 0.6777235772357724,
      "step": 3126,
      "training_loss": 4.846793174743652
    },
    {
      "epoch": 0.6777235772357724,
      "step": 3126,
      "training_loss": 4.163578033447266
    },
    {
      "epoch": 0.6777235772357724,
      "step": 3126,
      "training_loss": 5.110513687133789
    },
    {
      "epoch": 0.677940379403794,
      "step": 3127,
      "training_loss": 5.339785099029541
    },
    {
      "epoch": 0.677940379403794,
      "step": 3127,
      "training_loss": 6.379584789276123
    },
    {
      "epoch": 0.677940379403794,
      "step": 3127,
      "training_loss": 6.912178993225098
    },
    {
      "epoch": 0.677940379403794,
      "step": 3127,
      "training_loss": 6.96780252456665
    },
    {
      "epoch": 0.6781571815718157,
      "grad_norm": 17.801467895507812,
      "learning_rate": 1e-05,
      "loss": 6.2599,
      "step": 3128
    },
    {
      "epoch": 0.6781571815718157,
      "step": 3128,
      "training_loss": 7.590075492858887
    },
    {
      "epoch": 0.6781571815718157,
      "step": 3128,
      "training_loss": 5.7913432121276855
    },
    {
      "epoch": 0.6781571815718157,
      "step": 3128,
      "training_loss": 6.082973480224609
    },
    {
      "epoch": 0.6781571815718157,
      "step": 3128,
      "training_loss": 6.415534973144531
    },
    {
      "epoch": 0.6783739837398374,
      "step": 3129,
      "training_loss": 7.464682579040527
    },
    {
      "epoch": 0.6783739837398374,
      "step": 3129,
      "training_loss": 7.091795444488525
    },
    {
      "epoch": 0.6783739837398374,
      "step": 3129,
      "training_loss": 6.739763259887695
    },
    {
      "epoch": 0.6783739837398374,
      "step": 3129,
      "training_loss": 7.25267219543457
    },
    {
      "epoch": 0.678590785907859,
      "step": 3130,
      "training_loss": 6.665768623352051
    },
    {
      "epoch": 0.678590785907859,
      "step": 3130,
      "training_loss": 5.743488311767578
    },
    {
      "epoch": 0.678590785907859,
      "step": 3130,
      "training_loss": 4.678687572479248
    },
    {
      "epoch": 0.678590785907859,
      "step": 3130,
      "training_loss": 6.929443359375
    },
    {
      "epoch": 0.6788075880758807,
      "step": 3131,
      "training_loss": 7.94763708114624
    },
    {
      "epoch": 0.6788075880758807,
      "step": 3131,
      "training_loss": 6.016171455383301
    },
    {
      "epoch": 0.6788075880758807,
      "step": 3131,
      "training_loss": 7.7231855392456055
    },
    {
      "epoch": 0.6788075880758807,
      "step": 3131,
      "training_loss": 6.275125980377197
    },
    {
      "epoch": 0.6790243902439025,
      "grad_norm": 14.893044471740723,
      "learning_rate": 1e-05,
      "loss": 6.6505,
      "step": 3132
    },
    {
      "epoch": 0.6790243902439025,
      "step": 3132,
      "training_loss": 5.220901012420654
    },
    {
      "epoch": 0.6790243902439025,
      "step": 3132,
      "training_loss": 7.328845977783203
    },
    {
      "epoch": 0.6790243902439025,
      "step": 3132,
      "training_loss": 7.005304336547852
    },
    {
      "epoch": 0.6790243902439025,
      "step": 3132,
      "training_loss": 5.592543125152588
    },
    {
      "epoch": 0.6792411924119242,
      "step": 3133,
      "training_loss": 5.607308387756348
    },
    {
      "epoch": 0.6792411924119242,
      "step": 3133,
      "training_loss": 5.787682056427002
    },
    {
      "epoch": 0.6792411924119242,
      "step": 3133,
      "training_loss": 6.387479305267334
    },
    {
      "epoch": 0.6792411924119242,
      "step": 3133,
      "training_loss": 6.98956298828125
    },
    {
      "epoch": 0.6794579945799458,
      "step": 3134,
      "training_loss": 7.219797134399414
    },
    {
      "epoch": 0.6794579945799458,
      "step": 3134,
      "training_loss": 6.5099101066589355
    },
    {
      "epoch": 0.6794579945799458,
      "step": 3134,
      "training_loss": 8.26040267944336
    },
    {
      "epoch": 0.6794579945799458,
      "step": 3134,
      "training_loss": 6.27534818649292
    },
    {
      "epoch": 0.6796747967479675,
      "step": 3135,
      "training_loss": 6.4939775466918945
    },
    {
      "epoch": 0.6796747967479675,
      "step": 3135,
      "training_loss": 7.033634662628174
    },
    {
      "epoch": 0.6796747967479675,
      "step": 3135,
      "training_loss": 6.459213733673096
    },
    {
      "epoch": 0.6796747967479675,
      "step": 3135,
      "training_loss": 7.587006092071533
    },
    {
      "epoch": 0.6798915989159892,
      "grad_norm": 16.33784294128418,
      "learning_rate": 1e-05,
      "loss": 6.6099,
      "step": 3136
    },
    {
      "epoch": 0.6798915989159892,
      "step": 3136,
      "training_loss": 6.72605562210083
    },
    {
      "epoch": 0.6798915989159892,
      "step": 3136,
      "training_loss": 6.785032272338867
    },
    {
      "epoch": 0.6798915989159892,
      "step": 3136,
      "training_loss": 6.864444732666016
    },
    {
      "epoch": 0.6798915989159892,
      "step": 3136,
      "training_loss": 5.53523063659668
    },
    {
      "epoch": 0.6801084010840108,
      "step": 3137,
      "training_loss": 5.747213840484619
    },
    {
      "epoch": 0.6801084010840108,
      "step": 3137,
      "training_loss": 7.132681369781494
    },
    {
      "epoch": 0.6801084010840108,
      "step": 3137,
      "training_loss": 6.792864799499512
    },
    {
      "epoch": 0.6801084010840108,
      "step": 3137,
      "training_loss": 7.650155067443848
    },
    {
      "epoch": 0.6803252032520325,
      "step": 3138,
      "training_loss": 7.626125812530518
    },
    {
      "epoch": 0.6803252032520325,
      "step": 3138,
      "training_loss": 6.183080673217773
    },
    {
      "epoch": 0.6803252032520325,
      "step": 3138,
      "training_loss": 7.48746919631958
    },
    {
      "epoch": 0.6803252032520325,
      "step": 3138,
      "training_loss": 6.108120441436768
    },
    {
      "epoch": 0.6805420054200542,
      "step": 3139,
      "training_loss": 6.187135696411133
    },
    {
      "epoch": 0.6805420054200542,
      "step": 3139,
      "training_loss": 5.709400177001953
    },
    {
      "epoch": 0.6805420054200542,
      "step": 3139,
      "training_loss": 6.2205810546875
    },
    {
      "epoch": 0.6805420054200542,
      "step": 3139,
      "training_loss": 4.233829975128174
    },
    {
      "epoch": 0.6807588075880758,
      "grad_norm": 16.015316009521484,
      "learning_rate": 1e-05,
      "loss": 6.4368,
      "step": 3140
    },
    {
      "epoch": 0.6807588075880758,
      "step": 3140,
      "training_loss": 7.586705684661865
    },
    {
      "epoch": 0.6807588075880758,
      "step": 3140,
      "training_loss": 6.238997459411621
    },
    {
      "epoch": 0.6807588075880758,
      "step": 3140,
      "training_loss": 6.186690807342529
    },
    {
      "epoch": 0.6807588075880758,
      "step": 3140,
      "training_loss": 7.0112624168396
    },
    {
      "epoch": 0.6809756097560976,
      "step": 3141,
      "training_loss": 6.173752784729004
    },
    {
      "epoch": 0.6809756097560976,
      "step": 3141,
      "training_loss": 7.231212615966797
    },
    {
      "epoch": 0.6809756097560976,
      "step": 3141,
      "training_loss": 5.28575325012207
    },
    {
      "epoch": 0.6809756097560976,
      "step": 3141,
      "training_loss": 6.581515789031982
    },
    {
      "epoch": 0.6811924119241193,
      "step": 3142,
      "training_loss": 5.916955471038818
    },
    {
      "epoch": 0.6811924119241193,
      "step": 3142,
      "training_loss": 5.82343053817749
    },
    {
      "epoch": 0.6811924119241193,
      "step": 3142,
      "training_loss": 3.718810796737671
    },
    {
      "epoch": 0.6811924119241193,
      "step": 3142,
      "training_loss": 7.542102813720703
    },
    {
      "epoch": 0.681409214092141,
      "step": 3143,
      "training_loss": 8.116446495056152
    },
    {
      "epoch": 0.681409214092141,
      "step": 3143,
      "training_loss": 5.297613143920898
    },
    {
      "epoch": 0.681409214092141,
      "step": 3143,
      "training_loss": 6.351071357727051
    },
    {
      "epoch": 0.681409214092141,
      "step": 3143,
      "training_loss": 5.605363845825195
    },
    {
      "epoch": 0.6816260162601626,
      "grad_norm": 15.922858238220215,
      "learning_rate": 1e-05,
      "loss": 6.2917,
      "step": 3144
    },
    {
      "epoch": 0.6816260162601626,
      "step": 3144,
      "training_loss": 6.6176910400390625
    },
    {
      "epoch": 0.6816260162601626,
      "step": 3144,
      "training_loss": 6.317368984222412
    },
    {
      "epoch": 0.6816260162601626,
      "step": 3144,
      "training_loss": 7.016055583953857
    },
    {
      "epoch": 0.6816260162601626,
      "step": 3144,
      "training_loss": 7.1829118728637695
    },
    {
      "epoch": 0.6818428184281843,
      "step": 3145,
      "training_loss": 7.274691104888916
    },
    {
      "epoch": 0.6818428184281843,
      "step": 3145,
      "training_loss": 5.990079879760742
    },
    {
      "epoch": 0.6818428184281843,
      "step": 3145,
      "training_loss": 6.34380578994751
    },
    {
      "epoch": 0.6818428184281843,
      "step": 3145,
      "training_loss": 5.908628463745117
    },
    {
      "epoch": 0.682059620596206,
      "step": 3146,
      "training_loss": 8.066798210144043
    },
    {
      "epoch": 0.682059620596206,
      "step": 3146,
      "training_loss": 6.289304733276367
    },
    {
      "epoch": 0.682059620596206,
      "step": 3146,
      "training_loss": 7.513730525970459
    },
    {
      "epoch": 0.682059620596206,
      "step": 3146,
      "training_loss": 7.5930399894714355
    },
    {
      "epoch": 0.6822764227642276,
      "step": 3147,
      "training_loss": 6.084672451019287
    },
    {
      "epoch": 0.6822764227642276,
      "step": 3147,
      "training_loss": 6.785916328430176
    },
    {
      "epoch": 0.6822764227642276,
      "step": 3147,
      "training_loss": 4.493731498718262
    },
    {
      "epoch": 0.6822764227642276,
      "step": 3147,
      "training_loss": 6.653501987457275
    },
    {
      "epoch": 0.6824932249322493,
      "grad_norm": 21.5518856048584,
      "learning_rate": 1e-05,
      "loss": 6.6332,
      "step": 3148
    },
    {
      "epoch": 0.6824932249322493,
      "step": 3148,
      "training_loss": 6.429322242736816
    },
    {
      "epoch": 0.6824932249322493,
      "step": 3148,
      "training_loss": 5.925361156463623
    },
    {
      "epoch": 0.6824932249322493,
      "step": 3148,
      "training_loss": 6.400763511657715
    },
    {
      "epoch": 0.6824932249322493,
      "step": 3148,
      "training_loss": 5.866135120391846
    },
    {
      "epoch": 0.682710027100271,
      "step": 3149,
      "training_loss": 3.2883617877960205
    },
    {
      "epoch": 0.682710027100271,
      "step": 3149,
      "training_loss": 6.572234153747559
    },
    {
      "epoch": 0.682710027100271,
      "step": 3149,
      "training_loss": 8.22091007232666
    },
    {
      "epoch": 0.682710027100271,
      "step": 3149,
      "training_loss": 6.524672031402588
    },
    {
      "epoch": 0.6829268292682927,
      "step": 3150,
      "training_loss": 5.75614070892334
    },
    {
      "epoch": 0.6829268292682927,
      "step": 3150,
      "training_loss": 7.019603729248047
    },
    {
      "epoch": 0.6829268292682927,
      "step": 3150,
      "training_loss": 3.668734550476074
    },
    {
      "epoch": 0.6829268292682927,
      "step": 3150,
      "training_loss": 5.920408248901367
    },
    {
      "epoch": 0.6831436314363144,
      "step": 3151,
      "training_loss": 6.3173017501831055
    },
    {
      "epoch": 0.6831436314363144,
      "step": 3151,
      "training_loss": 6.285510540008545
    },
    {
      "epoch": 0.6831436314363144,
      "step": 3151,
      "training_loss": 3.683858633041382
    },
    {
      "epoch": 0.6831436314363144,
      "step": 3151,
      "training_loss": 5.892520904541016
    },
    {
      "epoch": 0.6833604336043361,
      "grad_norm": 21.429851531982422,
      "learning_rate": 1e-05,
      "loss": 5.8607,
      "step": 3152
    },
    {
      "epoch": 0.6833604336043361,
      "step": 3152,
      "training_loss": 6.133376121520996
    },
    {
      "epoch": 0.6833604336043361,
      "step": 3152,
      "training_loss": 6.982987880706787
    },
    {
      "epoch": 0.6833604336043361,
      "step": 3152,
      "training_loss": 6.530012130737305
    },
    {
      "epoch": 0.6833604336043361,
      "step": 3152,
      "training_loss": 6.322543144226074
    },
    {
      "epoch": 0.6835772357723577,
      "step": 3153,
      "training_loss": 6.2373199462890625
    },
    {
      "epoch": 0.6835772357723577,
      "step": 3153,
      "training_loss": 7.4961748123168945
    },
    {
      "epoch": 0.6835772357723577,
      "step": 3153,
      "training_loss": 6.6375732421875
    },
    {
      "epoch": 0.6835772357723577,
      "step": 3153,
      "training_loss": 7.761486053466797
    },
    {
      "epoch": 0.6837940379403794,
      "step": 3154,
      "training_loss": 7.078985214233398
    },
    {
      "epoch": 0.6837940379403794,
      "step": 3154,
      "training_loss": 3.1183698177337646
    },
    {
      "epoch": 0.6837940379403794,
      "step": 3154,
      "training_loss": 7.031432151794434
    },
    {
      "epoch": 0.6837940379403794,
      "step": 3154,
      "training_loss": 6.897679805755615
    },
    {
      "epoch": 0.6840108401084011,
      "step": 3155,
      "training_loss": 5.194997787475586
    },
    {
      "epoch": 0.6840108401084011,
      "step": 3155,
      "training_loss": 6.797075271606445
    },
    {
      "epoch": 0.6840108401084011,
      "step": 3155,
      "training_loss": 6.590002536773682
    },
    {
      "epoch": 0.6840108401084011,
      "step": 3155,
      "training_loss": 5.416676044464111
    },
    {
      "epoch": 0.6842276422764227,
      "grad_norm": 12.288631439208984,
      "learning_rate": 1e-05,
      "loss": 6.3892,
      "step": 3156
    },
    {
      "epoch": 0.6842276422764227,
      "step": 3156,
      "training_loss": 5.501023769378662
    },
    {
      "epoch": 0.6842276422764227,
      "step": 3156,
      "training_loss": 5.229869842529297
    },
    {
      "epoch": 0.6842276422764227,
      "step": 3156,
      "training_loss": 2.8335559368133545
    },
    {
      "epoch": 0.6842276422764227,
      "step": 3156,
      "training_loss": 5.8609514236450195
    },
    {
      "epoch": 0.6844444444444444,
      "step": 3157,
      "training_loss": 5.909180641174316
    },
    {
      "epoch": 0.6844444444444444,
      "step": 3157,
      "training_loss": 5.509745121002197
    },
    {
      "epoch": 0.6844444444444444,
      "step": 3157,
      "training_loss": 7.142965793609619
    },
    {
      "epoch": 0.6844444444444444,
      "step": 3157,
      "training_loss": 6.103604316711426
    },
    {
      "epoch": 0.6846612466124661,
      "step": 3158,
      "training_loss": 5.260125637054443
    },
    {
      "epoch": 0.6846612466124661,
      "step": 3158,
      "training_loss": 6.83688497543335
    },
    {
      "epoch": 0.6846612466124661,
      "step": 3158,
      "training_loss": 7.439049243927002
    },
    {
      "epoch": 0.6846612466124661,
      "step": 3158,
      "training_loss": 6.955148220062256
    },
    {
      "epoch": 0.6848780487804879,
      "step": 3159,
      "training_loss": 7.003152370452881
    },
    {
      "epoch": 0.6848780487804879,
      "step": 3159,
      "training_loss": 6.977590560913086
    },
    {
      "epoch": 0.6848780487804879,
      "step": 3159,
      "training_loss": 7.388129711151123
    },
    {
      "epoch": 0.6848780487804879,
      "step": 3159,
      "training_loss": 6.777059078216553
    },
    {
      "epoch": 0.6850948509485095,
      "grad_norm": 15.932578086853027,
      "learning_rate": 1e-05,
      "loss": 6.1705,
      "step": 3160
    },
    {
      "epoch": 0.6850948509485095,
      "step": 3160,
      "training_loss": 7.36143684387207
    },
    {
      "epoch": 0.6850948509485095,
      "step": 3160,
      "training_loss": 5.435733795166016
    },
    {
      "epoch": 0.6850948509485095,
      "step": 3160,
      "training_loss": 5.752313137054443
    },
    {
      "epoch": 0.6850948509485095,
      "step": 3160,
      "training_loss": 6.478646755218506
    },
    {
      "epoch": 0.6853116531165312,
      "step": 3161,
      "training_loss": 6.805517196655273
    },
    {
      "epoch": 0.6853116531165312,
      "step": 3161,
      "training_loss": 6.474625587463379
    },
    {
      "epoch": 0.6853116531165312,
      "step": 3161,
      "training_loss": 7.299375057220459
    },
    {
      "epoch": 0.6853116531165312,
      "step": 3161,
      "training_loss": 5.093585968017578
    },
    {
      "epoch": 0.6855284552845529,
      "step": 3162,
      "training_loss": 6.513754844665527
    },
    {
      "epoch": 0.6855284552845529,
      "step": 3162,
      "training_loss": 6.609193325042725
    },
    {
      "epoch": 0.6855284552845529,
      "step": 3162,
      "training_loss": 7.628345489501953
    },
    {
      "epoch": 0.6855284552845529,
      "step": 3162,
      "training_loss": 2.7956960201263428
    },
    {
      "epoch": 0.6857452574525745,
      "step": 3163,
      "training_loss": 6.583916664123535
    },
    {
      "epoch": 0.6857452574525745,
      "step": 3163,
      "training_loss": 5.760518550872803
    },
    {
      "epoch": 0.6857452574525745,
      "step": 3163,
      "training_loss": 6.32039737701416
    },
    {
      "epoch": 0.6857452574525745,
      "step": 3163,
      "training_loss": 8.427166938781738
    },
    {
      "epoch": 0.6859620596205962,
      "grad_norm": 18.34634017944336,
      "learning_rate": 1e-05,
      "loss": 6.3338,
      "step": 3164
    },
    {
      "epoch": 0.6859620596205962,
      "step": 3164,
      "training_loss": 7.449565887451172
    },
    {
      "epoch": 0.6859620596205962,
      "step": 3164,
      "training_loss": 6.171326160430908
    },
    {
      "epoch": 0.6859620596205962,
      "step": 3164,
      "training_loss": 6.297513961791992
    },
    {
      "epoch": 0.6859620596205962,
      "step": 3164,
      "training_loss": 5.110587120056152
    },
    {
      "epoch": 0.6861788617886179,
      "step": 3165,
      "training_loss": 7.592785358428955
    },
    {
      "epoch": 0.6861788617886179,
      "step": 3165,
      "training_loss": 7.2288665771484375
    },
    {
      "epoch": 0.6861788617886179,
      "step": 3165,
      "training_loss": 4.2179999351501465
    },
    {
      "epoch": 0.6861788617886179,
      "step": 3165,
      "training_loss": 6.931600570678711
    },
    {
      "epoch": 0.6863956639566395,
      "step": 3166,
      "training_loss": 7.867823600769043
    },
    {
      "epoch": 0.6863956639566395,
      "step": 3166,
      "training_loss": 6.606650352478027
    },
    {
      "epoch": 0.6863956639566395,
      "step": 3166,
      "training_loss": 5.921080589294434
    },
    {
      "epoch": 0.6863956639566395,
      "step": 3166,
      "training_loss": 6.144138336181641
    },
    {
      "epoch": 0.6866124661246612,
      "step": 3167,
      "training_loss": 6.325847625732422
    },
    {
      "epoch": 0.6866124661246612,
      "step": 3167,
      "training_loss": 6.275651931762695
    },
    {
      "epoch": 0.6866124661246612,
      "step": 3167,
      "training_loss": 6.407961368560791
    },
    {
      "epoch": 0.6866124661246612,
      "step": 3167,
      "training_loss": 7.737968921661377
    },
    {
      "epoch": 0.686829268292683,
      "grad_norm": 13.454208374023438,
      "learning_rate": 1e-05,
      "loss": 6.518,
      "step": 3168
    },
    {
      "epoch": 0.686829268292683,
      "step": 3168,
      "training_loss": 6.201971530914307
    },
    {
      "epoch": 0.686829268292683,
      "step": 3168,
      "training_loss": 6.119631290435791
    },
    {
      "epoch": 0.686829268292683,
      "step": 3168,
      "training_loss": 5.690219879150391
    },
    {
      "epoch": 0.686829268292683,
      "step": 3168,
      "training_loss": 6.250534534454346
    },
    {
      "epoch": 0.6870460704607046,
      "step": 3169,
      "training_loss": 2.710069417953491
    },
    {
      "epoch": 0.6870460704607046,
      "step": 3169,
      "training_loss": 6.970867156982422
    },
    {
      "epoch": 0.6870460704607046,
      "step": 3169,
      "training_loss": 7.192995548248291
    },
    {
      "epoch": 0.6870460704607046,
      "step": 3169,
      "training_loss": 7.763612270355225
    },
    {
      "epoch": 0.6872628726287263,
      "step": 3170,
      "training_loss": 7.044677734375
    },
    {
      "epoch": 0.6872628726287263,
      "step": 3170,
      "training_loss": 8.013686180114746
    },
    {
      "epoch": 0.6872628726287263,
      "step": 3170,
      "training_loss": 6.369525909423828
    },
    {
      "epoch": 0.6872628726287263,
      "step": 3170,
      "training_loss": 6.409629821777344
    },
    {
      "epoch": 0.687479674796748,
      "step": 3171,
      "training_loss": 7.584018230438232
    },
    {
      "epoch": 0.687479674796748,
      "step": 3171,
      "training_loss": 5.761322021484375
    },
    {
      "epoch": 0.687479674796748,
      "step": 3171,
      "training_loss": 6.466237545013428
    },
    {
      "epoch": 0.687479674796748,
      "step": 3171,
      "training_loss": 7.276576519012451
    },
    {
      "epoch": 0.6876964769647697,
      "grad_norm": 15.658393859863281,
      "learning_rate": 1e-05,
      "loss": 6.4891,
      "step": 3172
    },
    {
      "epoch": 0.6876964769647697,
      "step": 3172,
      "training_loss": 5.545997619628906
    },
    {
      "epoch": 0.6876964769647697,
      "step": 3172,
      "training_loss": 5.184842109680176
    },
    {
      "epoch": 0.6876964769647697,
      "step": 3172,
      "training_loss": 2.7165157794952393
    },
    {
      "epoch": 0.6876964769647697,
      "step": 3172,
      "training_loss": 5.45079231262207
    },
    {
      "epoch": 0.6879132791327913,
      "step": 3173,
      "training_loss": 7.0494489669799805
    },
    {
      "epoch": 0.6879132791327913,
      "step": 3173,
      "training_loss": 6.180213928222656
    },
    {
      "epoch": 0.6879132791327913,
      "step": 3173,
      "training_loss": 7.255246162414551
    },
    {
      "epoch": 0.6879132791327913,
      "step": 3173,
      "training_loss": 6.776115417480469
    },
    {
      "epoch": 0.688130081300813,
      "step": 3174,
      "training_loss": 8.637771606445312
    },
    {
      "epoch": 0.688130081300813,
      "step": 3174,
      "training_loss": 5.960609436035156
    },
    {
      "epoch": 0.688130081300813,
      "step": 3174,
      "training_loss": 5.431795120239258
    },
    {
      "epoch": 0.688130081300813,
      "step": 3174,
      "training_loss": 7.301187515258789
    },
    {
      "epoch": 0.6883468834688347,
      "step": 3175,
      "training_loss": 3.5809714794158936
    },
    {
      "epoch": 0.6883468834688347,
      "step": 3175,
      "training_loss": 8.674053192138672
    },
    {
      "epoch": 0.6883468834688347,
      "step": 3175,
      "training_loss": 3.891885280609131
    },
    {
      "epoch": 0.6883468834688347,
      "step": 3175,
      "training_loss": 3.0698142051696777
    },
    {
      "epoch": 0.6885636856368563,
      "grad_norm": 16.917268753051758,
      "learning_rate": 1e-05,
      "loss": 5.7942,
      "step": 3176
    },
    {
      "epoch": 0.6885636856368563,
      "step": 3176,
      "training_loss": 6.6755571365356445
    },
    {
      "epoch": 0.6885636856368563,
      "step": 3176,
      "training_loss": 7.132129669189453
    },
    {
      "epoch": 0.6885636856368563,
      "step": 3176,
      "training_loss": 5.436747074127197
    },
    {
      "epoch": 0.6885636856368563,
      "step": 3176,
      "training_loss": 7.813081741333008
    },
    {
      "epoch": 0.688780487804878,
      "step": 3177,
      "training_loss": 6.478958606719971
    },
    {
      "epoch": 0.688780487804878,
      "step": 3177,
      "training_loss": 6.625076770782471
    },
    {
      "epoch": 0.688780487804878,
      "step": 3177,
      "training_loss": 7.9178690910339355
    },
    {
      "epoch": 0.688780487804878,
      "step": 3177,
      "training_loss": 6.045764446258545
    },
    {
      "epoch": 0.6889972899728998,
      "step": 3178,
      "training_loss": 6.9487080574035645
    },
    {
      "epoch": 0.6889972899728998,
      "step": 3178,
      "training_loss": 6.751544952392578
    },
    {
      "epoch": 0.6889972899728998,
      "step": 3178,
      "training_loss": 5.6154985427856445
    },
    {
      "epoch": 0.6889972899728998,
      "step": 3178,
      "training_loss": 6.950141429901123
    },
    {
      "epoch": 0.6892140921409214,
      "step": 3179,
      "training_loss": 6.915116310119629
    },
    {
      "epoch": 0.6892140921409214,
      "step": 3179,
      "training_loss": 6.280455589294434
    },
    {
      "epoch": 0.6892140921409214,
      "step": 3179,
      "training_loss": 5.897075176239014
    },
    {
      "epoch": 0.6892140921409214,
      "step": 3179,
      "training_loss": 6.665713787078857
    },
    {
      "epoch": 0.6894308943089431,
      "grad_norm": 14.99804973602295,
      "learning_rate": 1e-05,
      "loss": 6.6343,
      "step": 3180
    },
    {
      "epoch": 0.6894308943089431,
      "step": 3180,
      "training_loss": 6.95599889755249
    },
    {
      "epoch": 0.6894308943089431,
      "step": 3180,
      "training_loss": 8.130922317504883
    },
    {
      "epoch": 0.6894308943089431,
      "step": 3180,
      "training_loss": 6.359582424163818
    },
    {
      "epoch": 0.6894308943089431,
      "step": 3180,
      "training_loss": 6.7255473136901855
    },
    {
      "epoch": 0.6896476964769648,
      "step": 3181,
      "training_loss": 7.1367878913879395
    },
    {
      "epoch": 0.6896476964769648,
      "step": 3181,
      "training_loss": 6.740783214569092
    },
    {
      "epoch": 0.6896476964769648,
      "step": 3181,
      "training_loss": 7.431958198547363
    },
    {
      "epoch": 0.6896476964769648,
      "step": 3181,
      "training_loss": 7.006082534790039
    },
    {
      "epoch": 0.6898644986449864,
      "step": 3182,
      "training_loss": 6.265896320343018
    },
    {
      "epoch": 0.6898644986449864,
      "step": 3182,
      "training_loss": 7.1965532302856445
    },
    {
      "epoch": 0.6898644986449864,
      "step": 3182,
      "training_loss": 6.868842601776123
    },
    {
      "epoch": 0.6898644986449864,
      "step": 3182,
      "training_loss": 6.360138893127441
    },
    {
      "epoch": 0.6900813008130081,
      "step": 3183,
      "training_loss": 5.309938430786133
    },
    {
      "epoch": 0.6900813008130081,
      "step": 3183,
      "training_loss": 7.092146396636963
    },
    {
      "epoch": 0.6900813008130081,
      "step": 3183,
      "training_loss": 6.601544380187988
    },
    {
      "epoch": 0.6900813008130081,
      "step": 3183,
      "training_loss": 6.670618534088135
    },
    {
      "epoch": 0.6902981029810298,
      "grad_norm": 21.254236221313477,
      "learning_rate": 1e-05,
      "loss": 6.8033,
      "step": 3184
    },
    {
      "epoch": 0.6902981029810298,
      "step": 3184,
      "training_loss": 7.08493185043335
    },
    {
      "epoch": 0.6902981029810298,
      "step": 3184,
      "training_loss": 6.38118839263916
    },
    {
      "epoch": 0.6902981029810298,
      "step": 3184,
      "training_loss": 6.725951194763184
    },
    {
      "epoch": 0.6902981029810298,
      "step": 3184,
      "training_loss": 7.3605546951293945
    },
    {
      "epoch": 0.6905149051490515,
      "step": 3185,
      "training_loss": 7.2406086921691895
    },
    {
      "epoch": 0.6905149051490515,
      "step": 3185,
      "training_loss": 8.348626136779785
    },
    {
      "epoch": 0.6905149051490515,
      "step": 3185,
      "training_loss": 6.361815452575684
    },
    {
      "epoch": 0.6905149051490515,
      "step": 3185,
      "training_loss": 6.8072710037231445
    },
    {
      "epoch": 0.6907317073170731,
      "step": 3186,
      "training_loss": 4.905064582824707
    },
    {
      "epoch": 0.6907317073170731,
      "step": 3186,
      "training_loss": 6.587992191314697
    },
    {
      "epoch": 0.6907317073170731,
      "step": 3186,
      "training_loss": 6.980788707733154
    },
    {
      "epoch": 0.6907317073170731,
      "step": 3186,
      "training_loss": 6.327462673187256
    },
    {
      "epoch": 0.6909485094850949,
      "step": 3187,
      "training_loss": 7.928014755249023
    },
    {
      "epoch": 0.6909485094850949,
      "step": 3187,
      "training_loss": 2.973679542541504
    },
    {
      "epoch": 0.6909485094850949,
      "step": 3187,
      "training_loss": 6.980981349945068
    },
    {
      "epoch": 0.6909485094850949,
      "step": 3187,
      "training_loss": 7.894748687744141
    },
    {
      "epoch": 0.6911653116531166,
      "grad_norm": 19.55517578125,
      "learning_rate": 1e-05,
      "loss": 6.6806,
      "step": 3188
    },
    {
      "epoch": 0.6911653116531166,
      "step": 3188,
      "training_loss": 5.965201377868652
    },
    {
      "epoch": 0.6911653116531166,
      "step": 3188,
      "training_loss": 6.099950790405273
    },
    {
      "epoch": 0.6911653116531166,
      "step": 3188,
      "training_loss": 6.790132999420166
    },
    {
      "epoch": 0.6911653116531166,
      "step": 3188,
      "training_loss": 6.739607334136963
    },
    {
      "epoch": 0.6913821138211382,
      "step": 3189,
      "training_loss": 7.55525016784668
    },
    {
      "epoch": 0.6913821138211382,
      "step": 3189,
      "training_loss": 6.643653392791748
    },
    {
      "epoch": 0.6913821138211382,
      "step": 3189,
      "training_loss": 6.718923091888428
    },
    {
      "epoch": 0.6913821138211382,
      "step": 3189,
      "training_loss": 6.332242965698242
    },
    {
      "epoch": 0.6915989159891599,
      "step": 3190,
      "training_loss": 5.30396842956543
    },
    {
      "epoch": 0.6915989159891599,
      "step": 3190,
      "training_loss": 8.693981170654297
    },
    {
      "epoch": 0.6915989159891599,
      "step": 3190,
      "training_loss": 2.951481580734253
    },
    {
      "epoch": 0.6915989159891599,
      "step": 3190,
      "training_loss": 6.886664867401123
    },
    {
      "epoch": 0.6918157181571816,
      "step": 3191,
      "training_loss": 3.3582875728607178
    },
    {
      "epoch": 0.6918157181571816,
      "step": 3191,
      "training_loss": 6.732887268066406
    },
    {
      "epoch": 0.6918157181571816,
      "step": 3191,
      "training_loss": 7.720556735992432
    },
    {
      "epoch": 0.6918157181571816,
      "step": 3191,
      "training_loss": 6.992115497589111
    },
    {
      "epoch": 0.6920325203252032,
      "grad_norm": 19.2294979095459,
      "learning_rate": 1e-05,
      "loss": 6.3428,
      "step": 3192
    },
    {
      "epoch": 0.6920325203252032,
      "step": 3192,
      "training_loss": 4.027905464172363
    },
    {
      "epoch": 0.6920325203252032,
      "step": 3192,
      "training_loss": 5.980741500854492
    },
    {
      "epoch": 0.6920325203252032,
      "step": 3192,
      "training_loss": 4.491489887237549
    },
    {
      "epoch": 0.6920325203252032,
      "step": 3192,
      "training_loss": 6.960203647613525
    },
    {
      "epoch": 0.6922493224932249,
      "step": 3193,
      "training_loss": 6.250097274780273
    },
    {
      "epoch": 0.6922493224932249,
      "step": 3193,
      "training_loss": 5.59196662902832
    },
    {
      "epoch": 0.6922493224932249,
      "step": 3193,
      "training_loss": 7.006092548370361
    },
    {
      "epoch": 0.6922493224932249,
      "step": 3193,
      "training_loss": 6.397244453430176
    },
    {
      "epoch": 0.6924661246612466,
      "step": 3194,
      "training_loss": 7.441795825958252
    },
    {
      "epoch": 0.6924661246612466,
      "step": 3194,
      "training_loss": 6.355734825134277
    },
    {
      "epoch": 0.6924661246612466,
      "step": 3194,
      "training_loss": 6.061272144317627
    },
    {
      "epoch": 0.6924661246612466,
      "step": 3194,
      "training_loss": 6.925380229949951
    },
    {
      "epoch": 0.6926829268292682,
      "step": 3195,
      "training_loss": 7.449796199798584
    },
    {
      "epoch": 0.6926829268292682,
      "step": 3195,
      "training_loss": 6.991480827331543
    },
    {
      "epoch": 0.6926829268292682,
      "step": 3195,
      "training_loss": 6.613929271697998
    },
    {
      "epoch": 0.6926829268292682,
      "step": 3195,
      "training_loss": 6.91741943359375
    },
    {
      "epoch": 0.69289972899729,
      "grad_norm": 17.260168075561523,
      "learning_rate": 1e-05,
      "loss": 6.3414,
      "step": 3196
    },
    {
      "epoch": 0.69289972899729,
      "step": 3196,
      "training_loss": 5.1343183517456055
    },
    {
      "epoch": 0.69289972899729,
      "step": 3196,
      "training_loss": 6.417924404144287
    },
    {
      "epoch": 0.69289972899729,
      "step": 3196,
      "training_loss": 6.484281539916992
    },
    {
      "epoch": 0.69289972899729,
      "step": 3196,
      "training_loss": 6.181567192077637
    },
    {
      "epoch": 0.6931165311653117,
      "step": 3197,
      "training_loss": 7.121387958526611
    },
    {
      "epoch": 0.6931165311653117,
      "step": 3197,
      "training_loss": 2.8440568447113037
    },
    {
      "epoch": 0.6931165311653117,
      "step": 3197,
      "training_loss": 5.91389274597168
    },
    {
      "epoch": 0.6931165311653117,
      "step": 3197,
      "training_loss": 7.352267265319824
    },
    {
      "epoch": 0.6933333333333334,
      "step": 3198,
      "training_loss": 4.4329023361206055
    },
    {
      "epoch": 0.6933333333333334,
      "step": 3198,
      "training_loss": 7.740390300750732
    },
    {
      "epoch": 0.6933333333333334,
      "step": 3198,
      "training_loss": 6.255215644836426
    },
    {
      "epoch": 0.6933333333333334,
      "step": 3198,
      "training_loss": 7.4058732986450195
    },
    {
      "epoch": 0.693550135501355,
      "step": 3199,
      "training_loss": 7.057074069976807
    },
    {
      "epoch": 0.693550135501355,
      "step": 3199,
      "training_loss": 4.995570182800293
    },
    {
      "epoch": 0.693550135501355,
      "step": 3199,
      "training_loss": 6.227792263031006
    },
    {
      "epoch": 0.693550135501355,
      "step": 3199,
      "training_loss": 6.498090744018555
    },
    {
      "epoch": 0.6937669376693767,
      "grad_norm": 22.561138153076172,
      "learning_rate": 1e-05,
      "loss": 6.1289,
      "step": 3200
    },
    {
      "epoch": 0.6937669376693767,
      "step": 3200,
      "training_loss": 6.552753448486328
    },
    {
      "epoch": 0.6937669376693767,
      "step": 3200,
      "training_loss": 5.305466175079346
    },
    {
      "epoch": 0.6937669376693767,
      "step": 3200,
      "training_loss": 6.328371047973633
    },
    {
      "epoch": 0.6937669376693767,
      "step": 3200,
      "training_loss": 6.950515270233154
    },
    {
      "epoch": 0.6939837398373984,
      "step": 3201,
      "training_loss": 7.477058410644531
    },
    {
      "epoch": 0.6939837398373984,
      "step": 3201,
      "training_loss": 6.695585250854492
    },
    {
      "epoch": 0.6939837398373984,
      "step": 3201,
      "training_loss": 6.901607036590576
    },
    {
      "epoch": 0.6939837398373984,
      "step": 3201,
      "training_loss": 6.366369724273682
    },
    {
      "epoch": 0.69420054200542,
      "step": 3202,
      "training_loss": 6.331208229064941
    },
    {
      "epoch": 0.69420054200542,
      "step": 3202,
      "training_loss": 5.61056661605835
    },
    {
      "epoch": 0.69420054200542,
      "step": 3202,
      "training_loss": 5.695509433746338
    },
    {
      "epoch": 0.69420054200542,
      "step": 3202,
      "training_loss": 5.764202117919922
    },
    {
      "epoch": 0.6944173441734417,
      "step": 3203,
      "training_loss": 6.593850135803223
    },
    {
      "epoch": 0.6944173441734417,
      "step": 3203,
      "training_loss": 6.977595329284668
    },
    {
      "epoch": 0.6944173441734417,
      "step": 3203,
      "training_loss": 6.8051438331604
    },
    {
      "epoch": 0.6944173441734417,
      "step": 3203,
      "training_loss": 6.634063243865967
    },
    {
      "epoch": 0.6946341463414634,
      "grad_norm": 19.495498657226562,
      "learning_rate": 1e-05,
      "loss": 6.4369,
      "step": 3204
    },
    {
      "epoch": 0.6946341463414634,
      "step": 3204,
      "training_loss": 5.989444255828857
    },
    {
      "epoch": 0.6946341463414634,
      "step": 3204,
      "training_loss": 6.4447340965271
    },
    {
      "epoch": 0.6946341463414634,
      "step": 3204,
      "training_loss": 6.254518985748291
    },
    {
      "epoch": 0.6946341463414634,
      "step": 3204,
      "training_loss": 6.567189693450928
    },
    {
      "epoch": 0.6948509485094851,
      "step": 3205,
      "training_loss": 6.974350452423096
    },
    {
      "epoch": 0.6948509485094851,
      "step": 3205,
      "training_loss": 5.988304138183594
    },
    {
      "epoch": 0.6948509485094851,
      "step": 3205,
      "training_loss": 6.401216506958008
    },
    {
      "epoch": 0.6948509485094851,
      "step": 3205,
      "training_loss": 7.123418807983398
    },
    {
      "epoch": 0.6950677506775068,
      "step": 3206,
      "training_loss": 5.548643589019775
    },
    {
      "epoch": 0.6950677506775068,
      "step": 3206,
      "training_loss": 5.3584885597229
    },
    {
      "epoch": 0.6950677506775068,
      "step": 3206,
      "training_loss": 7.308224201202393
    },
    {
      "epoch": 0.6950677506775068,
      "step": 3206,
      "training_loss": 7.3921732902526855
    },
    {
      "epoch": 0.6952845528455285,
      "step": 3207,
      "training_loss": 4.982675552368164
    },
    {
      "epoch": 0.6952845528455285,
      "step": 3207,
      "training_loss": 6.0003342628479
    },
    {
      "epoch": 0.6952845528455285,
      "step": 3207,
      "training_loss": 8.202981948852539
    },
    {
      "epoch": 0.6952845528455285,
      "step": 3207,
      "training_loss": 6.0083513259887695
    },
    {
      "epoch": 0.6955013550135501,
      "grad_norm": 15.104798316955566,
      "learning_rate": 1e-05,
      "loss": 6.4091,
      "step": 3208
    },
    {
      "epoch": 0.6955013550135501,
      "step": 3208,
      "training_loss": 6.769093990325928
    },
    {
      "epoch": 0.6955013550135501,
      "step": 3208,
      "training_loss": 3.2027828693389893
    },
    {
      "epoch": 0.6955013550135501,
      "step": 3208,
      "training_loss": 7.340142726898193
    },
    {
      "epoch": 0.6955013550135501,
      "step": 3208,
      "training_loss": 7.135552406311035
    },
    {
      "epoch": 0.6957181571815718,
      "step": 3209,
      "training_loss": 5.870955467224121
    },
    {
      "epoch": 0.6957181571815718,
      "step": 3209,
      "training_loss": 8.229269027709961
    },
    {
      "epoch": 0.6957181571815718,
      "step": 3209,
      "training_loss": 6.971532344818115
    },
    {
      "epoch": 0.6957181571815718,
      "step": 3209,
      "training_loss": 5.0083088874816895
    },
    {
      "epoch": 0.6959349593495935,
      "step": 3210,
      "training_loss": 5.644196033477783
    },
    {
      "epoch": 0.6959349593495935,
      "step": 3210,
      "training_loss": 6.647689342498779
    },
    {
      "epoch": 0.6959349593495935,
      "step": 3210,
      "training_loss": 7.23404598236084
    },
    {
      "epoch": 0.6959349593495935,
      "step": 3210,
      "training_loss": 5.4987688064575195
    },
    {
      "epoch": 0.6961517615176152,
      "step": 3211,
      "training_loss": 7.251621723175049
    },
    {
      "epoch": 0.6961517615176152,
      "step": 3211,
      "training_loss": 5.568029880523682
    },
    {
      "epoch": 0.6961517615176152,
      "step": 3211,
      "training_loss": 6.458364963531494
    },
    {
      "epoch": 0.6961517615176152,
      "step": 3211,
      "training_loss": 7.742403507232666
    },
    {
      "epoch": 0.6963685636856368,
      "grad_norm": 14.839282989501953,
      "learning_rate": 1e-05,
      "loss": 6.4108,
      "step": 3212
    },
    {
      "epoch": 0.6963685636856368,
      "step": 3212,
      "training_loss": 6.848922252655029
    },
    {
      "epoch": 0.6963685636856368,
      "step": 3212,
      "training_loss": 7.530839920043945
    },
    {
      "epoch": 0.6963685636856368,
      "step": 3212,
      "training_loss": 5.945679187774658
    },
    {
      "epoch": 0.6963685636856368,
      "step": 3212,
      "training_loss": 6.1092848777771
    },
    {
      "epoch": 0.6965853658536585,
      "step": 3213,
      "training_loss": 5.524290561676025
    },
    {
      "epoch": 0.6965853658536585,
      "step": 3213,
      "training_loss": 7.798829555511475
    },
    {
      "epoch": 0.6965853658536585,
      "step": 3213,
      "training_loss": 6.405336856842041
    },
    {
      "epoch": 0.6965853658536585,
      "step": 3213,
      "training_loss": 6.101936340332031
    },
    {
      "epoch": 0.6968021680216803,
      "step": 3214,
      "training_loss": 6.7078704833984375
    },
    {
      "epoch": 0.6968021680216803,
      "step": 3214,
      "training_loss": 7.184090614318848
    },
    {
      "epoch": 0.6968021680216803,
      "step": 3214,
      "training_loss": 5.779262542724609
    },
    {
      "epoch": 0.6968021680216803,
      "step": 3214,
      "training_loss": 6.725590229034424
    },
    {
      "epoch": 0.6970189701897019,
      "step": 3215,
      "training_loss": 7.759808540344238
    },
    {
      "epoch": 0.6970189701897019,
      "step": 3215,
      "training_loss": 5.401176929473877
    },
    {
      "epoch": 0.6970189701897019,
      "step": 3215,
      "training_loss": 7.3445916175842285
    },
    {
      "epoch": 0.6970189701897019,
      "step": 3215,
      "training_loss": 6.462840557098389
    },
    {
      "epoch": 0.6972357723577236,
      "grad_norm": 16.71668243408203,
      "learning_rate": 1e-05,
      "loss": 6.6019,
      "step": 3216
    },
    {
      "epoch": 0.6972357723577236,
      "step": 3216,
      "training_loss": 4.961745262145996
    },
    {
      "epoch": 0.6972357723577236,
      "step": 3216,
      "training_loss": 6.704484939575195
    },
    {
      "epoch": 0.6972357723577236,
      "step": 3216,
      "training_loss": 5.8260345458984375
    },
    {
      "epoch": 0.6972357723577236,
      "step": 3216,
      "training_loss": 3.683505058288574
    },
    {
      "epoch": 0.6974525745257453,
      "step": 3217,
      "training_loss": 6.757167339324951
    },
    {
      "epoch": 0.6974525745257453,
      "step": 3217,
      "training_loss": 6.051681041717529
    },
    {
      "epoch": 0.6974525745257453,
      "step": 3217,
      "training_loss": 6.841640949249268
    },
    {
      "epoch": 0.6974525745257453,
      "step": 3217,
      "training_loss": 7.4500813484191895
    },
    {
      "epoch": 0.6976693766937669,
      "step": 3218,
      "training_loss": 6.945221900939941
    },
    {
      "epoch": 0.6976693766937669,
      "step": 3218,
      "training_loss": 6.443806171417236
    },
    {
      "epoch": 0.6976693766937669,
      "step": 3218,
      "training_loss": 7.21404504776001
    },
    {
      "epoch": 0.6976693766937669,
      "step": 3218,
      "training_loss": 6.648681640625
    },
    {
      "epoch": 0.6978861788617886,
      "step": 3219,
      "training_loss": 6.957934379577637
    },
    {
      "epoch": 0.6978861788617886,
      "step": 3219,
      "training_loss": 8.271500587463379
    },
    {
      "epoch": 0.6978861788617886,
      "step": 3219,
      "training_loss": 7.321402072906494
    },
    {
      "epoch": 0.6978861788617886,
      "step": 3219,
      "training_loss": 7.114715099334717
    },
    {
      "epoch": 0.6981029810298103,
      "grad_norm": 24.207300186157227,
      "learning_rate": 1e-05,
      "loss": 6.5746,
      "step": 3220
    },
    {
      "epoch": 0.6981029810298103,
      "step": 3220,
      "training_loss": 6.075149059295654
    },
    {
      "epoch": 0.6981029810298103,
      "step": 3220,
      "training_loss": 6.473956108093262
    },
    {
      "epoch": 0.6981029810298103,
      "step": 3220,
      "training_loss": 4.991610527038574
    },
    {
      "epoch": 0.6981029810298103,
      "step": 3220,
      "training_loss": 7.143603801727295
    },
    {
      "epoch": 0.698319783197832,
      "step": 3221,
      "training_loss": 7.2455573081970215
    },
    {
      "epoch": 0.698319783197832,
      "step": 3221,
      "training_loss": 5.9760308265686035
    },
    {
      "epoch": 0.698319783197832,
      "step": 3221,
      "training_loss": 7.004199504852295
    },
    {
      "epoch": 0.698319783197832,
      "step": 3221,
      "training_loss": 5.652931213378906
    },
    {
      "epoch": 0.6985365853658536,
      "step": 3222,
      "training_loss": 6.752702713012695
    },
    {
      "epoch": 0.6985365853658536,
      "step": 3222,
      "training_loss": 5.867639541625977
    },
    {
      "epoch": 0.6985365853658536,
      "step": 3222,
      "training_loss": 5.113278865814209
    },
    {
      "epoch": 0.6985365853658536,
      "step": 3222,
      "training_loss": 7.066010475158691
    },
    {
      "epoch": 0.6987533875338754,
      "step": 3223,
      "training_loss": 6.848668575286865
    },
    {
      "epoch": 0.6987533875338754,
      "step": 3223,
      "training_loss": 6.350919723510742
    },
    {
      "epoch": 0.6987533875338754,
      "step": 3223,
      "training_loss": 6.767252445220947
    },
    {
      "epoch": 0.6987533875338754,
      "step": 3223,
      "training_loss": 7.071773529052734
    },
    {
      "epoch": 0.6989701897018971,
      "grad_norm": 11.952648162841797,
      "learning_rate": 1e-05,
      "loss": 6.4001,
      "step": 3224
    },
    {
      "epoch": 0.6989701897018971,
      "step": 3224,
      "training_loss": 5.844534397125244
    },
    {
      "epoch": 0.6989701897018971,
      "step": 3224,
      "training_loss": 5.37568998336792
    },
    {
      "epoch": 0.6989701897018971,
      "step": 3224,
      "training_loss": 5.452781677246094
    },
    {
      "epoch": 0.6989701897018971,
      "step": 3224,
      "training_loss": 6.021287441253662
    },
    {
      "epoch": 0.6991869918699187,
      "step": 3225,
      "training_loss": 5.966309547424316
    },
    {
      "epoch": 0.6991869918699187,
      "step": 3225,
      "training_loss": 7.258838653564453
    },
    {
      "epoch": 0.6991869918699187,
      "step": 3225,
      "training_loss": 7.099658489227295
    },
    {
      "epoch": 0.6991869918699187,
      "step": 3225,
      "training_loss": 6.1952996253967285
    },
    {
      "epoch": 0.6994037940379404,
      "step": 3226,
      "training_loss": 4.634546279907227
    },
    {
      "epoch": 0.6994037940379404,
      "step": 3226,
      "training_loss": 5.908798694610596
    },
    {
      "epoch": 0.6994037940379404,
      "step": 3226,
      "training_loss": 6.556793689727783
    },
    {
      "epoch": 0.6994037940379404,
      "step": 3226,
      "training_loss": 5.257028102874756
    },
    {
      "epoch": 0.6996205962059621,
      "step": 3227,
      "training_loss": 6.382028102874756
    },
    {
      "epoch": 0.6996205962059621,
      "step": 3227,
      "training_loss": 6.304915428161621
    },
    {
      "epoch": 0.6996205962059621,
      "step": 3227,
      "training_loss": 6.619667053222656
    },
    {
      "epoch": 0.6996205962059621,
      "step": 3227,
      "training_loss": 6.828258514404297
    },
    {
      "epoch": 0.6998373983739837,
      "grad_norm": 18.856868743896484,
      "learning_rate": 1e-05,
      "loss": 6.1067,
      "step": 3228
    },
    {
      "epoch": 0.6998373983739837,
      "step": 3228,
      "training_loss": 7.419657230377197
    },
    {
      "epoch": 0.6998373983739837,
      "step": 3228,
      "training_loss": 7.354797840118408
    },
    {
      "epoch": 0.6998373983739837,
      "step": 3228,
      "training_loss": 7.447760581970215
    },
    {
      "epoch": 0.6998373983739837,
      "step": 3228,
      "training_loss": 3.7911601066589355
    },
    {
      "epoch": 0.7000542005420054,
      "step": 3229,
      "training_loss": 7.6178789138793945
    },
    {
      "epoch": 0.7000542005420054,
      "step": 3229,
      "training_loss": 4.978937149047852
    },
    {
      "epoch": 0.7000542005420054,
      "step": 3229,
      "training_loss": 7.149218559265137
    },
    {
      "epoch": 0.7000542005420054,
      "step": 3229,
      "training_loss": 6.6686224937438965
    },
    {
      "epoch": 0.7002710027100271,
      "step": 3230,
      "training_loss": 7.814661979675293
    },
    {
      "epoch": 0.7002710027100271,
      "step": 3230,
      "training_loss": 5.169582366943359
    },
    {
      "epoch": 0.7002710027100271,
      "step": 3230,
      "training_loss": 6.737241744995117
    },
    {
      "epoch": 0.7002710027100271,
      "step": 3230,
      "training_loss": 6.145514488220215
    },
    {
      "epoch": 0.7004878048780487,
      "step": 3231,
      "training_loss": 7.557590007781982
    },
    {
      "epoch": 0.7004878048780487,
      "step": 3231,
      "training_loss": 5.642498970031738
    },
    {
      "epoch": 0.7004878048780487,
      "step": 3231,
      "training_loss": 4.636099338531494
    },
    {
      "epoch": 0.7004878048780487,
      "step": 3231,
      "training_loss": 4.872196674346924
    },
    {
      "epoch": 0.7007046070460705,
      "grad_norm": 19.909286499023438,
      "learning_rate": 1e-05,
      "loss": 6.3127,
      "step": 3232
    },
    {
      "epoch": 0.7007046070460705,
      "step": 3232,
      "training_loss": 7.330187797546387
    },
    {
      "epoch": 0.7007046070460705,
      "step": 3232,
      "training_loss": 7.943628311157227
    },
    {
      "epoch": 0.7007046070460705,
      "step": 3232,
      "training_loss": 10.210954666137695
    },
    {
      "epoch": 0.7007046070460705,
      "step": 3232,
      "training_loss": 4.764870643615723
    },
    {
      "epoch": 0.7009214092140922,
      "step": 3233,
      "training_loss": 7.267055511474609
    },
    {
      "epoch": 0.7009214092140922,
      "step": 3233,
      "training_loss": 6.738541126251221
    },
    {
      "epoch": 0.7009214092140922,
      "step": 3233,
      "training_loss": 7.593845844268799
    },
    {
      "epoch": 0.7009214092140922,
      "step": 3233,
      "training_loss": 6.334884166717529
    },
    {
      "epoch": 0.7011382113821139,
      "step": 3234,
      "training_loss": 2.7890782356262207
    },
    {
      "epoch": 0.7011382113821139,
      "step": 3234,
      "training_loss": 7.357683181762695
    },
    {
      "epoch": 0.7011382113821139,
      "step": 3234,
      "training_loss": 6.621310234069824
    },
    {
      "epoch": 0.7011382113821139,
      "step": 3234,
      "training_loss": 6.724091529846191
    },
    {
      "epoch": 0.7013550135501355,
      "step": 3235,
      "training_loss": 5.701984882354736
    },
    {
      "epoch": 0.7013550135501355,
      "step": 3235,
      "training_loss": 7.076802730560303
    },
    {
      "epoch": 0.7013550135501355,
      "step": 3235,
      "training_loss": 7.1371917724609375
    },
    {
      "epoch": 0.7013550135501355,
      "step": 3235,
      "training_loss": 6.917311191558838
    },
    {
      "epoch": 0.7015718157181572,
      "grad_norm": 23.055736541748047,
      "learning_rate": 1e-05,
      "loss": 6.7818,
      "step": 3236
    },
    {
      "epoch": 0.7015718157181572,
      "step": 3236,
      "training_loss": 6.75247049331665
    },
    {
      "epoch": 0.7015718157181572,
      "step": 3236,
      "training_loss": 7.045907497406006
    },
    {
      "epoch": 0.7015718157181572,
      "step": 3236,
      "training_loss": 4.985444068908691
    },
    {
      "epoch": 0.7015718157181572,
      "step": 3236,
      "training_loss": 6.551575183868408
    },
    {
      "epoch": 0.7017886178861789,
      "step": 3237,
      "training_loss": 6.8122663497924805
    },
    {
      "epoch": 0.7017886178861789,
      "step": 3237,
      "training_loss": 4.789841651916504
    },
    {
      "epoch": 0.7017886178861789,
      "step": 3237,
      "training_loss": 4.658650875091553
    },
    {
      "epoch": 0.7017886178861789,
      "step": 3237,
      "training_loss": 5.851783275604248
    },
    {
      "epoch": 0.7020054200542005,
      "step": 3238,
      "training_loss": 6.447097301483154
    },
    {
      "epoch": 0.7020054200542005,
      "step": 3238,
      "training_loss": 8.072447776794434
    },
    {
      "epoch": 0.7020054200542005,
      "step": 3238,
      "training_loss": 6.71575927734375
    },
    {
      "epoch": 0.7020054200542005,
      "step": 3238,
      "training_loss": 6.767519474029541
    },
    {
      "epoch": 0.7022222222222222,
      "step": 3239,
      "training_loss": 6.869841575622559
    },
    {
      "epoch": 0.7022222222222222,
      "step": 3239,
      "training_loss": 6.988497734069824
    },
    {
      "epoch": 0.7022222222222222,
      "step": 3239,
      "training_loss": 5.833254814147949
    },
    {
      "epoch": 0.7022222222222222,
      "step": 3239,
      "training_loss": 7.503995418548584
    },
    {
      "epoch": 0.7024390243902439,
      "grad_norm": 17.700937271118164,
      "learning_rate": 1e-05,
      "loss": 6.4154,
      "step": 3240
    },
    {
      "epoch": 0.7024390243902439,
      "step": 3240,
      "training_loss": 7.707098960876465
    },
    {
      "epoch": 0.7024390243902439,
      "step": 3240,
      "training_loss": 6.818327903747559
    },
    {
      "epoch": 0.7024390243902439,
      "step": 3240,
      "training_loss": 5.892416954040527
    },
    {
      "epoch": 0.7024390243902439,
      "step": 3240,
      "training_loss": 6.599346160888672
    },
    {
      "epoch": 0.7026558265582655,
      "step": 3241,
      "training_loss": 7.244025230407715
    },
    {
      "epoch": 0.7026558265582655,
      "step": 3241,
      "training_loss": 6.596479415893555
    },
    {
      "epoch": 0.7026558265582655,
      "step": 3241,
      "training_loss": 4.950135231018066
    },
    {
      "epoch": 0.7026558265582655,
      "step": 3241,
      "training_loss": 6.495161533355713
    },
    {
      "epoch": 0.7028726287262873,
      "step": 3242,
      "training_loss": 7.554079532623291
    },
    {
      "epoch": 0.7028726287262873,
      "step": 3242,
      "training_loss": 6.280211925506592
    },
    {
      "epoch": 0.7028726287262873,
      "step": 3242,
      "training_loss": 6.476317882537842
    },
    {
      "epoch": 0.7028726287262873,
      "step": 3242,
      "training_loss": 6.029844284057617
    },
    {
      "epoch": 0.703089430894309,
      "step": 3243,
      "training_loss": 8.000031471252441
    },
    {
      "epoch": 0.703089430894309,
      "step": 3243,
      "training_loss": 6.7765889167785645
    },
    {
      "epoch": 0.703089430894309,
      "step": 3243,
      "training_loss": 6.79780387878418
    },
    {
      "epoch": 0.703089430894309,
      "step": 3243,
      "training_loss": 7.365562438964844
    },
    {
      "epoch": 0.7033062330623306,
      "grad_norm": 15.02585506439209,
      "learning_rate": 1e-05,
      "loss": 6.724,
      "step": 3244
    },
    {
      "epoch": 0.7033062330623306,
      "step": 3244,
      "training_loss": 7.638637065887451
    },
    {
      "epoch": 0.7033062330623306,
      "step": 3244,
      "training_loss": 5.0202317237854
    },
    {
      "epoch": 0.7033062330623306,
      "step": 3244,
      "training_loss": 7.3303751945495605
    },
    {
      "epoch": 0.7033062330623306,
      "step": 3244,
      "training_loss": 6.240711212158203
    },
    {
      "epoch": 0.7035230352303523,
      "step": 3245,
      "training_loss": 7.550933837890625
    },
    {
      "epoch": 0.7035230352303523,
      "step": 3245,
      "training_loss": 6.2576069831848145
    },
    {
      "epoch": 0.7035230352303523,
      "step": 3245,
      "training_loss": 6.742323875427246
    },
    {
      "epoch": 0.7035230352303523,
      "step": 3245,
      "training_loss": 5.364785194396973
    },
    {
      "epoch": 0.703739837398374,
      "step": 3246,
      "training_loss": 7.751742839813232
    },
    {
      "epoch": 0.703739837398374,
      "step": 3246,
      "training_loss": 5.844108581542969
    },
    {
      "epoch": 0.703739837398374,
      "step": 3246,
      "training_loss": 5.854831695556641
    },
    {
      "epoch": 0.703739837398374,
      "step": 3246,
      "training_loss": 6.376284122467041
    },
    {
      "epoch": 0.7039566395663956,
      "step": 3247,
      "training_loss": 4.811328887939453
    },
    {
      "epoch": 0.7039566395663956,
      "step": 3247,
      "training_loss": 6.394301891326904
    },
    {
      "epoch": 0.7039566395663956,
      "step": 3247,
      "training_loss": 5.822855472564697
    },
    {
      "epoch": 0.7039566395663956,
      "step": 3247,
      "training_loss": 3.642023801803589
    },
    {
      "epoch": 0.7041734417344173,
      "grad_norm": 21.78307342529297,
      "learning_rate": 1e-05,
      "loss": 6.1652,
      "step": 3248
    },
    {
      "epoch": 0.7041734417344173,
      "step": 3248,
      "training_loss": 6.562030792236328
    },
    {
      "epoch": 0.7041734417344173,
      "step": 3248,
      "training_loss": 6.117539405822754
    },
    {
      "epoch": 0.7041734417344173,
      "step": 3248,
      "training_loss": 7.412166595458984
    },
    {
      "epoch": 0.7041734417344173,
      "step": 3248,
      "training_loss": 6.788995265960693
    },
    {
      "epoch": 0.704390243902439,
      "step": 3249,
      "training_loss": 7.346813201904297
    },
    {
      "epoch": 0.704390243902439,
      "step": 3249,
      "training_loss": 6.632768630981445
    },
    {
      "epoch": 0.704390243902439,
      "step": 3249,
      "training_loss": 6.955228805541992
    },
    {
      "epoch": 0.704390243902439,
      "step": 3249,
      "training_loss": 7.803002834320068
    },
    {
      "epoch": 0.7046070460704607,
      "step": 3250,
      "training_loss": 7.249201774597168
    },
    {
      "epoch": 0.7046070460704607,
      "step": 3250,
      "training_loss": 4.195110321044922
    },
    {
      "epoch": 0.7046070460704607,
      "step": 3250,
      "training_loss": 6.375729560852051
    },
    {
      "epoch": 0.7046070460704607,
      "step": 3250,
      "training_loss": 6.668062686920166
    },
    {
      "epoch": 0.7048238482384824,
      "step": 3251,
      "training_loss": 6.3289008140563965
    },
    {
      "epoch": 0.7048238482384824,
      "step": 3251,
      "training_loss": 6.343959808349609
    },
    {
      "epoch": 0.7048238482384824,
      "step": 3251,
      "training_loss": 7.354368686676025
    },
    {
      "epoch": 0.7048238482384824,
      "step": 3251,
      "training_loss": 4.345763206481934
    },
    {
      "epoch": 0.7050406504065041,
      "grad_norm": 16.709796905517578,
      "learning_rate": 1e-05,
      "loss": 6.53,
      "step": 3252
    },
    {
      "epoch": 0.7050406504065041,
      "step": 3252,
      "training_loss": 6.2004313468933105
    },
    {
      "epoch": 0.7050406504065041,
      "step": 3252,
      "training_loss": 5.645528793334961
    },
    {
      "epoch": 0.7050406504065041,
      "step": 3252,
      "training_loss": 7.167738914489746
    },
    {
      "epoch": 0.7050406504065041,
      "step": 3252,
      "training_loss": 5.903592109680176
    },
    {
      "epoch": 0.7052574525745258,
      "step": 3253,
      "training_loss": 5.374011039733887
    },
    {
      "epoch": 0.7052574525745258,
      "step": 3253,
      "training_loss": 5.718495845794678
    },
    {
      "epoch": 0.7052574525745258,
      "step": 3253,
      "training_loss": 7.051311016082764
    },
    {
      "epoch": 0.7052574525745258,
      "step": 3253,
      "training_loss": 4.900629043579102
    },
    {
      "epoch": 0.7054742547425474,
      "step": 3254,
      "training_loss": 6.845423698425293
    },
    {
      "epoch": 0.7054742547425474,
      "step": 3254,
      "training_loss": 6.717995643615723
    },
    {
      "epoch": 0.7054742547425474,
      "step": 3254,
      "training_loss": 6.453631401062012
    },
    {
      "epoch": 0.7054742547425474,
      "step": 3254,
      "training_loss": 7.438103199005127
    },
    {
      "epoch": 0.7056910569105691,
      "step": 3255,
      "training_loss": 7.564657211303711
    },
    {
      "epoch": 0.7056910569105691,
      "step": 3255,
      "training_loss": 7.410213470458984
    },
    {
      "epoch": 0.7056910569105691,
      "step": 3255,
      "training_loss": 6.697983264923096
    },
    {
      "epoch": 0.7056910569105691,
      "step": 3255,
      "training_loss": 6.295555591583252
    },
    {
      "epoch": 0.7059078590785908,
      "grad_norm": 17.866558074951172,
      "learning_rate": 1e-05,
      "loss": 6.4616,
      "step": 3256
    },
    {
      "epoch": 0.7059078590785908,
      "step": 3256,
      "training_loss": 8.974112510681152
    },
    {
      "epoch": 0.7059078590785908,
      "step": 3256,
      "training_loss": 7.9654541015625
    },
    {
      "epoch": 0.7059078590785908,
      "step": 3256,
      "training_loss": 7.449450969696045
    },
    {
      "epoch": 0.7059078590785908,
      "step": 3256,
      "training_loss": 8.209030151367188
    },
    {
      "epoch": 0.7061246612466124,
      "step": 3257,
      "training_loss": 6.119412899017334
    },
    {
      "epoch": 0.7061246612466124,
      "step": 3257,
      "training_loss": 5.5207719802856445
    },
    {
      "epoch": 0.7061246612466124,
      "step": 3257,
      "training_loss": 6.859623908996582
    },
    {
      "epoch": 0.7061246612466124,
      "step": 3257,
      "training_loss": 5.068817615509033
    },
    {
      "epoch": 0.7063414634146341,
      "step": 3258,
      "training_loss": 6.56990909576416
    },
    {
      "epoch": 0.7063414634146341,
      "step": 3258,
      "training_loss": 5.981714725494385
    },
    {
      "epoch": 0.7063414634146341,
      "step": 3258,
      "training_loss": 7.221780300140381
    },
    {
      "epoch": 0.7063414634146341,
      "step": 3258,
      "training_loss": 4.2206902503967285
    },
    {
      "epoch": 0.7065582655826558,
      "step": 3259,
      "training_loss": 6.887423992156982
    },
    {
      "epoch": 0.7065582655826558,
      "step": 3259,
      "training_loss": 6.834132194519043
    },
    {
      "epoch": 0.7065582655826558,
      "step": 3259,
      "training_loss": 6.3978376388549805
    },
    {
      "epoch": 0.7065582655826558,
      "step": 3259,
      "training_loss": 7.310178279876709
    },
    {
      "epoch": 0.7067750677506776,
      "grad_norm": 15.75673770904541,
      "learning_rate": 1e-05,
      "loss": 6.7244,
      "step": 3260
    },
    {
      "epoch": 0.7067750677506776,
      "step": 3260,
      "training_loss": 5.546553611755371
    },
    {
      "epoch": 0.7067750677506776,
      "step": 3260,
      "training_loss": 6.171907424926758
    },
    {
      "epoch": 0.7067750677506776,
      "step": 3260,
      "training_loss": 5.284987449645996
    },
    {
      "epoch": 0.7067750677506776,
      "step": 3260,
      "training_loss": 7.292611598968506
    },
    {
      "epoch": 0.7069918699186992,
      "step": 3261,
      "training_loss": 7.246593952178955
    },
    {
      "epoch": 0.7069918699186992,
      "step": 3261,
      "training_loss": 4.176302433013916
    },
    {
      "epoch": 0.7069918699186992,
      "step": 3261,
      "training_loss": 6.326757431030273
    },
    {
      "epoch": 0.7069918699186992,
      "step": 3261,
      "training_loss": 5.746661186218262
    },
    {
      "epoch": 0.7072086720867209,
      "step": 3262,
      "training_loss": 6.500096797943115
    },
    {
      "epoch": 0.7072086720867209,
      "step": 3262,
      "training_loss": 6.245558738708496
    },
    {
      "epoch": 0.7072086720867209,
      "step": 3262,
      "training_loss": 2.77431583404541
    },
    {
      "epoch": 0.7072086720867209,
      "step": 3262,
      "training_loss": 6.436136722564697
    },
    {
      "epoch": 0.7074254742547426,
      "step": 3263,
      "training_loss": 7.892662048339844
    },
    {
      "epoch": 0.7074254742547426,
      "step": 3263,
      "training_loss": 5.093060493469238
    },
    {
      "epoch": 0.7074254742547426,
      "step": 3263,
      "training_loss": 6.755126953125
    },
    {
      "epoch": 0.7074254742547426,
      "step": 3263,
      "training_loss": 7.972939968109131
    },
    {
      "epoch": 0.7076422764227642,
      "grad_norm": 19.06674575805664,
      "learning_rate": 1e-05,
      "loss": 6.0914,
      "step": 3264
    },
    {
      "epoch": 0.7076422764227642,
      "step": 3264,
      "training_loss": 8.718101501464844
    },
    {
      "epoch": 0.7076422764227642,
      "step": 3264,
      "training_loss": 7.114771842956543
    },
    {
      "epoch": 0.7076422764227642,
      "step": 3264,
      "training_loss": 6.955006122589111
    },
    {
      "epoch": 0.7076422764227642,
      "step": 3264,
      "training_loss": 4.340363025665283
    },
    {
      "epoch": 0.7078590785907859,
      "step": 3265,
      "training_loss": 5.7163190841674805
    },
    {
      "epoch": 0.7078590785907859,
      "step": 3265,
      "training_loss": 6.5068678855896
    },
    {
      "epoch": 0.7078590785907859,
      "step": 3265,
      "training_loss": 6.386962413787842
    },
    {
      "epoch": 0.7078590785907859,
      "step": 3265,
      "training_loss": 6.45911979675293
    },
    {
      "epoch": 0.7080758807588076,
      "step": 3266,
      "training_loss": 5.9564528465271
    },
    {
      "epoch": 0.7080758807588076,
      "step": 3266,
      "training_loss": 6.568336486816406
    },
    {
      "epoch": 0.7080758807588076,
      "step": 3266,
      "training_loss": 6.2944464683532715
    },
    {
      "epoch": 0.7080758807588076,
      "step": 3266,
      "training_loss": 6.737346649169922
    },
    {
      "epoch": 0.7082926829268292,
      "step": 3267,
      "training_loss": 5.905527591705322
    },
    {
      "epoch": 0.7082926829268292,
      "step": 3267,
      "training_loss": 7.925104141235352
    },
    {
      "epoch": 0.7082926829268292,
      "step": 3267,
      "training_loss": 6.988752365112305
    },
    {
      "epoch": 0.7082926829268292,
      "step": 3267,
      "training_loss": 5.750095844268799
    },
    {
      "epoch": 0.7085094850948509,
      "grad_norm": 18.757917404174805,
      "learning_rate": 1e-05,
      "loss": 6.5202,
      "step": 3268
    },
    {
      "epoch": 0.7085094850948509,
      "step": 3268,
      "training_loss": 7.061416149139404
    },
    {
      "epoch": 0.7085094850948509,
      "step": 3268,
      "training_loss": 6.695841312408447
    },
    {
      "epoch": 0.7085094850948509,
      "step": 3268,
      "training_loss": 6.376247406005859
    },
    {
      "epoch": 0.7085094850948509,
      "step": 3268,
      "training_loss": 5.216014862060547
    },
    {
      "epoch": 0.7087262872628727,
      "step": 3269,
      "training_loss": 6.57376766204834
    },
    {
      "epoch": 0.7087262872628727,
      "step": 3269,
      "training_loss": 6.971441268920898
    },
    {
      "epoch": 0.7087262872628727,
      "step": 3269,
      "training_loss": 7.413227558135986
    },
    {
      "epoch": 0.7087262872628727,
      "step": 3269,
      "training_loss": 7.367147445678711
    },
    {
      "epoch": 0.7089430894308943,
      "step": 3270,
      "training_loss": 6.936071395874023
    },
    {
      "epoch": 0.7089430894308943,
      "step": 3270,
      "training_loss": 7.699833393096924
    },
    {
      "epoch": 0.7089430894308943,
      "step": 3270,
      "training_loss": 7.166477203369141
    },
    {
      "epoch": 0.7089430894308943,
      "step": 3270,
      "training_loss": 5.059610843658447
    },
    {
      "epoch": 0.709159891598916,
      "step": 3271,
      "training_loss": 5.707638740539551
    },
    {
      "epoch": 0.709159891598916,
      "step": 3271,
      "training_loss": 4.929976463317871
    },
    {
      "epoch": 0.709159891598916,
      "step": 3271,
      "training_loss": 6.039107799530029
    },
    {
      "epoch": 0.709159891598916,
      "step": 3271,
      "training_loss": 7.139148712158203
    },
    {
      "epoch": 0.7093766937669377,
      "grad_norm": 17.704557418823242,
      "learning_rate": 1e-05,
      "loss": 6.5221,
      "step": 3272
    },
    {
      "epoch": 0.7093766937669377,
      "step": 3272,
      "training_loss": 5.922615051269531
    },
    {
      "epoch": 0.7093766937669377,
      "step": 3272,
      "training_loss": 6.694393634796143
    },
    {
      "epoch": 0.7093766937669377,
      "step": 3272,
      "training_loss": 6.706051349639893
    },
    {
      "epoch": 0.7093766937669377,
      "step": 3272,
      "training_loss": 6.533634185791016
    },
    {
      "epoch": 0.7095934959349594,
      "step": 3273,
      "training_loss": 6.156032085418701
    },
    {
      "epoch": 0.7095934959349594,
      "step": 3273,
      "training_loss": 8.727543830871582
    },
    {
      "epoch": 0.7095934959349594,
      "step": 3273,
      "training_loss": 6.673679828643799
    },
    {
      "epoch": 0.7095934959349594,
      "step": 3273,
      "training_loss": 4.097250461578369
    },
    {
      "epoch": 0.709810298102981,
      "step": 3274,
      "training_loss": 6.681227207183838
    },
    {
      "epoch": 0.709810298102981,
      "step": 3274,
      "training_loss": 6.717740058898926
    },
    {
      "epoch": 0.709810298102981,
      "step": 3274,
      "training_loss": 5.197203636169434
    },
    {
      "epoch": 0.709810298102981,
      "step": 3274,
      "training_loss": 5.187058925628662
    },
    {
      "epoch": 0.7100271002710027,
      "step": 3275,
      "training_loss": 7.551148414611816
    },
    {
      "epoch": 0.7100271002710027,
      "step": 3275,
      "training_loss": 7.670369625091553
    },
    {
      "epoch": 0.7100271002710027,
      "step": 3275,
      "training_loss": 6.963388919830322
    },
    {
      "epoch": 0.7100271002710027,
      "step": 3275,
      "training_loss": 4.613186359405518
    },
    {
      "epoch": 0.7102439024390244,
      "grad_norm": 27.83745002746582,
      "learning_rate": 1e-05,
      "loss": 6.3808,
      "step": 3276
    },
    {
      "epoch": 0.7102439024390244,
      "step": 3276,
      "training_loss": 7.519663333892822
    },
    {
      "epoch": 0.7102439024390244,
      "step": 3276,
      "training_loss": 6.048692226409912
    },
    {
      "epoch": 0.7102439024390244,
      "step": 3276,
      "training_loss": 5.934190273284912
    },
    {
      "epoch": 0.7102439024390244,
      "step": 3276,
      "training_loss": 7.113693714141846
    },
    {
      "epoch": 0.710460704607046,
      "step": 3277,
      "training_loss": 6.049579620361328
    },
    {
      "epoch": 0.710460704607046,
      "step": 3277,
      "training_loss": 6.998978137969971
    },
    {
      "epoch": 0.710460704607046,
      "step": 3277,
      "training_loss": 6.5424699783325195
    },
    {
      "epoch": 0.710460704607046,
      "step": 3277,
      "training_loss": 6.790166854858398
    },
    {
      "epoch": 0.7106775067750678,
      "step": 3278,
      "training_loss": 5.888152122497559
    },
    {
      "epoch": 0.7106775067750678,
      "step": 3278,
      "training_loss": 6.160428524017334
    },
    {
      "epoch": 0.7106775067750678,
      "step": 3278,
      "training_loss": 5.119399070739746
    },
    {
      "epoch": 0.7106775067750678,
      "step": 3278,
      "training_loss": 3.372910261154175
    },
    {
      "epoch": 0.7108943089430895,
      "step": 3279,
      "training_loss": 6.734951972961426
    },
    {
      "epoch": 0.7108943089430895,
      "step": 3279,
      "training_loss": 7.601409912109375
    },
    {
      "epoch": 0.7108943089430895,
      "step": 3279,
      "training_loss": 6.726966381072998
    },
    {
      "epoch": 0.7108943089430895,
      "step": 3279,
      "training_loss": 7.589437007904053
    },
    {
      "epoch": 0.7111111111111111,
      "grad_norm": 16.45716094970703,
      "learning_rate": 1e-05,
      "loss": 6.3869,
      "step": 3280
    },
    {
      "epoch": 0.7111111111111111,
      "step": 3280,
      "training_loss": 5.809656143188477
    },
    {
      "epoch": 0.7111111111111111,
      "step": 3280,
      "training_loss": 6.817668914794922
    },
    {
      "epoch": 0.7111111111111111,
      "step": 3280,
      "training_loss": 5.464910507202148
    },
    {
      "epoch": 0.7111111111111111,
      "step": 3280,
      "training_loss": 5.918206214904785
    },
    {
      "epoch": 0.7113279132791328,
      "step": 3281,
      "training_loss": 6.717378616333008
    },
    {
      "epoch": 0.7113279132791328,
      "step": 3281,
      "training_loss": 8.5756196975708
    },
    {
      "epoch": 0.7113279132791328,
      "step": 3281,
      "training_loss": 4.800445556640625
    },
    {
      "epoch": 0.7113279132791328,
      "step": 3281,
      "training_loss": 5.932180881500244
    },
    {
      "epoch": 0.7115447154471545,
      "step": 3282,
      "training_loss": 7.501706600189209
    },
    {
      "epoch": 0.7115447154471545,
      "step": 3282,
      "training_loss": 6.980207920074463
    },
    {
      "epoch": 0.7115447154471545,
      "step": 3282,
      "training_loss": 4.869354724884033
    },
    {
      "epoch": 0.7115447154471545,
      "step": 3282,
      "training_loss": 5.499576091766357
    },
    {
      "epoch": 0.7117615176151761,
      "step": 3283,
      "training_loss": 5.20485258102417
    },
    {
      "epoch": 0.7117615176151761,
      "step": 3283,
      "training_loss": 6.308621406555176
    },
    {
      "epoch": 0.7117615176151761,
      "step": 3283,
      "training_loss": 7.224209785461426
    },
    {
      "epoch": 0.7117615176151761,
      "step": 3283,
      "training_loss": 6.19498872756958
    },
    {
      "epoch": 0.7119783197831978,
      "grad_norm": 18.296833038330078,
      "learning_rate": 1e-05,
      "loss": 6.2387,
      "step": 3284
    },
    {
      "epoch": 0.7119783197831978,
      "step": 3284,
      "training_loss": 6.7554168701171875
    },
    {
      "epoch": 0.7119783197831978,
      "step": 3284,
      "training_loss": 6.780974864959717
    },
    {
      "epoch": 0.7119783197831978,
      "step": 3284,
      "training_loss": 6.16656494140625
    },
    {
      "epoch": 0.7119783197831978,
      "step": 3284,
      "training_loss": 5.619774341583252
    },
    {
      "epoch": 0.7121951219512195,
      "step": 3285,
      "training_loss": 6.521355152130127
    },
    {
      "epoch": 0.7121951219512195,
      "step": 3285,
      "training_loss": 7.834820747375488
    },
    {
      "epoch": 0.7121951219512195,
      "step": 3285,
      "training_loss": 7.697204113006592
    },
    {
      "epoch": 0.7121951219512195,
      "step": 3285,
      "training_loss": 3.9656169414520264
    },
    {
      "epoch": 0.7124119241192411,
      "step": 3286,
      "training_loss": 4.972761631011963
    },
    {
      "epoch": 0.7124119241192411,
      "step": 3286,
      "training_loss": 4.640703201293945
    },
    {
      "epoch": 0.7124119241192411,
      "step": 3286,
      "training_loss": 6.404727458953857
    },
    {
      "epoch": 0.7124119241192411,
      "step": 3286,
      "training_loss": 6.594114780426025
    },
    {
      "epoch": 0.7126287262872629,
      "step": 3287,
      "training_loss": 3.3106472492218018
    },
    {
      "epoch": 0.7126287262872629,
      "step": 3287,
      "training_loss": 4.5969977378845215
    },
    {
      "epoch": 0.7126287262872629,
      "step": 3287,
      "training_loss": 6.525857448577881
    },
    {
      "epoch": 0.7126287262872629,
      "step": 3287,
      "training_loss": 5.154240608215332
    },
    {
      "epoch": 0.7128455284552846,
      "grad_norm": 20.18553924560547,
      "learning_rate": 1e-05,
      "loss": 5.8464,
      "step": 3288
    },
    {
      "epoch": 0.7128455284552846,
      "step": 3288,
      "training_loss": 6.898680210113525
    },
    {
      "epoch": 0.7128455284552846,
      "step": 3288,
      "training_loss": 4.776285171508789
    },
    {
      "epoch": 0.7128455284552846,
      "step": 3288,
      "training_loss": 6.428489685058594
    },
    {
      "epoch": 0.7128455284552846,
      "step": 3288,
      "training_loss": 6.91285514831543
    },
    {
      "epoch": 0.7130623306233063,
      "step": 3289,
      "training_loss": 6.745452880859375
    },
    {
      "epoch": 0.7130623306233063,
      "step": 3289,
      "training_loss": 7.390141010284424
    },
    {
      "epoch": 0.7130623306233063,
      "step": 3289,
      "training_loss": 7.144887924194336
    },
    {
      "epoch": 0.7130623306233063,
      "step": 3289,
      "training_loss": 6.361740589141846
    },
    {
      "epoch": 0.7132791327913279,
      "step": 3290,
      "training_loss": 6.854256629943848
    },
    {
      "epoch": 0.7132791327913279,
      "step": 3290,
      "training_loss": 7.302623748779297
    },
    {
      "epoch": 0.7132791327913279,
      "step": 3290,
      "training_loss": 6.28508186340332
    },
    {
      "epoch": 0.7132791327913279,
      "step": 3290,
      "training_loss": 6.230624675750732
    },
    {
      "epoch": 0.7134959349593496,
      "step": 3291,
      "training_loss": 8.16696548461914
    },
    {
      "epoch": 0.7134959349593496,
      "step": 3291,
      "training_loss": 4.76409912109375
    },
    {
      "epoch": 0.7134959349593496,
      "step": 3291,
      "training_loss": 5.784188270568848
    },
    {
      "epoch": 0.7134959349593496,
      "step": 3291,
      "training_loss": 2.8315765857696533
    },
    {
      "epoch": 0.7137127371273713,
      "grad_norm": 20.89679718017578,
      "learning_rate": 1e-05,
      "loss": 6.3049,
      "step": 3292
    },
    {
      "epoch": 0.7137127371273713,
      "step": 3292,
      "training_loss": 8.678778648376465
    },
    {
      "epoch": 0.7137127371273713,
      "step": 3292,
      "training_loss": 6.911818027496338
    },
    {
      "epoch": 0.7137127371273713,
      "step": 3292,
      "training_loss": 6.697808265686035
    },
    {
      "epoch": 0.7137127371273713,
      "step": 3292,
      "training_loss": 6.289174556732178
    },
    {
      "epoch": 0.7139295392953929,
      "step": 3293,
      "training_loss": 6.405233860015869
    },
    {
      "epoch": 0.7139295392953929,
      "step": 3293,
      "training_loss": 7.640297889709473
    },
    {
      "epoch": 0.7139295392953929,
      "step": 3293,
      "training_loss": 7.291983127593994
    },
    {
      "epoch": 0.7139295392953929,
      "step": 3293,
      "training_loss": 6.668885231018066
    },
    {
      "epoch": 0.7141463414634146,
      "step": 3294,
      "training_loss": 6.159282207489014
    },
    {
      "epoch": 0.7141463414634146,
      "step": 3294,
      "training_loss": 7.124302864074707
    },
    {
      "epoch": 0.7141463414634146,
      "step": 3294,
      "training_loss": 3.3135411739349365
    },
    {
      "epoch": 0.7141463414634146,
      "step": 3294,
      "training_loss": 5.557960510253906
    },
    {
      "epoch": 0.7143631436314363,
      "step": 3295,
      "training_loss": 6.784581184387207
    },
    {
      "epoch": 0.7143631436314363,
      "step": 3295,
      "training_loss": 5.762129306793213
    },
    {
      "epoch": 0.7143631436314363,
      "step": 3295,
      "training_loss": 6.571767330169678
    },
    {
      "epoch": 0.7143631436314363,
      "step": 3295,
      "training_loss": 6.41032075881958
    },
    {
      "epoch": 0.714579945799458,
      "grad_norm": 17.474645614624023,
      "learning_rate": 1e-05,
      "loss": 6.5167,
      "step": 3296
    },
    {
      "epoch": 0.714579945799458,
      "step": 3296,
      "training_loss": 2.5531742572784424
    },
    {
      "epoch": 0.714579945799458,
      "step": 3296,
      "training_loss": 7.75830602645874
    },
    {
      "epoch": 0.714579945799458,
      "step": 3296,
      "training_loss": 6.551690578460693
    },
    {
      "epoch": 0.714579945799458,
      "step": 3296,
      "training_loss": 7.095224857330322
    },
    {
      "epoch": 0.7147967479674797,
      "step": 3297,
      "training_loss": 5.155409812927246
    },
    {
      "epoch": 0.7147967479674797,
      "step": 3297,
      "training_loss": 7.098853588104248
    },
    {
      "epoch": 0.7147967479674797,
      "step": 3297,
      "training_loss": 6.55515193939209
    },
    {
      "epoch": 0.7147967479674797,
      "step": 3297,
      "training_loss": 6.369427680969238
    },
    {
      "epoch": 0.7150135501355014,
      "step": 3298,
      "training_loss": 6.449789047241211
    },
    {
      "epoch": 0.7150135501355014,
      "step": 3298,
      "training_loss": 6.054934501647949
    },
    {
      "epoch": 0.7150135501355014,
      "step": 3298,
      "training_loss": 7.363792896270752
    },
    {
      "epoch": 0.7150135501355014,
      "step": 3298,
      "training_loss": 6.937661170959473
    },
    {
      "epoch": 0.715230352303523,
      "step": 3299,
      "training_loss": 5.241330623626709
    },
    {
      "epoch": 0.715230352303523,
      "step": 3299,
      "training_loss": 3.6618635654449463
    },
    {
      "epoch": 0.715230352303523,
      "step": 3299,
      "training_loss": 2.6536521911621094
    },
    {
      "epoch": 0.715230352303523,
      "step": 3299,
      "training_loss": 6.530699729919434
    },
    {
      "epoch": 0.7154471544715447,
      "grad_norm": 24.646074295043945,
      "learning_rate": 1e-05,
      "loss": 5.8769,
      "step": 3300
    },
    {
      "epoch": 0.7154471544715447,
      "step": 3300,
      "training_loss": 8.221455574035645
    },
    {
      "epoch": 0.7154471544715447,
      "step": 3300,
      "training_loss": 3.4273734092712402
    },
    {
      "epoch": 0.7154471544715447,
      "step": 3300,
      "training_loss": 5.579740524291992
    },
    {
      "epoch": 0.7154471544715447,
      "step": 3300,
      "training_loss": 5.311356067657471
    },
    {
      "epoch": 0.7156639566395664,
      "step": 3301,
      "training_loss": 2.8453562259674072
    },
    {
      "epoch": 0.7156639566395664,
      "step": 3301,
      "training_loss": 5.515941619873047
    },
    {
      "epoch": 0.7156639566395664,
      "step": 3301,
      "training_loss": 2.47594952583313
    },
    {
      "epoch": 0.7156639566395664,
      "step": 3301,
      "training_loss": 6.688076496124268
    },
    {
      "epoch": 0.7158807588075881,
      "step": 3302,
      "training_loss": 6.622745990753174
    },
    {
      "epoch": 0.7158807588075881,
      "step": 3302,
      "training_loss": 8.350643157958984
    },
    {
      "epoch": 0.7158807588075881,
      "step": 3302,
      "training_loss": 7.328131675720215
    },
    {
      "epoch": 0.7158807588075881,
      "step": 3302,
      "training_loss": 6.606770992279053
    },
    {
      "epoch": 0.7160975609756097,
      "step": 3303,
      "training_loss": 7.950990676879883
    },
    {
      "epoch": 0.7160975609756097,
      "step": 3303,
      "training_loss": 6.701195240020752
    },
    {
      "epoch": 0.7160975609756097,
      "step": 3303,
      "training_loss": 6.03985071182251
    },
    {
      "epoch": 0.7160975609756097,
      "step": 3303,
      "training_loss": 5.7246994972229
    },
    {
      "epoch": 0.7163143631436314,
      "grad_norm": 23.45844078063965,
      "learning_rate": 1e-05,
      "loss": 5.9619,
      "step": 3304
    },
    {
      "epoch": 0.7163143631436314,
      "step": 3304,
      "training_loss": 6.248086929321289
    },
    {
      "epoch": 0.7163143631436314,
      "step": 3304,
      "training_loss": 5.056768894195557
    },
    {
      "epoch": 0.7163143631436314,
      "step": 3304,
      "training_loss": 7.1949028968811035
    },
    {
      "epoch": 0.7163143631436314,
      "step": 3304,
      "training_loss": 7.61448860168457
    },
    {
      "epoch": 0.7165311653116531,
      "step": 3305,
      "training_loss": 6.256462097167969
    },
    {
      "epoch": 0.7165311653116531,
      "step": 3305,
      "training_loss": 4.796989917755127
    },
    {
      "epoch": 0.7165311653116531,
      "step": 3305,
      "training_loss": 5.689591407775879
    },
    {
      "epoch": 0.7165311653116531,
      "step": 3305,
      "training_loss": 6.541240215301514
    },
    {
      "epoch": 0.7167479674796748,
      "step": 3306,
      "training_loss": 6.788768768310547
    },
    {
      "epoch": 0.7167479674796748,
      "step": 3306,
      "training_loss": 6.737410545349121
    },
    {
      "epoch": 0.7167479674796748,
      "step": 3306,
      "training_loss": 7.305803298950195
    },
    {
      "epoch": 0.7167479674796748,
      "step": 3306,
      "training_loss": 5.122533798217773
    },
    {
      "epoch": 0.7169647696476965,
      "step": 3307,
      "training_loss": 7.488849639892578
    },
    {
      "epoch": 0.7169647696476965,
      "step": 3307,
      "training_loss": 5.338515758514404
    },
    {
      "epoch": 0.7169647696476965,
      "step": 3307,
      "training_loss": 2.4304635524749756
    },
    {
      "epoch": 0.7169647696476965,
      "step": 3307,
      "training_loss": 6.487484455108643
    },
    {
      "epoch": 0.7171815718157182,
      "grad_norm": 21.28504180908203,
      "learning_rate": 1e-05,
      "loss": 6.0686,
      "step": 3308
    },
    {
      "epoch": 0.7171815718157182,
      "step": 3308,
      "training_loss": 6.160282135009766
    },
    {
      "epoch": 0.7171815718157182,
      "step": 3308,
      "training_loss": 5.947109699249268
    },
    {
      "epoch": 0.7171815718157182,
      "step": 3308,
      "training_loss": 7.474806785583496
    },
    {
      "epoch": 0.7171815718157182,
      "step": 3308,
      "training_loss": 6.721998691558838
    },
    {
      "epoch": 0.7173983739837398,
      "step": 3309,
      "training_loss": 2.452871084213257
    },
    {
      "epoch": 0.7173983739837398,
      "step": 3309,
      "training_loss": 6.53848123550415
    },
    {
      "epoch": 0.7173983739837398,
      "step": 3309,
      "training_loss": 5.346312999725342
    },
    {
      "epoch": 0.7173983739837398,
      "step": 3309,
      "training_loss": 7.076648235321045
    },
    {
      "epoch": 0.7176151761517615,
      "step": 3310,
      "training_loss": 5.845006942749023
    },
    {
      "epoch": 0.7176151761517615,
      "step": 3310,
      "training_loss": 6.744797706604004
    },
    {
      "epoch": 0.7176151761517615,
      "step": 3310,
      "training_loss": 5.745834827423096
    },
    {
      "epoch": 0.7176151761517615,
      "step": 3310,
      "training_loss": 6.121273517608643
    },
    {
      "epoch": 0.7178319783197832,
      "step": 3311,
      "training_loss": 6.212186336517334
    },
    {
      "epoch": 0.7178319783197832,
      "step": 3311,
      "training_loss": 6.215951919555664
    },
    {
      "epoch": 0.7178319783197832,
      "step": 3311,
      "training_loss": 6.46571683883667
    },
    {
      "epoch": 0.7178319783197832,
      "step": 3311,
      "training_loss": 7.448704242706299
    },
    {
      "epoch": 0.7180487804878048,
      "grad_norm": 13.424793243408203,
      "learning_rate": 1e-05,
      "loss": 6.1574,
      "step": 3312
    },
    {
      "epoch": 0.7180487804878048,
      "step": 3312,
      "training_loss": 6.869588375091553
    },
    {
      "epoch": 0.7180487804878048,
      "step": 3312,
      "training_loss": 5.3969902992248535
    },
    {
      "epoch": 0.7180487804878048,
      "step": 3312,
      "training_loss": 5.158435344696045
    },
    {
      "epoch": 0.7180487804878048,
      "step": 3312,
      "training_loss": 5.827205657958984
    },
    {
      "epoch": 0.7182655826558265,
      "step": 3313,
      "training_loss": 3.7449934482574463
    },
    {
      "epoch": 0.7182655826558265,
      "step": 3313,
      "training_loss": 7.142655372619629
    },
    {
      "epoch": 0.7182655826558265,
      "step": 3313,
      "training_loss": 5.992532253265381
    },
    {
      "epoch": 0.7182655826558265,
      "step": 3313,
      "training_loss": 6.098762512207031
    },
    {
      "epoch": 0.7184823848238482,
      "step": 3314,
      "training_loss": 6.764726638793945
    },
    {
      "epoch": 0.7184823848238482,
      "step": 3314,
      "training_loss": 3.8344926834106445
    },
    {
      "epoch": 0.7184823848238482,
      "step": 3314,
      "training_loss": 8.991618156433105
    },
    {
      "epoch": 0.7184823848238482,
      "step": 3314,
      "training_loss": 6.938658237457275
    },
    {
      "epoch": 0.71869918699187,
      "step": 3315,
      "training_loss": 6.7722930908203125
    },
    {
      "epoch": 0.71869918699187,
      "step": 3315,
      "training_loss": 4.86021614074707
    },
    {
      "epoch": 0.71869918699187,
      "step": 3315,
      "training_loss": 7.472571849822998
    },
    {
      "epoch": 0.71869918699187,
      "step": 3315,
      "training_loss": 5.38618278503418
    },
    {
      "epoch": 0.7189159891598916,
      "grad_norm": 14.342304229736328,
      "learning_rate": 1e-05,
      "loss": 6.0782,
      "step": 3316
    },
    {
      "epoch": 0.7189159891598916,
      "step": 3316,
      "training_loss": 8.386993408203125
    },
    {
      "epoch": 0.7189159891598916,
      "step": 3316,
      "training_loss": 5.743960857391357
    },
    {
      "epoch": 0.7189159891598916,
      "step": 3316,
      "training_loss": 7.1314215660095215
    },
    {
      "epoch": 0.7189159891598916,
      "step": 3316,
      "training_loss": 3.0964772701263428
    },
    {
      "epoch": 0.7191327913279133,
      "step": 3317,
      "training_loss": 5.694291591644287
    },
    {
      "epoch": 0.7191327913279133,
      "step": 3317,
      "training_loss": 6.503837585449219
    },
    {
      "epoch": 0.7191327913279133,
      "step": 3317,
      "training_loss": 7.46552038192749
    },
    {
      "epoch": 0.7191327913279133,
      "step": 3317,
      "training_loss": 3.329904556274414
    },
    {
      "epoch": 0.719349593495935,
      "step": 3318,
      "training_loss": 7.124154567718506
    },
    {
      "epoch": 0.719349593495935,
      "step": 3318,
      "training_loss": 5.576228141784668
    },
    {
      "epoch": 0.719349593495935,
      "step": 3318,
      "training_loss": 6.882937908172607
    },
    {
      "epoch": 0.719349593495935,
      "step": 3318,
      "training_loss": 7.195732593536377
    },
    {
      "epoch": 0.7195663956639566,
      "step": 3319,
      "training_loss": 7.047249794006348
    },
    {
      "epoch": 0.7195663956639566,
      "step": 3319,
      "training_loss": 6.838045597076416
    },
    {
      "epoch": 0.7195663956639566,
      "step": 3319,
      "training_loss": 7.517048358917236
    },
    {
      "epoch": 0.7195663956639566,
      "step": 3319,
      "training_loss": 5.541378498077393
    },
    {
      "epoch": 0.7197831978319783,
      "grad_norm": 24.98591423034668,
      "learning_rate": 1e-05,
      "loss": 6.3172,
      "step": 3320
    },
    {
      "epoch": 0.7197831978319783,
      "step": 3320,
      "training_loss": 6.438120365142822
    },
    {
      "epoch": 0.7197831978319783,
      "step": 3320,
      "training_loss": 6.26807975769043
    },
    {
      "epoch": 0.7197831978319783,
      "step": 3320,
      "training_loss": 7.00496244430542
    },
    {
      "epoch": 0.7197831978319783,
      "step": 3320,
      "training_loss": 4.080680847167969
    },
    {
      "epoch": 0.72,
      "step": 3321,
      "training_loss": 2.9891655445098877
    },
    {
      "epoch": 0.72,
      "step": 3321,
      "training_loss": 6.026857376098633
    },
    {
      "epoch": 0.72,
      "step": 3321,
      "training_loss": 8.456979751586914
    },
    {
      "epoch": 0.72,
      "step": 3321,
      "training_loss": 6.96103048324585
    },
    {
      "epoch": 0.7202168021680216,
      "step": 3322,
      "training_loss": 6.546562671661377
    },
    {
      "epoch": 0.7202168021680216,
      "step": 3322,
      "training_loss": 6.193863868713379
    },
    {
      "epoch": 0.7202168021680216,
      "step": 3322,
      "training_loss": 5.761203765869141
    },
    {
      "epoch": 0.7202168021680216,
      "step": 3322,
      "training_loss": 6.528040885925293
    },
    {
      "epoch": 0.7204336043360433,
      "step": 3323,
      "training_loss": 6.197103023529053
    },
    {
      "epoch": 0.7204336043360433,
      "step": 3323,
      "training_loss": 7.619004249572754
    },
    {
      "epoch": 0.7204336043360433,
      "step": 3323,
      "training_loss": 2.6479222774505615
    },
    {
      "epoch": 0.7204336043360433,
      "step": 3323,
      "training_loss": 5.523630142211914
    },
    {
      "epoch": 0.7206504065040651,
      "grad_norm": 21.092174530029297,
      "learning_rate": 1e-05,
      "loss": 5.9527,
      "step": 3324
    },
    {
      "epoch": 0.7206504065040651,
      "step": 3324,
      "training_loss": 5.588817596435547
    },
    {
      "epoch": 0.7206504065040651,
      "step": 3324,
      "training_loss": 6.355126857757568
    },
    {
      "epoch": 0.7206504065040651,
      "step": 3324,
      "training_loss": 6.105976104736328
    },
    {
      "epoch": 0.7206504065040651,
      "step": 3324,
      "training_loss": 3.9255154132843018
    },
    {
      "epoch": 0.7208672086720868,
      "step": 3325,
      "training_loss": 6.5425496101379395
    },
    {
      "epoch": 0.7208672086720868,
      "step": 3325,
      "training_loss": 6.7739081382751465
    },
    {
      "epoch": 0.7208672086720868,
      "step": 3325,
      "training_loss": 5.367812156677246
    },
    {
      "epoch": 0.7208672086720868,
      "step": 3325,
      "training_loss": 6.0668511390686035
    },
    {
      "epoch": 0.7210840108401084,
      "step": 3326,
      "training_loss": 7.777595043182373
    },
    {
      "epoch": 0.7210840108401084,
      "step": 3326,
      "training_loss": 8.341368675231934
    },
    {
      "epoch": 0.7210840108401084,
      "step": 3326,
      "training_loss": 6.082538604736328
    },
    {
      "epoch": 0.7210840108401084,
      "step": 3326,
      "training_loss": 7.945933818817139
    },
    {
      "epoch": 0.7213008130081301,
      "step": 3327,
      "training_loss": 6.464187145233154
    },
    {
      "epoch": 0.7213008130081301,
      "step": 3327,
      "training_loss": 7.652529239654541
    },
    {
      "epoch": 0.7213008130081301,
      "step": 3327,
      "training_loss": 6.578407287597656
    },
    {
      "epoch": 0.7213008130081301,
      "step": 3327,
      "training_loss": 3.413916826248169
    },
    {
      "epoch": 0.7215176151761518,
      "grad_norm": 17.499603271484375,
      "learning_rate": 1e-05,
      "loss": 6.3114,
      "step": 3328
    },
    {
      "epoch": 0.7215176151761518,
      "step": 3328,
      "training_loss": 5.001299858093262
    },
    {
      "epoch": 0.7215176151761518,
      "step": 3328,
      "training_loss": 5.788229465484619
    },
    {
      "epoch": 0.7215176151761518,
      "step": 3328,
      "training_loss": 6.832059383392334
    },
    {
      "epoch": 0.7215176151761518,
      "step": 3328,
      "training_loss": 4.124669075012207
    },
    {
      "epoch": 0.7217344173441734,
      "step": 3329,
      "training_loss": 5.572203636169434
    },
    {
      "epoch": 0.7217344173441734,
      "step": 3329,
      "training_loss": 6.987218856811523
    },
    {
      "epoch": 0.7217344173441734,
      "step": 3329,
      "training_loss": 6.217533111572266
    },
    {
      "epoch": 0.7217344173441734,
      "step": 3329,
      "training_loss": 6.416476249694824
    },
    {
      "epoch": 0.7219512195121951,
      "step": 3330,
      "training_loss": 4.96303129196167
    },
    {
      "epoch": 0.7219512195121951,
      "step": 3330,
      "training_loss": 7.192381381988525
    },
    {
      "epoch": 0.7219512195121951,
      "step": 3330,
      "training_loss": 7.860305309295654
    },
    {
      "epoch": 0.7219512195121951,
      "step": 3330,
      "training_loss": 7.11279821395874
    },
    {
      "epoch": 0.7221680216802168,
      "step": 3331,
      "training_loss": 5.1678032875061035
    },
    {
      "epoch": 0.7221680216802168,
      "step": 3331,
      "training_loss": 7.365911483764648
    },
    {
      "epoch": 0.7221680216802168,
      "step": 3331,
      "training_loss": 8.265645980834961
    },
    {
      "epoch": 0.7221680216802168,
      "step": 3331,
      "training_loss": 6.087254524230957
    },
    {
      "epoch": 0.7223848238482384,
      "grad_norm": 17.11832046508789,
      "learning_rate": 1e-05,
      "loss": 6.3097,
      "step": 3332
    },
    {
      "epoch": 0.7223848238482384,
      "step": 3332,
      "training_loss": 7.097692966461182
    },
    {
      "epoch": 0.7223848238482384,
      "step": 3332,
      "training_loss": 4.685829162597656
    },
    {
      "epoch": 0.7223848238482384,
      "step": 3332,
      "training_loss": 7.421829700469971
    },
    {
      "epoch": 0.7223848238482384,
      "step": 3332,
      "training_loss": 7.355122089385986
    },
    {
      "epoch": 0.7226016260162602,
      "step": 3333,
      "training_loss": 7.488887786865234
    },
    {
      "epoch": 0.7226016260162602,
      "step": 3333,
      "training_loss": 6.390112400054932
    },
    {
      "epoch": 0.7226016260162602,
      "step": 3333,
      "training_loss": 4.8986496925354
    },
    {
      "epoch": 0.7226016260162602,
      "step": 3333,
      "training_loss": 6.884182929992676
    },
    {
      "epoch": 0.7228184281842819,
      "step": 3334,
      "training_loss": 5.5843505859375
    },
    {
      "epoch": 0.7228184281842819,
      "step": 3334,
      "training_loss": 7.319401264190674
    },
    {
      "epoch": 0.7228184281842819,
      "step": 3334,
      "training_loss": 6.777008533477783
    },
    {
      "epoch": 0.7228184281842819,
      "step": 3334,
      "training_loss": 6.195058822631836
    },
    {
      "epoch": 0.7230352303523035,
      "step": 3335,
      "training_loss": 7.223642826080322
    },
    {
      "epoch": 0.7230352303523035,
      "step": 3335,
      "training_loss": 6.1863274574279785
    },
    {
      "epoch": 0.7230352303523035,
      "step": 3335,
      "training_loss": 7.991448402404785
    },
    {
      "epoch": 0.7230352303523035,
      "step": 3335,
      "training_loss": 8.54471492767334
    },
    {
      "epoch": 0.7232520325203252,
      "grad_norm": 19.678081512451172,
      "learning_rate": 1e-05,
      "loss": 6.7528,
      "step": 3336
    },
    {
      "epoch": 0.7232520325203252,
      "step": 3336,
      "training_loss": 6.787146091461182
    },
    {
      "epoch": 0.7232520325203252,
      "step": 3336,
      "training_loss": 7.866629123687744
    },
    {
      "epoch": 0.7232520325203252,
      "step": 3336,
      "training_loss": 7.2781829833984375
    },
    {
      "epoch": 0.7232520325203252,
      "step": 3336,
      "training_loss": 6.864284992218018
    },
    {
      "epoch": 0.7234688346883469,
      "step": 3337,
      "training_loss": 6.684945106506348
    },
    {
      "epoch": 0.7234688346883469,
      "step": 3337,
      "training_loss": 8.761483192443848
    },
    {
      "epoch": 0.7234688346883469,
      "step": 3337,
      "training_loss": 6.441826343536377
    },
    {
      "epoch": 0.7234688346883469,
      "step": 3337,
      "training_loss": 7.024759769439697
    },
    {
      "epoch": 0.7236856368563686,
      "step": 3338,
      "training_loss": 4.527437210083008
    },
    {
      "epoch": 0.7236856368563686,
      "step": 3338,
      "training_loss": 7.806442737579346
    },
    {
      "epoch": 0.7236856368563686,
      "step": 3338,
      "training_loss": 7.608588218688965
    },
    {
      "epoch": 0.7236856368563686,
      "step": 3338,
      "training_loss": 6.510757923126221
    },
    {
      "epoch": 0.7239024390243902,
      "step": 3339,
      "training_loss": 6.048748016357422
    },
    {
      "epoch": 0.7239024390243902,
      "step": 3339,
      "training_loss": 7.148819446563721
    },
    {
      "epoch": 0.7239024390243902,
      "step": 3339,
      "training_loss": 8.83248519897461
    },
    {
      "epoch": 0.7239024390243902,
      "step": 3339,
      "training_loss": 7.410531044006348
    },
    {
      "epoch": 0.7241192411924119,
      "grad_norm": 29.022438049316406,
      "learning_rate": 1e-05,
      "loss": 7.1002,
      "step": 3340
    },
    {
      "epoch": 0.7241192411924119,
      "step": 3340,
      "training_loss": 6.8928303718566895
    },
    {
      "epoch": 0.7241192411924119,
      "step": 3340,
      "training_loss": 6.6587815284729
    },
    {
      "epoch": 0.7241192411924119,
      "step": 3340,
      "training_loss": 7.244501113891602
    },
    {
      "epoch": 0.7241192411924119,
      "step": 3340,
      "training_loss": 5.656912326812744
    },
    {
      "epoch": 0.7243360433604336,
      "step": 3341,
      "training_loss": 7.84242582321167
    },
    {
      "epoch": 0.7243360433604336,
      "step": 3341,
      "training_loss": 6.903653621673584
    },
    {
      "epoch": 0.7243360433604336,
      "step": 3341,
      "training_loss": 7.276284217834473
    },
    {
      "epoch": 0.7243360433604336,
      "step": 3341,
      "training_loss": 6.583371162414551
    },
    {
      "epoch": 0.7245528455284553,
      "step": 3342,
      "training_loss": 6.626897811889648
    },
    {
      "epoch": 0.7245528455284553,
      "step": 3342,
      "training_loss": 6.912393569946289
    },
    {
      "epoch": 0.7245528455284553,
      "step": 3342,
      "training_loss": 6.1371917724609375
    },
    {
      "epoch": 0.7245528455284553,
      "step": 3342,
      "training_loss": 6.470388412475586
    },
    {
      "epoch": 0.724769647696477,
      "step": 3343,
      "training_loss": 5.796166896820068
    },
    {
      "epoch": 0.724769647696477,
      "step": 3343,
      "training_loss": 6.4986653327941895
    },
    {
      "epoch": 0.724769647696477,
      "step": 3343,
      "training_loss": 6.311161518096924
    },
    {
      "epoch": 0.724769647696477,
      "step": 3343,
      "training_loss": 4.728763103485107
    },
    {
      "epoch": 0.7249864498644987,
      "grad_norm": 20.872879028320312,
      "learning_rate": 1e-05,
      "loss": 6.5338,
      "step": 3344
    },
    {
      "epoch": 0.7249864498644987,
      "step": 3344,
      "training_loss": 7.6865997314453125
    },
    {
      "epoch": 0.7249864498644987,
      "step": 3344,
      "training_loss": 6.465571880340576
    },
    {
      "epoch": 0.7249864498644987,
      "step": 3344,
      "training_loss": 7.969412803649902
    },
    {
      "epoch": 0.7249864498644987,
      "step": 3344,
      "training_loss": 7.102627277374268
    },
    {
      "epoch": 0.7252032520325203,
      "step": 3345,
      "training_loss": 6.806192874908447
    },
    {
      "epoch": 0.7252032520325203,
      "step": 3345,
      "training_loss": 7.068279266357422
    },
    {
      "epoch": 0.7252032520325203,
      "step": 3345,
      "training_loss": 7.227994918823242
    },
    {
      "epoch": 0.7252032520325203,
      "step": 3345,
      "training_loss": 7.085373878479004
    },
    {
      "epoch": 0.725420054200542,
      "step": 3346,
      "training_loss": 5.488677501678467
    },
    {
      "epoch": 0.725420054200542,
      "step": 3346,
      "training_loss": 5.662827968597412
    },
    {
      "epoch": 0.725420054200542,
      "step": 3346,
      "training_loss": 6.095710277557373
    },
    {
      "epoch": 0.725420054200542,
      "step": 3346,
      "training_loss": 5.741085052490234
    },
    {
      "epoch": 0.7256368563685637,
      "step": 3347,
      "training_loss": 5.764721870422363
    },
    {
      "epoch": 0.7256368563685637,
      "step": 3347,
      "training_loss": 6.758047103881836
    },
    {
      "epoch": 0.7256368563685637,
      "step": 3347,
      "training_loss": 6.827402114868164
    },
    {
      "epoch": 0.7256368563685637,
      "step": 3347,
      "training_loss": 6.8685760498046875
    },
    {
      "epoch": 0.7258536585365853,
      "grad_norm": 14.582656860351562,
      "learning_rate": 1e-05,
      "loss": 6.6637,
      "step": 3348
    },
    {
      "epoch": 0.7258536585365853,
      "step": 3348,
      "training_loss": 7.83254337310791
    },
    {
      "epoch": 0.7258536585365853,
      "step": 3348,
      "training_loss": 6.574540615081787
    },
    {
      "epoch": 0.7258536585365853,
      "step": 3348,
      "training_loss": 3.8437418937683105
    },
    {
      "epoch": 0.7258536585365853,
      "step": 3348,
      "training_loss": 6.912189483642578
    },
    {
      "epoch": 0.726070460704607,
      "step": 3349,
      "training_loss": 2.8671984672546387
    },
    {
      "epoch": 0.726070460704607,
      "step": 3349,
      "training_loss": 3.9269042015075684
    },
    {
      "epoch": 0.726070460704607,
      "step": 3349,
      "training_loss": 7.1927289962768555
    },
    {
      "epoch": 0.726070460704607,
      "step": 3349,
      "training_loss": 4.453819751739502
    },
    {
      "epoch": 0.7262872628726287,
      "step": 3350,
      "training_loss": 7.213383674621582
    },
    {
      "epoch": 0.7262872628726287,
      "step": 3350,
      "training_loss": 6.573843479156494
    },
    {
      "epoch": 0.7262872628726287,
      "step": 3350,
      "training_loss": 5.117547512054443
    },
    {
      "epoch": 0.7262872628726287,
      "step": 3350,
      "training_loss": 5.1122145652771
    },
    {
      "epoch": 0.7265040650406505,
      "step": 3351,
      "training_loss": 5.08834981918335
    },
    {
      "epoch": 0.7265040650406505,
      "step": 3351,
      "training_loss": 6.8106818199157715
    },
    {
      "epoch": 0.7265040650406505,
      "step": 3351,
      "training_loss": 6.72083854675293
    },
    {
      "epoch": 0.7265040650406505,
      "step": 3351,
      "training_loss": 5.731445789337158
    },
    {
      "epoch": 0.7267208672086721,
      "grad_norm": 15.139057159423828,
      "learning_rate": 1e-05,
      "loss": 5.7482,
      "step": 3352
    },
    {
      "epoch": 0.7267208672086721,
      "step": 3352,
      "training_loss": 7.728030204772949
    },
    {
      "epoch": 0.7267208672086721,
      "step": 3352,
      "training_loss": 7.554457664489746
    },
    {
      "epoch": 0.7267208672086721,
      "step": 3352,
      "training_loss": 7.353038311004639
    },
    {
      "epoch": 0.7267208672086721,
      "step": 3352,
      "training_loss": 6.599193096160889
    },
    {
      "epoch": 0.7269376693766938,
      "step": 3353,
      "training_loss": 6.596802234649658
    },
    {
      "epoch": 0.7269376693766938,
      "step": 3353,
      "training_loss": 5.496634483337402
    },
    {
      "epoch": 0.7269376693766938,
      "step": 3353,
      "training_loss": 5.939222812652588
    },
    {
      "epoch": 0.7269376693766938,
      "step": 3353,
      "training_loss": 6.244441986083984
    },
    {
      "epoch": 0.7271544715447155,
      "step": 3354,
      "training_loss": 5.911502838134766
    },
    {
      "epoch": 0.7271544715447155,
      "step": 3354,
      "training_loss": 6.368488311767578
    },
    {
      "epoch": 0.7271544715447155,
      "step": 3354,
      "training_loss": 6.1152191162109375
    },
    {
      "epoch": 0.7271544715447155,
      "step": 3354,
      "training_loss": 5.689112663269043
    },
    {
      "epoch": 0.7273712737127371,
      "step": 3355,
      "training_loss": 6.523252964019775
    },
    {
      "epoch": 0.7273712737127371,
      "step": 3355,
      "training_loss": 7.225204944610596
    },
    {
      "epoch": 0.7273712737127371,
      "step": 3355,
      "training_loss": 7.423250675201416
    },
    {
      "epoch": 0.7273712737127371,
      "step": 3355,
      "training_loss": 7.526119709014893
    },
    {
      "epoch": 0.7275880758807588,
      "grad_norm": 18.61560821533203,
      "learning_rate": 1e-05,
      "loss": 6.6434,
      "step": 3356
    },
    {
      "epoch": 0.7275880758807588,
      "step": 3356,
      "training_loss": 7.162371635437012
    },
    {
      "epoch": 0.7275880758807588,
      "step": 3356,
      "training_loss": 7.186352729797363
    },
    {
      "epoch": 0.7275880758807588,
      "step": 3356,
      "training_loss": 6.509942531585693
    },
    {
      "epoch": 0.7275880758807588,
      "step": 3356,
      "training_loss": 6.162529945373535
    },
    {
      "epoch": 0.7278048780487805,
      "step": 3357,
      "training_loss": 4.282952308654785
    },
    {
      "epoch": 0.7278048780487805,
      "step": 3357,
      "training_loss": 6.100783824920654
    },
    {
      "epoch": 0.7278048780487805,
      "step": 3357,
      "training_loss": 6.833410739898682
    },
    {
      "epoch": 0.7278048780487805,
      "step": 3357,
      "training_loss": 6.588354587554932
    },
    {
      "epoch": 0.7280216802168021,
      "step": 3358,
      "training_loss": 11.311570167541504
    },
    {
      "epoch": 0.7280216802168021,
      "step": 3358,
      "training_loss": 2.8402047157287598
    },
    {
      "epoch": 0.7280216802168021,
      "step": 3358,
      "training_loss": 6.9424824714660645
    },
    {
      "epoch": 0.7280216802168021,
      "step": 3358,
      "training_loss": 5.666409015655518
    },
    {
      "epoch": 0.7282384823848238,
      "step": 3359,
      "training_loss": 7.226840019226074
    },
    {
      "epoch": 0.7282384823848238,
      "step": 3359,
      "training_loss": 6.524741172790527
    },
    {
      "epoch": 0.7282384823848238,
      "step": 3359,
      "training_loss": 6.466885566711426
    },
    {
      "epoch": 0.7282384823848238,
      "step": 3359,
      "training_loss": 6.274130344390869
    },
    {
      "epoch": 0.7284552845528456,
      "grad_norm": 25.0185546875,
      "learning_rate": 1e-05,
      "loss": 6.505,
      "step": 3360
    },
    {
      "epoch": 0.7284552845528456,
      "step": 3360,
      "training_loss": 7.184571743011475
    },
    {
      "epoch": 0.7284552845528456,
      "step": 3360,
      "training_loss": 4.6727495193481445
    },
    {
      "epoch": 0.7284552845528456,
      "step": 3360,
      "training_loss": 6.650163173675537
    },
    {
      "epoch": 0.7284552845528456,
      "step": 3360,
      "training_loss": 7.588392734527588
    },
    {
      "epoch": 0.7286720867208673,
      "step": 3361,
      "training_loss": 7.607732772827148
    },
    {
      "epoch": 0.7286720867208673,
      "step": 3361,
      "training_loss": 5.416767597198486
    },
    {
      "epoch": 0.7286720867208673,
      "step": 3361,
      "training_loss": 7.739749431610107
    },
    {
      "epoch": 0.7286720867208673,
      "step": 3361,
      "training_loss": 6.830728054046631
    },
    {
      "epoch": 0.7288888888888889,
      "step": 3362,
      "training_loss": 5.60355281829834
    },
    {
      "epoch": 0.7288888888888889,
      "step": 3362,
      "training_loss": 6.72838830947876
    },
    {
      "epoch": 0.7288888888888889,
      "step": 3362,
      "training_loss": 7.097965240478516
    },
    {
      "epoch": 0.7288888888888889,
      "step": 3362,
      "training_loss": 5.963172912597656
    },
    {
      "epoch": 0.7291056910569106,
      "step": 3363,
      "training_loss": 5.2843804359436035
    },
    {
      "epoch": 0.7291056910569106,
      "step": 3363,
      "training_loss": 5.75443172454834
    },
    {
      "epoch": 0.7291056910569106,
      "step": 3363,
      "training_loss": 7.056328773498535
    },
    {
      "epoch": 0.7291056910569106,
      "step": 3363,
      "training_loss": 7.876281261444092
    },
    {
      "epoch": 0.7293224932249323,
      "grad_norm": 24.591869354248047,
      "learning_rate": 1e-05,
      "loss": 6.566,
      "step": 3364
    },
    {
      "epoch": 0.7293224932249323,
      "step": 3364,
      "training_loss": 7.0237579345703125
    },
    {
      "epoch": 0.7293224932249323,
      "step": 3364,
      "training_loss": 7.934289932250977
    },
    {
      "epoch": 0.7293224932249323,
      "step": 3364,
      "training_loss": 6.842210292816162
    },
    {
      "epoch": 0.7293224932249323,
      "step": 3364,
      "training_loss": 7.826498031616211
    },
    {
      "epoch": 0.7295392953929539,
      "step": 3365,
      "training_loss": 6.452699184417725
    },
    {
      "epoch": 0.7295392953929539,
      "step": 3365,
      "training_loss": 2.9384093284606934
    },
    {
      "epoch": 0.7295392953929539,
      "step": 3365,
      "training_loss": 6.924160003662109
    },
    {
      "epoch": 0.7295392953929539,
      "step": 3365,
      "training_loss": 5.765198707580566
    },
    {
      "epoch": 0.7297560975609756,
      "step": 3366,
      "training_loss": 6.787402153015137
    },
    {
      "epoch": 0.7297560975609756,
      "step": 3366,
      "training_loss": 5.852885723114014
    },
    {
      "epoch": 0.7297560975609756,
      "step": 3366,
      "training_loss": 5.902266502380371
    },
    {
      "epoch": 0.7297560975609756,
      "step": 3366,
      "training_loss": 6.227452278137207
    },
    {
      "epoch": 0.7299728997289973,
      "step": 3367,
      "training_loss": 6.274820327758789
    },
    {
      "epoch": 0.7299728997289973,
      "step": 3367,
      "training_loss": 6.967476844787598
    },
    {
      "epoch": 0.7299728997289973,
      "step": 3367,
      "training_loss": 7.015913963317871
    },
    {
      "epoch": 0.7299728997289973,
      "step": 3367,
      "training_loss": 6.991505146026611
    },
    {
      "epoch": 0.7301897018970189,
      "grad_norm": 13.999876022338867,
      "learning_rate": 1e-05,
      "loss": 6.4829,
      "step": 3368
    },
    {
      "epoch": 0.7301897018970189,
      "step": 3368,
      "training_loss": 5.85863733291626
    },
    {
      "epoch": 0.7301897018970189,
      "step": 3368,
      "training_loss": 6.372853755950928
    },
    {
      "epoch": 0.7301897018970189,
      "step": 3368,
      "training_loss": 5.724661827087402
    },
    {
      "epoch": 0.7301897018970189,
      "step": 3368,
      "training_loss": 6.413987159729004
    },
    {
      "epoch": 0.7304065040650406,
      "step": 3369,
      "training_loss": 6.435941696166992
    },
    {
      "epoch": 0.7304065040650406,
      "step": 3369,
      "training_loss": 5.15693998336792
    },
    {
      "epoch": 0.7304065040650406,
      "step": 3369,
      "training_loss": 6.975508689880371
    },
    {
      "epoch": 0.7304065040650406,
      "step": 3369,
      "training_loss": 7.607649326324463
    },
    {
      "epoch": 0.7306233062330624,
      "step": 3370,
      "training_loss": 5.228237152099609
    },
    {
      "epoch": 0.7306233062330624,
      "step": 3370,
      "training_loss": 7.365293979644775
    },
    {
      "epoch": 0.7306233062330624,
      "step": 3370,
      "training_loss": 7.763095378875732
    },
    {
      "epoch": 0.7306233062330624,
      "step": 3370,
      "training_loss": 3.407601833343506
    },
    {
      "epoch": 0.730840108401084,
      "step": 3371,
      "training_loss": 6.862696170806885
    },
    {
      "epoch": 0.730840108401084,
      "step": 3371,
      "training_loss": 6.374266147613525
    },
    {
      "epoch": 0.730840108401084,
      "step": 3371,
      "training_loss": 5.187985420227051
    },
    {
      "epoch": 0.730840108401084,
      "step": 3371,
      "training_loss": 7.38474178314209
    },
    {
      "epoch": 0.7310569105691057,
      "grad_norm": 21.639474868774414,
      "learning_rate": 1e-05,
      "loss": 6.2575,
      "step": 3372
    },
    {
      "epoch": 0.7310569105691057,
      "step": 3372,
      "training_loss": 6.279118537902832
    },
    {
      "epoch": 0.7310569105691057,
      "step": 3372,
      "training_loss": 6.633893013000488
    },
    {
      "epoch": 0.7310569105691057,
      "step": 3372,
      "training_loss": 6.755767822265625
    },
    {
      "epoch": 0.7310569105691057,
      "step": 3372,
      "training_loss": 6.757364749908447
    },
    {
      "epoch": 0.7312737127371274,
      "step": 3373,
      "training_loss": 4.832907676696777
    },
    {
      "epoch": 0.7312737127371274,
      "step": 3373,
      "training_loss": 6.7748541831970215
    },
    {
      "epoch": 0.7312737127371274,
      "step": 3373,
      "training_loss": 7.187924861907959
    },
    {
      "epoch": 0.7312737127371274,
      "step": 3373,
      "training_loss": 6.955724716186523
    },
    {
      "epoch": 0.731490514905149,
      "step": 3374,
      "training_loss": 7.122628211975098
    },
    {
      "epoch": 0.731490514905149,
      "step": 3374,
      "training_loss": 6.322683334350586
    },
    {
      "epoch": 0.731490514905149,
      "step": 3374,
      "training_loss": 4.968315124511719
    },
    {
      "epoch": 0.731490514905149,
      "step": 3374,
      "training_loss": 5.746801853179932
    },
    {
      "epoch": 0.7317073170731707,
      "step": 3375,
      "training_loss": 6.238499164581299
    },
    {
      "epoch": 0.7317073170731707,
      "step": 3375,
      "training_loss": 6.331068992614746
    },
    {
      "epoch": 0.7317073170731707,
      "step": 3375,
      "training_loss": 5.207098007202148
    },
    {
      "epoch": 0.7317073170731707,
      "step": 3375,
      "training_loss": 7.281862735748291
    },
    {
      "epoch": 0.7319241192411924,
      "grad_norm": 26.598791122436523,
      "learning_rate": 1e-05,
      "loss": 6.3373,
      "step": 3376
    },
    {
      "epoch": 0.7319241192411924,
      "step": 3376,
      "training_loss": 7.133115768432617
    },
    {
      "epoch": 0.7319241192411924,
      "step": 3376,
      "training_loss": 5.526584625244141
    },
    {
      "epoch": 0.7319241192411924,
      "step": 3376,
      "training_loss": 6.279330253601074
    },
    {
      "epoch": 0.7319241192411924,
      "step": 3376,
      "training_loss": 5.130403995513916
    },
    {
      "epoch": 0.732140921409214,
      "step": 3377,
      "training_loss": 6.6710052490234375
    },
    {
      "epoch": 0.732140921409214,
      "step": 3377,
      "training_loss": 6.472532272338867
    },
    {
      "epoch": 0.732140921409214,
      "step": 3377,
      "training_loss": 6.145473957061768
    },
    {
      "epoch": 0.732140921409214,
      "step": 3377,
      "training_loss": 4.112359523773193
    },
    {
      "epoch": 0.7323577235772357,
      "step": 3378,
      "training_loss": 5.714146137237549
    },
    {
      "epoch": 0.7323577235772357,
      "step": 3378,
      "training_loss": 5.499788284301758
    },
    {
      "epoch": 0.7323577235772357,
      "step": 3378,
      "training_loss": 7.018124103546143
    },
    {
      "epoch": 0.7323577235772357,
      "step": 3378,
      "training_loss": 6.570807456970215
    },
    {
      "epoch": 0.7325745257452575,
      "step": 3379,
      "training_loss": 7.203442096710205
    },
    {
      "epoch": 0.7325745257452575,
      "step": 3379,
      "training_loss": 5.601397514343262
    },
    {
      "epoch": 0.7325745257452575,
      "step": 3379,
      "training_loss": 4.1617021560668945
    },
    {
      "epoch": 0.7325745257452575,
      "step": 3379,
      "training_loss": 6.278779029846191
    },
    {
      "epoch": 0.7327913279132792,
      "grad_norm": 15.280577659606934,
      "learning_rate": 1e-05,
      "loss": 5.9699,
      "step": 3380
    },
    {
      "epoch": 0.7327913279132792,
      "step": 3380,
      "training_loss": 6.409566402435303
    },
    {
      "epoch": 0.7327913279132792,
      "step": 3380,
      "training_loss": 7.134435176849365
    },
    {
      "epoch": 0.7327913279132792,
      "step": 3380,
      "training_loss": 7.920884609222412
    },
    {
      "epoch": 0.7327913279132792,
      "step": 3380,
      "training_loss": 6.1536149978637695
    },
    {
      "epoch": 0.7330081300813008,
      "step": 3381,
      "training_loss": 5.244553565979004
    },
    {
      "epoch": 0.7330081300813008,
      "step": 3381,
      "training_loss": 7.671658039093018
    },
    {
      "epoch": 0.7330081300813008,
      "step": 3381,
      "training_loss": 6.655013084411621
    },
    {
      "epoch": 0.7330081300813008,
      "step": 3381,
      "training_loss": 6.947404861450195
    },
    {
      "epoch": 0.7332249322493225,
      "step": 3382,
      "training_loss": 11.295095443725586
    },
    {
      "epoch": 0.7332249322493225,
      "step": 3382,
      "training_loss": 5.9667181968688965
    },
    {
      "epoch": 0.7332249322493225,
      "step": 3382,
      "training_loss": 6.7626519203186035
    },
    {
      "epoch": 0.7332249322493225,
      "step": 3382,
      "training_loss": 7.3871169090271
    },
    {
      "epoch": 0.7334417344173442,
      "step": 3383,
      "training_loss": 6.782194137573242
    },
    {
      "epoch": 0.7334417344173442,
      "step": 3383,
      "training_loss": 6.230319023132324
    },
    {
      "epoch": 0.7334417344173442,
      "step": 3383,
      "training_loss": 9.497538566589355
    },
    {
      "epoch": 0.7334417344173442,
      "step": 3383,
      "training_loss": 5.631141662597656
    },
    {
      "epoch": 0.7336585365853658,
      "grad_norm": 19.18115234375,
      "learning_rate": 1e-05,
      "loss": 7.1056,
      "step": 3384
    },
    {
      "epoch": 0.7336585365853658,
      "step": 3384,
      "training_loss": 7.164903163909912
    },
    {
      "epoch": 0.7336585365853658,
      "step": 3384,
      "training_loss": 6.411752700805664
    },
    {
      "epoch": 0.7336585365853658,
      "step": 3384,
      "training_loss": 7.077939033508301
    },
    {
      "epoch": 0.7336585365853658,
      "step": 3384,
      "training_loss": 7.5654120445251465
    },
    {
      "epoch": 0.7338753387533875,
      "step": 3385,
      "training_loss": 6.344167232513428
    },
    {
      "epoch": 0.7338753387533875,
      "step": 3385,
      "training_loss": 3.3269271850585938
    },
    {
      "epoch": 0.7338753387533875,
      "step": 3385,
      "training_loss": 3.053992509841919
    },
    {
      "epoch": 0.7338753387533875,
      "step": 3385,
      "training_loss": 5.625129699707031
    },
    {
      "epoch": 0.7340921409214092,
      "step": 3386,
      "training_loss": 5.605490207672119
    },
    {
      "epoch": 0.7340921409214092,
      "step": 3386,
      "training_loss": 6.2375688552856445
    },
    {
      "epoch": 0.7340921409214092,
      "step": 3386,
      "training_loss": 7.292996883392334
    },
    {
      "epoch": 0.7340921409214092,
      "step": 3386,
      "training_loss": 5.51157808303833
    },
    {
      "epoch": 0.7343089430894308,
      "step": 3387,
      "training_loss": 5.044525146484375
    },
    {
      "epoch": 0.7343089430894308,
      "step": 3387,
      "training_loss": 6.616611003875732
    },
    {
      "epoch": 0.7343089430894308,
      "step": 3387,
      "training_loss": 7.960987567901611
    },
    {
      "epoch": 0.7343089430894308,
      "step": 3387,
      "training_loss": 3.89516019821167
    },
    {
      "epoch": 0.7345257452574526,
      "grad_norm": 17.65252113342285,
      "learning_rate": 1e-05,
      "loss": 5.9209,
      "step": 3388
    },
    {
      "epoch": 0.7345257452574526,
      "step": 3388,
      "training_loss": 5.57145357131958
    },
    {
      "epoch": 0.7345257452574526,
      "step": 3388,
      "training_loss": 6.285154819488525
    },
    {
      "epoch": 0.7345257452574526,
      "step": 3388,
      "training_loss": 5.3257222175598145
    },
    {
      "epoch": 0.7345257452574526,
      "step": 3388,
      "training_loss": 5.307706356048584
    },
    {
      "epoch": 0.7347425474254743,
      "step": 3389,
      "training_loss": 5.690892696380615
    },
    {
      "epoch": 0.7347425474254743,
      "step": 3389,
      "training_loss": 3.0668420791625977
    },
    {
      "epoch": 0.7347425474254743,
      "step": 3389,
      "training_loss": 6.623462677001953
    },
    {
      "epoch": 0.7347425474254743,
      "step": 3389,
      "training_loss": 7.5869951248168945
    },
    {
      "epoch": 0.734959349593496,
      "step": 3390,
      "training_loss": 8.093194961547852
    },
    {
      "epoch": 0.734959349593496,
      "step": 3390,
      "training_loss": 6.036326885223389
    },
    {
      "epoch": 0.734959349593496,
      "step": 3390,
      "training_loss": 4.741668701171875
    },
    {
      "epoch": 0.734959349593496,
      "step": 3390,
      "training_loss": 7.379758358001709
    },
    {
      "epoch": 0.7351761517615176,
      "step": 3391,
      "training_loss": 6.840207576751709
    },
    {
      "epoch": 0.7351761517615176,
      "step": 3391,
      "training_loss": 6.106155872344971
    },
    {
      "epoch": 0.7351761517615176,
      "step": 3391,
      "training_loss": 7.526029109954834
    },
    {
      "epoch": 0.7351761517615176,
      "step": 3391,
      "training_loss": 6.452359199523926
    },
    {
      "epoch": 0.7353929539295393,
      "grad_norm": 16.62291145324707,
      "learning_rate": 1e-05,
      "loss": 6.1646,
      "step": 3392
    },
    {
      "epoch": 0.7353929539295393,
      "step": 3392,
      "training_loss": 6.981446743011475
    },
    {
      "epoch": 0.7353929539295393,
      "step": 3392,
      "training_loss": 6.133368015289307
    },
    {
      "epoch": 0.7353929539295393,
      "step": 3392,
      "training_loss": 7.784256935119629
    },
    {
      "epoch": 0.7353929539295393,
      "step": 3392,
      "training_loss": 3.0231733322143555
    },
    {
      "epoch": 0.735609756097561,
      "step": 3393,
      "training_loss": 6.361353397369385
    },
    {
      "epoch": 0.735609756097561,
      "step": 3393,
      "training_loss": 6.777846813201904
    },
    {
      "epoch": 0.735609756097561,
      "step": 3393,
      "training_loss": 5.819839000701904
    },
    {
      "epoch": 0.735609756097561,
      "step": 3393,
      "training_loss": 7.495112895965576
    },
    {
      "epoch": 0.7358265582655826,
      "step": 3394,
      "training_loss": 4.3126912117004395
    },
    {
      "epoch": 0.7358265582655826,
      "step": 3394,
      "training_loss": 6.927029609680176
    },
    {
      "epoch": 0.7358265582655826,
      "step": 3394,
      "training_loss": 5.667750358581543
    },
    {
      "epoch": 0.7358265582655826,
      "step": 3394,
      "training_loss": 6.203469753265381
    },
    {
      "epoch": 0.7360433604336043,
      "step": 3395,
      "training_loss": 6.48174524307251
    },
    {
      "epoch": 0.7360433604336043,
      "step": 3395,
      "training_loss": 7.300665855407715
    },
    {
      "epoch": 0.7360433604336043,
      "step": 3395,
      "training_loss": 6.706243515014648
    },
    {
      "epoch": 0.7360433604336043,
      "step": 3395,
      "training_loss": 5.059637546539307
    },
    {
      "epoch": 0.736260162601626,
      "grad_norm": 15.302181243896484,
      "learning_rate": 1e-05,
      "loss": 6.1897,
      "step": 3396
    },
    {
      "epoch": 0.736260162601626,
      "step": 3396,
      "training_loss": 6.266432285308838
    },
    {
      "epoch": 0.736260162601626,
      "step": 3396,
      "training_loss": 6.112857818603516
    },
    {
      "epoch": 0.736260162601626,
      "step": 3396,
      "training_loss": 6.895462989807129
    },
    {
      "epoch": 0.736260162601626,
      "step": 3396,
      "training_loss": 6.563060760498047
    },
    {
      "epoch": 0.7364769647696477,
      "step": 3397,
      "training_loss": 4.7024335861206055
    },
    {
      "epoch": 0.7364769647696477,
      "step": 3397,
      "training_loss": 3.5165255069732666
    },
    {
      "epoch": 0.7364769647696477,
      "step": 3397,
      "training_loss": 5.632241725921631
    },
    {
      "epoch": 0.7364769647696477,
      "step": 3397,
      "training_loss": 6.890846252441406
    },
    {
      "epoch": 0.7366937669376694,
      "step": 3398,
      "training_loss": 5.569997310638428
    },
    {
      "epoch": 0.7366937669376694,
      "step": 3398,
      "training_loss": 6.174831867218018
    },
    {
      "epoch": 0.7366937669376694,
      "step": 3398,
      "training_loss": 6.786447048187256
    },
    {
      "epoch": 0.7366937669376694,
      "step": 3398,
      "training_loss": 6.5711588859558105
    },
    {
      "epoch": 0.7369105691056911,
      "step": 3399,
      "training_loss": 6.023016929626465
    },
    {
      "epoch": 0.7369105691056911,
      "step": 3399,
      "training_loss": 7.973376274108887
    },
    {
      "epoch": 0.7369105691056911,
      "step": 3399,
      "training_loss": 7.115681171417236
    },
    {
      "epoch": 0.7369105691056911,
      "step": 3399,
      "training_loss": 6.430724143981934
    },
    {
      "epoch": 0.7371273712737128,
      "grad_norm": 25.534589767456055,
      "learning_rate": 1e-05,
      "loss": 6.2016,
      "step": 3400
    },
    {
      "epoch": 0.7371273712737128,
      "step": 3400,
      "training_loss": 6.609970569610596
    },
    {
      "epoch": 0.7371273712737128,
      "step": 3400,
      "training_loss": 6.78589391708374
    },
    {
      "epoch": 0.7371273712737128,
      "step": 3400,
      "training_loss": 7.9645304679870605
    },
    {
      "epoch": 0.7371273712737128,
      "step": 3400,
      "training_loss": 5.983763694763184
    },
    {
      "epoch": 0.7373441734417344,
      "step": 3401,
      "training_loss": 6.797653675079346
    },
    {
      "epoch": 0.7373441734417344,
      "step": 3401,
      "training_loss": 7.044262886047363
    },
    {
      "epoch": 0.7373441734417344,
      "step": 3401,
      "training_loss": 7.42833137512207
    },
    {
      "epoch": 0.7373441734417344,
      "step": 3401,
      "training_loss": 6.772995471954346
    },
    {
      "epoch": 0.7375609756097561,
      "step": 3402,
      "training_loss": 5.262510776519775
    },
    {
      "epoch": 0.7375609756097561,
      "step": 3402,
      "training_loss": 7.451138019561768
    },
    {
      "epoch": 0.7375609756097561,
      "step": 3402,
      "training_loss": 7.80447244644165
    },
    {
      "epoch": 0.7375609756097561,
      "step": 3402,
      "training_loss": 7.37412691116333
    },
    {
      "epoch": 0.7377777777777778,
      "step": 3403,
      "training_loss": 5.868453502655029
    },
    {
      "epoch": 0.7377777777777778,
      "step": 3403,
      "training_loss": 6.573010444641113
    },
    {
      "epoch": 0.7377777777777778,
      "step": 3403,
      "training_loss": 6.573625564575195
    },
    {
      "epoch": 0.7377777777777778,
      "step": 3403,
      "training_loss": 6.2319111824035645
    },
    {
      "epoch": 0.7379945799457994,
      "grad_norm": 20.83588409423828,
      "learning_rate": 1e-05,
      "loss": 6.7829,
      "step": 3404
    },
    {
      "epoch": 0.7379945799457994,
      "step": 3404,
      "training_loss": 8.972956657409668
    },
    {
      "epoch": 0.7379945799457994,
      "step": 3404,
      "training_loss": 6.032907009124756
    },
    {
      "epoch": 0.7379945799457994,
      "step": 3404,
      "training_loss": 6.452122211456299
    },
    {
      "epoch": 0.7379945799457994,
      "step": 3404,
      "training_loss": 6.585458278656006
    },
    {
      "epoch": 0.7382113821138211,
      "step": 3405,
      "training_loss": 7.166996955871582
    },
    {
      "epoch": 0.7382113821138211,
      "step": 3405,
      "training_loss": 6.079438209533691
    },
    {
      "epoch": 0.7382113821138211,
      "step": 3405,
      "training_loss": 6.910220146179199
    },
    {
      "epoch": 0.7382113821138211,
      "step": 3405,
      "training_loss": 6.405973434448242
    },
    {
      "epoch": 0.7384281842818429,
      "step": 3406,
      "training_loss": 6.0331196784973145
    },
    {
      "epoch": 0.7384281842818429,
      "step": 3406,
      "training_loss": 4.721776485443115
    },
    {
      "epoch": 0.7384281842818429,
      "step": 3406,
      "training_loss": 6.387627601623535
    },
    {
      "epoch": 0.7384281842818429,
      "step": 3406,
      "training_loss": 6.040309906005859
    },
    {
      "epoch": 0.7386449864498645,
      "step": 3407,
      "training_loss": 4.617583274841309
    },
    {
      "epoch": 0.7386449864498645,
      "step": 3407,
      "training_loss": 6.717624187469482
    },
    {
      "epoch": 0.7386449864498645,
      "step": 3407,
      "training_loss": 5.333297252655029
    },
    {
      "epoch": 0.7386449864498645,
      "step": 3407,
      "training_loss": 3.306419610977173
    },
    {
      "epoch": 0.7388617886178862,
      "grad_norm": 37.55887222290039,
      "learning_rate": 1e-05,
      "loss": 6.1102,
      "step": 3408
    },
    {
      "epoch": 0.7388617886178862,
      "step": 3408,
      "training_loss": 6.52451753616333
    },
    {
      "epoch": 0.7388617886178862,
      "step": 3408,
      "training_loss": 7.382893085479736
    },
    {
      "epoch": 0.7388617886178862,
      "step": 3408,
      "training_loss": 6.001732349395752
    },
    {
      "epoch": 0.7388617886178862,
      "step": 3408,
      "training_loss": 5.61549711227417
    },
    {
      "epoch": 0.7390785907859079,
      "step": 3409,
      "training_loss": 6.184171676635742
    },
    {
      "epoch": 0.7390785907859079,
      "step": 3409,
      "training_loss": 7.322840690612793
    },
    {
      "epoch": 0.7390785907859079,
      "step": 3409,
      "training_loss": 7.008411407470703
    },
    {
      "epoch": 0.7390785907859079,
      "step": 3409,
      "training_loss": 5.295673847198486
    },
    {
      "epoch": 0.7392953929539295,
      "step": 3410,
      "training_loss": 3.131405830383301
    },
    {
      "epoch": 0.7392953929539295,
      "step": 3410,
      "training_loss": 6.727673053741455
    },
    {
      "epoch": 0.7392953929539295,
      "step": 3410,
      "training_loss": 7.147764682769775
    },
    {
      "epoch": 0.7392953929539295,
      "step": 3410,
      "training_loss": 6.344440937042236
    },
    {
      "epoch": 0.7395121951219512,
      "step": 3411,
      "training_loss": 7.2863922119140625
    },
    {
      "epoch": 0.7395121951219512,
      "step": 3411,
      "training_loss": 6.368521690368652
    },
    {
      "epoch": 0.7395121951219512,
      "step": 3411,
      "training_loss": 6.8423871994018555
    },
    {
      "epoch": 0.7395121951219512,
      "step": 3411,
      "training_loss": 7.681222915649414
    },
    {
      "epoch": 0.7397289972899729,
      "grad_norm": 13.689728736877441,
      "learning_rate": 1e-05,
      "loss": 6.4291,
      "step": 3412
    },
    {
      "epoch": 0.7397289972899729,
      "step": 3412,
      "training_loss": 6.573977470397949
    },
    {
      "epoch": 0.7397289972899729,
      "step": 3412,
      "training_loss": 6.256114959716797
    },
    {
      "epoch": 0.7397289972899729,
      "step": 3412,
      "training_loss": 5.894906044006348
    },
    {
      "epoch": 0.7397289972899729,
      "step": 3412,
      "training_loss": 5.306504726409912
    },
    {
      "epoch": 0.7399457994579945,
      "step": 3413,
      "training_loss": 7.3364129066467285
    },
    {
      "epoch": 0.7399457994579945,
      "step": 3413,
      "training_loss": 5.5513129234313965
    },
    {
      "epoch": 0.7399457994579945,
      "step": 3413,
      "training_loss": 4.477715492248535
    },
    {
      "epoch": 0.7399457994579945,
      "step": 3413,
      "training_loss": 6.707920551300049
    },
    {
      "epoch": 0.7401626016260162,
      "step": 3414,
      "training_loss": 7.224920272827148
    },
    {
      "epoch": 0.7401626016260162,
      "step": 3414,
      "training_loss": 5.762808322906494
    },
    {
      "epoch": 0.7401626016260162,
      "step": 3414,
      "training_loss": 8.233172416687012
    },
    {
      "epoch": 0.7401626016260162,
      "step": 3414,
      "training_loss": 7.18400239944458
    },
    {
      "epoch": 0.740379403794038,
      "step": 3415,
      "training_loss": 6.434619903564453
    },
    {
      "epoch": 0.740379403794038,
      "step": 3415,
      "training_loss": 6.79763650894165
    },
    {
      "epoch": 0.740379403794038,
      "step": 3415,
      "training_loss": 3.6868813037872314
    },
    {
      "epoch": 0.740379403794038,
      "step": 3415,
      "training_loss": 5.097304821014404
    },
    {
      "epoch": 0.7405962059620597,
      "grad_norm": 32.41967010498047,
      "learning_rate": 1e-05,
      "loss": 6.1579,
      "step": 3416
    },
    {
      "epoch": 0.7405962059620597,
      "step": 3416,
      "training_loss": 5.836782932281494
    },
    {
      "epoch": 0.7405962059620597,
      "step": 3416,
      "training_loss": 6.918885231018066
    },
    {
      "epoch": 0.7405962059620597,
      "step": 3416,
      "training_loss": 7.094751834869385
    },
    {
      "epoch": 0.7405962059620597,
      "step": 3416,
      "training_loss": 6.299551486968994
    },
    {
      "epoch": 0.7408130081300813,
      "step": 3417,
      "training_loss": 4.248265266418457
    },
    {
      "epoch": 0.7408130081300813,
      "step": 3417,
      "training_loss": 7.517732620239258
    },
    {
      "epoch": 0.7408130081300813,
      "step": 3417,
      "training_loss": 5.018832206726074
    },
    {
      "epoch": 0.7408130081300813,
      "step": 3417,
      "training_loss": 8.385100364685059
    },
    {
      "epoch": 0.741029810298103,
      "step": 3418,
      "training_loss": 6.755675792694092
    },
    {
      "epoch": 0.741029810298103,
      "step": 3418,
      "training_loss": 6.8408002853393555
    },
    {
      "epoch": 0.741029810298103,
      "step": 3418,
      "training_loss": 6.246015548706055
    },
    {
      "epoch": 0.741029810298103,
      "step": 3418,
      "training_loss": 2.9358556270599365
    },
    {
      "epoch": 0.7412466124661247,
      "step": 3419,
      "training_loss": 5.746184349060059
    },
    {
      "epoch": 0.7412466124661247,
      "step": 3419,
      "training_loss": 7.505370616912842
    },
    {
      "epoch": 0.7412466124661247,
      "step": 3419,
      "training_loss": 3.6677825450897217
    },
    {
      "epoch": 0.7412466124661247,
      "step": 3419,
      "training_loss": 6.581859111785889
    },
    {
      "epoch": 0.7414634146341463,
      "grad_norm": 26.98743438720703,
      "learning_rate": 1e-05,
      "loss": 6.1,
      "step": 3420
    },
    {
      "epoch": 0.7414634146341463,
      "step": 3420,
      "training_loss": 4.857058525085449
    },
    {
      "epoch": 0.7414634146341463,
      "step": 3420,
      "training_loss": 7.0669145584106445
    },
    {
      "epoch": 0.7414634146341463,
      "step": 3420,
      "training_loss": 3.1992175579071045
    },
    {
      "epoch": 0.7414634146341463,
      "step": 3420,
      "training_loss": 7.265835285186768
    },
    {
      "epoch": 0.741680216802168,
      "step": 3421,
      "training_loss": 6.362997531890869
    },
    {
      "epoch": 0.741680216802168,
      "step": 3421,
      "training_loss": 5.963283061981201
    },
    {
      "epoch": 0.741680216802168,
      "step": 3421,
      "training_loss": 5.649066925048828
    },
    {
      "epoch": 0.741680216802168,
      "step": 3421,
      "training_loss": 4.368290901184082
    },
    {
      "epoch": 0.7418970189701897,
      "step": 3422,
      "training_loss": 7.17441463470459
    },
    {
      "epoch": 0.7418970189701897,
      "step": 3422,
      "training_loss": 6.797848701477051
    },
    {
      "epoch": 0.7418970189701897,
      "step": 3422,
      "training_loss": 6.860104560852051
    },
    {
      "epoch": 0.7418970189701897,
      "step": 3422,
      "training_loss": 5.761626720428467
    },
    {
      "epoch": 0.7421138211382113,
      "step": 3423,
      "training_loss": 6.101382255554199
    },
    {
      "epoch": 0.7421138211382113,
      "step": 3423,
      "training_loss": 6.105673789978027
    },
    {
      "epoch": 0.7421138211382113,
      "step": 3423,
      "training_loss": 6.885311126708984
    },
    {
      "epoch": 0.7421138211382113,
      "step": 3423,
      "training_loss": 6.242270469665527
    },
    {
      "epoch": 0.7423306233062331,
      "grad_norm": 15.486692428588867,
      "learning_rate": 1e-05,
      "loss": 6.0413,
      "step": 3424
    },
    {
      "epoch": 0.7423306233062331,
      "step": 3424,
      "training_loss": 9.978148460388184
    },
    {
      "epoch": 0.7423306233062331,
      "step": 3424,
      "training_loss": 7.430857181549072
    },
    {
      "epoch": 0.7423306233062331,
      "step": 3424,
      "training_loss": 7.138404369354248
    },
    {
      "epoch": 0.7423306233062331,
      "step": 3424,
      "training_loss": 5.656004905700684
    },
    {
      "epoch": 0.7425474254742548,
      "step": 3425,
      "training_loss": 4.918727874755859
    },
    {
      "epoch": 0.7425474254742548,
      "step": 3425,
      "training_loss": 5.391225337982178
    },
    {
      "epoch": 0.7425474254742548,
      "step": 3425,
      "training_loss": 5.335246562957764
    },
    {
      "epoch": 0.7425474254742548,
      "step": 3425,
      "training_loss": 6.414251804351807
    },
    {
      "epoch": 0.7427642276422765,
      "step": 3426,
      "training_loss": 7.043727874755859
    },
    {
      "epoch": 0.7427642276422765,
      "step": 3426,
      "training_loss": 5.706928730010986
    },
    {
      "epoch": 0.7427642276422765,
      "step": 3426,
      "training_loss": 5.189602851867676
    },
    {
      "epoch": 0.7427642276422765,
      "step": 3426,
      "training_loss": 5.317661285400391
    },
    {
      "epoch": 0.7429810298102981,
      "step": 3427,
      "training_loss": 6.158461570739746
    },
    {
      "epoch": 0.7429810298102981,
      "step": 3427,
      "training_loss": 7.127566814422607
    },
    {
      "epoch": 0.7429810298102981,
      "step": 3427,
      "training_loss": 5.606227397918701
    },
    {
      "epoch": 0.7429810298102981,
      "step": 3427,
      "training_loss": 4.04177188873291
    },
    {
      "epoch": 0.7431978319783198,
      "grad_norm": 16.823163986206055,
      "learning_rate": 1e-05,
      "loss": 6.1534,
      "step": 3428
    },
    {
      "epoch": 0.7431978319783198,
      "step": 3428,
      "training_loss": 7.537752151489258
    },
    {
      "epoch": 0.7431978319783198,
      "step": 3428,
      "training_loss": 6.663710594177246
    },
    {
      "epoch": 0.7431978319783198,
      "step": 3428,
      "training_loss": 5.974430084228516
    },
    {
      "epoch": 0.7431978319783198,
      "step": 3428,
      "training_loss": 7.254878520965576
    },
    {
      "epoch": 0.7434146341463415,
      "step": 3429,
      "training_loss": 4.920175552368164
    },
    {
      "epoch": 0.7434146341463415,
      "step": 3429,
      "training_loss": 6.280169486999512
    },
    {
      "epoch": 0.7434146341463415,
      "step": 3429,
      "training_loss": 7.425327301025391
    },
    {
      "epoch": 0.7434146341463415,
      "step": 3429,
      "training_loss": 5.2562150955200195
    },
    {
      "epoch": 0.7436314363143631,
      "step": 3430,
      "training_loss": 4.985329627990723
    },
    {
      "epoch": 0.7436314363143631,
      "step": 3430,
      "training_loss": 6.303849697113037
    },
    {
      "epoch": 0.7436314363143631,
      "step": 3430,
      "training_loss": 7.432168960571289
    },
    {
      "epoch": 0.7436314363143631,
      "step": 3430,
      "training_loss": 6.357896327972412
    },
    {
      "epoch": 0.7438482384823848,
      "step": 3431,
      "training_loss": 8.141119003295898
    },
    {
      "epoch": 0.7438482384823848,
      "step": 3431,
      "training_loss": 7.462966442108154
    },
    {
      "epoch": 0.7438482384823848,
      "step": 3431,
      "training_loss": 5.041992664337158
    },
    {
      "epoch": 0.7438482384823848,
      "step": 3431,
      "training_loss": 4.095724105834961
    },
    {
      "epoch": 0.7440650406504065,
      "grad_norm": 14.492971420288086,
      "learning_rate": 1e-05,
      "loss": 6.3209,
      "step": 3432
    },
    {
      "epoch": 0.7440650406504065,
      "step": 3432,
      "training_loss": 7.601520538330078
    },
    {
      "epoch": 0.7440650406504065,
      "step": 3432,
      "training_loss": 5.86099910736084
    },
    {
      "epoch": 0.7440650406504065,
      "step": 3432,
      "training_loss": 6.7578816413879395
    },
    {
      "epoch": 0.7440650406504065,
      "step": 3432,
      "training_loss": 6.778922080993652
    },
    {
      "epoch": 0.7442818428184281,
      "step": 3433,
      "training_loss": 4.669509410858154
    },
    {
      "epoch": 0.7442818428184281,
      "step": 3433,
      "training_loss": 7.93344259262085
    },
    {
      "epoch": 0.7442818428184281,
      "step": 3433,
      "training_loss": 3.6069788932800293
    },
    {
      "epoch": 0.7442818428184281,
      "step": 3433,
      "training_loss": 6.371796607971191
    },
    {
      "epoch": 0.7444986449864499,
      "step": 3434,
      "training_loss": 6.199402809143066
    },
    {
      "epoch": 0.7444986449864499,
      "step": 3434,
      "training_loss": 6.17210054397583
    },
    {
      "epoch": 0.7444986449864499,
      "step": 3434,
      "training_loss": 7.15695333480835
    },
    {
      "epoch": 0.7444986449864499,
      "step": 3434,
      "training_loss": 7.4417853355407715
    },
    {
      "epoch": 0.7447154471544716,
      "step": 3435,
      "training_loss": 6.0041823387146
    },
    {
      "epoch": 0.7447154471544716,
      "step": 3435,
      "training_loss": 6.819980621337891
    },
    {
      "epoch": 0.7447154471544716,
      "step": 3435,
      "training_loss": 6.536599159240723
    },
    {
      "epoch": 0.7447154471544716,
      "step": 3435,
      "training_loss": 7.002749443054199
    },
    {
      "epoch": 0.7449322493224932,
      "grad_norm": 22.508806228637695,
      "learning_rate": 1e-05,
      "loss": 6.4322,
      "step": 3436
    },
    {
      "epoch": 0.7449322493224932,
      "step": 3436,
      "training_loss": 3.8288216590881348
    },
    {
      "epoch": 0.7449322493224932,
      "step": 3436,
      "training_loss": 5.5724568367004395
    },
    {
      "epoch": 0.7449322493224932,
      "step": 3436,
      "training_loss": 3.849447011947632
    },
    {
      "epoch": 0.7449322493224932,
      "step": 3436,
      "training_loss": 6.105536460876465
    },
    {
      "epoch": 0.7451490514905149,
      "step": 3437,
      "training_loss": 5.109694004058838
    },
    {
      "epoch": 0.7451490514905149,
      "step": 3437,
      "training_loss": 4.630603313446045
    },
    {
      "epoch": 0.7451490514905149,
      "step": 3437,
      "training_loss": 5.760220527648926
    },
    {
      "epoch": 0.7451490514905149,
      "step": 3437,
      "training_loss": 7.067245960235596
    },
    {
      "epoch": 0.7453658536585366,
      "step": 3438,
      "training_loss": 8.220438003540039
    },
    {
      "epoch": 0.7453658536585366,
      "step": 3438,
      "training_loss": 7.1512837409973145
    },
    {
      "epoch": 0.7453658536585366,
      "step": 3438,
      "training_loss": 5.312861919403076
    },
    {
      "epoch": 0.7453658536585366,
      "step": 3438,
      "training_loss": 6.671604633331299
    },
    {
      "epoch": 0.7455826558265582,
      "step": 3439,
      "training_loss": 6.984508514404297
    },
    {
      "epoch": 0.7455826558265582,
      "step": 3439,
      "training_loss": 6.486649513244629
    },
    {
      "epoch": 0.7455826558265582,
      "step": 3439,
      "training_loss": 6.908986568450928
    },
    {
      "epoch": 0.7455826558265582,
      "step": 3439,
      "training_loss": 6.368565559387207
    },
    {
      "epoch": 0.7457994579945799,
      "grad_norm": 22.75164794921875,
      "learning_rate": 1e-05,
      "loss": 6.0018,
      "step": 3440
    },
    {
      "epoch": 0.7457994579945799,
      "step": 3440,
      "training_loss": 6.353982925415039
    },
    {
      "epoch": 0.7457994579945799,
      "step": 3440,
      "training_loss": 6.501359939575195
    },
    {
      "epoch": 0.7457994579945799,
      "step": 3440,
      "training_loss": 6.116703510284424
    },
    {
      "epoch": 0.7457994579945799,
      "step": 3440,
      "training_loss": 5.601345062255859
    },
    {
      "epoch": 0.7460162601626016,
      "step": 3441,
      "training_loss": 6.605181694030762
    },
    {
      "epoch": 0.7460162601626016,
      "step": 3441,
      "training_loss": 5.479161262512207
    },
    {
      "epoch": 0.7460162601626016,
      "step": 3441,
      "training_loss": 6.586226940155029
    },
    {
      "epoch": 0.7460162601626016,
      "step": 3441,
      "training_loss": 6.230384349822998
    },
    {
      "epoch": 0.7462330623306233,
      "step": 3442,
      "training_loss": 6.59000301361084
    },
    {
      "epoch": 0.7462330623306233,
      "step": 3442,
      "training_loss": 7.4697089195251465
    },
    {
      "epoch": 0.7462330623306233,
      "step": 3442,
      "training_loss": 4.123306751251221
    },
    {
      "epoch": 0.7462330623306233,
      "step": 3442,
      "training_loss": 6.766531944274902
    },
    {
      "epoch": 0.746449864498645,
      "step": 3443,
      "training_loss": 6.97648811340332
    },
    {
      "epoch": 0.746449864498645,
      "step": 3443,
      "training_loss": 6.935976982116699
    },
    {
      "epoch": 0.746449864498645,
      "step": 3443,
      "training_loss": 5.002074718475342
    },
    {
      "epoch": 0.746449864498645,
      "step": 3443,
      "training_loss": 6.435616970062256
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 23.28921127319336,
      "learning_rate": 1e-05,
      "loss": 6.2359,
      "step": 3444
    },
    {
      "epoch": 0.7466666666666667,
      "step": 3444,
      "training_loss": 5.732544422149658
    },
    {
      "epoch": 0.7466666666666667,
      "step": 3444,
      "training_loss": 6.223855495452881
    },
    {
      "epoch": 0.7466666666666667,
      "step": 3444,
      "training_loss": 5.248371601104736
    },
    {
      "epoch": 0.7466666666666667,
      "step": 3444,
      "training_loss": 6.528120040893555
    },
    {
      "epoch": 0.7468834688346884,
      "step": 3445,
      "training_loss": 6.729011535644531
    },
    {
      "epoch": 0.7468834688346884,
      "step": 3445,
      "training_loss": 6.275257587432861
    },
    {
      "epoch": 0.7468834688346884,
      "step": 3445,
      "training_loss": 6.823239803314209
    },
    {
      "epoch": 0.7468834688346884,
      "step": 3445,
      "training_loss": 7.411924362182617
    },
    {
      "epoch": 0.74710027100271,
      "step": 3446,
      "training_loss": 4.787038326263428
    },
    {
      "epoch": 0.74710027100271,
      "step": 3446,
      "training_loss": 6.871099948883057
    },
    {
      "epoch": 0.74710027100271,
      "step": 3446,
      "training_loss": 7.16448974609375
    },
    {
      "epoch": 0.74710027100271,
      "step": 3446,
      "training_loss": 4.854610443115234
    },
    {
      "epoch": 0.7473170731707317,
      "step": 3447,
      "training_loss": 5.519947052001953
    },
    {
      "epoch": 0.7473170731707317,
      "step": 3447,
      "training_loss": 6.881529808044434
    },
    {
      "epoch": 0.7473170731707317,
      "step": 3447,
      "training_loss": 4.989269256591797
    },
    {
      "epoch": 0.7473170731707317,
      "step": 3447,
      "training_loss": 5.633215427398682
    },
    {
      "epoch": 0.7475338753387534,
      "grad_norm": 18.035402297973633,
      "learning_rate": 1e-05,
      "loss": 6.1046,
      "step": 3448
    },
    {
      "epoch": 0.7475338753387534,
      "step": 3448,
      "training_loss": 4.249330043792725
    },
    {
      "epoch": 0.7475338753387534,
      "step": 3448,
      "training_loss": 7.136765956878662
    },
    {
      "epoch": 0.7475338753387534,
      "step": 3448,
      "training_loss": 2.6716058254241943
    },
    {
      "epoch": 0.7475338753387534,
      "step": 3448,
      "training_loss": 7.730198860168457
    },
    {
      "epoch": 0.747750677506775,
      "step": 3449,
      "training_loss": 6.210872173309326
    },
    {
      "epoch": 0.747750677506775,
      "step": 3449,
      "training_loss": 7.167855262756348
    },
    {
      "epoch": 0.747750677506775,
      "step": 3449,
      "training_loss": 7.629325866699219
    },
    {
      "epoch": 0.747750677506775,
      "step": 3449,
      "training_loss": 7.01592493057251
    },
    {
      "epoch": 0.7479674796747967,
      "step": 3450,
      "training_loss": 2.7885215282440186
    },
    {
      "epoch": 0.7479674796747967,
      "step": 3450,
      "training_loss": 6.789458751678467
    },
    {
      "epoch": 0.7479674796747967,
      "step": 3450,
      "training_loss": 6.503366470336914
    },
    {
      "epoch": 0.7479674796747967,
      "step": 3450,
      "training_loss": 7.463356971740723
    },
    {
      "epoch": 0.7481842818428184,
      "step": 3451,
      "training_loss": 7.276621341705322
    },
    {
      "epoch": 0.7481842818428184,
      "step": 3451,
      "training_loss": 6.480397701263428
    },
    {
      "epoch": 0.7481842818428184,
      "step": 3451,
      "training_loss": 6.194604396820068
    },
    {
      "epoch": 0.7481842818428184,
      "step": 3451,
      "training_loss": 5.8857741355896
    },
    {
      "epoch": 0.7484010840108402,
      "grad_norm": 17.315288543701172,
      "learning_rate": 1e-05,
      "loss": 6.1996,
      "step": 3452
    },
    {
      "epoch": 0.7484010840108402,
      "step": 3452,
      "training_loss": 7.138829708099365
    },
    {
      "epoch": 0.7484010840108402,
      "step": 3452,
      "training_loss": 5.533924102783203
    },
    {
      "epoch": 0.7484010840108402,
      "step": 3452,
      "training_loss": 7.24552059173584
    },
    {
      "epoch": 0.7484010840108402,
      "step": 3452,
      "training_loss": 7.042402744293213
    },
    {
      "epoch": 0.7486178861788618,
      "step": 3453,
      "training_loss": 6.808778762817383
    },
    {
      "epoch": 0.7486178861788618,
      "step": 3453,
      "training_loss": 5.1064772605896
    },
    {
      "epoch": 0.7486178861788618,
      "step": 3453,
      "training_loss": 6.394777774810791
    },
    {
      "epoch": 0.7486178861788618,
      "step": 3453,
      "training_loss": 6.929793357849121
    },
    {
      "epoch": 0.7488346883468835,
      "step": 3454,
      "training_loss": 6.9100446701049805
    },
    {
      "epoch": 0.7488346883468835,
      "step": 3454,
      "training_loss": 6.985825061798096
    },
    {
      "epoch": 0.7488346883468835,
      "step": 3454,
      "training_loss": 6.865113735198975
    },
    {
      "epoch": 0.7488346883468835,
      "step": 3454,
      "training_loss": 7.332731246948242
    },
    {
      "epoch": 0.7490514905149052,
      "step": 3455,
      "training_loss": 6.2036519050598145
    },
    {
      "epoch": 0.7490514905149052,
      "step": 3455,
      "training_loss": 5.182655334472656
    },
    {
      "epoch": 0.7490514905149052,
      "step": 3455,
      "training_loss": 7.458362579345703
    },
    {
      "epoch": 0.7490514905149052,
      "step": 3455,
      "training_loss": 5.6733012199401855
    },
    {
      "epoch": 0.7492682926829268,
      "grad_norm": 18.290800094604492,
      "learning_rate": 1e-05,
      "loss": 6.5508,
      "step": 3456
    },
    {
      "epoch": 0.7492682926829268,
      "step": 3456,
      "training_loss": 6.891199588775635
    },
    {
      "epoch": 0.7492682926829268,
      "step": 3456,
      "training_loss": 2.978764772415161
    },
    {
      "epoch": 0.7492682926829268,
      "step": 3456,
      "training_loss": 5.779663562774658
    },
    {
      "epoch": 0.7492682926829268,
      "step": 3456,
      "training_loss": 7.8177266120910645
    },
    {
      "epoch": 0.7494850948509485,
      "step": 3457,
      "training_loss": 6.190379619598389
    },
    {
      "epoch": 0.7494850948509485,
      "step": 3457,
      "training_loss": 6.988373279571533
    },
    {
      "epoch": 0.7494850948509485,
      "step": 3457,
      "training_loss": 7.978738307952881
    },
    {
      "epoch": 0.7494850948509485,
      "step": 3457,
      "training_loss": 5.192923545837402
    },
    {
      "epoch": 0.7497018970189702,
      "step": 3458,
      "training_loss": 7.092935085296631
    },
    {
      "epoch": 0.7497018970189702,
      "step": 3458,
      "training_loss": 5.491015434265137
    },
    {
      "epoch": 0.7497018970189702,
      "step": 3458,
      "training_loss": 6.390981197357178
    },
    {
      "epoch": 0.7497018970189702,
      "step": 3458,
      "training_loss": 6.183981418609619
    },
    {
      "epoch": 0.7499186991869918,
      "step": 3459,
      "training_loss": 6.630313396453857
    },
    {
      "epoch": 0.7499186991869918,
      "step": 3459,
      "training_loss": 6.071142673492432
    },
    {
      "epoch": 0.7499186991869918,
      "step": 3459,
      "training_loss": 5.185680866241455
    },
    {
      "epoch": 0.7499186991869918,
      "step": 3459,
      "training_loss": 7.707509994506836
    },
    {
      "epoch": 0.7501355013550135,
      "grad_norm": 22.304723739624023,
      "learning_rate": 1e-05,
      "loss": 6.2857,
      "step": 3460
    },
    {
      "epoch": 0.7501355013550135,
      "step": 3460,
      "training_loss": 6.504304885864258
    },
    {
      "epoch": 0.7501355013550135,
      "step": 3460,
      "training_loss": 5.396966934204102
    },
    {
      "epoch": 0.7501355013550135,
      "step": 3460,
      "training_loss": 6.744819164276123
    },
    {
      "epoch": 0.7501355013550135,
      "step": 3460,
      "training_loss": 7.282560348510742
    },
    {
      "epoch": 0.7503523035230353,
      "step": 3461,
      "training_loss": 7.1229352951049805
    },
    {
      "epoch": 0.7503523035230353,
      "step": 3461,
      "training_loss": 7.646328926086426
    },
    {
      "epoch": 0.7503523035230353,
      "step": 3461,
      "training_loss": 5.351345539093018
    },
    {
      "epoch": 0.7503523035230353,
      "step": 3461,
      "training_loss": 6.7019124031066895
    },
    {
      "epoch": 0.750569105691057,
      "step": 3462,
      "training_loss": 6.263923645019531
    },
    {
      "epoch": 0.750569105691057,
      "step": 3462,
      "training_loss": 6.61123514175415
    },
    {
      "epoch": 0.750569105691057,
      "step": 3462,
      "training_loss": 5.6644816398620605
    },
    {
      "epoch": 0.750569105691057,
      "step": 3462,
      "training_loss": 5.656373977661133
    },
    {
      "epoch": 0.7507859078590786,
      "step": 3463,
      "training_loss": 6.15382194519043
    },
    {
      "epoch": 0.7507859078590786,
      "step": 3463,
      "training_loss": 6.6580328941345215
    },
    {
      "epoch": 0.7507859078590786,
      "step": 3463,
      "training_loss": 5.241007328033447
    },
    {
      "epoch": 0.7507859078590786,
      "step": 3463,
      "training_loss": 6.468690872192383
    },
    {
      "epoch": 0.7510027100271003,
      "grad_norm": 19.935039520263672,
      "learning_rate": 1e-05,
      "loss": 6.3418,
      "step": 3464
    },
    {
      "epoch": 0.7510027100271003,
      "step": 3464,
      "training_loss": 6.182565689086914
    },
    {
      "epoch": 0.7510027100271003,
      "step": 3464,
      "training_loss": 5.284096717834473
    },
    {
      "epoch": 0.7510027100271003,
      "step": 3464,
      "training_loss": 6.159164905548096
    },
    {
      "epoch": 0.7510027100271003,
      "step": 3464,
      "training_loss": 5.967231750488281
    },
    {
      "epoch": 0.751219512195122,
      "step": 3465,
      "training_loss": 6.314849853515625
    },
    {
      "epoch": 0.751219512195122,
      "step": 3465,
      "training_loss": 6.683141708374023
    },
    {
      "epoch": 0.751219512195122,
      "step": 3465,
      "training_loss": 7.060588836669922
    },
    {
      "epoch": 0.751219512195122,
      "step": 3465,
      "training_loss": 6.344827651977539
    },
    {
      "epoch": 0.7514363143631436,
      "step": 3466,
      "training_loss": 7.1799774169921875
    },
    {
      "epoch": 0.7514363143631436,
      "step": 3466,
      "training_loss": 5.357407093048096
    },
    {
      "epoch": 0.7514363143631436,
      "step": 3466,
      "training_loss": 5.119956970214844
    },
    {
      "epoch": 0.7514363143631436,
      "step": 3466,
      "training_loss": 7.8009490966796875
    },
    {
      "epoch": 0.7516531165311653,
      "step": 3467,
      "training_loss": 6.46072244644165
    },
    {
      "epoch": 0.7516531165311653,
      "step": 3467,
      "training_loss": 6.391382694244385
    },
    {
      "epoch": 0.7516531165311653,
      "step": 3467,
      "training_loss": 7.354017734527588
    },
    {
      "epoch": 0.7516531165311653,
      "step": 3467,
      "training_loss": 6.613404750823975
    },
    {
      "epoch": 0.751869918699187,
      "grad_norm": 15.727465629577637,
      "learning_rate": 1e-05,
      "loss": 6.3921,
      "step": 3468
    },
    {
      "epoch": 0.751869918699187,
      "step": 3468,
      "training_loss": 6.309930324554443
    },
    {
      "epoch": 0.751869918699187,
      "step": 3468,
      "training_loss": 5.42140007019043
    },
    {
      "epoch": 0.751869918699187,
      "step": 3468,
      "training_loss": 4.861464023590088
    },
    {
      "epoch": 0.751869918699187,
      "step": 3468,
      "training_loss": 6.80935525894165
    },
    {
      "epoch": 0.7520867208672086,
      "step": 3469,
      "training_loss": 6.820339679718018
    },
    {
      "epoch": 0.7520867208672086,
      "step": 3469,
      "training_loss": 6.461022853851318
    },
    {
      "epoch": 0.7520867208672086,
      "step": 3469,
      "training_loss": 6.376179218292236
    },
    {
      "epoch": 0.7520867208672086,
      "step": 3469,
      "training_loss": 6.345423698425293
    },
    {
      "epoch": 0.7523035230352304,
      "step": 3470,
      "training_loss": 7.301246643066406
    },
    {
      "epoch": 0.7523035230352304,
      "step": 3470,
      "training_loss": 6.562227249145508
    },
    {
      "epoch": 0.7523035230352304,
      "step": 3470,
      "training_loss": 7.487959384918213
    },
    {
      "epoch": 0.7523035230352304,
      "step": 3470,
      "training_loss": 7.237943172454834
    },
    {
      "epoch": 0.7525203252032521,
      "step": 3471,
      "training_loss": 6.725892543792725
    },
    {
      "epoch": 0.7525203252032521,
      "step": 3471,
      "training_loss": 7.480617523193359
    },
    {
      "epoch": 0.7525203252032521,
      "step": 3471,
      "training_loss": 6.839388370513916
    },
    {
      "epoch": 0.7525203252032521,
      "step": 3471,
      "training_loss": 5.285987854003906
    },
    {
      "epoch": 0.7527371273712737,
      "grad_norm": 21.792984008789062,
      "learning_rate": 1e-05,
      "loss": 6.5204,
      "step": 3472
    },
    {
      "epoch": 0.7527371273712737,
      "step": 3472,
      "training_loss": 5.612453460693359
    },
    {
      "epoch": 0.7527371273712737,
      "step": 3472,
      "training_loss": 7.289311408996582
    },
    {
      "epoch": 0.7527371273712737,
      "step": 3472,
      "training_loss": 6.818099021911621
    },
    {
      "epoch": 0.7527371273712737,
      "step": 3472,
      "training_loss": 5.858662128448486
    },
    {
      "epoch": 0.7529539295392954,
      "step": 3473,
      "training_loss": 6.980363368988037
    },
    {
      "epoch": 0.7529539295392954,
      "step": 3473,
      "training_loss": 6.9699506759643555
    },
    {
      "epoch": 0.7529539295392954,
      "step": 3473,
      "training_loss": 6.653355598449707
    },
    {
      "epoch": 0.7529539295392954,
      "step": 3473,
      "training_loss": 6.503958702087402
    },
    {
      "epoch": 0.7531707317073171,
      "step": 3474,
      "training_loss": 7.129576206207275
    },
    {
      "epoch": 0.7531707317073171,
      "step": 3474,
      "training_loss": 6.777512073516846
    },
    {
      "epoch": 0.7531707317073171,
      "step": 3474,
      "training_loss": 7.173347473144531
    },
    {
      "epoch": 0.7531707317073171,
      "step": 3474,
      "training_loss": 6.263806343078613
    },
    {
      "epoch": 0.7533875338753387,
      "step": 3475,
      "training_loss": 7.199880123138428
    },
    {
      "epoch": 0.7533875338753387,
      "step": 3475,
      "training_loss": 5.7670793533325195
    },
    {
      "epoch": 0.7533875338753387,
      "step": 3475,
      "training_loss": 3.779400587081909
    },
    {
      "epoch": 0.7533875338753387,
      "step": 3475,
      "training_loss": 3.582498073577881
    },
    {
      "epoch": 0.7536043360433604,
      "grad_norm": 27.548763275146484,
      "learning_rate": 1e-05,
      "loss": 6.2725,
      "step": 3476
    },
    {
      "epoch": 0.7536043360433604,
      "step": 3476,
      "training_loss": 7.010883808135986
    },
    {
      "epoch": 0.7536043360433604,
      "step": 3476,
      "training_loss": 7.2402424812316895
    },
    {
      "epoch": 0.7536043360433604,
      "step": 3476,
      "training_loss": 5.540970325469971
    },
    {
      "epoch": 0.7536043360433604,
      "step": 3476,
      "training_loss": 8.811689376831055
    },
    {
      "epoch": 0.7538211382113821,
      "step": 3477,
      "training_loss": 7.582359790802002
    },
    {
      "epoch": 0.7538211382113821,
      "step": 3477,
      "training_loss": 4.156310558319092
    },
    {
      "epoch": 0.7538211382113821,
      "step": 3477,
      "training_loss": 7.810529708862305
    },
    {
      "epoch": 0.7538211382113821,
      "step": 3477,
      "training_loss": 6.032632350921631
    },
    {
      "epoch": 0.7540379403794037,
      "step": 3478,
      "training_loss": 6.213868618011475
    },
    {
      "epoch": 0.7540379403794037,
      "step": 3478,
      "training_loss": 6.188503742218018
    },
    {
      "epoch": 0.7540379403794037,
      "step": 3478,
      "training_loss": 8.060768127441406
    },
    {
      "epoch": 0.7540379403794037,
      "step": 3478,
      "training_loss": 7.542943477630615
    },
    {
      "epoch": 0.7542547425474255,
      "step": 3479,
      "training_loss": 6.009603023529053
    },
    {
      "epoch": 0.7542547425474255,
      "step": 3479,
      "training_loss": 7.04998254776001
    },
    {
      "epoch": 0.7542547425474255,
      "step": 3479,
      "training_loss": 6.452853679656982
    },
    {
      "epoch": 0.7542547425474255,
      "step": 3479,
      "training_loss": 6.754131317138672
    },
    {
      "epoch": 0.7544715447154472,
      "grad_norm": 14.06961727142334,
      "learning_rate": 1e-05,
      "loss": 6.7786,
      "step": 3480
    },
    {
      "epoch": 0.7544715447154472,
      "step": 3480,
      "training_loss": 7.382936000823975
    },
    {
      "epoch": 0.7544715447154472,
      "step": 3480,
      "training_loss": 6.7114176750183105
    },
    {
      "epoch": 0.7544715447154472,
      "step": 3480,
      "training_loss": 6.323784828186035
    },
    {
      "epoch": 0.7544715447154472,
      "step": 3480,
      "training_loss": 5.545973777770996
    },
    {
      "epoch": 0.7546883468834689,
      "step": 3481,
      "training_loss": 5.017096519470215
    },
    {
      "epoch": 0.7546883468834689,
      "step": 3481,
      "training_loss": 6.551784515380859
    },
    {
      "epoch": 0.7546883468834689,
      "step": 3481,
      "training_loss": 6.840519428253174
    },
    {
      "epoch": 0.7546883468834689,
      "step": 3481,
      "training_loss": 6.255538463592529
    },
    {
      "epoch": 0.7549051490514905,
      "step": 3482,
      "training_loss": 5.903580665588379
    },
    {
      "epoch": 0.7549051490514905,
      "step": 3482,
      "training_loss": 6.292947292327881
    },
    {
      "epoch": 0.7549051490514905,
      "step": 3482,
      "training_loss": 6.542631149291992
    },
    {
      "epoch": 0.7549051490514905,
      "step": 3482,
      "training_loss": 6.602147579193115
    },
    {
      "epoch": 0.7551219512195122,
      "step": 3483,
      "training_loss": 5.201485633850098
    },
    {
      "epoch": 0.7551219512195122,
      "step": 3483,
      "training_loss": 7.107409954071045
    },
    {
      "epoch": 0.7551219512195122,
      "step": 3483,
      "training_loss": 5.338441371917725
    },
    {
      "epoch": 0.7551219512195122,
      "step": 3483,
      "training_loss": 5.206795692443848
    },
    {
      "epoch": 0.7553387533875339,
      "grad_norm": 19.79351234436035,
      "learning_rate": 1e-05,
      "loss": 6.1765,
      "step": 3484
    },
    {
      "epoch": 0.7553387533875339,
      "step": 3484,
      "training_loss": 7.088578701019287
    },
    {
      "epoch": 0.7553387533875339,
      "step": 3484,
      "training_loss": 3.5643556118011475
    },
    {
      "epoch": 0.7553387533875339,
      "step": 3484,
      "training_loss": 6.138468265533447
    },
    {
      "epoch": 0.7553387533875339,
      "step": 3484,
      "training_loss": 5.624701976776123
    },
    {
      "epoch": 0.7555555555555555,
      "step": 3485,
      "training_loss": 6.31353759765625
    },
    {
      "epoch": 0.7555555555555555,
      "step": 3485,
      "training_loss": 6.894576549530029
    },
    {
      "epoch": 0.7555555555555555,
      "step": 3485,
      "training_loss": 6.096510410308838
    },
    {
      "epoch": 0.7555555555555555,
      "step": 3485,
      "training_loss": 5.997473239898682
    },
    {
      "epoch": 0.7557723577235772,
      "step": 3486,
      "training_loss": 6.288601875305176
    },
    {
      "epoch": 0.7557723577235772,
      "step": 3486,
      "training_loss": 6.83178186416626
    },
    {
      "epoch": 0.7557723577235772,
      "step": 3486,
      "training_loss": 6.22520637512207
    },
    {
      "epoch": 0.7557723577235772,
      "step": 3486,
      "training_loss": 6.426266670227051
    },
    {
      "epoch": 0.7559891598915989,
      "step": 3487,
      "training_loss": 5.255735397338867
    },
    {
      "epoch": 0.7559891598915989,
      "step": 3487,
      "training_loss": 6.289564609527588
    },
    {
      "epoch": 0.7559891598915989,
      "step": 3487,
      "training_loss": 6.012165546417236
    },
    {
      "epoch": 0.7559891598915989,
      "step": 3487,
      "training_loss": 5.628364086151123
    },
    {
      "epoch": 0.7562059620596207,
      "grad_norm": 15.861088752746582,
      "learning_rate": 1e-05,
      "loss": 6.0422,
      "step": 3488
    },
    {
      "epoch": 0.7562059620596207,
      "step": 3488,
      "training_loss": 6.362340927124023
    },
    {
      "epoch": 0.7562059620596207,
      "step": 3488,
      "training_loss": 7.303447246551514
    },
    {
      "epoch": 0.7562059620596207,
      "step": 3488,
      "training_loss": 6.197972297668457
    },
    {
      "epoch": 0.7562059620596207,
      "step": 3488,
      "training_loss": 5.259657382965088
    },
    {
      "epoch": 0.7564227642276423,
      "step": 3489,
      "training_loss": 5.015413284301758
    },
    {
      "epoch": 0.7564227642276423,
      "step": 3489,
      "training_loss": 8.018033981323242
    },
    {
      "epoch": 0.7564227642276423,
      "step": 3489,
      "training_loss": 7.229771614074707
    },
    {
      "epoch": 0.7564227642276423,
      "step": 3489,
      "training_loss": 6.172943115234375
    },
    {
      "epoch": 0.756639566395664,
      "step": 3490,
      "training_loss": 6.080386638641357
    },
    {
      "epoch": 0.756639566395664,
      "step": 3490,
      "training_loss": 7.082947731018066
    },
    {
      "epoch": 0.756639566395664,
      "step": 3490,
      "training_loss": 6.186629772186279
    },
    {
      "epoch": 0.756639566395664,
      "step": 3490,
      "training_loss": 5.769345283508301
    },
    {
      "epoch": 0.7568563685636857,
      "step": 3491,
      "training_loss": 3.2091405391693115
    },
    {
      "epoch": 0.7568563685636857,
      "step": 3491,
      "training_loss": 3.616424798965454
    },
    {
      "epoch": 0.7568563685636857,
      "step": 3491,
      "training_loss": 6.919520378112793
    },
    {
      "epoch": 0.7568563685636857,
      "step": 3491,
      "training_loss": 7.198512077331543
    },
    {
      "epoch": 0.7570731707317073,
      "grad_norm": 16.154470443725586,
      "learning_rate": 1e-05,
      "loss": 6.1014,
      "step": 3492
    },
    {
      "epoch": 0.7570731707317073,
      "step": 3492,
      "training_loss": 6.511988162994385
    },
    {
      "epoch": 0.7570731707317073,
      "step": 3492,
      "training_loss": 7.914381504058838
    },
    {
      "epoch": 0.7570731707317073,
      "step": 3492,
      "training_loss": 7.569686412811279
    },
    {
      "epoch": 0.7570731707317073,
      "step": 3492,
      "training_loss": 7.299559116363525
    },
    {
      "epoch": 0.757289972899729,
      "step": 3493,
      "training_loss": 7.116878986358643
    },
    {
      "epoch": 0.757289972899729,
      "step": 3493,
      "training_loss": 4.695977210998535
    },
    {
      "epoch": 0.757289972899729,
      "step": 3493,
      "training_loss": 6.82373571395874
    },
    {
      "epoch": 0.757289972899729,
      "step": 3493,
      "training_loss": 6.273812294006348
    },
    {
      "epoch": 0.7575067750677507,
      "step": 3494,
      "training_loss": 6.661346912384033
    },
    {
      "epoch": 0.7575067750677507,
      "step": 3494,
      "training_loss": 6.978498458862305
    },
    {
      "epoch": 0.7575067750677507,
      "step": 3494,
      "training_loss": 6.034116744995117
    },
    {
      "epoch": 0.7575067750677507,
      "step": 3494,
      "training_loss": 6.2228827476501465
    },
    {
      "epoch": 0.7577235772357723,
      "step": 3495,
      "training_loss": 5.2730937004089355
    },
    {
      "epoch": 0.7577235772357723,
      "step": 3495,
      "training_loss": 6.893280506134033
    },
    {
      "epoch": 0.7577235772357723,
      "step": 3495,
      "training_loss": 5.885311126708984
    },
    {
      "epoch": 0.7577235772357723,
      "step": 3495,
      "training_loss": 7.216897487640381
    },
    {
      "epoch": 0.757940379403794,
      "grad_norm": 15.417759895324707,
      "learning_rate": 1e-05,
      "loss": 6.5857,
      "step": 3496
    },
    {
      "epoch": 0.757940379403794,
      "step": 3496,
      "training_loss": 6.8569464683532715
    },
    {
      "epoch": 0.757940379403794,
      "step": 3496,
      "training_loss": 6.633069038391113
    },
    {
      "epoch": 0.757940379403794,
      "step": 3496,
      "training_loss": 5.851850509643555
    },
    {
      "epoch": 0.757940379403794,
      "step": 3496,
      "training_loss": 5.878871440887451
    },
    {
      "epoch": 0.7581571815718157,
      "step": 3497,
      "training_loss": 5.978348255157471
    },
    {
      "epoch": 0.7581571815718157,
      "step": 3497,
      "training_loss": 6.242579936981201
    },
    {
      "epoch": 0.7581571815718157,
      "step": 3497,
      "training_loss": 5.687334060668945
    },
    {
      "epoch": 0.7581571815718157,
      "step": 3497,
      "training_loss": 7.828073978424072
    },
    {
      "epoch": 0.7583739837398374,
      "step": 3498,
      "training_loss": 6.639897346496582
    },
    {
      "epoch": 0.7583739837398374,
      "step": 3498,
      "training_loss": 7.264213562011719
    },
    {
      "epoch": 0.7583739837398374,
      "step": 3498,
      "training_loss": 5.851359844207764
    },
    {
      "epoch": 0.7583739837398374,
      "step": 3498,
      "training_loss": 7.120894432067871
    },
    {
      "epoch": 0.7585907859078591,
      "step": 3499,
      "training_loss": 6.524025917053223
    },
    {
      "epoch": 0.7585907859078591,
      "step": 3499,
      "training_loss": 7.37280797958374
    },
    {
      "epoch": 0.7585907859078591,
      "step": 3499,
      "training_loss": 6.7234320640563965
    },
    {
      "epoch": 0.7585907859078591,
      "step": 3499,
      "training_loss": 7.530310153961182
    },
    {
      "epoch": 0.7588075880758808,
      "grad_norm": 25.6579532623291,
      "learning_rate": 1e-05,
      "loss": 6.624,
      "step": 3500
    },
    {
      "epoch": 0.7588075880758808,
      "step": 3500,
      "training_loss": 5.771212100982666
    },
    {
      "epoch": 0.7588075880758808,
      "step": 3500,
      "training_loss": 5.511419773101807
    },
    {
      "epoch": 0.7588075880758808,
      "step": 3500,
      "training_loss": 5.2426934242248535
    },
    {
      "epoch": 0.7588075880758808,
      "step": 3500,
      "training_loss": 6.510385513305664
    },
    {
      "epoch": 0.7590243902439024,
      "step": 3501,
      "training_loss": 7.286310195922852
    },
    {
      "epoch": 0.7590243902439024,
      "step": 3501,
      "training_loss": 6.811868667602539
    },
    {
      "epoch": 0.7590243902439024,
      "step": 3501,
      "training_loss": 6.422402858734131
    },
    {
      "epoch": 0.7590243902439024,
      "step": 3501,
      "training_loss": 5.86605978012085
    },
    {
      "epoch": 0.7592411924119241,
      "step": 3502,
      "training_loss": 6.7081522941589355
    },
    {
      "epoch": 0.7592411924119241,
      "step": 3502,
      "training_loss": 6.781824111938477
    },
    {
      "epoch": 0.7592411924119241,
      "step": 3502,
      "training_loss": 5.385116100311279
    },
    {
      "epoch": 0.7592411924119241,
      "step": 3502,
      "training_loss": 7.573151588439941
    },
    {
      "epoch": 0.7594579945799458,
      "step": 3503,
      "training_loss": 6.455446720123291
    },
    {
      "epoch": 0.7594579945799458,
      "step": 3503,
      "training_loss": 6.282801151275635
    },
    {
      "epoch": 0.7594579945799458,
      "step": 3503,
      "training_loss": 5.6301116943359375
    },
    {
      "epoch": 0.7594579945799458,
      "step": 3503,
      "training_loss": 6.8384504318237305
    },
    {
      "epoch": 0.7596747967479675,
      "grad_norm": 27.845930099487305,
      "learning_rate": 1e-05,
      "loss": 6.3173,
      "step": 3504
    },
    {
      "epoch": 0.7596747967479675,
      "step": 3504,
      "training_loss": 5.698702812194824
    },
    {
      "epoch": 0.7596747967479675,
      "step": 3504,
      "training_loss": 6.283513069152832
    },
    {
      "epoch": 0.7596747967479675,
      "step": 3504,
      "training_loss": 5.905415058135986
    },
    {
      "epoch": 0.7596747967479675,
      "step": 3504,
      "training_loss": 5.092869281768799
    },
    {
      "epoch": 0.7598915989159891,
      "step": 3505,
      "training_loss": 7.556921005249023
    },
    {
      "epoch": 0.7598915989159891,
      "step": 3505,
      "training_loss": 5.781113147735596
    },
    {
      "epoch": 0.7598915989159891,
      "step": 3505,
      "training_loss": 6.485254764556885
    },
    {
      "epoch": 0.7598915989159891,
      "step": 3505,
      "training_loss": 5.610325336456299
    },
    {
      "epoch": 0.7601084010840108,
      "step": 3506,
      "training_loss": 7.916487693786621
    },
    {
      "epoch": 0.7601084010840108,
      "step": 3506,
      "training_loss": 7.373779296875
    },
    {
      "epoch": 0.7601084010840108,
      "step": 3506,
      "training_loss": 5.330427646636963
    },
    {
      "epoch": 0.7601084010840108,
      "step": 3506,
      "training_loss": 7.123096942901611
    },
    {
      "epoch": 0.7603252032520326,
      "step": 3507,
      "training_loss": 7.536819934844971
    },
    {
      "epoch": 0.7603252032520326,
      "step": 3507,
      "training_loss": 5.963736057281494
    },
    {
      "epoch": 0.7603252032520326,
      "step": 3507,
      "training_loss": 5.3151960372924805
    },
    {
      "epoch": 0.7603252032520326,
      "step": 3507,
      "training_loss": 6.488518714904785
    },
    {
      "epoch": 0.7605420054200542,
      "grad_norm": 18.81978416442871,
      "learning_rate": 1e-05,
      "loss": 6.3414,
      "step": 3508
    },
    {
      "epoch": 0.7605420054200542,
      "step": 3508,
      "training_loss": 5.880467891693115
    },
    {
      "epoch": 0.7605420054200542,
      "step": 3508,
      "training_loss": 6.61361026763916
    },
    {
      "epoch": 0.7605420054200542,
      "step": 3508,
      "training_loss": 6.873410224914551
    },
    {
      "epoch": 0.7605420054200542,
      "step": 3508,
      "training_loss": 7.031672477722168
    },
    {
      "epoch": 0.7607588075880759,
      "step": 3509,
      "training_loss": 6.3924336433410645
    },
    {
      "epoch": 0.7607588075880759,
      "step": 3509,
      "training_loss": 5.832387924194336
    },
    {
      "epoch": 0.7607588075880759,
      "step": 3509,
      "training_loss": 6.973331451416016
    },
    {
      "epoch": 0.7607588075880759,
      "step": 3509,
      "training_loss": 6.553679943084717
    },
    {
      "epoch": 0.7609756097560976,
      "step": 3510,
      "training_loss": 6.334612846374512
    },
    {
      "epoch": 0.7609756097560976,
      "step": 3510,
      "training_loss": 4.253520488739014
    },
    {
      "epoch": 0.7609756097560976,
      "step": 3510,
      "training_loss": 7.431807518005371
    },
    {
      "epoch": 0.7609756097560976,
      "step": 3510,
      "training_loss": 7.649766445159912
    },
    {
      "epoch": 0.7611924119241192,
      "step": 3511,
      "training_loss": 6.035328388214111
    },
    {
      "epoch": 0.7611924119241192,
      "step": 3511,
      "training_loss": 5.016829967498779
    },
    {
      "epoch": 0.7611924119241192,
      "step": 3511,
      "training_loss": 6.448306083679199
    },
    {
      "epoch": 0.7611924119241192,
      "step": 3511,
      "training_loss": 6.173338890075684
    },
    {
      "epoch": 0.7614092140921409,
      "grad_norm": 16.609146118164062,
      "learning_rate": 1e-05,
      "loss": 6.3434,
      "step": 3512
    },
    {
      "epoch": 0.7614092140921409,
      "step": 3512,
      "training_loss": 6.5575852394104
    },
    {
      "epoch": 0.7614092140921409,
      "step": 3512,
      "training_loss": 6.852205753326416
    },
    {
      "epoch": 0.7614092140921409,
      "step": 3512,
      "training_loss": 6.929644584655762
    },
    {
      "epoch": 0.7614092140921409,
      "step": 3512,
      "training_loss": 7.15183162689209
    },
    {
      "epoch": 0.7616260162601626,
      "step": 3513,
      "training_loss": 5.257972240447998
    },
    {
      "epoch": 0.7616260162601626,
      "step": 3513,
      "training_loss": 6.895843982696533
    },
    {
      "epoch": 0.7616260162601626,
      "step": 3513,
      "training_loss": 6.328615665435791
    },
    {
      "epoch": 0.7616260162601626,
      "step": 3513,
      "training_loss": 7.008638858795166
    },
    {
      "epoch": 0.7618428184281842,
      "step": 3514,
      "training_loss": 4.023130893707275
    },
    {
      "epoch": 0.7618428184281842,
      "step": 3514,
      "training_loss": 6.56506872177124
    },
    {
      "epoch": 0.7618428184281842,
      "step": 3514,
      "training_loss": 7.062801837921143
    },
    {
      "epoch": 0.7618428184281842,
      "step": 3514,
      "training_loss": 6.913001537322998
    },
    {
      "epoch": 0.7620596205962059,
      "step": 3515,
      "training_loss": 6.41155481338501
    },
    {
      "epoch": 0.7620596205962059,
      "step": 3515,
      "training_loss": 6.590357780456543
    },
    {
      "epoch": 0.7620596205962059,
      "step": 3515,
      "training_loss": 5.965530872344971
    },
    {
      "epoch": 0.7620596205962059,
      "step": 3515,
      "training_loss": 6.440335273742676
    },
    {
      "epoch": 0.7622764227642277,
      "grad_norm": 18.020475387573242,
      "learning_rate": 1e-05,
      "loss": 6.4346,
      "step": 3516
    },
    {
      "epoch": 0.7622764227642277,
      "step": 3516,
      "training_loss": 6.519604682922363
    },
    {
      "epoch": 0.7622764227642277,
      "step": 3516,
      "training_loss": 7.077526092529297
    },
    {
      "epoch": 0.7622764227642277,
      "step": 3516,
      "training_loss": 5.549078941345215
    },
    {
      "epoch": 0.7622764227642277,
      "step": 3516,
      "training_loss": 6.889068603515625
    },
    {
      "epoch": 0.7624932249322494,
      "step": 3517,
      "training_loss": 4.492497444152832
    },
    {
      "epoch": 0.7624932249322494,
      "step": 3517,
      "training_loss": 7.087974548339844
    },
    {
      "epoch": 0.7624932249322494,
      "step": 3517,
      "training_loss": 4.180731296539307
    },
    {
      "epoch": 0.7624932249322494,
      "step": 3517,
      "training_loss": 7.4015631675720215
    },
    {
      "epoch": 0.762710027100271,
      "step": 3518,
      "training_loss": 7.205418586730957
    },
    {
      "epoch": 0.762710027100271,
      "step": 3518,
      "training_loss": 6.427326679229736
    },
    {
      "epoch": 0.762710027100271,
      "step": 3518,
      "training_loss": 5.348014831542969
    },
    {
      "epoch": 0.762710027100271,
      "step": 3518,
      "training_loss": 6.305041790008545
    },
    {
      "epoch": 0.7629268292682927,
      "step": 3519,
      "training_loss": 5.7623419761657715
    },
    {
      "epoch": 0.7629268292682927,
      "step": 3519,
      "training_loss": 7.277928829193115
    },
    {
      "epoch": 0.7629268292682927,
      "step": 3519,
      "training_loss": 5.941621780395508
    },
    {
      "epoch": 0.7629268292682927,
      "step": 3519,
      "training_loss": 6.10565185546875
    },
    {
      "epoch": 0.7631436314363144,
      "grad_norm": 19.072589874267578,
      "learning_rate": 1e-05,
      "loss": 6.2232,
      "step": 3520
    },
    {
      "epoch": 0.7631436314363144,
      "step": 3520,
      "training_loss": 3.8297016620635986
    },
    {
      "epoch": 0.7631436314363144,
      "step": 3520,
      "training_loss": 10.794490814208984
    },
    {
      "epoch": 0.7631436314363144,
      "step": 3520,
      "training_loss": 7.803525447845459
    },
    {
      "epoch": 0.7631436314363144,
      "step": 3520,
      "training_loss": 6.366426944732666
    },
    {
      "epoch": 0.763360433604336,
      "step": 3521,
      "training_loss": 5.557493209838867
    },
    {
      "epoch": 0.763360433604336,
      "step": 3521,
      "training_loss": 4.458981037139893
    },
    {
      "epoch": 0.763360433604336,
      "step": 3521,
      "training_loss": 6.990473747253418
    },
    {
      "epoch": 0.763360433604336,
      "step": 3521,
      "training_loss": 6.989436626434326
    },
    {
      "epoch": 0.7635772357723577,
      "step": 3522,
      "training_loss": 6.71265172958374
    },
    {
      "epoch": 0.7635772357723577,
      "step": 3522,
      "training_loss": 6.0753607749938965
    },
    {
      "epoch": 0.7635772357723577,
      "step": 3522,
      "training_loss": 7.218554496765137
    },
    {
      "epoch": 0.7635772357723577,
      "step": 3522,
      "training_loss": 6.659079074859619
    },
    {
      "epoch": 0.7637940379403794,
      "step": 3523,
      "training_loss": 6.309762001037598
    },
    {
      "epoch": 0.7637940379403794,
      "step": 3523,
      "training_loss": 5.759639263153076
    },
    {
      "epoch": 0.7637940379403794,
      "step": 3523,
      "training_loss": 5.658898830413818
    },
    {
      "epoch": 0.7637940379403794,
      "step": 3523,
      "training_loss": 7.001544952392578
    },
    {
      "epoch": 0.764010840108401,
      "grad_norm": 18.73177146911621,
      "learning_rate": 1e-05,
      "loss": 6.5116,
      "step": 3524
    },
    {
      "epoch": 0.764010840108401,
      "step": 3524,
      "training_loss": 7.7887139320373535
    },
    {
      "epoch": 0.764010840108401,
      "step": 3524,
      "training_loss": 7.924746036529541
    },
    {
      "epoch": 0.764010840108401,
      "step": 3524,
      "training_loss": 6.138424396514893
    },
    {
      "epoch": 0.764010840108401,
      "step": 3524,
      "training_loss": 7.470177173614502
    },
    {
      "epoch": 0.7642276422764228,
      "step": 3525,
      "training_loss": 6.514616966247559
    },
    {
      "epoch": 0.7642276422764228,
      "step": 3525,
      "training_loss": 6.177936553955078
    },
    {
      "epoch": 0.7642276422764228,
      "step": 3525,
      "training_loss": 6.378853797912598
    },
    {
      "epoch": 0.7642276422764228,
      "step": 3525,
      "training_loss": 7.352798938751221
    },
    {
      "epoch": 0.7644444444444445,
      "step": 3526,
      "training_loss": 6.172786235809326
    },
    {
      "epoch": 0.7644444444444445,
      "step": 3526,
      "training_loss": 7.499279975891113
    },
    {
      "epoch": 0.7644444444444445,
      "step": 3526,
      "training_loss": 7.074596405029297
    },
    {
      "epoch": 0.7644444444444445,
      "step": 3526,
      "training_loss": 6.841230392456055
    },
    {
      "epoch": 0.7646612466124662,
      "step": 3527,
      "training_loss": 6.861598968505859
    },
    {
      "epoch": 0.7646612466124662,
      "step": 3527,
      "training_loss": 3.9604127407073975
    },
    {
      "epoch": 0.7646612466124662,
      "step": 3527,
      "training_loss": 6.7909722328186035
    },
    {
      "epoch": 0.7646612466124662,
      "step": 3527,
      "training_loss": 6.435820579528809
    },
    {
      "epoch": 0.7648780487804878,
      "grad_norm": 15.176115989685059,
      "learning_rate": 1e-05,
      "loss": 6.7114,
      "step": 3528
    },
    {
      "epoch": 0.7648780487804878,
      "step": 3528,
      "training_loss": 6.87092924118042
    },
    {
      "epoch": 0.7648780487804878,
      "step": 3528,
      "training_loss": 5.6895551681518555
    },
    {
      "epoch": 0.7648780487804878,
      "step": 3528,
      "training_loss": 6.316195487976074
    },
    {
      "epoch": 0.7648780487804878,
      "step": 3528,
      "training_loss": 6.066577911376953
    },
    {
      "epoch": 0.7650948509485095,
      "step": 3529,
      "training_loss": 6.97411584854126
    },
    {
      "epoch": 0.7650948509485095,
      "step": 3529,
      "training_loss": 4.681124687194824
    },
    {
      "epoch": 0.7650948509485095,
      "step": 3529,
      "training_loss": 6.161128520965576
    },
    {
      "epoch": 0.7650948509485095,
      "step": 3529,
      "training_loss": 5.464632034301758
    },
    {
      "epoch": 0.7653116531165312,
      "step": 3530,
      "training_loss": 5.82358980178833
    },
    {
      "epoch": 0.7653116531165312,
      "step": 3530,
      "training_loss": 4.2548909187316895
    },
    {
      "epoch": 0.7653116531165312,
      "step": 3530,
      "training_loss": 6.500715732574463
    },
    {
      "epoch": 0.7653116531165312,
      "step": 3530,
      "training_loss": 6.89995813369751
    },
    {
      "epoch": 0.7655284552845528,
      "step": 3531,
      "training_loss": 5.266537189483643
    },
    {
      "epoch": 0.7655284552845528,
      "step": 3531,
      "training_loss": 7.48099422454834
    },
    {
      "epoch": 0.7655284552845528,
      "step": 3531,
      "training_loss": 8.060073852539062
    },
    {
      "epoch": 0.7655284552845528,
      "step": 3531,
      "training_loss": 6.45588493347168
    },
    {
      "epoch": 0.7657452574525745,
      "grad_norm": 19.13410186767578,
      "learning_rate": 1e-05,
      "loss": 6.1854,
      "step": 3532
    },
    {
      "epoch": 0.7657452574525745,
      "step": 3532,
      "training_loss": 6.5210723876953125
    },
    {
      "epoch": 0.7657452574525745,
      "step": 3532,
      "training_loss": 6.511876106262207
    },
    {
      "epoch": 0.7657452574525745,
      "step": 3532,
      "training_loss": 8.813568115234375
    },
    {
      "epoch": 0.7657452574525745,
      "step": 3532,
      "training_loss": 5.474358558654785
    },
    {
      "epoch": 0.7659620596205962,
      "step": 3533,
      "training_loss": 6.650304317474365
    },
    {
      "epoch": 0.7659620596205962,
      "step": 3533,
      "training_loss": 6.476528644561768
    },
    {
      "epoch": 0.7659620596205962,
      "step": 3533,
      "training_loss": 7.394600868225098
    },
    {
      "epoch": 0.7659620596205962,
      "step": 3533,
      "training_loss": 7.219687461853027
    },
    {
      "epoch": 0.7661788617886179,
      "step": 3534,
      "training_loss": 4.79354190826416
    },
    {
      "epoch": 0.7661788617886179,
      "step": 3534,
      "training_loss": 7.03773307800293
    },
    {
      "epoch": 0.7661788617886179,
      "step": 3534,
      "training_loss": 6.405289649963379
    },
    {
      "epoch": 0.7661788617886179,
      "step": 3534,
      "training_loss": 7.854869842529297
    },
    {
      "epoch": 0.7663956639566396,
      "step": 3535,
      "training_loss": 8.340564727783203
    },
    {
      "epoch": 0.7663956639566396,
      "step": 3535,
      "training_loss": 6.008134365081787
    },
    {
      "epoch": 0.7663956639566396,
      "step": 3535,
      "training_loss": 5.946034908294678
    },
    {
      "epoch": 0.7663956639566396,
      "step": 3535,
      "training_loss": 7.377906322479248
    },
    {
      "epoch": 0.7666124661246613,
      "grad_norm": 22.664691925048828,
      "learning_rate": 1e-05,
      "loss": 6.8016,
      "step": 3536
    },
    {
      "epoch": 0.7666124661246613,
      "step": 3536,
      "training_loss": 6.402821063995361
    },
    {
      "epoch": 0.7666124661246613,
      "step": 3536,
      "training_loss": 6.916174411773682
    },
    {
      "epoch": 0.7666124661246613,
      "step": 3536,
      "training_loss": 6.751110553741455
    },
    {
      "epoch": 0.7666124661246613,
      "step": 3536,
      "training_loss": 6.864639759063721
    },
    {
      "epoch": 0.7668292682926829,
      "step": 3537,
      "training_loss": 6.952966690063477
    },
    {
      "epoch": 0.7668292682926829,
      "step": 3537,
      "training_loss": 8.702066421508789
    },
    {
      "epoch": 0.7668292682926829,
      "step": 3537,
      "training_loss": 6.755577564239502
    },
    {
      "epoch": 0.7668292682926829,
      "step": 3537,
      "training_loss": 6.216221809387207
    },
    {
      "epoch": 0.7670460704607046,
      "step": 3538,
      "training_loss": 7.0206451416015625
    },
    {
      "epoch": 0.7670460704607046,
      "step": 3538,
      "training_loss": 6.424287796020508
    },
    {
      "epoch": 0.7670460704607046,
      "step": 3538,
      "training_loss": 7.455684185028076
    },
    {
      "epoch": 0.7670460704607046,
      "step": 3538,
      "training_loss": 6.519599437713623
    },
    {
      "epoch": 0.7672628726287263,
      "step": 3539,
      "training_loss": 4.291869163513184
    },
    {
      "epoch": 0.7672628726287263,
      "step": 3539,
      "training_loss": 7.405770778656006
    },
    {
      "epoch": 0.7672628726287263,
      "step": 3539,
      "training_loss": 6.5648193359375
    },
    {
      "epoch": 0.7672628726287263,
      "step": 3539,
      "training_loss": 4.214151382446289
    },
    {
      "epoch": 0.767479674796748,
      "grad_norm": 12.040372848510742,
      "learning_rate": 1e-05,
      "loss": 6.5911,
      "step": 3540
    },
    {
      "epoch": 0.767479674796748,
      "step": 3540,
      "training_loss": 2.5789082050323486
    },
    {
      "epoch": 0.767479674796748,
      "step": 3540,
      "training_loss": 6.0869293212890625
    },
    {
      "epoch": 0.767479674796748,
      "step": 3540,
      "training_loss": 6.098444938659668
    },
    {
      "epoch": 0.767479674796748,
      "step": 3540,
      "training_loss": 7.403337001800537
    },
    {
      "epoch": 0.7676964769647696,
      "step": 3541,
      "training_loss": 6.244869232177734
    },
    {
      "epoch": 0.7676964769647696,
      "step": 3541,
      "training_loss": 7.623689651489258
    },
    {
      "epoch": 0.7676964769647696,
      "step": 3541,
      "training_loss": 4.221404552459717
    },
    {
      "epoch": 0.7676964769647696,
      "step": 3541,
      "training_loss": 5.776393890380859
    },
    {
      "epoch": 0.7679132791327913,
      "step": 3542,
      "training_loss": 6.677643299102783
    },
    {
      "epoch": 0.7679132791327913,
      "step": 3542,
      "training_loss": 6.565361976623535
    },
    {
      "epoch": 0.7679132791327913,
      "step": 3542,
      "training_loss": 5.687235355377197
    },
    {
      "epoch": 0.7679132791327913,
      "step": 3542,
      "training_loss": 6.815361499786377
    },
    {
      "epoch": 0.7681300813008131,
      "step": 3543,
      "training_loss": 7.199529647827148
    },
    {
      "epoch": 0.7681300813008131,
      "step": 3543,
      "training_loss": 5.864297866821289
    },
    {
      "epoch": 0.7681300813008131,
      "step": 3543,
      "training_loss": 6.805108070373535
    },
    {
      "epoch": 0.7681300813008131,
      "step": 3543,
      "training_loss": 6.899676322937012
    },
    {
      "epoch": 0.7683468834688347,
      "grad_norm": 19.943758010864258,
      "learning_rate": 1e-05,
      "loss": 6.1593,
      "step": 3544
    },
    {
      "epoch": 0.7683468834688347,
      "step": 3544,
      "training_loss": 6.10189962387085
    },
    {
      "epoch": 0.7683468834688347,
      "step": 3544,
      "training_loss": 5.660616874694824
    },
    {
      "epoch": 0.7683468834688347,
      "step": 3544,
      "training_loss": 7.395942687988281
    },
    {
      "epoch": 0.7683468834688347,
      "step": 3544,
      "training_loss": 6.936764240264893
    },
    {
      "epoch": 0.7685636856368564,
      "step": 3545,
      "training_loss": 5.383227825164795
    },
    {
      "epoch": 0.7685636856368564,
      "step": 3545,
      "training_loss": 6.972708702087402
    },
    {
      "epoch": 0.7685636856368564,
      "step": 3545,
      "training_loss": 7.33441686630249
    },
    {
      "epoch": 0.7685636856368564,
      "step": 3545,
      "training_loss": 5.9026265144348145
    },
    {
      "epoch": 0.7687804878048781,
      "step": 3546,
      "training_loss": 5.8487629890441895
    },
    {
      "epoch": 0.7687804878048781,
      "step": 3546,
      "training_loss": 4.3567986488342285
    },
    {
      "epoch": 0.7687804878048781,
      "step": 3546,
      "training_loss": 6.47797966003418
    },
    {
      "epoch": 0.7687804878048781,
      "step": 3546,
      "training_loss": 6.230513095855713
    },
    {
      "epoch": 0.7689972899728997,
      "step": 3547,
      "training_loss": 7.492249965667725
    },
    {
      "epoch": 0.7689972899728997,
      "step": 3547,
      "training_loss": 6.487924575805664
    },
    {
      "epoch": 0.7689972899728997,
      "step": 3547,
      "training_loss": 5.695488452911377
    },
    {
      "epoch": 0.7689972899728997,
      "step": 3547,
      "training_loss": 5.376143455505371
    },
    {
      "epoch": 0.7692140921409214,
      "grad_norm": 17.70676612854004,
      "learning_rate": 1e-05,
      "loss": 6.2284,
      "step": 3548
    },
    {
      "epoch": 0.7692140921409214,
      "step": 3548,
      "training_loss": 6.9065704345703125
    },
    {
      "epoch": 0.7692140921409214,
      "step": 3548,
      "training_loss": 6.19447660446167
    },
    {
      "epoch": 0.7692140921409214,
      "step": 3548,
      "training_loss": 6.193871021270752
    },
    {
      "epoch": 0.7692140921409214,
      "step": 3548,
      "training_loss": 6.627248287200928
    },
    {
      "epoch": 0.7694308943089431,
      "step": 3549,
      "training_loss": 6.445514678955078
    },
    {
      "epoch": 0.7694308943089431,
      "step": 3549,
      "training_loss": 6.565026760101318
    },
    {
      "epoch": 0.7694308943089431,
      "step": 3549,
      "training_loss": 3.168184995651245
    },
    {
      "epoch": 0.7694308943089431,
      "step": 3549,
      "training_loss": 6.057137966156006
    },
    {
      "epoch": 0.7696476964769647,
      "step": 3550,
      "training_loss": 6.283980369567871
    },
    {
      "epoch": 0.7696476964769647,
      "step": 3550,
      "training_loss": 5.452582836151123
    },
    {
      "epoch": 0.7696476964769647,
      "step": 3550,
      "training_loss": 7.219036102294922
    },
    {
      "epoch": 0.7696476964769647,
      "step": 3550,
      "training_loss": 6.473189830780029
    },
    {
      "epoch": 0.7698644986449864,
      "step": 3551,
      "training_loss": 6.701720714569092
    },
    {
      "epoch": 0.7698644986449864,
      "step": 3551,
      "training_loss": 6.97787618637085
    },
    {
      "epoch": 0.7698644986449864,
      "step": 3551,
      "training_loss": 4.553629398345947
    },
    {
      "epoch": 0.7698644986449864,
      "step": 3551,
      "training_loss": 5.6406450271606445
    },
    {
      "epoch": 0.7700813008130082,
      "grad_norm": 25.040239334106445,
      "learning_rate": 1e-05,
      "loss": 6.0913,
      "step": 3552
    },
    {
      "epoch": 0.7700813008130082,
      "step": 3552,
      "training_loss": 5.1696319580078125
    },
    {
      "epoch": 0.7700813008130082,
      "step": 3552,
      "training_loss": 6.165083408355713
    },
    {
      "epoch": 0.7700813008130082,
      "step": 3552,
      "training_loss": 5.827458381652832
    },
    {
      "epoch": 0.7700813008130082,
      "step": 3552,
      "training_loss": 7.19497537612915
    },
    {
      "epoch": 0.7702981029810299,
      "step": 3553,
      "training_loss": 6.898938179016113
    },
    {
      "epoch": 0.7702981029810299,
      "step": 3553,
      "training_loss": 6.965969562530518
    },
    {
      "epoch": 0.7702981029810299,
      "step": 3553,
      "training_loss": 7.402713298797607
    },
    {
      "epoch": 0.7702981029810299,
      "step": 3553,
      "training_loss": 6.582458019256592
    },
    {
      "epoch": 0.7705149051490515,
      "step": 3554,
      "training_loss": 7.042044639587402
    },
    {
      "epoch": 0.7705149051490515,
      "step": 3554,
      "training_loss": 6.340643405914307
    },
    {
      "epoch": 0.7705149051490515,
      "step": 3554,
      "training_loss": 4.664815902709961
    },
    {
      "epoch": 0.7705149051490515,
      "step": 3554,
      "training_loss": 5.075565814971924
    },
    {
      "epoch": 0.7707317073170732,
      "step": 3555,
      "training_loss": 4.751794815063477
    },
    {
      "epoch": 0.7707317073170732,
      "step": 3555,
      "training_loss": 7.086836338043213
    },
    {
      "epoch": 0.7707317073170732,
      "step": 3555,
      "training_loss": 6.113776683807373
    },
    {
      "epoch": 0.7707317073170732,
      "step": 3555,
      "training_loss": 5.422050952911377
    },
    {
      "epoch": 0.7709485094850949,
      "grad_norm": 17.03672981262207,
      "learning_rate": 1e-05,
      "loss": 6.169,
      "step": 3556
    },
    {
      "epoch": 0.7709485094850949,
      "step": 3556,
      "training_loss": 4.984989166259766
    },
    {
      "epoch": 0.7709485094850949,
      "step": 3556,
      "training_loss": 6.697789192199707
    },
    {
      "epoch": 0.7709485094850949,
      "step": 3556,
      "training_loss": 7.179718971252441
    },
    {
      "epoch": 0.7709485094850949,
      "step": 3556,
      "training_loss": 8.011431694030762
    },
    {
      "epoch": 0.7711653116531165,
      "step": 3557,
      "training_loss": 6.012063503265381
    },
    {
      "epoch": 0.7711653116531165,
      "step": 3557,
      "training_loss": 6.195355415344238
    },
    {
      "epoch": 0.7711653116531165,
      "step": 3557,
      "training_loss": 6.562931060791016
    },
    {
      "epoch": 0.7711653116531165,
      "step": 3557,
      "training_loss": 6.02959680557251
    },
    {
      "epoch": 0.7713821138211382,
      "step": 3558,
      "training_loss": 4.717875957489014
    },
    {
      "epoch": 0.7713821138211382,
      "step": 3558,
      "training_loss": 6.701826572418213
    },
    {
      "epoch": 0.7713821138211382,
      "step": 3558,
      "training_loss": 5.785423755645752
    },
    {
      "epoch": 0.7713821138211382,
      "step": 3558,
      "training_loss": 6.465738296508789
    },
    {
      "epoch": 0.7715989159891599,
      "step": 3559,
      "training_loss": 5.348296642303467
    },
    {
      "epoch": 0.7715989159891599,
      "step": 3559,
      "training_loss": 5.6196675300598145
    },
    {
      "epoch": 0.7715989159891599,
      "step": 3559,
      "training_loss": 5.770678997039795
    },
    {
      "epoch": 0.7715989159891599,
      "step": 3559,
      "training_loss": 6.603812217712402
    },
    {
      "epoch": 0.7718157181571815,
      "grad_norm": 16.32785415649414,
      "learning_rate": 1e-05,
      "loss": 6.1679,
      "step": 3560
    },
    {
      "epoch": 0.7718157181571815,
      "step": 3560,
      "training_loss": 7.274866104125977
    },
    {
      "epoch": 0.7718157181571815,
      "step": 3560,
      "training_loss": 5.863570690155029
    },
    {
      "epoch": 0.7718157181571815,
      "step": 3560,
      "training_loss": 5.508697986602783
    },
    {
      "epoch": 0.7718157181571815,
      "step": 3560,
      "training_loss": 5.741319179534912
    },
    {
      "epoch": 0.7720325203252032,
      "step": 3561,
      "training_loss": 7.098979473114014
    },
    {
      "epoch": 0.7720325203252032,
      "step": 3561,
      "training_loss": 5.543086528778076
    },
    {
      "epoch": 0.7720325203252032,
      "step": 3561,
      "training_loss": 5.587019443511963
    },
    {
      "epoch": 0.7720325203252032,
      "step": 3561,
      "training_loss": 6.619823455810547
    },
    {
      "epoch": 0.772249322493225,
      "step": 3562,
      "training_loss": 6.485118865966797
    },
    {
      "epoch": 0.772249322493225,
      "step": 3562,
      "training_loss": 6.400032997131348
    },
    {
      "epoch": 0.772249322493225,
      "step": 3562,
      "training_loss": 7.245418548583984
    },
    {
      "epoch": 0.772249322493225,
      "step": 3562,
      "training_loss": 6.561097621917725
    },
    {
      "epoch": 0.7724661246612466,
      "step": 3563,
      "training_loss": 6.4724602699279785
    },
    {
      "epoch": 0.7724661246612466,
      "step": 3563,
      "training_loss": 7.6672444343566895
    },
    {
      "epoch": 0.7724661246612466,
      "step": 3563,
      "training_loss": 6.914877414703369
    },
    {
      "epoch": 0.7724661246612466,
      "step": 3563,
      "training_loss": 7.100204944610596
    },
    {
      "epoch": 0.7726829268292683,
      "grad_norm": 27.179595947265625,
      "learning_rate": 1e-05,
      "loss": 6.5052,
      "step": 3564
    },
    {
      "epoch": 0.7726829268292683,
      "step": 3564,
      "training_loss": 6.274835109710693
    },
    {
      "epoch": 0.7726829268292683,
      "step": 3564,
      "training_loss": 4.3278374671936035
    },
    {
      "epoch": 0.7726829268292683,
      "step": 3564,
      "training_loss": 6.551324844360352
    },
    {
      "epoch": 0.7726829268292683,
      "step": 3564,
      "training_loss": 7.03248929977417
    },
    {
      "epoch": 0.77289972899729,
      "step": 3565,
      "training_loss": 6.126799583435059
    },
    {
      "epoch": 0.77289972899729,
      "step": 3565,
      "training_loss": 5.492644309997559
    },
    {
      "epoch": 0.77289972899729,
      "step": 3565,
      "training_loss": 6.950872421264648
    },
    {
      "epoch": 0.77289972899729,
      "step": 3565,
      "training_loss": 5.939738750457764
    },
    {
      "epoch": 0.7731165311653116,
      "step": 3566,
      "training_loss": 7.213252544403076
    },
    {
      "epoch": 0.7731165311653116,
      "step": 3566,
      "training_loss": 6.292171001434326
    },
    {
      "epoch": 0.7731165311653116,
      "step": 3566,
      "training_loss": 6.744418144226074
    },
    {
      "epoch": 0.7731165311653116,
      "step": 3566,
      "training_loss": 6.191269397735596
    },
    {
      "epoch": 0.7733333333333333,
      "step": 3567,
      "training_loss": 6.160415172576904
    },
    {
      "epoch": 0.7733333333333333,
      "step": 3567,
      "training_loss": 6.472530841827393
    },
    {
      "epoch": 0.7733333333333333,
      "step": 3567,
      "training_loss": 6.491631984710693
    },
    {
      "epoch": 0.7733333333333333,
      "step": 3567,
      "training_loss": 6.403817653656006
    },
    {
      "epoch": 0.773550135501355,
      "grad_norm": 14.878255844116211,
      "learning_rate": 1e-05,
      "loss": 6.2916,
      "step": 3568
    },
    {
      "epoch": 0.773550135501355,
      "step": 3568,
      "training_loss": 6.628126621246338
    },
    {
      "epoch": 0.773550135501355,
      "step": 3568,
      "training_loss": 6.562410831451416
    },
    {
      "epoch": 0.773550135501355,
      "step": 3568,
      "training_loss": 5.002694129943848
    },
    {
      "epoch": 0.773550135501355,
      "step": 3568,
      "training_loss": 7.360876083374023
    },
    {
      "epoch": 0.7737669376693767,
      "step": 3569,
      "training_loss": 4.837225437164307
    },
    {
      "epoch": 0.7737669376693767,
      "step": 3569,
      "training_loss": 7.390478610992432
    },
    {
      "epoch": 0.7737669376693767,
      "step": 3569,
      "training_loss": 6.45068359375
    },
    {
      "epoch": 0.7737669376693767,
      "step": 3569,
      "training_loss": 6.67281436920166
    },
    {
      "epoch": 0.7739837398373983,
      "step": 3570,
      "training_loss": 5.721229076385498
    },
    {
      "epoch": 0.7739837398373983,
      "step": 3570,
      "training_loss": 6.461109638214111
    },
    {
      "epoch": 0.7739837398373983,
      "step": 3570,
      "training_loss": 3.3109259605407715
    },
    {
      "epoch": 0.7739837398373983,
      "step": 3570,
      "training_loss": 7.18277645111084
    },
    {
      "epoch": 0.7742005420054201,
      "step": 3571,
      "training_loss": 5.0698747634887695
    },
    {
      "epoch": 0.7742005420054201,
      "step": 3571,
      "training_loss": 7.281612396240234
    },
    {
      "epoch": 0.7742005420054201,
      "step": 3571,
      "training_loss": 6.988448619842529
    },
    {
      "epoch": 0.7742005420054201,
      "step": 3571,
      "training_loss": 7.36269998550415
    },
    {
      "epoch": 0.7744173441734418,
      "grad_norm": 19.290660858154297,
      "learning_rate": 1e-05,
      "loss": 6.2677,
      "step": 3572
    },
    {
      "epoch": 0.7744173441734418,
      "step": 3572,
      "training_loss": 6.575263023376465
    },
    {
      "epoch": 0.7744173441734418,
      "step": 3572,
      "training_loss": 5.100119113922119
    },
    {
      "epoch": 0.7744173441734418,
      "step": 3572,
      "training_loss": 3.13097882270813
    },
    {
      "epoch": 0.7744173441734418,
      "step": 3572,
      "training_loss": 7.005729675292969
    },
    {
      "epoch": 0.7746341463414634,
      "step": 3573,
      "training_loss": 7.494854927062988
    },
    {
      "epoch": 0.7746341463414634,
      "step": 3573,
      "training_loss": 4.680800437927246
    },
    {
      "epoch": 0.7746341463414634,
      "step": 3573,
      "training_loss": 5.368433952331543
    },
    {
      "epoch": 0.7746341463414634,
      "step": 3573,
      "training_loss": 7.9509968757629395
    },
    {
      "epoch": 0.7748509485094851,
      "step": 3574,
      "training_loss": 6.405332088470459
    },
    {
      "epoch": 0.7748509485094851,
      "step": 3574,
      "training_loss": 6.358992576599121
    },
    {
      "epoch": 0.7748509485094851,
      "step": 3574,
      "training_loss": 7.328070640563965
    },
    {
      "epoch": 0.7748509485094851,
      "step": 3574,
      "training_loss": 7.374020576477051
    },
    {
      "epoch": 0.7750677506775068,
      "step": 3575,
      "training_loss": 8.180303573608398
    },
    {
      "epoch": 0.7750677506775068,
      "step": 3575,
      "training_loss": 6.374926567077637
    },
    {
      "epoch": 0.7750677506775068,
      "step": 3575,
      "training_loss": 7.527247428894043
    },
    {
      "epoch": 0.7750677506775068,
      "step": 3575,
      "training_loss": 6.268527507781982
    },
    {
      "epoch": 0.7752845528455284,
      "grad_norm": 20.349599838256836,
      "learning_rate": 1e-05,
      "loss": 6.4453,
      "step": 3576
    },
    {
      "epoch": 0.7752845528455284,
      "step": 3576,
      "training_loss": 6.430324554443359
    },
    {
      "epoch": 0.7752845528455284,
      "step": 3576,
      "training_loss": 5.533212661743164
    },
    {
      "epoch": 0.7752845528455284,
      "step": 3576,
      "training_loss": 6.51989221572876
    },
    {
      "epoch": 0.7752845528455284,
      "step": 3576,
      "training_loss": 7.004970073699951
    },
    {
      "epoch": 0.7755013550135501,
      "step": 3577,
      "training_loss": 5.8481621742248535
    },
    {
      "epoch": 0.7755013550135501,
      "step": 3577,
      "training_loss": 7.911037921905518
    },
    {
      "epoch": 0.7755013550135501,
      "step": 3577,
      "training_loss": 5.813507556915283
    },
    {
      "epoch": 0.7755013550135501,
      "step": 3577,
      "training_loss": 7.507632732391357
    },
    {
      "epoch": 0.7757181571815718,
      "step": 3578,
      "training_loss": 7.0717620849609375
    },
    {
      "epoch": 0.7757181571815718,
      "step": 3578,
      "training_loss": 6.943517208099365
    },
    {
      "epoch": 0.7757181571815718,
      "step": 3578,
      "training_loss": 6.178586959838867
    },
    {
      "epoch": 0.7757181571815718,
      "step": 3578,
      "training_loss": 7.408079624176025
    },
    {
      "epoch": 0.7759349593495934,
      "step": 3579,
      "training_loss": 6.919344902038574
    },
    {
      "epoch": 0.7759349593495934,
      "step": 3579,
      "training_loss": 6.360231876373291
    },
    {
      "epoch": 0.7759349593495934,
      "step": 3579,
      "training_loss": 8.238214492797852
    },
    {
      "epoch": 0.7759349593495934,
      "step": 3579,
      "training_loss": 3.8120107650756836
    },
    {
      "epoch": 0.7761517615176152,
      "grad_norm": 19.193344116210938,
      "learning_rate": 1e-05,
      "loss": 6.5938,
      "step": 3580
    },
    {
      "epoch": 0.7761517615176152,
      "step": 3580,
      "training_loss": 6.532961845397949
    },
    {
      "epoch": 0.7761517615176152,
      "step": 3580,
      "training_loss": 6.617725372314453
    },
    {
      "epoch": 0.7761517615176152,
      "step": 3580,
      "training_loss": 6.1646623611450195
    },
    {
      "epoch": 0.7761517615176152,
      "step": 3580,
      "training_loss": 5.948934555053711
    },
    {
      "epoch": 0.7763685636856369,
      "step": 3581,
      "training_loss": 5.952119827270508
    },
    {
      "epoch": 0.7763685636856369,
      "step": 3581,
      "training_loss": 4.430017471313477
    },
    {
      "epoch": 0.7763685636856369,
      "step": 3581,
      "training_loss": 6.603774547576904
    },
    {
      "epoch": 0.7763685636856369,
      "step": 3581,
      "training_loss": 6.459158897399902
    },
    {
      "epoch": 0.7765853658536586,
      "step": 3582,
      "training_loss": 6.316035747528076
    },
    {
      "epoch": 0.7765853658536586,
      "step": 3582,
      "training_loss": 5.925810813903809
    },
    {
      "epoch": 0.7765853658536586,
      "step": 3582,
      "training_loss": 6.749171257019043
    },
    {
      "epoch": 0.7765853658536586,
      "step": 3582,
      "training_loss": 7.19630241394043
    },
    {
      "epoch": 0.7768021680216802,
      "step": 3583,
      "training_loss": 6.650968551635742
    },
    {
      "epoch": 0.7768021680216802,
      "step": 3583,
      "training_loss": 6.798117160797119
    },
    {
      "epoch": 0.7768021680216802,
      "step": 3583,
      "training_loss": 7.230579853057861
    },
    {
      "epoch": 0.7768021680216802,
      "step": 3583,
      "training_loss": 4.644896984100342
    },
    {
      "epoch": 0.7770189701897019,
      "grad_norm": 16.652877807617188,
      "learning_rate": 1e-05,
      "loss": 6.2638,
      "step": 3584
    },
    {
      "epoch": 0.7770189701897019,
      "step": 3584,
      "training_loss": 6.344520092010498
    },
    {
      "epoch": 0.7770189701897019,
      "step": 3584,
      "training_loss": 7.397833824157715
    },
    {
      "epoch": 0.7770189701897019,
      "step": 3584,
      "training_loss": 7.139312744140625
    },
    {
      "epoch": 0.7770189701897019,
      "step": 3584,
      "training_loss": 5.036968231201172
    },
    {
      "epoch": 0.7772357723577236,
      "step": 3585,
      "training_loss": 6.821638584136963
    },
    {
      "epoch": 0.7772357723577236,
      "step": 3585,
      "training_loss": 5.708902835845947
    },
    {
      "epoch": 0.7772357723577236,
      "step": 3585,
      "training_loss": 7.3850836753845215
    },
    {
      "epoch": 0.7772357723577236,
      "step": 3585,
      "training_loss": 5.684340953826904
    },
    {
      "epoch": 0.7774525745257452,
      "step": 3586,
      "training_loss": 7.257350921630859
    },
    {
      "epoch": 0.7774525745257452,
      "step": 3586,
      "training_loss": 6.88242769241333
    },
    {
      "epoch": 0.7774525745257452,
      "step": 3586,
      "training_loss": 6.969080448150635
    },
    {
      "epoch": 0.7774525745257452,
      "step": 3586,
      "training_loss": 7.2874932289123535
    },
    {
      "epoch": 0.7776693766937669,
      "step": 3587,
      "training_loss": 5.641852855682373
    },
    {
      "epoch": 0.7776693766937669,
      "step": 3587,
      "training_loss": 6.995750904083252
    },
    {
      "epoch": 0.7776693766937669,
      "step": 3587,
      "training_loss": 5.524806022644043
    },
    {
      "epoch": 0.7776693766937669,
      "step": 3587,
      "training_loss": 3.6149065494537354
    },
    {
      "epoch": 0.7778861788617886,
      "grad_norm": 26.20203971862793,
      "learning_rate": 1e-05,
      "loss": 6.3558,
      "step": 3588
    },
    {
      "epoch": 0.7778861788617886,
      "step": 3588,
      "training_loss": 6.1574907302856445
    },
    {
      "epoch": 0.7778861788617886,
      "step": 3588,
      "training_loss": 5.971794605255127
    },
    {
      "epoch": 0.7778861788617886,
      "step": 3588,
      "training_loss": 6.389451503753662
    },
    {
      "epoch": 0.7778861788617886,
      "step": 3588,
      "training_loss": 4.729276180267334
    },
    {
      "epoch": 0.7781029810298103,
      "step": 3589,
      "training_loss": 7.359691143035889
    },
    {
      "epoch": 0.7781029810298103,
      "step": 3589,
      "training_loss": 7.352363109588623
    },
    {
      "epoch": 0.7781029810298103,
      "step": 3589,
      "training_loss": 8.78589916229248
    },
    {
      "epoch": 0.7781029810298103,
      "step": 3589,
      "training_loss": 8.444306373596191
    },
    {
      "epoch": 0.778319783197832,
      "step": 3590,
      "training_loss": 6.303359031677246
    },
    {
      "epoch": 0.778319783197832,
      "step": 3590,
      "training_loss": 8.29377555847168
    },
    {
      "epoch": 0.778319783197832,
      "step": 3590,
      "training_loss": 7.090322971343994
    },
    {
      "epoch": 0.778319783197832,
      "step": 3590,
      "training_loss": 5.977246284484863
    },
    {
      "epoch": 0.7785365853658537,
      "step": 3591,
      "training_loss": 4.446948528289795
    },
    {
      "epoch": 0.7785365853658537,
      "step": 3591,
      "training_loss": 6.3982133865356445
    },
    {
      "epoch": 0.7785365853658537,
      "step": 3591,
      "training_loss": 5.0709123611450195
    },
    {
      "epoch": 0.7785365853658537,
      "step": 3591,
      "training_loss": 5.700871467590332
    },
    {
      "epoch": 0.7787533875338754,
      "grad_norm": 16.495258331298828,
      "learning_rate": 1e-05,
      "loss": 6.5295,
      "step": 3592
    },
    {
      "epoch": 0.7787533875338754,
      "step": 3592,
      "training_loss": 5.222053527832031
    },
    {
      "epoch": 0.7787533875338754,
      "step": 3592,
      "training_loss": 6.563617706298828
    },
    {
      "epoch": 0.7787533875338754,
      "step": 3592,
      "training_loss": 6.372398376464844
    },
    {
      "epoch": 0.7787533875338754,
      "step": 3592,
      "training_loss": 4.479335784912109
    },
    {
      "epoch": 0.778970189701897,
      "step": 3593,
      "training_loss": 7.610037803649902
    },
    {
      "epoch": 0.778970189701897,
      "step": 3593,
      "training_loss": 5.18604040145874
    },
    {
      "epoch": 0.778970189701897,
      "step": 3593,
      "training_loss": 7.059323787689209
    },
    {
      "epoch": 0.778970189701897,
      "step": 3593,
      "training_loss": 8.634018898010254
    },
    {
      "epoch": 0.7791869918699187,
      "step": 3594,
      "training_loss": 4.292536735534668
    },
    {
      "epoch": 0.7791869918699187,
      "step": 3594,
      "training_loss": 7.071461200714111
    },
    {
      "epoch": 0.7791869918699187,
      "step": 3594,
      "training_loss": 5.037309169769287
    },
    {
      "epoch": 0.7791869918699187,
      "step": 3594,
      "training_loss": 6.890964508056641
    },
    {
      "epoch": 0.7794037940379404,
      "step": 3595,
      "training_loss": 6.861600399017334
    },
    {
      "epoch": 0.7794037940379404,
      "step": 3595,
      "training_loss": 6.66118860244751
    },
    {
      "epoch": 0.7794037940379404,
      "step": 3595,
      "training_loss": 6.2342305183410645
    },
    {
      "epoch": 0.7794037940379404,
      "step": 3595,
      "training_loss": 5.821092128753662
    },
    {
      "epoch": 0.779620596205962,
      "grad_norm": 14.870145797729492,
      "learning_rate": 1e-05,
      "loss": 6.2498,
      "step": 3596
    },
    {
      "epoch": 0.779620596205962,
      "step": 3596,
      "training_loss": 7.468728542327881
    },
    {
      "epoch": 0.779620596205962,
      "step": 3596,
      "training_loss": 6.861868381500244
    },
    {
      "epoch": 0.779620596205962,
      "step": 3596,
      "training_loss": 6.7924346923828125
    },
    {
      "epoch": 0.779620596205962,
      "step": 3596,
      "training_loss": 6.361454010009766
    },
    {
      "epoch": 0.7798373983739837,
      "step": 3597,
      "training_loss": 6.9089274406433105
    },
    {
      "epoch": 0.7798373983739837,
      "step": 3597,
      "training_loss": 7.011486530303955
    },
    {
      "epoch": 0.7798373983739837,
      "step": 3597,
      "training_loss": 6.565258979797363
    },
    {
      "epoch": 0.7798373983739837,
      "step": 3597,
      "training_loss": 6.662349224090576
    },
    {
      "epoch": 0.7800542005420055,
      "step": 3598,
      "training_loss": 6.697956562042236
    },
    {
      "epoch": 0.7800542005420055,
      "step": 3598,
      "training_loss": 7.452539920806885
    },
    {
      "epoch": 0.7800542005420055,
      "step": 3598,
      "training_loss": 7.07975435256958
    },
    {
      "epoch": 0.7800542005420055,
      "step": 3598,
      "training_loss": 7.1271867752075195
    },
    {
      "epoch": 0.7802710027100271,
      "step": 3599,
      "training_loss": 6.919548511505127
    },
    {
      "epoch": 0.7802710027100271,
      "step": 3599,
      "training_loss": 3.726191282272339
    },
    {
      "epoch": 0.7802710027100271,
      "step": 3599,
      "training_loss": 5.659448623657227
    },
    {
      "epoch": 0.7802710027100271,
      "step": 3599,
      "training_loss": 3.637359619140625
    },
    {
      "epoch": 0.7804878048780488,
      "grad_norm": 19.31475257873535,
      "learning_rate": 1e-05,
      "loss": 6.4333,
      "step": 3600
    },
    {
      "epoch": 0.7804878048780488,
      "step": 3600,
      "training_loss": 7.166961193084717
    },
    {
      "epoch": 0.7804878048780488,
      "step": 3600,
      "training_loss": 7.68213415145874
    },
    {
      "epoch": 0.7804878048780488,
      "step": 3600,
      "training_loss": 6.6017632484436035
    },
    {
      "epoch": 0.7804878048780488,
      "step": 3600,
      "training_loss": 5.961341857910156
    },
    {
      "epoch": 0.7807046070460705,
      "step": 3601,
      "training_loss": 5.918859481811523
    },
    {
      "epoch": 0.7807046070460705,
      "step": 3601,
      "training_loss": 6.713163375854492
    },
    {
      "epoch": 0.7807046070460705,
      "step": 3601,
      "training_loss": 7.131032943725586
    },
    {
      "epoch": 0.7807046070460705,
      "step": 3601,
      "training_loss": 6.065473556518555
    },
    {
      "epoch": 0.7809214092140921,
      "step": 3602,
      "training_loss": 7.251758098602295
    },
    {
      "epoch": 0.7809214092140921,
      "step": 3602,
      "training_loss": 7.278058052062988
    },
    {
      "epoch": 0.7809214092140921,
      "step": 3602,
      "training_loss": 6.7974042892456055
    },
    {
      "epoch": 0.7809214092140921,
      "step": 3602,
      "training_loss": 6.736867904663086
    },
    {
      "epoch": 0.7811382113821138,
      "step": 3603,
      "training_loss": 4.795048236846924
    },
    {
      "epoch": 0.7811382113821138,
      "step": 3603,
      "training_loss": 6.98832368850708
    },
    {
      "epoch": 0.7811382113821138,
      "step": 3603,
      "training_loss": 9.507747650146484
    },
    {
      "epoch": 0.7811382113821138,
      "step": 3603,
      "training_loss": 4.50685977935791
    },
    {
      "epoch": 0.7813550135501355,
      "grad_norm": 25.293941497802734,
      "learning_rate": 1e-05,
      "loss": 6.6939,
      "step": 3604
    },
    {
      "epoch": 0.7813550135501355,
      "step": 3604,
      "training_loss": 4.349668502807617
    },
    {
      "epoch": 0.7813550135501355,
      "step": 3604,
      "training_loss": 7.628188133239746
    },
    {
      "epoch": 0.7813550135501355,
      "step": 3604,
      "training_loss": 5.713718891143799
    },
    {
      "epoch": 0.7813550135501355,
      "step": 3604,
      "training_loss": 6.487022399902344
    },
    {
      "epoch": 0.7815718157181571,
      "step": 3605,
      "training_loss": 4.420236587524414
    },
    {
      "epoch": 0.7815718157181571,
      "step": 3605,
      "training_loss": 7.023375034332275
    },
    {
      "epoch": 0.7815718157181571,
      "step": 3605,
      "training_loss": 6.271474361419678
    },
    {
      "epoch": 0.7815718157181571,
      "step": 3605,
      "training_loss": 6.842092037200928
    },
    {
      "epoch": 0.7817886178861788,
      "step": 3606,
      "training_loss": 6.023326396942139
    },
    {
      "epoch": 0.7817886178861788,
      "step": 3606,
      "training_loss": 7.397937297821045
    },
    {
      "epoch": 0.7817886178861788,
      "step": 3606,
      "training_loss": 7.050869941711426
    },
    {
      "epoch": 0.7817886178861788,
      "step": 3606,
      "training_loss": 7.254323482513428
    },
    {
      "epoch": 0.7820054200542006,
      "step": 3607,
      "training_loss": 6.726019382476807
    },
    {
      "epoch": 0.7820054200542006,
      "step": 3607,
      "training_loss": 6.31794548034668
    },
    {
      "epoch": 0.7820054200542006,
      "step": 3607,
      "training_loss": 7.694000244140625
    },
    {
      "epoch": 0.7820054200542006,
      "step": 3607,
      "training_loss": 6.865993976593018
    },
    {
      "epoch": 0.7822222222222223,
      "grad_norm": 20.92650604248047,
      "learning_rate": 1e-05,
      "loss": 6.5041,
      "step": 3608
    },
    {
      "epoch": 0.7822222222222223,
      "step": 3608,
      "training_loss": 8.052925109863281
    },
    {
      "epoch": 0.7822222222222223,
      "step": 3608,
      "training_loss": 6.566828727722168
    },
    {
      "epoch": 0.7822222222222223,
      "step": 3608,
      "training_loss": 5.752450466156006
    },
    {
      "epoch": 0.7822222222222223,
      "step": 3608,
      "training_loss": 5.7477569580078125
    },
    {
      "epoch": 0.7824390243902439,
      "step": 3609,
      "training_loss": 7.233025550842285
    },
    {
      "epoch": 0.7824390243902439,
      "step": 3609,
      "training_loss": 6.9072585105896
    },
    {
      "epoch": 0.7824390243902439,
      "step": 3609,
      "training_loss": 4.781385898590088
    },
    {
      "epoch": 0.7824390243902439,
      "step": 3609,
      "training_loss": 6.6421217918396
    },
    {
      "epoch": 0.7826558265582656,
      "step": 3610,
      "training_loss": 5.307546138763428
    },
    {
      "epoch": 0.7826558265582656,
      "step": 3610,
      "training_loss": 5.395646095275879
    },
    {
      "epoch": 0.7826558265582656,
      "step": 3610,
      "training_loss": 6.1869635581970215
    },
    {
      "epoch": 0.7826558265582656,
      "step": 3610,
      "training_loss": 7.431750774383545
    },
    {
      "epoch": 0.7828726287262873,
      "step": 3611,
      "training_loss": 6.206840991973877
    },
    {
      "epoch": 0.7828726287262873,
      "step": 3611,
      "training_loss": 6.00736665725708
    },
    {
      "epoch": 0.7828726287262873,
      "step": 3611,
      "training_loss": 6.3377509117126465
    },
    {
      "epoch": 0.7828726287262873,
      "step": 3611,
      "training_loss": 6.94503116607666
    },
    {
      "epoch": 0.7830894308943089,
      "grad_norm": 21.18342399597168,
      "learning_rate": 1e-05,
      "loss": 6.3439,
      "step": 3612
    },
    {
      "epoch": 0.7830894308943089,
      "step": 3612,
      "training_loss": 6.8699421882629395
    },
    {
      "epoch": 0.7830894308943089,
      "step": 3612,
      "training_loss": 7.13023042678833
    },
    {
      "epoch": 0.7830894308943089,
      "step": 3612,
      "training_loss": 4.6922736167907715
    },
    {
      "epoch": 0.7830894308943089,
      "step": 3612,
      "training_loss": 7.902705669403076
    },
    {
      "epoch": 0.7833062330623306,
      "step": 3613,
      "training_loss": 6.087738037109375
    },
    {
      "epoch": 0.7833062330623306,
      "step": 3613,
      "training_loss": 4.355414867401123
    },
    {
      "epoch": 0.7833062330623306,
      "step": 3613,
      "training_loss": 7.057892322540283
    },
    {
      "epoch": 0.7833062330623306,
      "step": 3613,
      "training_loss": 5.270009517669678
    },
    {
      "epoch": 0.7835230352303523,
      "step": 3614,
      "training_loss": 5.8408942222595215
    },
    {
      "epoch": 0.7835230352303523,
      "step": 3614,
      "training_loss": 6.278680801391602
    },
    {
      "epoch": 0.7835230352303523,
      "step": 3614,
      "training_loss": 3.472041130065918
    },
    {
      "epoch": 0.7835230352303523,
      "step": 3614,
      "training_loss": 5.960662841796875
    },
    {
      "epoch": 0.7837398373983739,
      "step": 3615,
      "training_loss": 6.417697429656982
    },
    {
      "epoch": 0.7837398373983739,
      "step": 3615,
      "training_loss": 7.333220958709717
    },
    {
      "epoch": 0.7837398373983739,
      "step": 3615,
      "training_loss": 4.975172519683838
    },
    {
      "epoch": 0.7837398373983739,
      "step": 3615,
      "training_loss": 6.218928813934326
    },
    {
      "epoch": 0.7839566395663957,
      "grad_norm": 22.096223831176758,
      "learning_rate": 1e-05,
      "loss": 5.9915,
      "step": 3616
    },
    {
      "epoch": 0.7839566395663957,
      "step": 3616,
      "training_loss": 5.744616508483887
    },
    {
      "epoch": 0.7839566395663957,
      "step": 3616,
      "training_loss": 7.212321758270264
    },
    {
      "epoch": 0.7839566395663957,
      "step": 3616,
      "training_loss": 6.4206862449646
    },
    {
      "epoch": 0.7839566395663957,
      "step": 3616,
      "training_loss": 6.4445366859436035
    },
    {
      "epoch": 0.7841734417344174,
      "step": 3617,
      "training_loss": 3.6942057609558105
    },
    {
      "epoch": 0.7841734417344174,
      "step": 3617,
      "training_loss": 4.003133773803711
    },
    {
      "epoch": 0.7841734417344174,
      "step": 3617,
      "training_loss": 5.811816692352295
    },
    {
      "epoch": 0.7841734417344174,
      "step": 3617,
      "training_loss": 6.48128604888916
    },
    {
      "epoch": 0.784390243902439,
      "step": 3618,
      "training_loss": 7.617480278015137
    },
    {
      "epoch": 0.784390243902439,
      "step": 3618,
      "training_loss": 6.677628517150879
    },
    {
      "epoch": 0.784390243902439,
      "step": 3618,
      "training_loss": 6.930023193359375
    },
    {
      "epoch": 0.784390243902439,
      "step": 3618,
      "training_loss": 7.025407791137695
    },
    {
      "epoch": 0.7846070460704607,
      "step": 3619,
      "training_loss": 7.068282604217529
    },
    {
      "epoch": 0.7846070460704607,
      "step": 3619,
      "training_loss": 8.282001495361328
    },
    {
      "epoch": 0.7846070460704607,
      "step": 3619,
      "training_loss": 5.724961757659912
    },
    {
      "epoch": 0.7846070460704607,
      "step": 3619,
      "training_loss": 6.385249614715576
    },
    {
      "epoch": 0.7848238482384824,
      "grad_norm": 19.02619171142578,
      "learning_rate": 1e-05,
      "loss": 6.3452,
      "step": 3620
    },
    {
      "epoch": 0.7848238482384824,
      "step": 3620,
      "training_loss": 8.781258583068848
    },
    {
      "epoch": 0.7848238482384824,
      "step": 3620,
      "training_loss": 6.85418176651001
    },
    {
      "epoch": 0.7848238482384824,
      "step": 3620,
      "training_loss": 5.987600803375244
    },
    {
      "epoch": 0.7848238482384824,
      "step": 3620,
      "training_loss": 7.986309051513672
    },
    {
      "epoch": 0.7850406504065041,
      "step": 3621,
      "training_loss": 6.401426792144775
    },
    {
      "epoch": 0.7850406504065041,
      "step": 3621,
      "training_loss": 6.546215057373047
    },
    {
      "epoch": 0.7850406504065041,
      "step": 3621,
      "training_loss": 7.9040303230285645
    },
    {
      "epoch": 0.7850406504065041,
      "step": 3621,
      "training_loss": 5.698601722717285
    },
    {
      "epoch": 0.7852574525745257,
      "step": 3622,
      "training_loss": 7.269991874694824
    },
    {
      "epoch": 0.7852574525745257,
      "step": 3622,
      "training_loss": 7.029857158660889
    },
    {
      "epoch": 0.7852574525745257,
      "step": 3622,
      "training_loss": 5.853936672210693
    },
    {
      "epoch": 0.7852574525745257,
      "step": 3622,
      "training_loss": 7.562006950378418
    },
    {
      "epoch": 0.7854742547425474,
      "step": 3623,
      "training_loss": 6.854172229766846
    },
    {
      "epoch": 0.7854742547425474,
      "step": 3623,
      "training_loss": 7.34106969833374
    },
    {
      "epoch": 0.7854742547425474,
      "step": 3623,
      "training_loss": 5.274292469024658
    },
    {
      "epoch": 0.7854742547425474,
      "step": 3623,
      "training_loss": 5.8550567626953125
    },
    {
      "epoch": 0.7856910569105691,
      "grad_norm": 22.073013305664062,
      "learning_rate": 1e-05,
      "loss": 6.825,
      "step": 3624
    },
    {
      "epoch": 0.7856910569105691,
      "step": 3624,
      "training_loss": 7.515366554260254
    },
    {
      "epoch": 0.7856910569105691,
      "step": 3624,
      "training_loss": 5.5085577964782715
    },
    {
      "epoch": 0.7856910569105691,
      "step": 3624,
      "training_loss": 5.8312225341796875
    },
    {
      "epoch": 0.7856910569105691,
      "step": 3624,
      "training_loss": 7.168250560760498
    },
    {
      "epoch": 0.7859078590785907,
      "step": 3625,
      "training_loss": 6.414994716644287
    },
    {
      "epoch": 0.7859078590785907,
      "step": 3625,
      "training_loss": 6.28196382522583
    },
    {
      "epoch": 0.7859078590785907,
      "step": 3625,
      "training_loss": 6.217040538787842
    },
    {
      "epoch": 0.7859078590785907,
      "step": 3625,
      "training_loss": 6.748696327209473
    },
    {
      "epoch": 0.7861246612466125,
      "step": 3626,
      "training_loss": 6.526419639587402
    },
    {
      "epoch": 0.7861246612466125,
      "step": 3626,
      "training_loss": 7.049959659576416
    },
    {
      "epoch": 0.7861246612466125,
      "step": 3626,
      "training_loss": 7.285036087036133
    },
    {
      "epoch": 0.7861246612466125,
      "step": 3626,
      "training_loss": 3.9289867877960205
    },
    {
      "epoch": 0.7863414634146342,
      "step": 3627,
      "training_loss": 6.43833065032959
    },
    {
      "epoch": 0.7863414634146342,
      "step": 3627,
      "training_loss": 3.557234048843384
    },
    {
      "epoch": 0.7863414634146342,
      "step": 3627,
      "training_loss": 7.297059535980225
    },
    {
      "epoch": 0.7863414634146342,
      "step": 3627,
      "training_loss": 5.432839870452881
    },
    {
      "epoch": 0.7865582655826558,
      "grad_norm": 15.043268203735352,
      "learning_rate": 1e-05,
      "loss": 6.2001,
      "step": 3628
    },
    {
      "epoch": 0.7865582655826558,
      "step": 3628,
      "training_loss": 7.114760398864746
    },
    {
      "epoch": 0.7865582655826558,
      "step": 3628,
      "training_loss": 5.28565788269043
    },
    {
      "epoch": 0.7865582655826558,
      "step": 3628,
      "training_loss": 5.520939350128174
    },
    {
      "epoch": 0.7865582655826558,
      "step": 3628,
      "training_loss": 6.553970813751221
    },
    {
      "epoch": 0.7867750677506775,
      "step": 3629,
      "training_loss": 7.242199420928955
    },
    {
      "epoch": 0.7867750677506775,
      "step": 3629,
      "training_loss": 5.744128227233887
    },
    {
      "epoch": 0.7867750677506775,
      "step": 3629,
      "training_loss": 4.278885364532471
    },
    {
      "epoch": 0.7867750677506775,
      "step": 3629,
      "training_loss": 6.8316144943237305
    },
    {
      "epoch": 0.7869918699186992,
      "step": 3630,
      "training_loss": 8.500894546508789
    },
    {
      "epoch": 0.7869918699186992,
      "step": 3630,
      "training_loss": 5.5681538581848145
    },
    {
      "epoch": 0.7869918699186992,
      "step": 3630,
      "training_loss": 6.280437469482422
    },
    {
      "epoch": 0.7869918699186992,
      "step": 3630,
      "training_loss": 6.003224849700928
    },
    {
      "epoch": 0.7872086720867209,
      "step": 3631,
      "training_loss": 7.4729132652282715
    },
    {
      "epoch": 0.7872086720867209,
      "step": 3631,
      "training_loss": 6.985963344573975
    },
    {
      "epoch": 0.7872086720867209,
      "step": 3631,
      "training_loss": 7.461423397064209
    },
    {
      "epoch": 0.7872086720867209,
      "step": 3631,
      "training_loss": 7.397418975830078
    },
    {
      "epoch": 0.7874254742547425,
      "grad_norm": 15.513459205627441,
      "learning_rate": 1e-05,
      "loss": 6.5152,
      "step": 3632
    },
    {
      "epoch": 0.7874254742547425,
      "step": 3632,
      "training_loss": 6.902553081512451
    },
    {
      "epoch": 0.7874254742547425,
      "step": 3632,
      "training_loss": 5.1784749031066895
    },
    {
      "epoch": 0.7874254742547425,
      "step": 3632,
      "training_loss": 6.099571228027344
    },
    {
      "epoch": 0.7874254742547425,
      "step": 3632,
      "training_loss": 6.617948055267334
    },
    {
      "epoch": 0.7876422764227642,
      "step": 3633,
      "training_loss": 7.025952339172363
    },
    {
      "epoch": 0.7876422764227642,
      "step": 3633,
      "training_loss": 6.679272651672363
    },
    {
      "epoch": 0.7876422764227642,
      "step": 3633,
      "training_loss": 7.492659091949463
    },
    {
      "epoch": 0.7876422764227642,
      "step": 3633,
      "training_loss": 5.679006576538086
    },
    {
      "epoch": 0.7878590785907859,
      "step": 3634,
      "training_loss": 6.155095100402832
    },
    {
      "epoch": 0.7878590785907859,
      "step": 3634,
      "training_loss": 8.096794128417969
    },
    {
      "epoch": 0.7878590785907859,
      "step": 3634,
      "training_loss": 6.618950843811035
    },
    {
      "epoch": 0.7878590785907859,
      "step": 3634,
      "training_loss": 5.630878448486328
    },
    {
      "epoch": 0.7880758807588076,
      "step": 3635,
      "training_loss": 6.73203182220459
    },
    {
      "epoch": 0.7880758807588076,
      "step": 3635,
      "training_loss": 6.027935028076172
    },
    {
      "epoch": 0.7880758807588076,
      "step": 3635,
      "training_loss": 7.215747356414795
    },
    {
      "epoch": 0.7880758807588076,
      "step": 3635,
      "training_loss": 6.771332263946533
    },
    {
      "epoch": 0.7882926829268293,
      "grad_norm": 26.604633331298828,
      "learning_rate": 1e-05,
      "loss": 6.5578,
      "step": 3636
    },
    {
      "epoch": 0.7882926829268293,
      "step": 3636,
      "training_loss": 6.966098308563232
    },
    {
      "epoch": 0.7882926829268293,
      "step": 3636,
      "training_loss": 6.8574395179748535
    },
    {
      "epoch": 0.7882926829268293,
      "step": 3636,
      "training_loss": 7.271031856536865
    },
    {
      "epoch": 0.7882926829268293,
      "step": 3636,
      "training_loss": 7.5140275955200195
    },
    {
      "epoch": 0.788509485094851,
      "step": 3637,
      "training_loss": 6.196837902069092
    },
    {
      "epoch": 0.788509485094851,
      "step": 3637,
      "training_loss": 6.242031097412109
    },
    {
      "epoch": 0.788509485094851,
      "step": 3637,
      "training_loss": 6.949685573577881
    },
    {
      "epoch": 0.788509485094851,
      "step": 3637,
      "training_loss": 5.002203941345215
    },
    {
      "epoch": 0.7887262872628726,
      "step": 3638,
      "training_loss": 4.366823673248291
    },
    {
      "epoch": 0.7887262872628726,
      "step": 3638,
      "training_loss": 6.70656681060791
    },
    {
      "epoch": 0.7887262872628726,
      "step": 3638,
      "training_loss": 6.121940612792969
    },
    {
      "epoch": 0.7887262872628726,
      "step": 3638,
      "training_loss": 6.601750373840332
    },
    {
      "epoch": 0.7889430894308943,
      "step": 3639,
      "training_loss": 7.139235973358154
    },
    {
      "epoch": 0.7889430894308943,
      "step": 3639,
      "training_loss": 7.927853107452393
    },
    {
      "epoch": 0.7889430894308943,
      "step": 3639,
      "training_loss": 6.939610958099365
    },
    {
      "epoch": 0.7889430894308943,
      "step": 3639,
      "training_loss": 6.976593971252441
    },
    {
      "epoch": 0.789159891598916,
      "grad_norm": 17.767656326293945,
      "learning_rate": 1e-05,
      "loss": 6.6112,
      "step": 3640
    },
    {
      "epoch": 0.789159891598916,
      "step": 3640,
      "training_loss": 7.009128570556641
    },
    {
      "epoch": 0.789159891598916,
      "step": 3640,
      "training_loss": 6.6967034339904785
    },
    {
      "epoch": 0.789159891598916,
      "step": 3640,
      "training_loss": 6.03626012802124
    },
    {
      "epoch": 0.789159891598916,
      "step": 3640,
      "training_loss": 5.13599157333374
    },
    {
      "epoch": 0.7893766937669376,
      "step": 3641,
      "training_loss": 7.323640823364258
    },
    {
      "epoch": 0.7893766937669376,
      "step": 3641,
      "training_loss": 5.472011566162109
    },
    {
      "epoch": 0.7893766937669376,
      "step": 3641,
      "training_loss": 5.661805152893066
    },
    {
      "epoch": 0.7893766937669376,
      "step": 3641,
      "training_loss": 7.047693729400635
    },
    {
      "epoch": 0.7895934959349593,
      "step": 3642,
      "training_loss": 6.060185432434082
    },
    {
      "epoch": 0.7895934959349593,
      "step": 3642,
      "training_loss": 7.730987548828125
    },
    {
      "epoch": 0.7895934959349593,
      "step": 3642,
      "training_loss": 6.760748386383057
    },
    {
      "epoch": 0.7895934959349593,
      "step": 3642,
      "training_loss": 5.541802883148193
    },
    {
      "epoch": 0.789810298102981,
      "step": 3643,
      "training_loss": 7.486837863922119
    },
    {
      "epoch": 0.789810298102981,
      "step": 3643,
      "training_loss": 5.492730617523193
    },
    {
      "epoch": 0.789810298102981,
      "step": 3643,
      "training_loss": 6.809880256652832
    },
    {
      "epoch": 0.789810298102981,
      "step": 3643,
      "training_loss": 3.775406837463379
    },
    {
      "epoch": 0.7900271002710028,
      "grad_norm": 15.78346061706543,
      "learning_rate": 1e-05,
      "loss": 6.2526,
      "step": 3644
    },
    {
      "epoch": 0.7900271002710028,
      "step": 3644,
      "training_loss": 5.278811454772949
    },
    {
      "epoch": 0.7900271002710028,
      "step": 3644,
      "training_loss": 7.181197166442871
    },
    {
      "epoch": 0.7900271002710028,
      "step": 3644,
      "training_loss": 5.1104021072387695
    },
    {
      "epoch": 0.7900271002710028,
      "step": 3644,
      "training_loss": 6.863668918609619
    },
    {
      "epoch": 0.7902439024390244,
      "step": 3645,
      "training_loss": 5.300869464874268
    },
    {
      "epoch": 0.7902439024390244,
      "step": 3645,
      "training_loss": 5.976121425628662
    },
    {
      "epoch": 0.7902439024390244,
      "step": 3645,
      "training_loss": 6.435492515563965
    },
    {
      "epoch": 0.7902439024390244,
      "step": 3645,
      "training_loss": 4.065262317657471
    },
    {
      "epoch": 0.7904607046070461,
      "step": 3646,
      "training_loss": 5.961489200592041
    },
    {
      "epoch": 0.7904607046070461,
      "step": 3646,
      "training_loss": 6.378175258636475
    },
    {
      "epoch": 0.7904607046070461,
      "step": 3646,
      "training_loss": 6.407971382141113
    },
    {
      "epoch": 0.7904607046070461,
      "step": 3646,
      "training_loss": 5.61084508895874
    },
    {
      "epoch": 0.7906775067750678,
      "step": 3647,
      "training_loss": 6.026906490325928
    },
    {
      "epoch": 0.7906775067750678,
      "step": 3647,
      "training_loss": 5.9979753494262695
    },
    {
      "epoch": 0.7906775067750678,
      "step": 3647,
      "training_loss": 6.646697998046875
    },
    {
      "epoch": 0.7906775067750678,
      "step": 3647,
      "training_loss": 7.967935085296631
    },
    {
      "epoch": 0.7908943089430894,
      "grad_norm": 18.97150421142578,
      "learning_rate": 1e-05,
      "loss": 6.0756,
      "step": 3648
    },
    {
      "epoch": 0.7908943089430894,
      "step": 3648,
      "training_loss": 7.06239652633667
    },
    {
      "epoch": 0.7908943089430894,
      "step": 3648,
      "training_loss": 6.576308250427246
    },
    {
      "epoch": 0.7908943089430894,
      "step": 3648,
      "training_loss": 6.663920879364014
    },
    {
      "epoch": 0.7908943089430894,
      "step": 3648,
      "training_loss": 4.625157356262207
    },
    {
      "epoch": 0.7911111111111111,
      "step": 3649,
      "training_loss": 7.358879089355469
    },
    {
      "epoch": 0.7911111111111111,
      "step": 3649,
      "training_loss": 5.556941986083984
    },
    {
      "epoch": 0.7911111111111111,
      "step": 3649,
      "training_loss": 7.851799011230469
    },
    {
      "epoch": 0.7911111111111111,
      "step": 3649,
      "training_loss": 6.749706745147705
    },
    {
      "epoch": 0.7913279132791328,
      "step": 3650,
      "training_loss": 8.274544715881348
    },
    {
      "epoch": 0.7913279132791328,
      "step": 3650,
      "training_loss": 5.740067005157471
    },
    {
      "epoch": 0.7913279132791328,
      "step": 3650,
      "training_loss": 5.836526393890381
    },
    {
      "epoch": 0.7913279132791328,
      "step": 3650,
      "training_loss": 4.639317989349365
    },
    {
      "epoch": 0.7915447154471544,
      "step": 3651,
      "training_loss": 6.574657917022705
    },
    {
      "epoch": 0.7915447154471544,
      "step": 3651,
      "training_loss": 6.50887393951416
    },
    {
      "epoch": 0.7915447154471544,
      "step": 3651,
      "training_loss": 6.893324375152588
    },
    {
      "epoch": 0.7915447154471544,
      "step": 3651,
      "training_loss": 8.188608169555664
    },
    {
      "epoch": 0.7917615176151761,
      "grad_norm": 18.646224975585938,
      "learning_rate": 1e-05,
      "loss": 6.5688,
      "step": 3652
    },
    {
      "epoch": 0.7917615176151761,
      "step": 3652,
      "training_loss": 5.64160680770874
    },
    {
      "epoch": 0.7917615176151761,
      "step": 3652,
      "training_loss": 7.420857906341553
    },
    {
      "epoch": 0.7917615176151761,
      "step": 3652,
      "training_loss": 5.7498555183410645
    },
    {
      "epoch": 0.7917615176151761,
      "step": 3652,
      "training_loss": 5.029913902282715
    },
    {
      "epoch": 0.7919783197831979,
      "step": 3653,
      "training_loss": 7.2147345542907715
    },
    {
      "epoch": 0.7919783197831979,
      "step": 3653,
      "training_loss": 6.639739513397217
    },
    {
      "epoch": 0.7919783197831979,
      "step": 3653,
      "training_loss": 6.28700065612793
    },
    {
      "epoch": 0.7919783197831979,
      "step": 3653,
      "training_loss": 7.108222961425781
    },
    {
      "epoch": 0.7921951219512195,
      "step": 3654,
      "training_loss": 7.130067348480225
    },
    {
      "epoch": 0.7921951219512195,
      "step": 3654,
      "training_loss": 6.006209373474121
    },
    {
      "epoch": 0.7921951219512195,
      "step": 3654,
      "training_loss": 7.644200325012207
    },
    {
      "epoch": 0.7921951219512195,
      "step": 3654,
      "training_loss": 6.3733391761779785
    },
    {
      "epoch": 0.7924119241192412,
      "step": 3655,
      "training_loss": 6.083890914916992
    },
    {
      "epoch": 0.7924119241192412,
      "step": 3655,
      "training_loss": 6.849694728851318
    },
    {
      "epoch": 0.7924119241192412,
      "step": 3655,
      "training_loss": 6.326337814331055
    },
    {
      "epoch": 0.7924119241192412,
      "step": 3655,
      "training_loss": 8.167802810668945
    },
    {
      "epoch": 0.7926287262872629,
      "grad_norm": 21.85210418701172,
      "learning_rate": 1e-05,
      "loss": 6.6046,
      "step": 3656
    },
    {
      "epoch": 0.7926287262872629,
      "step": 3656,
      "training_loss": 6.221685409545898
    },
    {
      "epoch": 0.7926287262872629,
      "step": 3656,
      "training_loss": 5.925070285797119
    },
    {
      "epoch": 0.7926287262872629,
      "step": 3656,
      "training_loss": 6.451061725616455
    },
    {
      "epoch": 0.7926287262872629,
      "step": 3656,
      "training_loss": 7.05055570602417
    },
    {
      "epoch": 0.7928455284552846,
      "step": 3657,
      "training_loss": 6.589312553405762
    },
    {
      "epoch": 0.7928455284552846,
      "step": 3657,
      "training_loss": 7.187693119049072
    },
    {
      "epoch": 0.7928455284552846,
      "step": 3657,
      "training_loss": 5.086372375488281
    },
    {
      "epoch": 0.7928455284552846,
      "step": 3657,
      "training_loss": 7.334733486175537
    },
    {
      "epoch": 0.7930623306233062,
      "step": 3658,
      "training_loss": 7.075935363769531
    },
    {
      "epoch": 0.7930623306233062,
      "step": 3658,
      "training_loss": 7.478185176849365
    },
    {
      "epoch": 0.7930623306233062,
      "step": 3658,
      "training_loss": 6.029032230377197
    },
    {
      "epoch": 0.7930623306233062,
      "step": 3658,
      "training_loss": 6.876491546630859
    },
    {
      "epoch": 0.7932791327913279,
      "step": 3659,
      "training_loss": 4.606812953948975
    },
    {
      "epoch": 0.7932791327913279,
      "step": 3659,
      "training_loss": 6.870574474334717
    },
    {
      "epoch": 0.7932791327913279,
      "step": 3659,
      "training_loss": 8.463052749633789
    },
    {
      "epoch": 0.7932791327913279,
      "step": 3659,
      "training_loss": 7.561546325683594
    },
    {
      "epoch": 0.7934959349593496,
      "grad_norm": 15.943533897399902,
      "learning_rate": 1e-05,
      "loss": 6.6755,
      "step": 3660
    },
    {
      "epoch": 0.7934959349593496,
      "step": 3660,
      "training_loss": 6.1707611083984375
    },
    {
      "epoch": 0.7934959349593496,
      "step": 3660,
      "training_loss": 3.2000598907470703
    },
    {
      "epoch": 0.7934959349593496,
      "step": 3660,
      "training_loss": 3.9882428646087646
    },
    {
      "epoch": 0.7934959349593496,
      "step": 3660,
      "training_loss": 3.9879918098449707
    },
    {
      "epoch": 0.7937127371273712,
      "step": 3661,
      "training_loss": 6.765580177307129
    },
    {
      "epoch": 0.7937127371273712,
      "step": 3661,
      "training_loss": 7.251893997192383
    },
    {
      "epoch": 0.7937127371273712,
      "step": 3661,
      "training_loss": 6.749727725982666
    },
    {
      "epoch": 0.7937127371273712,
      "step": 3661,
      "training_loss": 7.090371608734131
    },
    {
      "epoch": 0.793929539295393,
      "step": 3662,
      "training_loss": 6.7191362380981445
    },
    {
      "epoch": 0.793929539295393,
      "step": 3662,
      "training_loss": 5.600887298583984
    },
    {
      "epoch": 0.793929539295393,
      "step": 3662,
      "training_loss": 5.269256591796875
    },
    {
      "epoch": 0.793929539295393,
      "step": 3662,
      "training_loss": 6.708569049835205
    },
    {
      "epoch": 0.7941463414634147,
      "step": 3663,
      "training_loss": 5.37654447555542
    },
    {
      "epoch": 0.7941463414634147,
      "step": 3663,
      "training_loss": 6.639903545379639
    },
    {
      "epoch": 0.7941463414634147,
      "step": 3663,
      "training_loss": 5.6469340324401855
    },
    {
      "epoch": 0.7941463414634147,
      "step": 3663,
      "training_loss": 4.8330841064453125
    },
    {
      "epoch": 0.7943631436314363,
      "grad_norm": 22.597518920898438,
      "learning_rate": 1e-05,
      "loss": 5.7499,
      "step": 3664
    },
    {
      "epoch": 0.7943631436314363,
      "step": 3664,
      "training_loss": 5.622255802154541
    },
    {
      "epoch": 0.7943631436314363,
      "step": 3664,
      "training_loss": 7.204723358154297
    },
    {
      "epoch": 0.7943631436314363,
      "step": 3664,
      "training_loss": 5.705626487731934
    },
    {
      "epoch": 0.7943631436314363,
      "step": 3664,
      "training_loss": 5.581510066986084
    },
    {
      "epoch": 0.794579945799458,
      "step": 3665,
      "training_loss": 3.867697238922119
    },
    {
      "epoch": 0.794579945799458,
      "step": 3665,
      "training_loss": 6.679510593414307
    },
    {
      "epoch": 0.794579945799458,
      "step": 3665,
      "training_loss": 6.048169136047363
    },
    {
      "epoch": 0.794579945799458,
      "step": 3665,
      "training_loss": 7.002642631530762
    },
    {
      "epoch": 0.7947967479674797,
      "step": 3666,
      "training_loss": 5.372493267059326
    },
    {
      "epoch": 0.7947967479674797,
      "step": 3666,
      "training_loss": 6.514773845672607
    },
    {
      "epoch": 0.7947967479674797,
      "step": 3666,
      "training_loss": 6.510958671569824
    },
    {
      "epoch": 0.7947967479674797,
      "step": 3666,
      "training_loss": 6.702403545379639
    },
    {
      "epoch": 0.7950135501355013,
      "step": 3667,
      "training_loss": 5.619217872619629
    },
    {
      "epoch": 0.7950135501355013,
      "step": 3667,
      "training_loss": 7.886163711547852
    },
    {
      "epoch": 0.7950135501355013,
      "step": 3667,
      "training_loss": 7.890566825866699
    },
    {
      "epoch": 0.7950135501355013,
      "step": 3667,
      "training_loss": 7.2857794761657715
    },
    {
      "epoch": 0.795230352303523,
      "grad_norm": 22.94381332397461,
      "learning_rate": 1e-05,
      "loss": 6.3434,
      "step": 3668
    },
    {
      "epoch": 0.795230352303523,
      "step": 3668,
      "training_loss": 6.550878524780273
    },
    {
      "epoch": 0.795230352303523,
      "step": 3668,
      "training_loss": 7.846669673919678
    },
    {
      "epoch": 0.795230352303523,
      "step": 3668,
      "training_loss": 7.80289363861084
    },
    {
      "epoch": 0.795230352303523,
      "step": 3668,
      "training_loss": 7.496818542480469
    },
    {
      "epoch": 0.7954471544715447,
      "step": 3669,
      "training_loss": 6.84798002243042
    },
    {
      "epoch": 0.7954471544715447,
      "step": 3669,
      "training_loss": 6.050039291381836
    },
    {
      "epoch": 0.7954471544715447,
      "step": 3669,
      "training_loss": 5.527953147888184
    },
    {
      "epoch": 0.7954471544715447,
      "step": 3669,
      "training_loss": 7.329593181610107
    },
    {
      "epoch": 0.7956639566395663,
      "step": 3670,
      "training_loss": 7.5373640060424805
    },
    {
      "epoch": 0.7956639566395663,
      "step": 3670,
      "training_loss": 6.002191066741943
    },
    {
      "epoch": 0.7956639566395663,
      "step": 3670,
      "training_loss": 7.06945276260376
    },
    {
      "epoch": 0.7956639566395663,
      "step": 3670,
      "training_loss": 5.537168502807617
    },
    {
      "epoch": 0.7958807588075881,
      "step": 3671,
      "training_loss": 5.985774517059326
    },
    {
      "epoch": 0.7958807588075881,
      "step": 3671,
      "training_loss": 6.600716590881348
    },
    {
      "epoch": 0.7958807588075881,
      "step": 3671,
      "training_loss": 6.741489410400391
    },
    {
      "epoch": 0.7958807588075881,
      "step": 3671,
      "training_loss": 6.819705963134766
    },
    {
      "epoch": 0.7960975609756098,
      "grad_norm": 20.741653442382812,
      "learning_rate": 1e-05,
      "loss": 6.7342,
      "step": 3672
    },
    {
      "epoch": 0.7960975609756098,
      "step": 3672,
      "training_loss": 6.764891147613525
    },
    {
      "epoch": 0.7960975609756098,
      "step": 3672,
      "training_loss": 6.9470438957214355
    },
    {
      "epoch": 0.7960975609756098,
      "step": 3672,
      "training_loss": 6.642775535583496
    },
    {
      "epoch": 0.7960975609756098,
      "step": 3672,
      "training_loss": 6.323569297790527
    },
    {
      "epoch": 0.7963143631436315,
      "step": 3673,
      "training_loss": 5.164841175079346
    },
    {
      "epoch": 0.7963143631436315,
      "step": 3673,
      "training_loss": 7.025266647338867
    },
    {
      "epoch": 0.7963143631436315,
      "step": 3673,
      "training_loss": 6.7279052734375
    },
    {
      "epoch": 0.7963143631436315,
      "step": 3673,
      "training_loss": 7.09254264831543
    },
    {
      "epoch": 0.7965311653116531,
      "step": 3674,
      "training_loss": 7.952813625335693
    },
    {
      "epoch": 0.7965311653116531,
      "step": 3674,
      "training_loss": 6.396469593048096
    },
    {
      "epoch": 0.7965311653116531,
      "step": 3674,
      "training_loss": 7.531911373138428
    },
    {
      "epoch": 0.7965311653116531,
      "step": 3674,
      "training_loss": 6.791182994842529
    },
    {
      "epoch": 0.7967479674796748,
      "step": 3675,
      "training_loss": 4.280670166015625
    },
    {
      "epoch": 0.7967479674796748,
      "step": 3675,
      "training_loss": 7.169312477111816
    },
    {
      "epoch": 0.7967479674796748,
      "step": 3675,
      "training_loss": 6.559256076812744
    },
    {
      "epoch": 0.7967479674796748,
      "step": 3675,
      "training_loss": 6.852921962738037
    },
    {
      "epoch": 0.7969647696476965,
      "grad_norm": 17.274852752685547,
      "learning_rate": 1e-05,
      "loss": 6.639,
      "step": 3676
    },
    {
      "epoch": 0.7969647696476965,
      "step": 3676,
      "training_loss": 6.269282341003418
    },
    {
      "epoch": 0.7969647696476965,
      "step": 3676,
      "training_loss": 5.326218128204346
    },
    {
      "epoch": 0.7969647696476965,
      "step": 3676,
      "training_loss": 6.351161956787109
    },
    {
      "epoch": 0.7969647696476965,
      "step": 3676,
      "training_loss": 6.528592109680176
    },
    {
      "epoch": 0.7971815718157181,
      "step": 3677,
      "training_loss": 6.69637393951416
    },
    {
      "epoch": 0.7971815718157181,
      "step": 3677,
      "training_loss": 5.450255870819092
    },
    {
      "epoch": 0.7971815718157181,
      "step": 3677,
      "training_loss": 7.874808311462402
    },
    {
      "epoch": 0.7971815718157181,
      "step": 3677,
      "training_loss": 6.694571018218994
    },
    {
      "epoch": 0.7973983739837398,
      "step": 3678,
      "training_loss": 7.02462911605835
    },
    {
      "epoch": 0.7973983739837398,
      "step": 3678,
      "training_loss": 7.418027400970459
    },
    {
      "epoch": 0.7973983739837398,
      "step": 3678,
      "training_loss": 5.806389331817627
    },
    {
      "epoch": 0.7973983739837398,
      "step": 3678,
      "training_loss": 7.0187087059021
    },
    {
      "epoch": 0.7976151761517615,
      "step": 3679,
      "training_loss": 5.7942304611206055
    },
    {
      "epoch": 0.7976151761517615,
      "step": 3679,
      "training_loss": 6.789443016052246
    },
    {
      "epoch": 0.7976151761517615,
      "step": 3679,
      "training_loss": 6.146224021911621
    },
    {
      "epoch": 0.7976151761517615,
      "step": 3679,
      "training_loss": 6.400998115539551
    },
    {
      "epoch": 0.7978319783197833,
      "grad_norm": 19.009748458862305,
      "learning_rate": 1e-05,
      "loss": 6.4744,
      "step": 3680
    },
    {
      "epoch": 0.7978319783197833,
      "step": 3680,
      "training_loss": 5.125702857971191
    },
    {
      "epoch": 0.7978319783197833,
      "step": 3680,
      "training_loss": 6.571189880371094
    },
    {
      "epoch": 0.7978319783197833,
      "step": 3680,
      "training_loss": 5.791492462158203
    },
    {
      "epoch": 0.7978319783197833,
      "step": 3680,
      "training_loss": 6.2891340255737305
    },
    {
      "epoch": 0.7980487804878049,
      "step": 3681,
      "training_loss": 7.1851606369018555
    },
    {
      "epoch": 0.7980487804878049,
      "step": 3681,
      "training_loss": 6.768903732299805
    },
    {
      "epoch": 0.7980487804878049,
      "step": 3681,
      "training_loss": 6.28548002243042
    },
    {
      "epoch": 0.7980487804878049,
      "step": 3681,
      "training_loss": 5.832189559936523
    },
    {
      "epoch": 0.7982655826558266,
      "step": 3682,
      "training_loss": 5.091320037841797
    },
    {
      "epoch": 0.7982655826558266,
      "step": 3682,
      "training_loss": 5.825415134429932
    },
    {
      "epoch": 0.7982655826558266,
      "step": 3682,
      "training_loss": 6.154515266418457
    },
    {
      "epoch": 0.7982655826558266,
      "step": 3682,
      "training_loss": 5.937006950378418
    },
    {
      "epoch": 0.7984823848238483,
      "step": 3683,
      "training_loss": 5.795594692230225
    },
    {
      "epoch": 0.7984823848238483,
      "step": 3683,
      "training_loss": 6.863819122314453
    },
    {
      "epoch": 0.7984823848238483,
      "step": 3683,
      "training_loss": 8.600289344787598
    },
    {
      "epoch": 0.7984823848238483,
      "step": 3683,
      "training_loss": 5.35996675491333
    },
    {
      "epoch": 0.7986991869918699,
      "grad_norm": 33.2279167175293,
      "learning_rate": 1e-05,
      "loss": 6.2173,
      "step": 3684
    },
    {
      "epoch": 0.7986991869918699,
      "step": 3684,
      "training_loss": 8.551026344299316
    },
    {
      "epoch": 0.7986991869918699,
      "step": 3684,
      "training_loss": 5.320705890655518
    },
    {
      "epoch": 0.7986991869918699,
      "step": 3684,
      "training_loss": 7.893583297729492
    },
    {
      "epoch": 0.7986991869918699,
      "step": 3684,
      "training_loss": 5.684440612792969
    },
    {
      "epoch": 0.7989159891598916,
      "step": 3685,
      "training_loss": 7.9670257568359375
    },
    {
      "epoch": 0.7989159891598916,
      "step": 3685,
      "training_loss": 7.570887565612793
    },
    {
      "epoch": 0.7989159891598916,
      "step": 3685,
      "training_loss": 6.8267669677734375
    },
    {
      "epoch": 0.7989159891598916,
      "step": 3685,
      "training_loss": 3.6075732707977295
    },
    {
      "epoch": 0.7991327913279133,
      "step": 3686,
      "training_loss": 9.719921112060547
    },
    {
      "epoch": 0.7991327913279133,
      "step": 3686,
      "training_loss": 7.427512168884277
    },
    {
      "epoch": 0.7991327913279133,
      "step": 3686,
      "training_loss": 5.806759357452393
    },
    {
      "epoch": 0.7991327913279133,
      "step": 3686,
      "training_loss": 4.977924823760986
    },
    {
      "epoch": 0.7993495934959349,
      "step": 3687,
      "training_loss": 5.738287925720215
    },
    {
      "epoch": 0.7993495934959349,
      "step": 3687,
      "training_loss": 7.027770042419434
    },
    {
      "epoch": 0.7993495934959349,
      "step": 3687,
      "training_loss": 6.149788856506348
    },
    {
      "epoch": 0.7993495934959349,
      "step": 3687,
      "training_loss": 6.212179660797119
    },
    {
      "epoch": 0.7995663956639566,
      "grad_norm": 18.155649185180664,
      "learning_rate": 1e-05,
      "loss": 6.6551,
      "step": 3688
    },
    {
      "epoch": 0.7995663956639566,
      "step": 3688,
      "training_loss": 5.866465091705322
    },
    {
      "epoch": 0.7995663956639566,
      "step": 3688,
      "training_loss": 6.589766025543213
    },
    {
      "epoch": 0.7995663956639566,
      "step": 3688,
      "training_loss": 7.955440044403076
    },
    {
      "epoch": 0.7995663956639566,
      "step": 3688,
      "training_loss": 6.952460289001465
    },
    {
      "epoch": 0.7997831978319783,
      "step": 3689,
      "training_loss": 5.006780624389648
    },
    {
      "epoch": 0.7997831978319783,
      "step": 3689,
      "training_loss": 6.16702938079834
    },
    {
      "epoch": 0.7997831978319783,
      "step": 3689,
      "training_loss": 6.877265453338623
    },
    {
      "epoch": 0.7997831978319783,
      "step": 3689,
      "training_loss": 5.3427863121032715
    },
    {
      "epoch": 0.8,
      "step": 3690,
      "training_loss": 4.824450492858887
    },
    {
      "epoch": 0.8,
      "step": 3690,
      "training_loss": 7.450894355773926
    },
    {
      "epoch": 0.8,
      "step": 3690,
      "training_loss": 6.440291404724121
    },
    {
      "epoch": 0.8,
      "step": 3690,
      "training_loss": 6.548330783843994
    },
    {
      "epoch": 0.8002168021680217,
      "step": 3691,
      "training_loss": 7.250230312347412
    },
    {
      "epoch": 0.8002168021680217,
      "step": 3691,
      "training_loss": 3.647808313369751
    },
    {
      "epoch": 0.8002168021680217,
      "step": 3691,
      "training_loss": 5.996930122375488
    },
    {
      "epoch": 0.8002168021680217,
      "step": 3691,
      "training_loss": 6.82438850402832
    },
    {
      "epoch": 0.8004336043360434,
      "grad_norm": 12.62257194519043,
      "learning_rate": 1e-05,
      "loss": 6.2338,
      "step": 3692
    },
    {
      "epoch": 0.8004336043360434,
      "step": 3692,
      "training_loss": 7.3462629318237305
    },
    {
      "epoch": 0.8004336043360434,
      "step": 3692,
      "training_loss": 4.2784552574157715
    },
    {
      "epoch": 0.8004336043360434,
      "step": 3692,
      "training_loss": 7.110960483551025
    },
    {
      "epoch": 0.8004336043360434,
      "step": 3692,
      "training_loss": 6.378568649291992
    },
    {
      "epoch": 0.800650406504065,
      "step": 3693,
      "training_loss": 6.799497604370117
    },
    {
      "epoch": 0.800650406504065,
      "step": 3693,
      "training_loss": 5.821745872497559
    },
    {
      "epoch": 0.800650406504065,
      "step": 3693,
      "training_loss": 6.088215351104736
    },
    {
      "epoch": 0.800650406504065,
      "step": 3693,
      "training_loss": 3.8731191158294678
    },
    {
      "epoch": 0.8008672086720867,
      "step": 3694,
      "training_loss": 5.823170185089111
    },
    {
      "epoch": 0.8008672086720867,
      "step": 3694,
      "training_loss": 7.1390604972839355
    },
    {
      "epoch": 0.8008672086720867,
      "step": 3694,
      "training_loss": 6.679286956787109
    },
    {
      "epoch": 0.8008672086720867,
      "step": 3694,
      "training_loss": 6.8644890785217285
    },
    {
      "epoch": 0.8010840108401084,
      "step": 3695,
      "training_loss": 6.191944122314453
    },
    {
      "epoch": 0.8010840108401084,
      "step": 3695,
      "training_loss": 6.496194839477539
    },
    {
      "epoch": 0.8010840108401084,
      "step": 3695,
      "training_loss": 6.358641624450684
    },
    {
      "epoch": 0.8010840108401084,
      "step": 3695,
      "training_loss": 6.450437068939209
    },
    {
      "epoch": 0.80130081300813,
      "grad_norm": 15.454055786132812,
      "learning_rate": 1e-05,
      "loss": 6.2313,
      "step": 3696
    },
    {
      "epoch": 0.80130081300813,
      "step": 3696,
      "training_loss": 6.043808460235596
    },
    {
      "epoch": 0.80130081300813,
      "step": 3696,
      "training_loss": 7.825471878051758
    },
    {
      "epoch": 0.80130081300813,
      "step": 3696,
      "training_loss": 7.712723731994629
    },
    {
      "epoch": 0.80130081300813,
      "step": 3696,
      "training_loss": 5.551633358001709
    },
    {
      "epoch": 0.8015176151761517,
      "step": 3697,
      "training_loss": 5.489098072052002
    },
    {
      "epoch": 0.8015176151761517,
      "step": 3697,
      "training_loss": 5.1198625564575195
    },
    {
      "epoch": 0.8015176151761517,
      "step": 3697,
      "training_loss": 7.0590105056762695
    },
    {
      "epoch": 0.8015176151761517,
      "step": 3697,
      "training_loss": 6.88535213470459
    },
    {
      "epoch": 0.8017344173441734,
      "step": 3698,
      "training_loss": 5.4532294273376465
    },
    {
      "epoch": 0.8017344173441734,
      "step": 3698,
      "training_loss": 6.206914901733398
    },
    {
      "epoch": 0.8017344173441734,
      "step": 3698,
      "training_loss": 6.439210414886475
    },
    {
      "epoch": 0.8017344173441734,
      "step": 3698,
      "training_loss": 6.7370147705078125
    },
    {
      "epoch": 0.8019512195121952,
      "step": 3699,
      "training_loss": 7.876895427703857
    },
    {
      "epoch": 0.8019512195121952,
      "step": 3699,
      "training_loss": 7.238389492034912
    },
    {
      "epoch": 0.8019512195121952,
      "step": 3699,
      "training_loss": 5.6772074699401855
    },
    {
      "epoch": 0.8019512195121952,
      "step": 3699,
      "training_loss": 7.241028308868408
    },
    {
      "epoch": 0.8021680216802168,
      "grad_norm": 32.319061279296875,
      "learning_rate": 1e-05,
      "loss": 6.5348,
      "step": 3700
    },
    {
      "epoch": 0.8021680216802168,
      "step": 3700,
      "training_loss": 6.739052772521973
    },
    {
      "epoch": 0.8021680216802168,
      "step": 3700,
      "training_loss": 7.012909889221191
    },
    {
      "epoch": 0.8021680216802168,
      "step": 3700,
      "training_loss": 3.8710241317749023
    },
    {
      "epoch": 0.8021680216802168,
      "step": 3700,
      "training_loss": 6.691061496734619
    },
    {
      "epoch": 0.8023848238482385,
      "step": 3701,
      "training_loss": 3.1963610649108887
    },
    {
      "epoch": 0.8023848238482385,
      "step": 3701,
      "training_loss": 6.179549217224121
    },
    {
      "epoch": 0.8023848238482385,
      "step": 3701,
      "training_loss": 5.717051029205322
    },
    {
      "epoch": 0.8023848238482385,
      "step": 3701,
      "training_loss": 4.628471851348877
    },
    {
      "epoch": 0.8026016260162602,
      "step": 3702,
      "training_loss": 6.830565452575684
    },
    {
      "epoch": 0.8026016260162602,
      "step": 3702,
      "training_loss": 7.603529930114746
    },
    {
      "epoch": 0.8026016260162602,
      "step": 3702,
      "training_loss": 6.978401184082031
    },
    {
      "epoch": 0.8026016260162602,
      "step": 3702,
      "training_loss": 6.317038536071777
    },
    {
      "epoch": 0.8028184281842818,
      "step": 3703,
      "training_loss": 4.84171199798584
    },
    {
      "epoch": 0.8028184281842818,
      "step": 3703,
      "training_loss": 5.056760311126709
    },
    {
      "epoch": 0.8028184281842818,
      "step": 3703,
      "training_loss": 6.489055633544922
    },
    {
      "epoch": 0.8028184281842818,
      "step": 3703,
      "training_loss": 6.472140312194824
    },
    {
      "epoch": 0.8030352303523035,
      "grad_norm": 18.679668426513672,
      "learning_rate": 1e-05,
      "loss": 5.914,
      "step": 3704
    },
    {
      "epoch": 0.8030352303523035,
      "step": 3704,
      "training_loss": 6.9526801109313965
    },
    {
      "epoch": 0.8030352303523035,
      "step": 3704,
      "training_loss": 4.883427143096924
    },
    {
      "epoch": 0.8030352303523035,
      "step": 3704,
      "training_loss": 6.666640758514404
    },
    {
      "epoch": 0.8030352303523035,
      "step": 3704,
      "training_loss": 4.461362838745117
    },
    {
      "epoch": 0.8032520325203252,
      "step": 3705,
      "training_loss": 7.161086082458496
    },
    {
      "epoch": 0.8032520325203252,
      "step": 3705,
      "training_loss": 6.652355194091797
    },
    {
      "epoch": 0.8032520325203252,
      "step": 3705,
      "training_loss": 6.331966400146484
    },
    {
      "epoch": 0.8032520325203252,
      "step": 3705,
      "training_loss": 5.914309501647949
    },
    {
      "epoch": 0.8034688346883468,
      "step": 3706,
      "training_loss": 5.846443176269531
    },
    {
      "epoch": 0.8034688346883468,
      "step": 3706,
      "training_loss": 7.258860111236572
    },
    {
      "epoch": 0.8034688346883468,
      "step": 3706,
      "training_loss": 7.105109214782715
    },
    {
      "epoch": 0.8034688346883468,
      "step": 3706,
      "training_loss": 4.176274299621582
    },
    {
      "epoch": 0.8036856368563685,
      "step": 3707,
      "training_loss": 3.4493424892425537
    },
    {
      "epoch": 0.8036856368563685,
      "step": 3707,
      "training_loss": 7.356347560882568
    },
    {
      "epoch": 0.8036856368563685,
      "step": 3707,
      "training_loss": 5.3304338455200195
    },
    {
      "epoch": 0.8036856368563685,
      "step": 3707,
      "training_loss": 5.520849227905273
    },
    {
      "epoch": 0.8039024390243903,
      "grad_norm": 22.947572708129883,
      "learning_rate": 1e-05,
      "loss": 5.9417,
      "step": 3708
    },
    {
      "epoch": 0.8039024390243903,
      "step": 3708,
      "training_loss": 5.225974082946777
    },
    {
      "epoch": 0.8039024390243903,
      "step": 3708,
      "training_loss": 6.589778423309326
    },
    {
      "epoch": 0.8039024390243903,
      "step": 3708,
      "training_loss": 6.939317226409912
    },
    {
      "epoch": 0.8039024390243903,
      "step": 3708,
      "training_loss": 7.545345783233643
    },
    {
      "epoch": 0.804119241192412,
      "step": 3709,
      "training_loss": 6.908289432525635
    },
    {
      "epoch": 0.804119241192412,
      "step": 3709,
      "training_loss": 5.632730007171631
    },
    {
      "epoch": 0.804119241192412,
      "step": 3709,
      "training_loss": 4.705244541168213
    },
    {
      "epoch": 0.804119241192412,
      "step": 3709,
      "training_loss": 6.305169105529785
    },
    {
      "epoch": 0.8043360433604336,
      "step": 3710,
      "training_loss": 5.7853922843933105
    },
    {
      "epoch": 0.8043360433604336,
      "step": 3710,
      "training_loss": 3.4811618328094482
    },
    {
      "epoch": 0.8043360433604336,
      "step": 3710,
      "training_loss": 4.988153457641602
    },
    {
      "epoch": 0.8043360433604336,
      "step": 3710,
      "training_loss": 5.914216041564941
    },
    {
      "epoch": 0.8045528455284553,
      "step": 3711,
      "training_loss": 5.634273052215576
    },
    {
      "epoch": 0.8045528455284553,
      "step": 3711,
      "training_loss": 6.7550554275512695
    },
    {
      "epoch": 0.8045528455284553,
      "step": 3711,
      "training_loss": 5.881645679473877
    },
    {
      "epoch": 0.8045528455284553,
      "step": 3711,
      "training_loss": 2.579411029815674
    },
    {
      "epoch": 0.804769647696477,
      "grad_norm": 21.535425186157227,
      "learning_rate": 1e-05,
      "loss": 5.6794,
      "step": 3712
    },
    {
      "epoch": 0.804769647696477,
      "step": 3712,
      "training_loss": 7.812916278839111
    },
    {
      "epoch": 0.804769647696477,
      "step": 3712,
      "training_loss": 4.284955978393555
    },
    {
      "epoch": 0.804769647696477,
      "step": 3712,
      "training_loss": 6.863516807556152
    },
    {
      "epoch": 0.804769647696477,
      "step": 3712,
      "training_loss": 6.562798976898193
    },
    {
      "epoch": 0.8049864498644986,
      "step": 3713,
      "training_loss": 6.5922088623046875
    },
    {
      "epoch": 0.8049864498644986,
      "step": 3713,
      "training_loss": 5.778942584991455
    },
    {
      "epoch": 0.8049864498644986,
      "step": 3713,
      "training_loss": 5.674129486083984
    },
    {
      "epoch": 0.8049864498644986,
      "step": 3713,
      "training_loss": 7.740686416625977
    },
    {
      "epoch": 0.8052032520325203,
      "step": 3714,
      "training_loss": 6.102348804473877
    },
    {
      "epoch": 0.8052032520325203,
      "step": 3714,
      "training_loss": 6.134366512298584
    },
    {
      "epoch": 0.8052032520325203,
      "step": 3714,
      "training_loss": 5.370227336883545
    },
    {
      "epoch": 0.8052032520325203,
      "step": 3714,
      "training_loss": 6.631303310394287
    },
    {
      "epoch": 0.805420054200542,
      "step": 3715,
      "training_loss": 7.653061866760254
    },
    {
      "epoch": 0.805420054200542,
      "step": 3715,
      "training_loss": 5.062788486480713
    },
    {
      "epoch": 0.805420054200542,
      "step": 3715,
      "training_loss": 7.74580192565918
    },
    {
      "epoch": 0.805420054200542,
      "step": 3715,
      "training_loss": 5.824967384338379
    },
    {
      "epoch": 0.8056368563685636,
      "grad_norm": 19.69257164001465,
      "learning_rate": 1e-05,
      "loss": 6.3647,
      "step": 3716
    },
    {
      "epoch": 0.8056368563685636,
      "step": 3716,
      "training_loss": 6.162266731262207
    },
    {
      "epoch": 0.8056368563685636,
      "step": 3716,
      "training_loss": 5.326507568359375
    },
    {
      "epoch": 0.8056368563685636,
      "step": 3716,
      "training_loss": 5.615602016448975
    },
    {
      "epoch": 0.8056368563685636,
      "step": 3716,
      "training_loss": 6.652660369873047
    },
    {
      "epoch": 0.8058536585365854,
      "step": 3717,
      "training_loss": 6.726738929748535
    },
    {
      "epoch": 0.8058536585365854,
      "step": 3717,
      "training_loss": 9.242154121398926
    },
    {
      "epoch": 0.8058536585365854,
      "step": 3717,
      "training_loss": 6.96516227722168
    },
    {
      "epoch": 0.8058536585365854,
      "step": 3717,
      "training_loss": 7.150768756866455
    },
    {
      "epoch": 0.8060704607046071,
      "step": 3718,
      "training_loss": 7.549771308898926
    },
    {
      "epoch": 0.8060704607046071,
      "step": 3718,
      "training_loss": 5.960338115692139
    },
    {
      "epoch": 0.8060704607046071,
      "step": 3718,
      "training_loss": 6.545774936676025
    },
    {
      "epoch": 0.8060704607046071,
      "step": 3718,
      "training_loss": 7.921413421630859
    },
    {
      "epoch": 0.8062872628726288,
      "step": 3719,
      "training_loss": 3.4728708267211914
    },
    {
      "epoch": 0.8062872628726288,
      "step": 3719,
      "training_loss": 6.87883996963501
    },
    {
      "epoch": 0.8062872628726288,
      "step": 3719,
      "training_loss": 6.965768337249756
    },
    {
      "epoch": 0.8062872628726288,
      "step": 3719,
      "training_loss": 4.005054950714111
    },
    {
      "epoch": 0.8065040650406504,
      "grad_norm": 24.08510971069336,
      "learning_rate": 1e-05,
      "loss": 6.4464,
      "step": 3720
    },
    {
      "epoch": 0.8065040650406504,
      "step": 3720,
      "training_loss": 7.802644729614258
    },
    {
      "epoch": 0.8065040650406504,
      "step": 3720,
      "training_loss": 4.865969657897949
    },
    {
      "epoch": 0.8065040650406504,
      "step": 3720,
      "training_loss": 6.723109722137451
    },
    {
      "epoch": 0.8065040650406504,
      "step": 3720,
      "training_loss": 6.6547532081604
    },
    {
      "epoch": 0.8067208672086721,
      "step": 3721,
      "training_loss": 2.7666196823120117
    },
    {
      "epoch": 0.8067208672086721,
      "step": 3721,
      "training_loss": 7.425632476806641
    },
    {
      "epoch": 0.8067208672086721,
      "step": 3721,
      "training_loss": 4.305506706237793
    },
    {
      "epoch": 0.8067208672086721,
      "step": 3721,
      "training_loss": 7.866246700286865
    },
    {
      "epoch": 0.8069376693766938,
      "step": 3722,
      "training_loss": 7.034125804901123
    },
    {
      "epoch": 0.8069376693766938,
      "step": 3722,
      "training_loss": 7.754158973693848
    },
    {
      "epoch": 0.8069376693766938,
      "step": 3722,
      "training_loss": 6.072303771972656
    },
    {
      "epoch": 0.8069376693766938,
      "step": 3722,
      "training_loss": 7.295139312744141
    },
    {
      "epoch": 0.8071544715447154,
      "step": 3723,
      "training_loss": 8.151884078979492
    },
    {
      "epoch": 0.8071544715447154,
      "step": 3723,
      "training_loss": 5.996523380279541
    },
    {
      "epoch": 0.8071544715447154,
      "step": 3723,
      "training_loss": 7.323522567749023
    },
    {
      "epoch": 0.8071544715447154,
      "step": 3723,
      "training_loss": 5.0368170738220215
    },
    {
      "epoch": 0.8073712737127371,
      "grad_norm": 25.93280029296875,
      "learning_rate": 1e-05,
      "loss": 6.4422,
      "step": 3724
    },
    {
      "epoch": 0.8073712737127371,
      "step": 3724,
      "training_loss": 7.352898597717285
    },
    {
      "epoch": 0.8073712737127371,
      "step": 3724,
      "training_loss": 6.829557418823242
    },
    {
      "epoch": 0.8073712737127371,
      "step": 3724,
      "training_loss": 8.236823081970215
    },
    {
      "epoch": 0.8073712737127371,
      "step": 3724,
      "training_loss": 7.0587897300720215
    },
    {
      "epoch": 0.8075880758807588,
      "step": 3725,
      "training_loss": 5.382729530334473
    },
    {
      "epoch": 0.8075880758807588,
      "step": 3725,
      "training_loss": 6.223752021789551
    },
    {
      "epoch": 0.8075880758807588,
      "step": 3725,
      "training_loss": 6.354679107666016
    },
    {
      "epoch": 0.8075880758807588,
      "step": 3725,
      "training_loss": 8.33471965789795
    },
    {
      "epoch": 0.8078048780487805,
      "step": 3726,
      "training_loss": 6.700980186462402
    },
    {
      "epoch": 0.8078048780487805,
      "step": 3726,
      "training_loss": 6.204687118530273
    },
    {
      "epoch": 0.8078048780487805,
      "step": 3726,
      "training_loss": 7.525880813598633
    },
    {
      "epoch": 0.8078048780487805,
      "step": 3726,
      "training_loss": 7.174004077911377
    },
    {
      "epoch": 0.8080216802168022,
      "step": 3727,
      "training_loss": 7.049694061279297
    },
    {
      "epoch": 0.8080216802168022,
      "step": 3727,
      "training_loss": 7.3232622146606445
    },
    {
      "epoch": 0.8080216802168022,
      "step": 3727,
      "training_loss": 8.107694625854492
    },
    {
      "epoch": 0.8080216802168022,
      "step": 3727,
      "training_loss": 6.942653656005859
    },
    {
      "epoch": 0.8082384823848239,
      "grad_norm": 13.692110061645508,
      "learning_rate": 1e-05,
      "loss": 7.0502,
      "step": 3728
    },
    {
      "epoch": 0.8082384823848239,
      "step": 3728,
      "training_loss": 6.522353172302246
    },
    {
      "epoch": 0.8082384823848239,
      "step": 3728,
      "training_loss": 6.123403072357178
    },
    {
      "epoch": 0.8082384823848239,
      "step": 3728,
      "training_loss": 6.1953582763671875
    },
    {
      "epoch": 0.8082384823848239,
      "step": 3728,
      "training_loss": 6.540966033935547
    },
    {
      "epoch": 0.8084552845528455,
      "step": 3729,
      "training_loss": 5.522655010223389
    },
    {
      "epoch": 0.8084552845528455,
      "step": 3729,
      "training_loss": 6.433753967285156
    },
    {
      "epoch": 0.8084552845528455,
      "step": 3729,
      "training_loss": 4.4666643142700195
    },
    {
      "epoch": 0.8084552845528455,
      "step": 3729,
      "training_loss": 7.626571178436279
    },
    {
      "epoch": 0.8086720867208672,
      "step": 3730,
      "training_loss": 7.382242202758789
    },
    {
      "epoch": 0.8086720867208672,
      "step": 3730,
      "training_loss": 6.080811500549316
    },
    {
      "epoch": 0.8086720867208672,
      "step": 3730,
      "training_loss": 5.8341145515441895
    },
    {
      "epoch": 0.8086720867208672,
      "step": 3730,
      "training_loss": 6.9728522300720215
    },
    {
      "epoch": 0.8088888888888889,
      "step": 3731,
      "training_loss": 4.0590972900390625
    },
    {
      "epoch": 0.8088888888888889,
      "step": 3731,
      "training_loss": 6.535152912139893
    },
    {
      "epoch": 0.8088888888888889,
      "step": 3731,
      "training_loss": 6.840424537658691
    },
    {
      "epoch": 0.8088888888888889,
      "step": 3731,
      "training_loss": 6.98565149307251
    },
    {
      "epoch": 0.8091056910569105,
      "grad_norm": 18.761335372924805,
      "learning_rate": 1e-05,
      "loss": 6.2576,
      "step": 3732
    },
    {
      "epoch": 0.8091056910569105,
      "step": 3732,
      "training_loss": 5.621438026428223
    },
    {
      "epoch": 0.8091056910569105,
      "step": 3732,
      "training_loss": 5.563060283660889
    },
    {
      "epoch": 0.8091056910569105,
      "step": 3732,
      "training_loss": 5.6851019859313965
    },
    {
      "epoch": 0.8091056910569105,
      "step": 3732,
      "training_loss": 5.976993560791016
    },
    {
      "epoch": 0.8093224932249322,
      "step": 3733,
      "training_loss": 7.665895462036133
    },
    {
      "epoch": 0.8093224932249322,
      "step": 3733,
      "training_loss": 6.748955249786377
    },
    {
      "epoch": 0.8093224932249322,
      "step": 3733,
      "training_loss": 6.02771520614624
    },
    {
      "epoch": 0.8093224932249322,
      "step": 3733,
      "training_loss": 7.229208469390869
    },
    {
      "epoch": 0.8095392953929539,
      "step": 3734,
      "training_loss": 5.578064441680908
    },
    {
      "epoch": 0.8095392953929539,
      "step": 3734,
      "training_loss": 7.641077518463135
    },
    {
      "epoch": 0.8095392953929539,
      "step": 3734,
      "training_loss": 6.67584228515625
    },
    {
      "epoch": 0.8095392953929539,
      "step": 3734,
      "training_loss": 7.3265790939331055
    },
    {
      "epoch": 0.8097560975609757,
      "step": 3735,
      "training_loss": 5.229020118713379
    },
    {
      "epoch": 0.8097560975609757,
      "step": 3735,
      "training_loss": 5.3198041915893555
    },
    {
      "epoch": 0.8097560975609757,
      "step": 3735,
      "training_loss": 6.544746398925781
    },
    {
      "epoch": 0.8097560975609757,
      "step": 3735,
      "training_loss": 6.314093589782715
    },
    {
      "epoch": 0.8099728997289973,
      "grad_norm": 18.511632919311523,
      "learning_rate": 1e-05,
      "loss": 6.3217,
      "step": 3736
    },
    {
      "epoch": 0.8099728997289973,
      "step": 3736,
      "training_loss": 7.1632513999938965
    },
    {
      "epoch": 0.8099728997289973,
      "step": 3736,
      "training_loss": 6.998213768005371
    },
    {
      "epoch": 0.8099728997289973,
      "step": 3736,
      "training_loss": 5.540383815765381
    },
    {
      "epoch": 0.8099728997289973,
      "step": 3736,
      "training_loss": 7.414982795715332
    },
    {
      "epoch": 0.810189701897019,
      "step": 3737,
      "training_loss": 7.422906875610352
    },
    {
      "epoch": 0.810189701897019,
      "step": 3737,
      "training_loss": 6.542577266693115
    },
    {
      "epoch": 0.810189701897019,
      "step": 3737,
      "training_loss": 5.664008617401123
    },
    {
      "epoch": 0.810189701897019,
      "step": 3737,
      "training_loss": 7.180749893188477
    },
    {
      "epoch": 0.8104065040650407,
      "step": 3738,
      "training_loss": 7.272294044494629
    },
    {
      "epoch": 0.8104065040650407,
      "step": 3738,
      "training_loss": 5.90369176864624
    },
    {
      "epoch": 0.8104065040650407,
      "step": 3738,
      "training_loss": 6.651766300201416
    },
    {
      "epoch": 0.8104065040650407,
      "step": 3738,
      "training_loss": 5.638040065765381
    },
    {
      "epoch": 0.8106233062330623,
      "step": 3739,
      "training_loss": 5.701485633850098
    },
    {
      "epoch": 0.8106233062330623,
      "step": 3739,
      "training_loss": 5.322268962860107
    },
    {
      "epoch": 0.8106233062330623,
      "step": 3739,
      "training_loss": 4.92747163772583
    },
    {
      "epoch": 0.8106233062330623,
      "step": 3739,
      "training_loss": 6.15627384185791
    },
    {
      "epoch": 0.810840108401084,
      "grad_norm": 20.275930404663086,
      "learning_rate": 1e-05,
      "loss": 6.3438,
      "step": 3740
    },
    {
      "epoch": 0.810840108401084,
      "step": 3740,
      "training_loss": 6.813319683074951
    },
    {
      "epoch": 0.810840108401084,
      "step": 3740,
      "training_loss": 6.118056297302246
    },
    {
      "epoch": 0.810840108401084,
      "step": 3740,
      "training_loss": 4.592981338500977
    },
    {
      "epoch": 0.810840108401084,
      "step": 3740,
      "training_loss": 6.831266403198242
    },
    {
      "epoch": 0.8110569105691057,
      "step": 3741,
      "training_loss": 5.641879081726074
    },
    {
      "epoch": 0.8110569105691057,
      "step": 3741,
      "training_loss": 6.6638946533203125
    },
    {
      "epoch": 0.8110569105691057,
      "step": 3741,
      "training_loss": 5.487252235412598
    },
    {
      "epoch": 0.8110569105691057,
      "step": 3741,
      "training_loss": 6.670202732086182
    },
    {
      "epoch": 0.8112737127371273,
      "step": 3742,
      "training_loss": 6.835723400115967
    },
    {
      "epoch": 0.8112737127371273,
      "step": 3742,
      "training_loss": 6.665702819824219
    },
    {
      "epoch": 0.8112737127371273,
      "step": 3742,
      "training_loss": 7.15492057800293
    },
    {
      "epoch": 0.8112737127371273,
      "step": 3742,
      "training_loss": 3.2827224731445312
    },
    {
      "epoch": 0.811490514905149,
      "step": 3743,
      "training_loss": 5.525104522705078
    },
    {
      "epoch": 0.811490514905149,
      "step": 3743,
      "training_loss": 5.585353851318359
    },
    {
      "epoch": 0.811490514905149,
      "step": 3743,
      "training_loss": 6.2186598777771
    },
    {
      "epoch": 0.811490514905149,
      "step": 3743,
      "training_loss": 7.9227681159973145
    },
    {
      "epoch": 0.8117073170731708,
      "grad_norm": 18.32202911376953,
      "learning_rate": 1e-05,
      "loss": 6.1256,
      "step": 3744
    },
    {
      "epoch": 0.8117073170731708,
      "step": 3744,
      "training_loss": 4.432487487792969
    },
    {
      "epoch": 0.8117073170731708,
      "step": 3744,
      "training_loss": 6.2592620849609375
    },
    {
      "epoch": 0.8117073170731708,
      "step": 3744,
      "training_loss": 6.825408935546875
    },
    {
      "epoch": 0.8117073170731708,
      "step": 3744,
      "training_loss": 7.3525285720825195
    },
    {
      "epoch": 0.8119241192411925,
      "step": 3745,
      "training_loss": 7.551499366760254
    },
    {
      "epoch": 0.8119241192411925,
      "step": 3745,
      "training_loss": 6.36550235748291
    },
    {
      "epoch": 0.8119241192411925,
      "step": 3745,
      "training_loss": 5.871620178222656
    },
    {
      "epoch": 0.8119241192411925,
      "step": 3745,
      "training_loss": 6.64516019821167
    },
    {
      "epoch": 0.8121409214092141,
      "step": 3746,
      "training_loss": 5.380963325500488
    },
    {
      "epoch": 0.8121409214092141,
      "step": 3746,
      "training_loss": 7.038351058959961
    },
    {
      "epoch": 0.8121409214092141,
      "step": 3746,
      "training_loss": 4.474283218383789
    },
    {
      "epoch": 0.8121409214092141,
      "step": 3746,
      "training_loss": 5.749202728271484
    },
    {
      "epoch": 0.8123577235772358,
      "step": 3747,
      "training_loss": 6.59014892578125
    },
    {
      "epoch": 0.8123577235772358,
      "step": 3747,
      "training_loss": 5.992781162261963
    },
    {
      "epoch": 0.8123577235772358,
      "step": 3747,
      "training_loss": 5.470704078674316
    },
    {
      "epoch": 0.8123577235772358,
      "step": 3747,
      "training_loss": 5.517704010009766
    },
    {
      "epoch": 0.8125745257452575,
      "grad_norm": 17.237550735473633,
      "learning_rate": 1e-05,
      "loss": 6.0948,
      "step": 3748
    },
    {
      "epoch": 0.8125745257452575,
      "step": 3748,
      "training_loss": 7.954441547393799
    },
    {
      "epoch": 0.8125745257452575,
      "step": 3748,
      "training_loss": 5.4864935874938965
    },
    {
      "epoch": 0.8125745257452575,
      "step": 3748,
      "training_loss": 5.854995250701904
    },
    {
      "epoch": 0.8125745257452575,
      "step": 3748,
      "training_loss": 5.46467924118042
    },
    {
      "epoch": 0.8127913279132791,
      "step": 3749,
      "training_loss": 8.4462890625
    },
    {
      "epoch": 0.8127913279132791,
      "step": 3749,
      "training_loss": 5.494518280029297
    },
    {
      "epoch": 0.8127913279132791,
      "step": 3749,
      "training_loss": 6.886346817016602
    },
    {
      "epoch": 0.8127913279132791,
      "step": 3749,
      "training_loss": 6.73163366317749
    },
    {
      "epoch": 0.8130081300813008,
      "step": 3750,
      "training_loss": 6.539515495300293
    },
    {
      "epoch": 0.8130081300813008,
      "step": 3750,
      "training_loss": 5.838139533996582
    },
    {
      "epoch": 0.8130081300813008,
      "step": 3750,
      "training_loss": 6.303111553192139
    },
    {
      "epoch": 0.8130081300813008,
      "step": 3750,
      "training_loss": 7.659320831298828
    },
    {
      "epoch": 0.8132249322493225,
      "step": 3751,
      "training_loss": 7.3468337059021
    },
    {
      "epoch": 0.8132249322493225,
      "step": 3751,
      "training_loss": 5.511128902435303
    },
    {
      "epoch": 0.8132249322493225,
      "step": 3751,
      "training_loss": 6.123236656188965
    },
    {
      "epoch": 0.8132249322493225,
      "step": 3751,
      "training_loss": 4.8787455558776855
    },
    {
      "epoch": 0.8134417344173441,
      "grad_norm": 18.372455596923828,
      "learning_rate": 1e-05,
      "loss": 6.4075,
      "step": 3752
    },
    {
      "epoch": 0.8134417344173441,
      "step": 3752,
      "training_loss": 5.512012958526611
    },
    {
      "epoch": 0.8134417344173441,
      "step": 3752,
      "training_loss": 7.156557083129883
    },
    {
      "epoch": 0.8134417344173441,
      "step": 3752,
      "training_loss": 5.131326198577881
    },
    {
      "epoch": 0.8134417344173441,
      "step": 3752,
      "training_loss": 6.811416149139404
    },
    {
      "epoch": 0.8136585365853658,
      "step": 3753,
      "training_loss": 5.091428279876709
    },
    {
      "epoch": 0.8136585365853658,
      "step": 3753,
      "training_loss": 7.033465385437012
    },
    {
      "epoch": 0.8136585365853658,
      "step": 3753,
      "training_loss": 6.764291763305664
    },
    {
      "epoch": 0.8136585365853658,
      "step": 3753,
      "training_loss": 6.718411445617676
    },
    {
      "epoch": 0.8138753387533876,
      "step": 3754,
      "training_loss": 7.965139865875244
    },
    {
      "epoch": 0.8138753387533876,
      "step": 3754,
      "training_loss": 6.592589378356934
    },
    {
      "epoch": 0.8138753387533876,
      "step": 3754,
      "training_loss": 6.6407999992370605
    },
    {
      "epoch": 0.8138753387533876,
      "step": 3754,
      "training_loss": 5.605419158935547
    },
    {
      "epoch": 0.8140921409214092,
      "step": 3755,
      "training_loss": 7.915085792541504
    },
    {
      "epoch": 0.8140921409214092,
      "step": 3755,
      "training_loss": 6.240256309509277
    },
    {
      "epoch": 0.8140921409214092,
      "step": 3755,
      "training_loss": 4.706804275512695
    },
    {
      "epoch": 0.8140921409214092,
      "step": 3755,
      "training_loss": 3.579207420349121
    },
    {
      "epoch": 0.8143089430894309,
      "grad_norm": 17.13597869873047,
      "learning_rate": 1e-05,
      "loss": 6.2165,
      "step": 3756
    },
    {
      "epoch": 0.8143089430894309,
      "step": 3756,
      "training_loss": 7.8689069747924805
    },
    {
      "epoch": 0.8143089430894309,
      "step": 3756,
      "training_loss": 5.714345932006836
    },
    {
      "epoch": 0.8143089430894309,
      "step": 3756,
      "training_loss": 6.859831809997559
    },
    {
      "epoch": 0.8143089430894309,
      "step": 3756,
      "training_loss": 5.894972801208496
    },
    {
      "epoch": 0.8145257452574526,
      "step": 3757,
      "training_loss": 5.507071495056152
    },
    {
      "epoch": 0.8145257452574526,
      "step": 3757,
      "training_loss": 7.017305850982666
    },
    {
      "epoch": 0.8145257452574526,
      "step": 3757,
      "training_loss": 6.464418411254883
    },
    {
      "epoch": 0.8145257452574526,
      "step": 3757,
      "training_loss": 3.368675947189331
    },
    {
      "epoch": 0.8147425474254743,
      "step": 3758,
      "training_loss": 4.6482391357421875
    },
    {
      "epoch": 0.8147425474254743,
      "step": 3758,
      "training_loss": 5.536514759063721
    },
    {
      "epoch": 0.8147425474254743,
      "step": 3758,
      "training_loss": 4.93987512588501
    },
    {
      "epoch": 0.8147425474254743,
      "step": 3758,
      "training_loss": 6.577730655670166
    },
    {
      "epoch": 0.8149593495934959,
      "step": 3759,
      "training_loss": 7.12230920791626
    },
    {
      "epoch": 0.8149593495934959,
      "step": 3759,
      "training_loss": 6.356868743896484
    },
    {
      "epoch": 0.8149593495934959,
      "step": 3759,
      "training_loss": 6.364744663238525
    },
    {
      "epoch": 0.8149593495934959,
      "step": 3759,
      "training_loss": 7.378065586090088
    },
    {
      "epoch": 0.8151761517615176,
      "grad_norm": 14.18167495727539,
      "learning_rate": 1e-05,
      "loss": 6.1012,
      "step": 3760
    },
    {
      "epoch": 0.8151761517615176,
      "step": 3760,
      "training_loss": 7.271030426025391
    },
    {
      "epoch": 0.8151761517615176,
      "step": 3760,
      "training_loss": 6.807144641876221
    },
    {
      "epoch": 0.8151761517615176,
      "step": 3760,
      "training_loss": 6.367666721343994
    },
    {
      "epoch": 0.8151761517615176,
      "step": 3760,
      "training_loss": 6.512246608734131
    },
    {
      "epoch": 0.8153929539295393,
      "step": 3761,
      "training_loss": 7.0964579582214355
    },
    {
      "epoch": 0.8153929539295393,
      "step": 3761,
      "training_loss": 4.354482173919678
    },
    {
      "epoch": 0.8153929539295393,
      "step": 3761,
      "training_loss": 4.927572250366211
    },
    {
      "epoch": 0.8153929539295393,
      "step": 3761,
      "training_loss": 6.26713752746582
    },
    {
      "epoch": 0.8156097560975609,
      "step": 3762,
      "training_loss": 5.315100193023682
    },
    {
      "epoch": 0.8156097560975609,
      "step": 3762,
      "training_loss": 8.489460945129395
    },
    {
      "epoch": 0.8156097560975609,
      "step": 3762,
      "training_loss": 5.364964008331299
    },
    {
      "epoch": 0.8156097560975609,
      "step": 3762,
      "training_loss": 6.365904808044434
    },
    {
      "epoch": 0.8158265582655827,
      "step": 3763,
      "training_loss": 6.554803371429443
    },
    {
      "epoch": 0.8158265582655827,
      "step": 3763,
      "training_loss": 4.609365940093994
    },
    {
      "epoch": 0.8158265582655827,
      "step": 3763,
      "training_loss": 5.496389389038086
    },
    {
      "epoch": 0.8158265582655827,
      "step": 3763,
      "training_loss": 5.952053546905518
    },
    {
      "epoch": 0.8160433604336044,
      "grad_norm": 23.956878662109375,
      "learning_rate": 1e-05,
      "loss": 6.1095,
      "step": 3764
    },
    {
      "epoch": 0.8160433604336044,
      "step": 3764,
      "training_loss": 7.451178550720215
    },
    {
      "epoch": 0.8160433604336044,
      "step": 3764,
      "training_loss": 5.6050872802734375
    },
    {
      "epoch": 0.8160433604336044,
      "step": 3764,
      "training_loss": 4.611342906951904
    },
    {
      "epoch": 0.8160433604336044,
      "step": 3764,
      "training_loss": 6.420082092285156
    },
    {
      "epoch": 0.816260162601626,
      "step": 3765,
      "training_loss": 7.058028221130371
    },
    {
      "epoch": 0.816260162601626,
      "step": 3765,
      "training_loss": 4.181820392608643
    },
    {
      "epoch": 0.816260162601626,
      "step": 3765,
      "training_loss": 6.340850830078125
    },
    {
      "epoch": 0.816260162601626,
      "step": 3765,
      "training_loss": 7.873538017272949
    },
    {
      "epoch": 0.8164769647696477,
      "step": 3766,
      "training_loss": 6.100584983825684
    },
    {
      "epoch": 0.8164769647696477,
      "step": 3766,
      "training_loss": 6.6965227127075195
    },
    {
      "epoch": 0.8164769647696477,
      "step": 3766,
      "training_loss": 5.065960884094238
    },
    {
      "epoch": 0.8164769647696477,
      "step": 3766,
      "training_loss": 5.669870376586914
    },
    {
      "epoch": 0.8166937669376694,
      "step": 3767,
      "training_loss": 6.639486789703369
    },
    {
      "epoch": 0.8166937669376694,
      "step": 3767,
      "training_loss": 6.909420013427734
    },
    {
      "epoch": 0.8166937669376694,
      "step": 3767,
      "training_loss": 6.823729991912842
    },
    {
      "epoch": 0.8166937669376694,
      "step": 3767,
      "training_loss": 6.527040004730225
    },
    {
      "epoch": 0.816910569105691,
      "grad_norm": 20.205398559570312,
      "learning_rate": 1e-05,
      "loss": 6.2484,
      "step": 3768
    },
    {
      "epoch": 0.816910569105691,
      "step": 3768,
      "training_loss": 5.048613548278809
    },
    {
      "epoch": 0.816910569105691,
      "step": 3768,
      "training_loss": 6.3328351974487305
    },
    {
      "epoch": 0.816910569105691,
      "step": 3768,
      "training_loss": 5.322770595550537
    },
    {
      "epoch": 0.816910569105691,
      "step": 3768,
      "training_loss": 6.650444507598877
    },
    {
      "epoch": 0.8171273712737127,
      "step": 3769,
      "training_loss": 6.657006740570068
    },
    {
      "epoch": 0.8171273712737127,
      "step": 3769,
      "training_loss": 7.459305763244629
    },
    {
      "epoch": 0.8171273712737127,
      "step": 3769,
      "training_loss": 5.584043025970459
    },
    {
      "epoch": 0.8171273712737127,
      "step": 3769,
      "training_loss": 3.843096971511841
    },
    {
      "epoch": 0.8173441734417344,
      "step": 3770,
      "training_loss": 6.2315449714660645
    },
    {
      "epoch": 0.8173441734417344,
      "step": 3770,
      "training_loss": 7.219921588897705
    },
    {
      "epoch": 0.8173441734417344,
      "step": 3770,
      "training_loss": 6.302849769592285
    },
    {
      "epoch": 0.8173441734417344,
      "step": 3770,
      "training_loss": 5.9525041580200195
    },
    {
      "epoch": 0.817560975609756,
      "step": 3771,
      "training_loss": 2.388262987136841
    },
    {
      "epoch": 0.817560975609756,
      "step": 3771,
      "training_loss": 3.9970340728759766
    },
    {
      "epoch": 0.817560975609756,
      "step": 3771,
      "training_loss": 6.830162525177002
    },
    {
      "epoch": 0.817560975609756,
      "step": 3771,
      "training_loss": 6.416971206665039
    },
    {
      "epoch": 0.8177777777777778,
      "grad_norm": 18.8776912689209,
      "learning_rate": 1e-05,
      "loss": 5.7648,
      "step": 3772
    },
    {
      "epoch": 0.8177777777777778,
      "step": 3772,
      "training_loss": 7.098219871520996
    },
    {
      "epoch": 0.8177777777777778,
      "step": 3772,
      "training_loss": 3.8270750045776367
    },
    {
      "epoch": 0.8177777777777778,
      "step": 3772,
      "training_loss": 6.748073101043701
    },
    {
      "epoch": 0.8177777777777778,
      "step": 3772,
      "training_loss": 6.12767219543457
    },
    {
      "epoch": 0.8179945799457995,
      "step": 3773,
      "training_loss": 5.8696465492248535
    },
    {
      "epoch": 0.8179945799457995,
      "step": 3773,
      "training_loss": 7.783276557922363
    },
    {
      "epoch": 0.8179945799457995,
      "step": 3773,
      "training_loss": 5.644631862640381
    },
    {
      "epoch": 0.8179945799457995,
      "step": 3773,
      "training_loss": 6.380189895629883
    },
    {
      "epoch": 0.8182113821138212,
      "step": 3774,
      "training_loss": 7.122743129730225
    },
    {
      "epoch": 0.8182113821138212,
      "step": 3774,
      "training_loss": 3.377020835876465
    },
    {
      "epoch": 0.8182113821138212,
      "step": 3774,
      "training_loss": 7.45912504196167
    },
    {
      "epoch": 0.8182113821138212,
      "step": 3774,
      "training_loss": 7.459592342376709
    },
    {
      "epoch": 0.8184281842818428,
      "step": 3775,
      "training_loss": 7.4762654304504395
    },
    {
      "epoch": 0.8184281842818428,
      "step": 3775,
      "training_loss": 7.077271461486816
    },
    {
      "epoch": 0.8184281842818428,
      "step": 3775,
      "training_loss": 6.916840076446533
    },
    {
      "epoch": 0.8184281842818428,
      "step": 3775,
      "training_loss": 6.615346908569336
    },
    {
      "epoch": 0.8186449864498645,
      "grad_norm": 17.81464385986328,
      "learning_rate": 1e-05,
      "loss": 6.4364,
      "step": 3776
    },
    {
      "epoch": 0.8186449864498645,
      "step": 3776,
      "training_loss": 5.445215225219727
    },
    {
      "epoch": 0.8186449864498645,
      "step": 3776,
      "training_loss": 6.619786739349365
    },
    {
      "epoch": 0.8186449864498645,
      "step": 3776,
      "training_loss": 5.683721542358398
    },
    {
      "epoch": 0.8186449864498645,
      "step": 3776,
      "training_loss": 7.527395248413086
    },
    {
      "epoch": 0.8188617886178862,
      "step": 3777,
      "training_loss": 6.632614612579346
    },
    {
      "epoch": 0.8188617886178862,
      "step": 3777,
      "training_loss": 2.9700520038604736
    },
    {
      "epoch": 0.8188617886178862,
      "step": 3777,
      "training_loss": 7.366711139678955
    },
    {
      "epoch": 0.8188617886178862,
      "step": 3777,
      "training_loss": 7.798882484436035
    },
    {
      "epoch": 0.8190785907859078,
      "step": 3778,
      "training_loss": 6.228900909423828
    },
    {
      "epoch": 0.8190785907859078,
      "step": 3778,
      "training_loss": 3.9272563457489014
    },
    {
      "epoch": 0.8190785907859078,
      "step": 3778,
      "training_loss": 5.83372688293457
    },
    {
      "epoch": 0.8190785907859078,
      "step": 3778,
      "training_loss": 5.8889241218566895
    },
    {
      "epoch": 0.8192953929539295,
      "step": 3779,
      "training_loss": 5.196390151977539
    },
    {
      "epoch": 0.8192953929539295,
      "step": 3779,
      "training_loss": 6.833523750305176
    },
    {
      "epoch": 0.8192953929539295,
      "step": 3779,
      "training_loss": 6.341930866241455
    },
    {
      "epoch": 0.8192953929539295,
      "step": 3779,
      "training_loss": 7.48784065246582
    },
    {
      "epoch": 0.8195121951219512,
      "grad_norm": 21.81259536743164,
      "learning_rate": 1e-05,
      "loss": 6.1114,
      "step": 3780
    },
    {
      "epoch": 0.8195121951219512,
      "step": 3780,
      "training_loss": 5.405467510223389
    },
    {
      "epoch": 0.8195121951219512,
      "step": 3780,
      "training_loss": 7.149889945983887
    },
    {
      "epoch": 0.8195121951219512,
      "step": 3780,
      "training_loss": 7.232926845550537
    },
    {
      "epoch": 0.8195121951219512,
      "step": 3780,
      "training_loss": 6.917077541351318
    },
    {
      "epoch": 0.819728997289973,
      "step": 3781,
      "training_loss": 6.571778774261475
    },
    {
      "epoch": 0.819728997289973,
      "step": 3781,
      "training_loss": 5.891940593719482
    },
    {
      "epoch": 0.819728997289973,
      "step": 3781,
      "training_loss": 7.3670783042907715
    },
    {
      "epoch": 0.819728997289973,
      "step": 3781,
      "training_loss": 5.487555980682373
    },
    {
      "epoch": 0.8199457994579946,
      "step": 3782,
      "training_loss": 6.565262794494629
    },
    {
      "epoch": 0.8199457994579946,
      "step": 3782,
      "training_loss": 6.350129127502441
    },
    {
      "epoch": 0.8199457994579946,
      "step": 3782,
      "training_loss": 8.069576263427734
    },
    {
      "epoch": 0.8199457994579946,
      "step": 3782,
      "training_loss": 6.660019874572754
    },
    {
      "epoch": 0.8201626016260163,
      "step": 3783,
      "training_loss": 8.03960132598877
    },
    {
      "epoch": 0.8201626016260163,
      "step": 3783,
      "training_loss": 7.377443790435791
    },
    {
      "epoch": 0.8201626016260163,
      "step": 3783,
      "training_loss": 6.175886154174805
    },
    {
      "epoch": 0.8201626016260163,
      "step": 3783,
      "training_loss": 7.252963542938232
    },
    {
      "epoch": 0.820379403794038,
      "grad_norm": 15.520837783813477,
      "learning_rate": 1e-05,
      "loss": 6.7822,
      "step": 3784
    },
    {
      "epoch": 0.820379403794038,
      "step": 3784,
      "training_loss": 6.164843559265137
    },
    {
      "epoch": 0.820379403794038,
      "step": 3784,
      "training_loss": 6.777253150939941
    },
    {
      "epoch": 0.820379403794038,
      "step": 3784,
      "training_loss": 7.1143798828125
    },
    {
      "epoch": 0.820379403794038,
      "step": 3784,
      "training_loss": 7.536706447601318
    },
    {
      "epoch": 0.8205962059620596,
      "step": 3785,
      "training_loss": 5.2272844314575195
    },
    {
      "epoch": 0.8205962059620596,
      "step": 3785,
      "training_loss": 6.655115604400635
    },
    {
      "epoch": 0.8205962059620596,
      "step": 3785,
      "training_loss": 7.829913139343262
    },
    {
      "epoch": 0.8205962059620596,
      "step": 3785,
      "training_loss": 6.83114767074585
    },
    {
      "epoch": 0.8208130081300813,
      "step": 3786,
      "training_loss": 6.264271259307861
    },
    {
      "epoch": 0.8208130081300813,
      "step": 3786,
      "training_loss": 6.650039196014404
    },
    {
      "epoch": 0.8208130081300813,
      "step": 3786,
      "training_loss": 6.492733478546143
    },
    {
      "epoch": 0.8208130081300813,
      "step": 3786,
      "training_loss": 5.394658088684082
    },
    {
      "epoch": 0.821029810298103,
      "step": 3787,
      "training_loss": 7.360655307769775
    },
    {
      "epoch": 0.821029810298103,
      "step": 3787,
      "training_loss": 6.766369342803955
    },
    {
      "epoch": 0.821029810298103,
      "step": 3787,
      "training_loss": 6.182912826538086
    },
    {
      "epoch": 0.821029810298103,
      "step": 3787,
      "training_loss": 4.999431133270264
    },
    {
      "epoch": 0.8212466124661246,
      "grad_norm": 15.63914680480957,
      "learning_rate": 1e-05,
      "loss": 6.5155,
      "step": 3788
    },
    {
      "epoch": 0.8212466124661246,
      "step": 3788,
      "training_loss": 6.490309715270996
    },
    {
      "epoch": 0.8212466124661246,
      "step": 3788,
      "training_loss": 6.5127434730529785
    },
    {
      "epoch": 0.8212466124661246,
      "step": 3788,
      "training_loss": 7.291016578674316
    },
    {
      "epoch": 0.8212466124661246,
      "step": 3788,
      "training_loss": 7.308122634887695
    },
    {
      "epoch": 0.8214634146341463,
      "step": 3789,
      "training_loss": 6.335877895355225
    },
    {
      "epoch": 0.8214634146341463,
      "step": 3789,
      "training_loss": 2.5366618633270264
    },
    {
      "epoch": 0.8214634146341463,
      "step": 3789,
      "training_loss": 7.237379550933838
    },
    {
      "epoch": 0.8214634146341463,
      "step": 3789,
      "training_loss": 4.078629970550537
    },
    {
      "epoch": 0.8216802168021681,
      "step": 3790,
      "training_loss": 7.225614547729492
    },
    {
      "epoch": 0.8216802168021681,
      "step": 3790,
      "training_loss": 6.110097408294678
    },
    {
      "epoch": 0.8216802168021681,
      "step": 3790,
      "training_loss": 6.3077073097229
    },
    {
      "epoch": 0.8216802168021681,
      "step": 3790,
      "training_loss": 6.652364253997803
    },
    {
      "epoch": 0.8218970189701897,
      "step": 3791,
      "training_loss": 6.317689895629883
    },
    {
      "epoch": 0.8218970189701897,
      "step": 3791,
      "training_loss": 6.872408390045166
    },
    {
      "epoch": 0.8218970189701897,
      "step": 3791,
      "training_loss": 5.565576076507568
    },
    {
      "epoch": 0.8218970189701897,
      "step": 3791,
      "training_loss": 6.257495880126953
    },
    {
      "epoch": 0.8221138211382114,
      "grad_norm": 20.44025421142578,
      "learning_rate": 1e-05,
      "loss": 6.1937,
      "step": 3792
    },
    {
      "epoch": 0.8221138211382114,
      "step": 3792,
      "training_loss": 7.02858829498291
    },
    {
      "epoch": 0.8221138211382114,
      "step": 3792,
      "training_loss": 7.0462493896484375
    },
    {
      "epoch": 0.8221138211382114,
      "step": 3792,
      "training_loss": 8.199163436889648
    },
    {
      "epoch": 0.8221138211382114,
      "step": 3792,
      "training_loss": 5.955174446105957
    },
    {
      "epoch": 0.8223306233062331,
      "step": 3793,
      "training_loss": 6.911552906036377
    },
    {
      "epoch": 0.8223306233062331,
      "step": 3793,
      "training_loss": 6.044580936431885
    },
    {
      "epoch": 0.8223306233062331,
      "step": 3793,
      "training_loss": 7.512726306915283
    },
    {
      "epoch": 0.8223306233062331,
      "step": 3793,
      "training_loss": 6.985048294067383
    },
    {
      "epoch": 0.8225474254742547,
      "step": 3794,
      "training_loss": 6.488970756530762
    },
    {
      "epoch": 0.8225474254742547,
      "step": 3794,
      "training_loss": 7.10756778717041
    },
    {
      "epoch": 0.8225474254742547,
      "step": 3794,
      "training_loss": 7.744505405426025
    },
    {
      "epoch": 0.8225474254742547,
      "step": 3794,
      "training_loss": 6.575782775878906
    },
    {
      "epoch": 0.8227642276422764,
      "step": 3795,
      "training_loss": 3.1074399948120117
    },
    {
      "epoch": 0.8227642276422764,
      "step": 3795,
      "training_loss": 5.69075870513916
    },
    {
      "epoch": 0.8227642276422764,
      "step": 3795,
      "training_loss": 6.830564975738525
    },
    {
      "epoch": 0.8227642276422764,
      "step": 3795,
      "training_loss": 6.724997043609619
    },
    {
      "epoch": 0.8229810298102981,
      "grad_norm": 15.958664894104004,
      "learning_rate": 1e-05,
      "loss": 6.6221,
      "step": 3796
    },
    {
      "epoch": 0.8229810298102981,
      "step": 3796,
      "training_loss": 5.465832710266113
    },
    {
      "epoch": 0.8229810298102981,
      "step": 3796,
      "training_loss": 6.807333946228027
    },
    {
      "epoch": 0.8229810298102981,
      "step": 3796,
      "training_loss": 5.331972122192383
    },
    {
      "epoch": 0.8229810298102981,
      "step": 3796,
      "training_loss": 7.268885135650635
    },
    {
      "epoch": 0.8231978319783197,
      "step": 3797,
      "training_loss": 7.57692813873291
    },
    {
      "epoch": 0.8231978319783197,
      "step": 3797,
      "training_loss": 6.497930526733398
    },
    {
      "epoch": 0.8231978319783197,
      "step": 3797,
      "training_loss": 3.6822919845581055
    },
    {
      "epoch": 0.8231978319783197,
      "step": 3797,
      "training_loss": 6.3190531730651855
    },
    {
      "epoch": 0.8234146341463414,
      "step": 3798,
      "training_loss": 4.53042459487915
    },
    {
      "epoch": 0.8234146341463414,
      "step": 3798,
      "training_loss": 5.460669040679932
    },
    {
      "epoch": 0.8234146341463414,
      "step": 3798,
      "training_loss": 6.1135053634643555
    },
    {
      "epoch": 0.8234146341463414,
      "step": 3798,
      "training_loss": 7.449766159057617
    },
    {
      "epoch": 0.8236314363143632,
      "step": 3799,
      "training_loss": 7.0627312660217285
    },
    {
      "epoch": 0.8236314363143632,
      "step": 3799,
      "training_loss": 5.790435314178467
    },
    {
      "epoch": 0.8236314363143632,
      "step": 3799,
      "training_loss": 5.110747337341309
    },
    {
      "epoch": 0.8236314363143632,
      "step": 3799,
      "training_loss": 6.902023792266846
    },
    {
      "epoch": 0.8238482384823849,
      "grad_norm": 19.19352149963379,
      "learning_rate": 1e-05,
      "loss": 6.0857,
      "step": 3800
    },
    {
      "epoch": 0.8238482384823849,
      "step": 3800,
      "training_loss": 6.636874198913574
    },
    {
      "epoch": 0.8238482384823849,
      "step": 3800,
      "training_loss": 5.44231653213501
    },
    {
      "epoch": 0.8238482384823849,
      "step": 3800,
      "training_loss": 5.550671100616455
    },
    {
      "epoch": 0.8238482384823849,
      "step": 3800,
      "training_loss": 3.7039880752563477
    },
    {
      "epoch": 0.8240650406504065,
      "step": 3801,
      "training_loss": 6.394766807556152
    },
    {
      "epoch": 0.8240650406504065,
      "step": 3801,
      "training_loss": 5.792722225189209
    },
    {
      "epoch": 0.8240650406504065,
      "step": 3801,
      "training_loss": 6.775758743286133
    },
    {
      "epoch": 0.8240650406504065,
      "step": 3801,
      "training_loss": 7.552015781402588
    },
    {
      "epoch": 0.8242818428184282,
      "step": 3802,
      "training_loss": 4.981088638305664
    },
    {
      "epoch": 0.8242818428184282,
      "step": 3802,
      "training_loss": 6.7261128425598145
    },
    {
      "epoch": 0.8242818428184282,
      "step": 3802,
      "training_loss": 7.131762981414795
    },
    {
      "epoch": 0.8242818428184282,
      "step": 3802,
      "training_loss": 7.02752685546875
    },
    {
      "epoch": 0.8244986449864499,
      "step": 3803,
      "training_loss": 7.027683258056641
    },
    {
      "epoch": 0.8244986449864499,
      "step": 3803,
      "training_loss": 6.673300266265869
    },
    {
      "epoch": 0.8244986449864499,
      "step": 3803,
      "training_loss": 6.753472805023193
    },
    {
      "epoch": 0.8244986449864499,
      "step": 3803,
      "training_loss": 5.255073547363281
    },
    {
      "epoch": 0.8247154471544715,
      "grad_norm": 15.913341522216797,
      "learning_rate": 1e-05,
      "loss": 6.2141,
      "step": 3804
    },
    {
      "epoch": 0.8247154471544715,
      "step": 3804,
      "training_loss": 7.493081569671631
    },
    {
      "epoch": 0.8247154471544715,
      "step": 3804,
      "training_loss": 7.040560245513916
    },
    {
      "epoch": 0.8247154471544715,
      "step": 3804,
      "training_loss": 5.55238676071167
    },
    {
      "epoch": 0.8247154471544715,
      "step": 3804,
      "training_loss": 6.170822620391846
    },
    {
      "epoch": 0.8249322493224932,
      "step": 3805,
      "training_loss": 6.568914413452148
    },
    {
      "epoch": 0.8249322493224932,
      "step": 3805,
      "training_loss": 6.625845432281494
    },
    {
      "epoch": 0.8249322493224932,
      "step": 3805,
      "training_loss": 3.8462440967559814
    },
    {
      "epoch": 0.8249322493224932,
      "step": 3805,
      "training_loss": 6.448005199432373
    },
    {
      "epoch": 0.8251490514905149,
      "step": 3806,
      "training_loss": 6.324843406677246
    },
    {
      "epoch": 0.8251490514905149,
      "step": 3806,
      "training_loss": 4.96551513671875
    },
    {
      "epoch": 0.8251490514905149,
      "step": 3806,
      "training_loss": 6.396294116973877
    },
    {
      "epoch": 0.8251490514905149,
      "step": 3806,
      "training_loss": 4.036387920379639
    },
    {
      "epoch": 0.8253658536585365,
      "step": 3807,
      "training_loss": 6.2006916999816895
    },
    {
      "epoch": 0.8253658536585365,
      "step": 3807,
      "training_loss": 6.465648651123047
    },
    {
      "epoch": 0.8253658536585365,
      "step": 3807,
      "training_loss": 6.4417853355407715
    },
    {
      "epoch": 0.8253658536585365,
      "step": 3807,
      "training_loss": 7.41798734664917
    },
    {
      "epoch": 0.8255826558265583,
      "grad_norm": 16.348360061645508,
      "learning_rate": 1e-05,
      "loss": 6.1247,
      "step": 3808
    },
    {
      "epoch": 0.8255826558265583,
      "step": 3808,
      "training_loss": 5.90945291519165
    },
    {
      "epoch": 0.8255826558265583,
      "step": 3808,
      "training_loss": 6.753117561340332
    },
    {
      "epoch": 0.8255826558265583,
      "step": 3808,
      "training_loss": 6.481978416442871
    },
    {
      "epoch": 0.8255826558265583,
      "step": 3808,
      "training_loss": 5.87685489654541
    },
    {
      "epoch": 0.82579945799458,
      "step": 3809,
      "training_loss": 6.985814094543457
    },
    {
      "epoch": 0.82579945799458,
      "step": 3809,
      "training_loss": 5.550373077392578
    },
    {
      "epoch": 0.82579945799458,
      "step": 3809,
      "training_loss": 6.289988040924072
    },
    {
      "epoch": 0.82579945799458,
      "step": 3809,
      "training_loss": 6.9770660400390625
    },
    {
      "epoch": 0.8260162601626017,
      "step": 3810,
      "training_loss": 7.156415939331055
    },
    {
      "epoch": 0.8260162601626017,
      "step": 3810,
      "training_loss": 6.343870162963867
    },
    {
      "epoch": 0.8260162601626017,
      "step": 3810,
      "training_loss": 5.991859436035156
    },
    {
      "epoch": 0.8260162601626017,
      "step": 3810,
      "training_loss": 5.373680114746094
    },
    {
      "epoch": 0.8262330623306233,
      "step": 3811,
      "training_loss": 3.7705273628234863
    },
    {
      "epoch": 0.8262330623306233,
      "step": 3811,
      "training_loss": 4.3229804039001465
    },
    {
      "epoch": 0.8262330623306233,
      "step": 3811,
      "training_loss": 4.759527206420898
    },
    {
      "epoch": 0.8262330623306233,
      "step": 3811,
      "training_loss": 6.466982364654541
    },
    {
      "epoch": 0.826449864498645,
      "grad_norm": 18.158533096313477,
      "learning_rate": 1e-05,
      "loss": 5.9382,
      "step": 3812
    },
    {
      "epoch": 0.826449864498645,
      "step": 3812,
      "training_loss": 5.597960472106934
    },
    {
      "epoch": 0.826449864498645,
      "step": 3812,
      "training_loss": 4.538228511810303
    },
    {
      "epoch": 0.826449864498645,
      "step": 3812,
      "training_loss": 3.6147103309631348
    },
    {
      "epoch": 0.826449864498645,
      "step": 3812,
      "training_loss": 6.3199896812438965
    },
    {
      "epoch": 0.8266666666666667,
      "step": 3813,
      "training_loss": 7.663644790649414
    },
    {
      "epoch": 0.8266666666666667,
      "step": 3813,
      "training_loss": 4.024979591369629
    },
    {
      "epoch": 0.8266666666666667,
      "step": 3813,
      "training_loss": 4.195939540863037
    },
    {
      "epoch": 0.8266666666666667,
      "step": 3813,
      "training_loss": 7.465378284454346
    },
    {
      "epoch": 0.8268834688346883,
      "step": 3814,
      "training_loss": 6.903275489807129
    },
    {
      "epoch": 0.8268834688346883,
      "step": 3814,
      "training_loss": 7.269750595092773
    },
    {
      "epoch": 0.8268834688346883,
      "step": 3814,
      "training_loss": 5.465851306915283
    },
    {
      "epoch": 0.8268834688346883,
      "step": 3814,
      "training_loss": 5.239226341247559
    },
    {
      "epoch": 0.82710027100271,
      "step": 3815,
      "training_loss": 5.846135139465332
    },
    {
      "epoch": 0.82710027100271,
      "step": 3815,
      "training_loss": 6.200965404510498
    },
    {
      "epoch": 0.82710027100271,
      "step": 3815,
      "training_loss": 6.647106647491455
    },
    {
      "epoch": 0.82710027100271,
      "step": 3815,
      "training_loss": 5.544479846954346
    },
    {
      "epoch": 0.8273170731707317,
      "grad_norm": 15.679332733154297,
      "learning_rate": 1e-05,
      "loss": 5.7836,
      "step": 3816
    },
    {
      "epoch": 0.8273170731707317,
      "step": 3816,
      "training_loss": 6.537381649017334
    },
    {
      "epoch": 0.8273170731707317,
      "step": 3816,
      "training_loss": 6.789761543273926
    },
    {
      "epoch": 0.8273170731707317,
      "step": 3816,
      "training_loss": 5.816272258758545
    },
    {
      "epoch": 0.8273170731707317,
      "step": 3816,
      "training_loss": 6.0217509269714355
    },
    {
      "epoch": 0.8275338753387533,
      "step": 3817,
      "training_loss": 7.567430019378662
    },
    {
      "epoch": 0.8275338753387533,
      "step": 3817,
      "training_loss": 5.763443946838379
    },
    {
      "epoch": 0.8275338753387533,
      "step": 3817,
      "training_loss": 6.574007987976074
    },
    {
      "epoch": 0.8275338753387533,
      "step": 3817,
      "training_loss": 6.816027641296387
    },
    {
      "epoch": 0.8277506775067751,
      "step": 3818,
      "training_loss": 3.2221953868865967
    },
    {
      "epoch": 0.8277506775067751,
      "step": 3818,
      "training_loss": 6.948816776275635
    },
    {
      "epoch": 0.8277506775067751,
      "step": 3818,
      "training_loss": 8.42795467376709
    },
    {
      "epoch": 0.8277506775067751,
      "step": 3818,
      "training_loss": 7.673153877258301
    },
    {
      "epoch": 0.8279674796747968,
      "step": 3819,
      "training_loss": 6.589739799499512
    },
    {
      "epoch": 0.8279674796747968,
      "step": 3819,
      "training_loss": 7.134622097015381
    },
    {
      "epoch": 0.8279674796747968,
      "step": 3819,
      "training_loss": 6.971002101898193
    },
    {
      "epoch": 0.8279674796747968,
      "step": 3819,
      "training_loss": 3.0630104541778564
    },
    {
      "epoch": 0.8281842818428184,
      "grad_norm": 15.975234031677246,
      "learning_rate": 1e-05,
      "loss": 6.3698,
      "step": 3820
    },
    {
      "epoch": 0.8281842818428184,
      "step": 3820,
      "training_loss": 3.9282963275909424
    },
    {
      "epoch": 0.8281842818428184,
      "step": 3820,
      "training_loss": 7.224200248718262
    },
    {
      "epoch": 0.8281842818428184,
      "step": 3820,
      "training_loss": 9.062050819396973
    },
    {
      "epoch": 0.8281842818428184,
      "step": 3820,
      "training_loss": 4.156650066375732
    },
    {
      "epoch": 0.8284010840108401,
      "step": 3821,
      "training_loss": 6.271923542022705
    },
    {
      "epoch": 0.8284010840108401,
      "step": 3821,
      "training_loss": 5.542883396148682
    },
    {
      "epoch": 0.8284010840108401,
      "step": 3821,
      "training_loss": 6.655983924865723
    },
    {
      "epoch": 0.8284010840108401,
      "step": 3821,
      "training_loss": 8.494837760925293
    },
    {
      "epoch": 0.8286178861788618,
      "step": 3822,
      "training_loss": 7.763512134552002
    },
    {
      "epoch": 0.8286178861788618,
      "step": 3822,
      "training_loss": 4.282705307006836
    },
    {
      "epoch": 0.8286178861788618,
      "step": 3822,
      "training_loss": 6.279926776885986
    },
    {
      "epoch": 0.8286178861788618,
      "step": 3822,
      "training_loss": 6.014429092407227
    },
    {
      "epoch": 0.8288346883468835,
      "step": 3823,
      "training_loss": 7.565658092498779
    },
    {
      "epoch": 0.8288346883468835,
      "step": 3823,
      "training_loss": 5.545292377471924
    },
    {
      "epoch": 0.8288346883468835,
      "step": 3823,
      "training_loss": 7.0243144035339355
    },
    {
      "epoch": 0.8288346883468835,
      "step": 3823,
      "training_loss": 5.046535491943359
    },
    {
      "epoch": 0.8290514905149051,
      "grad_norm": 18.781946182250977,
      "learning_rate": 1e-05,
      "loss": 6.3037,
      "step": 3824
    },
    {
      "epoch": 0.8290514905149051,
      "step": 3824,
      "training_loss": 5.868270397186279
    },
    {
      "epoch": 0.8290514905149051,
      "step": 3824,
      "training_loss": 3.4042038917541504
    },
    {
      "epoch": 0.8290514905149051,
      "step": 3824,
      "training_loss": 6.731259822845459
    },
    {
      "epoch": 0.8290514905149051,
      "step": 3824,
      "training_loss": 6.596724987030029
    },
    {
      "epoch": 0.8292682926829268,
      "step": 3825,
      "training_loss": 6.744362831115723
    },
    {
      "epoch": 0.8292682926829268,
      "step": 3825,
      "training_loss": 5.999807834625244
    },
    {
      "epoch": 0.8292682926829268,
      "step": 3825,
      "training_loss": 6.481104373931885
    },
    {
      "epoch": 0.8292682926829268,
      "step": 3825,
      "training_loss": 7.062436103820801
    },
    {
      "epoch": 0.8294850948509485,
      "step": 3826,
      "training_loss": 6.266270637512207
    },
    {
      "epoch": 0.8294850948509485,
      "step": 3826,
      "training_loss": 7.160979270935059
    },
    {
      "epoch": 0.8294850948509485,
      "step": 3826,
      "training_loss": 7.391218662261963
    },
    {
      "epoch": 0.8294850948509485,
      "step": 3826,
      "training_loss": 6.9404215812683105
    },
    {
      "epoch": 0.8297018970189702,
      "step": 3827,
      "training_loss": 6.094599723815918
    },
    {
      "epoch": 0.8297018970189702,
      "step": 3827,
      "training_loss": 6.614322185516357
    },
    {
      "epoch": 0.8297018970189702,
      "step": 3827,
      "training_loss": 4.603211402893066
    },
    {
      "epoch": 0.8297018970189702,
      "step": 3827,
      "training_loss": 4.446225166320801
    },
    {
      "epoch": 0.8299186991869919,
      "grad_norm": 26.781944274902344,
      "learning_rate": 1e-05,
      "loss": 6.1503,
      "step": 3828
    },
    {
      "epoch": 0.8299186991869919,
      "step": 3828,
      "training_loss": 6.854428768157959
    },
    {
      "epoch": 0.8299186991869919,
      "step": 3828,
      "training_loss": 6.186282157897949
    },
    {
      "epoch": 0.8299186991869919,
      "step": 3828,
      "training_loss": 8.189248085021973
    },
    {
      "epoch": 0.8299186991869919,
      "step": 3828,
      "training_loss": 7.352781772613525
    },
    {
      "epoch": 0.8301355013550136,
      "step": 3829,
      "training_loss": 5.446475505828857
    },
    {
      "epoch": 0.8301355013550136,
      "step": 3829,
      "training_loss": 5.88522481918335
    },
    {
      "epoch": 0.8301355013550136,
      "step": 3829,
      "training_loss": 6.452823638916016
    },
    {
      "epoch": 0.8301355013550136,
      "step": 3829,
      "training_loss": 4.251806259155273
    },
    {
      "epoch": 0.8303523035230352,
      "step": 3830,
      "training_loss": 7.372973442077637
    },
    {
      "epoch": 0.8303523035230352,
      "step": 3830,
      "training_loss": 5.9339399337768555
    },
    {
      "epoch": 0.8303523035230352,
      "step": 3830,
      "training_loss": 6.663637161254883
    },
    {
      "epoch": 0.8303523035230352,
      "step": 3830,
      "training_loss": 7.774499893188477
    },
    {
      "epoch": 0.8305691056910569,
      "step": 3831,
      "training_loss": 6.711503505706787
    },
    {
      "epoch": 0.8305691056910569,
      "step": 3831,
      "training_loss": 5.608346939086914
    },
    {
      "epoch": 0.8305691056910569,
      "step": 3831,
      "training_loss": 7.338199138641357
    },
    {
      "epoch": 0.8305691056910569,
      "step": 3831,
      "training_loss": 7.349916934967041
    },
    {
      "epoch": 0.8307859078590786,
      "grad_norm": 26.425756454467773,
      "learning_rate": 1e-05,
      "loss": 6.5858,
      "step": 3832
    },
    {
      "epoch": 0.8307859078590786,
      "step": 3832,
      "training_loss": 6.022895336151123
    },
    {
      "epoch": 0.8307859078590786,
      "step": 3832,
      "training_loss": 8.017752647399902
    },
    {
      "epoch": 0.8307859078590786,
      "step": 3832,
      "training_loss": 2.966611385345459
    },
    {
      "epoch": 0.8307859078590786,
      "step": 3832,
      "training_loss": 6.909867763519287
    },
    {
      "epoch": 0.8310027100271002,
      "step": 3833,
      "training_loss": 5.7911057472229
    },
    {
      "epoch": 0.8310027100271002,
      "step": 3833,
      "training_loss": 7.360097885131836
    },
    {
      "epoch": 0.8310027100271002,
      "step": 3833,
      "training_loss": 7.506741523742676
    },
    {
      "epoch": 0.8310027100271002,
      "step": 3833,
      "training_loss": 5.586696147918701
    },
    {
      "epoch": 0.8312195121951219,
      "step": 3834,
      "training_loss": 7.2130866050720215
    },
    {
      "epoch": 0.8312195121951219,
      "step": 3834,
      "training_loss": 6.628969192504883
    },
    {
      "epoch": 0.8312195121951219,
      "step": 3834,
      "training_loss": 7.587160110473633
    },
    {
      "epoch": 0.8312195121951219,
      "step": 3834,
      "training_loss": 6.770514488220215
    },
    {
      "epoch": 0.8314363143631436,
      "step": 3835,
      "training_loss": 5.381650447845459
    },
    {
      "epoch": 0.8314363143631436,
      "step": 3835,
      "training_loss": 6.715770244598389
    },
    {
      "epoch": 0.8314363143631436,
      "step": 3835,
      "training_loss": 7.194040298461914
    },
    {
      "epoch": 0.8314363143631436,
      "step": 3835,
      "training_loss": 6.516870498657227
    },
    {
      "epoch": 0.8316531165311654,
      "grad_norm": 19.8343563079834,
      "learning_rate": 1e-05,
      "loss": 6.5106,
      "step": 3836
    },
    {
      "epoch": 0.8316531165311654,
      "step": 3836,
      "training_loss": 7.5562896728515625
    },
    {
      "epoch": 0.8316531165311654,
      "step": 3836,
      "training_loss": 5.857377052307129
    },
    {
      "epoch": 0.8316531165311654,
      "step": 3836,
      "training_loss": 6.945571422576904
    },
    {
      "epoch": 0.8316531165311654,
      "step": 3836,
      "training_loss": 7.089734077453613
    },
    {
      "epoch": 0.831869918699187,
      "step": 3837,
      "training_loss": 7.23581075668335
    },
    {
      "epoch": 0.831869918699187,
      "step": 3837,
      "training_loss": 5.7981486320495605
    },
    {
      "epoch": 0.831869918699187,
      "step": 3837,
      "training_loss": 5.449441432952881
    },
    {
      "epoch": 0.831869918699187,
      "step": 3837,
      "training_loss": 7.114501953125
    },
    {
      "epoch": 0.8320867208672087,
      "step": 3838,
      "training_loss": 8.246021270751953
    },
    {
      "epoch": 0.8320867208672087,
      "step": 3838,
      "training_loss": 7.602269172668457
    },
    {
      "epoch": 0.8320867208672087,
      "step": 3838,
      "training_loss": 6.4984307289123535
    },
    {
      "epoch": 0.8320867208672087,
      "step": 3838,
      "training_loss": 6.276745796203613
    },
    {
      "epoch": 0.8323035230352304,
      "step": 3839,
      "training_loss": 6.392151832580566
    },
    {
      "epoch": 0.8323035230352304,
      "step": 3839,
      "training_loss": 6.801122188568115
    },
    {
      "epoch": 0.8323035230352304,
      "step": 3839,
      "training_loss": 8.410757064819336
    },
    {
      "epoch": 0.8323035230352304,
      "step": 3839,
      "training_loss": 7.889017581939697
    },
    {
      "epoch": 0.832520325203252,
      "grad_norm": 18.891902923583984,
      "learning_rate": 1e-05,
      "loss": 6.9477,
      "step": 3840
    },
    {
      "epoch": 0.832520325203252,
      "step": 3840,
      "training_loss": 6.875955104827881
    },
    {
      "epoch": 0.832520325203252,
      "step": 3840,
      "training_loss": 4.74068021774292
    },
    {
      "epoch": 0.832520325203252,
      "step": 3840,
      "training_loss": 6.068660259246826
    },
    {
      "epoch": 0.832520325203252,
      "step": 3840,
      "training_loss": 5.565470218658447
    },
    {
      "epoch": 0.8327371273712737,
      "step": 3841,
      "training_loss": 6.906855583190918
    },
    {
      "epoch": 0.8327371273712737,
      "step": 3841,
      "training_loss": 4.423534393310547
    },
    {
      "epoch": 0.8327371273712737,
      "step": 3841,
      "training_loss": 6.555243968963623
    },
    {
      "epoch": 0.8327371273712737,
      "step": 3841,
      "training_loss": 5.77750301361084
    },
    {
      "epoch": 0.8329539295392954,
      "step": 3842,
      "training_loss": 6.806594371795654
    },
    {
      "epoch": 0.8329539295392954,
      "step": 3842,
      "training_loss": 5.086381912231445
    },
    {
      "epoch": 0.8329539295392954,
      "step": 3842,
      "training_loss": 6.55825138092041
    },
    {
      "epoch": 0.8329539295392954,
      "step": 3842,
      "training_loss": 6.448800563812256
    },
    {
      "epoch": 0.833170731707317,
      "step": 3843,
      "training_loss": 6.813445568084717
    },
    {
      "epoch": 0.833170731707317,
      "step": 3843,
      "training_loss": 5.624787330627441
    },
    {
      "epoch": 0.833170731707317,
      "step": 3843,
      "training_loss": 7.114436626434326
    },
    {
      "epoch": 0.833170731707317,
      "step": 3843,
      "training_loss": 5.285130977630615
    },
    {
      "epoch": 0.8333875338753387,
      "grad_norm": 17.35015296936035,
      "learning_rate": 1e-05,
      "loss": 6.0407,
      "step": 3844
    },
    {
      "epoch": 0.8333875338753387,
      "step": 3844,
      "training_loss": 6.777787208557129
    },
    {
      "epoch": 0.8333875338753387,
      "step": 3844,
      "training_loss": 6.85648250579834
    },
    {
      "epoch": 0.8333875338753387,
      "step": 3844,
      "training_loss": 6.251249313354492
    },
    {
      "epoch": 0.8333875338753387,
      "step": 3844,
      "training_loss": 4.472784519195557
    },
    {
      "epoch": 0.8336043360433605,
      "step": 3845,
      "training_loss": 5.4521331787109375
    },
    {
      "epoch": 0.8336043360433605,
      "step": 3845,
      "training_loss": 6.8277668952941895
    },
    {
      "epoch": 0.8336043360433605,
      "step": 3845,
      "training_loss": 7.241846561431885
    },
    {
      "epoch": 0.8336043360433605,
      "step": 3845,
      "training_loss": 7.5821051597595215
    },
    {
      "epoch": 0.8338211382113822,
      "step": 3846,
      "training_loss": 6.556726455688477
    },
    {
      "epoch": 0.8338211382113822,
      "step": 3846,
      "training_loss": 5.289438247680664
    },
    {
      "epoch": 0.8338211382113822,
      "step": 3846,
      "training_loss": 5.465566635131836
    },
    {
      "epoch": 0.8338211382113822,
      "step": 3846,
      "training_loss": 5.43950891494751
    },
    {
      "epoch": 0.8340379403794038,
      "step": 3847,
      "training_loss": 7.922296047210693
    },
    {
      "epoch": 0.8340379403794038,
      "step": 3847,
      "training_loss": 3.8641035556793213
    },
    {
      "epoch": 0.8340379403794038,
      "step": 3847,
      "training_loss": 6.288332462310791
    },
    {
      "epoch": 0.8340379403794038,
      "step": 3847,
      "training_loss": 6.142618656158447
    },
    {
      "epoch": 0.8342547425474255,
      "grad_norm": 13.912725448608398,
      "learning_rate": 1e-05,
      "loss": 6.1519,
      "step": 3848
    },
    {
      "epoch": 0.8342547425474255,
      "step": 3848,
      "training_loss": 6.1865153312683105
    },
    {
      "epoch": 0.8342547425474255,
      "step": 3848,
      "training_loss": 7.730445384979248
    },
    {
      "epoch": 0.8342547425474255,
      "step": 3848,
      "training_loss": 5.402348518371582
    },
    {
      "epoch": 0.8342547425474255,
      "step": 3848,
      "training_loss": 5.808836936950684
    },
    {
      "epoch": 0.8344715447154472,
      "step": 3849,
      "training_loss": 7.4737701416015625
    },
    {
      "epoch": 0.8344715447154472,
      "step": 3849,
      "training_loss": 6.09251070022583
    },
    {
      "epoch": 0.8344715447154472,
      "step": 3849,
      "training_loss": 8.105978012084961
    },
    {
      "epoch": 0.8344715447154472,
      "step": 3849,
      "training_loss": 6.380435466766357
    },
    {
      "epoch": 0.8346883468834688,
      "step": 3850,
      "training_loss": 7.672928810119629
    },
    {
      "epoch": 0.8346883468834688,
      "step": 3850,
      "training_loss": 6.713436603546143
    },
    {
      "epoch": 0.8346883468834688,
      "step": 3850,
      "training_loss": 7.7696099281311035
    },
    {
      "epoch": 0.8346883468834688,
      "step": 3850,
      "training_loss": 8.519659996032715
    },
    {
      "epoch": 0.8349051490514905,
      "step": 3851,
      "training_loss": 6.627467155456543
    },
    {
      "epoch": 0.8349051490514905,
      "step": 3851,
      "training_loss": 6.759871959686279
    },
    {
      "epoch": 0.8349051490514905,
      "step": 3851,
      "training_loss": 4.856338977813721
    },
    {
      "epoch": 0.8349051490514905,
      "step": 3851,
      "training_loss": 6.903568267822266
    },
    {
      "epoch": 0.8351219512195122,
      "grad_norm": 13.85033893585205,
      "learning_rate": 1e-05,
      "loss": 6.8127,
      "step": 3852
    },
    {
      "epoch": 0.8351219512195122,
      "step": 3852,
      "training_loss": 7.682774066925049
    },
    {
      "epoch": 0.8351219512195122,
      "step": 3852,
      "training_loss": 7.000702857971191
    },
    {
      "epoch": 0.8351219512195122,
      "step": 3852,
      "training_loss": 6.291975975036621
    },
    {
      "epoch": 0.8351219512195122,
      "step": 3852,
      "training_loss": 4.931145191192627
    },
    {
      "epoch": 0.8353387533875338,
      "step": 3853,
      "training_loss": 6.548928260803223
    },
    {
      "epoch": 0.8353387533875338,
      "step": 3853,
      "training_loss": 6.672292709350586
    },
    {
      "epoch": 0.8353387533875338,
      "step": 3853,
      "training_loss": 6.143687725067139
    },
    {
      "epoch": 0.8353387533875338,
      "step": 3853,
      "training_loss": 5.079452037811279
    },
    {
      "epoch": 0.8355555555555556,
      "step": 3854,
      "training_loss": 4.5113935470581055
    },
    {
      "epoch": 0.8355555555555556,
      "step": 3854,
      "training_loss": 7.150919437408447
    },
    {
      "epoch": 0.8355555555555556,
      "step": 3854,
      "training_loss": 7.1941022872924805
    },
    {
      "epoch": 0.8355555555555556,
      "step": 3854,
      "training_loss": 6.557495594024658
    },
    {
      "epoch": 0.8357723577235773,
      "step": 3855,
      "training_loss": 4.6168975830078125
    },
    {
      "epoch": 0.8357723577235773,
      "step": 3855,
      "training_loss": 6.713906288146973
    },
    {
      "epoch": 0.8357723577235773,
      "step": 3855,
      "training_loss": 6.699481964111328
    },
    {
      "epoch": 0.8357723577235773,
      "step": 3855,
      "training_loss": 5.280875205993652
    },
    {
      "epoch": 0.8359891598915989,
      "grad_norm": 25.25060272216797,
      "learning_rate": 1e-05,
      "loss": 6.1923,
      "step": 3856
    },
    {
      "epoch": 0.8359891598915989,
      "step": 3856,
      "training_loss": 7.369061470031738
    },
    {
      "epoch": 0.8359891598915989,
      "step": 3856,
      "training_loss": 3.064682960510254
    },
    {
      "epoch": 0.8359891598915989,
      "step": 3856,
      "training_loss": 7.770781517028809
    },
    {
      "epoch": 0.8359891598915989,
      "step": 3856,
      "training_loss": 6.029193878173828
    },
    {
      "epoch": 0.8362059620596206,
      "step": 3857,
      "training_loss": 6.033410549163818
    },
    {
      "epoch": 0.8362059620596206,
      "step": 3857,
      "training_loss": 6.8612565994262695
    },
    {
      "epoch": 0.8362059620596206,
      "step": 3857,
      "training_loss": 4.765037536621094
    },
    {
      "epoch": 0.8362059620596206,
      "step": 3857,
      "training_loss": 6.582501411437988
    },
    {
      "epoch": 0.8364227642276423,
      "step": 3858,
      "training_loss": 7.031466007232666
    },
    {
      "epoch": 0.8364227642276423,
      "step": 3858,
      "training_loss": 11.35061264038086
    },
    {
      "epoch": 0.8364227642276423,
      "step": 3858,
      "training_loss": 6.896235466003418
    },
    {
      "epoch": 0.8364227642276423,
      "step": 3858,
      "training_loss": 6.601344108581543
    },
    {
      "epoch": 0.836639566395664,
      "step": 3859,
      "training_loss": 7.212979316711426
    },
    {
      "epoch": 0.836639566395664,
      "step": 3859,
      "training_loss": 5.904487609863281
    },
    {
      "epoch": 0.836639566395664,
      "step": 3859,
      "training_loss": 4.659023761749268
    },
    {
      "epoch": 0.836639566395664,
      "step": 3859,
      "training_loss": 7.147500991821289
    },
    {
      "epoch": 0.8368563685636856,
      "grad_norm": 19.781057357788086,
      "learning_rate": 1e-05,
      "loss": 6.58,
      "step": 3860
    },
    {
      "epoch": 0.8368563685636856,
      "step": 3860,
      "training_loss": 6.499497890472412
    },
    {
      "epoch": 0.8368563685636856,
      "step": 3860,
      "training_loss": 5.826554775238037
    },
    {
      "epoch": 0.8368563685636856,
      "step": 3860,
      "training_loss": 7.668145656585693
    },
    {
      "epoch": 0.8368563685636856,
      "step": 3860,
      "training_loss": 7.164709568023682
    },
    {
      "epoch": 0.8370731707317073,
      "step": 3861,
      "training_loss": 6.426096439361572
    },
    {
      "epoch": 0.8370731707317073,
      "step": 3861,
      "training_loss": 6.539495944976807
    },
    {
      "epoch": 0.8370731707317073,
      "step": 3861,
      "training_loss": 6.377352714538574
    },
    {
      "epoch": 0.8370731707317073,
      "step": 3861,
      "training_loss": 5.452984809875488
    },
    {
      "epoch": 0.837289972899729,
      "step": 3862,
      "training_loss": 6.644419193267822
    },
    {
      "epoch": 0.837289972899729,
      "step": 3862,
      "training_loss": 6.745818138122559
    },
    {
      "epoch": 0.837289972899729,
      "step": 3862,
      "training_loss": 5.48045015335083
    },
    {
      "epoch": 0.837289972899729,
      "step": 3862,
      "training_loss": 6.853044509887695
    },
    {
      "epoch": 0.8375067750677507,
      "step": 3863,
      "training_loss": 7.037569999694824
    },
    {
      "epoch": 0.8375067750677507,
      "step": 3863,
      "training_loss": 7.502862453460693
    },
    {
      "epoch": 0.8375067750677507,
      "step": 3863,
      "training_loss": 4.624444007873535
    },
    {
      "epoch": 0.8375067750677507,
      "step": 3863,
      "training_loss": 5.343074321746826
    },
    {
      "epoch": 0.8377235772357724,
      "grad_norm": 27.735475540161133,
      "learning_rate": 1e-05,
      "loss": 6.3867,
      "step": 3864
    },
    {
      "epoch": 0.8377235772357724,
      "step": 3864,
      "training_loss": 4.575331687927246
    },
    {
      "epoch": 0.8377235772357724,
      "step": 3864,
      "training_loss": 6.644775390625
    },
    {
      "epoch": 0.8377235772357724,
      "step": 3864,
      "training_loss": 7.027302265167236
    },
    {
      "epoch": 0.8377235772357724,
      "step": 3864,
      "training_loss": 6.696225643157959
    },
    {
      "epoch": 0.8379403794037941,
      "step": 3865,
      "training_loss": 6.492098808288574
    },
    {
      "epoch": 0.8379403794037941,
      "step": 3865,
      "training_loss": 6.6343092918396
    },
    {
      "epoch": 0.8379403794037941,
      "step": 3865,
      "training_loss": 5.881463527679443
    },
    {
      "epoch": 0.8379403794037941,
      "step": 3865,
      "training_loss": 5.933352947235107
    },
    {
      "epoch": 0.8381571815718157,
      "step": 3866,
      "training_loss": 6.329769611358643
    },
    {
      "epoch": 0.8381571815718157,
      "step": 3866,
      "training_loss": 6.879790782928467
    },
    {
      "epoch": 0.8381571815718157,
      "step": 3866,
      "training_loss": 7.241959571838379
    },
    {
      "epoch": 0.8381571815718157,
      "step": 3866,
      "training_loss": 3.025090456008911
    },
    {
      "epoch": 0.8383739837398374,
      "step": 3867,
      "training_loss": 5.994626998901367
    },
    {
      "epoch": 0.8383739837398374,
      "step": 3867,
      "training_loss": 6.237043380737305
    },
    {
      "epoch": 0.8383739837398374,
      "step": 3867,
      "training_loss": 3.046856164932251
    },
    {
      "epoch": 0.8383739837398374,
      "step": 3867,
      "training_loss": 8.283845901489258
    },
    {
      "epoch": 0.8385907859078591,
      "grad_norm": 18.709850311279297,
      "learning_rate": 1e-05,
      "loss": 6.0577,
      "step": 3868
    },
    {
      "epoch": 0.8385907859078591,
      "step": 3868,
      "training_loss": 6.959354400634766
    },
    {
      "epoch": 0.8385907859078591,
      "step": 3868,
      "training_loss": 5.762965202331543
    },
    {
      "epoch": 0.8385907859078591,
      "step": 3868,
      "training_loss": 5.229858875274658
    },
    {
      "epoch": 0.8385907859078591,
      "step": 3868,
      "training_loss": 6.705137252807617
    },
    {
      "epoch": 0.8388075880758807,
      "step": 3869,
      "training_loss": 6.6157450675964355
    },
    {
      "epoch": 0.8388075880758807,
      "step": 3869,
      "training_loss": 7.1078972816467285
    },
    {
      "epoch": 0.8388075880758807,
      "step": 3869,
      "training_loss": 4.147605895996094
    },
    {
      "epoch": 0.8388075880758807,
      "step": 3869,
      "training_loss": 4.977558612823486
    },
    {
      "epoch": 0.8390243902439024,
      "step": 3870,
      "training_loss": 7.554506778717041
    },
    {
      "epoch": 0.8390243902439024,
      "step": 3870,
      "training_loss": 7.049640655517578
    },
    {
      "epoch": 0.8390243902439024,
      "step": 3870,
      "training_loss": 5.830769062042236
    },
    {
      "epoch": 0.8390243902439024,
      "step": 3870,
      "training_loss": 4.824272155761719
    },
    {
      "epoch": 0.8392411924119241,
      "step": 3871,
      "training_loss": 7.327961444854736
    },
    {
      "epoch": 0.8392411924119241,
      "step": 3871,
      "training_loss": 5.953152179718018
    },
    {
      "epoch": 0.8392411924119241,
      "step": 3871,
      "training_loss": 6.636044502258301
    },
    {
      "epoch": 0.8392411924119241,
      "step": 3871,
      "training_loss": 6.993187427520752
    },
    {
      "epoch": 0.8394579945799459,
      "grad_norm": 20.80257797241211,
      "learning_rate": 1e-05,
      "loss": 6.2297,
      "step": 3872
    },
    {
      "epoch": 0.8394579945799459,
      "step": 3872,
      "training_loss": 6.6321539878845215
    },
    {
      "epoch": 0.8394579945799459,
      "step": 3872,
      "training_loss": 6.4065728187561035
    },
    {
      "epoch": 0.8394579945799459,
      "step": 3872,
      "training_loss": 4.034745693206787
    },
    {
      "epoch": 0.8394579945799459,
      "step": 3872,
      "training_loss": 6.574452877044678
    },
    {
      "epoch": 0.8396747967479675,
      "step": 3873,
      "training_loss": 5.747076988220215
    },
    {
      "epoch": 0.8396747967479675,
      "step": 3873,
      "training_loss": 8.603727340698242
    },
    {
      "epoch": 0.8396747967479675,
      "step": 3873,
      "training_loss": 7.3420867919921875
    },
    {
      "epoch": 0.8396747967479675,
      "step": 3873,
      "training_loss": 7.0057501792907715
    },
    {
      "epoch": 0.8398915989159892,
      "step": 3874,
      "training_loss": 4.902571678161621
    },
    {
      "epoch": 0.8398915989159892,
      "step": 3874,
      "training_loss": 7.63887882232666
    },
    {
      "epoch": 0.8398915989159892,
      "step": 3874,
      "training_loss": 7.9270806312561035
    },
    {
      "epoch": 0.8398915989159892,
      "step": 3874,
      "training_loss": 6.332674026489258
    },
    {
      "epoch": 0.8401084010840109,
      "step": 3875,
      "training_loss": 6.79152774810791
    },
    {
      "epoch": 0.8401084010840109,
      "step": 3875,
      "training_loss": 4.56191349029541
    },
    {
      "epoch": 0.8401084010840109,
      "step": 3875,
      "training_loss": 6.810804843902588
    },
    {
      "epoch": 0.8401084010840109,
      "step": 3875,
      "training_loss": 6.073486328125
    },
    {
      "epoch": 0.8403252032520325,
      "grad_norm": 18.711196899414062,
      "learning_rate": 1e-05,
      "loss": 6.4616,
      "step": 3876
    },
    {
      "epoch": 0.8403252032520325,
      "step": 3876,
      "training_loss": 6.120383262634277
    },
    {
      "epoch": 0.8403252032520325,
      "step": 3876,
      "training_loss": 5.86491584777832
    },
    {
      "epoch": 0.8403252032520325,
      "step": 3876,
      "training_loss": 5.326547622680664
    },
    {
      "epoch": 0.8403252032520325,
      "step": 3876,
      "training_loss": 6.382668495178223
    },
    {
      "epoch": 0.8405420054200542,
      "step": 3877,
      "training_loss": 7.032841682434082
    },
    {
      "epoch": 0.8405420054200542,
      "step": 3877,
      "training_loss": 4.987640857696533
    },
    {
      "epoch": 0.8405420054200542,
      "step": 3877,
      "training_loss": 6.543631553649902
    },
    {
      "epoch": 0.8405420054200542,
      "step": 3877,
      "training_loss": 6.5546488761901855
    },
    {
      "epoch": 0.8407588075880759,
      "step": 3878,
      "training_loss": 7.301701545715332
    },
    {
      "epoch": 0.8407588075880759,
      "step": 3878,
      "training_loss": 6.740808486938477
    },
    {
      "epoch": 0.8407588075880759,
      "step": 3878,
      "training_loss": 4.969403266906738
    },
    {
      "epoch": 0.8407588075880759,
      "step": 3878,
      "training_loss": 5.155272006988525
    },
    {
      "epoch": 0.8409756097560975,
      "step": 3879,
      "training_loss": 5.861179828643799
    },
    {
      "epoch": 0.8409756097560975,
      "step": 3879,
      "training_loss": 7.351367950439453
    },
    {
      "epoch": 0.8409756097560975,
      "step": 3879,
      "training_loss": 6.315793037414551
    },
    {
      "epoch": 0.8409756097560975,
      "step": 3879,
      "training_loss": 6.562428951263428
    },
    {
      "epoch": 0.8411924119241192,
      "grad_norm": 16.634029388427734,
      "learning_rate": 1e-05,
      "loss": 6.192,
      "step": 3880
    },
    {
      "epoch": 0.8411924119241192,
      "step": 3880,
      "training_loss": 2.449422836303711
    },
    {
      "epoch": 0.8411924119241192,
      "step": 3880,
      "training_loss": 6.5830078125
    },
    {
      "epoch": 0.8411924119241192,
      "step": 3880,
      "training_loss": 7.1658406257629395
    },
    {
      "epoch": 0.8411924119241192,
      "step": 3880,
      "training_loss": 6.482616901397705
    },
    {
      "epoch": 0.8414092140921409,
      "step": 3881,
      "training_loss": 6.876952648162842
    },
    {
      "epoch": 0.8414092140921409,
      "step": 3881,
      "training_loss": 7.271573066711426
    },
    {
      "epoch": 0.8414092140921409,
      "step": 3881,
      "training_loss": 5.952629566192627
    },
    {
      "epoch": 0.8414092140921409,
      "step": 3881,
      "training_loss": 6.425918102264404
    },
    {
      "epoch": 0.8416260162601626,
      "step": 3882,
      "training_loss": 6.4027791023254395
    },
    {
      "epoch": 0.8416260162601626,
      "step": 3882,
      "training_loss": 6.782132625579834
    },
    {
      "epoch": 0.8416260162601626,
      "step": 3882,
      "training_loss": 7.094804286956787
    },
    {
      "epoch": 0.8416260162601626,
      "step": 3882,
      "training_loss": 4.730370998382568
    },
    {
      "epoch": 0.8418428184281843,
      "step": 3883,
      "training_loss": 6.1407928466796875
    },
    {
      "epoch": 0.8418428184281843,
      "step": 3883,
      "training_loss": 7.491650104522705
    },
    {
      "epoch": 0.8418428184281843,
      "step": 3883,
      "training_loss": 6.84088659286499
    },
    {
      "epoch": 0.8418428184281843,
      "step": 3883,
      "training_loss": 7.2790021896362305
    },
    {
      "epoch": 0.842059620596206,
      "grad_norm": 18.207515716552734,
      "learning_rate": 1e-05,
      "loss": 6.3731,
      "step": 3884
    },
    {
      "epoch": 0.842059620596206,
      "step": 3884,
      "training_loss": 4.662993431091309
    },
    {
      "epoch": 0.842059620596206,
      "step": 3884,
      "training_loss": 6.874202728271484
    },
    {
      "epoch": 0.842059620596206,
      "step": 3884,
      "training_loss": 5.073843479156494
    },
    {
      "epoch": 0.842059620596206,
      "step": 3884,
      "training_loss": 6.58226203918457
    },
    {
      "epoch": 0.8422764227642277,
      "step": 3885,
      "training_loss": 7.590336799621582
    },
    {
      "epoch": 0.8422764227642277,
      "step": 3885,
      "training_loss": 6.35819673538208
    },
    {
      "epoch": 0.8422764227642277,
      "step": 3885,
      "training_loss": 4.408560752868652
    },
    {
      "epoch": 0.8422764227642277,
      "step": 3885,
      "training_loss": 7.283817768096924
    },
    {
      "epoch": 0.8424932249322493,
      "step": 3886,
      "training_loss": 6.332923412322998
    },
    {
      "epoch": 0.8424932249322493,
      "step": 3886,
      "training_loss": 6.8225417137146
    },
    {
      "epoch": 0.8424932249322493,
      "step": 3886,
      "training_loss": 6.96907901763916
    },
    {
      "epoch": 0.8424932249322493,
      "step": 3886,
      "training_loss": 7.7751264572143555
    },
    {
      "epoch": 0.842710027100271,
      "step": 3887,
      "training_loss": 6.169713020324707
    },
    {
      "epoch": 0.842710027100271,
      "step": 3887,
      "training_loss": 7.2519917488098145
    },
    {
      "epoch": 0.842710027100271,
      "step": 3887,
      "training_loss": 7.1756696701049805
    },
    {
      "epoch": 0.842710027100271,
      "step": 3887,
      "training_loss": 6.4275360107421875
    },
    {
      "epoch": 0.8429268292682927,
      "grad_norm": 16.27689552307129,
      "learning_rate": 1e-05,
      "loss": 6.4849,
      "step": 3888
    },
    {
      "epoch": 0.8429268292682927,
      "step": 3888,
      "training_loss": 6.264865875244141
    },
    {
      "epoch": 0.8429268292682927,
      "step": 3888,
      "training_loss": 6.648169994354248
    },
    {
      "epoch": 0.8429268292682927,
      "step": 3888,
      "training_loss": 7.801387786865234
    },
    {
      "epoch": 0.8429268292682927,
      "step": 3888,
      "training_loss": 6.173885822296143
    },
    {
      "epoch": 0.8431436314363143,
      "step": 3889,
      "training_loss": 6.97517728805542
    },
    {
      "epoch": 0.8431436314363143,
      "step": 3889,
      "training_loss": 7.168856143951416
    },
    {
      "epoch": 0.8431436314363143,
      "step": 3889,
      "training_loss": 6.385456562042236
    },
    {
      "epoch": 0.8431436314363143,
      "step": 3889,
      "training_loss": 6.87019157409668
    },
    {
      "epoch": 0.843360433604336,
      "step": 3890,
      "training_loss": 6.53567361831665
    },
    {
      "epoch": 0.843360433604336,
      "step": 3890,
      "training_loss": 4.775559902191162
    },
    {
      "epoch": 0.843360433604336,
      "step": 3890,
      "training_loss": 6.820925235748291
    },
    {
      "epoch": 0.843360433604336,
      "step": 3890,
      "training_loss": 6.6947407722473145
    },
    {
      "epoch": 0.8435772357723578,
      "step": 3891,
      "training_loss": 5.502415657043457
    },
    {
      "epoch": 0.8435772357723578,
      "step": 3891,
      "training_loss": 5.004973411560059
    },
    {
      "epoch": 0.8435772357723578,
      "step": 3891,
      "training_loss": 6.141543865203857
    },
    {
      "epoch": 0.8435772357723578,
      "step": 3891,
      "training_loss": 6.30206823348999
    },
    {
      "epoch": 0.8437940379403794,
      "grad_norm": 18.640087127685547,
      "learning_rate": 1e-05,
      "loss": 6.3791,
      "step": 3892
    },
    {
      "epoch": 0.8437940379403794,
      "step": 3892,
      "training_loss": 6.168709754943848
    },
    {
      "epoch": 0.8437940379403794,
      "step": 3892,
      "training_loss": 6.414283275604248
    },
    {
      "epoch": 0.8437940379403794,
      "step": 3892,
      "training_loss": 3.974928379058838
    },
    {
      "epoch": 0.8437940379403794,
      "step": 3892,
      "training_loss": 6.802946090698242
    },
    {
      "epoch": 0.8440108401084011,
      "step": 3893,
      "training_loss": 7.399621486663818
    },
    {
      "epoch": 0.8440108401084011,
      "step": 3893,
      "training_loss": 6.411172389984131
    },
    {
      "epoch": 0.8440108401084011,
      "step": 3893,
      "training_loss": 7.708958625793457
    },
    {
      "epoch": 0.8440108401084011,
      "step": 3893,
      "training_loss": 4.028584957122803
    },
    {
      "epoch": 0.8442276422764228,
      "step": 3894,
      "training_loss": 5.689854145050049
    },
    {
      "epoch": 0.8442276422764228,
      "step": 3894,
      "training_loss": 6.247289180755615
    },
    {
      "epoch": 0.8442276422764228,
      "step": 3894,
      "training_loss": 6.652953624725342
    },
    {
      "epoch": 0.8442276422764228,
      "step": 3894,
      "training_loss": 5.674839973449707
    },
    {
      "epoch": 0.8444444444444444,
      "step": 3895,
      "training_loss": 8.039327621459961
    },
    {
      "epoch": 0.8444444444444444,
      "step": 3895,
      "training_loss": 7.223811626434326
    },
    {
      "epoch": 0.8444444444444444,
      "step": 3895,
      "training_loss": 6.565482139587402
    },
    {
      "epoch": 0.8444444444444444,
      "step": 3895,
      "training_loss": 3.5994064807891846
    },
    {
      "epoch": 0.8446612466124661,
      "grad_norm": 17.129138946533203,
      "learning_rate": 1e-05,
      "loss": 6.1626,
      "step": 3896
    },
    {
      "epoch": 0.8446612466124661,
      "step": 3896,
      "training_loss": 7.225789546966553
    },
    {
      "epoch": 0.8446612466124661,
      "step": 3896,
      "training_loss": 7.730750560760498
    },
    {
      "epoch": 0.8446612466124661,
      "step": 3896,
      "training_loss": 6.897582530975342
    },
    {
      "epoch": 0.8446612466124661,
      "step": 3896,
      "training_loss": 6.860578536987305
    },
    {
      "epoch": 0.8448780487804878,
      "step": 3897,
      "training_loss": 5.7097554206848145
    },
    {
      "epoch": 0.8448780487804878,
      "step": 3897,
      "training_loss": 7.487835884094238
    },
    {
      "epoch": 0.8448780487804878,
      "step": 3897,
      "training_loss": 4.736178398132324
    },
    {
      "epoch": 0.8448780487804878,
      "step": 3897,
      "training_loss": 6.325882434844971
    },
    {
      "epoch": 0.8450948509485094,
      "step": 3898,
      "training_loss": 6.772636413574219
    },
    {
      "epoch": 0.8450948509485094,
      "step": 3898,
      "training_loss": 3.7144343852996826
    },
    {
      "epoch": 0.8450948509485094,
      "step": 3898,
      "training_loss": 7.3758368492126465
    },
    {
      "epoch": 0.8450948509485094,
      "step": 3898,
      "training_loss": 6.035892009735107
    },
    {
      "epoch": 0.8453116531165311,
      "step": 3899,
      "training_loss": 6.066047191619873
    },
    {
      "epoch": 0.8453116531165311,
      "step": 3899,
      "training_loss": 5.8770222663879395
    },
    {
      "epoch": 0.8453116531165311,
      "step": 3899,
      "training_loss": 8.728060722351074
    },
    {
      "epoch": 0.8453116531165311,
      "step": 3899,
      "training_loss": 7.219821453094482
    },
    {
      "epoch": 0.8455284552845529,
      "grad_norm": 21.55247688293457,
      "learning_rate": 1e-05,
      "loss": 6.5478,
      "step": 3900
    },
    {
      "epoch": 0.8455284552845529,
      "step": 3900,
      "training_loss": 5.339924335479736
    },
    {
      "epoch": 0.8455284552845529,
      "step": 3900,
      "training_loss": 6.269347667694092
    },
    {
      "epoch": 0.8455284552845529,
      "step": 3900,
      "training_loss": 6.045715808868408
    },
    {
      "epoch": 0.8455284552845529,
      "step": 3900,
      "training_loss": 6.248630523681641
    },
    {
      "epoch": 0.8457452574525746,
      "step": 3901,
      "training_loss": 5.76327657699585
    },
    {
      "epoch": 0.8457452574525746,
      "step": 3901,
      "training_loss": 5.16641092300415
    },
    {
      "epoch": 0.8457452574525746,
      "step": 3901,
      "training_loss": 5.816409587860107
    },
    {
      "epoch": 0.8457452574525746,
      "step": 3901,
      "training_loss": 3.709444761276245
    },
    {
      "epoch": 0.8459620596205962,
      "step": 3902,
      "training_loss": 7.04764986038208
    },
    {
      "epoch": 0.8459620596205962,
      "step": 3902,
      "training_loss": 5.126920700073242
    },
    {
      "epoch": 0.8459620596205962,
      "step": 3902,
      "training_loss": 6.238715648651123
    },
    {
      "epoch": 0.8459620596205962,
      "step": 3902,
      "training_loss": 4.443254470825195
    },
    {
      "epoch": 0.8461788617886179,
      "step": 3903,
      "training_loss": 5.505177974700928
    },
    {
      "epoch": 0.8461788617886179,
      "step": 3903,
      "training_loss": 6.544408798217773
    },
    {
      "epoch": 0.8461788617886179,
      "step": 3903,
      "training_loss": 6.8413190841674805
    },
    {
      "epoch": 0.8461788617886179,
      "step": 3903,
      "training_loss": 5.8392839431762695
    },
    {
      "epoch": 0.8463956639566396,
      "grad_norm": 16.536760330200195,
      "learning_rate": 1e-05,
      "loss": 5.7466,
      "step": 3904
    },
    {
      "epoch": 0.8463956639566396,
      "step": 3904,
      "training_loss": 7.251644134521484
    },
    {
      "epoch": 0.8463956639566396,
      "step": 3904,
      "training_loss": 6.405159950256348
    },
    {
      "epoch": 0.8463956639566396,
      "step": 3904,
      "training_loss": 7.102341651916504
    },
    {
      "epoch": 0.8463956639566396,
      "step": 3904,
      "training_loss": 6.307374954223633
    },
    {
      "epoch": 0.8466124661246612,
      "step": 3905,
      "training_loss": 7.393099784851074
    },
    {
      "epoch": 0.8466124661246612,
      "step": 3905,
      "training_loss": 6.287928104400635
    },
    {
      "epoch": 0.8466124661246612,
      "step": 3905,
      "training_loss": 6.651153564453125
    },
    {
      "epoch": 0.8466124661246612,
      "step": 3905,
      "training_loss": 3.204503059387207
    },
    {
      "epoch": 0.8468292682926829,
      "step": 3906,
      "training_loss": 6.671967029571533
    },
    {
      "epoch": 0.8468292682926829,
      "step": 3906,
      "training_loss": 6.586450099945068
    },
    {
      "epoch": 0.8468292682926829,
      "step": 3906,
      "training_loss": 6.793181419372559
    },
    {
      "epoch": 0.8468292682926829,
      "step": 3906,
      "training_loss": 7.601903915405273
    },
    {
      "epoch": 0.8470460704607046,
      "step": 3907,
      "training_loss": 7.5213727951049805
    },
    {
      "epoch": 0.8470460704607046,
      "step": 3907,
      "training_loss": 6.843557834625244
    },
    {
      "epoch": 0.8470460704607046,
      "step": 3907,
      "training_loss": 7.057773590087891
    },
    {
      "epoch": 0.8470460704607046,
      "step": 3907,
      "training_loss": 3.743345022201538
    },
    {
      "epoch": 0.8472628726287262,
      "grad_norm": 28.773052215576172,
      "learning_rate": 1e-05,
      "loss": 6.4639,
      "step": 3908
    },
    {
      "epoch": 0.8472628726287262,
      "step": 3908,
      "training_loss": 6.83701229095459
    },
    {
      "epoch": 0.8472628726287262,
      "step": 3908,
      "training_loss": 8.17284107208252
    },
    {
      "epoch": 0.8472628726287262,
      "step": 3908,
      "training_loss": 5.178112030029297
    },
    {
      "epoch": 0.8472628726287262,
      "step": 3908,
      "training_loss": 6.234167575836182
    },
    {
      "epoch": 0.847479674796748,
      "step": 3909,
      "training_loss": 5.191995620727539
    },
    {
      "epoch": 0.847479674796748,
      "step": 3909,
      "training_loss": 7.02521276473999
    },
    {
      "epoch": 0.847479674796748,
      "step": 3909,
      "training_loss": 7.373981475830078
    },
    {
      "epoch": 0.847479674796748,
      "step": 3909,
      "training_loss": 6.416614055633545
    },
    {
      "epoch": 0.8476964769647697,
      "step": 3910,
      "training_loss": 7.4756855964660645
    },
    {
      "epoch": 0.8476964769647697,
      "step": 3910,
      "training_loss": 4.861552715301514
    },
    {
      "epoch": 0.8476964769647697,
      "step": 3910,
      "training_loss": 5.977712154388428
    },
    {
      "epoch": 0.8476964769647697,
      "step": 3910,
      "training_loss": 6.375215530395508
    },
    {
      "epoch": 0.8479132791327914,
      "step": 3911,
      "training_loss": 5.882756233215332
    },
    {
      "epoch": 0.8479132791327914,
      "step": 3911,
      "training_loss": 6.143103122711182
    },
    {
      "epoch": 0.8479132791327914,
      "step": 3911,
      "training_loss": 6.458742618560791
    },
    {
      "epoch": 0.8479132791327914,
      "step": 3911,
      "training_loss": 6.7976884841918945
    },
    {
      "epoch": 0.848130081300813,
      "grad_norm": 18.87803077697754,
      "learning_rate": 1e-05,
      "loss": 6.4001,
      "step": 3912
    },
    {
      "epoch": 0.848130081300813,
      "step": 3912,
      "training_loss": 2.905876398086548
    },
    {
      "epoch": 0.848130081300813,
      "step": 3912,
      "training_loss": 7.559981346130371
    },
    {
      "epoch": 0.848130081300813,
      "step": 3912,
      "training_loss": 7.5567121505737305
    },
    {
      "epoch": 0.848130081300813,
      "step": 3912,
      "training_loss": 7.631585597991943
    },
    {
      "epoch": 0.8483468834688347,
      "step": 3913,
      "training_loss": 5.45585823059082
    },
    {
      "epoch": 0.8483468834688347,
      "step": 3913,
      "training_loss": 5.640047073364258
    },
    {
      "epoch": 0.8483468834688347,
      "step": 3913,
      "training_loss": 6.758260250091553
    },
    {
      "epoch": 0.8483468834688347,
      "step": 3913,
      "training_loss": 6.371042728424072
    },
    {
      "epoch": 0.8485636856368564,
      "step": 3914,
      "training_loss": 6.895597457885742
    },
    {
      "epoch": 0.8485636856368564,
      "step": 3914,
      "training_loss": 6.427186489105225
    },
    {
      "epoch": 0.8485636856368564,
      "step": 3914,
      "training_loss": 3.631436586380005
    },
    {
      "epoch": 0.8485636856368564,
      "step": 3914,
      "training_loss": 7.048752784729004
    },
    {
      "epoch": 0.848780487804878,
      "step": 3915,
      "training_loss": 2.693946361541748
    },
    {
      "epoch": 0.848780487804878,
      "step": 3915,
      "training_loss": 6.907293796539307
    },
    {
      "epoch": 0.848780487804878,
      "step": 3915,
      "training_loss": 6.632208347320557
    },
    {
      "epoch": 0.848780487804878,
      "step": 3915,
      "training_loss": 6.267493724822998
    },
    {
      "epoch": 0.8489972899728997,
      "grad_norm": 16.066444396972656,
      "learning_rate": 1e-05,
      "loss": 6.024,
      "step": 3916
    },
    {
      "epoch": 0.8489972899728997,
      "step": 3916,
      "training_loss": 5.498850345611572
    },
    {
      "epoch": 0.8489972899728997,
      "step": 3916,
      "training_loss": 5.973624229431152
    },
    {
      "epoch": 0.8489972899728997,
      "step": 3916,
      "training_loss": 7.392951965332031
    },
    {
      "epoch": 0.8489972899728997,
      "step": 3916,
      "training_loss": 6.451285362243652
    },
    {
      "epoch": 0.8492140921409214,
      "step": 3917,
      "training_loss": 5.059347629547119
    },
    {
      "epoch": 0.8492140921409214,
      "step": 3917,
      "training_loss": 6.705636978149414
    },
    {
      "epoch": 0.8492140921409214,
      "step": 3917,
      "training_loss": 6.207405090332031
    },
    {
      "epoch": 0.8492140921409214,
      "step": 3917,
      "training_loss": 7.098432540893555
    },
    {
      "epoch": 0.8494308943089431,
      "step": 3918,
      "training_loss": 5.715928077697754
    },
    {
      "epoch": 0.8494308943089431,
      "step": 3918,
      "training_loss": 6.780112266540527
    },
    {
      "epoch": 0.8494308943089431,
      "step": 3918,
      "training_loss": 3.4053666591644287
    },
    {
      "epoch": 0.8494308943089431,
      "step": 3918,
      "training_loss": 4.624246120452881
    },
    {
      "epoch": 0.8496476964769648,
      "step": 3919,
      "training_loss": 6.6184868812561035
    },
    {
      "epoch": 0.8496476964769648,
      "step": 3919,
      "training_loss": 7.401549816131592
    },
    {
      "epoch": 0.8496476964769648,
      "step": 3919,
      "training_loss": 6.378603458404541
    },
    {
      "epoch": 0.8496476964769648,
      "step": 3919,
      "training_loss": 6.30770206451416
    },
    {
      "epoch": 0.8498644986449865,
      "grad_norm": 11.362408638000488,
      "learning_rate": 1e-05,
      "loss": 6.1012,
      "step": 3920
    },
    {
      "epoch": 0.8498644986449865,
      "step": 3920,
      "training_loss": 6.55789041519165
    },
    {
      "epoch": 0.8498644986449865,
      "step": 3920,
      "training_loss": 3.4327945709228516
    },
    {
      "epoch": 0.8498644986449865,
      "step": 3920,
      "training_loss": 7.745349407196045
    },
    {
      "epoch": 0.8498644986449865,
      "step": 3920,
      "training_loss": 6.915260314941406
    },
    {
      "epoch": 0.8500813008130081,
      "step": 3921,
      "training_loss": 7.340800762176514
    },
    {
      "epoch": 0.8500813008130081,
      "step": 3921,
      "training_loss": 4.54572057723999
    },
    {
      "epoch": 0.8500813008130081,
      "step": 3921,
      "training_loss": 5.345682621002197
    },
    {
      "epoch": 0.8500813008130081,
      "step": 3921,
      "training_loss": 5.840341567993164
    },
    {
      "epoch": 0.8502981029810298,
      "step": 3922,
      "training_loss": 4.320455074310303
    },
    {
      "epoch": 0.8502981029810298,
      "step": 3922,
      "training_loss": 6.994888782501221
    },
    {
      "epoch": 0.8502981029810298,
      "step": 3922,
      "training_loss": 7.520362377166748
    },
    {
      "epoch": 0.8502981029810298,
      "step": 3922,
      "training_loss": 6.0933613777160645
    },
    {
      "epoch": 0.8505149051490515,
      "step": 3923,
      "training_loss": 6.002499580383301
    },
    {
      "epoch": 0.8505149051490515,
      "step": 3923,
      "training_loss": 6.416693687438965
    },
    {
      "epoch": 0.8505149051490515,
      "step": 3923,
      "training_loss": 5.675783157348633
    },
    {
      "epoch": 0.8505149051490515,
      "step": 3923,
      "training_loss": 5.557670593261719
    },
    {
      "epoch": 0.8507317073170731,
      "grad_norm": 22.432233810424805,
      "learning_rate": 1e-05,
      "loss": 6.0191,
      "step": 3924
    },
    {
      "epoch": 0.8507317073170731,
      "step": 3924,
      "training_loss": 6.211283206939697
    },
    {
      "epoch": 0.8507317073170731,
      "step": 3924,
      "training_loss": 7.971360683441162
    },
    {
      "epoch": 0.8507317073170731,
      "step": 3924,
      "training_loss": 5.488407611846924
    },
    {
      "epoch": 0.8507317073170731,
      "step": 3924,
      "training_loss": 5.732048988342285
    },
    {
      "epoch": 0.8509485094850948,
      "step": 3925,
      "training_loss": 7.5205769538879395
    },
    {
      "epoch": 0.8509485094850948,
      "step": 3925,
      "training_loss": 5.542614459991455
    },
    {
      "epoch": 0.8509485094850948,
      "step": 3925,
      "training_loss": 5.605192184448242
    },
    {
      "epoch": 0.8509485094850948,
      "step": 3925,
      "training_loss": 6.1376848220825195
    },
    {
      "epoch": 0.8511653116531165,
      "step": 3926,
      "training_loss": 6.452452182769775
    },
    {
      "epoch": 0.8511653116531165,
      "step": 3926,
      "training_loss": 4.8709611892700195
    },
    {
      "epoch": 0.8511653116531165,
      "step": 3926,
      "training_loss": 7.551961421966553
    },
    {
      "epoch": 0.8511653116531165,
      "step": 3926,
      "training_loss": 6.673862934112549
    },
    {
      "epoch": 0.8513821138211383,
      "step": 3927,
      "training_loss": 6.5729241371154785
    },
    {
      "epoch": 0.8513821138211383,
      "step": 3927,
      "training_loss": 6.870137691497803
    },
    {
      "epoch": 0.8513821138211383,
      "step": 3927,
      "training_loss": 4.071053504943848
    },
    {
      "epoch": 0.8513821138211383,
      "step": 3927,
      "training_loss": 7.340847969055176
    },
    {
      "epoch": 0.8515989159891599,
      "grad_norm": 21.403270721435547,
      "learning_rate": 1e-05,
      "loss": 6.2883,
      "step": 3928
    },
    {
      "epoch": 0.8515989159891599,
      "step": 3928,
      "training_loss": 5.81708288192749
    },
    {
      "epoch": 0.8515989159891599,
      "step": 3928,
      "training_loss": 6.83197546005249
    },
    {
      "epoch": 0.8515989159891599,
      "step": 3928,
      "training_loss": 6.558265686035156
    },
    {
      "epoch": 0.8515989159891599,
      "step": 3928,
      "training_loss": 4.97933292388916
    },
    {
      "epoch": 0.8518157181571816,
      "step": 3929,
      "training_loss": 6.620966911315918
    },
    {
      "epoch": 0.8518157181571816,
      "step": 3929,
      "training_loss": 7.892895698547363
    },
    {
      "epoch": 0.8518157181571816,
      "step": 3929,
      "training_loss": 5.612976551055908
    },
    {
      "epoch": 0.8518157181571816,
      "step": 3929,
      "training_loss": 7.234494209289551
    },
    {
      "epoch": 0.8520325203252033,
      "step": 3930,
      "training_loss": 7.010590553283691
    },
    {
      "epoch": 0.8520325203252033,
      "step": 3930,
      "training_loss": 7.553628921508789
    },
    {
      "epoch": 0.8520325203252033,
      "step": 3930,
      "training_loss": 7.055418014526367
    },
    {
      "epoch": 0.8520325203252033,
      "step": 3930,
      "training_loss": 5.598204612731934
    },
    {
      "epoch": 0.8522493224932249,
      "step": 3931,
      "training_loss": 6.540726184844971
    },
    {
      "epoch": 0.8522493224932249,
      "step": 3931,
      "training_loss": 6.439397811889648
    },
    {
      "epoch": 0.8522493224932249,
      "step": 3931,
      "training_loss": 7.373986721038818
    },
    {
      "epoch": 0.8522493224932249,
      "step": 3931,
      "training_loss": 5.778449058532715
    },
    {
      "epoch": 0.8524661246612466,
      "grad_norm": 16.526613235473633,
      "learning_rate": 1e-05,
      "loss": 6.5561,
      "step": 3932
    },
    {
      "epoch": 0.8524661246612466,
      "step": 3932,
      "training_loss": 4.692203998565674
    },
    {
      "epoch": 0.8524661246612466,
      "step": 3932,
      "training_loss": 3.112190008163452
    },
    {
      "epoch": 0.8524661246612466,
      "step": 3932,
      "training_loss": 7.023679256439209
    },
    {
      "epoch": 0.8524661246612466,
      "step": 3932,
      "training_loss": 7.887722015380859
    },
    {
      "epoch": 0.8526829268292683,
      "step": 3933,
      "training_loss": 5.935410976409912
    },
    {
      "epoch": 0.8526829268292683,
      "step": 3933,
      "training_loss": 7.81646203994751
    },
    {
      "epoch": 0.8526829268292683,
      "step": 3933,
      "training_loss": 5.7113871574401855
    },
    {
      "epoch": 0.8526829268292683,
      "step": 3933,
      "training_loss": 5.069761276245117
    },
    {
      "epoch": 0.8528997289972899,
      "step": 3934,
      "training_loss": 7.305950164794922
    },
    {
      "epoch": 0.8528997289972899,
      "step": 3934,
      "training_loss": 7.423855781555176
    },
    {
      "epoch": 0.8528997289972899,
      "step": 3934,
      "training_loss": 7.337756633758545
    },
    {
      "epoch": 0.8528997289972899,
      "step": 3934,
      "training_loss": 6.772899627685547
    },
    {
      "epoch": 0.8531165311653116,
      "step": 3935,
      "training_loss": 6.023013114929199
    },
    {
      "epoch": 0.8531165311653116,
      "step": 3935,
      "training_loss": 6.802765369415283
    },
    {
      "epoch": 0.8531165311653116,
      "step": 3935,
      "training_loss": 4.9513936042785645
    },
    {
      "epoch": 0.8531165311653116,
      "step": 3935,
      "training_loss": 4.977557182312012
    },
    {
      "epoch": 0.8533333333333334,
      "grad_norm": 26.728492736816406,
      "learning_rate": 1e-05,
      "loss": 6.1778,
      "step": 3936
    },
    {
      "epoch": 0.8533333333333334,
      "step": 3936,
      "training_loss": 6.690706253051758
    },
    {
      "epoch": 0.8533333333333334,
      "step": 3936,
      "training_loss": 7.201538562774658
    },
    {
      "epoch": 0.8533333333333334,
      "step": 3936,
      "training_loss": 5.247910022735596
    },
    {
      "epoch": 0.8533333333333334,
      "step": 3936,
      "training_loss": 4.717687129974365
    },
    {
      "epoch": 0.8535501355013551,
      "step": 3937,
      "training_loss": 6.6030049324035645
    },
    {
      "epoch": 0.8535501355013551,
      "step": 3937,
      "training_loss": 5.116179943084717
    },
    {
      "epoch": 0.8535501355013551,
      "step": 3937,
      "training_loss": 8.275699615478516
    },
    {
      "epoch": 0.8535501355013551,
      "step": 3937,
      "training_loss": 6.427313804626465
    },
    {
      "epoch": 0.8537669376693767,
      "step": 3938,
      "training_loss": 6.802861213684082
    },
    {
      "epoch": 0.8537669376693767,
      "step": 3938,
      "training_loss": 6.933671474456787
    },
    {
      "epoch": 0.8537669376693767,
      "step": 3938,
      "training_loss": 4.9201178550720215
    },
    {
      "epoch": 0.8537669376693767,
      "step": 3938,
      "training_loss": 7.047882556915283
    },
    {
      "epoch": 0.8539837398373984,
      "step": 3939,
      "training_loss": 6.896683692932129
    },
    {
      "epoch": 0.8539837398373984,
      "step": 3939,
      "training_loss": 4.867991924285889
    },
    {
      "epoch": 0.8539837398373984,
      "step": 3939,
      "training_loss": 7.476175308227539
    },
    {
      "epoch": 0.8539837398373984,
      "step": 3939,
      "training_loss": 4.229796886444092
    },
    {
      "epoch": 0.8542005420054201,
      "grad_norm": 16.52113151550293,
      "learning_rate": 1e-05,
      "loss": 6.216,
      "step": 3940
    },
    {
      "epoch": 0.8542005420054201,
      "step": 3940,
      "training_loss": 6.409818649291992
    },
    {
      "epoch": 0.8542005420054201,
      "step": 3940,
      "training_loss": 6.298580169677734
    },
    {
      "epoch": 0.8542005420054201,
      "step": 3940,
      "training_loss": 6.4939680099487305
    },
    {
      "epoch": 0.8542005420054201,
      "step": 3940,
      "training_loss": 6.880448341369629
    },
    {
      "epoch": 0.8544173441734417,
      "step": 3941,
      "training_loss": 6.512504577636719
    },
    {
      "epoch": 0.8544173441734417,
      "step": 3941,
      "training_loss": 6.3523945808410645
    },
    {
      "epoch": 0.8544173441734417,
      "step": 3941,
      "training_loss": 5.683462142944336
    },
    {
      "epoch": 0.8544173441734417,
      "step": 3941,
      "training_loss": 6.529006004333496
    },
    {
      "epoch": 0.8546341463414634,
      "step": 3942,
      "training_loss": 6.7928690910339355
    },
    {
      "epoch": 0.8546341463414634,
      "step": 3942,
      "training_loss": 7.988986492156982
    },
    {
      "epoch": 0.8546341463414634,
      "step": 3942,
      "training_loss": 6.518619537353516
    },
    {
      "epoch": 0.8546341463414634,
      "step": 3942,
      "training_loss": 6.062720775604248
    },
    {
      "epoch": 0.8548509485094851,
      "step": 3943,
      "training_loss": 2.590055465698242
    },
    {
      "epoch": 0.8548509485094851,
      "step": 3943,
      "training_loss": 6.4377641677856445
    },
    {
      "epoch": 0.8548509485094851,
      "step": 3943,
      "training_loss": 6.615908145904541
    },
    {
      "epoch": 0.8548509485094851,
      "step": 3943,
      "training_loss": 6.56121301651001
    },
    {
      "epoch": 0.8550677506775067,
      "grad_norm": 15.14400863647461,
      "learning_rate": 1e-05,
      "loss": 6.2955,
      "step": 3944
    },
    {
      "epoch": 0.8550677506775067,
      "step": 3944,
      "training_loss": 7.648031234741211
    },
    {
      "epoch": 0.8550677506775067,
      "step": 3944,
      "training_loss": 6.99130916595459
    },
    {
      "epoch": 0.8550677506775067,
      "step": 3944,
      "training_loss": 6.224080562591553
    },
    {
      "epoch": 0.8550677506775067,
      "step": 3944,
      "training_loss": 5.358831882476807
    },
    {
      "epoch": 0.8552845528455284,
      "step": 3945,
      "training_loss": 7.373241901397705
    },
    {
      "epoch": 0.8552845528455284,
      "step": 3945,
      "training_loss": 6.096347332000732
    },
    {
      "epoch": 0.8552845528455284,
      "step": 3945,
      "training_loss": 6.799296855926514
    },
    {
      "epoch": 0.8552845528455284,
      "step": 3945,
      "training_loss": 3.9127964973449707
    },
    {
      "epoch": 0.8555013550135502,
      "step": 3946,
      "training_loss": 6.540255546569824
    },
    {
      "epoch": 0.8555013550135502,
      "step": 3946,
      "training_loss": 7.7942633628845215
    },
    {
      "epoch": 0.8555013550135502,
      "step": 3946,
      "training_loss": 6.067052841186523
    },
    {
      "epoch": 0.8555013550135502,
      "step": 3946,
      "training_loss": 6.2772536277771
    },
    {
      "epoch": 0.8557181571815718,
      "step": 3947,
      "training_loss": 5.363169193267822
    },
    {
      "epoch": 0.8557181571815718,
      "step": 3947,
      "training_loss": 2.5871260166168213
    },
    {
      "epoch": 0.8557181571815718,
      "step": 3947,
      "training_loss": 7.373806953430176
    },
    {
      "epoch": 0.8557181571815718,
      "step": 3947,
      "training_loss": 7.720703601837158
    },
    {
      "epoch": 0.8559349593495935,
      "grad_norm": 16.26158905029297,
      "learning_rate": 1e-05,
      "loss": 6.258,
      "step": 3948
    },
    {
      "epoch": 0.8559349593495935,
      "step": 3948,
      "training_loss": 6.602304458618164
    },
    {
      "epoch": 0.8559349593495935,
      "step": 3948,
      "training_loss": 6.800868988037109
    },
    {
      "epoch": 0.8559349593495935,
      "step": 3948,
      "training_loss": 5.6210784912109375
    },
    {
      "epoch": 0.8559349593495935,
      "step": 3948,
      "training_loss": 6.527976989746094
    },
    {
      "epoch": 0.8561517615176152,
      "step": 3949,
      "training_loss": 4.302591323852539
    },
    {
      "epoch": 0.8561517615176152,
      "step": 3949,
      "training_loss": 6.316798210144043
    },
    {
      "epoch": 0.8561517615176152,
      "step": 3949,
      "training_loss": 4.566434383392334
    },
    {
      "epoch": 0.8561517615176152,
      "step": 3949,
      "training_loss": 6.729275226593018
    },
    {
      "epoch": 0.8563685636856369,
      "step": 3950,
      "training_loss": 6.124303340911865
    },
    {
      "epoch": 0.8563685636856369,
      "step": 3950,
      "training_loss": 7.021432876586914
    },
    {
      "epoch": 0.8563685636856369,
      "step": 3950,
      "training_loss": 5.607274532318115
    },
    {
      "epoch": 0.8563685636856369,
      "step": 3950,
      "training_loss": 7.847039699554443
    },
    {
      "epoch": 0.8565853658536585,
      "step": 3951,
      "training_loss": 6.540684700012207
    },
    {
      "epoch": 0.8565853658536585,
      "step": 3951,
      "training_loss": 6.820528507232666
    },
    {
      "epoch": 0.8565853658536585,
      "step": 3951,
      "training_loss": 7.873290061950684
    },
    {
      "epoch": 0.8565853658536585,
      "step": 3951,
      "training_loss": 6.552028179168701
    },
    {
      "epoch": 0.8568021680216802,
      "grad_norm": 17.612207412719727,
      "learning_rate": 1e-05,
      "loss": 6.3659,
      "step": 3952
    },
    {
      "epoch": 0.8568021680216802,
      "step": 3952,
      "training_loss": 4.272222995758057
    },
    {
      "epoch": 0.8568021680216802,
      "step": 3952,
      "training_loss": 5.170627593994141
    },
    {
      "epoch": 0.8568021680216802,
      "step": 3952,
      "training_loss": 6.992415904998779
    },
    {
      "epoch": 0.8568021680216802,
      "step": 3952,
      "training_loss": 5.15329647064209
    },
    {
      "epoch": 0.8570189701897019,
      "step": 3953,
      "training_loss": 7.544487953186035
    },
    {
      "epoch": 0.8570189701897019,
      "step": 3953,
      "training_loss": 6.610431671142578
    },
    {
      "epoch": 0.8570189701897019,
      "step": 3953,
      "training_loss": 7.233920097351074
    },
    {
      "epoch": 0.8570189701897019,
      "step": 3953,
      "training_loss": 2.719938278198242
    },
    {
      "epoch": 0.8572357723577235,
      "step": 3954,
      "training_loss": 6.035131454467773
    },
    {
      "epoch": 0.8572357723577235,
      "step": 3954,
      "training_loss": 5.395807266235352
    },
    {
      "epoch": 0.8572357723577235,
      "step": 3954,
      "training_loss": 5.203754425048828
    },
    {
      "epoch": 0.8572357723577235,
      "step": 3954,
      "training_loss": 3.080716609954834
    },
    {
      "epoch": 0.8574525745257453,
      "step": 3955,
      "training_loss": 4.048877716064453
    },
    {
      "epoch": 0.8574525745257453,
      "step": 3955,
      "training_loss": 4.082165718078613
    },
    {
      "epoch": 0.8574525745257453,
      "step": 3955,
      "training_loss": 6.133648872375488
    },
    {
      "epoch": 0.8574525745257453,
      "step": 3955,
      "training_loss": 6.728847026824951
    },
    {
      "epoch": 0.857669376693767,
      "grad_norm": 20.195240020751953,
      "learning_rate": 1e-05,
      "loss": 5.4004,
      "step": 3956
    },
    {
      "epoch": 0.857669376693767,
      "step": 3956,
      "training_loss": 6.624016284942627
    },
    {
      "epoch": 0.857669376693767,
      "step": 3956,
      "training_loss": 6.982654571533203
    },
    {
      "epoch": 0.857669376693767,
      "step": 3956,
      "training_loss": 7.476334571838379
    },
    {
      "epoch": 0.857669376693767,
      "step": 3956,
      "training_loss": 7.247535705566406
    },
    {
      "epoch": 0.8578861788617886,
      "step": 3957,
      "training_loss": 5.740158557891846
    },
    {
      "epoch": 0.8578861788617886,
      "step": 3957,
      "training_loss": 7.903294086456299
    },
    {
      "epoch": 0.8578861788617886,
      "step": 3957,
      "training_loss": 7.382284164428711
    },
    {
      "epoch": 0.8578861788617886,
      "step": 3957,
      "training_loss": 7.519094467163086
    },
    {
      "epoch": 0.8581029810298103,
      "step": 3958,
      "training_loss": 3.9459125995635986
    },
    {
      "epoch": 0.8581029810298103,
      "step": 3958,
      "training_loss": 7.340646743774414
    },
    {
      "epoch": 0.8581029810298103,
      "step": 3958,
      "training_loss": 6.981818199157715
    },
    {
      "epoch": 0.8581029810298103,
      "step": 3958,
      "training_loss": 7.294796466827393
    },
    {
      "epoch": 0.858319783197832,
      "step": 3959,
      "training_loss": 6.141105651855469
    },
    {
      "epoch": 0.858319783197832,
      "step": 3959,
      "training_loss": 3.119647979736328
    },
    {
      "epoch": 0.858319783197832,
      "step": 3959,
      "training_loss": 7.203762531280518
    },
    {
      "epoch": 0.858319783197832,
      "step": 3959,
      "training_loss": 4.94158935546875
    },
    {
      "epoch": 0.8585365853658536,
      "grad_norm": 14.695602416992188,
      "learning_rate": 1e-05,
      "loss": 6.4903,
      "step": 3960
    },
    {
      "epoch": 0.8585365853658536,
      "step": 3960,
      "training_loss": 7.182546615600586
    },
    {
      "epoch": 0.8585365853658536,
      "step": 3960,
      "training_loss": 3.692152738571167
    },
    {
      "epoch": 0.8585365853658536,
      "step": 3960,
      "training_loss": 7.61815881729126
    },
    {
      "epoch": 0.8585365853658536,
      "step": 3960,
      "training_loss": 5.748895168304443
    },
    {
      "epoch": 0.8587533875338753,
      "step": 3961,
      "training_loss": 6.293280601501465
    },
    {
      "epoch": 0.8587533875338753,
      "step": 3961,
      "training_loss": 3.4137120246887207
    },
    {
      "epoch": 0.8587533875338753,
      "step": 3961,
      "training_loss": 6.038539409637451
    },
    {
      "epoch": 0.8587533875338753,
      "step": 3961,
      "training_loss": 5.043593883514404
    },
    {
      "epoch": 0.858970189701897,
      "step": 3962,
      "training_loss": 7.01199197769165
    },
    {
      "epoch": 0.858970189701897,
      "step": 3962,
      "training_loss": 6.466291904449463
    },
    {
      "epoch": 0.858970189701897,
      "step": 3962,
      "training_loss": 7.269632339477539
    },
    {
      "epoch": 0.858970189701897,
      "step": 3962,
      "training_loss": 5.891312122344971
    },
    {
      "epoch": 0.8591869918699186,
      "step": 3963,
      "training_loss": 7.630949020385742
    },
    {
      "epoch": 0.8591869918699186,
      "step": 3963,
      "training_loss": 7.521653175354004
    },
    {
      "epoch": 0.8591869918699186,
      "step": 3963,
      "training_loss": 8.044594764709473
    },
    {
      "epoch": 0.8591869918699186,
      "step": 3963,
      "training_loss": 3.83524489402771
    },
    {
      "epoch": 0.8594037940379404,
      "grad_norm": 20.18852424621582,
      "learning_rate": 1e-05,
      "loss": 6.1689,
      "step": 3964
    },
    {
      "epoch": 0.8594037940379404,
      "step": 3964,
      "training_loss": 7.229306697845459
    },
    {
      "epoch": 0.8594037940379404,
      "step": 3964,
      "training_loss": 6.882011413574219
    },
    {
      "epoch": 0.8594037940379404,
      "step": 3964,
      "training_loss": 7.7438249588012695
    },
    {
      "epoch": 0.8594037940379404,
      "step": 3964,
      "training_loss": 7.367676258087158
    },
    {
      "epoch": 0.8596205962059621,
      "step": 3965,
      "training_loss": 7.047852993011475
    },
    {
      "epoch": 0.8596205962059621,
      "step": 3965,
      "training_loss": 6.826573848724365
    },
    {
      "epoch": 0.8596205962059621,
      "step": 3965,
      "training_loss": 6.878870487213135
    },
    {
      "epoch": 0.8596205962059621,
      "step": 3965,
      "training_loss": 2.962763547897339
    },
    {
      "epoch": 0.8598373983739838,
      "step": 3966,
      "training_loss": 6.4125776290893555
    },
    {
      "epoch": 0.8598373983739838,
      "step": 3966,
      "training_loss": 5.888138294219971
    },
    {
      "epoch": 0.8598373983739838,
      "step": 3966,
      "training_loss": 7.5487165451049805
    },
    {
      "epoch": 0.8598373983739838,
      "step": 3966,
      "training_loss": 6.868532180786133
    },
    {
      "epoch": 0.8600542005420054,
      "step": 3967,
      "training_loss": 5.00428581237793
    },
    {
      "epoch": 0.8600542005420054,
      "step": 3967,
      "training_loss": 6.969783782958984
    },
    {
      "epoch": 0.8600542005420054,
      "step": 3967,
      "training_loss": 5.887026309967041
    },
    {
      "epoch": 0.8600542005420054,
      "step": 3967,
      "training_loss": 6.3235979080200195
    },
    {
      "epoch": 0.8602710027100271,
      "grad_norm": 22.51830291748047,
      "learning_rate": 1e-05,
      "loss": 6.4901,
      "step": 3968
    },
    {
      "epoch": 0.8602710027100271,
      "step": 3968,
      "training_loss": 6.176650047302246
    },
    {
      "epoch": 0.8602710027100271,
      "step": 3968,
      "training_loss": 6.047443389892578
    },
    {
      "epoch": 0.8602710027100271,
      "step": 3968,
      "training_loss": 7.146336555480957
    },
    {
      "epoch": 0.8602710027100271,
      "step": 3968,
      "training_loss": 6.4764533042907715
    },
    {
      "epoch": 0.8604878048780488,
      "step": 3969,
      "training_loss": 8.853087425231934
    },
    {
      "epoch": 0.8604878048780488,
      "step": 3969,
      "training_loss": 8.45351791381836
    },
    {
      "epoch": 0.8604878048780488,
      "step": 3969,
      "training_loss": 6.834800720214844
    },
    {
      "epoch": 0.8604878048780488,
      "step": 3969,
      "training_loss": 6.954570293426514
    },
    {
      "epoch": 0.8607046070460704,
      "step": 3970,
      "training_loss": 5.2163825035095215
    },
    {
      "epoch": 0.8607046070460704,
      "step": 3970,
      "training_loss": 5.621175289154053
    },
    {
      "epoch": 0.8607046070460704,
      "step": 3970,
      "training_loss": 6.798094272613525
    },
    {
      "epoch": 0.8607046070460704,
      "step": 3970,
      "training_loss": 4.11237096786499
    },
    {
      "epoch": 0.8609214092140921,
      "step": 3971,
      "training_loss": 7.7058305740356445
    },
    {
      "epoch": 0.8609214092140921,
      "step": 3971,
      "training_loss": 7.109939098358154
    },
    {
      "epoch": 0.8609214092140921,
      "step": 3971,
      "training_loss": 6.35546875
    },
    {
      "epoch": 0.8609214092140921,
      "step": 3971,
      "training_loss": 5.859130859375
    },
    {
      "epoch": 0.8611382113821138,
      "grad_norm": 22.240806579589844,
      "learning_rate": 1e-05,
      "loss": 6.6076,
      "step": 3972
    },
    {
      "epoch": 0.8611382113821138,
      "step": 3972,
      "training_loss": 7.675147533416748
    },
    {
      "epoch": 0.8611382113821138,
      "step": 3972,
      "training_loss": 6.121329307556152
    },
    {
      "epoch": 0.8611382113821138,
      "step": 3972,
      "training_loss": 7.102405548095703
    },
    {
      "epoch": 0.8611382113821138,
      "step": 3972,
      "training_loss": 5.45816707611084
    },
    {
      "epoch": 0.8613550135501356,
      "step": 3973,
      "training_loss": 5.330069065093994
    },
    {
      "epoch": 0.8613550135501356,
      "step": 3973,
      "training_loss": 6.636380195617676
    },
    {
      "epoch": 0.8613550135501356,
      "step": 3973,
      "training_loss": 3.9115359783172607
    },
    {
      "epoch": 0.8613550135501356,
      "step": 3973,
      "training_loss": 7.755118370056152
    },
    {
      "epoch": 0.8615718157181572,
      "step": 3974,
      "training_loss": 6.623865127563477
    },
    {
      "epoch": 0.8615718157181572,
      "step": 3974,
      "training_loss": 6.899067401885986
    },
    {
      "epoch": 0.8615718157181572,
      "step": 3974,
      "training_loss": 6.783976078033447
    },
    {
      "epoch": 0.8615718157181572,
      "step": 3974,
      "training_loss": 5.51851749420166
    },
    {
      "epoch": 0.8617886178861789,
      "step": 3975,
      "training_loss": 6.901742458343506
    },
    {
      "epoch": 0.8617886178861789,
      "step": 3975,
      "training_loss": 7.76069974899292
    },
    {
      "epoch": 0.8617886178861789,
      "step": 3975,
      "training_loss": 7.0410051345825195
    },
    {
      "epoch": 0.8617886178861789,
      "step": 3975,
      "training_loss": 6.8486199378967285
    },
    {
      "epoch": 0.8620054200542006,
      "grad_norm": 23.405725479125977,
      "learning_rate": 1e-05,
      "loss": 6.523,
      "step": 3976
    },
    {
      "epoch": 0.8620054200542006,
      "step": 3976,
      "training_loss": 6.515862464904785
    },
    {
      "epoch": 0.8620054200542006,
      "step": 3976,
      "training_loss": 4.361069679260254
    },
    {
      "epoch": 0.8620054200542006,
      "step": 3976,
      "training_loss": 6.362516403198242
    },
    {
      "epoch": 0.8620054200542006,
      "step": 3976,
      "training_loss": 6.58140754699707
    },
    {
      "epoch": 0.8622222222222222,
      "step": 3977,
      "training_loss": 7.291032791137695
    },
    {
      "epoch": 0.8622222222222222,
      "step": 3977,
      "training_loss": 6.3196516036987305
    },
    {
      "epoch": 0.8622222222222222,
      "step": 3977,
      "training_loss": 6.0567779541015625
    },
    {
      "epoch": 0.8622222222222222,
      "step": 3977,
      "training_loss": 7.323163986206055
    },
    {
      "epoch": 0.8624390243902439,
      "step": 3978,
      "training_loss": 6.209512710571289
    },
    {
      "epoch": 0.8624390243902439,
      "step": 3978,
      "training_loss": 6.450268268585205
    },
    {
      "epoch": 0.8624390243902439,
      "step": 3978,
      "training_loss": 6.8857550621032715
    },
    {
      "epoch": 0.8624390243902439,
      "step": 3978,
      "training_loss": 4.175022602081299
    },
    {
      "epoch": 0.8626558265582656,
      "step": 3979,
      "training_loss": 3.5783565044403076
    },
    {
      "epoch": 0.8626558265582656,
      "step": 3979,
      "training_loss": 7.999969482421875
    },
    {
      "epoch": 0.8626558265582656,
      "step": 3979,
      "training_loss": 7.52354097366333
    },
    {
      "epoch": 0.8626558265582656,
      "step": 3979,
      "training_loss": 6.0152692794799805
    },
    {
      "epoch": 0.8628726287262872,
      "grad_norm": 20.24379539489746,
      "learning_rate": 1e-05,
      "loss": 6.2281,
      "step": 3980
    },
    {
      "epoch": 0.8628726287262872,
      "step": 3980,
      "training_loss": 6.751990795135498
    },
    {
      "epoch": 0.8628726287262872,
      "step": 3980,
      "training_loss": 6.925316333770752
    },
    {
      "epoch": 0.8628726287262872,
      "step": 3980,
      "training_loss": 5.256939888000488
    },
    {
      "epoch": 0.8628726287262872,
      "step": 3980,
      "training_loss": 6.766285419464111
    },
    {
      "epoch": 0.8630894308943089,
      "step": 3981,
      "training_loss": 3.9858176708221436
    },
    {
      "epoch": 0.8630894308943089,
      "step": 3981,
      "training_loss": 7.871204853057861
    },
    {
      "epoch": 0.8630894308943089,
      "step": 3981,
      "training_loss": 6.0713372230529785
    },
    {
      "epoch": 0.8630894308943089,
      "step": 3981,
      "training_loss": 4.267045021057129
    },
    {
      "epoch": 0.8633062330623307,
      "step": 3982,
      "training_loss": 6.2304863929748535
    },
    {
      "epoch": 0.8633062330623307,
      "step": 3982,
      "training_loss": 6.237077236175537
    },
    {
      "epoch": 0.8633062330623307,
      "step": 3982,
      "training_loss": 6.9500203132629395
    },
    {
      "epoch": 0.8633062330623307,
      "step": 3982,
      "training_loss": 6.496250152587891
    },
    {
      "epoch": 0.8635230352303523,
      "step": 3983,
      "training_loss": 6.626115798950195
    },
    {
      "epoch": 0.8635230352303523,
      "step": 3983,
      "training_loss": 5.5591959953308105
    },
    {
      "epoch": 0.8635230352303523,
      "step": 3983,
      "training_loss": 6.908259391784668
    },
    {
      "epoch": 0.8635230352303523,
      "step": 3983,
      "training_loss": 5.556188583374023
    },
    {
      "epoch": 0.863739837398374,
      "grad_norm": 22.59837532043457,
      "learning_rate": 1e-05,
      "loss": 6.1537,
      "step": 3984
    },
    {
      "epoch": 0.863739837398374,
      "step": 3984,
      "training_loss": 6.721592903137207
    },
    {
      "epoch": 0.863739837398374,
      "step": 3984,
      "training_loss": 6.683614253997803
    },
    {
      "epoch": 0.863739837398374,
      "step": 3984,
      "training_loss": 5.6824631690979
    },
    {
      "epoch": 0.863739837398374,
      "step": 3984,
      "training_loss": 7.21767520904541
    },
    {
      "epoch": 0.8639566395663957,
      "step": 3985,
      "training_loss": 5.168740272521973
    },
    {
      "epoch": 0.8639566395663957,
      "step": 3985,
      "training_loss": 6.547628402709961
    },
    {
      "epoch": 0.8639566395663957,
      "step": 3985,
      "training_loss": 8.00929069519043
    },
    {
      "epoch": 0.8639566395663957,
      "step": 3985,
      "training_loss": 7.144938945770264
    },
    {
      "epoch": 0.8641734417344173,
      "step": 3986,
      "training_loss": 6.46727991104126
    },
    {
      "epoch": 0.8641734417344173,
      "step": 3986,
      "training_loss": 5.91406774520874
    },
    {
      "epoch": 0.8641734417344173,
      "step": 3986,
      "training_loss": 3.3109755516052246
    },
    {
      "epoch": 0.8641734417344173,
      "step": 3986,
      "training_loss": 6.499418258666992
    },
    {
      "epoch": 0.864390243902439,
      "step": 3987,
      "training_loss": 5.024250030517578
    },
    {
      "epoch": 0.864390243902439,
      "step": 3987,
      "training_loss": 5.913745403289795
    },
    {
      "epoch": 0.864390243902439,
      "step": 3987,
      "training_loss": 6.39009952545166
    },
    {
      "epoch": 0.864390243902439,
      "step": 3987,
      "training_loss": 5.768782615661621
    },
    {
      "epoch": 0.8646070460704607,
      "grad_norm": 26.043607711791992,
      "learning_rate": 1e-05,
      "loss": 6.154,
      "step": 3988
    },
    {
      "epoch": 0.8646070460704607,
      "step": 3988,
      "training_loss": 6.793245315551758
    },
    {
      "epoch": 0.8646070460704607,
      "step": 3988,
      "training_loss": 6.019114971160889
    },
    {
      "epoch": 0.8646070460704607,
      "step": 3988,
      "training_loss": 7.154315948486328
    },
    {
      "epoch": 0.8646070460704607,
      "step": 3988,
      "training_loss": 6.066516876220703
    },
    {
      "epoch": 0.8648238482384824,
      "step": 3989,
      "training_loss": 5.680807113647461
    },
    {
      "epoch": 0.8648238482384824,
      "step": 3989,
      "training_loss": 5.664074420928955
    },
    {
      "epoch": 0.8648238482384824,
      "step": 3989,
      "training_loss": 6.477877616882324
    },
    {
      "epoch": 0.8648238482384824,
      "step": 3989,
      "training_loss": 7.261508464813232
    },
    {
      "epoch": 0.865040650406504,
      "step": 3990,
      "training_loss": 5.7273454666137695
    },
    {
      "epoch": 0.865040650406504,
      "step": 3990,
      "training_loss": 6.923739910125732
    },
    {
      "epoch": 0.865040650406504,
      "step": 3990,
      "training_loss": 6.98125696182251
    },
    {
      "epoch": 0.865040650406504,
      "step": 3990,
      "training_loss": 3.4909799098968506
    },
    {
      "epoch": 0.8652574525745258,
      "step": 3991,
      "training_loss": 6.357368469238281
    },
    {
      "epoch": 0.8652574525745258,
      "step": 3991,
      "training_loss": 7.409327030181885
    },
    {
      "epoch": 0.8652574525745258,
      "step": 3991,
      "training_loss": 5.437793254852295
    },
    {
      "epoch": 0.8652574525745258,
      "step": 3991,
      "training_loss": 4.963797569274902
    },
    {
      "epoch": 0.8654742547425475,
      "grad_norm": 20.985139846801758,
      "learning_rate": 1e-05,
      "loss": 6.1506,
      "step": 3992
    },
    {
      "epoch": 0.8654742547425475,
      "step": 3992,
      "training_loss": 4.555644512176514
    },
    {
      "epoch": 0.8654742547425475,
      "step": 3992,
      "training_loss": 5.538402080535889
    },
    {
      "epoch": 0.8654742547425475,
      "step": 3992,
      "training_loss": 6.0290422439575195
    },
    {
      "epoch": 0.8654742547425475,
      "step": 3992,
      "training_loss": 7.023820877075195
    },
    {
      "epoch": 0.8656910569105691,
      "step": 3993,
      "training_loss": 6.9115376472473145
    },
    {
      "epoch": 0.8656910569105691,
      "step": 3993,
      "training_loss": 5.010037899017334
    },
    {
      "epoch": 0.8656910569105691,
      "step": 3993,
      "training_loss": 6.784510612487793
    },
    {
      "epoch": 0.8656910569105691,
      "step": 3993,
      "training_loss": 7.001587390899658
    },
    {
      "epoch": 0.8659078590785908,
      "step": 3994,
      "training_loss": 7.2312469482421875
    },
    {
      "epoch": 0.8659078590785908,
      "step": 3994,
      "training_loss": 4.767910003662109
    },
    {
      "epoch": 0.8659078590785908,
      "step": 3994,
      "training_loss": 5.868075370788574
    },
    {
      "epoch": 0.8659078590785908,
      "step": 3994,
      "training_loss": 5.127485275268555
    },
    {
      "epoch": 0.8661246612466125,
      "step": 3995,
      "training_loss": 6.226379871368408
    },
    {
      "epoch": 0.8661246612466125,
      "step": 3995,
      "training_loss": 7.298565864562988
    },
    {
      "epoch": 0.8661246612466125,
      "step": 3995,
      "training_loss": 6.3940019607543945
    },
    {
      "epoch": 0.8661246612466125,
      "step": 3995,
      "training_loss": 7.775532245635986
    },
    {
      "epoch": 0.8663414634146341,
      "grad_norm": 25.54840087890625,
      "learning_rate": 1e-05,
      "loss": 6.2215,
      "step": 3996
    },
    {
      "epoch": 0.8663414634146341,
      "step": 3996,
      "training_loss": 2.7232742309570312
    },
    {
      "epoch": 0.8663414634146341,
      "step": 3996,
      "training_loss": 6.7492194175720215
    },
    {
      "epoch": 0.8663414634146341,
      "step": 3996,
      "training_loss": 6.866997718811035
    },
    {
      "epoch": 0.8663414634146341,
      "step": 3996,
      "training_loss": 7.348749160766602
    },
    {
      "epoch": 0.8665582655826558,
      "step": 3997,
      "training_loss": 5.676734447479248
    },
    {
      "epoch": 0.8665582655826558,
      "step": 3997,
      "training_loss": 4.729745388031006
    },
    {
      "epoch": 0.8665582655826558,
      "step": 3997,
      "training_loss": 7.712239742279053
    },
    {
      "epoch": 0.8665582655826558,
      "step": 3997,
      "training_loss": 6.093775749206543
    },
    {
      "epoch": 0.8667750677506775,
      "step": 3998,
      "training_loss": 6.319449424743652
    },
    {
      "epoch": 0.8667750677506775,
      "step": 3998,
      "training_loss": 6.735217571258545
    },
    {
      "epoch": 0.8667750677506775,
      "step": 3998,
      "training_loss": 7.224588394165039
    },
    {
      "epoch": 0.8667750677506775,
      "step": 3998,
      "training_loss": 7.180410385131836
    },
    {
      "epoch": 0.8669918699186991,
      "step": 3999,
      "training_loss": 6.55060338973999
    },
    {
      "epoch": 0.8669918699186991,
      "step": 3999,
      "training_loss": 6.839787483215332
    },
    {
      "epoch": 0.8669918699186991,
      "step": 3999,
      "training_loss": 6.0660881996154785
    },
    {
      "epoch": 0.8669918699186991,
      "step": 3999,
      "training_loss": 5.758275985717773
    },
    {
      "epoch": 0.8672086720867209,
      "grad_norm": 26.113689422607422,
      "learning_rate": 1e-05,
      "loss": 6.2859,
      "step": 4000
    },
    {
      "epoch": 0.8672086720867209,
      "step": 4000,
      "training_loss": 5.777626037597656
    },
    {
      "epoch": 0.8672086720867209,
      "step": 4000,
      "training_loss": 6.201377868652344
    },
    {
      "epoch": 0.8672086720867209,
      "step": 4000,
      "training_loss": 4.872890472412109
    },
    {
      "epoch": 0.8672086720867209,
      "step": 4000,
      "training_loss": 3.817164182662964
    },
    {
      "epoch": 0.8674254742547426,
      "step": 4001,
      "training_loss": 6.693077087402344
    },
    {
      "epoch": 0.8674254742547426,
      "step": 4001,
      "training_loss": 5.741837024688721
    },
    {
      "epoch": 0.8674254742547426,
      "step": 4001,
      "training_loss": 4.827956676483154
    },
    {
      "epoch": 0.8674254742547426,
      "step": 4001,
      "training_loss": 6.83266019821167
    },
    {
      "epoch": 0.8676422764227643,
      "step": 4002,
      "training_loss": 7.443270683288574
    },
    {
      "epoch": 0.8676422764227643,
      "step": 4002,
      "training_loss": 5.867156982421875
    },
    {
      "epoch": 0.8676422764227643,
      "step": 4002,
      "training_loss": 6.397748947143555
    },
    {
      "epoch": 0.8676422764227643,
      "step": 4002,
      "training_loss": 6.862500190734863
    },
    {
      "epoch": 0.8678590785907859,
      "step": 4003,
      "training_loss": 5.839539051055908
    },
    {
      "epoch": 0.8678590785907859,
      "step": 4003,
      "training_loss": 6.342963695526123
    },
    {
      "epoch": 0.8678590785907859,
      "step": 4003,
      "training_loss": 6.0393853187561035
    },
    {
      "epoch": 0.8678590785907859,
      "step": 4003,
      "training_loss": 4.98330545425415
    },
    {
      "epoch": 0.8680758807588076,
      "grad_norm": 25.580310821533203,
      "learning_rate": 1e-05,
      "loss": 5.9088,
      "step": 4004
    },
    {
      "epoch": 0.8680758807588076,
      "step": 4004,
      "training_loss": 4.501427173614502
    },
    {
      "epoch": 0.8680758807588076,
      "step": 4004,
      "training_loss": 5.287110328674316
    },
    {
      "epoch": 0.8680758807588076,
      "step": 4004,
      "training_loss": 6.188538551330566
    },
    {
      "epoch": 0.8680758807588076,
      "step": 4004,
      "training_loss": 7.342443943023682
    },
    {
      "epoch": 0.8682926829268293,
      "step": 4005,
      "training_loss": 7.213779449462891
    },
    {
      "epoch": 0.8682926829268293,
      "step": 4005,
      "training_loss": 6.768495559692383
    },
    {
      "epoch": 0.8682926829268293,
      "step": 4005,
      "training_loss": 6.016868591308594
    },
    {
      "epoch": 0.8682926829268293,
      "step": 4005,
      "training_loss": 7.365721225738525
    },
    {
      "epoch": 0.8685094850948509,
      "step": 4006,
      "training_loss": 6.920568466186523
    },
    {
      "epoch": 0.8685094850948509,
      "step": 4006,
      "training_loss": 4.2942423820495605
    },
    {
      "epoch": 0.8685094850948509,
      "step": 4006,
      "training_loss": 6.152622222900391
    },
    {
      "epoch": 0.8685094850948509,
      "step": 4006,
      "training_loss": 7.383836269378662
    },
    {
      "epoch": 0.8687262872628726,
      "step": 4007,
      "training_loss": 5.823199272155762
    },
    {
      "epoch": 0.8687262872628726,
      "step": 4007,
      "training_loss": 5.687819480895996
    },
    {
      "epoch": 0.8687262872628726,
      "step": 4007,
      "training_loss": 4.1377339363098145
    },
    {
      "epoch": 0.8687262872628726,
      "step": 4007,
      "training_loss": 6.380386829376221
    },
    {
      "epoch": 0.8689430894308943,
      "grad_norm": 19.320430755615234,
      "learning_rate": 1e-05,
      "loss": 6.0915,
      "step": 4008
    },
    {
      "epoch": 0.8689430894308943,
      "step": 4008,
      "training_loss": 7.088123798370361
    },
    {
      "epoch": 0.8689430894308943,
      "step": 4008,
      "training_loss": 7.165549278259277
    },
    {
      "epoch": 0.8689430894308943,
      "step": 4008,
      "training_loss": 6.610975742340088
    },
    {
      "epoch": 0.8689430894308943,
      "step": 4008,
      "training_loss": 7.7737717628479
    },
    {
      "epoch": 0.8691598915989159,
      "step": 4009,
      "training_loss": 6.27379035949707
    },
    {
      "epoch": 0.8691598915989159,
      "step": 4009,
      "training_loss": 5.601543426513672
    },
    {
      "epoch": 0.8691598915989159,
      "step": 4009,
      "training_loss": 6.914118766784668
    },
    {
      "epoch": 0.8691598915989159,
      "step": 4009,
      "training_loss": 6.648096084594727
    },
    {
      "epoch": 0.8693766937669377,
      "step": 4010,
      "training_loss": 4.2104902267456055
    },
    {
      "epoch": 0.8693766937669377,
      "step": 4010,
      "training_loss": 5.933943748474121
    },
    {
      "epoch": 0.8693766937669377,
      "step": 4010,
      "training_loss": 5.054407119750977
    },
    {
      "epoch": 0.8693766937669377,
      "step": 4010,
      "training_loss": 5.875481128692627
    },
    {
      "epoch": 0.8695934959349594,
      "step": 4011,
      "training_loss": 6.034221172332764
    },
    {
      "epoch": 0.8695934959349594,
      "step": 4011,
      "training_loss": 7.062831878662109
    },
    {
      "epoch": 0.8695934959349594,
      "step": 4011,
      "training_loss": 3.453545093536377
    },
    {
      "epoch": 0.8695934959349594,
      "step": 4011,
      "training_loss": 6.701772689819336
    },
    {
      "epoch": 0.869810298102981,
      "grad_norm": 19.20127296447754,
      "learning_rate": 1e-05,
      "loss": 6.1502,
      "step": 4012
    },
    {
      "epoch": 0.869810298102981,
      "step": 4012,
      "training_loss": 5.858415126800537
    },
    {
      "epoch": 0.869810298102981,
      "step": 4012,
      "training_loss": 4.617588996887207
    },
    {
      "epoch": 0.869810298102981,
      "step": 4012,
      "training_loss": 6.490523815155029
    },
    {
      "epoch": 0.869810298102981,
      "step": 4012,
      "training_loss": 6.444061279296875
    },
    {
      "epoch": 0.8700271002710027,
      "step": 4013,
      "training_loss": 3.7303006649017334
    },
    {
      "epoch": 0.8700271002710027,
      "step": 4013,
      "training_loss": 7.824991703033447
    },
    {
      "epoch": 0.8700271002710027,
      "step": 4013,
      "training_loss": 6.936295986175537
    },
    {
      "epoch": 0.8700271002710027,
      "step": 4013,
      "training_loss": 7.918940544128418
    },
    {
      "epoch": 0.8702439024390244,
      "step": 4014,
      "training_loss": 3.7694413661956787
    },
    {
      "epoch": 0.8702439024390244,
      "step": 4014,
      "training_loss": 4.812995433807373
    },
    {
      "epoch": 0.8702439024390244,
      "step": 4014,
      "training_loss": 6.327456474304199
    },
    {
      "epoch": 0.8702439024390244,
      "step": 4014,
      "training_loss": 5.940267562866211
    },
    {
      "epoch": 0.870460704607046,
      "step": 4015,
      "training_loss": 6.86772346496582
    },
    {
      "epoch": 0.870460704607046,
      "step": 4015,
      "training_loss": 7.451959133148193
    },
    {
      "epoch": 0.870460704607046,
      "step": 4015,
      "training_loss": 5.06533145904541
    },
    {
      "epoch": 0.870460704607046,
      "step": 4015,
      "training_loss": 6.4646897315979
    },
    {
      "epoch": 0.8706775067750677,
      "grad_norm": 20.542272567749023,
      "learning_rate": 1e-05,
      "loss": 6.0326,
      "step": 4016
    },
    {
      "epoch": 0.8706775067750677,
      "step": 4016,
      "training_loss": 6.145496845245361
    },
    {
      "epoch": 0.8706775067750677,
      "step": 4016,
      "training_loss": 6.077480792999268
    },
    {
      "epoch": 0.8706775067750677,
      "step": 4016,
      "training_loss": 3.613778829574585
    },
    {
      "epoch": 0.8706775067750677,
      "step": 4016,
      "training_loss": 6.800109386444092
    },
    {
      "epoch": 0.8708943089430894,
      "step": 4017,
      "training_loss": 5.2671966552734375
    },
    {
      "epoch": 0.8708943089430894,
      "step": 4017,
      "training_loss": 7.296754360198975
    },
    {
      "epoch": 0.8708943089430894,
      "step": 4017,
      "training_loss": 6.128973484039307
    },
    {
      "epoch": 0.8708943089430894,
      "step": 4017,
      "training_loss": 7.491987705230713
    },
    {
      "epoch": 0.8711111111111111,
      "step": 4018,
      "training_loss": 7.7831711769104
    },
    {
      "epoch": 0.8711111111111111,
      "step": 4018,
      "training_loss": 6.429325580596924
    },
    {
      "epoch": 0.8711111111111111,
      "step": 4018,
      "training_loss": 7.180767059326172
    },
    {
      "epoch": 0.8711111111111111,
      "step": 4018,
      "training_loss": 6.207059860229492
    },
    {
      "epoch": 0.8713279132791328,
      "step": 4019,
      "training_loss": 7.541540145874023
    },
    {
      "epoch": 0.8713279132791328,
      "step": 4019,
      "training_loss": 6.2374491691589355
    },
    {
      "epoch": 0.8713279132791328,
      "step": 4019,
      "training_loss": 5.1697468757629395
    },
    {
      "epoch": 0.8713279132791328,
      "step": 4019,
      "training_loss": 6.1728620529174805
    },
    {
      "epoch": 0.8715447154471545,
      "grad_norm": 19.619251251220703,
      "learning_rate": 1e-05,
      "loss": 6.3465,
      "step": 4020
    },
    {
      "epoch": 0.8715447154471545,
      "step": 4020,
      "training_loss": 4.490998268127441
    },
    {
      "epoch": 0.8715447154471545,
      "step": 4020,
      "training_loss": 4.639866352081299
    },
    {
      "epoch": 0.8715447154471545,
      "step": 4020,
      "training_loss": 5.744892120361328
    },
    {
      "epoch": 0.8715447154471545,
      "step": 4020,
      "training_loss": 6.8098978996276855
    },
    {
      "epoch": 0.8717615176151762,
      "step": 4021,
      "training_loss": 5.437013149261475
    },
    {
      "epoch": 0.8717615176151762,
      "step": 4021,
      "training_loss": 7.164384365081787
    },
    {
      "epoch": 0.8717615176151762,
      "step": 4021,
      "training_loss": 6.21713924407959
    },
    {
      "epoch": 0.8717615176151762,
      "step": 4021,
      "training_loss": 7.955013751983643
    },
    {
      "epoch": 0.8719783197831978,
      "step": 4022,
      "training_loss": 4.122043132781982
    },
    {
      "epoch": 0.8719783197831978,
      "step": 4022,
      "training_loss": 6.7230143547058105
    },
    {
      "epoch": 0.8719783197831978,
      "step": 4022,
      "training_loss": 7.79451847076416
    },
    {
      "epoch": 0.8719783197831978,
      "step": 4022,
      "training_loss": 6.966197967529297
    },
    {
      "epoch": 0.8721951219512195,
      "step": 4023,
      "training_loss": 5.642298698425293
    },
    {
      "epoch": 0.8721951219512195,
      "step": 4023,
      "training_loss": 7.544261455535889
    },
    {
      "epoch": 0.8721951219512195,
      "step": 4023,
      "training_loss": 6.308337688446045
    },
    {
      "epoch": 0.8721951219512195,
      "step": 4023,
      "training_loss": 7.163934230804443
    },
    {
      "epoch": 0.8724119241192412,
      "grad_norm": 22.37967300415039,
      "learning_rate": 1e-05,
      "loss": 6.2952,
      "step": 4024
    },
    {
      "epoch": 0.8724119241192412,
      "step": 4024,
      "training_loss": 7.392022132873535
    },
    {
      "epoch": 0.8724119241192412,
      "step": 4024,
      "training_loss": 4.887896537780762
    },
    {
      "epoch": 0.8724119241192412,
      "step": 4024,
      "training_loss": 6.333953380584717
    },
    {
      "epoch": 0.8724119241192412,
      "step": 4024,
      "training_loss": 7.005725860595703
    },
    {
      "epoch": 0.8726287262872628,
      "step": 4025,
      "training_loss": 6.096807479858398
    },
    {
      "epoch": 0.8726287262872628,
      "step": 4025,
      "training_loss": 6.604933738708496
    },
    {
      "epoch": 0.8726287262872628,
      "step": 4025,
      "training_loss": 7.334390640258789
    },
    {
      "epoch": 0.8726287262872628,
      "step": 4025,
      "training_loss": 4.892340660095215
    },
    {
      "epoch": 0.8728455284552845,
      "step": 4026,
      "training_loss": 6.767298221588135
    },
    {
      "epoch": 0.8728455284552845,
      "step": 4026,
      "training_loss": 6.827521324157715
    },
    {
      "epoch": 0.8728455284552845,
      "step": 4026,
      "training_loss": 7.011890888214111
    },
    {
      "epoch": 0.8728455284552845,
      "step": 4026,
      "training_loss": 5.801579475402832
    },
    {
      "epoch": 0.8730623306233062,
      "step": 4027,
      "training_loss": 6.399480819702148
    },
    {
      "epoch": 0.8730623306233062,
      "step": 4027,
      "training_loss": 6.668679714202881
    },
    {
      "epoch": 0.8730623306233062,
      "step": 4027,
      "training_loss": 5.593930244445801
    },
    {
      "epoch": 0.8730623306233062,
      "step": 4027,
      "training_loss": 5.737293720245361
    },
    {
      "epoch": 0.873279132791328,
      "grad_norm": 14.88752269744873,
      "learning_rate": 1e-05,
      "loss": 6.3347,
      "step": 4028
    },
    {
      "epoch": 0.873279132791328,
      "step": 4028,
      "training_loss": 7.217167377471924
    },
    {
      "epoch": 0.873279132791328,
      "step": 4028,
      "training_loss": 4.808928489685059
    },
    {
      "epoch": 0.873279132791328,
      "step": 4028,
      "training_loss": 7.024527072906494
    },
    {
      "epoch": 0.873279132791328,
      "step": 4028,
      "training_loss": 7.3883466720581055
    },
    {
      "epoch": 0.8734959349593496,
      "step": 4029,
      "training_loss": 5.10464334487915
    },
    {
      "epoch": 0.8734959349593496,
      "step": 4029,
      "training_loss": 6.502303123474121
    },
    {
      "epoch": 0.8734959349593496,
      "step": 4029,
      "training_loss": 5.7295823097229
    },
    {
      "epoch": 0.8734959349593496,
      "step": 4029,
      "training_loss": 7.550930976867676
    },
    {
      "epoch": 0.8737127371273713,
      "step": 4030,
      "training_loss": 6.319962978363037
    },
    {
      "epoch": 0.8737127371273713,
      "step": 4030,
      "training_loss": 6.8166327476501465
    },
    {
      "epoch": 0.8737127371273713,
      "step": 4030,
      "training_loss": 7.280097007751465
    },
    {
      "epoch": 0.8737127371273713,
      "step": 4030,
      "training_loss": 5.7193522453308105
    },
    {
      "epoch": 0.873929539295393,
      "step": 4031,
      "training_loss": 7.333573818206787
    },
    {
      "epoch": 0.873929539295393,
      "step": 4031,
      "training_loss": 5.102933883666992
    },
    {
      "epoch": 0.873929539295393,
      "step": 4031,
      "training_loss": 6.254123210906982
    },
    {
      "epoch": 0.873929539295393,
      "step": 4031,
      "training_loss": 6.323790073394775
    },
    {
      "epoch": 0.8741463414634146,
      "grad_norm": 23.221208572387695,
      "learning_rate": 1e-05,
      "loss": 6.4048,
      "step": 4032
    },
    {
      "epoch": 0.8741463414634146,
      "step": 4032,
      "training_loss": 6.8059821128845215
    },
    {
      "epoch": 0.8741463414634146,
      "step": 4032,
      "training_loss": 5.571646690368652
    },
    {
      "epoch": 0.8741463414634146,
      "step": 4032,
      "training_loss": 6.99040412902832
    },
    {
      "epoch": 0.8741463414634146,
      "step": 4032,
      "training_loss": 7.326417922973633
    },
    {
      "epoch": 0.8743631436314363,
      "step": 4033,
      "training_loss": 3.4799444675445557
    },
    {
      "epoch": 0.8743631436314363,
      "step": 4033,
      "training_loss": 7.7204813957214355
    },
    {
      "epoch": 0.8743631436314363,
      "step": 4033,
      "training_loss": 4.716419696807861
    },
    {
      "epoch": 0.8743631436314363,
      "step": 4033,
      "training_loss": 7.171932220458984
    },
    {
      "epoch": 0.874579945799458,
      "step": 4034,
      "training_loss": 6.652735233306885
    },
    {
      "epoch": 0.874579945799458,
      "step": 4034,
      "training_loss": 7.726859092712402
    },
    {
      "epoch": 0.874579945799458,
      "step": 4034,
      "training_loss": 5.995053291320801
    },
    {
      "epoch": 0.874579945799458,
      "step": 4034,
      "training_loss": 6.895412921905518
    },
    {
      "epoch": 0.8747967479674796,
      "step": 4035,
      "training_loss": 7.472890853881836
    },
    {
      "epoch": 0.8747967479674796,
      "step": 4035,
      "training_loss": 6.211027145385742
    },
    {
      "epoch": 0.8747967479674796,
      "step": 4035,
      "training_loss": 4.26930570602417
    },
    {
      "epoch": 0.8747967479674796,
      "step": 4035,
      "training_loss": 6.0249762535095215
    },
    {
      "epoch": 0.8750135501355013,
      "grad_norm": 25.07987403869629,
      "learning_rate": 1e-05,
      "loss": 6.3145,
      "step": 4036
    },
    {
      "epoch": 0.8750135501355013,
      "step": 4036,
      "training_loss": 6.986188888549805
    },
    {
      "epoch": 0.8750135501355013,
      "step": 4036,
      "training_loss": 6.631178379058838
    },
    {
      "epoch": 0.8750135501355013,
      "step": 4036,
      "training_loss": 2.830899953842163
    },
    {
      "epoch": 0.8750135501355013,
      "step": 4036,
      "training_loss": 5.827694892883301
    },
    {
      "epoch": 0.8752303523035231,
      "step": 4037,
      "training_loss": 6.652829170227051
    },
    {
      "epoch": 0.8752303523035231,
      "step": 4037,
      "training_loss": 4.22533655166626
    },
    {
      "epoch": 0.8752303523035231,
      "step": 4037,
      "training_loss": 7.434492111206055
    },
    {
      "epoch": 0.8752303523035231,
      "step": 4037,
      "training_loss": 7.990988731384277
    },
    {
      "epoch": 0.8754471544715448,
      "step": 4038,
      "training_loss": 6.814382076263428
    },
    {
      "epoch": 0.8754471544715448,
      "step": 4038,
      "training_loss": 7.403436183929443
    },
    {
      "epoch": 0.8754471544715448,
      "step": 4038,
      "training_loss": 7.876586437225342
    },
    {
      "epoch": 0.8754471544715448,
      "step": 4038,
      "training_loss": 8.121013641357422
    },
    {
      "epoch": 0.8756639566395664,
      "step": 4039,
      "training_loss": 7.202197551727295
    },
    {
      "epoch": 0.8756639566395664,
      "step": 4039,
      "training_loss": 6.709649085998535
    },
    {
      "epoch": 0.8756639566395664,
      "step": 4039,
      "training_loss": 6.222644329071045
    },
    {
      "epoch": 0.8756639566395664,
      "step": 4039,
      "training_loss": 7.440769672393799
    },
    {
      "epoch": 0.8758807588075881,
      "grad_norm": 26.09225082397461,
      "learning_rate": 1e-05,
      "loss": 6.6481,
      "step": 4040
    },
    {
      "epoch": 0.8758807588075881,
      "step": 4040,
      "training_loss": 8.283553123474121
    },
    {
      "epoch": 0.8758807588075881,
      "step": 4040,
      "training_loss": 6.820529460906982
    },
    {
      "epoch": 0.8758807588075881,
      "step": 4040,
      "training_loss": 5.01954460144043
    },
    {
      "epoch": 0.8758807588075881,
      "step": 4040,
      "training_loss": 6.652440547943115
    },
    {
      "epoch": 0.8760975609756098,
      "step": 4041,
      "training_loss": 7.302961349487305
    },
    {
      "epoch": 0.8760975609756098,
      "step": 4041,
      "training_loss": 6.547354221343994
    },
    {
      "epoch": 0.8760975609756098,
      "step": 4041,
      "training_loss": 6.780357837677002
    },
    {
      "epoch": 0.8760975609756098,
      "step": 4041,
      "training_loss": 7.454918384552002
    },
    {
      "epoch": 0.8763143631436314,
      "step": 4042,
      "training_loss": 5.76119327545166
    },
    {
      "epoch": 0.8763143631436314,
      "step": 4042,
      "training_loss": 5.566458225250244
    },
    {
      "epoch": 0.8763143631436314,
      "step": 4042,
      "training_loss": 2.816640853881836
    },
    {
      "epoch": 0.8763143631436314,
      "step": 4042,
      "training_loss": 6.921568393707275
    },
    {
      "epoch": 0.8765311653116531,
      "step": 4043,
      "training_loss": 7.23981237411499
    },
    {
      "epoch": 0.8765311653116531,
      "step": 4043,
      "training_loss": 5.883266448974609
    },
    {
      "epoch": 0.8765311653116531,
      "step": 4043,
      "training_loss": 7.642943382263184
    },
    {
      "epoch": 0.8765311653116531,
      "step": 4043,
      "training_loss": 3.84059739112854
    },
    {
      "epoch": 0.8767479674796748,
      "grad_norm": 17.16655731201172,
      "learning_rate": 1e-05,
      "loss": 6.2834,
      "step": 4044
    },
    {
      "epoch": 0.8767479674796748,
      "step": 4044,
      "training_loss": 6.790036201477051
    },
    {
      "epoch": 0.8767479674796748,
      "step": 4044,
      "training_loss": 6.739974498748779
    },
    {
      "epoch": 0.8767479674796748,
      "step": 4044,
      "training_loss": 6.755158424377441
    },
    {
      "epoch": 0.8767479674796748,
      "step": 4044,
      "training_loss": 6.980976104736328
    },
    {
      "epoch": 0.8769647696476964,
      "step": 4045,
      "training_loss": 7.00442361831665
    },
    {
      "epoch": 0.8769647696476964,
      "step": 4045,
      "training_loss": 6.499889850616455
    },
    {
      "epoch": 0.8769647696476964,
      "step": 4045,
      "training_loss": 6.02114200592041
    },
    {
      "epoch": 0.8769647696476964,
      "step": 4045,
      "training_loss": 6.786523342132568
    },
    {
      "epoch": 0.8771815718157182,
      "step": 4046,
      "training_loss": 5.938555717468262
    },
    {
      "epoch": 0.8771815718157182,
      "step": 4046,
      "training_loss": 7.112053871154785
    },
    {
      "epoch": 0.8771815718157182,
      "step": 4046,
      "training_loss": 7.573709964752197
    },
    {
      "epoch": 0.8771815718157182,
      "step": 4046,
      "training_loss": 6.423111915588379
    },
    {
      "epoch": 0.8773983739837399,
      "step": 4047,
      "training_loss": 7.438446044921875
    },
    {
      "epoch": 0.8773983739837399,
      "step": 4047,
      "training_loss": 6.777360916137695
    },
    {
      "epoch": 0.8773983739837399,
      "step": 4047,
      "training_loss": 7.185428142547607
    },
    {
      "epoch": 0.8773983739837399,
      "step": 4047,
      "training_loss": 6.682196617126465
    },
    {
      "epoch": 0.8776151761517615,
      "grad_norm": 17.420639038085938,
      "learning_rate": 1e-05,
      "loss": 6.7943,
      "step": 4048
    },
    {
      "epoch": 0.8776151761517615,
      "step": 4048,
      "training_loss": 6.400333881378174
    },
    {
      "epoch": 0.8776151761517615,
      "step": 4048,
      "training_loss": 7.48732852935791
    },
    {
      "epoch": 0.8776151761517615,
      "step": 4048,
      "training_loss": 5.250822067260742
    },
    {
      "epoch": 0.8776151761517615,
      "step": 4048,
      "training_loss": 4.842761039733887
    },
    {
      "epoch": 0.8778319783197832,
      "step": 4049,
      "training_loss": 5.978246688842773
    },
    {
      "epoch": 0.8778319783197832,
      "step": 4049,
      "training_loss": 6.142207145690918
    },
    {
      "epoch": 0.8778319783197832,
      "step": 4049,
      "training_loss": 6.220767498016357
    },
    {
      "epoch": 0.8778319783197832,
      "step": 4049,
      "training_loss": 7.344775199890137
    },
    {
      "epoch": 0.8780487804878049,
      "step": 4050,
      "training_loss": 7.195765495300293
    },
    {
      "epoch": 0.8780487804878049,
      "step": 4050,
      "training_loss": 5.604295253753662
    },
    {
      "epoch": 0.8780487804878049,
      "step": 4050,
      "training_loss": 7.3731889724731445
    },
    {
      "epoch": 0.8780487804878049,
      "step": 4050,
      "training_loss": 7.048754692077637
    },
    {
      "epoch": 0.8782655826558265,
      "step": 4051,
      "training_loss": 7.0770087242126465
    },
    {
      "epoch": 0.8782655826558265,
      "step": 4051,
      "training_loss": 6.559932231903076
    },
    {
      "epoch": 0.8782655826558265,
      "step": 4051,
      "training_loss": 6.384212970733643
    },
    {
      "epoch": 0.8782655826558265,
      "step": 4051,
      "training_loss": 6.72418212890625
    },
    {
      "epoch": 0.8784823848238482,
      "grad_norm": 15.227897644042969,
      "learning_rate": 1e-05,
      "loss": 6.4772,
      "step": 4052
    },
    {
      "epoch": 0.8784823848238482,
      "step": 4052,
      "training_loss": 6.879225254058838
    },
    {
      "epoch": 0.8784823848238482,
      "step": 4052,
      "training_loss": 6.156604766845703
    },
    {
      "epoch": 0.8784823848238482,
      "step": 4052,
      "training_loss": 5.111044406890869
    },
    {
      "epoch": 0.8784823848238482,
      "step": 4052,
      "training_loss": 4.823615074157715
    },
    {
      "epoch": 0.8786991869918699,
      "step": 4053,
      "training_loss": 6.7487688064575195
    },
    {
      "epoch": 0.8786991869918699,
      "step": 4053,
      "training_loss": 5.197580814361572
    },
    {
      "epoch": 0.8786991869918699,
      "step": 4053,
      "training_loss": 7.98778772354126
    },
    {
      "epoch": 0.8786991869918699,
      "step": 4053,
      "training_loss": 5.962286472320557
    },
    {
      "epoch": 0.8789159891598916,
      "step": 4054,
      "training_loss": 6.01294469833374
    },
    {
      "epoch": 0.8789159891598916,
      "step": 4054,
      "training_loss": 6.187473773956299
    },
    {
      "epoch": 0.8789159891598916,
      "step": 4054,
      "training_loss": 6.287303924560547
    },
    {
      "epoch": 0.8789159891598916,
      "step": 4054,
      "training_loss": 7.180152416229248
    },
    {
      "epoch": 0.8791327913279133,
      "step": 4055,
      "training_loss": 7.539515495300293
    },
    {
      "epoch": 0.8791327913279133,
      "step": 4055,
      "training_loss": 7.629431247711182
    },
    {
      "epoch": 0.8791327913279133,
      "step": 4055,
      "training_loss": 6.212229251861572
    },
    {
      "epoch": 0.8791327913279133,
      "step": 4055,
      "training_loss": 7.371654987335205
    },
    {
      "epoch": 0.879349593495935,
      "grad_norm": 19.758527755737305,
      "learning_rate": 1e-05,
      "loss": 6.4555,
      "step": 4056
    },
    {
      "epoch": 0.879349593495935,
      "step": 4056,
      "training_loss": 6.776848316192627
    },
    {
      "epoch": 0.879349593495935,
      "step": 4056,
      "training_loss": 5.82703161239624
    },
    {
      "epoch": 0.879349593495935,
      "step": 4056,
      "training_loss": 7.093405246734619
    },
    {
      "epoch": 0.879349593495935,
      "step": 4056,
      "training_loss": 6.030340671539307
    },
    {
      "epoch": 0.8795663956639567,
      "step": 4057,
      "training_loss": 5.589148044586182
    },
    {
      "epoch": 0.8795663956639567,
      "step": 4057,
      "training_loss": 6.661905288696289
    },
    {
      "epoch": 0.8795663956639567,
      "step": 4057,
      "training_loss": 6.630857467651367
    },
    {
      "epoch": 0.8795663956639567,
      "step": 4057,
      "training_loss": 6.175134658813477
    },
    {
      "epoch": 0.8797831978319783,
      "step": 4058,
      "training_loss": 5.0364274978637695
    },
    {
      "epoch": 0.8797831978319783,
      "step": 4058,
      "training_loss": 3.2888715267181396
    },
    {
      "epoch": 0.8797831978319783,
      "step": 4058,
      "training_loss": 6.577570915222168
    },
    {
      "epoch": 0.8797831978319783,
      "step": 4058,
      "training_loss": 7.6903557777404785
    },
    {
      "epoch": 0.88,
      "step": 4059,
      "training_loss": 7.233635425567627
    },
    {
      "epoch": 0.88,
      "step": 4059,
      "training_loss": 6.207185745239258
    },
    {
      "epoch": 0.88,
      "step": 4059,
      "training_loss": 4.963813304901123
    },
    {
      "epoch": 0.88,
      "step": 4059,
      "training_loss": 6.244637966156006
    },
    {
      "epoch": 0.8802168021680217,
      "grad_norm": 16.684104919433594,
      "learning_rate": 1e-05,
      "loss": 6.1267,
      "step": 4060
    },
    {
      "epoch": 0.8802168021680217,
      "step": 4060,
      "training_loss": 5.437343597412109
    },
    {
      "epoch": 0.8802168021680217,
      "step": 4060,
      "training_loss": 6.835655212402344
    },
    {
      "epoch": 0.8802168021680217,
      "step": 4060,
      "training_loss": 6.625020503997803
    },
    {
      "epoch": 0.8802168021680217,
      "step": 4060,
      "training_loss": 3.4993908405303955
    },
    {
      "epoch": 0.8804336043360433,
      "step": 4061,
      "training_loss": 6.2865309715271
    },
    {
      "epoch": 0.8804336043360433,
      "step": 4061,
      "training_loss": 4.555120468139648
    },
    {
      "epoch": 0.8804336043360433,
      "step": 4061,
      "training_loss": 6.136777400970459
    },
    {
      "epoch": 0.8804336043360433,
      "step": 4061,
      "training_loss": 7.823604106903076
    },
    {
      "epoch": 0.880650406504065,
      "step": 4062,
      "training_loss": 3.7055373191833496
    },
    {
      "epoch": 0.880650406504065,
      "step": 4062,
      "training_loss": 4.4401326179504395
    },
    {
      "epoch": 0.880650406504065,
      "step": 4062,
      "training_loss": 7.781717777252197
    },
    {
      "epoch": 0.880650406504065,
      "step": 4062,
      "training_loss": 8.904379844665527
    },
    {
      "epoch": 0.8808672086720867,
      "step": 4063,
      "training_loss": 6.361655235290527
    },
    {
      "epoch": 0.8808672086720867,
      "step": 4063,
      "training_loss": 5.941197872161865
    },
    {
      "epoch": 0.8808672086720867,
      "step": 4063,
      "training_loss": 5.54684591293335
    },
    {
      "epoch": 0.8808672086720867,
      "step": 4063,
      "training_loss": 6.699047088623047
    },
    {
      "epoch": 0.8810840108401085,
      "grad_norm": 25.004545211791992,
      "learning_rate": 1e-05,
      "loss": 6.0362,
      "step": 4064
    },
    {
      "epoch": 0.8810840108401085,
      "step": 4064,
      "training_loss": 6.475502967834473
    },
    {
      "epoch": 0.8810840108401085,
      "step": 4064,
      "training_loss": 4.252259254455566
    },
    {
      "epoch": 0.8810840108401085,
      "step": 4064,
      "training_loss": 5.793476104736328
    },
    {
      "epoch": 0.8810840108401085,
      "step": 4064,
      "training_loss": 6.697264194488525
    },
    {
      "epoch": 0.8813008130081301,
      "step": 4065,
      "training_loss": 7.597846508026123
    },
    {
      "epoch": 0.8813008130081301,
      "step": 4065,
      "training_loss": 7.1319451332092285
    },
    {
      "epoch": 0.8813008130081301,
      "step": 4065,
      "training_loss": 7.947328090667725
    },
    {
      "epoch": 0.8813008130081301,
      "step": 4065,
      "training_loss": 5.462745666503906
    },
    {
      "epoch": 0.8815176151761518,
      "step": 4066,
      "training_loss": 6.92222785949707
    },
    {
      "epoch": 0.8815176151761518,
      "step": 4066,
      "training_loss": 5.316251754760742
    },
    {
      "epoch": 0.8815176151761518,
      "step": 4066,
      "training_loss": 6.499124526977539
    },
    {
      "epoch": 0.8815176151761518,
      "step": 4066,
      "training_loss": 7.027797698974609
    },
    {
      "epoch": 0.8817344173441735,
      "step": 4067,
      "training_loss": 5.545068264007568
    },
    {
      "epoch": 0.8817344173441735,
      "step": 4067,
      "training_loss": 4.874500274658203
    },
    {
      "epoch": 0.8817344173441735,
      "step": 4067,
      "training_loss": 8.63535213470459
    },
    {
      "epoch": 0.8817344173441735,
      "step": 4067,
      "training_loss": 7.09857177734375
    },
    {
      "epoch": 0.8819512195121951,
      "grad_norm": 19.546253204345703,
      "learning_rate": 1e-05,
      "loss": 6.4548,
      "step": 4068
    },
    {
      "epoch": 0.8819512195121951,
      "step": 4068,
      "training_loss": 4.182682514190674
    },
    {
      "epoch": 0.8819512195121951,
      "step": 4068,
      "training_loss": 5.7519707679748535
    },
    {
      "epoch": 0.8819512195121951,
      "step": 4068,
      "training_loss": 4.312086582183838
    },
    {
      "epoch": 0.8819512195121951,
      "step": 4068,
      "training_loss": 5.234894275665283
    },
    {
      "epoch": 0.8821680216802168,
      "step": 4069,
      "training_loss": 7.195417881011963
    },
    {
      "epoch": 0.8821680216802168,
      "step": 4069,
      "training_loss": 6.550197601318359
    },
    {
      "epoch": 0.8821680216802168,
      "step": 4069,
      "training_loss": 5.409212112426758
    },
    {
      "epoch": 0.8821680216802168,
      "step": 4069,
      "training_loss": 4.687629699707031
    },
    {
      "epoch": 0.8823848238482385,
      "step": 4070,
      "training_loss": 7.362338542938232
    },
    {
      "epoch": 0.8823848238482385,
      "step": 4070,
      "training_loss": 6.40472412109375
    },
    {
      "epoch": 0.8823848238482385,
      "step": 4070,
      "training_loss": 4.3018341064453125
    },
    {
      "epoch": 0.8823848238482385,
      "step": 4070,
      "training_loss": 8.079240798950195
    },
    {
      "epoch": 0.8826016260162601,
      "step": 4071,
      "training_loss": 7.094765663146973
    },
    {
      "epoch": 0.8826016260162601,
      "step": 4071,
      "training_loss": 6.397642612457275
    },
    {
      "epoch": 0.8826016260162601,
      "step": 4071,
      "training_loss": 6.793680667877197
    },
    {
      "epoch": 0.8826016260162601,
      "step": 4071,
      "training_loss": 7.267760753631592
    },
    {
      "epoch": 0.8828184281842818,
      "grad_norm": 17.36028480529785,
      "learning_rate": 1e-05,
      "loss": 6.0641,
      "step": 4072
    },
    {
      "epoch": 0.8828184281842818,
      "step": 4072,
      "training_loss": 7.043219089508057
    },
    {
      "epoch": 0.8828184281842818,
      "step": 4072,
      "training_loss": 6.714441299438477
    },
    {
      "epoch": 0.8828184281842818,
      "step": 4072,
      "training_loss": 6.746426582336426
    },
    {
      "epoch": 0.8828184281842818,
      "step": 4072,
      "training_loss": 7.577476501464844
    },
    {
      "epoch": 0.8830352303523035,
      "step": 4073,
      "training_loss": 3.119260549545288
    },
    {
      "epoch": 0.8830352303523035,
      "step": 4073,
      "training_loss": 6.388378620147705
    },
    {
      "epoch": 0.8830352303523035,
      "step": 4073,
      "training_loss": 4.988319396972656
    },
    {
      "epoch": 0.8830352303523035,
      "step": 4073,
      "training_loss": 6.551600933074951
    },
    {
      "epoch": 0.8832520325203252,
      "step": 4074,
      "training_loss": 6.072044372558594
    },
    {
      "epoch": 0.8832520325203252,
      "step": 4074,
      "training_loss": 4.871196746826172
    },
    {
      "epoch": 0.8832520325203252,
      "step": 4074,
      "training_loss": 6.01839542388916
    },
    {
      "epoch": 0.8832520325203252,
      "step": 4074,
      "training_loss": 7.079641819000244
    },
    {
      "epoch": 0.8834688346883469,
      "step": 4075,
      "training_loss": 4.928049087524414
    },
    {
      "epoch": 0.8834688346883469,
      "step": 4075,
      "training_loss": 7.061304569244385
    },
    {
      "epoch": 0.8834688346883469,
      "step": 4075,
      "training_loss": 7.340644836425781
    },
    {
      "epoch": 0.8834688346883469,
      "step": 4075,
      "training_loss": 3.207604169845581
    },
    {
      "epoch": 0.8836856368563686,
      "grad_norm": 13.869942665100098,
      "learning_rate": 1e-05,
      "loss": 5.9818,
      "step": 4076
    },
    {
      "epoch": 0.8836856368563686,
      "step": 4076,
      "training_loss": 6.837130069732666
    },
    {
      "epoch": 0.8836856368563686,
      "step": 4076,
      "training_loss": 5.467987537384033
    },
    {
      "epoch": 0.8836856368563686,
      "step": 4076,
      "training_loss": 6.156922817230225
    },
    {
      "epoch": 0.8836856368563686,
      "step": 4076,
      "training_loss": 6.76610803604126
    },
    {
      "epoch": 0.8839024390243903,
      "step": 4077,
      "training_loss": 3.705601930618286
    },
    {
      "epoch": 0.8839024390243903,
      "step": 4077,
      "training_loss": 8.16834545135498
    },
    {
      "epoch": 0.8839024390243903,
      "step": 4077,
      "training_loss": 7.108102321624756
    },
    {
      "epoch": 0.8839024390243903,
      "step": 4077,
      "training_loss": 6.460026741027832
    },
    {
      "epoch": 0.8841192411924119,
      "step": 4078,
      "training_loss": 2.9854090213775635
    },
    {
      "epoch": 0.8841192411924119,
      "step": 4078,
      "training_loss": 5.162923812866211
    },
    {
      "epoch": 0.8841192411924119,
      "step": 4078,
      "training_loss": 6.221417427062988
    },
    {
      "epoch": 0.8841192411924119,
      "step": 4078,
      "training_loss": 6.557411193847656
    },
    {
      "epoch": 0.8843360433604336,
      "step": 4079,
      "training_loss": 2.4499924182891846
    },
    {
      "epoch": 0.8843360433604336,
      "step": 4079,
      "training_loss": 6.806685447692871
    },
    {
      "epoch": 0.8843360433604336,
      "step": 4079,
      "training_loss": 6.351051330566406
    },
    {
      "epoch": 0.8843360433604336,
      "step": 4079,
      "training_loss": 4.918578624725342
    },
    {
      "epoch": 0.8845528455284553,
      "grad_norm": 16.592304229736328,
      "learning_rate": 1e-05,
      "loss": 5.7577,
      "step": 4080
    },
    {
      "epoch": 0.8845528455284553,
      "step": 4080,
      "training_loss": 6.547588348388672
    },
    {
      "epoch": 0.8845528455284553,
      "step": 4080,
      "training_loss": 5.846896648406982
    },
    {
      "epoch": 0.8845528455284553,
      "step": 4080,
      "training_loss": 7.60869026184082
    },
    {
      "epoch": 0.8845528455284553,
      "step": 4080,
      "training_loss": 6.032585620880127
    },
    {
      "epoch": 0.8847696476964769,
      "step": 4081,
      "training_loss": 7.746645450592041
    },
    {
      "epoch": 0.8847696476964769,
      "step": 4081,
      "training_loss": 6.993377208709717
    },
    {
      "epoch": 0.8847696476964769,
      "step": 4081,
      "training_loss": 6.809483528137207
    },
    {
      "epoch": 0.8847696476964769,
      "step": 4081,
      "training_loss": 5.664167404174805
    },
    {
      "epoch": 0.8849864498644986,
      "step": 4082,
      "training_loss": 7.940215587615967
    },
    {
      "epoch": 0.8849864498644986,
      "step": 4082,
      "training_loss": 6.28892183303833
    },
    {
      "epoch": 0.8849864498644986,
      "step": 4082,
      "training_loss": 5.289709568023682
    },
    {
      "epoch": 0.8849864498644986,
      "step": 4082,
      "training_loss": 7.75875997543335
    },
    {
      "epoch": 0.8852032520325204,
      "step": 4083,
      "training_loss": 3.787654399871826
    },
    {
      "epoch": 0.8852032520325204,
      "step": 4083,
      "training_loss": 6.58490514755249
    },
    {
      "epoch": 0.8852032520325204,
      "step": 4083,
      "training_loss": 4.844923496246338
    },
    {
      "epoch": 0.8852032520325204,
      "step": 4083,
      "training_loss": 6.606764316558838
    },
    {
      "epoch": 0.885420054200542,
      "grad_norm": 14.380383491516113,
      "learning_rate": 1e-05,
      "loss": 6.397,
      "step": 4084
    },
    {
      "epoch": 0.885420054200542,
      "step": 4084,
      "training_loss": 6.373894214630127
    },
    {
      "epoch": 0.885420054200542,
      "step": 4084,
      "training_loss": 6.667700290679932
    },
    {
      "epoch": 0.885420054200542,
      "step": 4084,
      "training_loss": 5.695860385894775
    },
    {
      "epoch": 0.885420054200542,
      "step": 4084,
      "training_loss": 7.135026454925537
    },
    {
      "epoch": 0.8856368563685637,
      "step": 4085,
      "training_loss": 2.998971700668335
    },
    {
      "epoch": 0.8856368563685637,
      "step": 4085,
      "training_loss": 5.766110420227051
    },
    {
      "epoch": 0.8856368563685637,
      "step": 4085,
      "training_loss": 6.16002082824707
    },
    {
      "epoch": 0.8856368563685637,
      "step": 4085,
      "training_loss": 3.8254201412200928
    },
    {
      "epoch": 0.8858536585365854,
      "step": 4086,
      "training_loss": 7.345449447631836
    },
    {
      "epoch": 0.8858536585365854,
      "step": 4086,
      "training_loss": 7.760059833526611
    },
    {
      "epoch": 0.8858536585365854,
      "step": 4086,
      "training_loss": 5.987699031829834
    },
    {
      "epoch": 0.8858536585365854,
      "step": 4086,
      "training_loss": 6.835552215576172
    },
    {
      "epoch": 0.886070460704607,
      "step": 4087,
      "training_loss": 5.338864326477051
    },
    {
      "epoch": 0.886070460704607,
      "step": 4087,
      "training_loss": 6.879730224609375
    },
    {
      "epoch": 0.886070460704607,
      "step": 4087,
      "training_loss": 6.529350280761719
    },
    {
      "epoch": 0.886070460704607,
      "step": 4087,
      "training_loss": 6.982777118682861
    },
    {
      "epoch": 0.8862872628726287,
      "grad_norm": 21.945310592651367,
      "learning_rate": 1e-05,
      "loss": 6.1427,
      "step": 4088
    },
    {
      "epoch": 0.8862872628726287,
      "step": 4088,
      "training_loss": 6.951760292053223
    },
    {
      "epoch": 0.8862872628726287,
      "step": 4088,
      "training_loss": 8.145990371704102
    },
    {
      "epoch": 0.8862872628726287,
      "step": 4088,
      "training_loss": 5.589856147766113
    },
    {
      "epoch": 0.8862872628726287,
      "step": 4088,
      "training_loss": 6.776974678039551
    },
    {
      "epoch": 0.8865040650406504,
      "step": 4089,
      "training_loss": 6.395714282989502
    },
    {
      "epoch": 0.8865040650406504,
      "step": 4089,
      "training_loss": 7.528924465179443
    },
    {
      "epoch": 0.8865040650406504,
      "step": 4089,
      "training_loss": 6.706339359283447
    },
    {
      "epoch": 0.8865040650406504,
      "step": 4089,
      "training_loss": 8.204255104064941
    },
    {
      "epoch": 0.886720867208672,
      "step": 4090,
      "training_loss": 6.398102283477783
    },
    {
      "epoch": 0.886720867208672,
      "step": 4090,
      "training_loss": 6.867908000946045
    },
    {
      "epoch": 0.886720867208672,
      "step": 4090,
      "training_loss": 5.503391742706299
    },
    {
      "epoch": 0.886720867208672,
      "step": 4090,
      "training_loss": 5.0548505783081055
    },
    {
      "epoch": 0.8869376693766937,
      "step": 4091,
      "training_loss": 6.849502086639404
    },
    {
      "epoch": 0.8869376693766937,
      "step": 4091,
      "training_loss": 6.116467475891113
    },
    {
      "epoch": 0.8869376693766937,
      "step": 4091,
      "training_loss": 6.535058975219727
    },
    {
      "epoch": 0.8869376693766937,
      "step": 4091,
      "training_loss": 5.689640998840332
    },
    {
      "epoch": 0.8871544715447155,
      "grad_norm": 29.29030990600586,
      "learning_rate": 1e-05,
      "loss": 6.5822,
      "step": 4092
    },
    {
      "epoch": 0.8871544715447155,
      "step": 4092,
      "training_loss": 6.031860828399658
    },
    {
      "epoch": 0.8871544715447155,
      "step": 4092,
      "training_loss": 5.949797630310059
    },
    {
      "epoch": 0.8871544715447155,
      "step": 4092,
      "training_loss": 2.9815049171447754
    },
    {
      "epoch": 0.8871544715447155,
      "step": 4092,
      "training_loss": 6.025636672973633
    },
    {
      "epoch": 0.8873712737127372,
      "step": 4093,
      "training_loss": 6.0911712646484375
    },
    {
      "epoch": 0.8873712737127372,
      "step": 4093,
      "training_loss": 6.820331573486328
    },
    {
      "epoch": 0.8873712737127372,
      "step": 4093,
      "training_loss": 7.159931182861328
    },
    {
      "epoch": 0.8873712737127372,
      "step": 4093,
      "training_loss": 3.845776319503784
    },
    {
      "epoch": 0.8875880758807588,
      "step": 4094,
      "training_loss": 6.231144905090332
    },
    {
      "epoch": 0.8875880758807588,
      "step": 4094,
      "training_loss": 7.139910697937012
    },
    {
      "epoch": 0.8875880758807588,
      "step": 4094,
      "training_loss": 6.584168434143066
    },
    {
      "epoch": 0.8875880758807588,
      "step": 4094,
      "training_loss": 7.254952430725098
    },
    {
      "epoch": 0.8878048780487805,
      "step": 4095,
      "training_loss": 5.865952968597412
    },
    {
      "epoch": 0.8878048780487805,
      "step": 4095,
      "training_loss": 6.333068370819092
    },
    {
      "epoch": 0.8878048780487805,
      "step": 4095,
      "training_loss": 6.473057746887207
    },
    {
      "epoch": 0.8878048780487805,
      "step": 4095,
      "training_loss": 5.9363484382629395
    },
    {
      "epoch": 0.8880216802168022,
      "grad_norm": 20.842161178588867,
      "learning_rate": 1e-05,
      "loss": 6.0453,
      "step": 4096
    },
    {
      "epoch": 0.8880216802168022,
      "step": 4096,
      "training_loss": 5.428167819976807
    },
    {
      "epoch": 0.8880216802168022,
      "step": 4096,
      "training_loss": 6.779132843017578
    },
    {
      "epoch": 0.8880216802168022,
      "step": 4096,
      "training_loss": 6.200276851654053
    },
    {
      "epoch": 0.8880216802168022,
      "step": 4096,
      "training_loss": 5.777545928955078
    },
    {
      "epoch": 0.8882384823848238,
      "step": 4097,
      "training_loss": 6.094165802001953
    },
    {
      "epoch": 0.8882384823848238,
      "step": 4097,
      "training_loss": 4.886135578155518
    },
    {
      "epoch": 0.8882384823848238,
      "step": 4097,
      "training_loss": 5.5810441970825195
    },
    {
      "epoch": 0.8882384823848238,
      "step": 4097,
      "training_loss": 5.683715343475342
    },
    {
      "epoch": 0.8884552845528455,
      "step": 4098,
      "training_loss": 6.180953025817871
    },
    {
      "epoch": 0.8884552845528455,
      "step": 4098,
      "training_loss": 5.882664680480957
    },
    {
      "epoch": 0.8884552845528455,
      "step": 4098,
      "training_loss": 4.720163345336914
    },
    {
      "epoch": 0.8884552845528455,
      "step": 4098,
      "training_loss": 7.071417331695557
    },
    {
      "epoch": 0.8886720867208672,
      "step": 4099,
      "training_loss": 6.818479537963867
    },
    {
      "epoch": 0.8886720867208672,
      "step": 4099,
      "training_loss": 7.301397800445557
    },
    {
      "epoch": 0.8886720867208672,
      "step": 4099,
      "training_loss": 6.163598537445068
    },
    {
      "epoch": 0.8886720867208672,
      "step": 4099,
      "training_loss": 7.464669227600098
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 13.009636878967285,
      "learning_rate": 1e-05,
      "loss": 6.1271,
      "step": 4100
    },
    {
      "epoch": 0.8888888888888888,
      "step": 4100,
      "training_loss": 5.840938091278076
    },
    {
      "epoch": 0.8888888888888888,
      "step": 4100,
      "training_loss": 6.864857196807861
    },
    {
      "epoch": 0.8888888888888888,
      "step": 4100,
      "training_loss": 7.301661491394043
    },
    {
      "epoch": 0.8888888888888888,
      "step": 4100,
      "training_loss": 6.728633403778076
    },
    {
      "epoch": 0.8891056910569106,
      "step": 4101,
      "training_loss": 6.990399360656738
    },
    {
      "epoch": 0.8891056910569106,
      "step": 4101,
      "training_loss": 6.619740962982178
    },
    {
      "epoch": 0.8891056910569106,
      "step": 4101,
      "training_loss": 5.846786022186279
    },
    {
      "epoch": 0.8891056910569106,
      "step": 4101,
      "training_loss": 7.566487789154053
    },
    {
      "epoch": 0.8893224932249323,
      "step": 4102,
      "training_loss": 6.727093696594238
    },
    {
      "epoch": 0.8893224932249323,
      "step": 4102,
      "training_loss": 6.452146530151367
    },
    {
      "epoch": 0.8893224932249323,
      "step": 4102,
      "training_loss": 5.4222283363342285
    },
    {
      "epoch": 0.8893224932249323,
      "step": 4102,
      "training_loss": 6.3358001708984375
    },
    {
      "epoch": 0.889539295392954,
      "step": 4103,
      "training_loss": 6.662231922149658
    },
    {
      "epoch": 0.889539295392954,
      "step": 4103,
      "training_loss": 7.863996505737305
    },
    {
      "epoch": 0.889539295392954,
      "step": 4103,
      "training_loss": 6.2557549476623535
    },
    {
      "epoch": 0.889539295392954,
      "step": 4103,
      "training_loss": 6.375751972198486
    },
    {
      "epoch": 0.8897560975609756,
      "grad_norm": 17.663732528686523,
      "learning_rate": 1e-05,
      "loss": 6.6159,
      "step": 4104
    },
    {
      "epoch": 0.8897560975609756,
      "step": 4104,
      "training_loss": 5.899674415588379
    },
    {
      "epoch": 0.8897560975609756,
      "step": 4104,
      "training_loss": 6.0487847328186035
    },
    {
      "epoch": 0.8897560975609756,
      "step": 4104,
      "training_loss": 6.8164191246032715
    },
    {
      "epoch": 0.8897560975609756,
      "step": 4104,
      "training_loss": 6.110044002532959
    },
    {
      "epoch": 0.8899728997289973,
      "step": 4105,
      "training_loss": 6.5676140785217285
    },
    {
      "epoch": 0.8899728997289973,
      "step": 4105,
      "training_loss": 5.8959479331970215
    },
    {
      "epoch": 0.8899728997289973,
      "step": 4105,
      "training_loss": 5.792463779449463
    },
    {
      "epoch": 0.8899728997289973,
      "step": 4105,
      "training_loss": 6.07440185546875
    },
    {
      "epoch": 0.890189701897019,
      "step": 4106,
      "training_loss": 6.80009651184082
    },
    {
      "epoch": 0.890189701897019,
      "step": 4106,
      "training_loss": 6.869531154632568
    },
    {
      "epoch": 0.890189701897019,
      "step": 4106,
      "training_loss": 6.701502799987793
    },
    {
      "epoch": 0.890189701897019,
      "step": 4106,
      "training_loss": 7.439267158508301
    },
    {
      "epoch": 0.8904065040650406,
      "step": 4107,
      "training_loss": 6.6910600662231445
    },
    {
      "epoch": 0.8904065040650406,
      "step": 4107,
      "training_loss": 6.288403511047363
    },
    {
      "epoch": 0.8904065040650406,
      "step": 4107,
      "training_loss": 4.656711101531982
    },
    {
      "epoch": 0.8904065040650406,
      "step": 4107,
      "training_loss": 7.240418910980225
    },
    {
      "epoch": 0.8906233062330623,
      "grad_norm": 16.829059600830078,
      "learning_rate": 1e-05,
      "loss": 6.3683,
      "step": 4108
    },
    {
      "epoch": 0.8906233062330623,
      "step": 4108,
      "training_loss": 7.383669376373291
    },
    {
      "epoch": 0.8906233062330623,
      "step": 4108,
      "training_loss": 7.35686731338501
    },
    {
      "epoch": 0.8906233062330623,
      "step": 4108,
      "training_loss": 2.907700777053833
    },
    {
      "epoch": 0.8906233062330623,
      "step": 4108,
      "training_loss": 5.559650421142578
    },
    {
      "epoch": 0.890840108401084,
      "step": 4109,
      "training_loss": 6.257761001586914
    },
    {
      "epoch": 0.890840108401084,
      "step": 4109,
      "training_loss": 7.481703758239746
    },
    {
      "epoch": 0.890840108401084,
      "step": 4109,
      "training_loss": 5.2291998863220215
    },
    {
      "epoch": 0.890840108401084,
      "step": 4109,
      "training_loss": 6.941852569580078
    },
    {
      "epoch": 0.8910569105691057,
      "step": 4110,
      "training_loss": 5.532704830169678
    },
    {
      "epoch": 0.8910569105691057,
      "step": 4110,
      "training_loss": 7.689955711364746
    },
    {
      "epoch": 0.8910569105691057,
      "step": 4110,
      "training_loss": 7.312982559204102
    },
    {
      "epoch": 0.8910569105691057,
      "step": 4110,
      "training_loss": 5.537599563598633
    },
    {
      "epoch": 0.8912737127371274,
      "step": 4111,
      "training_loss": 6.641284942626953
    },
    {
      "epoch": 0.8912737127371274,
      "step": 4111,
      "training_loss": 7.695694923400879
    },
    {
      "epoch": 0.8912737127371274,
      "step": 4111,
      "training_loss": 5.499202251434326
    },
    {
      "epoch": 0.8912737127371274,
      "step": 4111,
      "training_loss": 7.813758850097656
    },
    {
      "epoch": 0.8914905149051491,
      "grad_norm": 22.27336311340332,
      "learning_rate": 1e-05,
      "loss": 6.4276,
      "step": 4112
    },
    {
      "epoch": 0.8914905149051491,
      "step": 4112,
      "training_loss": 5.138230323791504
    },
    {
      "epoch": 0.8914905149051491,
      "step": 4112,
      "training_loss": 6.42737340927124
    },
    {
      "epoch": 0.8914905149051491,
      "step": 4112,
      "training_loss": 4.394537448883057
    },
    {
      "epoch": 0.8914905149051491,
      "step": 4112,
      "training_loss": 6.606234073638916
    },
    {
      "epoch": 0.8917073170731707,
      "step": 4113,
      "training_loss": 7.2458672523498535
    },
    {
      "epoch": 0.8917073170731707,
      "step": 4113,
      "training_loss": 5.761814594268799
    },
    {
      "epoch": 0.8917073170731707,
      "step": 4113,
      "training_loss": 6.046599864959717
    },
    {
      "epoch": 0.8917073170731707,
      "step": 4113,
      "training_loss": 6.608985900878906
    },
    {
      "epoch": 0.8919241192411924,
      "step": 4114,
      "training_loss": 5.578901767730713
    },
    {
      "epoch": 0.8919241192411924,
      "step": 4114,
      "training_loss": 5.491050720214844
    },
    {
      "epoch": 0.8919241192411924,
      "step": 4114,
      "training_loss": 4.422133922576904
    },
    {
      "epoch": 0.8919241192411924,
      "step": 4114,
      "training_loss": 5.158595085144043
    },
    {
      "epoch": 0.8921409214092141,
      "step": 4115,
      "training_loss": 5.545312404632568
    },
    {
      "epoch": 0.8921409214092141,
      "step": 4115,
      "training_loss": 6.020584583282471
    },
    {
      "epoch": 0.8921409214092141,
      "step": 4115,
      "training_loss": 5.249550819396973
    },
    {
      "epoch": 0.8921409214092141,
      "step": 4115,
      "training_loss": 5.141318321228027
    },
    {
      "epoch": 0.8923577235772358,
      "grad_norm": 15.850379943847656,
      "learning_rate": 1e-05,
      "loss": 5.6773,
      "step": 4116
    },
    {
      "epoch": 0.8923577235772358,
      "step": 4116,
      "training_loss": 7.141247272491455
    },
    {
      "epoch": 0.8923577235772358,
      "step": 4116,
      "training_loss": 7.567742824554443
    },
    {
      "epoch": 0.8923577235772358,
      "step": 4116,
      "training_loss": 6.989232063293457
    },
    {
      "epoch": 0.8923577235772358,
      "step": 4116,
      "training_loss": 5.664669036865234
    },
    {
      "epoch": 0.8925745257452574,
      "step": 4117,
      "training_loss": 7.222596168518066
    },
    {
      "epoch": 0.8925745257452574,
      "step": 4117,
      "training_loss": 3.273655414581299
    },
    {
      "epoch": 0.8925745257452574,
      "step": 4117,
      "training_loss": 3.8570499420166016
    },
    {
      "epoch": 0.8925745257452574,
      "step": 4117,
      "training_loss": 6.611133098602295
    },
    {
      "epoch": 0.8927913279132791,
      "step": 4118,
      "training_loss": 6.6145524978637695
    },
    {
      "epoch": 0.8927913279132791,
      "step": 4118,
      "training_loss": 7.533972263336182
    },
    {
      "epoch": 0.8927913279132791,
      "step": 4118,
      "training_loss": 6.357668876647949
    },
    {
      "epoch": 0.8927913279132791,
      "step": 4118,
      "training_loss": 6.60348653793335
    },
    {
      "epoch": 0.8930081300813009,
      "step": 4119,
      "training_loss": 4.417904853820801
    },
    {
      "epoch": 0.8930081300813009,
      "step": 4119,
      "training_loss": 6.683840751647949
    },
    {
      "epoch": 0.8930081300813009,
      "step": 4119,
      "training_loss": 5.648143768310547
    },
    {
      "epoch": 0.8930081300813009,
      "step": 4119,
      "training_loss": 7.803708076477051
    },
    {
      "epoch": 0.8932249322493225,
      "grad_norm": 19.68126106262207,
      "learning_rate": 1e-05,
      "loss": 6.2494,
      "step": 4120
    },
    {
      "epoch": 0.8932249322493225,
      "step": 4120,
      "training_loss": 6.723635673522949
    },
    {
      "epoch": 0.8932249322493225,
      "step": 4120,
      "training_loss": 7.184828758239746
    },
    {
      "epoch": 0.8932249322493225,
      "step": 4120,
      "training_loss": 7.472036838531494
    },
    {
      "epoch": 0.8932249322493225,
      "step": 4120,
      "training_loss": 7.331043720245361
    },
    {
      "epoch": 0.8934417344173442,
      "step": 4121,
      "training_loss": 4.278820514678955
    },
    {
      "epoch": 0.8934417344173442,
      "step": 4121,
      "training_loss": 4.924734592437744
    },
    {
      "epoch": 0.8934417344173442,
      "step": 4121,
      "training_loss": 7.216177940368652
    },
    {
      "epoch": 0.8934417344173442,
      "step": 4121,
      "training_loss": 7.21127462387085
    },
    {
      "epoch": 0.8936585365853659,
      "step": 4122,
      "training_loss": 6.122307777404785
    },
    {
      "epoch": 0.8936585365853659,
      "step": 4122,
      "training_loss": 6.457540512084961
    },
    {
      "epoch": 0.8936585365853659,
      "step": 4122,
      "training_loss": 4.891684055328369
    },
    {
      "epoch": 0.8936585365853659,
      "step": 4122,
      "training_loss": 6.5427422523498535
    },
    {
      "epoch": 0.8938753387533875,
      "step": 4123,
      "training_loss": 7.495147705078125
    },
    {
      "epoch": 0.8938753387533875,
      "step": 4123,
      "training_loss": 5.569281101226807
    },
    {
      "epoch": 0.8938753387533875,
      "step": 4123,
      "training_loss": 7.301988124847412
    },
    {
      "epoch": 0.8938753387533875,
      "step": 4123,
      "training_loss": 6.432719707489014
    },
    {
      "epoch": 0.8940921409214092,
      "grad_norm": 16.16411590576172,
      "learning_rate": 1e-05,
      "loss": 6.4472,
      "step": 4124
    },
    {
      "epoch": 0.8940921409214092,
      "step": 4124,
      "training_loss": 6.565281391143799
    },
    {
      "epoch": 0.8940921409214092,
      "step": 4124,
      "training_loss": 6.1957621574401855
    },
    {
      "epoch": 0.8940921409214092,
      "step": 4124,
      "training_loss": 6.419304370880127
    },
    {
      "epoch": 0.8940921409214092,
      "step": 4124,
      "training_loss": 8.534361839294434
    },
    {
      "epoch": 0.8943089430894309,
      "step": 4125,
      "training_loss": 6.606227397918701
    },
    {
      "epoch": 0.8943089430894309,
      "step": 4125,
      "training_loss": 7.159631252288818
    },
    {
      "epoch": 0.8943089430894309,
      "step": 4125,
      "training_loss": 6.414233207702637
    },
    {
      "epoch": 0.8943089430894309,
      "step": 4125,
      "training_loss": 5.159389019012451
    },
    {
      "epoch": 0.8945257452574525,
      "step": 4126,
      "training_loss": 6.642746448516846
    },
    {
      "epoch": 0.8945257452574525,
      "step": 4126,
      "training_loss": 8.399500846862793
    },
    {
      "epoch": 0.8945257452574525,
      "step": 4126,
      "training_loss": 6.658336639404297
    },
    {
      "epoch": 0.8945257452574525,
      "step": 4126,
      "training_loss": 7.633028030395508
    },
    {
      "epoch": 0.8947425474254742,
      "step": 4127,
      "training_loss": 7.156891345977783
    },
    {
      "epoch": 0.8947425474254742,
      "step": 4127,
      "training_loss": 5.471240520477295
    },
    {
      "epoch": 0.8947425474254742,
      "step": 4127,
      "training_loss": 7.154416561126709
    },
    {
      "epoch": 0.8947425474254742,
      "step": 4127,
      "training_loss": 7.0919694900512695
    },
    {
      "epoch": 0.894959349593496,
      "grad_norm": 21.09823226928711,
      "learning_rate": 1e-05,
      "loss": 6.8289,
      "step": 4128
    },
    {
      "epoch": 0.894959349593496,
      "step": 4128,
      "training_loss": 6.726640701293945
    },
    {
      "epoch": 0.894959349593496,
      "step": 4128,
      "training_loss": 5.99201774597168
    },
    {
      "epoch": 0.894959349593496,
      "step": 4128,
      "training_loss": 6.186886310577393
    },
    {
      "epoch": 0.894959349593496,
      "step": 4128,
      "training_loss": 6.782756805419922
    },
    {
      "epoch": 0.8951761517615177,
      "step": 4129,
      "training_loss": 7.299293518066406
    },
    {
      "epoch": 0.8951761517615177,
      "step": 4129,
      "training_loss": 6.554586887359619
    },
    {
      "epoch": 0.8951761517615177,
      "step": 4129,
      "training_loss": 6.900013446807861
    },
    {
      "epoch": 0.8951761517615177,
      "step": 4129,
      "training_loss": 6.998874187469482
    },
    {
      "epoch": 0.8953929539295393,
      "step": 4130,
      "training_loss": 6.094365119934082
    },
    {
      "epoch": 0.8953929539295393,
      "step": 4130,
      "training_loss": 6.314747333526611
    },
    {
      "epoch": 0.8953929539295393,
      "step": 4130,
      "training_loss": 6.660050392150879
    },
    {
      "epoch": 0.8953929539295393,
      "step": 4130,
      "training_loss": 6.655138969421387
    },
    {
      "epoch": 0.895609756097561,
      "step": 4131,
      "training_loss": 7.043849468231201
    },
    {
      "epoch": 0.895609756097561,
      "step": 4131,
      "training_loss": 3.63199782371521
    },
    {
      "epoch": 0.895609756097561,
      "step": 4131,
      "training_loss": 8.276481628417969
    },
    {
      "epoch": 0.895609756097561,
      "step": 4131,
      "training_loss": 7.069542407989502
    },
    {
      "epoch": 0.8958265582655827,
      "grad_norm": 15.692538261413574,
      "learning_rate": 1e-05,
      "loss": 6.5742,
      "step": 4132
    },
    {
      "epoch": 0.8958265582655827,
      "step": 4132,
      "training_loss": 6.978647708892822
    },
    {
      "epoch": 0.8958265582655827,
      "step": 4132,
      "training_loss": 6.049382209777832
    },
    {
      "epoch": 0.8958265582655827,
      "step": 4132,
      "training_loss": 7.037489414215088
    },
    {
      "epoch": 0.8958265582655827,
      "step": 4132,
      "training_loss": 5.78428316116333
    },
    {
      "epoch": 0.8960433604336043,
      "step": 4133,
      "training_loss": 5.839103698730469
    },
    {
      "epoch": 0.8960433604336043,
      "step": 4133,
      "training_loss": 4.568490505218506
    },
    {
      "epoch": 0.8960433604336043,
      "step": 4133,
      "training_loss": 7.06265115737915
    },
    {
      "epoch": 0.8960433604336043,
      "step": 4133,
      "training_loss": 6.267350673675537
    },
    {
      "epoch": 0.896260162601626,
      "step": 4134,
      "training_loss": 6.6839447021484375
    },
    {
      "epoch": 0.896260162601626,
      "step": 4134,
      "training_loss": 6.6777024269104
    },
    {
      "epoch": 0.896260162601626,
      "step": 4134,
      "training_loss": 7.713300704956055
    },
    {
      "epoch": 0.896260162601626,
      "step": 4134,
      "training_loss": 4.704779624938965
    },
    {
      "epoch": 0.8964769647696477,
      "step": 4135,
      "training_loss": 5.84743070602417
    },
    {
      "epoch": 0.8964769647696477,
      "step": 4135,
      "training_loss": 6.483850479125977
    },
    {
      "epoch": 0.8964769647696477,
      "step": 4135,
      "training_loss": 6.262022495269775
    },
    {
      "epoch": 0.8964769647696477,
      "step": 4135,
      "training_loss": 7.3789544105529785
    },
    {
      "epoch": 0.8966937669376693,
      "grad_norm": 18.3900089263916,
      "learning_rate": 1e-05,
      "loss": 6.3337,
      "step": 4136
    },
    {
      "epoch": 0.8966937669376693,
      "step": 4136,
      "training_loss": 5.010498046875
    },
    {
      "epoch": 0.8966937669376693,
      "step": 4136,
      "training_loss": 6.064995765686035
    },
    {
      "epoch": 0.8966937669376693,
      "step": 4136,
      "training_loss": 5.553062438964844
    },
    {
      "epoch": 0.8966937669376693,
      "step": 4136,
      "training_loss": 5.9826459884643555
    },
    {
      "epoch": 0.896910569105691,
      "step": 4137,
      "training_loss": 6.023293495178223
    },
    {
      "epoch": 0.896910569105691,
      "step": 4137,
      "training_loss": 7.27319860458374
    },
    {
      "epoch": 0.896910569105691,
      "step": 4137,
      "training_loss": 6.557287216186523
    },
    {
      "epoch": 0.896910569105691,
      "step": 4137,
      "training_loss": 6.5537004470825195
    },
    {
      "epoch": 0.8971273712737128,
      "step": 4138,
      "training_loss": 6.866208553314209
    },
    {
      "epoch": 0.8971273712737128,
      "step": 4138,
      "training_loss": 7.663369178771973
    },
    {
      "epoch": 0.8971273712737128,
      "step": 4138,
      "training_loss": 4.100109100341797
    },
    {
      "epoch": 0.8971273712737128,
      "step": 4138,
      "training_loss": 6.527214527130127
    },
    {
      "epoch": 0.8973441734417344,
      "step": 4139,
      "training_loss": 4.773747444152832
    },
    {
      "epoch": 0.8973441734417344,
      "step": 4139,
      "training_loss": 10.163936614990234
    },
    {
      "epoch": 0.8973441734417344,
      "step": 4139,
      "training_loss": 4.321131706237793
    },
    {
      "epoch": 0.8973441734417344,
      "step": 4139,
      "training_loss": 4.446239948272705
    },
    {
      "epoch": 0.8975609756097561,
      "grad_norm": 22.07928466796875,
      "learning_rate": 1e-05,
      "loss": 6.1175,
      "step": 4140
    },
    {
      "epoch": 0.8975609756097561,
      "step": 4140,
      "training_loss": 4.34172248840332
    },
    {
      "epoch": 0.8975609756097561,
      "step": 4140,
      "training_loss": 8.58386516571045
    },
    {
      "epoch": 0.8975609756097561,
      "step": 4140,
      "training_loss": 7.039450645446777
    },
    {
      "epoch": 0.8975609756097561,
      "step": 4140,
      "training_loss": 7.700309753417969
    },
    {
      "epoch": 0.8977777777777778,
      "step": 4141,
      "training_loss": 6.2079362869262695
    },
    {
      "epoch": 0.8977777777777778,
      "step": 4141,
      "training_loss": 6.065925598144531
    },
    {
      "epoch": 0.8977777777777778,
      "step": 4141,
      "training_loss": 5.125283241271973
    },
    {
      "epoch": 0.8977777777777778,
      "step": 4141,
      "training_loss": 6.459512710571289
    },
    {
      "epoch": 0.8979945799457995,
      "step": 4142,
      "training_loss": 6.137064456939697
    },
    {
      "epoch": 0.8979945799457995,
      "step": 4142,
      "training_loss": 7.965592861175537
    },
    {
      "epoch": 0.8979945799457995,
      "step": 4142,
      "training_loss": 4.748149394989014
    },
    {
      "epoch": 0.8979945799457995,
      "step": 4142,
      "training_loss": 7.453985214233398
    },
    {
      "epoch": 0.8982113821138211,
      "step": 4143,
      "training_loss": 3.175679922103882
    },
    {
      "epoch": 0.8982113821138211,
      "step": 4143,
      "training_loss": 5.816170692443848
    },
    {
      "epoch": 0.8982113821138211,
      "step": 4143,
      "training_loss": 3.558467149734497
    },
    {
      "epoch": 0.8982113821138211,
      "step": 4143,
      "training_loss": 5.480656147003174
    },
    {
      "epoch": 0.8984281842818428,
      "grad_norm": 28.191801071166992,
      "learning_rate": 1e-05,
      "loss": 5.9912,
      "step": 4144
    },
    {
      "epoch": 0.8984281842818428,
      "step": 4144,
      "training_loss": 6.1362080574035645
    },
    {
      "epoch": 0.8984281842818428,
      "step": 4144,
      "training_loss": 7.822932720184326
    },
    {
      "epoch": 0.8984281842818428,
      "step": 4144,
      "training_loss": 3.6982979774475098
    },
    {
      "epoch": 0.8984281842818428,
      "step": 4144,
      "training_loss": 5.136359214782715
    },
    {
      "epoch": 0.8986449864498645,
      "step": 4145,
      "training_loss": 6.507760524749756
    },
    {
      "epoch": 0.8986449864498645,
      "step": 4145,
      "training_loss": 7.324085712432861
    },
    {
      "epoch": 0.8986449864498645,
      "step": 4145,
      "training_loss": 7.020320892333984
    },
    {
      "epoch": 0.8986449864498645,
      "step": 4145,
      "training_loss": 7.133553981781006
    },
    {
      "epoch": 0.8988617886178861,
      "step": 4146,
      "training_loss": 6.272705078125
    },
    {
      "epoch": 0.8988617886178861,
      "step": 4146,
      "training_loss": 6.990018367767334
    },
    {
      "epoch": 0.8988617886178861,
      "step": 4146,
      "training_loss": 7.141880035400391
    },
    {
      "epoch": 0.8988617886178861,
      "step": 4146,
      "training_loss": 6.042314052581787
    },
    {
      "epoch": 0.8990785907859079,
      "step": 4147,
      "training_loss": 7.824538707733154
    },
    {
      "epoch": 0.8990785907859079,
      "step": 4147,
      "training_loss": 8.023624420166016
    },
    {
      "epoch": 0.8990785907859079,
      "step": 4147,
      "training_loss": 6.84771203994751
    },
    {
      "epoch": 0.8990785907859079,
      "step": 4147,
      "training_loss": 5.360313892364502
    },
    {
      "epoch": 0.8992953929539296,
      "grad_norm": 19.027376174926758,
      "learning_rate": 1e-05,
      "loss": 6.5802,
      "step": 4148
    },
    {
      "epoch": 0.8992953929539296,
      "step": 4148,
      "training_loss": 6.914138317108154
    },
    {
      "epoch": 0.8992953929539296,
      "step": 4148,
      "training_loss": 5.786760330200195
    },
    {
      "epoch": 0.8992953929539296,
      "step": 4148,
      "training_loss": 7.992352485656738
    },
    {
      "epoch": 0.8992953929539296,
      "step": 4148,
      "training_loss": 5.7342915534973145
    },
    {
      "epoch": 0.8995121951219512,
      "step": 4149,
      "training_loss": 6.155433177947998
    },
    {
      "epoch": 0.8995121951219512,
      "step": 4149,
      "training_loss": 6.522851467132568
    },
    {
      "epoch": 0.8995121951219512,
      "step": 4149,
      "training_loss": 6.2256646156311035
    },
    {
      "epoch": 0.8995121951219512,
      "step": 4149,
      "training_loss": 6.45591402053833
    },
    {
      "epoch": 0.8997289972899729,
      "step": 4150,
      "training_loss": 6.493361473083496
    },
    {
      "epoch": 0.8997289972899729,
      "step": 4150,
      "training_loss": 5.757349491119385
    },
    {
      "epoch": 0.8997289972899729,
      "step": 4150,
      "training_loss": 7.632641792297363
    },
    {
      "epoch": 0.8997289972899729,
      "step": 4150,
      "training_loss": 3.5055336952209473
    },
    {
      "epoch": 0.8999457994579946,
      "step": 4151,
      "training_loss": 6.758732795715332
    },
    {
      "epoch": 0.8999457994579946,
      "step": 4151,
      "training_loss": 6.2354960441589355
    },
    {
      "epoch": 0.8999457994579946,
      "step": 4151,
      "training_loss": 6.398495197296143
    },
    {
      "epoch": 0.8999457994579946,
      "step": 4151,
      "training_loss": 6.3531975746154785
    },
    {
      "epoch": 0.9001626016260162,
      "grad_norm": 22.466175079345703,
      "learning_rate": 1e-05,
      "loss": 6.3076,
      "step": 4152
    },
    {
      "epoch": 0.9001626016260162,
      "step": 4152,
      "training_loss": 6.165982246398926
    },
    {
      "epoch": 0.9001626016260162,
      "step": 4152,
      "training_loss": 7.572190284729004
    },
    {
      "epoch": 0.9001626016260162,
      "step": 4152,
      "training_loss": 6.872735977172852
    },
    {
      "epoch": 0.9001626016260162,
      "step": 4152,
      "training_loss": 6.552931785583496
    },
    {
      "epoch": 0.9003794037940379,
      "step": 4153,
      "training_loss": 7.756325721740723
    },
    {
      "epoch": 0.9003794037940379,
      "step": 4153,
      "training_loss": 6.434319019317627
    },
    {
      "epoch": 0.9003794037940379,
      "step": 4153,
      "training_loss": 3.8520758152008057
    },
    {
      "epoch": 0.9003794037940379,
      "step": 4153,
      "training_loss": 6.823935031890869
    },
    {
      "epoch": 0.9005962059620596,
      "step": 4154,
      "training_loss": 4.321955680847168
    },
    {
      "epoch": 0.9005962059620596,
      "step": 4154,
      "training_loss": 5.957123756408691
    },
    {
      "epoch": 0.9005962059620596,
      "step": 4154,
      "training_loss": 7.591119289398193
    },
    {
      "epoch": 0.9005962059620596,
      "step": 4154,
      "training_loss": 6.051060676574707
    },
    {
      "epoch": 0.9008130081300812,
      "step": 4155,
      "training_loss": 5.629340171813965
    },
    {
      "epoch": 0.9008130081300812,
      "step": 4155,
      "training_loss": 5.815448760986328
    },
    {
      "epoch": 0.9008130081300812,
      "step": 4155,
      "training_loss": 6.57008171081543
    },
    {
      "epoch": 0.9008130081300812,
      "step": 4155,
      "training_loss": 2.6431872844696045
    },
    {
      "epoch": 0.901029810298103,
      "grad_norm": 18.557558059692383,
      "learning_rate": 1e-05,
      "loss": 6.0381,
      "step": 4156
    },
    {
      "epoch": 0.901029810298103,
      "step": 4156,
      "training_loss": 4.7346320152282715
    },
    {
      "epoch": 0.901029810298103,
      "step": 4156,
      "training_loss": 6.469791412353516
    },
    {
      "epoch": 0.901029810298103,
      "step": 4156,
      "training_loss": 7.895072937011719
    },
    {
      "epoch": 0.901029810298103,
      "step": 4156,
      "training_loss": 7.608758926391602
    },
    {
      "epoch": 0.9012466124661247,
      "step": 4157,
      "training_loss": 7.701303482055664
    },
    {
      "epoch": 0.9012466124661247,
      "step": 4157,
      "training_loss": 7.588640213012695
    },
    {
      "epoch": 0.9012466124661247,
      "step": 4157,
      "training_loss": 7.142902851104736
    },
    {
      "epoch": 0.9012466124661247,
      "step": 4157,
      "training_loss": 7.452606678009033
    },
    {
      "epoch": 0.9014634146341464,
      "step": 4158,
      "training_loss": 6.716911792755127
    },
    {
      "epoch": 0.9014634146341464,
      "step": 4158,
      "training_loss": 6.606344699859619
    },
    {
      "epoch": 0.9014634146341464,
      "step": 4158,
      "training_loss": 6.204594612121582
    },
    {
      "epoch": 0.9014634146341464,
      "step": 4158,
      "training_loss": 7.19356632232666
    },
    {
      "epoch": 0.901680216802168,
      "step": 4159,
      "training_loss": 7.208421230316162
    },
    {
      "epoch": 0.901680216802168,
      "step": 4159,
      "training_loss": 6.73848819732666
    },
    {
      "epoch": 0.901680216802168,
      "step": 4159,
      "training_loss": 7.74061918258667
    },
    {
      "epoch": 0.901680216802168,
      "step": 4159,
      "training_loss": 7.1020917892456055
    },
    {
      "epoch": 0.9018970189701897,
      "grad_norm": 20.974634170532227,
      "learning_rate": 1e-05,
      "loss": 7.0065,
      "step": 4160
    },
    {
      "epoch": 0.9018970189701897,
      "step": 4160,
      "training_loss": 5.086096286773682
    },
    {
      "epoch": 0.9018970189701897,
      "step": 4160,
      "training_loss": 7.190431118011475
    },
    {
      "epoch": 0.9018970189701897,
      "step": 4160,
      "training_loss": 7.4949870109558105
    },
    {
      "epoch": 0.9018970189701897,
      "step": 4160,
      "training_loss": 5.07225227355957
    },
    {
      "epoch": 0.9021138211382114,
      "step": 4161,
      "training_loss": 7.3299241065979
    },
    {
      "epoch": 0.9021138211382114,
      "step": 4161,
      "training_loss": 6.140148162841797
    },
    {
      "epoch": 0.9021138211382114,
      "step": 4161,
      "training_loss": 6.383931636810303
    },
    {
      "epoch": 0.9021138211382114,
      "step": 4161,
      "training_loss": 6.513930797576904
    },
    {
      "epoch": 0.902330623306233,
      "step": 4162,
      "training_loss": 6.942779541015625
    },
    {
      "epoch": 0.902330623306233,
      "step": 4162,
      "training_loss": 7.131479263305664
    },
    {
      "epoch": 0.902330623306233,
      "step": 4162,
      "training_loss": 6.697995662689209
    },
    {
      "epoch": 0.902330623306233,
      "step": 4162,
      "training_loss": 6.610294342041016
    },
    {
      "epoch": 0.9025474254742547,
      "step": 4163,
      "training_loss": 6.569187641143799
    },
    {
      "epoch": 0.9025474254742547,
      "step": 4163,
      "training_loss": 6.989846229553223
    },
    {
      "epoch": 0.9025474254742547,
      "step": 4163,
      "training_loss": 6.1002912521362305
    },
    {
      "epoch": 0.9025474254742547,
      "step": 4163,
      "training_loss": 5.415346145629883
    },
    {
      "epoch": 0.9027642276422764,
      "grad_norm": 21.034299850463867,
      "learning_rate": 1e-05,
      "loss": 6.4793,
      "step": 4164
    },
    {
      "epoch": 0.9027642276422764,
      "step": 4164,
      "training_loss": 6.815792560577393
    },
    {
      "epoch": 0.9027642276422764,
      "step": 4164,
      "training_loss": 5.255512237548828
    },
    {
      "epoch": 0.9027642276422764,
      "step": 4164,
      "training_loss": 6.954577445983887
    },
    {
      "epoch": 0.9027642276422764,
      "step": 4164,
      "training_loss": 5.311647891998291
    },
    {
      "epoch": 0.9029810298102982,
      "step": 4165,
      "training_loss": 5.328762054443359
    },
    {
      "epoch": 0.9029810298102982,
      "step": 4165,
      "training_loss": 6.5266571044921875
    },
    {
      "epoch": 0.9029810298102982,
      "step": 4165,
      "training_loss": 6.8340864181518555
    },
    {
      "epoch": 0.9029810298102982,
      "step": 4165,
      "training_loss": 6.98031759262085
    },
    {
      "epoch": 0.9031978319783198,
      "step": 4166,
      "training_loss": 3.0032083988189697
    },
    {
      "epoch": 0.9031978319783198,
      "step": 4166,
      "training_loss": 4.966434001922607
    },
    {
      "epoch": 0.9031978319783198,
      "step": 4166,
      "training_loss": 5.384085178375244
    },
    {
      "epoch": 0.9031978319783198,
      "step": 4166,
      "training_loss": 5.6831207275390625
    },
    {
      "epoch": 0.9034146341463415,
      "step": 4167,
      "training_loss": 5.348384380340576
    },
    {
      "epoch": 0.9034146341463415,
      "step": 4167,
      "training_loss": 6.82731819152832
    },
    {
      "epoch": 0.9034146341463415,
      "step": 4167,
      "training_loss": 6.145297050476074
    },
    {
      "epoch": 0.9034146341463415,
      "step": 4167,
      "training_loss": 6.298964500427246
    },
    {
      "epoch": 0.9036314363143632,
      "grad_norm": 22.47016716003418,
      "learning_rate": 1e-05,
      "loss": 5.854,
      "step": 4168
    },
    {
      "epoch": 0.9036314363143632,
      "step": 4168,
      "training_loss": 5.352078914642334
    },
    {
      "epoch": 0.9036314363143632,
      "step": 4168,
      "training_loss": 6.825521469116211
    },
    {
      "epoch": 0.9036314363143632,
      "step": 4168,
      "training_loss": 7.132114887237549
    },
    {
      "epoch": 0.9036314363143632,
      "step": 4168,
      "training_loss": 6.99515438079834
    },
    {
      "epoch": 0.9038482384823848,
      "step": 4169,
      "training_loss": 6.171607971191406
    },
    {
      "epoch": 0.9038482384823848,
      "step": 4169,
      "training_loss": 6.44818639755249
    },
    {
      "epoch": 0.9038482384823848,
      "step": 4169,
      "training_loss": 2.8064236640930176
    },
    {
      "epoch": 0.9038482384823848,
      "step": 4169,
      "training_loss": 6.734500885009766
    },
    {
      "epoch": 0.9040650406504065,
      "step": 4170,
      "training_loss": 6.506152153015137
    },
    {
      "epoch": 0.9040650406504065,
      "step": 4170,
      "training_loss": 6.244747638702393
    },
    {
      "epoch": 0.9040650406504065,
      "step": 4170,
      "training_loss": 6.089966773986816
    },
    {
      "epoch": 0.9040650406504065,
      "step": 4170,
      "training_loss": 6.494819164276123
    },
    {
      "epoch": 0.9042818428184282,
      "step": 4171,
      "training_loss": 2.540945053100586
    },
    {
      "epoch": 0.9042818428184282,
      "step": 4171,
      "training_loss": 7.092143535614014
    },
    {
      "epoch": 0.9042818428184282,
      "step": 4171,
      "training_loss": 5.452990531921387
    },
    {
      "epoch": 0.9042818428184282,
      "step": 4171,
      "training_loss": 6.549811840057373
    },
    {
      "epoch": 0.9044986449864498,
      "grad_norm": 16.23265838623047,
      "learning_rate": 1e-05,
      "loss": 5.9648,
      "step": 4172
    },
    {
      "epoch": 0.9044986449864498,
      "step": 4172,
      "training_loss": 6.823620796203613
    },
    {
      "epoch": 0.9044986449864498,
      "step": 4172,
      "training_loss": 6.075528621673584
    },
    {
      "epoch": 0.9044986449864498,
      "step": 4172,
      "training_loss": 6.254579544067383
    },
    {
      "epoch": 0.9044986449864498,
      "step": 4172,
      "training_loss": 6.4365153312683105
    },
    {
      "epoch": 0.9047154471544715,
      "step": 4173,
      "training_loss": 5.731762886047363
    },
    {
      "epoch": 0.9047154471544715,
      "step": 4173,
      "training_loss": 6.512601375579834
    },
    {
      "epoch": 0.9047154471544715,
      "step": 4173,
      "training_loss": 5.856424808502197
    },
    {
      "epoch": 0.9047154471544715,
      "step": 4173,
      "training_loss": 5.532703876495361
    },
    {
      "epoch": 0.9049322493224933,
      "step": 4174,
      "training_loss": 6.197700023651123
    },
    {
      "epoch": 0.9049322493224933,
      "step": 4174,
      "training_loss": 7.4612507820129395
    },
    {
      "epoch": 0.9049322493224933,
      "step": 4174,
      "training_loss": 5.623688697814941
    },
    {
      "epoch": 0.9049322493224933,
      "step": 4174,
      "training_loss": 6.463042259216309
    },
    {
      "epoch": 0.9051490514905149,
      "step": 4175,
      "training_loss": 6.449766159057617
    },
    {
      "epoch": 0.9051490514905149,
      "step": 4175,
      "training_loss": 6.888649940490723
    },
    {
      "epoch": 0.9051490514905149,
      "step": 4175,
      "training_loss": 6.1864495277404785
    },
    {
      "epoch": 0.9051490514905149,
      "step": 4175,
      "training_loss": 6.528079509735107
    },
    {
      "epoch": 0.9053658536585366,
      "grad_norm": 17.86772918701172,
      "learning_rate": 1e-05,
      "loss": 6.3139,
      "step": 4176
    },
    {
      "epoch": 0.9053658536585366,
      "step": 4176,
      "training_loss": 5.7601728439331055
    },
    {
      "epoch": 0.9053658536585366,
      "step": 4176,
      "training_loss": 5.547916889190674
    },
    {
      "epoch": 0.9053658536585366,
      "step": 4176,
      "training_loss": 8.203971862792969
    },
    {
      "epoch": 0.9053658536585366,
      "step": 4176,
      "training_loss": 7.540524005889893
    },
    {
      "epoch": 0.9055826558265583,
      "step": 4177,
      "training_loss": 6.82446813583374
    },
    {
      "epoch": 0.9055826558265583,
      "step": 4177,
      "training_loss": 6.281659126281738
    },
    {
      "epoch": 0.9055826558265583,
      "step": 4177,
      "training_loss": 7.628178596496582
    },
    {
      "epoch": 0.9055826558265583,
      "step": 4177,
      "training_loss": 6.519059181213379
    },
    {
      "epoch": 0.90579945799458,
      "step": 4178,
      "training_loss": 9.405903816223145
    },
    {
      "epoch": 0.90579945799458,
      "step": 4178,
      "training_loss": 6.685840129852295
    },
    {
      "epoch": 0.90579945799458,
      "step": 4178,
      "training_loss": 6.403295516967773
    },
    {
      "epoch": 0.90579945799458,
      "step": 4178,
      "training_loss": 6.5695953369140625
    },
    {
      "epoch": 0.9060162601626016,
      "step": 4179,
      "training_loss": 5.259677410125732
    },
    {
      "epoch": 0.9060162601626016,
      "step": 4179,
      "training_loss": 7.526355266571045
    },
    {
      "epoch": 0.9060162601626016,
      "step": 4179,
      "training_loss": 5.4876532554626465
    },
    {
      "epoch": 0.9060162601626016,
      "step": 4179,
      "training_loss": 7.032954692840576
    },
    {
      "epoch": 0.9062330623306233,
      "grad_norm": 18.382728576660156,
      "learning_rate": 1e-05,
      "loss": 6.7923,
      "step": 4180
    },
    {
      "epoch": 0.9062330623306233,
      "step": 4180,
      "training_loss": 7.778292179107666
    },
    {
      "epoch": 0.9062330623306233,
      "step": 4180,
      "training_loss": 5.832574844360352
    },
    {
      "epoch": 0.9062330623306233,
      "step": 4180,
      "training_loss": 5.1310038566589355
    },
    {
      "epoch": 0.9062330623306233,
      "step": 4180,
      "training_loss": 6.702106475830078
    },
    {
      "epoch": 0.906449864498645,
      "step": 4181,
      "training_loss": 4.5264387130737305
    },
    {
      "epoch": 0.906449864498645,
      "step": 4181,
      "training_loss": 6.888627052307129
    },
    {
      "epoch": 0.906449864498645,
      "step": 4181,
      "training_loss": 7.814698219299316
    },
    {
      "epoch": 0.906449864498645,
      "step": 4181,
      "training_loss": 5.627066135406494
    },
    {
      "epoch": 0.9066666666666666,
      "step": 4182,
      "training_loss": 6.97022008895874
    },
    {
      "epoch": 0.9066666666666666,
      "step": 4182,
      "training_loss": 6.494292736053467
    },
    {
      "epoch": 0.9066666666666666,
      "step": 4182,
      "training_loss": 6.858152389526367
    },
    {
      "epoch": 0.9066666666666666,
      "step": 4182,
      "training_loss": 5.065943717956543
    },
    {
      "epoch": 0.9068834688346884,
      "step": 4183,
      "training_loss": 5.23008918762207
    },
    {
      "epoch": 0.9068834688346884,
      "step": 4183,
      "training_loss": 6.291187763214111
    },
    {
      "epoch": 0.9068834688346884,
      "step": 4183,
      "training_loss": 5.542201042175293
    },
    {
      "epoch": 0.9068834688346884,
      "step": 4183,
      "training_loss": 5.926405906677246
    },
    {
      "epoch": 0.9071002710027101,
      "grad_norm": 25.258398056030273,
      "learning_rate": 1e-05,
      "loss": 6.1675,
      "step": 4184
    },
    {
      "epoch": 0.9071002710027101,
      "step": 4184,
      "training_loss": 6.435568332672119
    },
    {
      "epoch": 0.9071002710027101,
      "step": 4184,
      "training_loss": 5.728794574737549
    },
    {
      "epoch": 0.9071002710027101,
      "step": 4184,
      "training_loss": 7.872569561004639
    },
    {
      "epoch": 0.9071002710027101,
      "step": 4184,
      "training_loss": 3.793861150741577
    },
    {
      "epoch": 0.9073170731707317,
      "step": 4185,
      "training_loss": 6.306336402893066
    },
    {
      "epoch": 0.9073170731707317,
      "step": 4185,
      "training_loss": 6.650181293487549
    },
    {
      "epoch": 0.9073170731707317,
      "step": 4185,
      "training_loss": 6.496686935424805
    },
    {
      "epoch": 0.9073170731707317,
      "step": 4185,
      "training_loss": 3.6436073780059814
    },
    {
      "epoch": 0.9075338753387534,
      "step": 4186,
      "training_loss": 6.588118076324463
    },
    {
      "epoch": 0.9075338753387534,
      "step": 4186,
      "training_loss": 8.119053840637207
    },
    {
      "epoch": 0.9075338753387534,
      "step": 4186,
      "training_loss": 4.878871917724609
    },
    {
      "epoch": 0.9075338753387534,
      "step": 4186,
      "training_loss": 6.169556140899658
    },
    {
      "epoch": 0.9077506775067751,
      "step": 4187,
      "training_loss": 6.427785396575928
    },
    {
      "epoch": 0.9077506775067751,
      "step": 4187,
      "training_loss": 5.411567211151123
    },
    {
      "epoch": 0.9077506775067751,
      "step": 4187,
      "training_loss": 6.886251926422119
    },
    {
      "epoch": 0.9077506775067751,
      "step": 4187,
      "training_loss": 6.579495906829834
    },
    {
      "epoch": 0.9079674796747967,
      "grad_norm": 20.00391387939453,
      "learning_rate": 1e-05,
      "loss": 6.1243,
      "step": 4188
    },
    {
      "epoch": 0.9079674796747967,
      "step": 4188,
      "training_loss": 8.207225799560547
    },
    {
      "epoch": 0.9079674796747967,
      "step": 4188,
      "training_loss": 4.405703544616699
    },
    {
      "epoch": 0.9079674796747967,
      "step": 4188,
      "training_loss": 7.3909912109375
    },
    {
      "epoch": 0.9079674796747967,
      "step": 4188,
      "training_loss": 7.2763285636901855
    },
    {
      "epoch": 0.9081842818428184,
      "step": 4189,
      "training_loss": 5.899730205535889
    },
    {
      "epoch": 0.9081842818428184,
      "step": 4189,
      "training_loss": 7.489043712615967
    },
    {
      "epoch": 0.9081842818428184,
      "step": 4189,
      "training_loss": 6.955376625061035
    },
    {
      "epoch": 0.9081842818428184,
      "step": 4189,
      "training_loss": 6.698566913604736
    },
    {
      "epoch": 0.9084010840108401,
      "step": 4190,
      "training_loss": 5.212541580200195
    },
    {
      "epoch": 0.9084010840108401,
      "step": 4190,
      "training_loss": 6.306906700134277
    },
    {
      "epoch": 0.9084010840108401,
      "step": 4190,
      "training_loss": 7.342779636383057
    },
    {
      "epoch": 0.9084010840108401,
      "step": 4190,
      "training_loss": 3.7098476886749268
    },
    {
      "epoch": 0.9086178861788617,
      "step": 4191,
      "training_loss": 5.2265825271606445
    },
    {
      "epoch": 0.9086178861788617,
      "step": 4191,
      "training_loss": 6.209346294403076
    },
    {
      "epoch": 0.9086178861788617,
      "step": 4191,
      "training_loss": 6.553691387176514
    },
    {
      "epoch": 0.9086178861788617,
      "step": 4191,
      "training_loss": 4.411559581756592
    },
    {
      "epoch": 0.9088346883468835,
      "grad_norm": 31.65569496154785,
      "learning_rate": 1e-05,
      "loss": 6.206,
      "step": 4192
    },
    {
      "epoch": 0.9088346883468835,
      "step": 4192,
      "training_loss": 6.465846061706543
    },
    {
      "epoch": 0.9088346883468835,
      "step": 4192,
      "training_loss": 7.24370813369751
    },
    {
      "epoch": 0.9088346883468835,
      "step": 4192,
      "training_loss": 5.6922173500061035
    },
    {
      "epoch": 0.9088346883468835,
      "step": 4192,
      "training_loss": 6.554627895355225
    },
    {
      "epoch": 0.9090514905149052,
      "step": 4193,
      "training_loss": 4.4694318771362305
    },
    {
      "epoch": 0.9090514905149052,
      "step": 4193,
      "training_loss": 7.269322872161865
    },
    {
      "epoch": 0.9090514905149052,
      "step": 4193,
      "training_loss": 6.073585510253906
    },
    {
      "epoch": 0.9090514905149052,
      "step": 4193,
      "training_loss": 4.99178409576416
    },
    {
      "epoch": 0.9092682926829269,
      "step": 4194,
      "training_loss": 4.972430229187012
    },
    {
      "epoch": 0.9092682926829269,
      "step": 4194,
      "training_loss": 3.289815902709961
    },
    {
      "epoch": 0.9092682926829269,
      "step": 4194,
      "training_loss": 3.0826096534729004
    },
    {
      "epoch": 0.9092682926829269,
      "step": 4194,
      "training_loss": 6.385196208953857
    },
    {
      "epoch": 0.9094850948509485,
      "step": 4195,
      "training_loss": 5.292393684387207
    },
    {
      "epoch": 0.9094850948509485,
      "step": 4195,
      "training_loss": 6.797579765319824
    },
    {
      "epoch": 0.9094850948509485,
      "step": 4195,
      "training_loss": 9.16910457611084
    },
    {
      "epoch": 0.9094850948509485,
      "step": 4195,
      "training_loss": 7.751660346984863
    },
    {
      "epoch": 0.9097018970189702,
      "grad_norm": 26.313377380371094,
      "learning_rate": 1e-05,
      "loss": 5.9688,
      "step": 4196
    },
    {
      "epoch": 0.9097018970189702,
      "step": 4196,
      "training_loss": 8.082289695739746
    },
    {
      "epoch": 0.9097018970189702,
      "step": 4196,
      "training_loss": 6.177328109741211
    },
    {
      "epoch": 0.9097018970189702,
      "step": 4196,
      "training_loss": 6.267116546630859
    },
    {
      "epoch": 0.9097018970189702,
      "step": 4196,
      "training_loss": 7.309155464172363
    },
    {
      "epoch": 0.9099186991869919,
      "step": 4197,
      "training_loss": 7.624405860900879
    },
    {
      "epoch": 0.9099186991869919,
      "step": 4197,
      "training_loss": 7.142396450042725
    },
    {
      "epoch": 0.9099186991869919,
      "step": 4197,
      "training_loss": 7.48460578918457
    },
    {
      "epoch": 0.9099186991869919,
      "step": 4197,
      "training_loss": 5.5649094581604
    },
    {
      "epoch": 0.9101355013550135,
      "step": 4198,
      "training_loss": 6.719691753387451
    },
    {
      "epoch": 0.9101355013550135,
      "step": 4198,
      "training_loss": 5.758567810058594
    },
    {
      "epoch": 0.9101355013550135,
      "step": 4198,
      "training_loss": 7.611563205718994
    },
    {
      "epoch": 0.9101355013550135,
      "step": 4198,
      "training_loss": 6.723086833953857
    },
    {
      "epoch": 0.9103523035230352,
      "step": 4199,
      "training_loss": 4.820376396179199
    },
    {
      "epoch": 0.9103523035230352,
      "step": 4199,
      "training_loss": 5.82023286819458
    },
    {
      "epoch": 0.9103523035230352,
      "step": 4199,
      "training_loss": 5.914656639099121
    },
    {
      "epoch": 0.9103523035230352,
      "step": 4199,
      "training_loss": 6.28221321105957
    },
    {
      "epoch": 0.9105691056910569,
      "grad_norm": 26.135345458984375,
      "learning_rate": 1e-05,
      "loss": 6.5814,
      "step": 4200
    },
    {
      "epoch": 0.9105691056910569,
      "step": 4200,
      "training_loss": 3.0181491374969482
    },
    {
      "epoch": 0.9105691056910569,
      "step": 4200,
      "training_loss": 7.893982887268066
    },
    {
      "epoch": 0.9105691056910569,
      "step": 4200,
      "training_loss": 2.9350318908691406
    },
    {
      "epoch": 0.9105691056910569,
      "step": 4200,
      "training_loss": 5.415503025054932
    },
    {
      "epoch": 0.9107859078590785,
      "step": 4201,
      "training_loss": 7.475582122802734
    },
    {
      "epoch": 0.9107859078590785,
      "step": 4201,
      "training_loss": 3.1115689277648926
    },
    {
      "epoch": 0.9107859078590785,
      "step": 4201,
      "training_loss": 4.720435619354248
    },
    {
      "epoch": 0.9107859078590785,
      "step": 4201,
      "training_loss": 5.703818321228027
    },
    {
      "epoch": 0.9110027100271003,
      "step": 4202,
      "training_loss": 6.842443466186523
    },
    {
      "epoch": 0.9110027100271003,
      "step": 4202,
      "training_loss": 6.409778594970703
    },
    {
      "epoch": 0.9110027100271003,
      "step": 4202,
      "training_loss": 5.531398296356201
    },
    {
      "epoch": 0.9110027100271003,
      "step": 4202,
      "training_loss": 5.227174282073975
    },
    {
      "epoch": 0.911219512195122,
      "step": 4203,
      "training_loss": 7.095396995544434
    },
    {
      "epoch": 0.911219512195122,
      "step": 4203,
      "training_loss": 4.987219333648682
    },
    {
      "epoch": 0.911219512195122,
      "step": 4203,
      "training_loss": 6.995319366455078
    },
    {
      "epoch": 0.911219512195122,
      "step": 4203,
      "training_loss": 4.976965427398682
    },
    {
      "epoch": 0.9114363143631437,
      "grad_norm": 19.3689022064209,
      "learning_rate": 1e-05,
      "loss": 5.5212,
      "step": 4204
    },
    {
      "epoch": 0.9114363143631437,
      "step": 4204,
      "training_loss": 7.361857891082764
    },
    {
      "epoch": 0.9114363143631437,
      "step": 4204,
      "training_loss": 5.625353813171387
    },
    {
      "epoch": 0.9114363143631437,
      "step": 4204,
      "training_loss": 5.673218250274658
    },
    {
      "epoch": 0.9114363143631437,
      "step": 4204,
      "training_loss": 7.017758846282959
    },
    {
      "epoch": 0.9116531165311653,
      "step": 4205,
      "training_loss": 7.015762805938721
    },
    {
      "epoch": 0.9116531165311653,
      "step": 4205,
      "training_loss": 6.851108551025391
    },
    {
      "epoch": 0.9116531165311653,
      "step": 4205,
      "training_loss": 7.227328300476074
    },
    {
      "epoch": 0.9116531165311653,
      "step": 4205,
      "training_loss": 6.6425347328186035
    },
    {
      "epoch": 0.911869918699187,
      "step": 4206,
      "training_loss": 6.0940375328063965
    },
    {
      "epoch": 0.911869918699187,
      "step": 4206,
      "training_loss": 5.375031471252441
    },
    {
      "epoch": 0.911869918699187,
      "step": 4206,
      "training_loss": 3.363910436630249
    },
    {
      "epoch": 0.911869918699187,
      "step": 4206,
      "training_loss": 6.407862663269043
    },
    {
      "epoch": 0.9120867208672087,
      "step": 4207,
      "training_loss": 7.2142791748046875
    },
    {
      "epoch": 0.9120867208672087,
      "step": 4207,
      "training_loss": 6.210385322570801
    },
    {
      "epoch": 0.9120867208672087,
      "step": 4207,
      "training_loss": 4.145595550537109
    },
    {
      "epoch": 0.9120867208672087,
      "step": 4207,
      "training_loss": 7.880329608917236
    },
    {
      "epoch": 0.9123035230352303,
      "grad_norm": 14.630725860595703,
      "learning_rate": 1e-05,
      "loss": 6.2566,
      "step": 4208
    },
    {
      "epoch": 0.9123035230352303,
      "step": 4208,
      "training_loss": 7.287037372589111
    },
    {
      "epoch": 0.9123035230352303,
      "step": 4208,
      "training_loss": 6.7101006507873535
    },
    {
      "epoch": 0.9123035230352303,
      "step": 4208,
      "training_loss": 6.138545036315918
    },
    {
      "epoch": 0.9123035230352303,
      "step": 4208,
      "training_loss": 5.501446723937988
    },
    {
      "epoch": 0.912520325203252,
      "step": 4209,
      "training_loss": 6.30580472946167
    },
    {
      "epoch": 0.912520325203252,
      "step": 4209,
      "training_loss": 7.081647872924805
    },
    {
      "epoch": 0.912520325203252,
      "step": 4209,
      "training_loss": 6.26294469833374
    },
    {
      "epoch": 0.912520325203252,
      "step": 4209,
      "training_loss": 6.904209136962891
    },
    {
      "epoch": 0.9127371273712737,
      "step": 4210,
      "training_loss": 3.341693162918091
    },
    {
      "epoch": 0.9127371273712737,
      "step": 4210,
      "training_loss": 7.550126552581787
    },
    {
      "epoch": 0.9127371273712737,
      "step": 4210,
      "training_loss": 6.807930946350098
    },
    {
      "epoch": 0.9127371273712737,
      "step": 4210,
      "training_loss": 8.094185829162598
    },
    {
      "epoch": 0.9129539295392954,
      "step": 4211,
      "training_loss": 5.699087619781494
    },
    {
      "epoch": 0.9129539295392954,
      "step": 4211,
      "training_loss": 6.0983076095581055
    },
    {
      "epoch": 0.9129539295392954,
      "step": 4211,
      "training_loss": 3.3326735496520996
    },
    {
      "epoch": 0.9129539295392954,
      "step": 4211,
      "training_loss": 6.021954536437988
    },
    {
      "epoch": 0.9131707317073171,
      "grad_norm": 18.003007888793945,
      "learning_rate": 1e-05,
      "loss": 6.1961,
      "step": 4212
    },
    {
      "epoch": 0.9131707317073171,
      "step": 4212,
      "training_loss": 7.213033199310303
    },
    {
      "epoch": 0.9131707317073171,
      "step": 4212,
      "training_loss": 7.207345485687256
    },
    {
      "epoch": 0.9131707317073171,
      "step": 4212,
      "training_loss": 3.2105979919433594
    },
    {
      "epoch": 0.9131707317073171,
      "step": 4212,
      "training_loss": 5.904820919036865
    },
    {
      "epoch": 0.9133875338753388,
      "step": 4213,
      "training_loss": 5.649535179138184
    },
    {
      "epoch": 0.9133875338753388,
      "step": 4213,
      "training_loss": 7.509123802185059
    },
    {
      "epoch": 0.9133875338753388,
      "step": 4213,
      "training_loss": 5.556088924407959
    },
    {
      "epoch": 0.9133875338753388,
      "step": 4213,
      "training_loss": 5.621947288513184
    },
    {
      "epoch": 0.9136043360433604,
      "step": 4214,
      "training_loss": 6.2997941970825195
    },
    {
      "epoch": 0.9136043360433604,
      "step": 4214,
      "training_loss": 6.350870609283447
    },
    {
      "epoch": 0.9136043360433604,
      "step": 4214,
      "training_loss": 5.931530952453613
    },
    {
      "epoch": 0.9136043360433604,
      "step": 4214,
      "training_loss": 5.917196273803711
    },
    {
      "epoch": 0.9138211382113821,
      "step": 4215,
      "training_loss": 6.689052581787109
    },
    {
      "epoch": 0.9138211382113821,
      "step": 4215,
      "training_loss": 6.486130237579346
    },
    {
      "epoch": 0.9138211382113821,
      "step": 4215,
      "training_loss": 6.974430561065674
    },
    {
      "epoch": 0.9138211382113821,
      "step": 4215,
      "training_loss": 7.359237194061279
    },
    {
      "epoch": 0.9140379403794038,
      "grad_norm": 16.036794662475586,
      "learning_rate": 1e-05,
      "loss": 6.2425,
      "step": 4216
    },
    {
      "epoch": 0.9140379403794038,
      "step": 4216,
      "training_loss": 7.375856876373291
    },
    {
      "epoch": 0.9140379403794038,
      "step": 4216,
      "training_loss": 6.813395023345947
    },
    {
      "epoch": 0.9140379403794038,
      "step": 4216,
      "training_loss": 6.347548961639404
    },
    {
      "epoch": 0.9140379403794038,
      "step": 4216,
      "training_loss": 5.497898101806641
    },
    {
      "epoch": 0.9142547425474254,
      "step": 4217,
      "training_loss": 4.373844146728516
    },
    {
      "epoch": 0.9142547425474254,
      "step": 4217,
      "training_loss": 4.616020679473877
    },
    {
      "epoch": 0.9142547425474254,
      "step": 4217,
      "training_loss": 4.000123023986816
    },
    {
      "epoch": 0.9142547425474254,
      "step": 4217,
      "training_loss": 5.106894016265869
    },
    {
      "epoch": 0.9144715447154471,
      "step": 4218,
      "training_loss": 5.488675594329834
    },
    {
      "epoch": 0.9144715447154471,
      "step": 4218,
      "training_loss": 6.549614906311035
    },
    {
      "epoch": 0.9144715447154471,
      "step": 4218,
      "training_loss": 6.642823696136475
    },
    {
      "epoch": 0.9144715447154471,
      "step": 4218,
      "training_loss": 6.410966396331787
    },
    {
      "epoch": 0.9146883468834688,
      "step": 4219,
      "training_loss": 5.6252336502075195
    },
    {
      "epoch": 0.9146883468834688,
      "step": 4219,
      "training_loss": 5.771233558654785
    },
    {
      "epoch": 0.9146883468834688,
      "step": 4219,
      "training_loss": 5.175117015838623
    },
    {
      "epoch": 0.9146883468834688,
      "step": 4219,
      "training_loss": 7.358616352081299
    },
    {
      "epoch": 0.9149051490514906,
      "grad_norm": 18.51751708984375,
      "learning_rate": 1e-05,
      "loss": 5.8221,
      "step": 4220
    },
    {
      "epoch": 0.9149051490514906,
      "step": 4220,
      "training_loss": 7.190380096435547
    },
    {
      "epoch": 0.9149051490514906,
      "step": 4220,
      "training_loss": 6.065113544464111
    },
    {
      "epoch": 0.9149051490514906,
      "step": 4220,
      "training_loss": 4.994210720062256
    },
    {
      "epoch": 0.9149051490514906,
      "step": 4220,
      "training_loss": 6.998748779296875
    },
    {
      "epoch": 0.9151219512195122,
      "step": 4221,
      "training_loss": 6.2338666915893555
    },
    {
      "epoch": 0.9151219512195122,
      "step": 4221,
      "training_loss": 6.572742462158203
    },
    {
      "epoch": 0.9151219512195122,
      "step": 4221,
      "training_loss": 3.9980738162994385
    },
    {
      "epoch": 0.9151219512195122,
      "step": 4221,
      "training_loss": 5.2466254234313965
    },
    {
      "epoch": 0.9153387533875339,
      "step": 4222,
      "training_loss": 5.141672611236572
    },
    {
      "epoch": 0.9153387533875339,
      "step": 4222,
      "training_loss": 6.762826919555664
    },
    {
      "epoch": 0.9153387533875339,
      "step": 4222,
      "training_loss": 5.166489124298096
    },
    {
      "epoch": 0.9153387533875339,
      "step": 4222,
      "training_loss": 7.325926780700684
    },
    {
      "epoch": 0.9155555555555556,
      "step": 4223,
      "training_loss": 4.874819278717041
    },
    {
      "epoch": 0.9155555555555556,
      "step": 4223,
      "training_loss": 7.540858745574951
    },
    {
      "epoch": 0.9155555555555556,
      "step": 4223,
      "training_loss": 4.1746931076049805
    },
    {
      "epoch": 0.9155555555555556,
      "step": 4223,
      "training_loss": 6.425546646118164
    },
    {
      "epoch": 0.9157723577235772,
      "grad_norm": 26.37010955810547,
      "learning_rate": 1e-05,
      "loss": 5.9195,
      "step": 4224
    },
    {
      "epoch": 0.9157723577235772,
      "step": 4224,
      "training_loss": 6.832205772399902
    },
    {
      "epoch": 0.9157723577235772,
      "step": 4224,
      "training_loss": 7.4016804695129395
    },
    {
      "epoch": 0.9157723577235772,
      "step": 4224,
      "training_loss": 6.639188289642334
    },
    {
      "epoch": 0.9157723577235772,
      "step": 4224,
      "training_loss": 6.368566989898682
    },
    {
      "epoch": 0.9159891598915989,
      "step": 4225,
      "training_loss": 6.397976398468018
    },
    {
      "epoch": 0.9159891598915989,
      "step": 4225,
      "training_loss": 6.517727375030518
    },
    {
      "epoch": 0.9159891598915989,
      "step": 4225,
      "training_loss": 6.644313812255859
    },
    {
      "epoch": 0.9159891598915989,
      "step": 4225,
      "training_loss": 6.923980236053467
    },
    {
      "epoch": 0.9162059620596206,
      "step": 4226,
      "training_loss": 7.09420108795166
    },
    {
      "epoch": 0.9162059620596206,
      "step": 4226,
      "training_loss": 6.435331344604492
    },
    {
      "epoch": 0.9162059620596206,
      "step": 4226,
      "training_loss": 5.885345935821533
    },
    {
      "epoch": 0.9162059620596206,
      "step": 4226,
      "training_loss": 7.201780319213867
    },
    {
      "epoch": 0.9164227642276422,
      "step": 4227,
      "training_loss": 6.968657970428467
    },
    {
      "epoch": 0.9164227642276422,
      "step": 4227,
      "training_loss": 5.570977210998535
    },
    {
      "epoch": 0.9164227642276422,
      "step": 4227,
      "training_loss": 6.922639846801758
    },
    {
      "epoch": 0.9164227642276422,
      "step": 4227,
      "training_loss": 5.100473403930664
    },
    {
      "epoch": 0.9166395663956639,
      "grad_norm": 22.595945358276367,
      "learning_rate": 1e-05,
      "loss": 6.5566,
      "step": 4228
    },
    {
      "epoch": 0.9166395663956639,
      "step": 4228,
      "training_loss": 6.807867527008057
    },
    {
      "epoch": 0.9166395663956639,
      "step": 4228,
      "training_loss": 6.087604522705078
    },
    {
      "epoch": 0.9166395663956639,
      "step": 4228,
      "training_loss": 5.1214070320129395
    },
    {
      "epoch": 0.9166395663956639,
      "step": 4228,
      "training_loss": 6.660595893859863
    },
    {
      "epoch": 0.9168563685636857,
      "step": 4229,
      "training_loss": 5.134776592254639
    },
    {
      "epoch": 0.9168563685636857,
      "step": 4229,
      "training_loss": 8.131329536437988
    },
    {
      "epoch": 0.9168563685636857,
      "step": 4229,
      "training_loss": 6.076666355133057
    },
    {
      "epoch": 0.9168563685636857,
      "step": 4229,
      "training_loss": 3.277892827987671
    },
    {
      "epoch": 0.9170731707317074,
      "step": 4230,
      "training_loss": 7.039047718048096
    },
    {
      "epoch": 0.9170731707317074,
      "step": 4230,
      "training_loss": 6.646380424499512
    },
    {
      "epoch": 0.9170731707317074,
      "step": 4230,
      "training_loss": 7.1699981689453125
    },
    {
      "epoch": 0.9170731707317074,
      "step": 4230,
      "training_loss": 5.070970058441162
    },
    {
      "epoch": 0.917289972899729,
      "step": 4231,
      "training_loss": 6.25152587890625
    },
    {
      "epoch": 0.917289972899729,
      "step": 4231,
      "training_loss": 6.56123685836792
    },
    {
      "epoch": 0.917289972899729,
      "step": 4231,
      "training_loss": 6.03799295425415
    },
    {
      "epoch": 0.917289972899729,
      "step": 4231,
      "training_loss": 3.188750743865967
    },
    {
      "epoch": 0.9175067750677507,
      "grad_norm": 15.737676620483398,
      "learning_rate": 1e-05,
      "loss": 5.954,
      "step": 4232
    },
    {
      "epoch": 0.9175067750677507,
      "step": 4232,
      "training_loss": 7.291149616241455
    },
    {
      "epoch": 0.9175067750677507,
      "step": 4232,
      "training_loss": 5.494751930236816
    },
    {
      "epoch": 0.9175067750677507,
      "step": 4232,
      "training_loss": 6.382761478424072
    },
    {
      "epoch": 0.9175067750677507,
      "step": 4232,
      "training_loss": 6.203562259674072
    },
    {
      "epoch": 0.9177235772357724,
      "step": 4233,
      "training_loss": 4.575413227081299
    },
    {
      "epoch": 0.9177235772357724,
      "step": 4233,
      "training_loss": 7.629977226257324
    },
    {
      "epoch": 0.9177235772357724,
      "step": 4233,
      "training_loss": 5.865190029144287
    },
    {
      "epoch": 0.9177235772357724,
      "step": 4233,
      "training_loss": 7.818665981292725
    },
    {
      "epoch": 0.917940379403794,
      "step": 4234,
      "training_loss": 7.103429317474365
    },
    {
      "epoch": 0.917940379403794,
      "step": 4234,
      "training_loss": 7.018626689910889
    },
    {
      "epoch": 0.917940379403794,
      "step": 4234,
      "training_loss": 6.9211626052856445
    },
    {
      "epoch": 0.917940379403794,
      "step": 4234,
      "training_loss": 6.9484429359436035
    },
    {
      "epoch": 0.9181571815718157,
      "step": 4235,
      "training_loss": 8.023492813110352
    },
    {
      "epoch": 0.9181571815718157,
      "step": 4235,
      "training_loss": 5.55282735824585
    },
    {
      "epoch": 0.9181571815718157,
      "step": 4235,
      "training_loss": 6.753257751464844
    },
    {
      "epoch": 0.9181571815718157,
      "step": 4235,
      "training_loss": 6.731624126434326
    },
    {
      "epoch": 0.9183739837398374,
      "grad_norm": 19.359582901000977,
      "learning_rate": 1e-05,
      "loss": 6.6446,
      "step": 4236
    },
    {
      "epoch": 0.9183739837398374,
      "step": 4236,
      "training_loss": 7.093710899353027
    },
    {
      "epoch": 0.9183739837398374,
      "step": 4236,
      "training_loss": 6.835880756378174
    },
    {
      "epoch": 0.9183739837398374,
      "step": 4236,
      "training_loss": 5.430014610290527
    },
    {
      "epoch": 0.9183739837398374,
      "step": 4236,
      "training_loss": 6.409091949462891
    },
    {
      "epoch": 0.918590785907859,
      "step": 4237,
      "training_loss": 7.168886661529541
    },
    {
      "epoch": 0.918590785907859,
      "step": 4237,
      "training_loss": 6.691159725189209
    },
    {
      "epoch": 0.918590785907859,
      "step": 4237,
      "training_loss": 6.859799861907959
    },
    {
      "epoch": 0.918590785907859,
      "step": 4237,
      "training_loss": 5.076009750366211
    },
    {
      "epoch": 0.9188075880758808,
      "step": 4238,
      "training_loss": 6.897211074829102
    },
    {
      "epoch": 0.9188075880758808,
      "step": 4238,
      "training_loss": 6.823557376861572
    },
    {
      "epoch": 0.9188075880758808,
      "step": 4238,
      "training_loss": 6.072258472442627
    },
    {
      "epoch": 0.9188075880758808,
      "step": 4238,
      "training_loss": 6.879655361175537
    },
    {
      "epoch": 0.9190243902439025,
      "step": 4239,
      "training_loss": 6.70875883102417
    },
    {
      "epoch": 0.9190243902439025,
      "step": 4239,
      "training_loss": 7.017638206481934
    },
    {
      "epoch": 0.9190243902439025,
      "step": 4239,
      "training_loss": 5.837491035461426
    },
    {
      "epoch": 0.9190243902439025,
      "step": 4239,
      "training_loss": 4.5248870849609375
    },
    {
      "epoch": 0.9192411924119241,
      "grad_norm": 17.598037719726562,
      "learning_rate": 1e-05,
      "loss": 6.3954,
      "step": 4240
    },
    {
      "epoch": 0.9192411924119241,
      "step": 4240,
      "training_loss": 7.273894786834717
    },
    {
      "epoch": 0.9192411924119241,
      "step": 4240,
      "training_loss": 7.534669876098633
    },
    {
      "epoch": 0.9192411924119241,
      "step": 4240,
      "training_loss": 5.105953693389893
    },
    {
      "epoch": 0.9192411924119241,
      "step": 4240,
      "training_loss": 5.366058349609375
    },
    {
      "epoch": 0.9194579945799458,
      "step": 4241,
      "training_loss": 4.670206546783447
    },
    {
      "epoch": 0.9194579945799458,
      "step": 4241,
      "training_loss": 3.7475457191467285
    },
    {
      "epoch": 0.9194579945799458,
      "step": 4241,
      "training_loss": 7.6204352378845215
    },
    {
      "epoch": 0.9194579945799458,
      "step": 4241,
      "training_loss": 6.341351509094238
    },
    {
      "epoch": 0.9196747967479675,
      "step": 4242,
      "training_loss": 6.680610179901123
    },
    {
      "epoch": 0.9196747967479675,
      "step": 4242,
      "training_loss": 8.143646240234375
    },
    {
      "epoch": 0.9196747967479675,
      "step": 4242,
      "training_loss": 4.98249626159668
    },
    {
      "epoch": 0.9196747967479675,
      "step": 4242,
      "training_loss": 5.837040424346924
    },
    {
      "epoch": 0.9198915989159892,
      "step": 4243,
      "training_loss": 6.284899711608887
    },
    {
      "epoch": 0.9198915989159892,
      "step": 4243,
      "training_loss": 3.5308589935302734
    },
    {
      "epoch": 0.9198915989159892,
      "step": 4243,
      "training_loss": 6.616332054138184
    },
    {
      "epoch": 0.9198915989159892,
      "step": 4243,
      "training_loss": 5.363199234008789
    },
    {
      "epoch": 0.9201084010840108,
      "grad_norm": 26.982318878173828,
      "learning_rate": 1e-05,
      "loss": 5.9437,
      "step": 4244
    },
    {
      "epoch": 0.9201084010840108,
      "step": 4244,
      "training_loss": 3.380467414855957
    },
    {
      "epoch": 0.9201084010840108,
      "step": 4244,
      "training_loss": 5.325939178466797
    },
    {
      "epoch": 0.9201084010840108,
      "step": 4244,
      "training_loss": 7.8197431564331055
    },
    {
      "epoch": 0.9201084010840108,
      "step": 4244,
      "training_loss": 7.884316444396973
    },
    {
      "epoch": 0.9203252032520325,
      "step": 4245,
      "training_loss": 6.008049488067627
    },
    {
      "epoch": 0.9203252032520325,
      "step": 4245,
      "training_loss": 8.24367904663086
    },
    {
      "epoch": 0.9203252032520325,
      "step": 4245,
      "training_loss": 7.69700288772583
    },
    {
      "epoch": 0.9203252032520325,
      "step": 4245,
      "training_loss": 4.7595906257629395
    },
    {
      "epoch": 0.9205420054200542,
      "step": 4246,
      "training_loss": 6.132907867431641
    },
    {
      "epoch": 0.9205420054200542,
      "step": 4246,
      "training_loss": 6.239230632781982
    },
    {
      "epoch": 0.9205420054200542,
      "step": 4246,
      "training_loss": 6.627856254577637
    },
    {
      "epoch": 0.9205420054200542,
      "step": 4246,
      "training_loss": 6.072240829467773
    },
    {
      "epoch": 0.9207588075880759,
      "step": 4247,
      "training_loss": 6.432920455932617
    },
    {
      "epoch": 0.9207588075880759,
      "step": 4247,
      "training_loss": 6.5307087898254395
    },
    {
      "epoch": 0.9207588075880759,
      "step": 4247,
      "training_loss": 7.733202934265137
    },
    {
      "epoch": 0.9207588075880759,
      "step": 4247,
      "training_loss": 6.1899094581604
    },
    {
      "epoch": 0.9209756097560976,
      "grad_norm": 19.018522262573242,
      "learning_rate": 1e-05,
      "loss": 6.4424,
      "step": 4248
    },
    {
      "epoch": 0.9209756097560976,
      "step": 4248,
      "training_loss": 7.050192832946777
    },
    {
      "epoch": 0.9209756097560976,
      "step": 4248,
      "training_loss": 7.383187770843506
    },
    {
      "epoch": 0.9209756097560976,
      "step": 4248,
      "training_loss": 6.848055362701416
    },
    {
      "epoch": 0.9209756097560976,
      "step": 4248,
      "training_loss": 7.459481239318848
    },
    {
      "epoch": 0.9211924119241193,
      "step": 4249,
      "training_loss": 6.691503524780273
    },
    {
      "epoch": 0.9211924119241193,
      "step": 4249,
      "training_loss": 5.683101177215576
    },
    {
      "epoch": 0.9211924119241193,
      "step": 4249,
      "training_loss": 5.563724040985107
    },
    {
      "epoch": 0.9211924119241193,
      "step": 4249,
      "training_loss": 6.8326921463012695
    },
    {
      "epoch": 0.9214092140921409,
      "step": 4250,
      "training_loss": 5.372114181518555
    },
    {
      "epoch": 0.9214092140921409,
      "step": 4250,
      "training_loss": 5.2844719886779785
    },
    {
      "epoch": 0.9214092140921409,
      "step": 4250,
      "training_loss": 3.0636916160583496
    },
    {
      "epoch": 0.9214092140921409,
      "step": 4250,
      "training_loss": 6.514771938323975
    },
    {
      "epoch": 0.9216260162601626,
      "step": 4251,
      "training_loss": 7.791536331176758
    },
    {
      "epoch": 0.9216260162601626,
      "step": 4251,
      "training_loss": 6.483104705810547
    },
    {
      "epoch": 0.9216260162601626,
      "step": 4251,
      "training_loss": 5.236752033233643
    },
    {
      "epoch": 0.9216260162601626,
      "step": 4251,
      "training_loss": 6.923581600189209
    },
    {
      "epoch": 0.9218428184281843,
      "grad_norm": 23.68426513671875,
      "learning_rate": 1e-05,
      "loss": 6.2614,
      "step": 4252
    },
    {
      "epoch": 0.9218428184281843,
      "step": 4252,
      "training_loss": 6.591475009918213
    },
    {
      "epoch": 0.9218428184281843,
      "step": 4252,
      "training_loss": 7.875827312469482
    },
    {
      "epoch": 0.9218428184281843,
      "step": 4252,
      "training_loss": 6.892199993133545
    },
    {
      "epoch": 0.9218428184281843,
      "step": 4252,
      "training_loss": 2.820020914077759
    },
    {
      "epoch": 0.9220596205962059,
      "step": 4253,
      "training_loss": 5.547032356262207
    },
    {
      "epoch": 0.9220596205962059,
      "step": 4253,
      "training_loss": 9.733487129211426
    },
    {
      "epoch": 0.9220596205962059,
      "step": 4253,
      "training_loss": 4.303624629974365
    },
    {
      "epoch": 0.9220596205962059,
      "step": 4253,
      "training_loss": 4.5170745849609375
    },
    {
      "epoch": 0.9222764227642276,
      "step": 4254,
      "training_loss": 6.460670471191406
    },
    {
      "epoch": 0.9222764227642276,
      "step": 4254,
      "training_loss": 4.059504985809326
    },
    {
      "epoch": 0.9222764227642276,
      "step": 4254,
      "training_loss": 5.6273956298828125
    },
    {
      "epoch": 0.9222764227642276,
      "step": 4254,
      "training_loss": 5.177439212799072
    },
    {
      "epoch": 0.9224932249322493,
      "step": 4255,
      "training_loss": 6.901109218597412
    },
    {
      "epoch": 0.9224932249322493,
      "step": 4255,
      "training_loss": 6.244435787200928
    },
    {
      "epoch": 0.9224932249322493,
      "step": 4255,
      "training_loss": 8.06550121307373
    },
    {
      "epoch": 0.9224932249322493,
      "step": 4255,
      "training_loss": 7.260235786437988
    },
    {
      "epoch": 0.9227100271002711,
      "grad_norm": 15.62811279296875,
      "learning_rate": 1e-05,
      "loss": 6.1298,
      "step": 4256
    },
    {
      "epoch": 0.9227100271002711,
      "step": 4256,
      "training_loss": 7.676105976104736
    },
    {
      "epoch": 0.9227100271002711,
      "step": 4256,
      "training_loss": 7.048173427581787
    },
    {
      "epoch": 0.9227100271002711,
      "step": 4256,
      "training_loss": 4.457672595977783
    },
    {
      "epoch": 0.9227100271002711,
      "step": 4256,
      "training_loss": 6.608170509338379
    },
    {
      "epoch": 0.9229268292682927,
      "step": 4257,
      "training_loss": 6.807446002960205
    },
    {
      "epoch": 0.9229268292682927,
      "step": 4257,
      "training_loss": 6.677175045013428
    },
    {
      "epoch": 0.9229268292682927,
      "step": 4257,
      "training_loss": 4.964723110198975
    },
    {
      "epoch": 0.9229268292682927,
      "step": 4257,
      "training_loss": 7.50665807723999
    },
    {
      "epoch": 0.9231436314363144,
      "step": 4258,
      "training_loss": 6.292258262634277
    },
    {
      "epoch": 0.9231436314363144,
      "step": 4258,
      "training_loss": 3.072399377822876
    },
    {
      "epoch": 0.9231436314363144,
      "step": 4258,
      "training_loss": 6.831784248352051
    },
    {
      "epoch": 0.9231436314363144,
      "step": 4258,
      "training_loss": 7.058207988739014
    },
    {
      "epoch": 0.9233604336043361,
      "step": 4259,
      "training_loss": 3.9495582580566406
    },
    {
      "epoch": 0.9233604336043361,
      "step": 4259,
      "training_loss": 5.383200168609619
    },
    {
      "epoch": 0.9233604336043361,
      "step": 4259,
      "training_loss": 6.297447681427002
    },
    {
      "epoch": 0.9233604336043361,
      "step": 4259,
      "training_loss": 6.269244194030762
    },
    {
      "epoch": 0.9235772357723577,
      "grad_norm": 14.925999641418457,
      "learning_rate": 1e-05,
      "loss": 6.0563,
      "step": 4260
    },
    {
      "epoch": 0.9235772357723577,
      "step": 4260,
      "training_loss": 4.675545692443848
    },
    {
      "epoch": 0.9235772357723577,
      "step": 4260,
      "training_loss": 6.3038153648376465
    },
    {
      "epoch": 0.9235772357723577,
      "step": 4260,
      "training_loss": 4.828342437744141
    },
    {
      "epoch": 0.9235772357723577,
      "step": 4260,
      "training_loss": 6.001028060913086
    },
    {
      "epoch": 0.9237940379403794,
      "step": 4261,
      "training_loss": 6.778164386749268
    },
    {
      "epoch": 0.9237940379403794,
      "step": 4261,
      "training_loss": 6.70091438293457
    },
    {
      "epoch": 0.9237940379403794,
      "step": 4261,
      "training_loss": 7.2907938957214355
    },
    {
      "epoch": 0.9237940379403794,
      "step": 4261,
      "training_loss": 5.841175079345703
    },
    {
      "epoch": 0.9240108401084011,
      "step": 4262,
      "training_loss": 7.252900123596191
    },
    {
      "epoch": 0.9240108401084011,
      "step": 4262,
      "training_loss": 6.286895275115967
    },
    {
      "epoch": 0.9240108401084011,
      "step": 4262,
      "training_loss": 6.31552791595459
    },
    {
      "epoch": 0.9240108401084011,
      "step": 4262,
      "training_loss": 6.3018574714660645
    },
    {
      "epoch": 0.9242276422764227,
      "step": 4263,
      "training_loss": 7.2408246994018555
    },
    {
      "epoch": 0.9242276422764227,
      "step": 4263,
      "training_loss": 5.8019208908081055
    },
    {
      "epoch": 0.9242276422764227,
      "step": 4263,
      "training_loss": 6.855871677398682
    },
    {
      "epoch": 0.9242276422764227,
      "step": 4263,
      "training_loss": 5.145963191986084
    },
    {
      "epoch": 0.9244444444444444,
      "grad_norm": 15.168167114257812,
      "learning_rate": 1e-05,
      "loss": 6.2263,
      "step": 4264
    },
    {
      "epoch": 0.9244444444444444,
      "step": 4264,
      "training_loss": 7.158377647399902
    },
    {
      "epoch": 0.9244444444444444,
      "step": 4264,
      "training_loss": 7.030137538909912
    },
    {
      "epoch": 0.9244444444444444,
      "step": 4264,
      "training_loss": 7.438364505767822
    },
    {
      "epoch": 0.9244444444444444,
      "step": 4264,
      "training_loss": 6.245203971862793
    },
    {
      "epoch": 0.9246612466124661,
      "step": 4265,
      "training_loss": 6.592216968536377
    },
    {
      "epoch": 0.9246612466124661,
      "step": 4265,
      "training_loss": 7.626577854156494
    },
    {
      "epoch": 0.9246612466124661,
      "step": 4265,
      "training_loss": 7.1187357902526855
    },
    {
      "epoch": 0.9246612466124661,
      "step": 4265,
      "training_loss": 6.840991497039795
    },
    {
      "epoch": 0.9248780487804878,
      "step": 4266,
      "training_loss": 5.697538375854492
    },
    {
      "epoch": 0.9248780487804878,
      "step": 4266,
      "training_loss": 7.370185375213623
    },
    {
      "epoch": 0.9248780487804878,
      "step": 4266,
      "training_loss": 7.777992248535156
    },
    {
      "epoch": 0.9248780487804878,
      "step": 4266,
      "training_loss": 7.349170207977295
    },
    {
      "epoch": 0.9250948509485095,
      "step": 4267,
      "training_loss": 6.463440895080566
    },
    {
      "epoch": 0.9250948509485095,
      "step": 4267,
      "training_loss": 6.149903774261475
    },
    {
      "epoch": 0.9250948509485095,
      "step": 4267,
      "training_loss": 6.282740116119385
    },
    {
      "epoch": 0.9250948509485095,
      "step": 4267,
      "training_loss": 4.908540725708008
    },
    {
      "epoch": 0.9253116531165312,
      "grad_norm": 20.80484390258789,
      "learning_rate": 1e-05,
      "loss": 6.7531,
      "step": 4268
    },
    {
      "epoch": 0.9253116531165312,
      "step": 4268,
      "training_loss": 5.521336078643799
    },
    {
      "epoch": 0.9253116531165312,
      "step": 4268,
      "training_loss": 5.562433242797852
    },
    {
      "epoch": 0.9253116531165312,
      "step": 4268,
      "training_loss": 7.3708930015563965
    },
    {
      "epoch": 0.9253116531165312,
      "step": 4268,
      "training_loss": 3.4542362689971924
    },
    {
      "epoch": 0.9255284552845529,
      "step": 4269,
      "training_loss": 6.4165754318237305
    },
    {
      "epoch": 0.9255284552845529,
      "step": 4269,
      "training_loss": 6.053107261657715
    },
    {
      "epoch": 0.9255284552845529,
      "step": 4269,
      "training_loss": 6.237139701843262
    },
    {
      "epoch": 0.9255284552845529,
      "step": 4269,
      "training_loss": 4.99423885345459
    },
    {
      "epoch": 0.9257452574525745,
      "step": 4270,
      "training_loss": 5.113214015960693
    },
    {
      "epoch": 0.9257452574525745,
      "step": 4270,
      "training_loss": 7.3638200759887695
    },
    {
      "epoch": 0.9257452574525745,
      "step": 4270,
      "training_loss": 6.389370441436768
    },
    {
      "epoch": 0.9257452574525745,
      "step": 4270,
      "training_loss": 7.0387654304504395
    },
    {
      "epoch": 0.9259620596205962,
      "step": 4271,
      "training_loss": 3.290684461593628
    },
    {
      "epoch": 0.9259620596205962,
      "step": 4271,
      "training_loss": 6.525545120239258
    },
    {
      "epoch": 0.9259620596205962,
      "step": 4271,
      "training_loss": 5.646857738494873
    },
    {
      "epoch": 0.9259620596205962,
      "step": 4271,
      "training_loss": 4.310770511627197
    },
    {
      "epoch": 0.9261788617886179,
      "grad_norm": 17.23649024963379,
      "learning_rate": 1e-05,
      "loss": 5.7056,
      "step": 4272
    },
    {
      "epoch": 0.9261788617886179,
      "step": 4272,
      "training_loss": 7.733725070953369
    },
    {
      "epoch": 0.9261788617886179,
      "step": 4272,
      "training_loss": 4.5905585289001465
    },
    {
      "epoch": 0.9261788617886179,
      "step": 4272,
      "training_loss": 6.749102592468262
    },
    {
      "epoch": 0.9261788617886179,
      "step": 4272,
      "training_loss": 6.121828079223633
    },
    {
      "epoch": 0.9263956639566395,
      "step": 4273,
      "training_loss": 5.683700084686279
    },
    {
      "epoch": 0.9263956639566395,
      "step": 4273,
      "training_loss": 7.221662998199463
    },
    {
      "epoch": 0.9263956639566395,
      "step": 4273,
      "training_loss": 7.582334041595459
    },
    {
      "epoch": 0.9263956639566395,
      "step": 4273,
      "training_loss": 5.062625885009766
    },
    {
      "epoch": 0.9266124661246612,
      "step": 4274,
      "training_loss": 6.095881938934326
    },
    {
      "epoch": 0.9266124661246612,
      "step": 4274,
      "training_loss": 6.818102836608887
    },
    {
      "epoch": 0.9266124661246612,
      "step": 4274,
      "training_loss": 6.075327396392822
    },
    {
      "epoch": 0.9266124661246612,
      "step": 4274,
      "training_loss": 5.574254512786865
    },
    {
      "epoch": 0.926829268292683,
      "step": 4275,
      "training_loss": 7.857845306396484
    },
    {
      "epoch": 0.926829268292683,
      "step": 4275,
      "training_loss": 7.266656875610352
    },
    {
      "epoch": 0.926829268292683,
      "step": 4275,
      "training_loss": 6.0896148681640625
    },
    {
      "epoch": 0.926829268292683,
      "step": 4275,
      "training_loss": 7.6026482582092285
    },
    {
      "epoch": 0.9270460704607046,
      "grad_norm": 19.864116668701172,
      "learning_rate": 1e-05,
      "loss": 6.5079,
      "step": 4276
    },
    {
      "epoch": 0.9270460704607046,
      "step": 4276,
      "training_loss": 7.534083843231201
    },
    {
      "epoch": 0.9270460704607046,
      "step": 4276,
      "training_loss": 6.486171245574951
    },
    {
      "epoch": 0.9270460704607046,
      "step": 4276,
      "training_loss": 3.310757875442505
    },
    {
      "epoch": 0.9270460704607046,
      "step": 4276,
      "training_loss": 5.422888278961182
    },
    {
      "epoch": 0.9272628726287263,
      "step": 4277,
      "training_loss": 4.451279640197754
    },
    {
      "epoch": 0.9272628726287263,
      "step": 4277,
      "training_loss": 6.419741630554199
    },
    {
      "epoch": 0.9272628726287263,
      "step": 4277,
      "training_loss": 7.3962297439575195
    },
    {
      "epoch": 0.9272628726287263,
      "step": 4277,
      "training_loss": 7.403610706329346
    },
    {
      "epoch": 0.927479674796748,
      "step": 4278,
      "training_loss": 6.5701003074646
    },
    {
      "epoch": 0.927479674796748,
      "step": 4278,
      "training_loss": 7.108794689178467
    },
    {
      "epoch": 0.927479674796748,
      "step": 4278,
      "training_loss": 5.388720989227295
    },
    {
      "epoch": 0.927479674796748,
      "step": 4278,
      "training_loss": 7.187928199768066
    },
    {
      "epoch": 0.9276964769647696,
      "step": 4279,
      "training_loss": 6.671408176422119
    },
    {
      "epoch": 0.9276964769647696,
      "step": 4279,
      "training_loss": 3.2565600872039795
    },
    {
      "epoch": 0.9276964769647696,
      "step": 4279,
      "training_loss": 7.614570617675781
    },
    {
      "epoch": 0.9276964769647696,
      "step": 4279,
      "training_loss": 9.24040699005127
    },
    {
      "epoch": 0.9279132791327913,
      "grad_norm": 31.07538604736328,
      "learning_rate": 1e-05,
      "loss": 6.3415,
      "step": 4280
    },
    {
      "epoch": 0.9279132791327913,
      "step": 4280,
      "training_loss": 6.681320667266846
    },
    {
      "epoch": 0.9279132791327913,
      "step": 4280,
      "training_loss": 6.476029872894287
    },
    {
      "epoch": 0.9279132791327913,
      "step": 4280,
      "training_loss": 5.503454208374023
    },
    {
      "epoch": 0.9279132791327913,
      "step": 4280,
      "training_loss": 7.408880710601807
    },
    {
      "epoch": 0.928130081300813,
      "step": 4281,
      "training_loss": 6.695685863494873
    },
    {
      "epoch": 0.928130081300813,
      "step": 4281,
      "training_loss": 4.224656581878662
    },
    {
      "epoch": 0.928130081300813,
      "step": 4281,
      "training_loss": 7.638975620269775
    },
    {
      "epoch": 0.928130081300813,
      "step": 4281,
      "training_loss": 6.895853042602539
    },
    {
      "epoch": 0.9283468834688346,
      "step": 4282,
      "training_loss": 6.475865840911865
    },
    {
      "epoch": 0.9283468834688346,
      "step": 4282,
      "training_loss": 7.252603530883789
    },
    {
      "epoch": 0.9283468834688346,
      "step": 4282,
      "training_loss": 7.503712177276611
    },
    {
      "epoch": 0.9283468834688346,
      "step": 4282,
      "training_loss": 6.731682300567627
    },
    {
      "epoch": 0.9285636856368563,
      "step": 4283,
      "training_loss": 6.08200740814209
    },
    {
      "epoch": 0.9285636856368563,
      "step": 4283,
      "training_loss": 5.60313081741333
    },
    {
      "epoch": 0.9285636856368563,
      "step": 4283,
      "training_loss": 7.128406047821045
    },
    {
      "epoch": 0.9285636856368563,
      "step": 4283,
      "training_loss": 6.479773998260498
    },
    {
      "epoch": 0.9287804878048781,
      "grad_norm": 22.428346633911133,
      "learning_rate": 1e-05,
      "loss": 6.5489,
      "step": 4284
    },
    {
      "epoch": 0.9287804878048781,
      "step": 4284,
      "training_loss": 4.263165473937988
    },
    {
      "epoch": 0.9287804878048781,
      "step": 4284,
      "training_loss": 4.712902545928955
    },
    {
      "epoch": 0.9287804878048781,
      "step": 4284,
      "training_loss": 6.1289567947387695
    },
    {
      "epoch": 0.9287804878048781,
      "step": 4284,
      "training_loss": 6.347836017608643
    },
    {
      "epoch": 0.9289972899728998,
      "step": 4285,
      "training_loss": 7.03170919418335
    },
    {
      "epoch": 0.9289972899728998,
      "step": 4285,
      "training_loss": 7.310783863067627
    },
    {
      "epoch": 0.9289972899728998,
      "step": 4285,
      "training_loss": 3.984732151031494
    },
    {
      "epoch": 0.9289972899728998,
      "step": 4285,
      "training_loss": 7.034400463104248
    },
    {
      "epoch": 0.9292140921409214,
      "step": 4286,
      "training_loss": 6.166496753692627
    },
    {
      "epoch": 0.9292140921409214,
      "step": 4286,
      "training_loss": 5.845729827880859
    },
    {
      "epoch": 0.9292140921409214,
      "step": 4286,
      "training_loss": 6.788798809051514
    },
    {
      "epoch": 0.9292140921409214,
      "step": 4286,
      "training_loss": 7.050172328948975
    },
    {
      "epoch": 0.9294308943089431,
      "step": 4287,
      "training_loss": 6.0093817710876465
    },
    {
      "epoch": 0.9294308943089431,
      "step": 4287,
      "training_loss": 7.06815767288208
    },
    {
      "epoch": 0.9294308943089431,
      "step": 4287,
      "training_loss": 5.923285484313965
    },
    {
      "epoch": 0.9294308943089431,
      "step": 4287,
      "training_loss": 5.311715126037598
    },
    {
      "epoch": 0.9296476964769648,
      "grad_norm": 14.889792442321777,
      "learning_rate": 1e-05,
      "loss": 6.0611,
      "step": 4288
    },
    {
      "epoch": 0.9296476964769648,
      "step": 4288,
      "training_loss": 6.541898727416992
    },
    {
      "epoch": 0.9296476964769648,
      "step": 4288,
      "training_loss": 7.116058826446533
    },
    {
      "epoch": 0.9296476964769648,
      "step": 4288,
      "training_loss": 4.764488220214844
    },
    {
      "epoch": 0.9296476964769648,
      "step": 4288,
      "training_loss": 4.400796413421631
    },
    {
      "epoch": 0.9298644986449864,
      "step": 4289,
      "training_loss": 6.335179805755615
    },
    {
      "epoch": 0.9298644986449864,
      "step": 4289,
      "training_loss": 6.01079797744751
    },
    {
      "epoch": 0.9298644986449864,
      "step": 4289,
      "training_loss": 6.985204696655273
    },
    {
      "epoch": 0.9298644986449864,
      "step": 4289,
      "training_loss": 6.571112632751465
    },
    {
      "epoch": 0.9300813008130081,
      "step": 4290,
      "training_loss": 7.558297634124756
    },
    {
      "epoch": 0.9300813008130081,
      "step": 4290,
      "training_loss": 6.545877456665039
    },
    {
      "epoch": 0.9300813008130081,
      "step": 4290,
      "training_loss": 6.29432487487793
    },
    {
      "epoch": 0.9300813008130081,
      "step": 4290,
      "training_loss": 5.633041858673096
    },
    {
      "epoch": 0.9302981029810298,
      "step": 4291,
      "training_loss": 7.2860260009765625
    },
    {
      "epoch": 0.9302981029810298,
      "step": 4291,
      "training_loss": 3.787620782852173
    },
    {
      "epoch": 0.9302981029810298,
      "step": 4291,
      "training_loss": 7.137992858886719
    },
    {
      "epoch": 0.9302981029810298,
      "step": 4291,
      "training_loss": 6.780864715576172
    },
    {
      "epoch": 0.9305149051490514,
      "grad_norm": 18.14964485168457,
      "learning_rate": 1e-05,
      "loss": 6.2343,
      "step": 4292
    },
    {
      "epoch": 0.9305149051490514,
      "step": 4292,
      "training_loss": 6.434102535247803
    },
    {
      "epoch": 0.9305149051490514,
      "step": 4292,
      "training_loss": 6.073678016662598
    },
    {
      "epoch": 0.9305149051490514,
      "step": 4292,
      "training_loss": 6.403369426727295
    },
    {
      "epoch": 0.9305149051490514,
      "step": 4292,
      "training_loss": 6.477108478546143
    },
    {
      "epoch": 0.9307317073170732,
      "step": 4293,
      "training_loss": 5.875454902648926
    },
    {
      "epoch": 0.9307317073170732,
      "step": 4293,
      "training_loss": 6.983855724334717
    },
    {
      "epoch": 0.9307317073170732,
      "step": 4293,
      "training_loss": 6.377506256103516
    },
    {
      "epoch": 0.9307317073170732,
      "step": 4293,
      "training_loss": 2.925422191619873
    },
    {
      "epoch": 0.9309485094850949,
      "step": 4294,
      "training_loss": 3.154921531677246
    },
    {
      "epoch": 0.9309485094850949,
      "step": 4294,
      "training_loss": 6.32338285446167
    },
    {
      "epoch": 0.9309485094850949,
      "step": 4294,
      "training_loss": 3.8971993923187256
    },
    {
      "epoch": 0.9309485094850949,
      "step": 4294,
      "training_loss": 6.555447578430176
    },
    {
      "epoch": 0.9311653116531166,
      "step": 4295,
      "training_loss": 6.430873394012451
    },
    {
      "epoch": 0.9311653116531166,
      "step": 4295,
      "training_loss": 5.206009864807129
    },
    {
      "epoch": 0.9311653116531166,
      "step": 4295,
      "training_loss": 4.3556809425354
    },
    {
      "epoch": 0.9311653116531166,
      "step": 4295,
      "training_loss": 5.609290599822998
    },
    {
      "epoch": 0.9313821138211382,
      "grad_norm": 21.206981658935547,
      "learning_rate": 1e-05,
      "loss": 5.5677,
      "step": 4296
    },
    {
      "epoch": 0.9313821138211382,
      "step": 4296,
      "training_loss": 7.399652004241943
    },
    {
      "epoch": 0.9313821138211382,
      "step": 4296,
      "training_loss": 7.057575225830078
    },
    {
      "epoch": 0.9313821138211382,
      "step": 4296,
      "training_loss": 4.4845051765441895
    },
    {
      "epoch": 0.9313821138211382,
      "step": 4296,
      "training_loss": 6.821643352508545
    },
    {
      "epoch": 0.9315989159891599,
      "step": 4297,
      "training_loss": 6.82240629196167
    },
    {
      "epoch": 0.9315989159891599,
      "step": 4297,
      "training_loss": 7.218539714813232
    },
    {
      "epoch": 0.9315989159891599,
      "step": 4297,
      "training_loss": 5.7882513999938965
    },
    {
      "epoch": 0.9315989159891599,
      "step": 4297,
      "training_loss": 7.681188106536865
    },
    {
      "epoch": 0.9318157181571816,
      "step": 4298,
      "training_loss": 6.402048587799072
    },
    {
      "epoch": 0.9318157181571816,
      "step": 4298,
      "training_loss": 3.0781731605529785
    },
    {
      "epoch": 0.9318157181571816,
      "step": 4298,
      "training_loss": 6.407222270965576
    },
    {
      "epoch": 0.9318157181571816,
      "step": 4298,
      "training_loss": 4.586304187774658
    },
    {
      "epoch": 0.9320325203252032,
      "step": 4299,
      "training_loss": 7.675629615783691
    },
    {
      "epoch": 0.9320325203252032,
      "step": 4299,
      "training_loss": 4.166400909423828
    },
    {
      "epoch": 0.9320325203252032,
      "step": 4299,
      "training_loss": 6.589346408843994
    },
    {
      "epoch": 0.9320325203252032,
      "step": 4299,
      "training_loss": 5.839894771575928
    },
    {
      "epoch": 0.9322493224932249,
      "grad_norm": 30.0489501953125,
      "learning_rate": 1e-05,
      "loss": 6.1262,
      "step": 4300
    },
    {
      "epoch": 0.9322493224932249,
      "step": 4300,
      "training_loss": 4.2804975509643555
    },
    {
      "epoch": 0.9322493224932249,
      "step": 4300,
      "training_loss": 5.054226875305176
    },
    {
      "epoch": 0.9322493224932249,
      "step": 4300,
      "training_loss": 6.484694480895996
    },
    {
      "epoch": 0.9322493224932249,
      "step": 4300,
      "training_loss": 6.673876762390137
    },
    {
      "epoch": 0.9324661246612466,
      "step": 4301,
      "training_loss": 7.2894744873046875
    },
    {
      "epoch": 0.9324661246612466,
      "step": 4301,
      "training_loss": 7.7251434326171875
    },
    {
      "epoch": 0.9324661246612466,
      "step": 4301,
      "training_loss": 5.941806316375732
    },
    {
      "epoch": 0.9324661246612466,
      "step": 4301,
      "training_loss": 6.76962423324585
    },
    {
      "epoch": 0.9326829268292683,
      "step": 4302,
      "training_loss": 8.200639724731445
    },
    {
      "epoch": 0.9326829268292683,
      "step": 4302,
      "training_loss": 7.748571395874023
    },
    {
      "epoch": 0.9326829268292683,
      "step": 4302,
      "training_loss": 6.974893093109131
    },
    {
      "epoch": 0.9326829268292683,
      "step": 4302,
      "training_loss": 5.478403091430664
    },
    {
      "epoch": 0.93289972899729,
      "step": 4303,
      "training_loss": 7.059028625488281
    },
    {
      "epoch": 0.93289972899729,
      "step": 4303,
      "training_loss": 7.5587382316589355
    },
    {
      "epoch": 0.93289972899729,
      "step": 4303,
      "training_loss": 6.498142242431641
    },
    {
      "epoch": 0.93289972899729,
      "step": 4303,
      "training_loss": 5.683053493499756
    },
    {
      "epoch": 0.9331165311653117,
      "grad_norm": 23.495830535888672,
      "learning_rate": 1e-05,
      "loss": 6.5888,
      "step": 4304
    },
    {
      "epoch": 0.9331165311653117,
      "step": 4304,
      "training_loss": 7.2167744636535645
    },
    {
      "epoch": 0.9331165311653117,
      "step": 4304,
      "training_loss": 6.762582778930664
    },
    {
      "epoch": 0.9331165311653117,
      "step": 4304,
      "training_loss": 7.266534805297852
    },
    {
      "epoch": 0.9331165311653117,
      "step": 4304,
      "training_loss": 6.9365234375
    },
    {
      "epoch": 0.9333333333333333,
      "step": 4305,
      "training_loss": 6.497404098510742
    },
    {
      "epoch": 0.9333333333333333,
      "step": 4305,
      "training_loss": 6.232386112213135
    },
    {
      "epoch": 0.9333333333333333,
      "step": 4305,
      "training_loss": 7.035166263580322
    },
    {
      "epoch": 0.9333333333333333,
      "step": 4305,
      "training_loss": 7.546995639801025
    },
    {
      "epoch": 0.933550135501355,
      "step": 4306,
      "training_loss": 6.82370138168335
    },
    {
      "epoch": 0.933550135501355,
      "step": 4306,
      "training_loss": 6.327175140380859
    },
    {
      "epoch": 0.933550135501355,
      "step": 4306,
      "training_loss": 6.794777870178223
    },
    {
      "epoch": 0.933550135501355,
      "step": 4306,
      "training_loss": 6.200428009033203
    },
    {
      "epoch": 0.9337669376693767,
      "step": 4307,
      "training_loss": 5.4935407638549805
    },
    {
      "epoch": 0.9337669376693767,
      "step": 4307,
      "training_loss": 6.26507568359375
    },
    {
      "epoch": 0.9337669376693767,
      "step": 4307,
      "training_loss": 6.662224292755127
    },
    {
      "epoch": 0.9337669376693767,
      "step": 4307,
      "training_loss": 6.623678207397461
    },
    {
      "epoch": 0.9339837398373984,
      "grad_norm": 32.814476013183594,
      "learning_rate": 1e-05,
      "loss": 6.6678,
      "step": 4308
    },
    {
      "epoch": 0.9339837398373984,
      "step": 4308,
      "training_loss": 5.768085479736328
    },
    {
      "epoch": 0.9339837398373984,
      "step": 4308,
      "training_loss": 7.384498119354248
    },
    {
      "epoch": 0.9339837398373984,
      "step": 4308,
      "training_loss": 4.977715969085693
    },
    {
      "epoch": 0.9339837398373984,
      "step": 4308,
      "training_loss": 6.701714515686035
    },
    {
      "epoch": 0.93420054200542,
      "step": 4309,
      "training_loss": 7.872292518615723
    },
    {
      "epoch": 0.93420054200542,
      "step": 4309,
      "training_loss": 8.138528823852539
    },
    {
      "epoch": 0.93420054200542,
      "step": 4309,
      "training_loss": 6.3859968185424805
    },
    {
      "epoch": 0.93420054200542,
      "step": 4309,
      "training_loss": 5.326344013214111
    },
    {
      "epoch": 0.9344173441734417,
      "step": 4310,
      "training_loss": 6.725929260253906
    },
    {
      "epoch": 0.9344173441734417,
      "step": 4310,
      "training_loss": 7.9059882164001465
    },
    {
      "epoch": 0.9344173441734417,
      "step": 4310,
      "training_loss": 6.4538373947143555
    },
    {
      "epoch": 0.9344173441734417,
      "step": 4310,
      "training_loss": 3.095529317855835
    },
    {
      "epoch": 0.9346341463414635,
      "step": 4311,
      "training_loss": 6.925095081329346
    },
    {
      "epoch": 0.9346341463414635,
      "step": 4311,
      "training_loss": 6.52725887298584
    },
    {
      "epoch": 0.9346341463414635,
      "step": 4311,
      "training_loss": 7.223137378692627
    },
    {
      "epoch": 0.9346341463414635,
      "step": 4311,
      "training_loss": 6.301547527313232
    },
    {
      "epoch": 0.9348509485094851,
      "grad_norm": 16.462932586669922,
      "learning_rate": 1e-05,
      "loss": 6.4821,
      "step": 4312
    },
    {
      "epoch": 0.9348509485094851,
      "step": 4312,
      "training_loss": 5.351309299468994
    },
    {
      "epoch": 0.9348509485094851,
      "step": 4312,
      "training_loss": 7.570656776428223
    },
    {
      "epoch": 0.9348509485094851,
      "step": 4312,
      "training_loss": 5.97263240814209
    },
    {
      "epoch": 0.9348509485094851,
      "step": 4312,
      "training_loss": 6.515872001647949
    },
    {
      "epoch": 0.9350677506775068,
      "step": 4313,
      "training_loss": 3.3749382495880127
    },
    {
      "epoch": 0.9350677506775068,
      "step": 4313,
      "training_loss": 3.818916082382202
    },
    {
      "epoch": 0.9350677506775068,
      "step": 4313,
      "training_loss": 7.030222415924072
    },
    {
      "epoch": 0.9350677506775068,
      "step": 4313,
      "training_loss": 7.210524559020996
    },
    {
      "epoch": 0.9352845528455285,
      "step": 4314,
      "training_loss": 6.968225955963135
    },
    {
      "epoch": 0.9352845528455285,
      "step": 4314,
      "training_loss": 6.261636257171631
    },
    {
      "epoch": 0.9352845528455285,
      "step": 4314,
      "training_loss": 7.38392972946167
    },
    {
      "epoch": 0.9352845528455285,
      "step": 4314,
      "training_loss": 6.236428737640381
    },
    {
      "epoch": 0.9355013550135501,
      "step": 4315,
      "training_loss": 7.7620110511779785
    },
    {
      "epoch": 0.9355013550135501,
      "step": 4315,
      "training_loss": 6.103797435760498
    },
    {
      "epoch": 0.9355013550135501,
      "step": 4315,
      "training_loss": 6.183904647827148
    },
    {
      "epoch": 0.9355013550135501,
      "step": 4315,
      "training_loss": 5.761692523956299
    },
    {
      "epoch": 0.9357181571815718,
      "grad_norm": 19.51193618774414,
      "learning_rate": 1e-05,
      "loss": 6.2192,
      "step": 4316
    },
    {
      "epoch": 0.9357181571815718,
      "step": 4316,
      "training_loss": 5.556583881378174
    },
    {
      "epoch": 0.9357181571815718,
      "step": 4316,
      "training_loss": 5.441173076629639
    },
    {
      "epoch": 0.9357181571815718,
      "step": 4316,
      "training_loss": 6.900035858154297
    },
    {
      "epoch": 0.9357181571815718,
      "step": 4316,
      "training_loss": 6.036350727081299
    },
    {
      "epoch": 0.9359349593495935,
      "step": 4317,
      "training_loss": 7.29218053817749
    },
    {
      "epoch": 0.9359349593495935,
      "step": 4317,
      "training_loss": 6.137444972991943
    },
    {
      "epoch": 0.9359349593495935,
      "step": 4317,
      "training_loss": 7.544515609741211
    },
    {
      "epoch": 0.9359349593495935,
      "step": 4317,
      "training_loss": 5.676280498504639
    },
    {
      "epoch": 0.9361517615176151,
      "step": 4318,
      "training_loss": 5.831944465637207
    },
    {
      "epoch": 0.9361517615176151,
      "step": 4318,
      "training_loss": 6.070282459259033
    },
    {
      "epoch": 0.9361517615176151,
      "step": 4318,
      "training_loss": 4.903224945068359
    },
    {
      "epoch": 0.9361517615176151,
      "step": 4318,
      "training_loss": 7.5697431564331055
    },
    {
      "epoch": 0.9363685636856368,
      "step": 4319,
      "training_loss": 7.130636692047119
    },
    {
      "epoch": 0.9363685636856368,
      "step": 4319,
      "training_loss": 9.622325897216797
    },
    {
      "epoch": 0.9363685636856368,
      "step": 4319,
      "training_loss": 3.3273260593414307
    },
    {
      "epoch": 0.9363685636856368,
      "step": 4319,
      "training_loss": 5.3376922607421875
    },
    {
      "epoch": 0.9365853658536586,
      "grad_norm": 20.323774337768555,
      "learning_rate": 1e-05,
      "loss": 6.2736,
      "step": 4320
    },
    {
      "epoch": 0.9365853658536586,
      "step": 4320,
      "training_loss": 6.295133590698242
    },
    {
      "epoch": 0.9365853658536586,
      "step": 4320,
      "training_loss": 7.092339515686035
    },
    {
      "epoch": 0.9365853658536586,
      "step": 4320,
      "training_loss": 6.107997417449951
    },
    {
      "epoch": 0.9365853658536586,
      "step": 4320,
      "training_loss": 5.136885166168213
    },
    {
      "epoch": 0.9368021680216803,
      "step": 4321,
      "training_loss": 6.180731296539307
    },
    {
      "epoch": 0.9368021680216803,
      "step": 4321,
      "training_loss": 4.93539571762085
    },
    {
      "epoch": 0.9368021680216803,
      "step": 4321,
      "training_loss": 6.416167736053467
    },
    {
      "epoch": 0.9368021680216803,
      "step": 4321,
      "training_loss": 6.77897834777832
    },
    {
      "epoch": 0.9370189701897019,
      "step": 4322,
      "training_loss": 5.774925231933594
    },
    {
      "epoch": 0.9370189701897019,
      "step": 4322,
      "training_loss": 5.05185604095459
    },
    {
      "epoch": 0.9370189701897019,
      "step": 4322,
      "training_loss": 6.134842395782471
    },
    {
      "epoch": 0.9370189701897019,
      "step": 4322,
      "training_loss": 7.2850236892700195
    },
    {
      "epoch": 0.9372357723577236,
      "step": 4323,
      "training_loss": 6.353615760803223
    },
    {
      "epoch": 0.9372357723577236,
      "step": 4323,
      "training_loss": 6.463906288146973
    },
    {
      "epoch": 0.9372357723577236,
      "step": 4323,
      "training_loss": 6.700358867645264
    },
    {
      "epoch": 0.9372357723577236,
      "step": 4323,
      "training_loss": 6.382401943206787
    },
    {
      "epoch": 0.9374525745257453,
      "grad_norm": 27.24639129638672,
      "learning_rate": 1e-05,
      "loss": 6.1932,
      "step": 4324
    },
    {
      "epoch": 0.9374525745257453,
      "step": 4324,
      "training_loss": 7.966909408569336
    },
    {
      "epoch": 0.9374525745257453,
      "step": 4324,
      "training_loss": 7.1053948402404785
    },
    {
      "epoch": 0.9374525745257453,
      "step": 4324,
      "training_loss": 6.878091812133789
    },
    {
      "epoch": 0.9374525745257453,
      "step": 4324,
      "training_loss": 7.566709518432617
    },
    {
      "epoch": 0.9376693766937669,
      "step": 4325,
      "training_loss": 6.317119121551514
    },
    {
      "epoch": 0.9376693766937669,
      "step": 4325,
      "training_loss": 7.188891410827637
    },
    {
      "epoch": 0.9376693766937669,
      "step": 4325,
      "training_loss": 4.66168737411499
    },
    {
      "epoch": 0.9376693766937669,
      "step": 4325,
      "training_loss": 7.346325397491455
    },
    {
      "epoch": 0.9378861788617886,
      "step": 4326,
      "training_loss": 4.739040851593018
    },
    {
      "epoch": 0.9378861788617886,
      "step": 4326,
      "training_loss": 6.476603031158447
    },
    {
      "epoch": 0.9378861788617886,
      "step": 4326,
      "training_loss": 4.792054176330566
    },
    {
      "epoch": 0.9378861788617886,
      "step": 4326,
      "training_loss": 5.752130508422852
    },
    {
      "epoch": 0.9381029810298103,
      "step": 4327,
      "training_loss": 5.8706583976745605
    },
    {
      "epoch": 0.9381029810298103,
      "step": 4327,
      "training_loss": 6.092560291290283
    },
    {
      "epoch": 0.9381029810298103,
      "step": 4327,
      "training_loss": 3.340254068374634
    },
    {
      "epoch": 0.9381029810298103,
      "step": 4327,
      "training_loss": 3.7053887844085693
    },
    {
      "epoch": 0.9383197831978319,
      "grad_norm": 18.009113311767578,
      "learning_rate": 1e-05,
      "loss": 5.9875,
      "step": 4328
    },
    {
      "epoch": 0.9383197831978319,
      "step": 4328,
      "training_loss": 7.5119781494140625
    },
    {
      "epoch": 0.9383197831978319,
      "step": 4328,
      "training_loss": 6.669107913970947
    },
    {
      "epoch": 0.9383197831978319,
      "step": 4328,
      "training_loss": 6.469182968139648
    },
    {
      "epoch": 0.9383197831978319,
      "step": 4328,
      "training_loss": 6.75767183303833
    },
    {
      "epoch": 0.9385365853658536,
      "step": 4329,
      "training_loss": 6.298855781555176
    },
    {
      "epoch": 0.9385365853658536,
      "step": 4329,
      "training_loss": 6.390439987182617
    },
    {
      "epoch": 0.9385365853658536,
      "step": 4329,
      "training_loss": 6.0405755043029785
    },
    {
      "epoch": 0.9385365853658536,
      "step": 4329,
      "training_loss": 6.5949578285217285
    },
    {
      "epoch": 0.9387533875338754,
      "step": 4330,
      "training_loss": 7.449045181274414
    },
    {
      "epoch": 0.9387533875338754,
      "step": 4330,
      "training_loss": 6.240105152130127
    },
    {
      "epoch": 0.9387533875338754,
      "step": 4330,
      "training_loss": 5.144585132598877
    },
    {
      "epoch": 0.9387533875338754,
      "step": 4330,
      "training_loss": 7.056631088256836
    },
    {
      "epoch": 0.938970189701897,
      "step": 4331,
      "training_loss": 5.860288619995117
    },
    {
      "epoch": 0.938970189701897,
      "step": 4331,
      "training_loss": 5.99617862701416
    },
    {
      "epoch": 0.938970189701897,
      "step": 4331,
      "training_loss": 5.50562858581543
    },
    {
      "epoch": 0.938970189701897,
      "step": 4331,
      "training_loss": 5.9313578605651855
    },
    {
      "epoch": 0.9391869918699187,
      "grad_norm": 18.875043869018555,
      "learning_rate": 1e-05,
      "loss": 6.3698,
      "step": 4332
    },
    {
      "epoch": 0.9391869918699187,
      "step": 4332,
      "training_loss": 7.38592004776001
    },
    {
      "epoch": 0.9391869918699187,
      "step": 4332,
      "training_loss": 7.206754684448242
    },
    {
      "epoch": 0.9391869918699187,
      "step": 4332,
      "training_loss": 5.406006336212158
    },
    {
      "epoch": 0.9391869918699187,
      "step": 4332,
      "training_loss": 7.546834945678711
    },
    {
      "epoch": 0.9394037940379404,
      "step": 4333,
      "training_loss": 5.690767288208008
    },
    {
      "epoch": 0.9394037940379404,
      "step": 4333,
      "training_loss": 5.936704158782959
    },
    {
      "epoch": 0.9394037940379404,
      "step": 4333,
      "training_loss": 6.87966775894165
    },
    {
      "epoch": 0.9394037940379404,
      "step": 4333,
      "training_loss": 6.425239086151123
    },
    {
      "epoch": 0.939620596205962,
      "step": 4334,
      "training_loss": 7.186429977416992
    },
    {
      "epoch": 0.939620596205962,
      "step": 4334,
      "training_loss": 7.459742546081543
    },
    {
      "epoch": 0.939620596205962,
      "step": 4334,
      "training_loss": 6.0597124099731445
    },
    {
      "epoch": 0.939620596205962,
      "step": 4334,
      "training_loss": 4.081934928894043
    },
    {
      "epoch": 0.9398373983739837,
      "step": 4335,
      "training_loss": 3.197387456893921
    },
    {
      "epoch": 0.9398373983739837,
      "step": 4335,
      "training_loss": 6.3493475914001465
    },
    {
      "epoch": 0.9398373983739837,
      "step": 4335,
      "training_loss": 6.175477504730225
    },
    {
      "epoch": 0.9398373983739837,
      "step": 4335,
      "training_loss": 6.4963297843933105
    },
    {
      "epoch": 0.9400542005420054,
      "grad_norm": 22.875051498413086,
      "learning_rate": 1e-05,
      "loss": 6.2178,
      "step": 4336
    },
    {
      "epoch": 0.9400542005420054,
      "step": 4336,
      "training_loss": 5.974875450134277
    },
    {
      "epoch": 0.9400542005420054,
      "step": 4336,
      "training_loss": 7.77155065536499
    },
    {
      "epoch": 0.9400542005420054,
      "step": 4336,
      "training_loss": 6.824794292449951
    },
    {
      "epoch": 0.9400542005420054,
      "step": 4336,
      "training_loss": 6.864011764526367
    },
    {
      "epoch": 0.9402710027100271,
      "step": 4337,
      "training_loss": 6.7517313957214355
    },
    {
      "epoch": 0.9402710027100271,
      "step": 4337,
      "training_loss": 6.785769939422607
    },
    {
      "epoch": 0.9402710027100271,
      "step": 4337,
      "training_loss": 6.200949192047119
    },
    {
      "epoch": 0.9402710027100271,
      "step": 4337,
      "training_loss": 7.224980354309082
    },
    {
      "epoch": 0.9404878048780487,
      "step": 4338,
      "training_loss": 8.418946266174316
    },
    {
      "epoch": 0.9404878048780487,
      "step": 4338,
      "training_loss": 6.836733341217041
    },
    {
      "epoch": 0.9404878048780487,
      "step": 4338,
      "training_loss": 6.823036193847656
    },
    {
      "epoch": 0.9404878048780487,
      "step": 4338,
      "training_loss": 7.469423770904541
    },
    {
      "epoch": 0.9407046070460705,
      "step": 4339,
      "training_loss": 7.212244033813477
    },
    {
      "epoch": 0.9407046070460705,
      "step": 4339,
      "training_loss": 6.0219011306762695
    },
    {
      "epoch": 0.9407046070460705,
      "step": 4339,
      "training_loss": 7.0979838371276855
    },
    {
      "epoch": 0.9407046070460705,
      "step": 4339,
      "training_loss": 6.344400405883789
    },
    {
      "epoch": 0.9409214092140922,
      "grad_norm": 16.543495178222656,
      "learning_rate": 1e-05,
      "loss": 6.914,
      "step": 4340
    },
    {
      "epoch": 0.9409214092140922,
      "step": 4340,
      "training_loss": 6.9691548347473145
    },
    {
      "epoch": 0.9409214092140922,
      "step": 4340,
      "training_loss": 6.631425380706787
    },
    {
      "epoch": 0.9409214092140922,
      "step": 4340,
      "training_loss": 5.666520595550537
    },
    {
      "epoch": 0.9409214092140922,
      "step": 4340,
      "training_loss": 6.692494869232178
    },
    {
      "epoch": 0.9411382113821138,
      "step": 4341,
      "training_loss": 5.82011079788208
    },
    {
      "epoch": 0.9411382113821138,
      "step": 4341,
      "training_loss": 6.079250812530518
    },
    {
      "epoch": 0.9411382113821138,
      "step": 4341,
      "training_loss": 6.878331661224365
    },
    {
      "epoch": 0.9411382113821138,
      "step": 4341,
      "training_loss": 7.075666427612305
    },
    {
      "epoch": 0.9413550135501355,
      "step": 4342,
      "training_loss": 6.9903178215026855
    },
    {
      "epoch": 0.9413550135501355,
      "step": 4342,
      "training_loss": 6.121764659881592
    },
    {
      "epoch": 0.9413550135501355,
      "step": 4342,
      "training_loss": 6.773350715637207
    },
    {
      "epoch": 0.9413550135501355,
      "step": 4342,
      "training_loss": 5.5638651847839355
    },
    {
      "epoch": 0.9415718157181572,
      "step": 4343,
      "training_loss": 8.049708366394043
    },
    {
      "epoch": 0.9415718157181572,
      "step": 4343,
      "training_loss": 7.042616367340088
    },
    {
      "epoch": 0.9415718157181572,
      "step": 4343,
      "training_loss": 5.64253044128418
    },
    {
      "epoch": 0.9415718157181572,
      "step": 4343,
      "training_loss": 6.85759162902832
    },
    {
      "epoch": 0.9417886178861788,
      "grad_norm": 20.838159561157227,
      "learning_rate": 1e-05,
      "loss": 6.5534,
      "step": 4344
    },
    {
      "epoch": 0.9417886178861788,
      "step": 4344,
      "training_loss": 6.953702449798584
    },
    {
      "epoch": 0.9417886178861788,
      "step": 4344,
      "training_loss": 6.3498640060424805
    },
    {
      "epoch": 0.9417886178861788,
      "step": 4344,
      "training_loss": 6.883084774017334
    },
    {
      "epoch": 0.9417886178861788,
      "step": 4344,
      "training_loss": 6.1041460037231445
    },
    {
      "epoch": 0.9420054200542005,
      "step": 4345,
      "training_loss": 4.698155879974365
    },
    {
      "epoch": 0.9420054200542005,
      "step": 4345,
      "training_loss": 4.818936824798584
    },
    {
      "epoch": 0.9420054200542005,
      "step": 4345,
      "training_loss": 6.634598255157471
    },
    {
      "epoch": 0.9420054200542005,
      "step": 4345,
      "training_loss": 5.307726860046387
    },
    {
      "epoch": 0.9422222222222222,
      "step": 4346,
      "training_loss": 6.686233043670654
    },
    {
      "epoch": 0.9422222222222222,
      "step": 4346,
      "training_loss": 7.96529483795166
    },
    {
      "epoch": 0.9422222222222222,
      "step": 4346,
      "training_loss": 5.732542037963867
    },
    {
      "epoch": 0.9422222222222222,
      "step": 4346,
      "training_loss": 6.080560684204102
    },
    {
      "epoch": 0.9424390243902439,
      "step": 4347,
      "training_loss": 6.72433614730835
    },
    {
      "epoch": 0.9424390243902439,
      "step": 4347,
      "training_loss": 4.9683942794799805
    },
    {
      "epoch": 0.9424390243902439,
      "step": 4347,
      "training_loss": 6.81620454788208
    },
    {
      "epoch": 0.9424390243902439,
      "step": 4347,
      "training_loss": 6.835484027862549
    },
    {
      "epoch": 0.9426558265582656,
      "grad_norm": 17.362749099731445,
      "learning_rate": 1e-05,
      "loss": 6.2225,
      "step": 4348
    },
    {
      "epoch": 0.9426558265582656,
      "step": 4348,
      "training_loss": 6.5313639640808105
    },
    {
      "epoch": 0.9426558265582656,
      "step": 4348,
      "training_loss": 5.527961254119873
    },
    {
      "epoch": 0.9426558265582656,
      "step": 4348,
      "training_loss": 7.118870258331299
    },
    {
      "epoch": 0.9426558265582656,
      "step": 4348,
      "training_loss": 6.85736083984375
    },
    {
      "epoch": 0.9428726287262873,
      "step": 4349,
      "training_loss": 6.279880046844482
    },
    {
      "epoch": 0.9428726287262873,
      "step": 4349,
      "training_loss": 7.850570201873779
    },
    {
      "epoch": 0.9428726287262873,
      "step": 4349,
      "training_loss": 7.868442535400391
    },
    {
      "epoch": 0.9428726287262873,
      "step": 4349,
      "training_loss": 5.9477643966674805
    },
    {
      "epoch": 0.943089430894309,
      "step": 4350,
      "training_loss": 6.845125675201416
    },
    {
      "epoch": 0.943089430894309,
      "step": 4350,
      "training_loss": 5.359255790710449
    },
    {
      "epoch": 0.943089430894309,
      "step": 4350,
      "training_loss": 6.681480884552002
    },
    {
      "epoch": 0.943089430894309,
      "step": 4350,
      "training_loss": 5.4488420486450195
    },
    {
      "epoch": 0.9433062330623306,
      "step": 4351,
      "training_loss": 4.945539951324463
    },
    {
      "epoch": 0.9433062330623306,
      "step": 4351,
      "training_loss": 5.698253154754639
    },
    {
      "epoch": 0.9433062330623306,
      "step": 4351,
      "training_loss": 6.903937816619873
    },
    {
      "epoch": 0.9433062330623306,
      "step": 4351,
      "training_loss": 5.670902729034424
    },
    {
      "epoch": 0.9435230352303523,
      "grad_norm": 19.994844436645508,
      "learning_rate": 1e-05,
      "loss": 6.346,
      "step": 4352
    },
    {
      "epoch": 0.9435230352303523,
      "step": 4352,
      "training_loss": 6.99770975112915
    },
    {
      "epoch": 0.9435230352303523,
      "step": 4352,
      "training_loss": 3.584608793258667
    },
    {
      "epoch": 0.9435230352303523,
      "step": 4352,
      "training_loss": 7.362053871154785
    },
    {
      "epoch": 0.9435230352303523,
      "step": 4352,
      "training_loss": 6.014673233032227
    },
    {
      "epoch": 0.943739837398374,
      "step": 4353,
      "training_loss": 6.7346577644348145
    },
    {
      "epoch": 0.943739837398374,
      "step": 4353,
      "training_loss": 6.279465198516846
    },
    {
      "epoch": 0.943739837398374,
      "step": 4353,
      "training_loss": 6.770294189453125
    },
    {
      "epoch": 0.943739837398374,
      "step": 4353,
      "training_loss": 6.1678571701049805
    },
    {
      "epoch": 0.9439566395663956,
      "step": 4354,
      "training_loss": 6.2874860763549805
    },
    {
      "epoch": 0.9439566395663956,
      "step": 4354,
      "training_loss": 3.9487953186035156
    },
    {
      "epoch": 0.9439566395663956,
      "step": 4354,
      "training_loss": 4.792482376098633
    },
    {
      "epoch": 0.9439566395663956,
      "step": 4354,
      "training_loss": 5.285178184509277
    },
    {
      "epoch": 0.9441734417344173,
      "step": 4355,
      "training_loss": 6.345606803894043
    },
    {
      "epoch": 0.9441734417344173,
      "step": 4355,
      "training_loss": 7.320324420928955
    },
    {
      "epoch": 0.9441734417344173,
      "step": 4355,
      "training_loss": 6.016455173492432
    },
    {
      "epoch": 0.9441734417344173,
      "step": 4355,
      "training_loss": 3.0690903663635254
    },
    {
      "epoch": 0.944390243902439,
      "grad_norm": 18.11939239501953,
      "learning_rate": 1e-05,
      "loss": 5.811,
      "step": 4356
    },
    {
      "epoch": 0.944390243902439,
      "step": 4356,
      "training_loss": 6.778397083282471
    },
    {
      "epoch": 0.944390243902439,
      "step": 4356,
      "training_loss": 4.642310619354248
    },
    {
      "epoch": 0.944390243902439,
      "step": 4356,
      "training_loss": 5.174212455749512
    },
    {
      "epoch": 0.944390243902439,
      "step": 4356,
      "training_loss": 6.946944713592529
    },
    {
      "epoch": 0.9446070460704608,
      "step": 4357,
      "training_loss": 7.906623840332031
    },
    {
      "epoch": 0.9446070460704608,
      "step": 4357,
      "training_loss": 5.622469902038574
    },
    {
      "epoch": 0.9446070460704608,
      "step": 4357,
      "training_loss": 6.339692115783691
    },
    {
      "epoch": 0.9446070460704608,
      "step": 4357,
      "training_loss": 7.214267253875732
    },
    {
      "epoch": 0.9448238482384824,
      "step": 4358,
      "training_loss": 7.656498432159424
    },
    {
      "epoch": 0.9448238482384824,
      "step": 4358,
      "training_loss": 4.348291397094727
    },
    {
      "epoch": 0.9448238482384824,
      "step": 4358,
      "training_loss": 6.997874736785889
    },
    {
      "epoch": 0.9448238482384824,
      "step": 4358,
      "training_loss": 6.428538799285889
    },
    {
      "epoch": 0.9450406504065041,
      "step": 4359,
      "training_loss": 6.930564880371094
    },
    {
      "epoch": 0.9450406504065041,
      "step": 4359,
      "training_loss": 7.661830425262451
    },
    {
      "epoch": 0.9450406504065041,
      "step": 4359,
      "training_loss": 4.737460613250732
    },
    {
      "epoch": 0.9450406504065041,
      "step": 4359,
      "training_loss": 6.847109794616699
    },
    {
      "epoch": 0.9452574525745258,
      "grad_norm": 22.2617244720459,
      "learning_rate": 1e-05,
      "loss": 6.3896,
      "step": 4360
    },
    {
      "epoch": 0.9452574525745258,
      "step": 4360,
      "training_loss": 6.768829822540283
    },
    {
      "epoch": 0.9452574525745258,
      "step": 4360,
      "training_loss": 6.3805155754089355
    },
    {
      "epoch": 0.9452574525745258,
      "step": 4360,
      "training_loss": 7.9782562255859375
    },
    {
      "epoch": 0.9452574525745258,
      "step": 4360,
      "training_loss": 7.341477394104004
    },
    {
      "epoch": 0.9454742547425474,
      "step": 4361,
      "training_loss": 7.219301700592041
    },
    {
      "epoch": 0.9454742547425474,
      "step": 4361,
      "training_loss": 7.050457954406738
    },
    {
      "epoch": 0.9454742547425474,
      "step": 4361,
      "training_loss": 4.645730018615723
    },
    {
      "epoch": 0.9454742547425474,
      "step": 4361,
      "training_loss": 3.8818047046661377
    },
    {
      "epoch": 0.9456910569105691,
      "step": 4362,
      "training_loss": 7.756267070770264
    },
    {
      "epoch": 0.9456910569105691,
      "step": 4362,
      "training_loss": 6.2200236320495605
    },
    {
      "epoch": 0.9456910569105691,
      "step": 4362,
      "training_loss": 6.5356340408325195
    },
    {
      "epoch": 0.9456910569105691,
      "step": 4362,
      "training_loss": 5.913930416107178
    },
    {
      "epoch": 0.9459078590785908,
      "step": 4363,
      "training_loss": 6.622574806213379
    },
    {
      "epoch": 0.9459078590785908,
      "step": 4363,
      "training_loss": 6.43863582611084
    },
    {
      "epoch": 0.9459078590785908,
      "step": 4363,
      "training_loss": 5.478468418121338
    },
    {
      "epoch": 0.9459078590785908,
      "step": 4363,
      "training_loss": 7.190338611602783
    },
    {
      "epoch": 0.9461246612466124,
      "grad_norm": 15.392295837402344,
      "learning_rate": 1e-05,
      "loss": 6.4639,
      "step": 4364
    },
    {
      "epoch": 0.9461246612466124,
      "step": 4364,
      "training_loss": 6.3718647956848145
    },
    {
      "epoch": 0.9461246612466124,
      "step": 4364,
      "training_loss": 7.300025463104248
    },
    {
      "epoch": 0.9461246612466124,
      "step": 4364,
      "training_loss": 6.206905841827393
    },
    {
      "epoch": 0.9461246612466124,
      "step": 4364,
      "training_loss": 3.346585988998413
    },
    {
      "epoch": 0.9463414634146341,
      "step": 4365,
      "training_loss": 7.186653137207031
    },
    {
      "epoch": 0.9463414634146341,
      "step": 4365,
      "training_loss": 6.792270660400391
    },
    {
      "epoch": 0.9463414634146341,
      "step": 4365,
      "training_loss": 6.80551815032959
    },
    {
      "epoch": 0.9463414634146341,
      "step": 4365,
      "training_loss": 6.658795356750488
    },
    {
      "epoch": 0.9465582655826559,
      "step": 4366,
      "training_loss": 6.858003616333008
    },
    {
      "epoch": 0.9465582655826559,
      "step": 4366,
      "training_loss": 7.296626091003418
    },
    {
      "epoch": 0.9465582655826559,
      "step": 4366,
      "training_loss": 6.7393293380737305
    },
    {
      "epoch": 0.9465582655826559,
      "step": 4366,
      "training_loss": 6.606204032897949
    },
    {
      "epoch": 0.9467750677506775,
      "step": 4367,
      "training_loss": 7.011288166046143
    },
    {
      "epoch": 0.9467750677506775,
      "step": 4367,
      "training_loss": 5.543829917907715
    },
    {
      "epoch": 0.9467750677506775,
      "step": 4367,
      "training_loss": 7.023166179656982
    },
    {
      "epoch": 0.9467750677506775,
      "step": 4367,
      "training_loss": 4.9158759117126465
    },
    {
      "epoch": 0.9469918699186992,
      "grad_norm": 16.89325523376465,
      "learning_rate": 1e-05,
      "loss": 6.4164,
      "step": 4368
    },
    {
      "epoch": 0.9469918699186992,
      "step": 4368,
      "training_loss": 5.5340728759765625
    },
    {
      "epoch": 0.9469918699186992,
      "step": 4368,
      "training_loss": 7.325528621673584
    },
    {
      "epoch": 0.9469918699186992,
      "step": 4368,
      "training_loss": 6.0670881271362305
    },
    {
      "epoch": 0.9469918699186992,
      "step": 4368,
      "training_loss": 6.786035060882568
    },
    {
      "epoch": 0.9472086720867209,
      "step": 4369,
      "training_loss": 5.9545416831970215
    },
    {
      "epoch": 0.9472086720867209,
      "step": 4369,
      "training_loss": 3.3260817527770996
    },
    {
      "epoch": 0.9472086720867209,
      "step": 4369,
      "training_loss": 7.325342655181885
    },
    {
      "epoch": 0.9472086720867209,
      "step": 4369,
      "training_loss": 6.28743839263916
    },
    {
      "epoch": 0.9474254742547426,
      "step": 4370,
      "training_loss": 6.854732990264893
    },
    {
      "epoch": 0.9474254742547426,
      "step": 4370,
      "training_loss": 6.645165920257568
    },
    {
      "epoch": 0.9474254742547426,
      "step": 4370,
      "training_loss": 6.889537811279297
    },
    {
      "epoch": 0.9474254742547426,
      "step": 4370,
      "training_loss": 3.0818607807159424
    },
    {
      "epoch": 0.9476422764227642,
      "step": 4371,
      "training_loss": 5.850799083709717
    },
    {
      "epoch": 0.9476422764227642,
      "step": 4371,
      "training_loss": 7.582902431488037
    },
    {
      "epoch": 0.9476422764227642,
      "step": 4371,
      "training_loss": 7.013174533843994
    },
    {
      "epoch": 0.9476422764227642,
      "step": 4371,
      "training_loss": 8.470219612121582
    },
    {
      "epoch": 0.9478590785907859,
      "grad_norm": 25.108964920043945,
      "learning_rate": 1e-05,
      "loss": 6.3122,
      "step": 4372
    },
    {
      "epoch": 0.9478590785907859,
      "step": 4372,
      "training_loss": 6.281429290771484
    },
    {
      "epoch": 0.9478590785907859,
      "step": 4372,
      "training_loss": 5.141749858856201
    },
    {
      "epoch": 0.9478590785907859,
      "step": 4372,
      "training_loss": 6.417581558227539
    },
    {
      "epoch": 0.9478590785907859,
      "step": 4372,
      "training_loss": 3.192396879196167
    },
    {
      "epoch": 0.9480758807588076,
      "step": 4373,
      "training_loss": 6.426098823547363
    },
    {
      "epoch": 0.9480758807588076,
      "step": 4373,
      "training_loss": 6.353224277496338
    },
    {
      "epoch": 0.9480758807588076,
      "step": 4373,
      "training_loss": 5.3282060623168945
    },
    {
      "epoch": 0.9480758807588076,
      "step": 4373,
      "training_loss": 6.522359371185303
    },
    {
      "epoch": 0.9482926829268292,
      "step": 4374,
      "training_loss": 5.607708930969238
    },
    {
      "epoch": 0.9482926829268292,
      "step": 4374,
      "training_loss": 6.321325778961182
    },
    {
      "epoch": 0.9482926829268292,
      "step": 4374,
      "training_loss": 7.168000221252441
    },
    {
      "epoch": 0.9482926829268292,
      "step": 4374,
      "training_loss": 6.830089569091797
    },
    {
      "epoch": 0.948509485094851,
      "step": 4375,
      "training_loss": 6.844003200531006
    },
    {
      "epoch": 0.948509485094851,
      "step": 4375,
      "training_loss": 5.73692512512207
    },
    {
      "epoch": 0.948509485094851,
      "step": 4375,
      "training_loss": 6.732606887817383
    },
    {
      "epoch": 0.948509485094851,
      "step": 4375,
      "training_loss": 6.272238731384277
    },
    {
      "epoch": 0.9487262872628727,
      "grad_norm": 18.492109298706055,
      "learning_rate": 1e-05,
      "loss": 6.0735,
      "step": 4376
    },
    {
      "epoch": 0.9487262872628727,
      "step": 4376,
      "training_loss": 6.162537097930908
    },
    {
      "epoch": 0.9487262872628727,
      "step": 4376,
      "training_loss": 4.181253910064697
    },
    {
      "epoch": 0.9487262872628727,
      "step": 4376,
      "training_loss": 6.353506565093994
    },
    {
      "epoch": 0.9487262872628727,
      "step": 4376,
      "training_loss": 7.397559642791748
    },
    {
      "epoch": 0.9489430894308943,
      "step": 4377,
      "training_loss": 4.989758491516113
    },
    {
      "epoch": 0.9489430894308943,
      "step": 4377,
      "training_loss": 5.873950481414795
    },
    {
      "epoch": 0.9489430894308943,
      "step": 4377,
      "training_loss": 7.1934943199157715
    },
    {
      "epoch": 0.9489430894308943,
      "step": 4377,
      "training_loss": 5.510407447814941
    },
    {
      "epoch": 0.949159891598916,
      "step": 4378,
      "training_loss": 7.467483997344971
    },
    {
      "epoch": 0.949159891598916,
      "step": 4378,
      "training_loss": 7.7860331535339355
    },
    {
      "epoch": 0.949159891598916,
      "step": 4378,
      "training_loss": 6.358170032501221
    },
    {
      "epoch": 0.949159891598916,
      "step": 4378,
      "training_loss": 6.337348937988281
    },
    {
      "epoch": 0.9493766937669377,
      "step": 4379,
      "training_loss": 6.100272178649902
    },
    {
      "epoch": 0.9493766937669377,
      "step": 4379,
      "training_loss": 6.656844615936279
    },
    {
      "epoch": 0.9493766937669377,
      "step": 4379,
      "training_loss": 6.201950550079346
    },
    {
      "epoch": 0.9493766937669377,
      "step": 4379,
      "training_loss": 6.886720657348633
    },
    {
      "epoch": 0.9495934959349593,
      "grad_norm": 16.050264358520508,
      "learning_rate": 1e-05,
      "loss": 6.3411,
      "step": 4380
    },
    {
      "epoch": 0.9495934959349593,
      "step": 4380,
      "training_loss": 8.721397399902344
    },
    {
      "epoch": 0.9495934959349593,
      "step": 4380,
      "training_loss": 7.060261249542236
    },
    {
      "epoch": 0.9495934959349593,
      "step": 4380,
      "training_loss": 5.55925178527832
    },
    {
      "epoch": 0.9495934959349593,
      "step": 4380,
      "training_loss": 6.102953910827637
    },
    {
      "epoch": 0.949810298102981,
      "step": 4381,
      "training_loss": 5.866915702819824
    },
    {
      "epoch": 0.949810298102981,
      "step": 4381,
      "training_loss": 6.2887396812438965
    },
    {
      "epoch": 0.949810298102981,
      "step": 4381,
      "training_loss": 6.447799205780029
    },
    {
      "epoch": 0.949810298102981,
      "step": 4381,
      "training_loss": 6.633256435394287
    },
    {
      "epoch": 0.9500271002710027,
      "step": 4382,
      "training_loss": 5.803156852722168
    },
    {
      "epoch": 0.9500271002710027,
      "step": 4382,
      "training_loss": 7.084866523742676
    },
    {
      "epoch": 0.9500271002710027,
      "step": 4382,
      "training_loss": 6.315245151519775
    },
    {
      "epoch": 0.9500271002710027,
      "step": 4382,
      "training_loss": 6.796444416046143
    },
    {
      "epoch": 0.9502439024390243,
      "step": 4383,
      "training_loss": 6.360950469970703
    },
    {
      "epoch": 0.9502439024390243,
      "step": 4383,
      "training_loss": 7.825099945068359
    },
    {
      "epoch": 0.9502439024390243,
      "step": 4383,
      "training_loss": 5.777848720550537
    },
    {
      "epoch": 0.9502439024390243,
      "step": 4383,
      "training_loss": 7.143951416015625
    },
    {
      "epoch": 0.9504607046070461,
      "grad_norm": 17.423301696777344,
      "learning_rate": 1e-05,
      "loss": 6.6118,
      "step": 4384
    },
    {
      "epoch": 0.9504607046070461,
      "step": 4384,
      "training_loss": 6.646780490875244
    },
    {
      "epoch": 0.9504607046070461,
      "step": 4384,
      "training_loss": 7.508053779602051
    },
    {
      "epoch": 0.9504607046070461,
      "step": 4384,
      "training_loss": 5.01226282119751
    },
    {
      "epoch": 0.9504607046070461,
      "step": 4384,
      "training_loss": 7.349143981933594
    },
    {
      "epoch": 0.9506775067750678,
      "step": 4385,
      "training_loss": 7.024158000946045
    },
    {
      "epoch": 0.9506775067750678,
      "step": 4385,
      "training_loss": 6.086643218994141
    },
    {
      "epoch": 0.9506775067750678,
      "step": 4385,
      "training_loss": 6.931123733520508
    },
    {
      "epoch": 0.9506775067750678,
      "step": 4385,
      "training_loss": 7.16996955871582
    },
    {
      "epoch": 0.9508943089430895,
      "step": 4386,
      "training_loss": 6.767887592315674
    },
    {
      "epoch": 0.9508943089430895,
      "step": 4386,
      "training_loss": 5.912493705749512
    },
    {
      "epoch": 0.9508943089430895,
      "step": 4386,
      "training_loss": 6.906533241271973
    },
    {
      "epoch": 0.9508943089430895,
      "step": 4386,
      "training_loss": 6.447342395782471
    },
    {
      "epoch": 0.9511111111111111,
      "step": 4387,
      "training_loss": 7.04949426651001
    },
    {
      "epoch": 0.9511111111111111,
      "step": 4387,
      "training_loss": 6.75354528427124
    },
    {
      "epoch": 0.9511111111111111,
      "step": 4387,
      "training_loss": 7.363255023956299
    },
    {
      "epoch": 0.9511111111111111,
      "step": 4387,
      "training_loss": 7.786217212677002
    },
    {
      "epoch": 0.9513279132791328,
      "grad_norm": 15.784285545349121,
      "learning_rate": 1e-05,
      "loss": 6.7947,
      "step": 4388
    },
    {
      "epoch": 0.9513279132791328,
      "step": 4388,
      "training_loss": 5.975189208984375
    },
    {
      "epoch": 0.9513279132791328,
      "step": 4388,
      "training_loss": 5.3666276931762695
    },
    {
      "epoch": 0.9513279132791328,
      "step": 4388,
      "training_loss": 3.8096494674682617
    },
    {
      "epoch": 0.9513279132791328,
      "step": 4388,
      "training_loss": 5.803965091705322
    },
    {
      "epoch": 0.9515447154471545,
      "step": 4389,
      "training_loss": 5.752563953399658
    },
    {
      "epoch": 0.9515447154471545,
      "step": 4389,
      "training_loss": 6.523022651672363
    },
    {
      "epoch": 0.9515447154471545,
      "step": 4389,
      "training_loss": 6.969297409057617
    },
    {
      "epoch": 0.9515447154471545,
      "step": 4389,
      "training_loss": 6.443517684936523
    },
    {
      "epoch": 0.9517615176151761,
      "step": 4390,
      "training_loss": 7.5050458908081055
    },
    {
      "epoch": 0.9517615176151761,
      "step": 4390,
      "training_loss": 7.437037944793701
    },
    {
      "epoch": 0.9517615176151761,
      "step": 4390,
      "training_loss": 7.390954971313477
    },
    {
      "epoch": 0.9517615176151761,
      "step": 4390,
      "training_loss": 6.828707695007324
    },
    {
      "epoch": 0.9519783197831978,
      "step": 4391,
      "training_loss": 7.251837253570557
    },
    {
      "epoch": 0.9519783197831978,
      "step": 4391,
      "training_loss": 6.1510844230651855
    },
    {
      "epoch": 0.9519783197831978,
      "step": 4391,
      "training_loss": 7.31427001953125
    },
    {
      "epoch": 0.9519783197831978,
      "step": 4391,
      "training_loss": 7.82954740524292
    },
    {
      "epoch": 0.9521951219512195,
      "grad_norm": 13.004815101623535,
      "learning_rate": 1e-05,
      "loss": 6.522,
      "step": 4392
    },
    {
      "epoch": 0.9521951219512195,
      "step": 4392,
      "training_loss": 6.387333869934082
    },
    {
      "epoch": 0.9521951219512195,
      "step": 4392,
      "training_loss": 5.6687164306640625
    },
    {
      "epoch": 0.9521951219512195,
      "step": 4392,
      "training_loss": 7.474567413330078
    },
    {
      "epoch": 0.9521951219512195,
      "step": 4392,
      "training_loss": 6.366411209106445
    },
    {
      "epoch": 0.9524119241192411,
      "step": 4393,
      "training_loss": 4.463327407836914
    },
    {
      "epoch": 0.9524119241192411,
      "step": 4393,
      "training_loss": 6.4342193603515625
    },
    {
      "epoch": 0.9524119241192411,
      "step": 4393,
      "training_loss": 5.906416893005371
    },
    {
      "epoch": 0.9524119241192411,
      "step": 4393,
      "training_loss": 7.129884243011475
    },
    {
      "epoch": 0.9526287262872629,
      "step": 4394,
      "training_loss": 4.83681583404541
    },
    {
      "epoch": 0.9526287262872629,
      "step": 4394,
      "training_loss": 6.757066249847412
    },
    {
      "epoch": 0.9526287262872629,
      "step": 4394,
      "training_loss": 6.994844436645508
    },
    {
      "epoch": 0.9526287262872629,
      "step": 4394,
      "training_loss": 6.143500804901123
    },
    {
      "epoch": 0.9528455284552846,
      "step": 4395,
      "training_loss": 5.148683547973633
    },
    {
      "epoch": 0.9528455284552846,
      "step": 4395,
      "training_loss": 7.325941562652588
    },
    {
      "epoch": 0.9528455284552846,
      "step": 4395,
      "training_loss": 6.008329391479492
    },
    {
      "epoch": 0.9528455284552846,
      "step": 4395,
      "training_loss": 5.651482105255127
    },
    {
      "epoch": 0.9530623306233063,
      "grad_norm": 18.229244232177734,
      "learning_rate": 1e-05,
      "loss": 6.1686,
      "step": 4396
    },
    {
      "epoch": 0.9530623306233063,
      "step": 4396,
      "training_loss": 6.359617710113525
    },
    {
      "epoch": 0.9530623306233063,
      "step": 4396,
      "training_loss": 6.523313522338867
    },
    {
      "epoch": 0.9530623306233063,
      "step": 4396,
      "training_loss": 5.081190586090088
    },
    {
      "epoch": 0.9530623306233063,
      "step": 4396,
      "training_loss": 7.9976630210876465
    },
    {
      "epoch": 0.9532791327913279,
      "step": 4397,
      "training_loss": 7.496802806854248
    },
    {
      "epoch": 0.9532791327913279,
      "step": 4397,
      "training_loss": 5.8781561851501465
    },
    {
      "epoch": 0.9532791327913279,
      "step": 4397,
      "training_loss": 5.477052211761475
    },
    {
      "epoch": 0.9532791327913279,
      "step": 4397,
      "training_loss": 6.353512763977051
    },
    {
      "epoch": 0.9534959349593496,
      "step": 4398,
      "training_loss": 6.891701698303223
    },
    {
      "epoch": 0.9534959349593496,
      "step": 4398,
      "training_loss": 7.393789768218994
    },
    {
      "epoch": 0.9534959349593496,
      "step": 4398,
      "training_loss": 6.787088394165039
    },
    {
      "epoch": 0.9534959349593496,
      "step": 4398,
      "training_loss": 6.767736434936523
    },
    {
      "epoch": 0.9537127371273713,
      "step": 4399,
      "training_loss": 4.882214546203613
    },
    {
      "epoch": 0.9537127371273713,
      "step": 4399,
      "training_loss": 4.801107883453369
    },
    {
      "epoch": 0.9537127371273713,
      "step": 4399,
      "training_loss": 3.5448215007781982
    },
    {
      "epoch": 0.9537127371273713,
      "step": 4399,
      "training_loss": 3.7686541080474854
    },
    {
      "epoch": 0.9539295392953929,
      "grad_norm": 24.194791793823242,
      "learning_rate": 1e-05,
      "loss": 6.0003,
      "step": 4400
    },
    {
      "epoch": 0.9539295392953929,
      "step": 4400,
      "training_loss": 7.999790668487549
    },
    {
      "epoch": 0.9539295392953929,
      "step": 4400,
      "training_loss": 4.152231216430664
    },
    {
      "epoch": 0.9539295392953929,
      "step": 4400,
      "training_loss": 7.03935432434082
    },
    {
      "epoch": 0.9539295392953929,
      "step": 4400,
      "training_loss": 6.996597766876221
    },
    {
      "epoch": 0.9541463414634146,
      "step": 4401,
      "training_loss": 7.778014183044434
    },
    {
      "epoch": 0.9541463414634146,
      "step": 4401,
      "training_loss": 6.159132957458496
    },
    {
      "epoch": 0.9541463414634146,
      "step": 4401,
      "training_loss": 4.878674507141113
    },
    {
      "epoch": 0.9541463414634146,
      "step": 4401,
      "training_loss": 7.680752277374268
    },
    {
      "epoch": 0.9543631436314363,
      "step": 4402,
      "training_loss": 6.158412456512451
    },
    {
      "epoch": 0.9543631436314363,
      "step": 4402,
      "training_loss": 7.791895389556885
    },
    {
      "epoch": 0.9543631436314363,
      "step": 4402,
      "training_loss": 7.603625774383545
    },
    {
      "epoch": 0.9543631436314363,
      "step": 4402,
      "training_loss": 6.782487869262695
    },
    {
      "epoch": 0.954579945799458,
      "step": 4403,
      "training_loss": 7.53246545791626
    },
    {
      "epoch": 0.954579945799458,
      "step": 4403,
      "training_loss": 5.672668933868408
    },
    {
      "epoch": 0.954579945799458,
      "step": 4403,
      "training_loss": 5.982614994049072
    },
    {
      "epoch": 0.954579945799458,
      "step": 4403,
      "training_loss": 7.895190238952637
    },
    {
      "epoch": 0.9547967479674797,
      "grad_norm": 17.47873878479004,
      "learning_rate": 1e-05,
      "loss": 6.7565,
      "step": 4404
    },
    {
      "epoch": 0.9547967479674797,
      "step": 4404,
      "training_loss": 6.064988136291504
    },
    {
      "epoch": 0.9547967479674797,
      "step": 4404,
      "training_loss": 6.349429607391357
    },
    {
      "epoch": 0.9547967479674797,
      "step": 4404,
      "training_loss": 6.030640125274658
    },
    {
      "epoch": 0.9547967479674797,
      "step": 4404,
      "training_loss": 6.679362773895264
    },
    {
      "epoch": 0.9550135501355014,
      "step": 4405,
      "training_loss": 5.927253723144531
    },
    {
      "epoch": 0.9550135501355014,
      "step": 4405,
      "training_loss": 6.913328170776367
    },
    {
      "epoch": 0.9550135501355014,
      "step": 4405,
      "training_loss": 6.471949577331543
    },
    {
      "epoch": 0.9550135501355014,
      "step": 4405,
      "training_loss": 6.291038513183594
    },
    {
      "epoch": 0.955230352303523,
      "step": 4406,
      "training_loss": 7.1844258308410645
    },
    {
      "epoch": 0.955230352303523,
      "step": 4406,
      "training_loss": 7.175926685333252
    },
    {
      "epoch": 0.955230352303523,
      "step": 4406,
      "training_loss": 7.353531360626221
    },
    {
      "epoch": 0.955230352303523,
      "step": 4406,
      "training_loss": 7.421868801116943
    },
    {
      "epoch": 0.9554471544715447,
      "step": 4407,
      "training_loss": 5.077985763549805
    },
    {
      "epoch": 0.9554471544715447,
      "step": 4407,
      "training_loss": 5.802592754364014
    },
    {
      "epoch": 0.9554471544715447,
      "step": 4407,
      "training_loss": 6.832381725311279
    },
    {
      "epoch": 0.9554471544715447,
      "step": 4407,
      "training_loss": 6.308568477630615
    },
    {
      "epoch": 0.9556639566395664,
      "grad_norm": 16.202699661254883,
      "learning_rate": 1e-05,
      "loss": 6.4928,
      "step": 4408
    },
    {
      "epoch": 0.9556639566395664,
      "step": 4408,
      "training_loss": 4.468739986419678
    },
    {
      "epoch": 0.9556639566395664,
      "step": 4408,
      "training_loss": 6.372982501983643
    },
    {
      "epoch": 0.9556639566395664,
      "step": 4408,
      "training_loss": 3.193075180053711
    },
    {
      "epoch": 0.9556639566395664,
      "step": 4408,
      "training_loss": 6.434595584869385
    },
    {
      "epoch": 0.955880758807588,
      "step": 4409,
      "training_loss": 6.5047287940979
    },
    {
      "epoch": 0.955880758807588,
      "step": 4409,
      "training_loss": 3.8564834594726562
    },
    {
      "epoch": 0.955880758807588,
      "step": 4409,
      "training_loss": 6.5826096534729
    },
    {
      "epoch": 0.955880758807588,
      "step": 4409,
      "training_loss": 5.608777046203613
    },
    {
      "epoch": 0.9560975609756097,
      "step": 4410,
      "training_loss": 5.08368444442749
    },
    {
      "epoch": 0.9560975609756097,
      "step": 4410,
      "training_loss": 2.9935853481292725
    },
    {
      "epoch": 0.9560975609756097,
      "step": 4410,
      "training_loss": 6.665970802307129
    },
    {
      "epoch": 0.9560975609756097,
      "step": 4410,
      "training_loss": 6.073276519775391
    },
    {
      "epoch": 0.9563143631436314,
      "step": 4411,
      "training_loss": 7.310791969299316
    },
    {
      "epoch": 0.9563143631436314,
      "step": 4411,
      "training_loss": 6.006918907165527
    },
    {
      "epoch": 0.9563143631436314,
      "step": 4411,
      "training_loss": 3.6765403747558594
    },
    {
      "epoch": 0.9563143631436314,
      "step": 4411,
      "training_loss": 5.253010272979736
    },
    {
      "epoch": 0.9565311653116532,
      "grad_norm": 20.799205780029297,
      "learning_rate": 1e-05,
      "loss": 5.3804,
      "step": 4412
    },
    {
      "epoch": 0.9565311653116532,
      "step": 4412,
      "training_loss": 5.008391857147217
    },
    {
      "epoch": 0.9565311653116532,
      "step": 4412,
      "training_loss": 6.988759517669678
    },
    {
      "epoch": 0.9565311653116532,
      "step": 4412,
      "training_loss": 8.068689346313477
    },
    {
      "epoch": 0.9565311653116532,
      "step": 4412,
      "training_loss": 5.7177276611328125
    },
    {
      "epoch": 0.9567479674796748,
      "step": 4413,
      "training_loss": 6.651889801025391
    },
    {
      "epoch": 0.9567479674796748,
      "step": 4413,
      "training_loss": 6.078577041625977
    },
    {
      "epoch": 0.9567479674796748,
      "step": 4413,
      "training_loss": 6.059261322021484
    },
    {
      "epoch": 0.9567479674796748,
      "step": 4413,
      "training_loss": 4.799308776855469
    },
    {
      "epoch": 0.9569647696476965,
      "step": 4414,
      "training_loss": 7.157513618469238
    },
    {
      "epoch": 0.9569647696476965,
      "step": 4414,
      "training_loss": 7.132123947143555
    },
    {
      "epoch": 0.9569647696476965,
      "step": 4414,
      "training_loss": 5.395978927612305
    },
    {
      "epoch": 0.9569647696476965,
      "step": 4414,
      "training_loss": 4.5961480140686035
    },
    {
      "epoch": 0.9571815718157182,
      "step": 4415,
      "training_loss": 7.480215549468994
    },
    {
      "epoch": 0.9571815718157182,
      "step": 4415,
      "training_loss": 6.825652122497559
    },
    {
      "epoch": 0.9571815718157182,
      "step": 4415,
      "training_loss": 5.759645938873291
    },
    {
      "epoch": 0.9571815718157182,
      "step": 4415,
      "training_loss": 4.012781620025635
    },
    {
      "epoch": 0.9573983739837398,
      "grad_norm": 16.864595413208008,
      "learning_rate": 1e-05,
      "loss": 6.1083,
      "step": 4416
    },
    {
      "epoch": 0.9573983739837398,
      "step": 4416,
      "training_loss": 9.618501663208008
    },
    {
      "epoch": 0.9573983739837398,
      "step": 4416,
      "training_loss": 7.400233745574951
    },
    {
      "epoch": 0.9573983739837398,
      "step": 4416,
      "training_loss": 6.16571044921875
    },
    {
      "epoch": 0.9573983739837398,
      "step": 4416,
      "training_loss": 6.176821708679199
    },
    {
      "epoch": 0.9576151761517615,
      "step": 4417,
      "training_loss": 5.0889716148376465
    },
    {
      "epoch": 0.9576151761517615,
      "step": 4417,
      "training_loss": 6.641554355621338
    },
    {
      "epoch": 0.9576151761517615,
      "step": 4417,
      "training_loss": 6.386775493621826
    },
    {
      "epoch": 0.9576151761517615,
      "step": 4417,
      "training_loss": 5.734551429748535
    },
    {
      "epoch": 0.9578319783197832,
      "step": 4418,
      "training_loss": 6.892490863800049
    },
    {
      "epoch": 0.9578319783197832,
      "step": 4418,
      "training_loss": 7.61344051361084
    },
    {
      "epoch": 0.9578319783197832,
      "step": 4418,
      "training_loss": 5.670648097991943
    },
    {
      "epoch": 0.9578319783197832,
      "step": 4418,
      "training_loss": 7.3338541984558105
    },
    {
      "epoch": 0.9580487804878048,
      "step": 4419,
      "training_loss": 7.682848930358887
    },
    {
      "epoch": 0.9580487804878048,
      "step": 4419,
      "training_loss": 7.492648601531982
    },
    {
      "epoch": 0.9580487804878048,
      "step": 4419,
      "training_loss": 6.884313583374023
    },
    {
      "epoch": 0.9580487804878048,
      "step": 4419,
      "training_loss": 5.532902717590332
    },
    {
      "epoch": 0.9582655826558265,
      "grad_norm": 28.2777042388916,
      "learning_rate": 1e-05,
      "loss": 6.7698,
      "step": 4420
    },
    {
      "epoch": 0.9582655826558265,
      "step": 4420,
      "training_loss": 7.465359687805176
    },
    {
      "epoch": 0.9582655826558265,
      "step": 4420,
      "training_loss": 7.1793646812438965
    },
    {
      "epoch": 0.9582655826558265,
      "step": 4420,
      "training_loss": 6.455368995666504
    },
    {
      "epoch": 0.9582655826558265,
      "step": 4420,
      "training_loss": 6.381484031677246
    },
    {
      "epoch": 0.9584823848238483,
      "step": 4421,
      "training_loss": 6.46827507019043
    },
    {
      "epoch": 0.9584823848238483,
      "step": 4421,
      "training_loss": 7.023143291473389
    },
    {
      "epoch": 0.9584823848238483,
      "step": 4421,
      "training_loss": 6.859832763671875
    },
    {
      "epoch": 0.9584823848238483,
      "step": 4421,
      "training_loss": 6.743137359619141
    },
    {
      "epoch": 0.95869918699187,
      "step": 4422,
      "training_loss": 3.240044593811035
    },
    {
      "epoch": 0.95869918699187,
      "step": 4422,
      "training_loss": 6.5381693840026855
    },
    {
      "epoch": 0.95869918699187,
      "step": 4422,
      "training_loss": 6.488451957702637
    },
    {
      "epoch": 0.95869918699187,
      "step": 4422,
      "training_loss": 5.1757025718688965
    },
    {
      "epoch": 0.9589159891598916,
      "step": 4423,
      "training_loss": 6.614413261413574
    },
    {
      "epoch": 0.9589159891598916,
      "step": 4423,
      "training_loss": 5.880033493041992
    },
    {
      "epoch": 0.9589159891598916,
      "step": 4423,
      "training_loss": 7.118534564971924
    },
    {
      "epoch": 0.9589159891598916,
      "step": 4423,
      "training_loss": 5.552114963531494
    },
    {
      "epoch": 0.9591327913279133,
      "grad_norm": 18.027015686035156,
      "learning_rate": 1e-05,
      "loss": 6.324,
      "step": 4424
    },
    {
      "epoch": 0.9591327913279133,
      "step": 4424,
      "training_loss": 5.931506633758545
    },
    {
      "epoch": 0.9591327913279133,
      "step": 4424,
      "training_loss": 6.8641276359558105
    },
    {
      "epoch": 0.9591327913279133,
      "step": 4424,
      "training_loss": 6.823930740356445
    },
    {
      "epoch": 0.9591327913279133,
      "step": 4424,
      "training_loss": 2.653773307800293
    },
    {
      "epoch": 0.959349593495935,
      "step": 4425,
      "training_loss": 6.488003253936768
    },
    {
      "epoch": 0.959349593495935,
      "step": 4425,
      "training_loss": 6.156007289886475
    },
    {
      "epoch": 0.959349593495935,
      "step": 4425,
      "training_loss": 6.130378723144531
    },
    {
      "epoch": 0.959349593495935,
      "step": 4425,
      "training_loss": 3.7276554107666016
    },
    {
      "epoch": 0.9595663956639566,
      "step": 4426,
      "training_loss": 7.970926284790039
    },
    {
      "epoch": 0.9595663956639566,
      "step": 4426,
      "training_loss": 6.092618465423584
    },
    {
      "epoch": 0.9595663956639566,
      "step": 4426,
      "training_loss": 4.960702419281006
    },
    {
      "epoch": 0.9595663956639566,
      "step": 4426,
      "training_loss": 6.792346477508545
    },
    {
      "epoch": 0.9597831978319783,
      "step": 4427,
      "training_loss": 6.722817420959473
    },
    {
      "epoch": 0.9597831978319783,
      "step": 4427,
      "training_loss": 4.096415996551514
    },
    {
      "epoch": 0.9597831978319783,
      "step": 4427,
      "training_loss": 6.3291707038879395
    },
    {
      "epoch": 0.9597831978319783,
      "step": 4427,
      "training_loss": 7.3573713302612305
    },
    {
      "epoch": 0.96,
      "grad_norm": 19.869571685791016,
      "learning_rate": 1e-05,
      "loss": 5.9436,
      "step": 4428
    },
    {
      "epoch": 0.96,
      "step": 4428,
      "training_loss": 6.628665924072266
    },
    {
      "epoch": 0.96,
      "step": 4428,
      "training_loss": 7.362071514129639
    },
    {
      "epoch": 0.96,
      "step": 4428,
      "training_loss": 4.884105205535889
    },
    {
      "epoch": 0.96,
      "step": 4428,
      "training_loss": 3.971580743789673
    },
    {
      "epoch": 0.9602168021680216,
      "step": 4429,
      "training_loss": 6.253309726715088
    },
    {
      "epoch": 0.9602168021680216,
      "step": 4429,
      "training_loss": 7.5825018882751465
    },
    {
      "epoch": 0.9602168021680216,
      "step": 4429,
      "training_loss": 6.33638334274292
    },
    {
      "epoch": 0.9602168021680216,
      "step": 4429,
      "training_loss": 6.744688987731934
    },
    {
      "epoch": 0.9604336043360434,
      "step": 4430,
      "training_loss": 6.347868919372559
    },
    {
      "epoch": 0.9604336043360434,
      "step": 4430,
      "training_loss": 3.46185564994812
    },
    {
      "epoch": 0.9604336043360434,
      "step": 4430,
      "training_loss": 5.37253475189209
    },
    {
      "epoch": 0.9604336043360434,
      "step": 4430,
      "training_loss": 5.246165752410889
    },
    {
      "epoch": 0.9606504065040651,
      "step": 4431,
      "training_loss": 4.8054680824279785
    },
    {
      "epoch": 0.9606504065040651,
      "step": 4431,
      "training_loss": 7.0992350578308105
    },
    {
      "epoch": 0.9606504065040651,
      "step": 4431,
      "training_loss": 4.766811847686768
    },
    {
      "epoch": 0.9606504065040651,
      "step": 4431,
      "training_loss": 6.915003299713135
    },
    {
      "epoch": 0.9608672086720867,
      "grad_norm": 18.12745475769043,
      "learning_rate": 1e-05,
      "loss": 5.8611,
      "step": 4432
    },
    {
      "epoch": 0.9608672086720867,
      "step": 4432,
      "training_loss": 7.449426651000977
    },
    {
      "epoch": 0.9608672086720867,
      "step": 4432,
      "training_loss": 6.523789405822754
    },
    {
      "epoch": 0.9608672086720867,
      "step": 4432,
      "training_loss": 6.960297584533691
    },
    {
      "epoch": 0.9608672086720867,
      "step": 4432,
      "training_loss": 6.21517276763916
    },
    {
      "epoch": 0.9610840108401084,
      "step": 4433,
      "training_loss": 6.889178276062012
    },
    {
      "epoch": 0.9610840108401084,
      "step": 4433,
      "training_loss": 7.242706775665283
    },
    {
      "epoch": 0.9610840108401084,
      "step": 4433,
      "training_loss": 6.612668991088867
    },
    {
      "epoch": 0.9610840108401084,
      "step": 4433,
      "training_loss": 7.54011869430542
    },
    {
      "epoch": 0.9613008130081301,
      "step": 4434,
      "training_loss": 6.41370964050293
    },
    {
      "epoch": 0.9613008130081301,
      "step": 4434,
      "training_loss": 6.671518325805664
    },
    {
      "epoch": 0.9613008130081301,
      "step": 4434,
      "training_loss": 6.651192665100098
    },
    {
      "epoch": 0.9613008130081301,
      "step": 4434,
      "training_loss": 7.580275535583496
    },
    {
      "epoch": 0.9615176151761518,
      "step": 4435,
      "training_loss": 7.502043724060059
    },
    {
      "epoch": 0.9615176151761518,
      "step": 4435,
      "training_loss": 7.0909576416015625
    },
    {
      "epoch": 0.9615176151761518,
      "step": 4435,
      "training_loss": 6.353435039520264
    },
    {
      "epoch": 0.9615176151761518,
      "step": 4435,
      "training_loss": 7.260274410247803
    },
    {
      "epoch": 0.9617344173441734,
      "grad_norm": 13.915840148925781,
      "learning_rate": 1e-05,
      "loss": 6.9348,
      "step": 4436
    },
    {
      "epoch": 0.9617344173441734,
      "step": 4436,
      "training_loss": 6.541430950164795
    },
    {
      "epoch": 0.9617344173441734,
      "step": 4436,
      "training_loss": 6.761616230010986
    },
    {
      "epoch": 0.9617344173441734,
      "step": 4436,
      "training_loss": 6.788795471191406
    },
    {
      "epoch": 0.9617344173441734,
      "step": 4436,
      "training_loss": 6.009721755981445
    },
    {
      "epoch": 0.9619512195121951,
      "step": 4437,
      "training_loss": 6.046125888824463
    },
    {
      "epoch": 0.9619512195121951,
      "step": 4437,
      "training_loss": 5.444252967834473
    },
    {
      "epoch": 0.9619512195121951,
      "step": 4437,
      "training_loss": 7.29861307144165
    },
    {
      "epoch": 0.9619512195121951,
      "step": 4437,
      "training_loss": 6.684026718139648
    },
    {
      "epoch": 0.9621680216802168,
      "step": 4438,
      "training_loss": 6.471321105957031
    },
    {
      "epoch": 0.9621680216802168,
      "step": 4438,
      "training_loss": 6.725696086883545
    },
    {
      "epoch": 0.9621680216802168,
      "step": 4438,
      "training_loss": 6.6648640632629395
    },
    {
      "epoch": 0.9621680216802168,
      "step": 4438,
      "training_loss": 6.884334564208984
    },
    {
      "epoch": 0.9623848238482385,
      "step": 4439,
      "training_loss": 5.816895961761475
    },
    {
      "epoch": 0.9623848238482385,
      "step": 4439,
      "training_loss": 4.253420829772949
    },
    {
      "epoch": 0.9623848238482385,
      "step": 4439,
      "training_loss": 6.511646747589111
    },
    {
      "epoch": 0.9623848238482385,
      "step": 4439,
      "training_loss": 7.189821243286133
    },
    {
      "epoch": 0.9626016260162602,
      "grad_norm": 25.441083908081055,
      "learning_rate": 1e-05,
      "loss": 6.3808,
      "step": 4440
    },
    {
      "epoch": 0.9626016260162602,
      "step": 4440,
      "training_loss": 7.207362174987793
    },
    {
      "epoch": 0.9626016260162602,
      "step": 4440,
      "training_loss": 6.1206817626953125
    },
    {
      "epoch": 0.9626016260162602,
      "step": 4440,
      "training_loss": 6.217235088348389
    },
    {
      "epoch": 0.9626016260162602,
      "step": 4440,
      "training_loss": 6.863557815551758
    },
    {
      "epoch": 0.9628184281842819,
      "step": 4441,
      "training_loss": 5.836926460266113
    },
    {
      "epoch": 0.9628184281842819,
      "step": 4441,
      "training_loss": 5.561784744262695
    },
    {
      "epoch": 0.9628184281842819,
      "step": 4441,
      "training_loss": 5.866151332855225
    },
    {
      "epoch": 0.9628184281842819,
      "step": 4441,
      "training_loss": 6.624252796173096
    },
    {
      "epoch": 0.9630352303523035,
      "step": 4442,
      "training_loss": 6.27495813369751
    },
    {
      "epoch": 0.9630352303523035,
      "step": 4442,
      "training_loss": 6.571727275848389
    },
    {
      "epoch": 0.9630352303523035,
      "step": 4442,
      "training_loss": 5.803874969482422
    },
    {
      "epoch": 0.9630352303523035,
      "step": 4442,
      "training_loss": 7.510433673858643
    },
    {
      "epoch": 0.9632520325203252,
      "step": 4443,
      "training_loss": 2.716989755630493
    },
    {
      "epoch": 0.9632520325203252,
      "step": 4443,
      "training_loss": 7.358925819396973
    },
    {
      "epoch": 0.9632520325203252,
      "step": 4443,
      "training_loss": 4.853243827819824
    },
    {
      "epoch": 0.9632520325203252,
      "step": 4443,
      "training_loss": 6.897398948669434
    },
    {
      "epoch": 0.9634688346883469,
      "grad_norm": 21.350543975830078,
      "learning_rate": 1e-05,
      "loss": 6.1428,
      "step": 4444
    },
    {
      "epoch": 0.9634688346883469,
      "step": 4444,
      "training_loss": 7.253741264343262
    },
    {
      "epoch": 0.9634688346883469,
      "step": 4444,
      "training_loss": 7.042235374450684
    },
    {
      "epoch": 0.9634688346883469,
      "step": 4444,
      "training_loss": 6.103107929229736
    },
    {
      "epoch": 0.9634688346883469,
      "step": 4444,
      "training_loss": 5.750327110290527
    },
    {
      "epoch": 0.9636856368563685,
      "step": 4445,
      "training_loss": 7.5490522384643555
    },
    {
      "epoch": 0.9636856368563685,
      "step": 4445,
      "training_loss": 5.058868408203125
    },
    {
      "epoch": 0.9636856368563685,
      "step": 4445,
      "training_loss": 3.6158154010772705
    },
    {
      "epoch": 0.9636856368563685,
      "step": 4445,
      "training_loss": 4.999392986297607
    },
    {
      "epoch": 0.9639024390243902,
      "step": 4446,
      "training_loss": 3.584838390350342
    },
    {
      "epoch": 0.9639024390243902,
      "step": 4446,
      "training_loss": 6.081403732299805
    },
    {
      "epoch": 0.9639024390243902,
      "step": 4446,
      "training_loss": 7.641971111297607
    },
    {
      "epoch": 0.9639024390243902,
      "step": 4446,
      "training_loss": 4.295890808105469
    },
    {
      "epoch": 0.9641192411924119,
      "step": 4447,
      "training_loss": 5.543622016906738
    },
    {
      "epoch": 0.9641192411924119,
      "step": 4447,
      "training_loss": 6.00505256652832
    },
    {
      "epoch": 0.9641192411924119,
      "step": 4447,
      "training_loss": 3.616143226623535
    },
    {
      "epoch": 0.9641192411924119,
      "step": 4447,
      "training_loss": 6.632359981536865
    },
    {
      "epoch": 0.9643360433604337,
      "grad_norm": 22.57375144958496,
      "learning_rate": 1e-05,
      "loss": 5.6734,
      "step": 4448
    },
    {
      "epoch": 0.9643360433604337,
      "step": 4448,
      "training_loss": 6.110820293426514
    },
    {
      "epoch": 0.9643360433604337,
      "step": 4448,
      "training_loss": 7.352878570556641
    },
    {
      "epoch": 0.9643360433604337,
      "step": 4448,
      "training_loss": 6.715627670288086
    },
    {
      "epoch": 0.9643360433604337,
      "step": 4448,
      "training_loss": 4.367475509643555
    },
    {
      "epoch": 0.9645528455284553,
      "step": 4449,
      "training_loss": 6.43552303314209
    },
    {
      "epoch": 0.9645528455284553,
      "step": 4449,
      "training_loss": 5.863052845001221
    },
    {
      "epoch": 0.9645528455284553,
      "step": 4449,
      "training_loss": 4.9150390625
    },
    {
      "epoch": 0.9645528455284553,
      "step": 4449,
      "training_loss": 4.159427165985107
    },
    {
      "epoch": 0.964769647696477,
      "step": 4450,
      "training_loss": 5.46927547454834
    },
    {
      "epoch": 0.964769647696477,
      "step": 4450,
      "training_loss": 5.936655044555664
    },
    {
      "epoch": 0.964769647696477,
      "step": 4450,
      "training_loss": 6.072570323944092
    },
    {
      "epoch": 0.964769647696477,
      "step": 4450,
      "training_loss": 6.965121746063232
    },
    {
      "epoch": 0.9649864498644987,
      "step": 4451,
      "training_loss": 6.504447937011719
    },
    {
      "epoch": 0.9649864498644987,
      "step": 4451,
      "training_loss": 3.9886207580566406
    },
    {
      "epoch": 0.9649864498644987,
      "step": 4451,
      "training_loss": 6.379899501800537
    },
    {
      "epoch": 0.9649864498644987,
      "step": 4451,
      "training_loss": 5.744510650634766
    },
    {
      "epoch": 0.9652032520325203,
      "grad_norm": 19.552757263183594,
      "learning_rate": 1e-05,
      "loss": 5.8113,
      "step": 4452
    },
    {
      "epoch": 0.9652032520325203,
      "step": 4452,
      "training_loss": 6.9369635581970215
    },
    {
      "epoch": 0.9652032520325203,
      "step": 4452,
      "training_loss": 3.6787774562835693
    },
    {
      "epoch": 0.9652032520325203,
      "step": 4452,
      "training_loss": 6.915317535400391
    },
    {
      "epoch": 0.9652032520325203,
      "step": 4452,
      "training_loss": 4.454118728637695
    },
    {
      "epoch": 0.965420054200542,
      "step": 4453,
      "training_loss": 5.493110179901123
    },
    {
      "epoch": 0.965420054200542,
      "step": 4453,
      "training_loss": 5.992880821228027
    },
    {
      "epoch": 0.965420054200542,
      "step": 4453,
      "training_loss": 6.261994361877441
    },
    {
      "epoch": 0.965420054200542,
      "step": 4453,
      "training_loss": 5.42417573928833
    },
    {
      "epoch": 0.9656368563685637,
      "step": 4454,
      "training_loss": 7.723775386810303
    },
    {
      "epoch": 0.9656368563685637,
      "step": 4454,
      "training_loss": 7.698690414428711
    },
    {
      "epoch": 0.9656368563685637,
      "step": 4454,
      "training_loss": 5.905814170837402
    },
    {
      "epoch": 0.9656368563685637,
      "step": 4454,
      "training_loss": 5.760150909423828
    },
    {
      "epoch": 0.9658536585365853,
      "step": 4455,
      "training_loss": 6.358171463012695
    },
    {
      "epoch": 0.9658536585365853,
      "step": 4455,
      "training_loss": 6.760421276092529
    },
    {
      "epoch": 0.9658536585365853,
      "step": 4455,
      "training_loss": 5.822203159332275
    },
    {
      "epoch": 0.9658536585365853,
      "step": 4455,
      "training_loss": 6.270573616027832
    },
    {
      "epoch": 0.966070460704607,
      "grad_norm": 25.108993530273438,
      "learning_rate": 1e-05,
      "loss": 6.0911,
      "step": 4456
    },
    {
      "epoch": 0.966070460704607,
      "step": 4456,
      "training_loss": 7.778360366821289
    },
    {
      "epoch": 0.966070460704607,
      "step": 4456,
      "training_loss": 6.289760112762451
    },
    {
      "epoch": 0.966070460704607,
      "step": 4456,
      "training_loss": 5.473345756530762
    },
    {
      "epoch": 0.966070460704607,
      "step": 4456,
      "training_loss": 4.58574104309082
    },
    {
      "epoch": 0.9662872628726287,
      "step": 4457,
      "training_loss": 6.584701061248779
    },
    {
      "epoch": 0.9662872628726287,
      "step": 4457,
      "training_loss": 5.472537994384766
    },
    {
      "epoch": 0.9662872628726287,
      "step": 4457,
      "training_loss": 8.494497299194336
    },
    {
      "epoch": 0.9662872628726287,
      "step": 4457,
      "training_loss": 7.4089155197143555
    },
    {
      "epoch": 0.9665040650406505,
      "step": 4458,
      "training_loss": 6.757997035980225
    },
    {
      "epoch": 0.9665040650406505,
      "step": 4458,
      "training_loss": 6.078364849090576
    },
    {
      "epoch": 0.9665040650406505,
      "step": 4458,
      "training_loss": 5.117623329162598
    },
    {
      "epoch": 0.9665040650406505,
      "step": 4458,
      "training_loss": 6.554693698883057
    },
    {
      "epoch": 0.9667208672086721,
      "step": 4459,
      "training_loss": 6.701158046722412
    },
    {
      "epoch": 0.9667208672086721,
      "step": 4459,
      "training_loss": 5.877812385559082
    },
    {
      "epoch": 0.9667208672086721,
      "step": 4459,
      "training_loss": 5.916822910308838
    },
    {
      "epoch": 0.9667208672086721,
      "step": 4459,
      "training_loss": 6.696650981903076
    },
    {
      "epoch": 0.9669376693766938,
      "grad_norm": 22.2580623626709,
      "learning_rate": 1e-05,
      "loss": 6.3618,
      "step": 4460
    },
    {
      "epoch": 0.9669376693766938,
      "step": 4460,
      "training_loss": 6.189805030822754
    },
    {
      "epoch": 0.9669376693766938,
      "step": 4460,
      "training_loss": 7.089169502258301
    },
    {
      "epoch": 0.9669376693766938,
      "step": 4460,
      "training_loss": 5.012323379516602
    },
    {
      "epoch": 0.9669376693766938,
      "step": 4460,
      "training_loss": 6.413051128387451
    },
    {
      "epoch": 0.9671544715447155,
      "step": 4461,
      "training_loss": 4.351346969604492
    },
    {
      "epoch": 0.9671544715447155,
      "step": 4461,
      "training_loss": 6.210381984710693
    },
    {
      "epoch": 0.9671544715447155,
      "step": 4461,
      "training_loss": 5.4323248863220215
    },
    {
      "epoch": 0.9671544715447155,
      "step": 4461,
      "training_loss": 6.481992721557617
    },
    {
      "epoch": 0.9673712737127371,
      "step": 4462,
      "training_loss": 6.799393177032471
    },
    {
      "epoch": 0.9673712737127371,
      "step": 4462,
      "training_loss": 6.714468002319336
    },
    {
      "epoch": 0.9673712737127371,
      "step": 4462,
      "training_loss": 6.477797031402588
    },
    {
      "epoch": 0.9673712737127371,
      "step": 4462,
      "training_loss": 7.1924848556518555
    },
    {
      "epoch": 0.9675880758807588,
      "step": 4463,
      "training_loss": 6.954389572143555
    },
    {
      "epoch": 0.9675880758807588,
      "step": 4463,
      "training_loss": 9.09262752532959
    },
    {
      "epoch": 0.9675880758807588,
      "step": 4463,
      "training_loss": 6.379693031311035
    },
    {
      "epoch": 0.9675880758807588,
      "step": 4463,
      "training_loss": 2.5368974208831787
    },
    {
      "epoch": 0.9678048780487805,
      "grad_norm": 33.239898681640625,
      "learning_rate": 1e-05,
      "loss": 6.208,
      "step": 4464
    },
    {
      "epoch": 0.9678048780487805,
      "step": 4464,
      "training_loss": 6.932797908782959
    },
    {
      "epoch": 0.9678048780487805,
      "step": 4464,
      "training_loss": 6.4296979904174805
    },
    {
      "epoch": 0.9678048780487805,
      "step": 4464,
      "training_loss": 6.685347557067871
    },
    {
      "epoch": 0.9678048780487805,
      "step": 4464,
      "training_loss": 6.51682710647583
    },
    {
      "epoch": 0.9680216802168021,
      "step": 4465,
      "training_loss": 6.299350738525391
    },
    {
      "epoch": 0.9680216802168021,
      "step": 4465,
      "training_loss": 4.265326976776123
    },
    {
      "epoch": 0.9680216802168021,
      "step": 4465,
      "training_loss": 6.850532531738281
    },
    {
      "epoch": 0.9680216802168021,
      "step": 4465,
      "training_loss": 5.87110710144043
    },
    {
      "epoch": 0.9682384823848238,
      "step": 4466,
      "training_loss": 7.071712017059326
    },
    {
      "epoch": 0.9682384823848238,
      "step": 4466,
      "training_loss": 5.970369338989258
    },
    {
      "epoch": 0.9682384823848238,
      "step": 4466,
      "training_loss": 2.9042234420776367
    },
    {
      "epoch": 0.9682384823848238,
      "step": 4466,
      "training_loss": 6.143812656402588
    },
    {
      "epoch": 0.9684552845528456,
      "step": 4467,
      "training_loss": 3.806309938430786
    },
    {
      "epoch": 0.9684552845528456,
      "step": 4467,
      "training_loss": 7.499589920043945
    },
    {
      "epoch": 0.9684552845528456,
      "step": 4467,
      "training_loss": 8.226229667663574
    },
    {
      "epoch": 0.9684552845528456,
      "step": 4467,
      "training_loss": 3.967756986618042
    },
    {
      "epoch": 0.9686720867208672,
      "grad_norm": 24.713600158691406,
      "learning_rate": 1e-05,
      "loss": 5.9651,
      "step": 4468
    },
    {
      "epoch": 0.9686720867208672,
      "step": 4468,
      "training_loss": 6.922090530395508
    },
    {
      "epoch": 0.9686720867208672,
      "step": 4468,
      "training_loss": 5.9784393310546875
    },
    {
      "epoch": 0.9686720867208672,
      "step": 4468,
      "training_loss": 6.4053473472595215
    },
    {
      "epoch": 0.9686720867208672,
      "step": 4468,
      "training_loss": 3.535867691040039
    },
    {
      "epoch": 0.9688888888888889,
      "step": 4469,
      "training_loss": 6.992869853973389
    },
    {
      "epoch": 0.9688888888888889,
      "step": 4469,
      "training_loss": 6.8321027755737305
    },
    {
      "epoch": 0.9688888888888889,
      "step": 4469,
      "training_loss": 5.731043815612793
    },
    {
      "epoch": 0.9688888888888889,
      "step": 4469,
      "training_loss": 7.109376430511475
    },
    {
      "epoch": 0.9691056910569106,
      "step": 4470,
      "training_loss": 7.406187057495117
    },
    {
      "epoch": 0.9691056910569106,
      "step": 4470,
      "training_loss": 6.662271976470947
    },
    {
      "epoch": 0.9691056910569106,
      "step": 4470,
      "training_loss": 5.940131664276123
    },
    {
      "epoch": 0.9691056910569106,
      "step": 4470,
      "training_loss": 6.288865566253662
    },
    {
      "epoch": 0.9693224932249322,
      "step": 4471,
      "training_loss": 7.164622783660889
    },
    {
      "epoch": 0.9693224932249322,
      "step": 4471,
      "training_loss": 6.70161247253418
    },
    {
      "epoch": 0.9693224932249322,
      "step": 4471,
      "training_loss": 5.259268283843994
    },
    {
      "epoch": 0.9693224932249322,
      "step": 4471,
      "training_loss": 4.052402019500732
    },
    {
      "epoch": 0.9695392953929539,
      "grad_norm": 25.484628677368164,
      "learning_rate": 1e-05,
      "loss": 6.1864,
      "step": 4472
    },
    {
      "epoch": 0.9695392953929539,
      "step": 4472,
      "training_loss": 5.286439418792725
    },
    {
      "epoch": 0.9695392953929539,
      "step": 4472,
      "training_loss": 3.5816872119903564
    },
    {
      "epoch": 0.9695392953929539,
      "step": 4472,
      "training_loss": 6.396290302276611
    },
    {
      "epoch": 0.9695392953929539,
      "step": 4472,
      "training_loss": 6.432974338531494
    },
    {
      "epoch": 0.9697560975609756,
      "step": 4473,
      "training_loss": 7.236260890960693
    },
    {
      "epoch": 0.9697560975609756,
      "step": 4473,
      "training_loss": 7.597079753875732
    },
    {
      "epoch": 0.9697560975609756,
      "step": 4473,
      "training_loss": 7.464767932891846
    },
    {
      "epoch": 0.9697560975609756,
      "step": 4473,
      "training_loss": 4.668834209442139
    },
    {
      "epoch": 0.9699728997289973,
      "step": 4474,
      "training_loss": 6.478078842163086
    },
    {
      "epoch": 0.9699728997289973,
      "step": 4474,
      "training_loss": 6.633493900299072
    },
    {
      "epoch": 0.9699728997289973,
      "step": 4474,
      "training_loss": 7.514378070831299
    },
    {
      "epoch": 0.9699728997289973,
      "step": 4474,
      "training_loss": 5.875970840454102
    },
    {
      "epoch": 0.9701897018970189,
      "step": 4475,
      "training_loss": 5.950595378875732
    },
    {
      "epoch": 0.9701897018970189,
      "step": 4475,
      "training_loss": 6.436423301696777
    },
    {
      "epoch": 0.9701897018970189,
      "step": 4475,
      "training_loss": 8.642528533935547
    },
    {
      "epoch": 0.9701897018970189,
      "step": 4475,
      "training_loss": 6.625030517578125
    },
    {
      "epoch": 0.9704065040650407,
      "grad_norm": 27.10024642944336,
      "learning_rate": 1e-05,
      "loss": 6.4263,
      "step": 4476
    },
    {
      "epoch": 0.9704065040650407,
      "step": 4476,
      "training_loss": 5.500065326690674
    },
    {
      "epoch": 0.9704065040650407,
      "step": 4476,
      "training_loss": 6.770895957946777
    },
    {
      "epoch": 0.9704065040650407,
      "step": 4476,
      "training_loss": 6.5824761390686035
    },
    {
      "epoch": 0.9704065040650407,
      "step": 4476,
      "training_loss": 6.4899492263793945
    },
    {
      "epoch": 0.9706233062330624,
      "step": 4477,
      "training_loss": 6.740691661834717
    },
    {
      "epoch": 0.9706233062330624,
      "step": 4477,
      "training_loss": 6.531221389770508
    },
    {
      "epoch": 0.9706233062330624,
      "step": 4477,
      "training_loss": 4.835858345031738
    },
    {
      "epoch": 0.9706233062330624,
      "step": 4477,
      "training_loss": 6.440711975097656
    },
    {
      "epoch": 0.970840108401084,
      "step": 4478,
      "training_loss": 6.536721706390381
    },
    {
      "epoch": 0.970840108401084,
      "step": 4478,
      "training_loss": 6.1046223640441895
    },
    {
      "epoch": 0.970840108401084,
      "step": 4478,
      "training_loss": 5.458642959594727
    },
    {
      "epoch": 0.970840108401084,
      "step": 4478,
      "training_loss": 4.1126322746276855
    },
    {
      "epoch": 0.9710569105691057,
      "step": 4479,
      "training_loss": 4.379135608673096
    },
    {
      "epoch": 0.9710569105691057,
      "step": 4479,
      "training_loss": 4.556933403015137
    },
    {
      "epoch": 0.9710569105691057,
      "step": 4479,
      "training_loss": 6.704165935516357
    },
    {
      "epoch": 0.9710569105691057,
      "step": 4479,
      "training_loss": 7.191929340362549
    },
    {
      "epoch": 0.9712737127371274,
      "grad_norm": 16.905885696411133,
      "learning_rate": 1e-05,
      "loss": 5.9335,
      "step": 4480
    },
    {
      "epoch": 0.9712737127371274,
      "step": 4480,
      "training_loss": 6.249685764312744
    },
    {
      "epoch": 0.9712737127371274,
      "step": 4480,
      "training_loss": 6.772255897521973
    },
    {
      "epoch": 0.9712737127371274,
      "step": 4480,
      "training_loss": 6.401388645172119
    },
    {
      "epoch": 0.9712737127371274,
      "step": 4480,
      "training_loss": 7.076661109924316
    },
    {
      "epoch": 0.971490514905149,
      "step": 4481,
      "training_loss": 5.0943098068237305
    },
    {
      "epoch": 0.971490514905149,
      "step": 4481,
      "training_loss": 5.6864471435546875
    },
    {
      "epoch": 0.971490514905149,
      "step": 4481,
      "training_loss": 6.074679851531982
    },
    {
      "epoch": 0.971490514905149,
      "step": 4481,
      "training_loss": 6.838155269622803
    },
    {
      "epoch": 0.9717073170731707,
      "step": 4482,
      "training_loss": 6.094751358032227
    },
    {
      "epoch": 0.9717073170731707,
      "step": 4482,
      "training_loss": 6.504806995391846
    },
    {
      "epoch": 0.9717073170731707,
      "step": 4482,
      "training_loss": 6.491276264190674
    },
    {
      "epoch": 0.9717073170731707,
      "step": 4482,
      "training_loss": 6.669627666473389
    },
    {
      "epoch": 0.9719241192411924,
      "step": 4483,
      "training_loss": 6.511835098266602
    },
    {
      "epoch": 0.9719241192411924,
      "step": 4483,
      "training_loss": 5.285260200500488
    },
    {
      "epoch": 0.9719241192411924,
      "step": 4483,
      "training_loss": 6.084213733673096
    },
    {
      "epoch": 0.9719241192411924,
      "step": 4483,
      "training_loss": 7.040517330169678
    },
    {
      "epoch": 0.972140921409214,
      "grad_norm": 21.593990325927734,
      "learning_rate": 1e-05,
      "loss": 6.3047,
      "step": 4484
    },
    {
      "epoch": 0.972140921409214,
      "step": 4484,
      "training_loss": 6.795928478240967
    },
    {
      "epoch": 0.972140921409214,
      "step": 4484,
      "training_loss": 5.686872959136963
    },
    {
      "epoch": 0.972140921409214,
      "step": 4484,
      "training_loss": 4.645668029785156
    },
    {
      "epoch": 0.972140921409214,
      "step": 4484,
      "training_loss": 6.765585422515869
    },
    {
      "epoch": 0.9723577235772358,
      "step": 4485,
      "training_loss": 4.985584735870361
    },
    {
      "epoch": 0.9723577235772358,
      "step": 4485,
      "training_loss": 6.238927364349365
    },
    {
      "epoch": 0.9723577235772358,
      "step": 4485,
      "training_loss": 6.14139461517334
    },
    {
      "epoch": 0.9723577235772358,
      "step": 4485,
      "training_loss": 6.201986789703369
    },
    {
      "epoch": 0.9725745257452575,
      "step": 4486,
      "training_loss": 7.13040018081665
    },
    {
      "epoch": 0.9725745257452575,
      "step": 4486,
      "training_loss": 4.875907897949219
    },
    {
      "epoch": 0.9725745257452575,
      "step": 4486,
      "training_loss": 7.027585506439209
    },
    {
      "epoch": 0.9725745257452575,
      "step": 4486,
      "training_loss": 3.419952392578125
    },
    {
      "epoch": 0.9727913279132792,
      "step": 4487,
      "training_loss": 3.7814364433288574
    },
    {
      "epoch": 0.9727913279132792,
      "step": 4487,
      "training_loss": 6.2186360359191895
    },
    {
      "epoch": 0.9727913279132792,
      "step": 4487,
      "training_loss": 6.2335286140441895
    },
    {
      "epoch": 0.9727913279132792,
      "step": 4487,
      "training_loss": 4.910330772399902
    },
    {
      "epoch": 0.9730081300813008,
      "grad_norm": 26.36650276184082,
      "learning_rate": 1e-05,
      "loss": 5.6912,
      "step": 4488
    },
    {
      "epoch": 0.9730081300813008,
      "step": 4488,
      "training_loss": 5.252921104431152
    },
    {
      "epoch": 0.9730081300813008,
      "step": 4488,
      "training_loss": 5.2827277183532715
    },
    {
      "epoch": 0.9730081300813008,
      "step": 4488,
      "training_loss": 5.906976222991943
    },
    {
      "epoch": 0.9730081300813008,
      "step": 4488,
      "training_loss": 6.200081825256348
    },
    {
      "epoch": 0.9732249322493225,
      "step": 4489,
      "training_loss": 5.678590297698975
    },
    {
      "epoch": 0.9732249322493225,
      "step": 4489,
      "training_loss": 6.904232025146484
    },
    {
      "epoch": 0.9732249322493225,
      "step": 4489,
      "training_loss": 7.047065734863281
    },
    {
      "epoch": 0.9732249322493225,
      "step": 4489,
      "training_loss": 6.865101337432861
    },
    {
      "epoch": 0.9734417344173442,
      "step": 4490,
      "training_loss": 7.30896520614624
    },
    {
      "epoch": 0.9734417344173442,
      "step": 4490,
      "training_loss": 5.25346040725708
    },
    {
      "epoch": 0.9734417344173442,
      "step": 4490,
      "training_loss": 6.764379501342773
    },
    {
      "epoch": 0.9734417344173442,
      "step": 4490,
      "training_loss": 7.516383171081543
    },
    {
      "epoch": 0.9736585365853658,
      "step": 4491,
      "training_loss": 6.521378517150879
    },
    {
      "epoch": 0.9736585365853658,
      "step": 4491,
      "training_loss": 5.587615013122559
    },
    {
      "epoch": 0.9736585365853658,
      "step": 4491,
      "training_loss": 6.381022930145264
    },
    {
      "epoch": 0.9736585365853658,
      "step": 4491,
      "training_loss": 6.915714740753174
    },
    {
      "epoch": 0.9738753387533875,
      "grad_norm": 20.574514389038086,
      "learning_rate": 1e-05,
      "loss": 6.3367,
      "step": 4492
    },
    {
      "epoch": 0.9738753387533875,
      "step": 4492,
      "training_loss": 4.870016098022461
    },
    {
      "epoch": 0.9738753387533875,
      "step": 4492,
      "training_loss": 6.095873832702637
    },
    {
      "epoch": 0.9738753387533875,
      "step": 4492,
      "training_loss": 5.318548679351807
    },
    {
      "epoch": 0.9738753387533875,
      "step": 4492,
      "training_loss": 5.474155902862549
    },
    {
      "epoch": 0.9740921409214092,
      "step": 4493,
      "training_loss": 4.009456157684326
    },
    {
      "epoch": 0.9740921409214092,
      "step": 4493,
      "training_loss": 6.886193752288818
    },
    {
      "epoch": 0.9740921409214092,
      "step": 4493,
      "training_loss": 5.34991455078125
    },
    {
      "epoch": 0.9740921409214092,
      "step": 4493,
      "training_loss": 5.547806262969971
    },
    {
      "epoch": 0.974308943089431,
      "step": 4494,
      "training_loss": 7.5572309494018555
    },
    {
      "epoch": 0.974308943089431,
      "step": 4494,
      "training_loss": 5.594258785247803
    },
    {
      "epoch": 0.974308943089431,
      "step": 4494,
      "training_loss": 6.762884616851807
    },
    {
      "epoch": 0.974308943089431,
      "step": 4494,
      "training_loss": 7.908130645751953
    },
    {
      "epoch": 0.9745257452574526,
      "step": 4495,
      "training_loss": 5.971292018890381
    },
    {
      "epoch": 0.9745257452574526,
      "step": 4495,
      "training_loss": 7.115407943725586
    },
    {
      "epoch": 0.9745257452574526,
      "step": 4495,
      "training_loss": 5.91714334487915
    },
    {
      "epoch": 0.9745257452574526,
      "step": 4495,
      "training_loss": 6.888113021850586
    },
    {
      "epoch": 0.9747425474254743,
      "grad_norm": 23.197202682495117,
      "learning_rate": 1e-05,
      "loss": 6.0792,
      "step": 4496
    },
    {
      "epoch": 0.9747425474254743,
      "step": 4496,
      "training_loss": 6.345583915710449
    },
    {
      "epoch": 0.9747425474254743,
      "step": 4496,
      "training_loss": 6.953445911407471
    },
    {
      "epoch": 0.9747425474254743,
      "step": 4496,
      "training_loss": 7.658398151397705
    },
    {
      "epoch": 0.9747425474254743,
      "step": 4496,
      "training_loss": 6.538604736328125
    },
    {
      "epoch": 0.974959349593496,
      "step": 4497,
      "training_loss": 7.936640739440918
    },
    {
      "epoch": 0.974959349593496,
      "step": 4497,
      "training_loss": 5.491370677947998
    },
    {
      "epoch": 0.974959349593496,
      "step": 4497,
      "training_loss": 7.29692268371582
    },
    {
      "epoch": 0.974959349593496,
      "step": 4497,
      "training_loss": 6.326070785522461
    },
    {
      "epoch": 0.9751761517615176,
      "step": 4498,
      "training_loss": 6.396102428436279
    },
    {
      "epoch": 0.9751761517615176,
      "step": 4498,
      "training_loss": 6.380061149597168
    },
    {
      "epoch": 0.9751761517615176,
      "step": 4498,
      "training_loss": 7.444049835205078
    },
    {
      "epoch": 0.9751761517615176,
      "step": 4498,
      "training_loss": 3.9418275356292725
    },
    {
      "epoch": 0.9753929539295393,
      "step": 4499,
      "training_loss": 3.0565052032470703
    },
    {
      "epoch": 0.9753929539295393,
      "step": 4499,
      "training_loss": 7.122574806213379
    },
    {
      "epoch": 0.9753929539295393,
      "step": 4499,
      "training_loss": 5.609746932983398
    },
    {
      "epoch": 0.9753929539295393,
      "step": 4499,
      "training_loss": 6.686886787414551
    },
    {
      "epoch": 0.975609756097561,
      "grad_norm": 15.589825630187988,
      "learning_rate": 1e-05,
      "loss": 6.324,
      "step": 4500
    },
    {
      "epoch": 0.975609756097561,
      "step": 4500,
      "training_loss": 5.651945114135742
    },
    {
      "epoch": 0.975609756097561,
      "step": 4500,
      "training_loss": 7.7656941413879395
    },
    {
      "epoch": 0.975609756097561,
      "step": 4500,
      "training_loss": 6.866030693054199
    },
    {
      "epoch": 0.975609756097561,
      "step": 4500,
      "training_loss": 6.34693717956543
    },
    {
      "epoch": 0.9758265582655826,
      "step": 4501,
      "training_loss": 7.282785415649414
    },
    {
      "epoch": 0.9758265582655826,
      "step": 4501,
      "training_loss": 4.476587772369385
    },
    {
      "epoch": 0.9758265582655826,
      "step": 4501,
      "training_loss": 4.911282062530518
    },
    {
      "epoch": 0.9758265582655826,
      "step": 4501,
      "training_loss": 6.711077690124512
    },
    {
      "epoch": 0.9760433604336043,
      "step": 4502,
      "training_loss": 6.922451972961426
    },
    {
      "epoch": 0.9760433604336043,
      "step": 4502,
      "training_loss": 5.24110746383667
    },
    {
      "epoch": 0.9760433604336043,
      "step": 4502,
      "training_loss": 7.055415630340576
    },
    {
      "epoch": 0.9760433604336043,
      "step": 4502,
      "training_loss": 6.015104293823242
    },
    {
      "epoch": 0.9762601626016261,
      "step": 4503,
      "training_loss": 6.332045555114746
    },
    {
      "epoch": 0.9762601626016261,
      "step": 4503,
      "training_loss": 7.456815242767334
    },
    {
      "epoch": 0.9762601626016261,
      "step": 4503,
      "training_loss": 7.228172302246094
    },
    {
      "epoch": 0.9762601626016261,
      "step": 4503,
      "training_loss": 8.208418846130371
    },
    {
      "epoch": 0.9764769647696477,
      "grad_norm": 19.424455642700195,
      "learning_rate": 1e-05,
      "loss": 6.5295,
      "step": 4504
    },
    {
      "epoch": 0.9764769647696477,
      "step": 4504,
      "training_loss": 7.468533039093018
    },
    {
      "epoch": 0.9764769647696477,
      "step": 4504,
      "training_loss": 5.452487468719482
    },
    {
      "epoch": 0.9764769647696477,
      "step": 4504,
      "training_loss": 5.899759769439697
    },
    {
      "epoch": 0.9764769647696477,
      "step": 4504,
      "training_loss": 6.238783836364746
    },
    {
      "epoch": 0.9766937669376694,
      "step": 4505,
      "training_loss": 8.220820426940918
    },
    {
      "epoch": 0.9766937669376694,
      "step": 4505,
      "training_loss": 5.488142967224121
    },
    {
      "epoch": 0.9766937669376694,
      "step": 4505,
      "training_loss": 6.530056476593018
    },
    {
      "epoch": 0.9766937669376694,
      "step": 4505,
      "training_loss": 3.3113532066345215
    },
    {
      "epoch": 0.9769105691056911,
      "step": 4506,
      "training_loss": 5.699942111968994
    },
    {
      "epoch": 0.9769105691056911,
      "step": 4506,
      "training_loss": 6.353858470916748
    },
    {
      "epoch": 0.9769105691056911,
      "step": 4506,
      "training_loss": 5.565138816833496
    },
    {
      "epoch": 0.9769105691056911,
      "step": 4506,
      "training_loss": 5.571274757385254
    },
    {
      "epoch": 0.9771273712737127,
      "step": 4507,
      "training_loss": 3.404672622680664
    },
    {
      "epoch": 0.9771273712737127,
      "step": 4507,
      "training_loss": 8.207868576049805
    },
    {
      "epoch": 0.9771273712737127,
      "step": 4507,
      "training_loss": 3.591679573059082
    },
    {
      "epoch": 0.9771273712737127,
      "step": 4507,
      "training_loss": 6.3352155685424805
    },
    {
      "epoch": 0.9773441734417344,
      "grad_norm": 23.062957763671875,
      "learning_rate": 1e-05,
      "loss": 5.8337,
      "step": 4508
    },
    {
      "epoch": 0.9773441734417344,
      "step": 4508,
      "training_loss": 4.117406845092773
    },
    {
      "epoch": 0.9773441734417344,
      "step": 4508,
      "training_loss": 5.682272434234619
    },
    {
      "epoch": 0.9773441734417344,
      "step": 4508,
      "training_loss": 7.333215236663818
    },
    {
      "epoch": 0.9773441734417344,
      "step": 4508,
      "training_loss": 7.043338775634766
    },
    {
      "epoch": 0.9775609756097561,
      "step": 4509,
      "training_loss": 4.517765998840332
    },
    {
      "epoch": 0.9775609756097561,
      "step": 4509,
      "training_loss": 7.140654563903809
    },
    {
      "epoch": 0.9775609756097561,
      "step": 4509,
      "training_loss": 4.980669021606445
    },
    {
      "epoch": 0.9775609756097561,
      "step": 4509,
      "training_loss": 7.5897674560546875
    },
    {
      "epoch": 0.9777777777777777,
      "step": 4510,
      "training_loss": 6.648586750030518
    },
    {
      "epoch": 0.9777777777777777,
      "step": 4510,
      "training_loss": 5.763818740844727
    },
    {
      "epoch": 0.9777777777777777,
      "step": 4510,
      "training_loss": 6.050138473510742
    },
    {
      "epoch": 0.9777777777777777,
      "step": 4510,
      "training_loss": 6.533323764801025
    },
    {
      "epoch": 0.9779945799457994,
      "step": 4511,
      "training_loss": 7.292541027069092
    },
    {
      "epoch": 0.9779945799457994,
      "step": 4511,
      "training_loss": 5.701608180999756
    },
    {
      "epoch": 0.9779945799457994,
      "step": 4511,
      "training_loss": 6.066136360168457
    },
    {
      "epoch": 0.9779945799457994,
      "step": 4511,
      "training_loss": 6.928234577178955
    },
    {
      "epoch": 0.9782113821138212,
      "grad_norm": 21.03023338317871,
      "learning_rate": 1e-05,
      "loss": 6.2118,
      "step": 4512
    },
    {
      "epoch": 0.9782113821138212,
      "step": 4512,
      "training_loss": 5.079522609710693
    },
    {
      "epoch": 0.9782113821138212,
      "step": 4512,
      "training_loss": 7.0895819664001465
    },
    {
      "epoch": 0.9782113821138212,
      "step": 4512,
      "training_loss": 6.806350231170654
    },
    {
      "epoch": 0.9782113821138212,
      "step": 4512,
      "training_loss": 6.072114944458008
    },
    {
      "epoch": 0.9784281842818429,
      "step": 4513,
      "training_loss": 6.643428325653076
    },
    {
      "epoch": 0.9784281842818429,
      "step": 4513,
      "training_loss": 7.359714984893799
    },
    {
      "epoch": 0.9784281842818429,
      "step": 4513,
      "training_loss": 6.7832722663879395
    },
    {
      "epoch": 0.9784281842818429,
      "step": 4513,
      "training_loss": 6.885186672210693
    },
    {
      "epoch": 0.9786449864498645,
      "step": 4514,
      "training_loss": 5.602437973022461
    },
    {
      "epoch": 0.9786449864498645,
      "step": 4514,
      "training_loss": 7.1287617683410645
    },
    {
      "epoch": 0.9786449864498645,
      "step": 4514,
      "training_loss": 5.623751640319824
    },
    {
      "epoch": 0.9786449864498645,
      "step": 4514,
      "training_loss": 7.26678991317749
    },
    {
      "epoch": 0.9788617886178862,
      "step": 4515,
      "training_loss": 6.646383762359619
    },
    {
      "epoch": 0.9788617886178862,
      "step": 4515,
      "training_loss": 5.903814792633057
    },
    {
      "epoch": 0.9788617886178862,
      "step": 4515,
      "training_loss": 6.825092315673828
    },
    {
      "epoch": 0.9788617886178862,
      "step": 4515,
      "training_loss": 6.334533214569092
    },
    {
      "epoch": 0.9790785907859079,
      "grad_norm": 17.25689125061035,
      "learning_rate": 1e-05,
      "loss": 6.5032,
      "step": 4516
    },
    {
      "epoch": 0.9790785907859079,
      "step": 4516,
      "training_loss": 8.743431091308594
    },
    {
      "epoch": 0.9790785907859079,
      "step": 4516,
      "training_loss": 4.941938400268555
    },
    {
      "epoch": 0.9790785907859079,
      "step": 4516,
      "training_loss": 6.670263767242432
    },
    {
      "epoch": 0.9790785907859079,
      "step": 4516,
      "training_loss": 6.135982990264893
    },
    {
      "epoch": 0.9792953929539295,
      "step": 4517,
      "training_loss": 6.853857517242432
    },
    {
      "epoch": 0.9792953929539295,
      "step": 4517,
      "training_loss": 5.546393394470215
    },
    {
      "epoch": 0.9792953929539295,
      "step": 4517,
      "training_loss": 5.561917304992676
    },
    {
      "epoch": 0.9792953929539295,
      "step": 4517,
      "training_loss": 7.481860637664795
    },
    {
      "epoch": 0.9795121951219512,
      "step": 4518,
      "training_loss": 6.50181245803833
    },
    {
      "epoch": 0.9795121951219512,
      "step": 4518,
      "training_loss": 5.920609474182129
    },
    {
      "epoch": 0.9795121951219512,
      "step": 4518,
      "training_loss": 7.978573799133301
    },
    {
      "epoch": 0.9795121951219512,
      "step": 4518,
      "training_loss": 4.597091197967529
    },
    {
      "epoch": 0.9797289972899729,
      "step": 4519,
      "training_loss": 5.391407489776611
    },
    {
      "epoch": 0.9797289972899729,
      "step": 4519,
      "training_loss": 6.582146644592285
    },
    {
      "epoch": 0.9797289972899729,
      "step": 4519,
      "training_loss": 6.501794815063477
    },
    {
      "epoch": 0.9797289972899729,
      "step": 4519,
      "training_loss": 6.636150360107422
    },
    {
      "epoch": 0.9799457994579945,
      "grad_norm": 18.651586532592773,
      "learning_rate": 1e-05,
      "loss": 6.3778,
      "step": 4520
    },
    {
      "epoch": 0.9799457994579945,
      "step": 4520,
      "training_loss": 6.411377906799316
    },
    {
      "epoch": 0.9799457994579945,
      "step": 4520,
      "training_loss": 5.752857208251953
    },
    {
      "epoch": 0.9799457994579945,
      "step": 4520,
      "training_loss": 7.130402088165283
    },
    {
      "epoch": 0.9799457994579945,
      "step": 4520,
      "training_loss": 6.176347732543945
    },
    {
      "epoch": 0.9801626016260162,
      "step": 4521,
      "training_loss": 6.958113670349121
    },
    {
      "epoch": 0.9801626016260162,
      "step": 4521,
      "training_loss": 5.779644012451172
    },
    {
      "epoch": 0.9801626016260162,
      "step": 4521,
      "training_loss": 6.961277484893799
    },
    {
      "epoch": 0.9801626016260162,
      "step": 4521,
      "training_loss": 5.5770978927612305
    },
    {
      "epoch": 0.980379403794038,
      "step": 4522,
      "training_loss": 6.867965221405029
    },
    {
      "epoch": 0.980379403794038,
      "step": 4522,
      "training_loss": 5.61922025680542
    },
    {
      "epoch": 0.980379403794038,
      "step": 4522,
      "training_loss": 6.568108081817627
    },
    {
      "epoch": 0.980379403794038,
      "step": 4522,
      "training_loss": 6.192201614379883
    },
    {
      "epoch": 0.9805962059620597,
      "step": 4523,
      "training_loss": 5.436417579650879
    },
    {
      "epoch": 0.9805962059620597,
      "step": 4523,
      "training_loss": 6.605888843536377
    },
    {
      "epoch": 0.9805962059620597,
      "step": 4523,
      "training_loss": 6.2305426597595215
    },
    {
      "epoch": 0.9805962059620597,
      "step": 4523,
      "training_loss": 3.511409282684326
    },
    {
      "epoch": 0.9808130081300813,
      "grad_norm": 16.162738800048828,
      "learning_rate": 1e-05,
      "loss": 6.1112,
      "step": 4524
    },
    {
      "epoch": 0.9808130081300813,
      "step": 4524,
      "training_loss": 5.905764102935791
    },
    {
      "epoch": 0.9808130081300813,
      "step": 4524,
      "training_loss": 2.764482259750366
    },
    {
      "epoch": 0.9808130081300813,
      "step": 4524,
      "training_loss": 6.251266002655029
    },
    {
      "epoch": 0.9808130081300813,
      "step": 4524,
      "training_loss": 6.834895133972168
    },
    {
      "epoch": 0.981029810298103,
      "step": 4525,
      "training_loss": 3.4477972984313965
    },
    {
      "epoch": 0.981029810298103,
      "step": 4525,
      "training_loss": 3.809272527694702
    },
    {
      "epoch": 0.981029810298103,
      "step": 4525,
      "training_loss": 6.042511940002441
    },
    {
      "epoch": 0.981029810298103,
      "step": 4525,
      "training_loss": 6.983896255493164
    },
    {
      "epoch": 0.9812466124661247,
      "step": 4526,
      "training_loss": 5.0968098640441895
    },
    {
      "epoch": 0.9812466124661247,
      "step": 4526,
      "training_loss": 7.060394763946533
    },
    {
      "epoch": 0.9812466124661247,
      "step": 4526,
      "training_loss": 7.45657205581665
    },
    {
      "epoch": 0.9812466124661247,
      "step": 4526,
      "training_loss": 6.427067756652832
    },
    {
      "epoch": 0.9814634146341463,
      "step": 4527,
      "training_loss": 7.093878269195557
    },
    {
      "epoch": 0.9814634146341463,
      "step": 4527,
      "training_loss": 6.526552677154541
    },
    {
      "epoch": 0.9814634146341463,
      "step": 4527,
      "training_loss": 6.471766948699951
    },
    {
      "epoch": 0.9814634146341463,
      "step": 4527,
      "training_loss": 7.134654521942139
    },
    {
      "epoch": 0.981680216802168,
      "grad_norm": 26.224109649658203,
      "learning_rate": 1e-05,
      "loss": 5.9567,
      "step": 4528
    },
    {
      "epoch": 0.981680216802168,
      "step": 4528,
      "training_loss": 6.87442684173584
    },
    {
      "epoch": 0.981680216802168,
      "step": 4528,
      "training_loss": 5.60140323638916
    },
    {
      "epoch": 0.981680216802168,
      "step": 4528,
      "training_loss": 6.2252278327941895
    },
    {
      "epoch": 0.981680216802168,
      "step": 4528,
      "training_loss": 7.705708980560303
    },
    {
      "epoch": 0.9818970189701897,
      "step": 4529,
      "training_loss": 6.648471355438232
    },
    {
      "epoch": 0.9818970189701897,
      "step": 4529,
      "training_loss": 6.7376322746276855
    },
    {
      "epoch": 0.9818970189701897,
      "step": 4529,
      "training_loss": 7.970848560333252
    },
    {
      "epoch": 0.9818970189701897,
      "step": 4529,
      "training_loss": 8.306159973144531
    },
    {
      "epoch": 0.9821138211382113,
      "step": 4530,
      "training_loss": 7.102391242980957
    },
    {
      "epoch": 0.9821138211382113,
      "step": 4530,
      "training_loss": 5.94236946105957
    },
    {
      "epoch": 0.9821138211382113,
      "step": 4530,
      "training_loss": 5.474794864654541
    },
    {
      "epoch": 0.9821138211382113,
      "step": 4530,
      "training_loss": 6.374940872192383
    },
    {
      "epoch": 0.9823306233062331,
      "step": 4531,
      "training_loss": 6.591527462005615
    },
    {
      "epoch": 0.9823306233062331,
      "step": 4531,
      "training_loss": 5.045903205871582
    },
    {
      "epoch": 0.9823306233062331,
      "step": 4531,
      "training_loss": 9.043766021728516
    },
    {
      "epoch": 0.9823306233062331,
      "step": 4531,
      "training_loss": 7.057464122772217
    },
    {
      "epoch": 0.9825474254742548,
      "grad_norm": 23.212753295898438,
      "learning_rate": 1e-05,
      "loss": 6.7939,
      "step": 4532
    },
    {
      "epoch": 0.9825474254742548,
      "step": 4532,
      "training_loss": 6.765746116638184
    },
    {
      "epoch": 0.9825474254742548,
      "step": 4532,
      "training_loss": 6.265703201293945
    },
    {
      "epoch": 0.9825474254742548,
      "step": 4532,
      "training_loss": 6.3255228996276855
    },
    {
      "epoch": 0.9825474254742548,
      "step": 4532,
      "training_loss": 7.4123077392578125
    },
    {
      "epoch": 0.9827642276422764,
      "step": 4533,
      "training_loss": 6.577259540557861
    },
    {
      "epoch": 0.9827642276422764,
      "step": 4533,
      "training_loss": 6.425433158874512
    },
    {
      "epoch": 0.9827642276422764,
      "step": 4533,
      "training_loss": 7.107419967651367
    },
    {
      "epoch": 0.9827642276422764,
      "step": 4533,
      "training_loss": 6.509322643280029
    },
    {
      "epoch": 0.9829810298102981,
      "step": 4534,
      "training_loss": 5.671825885772705
    },
    {
      "epoch": 0.9829810298102981,
      "step": 4534,
      "training_loss": 5.192849636077881
    },
    {
      "epoch": 0.9829810298102981,
      "step": 4534,
      "training_loss": 6.50614070892334
    },
    {
      "epoch": 0.9829810298102981,
      "step": 4534,
      "training_loss": 6.140590667724609
    },
    {
      "epoch": 0.9831978319783198,
      "step": 4535,
      "training_loss": 6.244110584259033
    },
    {
      "epoch": 0.9831978319783198,
      "step": 4535,
      "training_loss": 6.809598922729492
    },
    {
      "epoch": 0.9831978319783198,
      "step": 4535,
      "training_loss": 7.458569049835205
    },
    {
      "epoch": 0.9831978319783198,
      "step": 4535,
      "training_loss": 7.490188121795654
    },
    {
      "epoch": 0.9834146341463414,
      "grad_norm": 17.764734268188477,
      "learning_rate": 1e-05,
      "loss": 6.5564,
      "step": 4536
    },
    {
      "epoch": 0.9834146341463414,
      "step": 4536,
      "training_loss": 6.32749605178833
    },
    {
      "epoch": 0.9834146341463414,
      "step": 4536,
      "training_loss": 7.319356441497803
    },
    {
      "epoch": 0.9834146341463414,
      "step": 4536,
      "training_loss": 6.846190929412842
    },
    {
      "epoch": 0.9834146341463414,
      "step": 4536,
      "training_loss": 7.577205181121826
    },
    {
      "epoch": 0.9836314363143631,
      "step": 4537,
      "training_loss": 7.64093542098999
    },
    {
      "epoch": 0.9836314363143631,
      "step": 4537,
      "training_loss": 5.0675787925720215
    },
    {
      "epoch": 0.9836314363143631,
      "step": 4537,
      "training_loss": 6.3180389404296875
    },
    {
      "epoch": 0.9836314363143631,
      "step": 4537,
      "training_loss": 5.050857067108154
    },
    {
      "epoch": 0.9838482384823848,
      "step": 4538,
      "training_loss": 6.1364054679870605
    },
    {
      "epoch": 0.9838482384823848,
      "step": 4538,
      "training_loss": 6.327029705047607
    },
    {
      "epoch": 0.9838482384823848,
      "step": 4538,
      "training_loss": 6.951027870178223
    },
    {
      "epoch": 0.9838482384823848,
      "step": 4538,
      "training_loss": 6.956125259399414
    },
    {
      "epoch": 0.9840650406504065,
      "step": 4539,
      "training_loss": 6.175394535064697
    },
    {
      "epoch": 0.9840650406504065,
      "step": 4539,
      "training_loss": 6.629508972167969
    },
    {
      "epoch": 0.9840650406504065,
      "step": 4539,
      "training_loss": 5.413086414337158
    },
    {
      "epoch": 0.9840650406504065,
      "step": 4539,
      "training_loss": 2.6450247764587402
    },
    {
      "epoch": 0.9842818428184282,
      "grad_norm": 16.73105239868164,
      "learning_rate": 1e-05,
      "loss": 6.2113,
      "step": 4540
    },
    {
      "epoch": 0.9842818428184282,
      "step": 4540,
      "training_loss": 6.564505577087402
    },
    {
      "epoch": 0.9842818428184282,
      "step": 4540,
      "training_loss": 7.3895158767700195
    },
    {
      "epoch": 0.9842818428184282,
      "step": 4540,
      "training_loss": 6.569551944732666
    },
    {
      "epoch": 0.9842818428184282,
      "step": 4540,
      "training_loss": 4.197332382202148
    },
    {
      "epoch": 0.9844986449864499,
      "step": 4541,
      "training_loss": 4.149771213531494
    },
    {
      "epoch": 0.9844986449864499,
      "step": 4541,
      "training_loss": 7.048327922821045
    },
    {
      "epoch": 0.9844986449864499,
      "step": 4541,
      "training_loss": 6.916790962219238
    },
    {
      "epoch": 0.9844986449864499,
      "step": 4541,
      "training_loss": 6.69897985458374
    },
    {
      "epoch": 0.9847154471544716,
      "step": 4542,
      "training_loss": 6.550267219543457
    },
    {
      "epoch": 0.9847154471544716,
      "step": 4542,
      "training_loss": 5.706997871398926
    },
    {
      "epoch": 0.9847154471544716,
      "step": 4542,
      "training_loss": 6.517879009246826
    },
    {
      "epoch": 0.9847154471544716,
      "step": 4542,
      "training_loss": 7.29557991027832
    },
    {
      "epoch": 0.9849322493224932,
      "step": 4543,
      "training_loss": 6.550863265991211
    },
    {
      "epoch": 0.9849322493224932,
      "step": 4543,
      "training_loss": 6.047408580780029
    },
    {
      "epoch": 0.9849322493224932,
      "step": 4543,
      "training_loss": 5.100647449493408
    },
    {
      "epoch": 0.9849322493224932,
      "step": 4543,
      "training_loss": 6.583555221557617
    },
    {
      "epoch": 0.9851490514905149,
      "grad_norm": 23.13645362854004,
      "learning_rate": 1e-05,
      "loss": 6.243,
      "step": 4544
    },
    {
      "epoch": 0.9851490514905149,
      "step": 4544,
      "training_loss": 5.511692047119141
    },
    {
      "epoch": 0.9851490514905149,
      "step": 4544,
      "training_loss": 7.739587306976318
    },
    {
      "epoch": 0.9851490514905149,
      "step": 4544,
      "training_loss": 6.378409385681152
    },
    {
      "epoch": 0.9851490514905149,
      "step": 4544,
      "training_loss": 7.217565059661865
    },
    {
      "epoch": 0.9853658536585366,
      "step": 4545,
      "training_loss": 6.794812202453613
    },
    {
      "epoch": 0.9853658536585366,
      "step": 4545,
      "training_loss": 2.922959566116333
    },
    {
      "epoch": 0.9853658536585366,
      "step": 4545,
      "training_loss": 6.945001602172852
    },
    {
      "epoch": 0.9853658536585366,
      "step": 4545,
      "training_loss": 6.5046257972717285
    },
    {
      "epoch": 0.9855826558265582,
      "step": 4546,
      "training_loss": 6.876295566558838
    },
    {
      "epoch": 0.9855826558265582,
      "step": 4546,
      "training_loss": 6.695793628692627
    },
    {
      "epoch": 0.9855826558265582,
      "step": 4546,
      "training_loss": 3.3440635204315186
    },
    {
      "epoch": 0.9855826558265582,
      "step": 4546,
      "training_loss": 4.057849407196045
    },
    {
      "epoch": 0.9857994579945799,
      "step": 4547,
      "training_loss": 5.0223164558410645
    },
    {
      "epoch": 0.9857994579945799,
      "step": 4547,
      "training_loss": 5.95048713684082
    },
    {
      "epoch": 0.9857994579945799,
      "step": 4547,
      "training_loss": 6.383026123046875
    },
    {
      "epoch": 0.9857994579945799,
      "step": 4547,
      "training_loss": 6.614714622497559
    },
    {
      "epoch": 0.9860162601626016,
      "grad_norm": 20.74634552001953,
      "learning_rate": 1e-05,
      "loss": 5.9349,
      "step": 4548
    },
    {
      "epoch": 0.9860162601626016,
      "step": 4548,
      "training_loss": 6.323328018188477
    },
    {
      "epoch": 0.9860162601626016,
      "step": 4548,
      "training_loss": 4.052883625030518
    },
    {
      "epoch": 0.9860162601626016,
      "step": 4548,
      "training_loss": 5.643504619598389
    },
    {
      "epoch": 0.9860162601626016,
      "step": 4548,
      "training_loss": 5.653629779815674
    },
    {
      "epoch": 0.9862330623306234,
      "step": 4549,
      "training_loss": 6.641551971435547
    },
    {
      "epoch": 0.9862330623306234,
      "step": 4549,
      "training_loss": 5.381260395050049
    },
    {
      "epoch": 0.9862330623306234,
      "step": 4549,
      "training_loss": 6.356293201446533
    },
    {
      "epoch": 0.9862330623306234,
      "step": 4549,
      "training_loss": 6.646152973175049
    },
    {
      "epoch": 0.986449864498645,
      "step": 4550,
      "training_loss": 6.061183452606201
    },
    {
      "epoch": 0.986449864498645,
      "step": 4550,
      "training_loss": 6.525218486785889
    },
    {
      "epoch": 0.986449864498645,
      "step": 4550,
      "training_loss": 7.44583797454834
    },
    {
      "epoch": 0.986449864498645,
      "step": 4550,
      "training_loss": 6.712320804595947
    },
    {
      "epoch": 0.9866666666666667,
      "step": 4551,
      "training_loss": 5.351509094238281
    },
    {
      "epoch": 0.9866666666666667,
      "step": 4551,
      "training_loss": 6.941788196563721
    },
    {
      "epoch": 0.9866666666666667,
      "step": 4551,
      "training_loss": 2.6836135387420654
    },
    {
      "epoch": 0.9866666666666667,
      "step": 4551,
      "training_loss": 4.370605945587158
    },
    {
      "epoch": 0.9868834688346884,
      "grad_norm": 19.778356552124023,
      "learning_rate": 1e-05,
      "loss": 5.7994,
      "step": 4552
    },
    {
      "epoch": 0.9868834688346884,
      "step": 4552,
      "training_loss": 2.8946235179901123
    },
    {
      "epoch": 0.9868834688346884,
      "step": 4552,
      "training_loss": 6.234950542449951
    },
    {
      "epoch": 0.9868834688346884,
      "step": 4552,
      "training_loss": 2.9734201431274414
    },
    {
      "epoch": 0.9868834688346884,
      "step": 4552,
      "training_loss": 5.40742301940918
    },
    {
      "epoch": 0.98710027100271,
      "step": 4553,
      "training_loss": 5.600057601928711
    },
    {
      "epoch": 0.98710027100271,
      "step": 4553,
      "training_loss": 4.234644889831543
    },
    {
      "epoch": 0.98710027100271,
      "step": 4553,
      "training_loss": 5.5444416999816895
    },
    {
      "epoch": 0.98710027100271,
      "step": 4553,
      "training_loss": 3.777971029281616
    },
    {
      "epoch": 0.9873170731707317,
      "step": 4554,
      "training_loss": 7.002318382263184
    },
    {
      "epoch": 0.9873170731707317,
      "step": 4554,
      "training_loss": 7.895789623260498
    },
    {
      "epoch": 0.9873170731707317,
      "step": 4554,
      "training_loss": 7.153531074523926
    },
    {
      "epoch": 0.9873170731707317,
      "step": 4554,
      "training_loss": 5.990425109863281
    },
    {
      "epoch": 0.9875338753387534,
      "step": 4555,
      "training_loss": 6.505523204803467
    },
    {
      "epoch": 0.9875338753387534,
      "step": 4555,
      "training_loss": 8.227754592895508
    },
    {
      "epoch": 0.9875338753387534,
      "step": 4555,
      "training_loss": 7.52094030380249
    },
    {
      "epoch": 0.9875338753387534,
      "step": 4555,
      "training_loss": 6.151606559753418
    },
    {
      "epoch": 0.987750677506775,
      "grad_norm": 22.580318450927734,
      "learning_rate": 1e-05,
      "loss": 5.8197,
      "step": 4556
    },
    {
      "epoch": 0.987750677506775,
      "step": 4556,
      "training_loss": 6.29682731628418
    },
    {
      "epoch": 0.987750677506775,
      "step": 4556,
      "training_loss": 6.474862575531006
    },
    {
      "epoch": 0.987750677506775,
      "step": 4556,
      "training_loss": 7.113088607788086
    },
    {
      "epoch": 0.987750677506775,
      "step": 4556,
      "training_loss": 6.287235260009766
    },
    {
      "epoch": 0.9879674796747967,
      "step": 4557,
      "training_loss": 5.666466236114502
    },
    {
      "epoch": 0.9879674796747967,
      "step": 4557,
      "training_loss": 5.1754045486450195
    },
    {
      "epoch": 0.9879674796747967,
      "step": 4557,
      "training_loss": 5.901864528656006
    },
    {
      "epoch": 0.9879674796747967,
      "step": 4557,
      "training_loss": 5.824640274047852
    },
    {
      "epoch": 0.9881842818428185,
      "step": 4558,
      "training_loss": 7.227992057800293
    },
    {
      "epoch": 0.9881842818428185,
      "step": 4558,
      "training_loss": 6.7101545333862305
    },
    {
      "epoch": 0.9881842818428185,
      "step": 4558,
      "training_loss": 7.285681247711182
    },
    {
      "epoch": 0.9881842818428185,
      "step": 4558,
      "training_loss": 6.883785724639893
    },
    {
      "epoch": 0.9884010840108401,
      "step": 4559,
      "training_loss": 7.792436122894287
    },
    {
      "epoch": 0.9884010840108401,
      "step": 4559,
      "training_loss": 6.6670637130737305
    },
    {
      "epoch": 0.9884010840108401,
      "step": 4559,
      "training_loss": 7.489134311676025
    },
    {
      "epoch": 0.9884010840108401,
      "step": 4559,
      "training_loss": 7.042270660400391
    },
    {
      "epoch": 0.9886178861788618,
      "grad_norm": 20.05077362060547,
      "learning_rate": 1e-05,
      "loss": 6.6149,
      "step": 4560
    },
    {
      "epoch": 0.9886178861788618,
      "step": 4560,
      "training_loss": 5.981196403503418
    },
    {
      "epoch": 0.9886178861788618,
      "step": 4560,
      "training_loss": 6.978512763977051
    },
    {
      "epoch": 0.9886178861788618,
      "step": 4560,
      "training_loss": 5.177371025085449
    },
    {
      "epoch": 0.9886178861788618,
      "step": 4560,
      "training_loss": 7.5445556640625
    },
    {
      "epoch": 0.9888346883468835,
      "step": 4561,
      "training_loss": 6.076086521148682
    },
    {
      "epoch": 0.9888346883468835,
      "step": 4561,
      "training_loss": 6.810035705566406
    },
    {
      "epoch": 0.9888346883468835,
      "step": 4561,
      "training_loss": 5.8088812828063965
    },
    {
      "epoch": 0.9888346883468835,
      "step": 4561,
      "training_loss": 6.226032733917236
    },
    {
      "epoch": 0.9890514905149052,
      "step": 4562,
      "training_loss": 6.925224781036377
    },
    {
      "epoch": 0.9890514905149052,
      "step": 4562,
      "training_loss": 6.208822250366211
    },
    {
      "epoch": 0.9890514905149052,
      "step": 4562,
      "training_loss": 6.714170932769775
    },
    {
      "epoch": 0.9890514905149052,
      "step": 4562,
      "training_loss": 7.120047092437744
    },
    {
      "epoch": 0.9892682926829268,
      "step": 4563,
      "training_loss": 8.421834945678711
    },
    {
      "epoch": 0.9892682926829268,
      "step": 4563,
      "training_loss": 6.9939117431640625
    },
    {
      "epoch": 0.9892682926829268,
      "step": 4563,
      "training_loss": 6.3174214363098145
    },
    {
      "epoch": 0.9892682926829268,
      "step": 4563,
      "training_loss": 7.778575420379639
    },
    {
      "epoch": 0.9894850948509485,
      "grad_norm": 16.048555374145508,
      "learning_rate": 1e-05,
      "loss": 6.6927,
      "step": 4564
    },
    {
      "epoch": 0.9894850948509485,
      "step": 4564,
      "training_loss": 6.861719131469727
    },
    {
      "epoch": 0.9894850948509485,
      "step": 4564,
      "training_loss": 4.478484630584717
    },
    {
      "epoch": 0.9894850948509485,
      "step": 4564,
      "training_loss": 7.544994831085205
    },
    {
      "epoch": 0.9894850948509485,
      "step": 4564,
      "training_loss": 6.319096565246582
    },
    {
      "epoch": 0.9897018970189702,
      "step": 4565,
      "training_loss": 6.71639347076416
    },
    {
      "epoch": 0.9897018970189702,
      "step": 4565,
      "training_loss": 6.879515171051025
    },
    {
      "epoch": 0.9897018970189702,
      "step": 4565,
      "training_loss": 7.431502819061279
    },
    {
      "epoch": 0.9897018970189702,
      "step": 4565,
      "training_loss": 7.167835712432861
    },
    {
      "epoch": 0.9899186991869918,
      "step": 4566,
      "training_loss": 6.829096794128418
    },
    {
      "epoch": 0.9899186991869918,
      "step": 4566,
      "training_loss": 5.309688568115234
    },
    {
      "epoch": 0.9899186991869918,
      "step": 4566,
      "training_loss": 6.694605350494385
    },
    {
      "epoch": 0.9899186991869918,
      "step": 4566,
      "training_loss": 6.630231857299805
    },
    {
      "epoch": 0.9901355013550136,
      "step": 4567,
      "training_loss": 6.9962263107299805
    },
    {
      "epoch": 0.9901355013550136,
      "step": 4567,
      "training_loss": 5.090122222900391
    },
    {
      "epoch": 0.9901355013550136,
      "step": 4567,
      "training_loss": 2.599315643310547
    },
    {
      "epoch": 0.9901355013550136,
      "step": 4567,
      "training_loss": 7.0121941566467285
    },
    {
      "epoch": 0.9903523035230353,
      "grad_norm": 22.841144561767578,
      "learning_rate": 1e-05,
      "loss": 6.2851,
      "step": 4568
    },
    {
      "epoch": 0.9903523035230353,
      "step": 4568,
      "training_loss": 6.449639797210693
    },
    {
      "epoch": 0.9903523035230353,
      "step": 4568,
      "training_loss": 7.2291741371154785
    },
    {
      "epoch": 0.9903523035230353,
      "step": 4568,
      "training_loss": 5.816399097442627
    },
    {
      "epoch": 0.9903523035230353,
      "step": 4568,
      "training_loss": 6.255155086517334
    },
    {
      "epoch": 0.9905691056910569,
      "step": 4569,
      "training_loss": 5.821771621704102
    },
    {
      "epoch": 0.9905691056910569,
      "step": 4569,
      "training_loss": 6.3489909172058105
    },
    {
      "epoch": 0.9905691056910569,
      "step": 4569,
      "training_loss": 5.2022504806518555
    },
    {
      "epoch": 0.9905691056910569,
      "step": 4569,
      "training_loss": 5.435718059539795
    },
    {
      "epoch": 0.9907859078590786,
      "step": 4570,
      "training_loss": 5.165410041809082
    },
    {
      "epoch": 0.9907859078590786,
      "step": 4570,
      "training_loss": 8.319300651550293
    },
    {
      "epoch": 0.9907859078590786,
      "step": 4570,
      "training_loss": 5.942105293273926
    },
    {
      "epoch": 0.9907859078590786,
      "step": 4570,
      "training_loss": 6.966240882873535
    },
    {
      "epoch": 0.9910027100271003,
      "step": 4571,
      "training_loss": 6.579546928405762
    },
    {
      "epoch": 0.9910027100271003,
      "step": 4571,
      "training_loss": 5.425154209136963
    },
    {
      "epoch": 0.9910027100271003,
      "step": 4571,
      "training_loss": 4.913035869598389
    },
    {
      "epoch": 0.9910027100271003,
      "step": 4571,
      "training_loss": 8.59609317779541
    },
    {
      "epoch": 0.9912195121951219,
      "grad_norm": 24.93427085876465,
      "learning_rate": 1e-05,
      "loss": 6.2791,
      "step": 4572
    },
    {
      "epoch": 0.9912195121951219,
      "step": 4572,
      "training_loss": 5.080141544342041
    },
    {
      "epoch": 0.9912195121951219,
      "step": 4572,
      "training_loss": 6.432394504547119
    },
    {
      "epoch": 0.9912195121951219,
      "step": 4572,
      "training_loss": 7.531096458435059
    },
    {
      "epoch": 0.9912195121951219,
      "step": 4572,
      "training_loss": 6.09142541885376
    },
    {
      "epoch": 0.9914363143631436,
      "step": 4573,
      "training_loss": 7.237880229949951
    },
    {
      "epoch": 0.9914363143631436,
      "step": 4573,
      "training_loss": 7.25224494934082
    },
    {
      "epoch": 0.9914363143631436,
      "step": 4573,
      "training_loss": 6.8035430908203125
    },
    {
      "epoch": 0.9914363143631436,
      "step": 4573,
      "training_loss": 6.362974166870117
    },
    {
      "epoch": 0.9916531165311653,
      "step": 4574,
      "training_loss": 6.657947063446045
    },
    {
      "epoch": 0.9916531165311653,
      "step": 4574,
      "training_loss": 4.796390533447266
    },
    {
      "epoch": 0.9916531165311653,
      "step": 4574,
      "training_loss": 7.363003730773926
    },
    {
      "epoch": 0.9916531165311653,
      "step": 4574,
      "training_loss": 5.845931053161621
    },
    {
      "epoch": 0.991869918699187,
      "step": 4575,
      "training_loss": 5.903975963592529
    },
    {
      "epoch": 0.991869918699187,
      "step": 4575,
      "training_loss": 3.2192869186401367
    },
    {
      "epoch": 0.991869918699187,
      "step": 4575,
      "training_loss": 6.530503273010254
    },
    {
      "epoch": 0.991869918699187,
      "step": 4575,
      "training_loss": 7.465349197387695
    },
    {
      "epoch": 0.9920867208672087,
      "grad_norm": 19.690214157104492,
      "learning_rate": 1e-05,
      "loss": 6.2859,
      "step": 4576
    },
    {
      "epoch": 0.9920867208672087,
      "step": 4576,
      "training_loss": 5.618861675262451
    },
    {
      "epoch": 0.9920867208672087,
      "step": 4576,
      "training_loss": 6.842522621154785
    },
    {
      "epoch": 0.9920867208672087,
      "step": 4576,
      "training_loss": 6.321353912353516
    },
    {
      "epoch": 0.9920867208672087,
      "step": 4576,
      "training_loss": 7.7601118087768555
    },
    {
      "epoch": 0.9923035230352304,
      "step": 4577,
      "training_loss": 6.560825347900391
    },
    {
      "epoch": 0.9923035230352304,
      "step": 4577,
      "training_loss": 7.565639019012451
    },
    {
      "epoch": 0.9923035230352304,
      "step": 4577,
      "training_loss": 7.087453365325928
    },
    {
      "epoch": 0.9923035230352304,
      "step": 4577,
      "training_loss": 7.402585983276367
    },
    {
      "epoch": 0.9925203252032521,
      "step": 4578,
      "training_loss": 5.575554847717285
    },
    {
      "epoch": 0.9925203252032521,
      "step": 4578,
      "training_loss": 3.5617449283599854
    },
    {
      "epoch": 0.9925203252032521,
      "step": 4578,
      "training_loss": 7.336010456085205
    },
    {
      "epoch": 0.9925203252032521,
      "step": 4578,
      "training_loss": 5.539090156555176
    },
    {
      "epoch": 0.9927371273712737,
      "step": 4579,
      "training_loss": 6.264956474304199
    },
    {
      "epoch": 0.9927371273712737,
      "step": 4579,
      "training_loss": 6.839603424072266
    },
    {
      "epoch": 0.9927371273712737,
      "step": 4579,
      "training_loss": 7.198457717895508
    },
    {
      "epoch": 0.9927371273712737,
      "step": 4579,
      "training_loss": 2.706765651702881
    },
    {
      "epoch": 0.9929539295392954,
      "grad_norm": 12.397870063781738,
      "learning_rate": 1e-05,
      "loss": 6.2613,
      "step": 4580
    },
    {
      "epoch": 0.9929539295392954,
      "step": 4580,
      "training_loss": 7.241779327392578
    },
    {
      "epoch": 0.9929539295392954,
      "step": 4580,
      "training_loss": 6.694146156311035
    },
    {
      "epoch": 0.9929539295392954,
      "step": 4580,
      "training_loss": 6.5483245849609375
    },
    {
      "epoch": 0.9929539295392954,
      "step": 4580,
      "training_loss": 7.028842449188232
    },
    {
      "epoch": 0.9931707317073171,
      "step": 4581,
      "training_loss": 6.9834065437316895
    },
    {
      "epoch": 0.9931707317073171,
      "step": 4581,
      "training_loss": 4.315793991088867
    },
    {
      "epoch": 0.9931707317073171,
      "step": 4581,
      "training_loss": 6.4736104011535645
    },
    {
      "epoch": 0.9931707317073171,
      "step": 4581,
      "training_loss": 6.098608493804932
    },
    {
      "epoch": 0.9933875338753387,
      "step": 4582,
      "training_loss": 5.6648969650268555
    },
    {
      "epoch": 0.9933875338753387,
      "step": 4582,
      "training_loss": 6.345648765563965
    },
    {
      "epoch": 0.9933875338753387,
      "step": 4582,
      "training_loss": 6.9640984535217285
    },
    {
      "epoch": 0.9933875338753387,
      "step": 4582,
      "training_loss": 6.732579708099365
    },
    {
      "epoch": 0.9936043360433604,
      "step": 4583,
      "training_loss": 6.983828067779541
    },
    {
      "epoch": 0.9936043360433604,
      "step": 4583,
      "training_loss": 6.765458106994629
    },
    {
      "epoch": 0.9936043360433604,
      "step": 4583,
      "training_loss": 4.739070415496826
    },
    {
      "epoch": 0.9936043360433604,
      "step": 4583,
      "training_loss": 6.261315822601318
    },
    {
      "epoch": 0.9938211382113821,
      "grad_norm": 27.256717681884766,
      "learning_rate": 1e-05,
      "loss": 6.3651,
      "step": 4584
    },
    {
      "epoch": 0.9938211382113821,
      "step": 4584,
      "training_loss": 7.787062644958496
    },
    {
      "epoch": 0.9938211382113821,
      "step": 4584,
      "training_loss": 6.694295406341553
    },
    {
      "epoch": 0.9938211382113821,
      "step": 4584,
      "training_loss": 7.2487030029296875
    },
    {
      "epoch": 0.9938211382113821,
      "step": 4584,
      "training_loss": 7.462438106536865
    },
    {
      "epoch": 0.9940379403794037,
      "step": 4585,
      "training_loss": 7.433987140655518
    },
    {
      "epoch": 0.9940379403794037,
      "step": 4585,
      "training_loss": 6.554726600646973
    },
    {
      "epoch": 0.9940379403794037,
      "step": 4585,
      "training_loss": 6.909654140472412
    },
    {
      "epoch": 0.9940379403794037,
      "step": 4585,
      "training_loss": 2.232637882232666
    },
    {
      "epoch": 0.9942547425474255,
      "step": 4586,
      "training_loss": 5.879143714904785
    },
    {
      "epoch": 0.9942547425474255,
      "step": 4586,
      "training_loss": 6.880990505218506
    },
    {
      "epoch": 0.9942547425474255,
      "step": 4586,
      "training_loss": 6.170224189758301
    },
    {
      "epoch": 0.9942547425474255,
      "step": 4586,
      "training_loss": 6.181629180908203
    },
    {
      "epoch": 0.9944715447154472,
      "step": 4587,
      "training_loss": 3.5021307468414307
    },
    {
      "epoch": 0.9944715447154472,
      "step": 4587,
      "training_loss": 7.118114471435547
    },
    {
      "epoch": 0.9944715447154472,
      "step": 4587,
      "training_loss": 6.673325538635254
    },
    {
      "epoch": 0.9944715447154472,
      "step": 4587,
      "training_loss": 6.5312724113464355
    },
    {
      "epoch": 0.9946883468834689,
      "grad_norm": 18.677141189575195,
      "learning_rate": 1e-05,
      "loss": 6.3288,
      "step": 4588
    },
    {
      "epoch": 0.9946883468834689,
      "step": 4588,
      "training_loss": 6.649681568145752
    },
    {
      "epoch": 0.9946883468834689,
      "step": 4588,
      "training_loss": 6.377766132354736
    },
    {
      "epoch": 0.9946883468834689,
      "step": 4588,
      "training_loss": 6.578914642333984
    },
    {
      "epoch": 0.9946883468834689,
      "step": 4588,
      "training_loss": 4.524516582489014
    },
    {
      "epoch": 0.9949051490514905,
      "step": 4589,
      "training_loss": 7.096114158630371
    },
    {
      "epoch": 0.9949051490514905,
      "step": 4589,
      "training_loss": 6.205738067626953
    },
    {
      "epoch": 0.9949051490514905,
      "step": 4589,
      "training_loss": 6.703108310699463
    },
    {
      "epoch": 0.9949051490514905,
      "step": 4589,
      "training_loss": 5.558855056762695
    },
    {
      "epoch": 0.9951219512195122,
      "step": 4590,
      "training_loss": 5.707640171051025
    },
    {
      "epoch": 0.9951219512195122,
      "step": 4590,
      "training_loss": 7.399548053741455
    },
    {
      "epoch": 0.9951219512195122,
      "step": 4590,
      "training_loss": 3.911827802658081
    },
    {
      "epoch": 0.9951219512195122,
      "step": 4590,
      "training_loss": 6.478881359100342
    },
    {
      "epoch": 0.9953387533875339,
      "step": 4591,
      "training_loss": 6.729036331176758
    },
    {
      "epoch": 0.9953387533875339,
      "step": 4591,
      "training_loss": 9.464141845703125
    },
    {
      "epoch": 0.9953387533875339,
      "step": 4591,
      "training_loss": 8.584482192993164
    },
    {
      "epoch": 0.9953387533875339,
      "step": 4591,
      "training_loss": 5.18766975402832
    },
    {
      "epoch": 0.9955555555555555,
      "grad_norm": 31.821212768554688,
      "learning_rate": 1e-05,
      "loss": 6.4474,
      "step": 4592
    },
    {
      "epoch": 0.9955555555555555,
      "step": 4592,
      "training_loss": 7.754064083099365
    },
    {
      "epoch": 0.9955555555555555,
      "step": 4592,
      "training_loss": 6.954435348510742
    },
    {
      "epoch": 0.9955555555555555,
      "step": 4592,
      "training_loss": 6.441511631011963
    },
    {
      "epoch": 0.9955555555555555,
      "step": 4592,
      "training_loss": 6.134073734283447
    },
    {
      "epoch": 0.9957723577235772,
      "step": 4593,
      "training_loss": 8.039359092712402
    },
    {
      "epoch": 0.9957723577235772,
      "step": 4593,
      "training_loss": 4.330814361572266
    },
    {
      "epoch": 0.9957723577235772,
      "step": 4593,
      "training_loss": 6.709028244018555
    },
    {
      "epoch": 0.9957723577235772,
      "step": 4593,
      "training_loss": 6.75136661529541
    },
    {
      "epoch": 0.9959891598915989,
      "step": 4594,
      "training_loss": 7.010552406311035
    },
    {
      "epoch": 0.9959891598915989,
      "step": 4594,
      "training_loss": 5.99408483505249
    },
    {
      "epoch": 0.9959891598915989,
      "step": 4594,
      "training_loss": 7.555422782897949
    },
    {
      "epoch": 0.9959891598915989,
      "step": 4594,
      "training_loss": 5.954087257385254
    },
    {
      "epoch": 0.9962059620596206,
      "step": 4595,
      "training_loss": 6.883938312530518
    },
    {
      "epoch": 0.9962059620596206,
      "step": 4595,
      "training_loss": 5.622395038604736
    },
    {
      "epoch": 0.9962059620596206,
      "step": 4595,
      "training_loss": 5.912394046783447
    },
    {
      "epoch": 0.9962059620596206,
      "step": 4595,
      "training_loss": 7.827168941497803
    },
    {
      "epoch": 0.9964227642276423,
      "grad_norm": 36.37785339355469,
      "learning_rate": 1e-05,
      "loss": 6.6172,
      "step": 4596
    },
    {
      "epoch": 0.9964227642276423,
      "step": 4596,
      "training_loss": 6.291626453399658
    },
    {
      "epoch": 0.9964227642276423,
      "step": 4596,
      "training_loss": 6.810532569885254
    },
    {
      "epoch": 0.9964227642276423,
      "step": 4596,
      "training_loss": 6.131629467010498
    },
    {
      "epoch": 0.9964227642276423,
      "step": 4596,
      "training_loss": 3.52750301361084
    },
    {
      "epoch": 0.996639566395664,
      "step": 4597,
      "training_loss": 6.6016058921813965
    },
    {
      "epoch": 0.996639566395664,
      "step": 4597,
      "training_loss": 6.440354824066162
    },
    {
      "epoch": 0.996639566395664,
      "step": 4597,
      "training_loss": 5.174551486968994
    },
    {
      "epoch": 0.996639566395664,
      "step": 4597,
      "training_loss": 6.354120254516602
    },
    {
      "epoch": 0.9968563685636856,
      "step": 4598,
      "training_loss": 6.510814666748047
    },
    {
      "epoch": 0.9968563685636856,
      "step": 4598,
      "training_loss": 6.831288814544678
    },
    {
      "epoch": 0.9968563685636856,
      "step": 4598,
      "training_loss": 5.946556568145752
    },
    {
      "epoch": 0.9968563685636856,
      "step": 4598,
      "training_loss": 4.764945983886719
    },
    {
      "epoch": 0.9970731707317073,
      "step": 4599,
      "training_loss": 7.108452796936035
    },
    {
      "epoch": 0.9970731707317073,
      "step": 4599,
      "training_loss": 5.84583044052124
    },
    {
      "epoch": 0.9970731707317073,
      "step": 4599,
      "training_loss": 5.952792644500732
    },
    {
      "epoch": 0.9970731707317073,
      "step": 4599,
      "training_loss": 5.352374076843262
    },
    {
      "epoch": 0.997289972899729,
      "grad_norm": 23.200618743896484,
      "learning_rate": 1e-05,
      "loss": 5.9778,
      "step": 4600
    },
    {
      "epoch": 0.997289972899729,
      "step": 4600,
      "training_loss": 5.543731212615967
    },
    {
      "epoch": 0.997289972899729,
      "step": 4600,
      "training_loss": 5.944657802581787
    },
    {
      "epoch": 0.997289972899729,
      "step": 4600,
      "training_loss": 7.51957893371582
    },
    {
      "epoch": 0.997289972899729,
      "step": 4600,
      "training_loss": 6.120674133300781
    },
    {
      "epoch": 0.9975067750677507,
      "step": 4601,
      "training_loss": 6.173590660095215
    },
    {
      "epoch": 0.9975067750677507,
      "step": 4601,
      "training_loss": 7.375741004943848
    },
    {
      "epoch": 0.9975067750677507,
      "step": 4601,
      "training_loss": 7.463725566864014
    },
    {
      "epoch": 0.9975067750677507,
      "step": 4601,
      "training_loss": 5.407587051391602
    },
    {
      "epoch": 0.9977235772357723,
      "step": 4602,
      "training_loss": 4.918133735656738
    },
    {
      "epoch": 0.9977235772357723,
      "step": 4602,
      "training_loss": 5.077037334442139
    },
    {
      "epoch": 0.9977235772357723,
      "step": 4602,
      "training_loss": 7.336857795715332
    },
    {
      "epoch": 0.9977235772357723,
      "step": 4602,
      "training_loss": 7.201618194580078
    },
    {
      "epoch": 0.997940379403794,
      "step": 4603,
      "training_loss": 5.127997875213623
    },
    {
      "epoch": 0.997940379403794,
      "step": 4603,
      "training_loss": 7.708051681518555
    },
    {
      "epoch": 0.997940379403794,
      "step": 4603,
      "training_loss": 7.3582024574279785
    },
    {
      "epoch": 0.997940379403794,
      "step": 4603,
      "training_loss": 5.838385581970215
    },
    {
      "epoch": 0.9981571815718158,
      "grad_norm": 26.072540283203125,
      "learning_rate": 1e-05,
      "loss": 6.3822,
      "step": 4604
    },
    {
      "epoch": 0.9981571815718158,
      "step": 4604,
      "training_loss": 7.095626354217529
    },
    {
      "epoch": 0.9981571815718158,
      "step": 4604,
      "training_loss": 7.043266773223877
    },
    {
      "epoch": 0.9981571815718158,
      "step": 4604,
      "training_loss": 7.490535736083984
    },
    {
      "epoch": 0.9981571815718158,
      "step": 4604,
      "training_loss": 6.312624931335449
    },
    {
      "epoch": 0.9983739837398374,
      "step": 4605,
      "training_loss": 8.024930000305176
    },
    {
      "epoch": 0.9983739837398374,
      "step": 4605,
      "training_loss": 7.369207382202148
    },
    {
      "epoch": 0.9983739837398374,
      "step": 4605,
      "training_loss": 6.432442665100098
    },
    {
      "epoch": 0.9983739837398374,
      "step": 4605,
      "training_loss": 6.583133220672607
    },
    {
      "epoch": 0.9985907859078591,
      "step": 4606,
      "training_loss": 2.635293960571289
    },
    {
      "epoch": 0.9985907859078591,
      "step": 4606,
      "training_loss": 8.448173522949219
    },
    {
      "epoch": 0.9985907859078591,
      "step": 4606,
      "training_loss": 3.982994794845581
    },
    {
      "epoch": 0.9985907859078591,
      "step": 4606,
      "training_loss": 7.082979202270508
    },
    {
      "epoch": 0.9988075880758808,
      "step": 4607,
      "training_loss": 6.81503438949585
    },
    {
      "epoch": 0.9988075880758808,
      "step": 4607,
      "training_loss": 5.843539237976074
    },
    {
      "epoch": 0.9988075880758808,
      "step": 4607,
      "training_loss": 5.722091197967529
    },
    {
      "epoch": 0.9988075880758808,
      "step": 4607,
      "training_loss": 6.462629795074463
    },
    {
      "epoch": 0.9990243902439024,
      "grad_norm": 20.46829605102539,
      "learning_rate": 1e-05,
      "loss": 6.459,
      "step": 4608
    },
    {
      "epoch": 0.9990243902439024,
      "step": 4608,
      "training_loss": 5.312460422515869
    },
    {
      "epoch": 0.9990243902439024,
      "step": 4608,
      "training_loss": 6.082822322845459
    },
    {
      "epoch": 0.9990243902439024,
      "step": 4608,
      "training_loss": 6.885432720184326
    },
    {
      "epoch": 0.9990243902439024,
      "step": 4608,
      "training_loss": 6.9534711837768555
    },
    {
      "epoch": 0.9992411924119241,
      "step": 4609,
      "training_loss": 5.978448867797852
    },
    {
      "epoch": 0.9992411924119241,
      "step": 4609,
      "training_loss": 5.44104528427124
    },
    {
      "epoch": 0.9992411924119241,
      "step": 4609,
      "training_loss": 6.4593939781188965
    },
    {
      "epoch": 0.9992411924119241,
      "step": 4609,
      "training_loss": 7.302763938903809
    },
    {
      "epoch": 0.9994579945799458,
      "step": 4610,
      "training_loss": 6.570058345794678
    },
    {
      "epoch": 0.9994579945799458,
      "step": 4610,
      "training_loss": 6.428919315338135
    },
    {
      "epoch": 0.9994579945799458,
      "step": 4610,
      "training_loss": 6.356733798980713
    },
    {
      "epoch": 0.9994579945799458,
      "step": 4610,
      "training_loss": 4.414967060089111
    },
    {
      "epoch": 0.9996747967479674,
      "step": 4611,
      "training_loss": 6.905200004577637
    },
    {
      "epoch": 0.9996747967479674,
      "step": 4611,
      "training_loss": 6.480345726013184
    },
    {
      "epoch": 0.9996747967479674,
      "step": 4611,
      "training_loss": 4.508734703063965
    },
    {
      "epoch": 0.9996747967479674,
      "step": 4611,
      "training_loss": 6.487608432769775
    },
    {
      "epoch": 0.9998915989159891,
      "grad_norm": 19.10370635986328,
      "learning_rate": 1e-05,
      "loss": 6.1605,
      "step": 4612
    }
  ],
  "logging_steps": 4,
  "max_steps": 4612,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 6000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.364493382999081e+17,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
