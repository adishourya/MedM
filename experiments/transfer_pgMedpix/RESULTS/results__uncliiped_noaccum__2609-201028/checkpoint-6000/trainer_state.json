{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.3252032520325203,
  "eval_steps": 6000,
  "global_step": 6000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0,
      "step": 0,
      "training_loss": 11.111725807189941
    },
    {
      "epoch": 5.4200542005420054e-05,
      "step": 1,
      "training_loss": 13.243904113769531
    },
    {
      "epoch": 0.00010840108401084011,
      "step": 2,
      "training_loss": 12.016393661499023
    },
    {
      "epoch": 0.00016260162601626016,
      "step": 3,
      "training_loss": 12.524957656860352
    },
    {
      "epoch": 0.00021680216802168022,
      "grad_norm": NaN,
      "learning_rate": 1e-05,
      "loss": 12.2242,
      "step": 4
    },
    {
      "epoch": 0.00021680216802168022,
      "step": 4,
      "training_loss": 12.043158531188965
    },
    {
      "epoch": 0.00027100271002710027,
      "step": 5,
      "training_loss": 12.434259414672852
    },
    {
      "epoch": 0.0003252032520325203,
      "step": 6,
      "training_loss": 14.311711311340332
    },
    {
      "epoch": 0.0003794037940379404,
      "step": 7,
      "training_loss": 11.889445304870605
    },
    {
      "epoch": 0.00043360433604336043,
      "grad_norm": NaN,
      "learning_rate": 1e-05,
      "loss": 12.6696,
      "step": 8
    },
    {
      "epoch": 0.00043360433604336043,
      "step": 8,
      "training_loss": 11.8179292678833
    },
    {
      "epoch": 0.0004878048780487805,
      "step": 9,
      "training_loss": 12.8345308303833
    },
    {
      "epoch": 0.0005420054200542005,
      "step": 10,
      "training_loss": 12.50240707397461
    },
    {
      "epoch": 0.0005962059620596206,
      "step": 11,
      "training_loss": 12.181693077087402
    },
    {
      "epoch": 0.0006504065040650406,
      "grad_norm": 31.525789260864258,
      "learning_rate": 1e-05,
      "loss": 12.3341,
      "step": 12
    },
    {
      "epoch": 0.0006504065040650406,
      "step": 12,
      "training_loss": 13.70712661743164
    },
    {
      "epoch": 0.0007046070460704607,
      "step": 13,
      "training_loss": 11.968474388122559
    },
    {
      "epoch": 0.0007588075880758808,
      "step": 14,
      "training_loss": 11.70648193359375
    },
    {
      "epoch": 0.0008130081300813008,
      "step": 15,
      "training_loss": 12.483585357666016
    },
    {
      "epoch": 0.0008672086720867209,
      "grad_norm": 109.42294311523438,
      "learning_rate": 1e-05,
      "loss": 12.4664,
      "step": 16
    },
    {
      "epoch": 0.0008672086720867209,
      "step": 16,
      "training_loss": 14.522932052612305
    },
    {
      "epoch": 0.0009214092140921409,
      "step": 17,
      "training_loss": 12.172041893005371
    },
    {
      "epoch": 0.000975609756097561,
      "step": 18,
      "training_loss": 11.292916297912598
    },
    {
      "epoch": 0.001029810298102981,
      "step": 19,
      "training_loss": 11.767900466918945
    },
    {
      "epoch": 0.001084010840108401,
      "grad_norm": 64.20651245117188,
      "learning_rate": 1e-05,
      "loss": 12.4389,
      "step": 20
    },
    {
      "epoch": 0.001084010840108401,
      "step": 20,
      "training_loss": 12.513604164123535
    },
    {
      "epoch": 0.0011382113821138211,
      "step": 21,
      "training_loss": 11.945090293884277
    },
    {
      "epoch": 0.0011924119241192412,
      "step": 22,
      "training_loss": 12.44311237335205
    },
    {
      "epoch": 0.0012466124661246612,
      "step": 23,
      "training_loss": 23.766647338867188
    },
    {
      "epoch": 0.0013008130081300813,
      "grad_norm": NaN,
      "learning_rate": 1e-05,
      "loss": 15.1671,
      "step": 24
    },
    {
      "epoch": 0.0013008130081300813,
      "step": 24,
      "training_loss": 12.468871116638184
    },
    {
      "epoch": 0.0013550135501355014,
      "step": 25,
      "training_loss": 11.110321044921875
    },
    {
      "epoch": 0.0014092140921409214,
      "step": 26,
      "training_loss": 12.302184104919434
    },
    {
      "epoch": 0.0014634146341463415,
      "step": 27,
      "training_loss": 10.503989219665527
    },
    {
      "epoch": 0.0015176151761517615,
      "grad_norm": 98.07079315185547,
      "learning_rate": 1e-05,
      "loss": 11.5963,
      "step": 28
    },
    {
      "epoch": 0.0015176151761517615,
      "step": 28,
      "training_loss": 12.163992881774902
    },
    {
      "epoch": 0.0015718157181571816,
      "step": 29,
      "training_loss": 11.964346885681152
    },
    {
      "epoch": 0.0016260162601626016,
      "step": 30,
      "training_loss": 18.407920837402344
    },
    {
      "epoch": 0.0016802168021680217,
      "step": 31,
      "training_loss": 11.686904907226562
    },
    {
      "epoch": 0.0017344173441734417,
      "grad_norm": 36.55788040161133,
      "learning_rate": 1e-05,
      "loss": 13.5558,
      "step": 32
    },
    {
      "epoch": 0.0017344173441734417,
      "step": 32,
      "training_loss": 12.702886581420898
    },
    {
      "epoch": 0.0017886178861788618,
      "step": 33,
      "training_loss": 12.914487838745117
    },
    {
      "epoch": 0.0018428184281842818,
      "step": 34,
      "training_loss": 10.897786140441895
    },
    {
      "epoch": 0.001897018970189702,
      "step": 35,
      "training_loss": 11.767158508300781
    },
    {
      "epoch": 0.001951219512195122,
      "grad_norm": 65.15444946289062,
      "learning_rate": 1e-05,
      "loss": 12.0706,
      "step": 36
    },
    {
      "epoch": 0.001951219512195122,
      "step": 36,
      "training_loss": 12.365867614746094
    },
    {
      "epoch": 0.002005420054200542,
      "step": 37,
      "training_loss": 11.822869300842285
    },
    {
      "epoch": 0.002059620596205962,
      "step": 38,
      "training_loss": 12.36075496673584
    },
    {
      "epoch": 0.002113821138211382,
      "step": 39,
      "training_loss": 12.555594444274902
    },
    {
      "epoch": 0.002168021680216802,
      "grad_norm": 37.7735710144043,
      "learning_rate": 1e-05,
      "loss": 12.2763,
      "step": 40
    },
    {
      "epoch": 0.002168021680216802,
      "step": 40,
      "training_loss": 11.8652925491333
    },
    {
      "epoch": 0.0022222222222222222,
      "step": 41,
      "training_loss": 11.969447135925293
    },
    {
      "epoch": 0.0022764227642276423,
      "step": 42,
      "training_loss": 12.587841033935547
    },
    {
      "epoch": 0.0023306233062330623,
      "step": 43,
      "training_loss": 12.134407997131348
    },
    {
      "epoch": 0.0023848238482384824,
      "grad_norm": 16.635469436645508,
      "learning_rate": 1e-05,
      "loss": 12.1392,
      "step": 44
    },
    {
      "epoch": 0.0023848238482384824,
      "step": 44,
      "training_loss": 11.510613441467285
    },
    {
      "epoch": 0.0024390243902439024,
      "step": 45,
      "training_loss": 12.909424781799316
    },
    {
      "epoch": 0.0024932249322493225,
      "step": 46,
      "training_loss": 11.108716011047363
    },
    {
      "epoch": 0.0025474254742547425,
      "step": 47,
      "training_loss": 11.486334800720215
    },
    {
      "epoch": 0.0026016260162601626,
      "grad_norm": 115.90557861328125,
      "learning_rate": 1e-05,
      "loss": 11.7538,
      "step": 48
    },
    {
      "epoch": 0.0026016260162601626,
      "step": 48,
      "training_loss": 10.617513656616211
    },
    {
      "epoch": 0.0026558265582655827,
      "step": 49,
      "training_loss": 11.362805366516113
    },
    {
      "epoch": 0.0027100271002710027,
      "step": 50,
      "training_loss": 13.34550666809082
    },
    {
      "epoch": 0.0027642276422764228,
      "step": 51,
      "training_loss": 13.236893653869629
    },
    {
      "epoch": 0.002818428184281843,
      "grad_norm": NaN,
      "learning_rate": 1e-05,
      "loss": 12.1407,
      "step": 52
    },
    {
      "epoch": 0.002818428184281843,
      "step": 52,
      "training_loss": 11.147956848144531
    },
    {
      "epoch": 0.002872628726287263,
      "step": 53,
      "training_loss": 11.111977577209473
    },
    {
      "epoch": 0.002926829268292683,
      "step": 54,
      "training_loss": 11.819609642028809
    },
    {
      "epoch": 0.002981029810298103,
      "step": 55,
      "training_loss": 18.253034591674805
    },
    {
      "epoch": 0.003035230352303523,
      "grad_norm": 3510.15576171875,
      "learning_rate": 1e-05,
      "loss": 13.0831,
      "step": 56
    },
    {
      "epoch": 0.003035230352303523,
      "step": 56,
      "training_loss": 11.89055347442627
    },
    {
      "epoch": 0.003089430894308943,
      "step": 57,
      "training_loss": 12.646760940551758
    },
    {
      "epoch": 0.003143631436314363,
      "step": 58,
      "training_loss": 12.024698257446289
    },
    {
      "epoch": 0.003197831978319783,
      "step": 59,
      "training_loss": 10.462615013122559
    },
    {
      "epoch": 0.0032520325203252032,
      "grad_norm": 66.9910659790039,
      "learning_rate": 1e-05,
      "loss": 11.7562,
      "step": 60
    },
    {
      "epoch": 0.0032520325203252032,
      "step": 60,
      "training_loss": 12.814921379089355
    },
    {
      "epoch": 0.0033062330623306233,
      "step": 61,
      "training_loss": 12.584982872009277
    },
    {
      "epoch": 0.0033604336043360434,
      "step": 62,
      "training_loss": 16.339969635009766
    },
    {
      "epoch": 0.0034146341463414634,
      "step": 63,
      "training_loss": 12.473222732543945
    },
    {
      "epoch": 0.0034688346883468835,
      "grad_norm": 144.7232208251953,
      "learning_rate": 1e-05,
      "loss": 13.5533,
      "step": 64
    },
    {
      "epoch": 0.0034688346883468835,
      "step": 64,
      "training_loss": 11.977269172668457
    },
    {
      "epoch": 0.0035230352303523035,
      "step": 65,
      "training_loss": 11.061274528503418
    },
    {
      "epoch": 0.0035772357723577236,
      "step": 66,
      "training_loss": 11.066757202148438
    },
    {
      "epoch": 0.0036314363143631436,
      "step": 67,
      "training_loss": 11.54782485961914
    },
    {
      "epoch": 0.0036856368563685637,
      "grad_norm": 30.801361083984375,
      "learning_rate": 1e-05,
      "loss": 11.4133,
      "step": 68
    },
    {
      "epoch": 0.0036856368563685637,
      "step": 68,
      "training_loss": 16.31871795654297
    },
    {
      "epoch": 0.0037398373983739837,
      "step": 69,
      "training_loss": 11.002864837646484
    },
    {
      "epoch": 0.003794037940379404,
      "step": 70,
      "training_loss": 10.380319595336914
    },
    {
      "epoch": 0.003848238482384824,
      "step": 71,
      "training_loss": 13.15201473236084
    },
    {
      "epoch": 0.003902439024390244,
      "grad_norm": 26.781593322753906,
      "learning_rate": 1e-05,
      "loss": 12.7135,
      "step": 72
    },
    {
      "epoch": 0.003902439024390244,
      "step": 72,
      "training_loss": 12.929259300231934
    },
    {
      "epoch": 0.003956639566395664,
      "step": 73,
      "training_loss": 12.527139663696289
    },
    {
      "epoch": 0.004010840108401084,
      "step": 74,
      "training_loss": 13.343894004821777
    },
    {
      "epoch": 0.0040650406504065045,
      "step": 75,
      "training_loss": 11.778653144836426
    },
    {
      "epoch": 0.004119241192411924,
      "grad_norm": 13.673876762390137,
      "learning_rate": 1e-05,
      "loss": 12.6447,
      "step": 76
    },
    {
      "epoch": 0.004119241192411924,
      "step": 76,
      "training_loss": 11.3930082321167
    },
    {
      "epoch": 0.004173441734417345,
      "step": 77,
      "training_loss": 11.176904678344727
    },
    {
      "epoch": 0.004227642276422764,
      "step": 78,
      "training_loss": 11.507253646850586
    },
    {
      "epoch": 0.004281842818428185,
      "step": 79,
      "training_loss": 18.210491180419922
    },
    {
      "epoch": 0.004336043360433604,
      "grad_norm": 2676.913818359375,
      "learning_rate": 1e-05,
      "loss": 13.0719,
      "step": 80
    },
    {
      "epoch": 0.004336043360433604,
      "step": 80,
      "training_loss": 11.651453018188477
    },
    {
      "epoch": 0.004390243902439025,
      "step": 81,
      "training_loss": 11.206361770629883
    },
    {
      "epoch": 0.0044444444444444444,
      "step": 82,
      "training_loss": 11.949674606323242
    },
    {
      "epoch": 0.004498644986449865,
      "step": 83,
      "training_loss": 11.430155754089355
    },
    {
      "epoch": 0.0045528455284552845,
      "grad_norm": 51.11741638183594,
      "learning_rate": 1e-05,
      "loss": 11.5594,
      "step": 84
    },
    {
      "epoch": 0.0045528455284552845,
      "step": 84,
      "training_loss": 11.974573135375977
    },
    {
      "epoch": 0.004607046070460705,
      "step": 85,
      "training_loss": 9.924421310424805
    },
    {
      "epoch": 0.004661246612466125,
      "step": 86,
      "training_loss": 11.799753189086914
    },
    {
      "epoch": 0.004715447154471545,
      "step": 87,
      "training_loss": 11.262210845947266
    },
    {
      "epoch": 0.004769647696476965,
      "grad_norm": 27.579992294311523,
      "learning_rate": 1e-05,
      "loss": 11.2402,
      "step": 88
    },
    {
      "epoch": 0.004769647696476965,
      "step": 88,
      "training_loss": 9.01122760772705
    },
    {
      "epoch": 0.004823848238482385,
      "step": 89,
      "training_loss": 11.549857139587402
    },
    {
      "epoch": 0.004878048780487805,
      "step": 90,
      "training_loss": 11.19095516204834
    },
    {
      "epoch": 0.004932249322493225,
      "step": 91,
      "training_loss": 10.911154747009277
    },
    {
      "epoch": 0.004986449864498645,
      "grad_norm": 42.25879669189453,
      "learning_rate": 1e-05,
      "loss": 10.6658,
      "step": 92
    },
    {
      "epoch": 0.004986449864498645,
      "step": 92,
      "training_loss": 10.490349769592285
    },
    {
      "epoch": 0.0050406504065040655,
      "step": 93,
      "training_loss": 12.937756538391113
    },
    {
      "epoch": 0.005094850948509485,
      "step": 94,
      "training_loss": 13.500983238220215
    },
    {
      "epoch": 0.005149051490514906,
      "step": 95,
      "training_loss": 12.888493537902832
    },
    {
      "epoch": 0.005203252032520325,
      "grad_norm": 46.61079788208008,
      "learning_rate": 1e-05,
      "loss": 12.4544,
      "step": 96
    },
    {
      "epoch": 0.005203252032520325,
      "step": 96,
      "training_loss": 11.705912590026855
    },
    {
      "epoch": 0.005257452574525746,
      "step": 97,
      "training_loss": 12.569985389709473
    },
    {
      "epoch": 0.005311653116531165,
      "step": 98,
      "training_loss": 11.238210678100586
    },
    {
      "epoch": 0.005365853658536586,
      "step": 99,
      "training_loss": 11.309539794921875
    },
    {
      "epoch": 0.005420054200542005,
      "grad_norm": 19.945053100585938,
      "learning_rate": 1e-05,
      "loss": 11.7059,
      "step": 100
    },
    {
      "epoch": 0.005420054200542005,
      "step": 100,
      "training_loss": 10.453259468078613
    },
    {
      "epoch": 0.005474254742547426,
      "step": 101,
      "training_loss": 9.973780632019043
    },
    {
      "epoch": 0.0055284552845528455,
      "step": 102,
      "training_loss": 10.217357635498047
    },
    {
      "epoch": 0.005582655826558266,
      "step": 103,
      "training_loss": 12.519991874694824
    },
    {
      "epoch": 0.005636856368563686,
      "grad_norm": 290.6909484863281,
      "learning_rate": 1e-05,
      "loss": 10.7911,
      "step": 104
    },
    {
      "epoch": 0.005636856368563686,
      "step": 104,
      "training_loss": 11.5534029006958
    },
    {
      "epoch": 0.005691056910569106,
      "step": 105,
      "training_loss": 11.72472095489502
    },
    {
      "epoch": 0.005745257452574526,
      "step": 106,
      "training_loss": 11.177977561950684
    },
    {
      "epoch": 0.005799457994579946,
      "step": 107,
      "training_loss": 9.76594066619873
    },
    {
      "epoch": 0.005853658536585366,
      "grad_norm": 65.16187286376953,
      "learning_rate": 1e-05,
      "loss": 11.0555,
      "step": 108
    },
    {
      "epoch": 0.005853658536585366,
      "step": 108,
      "training_loss": 10.47020435333252
    },
    {
      "epoch": 0.005907859078590786,
      "step": 109,
      "training_loss": 11.763017654418945
    },
    {
      "epoch": 0.005962059620596206,
      "step": 110,
      "training_loss": 10.09343147277832
    },
    {
      "epoch": 0.0060162601626016264,
      "step": 111,
      "training_loss": 12.833083152770996
    },
    {
      "epoch": 0.006070460704607046,
      "grad_norm": 416.72174072265625,
      "learning_rate": 1e-05,
      "loss": 11.2899,
      "step": 112
    },
    {
      "epoch": 0.006070460704607046,
      "step": 112,
      "training_loss": 10.665101051330566
    },
    {
      "epoch": 0.0061246612466124666,
      "step": 113,
      "training_loss": 11.124534606933594
    },
    {
      "epoch": 0.006178861788617886,
      "step": 114,
      "training_loss": 11.358909606933594
    },
    {
      "epoch": 0.006233062330623307,
      "step": 115,
      "training_loss": 11.18017292022705
    },
    {
      "epoch": 0.006287262872628726,
      "grad_norm": 65.0464096069336,
      "learning_rate": 1e-05,
      "loss": 11.0822,
      "step": 116
    },
    {
      "epoch": 0.006287262872628726,
      "step": 116,
      "training_loss": 12.137036323547363
    },
    {
      "epoch": 0.006341463414634147,
      "step": 117,
      "training_loss": 11.771164894104004
    },
    {
      "epoch": 0.006395663956639566,
      "step": 118,
      "training_loss": 10.574539184570312
    },
    {
      "epoch": 0.006449864498644987,
      "step": 119,
      "training_loss": 11.43765926361084
    },
    {
      "epoch": 0.0065040650406504065,
      "grad_norm": 124.70130920410156,
      "learning_rate": 1e-05,
      "loss": 11.4801,
      "step": 120
    },
    {
      "epoch": 0.0065040650406504065,
      "step": 120,
      "training_loss": 11.914714813232422
    },
    {
      "epoch": 0.006558265582655827,
      "step": 121,
      "training_loss": 9.085593223571777
    },
    {
      "epoch": 0.006612466124661247,
      "step": 122,
      "training_loss": 11.181285858154297
    },
    {
      "epoch": 0.006666666666666667,
      "step": 123,
      "training_loss": 10.497374534606934
    },
    {
      "epoch": 0.006720867208672087,
      "grad_norm": 57.40266418457031,
      "learning_rate": 1e-05,
      "loss": 10.6697,
      "step": 124
    },
    {
      "epoch": 0.006720867208672087,
      "step": 124,
      "training_loss": 11.198966026306152
    },
    {
      "epoch": 0.006775067750677507,
      "step": 125,
      "training_loss": 11.80430793762207
    },
    {
      "epoch": 0.006829268292682927,
      "step": 126,
      "training_loss": 10.973591804504395
    },
    {
      "epoch": 0.006883468834688347,
      "step": 127,
      "training_loss": 9.583660125732422
    },
    {
      "epoch": 0.006937669376693767,
      "grad_norm": 47.901145935058594,
      "learning_rate": 1e-05,
      "loss": 10.8901,
      "step": 128
    },
    {
      "epoch": 0.006937669376693767,
      "step": 128,
      "training_loss": 11.993356704711914
    },
    {
      "epoch": 0.006991869918699187,
      "step": 129,
      "training_loss": 10.64016342163086
    },
    {
      "epoch": 0.007046070460704607,
      "step": 130,
      "training_loss": 10.88064956665039
    },
    {
      "epoch": 0.0071002710027100275,
      "step": 131,
      "training_loss": 10.638195991516113
    },
    {
      "epoch": 0.007154471544715447,
      "grad_norm": 25.196043014526367,
      "learning_rate": 1e-05,
      "loss": 11.0381,
      "step": 132
    },
    {
      "epoch": 0.007154471544715447,
      "step": 132,
      "training_loss": 12.720953941345215
    },
    {
      "epoch": 0.007208672086720868,
      "step": 133,
      "training_loss": 11.219255447387695
    },
    {
      "epoch": 0.007262872628726287,
      "step": 134,
      "training_loss": 10.449835777282715
    },
    {
      "epoch": 0.007317073170731708,
      "step": 135,
      "training_loss": 11.332894325256348
    },
    {
      "epoch": 0.007371273712737127,
      "grad_norm": 35.33205795288086,
      "learning_rate": 1e-05,
      "loss": 11.4307,
      "step": 136
    },
    {
      "epoch": 0.007371273712737127,
      "step": 136,
      "training_loss": 8.992963790893555
    },
    {
      "epoch": 0.007425474254742548,
      "step": 137,
      "training_loss": 10.61905288696289
    },
    {
      "epoch": 0.0074796747967479675,
      "step": 138,
      "training_loss": 10.478516578674316
    },
    {
      "epoch": 0.007533875338753388,
      "step": 139,
      "training_loss": 10.031811714172363
    },
    {
      "epoch": 0.007588075880758808,
      "grad_norm": 56.115909576416016,
      "learning_rate": 1e-05,
      "loss": 10.0306,
      "step": 140
    },
    {
      "epoch": 0.007588075880758808,
      "step": 140,
      "training_loss": 10.708312034606934
    },
    {
      "epoch": 0.007642276422764228,
      "step": 141,
      "training_loss": 12.18755054473877
    },
    {
      "epoch": 0.007696476964769648,
      "step": 142,
      "training_loss": 10.794925689697266
    },
    {
      "epoch": 0.007750677506775068,
      "step": 143,
      "training_loss": 10.226948738098145
    },
    {
      "epoch": 0.007804878048780488,
      "grad_norm": 35.30724334716797,
      "learning_rate": 1e-05,
      "loss": 10.9794,
      "step": 144
    },
    {
      "epoch": 0.007804878048780488,
      "step": 144,
      "training_loss": 11.62662124633789
    },
    {
      "epoch": 0.007859078590785908,
      "step": 145,
      "training_loss": 10.435996055603027
    },
    {
      "epoch": 0.007913279132791329,
      "step": 146,
      "training_loss": 11.219482421875
    },
    {
      "epoch": 0.007967479674796748,
      "step": 147,
      "training_loss": 10.211543083190918
    },
    {
      "epoch": 0.008021680216802168,
      "grad_norm": 66.42169952392578,
      "learning_rate": 1e-05,
      "loss": 10.8734,
      "step": 148
    },
    {
      "epoch": 0.008021680216802168,
      "step": 148,
      "training_loss": 9.528517723083496
    },
    {
      "epoch": 0.008075880758807589,
      "step": 149,
      "training_loss": 10.306105613708496
    },
    {
      "epoch": 0.008130081300813009,
      "step": 150,
      "training_loss": 11.306597709655762
    },
    {
      "epoch": 0.008184281842818428,
      "step": 151,
      "training_loss": 10.273051261901855
    },
    {
      "epoch": 0.008238482384823848,
      "grad_norm": 25.634309768676758,
      "learning_rate": 1e-05,
      "loss": 10.3536,
      "step": 152
    },
    {
      "epoch": 0.008238482384823848,
      "step": 152,
      "training_loss": 11.826397895812988
    },
    {
      "epoch": 0.008292682926829269,
      "step": 153,
      "training_loss": 10.880592346191406
    },
    {
      "epoch": 0.00834688346883469,
      "step": 154,
      "training_loss": 10.433236122131348
    },
    {
      "epoch": 0.008401084010840108,
      "step": 155,
      "training_loss": 11.639178276062012
    },
    {
      "epoch": 0.008455284552845528,
      "grad_norm": 57.76725769042969,
      "learning_rate": 1e-05,
      "loss": 11.1949,
      "step": 156
    },
    {
      "epoch": 0.008455284552845528,
      "step": 156,
      "training_loss": 10.858449935913086
    },
    {
      "epoch": 0.008509485094850949,
      "step": 157,
      "training_loss": 11.179259300231934
    },
    {
      "epoch": 0.00856368563685637,
      "step": 158,
      "training_loss": 9.746270179748535
    },
    {
      "epoch": 0.008617886178861788,
      "step": 159,
      "training_loss": 10.675082206726074
    },
    {
      "epoch": 0.008672086720867209,
      "grad_norm": 43.02224349975586,
      "learning_rate": 1e-05,
      "loss": 10.6148,
      "step": 160
    },
    {
      "epoch": 0.008672086720867209,
      "step": 160,
      "training_loss": 12.17440414428711
    },
    {
      "epoch": 0.00872628726287263,
      "step": 161,
      "training_loss": 9.399737358093262
    },
    {
      "epoch": 0.00878048780487805,
      "step": 162,
      "training_loss": 11.896190643310547
    },
    {
      "epoch": 0.008834688346883468,
      "step": 163,
      "training_loss": 9.154244422912598
    },
    {
      "epoch": 0.008888888888888889,
      "grad_norm": 24.27657127380371,
      "learning_rate": 1e-05,
      "loss": 10.6561,
      "step": 164
    },
    {
      "epoch": 0.008888888888888889,
      "step": 164,
      "training_loss": 10.401182174682617
    },
    {
      "epoch": 0.00894308943089431,
      "step": 165,
      "training_loss": 11.158220291137695
    },
    {
      "epoch": 0.00899728997289973,
      "step": 166,
      "training_loss": 10.03610610961914
    },
    {
      "epoch": 0.009051490514905149,
      "step": 167,
      "training_loss": 10.533648490905762
    },
    {
      "epoch": 0.009105691056910569,
      "grad_norm": 27.298343658447266,
      "learning_rate": 1e-05,
      "loss": 10.5323,
      "step": 168
    },
    {
      "epoch": 0.009105691056910569,
      "step": 168,
      "training_loss": 10.831672668457031
    },
    {
      "epoch": 0.00915989159891599,
      "step": 169,
      "training_loss": 9.928610801696777
    },
    {
      "epoch": 0.00921409214092141,
      "step": 170,
      "training_loss": 9.534505844116211
    },
    {
      "epoch": 0.009268292682926829,
      "step": 171,
      "training_loss": 12.548449516296387
    },
    {
      "epoch": 0.00932249322493225,
      "grad_norm": 33.712833404541016,
      "learning_rate": 1e-05,
      "loss": 10.7108,
      "step": 172
    },
    {
      "epoch": 0.00932249322493225,
      "step": 172,
      "training_loss": 9.055968284606934
    },
    {
      "epoch": 0.00937669376693767,
      "step": 173,
      "training_loss": 10.076143264770508
    },
    {
      "epoch": 0.00943089430894309,
      "step": 174,
      "training_loss": 9.921630859375
    },
    {
      "epoch": 0.009485094850948509,
      "step": 175,
      "training_loss": 11.052468299865723
    },
    {
      "epoch": 0.00953929539295393,
      "grad_norm": 29.249650955200195,
      "learning_rate": 1e-05,
      "loss": 10.0266,
      "step": 176
    },
    {
      "epoch": 0.00953929539295393,
      "step": 176,
      "training_loss": 11.840372085571289
    },
    {
      "epoch": 0.00959349593495935,
      "step": 177,
      "training_loss": 9.16879940032959
    },
    {
      "epoch": 0.00964769647696477,
      "step": 178,
      "training_loss": 9.26314926147461
    },
    {
      "epoch": 0.00970189701897019,
      "step": 179,
      "training_loss": 10.310220718383789
    },
    {
      "epoch": 0.00975609756097561,
      "grad_norm": 57.88793182373047,
      "learning_rate": 1e-05,
      "loss": 10.1456,
      "step": 180
    },
    {
      "epoch": 0.00975609756097561,
      "step": 180,
      "training_loss": 11.384123802185059
    },
    {
      "epoch": 0.00981029810298103,
      "step": 181,
      "training_loss": 11.630406379699707
    },
    {
      "epoch": 0.00986449864498645,
      "step": 182,
      "training_loss": 10.63655948638916
    },
    {
      "epoch": 0.00991869918699187,
      "step": 183,
      "training_loss": 10.648456573486328
    },
    {
      "epoch": 0.00997289972899729,
      "grad_norm": 88.28411102294922,
      "learning_rate": 1e-05,
      "loss": 11.0749,
      "step": 184
    },
    {
      "epoch": 0.00997289972899729,
      "step": 184,
      "training_loss": 10.726205825805664
    },
    {
      "epoch": 0.01002710027100271,
      "step": 185,
      "training_loss": 12.421470642089844
    },
    {
      "epoch": 0.010081300813008131,
      "step": 186,
      "training_loss": 11.398855209350586
    },
    {
      "epoch": 0.01013550135501355,
      "step": 187,
      "training_loss": 10.761733055114746
    },
    {
      "epoch": 0.01018970189701897,
      "grad_norm": 40.832733154296875,
      "learning_rate": 1e-05,
      "loss": 11.3271,
      "step": 188
    },
    {
      "epoch": 0.01018970189701897,
      "step": 188,
      "training_loss": 11.110318183898926
    },
    {
      "epoch": 0.01024390243902439,
      "step": 189,
      "training_loss": 9.783599853515625
    },
    {
      "epoch": 0.010298102981029811,
      "step": 190,
      "training_loss": 10.568955421447754
    },
    {
      "epoch": 0.01035230352303523,
      "step": 191,
      "training_loss": 10.305909156799316
    },
    {
      "epoch": 0.01040650406504065,
      "grad_norm": 28.829519271850586,
      "learning_rate": 1e-05,
      "loss": 10.4422,
      "step": 192
    },
    {
      "epoch": 0.01040650406504065,
      "step": 192,
      "training_loss": 10.539288520812988
    },
    {
      "epoch": 0.010460704607046071,
      "step": 193,
      "training_loss": 10.521219253540039
    },
    {
      "epoch": 0.010514905149051491,
      "step": 194,
      "training_loss": 11.271158218383789
    },
    {
      "epoch": 0.01056910569105691,
      "step": 195,
      "training_loss": 9.358236312866211
    },
    {
      "epoch": 0.01062330623306233,
      "grad_norm": 30.441970825195312,
      "learning_rate": 1e-05,
      "loss": 10.4225,
      "step": 196
    },
    {
      "epoch": 0.01062330623306233,
      "step": 196,
      "training_loss": 11.183821678161621
    },
    {
      "epoch": 0.010677506775067751,
      "step": 197,
      "training_loss": 9.7353515625
    },
    {
      "epoch": 0.010731707317073172,
      "step": 198,
      "training_loss": 9.843719482421875
    },
    {
      "epoch": 0.01078590785907859,
      "step": 199,
      "training_loss": 9.85303783416748
    },
    {
      "epoch": 0.01084010840108401,
      "grad_norm": 27.371170043945312,
      "learning_rate": 1e-05,
      "loss": 10.154,
      "step": 200
    },
    {
      "epoch": 0.01084010840108401,
      "step": 200,
      "training_loss": 11.018733978271484
    },
    {
      "epoch": 0.010894308943089431,
      "step": 201,
      "training_loss": 8.939705848693848
    },
    {
      "epoch": 0.010948509485094852,
      "step": 202,
      "training_loss": 9.353633880615234
    },
    {
      "epoch": 0.01100271002710027,
      "step": 203,
      "training_loss": 10.075800895690918
    },
    {
      "epoch": 0.011056910569105691,
      "grad_norm": 32.45064163208008,
      "learning_rate": 1e-05,
      "loss": 9.847,
      "step": 204
    },
    {
      "epoch": 0.011056910569105691,
      "step": 204,
      "training_loss": 9.363607406616211
    },
    {
      "epoch": 0.011111111111111112,
      "step": 205,
      "training_loss": 10.482160568237305
    },
    {
      "epoch": 0.011165311653116532,
      "step": 206,
      "training_loss": 9.114072799682617
    },
    {
      "epoch": 0.01121951219512195,
      "step": 207,
      "training_loss": 10.320244789123535
    },
    {
      "epoch": 0.011273712737127371,
      "grad_norm": 42.16585159301758,
      "learning_rate": 1e-05,
      "loss": 9.82,
      "step": 208
    },
    {
      "epoch": 0.011273712737127371,
      "step": 208,
      "training_loss": 8.485520362854004
    },
    {
      "epoch": 0.011327913279132792,
      "step": 209,
      "training_loss": 10.149017333984375
    },
    {
      "epoch": 0.011382113821138212,
      "step": 210,
      "training_loss": 9.366436958312988
    },
    {
      "epoch": 0.011436314363143631,
      "step": 211,
      "training_loss": 9.488317489624023
    },
    {
      "epoch": 0.011490514905149051,
      "grad_norm": 76.35527801513672,
      "learning_rate": 1e-05,
      "loss": 9.3723,
      "step": 212
    },
    {
      "epoch": 0.011490514905149051,
      "step": 212,
      "training_loss": 9.656798362731934
    },
    {
      "epoch": 0.011544715447154472,
      "step": 213,
      "training_loss": 8.110672950744629
    },
    {
      "epoch": 0.011598915989159892,
      "step": 214,
      "training_loss": 9.679655075073242
    },
    {
      "epoch": 0.011653116531165311,
      "step": 215,
      "training_loss": 10.348111152648926
    },
    {
      "epoch": 0.011707317073170732,
      "grad_norm": 28.596580505371094,
      "learning_rate": 1e-05,
      "loss": 9.4488,
      "step": 216
    },
    {
      "epoch": 0.011707317073170732,
      "step": 216,
      "training_loss": 9.542818069458008
    },
    {
      "epoch": 0.011761517615176152,
      "step": 217,
      "training_loss": 10.631009101867676
    },
    {
      "epoch": 0.011815718157181573,
      "step": 218,
      "training_loss": 9.405866622924805
    },
    {
      "epoch": 0.011869918699186991,
      "step": 219,
      "training_loss": 11.114250183105469
    },
    {
      "epoch": 0.011924119241192412,
      "grad_norm": 57.84479904174805,
      "learning_rate": 1e-05,
      "loss": 10.1735,
      "step": 220
    },
    {
      "epoch": 0.011924119241192412,
      "step": 220,
      "training_loss": 10.114420890808105
    },
    {
      "epoch": 0.011978319783197832,
      "step": 221,
      "training_loss": 8.765615463256836
    },
    {
      "epoch": 0.012032520325203253,
      "step": 222,
      "training_loss": 8.557538986206055
    },
    {
      "epoch": 0.012086720867208672,
      "step": 223,
      "training_loss": 8.861695289611816
    },
    {
      "epoch": 0.012140921409214092,
      "grad_norm": 16.74322509765625,
      "learning_rate": 1e-05,
      "loss": 9.0748,
      "step": 224
    },
    {
      "epoch": 0.012140921409214092,
      "step": 224,
      "training_loss": 9.108297348022461
    },
    {
      "epoch": 0.012195121951219513,
      "step": 225,
      "training_loss": 9.796586036682129
    },
    {
      "epoch": 0.012249322493224933,
      "step": 226,
      "training_loss": 10.130340576171875
    },
    {
      "epoch": 0.012303523035230352,
      "step": 227,
      "training_loss": 8.97702407836914
    },
    {
      "epoch": 0.012357723577235772,
      "grad_norm": 27.27787208557129,
      "learning_rate": 1e-05,
      "loss": 9.5031,
      "step": 228
    },
    {
      "epoch": 0.012357723577235772,
      "step": 228,
      "training_loss": 11.316548347473145
    },
    {
      "epoch": 0.012411924119241193,
      "step": 229,
      "training_loss": 10.519478797912598
    },
    {
      "epoch": 0.012466124661246613,
      "step": 230,
      "training_loss": 8.426501274108887
    },
    {
      "epoch": 0.012520325203252032,
      "step": 231,
      "training_loss": 10.85566234588623
    },
    {
      "epoch": 0.012574525745257453,
      "grad_norm": 46.42202377319336,
      "learning_rate": 1e-05,
      "loss": 10.2795,
      "step": 232
    },
    {
      "epoch": 0.012574525745257453,
      "step": 232,
      "training_loss": 8.400527000427246
    },
    {
      "epoch": 0.012628726287262873,
      "step": 233,
      "training_loss": 8.767032623291016
    },
    {
      "epoch": 0.012682926829268294,
      "step": 234,
      "training_loss": 8.553905487060547
    },
    {
      "epoch": 0.012737127371273712,
      "step": 235,
      "training_loss": 9.756759643554688
    },
    {
      "epoch": 0.012791327913279133,
      "grad_norm": 17.56345558166504,
      "learning_rate": 1e-05,
      "loss": 8.8696,
      "step": 236
    },
    {
      "epoch": 0.012791327913279133,
      "step": 236,
      "training_loss": 8.415847778320312
    },
    {
      "epoch": 0.012845528455284553,
      "step": 237,
      "training_loss": 8.690194129943848
    },
    {
      "epoch": 0.012899728997289974,
      "step": 238,
      "training_loss": 10.289337158203125
    },
    {
      "epoch": 0.012953929539295393,
      "step": 239,
      "training_loss": 10.482305526733398
    },
    {
      "epoch": 0.013008130081300813,
      "grad_norm": 48.825138092041016,
      "learning_rate": 1e-05,
      "loss": 9.4694,
      "step": 240
    },
    {
      "epoch": 0.013008130081300813,
      "step": 240,
      "training_loss": 9.455025672912598
    },
    {
      "epoch": 0.013062330623306233,
      "step": 241,
      "training_loss": 7.950691223144531
    },
    {
      "epoch": 0.013116531165311654,
      "step": 242,
      "training_loss": 10.397004127502441
    },
    {
      "epoch": 0.013170731707317073,
      "step": 243,
      "training_loss": 9.911083221435547
    },
    {
      "epoch": 0.013224932249322493,
      "grad_norm": 71.18138122558594,
      "learning_rate": 1e-05,
      "loss": 9.4285,
      "step": 244
    },
    {
      "epoch": 0.013224932249322493,
      "step": 244,
      "training_loss": 10.34853458404541
    },
    {
      "epoch": 0.013279132791327914,
      "step": 245,
      "training_loss": 9.728510856628418
    },
    {
      "epoch": 0.013333333333333334,
      "step": 246,
      "training_loss": 9.254651069641113
    },
    {
      "epoch": 0.013387533875338753,
      "step": 247,
      "training_loss": 10.217768669128418
    },
    {
      "epoch": 0.013441734417344173,
      "grad_norm": 36.83623504638672,
      "learning_rate": 1e-05,
      "loss": 9.8874,
      "step": 248
    },
    {
      "epoch": 0.013441734417344173,
      "step": 248,
      "training_loss": 8.915474891662598
    },
    {
      "epoch": 0.013495934959349594,
      "step": 249,
      "training_loss": 10.080000877380371
    },
    {
      "epoch": 0.013550135501355014,
      "step": 250,
      "training_loss": 9.276690483093262
    },
    {
      "epoch": 0.013604336043360433,
      "step": 251,
      "training_loss": 8.97238540649414
    },
    {
      "epoch": 0.013658536585365854,
      "grad_norm": 27.018537521362305,
      "learning_rate": 1e-05,
      "loss": 9.3111,
      "step": 252
    },
    {
      "epoch": 0.013658536585365854,
      "step": 252,
      "training_loss": 8.47642707824707
    },
    {
      "epoch": 0.013712737127371274,
      "step": 253,
      "training_loss": 9.077317237854004
    },
    {
      "epoch": 0.013766937669376695,
      "step": 254,
      "training_loss": 8.320564270019531
    },
    {
      "epoch": 0.013821138211382113,
      "step": 255,
      "training_loss": 9.3147611618042
    },
    {
      "epoch": 0.013875338753387534,
      "grad_norm": 16.86305809020996,
      "learning_rate": 1e-05,
      "loss": 8.7973,
      "step": 256
    },
    {
      "epoch": 0.013875338753387534,
      "step": 256,
      "training_loss": 8.058910369873047
    },
    {
      "epoch": 0.013929539295392954,
      "step": 257,
      "training_loss": 8.54357624053955
    },
    {
      "epoch": 0.013983739837398375,
      "step": 258,
      "training_loss": 8.083870887756348
    },
    {
      "epoch": 0.014037940379403794,
      "step": 259,
      "training_loss": 9.136470794677734
    },
    {
      "epoch": 0.014092140921409214,
      "grad_norm": 91.17194366455078,
      "learning_rate": 1e-05,
      "loss": 8.4557,
      "step": 260
    },
    {
      "epoch": 0.014092140921409214,
      "step": 260,
      "training_loss": 10.743507385253906
    },
    {
      "epoch": 0.014146341463414635,
      "step": 261,
      "training_loss": 9.20523738861084
    },
    {
      "epoch": 0.014200542005420055,
      "step": 262,
      "training_loss": 10.02578353881836
    },
    {
      "epoch": 0.014254742547425474,
      "step": 263,
      "training_loss": 9.480998992919922
    },
    {
      "epoch": 0.014308943089430894,
      "grad_norm": 46.80637741088867,
      "learning_rate": 1e-05,
      "loss": 9.8639,
      "step": 264
    },
    {
      "epoch": 0.014308943089430894,
      "step": 264,
      "training_loss": 8.286177635192871
    },
    {
      "epoch": 0.014363143631436315,
      "step": 265,
      "training_loss": 9.340065002441406
    },
    {
      "epoch": 0.014417344173441735,
      "step": 266,
      "training_loss": 10.482244491577148
    },
    {
      "epoch": 0.014471544715447154,
      "step": 267,
      "training_loss": 9.251486778259277
    },
    {
      "epoch": 0.014525745257452575,
      "grad_norm": 27.2625732421875,
      "learning_rate": 1e-05,
      "loss": 9.34,
      "step": 268
    },
    {
      "epoch": 0.014525745257452575,
      "step": 268,
      "training_loss": 10.094221115112305
    },
    {
      "epoch": 0.014579945799457995,
      "step": 269,
      "training_loss": 8.98193073272705
    },
    {
      "epoch": 0.014634146341463415,
      "step": 270,
      "training_loss": 8.341784477233887
    },
    {
      "epoch": 0.014688346883468834,
      "step": 271,
      "training_loss": 9.468655586242676
    },
    {
      "epoch": 0.014742547425474255,
      "grad_norm": 20.20656967163086,
      "learning_rate": 1e-05,
      "loss": 9.2216,
      "step": 272
    },
    {
      "epoch": 0.014742547425474255,
      "step": 272,
      "training_loss": 11.27656364440918
    },
    {
      "epoch": 0.014796747967479675,
      "step": 273,
      "training_loss": 10.032068252563477
    },
    {
      "epoch": 0.014850948509485096,
      "step": 274,
      "training_loss": 8.886849403381348
    },
    {
      "epoch": 0.014905149051490514,
      "step": 275,
      "training_loss": 8.74573802947998
    },
    {
      "epoch": 0.014959349593495935,
      "grad_norm": 19.699695587158203,
      "learning_rate": 1e-05,
      "loss": 9.7353,
      "step": 276
    },
    {
      "epoch": 0.014959349593495935,
      "step": 276,
      "training_loss": 8.745194435119629
    },
    {
      "epoch": 0.015013550135501355,
      "step": 277,
      "training_loss": 8.684534072875977
    },
    {
      "epoch": 0.015067750677506776,
      "step": 278,
      "training_loss": 9.80054759979248
    },
    {
      "epoch": 0.015121951219512195,
      "step": 279,
      "training_loss": 8.774774551391602
    },
    {
      "epoch": 0.015176151761517615,
      "grad_norm": 21.437641143798828,
      "learning_rate": 1e-05,
      "loss": 9.0013,
      "step": 280
    },
    {
      "epoch": 0.015176151761517615,
      "step": 280,
      "training_loss": 10.099853515625
    },
    {
      "epoch": 0.015230352303523036,
      "step": 281,
      "training_loss": 9.792943954467773
    },
    {
      "epoch": 0.015284552845528456,
      "step": 282,
      "training_loss": 9.225262641906738
    },
    {
      "epoch": 0.015338753387533875,
      "step": 283,
      "training_loss": 9.37082290649414
    },
    {
      "epoch": 0.015392953929539295,
      "grad_norm": 30.1685848236084,
      "learning_rate": 1e-05,
      "loss": 9.6222,
      "step": 284
    },
    {
      "epoch": 0.015392953929539295,
      "step": 284,
      "training_loss": 9.023386001586914
    },
    {
      "epoch": 0.015447154471544716,
      "step": 285,
      "training_loss": 10.123665809631348
    },
    {
      "epoch": 0.015501355013550136,
      "step": 286,
      "training_loss": 9.353898048400879
    },
    {
      "epoch": 0.015555555555555555,
      "step": 287,
      "training_loss": 7.633894920349121
    },
    {
      "epoch": 0.015609756097560976,
      "grad_norm": 21.047245025634766,
      "learning_rate": 1e-05,
      "loss": 9.0337,
      "step": 288
    },
    {
      "epoch": 0.015609756097560976,
      "step": 288,
      "training_loss": 8.956399917602539
    },
    {
      "epoch": 0.015663956639566396,
      "step": 289,
      "training_loss": 8.501402854919434
    },
    {
      "epoch": 0.015718157181571817,
      "step": 290,
      "training_loss": 9.322672843933105
    },
    {
      "epoch": 0.015772357723577237,
      "step": 291,
      "training_loss": 8.748307228088379
    },
    {
      "epoch": 0.015826558265582658,
      "grad_norm": 29.912433624267578,
      "learning_rate": 1e-05,
      "loss": 8.8822,
      "step": 292
    },
    {
      "epoch": 0.015826558265582658,
      "step": 292,
      "training_loss": 9.276658058166504
    },
    {
      "epoch": 0.015880758807588075,
      "step": 293,
      "training_loss": 11.696881294250488
    },
    {
      "epoch": 0.015934959349593495,
      "step": 294,
      "training_loss": 9.638446807861328
    },
    {
      "epoch": 0.015989159891598916,
      "step": 295,
      "training_loss": 7.8476033210754395
    },
    {
      "epoch": 0.016043360433604336,
      "grad_norm": 17.330045700073242,
      "learning_rate": 1e-05,
      "loss": 9.6149,
      "step": 296
    },
    {
      "epoch": 0.016043360433604336,
      "step": 296,
      "training_loss": 9.440367698669434
    },
    {
      "epoch": 0.016097560975609757,
      "step": 297,
      "training_loss": 7.9831438064575195
    },
    {
      "epoch": 0.016151761517615177,
      "step": 298,
      "training_loss": 7.866391181945801
    },
    {
      "epoch": 0.016205962059620597,
      "step": 299,
      "training_loss": 9.351469039916992
    },
    {
      "epoch": 0.016260162601626018,
      "grad_norm": 15.587423324584961,
      "learning_rate": 1e-05,
      "loss": 8.6603,
      "step": 300
    },
    {
      "epoch": 0.016260162601626018,
      "step": 300,
      "training_loss": 8.607007026672363
    },
    {
      "epoch": 0.016314363143631435,
      "step": 301,
      "training_loss": 9.57178783416748
    },
    {
      "epoch": 0.016368563685636855,
      "step": 302,
      "training_loss": 9.801836013793945
    },
    {
      "epoch": 0.016422764227642276,
      "step": 303,
      "training_loss": 8.356504440307617
    },
    {
      "epoch": 0.016476964769647696,
      "grad_norm": 14.989166259765625,
      "learning_rate": 1e-05,
      "loss": 9.0843,
      "step": 304
    },
    {
      "epoch": 0.016476964769647696,
      "step": 304,
      "training_loss": 6.732490539550781
    },
    {
      "epoch": 0.016531165311653117,
      "step": 305,
      "training_loss": 9.923141479492188
    },
    {
      "epoch": 0.016585365853658537,
      "step": 306,
      "training_loss": 9.289495468139648
    },
    {
      "epoch": 0.016639566395663958,
      "step": 307,
      "training_loss": 8.725504875183105
    },
    {
      "epoch": 0.01669376693766938,
      "grad_norm": 16.531253814697266,
      "learning_rate": 1e-05,
      "loss": 8.6677,
      "step": 308
    },
    {
      "epoch": 0.01669376693766938,
      "step": 308,
      "training_loss": 9.91627311706543
    },
    {
      "epoch": 0.016747967479674795,
      "step": 309,
      "training_loss": 9.295805931091309
    },
    {
      "epoch": 0.016802168021680216,
      "step": 310,
      "training_loss": 8.279821395874023
    },
    {
      "epoch": 0.016856368563685636,
      "step": 311,
      "training_loss": 11.07154369354248
    },
    {
      "epoch": 0.016910569105691057,
      "grad_norm": 27.21591567993164,
      "learning_rate": 1e-05,
      "loss": 9.6409,
      "step": 312
    },
    {
      "epoch": 0.016910569105691057,
      "step": 312,
      "training_loss": 8.62761116027832
    },
    {
      "epoch": 0.016964769647696477,
      "step": 313,
      "training_loss": 8.079964637756348
    },
    {
      "epoch": 0.017018970189701898,
      "step": 314,
      "training_loss": 6.985485553741455
    },
    {
      "epoch": 0.01707317073170732,
      "step": 315,
      "training_loss": 9.265228271484375
    },
    {
      "epoch": 0.01712737127371274,
      "grad_norm": 23.863388061523438,
      "learning_rate": 1e-05,
      "loss": 8.2396,
      "step": 316
    },
    {
      "epoch": 0.01712737127371274,
      "step": 316,
      "training_loss": 9.071197509765625
    },
    {
      "epoch": 0.017181571815718156,
      "step": 317,
      "training_loss": 9.31923770904541
    },
    {
      "epoch": 0.017235772357723576,
      "step": 318,
      "training_loss": 9.46812915802002
    },
    {
      "epoch": 0.017289972899728997,
      "step": 319,
      "training_loss": 8.964067459106445
    },
    {
      "epoch": 0.017344173441734417,
      "grad_norm": 22.54400062561035,
      "learning_rate": 1e-05,
      "loss": 9.2057,
      "step": 320
    },
    {
      "epoch": 0.017344173441734417,
      "step": 320,
      "training_loss": 9.166092872619629
    },
    {
      "epoch": 0.017398373983739838,
      "step": 321,
      "training_loss": 8.846397399902344
    },
    {
      "epoch": 0.01745257452574526,
      "step": 322,
      "training_loss": 9.016765594482422
    },
    {
      "epoch": 0.01750677506775068,
      "step": 323,
      "training_loss": 8.721442222595215
    },
    {
      "epoch": 0.0175609756097561,
      "grad_norm": 24.970245361328125,
      "learning_rate": 1e-05,
      "loss": 8.9377,
      "step": 324
    },
    {
      "epoch": 0.0175609756097561,
      "step": 324,
      "training_loss": 8.666658401489258
    },
    {
      "epoch": 0.017615176151761516,
      "step": 325,
      "training_loss": 8.0747652053833
    },
    {
      "epoch": 0.017669376693766937,
      "step": 326,
      "training_loss": 9.385220527648926
    },
    {
      "epoch": 0.017723577235772357,
      "step": 327,
      "training_loss": 7.595514297485352
    },
    {
      "epoch": 0.017777777777777778,
      "grad_norm": 15.149890899658203,
      "learning_rate": 1e-05,
      "loss": 8.4305,
      "step": 328
    },
    {
      "epoch": 0.017777777777777778,
      "step": 328,
      "training_loss": 7.909492492675781
    },
    {
      "epoch": 0.017831978319783198,
      "step": 329,
      "training_loss": 8.959549903869629
    },
    {
      "epoch": 0.01788617886178862,
      "step": 330,
      "training_loss": 10.01013469696045
    },
    {
      "epoch": 0.01794037940379404,
      "step": 331,
      "training_loss": 10.243875503540039
    },
    {
      "epoch": 0.01799457994579946,
      "grad_norm": 29.232423782348633,
      "learning_rate": 1e-05,
      "loss": 9.2808,
      "step": 332
    },
    {
      "epoch": 0.01799457994579946,
      "step": 332,
      "training_loss": 8.528722763061523
    },
    {
      "epoch": 0.018048780487804877,
      "step": 333,
      "training_loss": 9.731560707092285
    },
    {
      "epoch": 0.018102981029810297,
      "step": 334,
      "training_loss": 9.827203750610352
    },
    {
      "epoch": 0.018157181571815718,
      "step": 335,
      "training_loss": 8.408852577209473
    },
    {
      "epoch": 0.018211382113821138,
      "grad_norm": 116.2842788696289,
      "learning_rate": 1e-05,
      "loss": 9.1241,
      "step": 336
    },
    {
      "epoch": 0.018211382113821138,
      "step": 336,
      "training_loss": 8.360906600952148
    },
    {
      "epoch": 0.01826558265582656,
      "step": 337,
      "training_loss": 8.78665542602539
    },
    {
      "epoch": 0.01831978319783198,
      "step": 338,
      "training_loss": 8.43526554107666
    },
    {
      "epoch": 0.0183739837398374,
      "step": 339,
      "training_loss": 12.098740577697754
    },
    {
      "epoch": 0.01842818428184282,
      "grad_norm": 325.4991455078125,
      "learning_rate": 1e-05,
      "loss": 9.4204,
      "step": 340
    },
    {
      "epoch": 0.01842818428184282,
      "step": 340,
      "training_loss": 7.886733055114746
    },
    {
      "epoch": 0.018482384823848237,
      "step": 341,
      "training_loss": 7.866455554962158
    },
    {
      "epoch": 0.018536585365853658,
      "step": 342,
      "training_loss": 8.546805381774902
    },
    {
      "epoch": 0.018590785907859078,
      "step": 343,
      "training_loss": 8.790390968322754
    },
    {
      "epoch": 0.0186449864498645,
      "grad_norm": 24.537687301635742,
      "learning_rate": 1e-05,
      "loss": 8.2726,
      "step": 344
    },
    {
      "epoch": 0.0186449864498645,
      "step": 344,
      "training_loss": 8.041144371032715
    },
    {
      "epoch": 0.01869918699186992,
      "step": 345,
      "training_loss": 9.099125862121582
    },
    {
      "epoch": 0.01875338753387534,
      "step": 346,
      "training_loss": 8.649168968200684
    },
    {
      "epoch": 0.01880758807588076,
      "step": 347,
      "training_loss": 8.315924644470215
    },
    {
      "epoch": 0.01886178861788618,
      "grad_norm": 19.284896850585938,
      "learning_rate": 1e-05,
      "loss": 8.5263,
      "step": 348
    },
    {
      "epoch": 0.01886178861788618,
      "step": 348,
      "training_loss": 8.381394386291504
    },
    {
      "epoch": 0.018915989159891598,
      "step": 349,
      "training_loss": 9.773293495178223
    },
    {
      "epoch": 0.018970189701897018,
      "step": 350,
      "training_loss": 9.729175567626953
    },
    {
      "epoch": 0.01902439024390244,
      "step": 351,
      "training_loss": 9.061675071716309
    },
    {
      "epoch": 0.01907859078590786,
      "grad_norm": 22.570791244506836,
      "learning_rate": 1e-05,
      "loss": 9.2364,
      "step": 352
    },
    {
      "epoch": 0.01907859078590786,
      "step": 352,
      "training_loss": 9.205097198486328
    },
    {
      "epoch": 0.01913279132791328,
      "step": 353,
      "training_loss": 8.28036880493164
    },
    {
      "epoch": 0.0191869918699187,
      "step": 354,
      "training_loss": 7.077770233154297
    },
    {
      "epoch": 0.01924119241192412,
      "step": 355,
      "training_loss": 9.249635696411133
    },
    {
      "epoch": 0.01929539295392954,
      "grad_norm": 20.977657318115234,
      "learning_rate": 1e-05,
      "loss": 8.4532,
      "step": 356
    },
    {
      "epoch": 0.01929539295392954,
      "step": 356,
      "training_loss": 7.114163875579834
    },
    {
      "epoch": 0.019349593495934958,
      "step": 357,
      "training_loss": 9.17654800415039
    },
    {
      "epoch": 0.01940379403794038,
      "step": 358,
      "training_loss": 8.6029691696167
    },
    {
      "epoch": 0.0194579945799458,
      "step": 359,
      "training_loss": 8.269427299499512
    },
    {
      "epoch": 0.01951219512195122,
      "grad_norm": 19.475473403930664,
      "learning_rate": 1e-05,
      "loss": 8.2908,
      "step": 360
    },
    {
      "epoch": 0.01951219512195122,
      "step": 360,
      "training_loss": 7.7233123779296875
    },
    {
      "epoch": 0.01956639566395664,
      "step": 361,
      "training_loss": 7.430850505828857
    },
    {
      "epoch": 0.01962059620596206,
      "step": 362,
      "training_loss": 8.918601989746094
    },
    {
      "epoch": 0.01967479674796748,
      "step": 363,
      "training_loss": 8.447885513305664
    },
    {
      "epoch": 0.0197289972899729,
      "grad_norm": 74.73638916015625,
      "learning_rate": 1e-05,
      "loss": 8.1302,
      "step": 364
    },
    {
      "epoch": 0.0197289972899729,
      "step": 364,
      "training_loss": 8.260132789611816
    },
    {
      "epoch": 0.01978319783197832,
      "step": 365,
      "training_loss": 8.113731384277344
    },
    {
      "epoch": 0.01983739837398374,
      "step": 366,
      "training_loss": 8.079197883605957
    },
    {
      "epoch": 0.01989159891598916,
      "step": 367,
      "training_loss": 8.11274242401123
    },
    {
      "epoch": 0.01994579945799458,
      "grad_norm": 11.171524047851562,
      "learning_rate": 1e-05,
      "loss": 8.1415,
      "step": 368
    },
    {
      "epoch": 0.01994579945799458,
      "step": 368,
      "training_loss": 9.181455612182617
    },
    {
      "epoch": 0.02,
      "step": 369,
      "training_loss": 8.42985725402832
    },
    {
      "epoch": 0.02005420054200542,
      "step": 370,
      "training_loss": 8.581767082214355
    },
    {
      "epoch": 0.02010840108401084,
      "step": 371,
      "training_loss": 8.306815147399902
    },
    {
      "epoch": 0.020162601626016262,
      "grad_norm": 15.505874633789062,
      "learning_rate": 1e-05,
      "loss": 8.625,
      "step": 372
    },
    {
      "epoch": 0.020162601626016262,
      "step": 372,
      "training_loss": 8.882105827331543
    },
    {
      "epoch": 0.02021680216802168,
      "step": 373,
      "training_loss": 7.933631420135498
    },
    {
      "epoch": 0.0202710027100271,
      "step": 374,
      "training_loss": 7.909082412719727
    },
    {
      "epoch": 0.02032520325203252,
      "step": 375,
      "training_loss": 8.631102561950684
    },
    {
      "epoch": 0.02037940379403794,
      "grad_norm": 16.70805549621582,
      "learning_rate": 1e-05,
      "loss": 8.339,
      "step": 376
    },
    {
      "epoch": 0.02037940379403794,
      "step": 376,
      "training_loss": 6.825157165527344
    },
    {
      "epoch": 0.02043360433604336,
      "step": 377,
      "training_loss": 8.17951488494873
    },
    {
      "epoch": 0.02048780487804878,
      "step": 378,
      "training_loss": 7.624025344848633
    },
    {
      "epoch": 0.020542005420054202,
      "step": 379,
      "training_loss": 7.595602989196777
    },
    {
      "epoch": 0.020596205962059622,
      "grad_norm": 9.132671356201172,
      "learning_rate": 1e-05,
      "loss": 7.5561,
      "step": 380
    },
    {
      "epoch": 0.020596205962059622,
      "step": 380,
      "training_loss": 8.969121932983398
    },
    {
      "epoch": 0.02065040650406504,
      "step": 381,
      "training_loss": 9.680608749389648
    },
    {
      "epoch": 0.02070460704607046,
      "step": 382,
      "training_loss": 10.503753662109375
    },
    {
      "epoch": 0.02075880758807588,
      "step": 383,
      "training_loss": 7.353460788726807
    },
    {
      "epoch": 0.0208130081300813,
      "grad_norm": 16.1645565032959,
      "learning_rate": 1e-05,
      "loss": 9.1267,
      "step": 384
    },
    {
      "epoch": 0.0208130081300813,
      "step": 384,
      "training_loss": 8.351395606994629
    },
    {
      "epoch": 0.02086720867208672,
      "step": 385,
      "training_loss": 7.837363243103027
    },
    {
      "epoch": 0.020921409214092142,
      "step": 386,
      "training_loss": 8.236838340759277
    },
    {
      "epoch": 0.020975609756097562,
      "step": 387,
      "training_loss": 6.893348693847656
    },
    {
      "epoch": 0.021029810298102983,
      "grad_norm": 161.31109619140625,
      "learning_rate": 1e-05,
      "loss": 7.8297,
      "step": 388
    },
    {
      "epoch": 0.021029810298102983,
      "step": 388,
      "training_loss": 7.9343085289001465
    },
    {
      "epoch": 0.0210840108401084,
      "step": 389,
      "training_loss": 8.428815841674805
    },
    {
      "epoch": 0.02113821138211382,
      "step": 390,
      "training_loss": 9.046061515808105
    },
    {
      "epoch": 0.02119241192411924,
      "step": 391,
      "training_loss": 8.30547046661377
    },
    {
      "epoch": 0.02124661246612466,
      "grad_norm": 22.251420974731445,
      "learning_rate": 1e-05,
      "loss": 8.4287,
      "step": 392
    },
    {
      "epoch": 0.02124661246612466,
      "step": 392,
      "training_loss": 8.12428092956543
    },
    {
      "epoch": 0.02130081300813008,
      "step": 393,
      "training_loss": 7.956228256225586
    },
    {
      "epoch": 0.021355013550135502,
      "step": 394,
      "training_loss": 7.874785423278809
    },
    {
      "epoch": 0.021409214092140923,
      "step": 395,
      "training_loss": 8.476790428161621
    },
    {
      "epoch": 0.021463414634146343,
      "grad_norm": 11.951824188232422,
      "learning_rate": 1e-05,
      "loss": 8.108,
      "step": 396
    },
    {
      "epoch": 0.021463414634146343,
      "step": 396,
      "training_loss": 8.065881729125977
    },
    {
      "epoch": 0.02151761517615176,
      "step": 397,
      "training_loss": 7.801947116851807
    },
    {
      "epoch": 0.02157181571815718,
      "step": 398,
      "training_loss": 7.0356669425964355
    },
    {
      "epoch": 0.0216260162601626,
      "step": 399,
      "training_loss": 7.137936115264893
    },
    {
      "epoch": 0.02168021680216802,
      "grad_norm": 15.099337577819824,
      "learning_rate": 1e-05,
      "loss": 7.5104,
      "step": 400
    },
    {
      "epoch": 0.02168021680216802,
      "step": 400,
      "training_loss": 7.382430076599121
    },
    {
      "epoch": 0.021734417344173442,
      "step": 401,
      "training_loss": 7.641194820404053
    },
    {
      "epoch": 0.021788617886178863,
      "step": 402,
      "training_loss": 8.275113105773926
    },
    {
      "epoch": 0.021842818428184283,
      "step": 403,
      "training_loss": 7.928378105163574
    },
    {
      "epoch": 0.021897018970189704,
      "grad_norm": 22.510404586791992,
      "learning_rate": 1e-05,
      "loss": 7.8068,
      "step": 404
    },
    {
      "epoch": 0.021897018970189704,
      "step": 404,
      "training_loss": 7.857913494110107
    },
    {
      "epoch": 0.02195121951219512,
      "step": 405,
      "training_loss": 7.97804594039917
    },
    {
      "epoch": 0.02200542005420054,
      "step": 406,
      "training_loss": 6.97877836227417
    },
    {
      "epoch": 0.02205962059620596,
      "step": 407,
      "training_loss": 8.23125171661377
    },
    {
      "epoch": 0.022113821138211382,
      "grad_norm": 12.540722846984863,
      "learning_rate": 1e-05,
      "loss": 7.7615,
      "step": 408
    },
    {
      "epoch": 0.022113821138211382,
      "step": 408,
      "training_loss": 8.743800163269043
    },
    {
      "epoch": 0.022168021680216803,
      "step": 409,
      "training_loss": 8.306242942810059
    },
    {
      "epoch": 0.022222222222222223,
      "step": 410,
      "training_loss": 10.09003734588623
    },
    {
      "epoch": 0.022276422764227644,
      "step": 411,
      "training_loss": 8.354555130004883
    },
    {
      "epoch": 0.022330623306233064,
      "grad_norm": 17.296802520751953,
      "learning_rate": 1e-05,
      "loss": 8.8737,
      "step": 412
    },
    {
      "epoch": 0.022330623306233064,
      "step": 412,
      "training_loss": 8.873902320861816
    },
    {
      "epoch": 0.02238482384823848,
      "step": 413,
      "training_loss": 7.847009181976318
    },
    {
      "epoch": 0.0224390243902439,
      "step": 414,
      "training_loss": 7.264033794403076
    },
    {
      "epoch": 0.022493224932249322,
      "step": 415,
      "training_loss": 7.698939800262451
    },
    {
      "epoch": 0.022547425474254743,
      "grad_norm": 44.4743537902832,
      "learning_rate": 1e-05,
      "loss": 7.921,
      "step": 416
    },
    {
      "epoch": 0.022547425474254743,
      "step": 416,
      "training_loss": 8.31988525390625
    },
    {
      "epoch": 0.022601626016260163,
      "step": 417,
      "training_loss": 7.558592319488525
    },
    {
      "epoch": 0.022655826558265584,
      "step": 418,
      "training_loss": 8.039164543151855
    },
    {
      "epoch": 0.022710027100271004,
      "step": 419,
      "training_loss": 7.301027774810791
    },
    {
      "epoch": 0.022764227642276424,
      "grad_norm": 14.116792678833008,
      "learning_rate": 1e-05,
      "loss": 7.8047,
      "step": 420
    },
    {
      "epoch": 0.022764227642276424,
      "step": 420,
      "training_loss": 9.309996604919434
    },
    {
      "epoch": 0.02281842818428184,
      "step": 421,
      "training_loss": 8.074880599975586
    },
    {
      "epoch": 0.022872628726287262,
      "step": 422,
      "training_loss": 6.638279914855957
    },
    {
      "epoch": 0.022926829268292682,
      "step": 423,
      "training_loss": 8.46131420135498
    },
    {
      "epoch": 0.022981029810298103,
      "grad_norm": 13.438453674316406,
      "learning_rate": 1e-05,
      "loss": 8.1211,
      "step": 424
    },
    {
      "epoch": 0.022981029810298103,
      "step": 424,
      "training_loss": 7.890506267547607
    },
    {
      "epoch": 0.023035230352303523,
      "step": 425,
      "training_loss": 7.3360209465026855
    },
    {
      "epoch": 0.023089430894308944,
      "step": 426,
      "training_loss": 7.732000350952148
    },
    {
      "epoch": 0.023143631436314364,
      "step": 427,
      "training_loss": 6.986161708831787
    },
    {
      "epoch": 0.023197831978319785,
      "grad_norm": 22.73676872253418,
      "learning_rate": 1e-05,
      "loss": 7.4862,
      "step": 428
    },
    {
      "epoch": 0.023197831978319785,
      "step": 428,
      "training_loss": 6.317811489105225
    },
    {
      "epoch": 0.023252032520325202,
      "step": 429,
      "training_loss": 7.956981182098389
    },
    {
      "epoch": 0.023306233062330622,
      "step": 430,
      "training_loss": 7.851401329040527
    },
    {
      "epoch": 0.023360433604336043,
      "step": 431,
      "training_loss": 7.931685447692871
    },
    {
      "epoch": 0.023414634146341463,
      "grad_norm": 19.4801025390625,
      "learning_rate": 1e-05,
      "loss": 7.5145,
      "step": 432
    },
    {
      "epoch": 0.023414634146341463,
      "step": 432,
      "training_loss": 8.250307083129883
    },
    {
      "epoch": 0.023468834688346884,
      "step": 433,
      "training_loss": 6.365569591522217
    },
    {
      "epoch": 0.023523035230352304,
      "step": 434,
      "training_loss": 8.734480857849121
    },
    {
      "epoch": 0.023577235772357725,
      "step": 435,
      "training_loss": 7.921696186065674
    },
    {
      "epoch": 0.023631436314363145,
      "grad_norm": 49.99014663696289,
      "learning_rate": 1e-05,
      "loss": 7.818,
      "step": 436
    },
    {
      "epoch": 0.023631436314363145,
      "step": 436,
      "training_loss": 9.406049728393555
    },
    {
      "epoch": 0.023685636856368562,
      "step": 437,
      "training_loss": 8.261984825134277
    },
    {
      "epoch": 0.023739837398373983,
      "step": 438,
      "training_loss": 7.801459312438965
    },
    {
      "epoch": 0.023794037940379403,
      "step": 439,
      "training_loss": 6.366605281829834
    },
    {
      "epoch": 0.023848238482384824,
      "grad_norm": 14.054723739624023,
      "learning_rate": 1e-05,
      "loss": 7.959,
      "step": 440
    },
    {
      "epoch": 0.023848238482384824,
      "step": 440,
      "training_loss": 8.463953018188477
    },
    {
      "epoch": 0.023902439024390244,
      "step": 441,
      "training_loss": 11.978558540344238
    },
    {
      "epoch": 0.023956639566395665,
      "step": 442,
      "training_loss": 7.241355895996094
    },
    {
      "epoch": 0.024010840108401085,
      "step": 443,
      "training_loss": 7.96688985824585
    },
    {
      "epoch": 0.024065040650406506,
      "grad_norm": 20.506898880004883,
      "learning_rate": 1e-05,
      "loss": 8.9127,
      "step": 444
    },
    {
      "epoch": 0.024065040650406506,
      "step": 444,
      "training_loss": 8.147042274475098
    },
    {
      "epoch": 0.024119241192411923,
      "step": 445,
      "training_loss": 8.120525360107422
    },
    {
      "epoch": 0.024173441734417343,
      "step": 446,
      "training_loss": 7.316007614135742
    },
    {
      "epoch": 0.024227642276422764,
      "step": 447,
      "training_loss": 7.652131080627441
    },
    {
      "epoch": 0.024281842818428184,
      "grad_norm": 13.76490592956543,
      "learning_rate": 1e-05,
      "loss": 7.8089,
      "step": 448
    },
    {
      "epoch": 0.024281842818428184,
      "step": 448,
      "training_loss": 7.5631890296936035
    },
    {
      "epoch": 0.024336043360433605,
      "step": 449,
      "training_loss": 8.733774185180664
    },
    {
      "epoch": 0.024390243902439025,
      "step": 450,
      "training_loss": 7.819546699523926
    },
    {
      "epoch": 0.024444444444444446,
      "step": 451,
      "training_loss": 8.358509063720703
    },
    {
      "epoch": 0.024498644986449866,
      "grad_norm": 11.693607330322266,
      "learning_rate": 1e-05,
      "loss": 8.1188,
      "step": 452
    },
    {
      "epoch": 0.024498644986449866,
      "step": 452,
      "training_loss": 8.049361228942871
    },
    {
      "epoch": 0.024552845528455283,
      "step": 453,
      "training_loss": 7.477034568786621
    },
    {
      "epoch": 0.024607046070460704,
      "step": 454,
      "training_loss": 8.694418907165527
    },
    {
      "epoch": 0.024661246612466124,
      "step": 455,
      "training_loss": 7.679545879364014
    },
    {
      "epoch": 0.024715447154471545,
      "grad_norm": 14.152242660522461,
      "learning_rate": 1e-05,
      "loss": 7.9751,
      "step": 456
    },
    {
      "epoch": 0.024715447154471545,
      "step": 456,
      "training_loss": 6.727057456970215
    },
    {
      "epoch": 0.024769647696476965,
      "step": 457,
      "training_loss": 8.258109092712402
    },
    {
      "epoch": 0.024823848238482386,
      "step": 458,
      "training_loss": 6.571463108062744
    },
    {
      "epoch": 0.024878048780487806,
      "step": 459,
      "training_loss": 9.265181541442871
    },
    {
      "epoch": 0.024932249322493227,
      "grad_norm": 95.29155731201172,
      "learning_rate": 1e-05,
      "loss": 7.7055,
      "step": 460
    },
    {
      "epoch": 0.024932249322493227,
      "step": 460,
      "training_loss": 7.959630012512207
    },
    {
      "epoch": 0.024986449864498644,
      "step": 461,
      "training_loss": 8.809687614440918
    },
    {
      "epoch": 0.025040650406504064,
      "step": 462,
      "training_loss": 8.034317970275879
    },
    {
      "epoch": 0.025094850948509485,
      "step": 463,
      "training_loss": 9.111335754394531
    },
    {
      "epoch": 0.025149051490514905,
      "grad_norm": 17.28402328491211,
      "learning_rate": 1e-05,
      "loss": 8.4787,
      "step": 464
    },
    {
      "epoch": 0.025149051490514905,
      "step": 464,
      "training_loss": 7.907818794250488
    },
    {
      "epoch": 0.025203252032520326,
      "step": 465,
      "training_loss": 6.366112232208252
    },
    {
      "epoch": 0.025257452574525746,
      "step": 466,
      "training_loss": 8.446928977966309
    },
    {
      "epoch": 0.025311653116531167,
      "step": 467,
      "training_loss": 8.859402656555176
    },
    {
      "epoch": 0.025365853658536587,
      "grad_norm": 25.955493927001953,
      "learning_rate": 1e-05,
      "loss": 7.8951,
      "step": 468
    },
    {
      "epoch": 0.025365853658536587,
      "step": 468,
      "training_loss": 7.934590816497803
    },
    {
      "epoch": 0.025420054200542004,
      "step": 469,
      "training_loss": 7.631979942321777
    },
    {
      "epoch": 0.025474254742547425,
      "step": 470,
      "training_loss": 9.382837295532227
    },
    {
      "epoch": 0.025528455284552845,
      "step": 471,
      "training_loss": 7.657494068145752
    },
    {
      "epoch": 0.025582655826558266,
      "grad_norm": 12.350764274597168,
      "learning_rate": 1e-05,
      "loss": 8.1517,
      "step": 472
    },
    {
      "epoch": 0.025582655826558266,
      "step": 472,
      "training_loss": 7.057915210723877
    },
    {
      "epoch": 0.025636856368563686,
      "step": 473,
      "training_loss": 8.141605377197266
    },
    {
      "epoch": 0.025691056910569107,
      "step": 474,
      "training_loss": 6.964909553527832
    },
    {
      "epoch": 0.025745257452574527,
      "step": 475,
      "training_loss": 8.538036346435547
    },
    {
      "epoch": 0.025799457994579948,
      "grad_norm": 11.41733169555664,
      "learning_rate": 1e-05,
      "loss": 7.6756,
      "step": 476
    },
    {
      "epoch": 0.025799457994579948,
      "step": 476,
      "training_loss": 6.934912204742432
    },
    {
      "epoch": 0.025853658536585365,
      "step": 477,
      "training_loss": 6.558340549468994
    },
    {
      "epoch": 0.025907859078590785,
      "step": 478,
      "training_loss": 8.120965003967285
    },
    {
      "epoch": 0.025962059620596206,
      "step": 479,
      "training_loss": 8.37429428100586
    },
    {
      "epoch": 0.026016260162601626,
      "grad_norm": 29.7208194732666,
      "learning_rate": 1e-05,
      "loss": 7.4971,
      "step": 480
    },
    {
      "epoch": 0.026016260162601626,
      "step": 480,
      "training_loss": 6.973459243774414
    },
    {
      "epoch": 0.026070460704607046,
      "step": 481,
      "training_loss": 7.689476490020752
    },
    {
      "epoch": 0.026124661246612467,
      "step": 482,
      "training_loss": 7.858756065368652
    },
    {
      "epoch": 0.026178861788617887,
      "step": 483,
      "training_loss": 7.241107940673828
    },
    {
      "epoch": 0.026233062330623308,
      "grad_norm": 24.06954002380371,
      "learning_rate": 1e-05,
      "loss": 7.4407,
      "step": 484
    },
    {
      "epoch": 0.026233062330623308,
      "step": 484,
      "training_loss": 8.792257308959961
    },
    {
      "epoch": 0.026287262872628725,
      "step": 485,
      "training_loss": 8.03976058959961
    },
    {
      "epoch": 0.026341463414634145,
      "step": 486,
      "training_loss": 7.354308605194092
    },
    {
      "epoch": 0.026395663956639566,
      "step": 487,
      "training_loss": 8.887027740478516
    },
    {
      "epoch": 0.026449864498644986,
      "grad_norm": 20.02303695678711,
      "learning_rate": 1e-05,
      "loss": 8.2683,
      "step": 488
    },
    {
      "epoch": 0.026449864498644986,
      "step": 488,
      "training_loss": 8.766927719116211
    },
    {
      "epoch": 0.026504065040650407,
      "step": 489,
      "training_loss": 9.024206161499023
    },
    {
      "epoch": 0.026558265582655827,
      "step": 490,
      "training_loss": 6.299105644226074
    },
    {
      "epoch": 0.026612466124661248,
      "step": 491,
      "training_loss": 6.3097405433654785
    },
    {
      "epoch": 0.02666666666666667,
      "grad_norm": 16.101600646972656,
      "learning_rate": 1e-05,
      "loss": 7.6,
      "step": 492
    },
    {
      "epoch": 0.02666666666666667,
      "step": 492,
      "training_loss": 8.434672355651855
    },
    {
      "epoch": 0.026720867208672085,
      "step": 493,
      "training_loss": 7.963082313537598
    },
    {
      "epoch": 0.026775067750677506,
      "step": 494,
      "training_loss": 8.284965515136719
    },
    {
      "epoch": 0.026829268292682926,
      "step": 495,
      "training_loss": 7.380144119262695
    },
    {
      "epoch": 0.026883468834688347,
      "grad_norm": 13.190058708190918,
      "learning_rate": 1e-05,
      "loss": 8.0157,
      "step": 496
    },
    {
      "epoch": 0.026883468834688347,
      "step": 496,
      "training_loss": 8.209112167358398
    },
    {
      "epoch": 0.026937669376693767,
      "step": 497,
      "training_loss": 6.986357688903809
    },
    {
      "epoch": 0.026991869918699188,
      "step": 498,
      "training_loss": 8.515549659729004
    },
    {
      "epoch": 0.02704607046070461,
      "step": 499,
      "training_loss": 7.201557159423828
    },
    {
      "epoch": 0.02710027100271003,
      "grad_norm": 12.486780166625977,
      "learning_rate": 1e-05,
      "loss": 7.7281,
      "step": 500
    },
    {
      "epoch": 0.02710027100271003,
      "step": 500,
      "training_loss": 7.13062858581543
    },
    {
      "epoch": 0.027154471544715446,
      "step": 501,
      "training_loss": 8.09872055053711
    },
    {
      "epoch": 0.027208672086720866,
      "step": 502,
      "training_loss": 8.761409759521484
    },
    {
      "epoch": 0.027262872628726287,
      "step": 503,
      "training_loss": 6.616372108459473
    },
    {
      "epoch": 0.027317073170731707,
      "grad_norm": 25.115819931030273,
      "learning_rate": 1e-05,
      "loss": 7.6518,
      "step": 504
    },
    {
      "epoch": 0.027317073170731707,
      "step": 504,
      "training_loss": 8.176163673400879
    },
    {
      "epoch": 0.027371273712737128,
      "step": 505,
      "training_loss": 7.771926403045654
    },
    {
      "epoch": 0.02742547425474255,
      "step": 506,
      "training_loss": 7.734491348266602
    },
    {
      "epoch": 0.02747967479674797,
      "step": 507,
      "training_loss": 7.998220443725586
    },
    {
      "epoch": 0.02753387533875339,
      "grad_norm": 13.558465957641602,
      "learning_rate": 1e-05,
      "loss": 7.9202,
      "step": 508
    },
    {
      "epoch": 0.02753387533875339,
      "step": 508,
      "training_loss": 7.738372802734375
    },
    {
      "epoch": 0.027588075880758806,
      "step": 509,
      "training_loss": 8.241597175598145
    },
    {
      "epoch": 0.027642276422764227,
      "step": 510,
      "training_loss": 8.918270111083984
    },
    {
      "epoch": 0.027696476964769647,
      "step": 511,
      "training_loss": 6.77186918258667
    },
    {
      "epoch": 0.027750677506775068,
      "grad_norm": 10.977042198181152,
      "learning_rate": 1e-05,
      "loss": 7.9175,
      "step": 512
    },
    {
      "epoch": 0.027750677506775068,
      "step": 512,
      "training_loss": 7.243427276611328
    },
    {
      "epoch": 0.027804878048780488,
      "step": 513,
      "training_loss": 7.145732402801514
    },
    {
      "epoch": 0.02785907859078591,
      "step": 514,
      "training_loss": 7.725897789001465
    },
    {
      "epoch": 0.02791327913279133,
      "step": 515,
      "training_loss": 7.917025566101074
    },
    {
      "epoch": 0.02796747967479675,
      "grad_norm": 51.873714447021484,
      "learning_rate": 1e-05,
      "loss": 7.508,
      "step": 516
    },
    {
      "epoch": 0.02796747967479675,
      "step": 516,
      "training_loss": 8.662243843078613
    },
    {
      "epoch": 0.028021680216802167,
      "step": 517,
      "training_loss": 7.13602876663208
    },
    {
      "epoch": 0.028075880758807587,
      "step": 518,
      "training_loss": 7.190653324127197
    },
    {
      "epoch": 0.028130081300813008,
      "step": 519,
      "training_loss": 7.300667762756348
    },
    {
      "epoch": 0.028184281842818428,
      "grad_norm": 83.92121887207031,
      "learning_rate": 1e-05,
      "loss": 7.5724,
      "step": 520
    },
    {
      "epoch": 0.028184281842818428,
      "step": 520,
      "training_loss": 7.369582653045654
    },
    {
      "epoch": 0.02823848238482385,
      "step": 521,
      "training_loss": 8.244224548339844
    },
    {
      "epoch": 0.02829268292682927,
      "step": 522,
      "training_loss": 7.870123386383057
    },
    {
      "epoch": 0.02834688346883469,
      "step": 523,
      "training_loss": 8.35583209991455
    },
    {
      "epoch": 0.02840108401084011,
      "grad_norm": 14.924042701721191,
      "learning_rate": 1e-05,
      "loss": 7.9599,
      "step": 524
    },
    {
      "epoch": 0.02840108401084011,
      "step": 524,
      "training_loss": 7.588839054107666
    },
    {
      "epoch": 0.028455284552845527,
      "step": 525,
      "training_loss": 7.083045482635498
    },
    {
      "epoch": 0.028509485094850948,
      "step": 526,
      "training_loss": 7.160036563873291
    },
    {
      "epoch": 0.028563685636856368,
      "step": 527,
      "training_loss": 7.558352947235107
    },
    {
      "epoch": 0.02861788617886179,
      "grad_norm": 10.282278060913086,
      "learning_rate": 1e-05,
      "loss": 7.3476,
      "step": 528
    },
    {
      "epoch": 0.02861788617886179,
      "step": 528,
      "training_loss": 7.837279796600342
    },
    {
      "epoch": 0.02867208672086721,
      "step": 529,
      "training_loss": 8.573434829711914
    },
    {
      "epoch": 0.02872628726287263,
      "step": 530,
      "training_loss": 7.385232925415039
    },
    {
      "epoch": 0.02878048780487805,
      "step": 531,
      "training_loss": 8.93227481842041
    },
    {
      "epoch": 0.02883468834688347,
      "grad_norm": 28.24067497253418,
      "learning_rate": 1e-05,
      "loss": 8.1821,
      "step": 532
    },
    {
      "epoch": 0.02883468834688347,
      "step": 532,
      "training_loss": 8.903108596801758
    },
    {
      "epoch": 0.028888888888888888,
      "step": 533,
      "training_loss": 8.694533348083496
    },
    {
      "epoch": 0.028943089430894308,
      "step": 534,
      "training_loss": 12.607704162597656
    },
    {
      "epoch": 0.02899728997289973,
      "step": 535,
      "training_loss": 7.098885536193848
    },
    {
      "epoch": 0.02905149051490515,
      "grad_norm": 19.555856704711914,
      "learning_rate": 1e-05,
      "loss": 9.3261,
      "step": 536
    },
    {
      "epoch": 0.02905149051490515,
      "step": 536,
      "training_loss": 7.145205974578857
    },
    {
      "epoch": 0.02910569105691057,
      "step": 537,
      "training_loss": 7.424733638763428
    },
    {
      "epoch": 0.02915989159891599,
      "step": 538,
      "training_loss": 9.446101188659668
    },
    {
      "epoch": 0.02921409214092141,
      "step": 539,
      "training_loss": 8.077714920043945
    },
    {
      "epoch": 0.02926829268292683,
      "grad_norm": 13.626842498779297,
      "learning_rate": 1e-05,
      "loss": 8.0234,
      "step": 540
    },
    {
      "epoch": 0.02926829268292683,
      "step": 540,
      "training_loss": 8.232004165649414
    },
    {
      "epoch": 0.029322493224932248,
      "step": 541,
      "training_loss": 8.675039291381836
    },
    {
      "epoch": 0.02937669376693767,
      "step": 542,
      "training_loss": 8.267992973327637
    },
    {
      "epoch": 0.02943089430894309,
      "step": 543,
      "training_loss": 7.682991981506348
    },
    {
      "epoch": 0.02948509485094851,
      "grad_norm": 13.600375175476074,
      "learning_rate": 1e-05,
      "loss": 8.2145,
      "step": 544
    },
    {
      "epoch": 0.02948509485094851,
      "step": 544,
      "training_loss": 7.752343654632568
    },
    {
      "epoch": 0.02953929539295393,
      "step": 545,
      "training_loss": 7.010008811950684
    },
    {
      "epoch": 0.02959349593495935,
      "step": 546,
      "training_loss": 7.402151584625244
    },
    {
      "epoch": 0.02964769647696477,
      "step": 547,
      "training_loss": 8.647198677062988
    },
    {
      "epoch": 0.02970189701897019,
      "grad_norm": 18.925647735595703,
      "learning_rate": 1e-05,
      "loss": 7.7029,
      "step": 548
    },
    {
      "epoch": 0.02970189701897019,
      "step": 548,
      "training_loss": 7.286717891693115
    },
    {
      "epoch": 0.02975609756097561,
      "step": 549,
      "training_loss": 8.617945671081543
    },
    {
      "epoch": 0.02981029810298103,
      "step": 550,
      "training_loss": 6.48945426940918
    },
    {
      "epoch": 0.02986449864498645,
      "step": 551,
      "training_loss": 7.536012172698975
    },
    {
      "epoch": 0.02991869918699187,
      "grad_norm": 8.868014335632324,
      "learning_rate": 1e-05,
      "loss": 7.4825,
      "step": 552
    },
    {
      "epoch": 0.02991869918699187,
      "step": 552,
      "training_loss": 7.655765533447266
    },
    {
      "epoch": 0.02997289972899729,
      "step": 553,
      "training_loss": 8.01198673248291
    },
    {
      "epoch": 0.03002710027100271,
      "step": 554,
      "training_loss": 8.47567367553711
    },
    {
      "epoch": 0.03008130081300813,
      "step": 555,
      "training_loss": 7.649114608764648
    },
    {
      "epoch": 0.030135501355013552,
      "grad_norm": 19.67048454284668,
      "learning_rate": 1e-05,
      "loss": 7.9481,
      "step": 556
    },
    {
      "epoch": 0.030135501355013552,
      "step": 556,
      "training_loss": 8.006077766418457
    },
    {
      "epoch": 0.03018970189701897,
      "step": 557,
      "training_loss": 7.4127960205078125
    },
    {
      "epoch": 0.03024390243902439,
      "step": 558,
      "training_loss": 7.562286853790283
    },
    {
      "epoch": 0.03029810298102981,
      "step": 559,
      "training_loss": 6.267316818237305
    },
    {
      "epoch": 0.03035230352303523,
      "grad_norm": 13.241568565368652,
      "learning_rate": 1e-05,
      "loss": 7.3121,
      "step": 560
    },
    {
      "epoch": 0.03035230352303523,
      "step": 560,
      "training_loss": 8.27786636352539
    },
    {
      "epoch": 0.03040650406504065,
      "step": 561,
      "training_loss": 7.505637168884277
    },
    {
      "epoch": 0.03046070460704607,
      "step": 562,
      "training_loss": 6.8319244384765625
    },
    {
      "epoch": 0.030514905149051492,
      "step": 563,
      "training_loss": 6.262115955352783
    },
    {
      "epoch": 0.030569105691056912,
      "grad_norm": 13.091245651245117,
      "learning_rate": 1e-05,
      "loss": 7.2194,
      "step": 564
    },
    {
      "epoch": 0.030569105691056912,
      "step": 564,
      "training_loss": 7.486822605133057
    },
    {
      "epoch": 0.03062330623306233,
      "step": 565,
      "training_loss": 6.506314754486084
    },
    {
      "epoch": 0.03067750677506775,
      "step": 566,
      "training_loss": 7.852730751037598
    },
    {
      "epoch": 0.03073170731707317,
      "step": 567,
      "training_loss": 9.516322135925293
    },
    {
      "epoch": 0.03078590785907859,
      "grad_norm": 22.927791595458984,
      "learning_rate": 1e-05,
      "loss": 7.8405,
      "step": 568
    },
    {
      "epoch": 0.03078590785907859,
      "step": 568,
      "training_loss": 7.596273899078369
    },
    {
      "epoch": 0.03084010840108401,
      "step": 569,
      "training_loss": 6.8946309089660645
    },
    {
      "epoch": 0.030894308943089432,
      "step": 570,
      "training_loss": 7.808889865875244
    },
    {
      "epoch": 0.030948509485094852,
      "step": 571,
      "training_loss": 7.22052001953125
    },
    {
      "epoch": 0.031002710027100273,
      "grad_norm": 19.072341918945312,
      "learning_rate": 1e-05,
      "loss": 7.3801,
      "step": 572
    },
    {
      "epoch": 0.031002710027100273,
      "step": 572,
      "training_loss": 7.628958225250244
    },
    {
      "epoch": 0.03105691056910569,
      "step": 573,
      "training_loss": 7.715534687042236
    },
    {
      "epoch": 0.03111111111111111,
      "step": 574,
      "training_loss": 9.171743392944336
    },
    {
      "epoch": 0.03116531165311653,
      "step": 575,
      "training_loss": 6.987898349761963
    },
    {
      "epoch": 0.03121951219512195,
      "grad_norm": 13.227346420288086,
      "learning_rate": 1e-05,
      "loss": 7.876,
      "step": 576
    },
    {
      "epoch": 0.03121951219512195,
      "step": 576,
      "training_loss": 7.856621742248535
    },
    {
      "epoch": 0.03127371273712737,
      "step": 577,
      "training_loss": 8.118185043334961
    },
    {
      "epoch": 0.03132791327913279,
      "step": 578,
      "training_loss": 7.513347625732422
    },
    {
      "epoch": 0.03138211382113821,
      "step": 579,
      "training_loss": 6.811148166656494
    },
    {
      "epoch": 0.03143631436314363,
      "grad_norm": 21.458024978637695,
      "learning_rate": 1e-05,
      "loss": 7.5748,
      "step": 580
    },
    {
      "epoch": 0.03143631436314363,
      "step": 580,
      "training_loss": 6.786680698394775
    },
    {
      "epoch": 0.03149051490514905,
      "step": 581,
      "training_loss": 7.159631252288818
    },
    {
      "epoch": 0.031544715447154474,
      "step": 582,
      "training_loss": 8.047096252441406
    },
    {
      "epoch": 0.03159891598915989,
      "step": 583,
      "training_loss": 7.907321929931641
    },
    {
      "epoch": 0.031653116531165315,
      "grad_norm": 21.504545211791992,
      "learning_rate": 1e-05,
      "loss": 7.4752,
      "step": 584
    },
    {
      "epoch": 0.031653116531165315,
      "step": 584,
      "training_loss": 7.784085273742676
    },
    {
      "epoch": 0.03170731707317073,
      "step": 585,
      "training_loss": 8.942150115966797
    },
    {
      "epoch": 0.03176151761517615,
      "step": 586,
      "training_loss": 9.160422325134277
    },
    {
      "epoch": 0.03181571815718157,
      "step": 587,
      "training_loss": 8.868200302124023
    },
    {
      "epoch": 0.03186991869918699,
      "grad_norm": 29.23548126220703,
      "learning_rate": 1e-05,
      "loss": 8.6887,
      "step": 588
    },
    {
      "epoch": 0.03186991869918699,
      "step": 588,
      "training_loss": 6.81386661529541
    },
    {
      "epoch": 0.031924119241192414,
      "step": 589,
      "training_loss": 8.087862968444824
    },
    {
      "epoch": 0.03197831978319783,
      "step": 590,
      "training_loss": 7.766304016113281
    },
    {
      "epoch": 0.032032520325203255,
      "step": 591,
      "training_loss": 7.51026725769043
    },
    {
      "epoch": 0.03208672086720867,
      "grad_norm": 10.99610710144043,
      "learning_rate": 1e-05,
      "loss": 7.5446,
      "step": 592
    },
    {
      "epoch": 0.03208672086720867,
      "step": 592,
      "training_loss": 7.143747806549072
    },
    {
      "epoch": 0.03214092140921409,
      "step": 593,
      "training_loss": 7.672973155975342
    },
    {
      "epoch": 0.03219512195121951,
      "step": 594,
      "training_loss": 7.325706481933594
    },
    {
      "epoch": 0.03224932249322493,
      "step": 595,
      "training_loss": 8.01272201538086
    },
    {
      "epoch": 0.032303523035230354,
      "grad_norm": 19.675317764282227,
      "learning_rate": 1e-05,
      "loss": 7.5388,
      "step": 596
    },
    {
      "epoch": 0.032303523035230354,
      "step": 596,
      "training_loss": 7.754485607147217
    },
    {
      "epoch": 0.03235772357723577,
      "step": 597,
      "training_loss": 8.303577423095703
    },
    {
      "epoch": 0.032411924119241195,
      "step": 598,
      "training_loss": 7.519500255584717
    },
    {
      "epoch": 0.03246612466124661,
      "step": 599,
      "training_loss": 7.545088291168213
    },
    {
      "epoch": 0.032520325203252036,
      "grad_norm": 11.03748607635498,
      "learning_rate": 1e-05,
      "loss": 7.7807,
      "step": 600
    },
    {
      "epoch": 0.032520325203252036,
      "step": 600,
      "training_loss": 8.318832397460938
    },
    {
      "epoch": 0.03257452574525745,
      "step": 601,
      "training_loss": 8.336999893188477
    },
    {
      "epoch": 0.03262872628726287,
      "step": 602,
      "training_loss": 8.142541885375977
    },
    {
      "epoch": 0.032682926829268294,
      "step": 603,
      "training_loss": 7.395812511444092
    },
    {
      "epoch": 0.03273712737127371,
      "grad_norm": 13.745853424072266,
      "learning_rate": 1e-05,
      "loss": 8.0485,
      "step": 604
    },
    {
      "epoch": 0.03273712737127371,
      "step": 604,
      "training_loss": 7.020951747894287
    },
    {
      "epoch": 0.032791327913279135,
      "step": 605,
      "training_loss": 6.944255828857422
    },
    {
      "epoch": 0.03284552845528455,
      "step": 606,
      "training_loss": 7.477126121520996
    },
    {
      "epoch": 0.032899728997289976,
      "step": 607,
      "training_loss": 5.148961544036865
    },
    {
      "epoch": 0.03295392953929539,
      "grad_norm": 27.703140258789062,
      "learning_rate": 1e-05,
      "loss": 6.6478,
      "step": 608
    },
    {
      "epoch": 0.03295392953929539,
      "step": 608,
      "training_loss": 7.075716972351074
    },
    {
      "epoch": 0.03300813008130081,
      "step": 609,
      "training_loss": 7.870292663574219
    },
    {
      "epoch": 0.033062330623306234,
      "step": 610,
      "training_loss": 7.562296390533447
    },
    {
      "epoch": 0.03311653116531165,
      "step": 611,
      "training_loss": 6.005953788757324
    },
    {
      "epoch": 0.033170731707317075,
      "grad_norm": 16.46178436279297,
      "learning_rate": 1e-05,
      "loss": 7.1286,
      "step": 612
    },
    {
      "epoch": 0.033170731707317075,
      "step": 612,
      "training_loss": 7.661654472351074
    },
    {
      "epoch": 0.03322493224932249,
      "step": 613,
      "training_loss": 8.430442810058594
    },
    {
      "epoch": 0.033279132791327916,
      "step": 614,
      "training_loss": 8.001923561096191
    },
    {
      "epoch": 0.03333333333333333,
      "step": 615,
      "training_loss": 6.9261674880981445
    },
    {
      "epoch": 0.03338753387533876,
      "grad_norm": 14.56048583984375,
      "learning_rate": 1e-05,
      "loss": 7.755,
      "step": 616
    },
    {
      "epoch": 0.03338753387533876,
      "step": 616,
      "training_loss": 7.840579509735107
    },
    {
      "epoch": 0.033441734417344174,
      "step": 617,
      "training_loss": 7.342605113983154
    },
    {
      "epoch": 0.03349593495934959,
      "step": 618,
      "training_loss": 6.175075531005859
    },
    {
      "epoch": 0.033550135501355015,
      "step": 619,
      "training_loss": 7.016615867614746
    },
    {
      "epoch": 0.03360433604336043,
      "grad_norm": 14.835762023925781,
      "learning_rate": 1e-05,
      "loss": 7.0937,
      "step": 620
    },
    {
      "epoch": 0.03360433604336043,
      "step": 620,
      "training_loss": 8.156594276428223
    },
    {
      "epoch": 0.033658536585365856,
      "step": 621,
      "training_loss": 7.633522987365723
    },
    {
      "epoch": 0.03371273712737127,
      "step": 622,
      "training_loss": 7.895918846130371
    },
    {
      "epoch": 0.0337669376693767,
      "step": 623,
      "training_loss": 8.076791763305664
    },
    {
      "epoch": 0.033821138211382114,
      "grad_norm": 18.83048439025879,
      "learning_rate": 1e-05,
      "loss": 7.9407,
      "step": 624
    },
    {
      "epoch": 0.033821138211382114,
      "step": 624,
      "training_loss": 8.693800926208496
    },
    {
      "epoch": 0.03387533875338753,
      "step": 625,
      "training_loss": 8.160466194152832
    },
    {
      "epoch": 0.033929539295392955,
      "step": 626,
      "training_loss": 7.449671745300293
    },
    {
      "epoch": 0.03398373983739837,
      "step": 627,
      "training_loss": 7.530853271484375
    },
    {
      "epoch": 0.034037940379403796,
      "grad_norm": 16.453369140625,
      "learning_rate": 1e-05,
      "loss": 7.9587,
      "step": 628
    },
    {
      "epoch": 0.034037940379403796,
      "step": 628,
      "training_loss": 6.371591567993164
    },
    {
      "epoch": 0.03409214092140921,
      "step": 629,
      "training_loss": 7.984294891357422
    },
    {
      "epoch": 0.03414634146341464,
      "step": 630,
      "training_loss": 6.843656063079834
    },
    {
      "epoch": 0.034200542005420054,
      "step": 631,
      "training_loss": 7.089364528656006
    },
    {
      "epoch": 0.03425474254742548,
      "grad_norm": 18.147918701171875,
      "learning_rate": 1e-05,
      "loss": 7.0722,
      "step": 632
    },
    {
      "epoch": 0.03425474254742548,
      "step": 632,
      "training_loss": 5.8620734214782715
    },
    {
      "epoch": 0.034308943089430895,
      "step": 633,
      "training_loss": 7.434935092926025
    },
    {
      "epoch": 0.03436314363143631,
      "step": 634,
      "training_loss": 7.697505474090576
    },
    {
      "epoch": 0.034417344173441736,
      "step": 635,
      "training_loss": 8.205038070678711
    },
    {
      "epoch": 0.03447154471544715,
      "grad_norm": 14.699790954589844,
      "learning_rate": 1e-05,
      "loss": 7.2999,
      "step": 636
    },
    {
      "epoch": 0.03447154471544715,
      "step": 636,
      "training_loss": 8.378556251525879
    },
    {
      "epoch": 0.03452574525745258,
      "step": 637,
      "training_loss": 7.40627908706665
    },
    {
      "epoch": 0.034579945799457994,
      "step": 638,
      "training_loss": 6.95586633682251
    },
    {
      "epoch": 0.03463414634146342,
      "step": 639,
      "training_loss": 8.133492469787598
    },
    {
      "epoch": 0.034688346883468835,
      "grad_norm": 18.504623413085938,
      "learning_rate": 1e-05,
      "loss": 7.7185,
      "step": 640
    },
    {
      "epoch": 0.034688346883468835,
      "step": 640,
      "training_loss": 6.613019943237305
    },
    {
      "epoch": 0.03474254742547425,
      "step": 641,
      "training_loss": 6.14841890335083
    },
    {
      "epoch": 0.034796747967479676,
      "step": 642,
      "training_loss": 6.311915874481201
    },
    {
      "epoch": 0.03485094850948509,
      "step": 643,
      "training_loss": 7.22858190536499
    },
    {
      "epoch": 0.03490514905149052,
      "grad_norm": 15.910090446472168,
      "learning_rate": 1e-05,
      "loss": 6.5755,
      "step": 644
    },
    {
      "epoch": 0.03490514905149052,
      "step": 644,
      "training_loss": 8.16611099243164
    },
    {
      "epoch": 0.034959349593495934,
      "step": 645,
      "training_loss": 8.402572631835938
    },
    {
      "epoch": 0.03501355013550136,
      "step": 646,
      "training_loss": 6.247028827667236
    },
    {
      "epoch": 0.035067750677506775,
      "step": 647,
      "training_loss": 7.658950328826904
    },
    {
      "epoch": 0.0351219512195122,
      "grad_norm": 14.55771255493164,
      "learning_rate": 1e-05,
      "loss": 7.6187,
      "step": 648
    },
    {
      "epoch": 0.0351219512195122,
      "step": 648,
      "training_loss": 8.1662015914917
    },
    {
      "epoch": 0.035176151761517616,
      "step": 649,
      "training_loss": 5.8511457443237305
    },
    {
      "epoch": 0.03523035230352303,
      "step": 650,
      "training_loss": 7.341268539428711
    },
    {
      "epoch": 0.03528455284552846,
      "step": 651,
      "training_loss": 9.425777435302734
    },
    {
      "epoch": 0.035338753387533874,
      "grad_norm": 22.843904495239258,
      "learning_rate": 1e-05,
      "loss": 7.6961,
      "step": 652
    },
    {
      "epoch": 0.035338753387533874,
      "step": 652,
      "training_loss": 7.36356782913208
    },
    {
      "epoch": 0.0353929539295393,
      "step": 653,
      "training_loss": 7.418573379516602
    },
    {
      "epoch": 0.035447154471544715,
      "step": 654,
      "training_loss": 8.25290298461914
    },
    {
      "epoch": 0.03550135501355014,
      "step": 655,
      "training_loss": 7.1481451988220215
    },
    {
      "epoch": 0.035555555555555556,
      "grad_norm": 16.31294822692871,
      "learning_rate": 1e-05,
      "loss": 7.5458,
      "step": 656
    },
    {
      "epoch": 0.035555555555555556,
      "step": 656,
      "training_loss": 7.939426422119141
    },
    {
      "epoch": 0.03560975609756097,
      "step": 657,
      "training_loss": 6.82961893081665
    },
    {
      "epoch": 0.035663956639566397,
      "step": 658,
      "training_loss": 8.04493236541748
    },
    {
      "epoch": 0.035718157181571814,
      "step": 659,
      "training_loss": 6.600935459136963
    },
    {
      "epoch": 0.03577235772357724,
      "grad_norm": 11.468917846679688,
      "learning_rate": 1e-05,
      "loss": 7.3537,
      "step": 660
    },
    {
      "epoch": 0.03577235772357724,
      "step": 660,
      "training_loss": 5.9979729652404785
    },
    {
      "epoch": 0.035826558265582654,
      "step": 661,
      "training_loss": 7.510555267333984
    },
    {
      "epoch": 0.03588075880758808,
      "step": 662,
      "training_loss": 6.637672424316406
    },
    {
      "epoch": 0.035934959349593495,
      "step": 663,
      "training_loss": 6.011204719543457
    },
    {
      "epoch": 0.03598915989159892,
      "grad_norm": 21.313053131103516,
      "learning_rate": 1e-05,
      "loss": 6.5394,
      "step": 664
    },
    {
      "epoch": 0.03598915989159892,
      "step": 664,
      "training_loss": 7.330417156219482
    },
    {
      "epoch": 0.036043360433604336,
      "step": 665,
      "training_loss": 6.95147705078125
    },
    {
      "epoch": 0.03609756097560975,
      "step": 666,
      "training_loss": 8.35532283782959
    },
    {
      "epoch": 0.03615176151761518,
      "step": 667,
      "training_loss": 8.489896774291992
    },
    {
      "epoch": 0.036205962059620594,
      "grad_norm": 14.010684967041016,
      "learning_rate": 1e-05,
      "loss": 7.7818,
      "step": 668
    },
    {
      "epoch": 0.036205962059620594,
      "step": 668,
      "training_loss": 7.910189151763916
    },
    {
      "epoch": 0.03626016260162602,
      "step": 669,
      "training_loss": 7.940420150756836
    },
    {
      "epoch": 0.036314363143631435,
      "step": 670,
      "training_loss": 7.8779449462890625
    },
    {
      "epoch": 0.03636856368563686,
      "step": 671,
      "training_loss": 6.968067646026611
    },
    {
      "epoch": 0.036422764227642276,
      "grad_norm": 16.165576934814453,
      "learning_rate": 1e-05,
      "loss": 7.6742,
      "step": 672
    },
    {
      "epoch": 0.036422764227642276,
      "step": 672,
      "training_loss": 7.153119087219238
    },
    {
      "epoch": 0.03647696476964769,
      "step": 673,
      "training_loss": 8.627752304077148
    },
    {
      "epoch": 0.03653116531165312,
      "step": 674,
      "training_loss": 5.915846347808838
    },
    {
      "epoch": 0.036585365853658534,
      "step": 675,
      "training_loss": 8.214350700378418
    },
    {
      "epoch": 0.03663956639566396,
      "grad_norm": 18.81600570678711,
      "learning_rate": 1e-05,
      "loss": 7.4778,
      "step": 676
    },
    {
      "epoch": 0.03663956639566396,
      "step": 676,
      "training_loss": 8.202044486999512
    },
    {
      "epoch": 0.036693766937669375,
      "step": 677,
      "training_loss": 7.135343074798584
    },
    {
      "epoch": 0.0367479674796748,
      "step": 678,
      "training_loss": 6.442089080810547
    },
    {
      "epoch": 0.036802168021680216,
      "step": 679,
      "training_loss": 5.712493419647217
    },
    {
      "epoch": 0.03685636856368564,
      "grad_norm": 29.244075775146484,
      "learning_rate": 1e-05,
      "loss": 6.873,
      "step": 680
    },
    {
      "epoch": 0.03685636856368564,
      "step": 680,
      "training_loss": 8.153648376464844
    },
    {
      "epoch": 0.03691056910569106,
      "step": 681,
      "training_loss": 8.160945892333984
    },
    {
      "epoch": 0.036964769647696474,
      "step": 682,
      "training_loss": 8.450806617736816
    },
    {
      "epoch": 0.0370189701897019,
      "step": 683,
      "training_loss": 7.582562446594238
    },
    {
      "epoch": 0.037073170731707315,
      "grad_norm": 12.198261260986328,
      "learning_rate": 1e-05,
      "loss": 8.087,
      "step": 684
    },
    {
      "epoch": 0.037073170731707315,
      "step": 684,
      "training_loss": 7.470959663391113
    },
    {
      "epoch": 0.03712737127371274,
      "step": 685,
      "training_loss": 7.416088104248047
    },
    {
      "epoch": 0.037181571815718156,
      "step": 686,
      "training_loss": 9.539102554321289
    },
    {
      "epoch": 0.03723577235772358,
      "step": 687,
      "training_loss": 7.1583147048950195
    },
    {
      "epoch": 0.037289972899729,
      "grad_norm": 25.945592880249023,
      "learning_rate": 1e-05,
      "loss": 7.8961,
      "step": 688
    },
    {
      "epoch": 0.037289972899729,
      "step": 688,
      "training_loss": 8.239164352416992
    },
    {
      "epoch": 0.037344173441734414,
      "step": 689,
      "training_loss": 7.099133491516113
    },
    {
      "epoch": 0.03739837398373984,
      "step": 690,
      "training_loss": 8.468503952026367
    },
    {
      "epoch": 0.037452574525745255,
      "step": 691,
      "training_loss": 7.67402982711792
    },
    {
      "epoch": 0.03750677506775068,
      "grad_norm": 11.716397285461426,
      "learning_rate": 1e-05,
      "loss": 7.8702,
      "step": 692
    },
    {
      "epoch": 0.03750677506775068,
      "step": 692,
      "training_loss": 7.378066062927246
    },
    {
      "epoch": 0.037560975609756096,
      "step": 693,
      "training_loss": 6.541484832763672
    },
    {
      "epoch": 0.03761517615176152,
      "step": 694,
      "training_loss": 7.551252365112305
    },
    {
      "epoch": 0.03766937669376694,
      "step": 695,
      "training_loss": 8.089284896850586
    },
    {
      "epoch": 0.03772357723577236,
      "grad_norm": 12.184758186340332,
      "learning_rate": 1e-05,
      "loss": 7.39,
      "step": 696
    },
    {
      "epoch": 0.03772357723577236,
      "step": 696,
      "training_loss": 8.257721900939941
    },
    {
      "epoch": 0.03777777777777778,
      "step": 697,
      "training_loss": 6.292238712310791
    },
    {
      "epoch": 0.037831978319783195,
      "step": 698,
      "training_loss": 6.569148540496826
    },
    {
      "epoch": 0.03788617886178862,
      "step": 699,
      "training_loss": 7.420546531677246
    },
    {
      "epoch": 0.037940379403794036,
      "grad_norm": 14.983966827392578,
      "learning_rate": 1e-05,
      "loss": 7.1349,
      "step": 700
    },
    {
      "epoch": 0.037940379403794036,
      "step": 700,
      "training_loss": 6.714663982391357
    },
    {
      "epoch": 0.03799457994579946,
      "step": 701,
      "training_loss": 6.584252834320068
    },
    {
      "epoch": 0.03804878048780488,
      "step": 702,
      "training_loss": 6.620905876159668
    },
    {
      "epoch": 0.0381029810298103,
      "step": 703,
      "training_loss": 6.902586460113525
    },
    {
      "epoch": 0.03815718157181572,
      "grad_norm": 17.378864288330078,
      "learning_rate": 1e-05,
      "loss": 6.7056,
      "step": 704
    },
    {
      "epoch": 0.03815718157181572,
      "step": 704,
      "training_loss": 6.1832756996154785
    },
    {
      "epoch": 0.038211382113821135,
      "step": 705,
      "training_loss": 5.44205379486084
    },
    {
      "epoch": 0.03826558265582656,
      "step": 706,
      "training_loss": 8.830633163452148
    },
    {
      "epoch": 0.038319783197831976,
      "step": 707,
      "training_loss": 7.2137064933776855
    },
    {
      "epoch": 0.0383739837398374,
      "grad_norm": 22.71533966064453,
      "learning_rate": 1e-05,
      "loss": 6.9174,
      "step": 708
    },
    {
      "epoch": 0.0383739837398374,
      "step": 708,
      "training_loss": 8.376605033874512
    },
    {
      "epoch": 0.03842818428184282,
      "step": 709,
      "training_loss": 7.329748153686523
    },
    {
      "epoch": 0.03848238482384824,
      "step": 710,
      "training_loss": 7.6830878257751465
    },
    {
      "epoch": 0.03853658536585366,
      "step": 711,
      "training_loss": 7.433039665222168
    },
    {
      "epoch": 0.03859078590785908,
      "grad_norm": 15.80206298828125,
      "learning_rate": 1e-05,
      "loss": 7.7056,
      "step": 712
    },
    {
      "epoch": 0.03859078590785908,
      "step": 712,
      "training_loss": 7.516097545623779
    },
    {
      "epoch": 0.0386449864498645,
      "step": 713,
      "training_loss": 6.4643330574035645
    },
    {
      "epoch": 0.038699186991869916,
      "step": 714,
      "training_loss": 5.9585723876953125
    },
    {
      "epoch": 0.03875338753387534,
      "step": 715,
      "training_loss": 8.432138442993164
    },
    {
      "epoch": 0.03880758807588076,
      "grad_norm": 18.99262237548828,
      "learning_rate": 1e-05,
      "loss": 7.0928,
      "step": 716
    },
    {
      "epoch": 0.03880758807588076,
      "step": 716,
      "training_loss": 8.328255653381348
    },
    {
      "epoch": 0.03886178861788618,
      "step": 717,
      "training_loss": 8.119059562683105
    },
    {
      "epoch": 0.0389159891598916,
      "step": 718,
      "training_loss": 6.933791637420654
    },
    {
      "epoch": 0.03897018970189702,
      "step": 719,
      "training_loss": 7.218845844268799
    },
    {
      "epoch": 0.03902439024390244,
      "grad_norm": 18.767086029052734,
      "learning_rate": 1e-05,
      "loss": 7.65,
      "step": 720
    },
    {
      "epoch": 0.03902439024390244,
      "step": 720,
      "training_loss": 6.973489761352539
    },
    {
      "epoch": 0.039078590785907856,
      "step": 721,
      "training_loss": 7.563920021057129
    },
    {
      "epoch": 0.03913279132791328,
      "step": 722,
      "training_loss": 7.615703582763672
    },
    {
      "epoch": 0.0391869918699187,
      "step": 723,
      "training_loss": 7.2648091316223145
    },
    {
      "epoch": 0.03924119241192412,
      "grad_norm": 12.964652061462402,
      "learning_rate": 1e-05,
      "loss": 7.3545,
      "step": 724
    },
    {
      "epoch": 0.03924119241192412,
      "step": 724,
      "training_loss": 7.467070579528809
    },
    {
      "epoch": 0.03929539295392954,
      "step": 725,
      "training_loss": 8.62618637084961
    },
    {
      "epoch": 0.03934959349593496,
      "step": 726,
      "training_loss": 6.898422718048096
    },
    {
      "epoch": 0.03940379403794038,
      "step": 727,
      "training_loss": 7.6373610496521
    },
    {
      "epoch": 0.0394579945799458,
      "grad_norm": 16.635196685791016,
      "learning_rate": 1e-05,
      "loss": 7.6573,
      "step": 728
    },
    {
      "epoch": 0.0394579945799458,
      "step": 728,
      "training_loss": 7.55942964553833
    },
    {
      "epoch": 0.03951219512195122,
      "step": 729,
      "training_loss": 7.1468329429626465
    },
    {
      "epoch": 0.03956639566395664,
      "step": 730,
      "training_loss": 7.739222049713135
    },
    {
      "epoch": 0.03962059620596206,
      "step": 731,
      "training_loss": 7.401004314422607
    },
    {
      "epoch": 0.03967479674796748,
      "grad_norm": 14.708269119262695,
      "learning_rate": 1e-05,
      "loss": 7.4616,
      "step": 732
    },
    {
      "epoch": 0.03967479674796748,
      "step": 732,
      "training_loss": 7.24678373336792
    },
    {
      "epoch": 0.0397289972899729,
      "step": 733,
      "training_loss": 5.748908519744873
    },
    {
      "epoch": 0.03978319783197832,
      "step": 734,
      "training_loss": 6.049829483032227
    },
    {
      "epoch": 0.03983739837398374,
      "step": 735,
      "training_loss": 6.159605026245117
    },
    {
      "epoch": 0.03989159891598916,
      "grad_norm": 13.45759391784668,
      "learning_rate": 1e-05,
      "loss": 6.3013,
      "step": 736
    },
    {
      "epoch": 0.03989159891598916,
      "step": 736,
      "training_loss": 5.773135662078857
    },
    {
      "epoch": 0.03994579945799458,
      "step": 737,
      "training_loss": 7.824631214141846
    },
    {
      "epoch": 0.04,
      "step": 738,
      "training_loss": 7.847968578338623
    },
    {
      "epoch": 0.04005420054200542,
      "step": 739,
      "training_loss": 7.686949253082275
    },
    {
      "epoch": 0.04010840108401084,
      "grad_norm": 16.186946868896484,
      "learning_rate": 1e-05,
      "loss": 7.2832,
      "step": 740
    },
    {
      "epoch": 0.04010840108401084,
      "step": 740,
      "training_loss": 7.519392490386963
    },
    {
      "epoch": 0.04016260162601626,
      "step": 741,
      "training_loss": 7.683074474334717
    },
    {
      "epoch": 0.04021680216802168,
      "step": 742,
      "training_loss": 7.522810459136963
    },
    {
      "epoch": 0.0402710027100271,
      "step": 743,
      "training_loss": 8.632411003112793
    },
    {
      "epoch": 0.040325203252032524,
      "grad_norm": 15.69361686706543,
      "learning_rate": 1e-05,
      "loss": 7.8394,
      "step": 744
    },
    {
      "epoch": 0.040325203252032524,
      "step": 744,
      "training_loss": 7.715400695800781
    },
    {
      "epoch": 0.04037940379403794,
      "step": 745,
      "training_loss": 7.8688578605651855
    },
    {
      "epoch": 0.04043360433604336,
      "step": 746,
      "training_loss": 7.59503173828125
    },
    {
      "epoch": 0.04048780487804878,
      "step": 747,
      "training_loss": 8.285539627075195
    },
    {
      "epoch": 0.0405420054200542,
      "grad_norm": 11.197763442993164,
      "learning_rate": 1e-05,
      "loss": 7.8662,
      "step": 748
    },
    {
      "epoch": 0.0405420054200542,
      "step": 748,
      "training_loss": 7.2277512550354
    },
    {
      "epoch": 0.04059620596205962,
      "step": 749,
      "training_loss": 8.54175090789795
    },
    {
      "epoch": 0.04065040650406504,
      "step": 750,
      "training_loss": 6.748481750488281
    },
    {
      "epoch": 0.040704607046070464,
      "step": 751,
      "training_loss": 7.085055351257324
    },
    {
      "epoch": 0.04075880758807588,
      "grad_norm": 64.28804016113281,
      "learning_rate": 1e-05,
      "loss": 7.4008,
      "step": 752
    },
    {
      "epoch": 0.04075880758807588,
      "step": 752,
      "training_loss": 7.578892230987549
    },
    {
      "epoch": 0.0408130081300813,
      "step": 753,
      "training_loss": 6.7549824714660645
    },
    {
      "epoch": 0.04086720867208672,
      "step": 754,
      "training_loss": 7.358738422393799
    },
    {
      "epoch": 0.04092140921409214,
      "step": 755,
      "training_loss": 8.599678993225098
    },
    {
      "epoch": 0.04097560975609756,
      "grad_norm": 29.437355041503906,
      "learning_rate": 1e-05,
      "loss": 7.5731,
      "step": 756
    },
    {
      "epoch": 0.04097560975609756,
      "step": 756,
      "training_loss": 6.446199893951416
    },
    {
      "epoch": 0.04102981029810298,
      "step": 757,
      "training_loss": 7.97297477722168
    },
    {
      "epoch": 0.041084010840108404,
      "step": 758,
      "training_loss": 8.171884536743164
    },
    {
      "epoch": 0.04113821138211382,
      "step": 759,
      "training_loss": 7.620614051818848
    },
    {
      "epoch": 0.041192411924119245,
      "grad_norm": 15.04110050201416,
      "learning_rate": 1e-05,
      "loss": 7.5529,
      "step": 760
    },
    {
      "epoch": 0.041192411924119245,
      "step": 760,
      "training_loss": 7.398055076599121
    },
    {
      "epoch": 0.04124661246612466,
      "step": 761,
      "training_loss": 7.762007236480713
    },
    {
      "epoch": 0.04130081300813008,
      "step": 762,
      "training_loss": 7.657063007354736
    },
    {
      "epoch": 0.0413550135501355,
      "step": 763,
      "training_loss": 6.44507360458374
    },
    {
      "epoch": 0.04140921409214092,
      "grad_norm": 19.448123931884766,
      "learning_rate": 1e-05,
      "loss": 7.3155,
      "step": 764
    },
    {
      "epoch": 0.04140921409214092,
      "step": 764,
      "training_loss": 8.893157005310059
    },
    {
      "epoch": 0.041463414634146344,
      "step": 765,
      "training_loss": 8.004855155944824
    },
    {
      "epoch": 0.04151761517615176,
      "step": 766,
      "training_loss": 8.95726490020752
    },
    {
      "epoch": 0.041571815718157185,
      "step": 767,
      "training_loss": 7.944192409515381
    },
    {
      "epoch": 0.0416260162601626,
      "grad_norm": 12.517049789428711,
      "learning_rate": 1e-05,
      "loss": 8.4499,
      "step": 768
    },
    {
      "epoch": 0.0416260162601626,
      "step": 768,
      "training_loss": 7.222275257110596
    },
    {
      "epoch": 0.04168021680216802,
      "step": 769,
      "training_loss": 6.584643363952637
    },
    {
      "epoch": 0.04173441734417344,
      "step": 770,
      "training_loss": 7.481154918670654
    },
    {
      "epoch": 0.04178861788617886,
      "step": 771,
      "training_loss": 7.401394367218018
    },
    {
      "epoch": 0.041842818428184284,
      "grad_norm": 19.670612335205078,
      "learning_rate": 1e-05,
      "loss": 7.1724,
      "step": 772
    },
    {
      "epoch": 0.041842818428184284,
      "step": 772,
      "training_loss": 6.846065521240234
    },
    {
      "epoch": 0.0418970189701897,
      "step": 773,
      "training_loss": 7.272346019744873
    },
    {
      "epoch": 0.041951219512195125,
      "step": 774,
      "training_loss": 7.539729595184326
    },
    {
      "epoch": 0.04200542005420054,
      "step": 775,
      "training_loss": 7.643473148345947
    },
    {
      "epoch": 0.042059620596205965,
      "grad_norm": 15.767237663269043,
      "learning_rate": 1e-05,
      "loss": 7.3254,
      "step": 776
    },
    {
      "epoch": 0.042059620596205965,
      "step": 776,
      "training_loss": 7.768119812011719
    },
    {
      "epoch": 0.04211382113821138,
      "step": 777,
      "training_loss": 8.141231536865234
    },
    {
      "epoch": 0.0421680216802168,
      "step": 778,
      "training_loss": 7.020600318908691
    },
    {
      "epoch": 0.042222222222222223,
      "step": 779,
      "training_loss": 7.396282196044922
    },
    {
      "epoch": 0.04227642276422764,
      "grad_norm": 13.881558418273926,
      "learning_rate": 1e-05,
      "loss": 7.5816,
      "step": 780
    },
    {
      "epoch": 0.04227642276422764,
      "step": 780,
      "training_loss": 7.871460437774658
    },
    {
      "epoch": 0.042330623306233064,
      "step": 781,
      "training_loss": 7.26619291305542
    },
    {
      "epoch": 0.04238482384823848,
      "step": 782,
      "training_loss": 7.747412204742432
    },
    {
      "epoch": 0.042439024390243905,
      "step": 783,
      "training_loss": 9.669589042663574
    },
    {
      "epoch": 0.04249322493224932,
      "grad_norm": 50.625267028808594,
      "learning_rate": 1e-05,
      "loss": 8.1387,
      "step": 784
    },
    {
      "epoch": 0.04249322493224932,
      "step": 784,
      "training_loss": 7.909923553466797
    },
    {
      "epoch": 0.04254742547425474,
      "step": 785,
      "training_loss": 7.110124588012695
    },
    {
      "epoch": 0.04260162601626016,
      "step": 786,
      "training_loss": 8.00576400756836
    },
    {
      "epoch": 0.04265582655826558,
      "step": 787,
      "training_loss": 7.681179523468018
    },
    {
      "epoch": 0.042710027100271004,
      "grad_norm": 11.550066947937012,
      "learning_rate": 1e-05,
      "loss": 7.6767,
      "step": 788
    },
    {
      "epoch": 0.042710027100271004,
      "step": 788,
      "training_loss": 7.251941680908203
    },
    {
      "epoch": 0.04276422764227642,
      "step": 789,
      "training_loss": 6.5937700271606445
    },
    {
      "epoch": 0.042818428184281845,
      "step": 790,
      "training_loss": 6.456654071807861
    },
    {
      "epoch": 0.04287262872628726,
      "step": 791,
      "training_loss": 7.992836952209473
    },
    {
      "epoch": 0.042926829268292686,
      "grad_norm": 25.286224365234375,
      "learning_rate": 1e-05,
      "loss": 7.0738,
      "step": 792
    },
    {
      "epoch": 0.042926829268292686,
      "step": 792,
      "training_loss": 7.279108047485352
    },
    {
      "epoch": 0.0429810298102981,
      "step": 793,
      "training_loss": 7.841796398162842
    },
    {
      "epoch": 0.04303523035230352,
      "step": 794,
      "training_loss": 7.987884521484375
    },
    {
      "epoch": 0.043089430894308944,
      "step": 795,
      "training_loss": 7.5241570472717285
    },
    {
      "epoch": 0.04314363143631436,
      "grad_norm": 20.65587615966797,
      "learning_rate": 1e-05,
      "loss": 7.6582,
      "step": 796
    },
    {
      "epoch": 0.04314363143631436,
      "step": 796,
      "training_loss": 6.852330684661865
    },
    {
      "epoch": 0.043197831978319785,
      "step": 797,
      "training_loss": 6.557071208953857
    },
    {
      "epoch": 0.0432520325203252,
      "step": 798,
      "training_loss": 5.717689514160156
    },
    {
      "epoch": 0.043306233062330626,
      "step": 799,
      "training_loss": 6.757059574127197
    },
    {
      "epoch": 0.04336043360433604,
      "grad_norm": 24.131460189819336,
      "learning_rate": 1e-05,
      "loss": 6.471,
      "step": 800
    },
    {
      "epoch": 0.04336043360433604,
      "step": 800,
      "training_loss": 7.316311836242676
    },
    {
      "epoch": 0.04341463414634146,
      "step": 801,
      "training_loss": 8.068388938903809
    },
    {
      "epoch": 0.043468834688346884,
      "step": 802,
      "training_loss": 6.338033199310303
    },
    {
      "epoch": 0.0435230352303523,
      "step": 803,
      "training_loss": 7.67354154586792
    },
    {
      "epoch": 0.043577235772357725,
      "grad_norm": 30.85428237915039,
      "learning_rate": 1e-05,
      "loss": 7.3491,
      "step": 804
    },
    {
      "epoch": 0.043577235772357725,
      "step": 804,
      "training_loss": 6.531081199645996
    },
    {
      "epoch": 0.04363143631436314,
      "step": 805,
      "training_loss": 7.523169040679932
    },
    {
      "epoch": 0.043685636856368566,
      "step": 806,
      "training_loss": 7.332772254943848
    },
    {
      "epoch": 0.04373983739837398,
      "step": 807,
      "training_loss": 7.115194320678711
    },
    {
      "epoch": 0.04379403794037941,
      "grad_norm": 19.423254013061523,
      "learning_rate": 1e-05,
      "loss": 7.1256,
      "step": 808
    },
    {
      "epoch": 0.04379403794037941,
      "step": 808,
      "training_loss": 7.355279922485352
    },
    {
      "epoch": 0.043848238482384824,
      "step": 809,
      "training_loss": 7.15977144241333
    },
    {
      "epoch": 0.04390243902439024,
      "step": 810,
      "training_loss": 7.304504871368408
    },
    {
      "epoch": 0.043956639566395665,
      "step": 811,
      "training_loss": 8.165520668029785
    },
    {
      "epoch": 0.04401084010840108,
      "grad_norm": 15.658426284790039,
      "learning_rate": 1e-05,
      "loss": 7.4963,
      "step": 812
    },
    {
      "epoch": 0.04401084010840108,
      "step": 812,
      "training_loss": 7.243791103363037
    },
    {
      "epoch": 0.044065040650406506,
      "step": 813,
      "training_loss": 7.818070411682129
    },
    {
      "epoch": 0.04411924119241192,
      "step": 814,
      "training_loss": 7.665558338165283
    },
    {
      "epoch": 0.04417344173441735,
      "step": 815,
      "training_loss": 8.180334091186523
    },
    {
      "epoch": 0.044227642276422764,
      "grad_norm": 27.220943450927734,
      "learning_rate": 1e-05,
      "loss": 7.7269,
      "step": 816
    },
    {
      "epoch": 0.044227642276422764,
      "step": 816,
      "training_loss": 6.488495349884033
    },
    {
      "epoch": 0.04428184281842818,
      "step": 817,
      "training_loss": 5.744063854217529
    },
    {
      "epoch": 0.044336043360433605,
      "step": 818,
      "training_loss": 7.816483497619629
    },
    {
      "epoch": 0.04439024390243902,
      "step": 819,
      "training_loss": 6.1043829917907715
    },
    {
      "epoch": 0.044444444444444446,
      "grad_norm": 20.663820266723633,
      "learning_rate": 1e-05,
      "loss": 6.5384,
      "step": 820
    },
    {
      "epoch": 0.044444444444444446,
      "step": 820,
      "training_loss": 7.831073760986328
    },
    {
      "epoch": 0.04449864498644986,
      "step": 821,
      "training_loss": 6.76140022277832
    },
    {
      "epoch": 0.04455284552845529,
      "step": 822,
      "training_loss": 6.868954658508301
    },
    {
      "epoch": 0.044607046070460704,
      "step": 823,
      "training_loss": 9.232460975646973
    },
    {
      "epoch": 0.04466124661246613,
      "grad_norm": 26.163970947265625,
      "learning_rate": 1e-05,
      "loss": 7.6735,
      "step": 824
    },
    {
      "epoch": 0.04466124661246613,
      "step": 824,
      "training_loss": 8.625916481018066
    },
    {
      "epoch": 0.044715447154471545,
      "step": 825,
      "training_loss": 7.054008483886719
    },
    {
      "epoch": 0.04476964769647696,
      "step": 826,
      "training_loss": 7.3619561195373535
    },
    {
      "epoch": 0.044823848238482386,
      "step": 827,
      "training_loss": 7.113012790679932
    },
    {
      "epoch": 0.0448780487804878,
      "grad_norm": 14.996211051940918,
      "learning_rate": 1e-05,
      "loss": 7.5387,
      "step": 828
    },
    {
      "epoch": 0.0448780487804878,
      "step": 828,
      "training_loss": 7.43806266784668
    },
    {
      "epoch": 0.04493224932249323,
      "step": 829,
      "training_loss": 6.783214569091797
    },
    {
      "epoch": 0.044986449864498644,
      "step": 830,
      "training_loss": 7.515467643737793
    },
    {
      "epoch": 0.04504065040650407,
      "step": 831,
      "training_loss": 7.85584831237793
    },
    {
      "epoch": 0.045094850948509485,
      "grad_norm": 30.54755973815918,
      "learning_rate": 1e-05,
      "loss": 7.3981,
      "step": 832
    },
    {
      "epoch": 0.045094850948509485,
      "step": 832,
      "training_loss": 7.160261631011963
    },
    {
      "epoch": 0.0451490514905149,
      "step": 833,
      "training_loss": 7.93576717376709
    },
    {
      "epoch": 0.045203252032520326,
      "step": 834,
      "training_loss": 7.352416515350342
    },
    {
      "epoch": 0.04525745257452574,
      "step": 835,
      "training_loss": 6.516341209411621
    },
    {
      "epoch": 0.04531165311653117,
      "grad_norm": 17.588171005249023,
      "learning_rate": 1e-05,
      "loss": 7.2412,
      "step": 836
    },
    {
      "epoch": 0.04531165311653117,
      "step": 836,
      "training_loss": 6.1782073974609375
    },
    {
      "epoch": 0.045365853658536584,
      "step": 837,
      "training_loss": 7.5690598487854
    },
    {
      "epoch": 0.04542005420054201,
      "step": 838,
      "training_loss": 5.679376125335693
    },
    {
      "epoch": 0.045474254742547425,
      "step": 839,
      "training_loss": 8.154483795166016
    },
    {
      "epoch": 0.04552845528455285,
      "grad_norm": 19.348909378051758,
      "learning_rate": 1e-05,
      "loss": 6.8953,
      "step": 840
    },
    {
      "epoch": 0.04552845528455285,
      "step": 840,
      "training_loss": 6.571814060211182
    },
    {
      "epoch": 0.045582655826558266,
      "step": 841,
      "training_loss": 7.2438530921936035
    },
    {
      "epoch": 0.04563685636856368,
      "step": 842,
      "training_loss": 8.407193183898926
    },
    {
      "epoch": 0.04569105691056911,
      "step": 843,
      "training_loss": 7.19254207611084
    },
    {
      "epoch": 0.045745257452574524,
      "grad_norm": 13.141278266906738,
      "learning_rate": 1e-05,
      "loss": 7.3539,
      "step": 844
    },
    {
      "epoch": 0.045745257452574524,
      "step": 844,
      "training_loss": 7.549839019775391
    },
    {
      "epoch": 0.04579945799457995,
      "step": 845,
      "training_loss": 7.3195881843566895
    },
    {
      "epoch": 0.045853658536585365,
      "step": 846,
      "training_loss": 6.036137104034424
    },
    {
      "epoch": 0.04590785907859079,
      "step": 847,
      "training_loss": 7.111788272857666
    },
    {
      "epoch": 0.045962059620596206,
      "grad_norm": 16.057788848876953,
      "learning_rate": 1e-05,
      "loss": 7.0043,
      "step": 848
    },
    {
      "epoch": 0.045962059620596206,
      "step": 848,
      "training_loss": 6.979340553283691
    },
    {
      "epoch": 0.04601626016260162,
      "step": 849,
      "training_loss": 7.636937618255615
    },
    {
      "epoch": 0.04607046070460705,
      "step": 850,
      "training_loss": 7.127559185028076
    },
    {
      "epoch": 0.046124661246612464,
      "step": 851,
      "training_loss": 6.829360008239746
    },
    {
      "epoch": 0.04617886178861789,
      "grad_norm": 20.51664924621582,
      "learning_rate": 1e-05,
      "loss": 7.1433,
      "step": 852
    },
    {
      "epoch": 0.04617886178861789,
      "step": 852,
      "training_loss": 6.6409101486206055
    },
    {
      "epoch": 0.046233062330623305,
      "step": 853,
      "training_loss": 6.708644866943359
    },
    {
      "epoch": 0.04628726287262873,
      "step": 854,
      "training_loss": 7.283260822296143
    },
    {
      "epoch": 0.046341463414634146,
      "step": 855,
      "training_loss": 7.675302982330322
    },
    {
      "epoch": 0.04639566395663957,
      "grad_norm": 25.710466384887695,
      "learning_rate": 1e-05,
      "loss": 7.077,
      "step": 856
    },
    {
      "epoch": 0.04639566395663957,
      "step": 856,
      "training_loss": 6.524420738220215
    },
    {
      "epoch": 0.04644986449864499,
      "step": 857,
      "training_loss": 6.3211822509765625
    },
    {
      "epoch": 0.046504065040650404,
      "step": 858,
      "training_loss": 8.711405754089355
    },
    {
      "epoch": 0.04655826558265583,
      "step": 859,
      "training_loss": 7.500962257385254
    },
    {
      "epoch": 0.046612466124661245,
      "grad_norm": 16.647005081176758,
      "learning_rate": 1e-05,
      "loss": 7.2645,
      "step": 860
    },
    {
      "epoch": 0.046612466124661245,
      "step": 860,
      "training_loss": 7.2642974853515625
    },
    {
      "epoch": 0.04666666666666667,
      "step": 861,
      "training_loss": 8.148364067077637
    },
    {
      "epoch": 0.046720867208672086,
      "step": 862,
      "training_loss": 7.357884407043457
    },
    {
      "epoch": 0.04677506775067751,
      "step": 863,
      "training_loss": 6.77963924407959
    },
    {
      "epoch": 0.04682926829268293,
      "grad_norm": 18.547786712646484,
      "learning_rate": 1e-05,
      "loss": 7.3875,
      "step": 864
    },
    {
      "epoch": 0.04682926829268293,
      "step": 864,
      "training_loss": 7.273497104644775
    },
    {
      "epoch": 0.046883468834688344,
      "step": 865,
      "training_loss": 7.8357439041137695
    },
    {
      "epoch": 0.04693766937669377,
      "step": 866,
      "training_loss": 7.008145809173584
    },
    {
      "epoch": 0.046991869918699185,
      "step": 867,
      "training_loss": 7.876757621765137
    },
    {
      "epoch": 0.04704607046070461,
      "grad_norm": 29.133487701416016,
      "learning_rate": 1e-05,
      "loss": 7.4985,
      "step": 868
    },
    {
      "epoch": 0.04704607046070461,
      "step": 868,
      "training_loss": 7.528341770172119
    },
    {
      "epoch": 0.047100271002710026,
      "step": 869,
      "training_loss": 8.349176406860352
    },
    {
      "epoch": 0.04715447154471545,
      "step": 870,
      "training_loss": 8.014127731323242
    },
    {
      "epoch": 0.04720867208672087,
      "step": 871,
      "training_loss": 6.581873416900635
    },
    {
      "epoch": 0.04726287262872629,
      "grad_norm": 13.748699188232422,
      "learning_rate": 1e-05,
      "loss": 7.6184,
      "step": 872
    },
    {
      "epoch": 0.04726287262872629,
      "step": 872,
      "training_loss": 6.829333305358887
    },
    {
      "epoch": 0.04731707317073171,
      "step": 873,
      "training_loss": 7.062126159667969
    },
    {
      "epoch": 0.047371273712737125,
      "step": 874,
      "training_loss": 7.849270820617676
    },
    {
      "epoch": 0.04742547425474255,
      "step": 875,
      "training_loss": 7.982462406158447
    },
    {
      "epoch": 0.047479674796747966,
      "grad_norm": 13.907047271728516,
      "learning_rate": 1e-05,
      "loss": 7.4308,
      "step": 876
    },
    {
      "epoch": 0.047479674796747966,
      "step": 876,
      "training_loss": 9.103348731994629
    },
    {
      "epoch": 0.04753387533875339,
      "step": 877,
      "training_loss": 7.604116916656494
    },
    {
      "epoch": 0.04758807588075881,
      "step": 878,
      "training_loss": 7.614312171936035
    },
    {
      "epoch": 0.04764227642276423,
      "step": 879,
      "training_loss": 5.669027328491211
    },
    {
      "epoch": 0.04769647696476965,
      "grad_norm": 14.9942626953125,
      "learning_rate": 1e-05,
      "loss": 7.4977,
      "step": 880
    },
    {
      "epoch": 0.04769647696476965,
      "step": 880,
      "training_loss": 6.862196922302246
    },
    {
      "epoch": 0.047750677506775065,
      "step": 881,
      "training_loss": 6.237283706665039
    },
    {
      "epoch": 0.04780487804878049,
      "step": 882,
      "training_loss": 8.14236831665039
    },
    {
      "epoch": 0.047859078590785906,
      "step": 883,
      "training_loss": 7.211472511291504
    },
    {
      "epoch": 0.04791327913279133,
      "grad_norm": 22.17084503173828,
      "learning_rate": 1e-05,
      "loss": 7.1133,
      "step": 884
    },
    {
      "epoch": 0.04791327913279133,
      "step": 884,
      "training_loss": 7.483644008636475
    },
    {
      "epoch": 0.04796747967479675,
      "step": 885,
      "training_loss": 9.229742050170898
    },
    {
      "epoch": 0.04802168021680217,
      "step": 886,
      "training_loss": 6.443379878997803
    },
    {
      "epoch": 0.04807588075880759,
      "step": 887,
      "training_loss": 5.85428524017334
    },
    {
      "epoch": 0.04813008130081301,
      "grad_norm": 18.950946807861328,
      "learning_rate": 1e-05,
      "loss": 7.2528,
      "step": 888
    },
    {
      "epoch": 0.04813008130081301,
      "step": 888,
      "training_loss": 6.936169147491455
    },
    {
      "epoch": 0.04818428184281843,
      "step": 889,
      "training_loss": 6.417416095733643
    },
    {
      "epoch": 0.048238482384823846,
      "step": 890,
      "training_loss": 8.328697204589844
    },
    {
      "epoch": 0.04829268292682927,
      "step": 891,
      "training_loss": 6.93245792388916
    },
    {
      "epoch": 0.04834688346883469,
      "grad_norm": 15.778785705566406,
      "learning_rate": 1e-05,
      "loss": 7.1537,
      "step": 892
    },
    {
      "epoch": 0.04834688346883469,
      "step": 892,
      "training_loss": 7.177065372467041
    },
    {
      "epoch": 0.04840108401084011,
      "step": 893,
      "training_loss": 6.413909435272217
    },
    {
      "epoch": 0.04845528455284553,
      "step": 894,
      "training_loss": 7.5944600105285645
    },
    {
      "epoch": 0.04850948509485095,
      "step": 895,
      "training_loss": 7.347728252410889
    },
    {
      "epoch": 0.04856368563685637,
      "grad_norm": 17.56531524658203,
      "learning_rate": 1e-05,
      "loss": 7.1333,
      "step": 896
    },
    {
      "epoch": 0.04856368563685637,
      "step": 896,
      "training_loss": 7.512213706970215
    },
    {
      "epoch": 0.048617886178861786,
      "step": 897,
      "training_loss": 6.147216796875
    },
    {
      "epoch": 0.04867208672086721,
      "step": 898,
      "training_loss": 7.0170135498046875
    },
    {
      "epoch": 0.048726287262872627,
      "step": 899,
      "training_loss": 7.627912521362305
    },
    {
      "epoch": 0.04878048780487805,
      "grad_norm": 15.260225296020508,
      "learning_rate": 1e-05,
      "loss": 7.0761,
      "step": 900
    },
    {
      "epoch": 0.04878048780487805,
      "step": 900,
      "training_loss": 7.364727973937988
    },
    {
      "epoch": 0.04883468834688347,
      "step": 901,
      "training_loss": 7.467303276062012
    },
    {
      "epoch": 0.04888888888888889,
      "step": 902,
      "training_loss": 5.502925395965576
    },
    {
      "epoch": 0.04894308943089431,
      "step": 903,
      "training_loss": 6.91231107711792
    },
    {
      "epoch": 0.04899728997289973,
      "grad_norm": 19.97547721862793,
      "learning_rate": 1e-05,
      "loss": 6.8118,
      "step": 904
    },
    {
      "epoch": 0.04899728997289973,
      "step": 904,
      "training_loss": 6.865436553955078
    },
    {
      "epoch": 0.04905149051490515,
      "step": 905,
      "training_loss": 7.5608296394348145
    },
    {
      "epoch": 0.049105691056910566,
      "step": 906,
      "training_loss": 6.88442325592041
    },
    {
      "epoch": 0.04915989159891599,
      "step": 907,
      "training_loss": 7.847963333129883
    },
    {
      "epoch": 0.04921409214092141,
      "grad_norm": 18.614635467529297,
      "learning_rate": 1e-05,
      "loss": 7.2897,
      "step": 908
    },
    {
      "epoch": 0.04921409214092141,
      "step": 908,
      "training_loss": 7.936404705047607
    },
    {
      "epoch": 0.04926829268292683,
      "step": 909,
      "training_loss": 5.243791580200195
    },
    {
      "epoch": 0.04932249322493225,
      "step": 910,
      "training_loss": 5.497528076171875
    },
    {
      "epoch": 0.04937669376693767,
      "step": 911,
      "training_loss": 5.459578990936279
    },
    {
      "epoch": 0.04943089430894309,
      "grad_norm": 12.529241561889648,
      "learning_rate": 1e-05,
      "loss": 6.0343,
      "step": 912
    },
    {
      "epoch": 0.04943089430894309,
      "step": 912,
      "training_loss": 6.455092430114746
    },
    {
      "epoch": 0.049485094850948506,
      "step": 913,
      "training_loss": 7.590631008148193
    },
    {
      "epoch": 0.04953929539295393,
      "step": 914,
      "training_loss": 6.278628826141357
    },
    {
      "epoch": 0.04959349593495935,
      "step": 915,
      "training_loss": 7.92773962020874
    },
    {
      "epoch": 0.04964769647696477,
      "grad_norm": 17.33404541015625,
      "learning_rate": 1e-05,
      "loss": 7.063,
      "step": 916
    },
    {
      "epoch": 0.04964769647696477,
      "step": 916,
      "training_loss": 7.086029529571533
    },
    {
      "epoch": 0.04970189701897019,
      "step": 917,
      "training_loss": 7.635897636413574
    },
    {
      "epoch": 0.04975609756097561,
      "step": 918,
      "training_loss": 7.731043815612793
    },
    {
      "epoch": 0.04981029810298103,
      "step": 919,
      "training_loss": 7.639596939086914
    },
    {
      "epoch": 0.04986449864498645,
      "grad_norm": 18.917165756225586,
      "learning_rate": 1e-05,
      "loss": 7.5231,
      "step": 920
    },
    {
      "epoch": 0.04986449864498645,
      "step": 920,
      "training_loss": 6.5616278648376465
    },
    {
      "epoch": 0.04991869918699187,
      "step": 921,
      "training_loss": 6.927579402923584
    },
    {
      "epoch": 0.04997289972899729,
      "step": 922,
      "training_loss": 9.750565528869629
    },
    {
      "epoch": 0.05002710027100271,
      "step": 923,
      "training_loss": 8.237527847290039
    },
    {
      "epoch": 0.05008130081300813,
      "grad_norm": 25.651338577270508,
      "learning_rate": 1e-05,
      "loss": 7.8693,
      "step": 924
    },
    {
      "epoch": 0.05008130081300813,
      "step": 924,
      "training_loss": 8.394700050354004
    },
    {
      "epoch": 0.05013550135501355,
      "step": 925,
      "training_loss": 6.714446544647217
    },
    {
      "epoch": 0.05018970189701897,
      "step": 926,
      "training_loss": 7.629763126373291
    },
    {
      "epoch": 0.05024390243902439,
      "step": 927,
      "training_loss": 7.59296178817749
    },
    {
      "epoch": 0.05029810298102981,
      "grad_norm": 25.557334899902344,
      "learning_rate": 1e-05,
      "loss": 7.583,
      "step": 928
    },
    {
      "epoch": 0.05029810298102981,
      "step": 928,
      "training_loss": 6.919543266296387
    },
    {
      "epoch": 0.05035230352303523,
      "step": 929,
      "training_loss": 6.082066535949707
    },
    {
      "epoch": 0.05040650406504065,
      "step": 930,
      "training_loss": 5.740822792053223
    },
    {
      "epoch": 0.05046070460704607,
      "step": 931,
      "training_loss": 5.476573944091797
    },
    {
      "epoch": 0.05051490514905149,
      "grad_norm": 22.110902786254883,
      "learning_rate": 1e-05,
      "loss": 6.0548,
      "step": 932
    },
    {
      "epoch": 0.05051490514905149,
      "step": 932,
      "training_loss": 7.100552082061768
    },
    {
      "epoch": 0.05056910569105691,
      "step": 933,
      "training_loss": 7.1756815910339355
    },
    {
      "epoch": 0.05062330623306233,
      "step": 934,
      "training_loss": 6.261508941650391
    },
    {
      "epoch": 0.05067750677506775,
      "step": 935,
      "training_loss": 7.468195915222168
    },
    {
      "epoch": 0.050731707317073174,
      "grad_norm": 24.568561553955078,
      "learning_rate": 1e-05,
      "loss": 7.0015,
      "step": 936
    },
    {
      "epoch": 0.050731707317073174,
      "step": 936,
      "training_loss": 7.972352504730225
    },
    {
      "epoch": 0.05078590785907859,
      "step": 937,
      "training_loss": 5.584224700927734
    },
    {
      "epoch": 0.05084010840108401,
      "step": 938,
      "training_loss": 7.157180309295654
    },
    {
      "epoch": 0.05089430894308943,
      "step": 939,
      "training_loss": 7.206649303436279
    },
    {
      "epoch": 0.05094850948509485,
      "grad_norm": 16.646568298339844,
      "learning_rate": 1e-05,
      "loss": 6.9801,
      "step": 940
    },
    {
      "epoch": 0.05094850948509485,
      "step": 940,
      "training_loss": 7.693033695220947
    },
    {
      "epoch": 0.05100271002710027,
      "step": 941,
      "training_loss": 6.436394691467285
    },
    {
      "epoch": 0.05105691056910569,
      "step": 942,
      "training_loss": 7.506513595581055
    },
    {
      "epoch": 0.051111111111111114,
      "step": 943,
      "training_loss": 7.7202067375183105
    },
    {
      "epoch": 0.05116531165311653,
      "grad_norm": 24.43022346496582,
      "learning_rate": 1e-05,
      "loss": 7.339,
      "step": 944
    },
    {
      "epoch": 0.05116531165311653,
      "step": 944,
      "training_loss": 6.399364948272705
    },
    {
      "epoch": 0.05121951219512195,
      "step": 945,
      "training_loss": 7.6773271560668945
    },
    {
      "epoch": 0.05127371273712737,
      "step": 946,
      "training_loss": 8.39827823638916
    },
    {
      "epoch": 0.05132791327913279,
      "step": 947,
      "training_loss": 7.55624532699585
    },
    {
      "epoch": 0.05138211382113821,
      "grad_norm": 15.871807098388672,
      "learning_rate": 1e-05,
      "loss": 7.5078,
      "step": 948
    },
    {
      "epoch": 0.05138211382113821,
      "step": 948,
      "training_loss": 6.345094203948975
    },
    {
      "epoch": 0.05143631436314363,
      "step": 949,
      "training_loss": 7.883281707763672
    },
    {
      "epoch": 0.051490514905149054,
      "step": 950,
      "training_loss": 7.269168376922607
    },
    {
      "epoch": 0.05154471544715447,
      "step": 951,
      "training_loss": 6.733432292938232
    },
    {
      "epoch": 0.051598915989159895,
      "grad_norm": 22.68878746032715,
      "learning_rate": 1e-05,
      "loss": 7.0577,
      "step": 952
    },
    {
      "epoch": 0.051598915989159895,
      "step": 952,
      "training_loss": 5.5579047203063965
    },
    {
      "epoch": 0.05165311653116531,
      "step": 953,
      "training_loss": 7.477138042449951
    },
    {
      "epoch": 0.05170731707317073,
      "step": 954,
      "training_loss": 7.50747013092041
    },
    {
      "epoch": 0.05176151761517615,
      "step": 955,
      "training_loss": 9.118658065795898
    },
    {
      "epoch": 0.05181571815718157,
      "grad_norm": 31.263294219970703,
      "learning_rate": 1e-05,
      "loss": 7.4153,
      "step": 956
    },
    {
      "epoch": 0.05181571815718157,
      "step": 956,
      "training_loss": 6.553242206573486
    },
    {
      "epoch": 0.051869918699186994,
      "step": 957,
      "training_loss": 7.7325310707092285
    },
    {
      "epoch": 0.05192411924119241,
      "step": 958,
      "training_loss": 7.319624423980713
    },
    {
      "epoch": 0.051978319783197835,
      "step": 959,
      "training_loss": 6.589485168457031
    },
    {
      "epoch": 0.05203252032520325,
      "grad_norm": 16.19386863708496,
      "learning_rate": 1e-05,
      "loss": 7.0487,
      "step": 960
    },
    {
      "epoch": 0.05203252032520325,
      "step": 960,
      "training_loss": 5.326778411865234
    },
    {
      "epoch": 0.05208672086720867,
      "step": 961,
      "training_loss": 7.784420490264893
    },
    {
      "epoch": 0.05214092140921409,
      "step": 962,
      "training_loss": 6.473048686981201
    },
    {
      "epoch": 0.05219512195121951,
      "step": 963,
      "training_loss": 7.905091762542725
    },
    {
      "epoch": 0.052249322493224934,
      "grad_norm": 16.46824073791504,
      "learning_rate": 1e-05,
      "loss": 6.8723,
      "step": 964
    },
    {
      "epoch": 0.052249322493224934,
      "step": 964,
      "training_loss": 6.160741806030273
    },
    {
      "epoch": 0.05230352303523035,
      "step": 965,
      "training_loss": 6.883854389190674
    },
    {
      "epoch": 0.052357723577235775,
      "step": 966,
      "training_loss": 6.336925983428955
    },
    {
      "epoch": 0.05241192411924119,
      "step": 967,
      "training_loss": 5.87310791015625
    },
    {
      "epoch": 0.052466124661246616,
      "grad_norm": 29.191425323486328,
      "learning_rate": 1e-05,
      "loss": 6.3137,
      "step": 968
    },
    {
      "epoch": 0.052466124661246616,
      "step": 968,
      "training_loss": 7.205433368682861
    },
    {
      "epoch": 0.05252032520325203,
      "step": 969,
      "training_loss": 7.132815837860107
    },
    {
      "epoch": 0.05257452574525745,
      "step": 970,
      "training_loss": 7.370950222015381
    },
    {
      "epoch": 0.052628726287262874,
      "step": 971,
      "training_loss": 7.025955677032471
    },
    {
      "epoch": 0.05268292682926829,
      "grad_norm": 12.859195709228516,
      "learning_rate": 1e-05,
      "loss": 7.1838,
      "step": 972
    },
    {
      "epoch": 0.05268292682926829,
      "step": 972,
      "training_loss": 7.627626895904541
    },
    {
      "epoch": 0.052737127371273715,
      "step": 973,
      "training_loss": 7.323227882385254
    },
    {
      "epoch": 0.05279132791327913,
      "step": 974,
      "training_loss": 7.576320171356201
    },
    {
      "epoch": 0.052845528455284556,
      "step": 975,
      "training_loss": 5.841156482696533
    },
    {
      "epoch": 0.05289972899728997,
      "grad_norm": 22.999853134155273,
      "learning_rate": 1e-05,
      "loss": 7.0921,
      "step": 976
    },
    {
      "epoch": 0.05289972899728997,
      "step": 976,
      "training_loss": 7.416243076324463
    },
    {
      "epoch": 0.05295392953929539,
      "step": 977,
      "training_loss": 7.807962417602539
    },
    {
      "epoch": 0.053008130081300814,
      "step": 978,
      "training_loss": 8.554228782653809
    },
    {
      "epoch": 0.05306233062330623,
      "step": 979,
      "training_loss": 7.037246227264404
    },
    {
      "epoch": 0.053116531165311655,
      "grad_norm": 20.322328567504883,
      "learning_rate": 1e-05,
      "loss": 7.7039,
      "step": 980
    },
    {
      "epoch": 0.053116531165311655,
      "step": 980,
      "training_loss": 6.495983600616455
    },
    {
      "epoch": 0.05317073170731707,
      "step": 981,
      "training_loss": 8.01674747467041
    },
    {
      "epoch": 0.053224932249322496,
      "step": 982,
      "training_loss": 6.966456413269043
    },
    {
      "epoch": 0.05327913279132791,
      "step": 983,
      "training_loss": 7.385901927947998
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 11.912016868591309,
      "learning_rate": 1e-05,
      "loss": 7.2163,
      "step": 984
    },
    {
      "epoch": 0.05333333333333334,
      "step": 984,
      "training_loss": 7.523409366607666
    },
    {
      "epoch": 0.053387533875338754,
      "step": 985,
      "training_loss": 8.920205116271973
    },
    {
      "epoch": 0.05344173441734417,
      "step": 986,
      "training_loss": 8.082175254821777
    },
    {
      "epoch": 0.053495934959349595,
      "step": 987,
      "training_loss": 8.559883117675781
    },
    {
      "epoch": 0.05355013550135501,
      "grad_norm": 26.502840042114258,
      "learning_rate": 1e-05,
      "loss": 8.2714,
      "step": 988
    },
    {
      "epoch": 0.05355013550135501,
      "step": 988,
      "training_loss": 6.126516819000244
    },
    {
      "epoch": 0.053604336043360436,
      "step": 989,
      "training_loss": 6.245349884033203
    },
    {
      "epoch": 0.05365853658536585,
      "step": 990,
      "training_loss": 8.391536712646484
    },
    {
      "epoch": 0.05371273712737128,
      "step": 991,
      "training_loss": 7.194152355194092
    },
    {
      "epoch": 0.053766937669376694,
      "grad_norm": 21.345285415649414,
      "learning_rate": 1e-05,
      "loss": 6.9894,
      "step": 992
    },
    {
      "epoch": 0.053766937669376694,
      "step": 992,
      "training_loss": 7.435711860656738
    },
    {
      "epoch": 0.05382113821138211,
      "step": 993,
      "training_loss": 6.294216632843018
    },
    {
      "epoch": 0.053875338753387535,
      "step": 994,
      "training_loss": 7.401108264923096
    },
    {
      "epoch": 0.05392953929539295,
      "step": 995,
      "training_loss": 6.909874439239502
    },
    {
      "epoch": 0.053983739837398376,
      "grad_norm": 12.897551536560059,
      "learning_rate": 1e-05,
      "loss": 7.0102,
      "step": 996
    },
    {
      "epoch": 0.053983739837398376,
      "step": 996,
      "training_loss": 6.611092567443848
    },
    {
      "epoch": 0.05403794037940379,
      "step": 997,
      "training_loss": 7.404722213745117
    },
    {
      "epoch": 0.05409214092140922,
      "step": 998,
      "training_loss": 7.886529922485352
    },
    {
      "epoch": 0.054146341463414634,
      "step": 999,
      "training_loss": 7.304025650024414
    },
    {
      "epoch": 0.05420054200542006,
      "grad_norm": 18.598155975341797,
      "learning_rate": 1e-05,
      "loss": 7.3016,
      "step": 1000
    },
    {
      "epoch": 0.05420054200542006,
      "step": 1000,
      "training_loss": 5.423346996307373
    },
    {
      "epoch": 0.054254742547425475,
      "step": 1001,
      "training_loss": 8.353638648986816
    },
    {
      "epoch": 0.05430894308943089,
      "step": 1002,
      "training_loss": 5.708527565002441
    },
    {
      "epoch": 0.054363143631436316,
      "step": 1003,
      "training_loss": 7.219461441040039
    },
    {
      "epoch": 0.05441734417344173,
      "grad_norm": 14.725149154663086,
      "learning_rate": 1e-05,
      "loss": 6.6762,
      "step": 1004
    },
    {
      "epoch": 0.05441734417344173,
      "step": 1004,
      "training_loss": 7.737529754638672
    },
    {
      "epoch": 0.05447154471544716,
      "step": 1005,
      "training_loss": 7.195572853088379
    },
    {
      "epoch": 0.054525745257452574,
      "step": 1006,
      "training_loss": 7.598361968994141
    },
    {
      "epoch": 0.054579945799458,
      "step": 1007,
      "training_loss": 6.809720516204834
    },
    {
      "epoch": 0.054634146341463415,
      "grad_norm": 14.658546447753906,
      "learning_rate": 1e-05,
      "loss": 7.3353,
      "step": 1008
    },
    {
      "epoch": 0.054634146341463415,
      "step": 1008,
      "training_loss": 8.26030445098877
    },
    {
      "epoch": 0.05468834688346883,
      "step": 1009,
      "training_loss": 8.589308738708496
    },
    {
      "epoch": 0.054742547425474256,
      "step": 1010,
      "training_loss": 7.513148784637451
    },
    {
      "epoch": 0.05479674796747967,
      "step": 1011,
      "training_loss": 8.2163667678833
    },
    {
      "epoch": 0.0548509485094851,
      "grad_norm": 23.300668716430664,
      "learning_rate": 1e-05,
      "loss": 8.1448,
      "step": 1012
    },
    {
      "epoch": 0.0548509485094851,
      "step": 1012,
      "training_loss": 7.554819583892822
    },
    {
      "epoch": 0.054905149051490514,
      "step": 1013,
      "training_loss": 8.300232887268066
    },
    {
      "epoch": 0.05495934959349594,
      "step": 1014,
      "training_loss": 7.958643913269043
    },
    {
      "epoch": 0.055013550135501355,
      "step": 1015,
      "training_loss": 6.952342510223389
    },
    {
      "epoch": 0.05506775067750678,
      "grad_norm": 10.139036178588867,
      "learning_rate": 1e-05,
      "loss": 7.6915,
      "step": 1016
    },
    {
      "epoch": 0.05506775067750678,
      "step": 1016,
      "training_loss": 7.0158371925354
    },
    {
      "epoch": 0.055121951219512196,
      "step": 1017,
      "training_loss": 6.726415157318115
    },
    {
      "epoch": 0.05517615176151761,
      "step": 1018,
      "training_loss": 6.993637561798096
    },
    {
      "epoch": 0.055230352303523036,
      "step": 1019,
      "training_loss": 7.459524631500244
    },
    {
      "epoch": 0.055284552845528454,
      "grad_norm": 16.256820678710938,
      "learning_rate": 1e-05,
      "loss": 7.0489,
      "step": 1020
    },
    {
      "epoch": 0.055284552845528454,
      "step": 1020,
      "training_loss": 6.971411228179932
    },
    {
      "epoch": 0.05533875338753388,
      "step": 1021,
      "training_loss": 8.128894805908203
    },
    {
      "epoch": 0.055392953929539294,
      "step": 1022,
      "training_loss": 7.179687976837158
    },
    {
      "epoch": 0.05544715447154472,
      "step": 1023,
      "training_loss": 7.043475151062012
    },
    {
      "epoch": 0.055501355013550135,
      "grad_norm": 14.338786125183105,
      "learning_rate": 1e-05,
      "loss": 7.3309,
      "step": 1024
    },
    {
      "epoch": 0.055501355013550135,
      "step": 1024,
      "training_loss": 5.971951961517334
    },
    {
      "epoch": 0.05555555555555555,
      "step": 1025,
      "training_loss": 6.324031352996826
    },
    {
      "epoch": 0.055609756097560976,
      "step": 1026,
      "training_loss": 8.277191162109375
    },
    {
      "epoch": 0.05566395663956639,
      "step": 1027,
      "training_loss": 7.003145694732666
    },
    {
      "epoch": 0.05571815718157182,
      "grad_norm": 15.781475067138672,
      "learning_rate": 1e-05,
      "loss": 6.8941,
      "step": 1028
    },
    {
      "epoch": 0.05571815718157182,
      "step": 1028,
      "training_loss": 8.174275398254395
    },
    {
      "epoch": 0.055772357723577234,
      "step": 1029,
      "training_loss": 7.52393102645874
    },
    {
      "epoch": 0.05582655826558266,
      "step": 1030,
      "training_loss": 6.024376392364502
    },
    {
      "epoch": 0.055880758807588075,
      "step": 1031,
      "training_loss": 6.678736209869385
    },
    {
      "epoch": 0.0559349593495935,
      "grad_norm": 22.665674209594727,
      "learning_rate": 1e-05,
      "loss": 7.1003,
      "step": 1032
    },
    {
      "epoch": 0.0559349593495935,
      "step": 1032,
      "training_loss": 7.96961784362793
    },
    {
      "epoch": 0.055989159891598916,
      "step": 1033,
      "training_loss": 8.026301383972168
    },
    {
      "epoch": 0.05604336043360433,
      "step": 1034,
      "training_loss": 7.402660369873047
    },
    {
      "epoch": 0.05609756097560976,
      "step": 1035,
      "training_loss": 6.123022556304932
    },
    {
      "epoch": 0.056151761517615174,
      "grad_norm": 25.17579460144043,
      "learning_rate": 1e-05,
      "loss": 7.3804,
      "step": 1036
    },
    {
      "epoch": 0.056151761517615174,
      "step": 1036,
      "training_loss": 7.847700595855713
    },
    {
      "epoch": 0.0562059620596206,
      "step": 1037,
      "training_loss": 7.650993824005127
    },
    {
      "epoch": 0.056260162601626015,
      "step": 1038,
      "training_loss": 7.159902572631836
    },
    {
      "epoch": 0.05631436314363144,
      "step": 1039,
      "training_loss": 6.795316696166992
    },
    {
      "epoch": 0.056368563685636856,
      "grad_norm": 19.09967613220215,
      "learning_rate": 1e-05,
      "loss": 7.3635,
      "step": 1040
    },
    {
      "epoch": 0.056368563685636856,
      "step": 1040,
      "training_loss": 7.257760047912598
    },
    {
      "epoch": 0.05642276422764227,
      "step": 1041,
      "training_loss": 6.9885077476501465
    },
    {
      "epoch": 0.0564769647696477,
      "step": 1042,
      "training_loss": 7.7369256019592285
    },
    {
      "epoch": 0.056531165311653114,
      "step": 1043,
      "training_loss": 8.525167465209961
    },
    {
      "epoch": 0.05658536585365854,
      "grad_norm": 15.99631118774414,
      "learning_rate": 1e-05,
      "loss": 7.6271,
      "step": 1044
    },
    {
      "epoch": 0.05658536585365854,
      "step": 1044,
      "training_loss": 5.7938737869262695
    },
    {
      "epoch": 0.056639566395663955,
      "step": 1045,
      "training_loss": 7.175351619720459
    },
    {
      "epoch": 0.05669376693766938,
      "step": 1046,
      "training_loss": 6.569903373718262
    },
    {
      "epoch": 0.056747967479674796,
      "step": 1047,
      "training_loss": 5.586272716522217
    },
    {
      "epoch": 0.05680216802168022,
      "grad_norm": 19.07814598083496,
      "learning_rate": 1e-05,
      "loss": 6.2814,
      "step": 1048
    },
    {
      "epoch": 0.05680216802168022,
      "step": 1048,
      "training_loss": 7.7877068519592285
    },
    {
      "epoch": 0.05685636856368564,
      "step": 1049,
      "training_loss": 7.5955400466918945
    },
    {
      "epoch": 0.056910569105691054,
      "step": 1050,
      "training_loss": 6.238462924957275
    },
    {
      "epoch": 0.05696476964769648,
      "step": 1051,
      "training_loss": 6.607945442199707
    },
    {
      "epoch": 0.057018970189701895,
      "grad_norm": 23.0045108795166,
      "learning_rate": 1e-05,
      "loss": 7.0574,
      "step": 1052
    },
    {
      "epoch": 0.057018970189701895,
      "step": 1052,
      "training_loss": 7.107295989990234
    },
    {
      "epoch": 0.05707317073170732,
      "step": 1053,
      "training_loss": 7.168727397918701
    },
    {
      "epoch": 0.057127371273712736,
      "step": 1054,
      "training_loss": 7.283388137817383
    },
    {
      "epoch": 0.05718157181571816,
      "step": 1055,
      "training_loss": 7.24202823638916
    },
    {
      "epoch": 0.05723577235772358,
      "grad_norm": 16.806169509887695,
      "learning_rate": 1e-05,
      "loss": 7.2004,
      "step": 1056
    },
    {
      "epoch": 0.05723577235772358,
      "step": 1056,
      "training_loss": 7.69818639755249
    },
    {
      "epoch": 0.057289972899728994,
      "step": 1057,
      "training_loss": 7.180434703826904
    },
    {
      "epoch": 0.05734417344173442,
      "step": 1058,
      "training_loss": 7.1734466552734375
    },
    {
      "epoch": 0.057398373983739835,
      "step": 1059,
      "training_loss": 7.024773597717285
    },
    {
      "epoch": 0.05745257452574526,
      "grad_norm": 13.300437927246094,
      "learning_rate": 1e-05,
      "loss": 7.2692,
      "step": 1060
    },
    {
      "epoch": 0.05745257452574526,
      "step": 1060,
      "training_loss": 6.418539524078369
    },
    {
      "epoch": 0.057506775067750676,
      "step": 1061,
      "training_loss": 7.101564407348633
    },
    {
      "epoch": 0.0575609756097561,
      "step": 1062,
      "training_loss": 6.283143997192383
    },
    {
      "epoch": 0.05761517615176152,
      "step": 1063,
      "training_loss": 8.322888374328613
    },
    {
      "epoch": 0.05766937669376694,
      "grad_norm": 20.906089782714844,
      "learning_rate": 1e-05,
      "loss": 7.0315,
      "step": 1064
    },
    {
      "epoch": 0.05766937669376694,
      "step": 1064,
      "training_loss": 8.54878044128418
    },
    {
      "epoch": 0.05772357723577236,
      "step": 1065,
      "training_loss": 7.914668560028076
    },
    {
      "epoch": 0.057777777777777775,
      "step": 1066,
      "training_loss": 5.867153167724609
    },
    {
      "epoch": 0.0578319783197832,
      "step": 1067,
      "training_loss": 7.627315521240234
    },
    {
      "epoch": 0.057886178861788616,
      "grad_norm": 14.542054176330566,
      "learning_rate": 1e-05,
      "loss": 7.4895,
      "step": 1068
    },
    {
      "epoch": 0.057886178861788616,
      "step": 1068,
      "training_loss": 7.8690290451049805
    },
    {
      "epoch": 0.05794037940379404,
      "step": 1069,
      "training_loss": 9.733686447143555
    },
    {
      "epoch": 0.05799457994579946,
      "step": 1070,
      "training_loss": 7.7432122230529785
    },
    {
      "epoch": 0.05804878048780488,
      "step": 1071,
      "training_loss": 7.2561235427856445
    },
    {
      "epoch": 0.0581029810298103,
      "grad_norm": 12.425250053405762,
      "learning_rate": 1e-05,
      "loss": 8.1505,
      "step": 1072
    },
    {
      "epoch": 0.0581029810298103,
      "step": 1072,
      "training_loss": 5.981330394744873
    },
    {
      "epoch": 0.058157181571815715,
      "step": 1073,
      "training_loss": 6.773984432220459
    },
    {
      "epoch": 0.05821138211382114,
      "step": 1074,
      "training_loss": 7.977208614349365
    },
    {
      "epoch": 0.058265582655826556,
      "step": 1075,
      "training_loss": 7.5596089363098145
    },
    {
      "epoch": 0.05831978319783198,
      "grad_norm": 21.413118362426758,
      "learning_rate": 1e-05,
      "loss": 7.073,
      "step": 1076
    },
    {
      "epoch": 0.05831978319783198,
      "step": 1076,
      "training_loss": 6.7426934242248535
    },
    {
      "epoch": 0.0583739837398374,
      "step": 1077,
      "training_loss": 7.0705342292785645
    },
    {
      "epoch": 0.05842818428184282,
      "step": 1078,
      "training_loss": 6.2885260581970215
    },
    {
      "epoch": 0.05848238482384824,
      "step": 1079,
      "training_loss": 7.129429817199707
    },
    {
      "epoch": 0.05853658536585366,
      "grad_norm": 19.159372329711914,
      "learning_rate": 1e-05,
      "loss": 6.8078,
      "step": 1080
    },
    {
      "epoch": 0.05853658536585366,
      "step": 1080,
      "training_loss": 8.059964179992676
    },
    {
      "epoch": 0.05859078590785908,
      "step": 1081,
      "training_loss": 7.319461345672607
    },
    {
      "epoch": 0.058644986449864496,
      "step": 1082,
      "training_loss": 4.8918776512146
    },
    {
      "epoch": 0.05869918699186992,
      "step": 1083,
      "training_loss": 6.573831558227539
    },
    {
      "epoch": 0.05875338753387534,
      "grad_norm": 19.735681533813477,
      "learning_rate": 1e-05,
      "loss": 6.7113,
      "step": 1084
    },
    {
      "epoch": 0.05875338753387534,
      "step": 1084,
      "training_loss": 8.980514526367188
    },
    {
      "epoch": 0.05880758807588076,
      "step": 1085,
      "training_loss": 7.188004970550537
    },
    {
      "epoch": 0.05886178861788618,
      "step": 1086,
      "training_loss": 5.788382053375244
    },
    {
      "epoch": 0.0589159891598916,
      "step": 1087,
      "training_loss": 5.593386650085449
    },
    {
      "epoch": 0.05897018970189702,
      "grad_norm": 15.628474235534668,
      "learning_rate": 1e-05,
      "loss": 6.8876,
      "step": 1088
    },
    {
      "epoch": 0.05897018970189702,
      "step": 1088,
      "training_loss": 7.7224016189575195
    },
    {
      "epoch": 0.059024390243902436,
      "step": 1089,
      "training_loss": 6.692620277404785
    },
    {
      "epoch": 0.05907859078590786,
      "step": 1090,
      "training_loss": 7.205183029174805
    },
    {
      "epoch": 0.05913279132791328,
      "step": 1091,
      "training_loss": 7.5396223068237305
    },
    {
      "epoch": 0.0591869918699187,
      "grad_norm": 25.3314266204834,
      "learning_rate": 1e-05,
      "loss": 7.29,
      "step": 1092
    },
    {
      "epoch": 0.0591869918699187,
      "step": 1092,
      "training_loss": 5.666321277618408
    },
    {
      "epoch": 0.05924119241192412,
      "step": 1093,
      "training_loss": 7.4689764976501465
    },
    {
      "epoch": 0.05929539295392954,
      "step": 1094,
      "training_loss": 7.5067644119262695
    },
    {
      "epoch": 0.05934959349593496,
      "step": 1095,
      "training_loss": 6.197824478149414
    },
    {
      "epoch": 0.05940379403794038,
      "grad_norm": 15.697505950927734,
      "learning_rate": 1e-05,
      "loss": 6.71,
      "step": 1096
    },
    {
      "epoch": 0.05940379403794038,
      "step": 1096,
      "training_loss": 7.322272300720215
    },
    {
      "epoch": 0.0594579945799458,
      "step": 1097,
      "training_loss": 8.127367973327637
    },
    {
      "epoch": 0.05951219512195122,
      "step": 1098,
      "training_loss": 6.487936973571777
    },
    {
      "epoch": 0.05956639566395664,
      "step": 1099,
      "training_loss": 6.665485382080078
    },
    {
      "epoch": 0.05962059620596206,
      "grad_norm": 48.31156539916992,
      "learning_rate": 1e-05,
      "loss": 7.1508,
      "step": 1100
    },
    {
      "epoch": 0.05962059620596206,
      "step": 1100,
      "training_loss": 7.958281993865967
    },
    {
      "epoch": 0.05967479674796748,
      "step": 1101,
      "training_loss": 7.2205810546875
    },
    {
      "epoch": 0.0597289972899729,
      "step": 1102,
      "training_loss": 6.436598777770996
    },
    {
      "epoch": 0.05978319783197832,
      "step": 1103,
      "training_loss": 7.073470115661621
    },
    {
      "epoch": 0.05983739837398374,
      "grad_norm": 15.994619369506836,
      "learning_rate": 1e-05,
      "loss": 7.1722,
      "step": 1104
    },
    {
      "epoch": 0.05983739837398374,
      "step": 1104,
      "training_loss": 6.58692741394043
    },
    {
      "epoch": 0.05989159891598916,
      "step": 1105,
      "training_loss": 7.67822790145874
    },
    {
      "epoch": 0.05994579945799458,
      "step": 1106,
      "training_loss": 5.738107204437256
    },
    {
      "epoch": 0.06,
      "step": 1107,
      "training_loss": 5.804506778717041
    },
    {
      "epoch": 0.06005420054200542,
      "grad_norm": 16.356191635131836,
      "learning_rate": 1e-05,
      "loss": 6.4519,
      "step": 1108
    },
    {
      "epoch": 0.06005420054200542,
      "step": 1108,
      "training_loss": 7.362017631530762
    },
    {
      "epoch": 0.06010840108401084,
      "step": 1109,
      "training_loss": 7.348652362823486
    },
    {
      "epoch": 0.06016260162601626,
      "step": 1110,
      "training_loss": 7.012127876281738
    },
    {
      "epoch": 0.06021680216802168,
      "step": 1111,
      "training_loss": 7.323354721069336
    },
    {
      "epoch": 0.060271002710027104,
      "grad_norm": 16.182912826538086,
      "learning_rate": 1e-05,
      "loss": 7.2615,
      "step": 1112
    },
    {
      "epoch": 0.060271002710027104,
      "step": 1112,
      "training_loss": 7.774411201477051
    },
    {
      "epoch": 0.06032520325203252,
      "step": 1113,
      "training_loss": 7.275075435638428
    },
    {
      "epoch": 0.06037940379403794,
      "step": 1114,
      "training_loss": 7.010379314422607
    },
    {
      "epoch": 0.06043360433604336,
      "step": 1115,
      "training_loss": 8.298691749572754
    },
    {
      "epoch": 0.06048780487804878,
      "grad_norm": 32.006248474121094,
      "learning_rate": 1e-05,
      "loss": 7.5896,
      "step": 1116
    },
    {
      "epoch": 0.06048780487804878,
      "step": 1116,
      "training_loss": 6.9379048347473145
    },
    {
      "epoch": 0.0605420054200542,
      "step": 1117,
      "training_loss": 7.320272445678711
    },
    {
      "epoch": 0.06059620596205962,
      "step": 1118,
      "training_loss": 7.9644975662231445
    },
    {
      "epoch": 0.060650406504065044,
      "step": 1119,
      "training_loss": 7.903962135314941
    },
    {
      "epoch": 0.06070460704607046,
      "grad_norm": 27.69833755493164,
      "learning_rate": 1e-05,
      "loss": 7.5317,
      "step": 1120
    },
    {
      "epoch": 0.06070460704607046,
      "step": 1120,
      "training_loss": 7.244228839874268
    },
    {
      "epoch": 0.06075880758807588,
      "step": 1121,
      "training_loss": 9.534761428833008
    },
    {
      "epoch": 0.0608130081300813,
      "step": 1122,
      "training_loss": 6.4127912521362305
    },
    {
      "epoch": 0.06086720867208672,
      "step": 1123,
      "training_loss": 7.024827003479004
    },
    {
      "epoch": 0.06092140921409214,
      "grad_norm": 14.304356575012207,
      "learning_rate": 1e-05,
      "loss": 7.5542,
      "step": 1124
    },
    {
      "epoch": 0.06092140921409214,
      "step": 1124,
      "training_loss": 7.087276935577393
    },
    {
      "epoch": 0.06097560975609756,
      "step": 1125,
      "training_loss": 8.39791202545166
    },
    {
      "epoch": 0.061029810298102984,
      "step": 1126,
      "training_loss": 7.025268077850342
    },
    {
      "epoch": 0.0610840108401084,
      "step": 1127,
      "training_loss": 6.795405864715576
    },
    {
      "epoch": 0.061138211382113825,
      "grad_norm": 20.11495590209961,
      "learning_rate": 1e-05,
      "loss": 7.3265,
      "step": 1128
    },
    {
      "epoch": 0.061138211382113825,
      "step": 1128,
      "training_loss": 7.0560994148254395
    },
    {
      "epoch": 0.06119241192411924,
      "step": 1129,
      "training_loss": 6.819112300872803
    },
    {
      "epoch": 0.06124661246612466,
      "step": 1130,
      "training_loss": 6.259636402130127
    },
    {
      "epoch": 0.06130081300813008,
      "step": 1131,
      "training_loss": 6.642788887023926
    },
    {
      "epoch": 0.0613550135501355,
      "grad_norm": 35.22883987426758,
      "learning_rate": 1e-05,
      "loss": 6.6944,
      "step": 1132
    },
    {
      "epoch": 0.0613550135501355,
      "step": 1132,
      "training_loss": 8.563484191894531
    },
    {
      "epoch": 0.061409214092140924,
      "step": 1133,
      "training_loss": 8.603116989135742
    },
    {
      "epoch": 0.06146341463414634,
      "step": 1134,
      "training_loss": 4.8863983154296875
    },
    {
      "epoch": 0.061517615176151765,
      "step": 1135,
      "training_loss": 7.302838325500488
    },
    {
      "epoch": 0.06157181571815718,
      "grad_norm": 24.517066955566406,
      "learning_rate": 1e-05,
      "loss": 7.339,
      "step": 1136
    },
    {
      "epoch": 0.06157181571815718,
      "step": 1136,
      "training_loss": 7.574068546295166
    },
    {
      "epoch": 0.0616260162601626,
      "step": 1137,
      "training_loss": 7.282171726226807
    },
    {
      "epoch": 0.06168021680216802,
      "step": 1138,
      "training_loss": 8.10904598236084
    },
    {
      "epoch": 0.06173441734417344,
      "step": 1139,
      "training_loss": 6.1899614334106445
    },
    {
      "epoch": 0.061788617886178863,
      "grad_norm": 35.59292984008789,
      "learning_rate": 1e-05,
      "loss": 7.2888,
      "step": 1140
    },
    {
      "epoch": 0.061788617886178863,
      "step": 1140,
      "training_loss": 7.364821434020996
    },
    {
      "epoch": 0.06184281842818428,
      "step": 1141,
      "training_loss": 7.42295503616333
    },
    {
      "epoch": 0.061897018970189704,
      "step": 1142,
      "training_loss": 7.42645263671875
    },
    {
      "epoch": 0.06195121951219512,
      "step": 1143,
      "training_loss": 7.762019157409668
    },
    {
      "epoch": 0.062005420054200545,
      "grad_norm": 21.304540634155273,
      "learning_rate": 1e-05,
      "loss": 7.4941,
      "step": 1144
    },
    {
      "epoch": 0.062005420054200545,
      "step": 1144,
      "training_loss": 7.9398603439331055
    },
    {
      "epoch": 0.06205962059620596,
      "step": 1145,
      "training_loss": 6.86506986618042
    },
    {
      "epoch": 0.06211382113821138,
      "step": 1146,
      "training_loss": 6.883852005004883
    },
    {
      "epoch": 0.0621680216802168,
      "step": 1147,
      "training_loss": 9.342705726623535
    },
    {
      "epoch": 0.06222222222222222,
      "grad_norm": 25.14708137512207,
      "learning_rate": 1e-05,
      "loss": 7.7579,
      "step": 1148
    },
    {
      "epoch": 0.06222222222222222,
      "step": 1148,
      "training_loss": 6.3046875
    },
    {
      "epoch": 0.062276422764227644,
      "step": 1149,
      "training_loss": 7.595922946929932
    },
    {
      "epoch": 0.06233062330623306,
      "step": 1150,
      "training_loss": 6.738684177398682
    },
    {
      "epoch": 0.062384823848238485,
      "step": 1151,
      "training_loss": 6.993240833282471
    },
    {
      "epoch": 0.0624390243902439,
      "grad_norm": 17.268102645874023,
      "learning_rate": 1e-05,
      "loss": 6.9081,
      "step": 1152
    },
    {
      "epoch": 0.0624390243902439,
      "step": 1152,
      "training_loss": 7.0030012130737305
    },
    {
      "epoch": 0.06249322493224932,
      "step": 1153,
      "training_loss": 7.204982757568359
    },
    {
      "epoch": 0.06254742547425474,
      "step": 1154,
      "training_loss": 5.619539260864258
    },
    {
      "epoch": 0.06260162601626017,
      "step": 1155,
      "training_loss": 8.045683860778809
    },
    {
      "epoch": 0.06265582655826558,
      "grad_norm": 24.4381160736084,
      "learning_rate": 1e-05,
      "loss": 6.9683,
      "step": 1156
    },
    {
      "epoch": 0.06265582655826558,
      "step": 1156,
      "training_loss": 7.545524597167969
    },
    {
      "epoch": 0.062710027100271,
      "step": 1157,
      "training_loss": 8.503303527832031
    },
    {
      "epoch": 0.06276422764227642,
      "step": 1158,
      "training_loss": 7.1374592781066895
    },
    {
      "epoch": 0.06281842818428185,
      "step": 1159,
      "training_loss": 8.5801420211792
    },
    {
      "epoch": 0.06287262872628727,
      "grad_norm": 22.50828742980957,
      "learning_rate": 1e-05,
      "loss": 7.9416,
      "step": 1160
    },
    {
      "epoch": 0.06287262872628727,
      "step": 1160,
      "training_loss": 6.086567401885986
    },
    {
      "epoch": 0.06292682926829268,
      "step": 1161,
      "training_loss": 5.9756245613098145
    },
    {
      "epoch": 0.0629810298102981,
      "step": 1162,
      "training_loss": 7.592171669006348
    },
    {
      "epoch": 0.06303523035230352,
      "step": 1163,
      "training_loss": 5.462793350219727
    },
    {
      "epoch": 0.06308943089430895,
      "grad_norm": 18.95348358154297,
      "learning_rate": 1e-05,
      "loss": 6.2793,
      "step": 1164
    },
    {
      "epoch": 0.06308943089430895,
      "step": 1164,
      "training_loss": 5.664377689361572
    },
    {
      "epoch": 0.06314363143631437,
      "step": 1165,
      "training_loss": 6.78983736038208
    },
    {
      "epoch": 0.06319783197831978,
      "step": 1166,
      "training_loss": 6.579728603363037
    },
    {
      "epoch": 0.0632520325203252,
      "step": 1167,
      "training_loss": 7.7860026359558105
    },
    {
      "epoch": 0.06330623306233063,
      "grad_norm": 20.34470558166504,
      "learning_rate": 1e-05,
      "loss": 6.705,
      "step": 1168
    },
    {
      "epoch": 0.06330623306233063,
      "step": 1168,
      "training_loss": 6.937973976135254
    },
    {
      "epoch": 0.06336043360433605,
      "step": 1169,
      "training_loss": 6.824165344238281
    },
    {
      "epoch": 0.06341463414634146,
      "step": 1170,
      "training_loss": 7.609042167663574
    },
    {
      "epoch": 0.06346883468834688,
      "step": 1171,
      "training_loss": 6.742302894592285
    },
    {
      "epoch": 0.0635230352303523,
      "grad_norm": 29.362123489379883,
      "learning_rate": 1e-05,
      "loss": 7.0284,
      "step": 1172
    },
    {
      "epoch": 0.0635230352303523,
      "step": 1172,
      "training_loss": 7.029359817504883
    },
    {
      "epoch": 0.06357723577235773,
      "step": 1173,
      "training_loss": 8.914628028869629
    },
    {
      "epoch": 0.06363143631436315,
      "step": 1174,
      "training_loss": 6.150058746337891
    },
    {
      "epoch": 0.06368563685636856,
      "step": 1175,
      "training_loss": 5.799278259277344
    },
    {
      "epoch": 0.06373983739837398,
      "grad_norm": 17.885576248168945,
      "learning_rate": 1e-05,
      "loss": 6.9733,
      "step": 1176
    },
    {
      "epoch": 0.06373983739837398,
      "step": 1176,
      "training_loss": 6.958876132965088
    },
    {
      "epoch": 0.0637940379403794,
      "step": 1177,
      "training_loss": 7.831454277038574
    },
    {
      "epoch": 0.06384823848238483,
      "step": 1178,
      "training_loss": 7.596394062042236
    },
    {
      "epoch": 0.06390243902439025,
      "step": 1179,
      "training_loss": 7.480057716369629
    },
    {
      "epoch": 0.06395663956639566,
      "grad_norm": 24.247356414794922,
      "learning_rate": 1e-05,
      "loss": 7.4667,
      "step": 1180
    },
    {
      "epoch": 0.06395663956639566,
      "step": 1180,
      "training_loss": 7.515820503234863
    },
    {
      "epoch": 0.06401084010840108,
      "step": 1181,
      "training_loss": 6.420022964477539
    },
    {
      "epoch": 0.06406504065040651,
      "step": 1182,
      "training_loss": 6.783469200134277
    },
    {
      "epoch": 0.06411924119241193,
      "step": 1183,
      "training_loss": 8.36957836151123
    },
    {
      "epoch": 0.06417344173441734,
      "grad_norm": 21.682851791381836,
      "learning_rate": 1e-05,
      "loss": 7.2722,
      "step": 1184
    },
    {
      "epoch": 0.06417344173441734,
      "step": 1184,
      "training_loss": 8.501017570495605
    },
    {
      "epoch": 0.06422764227642276,
      "step": 1185,
      "training_loss": 6.5178728103637695
    },
    {
      "epoch": 0.06428184281842818,
      "step": 1186,
      "training_loss": 5.253070831298828
    },
    {
      "epoch": 0.06433604336043361,
      "step": 1187,
      "training_loss": 7.070279598236084
    },
    {
      "epoch": 0.06439024390243903,
      "grad_norm": 15.383620262145996,
      "learning_rate": 1e-05,
      "loss": 6.8356,
      "step": 1188
    },
    {
      "epoch": 0.06439024390243903,
      "step": 1188,
      "training_loss": 6.496452331542969
    },
    {
      "epoch": 0.06444444444444444,
      "step": 1189,
      "training_loss": 7.044044017791748
    },
    {
      "epoch": 0.06449864498644986,
      "step": 1190,
      "training_loss": 6.293707370758057
    },
    {
      "epoch": 0.06455284552845529,
      "step": 1191,
      "training_loss": 7.0924153327941895
    },
    {
      "epoch": 0.06460704607046071,
      "grad_norm": 18.98362159729004,
      "learning_rate": 1e-05,
      "loss": 6.7317,
      "step": 1192
    },
    {
      "epoch": 0.06460704607046071,
      "step": 1192,
      "training_loss": 7.6227192878723145
    },
    {
      "epoch": 0.06466124661246613,
      "step": 1193,
      "training_loss": 6.203481197357178
    },
    {
      "epoch": 0.06471544715447154,
      "step": 1194,
      "training_loss": 7.169016361236572
    },
    {
      "epoch": 0.06476964769647696,
      "step": 1195,
      "training_loss": 7.543185710906982
    },
    {
      "epoch": 0.06482384823848239,
      "grad_norm": 11.688157081604004,
      "learning_rate": 1e-05,
      "loss": 7.1346,
      "step": 1196
    },
    {
      "epoch": 0.06482384823848239,
      "step": 1196,
      "training_loss": 7.258532524108887
    },
    {
      "epoch": 0.06487804878048781,
      "step": 1197,
      "training_loss": 7.11359977722168
    },
    {
      "epoch": 0.06493224932249322,
      "step": 1198,
      "training_loss": 6.661787986755371
    },
    {
      "epoch": 0.06498644986449864,
      "step": 1199,
      "training_loss": 6.991456508636475
    },
    {
      "epoch": 0.06504065040650407,
      "grad_norm": 17.794189453125,
      "learning_rate": 1e-05,
      "loss": 7.0063,
      "step": 1200
    },
    {
      "epoch": 0.06504065040650407,
      "step": 1200,
      "training_loss": 7.901155948638916
    },
    {
      "epoch": 0.06509485094850949,
      "step": 1201,
      "training_loss": 8.227991104125977
    },
    {
      "epoch": 0.0651490514905149,
      "step": 1202,
      "training_loss": 6.608217716217041
    },
    {
      "epoch": 0.06520325203252032,
      "step": 1203,
      "training_loss": 7.616850852966309
    },
    {
      "epoch": 0.06525745257452574,
      "grad_norm": 20.819608688354492,
      "learning_rate": 1e-05,
      "loss": 7.5886,
      "step": 1204
    },
    {
      "epoch": 0.06525745257452574,
      "step": 1204,
      "training_loss": 7.557468891143799
    },
    {
      "epoch": 0.06531165311653117,
      "step": 1205,
      "training_loss": 6.961863994598389
    },
    {
      "epoch": 0.06536585365853659,
      "step": 1206,
      "training_loss": 7.553560733795166
    },
    {
      "epoch": 0.065420054200542,
      "step": 1207,
      "training_loss": 6.905263423919678
    },
    {
      "epoch": 0.06547425474254742,
      "grad_norm": 13.821636199951172,
      "learning_rate": 1e-05,
      "loss": 7.2445,
      "step": 1208
    },
    {
      "epoch": 0.06547425474254742,
      "step": 1208,
      "training_loss": 7.615242958068848
    },
    {
      "epoch": 0.06552845528455284,
      "step": 1209,
      "training_loss": 6.089145183563232
    },
    {
      "epoch": 0.06558265582655827,
      "step": 1210,
      "training_loss": 7.811246871948242
    },
    {
      "epoch": 0.06563685636856369,
      "step": 1211,
      "training_loss": 7.233102321624756
    },
    {
      "epoch": 0.0656910569105691,
      "grad_norm": 17.266504287719727,
      "learning_rate": 1e-05,
      "loss": 7.1872,
      "step": 1212
    },
    {
      "epoch": 0.0656910569105691,
      "step": 1212,
      "training_loss": 6.4957594871521
    },
    {
      "epoch": 0.06574525745257452,
      "step": 1213,
      "training_loss": 7.054527282714844
    },
    {
      "epoch": 0.06579945799457995,
      "step": 1214,
      "training_loss": 6.697423934936523
    },
    {
      "epoch": 0.06585365853658537,
      "step": 1215,
      "training_loss": 6.35510778427124
    },
    {
      "epoch": 0.06590785907859079,
      "grad_norm": 24.541244506835938,
      "learning_rate": 1e-05,
      "loss": 6.6507,
      "step": 1216
    },
    {
      "epoch": 0.06590785907859079,
      "step": 1216,
      "training_loss": 6.923638343811035
    },
    {
      "epoch": 0.0659620596205962,
      "step": 1217,
      "training_loss": 6.537091255187988
    },
    {
      "epoch": 0.06601626016260162,
      "step": 1218,
      "training_loss": 7.848398208618164
    },
    {
      "epoch": 0.06607046070460705,
      "step": 1219,
      "training_loss": 7.84337043762207
    },
    {
      "epoch": 0.06612466124661247,
      "grad_norm": 29.432851791381836,
      "learning_rate": 1e-05,
      "loss": 7.2881,
      "step": 1220
    },
    {
      "epoch": 0.06612466124661247,
      "step": 1220,
      "training_loss": 7.562560081481934
    },
    {
      "epoch": 0.06617886178861788,
      "step": 1221,
      "training_loss": 7.682186126708984
    },
    {
      "epoch": 0.0662330623306233,
      "step": 1222,
      "training_loss": 7.396001815795898
    },
    {
      "epoch": 0.06628726287262873,
      "step": 1223,
      "training_loss": 7.484533309936523
    },
    {
      "epoch": 0.06634146341463415,
      "grad_norm": 18.112091064453125,
      "learning_rate": 1e-05,
      "loss": 7.5313,
      "step": 1224
    },
    {
      "epoch": 0.06634146341463415,
      "step": 1224,
      "training_loss": 7.489201068878174
    },
    {
      "epoch": 0.06639566395663957,
      "step": 1225,
      "training_loss": 7.112617492675781
    },
    {
      "epoch": 0.06644986449864498,
      "step": 1226,
      "training_loss": 7.391746520996094
    },
    {
      "epoch": 0.0665040650406504,
      "step": 1227,
      "training_loss": 7.2776618003845215
    },
    {
      "epoch": 0.06655826558265583,
      "grad_norm": 15.220815658569336,
      "learning_rate": 1e-05,
      "loss": 7.3178,
      "step": 1228
    },
    {
      "epoch": 0.06655826558265583,
      "step": 1228,
      "training_loss": 7.107320785522461
    },
    {
      "epoch": 0.06661246612466125,
      "step": 1229,
      "training_loss": 7.567112922668457
    },
    {
      "epoch": 0.06666666666666667,
      "step": 1230,
      "training_loss": 6.95797061920166
    },
    {
      "epoch": 0.06672086720867208,
      "step": 1231,
      "training_loss": 7.65869665145874
    },
    {
      "epoch": 0.06677506775067751,
      "grad_norm": 16.2276668548584,
      "learning_rate": 1e-05,
      "loss": 7.3228,
      "step": 1232
    },
    {
      "epoch": 0.06677506775067751,
      "step": 1232,
      "training_loss": 7.971121788024902
    },
    {
      "epoch": 0.06682926829268293,
      "step": 1233,
      "training_loss": 6.615100860595703
    },
    {
      "epoch": 0.06688346883468835,
      "step": 1234,
      "training_loss": 7.738020420074463
    },
    {
      "epoch": 0.06693766937669376,
      "step": 1235,
      "training_loss": 7.199188232421875
    },
    {
      "epoch": 0.06699186991869918,
      "grad_norm": 20.82866859436035,
      "learning_rate": 1e-05,
      "loss": 7.3809,
      "step": 1236
    },
    {
      "epoch": 0.06699186991869918,
      "step": 1236,
      "training_loss": 6.486534595489502
    },
    {
      "epoch": 0.06704607046070461,
      "step": 1237,
      "training_loss": 8.544031143188477
    },
    {
      "epoch": 0.06710027100271003,
      "step": 1238,
      "training_loss": 6.856101036071777
    },
    {
      "epoch": 0.06715447154471545,
      "step": 1239,
      "training_loss": 5.30283260345459
    },
    {
      "epoch": 0.06720867208672086,
      "grad_norm": 16.347774505615234,
      "learning_rate": 1e-05,
      "loss": 6.7974,
      "step": 1240
    },
    {
      "epoch": 0.06720867208672086,
      "step": 1240,
      "training_loss": 7.23225212097168
    },
    {
      "epoch": 0.06726287262872628,
      "step": 1241,
      "training_loss": 6.07886266708374
    },
    {
      "epoch": 0.06731707317073171,
      "step": 1242,
      "training_loss": 6.329558372497559
    },
    {
      "epoch": 0.06737127371273713,
      "step": 1243,
      "training_loss": 6.819217205047607
    },
    {
      "epoch": 0.06742547425474255,
      "grad_norm": 22.68744659423828,
      "learning_rate": 1e-05,
      "loss": 6.615,
      "step": 1244
    },
    {
      "epoch": 0.06742547425474255,
      "step": 1244,
      "training_loss": 7.869333744049072
    },
    {
      "epoch": 0.06747967479674796,
      "step": 1245,
      "training_loss": 5.674101829528809
    },
    {
      "epoch": 0.0675338753387534,
      "step": 1246,
      "training_loss": 6.418008327484131
    },
    {
      "epoch": 0.06758807588075881,
      "step": 1247,
      "training_loss": 7.838515281677246
    },
    {
      "epoch": 0.06764227642276423,
      "grad_norm": 14.113866806030273,
      "learning_rate": 1e-05,
      "loss": 6.95,
      "step": 1248
    },
    {
      "epoch": 0.06764227642276423,
      "step": 1248,
      "training_loss": 5.779331207275391
    },
    {
      "epoch": 0.06769647696476964,
      "step": 1249,
      "training_loss": 6.970399379730225
    },
    {
      "epoch": 0.06775067750677506,
      "step": 1250,
      "training_loss": 7.544131755828857
    },
    {
      "epoch": 0.06780487804878049,
      "step": 1251,
      "training_loss": 5.306141376495361
    },
    {
      "epoch": 0.06785907859078591,
      "grad_norm": 21.736698150634766,
      "learning_rate": 1e-05,
      "loss": 6.4,
      "step": 1252
    },
    {
      "epoch": 0.06785907859078591,
      "step": 1252,
      "training_loss": 6.99942684173584
    },
    {
      "epoch": 0.06791327913279133,
      "step": 1253,
      "training_loss": 6.643209457397461
    },
    {
      "epoch": 0.06796747967479674,
      "step": 1254,
      "training_loss": 7.06787633895874
    },
    {
      "epoch": 0.06802168021680217,
      "step": 1255,
      "training_loss": 7.495128631591797
    },
    {
      "epoch": 0.06807588075880759,
      "grad_norm": 19.095090866088867,
      "learning_rate": 1e-05,
      "loss": 7.0514,
      "step": 1256
    },
    {
      "epoch": 0.06807588075880759,
      "step": 1256,
      "training_loss": 7.004937171936035
    },
    {
      "epoch": 0.06813008130081301,
      "step": 1257,
      "training_loss": 7.208305358886719
    },
    {
      "epoch": 0.06818428184281843,
      "step": 1258,
      "training_loss": 7.423425197601318
    },
    {
      "epoch": 0.06823848238482384,
      "step": 1259,
      "training_loss": 6.997070789337158
    },
    {
      "epoch": 0.06829268292682927,
      "grad_norm": 23.80327033996582,
      "learning_rate": 1e-05,
      "loss": 7.1584,
      "step": 1260
    },
    {
      "epoch": 0.06829268292682927,
      "step": 1260,
      "training_loss": 6.68543004989624
    },
    {
      "epoch": 0.06834688346883469,
      "step": 1261,
      "training_loss": 6.052387714385986
    },
    {
      "epoch": 0.06840108401084011,
      "step": 1262,
      "training_loss": 6.052464962005615
    },
    {
      "epoch": 0.06845528455284552,
      "step": 1263,
      "training_loss": 6.403851509094238
    },
    {
      "epoch": 0.06850948509485096,
      "grad_norm": 18.446142196655273,
      "learning_rate": 1e-05,
      "loss": 6.2985,
      "step": 1264
    },
    {
      "epoch": 0.06850948509485096,
      "step": 1264,
      "training_loss": 7.03385066986084
    },
    {
      "epoch": 0.06856368563685637,
      "step": 1265,
      "training_loss": 8.072417259216309
    },
    {
      "epoch": 0.06861788617886179,
      "step": 1266,
      "training_loss": 7.442957401275635
    },
    {
      "epoch": 0.0686720867208672,
      "step": 1267,
      "training_loss": 6.9312052726745605
    },
    {
      "epoch": 0.06872628726287262,
      "grad_norm": 16.32529067993164,
      "learning_rate": 1e-05,
      "loss": 7.3701,
      "step": 1268
    },
    {
      "epoch": 0.06872628726287262,
      "step": 1268,
      "training_loss": 6.8176045417785645
    },
    {
      "epoch": 0.06878048780487805,
      "step": 1269,
      "training_loss": 7.9781880378723145
    },
    {
      "epoch": 0.06883468834688347,
      "step": 1270,
      "training_loss": 6.628145694732666
    },
    {
      "epoch": 0.06888888888888889,
      "step": 1271,
      "training_loss": 7.4174089431762695
    },
    {
      "epoch": 0.0689430894308943,
      "grad_norm": 19.21282386779785,
      "learning_rate": 1e-05,
      "loss": 7.2103,
      "step": 1272
    },
    {
      "epoch": 0.0689430894308943,
      "step": 1272,
      "training_loss": 5.894661903381348
    },
    {
      "epoch": 0.06899728997289972,
      "step": 1273,
      "training_loss": 8.32895278930664
    },
    {
      "epoch": 0.06905149051490515,
      "step": 1274,
      "training_loss": 7.910914421081543
    },
    {
      "epoch": 0.06910569105691057,
      "step": 1275,
      "training_loss": 6.963369846343994
    },
    {
      "epoch": 0.06915989159891599,
      "grad_norm": 17.936185836791992,
      "learning_rate": 1e-05,
      "loss": 7.2745,
      "step": 1276
    },
    {
      "epoch": 0.06915989159891599,
      "step": 1276,
      "training_loss": 9.25946044921875
    },
    {
      "epoch": 0.0692140921409214,
      "step": 1277,
      "training_loss": 7.969833850860596
    },
    {
      "epoch": 0.06926829268292684,
      "step": 1278,
      "training_loss": 7.078672885894775
    },
    {
      "epoch": 0.06932249322493225,
      "step": 1279,
      "training_loss": 7.192852020263672
    },
    {
      "epoch": 0.06937669376693767,
      "grad_norm": 12.04131031036377,
      "learning_rate": 1e-05,
      "loss": 7.8752,
      "step": 1280
    },
    {
      "epoch": 0.06937669376693767,
      "step": 1280,
      "training_loss": 7.375736713409424
    },
    {
      "epoch": 0.06943089430894309,
      "step": 1281,
      "training_loss": 6.965559005737305
    },
    {
      "epoch": 0.0694850948509485,
      "step": 1282,
      "training_loss": 11.058287620544434
    },
    {
      "epoch": 0.06953929539295393,
      "step": 1283,
      "training_loss": 7.674808979034424
    },
    {
      "epoch": 0.06959349593495935,
      "grad_norm": 13.566668510437012,
      "learning_rate": 1e-05,
      "loss": 8.2686,
      "step": 1284
    },
    {
      "epoch": 0.06959349593495935,
      "step": 1284,
      "training_loss": 9.452164649963379
    },
    {
      "epoch": 0.06964769647696477,
      "step": 1285,
      "training_loss": 8.833514213562012
    },
    {
      "epoch": 0.06970189701897019,
      "step": 1286,
      "training_loss": 6.692436218261719
    },
    {
      "epoch": 0.06975609756097562,
      "step": 1287,
      "training_loss": 6.7555742263793945
    },
    {
      "epoch": 0.06981029810298103,
      "grad_norm": 12.378443717956543,
      "learning_rate": 1e-05,
      "loss": 7.9334,
      "step": 1288
    },
    {
      "epoch": 0.06981029810298103,
      "step": 1288,
      "training_loss": 7.519520282745361
    },
    {
      "epoch": 0.06986449864498645,
      "step": 1289,
      "training_loss": 6.439953327178955
    },
    {
      "epoch": 0.06991869918699187,
      "step": 1290,
      "training_loss": 6.692440509796143
    },
    {
      "epoch": 0.06997289972899728,
      "step": 1291,
      "training_loss": 6.244106292724609
    },
    {
      "epoch": 0.07002710027100272,
      "grad_norm": 12.969138145446777,
      "learning_rate": 1e-05,
      "loss": 6.724,
      "step": 1292
    },
    {
      "epoch": 0.07002710027100272,
      "step": 1292,
      "training_loss": 8.146893501281738
    },
    {
      "epoch": 0.07008130081300813,
      "step": 1293,
      "training_loss": 6.87779426574707
    },
    {
      "epoch": 0.07013550135501355,
      "step": 1294,
      "training_loss": 5.506044387817383
    },
    {
      "epoch": 0.07018970189701897,
      "step": 1295,
      "training_loss": 7.793099880218506
    },
    {
      "epoch": 0.0702439024390244,
      "grad_norm": 18.20217514038086,
      "learning_rate": 1e-05,
      "loss": 7.081,
      "step": 1296
    },
    {
      "epoch": 0.0702439024390244,
      "step": 1296,
      "training_loss": 7.330090045928955
    },
    {
      "epoch": 0.07029810298102981,
      "step": 1297,
      "training_loss": 7.517210006713867
    },
    {
      "epoch": 0.07035230352303523,
      "step": 1298,
      "training_loss": 7.536279201507568
    },
    {
      "epoch": 0.07040650406504065,
      "step": 1299,
      "training_loss": 6.697569847106934
    },
    {
      "epoch": 0.07046070460704607,
      "grad_norm": 23.272714614868164,
      "learning_rate": 1e-05,
      "loss": 7.2703,
      "step": 1300
    },
    {
      "epoch": 0.07046070460704607,
      "step": 1300,
      "training_loss": 7.980480194091797
    },
    {
      "epoch": 0.0705149051490515,
      "step": 1301,
      "training_loss": 5.9181928634643555
    },
    {
      "epoch": 0.07056910569105691,
      "step": 1302,
      "training_loss": 8.12515640258789
    },
    {
      "epoch": 0.07062330623306233,
      "step": 1303,
      "training_loss": 7.587394714355469
    },
    {
      "epoch": 0.07067750677506775,
      "grad_norm": 14.558653831481934,
      "learning_rate": 1e-05,
      "loss": 7.4028,
      "step": 1304
    },
    {
      "epoch": 0.07067750677506775,
      "step": 1304,
      "training_loss": 7.514743804931641
    },
    {
      "epoch": 0.07073170731707316,
      "step": 1305,
      "training_loss": 8.084977149963379
    },
    {
      "epoch": 0.0707859078590786,
      "step": 1306,
      "training_loss": 6.610060214996338
    },
    {
      "epoch": 0.07084010840108401,
      "step": 1307,
      "training_loss": 7.934784889221191
    },
    {
      "epoch": 0.07089430894308943,
      "grad_norm": 25.570402145385742,
      "learning_rate": 1e-05,
      "loss": 7.5361,
      "step": 1308
    },
    {
      "epoch": 0.07089430894308943,
      "step": 1308,
      "training_loss": 8.580503463745117
    },
    {
      "epoch": 0.07094850948509485,
      "step": 1309,
      "training_loss": 7.062569618225098
    },
    {
      "epoch": 0.07100271002710028,
      "step": 1310,
      "training_loss": 7.262284278869629
    },
    {
      "epoch": 0.0710569105691057,
      "step": 1311,
      "training_loss": 6.484193325042725
    },
    {
      "epoch": 0.07111111111111111,
      "grad_norm": 19.427053451538086,
      "learning_rate": 1e-05,
      "loss": 7.3474,
      "step": 1312
    },
    {
      "epoch": 0.07111111111111111,
      "step": 1312,
      "training_loss": 5.9641008377075195
    },
    {
      "epoch": 0.07116531165311653,
      "step": 1313,
      "training_loss": 6.6972808837890625
    },
    {
      "epoch": 0.07121951219512195,
      "step": 1314,
      "training_loss": 6.620757102966309
    },
    {
      "epoch": 0.07127371273712738,
      "step": 1315,
      "training_loss": 6.24309778213501
    },
    {
      "epoch": 0.07132791327913279,
      "grad_norm": 27.212825775146484,
      "learning_rate": 1e-05,
      "loss": 6.3813,
      "step": 1316
    },
    {
      "epoch": 0.07132791327913279,
      "step": 1316,
      "training_loss": 6.594232082366943
    },
    {
      "epoch": 0.07138211382113821,
      "step": 1317,
      "training_loss": 7.6760029792785645
    },
    {
      "epoch": 0.07143631436314363,
      "step": 1318,
      "training_loss": 7.49739408493042
    },
    {
      "epoch": 0.07149051490514906,
      "step": 1319,
      "training_loss": 7.373599052429199
    },
    {
      "epoch": 0.07154471544715447,
      "grad_norm": 26.727933883666992,
      "learning_rate": 1e-05,
      "loss": 7.2853,
      "step": 1320
    },
    {
      "epoch": 0.07154471544715447,
      "step": 1320,
      "training_loss": 6.636991500854492
    },
    {
      "epoch": 0.07159891598915989,
      "step": 1321,
      "training_loss": 7.629459381103516
    },
    {
      "epoch": 0.07165311653116531,
      "step": 1322,
      "training_loss": 6.804813861846924
    },
    {
      "epoch": 0.07170731707317073,
      "step": 1323,
      "training_loss": 6.669712543487549
    },
    {
      "epoch": 0.07176151761517616,
      "grad_norm": 13.523297309875488,
      "learning_rate": 1e-05,
      "loss": 6.9352,
      "step": 1324
    },
    {
      "epoch": 0.07176151761517616,
      "step": 1324,
      "training_loss": 9.069578170776367
    },
    {
      "epoch": 0.07181571815718157,
      "step": 1325,
      "training_loss": 8.159207344055176
    },
    {
      "epoch": 0.07186991869918699,
      "step": 1326,
      "training_loss": 9.148430824279785
    },
    {
      "epoch": 0.07192411924119241,
      "step": 1327,
      "training_loss": 7.067361831665039
    },
    {
      "epoch": 0.07197831978319784,
      "grad_norm": 25.682552337646484,
      "learning_rate": 1e-05,
      "loss": 8.3611,
      "step": 1328
    },
    {
      "epoch": 0.07197831978319784,
      "step": 1328,
      "training_loss": 7.775543212890625
    },
    {
      "epoch": 0.07203252032520326,
      "step": 1329,
      "training_loss": 7.616308212280273
    },
    {
      "epoch": 0.07208672086720867,
      "step": 1330,
      "training_loss": 7.10444974899292
    },
    {
      "epoch": 0.07214092140921409,
      "step": 1331,
      "training_loss": 8.170500755310059
    },
    {
      "epoch": 0.0721951219512195,
      "grad_norm": 16.557764053344727,
      "learning_rate": 1e-05,
      "loss": 7.6667,
      "step": 1332
    },
    {
      "epoch": 0.0721951219512195,
      "step": 1332,
      "training_loss": 5.681288242340088
    },
    {
      "epoch": 0.07224932249322494,
      "step": 1333,
      "training_loss": 7.5265960693359375
    },
    {
      "epoch": 0.07230352303523035,
      "step": 1334,
      "training_loss": 7.6290283203125
    },
    {
      "epoch": 0.07235772357723577,
      "step": 1335,
      "training_loss": 8.207475662231445
    },
    {
      "epoch": 0.07241192411924119,
      "grad_norm": 19.8802547454834,
      "learning_rate": 1e-05,
      "loss": 7.2611,
      "step": 1336
    },
    {
      "epoch": 0.07241192411924119,
      "step": 1336,
      "training_loss": 7.050888538360596
    },
    {
      "epoch": 0.0724661246612466,
      "step": 1337,
      "training_loss": 5.191099166870117
    },
    {
      "epoch": 0.07252032520325204,
      "step": 1338,
      "training_loss": 7.669728755950928
    },
    {
      "epoch": 0.07257452574525745,
      "step": 1339,
      "training_loss": 8.126837730407715
    },
    {
      "epoch": 0.07262872628726287,
      "grad_norm": 13.511286735534668,
      "learning_rate": 1e-05,
      "loss": 7.0096,
      "step": 1340
    },
    {
      "epoch": 0.07262872628726287,
      "step": 1340,
      "training_loss": 6.420637607574463
    },
    {
      "epoch": 0.07268292682926829,
      "step": 1341,
      "training_loss": 5.995233535766602
    },
    {
      "epoch": 0.07273712737127372,
      "step": 1342,
      "training_loss": 7.170783042907715
    },
    {
      "epoch": 0.07279132791327914,
      "step": 1343,
      "training_loss": 7.402502536773682
    },
    {
      "epoch": 0.07284552845528455,
      "grad_norm": 15.774076461791992,
      "learning_rate": 1e-05,
      "loss": 6.7473,
      "step": 1344
    },
    {
      "epoch": 0.07284552845528455,
      "step": 1344,
      "training_loss": 8.018043518066406
    },
    {
      "epoch": 0.07289972899728997,
      "step": 1345,
      "training_loss": 7.46043586730957
    },
    {
      "epoch": 0.07295392953929539,
      "step": 1346,
      "training_loss": 7.304476261138916
    },
    {
      "epoch": 0.07300813008130082,
      "step": 1347,
      "training_loss": 7.7636189460754395
    },
    {
      "epoch": 0.07306233062330623,
      "grad_norm": 20.593793869018555,
      "learning_rate": 1e-05,
      "loss": 7.6366,
      "step": 1348
    },
    {
      "epoch": 0.07306233062330623,
      "step": 1348,
      "training_loss": 6.435647487640381
    },
    {
      "epoch": 0.07311653116531165,
      "step": 1349,
      "training_loss": 7.217040061950684
    },
    {
      "epoch": 0.07317073170731707,
      "step": 1350,
      "training_loss": 7.993233680725098
    },
    {
      "epoch": 0.0732249322493225,
      "step": 1351,
      "training_loss": 6.630616664886475
    },
    {
      "epoch": 0.07327913279132792,
      "grad_norm": 13.688066482543945,
      "learning_rate": 1e-05,
      "loss": 7.0691,
      "step": 1352
    },
    {
      "epoch": 0.07327913279132792,
      "step": 1352,
      "training_loss": 8.521154403686523
    },
    {
      "epoch": 0.07333333333333333,
      "step": 1353,
      "training_loss": 6.450411796569824
    },
    {
      "epoch": 0.07338753387533875,
      "step": 1354,
      "training_loss": 7.5537238121032715
    },
    {
      "epoch": 0.07344173441734417,
      "step": 1355,
      "training_loss": 7.611783504486084
    },
    {
      "epoch": 0.0734959349593496,
      "grad_norm": 52.94451904296875,
      "learning_rate": 1e-05,
      "loss": 7.5343,
      "step": 1356
    },
    {
      "epoch": 0.0734959349593496,
      "step": 1356,
      "training_loss": 5.538791179656982
    },
    {
      "epoch": 0.07355013550135502,
      "step": 1357,
      "training_loss": 7.1327128410339355
    },
    {
      "epoch": 0.07360433604336043,
      "step": 1358,
      "training_loss": 7.333003997802734
    },
    {
      "epoch": 0.07365853658536585,
      "step": 1359,
      "training_loss": 6.648930549621582
    },
    {
      "epoch": 0.07371273712737128,
      "grad_norm": 17.89681053161621,
      "learning_rate": 1e-05,
      "loss": 6.6634,
      "step": 1360
    },
    {
      "epoch": 0.07371273712737128,
      "step": 1360,
      "training_loss": 6.737459182739258
    },
    {
      "epoch": 0.0737669376693767,
      "step": 1361,
      "training_loss": 7.360080242156982
    },
    {
      "epoch": 0.07382113821138211,
      "step": 1362,
      "training_loss": 7.177248001098633
    },
    {
      "epoch": 0.07387533875338753,
      "step": 1363,
      "training_loss": 8.061264991760254
    },
    {
      "epoch": 0.07392953929539295,
      "grad_norm": 24.463987350463867,
      "learning_rate": 1e-05,
      "loss": 7.334,
      "step": 1364
    },
    {
      "epoch": 0.07392953929539295,
      "step": 1364,
      "training_loss": 7.587916851043701
    },
    {
      "epoch": 0.07398373983739838,
      "step": 1365,
      "training_loss": 7.355554580688477
    },
    {
      "epoch": 0.0740379403794038,
      "step": 1366,
      "training_loss": 6.500428199768066
    },
    {
      "epoch": 0.07409214092140921,
      "step": 1367,
      "training_loss": 7.597382068634033
    },
    {
      "epoch": 0.07414634146341463,
      "grad_norm": 15.095193862915039,
      "learning_rate": 1e-05,
      "loss": 7.2603,
      "step": 1368
    },
    {
      "epoch": 0.07414634146341463,
      "step": 1368,
      "training_loss": 6.5892229080200195
    },
    {
      "epoch": 0.07420054200542005,
      "step": 1369,
      "training_loss": 6.494722843170166
    },
    {
      "epoch": 0.07425474254742548,
      "step": 1370,
      "training_loss": 7.964849948883057
    },
    {
      "epoch": 0.0743089430894309,
      "step": 1371,
      "training_loss": 8.512529373168945
    },
    {
      "epoch": 0.07436314363143631,
      "grad_norm": 21.977903366088867,
      "learning_rate": 1e-05,
      "loss": 7.3903,
      "step": 1372
    },
    {
      "epoch": 0.07436314363143631,
      "step": 1372,
      "training_loss": 7.559330463409424
    },
    {
      "epoch": 0.07441734417344173,
      "step": 1373,
      "training_loss": 7.239760875701904
    },
    {
      "epoch": 0.07447154471544716,
      "step": 1374,
      "training_loss": 7.072460174560547
    },
    {
      "epoch": 0.07452574525745258,
      "step": 1375,
      "training_loss": 7.119217395782471
    },
    {
      "epoch": 0.074579945799458,
      "grad_norm": 18.09760856628418,
      "learning_rate": 1e-05,
      "loss": 7.2477,
      "step": 1376
    },
    {
      "epoch": 0.074579945799458,
      "step": 1376,
      "training_loss": 7.294633388519287
    },
    {
      "epoch": 0.07463414634146341,
      "step": 1377,
      "training_loss": 6.429388523101807
    },
    {
      "epoch": 0.07468834688346883,
      "step": 1378,
      "training_loss": 7.016976356506348
    },
    {
      "epoch": 0.07474254742547426,
      "step": 1379,
      "training_loss": 6.5732951164245605
    },
    {
      "epoch": 0.07479674796747968,
      "grad_norm": 14.66733455657959,
      "learning_rate": 1e-05,
      "loss": 6.8286,
      "step": 1380
    },
    {
      "epoch": 0.07479674796747968,
      "step": 1380,
      "training_loss": 7.496856212615967
    },
    {
      "epoch": 0.0748509485094851,
      "step": 1381,
      "training_loss": 7.9602952003479
    },
    {
      "epoch": 0.07490514905149051,
      "step": 1382,
      "training_loss": 6.639913558959961
    },
    {
      "epoch": 0.07495934959349594,
      "step": 1383,
      "training_loss": 7.5907392501831055
    },
    {
      "epoch": 0.07501355013550136,
      "grad_norm": 17.689172744750977,
      "learning_rate": 1e-05,
      "loss": 7.422,
      "step": 1384
    },
    {
      "epoch": 0.07501355013550136,
      "step": 1384,
      "training_loss": 7.628103733062744
    },
    {
      "epoch": 0.07506775067750678,
      "step": 1385,
      "training_loss": 8.529654502868652
    },
    {
      "epoch": 0.07512195121951219,
      "step": 1386,
      "training_loss": 5.507836818695068
    },
    {
      "epoch": 0.07517615176151761,
      "step": 1387,
      "training_loss": 7.2611494064331055
    },
    {
      "epoch": 0.07523035230352304,
      "grad_norm": 29.80289077758789,
      "learning_rate": 1e-05,
      "loss": 7.2317,
      "step": 1388
    },
    {
      "epoch": 0.07523035230352304,
      "step": 1388,
      "training_loss": 7.598761081695557
    },
    {
      "epoch": 0.07528455284552846,
      "step": 1389,
      "training_loss": 6.10706901550293
    },
    {
      "epoch": 0.07533875338753387,
      "step": 1390,
      "training_loss": 9.289928436279297
    },
    {
      "epoch": 0.07539295392953929,
      "step": 1391,
      "training_loss": 8.023929595947266
    },
    {
      "epoch": 0.07544715447154472,
      "grad_norm": 30.00541877746582,
      "learning_rate": 1e-05,
      "loss": 7.7549,
      "step": 1392
    },
    {
      "epoch": 0.07544715447154472,
      "step": 1392,
      "training_loss": 6.688792705535889
    },
    {
      "epoch": 0.07550135501355014,
      "step": 1393,
      "training_loss": 7.545599937438965
    },
    {
      "epoch": 0.07555555555555556,
      "step": 1394,
      "training_loss": 8.403003692626953
    },
    {
      "epoch": 0.07560975609756097,
      "step": 1395,
      "training_loss": 7.7653279304504395
    },
    {
      "epoch": 0.07566395663956639,
      "grad_norm": 23.372568130493164,
      "learning_rate": 1e-05,
      "loss": 7.6007,
      "step": 1396
    },
    {
      "epoch": 0.07566395663956639,
      "step": 1396,
      "training_loss": 7.430633068084717
    },
    {
      "epoch": 0.07571815718157182,
      "step": 1397,
      "training_loss": 6.868967056274414
    },
    {
      "epoch": 0.07577235772357724,
      "step": 1398,
      "training_loss": 6.976731300354004
    },
    {
      "epoch": 0.07582655826558266,
      "step": 1399,
      "training_loss": 7.246401309967041
    },
    {
      "epoch": 0.07588075880758807,
      "grad_norm": 15.635798454284668,
      "learning_rate": 1e-05,
      "loss": 7.1307,
      "step": 1400
    },
    {
      "epoch": 0.07588075880758807,
      "step": 1400,
      "training_loss": 6.245474338531494
    },
    {
      "epoch": 0.07593495934959349,
      "step": 1401,
      "training_loss": 6.546582221984863
    },
    {
      "epoch": 0.07598915989159892,
      "step": 1402,
      "training_loss": 8.040111541748047
    },
    {
      "epoch": 0.07604336043360434,
      "step": 1403,
      "training_loss": 7.86308479309082
    },
    {
      "epoch": 0.07609756097560975,
      "grad_norm": 15.693978309631348,
      "learning_rate": 1e-05,
      "loss": 7.1738,
      "step": 1404
    },
    {
      "epoch": 0.07609756097560975,
      "step": 1404,
      "training_loss": 5.040076732635498
    },
    {
      "epoch": 0.07615176151761517,
      "step": 1405,
      "training_loss": 7.231629371643066
    },
    {
      "epoch": 0.0762059620596206,
      "step": 1406,
      "training_loss": 7.01637077331543
    },
    {
      "epoch": 0.07626016260162602,
      "step": 1407,
      "training_loss": 6.504293918609619
    },
    {
      "epoch": 0.07631436314363144,
      "grad_norm": 18.494321823120117,
      "learning_rate": 1e-05,
      "loss": 6.4481,
      "step": 1408
    },
    {
      "epoch": 0.07631436314363144,
      "step": 1408,
      "training_loss": 6.78996467590332
    },
    {
      "epoch": 0.07636856368563685,
      "step": 1409,
      "training_loss": 6.724791526794434
    },
    {
      "epoch": 0.07642276422764227,
      "step": 1410,
      "training_loss": 6.271981239318848
    },
    {
      "epoch": 0.0764769647696477,
      "step": 1411,
      "training_loss": 7.091998100280762
    },
    {
      "epoch": 0.07653116531165312,
      "grad_norm": 13.725008964538574,
      "learning_rate": 1e-05,
      "loss": 6.7197,
      "step": 1412
    },
    {
      "epoch": 0.07653116531165312,
      "step": 1412,
      "training_loss": 7.8359785079956055
    },
    {
      "epoch": 0.07658536585365854,
      "step": 1413,
      "training_loss": 7.817992687225342
    },
    {
      "epoch": 0.07663956639566395,
      "step": 1414,
      "training_loss": 7.339719295501709
    },
    {
      "epoch": 0.07669376693766938,
      "step": 1415,
      "training_loss": 7.190313339233398
    },
    {
      "epoch": 0.0767479674796748,
      "grad_norm": 37.538291931152344,
      "learning_rate": 1e-05,
      "loss": 7.546,
      "step": 1416
    },
    {
      "epoch": 0.0767479674796748,
      "step": 1416,
      "training_loss": 7.485129356384277
    },
    {
      "epoch": 0.07680216802168022,
      "step": 1417,
      "training_loss": 6.963754653930664
    },
    {
      "epoch": 0.07685636856368563,
      "step": 1418,
      "training_loss": 6.3950676918029785
    },
    {
      "epoch": 0.07691056910569105,
      "step": 1419,
      "training_loss": 8.116442680358887
    },
    {
      "epoch": 0.07696476964769648,
      "grad_norm": 14.048554420471191,
      "learning_rate": 1e-05,
      "loss": 7.2401,
      "step": 1420
    },
    {
      "epoch": 0.07696476964769648,
      "step": 1420,
      "training_loss": 6.996037006378174
    },
    {
      "epoch": 0.0770189701897019,
      "step": 1421,
      "training_loss": 8.285539627075195
    },
    {
      "epoch": 0.07707317073170732,
      "step": 1422,
      "training_loss": 6.649250507354736
    },
    {
      "epoch": 0.07712737127371273,
      "step": 1423,
      "training_loss": 7.309784412384033
    },
    {
      "epoch": 0.07718157181571816,
      "grad_norm": 19.633821487426758,
      "learning_rate": 1e-05,
      "loss": 7.3102,
      "step": 1424
    },
    {
      "epoch": 0.07718157181571816,
      "step": 1424,
      "training_loss": 6.99269962310791
    },
    {
      "epoch": 0.07723577235772358,
      "step": 1425,
      "training_loss": 6.368566513061523
    },
    {
      "epoch": 0.077289972899729,
      "step": 1426,
      "training_loss": 7.817479610443115
    },
    {
      "epoch": 0.07734417344173442,
      "step": 1427,
      "training_loss": 6.435400009155273
    },
    {
      "epoch": 0.07739837398373983,
      "grad_norm": 21.16888999938965,
      "learning_rate": 1e-05,
      "loss": 6.9035,
      "step": 1428
    },
    {
      "epoch": 0.07739837398373983,
      "step": 1428,
      "training_loss": 6.521190166473389
    },
    {
      "epoch": 0.07745257452574526,
      "step": 1429,
      "training_loss": 7.970839977264404
    },
    {
      "epoch": 0.07750677506775068,
      "step": 1430,
      "training_loss": 7.6834917068481445
    },
    {
      "epoch": 0.0775609756097561,
      "step": 1431,
      "training_loss": 7.40251350402832
    },
    {
      "epoch": 0.07761517615176151,
      "grad_norm": 23.670787811279297,
      "learning_rate": 1e-05,
      "loss": 7.3945,
      "step": 1432
    },
    {
      "epoch": 0.07761517615176151,
      "step": 1432,
      "training_loss": 9.035991668701172
    },
    {
      "epoch": 0.07766937669376693,
      "step": 1433,
      "training_loss": 6.817252159118652
    },
    {
      "epoch": 0.07772357723577236,
      "step": 1434,
      "training_loss": 6.138806343078613
    },
    {
      "epoch": 0.07777777777777778,
      "step": 1435,
      "training_loss": 7.0732011795043945
    },
    {
      "epoch": 0.0778319783197832,
      "grad_norm": 14.223114967346191,
      "learning_rate": 1e-05,
      "loss": 7.2663,
      "step": 1436
    },
    {
      "epoch": 0.0778319783197832,
      "step": 1436,
      "training_loss": 7.20578145980835
    },
    {
      "epoch": 0.07788617886178861,
      "step": 1437,
      "training_loss": 8.071422576904297
    },
    {
      "epoch": 0.07794037940379404,
      "step": 1438,
      "training_loss": 7.480517387390137
    },
    {
      "epoch": 0.07799457994579946,
      "step": 1439,
      "training_loss": 7.467219829559326
    },
    {
      "epoch": 0.07804878048780488,
      "grad_norm": 24.664318084716797,
      "learning_rate": 1e-05,
      "loss": 7.5562,
      "step": 1440
    },
    {
      "epoch": 0.07804878048780488,
      "step": 1440,
      "training_loss": 7.579808712005615
    },
    {
      "epoch": 0.0781029810298103,
      "step": 1441,
      "training_loss": 6.848506450653076
    },
    {
      "epoch": 0.07815718157181571,
      "step": 1442,
      "training_loss": 7.306116580963135
    },
    {
      "epoch": 0.07821138211382114,
      "step": 1443,
      "training_loss": 6.494561672210693
    },
    {
      "epoch": 0.07826558265582656,
      "grad_norm": 42.137451171875,
      "learning_rate": 1e-05,
      "loss": 7.0572,
      "step": 1444
    },
    {
      "epoch": 0.07826558265582656,
      "step": 1444,
      "training_loss": 7.392282009124756
    },
    {
      "epoch": 0.07831978319783198,
      "step": 1445,
      "training_loss": 6.134659290313721
    },
    {
      "epoch": 0.0783739837398374,
      "step": 1446,
      "training_loss": 5.638955593109131
    },
    {
      "epoch": 0.07842818428184282,
      "step": 1447,
      "training_loss": 6.898643493652344
    },
    {
      "epoch": 0.07848238482384824,
      "grad_norm": 23.83702278137207,
      "learning_rate": 1e-05,
      "loss": 6.5161,
      "step": 1448
    },
    {
      "epoch": 0.07848238482384824,
      "step": 1448,
      "training_loss": 6.566101551055908
    },
    {
      "epoch": 0.07853658536585366,
      "step": 1449,
      "training_loss": 6.311508655548096
    },
    {
      "epoch": 0.07859078590785908,
      "step": 1450,
      "training_loss": 7.895265579223633
    },
    {
      "epoch": 0.07864498644986449,
      "step": 1451,
      "training_loss": 7.614598274230957
    },
    {
      "epoch": 0.07869918699186992,
      "grad_norm": 17.752578735351562,
      "learning_rate": 1e-05,
      "loss": 7.0969,
      "step": 1452
    },
    {
      "epoch": 0.07869918699186992,
      "step": 1452,
      "training_loss": 6.970240592956543
    },
    {
      "epoch": 0.07875338753387534,
      "step": 1453,
      "training_loss": 8.099026679992676
    },
    {
      "epoch": 0.07880758807588076,
      "step": 1454,
      "training_loss": 7.569939136505127
    },
    {
      "epoch": 0.07886178861788617,
      "step": 1455,
      "training_loss": 7.156240463256836
    },
    {
      "epoch": 0.0789159891598916,
      "grad_norm": 18.15254783630371,
      "learning_rate": 1e-05,
      "loss": 7.4489,
      "step": 1456
    },
    {
      "epoch": 0.0789159891598916,
      "step": 1456,
      "training_loss": 7.655505180358887
    },
    {
      "epoch": 0.07897018970189702,
      "step": 1457,
      "training_loss": 6.60431432723999
    },
    {
      "epoch": 0.07902439024390244,
      "step": 1458,
      "training_loss": 7.953263282775879
    },
    {
      "epoch": 0.07907859078590786,
      "step": 1459,
      "training_loss": 7.432521820068359
    },
    {
      "epoch": 0.07913279132791327,
      "grad_norm": 21.70386505126953,
      "learning_rate": 1e-05,
      "loss": 7.4114,
      "step": 1460
    },
    {
      "epoch": 0.07913279132791327,
      "step": 1460,
      "training_loss": 6.045256614685059
    },
    {
      "epoch": 0.0791869918699187,
      "step": 1461,
      "training_loss": 6.47216796875
    },
    {
      "epoch": 0.07924119241192412,
      "step": 1462,
      "training_loss": 8.049202919006348
    },
    {
      "epoch": 0.07929539295392954,
      "step": 1463,
      "training_loss": 7.122693061828613
    },
    {
      "epoch": 0.07934959349593496,
      "grad_norm": 15.110715866088867,
      "learning_rate": 1e-05,
      "loss": 6.9223,
      "step": 1464
    },
    {
      "epoch": 0.07934959349593496,
      "step": 1464,
      "training_loss": 6.9110589027404785
    },
    {
      "epoch": 0.07940379403794037,
      "step": 1465,
      "training_loss": 6.566760063171387
    },
    {
      "epoch": 0.0794579945799458,
      "step": 1466,
      "training_loss": 5.219304084777832
    },
    {
      "epoch": 0.07951219512195122,
      "step": 1467,
      "training_loss": 7.674643516540527
    },
    {
      "epoch": 0.07956639566395664,
      "grad_norm": 13.845264434814453,
      "learning_rate": 1e-05,
      "loss": 6.5929,
      "step": 1468
    },
    {
      "epoch": 0.07956639566395664,
      "step": 1468,
      "training_loss": 7.560214519500732
    },
    {
      "epoch": 0.07962059620596205,
      "step": 1469,
      "training_loss": 5.237473964691162
    },
    {
      "epoch": 0.07967479674796749,
      "step": 1470,
      "training_loss": 6.023062705993652
    },
    {
      "epoch": 0.0797289972899729,
      "step": 1471,
      "training_loss": 7.002035140991211
    },
    {
      "epoch": 0.07978319783197832,
      "grad_norm": 20.4355411529541,
      "learning_rate": 1e-05,
      "loss": 6.4557,
      "step": 1472
    },
    {
      "epoch": 0.07978319783197832,
      "step": 1472,
      "training_loss": 6.8937907218933105
    },
    {
      "epoch": 0.07983739837398374,
      "step": 1473,
      "training_loss": 6.148158073425293
    },
    {
      "epoch": 0.07989159891598915,
      "step": 1474,
      "training_loss": 8.882543563842773
    },
    {
      "epoch": 0.07994579945799458,
      "step": 1475,
      "training_loss": 6.937169075012207
    },
    {
      "epoch": 0.08,
      "grad_norm": 25.23741912841797,
      "learning_rate": 1e-05,
      "loss": 7.2154,
      "step": 1476
    },
    {
      "epoch": 0.08,
      "step": 1476,
      "training_loss": 7.165998935699463
    },
    {
      "epoch": 0.08005420054200542,
      "step": 1477,
      "training_loss": 7.397009372711182
    },
    {
      "epoch": 0.08010840108401084,
      "step": 1478,
      "training_loss": 5.513299465179443
    },
    {
      "epoch": 0.08016260162601627,
      "step": 1479,
      "training_loss": 6.627427577972412
    },
    {
      "epoch": 0.08021680216802168,
      "grad_norm": 14.394490242004395,
      "learning_rate": 1e-05,
      "loss": 6.6759,
      "step": 1480
    },
    {
      "epoch": 0.08021680216802168,
      "step": 1480,
      "training_loss": 6.956590175628662
    },
    {
      "epoch": 0.0802710027100271,
      "step": 1481,
      "training_loss": 6.424236297607422
    },
    {
      "epoch": 0.08032520325203252,
      "step": 1482,
      "training_loss": 7.15387487411499
    },
    {
      "epoch": 0.08037940379403793,
      "step": 1483,
      "training_loss": 6.621458053588867
    },
    {
      "epoch": 0.08043360433604337,
      "grad_norm": 16.38645362854004,
      "learning_rate": 1e-05,
      "loss": 6.789,
      "step": 1484
    },
    {
      "epoch": 0.08043360433604337,
      "step": 1484,
      "training_loss": 6.7173333168029785
    },
    {
      "epoch": 0.08048780487804878,
      "step": 1485,
      "training_loss": 5.35543155670166
    },
    {
      "epoch": 0.0805420054200542,
      "step": 1486,
      "training_loss": 8.152166366577148
    },
    {
      "epoch": 0.08059620596205962,
      "step": 1487,
      "training_loss": 7.406446933746338
    },
    {
      "epoch": 0.08065040650406505,
      "grad_norm": 20.081880569458008,
      "learning_rate": 1e-05,
      "loss": 6.9078,
      "step": 1488
    },
    {
      "epoch": 0.08065040650406505,
      "step": 1488,
      "training_loss": 7.458789348602295
    },
    {
      "epoch": 0.08070460704607046,
      "step": 1489,
      "training_loss": 6.390273571014404
    },
    {
      "epoch": 0.08075880758807588,
      "step": 1490,
      "training_loss": 7.265676975250244
    },
    {
      "epoch": 0.0808130081300813,
      "step": 1491,
      "training_loss": 8.377300262451172
    },
    {
      "epoch": 0.08086720867208672,
      "grad_norm": 22.48006820678711,
      "learning_rate": 1e-05,
      "loss": 7.373,
      "step": 1492
    },
    {
      "epoch": 0.08086720867208672,
      "step": 1492,
      "training_loss": 6.276495933532715
    },
    {
      "epoch": 0.08092140921409215,
      "step": 1493,
      "training_loss": 6.643661975860596
    },
    {
      "epoch": 0.08097560975609756,
      "step": 1494,
      "training_loss": 6.761312484741211
    },
    {
      "epoch": 0.08102981029810298,
      "step": 1495,
      "training_loss": 7.584204196929932
    },
    {
      "epoch": 0.0810840108401084,
      "grad_norm": 15.860672950744629,
      "learning_rate": 1e-05,
      "loss": 6.8164,
      "step": 1496
    },
    {
      "epoch": 0.0810840108401084,
      "step": 1496,
      "training_loss": 5.024026393890381
    },
    {
      "epoch": 0.08113821138211381,
      "step": 1497,
      "training_loss": 7.907277584075928
    },
    {
      "epoch": 0.08119241192411925,
      "step": 1498,
      "training_loss": 6.9513092041015625
    },
    {
      "epoch": 0.08124661246612466,
      "step": 1499,
      "training_loss": 6.9853410720825195
    },
    {
      "epoch": 0.08130081300813008,
      "grad_norm": 17.394542694091797,
      "learning_rate": 1e-05,
      "loss": 6.717,
      "step": 1500
    },
    {
      "epoch": 0.08130081300813008,
      "step": 1500,
      "training_loss": 6.105336666107178
    },
    {
      "epoch": 0.0813550135501355,
      "step": 1501,
      "training_loss": 7.092627048492432
    },
    {
      "epoch": 0.08140921409214093,
      "step": 1502,
      "training_loss": 6.558999538421631
    },
    {
      "epoch": 0.08146341463414634,
      "step": 1503,
      "training_loss": 8.096537590026855
    },
    {
      "epoch": 0.08151761517615176,
      "grad_norm": 22.192474365234375,
      "learning_rate": 1e-05,
      "loss": 6.9634,
      "step": 1504
    },
    {
      "epoch": 0.08151761517615176,
      "step": 1504,
      "training_loss": 7.470880508422852
    },
    {
      "epoch": 0.08157181571815718,
      "step": 1505,
      "training_loss": 6.671574115753174
    },
    {
      "epoch": 0.0816260162601626,
      "step": 1506,
      "training_loss": 6.071920394897461
    },
    {
      "epoch": 0.08168021680216803,
      "step": 1507,
      "training_loss": 6.956241130828857
    },
    {
      "epoch": 0.08173441734417344,
      "grad_norm": 26.47064781188965,
      "learning_rate": 1e-05,
      "loss": 6.7927,
      "step": 1508
    },
    {
      "epoch": 0.08173441734417344,
      "step": 1508,
      "training_loss": 6.8162970542907715
    },
    {
      "epoch": 0.08178861788617886,
      "step": 1509,
      "training_loss": 6.921463966369629
    },
    {
      "epoch": 0.08184281842818428,
      "step": 1510,
      "training_loss": 7.125483512878418
    },
    {
      "epoch": 0.08189701897018971,
      "step": 1511,
      "training_loss": 5.455466270446777
    },
    {
      "epoch": 0.08195121951219513,
      "grad_norm": 18.126893997192383,
      "learning_rate": 1e-05,
      "loss": 6.5797,
      "step": 1512
    },
    {
      "epoch": 0.08195121951219513,
      "step": 1512,
      "training_loss": 6.609006404876709
    },
    {
      "epoch": 0.08200542005420054,
      "step": 1513,
      "training_loss": 7.985594749450684
    },
    {
      "epoch": 0.08205962059620596,
      "step": 1514,
      "training_loss": 6.592886447906494
    },
    {
      "epoch": 0.08211382113821138,
      "step": 1515,
      "training_loss": 6.069246768951416
    },
    {
      "epoch": 0.08216802168021681,
      "grad_norm": 21.564937591552734,
      "learning_rate": 1e-05,
      "loss": 6.8142,
      "step": 1516
    },
    {
      "epoch": 0.08216802168021681,
      "step": 1516,
      "training_loss": 7.090270042419434
    },
    {
      "epoch": 0.08222222222222222,
      "step": 1517,
      "training_loss": 7.298571586608887
    },
    {
      "epoch": 0.08227642276422764,
      "step": 1518,
      "training_loss": 6.740000247955322
    },
    {
      "epoch": 0.08233062330623306,
      "step": 1519,
      "training_loss": 11.398785591125488
    },
    {
      "epoch": 0.08238482384823849,
      "grad_norm": 37.81239318847656,
      "learning_rate": 1e-05,
      "loss": 8.1319,
      "step": 1520
    },
    {
      "epoch": 0.08238482384823849,
      "step": 1520,
      "training_loss": 7.549437999725342
    },
    {
      "epoch": 0.0824390243902439,
      "step": 1521,
      "training_loss": 7.608947277069092
    },
    {
      "epoch": 0.08249322493224932,
      "step": 1522,
      "training_loss": 6.870312690734863
    },
    {
      "epoch": 0.08254742547425474,
      "step": 1523,
      "training_loss": 6.6610517501831055
    },
    {
      "epoch": 0.08260162601626016,
      "grad_norm": 25.696657180786133,
      "learning_rate": 1e-05,
      "loss": 7.1724,
      "step": 1524
    },
    {
      "epoch": 0.08260162601626016,
      "step": 1524,
      "training_loss": 7.4949750900268555
    },
    {
      "epoch": 0.08265582655826559,
      "step": 1525,
      "training_loss": 5.225578308105469
    },
    {
      "epoch": 0.082710027100271,
      "step": 1526,
      "training_loss": 7.049113750457764
    },
    {
      "epoch": 0.08276422764227642,
      "step": 1527,
      "training_loss": 7.2169189453125
    },
    {
      "epoch": 0.08281842818428184,
      "grad_norm": 24.93592071533203,
      "learning_rate": 1e-05,
      "loss": 6.7466,
      "step": 1528
    },
    {
      "epoch": 0.08281842818428184,
      "step": 1528,
      "training_loss": 7.409938335418701
    },
    {
      "epoch": 0.08287262872628726,
      "step": 1529,
      "training_loss": 7.165013790130615
    },
    {
      "epoch": 0.08292682926829269,
      "step": 1530,
      "training_loss": 8.702951431274414
    },
    {
      "epoch": 0.0829810298102981,
      "step": 1531,
      "training_loss": 8.115078926086426
    },
    {
      "epoch": 0.08303523035230352,
      "grad_norm": 23.805255889892578,
      "learning_rate": 1e-05,
      "loss": 7.8482,
      "step": 1532
    },
    {
      "epoch": 0.08303523035230352,
      "step": 1532,
      "training_loss": 7.938722610473633
    },
    {
      "epoch": 0.08308943089430894,
      "step": 1533,
      "training_loss": 5.741585731506348
    },
    {
      "epoch": 0.08314363143631437,
      "step": 1534,
      "training_loss": 7.104894638061523
    },
    {
      "epoch": 0.08319783197831979,
      "step": 1535,
      "training_loss": 6.0118327140808105
    },
    {
      "epoch": 0.0832520325203252,
      "grad_norm": 14.243522644042969,
      "learning_rate": 1e-05,
      "loss": 6.6993,
      "step": 1536
    },
    {
      "epoch": 0.0832520325203252,
      "step": 1536,
      "training_loss": 7.262784481048584
    },
    {
      "epoch": 0.08330623306233062,
      "step": 1537,
      "training_loss": 6.9191131591796875
    },
    {
      "epoch": 0.08336043360433604,
      "step": 1538,
      "training_loss": 7.988018989562988
    },
    {
      "epoch": 0.08341463414634147,
      "step": 1539,
      "training_loss": 8.090574264526367
    },
    {
      "epoch": 0.08346883468834689,
      "grad_norm": 19.91983413696289,
      "learning_rate": 1e-05,
      "loss": 7.5651,
      "step": 1540
    },
    {
      "epoch": 0.08346883468834689,
      "step": 1540,
      "training_loss": 8.881203651428223
    },
    {
      "epoch": 0.0835230352303523,
      "step": 1541,
      "training_loss": 7.409794330596924
    },
    {
      "epoch": 0.08357723577235772,
      "step": 1542,
      "training_loss": 8.56857967376709
    },
    {
      "epoch": 0.08363143631436315,
      "step": 1543,
      "training_loss": 6.943595886230469
    },
    {
      "epoch": 0.08368563685636857,
      "grad_norm": 15.645379066467285,
      "learning_rate": 1e-05,
      "loss": 7.9508,
      "step": 1544
    },
    {
      "epoch": 0.08368563685636857,
      "step": 1544,
      "training_loss": 7.356025218963623
    },
    {
      "epoch": 0.08373983739837398,
      "step": 1545,
      "training_loss": 7.733541488647461
    },
    {
      "epoch": 0.0837940379403794,
      "step": 1546,
      "training_loss": 7.200157642364502
    },
    {
      "epoch": 0.08384823848238482,
      "step": 1547,
      "training_loss": 7.594922065734863
    },
    {
      "epoch": 0.08390243902439025,
      "grad_norm": 13.607351303100586,
      "learning_rate": 1e-05,
      "loss": 7.4712,
      "step": 1548
    },
    {
      "epoch": 0.08390243902439025,
      "step": 1548,
      "training_loss": 6.70325231552124
    },
    {
      "epoch": 0.08395663956639567,
      "step": 1549,
      "training_loss": 7.938178539276123
    },
    {
      "epoch": 0.08401084010840108,
      "step": 1550,
      "training_loss": 8.155025482177734
    },
    {
      "epoch": 0.0840650406504065,
      "step": 1551,
      "training_loss": 7.428589344024658
    },
    {
      "epoch": 0.08411924119241193,
      "grad_norm": 31.541887283325195,
      "learning_rate": 1e-05,
      "loss": 7.5563,
      "step": 1552
    },
    {
      "epoch": 0.08411924119241193,
      "step": 1552,
      "training_loss": 6.888820648193359
    },
    {
      "epoch": 0.08417344173441735,
      "step": 1553,
      "training_loss": 6.773348331451416
    },
    {
      "epoch": 0.08422764227642277,
      "step": 1554,
      "training_loss": 7.2355217933654785
    },
    {
      "epoch": 0.08428184281842818,
      "step": 1555,
      "training_loss": 8.344488143920898
    },
    {
      "epoch": 0.0843360433604336,
      "grad_norm": 23.43170928955078,
      "learning_rate": 1e-05,
      "loss": 7.3105,
      "step": 1556
    },
    {
      "epoch": 0.0843360433604336,
      "step": 1556,
      "training_loss": 7.104752540588379
    },
    {
      "epoch": 0.08439024390243903,
      "step": 1557,
      "training_loss": 5.187212944030762
    },
    {
      "epoch": 0.08444444444444445,
      "step": 1558,
      "training_loss": 7.240516185760498
    },
    {
      "epoch": 0.08449864498644986,
      "step": 1559,
      "training_loss": 7.27593994140625
    },
    {
      "epoch": 0.08455284552845528,
      "grad_norm": 18.487150192260742,
      "learning_rate": 1e-05,
      "loss": 6.7021,
      "step": 1560
    },
    {
      "epoch": 0.08455284552845528,
      "step": 1560,
      "training_loss": 5.9194769859313965
    },
    {
      "epoch": 0.0846070460704607,
      "step": 1561,
      "training_loss": 7.973294258117676
    },
    {
      "epoch": 0.08466124661246613,
      "step": 1562,
      "training_loss": 7.3256096839904785
    },
    {
      "epoch": 0.08471544715447155,
      "step": 1563,
      "training_loss": 7.686769485473633
    },
    {
      "epoch": 0.08476964769647696,
      "grad_norm": 28.198734283447266,
      "learning_rate": 1e-05,
      "loss": 7.2263,
      "step": 1564
    },
    {
      "epoch": 0.08476964769647696,
      "step": 1564,
      "training_loss": 7.542346954345703
    },
    {
      "epoch": 0.08482384823848238,
      "step": 1565,
      "training_loss": 6.45596981048584
    },
    {
      "epoch": 0.08487804878048781,
      "step": 1566,
      "training_loss": 6.859743595123291
    },
    {
      "epoch": 0.08493224932249323,
      "step": 1567,
      "training_loss": 5.823577404022217
    },
    {
      "epoch": 0.08498644986449864,
      "grad_norm": 15.307823181152344,
      "learning_rate": 1e-05,
      "loss": 6.6704,
      "step": 1568
    },
    {
      "epoch": 0.08498644986449864,
      "step": 1568,
      "training_loss": 7.583731174468994
    },
    {
      "epoch": 0.08504065040650406,
      "step": 1569,
      "training_loss": 7.5811767578125
    },
    {
      "epoch": 0.08509485094850948,
      "step": 1570,
      "training_loss": 6.795912265777588
    },
    {
      "epoch": 0.08514905149051491,
      "step": 1571,
      "training_loss": 7.405450344085693
    },
    {
      "epoch": 0.08520325203252033,
      "grad_norm": 26.650653839111328,
      "learning_rate": 1e-05,
      "loss": 7.3416,
      "step": 1572
    },
    {
      "epoch": 0.08520325203252033,
      "step": 1572,
      "training_loss": 6.925144672393799
    },
    {
      "epoch": 0.08525745257452574,
      "step": 1573,
      "training_loss": 7.117498874664307
    },
    {
      "epoch": 0.08531165311653116,
      "step": 1574,
      "training_loss": 7.433722972869873
    },
    {
      "epoch": 0.08536585365853659,
      "step": 1575,
      "training_loss": 6.889317035675049
    },
    {
      "epoch": 0.08542005420054201,
      "grad_norm": 18.049654006958008,
      "learning_rate": 1e-05,
      "loss": 7.0914,
      "step": 1576
    },
    {
      "epoch": 0.08542005420054201,
      "step": 1576,
      "training_loss": 6.523434162139893
    },
    {
      "epoch": 0.08547425474254743,
      "step": 1577,
      "training_loss": 6.53915548324585
    },
    {
      "epoch": 0.08552845528455284,
      "step": 1578,
      "training_loss": 7.580345630645752
    },
    {
      "epoch": 0.08558265582655826,
      "step": 1579,
      "training_loss": 7.635035991668701
    },
    {
      "epoch": 0.08563685636856369,
      "grad_norm": 21.523439407348633,
      "learning_rate": 1e-05,
      "loss": 7.0695,
      "step": 1580
    },
    {
      "epoch": 0.08563685636856369,
      "step": 1580,
      "training_loss": 6.668606281280518
    },
    {
      "epoch": 0.08569105691056911,
      "step": 1581,
      "training_loss": 6.72382116317749
    },
    {
      "epoch": 0.08574525745257452,
      "step": 1582,
      "training_loss": 7.158984184265137
    },
    {
      "epoch": 0.08579945799457994,
      "step": 1583,
      "training_loss": 7.330589771270752
    },
    {
      "epoch": 0.08585365853658537,
      "grad_norm": 17.996816635131836,
      "learning_rate": 1e-05,
      "loss": 6.9705,
      "step": 1584
    },
    {
      "epoch": 0.08585365853658537,
      "step": 1584,
      "training_loss": 6.535168170928955
    },
    {
      "epoch": 0.08590785907859079,
      "step": 1585,
      "training_loss": 7.677383899688721
    },
    {
      "epoch": 0.0859620596205962,
      "step": 1586,
      "training_loss": 6.562088966369629
    },
    {
      "epoch": 0.08601626016260162,
      "step": 1587,
      "training_loss": 6.779745578765869
    },
    {
      "epoch": 0.08607046070460704,
      "grad_norm": 21.74555015563965,
      "learning_rate": 1e-05,
      "loss": 6.8886,
      "step": 1588
    },
    {
      "epoch": 0.08607046070460704,
      "step": 1588,
      "training_loss": 8.241918563842773
    },
    {
      "epoch": 0.08612466124661247,
      "step": 1589,
      "training_loss": 6.6531901359558105
    },
    {
      "epoch": 0.08617886178861789,
      "step": 1590,
      "training_loss": 7.717720985412598
    },
    {
      "epoch": 0.0862330623306233,
      "step": 1591,
      "training_loss": 6.020966053009033
    },
    {
      "epoch": 0.08628726287262872,
      "grad_norm": 16.044780731201172,
      "learning_rate": 1e-05,
      "loss": 7.1584,
      "step": 1592
    },
    {
      "epoch": 0.08628726287262872,
      "step": 1592,
      "training_loss": 9.817872047424316
    },
    {
      "epoch": 0.08634146341463414,
      "step": 1593,
      "training_loss": 7.6298723220825195
    },
    {
      "epoch": 0.08639566395663957,
      "step": 1594,
      "training_loss": 6.75858211517334
    },
    {
      "epoch": 0.08644986449864499,
      "step": 1595,
      "training_loss": 9.481396675109863
    },
    {
      "epoch": 0.0865040650406504,
      "grad_norm": 33.85790252685547,
      "learning_rate": 1e-05,
      "loss": 8.4219,
      "step": 1596
    },
    {
      "epoch": 0.0865040650406504,
      "step": 1596,
      "training_loss": 7.313276767730713
    },
    {
      "epoch": 0.08655826558265582,
      "step": 1597,
      "training_loss": 7.281013011932373
    },
    {
      "epoch": 0.08661246612466125,
      "step": 1598,
      "training_loss": 7.151163101196289
    },
    {
      "epoch": 0.08666666666666667,
      "step": 1599,
      "training_loss": 6.7644219398498535
    },
    {
      "epoch": 0.08672086720867209,
      "grad_norm": 17.21185302734375,
      "learning_rate": 1e-05,
      "loss": 7.1275,
      "step": 1600
    },
    {
      "epoch": 0.08672086720867209,
      "step": 1600,
      "training_loss": 7.123671531677246
    },
    {
      "epoch": 0.0867750677506775,
      "step": 1601,
      "training_loss": 6.964865207672119
    },
    {
      "epoch": 0.08682926829268292,
      "step": 1602,
      "training_loss": 5.422028064727783
    },
    {
      "epoch": 0.08688346883468835,
      "step": 1603,
      "training_loss": 5.575869560241699
    },
    {
      "epoch": 0.08693766937669377,
      "grad_norm": 36.85334396362305,
      "learning_rate": 1e-05,
      "loss": 6.2716,
      "step": 1604
    },
    {
      "epoch": 0.08693766937669377,
      "step": 1604,
      "training_loss": 7.80880880355835
    },
    {
      "epoch": 0.08699186991869919,
      "step": 1605,
      "training_loss": 7.339385509490967
    },
    {
      "epoch": 0.0870460704607046,
      "step": 1606,
      "training_loss": 7.033257007598877
    },
    {
      "epoch": 0.08710027100271003,
      "step": 1607,
      "training_loss": 7.587625503540039
    },
    {
      "epoch": 0.08715447154471545,
      "grad_norm": 26.551273345947266,
      "learning_rate": 1e-05,
      "loss": 7.4423,
      "step": 1608
    },
    {
      "epoch": 0.08715447154471545,
      "step": 1608,
      "training_loss": 6.939778804779053
    },
    {
      "epoch": 0.08720867208672087,
      "step": 1609,
      "training_loss": 5.784375190734863
    },
    {
      "epoch": 0.08726287262872628,
      "step": 1610,
      "training_loss": 6.988247394561768
    },
    {
      "epoch": 0.0873170731707317,
      "step": 1611,
      "training_loss": 9.202912330627441
    },
    {
      "epoch": 0.08737127371273713,
      "grad_norm": 47.047611236572266,
      "learning_rate": 1e-05,
      "loss": 7.2288,
      "step": 1612
    },
    {
      "epoch": 0.08737127371273713,
      "step": 1612,
      "training_loss": 6.002492904663086
    },
    {
      "epoch": 0.08742547425474255,
      "step": 1613,
      "training_loss": 6.229701519012451
    },
    {
      "epoch": 0.08747967479674797,
      "step": 1614,
      "training_loss": 7.681308269500732
    },
    {
      "epoch": 0.08753387533875338,
      "step": 1615,
      "training_loss": 6.27416467666626
    },
    {
      "epoch": 0.08758807588075881,
      "grad_norm": 22.022615432739258,
      "learning_rate": 1e-05,
      "loss": 6.5469,
      "step": 1616
    },
    {
      "epoch": 0.08758807588075881,
      "step": 1616,
      "training_loss": 5.585240840911865
    },
    {
      "epoch": 0.08764227642276423,
      "step": 1617,
      "training_loss": 7.478819370269775
    },
    {
      "epoch": 0.08769647696476965,
      "step": 1618,
      "training_loss": 7.32618522644043
    },
    {
      "epoch": 0.08775067750677507,
      "step": 1619,
      "training_loss": 6.169351577758789
    },
    {
      "epoch": 0.08780487804878048,
      "grad_norm": 14.736547470092773,
      "learning_rate": 1e-05,
      "loss": 6.6399,
      "step": 1620
    },
    {
      "epoch": 0.08780487804878048,
      "step": 1620,
      "training_loss": 7.288869380950928
    },
    {
      "epoch": 0.08785907859078591,
      "step": 1621,
      "training_loss": 4.688216209411621
    },
    {
      "epoch": 0.08791327913279133,
      "step": 1622,
      "training_loss": 5.676927089691162
    },
    {
      "epoch": 0.08796747967479675,
      "step": 1623,
      "training_loss": 6.484979629516602
    },
    {
      "epoch": 0.08802168021680216,
      "grad_norm": 15.032989501953125,
      "learning_rate": 1e-05,
      "loss": 6.0347,
      "step": 1624
    },
    {
      "epoch": 0.08802168021680216,
      "step": 1624,
      "training_loss": 6.748158931732178
    },
    {
      "epoch": 0.08807588075880758,
      "step": 1625,
      "training_loss": 8.676514625549316
    },
    {
      "epoch": 0.08813008130081301,
      "step": 1626,
      "training_loss": 6.736316680908203
    },
    {
      "epoch": 0.08818428184281843,
      "step": 1627,
      "training_loss": 8.372291564941406
    },
    {
      "epoch": 0.08823848238482385,
      "grad_norm": 22.17508888244629,
      "learning_rate": 1e-05,
      "loss": 7.6333,
      "step": 1628
    },
    {
      "epoch": 0.08823848238482385,
      "step": 1628,
      "training_loss": 7.854903697967529
    },
    {
      "epoch": 0.08829268292682926,
      "step": 1629,
      "training_loss": 8.114118576049805
    },
    {
      "epoch": 0.0883468834688347,
      "step": 1630,
      "training_loss": 7.274725914001465
    },
    {
      "epoch": 0.08840108401084011,
      "step": 1631,
      "training_loss": 7.442898273468018
    },
    {
      "epoch": 0.08845528455284553,
      "grad_norm": 20.545764923095703,
      "learning_rate": 1e-05,
      "loss": 7.6717,
      "step": 1632
    },
    {
      "epoch": 0.08845528455284553,
      "step": 1632,
      "training_loss": 5.580777168273926
    },
    {
      "epoch": 0.08850948509485095,
      "step": 1633,
      "training_loss": 8.175048828125
    },
    {
      "epoch": 0.08856368563685636,
      "step": 1634,
      "training_loss": 6.947646617889404
    },
    {
      "epoch": 0.0886178861788618,
      "step": 1635,
      "training_loss": 6.21968936920166
    },
    {
      "epoch": 0.08867208672086721,
      "grad_norm": 21.360780715942383,
      "learning_rate": 1e-05,
      "loss": 6.7308,
      "step": 1636
    },
    {
      "epoch": 0.08867208672086721,
      "step": 1636,
      "training_loss": 7.093921184539795
    },
    {
      "epoch": 0.08872628726287263,
      "step": 1637,
      "training_loss": 7.506170749664307
    },
    {
      "epoch": 0.08878048780487804,
      "step": 1638,
      "training_loss": 7.138119220733643
    },
    {
      "epoch": 0.08883468834688348,
      "step": 1639,
      "training_loss": 8.075820922851562
    },
    {
      "epoch": 0.08888888888888889,
      "grad_norm": 15.183390617370605,
      "learning_rate": 1e-05,
      "loss": 7.4535,
      "step": 1640
    },
    {
      "epoch": 0.08888888888888889,
      "step": 1640,
      "training_loss": 6.718434810638428
    },
    {
      "epoch": 0.08894308943089431,
      "step": 1641,
      "training_loss": 6.749777317047119
    },
    {
      "epoch": 0.08899728997289973,
      "step": 1642,
      "training_loss": 7.979598522186279
    },
    {
      "epoch": 0.08905149051490514,
      "step": 1643,
      "training_loss": 6.925064563751221
    },
    {
      "epoch": 0.08910569105691057,
      "grad_norm": 39.536964416503906,
      "learning_rate": 1e-05,
      "loss": 7.0932,
      "step": 1644
    },
    {
      "epoch": 0.08910569105691057,
      "step": 1644,
      "training_loss": 7.009535789489746
    },
    {
      "epoch": 0.08915989159891599,
      "step": 1645,
      "training_loss": 6.237033367156982
    },
    {
      "epoch": 0.08921409214092141,
      "step": 1646,
      "training_loss": 5.891733169555664
    },
    {
      "epoch": 0.08926829268292683,
      "step": 1647,
      "training_loss": 6.250555038452148
    },
    {
      "epoch": 0.08932249322493226,
      "grad_norm": 29.263757705688477,
      "learning_rate": 1e-05,
      "loss": 6.3472,
      "step": 1648
    },
    {
      "epoch": 0.08932249322493226,
      "step": 1648,
      "training_loss": 6.953632354736328
    },
    {
      "epoch": 0.08937669376693767,
      "step": 1649,
      "training_loss": 8.989429473876953
    },
    {
      "epoch": 0.08943089430894309,
      "step": 1650,
      "training_loss": 4.6601457595825195
    },
    {
      "epoch": 0.08948509485094851,
      "step": 1651,
      "training_loss": 7.617650032043457
    },
    {
      "epoch": 0.08953929539295392,
      "grad_norm": 20.728431701660156,
      "learning_rate": 1e-05,
      "loss": 7.0552,
      "step": 1652
    },
    {
      "epoch": 0.08953929539295392,
      "step": 1652,
      "training_loss": 6.447307586669922
    },
    {
      "epoch": 0.08959349593495936,
      "step": 1653,
      "training_loss": 7.548569202423096
    },
    {
      "epoch": 0.08964769647696477,
      "step": 1654,
      "training_loss": 7.182150840759277
    },
    {
      "epoch": 0.08970189701897019,
      "step": 1655,
      "training_loss": 8.147107124328613
    },
    {
      "epoch": 0.0897560975609756,
      "grad_norm": 17.337038040161133,
      "learning_rate": 1e-05,
      "loss": 7.3313,
      "step": 1656
    },
    {
      "epoch": 0.0897560975609756,
      "step": 1656,
      "training_loss": 7.160848140716553
    },
    {
      "epoch": 0.08981029810298102,
      "step": 1657,
      "training_loss": 7.219114780426025
    },
    {
      "epoch": 0.08986449864498645,
      "step": 1658,
      "training_loss": 7.852097034454346
    },
    {
      "epoch": 0.08991869918699187,
      "step": 1659,
      "training_loss": 7.2736616134643555
    },
    {
      "epoch": 0.08997289972899729,
      "grad_norm": 15.549280166625977,
      "learning_rate": 1e-05,
      "loss": 7.3764,
      "step": 1660
    },
    {
      "epoch": 0.08997289972899729,
      "step": 1660,
      "training_loss": 8.21684741973877
    },
    {
      "epoch": 0.0900271002710027,
      "step": 1661,
      "training_loss": 6.508275508880615
    },
    {
      "epoch": 0.09008130081300814,
      "step": 1662,
      "training_loss": 6.70107364654541
    },
    {
      "epoch": 0.09013550135501355,
      "step": 1663,
      "training_loss": 7.71552848815918
    },
    {
      "epoch": 0.09018970189701897,
      "grad_norm": 16.131174087524414,
      "learning_rate": 1e-05,
      "loss": 7.2854,
      "step": 1664
    },
    {
      "epoch": 0.09018970189701897,
      "step": 1664,
      "training_loss": 7.004007816314697
    },
    {
      "epoch": 0.09024390243902439,
      "step": 1665,
      "training_loss": 6.639325141906738
    },
    {
      "epoch": 0.0902981029810298,
      "step": 1666,
      "training_loss": 6.506496906280518
    },
    {
      "epoch": 0.09035230352303524,
      "step": 1667,
      "training_loss": 6.646972179412842
    },
    {
      "epoch": 0.09040650406504065,
      "grad_norm": 21.314233779907227,
      "learning_rate": 1e-05,
      "loss": 6.6992,
      "step": 1668
    },
    {
      "epoch": 0.09040650406504065,
      "step": 1668,
      "training_loss": 6.48427677154541
    },
    {
      "epoch": 0.09046070460704607,
      "step": 1669,
      "training_loss": 6.787038326263428
    },
    {
      "epoch": 0.09051490514905149,
      "step": 1670,
      "training_loss": 6.616793632507324
    },
    {
      "epoch": 0.09056910569105692,
      "step": 1671,
      "training_loss": 6.998480796813965
    },
    {
      "epoch": 0.09062330623306233,
      "grad_norm": 24.879770278930664,
      "learning_rate": 1e-05,
      "loss": 6.7216,
      "step": 1672
    },
    {
      "epoch": 0.09062330623306233,
      "step": 1672,
      "training_loss": 6.941429615020752
    },
    {
      "epoch": 0.09067750677506775,
      "step": 1673,
      "training_loss": 5.861907958984375
    },
    {
      "epoch": 0.09073170731707317,
      "step": 1674,
      "training_loss": 6.384670257568359
    },
    {
      "epoch": 0.09078590785907859,
      "step": 1675,
      "training_loss": 6.893835544586182
    },
    {
      "epoch": 0.09084010840108402,
      "grad_norm": 16.238502502441406,
      "learning_rate": 1e-05,
      "loss": 6.5205,
      "step": 1676
    },
    {
      "epoch": 0.09084010840108402,
      "step": 1676,
      "training_loss": 8.070765495300293
    },
    {
      "epoch": 0.09089430894308943,
      "step": 1677,
      "training_loss": 6.743760108947754
    },
    {
      "epoch": 0.09094850948509485,
      "step": 1678,
      "training_loss": 6.405947208404541
    },
    {
      "epoch": 0.09100271002710027,
      "step": 1679,
      "training_loss": 8.092854499816895
    },
    {
      "epoch": 0.0910569105691057,
      "grad_norm": 17.252174377441406,
      "learning_rate": 1e-05,
      "loss": 7.3283,
      "step": 1680
    },
    {
      "epoch": 0.0910569105691057,
      "step": 1680,
      "training_loss": 7.3212080001831055
    },
    {
      "epoch": 0.09111111111111111,
      "step": 1681,
      "training_loss": 8.87846565246582
    },
    {
      "epoch": 0.09116531165311653,
      "step": 1682,
      "training_loss": 7.089105129241943
    },
    {
      "epoch": 0.09121951219512195,
      "step": 1683,
      "training_loss": 6.897226810455322
    },
    {
      "epoch": 0.09127371273712737,
      "grad_norm": 22.430179595947266,
      "learning_rate": 1e-05,
      "loss": 7.5465,
      "step": 1684
    },
    {
      "epoch": 0.09127371273712737,
      "step": 1684,
      "training_loss": 6.415831089019775
    },
    {
      "epoch": 0.0913279132791328,
      "step": 1685,
      "training_loss": 6.781657695770264
    },
    {
      "epoch": 0.09138211382113821,
      "step": 1686,
      "training_loss": 5.891106128692627
    },
    {
      "epoch": 0.09143631436314363,
      "step": 1687,
      "training_loss": 5.012064456939697
    },
    {
      "epoch": 0.09149051490514905,
      "grad_norm": 19.26152801513672,
      "learning_rate": 1e-05,
      "loss": 6.0252,
      "step": 1688
    },
    {
      "epoch": 0.09149051490514905,
      "step": 1688,
      "training_loss": 8.217626571655273
    },
    {
      "epoch": 0.09154471544715446,
      "step": 1689,
      "training_loss": 7.038978099822998
    },
    {
      "epoch": 0.0915989159891599,
      "step": 1690,
      "training_loss": 6.29246711730957
    },
    {
      "epoch": 0.09165311653116531,
      "step": 1691,
      "training_loss": 7.3843865394592285
    },
    {
      "epoch": 0.09170731707317073,
      "grad_norm": 15.88426399230957,
      "learning_rate": 1e-05,
      "loss": 7.2334,
      "step": 1692
    },
    {
      "epoch": 0.09170731707317073,
      "step": 1692,
      "training_loss": 7.540465354919434
    },
    {
      "epoch": 0.09176151761517615,
      "step": 1693,
      "training_loss": 7.558072090148926
    },
    {
      "epoch": 0.09181571815718158,
      "step": 1694,
      "training_loss": 7.320772647857666
    },
    {
      "epoch": 0.091869918699187,
      "step": 1695,
      "training_loss": 7.443187236785889
    },
    {
      "epoch": 0.09192411924119241,
      "grad_norm": 17.922340393066406,
      "learning_rate": 1e-05,
      "loss": 7.4656,
      "step": 1696
    },
    {
      "epoch": 0.09192411924119241,
      "step": 1696,
      "training_loss": 7.122110366821289
    },
    {
      "epoch": 0.09197831978319783,
      "step": 1697,
      "training_loss": 5.091551303863525
    },
    {
      "epoch": 0.09203252032520325,
      "step": 1698,
      "training_loss": 6.835752487182617
    },
    {
      "epoch": 0.09208672086720868,
      "step": 1699,
      "training_loss": 6.449139595031738
    },
    {
      "epoch": 0.0921409214092141,
      "grad_norm": 15.705408096313477,
      "learning_rate": 1e-05,
      "loss": 6.3746,
      "step": 1700
    },
    {
      "epoch": 0.0921409214092141,
      "step": 1700,
      "training_loss": 9.28454875946045
    },
    {
      "epoch": 0.09219512195121951,
      "step": 1701,
      "training_loss": 6.343812465667725
    },
    {
      "epoch": 0.09224932249322493,
      "step": 1702,
      "training_loss": 4.7976813316345215
    },
    {
      "epoch": 0.09230352303523036,
      "step": 1703,
      "training_loss": 7.948275566101074
    },
    {
      "epoch": 0.09235772357723578,
      "grad_norm": 23.639522552490234,
      "learning_rate": 1e-05,
      "loss": 7.0936,
      "step": 1704
    },
    {
      "epoch": 0.09235772357723578,
      "step": 1704,
      "training_loss": 7.589580059051514
    },
    {
      "epoch": 0.09241192411924119,
      "step": 1705,
      "training_loss": 5.874301910400391
    },
    {
      "epoch": 0.09246612466124661,
      "step": 1706,
      "training_loss": 6.815415859222412
    },
    {
      "epoch": 0.09252032520325203,
      "step": 1707,
      "training_loss": 7.598508834838867
    },
    {
      "epoch": 0.09257452574525746,
      "grad_norm": 16.76819610595703,
      "learning_rate": 1e-05,
      "loss": 6.9695,
      "step": 1708
    },
    {
      "epoch": 0.09257452574525746,
      "step": 1708,
      "training_loss": 6.924911975860596
    },
    {
      "epoch": 0.09262872628726287,
      "step": 1709,
      "training_loss": 7.427004337310791
    },
    {
      "epoch": 0.09268292682926829,
      "step": 1710,
      "training_loss": 7.596051216125488
    },
    {
      "epoch": 0.09273712737127371,
      "step": 1711,
      "training_loss": 7.249311447143555
    },
    {
      "epoch": 0.09279132791327914,
      "grad_norm": 36.00410461425781,
      "learning_rate": 1e-05,
      "loss": 7.2993,
      "step": 1712
    },
    {
      "epoch": 0.09279132791327914,
      "step": 1712,
      "training_loss": 7.356167316436768
    },
    {
      "epoch": 0.09284552845528456,
      "step": 1713,
      "training_loss": 5.279910087585449
    },
    {
      "epoch": 0.09289972899728997,
      "step": 1714,
      "training_loss": 7.40649938583374
    },
    {
      "epoch": 0.09295392953929539,
      "step": 1715,
      "training_loss": 7.539501667022705
    },
    {
      "epoch": 0.09300813008130081,
      "grad_norm": 22.652135848999023,
      "learning_rate": 1e-05,
      "loss": 6.8955,
      "step": 1716
    },
    {
      "epoch": 0.09300813008130081,
      "step": 1716,
      "training_loss": 8.626862525939941
    },
    {
      "epoch": 0.09306233062330624,
      "step": 1717,
      "training_loss": 7.179759979248047
    },
    {
      "epoch": 0.09311653116531166,
      "step": 1718,
      "training_loss": 7.615808010101318
    },
    {
      "epoch": 0.09317073170731707,
      "step": 1719,
      "training_loss": 11.11100959777832
    },
    {
      "epoch": 0.09322493224932249,
      "grad_norm": 52.94569778442383,
      "learning_rate": 1e-05,
      "loss": 8.6334,
      "step": 1720
    },
    {
      "epoch": 0.09322493224932249,
      "step": 1720,
      "training_loss": 7.584996223449707
    },
    {
      "epoch": 0.0932791327913279,
      "step": 1721,
      "training_loss": 7.91416597366333
    },
    {
      "epoch": 0.09333333333333334,
      "step": 1722,
      "training_loss": 6.997749328613281
    },
    {
      "epoch": 0.09338753387533875,
      "step": 1723,
      "training_loss": 7.072558879852295
    },
    {
      "epoch": 0.09344173441734417,
      "grad_norm": 23.074668884277344,
      "learning_rate": 1e-05,
      "loss": 7.3924,
      "step": 1724
    },
    {
      "epoch": 0.09344173441734417,
      "step": 1724,
      "training_loss": 7.791093826293945
    },
    {
      "epoch": 0.09349593495934959,
      "step": 1725,
      "training_loss": 9.33739948272705
    },
    {
      "epoch": 0.09355013550135502,
      "step": 1726,
      "training_loss": 8.524396896362305
    },
    {
      "epoch": 0.09360433604336044,
      "step": 1727,
      "training_loss": 7.42374849319458
    },
    {
      "epoch": 0.09365853658536585,
      "grad_norm": 13.249479293823242,
      "learning_rate": 1e-05,
      "loss": 8.2692,
      "step": 1728
    },
    {
      "epoch": 0.09365853658536585,
      "step": 1728,
      "training_loss": 9.12932300567627
    },
    {
      "epoch": 0.09371273712737127,
      "step": 1729,
      "training_loss": 7.320218086242676
    },
    {
      "epoch": 0.09376693766937669,
      "step": 1730,
      "training_loss": 6.8992018699646
    },
    {
      "epoch": 0.09382113821138212,
      "step": 1731,
      "training_loss": 7.438861846923828
    },
    {
      "epoch": 0.09387533875338754,
      "grad_norm": 24.36119270324707,
      "learning_rate": 1e-05,
      "loss": 7.6969,
      "step": 1732
    },
    {
      "epoch": 0.09387533875338754,
      "step": 1732,
      "training_loss": 6.066250801086426
    },
    {
      "epoch": 0.09392953929539295,
      "step": 1733,
      "training_loss": 8.088546752929688
    },
    {
      "epoch": 0.09398373983739837,
      "step": 1734,
      "training_loss": 7.038322448730469
    },
    {
      "epoch": 0.0940379403794038,
      "step": 1735,
      "training_loss": 7.741424560546875
    },
    {
      "epoch": 0.09409214092140922,
      "grad_norm": 17.9685001373291,
      "learning_rate": 1e-05,
      "loss": 7.2336,
      "step": 1736
    },
    {
      "epoch": 0.09409214092140922,
      "step": 1736,
      "training_loss": 6.628321647644043
    },
    {
      "epoch": 0.09414634146341463,
      "step": 1737,
      "training_loss": 7.795683860778809
    },
    {
      "epoch": 0.09420054200542005,
      "step": 1738,
      "training_loss": 6.239549160003662
    },
    {
      "epoch": 0.09425474254742547,
      "step": 1739,
      "training_loss": 5.0602288246154785
    },
    {
      "epoch": 0.0943089430894309,
      "grad_norm": 17.41661262512207,
      "learning_rate": 1e-05,
      "loss": 6.4309,
      "step": 1740
    },
    {
      "epoch": 0.0943089430894309,
      "step": 1740,
      "training_loss": 8.33154582977295
    },
    {
      "epoch": 0.09436314363143632,
      "step": 1741,
      "training_loss": 7.251248359680176
    },
    {
      "epoch": 0.09441734417344173,
      "step": 1742,
      "training_loss": 7.42059850692749
    },
    {
      "epoch": 0.09447154471544715,
      "step": 1743,
      "training_loss": 6.009142875671387
    },
    {
      "epoch": 0.09452574525745258,
      "grad_norm": 24.224031448364258,
      "learning_rate": 1e-05,
      "loss": 7.2531,
      "step": 1744
    },
    {
      "epoch": 0.09452574525745258,
      "step": 1744,
      "training_loss": 8.310248374938965
    },
    {
      "epoch": 0.094579945799458,
      "step": 1745,
      "training_loss": 7.091411590576172
    },
    {
      "epoch": 0.09463414634146342,
      "step": 1746,
      "training_loss": 5.931295871734619
    },
    {
      "epoch": 0.09468834688346883,
      "step": 1747,
      "training_loss": 6.3198466300964355
    },
    {
      "epoch": 0.09474254742547425,
      "grad_norm": 12.753416061401367,
      "learning_rate": 1e-05,
      "loss": 6.9132,
      "step": 1748
    },
    {
      "epoch": 0.09474254742547425,
      "step": 1748,
      "training_loss": 7.669896125793457
    },
    {
      "epoch": 0.09479674796747968,
      "step": 1749,
      "training_loss": 7.236912250518799
    },
    {
      "epoch": 0.0948509485094851,
      "step": 1750,
      "training_loss": 4.905777454376221
    },
    {
      "epoch": 0.09490514905149051,
      "step": 1751,
      "training_loss": 6.838205814361572
    },
    {
      "epoch": 0.09495934959349593,
      "grad_norm": 17.959123611450195,
      "learning_rate": 1e-05,
      "loss": 6.6627,
      "step": 1752
    },
    {
      "epoch": 0.09495934959349593,
      "step": 1752,
      "training_loss": 6.276697635650635
    },
    {
      "epoch": 0.09501355013550135,
      "step": 1753,
      "training_loss": 6.445558071136475
    },
    {
      "epoch": 0.09506775067750678,
      "step": 1754,
      "training_loss": 7.154428005218506
    },
    {
      "epoch": 0.0951219512195122,
      "step": 1755,
      "training_loss": 5.825795650482178
    },
    {
      "epoch": 0.09517615176151761,
      "grad_norm": 21.541080474853516,
      "learning_rate": 1e-05,
      "loss": 6.4256,
      "step": 1756
    },
    {
      "epoch": 0.09517615176151761,
      "step": 1756,
      "training_loss": 7.253842353820801
    },
    {
      "epoch": 0.09523035230352303,
      "step": 1757,
      "training_loss": 7.106935024261475
    },
    {
      "epoch": 0.09528455284552846,
      "step": 1758,
      "training_loss": 5.761862277984619
    },
    {
      "epoch": 0.09533875338753388,
      "step": 1759,
      "training_loss": 6.92268180847168
    },
    {
      "epoch": 0.0953929539295393,
      "grad_norm": 16.41026496887207,
      "learning_rate": 1e-05,
      "loss": 6.7613,
      "step": 1760
    },
    {
      "epoch": 0.0953929539295393,
      "step": 1760,
      "training_loss": 6.975717067718506
    },
    {
      "epoch": 0.09544715447154471,
      "step": 1761,
      "training_loss": 6.2652740478515625
    },
    {
      "epoch": 0.09550135501355013,
      "step": 1762,
      "training_loss": 7.610599994659424
    },
    {
      "epoch": 0.09555555555555556,
      "step": 1763,
      "training_loss": 7.090829372406006
    },
    {
      "epoch": 0.09560975609756098,
      "grad_norm": 16.02631378173828,
      "learning_rate": 1e-05,
      "loss": 6.9856,
      "step": 1764
    },
    {
      "epoch": 0.09560975609756098,
      "step": 1764,
      "training_loss": 7.206759929656982
    },
    {
      "epoch": 0.0956639566395664,
      "step": 1765,
      "training_loss": 7.0517048835754395
    },
    {
      "epoch": 0.09571815718157181,
      "step": 1766,
      "training_loss": 7.74597692489624
    },
    {
      "epoch": 0.09577235772357724,
      "step": 1767,
      "training_loss": 6.3255510330200195
    },
    {
      "epoch": 0.09582655826558266,
      "grad_norm": 23.97528648376465,
      "learning_rate": 1e-05,
      "loss": 7.0825,
      "step": 1768
    },
    {
      "epoch": 0.09582655826558266,
      "step": 1768,
      "training_loss": 8.020119667053223
    },
    {
      "epoch": 0.09588075880758808,
      "step": 1769,
      "training_loss": 7.313936710357666
    },
    {
      "epoch": 0.0959349593495935,
      "step": 1770,
      "training_loss": 6.772221088409424
    },
    {
      "epoch": 0.09598915989159891,
      "step": 1771,
      "training_loss": 7.0258026123046875
    },
    {
      "epoch": 0.09604336043360434,
      "grad_norm": 17.637819290161133,
      "learning_rate": 1e-05,
      "loss": 7.283,
      "step": 1772
    },
    {
      "epoch": 0.09604336043360434,
      "step": 1772,
      "training_loss": 7.6663994789123535
    },
    {
      "epoch": 0.09609756097560976,
      "step": 1773,
      "training_loss": 7.6933112144470215
    },
    {
      "epoch": 0.09615176151761518,
      "step": 1774,
      "training_loss": 6.320790767669678
    },
    {
      "epoch": 0.09620596205962059,
      "step": 1775,
      "training_loss": 8.899639129638672
    },
    {
      "epoch": 0.09626016260162602,
      "grad_norm": 20.46413230895996,
      "learning_rate": 1e-05,
      "loss": 7.645,
      "step": 1776
    },
    {
      "epoch": 0.09626016260162602,
      "step": 1776,
      "training_loss": 7.159790515899658
    },
    {
      "epoch": 0.09631436314363144,
      "step": 1777,
      "training_loss": 7.281558990478516
    },
    {
      "epoch": 0.09636856368563686,
      "step": 1778,
      "training_loss": 6.548126697540283
    },
    {
      "epoch": 0.09642276422764227,
      "step": 1779,
      "training_loss": 7.986053466796875
    },
    {
      "epoch": 0.09647696476964769,
      "grad_norm": 24.946428298950195,
      "learning_rate": 1e-05,
      "loss": 7.2439,
      "step": 1780
    },
    {
      "epoch": 0.09647696476964769,
      "step": 1780,
      "training_loss": 7.789584636688232
    },
    {
      "epoch": 0.09653116531165312,
      "step": 1781,
      "training_loss": 7.309199810028076
    },
    {
      "epoch": 0.09658536585365854,
      "step": 1782,
      "training_loss": 7.847604274749756
    },
    {
      "epoch": 0.09663956639566396,
      "step": 1783,
      "training_loss": 5.337222576141357
    },
    {
      "epoch": 0.09669376693766937,
      "grad_norm": 17.567522048950195,
      "learning_rate": 1e-05,
      "loss": 7.0709,
      "step": 1784
    },
    {
      "epoch": 0.09669376693766937,
      "step": 1784,
      "training_loss": 7.6073126792907715
    },
    {
      "epoch": 0.09674796747967479,
      "step": 1785,
      "training_loss": 7.344090938568115
    },
    {
      "epoch": 0.09680216802168022,
      "step": 1786,
      "training_loss": 7.879390716552734
    },
    {
      "epoch": 0.09685636856368564,
      "step": 1787,
      "training_loss": 6.820295810699463
    },
    {
      "epoch": 0.09691056910569106,
      "grad_norm": 11.551276206970215,
      "learning_rate": 1e-05,
      "loss": 7.4128,
      "step": 1788
    },
    {
      "epoch": 0.09691056910569106,
      "step": 1788,
      "training_loss": 7.3769731521606445
    },
    {
      "epoch": 0.09696476964769647,
      "step": 1789,
      "training_loss": 6.810166358947754
    },
    {
      "epoch": 0.0970189701897019,
      "step": 1790,
      "training_loss": 6.943021297454834
    },
    {
      "epoch": 0.09707317073170732,
      "step": 1791,
      "training_loss": 6.193000316619873
    },
    {
      "epoch": 0.09712737127371274,
      "grad_norm": 13.75859546661377,
      "learning_rate": 1e-05,
      "loss": 6.8308,
      "step": 1792
    },
    {
      "epoch": 0.09712737127371274,
      "step": 1792,
      "training_loss": 7.501184463500977
    },
    {
      "epoch": 0.09718157181571815,
      "step": 1793,
      "training_loss": 6.744534015655518
    },
    {
      "epoch": 0.09723577235772357,
      "step": 1794,
      "training_loss": 8.088337898254395
    },
    {
      "epoch": 0.097289972899729,
      "step": 1795,
      "training_loss": 4.722229957580566
    },
    {
      "epoch": 0.09734417344173442,
      "grad_norm": 16.40852928161621,
      "learning_rate": 1e-05,
      "loss": 6.7641,
      "step": 1796
    },
    {
      "epoch": 0.09734417344173442,
      "step": 1796,
      "training_loss": 5.999045372009277
    },
    {
      "epoch": 0.09739837398373984,
      "step": 1797,
      "training_loss": 7.9346771240234375
    },
    {
      "epoch": 0.09745257452574525,
      "step": 1798,
      "training_loss": 4.874082565307617
    },
    {
      "epoch": 0.09750677506775068,
      "step": 1799,
      "training_loss": 7.72590970993042
    },
    {
      "epoch": 0.0975609756097561,
      "grad_norm": 17.744157791137695,
      "learning_rate": 1e-05,
      "loss": 6.6334,
      "step": 1800
    },
    {
      "epoch": 0.0975609756097561,
      "step": 1800,
      "training_loss": 6.864748954772949
    },
    {
      "epoch": 0.09761517615176152,
      "step": 1801,
      "training_loss": 7.550734996795654
    },
    {
      "epoch": 0.09766937669376693,
      "step": 1802,
      "training_loss": 6.66982889175415
    },
    {
      "epoch": 0.09772357723577235,
      "step": 1803,
      "training_loss": 8.060007095336914
    },
    {
      "epoch": 0.09777777777777778,
      "grad_norm": 28.389541625976562,
      "learning_rate": 1e-05,
      "loss": 7.2863,
      "step": 1804
    },
    {
      "epoch": 0.09777777777777778,
      "step": 1804,
      "training_loss": 7.176174163818359
    },
    {
      "epoch": 0.0978319783197832,
      "step": 1805,
      "training_loss": 7.384130001068115
    },
    {
      "epoch": 0.09788617886178862,
      "step": 1806,
      "training_loss": 7.144604682922363
    },
    {
      "epoch": 0.09794037940379403,
      "step": 1807,
      "training_loss": 6.732770919799805
    },
    {
      "epoch": 0.09799457994579946,
      "grad_norm": 20.322484970092773,
      "learning_rate": 1e-05,
      "loss": 7.1094,
      "step": 1808
    },
    {
      "epoch": 0.09799457994579946,
      "step": 1808,
      "training_loss": 7.012442111968994
    },
    {
      "epoch": 0.09804878048780488,
      "step": 1809,
      "training_loss": 7.252750873565674
    },
    {
      "epoch": 0.0981029810298103,
      "step": 1810,
      "training_loss": 7.621109485626221
    },
    {
      "epoch": 0.09815718157181572,
      "step": 1811,
      "training_loss": 7.669775009155273
    },
    {
      "epoch": 0.09821138211382113,
      "grad_norm": 20.952239990234375,
      "learning_rate": 1e-05,
      "loss": 7.389,
      "step": 1812
    },
    {
      "epoch": 0.09821138211382113,
      "step": 1812,
      "training_loss": 6.4778947830200195
    },
    {
      "epoch": 0.09826558265582656,
      "step": 1813,
      "training_loss": 7.008749008178711
    },
    {
      "epoch": 0.09831978319783198,
      "step": 1814,
      "training_loss": 4.594441890716553
    },
    {
      "epoch": 0.0983739837398374,
      "step": 1815,
      "training_loss": 6.74560546875
    },
    {
      "epoch": 0.09842818428184281,
      "grad_norm": 23.24747657775879,
      "learning_rate": 1e-05,
      "loss": 6.2067,
      "step": 1816
    },
    {
      "epoch": 0.09842818428184281,
      "step": 1816,
      "training_loss": 7.7325758934021
    },
    {
      "epoch": 0.09848238482384823,
      "step": 1817,
      "training_loss": 6.696701526641846
    },
    {
      "epoch": 0.09853658536585366,
      "step": 1818,
      "training_loss": 4.698680400848389
    },
    {
      "epoch": 0.09859078590785908,
      "step": 1819,
      "training_loss": 6.987692832946777
    },
    {
      "epoch": 0.0986449864498645,
      "grad_norm": 15.378469467163086,
      "learning_rate": 1e-05,
      "loss": 6.5289,
      "step": 1820
    },
    {
      "epoch": 0.0986449864498645,
      "step": 1820,
      "training_loss": 7.661124229431152
    },
    {
      "epoch": 0.09869918699186991,
      "step": 1821,
      "training_loss": 6.872621536254883
    },
    {
      "epoch": 0.09875338753387534,
      "step": 1822,
      "training_loss": 8.3343505859375
    },
    {
      "epoch": 0.09880758807588076,
      "step": 1823,
      "training_loss": 7.249980926513672
    },
    {
      "epoch": 0.09886178861788618,
      "grad_norm": 19.45137596130371,
      "learning_rate": 1e-05,
      "loss": 7.5295,
      "step": 1824
    },
    {
      "epoch": 0.09886178861788618,
      "step": 1824,
      "training_loss": 7.07426118850708
    },
    {
      "epoch": 0.0989159891598916,
      "step": 1825,
      "training_loss": 6.6371541023254395
    },
    {
      "epoch": 0.09897018970189701,
      "step": 1826,
      "training_loss": 6.09772253036499
    },
    {
      "epoch": 0.09902439024390244,
      "step": 1827,
      "training_loss": 7.62018346786499
    },
    {
      "epoch": 0.09907859078590786,
      "grad_norm": 19.541664123535156,
      "learning_rate": 1e-05,
      "loss": 6.8573,
      "step": 1828
    },
    {
      "epoch": 0.09907859078590786,
      "step": 1828,
      "training_loss": 7.037596225738525
    },
    {
      "epoch": 0.09913279132791328,
      "step": 1829,
      "training_loss": 5.411575794219971
    },
    {
      "epoch": 0.0991869918699187,
      "step": 1830,
      "training_loss": 5.918088436126709
    },
    {
      "epoch": 0.09924119241192413,
      "step": 1831,
      "training_loss": 6.0435404777526855
    },
    {
      "epoch": 0.09929539295392954,
      "grad_norm": 32.72734451293945,
      "learning_rate": 1e-05,
      "loss": 6.1027,
      "step": 1832
    },
    {
      "epoch": 0.09929539295392954,
      "step": 1832,
      "training_loss": 6.31008243560791
    },
    {
      "epoch": 0.09934959349593496,
      "step": 1833,
      "training_loss": 7.640936374664307
    },
    {
      "epoch": 0.09940379403794038,
      "step": 1834,
      "training_loss": 7.680691242218018
    },
    {
      "epoch": 0.0994579945799458,
      "step": 1835,
      "training_loss": 6.665595531463623
    },
    {
      "epoch": 0.09951219512195122,
      "grad_norm": 14.271285057067871,
      "learning_rate": 1e-05,
      "loss": 7.0743,
      "step": 1836
    },
    {
      "epoch": 0.09951219512195122,
      "step": 1836,
      "training_loss": 5.690708637237549
    },
    {
      "epoch": 0.09956639566395664,
      "step": 1837,
      "training_loss": 8.010293960571289
    },
    {
      "epoch": 0.09962059620596206,
      "step": 1838,
      "training_loss": 5.154156684875488
    },
    {
      "epoch": 0.09967479674796748,
      "step": 1839,
      "training_loss": 7.071045398712158
    },
    {
      "epoch": 0.0997289972899729,
      "grad_norm": 12.849090576171875,
      "learning_rate": 1e-05,
      "loss": 6.4816,
      "step": 1840
    },
    {
      "epoch": 0.0997289972899729,
      "step": 1840,
      "training_loss": 7.014519214630127
    },
    {
      "epoch": 0.09978319783197832,
      "step": 1841,
      "training_loss": 7.2970380783081055
    },
    {
      "epoch": 0.09983739837398374,
      "step": 1842,
      "training_loss": 6.379818916320801
    },
    {
      "epoch": 0.09989159891598916,
      "step": 1843,
      "training_loss": 5.890921592712402
    },
    {
      "epoch": 0.09994579945799457,
      "grad_norm": 22.724231719970703,
      "learning_rate": 1e-05,
      "loss": 6.6456,
      "step": 1844
    },
    {
      "epoch": 0.09994579945799457,
      "step": 1844,
      "training_loss": 6.573121070861816
    },
    {
      "epoch": 0.1,
      "step": 1845,
      "training_loss": 6.535424709320068
    },
    {
      "epoch": 0.10005420054200542,
      "step": 1846,
      "training_loss": 6.750287055969238
    },
    {
      "epoch": 0.10010840108401084,
      "step": 1847,
      "training_loss": 6.688355922698975
    },
    {
      "epoch": 0.10016260162601626,
      "grad_norm": 20.58989143371582,
      "learning_rate": 1e-05,
      "loss": 6.6368,
      "step": 1848
    },
    {
      "epoch": 0.10016260162601626,
      "step": 1848,
      "training_loss": 7.169182777404785
    },
    {
      "epoch": 0.10021680216802167,
      "step": 1849,
      "training_loss": 6.872289657592773
    },
    {
      "epoch": 0.1002710027100271,
      "step": 1850,
      "training_loss": 6.915380001068115
    },
    {
      "epoch": 0.10032520325203252,
      "step": 1851,
      "training_loss": 9.504040718078613
    },
    {
      "epoch": 0.10037940379403794,
      "grad_norm": 56.86107635498047,
      "learning_rate": 1e-05,
      "loss": 7.6152,
      "step": 1852
    },
    {
      "epoch": 0.10037940379403794,
      "step": 1852,
      "training_loss": 6.494965553283691
    },
    {
      "epoch": 0.10043360433604336,
      "step": 1853,
      "training_loss": 6.6870951652526855
    },
    {
      "epoch": 0.10048780487804879,
      "step": 1854,
      "training_loss": 8.392060279846191
    },
    {
      "epoch": 0.1005420054200542,
      "step": 1855,
      "training_loss": 7.3568434715271
    },
    {
      "epoch": 0.10059620596205962,
      "grad_norm": 49.58441925048828,
      "learning_rate": 1e-05,
      "loss": 7.2327,
      "step": 1856
    },
    {
      "epoch": 0.10059620596205962,
      "step": 1856,
      "training_loss": 6.857938289642334
    },
    {
      "epoch": 0.10065040650406504,
      "step": 1857,
      "training_loss": 7.114043235778809
    },
    {
      "epoch": 0.10070460704607045,
      "step": 1858,
      "training_loss": 7.980775833129883
    },
    {
      "epoch": 0.10075880758807589,
      "step": 1859,
      "training_loss": 7.852813720703125
    },
    {
      "epoch": 0.1008130081300813,
      "grad_norm": 53.052452087402344,
      "learning_rate": 1e-05,
      "loss": 7.4514,
      "step": 1860
    },
    {
      "epoch": 0.1008130081300813,
      "step": 1860,
      "training_loss": 7.443462371826172
    },
    {
      "epoch": 0.10086720867208672,
      "step": 1861,
      "training_loss": 6.19350004196167
    },
    {
      "epoch": 0.10092140921409214,
      "step": 1862,
      "training_loss": 7.3889031410217285
    },
    {
      "epoch": 0.10097560975609757,
      "step": 1863,
      "training_loss": 7.585278511047363
    },
    {
      "epoch": 0.10102981029810298,
      "grad_norm": 27.88114356994629,
      "learning_rate": 1e-05,
      "loss": 7.1528,
      "step": 1864
    },
    {
      "epoch": 0.10102981029810298,
      "step": 1864,
      "training_loss": 8.452298164367676
    },
    {
      "epoch": 0.1010840108401084,
      "step": 1865,
      "training_loss": 5.742522716522217
    },
    {
      "epoch": 0.10113821138211382,
      "step": 1866,
      "training_loss": 7.657788276672363
    },
    {
      "epoch": 0.10119241192411924,
      "step": 1867,
      "training_loss": 7.492915630340576
    },
    {
      "epoch": 0.10124661246612467,
      "grad_norm": 18.857528686523438,
      "learning_rate": 1e-05,
      "loss": 7.3364,
      "step": 1868
    },
    {
      "epoch": 0.10124661246612467,
      "step": 1868,
      "training_loss": 5.594577312469482
    },
    {
      "epoch": 0.10130081300813008,
      "step": 1869,
      "training_loss": 8.023194313049316
    },
    {
      "epoch": 0.1013550135501355,
      "step": 1870,
      "training_loss": 6.285765171051025
    },
    {
      "epoch": 0.10140921409214092,
      "step": 1871,
      "training_loss": 7.735846996307373
    },
    {
      "epoch": 0.10146341463414635,
      "grad_norm": 23.132484436035156,
      "learning_rate": 1e-05,
      "loss": 6.9098,
      "step": 1872
    },
    {
      "epoch": 0.10146341463414635,
      "step": 1872,
      "training_loss": 8.006044387817383
    },
    {
      "epoch": 0.10151761517615177,
      "step": 1873,
      "training_loss": 6.534153938293457
    },
    {
      "epoch": 0.10157181571815718,
      "step": 1874,
      "training_loss": 6.306844711303711
    },
    {
      "epoch": 0.1016260162601626,
      "step": 1875,
      "training_loss": 6.963500022888184
    },
    {
      "epoch": 0.10168021680216802,
      "grad_norm": 14.774385452270508,
      "learning_rate": 1e-05,
      "loss": 6.9526,
      "step": 1876
    },
    {
      "epoch": 0.10168021680216802,
      "step": 1876,
      "training_loss": 8.16310977935791
    },
    {
      "epoch": 0.10173441734417345,
      "step": 1877,
      "training_loss": 6.408857345581055
    },
    {
      "epoch": 0.10178861788617886,
      "step": 1878,
      "training_loss": 7.996594429016113
    },
    {
      "epoch": 0.10184281842818428,
      "step": 1879,
      "training_loss": 6.072828769683838
    },
    {
      "epoch": 0.1018970189701897,
      "grad_norm": 14.568513870239258,
      "learning_rate": 1e-05,
      "loss": 7.1603,
      "step": 1880
    },
    {
      "epoch": 0.1018970189701897,
      "step": 1880,
      "training_loss": 6.176990509033203
    },
    {
      "epoch": 0.10195121951219512,
      "step": 1881,
      "training_loss": 7.096137046813965
    },
    {
      "epoch": 0.10200542005420055,
      "step": 1882,
      "training_loss": 7.603650093078613
    },
    {
      "epoch": 0.10205962059620596,
      "step": 1883,
      "training_loss": 7.451887607574463
    },
    {
      "epoch": 0.10211382113821138,
      "grad_norm": 23.526025772094727,
      "learning_rate": 1e-05,
      "loss": 7.0822,
      "step": 1884
    },
    {
      "epoch": 0.10211382113821138,
      "step": 1884,
      "training_loss": 6.8904709815979
    },
    {
      "epoch": 0.1021680216802168,
      "step": 1885,
      "training_loss": 8.41188907623291
    },
    {
      "epoch": 0.10222222222222223,
      "step": 1886,
      "training_loss": 7.375153064727783
    },
    {
      "epoch": 0.10227642276422765,
      "step": 1887,
      "training_loss": 7.26914644241333
    },
    {
      "epoch": 0.10233062330623306,
      "grad_norm": 21.91248321533203,
      "learning_rate": 1e-05,
      "loss": 7.4867,
      "step": 1888
    },
    {
      "epoch": 0.10233062330623306,
      "step": 1888,
      "training_loss": 7.367809772491455
    },
    {
      "epoch": 0.10238482384823848,
      "step": 1889,
      "training_loss": 7.01220703125
    },
    {
      "epoch": 0.1024390243902439,
      "step": 1890,
      "training_loss": 5.942544460296631
    },
    {
      "epoch": 0.10249322493224933,
      "step": 1891,
      "training_loss": 7.407491207122803
    },
    {
      "epoch": 0.10254742547425474,
      "grad_norm": 23.44442367553711,
      "learning_rate": 1e-05,
      "loss": 6.9325,
      "step": 1892
    },
    {
      "epoch": 0.10254742547425474,
      "step": 1892,
      "training_loss": 6.786383628845215
    },
    {
      "epoch": 0.10260162601626016,
      "step": 1893,
      "training_loss": 7.513130187988281
    },
    {
      "epoch": 0.10265582655826558,
      "step": 1894,
      "training_loss": 8.256592750549316
    },
    {
      "epoch": 0.10271002710027101,
      "step": 1895,
      "training_loss": 7.2881760597229
    },
    {
      "epoch": 0.10276422764227643,
      "grad_norm": 32.483028411865234,
      "learning_rate": 1e-05,
      "loss": 7.4611,
      "step": 1896
    },
    {
      "epoch": 0.10276422764227643,
      "step": 1896,
      "training_loss": 4.818662166595459
    },
    {
      "epoch": 0.10281842818428184,
      "step": 1897,
      "training_loss": 6.713764667510986
    },
    {
      "epoch": 0.10287262872628726,
      "step": 1898,
      "training_loss": 6.064594268798828
    },
    {
      "epoch": 0.10292682926829268,
      "step": 1899,
      "training_loss": 8.250630378723145
    },
    {
      "epoch": 0.10298102981029811,
      "grad_norm": 29.14861297607422,
      "learning_rate": 1e-05,
      "loss": 6.4619,
      "step": 1900
    },
    {
      "epoch": 0.10298102981029811,
      "step": 1900,
      "training_loss": 5.687113285064697
    },
    {
      "epoch": 0.10303523035230353,
      "step": 1901,
      "training_loss": 7.016377925872803
    },
    {
      "epoch": 0.10308943089430894,
      "step": 1902,
      "training_loss": 7.7814531326293945
    },
    {
      "epoch": 0.10314363143631436,
      "step": 1903,
      "training_loss": 7.398102760314941
    },
    {
      "epoch": 0.10319783197831979,
      "grad_norm": 27.779050827026367,
      "learning_rate": 1e-05,
      "loss": 6.9708,
      "step": 1904
    },
    {
      "epoch": 0.10319783197831979,
      "step": 1904,
      "training_loss": 6.42487907409668
    },
    {
      "epoch": 0.10325203252032521,
      "step": 1905,
      "training_loss": 7.301225185394287
    },
    {
      "epoch": 0.10330623306233062,
      "step": 1906,
      "training_loss": 7.408722400665283
    },
    {
      "epoch": 0.10336043360433604,
      "step": 1907,
      "training_loss": 7.125107765197754
    },
    {
      "epoch": 0.10341463414634146,
      "grad_norm": 18.092239379882812,
      "learning_rate": 1e-05,
      "loss": 7.065,
      "step": 1908
    },
    {
      "epoch": 0.10341463414634146,
      "step": 1908,
      "training_loss": 7.588310241699219
    },
    {
      "epoch": 0.10346883468834689,
      "step": 1909,
      "training_loss": 7.085721015930176
    },
    {
      "epoch": 0.1035230352303523,
      "step": 1910,
      "training_loss": 7.9064435958862305
    },
    {
      "epoch": 0.10357723577235772,
      "step": 1911,
      "training_loss": 8.664698600769043
    },
    {
      "epoch": 0.10363143631436314,
      "grad_norm": 32.35499954223633,
      "learning_rate": 1e-05,
      "loss": 7.8113,
      "step": 1912
    },
    {
      "epoch": 0.10363143631436314,
      "step": 1912,
      "training_loss": 7.165069103240967
    },
    {
      "epoch": 0.10368563685636856,
      "step": 1913,
      "training_loss": 6.031279563903809
    },
    {
      "epoch": 0.10373983739837399,
      "step": 1914,
      "training_loss": 8.030135154724121
    },
    {
      "epoch": 0.1037940379403794,
      "step": 1915,
      "training_loss": 6.911118030548096
    },
    {
      "epoch": 0.10384823848238482,
      "grad_norm": 23.252477645874023,
      "learning_rate": 1e-05,
      "loss": 7.0344,
      "step": 1916
    },
    {
      "epoch": 0.10384823848238482,
      "step": 1916,
      "training_loss": 6.886211395263672
    },
    {
      "epoch": 0.10390243902439024,
      "step": 1917,
      "training_loss": 7.567008018493652
    },
    {
      "epoch": 0.10395663956639567,
      "step": 1918,
      "training_loss": 6.618258476257324
    },
    {
      "epoch": 0.10401084010840109,
      "step": 1919,
      "training_loss": 6.368618965148926
    },
    {
      "epoch": 0.1040650406504065,
      "grad_norm": 34.3057746887207,
      "learning_rate": 1e-05,
      "loss": 6.86,
      "step": 1920
    },
    {
      "epoch": 0.1040650406504065,
      "step": 1920,
      "training_loss": 5.036281108856201
    },
    {
      "epoch": 0.10411924119241192,
      "step": 1921,
      "training_loss": 7.437923431396484
    },
    {
      "epoch": 0.10417344173441734,
      "step": 1922,
      "training_loss": 5.69644832611084
    },
    {
      "epoch": 0.10422764227642277,
      "step": 1923,
      "training_loss": 7.384374618530273
    },
    {
      "epoch": 0.10428184281842819,
      "grad_norm": 25.513246536254883,
      "learning_rate": 1e-05,
      "loss": 6.3888,
      "step": 1924
    },
    {
      "epoch": 0.10428184281842819,
      "step": 1924,
      "training_loss": 7.976056098937988
    },
    {
      "epoch": 0.1043360433604336,
      "step": 1925,
      "training_loss": 7.0879740715026855
    },
    {
      "epoch": 0.10439024390243902,
      "step": 1926,
      "training_loss": 5.618669033050537
    },
    {
      "epoch": 0.10444444444444445,
      "step": 1927,
      "training_loss": 7.607583045959473
    },
    {
      "epoch": 0.10449864498644987,
      "grad_norm": 19.35965347290039,
      "learning_rate": 1e-05,
      "loss": 7.0726,
      "step": 1928
    },
    {
      "epoch": 0.10449864498644987,
      "step": 1928,
      "training_loss": 6.183177471160889
    },
    {
      "epoch": 0.10455284552845528,
      "step": 1929,
      "training_loss": 7.9468793869018555
    },
    {
      "epoch": 0.1046070460704607,
      "step": 1930,
      "training_loss": 6.587648868560791
    },
    {
      "epoch": 0.10466124661246612,
      "step": 1931,
      "training_loss": 5.988985061645508
    },
    {
      "epoch": 0.10471544715447155,
      "grad_norm": 15.967059135437012,
      "learning_rate": 1e-05,
      "loss": 6.6767,
      "step": 1932
    },
    {
      "epoch": 0.10471544715447155,
      "step": 1932,
      "training_loss": 6.778229236602783
    },
    {
      "epoch": 0.10476964769647697,
      "step": 1933,
      "training_loss": 7.788546562194824
    },
    {
      "epoch": 0.10482384823848238,
      "step": 1934,
      "training_loss": 5.470512866973877
    },
    {
      "epoch": 0.1048780487804878,
      "step": 1935,
      "training_loss": 7.287130832672119
    },
    {
      "epoch": 0.10493224932249323,
      "grad_norm": 18.69687271118164,
      "learning_rate": 1e-05,
      "loss": 6.8311,
      "step": 1936
    },
    {
      "epoch": 0.10493224932249323,
      "step": 1936,
      "training_loss": 7.2460432052612305
    },
    {
      "epoch": 0.10498644986449865,
      "step": 1937,
      "training_loss": 5.408318519592285
    },
    {
      "epoch": 0.10504065040650407,
      "step": 1938,
      "training_loss": 7.379900932312012
    },
    {
      "epoch": 0.10509485094850948,
      "step": 1939,
      "training_loss": 7.259235858917236
    },
    {
      "epoch": 0.1051490514905149,
      "grad_norm": 19.686471939086914,
      "learning_rate": 1e-05,
      "loss": 6.8234,
      "step": 1940
    },
    {
      "epoch": 0.1051490514905149,
      "step": 1940,
      "training_loss": 7.323550701141357
    },
    {
      "epoch": 0.10520325203252033,
      "step": 1941,
      "training_loss": 7.5910115242004395
    },
    {
      "epoch": 0.10525745257452575,
      "step": 1942,
      "training_loss": 6.2993879318237305
    },
    {
      "epoch": 0.10531165311653116,
      "step": 1943,
      "training_loss": 7.883515357971191
    },
    {
      "epoch": 0.10536585365853658,
      "grad_norm": 32.84680938720703,
      "learning_rate": 1e-05,
      "loss": 7.2744,
      "step": 1944
    },
    {
      "epoch": 0.10536585365853658,
      "step": 1944,
      "training_loss": 7.150788307189941
    },
    {
      "epoch": 0.105420054200542,
      "step": 1945,
      "training_loss": 6.813632965087891
    },
    {
      "epoch": 0.10547425474254743,
      "step": 1946,
      "training_loss": 6.969394207000732
    },
    {
      "epoch": 0.10552845528455285,
      "step": 1947,
      "training_loss": 7.042242527008057
    },
    {
      "epoch": 0.10558265582655826,
      "grad_norm": 14.737787246704102,
      "learning_rate": 1e-05,
      "loss": 6.994,
      "step": 1948
    },
    {
      "epoch": 0.10558265582655826,
      "step": 1948,
      "training_loss": 6.207190990447998
    },
    {
      "epoch": 0.10563685636856368,
      "step": 1949,
      "training_loss": 7.253708362579346
    },
    {
      "epoch": 0.10569105691056911,
      "step": 1950,
      "training_loss": 7.475819110870361
    },
    {
      "epoch": 0.10574525745257453,
      "step": 1951,
      "training_loss": 6.559276580810547
    },
    {
      "epoch": 0.10579945799457995,
      "grad_norm": 13.06650447845459,
      "learning_rate": 1e-05,
      "loss": 6.874,
      "step": 1952
    },
    {
      "epoch": 0.10579945799457995,
      "step": 1952,
      "training_loss": 7.1003923416137695
    },
    {
      "epoch": 0.10585365853658536,
      "step": 1953,
      "training_loss": 7.40924596786499
    },
    {
      "epoch": 0.10590785907859078,
      "step": 1954,
      "training_loss": 7.4768805503845215
    },
    {
      "epoch": 0.10596205962059621,
      "step": 1955,
      "training_loss": 6.6013569831848145
    },
    {
      "epoch": 0.10601626016260163,
      "grad_norm": 17.387710571289062,
      "learning_rate": 1e-05,
      "loss": 7.147,
      "step": 1956
    },
    {
      "epoch": 0.10601626016260163,
      "step": 1956,
      "training_loss": 6.421169281005859
    },
    {
      "epoch": 0.10607046070460704,
      "step": 1957,
      "training_loss": 7.327566623687744
    },
    {
      "epoch": 0.10612466124661246,
      "step": 1958,
      "training_loss": 7.779445171356201
    },
    {
      "epoch": 0.10617886178861789,
      "step": 1959,
      "training_loss": 6.880326271057129
    },
    {
      "epoch": 0.10623306233062331,
      "grad_norm": 15.404858589172363,
      "learning_rate": 1e-05,
      "loss": 7.1021,
      "step": 1960
    },
    {
      "epoch": 0.10623306233062331,
      "step": 1960,
      "training_loss": 6.2346577644348145
    },
    {
      "epoch": 0.10628726287262873,
      "step": 1961,
      "training_loss": 8.373960494995117
    },
    {
      "epoch": 0.10634146341463414,
      "step": 1962,
      "training_loss": 7.205341339111328
    },
    {
      "epoch": 0.10639566395663956,
      "step": 1963,
      "training_loss": 9.370823860168457
    },
    {
      "epoch": 0.10644986449864499,
      "grad_norm": 27.95480728149414,
      "learning_rate": 1e-05,
      "loss": 7.7962,
      "step": 1964
    },
    {
      "epoch": 0.10644986449864499,
      "step": 1964,
      "training_loss": 6.605642795562744
    },
    {
      "epoch": 0.10650406504065041,
      "step": 1965,
      "training_loss": 6.541418075561523
    },
    {
      "epoch": 0.10655826558265583,
      "step": 1966,
      "training_loss": 8.00596809387207
    },
    {
      "epoch": 0.10661246612466124,
      "step": 1967,
      "training_loss": 7.321264743804932
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 19.02974510192871,
      "learning_rate": 1e-05,
      "loss": 7.1186,
      "step": 1968
    },
    {
      "epoch": 0.10666666666666667,
      "step": 1968,
      "training_loss": 6.069786071777344
    },
    {
      "epoch": 0.10672086720867209,
      "step": 1969,
      "training_loss": 9.089376449584961
    },
    {
      "epoch": 0.10677506775067751,
      "step": 1970,
      "training_loss": 6.151271820068359
    },
    {
      "epoch": 0.10682926829268292,
      "step": 1971,
      "training_loss": 6.22847843170166
    },
    {
      "epoch": 0.10688346883468834,
      "grad_norm": 15.762473106384277,
      "learning_rate": 1e-05,
      "loss": 6.8847,
      "step": 1972
    },
    {
      "epoch": 0.10688346883468834,
      "step": 1972,
      "training_loss": 6.670057773590088
    },
    {
      "epoch": 0.10693766937669377,
      "step": 1973,
      "training_loss": 8.368659019470215
    },
    {
      "epoch": 0.10699186991869919,
      "step": 1974,
      "training_loss": 6.4137115478515625
    },
    {
      "epoch": 0.1070460704607046,
      "step": 1975,
      "training_loss": 7.357468128204346
    },
    {
      "epoch": 0.10710027100271002,
      "grad_norm": 15.742369651794434,
      "learning_rate": 1e-05,
      "loss": 7.2025,
      "step": 1976
    },
    {
      "epoch": 0.10710027100271002,
      "step": 1976,
      "training_loss": 6.975642681121826
    },
    {
      "epoch": 0.10715447154471544,
      "step": 1977,
      "training_loss": 5.945455074310303
    },
    {
      "epoch": 0.10720867208672087,
      "step": 1978,
      "training_loss": 6.926813125610352
    },
    {
      "epoch": 0.10726287262872629,
      "step": 1979,
      "training_loss": 7.6792144775390625
    },
    {
      "epoch": 0.1073170731707317,
      "grad_norm": 18.588665008544922,
      "learning_rate": 1e-05,
      "loss": 6.8818,
      "step": 1980
    },
    {
      "epoch": 0.1073170731707317,
      "step": 1980,
      "training_loss": 6.699853897094727
    },
    {
      "epoch": 0.10737127371273712,
      "step": 1981,
      "training_loss": 6.843050003051758
    },
    {
      "epoch": 0.10742547425474255,
      "step": 1982,
      "training_loss": 6.9632134437561035
    },
    {
      "epoch": 0.10747967479674797,
      "step": 1983,
      "training_loss": 7.026053428649902
    },
    {
      "epoch": 0.10753387533875339,
      "grad_norm": 14.146771430969238,
      "learning_rate": 1e-05,
      "loss": 6.883,
      "step": 1984
    },
    {
      "epoch": 0.10753387533875339,
      "step": 1984,
      "training_loss": 6.589237689971924
    },
    {
      "epoch": 0.1075880758807588,
      "step": 1985,
      "training_loss": 7.369289875030518
    },
    {
      "epoch": 0.10764227642276422,
      "step": 1986,
      "training_loss": 7.209839820861816
    },
    {
      "epoch": 0.10769647696476965,
      "step": 1987,
      "training_loss": 5.73673677444458
    },
    {
      "epoch": 0.10775067750677507,
      "grad_norm": 22.378562927246094,
      "learning_rate": 1e-05,
      "loss": 6.7263,
      "step": 1988
    },
    {
      "epoch": 0.10775067750677507,
      "step": 1988,
      "training_loss": 5.998231887817383
    },
    {
      "epoch": 0.10780487804878049,
      "step": 1989,
      "training_loss": 8.2850341796875
    },
    {
      "epoch": 0.1078590785907859,
      "step": 1990,
      "training_loss": 6.635718822479248
    },
    {
      "epoch": 0.10791327913279133,
      "step": 1991,
      "training_loss": 6.44923734664917
    },
    {
      "epoch": 0.10796747967479675,
      "grad_norm": 19.15929412841797,
      "learning_rate": 1e-05,
      "loss": 6.8421,
      "step": 1992
    },
    {
      "epoch": 0.10796747967479675,
      "step": 1992,
      "training_loss": 6.53092098236084
    },
    {
      "epoch": 0.10802168021680217,
      "step": 1993,
      "training_loss": 5.64456033706665
    },
    {
      "epoch": 0.10807588075880759,
      "step": 1994,
      "training_loss": 7.11775541305542
    },
    {
      "epoch": 0.108130081300813,
      "step": 1995,
      "training_loss": 7.091853618621826
    },
    {
      "epoch": 0.10818428184281843,
      "grad_norm": 12.573721885681152,
      "learning_rate": 1e-05,
      "loss": 6.5963,
      "step": 1996
    },
    {
      "epoch": 0.10818428184281843,
      "step": 1996,
      "training_loss": 8.003024101257324
    },
    {
      "epoch": 0.10823848238482385,
      "step": 1997,
      "training_loss": 7.554884433746338
    },
    {
      "epoch": 0.10829268292682927,
      "step": 1998,
      "training_loss": 6.000514507293701
    },
    {
      "epoch": 0.10834688346883468,
      "step": 1999,
      "training_loss": 7.263428688049316
    },
    {
      "epoch": 0.10840108401084012,
      "grad_norm": 29.47939682006836,
      "learning_rate": 1e-05,
      "loss": 7.2055,
      "step": 2000
    },
    {
      "epoch": 0.10840108401084012,
      "step": 2000,
      "training_loss": 4.917733669281006
    },
    {
      "epoch": 0.10845528455284553,
      "step": 2001,
      "training_loss": 5.301447868347168
    },
    {
      "epoch": 0.10850948509485095,
      "step": 2002,
      "training_loss": 7.338685035705566
    },
    {
      "epoch": 0.10856368563685637,
      "step": 2003,
      "training_loss": 6.105548858642578
    },
    {
      "epoch": 0.10861788617886178,
      "grad_norm": 38.752967834472656,
      "learning_rate": 1e-05,
      "loss": 5.9159,
      "step": 2004
    },
    {
      "epoch": 0.10861788617886178,
      "step": 2004,
      "training_loss": 7.486490249633789
    },
    {
      "epoch": 0.10867208672086721,
      "step": 2005,
      "training_loss": 6.366743087768555
    },
    {
      "epoch": 0.10872628726287263,
      "step": 2006,
      "training_loss": 7.139036178588867
    },
    {
      "epoch": 0.10878048780487805,
      "step": 2007,
      "training_loss": 7.025553226470947
    },
    {
      "epoch": 0.10883468834688347,
      "grad_norm": 20.09546661376953,
      "learning_rate": 1e-05,
      "loss": 7.0045,
      "step": 2008
    },
    {
      "epoch": 0.10883468834688347,
      "step": 2008,
      "training_loss": 7.152218818664551
    },
    {
      "epoch": 0.10888888888888888,
      "step": 2009,
      "training_loss": 7.274974822998047
    },
    {
      "epoch": 0.10894308943089431,
      "step": 2010,
      "training_loss": 5.628118991851807
    },
    {
      "epoch": 0.10899728997289973,
      "step": 2011,
      "training_loss": 5.626953125
    },
    {
      "epoch": 0.10905149051490515,
      "grad_norm": 21.96625518798828,
      "learning_rate": 1e-05,
      "loss": 6.4206,
      "step": 2012
    },
    {
      "epoch": 0.10905149051490515,
      "step": 2012,
      "training_loss": 7.25082540512085
    },
    {
      "epoch": 0.10910569105691056,
      "step": 2013,
      "training_loss": 7.4336066246032715
    },
    {
      "epoch": 0.109159891598916,
      "step": 2014,
      "training_loss": 7.142324447631836
    },
    {
      "epoch": 0.10921409214092141,
      "step": 2015,
      "training_loss": 6.022584915161133
    },
    {
      "epoch": 0.10926829268292683,
      "grad_norm": 18.266977310180664,
      "learning_rate": 1e-05,
      "loss": 6.9623,
      "step": 2016
    },
    {
      "epoch": 0.10926829268292683,
      "step": 2016,
      "training_loss": 6.441844463348389
    },
    {
      "epoch": 0.10932249322493225,
      "step": 2017,
      "training_loss": 7.817961692810059
    },
    {
      "epoch": 0.10937669376693766,
      "step": 2018,
      "training_loss": 5.661920070648193
    },
    {
      "epoch": 0.1094308943089431,
      "step": 2019,
      "training_loss": 6.9994378089904785
    },
    {
      "epoch": 0.10948509485094851,
      "grad_norm": 32.17500686645508,
      "learning_rate": 1e-05,
      "loss": 6.7303,
      "step": 2020
    },
    {
      "epoch": 0.10948509485094851,
      "step": 2020,
      "training_loss": 5.38447904586792
    },
    {
      "epoch": 0.10953929539295393,
      "step": 2021,
      "training_loss": 8.099637031555176
    },
    {
      "epoch": 0.10959349593495935,
      "step": 2022,
      "training_loss": 6.31291389465332
    },
    {
      "epoch": 0.10964769647696478,
      "step": 2023,
      "training_loss": 7.197179794311523
    },
    {
      "epoch": 0.1097018970189702,
      "grad_norm": 17.256973266601562,
      "learning_rate": 1e-05,
      "loss": 6.7486,
      "step": 2024
    },
    {
      "epoch": 0.1097018970189702,
      "step": 2024,
      "training_loss": 6.13470983505249
    },
    {
      "epoch": 0.10975609756097561,
      "step": 2025,
      "training_loss": 7.179380893707275
    },
    {
      "epoch": 0.10981029810298103,
      "step": 2026,
      "training_loss": 6.916756629943848
    },
    {
      "epoch": 0.10986449864498644,
      "step": 2027,
      "training_loss": 7.053202152252197
    },
    {
      "epoch": 0.10991869918699188,
      "grad_norm": 23.984786987304688,
      "learning_rate": 1e-05,
      "loss": 6.821,
      "step": 2028
    },
    {
      "epoch": 0.10991869918699188,
      "step": 2028,
      "training_loss": 7.695729732513428
    },
    {
      "epoch": 0.10997289972899729,
      "step": 2029,
      "training_loss": 7.657358646392822
    },
    {
      "epoch": 0.11002710027100271,
      "step": 2030,
      "training_loss": 6.535762786865234
    },
    {
      "epoch": 0.11008130081300813,
      "step": 2031,
      "training_loss": 6.6994500160217285
    },
    {
      "epoch": 0.11013550135501356,
      "grad_norm": 23.21292495727539,
      "learning_rate": 1e-05,
      "loss": 7.1471,
      "step": 2032
    },
    {
      "epoch": 0.11013550135501356,
      "step": 2032,
      "training_loss": 7.39978551864624
    },
    {
      "epoch": 0.11018970189701897,
      "step": 2033,
      "training_loss": 7.436779975891113
    },
    {
      "epoch": 0.11024390243902439,
      "step": 2034,
      "training_loss": 7.955730438232422
    },
    {
      "epoch": 0.11029810298102981,
      "step": 2035,
      "training_loss": 6.83694314956665
    },
    {
      "epoch": 0.11035230352303523,
      "grad_norm": 13.935140609741211,
      "learning_rate": 1e-05,
      "loss": 7.4073,
      "step": 2036
    },
    {
      "epoch": 0.11035230352303523,
      "step": 2036,
      "training_loss": 8.060543060302734
    },
    {
      "epoch": 0.11040650406504066,
      "step": 2037,
      "training_loss": 6.091076374053955
    },
    {
      "epoch": 0.11046070460704607,
      "step": 2038,
      "training_loss": 5.8447265625
    },
    {
      "epoch": 0.11051490514905149,
      "step": 2039,
      "training_loss": 6.937036991119385
    },
    {
      "epoch": 0.11056910569105691,
      "grad_norm": 18.470890045166016,
      "learning_rate": 1e-05,
      "loss": 6.7333,
      "step": 2040
    },
    {
      "epoch": 0.11056910569105691,
      "step": 2040,
      "training_loss": 7.043863773345947
    },
    {
      "epoch": 0.11062330623306232,
      "step": 2041,
      "training_loss": 6.365616321563721
    },
    {
      "epoch": 0.11067750677506775,
      "step": 2042,
      "training_loss": 7.37481164932251
    },
    {
      "epoch": 0.11073170731707317,
      "step": 2043,
      "training_loss": 6.091265678405762
    },
    {
      "epoch": 0.11078590785907859,
      "grad_norm": 18.24860191345215,
      "learning_rate": 1e-05,
      "loss": 6.7189,
      "step": 2044
    },
    {
      "epoch": 0.11078590785907859,
      "step": 2044,
      "training_loss": 7.589911937713623
    },
    {
      "epoch": 0.110840108401084,
      "step": 2045,
      "training_loss": 7.25822639465332
    },
    {
      "epoch": 0.11089430894308944,
      "step": 2046,
      "training_loss": 7.216233253479004
    },
    {
      "epoch": 0.11094850948509485,
      "step": 2047,
      "training_loss": 6.628027439117432
    },
    {
      "epoch": 0.11100271002710027,
      "grad_norm": 25.72601890563965,
      "learning_rate": 1e-05,
      "loss": 7.1731,
      "step": 2048
    },
    {
      "epoch": 0.11100271002710027,
      "step": 2048,
      "training_loss": 7.169719696044922
    },
    {
      "epoch": 0.11105691056910569,
      "step": 2049,
      "training_loss": 6.779860019683838
    },
    {
      "epoch": 0.1111111111111111,
      "step": 2050,
      "training_loss": 6.15336275100708
    },
    {
      "epoch": 0.11116531165311654,
      "step": 2051,
      "training_loss": 6.842559814453125
    },
    {
      "epoch": 0.11121951219512195,
      "grad_norm": 19.939983367919922,
      "learning_rate": 1e-05,
      "loss": 6.7364,
      "step": 2052
    },
    {
      "epoch": 0.11121951219512195,
      "step": 2052,
      "training_loss": 7.427682399749756
    },
    {
      "epoch": 0.11127371273712737,
      "step": 2053,
      "training_loss": 6.81777811050415
    },
    {
      "epoch": 0.11132791327913279,
      "step": 2054,
      "training_loss": 5.9099955558776855
    },
    {
      "epoch": 0.11138211382113822,
      "step": 2055,
      "training_loss": 4.812455177307129
    },
    {
      "epoch": 0.11143631436314363,
      "grad_norm": 30.833858489990234,
      "learning_rate": 1e-05,
      "loss": 6.242,
      "step": 2056
    },
    {
      "epoch": 0.11143631436314363,
      "step": 2056,
      "training_loss": 7.872157096862793
    },
    {
      "epoch": 0.11149051490514905,
      "step": 2057,
      "training_loss": 7.923331260681152
    },
    {
      "epoch": 0.11154471544715447,
      "step": 2058,
      "training_loss": 7.510112762451172
    },
    {
      "epoch": 0.11159891598915989,
      "step": 2059,
      "training_loss": 7.7613911628723145
    },
    {
      "epoch": 0.11165311653116532,
      "grad_norm": 16.247591018676758,
      "learning_rate": 1e-05,
      "loss": 7.7667,
      "step": 2060
    },
    {
      "epoch": 0.11165311653116532,
      "step": 2060,
      "training_loss": 5.250670909881592
    },
    {
      "epoch": 0.11170731707317073,
      "step": 2061,
      "training_loss": 7.279896259307861
    },
    {
      "epoch": 0.11176151761517615,
      "step": 2062,
      "training_loss": 6.21408224105835
    },
    {
      "epoch": 0.11181571815718157,
      "step": 2063,
      "training_loss": 7.445474624633789
    },
    {
      "epoch": 0.111869918699187,
      "grad_norm": 18.06972885131836,
      "learning_rate": 1e-05,
      "loss": 6.5475,
      "step": 2064
    },
    {
      "epoch": 0.111869918699187,
      "step": 2064,
      "training_loss": 6.311251163482666
    },
    {
      "epoch": 0.11192411924119242,
      "step": 2065,
      "training_loss": 5.578175067901611
    },
    {
      "epoch": 0.11197831978319783,
      "step": 2066,
      "training_loss": 7.900197982788086
    },
    {
      "epoch": 0.11203252032520325,
      "step": 2067,
      "training_loss": 7.469659328460693
    },
    {
      "epoch": 0.11208672086720867,
      "grad_norm": 22.75433921813965,
      "learning_rate": 1e-05,
      "loss": 6.8148,
      "step": 2068
    },
    {
      "epoch": 0.11208672086720867,
      "step": 2068,
      "training_loss": 6.8019514083862305
    },
    {
      "epoch": 0.1121409214092141,
      "step": 2069,
      "training_loss": 7.554027557373047
    },
    {
      "epoch": 0.11219512195121951,
      "step": 2070,
      "training_loss": 7.7410664558410645
    },
    {
      "epoch": 0.11224932249322493,
      "step": 2071,
      "training_loss": 7.443899631500244
    },
    {
      "epoch": 0.11230352303523035,
      "grad_norm": 28.041431427001953,
      "learning_rate": 1e-05,
      "loss": 7.3852,
      "step": 2072
    },
    {
      "epoch": 0.11230352303523035,
      "step": 2072,
      "training_loss": 8.143228530883789
    },
    {
      "epoch": 0.11235772357723577,
      "step": 2073,
      "training_loss": 6.881195545196533
    },
    {
      "epoch": 0.1124119241192412,
      "step": 2074,
      "training_loss": 6.679059028625488
    },
    {
      "epoch": 0.11246612466124661,
      "step": 2075,
      "training_loss": 7.1452250480651855
    },
    {
      "epoch": 0.11252032520325203,
      "grad_norm": 24.63528823852539,
      "learning_rate": 1e-05,
      "loss": 7.2122,
      "step": 2076
    },
    {
      "epoch": 0.11252032520325203,
      "step": 2076,
      "training_loss": 7.797855854034424
    },
    {
      "epoch": 0.11257452574525745,
      "step": 2077,
      "training_loss": 7.4600934982299805
    },
    {
      "epoch": 0.11262872628726288,
      "step": 2078,
      "training_loss": 4.757382392883301
    },
    {
      "epoch": 0.1126829268292683,
      "step": 2079,
      "training_loss": 7.314043045043945
    },
    {
      "epoch": 0.11273712737127371,
      "grad_norm": 14.911195755004883,
      "learning_rate": 1e-05,
      "loss": 6.8323,
      "step": 2080
    },
    {
      "epoch": 0.11273712737127371,
      "step": 2080,
      "training_loss": 7.941863536834717
    },
    {
      "epoch": 0.11279132791327913,
      "step": 2081,
      "training_loss": 6.773160934448242
    },
    {
      "epoch": 0.11284552845528455,
      "step": 2082,
      "training_loss": 8.481539726257324
    },
    {
      "epoch": 0.11289972899728998,
      "step": 2083,
      "training_loss": 6.505673408508301
    },
    {
      "epoch": 0.1129539295392954,
      "grad_norm": 18.554052352905273,
      "learning_rate": 1e-05,
      "loss": 7.4256,
      "step": 2084
    },
    {
      "epoch": 0.1129539295392954,
      "step": 2084,
      "training_loss": 6.93223762512207
    },
    {
      "epoch": 0.11300813008130081,
      "step": 2085,
      "training_loss": 7.408477306365967
    },
    {
      "epoch": 0.11306233062330623,
      "step": 2086,
      "training_loss": 7.136406898498535
    },
    {
      "epoch": 0.11311653116531166,
      "step": 2087,
      "training_loss": 11.030338287353516
    },
    {
      "epoch": 0.11317073170731708,
      "grad_norm": 63.35004806518555,
      "learning_rate": 1e-05,
      "loss": 8.1269,
      "step": 2088
    },
    {
      "epoch": 0.11317073170731708,
      "step": 2088,
      "training_loss": 7.2723541259765625
    },
    {
      "epoch": 0.1132249322493225,
      "step": 2089,
      "training_loss": 6.949575901031494
    },
    {
      "epoch": 0.11327913279132791,
      "step": 2090,
      "training_loss": 8.952471733093262
    },
    {
      "epoch": 0.11333333333333333,
      "step": 2091,
      "training_loss": 6.025363445281982
    },
    {
      "epoch": 0.11338753387533876,
      "grad_norm": 25.684778213500977,
      "learning_rate": 1e-05,
      "loss": 7.2999,
      "step": 2092
    },
    {
      "epoch": 0.11338753387533876,
      "step": 2092,
      "training_loss": 7.202154159545898
    },
    {
      "epoch": 0.11344173441734418,
      "step": 2093,
      "training_loss": 7.344385147094727
    },
    {
      "epoch": 0.11349593495934959,
      "step": 2094,
      "training_loss": 6.950316429138184
    },
    {
      "epoch": 0.11355013550135501,
      "step": 2095,
      "training_loss": 6.523730278015137
    },
    {
      "epoch": 0.11360433604336044,
      "grad_norm": 19.213626861572266,
      "learning_rate": 1e-05,
      "loss": 7.0051,
      "step": 2096
    },
    {
      "epoch": 0.11360433604336044,
      "step": 2096,
      "training_loss": 4.602724552154541
    },
    {
      "epoch": 0.11365853658536586,
      "step": 2097,
      "training_loss": 6.775113582611084
    },
    {
      "epoch": 0.11371273712737127,
      "step": 2098,
      "training_loss": 7.911966323852539
    },
    {
      "epoch": 0.11376693766937669,
      "step": 2099,
      "training_loss": 5.996678352355957
    },
    {
      "epoch": 0.11382113821138211,
      "grad_norm": 26.162574768066406,
      "learning_rate": 1e-05,
      "loss": 6.3216,
      "step": 2100
    },
    {
      "epoch": 0.11382113821138211,
      "step": 2100,
      "training_loss": 6.781576633453369
    },
    {
      "epoch": 0.11387533875338754,
      "step": 2101,
      "training_loss": 7.5383172035217285
    },
    {
      "epoch": 0.11392953929539296,
      "step": 2102,
      "training_loss": 7.247105121612549
    },
    {
      "epoch": 0.11398373983739837,
      "step": 2103,
      "training_loss": 5.870489120483398
    },
    {
      "epoch": 0.11403794037940379,
      "grad_norm": 27.816017150878906,
      "learning_rate": 1e-05,
      "loss": 6.8594,
      "step": 2104
    },
    {
      "epoch": 0.11403794037940379,
      "step": 2104,
      "training_loss": 7.518408298492432
    },
    {
      "epoch": 0.11409214092140921,
      "step": 2105,
      "training_loss": 8.428993225097656
    },
    {
      "epoch": 0.11414634146341464,
      "step": 2106,
      "training_loss": 6.692682266235352
    },
    {
      "epoch": 0.11420054200542006,
      "step": 2107,
      "training_loss": 8.433989524841309
    },
    {
      "epoch": 0.11425474254742547,
      "grad_norm": 32.873779296875,
      "learning_rate": 1e-05,
      "loss": 7.7685,
      "step": 2108
    },
    {
      "epoch": 0.11425474254742547,
      "step": 2108,
      "training_loss": 6.494830131530762
    },
    {
      "epoch": 0.11430894308943089,
      "step": 2109,
      "training_loss": 8.253833770751953
    },
    {
      "epoch": 0.11436314363143632,
      "step": 2110,
      "training_loss": 6.045570373535156
    },
    {
      "epoch": 0.11441734417344174,
      "step": 2111,
      "training_loss": 7.167105197906494
    },
    {
      "epoch": 0.11447154471544715,
      "grad_norm": 18.969377517700195,
      "learning_rate": 1e-05,
      "loss": 6.9903,
      "step": 2112
    },
    {
      "epoch": 0.11447154471544715,
      "step": 2112,
      "training_loss": 7.4354658126831055
    },
    {
      "epoch": 0.11452574525745257,
      "step": 2113,
      "training_loss": 7.504770278930664
    },
    {
      "epoch": 0.11457994579945799,
      "step": 2114,
      "training_loss": 5.098837375640869
    },
    {
      "epoch": 0.11463414634146342,
      "step": 2115,
      "training_loss": 7.09758996963501
    },
    {
      "epoch": 0.11468834688346884,
      "grad_norm": 17.852073669433594,
      "learning_rate": 1e-05,
      "loss": 6.7842,
      "step": 2116
    },
    {
      "epoch": 0.11468834688346884,
      "step": 2116,
      "training_loss": 6.416047096252441
    },
    {
      "epoch": 0.11474254742547425,
      "step": 2117,
      "training_loss": 5.340090751647949
    },
    {
      "epoch": 0.11479674796747967,
      "step": 2118,
      "training_loss": 6.885333061218262
    },
    {
      "epoch": 0.1148509485094851,
      "step": 2119,
      "training_loss": 6.9160051345825195
    },
    {
      "epoch": 0.11490514905149052,
      "grad_norm": 22.28732681274414,
      "learning_rate": 1e-05,
      "loss": 6.3894,
      "step": 2120
    },
    {
      "epoch": 0.11490514905149052,
      "step": 2120,
      "training_loss": 7.809572219848633
    },
    {
      "epoch": 0.11495934959349594,
      "step": 2121,
      "training_loss": 6.504796981811523
    },
    {
      "epoch": 0.11501355013550135,
      "step": 2122,
      "training_loss": 6.268870830535889
    },
    {
      "epoch": 0.11506775067750677,
      "step": 2123,
      "training_loss": 7.7261528968811035
    },
    {
      "epoch": 0.1151219512195122,
      "grad_norm": 26.758596420288086,
      "learning_rate": 1e-05,
      "loss": 7.0773,
      "step": 2124
    },
    {
      "epoch": 0.1151219512195122,
      "step": 2124,
      "training_loss": 7.725598335266113
    },
    {
      "epoch": 0.11517615176151762,
      "step": 2125,
      "training_loss": 7.527799129486084
    },
    {
      "epoch": 0.11523035230352303,
      "step": 2126,
      "training_loss": 7.7196478843688965
    },
    {
      "epoch": 0.11528455284552845,
      "step": 2127,
      "training_loss": 6.61648416519165
    },
    {
      "epoch": 0.11533875338753388,
      "grad_norm": 22.383005142211914,
      "learning_rate": 1e-05,
      "loss": 7.3974,
      "step": 2128
    },
    {
      "epoch": 0.11533875338753388,
      "step": 2128,
      "training_loss": 7.687233924865723
    },
    {
      "epoch": 0.1153929539295393,
      "step": 2129,
      "training_loss": 4.464258670806885
    },
    {
      "epoch": 0.11544715447154472,
      "step": 2130,
      "training_loss": 5.528926849365234
    },
    {
      "epoch": 0.11550135501355013,
      "step": 2131,
      "training_loss": 6.233382225036621
    },
    {
      "epoch": 0.11555555555555555,
      "grad_norm": 26.185707092285156,
      "learning_rate": 1e-05,
      "loss": 5.9785,
      "step": 2132
    },
    {
      "epoch": 0.11555555555555555,
      "step": 2132,
      "training_loss": 5.084183216094971
    },
    {
      "epoch": 0.11560975609756098,
      "step": 2133,
      "training_loss": 6.269379615783691
    },
    {
      "epoch": 0.1156639566395664,
      "step": 2134,
      "training_loss": 6.6263041496276855
    },
    {
      "epoch": 0.11571815718157182,
      "step": 2135,
      "training_loss": 7.871219158172607
    },
    {
      "epoch": 0.11577235772357723,
      "grad_norm": 22.49785614013672,
      "learning_rate": 1e-05,
      "loss": 6.4628,
      "step": 2136
    },
    {
      "epoch": 0.11577235772357723,
      "step": 2136,
      "training_loss": 7.294027805328369
    },
    {
      "epoch": 0.11582655826558265,
      "step": 2137,
      "training_loss": 6.727995872497559
    },
    {
      "epoch": 0.11588075880758808,
      "step": 2138,
      "training_loss": 5.219162464141846
    },
    {
      "epoch": 0.1159349593495935,
      "step": 2139,
      "training_loss": 8.14572525024414
    },
    {
      "epoch": 0.11598915989159891,
      "grad_norm": 19.990718841552734,
      "learning_rate": 1e-05,
      "loss": 6.8467,
      "step": 2140
    },
    {
      "epoch": 0.11598915989159891,
      "step": 2140,
      "training_loss": 6.7659759521484375
    },
    {
      "epoch": 0.11604336043360433,
      "step": 2141,
      "training_loss": 6.988767623901367
    },
    {
      "epoch": 0.11609756097560976,
      "step": 2142,
      "training_loss": 6.989974498748779
    },
    {
      "epoch": 0.11615176151761518,
      "step": 2143,
      "training_loss": 6.723678112030029
    },
    {
      "epoch": 0.1162059620596206,
      "grad_norm": 23.07312774658203,
      "learning_rate": 1e-05,
      "loss": 6.8671,
      "step": 2144
    },
    {
      "epoch": 0.1162059620596206,
      "step": 2144,
      "training_loss": 6.134854793548584
    },
    {
      "epoch": 0.11626016260162601,
      "step": 2145,
      "training_loss": 5.1014485359191895
    },
    {
      "epoch": 0.11631436314363143,
      "step": 2146,
      "training_loss": 5.724407196044922
    },
    {
      "epoch": 0.11636856368563686,
      "step": 2147,
      "training_loss": 7.357482433319092
    },
    {
      "epoch": 0.11642276422764228,
      "grad_norm": 15.279850006103516,
      "learning_rate": 1e-05,
      "loss": 6.0795,
      "step": 2148
    },
    {
      "epoch": 0.11642276422764228,
      "step": 2148,
      "training_loss": 5.9882097244262695
    },
    {
      "epoch": 0.1164769647696477,
      "step": 2149,
      "training_loss": 7.7010817527771
    },
    {
      "epoch": 0.11653116531165311,
      "step": 2150,
      "training_loss": 7.026291370391846
    },
    {
      "epoch": 0.11658536585365854,
      "step": 2151,
      "training_loss": 6.788961887359619
    },
    {
      "epoch": 0.11663956639566396,
      "grad_norm": 70.93637084960938,
      "learning_rate": 1e-05,
      "loss": 6.8761,
      "step": 2152
    },
    {
      "epoch": 0.11663956639566396,
      "step": 2152,
      "training_loss": 6.7039103507995605
    },
    {
      "epoch": 0.11669376693766938,
      "step": 2153,
      "training_loss": 7.3231120109558105
    },
    {
      "epoch": 0.1167479674796748,
      "step": 2154,
      "training_loss": 5.800763130187988
    },
    {
      "epoch": 0.11680216802168021,
      "step": 2155,
      "training_loss": 7.861323356628418
    },
    {
      "epoch": 0.11685636856368564,
      "grad_norm": 19.008670806884766,
      "learning_rate": 1e-05,
      "loss": 6.9223,
      "step": 2156
    },
    {
      "epoch": 0.11685636856368564,
      "step": 2156,
      "training_loss": 7.269955635070801
    },
    {
      "epoch": 0.11691056910569106,
      "step": 2157,
      "training_loss": 4.812743663787842
    },
    {
      "epoch": 0.11696476964769648,
      "step": 2158,
      "training_loss": 7.735233783721924
    },
    {
      "epoch": 0.11701897018970189,
      "step": 2159,
      "training_loss": 6.502394199371338
    },
    {
      "epoch": 0.11707317073170732,
      "grad_norm": 23.99907684326172,
      "learning_rate": 1e-05,
      "loss": 6.5801,
      "step": 2160
    },
    {
      "epoch": 0.11707317073170732,
      "step": 2160,
      "training_loss": 7.685892105102539
    },
    {
      "epoch": 0.11712737127371274,
      "step": 2161,
      "training_loss": 7.871717929840088
    },
    {
      "epoch": 0.11718157181571816,
      "step": 2162,
      "training_loss": 6.920495986938477
    },
    {
      "epoch": 0.11723577235772357,
      "step": 2163,
      "training_loss": 7.4070515632629395
    },
    {
      "epoch": 0.11728997289972899,
      "grad_norm": 20.27174186706543,
      "learning_rate": 1e-05,
      "loss": 7.4713,
      "step": 2164
    },
    {
      "epoch": 0.11728997289972899,
      "step": 2164,
      "training_loss": 7.945376396179199
    },
    {
      "epoch": 0.11734417344173442,
      "step": 2165,
      "training_loss": 4.502406120300293
    },
    {
      "epoch": 0.11739837398373984,
      "step": 2166,
      "training_loss": 7.04753303527832
    },
    {
      "epoch": 0.11745257452574526,
      "step": 2167,
      "training_loss": 7.5309343338012695
    },
    {
      "epoch": 0.11750677506775067,
      "grad_norm": 22.33063316345215,
      "learning_rate": 1e-05,
      "loss": 6.7566,
      "step": 2168
    },
    {
      "epoch": 0.11750677506775067,
      "step": 2168,
      "training_loss": 6.14025354385376
    },
    {
      "epoch": 0.11756097560975609,
      "step": 2169,
      "training_loss": 6.563918590545654
    },
    {
      "epoch": 0.11761517615176152,
      "step": 2170,
      "training_loss": 7.859302520751953
    },
    {
      "epoch": 0.11766937669376694,
      "step": 2171,
      "training_loss": 6.076991558074951
    },
    {
      "epoch": 0.11772357723577236,
      "grad_norm": 17.46418571472168,
      "learning_rate": 1e-05,
      "loss": 6.6601,
      "step": 2172
    },
    {
      "epoch": 0.11772357723577236,
      "step": 2172,
      "training_loss": 5.954078674316406
    },
    {
      "epoch": 0.11777777777777777,
      "step": 2173,
      "training_loss": 7.260891914367676
    },
    {
      "epoch": 0.1178319783197832,
      "step": 2174,
      "training_loss": 9.489696502685547
    },
    {
      "epoch": 0.11788617886178862,
      "step": 2175,
      "training_loss": 8.630245208740234
    },
    {
      "epoch": 0.11794037940379404,
      "grad_norm": 34.31849670410156,
      "learning_rate": 1e-05,
      "loss": 7.8337,
      "step": 2176
    },
    {
      "epoch": 0.11794037940379404,
      "step": 2176,
      "training_loss": 7.6533074378967285
    },
    {
      "epoch": 0.11799457994579945,
      "step": 2177,
      "training_loss": 5.671876430511475
    },
    {
      "epoch": 0.11804878048780487,
      "step": 2178,
      "training_loss": 6.775413990020752
    },
    {
      "epoch": 0.1181029810298103,
      "step": 2179,
      "training_loss": 5.70692253112793
    },
    {
      "epoch": 0.11815718157181572,
      "grad_norm": 18.061006546020508,
      "learning_rate": 1e-05,
      "loss": 6.4519,
      "step": 2180
    },
    {
      "epoch": 0.11815718157181572,
      "step": 2180,
      "training_loss": 7.063594818115234
    },
    {
      "epoch": 0.11821138211382114,
      "step": 2181,
      "training_loss": 7.9276885986328125
    },
    {
      "epoch": 0.11826558265582655,
      "step": 2182,
      "training_loss": 9.29543399810791
    },
    {
      "epoch": 0.11831978319783198,
      "step": 2183,
      "training_loss": 6.55404806137085
    },
    {
      "epoch": 0.1183739837398374,
      "grad_norm": 15.392293930053711,
      "learning_rate": 1e-05,
      "loss": 7.7102,
      "step": 2184
    },
    {
      "epoch": 0.1183739837398374,
      "step": 2184,
      "training_loss": 8.104308128356934
    },
    {
      "epoch": 0.11842818428184282,
      "step": 2185,
      "training_loss": 6.652906894683838
    },
    {
      "epoch": 0.11848238482384824,
      "step": 2186,
      "training_loss": 6.879006385803223
    },
    {
      "epoch": 0.11853658536585365,
      "step": 2187,
      "training_loss": 6.587248802185059
    },
    {
      "epoch": 0.11859078590785908,
      "grad_norm": 18.70789337158203,
      "learning_rate": 1e-05,
      "loss": 7.0559,
      "step": 2188
    },
    {
      "epoch": 0.11859078590785908,
      "step": 2188,
      "training_loss": 6.450517654418945
    },
    {
      "epoch": 0.1186449864498645,
      "step": 2189,
      "training_loss": 5.038748741149902
    },
    {
      "epoch": 0.11869918699186992,
      "step": 2190,
      "training_loss": 8.775609016418457
    },
    {
      "epoch": 0.11875338753387533,
      "step": 2191,
      "training_loss": 6.797126293182373
    },
    {
      "epoch": 0.11880758807588077,
      "grad_norm": 18.439281463623047,
      "learning_rate": 1e-05,
      "loss": 6.7655,
      "step": 2192
    },
    {
      "epoch": 0.11880758807588077,
      "step": 2192,
      "training_loss": 6.416903495788574
    },
    {
      "epoch": 0.11886178861788618,
      "step": 2193,
      "training_loss": 5.5691962242126465
    },
    {
      "epoch": 0.1189159891598916,
      "step": 2194,
      "training_loss": 6.2019944190979
    },
    {
      "epoch": 0.11897018970189702,
      "step": 2195,
      "training_loss": 6.4698991775512695
    },
    {
      "epoch": 0.11902439024390243,
      "grad_norm": 22.409799575805664,
      "learning_rate": 1e-05,
      "loss": 6.1645,
      "step": 2196
    },
    {
      "epoch": 0.11902439024390243,
      "step": 2196,
      "training_loss": 6.789050579071045
    },
    {
      "epoch": 0.11907859078590786,
      "step": 2197,
      "training_loss": 6.300307273864746
    },
    {
      "epoch": 0.11913279132791328,
      "step": 2198,
      "training_loss": 5.709183692932129
    },
    {
      "epoch": 0.1191869918699187,
      "step": 2199,
      "training_loss": 7.36451530456543
    },
    {
      "epoch": 0.11924119241192412,
      "grad_norm": 11.4341402053833,
      "learning_rate": 1e-05,
      "loss": 6.5408,
      "step": 2200
    },
    {
      "epoch": 0.11924119241192412,
      "step": 2200,
      "training_loss": 4.954929828643799
    },
    {
      "epoch": 0.11929539295392953,
      "step": 2201,
      "training_loss": 5.926755428314209
    },
    {
      "epoch": 0.11934959349593496,
      "step": 2202,
      "training_loss": 7.742156982421875
    },
    {
      "epoch": 0.11940379403794038,
      "step": 2203,
      "training_loss": 7.57988166809082
    },
    {
      "epoch": 0.1194579945799458,
      "grad_norm": 28.034393310546875,
      "learning_rate": 1e-05,
      "loss": 6.5509,
      "step": 2204
    },
    {
      "epoch": 0.1194579945799458,
      "step": 2204,
      "training_loss": 8.24657917022705
    },
    {
      "epoch": 0.11951219512195121,
      "step": 2205,
      "training_loss": 6.572117805480957
    },
    {
      "epoch": 0.11956639566395665,
      "step": 2206,
      "training_loss": 7.496987819671631
    },
    {
      "epoch": 0.11962059620596206,
      "step": 2207,
      "training_loss": 7.6184000968933105
    },
    {
      "epoch": 0.11967479674796748,
      "grad_norm": 31.050830841064453,
      "learning_rate": 1e-05,
      "loss": 7.4835,
      "step": 2208
    },
    {
      "epoch": 0.11967479674796748,
      "step": 2208,
      "training_loss": 4.887528419494629
    },
    {
      "epoch": 0.1197289972899729,
      "step": 2209,
      "training_loss": 6.160145282745361
    },
    {
      "epoch": 0.11978319783197831,
      "step": 2210,
      "training_loss": 7.176706790924072
    },
    {
      "epoch": 0.11983739837398374,
      "step": 2211,
      "training_loss": 6.819027423858643
    },
    {
      "epoch": 0.11989159891598916,
      "grad_norm": 19.507844924926758,
      "learning_rate": 1e-05,
      "loss": 6.2609,
      "step": 2212
    },
    {
      "epoch": 0.11989159891598916,
      "step": 2212,
      "training_loss": 7.826619625091553
    },
    {
      "epoch": 0.11994579945799458,
      "step": 2213,
      "training_loss": 7.772364616394043
    },
    {
      "epoch": 0.12,
      "step": 2214,
      "training_loss": 5.403945446014404
    },
    {
      "epoch": 0.12005420054200543,
      "step": 2215,
      "training_loss": 8.485770225524902
    },
    {
      "epoch": 0.12010840108401084,
      "grad_norm": 32.67414093017578,
      "learning_rate": 1e-05,
      "loss": 7.3722,
      "step": 2216
    },
    {
      "epoch": 0.12010840108401084,
      "step": 2216,
      "training_loss": 7.541272163391113
    },
    {
      "epoch": 0.12016260162601626,
      "step": 2217,
      "training_loss": 7.084115028381348
    },
    {
      "epoch": 0.12021680216802168,
      "step": 2218,
      "training_loss": 6.98788595199585
    },
    {
      "epoch": 0.1202710027100271,
      "step": 2219,
      "training_loss": 5.689939975738525
    },
    {
      "epoch": 0.12032520325203253,
      "grad_norm": 33.34212875366211,
      "learning_rate": 1e-05,
      "loss": 6.8258,
      "step": 2220
    },
    {
      "epoch": 0.12032520325203253,
      "step": 2220,
      "training_loss": 7.090573787689209
    },
    {
      "epoch": 0.12037940379403794,
      "step": 2221,
      "training_loss": 5.047291278839111
    },
    {
      "epoch": 0.12043360433604336,
      "step": 2222,
      "training_loss": 6.384059429168701
    },
    {
      "epoch": 0.12048780487804878,
      "step": 2223,
      "training_loss": 7.2189717292785645
    },
    {
      "epoch": 0.12054200542005421,
      "grad_norm": 18.934520721435547,
      "learning_rate": 1e-05,
      "loss": 6.4352,
      "step": 2224
    },
    {
      "epoch": 0.12054200542005421,
      "step": 2224,
      "training_loss": 6.766025066375732
    },
    {
      "epoch": 0.12059620596205962,
      "step": 2225,
      "training_loss": 7.633221626281738
    },
    {
      "epoch": 0.12065040650406504,
      "step": 2226,
      "training_loss": 7.633810997009277
    },
    {
      "epoch": 0.12070460704607046,
      "step": 2227,
      "training_loss": 7.482257843017578
    },
    {
      "epoch": 0.12075880758807588,
      "grad_norm": 23.372957229614258,
      "learning_rate": 1e-05,
      "loss": 7.3788,
      "step": 2228
    },
    {
      "epoch": 0.12075880758807588,
      "step": 2228,
      "training_loss": 6.6373291015625
    },
    {
      "epoch": 0.1208130081300813,
      "step": 2229,
      "training_loss": 5.038592338562012
    },
    {
      "epoch": 0.12086720867208672,
      "step": 2230,
      "training_loss": 8.039042472839355
    },
    {
      "epoch": 0.12092140921409214,
      "step": 2231,
      "training_loss": 6.464637756347656
    },
    {
      "epoch": 0.12097560975609756,
      "grad_norm": 33.631282806396484,
      "learning_rate": 1e-05,
      "loss": 6.5449,
      "step": 2232
    },
    {
      "epoch": 0.12097560975609756,
      "step": 2232,
      "training_loss": 7.694478988647461
    },
    {
      "epoch": 0.12102981029810297,
      "step": 2233,
      "training_loss": 6.953336238861084
    },
    {
      "epoch": 0.1210840108401084,
      "step": 2234,
      "training_loss": 6.987929821014404
    },
    {
      "epoch": 0.12113821138211382,
      "step": 2235,
      "training_loss": 7.201374053955078
    },
    {
      "epoch": 0.12119241192411924,
      "grad_norm": 24.31184959411621,
      "learning_rate": 1e-05,
      "loss": 7.2093,
      "step": 2236
    },
    {
      "epoch": 0.12119241192411924,
      "step": 2236,
      "training_loss": 6.796027660369873
    },
    {
      "epoch": 0.12124661246612466,
      "step": 2237,
      "training_loss": 7.678626537322998
    },
    {
      "epoch": 0.12130081300813009,
      "step": 2238,
      "training_loss": 6.350281715393066
    },
    {
      "epoch": 0.1213550135501355,
      "step": 2239,
      "training_loss": 7.147307872772217
    },
    {
      "epoch": 0.12140921409214092,
      "grad_norm": 18.23291015625,
      "learning_rate": 1e-05,
      "loss": 6.9931,
      "step": 2240
    },
    {
      "epoch": 0.12140921409214092,
      "step": 2240,
      "training_loss": 7.355422496795654
    },
    {
      "epoch": 0.12146341463414634,
      "step": 2241,
      "training_loss": 6.673851490020752
    },
    {
      "epoch": 0.12151761517615176,
      "step": 2242,
      "training_loss": 6.4347920417785645
    },
    {
      "epoch": 0.12157181571815719,
      "step": 2243,
      "training_loss": 6.456027984619141
    },
    {
      "epoch": 0.1216260162601626,
      "grad_norm": 16.660287857055664,
      "learning_rate": 1e-05,
      "loss": 6.73,
      "step": 2244
    },
    {
      "epoch": 0.1216260162601626,
      "step": 2244,
      "training_loss": 7.085278034210205
    },
    {
      "epoch": 0.12168021680216802,
      "step": 2245,
      "training_loss": 6.997116565704346
    },
    {
      "epoch": 0.12173441734417344,
      "step": 2246,
      "training_loss": 6.541682720184326
    },
    {
      "epoch": 0.12178861788617887,
      "step": 2247,
      "training_loss": 5.74325704574585
    },
    {
      "epoch": 0.12184281842818429,
      "grad_norm": 14.664073944091797,
      "learning_rate": 1e-05,
      "loss": 6.5918,
      "step": 2248
    },
    {
      "epoch": 0.12184281842818429,
      "step": 2248,
      "training_loss": 4.703540802001953
    },
    {
      "epoch": 0.1218970189701897,
      "step": 2249,
      "training_loss": 5.0067267417907715
    },
    {
      "epoch": 0.12195121951219512,
      "step": 2250,
      "training_loss": 5.738106727600098
    },
    {
      "epoch": 0.12200542005420054,
      "step": 2251,
      "training_loss": 7.200331211090088
    },
    {
      "epoch": 0.12205962059620597,
      "grad_norm": 19.849672317504883,
      "learning_rate": 1e-05,
      "loss": 5.6622,
      "step": 2252
    },
    {
      "epoch": 0.12205962059620597,
      "step": 2252,
      "training_loss": 7.078901767730713
    },
    {
      "epoch": 0.12211382113821138,
      "step": 2253,
      "training_loss": 7.500701427459717
    },
    {
      "epoch": 0.1221680216802168,
      "step": 2254,
      "training_loss": 5.436148643493652
    },
    {
      "epoch": 0.12222222222222222,
      "step": 2255,
      "training_loss": 7.110401153564453
    },
    {
      "epoch": 0.12227642276422765,
      "grad_norm": 29.79174041748047,
      "learning_rate": 1e-05,
      "loss": 6.7815,
      "step": 2256
    },
    {
      "epoch": 0.12227642276422765,
      "step": 2256,
      "training_loss": 7.0023369789123535
    },
    {
      "epoch": 0.12233062330623307,
      "step": 2257,
      "training_loss": 7.499988555908203
    },
    {
      "epoch": 0.12238482384823848,
      "step": 2258,
      "training_loss": 7.7180986404418945
    },
    {
      "epoch": 0.1224390243902439,
      "step": 2259,
      "training_loss": 7.783355236053467
    },
    {
      "epoch": 0.12249322493224932,
      "grad_norm": 21.479135513305664,
      "learning_rate": 1e-05,
      "loss": 7.5009,
      "step": 2260
    },
    {
      "epoch": 0.12249322493224932,
      "step": 2260,
      "training_loss": 7.4851884841918945
    },
    {
      "epoch": 0.12254742547425475,
      "step": 2261,
      "training_loss": 6.882392883300781
    },
    {
      "epoch": 0.12260162601626017,
      "step": 2262,
      "training_loss": 6.7331366539001465
    },
    {
      "epoch": 0.12265582655826558,
      "step": 2263,
      "training_loss": 9.494770050048828
    },
    {
      "epoch": 0.122710027100271,
      "grad_norm": 46.98485565185547,
      "learning_rate": 1e-05,
      "loss": 7.6489,
      "step": 2264
    },
    {
      "epoch": 0.122710027100271,
      "step": 2264,
      "training_loss": 7.01367712020874
    },
    {
      "epoch": 0.12276422764227642,
      "step": 2265,
      "training_loss": 6.715948104858398
    },
    {
      "epoch": 0.12281842818428185,
      "step": 2266,
      "training_loss": 6.553761005401611
    },
    {
      "epoch": 0.12287262872628726,
      "step": 2267,
      "training_loss": 4.769719123840332
    },
    {
      "epoch": 0.12292682926829268,
      "grad_norm": 17.23247528076172,
      "learning_rate": 1e-05,
      "loss": 6.2633,
      "step": 2268
    },
    {
      "epoch": 0.12292682926829268,
      "step": 2268,
      "training_loss": 6.697353363037109
    },
    {
      "epoch": 0.1229810298102981,
      "step": 2269,
      "training_loss": 4.626371383666992
    },
    {
      "epoch": 0.12303523035230353,
      "step": 2270,
      "training_loss": 7.181683540344238
    },
    {
      "epoch": 0.12308943089430895,
      "step": 2271,
      "training_loss": 8.144038200378418
    },
    {
      "epoch": 0.12314363143631436,
      "grad_norm": 20.310401916503906,
      "learning_rate": 1e-05,
      "loss": 6.6624,
      "step": 2272
    },
    {
      "epoch": 0.12314363143631436,
      "step": 2272,
      "training_loss": 7.420018672943115
    },
    {
      "epoch": 0.12319783197831978,
      "step": 2273,
      "training_loss": 7.878206729888916
    },
    {
      "epoch": 0.1232520325203252,
      "step": 2274,
      "training_loss": 6.313616752624512
    },
    {
      "epoch": 0.12330623306233063,
      "step": 2275,
      "training_loss": 7.545258522033691
    },
    {
      "epoch": 0.12336043360433604,
      "grad_norm": 32.66688919067383,
      "learning_rate": 1e-05,
      "loss": 7.2893,
      "step": 2276
    },
    {
      "epoch": 0.12336043360433604,
      "step": 2276,
      "training_loss": 7.857612609863281
    },
    {
      "epoch": 0.12341463414634146,
      "step": 2277,
      "training_loss": 6.832972526550293
    },
    {
      "epoch": 0.12346883468834688,
      "step": 2278,
      "training_loss": 7.7751851081848145
    },
    {
      "epoch": 0.12352303523035231,
      "step": 2279,
      "training_loss": 7.564754486083984
    },
    {
      "epoch": 0.12357723577235773,
      "grad_norm": 21.442054748535156,
      "learning_rate": 1e-05,
      "loss": 7.5076,
      "step": 2280
    },
    {
      "epoch": 0.12357723577235773,
      "step": 2280,
      "training_loss": 8.066852569580078
    },
    {
      "epoch": 0.12363143631436314,
      "step": 2281,
      "training_loss": 6.586187839508057
    },
    {
      "epoch": 0.12368563685636856,
      "step": 2282,
      "training_loss": 7.1680521965026855
    },
    {
      "epoch": 0.12373983739837398,
      "step": 2283,
      "training_loss": 4.69591760635376
    },
    {
      "epoch": 0.12379403794037941,
      "grad_norm": 18.26604461669922,
      "learning_rate": 1e-05,
      "loss": 6.6293,
      "step": 2284
    },
    {
      "epoch": 0.12379403794037941,
      "step": 2284,
      "training_loss": 6.899262428283691
    },
    {
      "epoch": 0.12384823848238483,
      "step": 2285,
      "training_loss": 8.297028541564941
    },
    {
      "epoch": 0.12390243902439024,
      "step": 2286,
      "training_loss": 6.748493671417236
    },
    {
      "epoch": 0.12395663956639566,
      "step": 2287,
      "training_loss": 5.633950710296631
    },
    {
      "epoch": 0.12401084010840109,
      "grad_norm": 21.80858039855957,
      "learning_rate": 1e-05,
      "loss": 6.8947,
      "step": 2288
    },
    {
      "epoch": 0.12401084010840109,
      "step": 2288,
      "training_loss": 7.935492992401123
    },
    {
      "epoch": 0.12406504065040651,
      "step": 2289,
      "training_loss": 6.318075656890869
    },
    {
      "epoch": 0.12411924119241192,
      "step": 2290,
      "training_loss": 8.735027313232422
    },
    {
      "epoch": 0.12417344173441734,
      "step": 2291,
      "training_loss": 7.181682109832764
    },
    {
      "epoch": 0.12422764227642276,
      "grad_norm": 25.777793884277344,
      "learning_rate": 1e-05,
      "loss": 7.5426,
      "step": 2292
    },
    {
      "epoch": 0.12422764227642276,
      "step": 2292,
      "training_loss": 6.749355792999268
    },
    {
      "epoch": 0.12428184281842819,
      "step": 2293,
      "training_loss": 7.916969299316406
    },
    {
      "epoch": 0.1243360433604336,
      "step": 2294,
      "training_loss": 7.639201641082764
    },
    {
      "epoch": 0.12439024390243902,
      "step": 2295,
      "training_loss": 5.085854530334473
    },
    {
      "epoch": 0.12444444444444444,
      "grad_norm": 23.33244514465332,
      "learning_rate": 1e-05,
      "loss": 6.8478,
      "step": 2296
    },
    {
      "epoch": 0.12444444444444444,
      "step": 2296,
      "training_loss": 7.673794269561768
    },
    {
      "epoch": 0.12449864498644986,
      "step": 2297,
      "training_loss": 7.263751983642578
    },
    {
      "epoch": 0.12455284552845529,
      "step": 2298,
      "training_loss": 7.893367290496826
    },
    {
      "epoch": 0.1246070460704607,
      "step": 2299,
      "training_loss": 6.223792552947998
    },
    {
      "epoch": 0.12466124661246612,
      "grad_norm": 26.077638626098633,
      "learning_rate": 1e-05,
      "loss": 7.2637,
      "step": 2300
    },
    {
      "epoch": 0.12466124661246612,
      "step": 2300,
      "training_loss": 7.192048072814941
    },
    {
      "epoch": 0.12471544715447154,
      "step": 2301,
      "training_loss": 5.456752300262451
    },
    {
      "epoch": 0.12476964769647697,
      "step": 2302,
      "training_loss": 7.792191028594971
    },
    {
      "epoch": 0.12482384823848239,
      "step": 2303,
      "training_loss": 6.6484270095825195
    },
    {
      "epoch": 0.1248780487804878,
      "grad_norm": 33.54572296142578,
      "learning_rate": 1e-05,
      "loss": 6.7724,
      "step": 2304
    },
    {
      "epoch": 0.1248780487804878,
      "step": 2304,
      "training_loss": 5.881302833557129
    },
    {
      "epoch": 0.12493224932249322,
      "step": 2305,
      "training_loss": 6.88964319229126
    },
    {
      "epoch": 0.12498644986449864,
      "step": 2306,
      "training_loss": 6.558231353759766
    },
    {
      "epoch": 0.12504065040650406,
      "step": 2307,
      "training_loss": 5.7404398918151855
    },
    {
      "epoch": 0.12509485094850947,
      "grad_norm": 16.859508514404297,
      "learning_rate": 1e-05,
      "loss": 6.2674,
      "step": 2308
    },
    {
      "epoch": 0.12509485094850947,
      "step": 2308,
      "training_loss": 5.9354681968688965
    },
    {
      "epoch": 0.12514905149051492,
      "step": 2309,
      "training_loss": 7.143780708312988
    },
    {
      "epoch": 0.12520325203252033,
      "step": 2310,
      "training_loss": 6.716307640075684
    },
    {
      "epoch": 0.12525745257452575,
      "step": 2311,
      "training_loss": 6.3847808837890625
    },
    {
      "epoch": 0.12531165311653117,
      "grad_norm": 15.5435152053833,
      "learning_rate": 1e-05,
      "loss": 6.5451,
      "step": 2312
    },
    {
      "epoch": 0.12531165311653117,
      "step": 2312,
      "training_loss": 5.398257732391357
    },
    {
      "epoch": 0.12536585365853659,
      "step": 2313,
      "training_loss": 6.784984111785889
    },
    {
      "epoch": 0.125420054200542,
      "step": 2314,
      "training_loss": 7.342296123504639
    },
    {
      "epoch": 0.12547425474254742,
      "step": 2315,
      "training_loss": 5.805182933807373
    },
    {
      "epoch": 0.12552845528455284,
      "grad_norm": 24.448890686035156,
      "learning_rate": 1e-05,
      "loss": 6.3327,
      "step": 2316
    },
    {
      "epoch": 0.12552845528455284,
      "step": 2316,
      "training_loss": 6.260063171386719
    },
    {
      "epoch": 0.12558265582655825,
      "step": 2317,
      "training_loss": 7.237641334533691
    },
    {
      "epoch": 0.1256368563685637,
      "step": 2318,
      "training_loss": 7.885072708129883
    },
    {
      "epoch": 0.12569105691056912,
      "step": 2319,
      "training_loss": 6.006633281707764
    },
    {
      "epoch": 0.12574525745257453,
      "grad_norm": 14.371999740600586,
      "learning_rate": 1e-05,
      "loss": 6.8474,
      "step": 2320
    },
    {
      "epoch": 0.12574525745257453,
      "step": 2320,
      "training_loss": 7.109259128570557
    },
    {
      "epoch": 0.12579945799457995,
      "step": 2321,
      "training_loss": 6.004148960113525
    },
    {
      "epoch": 0.12585365853658537,
      "step": 2322,
      "training_loss": 6.886720657348633
    },
    {
      "epoch": 0.12590785907859078,
      "step": 2323,
      "training_loss": 8.63757038116455
    },
    {
      "epoch": 0.1259620596205962,
      "grad_norm": 42.692726135253906,
      "learning_rate": 1e-05,
      "loss": 7.1594,
      "step": 2324
    },
    {
      "epoch": 0.1259620596205962,
      "step": 2324,
      "training_loss": 9.097541809082031
    },
    {
      "epoch": 0.12601626016260162,
      "step": 2325,
      "training_loss": 6.228115558624268
    },
    {
      "epoch": 0.12607046070460703,
      "step": 2326,
      "training_loss": 6.93276309967041
    },
    {
      "epoch": 0.12612466124661248,
      "step": 2327,
      "training_loss": 7.2412428855896
    },
    {
      "epoch": 0.1261788617886179,
      "grad_norm": 18.923810958862305,
      "learning_rate": 1e-05,
      "loss": 7.3749,
      "step": 2328
    },
    {
      "epoch": 0.1261788617886179,
      "step": 2328,
      "training_loss": 7.3738603591918945
    },
    {
      "epoch": 0.1262330623306233,
      "step": 2329,
      "training_loss": 6.653367042541504
    },
    {
      "epoch": 0.12628726287262873,
      "step": 2330,
      "training_loss": 7.742931842803955
    },
    {
      "epoch": 0.12634146341463415,
      "step": 2331,
      "training_loss": 7.738585472106934
    },
    {
      "epoch": 0.12639566395663956,
      "grad_norm": 20.050537109375,
      "learning_rate": 1e-05,
      "loss": 7.3772,
      "step": 2332
    },
    {
      "epoch": 0.12639566395663956,
      "step": 2332,
      "training_loss": 7.737110137939453
    },
    {
      "epoch": 0.12644986449864498,
      "step": 2333,
      "training_loss": 8.401562690734863
    },
    {
      "epoch": 0.1265040650406504,
      "step": 2334,
      "training_loss": 6.940257549285889
    },
    {
      "epoch": 0.12655826558265582,
      "step": 2335,
      "training_loss": 5.1582841873168945
    },
    {
      "epoch": 0.12661246612466126,
      "grad_norm": 25.309974670410156,
      "learning_rate": 1e-05,
      "loss": 7.0593,
      "step": 2336
    },
    {
      "epoch": 0.12661246612466126,
      "step": 2336,
      "training_loss": 6.9512810707092285
    },
    {
      "epoch": 0.12666666666666668,
      "step": 2337,
      "training_loss": 7.266663074493408
    },
    {
      "epoch": 0.1267208672086721,
      "step": 2338,
      "training_loss": 7.459133148193359
    },
    {
      "epoch": 0.1267750677506775,
      "step": 2339,
      "training_loss": 7.0086774826049805
    },
    {
      "epoch": 0.12682926829268293,
      "grad_norm": 23.231477737426758,
      "learning_rate": 1e-05,
      "loss": 7.1714,
      "step": 2340
    },
    {
      "epoch": 0.12682926829268293,
      "step": 2340,
      "training_loss": 6.045403957366943
    },
    {
      "epoch": 0.12688346883468835,
      "step": 2341,
      "training_loss": 7.154129981994629
    },
    {
      "epoch": 0.12693766937669376,
      "step": 2342,
      "training_loss": 7.1170148849487305
    },
    {
      "epoch": 0.12699186991869918,
      "step": 2343,
      "training_loss": 8.479242324829102
    },
    {
      "epoch": 0.1270460704607046,
      "grad_norm": 30.86034393310547,
      "learning_rate": 1e-05,
      "loss": 7.1989,
      "step": 2344
    },
    {
      "epoch": 0.1270460704607046,
      "step": 2344,
      "training_loss": 6.553088665008545
    },
    {
      "epoch": 0.12710027100271,
      "step": 2345,
      "training_loss": 6.300509929656982
    },
    {
      "epoch": 0.12715447154471546,
      "step": 2346,
      "training_loss": 8.175697326660156
    },
    {
      "epoch": 0.12720867208672088,
      "step": 2347,
      "training_loss": 7.6241679191589355
    },
    {
      "epoch": 0.1272628726287263,
      "grad_norm": 19.317899703979492,
      "learning_rate": 1e-05,
      "loss": 7.1634,
      "step": 2348
    },
    {
      "epoch": 0.1272628726287263,
      "step": 2348,
      "training_loss": 5.1277031898498535
    },
    {
      "epoch": 0.1273170731707317,
      "step": 2349,
      "training_loss": 7.588439464569092
    },
    {
      "epoch": 0.12737127371273713,
      "step": 2350,
      "training_loss": 7.3179473876953125
    },
    {
      "epoch": 0.12742547425474254,
      "step": 2351,
      "training_loss": 6.514327049255371
    },
    {
      "epoch": 0.12747967479674796,
      "grad_norm": 27.796995162963867,
      "learning_rate": 1e-05,
      "loss": 6.6371,
      "step": 2352
    },
    {
      "epoch": 0.12747967479674796,
      "step": 2352,
      "training_loss": 6.474440097808838
    },
    {
      "epoch": 0.12753387533875338,
      "step": 2353,
      "training_loss": 5.062373161315918
    },
    {
      "epoch": 0.1275880758807588,
      "step": 2354,
      "training_loss": 6.981166362762451
    },
    {
      "epoch": 0.12764227642276424,
      "step": 2355,
      "training_loss": 7.293150901794434
    },
    {
      "epoch": 0.12769647696476966,
      "grad_norm": 23.094104766845703,
      "learning_rate": 1e-05,
      "loss": 6.4528,
      "step": 2356
    },
    {
      "epoch": 0.12769647696476966,
      "step": 2356,
      "training_loss": 6.672297477722168
    },
    {
      "epoch": 0.12775067750677507,
      "step": 2357,
      "training_loss": 7.342505931854248
    },
    {
      "epoch": 0.1278048780487805,
      "step": 2358,
      "training_loss": 7.209371089935303
    },
    {
      "epoch": 0.1278590785907859,
      "step": 2359,
      "training_loss": 7.44959831237793
    },
    {
      "epoch": 0.12791327913279132,
      "grad_norm": 15.741528511047363,
      "learning_rate": 1e-05,
      "loss": 7.1684,
      "step": 2360
    },
    {
      "epoch": 0.12791327913279132,
      "step": 2360,
      "training_loss": 7.147550582885742
    },
    {
      "epoch": 0.12796747967479674,
      "step": 2361,
      "training_loss": 9.249164581298828
    },
    {
      "epoch": 0.12802168021680216,
      "step": 2362,
      "training_loss": 7.309779167175293
    },
    {
      "epoch": 0.12807588075880758,
      "step": 2363,
      "training_loss": 7.205150127410889
    },
    {
      "epoch": 0.12813008130081302,
      "grad_norm": 16.930511474609375,
      "learning_rate": 1e-05,
      "loss": 7.7279,
      "step": 2364
    },
    {
      "epoch": 0.12813008130081302,
      "step": 2364,
      "training_loss": 6.0998759269714355
    },
    {
      "epoch": 0.12818428184281844,
      "step": 2365,
      "training_loss": 8.268562316894531
    },
    {
      "epoch": 0.12823848238482385,
      "step": 2366,
      "training_loss": 8.989215850830078
    },
    {
      "epoch": 0.12829268292682927,
      "step": 2367,
      "training_loss": 7.231636047363281
    },
    {
      "epoch": 0.1283468834688347,
      "grad_norm": 19.490684509277344,
      "learning_rate": 1e-05,
      "loss": 7.6473,
      "step": 2368
    },
    {
      "epoch": 0.1283468834688347,
      "step": 2368,
      "training_loss": 6.891519546508789
    },
    {
      "epoch": 0.1284010840108401,
      "step": 2369,
      "training_loss": 7.0190229415893555
    },
    {
      "epoch": 0.12845528455284552,
      "step": 2370,
      "training_loss": 5.833025932312012
    },
    {
      "epoch": 0.12850948509485094,
      "step": 2371,
      "training_loss": 7.318814754486084
    },
    {
      "epoch": 0.12856368563685636,
      "grad_norm": 19.696821212768555,
      "learning_rate": 1e-05,
      "loss": 6.7656,
      "step": 2372
    },
    {
      "epoch": 0.12856368563685636,
      "step": 2372,
      "training_loss": 7.996345520019531
    },
    {
      "epoch": 0.1286178861788618,
      "step": 2373,
      "training_loss": 7.4722065925598145
    },
    {
      "epoch": 0.12867208672086722,
      "step": 2374,
      "training_loss": 7.2403950691223145
    },
    {
      "epoch": 0.12872628726287264,
      "step": 2375,
      "training_loss": 6.946951866149902
    },
    {
      "epoch": 0.12878048780487805,
      "grad_norm": 18.996192932128906,
      "learning_rate": 1e-05,
      "loss": 7.414,
      "step": 2376
    },
    {
      "epoch": 0.12878048780487805,
      "step": 2376,
      "training_loss": 6.9443559646606445
    },
    {
      "epoch": 0.12883468834688347,
      "step": 2377,
      "training_loss": 10.435005187988281
    },
    {
      "epoch": 0.1288888888888889,
      "step": 2378,
      "training_loss": 8.213976860046387
    },
    {
      "epoch": 0.1289430894308943,
      "step": 2379,
      "training_loss": 7.399564743041992
    },
    {
      "epoch": 0.12899728997289972,
      "grad_norm": 18.470012664794922,
      "learning_rate": 1e-05,
      "loss": 8.2482,
      "step": 2380
    },
    {
      "epoch": 0.12899728997289972,
      "step": 2380,
      "training_loss": 6.8458147048950195
    },
    {
      "epoch": 0.12905149051490514,
      "step": 2381,
      "training_loss": 7.100912570953369
    },
    {
      "epoch": 0.12910569105691058,
      "step": 2382,
      "training_loss": 8.331835746765137
    },
    {
      "epoch": 0.129159891598916,
      "step": 2383,
      "training_loss": 7.173605442047119
    },
    {
      "epoch": 0.12921409214092142,
      "grad_norm": 17.97764778137207,
      "learning_rate": 1e-05,
      "loss": 7.363,
      "step": 2384
    },
    {
      "epoch": 0.12921409214092142,
      "step": 2384,
      "training_loss": 8.817522048950195
    },
    {
      "epoch": 0.12926829268292683,
      "step": 2385,
      "training_loss": 7.092715263366699
    },
    {
      "epoch": 0.12932249322493225,
      "step": 2386,
      "training_loss": 5.736758232116699
    },
    {
      "epoch": 0.12937669376693767,
      "step": 2387,
      "training_loss": 7.840070724487305
    },
    {
      "epoch": 0.12943089430894308,
      "grad_norm": 16.82536506652832,
      "learning_rate": 1e-05,
      "loss": 7.3718,
      "step": 2388
    },
    {
      "epoch": 0.12943089430894308,
      "step": 2388,
      "training_loss": 6.460188865661621
    },
    {
      "epoch": 0.1294850948509485,
      "step": 2389,
      "training_loss": 7.762294292449951
    },
    {
      "epoch": 0.12953929539295392,
      "step": 2390,
      "training_loss": 7.433584213256836
    },
    {
      "epoch": 0.12959349593495936,
      "step": 2391,
      "training_loss": 6.45660400390625
    },
    {
      "epoch": 0.12964769647696478,
      "grad_norm": 16.68767738342285,
      "learning_rate": 1e-05,
      "loss": 7.0282,
      "step": 2392
    },
    {
      "epoch": 0.12964769647696478,
      "step": 2392,
      "training_loss": 5.8159308433532715
    },
    {
      "epoch": 0.1297018970189702,
      "step": 2393,
      "training_loss": 7.891869068145752
    },
    {
      "epoch": 0.12975609756097561,
      "step": 2394,
      "training_loss": 6.140820503234863
    },
    {
      "epoch": 0.12981029810298103,
      "step": 2395,
      "training_loss": 6.949145317077637
    },
    {
      "epoch": 0.12986449864498645,
      "grad_norm": 24.94117546081543,
      "learning_rate": 1e-05,
      "loss": 6.6994,
      "step": 2396
    },
    {
      "epoch": 0.12986449864498645,
      "step": 2396,
      "training_loss": 6.84550142288208
    },
    {
      "epoch": 0.12991869918699187,
      "step": 2397,
      "training_loss": 6.777259349822998
    },
    {
      "epoch": 0.12997289972899728,
      "step": 2398,
      "training_loss": 7.136823654174805
    },
    {
      "epoch": 0.1300271002710027,
      "step": 2399,
      "training_loss": 8.293983459472656
    },
    {
      "epoch": 0.13008130081300814,
      "grad_norm": 22.999238967895508,
      "learning_rate": 1e-05,
      "loss": 7.2634,
      "step": 2400
    },
    {
      "epoch": 0.13008130081300814,
      "step": 2400,
      "training_loss": 6.7133636474609375
    },
    {
      "epoch": 0.13013550135501356,
      "step": 2401,
      "training_loss": 7.599674224853516
    },
    {
      "epoch": 0.13018970189701898,
      "step": 2402,
      "training_loss": 7.431965351104736
    },
    {
      "epoch": 0.1302439024390244,
      "step": 2403,
      "training_loss": 6.959190845489502
    },
    {
      "epoch": 0.1302981029810298,
      "grad_norm": 17.997512817382812,
      "learning_rate": 1e-05,
      "loss": 7.176,
      "step": 2404
    },
    {
      "epoch": 0.1302981029810298,
      "step": 2404,
      "training_loss": 7.372251033782959
    },
    {
      "epoch": 0.13035230352303523,
      "step": 2405,
      "training_loss": 6.6398234367370605
    },
    {
      "epoch": 0.13040650406504065,
      "step": 2406,
      "training_loss": 7.129350662231445
    },
    {
      "epoch": 0.13046070460704606,
      "step": 2407,
      "training_loss": 6.397891044616699
    },
    {
      "epoch": 0.13051490514905148,
      "grad_norm": 33.74754333496094,
      "learning_rate": 1e-05,
      "loss": 6.8848,
      "step": 2408
    },
    {
      "epoch": 0.13051490514905148,
      "step": 2408,
      "training_loss": 5.21405029296875
    },
    {
      "epoch": 0.1305691056910569,
      "step": 2409,
      "training_loss": 6.430717468261719
    },
    {
      "epoch": 0.13062330623306234,
      "step": 2410,
      "training_loss": 5.143457889556885
    },
    {
      "epoch": 0.13067750677506776,
      "step": 2411,
      "training_loss": 7.825999736785889
    },
    {
      "epoch": 0.13073170731707318,
      "grad_norm": 24.811138153076172,
      "learning_rate": 1e-05,
      "loss": 6.1536,
      "step": 2412
    },
    {
      "epoch": 0.13073170731707318,
      "step": 2412,
      "training_loss": 8.10484504699707
    },
    {
      "epoch": 0.1307859078590786,
      "step": 2413,
      "training_loss": 6.983096122741699
    },
    {
      "epoch": 0.130840108401084,
      "step": 2414,
      "training_loss": 5.724494934082031
    },
    {
      "epoch": 0.13089430894308943,
      "step": 2415,
      "training_loss": 6.397602081298828
    },
    {
      "epoch": 0.13094850948509484,
      "grad_norm": 16.252355575561523,
      "learning_rate": 1e-05,
      "loss": 6.8025,
      "step": 2416
    },
    {
      "epoch": 0.13094850948509484,
      "step": 2416,
      "training_loss": 6.791269302368164
    },
    {
      "epoch": 0.13100271002710026,
      "step": 2417,
      "training_loss": 6.114009380340576
    },
    {
      "epoch": 0.13105691056910568,
      "step": 2418,
      "training_loss": 5.585865497589111
    },
    {
      "epoch": 0.13111111111111112,
      "step": 2419,
      "training_loss": 6.7568440437316895
    },
    {
      "epoch": 0.13116531165311654,
      "grad_norm": 17.54572296142578,
      "learning_rate": 1e-05,
      "loss": 6.312,
      "step": 2420
    },
    {
      "epoch": 0.13116531165311654,
      "step": 2420,
      "training_loss": 6.756948947906494
    },
    {
      "epoch": 0.13121951219512196,
      "step": 2421,
      "training_loss": 6.776217460632324
    },
    {
      "epoch": 0.13127371273712737,
      "step": 2422,
      "training_loss": 6.766881465911865
    },
    {
      "epoch": 0.1313279132791328,
      "step": 2423,
      "training_loss": 7.655012130737305
    },
    {
      "epoch": 0.1313821138211382,
      "grad_norm": 18.40644645690918,
      "learning_rate": 1e-05,
      "loss": 6.9888,
      "step": 2424
    },
    {
      "epoch": 0.1313821138211382,
      "step": 2424,
      "training_loss": 7.084962844848633
    },
    {
      "epoch": 0.13143631436314362,
      "step": 2425,
      "training_loss": 6.592479228973389
    },
    {
      "epoch": 0.13149051490514904,
      "step": 2426,
      "training_loss": 6.49119758605957
    },
    {
      "epoch": 0.13154471544715446,
      "step": 2427,
      "training_loss": 6.095357418060303
    },
    {
      "epoch": 0.1315989159891599,
      "grad_norm": 40.02559280395508,
      "learning_rate": 1e-05,
      "loss": 6.566,
      "step": 2428
    },
    {
      "epoch": 0.1315989159891599,
      "step": 2428,
      "training_loss": 7.425017833709717
    },
    {
      "epoch": 0.13165311653116532,
      "step": 2429,
      "training_loss": 7.837277889251709
    },
    {
      "epoch": 0.13170731707317074,
      "step": 2430,
      "training_loss": 7.50784969329834
    },
    {
      "epoch": 0.13176151761517615,
      "step": 2431,
      "training_loss": 7.3861846923828125
    },
    {
      "epoch": 0.13181571815718157,
      "grad_norm": 17.26953887939453,
      "learning_rate": 1e-05,
      "loss": 7.5391,
      "step": 2432
    },
    {
      "epoch": 0.13181571815718157,
      "step": 2432,
      "training_loss": 7.383130073547363
    },
    {
      "epoch": 0.131869918699187,
      "step": 2433,
      "training_loss": 7.492430686950684
    },
    {
      "epoch": 0.1319241192411924,
      "step": 2434,
      "training_loss": 7.4050679206848145
    },
    {
      "epoch": 0.13197831978319782,
      "step": 2435,
      "training_loss": 5.913609027862549
    },
    {
      "epoch": 0.13203252032520324,
      "grad_norm": 22.826536178588867,
      "learning_rate": 1e-05,
      "loss": 7.0486,
      "step": 2436
    },
    {
      "epoch": 0.13203252032520324,
      "step": 2436,
      "training_loss": 7.549715042114258
    },
    {
      "epoch": 0.13208672086720868,
      "step": 2437,
      "training_loss": 8.091358184814453
    },
    {
      "epoch": 0.1321409214092141,
      "step": 2438,
      "training_loss": 8.662674903869629
    },
    {
      "epoch": 0.13219512195121952,
      "step": 2439,
      "training_loss": 7.535335063934326
    },
    {
      "epoch": 0.13224932249322494,
      "grad_norm": 39.024600982666016,
      "learning_rate": 1e-05,
      "loss": 7.9598,
      "step": 2440
    },
    {
      "epoch": 0.13224932249322494,
      "step": 2440,
      "training_loss": 6.582455158233643
    },
    {
      "epoch": 0.13230352303523035,
      "step": 2441,
      "training_loss": 7.434326171875
    },
    {
      "epoch": 0.13235772357723577,
      "step": 2442,
      "training_loss": 6.895413875579834
    },
    {
      "epoch": 0.1324119241192412,
      "step": 2443,
      "training_loss": 6.560115814208984
    },
    {
      "epoch": 0.1324661246612466,
      "grad_norm": 28.078418731689453,
      "learning_rate": 1e-05,
      "loss": 6.8681,
      "step": 2444
    },
    {
      "epoch": 0.1324661246612466,
      "step": 2444,
      "training_loss": 7.337508678436279
    },
    {
      "epoch": 0.13252032520325202,
      "step": 2445,
      "training_loss": 6.848161220550537
    },
    {
      "epoch": 0.13257452574525747,
      "step": 2446,
      "training_loss": 6.6051764488220215
    },
    {
      "epoch": 0.13262872628726288,
      "step": 2447,
      "training_loss": 7.909592151641846
    },
    {
      "epoch": 0.1326829268292683,
      "grad_norm": 29.793598175048828,
      "learning_rate": 1e-05,
      "loss": 7.1751,
      "step": 2448
    },
    {
      "epoch": 0.1326829268292683,
      "step": 2448,
      "training_loss": 7.942841529846191
    },
    {
      "epoch": 0.13273712737127372,
      "step": 2449,
      "training_loss": 7.339183807373047
    },
    {
      "epoch": 0.13279132791327913,
      "step": 2450,
      "training_loss": 7.739979267120361
    },
    {
      "epoch": 0.13284552845528455,
      "step": 2451,
      "training_loss": 7.990419864654541
    },
    {
      "epoch": 0.13289972899728997,
      "grad_norm": 19.519268035888672,
      "learning_rate": 1e-05,
      "loss": 7.7531,
      "step": 2452
    },
    {
      "epoch": 0.13289972899728997,
      "step": 2452,
      "training_loss": 7.545776844024658
    },
    {
      "epoch": 0.13295392953929538,
      "step": 2453,
      "training_loss": 5.994248390197754
    },
    {
      "epoch": 0.1330081300813008,
      "step": 2454,
      "training_loss": 7.444095134735107
    },
    {
      "epoch": 0.13306233062330625,
      "step": 2455,
      "training_loss": 7.267855167388916
    },
    {
      "epoch": 0.13311653116531166,
      "grad_norm": 19.37725067138672,
      "learning_rate": 1e-05,
      "loss": 7.063,
      "step": 2456
    },
    {
      "epoch": 0.13311653116531166,
      "step": 2456,
      "training_loss": 7.487898826599121
    },
    {
      "epoch": 0.13317073170731708,
      "step": 2457,
      "training_loss": 7.750001430511475
    },
    {
      "epoch": 0.1332249322493225,
      "step": 2458,
      "training_loss": 5.965328693389893
    },
    {
      "epoch": 0.13327913279132791,
      "step": 2459,
      "training_loss": 7.28836727142334
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 28.248313903808594,
      "learning_rate": 1e-05,
      "loss": 7.1229,
      "step": 2460
    },
    {
      "epoch": 0.13333333333333333,
      "step": 2460,
      "training_loss": 6.611510753631592
    },
    {
      "epoch": 0.13338753387533875,
      "step": 2461,
      "training_loss": 8.610699653625488
    },
    {
      "epoch": 0.13344173441734417,
      "step": 2462,
      "training_loss": 7.95767068862915
    },
    {
      "epoch": 0.13349593495934958,
      "step": 2463,
      "training_loss": 6.774229526519775
    },
    {
      "epoch": 0.13355013550135503,
      "grad_norm": 19.879915237426758,
      "learning_rate": 1e-05,
      "loss": 7.4885,
      "step": 2464
    },
    {
      "epoch": 0.13355013550135503,
      "step": 2464,
      "training_loss": 7.73317289352417
    },
    {
      "epoch": 0.13360433604336044,
      "step": 2465,
      "training_loss": 7.757315635681152
    },
    {
      "epoch": 0.13365853658536586,
      "step": 2466,
      "training_loss": 7.437290191650391
    },
    {
      "epoch": 0.13371273712737128,
      "step": 2467,
      "training_loss": 7.686046600341797
    },
    {
      "epoch": 0.1337669376693767,
      "grad_norm": 31.695730209350586,
      "learning_rate": 1e-05,
      "loss": 7.6535,
      "step": 2468
    },
    {
      "epoch": 0.1337669376693767,
      "step": 2468,
      "training_loss": 7.093227863311768
    },
    {
      "epoch": 0.1338211382113821,
      "step": 2469,
      "training_loss": 8.419249534606934
    },
    {
      "epoch": 0.13387533875338753,
      "step": 2470,
      "training_loss": 8.040327072143555
    },
    {
      "epoch": 0.13392953929539295,
      "step": 2471,
      "training_loss": 7.246767044067383
    },
    {
      "epoch": 0.13398373983739836,
      "grad_norm": 20.5117130279541,
      "learning_rate": 1e-05,
      "loss": 7.6999,
      "step": 2472
    },
    {
      "epoch": 0.13398373983739836,
      "step": 2472,
      "training_loss": 6.817556381225586
    },
    {
      "epoch": 0.13403794037940378,
      "step": 2473,
      "training_loss": 5.766285419464111
    },
    {
      "epoch": 0.13409214092140923,
      "step": 2474,
      "training_loss": 7.6953816413879395
    },
    {
      "epoch": 0.13414634146341464,
      "step": 2475,
      "training_loss": 6.556519985198975
    },
    {
      "epoch": 0.13420054200542006,
      "grad_norm": 16.53655242919922,
      "learning_rate": 1e-05,
      "loss": 6.7089,
      "step": 2476
    },
    {
      "epoch": 0.13420054200542006,
      "step": 2476,
      "training_loss": 6.717647552490234
    },
    {
      "epoch": 0.13425474254742548,
      "step": 2477,
      "training_loss": 6.436633586883545
    },
    {
      "epoch": 0.1343089430894309,
      "step": 2478,
      "training_loss": 6.870391368865967
    },
    {
      "epoch": 0.1343631436314363,
      "step": 2479,
      "training_loss": 5.539710998535156
    },
    {
      "epoch": 0.13441734417344173,
      "grad_norm": 18.03468894958496,
      "learning_rate": 1e-05,
      "loss": 6.3911,
      "step": 2480
    },
    {
      "epoch": 0.13441734417344173,
      "step": 2480,
      "training_loss": 7.421394348144531
    },
    {
      "epoch": 0.13447154471544714,
      "step": 2481,
      "training_loss": 7.195505142211914
    },
    {
      "epoch": 0.13452574525745256,
      "step": 2482,
      "training_loss": 6.995494842529297
    },
    {
      "epoch": 0.134579945799458,
      "step": 2483,
      "training_loss": 7.791748046875
    },
    {
      "epoch": 0.13463414634146342,
      "grad_norm": 23.69862174987793,
      "learning_rate": 1e-05,
      "loss": 7.351,
      "step": 2484
    },
    {
      "epoch": 0.13463414634146342,
      "step": 2484,
      "training_loss": 7.868696212768555
    },
    {
      "epoch": 0.13468834688346884,
      "step": 2485,
      "training_loss": 7.784933090209961
    },
    {
      "epoch": 0.13474254742547426,
      "step": 2486,
      "training_loss": 4.909270763397217
    },
    {
      "epoch": 0.13479674796747967,
      "step": 2487,
      "training_loss": 5.411971569061279
    },
    {
      "epoch": 0.1348509485094851,
      "grad_norm": 19.184114456176758,
      "learning_rate": 1e-05,
      "loss": 6.4937,
      "step": 2488
    },
    {
      "epoch": 0.1348509485094851,
      "step": 2488,
      "training_loss": 6.562197685241699
    },
    {
      "epoch": 0.1349051490514905,
      "step": 2489,
      "training_loss": 5.837830066680908
    },
    {
      "epoch": 0.13495934959349593,
      "step": 2490,
      "training_loss": 7.1732096672058105
    },
    {
      "epoch": 0.13501355013550134,
      "step": 2491,
      "training_loss": 7.060583591461182
    },
    {
      "epoch": 0.1350677506775068,
      "grad_norm": 19.953832626342773,
      "learning_rate": 1e-05,
      "loss": 6.6585,
      "step": 2492
    },
    {
      "epoch": 0.1350677506775068,
      "step": 2492,
      "training_loss": 7.216676235198975
    },
    {
      "epoch": 0.1351219512195122,
      "step": 2493,
      "training_loss": 6.966799736022949
    },
    {
      "epoch": 0.13517615176151762,
      "step": 2494,
      "training_loss": 7.49847936630249
    },
    {
      "epoch": 0.13523035230352304,
      "step": 2495,
      "training_loss": 4.347060203552246
    },
    {
      "epoch": 0.13528455284552846,
      "grad_norm": 35.6468620300293,
      "learning_rate": 1e-05,
      "loss": 6.5073,
      "step": 2496
    },
    {
      "epoch": 0.13528455284552846,
      "step": 2496,
      "training_loss": 6.736688613891602
    },
    {
      "epoch": 0.13533875338753387,
      "step": 2497,
      "training_loss": 6.622025489807129
    },
    {
      "epoch": 0.1353929539295393,
      "step": 2498,
      "training_loss": 7.7705488204956055
    },
    {
      "epoch": 0.1354471544715447,
      "step": 2499,
      "training_loss": 6.626865863800049
    },
    {
      "epoch": 0.13550135501355012,
      "grad_norm": 20.903995513916016,
      "learning_rate": 1e-05,
      "loss": 6.939,
      "step": 2500
    },
    {
      "epoch": 0.13550135501355012,
      "step": 2500,
      "training_loss": 6.2944865226745605
    },
    {
      "epoch": 0.13555555555555557,
      "step": 2501,
      "training_loss": 7.82246732711792
    },
    {
      "epoch": 0.13560975609756099,
      "step": 2502,
      "training_loss": 7.1069254875183105
    },
    {
      "epoch": 0.1356639566395664,
      "step": 2503,
      "training_loss": 7.054758071899414
    },
    {
      "epoch": 0.13571815718157182,
      "grad_norm": 16.166383743286133,
      "learning_rate": 1e-05,
      "loss": 7.0697,
      "step": 2504
    },
    {
      "epoch": 0.13571815718157182,
      "step": 2504,
      "training_loss": 6.958922863006592
    },
    {
      "epoch": 0.13577235772357724,
      "step": 2505,
      "training_loss": 6.614577770233154
    },
    {
      "epoch": 0.13582655826558265,
      "step": 2506,
      "training_loss": 7.345183849334717
    },
    {
      "epoch": 0.13588075880758807,
      "step": 2507,
      "training_loss": 6.462575435638428
    },
    {
      "epoch": 0.1359349593495935,
      "grad_norm": 67.98273468017578,
      "learning_rate": 1e-05,
      "loss": 6.8453,
      "step": 2508
    },
    {
      "epoch": 0.1359349593495935,
      "step": 2508,
      "training_loss": 6.912648677825928
    },
    {
      "epoch": 0.1359891598915989,
      "step": 2509,
      "training_loss": 7.60536003112793
    },
    {
      "epoch": 0.13604336043360435,
      "step": 2510,
      "training_loss": 7.151080131530762
    },
    {
      "epoch": 0.13609756097560977,
      "step": 2511,
      "training_loss": 6.910722255706787
    },
    {
      "epoch": 0.13615176151761518,
      "grad_norm": 30.519861221313477,
      "learning_rate": 1e-05,
      "loss": 7.145,
      "step": 2512
    },
    {
      "epoch": 0.13615176151761518,
      "step": 2512,
      "training_loss": 7.603370189666748
    },
    {
      "epoch": 0.1362059620596206,
      "step": 2513,
      "training_loss": 6.863235950469971
    },
    {
      "epoch": 0.13626016260162602,
      "step": 2514,
      "training_loss": 7.850847244262695
    },
    {
      "epoch": 0.13631436314363143,
      "step": 2515,
      "training_loss": 6.552717208862305
    },
    {
      "epoch": 0.13636856368563685,
      "grad_norm": 20.339080810546875,
      "learning_rate": 1e-05,
      "loss": 7.2175,
      "step": 2516
    },
    {
      "epoch": 0.13636856368563685,
      "step": 2516,
      "training_loss": 7.756474494934082
    },
    {
      "epoch": 0.13642276422764227,
      "step": 2517,
      "training_loss": 7.400864601135254
    },
    {
      "epoch": 0.13647696476964769,
      "step": 2518,
      "training_loss": 7.056346893310547
    },
    {
      "epoch": 0.13653116531165313,
      "step": 2519,
      "training_loss": 7.149261951446533
    },
    {
      "epoch": 0.13658536585365855,
      "grad_norm": 17.49959373474121,
      "learning_rate": 1e-05,
      "loss": 7.3407,
      "step": 2520
    },
    {
      "epoch": 0.13658536585365855,
      "step": 2520,
      "training_loss": 7.862305641174316
    },
    {
      "epoch": 0.13663956639566396,
      "step": 2521,
      "training_loss": 4.291009902954102
    },
    {
      "epoch": 0.13669376693766938,
      "step": 2522,
      "training_loss": 6.7023115158081055
    },
    {
      "epoch": 0.1367479674796748,
      "step": 2523,
      "training_loss": 6.281989097595215
    },
    {
      "epoch": 0.13680216802168021,
      "grad_norm": 25.290563583374023,
      "learning_rate": 1e-05,
      "loss": 6.2844,
      "step": 2524
    },
    {
      "epoch": 0.13680216802168021,
      "step": 2524,
      "training_loss": 7.517651081085205
    },
    {
      "epoch": 0.13685636856368563,
      "step": 2525,
      "training_loss": 7.353532314300537
    },
    {
      "epoch": 0.13691056910569105,
      "step": 2526,
      "training_loss": 7.968490123748779
    },
    {
      "epoch": 0.13696476964769647,
      "step": 2527,
      "training_loss": 6.855169296264648
    },
    {
      "epoch": 0.1370189701897019,
      "grad_norm": 33.6573371887207,
      "learning_rate": 1e-05,
      "loss": 7.4237,
      "step": 2528
    },
    {
      "epoch": 0.1370189701897019,
      "step": 2528,
      "training_loss": 6.804192066192627
    },
    {
      "epoch": 0.13707317073170733,
      "step": 2529,
      "training_loss": 7.79111909866333
    },
    {
      "epoch": 0.13712737127371274,
      "step": 2530,
      "training_loss": 7.622513771057129
    },
    {
      "epoch": 0.13718157181571816,
      "step": 2531,
      "training_loss": 7.120431900024414
    },
    {
      "epoch": 0.13723577235772358,
      "grad_norm": 21.62851905822754,
      "learning_rate": 1e-05,
      "loss": 7.3346,
      "step": 2532
    },
    {
      "epoch": 0.13723577235772358,
      "step": 2532,
      "training_loss": 7.346731662750244
    },
    {
      "epoch": 0.137289972899729,
      "step": 2533,
      "training_loss": 7.026785373687744
    },
    {
      "epoch": 0.1373441734417344,
      "step": 2534,
      "training_loss": 6.719282150268555
    },
    {
      "epoch": 0.13739837398373983,
      "step": 2535,
      "training_loss": 7.658319473266602
    },
    {
      "epoch": 0.13745257452574525,
      "grad_norm": 18.880321502685547,
      "learning_rate": 1e-05,
      "loss": 7.1878,
      "step": 2536
    },
    {
      "epoch": 0.13745257452574525,
      "step": 2536,
      "training_loss": 5.669539451599121
    },
    {
      "epoch": 0.13750677506775066,
      "step": 2537,
      "training_loss": 6.377847671508789
    },
    {
      "epoch": 0.1375609756097561,
      "step": 2538,
      "training_loss": 6.079473972320557
    },
    {
      "epoch": 0.13761517615176153,
      "step": 2539,
      "training_loss": 6.654615879058838
    },
    {
      "epoch": 0.13766937669376694,
      "grad_norm": 21.901548385620117,
      "learning_rate": 1e-05,
      "loss": 6.1954,
      "step": 2540
    },
    {
      "epoch": 0.13766937669376694,
      "step": 2540,
      "training_loss": 5.615240097045898
    },
    {
      "epoch": 0.13772357723577236,
      "step": 2541,
      "training_loss": 6.780345439910889
    },
    {
      "epoch": 0.13777777777777778,
      "step": 2542,
      "training_loss": 7.1078267097473145
    },
    {
      "epoch": 0.1378319783197832,
      "step": 2543,
      "training_loss": 7.742656707763672
    },
    {
      "epoch": 0.1378861788617886,
      "grad_norm": 27.089338302612305,
      "learning_rate": 1e-05,
      "loss": 6.8115,
      "step": 2544
    },
    {
      "epoch": 0.1378861788617886,
      "step": 2544,
      "training_loss": 7.532241344451904
    },
    {
      "epoch": 0.13794037940379403,
      "step": 2545,
      "training_loss": 7.4307861328125
    },
    {
      "epoch": 0.13799457994579944,
      "step": 2546,
      "training_loss": 6.04208517074585
    },
    {
      "epoch": 0.1380487804878049,
      "step": 2547,
      "training_loss": 7.581478118896484
    },
    {
      "epoch": 0.1381029810298103,
      "grad_norm": 23.44907569885254,
      "learning_rate": 1e-05,
      "loss": 7.1466,
      "step": 2548
    },
    {
      "epoch": 0.1381029810298103,
      "step": 2548,
      "training_loss": 7.040570259094238
    },
    {
      "epoch": 0.13815718157181572,
      "step": 2549,
      "training_loss": 4.930980205535889
    },
    {
      "epoch": 0.13821138211382114,
      "step": 2550,
      "training_loss": 5.0986456871032715
    },
    {
      "epoch": 0.13826558265582656,
      "step": 2551,
      "training_loss": 9.067586898803711
    },
    {
      "epoch": 0.13831978319783197,
      "grad_norm": 44.085933685302734,
      "learning_rate": 1e-05,
      "loss": 6.5344,
      "step": 2552
    },
    {
      "epoch": 0.13831978319783197,
      "step": 2552,
      "training_loss": 7.21317195892334
    },
    {
      "epoch": 0.1383739837398374,
      "step": 2553,
      "training_loss": 6.495886325836182
    },
    {
      "epoch": 0.1384281842818428,
      "step": 2554,
      "training_loss": 8.241730690002441
    },
    {
      "epoch": 0.13848238482384823,
      "step": 2555,
      "training_loss": 5.781787395477295
    },
    {
      "epoch": 0.13853658536585367,
      "grad_norm": 16.974815368652344,
      "learning_rate": 1e-05,
      "loss": 6.9331,
      "step": 2556
    },
    {
      "epoch": 0.13853658536585367,
      "step": 2556,
      "training_loss": 6.670403957366943
    },
    {
      "epoch": 0.1385907859078591,
      "step": 2557,
      "training_loss": 6.396185874938965
    },
    {
      "epoch": 0.1386449864498645,
      "step": 2558,
      "training_loss": 7.178925514221191
    },
    {
      "epoch": 0.13869918699186992,
      "step": 2559,
      "training_loss": 7.673912525177002
    },
    {
      "epoch": 0.13875338753387534,
      "grad_norm": 30.165178298950195,
      "learning_rate": 1e-05,
      "loss": 6.9799,
      "step": 2560
    },
    {
      "epoch": 0.13875338753387534,
      "step": 2560,
      "training_loss": 7.843574047088623
    },
    {
      "epoch": 0.13880758807588076,
      "step": 2561,
      "training_loss": 6.451774597167969
    },
    {
      "epoch": 0.13886178861788617,
      "step": 2562,
      "training_loss": 6.670575141906738
    },
    {
      "epoch": 0.1389159891598916,
      "step": 2563,
      "training_loss": 8.523720741271973
    },
    {
      "epoch": 0.138970189701897,
      "grad_norm": 25.6728515625,
      "learning_rate": 1e-05,
      "loss": 7.3724,
      "step": 2564
    },
    {
      "epoch": 0.138970189701897,
      "step": 2564,
      "training_loss": 5.8212127685546875
    },
    {
      "epoch": 0.13902439024390245,
      "step": 2565,
      "training_loss": 4.848917007446289
    },
    {
      "epoch": 0.13907859078590787,
      "step": 2566,
      "training_loss": 7.615372657775879
    },
    {
      "epoch": 0.13913279132791329,
      "step": 2567,
      "training_loss": 7.4322733879089355
    },
    {
      "epoch": 0.1391869918699187,
      "grad_norm": 34.8857307434082,
      "learning_rate": 1e-05,
      "loss": 6.4294,
      "step": 2568
    },
    {
      "epoch": 0.1391869918699187,
      "step": 2568,
      "training_loss": 7.6629133224487305
    },
    {
      "epoch": 0.13924119241192412,
      "step": 2569,
      "training_loss": 7.003819465637207
    },
    {
      "epoch": 0.13929539295392954,
      "step": 2570,
      "training_loss": 7.350333213806152
    },
    {
      "epoch": 0.13934959349593495,
      "step": 2571,
      "training_loss": 6.4309587478637695
    },
    {
      "epoch": 0.13940379403794037,
      "grad_norm": 17.854223251342773,
      "learning_rate": 1e-05,
      "loss": 7.112,
      "step": 2572
    },
    {
      "epoch": 0.13940379403794037,
      "step": 2572,
      "training_loss": 6.125321865081787
    },
    {
      "epoch": 0.1394579945799458,
      "step": 2573,
      "training_loss": 6.790889739990234
    },
    {
      "epoch": 0.13951219512195123,
      "step": 2574,
      "training_loss": 7.109437465667725
    },
    {
      "epoch": 0.13956639566395665,
      "step": 2575,
      "training_loss": 6.869138240814209
    },
    {
      "epoch": 0.13962059620596207,
      "grad_norm": 17.120479583740234,
      "learning_rate": 1e-05,
      "loss": 6.7237,
      "step": 2576
    },
    {
      "epoch": 0.13962059620596207,
      "step": 2576,
      "training_loss": 7.03721809387207
    },
    {
      "epoch": 0.13967479674796748,
      "step": 2577,
      "training_loss": 7.747026443481445
    },
    {
      "epoch": 0.1397289972899729,
      "step": 2578,
      "training_loss": 6.946017742156982
    },
    {
      "epoch": 0.13978319783197832,
      "step": 2579,
      "training_loss": 6.172929286956787
    },
    {
      "epoch": 0.13983739837398373,
      "grad_norm": 21.055814743041992,
      "learning_rate": 1e-05,
      "loss": 6.9758,
      "step": 2580
    },
    {
      "epoch": 0.13983739837398373,
      "step": 2580,
      "training_loss": 6.768195629119873
    },
    {
      "epoch": 0.13989159891598915,
      "step": 2581,
      "training_loss": 6.940977096557617
    },
    {
      "epoch": 0.13994579945799457,
      "step": 2582,
      "training_loss": 5.660757541656494
    },
    {
      "epoch": 0.14,
      "step": 2583,
      "training_loss": 6.886831760406494
    },
    {
      "epoch": 0.14005420054200543,
      "grad_norm": 23.115251541137695,
      "learning_rate": 1e-05,
      "loss": 6.5642,
      "step": 2584
    },
    {
      "epoch": 0.14005420054200543,
      "step": 2584,
      "training_loss": 7.897873878479004
    },
    {
      "epoch": 0.14010840108401085,
      "step": 2585,
      "training_loss": 7.113914966583252
    },
    {
      "epoch": 0.14016260162601626,
      "step": 2586,
      "training_loss": 6.489286422729492
    },
    {
      "epoch": 0.14021680216802168,
      "step": 2587,
      "training_loss": 6.589053153991699
    },
    {
      "epoch": 0.1402710027100271,
      "grad_norm": 17.186826705932617,
      "learning_rate": 1e-05,
      "loss": 7.0225,
      "step": 2588
    },
    {
      "epoch": 0.1402710027100271,
      "step": 2588,
      "training_loss": 5.904541015625
    },
    {
      "epoch": 0.14032520325203252,
      "step": 2589,
      "training_loss": 7.777456283569336
    },
    {
      "epoch": 0.14037940379403793,
      "step": 2590,
      "training_loss": 6.57749080657959
    },
    {
      "epoch": 0.14043360433604335,
      "step": 2591,
      "training_loss": 7.421666145324707
    },
    {
      "epoch": 0.1404878048780488,
      "grad_norm": 24.025636672973633,
      "learning_rate": 1e-05,
      "loss": 6.9203,
      "step": 2592
    },
    {
      "epoch": 0.1404878048780488,
      "step": 2592,
      "training_loss": 6.866920471191406
    },
    {
      "epoch": 0.1405420054200542,
      "step": 2593,
      "training_loss": 6.356772422790527
    },
    {
      "epoch": 0.14059620596205963,
      "step": 2594,
      "training_loss": 7.865425109863281
    },
    {
      "epoch": 0.14065040650406505,
      "step": 2595,
      "training_loss": 6.310418605804443
    },
    {
      "epoch": 0.14070460704607046,
      "grad_norm": 16.37093162536621,
      "learning_rate": 1e-05,
      "loss": 6.8499,
      "step": 2596
    },
    {
      "epoch": 0.14070460704607046,
      "step": 2596,
      "training_loss": 7.986814975738525
    },
    {
      "epoch": 0.14075880758807588,
      "step": 2597,
      "training_loss": 7.093797206878662
    },
    {
      "epoch": 0.1408130081300813,
      "step": 2598,
      "training_loss": 7.266373634338379
    },
    {
      "epoch": 0.1408672086720867,
      "step": 2599,
      "training_loss": 7.495234966278076
    },
    {
      "epoch": 0.14092140921409213,
      "grad_norm": 24.399343490600586,
      "learning_rate": 1e-05,
      "loss": 7.4606,
      "step": 2600
    },
    {
      "epoch": 0.14092140921409213,
      "step": 2600,
      "training_loss": 6.410475730895996
    },
    {
      "epoch": 0.14097560975609755,
      "step": 2601,
      "training_loss": 6.86810302734375
    },
    {
      "epoch": 0.141029810298103,
      "step": 2602,
      "training_loss": 7.801304340362549
    },
    {
      "epoch": 0.1410840108401084,
      "step": 2603,
      "training_loss": 7.721095085144043
    },
    {
      "epoch": 0.14113821138211383,
      "grad_norm": 24.383296966552734,
      "learning_rate": 1e-05,
      "loss": 7.2002,
      "step": 2604
    },
    {
      "epoch": 0.14113821138211383,
      "step": 2604,
      "training_loss": 7.8185529708862305
    },
    {
      "epoch": 0.14119241192411924,
      "step": 2605,
      "training_loss": 7.029047012329102
    },
    {
      "epoch": 0.14124661246612466,
      "step": 2606,
      "training_loss": 7.460811614990234
    },
    {
      "epoch": 0.14130081300813008,
      "step": 2607,
      "training_loss": 7.480808734893799
    },
    {
      "epoch": 0.1413550135501355,
      "grad_norm": 15.486342430114746,
      "learning_rate": 1e-05,
      "loss": 7.4473,
      "step": 2608
    },
    {
      "epoch": 0.1413550135501355,
      "step": 2608,
      "training_loss": 7.195689678192139
    },
    {
      "epoch": 0.1414092140921409,
      "step": 2609,
      "training_loss": 7.599444389343262
    },
    {
      "epoch": 0.14146341463414633,
      "step": 2610,
      "training_loss": 6.489224910736084
    },
    {
      "epoch": 0.14151761517615177,
      "step": 2611,
      "training_loss": 5.484253406524658
    },
    {
      "epoch": 0.1415718157181572,
      "grad_norm": 20.400161743164062,
      "learning_rate": 1e-05,
      "loss": 6.6922,
      "step": 2612
    },
    {
      "epoch": 0.1415718157181572,
      "step": 2612,
      "training_loss": 7.4624247550964355
    },
    {
      "epoch": 0.1416260162601626,
      "step": 2613,
      "training_loss": 9.60594654083252
    },
    {
      "epoch": 0.14168021680216802,
      "step": 2614,
      "training_loss": 6.936070919036865
    },
    {
      "epoch": 0.14173441734417344,
      "step": 2615,
      "training_loss": 7.8860578536987305
    },
    {
      "epoch": 0.14178861788617886,
      "grad_norm": 14.954551696777344,
      "learning_rate": 1e-05,
      "loss": 7.9726,
      "step": 2616
    },
    {
      "epoch": 0.14178861788617886,
      "step": 2616,
      "training_loss": 6.348743438720703
    },
    {
      "epoch": 0.14184281842818428,
      "step": 2617,
      "training_loss": 7.02944278717041
    },
    {
      "epoch": 0.1418970189701897,
      "step": 2618,
      "training_loss": 7.17039680480957
    },
    {
      "epoch": 0.1419512195121951,
      "step": 2619,
      "training_loss": 8.11153793334961
    },
    {
      "epoch": 0.14200542005420055,
      "grad_norm": 19.25794219970703,
      "learning_rate": 1e-05,
      "loss": 7.165,
      "step": 2620
    },
    {
      "epoch": 0.14200542005420055,
      "step": 2620,
      "training_loss": 6.456801414489746
    },
    {
      "epoch": 0.14205962059620597,
      "step": 2621,
      "training_loss": 7.190435886383057
    },
    {
      "epoch": 0.1421138211382114,
      "step": 2622,
      "training_loss": 6.131196975708008
    },
    {
      "epoch": 0.1421680216802168,
      "step": 2623,
      "training_loss": 7.137503623962402
    },
    {
      "epoch": 0.14222222222222222,
      "grad_norm": 16.46312141418457,
      "learning_rate": 1e-05,
      "loss": 6.729,
      "step": 2624
    },
    {
      "epoch": 0.14222222222222222,
      "step": 2624,
      "training_loss": 7.21945333480835
    },
    {
      "epoch": 0.14227642276422764,
      "step": 2625,
      "training_loss": 6.304246425628662
    },
    {
      "epoch": 0.14233062330623306,
      "step": 2626,
      "training_loss": 6.86186408996582
    },
    {
      "epoch": 0.14238482384823847,
      "step": 2627,
      "training_loss": 8.161542892456055
    },
    {
      "epoch": 0.1424390243902439,
      "grad_norm": 24.044849395751953,
      "learning_rate": 1e-05,
      "loss": 7.1368,
      "step": 2628
    },
    {
      "epoch": 0.1424390243902439,
      "step": 2628,
      "training_loss": 6.6491618156433105
    },
    {
      "epoch": 0.14249322493224933,
      "step": 2629,
      "training_loss": 7.545411586761475
    },
    {
      "epoch": 0.14254742547425475,
      "step": 2630,
      "training_loss": 7.041926860809326
    },
    {
      "epoch": 0.14260162601626017,
      "step": 2631,
      "training_loss": 7.190550327301025
    },
    {
      "epoch": 0.14265582655826559,
      "grad_norm": 17.28243064880371,
      "learning_rate": 1e-05,
      "loss": 7.1068,
      "step": 2632
    },
    {
      "epoch": 0.14265582655826559,
      "step": 2632,
      "training_loss": 7.361677646636963
    },
    {
      "epoch": 0.142710027100271,
      "step": 2633,
      "training_loss": 7.30226469039917
    },
    {
      "epoch": 0.14276422764227642,
      "step": 2634,
      "training_loss": 8.460108757019043
    },
    {
      "epoch": 0.14281842818428184,
      "step": 2635,
      "training_loss": 7.010674953460693
    },
    {
      "epoch": 0.14287262872628725,
      "grad_norm": 17.456485748291016,
      "learning_rate": 1e-05,
      "loss": 7.5337,
      "step": 2636
    },
    {
      "epoch": 0.14287262872628725,
      "step": 2636,
      "training_loss": 7.053275108337402
    },
    {
      "epoch": 0.14292682926829267,
      "step": 2637,
      "training_loss": 7.076700687408447
    },
    {
      "epoch": 0.14298102981029812,
      "step": 2638,
      "training_loss": 7.728366851806641
    },
    {
      "epoch": 0.14303523035230353,
      "step": 2639,
      "training_loss": 7.305314540863037
    },
    {
      "epoch": 0.14308943089430895,
      "grad_norm": 21.654850006103516,
      "learning_rate": 1e-05,
      "loss": 7.2909,
      "step": 2640
    },
    {
      "epoch": 0.14308943089430895,
      "step": 2640,
      "training_loss": 6.181407928466797
    },
    {
      "epoch": 0.14314363143631437,
      "step": 2641,
      "training_loss": 7.536325454711914
    },
    {
      "epoch": 0.14319783197831978,
      "step": 2642,
      "training_loss": 7.40533971786499
    },
    {
      "epoch": 0.1432520325203252,
      "step": 2643,
      "training_loss": 6.647276878356934
    },
    {
      "epoch": 0.14330623306233062,
      "grad_norm": 14.121835708618164,
      "learning_rate": 1e-05,
      "loss": 6.9426,
      "step": 2644
    },
    {
      "epoch": 0.14330623306233062,
      "step": 2644,
      "training_loss": 7.893548488616943
    },
    {
      "epoch": 0.14336043360433603,
      "step": 2645,
      "training_loss": 6.1227240562438965
    },
    {
      "epoch": 0.14341463414634145,
      "step": 2646,
      "training_loss": 8.516410827636719
    },
    {
      "epoch": 0.1434688346883469,
      "step": 2647,
      "training_loss": 7.292142391204834
    },
    {
      "epoch": 0.1435230352303523,
      "grad_norm": 17.110750198364258,
      "learning_rate": 1e-05,
      "loss": 7.4562,
      "step": 2648
    },
    {
      "epoch": 0.1435230352303523,
      "step": 2648,
      "training_loss": 6.254172325134277
    },
    {
      "epoch": 0.14357723577235773,
      "step": 2649,
      "training_loss": 5.857818603515625
    },
    {
      "epoch": 0.14363143631436315,
      "step": 2650,
      "training_loss": 6.030587196350098
    },
    {
      "epoch": 0.14368563685636856,
      "step": 2651,
      "training_loss": 7.093649387359619
    },
    {
      "epoch": 0.14373983739837398,
      "grad_norm": 25.02707290649414,
      "learning_rate": 1e-05,
      "loss": 6.3091,
      "step": 2652
    },
    {
      "epoch": 0.14373983739837398,
      "step": 2652,
      "training_loss": 7.9951934814453125
    },
    {
      "epoch": 0.1437940379403794,
      "step": 2653,
      "training_loss": 6.886555194854736
    },
    {
      "epoch": 0.14384823848238482,
      "step": 2654,
      "training_loss": 7.544373989105225
    },
    {
      "epoch": 0.14390243902439023,
      "step": 2655,
      "training_loss": 5.216752052307129
    },
    {
      "epoch": 0.14395663956639568,
      "grad_norm": 17.857152938842773,
      "learning_rate": 1e-05,
      "loss": 6.9107,
      "step": 2656
    },
    {
      "epoch": 0.14395663956639568,
      "step": 2656,
      "training_loss": 6.886301040649414
    },
    {
      "epoch": 0.1440108401084011,
      "step": 2657,
      "training_loss": 7.307267665863037
    },
    {
      "epoch": 0.1440650406504065,
      "step": 2658,
      "training_loss": 7.07749080657959
    },
    {
      "epoch": 0.14411924119241193,
      "step": 2659,
      "training_loss": 5.898308753967285
    },
    {
      "epoch": 0.14417344173441735,
      "grad_norm": 21.145387649536133,
      "learning_rate": 1e-05,
      "loss": 6.7923,
      "step": 2660
    },
    {
      "epoch": 0.14417344173441735,
      "step": 2660,
      "training_loss": 8.123764038085938
    },
    {
      "epoch": 0.14422764227642276,
      "step": 2661,
      "training_loss": 6.961852073669434
    },
    {
      "epoch": 0.14428184281842818,
      "step": 2662,
      "training_loss": 6.3797736167907715
    },
    {
      "epoch": 0.1443360433604336,
      "step": 2663,
      "training_loss": 7.33880615234375
    },
    {
      "epoch": 0.144390243902439,
      "grad_norm": 23.6699275970459,
      "learning_rate": 1e-05,
      "loss": 7.201,
      "step": 2664
    },
    {
      "epoch": 0.144390243902439,
      "step": 2664,
      "training_loss": 6.75139045715332
    },
    {
      "epoch": 0.14444444444444443,
      "step": 2665,
      "training_loss": 6.73094367980957
    },
    {
      "epoch": 0.14449864498644988,
      "step": 2666,
      "training_loss": 6.6419501304626465
    },
    {
      "epoch": 0.1445528455284553,
      "step": 2667,
      "training_loss": 5.977971076965332
    },
    {
      "epoch": 0.1446070460704607,
      "grad_norm": 21.326229095458984,
      "learning_rate": 1e-05,
      "loss": 6.5256,
      "step": 2668
    },
    {
      "epoch": 0.1446070460704607,
      "step": 2668,
      "training_loss": 8.152270317077637
    },
    {
      "epoch": 0.14466124661246613,
      "step": 2669,
      "training_loss": 7.795559406280518
    },
    {
      "epoch": 0.14471544715447154,
      "step": 2670,
      "training_loss": 6.828698635101318
    },
    {
      "epoch": 0.14476964769647696,
      "step": 2671,
      "training_loss": 8.477325439453125
    },
    {
      "epoch": 0.14482384823848238,
      "grad_norm": 23.462650299072266,
      "learning_rate": 1e-05,
      "loss": 7.8135,
      "step": 2672
    },
    {
      "epoch": 0.14482384823848238,
      "step": 2672,
      "training_loss": 7.181950092315674
    },
    {
      "epoch": 0.1448780487804878,
      "step": 2673,
      "training_loss": 6.300014972686768
    },
    {
      "epoch": 0.1449322493224932,
      "step": 2674,
      "training_loss": 7.548744201660156
    },
    {
      "epoch": 0.14498644986449866,
      "step": 2675,
      "training_loss": 7.495262622833252
    },
    {
      "epoch": 0.14504065040650407,
      "grad_norm": 31.267736434936523,
      "learning_rate": 1e-05,
      "loss": 7.1315,
      "step": 2676
    },
    {
      "epoch": 0.14504065040650407,
      "step": 2676,
      "training_loss": 4.403129577636719
    },
    {
      "epoch": 0.1450948509485095,
      "step": 2677,
      "training_loss": 7.270368576049805
    },
    {
      "epoch": 0.1451490514905149,
      "step": 2678,
      "training_loss": 7.146984577178955
    },
    {
      "epoch": 0.14520325203252032,
      "step": 2679,
      "training_loss": 6.9243364334106445
    },
    {
      "epoch": 0.14525745257452574,
      "grad_norm": 26.08025550842285,
      "learning_rate": 1e-05,
      "loss": 6.4362,
      "step": 2680
    },
    {
      "epoch": 0.14525745257452574,
      "step": 2680,
      "training_loss": 8.023134231567383
    },
    {
      "epoch": 0.14531165311653116,
      "step": 2681,
      "training_loss": 8.00939655303955
    },
    {
      "epoch": 0.14536585365853658,
      "step": 2682,
      "training_loss": 7.32981014251709
    },
    {
      "epoch": 0.145420054200542,
      "step": 2683,
      "training_loss": 7.435822010040283
    },
    {
      "epoch": 0.14547425474254744,
      "grad_norm": 16.1890869140625,
      "learning_rate": 1e-05,
      "loss": 7.6995,
      "step": 2684
    },
    {
      "epoch": 0.14547425474254744,
      "step": 2684,
      "training_loss": 4.415499687194824
    },
    {
      "epoch": 0.14552845528455285,
      "step": 2685,
      "training_loss": 6.979267120361328
    },
    {
      "epoch": 0.14558265582655827,
      "step": 2686,
      "training_loss": 7.1596808433532715
    },
    {
      "epoch": 0.1456368563685637,
      "step": 2687,
      "training_loss": 6.932631492614746
    },
    {
      "epoch": 0.1456910569105691,
      "grad_norm": 46.051815032958984,
      "learning_rate": 1e-05,
      "loss": 6.3718,
      "step": 2688
    },
    {
      "epoch": 0.1456910569105691,
      "step": 2688,
      "training_loss": 7.968842506408691
    },
    {
      "epoch": 0.14574525745257452,
      "step": 2689,
      "training_loss": 5.563681125640869
    },
    {
      "epoch": 0.14579945799457994,
      "step": 2690,
      "training_loss": 5.70306396484375
    },
    {
      "epoch": 0.14585365853658536,
      "step": 2691,
      "training_loss": 7.031442165374756
    },
    {
      "epoch": 0.14590785907859077,
      "grad_norm": 30.15680503845215,
      "learning_rate": 1e-05,
      "loss": 6.5668,
      "step": 2692
    },
    {
      "epoch": 0.14590785907859077,
      "step": 2692,
      "training_loss": 5.624709129333496
    },
    {
      "epoch": 0.14596205962059622,
      "step": 2693,
      "training_loss": 6.83181095123291
    },
    {
      "epoch": 0.14601626016260164,
      "step": 2694,
      "training_loss": 7.501360893249512
    },
    {
      "epoch": 0.14607046070460705,
      "step": 2695,
      "training_loss": 5.888236999511719
    },
    {
      "epoch": 0.14612466124661247,
      "grad_norm": 28.95340919494629,
      "learning_rate": 1e-05,
      "loss": 6.4615,
      "step": 2696
    },
    {
      "epoch": 0.14612466124661247,
      "step": 2696,
      "training_loss": 7.543758869171143
    },
    {
      "epoch": 0.1461788617886179,
      "step": 2697,
      "training_loss": 6.650112628936768
    },
    {
      "epoch": 0.1462330623306233,
      "step": 2698,
      "training_loss": 6.910295486450195
    },
    {
      "epoch": 0.14628726287262872,
      "step": 2699,
      "training_loss": 6.898913383483887
    },
    {
      "epoch": 0.14634146341463414,
      "grad_norm": 29.399581909179688,
      "learning_rate": 1e-05,
      "loss": 7.0008,
      "step": 2700
    },
    {
      "epoch": 0.14634146341463414,
      "step": 2700,
      "training_loss": 5.190524101257324
    },
    {
      "epoch": 0.14639566395663955,
      "step": 2701,
      "training_loss": 7.194784164428711
    },
    {
      "epoch": 0.146449864498645,
      "step": 2702,
      "training_loss": 7.428859233856201
    },
    {
      "epoch": 0.14650406504065042,
      "step": 2703,
      "training_loss": 7.772037982940674
    },
    {
      "epoch": 0.14655826558265583,
      "grad_norm": 25.330265045166016,
      "learning_rate": 1e-05,
      "loss": 6.8966,
      "step": 2704
    },
    {
      "epoch": 0.14655826558265583,
      "step": 2704,
      "training_loss": 6.064313888549805
    },
    {
      "epoch": 0.14661246612466125,
      "step": 2705,
      "training_loss": 6.1325201988220215
    },
    {
      "epoch": 0.14666666666666667,
      "step": 2706,
      "training_loss": 6.746006011962891
    },
    {
      "epoch": 0.14672086720867208,
      "step": 2707,
      "training_loss": 7.027482032775879
    },
    {
      "epoch": 0.1467750677506775,
      "grad_norm": 29.07795524597168,
      "learning_rate": 1e-05,
      "loss": 6.4926,
      "step": 2708
    },
    {
      "epoch": 0.1467750677506775,
      "step": 2708,
      "training_loss": 6.280714511871338
    },
    {
      "epoch": 0.14682926829268292,
      "step": 2709,
      "training_loss": 6.554640769958496
    },
    {
      "epoch": 0.14688346883468834,
      "step": 2710,
      "training_loss": 7.091335773468018
    },
    {
      "epoch": 0.14693766937669378,
      "step": 2711,
      "training_loss": 7.897545337677002
    },
    {
      "epoch": 0.1469918699186992,
      "grad_norm": 23.965288162231445,
      "learning_rate": 1e-05,
      "loss": 6.9561,
      "step": 2712
    },
    {
      "epoch": 0.1469918699186992,
      "step": 2712,
      "training_loss": 8.26445198059082
    },
    {
      "epoch": 0.14704607046070461,
      "step": 2713,
      "training_loss": 6.800072193145752
    },
    {
      "epoch": 0.14710027100271003,
      "step": 2714,
      "training_loss": 6.746550559997559
    },
    {
      "epoch": 0.14715447154471545,
      "step": 2715,
      "training_loss": 4.743051052093506
    },
    {
      "epoch": 0.14720867208672087,
      "grad_norm": 26.738666534423828,
      "learning_rate": 1e-05,
      "loss": 6.6385,
      "step": 2716
    },
    {
      "epoch": 0.14720867208672087,
      "step": 2716,
      "training_loss": 6.969179153442383
    },
    {
      "epoch": 0.14726287262872628,
      "step": 2717,
      "training_loss": 7.300539493560791
    },
    {
      "epoch": 0.1473170731707317,
      "step": 2718,
      "training_loss": 7.881967067718506
    },
    {
      "epoch": 0.14737127371273712,
      "step": 2719,
      "training_loss": 8.212117195129395
    },
    {
      "epoch": 0.14742547425474256,
      "grad_norm": 43.5081901550293,
      "learning_rate": 1e-05,
      "loss": 7.591,
      "step": 2720
    },
    {
      "epoch": 0.14742547425474256,
      "step": 2720,
      "training_loss": 7.683688163757324
    },
    {
      "epoch": 0.14747967479674798,
      "step": 2721,
      "training_loss": 6.935351371765137
    },
    {
      "epoch": 0.1475338753387534,
      "step": 2722,
      "training_loss": 7.5208563804626465
    },
    {
      "epoch": 0.1475880758807588,
      "step": 2723,
      "training_loss": 6.421712398529053
    },
    {
      "epoch": 0.14764227642276423,
      "grad_norm": 15.367502212524414,
      "learning_rate": 1e-05,
      "loss": 7.1404,
      "step": 2724
    },
    {
      "epoch": 0.14764227642276423,
      "step": 2724,
      "training_loss": 8.721485137939453
    },
    {
      "epoch": 0.14769647696476965,
      "step": 2725,
      "training_loss": 7.487117290496826
    },
    {
      "epoch": 0.14775067750677506,
      "step": 2726,
      "training_loss": 6.056400775909424
    },
    {
      "epoch": 0.14780487804878048,
      "step": 2727,
      "training_loss": 8.04054069519043
    },
    {
      "epoch": 0.1478590785907859,
      "grad_norm": 24.12108612060547,
      "learning_rate": 1e-05,
      "loss": 7.5764,
      "step": 2728
    },
    {
      "epoch": 0.1478590785907859,
      "step": 2728,
      "training_loss": 5.131184101104736
    },
    {
      "epoch": 0.14791327913279131,
      "step": 2729,
      "training_loss": 7.3170342445373535
    },
    {
      "epoch": 0.14796747967479676,
      "step": 2730,
      "training_loss": 6.484695911407471
    },
    {
      "epoch": 0.14802168021680218,
      "step": 2731,
      "training_loss": 6.992718696594238
    },
    {
      "epoch": 0.1480758807588076,
      "grad_norm": 19.52605628967285,
      "learning_rate": 1e-05,
      "loss": 6.4814,
      "step": 2732
    },
    {
      "epoch": 0.1480758807588076,
      "step": 2732,
      "training_loss": 7.523624420166016
    },
    {
      "epoch": 0.148130081300813,
      "step": 2733,
      "training_loss": 7.454044818878174
    },
    {
      "epoch": 0.14818428184281843,
      "step": 2734,
      "training_loss": 6.95874547958374
    },
    {
      "epoch": 0.14823848238482384,
      "step": 2735,
      "training_loss": 8.376072883605957
    },
    {
      "epoch": 0.14829268292682926,
      "grad_norm": 26.37027931213379,
      "learning_rate": 1e-05,
      "loss": 7.5781,
      "step": 2736
    },
    {
      "epoch": 0.14829268292682926,
      "step": 2736,
      "training_loss": 7.0401201248168945
    },
    {
      "epoch": 0.14834688346883468,
      "step": 2737,
      "training_loss": 7.085273742675781
    },
    {
      "epoch": 0.1484010840108401,
      "step": 2738,
      "training_loss": 6.184514999389648
    },
    {
      "epoch": 0.14845528455284554,
      "step": 2739,
      "training_loss": 8.308947563171387
    },
    {
      "epoch": 0.14850948509485096,
      "grad_norm": 38.29178237915039,
      "learning_rate": 1e-05,
      "loss": 7.1547,
      "step": 2740
    },
    {
      "epoch": 0.14850948509485096,
      "step": 2740,
      "training_loss": 7.816874027252197
    },
    {
      "epoch": 0.14856368563685637,
      "step": 2741,
      "training_loss": 7.641136646270752
    },
    {
      "epoch": 0.1486178861788618,
      "step": 2742,
      "training_loss": 6.590243339538574
    },
    {
      "epoch": 0.1486720867208672,
      "step": 2743,
      "training_loss": 6.91975212097168
    },
    {
      "epoch": 0.14872628726287263,
      "grad_norm": 28.48296546936035,
      "learning_rate": 1e-05,
      "loss": 7.242,
      "step": 2744
    },
    {
      "epoch": 0.14872628726287263,
      "step": 2744,
      "training_loss": 7.07703971862793
    },
    {
      "epoch": 0.14878048780487804,
      "step": 2745,
      "training_loss": 5.6660261154174805
    },
    {
      "epoch": 0.14883468834688346,
      "step": 2746,
      "training_loss": 4.158545970916748
    },
    {
      "epoch": 0.14888888888888888,
      "step": 2747,
      "training_loss": 5.982562065124512
    },
    {
      "epoch": 0.14894308943089432,
      "grad_norm": 22.693960189819336,
      "learning_rate": 1e-05,
      "loss": 5.721,
      "step": 2748
    },
    {
      "epoch": 0.14894308943089432,
      "step": 2748,
      "training_loss": 4.633694648742676
    },
    {
      "epoch": 0.14899728997289974,
      "step": 2749,
      "training_loss": 7.384731292724609
    },
    {
      "epoch": 0.14905149051490515,
      "step": 2750,
      "training_loss": 6.685415744781494
    },
    {
      "epoch": 0.14910569105691057,
      "step": 2751,
      "training_loss": 5.716179847717285
    },
    {
      "epoch": 0.149159891598916,
      "grad_norm": 29.589332580566406,
      "learning_rate": 1e-05,
      "loss": 6.105,
      "step": 2752
    },
    {
      "epoch": 0.149159891598916,
      "step": 2752,
      "training_loss": 5.508753776550293
    },
    {
      "epoch": 0.1492140921409214,
      "step": 2753,
      "training_loss": 6.403130531311035
    },
    {
      "epoch": 0.14926829268292682,
      "step": 2754,
      "training_loss": 6.1644158363342285
    },
    {
      "epoch": 0.14932249322493224,
      "step": 2755,
      "training_loss": 7.2176713943481445
    },
    {
      "epoch": 0.14937669376693766,
      "grad_norm": 15.610504150390625,
      "learning_rate": 1e-05,
      "loss": 6.3235,
      "step": 2756
    },
    {
      "epoch": 0.14937669376693766,
      "step": 2756,
      "training_loss": 7.52488899230957
    },
    {
      "epoch": 0.1494308943089431,
      "step": 2757,
      "training_loss": 7.013801097869873
    },
    {
      "epoch": 0.14948509485094852,
      "step": 2758,
      "training_loss": 6.904720306396484
    },
    {
      "epoch": 0.14953929539295394,
      "step": 2759,
      "training_loss": 7.126418590545654
    },
    {
      "epoch": 0.14959349593495935,
      "grad_norm": 24.772092819213867,
      "learning_rate": 1e-05,
      "loss": 7.1425,
      "step": 2760
    },
    {
      "epoch": 0.14959349593495935,
      "step": 2760,
      "training_loss": 6.748104095458984
    },
    {
      "epoch": 0.14964769647696477,
      "step": 2761,
      "training_loss": 4.5430073738098145
    },
    {
      "epoch": 0.1497018970189702,
      "step": 2762,
      "training_loss": 6.75314474105835
    },
    {
      "epoch": 0.1497560975609756,
      "step": 2763,
      "training_loss": 6.927399158477783
    },
    {
      "epoch": 0.14981029810298102,
      "grad_norm": 26.12412452697754,
      "learning_rate": 1e-05,
      "loss": 6.2429,
      "step": 2764
    },
    {
      "epoch": 0.14981029810298102,
      "step": 2764,
      "training_loss": 6.580842971801758
    },
    {
      "epoch": 0.14986449864498644,
      "step": 2765,
      "training_loss": 7.397954940795898
    },
    {
      "epoch": 0.14991869918699188,
      "step": 2766,
      "training_loss": 7.401541233062744
    },
    {
      "epoch": 0.1499728997289973,
      "step": 2767,
      "training_loss": 7.023067474365234
    },
    {
      "epoch": 0.15002710027100272,
      "grad_norm": 15.717175483703613,
      "learning_rate": 1e-05,
      "loss": 7.1009,
      "step": 2768
    },
    {
      "epoch": 0.15002710027100272,
      "step": 2768,
      "training_loss": 5.815362930297852
    },
    {
      "epoch": 0.15008130081300813,
      "step": 2769,
      "training_loss": 4.521266937255859
    },
    {
      "epoch": 0.15013550135501355,
      "step": 2770,
      "training_loss": 7.441351890563965
    },
    {
      "epoch": 0.15018970189701897,
      "step": 2771,
      "training_loss": 5.383084774017334
    },
    {
      "epoch": 0.15024390243902438,
      "grad_norm": 21.512527465820312,
      "learning_rate": 1e-05,
      "loss": 5.7903,
      "step": 2772
    },
    {
      "epoch": 0.15024390243902438,
      "step": 2772,
      "training_loss": 5.500888347625732
    },
    {
      "epoch": 0.1502981029810298,
      "step": 2773,
      "training_loss": 6.990847110748291
    },
    {
      "epoch": 0.15035230352303522,
      "step": 2774,
      "training_loss": 6.203507900238037
    },
    {
      "epoch": 0.15040650406504066,
      "step": 2775,
      "training_loss": 6.280167579650879
    },
    {
      "epoch": 0.15046070460704608,
      "grad_norm": 17.69522476196289,
      "learning_rate": 1e-05,
      "loss": 6.2439,
      "step": 2776
    },
    {
      "epoch": 0.15046070460704608,
      "step": 2776,
      "training_loss": 7.131898880004883
    },
    {
      "epoch": 0.1505149051490515,
      "step": 2777,
      "training_loss": 8.264874458312988
    },
    {
      "epoch": 0.15056910569105691,
      "step": 2778,
      "training_loss": 6.983683109283447
    },
    {
      "epoch": 0.15062330623306233,
      "step": 2779,
      "training_loss": 6.052300930023193
    },
    {
      "epoch": 0.15067750677506775,
      "grad_norm": 15.918036460876465,
      "learning_rate": 1e-05,
      "loss": 7.1082,
      "step": 2780
    },
    {
      "epoch": 0.15067750677506775,
      "step": 2780,
      "training_loss": 7.474325180053711
    },
    {
      "epoch": 0.15073170731707317,
      "step": 2781,
      "training_loss": 7.750882148742676
    },
    {
      "epoch": 0.15078590785907858,
      "step": 2782,
      "training_loss": 7.630563735961914
    },
    {
      "epoch": 0.150840108401084,
      "step": 2783,
      "training_loss": 6.802700996398926
    },
    {
      "epoch": 0.15089430894308944,
      "grad_norm": 22.999780654907227,
      "learning_rate": 1e-05,
      "loss": 7.4146,
      "step": 2784
    },
    {
      "epoch": 0.15089430894308944,
      "step": 2784,
      "training_loss": 6.824918746948242
    },
    {
      "epoch": 0.15094850948509486,
      "step": 2785,
      "training_loss": 7.499577045440674
    },
    {
      "epoch": 0.15100271002710028,
      "step": 2786,
      "training_loss": 7.234065055847168
    },
    {
      "epoch": 0.1510569105691057,
      "step": 2787,
      "training_loss": 7.381229877471924
    },
    {
      "epoch": 0.1511111111111111,
      "grad_norm": 28.34613800048828,
      "learning_rate": 1e-05,
      "loss": 7.2349,
      "step": 2788
    },
    {
      "epoch": 0.1511111111111111,
      "step": 2788,
      "training_loss": 6.11315393447876
    },
    {
      "epoch": 0.15116531165311653,
      "step": 2789,
      "training_loss": 6.908787250518799
    },
    {
      "epoch": 0.15121951219512195,
      "step": 2790,
      "training_loss": 6.644617557525635
    },
    {
      "epoch": 0.15127371273712736,
      "step": 2791,
      "training_loss": 8.45887565612793
    },
    {
      "epoch": 0.15132791327913278,
      "grad_norm": 18.90997886657715,
      "learning_rate": 1e-05,
      "loss": 7.0314,
      "step": 2792
    },
    {
      "epoch": 0.15132791327913278,
      "step": 2792,
      "training_loss": 6.93646240234375
    },
    {
      "epoch": 0.1513821138211382,
      "step": 2793,
      "training_loss": 7.207126617431641
    },
    {
      "epoch": 0.15143631436314364,
      "step": 2794,
      "training_loss": 7.815772533416748
    },
    {
      "epoch": 0.15149051490514906,
      "step": 2795,
      "training_loss": 7.011143207550049
    },
    {
      "epoch": 0.15154471544715448,
      "grad_norm": 17.711400985717773,
      "learning_rate": 1e-05,
      "loss": 7.2426,
      "step": 2796
    },
    {
      "epoch": 0.15154471544715448,
      "step": 2796,
      "training_loss": 7.868875503540039
    },
    {
      "epoch": 0.1515989159891599,
      "step": 2797,
      "training_loss": 5.351524353027344
    },
    {
      "epoch": 0.1516531165311653,
      "step": 2798,
      "training_loss": 6.708816051483154
    },
    {
      "epoch": 0.15170731707317073,
      "step": 2799,
      "training_loss": 7.103667736053467
    },
    {
      "epoch": 0.15176151761517614,
      "grad_norm": 21.83403968811035,
      "learning_rate": 1e-05,
      "loss": 6.7582,
      "step": 2800
    },
    {
      "epoch": 0.15176151761517614,
      "step": 2800,
      "training_loss": 7.187362194061279
    },
    {
      "epoch": 0.15181571815718156,
      "step": 2801,
      "training_loss": 5.375797271728516
    },
    {
      "epoch": 0.15186991869918698,
      "step": 2802,
      "training_loss": 7.37620210647583
    },
    {
      "epoch": 0.15192411924119242,
      "step": 2803,
      "training_loss": 5.788379192352295
    },
    {
      "epoch": 0.15197831978319784,
      "grad_norm": 48.4687614440918,
      "learning_rate": 1e-05,
      "loss": 6.4319,
      "step": 2804
    },
    {
      "epoch": 0.15197831978319784,
      "step": 2804,
      "training_loss": 6.213126182556152
    },
    {
      "epoch": 0.15203252032520326,
      "step": 2805,
      "training_loss": 5.87962532043457
    },
    {
      "epoch": 0.15208672086720867,
      "step": 2806,
      "training_loss": 6.834182262420654
    },
    {
      "epoch": 0.1521409214092141,
      "step": 2807,
      "training_loss": 6.190089225769043
    },
    {
      "epoch": 0.1521951219512195,
      "grad_norm": 18.155990600585938,
      "learning_rate": 1e-05,
      "loss": 6.2793,
      "step": 2808
    },
    {
      "epoch": 0.1521951219512195,
      "step": 2808,
      "training_loss": 6.960704326629639
    },
    {
      "epoch": 0.15224932249322493,
      "step": 2809,
      "training_loss": 6.816217422485352
    },
    {
      "epoch": 0.15230352303523034,
      "step": 2810,
      "training_loss": 6.386111736297607
    },
    {
      "epoch": 0.15235772357723576,
      "step": 2811,
      "training_loss": 8.093706130981445
    },
    {
      "epoch": 0.1524119241192412,
      "grad_norm": 34.86417007446289,
      "learning_rate": 1e-05,
      "loss": 7.0642,
      "step": 2812
    },
    {
      "epoch": 0.1524119241192412,
      "step": 2812,
      "training_loss": 7.025763034820557
    },
    {
      "epoch": 0.15246612466124662,
      "step": 2813,
      "training_loss": 5.740941524505615
    },
    {
      "epoch": 0.15252032520325204,
      "step": 2814,
      "training_loss": 6.446156024932861
    },
    {
      "epoch": 0.15257452574525746,
      "step": 2815,
      "training_loss": 7.457784175872803
    },
    {
      "epoch": 0.15262872628726287,
      "grad_norm": 17.195314407348633,
      "learning_rate": 1e-05,
      "loss": 6.6677,
      "step": 2816
    },
    {
      "epoch": 0.15262872628726287,
      "step": 2816,
      "training_loss": 6.662289619445801
    },
    {
      "epoch": 0.1526829268292683,
      "step": 2817,
      "training_loss": 7.3425397872924805
    },
    {
      "epoch": 0.1527371273712737,
      "step": 2818,
      "training_loss": 6.903387069702148
    },
    {
      "epoch": 0.15279132791327912,
      "step": 2819,
      "training_loss": 7.31217098236084
    },
    {
      "epoch": 0.15284552845528454,
      "grad_norm": 23.215063095092773,
      "learning_rate": 1e-05,
      "loss": 7.0551,
      "step": 2820
    },
    {
      "epoch": 0.15284552845528454,
      "step": 2820,
      "training_loss": 7.486176490783691
    },
    {
      "epoch": 0.15289972899728999,
      "step": 2821,
      "training_loss": 7.081368446350098
    },
    {
      "epoch": 0.1529539295392954,
      "step": 2822,
      "training_loss": 6.873575210571289
    },
    {
      "epoch": 0.15300813008130082,
      "step": 2823,
      "training_loss": 7.563320636749268
    },
    {
      "epoch": 0.15306233062330624,
      "grad_norm": 17.66584587097168,
      "learning_rate": 1e-05,
      "loss": 7.2511,
      "step": 2824
    },
    {
      "epoch": 0.15306233062330624,
      "step": 2824,
      "training_loss": 5.7166924476623535
    },
    {
      "epoch": 0.15311653116531165,
      "step": 2825,
      "training_loss": 6.489023685455322
    },
    {
      "epoch": 0.15317073170731707,
      "step": 2826,
      "training_loss": 6.718019008636475
    },
    {
      "epoch": 0.1532249322493225,
      "step": 2827,
      "training_loss": 7.365563869476318
    },
    {
      "epoch": 0.1532791327913279,
      "grad_norm": 21.14395523071289,
      "learning_rate": 1e-05,
      "loss": 6.5723,
      "step": 2828
    },
    {
      "epoch": 0.1532791327913279,
      "step": 2828,
      "training_loss": 7.259206295013428
    },
    {
      "epoch": 0.15333333333333332,
      "step": 2829,
      "training_loss": 6.249456405639648
    },
    {
      "epoch": 0.15338753387533877,
      "step": 2830,
      "training_loss": 5.700502872467041
    },
    {
      "epoch": 0.15344173441734418,
      "step": 2831,
      "training_loss": 7.0566725730896
    },
    {
      "epoch": 0.1534959349593496,
      "grad_norm": 14.497147560119629,
      "learning_rate": 1e-05,
      "loss": 6.5665,
      "step": 2832
    },
    {
      "epoch": 0.1534959349593496,
      "step": 2832,
      "training_loss": 5.515836238861084
    },
    {
      "epoch": 0.15355013550135502,
      "step": 2833,
      "training_loss": 7.052157402038574
    },
    {
      "epoch": 0.15360433604336043,
      "step": 2834,
      "training_loss": 6.649300575256348
    },
    {
      "epoch": 0.15365853658536585,
      "step": 2835,
      "training_loss": 7.26254940032959
    },
    {
      "epoch": 0.15371273712737127,
      "grad_norm": 20.538881301879883,
      "learning_rate": 1e-05,
      "loss": 6.62,
      "step": 2836
    },
    {
      "epoch": 0.15371273712737127,
      "step": 2836,
      "training_loss": 7.1271796226501465
    },
    {
      "epoch": 0.15376693766937669,
      "step": 2837,
      "training_loss": 6.447570323944092
    },
    {
      "epoch": 0.1538211382113821,
      "step": 2838,
      "training_loss": 6.9930100440979
    },
    {
      "epoch": 0.15387533875338755,
      "step": 2839,
      "training_loss": 7.503795146942139
    },
    {
      "epoch": 0.15392953929539296,
      "grad_norm": 20.24793243408203,
      "learning_rate": 1e-05,
      "loss": 7.0179,
      "step": 2840
    },
    {
      "epoch": 0.15392953929539296,
      "step": 2840,
      "training_loss": 7.438872814178467
    },
    {
      "epoch": 0.15398373983739838,
      "step": 2841,
      "training_loss": 6.756739616394043
    },
    {
      "epoch": 0.1540379403794038,
      "step": 2842,
      "training_loss": 7.526775360107422
    },
    {
      "epoch": 0.15409214092140922,
      "step": 2843,
      "training_loss": 6.844085693359375
    },
    {
      "epoch": 0.15414634146341463,
      "grad_norm": 24.70453453063965,
      "learning_rate": 1e-05,
      "loss": 7.1416,
      "step": 2844
    },
    {
      "epoch": 0.15414634146341463,
      "step": 2844,
      "training_loss": 6.6657490730285645
    },
    {
      "epoch": 0.15420054200542005,
      "step": 2845,
      "training_loss": 6.371633529663086
    },
    {
      "epoch": 0.15425474254742547,
      "step": 2846,
      "training_loss": 6.308870315551758
    },
    {
      "epoch": 0.15430894308943088,
      "step": 2847,
      "training_loss": 6.957880020141602
    },
    {
      "epoch": 0.15436314363143633,
      "grad_norm": 36.64063262939453,
      "learning_rate": 1e-05,
      "loss": 6.576,
      "step": 2848
    },
    {
      "epoch": 0.15436314363143633,
      "step": 2848,
      "training_loss": 6.810107231140137
    },
    {
      "epoch": 0.15441734417344175,
      "step": 2849,
      "training_loss": 8.678108215332031
    },
    {
      "epoch": 0.15447154471544716,
      "step": 2850,
      "training_loss": 6.493203163146973
    },
    {
      "epoch": 0.15452574525745258,
      "step": 2851,
      "training_loss": 7.394636154174805
    },
    {
      "epoch": 0.154579945799458,
      "grad_norm": 21.090574264526367,
      "learning_rate": 1e-05,
      "loss": 7.344,
      "step": 2852
    },
    {
      "epoch": 0.154579945799458,
      "step": 2852,
      "training_loss": 6.893336772918701
    },
    {
      "epoch": 0.1546341463414634,
      "step": 2853,
      "training_loss": 8.11740493774414
    },
    {
      "epoch": 0.15468834688346883,
      "step": 2854,
      "training_loss": 7.34279727935791
    },
    {
      "epoch": 0.15474254742547425,
      "step": 2855,
      "training_loss": 7.38548469543457
    },
    {
      "epoch": 0.15479674796747966,
      "grad_norm": 18.32781410217285,
      "learning_rate": 1e-05,
      "loss": 7.4348,
      "step": 2856
    },
    {
      "epoch": 0.15479674796747966,
      "step": 2856,
      "training_loss": 7.451510429382324
    },
    {
      "epoch": 0.15485094850948508,
      "step": 2857,
      "training_loss": 3.8825275897979736
    },
    {
      "epoch": 0.15490514905149053,
      "step": 2858,
      "training_loss": 7.525318622589111
    },
    {
      "epoch": 0.15495934959349594,
      "step": 2859,
      "training_loss": 7.2286176681518555
    },
    {
      "epoch": 0.15501355013550136,
      "grad_norm": 19.2498779296875,
      "learning_rate": 1e-05,
      "loss": 6.522,
      "step": 2860
    },
    {
      "epoch": 0.15501355013550136,
      "step": 2860,
      "training_loss": 5.746329307556152
    },
    {
      "epoch": 0.15506775067750678,
      "step": 2861,
      "training_loss": 8.426191329956055
    },
    {
      "epoch": 0.1551219512195122,
      "step": 2862,
      "training_loss": 6.7994866371154785
    },
    {
      "epoch": 0.1551761517615176,
      "step": 2863,
      "training_loss": 5.427798271179199
    },
    {
      "epoch": 0.15523035230352303,
      "grad_norm": 46.90581130981445,
      "learning_rate": 1e-05,
      "loss": 6.6,
      "step": 2864
    },
    {
      "epoch": 0.15523035230352303,
      "step": 2864,
      "training_loss": 5.347766876220703
    },
    {
      "epoch": 0.15528455284552845,
      "step": 2865,
      "training_loss": 7.108022689819336
    },
    {
      "epoch": 0.15533875338753386,
      "step": 2866,
      "training_loss": 7.885133266448975
    },
    {
      "epoch": 0.1553929539295393,
      "step": 2867,
      "training_loss": 7.103589057922363
    },
    {
      "epoch": 0.15544715447154472,
      "grad_norm": 27.888778686523438,
      "learning_rate": 1e-05,
      "loss": 6.8611,
      "step": 2868
    },
    {
      "epoch": 0.15544715447154472,
      "step": 2868,
      "training_loss": 6.86094331741333
    },
    {
      "epoch": 0.15550135501355014,
      "step": 2869,
      "training_loss": 6.1615986824035645
    },
    {
      "epoch": 0.15555555555555556,
      "step": 2870,
      "training_loss": 5.908816337585449
    },
    {
      "epoch": 0.15560975609756098,
      "step": 2871,
      "training_loss": 7.663077354431152
    },
    {
      "epoch": 0.1556639566395664,
      "grad_norm": 34.526859283447266,
      "learning_rate": 1e-05,
      "loss": 6.6486,
      "step": 2872
    },
    {
      "epoch": 0.1556639566395664,
      "step": 2872,
      "training_loss": 6.351736068725586
    },
    {
      "epoch": 0.1557181571815718,
      "step": 2873,
      "training_loss": 5.865800380706787
    },
    {
      "epoch": 0.15577235772357723,
      "step": 2874,
      "training_loss": 6.7286224365234375
    },
    {
      "epoch": 0.15582655826558264,
      "step": 2875,
      "training_loss": 6.431277275085449
    },
    {
      "epoch": 0.1558807588075881,
      "grad_norm": 20.1003475189209,
      "learning_rate": 1e-05,
      "loss": 6.3444,
      "step": 2876
    },
    {
      "epoch": 0.1558807588075881,
      "step": 2876,
      "training_loss": 6.754427909851074
    },
    {
      "epoch": 0.1559349593495935,
      "step": 2877,
      "training_loss": 6.780934810638428
    },
    {
      "epoch": 0.15598915989159892,
      "step": 2878,
      "training_loss": 7.198507785797119
    },
    {
      "epoch": 0.15604336043360434,
      "step": 2879,
      "training_loss": 8.61989688873291
    },
    {
      "epoch": 0.15609756097560976,
      "grad_norm": 30.530954360961914,
      "learning_rate": 1e-05,
      "loss": 7.3384,
      "step": 2880
    },
    {
      "epoch": 0.15609756097560976,
      "step": 2880,
      "training_loss": 4.937347888946533
    },
    {
      "epoch": 0.15615176151761517,
      "step": 2881,
      "training_loss": 6.741906642913818
    },
    {
      "epoch": 0.1562059620596206,
      "step": 2882,
      "training_loss": 6.947394847869873
    },
    {
      "epoch": 0.156260162601626,
      "step": 2883,
      "training_loss": 7.205965518951416
    },
    {
      "epoch": 0.15631436314363142,
      "grad_norm": 19.211959838867188,
      "learning_rate": 1e-05,
      "loss": 6.4582,
      "step": 2884
    },
    {
      "epoch": 0.15631436314363142,
      "step": 2884,
      "training_loss": 6.8303608894348145
    },
    {
      "epoch": 0.15636856368563687,
      "step": 2885,
      "training_loss": 5.653949737548828
    },
    {
      "epoch": 0.15642276422764229,
      "step": 2886,
      "training_loss": 6.185618877410889
    },
    {
      "epoch": 0.1564769647696477,
      "step": 2887,
      "training_loss": 7.253644943237305
    },
    {
      "epoch": 0.15653116531165312,
      "grad_norm": 34.4869499206543,
      "learning_rate": 1e-05,
      "loss": 6.4809,
      "step": 2888
    },
    {
      "epoch": 0.15653116531165312,
      "step": 2888,
      "training_loss": 7.342574119567871
    },
    {
      "epoch": 0.15658536585365854,
      "step": 2889,
      "training_loss": 6.886693477630615
    },
    {
      "epoch": 0.15663956639566395,
      "step": 2890,
      "training_loss": 6.285322666168213
    },
    {
      "epoch": 0.15669376693766937,
      "step": 2891,
      "training_loss": 5.830822944641113
    },
    {
      "epoch": 0.1567479674796748,
      "grad_norm": 22.661685943603516,
      "learning_rate": 1e-05,
      "loss": 6.5864,
      "step": 2892
    },
    {
      "epoch": 0.1567479674796748,
      "step": 2892,
      "training_loss": 6.716421604156494
    },
    {
      "epoch": 0.1568021680216802,
      "step": 2893,
      "training_loss": 7.420016765594482
    },
    {
      "epoch": 0.15685636856368565,
      "step": 2894,
      "training_loss": 7.3783063888549805
    },
    {
      "epoch": 0.15691056910569107,
      "step": 2895,
      "training_loss": 5.9821038246154785
    },
    {
      "epoch": 0.15696476964769648,
      "grad_norm": 19.748342514038086,
      "learning_rate": 1e-05,
      "loss": 6.8742,
      "step": 2896
    },
    {
      "epoch": 0.15696476964769648,
      "step": 2896,
      "training_loss": 6.272614002227783
    },
    {
      "epoch": 0.1570189701897019,
      "step": 2897,
      "training_loss": 6.163705825805664
    },
    {
      "epoch": 0.15707317073170732,
      "step": 2898,
      "training_loss": 7.571549415588379
    },
    {
      "epoch": 0.15712737127371273,
      "step": 2899,
      "training_loss": 6.90871000289917
    },
    {
      "epoch": 0.15718157181571815,
      "grad_norm": 33.41717529296875,
      "learning_rate": 1e-05,
      "loss": 6.7291,
      "step": 2900
    },
    {
      "epoch": 0.15718157181571815,
      "step": 2900,
      "training_loss": 5.12941837310791
    },
    {
      "epoch": 0.15723577235772357,
      "step": 2901,
      "training_loss": 7.404757499694824
    },
    {
      "epoch": 0.15728997289972899,
      "step": 2902,
      "training_loss": 7.612482070922852
    },
    {
      "epoch": 0.15734417344173443,
      "step": 2903,
      "training_loss": 4.866520881652832
    },
    {
      "epoch": 0.15739837398373985,
      "grad_norm": 27.730783462524414,
      "learning_rate": 1e-05,
      "loss": 6.2533,
      "step": 2904
    },
    {
      "epoch": 0.15739837398373985,
      "step": 2904,
      "training_loss": 5.585017681121826
    },
    {
      "epoch": 0.15745257452574526,
      "step": 2905,
      "training_loss": 6.588736057281494
    },
    {
      "epoch": 0.15750677506775068,
      "step": 2906,
      "training_loss": 8.834848403930664
    },
    {
      "epoch": 0.1575609756097561,
      "step": 2907,
      "training_loss": 6.759108066558838
    },
    {
      "epoch": 0.15761517615176152,
      "grad_norm": 19.158987045288086,
      "learning_rate": 1e-05,
      "loss": 6.9419,
      "step": 2908
    },
    {
      "epoch": 0.15761517615176152,
      "step": 2908,
      "training_loss": 6.351749420166016
    },
    {
      "epoch": 0.15766937669376693,
      "step": 2909,
      "training_loss": 6.403289794921875
    },
    {
      "epoch": 0.15772357723577235,
      "step": 2910,
      "training_loss": 6.54787015914917
    },
    {
      "epoch": 0.15777777777777777,
      "step": 2911,
      "training_loss": 7.141826152801514
    },
    {
      "epoch": 0.1578319783197832,
      "grad_norm": 31.344697952270508,
      "learning_rate": 1e-05,
      "loss": 6.6112,
      "step": 2912
    },
    {
      "epoch": 0.1578319783197832,
      "step": 2912,
      "training_loss": 7.155374526977539
    },
    {
      "epoch": 0.15788617886178863,
      "step": 2913,
      "training_loss": 7.525177955627441
    },
    {
      "epoch": 0.15794037940379405,
      "step": 2914,
      "training_loss": 6.3772430419921875
    },
    {
      "epoch": 0.15799457994579946,
      "step": 2915,
      "training_loss": 7.657386779785156
    },
    {
      "epoch": 0.15804878048780488,
      "grad_norm": 24.08736801147461,
      "learning_rate": 1e-05,
      "loss": 7.1788,
      "step": 2916
    },
    {
      "epoch": 0.15804878048780488,
      "step": 2916,
      "training_loss": 7.312280654907227
    },
    {
      "epoch": 0.1581029810298103,
      "step": 2917,
      "training_loss": 7.298388481140137
    },
    {
      "epoch": 0.1581571815718157,
      "step": 2918,
      "training_loss": 7.257785320281982
    },
    {
      "epoch": 0.15821138211382113,
      "step": 2919,
      "training_loss": 6.690280437469482
    },
    {
      "epoch": 0.15826558265582655,
      "grad_norm": 19.093996047973633,
      "learning_rate": 1e-05,
      "loss": 7.1397,
      "step": 2920
    },
    {
      "epoch": 0.15826558265582655,
      "step": 2920,
      "training_loss": 7.184230327606201
    },
    {
      "epoch": 0.15831978319783196,
      "step": 2921,
      "training_loss": 6.872853755950928
    },
    {
      "epoch": 0.1583739837398374,
      "step": 2922,
      "training_loss": 6.759646892547607
    },
    {
      "epoch": 0.15842818428184283,
      "step": 2923,
      "training_loss": 8.62765121459961
    },
    {
      "epoch": 0.15848238482384824,
      "grad_norm": 20.097896575927734,
      "learning_rate": 1e-05,
      "loss": 7.3611,
      "step": 2924
    },
    {
      "epoch": 0.15848238482384824,
      "step": 2924,
      "training_loss": 8.058176040649414
    },
    {
      "epoch": 0.15853658536585366,
      "step": 2925,
      "training_loss": 6.972200870513916
    },
    {
      "epoch": 0.15859078590785908,
      "step": 2926,
      "training_loss": 7.216691493988037
    },
    {
      "epoch": 0.1586449864498645,
      "step": 2927,
      "training_loss": 8.45913028717041
    },
    {
      "epoch": 0.1586991869918699,
      "grad_norm": 60.1237678527832,
      "learning_rate": 1e-05,
      "loss": 7.6765,
      "step": 2928
    },
    {
      "epoch": 0.1586991869918699,
      "step": 2928,
      "training_loss": 7.066335678100586
    },
    {
      "epoch": 0.15875338753387533,
      "step": 2929,
      "training_loss": 6.921389102935791
    },
    {
      "epoch": 0.15880758807588075,
      "step": 2930,
      "training_loss": 6.835501194000244
    },
    {
      "epoch": 0.1588617886178862,
      "step": 2931,
      "training_loss": 7.324372291564941
    },
    {
      "epoch": 0.1589159891598916,
      "grad_norm": 18.133073806762695,
      "learning_rate": 1e-05,
      "loss": 7.0369,
      "step": 2932
    },
    {
      "epoch": 0.1589159891598916,
      "step": 2932,
      "training_loss": 6.630540370941162
    },
    {
      "epoch": 0.15897018970189702,
      "step": 2933,
      "training_loss": 5.975952625274658
    },
    {
      "epoch": 0.15902439024390244,
      "step": 2934,
      "training_loss": 8.555869102478027
    },
    {
      "epoch": 0.15907859078590786,
      "step": 2935,
      "training_loss": 5.589618682861328
    },
    {
      "epoch": 0.15913279132791328,
      "grad_norm": 27.929931640625,
      "learning_rate": 1e-05,
      "loss": 6.688,
      "step": 2936
    },
    {
      "epoch": 0.15913279132791328,
      "step": 2936,
      "training_loss": 7.464033126831055
    },
    {
      "epoch": 0.1591869918699187,
      "step": 2937,
      "training_loss": 7.311155796051025
    },
    {
      "epoch": 0.1592411924119241,
      "step": 2938,
      "training_loss": 6.561821937561035
    },
    {
      "epoch": 0.15929539295392953,
      "step": 2939,
      "training_loss": 7.215827465057373
    },
    {
      "epoch": 0.15934959349593497,
      "grad_norm": 31.612863540649414,
      "learning_rate": 1e-05,
      "loss": 7.1382,
      "step": 2940
    },
    {
      "epoch": 0.15934959349593497,
      "step": 2940,
      "training_loss": 7.76384973526001
    },
    {
      "epoch": 0.1594037940379404,
      "step": 2941,
      "training_loss": 5.759899616241455
    },
    {
      "epoch": 0.1594579945799458,
      "step": 2942,
      "training_loss": 6.909980773925781
    },
    {
      "epoch": 0.15951219512195122,
      "step": 2943,
      "training_loss": 5.849776744842529
    },
    {
      "epoch": 0.15956639566395664,
      "grad_norm": 23.873563766479492,
      "learning_rate": 1e-05,
      "loss": 6.5709,
      "step": 2944
    },
    {
      "epoch": 0.15956639566395664,
      "step": 2944,
      "training_loss": 7.537816524505615
    },
    {
      "epoch": 0.15962059620596206,
      "step": 2945,
      "training_loss": 6.690806865692139
    },
    {
      "epoch": 0.15967479674796747,
      "step": 2946,
      "training_loss": 6.7462849617004395
    },
    {
      "epoch": 0.1597289972899729,
      "step": 2947,
      "training_loss": 6.3421173095703125
    },
    {
      "epoch": 0.1597831978319783,
      "grad_norm": 28.810468673706055,
      "learning_rate": 1e-05,
      "loss": 6.8293,
      "step": 2948
    },
    {
      "epoch": 0.1597831978319783,
      "step": 2948,
      "training_loss": 6.755227088928223
    },
    {
      "epoch": 0.15983739837398375,
      "step": 2949,
      "training_loss": 6.545165538787842
    },
    {
      "epoch": 0.15989159891598917,
      "step": 2950,
      "training_loss": 7.795363426208496
    },
    {
      "epoch": 0.1599457994579946,
      "step": 2951,
      "training_loss": 6.14130163192749
    },
    {
      "epoch": 0.16,
      "grad_norm": 15.153792381286621,
      "learning_rate": 1e-05,
      "loss": 6.8093,
      "step": 2952
    },
    {
      "epoch": 0.16,
      "step": 2952,
      "training_loss": 7.396259784698486
    },
    {
      "epoch": 0.16005420054200542,
      "step": 2953,
      "training_loss": 7.645601272583008
    },
    {
      "epoch": 0.16010840108401084,
      "step": 2954,
      "training_loss": 7.682761192321777
    },
    {
      "epoch": 0.16016260162601625,
      "step": 2955,
      "training_loss": 6.86782693862915
    },
    {
      "epoch": 0.16021680216802167,
      "grad_norm": 22.44603157043457,
      "learning_rate": 1e-05,
      "loss": 7.3981,
      "step": 2956
    },
    {
      "epoch": 0.16021680216802167,
      "step": 2956,
      "training_loss": 6.718705177307129
    },
    {
      "epoch": 0.1602710027100271,
      "step": 2957,
      "training_loss": 7.335902214050293
    },
    {
      "epoch": 0.16032520325203253,
      "step": 2958,
      "training_loss": 5.903501510620117
    },
    {
      "epoch": 0.16037940379403795,
      "step": 2959,
      "training_loss": 7.022304058074951
    },
    {
      "epoch": 0.16043360433604337,
      "grad_norm": 16.520267486572266,
      "learning_rate": 1e-05,
      "loss": 6.7451,
      "step": 2960
    },
    {
      "epoch": 0.16043360433604337,
      "step": 2960,
      "training_loss": 6.148603439331055
    },
    {
      "epoch": 0.16048780487804878,
      "step": 2961,
      "training_loss": 6.776230812072754
    },
    {
      "epoch": 0.1605420054200542,
      "step": 2962,
      "training_loss": 7.218337535858154
    },
    {
      "epoch": 0.16059620596205962,
      "step": 2963,
      "training_loss": 5.7717790603637695
    },
    {
      "epoch": 0.16065040650406504,
      "grad_norm": 17.16210174560547,
      "learning_rate": 1e-05,
      "loss": 6.4787,
      "step": 2964
    },
    {
      "epoch": 0.16065040650406504,
      "step": 2964,
      "training_loss": 7.78719425201416
    },
    {
      "epoch": 0.16070460704607045,
      "step": 2965,
      "training_loss": 5.902966022491455
    },
    {
      "epoch": 0.16075880758807587,
      "step": 2966,
      "training_loss": 6.517284393310547
    },
    {
      "epoch": 0.16081300813008131,
      "step": 2967,
      "training_loss": 4.077859401702881
    },
    {
      "epoch": 0.16086720867208673,
      "grad_norm": 23.341978073120117,
      "learning_rate": 1e-05,
      "loss": 6.0713,
      "step": 2968
    },
    {
      "epoch": 0.16086720867208673,
      "step": 2968,
      "training_loss": 6.580926418304443
    },
    {
      "epoch": 0.16092140921409215,
      "step": 2969,
      "training_loss": 7.023199558258057
    },
    {
      "epoch": 0.16097560975609757,
      "step": 2970,
      "training_loss": 7.310860633850098
    },
    {
      "epoch": 0.16102981029810298,
      "step": 2971,
      "training_loss": 6.431434631347656
    },
    {
      "epoch": 0.1610840108401084,
      "grad_norm": 33.148582458496094,
      "learning_rate": 1e-05,
      "loss": 6.8366,
      "step": 2972
    },
    {
      "epoch": 0.1610840108401084,
      "step": 2972,
      "training_loss": 5.974183559417725
    },
    {
      "epoch": 0.16113821138211382,
      "step": 2973,
      "training_loss": 5.247602462768555
    },
    {
      "epoch": 0.16119241192411923,
      "step": 2974,
      "training_loss": 7.360989093780518
    },
    {
      "epoch": 0.16124661246612465,
      "step": 2975,
      "training_loss": 7.905933856964111
    },
    {
      "epoch": 0.1613008130081301,
      "grad_norm": 18.197437286376953,
      "learning_rate": 1e-05,
      "loss": 6.6222,
      "step": 2976
    },
    {
      "epoch": 0.1613008130081301,
      "step": 2976,
      "training_loss": 6.763075351715088
    },
    {
      "epoch": 0.1613550135501355,
      "step": 2977,
      "training_loss": 6.837372303009033
    },
    {
      "epoch": 0.16140921409214093,
      "step": 2978,
      "training_loss": 6.511440277099609
    },
    {
      "epoch": 0.16146341463414635,
      "step": 2979,
      "training_loss": 6.540027618408203
    },
    {
      "epoch": 0.16151761517615176,
      "grad_norm": 21.105897903442383,
      "learning_rate": 1e-05,
      "loss": 6.663,
      "step": 2980
    },
    {
      "epoch": 0.16151761517615176,
      "step": 2980,
      "training_loss": 7.701763153076172
    },
    {
      "epoch": 0.16157181571815718,
      "step": 2981,
      "training_loss": 6.073128700256348
    },
    {
      "epoch": 0.1616260162601626,
      "step": 2982,
      "training_loss": 6.560590744018555
    },
    {
      "epoch": 0.16168021680216801,
      "step": 2983,
      "training_loss": 7.808140277862549
    },
    {
      "epoch": 0.16173441734417343,
      "grad_norm": 30.810598373413086,
      "learning_rate": 1e-05,
      "loss": 7.0359,
      "step": 2984
    },
    {
      "epoch": 0.16173441734417343,
      "step": 2984,
      "training_loss": 7.014292240142822
    },
    {
      "epoch": 0.16178861788617885,
      "step": 2985,
      "training_loss": 7.17975378036499
    },
    {
      "epoch": 0.1618428184281843,
      "step": 2986,
      "training_loss": 6.069551944732666
    },
    {
      "epoch": 0.1618970189701897,
      "step": 2987,
      "training_loss": 7.328714847564697
    },
    {
      "epoch": 0.16195121951219513,
      "grad_norm": 14.939138412475586,
      "learning_rate": 1e-05,
      "loss": 6.8981,
      "step": 2988
    },
    {
      "epoch": 0.16195121951219513,
      "step": 2988,
      "training_loss": 7.748476982116699
    },
    {
      "epoch": 0.16200542005420054,
      "step": 2989,
      "training_loss": 5.465795040130615
    },
    {
      "epoch": 0.16205962059620596,
      "step": 2990,
      "training_loss": 7.206252574920654
    },
    {
      "epoch": 0.16211382113821138,
      "step": 2991,
      "training_loss": 6.936278820037842
    },
    {
      "epoch": 0.1621680216802168,
      "grad_norm": 15.578089714050293,
      "learning_rate": 1e-05,
      "loss": 6.8392,
      "step": 2992
    },
    {
      "epoch": 0.1621680216802168,
      "step": 2992,
      "training_loss": 7.004034996032715
    },
    {
      "epoch": 0.1622222222222222,
      "step": 2993,
      "training_loss": 7.50494909286499
    },
    {
      "epoch": 0.16227642276422763,
      "step": 2994,
      "training_loss": 7.0521392822265625
    },
    {
      "epoch": 0.16233062330623307,
      "step": 2995,
      "training_loss": 5.744019508361816
    },
    {
      "epoch": 0.1623848238482385,
      "grad_norm": 25.260517120361328,
      "learning_rate": 1e-05,
      "loss": 6.8263,
      "step": 2996
    },
    {
      "epoch": 0.1623848238482385,
      "step": 2996,
      "training_loss": 6.471571922302246
    },
    {
      "epoch": 0.1624390243902439,
      "step": 2997,
      "training_loss": 7.002123832702637
    },
    {
      "epoch": 0.16249322493224932,
      "step": 2998,
      "training_loss": 7.546938896179199
    },
    {
      "epoch": 0.16254742547425474,
      "step": 2999,
      "training_loss": 7.516677379608154
    },
    {
      "epoch": 0.16260162601626016,
      "grad_norm": 20.08682632446289,
      "learning_rate": 1e-05,
      "loss": 7.1343,
      "step": 3000
    },
    {
      "epoch": 0.16260162601626016,
      "step": 3000,
      "training_loss": 5.769819259643555
    },
    {
      "epoch": 0.16265582655826558,
      "step": 3001,
      "training_loss": 10.680994987487793
    },
    {
      "epoch": 0.162710027100271,
      "step": 3002,
      "training_loss": 6.731485843658447
    },
    {
      "epoch": 0.1627642276422764,
      "step": 3003,
      "training_loss": 6.923071384429932
    },
    {
      "epoch": 0.16281842818428185,
      "grad_norm": 14.367572784423828,
      "learning_rate": 1e-05,
      "loss": 7.5263,
      "step": 3004
    },
    {
      "epoch": 0.16281842818428185,
      "step": 3004,
      "training_loss": 7.621476173400879
    },
    {
      "epoch": 0.16287262872628727,
      "step": 3005,
      "training_loss": 7.175113201141357
    },
    {
      "epoch": 0.1629268292682927,
      "step": 3006,
      "training_loss": 7.4440999031066895
    },
    {
      "epoch": 0.1629810298102981,
      "step": 3007,
      "training_loss": 7.67173433303833
    },
    {
      "epoch": 0.16303523035230352,
      "grad_norm": 23.191003799438477,
      "learning_rate": 1e-05,
      "loss": 7.4781,
      "step": 3008
    },
    {
      "epoch": 0.16303523035230352,
      "step": 3008,
      "training_loss": 7.437249183654785
    },
    {
      "epoch": 0.16308943089430894,
      "step": 3009,
      "training_loss": 7.108950138092041
    },
    {
      "epoch": 0.16314363143631436,
      "step": 3010,
      "training_loss": 5.40097188949585
    },
    {
      "epoch": 0.16319783197831977,
      "step": 3011,
      "training_loss": 6.818798542022705
    },
    {
      "epoch": 0.1632520325203252,
      "grad_norm": 24.684803009033203,
      "learning_rate": 1e-05,
      "loss": 6.6915,
      "step": 3012
    },
    {
      "epoch": 0.1632520325203252,
      "step": 3012,
      "training_loss": 7.505446434020996
    },
    {
      "epoch": 0.16330623306233064,
      "step": 3013,
      "training_loss": 7.148746490478516
    },
    {
      "epoch": 0.16336043360433605,
      "step": 3014,
      "training_loss": 6.43250846862793
    },
    {
      "epoch": 0.16341463414634147,
      "step": 3015,
      "training_loss": 6.915080547332764
    },
    {
      "epoch": 0.1634688346883469,
      "grad_norm": 23.25904655456543,
      "learning_rate": 1e-05,
      "loss": 7.0004,
      "step": 3016
    },
    {
      "epoch": 0.1634688346883469,
      "step": 3016,
      "training_loss": 7.0306196212768555
    },
    {
      "epoch": 0.1635230352303523,
      "step": 3017,
      "training_loss": 6.743037700653076
    },
    {
      "epoch": 0.16357723577235772,
      "step": 3018,
      "training_loss": 6.103106498718262
    },
    {
      "epoch": 0.16363143631436314,
      "step": 3019,
      "training_loss": 6.64677619934082
    },
    {
      "epoch": 0.16368563685636855,
      "grad_norm": 20.784542083740234,
      "learning_rate": 1e-05,
      "loss": 6.6309,
      "step": 3020
    },
    {
      "epoch": 0.16368563685636855,
      "step": 3020,
      "training_loss": 7.416233539581299
    },
    {
      "epoch": 0.16373983739837397,
      "step": 3021,
      "training_loss": 7.746795177459717
    },
    {
      "epoch": 0.16379403794037942,
      "step": 3022,
      "training_loss": 7.151573657989502
    },
    {
      "epoch": 0.16384823848238483,
      "step": 3023,
      "training_loss": 6.856207370758057
    },
    {
      "epoch": 0.16390243902439025,
      "grad_norm": 15.727557182312012,
      "learning_rate": 1e-05,
      "loss": 7.2927,
      "step": 3024
    },
    {
      "epoch": 0.16390243902439025,
      "step": 3024,
      "training_loss": 6.165416240692139
    },
    {
      "epoch": 0.16395663956639567,
      "step": 3025,
      "training_loss": 8.213289260864258
    },
    {
      "epoch": 0.16401084010840108,
      "step": 3026,
      "training_loss": 6.973206996917725
    },
    {
      "epoch": 0.1640650406504065,
      "step": 3027,
      "training_loss": 6.507347106933594
    },
    {
      "epoch": 0.16411924119241192,
      "grad_norm": 27.191898345947266,
      "learning_rate": 1e-05,
      "loss": 6.9648,
      "step": 3028
    },
    {
      "epoch": 0.16411924119241192,
      "step": 3028,
      "training_loss": 7.474644184112549
    },
    {
      "epoch": 0.16417344173441734,
      "step": 3029,
      "training_loss": 6.960941314697266
    },
    {
      "epoch": 0.16422764227642275,
      "step": 3030,
      "training_loss": 7.219117641448975
    },
    {
      "epoch": 0.1642818428184282,
      "step": 3031,
      "training_loss": 7.206991195678711
    },
    {
      "epoch": 0.16433604336043361,
      "grad_norm": 30.17197608947754,
      "learning_rate": 1e-05,
      "loss": 7.2154,
      "step": 3032
    },
    {
      "epoch": 0.16433604336043361,
      "step": 3032,
      "training_loss": 6.730391025543213
    },
    {
      "epoch": 0.16439024390243903,
      "step": 3033,
      "training_loss": 4.737142086029053
    },
    {
      "epoch": 0.16444444444444445,
      "step": 3034,
      "training_loss": 6.7536725997924805
    },
    {
      "epoch": 0.16449864498644987,
      "step": 3035,
      "training_loss": 6.878812313079834
    },
    {
      "epoch": 0.16455284552845528,
      "grad_norm": 38.94382858276367,
      "learning_rate": 1e-05,
      "loss": 6.275,
      "step": 3036
    },
    {
      "epoch": 0.16455284552845528,
      "step": 3036,
      "training_loss": 6.4678215980529785
    },
    {
      "epoch": 0.1646070460704607,
      "step": 3037,
      "training_loss": 6.912356853485107
    },
    {
      "epoch": 0.16466124661246612,
      "step": 3038,
      "training_loss": 4.957960605621338
    },
    {
      "epoch": 0.16471544715447153,
      "step": 3039,
      "training_loss": 7.39825963973999
    },
    {
      "epoch": 0.16476964769647698,
      "grad_norm": 41.762451171875,
      "learning_rate": 1e-05,
      "loss": 6.4341,
      "step": 3040
    },
    {
      "epoch": 0.16476964769647698,
      "step": 3040,
      "training_loss": 7.542705535888672
    },
    {
      "epoch": 0.1648238482384824,
      "step": 3041,
      "training_loss": 5.586994171142578
    },
    {
      "epoch": 0.1648780487804878,
      "step": 3042,
      "training_loss": 6.544967174530029
    },
    {
      "epoch": 0.16493224932249323,
      "step": 3043,
      "training_loss": 7.472884178161621
    },
    {
      "epoch": 0.16498644986449865,
      "grad_norm": 33.550743103027344,
      "learning_rate": 1e-05,
      "loss": 6.7869,
      "step": 3044
    },
    {
      "epoch": 0.16498644986449865,
      "step": 3044,
      "training_loss": 7.897436141967773
    },
    {
      "epoch": 0.16504065040650406,
      "step": 3045,
      "training_loss": 7.175792694091797
    },
    {
      "epoch": 0.16509485094850948,
      "step": 3046,
      "training_loss": 4.800732135772705
    },
    {
      "epoch": 0.1651490514905149,
      "step": 3047,
      "training_loss": 6.889263153076172
    },
    {
      "epoch": 0.16520325203252031,
      "grad_norm": 20.25153923034668,
      "learning_rate": 1e-05,
      "loss": 6.6908,
      "step": 3048
    },
    {
      "epoch": 0.16520325203252031,
      "step": 3048,
      "training_loss": 7.55864953994751
    },
    {
      "epoch": 0.16525745257452573,
      "step": 3049,
      "training_loss": 4.602255821228027
    },
    {
      "epoch": 0.16531165311653118,
      "step": 3050,
      "training_loss": 6.225058555603027
    },
    {
      "epoch": 0.1653658536585366,
      "step": 3051,
      "training_loss": 6.786135673522949
    },
    {
      "epoch": 0.165420054200542,
      "grad_norm": 27.394487380981445,
      "learning_rate": 1e-05,
      "loss": 6.293,
      "step": 3052
    },
    {
      "epoch": 0.165420054200542,
      "step": 3052,
      "training_loss": 6.06257963180542
    },
    {
      "epoch": 0.16547425474254743,
      "step": 3053,
      "training_loss": 6.010027885437012
    },
    {
      "epoch": 0.16552845528455284,
      "step": 3054,
      "training_loss": 7.60477876663208
    },
    {
      "epoch": 0.16558265582655826,
      "step": 3055,
      "training_loss": 6.959472179412842
    },
    {
      "epoch": 0.16563685636856368,
      "grad_norm": 22.176677703857422,
      "learning_rate": 1e-05,
      "loss": 6.6592,
      "step": 3056
    },
    {
      "epoch": 0.16563685636856368,
      "step": 3056,
      "training_loss": 7.926078796386719
    },
    {
      "epoch": 0.1656910569105691,
      "step": 3057,
      "training_loss": 8.182756423950195
    },
    {
      "epoch": 0.1657452574525745,
      "step": 3058,
      "training_loss": 5.917239189147949
    },
    {
      "epoch": 0.16579945799457996,
      "step": 3059,
      "training_loss": 6.761448383331299
    },
    {
      "epoch": 0.16585365853658537,
      "grad_norm": 24.537160873413086,
      "learning_rate": 1e-05,
      "loss": 7.1969,
      "step": 3060
    },
    {
      "epoch": 0.16585365853658537,
      "step": 3060,
      "training_loss": 7.081878185272217
    },
    {
      "epoch": 0.1659078590785908,
      "step": 3061,
      "training_loss": 7.328185558319092
    },
    {
      "epoch": 0.1659620596205962,
      "step": 3062,
      "training_loss": 6.369275093078613
    },
    {
      "epoch": 0.16601626016260163,
      "step": 3063,
      "training_loss": 6.763922691345215
    },
    {
      "epoch": 0.16607046070460704,
      "grad_norm": 31.511489868164062,
      "learning_rate": 1e-05,
      "loss": 6.8858,
      "step": 3064
    },
    {
      "epoch": 0.16607046070460704,
      "step": 3064,
      "training_loss": 7.178269863128662
    },
    {
      "epoch": 0.16612466124661246,
      "step": 3065,
      "training_loss": 6.631086349487305
    },
    {
      "epoch": 0.16617886178861788,
      "step": 3066,
      "training_loss": 4.1674723625183105
    },
    {
      "epoch": 0.1662330623306233,
      "step": 3067,
      "training_loss": 6.83041524887085
    },
    {
      "epoch": 0.16628726287262874,
      "grad_norm": 18.849241256713867,
      "learning_rate": 1e-05,
      "loss": 6.2018,
      "step": 3068
    },
    {
      "epoch": 0.16628726287262874,
      "step": 3068,
      "training_loss": 7.932281494140625
    },
    {
      "epoch": 0.16634146341463416,
      "step": 3069,
      "training_loss": 6.995041370391846
    },
    {
      "epoch": 0.16639566395663957,
      "step": 3070,
      "training_loss": 4.595454216003418
    },
    {
      "epoch": 0.166449864498645,
      "step": 3071,
      "training_loss": 6.943033695220947
    },
    {
      "epoch": 0.1665040650406504,
      "grad_norm": 17.41122817993164,
      "learning_rate": 1e-05,
      "loss": 6.6165,
      "step": 3072
    },
    {
      "epoch": 0.1665040650406504,
      "step": 3072,
      "training_loss": 8.204055786132812
    },
    {
      "epoch": 0.16655826558265582,
      "step": 3073,
      "training_loss": 8.037402153015137
    },
    {
      "epoch": 0.16661246612466124,
      "step": 3074,
      "training_loss": 5.65858268737793
    },
    {
      "epoch": 0.16666666666666666,
      "step": 3075,
      "training_loss": 7.237144947052002
    },
    {
      "epoch": 0.16672086720867207,
      "grad_norm": 21.637319564819336,
      "learning_rate": 1e-05,
      "loss": 7.2843,
      "step": 3076
    },
    {
      "epoch": 0.16672086720867207,
      "step": 3076,
      "training_loss": 7.160490989685059
    },
    {
      "epoch": 0.16677506775067752,
      "step": 3077,
      "training_loss": 4.883201599121094
    },
    {
      "epoch": 0.16682926829268294,
      "step": 3078,
      "training_loss": 7.745425701141357
    },
    {
      "epoch": 0.16688346883468835,
      "step": 3079,
      "training_loss": 7.029091835021973
    },
    {
      "epoch": 0.16693766937669377,
      "grad_norm": 36.4002571105957,
      "learning_rate": 1e-05,
      "loss": 6.7046,
      "step": 3080
    },
    {
      "epoch": 0.16693766937669377,
      "step": 3080,
      "training_loss": 5.811226844787598
    },
    {
      "epoch": 0.1669918699186992,
      "step": 3081,
      "training_loss": 7.73484468460083
    },
    {
      "epoch": 0.1670460704607046,
      "step": 3082,
      "training_loss": 5.814998149871826
    },
    {
      "epoch": 0.16710027100271002,
      "step": 3083,
      "training_loss": 7.497524738311768
    },
    {
      "epoch": 0.16715447154471544,
      "grad_norm": 25.36642074584961,
      "learning_rate": 1e-05,
      "loss": 6.7146,
      "step": 3084
    },
    {
      "epoch": 0.16715447154471544,
      "step": 3084,
      "training_loss": 8.101901054382324
    },
    {
      "epoch": 0.16720867208672086,
      "step": 3085,
      "training_loss": 6.929892539978027
    },
    {
      "epoch": 0.1672628726287263,
      "step": 3086,
      "training_loss": 6.97938871383667
    },
    {
      "epoch": 0.16731707317073172,
      "step": 3087,
      "training_loss": 7.118065357208252
    },
    {
      "epoch": 0.16737127371273713,
      "grad_norm": 15.914959907531738,
      "learning_rate": 1e-05,
      "loss": 7.2823,
      "step": 3088
    },
    {
      "epoch": 0.16737127371273713,
      "step": 3088,
      "training_loss": 6.880626678466797
    },
    {
      "epoch": 0.16742547425474255,
      "step": 3089,
      "training_loss": 5.085703372955322
    },
    {
      "epoch": 0.16747967479674797,
      "step": 3090,
      "training_loss": 7.088844299316406
    },
    {
      "epoch": 0.16753387533875339,
      "step": 3091,
      "training_loss": 5.724661827087402
    },
    {
      "epoch": 0.1675880758807588,
      "grad_norm": 25.180042266845703,
      "learning_rate": 1e-05,
      "loss": 6.195,
      "step": 3092
    },
    {
      "epoch": 0.1675880758807588,
      "step": 3092,
      "training_loss": 6.92713737487793
    },
    {
      "epoch": 0.16764227642276422,
      "step": 3093,
      "training_loss": 7.261476039886475
    },
    {
      "epoch": 0.16769647696476964,
      "step": 3094,
      "training_loss": 6.161435127258301
    },
    {
      "epoch": 0.16775067750677508,
      "step": 3095,
      "training_loss": 6.982956409454346
    },
    {
      "epoch": 0.1678048780487805,
      "grad_norm": 17.577503204345703,
      "learning_rate": 1e-05,
      "loss": 6.8333,
      "step": 3096
    },
    {
      "epoch": 0.1678048780487805,
      "step": 3096,
      "training_loss": 7.264886856079102
    },
    {
      "epoch": 0.16785907859078592,
      "step": 3097,
      "training_loss": 6.866893291473389
    },
    {
      "epoch": 0.16791327913279133,
      "step": 3098,
      "training_loss": 7.713854789733887
    },
    {
      "epoch": 0.16796747967479675,
      "step": 3099,
      "training_loss": 5.279563903808594
    },
    {
      "epoch": 0.16802168021680217,
      "grad_norm": 47.223445892333984,
      "learning_rate": 1e-05,
      "loss": 6.7813,
      "step": 3100
    },
    {
      "epoch": 0.16802168021680217,
      "step": 3100,
      "training_loss": 5.647733211517334
    },
    {
      "epoch": 0.16807588075880758,
      "step": 3101,
      "training_loss": 7.685414791107178
    },
    {
      "epoch": 0.168130081300813,
      "step": 3102,
      "training_loss": 7.0499091148376465
    },
    {
      "epoch": 0.16818428184281842,
      "step": 3103,
      "training_loss": 7.747322082519531
    },
    {
      "epoch": 0.16823848238482386,
      "grad_norm": 16.89069175720215,
      "learning_rate": 1e-05,
      "loss": 7.0326,
      "step": 3104
    },
    {
      "epoch": 0.16823848238482386,
      "step": 3104,
      "training_loss": 6.98338508605957
    },
    {
      "epoch": 0.16829268292682928,
      "step": 3105,
      "training_loss": 6.849808216094971
    },
    {
      "epoch": 0.1683468834688347,
      "step": 3106,
      "training_loss": 6.071924686431885
    },
    {
      "epoch": 0.1684010840108401,
      "step": 3107,
      "training_loss": 6.734445095062256
    },
    {
      "epoch": 0.16845528455284553,
      "grad_norm": 23.535348892211914,
      "learning_rate": 1e-05,
      "loss": 6.6599,
      "step": 3108
    },
    {
      "epoch": 0.16845528455284553,
      "step": 3108,
      "training_loss": 7.023128032684326
    },
    {
      "epoch": 0.16850948509485095,
      "step": 3109,
      "training_loss": 4.155453681945801
    },
    {
      "epoch": 0.16856368563685636,
      "step": 3110,
      "training_loss": 4.943539619445801
    },
    {
      "epoch": 0.16861788617886178,
      "step": 3111,
      "training_loss": 7.421543121337891
    },
    {
      "epoch": 0.1686720867208672,
      "grad_norm": 20.524398803710938,
      "learning_rate": 1e-05,
      "loss": 5.8859,
      "step": 3112
    },
    {
      "epoch": 0.1686720867208672,
      "step": 3112,
      "training_loss": 5.883206844329834
    },
    {
      "epoch": 0.16872628726287262,
      "step": 3113,
      "training_loss": 5.795571327209473
    },
    {
      "epoch": 0.16878048780487806,
      "step": 3114,
      "training_loss": 6.883586406707764
    },
    {
      "epoch": 0.16883468834688348,
      "step": 3115,
      "training_loss": 7.044589519500732
    },
    {
      "epoch": 0.1688888888888889,
      "grad_norm": 17.307992935180664,
      "learning_rate": 1e-05,
      "loss": 6.4017,
      "step": 3116
    },
    {
      "epoch": 0.1688888888888889,
      "step": 3116,
      "training_loss": 7.030011177062988
    },
    {
      "epoch": 0.1689430894308943,
      "step": 3117,
      "training_loss": 6.61893367767334
    },
    {
      "epoch": 0.16899728997289973,
      "step": 3118,
      "training_loss": 6.592992305755615
    },
    {
      "epoch": 0.16905149051490515,
      "step": 3119,
      "training_loss": 7.057188034057617
    },
    {
      "epoch": 0.16910569105691056,
      "grad_norm": 32.32083511352539,
      "learning_rate": 1e-05,
      "loss": 6.8248,
      "step": 3120
    },
    {
      "epoch": 0.16910569105691056,
      "step": 3120,
      "training_loss": 7.2521491050720215
    },
    {
      "epoch": 0.16915989159891598,
      "step": 3121,
      "training_loss": 9.490034103393555
    },
    {
      "epoch": 0.1692140921409214,
      "step": 3122,
      "training_loss": 8.063058853149414
    },
    {
      "epoch": 0.16926829268292684,
      "step": 3123,
      "training_loss": 7.575556755065918
    },
    {
      "epoch": 0.16932249322493226,
      "grad_norm": 20.32110595703125,
      "learning_rate": 1e-05,
      "loss": 8.0952,
      "step": 3124
    },
    {
      "epoch": 0.16932249322493226,
      "step": 3124,
      "training_loss": 7.333987712860107
    },
    {
      "epoch": 0.16937669376693767,
      "step": 3125,
      "training_loss": 7.037868022918701
    },
    {
      "epoch": 0.1694308943089431,
      "step": 3126,
      "training_loss": 6.261382579803467
    },
    {
      "epoch": 0.1694850948509485,
      "step": 3127,
      "training_loss": 6.719054698944092
    },
    {
      "epoch": 0.16953929539295393,
      "grad_norm": 21.622461318969727,
      "learning_rate": 1e-05,
      "loss": 6.8381,
      "step": 3128
    },
    {
      "epoch": 0.16953929539295393,
      "step": 3128,
      "training_loss": 8.399873733520508
    },
    {
      "epoch": 0.16959349593495934,
      "step": 3129,
      "training_loss": 6.6672563552856445
    },
    {
      "epoch": 0.16964769647696476,
      "step": 3130,
      "training_loss": 7.172022342681885
    },
    {
      "epoch": 0.16970189701897018,
      "step": 3131,
      "training_loss": 8.93308162689209
    },
    {
      "epoch": 0.16975609756097562,
      "grad_norm": 51.62116622924805,
      "learning_rate": 1e-05,
      "loss": 7.7931,
      "step": 3132
    },
    {
      "epoch": 0.16975609756097562,
      "step": 3132,
      "training_loss": 7.328136444091797
    },
    {
      "epoch": 0.16981029810298104,
      "step": 3133,
      "training_loss": 7.744343280792236
    },
    {
      "epoch": 0.16986449864498646,
      "step": 3134,
      "training_loss": 6.7554240226745605
    },
    {
      "epoch": 0.16991869918699187,
      "step": 3135,
      "training_loss": 7.3132100105285645
    },
    {
      "epoch": 0.1699728997289973,
      "grad_norm": 22.913816452026367,
      "learning_rate": 1e-05,
      "loss": 7.2853,
      "step": 3136
    },
    {
      "epoch": 0.1699728997289973,
      "step": 3136,
      "training_loss": 7.068224906921387
    },
    {
      "epoch": 0.1700271002710027,
      "step": 3137,
      "training_loss": 6.088888168334961
    },
    {
      "epoch": 0.17008130081300812,
      "step": 3138,
      "training_loss": 7.17848014831543
    },
    {
      "epoch": 0.17013550135501354,
      "step": 3139,
      "training_loss": 7.391305446624756
    },
    {
      "epoch": 0.17018970189701896,
      "grad_norm": 18.83468246459961,
      "learning_rate": 1e-05,
      "loss": 6.9317,
      "step": 3140
    },
    {
      "epoch": 0.17018970189701896,
      "step": 3140,
      "training_loss": 7.276265621185303
    },
    {
      "epoch": 0.1702439024390244,
      "step": 3141,
      "training_loss": 7.125787734985352
    },
    {
      "epoch": 0.17029810298102982,
      "step": 3142,
      "training_loss": 7.342701435089111
    },
    {
      "epoch": 0.17035230352303524,
      "step": 3143,
      "training_loss": 6.469883918762207
    },
    {
      "epoch": 0.17040650406504065,
      "grad_norm": 29.855716705322266,
      "learning_rate": 1e-05,
      "loss": 7.0537,
      "step": 3144
    },
    {
      "epoch": 0.17040650406504065,
      "step": 3144,
      "training_loss": 7.030792236328125
    },
    {
      "epoch": 0.17046070460704607,
      "step": 3145,
      "training_loss": 7.1502685546875
    },
    {
      "epoch": 0.1705149051490515,
      "step": 3146,
      "training_loss": 8.052061080932617
    },
    {
      "epoch": 0.1705691056910569,
      "step": 3147,
      "training_loss": 6.9613847732543945
    },
    {
      "epoch": 0.17062330623306232,
      "grad_norm": 28.797874450683594,
      "learning_rate": 1e-05,
      "loss": 7.2986,
      "step": 3148
    },
    {
      "epoch": 0.17062330623306232,
      "step": 3148,
      "training_loss": 7.702326774597168
    },
    {
      "epoch": 0.17067750677506774,
      "step": 3149,
      "training_loss": 8.024182319641113
    },
    {
      "epoch": 0.17073170731707318,
      "step": 3150,
      "training_loss": 5.567826747894287
    },
    {
      "epoch": 0.1707859078590786,
      "step": 3151,
      "training_loss": 6.948361396789551
    },
    {
      "epoch": 0.17084010840108402,
      "grad_norm": 21.152616500854492,
      "learning_rate": 1e-05,
      "loss": 7.0607,
      "step": 3152
    },
    {
      "epoch": 0.17084010840108402,
      "step": 3152,
      "training_loss": 5.869195938110352
    },
    {
      "epoch": 0.17089430894308943,
      "step": 3153,
      "training_loss": 6.852989673614502
    },
    {
      "epoch": 0.17094850948509485,
      "step": 3154,
      "training_loss": 7.8581743240356445
    },
    {
      "epoch": 0.17100271002710027,
      "step": 3155,
      "training_loss": 7.477871417999268
    },
    {
      "epoch": 0.17105691056910569,
      "grad_norm": 23.43837547302246,
      "learning_rate": 1e-05,
      "loss": 7.0146,
      "step": 3156
    },
    {
      "epoch": 0.17105691056910569,
      "step": 3156,
      "training_loss": 7.0970330238342285
    },
    {
      "epoch": 0.1711111111111111,
      "step": 3157,
      "training_loss": 5.651440620422363
    },
    {
      "epoch": 0.17116531165311652,
      "step": 3158,
      "training_loss": 7.003447532653809
    },
    {
      "epoch": 0.17121951219512196,
      "step": 3159,
      "training_loss": 6.804032325744629
    },
    {
      "epoch": 0.17127371273712738,
      "grad_norm": 29.328453063964844,
      "learning_rate": 1e-05,
      "loss": 6.639,
      "step": 3160
    },
    {
      "epoch": 0.17127371273712738,
      "step": 3160,
      "training_loss": 6.867862701416016
    },
    {
      "epoch": 0.1713279132791328,
      "step": 3161,
      "training_loss": 7.63116455078125
    },
    {
      "epoch": 0.17138211382113822,
      "step": 3162,
      "training_loss": 6.925065994262695
    },
    {
      "epoch": 0.17143631436314363,
      "step": 3163,
      "training_loss": 6.144101619720459
    },
    {
      "epoch": 0.17149051490514905,
      "grad_norm": 35.97827911376953,
      "learning_rate": 1e-05,
      "loss": 6.892,
      "step": 3164
    },
    {
      "epoch": 0.17149051490514905,
      "step": 3164,
      "training_loss": 7.160562992095947
    },
    {
      "epoch": 0.17154471544715447,
      "step": 3165,
      "training_loss": 7.436864376068115
    },
    {
      "epoch": 0.17159891598915988,
      "step": 3166,
      "training_loss": 7.45028018951416
    },
    {
      "epoch": 0.1716531165311653,
      "step": 3167,
      "training_loss": 6.950116157531738
    },
    {
      "epoch": 0.17170731707317075,
      "grad_norm": 22.69205665588379,
      "learning_rate": 1e-05,
      "loss": 7.2495,
      "step": 3168
    },
    {
      "epoch": 0.17170731707317075,
      "step": 3168,
      "training_loss": 7.836371421813965
    },
    {
      "epoch": 0.17176151761517616,
      "step": 3169,
      "training_loss": 6.688650131225586
    },
    {
      "epoch": 0.17181571815718158,
      "step": 3170,
      "training_loss": 7.153052806854248
    },
    {
      "epoch": 0.171869918699187,
      "step": 3171,
      "training_loss": 7.338819980621338
    },
    {
      "epoch": 0.1719241192411924,
      "grad_norm": 29.253459930419922,
      "learning_rate": 1e-05,
      "loss": 7.2542,
      "step": 3172
    },
    {
      "epoch": 0.1719241192411924,
      "step": 3172,
      "training_loss": 7.612550735473633
    },
    {
      "epoch": 0.17197831978319783,
      "step": 3173,
      "training_loss": 5.6997575759887695
    },
    {
      "epoch": 0.17203252032520325,
      "step": 3174,
      "training_loss": 6.6111578941345215
    },
    {
      "epoch": 0.17208672086720866,
      "step": 3175,
      "training_loss": 7.370857238769531
    },
    {
      "epoch": 0.17214092140921408,
      "grad_norm": 40.226863861083984,
      "learning_rate": 1e-05,
      "loss": 6.8236,
      "step": 3176
    },
    {
      "epoch": 0.17214092140921408,
      "step": 3176,
      "training_loss": 7.253020763397217
    },
    {
      "epoch": 0.1721951219512195,
      "step": 3177,
      "training_loss": 6.341847896575928
    },
    {
      "epoch": 0.17224932249322494,
      "step": 3178,
      "training_loss": 7.532234191894531
    },
    {
      "epoch": 0.17230352303523036,
      "step": 3179,
      "training_loss": 6.378076553344727
    },
    {
      "epoch": 0.17235772357723578,
      "grad_norm": 21.195556640625,
      "learning_rate": 1e-05,
      "loss": 6.8763,
      "step": 3180
    },
    {
      "epoch": 0.17235772357723578,
      "step": 3180,
      "training_loss": 7.553975582122803
    },
    {
      "epoch": 0.1724119241192412,
      "step": 3181,
      "training_loss": 6.838134765625
    },
    {
      "epoch": 0.1724661246612466,
      "step": 3182,
      "training_loss": 7.186484336853027
    },
    {
      "epoch": 0.17252032520325203,
      "step": 3183,
      "training_loss": 4.762709140777588
    },
    {
      "epoch": 0.17257452574525745,
      "grad_norm": 27.985027313232422,
      "learning_rate": 1e-05,
      "loss": 6.5853,
      "step": 3184
    },
    {
      "epoch": 0.17257452574525745,
      "step": 3184,
      "training_loss": 7.077261447906494
    },
    {
      "epoch": 0.17262872628726286,
      "step": 3185,
      "training_loss": 6.913172245025635
    },
    {
      "epoch": 0.17268292682926828,
      "step": 3186,
      "training_loss": 6.7933526039123535
    },
    {
      "epoch": 0.17273712737127372,
      "step": 3187,
      "training_loss": 7.11514949798584
    },
    {
      "epoch": 0.17279132791327914,
      "grad_norm": 16.404638290405273,
      "learning_rate": 1e-05,
      "loss": 6.9747,
      "step": 3188
    },
    {
      "epoch": 0.17279132791327914,
      "step": 3188,
      "training_loss": 7.390235424041748
    },
    {
      "epoch": 0.17284552845528456,
      "step": 3189,
      "training_loss": 7.369512557983398
    },
    {
      "epoch": 0.17289972899728998,
      "step": 3190,
      "training_loss": 5.726130962371826
    },
    {
      "epoch": 0.1729539295392954,
      "step": 3191,
      "training_loss": 7.54180908203125
    },
    {
      "epoch": 0.1730081300813008,
      "grad_norm": 21.78600311279297,
      "learning_rate": 1e-05,
      "loss": 7.0069,
      "step": 3192
    },
    {
      "epoch": 0.1730081300813008,
      "step": 3192,
      "training_loss": 6.878592491149902
    },
    {
      "epoch": 0.17306233062330623,
      "step": 3193,
      "training_loss": 7.664971351623535
    },
    {
      "epoch": 0.17311653116531164,
      "step": 3194,
      "training_loss": 7.205855369567871
    },
    {
      "epoch": 0.17317073170731706,
      "step": 3195,
      "training_loss": 7.454861164093018
    },
    {
      "epoch": 0.1732249322493225,
      "grad_norm": 13.264493942260742,
      "learning_rate": 1e-05,
      "loss": 7.3011,
      "step": 3196
    },
    {
      "epoch": 0.1732249322493225,
      "step": 3196,
      "training_loss": 6.272412300109863
    },
    {
      "epoch": 0.17327913279132792,
      "step": 3197,
      "training_loss": 5.989239692687988
    },
    {
      "epoch": 0.17333333333333334,
      "step": 3198,
      "training_loss": 6.642259120941162
    },
    {
      "epoch": 0.17338753387533876,
      "step": 3199,
      "training_loss": 7.241647720336914
    },
    {
      "epoch": 0.17344173441734417,
      "grad_norm": 36.143707275390625,
      "learning_rate": 1e-05,
      "loss": 6.5364,
      "step": 3200
    },
    {
      "epoch": 0.17344173441734417,
      "step": 3200,
      "training_loss": 4.644891738891602
    },
    {
      "epoch": 0.1734959349593496,
      "step": 3201,
      "training_loss": 7.050055503845215
    },
    {
      "epoch": 0.173550135501355,
      "step": 3202,
      "training_loss": 6.916146755218506
    },
    {
      "epoch": 0.17360433604336042,
      "step": 3203,
      "training_loss": 5.252574920654297
    },
    {
      "epoch": 0.17365853658536584,
      "grad_norm": 37.01118087768555,
      "learning_rate": 1e-05,
      "loss": 5.9659,
      "step": 3204
    },
    {
      "epoch": 0.17365853658536584,
      "step": 3204,
      "training_loss": 6.228144645690918
    },
    {
      "epoch": 0.1737127371273713,
      "step": 3205,
      "training_loss": 7.36279821395874
    },
    {
      "epoch": 0.1737669376693767,
      "step": 3206,
      "training_loss": 7.684335708618164
    },
    {
      "epoch": 0.17382113821138212,
      "step": 3207,
      "training_loss": 7.533237934112549
    },
    {
      "epoch": 0.17387533875338754,
      "grad_norm": 29.32270050048828,
      "learning_rate": 1e-05,
      "loss": 7.2021,
      "step": 3208
    },
    {
      "epoch": 0.17387533875338754,
      "step": 3208,
      "training_loss": 7.269965171813965
    },
    {
      "epoch": 0.17392953929539295,
      "step": 3209,
      "training_loss": 6.206577777862549
    },
    {
      "epoch": 0.17398373983739837,
      "step": 3210,
      "training_loss": 7.829830169677734
    },
    {
      "epoch": 0.1740379403794038,
      "step": 3211,
      "training_loss": 5.941713333129883
    },
    {
      "epoch": 0.1740921409214092,
      "grad_norm": 35.47455596923828,
      "learning_rate": 1e-05,
      "loss": 6.812,
      "step": 3212
    },
    {
      "epoch": 0.1740921409214092,
      "step": 3212,
      "training_loss": 7.325170993804932
    },
    {
      "epoch": 0.17414634146341462,
      "step": 3213,
      "training_loss": 6.9300713539123535
    },
    {
      "epoch": 0.17420054200542007,
      "step": 3214,
      "training_loss": 7.543281078338623
    },
    {
      "epoch": 0.17425474254742548,
      "step": 3215,
      "training_loss": 7.318941116333008
    },
    {
      "epoch": 0.1743089430894309,
      "grad_norm": 21.31529998779297,
      "learning_rate": 1e-05,
      "loss": 7.2794,
      "step": 3216
    },
    {
      "epoch": 0.1743089430894309,
      "step": 3216,
      "training_loss": 7.178551197052002
    },
    {
      "epoch": 0.17436314363143632,
      "step": 3217,
      "training_loss": 7.263692855834961
    },
    {
      "epoch": 0.17441734417344174,
      "step": 3218,
      "training_loss": 7.215605735778809
    },
    {
      "epoch": 0.17447154471544715,
      "step": 3219,
      "training_loss": 6.388561725616455
    },
    {
      "epoch": 0.17452574525745257,
      "grad_norm": 17.529579162597656,
      "learning_rate": 1e-05,
      "loss": 7.0116,
      "step": 3220
    },
    {
      "epoch": 0.17452574525745257,
      "step": 3220,
      "training_loss": 7.307718753814697
    },
    {
      "epoch": 0.174579945799458,
      "step": 3221,
      "training_loss": 7.286611557006836
    },
    {
      "epoch": 0.1746341463414634,
      "step": 3222,
      "training_loss": 7.494560718536377
    },
    {
      "epoch": 0.17468834688346885,
      "step": 3223,
      "training_loss": 6.876011848449707
    },
    {
      "epoch": 0.17474254742547426,
      "grad_norm": 34.818321228027344,
      "learning_rate": 1e-05,
      "loss": 7.2412,
      "step": 3224
    },
    {
      "epoch": 0.17474254742547426,
      "step": 3224,
      "training_loss": 6.961209774017334
    },
    {
      "epoch": 0.17479674796747968,
      "step": 3225,
      "training_loss": 7.2172441482543945
    },
    {
      "epoch": 0.1748509485094851,
      "step": 3226,
      "training_loss": 6.1304707527160645
    },
    {
      "epoch": 0.17490514905149052,
      "step": 3227,
      "training_loss": 5.204795837402344
    },
    {
      "epoch": 0.17495934959349593,
      "grad_norm": 27.94009017944336,
      "learning_rate": 1e-05,
      "loss": 6.3784,
      "step": 3228
    },
    {
      "epoch": 0.17495934959349593,
      "step": 3228,
      "training_loss": 7.87689208984375
    },
    {
      "epoch": 0.17501355013550135,
      "step": 3229,
      "training_loss": 8.491917610168457
    },
    {
      "epoch": 0.17506775067750677,
      "step": 3230,
      "training_loss": 6.5653076171875
    },
    {
      "epoch": 0.17512195121951218,
      "step": 3231,
      "training_loss": 7.178850173950195
    },
    {
      "epoch": 0.17517615176151763,
      "grad_norm": 15.335867881774902,
      "learning_rate": 1e-05,
      "loss": 7.5282,
      "step": 3232
    },
    {
      "epoch": 0.17517615176151763,
      "step": 3232,
      "training_loss": 6.095301628112793
    },
    {
      "epoch": 0.17523035230352305,
      "step": 3233,
      "training_loss": 7.424407005310059
    },
    {
      "epoch": 0.17528455284552846,
      "step": 3234,
      "training_loss": 4.420038223266602
    },
    {
      "epoch": 0.17533875338753388,
      "step": 3235,
      "training_loss": 6.062870979309082
    },
    {
      "epoch": 0.1753929539295393,
      "grad_norm": 26.156465530395508,
      "learning_rate": 1e-05,
      "loss": 6.0007,
      "step": 3236
    },
    {
      "epoch": 0.1753929539295393,
      "step": 3236,
      "training_loss": 6.944444179534912
    },
    {
      "epoch": 0.17544715447154471,
      "step": 3237,
      "training_loss": 7.077493667602539
    },
    {
      "epoch": 0.17550135501355013,
      "step": 3238,
      "training_loss": 7.582773208618164
    },
    {
      "epoch": 0.17555555555555555,
      "step": 3239,
      "training_loss": 7.386801719665527
    },
    {
      "epoch": 0.17560975609756097,
      "grad_norm": 14.9144287109375,
      "learning_rate": 1e-05,
      "loss": 7.2479,
      "step": 3240
    },
    {
      "epoch": 0.17560975609756097,
      "step": 3240,
      "training_loss": 7.715625286102295
    },
    {
      "epoch": 0.17566395663956638,
      "step": 3241,
      "training_loss": 7.068658828735352
    },
    {
      "epoch": 0.17571815718157183,
      "step": 3242,
      "training_loss": 6.4366679191589355
    },
    {
      "epoch": 0.17577235772357724,
      "step": 3243,
      "training_loss": 7.5511698722839355
    },
    {
      "epoch": 0.17582655826558266,
      "grad_norm": 21.348920822143555,
      "learning_rate": 1e-05,
      "loss": 7.193,
      "step": 3244
    },
    {
      "epoch": 0.17582655826558266,
      "step": 3244,
      "training_loss": 4.703373432159424
    },
    {
      "epoch": 0.17588075880758808,
      "step": 3245,
      "training_loss": 7.087835788726807
    },
    {
      "epoch": 0.1759349593495935,
      "step": 3246,
      "training_loss": 3.859253168106079
    },
    {
      "epoch": 0.1759891598915989,
      "step": 3247,
      "training_loss": 6.577881336212158
    },
    {
      "epoch": 0.17604336043360433,
      "grad_norm": 22.96194839477539,
      "learning_rate": 1e-05,
      "loss": 5.5571,
      "step": 3248
    },
    {
      "epoch": 0.17604336043360433,
      "step": 3248,
      "training_loss": 6.751131057739258
    },
    {
      "epoch": 0.17609756097560975,
      "step": 3249,
      "training_loss": 6.933318614959717
    },
    {
      "epoch": 0.17615176151761516,
      "step": 3250,
      "training_loss": 6.484048843383789
    },
    {
      "epoch": 0.1762059620596206,
      "step": 3251,
      "training_loss": 8.3451566696167
    },
    {
      "epoch": 0.17626016260162602,
      "grad_norm": 30.493253707885742,
      "learning_rate": 1e-05,
      "loss": 7.1284,
      "step": 3252
    },
    {
      "epoch": 0.17626016260162602,
      "step": 3252,
      "training_loss": 6.663568019866943
    },
    {
      "epoch": 0.17631436314363144,
      "step": 3253,
      "training_loss": 7.993785381317139
    },
    {
      "epoch": 0.17636856368563686,
      "step": 3254,
      "training_loss": 5.558472156524658
    },
    {
      "epoch": 0.17642276422764228,
      "step": 3255,
      "training_loss": 6.624032497406006
    },
    {
      "epoch": 0.1764769647696477,
      "grad_norm": 22.208112716674805,
      "learning_rate": 1e-05,
      "loss": 6.71,
      "step": 3256
    },
    {
      "epoch": 0.1764769647696477,
      "step": 3256,
      "training_loss": 6.806300163269043
    },
    {
      "epoch": 0.1765311653116531,
      "step": 3257,
      "training_loss": 5.946181297302246
    },
    {
      "epoch": 0.17658536585365853,
      "step": 3258,
      "training_loss": 8.124853134155273
    },
    {
      "epoch": 0.17663956639566394,
      "step": 3259,
      "training_loss": 6.726989269256592
    },
    {
      "epoch": 0.1766937669376694,
      "grad_norm": 17.16012954711914,
      "learning_rate": 1e-05,
      "loss": 6.9011,
      "step": 3260
    },
    {
      "epoch": 0.1766937669376694,
      "step": 3260,
      "training_loss": 6.624676704406738
    },
    {
      "epoch": 0.1767479674796748,
      "step": 3261,
      "training_loss": 6.152055263519287
    },
    {
      "epoch": 0.17680216802168022,
      "step": 3262,
      "training_loss": 6.597365856170654
    },
    {
      "epoch": 0.17685636856368564,
      "step": 3263,
      "training_loss": 7.128118515014648
    },
    {
      "epoch": 0.17691056910569106,
      "grad_norm": 26.104700088500977,
      "learning_rate": 1e-05,
      "loss": 6.6256,
      "step": 3264
    },
    {
      "epoch": 0.17691056910569106,
      "step": 3264,
      "training_loss": 7.03904914855957
    },
    {
      "epoch": 0.17696476964769647,
      "step": 3265,
      "training_loss": 7.596627712249756
    },
    {
      "epoch": 0.1770189701897019,
      "step": 3266,
      "training_loss": 5.742495059967041
    },
    {
      "epoch": 0.1770731707317073,
      "step": 3267,
      "training_loss": 7.074875831604004
    },
    {
      "epoch": 0.17712737127371272,
      "grad_norm": 19.43735122680664,
      "learning_rate": 1e-05,
      "loss": 6.8633,
      "step": 3268
    },
    {
      "epoch": 0.17712737127371272,
      "step": 3268,
      "training_loss": 7.695891380310059
    },
    {
      "epoch": 0.17718157181571817,
      "step": 3269,
      "training_loss": 6.900552272796631
    },
    {
      "epoch": 0.1772357723577236,
      "step": 3270,
      "training_loss": 5.291436195373535
    },
    {
      "epoch": 0.177289972899729,
      "step": 3271,
      "training_loss": 6.013469696044922
    },
    {
      "epoch": 0.17734417344173442,
      "grad_norm": 16.803646087646484,
      "learning_rate": 1e-05,
      "loss": 6.4753,
      "step": 3272
    },
    {
      "epoch": 0.17734417344173442,
      "step": 3272,
      "training_loss": 7.2746663093566895
    },
    {
      "epoch": 0.17739837398373984,
      "step": 3273,
      "training_loss": 6.102732181549072
    },
    {
      "epoch": 0.17745257452574525,
      "step": 3274,
      "training_loss": 7.757063865661621
    },
    {
      "epoch": 0.17750677506775067,
      "step": 3275,
      "training_loss": 6.618338108062744
    },
    {
      "epoch": 0.1775609756097561,
      "grad_norm": 19.317054748535156,
      "learning_rate": 1e-05,
      "loss": 6.9382,
      "step": 3276
    },
    {
      "epoch": 0.1775609756097561,
      "step": 3276,
      "training_loss": 7.068641662597656
    },
    {
      "epoch": 0.1776151761517615,
      "step": 3277,
      "training_loss": 6.432555675506592
    },
    {
      "epoch": 0.17766937669376695,
      "step": 3278,
      "training_loss": 4.457352161407471
    },
    {
      "epoch": 0.17772357723577237,
      "step": 3279,
      "training_loss": 6.704146862030029
    },
    {
      "epoch": 0.17777777777777778,
      "grad_norm": 23.30915069580078,
      "learning_rate": 1e-05,
      "loss": 6.1657,
      "step": 3280
    },
    {
      "epoch": 0.17777777777777778,
      "step": 3280,
      "training_loss": 6.910881519317627
    },
    {
      "epoch": 0.1778319783197832,
      "step": 3281,
      "training_loss": 7.2862935066223145
    },
    {
      "epoch": 0.17788617886178862,
      "step": 3282,
      "training_loss": 7.420891284942627
    },
    {
      "epoch": 0.17794037940379404,
      "step": 3283,
      "training_loss": 6.523392200469971
    },
    {
      "epoch": 0.17799457994579945,
      "grad_norm": 28.09433937072754,
      "learning_rate": 1e-05,
      "loss": 7.0354,
      "step": 3284
    },
    {
      "epoch": 0.17799457994579945,
      "step": 3284,
      "training_loss": 8.242647171020508
    },
    {
      "epoch": 0.17804878048780487,
      "step": 3285,
      "training_loss": 6.408278465270996
    },
    {
      "epoch": 0.1781029810298103,
      "step": 3286,
      "training_loss": 6.981882095336914
    },
    {
      "epoch": 0.17815718157181573,
      "step": 3287,
      "training_loss": 7.152814865112305
    },
    {
      "epoch": 0.17821138211382115,
      "grad_norm": 21.75108528137207,
      "learning_rate": 1e-05,
      "loss": 7.1964,
      "step": 3288
    },
    {
      "epoch": 0.17821138211382115,
      "step": 3288,
      "training_loss": 6.5876264572143555
    },
    {
      "epoch": 0.17826558265582657,
      "step": 3289,
      "training_loss": 7.191148281097412
    },
    {
      "epoch": 0.17831978319783198,
      "step": 3290,
      "training_loss": 5.793615341186523
    },
    {
      "epoch": 0.1783739837398374,
      "step": 3291,
      "training_loss": 7.491645812988281
    },
    {
      "epoch": 0.17842818428184282,
      "grad_norm": 15.205506324768066,
      "learning_rate": 1e-05,
      "loss": 6.766,
      "step": 3292
    },
    {
      "epoch": 0.17842818428184282,
      "step": 3292,
      "training_loss": 7.271615505218506
    },
    {
      "epoch": 0.17848238482384823,
      "step": 3293,
      "training_loss": 8.042524337768555
    },
    {
      "epoch": 0.17853658536585365,
      "step": 3294,
      "training_loss": 5.623101711273193
    },
    {
      "epoch": 0.17859078590785907,
      "step": 3295,
      "training_loss": 3.649507522583008
    },
    {
      "epoch": 0.1786449864498645,
      "grad_norm": 22.416088104248047,
      "learning_rate": 1e-05,
      "loss": 6.1467,
      "step": 3296
    },
    {
      "epoch": 0.1786449864498645,
      "step": 3296,
      "training_loss": 6.499926567077637
    },
    {
      "epoch": 0.17869918699186993,
      "step": 3297,
      "training_loss": 7.30272912979126
    },
    {
      "epoch": 0.17875338753387535,
      "step": 3298,
      "training_loss": 7.140902519226074
    },
    {
      "epoch": 0.17880758807588076,
      "step": 3299,
      "training_loss": 5.435976505279541
    },
    {
      "epoch": 0.17886178861788618,
      "grad_norm": 15.576151847839355,
      "learning_rate": 1e-05,
      "loss": 6.5949,
      "step": 3300
    },
    {
      "epoch": 0.17886178861788618,
      "step": 3300,
      "training_loss": 7.834137439727783
    },
    {
      "epoch": 0.1789159891598916,
      "step": 3301,
      "training_loss": 6.638140678405762
    },
    {
      "epoch": 0.17897018970189701,
      "step": 3302,
      "training_loss": 6.912834644317627
    },
    {
      "epoch": 0.17902439024390243,
      "step": 3303,
      "training_loss": 6.344339370727539
    },
    {
      "epoch": 0.17907859078590785,
      "grad_norm": 18.460460662841797,
      "learning_rate": 1e-05,
      "loss": 6.9324,
      "step": 3304
    },
    {
      "epoch": 0.17907859078590785,
      "step": 3304,
      "training_loss": 7.457200050354004
    },
    {
      "epoch": 0.17913279132791327,
      "step": 3305,
      "training_loss": 6.275417327880859
    },
    {
      "epoch": 0.1791869918699187,
      "step": 3306,
      "training_loss": 7.31915807723999
    },
    {
      "epoch": 0.17924119241192413,
      "step": 3307,
      "training_loss": 7.382887840270996
    },
    {
      "epoch": 0.17929539295392954,
      "grad_norm": 31.38921546936035,
      "learning_rate": 1e-05,
      "loss": 7.1087,
      "step": 3308
    },
    {
      "epoch": 0.17929539295392954,
      "step": 3308,
      "training_loss": 7.389094829559326
    },
    {
      "epoch": 0.17934959349593496,
      "step": 3309,
      "training_loss": 7.320819854736328
    },
    {
      "epoch": 0.17940379403794038,
      "step": 3310,
      "training_loss": 7.33685302734375
    },
    {
      "epoch": 0.1794579945799458,
      "step": 3311,
      "training_loss": 5.047056674957275
    },
    {
      "epoch": 0.1795121951219512,
      "grad_norm": 26.34992027282715,
      "learning_rate": 1e-05,
      "loss": 6.7735,
      "step": 3312
    },
    {
      "epoch": 0.1795121951219512,
      "step": 3312,
      "training_loss": 6.802356719970703
    },
    {
      "epoch": 0.17956639566395663,
      "step": 3313,
      "training_loss": 7.65686559677124
    },
    {
      "epoch": 0.17962059620596205,
      "step": 3314,
      "training_loss": 7.394371509552002
    },
    {
      "epoch": 0.1796747967479675,
      "step": 3315,
      "training_loss": 7.776859760284424
    },
    {
      "epoch": 0.1797289972899729,
      "grad_norm": 32.12604904174805,
      "learning_rate": 1e-05,
      "loss": 7.4076,
      "step": 3316
    },
    {
      "epoch": 0.1797289972899729,
      "step": 3316,
      "training_loss": 7.0990495681762695
    },
    {
      "epoch": 0.17978319783197833,
      "step": 3317,
      "training_loss": 6.183715343475342
    },
    {
      "epoch": 0.17983739837398374,
      "step": 3318,
      "training_loss": 7.7558135986328125
    },
    {
      "epoch": 0.17989159891598916,
      "step": 3319,
      "training_loss": 6.838190078735352
    },
    {
      "epoch": 0.17994579945799458,
      "grad_norm": 26.998388290405273,
      "learning_rate": 1e-05,
      "loss": 6.9692,
      "step": 3320
    },
    {
      "epoch": 0.17994579945799458,
      "step": 3320,
      "training_loss": 7.519218921661377
    },
    {
      "epoch": 0.18,
      "step": 3321,
      "training_loss": 7.506350517272949
    },
    {
      "epoch": 0.1800542005420054,
      "step": 3322,
      "training_loss": 7.00709867477417
    },
    {
      "epoch": 0.18010840108401083,
      "step": 3323,
      "training_loss": 6.29004430770874
    },
    {
      "epoch": 0.18016260162601627,
      "grad_norm": 23.616519927978516,
      "learning_rate": 1e-05,
      "loss": 7.0807,
      "step": 3324
    },
    {
      "epoch": 0.18016260162601627,
      "step": 3324,
      "training_loss": 7.54437255859375
    },
    {
      "epoch": 0.1802168021680217,
      "step": 3325,
      "training_loss": 6.813650608062744
    },
    {
      "epoch": 0.1802710027100271,
      "step": 3326,
      "training_loss": 5.768085956573486
    },
    {
      "epoch": 0.18032520325203252,
      "step": 3327,
      "training_loss": 4.22550630569458
    },
    {
      "epoch": 0.18037940379403794,
      "grad_norm": 24.43640899658203,
      "learning_rate": 1e-05,
      "loss": 6.0879,
      "step": 3328
    },
    {
      "epoch": 0.18037940379403794,
      "step": 3328,
      "training_loss": 6.517227649688721
    },
    {
      "epoch": 0.18043360433604336,
      "step": 3329,
      "training_loss": 6.517622470855713
    },
    {
      "epoch": 0.18048780487804877,
      "step": 3330,
      "training_loss": 7.013121128082275
    },
    {
      "epoch": 0.1805420054200542,
      "step": 3331,
      "training_loss": 6.851014614105225
    },
    {
      "epoch": 0.1805962059620596,
      "grad_norm": 27.333852767944336,
      "learning_rate": 1e-05,
      "loss": 6.7247,
      "step": 3332
    },
    {
      "epoch": 0.1805962059620596,
      "step": 3332,
      "training_loss": 5.669787406921387
    },
    {
      "epoch": 0.18065040650406505,
      "step": 3333,
      "training_loss": 7.989651203155518
    },
    {
      "epoch": 0.18070460704607047,
      "step": 3334,
      "training_loss": 7.478070259094238
    },
    {
      "epoch": 0.1807588075880759,
      "step": 3335,
      "training_loss": 5.707287788391113
    },
    {
      "epoch": 0.1808130081300813,
      "grad_norm": 16.548011779785156,
      "learning_rate": 1e-05,
      "loss": 6.7112,
      "step": 3336
    },
    {
      "epoch": 0.1808130081300813,
      "step": 3336,
      "training_loss": 7.21728515625
    },
    {
      "epoch": 0.18086720867208672,
      "step": 3337,
      "training_loss": 4.729776382446289
    },
    {
      "epoch": 0.18092140921409214,
      "step": 3338,
      "training_loss": 7.253871917724609
    },
    {
      "epoch": 0.18097560975609756,
      "step": 3339,
      "training_loss": 7.343036651611328
    },
    {
      "epoch": 0.18102981029810297,
      "grad_norm": 20.118356704711914,
      "learning_rate": 1e-05,
      "loss": 6.636,
      "step": 3340
    },
    {
      "epoch": 0.18102981029810297,
      "step": 3340,
      "training_loss": 6.8309221267700195
    },
    {
      "epoch": 0.1810840108401084,
      "step": 3341,
      "training_loss": 5.049726486206055
    },
    {
      "epoch": 0.18113821138211383,
      "step": 3342,
      "training_loss": 7.507279396057129
    },
    {
      "epoch": 0.18119241192411925,
      "step": 3343,
      "training_loss": 6.627949237823486
    },
    {
      "epoch": 0.18124661246612467,
      "grad_norm": 35.261566162109375,
      "learning_rate": 1e-05,
      "loss": 6.504,
      "step": 3344
    },
    {
      "epoch": 0.18124661246612467,
      "step": 3344,
      "training_loss": 6.978277683258057
    },
    {
      "epoch": 0.18130081300813009,
      "step": 3345,
      "training_loss": 7.691988468170166
    },
    {
      "epoch": 0.1813550135501355,
      "step": 3346,
      "training_loss": 6.0778045654296875
    },
    {
      "epoch": 0.18140921409214092,
      "step": 3347,
      "training_loss": 6.906822681427002
    },
    {
      "epoch": 0.18146341463414634,
      "grad_norm": 20.806055068969727,
      "learning_rate": 1e-05,
      "loss": 6.9137,
      "step": 3348
    },
    {
      "epoch": 0.18146341463414634,
      "step": 3348,
      "training_loss": 7.204168319702148
    },
    {
      "epoch": 0.18151761517615175,
      "step": 3349,
      "training_loss": 7.180367469787598
    },
    {
      "epoch": 0.18157181571815717,
      "step": 3350,
      "training_loss": 6.33204984664917
    },
    {
      "epoch": 0.18162601626016261,
      "step": 3351,
      "training_loss": 8.198505401611328
    },
    {
      "epoch": 0.18168021680216803,
      "grad_norm": 39.35863494873047,
      "learning_rate": 1e-05,
      "loss": 7.2288,
      "step": 3352
    },
    {
      "epoch": 0.18168021680216803,
      "step": 3352,
      "training_loss": 5.649402141571045
    },
    {
      "epoch": 0.18173441734417345,
      "step": 3353,
      "training_loss": 6.981473922729492
    },
    {
      "epoch": 0.18178861788617887,
      "step": 3354,
      "training_loss": 6.505300998687744
    },
    {
      "epoch": 0.18184281842818428,
      "step": 3355,
      "training_loss": 7.557161808013916
    },
    {
      "epoch": 0.1818970189701897,
      "grad_norm": 24.307798385620117,
      "learning_rate": 1e-05,
      "loss": 6.6733,
      "step": 3356
    },
    {
      "epoch": 0.1818970189701897,
      "step": 3356,
      "training_loss": 8.475086212158203
    },
    {
      "epoch": 0.18195121951219512,
      "step": 3357,
      "training_loss": 7.619329929351807
    },
    {
      "epoch": 0.18200542005420053,
      "step": 3358,
      "training_loss": 6.289558410644531
    },
    {
      "epoch": 0.18205962059620595,
      "step": 3359,
      "training_loss": 7.4788618087768555
    },
    {
      "epoch": 0.1821138211382114,
      "grad_norm": 48.06265640258789,
      "learning_rate": 1e-05,
      "loss": 7.4657,
      "step": 3360
    },
    {
      "epoch": 0.1821138211382114,
      "step": 3360,
      "training_loss": 5.435580730438232
    },
    {
      "epoch": 0.1821680216802168,
      "step": 3361,
      "training_loss": 5.644412517547607
    },
    {
      "epoch": 0.18222222222222223,
      "step": 3362,
      "training_loss": 7.408228874206543
    },
    {
      "epoch": 0.18227642276422765,
      "step": 3363,
      "training_loss": 7.279889106750488
    },
    {
      "epoch": 0.18233062330623306,
      "grad_norm": 17.207822799682617,
      "learning_rate": 1e-05,
      "loss": 6.442,
      "step": 3364
    },
    {
      "epoch": 0.18233062330623306,
      "step": 3364,
      "training_loss": 6.933803558349609
    },
    {
      "epoch": 0.18238482384823848,
      "step": 3365,
      "training_loss": 4.771103382110596
    },
    {
      "epoch": 0.1824390243902439,
      "step": 3366,
      "training_loss": 7.1827473640441895
    },
    {
      "epoch": 0.18249322493224931,
      "step": 3367,
      "training_loss": 7.059759140014648
    },
    {
      "epoch": 0.18254742547425473,
      "grad_norm": 16.411314010620117,
      "learning_rate": 1e-05,
      "loss": 6.4869,
      "step": 3368
    },
    {
      "epoch": 0.18254742547425473,
      "step": 3368,
      "training_loss": 7.067473888397217
    },
    {
      "epoch": 0.18260162601626015,
      "step": 3369,
      "training_loss": 7.472407817840576
    },
    {
      "epoch": 0.1826558265582656,
      "step": 3370,
      "training_loss": 7.13031005859375
    },
    {
      "epoch": 0.182710027100271,
      "step": 3371,
      "training_loss": 7.098349094390869
    },
    {
      "epoch": 0.18276422764227643,
      "grad_norm": 31.09568214416504,
      "learning_rate": 1e-05,
      "loss": 7.1921,
      "step": 3372
    },
    {
      "epoch": 0.18276422764227643,
      "step": 3372,
      "training_loss": 6.898789882659912
    },
    {
      "epoch": 0.18281842818428184,
      "step": 3373,
      "training_loss": 7.289907455444336
    },
    {
      "epoch": 0.18287262872628726,
      "step": 3374,
      "training_loss": 7.373929500579834
    },
    {
      "epoch": 0.18292682926829268,
      "step": 3375,
      "training_loss": 7.536787033081055
    },
    {
      "epoch": 0.1829810298102981,
      "grad_norm": 18.382659912109375,
      "learning_rate": 1e-05,
      "loss": 7.2749,
      "step": 3376
    },
    {
      "epoch": 0.1829810298102981,
      "step": 3376,
      "training_loss": 8.545639991760254
    },
    {
      "epoch": 0.1830352303523035,
      "step": 3377,
      "training_loss": 7.308839321136475
    },
    {
      "epoch": 0.18308943089430893,
      "step": 3378,
      "training_loss": 6.600259304046631
    },
    {
      "epoch": 0.18314363143631437,
      "step": 3379,
      "training_loss": 7.262246131896973
    },
    {
      "epoch": 0.1831978319783198,
      "grad_norm": 37.13809585571289,
      "learning_rate": 1e-05,
      "loss": 7.4292,
      "step": 3380
    },
    {
      "epoch": 0.1831978319783198,
      "step": 3380,
      "training_loss": 4.7367048263549805
    },
    {
      "epoch": 0.1832520325203252,
      "step": 3381,
      "training_loss": 7.384112358093262
    },
    {
      "epoch": 0.18330623306233063,
      "step": 3382,
      "training_loss": 6.965579032897949
    },
    {
      "epoch": 0.18336043360433604,
      "step": 3383,
      "training_loss": 6.959291458129883
    },
    {
      "epoch": 0.18341463414634146,
      "grad_norm": 25.015670776367188,
      "learning_rate": 1e-05,
      "loss": 6.5114,
      "step": 3384
    },
    {
      "epoch": 0.18341463414634146,
      "step": 3384,
      "training_loss": 6.952235698699951
    },
    {
      "epoch": 0.18346883468834688,
      "step": 3385,
      "training_loss": 6.194606781005859
    },
    {
      "epoch": 0.1835230352303523,
      "step": 3386,
      "training_loss": 6.711449146270752
    },
    {
      "epoch": 0.1835772357723577,
      "step": 3387,
      "training_loss": 5.779006004333496
    },
    {
      "epoch": 0.18363143631436316,
      "grad_norm": 23.652664184570312,
      "learning_rate": 1e-05,
      "loss": 6.4093,
      "step": 3388
    },
    {
      "epoch": 0.18363143631436316,
      "step": 3388,
      "training_loss": 7.395166873931885
    },
    {
      "epoch": 0.18368563685636857,
      "step": 3389,
      "training_loss": 6.696549892425537
    },
    {
      "epoch": 0.183739837398374,
      "step": 3390,
      "training_loss": 5.127013683319092
    },
    {
      "epoch": 0.1837940379403794,
      "step": 3391,
      "training_loss": 6.125940799713135
    },
    {
      "epoch": 0.18384823848238482,
      "grad_norm": 25.875394821166992,
      "learning_rate": 1e-05,
      "loss": 6.3362,
      "step": 3392
    },
    {
      "epoch": 0.18384823848238482,
      "step": 3392,
      "training_loss": 7.43597412109375
    },
    {
      "epoch": 0.18390243902439024,
      "step": 3393,
      "training_loss": 6.802240371704102
    },
    {
      "epoch": 0.18395663956639566,
      "step": 3394,
      "training_loss": 6.806080341339111
    },
    {
      "epoch": 0.18401084010840107,
      "step": 3395,
      "training_loss": 6.915140151977539
    },
    {
      "epoch": 0.1840650406504065,
      "grad_norm": 20.619792938232422,
      "learning_rate": 1e-05,
      "loss": 6.9899,
      "step": 3396
    },
    {
      "epoch": 0.1840650406504065,
      "step": 3396,
      "training_loss": 6.334334373474121
    },
    {
      "epoch": 0.18411924119241194,
      "step": 3397,
      "training_loss": 7.213754653930664
    },
    {
      "epoch": 0.18417344173441735,
      "step": 3398,
      "training_loss": 6.9066925048828125
    },
    {
      "epoch": 0.18422764227642277,
      "step": 3399,
      "training_loss": 7.491916656494141
    },
    {
      "epoch": 0.1842818428184282,
      "grad_norm": 25.376258850097656,
      "learning_rate": 1e-05,
      "loss": 6.9867,
      "step": 3400
    },
    {
      "epoch": 0.1842818428184282,
      "step": 3400,
      "training_loss": 8.201772689819336
    },
    {
      "epoch": 0.1843360433604336,
      "step": 3401,
      "training_loss": 4.462465286254883
    },
    {
      "epoch": 0.18439024390243902,
      "step": 3402,
      "training_loss": 6.443544387817383
    },
    {
      "epoch": 0.18444444444444444,
      "step": 3403,
      "training_loss": 6.162970066070557
    },
    {
      "epoch": 0.18449864498644986,
      "grad_norm": 16.94151496887207,
      "learning_rate": 1e-05,
      "loss": 6.3177,
      "step": 3404
    },
    {
      "epoch": 0.18449864498644986,
      "step": 3404,
      "training_loss": 5.379667282104492
    },
    {
      "epoch": 0.18455284552845527,
      "step": 3405,
      "training_loss": 7.251896381378174
    },
    {
      "epoch": 0.18460704607046072,
      "step": 3406,
      "training_loss": 6.159588813781738
    },
    {
      "epoch": 0.18466124661246613,
      "step": 3407,
      "training_loss": 7.735924243927002
    },
    {
      "epoch": 0.18471544715447155,
      "grad_norm": 28.975675582885742,
      "learning_rate": 1e-05,
      "loss": 6.6318,
      "step": 3408
    },
    {
      "epoch": 0.18471544715447155,
      "step": 3408,
      "training_loss": 6.941023826599121
    },
    {
      "epoch": 0.18476964769647697,
      "step": 3409,
      "training_loss": 7.57454252243042
    },
    {
      "epoch": 0.18482384823848239,
      "step": 3410,
      "training_loss": 6.454051494598389
    },
    {
      "epoch": 0.1848780487804878,
      "step": 3411,
      "training_loss": 6.925812244415283
    },
    {
      "epoch": 0.18493224932249322,
      "grad_norm": 16.656503677368164,
      "learning_rate": 1e-05,
      "loss": 6.9739,
      "step": 3412
    },
    {
      "epoch": 0.18493224932249322,
      "step": 3412,
      "training_loss": 6.546768665313721
    },
    {
      "epoch": 0.18498644986449864,
      "step": 3413,
      "training_loss": 5.75166130065918
    },
    {
      "epoch": 0.18504065040650405,
      "step": 3414,
      "training_loss": 6.919075965881348
    },
    {
      "epoch": 0.1850948509485095,
      "step": 3415,
      "training_loss": 7.345009803771973
    },
    {
      "epoch": 0.18514905149051492,
      "grad_norm": 17.101131439208984,
      "learning_rate": 1e-05,
      "loss": 6.6406,
      "step": 3416
    },
    {
      "epoch": 0.18514905149051492,
      "step": 3416,
      "training_loss": 6.298130989074707
    },
    {
      "epoch": 0.18520325203252033,
      "step": 3417,
      "training_loss": 6.830080509185791
    },
    {
      "epoch": 0.18525745257452575,
      "step": 3418,
      "training_loss": 9.819458961486816
    },
    {
      "epoch": 0.18531165311653117,
      "step": 3419,
      "training_loss": 6.7071709632873535
    },
    {
      "epoch": 0.18536585365853658,
      "grad_norm": 20.146581649780273,
      "learning_rate": 1e-05,
      "loss": 7.4137,
      "step": 3420
    },
    {
      "epoch": 0.18536585365853658,
      "step": 3420,
      "training_loss": 7.384185791015625
    },
    {
      "epoch": 0.185420054200542,
      "step": 3421,
      "training_loss": 6.3185224533081055
    },
    {
      "epoch": 0.18547425474254742,
      "step": 3422,
      "training_loss": 5.7343010902404785
    },
    {
      "epoch": 0.18552845528455283,
      "step": 3423,
      "training_loss": 7.767590522766113
    },
    {
      "epoch": 0.18558265582655828,
      "grad_norm": 20.473827362060547,
      "learning_rate": 1e-05,
      "loss": 6.8012,
      "step": 3424
    },
    {
      "epoch": 0.18558265582655828,
      "step": 3424,
      "training_loss": 7.124728202819824
    },
    {
      "epoch": 0.1856368563685637,
      "step": 3425,
      "training_loss": 6.565446376800537
    },
    {
      "epoch": 0.1856910569105691,
      "step": 3426,
      "training_loss": 6.312071323394775
    },
    {
      "epoch": 0.18574525745257453,
      "step": 3427,
      "training_loss": 6.863083362579346
    },
    {
      "epoch": 0.18579945799457995,
      "grad_norm": 19.267183303833008,
      "learning_rate": 1e-05,
      "loss": 6.7163,
      "step": 3428
    },
    {
      "epoch": 0.18579945799457995,
      "step": 3428,
      "training_loss": 6.901342868804932
    },
    {
      "epoch": 0.18585365853658536,
      "step": 3429,
      "training_loss": 6.820026874542236
    },
    {
      "epoch": 0.18590785907859078,
      "step": 3430,
      "training_loss": 6.04063606262207
    },
    {
      "epoch": 0.1859620596205962,
      "step": 3431,
      "training_loss": 7.929154872894287
    },
    {
      "epoch": 0.18601626016260162,
      "grad_norm": 22.01738739013672,
      "learning_rate": 1e-05,
      "loss": 6.9228,
      "step": 3432
    },
    {
      "epoch": 0.18601626016260162,
      "step": 3432,
      "training_loss": 5.661021709442139
    },
    {
      "epoch": 0.18607046070460703,
      "step": 3433,
      "training_loss": 5.103138446807861
    },
    {
      "epoch": 0.18612466124661248,
      "step": 3434,
      "training_loss": 7.084292888641357
    },
    {
      "epoch": 0.1861788617886179,
      "step": 3435,
      "training_loss": 5.237099647521973
    },
    {
      "epoch": 0.1862330623306233,
      "grad_norm": 22.17915153503418,
      "learning_rate": 1e-05,
      "loss": 5.7714,
      "step": 3436
    },
    {
      "epoch": 0.1862330623306233,
      "step": 3436,
      "training_loss": 6.630657196044922
    },
    {
      "epoch": 0.18628726287262873,
      "step": 3437,
      "training_loss": 8.178718566894531
    },
    {
      "epoch": 0.18634146341463415,
      "step": 3438,
      "training_loss": 6.116684436798096
    },
    {
      "epoch": 0.18639566395663956,
      "step": 3439,
      "training_loss": 7.480030059814453
    },
    {
      "epoch": 0.18644986449864498,
      "grad_norm": 23.384624481201172,
      "learning_rate": 1e-05,
      "loss": 7.1015,
      "step": 3440
    },
    {
      "epoch": 0.18644986449864498,
      "step": 3440,
      "training_loss": 6.911051273345947
    },
    {
      "epoch": 0.1865040650406504,
      "step": 3441,
      "training_loss": 6.944309711456299
    },
    {
      "epoch": 0.1865582655826558,
      "step": 3442,
      "training_loss": 7.681085586547852
    },
    {
      "epoch": 0.18661246612466126,
      "step": 3443,
      "training_loss": 7.258789539337158
    },
    {
      "epoch": 0.18666666666666668,
      "grad_norm": 24.848281860351562,
      "learning_rate": 1e-05,
      "loss": 7.1988,
      "step": 3444
    },
    {
      "epoch": 0.18666666666666668,
      "step": 3444,
      "training_loss": 8.989505767822266
    },
    {
      "epoch": 0.1867208672086721,
      "step": 3445,
      "training_loss": 7.3001604080200195
    },
    {
      "epoch": 0.1867750677506775,
      "step": 3446,
      "training_loss": 6.821796417236328
    },
    {
      "epoch": 0.18682926829268293,
      "step": 3447,
      "training_loss": 6.781571388244629
    },
    {
      "epoch": 0.18688346883468834,
      "grad_norm": 40.569580078125,
      "learning_rate": 1e-05,
      "loss": 7.4733,
      "step": 3448
    },
    {
      "epoch": 0.18688346883468834,
      "step": 3448,
      "training_loss": 7.203474998474121
    },
    {
      "epoch": 0.18693766937669376,
      "step": 3449,
      "training_loss": 6.693836212158203
    },
    {
      "epoch": 0.18699186991869918,
      "step": 3450,
      "training_loss": 6.9434380531311035
    },
    {
      "epoch": 0.1870460704607046,
      "step": 3451,
      "training_loss": 6.708651065826416
    },
    {
      "epoch": 0.18710027100271004,
      "grad_norm": 51.20587158203125,
      "learning_rate": 1e-05,
      "loss": 6.8874,
      "step": 3452
    },
    {
      "epoch": 0.18710027100271004,
      "step": 3452,
      "training_loss": 7.080171585083008
    },
    {
      "epoch": 0.18715447154471546,
      "step": 3453,
      "training_loss": 7.378125190734863
    },
    {
      "epoch": 0.18720867208672087,
      "step": 3454,
      "training_loss": 5.928691387176514
    },
    {
      "epoch": 0.1872628726287263,
      "step": 3455,
      "training_loss": 6.236708641052246
    },
    {
      "epoch": 0.1873170731707317,
      "grad_norm": 21.638193130493164,
      "learning_rate": 1e-05,
      "loss": 6.6559,
      "step": 3456
    },
    {
      "epoch": 0.1873170731707317,
      "step": 3456,
      "training_loss": 7.058627128601074
    },
    {
      "epoch": 0.18737127371273712,
      "step": 3457,
      "training_loss": 6.924045562744141
    },
    {
      "epoch": 0.18742547425474254,
      "step": 3458,
      "training_loss": 6.858800411224365
    },
    {
      "epoch": 0.18747967479674796,
      "step": 3459,
      "training_loss": 6.867778778076172
    },
    {
      "epoch": 0.18753387533875338,
      "grad_norm": 24.784788131713867,
      "learning_rate": 1e-05,
      "loss": 6.9273,
      "step": 3460
    },
    {
      "epoch": 0.18753387533875338,
      "step": 3460,
      "training_loss": 6.287567138671875
    },
    {
      "epoch": 0.18758807588075882,
      "step": 3461,
      "training_loss": 6.86269474029541
    },
    {
      "epoch": 0.18764227642276424,
      "step": 3462,
      "training_loss": 6.133185863494873
    },
    {
      "epoch": 0.18769647696476965,
      "step": 3463,
      "training_loss": 6.548514366149902
    },
    {
      "epoch": 0.18775067750677507,
      "grad_norm": 20.304061889648438,
      "learning_rate": 1e-05,
      "loss": 6.458,
      "step": 3464
    },
    {
      "epoch": 0.18775067750677507,
      "step": 3464,
      "training_loss": 7.936813831329346
    },
    {
      "epoch": 0.1878048780487805,
      "step": 3465,
      "training_loss": 7.206125259399414
    },
    {
      "epoch": 0.1878590785907859,
      "step": 3466,
      "training_loss": 8.0089693069458
    },
    {
      "epoch": 0.18791327913279132,
      "step": 3467,
      "training_loss": 6.7452874183654785
    },
    {
      "epoch": 0.18796747967479674,
      "grad_norm": 19.755260467529297,
      "learning_rate": 1e-05,
      "loss": 7.4743,
      "step": 3468
    },
    {
      "epoch": 0.18796747967479674,
      "step": 3468,
      "training_loss": 5.797020435333252
    },
    {
      "epoch": 0.18802168021680216,
      "step": 3469,
      "training_loss": 7.387396812438965
    },
    {
      "epoch": 0.1880758807588076,
      "step": 3470,
      "training_loss": 6.846914768218994
    },
    {
      "epoch": 0.18813008130081302,
      "step": 3471,
      "training_loss": 7.254397392272949
    },
    {
      "epoch": 0.18818428184281843,
      "grad_norm": 25.550495147705078,
      "learning_rate": 1e-05,
      "loss": 6.8214,
      "step": 3472
    },
    {
      "epoch": 0.18818428184281843,
      "step": 3472,
      "training_loss": 7.41315221786499
    },
    {
      "epoch": 0.18823848238482385,
      "step": 3473,
      "training_loss": 8.467247009277344
    },
    {
      "epoch": 0.18829268292682927,
      "step": 3474,
      "training_loss": 7.787130832672119
    },
    {
      "epoch": 0.18834688346883469,
      "step": 3475,
      "training_loss": 6.269234657287598
    },
    {
      "epoch": 0.1884010840108401,
      "grad_norm": 17.83312225341797,
      "learning_rate": 1e-05,
      "loss": 7.4842,
      "step": 3476
    },
    {
      "epoch": 0.1884010840108401,
      "step": 3476,
      "training_loss": 6.103306293487549
    },
    {
      "epoch": 0.18845528455284552,
      "step": 3477,
      "training_loss": 7.817312717437744
    },
    {
      "epoch": 0.18850948509485094,
      "step": 3478,
      "training_loss": 6.307263374328613
    },
    {
      "epoch": 0.18856368563685638,
      "step": 3479,
      "training_loss": 7.446882247924805
    },
    {
      "epoch": 0.1886178861788618,
      "grad_norm": 21.80047035217285,
      "learning_rate": 1e-05,
      "loss": 6.9187,
      "step": 3480
    },
    {
      "epoch": 0.1886178861788618,
      "step": 3480,
      "training_loss": 6.416016101837158
    },
    {
      "epoch": 0.18867208672086722,
      "step": 3481,
      "training_loss": 7.96049165725708
    },
    {
      "epoch": 0.18872628726287263,
      "step": 3482,
      "training_loss": 7.121279239654541
    },
    {
      "epoch": 0.18878048780487805,
      "step": 3483,
      "training_loss": 7.533348560333252
    },
    {
      "epoch": 0.18883468834688347,
      "grad_norm": 20.86664581298828,
      "learning_rate": 1e-05,
      "loss": 7.2578,
      "step": 3484
    },
    {
      "epoch": 0.18883468834688347,
      "step": 3484,
      "training_loss": 6.258667945861816
    },
    {
      "epoch": 0.18888888888888888,
      "step": 3485,
      "training_loss": 6.708319187164307
    },
    {
      "epoch": 0.1889430894308943,
      "step": 3486,
      "training_loss": 5.572155952453613
    },
    {
      "epoch": 0.18899728997289972,
      "step": 3487,
      "training_loss": 6.677277088165283
    },
    {
      "epoch": 0.18905149051490516,
      "grad_norm": 46.096649169921875,
      "learning_rate": 1e-05,
      "loss": 6.3041,
      "step": 3488
    },
    {
      "epoch": 0.18905149051490516,
      "step": 3488,
      "training_loss": 5.042290210723877
    },
    {
      "epoch": 0.18910569105691058,
      "step": 3489,
      "training_loss": 6.765042304992676
    },
    {
      "epoch": 0.189159891598916,
      "step": 3490,
      "training_loss": 6.946923732757568
    },
    {
      "epoch": 0.1892140921409214,
      "step": 3491,
      "training_loss": 7.128706932067871
    },
    {
      "epoch": 0.18926829268292683,
      "grad_norm": 19.224590301513672,
      "learning_rate": 1e-05,
      "loss": 6.4707,
      "step": 3492
    },
    {
      "epoch": 0.18926829268292683,
      "step": 3492,
      "training_loss": 6.520163536071777
    },
    {
      "epoch": 0.18932249322493225,
      "step": 3493,
      "training_loss": 5.814317226409912
    },
    {
      "epoch": 0.18937669376693766,
      "step": 3494,
      "training_loss": 4.713820934295654
    },
    {
      "epoch": 0.18943089430894308,
      "step": 3495,
      "training_loss": 6.914499282836914
    },
    {
      "epoch": 0.1894850948509485,
      "grad_norm": 25.591083526611328,
      "learning_rate": 1e-05,
      "loss": 5.9907,
      "step": 3496
    },
    {
      "epoch": 0.1894850948509485,
      "step": 3496,
      "training_loss": 5.882189750671387
    },
    {
      "epoch": 0.18953929539295392,
      "step": 3497,
      "training_loss": 6.789541244506836
    },
    {
      "epoch": 0.18959349593495936,
      "step": 3498,
      "training_loss": 8.489331245422363
    },
    {
      "epoch": 0.18964769647696478,
      "step": 3499,
      "training_loss": 8.511167526245117
    },
    {
      "epoch": 0.1897018970189702,
      "grad_norm": 62.952980041503906,
      "learning_rate": 1e-05,
      "loss": 7.4181,
      "step": 3500
    },
    {
      "epoch": 0.1897018970189702,
      "step": 3500,
      "training_loss": 7.399101734161377
    },
    {
      "epoch": 0.1897560975609756,
      "step": 3501,
      "training_loss": 7.1829514503479
    },
    {
      "epoch": 0.18981029810298103,
      "step": 3502,
      "training_loss": 7.134843349456787
    },
    {
      "epoch": 0.18986449864498645,
      "step": 3503,
      "training_loss": 6.642043113708496
    },
    {
      "epoch": 0.18991869918699186,
      "grad_norm": 23.504793167114258,
      "learning_rate": 1e-05,
      "loss": 7.0897,
      "step": 3504
    },
    {
      "epoch": 0.18991869918699186,
      "step": 3504,
      "training_loss": 6.446897983551025
    },
    {
      "epoch": 0.18997289972899728,
      "step": 3505,
      "training_loss": 7.196646213531494
    },
    {
      "epoch": 0.1900271002710027,
      "step": 3506,
      "training_loss": 7.2335357666015625
    },
    {
      "epoch": 0.19008130081300814,
      "step": 3507,
      "training_loss": 3.8451244831085205
    },
    {
      "epoch": 0.19013550135501356,
      "grad_norm": 31.52877426147461,
      "learning_rate": 1e-05,
      "loss": 6.1806,
      "step": 3508
    },
    {
      "epoch": 0.19013550135501356,
      "step": 3508,
      "training_loss": 8.038898468017578
    },
    {
      "epoch": 0.19018970189701898,
      "step": 3509,
      "training_loss": 8.071480751037598
    },
    {
      "epoch": 0.1902439024390244,
      "step": 3510,
      "training_loss": 7.032698154449463
    },
    {
      "epoch": 0.1902981029810298,
      "step": 3511,
      "training_loss": 6.798803806304932
    },
    {
      "epoch": 0.19035230352303523,
      "grad_norm": 19.408241271972656,
      "learning_rate": 1e-05,
      "loss": 7.4855,
      "step": 3512
    },
    {
      "epoch": 0.19035230352303523,
      "step": 3512,
      "training_loss": 8.390660285949707
    },
    {
      "epoch": 0.19040650406504064,
      "step": 3513,
      "training_loss": 5.963978290557861
    },
    {
      "epoch": 0.19046070460704606,
      "step": 3514,
      "training_loss": 8.271682739257812
    },
    {
      "epoch": 0.19051490514905148,
      "step": 3515,
      "training_loss": 6.1907453536987305
    },
    {
      "epoch": 0.19056910569105692,
      "grad_norm": 29.050853729248047,
      "learning_rate": 1e-05,
      "loss": 7.2043,
      "step": 3516
    },
    {
      "epoch": 0.19056910569105692,
      "step": 3516,
      "training_loss": 6.836751461029053
    },
    {
      "epoch": 0.19062330623306234,
      "step": 3517,
      "training_loss": 6.775646209716797
    },
    {
      "epoch": 0.19067750677506776,
      "step": 3518,
      "training_loss": 6.142198085784912
    },
    {
      "epoch": 0.19073170731707317,
      "step": 3519,
      "training_loss": 7.033194065093994
    },
    {
      "epoch": 0.1907859078590786,
      "grad_norm": 19.76761817932129,
      "learning_rate": 1e-05,
      "loss": 6.6969,
      "step": 3520
    },
    {
      "epoch": 0.1907859078590786,
      "step": 3520,
      "training_loss": 7.350064754486084
    },
    {
      "epoch": 0.190840108401084,
      "step": 3521,
      "training_loss": 7.468199253082275
    },
    {
      "epoch": 0.19089430894308942,
      "step": 3522,
      "training_loss": 6.538405895233154
    },
    {
      "epoch": 0.19094850948509484,
      "step": 3523,
      "training_loss": 6.253392219543457
    },
    {
      "epoch": 0.19100271002710026,
      "grad_norm": 27.41691017150879,
      "learning_rate": 1e-05,
      "loss": 6.9025,
      "step": 3524
    },
    {
      "epoch": 0.19100271002710026,
      "step": 3524,
      "training_loss": 7.649094104766846
    },
    {
      "epoch": 0.1910569105691057,
      "step": 3525,
      "training_loss": 6.199626445770264
    },
    {
      "epoch": 0.19111111111111112,
      "step": 3526,
      "training_loss": 6.262503623962402
    },
    {
      "epoch": 0.19116531165311654,
      "step": 3527,
      "training_loss": 7.09645938873291
    },
    {
      "epoch": 0.19121951219512195,
      "grad_norm": 19.283796310424805,
      "learning_rate": 1e-05,
      "loss": 6.8019,
      "step": 3528
    },
    {
      "epoch": 0.19121951219512195,
      "step": 3528,
      "training_loss": 6.632265567779541
    },
    {
      "epoch": 0.19127371273712737,
      "step": 3529,
      "training_loss": 7.377540111541748
    },
    {
      "epoch": 0.1913279132791328,
      "step": 3530,
      "training_loss": 7.22407341003418
    },
    {
      "epoch": 0.1913821138211382,
      "step": 3531,
      "training_loss": 6.923447132110596
    },
    {
      "epoch": 0.19143631436314362,
      "grad_norm": 21.516965866088867,
      "learning_rate": 1e-05,
      "loss": 7.0393,
      "step": 3532
    },
    {
      "epoch": 0.19143631436314362,
      "step": 3532,
      "training_loss": 7.718166828155518
    },
    {
      "epoch": 0.19149051490514904,
      "step": 3533,
      "training_loss": 7.17830228805542
    },
    {
      "epoch": 0.19154471544715448,
      "step": 3534,
      "training_loss": 9.594426155090332
    },
    {
      "epoch": 0.1915989159891599,
      "step": 3535,
      "training_loss": 6.626454830169678
    },
    {
      "epoch": 0.19165311653116532,
      "grad_norm": 21.104188919067383,
      "learning_rate": 1e-05,
      "loss": 7.7793,
      "step": 3536
    },
    {
      "epoch": 0.19165311653116532,
      "step": 3536,
      "training_loss": 4.722585678100586
    },
    {
      "epoch": 0.19170731707317074,
      "step": 3537,
      "training_loss": 6.965334415435791
    },
    {
      "epoch": 0.19176151761517615,
      "step": 3538,
      "training_loss": 7.1152753829956055
    },
    {
      "epoch": 0.19181571815718157,
      "step": 3539,
      "training_loss": 6.453128814697266
    },
    {
      "epoch": 0.191869918699187,
      "grad_norm": 29.41900634765625,
      "learning_rate": 1e-05,
      "loss": 6.3141,
      "step": 3540
    },
    {
      "epoch": 0.191869918699187,
      "step": 3540,
      "training_loss": 7.118203639984131
    },
    {
      "epoch": 0.1919241192411924,
      "step": 3541,
      "training_loss": 5.378966808319092
    },
    {
      "epoch": 0.19197831978319782,
      "step": 3542,
      "training_loss": 7.774205207824707
    },
    {
      "epoch": 0.19203252032520327,
      "step": 3543,
      "training_loss": 7.703166961669922
    },
    {
      "epoch": 0.19208672086720868,
      "grad_norm": 16.592958450317383,
      "learning_rate": 1e-05,
      "loss": 6.9936,
      "step": 3544
    },
    {
      "epoch": 0.19208672086720868,
      "step": 3544,
      "training_loss": 6.540310382843018
    },
    {
      "epoch": 0.1921409214092141,
      "step": 3545,
      "training_loss": 5.651665210723877
    },
    {
      "epoch": 0.19219512195121952,
      "step": 3546,
      "training_loss": 5.747505187988281
    },
    {
      "epoch": 0.19224932249322493,
      "step": 3547,
      "training_loss": 7.715489387512207
    },
    {
      "epoch": 0.19230352303523035,
      "grad_norm": 23.626712799072266,
      "learning_rate": 1e-05,
      "loss": 6.4137,
      "step": 3548
    },
    {
      "epoch": 0.19230352303523035,
      "step": 3548,
      "training_loss": 7.46294641494751
    },
    {
      "epoch": 0.19235772357723577,
      "step": 3549,
      "training_loss": 6.186453342437744
    },
    {
      "epoch": 0.19241192411924118,
      "step": 3550,
      "training_loss": 8.385303497314453
    },
    {
      "epoch": 0.1924661246612466,
      "step": 3551,
      "training_loss": 7.523306369781494
    },
    {
      "epoch": 0.19252032520325205,
      "grad_norm": 42.851768493652344,
      "learning_rate": 1e-05,
      "loss": 7.3895,
      "step": 3552
    },
    {
      "epoch": 0.19252032520325205,
      "step": 3552,
      "training_loss": 6.682075500488281
    },
    {
      "epoch": 0.19257452574525746,
      "step": 3553,
      "training_loss": 7.262420654296875
    },
    {
      "epoch": 0.19262872628726288,
      "step": 3554,
      "training_loss": 7.950928211212158
    },
    {
      "epoch": 0.1926829268292683,
      "step": 3555,
      "training_loss": 6.828555583953857
    },
    {
      "epoch": 0.19273712737127371,
      "grad_norm": 18.830814361572266,
      "learning_rate": 1e-05,
      "loss": 7.181,
      "step": 3556
    },
    {
      "epoch": 0.19273712737127371,
      "step": 3556,
      "training_loss": 6.7854838371276855
    },
    {
      "epoch": 0.19279132791327913,
      "step": 3557,
      "training_loss": 6.101210594177246
    },
    {
      "epoch": 0.19284552845528455,
      "step": 3558,
      "training_loss": 5.956278324127197
    },
    {
      "epoch": 0.19289972899728997,
      "step": 3559,
      "training_loss": 7.574970245361328
    },
    {
      "epoch": 0.19295392953929538,
      "grad_norm": 33.00794982910156,
      "learning_rate": 1e-05,
      "loss": 6.6045,
      "step": 3560
    },
    {
      "epoch": 0.19295392953929538,
      "step": 3560,
      "training_loss": 6.590302467346191
    },
    {
      "epoch": 0.1930081300813008,
      "step": 3561,
      "training_loss": 4.756896495819092
    },
    {
      "epoch": 0.19306233062330624,
      "step": 3562,
      "training_loss": 7.241307258605957
    },
    {
      "epoch": 0.19311653116531166,
      "step": 3563,
      "training_loss": 7.090335369110107
    },
    {
      "epoch": 0.19317073170731708,
      "grad_norm": 19.086322784423828,
      "learning_rate": 1e-05,
      "loss": 6.4197,
      "step": 3564
    },
    {
      "epoch": 0.19317073170731708,
      "step": 3564,
      "training_loss": 7.436647891998291
    },
    {
      "epoch": 0.1932249322493225,
      "step": 3565,
      "training_loss": 8.379436492919922
    },
    {
      "epoch": 0.1932791327913279,
      "step": 3566,
      "training_loss": 6.523434638977051
    },
    {
      "epoch": 0.19333333333333333,
      "step": 3567,
      "training_loss": 6.130964279174805
    },
    {
      "epoch": 0.19338753387533875,
      "grad_norm": 38.70289611816406,
      "learning_rate": 1e-05,
      "loss": 7.1176,
      "step": 3568
    },
    {
      "epoch": 0.19338753387533875,
      "step": 3568,
      "training_loss": 7.204666614532471
    },
    {
      "epoch": 0.19344173441734416,
      "step": 3569,
      "training_loss": 7.106040000915527
    },
    {
      "epoch": 0.19349593495934958,
      "step": 3570,
      "training_loss": 6.819925308227539
    },
    {
      "epoch": 0.19355013550135503,
      "step": 3571,
      "training_loss": 6.470937728881836
    },
    {
      "epoch": 0.19360433604336044,
      "grad_norm": 19.954730987548828,
      "learning_rate": 1e-05,
      "loss": 6.9004,
      "step": 3572
    },
    {
      "epoch": 0.19360433604336044,
      "step": 3572,
      "training_loss": 6.625735759735107
    },
    {
      "epoch": 0.19365853658536586,
      "step": 3573,
      "training_loss": 7.299422740936279
    },
    {
      "epoch": 0.19371273712737128,
      "step": 3574,
      "training_loss": 7.502279758453369
    },
    {
      "epoch": 0.1937669376693767,
      "step": 3575,
      "training_loss": 7.361159324645996
    },
    {
      "epoch": 0.1938211382113821,
      "grad_norm": 27.219772338867188,
      "learning_rate": 1e-05,
      "loss": 7.1971,
      "step": 3576
    },
    {
      "epoch": 0.1938211382113821,
      "step": 3576,
      "training_loss": 6.324658393859863
    },
    {
      "epoch": 0.19387533875338753,
      "step": 3577,
      "training_loss": 6.833270072937012
    },
    {
      "epoch": 0.19392953929539294,
      "step": 3578,
      "training_loss": 7.275321006774902
    },
    {
      "epoch": 0.19398373983739836,
      "step": 3579,
      "training_loss": 7.868011951446533
    },
    {
      "epoch": 0.1940379403794038,
      "grad_norm": 59.7314453125,
      "learning_rate": 1e-05,
      "loss": 7.0753,
      "step": 3580
    },
    {
      "epoch": 0.1940379403794038,
      "step": 3580,
      "training_loss": 5.897635459899902
    },
    {
      "epoch": 0.19409214092140922,
      "step": 3581,
      "training_loss": 6.910755634307861
    },
    {
      "epoch": 0.19414634146341464,
      "step": 3582,
      "training_loss": 8.202655792236328
    },
    {
      "epoch": 0.19420054200542006,
      "step": 3583,
      "training_loss": 6.196660041809082
    },
    {
      "epoch": 0.19425474254742547,
      "grad_norm": 21.58075714111328,
      "learning_rate": 1e-05,
      "loss": 6.8019,
      "step": 3584
    },
    {
      "epoch": 0.19425474254742547,
      "step": 3584,
      "training_loss": 6.014052867889404
    },
    {
      "epoch": 0.1943089430894309,
      "step": 3585,
      "training_loss": 7.2250823974609375
    },
    {
      "epoch": 0.1943631436314363,
      "step": 3586,
      "training_loss": 5.885572910308838
    },
    {
      "epoch": 0.19441734417344173,
      "step": 3587,
      "training_loss": 7.1241350173950195
    },
    {
      "epoch": 0.19447154471544714,
      "grad_norm": 15.550854682922363,
      "learning_rate": 1e-05,
      "loss": 6.5622,
      "step": 3588
    },
    {
      "epoch": 0.19447154471544714,
      "step": 3588,
      "training_loss": 5.759889125823975
    },
    {
      "epoch": 0.1945257452574526,
      "step": 3589,
      "training_loss": 5.750014305114746
    },
    {
      "epoch": 0.194579945799458,
      "step": 3590,
      "training_loss": 6.830233573913574
    },
    {
      "epoch": 0.19463414634146342,
      "step": 3591,
      "training_loss": 7.1210479736328125
    },
    {
      "epoch": 0.19468834688346884,
      "grad_norm": 41.917236328125,
      "learning_rate": 1e-05,
      "loss": 6.3653,
      "step": 3592
    },
    {
      "epoch": 0.19468834688346884,
      "step": 3592,
      "training_loss": 6.646726608276367
    },
    {
      "epoch": 0.19474254742547426,
      "step": 3593,
      "training_loss": 6.808524131774902
    },
    {
      "epoch": 0.19479674796747967,
      "step": 3594,
      "training_loss": 7.074746131896973
    },
    {
      "epoch": 0.1948509485094851,
      "step": 3595,
      "training_loss": 7.535642623901367
    },
    {
      "epoch": 0.1949051490514905,
      "grad_norm": 15.21619987487793,
      "learning_rate": 1e-05,
      "loss": 7.0164,
      "step": 3596
    },
    {
      "epoch": 0.1949051490514905,
      "step": 3596,
      "training_loss": 7.188093662261963
    },
    {
      "epoch": 0.19495934959349592,
      "step": 3597,
      "training_loss": 6.375606536865234
    },
    {
      "epoch": 0.19501355013550137,
      "step": 3598,
      "training_loss": 6.8431077003479
    },
    {
      "epoch": 0.19506775067750678,
      "step": 3599,
      "training_loss": 6.619203090667725
    },
    {
      "epoch": 0.1951219512195122,
      "grad_norm": 17.445575714111328,
      "learning_rate": 1e-05,
      "loss": 6.7565,
      "step": 3600
    },
    {
      "epoch": 0.1951219512195122,
      "step": 3600,
      "training_loss": 6.647426605224609
    },
    {
      "epoch": 0.19517615176151762,
      "step": 3601,
      "training_loss": 7.542350769042969
    },
    {
      "epoch": 0.19523035230352304,
      "step": 3602,
      "training_loss": 5.918890476226807
    },
    {
      "epoch": 0.19528455284552845,
      "step": 3603,
      "training_loss": 7.953139781951904
    },
    {
      "epoch": 0.19533875338753387,
      "grad_norm": 26.28270149230957,
      "learning_rate": 1e-05,
      "loss": 7.0155,
      "step": 3604
    },
    {
      "epoch": 0.19533875338753387,
      "step": 3604,
      "training_loss": 5.173587799072266
    },
    {
      "epoch": 0.1953929539295393,
      "step": 3605,
      "training_loss": 6.5647196769714355
    },
    {
      "epoch": 0.1954471544715447,
      "step": 3606,
      "training_loss": 5.8073225021362305
    },
    {
      "epoch": 0.19550135501355015,
      "step": 3607,
      "training_loss": 7.742386817932129
    },
    {
      "epoch": 0.19555555555555557,
      "grad_norm": 30.476964950561523,
      "learning_rate": 1e-05,
      "loss": 6.322,
      "step": 3608
    },
    {
      "epoch": 0.19555555555555557,
      "step": 3608,
      "training_loss": 7.219214916229248
    },
    {
      "epoch": 0.19560975609756098,
      "step": 3609,
      "training_loss": 5.867938041687012
    },
    {
      "epoch": 0.1956639566395664,
      "step": 3610,
      "training_loss": 7.985525608062744
    },
    {
      "epoch": 0.19571815718157182,
      "step": 3611,
      "training_loss": 7.248887538909912
    },
    {
      "epoch": 0.19577235772357723,
      "grad_norm": 23.288326263427734,
      "learning_rate": 1e-05,
      "loss": 7.0804,
      "step": 3612
    },
    {
      "epoch": 0.19577235772357723,
      "step": 3612,
      "training_loss": 6.982251167297363
    },
    {
      "epoch": 0.19582655826558265,
      "step": 3613,
      "training_loss": 8.432421684265137
    },
    {
      "epoch": 0.19588075880758807,
      "step": 3614,
      "training_loss": 7.005619525909424
    },
    {
      "epoch": 0.19593495934959348,
      "step": 3615,
      "training_loss": 7.306734561920166
    },
    {
      "epoch": 0.19598915989159893,
      "grad_norm": 17.739580154418945,
      "learning_rate": 1e-05,
      "loss": 7.4318,
      "step": 3616
    },
    {
      "epoch": 0.19598915989159893,
      "step": 3616,
      "training_loss": 7.70357084274292
    },
    {
      "epoch": 0.19604336043360435,
      "step": 3617,
      "training_loss": 7.45521354675293
    },
    {
      "epoch": 0.19609756097560976,
      "step": 3618,
      "training_loss": 8.275249481201172
    },
    {
      "epoch": 0.19615176151761518,
      "step": 3619,
      "training_loss": 6.4157891273498535
    },
    {
      "epoch": 0.1962059620596206,
      "grad_norm": 25.05075454711914,
      "learning_rate": 1e-05,
      "loss": 7.4625,
      "step": 3620
    },
    {
      "epoch": 0.1962059620596206,
      "step": 3620,
      "training_loss": 8.002338409423828
    },
    {
      "epoch": 0.19626016260162601,
      "step": 3621,
      "training_loss": 7.151495456695557
    },
    {
      "epoch": 0.19631436314363143,
      "step": 3622,
      "training_loss": 7.340665340423584
    },
    {
      "epoch": 0.19636856368563685,
      "step": 3623,
      "training_loss": 5.787600994110107
    },
    {
      "epoch": 0.19642276422764227,
      "grad_norm": 21.339664459228516,
      "learning_rate": 1e-05,
      "loss": 7.0705,
      "step": 3624
    },
    {
      "epoch": 0.19642276422764227,
      "step": 3624,
      "training_loss": 5.194981098175049
    },
    {
      "epoch": 0.19647696476964768,
      "step": 3625,
      "training_loss": 7.2146382331848145
    },
    {
      "epoch": 0.19653116531165313,
      "step": 3626,
      "training_loss": 6.672301769256592
    },
    {
      "epoch": 0.19658536585365854,
      "step": 3627,
      "training_loss": 6.368736743927002
    },
    {
      "epoch": 0.19663956639566396,
      "grad_norm": 21.93890380859375,
      "learning_rate": 1e-05,
      "loss": 6.3627,
      "step": 3628
    },
    {
      "epoch": 0.19663956639566396,
      "step": 3628,
      "training_loss": 5.3946757316589355
    },
    {
      "epoch": 0.19669376693766938,
      "step": 3629,
      "training_loss": 6.938905715942383
    },
    {
      "epoch": 0.1967479674796748,
      "step": 3630,
      "training_loss": 6.957594871520996
    },
    {
      "epoch": 0.1968021680216802,
      "step": 3631,
      "training_loss": 4.491446018218994
    },
    {
      "epoch": 0.19685636856368563,
      "grad_norm": 21.67856788635254,
      "learning_rate": 1e-05,
      "loss": 5.9457,
      "step": 3632
    },
    {
      "epoch": 0.19685636856368563,
      "step": 3632,
      "training_loss": 10.10795783996582
    },
    {
      "epoch": 0.19691056910569105,
      "step": 3633,
      "training_loss": 7.300577640533447
    },
    {
      "epoch": 0.19696476964769646,
      "step": 3634,
      "training_loss": 9.514747619628906
    },
    {
      "epoch": 0.1970189701897019,
      "step": 3635,
      "training_loss": 5.756318092346191
    },
    {
      "epoch": 0.19707317073170733,
      "grad_norm": 24.290868759155273,
      "learning_rate": 1e-05,
      "loss": 8.1699,
      "step": 3636
    },
    {
      "epoch": 0.19707317073170733,
      "step": 3636,
      "training_loss": 5.519053936004639
    },
    {
      "epoch": 0.19712737127371274,
      "step": 3637,
      "training_loss": 6.770873546600342
    },
    {
      "epoch": 0.19718157181571816,
      "step": 3638,
      "training_loss": 7.256626605987549
    },
    {
      "epoch": 0.19723577235772358,
      "step": 3639,
      "training_loss": 8.113604545593262
    },
    {
      "epoch": 0.197289972899729,
      "grad_norm": 37.62157440185547,
      "learning_rate": 1e-05,
      "loss": 6.915,
      "step": 3640
    },
    {
      "epoch": 0.197289972899729,
      "step": 3640,
      "training_loss": 4.712960720062256
    },
    {
      "epoch": 0.1973441734417344,
      "step": 3641,
      "training_loss": 7.667952537536621
    },
    {
      "epoch": 0.19739837398373983,
      "step": 3642,
      "training_loss": 7.2596564292907715
    },
    {
      "epoch": 0.19745257452574524,
      "step": 3643,
      "training_loss": 6.276018142700195
    },
    {
      "epoch": 0.1975067750677507,
      "grad_norm": 16.380876541137695,
      "learning_rate": 1e-05,
      "loss": 6.4791,
      "step": 3644
    },
    {
      "epoch": 0.1975067750677507,
      "step": 3644,
      "training_loss": 6.901395320892334
    },
    {
      "epoch": 0.1975609756097561,
      "step": 3645,
      "training_loss": 7.820331573486328
    },
    {
      "epoch": 0.19761517615176152,
      "step": 3646,
      "training_loss": 6.393054008483887
    },
    {
      "epoch": 0.19766937669376694,
      "step": 3647,
      "training_loss": 6.307424068450928
    },
    {
      "epoch": 0.19772357723577236,
      "grad_norm": 28.34696388244629,
      "learning_rate": 1e-05,
      "loss": 6.8556,
      "step": 3648
    },
    {
      "epoch": 0.19772357723577236,
      "step": 3648,
      "training_loss": 7.298562526702881
    },
    {
      "epoch": 0.19777777777777777,
      "step": 3649,
      "training_loss": 7.6671342849731445
    },
    {
      "epoch": 0.1978319783197832,
      "step": 3650,
      "training_loss": 8.117452621459961
    },
    {
      "epoch": 0.1978861788617886,
      "step": 3651,
      "training_loss": 6.682476997375488
    },
    {
      "epoch": 0.19794037940379403,
      "grad_norm": 19.50032615661621,
      "learning_rate": 1e-05,
      "loss": 7.4414,
      "step": 3652
    },
    {
      "epoch": 0.19794037940379403,
      "step": 3652,
      "training_loss": 7.641303062438965
    },
    {
      "epoch": 0.19799457994579947,
      "step": 3653,
      "training_loss": 7.8183393478393555
    },
    {
      "epoch": 0.1980487804878049,
      "step": 3654,
      "training_loss": 7.477652072906494
    },
    {
      "epoch": 0.1981029810298103,
      "step": 3655,
      "training_loss": 5.818542003631592
    },
    {
      "epoch": 0.19815718157181572,
      "grad_norm": 27.886316299438477,
      "learning_rate": 1e-05,
      "loss": 7.189,
      "step": 3656
    },
    {
      "epoch": 0.19815718157181572,
      "step": 3656,
      "training_loss": 7.092739582061768
    },
    {
      "epoch": 0.19821138211382114,
      "step": 3657,
      "training_loss": 8.065757751464844
    },
    {
      "epoch": 0.19826558265582656,
      "step": 3658,
      "training_loss": 8.45667552947998
    },
    {
      "epoch": 0.19831978319783197,
      "step": 3659,
      "training_loss": 6.902286529541016
    },
    {
      "epoch": 0.1983739837398374,
      "grad_norm": 29.202165603637695,
      "learning_rate": 1e-05,
      "loss": 7.6294,
      "step": 3660
    },
    {
      "epoch": 0.1983739837398374,
      "step": 3660,
      "training_loss": 6.8144612312316895
    },
    {
      "epoch": 0.1984281842818428,
      "step": 3661,
      "training_loss": 7.959171772003174
    },
    {
      "epoch": 0.19848238482384825,
      "step": 3662,
      "training_loss": 7.68939208984375
    },
    {
      "epoch": 0.19853658536585367,
      "step": 3663,
      "training_loss": 7.173436641693115
    },
    {
      "epoch": 0.19859078590785909,
      "grad_norm": 19.999155044555664,
      "learning_rate": 1e-05,
      "loss": 7.4091,
      "step": 3664
    },
    {
      "epoch": 0.19859078590785909,
      "step": 3664,
      "training_loss": 7.908653736114502
    },
    {
      "epoch": 0.1986449864498645,
      "step": 3665,
      "training_loss": 6.844855785369873
    },
    {
      "epoch": 0.19869918699186992,
      "step": 3666,
      "training_loss": 7.200395584106445
    },
    {
      "epoch": 0.19875338753387534,
      "step": 3667,
      "training_loss": 7.60434627532959
    },
    {
      "epoch": 0.19880758807588075,
      "grad_norm": 23.82035255432129,
      "learning_rate": 1e-05,
      "loss": 7.3896,
      "step": 3668
    },
    {
      "epoch": 0.19880758807588075,
      "step": 3668,
      "training_loss": 6.539467811584473
    },
    {
      "epoch": 0.19886178861788617,
      "step": 3669,
      "training_loss": 7.424093723297119
    },
    {
      "epoch": 0.1989159891598916,
      "step": 3670,
      "training_loss": 6.190935134887695
    },
    {
      "epoch": 0.19897018970189703,
      "step": 3671,
      "training_loss": 6.709878444671631
    },
    {
      "epoch": 0.19902439024390245,
      "grad_norm": 17.562734603881836,
      "learning_rate": 1e-05,
      "loss": 6.7161,
      "step": 3672
    },
    {
      "epoch": 0.19902439024390245,
      "step": 3672,
      "training_loss": 6.6650800704956055
    },
    {
      "epoch": 0.19907859078590787,
      "step": 3673,
      "training_loss": 7.3142218589782715
    },
    {
      "epoch": 0.19913279132791328,
      "step": 3674,
      "training_loss": 7.112971782684326
    },
    {
      "epoch": 0.1991869918699187,
      "step": 3675,
      "training_loss": 7.9690775871276855
    },
    {
      "epoch": 0.19924119241192412,
      "grad_norm": 20.131502151489258,
      "learning_rate": 1e-05,
      "loss": 7.2653,
      "step": 3676
    },
    {
      "epoch": 0.19924119241192412,
      "step": 3676,
      "training_loss": 7.467028617858887
    },
    {
      "epoch": 0.19929539295392953,
      "step": 3677,
      "training_loss": 7.4162116050720215
    },
    {
      "epoch": 0.19934959349593495,
      "step": 3678,
      "training_loss": 6.486652851104736
    },
    {
      "epoch": 0.19940379403794037,
      "step": 3679,
      "training_loss": 7.118208408355713
    },
    {
      "epoch": 0.1994579945799458,
      "grad_norm": 22.733455657958984,
      "learning_rate": 1e-05,
      "loss": 7.122,
      "step": 3680
    },
    {
      "epoch": 0.1994579945799458,
      "step": 3680,
      "training_loss": 6.63466739654541
    },
    {
      "epoch": 0.19951219512195123,
      "step": 3681,
      "training_loss": 7.488204002380371
    },
    {
      "epoch": 0.19956639566395665,
      "step": 3682,
      "training_loss": 6.862729549407959
    },
    {
      "epoch": 0.19962059620596206,
      "step": 3683,
      "training_loss": 6.670866966247559
    },
    {
      "epoch": 0.19967479674796748,
      "grad_norm": 32.30569076538086,
      "learning_rate": 1e-05,
      "loss": 6.9141,
      "step": 3684
    },
    {
      "epoch": 0.19967479674796748,
      "step": 3684,
      "training_loss": 7.37429141998291
    },
    {
      "epoch": 0.1997289972899729,
      "step": 3685,
      "training_loss": 7.108931541442871
    },
    {
      "epoch": 0.19978319783197832,
      "step": 3686,
      "training_loss": 5.195807456970215
    },
    {
      "epoch": 0.19983739837398373,
      "step": 3687,
      "training_loss": 7.150634288787842
    },
    {
      "epoch": 0.19989159891598915,
      "grad_norm": 17.38572883605957,
      "learning_rate": 1e-05,
      "loss": 6.7074,
      "step": 3688
    },
    {
      "epoch": 0.19989159891598915,
      "step": 3688,
      "training_loss": 6.8793511390686035
    },
    {
      "epoch": 0.19994579945799457,
      "step": 3689,
      "training_loss": 6.780917167663574
    },
    {
      "epoch": 0.2,
      "step": 3690,
      "training_loss": 6.2888054847717285
    },
    {
      "epoch": 0.20005420054200543,
      "step": 3691,
      "training_loss": 5.793900966644287
    },
    {
      "epoch": 0.20010840108401085,
      "grad_norm": 27.417531967163086,
      "learning_rate": 1e-05,
      "loss": 6.4357,
      "step": 3692
    },
    {
      "epoch": 0.20010840108401085,
      "step": 3692,
      "training_loss": 7.059779167175293
    },
    {
      "epoch": 0.20016260162601626,
      "step": 3693,
      "training_loss": 7.7476959228515625
    },
    {
      "epoch": 0.20021680216802168,
      "step": 3694,
      "training_loss": 8.078887939453125
    },
    {
      "epoch": 0.2002710027100271,
      "step": 3695,
      "training_loss": 7.748600006103516
    },
    {
      "epoch": 0.2003252032520325,
      "grad_norm": 24.214841842651367,
      "learning_rate": 1e-05,
      "loss": 7.6587,
      "step": 3696
    },
    {
      "epoch": 0.2003252032520325,
      "step": 3696,
      "training_loss": 5.402561187744141
    },
    {
      "epoch": 0.20037940379403793,
      "step": 3697,
      "training_loss": 7.602196216583252
    },
    {
      "epoch": 0.20043360433604335,
      "step": 3698,
      "training_loss": 6.460982322692871
    },
    {
      "epoch": 0.2004878048780488,
      "step": 3699,
      "training_loss": 6.8257246017456055
    },
    {
      "epoch": 0.2005420054200542,
      "grad_norm": 35.73275375366211,
      "learning_rate": 1e-05,
      "loss": 6.5729,
      "step": 3700
    },
    {
      "epoch": 0.2005420054200542,
      "step": 3700,
      "training_loss": 7.636689186096191
    },
    {
      "epoch": 0.20059620596205963,
      "step": 3701,
      "training_loss": 5.736073970794678
    },
    {
      "epoch": 0.20065040650406504,
      "step": 3702,
      "training_loss": 6.629544258117676
    },
    {
      "epoch": 0.20070460704607046,
      "step": 3703,
      "training_loss": 6.984662055969238
    },
    {
      "epoch": 0.20075880758807588,
      "grad_norm": 26.969566345214844,
      "learning_rate": 1e-05,
      "loss": 6.7467,
      "step": 3704
    },
    {
      "epoch": 0.20075880758807588,
      "step": 3704,
      "training_loss": 7.584930419921875
    },
    {
      "epoch": 0.2008130081300813,
      "step": 3705,
      "training_loss": 6.989312171936035
    },
    {
      "epoch": 0.2008672086720867,
      "step": 3706,
      "training_loss": 8.103487968444824
    },
    {
      "epoch": 0.20092140921409213,
      "step": 3707,
      "training_loss": 5.944775104522705
    },
    {
      "epoch": 0.20097560975609757,
      "grad_norm": 20.99077606201172,
      "learning_rate": 1e-05,
      "loss": 7.1556,
      "step": 3708
    },
    {
      "epoch": 0.20097560975609757,
      "step": 3708,
      "training_loss": 7.127499103546143
    },
    {
      "epoch": 0.201029810298103,
      "step": 3709,
      "training_loss": 7.067489147186279
    },
    {
      "epoch": 0.2010840108401084,
      "step": 3710,
      "training_loss": 8.529799461364746
    },
    {
      "epoch": 0.20113821138211382,
      "step": 3711,
      "training_loss": 5.936741352081299
    },
    {
      "epoch": 0.20119241192411924,
      "grad_norm": 14.440022468566895,
      "learning_rate": 1e-05,
      "loss": 7.1654,
      "step": 3712
    },
    {
      "epoch": 0.20119241192411924,
      "step": 3712,
      "training_loss": 5.79723596572876
    },
    {
      "epoch": 0.20124661246612466,
      "step": 3713,
      "training_loss": 6.195626258850098
    },
    {
      "epoch": 0.20130081300813008,
      "step": 3714,
      "training_loss": 7.336697101593018
    },
    {
      "epoch": 0.2013550135501355,
      "step": 3715,
      "training_loss": 6.103996276855469
    },
    {
      "epoch": 0.2014092140921409,
      "grad_norm": 27.086442947387695,
      "learning_rate": 1e-05,
      "loss": 6.3584,
      "step": 3716
    },
    {
      "epoch": 0.2014092140921409,
      "step": 3716,
      "training_loss": 9.423589706420898
    },
    {
      "epoch": 0.20146341463414635,
      "step": 3717,
      "training_loss": 7.2984466552734375
    },
    {
      "epoch": 0.20151761517615177,
      "step": 3718,
      "training_loss": 7.39729642868042
    },
    {
      "epoch": 0.2015718157181572,
      "step": 3719,
      "training_loss": 6.507932186126709
    },
    {
      "epoch": 0.2016260162601626,
      "grad_norm": 21.38241195678711,
      "learning_rate": 1e-05,
      "loss": 7.6568,
      "step": 3720
    },
    {
      "epoch": 0.2016260162601626,
      "step": 3720,
      "training_loss": 6.14583683013916
    },
    {
      "epoch": 0.20168021680216802,
      "step": 3721,
      "training_loss": 7.773763656616211
    },
    {
      "epoch": 0.20173441734417344,
      "step": 3722,
      "training_loss": 6.346989154815674
    },
    {
      "epoch": 0.20178861788617886,
      "step": 3723,
      "training_loss": 7.759746074676514
    },
    {
      "epoch": 0.20184281842818427,
      "grad_norm": 28.2387638092041,
      "learning_rate": 1e-05,
      "loss": 7.0066,
      "step": 3724
    },
    {
      "epoch": 0.20184281842818427,
      "step": 3724,
      "training_loss": 6.691409111022949
    },
    {
      "epoch": 0.2018970189701897,
      "step": 3725,
      "training_loss": 7.230884552001953
    },
    {
      "epoch": 0.20195121951219513,
      "step": 3726,
      "training_loss": 7.2621283531188965
    },
    {
      "epoch": 0.20200542005420055,
      "step": 3727,
      "training_loss": 6.4293975830078125
    },
    {
      "epoch": 0.20205962059620597,
      "grad_norm": 18.04242706298828,
      "learning_rate": 1e-05,
      "loss": 6.9035,
      "step": 3728
    },
    {
      "epoch": 0.20205962059620597,
      "step": 3728,
      "training_loss": 6.093830585479736
    },
    {
      "epoch": 0.20211382113821139,
      "step": 3729,
      "training_loss": 7.133187294006348
    },
    {
      "epoch": 0.2021680216802168,
      "step": 3730,
      "training_loss": 7.707437515258789
    },
    {
      "epoch": 0.20222222222222222,
      "step": 3731,
      "training_loss": 6.288330078125
    },
    {
      "epoch": 0.20227642276422764,
      "grad_norm": 25.852039337158203,
      "learning_rate": 1e-05,
      "loss": 6.8057,
      "step": 3732
    },
    {
      "epoch": 0.20227642276422764,
      "step": 3732,
      "training_loss": 8.462844848632812
    },
    {
      "epoch": 0.20233062330623305,
      "step": 3733,
      "training_loss": 7.386443138122559
    },
    {
      "epoch": 0.20238482384823847,
      "step": 3734,
      "training_loss": 5.888491630554199
    },
    {
      "epoch": 0.20243902439024392,
      "step": 3735,
      "training_loss": 10.228243827819824
    },
    {
      "epoch": 0.20249322493224933,
      "grad_norm": 62.90739059448242,
      "learning_rate": 1e-05,
      "loss": 7.9915,
      "step": 3736
    },
    {
      "epoch": 0.20249322493224933,
      "step": 3736,
      "training_loss": 6.906584739685059
    },
    {
      "epoch": 0.20254742547425475,
      "step": 3737,
      "training_loss": 7.593590259552002
    },
    {
      "epoch": 0.20260162601626017,
      "step": 3738,
      "training_loss": 6.996849536895752
    },
    {
      "epoch": 0.20265582655826558,
      "step": 3739,
      "training_loss": 7.42687463760376
    },
    {
      "epoch": 0.202710027100271,
      "grad_norm": 32.043479919433594,
      "learning_rate": 1e-05,
      "loss": 7.231,
      "step": 3740
    },
    {
      "epoch": 0.202710027100271,
      "step": 3740,
      "training_loss": 6.641480922698975
    },
    {
      "epoch": 0.20276422764227642,
      "step": 3741,
      "training_loss": 5.6986470222473145
    },
    {
      "epoch": 0.20281842818428183,
      "step": 3742,
      "training_loss": 7.252570629119873
    },
    {
      "epoch": 0.20287262872628725,
      "step": 3743,
      "training_loss": 7.341577053070068
    },
    {
      "epoch": 0.2029268292682927,
      "grad_norm": 18.72827911376953,
      "learning_rate": 1e-05,
      "loss": 6.7336,
      "step": 3744
    },
    {
      "epoch": 0.2029268292682927,
      "step": 3744,
      "training_loss": 7.952272891998291
    },
    {
      "epoch": 0.2029810298102981,
      "step": 3745,
      "training_loss": 6.965378284454346
    },
    {
      "epoch": 0.20303523035230353,
      "step": 3746,
      "training_loss": 4.7070088386535645
    },
    {
      "epoch": 0.20308943089430895,
      "step": 3747,
      "training_loss": 7.248885154724121
    },
    {
      "epoch": 0.20314363143631436,
      "grad_norm": 25.276844024658203,
      "learning_rate": 1e-05,
      "loss": 6.7184,
      "step": 3748
    },
    {
      "epoch": 0.20314363143631436,
      "step": 3748,
      "training_loss": 6.406479835510254
    },
    {
      "epoch": 0.20319783197831978,
      "step": 3749,
      "training_loss": 7.403736591339111
    },
    {
      "epoch": 0.2032520325203252,
      "step": 3750,
      "training_loss": 5.089934825897217
    },
    {
      "epoch": 0.20330623306233062,
      "step": 3751,
      "training_loss": 7.201196670532227
    },
    {
      "epoch": 0.20336043360433603,
      "grad_norm": 23.481536865234375,
      "learning_rate": 1e-05,
      "loss": 6.5253,
      "step": 3752
    },
    {
      "epoch": 0.20336043360433603,
      "step": 3752,
      "training_loss": 4.735683917999268
    },
    {
      "epoch": 0.20341463414634145,
      "step": 3753,
      "training_loss": 7.275781154632568
    },
    {
      "epoch": 0.2034688346883469,
      "step": 3754,
      "training_loss": 6.5600810050964355
    },
    {
      "epoch": 0.2035230352303523,
      "step": 3755,
      "training_loss": 7.231378078460693
    },
    {
      "epoch": 0.20357723577235773,
      "grad_norm": 16.868249893188477,
      "learning_rate": 1e-05,
      "loss": 6.4507,
      "step": 3756
    },
    {
      "epoch": 0.20357723577235773,
      "step": 3756,
      "training_loss": 6.266883373260498
    },
    {
      "epoch": 0.20363143631436315,
      "step": 3757,
      "training_loss": 7.108072280883789
    },
    {
      "epoch": 0.20368563685636856,
      "step": 3758,
      "training_loss": 7.5145463943481445
    },
    {
      "epoch": 0.20373983739837398,
      "step": 3759,
      "training_loss": 6.8705573081970215
    },
    {
      "epoch": 0.2037940379403794,
      "grad_norm": 19.499956130981445,
      "learning_rate": 1e-05,
      "loss": 6.94,
      "step": 3760
    },
    {
      "epoch": 0.2037940379403794,
      "step": 3760,
      "training_loss": 7.254319190979004
    },
    {
      "epoch": 0.2038482384823848,
      "step": 3761,
      "training_loss": 7.747385501861572
    },
    {
      "epoch": 0.20390243902439023,
      "step": 3762,
      "training_loss": 6.521635055541992
    },
    {
      "epoch": 0.20395663956639568,
      "step": 3763,
      "training_loss": 7.14340353012085
    },
    {
      "epoch": 0.2040108401084011,
      "grad_norm": 15.998696327209473,
      "learning_rate": 1e-05,
      "loss": 7.1667,
      "step": 3764
    },
    {
      "epoch": 0.2040108401084011,
      "step": 3764,
      "training_loss": 6.375705718994141
    },
    {
      "epoch": 0.2040650406504065,
      "step": 3765,
      "training_loss": 6.8692307472229
    },
    {
      "epoch": 0.20411924119241193,
      "step": 3766,
      "training_loss": 5.7958879470825195
    },
    {
      "epoch": 0.20417344173441734,
      "step": 3767,
      "training_loss": 5.278838157653809
    },
    {
      "epoch": 0.20422764227642276,
      "grad_norm": 23.002012252807617,
      "learning_rate": 1e-05,
      "loss": 6.0799,
      "step": 3768
    },
    {
      "epoch": 0.20422764227642276,
      "step": 3768,
      "training_loss": 6.560371398925781
    },
    {
      "epoch": 0.20428184281842818,
      "step": 3769,
      "training_loss": 7.05556058883667
    },
    {
      "epoch": 0.2043360433604336,
      "step": 3770,
      "training_loss": 5.911425590515137
    },
    {
      "epoch": 0.204390243902439,
      "step": 3771,
      "training_loss": 6.529625415802002
    },
    {
      "epoch": 0.20444444444444446,
      "grad_norm": 32.05925750732422,
      "learning_rate": 1e-05,
      "loss": 6.5142,
      "step": 3772
    },
    {
      "epoch": 0.20444444444444446,
      "step": 3772,
      "training_loss": 7.186750888824463
    },
    {
      "epoch": 0.20449864498644987,
      "step": 3773,
      "training_loss": 7.254762172698975
    },
    {
      "epoch": 0.2045528455284553,
      "step": 3774,
      "training_loss": 7.112131595611572
    },
    {
      "epoch": 0.2046070460704607,
      "step": 3775,
      "training_loss": 4.544216632843018
    },
    {
      "epoch": 0.20466124661246612,
      "grad_norm": 30.24347686767578,
      "learning_rate": 1e-05,
      "loss": 6.5245,
      "step": 3776
    },
    {
      "epoch": 0.20466124661246612,
      "step": 3776,
      "training_loss": 5.960275650024414
    },
    {
      "epoch": 0.20471544715447154,
      "step": 3777,
      "training_loss": 5.438753604888916
    },
    {
      "epoch": 0.20476964769647696,
      "step": 3778,
      "training_loss": 6.213459491729736
    },
    {
      "epoch": 0.20482384823848238,
      "step": 3779,
      "training_loss": 7.096193313598633
    },
    {
      "epoch": 0.2048780487804878,
      "grad_norm": 26.780899047851562,
      "learning_rate": 1e-05,
      "loss": 6.1772,
      "step": 3780
    },
    {
      "epoch": 0.2048780487804878,
      "step": 3780,
      "training_loss": 6.912062168121338
    },
    {
      "epoch": 0.20493224932249324,
      "step": 3781,
      "training_loss": 6.910130500793457
    },
    {
      "epoch": 0.20498644986449865,
      "step": 3782,
      "training_loss": 5.577674388885498
    },
    {
      "epoch": 0.20504065040650407,
      "step": 3783,
      "training_loss": 6.965164661407471
    },
    {
      "epoch": 0.2050948509485095,
      "grad_norm": 27.676620483398438,
      "learning_rate": 1e-05,
      "loss": 6.5913,
      "step": 3784
    },
    {
      "epoch": 0.2050948509485095,
      "step": 3784,
      "training_loss": 6.469407081604004
    },
    {
      "epoch": 0.2051490514905149,
      "step": 3785,
      "training_loss": 4.501212120056152
    },
    {
      "epoch": 0.20520325203252032,
      "step": 3786,
      "training_loss": 7.512286186218262
    },
    {
      "epoch": 0.20525745257452574,
      "step": 3787,
      "training_loss": 6.7867560386657715
    },
    {
      "epoch": 0.20531165311653116,
      "grad_norm": 13.730051040649414,
      "learning_rate": 1e-05,
      "loss": 6.3174,
      "step": 3788
    },
    {
      "epoch": 0.20531165311653116,
      "step": 3788,
      "training_loss": 5.275069236755371
    },
    {
      "epoch": 0.20536585365853657,
      "step": 3789,
      "training_loss": 8.25909423828125
    },
    {
      "epoch": 0.20542005420054202,
      "step": 3790,
      "training_loss": 5.658388137817383
    },
    {
      "epoch": 0.20547425474254744,
      "step": 3791,
      "training_loss": 5.53835391998291
    },
    {
      "epoch": 0.20552845528455285,
      "grad_norm": 24.392990112304688,
      "learning_rate": 1e-05,
      "loss": 6.1827,
      "step": 3792
    },
    {
      "epoch": 0.20552845528455285,
      "step": 3792,
      "training_loss": 7.258354663848877
    },
    {
      "epoch": 0.20558265582655827,
      "step": 3793,
      "training_loss": 6.920764923095703
    },
    {
      "epoch": 0.2056368563685637,
      "step": 3794,
      "training_loss": 6.990993022918701
    },
    {
      "epoch": 0.2056910569105691,
      "step": 3795,
      "training_loss": 8.564592361450195
    },
    {
      "epoch": 0.20574525745257452,
      "grad_norm": 23.733346939086914,
      "learning_rate": 1e-05,
      "loss": 7.4337,
      "step": 3796
    },
    {
      "epoch": 0.20574525745257452,
      "step": 3796,
      "training_loss": 5.875835418701172
    },
    {
      "epoch": 0.20579945799457994,
      "step": 3797,
      "training_loss": 6.975363731384277
    },
    {
      "epoch": 0.20585365853658535,
      "step": 3798,
      "training_loss": 7.160928249359131
    },
    {
      "epoch": 0.2059078590785908,
      "step": 3799,
      "training_loss": 5.941561698913574
    },
    {
      "epoch": 0.20596205962059622,
      "grad_norm": 35.57673645019531,
      "learning_rate": 1e-05,
      "loss": 6.4884,
      "step": 3800
    },
    {
      "epoch": 0.20596205962059622,
      "step": 3800,
      "training_loss": 6.046288967132568
    },
    {
      "epoch": 0.20601626016260163,
      "step": 3801,
      "training_loss": 7.517364025115967
    },
    {
      "epoch": 0.20607046070460705,
      "step": 3802,
      "training_loss": 8.301523208618164
    },
    {
      "epoch": 0.20612466124661247,
      "step": 3803,
      "training_loss": 6.801531791687012
    },
    {
      "epoch": 0.20617886178861788,
      "grad_norm": 26.156278610229492,
      "learning_rate": 1e-05,
      "loss": 7.1667,
      "step": 3804
    },
    {
      "epoch": 0.20617886178861788,
      "step": 3804,
      "training_loss": 4.75673246383667
    },
    {
      "epoch": 0.2062330623306233,
      "step": 3805,
      "training_loss": 7.181939601898193
    },
    {
      "epoch": 0.20628726287262872,
      "step": 3806,
      "training_loss": 6.025169372558594
    },
    {
      "epoch": 0.20634146341463414,
      "step": 3807,
      "training_loss": 4.7658371925354
    },
    {
      "epoch": 0.20639566395663958,
      "grad_norm": 27.089412689208984,
      "learning_rate": 1e-05,
      "loss": 5.6824,
      "step": 3808
    },
    {
      "epoch": 0.20639566395663958,
      "step": 3808,
      "training_loss": 5.95260763168335
    },
    {
      "epoch": 0.206449864498645,
      "step": 3809,
      "training_loss": 7.122954845428467
    },
    {
      "epoch": 0.20650406504065041,
      "step": 3810,
      "training_loss": 7.892776012420654
    },
    {
      "epoch": 0.20655826558265583,
      "step": 3811,
      "training_loss": 6.233120918273926
    },
    {
      "epoch": 0.20661246612466125,
      "grad_norm": 22.90690803527832,
      "learning_rate": 1e-05,
      "loss": 6.8004,
      "step": 3812
    },
    {
      "epoch": 0.20661246612466125,
      "step": 3812,
      "training_loss": 8.103516578674316
    },
    {
      "epoch": 0.20666666666666667,
      "step": 3813,
      "training_loss": 6.692227363586426
    },
    {
      "epoch": 0.20672086720867208,
      "step": 3814,
      "training_loss": 7.558800220489502
    },
    {
      "epoch": 0.2067750677506775,
      "step": 3815,
      "training_loss": 5.164167881011963
    },
    {
      "epoch": 0.20682926829268292,
      "grad_norm": 19.71681022644043,
      "learning_rate": 1e-05,
      "loss": 6.8797,
      "step": 3816
    },
    {
      "epoch": 0.20682926829268292,
      "step": 3816,
      "training_loss": 7.053528785705566
    },
    {
      "epoch": 0.20688346883468833,
      "step": 3817,
      "training_loss": 7.225793361663818
    },
    {
      "epoch": 0.20693766937669378,
      "step": 3818,
      "training_loss": 6.050297260284424
    },
    {
      "epoch": 0.2069918699186992,
      "step": 3819,
      "training_loss": 5.7134222984313965
    },
    {
      "epoch": 0.2070460704607046,
      "grad_norm": 31.173913955688477,
      "learning_rate": 1e-05,
      "loss": 6.5108,
      "step": 3820
    },
    {
      "epoch": 0.2070460704607046,
      "step": 3820,
      "training_loss": 6.234885215759277
    },
    {
      "epoch": 0.20710027100271003,
      "step": 3821,
      "training_loss": 6.689486026763916
    },
    {
      "epoch": 0.20715447154471545,
      "step": 3822,
      "training_loss": 7.462898254394531
    },
    {
      "epoch": 0.20720867208672086,
      "step": 3823,
      "training_loss": 6.996607303619385
    },
    {
      "epoch": 0.20726287262872628,
      "grad_norm": 19.794387817382812,
      "learning_rate": 1e-05,
      "loss": 6.846,
      "step": 3824
    },
    {
      "epoch": 0.20726287262872628,
      "step": 3824,
      "training_loss": 5.971132755279541
    },
    {
      "epoch": 0.2073170731707317,
      "step": 3825,
      "training_loss": 6.921026706695557
    },
    {
      "epoch": 0.20737127371273711,
      "step": 3826,
      "training_loss": 8.004469871520996
    },
    {
      "epoch": 0.20742547425474256,
      "step": 3827,
      "training_loss": 7.335885047912598
    },
    {
      "epoch": 0.20747967479674798,
      "grad_norm": 15.814802169799805,
      "learning_rate": 1e-05,
      "loss": 7.0581,
      "step": 3828
    },
    {
      "epoch": 0.20747967479674798,
      "step": 3828,
      "training_loss": 5.919142246246338
    },
    {
      "epoch": 0.2075338753387534,
      "step": 3829,
      "training_loss": 8.005236625671387
    },
    {
      "epoch": 0.2075880758807588,
      "step": 3830,
      "training_loss": 6.948180675506592
    },
    {
      "epoch": 0.20764227642276423,
      "step": 3831,
      "training_loss": 6.789438247680664
    },
    {
      "epoch": 0.20769647696476964,
      "grad_norm": 17.76123809814453,
      "learning_rate": 1e-05,
      "loss": 6.9155,
      "step": 3832
    },
    {
      "epoch": 0.20769647696476964,
      "step": 3832,
      "training_loss": 5.358366012573242
    },
    {
      "epoch": 0.20775067750677506,
      "step": 3833,
      "training_loss": 7.674780368804932
    },
    {
      "epoch": 0.20780487804878048,
      "step": 3834,
      "training_loss": 5.386256217956543
    },
    {
      "epoch": 0.2078590785907859,
      "step": 3835,
      "training_loss": 8.076111793518066
    },
    {
      "epoch": 0.20791327913279134,
      "grad_norm": 39.06454849243164,
      "learning_rate": 1e-05,
      "loss": 6.6239,
      "step": 3836
    },
    {
      "epoch": 0.20791327913279134,
      "step": 3836,
      "training_loss": 7.03363561630249
    },
    {
      "epoch": 0.20796747967479676,
      "step": 3837,
      "training_loss": 7.025528907775879
    },
    {
      "epoch": 0.20802168021680217,
      "step": 3838,
      "training_loss": 6.799386978149414
    },
    {
      "epoch": 0.2080758807588076,
      "step": 3839,
      "training_loss": 7.1652607917785645
    },
    {
      "epoch": 0.208130081300813,
      "grad_norm": 21.7110652923584,
      "learning_rate": 1e-05,
      "loss": 7.006,
      "step": 3840
    },
    {
      "epoch": 0.208130081300813,
      "step": 3840,
      "training_loss": 6.302520275115967
    },
    {
      "epoch": 0.20818428184281842,
      "step": 3841,
      "training_loss": 6.851632118225098
    },
    {
      "epoch": 0.20823848238482384,
      "step": 3842,
      "training_loss": 7.327374458312988
    },
    {
      "epoch": 0.20829268292682926,
      "step": 3843,
      "training_loss": 8.052748680114746
    },
    {
      "epoch": 0.20834688346883468,
      "grad_norm": 24.734600067138672,
      "learning_rate": 1e-05,
      "loss": 7.1336,
      "step": 3844
    },
    {
      "epoch": 0.20834688346883468,
      "step": 3844,
      "training_loss": 7.117315769195557
    },
    {
      "epoch": 0.20840108401084012,
      "step": 3845,
      "training_loss": 6.951404571533203
    },
    {
      "epoch": 0.20845528455284554,
      "step": 3846,
      "training_loss": 7.251163005828857
    },
    {
      "epoch": 0.20850948509485095,
      "step": 3847,
      "training_loss": 5.117646217346191
    },
    {
      "epoch": 0.20856368563685637,
      "grad_norm": 20.883214950561523,
      "learning_rate": 1e-05,
      "loss": 6.6094,
      "step": 3848
    },
    {
      "epoch": 0.20856368563685637,
      "step": 3848,
      "training_loss": 6.260022163391113
    },
    {
      "epoch": 0.2086178861788618,
      "step": 3849,
      "training_loss": 7.564265727996826
    },
    {
      "epoch": 0.2086720867208672,
      "step": 3850,
      "training_loss": 7.024861812591553
    },
    {
      "epoch": 0.20872628726287262,
      "step": 3851,
      "training_loss": 7.861347675323486
    },
    {
      "epoch": 0.20878048780487804,
      "grad_norm": 22.853374481201172,
      "learning_rate": 1e-05,
      "loss": 7.1776,
      "step": 3852
    },
    {
      "epoch": 0.20878048780487804,
      "step": 3852,
      "training_loss": 6.137435436248779
    },
    {
      "epoch": 0.20883468834688346,
      "step": 3853,
      "training_loss": 6.759093284606934
    },
    {
      "epoch": 0.2088888888888889,
      "step": 3854,
      "training_loss": 7.878698825836182
    },
    {
      "epoch": 0.20894308943089432,
      "step": 3855,
      "training_loss": 8.156208992004395
    },
    {
      "epoch": 0.20899728997289974,
      "grad_norm": 31.50153350830078,
      "learning_rate": 1e-05,
      "loss": 7.2329,
      "step": 3856
    },
    {
      "epoch": 0.20899728997289974,
      "step": 3856,
      "training_loss": 7.417598247528076
    },
    {
      "epoch": 0.20905149051490515,
      "step": 3857,
      "training_loss": 6.221846103668213
    },
    {
      "epoch": 0.20910569105691057,
      "step": 3858,
      "training_loss": 7.76515007019043
    },
    {
      "epoch": 0.209159891598916,
      "step": 3859,
      "training_loss": 6.516568660736084
    },
    {
      "epoch": 0.2092140921409214,
      "grad_norm": 17.897579193115234,
      "learning_rate": 1e-05,
      "loss": 6.9803,
      "step": 3860
    },
    {
      "epoch": 0.2092140921409214,
      "step": 3860,
      "training_loss": 6.842835426330566
    },
    {
      "epoch": 0.20926829268292682,
      "step": 3861,
      "training_loss": 4.546152591705322
    },
    {
      "epoch": 0.20932249322493224,
      "step": 3862,
      "training_loss": 5.94821834564209
    },
    {
      "epoch": 0.20937669376693768,
      "step": 3863,
      "training_loss": 7.85129451751709
    },
    {
      "epoch": 0.2094308943089431,
      "grad_norm": 29.55127716064453,
      "learning_rate": 1e-05,
      "loss": 6.2971,
      "step": 3864
    },
    {
      "epoch": 0.2094308943089431,
      "step": 3864,
      "training_loss": 7.8184494972229
    },
    {
      "epoch": 0.20948509485094852,
      "step": 3865,
      "training_loss": 9.428171157836914
    },
    {
      "epoch": 0.20953929539295393,
      "step": 3866,
      "training_loss": 7.256155967712402
    },
    {
      "epoch": 0.20959349593495935,
      "step": 3867,
      "training_loss": 7.46991491317749
    },
    {
      "epoch": 0.20964769647696477,
      "grad_norm": 21.921478271484375,
      "learning_rate": 1e-05,
      "loss": 7.9932,
      "step": 3868
    },
    {
      "epoch": 0.20964769647696477,
      "step": 3868,
      "training_loss": 7.719578266143799
    },
    {
      "epoch": 0.20970189701897018,
      "step": 3869,
      "training_loss": 7.902423858642578
    },
    {
      "epoch": 0.2097560975609756,
      "step": 3870,
      "training_loss": 6.133706092834473
    },
    {
      "epoch": 0.20981029810298102,
      "step": 3871,
      "training_loss": 5.818116188049316
    },
    {
      "epoch": 0.20986449864498646,
      "grad_norm": 28.026477813720703,
      "learning_rate": 1e-05,
      "loss": 6.8935,
      "step": 3872
    },
    {
      "epoch": 0.20986449864498646,
      "step": 3872,
      "training_loss": 8.170235633850098
    },
    {
      "epoch": 0.20991869918699188,
      "step": 3873,
      "training_loss": 7.002233028411865
    },
    {
      "epoch": 0.2099728997289973,
      "step": 3874,
      "training_loss": 3.6572487354278564
    },
    {
      "epoch": 0.21002710027100271,
      "step": 3875,
      "training_loss": 7.142763137817383
    },
    {
      "epoch": 0.21008130081300813,
      "grad_norm": 18.906471252441406,
      "learning_rate": 1e-05,
      "loss": 6.4931,
      "step": 3876
    },
    {
      "epoch": 0.21008130081300813,
      "step": 3876,
      "training_loss": 4.376753807067871
    },
    {
      "epoch": 0.21013550135501355,
      "step": 3877,
      "training_loss": 5.4424920082092285
    },
    {
      "epoch": 0.21018970189701897,
      "step": 3878,
      "training_loss": 7.0227556228637695
    },
    {
      "epoch": 0.21024390243902438,
      "step": 3879,
      "training_loss": 7.278994083404541
    },
    {
      "epoch": 0.2102981029810298,
      "grad_norm": 35.929412841796875,
      "learning_rate": 1e-05,
      "loss": 6.0302,
      "step": 3880
    },
    {
      "epoch": 0.2102981029810298,
      "step": 3880,
      "training_loss": 5.761898040771484
    },
    {
      "epoch": 0.21035230352303522,
      "step": 3881,
      "training_loss": 6.787208080291748
    },
    {
      "epoch": 0.21040650406504066,
      "step": 3882,
      "training_loss": 6.820392608642578
    },
    {
      "epoch": 0.21046070460704608,
      "step": 3883,
      "training_loss": 6.706552982330322
    },
    {
      "epoch": 0.2105149051490515,
      "grad_norm": 30.171180725097656,
      "learning_rate": 1e-05,
      "loss": 6.519,
      "step": 3884
    },
    {
      "epoch": 0.2105149051490515,
      "step": 3884,
      "training_loss": 6.850915431976318
    },
    {
      "epoch": 0.2105691056910569,
      "step": 3885,
      "training_loss": 7.010969638824463
    },
    {
      "epoch": 0.21062330623306233,
      "step": 3886,
      "training_loss": 6.954214572906494
    },
    {
      "epoch": 0.21067750677506775,
      "step": 3887,
      "training_loss": 7.230890274047852
    },
    {
      "epoch": 0.21073170731707316,
      "grad_norm": 16.000232696533203,
      "learning_rate": 1e-05,
      "loss": 7.0117,
      "step": 3888
    },
    {
      "epoch": 0.21073170731707316,
      "step": 3888,
      "training_loss": 7.519984245300293
    },
    {
      "epoch": 0.21078590785907858,
      "step": 3889,
      "training_loss": 6.69010591506958
    },
    {
      "epoch": 0.210840108401084,
      "step": 3890,
      "training_loss": 6.395223140716553
    },
    {
      "epoch": 0.21089430894308944,
      "step": 3891,
      "training_loss": 8.101165771484375
    },
    {
      "epoch": 0.21094850948509486,
      "grad_norm": 26.299665451049805,
      "learning_rate": 1e-05,
      "loss": 7.1766,
      "step": 3892
    },
    {
      "epoch": 0.21094850948509486,
      "step": 3892,
      "training_loss": 4.784836769104004
    },
    {
      "epoch": 0.21100271002710028,
      "step": 3893,
      "training_loss": 6.6865057945251465
    },
    {
      "epoch": 0.2110569105691057,
      "step": 3894,
      "training_loss": 9.5493803024292
    },
    {
      "epoch": 0.2111111111111111,
      "step": 3895,
      "training_loss": 6.849838733673096
    },
    {
      "epoch": 0.21116531165311653,
      "grad_norm": 32.127017974853516,
      "learning_rate": 1e-05,
      "loss": 6.9676,
      "step": 3896
    },
    {
      "epoch": 0.21116531165311653,
      "step": 3896,
      "training_loss": 6.4462103843688965
    },
    {
      "epoch": 0.21121951219512194,
      "step": 3897,
      "training_loss": 5.083954811096191
    },
    {
      "epoch": 0.21127371273712736,
      "step": 3898,
      "training_loss": 6.025144577026367
    },
    {
      "epoch": 0.21132791327913278,
      "step": 3899,
      "training_loss": 6.726621627807617
    },
    {
      "epoch": 0.21138211382113822,
      "grad_norm": 29.129610061645508,
      "learning_rate": 1e-05,
      "loss": 6.0705,
      "step": 3900
    },
    {
      "epoch": 0.21138211382113822,
      "step": 3900,
      "training_loss": 4.559609413146973
    },
    {
      "epoch": 0.21143631436314364,
      "step": 3901,
      "training_loss": 8.209848403930664
    },
    {
      "epoch": 0.21149051490514906,
      "step": 3902,
      "training_loss": 6.339117050170898
    },
    {
      "epoch": 0.21154471544715447,
      "step": 3903,
      "training_loss": 7.030979156494141
    },
    {
      "epoch": 0.2115989159891599,
      "grad_norm": 25.902301788330078,
      "learning_rate": 1e-05,
      "loss": 6.5349,
      "step": 3904
    },
    {
      "epoch": 0.2115989159891599,
      "step": 3904,
      "training_loss": 7.418928623199463
    },
    {
      "epoch": 0.2116531165311653,
      "step": 3905,
      "training_loss": 4.027886867523193
    },
    {
      "epoch": 0.21170731707317073,
      "step": 3906,
      "training_loss": 6.433897972106934
    },
    {
      "epoch": 0.21176151761517614,
      "step": 3907,
      "training_loss": 7.607709884643555
    },
    {
      "epoch": 0.21181571815718156,
      "grad_norm": 36.51329803466797,
      "learning_rate": 1e-05,
      "loss": 6.3721,
      "step": 3908
    },
    {
      "epoch": 0.21181571815718156,
      "step": 3908,
      "training_loss": 4.887247562408447
    },
    {
      "epoch": 0.211869918699187,
      "step": 3909,
      "training_loss": 6.063617706298828
    },
    {
      "epoch": 0.21192411924119242,
      "step": 3910,
      "training_loss": 5.577686786651611
    },
    {
      "epoch": 0.21197831978319784,
      "step": 3911,
      "training_loss": 6.3955793380737305
    },
    {
      "epoch": 0.21203252032520326,
      "grad_norm": 17.389474868774414,
      "learning_rate": 1e-05,
      "loss": 5.731,
      "step": 3912
    },
    {
      "epoch": 0.21203252032520326,
      "step": 3912,
      "training_loss": 7.088480472564697
    },
    {
      "epoch": 0.21208672086720867,
      "step": 3913,
      "training_loss": 6.919827461242676
    },
    {
      "epoch": 0.2121409214092141,
      "step": 3914,
      "training_loss": 5.264483451843262
    },
    {
      "epoch": 0.2121951219512195,
      "step": 3915,
      "training_loss": 6.596003532409668
    },
    {
      "epoch": 0.21224932249322492,
      "grad_norm": 29.7912540435791,
      "learning_rate": 1e-05,
      "loss": 6.4672,
      "step": 3916
    },
    {
      "epoch": 0.21224932249322492,
      "step": 3916,
      "training_loss": 6.253208160400391
    },
    {
      "epoch": 0.21230352303523034,
      "step": 3917,
      "training_loss": 7.958639621734619
    },
    {
      "epoch": 0.21235772357723579,
      "step": 3918,
      "training_loss": 6.201200008392334
    },
    {
      "epoch": 0.2124119241192412,
      "step": 3919,
      "training_loss": 7.362308025360107
    },
    {
      "epoch": 0.21246612466124662,
      "grad_norm": 17.51653289794922,
      "learning_rate": 1e-05,
      "loss": 6.9438,
      "step": 3920
    },
    {
      "epoch": 0.21246612466124662,
      "step": 3920,
      "training_loss": 7.547051906585693
    },
    {
      "epoch": 0.21252032520325204,
      "step": 3921,
      "training_loss": 6.605013370513916
    },
    {
      "epoch": 0.21257452574525745,
      "step": 3922,
      "training_loss": 7.500350475311279
    },
    {
      "epoch": 0.21262872628726287,
      "step": 3923,
      "training_loss": 6.36594295501709
    },
    {
      "epoch": 0.2126829268292683,
      "grad_norm": 22.6464786529541,
      "learning_rate": 1e-05,
      "loss": 7.0046,
      "step": 3924
    },
    {
      "epoch": 0.2126829268292683,
      "step": 3924,
      "training_loss": 4.848431587219238
    },
    {
      "epoch": 0.2127371273712737,
      "step": 3925,
      "training_loss": 7.187383651733398
    },
    {
      "epoch": 0.21279132791327912,
      "step": 3926,
      "training_loss": 7.2261643409729
    },
    {
      "epoch": 0.21284552845528457,
      "step": 3927,
      "training_loss": 6.788715839385986
    },
    {
      "epoch": 0.21289972899728998,
      "grad_norm": 24.177133560180664,
      "learning_rate": 1e-05,
      "loss": 6.5127,
      "step": 3928
    },
    {
      "epoch": 0.21289972899728998,
      "step": 3928,
      "training_loss": 5.853002548217773
    },
    {
      "epoch": 0.2129539295392954,
      "step": 3929,
      "training_loss": 7.211835861206055
    },
    {
      "epoch": 0.21300813008130082,
      "step": 3930,
      "training_loss": 6.760707855224609
    },
    {
      "epoch": 0.21306233062330623,
      "step": 3931,
      "training_loss": 6.116564750671387
    },
    {
      "epoch": 0.21311653116531165,
      "grad_norm": 31.79603385925293,
      "learning_rate": 1e-05,
      "loss": 6.4855,
      "step": 3932
    },
    {
      "epoch": 0.21311653116531165,
      "step": 3932,
      "training_loss": 5.856260299682617
    },
    {
      "epoch": 0.21317073170731707,
      "step": 3933,
      "training_loss": 6.58408260345459
    },
    {
      "epoch": 0.21322493224932249,
      "step": 3934,
      "training_loss": 6.241202354431152
    },
    {
      "epoch": 0.2132791327913279,
      "step": 3935,
      "training_loss": 7.325661659240723
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 28.12739372253418,
      "learning_rate": 1e-05,
      "loss": 6.5018,
      "step": 3936
    },
    {
      "epoch": 0.21333333333333335,
      "step": 3936,
      "training_loss": 8.290103912353516
    },
    {
      "epoch": 0.21338753387533876,
      "step": 3937,
      "training_loss": 5.431718826293945
    },
    {
      "epoch": 0.21344173441734418,
      "step": 3938,
      "training_loss": 5.692765712738037
    },
    {
      "epoch": 0.2134959349593496,
      "step": 3939,
      "training_loss": 7.300207614898682
    },
    {
      "epoch": 0.21355013550135502,
      "grad_norm": 20.010469436645508,
      "learning_rate": 1e-05,
      "loss": 6.6787,
      "step": 3940
    },
    {
      "epoch": 0.21355013550135502,
      "step": 3940,
      "training_loss": 7.697545528411865
    },
    {
      "epoch": 0.21360433604336043,
      "step": 3941,
      "training_loss": 8.208379745483398
    },
    {
      "epoch": 0.21365853658536585,
      "step": 3942,
      "training_loss": 6.883516788482666
    },
    {
      "epoch": 0.21371273712737127,
      "step": 3943,
      "training_loss": 7.677276611328125
    },
    {
      "epoch": 0.21376693766937668,
      "grad_norm": 27.64386558532715,
      "learning_rate": 1e-05,
      "loss": 7.6167,
      "step": 3944
    },
    {
      "epoch": 0.21376693766937668,
      "step": 3944,
      "training_loss": 4.435361862182617
    },
    {
      "epoch": 0.2138211382113821,
      "step": 3945,
      "training_loss": 6.47470760345459
    },
    {
      "epoch": 0.21387533875338754,
      "step": 3946,
      "training_loss": 8.673595428466797
    },
    {
      "epoch": 0.21392953929539296,
      "step": 3947,
      "training_loss": 8.247069358825684
    },
    {
      "epoch": 0.21398373983739838,
      "grad_norm": 31.07786750793457,
      "learning_rate": 1e-05,
      "loss": 6.9577,
      "step": 3948
    },
    {
      "epoch": 0.21398373983739838,
      "step": 3948,
      "training_loss": 5.961287498474121
    },
    {
      "epoch": 0.2140379403794038,
      "step": 3949,
      "training_loss": 6.879069805145264
    },
    {
      "epoch": 0.2140921409214092,
      "step": 3950,
      "training_loss": 7.060001850128174
    },
    {
      "epoch": 0.21414634146341463,
      "step": 3951,
      "training_loss": 7.368741035461426
    },
    {
      "epoch": 0.21420054200542005,
      "grad_norm": 20.216026306152344,
      "learning_rate": 1e-05,
      "loss": 6.8173,
      "step": 3952
    },
    {
      "epoch": 0.21420054200542005,
      "step": 3952,
      "training_loss": 7.894190311431885
    },
    {
      "epoch": 0.21425474254742546,
      "step": 3953,
      "training_loss": 7.171477317810059
    },
    {
      "epoch": 0.21430894308943088,
      "step": 3954,
      "training_loss": 7.401179790496826
    },
    {
      "epoch": 0.21436314363143633,
      "step": 3955,
      "training_loss": 6.653152942657471
    },
    {
      "epoch": 0.21441734417344174,
      "grad_norm": 24.149723052978516,
      "learning_rate": 1e-05,
      "loss": 7.28,
      "step": 3956
    },
    {
      "epoch": 0.21441734417344174,
      "step": 3956,
      "training_loss": 6.920646667480469
    },
    {
      "epoch": 0.21447154471544716,
      "step": 3957,
      "training_loss": 7.3410725593566895
    },
    {
      "epoch": 0.21452574525745258,
      "step": 3958,
      "training_loss": 7.406496047973633
    },
    {
      "epoch": 0.214579945799458,
      "step": 3959,
      "training_loss": 6.755458354949951
    },
    {
      "epoch": 0.2146341463414634,
      "grad_norm": 16.520530700683594,
      "learning_rate": 1e-05,
      "loss": 7.1059,
      "step": 3960
    },
    {
      "epoch": 0.2146341463414634,
      "step": 3960,
      "training_loss": 7.261811256408691
    },
    {
      "epoch": 0.21468834688346883,
      "step": 3961,
      "training_loss": 6.761796474456787
    },
    {
      "epoch": 0.21474254742547425,
      "step": 3962,
      "training_loss": 7.253380298614502
    },
    {
      "epoch": 0.21479674796747966,
      "step": 3963,
      "training_loss": 6.8646087646484375
    },
    {
      "epoch": 0.2148509485094851,
      "grad_norm": 24.80393409729004,
      "learning_rate": 1e-05,
      "loss": 7.0354,
      "step": 3964
    },
    {
      "epoch": 0.2148509485094851,
      "step": 3964,
      "training_loss": 6.807260513305664
    },
    {
      "epoch": 0.21490514905149052,
      "step": 3965,
      "training_loss": 7.798518180847168
    },
    {
      "epoch": 0.21495934959349594,
      "step": 3966,
      "training_loss": 5.643454074859619
    },
    {
      "epoch": 0.21501355013550136,
      "step": 3967,
      "training_loss": 6.414785861968994
    },
    {
      "epoch": 0.21506775067750677,
      "grad_norm": 37.713035583496094,
      "learning_rate": 1e-05,
      "loss": 6.666,
      "step": 3968
    },
    {
      "epoch": 0.21506775067750677,
      "step": 3968,
      "training_loss": 4.464761257171631
    },
    {
      "epoch": 0.2151219512195122,
      "step": 3969,
      "training_loss": 6.2351884841918945
    },
    {
      "epoch": 0.2151761517615176,
      "step": 3970,
      "training_loss": 6.8583083152771
    },
    {
      "epoch": 0.21523035230352303,
      "step": 3971,
      "training_loss": 8.083370208740234
    },
    {
      "epoch": 0.21528455284552844,
      "grad_norm": 43.66646194458008,
      "learning_rate": 1e-05,
      "loss": 6.4104,
      "step": 3972
    },
    {
      "epoch": 0.21528455284552844,
      "step": 3972,
      "training_loss": 7.83139181137085
    },
    {
      "epoch": 0.2153387533875339,
      "step": 3973,
      "training_loss": 5.919060707092285
    },
    {
      "epoch": 0.2153929539295393,
      "step": 3974,
      "training_loss": 7.18191385269165
    },
    {
      "epoch": 0.21544715447154472,
      "step": 3975,
      "training_loss": 8.761238098144531
    },
    {
      "epoch": 0.21550135501355014,
      "grad_norm": 24.323101043701172,
      "learning_rate": 1e-05,
      "loss": 7.4234,
      "step": 3976
    },
    {
      "epoch": 0.21550135501355014,
      "step": 3976,
      "training_loss": 6.746639251708984
    },
    {
      "epoch": 0.21555555555555556,
      "step": 3977,
      "training_loss": 5.926162242889404
    },
    {
      "epoch": 0.21560975609756097,
      "step": 3978,
      "training_loss": 7.56758451461792
    },
    {
      "epoch": 0.2156639566395664,
      "step": 3979,
      "training_loss": 7.3034467697143555
    },
    {
      "epoch": 0.2157181571815718,
      "grad_norm": 27.82272720336914,
      "learning_rate": 1e-05,
      "loss": 6.886,
      "step": 3980
    },
    {
      "epoch": 0.2157181571815718,
      "step": 3980,
      "training_loss": 5.06129264831543
    },
    {
      "epoch": 0.21577235772357722,
      "step": 3981,
      "training_loss": 7.32918119430542
    },
    {
      "epoch": 0.21582655826558267,
      "step": 3982,
      "training_loss": 7.543288230895996
    },
    {
      "epoch": 0.21588075880758809,
      "step": 3983,
      "training_loss": 6.033237457275391
    },
    {
      "epoch": 0.2159349593495935,
      "grad_norm": 39.033084869384766,
      "learning_rate": 1e-05,
      "loss": 6.4917,
      "step": 3984
    },
    {
      "epoch": 0.2159349593495935,
      "step": 3984,
      "training_loss": 7.87290620803833
    },
    {
      "epoch": 0.21598915989159892,
      "step": 3985,
      "training_loss": 7.139349460601807
    },
    {
      "epoch": 0.21604336043360434,
      "step": 3986,
      "training_loss": 7.95499324798584
    },
    {
      "epoch": 0.21609756097560975,
      "step": 3987,
      "training_loss": 7.343323230743408
    },
    {
      "epoch": 0.21615176151761517,
      "grad_norm": 18.44797706604004,
      "learning_rate": 1e-05,
      "loss": 7.5776,
      "step": 3988
    },
    {
      "epoch": 0.21615176151761517,
      "step": 3988,
      "training_loss": 7.071167469024658
    },
    {
      "epoch": 0.2162059620596206,
      "step": 3989,
      "training_loss": 6.719727993011475
    },
    {
      "epoch": 0.216260162601626,
      "step": 3990,
      "training_loss": 7.5694708824157715
    },
    {
      "epoch": 0.21631436314363145,
      "step": 3991,
      "training_loss": 7.385441780090332
    },
    {
      "epoch": 0.21636856368563687,
      "grad_norm": 24.510669708251953,
      "learning_rate": 1e-05,
      "loss": 7.1865,
      "step": 3992
    },
    {
      "epoch": 0.21636856368563687,
      "step": 3992,
      "training_loss": 6.682980537414551
    },
    {
      "epoch": 0.21642276422764228,
      "step": 3993,
      "training_loss": 6.567015171051025
    },
    {
      "epoch": 0.2164769647696477,
      "step": 3994,
      "training_loss": 8.345881462097168
    },
    {
      "epoch": 0.21653116531165312,
      "step": 3995,
      "training_loss": 8.279413223266602
    },
    {
      "epoch": 0.21658536585365853,
      "grad_norm": 22.561193466186523,
      "learning_rate": 1e-05,
      "loss": 7.4688,
      "step": 3996
    },
    {
      "epoch": 0.21658536585365853,
      "step": 3996,
      "training_loss": 7.466126441955566
    },
    {
      "epoch": 0.21663956639566395,
      "step": 3997,
      "training_loss": 6.230143070220947
    },
    {
      "epoch": 0.21669376693766937,
      "step": 3998,
      "training_loss": 6.3392333984375
    },
    {
      "epoch": 0.21674796747967479,
      "step": 3999,
      "training_loss": 6.9574785232543945
    },
    {
      "epoch": 0.21680216802168023,
      "grad_norm": 33.403350830078125,
      "learning_rate": 1e-05,
      "loss": 6.7482,
      "step": 4000
    },
    {
      "epoch": 0.21680216802168023,
      "step": 4000,
      "training_loss": 7.1234331130981445
    },
    {
      "epoch": 0.21685636856368565,
      "step": 4001,
      "training_loss": 6.23089599609375
    },
    {
      "epoch": 0.21691056910569106,
      "step": 4002,
      "training_loss": 6.679632186889648
    },
    {
      "epoch": 0.21696476964769648,
      "step": 4003,
      "training_loss": 8.928170204162598
    },
    {
      "epoch": 0.2170189701897019,
      "grad_norm": 35.96221160888672,
      "learning_rate": 1e-05,
      "loss": 7.2405,
      "step": 4004
    },
    {
      "epoch": 0.2170189701897019,
      "step": 4004,
      "training_loss": 7.260451793670654
    },
    {
      "epoch": 0.21707317073170732,
      "step": 4005,
      "training_loss": 10.046250343322754
    },
    {
      "epoch": 0.21712737127371273,
      "step": 4006,
      "training_loss": 7.7572126388549805
    },
    {
      "epoch": 0.21718157181571815,
      "step": 4007,
      "training_loss": 7.303957462310791
    },
    {
      "epoch": 0.21723577235772357,
      "grad_norm": 35.875999450683594,
      "learning_rate": 1e-05,
      "loss": 8.092,
      "step": 4008
    },
    {
      "epoch": 0.21723577235772357,
      "step": 4008,
      "training_loss": 6.981508731842041
    },
    {
      "epoch": 0.21728997289972898,
      "step": 4009,
      "training_loss": 6.686259746551514
    },
    {
      "epoch": 0.21734417344173443,
      "step": 4010,
      "training_loss": 7.673064708709717
    },
    {
      "epoch": 0.21739837398373985,
      "step": 4011,
      "training_loss": 6.770108699798584
    },
    {
      "epoch": 0.21745257452574526,
      "grad_norm": 14.917753219604492,
      "learning_rate": 1e-05,
      "loss": 7.0277,
      "step": 4012
    },
    {
      "epoch": 0.21745257452574526,
      "step": 4012,
      "training_loss": 7.362985610961914
    },
    {
      "epoch": 0.21750677506775068,
      "step": 4013,
      "training_loss": 7.669528007507324
    },
    {
      "epoch": 0.2175609756097561,
      "step": 4014,
      "training_loss": 5.989074230194092
    },
    {
      "epoch": 0.2176151761517615,
      "step": 4015,
      "training_loss": 7.576344013214111
    },
    {
      "epoch": 0.21766937669376693,
      "grad_norm": 36.9124641418457,
      "learning_rate": 1e-05,
      "loss": 7.1495,
      "step": 4016
    },
    {
      "epoch": 0.21766937669376693,
      "step": 4016,
      "training_loss": 7.360233306884766
    },
    {
      "epoch": 0.21772357723577235,
      "step": 4017,
      "training_loss": 8.645576477050781
    },
    {
      "epoch": 0.21777777777777776,
      "step": 4018,
      "training_loss": 7.079346179962158
    },
    {
      "epoch": 0.2178319783197832,
      "step": 4019,
      "training_loss": 6.884378910064697
    },
    {
      "epoch": 0.21788617886178863,
      "grad_norm": 23.944272994995117,
      "learning_rate": 1e-05,
      "loss": 7.4924,
      "step": 4020
    },
    {
      "epoch": 0.21788617886178863,
      "step": 4020,
      "training_loss": 6.64524507522583
    },
    {
      "epoch": 0.21794037940379404,
      "step": 4021,
      "training_loss": 6.1658735275268555
    },
    {
      "epoch": 0.21799457994579946,
      "step": 4022,
      "training_loss": 5.417333126068115
    },
    {
      "epoch": 0.21804878048780488,
      "step": 4023,
      "training_loss": 5.909547328948975
    },
    {
      "epoch": 0.2181029810298103,
      "grad_norm": 24.732955932617188,
      "learning_rate": 1e-05,
      "loss": 6.0345,
      "step": 4024
    },
    {
      "epoch": 0.2181029810298103,
      "step": 4024,
      "training_loss": 6.178500652313232
    },
    {
      "epoch": 0.2181571815718157,
      "step": 4025,
      "training_loss": 7.989866733551025
    },
    {
      "epoch": 0.21821138211382113,
      "step": 4026,
      "training_loss": 4.387531757354736
    },
    {
      "epoch": 0.21826558265582655,
      "step": 4027,
      "training_loss": 7.167354583740234
    },
    {
      "epoch": 0.218319783197832,
      "grad_norm": 21.86565589904785,
      "learning_rate": 1e-05,
      "loss": 6.4308,
      "step": 4028
    },
    {
      "epoch": 0.218319783197832,
      "step": 4028,
      "training_loss": 6.924036979675293
    },
    {
      "epoch": 0.2183739837398374,
      "step": 4029,
      "training_loss": 7.1644134521484375
    },
    {
      "epoch": 0.21842818428184282,
      "step": 4030,
      "training_loss": 6.938899040222168
    },
    {
      "epoch": 0.21848238482384824,
      "step": 4031,
      "training_loss": 6.343527793884277
    },
    {
      "epoch": 0.21853658536585366,
      "grad_norm": 37.84022521972656,
      "learning_rate": 1e-05,
      "loss": 6.8427,
      "step": 4032
    },
    {
      "epoch": 0.21853658536585366,
      "step": 4032,
      "training_loss": 4.816824913024902
    },
    {
      "epoch": 0.21859078590785908,
      "step": 4033,
      "training_loss": 7.394515037536621
    },
    {
      "epoch": 0.2186449864498645,
      "step": 4034,
      "training_loss": 7.1368088722229
    },
    {
      "epoch": 0.2186991869918699,
      "step": 4035,
      "training_loss": 7.048422813415527
    },
    {
      "epoch": 0.21875338753387533,
      "grad_norm": 17.616886138916016,
      "learning_rate": 1e-05,
      "loss": 6.5991,
      "step": 4036
    },
    {
      "epoch": 0.21875338753387533,
      "step": 4036,
      "training_loss": 6.9381256103515625
    },
    {
      "epoch": 0.21880758807588077,
      "step": 4037,
      "training_loss": 7.411272048950195
    },
    {
      "epoch": 0.2188617886178862,
      "step": 4038,
      "training_loss": 7.592193126678467
    },
    {
      "epoch": 0.2189159891598916,
      "step": 4039,
      "training_loss": 6.4018235206604
    },
    {
      "epoch": 0.21897018970189702,
      "grad_norm": 35.38779067993164,
      "learning_rate": 1e-05,
      "loss": 7.0859,
      "step": 4040
    },
    {
      "epoch": 0.21897018970189702,
      "step": 4040,
      "training_loss": 6.574548244476318
    },
    {
      "epoch": 0.21902439024390244,
      "step": 4041,
      "training_loss": 4.18763542175293
    },
    {
      "epoch": 0.21907859078590786,
      "step": 4042,
      "training_loss": 7.29395055770874
    },
    {
      "epoch": 0.21913279132791327,
      "step": 4043,
      "training_loss": 7.049490928649902
    },
    {
      "epoch": 0.2191869918699187,
      "grad_norm": 17.02776336669922,
      "learning_rate": 1e-05,
      "loss": 6.2764,
      "step": 4044
    },
    {
      "epoch": 0.2191869918699187,
      "step": 4044,
      "training_loss": 6.527430057525635
    },
    {
      "epoch": 0.2192411924119241,
      "step": 4045,
      "training_loss": 7.195842742919922
    },
    {
      "epoch": 0.21929539295392955,
      "step": 4046,
      "training_loss": 6.855818748474121
    },
    {
      "epoch": 0.21934959349593497,
      "step": 4047,
      "training_loss": 7.281011581420898
    },
    {
      "epoch": 0.2194037940379404,
      "grad_norm": 49.759891510009766,
      "learning_rate": 1e-05,
      "loss": 6.965,
      "step": 4048
    },
    {
      "epoch": 0.2194037940379404,
      "step": 4048,
      "training_loss": 5.390628814697266
    },
    {
      "epoch": 0.2194579945799458,
      "step": 4049,
      "training_loss": 6.446857452392578
    },
    {
      "epoch": 0.21951219512195122,
      "step": 4050,
      "training_loss": 6.016637802124023
    },
    {
      "epoch": 0.21956639566395664,
      "step": 4051,
      "training_loss": 7.54254150390625
    },
    {
      "epoch": 0.21962059620596205,
      "grad_norm": 32.00193405151367,
      "learning_rate": 1e-05,
      "loss": 6.3492,
      "step": 4052
    },
    {
      "epoch": 0.21962059620596205,
      "step": 4052,
      "training_loss": 7.071527004241943
    },
    {
      "epoch": 0.21967479674796747,
      "step": 4053,
      "training_loss": 6.744954586029053
    },
    {
      "epoch": 0.2197289972899729,
      "step": 4054,
      "training_loss": 5.854695796966553
    },
    {
      "epoch": 0.21978319783197833,
      "step": 4055,
      "training_loss": 6.7136335372924805
    },
    {
      "epoch": 0.21983739837398375,
      "grad_norm": 20.965545654296875,
      "learning_rate": 1e-05,
      "loss": 6.5962,
      "step": 4056
    },
    {
      "epoch": 0.21983739837398375,
      "step": 4056,
      "training_loss": 5.615957736968994
    },
    {
      "epoch": 0.21989159891598917,
      "step": 4057,
      "training_loss": 6.525793552398682
    },
    {
      "epoch": 0.21994579945799458,
      "step": 4058,
      "training_loss": 6.771556377410889
    },
    {
      "epoch": 0.22,
      "step": 4059,
      "training_loss": 5.97222900390625
    },
    {
      "epoch": 0.22005420054200542,
      "grad_norm": 37.72552490234375,
      "learning_rate": 1e-05,
      "loss": 6.2214,
      "step": 4060
    },
    {
      "epoch": 0.22005420054200542,
      "step": 4060,
      "training_loss": 6.7936201095581055
    },
    {
      "epoch": 0.22010840108401084,
      "step": 4061,
      "training_loss": 6.342365741729736
    },
    {
      "epoch": 0.22016260162601625,
      "step": 4062,
      "training_loss": 6.820255756378174
    },
    {
      "epoch": 0.22021680216802167,
      "step": 4063,
      "training_loss": 5.9772467613220215
    },
    {
      "epoch": 0.22027100271002711,
      "grad_norm": 27.470779418945312,
      "learning_rate": 1e-05,
      "loss": 6.4834,
      "step": 4064
    },
    {
      "epoch": 0.22027100271002711,
      "step": 4064,
      "training_loss": 7.207183837890625
    },
    {
      "epoch": 0.22032520325203253,
      "step": 4065,
      "training_loss": 6.88875150680542
    },
    {
      "epoch": 0.22037940379403795,
      "step": 4066,
      "training_loss": 6.725322723388672
    },
    {
      "epoch": 0.22043360433604337,
      "step": 4067,
      "training_loss": 5.365711688995361
    },
    {
      "epoch": 0.22048780487804878,
      "grad_norm": 34.53125,
      "learning_rate": 1e-05,
      "loss": 6.5467,
      "step": 4068
    },
    {
      "epoch": 0.22048780487804878,
      "step": 4068,
      "training_loss": 7.22627592086792
    },
    {
      "epoch": 0.2205420054200542,
      "step": 4069,
      "training_loss": 7.478855609893799
    },
    {
      "epoch": 0.22059620596205962,
      "step": 4070,
      "training_loss": 5.36694860458374
    },
    {
      "epoch": 0.22065040650406503,
      "step": 4071,
      "training_loss": 6.9834699630737305
    },
    {
      "epoch": 0.22070460704607045,
      "grad_norm": 20.059154510498047,
      "learning_rate": 1e-05,
      "loss": 6.7639,
      "step": 4072
    },
    {
      "epoch": 0.22070460704607045,
      "step": 4072,
      "training_loss": 8.588367462158203
    },
    {
      "epoch": 0.22075880758807587,
      "step": 4073,
      "training_loss": 6.415325164794922
    },
    {
      "epoch": 0.2208130081300813,
      "step": 4074,
      "training_loss": 6.20147705078125
    },
    {
      "epoch": 0.22086720867208673,
      "step": 4075,
      "training_loss": 6.616713523864746
    },
    {
      "epoch": 0.22092140921409215,
      "grad_norm": 23.016292572021484,
      "learning_rate": 1e-05,
      "loss": 6.9555,
      "step": 4076
    },
    {
      "epoch": 0.22092140921409215,
      "step": 4076,
      "training_loss": 5.838983058929443
    },
    {
      "epoch": 0.22097560975609756,
      "step": 4077,
      "training_loss": 7.553499698638916
    },
    {
      "epoch": 0.22102981029810298,
      "step": 4078,
      "training_loss": 5.4524431228637695
    },
    {
      "epoch": 0.2210840108401084,
      "step": 4079,
      "training_loss": 6.952179908752441
    },
    {
      "epoch": 0.22113821138211381,
      "grad_norm": 21.014917373657227,
      "learning_rate": 1e-05,
      "loss": 6.4493,
      "step": 4080
    },
    {
      "epoch": 0.22113821138211381,
      "step": 4080,
      "training_loss": 6.931869029998779
    },
    {
      "epoch": 0.22119241192411923,
      "step": 4081,
      "training_loss": 7.305164337158203
    },
    {
      "epoch": 0.22124661246612465,
      "step": 4082,
      "training_loss": 8.35661506652832
    },
    {
      "epoch": 0.2213008130081301,
      "step": 4083,
      "training_loss": 7.171274185180664
    },
    {
      "epoch": 0.2213550135501355,
      "grad_norm": 21.008018493652344,
      "learning_rate": 1e-05,
      "loss": 7.4412,
      "step": 4084
    },
    {
      "epoch": 0.2213550135501355,
      "step": 4084,
      "training_loss": 8.232091903686523
    },
    {
      "epoch": 0.22140921409214093,
      "step": 4085,
      "training_loss": 4.439918518066406
    },
    {
      "epoch": 0.22146341463414634,
      "step": 4086,
      "training_loss": 7.365460395812988
    },
    {
      "epoch": 0.22151761517615176,
      "step": 4087,
      "training_loss": 7.116079330444336
    },
    {
      "epoch": 0.22157181571815718,
      "grad_norm": 21.859619140625,
      "learning_rate": 1e-05,
      "loss": 6.7884,
      "step": 4088
    },
    {
      "epoch": 0.22157181571815718,
      "step": 4088,
      "training_loss": 7.2144904136657715
    },
    {
      "epoch": 0.2216260162601626,
      "step": 4089,
      "training_loss": 7.732489585876465
    },
    {
      "epoch": 0.221680216802168,
      "step": 4090,
      "training_loss": 7.292977809906006
    },
    {
      "epoch": 0.22173441734417343,
      "step": 4091,
      "training_loss": 6.6245903968811035
    },
    {
      "epoch": 0.22178861788617887,
      "grad_norm": 20.061840057373047,
      "learning_rate": 1e-05,
      "loss": 7.2161,
      "step": 4092
    },
    {
      "epoch": 0.22178861788617887,
      "step": 4092,
      "training_loss": 6.798467636108398
    },
    {
      "epoch": 0.2218428184281843,
      "step": 4093,
      "training_loss": 6.682686805725098
    },
    {
      "epoch": 0.2218970189701897,
      "step": 4094,
      "training_loss": 6.902859210968018
    },
    {
      "epoch": 0.22195121951219512,
      "step": 4095,
      "training_loss": 6.949934005737305
    },
    {
      "epoch": 0.22200542005420054,
      "grad_norm": 20.005212783813477,
      "learning_rate": 1e-05,
      "loss": 6.8335,
      "step": 4096
    },
    {
      "epoch": 0.22200542005420054,
      "step": 4096,
      "training_loss": 4.53184175491333
    },
    {
      "epoch": 0.22205962059620596,
      "step": 4097,
      "training_loss": 6.054826259613037
    },
    {
      "epoch": 0.22211382113821138,
      "step": 4098,
      "training_loss": 6.120676517486572
    },
    {
      "epoch": 0.2221680216802168,
      "step": 4099,
      "training_loss": 6.467099666595459
    },
    {
      "epoch": 0.2222222222222222,
      "grad_norm": 22.558429718017578,
      "learning_rate": 1e-05,
      "loss": 5.7936,
      "step": 4100
    },
    {
      "epoch": 0.2222222222222222,
      "step": 4100,
      "training_loss": 6.183416843414307
    },
    {
      "epoch": 0.22227642276422765,
      "step": 4101,
      "training_loss": 6.821567535400391
    },
    {
      "epoch": 0.22233062330623307,
      "step": 4102,
      "training_loss": 6.113670349121094
    },
    {
      "epoch": 0.2223848238482385,
      "step": 4103,
      "training_loss": 6.852871894836426
    },
    {
      "epoch": 0.2224390243902439,
      "grad_norm": 18.4316349029541,
      "learning_rate": 1e-05,
      "loss": 6.4929,
      "step": 4104
    },
    {
      "epoch": 0.2224390243902439,
      "step": 4104,
      "training_loss": 8.151611328125
    },
    {
      "epoch": 0.22249322493224932,
      "step": 4105,
      "training_loss": 6.8867411613464355
    },
    {
      "epoch": 0.22254742547425474,
      "step": 4106,
      "training_loss": 7.003637313842773
    },
    {
      "epoch": 0.22260162601626016,
      "step": 4107,
      "training_loss": 7.180736064910889
    },
    {
      "epoch": 0.22265582655826557,
      "grad_norm": 24.627662658691406,
      "learning_rate": 1e-05,
      "loss": 7.3057,
      "step": 4108
    },
    {
      "epoch": 0.22265582655826557,
      "step": 4108,
      "training_loss": 6.648873329162598
    },
    {
      "epoch": 0.222710027100271,
      "step": 4109,
      "training_loss": 8.099369049072266
    },
    {
      "epoch": 0.22276422764227644,
      "step": 4110,
      "training_loss": 6.947166442871094
    },
    {
      "epoch": 0.22281842818428185,
      "step": 4111,
      "training_loss": 8.297039985656738
    },
    {
      "epoch": 0.22287262872628727,
      "grad_norm": 29.77553367614746,
      "learning_rate": 1e-05,
      "loss": 7.4981,
      "step": 4112
    },
    {
      "epoch": 0.22287262872628727,
      "step": 4112,
      "training_loss": 7.80922794342041
    },
    {
      "epoch": 0.2229268292682927,
      "step": 4113,
      "training_loss": 5.678173065185547
    },
    {
      "epoch": 0.2229810298102981,
      "step": 4114,
      "training_loss": 7.511624336242676
    },
    {
      "epoch": 0.22303523035230352,
      "step": 4115,
      "training_loss": 7.506175994873047
    },
    {
      "epoch": 0.22308943089430894,
      "grad_norm": 20.34319496154785,
      "learning_rate": 1e-05,
      "loss": 7.1263,
      "step": 4116
    },
    {
      "epoch": 0.22308943089430894,
      "step": 4116,
      "training_loss": 6.923946380615234
    },
    {
      "epoch": 0.22314363143631435,
      "step": 4117,
      "training_loss": 7.284884929656982
    },
    {
      "epoch": 0.22319783197831977,
      "step": 4118,
      "training_loss": 6.614790916442871
    },
    {
      "epoch": 0.22325203252032522,
      "step": 4119,
      "training_loss": 6.570936679840088
    },
    {
      "epoch": 0.22330623306233063,
      "grad_norm": 34.592376708984375,
      "learning_rate": 1e-05,
      "loss": 6.8486,
      "step": 4120
    },
    {
      "epoch": 0.22330623306233063,
      "step": 4120,
      "training_loss": 6.971793174743652
    },
    {
      "epoch": 0.22336043360433605,
      "step": 4121,
      "training_loss": 5.628452777862549
    },
    {
      "epoch": 0.22341463414634147,
      "step": 4122,
      "training_loss": 7.615406036376953
    },
    {
      "epoch": 0.22346883468834688,
      "step": 4123,
      "training_loss": 6.166704177856445
    },
    {
      "epoch": 0.2235230352303523,
      "grad_norm": 26.216625213623047,
      "learning_rate": 1e-05,
      "loss": 6.5956,
      "step": 4124
    },
    {
      "epoch": 0.2235230352303523,
      "step": 4124,
      "training_loss": 7.321887493133545
    },
    {
      "epoch": 0.22357723577235772,
      "step": 4125,
      "training_loss": 7.339171409606934
    },
    {
      "epoch": 0.22363143631436314,
      "step": 4126,
      "training_loss": 5.212711811065674
    },
    {
      "epoch": 0.22368563685636855,
      "step": 4127,
      "training_loss": 7.325892448425293
    },
    {
      "epoch": 0.223739837398374,
      "grad_norm": 18.301706314086914,
      "learning_rate": 1e-05,
      "loss": 6.7999,
      "step": 4128
    },
    {
      "epoch": 0.223739837398374,
      "step": 4128,
      "training_loss": 7.753897190093994
    },
    {
      "epoch": 0.22379403794037941,
      "step": 4129,
      "training_loss": 8.935059547424316
    },
    {
      "epoch": 0.22384823848238483,
      "step": 4130,
      "training_loss": 7.328277111053467
    },
    {
      "epoch": 0.22390243902439025,
      "step": 4131,
      "training_loss": 6.892980098724365
    },
    {
      "epoch": 0.22395663956639567,
      "grad_norm": 59.344520568847656,
      "learning_rate": 1e-05,
      "loss": 7.7276,
      "step": 4132
    },
    {
      "epoch": 0.22395663956639567,
      "step": 4132,
      "training_loss": 6.024771690368652
    },
    {
      "epoch": 0.22401084010840108,
      "step": 4133,
      "training_loss": 7.470433712005615
    },
    {
      "epoch": 0.2240650406504065,
      "step": 4134,
      "training_loss": 8.064765930175781
    },
    {
      "epoch": 0.22411924119241192,
      "step": 4135,
      "training_loss": 8.091643333435059
    },
    {
      "epoch": 0.22417344173441733,
      "grad_norm": 52.90707015991211,
      "learning_rate": 1e-05,
      "loss": 7.4129,
      "step": 4136
    },
    {
      "epoch": 0.22417344173441733,
      "step": 4136,
      "training_loss": 6.799311637878418
    },
    {
      "epoch": 0.22422764227642275,
      "step": 4137,
      "training_loss": 7.275129318237305
    },
    {
      "epoch": 0.2242818428184282,
      "step": 4138,
      "training_loss": 8.116592407226562
    },
    {
      "epoch": 0.2243360433604336,
      "step": 4139,
      "training_loss": 6.20548152923584
    },
    {
      "epoch": 0.22439024390243903,
      "grad_norm": 24.07093048095703,
      "learning_rate": 1e-05,
      "loss": 7.0991,
      "step": 4140
    },
    {
      "epoch": 0.22439024390243903,
      "step": 4140,
      "training_loss": 7.733544826507568
    },
    {
      "epoch": 0.22444444444444445,
      "step": 4141,
      "training_loss": 6.619667053222656
    },
    {
      "epoch": 0.22449864498644986,
      "step": 4142,
      "training_loss": 6.054275035858154
    },
    {
      "epoch": 0.22455284552845528,
      "step": 4143,
      "training_loss": 6.7412109375
    },
    {
      "epoch": 0.2246070460704607,
      "grad_norm": 16.543994903564453,
      "learning_rate": 1e-05,
      "loss": 6.7872,
      "step": 4144
    },
    {
      "epoch": 0.2246070460704607,
      "step": 4144,
      "training_loss": 4.418519496917725
    },
    {
      "epoch": 0.22466124661246611,
      "step": 4145,
      "training_loss": 6.084198951721191
    },
    {
      "epoch": 0.22471544715447153,
      "step": 4146,
      "training_loss": 6.8445658683776855
    },
    {
      "epoch": 0.22476964769647698,
      "step": 4147,
      "training_loss": 7.1573920249938965
    },
    {
      "epoch": 0.2248238482384824,
      "grad_norm": 20.34365463256836,
      "learning_rate": 1e-05,
      "loss": 6.1262,
      "step": 4148
    },
    {
      "epoch": 0.2248238482384824,
      "step": 4148,
      "training_loss": 7.604632377624512
    },
    {
      "epoch": 0.2248780487804878,
      "step": 4149,
      "training_loss": 7.170168399810791
    },
    {
      "epoch": 0.22493224932249323,
      "step": 4150,
      "training_loss": 11.59896183013916
    },
    {
      "epoch": 0.22498644986449864,
      "step": 4151,
      "training_loss": 5.432461261749268
    },
    {
      "epoch": 0.22504065040650406,
      "grad_norm": 45.05095291137695,
      "learning_rate": 1e-05,
      "loss": 7.9516,
      "step": 4152
    },
    {
      "epoch": 0.22504065040650406,
      "step": 4152,
      "training_loss": 7.285581588745117
    },
    {
      "epoch": 0.22509485094850948,
      "step": 4153,
      "training_loss": 6.110910892486572
    },
    {
      "epoch": 0.2251490514905149,
      "step": 4154,
      "training_loss": 6.8191118240356445
    },
    {
      "epoch": 0.2252032520325203,
      "step": 4155,
      "training_loss": 7.5815839767456055
    },
    {
      "epoch": 0.22525745257452576,
      "grad_norm": 16.72846794128418,
      "learning_rate": 1e-05,
      "loss": 6.9493,
      "step": 4156
    },
    {
      "epoch": 0.22525745257452576,
      "step": 4156,
      "training_loss": 6.840685844421387
    },
    {
      "epoch": 0.22531165311653117,
      "step": 4157,
      "training_loss": 6.894312858581543
    },
    {
      "epoch": 0.2253658536585366,
      "step": 4158,
      "training_loss": 7.066439151763916
    },
    {
      "epoch": 0.225420054200542,
      "step": 4159,
      "training_loss": 6.329594612121582
    },
    {
      "epoch": 0.22547425474254743,
      "grad_norm": 24.23981285095215,
      "learning_rate": 1e-05,
      "loss": 6.7828,
      "step": 4160
    },
    {
      "epoch": 0.22547425474254743,
      "step": 4160,
      "training_loss": 8.1886568069458
    },
    {
      "epoch": 0.22552845528455284,
      "step": 4161,
      "training_loss": 5.553374767303467
    },
    {
      "epoch": 0.22558265582655826,
      "step": 4162,
      "training_loss": 6.8200459480285645
    },
    {
      "epoch": 0.22563685636856368,
      "step": 4163,
      "training_loss": 7.016386985778809
    },
    {
      "epoch": 0.2256910569105691,
      "grad_norm": 24.214054107666016,
      "learning_rate": 1e-05,
      "loss": 6.8946,
      "step": 4164
    },
    {
      "epoch": 0.2256910569105691,
      "step": 4164,
      "training_loss": 5.9942708015441895
    },
    {
      "epoch": 0.22574525745257454,
      "step": 4165,
      "training_loss": 7.201555252075195
    },
    {
      "epoch": 0.22579945799457996,
      "step": 4166,
      "training_loss": 7.0515265464782715
    },
    {
      "epoch": 0.22585365853658537,
      "step": 4167,
      "training_loss": 6.194610595703125
    },
    {
      "epoch": 0.2259078590785908,
      "grad_norm": 16.93990707397461,
      "learning_rate": 1e-05,
      "loss": 6.6105,
      "step": 4168
    },
    {
      "epoch": 0.2259078590785908,
      "step": 4168,
      "training_loss": 6.100837230682373
    },
    {
      "epoch": 0.2259620596205962,
      "step": 4169,
      "training_loss": 7.079367637634277
    },
    {
      "epoch": 0.22601626016260162,
      "step": 4170,
      "training_loss": 6.511838912963867
    },
    {
      "epoch": 0.22607046070460704,
      "step": 4171,
      "training_loss": 7.363991737365723
    },
    {
      "epoch": 0.22612466124661246,
      "grad_norm": 19.55134391784668,
      "learning_rate": 1e-05,
      "loss": 6.764,
      "step": 4172
    },
    {
      "epoch": 0.22612466124661246,
      "step": 4172,
      "training_loss": 6.4290452003479
    },
    {
      "epoch": 0.22617886178861787,
      "step": 4173,
      "training_loss": 6.499814510345459
    },
    {
      "epoch": 0.22623306233062332,
      "step": 4174,
      "training_loss": 4.797901630401611
    },
    {
      "epoch": 0.22628726287262874,
      "step": 4175,
      "training_loss": 7.2690300941467285
    },
    {
      "epoch": 0.22634146341463415,
      "grad_norm": 41.34626388549805,
      "learning_rate": 1e-05,
      "loss": 6.2489,
      "step": 4176
    },
    {
      "epoch": 0.22634146341463415,
      "step": 4176,
      "training_loss": 5.624520301818848
    },
    {
      "epoch": 0.22639566395663957,
      "step": 4177,
      "training_loss": 7.718352317810059
    },
    {
      "epoch": 0.226449864498645,
      "step": 4178,
      "training_loss": 6.857205390930176
    },
    {
      "epoch": 0.2265040650406504,
      "step": 4179,
      "training_loss": 6.757772922515869
    },
    {
      "epoch": 0.22655826558265582,
      "grad_norm": 29.994068145751953,
      "learning_rate": 1e-05,
      "loss": 6.7395,
      "step": 4180
    },
    {
      "epoch": 0.22655826558265582,
      "step": 4180,
      "training_loss": 6.795315742492676
    },
    {
      "epoch": 0.22661246612466124,
      "step": 4181,
      "training_loss": 5.929982662200928
    },
    {
      "epoch": 0.22666666666666666,
      "step": 4182,
      "training_loss": 7.619246482849121
    },
    {
      "epoch": 0.2267208672086721,
      "step": 4183,
      "training_loss": 7.148619174957275
    },
    {
      "epoch": 0.22677506775067752,
      "grad_norm": 17.393035888671875,
      "learning_rate": 1e-05,
      "loss": 6.8733,
      "step": 4184
    },
    {
      "epoch": 0.22677506775067752,
      "step": 4184,
      "training_loss": 4.824831008911133
    },
    {
      "epoch": 0.22682926829268293,
      "step": 4185,
      "training_loss": 7.146673679351807
    },
    {
      "epoch": 0.22688346883468835,
      "step": 4186,
      "training_loss": 6.848747253417969
    },
    {
      "epoch": 0.22693766937669377,
      "step": 4187,
      "training_loss": 7.469182968139648
    },
    {
      "epoch": 0.22699186991869919,
      "grad_norm": 19.43671226501465,
      "learning_rate": 1e-05,
      "loss": 6.5724,
      "step": 4188
    },
    {
      "epoch": 0.22699186991869919,
      "step": 4188,
      "training_loss": 6.738481521606445
    },
    {
      "epoch": 0.2270460704607046,
      "step": 4189,
      "training_loss": 7.04539155960083
    },
    {
      "epoch": 0.22710027100271002,
      "step": 4190,
      "training_loss": 6.7117509841918945
    },
    {
      "epoch": 0.22715447154471544,
      "step": 4191,
      "training_loss": 6.724305629730225
    },
    {
      "epoch": 0.22720867208672088,
      "grad_norm": 31.04790496826172,
      "learning_rate": 1e-05,
      "loss": 6.805,
      "step": 4192
    },
    {
      "epoch": 0.22720867208672088,
      "step": 4192,
      "training_loss": 7.146434783935547
    },
    {
      "epoch": 0.2272628726287263,
      "step": 4193,
      "training_loss": 7.600211143493652
    },
    {
      "epoch": 0.22731707317073171,
      "step": 4194,
      "training_loss": 7.213775157928467
    },
    {
      "epoch": 0.22737127371273713,
      "step": 4195,
      "training_loss": 7.148360252380371
    },
    {
      "epoch": 0.22742547425474255,
      "grad_norm": 24.334524154663086,
      "learning_rate": 1e-05,
      "loss": 7.2772,
      "step": 4196
    },
    {
      "epoch": 0.22742547425474255,
      "step": 4196,
      "training_loss": 6.7052483558654785
    },
    {
      "epoch": 0.22747967479674797,
      "step": 4197,
      "training_loss": 6.868356704711914
    },
    {
      "epoch": 0.22753387533875338,
      "step": 4198,
      "training_loss": 7.4333882331848145
    },
    {
      "epoch": 0.2275880758807588,
      "step": 4199,
      "training_loss": 6.690549373626709
    },
    {
      "epoch": 0.22764227642276422,
      "grad_norm": 40.33662796020508,
      "learning_rate": 1e-05,
      "loss": 6.9244,
      "step": 4200
    },
    {
      "epoch": 0.22764227642276422,
      "step": 4200,
      "training_loss": 8.845355033874512
    },
    {
      "epoch": 0.22769647696476963,
      "step": 4201,
      "training_loss": 5.854869365692139
    },
    {
      "epoch": 0.22775067750677508,
      "step": 4202,
      "training_loss": 6.587584018707275
    },
    {
      "epoch": 0.2278048780487805,
      "step": 4203,
      "training_loss": 7.641119956970215
    },
    {
      "epoch": 0.2278590785907859,
      "grad_norm": 34.47628402709961,
      "learning_rate": 1e-05,
      "loss": 7.2322,
      "step": 4204
    },
    {
      "epoch": 0.2278590785907859,
      "step": 4204,
      "training_loss": 8.692529678344727
    },
    {
      "epoch": 0.22791327913279133,
      "step": 4205,
      "training_loss": 7.199920177459717
    },
    {
      "epoch": 0.22796747967479675,
      "step": 4206,
      "training_loss": 6.365113735198975
    },
    {
      "epoch": 0.22802168021680216,
      "step": 4207,
      "training_loss": 7.408240795135498
    },
    {
      "epoch": 0.22807588075880758,
      "grad_norm": 20.117053985595703,
      "learning_rate": 1e-05,
      "loss": 7.4165,
      "step": 4208
    },
    {
      "epoch": 0.22807588075880758,
      "step": 4208,
      "training_loss": 7.338084697723389
    },
    {
      "epoch": 0.228130081300813,
      "step": 4209,
      "training_loss": 6.603647708892822
    },
    {
      "epoch": 0.22818428184281841,
      "step": 4210,
      "training_loss": 6.460409641265869
    },
    {
      "epoch": 0.22823848238482386,
      "step": 4211,
      "training_loss": 7.310305595397949
    },
    {
      "epoch": 0.22829268292682928,
      "grad_norm": 29.24696922302246,
      "learning_rate": 1e-05,
      "loss": 6.9281,
      "step": 4212
    },
    {
      "epoch": 0.22829268292682928,
      "step": 4212,
      "training_loss": 6.535386562347412
    },
    {
      "epoch": 0.2283468834688347,
      "step": 4213,
      "training_loss": 6.860266208648682
    },
    {
      "epoch": 0.2284010840108401,
      "step": 4214,
      "training_loss": 7.141116142272949
    },
    {
      "epoch": 0.22845528455284553,
      "step": 4215,
      "training_loss": 7.002740383148193
    },
    {
      "epoch": 0.22850948509485094,
      "grad_norm": 31.001296997070312,
      "learning_rate": 1e-05,
      "loss": 6.8849,
      "step": 4216
    },
    {
      "epoch": 0.22850948509485094,
      "step": 4216,
      "training_loss": 6.864316463470459
    },
    {
      "epoch": 0.22856368563685636,
      "step": 4217,
      "training_loss": 7.001038551330566
    },
    {
      "epoch": 0.22861788617886178,
      "step": 4218,
      "training_loss": 5.335446357727051
    },
    {
      "epoch": 0.2286720867208672,
      "step": 4219,
      "training_loss": 6.358856201171875
    },
    {
      "epoch": 0.22872628726287264,
      "grad_norm": 24.977581024169922,
      "learning_rate": 1e-05,
      "loss": 6.3899,
      "step": 4220
    },
    {
      "epoch": 0.22872628726287264,
      "step": 4220,
      "training_loss": 7.6785454750061035
    },
    {
      "epoch": 0.22878048780487806,
      "step": 4221,
      "training_loss": 6.9177327156066895
    },
    {
      "epoch": 0.22883468834688347,
      "step": 4222,
      "training_loss": 7.76539421081543
    },
    {
      "epoch": 0.2288888888888889,
      "step": 4223,
      "training_loss": 7.284115314483643
    },
    {
      "epoch": 0.2289430894308943,
      "grad_norm": 24.794361114501953,
      "learning_rate": 1e-05,
      "loss": 7.4114,
      "step": 4224
    },
    {
      "epoch": 0.2289430894308943,
      "step": 4224,
      "training_loss": 7.619570732116699
    },
    {
      "epoch": 0.22899728997289973,
      "step": 4225,
      "training_loss": 5.207873344421387
    },
    {
      "epoch": 0.22905149051490514,
      "step": 4226,
      "training_loss": 4.472482681274414
    },
    {
      "epoch": 0.22910569105691056,
      "step": 4227,
      "training_loss": 6.558895111083984
    },
    {
      "epoch": 0.22915989159891598,
      "grad_norm": 33.103275299072266,
      "learning_rate": 1e-05,
      "loss": 5.9647,
      "step": 4228
    },
    {
      "epoch": 0.22915989159891598,
      "step": 4228,
      "training_loss": 6.773101329803467
    },
    {
      "epoch": 0.22921409214092142,
      "step": 4229,
      "training_loss": 6.778286457061768
    },
    {
      "epoch": 0.22926829268292684,
      "step": 4230,
      "training_loss": 7.39033842086792
    },
    {
      "epoch": 0.22932249322493226,
      "step": 4231,
      "training_loss": 6.01769495010376
    },
    {
      "epoch": 0.22937669376693767,
      "grad_norm": 25.01999855041504,
      "learning_rate": 1e-05,
      "loss": 6.7399,
      "step": 4232
    },
    {
      "epoch": 0.22937669376693767,
      "step": 4232,
      "training_loss": 8.08940601348877
    },
    {
      "epoch": 0.2294308943089431,
      "step": 4233,
      "training_loss": 7.029899597167969
    },
    {
      "epoch": 0.2294850948509485,
      "step": 4234,
      "training_loss": 4.804824352264404
    },
    {
      "epoch": 0.22953929539295392,
      "step": 4235,
      "training_loss": 6.4619927406311035
    },
    {
      "epoch": 0.22959349593495934,
      "grad_norm": 20.81642723083496,
      "learning_rate": 1e-05,
      "loss": 6.5965,
      "step": 4236
    },
    {
      "epoch": 0.22959349593495934,
      "step": 4236,
      "training_loss": 6.856471061706543
    },
    {
      "epoch": 0.22964769647696476,
      "step": 4237,
      "training_loss": 7.169760227203369
    },
    {
      "epoch": 0.2297018970189702,
      "step": 4238,
      "training_loss": 6.763638019561768
    },
    {
      "epoch": 0.22975609756097562,
      "step": 4239,
      "training_loss": 7.783140659332275
    },
    {
      "epoch": 0.22981029810298104,
      "grad_norm": 25.889514923095703,
      "learning_rate": 1e-05,
      "loss": 7.1433,
      "step": 4240
    },
    {
      "epoch": 0.22981029810298104,
      "step": 4240,
      "training_loss": 6.09543514251709
    },
    {
      "epoch": 0.22986449864498645,
      "step": 4241,
      "training_loss": 5.838420867919922
    },
    {
      "epoch": 0.22991869918699187,
      "step": 4242,
      "training_loss": 6.8158745765686035
    },
    {
      "epoch": 0.2299728997289973,
      "step": 4243,
      "training_loss": 5.121786594390869
    },
    {
      "epoch": 0.2300271002710027,
      "grad_norm": 19.348005294799805,
      "learning_rate": 1e-05,
      "loss": 5.9679,
      "step": 4244
    },
    {
      "epoch": 0.2300271002710027,
      "step": 4244,
      "training_loss": 5.525928020477295
    },
    {
      "epoch": 0.23008130081300812,
      "step": 4245,
      "training_loss": 7.455216407775879
    },
    {
      "epoch": 0.23013550135501354,
      "step": 4246,
      "training_loss": 6.978044033050537
    },
    {
      "epoch": 0.23018970189701898,
      "step": 4247,
      "training_loss": 6.53294038772583
    },
    {
      "epoch": 0.2302439024390244,
      "grad_norm": 18.038639068603516,
      "learning_rate": 1e-05,
      "loss": 6.623,
      "step": 4248
    },
    {
      "epoch": 0.2302439024390244,
      "step": 4248,
      "training_loss": 6.75203800201416
    },
    {
      "epoch": 0.23029810298102982,
      "step": 4249,
      "training_loss": 7.35309362411499
    },
    {
      "epoch": 0.23035230352303523,
      "step": 4250,
      "training_loss": 6.266088485717773
    },
    {
      "epoch": 0.23040650406504065,
      "step": 4251,
      "training_loss": 7.271257400512695
    },
    {
      "epoch": 0.23046070460704607,
      "grad_norm": 21.460468292236328,
      "learning_rate": 1e-05,
      "loss": 6.9106,
      "step": 4252
    },
    {
      "epoch": 0.23046070460704607,
      "step": 4252,
      "training_loss": 7.242105960845947
    },
    {
      "epoch": 0.23051490514905149,
      "step": 4253,
      "training_loss": 7.505939483642578
    },
    {
      "epoch": 0.2305691056910569,
      "step": 4254,
      "training_loss": 7.153629779815674
    },
    {
      "epoch": 0.23062330623306232,
      "step": 4255,
      "training_loss": 5.629159927368164
    },
    {
      "epoch": 0.23067750677506776,
      "grad_norm": 20.276254653930664,
      "learning_rate": 1e-05,
      "loss": 6.8827,
      "step": 4256
    },
    {
      "epoch": 0.23067750677506776,
      "step": 4256,
      "training_loss": 6.505120277404785
    },
    {
      "epoch": 0.23073170731707318,
      "step": 4257,
      "training_loss": 7.813936710357666
    },
    {
      "epoch": 0.2307859078590786,
      "step": 4258,
      "training_loss": 7.301130771636963
    },
    {
      "epoch": 0.23084010840108402,
      "step": 4259,
      "training_loss": 7.717890739440918
    },
    {
      "epoch": 0.23089430894308943,
      "grad_norm": 17.935928344726562,
      "learning_rate": 1e-05,
      "loss": 7.3345,
      "step": 4260
    },
    {
      "epoch": 0.23089430894308943,
      "step": 4260,
      "training_loss": 5.755926132202148
    },
    {
      "epoch": 0.23094850948509485,
      "step": 4261,
      "training_loss": 7.042089939117432
    },
    {
      "epoch": 0.23100271002710027,
      "step": 4262,
      "training_loss": 6.278707027435303
    },
    {
      "epoch": 0.23105691056910568,
      "step": 4263,
      "training_loss": 6.90524435043335
    },
    {
      "epoch": 0.2311111111111111,
      "grad_norm": 14.44227123260498,
      "learning_rate": 1e-05,
      "loss": 6.4955,
      "step": 4264
    },
    {
      "epoch": 0.2311111111111111,
      "step": 4264,
      "training_loss": 7.544012546539307
    },
    {
      "epoch": 0.23116531165311652,
      "step": 4265,
      "training_loss": 7.387238502502441
    },
    {
      "epoch": 0.23121951219512196,
      "step": 4266,
      "training_loss": 8.332276344299316
    },
    {
      "epoch": 0.23127371273712738,
      "step": 4267,
      "training_loss": 7.205306529998779
    },
    {
      "epoch": 0.2313279132791328,
      "grad_norm": 28.627113342285156,
      "learning_rate": 1e-05,
      "loss": 7.6172,
      "step": 4268
    },
    {
      "epoch": 0.2313279132791328,
      "step": 4268,
      "training_loss": 7.700301647186279
    },
    {
      "epoch": 0.2313821138211382,
      "step": 4269,
      "training_loss": 7.115381717681885
    },
    {
      "epoch": 0.23143631436314363,
      "step": 4270,
      "training_loss": 7.281911849975586
    },
    {
      "epoch": 0.23149051490514905,
      "step": 4271,
      "training_loss": 8.2998046875
    },
    {
      "epoch": 0.23154471544715446,
      "grad_norm": 23.420183181762695,
      "learning_rate": 1e-05,
      "loss": 7.5993,
      "step": 4272
    },
    {
      "epoch": 0.23154471544715446,
      "step": 4272,
      "training_loss": 7.440885543823242
    },
    {
      "epoch": 0.23159891598915988,
      "step": 4273,
      "training_loss": 6.438448429107666
    },
    {
      "epoch": 0.2316531165311653,
      "step": 4274,
      "training_loss": 6.482273578643799
    },
    {
      "epoch": 0.23170731707317074,
      "step": 4275,
      "training_loss": 6.972700119018555
    },
    {
      "epoch": 0.23176151761517616,
      "grad_norm": 29.951087951660156,
      "learning_rate": 1e-05,
      "loss": 6.8336,
      "step": 4276
    },
    {
      "epoch": 0.23176151761517616,
      "step": 4276,
      "training_loss": 6.996655464172363
    },
    {
      "epoch": 0.23181571815718158,
      "step": 4277,
      "training_loss": 6.064216613769531
    },
    {
      "epoch": 0.231869918699187,
      "step": 4278,
      "training_loss": 6.33924674987793
    },
    {
      "epoch": 0.2319241192411924,
      "step": 4279,
      "training_loss": 5.169548988342285
    },
    {
      "epoch": 0.23197831978319783,
      "grad_norm": 27.32291030883789,
      "learning_rate": 1e-05,
      "loss": 6.1424,
      "step": 4280
    },
    {
      "epoch": 0.23197831978319783,
      "step": 4280,
      "training_loss": 6.970345973968506
    },
    {
      "epoch": 0.23203252032520325,
      "step": 4281,
      "training_loss": 6.944186210632324
    },
    {
      "epoch": 0.23208672086720866,
      "step": 4282,
      "training_loss": 5.395129203796387
    },
    {
      "epoch": 0.23214092140921408,
      "step": 4283,
      "training_loss": 6.404702186584473
    },
    {
      "epoch": 0.23219512195121952,
      "grad_norm": 23.12282943725586,
      "learning_rate": 1e-05,
      "loss": 6.4286,
      "step": 4284
    },
    {
      "epoch": 0.23219512195121952,
      "step": 4284,
      "training_loss": 5.905412197113037
    },
    {
      "epoch": 0.23224932249322494,
      "step": 4285,
      "training_loss": 6.4146223068237305
    },
    {
      "epoch": 0.23230352303523036,
      "step": 4286,
      "training_loss": 6.402682304382324
    },
    {
      "epoch": 0.23235772357723578,
      "step": 4287,
      "training_loss": 7.227110385894775
    },
    {
      "epoch": 0.2324119241192412,
      "grad_norm": 27.11325454711914,
      "learning_rate": 1e-05,
      "loss": 6.4875,
      "step": 4288
    },
    {
      "epoch": 0.2324119241192412,
      "step": 4288,
      "training_loss": 6.795136451721191
    },
    {
      "epoch": 0.2324661246612466,
      "step": 4289,
      "training_loss": 5.492066860198975
    },
    {
      "epoch": 0.23252032520325203,
      "step": 4290,
      "training_loss": 6.959251880645752
    },
    {
      "epoch": 0.23257452574525744,
      "step": 4291,
      "training_loss": 7.216732501983643
    },
    {
      "epoch": 0.23262872628726286,
      "grad_norm": 32.04901885986328,
      "learning_rate": 1e-05,
      "loss": 6.6158,
      "step": 4292
    },
    {
      "epoch": 0.23262872628726286,
      "step": 4292,
      "training_loss": 8.127777099609375
    },
    {
      "epoch": 0.2326829268292683,
      "step": 4293,
      "training_loss": 6.613431930541992
    },
    {
      "epoch": 0.23273712737127372,
      "step": 4294,
      "training_loss": 7.988693714141846
    },
    {
      "epoch": 0.23279132791327914,
      "step": 4295,
      "training_loss": 6.850008487701416
    },
    {
      "epoch": 0.23284552845528456,
      "grad_norm": 16.248441696166992,
      "learning_rate": 1e-05,
      "loss": 7.395,
      "step": 4296
    },
    {
      "epoch": 0.23284552845528456,
      "step": 4296,
      "training_loss": 6.224175930023193
    },
    {
      "epoch": 0.23289972899728997,
      "step": 4297,
      "training_loss": 7.074512481689453
    },
    {
      "epoch": 0.2329539295392954,
      "step": 4298,
      "training_loss": 4.89918851852417
    },
    {
      "epoch": 0.2330081300813008,
      "step": 4299,
      "training_loss": 7.010467529296875
    },
    {
      "epoch": 0.23306233062330622,
      "grad_norm": 21.734590530395508,
      "learning_rate": 1e-05,
      "loss": 6.3021,
      "step": 4300
    },
    {
      "epoch": 0.23306233062330622,
      "step": 4300,
      "training_loss": 7.06980037689209
    },
    {
      "epoch": 0.23311653116531164,
      "step": 4301,
      "training_loss": 8.148900032043457
    },
    {
      "epoch": 0.23317073170731709,
      "step": 4302,
      "training_loss": 6.709314346313477
    },
    {
      "epoch": 0.2332249322493225,
      "step": 4303,
      "training_loss": 6.533067226409912
    },
    {
      "epoch": 0.23327913279132792,
      "grad_norm": 21.139089584350586,
      "learning_rate": 1e-05,
      "loss": 7.1153,
      "step": 4304
    },
    {
      "epoch": 0.23327913279132792,
      "step": 4304,
      "training_loss": 6.780628204345703
    },
    {
      "epoch": 0.23333333333333334,
      "step": 4305,
      "training_loss": 6.279298305511475
    },
    {
      "epoch": 0.23338753387533875,
      "step": 4306,
      "training_loss": 7.58099889755249
    },
    {
      "epoch": 0.23344173441734417,
      "step": 4307,
      "training_loss": 5.68433952331543
    },
    {
      "epoch": 0.2334959349593496,
      "grad_norm": 21.258888244628906,
      "learning_rate": 1e-05,
      "loss": 6.5813,
      "step": 4308
    },
    {
      "epoch": 0.2334959349593496,
      "step": 4308,
      "training_loss": 7.180887699127197
    },
    {
      "epoch": 0.233550135501355,
      "step": 4309,
      "training_loss": 6.030940055847168
    },
    {
      "epoch": 0.23360433604336042,
      "step": 4310,
      "training_loss": 6.864538669586182
    },
    {
      "epoch": 0.23365853658536587,
      "step": 4311,
      "training_loss": 5.611827373504639
    },
    {
      "epoch": 0.23371273712737128,
      "grad_norm": 26.106382369995117,
      "learning_rate": 1e-05,
      "loss": 6.422,
      "step": 4312
    },
    {
      "epoch": 0.23371273712737128,
      "step": 4312,
      "training_loss": 7.384819507598877
    },
    {
      "epoch": 0.2337669376693767,
      "step": 4313,
      "training_loss": 6.951534271240234
    },
    {
      "epoch": 0.23382113821138212,
      "step": 4314,
      "training_loss": 6.653585433959961
    },
    {
      "epoch": 0.23387533875338753,
      "step": 4315,
      "training_loss": 7.824566841125488
    },
    {
      "epoch": 0.23392953929539295,
      "grad_norm": 29.15214729309082,
      "learning_rate": 1e-05,
      "loss": 7.2036,
      "step": 4316
    },
    {
      "epoch": 0.23392953929539295,
      "step": 4316,
      "training_loss": 6.0780029296875
    },
    {
      "epoch": 0.23398373983739837,
      "step": 4317,
      "training_loss": 5.289434432983398
    },
    {
      "epoch": 0.23403794037940379,
      "step": 4318,
      "training_loss": 6.497130870819092
    },
    {
      "epoch": 0.2340921409214092,
      "step": 4319,
      "training_loss": 6.853566646575928
    },
    {
      "epoch": 0.23414634146341465,
      "grad_norm": 24.81565284729004,
      "learning_rate": 1e-05,
      "loss": 6.1795,
      "step": 4320
    },
    {
      "epoch": 0.23414634146341465,
      "step": 4320,
      "training_loss": 6.213203430175781
    },
    {
      "epoch": 0.23420054200542006,
      "step": 4321,
      "training_loss": 6.970090866088867
    },
    {
      "epoch": 0.23425474254742548,
      "step": 4322,
      "training_loss": 7.083038330078125
    },
    {
      "epoch": 0.2343089430894309,
      "step": 4323,
      "training_loss": 7.925377368927002
    },
    {
      "epoch": 0.23436314363143632,
      "grad_norm": 25.734222412109375,
      "learning_rate": 1e-05,
      "loss": 7.0479,
      "step": 4324
    },
    {
      "epoch": 0.23436314363143632,
      "step": 4324,
      "training_loss": 8.052496910095215
    },
    {
      "epoch": 0.23441734417344173,
      "step": 4325,
      "training_loss": 7.318731307983398
    },
    {
      "epoch": 0.23447154471544715,
      "step": 4326,
      "training_loss": 7.2207350730896
    },
    {
      "epoch": 0.23452574525745257,
      "step": 4327,
      "training_loss": 6.733564853668213
    },
    {
      "epoch": 0.23457994579945798,
      "grad_norm": 18.650577545166016,
      "learning_rate": 1e-05,
      "loss": 7.3314,
      "step": 4328
    },
    {
      "epoch": 0.23457994579945798,
      "step": 4328,
      "training_loss": 6.689419269561768
    },
    {
      "epoch": 0.2346341463414634,
      "step": 4329,
      "training_loss": 7.002178192138672
    },
    {
      "epoch": 0.23468834688346885,
      "step": 4330,
      "training_loss": 7.577493667602539
    },
    {
      "epoch": 0.23474254742547426,
      "step": 4331,
      "training_loss": 7.1103925704956055
    },
    {
      "epoch": 0.23479674796747968,
      "grad_norm": 21.00897979736328,
      "learning_rate": 1e-05,
      "loss": 7.0949,
      "step": 4332
    },
    {
      "epoch": 0.23479674796747968,
      "step": 4332,
      "training_loss": 7.373303413391113
    },
    {
      "epoch": 0.2348509485094851,
      "step": 4333,
      "training_loss": 7.176000118255615
    },
    {
      "epoch": 0.2349051490514905,
      "step": 4334,
      "training_loss": 7.78582239151001
    },
    {
      "epoch": 0.23495934959349593,
      "step": 4335,
      "training_loss": 7.943779468536377
    },
    {
      "epoch": 0.23501355013550135,
      "grad_norm": 16.026491165161133,
      "learning_rate": 1e-05,
      "loss": 7.5697,
      "step": 4336
    },
    {
      "epoch": 0.23501355013550135,
      "step": 4336,
      "training_loss": 6.4475202560424805
    },
    {
      "epoch": 0.23506775067750676,
      "step": 4337,
      "training_loss": 6.2717437744140625
    },
    {
      "epoch": 0.23512195121951218,
      "step": 4338,
      "training_loss": 5.523937702178955
    },
    {
      "epoch": 0.23517615176151763,
      "step": 4339,
      "training_loss": 4.958921909332275
    },
    {
      "epoch": 0.23523035230352304,
      "grad_norm": 19.618003845214844,
      "learning_rate": 1e-05,
      "loss": 5.8005,
      "step": 4340
    },
    {
      "epoch": 0.23523035230352304,
      "step": 4340,
      "training_loss": 6.8440327644348145
    },
    {
      "epoch": 0.23528455284552846,
      "step": 4341,
      "training_loss": 6.567181587219238
    },
    {
      "epoch": 0.23533875338753388,
      "step": 4342,
      "training_loss": 6.764875411987305
    },
    {
      "epoch": 0.2353929539295393,
      "step": 4343,
      "training_loss": 7.353209495544434
    },
    {
      "epoch": 0.2354471544715447,
      "grad_norm": 21.754444122314453,
      "learning_rate": 1e-05,
      "loss": 6.8823,
      "step": 4344
    },
    {
      "epoch": 0.2354471544715447,
      "step": 4344,
      "training_loss": 7.613955020904541
    },
    {
      "epoch": 0.23550135501355013,
      "step": 4345,
      "training_loss": 6.435595512390137
    },
    {
      "epoch": 0.23555555555555555,
      "step": 4346,
      "training_loss": 5.921867370605469
    },
    {
      "epoch": 0.23560975609756096,
      "step": 4347,
      "training_loss": 7.431410789489746
    },
    {
      "epoch": 0.2356639566395664,
      "grad_norm": 23.11284065246582,
      "learning_rate": 1e-05,
      "loss": 6.8507,
      "step": 4348
    },
    {
      "epoch": 0.2356639566395664,
      "step": 4348,
      "training_loss": 7.884244441986084
    },
    {
      "epoch": 0.23571815718157182,
      "step": 4349,
      "training_loss": 7.529660701751709
    },
    {
      "epoch": 0.23577235772357724,
      "step": 4350,
      "training_loss": 7.712798118591309
    },
    {
      "epoch": 0.23582655826558266,
      "step": 4351,
      "training_loss": 6.622200012207031
    },
    {
      "epoch": 0.23588075880758808,
      "grad_norm": 23.998422622680664,
      "learning_rate": 1e-05,
      "loss": 7.4372,
      "step": 4352
    },
    {
      "epoch": 0.23588075880758808,
      "step": 4352,
      "training_loss": 6.048445701599121
    },
    {
      "epoch": 0.2359349593495935,
      "step": 4353,
      "training_loss": 4.9421868324279785
    },
    {
      "epoch": 0.2359891598915989,
      "step": 4354,
      "training_loss": 6.4204792976379395
    },
    {
      "epoch": 0.23604336043360433,
      "step": 4355,
      "training_loss": 7.2341413497924805
    },
    {
      "epoch": 0.23609756097560974,
      "grad_norm": 14.75236988067627,
      "learning_rate": 1e-05,
      "loss": 6.1613,
      "step": 4356
    },
    {
      "epoch": 0.23609756097560974,
      "step": 4356,
      "training_loss": 7.485603332519531
    },
    {
      "epoch": 0.2361517615176152,
      "step": 4357,
      "training_loss": 6.263051509857178
    },
    {
      "epoch": 0.2362059620596206,
      "step": 4358,
      "training_loss": 8.23328685760498
    },
    {
      "epoch": 0.23626016260162602,
      "step": 4359,
      "training_loss": 7.664773941040039
    },
    {
      "epoch": 0.23631436314363144,
      "grad_norm": 24.57575798034668,
      "learning_rate": 1e-05,
      "loss": 7.4117,
      "step": 4360
    },
    {
      "epoch": 0.23631436314363144,
      "step": 4360,
      "training_loss": 8.359538078308105
    },
    {
      "epoch": 0.23636856368563686,
      "step": 4361,
      "training_loss": 7.356449127197266
    },
    {
      "epoch": 0.23642276422764227,
      "step": 4362,
      "training_loss": 6.2679524421691895
    },
    {
      "epoch": 0.2364769647696477,
      "step": 4363,
      "training_loss": 6.665120601654053
    },
    {
      "epoch": 0.2365311653116531,
      "grad_norm": 26.507112503051758,
      "learning_rate": 1e-05,
      "loss": 7.1623,
      "step": 4364
    },
    {
      "epoch": 0.2365311653116531,
      "step": 4364,
      "training_loss": 6.9254536628723145
    },
    {
      "epoch": 0.23658536585365852,
      "step": 4365,
      "training_loss": 6.877685070037842
    },
    {
      "epoch": 0.23663956639566397,
      "step": 4366,
      "training_loss": 7.397954940795898
    },
    {
      "epoch": 0.2366937669376694,
      "step": 4367,
      "training_loss": 7.783182621002197
    },
    {
      "epoch": 0.2367479674796748,
      "grad_norm": 21.223737716674805,
      "learning_rate": 1e-05,
      "loss": 7.2461,
      "step": 4368
    },
    {
      "epoch": 0.2367479674796748,
      "step": 4368,
      "training_loss": 7.627262592315674
    },
    {
      "epoch": 0.23680216802168022,
      "step": 4369,
      "training_loss": 7.412837028503418
    },
    {
      "epoch": 0.23685636856368564,
      "step": 4370,
      "training_loss": 7.338603973388672
    },
    {
      "epoch": 0.23691056910569105,
      "step": 4371,
      "training_loss": 5.434022903442383
    },
    {
      "epoch": 0.23696476964769647,
      "grad_norm": 22.177261352539062,
      "learning_rate": 1e-05,
      "loss": 6.9532,
      "step": 4372
    },
    {
      "epoch": 0.23696476964769647,
      "step": 4372,
      "training_loss": 7.146210193634033
    },
    {
      "epoch": 0.2370189701897019,
      "step": 4373,
      "training_loss": 5.160923004150391
    },
    {
      "epoch": 0.2370731707317073,
      "step": 4374,
      "training_loss": 7.406973838806152
    },
    {
      "epoch": 0.23712737127371275,
      "step": 4375,
      "training_loss": 5.317807674407959
    },
    {
      "epoch": 0.23718157181571817,
      "grad_norm": 53.280521392822266,
      "learning_rate": 1e-05,
      "loss": 6.258,
      "step": 4376
    },
    {
      "epoch": 0.23718157181571817,
      "step": 4376,
      "training_loss": 6.169683933258057
    },
    {
      "epoch": 0.23723577235772358,
      "step": 4377,
      "training_loss": 6.942132472991943
    },
    {
      "epoch": 0.237289972899729,
      "step": 4378,
      "training_loss": 6.819205284118652
    },
    {
      "epoch": 0.23734417344173442,
      "step": 4379,
      "training_loss": 7.4781413078308105
    },
    {
      "epoch": 0.23739837398373984,
      "grad_norm": 23.46261215209961,
      "learning_rate": 1e-05,
      "loss": 6.8523,
      "step": 4380
    },
    {
      "epoch": 0.23739837398373984,
      "step": 4380,
      "training_loss": 6.8815531730651855
    },
    {
      "epoch": 0.23745257452574525,
      "step": 4381,
      "training_loss": 7.065310001373291
    },
    {
      "epoch": 0.23750677506775067,
      "step": 4382,
      "training_loss": 7.8840789794921875
    },
    {
      "epoch": 0.2375609756097561,
      "step": 4383,
      "training_loss": 6.5218729972839355
    },
    {
      "epoch": 0.23761517615176153,
      "grad_norm": 56.67045974731445,
      "learning_rate": 1e-05,
      "loss": 7.0882,
      "step": 4384
    },
    {
      "epoch": 0.23761517615176153,
      "step": 4384,
      "training_loss": 7.386837959289551
    },
    {
      "epoch": 0.23766937669376695,
      "step": 4385,
      "training_loss": 6.462287425994873
    },
    {
      "epoch": 0.23772357723577237,
      "step": 4386,
      "training_loss": 5.866318225860596
    },
    {
      "epoch": 0.23777777777777778,
      "step": 4387,
      "training_loss": 6.120508670806885
    },
    {
      "epoch": 0.2378319783197832,
      "grad_norm": 20.200637817382812,
      "learning_rate": 1e-05,
      "loss": 6.459,
      "step": 4388
    },
    {
      "epoch": 0.2378319783197832,
      "step": 4388,
      "training_loss": 7.125944137573242
    },
    {
      "epoch": 0.23788617886178862,
      "step": 4389,
      "training_loss": 6.393800258636475
    },
    {
      "epoch": 0.23794037940379403,
      "step": 4390,
      "training_loss": 7.4474101066589355
    },
    {
      "epoch": 0.23799457994579945,
      "step": 4391,
      "training_loss": 8.869770050048828
    },
    {
      "epoch": 0.23804878048780487,
      "grad_norm": 24.917264938354492,
      "learning_rate": 1e-05,
      "loss": 7.4592,
      "step": 4392
    },
    {
      "epoch": 0.23804878048780487,
      "step": 4392,
      "training_loss": 7.319926738739014
    },
    {
      "epoch": 0.23810298102981028,
      "step": 4393,
      "training_loss": 6.81741189956665
    },
    {
      "epoch": 0.23815718157181573,
      "step": 4394,
      "training_loss": 7.273848056793213
    },
    {
      "epoch": 0.23821138211382115,
      "step": 4395,
      "training_loss": 6.321357250213623
    },
    {
      "epoch": 0.23826558265582656,
      "grad_norm": 26.68958282470703,
      "learning_rate": 1e-05,
      "loss": 6.9331,
      "step": 4396
    },
    {
      "epoch": 0.23826558265582656,
      "step": 4396,
      "training_loss": 7.1799211502075195
    },
    {
      "epoch": 0.23831978319783198,
      "step": 4397,
      "training_loss": 6.1893439292907715
    },
    {
      "epoch": 0.2383739837398374,
      "step": 4398,
      "training_loss": 6.818233966827393
    },
    {
      "epoch": 0.23842818428184281,
      "step": 4399,
      "training_loss": 7.8539018630981445
    },
    {
      "epoch": 0.23848238482384823,
      "grad_norm": 29.28624153137207,
      "learning_rate": 1e-05,
      "loss": 7.0104,
      "step": 4400
    },
    {
      "epoch": 0.23848238482384823,
      "step": 4400,
      "training_loss": 6.553771495819092
    },
    {
      "epoch": 0.23853658536585365,
      "step": 4401,
      "training_loss": 7.184660911560059
    },
    {
      "epoch": 0.23859078590785907,
      "step": 4402,
      "training_loss": 6.337023735046387
    },
    {
      "epoch": 0.2386449864498645,
      "step": 4403,
      "training_loss": 7.033659934997559
    },
    {
      "epoch": 0.23869918699186993,
      "grad_norm": 17.967388153076172,
      "learning_rate": 1e-05,
      "loss": 6.7773,
      "step": 4404
    },
    {
      "epoch": 0.23869918699186993,
      "step": 4404,
      "training_loss": 7.906445026397705
    },
    {
      "epoch": 0.23875338753387534,
      "step": 4405,
      "training_loss": 7.531702518463135
    },
    {
      "epoch": 0.23880758807588076,
      "step": 4406,
      "training_loss": 6.814608573913574
    },
    {
      "epoch": 0.23886178861788618,
      "step": 4407,
      "training_loss": 7.9899139404296875
    },
    {
      "epoch": 0.2389159891598916,
      "grad_norm": 17.45176887512207,
      "learning_rate": 1e-05,
      "loss": 7.5607,
      "step": 4408
    },
    {
      "epoch": 0.2389159891598916,
      "step": 4408,
      "training_loss": 4.271721839904785
    },
    {
      "epoch": 0.238970189701897,
      "step": 4409,
      "training_loss": 7.046867847442627
    },
    {
      "epoch": 0.23902439024390243,
      "step": 4410,
      "training_loss": 6.892483711242676
    },
    {
      "epoch": 0.23907859078590785,
      "step": 4411,
      "training_loss": 7.4565019607543945
    },
    {
      "epoch": 0.2391327913279133,
      "grad_norm": 19.478090286254883,
      "learning_rate": 1e-05,
      "loss": 6.4169,
      "step": 4412
    },
    {
      "epoch": 0.2391327913279133,
      "step": 4412,
      "training_loss": 6.644140720367432
    },
    {
      "epoch": 0.2391869918699187,
      "step": 4413,
      "training_loss": 7.1792426109313965
    },
    {
      "epoch": 0.23924119241192413,
      "step": 4414,
      "training_loss": 7.179388523101807
    },
    {
      "epoch": 0.23929539295392954,
      "step": 4415,
      "training_loss": 6.593448162078857
    },
    {
      "epoch": 0.23934959349593496,
      "grad_norm": 41.471309661865234,
      "learning_rate": 1e-05,
      "loss": 6.8991,
      "step": 4416
    },
    {
      "epoch": 0.23934959349593496,
      "step": 4416,
      "training_loss": 7.330967426300049
    },
    {
      "epoch": 0.23940379403794038,
      "step": 4417,
      "training_loss": 6.526386260986328
    },
    {
      "epoch": 0.2394579945799458,
      "step": 4418,
      "training_loss": 7.2749481201171875
    },
    {
      "epoch": 0.2395121951219512,
      "step": 4419,
      "training_loss": 6.88484525680542
    },
    {
      "epoch": 0.23956639566395663,
      "grad_norm": 24.583946228027344,
      "learning_rate": 1e-05,
      "loss": 7.0043,
      "step": 4420
    },
    {
      "epoch": 0.23956639566395663,
      "step": 4420,
      "training_loss": 7.430469512939453
    },
    {
      "epoch": 0.23962059620596207,
      "step": 4421,
      "training_loss": 6.7609968185424805
    },
    {
      "epoch": 0.2396747967479675,
      "step": 4422,
      "training_loss": 5.931519508361816
    },
    {
      "epoch": 0.2397289972899729,
      "step": 4423,
      "training_loss": 6.582832336425781
    },
    {
      "epoch": 0.23978319783197832,
      "grad_norm": 27.119464874267578,
      "learning_rate": 1e-05,
      "loss": 6.6765,
      "step": 4424
    },
    {
      "epoch": 0.23978319783197832,
      "step": 4424,
      "training_loss": 7.351327896118164
    },
    {
      "epoch": 0.23983739837398374,
      "step": 4425,
      "training_loss": 7.715068817138672
    },
    {
      "epoch": 0.23989159891598916,
      "step": 4426,
      "training_loss": 6.197259902954102
    },
    {
      "epoch": 0.23994579945799457,
      "step": 4427,
      "training_loss": 6.558686256408691
    },
    {
      "epoch": 0.24,
      "grad_norm": 18.540584564208984,
      "learning_rate": 1e-05,
      "loss": 6.9556,
      "step": 4428
    },
    {
      "epoch": 0.24,
      "step": 4428,
      "training_loss": 5.920137405395508
    },
    {
      "epoch": 0.2400542005420054,
      "step": 4429,
      "training_loss": 5.918654441833496
    },
    {
      "epoch": 0.24010840108401085,
      "step": 4430,
      "training_loss": 7.253365993499756
    },
    {
      "epoch": 0.24016260162601627,
      "step": 4431,
      "training_loss": 6.813449382781982
    },
    {
      "epoch": 0.2402168021680217,
      "grad_norm": 19.185752868652344,
      "learning_rate": 1e-05,
      "loss": 6.4764,
      "step": 4432
    },
    {
      "epoch": 0.2402168021680217,
      "step": 4432,
      "training_loss": 4.627323150634766
    },
    {
      "epoch": 0.2402710027100271,
      "step": 4433,
      "training_loss": 6.682458400726318
    },
    {
      "epoch": 0.24032520325203252,
      "step": 4434,
      "training_loss": 7.5768141746521
    },
    {
      "epoch": 0.24037940379403794,
      "step": 4435,
      "training_loss": 7.143962383270264
    },
    {
      "epoch": 0.24043360433604336,
      "grad_norm": 23.54817008972168,
      "learning_rate": 1e-05,
      "loss": 6.5076,
      "step": 4436
    },
    {
      "epoch": 0.24043360433604336,
      "step": 4436,
      "training_loss": 7.684629917144775
    },
    {
      "epoch": 0.24048780487804877,
      "step": 4437,
      "training_loss": 8.65621280670166
    },
    {
      "epoch": 0.2405420054200542,
      "step": 4438,
      "training_loss": 6.6017913818359375
    },
    {
      "epoch": 0.24059620596205963,
      "step": 4439,
      "training_loss": 7.6429338455200195
    },
    {
      "epoch": 0.24065040650406505,
      "grad_norm": 16.749080657958984,
      "learning_rate": 1e-05,
      "loss": 7.6464,
      "step": 4440
    },
    {
      "epoch": 0.24065040650406505,
      "step": 4440,
      "training_loss": 7.656450271606445
    },
    {
      "epoch": 0.24070460704607047,
      "step": 4441,
      "training_loss": 11.895524024963379
    },
    {
      "epoch": 0.24075880758807588,
      "step": 4442,
      "training_loss": 7.372775077819824
    },
    {
      "epoch": 0.2408130081300813,
      "step": 4443,
      "training_loss": 6.2542219161987305
    },
    {
      "epoch": 0.24086720867208672,
      "grad_norm": 21.486587524414062,
      "learning_rate": 1e-05,
      "loss": 8.2947,
      "step": 4444
    },
    {
      "epoch": 0.24086720867208672,
      "step": 4444,
      "training_loss": 5.528926372528076
    },
    {
      "epoch": 0.24092140921409214,
      "step": 4445,
      "training_loss": 7.64396858215332
    },
    {
      "epoch": 0.24097560975609755,
      "step": 4446,
      "training_loss": 6.5410332679748535
    },
    {
      "epoch": 0.24102981029810297,
      "step": 4447,
      "training_loss": 7.622759819030762
    },
    {
      "epoch": 0.24108401084010841,
      "grad_norm": 24.22739028930664,
      "learning_rate": 1e-05,
      "loss": 6.8342,
      "step": 4448
    },
    {
      "epoch": 0.24108401084010841,
      "step": 4448,
      "training_loss": 5.470860481262207
    },
    {
      "epoch": 0.24113821138211383,
      "step": 4449,
      "training_loss": 4.471888542175293
    },
    {
      "epoch": 0.24119241192411925,
      "step": 4450,
      "training_loss": 7.626521587371826
    },
    {
      "epoch": 0.24124661246612467,
      "step": 4451,
      "training_loss": 7.541464805603027
    },
    {
      "epoch": 0.24130081300813008,
      "grad_norm": 19.522865295410156,
      "learning_rate": 1e-05,
      "loss": 6.2777,
      "step": 4452
    },
    {
      "epoch": 0.24130081300813008,
      "step": 4452,
      "training_loss": 6.877355098724365
    },
    {
      "epoch": 0.2413550135501355,
      "step": 4453,
      "training_loss": 6.489975929260254
    },
    {
      "epoch": 0.24140921409214092,
      "step": 4454,
      "training_loss": 7.96221399307251
    },
    {
      "epoch": 0.24146341463414633,
      "step": 4455,
      "training_loss": 6.278899669647217
    },
    {
      "epoch": 0.24151761517615175,
      "grad_norm": 22.807878494262695,
      "learning_rate": 1e-05,
      "loss": 6.9021,
      "step": 4456
    },
    {
      "epoch": 0.24151761517615175,
      "step": 4456,
      "training_loss": 6.624127388000488
    },
    {
      "epoch": 0.24157181571815717,
      "step": 4457,
      "training_loss": 7.170284748077393
    },
    {
      "epoch": 0.2416260162601626,
      "step": 4458,
      "training_loss": 7.182557582855225
    },
    {
      "epoch": 0.24168021680216803,
      "step": 4459,
      "training_loss": 7.689818382263184
    },
    {
      "epoch": 0.24173441734417345,
      "grad_norm": 27.096721649169922,
      "learning_rate": 1e-05,
      "loss": 7.1667,
      "step": 4460
    },
    {
      "epoch": 0.24173441734417345,
      "step": 4460,
      "training_loss": 8.005291938781738
    },
    {
      "epoch": 0.24178861788617886,
      "step": 4461,
      "training_loss": 6.566344738006592
    },
    {
      "epoch": 0.24184281842818428,
      "step": 4462,
      "training_loss": 6.099650859832764
    },
    {
      "epoch": 0.2418970189701897,
      "step": 4463,
      "training_loss": 5.755043983459473
    },
    {
      "epoch": 0.24195121951219511,
      "grad_norm": 18.951732635498047,
      "learning_rate": 1e-05,
      "loss": 6.6066,
      "step": 4464
    },
    {
      "epoch": 0.24195121951219511,
      "step": 4464,
      "training_loss": 7.081868648529053
    },
    {
      "epoch": 0.24200542005420053,
      "step": 4465,
      "training_loss": 5.666322231292725
    },
    {
      "epoch": 0.24205962059620595,
      "step": 4466,
      "training_loss": 6.484644412994385
    },
    {
      "epoch": 0.2421138211382114,
      "step": 4467,
      "training_loss": 7.312374114990234
    },
    {
      "epoch": 0.2421680216802168,
      "grad_norm": 27.77928352355957,
      "learning_rate": 1e-05,
      "loss": 6.6363,
      "step": 4468
    },
    {
      "epoch": 0.2421680216802168,
      "step": 4468,
      "training_loss": 7.13197135925293
    },
    {
      "epoch": 0.24222222222222223,
      "step": 4469,
      "training_loss": 7.156121730804443
    },
    {
      "epoch": 0.24227642276422764,
      "step": 4470,
      "training_loss": 6.726797103881836
    },
    {
      "epoch": 0.24233062330623306,
      "step": 4471,
      "training_loss": 7.277840614318848
    },
    {
      "epoch": 0.24238482384823848,
      "grad_norm": 18.644439697265625,
      "learning_rate": 1e-05,
      "loss": 7.0732,
      "step": 4472
    },
    {
      "epoch": 0.24238482384823848,
      "step": 4472,
      "training_loss": 6.4107513427734375
    },
    {
      "epoch": 0.2424390243902439,
      "step": 4473,
      "training_loss": 5.821068286895752
    },
    {
      "epoch": 0.2424932249322493,
      "step": 4474,
      "training_loss": 6.864126682281494
    },
    {
      "epoch": 0.24254742547425473,
      "step": 4475,
      "training_loss": 5.94859504699707
    },
    {
      "epoch": 0.24260162601626017,
      "grad_norm": 61.698448181152344,
      "learning_rate": 1e-05,
      "loss": 6.2611,
      "step": 4476
    },
    {
      "epoch": 0.24260162601626017,
      "step": 4476,
      "training_loss": 4.097338676452637
    },
    {
      "epoch": 0.2426558265582656,
      "step": 4477,
      "training_loss": 5.324566841125488
    },
    {
      "epoch": 0.242710027100271,
      "step": 4478,
      "training_loss": 8.010733604431152
    },
    {
      "epoch": 0.24276422764227643,
      "step": 4479,
      "training_loss": 7.303486347198486
    },
    {
      "epoch": 0.24281842818428184,
      "grad_norm": 18.445661544799805,
      "learning_rate": 1e-05,
      "loss": 6.184,
      "step": 4480
    },
    {
      "epoch": 0.24281842818428184,
      "step": 4480,
      "training_loss": 6.996105194091797
    },
    {
      "epoch": 0.24287262872628726,
      "step": 4481,
      "training_loss": 6.765478610992432
    },
    {
      "epoch": 0.24292682926829268,
      "step": 4482,
      "training_loss": 6.995742321014404
    },
    {
      "epoch": 0.2429810298102981,
      "step": 4483,
      "training_loss": 6.537970066070557
    },
    {
      "epoch": 0.2430352303523035,
      "grad_norm": 21.474321365356445,
      "learning_rate": 1e-05,
      "loss": 6.8238,
      "step": 4484
    },
    {
      "epoch": 0.2430352303523035,
      "step": 4484,
      "training_loss": 6.791171550750732
    },
    {
      "epoch": 0.24308943089430896,
      "step": 4485,
      "training_loss": 6.430607318878174
    },
    {
      "epoch": 0.24314363143631437,
      "step": 4486,
      "training_loss": 7.579246997833252
    },
    {
      "epoch": 0.2431978319783198,
      "step": 4487,
      "training_loss": 7.821079254150391
    },
    {
      "epoch": 0.2432520325203252,
      "grad_norm": 28.302509307861328,
      "learning_rate": 1e-05,
      "loss": 7.1555,
      "step": 4488
    },
    {
      "epoch": 0.2432520325203252,
      "step": 4488,
      "training_loss": 7.388105392456055
    },
    {
      "epoch": 0.24330623306233062,
      "step": 4489,
      "training_loss": 4.482167720794678
    },
    {
      "epoch": 0.24336043360433604,
      "step": 4490,
      "training_loss": 6.577086925506592
    },
    {
      "epoch": 0.24341463414634146,
      "step": 4491,
      "training_loss": 7.3038434982299805
    },
    {
      "epoch": 0.24346883468834687,
      "grad_norm": 28.162363052368164,
      "learning_rate": 1e-05,
      "loss": 6.4378,
      "step": 4492
    },
    {
      "epoch": 0.24346883468834687,
      "step": 4492,
      "training_loss": 7.181783199310303
    },
    {
      "epoch": 0.2435230352303523,
      "step": 4493,
      "training_loss": 5.787737846374512
    },
    {
      "epoch": 0.24357723577235774,
      "step": 4494,
      "training_loss": 5.429065227508545
    },
    {
      "epoch": 0.24363143631436315,
      "step": 4495,
      "training_loss": 8.203770637512207
    },
    {
      "epoch": 0.24368563685636857,
      "grad_norm": 16.483322143554688,
      "learning_rate": 1e-05,
      "loss": 6.6506,
      "step": 4496
    },
    {
      "epoch": 0.24368563685636857,
      "step": 4496,
      "training_loss": 5.569873332977295
    },
    {
      "epoch": 0.243739837398374,
      "step": 4497,
      "training_loss": 6.18034553527832
    },
    {
      "epoch": 0.2437940379403794,
      "step": 4498,
      "training_loss": 7.83945894241333
    },
    {
      "epoch": 0.24384823848238482,
      "step": 4499,
      "training_loss": 7.164546012878418
    },
    {
      "epoch": 0.24390243902439024,
      "grad_norm": 41.87200927734375,
      "learning_rate": 1e-05,
      "loss": 6.6886,
      "step": 4500
    },
    {
      "epoch": 0.24390243902439024,
      "step": 4500,
      "training_loss": 7.404544830322266
    },
    {
      "epoch": 0.24395663956639566,
      "step": 4501,
      "training_loss": 7.072329998016357
    },
    {
      "epoch": 0.24401084010840107,
      "step": 4502,
      "training_loss": 5.771177291870117
    },
    {
      "epoch": 0.24406504065040652,
      "step": 4503,
      "training_loss": 8.163029670715332
    },
    {
      "epoch": 0.24411924119241193,
      "grad_norm": 36.2460823059082,
      "learning_rate": 1e-05,
      "loss": 7.1028,
      "step": 4504
    },
    {
      "epoch": 0.24411924119241193,
      "step": 4504,
      "training_loss": 5.552910804748535
    },
    {
      "epoch": 0.24417344173441735,
      "step": 4505,
      "training_loss": 6.729031562805176
    },
    {
      "epoch": 0.24422764227642277,
      "step": 4506,
      "training_loss": 7.443583011627197
    },
    {
      "epoch": 0.24428184281842819,
      "step": 4507,
      "training_loss": 7.498842716217041
    },
    {
      "epoch": 0.2443360433604336,
      "grad_norm": 20.700660705566406,
      "learning_rate": 1e-05,
      "loss": 6.8061,
      "step": 4508
    },
    {
      "epoch": 0.2443360433604336,
      "step": 4508,
      "training_loss": 7.813329219818115
    },
    {
      "epoch": 0.24439024390243902,
      "step": 4509,
      "training_loss": 5.609825611114502
    },
    {
      "epoch": 0.24444444444444444,
      "step": 4510,
      "training_loss": 6.069540500640869
    },
    {
      "epoch": 0.24449864498644985,
      "step": 4511,
      "training_loss": 7.16575288772583
    },
    {
      "epoch": 0.2445528455284553,
      "grad_norm": 21.64506721496582,
      "learning_rate": 1e-05,
      "loss": 6.6646,
      "step": 4512
    },
    {
      "epoch": 0.2445528455284553,
      "step": 4512,
      "training_loss": 5.135182857513428
    },
    {
      "epoch": 0.24460704607046072,
      "step": 4513,
      "training_loss": 6.346367359161377
    },
    {
      "epoch": 0.24466124661246613,
      "step": 4514,
      "training_loss": 7.509317398071289
    },
    {
      "epoch": 0.24471544715447155,
      "step": 4515,
      "training_loss": 6.550971508026123
    },
    {
      "epoch": 0.24476964769647697,
      "grad_norm": 20.282211303710938,
      "learning_rate": 1e-05,
      "loss": 6.3855,
      "step": 4516
    },
    {
      "epoch": 0.24476964769647697,
      "step": 4516,
      "training_loss": 8.341081619262695
    },
    {
      "epoch": 0.24482384823848238,
      "step": 4517,
      "training_loss": 6.284140110015869
    },
    {
      "epoch": 0.2448780487804878,
      "step": 4518,
      "training_loss": 4.640686511993408
    },
    {
      "epoch": 0.24493224932249322,
      "step": 4519,
      "training_loss": 4.598402976989746
    },
    {
      "epoch": 0.24498644986449863,
      "grad_norm": 26.01274299621582,
      "learning_rate": 1e-05,
      "loss": 5.9661,
      "step": 4520
    },
    {
      "epoch": 0.24498644986449863,
      "step": 4520,
      "training_loss": 6.042552471160889
    },
    {
      "epoch": 0.24504065040650405,
      "step": 4521,
      "training_loss": 6.234198570251465
    },
    {
      "epoch": 0.2450948509485095,
      "step": 4522,
      "training_loss": 7.533639907836914
    },
    {
      "epoch": 0.2451490514905149,
      "step": 4523,
      "training_loss": 7.145086288452148
    },
    {
      "epoch": 0.24520325203252033,
      "grad_norm": 16.01547622680664,
      "learning_rate": 1e-05,
      "loss": 6.7389,
      "step": 4524
    },
    {
      "epoch": 0.24520325203252033,
      "step": 4524,
      "training_loss": 3.889970064163208
    },
    {
      "epoch": 0.24525745257452575,
      "step": 4525,
      "training_loss": 6.775411128997803
    },
    {
      "epoch": 0.24531165311653116,
      "step": 4526,
      "training_loss": 7.044589996337891
    },
    {
      "epoch": 0.24536585365853658,
      "step": 4527,
      "training_loss": 7.3013434410095215
    },
    {
      "epoch": 0.245420054200542,
      "grad_norm": 15.554230690002441,
      "learning_rate": 1e-05,
      "loss": 6.2528,
      "step": 4528
    },
    {
      "epoch": 0.245420054200542,
      "step": 4528,
      "training_loss": 6.514624118804932
    },
    {
      "epoch": 0.24547425474254742,
      "step": 4529,
      "training_loss": 6.318849086761475
    },
    {
      "epoch": 0.24552845528455283,
      "step": 4530,
      "training_loss": 7.1814374923706055
    },
    {
      "epoch": 0.24558265582655828,
      "step": 4531,
      "training_loss": 7.397181987762451
    },
    {
      "epoch": 0.2456368563685637,
      "grad_norm": 16.00885009765625,
      "learning_rate": 1e-05,
      "loss": 6.853,
      "step": 4532
    },
    {
      "epoch": 0.2456368563685637,
      "step": 4532,
      "training_loss": 6.759978294372559
    },
    {
      "epoch": 0.2456910569105691,
      "step": 4533,
      "training_loss": 6.4628682136535645
    },
    {
      "epoch": 0.24574525745257453,
      "step": 4534,
      "training_loss": 5.1588969230651855
    },
    {
      "epoch": 0.24579945799457995,
      "step": 4535,
      "training_loss": 6.574718475341797
    },
    {
      "epoch": 0.24585365853658536,
      "grad_norm": 15.954670906066895,
      "learning_rate": 1e-05,
      "loss": 6.2391,
      "step": 4536
    },
    {
      "epoch": 0.24585365853658536,
      "step": 4536,
      "training_loss": 7.6104326248168945
    },
    {
      "epoch": 0.24590785907859078,
      "step": 4537,
      "training_loss": 7.137988090515137
    },
    {
      "epoch": 0.2459620596205962,
      "step": 4538,
      "training_loss": 7.186797142028809
    },
    {
      "epoch": 0.2460162601626016,
      "step": 4539,
      "training_loss": 7.093160629272461
    },
    {
      "epoch": 0.24607046070460706,
      "grad_norm": 22.790943145751953,
      "learning_rate": 1e-05,
      "loss": 7.2571,
      "step": 4540
    },
    {
      "epoch": 0.24607046070460706,
      "step": 4540,
      "training_loss": 7.440145969390869
    },
    {
      "epoch": 0.24612466124661248,
      "step": 4541,
      "training_loss": 6.835419178009033
    },
    {
      "epoch": 0.2461788617886179,
      "step": 4542,
      "training_loss": 7.7702741622924805
    },
    {
      "epoch": 0.2462330623306233,
      "step": 4543,
      "training_loss": 6.990754127502441
    },
    {
      "epoch": 0.24628726287262873,
      "grad_norm": 20.287410736083984,
      "learning_rate": 1e-05,
      "loss": 7.2591,
      "step": 4544
    },
    {
      "epoch": 0.24628726287262873,
      "step": 4544,
      "training_loss": 7.494838714599609
    },
    {
      "epoch": 0.24634146341463414,
      "step": 4545,
      "training_loss": 5.750039577484131
    },
    {
      "epoch": 0.24639566395663956,
      "step": 4546,
      "training_loss": 7.869279861450195
    },
    {
      "epoch": 0.24644986449864498,
      "step": 4547,
      "training_loss": 5.887230396270752
    },
    {
      "epoch": 0.2465040650406504,
      "grad_norm": 18.272802352905273,
      "learning_rate": 1e-05,
      "loss": 6.7503,
      "step": 4548
    },
    {
      "epoch": 0.2465040650406504,
      "step": 4548,
      "training_loss": 7.684829235076904
    },
    {
      "epoch": 0.24655826558265584,
      "step": 4549,
      "training_loss": 5.7760748863220215
    },
    {
      "epoch": 0.24661246612466126,
      "step": 4550,
      "training_loss": 5.750380039215088
    },
    {
      "epoch": 0.24666666666666667,
      "step": 4551,
      "training_loss": 6.819571495056152
    },
    {
      "epoch": 0.2467208672086721,
      "grad_norm": 24.018138885498047,
      "learning_rate": 1e-05,
      "loss": 6.5077,
      "step": 4552
    },
    {
      "epoch": 0.2467208672086721,
      "step": 4552,
      "training_loss": 6.890917778015137
    },
    {
      "epoch": 0.2467750677506775,
      "step": 4553,
      "training_loss": 7.182192802429199
    },
    {
      "epoch": 0.24682926829268292,
      "step": 4554,
      "training_loss": 6.704033374786377
    },
    {
      "epoch": 0.24688346883468834,
      "step": 4555,
      "training_loss": 6.5764007568359375
    },
    {
      "epoch": 0.24693766937669376,
      "grad_norm": 22.37562370300293,
      "learning_rate": 1e-05,
      "loss": 6.8384,
      "step": 4556
    },
    {
      "epoch": 0.24693766937669376,
      "step": 4556,
      "training_loss": 7.25836706161499
    },
    {
      "epoch": 0.24699186991869918,
      "step": 4557,
      "training_loss": 6.968473434448242
    },
    {
      "epoch": 0.24704607046070462,
      "step": 4558,
      "training_loss": 7.078373908996582
    },
    {
      "epoch": 0.24710027100271004,
      "step": 4559,
      "training_loss": 7.788518905639648
    },
    {
      "epoch": 0.24715447154471545,
      "grad_norm": 23.589799880981445,
      "learning_rate": 1e-05,
      "loss": 7.2734,
      "step": 4560
    },
    {
      "epoch": 0.24715447154471545,
      "step": 4560,
      "training_loss": 6.329687595367432
    },
    {
      "epoch": 0.24720867208672087,
      "step": 4561,
      "training_loss": 6.079620361328125
    },
    {
      "epoch": 0.2472628726287263,
      "step": 4562,
      "training_loss": 7.68562650680542
    },
    {
      "epoch": 0.2473170731707317,
      "step": 4563,
      "training_loss": 6.342005729675293
    },
    {
      "epoch": 0.24737127371273712,
      "grad_norm": 30.73809814453125,
      "learning_rate": 1e-05,
      "loss": 6.6092,
      "step": 4564
    },
    {
      "epoch": 0.24737127371273712,
      "step": 4564,
      "training_loss": 7.128328323364258
    },
    {
      "epoch": 0.24742547425474254,
      "step": 4565,
      "training_loss": 7.136937618255615
    },
    {
      "epoch": 0.24747967479674796,
      "step": 4566,
      "training_loss": 7.031460762023926
    },
    {
      "epoch": 0.2475338753387534,
      "step": 4567,
      "training_loss": 7.506978988647461
    },
    {
      "epoch": 0.24758807588075882,
      "grad_norm": 16.014278411865234,
      "learning_rate": 1e-05,
      "loss": 7.2009,
      "step": 4568
    },
    {
      "epoch": 0.24758807588075882,
      "step": 4568,
      "training_loss": 6.431039333343506
    },
    {
      "epoch": 0.24764227642276423,
      "step": 4569,
      "training_loss": 6.409792423248291
    },
    {
      "epoch": 0.24769647696476965,
      "step": 4570,
      "training_loss": 6.8296380043029785
    },
    {
      "epoch": 0.24775067750677507,
      "step": 4571,
      "training_loss": 6.548471927642822
    },
    {
      "epoch": 0.24780487804878049,
      "grad_norm": 19.125904083251953,
      "learning_rate": 1e-05,
      "loss": 6.5547,
      "step": 4572
    },
    {
      "epoch": 0.24780487804878049,
      "step": 4572,
      "training_loss": 6.9530930519104
    },
    {
      "epoch": 0.2478590785907859,
      "step": 4573,
      "training_loss": 7.27553129196167
    },
    {
      "epoch": 0.24791327913279132,
      "step": 4574,
      "training_loss": 6.520015716552734
    },
    {
      "epoch": 0.24796747967479674,
      "step": 4575,
      "training_loss": 7.90704345703125
    },
    {
      "epoch": 0.24802168021680218,
      "grad_norm": 17.368438720703125,
      "learning_rate": 1e-05,
      "loss": 7.1639,
      "step": 4576
    },
    {
      "epoch": 0.24802168021680218,
      "step": 4576,
      "training_loss": 6.481507778167725
    },
    {
      "epoch": 0.2480758807588076,
      "step": 4577,
      "training_loss": 8.354305267333984
    },
    {
      "epoch": 0.24813008130081302,
      "step": 4578,
      "training_loss": 5.429924488067627
    },
    {
      "epoch": 0.24818428184281843,
      "step": 4579,
      "training_loss": 7.604311466217041
    },
    {
      "epoch": 0.24823848238482385,
      "grad_norm": 20.3966007232666,
      "learning_rate": 1e-05,
      "loss": 6.9675,
      "step": 4580
    },
    {
      "epoch": 0.24823848238482385,
      "step": 4580,
      "training_loss": 6.9965291023254395
    },
    {
      "epoch": 0.24829268292682927,
      "step": 4581,
      "training_loss": 8.372895240783691
    },
    {
      "epoch": 0.24834688346883468,
      "step": 4582,
      "training_loss": 6.701887130737305
    },
    {
      "epoch": 0.2484010840108401,
      "step": 4583,
      "training_loss": 4.441675186157227
    },
    {
      "epoch": 0.24845528455284552,
      "grad_norm": 31.714881896972656,
      "learning_rate": 1e-05,
      "loss": 6.6282,
      "step": 4584
    },
    {
      "epoch": 0.24845528455284552,
      "step": 4584,
      "training_loss": 5.69799280166626
    },
    {
      "epoch": 0.24850948509485093,
      "step": 4585,
      "training_loss": 4.320451259613037
    },
    {
      "epoch": 0.24856368563685638,
      "step": 4586,
      "training_loss": 7.557811260223389
    },
    {
      "epoch": 0.2486178861788618,
      "step": 4587,
      "training_loss": 6.94527006149292
    },
    {
      "epoch": 0.2486720867208672,
      "grad_norm": 21.169239044189453,
      "learning_rate": 1e-05,
      "loss": 6.1304,
      "step": 4588
    },
    {
      "epoch": 0.2486720867208672,
      "step": 4588,
      "training_loss": 7.605837821960449
    },
    {
      "epoch": 0.24872628726287263,
      "step": 4589,
      "training_loss": 7.093896389007568
    },
    {
      "epoch": 0.24878048780487805,
      "step": 4590,
      "training_loss": 8.130249977111816
    },
    {
      "epoch": 0.24883468834688346,
      "step": 4591,
      "training_loss": 7.392412185668945
    },
    {
      "epoch": 0.24888888888888888,
      "grad_norm": 27.44292449951172,
      "learning_rate": 1e-05,
      "loss": 7.5556,
      "step": 4592
    },
    {
      "epoch": 0.24888888888888888,
      "step": 4592,
      "training_loss": 4.4372382164001465
    },
    {
      "epoch": 0.2489430894308943,
      "step": 4593,
      "training_loss": 7.897927284240723
    },
    {
      "epoch": 0.24899728997289972,
      "step": 4594,
      "training_loss": 7.018857955932617
    },
    {
      "epoch": 0.24905149051490516,
      "step": 4595,
      "training_loss": 7.199654579162598
    },
    {
      "epoch": 0.24910569105691058,
      "grad_norm": 24.1024112701416,
      "learning_rate": 1e-05,
      "loss": 6.6384,
      "step": 4596
    },
    {
      "epoch": 0.24910569105691058,
      "step": 4596,
      "training_loss": 7.103385925292969
    },
    {
      "epoch": 0.249159891598916,
      "step": 4597,
      "training_loss": 7.209924221038818
    },
    {
      "epoch": 0.2492140921409214,
      "step": 4598,
      "training_loss": 7.0638203620910645
    },
    {
      "epoch": 0.24926829268292683,
      "step": 4599,
      "training_loss": 7.41127347946167
    },
    {
      "epoch": 0.24932249322493225,
      "grad_norm": 22.770994186401367,
      "learning_rate": 1e-05,
      "loss": 7.1971,
      "step": 4600
    },
    {
      "epoch": 0.24932249322493225,
      "step": 4600,
      "training_loss": 6.04990816116333
    },
    {
      "epoch": 0.24937669376693766,
      "step": 4601,
      "training_loss": 4.893494606018066
    },
    {
      "epoch": 0.24943089430894308,
      "step": 4602,
      "training_loss": 5.409361839294434
    },
    {
      "epoch": 0.2494850948509485,
      "step": 4603,
      "training_loss": 6.867082118988037
    },
    {
      "epoch": 0.24953929539295394,
      "grad_norm": 18.806711196899414,
      "learning_rate": 1e-05,
      "loss": 5.805,
      "step": 4604
    },
    {
      "epoch": 0.24953929539295394,
      "step": 4604,
      "training_loss": 7.8328022956848145
    },
    {
      "epoch": 0.24959349593495936,
      "step": 4605,
      "training_loss": 7.59335994720459
    },
    {
      "epoch": 0.24964769647696478,
      "step": 4606,
      "training_loss": 6.457448482513428
    },
    {
      "epoch": 0.2497018970189702,
      "step": 4607,
      "training_loss": 7.907302379608154
    },
    {
      "epoch": 0.2497560975609756,
      "grad_norm": 41.81088638305664,
      "learning_rate": 1e-05,
      "loss": 7.4477,
      "step": 4608
    },
    {
      "epoch": 0.2497560975609756,
      "step": 4608,
      "training_loss": 6.2744669914245605
    },
    {
      "epoch": 0.24981029810298103,
      "step": 4609,
      "training_loss": 7.387753963470459
    },
    {
      "epoch": 0.24986449864498644,
      "step": 4610,
      "training_loss": 6.343862056732178
    },
    {
      "epoch": 0.24991869918699186,
      "step": 4611,
      "training_loss": 7.204009532928467
    },
    {
      "epoch": 0.24997289972899728,
      "grad_norm": 37.41841125488281,
      "learning_rate": 1e-05,
      "loss": 6.8025,
      "step": 4612
    },
    {
      "epoch": 0.24997289972899728,
      "step": 4612,
      "training_loss": 7.245847225189209
    },
    {
      "epoch": 0.2500271002710027,
      "step": 4613,
      "training_loss": 6.64047384262085
    },
    {
      "epoch": 0.2500813008130081,
      "step": 4614,
      "training_loss": 8.074647903442383
    },
    {
      "epoch": 0.25013550135501356,
      "step": 4615,
      "training_loss": 6.702983379364014
    },
    {
      "epoch": 0.25018970189701895,
      "grad_norm": 23.12486457824707,
      "learning_rate": 1e-05,
      "loss": 7.166,
      "step": 4616
    },
    {
      "epoch": 0.25018970189701895,
      "step": 4616,
      "training_loss": 6.4627685546875
    },
    {
      "epoch": 0.2502439024390244,
      "step": 4617,
      "training_loss": 5.977531433105469
    },
    {
      "epoch": 0.25029810298102984,
      "step": 4618,
      "training_loss": 7.506042003631592
    },
    {
      "epoch": 0.2503523035230352,
      "step": 4619,
      "training_loss": 6.419808387756348
    },
    {
      "epoch": 0.25040650406504067,
      "grad_norm": 82.96723175048828,
      "learning_rate": 1e-05,
      "loss": 6.5915,
      "step": 4620
    },
    {
      "epoch": 0.25040650406504067,
      "step": 4620,
      "training_loss": 6.733415603637695
    },
    {
      "epoch": 0.25046070460704606,
      "step": 4621,
      "training_loss": 6.802484035491943
    },
    {
      "epoch": 0.2505149051490515,
      "step": 4622,
      "training_loss": 8.205282211303711
    },
    {
      "epoch": 0.2505691056910569,
      "step": 4623,
      "training_loss": 5.581820964813232
    },
    {
      "epoch": 0.25062330623306234,
      "grad_norm": 26.381759643554688,
      "learning_rate": 1e-05,
      "loss": 6.8308,
      "step": 4624
    },
    {
      "epoch": 0.25062330623306234,
      "step": 4624,
      "training_loss": 4.65245246887207
    },
    {
      "epoch": 0.2506775067750677,
      "step": 4625,
      "training_loss": 7.4611663818359375
    },
    {
      "epoch": 0.25073170731707317,
      "step": 4626,
      "training_loss": 5.341056823730469
    },
    {
      "epoch": 0.2507859078590786,
      "step": 4627,
      "training_loss": 5.817070960998535
    },
    {
      "epoch": 0.250840108401084,
      "grad_norm": 20.74981117248535,
      "learning_rate": 1e-05,
      "loss": 5.8179,
      "step": 4628
    },
    {
      "epoch": 0.250840108401084,
      "step": 4628,
      "training_loss": 6.704030513763428
    },
    {
      "epoch": 0.25089430894308945,
      "step": 4629,
      "training_loss": 6.240573406219482
    },
    {
      "epoch": 0.25094850948509484,
      "step": 4630,
      "training_loss": 6.458856105804443
    },
    {
      "epoch": 0.2510027100271003,
      "step": 4631,
      "training_loss": 7.970684051513672
    },
    {
      "epoch": 0.2510569105691057,
      "grad_norm": 37.188724517822266,
      "learning_rate": 1e-05,
      "loss": 6.8435,
      "step": 4632
    },
    {
      "epoch": 0.2510569105691057,
      "step": 4632,
      "training_loss": 6.27128791809082
    },
    {
      "epoch": 0.2511111111111111,
      "step": 4633,
      "training_loss": 7.226425647735596
    },
    {
      "epoch": 0.2511653116531165,
      "step": 4634,
      "training_loss": 7.015195846557617
    },
    {
      "epoch": 0.25121951219512195,
      "step": 4635,
      "training_loss": 7.166135311126709
    },
    {
      "epoch": 0.2512737127371274,
      "grad_norm": 40.6094970703125,
      "learning_rate": 1e-05,
      "loss": 6.9198,
      "step": 4636
    },
    {
      "epoch": 0.2512737127371274,
      "step": 4636,
      "training_loss": 6.859950065612793
    },
    {
      "epoch": 0.2513279132791328,
      "step": 4637,
      "training_loss": 8.376907348632812
    },
    {
      "epoch": 0.25138211382113823,
      "step": 4638,
      "training_loss": 7.321602821350098
    },
    {
      "epoch": 0.2514363143631436,
      "step": 4639,
      "training_loss": 7.25193452835083
    },
    {
      "epoch": 0.25149051490514907,
      "grad_norm": 21.85409927368164,
      "learning_rate": 1e-05,
      "loss": 7.4526,
      "step": 4640
    },
    {
      "epoch": 0.25149051490514907,
      "step": 4640,
      "training_loss": 6.823029041290283
    },
    {
      "epoch": 0.25154471544715445,
      "step": 4641,
      "training_loss": 6.952274322509766
    },
    {
      "epoch": 0.2515989159891599,
      "step": 4642,
      "training_loss": 6.236060619354248
    },
    {
      "epoch": 0.2516531165311653,
      "step": 4643,
      "training_loss": 6.918301582336426
    },
    {
      "epoch": 0.25170731707317073,
      "grad_norm": 24.01155662536621,
      "learning_rate": 1e-05,
      "loss": 6.7324,
      "step": 4644
    },
    {
      "epoch": 0.25170731707317073,
      "step": 4644,
      "training_loss": 6.819944381713867
    },
    {
      "epoch": 0.2517615176151762,
      "step": 4645,
      "training_loss": 7.387278079986572
    },
    {
      "epoch": 0.25181571815718157,
      "step": 4646,
      "training_loss": 7.952418327331543
    },
    {
      "epoch": 0.251869918699187,
      "step": 4647,
      "training_loss": 7.534212112426758
    },
    {
      "epoch": 0.2519241192411924,
      "grad_norm": 26.80008316040039,
      "learning_rate": 1e-05,
      "loss": 7.4235,
      "step": 4648
    },
    {
      "epoch": 0.2519241192411924,
      "step": 4648,
      "training_loss": 5.65468168258667
    },
    {
      "epoch": 0.25197831978319785,
      "step": 4649,
      "training_loss": 4.691615104675293
    },
    {
      "epoch": 0.25203252032520324,
      "step": 4650,
      "training_loss": 8.580548286437988
    },
    {
      "epoch": 0.2520867208672087,
      "step": 4651,
      "training_loss": 7.169849872589111
    },
    {
      "epoch": 0.25214092140921407,
      "grad_norm": 27.231306076049805,
      "learning_rate": 1e-05,
      "loss": 6.5242,
      "step": 4652
    },
    {
      "epoch": 0.25214092140921407,
      "step": 4652,
      "training_loss": 6.833973407745361
    },
    {
      "epoch": 0.2521951219512195,
      "step": 4653,
      "training_loss": 5.172091960906982
    },
    {
      "epoch": 0.25224932249322496,
      "step": 4654,
      "training_loss": 6.423611164093018
    },
    {
      "epoch": 0.25230352303523035,
      "step": 4655,
      "training_loss": 7.619742393493652
    },
    {
      "epoch": 0.2523577235772358,
      "grad_norm": 36.05659484863281,
      "learning_rate": 1e-05,
      "loss": 6.5124,
      "step": 4656
    },
    {
      "epoch": 0.2523577235772358,
      "step": 4656,
      "training_loss": 6.813578128814697
    },
    {
      "epoch": 0.2524119241192412,
      "step": 4657,
      "training_loss": 7.633707523345947
    },
    {
      "epoch": 0.2524661246612466,
      "step": 4658,
      "training_loss": 6.711893558502197
    },
    {
      "epoch": 0.252520325203252,
      "step": 4659,
      "training_loss": 6.7235236167907715
    },
    {
      "epoch": 0.25257452574525746,
      "grad_norm": 35.05952835083008,
      "learning_rate": 1e-05,
      "loss": 6.9707,
      "step": 4660
    },
    {
      "epoch": 0.25257452574525746,
      "step": 4660,
      "training_loss": 6.633430480957031
    },
    {
      "epoch": 0.25262872628726285,
      "step": 4661,
      "training_loss": 5.657050609588623
    },
    {
      "epoch": 0.2526829268292683,
      "step": 4662,
      "training_loss": 7.177713394165039
    },
    {
      "epoch": 0.25273712737127374,
      "step": 4663,
      "training_loss": 7.371050834655762
    },
    {
      "epoch": 0.25279132791327913,
      "grad_norm": 16.79178237915039,
      "learning_rate": 1e-05,
      "loss": 6.7098,
      "step": 4664
    },
    {
      "epoch": 0.25279132791327913,
      "step": 4664,
      "training_loss": 7.751221656799316
    },
    {
      "epoch": 0.2528455284552846,
      "step": 4665,
      "training_loss": 6.124966621398926
    },
    {
      "epoch": 0.25289972899728996,
      "step": 4666,
      "training_loss": 6.888044834136963
    },
    {
      "epoch": 0.2529539295392954,
      "step": 4667,
      "training_loss": 7.544567584991455
    },
    {
      "epoch": 0.2530081300813008,
      "grad_norm": 32.01638412475586,
      "learning_rate": 1e-05,
      "loss": 7.0772,
      "step": 4668
    },
    {
      "epoch": 0.2530081300813008,
      "step": 4668,
      "training_loss": 6.50324010848999
    },
    {
      "epoch": 0.25306233062330624,
      "step": 4669,
      "training_loss": 11.355605125427246
    },
    {
      "epoch": 0.25311653116531163,
      "step": 4670,
      "training_loss": 7.093403339385986
    },
    {
      "epoch": 0.2531707317073171,
      "step": 4671,
      "training_loss": 7.122182846069336
    },
    {
      "epoch": 0.2532249322493225,
      "grad_norm": 23.703500747680664,
      "learning_rate": 1e-05,
      "loss": 8.0186,
      "step": 4672
    },
    {
      "epoch": 0.2532249322493225,
      "step": 4672,
      "training_loss": 7.069399356842041
    },
    {
      "epoch": 0.2532791327913279,
      "step": 4673,
      "training_loss": 6.60746431350708
    },
    {
      "epoch": 0.25333333333333335,
      "step": 4674,
      "training_loss": 7.07022762298584
    },
    {
      "epoch": 0.25338753387533874,
      "step": 4675,
      "training_loss": 7.068127632141113
    },
    {
      "epoch": 0.2534417344173442,
      "grad_norm": 22.649747848510742,
      "learning_rate": 1e-05,
      "loss": 6.9538,
      "step": 4676
    },
    {
      "epoch": 0.2534417344173442,
      "step": 4676,
      "training_loss": 6.763116359710693
    },
    {
      "epoch": 0.2534959349593496,
      "step": 4677,
      "training_loss": 7.2142133712768555
    },
    {
      "epoch": 0.253550135501355,
      "step": 4678,
      "training_loss": 6.372584819793701
    },
    {
      "epoch": 0.2536043360433604,
      "step": 4679,
      "training_loss": 7.094247341156006
    },
    {
      "epoch": 0.25365853658536586,
      "grad_norm": 19.366981506347656,
      "learning_rate": 1e-05,
      "loss": 6.861,
      "step": 4680
    },
    {
      "epoch": 0.25365853658536586,
      "step": 4680,
      "training_loss": 6.370521068572998
    },
    {
      "epoch": 0.25371273712737125,
      "step": 4681,
      "training_loss": 6.98710298538208
    },
    {
      "epoch": 0.2537669376693767,
      "step": 4682,
      "training_loss": 4.873260974884033
    },
    {
      "epoch": 0.25382113821138214,
      "step": 4683,
      "training_loss": 5.733206748962402
    },
    {
      "epoch": 0.2538753387533875,
      "grad_norm": 22.866430282592773,
      "learning_rate": 1e-05,
      "loss": 5.991,
      "step": 4684
    },
    {
      "epoch": 0.2538753387533875,
      "step": 4684,
      "training_loss": 6.733104228973389
    },
    {
      "epoch": 0.25392953929539297,
      "step": 4685,
      "training_loss": 9.011604309082031
    },
    {
      "epoch": 0.25398373983739836,
      "step": 4686,
      "training_loss": 6.0508904457092285
    },
    {
      "epoch": 0.2540379403794038,
      "step": 4687,
      "training_loss": 7.450942516326904
    },
    {
      "epoch": 0.2540921409214092,
      "grad_norm": 21.511890411376953,
      "learning_rate": 1e-05,
      "loss": 7.3116,
      "step": 4688
    },
    {
      "epoch": 0.2540921409214092,
      "step": 4688,
      "training_loss": 7.3003926277160645
    },
    {
      "epoch": 0.25414634146341464,
      "step": 4689,
      "training_loss": 6.508411407470703
    },
    {
      "epoch": 0.25420054200542,
      "step": 4690,
      "training_loss": 7.240006923675537
    },
    {
      "epoch": 0.25425474254742547,
      "step": 4691,
      "training_loss": 7.517923355102539
    },
    {
      "epoch": 0.2543089430894309,
      "grad_norm": 25.021516799926758,
      "learning_rate": 1e-05,
      "loss": 7.1417,
      "step": 4692
    },
    {
      "epoch": 0.2543089430894309,
      "step": 4692,
      "training_loss": 4.939881324768066
    },
    {
      "epoch": 0.2543631436314363,
      "step": 4693,
      "training_loss": 7.399707317352295
    },
    {
      "epoch": 0.25441734417344175,
      "step": 4694,
      "training_loss": 6.037477493286133
    },
    {
      "epoch": 0.25447154471544714,
      "step": 4695,
      "training_loss": 7.461540222167969
    },
    {
      "epoch": 0.2545257452574526,
      "grad_norm": 16.980327606201172,
      "learning_rate": 1e-05,
      "loss": 6.4597,
      "step": 4696
    },
    {
      "epoch": 0.2545257452574526,
      "step": 4696,
      "training_loss": 7.288647174835205
    },
    {
      "epoch": 0.254579945799458,
      "step": 4697,
      "training_loss": 7.834609031677246
    },
    {
      "epoch": 0.2546341463414634,
      "step": 4698,
      "training_loss": 7.004611015319824
    },
    {
      "epoch": 0.2546883468834688,
      "step": 4699,
      "training_loss": 5.878342151641846
    },
    {
      "epoch": 0.25474254742547425,
      "grad_norm": 23.178443908691406,
      "learning_rate": 1e-05,
      "loss": 7.0016,
      "step": 4700
    },
    {
      "epoch": 0.25474254742547425,
      "step": 4700,
      "training_loss": 6.801117420196533
    },
    {
      "epoch": 0.2547967479674797,
      "step": 4701,
      "training_loss": 7.014598369598389
    },
    {
      "epoch": 0.2548509485094851,
      "step": 4702,
      "training_loss": 6.798978328704834
    },
    {
      "epoch": 0.25490514905149053,
      "step": 4703,
      "training_loss": 6.22695779800415
    },
    {
      "epoch": 0.2549593495934959,
      "grad_norm": 20.569637298583984,
      "learning_rate": 1e-05,
      "loss": 6.7104,
      "step": 4704
    },
    {
      "epoch": 0.2549593495934959,
      "step": 4704,
      "training_loss": 7.608172416687012
    },
    {
      "epoch": 0.25501355013550137,
      "step": 4705,
      "training_loss": 7.65728759765625
    },
    {
      "epoch": 0.25506775067750675,
      "step": 4706,
      "training_loss": 6.621956825256348
    },
    {
      "epoch": 0.2551219512195122,
      "step": 4707,
      "training_loss": 6.784362316131592
    },
    {
      "epoch": 0.2551761517615176,
      "grad_norm": 33.51100158691406,
      "learning_rate": 1e-05,
      "loss": 7.1679,
      "step": 4708
    },
    {
      "epoch": 0.2551761517615176,
      "step": 4708,
      "training_loss": 6.368015766143799
    },
    {
      "epoch": 0.25523035230352303,
      "step": 4709,
      "training_loss": 7.996453285217285
    },
    {
      "epoch": 0.2552845528455285,
      "step": 4710,
      "training_loss": 6.792501926422119
    },
    {
      "epoch": 0.25533875338753387,
      "step": 4711,
      "training_loss": 5.792491436004639
    },
    {
      "epoch": 0.2553929539295393,
      "grad_norm": 29.033891677856445,
      "learning_rate": 1e-05,
      "loss": 6.7374,
      "step": 4712
    },
    {
      "epoch": 0.2553929539295393,
      "step": 4712,
      "training_loss": 7.160445213317871
    },
    {
      "epoch": 0.2554471544715447,
      "step": 4713,
      "training_loss": 6.366353511810303
    },
    {
      "epoch": 0.25550135501355015,
      "step": 4714,
      "training_loss": 7.013277530670166
    },
    {
      "epoch": 0.25555555555555554,
      "step": 4715,
      "training_loss": 7.063840389251709
    },
    {
      "epoch": 0.255609756097561,
      "grad_norm": 18.709552764892578,
      "learning_rate": 1e-05,
      "loss": 6.901,
      "step": 4716
    },
    {
      "epoch": 0.255609756097561,
      "step": 4716,
      "training_loss": 8.570260047912598
    },
    {
      "epoch": 0.25566395663956637,
      "step": 4717,
      "training_loss": 6.790302276611328
    },
    {
      "epoch": 0.2557181571815718,
      "step": 4718,
      "training_loss": 7.396368026733398
    },
    {
      "epoch": 0.25577235772357726,
      "step": 4719,
      "training_loss": 8.204614639282227
    },
    {
      "epoch": 0.25582655826558265,
      "grad_norm": 47.08232879638672,
      "learning_rate": 1e-05,
      "loss": 7.7404,
      "step": 4720
    },
    {
      "epoch": 0.25582655826558265,
      "step": 4720,
      "training_loss": 5.876017093658447
    },
    {
      "epoch": 0.2558807588075881,
      "step": 4721,
      "training_loss": 6.935137748718262
    },
    {
      "epoch": 0.2559349593495935,
      "step": 4722,
      "training_loss": 7.8045878410339355
    },
    {
      "epoch": 0.2559891598915989,
      "step": 4723,
      "training_loss": 6.336191654205322
    },
    {
      "epoch": 0.2560433604336043,
      "grad_norm": 20.57781219482422,
      "learning_rate": 1e-05,
      "loss": 6.738,
      "step": 4724
    },
    {
      "epoch": 0.2560433604336043,
      "step": 4724,
      "training_loss": 7.884202480316162
    },
    {
      "epoch": 0.25609756097560976,
      "step": 4725,
      "training_loss": 7.122417449951172
    },
    {
      "epoch": 0.25615176151761515,
      "step": 4726,
      "training_loss": 6.764018535614014
    },
    {
      "epoch": 0.2562059620596206,
      "step": 4727,
      "training_loss": 7.1979851722717285
    },
    {
      "epoch": 0.25626016260162604,
      "grad_norm": 16.583280563354492,
      "learning_rate": 1e-05,
      "loss": 7.2422,
      "step": 4728
    },
    {
      "epoch": 0.25626016260162604,
      "step": 4728,
      "training_loss": 7.041479587554932
    },
    {
      "epoch": 0.25631436314363143,
      "step": 4729,
      "training_loss": 7.053718090057373
    },
    {
      "epoch": 0.2563685636856369,
      "step": 4730,
      "training_loss": 6.89370584487915
    },
    {
      "epoch": 0.25642276422764226,
      "step": 4731,
      "training_loss": 7.866270542144775
    },
    {
      "epoch": 0.2564769647696477,
      "grad_norm": 30.00738525390625,
      "learning_rate": 1e-05,
      "loss": 7.2138,
      "step": 4732
    },
    {
      "epoch": 0.2564769647696477,
      "step": 4732,
      "training_loss": 4.692131519317627
    },
    {
      "epoch": 0.2565311653116531,
      "step": 4733,
      "training_loss": 8.504878997802734
    },
    {
      "epoch": 0.25658536585365854,
      "step": 4734,
      "training_loss": 8.180495262145996
    },
    {
      "epoch": 0.25663956639566393,
      "step": 4735,
      "training_loss": 6.914626598358154
    },
    {
      "epoch": 0.2566937669376694,
      "grad_norm": 23.290536880493164,
      "learning_rate": 1e-05,
      "loss": 7.073,
      "step": 4736
    },
    {
      "epoch": 0.2566937669376694,
      "step": 4736,
      "training_loss": 7.005627155303955
    },
    {
      "epoch": 0.2567479674796748,
      "step": 4737,
      "training_loss": 8.017702102661133
    },
    {
      "epoch": 0.2568021680216802,
      "step": 4738,
      "training_loss": 7.499058723449707
    },
    {
      "epoch": 0.25685636856368566,
      "step": 4739,
      "training_loss": 6.350644111633301
    },
    {
      "epoch": 0.25691056910569104,
      "grad_norm": 16.757465362548828,
      "learning_rate": 1e-05,
      "loss": 7.2183,
      "step": 4740
    },
    {
      "epoch": 0.25691056910569104,
      "step": 4740,
      "training_loss": 6.397661209106445
    },
    {
      "epoch": 0.2569647696476965,
      "step": 4741,
      "training_loss": 6.939577579498291
    },
    {
      "epoch": 0.2570189701897019,
      "step": 4742,
      "training_loss": 6.013177394866943
    },
    {
      "epoch": 0.2570731707317073,
      "step": 4743,
      "training_loss": 5.072497844696045
    },
    {
      "epoch": 0.2571273712737127,
      "grad_norm": 16.83380699157715,
      "learning_rate": 1e-05,
      "loss": 6.1057,
      "step": 4744
    },
    {
      "epoch": 0.2571273712737127,
      "step": 4744,
      "training_loss": 3.6848556995391846
    },
    {
      "epoch": 0.25718157181571816,
      "step": 4745,
      "training_loss": 5.314875602722168
    },
    {
      "epoch": 0.2572357723577236,
      "step": 4746,
      "training_loss": 7.135481357574463
    },
    {
      "epoch": 0.257289972899729,
      "step": 4747,
      "training_loss": 5.012190818786621
    },
    {
      "epoch": 0.25734417344173444,
      "grad_norm": 21.291763305664062,
      "learning_rate": 1e-05,
      "loss": 5.2869,
      "step": 4748
    },
    {
      "epoch": 0.25734417344173444,
      "step": 4748,
      "training_loss": 6.821893692016602
    },
    {
      "epoch": 0.2573983739837398,
      "step": 4749,
      "training_loss": 6.750768184661865
    },
    {
      "epoch": 0.25745257452574527,
      "step": 4750,
      "training_loss": 8.09833812713623
    },
    {
      "epoch": 0.25750677506775066,
      "step": 4751,
      "training_loss": 7.114133834838867
    },
    {
      "epoch": 0.2575609756097561,
      "grad_norm": 48.92202377319336,
      "learning_rate": 1e-05,
      "loss": 7.1963,
      "step": 4752
    },
    {
      "epoch": 0.2575609756097561,
      "step": 4752,
      "training_loss": 6.211601257324219
    },
    {
      "epoch": 0.2576151761517615,
      "step": 4753,
      "training_loss": 7.121237754821777
    },
    {
      "epoch": 0.25766937669376694,
      "step": 4754,
      "training_loss": 6.11627197265625
    },
    {
      "epoch": 0.2577235772357724,
      "step": 4755,
      "training_loss": 6.1874680519104
    },
    {
      "epoch": 0.2577777777777778,
      "grad_norm": 23.005651473999023,
      "learning_rate": 1e-05,
      "loss": 6.4091,
      "step": 4756
    },
    {
      "epoch": 0.2577777777777778,
      "step": 4756,
      "training_loss": 6.434803009033203
    },
    {
      "epoch": 0.2578319783197832,
      "step": 4757,
      "training_loss": 5.556206703186035
    },
    {
      "epoch": 0.2578861788617886,
      "step": 4758,
      "training_loss": 4.186215877532959
    },
    {
      "epoch": 0.25794037940379405,
      "step": 4759,
      "training_loss": 4.125746726989746
    },
    {
      "epoch": 0.25799457994579944,
      "grad_norm": 37.234100341796875,
      "learning_rate": 1e-05,
      "loss": 5.0757,
      "step": 4760
    },
    {
      "epoch": 0.25799457994579944,
      "step": 4760,
      "training_loss": 7.370724201202393
    },
    {
      "epoch": 0.2580487804878049,
      "step": 4761,
      "training_loss": 7.147902488708496
    },
    {
      "epoch": 0.2581029810298103,
      "step": 4762,
      "training_loss": 8.05123519897461
    },
    {
      "epoch": 0.2581571815718157,
      "step": 4763,
      "training_loss": 6.469763278961182
    },
    {
      "epoch": 0.25821138211382116,
      "grad_norm": 27.233915328979492,
      "learning_rate": 1e-05,
      "loss": 7.2599,
      "step": 4764
    },
    {
      "epoch": 0.25821138211382116,
      "step": 4764,
      "training_loss": 6.541208267211914
    },
    {
      "epoch": 0.25826558265582655,
      "step": 4765,
      "training_loss": 7.979613304138184
    },
    {
      "epoch": 0.258319783197832,
      "step": 4766,
      "training_loss": 8.0649995803833
    },
    {
      "epoch": 0.2583739837398374,
      "step": 4767,
      "training_loss": 7.176191329956055
    },
    {
      "epoch": 0.25842818428184283,
      "grad_norm": 29.848480224609375,
      "learning_rate": 1e-05,
      "loss": 7.4405,
      "step": 4768
    },
    {
      "epoch": 0.25842818428184283,
      "step": 4768,
      "training_loss": 6.6219587326049805
    },
    {
      "epoch": 0.2584823848238482,
      "step": 4769,
      "training_loss": 7.738934516906738
    },
    {
      "epoch": 0.25853658536585367,
      "step": 4770,
      "training_loss": 6.82429313659668
    },
    {
      "epoch": 0.25859078590785906,
      "step": 4771,
      "training_loss": 8.224771499633789
    },
    {
      "epoch": 0.2586449864498645,
      "grad_norm": 25.921215057373047,
      "learning_rate": 1e-05,
      "loss": 7.3525,
      "step": 4772
    },
    {
      "epoch": 0.2586449864498645,
      "step": 4772,
      "training_loss": 6.676982402801514
    },
    {
      "epoch": 0.25869918699186994,
      "step": 4773,
      "training_loss": 6.622319221496582
    },
    {
      "epoch": 0.25875338753387533,
      "step": 4774,
      "training_loss": 6.78183126449585
    },
    {
      "epoch": 0.2588075880758808,
      "step": 4775,
      "training_loss": 6.8583149909973145
    },
    {
      "epoch": 0.25886178861788617,
      "grad_norm": 24.91596031188965,
      "learning_rate": 1e-05,
      "loss": 6.7349,
      "step": 4776
    },
    {
      "epoch": 0.25886178861788617,
      "step": 4776,
      "training_loss": 8.347890853881836
    },
    {
      "epoch": 0.2589159891598916,
      "step": 4777,
      "training_loss": 5.515162467956543
    },
    {
      "epoch": 0.258970189701897,
      "step": 4778,
      "training_loss": 6.924659252166748
    },
    {
      "epoch": 0.25902439024390245,
      "step": 4779,
      "training_loss": 6.916841506958008
    },
    {
      "epoch": 0.25907859078590784,
      "grad_norm": 15.96618366241455,
      "learning_rate": 1e-05,
      "loss": 6.9261,
      "step": 4780
    },
    {
      "epoch": 0.25907859078590784,
      "step": 4780,
      "training_loss": 6.75818395614624
    },
    {
      "epoch": 0.2591327913279133,
      "step": 4781,
      "training_loss": 7.585190773010254
    },
    {
      "epoch": 0.2591869918699187,
      "step": 4782,
      "training_loss": 6.595373630523682
    },
    {
      "epoch": 0.2592411924119241,
      "step": 4783,
      "training_loss": 5.693598747253418
    },
    {
      "epoch": 0.25929539295392956,
      "grad_norm": 24.765138626098633,
      "learning_rate": 1e-05,
      "loss": 6.6581,
      "step": 4784
    },
    {
      "epoch": 0.25929539295392956,
      "step": 4784,
      "training_loss": 6.746213912963867
    },
    {
      "epoch": 0.25934959349593495,
      "step": 4785,
      "training_loss": 6.618460655212402
    },
    {
      "epoch": 0.2594037940379404,
      "step": 4786,
      "training_loss": 6.4064555168151855
    },
    {
      "epoch": 0.2594579945799458,
      "step": 4787,
      "training_loss": 7.501226425170898
    },
    {
      "epoch": 0.25951219512195123,
      "grad_norm": 22.372682571411133,
      "learning_rate": 1e-05,
      "loss": 6.8181,
      "step": 4788
    },
    {
      "epoch": 0.25951219512195123,
      "step": 4788,
      "training_loss": 6.6543779373168945
    },
    {
      "epoch": 0.2595663956639566,
      "step": 4789,
      "training_loss": 6.815804958343506
    },
    {
      "epoch": 0.25962059620596206,
      "step": 4790,
      "training_loss": 7.315384387969971
    },
    {
      "epoch": 0.2596747967479675,
      "step": 4791,
      "training_loss": 6.02483606338501
    },
    {
      "epoch": 0.2597289972899729,
      "grad_norm": 30.280776977539062,
      "learning_rate": 1e-05,
      "loss": 6.7026,
      "step": 4792
    },
    {
      "epoch": 0.2597289972899729,
      "step": 4792,
      "training_loss": 7.538172721862793
    },
    {
      "epoch": 0.25978319783197834,
      "step": 4793,
      "training_loss": 6.887235164642334
    },
    {
      "epoch": 0.25983739837398373,
      "step": 4794,
      "training_loss": 6.96810245513916
    },
    {
      "epoch": 0.2598915989159892,
      "step": 4795,
      "training_loss": 6.018955707550049
    },
    {
      "epoch": 0.25994579945799456,
      "grad_norm": 24.2818660736084,
      "learning_rate": 1e-05,
      "loss": 6.8531,
      "step": 4796
    },
    {
      "epoch": 0.25994579945799456,
      "step": 4796,
      "training_loss": 7.401679515838623
    },
    {
      "epoch": 0.26,
      "step": 4797,
      "training_loss": 7.00376558303833
    },
    {
      "epoch": 0.2600542005420054,
      "step": 4798,
      "training_loss": 7.407041072845459
    },
    {
      "epoch": 0.26010840108401084,
      "step": 4799,
      "training_loss": 5.800042629241943
    },
    {
      "epoch": 0.2601626016260163,
      "grad_norm": 29.239648818969727,
      "learning_rate": 1e-05,
      "loss": 6.9031,
      "step": 4800
    },
    {
      "epoch": 0.2601626016260163,
      "step": 4800,
      "training_loss": 7.03473424911499
    },
    {
      "epoch": 0.2602168021680217,
      "step": 4801,
      "training_loss": 7.587743759155273
    },
    {
      "epoch": 0.2602710027100271,
      "step": 4802,
      "training_loss": 6.96549654006958
    },
    {
      "epoch": 0.2603252032520325,
      "step": 4803,
      "training_loss": 7.239055633544922
    },
    {
      "epoch": 0.26037940379403796,
      "grad_norm": 19.655977249145508,
      "learning_rate": 1e-05,
      "loss": 7.2068,
      "step": 4804
    },
    {
      "epoch": 0.26037940379403796,
      "step": 4804,
      "training_loss": 6.653024196624756
    },
    {
      "epoch": 0.26043360433604335,
      "step": 4805,
      "training_loss": 5.323056221008301
    },
    {
      "epoch": 0.2604878048780488,
      "step": 4806,
      "training_loss": 6.282486438751221
    },
    {
      "epoch": 0.2605420054200542,
      "step": 4807,
      "training_loss": 7.995870113372803
    },
    {
      "epoch": 0.2605962059620596,
      "grad_norm": 55.86457443237305,
      "learning_rate": 1e-05,
      "loss": 6.5636,
      "step": 4808
    },
    {
      "epoch": 0.2605962059620596,
      "step": 4808,
      "training_loss": 7.903599739074707
    },
    {
      "epoch": 0.260650406504065,
      "step": 4809,
      "training_loss": 8.078314781188965
    },
    {
      "epoch": 0.26070460704607046,
      "step": 4810,
      "training_loss": 4.749776363372803
    },
    {
      "epoch": 0.2607588075880759,
      "step": 4811,
      "training_loss": 6.693337917327881
    },
    {
      "epoch": 0.2608130081300813,
      "grad_norm": 32.427337646484375,
      "learning_rate": 1e-05,
      "loss": 6.8563,
      "step": 4812
    },
    {
      "epoch": 0.2608130081300813,
      "step": 4812,
      "training_loss": 8.74120807647705
    },
    {
      "epoch": 0.26086720867208674,
      "step": 4813,
      "training_loss": 6.5647783279418945
    },
    {
      "epoch": 0.2609214092140921,
      "step": 4814,
      "training_loss": 7.683102607727051
    },
    {
      "epoch": 0.26097560975609757,
      "step": 4815,
      "training_loss": 6.456812381744385
    },
    {
      "epoch": 0.26102981029810296,
      "grad_norm": 24.26529884338379,
      "learning_rate": 1e-05,
      "loss": 7.3615,
      "step": 4816
    },
    {
      "epoch": 0.26102981029810296,
      "step": 4816,
      "training_loss": 6.7926177978515625
    },
    {
      "epoch": 0.2610840108401084,
      "step": 4817,
      "training_loss": 7.516434669494629
    },
    {
      "epoch": 0.2611382113821138,
      "step": 4818,
      "training_loss": 6.01669979095459
    },
    {
      "epoch": 0.26119241192411924,
      "step": 4819,
      "training_loss": 7.223236083984375
    },
    {
      "epoch": 0.2612466124661247,
      "grad_norm": 33.34225082397461,
      "learning_rate": 1e-05,
      "loss": 6.8872,
      "step": 4820
    },
    {
      "epoch": 0.2612466124661247,
      "step": 4820,
      "training_loss": 8.027338027954102
    },
    {
      "epoch": 0.2613008130081301,
      "step": 4821,
      "training_loss": 7.103915691375732
    },
    {
      "epoch": 0.2613550135501355,
      "step": 4822,
      "training_loss": 7.132882118225098
    },
    {
      "epoch": 0.2614092140921409,
      "step": 4823,
      "training_loss": 6.559634208679199
    },
    {
      "epoch": 0.26146341463414635,
      "grad_norm": 48.379241943359375,
      "learning_rate": 1e-05,
      "loss": 7.2059,
      "step": 4824
    },
    {
      "epoch": 0.26146341463414635,
      "step": 4824,
      "training_loss": 4.663775444030762
    },
    {
      "epoch": 0.26151761517615174,
      "step": 4825,
      "training_loss": 6.274765491485596
    },
    {
      "epoch": 0.2615718157181572,
      "step": 4826,
      "training_loss": 7.439170837402344
    },
    {
      "epoch": 0.2616260162601626,
      "step": 4827,
      "training_loss": 6.4873504638671875
    },
    {
      "epoch": 0.261680216802168,
      "grad_norm": 31.360361099243164,
      "learning_rate": 1e-05,
      "loss": 6.2163,
      "step": 4828
    },
    {
      "epoch": 0.261680216802168,
      "step": 4828,
      "training_loss": 6.543314456939697
    },
    {
      "epoch": 0.26173441734417346,
      "step": 4829,
      "training_loss": 4.955932140350342
    },
    {
      "epoch": 0.26178861788617885,
      "step": 4830,
      "training_loss": 6.999753475189209
    },
    {
      "epoch": 0.2618428184281843,
      "step": 4831,
      "training_loss": 7.164976119995117
    },
    {
      "epoch": 0.2618970189701897,
      "grad_norm": 19.549957275390625,
      "learning_rate": 1e-05,
      "loss": 6.416,
      "step": 4832
    },
    {
      "epoch": 0.2618970189701897,
      "step": 4832,
      "training_loss": 5.509737968444824
    },
    {
      "epoch": 0.26195121951219513,
      "step": 4833,
      "training_loss": 4.545966148376465
    },
    {
      "epoch": 0.2620054200542005,
      "step": 4834,
      "training_loss": 7.066369533538818
    },
    {
      "epoch": 0.26205962059620597,
      "step": 4835,
      "training_loss": 4.158344268798828
    },
    {
      "epoch": 0.26211382113821136,
      "grad_norm": 22.723669052124023,
      "learning_rate": 1e-05,
      "loss": 5.3201,
      "step": 4836
    },
    {
      "epoch": 0.26211382113821136,
      "step": 4836,
      "training_loss": 6.641505241394043
    },
    {
      "epoch": 0.2621680216802168,
      "step": 4837,
      "training_loss": 6.911890506744385
    },
    {
      "epoch": 0.26222222222222225,
      "step": 4838,
      "training_loss": 7.751863956451416
    },
    {
      "epoch": 0.26227642276422763,
      "step": 4839,
      "training_loss": 6.661981105804443
    },
    {
      "epoch": 0.2623306233062331,
      "grad_norm": 26.848857879638672,
      "learning_rate": 1e-05,
      "loss": 6.9918,
      "step": 4840
    },
    {
      "epoch": 0.2623306233062331,
      "step": 4840,
      "training_loss": 7.032006740570068
    },
    {
      "epoch": 0.26238482384823847,
      "step": 4841,
      "training_loss": 7.130910873413086
    },
    {
      "epoch": 0.2624390243902439,
      "step": 4842,
      "training_loss": 5.854705333709717
    },
    {
      "epoch": 0.2624932249322493,
      "step": 4843,
      "training_loss": 5.652467727661133
    },
    {
      "epoch": 0.26254742547425475,
      "grad_norm": 20.525835037231445,
      "learning_rate": 1e-05,
      "loss": 6.4175,
      "step": 4844
    },
    {
      "epoch": 0.26254742547425475,
      "step": 4844,
      "training_loss": 6.253573894500732
    },
    {
      "epoch": 0.26260162601626014,
      "step": 4845,
      "training_loss": 6.104823112487793
    },
    {
      "epoch": 0.2626558265582656,
      "step": 4846,
      "training_loss": 6.407424449920654
    },
    {
      "epoch": 0.262710027100271,
      "step": 4847,
      "training_loss": 6.159245491027832
    },
    {
      "epoch": 0.2627642276422764,
      "grad_norm": 32.34025955200195,
      "learning_rate": 1e-05,
      "loss": 6.2313,
      "step": 4848
    },
    {
      "epoch": 0.2627642276422764,
      "step": 4848,
      "training_loss": 6.343488693237305
    },
    {
      "epoch": 0.26281842818428186,
      "step": 4849,
      "training_loss": 3.463752269744873
    },
    {
      "epoch": 0.26287262872628725,
      "step": 4850,
      "training_loss": 7.3799662590026855
    },
    {
      "epoch": 0.2629268292682927,
      "step": 4851,
      "training_loss": 7.650152206420898
    },
    {
      "epoch": 0.2629810298102981,
      "grad_norm": 26.656349182128906,
      "learning_rate": 1e-05,
      "loss": 6.2093,
      "step": 4852
    },
    {
      "epoch": 0.2629810298102981,
      "step": 4852,
      "training_loss": 8.01840591430664
    },
    {
      "epoch": 0.26303523035230353,
      "step": 4853,
      "training_loss": 7.63856840133667
    },
    {
      "epoch": 0.2630894308943089,
      "step": 4854,
      "training_loss": 5.44691801071167
    },
    {
      "epoch": 0.26314363143631436,
      "step": 4855,
      "training_loss": 7.764342784881592
    },
    {
      "epoch": 0.2631978319783198,
      "grad_norm": 61.70840835571289,
      "learning_rate": 1e-05,
      "loss": 7.2171,
      "step": 4856
    },
    {
      "epoch": 0.2631978319783198,
      "step": 4856,
      "training_loss": 7.310907363891602
    },
    {
      "epoch": 0.2632520325203252,
      "step": 4857,
      "training_loss": 6.803649425506592
    },
    {
      "epoch": 0.26330623306233064,
      "step": 4858,
      "training_loss": 6.427469253540039
    },
    {
      "epoch": 0.26336043360433603,
      "step": 4859,
      "training_loss": 5.6680402755737305
    },
    {
      "epoch": 0.2634146341463415,
      "grad_norm": 29.596471786499023,
      "learning_rate": 1e-05,
      "loss": 6.5525,
      "step": 4860
    },
    {
      "epoch": 0.2634146341463415,
      "step": 4860,
      "training_loss": 6.297351837158203
    },
    {
      "epoch": 0.26346883468834686,
      "step": 4861,
      "training_loss": 6.702069282531738
    },
    {
      "epoch": 0.2635230352303523,
      "step": 4862,
      "training_loss": 6.743041515350342
    },
    {
      "epoch": 0.2635772357723577,
      "step": 4863,
      "training_loss": 5.9014506340026855
    },
    {
      "epoch": 0.26363143631436314,
      "grad_norm": 33.422752380371094,
      "learning_rate": 1e-05,
      "loss": 6.411,
      "step": 4864
    },
    {
      "epoch": 0.26363143631436314,
      "step": 4864,
      "training_loss": 6.9830169677734375
    },
    {
      "epoch": 0.2636856368563686,
      "step": 4865,
      "training_loss": 7.094808101654053
    },
    {
      "epoch": 0.263739837398374,
      "step": 4866,
      "training_loss": 6.969700336456299
    },
    {
      "epoch": 0.2637940379403794,
      "step": 4867,
      "training_loss": 7.193997859954834
    },
    {
      "epoch": 0.2638482384823848,
      "grad_norm": 25.80554962158203,
      "learning_rate": 1e-05,
      "loss": 7.0604,
      "step": 4868
    },
    {
      "epoch": 0.2638482384823848,
      "step": 4868,
      "training_loss": 7.023822784423828
    },
    {
      "epoch": 0.26390243902439026,
      "step": 4869,
      "training_loss": 6.904531478881836
    },
    {
      "epoch": 0.26395663956639565,
      "step": 4870,
      "training_loss": 6.532861232757568
    },
    {
      "epoch": 0.2640108401084011,
      "step": 4871,
      "training_loss": 7.518523693084717
    },
    {
      "epoch": 0.2640650406504065,
      "grad_norm": 16.098114013671875,
      "learning_rate": 1e-05,
      "loss": 6.9949,
      "step": 4872
    },
    {
      "epoch": 0.2640650406504065,
      "step": 4872,
      "training_loss": 5.525960445404053
    },
    {
      "epoch": 0.2641192411924119,
      "step": 4873,
      "training_loss": 6.4418745040893555
    },
    {
      "epoch": 0.26417344173441737,
      "step": 4874,
      "training_loss": 5.409494400024414
    },
    {
      "epoch": 0.26422764227642276,
      "step": 4875,
      "training_loss": 7.028253555297852
    },
    {
      "epoch": 0.2642818428184282,
      "grad_norm": 17.365798950195312,
      "learning_rate": 1e-05,
      "loss": 6.1014,
      "step": 4876
    },
    {
      "epoch": 0.2642818428184282,
      "step": 4876,
      "training_loss": 7.705293655395508
    },
    {
      "epoch": 0.2643360433604336,
      "step": 4877,
      "training_loss": 7.6135101318359375
    },
    {
      "epoch": 0.26439024390243904,
      "step": 4878,
      "training_loss": 6.538888454437256
    },
    {
      "epoch": 0.2644444444444444,
      "step": 4879,
      "training_loss": 6.8404860496521
    },
    {
      "epoch": 0.26449864498644987,
      "grad_norm": 22.497493743896484,
      "learning_rate": 1e-05,
      "loss": 7.1745,
      "step": 4880
    },
    {
      "epoch": 0.26449864498644987,
      "step": 4880,
      "training_loss": 6.473151683807373
    },
    {
      "epoch": 0.26455284552845526,
      "step": 4881,
      "training_loss": 7.773501873016357
    },
    {
      "epoch": 0.2646070460704607,
      "step": 4882,
      "training_loss": 7.117162227630615
    },
    {
      "epoch": 0.26466124661246615,
      "step": 4883,
      "training_loss": 4.842599391937256
    },
    {
      "epoch": 0.26471544715447154,
      "grad_norm": 20.025226593017578,
      "learning_rate": 1e-05,
      "loss": 6.5516,
      "step": 4884
    },
    {
      "epoch": 0.26471544715447154,
      "step": 4884,
      "training_loss": 8.028918266296387
    },
    {
      "epoch": 0.264769647696477,
      "step": 4885,
      "training_loss": 4.613664627075195
    },
    {
      "epoch": 0.2648238482384824,
      "step": 4886,
      "training_loss": 5.689857006072998
    },
    {
      "epoch": 0.2648780487804878,
      "step": 4887,
      "training_loss": 7.2086944580078125
    },
    {
      "epoch": 0.2649322493224932,
      "grad_norm": 16.11890983581543,
      "learning_rate": 1e-05,
      "loss": 6.3853,
      "step": 4888
    },
    {
      "epoch": 0.2649322493224932,
      "step": 4888,
      "training_loss": 6.974789142608643
    },
    {
      "epoch": 0.26498644986449865,
      "step": 4889,
      "training_loss": 6.038115501403809
    },
    {
      "epoch": 0.26504065040650404,
      "step": 4890,
      "training_loss": 7.399166107177734
    },
    {
      "epoch": 0.2650948509485095,
      "step": 4891,
      "training_loss": 9.039830207824707
    },
    {
      "epoch": 0.26514905149051493,
      "grad_norm": 28.489824295043945,
      "learning_rate": 1e-05,
      "loss": 7.363,
      "step": 4892
    },
    {
      "epoch": 0.26514905149051493,
      "step": 4892,
      "training_loss": 6.314671039581299
    },
    {
      "epoch": 0.2652032520325203,
      "step": 4893,
      "training_loss": 7.068612575531006
    },
    {
      "epoch": 0.26525745257452576,
      "step": 4894,
      "training_loss": 6.624389171600342
    },
    {
      "epoch": 0.26531165311653115,
      "step": 4895,
      "training_loss": 7.188756465911865
    },
    {
      "epoch": 0.2653658536585366,
      "grad_norm": 32.66558074951172,
      "learning_rate": 1e-05,
      "loss": 6.7991,
      "step": 4896
    },
    {
      "epoch": 0.2653658536585366,
      "step": 4896,
      "training_loss": 7.902585506439209
    },
    {
      "epoch": 0.265420054200542,
      "step": 4897,
      "training_loss": 7.601114273071289
    },
    {
      "epoch": 0.26547425474254743,
      "step": 4898,
      "training_loss": 6.281242847442627
    },
    {
      "epoch": 0.2655284552845528,
      "step": 4899,
      "training_loss": 6.994826316833496
    },
    {
      "epoch": 0.26558265582655827,
      "grad_norm": 24.744199752807617,
      "learning_rate": 1e-05,
      "loss": 7.1949,
      "step": 4900
    },
    {
      "epoch": 0.26558265582655827,
      "step": 4900,
      "training_loss": 4.899870872497559
    },
    {
      "epoch": 0.2656368563685637,
      "step": 4901,
      "training_loss": 5.6681294441223145
    },
    {
      "epoch": 0.2656910569105691,
      "step": 4902,
      "training_loss": 6.671302795410156
    },
    {
      "epoch": 0.26574525745257455,
      "step": 4903,
      "training_loss": 7.625035285949707
    },
    {
      "epoch": 0.26579945799457994,
      "grad_norm": 32.96500778198242,
      "learning_rate": 1e-05,
      "loss": 6.2161,
      "step": 4904
    },
    {
      "epoch": 0.26579945799457994,
      "step": 4904,
      "training_loss": 6.943455696105957
    },
    {
      "epoch": 0.2658536585365854,
      "step": 4905,
      "training_loss": 7.1414384841918945
    },
    {
      "epoch": 0.26590785907859077,
      "step": 4906,
      "training_loss": 6.6416425704956055
    },
    {
      "epoch": 0.2659620596205962,
      "step": 4907,
      "training_loss": 7.8988728523254395
    },
    {
      "epoch": 0.2660162601626016,
      "grad_norm": 33.226688385009766,
      "learning_rate": 1e-05,
      "loss": 7.1564,
      "step": 4908
    },
    {
      "epoch": 0.2660162601626016,
      "step": 4908,
      "training_loss": 6.752378463745117
    },
    {
      "epoch": 0.26607046070460705,
      "step": 4909,
      "training_loss": 6.614992618560791
    },
    {
      "epoch": 0.2661246612466125,
      "step": 4910,
      "training_loss": 7.080846786499023
    },
    {
      "epoch": 0.2661788617886179,
      "step": 4911,
      "training_loss": 6.1626763343811035
    },
    {
      "epoch": 0.2662330623306233,
      "grad_norm": 28.04447364807129,
      "learning_rate": 1e-05,
      "loss": 6.6527,
      "step": 4912
    },
    {
      "epoch": 0.2662330623306233,
      "step": 4912,
      "training_loss": 8.245169639587402
    },
    {
      "epoch": 0.2662872628726287,
      "step": 4913,
      "training_loss": 6.370792865753174
    },
    {
      "epoch": 0.26634146341463416,
      "step": 4914,
      "training_loss": 7.199748992919922
    },
    {
      "epoch": 0.26639566395663955,
      "step": 4915,
      "training_loss": 6.339925289154053
    },
    {
      "epoch": 0.266449864498645,
      "grad_norm": 22.966571807861328,
      "learning_rate": 1e-05,
      "loss": 7.0389,
      "step": 4916
    },
    {
      "epoch": 0.266449864498645,
      "step": 4916,
      "training_loss": 6.000926494598389
    },
    {
      "epoch": 0.2665040650406504,
      "step": 4917,
      "training_loss": 6.407894611358643
    },
    {
      "epoch": 0.26655826558265583,
      "step": 4918,
      "training_loss": 7.498404026031494
    },
    {
      "epoch": 0.2666124661246613,
      "step": 4919,
      "training_loss": 6.698840141296387
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 19.85947608947754,
      "learning_rate": 1e-05,
      "loss": 6.6515,
      "step": 4920
    },
    {
      "epoch": 0.26666666666666666,
      "step": 4920,
      "training_loss": 7.020698547363281
    },
    {
      "epoch": 0.2667208672086721,
      "step": 4921,
      "training_loss": 4.550604343414307
    },
    {
      "epoch": 0.2667750677506775,
      "step": 4922,
      "training_loss": 7.1332879066467285
    },
    {
      "epoch": 0.26682926829268294,
      "step": 4923,
      "training_loss": 6.6504740715026855
    },
    {
      "epoch": 0.26688346883468833,
      "grad_norm": 15.641388893127441,
      "learning_rate": 1e-05,
      "loss": 6.3388,
      "step": 4924
    },
    {
      "epoch": 0.26688346883468833,
      "step": 4924,
      "training_loss": 7.641758918762207
    },
    {
      "epoch": 0.2669376693766938,
      "step": 4925,
      "training_loss": 6.5288591384887695
    },
    {
      "epoch": 0.26699186991869917,
      "step": 4926,
      "training_loss": 6.759339809417725
    },
    {
      "epoch": 0.2670460704607046,
      "step": 4927,
      "training_loss": 6.272345066070557
    },
    {
      "epoch": 0.26710027100271005,
      "grad_norm": 24.616151809692383,
      "learning_rate": 1e-05,
      "loss": 6.8006,
      "step": 4928
    },
    {
      "epoch": 0.26710027100271005,
      "step": 4928,
      "training_loss": 7.08860445022583
    },
    {
      "epoch": 0.26715447154471544,
      "step": 4929,
      "training_loss": 7.004964828491211
    },
    {
      "epoch": 0.2672086720867209,
      "step": 4930,
      "training_loss": 4.900237560272217
    },
    {
      "epoch": 0.2672628726287263,
      "step": 4931,
      "training_loss": 6.592990875244141
    },
    {
      "epoch": 0.2673170731707317,
      "grad_norm": 21.048871994018555,
      "learning_rate": 1e-05,
      "loss": 6.3967,
      "step": 4932
    },
    {
      "epoch": 0.2673170731707317,
      "step": 4932,
      "training_loss": 8.386610984802246
    },
    {
      "epoch": 0.2673712737127371,
      "step": 4933,
      "training_loss": 7.453389644622803
    },
    {
      "epoch": 0.26742547425474256,
      "step": 4934,
      "training_loss": 6.80778694152832
    },
    {
      "epoch": 0.26747967479674795,
      "step": 4935,
      "training_loss": 5.854772567749023
    },
    {
      "epoch": 0.2675338753387534,
      "grad_norm": 24.39864730834961,
      "learning_rate": 1e-05,
      "loss": 7.1256,
      "step": 4936
    },
    {
      "epoch": 0.2675338753387534,
      "step": 4936,
      "training_loss": 5.845946788787842
    },
    {
      "epoch": 0.2675880758807588,
      "step": 4937,
      "training_loss": 5.502752304077148
    },
    {
      "epoch": 0.2676422764227642,
      "step": 4938,
      "training_loss": 7.993241786956787
    },
    {
      "epoch": 0.26769647696476967,
      "step": 4939,
      "training_loss": 7.720244407653809
    },
    {
      "epoch": 0.26775067750677506,
      "grad_norm": 22.063528060913086,
      "learning_rate": 1e-05,
      "loss": 6.7655,
      "step": 4940
    },
    {
      "epoch": 0.26775067750677506,
      "step": 4940,
      "training_loss": 8.788993835449219
    },
    {
      "epoch": 0.2678048780487805,
      "step": 4941,
      "training_loss": 6.705447673797607
    },
    {
      "epoch": 0.2678590785907859,
      "step": 4942,
      "training_loss": 6.750504970550537
    },
    {
      "epoch": 0.26791327913279134,
      "step": 4943,
      "training_loss": 7.156973361968994
    },
    {
      "epoch": 0.2679674796747967,
      "grad_norm": 25.818771362304688,
      "learning_rate": 1e-05,
      "loss": 7.3505,
      "step": 4944
    },
    {
      "epoch": 0.2679674796747967,
      "step": 4944,
      "training_loss": 6.248327732086182
    },
    {
      "epoch": 0.26802168021680217,
      "step": 4945,
      "training_loss": 4.493154048919678
    },
    {
      "epoch": 0.26807588075880756,
      "step": 4946,
      "training_loss": 7.335882663726807
    },
    {
      "epoch": 0.268130081300813,
      "step": 4947,
      "training_loss": 7.410265922546387
    },
    {
      "epoch": 0.26818428184281845,
      "grad_norm": 20.019559860229492,
      "learning_rate": 1e-05,
      "loss": 6.3719,
      "step": 4948
    },
    {
      "epoch": 0.26818428184281845,
      "step": 4948,
      "training_loss": 7.330765724182129
    },
    {
      "epoch": 0.26823848238482384,
      "step": 4949,
      "training_loss": 7.232378005981445
    },
    {
      "epoch": 0.2682926829268293,
      "step": 4950,
      "training_loss": 6.155293941497803
    },
    {
      "epoch": 0.2683468834688347,
      "step": 4951,
      "training_loss": 6.044593334197998
    },
    {
      "epoch": 0.2684010840108401,
      "grad_norm": 20.529251098632812,
      "learning_rate": 1e-05,
      "loss": 6.6908,
      "step": 4952
    },
    {
      "epoch": 0.2684010840108401,
      "step": 4952,
      "training_loss": 6.961282730102539
    },
    {
      "epoch": 0.2684552845528455,
      "step": 4953,
      "training_loss": 6.80707311630249
    },
    {
      "epoch": 0.26850948509485095,
      "step": 4954,
      "training_loss": 7.294464588165283
    },
    {
      "epoch": 0.26856368563685634,
      "step": 4955,
      "training_loss": 7.190250396728516
    },
    {
      "epoch": 0.2686178861788618,
      "grad_norm": 17.028221130371094,
      "learning_rate": 1e-05,
      "loss": 7.0633,
      "step": 4956
    },
    {
      "epoch": 0.2686178861788618,
      "step": 4956,
      "training_loss": 6.37131404876709
    },
    {
      "epoch": 0.26867208672086723,
      "step": 4957,
      "training_loss": 6.238094806671143
    },
    {
      "epoch": 0.2687262872628726,
      "step": 4958,
      "training_loss": 6.145900249481201
    },
    {
      "epoch": 0.26878048780487807,
      "step": 4959,
      "training_loss": 6.130329132080078
    },
    {
      "epoch": 0.26883468834688345,
      "grad_norm": 25.210399627685547,
      "learning_rate": 1e-05,
      "loss": 6.2214,
      "step": 4960
    },
    {
      "epoch": 0.26883468834688345,
      "step": 4960,
      "training_loss": 5.908989429473877
    },
    {
      "epoch": 0.2688888888888889,
      "step": 4961,
      "training_loss": 6.660285472869873
    },
    {
      "epoch": 0.2689430894308943,
      "step": 4962,
      "training_loss": 6.9891791343688965
    },
    {
      "epoch": 0.26899728997289973,
      "step": 4963,
      "training_loss": 5.959968566894531
    },
    {
      "epoch": 0.2690514905149051,
      "grad_norm": 24.21146011352539,
      "learning_rate": 1e-05,
      "loss": 6.3796,
      "step": 4964
    },
    {
      "epoch": 0.2690514905149051,
      "step": 4964,
      "training_loss": 6.107871055603027
    },
    {
      "epoch": 0.26910569105691057,
      "step": 4965,
      "training_loss": 7.278170585632324
    },
    {
      "epoch": 0.269159891598916,
      "step": 4966,
      "training_loss": 7.867782115936279
    },
    {
      "epoch": 0.2692140921409214,
      "step": 4967,
      "training_loss": 7.0565667152404785
    },
    {
      "epoch": 0.26926829268292685,
      "grad_norm": 17.299789428710938,
      "learning_rate": 1e-05,
      "loss": 7.0776,
      "step": 4968
    },
    {
      "epoch": 0.26926829268292685,
      "step": 4968,
      "training_loss": 6.16890811920166
    },
    {
      "epoch": 0.26932249322493224,
      "step": 4969,
      "training_loss": 7.701040267944336
    },
    {
      "epoch": 0.2693766937669377,
      "step": 4970,
      "training_loss": 4.338235855102539
    },
    {
      "epoch": 0.26943089430894307,
      "step": 4971,
      "training_loss": 6.553873062133789
    },
    {
      "epoch": 0.2694850948509485,
      "grad_norm": 23.4880428314209,
      "learning_rate": 1e-05,
      "loss": 6.1905,
      "step": 4972
    },
    {
      "epoch": 0.2694850948509485,
      "step": 4972,
      "training_loss": 5.885667324066162
    },
    {
      "epoch": 0.2695392953929539,
      "step": 4973,
      "training_loss": 6.927097797393799
    },
    {
      "epoch": 0.26959349593495935,
      "step": 4974,
      "training_loss": 6.96165132522583
    },
    {
      "epoch": 0.2696476964769648,
      "step": 4975,
      "training_loss": 4.967837333679199
    },
    {
      "epoch": 0.2697018970189702,
      "grad_norm": 21.30640411376953,
      "learning_rate": 1e-05,
      "loss": 6.1856,
      "step": 4976
    },
    {
      "epoch": 0.2697018970189702,
      "step": 4976,
      "training_loss": 6.866994857788086
    },
    {
      "epoch": 0.2697560975609756,
      "step": 4977,
      "training_loss": 6.377378463745117
    },
    {
      "epoch": 0.269810298102981,
      "step": 4978,
      "training_loss": 5.97785758972168
    },
    {
      "epoch": 0.26986449864498646,
      "step": 4979,
      "training_loss": 6.238738059997559
    },
    {
      "epoch": 0.26991869918699185,
      "grad_norm": 25.346500396728516,
      "learning_rate": 1e-05,
      "loss": 6.3652,
      "step": 4980
    },
    {
      "epoch": 0.26991869918699185,
      "step": 4980,
      "training_loss": 6.406407833099365
    },
    {
      "epoch": 0.2699728997289973,
      "step": 4981,
      "training_loss": 6.7348456382751465
    },
    {
      "epoch": 0.2700271002710027,
      "step": 4982,
      "training_loss": 5.688357353210449
    },
    {
      "epoch": 0.27008130081300813,
      "step": 4983,
      "training_loss": 6.648710250854492
    },
    {
      "epoch": 0.2701355013550136,
      "grad_norm": 55.215389251708984,
      "learning_rate": 1e-05,
      "loss": 6.3696,
      "step": 4984
    },
    {
      "epoch": 0.2701355013550136,
      "step": 4984,
      "training_loss": 8.202692985534668
    },
    {
      "epoch": 0.27018970189701896,
      "step": 4985,
      "training_loss": 7.671238899230957
    },
    {
      "epoch": 0.2702439024390244,
      "step": 4986,
      "training_loss": 7.342266082763672
    },
    {
      "epoch": 0.2702981029810298,
      "step": 4987,
      "training_loss": 7.1196393966674805
    },
    {
      "epoch": 0.27035230352303524,
      "grad_norm": 18.4271183013916,
      "learning_rate": 1e-05,
      "loss": 7.584,
      "step": 4988
    },
    {
      "epoch": 0.27035230352303524,
      "step": 4988,
      "training_loss": 5.983510494232178
    },
    {
      "epoch": 0.27040650406504063,
      "step": 4989,
      "training_loss": 4.742553234100342
    },
    {
      "epoch": 0.2704607046070461,
      "step": 4990,
      "training_loss": 7.378482341766357
    },
    {
      "epoch": 0.27051490514905147,
      "step": 4991,
      "training_loss": 5.532063007354736
    },
    {
      "epoch": 0.2705691056910569,
      "grad_norm": 33.3089485168457,
      "learning_rate": 1e-05,
      "loss": 5.9092,
      "step": 4992
    },
    {
      "epoch": 0.2705691056910569,
      "step": 4992,
      "training_loss": 6.453804016113281
    },
    {
      "epoch": 0.27062330623306236,
      "step": 4993,
      "training_loss": 6.779522895812988
    },
    {
      "epoch": 0.27067750677506774,
      "step": 4994,
      "training_loss": 6.931865692138672
    },
    {
      "epoch": 0.2707317073170732,
      "step": 4995,
      "training_loss": 6.8208909034729
    },
    {
      "epoch": 0.2707859078590786,
      "grad_norm": 28.626035690307617,
      "learning_rate": 1e-05,
      "loss": 6.7465,
      "step": 4996
    },
    {
      "epoch": 0.2707859078590786,
      "step": 4996,
      "training_loss": 7.275331974029541
    },
    {
      "epoch": 0.270840108401084,
      "step": 4997,
      "training_loss": 6.914759159088135
    },
    {
      "epoch": 0.2708943089430894,
      "step": 4998,
      "training_loss": 6.469493865966797
    },
    {
      "epoch": 0.27094850948509486,
      "step": 4999,
      "training_loss": 6.281454086303711
    },
    {
      "epoch": 0.27100271002710025,
      "grad_norm": 20.635196685791016,
      "learning_rate": 1e-05,
      "loss": 6.7353,
      "step": 5000
    },
    {
      "epoch": 0.27100271002710025,
      "step": 5000,
      "training_loss": 6.846871376037598
    },
    {
      "epoch": 0.2710569105691057,
      "step": 5001,
      "training_loss": 6.084092617034912
    },
    {
      "epoch": 0.27111111111111114,
      "step": 5002,
      "training_loss": 6.354703426361084
    },
    {
      "epoch": 0.2711653116531165,
      "step": 5003,
      "training_loss": 5.487571716308594
    },
    {
      "epoch": 0.27121951219512197,
      "grad_norm": 33.11050796508789,
      "learning_rate": 1e-05,
      "loss": 6.1933,
      "step": 5004
    },
    {
      "epoch": 0.27121951219512197,
      "step": 5004,
      "training_loss": 4.632855415344238
    },
    {
      "epoch": 0.27127371273712736,
      "step": 5005,
      "training_loss": 8.17302131652832
    },
    {
      "epoch": 0.2713279132791328,
      "step": 5006,
      "training_loss": 8.119634628295898
    },
    {
      "epoch": 0.2713821138211382,
      "step": 5007,
      "training_loss": 6.907395839691162
    },
    {
      "epoch": 0.27143631436314364,
      "grad_norm": 27.857711791992188,
      "learning_rate": 1e-05,
      "loss": 6.9582,
      "step": 5008
    },
    {
      "epoch": 0.27143631436314364,
      "step": 5008,
      "training_loss": 6.668277263641357
    },
    {
      "epoch": 0.271490514905149,
      "step": 5009,
      "training_loss": 6.824997901916504
    },
    {
      "epoch": 0.27154471544715447,
      "step": 5010,
      "training_loss": 6.480597496032715
    },
    {
      "epoch": 0.2715989159891599,
      "step": 5011,
      "training_loss": 6.35274600982666
    },
    {
      "epoch": 0.2716531165311653,
      "grad_norm": 29.897846221923828,
      "learning_rate": 1e-05,
      "loss": 6.5817,
      "step": 5012
    },
    {
      "epoch": 0.2716531165311653,
      "step": 5012,
      "training_loss": 5.531122207641602
    },
    {
      "epoch": 0.27170731707317075,
      "step": 5013,
      "training_loss": 6.113900184631348
    },
    {
      "epoch": 0.27176151761517614,
      "step": 5014,
      "training_loss": 6.961343765258789
    },
    {
      "epoch": 0.2718157181571816,
      "step": 5015,
      "training_loss": 7.088756084442139
    },
    {
      "epoch": 0.271869918699187,
      "grad_norm": 19.20054817199707,
      "learning_rate": 1e-05,
      "loss": 6.4238,
      "step": 5016
    },
    {
      "epoch": 0.271869918699187,
      "step": 5016,
      "training_loss": 7.603376388549805
    },
    {
      "epoch": 0.2719241192411924,
      "step": 5017,
      "training_loss": 6.75981330871582
    },
    {
      "epoch": 0.2719783197831978,
      "step": 5018,
      "training_loss": 6.7920966148376465
    },
    {
      "epoch": 0.27203252032520325,
      "step": 5019,
      "training_loss": 9.877098083496094
    },
    {
      "epoch": 0.2720867208672087,
      "grad_norm": 60.20651626586914,
      "learning_rate": 1e-05,
      "loss": 7.7581,
      "step": 5020
    },
    {
      "epoch": 0.2720867208672087,
      "step": 5020,
      "training_loss": 6.8385491371154785
    },
    {
      "epoch": 0.2721409214092141,
      "step": 5021,
      "training_loss": 6.908149719238281
    },
    {
      "epoch": 0.27219512195121953,
      "step": 5022,
      "training_loss": 4.465498924255371
    },
    {
      "epoch": 0.2722493224932249,
      "step": 5023,
      "training_loss": 7.869126319885254
    },
    {
      "epoch": 0.27230352303523037,
      "grad_norm": 25.243654251098633,
      "learning_rate": 1e-05,
      "loss": 6.5203,
      "step": 5024
    },
    {
      "epoch": 0.27230352303523037,
      "step": 5024,
      "training_loss": 7.12803316116333
    },
    {
      "epoch": 0.27235772357723576,
      "step": 5025,
      "training_loss": 7.4212870597839355
    },
    {
      "epoch": 0.2724119241192412,
      "step": 5026,
      "training_loss": 6.751155376434326
    },
    {
      "epoch": 0.2724661246612466,
      "step": 5027,
      "training_loss": 7.263567924499512
    },
    {
      "epoch": 0.27252032520325203,
      "grad_norm": 20.02451515197754,
      "learning_rate": 1e-05,
      "loss": 7.141,
      "step": 5028
    },
    {
      "epoch": 0.27252032520325203,
      "step": 5028,
      "training_loss": 7.132732391357422
    },
    {
      "epoch": 0.2725745257452575,
      "step": 5029,
      "training_loss": 6.012722492218018
    },
    {
      "epoch": 0.27262872628726287,
      "step": 5030,
      "training_loss": 6.621293067932129
    },
    {
      "epoch": 0.2726829268292683,
      "step": 5031,
      "training_loss": 7.5128068923950195
    },
    {
      "epoch": 0.2727371273712737,
      "grad_norm": 23.665990829467773,
      "learning_rate": 1e-05,
      "loss": 6.8199,
      "step": 5032
    },
    {
      "epoch": 0.2727371273712737,
      "step": 5032,
      "training_loss": 5.978063583374023
    },
    {
      "epoch": 0.27279132791327915,
      "step": 5033,
      "training_loss": 6.502889633178711
    },
    {
      "epoch": 0.27284552845528454,
      "step": 5034,
      "training_loss": 7.972695350646973
    },
    {
      "epoch": 0.27289972899729,
      "step": 5035,
      "training_loss": 5.6975884437561035
    },
    {
      "epoch": 0.27295392953929537,
      "grad_norm": 26.227331161499023,
      "learning_rate": 1e-05,
      "loss": 6.5378,
      "step": 5036
    },
    {
      "epoch": 0.27295392953929537,
      "step": 5036,
      "training_loss": 4.315586090087891
    },
    {
      "epoch": 0.2730081300813008,
      "step": 5037,
      "training_loss": 7.259707450866699
    },
    {
      "epoch": 0.27306233062330626,
      "step": 5038,
      "training_loss": 6.864482879638672
    },
    {
      "epoch": 0.27311653116531165,
      "step": 5039,
      "training_loss": 6.551335334777832
    },
    {
      "epoch": 0.2731707317073171,
      "grad_norm": 27.97097396850586,
      "learning_rate": 1e-05,
      "loss": 6.2478,
      "step": 5040
    },
    {
      "epoch": 0.2731707317073171,
      "step": 5040,
      "training_loss": 6.701514720916748
    },
    {
      "epoch": 0.2732249322493225,
      "step": 5041,
      "training_loss": 7.216390132904053
    },
    {
      "epoch": 0.27327913279132793,
      "step": 5042,
      "training_loss": 8.100756645202637
    },
    {
      "epoch": 0.2733333333333333,
      "step": 5043,
      "training_loss": 7.402855396270752
    },
    {
      "epoch": 0.27338753387533876,
      "grad_norm": 24.500633239746094,
      "learning_rate": 1e-05,
      "loss": 7.3554,
      "step": 5044
    },
    {
      "epoch": 0.27338753387533876,
      "step": 5044,
      "training_loss": 6.1606245040893555
    },
    {
      "epoch": 0.27344173441734415,
      "step": 5045,
      "training_loss": 5.402135372161865
    },
    {
      "epoch": 0.2734959349593496,
      "step": 5046,
      "training_loss": 6.702467918395996
    },
    {
      "epoch": 0.27355013550135504,
      "step": 5047,
      "training_loss": 6.352453231811523
    },
    {
      "epoch": 0.27360433604336043,
      "grad_norm": 21.45200538635254,
      "learning_rate": 1e-05,
      "loss": 6.1544,
      "step": 5048
    },
    {
      "epoch": 0.27360433604336043,
      "step": 5048,
      "training_loss": 6.0108489990234375
    },
    {
      "epoch": 0.2736585365853659,
      "step": 5049,
      "training_loss": 6.343145370483398
    },
    {
      "epoch": 0.27371273712737126,
      "step": 5050,
      "training_loss": 6.253802299499512
    },
    {
      "epoch": 0.2737669376693767,
      "step": 5051,
      "training_loss": 8.04179859161377
    },
    {
      "epoch": 0.2738211382113821,
      "grad_norm": 24.116987228393555,
      "learning_rate": 1e-05,
      "loss": 6.6624,
      "step": 5052
    },
    {
      "epoch": 0.2738211382113821,
      "step": 5052,
      "training_loss": 3.90155029296875
    },
    {
      "epoch": 0.27387533875338754,
      "step": 5053,
      "training_loss": 7.348795413970947
    },
    {
      "epoch": 0.27392953929539293,
      "step": 5054,
      "training_loss": 7.3843865394592285
    },
    {
      "epoch": 0.2739837398373984,
      "step": 5055,
      "training_loss": 8.598566055297852
    },
    {
      "epoch": 0.2740379403794038,
      "grad_norm": 27.270095825195312,
      "learning_rate": 1e-05,
      "loss": 6.8083,
      "step": 5056
    },
    {
      "epoch": 0.2740379403794038,
      "step": 5056,
      "training_loss": 7.274750709533691
    },
    {
      "epoch": 0.2740921409214092,
      "step": 5057,
      "training_loss": 5.564451694488525
    },
    {
      "epoch": 0.27414634146341466,
      "step": 5058,
      "training_loss": 6.425874710083008
    },
    {
      "epoch": 0.27420054200542004,
      "step": 5059,
      "training_loss": 8.992061614990234
    },
    {
      "epoch": 0.2742547425474255,
      "grad_norm": 42.347007751464844,
      "learning_rate": 1e-05,
      "loss": 7.0643,
      "step": 5060
    },
    {
      "epoch": 0.2742547425474255,
      "step": 5060,
      "training_loss": 7.033367156982422
    },
    {
      "epoch": 0.2743089430894309,
      "step": 5061,
      "training_loss": 6.919116020202637
    },
    {
      "epoch": 0.2743631436314363,
      "step": 5062,
      "training_loss": 7.683764934539795
    },
    {
      "epoch": 0.2744173441734417,
      "step": 5063,
      "training_loss": 6.566936492919922
    },
    {
      "epoch": 0.27447154471544716,
      "grad_norm": 34.346927642822266,
      "learning_rate": 1e-05,
      "loss": 7.0508,
      "step": 5064
    },
    {
      "epoch": 0.27447154471544716,
      "step": 5064,
      "training_loss": 6.86555814743042
    },
    {
      "epoch": 0.27452574525745255,
      "step": 5065,
      "training_loss": 7.374061584472656
    },
    {
      "epoch": 0.274579945799458,
      "step": 5066,
      "training_loss": 8.585213661193848
    },
    {
      "epoch": 0.27463414634146344,
      "step": 5067,
      "training_loss": 6.180380344390869
    },
    {
      "epoch": 0.2746883468834688,
      "grad_norm": 25.922157287597656,
      "learning_rate": 1e-05,
      "loss": 7.2513,
      "step": 5068
    },
    {
      "epoch": 0.2746883468834688,
      "step": 5068,
      "training_loss": 7.020946025848389
    },
    {
      "epoch": 0.27474254742547427,
      "step": 5069,
      "training_loss": 6.561461448669434
    },
    {
      "epoch": 0.27479674796747966,
      "step": 5070,
      "training_loss": 6.494003772735596
    },
    {
      "epoch": 0.2748509485094851,
      "step": 5071,
      "training_loss": 8.637481689453125
    },
    {
      "epoch": 0.2749051490514905,
      "grad_norm": 27.33583641052246,
      "learning_rate": 1e-05,
      "loss": 7.1785,
      "step": 5072
    },
    {
      "epoch": 0.2749051490514905,
      "step": 5072,
      "training_loss": 7.536584377288818
    },
    {
      "epoch": 0.27495934959349594,
      "step": 5073,
      "training_loss": 7.6446852684021
    },
    {
      "epoch": 0.27501355013550133,
      "step": 5074,
      "training_loss": 8.392212867736816
    },
    {
      "epoch": 0.2750677506775068,
      "step": 5075,
      "training_loss": 6.30781888961792
    },
    {
      "epoch": 0.2751219512195122,
      "grad_norm": 24.775413513183594,
      "learning_rate": 1e-05,
      "loss": 7.4703,
      "step": 5076
    },
    {
      "epoch": 0.2751219512195122,
      "step": 5076,
      "training_loss": 3.8457186222076416
    },
    {
      "epoch": 0.2751761517615176,
      "step": 5077,
      "training_loss": 6.927586555480957
    },
    {
      "epoch": 0.27523035230352305,
      "step": 5078,
      "training_loss": 5.750773906707764
    },
    {
      "epoch": 0.27528455284552844,
      "step": 5079,
      "training_loss": 6.9924211502075195
    },
    {
      "epoch": 0.2753387533875339,
      "grad_norm": 23.354942321777344,
      "learning_rate": 1e-05,
      "loss": 5.8791,
      "step": 5080
    },
    {
      "epoch": 0.2753387533875339,
      "step": 5080,
      "training_loss": 7.685932636260986
    },
    {
      "epoch": 0.2753929539295393,
      "step": 5081,
      "training_loss": 7.648141860961914
    },
    {
      "epoch": 0.2754471544715447,
      "step": 5082,
      "training_loss": 6.837948322296143
    },
    {
      "epoch": 0.2755013550135501,
      "step": 5083,
      "training_loss": 5.5679612159729
    },
    {
      "epoch": 0.27555555555555555,
      "grad_norm": 24.68473243713379,
      "learning_rate": 1e-05,
      "loss": 6.935,
      "step": 5084
    },
    {
      "epoch": 0.27555555555555555,
      "step": 5084,
      "training_loss": 7.605480194091797
    },
    {
      "epoch": 0.275609756097561,
      "step": 5085,
      "training_loss": 7.336971759796143
    },
    {
      "epoch": 0.2756639566395664,
      "step": 5086,
      "training_loss": 6.379064083099365
    },
    {
      "epoch": 0.27571815718157183,
      "step": 5087,
      "training_loss": 6.675513744354248
    },
    {
      "epoch": 0.2757723577235772,
      "grad_norm": 24.13417625427246,
      "learning_rate": 1e-05,
      "loss": 6.9993,
      "step": 5088
    },
    {
      "epoch": 0.2757723577235772,
      "step": 5088,
      "training_loss": 6.234110355377197
    },
    {
      "epoch": 0.27582655826558267,
      "step": 5089,
      "training_loss": 6.761823654174805
    },
    {
      "epoch": 0.27588075880758806,
      "step": 5090,
      "training_loss": 7.025148391723633
    },
    {
      "epoch": 0.2759349593495935,
      "step": 5091,
      "training_loss": 7.738255977630615
    },
    {
      "epoch": 0.2759891598915989,
      "grad_norm": 30.339004516601562,
      "learning_rate": 1e-05,
      "loss": 6.9398,
      "step": 5092
    },
    {
      "epoch": 0.2759891598915989,
      "step": 5092,
      "training_loss": 7.103665828704834
    },
    {
      "epoch": 0.27604336043360433,
      "step": 5093,
      "training_loss": 5.269614219665527
    },
    {
      "epoch": 0.2760975609756098,
      "step": 5094,
      "training_loss": 7.804317474365234
    },
    {
      "epoch": 0.27615176151761517,
      "step": 5095,
      "training_loss": 6.4863176345825195
    },
    {
      "epoch": 0.2762059620596206,
      "grad_norm": 27.882822036743164,
      "learning_rate": 1e-05,
      "loss": 6.666,
      "step": 5096
    },
    {
      "epoch": 0.2762059620596206,
      "step": 5096,
      "training_loss": 5.433704853057861
    },
    {
      "epoch": 0.276260162601626,
      "step": 5097,
      "training_loss": 6.801220417022705
    },
    {
      "epoch": 0.27631436314363145,
      "step": 5098,
      "training_loss": 7.806066036224365
    },
    {
      "epoch": 0.27636856368563684,
      "step": 5099,
      "training_loss": 4.798552989959717
    },
    {
      "epoch": 0.2764227642276423,
      "grad_norm": 42.81672668457031,
      "learning_rate": 1e-05,
      "loss": 6.2099,
      "step": 5100
    },
    {
      "epoch": 0.2764227642276423,
      "step": 5100,
      "training_loss": 5.2487263679504395
    },
    {
      "epoch": 0.27647696476964767,
      "step": 5101,
      "training_loss": 7.74701452255249
    },
    {
      "epoch": 0.2765311653116531,
      "step": 5102,
      "training_loss": 6.846710681915283
    },
    {
      "epoch": 0.27658536585365856,
      "step": 5103,
      "training_loss": 4.7576189041137695
    },
    {
      "epoch": 0.27663956639566395,
      "grad_norm": 22.564355850219727,
      "learning_rate": 1e-05,
      "loss": 6.15,
      "step": 5104
    },
    {
      "epoch": 0.27663956639566395,
      "step": 5104,
      "training_loss": 4.055253982543945
    },
    {
      "epoch": 0.2766937669376694,
      "step": 5105,
      "training_loss": 7.487155437469482
    },
    {
      "epoch": 0.2767479674796748,
      "step": 5106,
      "training_loss": 8.462461471557617
    },
    {
      "epoch": 0.27680216802168023,
      "step": 5107,
      "training_loss": 6.205909252166748
    },
    {
      "epoch": 0.2768563685636856,
      "grad_norm": 22.684247970581055,
      "learning_rate": 1e-05,
      "loss": 6.5527,
      "step": 5108
    },
    {
      "epoch": 0.2768563685636856,
      "step": 5108,
      "training_loss": 6.215158939361572
    },
    {
      "epoch": 0.27691056910569106,
      "step": 5109,
      "training_loss": 7.340853691101074
    },
    {
      "epoch": 0.27696476964769645,
      "step": 5110,
      "training_loss": 7.656935691833496
    },
    {
      "epoch": 0.2770189701897019,
      "step": 5111,
      "training_loss": 6.677061557769775
    },
    {
      "epoch": 0.27707317073170734,
      "grad_norm": 16.870849609375,
      "learning_rate": 1e-05,
      "loss": 6.9725,
      "step": 5112
    },
    {
      "epoch": 0.27707317073170734,
      "step": 5112,
      "training_loss": 7.626512050628662
    },
    {
      "epoch": 0.27712737127371273,
      "step": 5113,
      "training_loss": 4.525638103485107
    },
    {
      "epoch": 0.2771815718157182,
      "step": 5114,
      "training_loss": 6.450981140136719
    },
    {
      "epoch": 0.27723577235772356,
      "step": 5115,
      "training_loss": 7.713808059692383
    },
    {
      "epoch": 0.277289972899729,
      "grad_norm": 21.2799129486084,
      "learning_rate": 1e-05,
      "loss": 6.5792,
      "step": 5116
    },
    {
      "epoch": 0.277289972899729,
      "step": 5116,
      "training_loss": 7.57472562789917
    },
    {
      "epoch": 0.2773441734417344,
      "step": 5117,
      "training_loss": 7.269898891448975
    },
    {
      "epoch": 0.27739837398373984,
      "step": 5118,
      "training_loss": 7.2742390632629395
    },
    {
      "epoch": 0.27745257452574523,
      "step": 5119,
      "training_loss": 5.78804349899292
    },
    {
      "epoch": 0.2775067750677507,
      "grad_norm": 27.47576141357422,
      "learning_rate": 1e-05,
      "loss": 6.9767,
      "step": 5120
    },
    {
      "epoch": 0.2775067750677507,
      "step": 5120,
      "training_loss": 3.971553325653076
    },
    {
      "epoch": 0.2775609756097561,
      "step": 5121,
      "training_loss": 7.555501461029053
    },
    {
      "epoch": 0.2776151761517615,
      "step": 5122,
      "training_loss": 7.695818901062012
    },
    {
      "epoch": 0.27766937669376696,
      "step": 5123,
      "training_loss": 7.948122501373291
    },
    {
      "epoch": 0.27772357723577235,
      "grad_norm": 37.73394012451172,
      "learning_rate": 1e-05,
      "loss": 6.7927,
      "step": 5124
    },
    {
      "epoch": 0.27772357723577235,
      "step": 5124,
      "training_loss": 6.556992530822754
    },
    {
      "epoch": 0.2777777777777778,
      "step": 5125,
      "training_loss": 6.840548038482666
    },
    {
      "epoch": 0.2778319783197832,
      "step": 5126,
      "training_loss": 6.6039042472839355
    },
    {
      "epoch": 0.2778861788617886,
      "step": 5127,
      "training_loss": 6.574808120727539
    },
    {
      "epoch": 0.277940379403794,
      "grad_norm": 21.144800186157227,
      "learning_rate": 1e-05,
      "loss": 6.6441,
      "step": 5128
    },
    {
      "epoch": 0.277940379403794,
      "step": 5128,
      "training_loss": 6.023215293884277
    },
    {
      "epoch": 0.27799457994579946,
      "step": 5129,
      "training_loss": 6.987311363220215
    },
    {
      "epoch": 0.2780487804878049,
      "step": 5130,
      "training_loss": 7.173766613006592
    },
    {
      "epoch": 0.2781029810298103,
      "step": 5131,
      "training_loss": 6.386744499206543
    },
    {
      "epoch": 0.27815718157181574,
      "grad_norm": 21.99808692932129,
      "learning_rate": 1e-05,
      "loss": 6.6428,
      "step": 5132
    },
    {
      "epoch": 0.27815718157181574,
      "step": 5132,
      "training_loss": 3.740746259689331
    },
    {
      "epoch": 0.2782113821138211,
      "step": 5133,
      "training_loss": 6.378673076629639
    },
    {
      "epoch": 0.27826558265582657,
      "step": 5134,
      "training_loss": 7.0471954345703125
    },
    {
      "epoch": 0.27831978319783196,
      "step": 5135,
      "training_loss": 4.465422630310059
    },
    {
      "epoch": 0.2783739837398374,
      "grad_norm": 28.879209518432617,
      "learning_rate": 1e-05,
      "loss": 5.408,
      "step": 5136
    },
    {
      "epoch": 0.2783739837398374,
      "step": 5136,
      "training_loss": 7.169620990753174
    },
    {
      "epoch": 0.2784281842818428,
      "step": 5137,
      "training_loss": 6.835855960845947
    },
    {
      "epoch": 0.27848238482384824,
      "step": 5138,
      "training_loss": 6.894830226898193
    },
    {
      "epoch": 0.2785365853658537,
      "step": 5139,
      "training_loss": 6.159998893737793
    },
    {
      "epoch": 0.2785907859078591,
      "grad_norm": 24.268579483032227,
      "learning_rate": 1e-05,
      "loss": 6.7651,
      "step": 5140
    },
    {
      "epoch": 0.2785907859078591,
      "step": 5140,
      "training_loss": 5.784419059753418
    },
    {
      "epoch": 0.2786449864498645,
      "step": 5141,
      "training_loss": 7.549221515655518
    },
    {
      "epoch": 0.2786991869918699,
      "step": 5142,
      "training_loss": 8.798530578613281
    },
    {
      "epoch": 0.27875338753387535,
      "step": 5143,
      "training_loss": 6.7353291511535645
    },
    {
      "epoch": 0.27880758807588074,
      "grad_norm": 20.075687408447266,
      "learning_rate": 1e-05,
      "loss": 7.2169,
      "step": 5144
    },
    {
      "epoch": 0.27880758807588074,
      "step": 5144,
      "training_loss": 7.566021919250488
    },
    {
      "epoch": 0.2788617886178862,
      "step": 5145,
      "training_loss": 7.280628204345703
    },
    {
      "epoch": 0.2789159891598916,
      "step": 5146,
      "training_loss": 6.950271129608154
    },
    {
      "epoch": 0.278970189701897,
      "step": 5147,
      "training_loss": 5.411248683929443
    },
    {
      "epoch": 0.27902439024390246,
      "grad_norm": 21.45005989074707,
      "learning_rate": 1e-05,
      "loss": 6.802,
      "step": 5148
    },
    {
      "epoch": 0.27902439024390246,
      "step": 5148,
      "training_loss": 6.991097450256348
    },
    {
      "epoch": 0.27907859078590785,
      "step": 5149,
      "training_loss": 7.274267673492432
    },
    {
      "epoch": 0.2791327913279133,
      "step": 5150,
      "training_loss": 6.727305889129639
    },
    {
      "epoch": 0.2791869918699187,
      "step": 5151,
      "training_loss": 5.354216575622559
    },
    {
      "epoch": 0.27924119241192413,
      "grad_norm": 22.670394897460938,
      "learning_rate": 1e-05,
      "loss": 6.5867,
      "step": 5152
    },
    {
      "epoch": 0.27924119241192413,
      "step": 5152,
      "training_loss": 6.7358832359313965
    },
    {
      "epoch": 0.2792953929539295,
      "step": 5153,
      "training_loss": 6.445342540740967
    },
    {
      "epoch": 0.27934959349593497,
      "step": 5154,
      "training_loss": 7.842922687530518
    },
    {
      "epoch": 0.27940379403794036,
      "step": 5155,
      "training_loss": 7.114840984344482
    },
    {
      "epoch": 0.2794579945799458,
      "grad_norm": 17.341154098510742,
      "learning_rate": 1e-05,
      "loss": 7.0347,
      "step": 5156
    },
    {
      "epoch": 0.2794579945799458,
      "step": 5156,
      "training_loss": 7.133351802825928
    },
    {
      "epoch": 0.27951219512195125,
      "step": 5157,
      "training_loss": 5.844870090484619
    },
    {
      "epoch": 0.27956639566395663,
      "step": 5158,
      "training_loss": 7.485314846038818
    },
    {
      "epoch": 0.2796205962059621,
      "step": 5159,
      "training_loss": 6.714995384216309
    },
    {
      "epoch": 0.27967479674796747,
      "grad_norm": 16.67828941345215,
      "learning_rate": 1e-05,
      "loss": 6.7946,
      "step": 5160
    },
    {
      "epoch": 0.27967479674796747,
      "step": 5160,
      "training_loss": 6.3726372718811035
    },
    {
      "epoch": 0.2797289972899729,
      "step": 5161,
      "training_loss": 7.45034646987915
    },
    {
      "epoch": 0.2797831978319783,
      "step": 5162,
      "training_loss": 7.450637340545654
    },
    {
      "epoch": 0.27983739837398375,
      "step": 5163,
      "training_loss": 7.76308536529541
    },
    {
      "epoch": 0.27989159891598914,
      "grad_norm": 19.63082504272461,
      "learning_rate": 1e-05,
      "loss": 7.2592,
      "step": 5164
    },
    {
      "epoch": 0.27989159891598914,
      "step": 5164,
      "training_loss": 5.989553928375244
    },
    {
      "epoch": 0.2799457994579946,
      "step": 5165,
      "training_loss": 7.993453502655029
    },
    {
      "epoch": 0.28,
      "step": 5166,
      "training_loss": 7.123623371124268
    },
    {
      "epoch": 0.2800542005420054,
      "step": 5167,
      "training_loss": 7.363927364349365
    },
    {
      "epoch": 0.28010840108401086,
      "grad_norm": 41.5241813659668,
      "learning_rate": 1e-05,
      "loss": 7.1176,
      "step": 5168
    },
    {
      "epoch": 0.28010840108401086,
      "step": 5168,
      "training_loss": 6.673237323760986
    },
    {
      "epoch": 0.28016260162601625,
      "step": 5169,
      "training_loss": 7.022278785705566
    },
    {
      "epoch": 0.2802168021680217,
      "step": 5170,
      "training_loss": 5.6270599365234375
    },
    {
      "epoch": 0.2802710027100271,
      "step": 5171,
      "training_loss": 6.243344783782959
    },
    {
      "epoch": 0.28032520325203253,
      "grad_norm": 32.25327682495117,
      "learning_rate": 1e-05,
      "loss": 6.3915,
      "step": 5172
    },
    {
      "epoch": 0.28032520325203253,
      "step": 5172,
      "training_loss": 6.562553882598877
    },
    {
      "epoch": 0.2803794037940379,
      "step": 5173,
      "training_loss": 6.111568927764893
    },
    {
      "epoch": 0.28043360433604336,
      "step": 5174,
      "training_loss": 7.07467794418335
    },
    {
      "epoch": 0.2804878048780488,
      "step": 5175,
      "training_loss": 6.762960433959961
    },
    {
      "epoch": 0.2805420054200542,
      "grad_norm": 33.26948165893555,
      "learning_rate": 1e-05,
      "loss": 6.6279,
      "step": 5176
    },
    {
      "epoch": 0.2805420054200542,
      "step": 5176,
      "training_loss": 5.911686897277832
    },
    {
      "epoch": 0.28059620596205964,
      "step": 5177,
      "training_loss": 7.487447738647461
    },
    {
      "epoch": 0.28065040650406503,
      "step": 5178,
      "training_loss": 6.62183952331543
    },
    {
      "epoch": 0.2807046070460705,
      "step": 5179,
      "training_loss": 7.121008396148682
    },
    {
      "epoch": 0.28075880758807586,
      "grad_norm": 37.367462158203125,
      "learning_rate": 1e-05,
      "loss": 6.7855,
      "step": 5180
    },
    {
      "epoch": 0.28075880758807586,
      "step": 5180,
      "training_loss": 7.644127368927002
    },
    {
      "epoch": 0.2808130081300813,
      "step": 5181,
      "training_loss": 7.107113838195801
    },
    {
      "epoch": 0.2808672086720867,
      "step": 5182,
      "training_loss": 6.588601589202881
    },
    {
      "epoch": 0.28092140921409214,
      "step": 5183,
      "training_loss": 6.829239368438721
    },
    {
      "epoch": 0.2809756097560976,
      "grad_norm": 37.35757064819336,
      "learning_rate": 1e-05,
      "loss": 7.0423,
      "step": 5184
    },
    {
      "epoch": 0.2809756097560976,
      "step": 5184,
      "training_loss": 4.624415397644043
    },
    {
      "epoch": 0.281029810298103,
      "step": 5185,
      "training_loss": 6.896310806274414
    },
    {
      "epoch": 0.2810840108401084,
      "step": 5186,
      "training_loss": 7.160922527313232
    },
    {
      "epoch": 0.2811382113821138,
      "step": 5187,
      "training_loss": 5.826164245605469
    },
    {
      "epoch": 0.28119241192411926,
      "grad_norm": 22.8209228515625,
      "learning_rate": 1e-05,
      "loss": 6.127,
      "step": 5188
    },
    {
      "epoch": 0.28119241192411926,
      "step": 5188,
      "training_loss": 7.945351600646973
    },
    {
      "epoch": 0.28124661246612465,
      "step": 5189,
      "training_loss": 5.796289443969727
    },
    {
      "epoch": 0.2813008130081301,
      "step": 5190,
      "training_loss": 6.830871105194092
    },
    {
      "epoch": 0.2813550135501355,
      "step": 5191,
      "training_loss": 7.005458354949951
    },
    {
      "epoch": 0.2814092140921409,
      "grad_norm": 18.31974220275879,
      "learning_rate": 1e-05,
      "loss": 6.8945,
      "step": 5192
    },
    {
      "epoch": 0.2814092140921409,
      "step": 5192,
      "training_loss": 7.952748775482178
    },
    {
      "epoch": 0.2814634146341463,
      "step": 5193,
      "training_loss": 7.311100482940674
    },
    {
      "epoch": 0.28151761517615176,
      "step": 5194,
      "training_loss": 6.510183334350586
    },
    {
      "epoch": 0.2815718157181572,
      "step": 5195,
      "training_loss": 7.906668663024902
    },
    {
      "epoch": 0.2816260162601626,
      "grad_norm": 77.68608856201172,
      "learning_rate": 1e-05,
      "loss": 7.4202,
      "step": 5196
    },
    {
      "epoch": 0.2816260162601626,
      "step": 5196,
      "training_loss": 7.115444183349609
    },
    {
      "epoch": 0.28168021680216804,
      "step": 5197,
      "training_loss": 6.917321681976318
    },
    {
      "epoch": 0.2817344173441734,
      "step": 5198,
      "training_loss": 6.136656761169434
    },
    {
      "epoch": 0.28178861788617887,
      "step": 5199,
      "training_loss": 7.709949493408203
    },
    {
      "epoch": 0.28184281842818426,
      "grad_norm": 22.127769470214844,
      "learning_rate": 1e-05,
      "loss": 6.9698,
      "step": 5200
    },
    {
      "epoch": 0.28184281842818426,
      "step": 5200,
      "training_loss": 5.764556884765625
    },
    {
      "epoch": 0.2818970189701897,
      "step": 5201,
      "training_loss": 7.877197265625
    },
    {
      "epoch": 0.2819512195121951,
      "step": 5202,
      "training_loss": 6.7823805809021
    },
    {
      "epoch": 0.28200542005420054,
      "step": 5203,
      "training_loss": 8.165361404418945
    },
    {
      "epoch": 0.282059620596206,
      "grad_norm": 25.326120376586914,
      "learning_rate": 1e-05,
      "loss": 7.1474,
      "step": 5204
    },
    {
      "epoch": 0.282059620596206,
      "step": 5204,
      "training_loss": 6.388025760650635
    },
    {
      "epoch": 0.2821138211382114,
      "step": 5205,
      "training_loss": 6.351504325866699
    },
    {
      "epoch": 0.2821680216802168,
      "step": 5206,
      "training_loss": 7.861251354217529
    },
    {
      "epoch": 0.2822222222222222,
      "step": 5207,
      "training_loss": 7.390162944793701
    },
    {
      "epoch": 0.28227642276422765,
      "grad_norm": 16.987268447875977,
      "learning_rate": 1e-05,
      "loss": 6.9977,
      "step": 5208
    },
    {
      "epoch": 0.28227642276422765,
      "step": 5208,
      "training_loss": 7.283344268798828
    },
    {
      "epoch": 0.28233062330623304,
      "step": 5209,
      "training_loss": 7.125881671905518
    },
    {
      "epoch": 0.2823848238482385,
      "step": 5210,
      "training_loss": 7.1070075035095215
    },
    {
      "epoch": 0.2824390243902439,
      "step": 5211,
      "training_loss": 7.021738052368164
    },
    {
      "epoch": 0.2824932249322493,
      "grad_norm": 27.06585121154785,
      "learning_rate": 1e-05,
      "loss": 7.1345,
      "step": 5212
    },
    {
      "epoch": 0.2824932249322493,
      "step": 5212,
      "training_loss": 6.187038421630859
    },
    {
      "epoch": 0.28254742547425477,
      "step": 5213,
      "training_loss": 6.556807994842529
    },
    {
      "epoch": 0.28260162601626015,
      "step": 5214,
      "training_loss": 7.579919338226318
    },
    {
      "epoch": 0.2826558265582656,
      "step": 5215,
      "training_loss": 7.545155048370361
    },
    {
      "epoch": 0.282710027100271,
      "grad_norm": 31.178661346435547,
      "learning_rate": 1e-05,
      "loss": 6.9672,
      "step": 5216
    },
    {
      "epoch": 0.282710027100271,
      "step": 5216,
      "training_loss": 7.667713642120361
    },
    {
      "epoch": 0.28276422764227643,
      "step": 5217,
      "training_loss": 7.4069037437438965
    },
    {
      "epoch": 0.2828184281842818,
      "step": 5218,
      "training_loss": 4.635534763336182
    },
    {
      "epoch": 0.28287262872628727,
      "step": 5219,
      "training_loss": 5.5687665939331055
    },
    {
      "epoch": 0.28292682926829266,
      "grad_norm": 23.188932418823242,
      "learning_rate": 1e-05,
      "loss": 6.3197,
      "step": 5220
    },
    {
      "epoch": 0.28292682926829266,
      "step": 5220,
      "training_loss": 4.1709675788879395
    },
    {
      "epoch": 0.2829810298102981,
      "step": 5221,
      "training_loss": 6.814461708068848
    },
    {
      "epoch": 0.28303523035230355,
      "step": 5222,
      "training_loss": 7.102078437805176
    },
    {
      "epoch": 0.28308943089430894,
      "step": 5223,
      "training_loss": 7.401483058929443
    },
    {
      "epoch": 0.2831436314363144,
      "grad_norm": 31.887855529785156,
      "learning_rate": 1e-05,
      "loss": 6.3722,
      "step": 5224
    },
    {
      "epoch": 0.2831436314363144,
      "step": 5224,
      "training_loss": 7.350093364715576
    },
    {
      "epoch": 0.28319783197831977,
      "step": 5225,
      "training_loss": 6.1855034828186035
    },
    {
      "epoch": 0.2832520325203252,
      "step": 5226,
      "training_loss": 7.006816864013672
    },
    {
      "epoch": 0.2833062330623306,
      "step": 5227,
      "training_loss": 6.887613773345947
    },
    {
      "epoch": 0.28336043360433605,
      "grad_norm": 42.62064743041992,
      "learning_rate": 1e-05,
      "loss": 6.8575,
      "step": 5228
    },
    {
      "epoch": 0.28336043360433605,
      "step": 5228,
      "training_loss": 6.0656843185424805
    },
    {
      "epoch": 0.28341463414634144,
      "step": 5229,
      "training_loss": 7.7234110832214355
    },
    {
      "epoch": 0.2834688346883469,
      "step": 5230,
      "training_loss": 6.922615051269531
    },
    {
      "epoch": 0.2835230352303523,
      "step": 5231,
      "training_loss": 6.5160908699035645
    },
    {
      "epoch": 0.2835772357723577,
      "grad_norm": 17.85529136657715,
      "learning_rate": 1e-05,
      "loss": 6.807,
      "step": 5232
    },
    {
      "epoch": 0.2835772357723577,
      "step": 5232,
      "training_loss": 6.273229598999023
    },
    {
      "epoch": 0.28363143631436316,
      "step": 5233,
      "training_loss": 6.816527366638184
    },
    {
      "epoch": 0.28368563685636855,
      "step": 5234,
      "training_loss": 8.570627212524414
    },
    {
      "epoch": 0.283739837398374,
      "step": 5235,
      "training_loss": 6.764116287231445
    },
    {
      "epoch": 0.2837940379403794,
      "grad_norm": 21.80604362487793,
      "learning_rate": 1e-05,
      "loss": 7.1061,
      "step": 5236
    },
    {
      "epoch": 0.2837940379403794,
      "step": 5236,
      "training_loss": 6.635782241821289
    },
    {
      "epoch": 0.28384823848238483,
      "step": 5237,
      "training_loss": 6.256566524505615
    },
    {
      "epoch": 0.2839024390243902,
      "step": 5238,
      "training_loss": 5.38894510269165
    },
    {
      "epoch": 0.28395663956639566,
      "step": 5239,
      "training_loss": 7.5237908363342285
    },
    {
      "epoch": 0.2840108401084011,
      "grad_norm": 22.22565269470215,
      "learning_rate": 1e-05,
      "loss": 6.4513,
      "step": 5240
    },
    {
      "epoch": 0.2840108401084011,
      "step": 5240,
      "training_loss": 6.651532173156738
    },
    {
      "epoch": 0.2840650406504065,
      "step": 5241,
      "training_loss": 6.7688117027282715
    },
    {
      "epoch": 0.28411924119241194,
      "step": 5242,
      "training_loss": 7.012316703796387
    },
    {
      "epoch": 0.28417344173441733,
      "step": 5243,
      "training_loss": 7.42957878112793
    },
    {
      "epoch": 0.2842276422764228,
      "grad_norm": 18.078596115112305,
      "learning_rate": 1e-05,
      "loss": 6.9656,
      "step": 5244
    },
    {
      "epoch": 0.2842276422764228,
      "step": 5244,
      "training_loss": 7.1429762840271
    },
    {
      "epoch": 0.28428184281842817,
      "step": 5245,
      "training_loss": 6.842650413513184
    },
    {
      "epoch": 0.2843360433604336,
      "step": 5246,
      "training_loss": 7.20407772064209
    },
    {
      "epoch": 0.284390243902439,
      "step": 5247,
      "training_loss": 6.572466850280762
    },
    {
      "epoch": 0.28444444444444444,
      "grad_norm": 25.562299728393555,
      "learning_rate": 1e-05,
      "loss": 6.9405,
      "step": 5248
    },
    {
      "epoch": 0.28444444444444444,
      "step": 5248,
      "training_loss": 6.074676990509033
    },
    {
      "epoch": 0.2844986449864499,
      "step": 5249,
      "training_loss": 7.043507099151611
    },
    {
      "epoch": 0.2845528455284553,
      "step": 5250,
      "training_loss": 6.2238335609436035
    },
    {
      "epoch": 0.2846070460704607,
      "step": 5251,
      "training_loss": 6.5402727127075195
    },
    {
      "epoch": 0.2846612466124661,
      "grad_norm": 23.977296829223633,
      "learning_rate": 1e-05,
      "loss": 6.4706,
      "step": 5252
    },
    {
      "epoch": 0.2846612466124661,
      "step": 5252,
      "training_loss": 6.156836032867432
    },
    {
      "epoch": 0.28471544715447156,
      "step": 5253,
      "training_loss": 6.554589748382568
    },
    {
      "epoch": 0.28476964769647695,
      "step": 5254,
      "training_loss": 5.548050403594971
    },
    {
      "epoch": 0.2848238482384824,
      "step": 5255,
      "training_loss": 8.805357933044434
    },
    {
      "epoch": 0.2848780487804878,
      "grad_norm": 30.57151222229004,
      "learning_rate": 1e-05,
      "loss": 6.7662,
      "step": 5256
    },
    {
      "epoch": 0.2848780487804878,
      "step": 5256,
      "training_loss": 6.236405372619629
    },
    {
      "epoch": 0.2849322493224932,
      "step": 5257,
      "training_loss": 6.987600326538086
    },
    {
      "epoch": 0.28498644986449867,
      "step": 5258,
      "training_loss": 5.1495513916015625
    },
    {
      "epoch": 0.28504065040650406,
      "step": 5259,
      "training_loss": 7.092639446258545
    },
    {
      "epoch": 0.2850948509485095,
      "grad_norm": 36.871376037597656,
      "learning_rate": 1e-05,
      "loss": 6.3665,
      "step": 5260
    },
    {
      "epoch": 0.2850948509485095,
      "step": 5260,
      "training_loss": 8.083032608032227
    },
    {
      "epoch": 0.2851490514905149,
      "step": 5261,
      "training_loss": 4.392541885375977
    },
    {
      "epoch": 0.28520325203252034,
      "step": 5262,
      "training_loss": 7.419931888580322
    },
    {
      "epoch": 0.2852574525745257,
      "step": 5263,
      "training_loss": 5.5116682052612305
    },
    {
      "epoch": 0.28531165311653117,
      "grad_norm": 23.41350555419922,
      "learning_rate": 1e-05,
      "loss": 6.3518,
      "step": 5264
    },
    {
      "epoch": 0.28531165311653117,
      "step": 5264,
      "training_loss": 7.548313140869141
    },
    {
      "epoch": 0.28536585365853656,
      "step": 5265,
      "training_loss": 6.520968914031982
    },
    {
      "epoch": 0.285420054200542,
      "step": 5266,
      "training_loss": 6.9198174476623535
    },
    {
      "epoch": 0.28547425474254745,
      "step": 5267,
      "training_loss": 6.782238006591797
    },
    {
      "epoch": 0.28552845528455284,
      "grad_norm": 17.492534637451172,
      "learning_rate": 1e-05,
      "loss": 6.9428,
      "step": 5268
    },
    {
      "epoch": 0.28552845528455284,
      "step": 5268,
      "training_loss": 7.270510196685791
    },
    {
      "epoch": 0.2855826558265583,
      "step": 5269,
      "training_loss": 7.786079406738281
    },
    {
      "epoch": 0.2856368563685637,
      "step": 5270,
      "training_loss": 7.101654052734375
    },
    {
      "epoch": 0.2856910569105691,
      "step": 5271,
      "training_loss": 7.642735004425049
    },
    {
      "epoch": 0.2857452574525745,
      "grad_norm": 24.60482406616211,
      "learning_rate": 1e-05,
      "loss": 7.4502,
      "step": 5272
    },
    {
      "epoch": 0.2857452574525745,
      "step": 5272,
      "training_loss": 6.340524196624756
    },
    {
      "epoch": 0.28579945799457995,
      "step": 5273,
      "training_loss": 7.765787124633789
    },
    {
      "epoch": 0.28585365853658534,
      "step": 5274,
      "training_loss": 8.387922286987305
    },
    {
      "epoch": 0.2859078590785908,
      "step": 5275,
      "training_loss": 6.0428853034973145
    },
    {
      "epoch": 0.28596205962059623,
      "grad_norm": 19.092060089111328,
      "learning_rate": 1e-05,
      "loss": 7.1343,
      "step": 5276
    },
    {
      "epoch": 0.28596205962059623,
      "step": 5276,
      "training_loss": 7.413949966430664
    },
    {
      "epoch": 0.2860162601626016,
      "step": 5277,
      "training_loss": 6.7883172035217285
    },
    {
      "epoch": 0.28607046070460707,
      "step": 5278,
      "training_loss": 5.374650955200195
    },
    {
      "epoch": 0.28612466124661246,
      "step": 5279,
      "training_loss": 7.0377421379089355
    },
    {
      "epoch": 0.2861788617886179,
      "grad_norm": 20.909955978393555,
      "learning_rate": 1e-05,
      "loss": 6.6537,
      "step": 5280
    },
    {
      "epoch": 0.2861788617886179,
      "step": 5280,
      "training_loss": 7.689935684204102
    },
    {
      "epoch": 0.2862330623306233,
      "step": 5281,
      "training_loss": 6.01671838760376
    },
    {
      "epoch": 0.28628726287262873,
      "step": 5282,
      "training_loss": 7.1751227378845215
    },
    {
      "epoch": 0.2863414634146341,
      "step": 5283,
      "training_loss": 7.382654190063477
    },
    {
      "epoch": 0.28639566395663957,
      "grad_norm": 27.168909072875977,
      "learning_rate": 1e-05,
      "loss": 7.0661,
      "step": 5284
    },
    {
      "epoch": 0.28639566395663957,
      "step": 5284,
      "training_loss": 6.783395767211914
    },
    {
      "epoch": 0.286449864498645,
      "step": 5285,
      "training_loss": 6.651967525482178
    },
    {
      "epoch": 0.2865040650406504,
      "step": 5286,
      "training_loss": 6.406911373138428
    },
    {
      "epoch": 0.28655826558265585,
      "step": 5287,
      "training_loss": 6.967129230499268
    },
    {
      "epoch": 0.28661246612466124,
      "grad_norm": 19.180513381958008,
      "learning_rate": 1e-05,
      "loss": 6.7024,
      "step": 5288
    },
    {
      "epoch": 0.28661246612466124,
      "step": 5288,
      "training_loss": 6.628426551818848
    },
    {
      "epoch": 0.2866666666666667,
      "step": 5289,
      "training_loss": 6.829556465148926
    },
    {
      "epoch": 0.28672086720867207,
      "step": 5290,
      "training_loss": 9.073548316955566
    },
    {
      "epoch": 0.2867750677506775,
      "step": 5291,
      "training_loss": 6.309055328369141
    },
    {
      "epoch": 0.2868292682926829,
      "grad_norm": 21.336000442504883,
      "learning_rate": 1e-05,
      "loss": 7.2101,
      "step": 5292
    },
    {
      "epoch": 0.2868292682926829,
      "step": 5292,
      "training_loss": 7.1395792961120605
    },
    {
      "epoch": 0.28688346883468835,
      "step": 5293,
      "training_loss": 7.018993377685547
    },
    {
      "epoch": 0.2869376693766938,
      "step": 5294,
      "training_loss": 7.349099159240723
    },
    {
      "epoch": 0.2869918699186992,
      "step": 5295,
      "training_loss": 7.6808366775512695
    },
    {
      "epoch": 0.2870460704607046,
      "grad_norm": 27.94015884399414,
      "learning_rate": 1e-05,
      "loss": 7.2971,
      "step": 5296
    },
    {
      "epoch": 0.2870460704607046,
      "step": 5296,
      "training_loss": 6.661665916442871
    },
    {
      "epoch": 0.28710027100271,
      "step": 5297,
      "training_loss": 3.640577793121338
    },
    {
      "epoch": 0.28715447154471546,
      "step": 5298,
      "training_loss": 7.739356517791748
    },
    {
      "epoch": 0.28720867208672085,
      "step": 5299,
      "training_loss": 6.204237937927246
    },
    {
      "epoch": 0.2872628726287263,
      "grad_norm": 48.14756393432617,
      "learning_rate": 1e-05,
      "loss": 6.0615,
      "step": 5300
    },
    {
      "epoch": 0.2872628726287263,
      "step": 5300,
      "training_loss": 6.85186243057251
    },
    {
      "epoch": 0.2873170731707317,
      "step": 5301,
      "training_loss": 7.671169757843018
    },
    {
      "epoch": 0.28737127371273713,
      "step": 5302,
      "training_loss": 8.179996490478516
    },
    {
      "epoch": 0.2874254742547426,
      "step": 5303,
      "training_loss": 6.823071002960205
    },
    {
      "epoch": 0.28747967479674796,
      "grad_norm": 40.712867736816406,
      "learning_rate": 1e-05,
      "loss": 7.3815,
      "step": 5304
    },
    {
      "epoch": 0.28747967479674796,
      "step": 5304,
      "training_loss": 6.155437469482422
    },
    {
      "epoch": 0.2875338753387534,
      "step": 5305,
      "training_loss": 6.137989044189453
    },
    {
      "epoch": 0.2875880758807588,
      "step": 5306,
      "training_loss": 7.15587854385376
    },
    {
      "epoch": 0.28764227642276424,
      "step": 5307,
      "training_loss": 8.414701461791992
    },
    {
      "epoch": 0.28769647696476963,
      "grad_norm": 78.17243194580078,
      "learning_rate": 1e-05,
      "loss": 6.966,
      "step": 5308
    },
    {
      "epoch": 0.28769647696476963,
      "step": 5308,
      "training_loss": 6.297510147094727
    },
    {
      "epoch": 0.2877506775067751,
      "step": 5309,
      "training_loss": 6.956943988800049
    },
    {
      "epoch": 0.28780487804878047,
      "step": 5310,
      "training_loss": 7.547102928161621
    },
    {
      "epoch": 0.2878590785907859,
      "step": 5311,
      "training_loss": 3.8531954288482666
    },
    {
      "epoch": 0.28791327913279136,
      "grad_norm": 26.05545997619629,
      "learning_rate": 1e-05,
      "loss": 6.1637,
      "step": 5312
    },
    {
      "epoch": 0.28791327913279136,
      "step": 5312,
      "training_loss": 7.009917259216309
    },
    {
      "epoch": 0.28796747967479674,
      "step": 5313,
      "training_loss": 6.706622123718262
    },
    {
      "epoch": 0.2880216802168022,
      "step": 5314,
      "training_loss": 6.684818744659424
    },
    {
      "epoch": 0.2880758807588076,
      "step": 5315,
      "training_loss": 5.638955116271973
    },
    {
      "epoch": 0.288130081300813,
      "grad_norm": 21.3287353515625,
      "learning_rate": 1e-05,
      "loss": 6.5101,
      "step": 5316
    },
    {
      "epoch": 0.288130081300813,
      "step": 5316,
      "training_loss": 7.250980377197266
    },
    {
      "epoch": 0.2881842818428184,
      "step": 5317,
      "training_loss": 6.224137783050537
    },
    {
      "epoch": 0.28823848238482386,
      "step": 5318,
      "training_loss": 6.468808650970459
    },
    {
      "epoch": 0.28829268292682925,
      "step": 5319,
      "training_loss": 6.701232433319092
    },
    {
      "epoch": 0.2883468834688347,
      "grad_norm": 18.970199584960938,
      "learning_rate": 1e-05,
      "loss": 6.6613,
      "step": 5320
    },
    {
      "epoch": 0.2883468834688347,
      "step": 5320,
      "training_loss": 6.83026123046875
    },
    {
      "epoch": 0.2884010840108401,
      "step": 5321,
      "training_loss": 6.868050575256348
    },
    {
      "epoch": 0.2884552845528455,
      "step": 5322,
      "training_loss": 7.054967403411865
    },
    {
      "epoch": 0.28850948509485097,
      "step": 5323,
      "training_loss": 7.402952194213867
    },
    {
      "epoch": 0.28856368563685636,
      "grad_norm": 22.500350952148438,
      "learning_rate": 1e-05,
      "loss": 7.0391,
      "step": 5324
    },
    {
      "epoch": 0.28856368563685636,
      "step": 5324,
      "training_loss": 7.40770959854126
    },
    {
      "epoch": 0.2886178861788618,
      "step": 5325,
      "training_loss": 7.596284866333008
    },
    {
      "epoch": 0.2886720867208672,
      "step": 5326,
      "training_loss": 4.44111967086792
    },
    {
      "epoch": 0.28872628726287264,
      "step": 5327,
      "training_loss": 7.2621846199035645
    },
    {
      "epoch": 0.288780487804878,
      "grad_norm": 27.319278717041016,
      "learning_rate": 1e-05,
      "loss": 6.6768,
      "step": 5328
    },
    {
      "epoch": 0.288780487804878,
      "step": 5328,
      "training_loss": 6.463940620422363
    },
    {
      "epoch": 0.2888346883468835,
      "step": 5329,
      "training_loss": 8.231742858886719
    },
    {
      "epoch": 0.28888888888888886,
      "step": 5330,
      "training_loss": 7.187008857727051
    },
    {
      "epoch": 0.2889430894308943,
      "step": 5331,
      "training_loss": 6.863292217254639
    },
    {
      "epoch": 0.28899728997289975,
      "grad_norm": 17.677885055541992,
      "learning_rate": 1e-05,
      "loss": 7.1865,
      "step": 5332
    },
    {
      "epoch": 0.28899728997289975,
      "step": 5332,
      "training_loss": 6.664202690124512
    },
    {
      "epoch": 0.28905149051490514,
      "step": 5333,
      "training_loss": 6.469142436981201
    },
    {
      "epoch": 0.2891056910569106,
      "step": 5334,
      "training_loss": 6.693777561187744
    },
    {
      "epoch": 0.289159891598916,
      "step": 5335,
      "training_loss": 7.494684219360352
    },
    {
      "epoch": 0.2892140921409214,
      "grad_norm": 20.342342376708984,
      "learning_rate": 1e-05,
      "loss": 6.8305,
      "step": 5336
    },
    {
      "epoch": 0.2892140921409214,
      "step": 5336,
      "training_loss": 7.786673545837402
    },
    {
      "epoch": 0.2892682926829268,
      "step": 5337,
      "training_loss": 4.48774528503418
    },
    {
      "epoch": 0.28932249322493225,
      "step": 5338,
      "training_loss": 7.496335983276367
    },
    {
      "epoch": 0.28937669376693764,
      "step": 5339,
      "training_loss": 6.379673480987549
    },
    {
      "epoch": 0.2894308943089431,
      "grad_norm": 41.221282958984375,
      "learning_rate": 1e-05,
      "loss": 6.5376,
      "step": 5340
    },
    {
      "epoch": 0.2894308943089431,
      "step": 5340,
      "training_loss": 6.394215106964111
    },
    {
      "epoch": 0.28948509485094853,
      "step": 5341,
      "training_loss": 7.209780693054199
    },
    {
      "epoch": 0.2895392953929539,
      "step": 5342,
      "training_loss": 6.103243350982666
    },
    {
      "epoch": 0.28959349593495937,
      "step": 5343,
      "training_loss": 5.598082542419434
    },
    {
      "epoch": 0.28964769647696476,
      "grad_norm": 37.81312561035156,
      "learning_rate": 1e-05,
      "loss": 6.3263,
      "step": 5344
    },
    {
      "epoch": 0.28964769647696476,
      "step": 5344,
      "training_loss": 7.411798000335693
    },
    {
      "epoch": 0.2897018970189702,
      "step": 5345,
      "training_loss": 7.15424919128418
    },
    {
      "epoch": 0.2897560975609756,
      "step": 5346,
      "training_loss": 7.311041355133057
    },
    {
      "epoch": 0.28981029810298103,
      "step": 5347,
      "training_loss": 6.543461799621582
    },
    {
      "epoch": 0.2898644986449864,
      "grad_norm": 17.882768630981445,
      "learning_rate": 1e-05,
      "loss": 7.1051,
      "step": 5348
    },
    {
      "epoch": 0.2898644986449864,
      "step": 5348,
      "training_loss": 6.930213451385498
    },
    {
      "epoch": 0.28991869918699187,
      "step": 5349,
      "training_loss": 6.931403636932373
    },
    {
      "epoch": 0.2899728997289973,
      "step": 5350,
      "training_loss": 7.70989465713501
    },
    {
      "epoch": 0.2900271002710027,
      "step": 5351,
      "training_loss": 6.400733947753906
    },
    {
      "epoch": 0.29008130081300815,
      "grad_norm": 25.23141098022461,
      "learning_rate": 1e-05,
      "loss": 6.9931,
      "step": 5352
    },
    {
      "epoch": 0.29008130081300815,
      "step": 5352,
      "training_loss": 8.181900024414062
    },
    {
      "epoch": 0.29013550135501354,
      "step": 5353,
      "training_loss": 6.972903728485107
    },
    {
      "epoch": 0.290189701897019,
      "step": 5354,
      "training_loss": 6.9252400398254395
    },
    {
      "epoch": 0.29024390243902437,
      "step": 5355,
      "training_loss": 6.932626247406006
    },
    {
      "epoch": 0.2902981029810298,
      "grad_norm": 17.846702575683594,
      "learning_rate": 1e-05,
      "loss": 7.2532,
      "step": 5356
    },
    {
      "epoch": 0.2902981029810298,
      "step": 5356,
      "training_loss": 5.080899238586426
    },
    {
      "epoch": 0.2903523035230352,
      "step": 5357,
      "training_loss": 7.067171573638916
    },
    {
      "epoch": 0.29040650406504065,
      "step": 5358,
      "training_loss": 5.63410758972168
    },
    {
      "epoch": 0.2904607046070461,
      "step": 5359,
      "training_loss": 6.71927547454834
    },
    {
      "epoch": 0.2905149051490515,
      "grad_norm": 25.468624114990234,
      "learning_rate": 1e-05,
      "loss": 6.1254,
      "step": 5360
    },
    {
      "epoch": 0.2905149051490515,
      "step": 5360,
      "training_loss": 9.797452926635742
    },
    {
      "epoch": 0.29056910569105693,
      "step": 5361,
      "training_loss": 7.896750450134277
    },
    {
      "epoch": 0.2906233062330623,
      "step": 5362,
      "training_loss": 6.997118949890137
    },
    {
      "epoch": 0.29067750677506776,
      "step": 5363,
      "training_loss": 7.469674110412598
    },
    {
      "epoch": 0.29073170731707315,
      "grad_norm": 20.753643035888672,
      "learning_rate": 1e-05,
      "loss": 8.0402,
      "step": 5364
    },
    {
      "epoch": 0.29073170731707315,
      "step": 5364,
      "training_loss": 5.922784805297852
    },
    {
      "epoch": 0.2907859078590786,
      "step": 5365,
      "training_loss": 7.277478218078613
    },
    {
      "epoch": 0.290840108401084,
      "step": 5366,
      "training_loss": 6.92458438873291
    },
    {
      "epoch": 0.29089430894308943,
      "step": 5367,
      "training_loss": 6.937494277954102
    },
    {
      "epoch": 0.2909485094850949,
      "grad_norm": 17.402557373046875,
      "learning_rate": 1e-05,
      "loss": 6.7656,
      "step": 5368
    },
    {
      "epoch": 0.2909485094850949,
      "step": 5368,
      "training_loss": 7.587395668029785
    },
    {
      "epoch": 0.29100271002710026,
      "step": 5369,
      "training_loss": 8.406533241271973
    },
    {
      "epoch": 0.2910569105691057,
      "step": 5370,
      "training_loss": 6.470522880554199
    },
    {
      "epoch": 0.2911111111111111,
      "step": 5371,
      "training_loss": 6.343951225280762
    },
    {
      "epoch": 0.29116531165311654,
      "grad_norm": 22.623754501342773,
      "learning_rate": 1e-05,
      "loss": 7.2021,
      "step": 5372
    },
    {
      "epoch": 0.29116531165311654,
      "step": 5372,
      "training_loss": 7.1260223388671875
    },
    {
      "epoch": 0.29121951219512193,
      "step": 5373,
      "training_loss": 6.817702770233154
    },
    {
      "epoch": 0.2912737127371274,
      "step": 5374,
      "training_loss": 4.984694957733154
    },
    {
      "epoch": 0.29132791327913277,
      "step": 5375,
      "training_loss": 6.888352870941162
    },
    {
      "epoch": 0.2913821138211382,
      "grad_norm": 28.936830520629883,
      "learning_rate": 1e-05,
      "loss": 6.4542,
      "step": 5376
    },
    {
      "epoch": 0.2913821138211382,
      "step": 5376,
      "training_loss": 4.702929973602295
    },
    {
      "epoch": 0.29143631436314366,
      "step": 5377,
      "training_loss": 6.706864356994629
    },
    {
      "epoch": 0.29149051490514905,
      "step": 5378,
      "training_loss": 6.540095329284668
    },
    {
      "epoch": 0.2915447154471545,
      "step": 5379,
      "training_loss": 6.9532952308654785
    },
    {
      "epoch": 0.2915989159891599,
      "grad_norm": 44.36650085449219,
      "learning_rate": 1e-05,
      "loss": 6.2258,
      "step": 5380
    },
    {
      "epoch": 0.2915989159891599,
      "step": 5380,
      "training_loss": 6.387392520904541
    },
    {
      "epoch": 0.2916531165311653,
      "step": 5381,
      "training_loss": 7.2492170333862305
    },
    {
      "epoch": 0.2917073170731707,
      "step": 5382,
      "training_loss": 4.036409378051758
    },
    {
      "epoch": 0.29176151761517616,
      "step": 5383,
      "training_loss": 7.6299848556518555
    },
    {
      "epoch": 0.29181571815718155,
      "grad_norm": 24.41399574279785,
      "learning_rate": 1e-05,
      "loss": 6.3258,
      "step": 5384
    },
    {
      "epoch": 0.29181571815718155,
      "step": 5384,
      "training_loss": 6.632418632507324
    },
    {
      "epoch": 0.291869918699187,
      "step": 5385,
      "training_loss": 6.2361931800842285
    },
    {
      "epoch": 0.29192411924119244,
      "step": 5386,
      "training_loss": 7.371770858764648
    },
    {
      "epoch": 0.2919783197831978,
      "step": 5387,
      "training_loss": 6.970407962799072
    },
    {
      "epoch": 0.29203252032520327,
      "grad_norm": 33.87522506713867,
      "learning_rate": 1e-05,
      "loss": 6.8027,
      "step": 5388
    },
    {
      "epoch": 0.29203252032520327,
      "step": 5388,
      "training_loss": 6.624364376068115
    },
    {
      "epoch": 0.29208672086720866,
      "step": 5389,
      "training_loss": 6.542552471160889
    },
    {
      "epoch": 0.2921409214092141,
      "step": 5390,
      "training_loss": 7.375150680541992
    },
    {
      "epoch": 0.2921951219512195,
      "step": 5391,
      "training_loss": 6.9036641120910645
    },
    {
      "epoch": 0.29224932249322494,
      "grad_norm": 22.645727157592773,
      "learning_rate": 1e-05,
      "loss": 6.8614,
      "step": 5392
    },
    {
      "epoch": 0.29224932249322494,
      "step": 5392,
      "training_loss": 7.154378890991211
    },
    {
      "epoch": 0.29230352303523033,
      "step": 5393,
      "training_loss": 6.4620256423950195
    },
    {
      "epoch": 0.2923577235772358,
      "step": 5394,
      "training_loss": 6.520908832550049
    },
    {
      "epoch": 0.2924119241192412,
      "step": 5395,
      "training_loss": 4.332579135894775
    },
    {
      "epoch": 0.2924661246612466,
      "grad_norm": 24.443540573120117,
      "learning_rate": 1e-05,
      "loss": 6.1175,
      "step": 5396
    },
    {
      "epoch": 0.2924661246612466,
      "step": 5396,
      "training_loss": 6.589319705963135
    },
    {
      "epoch": 0.29252032520325205,
      "step": 5397,
      "training_loss": 6.474750518798828
    },
    {
      "epoch": 0.29257452574525744,
      "step": 5398,
      "training_loss": 7.461750507354736
    },
    {
      "epoch": 0.2926287262872629,
      "step": 5399,
      "training_loss": 7.071953773498535
    },
    {
      "epoch": 0.2926829268292683,
      "grad_norm": 39.68954086303711,
      "learning_rate": 1e-05,
      "loss": 6.8994,
      "step": 5400
    },
    {
      "epoch": 0.2926829268292683,
      "step": 5400,
      "training_loss": 9.482519149780273
    },
    {
      "epoch": 0.2927371273712737,
      "step": 5401,
      "training_loss": 7.418614864349365
    },
    {
      "epoch": 0.2927913279132791,
      "step": 5402,
      "training_loss": 6.50262975692749
    },
    {
      "epoch": 0.29284552845528455,
      "step": 5403,
      "training_loss": 6.139232158660889
    },
    {
      "epoch": 0.29289972899729,
      "grad_norm": 25.16407585144043,
      "learning_rate": 1e-05,
      "loss": 7.3857,
      "step": 5404
    },
    {
      "epoch": 0.29289972899729,
      "step": 5404,
      "training_loss": 6.533246040344238
    },
    {
      "epoch": 0.2929539295392954,
      "step": 5405,
      "training_loss": 7.16412353515625
    },
    {
      "epoch": 0.29300813008130083,
      "step": 5406,
      "training_loss": 7.42618989944458
    },
    {
      "epoch": 0.2930623306233062,
      "step": 5407,
      "training_loss": 7.0395989418029785
    },
    {
      "epoch": 0.29311653116531167,
      "grad_norm": 24.8836727142334,
      "learning_rate": 1e-05,
      "loss": 7.0408,
      "step": 5408
    },
    {
      "epoch": 0.29311653116531167,
      "step": 5408,
      "training_loss": 5.691144943237305
    },
    {
      "epoch": 0.29317073170731706,
      "step": 5409,
      "training_loss": 7.244941234588623
    },
    {
      "epoch": 0.2932249322493225,
      "step": 5410,
      "training_loss": 7.340455532073975
    },
    {
      "epoch": 0.2932791327913279,
      "step": 5411,
      "training_loss": 7.3873395919799805
    },
    {
      "epoch": 0.29333333333333333,
      "grad_norm": 27.80739402770996,
      "learning_rate": 1e-05,
      "loss": 6.916,
      "step": 5412
    },
    {
      "epoch": 0.29333333333333333,
      "step": 5412,
      "training_loss": 6.593052864074707
    },
    {
      "epoch": 0.2933875338753388,
      "step": 5413,
      "training_loss": 8.23981761932373
    },
    {
      "epoch": 0.29344173441734417,
      "step": 5414,
      "training_loss": 8.105524063110352
    },
    {
      "epoch": 0.2934959349593496,
      "step": 5415,
      "training_loss": 7.857624530792236
    },
    {
      "epoch": 0.293550135501355,
      "grad_norm": 33.67546463012695,
      "learning_rate": 1e-05,
      "loss": 7.699,
      "step": 5416
    },
    {
      "epoch": 0.293550135501355,
      "step": 5416,
      "training_loss": 7.672733783721924
    },
    {
      "epoch": 0.29360433604336045,
      "step": 5417,
      "training_loss": 6.510993957519531
    },
    {
      "epoch": 0.29365853658536584,
      "step": 5418,
      "training_loss": 6.485279083251953
    },
    {
      "epoch": 0.2937127371273713,
      "step": 5419,
      "training_loss": 6.819220542907715
    },
    {
      "epoch": 0.29376693766937667,
      "grad_norm": 23.00949478149414,
      "learning_rate": 1e-05,
      "loss": 6.8721,
      "step": 5420
    },
    {
      "epoch": 0.29376693766937667,
      "step": 5420,
      "training_loss": 4.885075569152832
    },
    {
      "epoch": 0.2938211382113821,
      "step": 5421,
      "training_loss": 6.473326683044434
    },
    {
      "epoch": 0.29387533875338756,
      "step": 5422,
      "training_loss": 7.925174713134766
    },
    {
      "epoch": 0.29392953929539295,
      "step": 5423,
      "training_loss": 7.347281455993652
    },
    {
      "epoch": 0.2939837398373984,
      "grad_norm": 32.032718658447266,
      "learning_rate": 1e-05,
      "loss": 6.6577,
      "step": 5424
    },
    {
      "epoch": 0.2939837398373984,
      "step": 5424,
      "training_loss": 7.257643699645996
    },
    {
      "epoch": 0.2940379403794038,
      "step": 5425,
      "training_loss": 5.469919681549072
    },
    {
      "epoch": 0.29409214092140923,
      "step": 5426,
      "training_loss": 8.010518074035645
    },
    {
      "epoch": 0.2941463414634146,
      "step": 5427,
      "training_loss": 6.140584945678711
    },
    {
      "epoch": 0.29420054200542006,
      "grad_norm": 24.184797286987305,
      "learning_rate": 1e-05,
      "loss": 6.7197,
      "step": 5428
    },
    {
      "epoch": 0.29420054200542006,
      "step": 5428,
      "training_loss": 6.17633581161499
    },
    {
      "epoch": 0.29425474254742545,
      "step": 5429,
      "training_loss": 6.6990885734558105
    },
    {
      "epoch": 0.2943089430894309,
      "step": 5430,
      "training_loss": 7.710830211639404
    },
    {
      "epoch": 0.29436314363143634,
      "step": 5431,
      "training_loss": 5.8875532150268555
    },
    {
      "epoch": 0.29441734417344173,
      "grad_norm": 24.823833465576172,
      "learning_rate": 1e-05,
      "loss": 6.6185,
      "step": 5432
    },
    {
      "epoch": 0.29441734417344173,
      "step": 5432,
      "training_loss": 7.469133377075195
    },
    {
      "epoch": 0.2944715447154472,
      "step": 5433,
      "training_loss": 6.711974620819092
    },
    {
      "epoch": 0.29452574525745256,
      "step": 5434,
      "training_loss": 7.265709400177002
    },
    {
      "epoch": 0.294579945799458,
      "step": 5435,
      "training_loss": 7.589774131774902
    },
    {
      "epoch": 0.2946341463414634,
      "grad_norm": 16.761837005615234,
      "learning_rate": 1e-05,
      "loss": 7.2591,
      "step": 5436
    },
    {
      "epoch": 0.2946341463414634,
      "step": 5436,
      "training_loss": 6.662492752075195
    },
    {
      "epoch": 0.29468834688346884,
      "step": 5437,
      "training_loss": 6.290999889373779
    },
    {
      "epoch": 0.29474254742547423,
      "step": 5438,
      "training_loss": 6.79396915435791
    },
    {
      "epoch": 0.2947967479674797,
      "step": 5439,
      "training_loss": 7.089818954467773
    },
    {
      "epoch": 0.2948509485094851,
      "grad_norm": 23.903217315673828,
      "learning_rate": 1e-05,
      "loss": 6.7093,
      "step": 5440
    },
    {
      "epoch": 0.2948509485094851,
      "step": 5440,
      "training_loss": 4.897316932678223
    },
    {
      "epoch": 0.2949051490514905,
      "step": 5441,
      "training_loss": 6.081754684448242
    },
    {
      "epoch": 0.29495934959349596,
      "step": 5442,
      "training_loss": 6.971445083618164
    },
    {
      "epoch": 0.29501355013550135,
      "step": 5443,
      "training_loss": 7.104244232177734
    },
    {
      "epoch": 0.2950677506775068,
      "grad_norm": 32.4334602355957,
      "learning_rate": 1e-05,
      "loss": 6.2637,
      "step": 5444
    },
    {
      "epoch": 0.2950677506775068,
      "step": 5444,
      "training_loss": 6.729641437530518
    },
    {
      "epoch": 0.2951219512195122,
      "step": 5445,
      "training_loss": 6.964099884033203
    },
    {
      "epoch": 0.2951761517615176,
      "step": 5446,
      "training_loss": 5.476464748382568
    },
    {
      "epoch": 0.295230352303523,
      "step": 5447,
      "training_loss": 7.421701431274414
    },
    {
      "epoch": 0.29528455284552846,
      "grad_norm": 36.65415573120117,
      "learning_rate": 1e-05,
      "loss": 6.648,
      "step": 5448
    },
    {
      "epoch": 0.29528455284552846,
      "step": 5448,
      "training_loss": 7.460057735443115
    },
    {
      "epoch": 0.29533875338753385,
      "step": 5449,
      "training_loss": 6.437042713165283
    },
    {
      "epoch": 0.2953929539295393,
      "step": 5450,
      "training_loss": 7.289834022521973
    },
    {
      "epoch": 0.29544715447154474,
      "step": 5451,
      "training_loss": 6.66705322265625
    },
    {
      "epoch": 0.2955013550135501,
      "grad_norm": 21.2213191986084,
      "learning_rate": 1e-05,
      "loss": 6.9635,
      "step": 5452
    },
    {
      "epoch": 0.2955013550135501,
      "step": 5452,
      "training_loss": 5.850754261016846
    },
    {
      "epoch": 0.29555555555555557,
      "step": 5453,
      "training_loss": 6.881796836853027
    },
    {
      "epoch": 0.29560975609756096,
      "step": 5454,
      "training_loss": 7.066958427429199
    },
    {
      "epoch": 0.2956639566395664,
      "step": 5455,
      "training_loss": 6.367775917053223
    },
    {
      "epoch": 0.2957181571815718,
      "grad_norm": 21.94205093383789,
      "learning_rate": 1e-05,
      "loss": 6.5418,
      "step": 5456
    },
    {
      "epoch": 0.2957181571815718,
      "step": 5456,
      "training_loss": 8.778267860412598
    },
    {
      "epoch": 0.29577235772357724,
      "step": 5457,
      "training_loss": 6.910949230194092
    },
    {
      "epoch": 0.29582655826558263,
      "step": 5458,
      "training_loss": 8.253515243530273
    },
    {
      "epoch": 0.2958807588075881,
      "step": 5459,
      "training_loss": 6.2879252433776855
    },
    {
      "epoch": 0.2959349593495935,
      "grad_norm": 24.878326416015625,
      "learning_rate": 1e-05,
      "loss": 7.5577,
      "step": 5460
    },
    {
      "epoch": 0.2959349593495935,
      "step": 5460,
      "training_loss": 6.262354850769043
    },
    {
      "epoch": 0.2959891598915989,
      "step": 5461,
      "training_loss": 6.437631130218506
    },
    {
      "epoch": 0.29604336043360435,
      "step": 5462,
      "training_loss": 7.129105567932129
    },
    {
      "epoch": 0.29609756097560974,
      "step": 5463,
      "training_loss": 7.29028844833374
    },
    {
      "epoch": 0.2961517615176152,
      "grad_norm": 18.525371551513672,
      "learning_rate": 1e-05,
      "loss": 6.7798,
      "step": 5464
    },
    {
      "epoch": 0.2961517615176152,
      "step": 5464,
      "training_loss": 7.168431758880615
    },
    {
      "epoch": 0.2962059620596206,
      "step": 5465,
      "training_loss": 5.124592304229736
    },
    {
      "epoch": 0.296260162601626,
      "step": 5466,
      "training_loss": 6.54768180847168
    },
    {
      "epoch": 0.2963143631436314,
      "step": 5467,
      "training_loss": 6.717886924743652
    },
    {
      "epoch": 0.29636856368563685,
      "grad_norm": 29.66290855407715,
      "learning_rate": 1e-05,
      "loss": 6.3896,
      "step": 5468
    },
    {
      "epoch": 0.29636856368563685,
      "step": 5468,
      "training_loss": 6.176183700561523
    },
    {
      "epoch": 0.2964227642276423,
      "step": 5469,
      "training_loss": 7.376357078552246
    },
    {
      "epoch": 0.2964769647696477,
      "step": 5470,
      "training_loss": 4.546340465545654
    },
    {
      "epoch": 0.29653116531165313,
      "step": 5471,
      "training_loss": 7.62912130355835
    },
    {
      "epoch": 0.2965853658536585,
      "grad_norm": 27.918865203857422,
      "learning_rate": 1e-05,
      "loss": 6.432,
      "step": 5472
    },
    {
      "epoch": 0.2965853658536585,
      "step": 5472,
      "training_loss": 6.5108489990234375
    },
    {
      "epoch": 0.29663956639566397,
      "step": 5473,
      "training_loss": 7.312901973724365
    },
    {
      "epoch": 0.29669376693766936,
      "step": 5474,
      "training_loss": 6.724462985992432
    },
    {
      "epoch": 0.2967479674796748,
      "step": 5475,
      "training_loss": 8.036532402038574
    },
    {
      "epoch": 0.2968021680216802,
      "grad_norm": 32.404720306396484,
      "learning_rate": 1e-05,
      "loss": 7.1462,
      "step": 5476
    },
    {
      "epoch": 0.2968021680216802,
      "step": 5476,
      "training_loss": 6.66433572769165
    },
    {
      "epoch": 0.29685636856368564,
      "step": 5477,
      "training_loss": 6.960991859436035
    },
    {
      "epoch": 0.2969105691056911,
      "step": 5478,
      "training_loss": 7.266385555267334
    },
    {
      "epoch": 0.29696476964769647,
      "step": 5479,
      "training_loss": 5.550258159637451
    },
    {
      "epoch": 0.2970189701897019,
      "grad_norm": 35.68106460571289,
      "learning_rate": 1e-05,
      "loss": 6.6105,
      "step": 5480
    },
    {
      "epoch": 0.2970189701897019,
      "step": 5480,
      "training_loss": 7.316122055053711
    },
    {
      "epoch": 0.2970731707317073,
      "step": 5481,
      "training_loss": 6.1057868003845215
    },
    {
      "epoch": 0.29712737127371275,
      "step": 5482,
      "training_loss": 6.1804375648498535
    },
    {
      "epoch": 0.29718157181571814,
      "step": 5483,
      "training_loss": 5.860891819000244
    },
    {
      "epoch": 0.2972357723577236,
      "grad_norm": 69.05267333984375,
      "learning_rate": 1e-05,
      "loss": 6.3658,
      "step": 5484
    },
    {
      "epoch": 0.2972357723577236,
      "step": 5484,
      "training_loss": 7.455819129943848
    },
    {
      "epoch": 0.29728997289972897,
      "step": 5485,
      "training_loss": 8.266287803649902
    },
    {
      "epoch": 0.2973441734417344,
      "step": 5486,
      "training_loss": 6.524265766143799
    },
    {
      "epoch": 0.29739837398373986,
      "step": 5487,
      "training_loss": 6.7133965492248535
    },
    {
      "epoch": 0.29745257452574525,
      "grad_norm": 25.6127872467041,
      "learning_rate": 1e-05,
      "loss": 7.2399,
      "step": 5488
    },
    {
      "epoch": 0.29745257452574525,
      "step": 5488,
      "training_loss": 6.975706577301025
    },
    {
      "epoch": 0.2975067750677507,
      "step": 5489,
      "training_loss": 4.948730945587158
    },
    {
      "epoch": 0.2975609756097561,
      "step": 5490,
      "training_loss": 6.9163360595703125
    },
    {
      "epoch": 0.29761517615176153,
      "step": 5491,
      "training_loss": 7.129118919372559
    },
    {
      "epoch": 0.2976693766937669,
      "grad_norm": 18.8866024017334,
      "learning_rate": 1e-05,
      "loss": 6.4925,
      "step": 5492
    },
    {
      "epoch": 0.2976693766937669,
      "step": 5492,
      "training_loss": 5.022350311279297
    },
    {
      "epoch": 0.29772357723577236,
      "step": 5493,
      "training_loss": 6.070078372955322
    },
    {
      "epoch": 0.29777777777777775,
      "step": 5494,
      "training_loss": 6.224246501922607
    },
    {
      "epoch": 0.2978319783197832,
      "step": 5495,
      "training_loss": 8.201390266418457
    },
    {
      "epoch": 0.29788617886178864,
      "grad_norm": 18.18089485168457,
      "learning_rate": 1e-05,
      "loss": 6.3795,
      "step": 5496
    },
    {
      "epoch": 0.29788617886178864,
      "step": 5496,
      "training_loss": 7.984183311462402
    },
    {
      "epoch": 0.29794037940379403,
      "step": 5497,
      "training_loss": 6.755579471588135
    },
    {
      "epoch": 0.2979945799457995,
      "step": 5498,
      "training_loss": 6.608581066131592
    },
    {
      "epoch": 0.29804878048780487,
      "step": 5499,
      "training_loss": 7.175459384918213
    },
    {
      "epoch": 0.2981029810298103,
      "grad_norm": 41.69963836669922,
      "learning_rate": 1e-05,
      "loss": 7.131,
      "step": 5500
    },
    {
      "epoch": 0.2981029810298103,
      "step": 5500,
      "training_loss": 7.328845024108887
    },
    {
      "epoch": 0.2981571815718157,
      "step": 5501,
      "training_loss": 4.071731090545654
    },
    {
      "epoch": 0.29821138211382114,
      "step": 5502,
      "training_loss": 6.575993061065674
    },
    {
      "epoch": 0.29826558265582653,
      "step": 5503,
      "training_loss": 4.091268539428711
    },
    {
      "epoch": 0.298319783197832,
      "grad_norm": 29.184398651123047,
      "learning_rate": 1e-05,
      "loss": 5.517,
      "step": 5504
    },
    {
      "epoch": 0.298319783197832,
      "step": 5504,
      "training_loss": 7.375997543334961
    },
    {
      "epoch": 0.2983739837398374,
      "step": 5505,
      "training_loss": 6.988004684448242
    },
    {
      "epoch": 0.2984281842818428,
      "step": 5506,
      "training_loss": 7.076620578765869
    },
    {
      "epoch": 0.29848238482384826,
      "step": 5507,
      "training_loss": 5.177403450012207
    },
    {
      "epoch": 0.29853658536585365,
      "grad_norm": 30.834688186645508,
      "learning_rate": 1e-05,
      "loss": 6.6545,
      "step": 5508
    },
    {
      "epoch": 0.29853658536585365,
      "step": 5508,
      "training_loss": 7.039413928985596
    },
    {
      "epoch": 0.2985907859078591,
      "step": 5509,
      "training_loss": 7.739176273345947
    },
    {
      "epoch": 0.2986449864498645,
      "step": 5510,
      "training_loss": 4.8256096839904785
    },
    {
      "epoch": 0.2986991869918699,
      "step": 5511,
      "training_loss": 6.697467803955078
    },
    {
      "epoch": 0.2987533875338753,
      "grad_norm": 19.76763343811035,
      "learning_rate": 1e-05,
      "loss": 6.5754,
      "step": 5512
    },
    {
      "epoch": 0.2987533875338753,
      "step": 5512,
      "training_loss": 6.049251556396484
    },
    {
      "epoch": 0.29880758807588076,
      "step": 5513,
      "training_loss": 4.8466668128967285
    },
    {
      "epoch": 0.2988617886178862,
      "step": 5514,
      "training_loss": 7.635298728942871
    },
    {
      "epoch": 0.2989159891598916,
      "step": 5515,
      "training_loss": 6.132689476013184
    },
    {
      "epoch": 0.29897018970189704,
      "grad_norm": 39.671871185302734,
      "learning_rate": 1e-05,
      "loss": 6.166,
      "step": 5516
    },
    {
      "epoch": 0.29897018970189704,
      "step": 5516,
      "training_loss": 6.974247455596924
    },
    {
      "epoch": 0.2990243902439024,
      "step": 5517,
      "training_loss": 7.380183696746826
    },
    {
      "epoch": 0.29907859078590787,
      "step": 5518,
      "training_loss": 7.232763290405273
    },
    {
      "epoch": 0.29913279132791326,
      "step": 5519,
      "training_loss": 7.783301830291748
    },
    {
      "epoch": 0.2991869918699187,
      "grad_norm": 26.479284286499023,
      "learning_rate": 1e-05,
      "loss": 7.3426,
      "step": 5520
    },
    {
      "epoch": 0.2991869918699187,
      "step": 5520,
      "training_loss": 6.702774524688721
    },
    {
      "epoch": 0.2992411924119241,
      "step": 5521,
      "training_loss": 7.255177974700928
    },
    {
      "epoch": 0.29929539295392954,
      "step": 5522,
      "training_loss": 8.21551513671875
    },
    {
      "epoch": 0.299349593495935,
      "step": 5523,
      "training_loss": 5.2431535720825195
    },
    {
      "epoch": 0.2994037940379404,
      "grad_norm": 31.80640411376953,
      "learning_rate": 1e-05,
      "loss": 6.8542,
      "step": 5524
    },
    {
      "epoch": 0.2994037940379404,
      "step": 5524,
      "training_loss": 7.038202285766602
    },
    {
      "epoch": 0.2994579945799458,
      "step": 5525,
      "training_loss": 7.91202449798584
    },
    {
      "epoch": 0.2995121951219512,
      "step": 5526,
      "training_loss": 6.446844577789307
    },
    {
      "epoch": 0.29956639566395665,
      "step": 5527,
      "training_loss": 7.139042377471924
    },
    {
      "epoch": 0.29962059620596204,
      "grad_norm": 20.05819320678711,
      "learning_rate": 1e-05,
      "loss": 7.134,
      "step": 5528
    },
    {
      "epoch": 0.29962059620596204,
      "step": 5528,
      "training_loss": 7.820009708404541
    },
    {
      "epoch": 0.2996747967479675,
      "step": 5529,
      "training_loss": 5.994859218597412
    },
    {
      "epoch": 0.2997289972899729,
      "step": 5530,
      "training_loss": 6.723250389099121
    },
    {
      "epoch": 0.2997831978319783,
      "step": 5531,
      "training_loss": 6.849370956420898
    },
    {
      "epoch": 0.29983739837398377,
      "grad_norm": 16.394268035888672,
      "learning_rate": 1e-05,
      "loss": 6.8469,
      "step": 5532
    },
    {
      "epoch": 0.29983739837398377,
      "step": 5532,
      "training_loss": 7.273368835449219
    },
    {
      "epoch": 0.29989159891598915,
      "step": 5533,
      "training_loss": 7.238754749298096
    },
    {
      "epoch": 0.2999457994579946,
      "step": 5534,
      "training_loss": 6.296139717102051
    },
    {
      "epoch": 0.3,
      "step": 5535,
      "training_loss": 6.499319553375244
    },
    {
      "epoch": 0.30005420054200543,
      "grad_norm": 34.7421875,
      "learning_rate": 1e-05,
      "loss": 6.8269,
      "step": 5536
    },
    {
      "epoch": 0.30005420054200543,
      "step": 5536,
      "training_loss": 7.484922885894775
    },
    {
      "epoch": 0.3001084010840108,
      "step": 5537,
      "training_loss": 6.314769744873047
    },
    {
      "epoch": 0.30016260162601627,
      "step": 5538,
      "training_loss": 6.730964183807373
    },
    {
      "epoch": 0.30021680216802166,
      "step": 5539,
      "training_loss": 6.804922580718994
    },
    {
      "epoch": 0.3002710027100271,
      "grad_norm": 21.85296058654785,
      "learning_rate": 1e-05,
      "loss": 6.8339,
      "step": 5540
    },
    {
      "epoch": 0.3002710027100271,
      "step": 5540,
      "training_loss": 7.180948734283447
    },
    {
      "epoch": 0.30032520325203255,
      "step": 5541,
      "training_loss": 6.544251918792725
    },
    {
      "epoch": 0.30037940379403794,
      "step": 5542,
      "training_loss": 6.958643436431885
    },
    {
      "epoch": 0.3004336043360434,
      "step": 5543,
      "training_loss": 6.361026287078857
    },
    {
      "epoch": 0.30048780487804877,
      "grad_norm": 36.71466064453125,
      "learning_rate": 1e-05,
      "loss": 6.7612,
      "step": 5544
    },
    {
      "epoch": 0.30048780487804877,
      "step": 5544,
      "training_loss": 6.8731818199157715
    },
    {
      "epoch": 0.3005420054200542,
      "step": 5545,
      "training_loss": 6.841780662536621
    },
    {
      "epoch": 0.3005962059620596,
      "step": 5546,
      "training_loss": 6.088544845581055
    },
    {
      "epoch": 0.30065040650406505,
      "step": 5547,
      "training_loss": 6.454706192016602
    },
    {
      "epoch": 0.30070460704607044,
      "grad_norm": 22.931716918945312,
      "learning_rate": 1e-05,
      "loss": 6.5646,
      "step": 5548
    },
    {
      "epoch": 0.30070460704607044,
      "step": 5548,
      "training_loss": 7.024929046630859
    },
    {
      "epoch": 0.3007588075880759,
      "step": 5549,
      "training_loss": 6.912380695343018
    },
    {
      "epoch": 0.3008130081300813,
      "step": 5550,
      "training_loss": 7.809717178344727
    },
    {
      "epoch": 0.3008672086720867,
      "step": 5551,
      "training_loss": 4.381865501403809
    },
    {
      "epoch": 0.30092140921409216,
      "grad_norm": 21.23476219177246,
      "learning_rate": 1e-05,
      "loss": 6.5322,
      "step": 5552
    },
    {
      "epoch": 0.30092140921409216,
      "step": 5552,
      "training_loss": 7.821357727050781
    },
    {
      "epoch": 0.30097560975609755,
      "step": 5553,
      "training_loss": 6.342700481414795
    },
    {
      "epoch": 0.301029810298103,
      "step": 5554,
      "training_loss": 5.828577518463135
    },
    {
      "epoch": 0.3010840108401084,
      "step": 5555,
      "training_loss": 6.992353916168213
    },
    {
      "epoch": 0.30113821138211383,
      "grad_norm": 19.846437454223633,
      "learning_rate": 1e-05,
      "loss": 6.7462,
      "step": 5556
    },
    {
      "epoch": 0.30113821138211383,
      "step": 5556,
      "training_loss": 6.98183012008667
    },
    {
      "epoch": 0.3011924119241192,
      "step": 5557,
      "training_loss": 6.840639114379883
    },
    {
      "epoch": 0.30124661246612466,
      "step": 5558,
      "training_loss": 6.654868125915527
    },
    {
      "epoch": 0.3013008130081301,
      "step": 5559,
      "training_loss": 6.637481689453125
    },
    {
      "epoch": 0.3013550135501355,
      "grad_norm": 29.733474731445312,
      "learning_rate": 1e-05,
      "loss": 6.7787,
      "step": 5560
    },
    {
      "epoch": 0.3013550135501355,
      "step": 5560,
      "training_loss": 5.09548807144165
    },
    {
      "epoch": 0.30140921409214094,
      "step": 5561,
      "training_loss": 7.296283721923828
    },
    {
      "epoch": 0.30146341463414633,
      "step": 5562,
      "training_loss": 8.032248497009277
    },
    {
      "epoch": 0.3015176151761518,
      "step": 5563,
      "training_loss": 5.314632415771484
    },
    {
      "epoch": 0.30157181571815717,
      "grad_norm": 25.621479034423828,
      "learning_rate": 1e-05,
      "loss": 6.4347,
      "step": 5564
    },
    {
      "epoch": 0.30157181571815717,
      "step": 5564,
      "training_loss": 7.616086006164551
    },
    {
      "epoch": 0.3016260162601626,
      "step": 5565,
      "training_loss": 8.321093559265137
    },
    {
      "epoch": 0.301680216802168,
      "step": 5566,
      "training_loss": 8.170673370361328
    },
    {
      "epoch": 0.30173441734417344,
      "step": 5567,
      "training_loss": 5.390249252319336
    },
    {
      "epoch": 0.3017886178861789,
      "grad_norm": 20.957359313964844,
      "learning_rate": 1e-05,
      "loss": 7.3745,
      "step": 5568
    },
    {
      "epoch": 0.3017886178861789,
      "step": 5568,
      "training_loss": 7.149376392364502
    },
    {
      "epoch": 0.3018428184281843,
      "step": 5569,
      "training_loss": 6.768954753875732
    },
    {
      "epoch": 0.3018970189701897,
      "step": 5570,
      "training_loss": 7.549567699432373
    },
    {
      "epoch": 0.3019512195121951,
      "step": 5571,
      "training_loss": 7.015838623046875
    },
    {
      "epoch": 0.30200542005420056,
      "grad_norm": 27.7188663482666,
      "learning_rate": 1e-05,
      "loss": 7.1209,
      "step": 5572
    },
    {
      "epoch": 0.30200542005420056,
      "step": 5572,
      "training_loss": 8.161271095275879
    },
    {
      "epoch": 0.30205962059620595,
      "step": 5573,
      "training_loss": 6.862929821014404
    },
    {
      "epoch": 0.3021138211382114,
      "step": 5574,
      "training_loss": 7.346306800842285
    },
    {
      "epoch": 0.3021680216802168,
      "step": 5575,
      "training_loss": 6.5987324714660645
    },
    {
      "epoch": 0.3022222222222222,
      "grad_norm": 29.505937576293945,
      "learning_rate": 1e-05,
      "loss": 7.2423,
      "step": 5576
    },
    {
      "epoch": 0.3022222222222222,
      "step": 5576,
      "training_loss": 8.413679122924805
    },
    {
      "epoch": 0.3022764227642276,
      "step": 5577,
      "training_loss": 6.430257797241211
    },
    {
      "epoch": 0.30233062330623306,
      "step": 5578,
      "training_loss": 6.31581974029541
    },
    {
      "epoch": 0.3023848238482385,
      "step": 5579,
      "training_loss": 5.180459976196289
    },
    {
      "epoch": 0.3024390243902439,
      "grad_norm": 32.86572265625,
      "learning_rate": 1e-05,
      "loss": 6.5851,
      "step": 5580
    },
    {
      "epoch": 0.3024390243902439,
      "step": 5580,
      "training_loss": 7.073565483093262
    },
    {
      "epoch": 0.30249322493224934,
      "step": 5581,
      "training_loss": 6.625865459442139
    },
    {
      "epoch": 0.3025474254742547,
      "step": 5582,
      "training_loss": 8.067943572998047
    },
    {
      "epoch": 0.3026016260162602,
      "step": 5583,
      "training_loss": 6.298433780670166
    },
    {
      "epoch": 0.30265582655826556,
      "grad_norm": 24.368162155151367,
      "learning_rate": 1e-05,
      "loss": 7.0165,
      "step": 5584
    },
    {
      "epoch": 0.30265582655826556,
      "step": 5584,
      "training_loss": 6.936344623565674
    },
    {
      "epoch": 0.302710027100271,
      "step": 5585,
      "training_loss": 7.3765387535095215
    },
    {
      "epoch": 0.3027642276422764,
      "step": 5586,
      "training_loss": 7.2839436531066895
    },
    {
      "epoch": 0.30281842818428184,
      "step": 5587,
      "training_loss": 8.006497383117676
    },
    {
      "epoch": 0.3028726287262873,
      "grad_norm": 53.2348518371582,
      "learning_rate": 1e-05,
      "loss": 7.4008,
      "step": 5588
    },
    {
      "epoch": 0.3028726287262873,
      "step": 5588,
      "training_loss": 7.6632609367370605
    },
    {
      "epoch": 0.3029268292682927,
      "step": 5589,
      "training_loss": 6.79746150970459
    },
    {
      "epoch": 0.3029810298102981,
      "step": 5590,
      "training_loss": 8.137222290039062
    },
    {
      "epoch": 0.3030352303523035,
      "step": 5591,
      "training_loss": 6.861934185028076
    },
    {
      "epoch": 0.30308943089430895,
      "grad_norm": 21.730628967285156,
      "learning_rate": 1e-05,
      "loss": 7.365,
      "step": 5592
    },
    {
      "epoch": 0.30308943089430895,
      "step": 5592,
      "training_loss": 8.058844566345215
    },
    {
      "epoch": 0.30314363143631434,
      "step": 5593,
      "training_loss": 7.757909297943115
    },
    {
      "epoch": 0.3031978319783198,
      "step": 5594,
      "training_loss": 5.605886459350586
    },
    {
      "epoch": 0.3032520325203252,
      "step": 5595,
      "training_loss": 6.188580513000488
    },
    {
      "epoch": 0.3033062330623306,
      "grad_norm": 18.49835968017578,
      "learning_rate": 1e-05,
      "loss": 6.9028,
      "step": 5596
    },
    {
      "epoch": 0.3033062330623306,
      "step": 5596,
      "training_loss": 6.364943981170654
    },
    {
      "epoch": 0.30336043360433607,
      "step": 5597,
      "training_loss": 6.770594120025635
    },
    {
      "epoch": 0.30341463414634146,
      "step": 5598,
      "training_loss": 7.060952186584473
    },
    {
      "epoch": 0.3034688346883469,
      "step": 5599,
      "training_loss": 6.430978298187256
    },
    {
      "epoch": 0.3035230352303523,
      "grad_norm": 33.04143142700195,
      "learning_rate": 1e-05,
      "loss": 6.6569,
      "step": 5600
    },
    {
      "epoch": 0.3035230352303523,
      "step": 5600,
      "training_loss": 6.1977949142456055
    },
    {
      "epoch": 0.30357723577235773,
      "step": 5601,
      "training_loss": 6.835841655731201
    },
    {
      "epoch": 0.3036314363143631,
      "step": 5602,
      "training_loss": 3.5345358848571777
    },
    {
      "epoch": 0.30368563685636857,
      "step": 5603,
      "training_loss": 7.248349666595459
    },
    {
      "epoch": 0.30373983739837396,
      "grad_norm": 18.628734588623047,
      "learning_rate": 1e-05,
      "loss": 5.9541,
      "step": 5604
    },
    {
      "epoch": 0.30373983739837396,
      "step": 5604,
      "training_loss": 6.899853229522705
    },
    {
      "epoch": 0.3037940379403794,
      "step": 5605,
      "training_loss": 7.664999008178711
    },
    {
      "epoch": 0.30384823848238485,
      "step": 5606,
      "training_loss": 5.464599609375
    },
    {
      "epoch": 0.30390243902439024,
      "step": 5607,
      "training_loss": 6.951932907104492
    },
    {
      "epoch": 0.3039566395663957,
      "grad_norm": 22.734699249267578,
      "learning_rate": 1e-05,
      "loss": 6.7453,
      "step": 5608
    },
    {
      "epoch": 0.3039566395663957,
      "step": 5608,
      "training_loss": 5.440035343170166
    },
    {
      "epoch": 0.30401084010840107,
      "step": 5609,
      "training_loss": 7.290704250335693
    },
    {
      "epoch": 0.3040650406504065,
      "step": 5610,
      "training_loss": 9.008588790893555
    },
    {
      "epoch": 0.3041192411924119,
      "step": 5611,
      "training_loss": 7.613358974456787
    },
    {
      "epoch": 0.30417344173441735,
      "grad_norm": 29.5846004486084,
      "learning_rate": 1e-05,
      "loss": 7.3382,
      "step": 5612
    },
    {
      "epoch": 0.30417344173441735,
      "step": 5612,
      "training_loss": 7.3649091720581055
    },
    {
      "epoch": 0.30422764227642274,
      "step": 5613,
      "training_loss": 6.522493362426758
    },
    {
      "epoch": 0.3042818428184282,
      "step": 5614,
      "training_loss": 7.117709159851074
    },
    {
      "epoch": 0.30433604336043363,
      "step": 5615,
      "training_loss": 4.888985633850098
    },
    {
      "epoch": 0.304390243902439,
      "grad_norm": 18.588655471801758,
      "learning_rate": 1e-05,
      "loss": 6.4735,
      "step": 5616
    },
    {
      "epoch": 0.304390243902439,
      "step": 5616,
      "training_loss": 7.766632080078125
    },
    {
      "epoch": 0.30444444444444446,
      "step": 5617,
      "training_loss": 5.686806678771973
    },
    {
      "epoch": 0.30449864498644985,
      "step": 5618,
      "training_loss": 7.811820030212402
    },
    {
      "epoch": 0.3045528455284553,
      "step": 5619,
      "training_loss": 7.102019786834717
    },
    {
      "epoch": 0.3046070460704607,
      "grad_norm": 20.60364532470703,
      "learning_rate": 1e-05,
      "loss": 7.0918,
      "step": 5620
    },
    {
      "epoch": 0.3046070460704607,
      "step": 5620,
      "training_loss": 7.803108215332031
    },
    {
      "epoch": 0.30466124661246613,
      "step": 5621,
      "training_loss": 4.884634494781494
    },
    {
      "epoch": 0.3047154471544715,
      "step": 5622,
      "training_loss": 5.482209205627441
    },
    {
      "epoch": 0.30476964769647696,
      "step": 5623,
      "training_loss": 7.288241386413574
    },
    {
      "epoch": 0.3048238482384824,
      "grad_norm": 22.279600143432617,
      "learning_rate": 1e-05,
      "loss": 6.3645,
      "step": 5624
    },
    {
      "epoch": 0.3048238482384824,
      "step": 5624,
      "training_loss": 6.632844924926758
    },
    {
      "epoch": 0.3048780487804878,
      "step": 5625,
      "training_loss": 6.580577373504639
    },
    {
      "epoch": 0.30493224932249324,
      "step": 5626,
      "training_loss": 7.92974853515625
    },
    {
      "epoch": 0.30498644986449863,
      "step": 5627,
      "training_loss": 7.063093662261963
    },
    {
      "epoch": 0.3050406504065041,
      "grad_norm": 18.670408248901367,
      "learning_rate": 1e-05,
      "loss": 7.0516,
      "step": 5628
    },
    {
      "epoch": 0.3050406504065041,
      "step": 5628,
      "training_loss": 7.953441619873047
    },
    {
      "epoch": 0.30509485094850947,
      "step": 5629,
      "training_loss": 7.060548782348633
    },
    {
      "epoch": 0.3051490514905149,
      "step": 5630,
      "training_loss": 7.255799293518066
    },
    {
      "epoch": 0.3052032520325203,
      "step": 5631,
      "training_loss": 8.333537101745605
    },
    {
      "epoch": 0.30525745257452574,
      "grad_norm": 40.10157012939453,
      "learning_rate": 1e-05,
      "loss": 7.6508,
      "step": 5632
    },
    {
      "epoch": 0.30525745257452574,
      "step": 5632,
      "training_loss": 5.497543811798096
    },
    {
      "epoch": 0.3053116531165312,
      "step": 5633,
      "training_loss": 7.591534614562988
    },
    {
      "epoch": 0.3053658536585366,
      "step": 5634,
      "training_loss": 5.894743919372559
    },
    {
      "epoch": 0.305420054200542,
      "step": 5635,
      "training_loss": 6.397545337677002
    },
    {
      "epoch": 0.3054742547425474,
      "grad_norm": 22.630298614501953,
      "learning_rate": 1e-05,
      "loss": 6.3453,
      "step": 5636
    },
    {
      "epoch": 0.3054742547425474,
      "step": 5636,
      "training_loss": 7.515269756317139
    },
    {
      "epoch": 0.30552845528455286,
      "step": 5637,
      "training_loss": 7.740232467651367
    },
    {
      "epoch": 0.30558265582655825,
      "step": 5638,
      "training_loss": 6.244409084320068
    },
    {
      "epoch": 0.3056368563685637,
      "step": 5639,
      "training_loss": 6.640351295471191
    },
    {
      "epoch": 0.3056910569105691,
      "grad_norm": 50.59546661376953,
      "learning_rate": 1e-05,
      "loss": 7.0351,
      "step": 5640
    },
    {
      "epoch": 0.3056910569105691,
      "step": 5640,
      "training_loss": 7.902849197387695
    },
    {
      "epoch": 0.3057452574525745,
      "step": 5641,
      "training_loss": 6.594899654388428
    },
    {
      "epoch": 0.30579945799457997,
      "step": 5642,
      "training_loss": 5.061570167541504
    },
    {
      "epoch": 0.30585365853658536,
      "step": 5643,
      "training_loss": 5.198919296264648
    },
    {
      "epoch": 0.3059078590785908,
      "grad_norm": 30.461374282836914,
      "learning_rate": 1e-05,
      "loss": 6.1896,
      "step": 5644
    },
    {
      "epoch": 0.3059078590785908,
      "step": 5644,
      "training_loss": 7.294280529022217
    },
    {
      "epoch": 0.3059620596205962,
      "step": 5645,
      "training_loss": 8.218485832214355
    },
    {
      "epoch": 0.30601626016260164,
      "step": 5646,
      "training_loss": 6.215823173522949
    },
    {
      "epoch": 0.30607046070460703,
      "step": 5647,
      "training_loss": 6.693709373474121
    },
    {
      "epoch": 0.3061246612466125,
      "grad_norm": 34.499656677246094,
      "learning_rate": 1e-05,
      "loss": 7.1056,
      "step": 5648
    },
    {
      "epoch": 0.3061246612466125,
      "step": 5648,
      "training_loss": 6.997509956359863
    },
    {
      "epoch": 0.30617886178861786,
      "step": 5649,
      "training_loss": 7.000695705413818
    },
    {
      "epoch": 0.3062330623306233,
      "step": 5650,
      "training_loss": 7.170607089996338
    },
    {
      "epoch": 0.30628726287262875,
      "step": 5651,
      "training_loss": 6.930575847625732
    },
    {
      "epoch": 0.30634146341463414,
      "grad_norm": 25.51556968688965,
      "learning_rate": 1e-05,
      "loss": 7.0248,
      "step": 5652
    },
    {
      "epoch": 0.30634146341463414,
      "step": 5652,
      "training_loss": 7.301340579986572
    },
    {
      "epoch": 0.3063956639566396,
      "step": 5653,
      "training_loss": 6.4238996505737305
    },
    {
      "epoch": 0.306449864498645,
      "step": 5654,
      "training_loss": 6.134068965911865
    },
    {
      "epoch": 0.3065040650406504,
      "step": 5655,
      "training_loss": 5.936971187591553
    },
    {
      "epoch": 0.3065582655826558,
      "grad_norm": 40.66069412231445,
      "learning_rate": 1e-05,
      "loss": 6.4491,
      "step": 5656
    },
    {
      "epoch": 0.3065582655826558,
      "step": 5656,
      "training_loss": 8.067761421203613
    },
    {
      "epoch": 0.30661246612466125,
      "step": 5657,
      "training_loss": 9.802783012390137
    },
    {
      "epoch": 0.30666666666666664,
      "step": 5658,
      "training_loss": 4.4486212730407715
    },
    {
      "epoch": 0.3067208672086721,
      "step": 5659,
      "training_loss": 7.168184280395508
    },
    {
      "epoch": 0.30677506775067753,
      "grad_norm": 29.660341262817383,
      "learning_rate": 1e-05,
      "loss": 7.3718,
      "step": 5660
    },
    {
      "epoch": 0.30677506775067753,
      "step": 5660,
      "training_loss": 3.9070136547088623
    },
    {
      "epoch": 0.3068292682926829,
      "step": 5661,
      "training_loss": 4.118085861206055
    },
    {
      "epoch": 0.30688346883468837,
      "step": 5662,
      "training_loss": 6.458305835723877
    },
    {
      "epoch": 0.30693766937669376,
      "step": 5663,
      "training_loss": 6.403006553649902
    },
    {
      "epoch": 0.3069918699186992,
      "grad_norm": 25.39635467529297,
      "learning_rate": 1e-05,
      "loss": 5.2216,
      "step": 5664
    },
    {
      "epoch": 0.3069918699186992,
      "step": 5664,
      "training_loss": 6.902246475219727
    },
    {
      "epoch": 0.3070460704607046,
      "step": 5665,
      "training_loss": 7.556827545166016
    },
    {
      "epoch": 0.30710027100271003,
      "step": 5666,
      "training_loss": 7.115594387054443
    },
    {
      "epoch": 0.3071544715447154,
      "step": 5667,
      "training_loss": 6.0891571044921875
    },
    {
      "epoch": 0.30720867208672087,
      "grad_norm": 32.9075927734375,
      "learning_rate": 1e-05,
      "loss": 6.916,
      "step": 5668
    },
    {
      "epoch": 0.30720867208672087,
      "step": 5668,
      "training_loss": 6.099724292755127
    },
    {
      "epoch": 0.3072628726287263,
      "step": 5669,
      "training_loss": 6.062244415283203
    },
    {
      "epoch": 0.3073170731707317,
      "step": 5670,
      "training_loss": 4.740251064300537
    },
    {
      "epoch": 0.30737127371273715,
      "step": 5671,
      "training_loss": 6.853412628173828
    },
    {
      "epoch": 0.30742547425474254,
      "grad_norm": 21.484861373901367,
      "learning_rate": 1e-05,
      "loss": 5.9389,
      "step": 5672
    },
    {
      "epoch": 0.30742547425474254,
      "step": 5672,
      "training_loss": 7.3955183029174805
    },
    {
      "epoch": 0.307479674796748,
      "step": 5673,
      "training_loss": 6.897545337677002
    },
    {
      "epoch": 0.30753387533875337,
      "step": 5674,
      "training_loss": 4.39104700088501
    },
    {
      "epoch": 0.3075880758807588,
      "step": 5675,
      "training_loss": 6.0901923179626465
    },
    {
      "epoch": 0.3076422764227642,
      "grad_norm": 46.783050537109375,
      "learning_rate": 1e-05,
      "loss": 6.1936,
      "step": 5676
    },
    {
      "epoch": 0.3076422764227642,
      "step": 5676,
      "training_loss": 6.80014705657959
    },
    {
      "epoch": 0.30769647696476965,
      "step": 5677,
      "training_loss": 8.72382926940918
    },
    {
      "epoch": 0.3077506775067751,
      "step": 5678,
      "training_loss": 5.271643161773682
    },
    {
      "epoch": 0.3078048780487805,
      "step": 5679,
      "training_loss": 5.660804271697998
    },
    {
      "epoch": 0.30785907859078593,
      "grad_norm": 23.02401351928711,
      "learning_rate": 1e-05,
      "loss": 6.6141,
      "step": 5680
    },
    {
      "epoch": 0.30785907859078593,
      "step": 5680,
      "training_loss": 6.784835338592529
    },
    {
      "epoch": 0.3079132791327913,
      "step": 5681,
      "training_loss": 6.966787338256836
    },
    {
      "epoch": 0.30796747967479676,
      "step": 5682,
      "training_loss": 6.085117816925049
    },
    {
      "epoch": 0.30802168021680215,
      "step": 5683,
      "training_loss": 5.332808017730713
    },
    {
      "epoch": 0.3080758807588076,
      "grad_norm": 22.09832000732422,
      "learning_rate": 1e-05,
      "loss": 6.2924,
      "step": 5684
    },
    {
      "epoch": 0.3080758807588076,
      "step": 5684,
      "training_loss": 5.999695777893066
    },
    {
      "epoch": 0.308130081300813,
      "step": 5685,
      "training_loss": 5.4123854637146
    },
    {
      "epoch": 0.30818428184281843,
      "step": 5686,
      "training_loss": 6.607239246368408
    },
    {
      "epoch": 0.3082384823848239,
      "step": 5687,
      "training_loss": 6.707234859466553
    },
    {
      "epoch": 0.30829268292682926,
      "grad_norm": 24.895450592041016,
      "learning_rate": 1e-05,
      "loss": 6.1816,
      "step": 5688
    },
    {
      "epoch": 0.30829268292682926,
      "step": 5688,
      "training_loss": 6.759856700897217
    },
    {
      "epoch": 0.3083468834688347,
      "step": 5689,
      "training_loss": 6.852887153625488
    },
    {
      "epoch": 0.3084010840108401,
      "step": 5690,
      "training_loss": 6.268793106079102
    },
    {
      "epoch": 0.30845528455284554,
      "step": 5691,
      "training_loss": 5.976069450378418
    },
    {
      "epoch": 0.30850948509485093,
      "grad_norm": 19.641803741455078,
      "learning_rate": 1e-05,
      "loss": 6.4644,
      "step": 5692
    },
    {
      "epoch": 0.30850948509485093,
      "step": 5692,
      "training_loss": 7.07320499420166
    },
    {
      "epoch": 0.3085636856368564,
      "step": 5693,
      "training_loss": 7.039528846740723
    },
    {
      "epoch": 0.30861788617886177,
      "step": 5694,
      "training_loss": 7.1125006675720215
    },
    {
      "epoch": 0.3086720867208672,
      "step": 5695,
      "training_loss": 6.930583953857422
    },
    {
      "epoch": 0.30872628726287266,
      "grad_norm": 17.801191329956055,
      "learning_rate": 1e-05,
      "loss": 7.039,
      "step": 5696
    },
    {
      "epoch": 0.30872628726287266,
      "step": 5696,
      "training_loss": 7.122097492218018
    },
    {
      "epoch": 0.30878048780487805,
      "step": 5697,
      "training_loss": 6.535782814025879
    },
    {
      "epoch": 0.3088346883468835,
      "step": 5698,
      "training_loss": 6.257598876953125
    },
    {
      "epoch": 0.3088888888888889,
      "step": 5699,
      "training_loss": 6.308672904968262
    },
    {
      "epoch": 0.3089430894308943,
      "grad_norm": 52.232933044433594,
      "learning_rate": 1e-05,
      "loss": 6.556,
      "step": 5700
    },
    {
      "epoch": 0.3089430894308943,
      "step": 5700,
      "training_loss": 5.993052959442139
    },
    {
      "epoch": 0.3089972899728997,
      "step": 5701,
      "training_loss": 7.701688289642334
    },
    {
      "epoch": 0.30905149051490516,
      "step": 5702,
      "training_loss": 8.550039291381836
    },
    {
      "epoch": 0.30910569105691055,
      "step": 5703,
      "training_loss": 6.700006008148193
    },
    {
      "epoch": 0.309159891598916,
      "grad_norm": 26.465267181396484,
      "learning_rate": 1e-05,
      "loss": 7.2362,
      "step": 5704
    },
    {
      "epoch": 0.309159891598916,
      "step": 5704,
      "training_loss": 6.991594314575195
    },
    {
      "epoch": 0.3092140921409214,
      "step": 5705,
      "training_loss": 7.836162090301514
    },
    {
      "epoch": 0.3092682926829268,
      "step": 5706,
      "training_loss": 6.7906341552734375
    },
    {
      "epoch": 0.30932249322493227,
      "step": 5707,
      "training_loss": 5.498788356781006
    },
    {
      "epoch": 0.30937669376693766,
      "grad_norm": 45.878257751464844,
      "learning_rate": 1e-05,
      "loss": 6.7793,
      "step": 5708
    },
    {
      "epoch": 0.30937669376693766,
      "step": 5708,
      "training_loss": 6.691109657287598
    },
    {
      "epoch": 0.3094308943089431,
      "step": 5709,
      "training_loss": 7.044139385223389
    },
    {
      "epoch": 0.3094850948509485,
      "step": 5710,
      "training_loss": 7.263749599456787
    },
    {
      "epoch": 0.30953929539295394,
      "step": 5711,
      "training_loss": 6.768485069274902
    },
    {
      "epoch": 0.30959349593495933,
      "grad_norm": 42.97751235961914,
      "learning_rate": 1e-05,
      "loss": 6.9419,
      "step": 5712
    },
    {
      "epoch": 0.30959349593495933,
      "step": 5712,
      "training_loss": 3.9524691104888916
    },
    {
      "epoch": 0.3096476964769648,
      "step": 5713,
      "training_loss": 7.4976325035095215
    },
    {
      "epoch": 0.30970189701897016,
      "step": 5714,
      "training_loss": 7.5047688484191895
    },
    {
      "epoch": 0.3097560975609756,
      "step": 5715,
      "training_loss": 6.800693035125732
    },
    {
      "epoch": 0.30981029810298105,
      "grad_norm": 15.038965225219727,
      "learning_rate": 1e-05,
      "loss": 6.4389,
      "step": 5716
    },
    {
      "epoch": 0.30981029810298105,
      "step": 5716,
      "training_loss": 7.289487838745117
    },
    {
      "epoch": 0.30986449864498644,
      "step": 5717,
      "training_loss": 7.239224910736084
    },
    {
      "epoch": 0.3099186991869919,
      "step": 5718,
      "training_loss": 7.38472318649292
    },
    {
      "epoch": 0.3099728997289973,
      "step": 5719,
      "training_loss": 8.070430755615234
    },
    {
      "epoch": 0.3100271002710027,
      "grad_norm": 82.74951934814453,
      "learning_rate": 1e-05,
      "loss": 7.496,
      "step": 5720
    },
    {
      "epoch": 0.3100271002710027,
      "step": 5720,
      "training_loss": 7.786215782165527
    },
    {
      "epoch": 0.3100813008130081,
      "step": 5721,
      "training_loss": 5.5870866775512695
    },
    {
      "epoch": 0.31013550135501355,
      "step": 5722,
      "training_loss": 4.916363716125488
    },
    {
      "epoch": 0.31018970189701894,
      "step": 5723,
      "training_loss": 6.648750305175781
    },
    {
      "epoch": 0.3102439024390244,
      "grad_norm": 21.794355392456055,
      "learning_rate": 1e-05,
      "loss": 6.2346,
      "step": 5724
    },
    {
      "epoch": 0.3102439024390244,
      "step": 5724,
      "training_loss": 7.2577805519104
    },
    {
      "epoch": 0.31029810298102983,
      "step": 5725,
      "training_loss": 6.099295616149902
    },
    {
      "epoch": 0.3103523035230352,
      "step": 5726,
      "training_loss": 6.274988651275635
    },
    {
      "epoch": 0.31040650406504067,
      "step": 5727,
      "training_loss": 6.728343963623047
    },
    {
      "epoch": 0.31046070460704606,
      "grad_norm": 23.212095260620117,
      "learning_rate": 1e-05,
      "loss": 6.5901,
      "step": 5728
    },
    {
      "epoch": 0.31046070460704606,
      "step": 5728,
      "training_loss": 7.5760979652404785
    },
    {
      "epoch": 0.3105149051490515,
      "step": 5729,
      "training_loss": 5.834286212921143
    },
    {
      "epoch": 0.3105691056910569,
      "step": 5730,
      "training_loss": 4.870362758636475
    },
    {
      "epoch": 0.31062330623306234,
      "step": 5731,
      "training_loss": 7.167490482330322
    },
    {
      "epoch": 0.3106775067750677,
      "grad_norm": 42.2354850769043,
      "learning_rate": 1e-05,
      "loss": 6.3621,
      "step": 5732
    },
    {
      "epoch": 0.3106775067750677,
      "step": 5732,
      "training_loss": 7.16312837600708
    },
    {
      "epoch": 0.31073170731707317,
      "step": 5733,
      "training_loss": 6.545947074890137
    },
    {
      "epoch": 0.3107859078590786,
      "step": 5734,
      "training_loss": 6.779641151428223
    },
    {
      "epoch": 0.310840108401084,
      "step": 5735,
      "training_loss": 5.822399139404297
    },
    {
      "epoch": 0.31089430894308945,
      "grad_norm": 27.367969512939453,
      "learning_rate": 1e-05,
      "loss": 6.5778,
      "step": 5736
    },
    {
      "epoch": 0.31089430894308945,
      "step": 5736,
      "training_loss": 6.414102554321289
    },
    {
      "epoch": 0.31094850948509484,
      "step": 5737,
      "training_loss": 7.356184959411621
    },
    {
      "epoch": 0.3110027100271003,
      "step": 5738,
      "training_loss": 6.262912750244141
    },
    {
      "epoch": 0.31105691056910567,
      "step": 5739,
      "training_loss": 7.064983367919922
    },
    {
      "epoch": 0.3111111111111111,
      "grad_norm": 41.36314392089844,
      "learning_rate": 1e-05,
      "loss": 6.7745,
      "step": 5740
    },
    {
      "epoch": 0.3111111111111111,
      "step": 5740,
      "training_loss": 6.285340785980225
    },
    {
      "epoch": 0.3111653116531165,
      "step": 5741,
      "training_loss": 6.940309047698975
    },
    {
      "epoch": 0.31121951219512195,
      "step": 5742,
      "training_loss": 6.396690368652344
    },
    {
      "epoch": 0.3112737127371274,
      "step": 5743,
      "training_loss": 7.294802188873291
    },
    {
      "epoch": 0.3113279132791328,
      "grad_norm": 31.526874542236328,
      "learning_rate": 1e-05,
      "loss": 6.7293,
      "step": 5744
    },
    {
      "epoch": 0.3113279132791328,
      "step": 5744,
      "training_loss": 7.212146759033203
    },
    {
      "epoch": 0.31138211382113823,
      "step": 5745,
      "training_loss": 7.118375301361084
    },
    {
      "epoch": 0.3114363143631436,
      "step": 5746,
      "training_loss": 6.7352142333984375
    },
    {
      "epoch": 0.31149051490514906,
      "step": 5747,
      "training_loss": 5.793942451477051
    },
    {
      "epoch": 0.31154471544715445,
      "grad_norm": 29.407386779785156,
      "learning_rate": 1e-05,
      "loss": 6.7149,
      "step": 5748
    },
    {
      "epoch": 0.31154471544715445,
      "step": 5748,
      "training_loss": 5.5595784187316895
    },
    {
      "epoch": 0.3115989159891599,
      "step": 5749,
      "training_loss": 4.273542404174805
    },
    {
      "epoch": 0.3116531165311653,
      "step": 5750,
      "training_loss": 6.081841945648193
    },
    {
      "epoch": 0.31170731707317073,
      "step": 5751,
      "training_loss": 7.299894332885742
    },
    {
      "epoch": 0.3117615176151762,
      "grad_norm": 21.688682556152344,
      "learning_rate": 1e-05,
      "loss": 5.8037,
      "step": 5752
    },
    {
      "epoch": 0.3117615176151762,
      "step": 5752,
      "training_loss": 6.1036481857299805
    },
    {
      "epoch": 0.31181571815718157,
      "step": 5753,
      "training_loss": 7.1180806159973145
    },
    {
      "epoch": 0.311869918699187,
      "step": 5754,
      "training_loss": 4.290517330169678
    },
    {
      "epoch": 0.3119241192411924,
      "step": 5755,
      "training_loss": 5.909097194671631
    },
    {
      "epoch": 0.31197831978319784,
      "grad_norm": 34.878910064697266,
      "learning_rate": 1e-05,
      "loss": 5.8553,
      "step": 5756
    },
    {
      "epoch": 0.31197831978319784,
      "step": 5756,
      "training_loss": 6.577840328216553
    },
    {
      "epoch": 0.31203252032520323,
      "step": 5757,
      "training_loss": 6.795447826385498
    },
    {
      "epoch": 0.3120867208672087,
      "step": 5758,
      "training_loss": 5.773514270782471
    },
    {
      "epoch": 0.31214092140921407,
      "step": 5759,
      "training_loss": 7.0548481941223145
    },
    {
      "epoch": 0.3121951219512195,
      "grad_norm": 25.343135833740234,
      "learning_rate": 1e-05,
      "loss": 6.5504,
      "step": 5760
    },
    {
      "epoch": 0.3121951219512195,
      "step": 5760,
      "training_loss": 6.703137397766113
    },
    {
      "epoch": 0.31224932249322496,
      "step": 5761,
      "training_loss": 6.282063961029053
    },
    {
      "epoch": 0.31230352303523035,
      "step": 5762,
      "training_loss": 6.596960544586182
    },
    {
      "epoch": 0.3123577235772358,
      "step": 5763,
      "training_loss": 6.0319504737854
    },
    {
      "epoch": 0.3124119241192412,
      "grad_norm": 24.636404037475586,
      "learning_rate": 1e-05,
      "loss": 6.4035,
      "step": 5764
    },
    {
      "epoch": 0.3124119241192412,
      "step": 5764,
      "training_loss": 7.311299800872803
    },
    {
      "epoch": 0.3124661246612466,
      "step": 5765,
      "training_loss": 6.8936920166015625
    },
    {
      "epoch": 0.312520325203252,
      "step": 5766,
      "training_loss": 5.310729026794434
    },
    {
      "epoch": 0.31257452574525746,
      "step": 5767,
      "training_loss": 6.615461349487305
    },
    {
      "epoch": 0.31262872628726285,
      "grad_norm": 47.350494384765625,
      "learning_rate": 1e-05,
      "loss": 6.5328,
      "step": 5768
    },
    {
      "epoch": 0.31262872628726285,
      "step": 5768,
      "training_loss": 7.56817102432251
    },
    {
      "epoch": 0.3126829268292683,
      "step": 5769,
      "training_loss": 5.7881646156311035
    },
    {
      "epoch": 0.31273712737127374,
      "step": 5770,
      "training_loss": 7.583795070648193
    },
    {
      "epoch": 0.3127913279132791,
      "step": 5771,
      "training_loss": 6.775852203369141
    },
    {
      "epoch": 0.31284552845528457,
      "grad_norm": 25.977781295776367,
      "learning_rate": 1e-05,
      "loss": 6.929,
      "step": 5772
    },
    {
      "epoch": 0.31284552845528457,
      "step": 5772,
      "training_loss": 5.840571403503418
    },
    {
      "epoch": 0.31289972899728996,
      "step": 5773,
      "training_loss": 6.1451416015625
    },
    {
      "epoch": 0.3129539295392954,
      "step": 5774,
      "training_loss": 7.260654926300049
    },
    {
      "epoch": 0.3130081300813008,
      "step": 5775,
      "training_loss": 6.209415435791016
    },
    {
      "epoch": 0.31306233062330624,
      "grad_norm": 42.23783874511719,
      "learning_rate": 1e-05,
      "loss": 6.3639,
      "step": 5776
    },
    {
      "epoch": 0.31306233062330624,
      "step": 5776,
      "training_loss": 6.256805896759033
    },
    {
      "epoch": 0.31311653116531163,
      "step": 5777,
      "training_loss": 7.376813888549805
    },
    {
      "epoch": 0.3131707317073171,
      "step": 5778,
      "training_loss": 5.702299118041992
    },
    {
      "epoch": 0.3132249322493225,
      "step": 5779,
      "training_loss": 7.403219223022461
    },
    {
      "epoch": 0.3132791327913279,
      "grad_norm": 19.175630569458008,
      "learning_rate": 1e-05,
      "loss": 6.6848,
      "step": 5780
    },
    {
      "epoch": 0.3132791327913279,
      "step": 5780,
      "training_loss": 6.411343097686768
    },
    {
      "epoch": 0.31333333333333335,
      "step": 5781,
      "training_loss": 6.798467636108398
    },
    {
      "epoch": 0.31338753387533874,
      "step": 5782,
      "training_loss": 4.88440465927124
    },
    {
      "epoch": 0.3134417344173442,
      "step": 5783,
      "training_loss": 5.670746326446533
    },
    {
      "epoch": 0.3134959349593496,
      "grad_norm": 28.41655921936035,
      "learning_rate": 1e-05,
      "loss": 5.9412,
      "step": 5784
    },
    {
      "epoch": 0.3134959349593496,
      "step": 5784,
      "training_loss": 5.592528343200684
    },
    {
      "epoch": 0.313550135501355,
      "step": 5785,
      "training_loss": 7.271027565002441
    },
    {
      "epoch": 0.3136043360433604,
      "step": 5786,
      "training_loss": 5.115762233734131
    },
    {
      "epoch": 0.31365853658536585,
      "step": 5787,
      "training_loss": 6.620197772979736
    },
    {
      "epoch": 0.3137127371273713,
      "grad_norm": 24.834152221679688,
      "learning_rate": 1e-05,
      "loss": 6.1499,
      "step": 5788
    },
    {
      "epoch": 0.3137127371273713,
      "step": 5788,
      "training_loss": 7.693042755126953
    },
    {
      "epoch": 0.3137669376693767,
      "step": 5789,
      "training_loss": 6.801689624786377
    },
    {
      "epoch": 0.31382113821138213,
      "step": 5790,
      "training_loss": 7.480729103088379
    },
    {
      "epoch": 0.3138753387533875,
      "step": 5791,
      "training_loss": 6.696623802185059
    },
    {
      "epoch": 0.31392953929539297,
      "grad_norm": 22.940128326416016,
      "learning_rate": 1e-05,
      "loss": 7.168,
      "step": 5792
    },
    {
      "epoch": 0.31392953929539297,
      "step": 5792,
      "training_loss": 7.058065414428711
    },
    {
      "epoch": 0.31398373983739836,
      "step": 5793,
      "training_loss": 6.728540897369385
    },
    {
      "epoch": 0.3140379403794038,
      "step": 5794,
      "training_loss": 8.452890396118164
    },
    {
      "epoch": 0.3140921409214092,
      "step": 5795,
      "training_loss": 6.7228264808654785
    },
    {
      "epoch": 0.31414634146341464,
      "grad_norm": 22.840595245361328,
      "learning_rate": 1e-05,
      "loss": 7.2406,
      "step": 5796
    },
    {
      "epoch": 0.31414634146341464,
      "step": 5796,
      "training_loss": 6.047886371612549
    },
    {
      "epoch": 0.3142005420054201,
      "step": 5797,
      "training_loss": 6.668405055999756
    },
    {
      "epoch": 0.31425474254742547,
      "step": 5798,
      "training_loss": 7.558332443237305
    },
    {
      "epoch": 0.3143089430894309,
      "step": 5799,
      "training_loss": 6.345000743865967
    },
    {
      "epoch": 0.3143631436314363,
      "grad_norm": 23.17375373840332,
      "learning_rate": 1e-05,
      "loss": 6.6549,
      "step": 5800
    },
    {
      "epoch": 0.3143631436314363,
      "step": 5800,
      "training_loss": 7.4203901290893555
    },
    {
      "epoch": 0.31441734417344175,
      "step": 5801,
      "training_loss": 5.373239517211914
    },
    {
      "epoch": 0.31447154471544714,
      "step": 5802,
      "training_loss": 4.321131229400635
    },
    {
      "epoch": 0.3145257452574526,
      "step": 5803,
      "training_loss": 6.611727714538574
    },
    {
      "epoch": 0.31457994579945797,
      "grad_norm": 16.8461971282959,
      "learning_rate": 1e-05,
      "loss": 5.9316,
      "step": 5804
    },
    {
      "epoch": 0.31457994579945797,
      "step": 5804,
      "training_loss": 6.4659504890441895
    },
    {
      "epoch": 0.3146341463414634,
      "step": 5805,
      "training_loss": 7.185008525848389
    },
    {
      "epoch": 0.31468834688346886,
      "step": 5806,
      "training_loss": 3.449525833129883
    },
    {
      "epoch": 0.31474254742547425,
      "step": 5807,
      "training_loss": 4.159731388092041
    },
    {
      "epoch": 0.3147967479674797,
      "grad_norm": 26.505292892456055,
      "learning_rate": 1e-05,
      "loss": 5.3151,
      "step": 5808
    },
    {
      "epoch": 0.3147967479674797,
      "step": 5808,
      "training_loss": 6.248623371124268
    },
    {
      "epoch": 0.3148509485094851,
      "step": 5809,
      "training_loss": 7.463626861572266
    },
    {
      "epoch": 0.31490514905149053,
      "step": 5810,
      "training_loss": 6.624380111694336
    },
    {
      "epoch": 0.3149593495934959,
      "step": 5811,
      "training_loss": 5.520007133483887
    },
    {
      "epoch": 0.31501355013550136,
      "grad_norm": 21.78594970703125,
      "learning_rate": 1e-05,
      "loss": 6.4642,
      "step": 5812
    },
    {
      "epoch": 0.31501355013550136,
      "step": 5812,
      "training_loss": 7.619346618652344
    },
    {
      "epoch": 0.31506775067750675,
      "step": 5813,
      "training_loss": 6.231497287750244
    },
    {
      "epoch": 0.3151219512195122,
      "step": 5814,
      "training_loss": 6.083370685577393
    },
    {
      "epoch": 0.31517615176151764,
      "step": 5815,
      "training_loss": 6.920437812805176
    },
    {
      "epoch": 0.31523035230352303,
      "grad_norm": 35.20348358154297,
      "learning_rate": 1e-05,
      "loss": 6.7137,
      "step": 5816
    },
    {
      "epoch": 0.31523035230352303,
      "step": 5816,
      "training_loss": 6.842915058135986
    },
    {
      "epoch": 0.3152845528455285,
      "step": 5817,
      "training_loss": 5.498743534088135
    },
    {
      "epoch": 0.31533875338753387,
      "step": 5818,
      "training_loss": 7.944902420043945
    },
    {
      "epoch": 0.3153929539295393,
      "step": 5819,
      "training_loss": 4.770860195159912
    },
    {
      "epoch": 0.3154471544715447,
      "grad_norm": 72.4189224243164,
      "learning_rate": 1e-05,
      "loss": 6.2644,
      "step": 5820
    },
    {
      "epoch": 0.3154471544715447,
      "step": 5820,
      "training_loss": 6.06373929977417
    },
    {
      "epoch": 0.31550135501355014,
      "step": 5821,
      "training_loss": 5.523509979248047
    },
    {
      "epoch": 0.31555555555555553,
      "step": 5822,
      "training_loss": 7.252439975738525
    },
    {
      "epoch": 0.315609756097561,
      "step": 5823,
      "training_loss": 6.740824222564697
    },
    {
      "epoch": 0.3156639566395664,
      "grad_norm": 26.553478240966797,
      "learning_rate": 1e-05,
      "loss": 6.3951,
      "step": 5824
    },
    {
      "epoch": 0.3156639566395664,
      "step": 5824,
      "training_loss": 11.267218589782715
    },
    {
      "epoch": 0.3157181571815718,
      "step": 5825,
      "training_loss": 6.311057090759277
    },
    {
      "epoch": 0.31577235772357726,
      "step": 5826,
      "training_loss": 6.704049110412598
    },
    {
      "epoch": 0.31582655826558265,
      "step": 5827,
      "training_loss": 6.219981670379639
    },
    {
      "epoch": 0.3158807588075881,
      "grad_norm": 26.473173141479492,
      "learning_rate": 1e-05,
      "loss": 7.6256,
      "step": 5828
    },
    {
      "epoch": 0.3158807588075881,
      "step": 5828,
      "training_loss": 7.710574150085449
    },
    {
      "epoch": 0.3159349593495935,
      "step": 5829,
      "training_loss": 6.91347074508667
    },
    {
      "epoch": 0.3159891598915989,
      "step": 5830,
      "training_loss": 7.192059516906738
    },
    {
      "epoch": 0.3160433604336043,
      "step": 5831,
      "training_loss": 6.113874435424805
    },
    {
      "epoch": 0.31609756097560976,
      "grad_norm": 29.36288070678711,
      "learning_rate": 1e-05,
      "loss": 6.9825,
      "step": 5832
    },
    {
      "epoch": 0.31609756097560976,
      "step": 5832,
      "training_loss": 4.775993824005127
    },
    {
      "epoch": 0.31615176151761515,
      "step": 5833,
      "training_loss": 7.3392486572265625
    },
    {
      "epoch": 0.3162059620596206,
      "step": 5834,
      "training_loss": 6.62311315536499
    },
    {
      "epoch": 0.31626016260162604,
      "step": 5835,
      "training_loss": 7.8624267578125
    },
    {
      "epoch": 0.3163143631436314,
      "grad_norm": 30.216135025024414,
      "learning_rate": 1e-05,
      "loss": 6.6502,
      "step": 5836
    },
    {
      "epoch": 0.3163143631436314,
      "step": 5836,
      "training_loss": 4.78987979888916
    },
    {
      "epoch": 0.31636856368563687,
      "step": 5837,
      "training_loss": 7.154413223266602
    },
    {
      "epoch": 0.31642276422764226,
      "step": 5838,
      "training_loss": 6.834336280822754
    },
    {
      "epoch": 0.3164769647696477,
      "step": 5839,
      "training_loss": 4.923222064971924
    },
    {
      "epoch": 0.3165311653116531,
      "grad_norm": 69.00007629394531,
      "learning_rate": 1e-05,
      "loss": 5.9255,
      "step": 5840
    },
    {
      "epoch": 0.3165311653116531,
      "step": 5840,
      "training_loss": 7.296465873718262
    },
    {
      "epoch": 0.31658536585365854,
      "step": 5841,
      "training_loss": 7.121496677398682
    },
    {
      "epoch": 0.31663956639566393,
      "step": 5842,
      "training_loss": 5.672130107879639
    },
    {
      "epoch": 0.3166937669376694,
      "step": 5843,
      "training_loss": 6.694658279418945
    },
    {
      "epoch": 0.3167479674796748,
      "grad_norm": 24.16042137145996,
      "learning_rate": 1e-05,
      "loss": 6.6962,
      "step": 5844
    },
    {
      "epoch": 0.3167479674796748,
      "step": 5844,
      "training_loss": 5.461847305297852
    },
    {
      "epoch": 0.3168021680216802,
      "step": 5845,
      "training_loss": 6.7026238441467285
    },
    {
      "epoch": 0.31685636856368565,
      "step": 5846,
      "training_loss": 6.57052755355835
    },
    {
      "epoch": 0.31691056910569104,
      "step": 5847,
      "training_loss": 6.900490760803223
    },
    {
      "epoch": 0.3169647696476965,
      "grad_norm": 17.34479331970215,
      "learning_rate": 1e-05,
      "loss": 6.4089,
      "step": 5848
    },
    {
      "epoch": 0.3169647696476965,
      "step": 5848,
      "training_loss": 6.513444423675537
    },
    {
      "epoch": 0.3170189701897019,
      "step": 5849,
      "training_loss": 6.666845798492432
    },
    {
      "epoch": 0.3170731707317073,
      "step": 5850,
      "training_loss": 7.298574924468994
    },
    {
      "epoch": 0.3171273712737127,
      "step": 5851,
      "training_loss": 5.02549409866333
    },
    {
      "epoch": 0.31718157181571816,
      "grad_norm": 58.74439239501953,
      "learning_rate": 1e-05,
      "loss": 6.3761,
      "step": 5852
    },
    {
      "epoch": 0.31718157181571816,
      "step": 5852,
      "training_loss": 8.06676959991455
    },
    {
      "epoch": 0.3172357723577236,
      "step": 5853,
      "training_loss": 7.628046989440918
    },
    {
      "epoch": 0.317289972899729,
      "step": 5854,
      "training_loss": 8.051897048950195
    },
    {
      "epoch": 0.31734417344173443,
      "step": 5855,
      "training_loss": 6.780163764953613
    },
    {
      "epoch": 0.3173983739837398,
      "grad_norm": 17.77842903137207,
      "learning_rate": 1e-05,
      "loss": 7.6317,
      "step": 5856
    },
    {
      "epoch": 0.3173983739837398,
      "step": 5856,
      "training_loss": 4.569359302520752
    },
    {
      "epoch": 0.31745257452574527,
      "step": 5857,
      "training_loss": 5.626171112060547
    },
    {
      "epoch": 0.31750677506775066,
      "step": 5858,
      "training_loss": 6.286734580993652
    },
    {
      "epoch": 0.3175609756097561,
      "step": 5859,
      "training_loss": 6.929113388061523
    },
    {
      "epoch": 0.3176151761517615,
      "grad_norm": 23.13702964782715,
      "learning_rate": 1e-05,
      "loss": 5.8528,
      "step": 5860
    },
    {
      "epoch": 0.3176151761517615,
      "step": 5860,
      "training_loss": 5.806484222412109
    },
    {
      "epoch": 0.31766937669376694,
      "step": 5861,
      "training_loss": 5.913209915161133
    },
    {
      "epoch": 0.3177235772357724,
      "step": 5862,
      "training_loss": 7.151899814605713
    },
    {
      "epoch": 0.31777777777777777,
      "step": 5863,
      "training_loss": 6.612955570220947
    },
    {
      "epoch": 0.3178319783197832,
      "grad_norm": 21.074583053588867,
      "learning_rate": 1e-05,
      "loss": 6.3711,
      "step": 5864
    },
    {
      "epoch": 0.3178319783197832,
      "step": 5864,
      "training_loss": 8.574155807495117
    },
    {
      "epoch": 0.3178861788617886,
      "step": 5865,
      "training_loss": 6.501039028167725
    },
    {
      "epoch": 0.31794037940379405,
      "step": 5866,
      "training_loss": 7.505618572235107
    },
    {
      "epoch": 0.31799457994579944,
      "step": 5867,
      "training_loss": 5.319931983947754
    },
    {
      "epoch": 0.3180487804878049,
      "grad_norm": 20.204391479492188,
      "learning_rate": 1e-05,
      "loss": 6.9752,
      "step": 5868
    },
    {
      "epoch": 0.3180487804878049,
      "step": 5868,
      "training_loss": 5.152632713317871
    },
    {
      "epoch": 0.31810298102981027,
      "step": 5869,
      "training_loss": 5.940826892852783
    },
    {
      "epoch": 0.3181571815718157,
      "step": 5870,
      "training_loss": 7.489578723907471
    },
    {
      "epoch": 0.31821138211382116,
      "step": 5871,
      "training_loss": 7.042148590087891
    },
    {
      "epoch": 0.31826558265582655,
      "grad_norm": 22.611831665039062,
      "learning_rate": 1e-05,
      "loss": 6.4063,
      "step": 5872
    },
    {
      "epoch": 0.31826558265582655,
      "step": 5872,
      "training_loss": 7.6668314933776855
    },
    {
      "epoch": 0.318319783197832,
      "step": 5873,
      "training_loss": 7.248950481414795
    },
    {
      "epoch": 0.3183739837398374,
      "step": 5874,
      "training_loss": 6.897092819213867
    },
    {
      "epoch": 0.31842818428184283,
      "step": 5875,
      "training_loss": 7.705967903137207
    },
    {
      "epoch": 0.3184823848238482,
      "grad_norm": 17.765077590942383,
      "learning_rate": 1e-05,
      "loss": 7.3797,
      "step": 5876
    },
    {
      "epoch": 0.3184823848238482,
      "step": 5876,
      "training_loss": 5.937889575958252
    },
    {
      "epoch": 0.31853658536585366,
      "step": 5877,
      "training_loss": 6.413964748382568
    },
    {
      "epoch": 0.31859078590785905,
      "step": 5878,
      "training_loss": 7.2751922607421875
    },
    {
      "epoch": 0.3186449864498645,
      "step": 5879,
      "training_loss": 5.699212551116943
    },
    {
      "epoch": 0.31869918699186994,
      "grad_norm": 40.64456558227539,
      "learning_rate": 1e-05,
      "loss": 6.3316,
      "step": 5880
    },
    {
      "epoch": 0.31869918699186994,
      "step": 5880,
      "training_loss": 7.298428058624268
    },
    {
      "epoch": 0.31875338753387533,
      "step": 5881,
      "training_loss": 6.805619239807129
    },
    {
      "epoch": 0.3188075880758808,
      "step": 5882,
      "training_loss": 7.731278419494629
    },
    {
      "epoch": 0.31886178861788617,
      "step": 5883,
      "training_loss": 7.825931072235107
    },
    {
      "epoch": 0.3189159891598916,
      "grad_norm": 68.83385467529297,
      "learning_rate": 1e-05,
      "loss": 7.4153,
      "step": 5884
    },
    {
      "epoch": 0.3189159891598916,
      "step": 5884,
      "training_loss": 6.804432392120361
    },
    {
      "epoch": 0.318970189701897,
      "step": 5885,
      "training_loss": 6.3274359703063965
    },
    {
      "epoch": 0.31902439024390244,
      "step": 5886,
      "training_loss": 6.979660511016846
    },
    {
      "epoch": 0.31907859078590783,
      "step": 5887,
      "training_loss": 7.149112224578857
    },
    {
      "epoch": 0.3191327913279133,
      "grad_norm": 21.761354446411133,
      "learning_rate": 1e-05,
      "loss": 6.8152,
      "step": 5888
    },
    {
      "epoch": 0.3191327913279133,
      "step": 5888,
      "training_loss": 8.22008991241455
    },
    {
      "epoch": 0.3191869918699187,
      "step": 5889,
      "training_loss": 5.448006629943848
    },
    {
      "epoch": 0.3192411924119241,
      "step": 5890,
      "training_loss": 8.100433349609375
    },
    {
      "epoch": 0.31929539295392956,
      "step": 5891,
      "training_loss": 6.4691362380981445
    },
    {
      "epoch": 0.31934959349593495,
      "grad_norm": 23.328535079956055,
      "learning_rate": 1e-05,
      "loss": 7.0594,
      "step": 5892
    },
    {
      "epoch": 0.31934959349593495,
      "step": 5892,
      "training_loss": 6.8321123123168945
    },
    {
      "epoch": 0.3194037940379404,
      "step": 5893,
      "training_loss": 6.7137932777404785
    },
    {
      "epoch": 0.3194579945799458,
      "step": 5894,
      "training_loss": 6.974546432495117
    },
    {
      "epoch": 0.3195121951219512,
      "step": 5895,
      "training_loss": 6.467870235443115
    },
    {
      "epoch": 0.3195663956639566,
      "grad_norm": 22.96050453186035,
      "learning_rate": 1e-05,
      "loss": 6.7471,
      "step": 5896
    },
    {
      "epoch": 0.3195663956639566,
      "step": 5896,
      "training_loss": 5.113171577453613
    },
    {
      "epoch": 0.31962059620596206,
      "step": 5897,
      "training_loss": 6.435691833496094
    },
    {
      "epoch": 0.3196747967479675,
      "step": 5898,
      "training_loss": 6.466738224029541
    },
    {
      "epoch": 0.3197289972899729,
      "step": 5899,
      "training_loss": 7.712418556213379
    },
    {
      "epoch": 0.31978319783197834,
      "grad_norm": 35.82182312011719,
      "learning_rate": 1e-05,
      "loss": 6.432,
      "step": 5900
    },
    {
      "epoch": 0.31978319783197834,
      "step": 5900,
      "training_loss": 6.475142955780029
    },
    {
      "epoch": 0.31983739837398373,
      "step": 5901,
      "training_loss": 7.726949691772461
    },
    {
      "epoch": 0.3198915989159892,
      "step": 5902,
      "training_loss": 8.509922981262207
    },
    {
      "epoch": 0.31994579945799456,
      "step": 5903,
      "training_loss": 6.493100643157959
    },
    {
      "epoch": 0.32,
      "grad_norm": 19.61806297302246,
      "learning_rate": 1e-05,
      "loss": 7.3013,
      "step": 5904
    },
    {
      "epoch": 0.32,
      "step": 5904,
      "training_loss": 7.08864688873291
    },
    {
      "epoch": 0.3200542005420054,
      "step": 5905,
      "training_loss": 7.151829242706299
    },
    {
      "epoch": 0.32010840108401084,
      "step": 5906,
      "training_loss": 6.58980131149292
    },
    {
      "epoch": 0.3201626016260163,
      "step": 5907,
      "training_loss": 7.5547895431518555
    },
    {
      "epoch": 0.3202168021680217,
      "grad_norm": 45.1436767578125,
      "learning_rate": 1e-05,
      "loss": 7.0963,
      "step": 5908
    },
    {
      "epoch": 0.3202168021680217,
      "step": 5908,
      "training_loss": 5.922755718231201
    },
    {
      "epoch": 0.3202710027100271,
      "step": 5909,
      "training_loss": 7.355559825897217
    },
    {
      "epoch": 0.3203252032520325,
      "step": 5910,
      "training_loss": 6.828775405883789
    },
    {
      "epoch": 0.32037940379403795,
      "step": 5911,
      "training_loss": 6.866682529449463
    },
    {
      "epoch": 0.32043360433604334,
      "grad_norm": 31.493366241455078,
      "learning_rate": 1e-05,
      "loss": 6.7434,
      "step": 5912
    },
    {
      "epoch": 0.32043360433604334,
      "step": 5912,
      "training_loss": 4.105734348297119
    },
    {
      "epoch": 0.3204878048780488,
      "step": 5913,
      "training_loss": 7.107813835144043
    },
    {
      "epoch": 0.3205420054200542,
      "step": 5914,
      "training_loss": 5.569887638092041
    },
    {
      "epoch": 0.3205962059620596,
      "step": 5915,
      "training_loss": 6.881999492645264
    },
    {
      "epoch": 0.32065040650406507,
      "grad_norm": 19.699813842773438,
      "learning_rate": 1e-05,
      "loss": 5.9164,
      "step": 5916
    },
    {
      "epoch": 0.32065040650406507,
      "step": 5916,
      "training_loss": 7.782650470733643
    },
    {
      "epoch": 0.32070460704607046,
      "step": 5917,
      "training_loss": 8.119726181030273
    },
    {
      "epoch": 0.3207588075880759,
      "step": 5918,
      "training_loss": 6.7699785232543945
    },
    {
      "epoch": 0.3208130081300813,
      "step": 5919,
      "training_loss": 6.616827487945557
    },
    {
      "epoch": 0.32086720867208673,
      "grad_norm": 20.829360961914062,
      "learning_rate": 1e-05,
      "loss": 7.3223,
      "step": 5920
    },
    {
      "epoch": 0.32086720867208673,
      "step": 5920,
      "training_loss": 7.041062355041504
    },
    {
      "epoch": 0.3209214092140921,
      "step": 5921,
      "training_loss": 8.50377082824707
    },
    {
      "epoch": 0.32097560975609757,
      "step": 5922,
      "training_loss": 7.007730007171631
    },
    {
      "epoch": 0.32102981029810296,
      "step": 5923,
      "training_loss": 9.507293701171875
    },
    {
      "epoch": 0.3210840108401084,
      "grad_norm": 62.89863204956055,
      "learning_rate": 1e-05,
      "loss": 8.015,
      "step": 5924
    },
    {
      "epoch": 0.3210840108401084,
      "step": 5924,
      "training_loss": 7.601363658905029
    },
    {
      "epoch": 0.32113821138211385,
      "step": 5925,
      "training_loss": 5.908513069152832
    },
    {
      "epoch": 0.32119241192411924,
      "step": 5926,
      "training_loss": 6.979511260986328
    },
    {
      "epoch": 0.3212466124661247,
      "step": 5927,
      "training_loss": 7.225335597991943
    },
    {
      "epoch": 0.32130081300813007,
      "grad_norm": 19.092966079711914,
      "learning_rate": 1e-05,
      "loss": 6.9287,
      "step": 5928
    },
    {
      "epoch": 0.32130081300813007,
      "step": 5928,
      "training_loss": 6.8022541999816895
    },
    {
      "epoch": 0.3213550135501355,
      "step": 5929,
      "training_loss": 7.5333404541015625
    },
    {
      "epoch": 0.3214092140921409,
      "step": 5930,
      "training_loss": 5.556742191314697
    },
    {
      "epoch": 0.32146341463414635,
      "step": 5931,
      "training_loss": 7.535274505615234
    },
    {
      "epoch": 0.32151761517615174,
      "grad_norm": 38.99094772338867,
      "learning_rate": 1e-05,
      "loss": 6.8569,
      "step": 5932
    },
    {
      "epoch": 0.32151761517615174,
      "step": 5932,
      "training_loss": 8.799294471740723
    },
    {
      "epoch": 0.3215718157181572,
      "step": 5933,
      "training_loss": 6.37860107421875
    },
    {
      "epoch": 0.32162601626016263,
      "step": 5934,
      "training_loss": 5.1690568923950195
    },
    {
      "epoch": 0.321680216802168,
      "step": 5935,
      "training_loss": 7.376804351806641
    },
    {
      "epoch": 0.32173441734417346,
      "grad_norm": 43.752742767333984,
      "learning_rate": 1e-05,
      "loss": 6.9309,
      "step": 5936
    },
    {
      "epoch": 0.32173441734417346,
      "step": 5936,
      "training_loss": 6.840096473693848
    },
    {
      "epoch": 0.32178861788617885,
      "step": 5937,
      "training_loss": 5.280869960784912
    },
    {
      "epoch": 0.3218428184281843,
      "step": 5938,
      "training_loss": 6.969644069671631
    },
    {
      "epoch": 0.3218970189701897,
      "step": 5939,
      "training_loss": 6.431957244873047
    },
    {
      "epoch": 0.32195121951219513,
      "grad_norm": 47.9928092956543,
      "learning_rate": 1e-05,
      "loss": 6.3806,
      "step": 5940
    },
    {
      "epoch": 0.32195121951219513,
      "step": 5940,
      "training_loss": 7.174919605255127
    },
    {
      "epoch": 0.3220054200542005,
      "step": 5941,
      "training_loss": 6.889699935913086
    },
    {
      "epoch": 0.32205962059620596,
      "step": 5942,
      "training_loss": 6.3323655128479
    },
    {
      "epoch": 0.3221138211382114,
      "step": 5943,
      "training_loss": 7.808323383331299
    },
    {
      "epoch": 0.3221680216802168,
      "grad_norm": 43.655914306640625,
      "learning_rate": 1e-05,
      "loss": 7.0513,
      "step": 5944
    },
    {
      "epoch": 0.3221680216802168,
      "step": 5944,
      "training_loss": 7.0808634757995605
    },
    {
      "epoch": 0.32222222222222224,
      "step": 5945,
      "training_loss": 6.415621757507324
    },
    {
      "epoch": 0.32227642276422763,
      "step": 5946,
      "training_loss": 5.71527624130249
    },
    {
      "epoch": 0.3223306233062331,
      "step": 5947,
      "training_loss": 6.594751358032227
    },
    {
      "epoch": 0.32238482384823847,
      "grad_norm": 33.094947814941406,
      "learning_rate": 1e-05,
      "loss": 6.4516,
      "step": 5948
    },
    {
      "epoch": 0.32238482384823847,
      "step": 5948,
      "training_loss": 6.581784725189209
    },
    {
      "epoch": 0.3224390243902439,
      "step": 5949,
      "training_loss": 6.839021682739258
    },
    {
      "epoch": 0.3224932249322493,
      "step": 5950,
      "training_loss": 7.4682793617248535
    },
    {
      "epoch": 0.32254742547425475,
      "step": 5951,
      "training_loss": 6.239390850067139
    },
    {
      "epoch": 0.3226016260162602,
      "grad_norm": 27.238805770874023,
      "learning_rate": 1e-05,
      "loss": 6.7821,
      "step": 5952
    },
    {
      "epoch": 0.3226016260162602,
      "step": 5952,
      "training_loss": 7.336747646331787
    },
    {
      "epoch": 0.3226558265582656,
      "step": 5953,
      "training_loss": 5.941938877105713
    },
    {
      "epoch": 0.322710027100271,
      "step": 5954,
      "training_loss": 3.9204800128936768
    },
    {
      "epoch": 0.3227642276422764,
      "step": 5955,
      "training_loss": 6.811975955963135
    },
    {
      "epoch": 0.32281842818428186,
      "grad_norm": 31.048606872558594,
      "learning_rate": 1e-05,
      "loss": 6.0028,
      "step": 5956
    },
    {
      "epoch": 0.32281842818428186,
      "step": 5956,
      "training_loss": 6.369053840637207
    },
    {
      "epoch": 0.32287262872628725,
      "step": 5957,
      "training_loss": 6.846630573272705
    },
    {
      "epoch": 0.3229268292682927,
      "step": 5958,
      "training_loss": 5.370584964752197
    },
    {
      "epoch": 0.3229810298102981,
      "step": 5959,
      "training_loss": 5.332876682281494
    },
    {
      "epoch": 0.3230352303523035,
      "grad_norm": 30.726064682006836,
      "learning_rate": 1e-05,
      "loss": 5.9798,
      "step": 5960
    },
    {
      "epoch": 0.3230352303523035,
      "step": 5960,
      "training_loss": 7.109951972961426
    },
    {
      "epoch": 0.3230894308943089,
      "step": 5961,
      "training_loss": 9.305908203125
    },
    {
      "epoch": 0.32314363143631436,
      "step": 5962,
      "training_loss": 5.186137676239014
    },
    {
      "epoch": 0.3231978319783198,
      "step": 5963,
      "training_loss": 6.467779636383057
    },
    {
      "epoch": 0.3232520325203252,
      "grad_norm": 22.865385055541992,
      "learning_rate": 1e-05,
      "loss": 7.0174,
      "step": 5964
    },
    {
      "epoch": 0.3232520325203252,
      "step": 5964,
      "training_loss": 7.366249084472656
    },
    {
      "epoch": 0.32330623306233064,
      "step": 5965,
      "training_loss": 7.091031551361084
    },
    {
      "epoch": 0.32336043360433603,
      "step": 5966,
      "training_loss": 5.874504566192627
    },
    {
      "epoch": 0.3234146341463415,
      "step": 5967,
      "training_loss": 5.9192728996276855
    },
    {
      "epoch": 0.32346883468834686,
      "grad_norm": 25.73246955871582,
      "learning_rate": 1e-05,
      "loss": 6.5628,
      "step": 5968
    },
    {
      "epoch": 0.32346883468834686,
      "step": 5968,
      "training_loss": 6.849892616271973
    },
    {
      "epoch": 0.3235230352303523,
      "step": 5969,
      "training_loss": 6.4188456535339355
    },
    {
      "epoch": 0.3235772357723577,
      "step": 5970,
      "training_loss": 7.833365440368652
    },
    {
      "epoch": 0.32363143631436314,
      "step": 5971,
      "training_loss": 6.230817794799805
    },
    {
      "epoch": 0.3236856368563686,
      "grad_norm": 24.091510772705078,
      "learning_rate": 1e-05,
      "loss": 6.8332,
      "step": 5972
    },
    {
      "epoch": 0.3236856368563686,
      "step": 5972,
      "training_loss": 6.90035343170166
    },
    {
      "epoch": 0.323739837398374,
      "step": 5973,
      "training_loss": 5.66616678237915
    },
    {
      "epoch": 0.3237940379403794,
      "step": 5974,
      "training_loss": 7.194713592529297
    },
    {
      "epoch": 0.3238482384823848,
      "step": 5975,
      "training_loss": 6.297544956207275
    },
    {
      "epoch": 0.32390243902439025,
      "grad_norm": 19.37042999267578,
      "learning_rate": 1e-05,
      "loss": 6.5147,
      "step": 5976
    },
    {
      "epoch": 0.32390243902439025,
      "step": 5976,
      "training_loss": 5.767319202423096
    },
    {
      "epoch": 0.32395663956639564,
      "step": 5977,
      "training_loss": 7.182741641998291
    },
    {
      "epoch": 0.3240108401084011,
      "step": 5978,
      "training_loss": 6.630466461181641
    },
    {
      "epoch": 0.3240650406504065,
      "step": 5979,
      "training_loss": 7.34871768951416
    },
    {
      "epoch": 0.3241192411924119,
      "grad_norm": 20.015714645385742,
      "learning_rate": 1e-05,
      "loss": 6.7323,
      "step": 5980
    },
    {
      "epoch": 0.3241192411924119,
      "step": 5980,
      "training_loss": 7.975666046142578
    },
    {
      "epoch": 0.32417344173441737,
      "step": 5981,
      "training_loss": 6.6924262046813965
    },
    {
      "epoch": 0.32422764227642276,
      "step": 5982,
      "training_loss": 5.228850841522217
    },
    {
      "epoch": 0.3242818428184282,
      "step": 5983,
      "training_loss": 8.242481231689453
    },
    {
      "epoch": 0.3243360433604336,
      "grad_norm": 24.73629379272461,
      "learning_rate": 1e-05,
      "loss": 7.0349,
      "step": 5984
    },
    {
      "epoch": 0.3243360433604336,
      "step": 5984,
      "training_loss": 6.960901737213135
    },
    {
      "epoch": 0.32439024390243903,
      "step": 5985,
      "training_loss": 8.45429515838623
    },
    {
      "epoch": 0.3244444444444444,
      "step": 5986,
      "training_loss": 6.409183025360107
    },
    {
      "epoch": 0.32449864498644987,
      "step": 5987,
      "training_loss": 6.450237274169922
    },
    {
      "epoch": 0.32455284552845526,
      "grad_norm": 19.96232795715332,
      "learning_rate": 1e-05,
      "loss": 7.0687,
      "step": 5988
    },
    {
      "epoch": 0.32455284552845526,
      "step": 5988,
      "training_loss": 7.871325492858887
    },
    {
      "epoch": 0.3246070460704607,
      "step": 5989,
      "training_loss": 7.060184001922607
    },
    {
      "epoch": 0.32466124661246615,
      "step": 5990,
      "training_loss": 6.10964298248291
    },
    {
      "epoch": 0.32471544715447154,
      "step": 5991,
      "training_loss": 6.8624677658081055
    },
    {
      "epoch": 0.324769647696477,
      "grad_norm": 29.2587890625,
      "learning_rate": 1e-05,
      "loss": 6.9759,
      "step": 5992
    },
    {
      "epoch": 0.324769647696477,
      "step": 5992,
      "training_loss": 6.718069553375244
    },
    {
      "epoch": 0.32482384823848237,
      "step": 5993,
      "training_loss": 5.554644584655762
    },
    {
      "epoch": 0.3248780487804878,
      "step": 5994,
      "training_loss": 7.989099502563477
    },
    {
      "epoch": 0.3249322493224932,
      "step": 5995,
      "training_loss": 6.593599319458008
    },
    {
      "epoch": 0.32498644986449865,
      "grad_norm": 32.938838958740234,
      "learning_rate": 1e-05,
      "loss": 6.7139,
      "step": 5996
    },
    {
      "epoch": 0.32498644986449865,
      "step": 5996,
      "training_loss": 6.432736873626709
    },
    {
      "epoch": 0.32504065040650404,
      "step": 5997,
      "training_loss": 6.925594329833984
    },
    {
      "epoch": 0.3250948509485095,
      "step": 5998,
      "training_loss": 7.199916839599609
    },
    {
      "epoch": 0.32514905149051493,
      "step": 5999,
      "training_loss": 7.935614109039307
    },
    {
      "epoch": 0.3252032520325203,
      "grad_norm": 36.047637939453125,
      "learning_rate": 1e-05,
      "loss": 7.1235,
      "step": 6000
    },
    {
      "epoch": 0.3252032520325203,
      "eval_runtime": 462.0402,
      "eval_samples_per_second": 4.437,
      "eval_steps_per_second": 4.437,
      "step": 6000
    }
  ],
  "logging_steps": 4,
  "max_steps": 18450,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 6000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 3.0409370855424e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
