{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.6504065040650406,
  "eval_steps": 6000,
  "global_step": 12000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0,
      "step": 0,
      "training_loss": 11.111725807189941
    },
    {
      "epoch": 5.4200542005420054e-05,
      "step": 1,
      "training_loss": 13.243904113769531
    },
    {
      "epoch": 0.00010840108401084011,
      "step": 2,
      "training_loss": 12.016393661499023
    },
    {
      "epoch": 0.00016260162601626016,
      "step": 3,
      "training_loss": 12.524957656860352
    },
    {
      "epoch": 0.00021680216802168022,
      "grad_norm": NaN,
      "learning_rate": 1e-05,
      "loss": 12.2242,
      "step": 4
    },
    {
      "epoch": 0.00021680216802168022,
      "step": 4,
      "training_loss": 12.043158531188965
    },
    {
      "epoch": 0.00027100271002710027,
      "step": 5,
      "training_loss": 12.434259414672852
    },
    {
      "epoch": 0.0003252032520325203,
      "step": 6,
      "training_loss": 14.311711311340332
    },
    {
      "epoch": 0.0003794037940379404,
      "step": 7,
      "training_loss": 11.889445304870605
    },
    {
      "epoch": 0.00043360433604336043,
      "grad_norm": NaN,
      "learning_rate": 1e-05,
      "loss": 12.6696,
      "step": 8
    },
    {
      "epoch": 0.00043360433604336043,
      "step": 8,
      "training_loss": 11.8179292678833
    },
    {
      "epoch": 0.0004878048780487805,
      "step": 9,
      "training_loss": 12.8345308303833
    },
    {
      "epoch": 0.0005420054200542005,
      "step": 10,
      "training_loss": 12.50240707397461
    },
    {
      "epoch": 0.0005962059620596206,
      "step": 11,
      "training_loss": 12.181693077087402
    },
    {
      "epoch": 0.0006504065040650406,
      "grad_norm": 31.525789260864258,
      "learning_rate": 1e-05,
      "loss": 12.3341,
      "step": 12
    },
    {
      "epoch": 0.0006504065040650406,
      "step": 12,
      "training_loss": 13.70712661743164
    },
    {
      "epoch": 0.0007046070460704607,
      "step": 13,
      "training_loss": 11.968474388122559
    },
    {
      "epoch": 0.0007588075880758808,
      "step": 14,
      "training_loss": 11.70648193359375
    },
    {
      "epoch": 0.0008130081300813008,
      "step": 15,
      "training_loss": 12.483585357666016
    },
    {
      "epoch": 0.0008672086720867209,
      "grad_norm": 109.42294311523438,
      "learning_rate": 1e-05,
      "loss": 12.4664,
      "step": 16
    },
    {
      "epoch": 0.0008672086720867209,
      "step": 16,
      "training_loss": 14.522932052612305
    },
    {
      "epoch": 0.0009214092140921409,
      "step": 17,
      "training_loss": 12.172041893005371
    },
    {
      "epoch": 0.000975609756097561,
      "step": 18,
      "training_loss": 11.292916297912598
    },
    {
      "epoch": 0.001029810298102981,
      "step": 19,
      "training_loss": 11.767900466918945
    },
    {
      "epoch": 0.001084010840108401,
      "grad_norm": 64.20651245117188,
      "learning_rate": 1e-05,
      "loss": 12.4389,
      "step": 20
    },
    {
      "epoch": 0.001084010840108401,
      "step": 20,
      "training_loss": 12.513604164123535
    },
    {
      "epoch": 0.0011382113821138211,
      "step": 21,
      "training_loss": 11.945090293884277
    },
    {
      "epoch": 0.0011924119241192412,
      "step": 22,
      "training_loss": 12.44311237335205
    },
    {
      "epoch": 0.0012466124661246612,
      "step": 23,
      "training_loss": 23.766647338867188
    },
    {
      "epoch": 0.0013008130081300813,
      "grad_norm": NaN,
      "learning_rate": 1e-05,
      "loss": 15.1671,
      "step": 24
    },
    {
      "epoch": 0.0013008130081300813,
      "step": 24,
      "training_loss": 12.468871116638184
    },
    {
      "epoch": 0.0013550135501355014,
      "step": 25,
      "training_loss": 11.110321044921875
    },
    {
      "epoch": 0.0014092140921409214,
      "step": 26,
      "training_loss": 12.302184104919434
    },
    {
      "epoch": 0.0014634146341463415,
      "step": 27,
      "training_loss": 10.503989219665527
    },
    {
      "epoch": 0.0015176151761517615,
      "grad_norm": 98.07079315185547,
      "learning_rate": 1e-05,
      "loss": 11.5963,
      "step": 28
    },
    {
      "epoch": 0.0015176151761517615,
      "step": 28,
      "training_loss": 12.163992881774902
    },
    {
      "epoch": 0.0015718157181571816,
      "step": 29,
      "training_loss": 11.964346885681152
    },
    {
      "epoch": 0.0016260162601626016,
      "step": 30,
      "training_loss": 18.407920837402344
    },
    {
      "epoch": 0.0016802168021680217,
      "step": 31,
      "training_loss": 11.686904907226562
    },
    {
      "epoch": 0.0017344173441734417,
      "grad_norm": 36.55788040161133,
      "learning_rate": 1e-05,
      "loss": 13.5558,
      "step": 32
    },
    {
      "epoch": 0.0017344173441734417,
      "step": 32,
      "training_loss": 12.702886581420898
    },
    {
      "epoch": 0.0017886178861788618,
      "step": 33,
      "training_loss": 12.914487838745117
    },
    {
      "epoch": 0.0018428184281842818,
      "step": 34,
      "training_loss": 10.897786140441895
    },
    {
      "epoch": 0.001897018970189702,
      "step": 35,
      "training_loss": 11.767158508300781
    },
    {
      "epoch": 0.001951219512195122,
      "grad_norm": 65.15444946289062,
      "learning_rate": 1e-05,
      "loss": 12.0706,
      "step": 36
    },
    {
      "epoch": 0.001951219512195122,
      "step": 36,
      "training_loss": 12.365867614746094
    },
    {
      "epoch": 0.002005420054200542,
      "step": 37,
      "training_loss": 11.822869300842285
    },
    {
      "epoch": 0.002059620596205962,
      "step": 38,
      "training_loss": 12.36075496673584
    },
    {
      "epoch": 0.002113821138211382,
      "step": 39,
      "training_loss": 12.555594444274902
    },
    {
      "epoch": 0.002168021680216802,
      "grad_norm": 37.7735710144043,
      "learning_rate": 1e-05,
      "loss": 12.2763,
      "step": 40
    },
    {
      "epoch": 0.002168021680216802,
      "step": 40,
      "training_loss": 11.8652925491333
    },
    {
      "epoch": 0.0022222222222222222,
      "step": 41,
      "training_loss": 11.969447135925293
    },
    {
      "epoch": 0.0022764227642276423,
      "step": 42,
      "training_loss": 12.587841033935547
    },
    {
      "epoch": 0.0023306233062330623,
      "step": 43,
      "training_loss": 12.134407997131348
    },
    {
      "epoch": 0.0023848238482384824,
      "grad_norm": 16.635469436645508,
      "learning_rate": 1e-05,
      "loss": 12.1392,
      "step": 44
    },
    {
      "epoch": 0.0023848238482384824,
      "step": 44,
      "training_loss": 11.510613441467285
    },
    {
      "epoch": 0.0024390243902439024,
      "step": 45,
      "training_loss": 12.909424781799316
    },
    {
      "epoch": 0.0024932249322493225,
      "step": 46,
      "training_loss": 11.108716011047363
    },
    {
      "epoch": 0.0025474254742547425,
      "step": 47,
      "training_loss": 11.486334800720215
    },
    {
      "epoch": 0.0026016260162601626,
      "grad_norm": 115.90557861328125,
      "learning_rate": 1e-05,
      "loss": 11.7538,
      "step": 48
    },
    {
      "epoch": 0.0026016260162601626,
      "step": 48,
      "training_loss": 10.617513656616211
    },
    {
      "epoch": 0.0026558265582655827,
      "step": 49,
      "training_loss": 11.362805366516113
    },
    {
      "epoch": 0.0027100271002710027,
      "step": 50,
      "training_loss": 13.34550666809082
    },
    {
      "epoch": 0.0027642276422764228,
      "step": 51,
      "training_loss": 13.236893653869629
    },
    {
      "epoch": 0.002818428184281843,
      "grad_norm": NaN,
      "learning_rate": 1e-05,
      "loss": 12.1407,
      "step": 52
    },
    {
      "epoch": 0.002818428184281843,
      "step": 52,
      "training_loss": 11.147956848144531
    },
    {
      "epoch": 0.002872628726287263,
      "step": 53,
      "training_loss": 11.111977577209473
    },
    {
      "epoch": 0.002926829268292683,
      "step": 54,
      "training_loss": 11.819609642028809
    },
    {
      "epoch": 0.002981029810298103,
      "step": 55,
      "training_loss": 18.253034591674805
    },
    {
      "epoch": 0.003035230352303523,
      "grad_norm": 3510.15576171875,
      "learning_rate": 1e-05,
      "loss": 13.0831,
      "step": 56
    },
    {
      "epoch": 0.003035230352303523,
      "step": 56,
      "training_loss": 11.89055347442627
    },
    {
      "epoch": 0.003089430894308943,
      "step": 57,
      "training_loss": 12.646760940551758
    },
    {
      "epoch": 0.003143631436314363,
      "step": 58,
      "training_loss": 12.024698257446289
    },
    {
      "epoch": 0.003197831978319783,
      "step": 59,
      "training_loss": 10.462615013122559
    },
    {
      "epoch": 0.0032520325203252032,
      "grad_norm": 66.9910659790039,
      "learning_rate": 1e-05,
      "loss": 11.7562,
      "step": 60
    },
    {
      "epoch": 0.0032520325203252032,
      "step": 60,
      "training_loss": 12.814921379089355
    },
    {
      "epoch": 0.0033062330623306233,
      "step": 61,
      "training_loss": 12.584982872009277
    },
    {
      "epoch": 0.0033604336043360434,
      "step": 62,
      "training_loss": 16.339969635009766
    },
    {
      "epoch": 0.0034146341463414634,
      "step": 63,
      "training_loss": 12.473222732543945
    },
    {
      "epoch": 0.0034688346883468835,
      "grad_norm": 144.7232208251953,
      "learning_rate": 1e-05,
      "loss": 13.5533,
      "step": 64
    },
    {
      "epoch": 0.0034688346883468835,
      "step": 64,
      "training_loss": 11.977269172668457
    },
    {
      "epoch": 0.0035230352303523035,
      "step": 65,
      "training_loss": 11.061274528503418
    },
    {
      "epoch": 0.0035772357723577236,
      "step": 66,
      "training_loss": 11.066757202148438
    },
    {
      "epoch": 0.0036314363143631436,
      "step": 67,
      "training_loss": 11.54782485961914
    },
    {
      "epoch": 0.0036856368563685637,
      "grad_norm": 30.801361083984375,
      "learning_rate": 1e-05,
      "loss": 11.4133,
      "step": 68
    },
    {
      "epoch": 0.0036856368563685637,
      "step": 68,
      "training_loss": 16.31871795654297
    },
    {
      "epoch": 0.0037398373983739837,
      "step": 69,
      "training_loss": 11.002864837646484
    },
    {
      "epoch": 0.003794037940379404,
      "step": 70,
      "training_loss": 10.380319595336914
    },
    {
      "epoch": 0.003848238482384824,
      "step": 71,
      "training_loss": 13.15201473236084
    },
    {
      "epoch": 0.003902439024390244,
      "grad_norm": 26.781593322753906,
      "learning_rate": 1e-05,
      "loss": 12.7135,
      "step": 72
    },
    {
      "epoch": 0.003902439024390244,
      "step": 72,
      "training_loss": 12.929259300231934
    },
    {
      "epoch": 0.003956639566395664,
      "step": 73,
      "training_loss": 12.527139663696289
    },
    {
      "epoch": 0.004010840108401084,
      "step": 74,
      "training_loss": 13.343894004821777
    },
    {
      "epoch": 0.0040650406504065045,
      "step": 75,
      "training_loss": 11.778653144836426
    },
    {
      "epoch": 0.004119241192411924,
      "grad_norm": 13.673876762390137,
      "learning_rate": 1e-05,
      "loss": 12.6447,
      "step": 76
    },
    {
      "epoch": 0.004119241192411924,
      "step": 76,
      "training_loss": 11.3930082321167
    },
    {
      "epoch": 0.004173441734417345,
      "step": 77,
      "training_loss": 11.176904678344727
    },
    {
      "epoch": 0.004227642276422764,
      "step": 78,
      "training_loss": 11.507253646850586
    },
    {
      "epoch": 0.004281842818428185,
      "step": 79,
      "training_loss": 18.210491180419922
    },
    {
      "epoch": 0.004336043360433604,
      "grad_norm": 2676.913818359375,
      "learning_rate": 1e-05,
      "loss": 13.0719,
      "step": 80
    },
    {
      "epoch": 0.004336043360433604,
      "step": 80,
      "training_loss": 11.651453018188477
    },
    {
      "epoch": 0.004390243902439025,
      "step": 81,
      "training_loss": 11.206361770629883
    },
    {
      "epoch": 0.0044444444444444444,
      "step": 82,
      "training_loss": 11.949674606323242
    },
    {
      "epoch": 0.004498644986449865,
      "step": 83,
      "training_loss": 11.430155754089355
    },
    {
      "epoch": 0.0045528455284552845,
      "grad_norm": 51.11741638183594,
      "learning_rate": 1e-05,
      "loss": 11.5594,
      "step": 84
    },
    {
      "epoch": 0.0045528455284552845,
      "step": 84,
      "training_loss": 11.974573135375977
    },
    {
      "epoch": 0.004607046070460705,
      "step": 85,
      "training_loss": 9.924421310424805
    },
    {
      "epoch": 0.004661246612466125,
      "step": 86,
      "training_loss": 11.799753189086914
    },
    {
      "epoch": 0.004715447154471545,
      "step": 87,
      "training_loss": 11.262210845947266
    },
    {
      "epoch": 0.004769647696476965,
      "grad_norm": 27.579992294311523,
      "learning_rate": 1e-05,
      "loss": 11.2402,
      "step": 88
    },
    {
      "epoch": 0.004769647696476965,
      "step": 88,
      "training_loss": 9.01122760772705
    },
    {
      "epoch": 0.004823848238482385,
      "step": 89,
      "training_loss": 11.549857139587402
    },
    {
      "epoch": 0.004878048780487805,
      "step": 90,
      "training_loss": 11.19095516204834
    },
    {
      "epoch": 0.004932249322493225,
      "step": 91,
      "training_loss": 10.911154747009277
    },
    {
      "epoch": 0.004986449864498645,
      "grad_norm": 42.25879669189453,
      "learning_rate": 1e-05,
      "loss": 10.6658,
      "step": 92
    },
    {
      "epoch": 0.004986449864498645,
      "step": 92,
      "training_loss": 10.490349769592285
    },
    {
      "epoch": 0.0050406504065040655,
      "step": 93,
      "training_loss": 12.937756538391113
    },
    {
      "epoch": 0.005094850948509485,
      "step": 94,
      "training_loss": 13.500983238220215
    },
    {
      "epoch": 0.005149051490514906,
      "step": 95,
      "training_loss": 12.888493537902832
    },
    {
      "epoch": 0.005203252032520325,
      "grad_norm": 46.61079788208008,
      "learning_rate": 1e-05,
      "loss": 12.4544,
      "step": 96
    },
    {
      "epoch": 0.005203252032520325,
      "step": 96,
      "training_loss": 11.705912590026855
    },
    {
      "epoch": 0.005257452574525746,
      "step": 97,
      "training_loss": 12.569985389709473
    },
    {
      "epoch": 0.005311653116531165,
      "step": 98,
      "training_loss": 11.238210678100586
    },
    {
      "epoch": 0.005365853658536586,
      "step": 99,
      "training_loss": 11.309539794921875
    },
    {
      "epoch": 0.005420054200542005,
      "grad_norm": 19.945053100585938,
      "learning_rate": 1e-05,
      "loss": 11.7059,
      "step": 100
    },
    {
      "epoch": 0.005420054200542005,
      "step": 100,
      "training_loss": 10.453259468078613
    },
    {
      "epoch": 0.005474254742547426,
      "step": 101,
      "training_loss": 9.973780632019043
    },
    {
      "epoch": 0.0055284552845528455,
      "step": 102,
      "training_loss": 10.217357635498047
    },
    {
      "epoch": 0.005582655826558266,
      "step": 103,
      "training_loss": 12.519991874694824
    },
    {
      "epoch": 0.005636856368563686,
      "grad_norm": 290.6909484863281,
      "learning_rate": 1e-05,
      "loss": 10.7911,
      "step": 104
    },
    {
      "epoch": 0.005636856368563686,
      "step": 104,
      "training_loss": 11.5534029006958
    },
    {
      "epoch": 0.005691056910569106,
      "step": 105,
      "training_loss": 11.72472095489502
    },
    {
      "epoch": 0.005745257452574526,
      "step": 106,
      "training_loss": 11.177977561950684
    },
    {
      "epoch": 0.005799457994579946,
      "step": 107,
      "training_loss": 9.76594066619873
    },
    {
      "epoch": 0.005853658536585366,
      "grad_norm": 65.16187286376953,
      "learning_rate": 1e-05,
      "loss": 11.0555,
      "step": 108
    },
    {
      "epoch": 0.005853658536585366,
      "step": 108,
      "training_loss": 10.47020435333252
    },
    {
      "epoch": 0.005907859078590786,
      "step": 109,
      "training_loss": 11.763017654418945
    },
    {
      "epoch": 0.005962059620596206,
      "step": 110,
      "training_loss": 10.09343147277832
    },
    {
      "epoch": 0.0060162601626016264,
      "step": 111,
      "training_loss": 12.833083152770996
    },
    {
      "epoch": 0.006070460704607046,
      "grad_norm": 416.72174072265625,
      "learning_rate": 1e-05,
      "loss": 11.2899,
      "step": 112
    },
    {
      "epoch": 0.006070460704607046,
      "step": 112,
      "training_loss": 10.665101051330566
    },
    {
      "epoch": 0.0061246612466124666,
      "step": 113,
      "training_loss": 11.124534606933594
    },
    {
      "epoch": 0.006178861788617886,
      "step": 114,
      "training_loss": 11.358909606933594
    },
    {
      "epoch": 0.006233062330623307,
      "step": 115,
      "training_loss": 11.18017292022705
    },
    {
      "epoch": 0.006287262872628726,
      "grad_norm": 65.0464096069336,
      "learning_rate": 1e-05,
      "loss": 11.0822,
      "step": 116
    },
    {
      "epoch": 0.006287262872628726,
      "step": 116,
      "training_loss": 12.137036323547363
    },
    {
      "epoch": 0.006341463414634147,
      "step": 117,
      "training_loss": 11.771164894104004
    },
    {
      "epoch": 0.006395663956639566,
      "step": 118,
      "training_loss": 10.574539184570312
    },
    {
      "epoch": 0.006449864498644987,
      "step": 119,
      "training_loss": 11.43765926361084
    },
    {
      "epoch": 0.0065040650406504065,
      "grad_norm": 124.70130920410156,
      "learning_rate": 1e-05,
      "loss": 11.4801,
      "step": 120
    },
    {
      "epoch": 0.0065040650406504065,
      "step": 120,
      "training_loss": 11.914714813232422
    },
    {
      "epoch": 0.006558265582655827,
      "step": 121,
      "training_loss": 9.085593223571777
    },
    {
      "epoch": 0.006612466124661247,
      "step": 122,
      "training_loss": 11.181285858154297
    },
    {
      "epoch": 0.006666666666666667,
      "step": 123,
      "training_loss": 10.497374534606934
    },
    {
      "epoch": 0.006720867208672087,
      "grad_norm": 57.40266418457031,
      "learning_rate": 1e-05,
      "loss": 10.6697,
      "step": 124
    },
    {
      "epoch": 0.006720867208672087,
      "step": 124,
      "training_loss": 11.198966026306152
    },
    {
      "epoch": 0.006775067750677507,
      "step": 125,
      "training_loss": 11.80430793762207
    },
    {
      "epoch": 0.006829268292682927,
      "step": 126,
      "training_loss": 10.973591804504395
    },
    {
      "epoch": 0.006883468834688347,
      "step": 127,
      "training_loss": 9.583660125732422
    },
    {
      "epoch": 0.006937669376693767,
      "grad_norm": 47.901145935058594,
      "learning_rate": 1e-05,
      "loss": 10.8901,
      "step": 128
    },
    {
      "epoch": 0.006937669376693767,
      "step": 128,
      "training_loss": 11.993356704711914
    },
    {
      "epoch": 0.006991869918699187,
      "step": 129,
      "training_loss": 10.64016342163086
    },
    {
      "epoch": 0.007046070460704607,
      "step": 130,
      "training_loss": 10.88064956665039
    },
    {
      "epoch": 0.0071002710027100275,
      "step": 131,
      "training_loss": 10.638195991516113
    },
    {
      "epoch": 0.007154471544715447,
      "grad_norm": 25.196043014526367,
      "learning_rate": 1e-05,
      "loss": 11.0381,
      "step": 132
    },
    {
      "epoch": 0.007154471544715447,
      "step": 132,
      "training_loss": 12.720953941345215
    },
    {
      "epoch": 0.007208672086720868,
      "step": 133,
      "training_loss": 11.219255447387695
    },
    {
      "epoch": 0.007262872628726287,
      "step": 134,
      "training_loss": 10.449835777282715
    },
    {
      "epoch": 0.007317073170731708,
      "step": 135,
      "training_loss": 11.332894325256348
    },
    {
      "epoch": 0.007371273712737127,
      "grad_norm": 35.33205795288086,
      "learning_rate": 1e-05,
      "loss": 11.4307,
      "step": 136
    },
    {
      "epoch": 0.007371273712737127,
      "step": 136,
      "training_loss": 8.992963790893555
    },
    {
      "epoch": 0.007425474254742548,
      "step": 137,
      "training_loss": 10.61905288696289
    },
    {
      "epoch": 0.0074796747967479675,
      "step": 138,
      "training_loss": 10.478516578674316
    },
    {
      "epoch": 0.007533875338753388,
      "step": 139,
      "training_loss": 10.031811714172363
    },
    {
      "epoch": 0.007588075880758808,
      "grad_norm": 56.115909576416016,
      "learning_rate": 1e-05,
      "loss": 10.0306,
      "step": 140
    },
    {
      "epoch": 0.007588075880758808,
      "step": 140,
      "training_loss": 10.708312034606934
    },
    {
      "epoch": 0.007642276422764228,
      "step": 141,
      "training_loss": 12.18755054473877
    },
    {
      "epoch": 0.007696476964769648,
      "step": 142,
      "training_loss": 10.794925689697266
    },
    {
      "epoch": 0.007750677506775068,
      "step": 143,
      "training_loss": 10.226948738098145
    },
    {
      "epoch": 0.007804878048780488,
      "grad_norm": 35.30724334716797,
      "learning_rate": 1e-05,
      "loss": 10.9794,
      "step": 144
    },
    {
      "epoch": 0.007804878048780488,
      "step": 144,
      "training_loss": 11.62662124633789
    },
    {
      "epoch": 0.007859078590785908,
      "step": 145,
      "training_loss": 10.435996055603027
    },
    {
      "epoch": 0.007913279132791329,
      "step": 146,
      "training_loss": 11.219482421875
    },
    {
      "epoch": 0.007967479674796748,
      "step": 147,
      "training_loss": 10.211543083190918
    },
    {
      "epoch": 0.008021680216802168,
      "grad_norm": 66.42169952392578,
      "learning_rate": 1e-05,
      "loss": 10.8734,
      "step": 148
    },
    {
      "epoch": 0.008021680216802168,
      "step": 148,
      "training_loss": 9.528517723083496
    },
    {
      "epoch": 0.008075880758807589,
      "step": 149,
      "training_loss": 10.306105613708496
    },
    {
      "epoch": 0.008130081300813009,
      "step": 150,
      "training_loss": 11.306597709655762
    },
    {
      "epoch": 0.008184281842818428,
      "step": 151,
      "training_loss": 10.273051261901855
    },
    {
      "epoch": 0.008238482384823848,
      "grad_norm": 25.634309768676758,
      "learning_rate": 1e-05,
      "loss": 10.3536,
      "step": 152
    },
    {
      "epoch": 0.008238482384823848,
      "step": 152,
      "training_loss": 11.826397895812988
    },
    {
      "epoch": 0.008292682926829269,
      "step": 153,
      "training_loss": 10.880592346191406
    },
    {
      "epoch": 0.00834688346883469,
      "step": 154,
      "training_loss": 10.433236122131348
    },
    {
      "epoch": 0.008401084010840108,
      "step": 155,
      "training_loss": 11.639178276062012
    },
    {
      "epoch": 0.008455284552845528,
      "grad_norm": 57.76725769042969,
      "learning_rate": 1e-05,
      "loss": 11.1949,
      "step": 156
    },
    {
      "epoch": 0.008455284552845528,
      "step": 156,
      "training_loss": 10.858449935913086
    },
    {
      "epoch": 0.008509485094850949,
      "step": 157,
      "training_loss": 11.179259300231934
    },
    {
      "epoch": 0.00856368563685637,
      "step": 158,
      "training_loss": 9.746270179748535
    },
    {
      "epoch": 0.008617886178861788,
      "step": 159,
      "training_loss": 10.675082206726074
    },
    {
      "epoch": 0.008672086720867209,
      "grad_norm": 43.02224349975586,
      "learning_rate": 1e-05,
      "loss": 10.6148,
      "step": 160
    },
    {
      "epoch": 0.008672086720867209,
      "step": 160,
      "training_loss": 12.17440414428711
    },
    {
      "epoch": 0.00872628726287263,
      "step": 161,
      "training_loss": 9.399737358093262
    },
    {
      "epoch": 0.00878048780487805,
      "step": 162,
      "training_loss": 11.896190643310547
    },
    {
      "epoch": 0.008834688346883468,
      "step": 163,
      "training_loss": 9.154244422912598
    },
    {
      "epoch": 0.008888888888888889,
      "grad_norm": 24.27657127380371,
      "learning_rate": 1e-05,
      "loss": 10.6561,
      "step": 164
    },
    {
      "epoch": 0.008888888888888889,
      "step": 164,
      "training_loss": 10.401182174682617
    },
    {
      "epoch": 0.00894308943089431,
      "step": 165,
      "training_loss": 11.158220291137695
    },
    {
      "epoch": 0.00899728997289973,
      "step": 166,
      "training_loss": 10.03610610961914
    },
    {
      "epoch": 0.009051490514905149,
      "step": 167,
      "training_loss": 10.533648490905762
    },
    {
      "epoch": 0.009105691056910569,
      "grad_norm": 27.298343658447266,
      "learning_rate": 1e-05,
      "loss": 10.5323,
      "step": 168
    },
    {
      "epoch": 0.009105691056910569,
      "step": 168,
      "training_loss": 10.831672668457031
    },
    {
      "epoch": 0.00915989159891599,
      "step": 169,
      "training_loss": 9.928610801696777
    },
    {
      "epoch": 0.00921409214092141,
      "step": 170,
      "training_loss": 9.534505844116211
    },
    {
      "epoch": 0.009268292682926829,
      "step": 171,
      "training_loss": 12.548449516296387
    },
    {
      "epoch": 0.00932249322493225,
      "grad_norm": 33.712833404541016,
      "learning_rate": 1e-05,
      "loss": 10.7108,
      "step": 172
    },
    {
      "epoch": 0.00932249322493225,
      "step": 172,
      "training_loss": 9.055968284606934
    },
    {
      "epoch": 0.00937669376693767,
      "step": 173,
      "training_loss": 10.076143264770508
    },
    {
      "epoch": 0.00943089430894309,
      "step": 174,
      "training_loss": 9.921630859375
    },
    {
      "epoch": 0.009485094850948509,
      "step": 175,
      "training_loss": 11.052468299865723
    },
    {
      "epoch": 0.00953929539295393,
      "grad_norm": 29.249650955200195,
      "learning_rate": 1e-05,
      "loss": 10.0266,
      "step": 176
    },
    {
      "epoch": 0.00953929539295393,
      "step": 176,
      "training_loss": 11.840372085571289
    },
    {
      "epoch": 0.00959349593495935,
      "step": 177,
      "training_loss": 9.16879940032959
    },
    {
      "epoch": 0.00964769647696477,
      "step": 178,
      "training_loss": 9.26314926147461
    },
    {
      "epoch": 0.00970189701897019,
      "step": 179,
      "training_loss": 10.310220718383789
    },
    {
      "epoch": 0.00975609756097561,
      "grad_norm": 57.88793182373047,
      "learning_rate": 1e-05,
      "loss": 10.1456,
      "step": 180
    },
    {
      "epoch": 0.00975609756097561,
      "step": 180,
      "training_loss": 11.384123802185059
    },
    {
      "epoch": 0.00981029810298103,
      "step": 181,
      "training_loss": 11.630406379699707
    },
    {
      "epoch": 0.00986449864498645,
      "step": 182,
      "training_loss": 10.63655948638916
    },
    {
      "epoch": 0.00991869918699187,
      "step": 183,
      "training_loss": 10.648456573486328
    },
    {
      "epoch": 0.00997289972899729,
      "grad_norm": 88.28411102294922,
      "learning_rate": 1e-05,
      "loss": 11.0749,
      "step": 184
    },
    {
      "epoch": 0.00997289972899729,
      "step": 184,
      "training_loss": 10.726205825805664
    },
    {
      "epoch": 0.01002710027100271,
      "step": 185,
      "training_loss": 12.421470642089844
    },
    {
      "epoch": 0.010081300813008131,
      "step": 186,
      "training_loss": 11.398855209350586
    },
    {
      "epoch": 0.01013550135501355,
      "step": 187,
      "training_loss": 10.761733055114746
    },
    {
      "epoch": 0.01018970189701897,
      "grad_norm": 40.832733154296875,
      "learning_rate": 1e-05,
      "loss": 11.3271,
      "step": 188
    },
    {
      "epoch": 0.01018970189701897,
      "step": 188,
      "training_loss": 11.110318183898926
    },
    {
      "epoch": 0.01024390243902439,
      "step": 189,
      "training_loss": 9.783599853515625
    },
    {
      "epoch": 0.010298102981029811,
      "step": 190,
      "training_loss": 10.568955421447754
    },
    {
      "epoch": 0.01035230352303523,
      "step": 191,
      "training_loss": 10.305909156799316
    },
    {
      "epoch": 0.01040650406504065,
      "grad_norm": 28.829519271850586,
      "learning_rate": 1e-05,
      "loss": 10.4422,
      "step": 192
    },
    {
      "epoch": 0.01040650406504065,
      "step": 192,
      "training_loss": 10.539288520812988
    },
    {
      "epoch": 0.010460704607046071,
      "step": 193,
      "training_loss": 10.521219253540039
    },
    {
      "epoch": 0.010514905149051491,
      "step": 194,
      "training_loss": 11.271158218383789
    },
    {
      "epoch": 0.01056910569105691,
      "step": 195,
      "training_loss": 9.358236312866211
    },
    {
      "epoch": 0.01062330623306233,
      "grad_norm": 30.441970825195312,
      "learning_rate": 1e-05,
      "loss": 10.4225,
      "step": 196
    },
    {
      "epoch": 0.01062330623306233,
      "step": 196,
      "training_loss": 11.183821678161621
    },
    {
      "epoch": 0.010677506775067751,
      "step": 197,
      "training_loss": 9.7353515625
    },
    {
      "epoch": 0.010731707317073172,
      "step": 198,
      "training_loss": 9.843719482421875
    },
    {
      "epoch": 0.01078590785907859,
      "step": 199,
      "training_loss": 9.85303783416748
    },
    {
      "epoch": 0.01084010840108401,
      "grad_norm": 27.371170043945312,
      "learning_rate": 1e-05,
      "loss": 10.154,
      "step": 200
    },
    {
      "epoch": 0.01084010840108401,
      "step": 200,
      "training_loss": 11.018733978271484
    },
    {
      "epoch": 0.010894308943089431,
      "step": 201,
      "training_loss": 8.939705848693848
    },
    {
      "epoch": 0.010948509485094852,
      "step": 202,
      "training_loss": 9.353633880615234
    },
    {
      "epoch": 0.01100271002710027,
      "step": 203,
      "training_loss": 10.075800895690918
    },
    {
      "epoch": 0.011056910569105691,
      "grad_norm": 32.45064163208008,
      "learning_rate": 1e-05,
      "loss": 9.847,
      "step": 204
    },
    {
      "epoch": 0.011056910569105691,
      "step": 204,
      "training_loss": 9.363607406616211
    },
    {
      "epoch": 0.011111111111111112,
      "step": 205,
      "training_loss": 10.482160568237305
    },
    {
      "epoch": 0.011165311653116532,
      "step": 206,
      "training_loss": 9.114072799682617
    },
    {
      "epoch": 0.01121951219512195,
      "step": 207,
      "training_loss": 10.320244789123535
    },
    {
      "epoch": 0.011273712737127371,
      "grad_norm": 42.16585159301758,
      "learning_rate": 1e-05,
      "loss": 9.82,
      "step": 208
    },
    {
      "epoch": 0.011273712737127371,
      "step": 208,
      "training_loss": 8.485520362854004
    },
    {
      "epoch": 0.011327913279132792,
      "step": 209,
      "training_loss": 10.149017333984375
    },
    {
      "epoch": 0.011382113821138212,
      "step": 210,
      "training_loss": 9.366436958312988
    },
    {
      "epoch": 0.011436314363143631,
      "step": 211,
      "training_loss": 9.488317489624023
    },
    {
      "epoch": 0.011490514905149051,
      "grad_norm": 76.35527801513672,
      "learning_rate": 1e-05,
      "loss": 9.3723,
      "step": 212
    },
    {
      "epoch": 0.011490514905149051,
      "step": 212,
      "training_loss": 9.656798362731934
    },
    {
      "epoch": 0.011544715447154472,
      "step": 213,
      "training_loss": 8.110672950744629
    },
    {
      "epoch": 0.011598915989159892,
      "step": 214,
      "training_loss": 9.679655075073242
    },
    {
      "epoch": 0.011653116531165311,
      "step": 215,
      "training_loss": 10.348111152648926
    },
    {
      "epoch": 0.011707317073170732,
      "grad_norm": 28.596580505371094,
      "learning_rate": 1e-05,
      "loss": 9.4488,
      "step": 216
    },
    {
      "epoch": 0.011707317073170732,
      "step": 216,
      "training_loss": 9.542818069458008
    },
    {
      "epoch": 0.011761517615176152,
      "step": 217,
      "training_loss": 10.631009101867676
    },
    {
      "epoch": 0.011815718157181573,
      "step": 218,
      "training_loss": 9.405866622924805
    },
    {
      "epoch": 0.011869918699186991,
      "step": 219,
      "training_loss": 11.114250183105469
    },
    {
      "epoch": 0.011924119241192412,
      "grad_norm": 57.84479904174805,
      "learning_rate": 1e-05,
      "loss": 10.1735,
      "step": 220
    },
    {
      "epoch": 0.011924119241192412,
      "step": 220,
      "training_loss": 10.114420890808105
    },
    {
      "epoch": 0.011978319783197832,
      "step": 221,
      "training_loss": 8.765615463256836
    },
    {
      "epoch": 0.012032520325203253,
      "step": 222,
      "training_loss": 8.557538986206055
    },
    {
      "epoch": 0.012086720867208672,
      "step": 223,
      "training_loss": 8.861695289611816
    },
    {
      "epoch": 0.012140921409214092,
      "grad_norm": 16.74322509765625,
      "learning_rate": 1e-05,
      "loss": 9.0748,
      "step": 224
    },
    {
      "epoch": 0.012140921409214092,
      "step": 224,
      "training_loss": 9.108297348022461
    },
    {
      "epoch": 0.012195121951219513,
      "step": 225,
      "training_loss": 9.796586036682129
    },
    {
      "epoch": 0.012249322493224933,
      "step": 226,
      "training_loss": 10.130340576171875
    },
    {
      "epoch": 0.012303523035230352,
      "step": 227,
      "training_loss": 8.97702407836914
    },
    {
      "epoch": 0.012357723577235772,
      "grad_norm": 27.27787208557129,
      "learning_rate": 1e-05,
      "loss": 9.5031,
      "step": 228
    },
    {
      "epoch": 0.012357723577235772,
      "step": 228,
      "training_loss": 11.316548347473145
    },
    {
      "epoch": 0.012411924119241193,
      "step": 229,
      "training_loss": 10.519478797912598
    },
    {
      "epoch": 0.012466124661246613,
      "step": 230,
      "training_loss": 8.426501274108887
    },
    {
      "epoch": 0.012520325203252032,
      "step": 231,
      "training_loss": 10.85566234588623
    },
    {
      "epoch": 0.012574525745257453,
      "grad_norm": 46.42202377319336,
      "learning_rate": 1e-05,
      "loss": 10.2795,
      "step": 232
    },
    {
      "epoch": 0.012574525745257453,
      "step": 232,
      "training_loss": 8.400527000427246
    },
    {
      "epoch": 0.012628726287262873,
      "step": 233,
      "training_loss": 8.767032623291016
    },
    {
      "epoch": 0.012682926829268294,
      "step": 234,
      "training_loss": 8.553905487060547
    },
    {
      "epoch": 0.012737127371273712,
      "step": 235,
      "training_loss": 9.756759643554688
    },
    {
      "epoch": 0.012791327913279133,
      "grad_norm": 17.56345558166504,
      "learning_rate": 1e-05,
      "loss": 8.8696,
      "step": 236
    },
    {
      "epoch": 0.012791327913279133,
      "step": 236,
      "training_loss": 8.415847778320312
    },
    {
      "epoch": 0.012845528455284553,
      "step": 237,
      "training_loss": 8.690194129943848
    },
    {
      "epoch": 0.012899728997289974,
      "step": 238,
      "training_loss": 10.289337158203125
    },
    {
      "epoch": 0.012953929539295393,
      "step": 239,
      "training_loss": 10.482305526733398
    },
    {
      "epoch": 0.013008130081300813,
      "grad_norm": 48.825138092041016,
      "learning_rate": 1e-05,
      "loss": 9.4694,
      "step": 240
    },
    {
      "epoch": 0.013008130081300813,
      "step": 240,
      "training_loss": 9.455025672912598
    },
    {
      "epoch": 0.013062330623306233,
      "step": 241,
      "training_loss": 7.950691223144531
    },
    {
      "epoch": 0.013116531165311654,
      "step": 242,
      "training_loss": 10.397004127502441
    },
    {
      "epoch": 0.013170731707317073,
      "step": 243,
      "training_loss": 9.911083221435547
    },
    {
      "epoch": 0.013224932249322493,
      "grad_norm": 71.18138122558594,
      "learning_rate": 1e-05,
      "loss": 9.4285,
      "step": 244
    },
    {
      "epoch": 0.013224932249322493,
      "step": 244,
      "training_loss": 10.34853458404541
    },
    {
      "epoch": 0.013279132791327914,
      "step": 245,
      "training_loss": 9.728510856628418
    },
    {
      "epoch": 0.013333333333333334,
      "step": 246,
      "training_loss": 9.254651069641113
    },
    {
      "epoch": 0.013387533875338753,
      "step": 247,
      "training_loss": 10.217768669128418
    },
    {
      "epoch": 0.013441734417344173,
      "grad_norm": 36.83623504638672,
      "learning_rate": 1e-05,
      "loss": 9.8874,
      "step": 248
    },
    {
      "epoch": 0.013441734417344173,
      "step": 248,
      "training_loss": 8.915474891662598
    },
    {
      "epoch": 0.013495934959349594,
      "step": 249,
      "training_loss": 10.080000877380371
    },
    {
      "epoch": 0.013550135501355014,
      "step": 250,
      "training_loss": 9.276690483093262
    },
    {
      "epoch": 0.013604336043360433,
      "step": 251,
      "training_loss": 8.97238540649414
    },
    {
      "epoch": 0.013658536585365854,
      "grad_norm": 27.018537521362305,
      "learning_rate": 1e-05,
      "loss": 9.3111,
      "step": 252
    },
    {
      "epoch": 0.013658536585365854,
      "step": 252,
      "training_loss": 8.47642707824707
    },
    {
      "epoch": 0.013712737127371274,
      "step": 253,
      "training_loss": 9.077317237854004
    },
    {
      "epoch": 0.013766937669376695,
      "step": 254,
      "training_loss": 8.320564270019531
    },
    {
      "epoch": 0.013821138211382113,
      "step": 255,
      "training_loss": 9.3147611618042
    },
    {
      "epoch": 0.013875338753387534,
      "grad_norm": 16.86305809020996,
      "learning_rate": 1e-05,
      "loss": 8.7973,
      "step": 256
    },
    {
      "epoch": 0.013875338753387534,
      "step": 256,
      "training_loss": 8.058910369873047
    },
    {
      "epoch": 0.013929539295392954,
      "step": 257,
      "training_loss": 8.54357624053955
    },
    {
      "epoch": 0.013983739837398375,
      "step": 258,
      "training_loss": 8.083870887756348
    },
    {
      "epoch": 0.014037940379403794,
      "step": 259,
      "training_loss": 9.136470794677734
    },
    {
      "epoch": 0.014092140921409214,
      "grad_norm": 91.17194366455078,
      "learning_rate": 1e-05,
      "loss": 8.4557,
      "step": 260
    },
    {
      "epoch": 0.014092140921409214,
      "step": 260,
      "training_loss": 10.743507385253906
    },
    {
      "epoch": 0.014146341463414635,
      "step": 261,
      "training_loss": 9.20523738861084
    },
    {
      "epoch": 0.014200542005420055,
      "step": 262,
      "training_loss": 10.02578353881836
    },
    {
      "epoch": 0.014254742547425474,
      "step": 263,
      "training_loss": 9.480998992919922
    },
    {
      "epoch": 0.014308943089430894,
      "grad_norm": 46.80637741088867,
      "learning_rate": 1e-05,
      "loss": 9.8639,
      "step": 264
    },
    {
      "epoch": 0.014308943089430894,
      "step": 264,
      "training_loss": 8.286177635192871
    },
    {
      "epoch": 0.014363143631436315,
      "step": 265,
      "training_loss": 9.340065002441406
    },
    {
      "epoch": 0.014417344173441735,
      "step": 266,
      "training_loss": 10.482244491577148
    },
    {
      "epoch": 0.014471544715447154,
      "step": 267,
      "training_loss": 9.251486778259277
    },
    {
      "epoch": 0.014525745257452575,
      "grad_norm": 27.2625732421875,
      "learning_rate": 1e-05,
      "loss": 9.34,
      "step": 268
    },
    {
      "epoch": 0.014525745257452575,
      "step": 268,
      "training_loss": 10.094221115112305
    },
    {
      "epoch": 0.014579945799457995,
      "step": 269,
      "training_loss": 8.98193073272705
    },
    {
      "epoch": 0.014634146341463415,
      "step": 270,
      "training_loss": 8.341784477233887
    },
    {
      "epoch": 0.014688346883468834,
      "step": 271,
      "training_loss": 9.468655586242676
    },
    {
      "epoch": 0.014742547425474255,
      "grad_norm": 20.20656967163086,
      "learning_rate": 1e-05,
      "loss": 9.2216,
      "step": 272
    },
    {
      "epoch": 0.014742547425474255,
      "step": 272,
      "training_loss": 11.27656364440918
    },
    {
      "epoch": 0.014796747967479675,
      "step": 273,
      "training_loss": 10.032068252563477
    },
    {
      "epoch": 0.014850948509485096,
      "step": 274,
      "training_loss": 8.886849403381348
    },
    {
      "epoch": 0.014905149051490514,
      "step": 275,
      "training_loss": 8.74573802947998
    },
    {
      "epoch": 0.014959349593495935,
      "grad_norm": 19.699695587158203,
      "learning_rate": 1e-05,
      "loss": 9.7353,
      "step": 276
    },
    {
      "epoch": 0.014959349593495935,
      "step": 276,
      "training_loss": 8.745194435119629
    },
    {
      "epoch": 0.015013550135501355,
      "step": 277,
      "training_loss": 8.684534072875977
    },
    {
      "epoch": 0.015067750677506776,
      "step": 278,
      "training_loss": 9.80054759979248
    },
    {
      "epoch": 0.015121951219512195,
      "step": 279,
      "training_loss": 8.774774551391602
    },
    {
      "epoch": 0.015176151761517615,
      "grad_norm": 21.437641143798828,
      "learning_rate": 1e-05,
      "loss": 9.0013,
      "step": 280
    },
    {
      "epoch": 0.015176151761517615,
      "step": 280,
      "training_loss": 10.099853515625
    },
    {
      "epoch": 0.015230352303523036,
      "step": 281,
      "training_loss": 9.792943954467773
    },
    {
      "epoch": 0.015284552845528456,
      "step": 282,
      "training_loss": 9.225262641906738
    },
    {
      "epoch": 0.015338753387533875,
      "step": 283,
      "training_loss": 9.37082290649414
    },
    {
      "epoch": 0.015392953929539295,
      "grad_norm": 30.1685848236084,
      "learning_rate": 1e-05,
      "loss": 9.6222,
      "step": 284
    },
    {
      "epoch": 0.015392953929539295,
      "step": 284,
      "training_loss": 9.023386001586914
    },
    {
      "epoch": 0.015447154471544716,
      "step": 285,
      "training_loss": 10.123665809631348
    },
    {
      "epoch": 0.015501355013550136,
      "step": 286,
      "training_loss": 9.353898048400879
    },
    {
      "epoch": 0.015555555555555555,
      "step": 287,
      "training_loss": 7.633894920349121
    },
    {
      "epoch": 0.015609756097560976,
      "grad_norm": 21.047245025634766,
      "learning_rate": 1e-05,
      "loss": 9.0337,
      "step": 288
    },
    {
      "epoch": 0.015609756097560976,
      "step": 288,
      "training_loss": 8.956399917602539
    },
    {
      "epoch": 0.015663956639566396,
      "step": 289,
      "training_loss": 8.501402854919434
    },
    {
      "epoch": 0.015718157181571817,
      "step": 290,
      "training_loss": 9.322672843933105
    },
    {
      "epoch": 0.015772357723577237,
      "step": 291,
      "training_loss": 8.748307228088379
    },
    {
      "epoch": 0.015826558265582658,
      "grad_norm": 29.912433624267578,
      "learning_rate": 1e-05,
      "loss": 8.8822,
      "step": 292
    },
    {
      "epoch": 0.015826558265582658,
      "step": 292,
      "training_loss": 9.276658058166504
    },
    {
      "epoch": 0.015880758807588075,
      "step": 293,
      "training_loss": 11.696881294250488
    },
    {
      "epoch": 0.015934959349593495,
      "step": 294,
      "training_loss": 9.638446807861328
    },
    {
      "epoch": 0.015989159891598916,
      "step": 295,
      "training_loss": 7.8476033210754395
    },
    {
      "epoch": 0.016043360433604336,
      "grad_norm": 17.330045700073242,
      "learning_rate": 1e-05,
      "loss": 9.6149,
      "step": 296
    },
    {
      "epoch": 0.016043360433604336,
      "step": 296,
      "training_loss": 9.440367698669434
    },
    {
      "epoch": 0.016097560975609757,
      "step": 297,
      "training_loss": 7.9831438064575195
    },
    {
      "epoch": 0.016151761517615177,
      "step": 298,
      "training_loss": 7.866391181945801
    },
    {
      "epoch": 0.016205962059620597,
      "step": 299,
      "training_loss": 9.351469039916992
    },
    {
      "epoch": 0.016260162601626018,
      "grad_norm": 15.587423324584961,
      "learning_rate": 1e-05,
      "loss": 8.6603,
      "step": 300
    },
    {
      "epoch": 0.016260162601626018,
      "step": 300,
      "training_loss": 8.607007026672363
    },
    {
      "epoch": 0.016314363143631435,
      "step": 301,
      "training_loss": 9.57178783416748
    },
    {
      "epoch": 0.016368563685636855,
      "step": 302,
      "training_loss": 9.801836013793945
    },
    {
      "epoch": 0.016422764227642276,
      "step": 303,
      "training_loss": 8.356504440307617
    },
    {
      "epoch": 0.016476964769647696,
      "grad_norm": 14.989166259765625,
      "learning_rate": 1e-05,
      "loss": 9.0843,
      "step": 304
    },
    {
      "epoch": 0.016476964769647696,
      "step": 304,
      "training_loss": 6.732490539550781
    },
    {
      "epoch": 0.016531165311653117,
      "step": 305,
      "training_loss": 9.923141479492188
    },
    {
      "epoch": 0.016585365853658537,
      "step": 306,
      "training_loss": 9.289495468139648
    },
    {
      "epoch": 0.016639566395663958,
      "step": 307,
      "training_loss": 8.725504875183105
    },
    {
      "epoch": 0.01669376693766938,
      "grad_norm": 16.531253814697266,
      "learning_rate": 1e-05,
      "loss": 8.6677,
      "step": 308
    },
    {
      "epoch": 0.01669376693766938,
      "step": 308,
      "training_loss": 9.91627311706543
    },
    {
      "epoch": 0.016747967479674795,
      "step": 309,
      "training_loss": 9.295805931091309
    },
    {
      "epoch": 0.016802168021680216,
      "step": 310,
      "training_loss": 8.279821395874023
    },
    {
      "epoch": 0.016856368563685636,
      "step": 311,
      "training_loss": 11.07154369354248
    },
    {
      "epoch": 0.016910569105691057,
      "grad_norm": 27.21591567993164,
      "learning_rate": 1e-05,
      "loss": 9.6409,
      "step": 312
    },
    {
      "epoch": 0.016910569105691057,
      "step": 312,
      "training_loss": 8.62761116027832
    },
    {
      "epoch": 0.016964769647696477,
      "step": 313,
      "training_loss": 8.079964637756348
    },
    {
      "epoch": 0.017018970189701898,
      "step": 314,
      "training_loss": 6.985485553741455
    },
    {
      "epoch": 0.01707317073170732,
      "step": 315,
      "training_loss": 9.265228271484375
    },
    {
      "epoch": 0.01712737127371274,
      "grad_norm": 23.863388061523438,
      "learning_rate": 1e-05,
      "loss": 8.2396,
      "step": 316
    },
    {
      "epoch": 0.01712737127371274,
      "step": 316,
      "training_loss": 9.071197509765625
    },
    {
      "epoch": 0.017181571815718156,
      "step": 317,
      "training_loss": 9.31923770904541
    },
    {
      "epoch": 0.017235772357723576,
      "step": 318,
      "training_loss": 9.46812915802002
    },
    {
      "epoch": 0.017289972899728997,
      "step": 319,
      "training_loss": 8.964067459106445
    },
    {
      "epoch": 0.017344173441734417,
      "grad_norm": 22.54400062561035,
      "learning_rate": 1e-05,
      "loss": 9.2057,
      "step": 320
    },
    {
      "epoch": 0.017344173441734417,
      "step": 320,
      "training_loss": 9.166092872619629
    },
    {
      "epoch": 0.017398373983739838,
      "step": 321,
      "training_loss": 8.846397399902344
    },
    {
      "epoch": 0.01745257452574526,
      "step": 322,
      "training_loss": 9.016765594482422
    },
    {
      "epoch": 0.01750677506775068,
      "step": 323,
      "training_loss": 8.721442222595215
    },
    {
      "epoch": 0.0175609756097561,
      "grad_norm": 24.970245361328125,
      "learning_rate": 1e-05,
      "loss": 8.9377,
      "step": 324
    },
    {
      "epoch": 0.0175609756097561,
      "step": 324,
      "training_loss": 8.666658401489258
    },
    {
      "epoch": 0.017615176151761516,
      "step": 325,
      "training_loss": 8.0747652053833
    },
    {
      "epoch": 0.017669376693766937,
      "step": 326,
      "training_loss": 9.385220527648926
    },
    {
      "epoch": 0.017723577235772357,
      "step": 327,
      "training_loss": 7.595514297485352
    },
    {
      "epoch": 0.017777777777777778,
      "grad_norm": 15.149890899658203,
      "learning_rate": 1e-05,
      "loss": 8.4305,
      "step": 328
    },
    {
      "epoch": 0.017777777777777778,
      "step": 328,
      "training_loss": 7.909492492675781
    },
    {
      "epoch": 0.017831978319783198,
      "step": 329,
      "training_loss": 8.959549903869629
    },
    {
      "epoch": 0.01788617886178862,
      "step": 330,
      "training_loss": 10.01013469696045
    },
    {
      "epoch": 0.01794037940379404,
      "step": 331,
      "training_loss": 10.243875503540039
    },
    {
      "epoch": 0.01799457994579946,
      "grad_norm": 29.232423782348633,
      "learning_rate": 1e-05,
      "loss": 9.2808,
      "step": 332
    },
    {
      "epoch": 0.01799457994579946,
      "step": 332,
      "training_loss": 8.528722763061523
    },
    {
      "epoch": 0.018048780487804877,
      "step": 333,
      "training_loss": 9.731560707092285
    },
    {
      "epoch": 0.018102981029810297,
      "step": 334,
      "training_loss": 9.827203750610352
    },
    {
      "epoch": 0.018157181571815718,
      "step": 335,
      "training_loss": 8.408852577209473
    },
    {
      "epoch": 0.018211382113821138,
      "grad_norm": 116.2842788696289,
      "learning_rate": 1e-05,
      "loss": 9.1241,
      "step": 336
    },
    {
      "epoch": 0.018211382113821138,
      "step": 336,
      "training_loss": 8.360906600952148
    },
    {
      "epoch": 0.01826558265582656,
      "step": 337,
      "training_loss": 8.78665542602539
    },
    {
      "epoch": 0.01831978319783198,
      "step": 338,
      "training_loss": 8.43526554107666
    },
    {
      "epoch": 0.0183739837398374,
      "step": 339,
      "training_loss": 12.098740577697754
    },
    {
      "epoch": 0.01842818428184282,
      "grad_norm": 325.4991455078125,
      "learning_rate": 1e-05,
      "loss": 9.4204,
      "step": 340
    },
    {
      "epoch": 0.01842818428184282,
      "step": 340,
      "training_loss": 7.886733055114746
    },
    {
      "epoch": 0.018482384823848237,
      "step": 341,
      "training_loss": 7.866455554962158
    },
    {
      "epoch": 0.018536585365853658,
      "step": 342,
      "training_loss": 8.546805381774902
    },
    {
      "epoch": 0.018590785907859078,
      "step": 343,
      "training_loss": 8.790390968322754
    },
    {
      "epoch": 0.0186449864498645,
      "grad_norm": 24.537687301635742,
      "learning_rate": 1e-05,
      "loss": 8.2726,
      "step": 344
    },
    {
      "epoch": 0.0186449864498645,
      "step": 344,
      "training_loss": 8.041144371032715
    },
    {
      "epoch": 0.01869918699186992,
      "step": 345,
      "training_loss": 9.099125862121582
    },
    {
      "epoch": 0.01875338753387534,
      "step": 346,
      "training_loss": 8.649168968200684
    },
    {
      "epoch": 0.01880758807588076,
      "step": 347,
      "training_loss": 8.315924644470215
    },
    {
      "epoch": 0.01886178861788618,
      "grad_norm": 19.284896850585938,
      "learning_rate": 1e-05,
      "loss": 8.5263,
      "step": 348
    },
    {
      "epoch": 0.01886178861788618,
      "step": 348,
      "training_loss": 8.381394386291504
    },
    {
      "epoch": 0.018915989159891598,
      "step": 349,
      "training_loss": 9.773293495178223
    },
    {
      "epoch": 0.018970189701897018,
      "step": 350,
      "training_loss": 9.729175567626953
    },
    {
      "epoch": 0.01902439024390244,
      "step": 351,
      "training_loss": 9.061675071716309
    },
    {
      "epoch": 0.01907859078590786,
      "grad_norm": 22.570791244506836,
      "learning_rate": 1e-05,
      "loss": 9.2364,
      "step": 352
    },
    {
      "epoch": 0.01907859078590786,
      "step": 352,
      "training_loss": 9.205097198486328
    },
    {
      "epoch": 0.01913279132791328,
      "step": 353,
      "training_loss": 8.28036880493164
    },
    {
      "epoch": 0.0191869918699187,
      "step": 354,
      "training_loss": 7.077770233154297
    },
    {
      "epoch": 0.01924119241192412,
      "step": 355,
      "training_loss": 9.249635696411133
    },
    {
      "epoch": 0.01929539295392954,
      "grad_norm": 20.977657318115234,
      "learning_rate": 1e-05,
      "loss": 8.4532,
      "step": 356
    },
    {
      "epoch": 0.01929539295392954,
      "step": 356,
      "training_loss": 7.114163875579834
    },
    {
      "epoch": 0.019349593495934958,
      "step": 357,
      "training_loss": 9.17654800415039
    },
    {
      "epoch": 0.01940379403794038,
      "step": 358,
      "training_loss": 8.6029691696167
    },
    {
      "epoch": 0.0194579945799458,
      "step": 359,
      "training_loss": 8.269427299499512
    },
    {
      "epoch": 0.01951219512195122,
      "grad_norm": 19.475473403930664,
      "learning_rate": 1e-05,
      "loss": 8.2908,
      "step": 360
    },
    {
      "epoch": 0.01951219512195122,
      "step": 360,
      "training_loss": 7.7233123779296875
    },
    {
      "epoch": 0.01956639566395664,
      "step": 361,
      "training_loss": 7.430850505828857
    },
    {
      "epoch": 0.01962059620596206,
      "step": 362,
      "training_loss": 8.918601989746094
    },
    {
      "epoch": 0.01967479674796748,
      "step": 363,
      "training_loss": 8.447885513305664
    },
    {
      "epoch": 0.0197289972899729,
      "grad_norm": 74.73638916015625,
      "learning_rate": 1e-05,
      "loss": 8.1302,
      "step": 364
    },
    {
      "epoch": 0.0197289972899729,
      "step": 364,
      "training_loss": 8.260132789611816
    },
    {
      "epoch": 0.01978319783197832,
      "step": 365,
      "training_loss": 8.113731384277344
    },
    {
      "epoch": 0.01983739837398374,
      "step": 366,
      "training_loss": 8.079197883605957
    },
    {
      "epoch": 0.01989159891598916,
      "step": 367,
      "training_loss": 8.11274242401123
    },
    {
      "epoch": 0.01994579945799458,
      "grad_norm": 11.171524047851562,
      "learning_rate": 1e-05,
      "loss": 8.1415,
      "step": 368
    },
    {
      "epoch": 0.01994579945799458,
      "step": 368,
      "training_loss": 9.181455612182617
    },
    {
      "epoch": 0.02,
      "step": 369,
      "training_loss": 8.42985725402832
    },
    {
      "epoch": 0.02005420054200542,
      "step": 370,
      "training_loss": 8.581767082214355
    },
    {
      "epoch": 0.02010840108401084,
      "step": 371,
      "training_loss": 8.306815147399902
    },
    {
      "epoch": 0.020162601626016262,
      "grad_norm": 15.505874633789062,
      "learning_rate": 1e-05,
      "loss": 8.625,
      "step": 372
    },
    {
      "epoch": 0.020162601626016262,
      "step": 372,
      "training_loss": 8.882105827331543
    },
    {
      "epoch": 0.02021680216802168,
      "step": 373,
      "training_loss": 7.933631420135498
    },
    {
      "epoch": 0.0202710027100271,
      "step": 374,
      "training_loss": 7.909082412719727
    },
    {
      "epoch": 0.02032520325203252,
      "step": 375,
      "training_loss": 8.631102561950684
    },
    {
      "epoch": 0.02037940379403794,
      "grad_norm": 16.70805549621582,
      "learning_rate": 1e-05,
      "loss": 8.339,
      "step": 376
    },
    {
      "epoch": 0.02037940379403794,
      "step": 376,
      "training_loss": 6.825157165527344
    },
    {
      "epoch": 0.02043360433604336,
      "step": 377,
      "training_loss": 8.17951488494873
    },
    {
      "epoch": 0.02048780487804878,
      "step": 378,
      "training_loss": 7.624025344848633
    },
    {
      "epoch": 0.020542005420054202,
      "step": 379,
      "training_loss": 7.595602989196777
    },
    {
      "epoch": 0.020596205962059622,
      "grad_norm": 9.132671356201172,
      "learning_rate": 1e-05,
      "loss": 7.5561,
      "step": 380
    },
    {
      "epoch": 0.020596205962059622,
      "step": 380,
      "training_loss": 8.969121932983398
    },
    {
      "epoch": 0.02065040650406504,
      "step": 381,
      "training_loss": 9.680608749389648
    },
    {
      "epoch": 0.02070460704607046,
      "step": 382,
      "training_loss": 10.503753662109375
    },
    {
      "epoch": 0.02075880758807588,
      "step": 383,
      "training_loss": 7.353460788726807
    },
    {
      "epoch": 0.0208130081300813,
      "grad_norm": 16.1645565032959,
      "learning_rate": 1e-05,
      "loss": 9.1267,
      "step": 384
    },
    {
      "epoch": 0.0208130081300813,
      "step": 384,
      "training_loss": 8.351395606994629
    },
    {
      "epoch": 0.02086720867208672,
      "step": 385,
      "training_loss": 7.837363243103027
    },
    {
      "epoch": 0.020921409214092142,
      "step": 386,
      "training_loss": 8.236838340759277
    },
    {
      "epoch": 0.020975609756097562,
      "step": 387,
      "training_loss": 6.893348693847656
    },
    {
      "epoch": 0.021029810298102983,
      "grad_norm": 161.31109619140625,
      "learning_rate": 1e-05,
      "loss": 7.8297,
      "step": 388
    },
    {
      "epoch": 0.021029810298102983,
      "step": 388,
      "training_loss": 7.9343085289001465
    },
    {
      "epoch": 0.0210840108401084,
      "step": 389,
      "training_loss": 8.428815841674805
    },
    {
      "epoch": 0.02113821138211382,
      "step": 390,
      "training_loss": 9.046061515808105
    },
    {
      "epoch": 0.02119241192411924,
      "step": 391,
      "training_loss": 8.30547046661377
    },
    {
      "epoch": 0.02124661246612466,
      "grad_norm": 22.251420974731445,
      "learning_rate": 1e-05,
      "loss": 8.4287,
      "step": 392
    },
    {
      "epoch": 0.02124661246612466,
      "step": 392,
      "training_loss": 8.12428092956543
    },
    {
      "epoch": 0.02130081300813008,
      "step": 393,
      "training_loss": 7.956228256225586
    },
    {
      "epoch": 0.021355013550135502,
      "step": 394,
      "training_loss": 7.874785423278809
    },
    {
      "epoch": 0.021409214092140923,
      "step": 395,
      "training_loss": 8.476790428161621
    },
    {
      "epoch": 0.021463414634146343,
      "grad_norm": 11.951824188232422,
      "learning_rate": 1e-05,
      "loss": 8.108,
      "step": 396
    },
    {
      "epoch": 0.021463414634146343,
      "step": 396,
      "training_loss": 8.065881729125977
    },
    {
      "epoch": 0.02151761517615176,
      "step": 397,
      "training_loss": 7.801947116851807
    },
    {
      "epoch": 0.02157181571815718,
      "step": 398,
      "training_loss": 7.0356669425964355
    },
    {
      "epoch": 0.0216260162601626,
      "step": 399,
      "training_loss": 7.137936115264893
    },
    {
      "epoch": 0.02168021680216802,
      "grad_norm": 15.099337577819824,
      "learning_rate": 1e-05,
      "loss": 7.5104,
      "step": 400
    },
    {
      "epoch": 0.02168021680216802,
      "step": 400,
      "training_loss": 7.382430076599121
    },
    {
      "epoch": 0.021734417344173442,
      "step": 401,
      "training_loss": 7.641194820404053
    },
    {
      "epoch": 0.021788617886178863,
      "step": 402,
      "training_loss": 8.275113105773926
    },
    {
      "epoch": 0.021842818428184283,
      "step": 403,
      "training_loss": 7.928378105163574
    },
    {
      "epoch": 0.021897018970189704,
      "grad_norm": 22.510404586791992,
      "learning_rate": 1e-05,
      "loss": 7.8068,
      "step": 404
    },
    {
      "epoch": 0.021897018970189704,
      "step": 404,
      "training_loss": 7.857913494110107
    },
    {
      "epoch": 0.02195121951219512,
      "step": 405,
      "training_loss": 7.97804594039917
    },
    {
      "epoch": 0.02200542005420054,
      "step": 406,
      "training_loss": 6.97877836227417
    },
    {
      "epoch": 0.02205962059620596,
      "step": 407,
      "training_loss": 8.23125171661377
    },
    {
      "epoch": 0.022113821138211382,
      "grad_norm": 12.540722846984863,
      "learning_rate": 1e-05,
      "loss": 7.7615,
      "step": 408
    },
    {
      "epoch": 0.022113821138211382,
      "step": 408,
      "training_loss": 8.743800163269043
    },
    {
      "epoch": 0.022168021680216803,
      "step": 409,
      "training_loss": 8.306242942810059
    },
    {
      "epoch": 0.022222222222222223,
      "step": 410,
      "training_loss": 10.09003734588623
    },
    {
      "epoch": 0.022276422764227644,
      "step": 411,
      "training_loss": 8.354555130004883
    },
    {
      "epoch": 0.022330623306233064,
      "grad_norm": 17.296802520751953,
      "learning_rate": 1e-05,
      "loss": 8.8737,
      "step": 412
    },
    {
      "epoch": 0.022330623306233064,
      "step": 412,
      "training_loss": 8.873902320861816
    },
    {
      "epoch": 0.02238482384823848,
      "step": 413,
      "training_loss": 7.847009181976318
    },
    {
      "epoch": 0.0224390243902439,
      "step": 414,
      "training_loss": 7.264033794403076
    },
    {
      "epoch": 0.022493224932249322,
      "step": 415,
      "training_loss": 7.698939800262451
    },
    {
      "epoch": 0.022547425474254743,
      "grad_norm": 44.4743537902832,
      "learning_rate": 1e-05,
      "loss": 7.921,
      "step": 416
    },
    {
      "epoch": 0.022547425474254743,
      "step": 416,
      "training_loss": 8.31988525390625
    },
    {
      "epoch": 0.022601626016260163,
      "step": 417,
      "training_loss": 7.558592319488525
    },
    {
      "epoch": 0.022655826558265584,
      "step": 418,
      "training_loss": 8.039164543151855
    },
    {
      "epoch": 0.022710027100271004,
      "step": 419,
      "training_loss": 7.301027774810791
    },
    {
      "epoch": 0.022764227642276424,
      "grad_norm": 14.116792678833008,
      "learning_rate": 1e-05,
      "loss": 7.8047,
      "step": 420
    },
    {
      "epoch": 0.022764227642276424,
      "step": 420,
      "training_loss": 9.309996604919434
    },
    {
      "epoch": 0.02281842818428184,
      "step": 421,
      "training_loss": 8.074880599975586
    },
    {
      "epoch": 0.022872628726287262,
      "step": 422,
      "training_loss": 6.638279914855957
    },
    {
      "epoch": 0.022926829268292682,
      "step": 423,
      "training_loss": 8.46131420135498
    },
    {
      "epoch": 0.022981029810298103,
      "grad_norm": 13.438453674316406,
      "learning_rate": 1e-05,
      "loss": 8.1211,
      "step": 424
    },
    {
      "epoch": 0.022981029810298103,
      "step": 424,
      "training_loss": 7.890506267547607
    },
    {
      "epoch": 0.023035230352303523,
      "step": 425,
      "training_loss": 7.3360209465026855
    },
    {
      "epoch": 0.023089430894308944,
      "step": 426,
      "training_loss": 7.732000350952148
    },
    {
      "epoch": 0.023143631436314364,
      "step": 427,
      "training_loss": 6.986161708831787
    },
    {
      "epoch": 0.023197831978319785,
      "grad_norm": 22.73676872253418,
      "learning_rate": 1e-05,
      "loss": 7.4862,
      "step": 428
    },
    {
      "epoch": 0.023197831978319785,
      "step": 428,
      "training_loss": 6.317811489105225
    },
    {
      "epoch": 0.023252032520325202,
      "step": 429,
      "training_loss": 7.956981182098389
    },
    {
      "epoch": 0.023306233062330622,
      "step": 430,
      "training_loss": 7.851401329040527
    },
    {
      "epoch": 0.023360433604336043,
      "step": 431,
      "training_loss": 7.931685447692871
    },
    {
      "epoch": 0.023414634146341463,
      "grad_norm": 19.4801025390625,
      "learning_rate": 1e-05,
      "loss": 7.5145,
      "step": 432
    },
    {
      "epoch": 0.023414634146341463,
      "step": 432,
      "training_loss": 8.250307083129883
    },
    {
      "epoch": 0.023468834688346884,
      "step": 433,
      "training_loss": 6.365569591522217
    },
    {
      "epoch": 0.023523035230352304,
      "step": 434,
      "training_loss": 8.734480857849121
    },
    {
      "epoch": 0.023577235772357725,
      "step": 435,
      "training_loss": 7.921696186065674
    },
    {
      "epoch": 0.023631436314363145,
      "grad_norm": 49.99014663696289,
      "learning_rate": 1e-05,
      "loss": 7.818,
      "step": 436
    },
    {
      "epoch": 0.023631436314363145,
      "step": 436,
      "training_loss": 9.406049728393555
    },
    {
      "epoch": 0.023685636856368562,
      "step": 437,
      "training_loss": 8.261984825134277
    },
    {
      "epoch": 0.023739837398373983,
      "step": 438,
      "training_loss": 7.801459312438965
    },
    {
      "epoch": 0.023794037940379403,
      "step": 439,
      "training_loss": 6.366605281829834
    },
    {
      "epoch": 0.023848238482384824,
      "grad_norm": 14.054723739624023,
      "learning_rate": 1e-05,
      "loss": 7.959,
      "step": 440
    },
    {
      "epoch": 0.023848238482384824,
      "step": 440,
      "training_loss": 8.463953018188477
    },
    {
      "epoch": 0.023902439024390244,
      "step": 441,
      "training_loss": 11.978558540344238
    },
    {
      "epoch": 0.023956639566395665,
      "step": 442,
      "training_loss": 7.241355895996094
    },
    {
      "epoch": 0.024010840108401085,
      "step": 443,
      "training_loss": 7.96688985824585
    },
    {
      "epoch": 0.024065040650406506,
      "grad_norm": 20.506898880004883,
      "learning_rate": 1e-05,
      "loss": 8.9127,
      "step": 444
    },
    {
      "epoch": 0.024065040650406506,
      "step": 444,
      "training_loss": 8.147042274475098
    },
    {
      "epoch": 0.024119241192411923,
      "step": 445,
      "training_loss": 8.120525360107422
    },
    {
      "epoch": 0.024173441734417343,
      "step": 446,
      "training_loss": 7.316007614135742
    },
    {
      "epoch": 0.024227642276422764,
      "step": 447,
      "training_loss": 7.652131080627441
    },
    {
      "epoch": 0.024281842818428184,
      "grad_norm": 13.76490592956543,
      "learning_rate": 1e-05,
      "loss": 7.8089,
      "step": 448
    },
    {
      "epoch": 0.024281842818428184,
      "step": 448,
      "training_loss": 7.5631890296936035
    },
    {
      "epoch": 0.024336043360433605,
      "step": 449,
      "training_loss": 8.733774185180664
    },
    {
      "epoch": 0.024390243902439025,
      "step": 450,
      "training_loss": 7.819546699523926
    },
    {
      "epoch": 0.024444444444444446,
      "step": 451,
      "training_loss": 8.358509063720703
    },
    {
      "epoch": 0.024498644986449866,
      "grad_norm": 11.693607330322266,
      "learning_rate": 1e-05,
      "loss": 8.1188,
      "step": 452
    },
    {
      "epoch": 0.024498644986449866,
      "step": 452,
      "training_loss": 8.049361228942871
    },
    {
      "epoch": 0.024552845528455283,
      "step": 453,
      "training_loss": 7.477034568786621
    },
    {
      "epoch": 0.024607046070460704,
      "step": 454,
      "training_loss": 8.694418907165527
    },
    {
      "epoch": 0.024661246612466124,
      "step": 455,
      "training_loss": 7.679545879364014
    },
    {
      "epoch": 0.024715447154471545,
      "grad_norm": 14.152242660522461,
      "learning_rate": 1e-05,
      "loss": 7.9751,
      "step": 456
    },
    {
      "epoch": 0.024715447154471545,
      "step": 456,
      "training_loss": 6.727057456970215
    },
    {
      "epoch": 0.024769647696476965,
      "step": 457,
      "training_loss": 8.258109092712402
    },
    {
      "epoch": 0.024823848238482386,
      "step": 458,
      "training_loss": 6.571463108062744
    },
    {
      "epoch": 0.024878048780487806,
      "step": 459,
      "training_loss": 9.265181541442871
    },
    {
      "epoch": 0.024932249322493227,
      "grad_norm": 95.29155731201172,
      "learning_rate": 1e-05,
      "loss": 7.7055,
      "step": 460
    },
    {
      "epoch": 0.024932249322493227,
      "step": 460,
      "training_loss": 7.959630012512207
    },
    {
      "epoch": 0.024986449864498644,
      "step": 461,
      "training_loss": 8.809687614440918
    },
    {
      "epoch": 0.025040650406504064,
      "step": 462,
      "training_loss": 8.034317970275879
    },
    {
      "epoch": 0.025094850948509485,
      "step": 463,
      "training_loss": 9.111335754394531
    },
    {
      "epoch": 0.025149051490514905,
      "grad_norm": 17.28402328491211,
      "learning_rate": 1e-05,
      "loss": 8.4787,
      "step": 464
    },
    {
      "epoch": 0.025149051490514905,
      "step": 464,
      "training_loss": 7.907818794250488
    },
    {
      "epoch": 0.025203252032520326,
      "step": 465,
      "training_loss": 6.366112232208252
    },
    {
      "epoch": 0.025257452574525746,
      "step": 466,
      "training_loss": 8.446928977966309
    },
    {
      "epoch": 0.025311653116531167,
      "step": 467,
      "training_loss": 8.859402656555176
    },
    {
      "epoch": 0.025365853658536587,
      "grad_norm": 25.955493927001953,
      "learning_rate": 1e-05,
      "loss": 7.8951,
      "step": 468
    },
    {
      "epoch": 0.025365853658536587,
      "step": 468,
      "training_loss": 7.934590816497803
    },
    {
      "epoch": 0.025420054200542004,
      "step": 469,
      "training_loss": 7.631979942321777
    },
    {
      "epoch": 0.025474254742547425,
      "step": 470,
      "training_loss": 9.382837295532227
    },
    {
      "epoch": 0.025528455284552845,
      "step": 471,
      "training_loss": 7.657494068145752
    },
    {
      "epoch": 0.025582655826558266,
      "grad_norm": 12.350764274597168,
      "learning_rate": 1e-05,
      "loss": 8.1517,
      "step": 472
    },
    {
      "epoch": 0.025582655826558266,
      "step": 472,
      "training_loss": 7.057915210723877
    },
    {
      "epoch": 0.025636856368563686,
      "step": 473,
      "training_loss": 8.141605377197266
    },
    {
      "epoch": 0.025691056910569107,
      "step": 474,
      "training_loss": 6.964909553527832
    },
    {
      "epoch": 0.025745257452574527,
      "step": 475,
      "training_loss": 8.538036346435547
    },
    {
      "epoch": 0.025799457994579948,
      "grad_norm": 11.41733169555664,
      "learning_rate": 1e-05,
      "loss": 7.6756,
      "step": 476
    },
    {
      "epoch": 0.025799457994579948,
      "step": 476,
      "training_loss": 6.934912204742432
    },
    {
      "epoch": 0.025853658536585365,
      "step": 477,
      "training_loss": 6.558340549468994
    },
    {
      "epoch": 0.025907859078590785,
      "step": 478,
      "training_loss": 8.120965003967285
    },
    {
      "epoch": 0.025962059620596206,
      "step": 479,
      "training_loss": 8.37429428100586
    },
    {
      "epoch": 0.026016260162601626,
      "grad_norm": 29.7208194732666,
      "learning_rate": 1e-05,
      "loss": 7.4971,
      "step": 480
    },
    {
      "epoch": 0.026016260162601626,
      "step": 480,
      "training_loss": 6.973459243774414
    },
    {
      "epoch": 0.026070460704607046,
      "step": 481,
      "training_loss": 7.689476490020752
    },
    {
      "epoch": 0.026124661246612467,
      "step": 482,
      "training_loss": 7.858756065368652
    },
    {
      "epoch": 0.026178861788617887,
      "step": 483,
      "training_loss": 7.241107940673828
    },
    {
      "epoch": 0.026233062330623308,
      "grad_norm": 24.06954002380371,
      "learning_rate": 1e-05,
      "loss": 7.4407,
      "step": 484
    },
    {
      "epoch": 0.026233062330623308,
      "step": 484,
      "training_loss": 8.792257308959961
    },
    {
      "epoch": 0.026287262872628725,
      "step": 485,
      "training_loss": 8.03976058959961
    },
    {
      "epoch": 0.026341463414634145,
      "step": 486,
      "training_loss": 7.354308605194092
    },
    {
      "epoch": 0.026395663956639566,
      "step": 487,
      "training_loss": 8.887027740478516
    },
    {
      "epoch": 0.026449864498644986,
      "grad_norm": 20.02303695678711,
      "learning_rate": 1e-05,
      "loss": 8.2683,
      "step": 488
    },
    {
      "epoch": 0.026449864498644986,
      "step": 488,
      "training_loss": 8.766927719116211
    },
    {
      "epoch": 0.026504065040650407,
      "step": 489,
      "training_loss": 9.024206161499023
    },
    {
      "epoch": 0.026558265582655827,
      "step": 490,
      "training_loss": 6.299105644226074
    },
    {
      "epoch": 0.026612466124661248,
      "step": 491,
      "training_loss": 6.3097405433654785
    },
    {
      "epoch": 0.02666666666666667,
      "grad_norm": 16.101600646972656,
      "learning_rate": 1e-05,
      "loss": 7.6,
      "step": 492
    },
    {
      "epoch": 0.02666666666666667,
      "step": 492,
      "training_loss": 8.434672355651855
    },
    {
      "epoch": 0.026720867208672085,
      "step": 493,
      "training_loss": 7.963082313537598
    },
    {
      "epoch": 0.026775067750677506,
      "step": 494,
      "training_loss": 8.284965515136719
    },
    {
      "epoch": 0.026829268292682926,
      "step": 495,
      "training_loss": 7.380144119262695
    },
    {
      "epoch": 0.026883468834688347,
      "grad_norm": 13.190058708190918,
      "learning_rate": 1e-05,
      "loss": 8.0157,
      "step": 496
    },
    {
      "epoch": 0.026883468834688347,
      "step": 496,
      "training_loss": 8.209112167358398
    },
    {
      "epoch": 0.026937669376693767,
      "step": 497,
      "training_loss": 6.986357688903809
    },
    {
      "epoch": 0.026991869918699188,
      "step": 498,
      "training_loss": 8.515549659729004
    },
    {
      "epoch": 0.02704607046070461,
      "step": 499,
      "training_loss": 7.201557159423828
    },
    {
      "epoch": 0.02710027100271003,
      "grad_norm": 12.486780166625977,
      "learning_rate": 1e-05,
      "loss": 7.7281,
      "step": 500
    },
    {
      "epoch": 0.02710027100271003,
      "step": 500,
      "training_loss": 7.13062858581543
    },
    {
      "epoch": 0.027154471544715446,
      "step": 501,
      "training_loss": 8.09872055053711
    },
    {
      "epoch": 0.027208672086720866,
      "step": 502,
      "training_loss": 8.761409759521484
    },
    {
      "epoch": 0.027262872628726287,
      "step": 503,
      "training_loss": 6.616372108459473
    },
    {
      "epoch": 0.027317073170731707,
      "grad_norm": 25.115819931030273,
      "learning_rate": 1e-05,
      "loss": 7.6518,
      "step": 504
    },
    {
      "epoch": 0.027317073170731707,
      "step": 504,
      "training_loss": 8.176163673400879
    },
    {
      "epoch": 0.027371273712737128,
      "step": 505,
      "training_loss": 7.771926403045654
    },
    {
      "epoch": 0.02742547425474255,
      "step": 506,
      "training_loss": 7.734491348266602
    },
    {
      "epoch": 0.02747967479674797,
      "step": 507,
      "training_loss": 7.998220443725586
    },
    {
      "epoch": 0.02753387533875339,
      "grad_norm": 13.558465957641602,
      "learning_rate": 1e-05,
      "loss": 7.9202,
      "step": 508
    },
    {
      "epoch": 0.02753387533875339,
      "step": 508,
      "training_loss": 7.738372802734375
    },
    {
      "epoch": 0.027588075880758806,
      "step": 509,
      "training_loss": 8.241597175598145
    },
    {
      "epoch": 0.027642276422764227,
      "step": 510,
      "training_loss": 8.918270111083984
    },
    {
      "epoch": 0.027696476964769647,
      "step": 511,
      "training_loss": 6.77186918258667
    },
    {
      "epoch": 0.027750677506775068,
      "grad_norm": 10.977042198181152,
      "learning_rate": 1e-05,
      "loss": 7.9175,
      "step": 512
    },
    {
      "epoch": 0.027750677506775068,
      "step": 512,
      "training_loss": 7.243427276611328
    },
    {
      "epoch": 0.027804878048780488,
      "step": 513,
      "training_loss": 7.145732402801514
    },
    {
      "epoch": 0.02785907859078591,
      "step": 514,
      "training_loss": 7.725897789001465
    },
    {
      "epoch": 0.02791327913279133,
      "step": 515,
      "training_loss": 7.917025566101074
    },
    {
      "epoch": 0.02796747967479675,
      "grad_norm": 51.873714447021484,
      "learning_rate": 1e-05,
      "loss": 7.508,
      "step": 516
    },
    {
      "epoch": 0.02796747967479675,
      "step": 516,
      "training_loss": 8.662243843078613
    },
    {
      "epoch": 0.028021680216802167,
      "step": 517,
      "training_loss": 7.13602876663208
    },
    {
      "epoch": 0.028075880758807587,
      "step": 518,
      "training_loss": 7.190653324127197
    },
    {
      "epoch": 0.028130081300813008,
      "step": 519,
      "training_loss": 7.300667762756348
    },
    {
      "epoch": 0.028184281842818428,
      "grad_norm": 83.92121887207031,
      "learning_rate": 1e-05,
      "loss": 7.5724,
      "step": 520
    },
    {
      "epoch": 0.028184281842818428,
      "step": 520,
      "training_loss": 7.369582653045654
    },
    {
      "epoch": 0.02823848238482385,
      "step": 521,
      "training_loss": 8.244224548339844
    },
    {
      "epoch": 0.02829268292682927,
      "step": 522,
      "training_loss": 7.870123386383057
    },
    {
      "epoch": 0.02834688346883469,
      "step": 523,
      "training_loss": 8.35583209991455
    },
    {
      "epoch": 0.02840108401084011,
      "grad_norm": 14.924042701721191,
      "learning_rate": 1e-05,
      "loss": 7.9599,
      "step": 524
    },
    {
      "epoch": 0.02840108401084011,
      "step": 524,
      "training_loss": 7.588839054107666
    },
    {
      "epoch": 0.028455284552845527,
      "step": 525,
      "training_loss": 7.083045482635498
    },
    {
      "epoch": 0.028509485094850948,
      "step": 526,
      "training_loss": 7.160036563873291
    },
    {
      "epoch": 0.028563685636856368,
      "step": 527,
      "training_loss": 7.558352947235107
    },
    {
      "epoch": 0.02861788617886179,
      "grad_norm": 10.282278060913086,
      "learning_rate": 1e-05,
      "loss": 7.3476,
      "step": 528
    },
    {
      "epoch": 0.02861788617886179,
      "step": 528,
      "training_loss": 7.837279796600342
    },
    {
      "epoch": 0.02867208672086721,
      "step": 529,
      "training_loss": 8.573434829711914
    },
    {
      "epoch": 0.02872628726287263,
      "step": 530,
      "training_loss": 7.385232925415039
    },
    {
      "epoch": 0.02878048780487805,
      "step": 531,
      "training_loss": 8.93227481842041
    },
    {
      "epoch": 0.02883468834688347,
      "grad_norm": 28.24067497253418,
      "learning_rate": 1e-05,
      "loss": 8.1821,
      "step": 532
    },
    {
      "epoch": 0.02883468834688347,
      "step": 532,
      "training_loss": 8.903108596801758
    },
    {
      "epoch": 0.028888888888888888,
      "step": 533,
      "training_loss": 8.694533348083496
    },
    {
      "epoch": 0.028943089430894308,
      "step": 534,
      "training_loss": 12.607704162597656
    },
    {
      "epoch": 0.02899728997289973,
      "step": 535,
      "training_loss": 7.098885536193848
    },
    {
      "epoch": 0.02905149051490515,
      "grad_norm": 19.555856704711914,
      "learning_rate": 1e-05,
      "loss": 9.3261,
      "step": 536
    },
    {
      "epoch": 0.02905149051490515,
      "step": 536,
      "training_loss": 7.145205974578857
    },
    {
      "epoch": 0.02910569105691057,
      "step": 537,
      "training_loss": 7.424733638763428
    },
    {
      "epoch": 0.02915989159891599,
      "step": 538,
      "training_loss": 9.446101188659668
    },
    {
      "epoch": 0.02921409214092141,
      "step": 539,
      "training_loss": 8.077714920043945
    },
    {
      "epoch": 0.02926829268292683,
      "grad_norm": 13.626842498779297,
      "learning_rate": 1e-05,
      "loss": 8.0234,
      "step": 540
    },
    {
      "epoch": 0.02926829268292683,
      "step": 540,
      "training_loss": 8.232004165649414
    },
    {
      "epoch": 0.029322493224932248,
      "step": 541,
      "training_loss": 8.675039291381836
    },
    {
      "epoch": 0.02937669376693767,
      "step": 542,
      "training_loss": 8.267992973327637
    },
    {
      "epoch": 0.02943089430894309,
      "step": 543,
      "training_loss": 7.682991981506348
    },
    {
      "epoch": 0.02948509485094851,
      "grad_norm": 13.600375175476074,
      "learning_rate": 1e-05,
      "loss": 8.2145,
      "step": 544
    },
    {
      "epoch": 0.02948509485094851,
      "step": 544,
      "training_loss": 7.752343654632568
    },
    {
      "epoch": 0.02953929539295393,
      "step": 545,
      "training_loss": 7.010008811950684
    },
    {
      "epoch": 0.02959349593495935,
      "step": 546,
      "training_loss": 7.402151584625244
    },
    {
      "epoch": 0.02964769647696477,
      "step": 547,
      "training_loss": 8.647198677062988
    },
    {
      "epoch": 0.02970189701897019,
      "grad_norm": 18.925647735595703,
      "learning_rate": 1e-05,
      "loss": 7.7029,
      "step": 548
    },
    {
      "epoch": 0.02970189701897019,
      "step": 548,
      "training_loss": 7.286717891693115
    },
    {
      "epoch": 0.02975609756097561,
      "step": 549,
      "training_loss": 8.617945671081543
    },
    {
      "epoch": 0.02981029810298103,
      "step": 550,
      "training_loss": 6.48945426940918
    },
    {
      "epoch": 0.02986449864498645,
      "step": 551,
      "training_loss": 7.536012172698975
    },
    {
      "epoch": 0.02991869918699187,
      "grad_norm": 8.868014335632324,
      "learning_rate": 1e-05,
      "loss": 7.4825,
      "step": 552
    },
    {
      "epoch": 0.02991869918699187,
      "step": 552,
      "training_loss": 7.655765533447266
    },
    {
      "epoch": 0.02997289972899729,
      "step": 553,
      "training_loss": 8.01198673248291
    },
    {
      "epoch": 0.03002710027100271,
      "step": 554,
      "training_loss": 8.47567367553711
    },
    {
      "epoch": 0.03008130081300813,
      "step": 555,
      "training_loss": 7.649114608764648
    },
    {
      "epoch": 0.030135501355013552,
      "grad_norm": 19.67048454284668,
      "learning_rate": 1e-05,
      "loss": 7.9481,
      "step": 556
    },
    {
      "epoch": 0.030135501355013552,
      "step": 556,
      "training_loss": 8.006077766418457
    },
    {
      "epoch": 0.03018970189701897,
      "step": 557,
      "training_loss": 7.4127960205078125
    },
    {
      "epoch": 0.03024390243902439,
      "step": 558,
      "training_loss": 7.562286853790283
    },
    {
      "epoch": 0.03029810298102981,
      "step": 559,
      "training_loss": 6.267316818237305
    },
    {
      "epoch": 0.03035230352303523,
      "grad_norm": 13.241568565368652,
      "learning_rate": 1e-05,
      "loss": 7.3121,
      "step": 560
    },
    {
      "epoch": 0.03035230352303523,
      "step": 560,
      "training_loss": 8.27786636352539
    },
    {
      "epoch": 0.03040650406504065,
      "step": 561,
      "training_loss": 7.505637168884277
    },
    {
      "epoch": 0.03046070460704607,
      "step": 562,
      "training_loss": 6.8319244384765625
    },
    {
      "epoch": 0.030514905149051492,
      "step": 563,
      "training_loss": 6.262115955352783
    },
    {
      "epoch": 0.030569105691056912,
      "grad_norm": 13.091245651245117,
      "learning_rate": 1e-05,
      "loss": 7.2194,
      "step": 564
    },
    {
      "epoch": 0.030569105691056912,
      "step": 564,
      "training_loss": 7.486822605133057
    },
    {
      "epoch": 0.03062330623306233,
      "step": 565,
      "training_loss": 6.506314754486084
    },
    {
      "epoch": 0.03067750677506775,
      "step": 566,
      "training_loss": 7.852730751037598
    },
    {
      "epoch": 0.03073170731707317,
      "step": 567,
      "training_loss": 9.516322135925293
    },
    {
      "epoch": 0.03078590785907859,
      "grad_norm": 22.927791595458984,
      "learning_rate": 1e-05,
      "loss": 7.8405,
      "step": 568
    },
    {
      "epoch": 0.03078590785907859,
      "step": 568,
      "training_loss": 7.596273899078369
    },
    {
      "epoch": 0.03084010840108401,
      "step": 569,
      "training_loss": 6.8946309089660645
    },
    {
      "epoch": 0.030894308943089432,
      "step": 570,
      "training_loss": 7.808889865875244
    },
    {
      "epoch": 0.030948509485094852,
      "step": 571,
      "training_loss": 7.22052001953125
    },
    {
      "epoch": 0.031002710027100273,
      "grad_norm": 19.072341918945312,
      "learning_rate": 1e-05,
      "loss": 7.3801,
      "step": 572
    },
    {
      "epoch": 0.031002710027100273,
      "step": 572,
      "training_loss": 7.628958225250244
    },
    {
      "epoch": 0.03105691056910569,
      "step": 573,
      "training_loss": 7.715534687042236
    },
    {
      "epoch": 0.03111111111111111,
      "step": 574,
      "training_loss": 9.171743392944336
    },
    {
      "epoch": 0.03116531165311653,
      "step": 575,
      "training_loss": 6.987898349761963
    },
    {
      "epoch": 0.03121951219512195,
      "grad_norm": 13.227346420288086,
      "learning_rate": 1e-05,
      "loss": 7.876,
      "step": 576
    },
    {
      "epoch": 0.03121951219512195,
      "step": 576,
      "training_loss": 7.856621742248535
    },
    {
      "epoch": 0.03127371273712737,
      "step": 577,
      "training_loss": 8.118185043334961
    },
    {
      "epoch": 0.03132791327913279,
      "step": 578,
      "training_loss": 7.513347625732422
    },
    {
      "epoch": 0.03138211382113821,
      "step": 579,
      "training_loss": 6.811148166656494
    },
    {
      "epoch": 0.03143631436314363,
      "grad_norm": 21.458024978637695,
      "learning_rate": 1e-05,
      "loss": 7.5748,
      "step": 580
    },
    {
      "epoch": 0.03143631436314363,
      "step": 580,
      "training_loss": 6.786680698394775
    },
    {
      "epoch": 0.03149051490514905,
      "step": 581,
      "training_loss": 7.159631252288818
    },
    {
      "epoch": 0.031544715447154474,
      "step": 582,
      "training_loss": 8.047096252441406
    },
    {
      "epoch": 0.03159891598915989,
      "step": 583,
      "training_loss": 7.907321929931641
    },
    {
      "epoch": 0.031653116531165315,
      "grad_norm": 21.504545211791992,
      "learning_rate": 1e-05,
      "loss": 7.4752,
      "step": 584
    },
    {
      "epoch": 0.031653116531165315,
      "step": 584,
      "training_loss": 7.784085273742676
    },
    {
      "epoch": 0.03170731707317073,
      "step": 585,
      "training_loss": 8.942150115966797
    },
    {
      "epoch": 0.03176151761517615,
      "step": 586,
      "training_loss": 9.160422325134277
    },
    {
      "epoch": 0.03181571815718157,
      "step": 587,
      "training_loss": 8.868200302124023
    },
    {
      "epoch": 0.03186991869918699,
      "grad_norm": 29.23548126220703,
      "learning_rate": 1e-05,
      "loss": 8.6887,
      "step": 588
    },
    {
      "epoch": 0.03186991869918699,
      "step": 588,
      "training_loss": 6.81386661529541
    },
    {
      "epoch": 0.031924119241192414,
      "step": 589,
      "training_loss": 8.087862968444824
    },
    {
      "epoch": 0.03197831978319783,
      "step": 590,
      "training_loss": 7.766304016113281
    },
    {
      "epoch": 0.032032520325203255,
      "step": 591,
      "training_loss": 7.51026725769043
    },
    {
      "epoch": 0.03208672086720867,
      "grad_norm": 10.99610710144043,
      "learning_rate": 1e-05,
      "loss": 7.5446,
      "step": 592
    },
    {
      "epoch": 0.03208672086720867,
      "step": 592,
      "training_loss": 7.143747806549072
    },
    {
      "epoch": 0.03214092140921409,
      "step": 593,
      "training_loss": 7.672973155975342
    },
    {
      "epoch": 0.03219512195121951,
      "step": 594,
      "training_loss": 7.325706481933594
    },
    {
      "epoch": 0.03224932249322493,
      "step": 595,
      "training_loss": 8.01272201538086
    },
    {
      "epoch": 0.032303523035230354,
      "grad_norm": 19.675317764282227,
      "learning_rate": 1e-05,
      "loss": 7.5388,
      "step": 596
    },
    {
      "epoch": 0.032303523035230354,
      "step": 596,
      "training_loss": 7.754485607147217
    },
    {
      "epoch": 0.03235772357723577,
      "step": 597,
      "training_loss": 8.303577423095703
    },
    {
      "epoch": 0.032411924119241195,
      "step": 598,
      "training_loss": 7.519500255584717
    },
    {
      "epoch": 0.03246612466124661,
      "step": 599,
      "training_loss": 7.545088291168213
    },
    {
      "epoch": 0.032520325203252036,
      "grad_norm": 11.03748607635498,
      "learning_rate": 1e-05,
      "loss": 7.7807,
      "step": 600
    },
    {
      "epoch": 0.032520325203252036,
      "step": 600,
      "training_loss": 8.318832397460938
    },
    {
      "epoch": 0.03257452574525745,
      "step": 601,
      "training_loss": 8.336999893188477
    },
    {
      "epoch": 0.03262872628726287,
      "step": 602,
      "training_loss": 8.142541885375977
    },
    {
      "epoch": 0.032682926829268294,
      "step": 603,
      "training_loss": 7.395812511444092
    },
    {
      "epoch": 0.03273712737127371,
      "grad_norm": 13.745853424072266,
      "learning_rate": 1e-05,
      "loss": 8.0485,
      "step": 604
    },
    {
      "epoch": 0.03273712737127371,
      "step": 604,
      "training_loss": 7.020951747894287
    },
    {
      "epoch": 0.032791327913279135,
      "step": 605,
      "training_loss": 6.944255828857422
    },
    {
      "epoch": 0.03284552845528455,
      "step": 606,
      "training_loss": 7.477126121520996
    },
    {
      "epoch": 0.032899728997289976,
      "step": 607,
      "training_loss": 5.148961544036865
    },
    {
      "epoch": 0.03295392953929539,
      "grad_norm": 27.703140258789062,
      "learning_rate": 1e-05,
      "loss": 6.6478,
      "step": 608
    },
    {
      "epoch": 0.03295392953929539,
      "step": 608,
      "training_loss": 7.075716972351074
    },
    {
      "epoch": 0.03300813008130081,
      "step": 609,
      "training_loss": 7.870292663574219
    },
    {
      "epoch": 0.033062330623306234,
      "step": 610,
      "training_loss": 7.562296390533447
    },
    {
      "epoch": 0.03311653116531165,
      "step": 611,
      "training_loss": 6.005953788757324
    },
    {
      "epoch": 0.033170731707317075,
      "grad_norm": 16.46178436279297,
      "learning_rate": 1e-05,
      "loss": 7.1286,
      "step": 612
    },
    {
      "epoch": 0.033170731707317075,
      "step": 612,
      "training_loss": 7.661654472351074
    },
    {
      "epoch": 0.03322493224932249,
      "step": 613,
      "training_loss": 8.430442810058594
    },
    {
      "epoch": 0.033279132791327916,
      "step": 614,
      "training_loss": 8.001923561096191
    },
    {
      "epoch": 0.03333333333333333,
      "step": 615,
      "training_loss": 6.9261674880981445
    },
    {
      "epoch": 0.03338753387533876,
      "grad_norm": 14.56048583984375,
      "learning_rate": 1e-05,
      "loss": 7.755,
      "step": 616
    },
    {
      "epoch": 0.03338753387533876,
      "step": 616,
      "training_loss": 7.840579509735107
    },
    {
      "epoch": 0.033441734417344174,
      "step": 617,
      "training_loss": 7.342605113983154
    },
    {
      "epoch": 0.03349593495934959,
      "step": 618,
      "training_loss": 6.175075531005859
    },
    {
      "epoch": 0.033550135501355015,
      "step": 619,
      "training_loss": 7.016615867614746
    },
    {
      "epoch": 0.03360433604336043,
      "grad_norm": 14.835762023925781,
      "learning_rate": 1e-05,
      "loss": 7.0937,
      "step": 620
    },
    {
      "epoch": 0.03360433604336043,
      "step": 620,
      "training_loss": 8.156594276428223
    },
    {
      "epoch": 0.033658536585365856,
      "step": 621,
      "training_loss": 7.633522987365723
    },
    {
      "epoch": 0.03371273712737127,
      "step": 622,
      "training_loss": 7.895918846130371
    },
    {
      "epoch": 0.0337669376693767,
      "step": 623,
      "training_loss": 8.076791763305664
    },
    {
      "epoch": 0.033821138211382114,
      "grad_norm": 18.83048439025879,
      "learning_rate": 1e-05,
      "loss": 7.9407,
      "step": 624
    },
    {
      "epoch": 0.033821138211382114,
      "step": 624,
      "training_loss": 8.693800926208496
    },
    {
      "epoch": 0.03387533875338753,
      "step": 625,
      "training_loss": 8.160466194152832
    },
    {
      "epoch": 0.033929539295392955,
      "step": 626,
      "training_loss": 7.449671745300293
    },
    {
      "epoch": 0.03398373983739837,
      "step": 627,
      "training_loss": 7.530853271484375
    },
    {
      "epoch": 0.034037940379403796,
      "grad_norm": 16.453369140625,
      "learning_rate": 1e-05,
      "loss": 7.9587,
      "step": 628
    },
    {
      "epoch": 0.034037940379403796,
      "step": 628,
      "training_loss": 6.371591567993164
    },
    {
      "epoch": 0.03409214092140921,
      "step": 629,
      "training_loss": 7.984294891357422
    },
    {
      "epoch": 0.03414634146341464,
      "step": 630,
      "training_loss": 6.843656063079834
    },
    {
      "epoch": 0.034200542005420054,
      "step": 631,
      "training_loss": 7.089364528656006
    },
    {
      "epoch": 0.03425474254742548,
      "grad_norm": 18.147918701171875,
      "learning_rate": 1e-05,
      "loss": 7.0722,
      "step": 632
    },
    {
      "epoch": 0.03425474254742548,
      "step": 632,
      "training_loss": 5.8620734214782715
    },
    {
      "epoch": 0.034308943089430895,
      "step": 633,
      "training_loss": 7.434935092926025
    },
    {
      "epoch": 0.03436314363143631,
      "step": 634,
      "training_loss": 7.697505474090576
    },
    {
      "epoch": 0.034417344173441736,
      "step": 635,
      "training_loss": 8.205038070678711
    },
    {
      "epoch": 0.03447154471544715,
      "grad_norm": 14.699790954589844,
      "learning_rate": 1e-05,
      "loss": 7.2999,
      "step": 636
    },
    {
      "epoch": 0.03447154471544715,
      "step": 636,
      "training_loss": 8.378556251525879
    },
    {
      "epoch": 0.03452574525745258,
      "step": 637,
      "training_loss": 7.40627908706665
    },
    {
      "epoch": 0.034579945799457994,
      "step": 638,
      "training_loss": 6.95586633682251
    },
    {
      "epoch": 0.03463414634146342,
      "step": 639,
      "training_loss": 8.133492469787598
    },
    {
      "epoch": 0.034688346883468835,
      "grad_norm": 18.504623413085938,
      "learning_rate": 1e-05,
      "loss": 7.7185,
      "step": 640
    },
    {
      "epoch": 0.034688346883468835,
      "step": 640,
      "training_loss": 6.613019943237305
    },
    {
      "epoch": 0.03474254742547425,
      "step": 641,
      "training_loss": 6.14841890335083
    },
    {
      "epoch": 0.034796747967479676,
      "step": 642,
      "training_loss": 6.311915874481201
    },
    {
      "epoch": 0.03485094850948509,
      "step": 643,
      "training_loss": 7.22858190536499
    },
    {
      "epoch": 0.03490514905149052,
      "grad_norm": 15.910090446472168,
      "learning_rate": 1e-05,
      "loss": 6.5755,
      "step": 644
    },
    {
      "epoch": 0.03490514905149052,
      "step": 644,
      "training_loss": 8.16611099243164
    },
    {
      "epoch": 0.034959349593495934,
      "step": 645,
      "training_loss": 8.402572631835938
    },
    {
      "epoch": 0.03501355013550136,
      "step": 646,
      "training_loss": 6.247028827667236
    },
    {
      "epoch": 0.035067750677506775,
      "step": 647,
      "training_loss": 7.658950328826904
    },
    {
      "epoch": 0.0351219512195122,
      "grad_norm": 14.55771255493164,
      "learning_rate": 1e-05,
      "loss": 7.6187,
      "step": 648
    },
    {
      "epoch": 0.0351219512195122,
      "step": 648,
      "training_loss": 8.1662015914917
    },
    {
      "epoch": 0.035176151761517616,
      "step": 649,
      "training_loss": 5.8511457443237305
    },
    {
      "epoch": 0.03523035230352303,
      "step": 650,
      "training_loss": 7.341268539428711
    },
    {
      "epoch": 0.03528455284552846,
      "step": 651,
      "training_loss": 9.425777435302734
    },
    {
      "epoch": 0.035338753387533874,
      "grad_norm": 22.843904495239258,
      "learning_rate": 1e-05,
      "loss": 7.6961,
      "step": 652
    },
    {
      "epoch": 0.035338753387533874,
      "step": 652,
      "training_loss": 7.36356782913208
    },
    {
      "epoch": 0.0353929539295393,
      "step": 653,
      "training_loss": 7.418573379516602
    },
    {
      "epoch": 0.035447154471544715,
      "step": 654,
      "training_loss": 8.25290298461914
    },
    {
      "epoch": 0.03550135501355014,
      "step": 655,
      "training_loss": 7.1481451988220215
    },
    {
      "epoch": 0.035555555555555556,
      "grad_norm": 16.31294822692871,
      "learning_rate": 1e-05,
      "loss": 7.5458,
      "step": 656
    },
    {
      "epoch": 0.035555555555555556,
      "step": 656,
      "training_loss": 7.939426422119141
    },
    {
      "epoch": 0.03560975609756097,
      "step": 657,
      "training_loss": 6.82961893081665
    },
    {
      "epoch": 0.035663956639566397,
      "step": 658,
      "training_loss": 8.04493236541748
    },
    {
      "epoch": 0.035718157181571814,
      "step": 659,
      "training_loss": 6.600935459136963
    },
    {
      "epoch": 0.03577235772357724,
      "grad_norm": 11.468917846679688,
      "learning_rate": 1e-05,
      "loss": 7.3537,
      "step": 660
    },
    {
      "epoch": 0.03577235772357724,
      "step": 660,
      "training_loss": 5.9979729652404785
    },
    {
      "epoch": 0.035826558265582654,
      "step": 661,
      "training_loss": 7.510555267333984
    },
    {
      "epoch": 0.03588075880758808,
      "step": 662,
      "training_loss": 6.637672424316406
    },
    {
      "epoch": 0.035934959349593495,
      "step": 663,
      "training_loss": 6.011204719543457
    },
    {
      "epoch": 0.03598915989159892,
      "grad_norm": 21.313053131103516,
      "learning_rate": 1e-05,
      "loss": 6.5394,
      "step": 664
    },
    {
      "epoch": 0.03598915989159892,
      "step": 664,
      "training_loss": 7.330417156219482
    },
    {
      "epoch": 0.036043360433604336,
      "step": 665,
      "training_loss": 6.95147705078125
    },
    {
      "epoch": 0.03609756097560975,
      "step": 666,
      "training_loss": 8.35532283782959
    },
    {
      "epoch": 0.03615176151761518,
      "step": 667,
      "training_loss": 8.489896774291992
    },
    {
      "epoch": 0.036205962059620594,
      "grad_norm": 14.010684967041016,
      "learning_rate": 1e-05,
      "loss": 7.7818,
      "step": 668
    },
    {
      "epoch": 0.036205962059620594,
      "step": 668,
      "training_loss": 7.910189151763916
    },
    {
      "epoch": 0.03626016260162602,
      "step": 669,
      "training_loss": 7.940420150756836
    },
    {
      "epoch": 0.036314363143631435,
      "step": 670,
      "training_loss": 7.8779449462890625
    },
    {
      "epoch": 0.03636856368563686,
      "step": 671,
      "training_loss": 6.968067646026611
    },
    {
      "epoch": 0.036422764227642276,
      "grad_norm": 16.165576934814453,
      "learning_rate": 1e-05,
      "loss": 7.6742,
      "step": 672
    },
    {
      "epoch": 0.036422764227642276,
      "step": 672,
      "training_loss": 7.153119087219238
    },
    {
      "epoch": 0.03647696476964769,
      "step": 673,
      "training_loss": 8.627752304077148
    },
    {
      "epoch": 0.03653116531165312,
      "step": 674,
      "training_loss": 5.915846347808838
    },
    {
      "epoch": 0.036585365853658534,
      "step": 675,
      "training_loss": 8.214350700378418
    },
    {
      "epoch": 0.03663956639566396,
      "grad_norm": 18.81600570678711,
      "learning_rate": 1e-05,
      "loss": 7.4778,
      "step": 676
    },
    {
      "epoch": 0.03663956639566396,
      "step": 676,
      "training_loss": 8.202044486999512
    },
    {
      "epoch": 0.036693766937669375,
      "step": 677,
      "training_loss": 7.135343074798584
    },
    {
      "epoch": 0.0367479674796748,
      "step": 678,
      "training_loss": 6.442089080810547
    },
    {
      "epoch": 0.036802168021680216,
      "step": 679,
      "training_loss": 5.712493419647217
    },
    {
      "epoch": 0.03685636856368564,
      "grad_norm": 29.244075775146484,
      "learning_rate": 1e-05,
      "loss": 6.873,
      "step": 680
    },
    {
      "epoch": 0.03685636856368564,
      "step": 680,
      "training_loss": 8.153648376464844
    },
    {
      "epoch": 0.03691056910569106,
      "step": 681,
      "training_loss": 8.160945892333984
    },
    {
      "epoch": 0.036964769647696474,
      "step": 682,
      "training_loss": 8.450806617736816
    },
    {
      "epoch": 0.0370189701897019,
      "step": 683,
      "training_loss": 7.582562446594238
    },
    {
      "epoch": 0.037073170731707315,
      "grad_norm": 12.198261260986328,
      "learning_rate": 1e-05,
      "loss": 8.087,
      "step": 684
    },
    {
      "epoch": 0.037073170731707315,
      "step": 684,
      "training_loss": 7.470959663391113
    },
    {
      "epoch": 0.03712737127371274,
      "step": 685,
      "training_loss": 7.416088104248047
    },
    {
      "epoch": 0.037181571815718156,
      "step": 686,
      "training_loss": 9.539102554321289
    },
    {
      "epoch": 0.03723577235772358,
      "step": 687,
      "training_loss": 7.1583147048950195
    },
    {
      "epoch": 0.037289972899729,
      "grad_norm": 25.945592880249023,
      "learning_rate": 1e-05,
      "loss": 7.8961,
      "step": 688
    },
    {
      "epoch": 0.037289972899729,
      "step": 688,
      "training_loss": 8.239164352416992
    },
    {
      "epoch": 0.037344173441734414,
      "step": 689,
      "training_loss": 7.099133491516113
    },
    {
      "epoch": 0.03739837398373984,
      "step": 690,
      "training_loss": 8.468503952026367
    },
    {
      "epoch": 0.037452574525745255,
      "step": 691,
      "training_loss": 7.67402982711792
    },
    {
      "epoch": 0.03750677506775068,
      "grad_norm": 11.716397285461426,
      "learning_rate": 1e-05,
      "loss": 7.8702,
      "step": 692
    },
    {
      "epoch": 0.03750677506775068,
      "step": 692,
      "training_loss": 7.378066062927246
    },
    {
      "epoch": 0.037560975609756096,
      "step": 693,
      "training_loss": 6.541484832763672
    },
    {
      "epoch": 0.03761517615176152,
      "step": 694,
      "training_loss": 7.551252365112305
    },
    {
      "epoch": 0.03766937669376694,
      "step": 695,
      "training_loss": 8.089284896850586
    },
    {
      "epoch": 0.03772357723577236,
      "grad_norm": 12.184758186340332,
      "learning_rate": 1e-05,
      "loss": 7.39,
      "step": 696
    },
    {
      "epoch": 0.03772357723577236,
      "step": 696,
      "training_loss": 8.257721900939941
    },
    {
      "epoch": 0.03777777777777778,
      "step": 697,
      "training_loss": 6.292238712310791
    },
    {
      "epoch": 0.037831978319783195,
      "step": 698,
      "training_loss": 6.569148540496826
    },
    {
      "epoch": 0.03788617886178862,
      "step": 699,
      "training_loss": 7.420546531677246
    },
    {
      "epoch": 0.037940379403794036,
      "grad_norm": 14.983966827392578,
      "learning_rate": 1e-05,
      "loss": 7.1349,
      "step": 700
    },
    {
      "epoch": 0.037940379403794036,
      "step": 700,
      "training_loss": 6.714663982391357
    },
    {
      "epoch": 0.03799457994579946,
      "step": 701,
      "training_loss": 6.584252834320068
    },
    {
      "epoch": 0.03804878048780488,
      "step": 702,
      "training_loss": 6.620905876159668
    },
    {
      "epoch": 0.0381029810298103,
      "step": 703,
      "training_loss": 6.902586460113525
    },
    {
      "epoch": 0.03815718157181572,
      "grad_norm": 17.378864288330078,
      "learning_rate": 1e-05,
      "loss": 6.7056,
      "step": 704
    },
    {
      "epoch": 0.03815718157181572,
      "step": 704,
      "training_loss": 6.1832756996154785
    },
    {
      "epoch": 0.038211382113821135,
      "step": 705,
      "training_loss": 5.44205379486084
    },
    {
      "epoch": 0.03826558265582656,
      "step": 706,
      "training_loss": 8.830633163452148
    },
    {
      "epoch": 0.038319783197831976,
      "step": 707,
      "training_loss": 7.2137064933776855
    },
    {
      "epoch": 0.0383739837398374,
      "grad_norm": 22.71533966064453,
      "learning_rate": 1e-05,
      "loss": 6.9174,
      "step": 708
    },
    {
      "epoch": 0.0383739837398374,
      "step": 708,
      "training_loss": 8.376605033874512
    },
    {
      "epoch": 0.03842818428184282,
      "step": 709,
      "training_loss": 7.329748153686523
    },
    {
      "epoch": 0.03848238482384824,
      "step": 710,
      "training_loss": 7.6830878257751465
    },
    {
      "epoch": 0.03853658536585366,
      "step": 711,
      "training_loss": 7.433039665222168
    },
    {
      "epoch": 0.03859078590785908,
      "grad_norm": 15.80206298828125,
      "learning_rate": 1e-05,
      "loss": 7.7056,
      "step": 712
    },
    {
      "epoch": 0.03859078590785908,
      "step": 712,
      "training_loss": 7.516097545623779
    },
    {
      "epoch": 0.0386449864498645,
      "step": 713,
      "training_loss": 6.4643330574035645
    },
    {
      "epoch": 0.038699186991869916,
      "step": 714,
      "training_loss": 5.9585723876953125
    },
    {
      "epoch": 0.03875338753387534,
      "step": 715,
      "training_loss": 8.432138442993164
    },
    {
      "epoch": 0.03880758807588076,
      "grad_norm": 18.99262237548828,
      "learning_rate": 1e-05,
      "loss": 7.0928,
      "step": 716
    },
    {
      "epoch": 0.03880758807588076,
      "step": 716,
      "training_loss": 8.328255653381348
    },
    {
      "epoch": 0.03886178861788618,
      "step": 717,
      "training_loss": 8.119059562683105
    },
    {
      "epoch": 0.0389159891598916,
      "step": 718,
      "training_loss": 6.933791637420654
    },
    {
      "epoch": 0.03897018970189702,
      "step": 719,
      "training_loss": 7.218845844268799
    },
    {
      "epoch": 0.03902439024390244,
      "grad_norm": 18.767086029052734,
      "learning_rate": 1e-05,
      "loss": 7.65,
      "step": 720
    },
    {
      "epoch": 0.03902439024390244,
      "step": 720,
      "training_loss": 6.973489761352539
    },
    {
      "epoch": 0.039078590785907856,
      "step": 721,
      "training_loss": 7.563920021057129
    },
    {
      "epoch": 0.03913279132791328,
      "step": 722,
      "training_loss": 7.615703582763672
    },
    {
      "epoch": 0.0391869918699187,
      "step": 723,
      "training_loss": 7.2648091316223145
    },
    {
      "epoch": 0.03924119241192412,
      "grad_norm": 12.964652061462402,
      "learning_rate": 1e-05,
      "loss": 7.3545,
      "step": 724
    },
    {
      "epoch": 0.03924119241192412,
      "step": 724,
      "training_loss": 7.467070579528809
    },
    {
      "epoch": 0.03929539295392954,
      "step": 725,
      "training_loss": 8.62618637084961
    },
    {
      "epoch": 0.03934959349593496,
      "step": 726,
      "training_loss": 6.898422718048096
    },
    {
      "epoch": 0.03940379403794038,
      "step": 727,
      "training_loss": 7.6373610496521
    },
    {
      "epoch": 0.0394579945799458,
      "grad_norm": 16.635196685791016,
      "learning_rate": 1e-05,
      "loss": 7.6573,
      "step": 728
    },
    {
      "epoch": 0.0394579945799458,
      "step": 728,
      "training_loss": 7.55942964553833
    },
    {
      "epoch": 0.03951219512195122,
      "step": 729,
      "training_loss": 7.1468329429626465
    },
    {
      "epoch": 0.03956639566395664,
      "step": 730,
      "training_loss": 7.739222049713135
    },
    {
      "epoch": 0.03962059620596206,
      "step": 731,
      "training_loss": 7.401004314422607
    },
    {
      "epoch": 0.03967479674796748,
      "grad_norm": 14.708269119262695,
      "learning_rate": 1e-05,
      "loss": 7.4616,
      "step": 732
    },
    {
      "epoch": 0.03967479674796748,
      "step": 732,
      "training_loss": 7.24678373336792
    },
    {
      "epoch": 0.0397289972899729,
      "step": 733,
      "training_loss": 5.748908519744873
    },
    {
      "epoch": 0.03978319783197832,
      "step": 734,
      "training_loss": 6.049829483032227
    },
    {
      "epoch": 0.03983739837398374,
      "step": 735,
      "training_loss": 6.159605026245117
    },
    {
      "epoch": 0.03989159891598916,
      "grad_norm": 13.45759391784668,
      "learning_rate": 1e-05,
      "loss": 6.3013,
      "step": 736
    },
    {
      "epoch": 0.03989159891598916,
      "step": 736,
      "training_loss": 5.773135662078857
    },
    {
      "epoch": 0.03994579945799458,
      "step": 737,
      "training_loss": 7.824631214141846
    },
    {
      "epoch": 0.04,
      "step": 738,
      "training_loss": 7.847968578338623
    },
    {
      "epoch": 0.04005420054200542,
      "step": 739,
      "training_loss": 7.686949253082275
    },
    {
      "epoch": 0.04010840108401084,
      "grad_norm": 16.186946868896484,
      "learning_rate": 1e-05,
      "loss": 7.2832,
      "step": 740
    },
    {
      "epoch": 0.04010840108401084,
      "step": 740,
      "training_loss": 7.519392490386963
    },
    {
      "epoch": 0.04016260162601626,
      "step": 741,
      "training_loss": 7.683074474334717
    },
    {
      "epoch": 0.04021680216802168,
      "step": 742,
      "training_loss": 7.522810459136963
    },
    {
      "epoch": 0.0402710027100271,
      "step": 743,
      "training_loss": 8.632411003112793
    },
    {
      "epoch": 0.040325203252032524,
      "grad_norm": 15.69361686706543,
      "learning_rate": 1e-05,
      "loss": 7.8394,
      "step": 744
    },
    {
      "epoch": 0.040325203252032524,
      "step": 744,
      "training_loss": 7.715400695800781
    },
    {
      "epoch": 0.04037940379403794,
      "step": 745,
      "training_loss": 7.8688578605651855
    },
    {
      "epoch": 0.04043360433604336,
      "step": 746,
      "training_loss": 7.59503173828125
    },
    {
      "epoch": 0.04048780487804878,
      "step": 747,
      "training_loss": 8.285539627075195
    },
    {
      "epoch": 0.0405420054200542,
      "grad_norm": 11.197763442993164,
      "learning_rate": 1e-05,
      "loss": 7.8662,
      "step": 748
    },
    {
      "epoch": 0.0405420054200542,
      "step": 748,
      "training_loss": 7.2277512550354
    },
    {
      "epoch": 0.04059620596205962,
      "step": 749,
      "training_loss": 8.54175090789795
    },
    {
      "epoch": 0.04065040650406504,
      "step": 750,
      "training_loss": 6.748481750488281
    },
    {
      "epoch": 0.040704607046070464,
      "step": 751,
      "training_loss": 7.085055351257324
    },
    {
      "epoch": 0.04075880758807588,
      "grad_norm": 64.28804016113281,
      "learning_rate": 1e-05,
      "loss": 7.4008,
      "step": 752
    },
    {
      "epoch": 0.04075880758807588,
      "step": 752,
      "training_loss": 7.578892230987549
    },
    {
      "epoch": 0.0408130081300813,
      "step": 753,
      "training_loss": 6.7549824714660645
    },
    {
      "epoch": 0.04086720867208672,
      "step": 754,
      "training_loss": 7.358738422393799
    },
    {
      "epoch": 0.04092140921409214,
      "step": 755,
      "training_loss": 8.599678993225098
    },
    {
      "epoch": 0.04097560975609756,
      "grad_norm": 29.437355041503906,
      "learning_rate": 1e-05,
      "loss": 7.5731,
      "step": 756
    },
    {
      "epoch": 0.04097560975609756,
      "step": 756,
      "training_loss": 6.446199893951416
    },
    {
      "epoch": 0.04102981029810298,
      "step": 757,
      "training_loss": 7.97297477722168
    },
    {
      "epoch": 0.041084010840108404,
      "step": 758,
      "training_loss": 8.171884536743164
    },
    {
      "epoch": 0.04113821138211382,
      "step": 759,
      "training_loss": 7.620614051818848
    },
    {
      "epoch": 0.041192411924119245,
      "grad_norm": 15.04110050201416,
      "learning_rate": 1e-05,
      "loss": 7.5529,
      "step": 760
    },
    {
      "epoch": 0.041192411924119245,
      "step": 760,
      "training_loss": 7.398055076599121
    },
    {
      "epoch": 0.04124661246612466,
      "step": 761,
      "training_loss": 7.762007236480713
    },
    {
      "epoch": 0.04130081300813008,
      "step": 762,
      "training_loss": 7.657063007354736
    },
    {
      "epoch": 0.0413550135501355,
      "step": 763,
      "training_loss": 6.44507360458374
    },
    {
      "epoch": 0.04140921409214092,
      "grad_norm": 19.448123931884766,
      "learning_rate": 1e-05,
      "loss": 7.3155,
      "step": 764
    },
    {
      "epoch": 0.04140921409214092,
      "step": 764,
      "training_loss": 8.893157005310059
    },
    {
      "epoch": 0.041463414634146344,
      "step": 765,
      "training_loss": 8.004855155944824
    },
    {
      "epoch": 0.04151761517615176,
      "step": 766,
      "training_loss": 8.95726490020752
    },
    {
      "epoch": 0.041571815718157185,
      "step": 767,
      "training_loss": 7.944192409515381
    },
    {
      "epoch": 0.0416260162601626,
      "grad_norm": 12.517049789428711,
      "learning_rate": 1e-05,
      "loss": 8.4499,
      "step": 768
    },
    {
      "epoch": 0.0416260162601626,
      "step": 768,
      "training_loss": 7.222275257110596
    },
    {
      "epoch": 0.04168021680216802,
      "step": 769,
      "training_loss": 6.584643363952637
    },
    {
      "epoch": 0.04173441734417344,
      "step": 770,
      "training_loss": 7.481154918670654
    },
    {
      "epoch": 0.04178861788617886,
      "step": 771,
      "training_loss": 7.401394367218018
    },
    {
      "epoch": 0.041842818428184284,
      "grad_norm": 19.670612335205078,
      "learning_rate": 1e-05,
      "loss": 7.1724,
      "step": 772
    },
    {
      "epoch": 0.041842818428184284,
      "step": 772,
      "training_loss": 6.846065521240234
    },
    {
      "epoch": 0.0418970189701897,
      "step": 773,
      "training_loss": 7.272346019744873
    },
    {
      "epoch": 0.041951219512195125,
      "step": 774,
      "training_loss": 7.539729595184326
    },
    {
      "epoch": 0.04200542005420054,
      "step": 775,
      "training_loss": 7.643473148345947
    },
    {
      "epoch": 0.042059620596205965,
      "grad_norm": 15.767237663269043,
      "learning_rate": 1e-05,
      "loss": 7.3254,
      "step": 776
    },
    {
      "epoch": 0.042059620596205965,
      "step": 776,
      "training_loss": 7.768119812011719
    },
    {
      "epoch": 0.04211382113821138,
      "step": 777,
      "training_loss": 8.141231536865234
    },
    {
      "epoch": 0.0421680216802168,
      "step": 778,
      "training_loss": 7.020600318908691
    },
    {
      "epoch": 0.042222222222222223,
      "step": 779,
      "training_loss": 7.396282196044922
    },
    {
      "epoch": 0.04227642276422764,
      "grad_norm": 13.881558418273926,
      "learning_rate": 1e-05,
      "loss": 7.5816,
      "step": 780
    },
    {
      "epoch": 0.04227642276422764,
      "step": 780,
      "training_loss": 7.871460437774658
    },
    {
      "epoch": 0.042330623306233064,
      "step": 781,
      "training_loss": 7.26619291305542
    },
    {
      "epoch": 0.04238482384823848,
      "step": 782,
      "training_loss": 7.747412204742432
    },
    {
      "epoch": 0.042439024390243905,
      "step": 783,
      "training_loss": 9.669589042663574
    },
    {
      "epoch": 0.04249322493224932,
      "grad_norm": 50.625267028808594,
      "learning_rate": 1e-05,
      "loss": 8.1387,
      "step": 784
    },
    {
      "epoch": 0.04249322493224932,
      "step": 784,
      "training_loss": 7.909923553466797
    },
    {
      "epoch": 0.04254742547425474,
      "step": 785,
      "training_loss": 7.110124588012695
    },
    {
      "epoch": 0.04260162601626016,
      "step": 786,
      "training_loss": 8.00576400756836
    },
    {
      "epoch": 0.04265582655826558,
      "step": 787,
      "training_loss": 7.681179523468018
    },
    {
      "epoch": 0.042710027100271004,
      "grad_norm": 11.550066947937012,
      "learning_rate": 1e-05,
      "loss": 7.6767,
      "step": 788
    },
    {
      "epoch": 0.042710027100271004,
      "step": 788,
      "training_loss": 7.251941680908203
    },
    {
      "epoch": 0.04276422764227642,
      "step": 789,
      "training_loss": 6.5937700271606445
    },
    {
      "epoch": 0.042818428184281845,
      "step": 790,
      "training_loss": 6.456654071807861
    },
    {
      "epoch": 0.04287262872628726,
      "step": 791,
      "training_loss": 7.992836952209473
    },
    {
      "epoch": 0.042926829268292686,
      "grad_norm": 25.286224365234375,
      "learning_rate": 1e-05,
      "loss": 7.0738,
      "step": 792
    },
    {
      "epoch": 0.042926829268292686,
      "step": 792,
      "training_loss": 7.279108047485352
    },
    {
      "epoch": 0.0429810298102981,
      "step": 793,
      "training_loss": 7.841796398162842
    },
    {
      "epoch": 0.04303523035230352,
      "step": 794,
      "training_loss": 7.987884521484375
    },
    {
      "epoch": 0.043089430894308944,
      "step": 795,
      "training_loss": 7.5241570472717285
    },
    {
      "epoch": 0.04314363143631436,
      "grad_norm": 20.65587615966797,
      "learning_rate": 1e-05,
      "loss": 7.6582,
      "step": 796
    },
    {
      "epoch": 0.04314363143631436,
      "step": 796,
      "training_loss": 6.852330684661865
    },
    {
      "epoch": 0.043197831978319785,
      "step": 797,
      "training_loss": 6.557071208953857
    },
    {
      "epoch": 0.0432520325203252,
      "step": 798,
      "training_loss": 5.717689514160156
    },
    {
      "epoch": 0.043306233062330626,
      "step": 799,
      "training_loss": 6.757059574127197
    },
    {
      "epoch": 0.04336043360433604,
      "grad_norm": 24.131460189819336,
      "learning_rate": 1e-05,
      "loss": 6.471,
      "step": 800
    },
    {
      "epoch": 0.04336043360433604,
      "step": 800,
      "training_loss": 7.316311836242676
    },
    {
      "epoch": 0.04341463414634146,
      "step": 801,
      "training_loss": 8.068388938903809
    },
    {
      "epoch": 0.043468834688346884,
      "step": 802,
      "training_loss": 6.338033199310303
    },
    {
      "epoch": 0.0435230352303523,
      "step": 803,
      "training_loss": 7.67354154586792
    },
    {
      "epoch": 0.043577235772357725,
      "grad_norm": 30.85428237915039,
      "learning_rate": 1e-05,
      "loss": 7.3491,
      "step": 804
    },
    {
      "epoch": 0.043577235772357725,
      "step": 804,
      "training_loss": 6.531081199645996
    },
    {
      "epoch": 0.04363143631436314,
      "step": 805,
      "training_loss": 7.523169040679932
    },
    {
      "epoch": 0.043685636856368566,
      "step": 806,
      "training_loss": 7.332772254943848
    },
    {
      "epoch": 0.04373983739837398,
      "step": 807,
      "training_loss": 7.115194320678711
    },
    {
      "epoch": 0.04379403794037941,
      "grad_norm": 19.423254013061523,
      "learning_rate": 1e-05,
      "loss": 7.1256,
      "step": 808
    },
    {
      "epoch": 0.04379403794037941,
      "step": 808,
      "training_loss": 7.355279922485352
    },
    {
      "epoch": 0.043848238482384824,
      "step": 809,
      "training_loss": 7.15977144241333
    },
    {
      "epoch": 0.04390243902439024,
      "step": 810,
      "training_loss": 7.304504871368408
    },
    {
      "epoch": 0.043956639566395665,
      "step": 811,
      "training_loss": 8.165520668029785
    },
    {
      "epoch": 0.04401084010840108,
      "grad_norm": 15.658426284790039,
      "learning_rate": 1e-05,
      "loss": 7.4963,
      "step": 812
    },
    {
      "epoch": 0.04401084010840108,
      "step": 812,
      "training_loss": 7.243791103363037
    },
    {
      "epoch": 0.044065040650406506,
      "step": 813,
      "training_loss": 7.818070411682129
    },
    {
      "epoch": 0.04411924119241192,
      "step": 814,
      "training_loss": 7.665558338165283
    },
    {
      "epoch": 0.04417344173441735,
      "step": 815,
      "training_loss": 8.180334091186523
    },
    {
      "epoch": 0.044227642276422764,
      "grad_norm": 27.220943450927734,
      "learning_rate": 1e-05,
      "loss": 7.7269,
      "step": 816
    },
    {
      "epoch": 0.044227642276422764,
      "step": 816,
      "training_loss": 6.488495349884033
    },
    {
      "epoch": 0.04428184281842818,
      "step": 817,
      "training_loss": 5.744063854217529
    },
    {
      "epoch": 0.044336043360433605,
      "step": 818,
      "training_loss": 7.816483497619629
    },
    {
      "epoch": 0.04439024390243902,
      "step": 819,
      "training_loss": 6.1043829917907715
    },
    {
      "epoch": 0.044444444444444446,
      "grad_norm": 20.663820266723633,
      "learning_rate": 1e-05,
      "loss": 6.5384,
      "step": 820
    },
    {
      "epoch": 0.044444444444444446,
      "step": 820,
      "training_loss": 7.831073760986328
    },
    {
      "epoch": 0.04449864498644986,
      "step": 821,
      "training_loss": 6.76140022277832
    },
    {
      "epoch": 0.04455284552845529,
      "step": 822,
      "training_loss": 6.868954658508301
    },
    {
      "epoch": 0.044607046070460704,
      "step": 823,
      "training_loss": 9.232460975646973
    },
    {
      "epoch": 0.04466124661246613,
      "grad_norm": 26.163970947265625,
      "learning_rate": 1e-05,
      "loss": 7.6735,
      "step": 824
    },
    {
      "epoch": 0.04466124661246613,
      "step": 824,
      "training_loss": 8.625916481018066
    },
    {
      "epoch": 0.044715447154471545,
      "step": 825,
      "training_loss": 7.054008483886719
    },
    {
      "epoch": 0.04476964769647696,
      "step": 826,
      "training_loss": 7.3619561195373535
    },
    {
      "epoch": 0.044823848238482386,
      "step": 827,
      "training_loss": 7.113012790679932
    },
    {
      "epoch": 0.0448780487804878,
      "grad_norm": 14.996211051940918,
      "learning_rate": 1e-05,
      "loss": 7.5387,
      "step": 828
    },
    {
      "epoch": 0.0448780487804878,
      "step": 828,
      "training_loss": 7.43806266784668
    },
    {
      "epoch": 0.04493224932249323,
      "step": 829,
      "training_loss": 6.783214569091797
    },
    {
      "epoch": 0.044986449864498644,
      "step": 830,
      "training_loss": 7.515467643737793
    },
    {
      "epoch": 0.04504065040650407,
      "step": 831,
      "training_loss": 7.85584831237793
    },
    {
      "epoch": 0.045094850948509485,
      "grad_norm": 30.54755973815918,
      "learning_rate": 1e-05,
      "loss": 7.3981,
      "step": 832
    },
    {
      "epoch": 0.045094850948509485,
      "step": 832,
      "training_loss": 7.160261631011963
    },
    {
      "epoch": 0.0451490514905149,
      "step": 833,
      "training_loss": 7.93576717376709
    },
    {
      "epoch": 0.045203252032520326,
      "step": 834,
      "training_loss": 7.352416515350342
    },
    {
      "epoch": 0.04525745257452574,
      "step": 835,
      "training_loss": 6.516341209411621
    },
    {
      "epoch": 0.04531165311653117,
      "grad_norm": 17.588171005249023,
      "learning_rate": 1e-05,
      "loss": 7.2412,
      "step": 836
    },
    {
      "epoch": 0.04531165311653117,
      "step": 836,
      "training_loss": 6.1782073974609375
    },
    {
      "epoch": 0.045365853658536584,
      "step": 837,
      "training_loss": 7.5690598487854
    },
    {
      "epoch": 0.04542005420054201,
      "step": 838,
      "training_loss": 5.679376125335693
    },
    {
      "epoch": 0.045474254742547425,
      "step": 839,
      "training_loss": 8.154483795166016
    },
    {
      "epoch": 0.04552845528455285,
      "grad_norm": 19.348909378051758,
      "learning_rate": 1e-05,
      "loss": 6.8953,
      "step": 840
    },
    {
      "epoch": 0.04552845528455285,
      "step": 840,
      "training_loss": 6.571814060211182
    },
    {
      "epoch": 0.045582655826558266,
      "step": 841,
      "training_loss": 7.2438530921936035
    },
    {
      "epoch": 0.04563685636856368,
      "step": 842,
      "training_loss": 8.407193183898926
    },
    {
      "epoch": 0.04569105691056911,
      "step": 843,
      "training_loss": 7.19254207611084
    },
    {
      "epoch": 0.045745257452574524,
      "grad_norm": 13.141278266906738,
      "learning_rate": 1e-05,
      "loss": 7.3539,
      "step": 844
    },
    {
      "epoch": 0.045745257452574524,
      "step": 844,
      "training_loss": 7.549839019775391
    },
    {
      "epoch": 0.04579945799457995,
      "step": 845,
      "training_loss": 7.3195881843566895
    },
    {
      "epoch": 0.045853658536585365,
      "step": 846,
      "training_loss": 6.036137104034424
    },
    {
      "epoch": 0.04590785907859079,
      "step": 847,
      "training_loss": 7.111788272857666
    },
    {
      "epoch": 0.045962059620596206,
      "grad_norm": 16.057788848876953,
      "learning_rate": 1e-05,
      "loss": 7.0043,
      "step": 848
    },
    {
      "epoch": 0.045962059620596206,
      "step": 848,
      "training_loss": 6.979340553283691
    },
    {
      "epoch": 0.04601626016260162,
      "step": 849,
      "training_loss": 7.636937618255615
    },
    {
      "epoch": 0.04607046070460705,
      "step": 850,
      "training_loss": 7.127559185028076
    },
    {
      "epoch": 0.046124661246612464,
      "step": 851,
      "training_loss": 6.829360008239746
    },
    {
      "epoch": 0.04617886178861789,
      "grad_norm": 20.51664924621582,
      "learning_rate": 1e-05,
      "loss": 7.1433,
      "step": 852
    },
    {
      "epoch": 0.04617886178861789,
      "step": 852,
      "training_loss": 6.6409101486206055
    },
    {
      "epoch": 0.046233062330623305,
      "step": 853,
      "training_loss": 6.708644866943359
    },
    {
      "epoch": 0.04628726287262873,
      "step": 854,
      "training_loss": 7.283260822296143
    },
    {
      "epoch": 0.046341463414634146,
      "step": 855,
      "training_loss": 7.675302982330322
    },
    {
      "epoch": 0.04639566395663957,
      "grad_norm": 25.710466384887695,
      "learning_rate": 1e-05,
      "loss": 7.077,
      "step": 856
    },
    {
      "epoch": 0.04639566395663957,
      "step": 856,
      "training_loss": 6.524420738220215
    },
    {
      "epoch": 0.04644986449864499,
      "step": 857,
      "training_loss": 6.3211822509765625
    },
    {
      "epoch": 0.046504065040650404,
      "step": 858,
      "training_loss": 8.711405754089355
    },
    {
      "epoch": 0.04655826558265583,
      "step": 859,
      "training_loss": 7.500962257385254
    },
    {
      "epoch": 0.046612466124661245,
      "grad_norm": 16.647005081176758,
      "learning_rate": 1e-05,
      "loss": 7.2645,
      "step": 860
    },
    {
      "epoch": 0.046612466124661245,
      "step": 860,
      "training_loss": 7.2642974853515625
    },
    {
      "epoch": 0.04666666666666667,
      "step": 861,
      "training_loss": 8.148364067077637
    },
    {
      "epoch": 0.046720867208672086,
      "step": 862,
      "training_loss": 7.357884407043457
    },
    {
      "epoch": 0.04677506775067751,
      "step": 863,
      "training_loss": 6.77963924407959
    },
    {
      "epoch": 0.04682926829268293,
      "grad_norm": 18.547786712646484,
      "learning_rate": 1e-05,
      "loss": 7.3875,
      "step": 864
    },
    {
      "epoch": 0.04682926829268293,
      "step": 864,
      "training_loss": 7.273497104644775
    },
    {
      "epoch": 0.046883468834688344,
      "step": 865,
      "training_loss": 7.8357439041137695
    },
    {
      "epoch": 0.04693766937669377,
      "step": 866,
      "training_loss": 7.008145809173584
    },
    {
      "epoch": 0.046991869918699185,
      "step": 867,
      "training_loss": 7.876757621765137
    },
    {
      "epoch": 0.04704607046070461,
      "grad_norm": 29.133487701416016,
      "learning_rate": 1e-05,
      "loss": 7.4985,
      "step": 868
    },
    {
      "epoch": 0.04704607046070461,
      "step": 868,
      "training_loss": 7.528341770172119
    },
    {
      "epoch": 0.047100271002710026,
      "step": 869,
      "training_loss": 8.349176406860352
    },
    {
      "epoch": 0.04715447154471545,
      "step": 870,
      "training_loss": 8.014127731323242
    },
    {
      "epoch": 0.04720867208672087,
      "step": 871,
      "training_loss": 6.581873416900635
    },
    {
      "epoch": 0.04726287262872629,
      "grad_norm": 13.748699188232422,
      "learning_rate": 1e-05,
      "loss": 7.6184,
      "step": 872
    },
    {
      "epoch": 0.04726287262872629,
      "step": 872,
      "training_loss": 6.829333305358887
    },
    {
      "epoch": 0.04731707317073171,
      "step": 873,
      "training_loss": 7.062126159667969
    },
    {
      "epoch": 0.047371273712737125,
      "step": 874,
      "training_loss": 7.849270820617676
    },
    {
      "epoch": 0.04742547425474255,
      "step": 875,
      "training_loss": 7.982462406158447
    },
    {
      "epoch": 0.047479674796747966,
      "grad_norm": 13.907047271728516,
      "learning_rate": 1e-05,
      "loss": 7.4308,
      "step": 876
    },
    {
      "epoch": 0.047479674796747966,
      "step": 876,
      "training_loss": 9.103348731994629
    },
    {
      "epoch": 0.04753387533875339,
      "step": 877,
      "training_loss": 7.604116916656494
    },
    {
      "epoch": 0.04758807588075881,
      "step": 878,
      "training_loss": 7.614312171936035
    },
    {
      "epoch": 0.04764227642276423,
      "step": 879,
      "training_loss": 5.669027328491211
    },
    {
      "epoch": 0.04769647696476965,
      "grad_norm": 14.9942626953125,
      "learning_rate": 1e-05,
      "loss": 7.4977,
      "step": 880
    },
    {
      "epoch": 0.04769647696476965,
      "step": 880,
      "training_loss": 6.862196922302246
    },
    {
      "epoch": 0.047750677506775065,
      "step": 881,
      "training_loss": 6.237283706665039
    },
    {
      "epoch": 0.04780487804878049,
      "step": 882,
      "training_loss": 8.14236831665039
    },
    {
      "epoch": 0.047859078590785906,
      "step": 883,
      "training_loss": 7.211472511291504
    },
    {
      "epoch": 0.04791327913279133,
      "grad_norm": 22.17084503173828,
      "learning_rate": 1e-05,
      "loss": 7.1133,
      "step": 884
    },
    {
      "epoch": 0.04791327913279133,
      "step": 884,
      "training_loss": 7.483644008636475
    },
    {
      "epoch": 0.04796747967479675,
      "step": 885,
      "training_loss": 9.229742050170898
    },
    {
      "epoch": 0.04802168021680217,
      "step": 886,
      "training_loss": 6.443379878997803
    },
    {
      "epoch": 0.04807588075880759,
      "step": 887,
      "training_loss": 5.85428524017334
    },
    {
      "epoch": 0.04813008130081301,
      "grad_norm": 18.950946807861328,
      "learning_rate": 1e-05,
      "loss": 7.2528,
      "step": 888
    },
    {
      "epoch": 0.04813008130081301,
      "step": 888,
      "training_loss": 6.936169147491455
    },
    {
      "epoch": 0.04818428184281843,
      "step": 889,
      "training_loss": 6.417416095733643
    },
    {
      "epoch": 0.048238482384823846,
      "step": 890,
      "training_loss": 8.328697204589844
    },
    {
      "epoch": 0.04829268292682927,
      "step": 891,
      "training_loss": 6.93245792388916
    },
    {
      "epoch": 0.04834688346883469,
      "grad_norm": 15.778785705566406,
      "learning_rate": 1e-05,
      "loss": 7.1537,
      "step": 892
    },
    {
      "epoch": 0.04834688346883469,
      "step": 892,
      "training_loss": 7.177065372467041
    },
    {
      "epoch": 0.04840108401084011,
      "step": 893,
      "training_loss": 6.413909435272217
    },
    {
      "epoch": 0.04845528455284553,
      "step": 894,
      "training_loss": 7.5944600105285645
    },
    {
      "epoch": 0.04850948509485095,
      "step": 895,
      "training_loss": 7.347728252410889
    },
    {
      "epoch": 0.04856368563685637,
      "grad_norm": 17.56531524658203,
      "learning_rate": 1e-05,
      "loss": 7.1333,
      "step": 896
    },
    {
      "epoch": 0.04856368563685637,
      "step": 896,
      "training_loss": 7.512213706970215
    },
    {
      "epoch": 0.048617886178861786,
      "step": 897,
      "training_loss": 6.147216796875
    },
    {
      "epoch": 0.04867208672086721,
      "step": 898,
      "training_loss": 7.0170135498046875
    },
    {
      "epoch": 0.048726287262872627,
      "step": 899,
      "training_loss": 7.627912521362305
    },
    {
      "epoch": 0.04878048780487805,
      "grad_norm": 15.260225296020508,
      "learning_rate": 1e-05,
      "loss": 7.0761,
      "step": 900
    },
    {
      "epoch": 0.04878048780487805,
      "step": 900,
      "training_loss": 7.364727973937988
    },
    {
      "epoch": 0.04883468834688347,
      "step": 901,
      "training_loss": 7.467303276062012
    },
    {
      "epoch": 0.04888888888888889,
      "step": 902,
      "training_loss": 5.502925395965576
    },
    {
      "epoch": 0.04894308943089431,
      "step": 903,
      "training_loss": 6.91231107711792
    },
    {
      "epoch": 0.04899728997289973,
      "grad_norm": 19.97547721862793,
      "learning_rate": 1e-05,
      "loss": 6.8118,
      "step": 904
    },
    {
      "epoch": 0.04899728997289973,
      "step": 904,
      "training_loss": 6.865436553955078
    },
    {
      "epoch": 0.04905149051490515,
      "step": 905,
      "training_loss": 7.5608296394348145
    },
    {
      "epoch": 0.049105691056910566,
      "step": 906,
      "training_loss": 6.88442325592041
    },
    {
      "epoch": 0.04915989159891599,
      "step": 907,
      "training_loss": 7.847963333129883
    },
    {
      "epoch": 0.04921409214092141,
      "grad_norm": 18.614635467529297,
      "learning_rate": 1e-05,
      "loss": 7.2897,
      "step": 908
    },
    {
      "epoch": 0.04921409214092141,
      "step": 908,
      "training_loss": 7.936404705047607
    },
    {
      "epoch": 0.04926829268292683,
      "step": 909,
      "training_loss": 5.243791580200195
    },
    {
      "epoch": 0.04932249322493225,
      "step": 910,
      "training_loss": 5.497528076171875
    },
    {
      "epoch": 0.04937669376693767,
      "step": 911,
      "training_loss": 5.459578990936279
    },
    {
      "epoch": 0.04943089430894309,
      "grad_norm": 12.529241561889648,
      "learning_rate": 1e-05,
      "loss": 6.0343,
      "step": 912
    },
    {
      "epoch": 0.04943089430894309,
      "step": 912,
      "training_loss": 6.455092430114746
    },
    {
      "epoch": 0.049485094850948506,
      "step": 913,
      "training_loss": 7.590631008148193
    },
    {
      "epoch": 0.04953929539295393,
      "step": 914,
      "training_loss": 6.278628826141357
    },
    {
      "epoch": 0.04959349593495935,
      "step": 915,
      "training_loss": 7.92773962020874
    },
    {
      "epoch": 0.04964769647696477,
      "grad_norm": 17.33404541015625,
      "learning_rate": 1e-05,
      "loss": 7.063,
      "step": 916
    },
    {
      "epoch": 0.04964769647696477,
      "step": 916,
      "training_loss": 7.086029529571533
    },
    {
      "epoch": 0.04970189701897019,
      "step": 917,
      "training_loss": 7.635897636413574
    },
    {
      "epoch": 0.04975609756097561,
      "step": 918,
      "training_loss": 7.731043815612793
    },
    {
      "epoch": 0.04981029810298103,
      "step": 919,
      "training_loss": 7.639596939086914
    },
    {
      "epoch": 0.04986449864498645,
      "grad_norm": 18.917165756225586,
      "learning_rate": 1e-05,
      "loss": 7.5231,
      "step": 920
    },
    {
      "epoch": 0.04986449864498645,
      "step": 920,
      "training_loss": 6.5616278648376465
    },
    {
      "epoch": 0.04991869918699187,
      "step": 921,
      "training_loss": 6.927579402923584
    },
    {
      "epoch": 0.04997289972899729,
      "step": 922,
      "training_loss": 9.750565528869629
    },
    {
      "epoch": 0.05002710027100271,
      "step": 923,
      "training_loss": 8.237527847290039
    },
    {
      "epoch": 0.05008130081300813,
      "grad_norm": 25.651338577270508,
      "learning_rate": 1e-05,
      "loss": 7.8693,
      "step": 924
    },
    {
      "epoch": 0.05008130081300813,
      "step": 924,
      "training_loss": 8.394700050354004
    },
    {
      "epoch": 0.05013550135501355,
      "step": 925,
      "training_loss": 6.714446544647217
    },
    {
      "epoch": 0.05018970189701897,
      "step": 926,
      "training_loss": 7.629763126373291
    },
    {
      "epoch": 0.05024390243902439,
      "step": 927,
      "training_loss": 7.59296178817749
    },
    {
      "epoch": 0.05029810298102981,
      "grad_norm": 25.557334899902344,
      "learning_rate": 1e-05,
      "loss": 7.583,
      "step": 928
    },
    {
      "epoch": 0.05029810298102981,
      "step": 928,
      "training_loss": 6.919543266296387
    },
    {
      "epoch": 0.05035230352303523,
      "step": 929,
      "training_loss": 6.082066535949707
    },
    {
      "epoch": 0.05040650406504065,
      "step": 930,
      "training_loss": 5.740822792053223
    },
    {
      "epoch": 0.05046070460704607,
      "step": 931,
      "training_loss": 5.476573944091797
    },
    {
      "epoch": 0.05051490514905149,
      "grad_norm": 22.110902786254883,
      "learning_rate": 1e-05,
      "loss": 6.0548,
      "step": 932
    },
    {
      "epoch": 0.05051490514905149,
      "step": 932,
      "training_loss": 7.100552082061768
    },
    {
      "epoch": 0.05056910569105691,
      "step": 933,
      "training_loss": 7.1756815910339355
    },
    {
      "epoch": 0.05062330623306233,
      "step": 934,
      "training_loss": 6.261508941650391
    },
    {
      "epoch": 0.05067750677506775,
      "step": 935,
      "training_loss": 7.468195915222168
    },
    {
      "epoch": 0.050731707317073174,
      "grad_norm": 24.568561553955078,
      "learning_rate": 1e-05,
      "loss": 7.0015,
      "step": 936
    },
    {
      "epoch": 0.050731707317073174,
      "step": 936,
      "training_loss": 7.972352504730225
    },
    {
      "epoch": 0.05078590785907859,
      "step": 937,
      "training_loss": 5.584224700927734
    },
    {
      "epoch": 0.05084010840108401,
      "step": 938,
      "training_loss": 7.157180309295654
    },
    {
      "epoch": 0.05089430894308943,
      "step": 939,
      "training_loss": 7.206649303436279
    },
    {
      "epoch": 0.05094850948509485,
      "grad_norm": 16.646568298339844,
      "learning_rate": 1e-05,
      "loss": 6.9801,
      "step": 940
    },
    {
      "epoch": 0.05094850948509485,
      "step": 940,
      "training_loss": 7.693033695220947
    },
    {
      "epoch": 0.05100271002710027,
      "step": 941,
      "training_loss": 6.436394691467285
    },
    {
      "epoch": 0.05105691056910569,
      "step": 942,
      "training_loss": 7.506513595581055
    },
    {
      "epoch": 0.051111111111111114,
      "step": 943,
      "training_loss": 7.7202067375183105
    },
    {
      "epoch": 0.05116531165311653,
      "grad_norm": 24.43022346496582,
      "learning_rate": 1e-05,
      "loss": 7.339,
      "step": 944
    },
    {
      "epoch": 0.05116531165311653,
      "step": 944,
      "training_loss": 6.399364948272705
    },
    {
      "epoch": 0.05121951219512195,
      "step": 945,
      "training_loss": 7.6773271560668945
    },
    {
      "epoch": 0.05127371273712737,
      "step": 946,
      "training_loss": 8.39827823638916
    },
    {
      "epoch": 0.05132791327913279,
      "step": 947,
      "training_loss": 7.55624532699585
    },
    {
      "epoch": 0.05138211382113821,
      "grad_norm": 15.871807098388672,
      "learning_rate": 1e-05,
      "loss": 7.5078,
      "step": 948
    },
    {
      "epoch": 0.05138211382113821,
      "step": 948,
      "training_loss": 6.345094203948975
    },
    {
      "epoch": 0.05143631436314363,
      "step": 949,
      "training_loss": 7.883281707763672
    },
    {
      "epoch": 0.051490514905149054,
      "step": 950,
      "training_loss": 7.269168376922607
    },
    {
      "epoch": 0.05154471544715447,
      "step": 951,
      "training_loss": 6.733432292938232
    },
    {
      "epoch": 0.051598915989159895,
      "grad_norm": 22.68878746032715,
      "learning_rate": 1e-05,
      "loss": 7.0577,
      "step": 952
    },
    {
      "epoch": 0.051598915989159895,
      "step": 952,
      "training_loss": 5.5579047203063965
    },
    {
      "epoch": 0.05165311653116531,
      "step": 953,
      "training_loss": 7.477138042449951
    },
    {
      "epoch": 0.05170731707317073,
      "step": 954,
      "training_loss": 7.50747013092041
    },
    {
      "epoch": 0.05176151761517615,
      "step": 955,
      "training_loss": 9.118658065795898
    },
    {
      "epoch": 0.05181571815718157,
      "grad_norm": 31.263294219970703,
      "learning_rate": 1e-05,
      "loss": 7.4153,
      "step": 956
    },
    {
      "epoch": 0.05181571815718157,
      "step": 956,
      "training_loss": 6.553242206573486
    },
    {
      "epoch": 0.051869918699186994,
      "step": 957,
      "training_loss": 7.7325310707092285
    },
    {
      "epoch": 0.05192411924119241,
      "step": 958,
      "training_loss": 7.319624423980713
    },
    {
      "epoch": 0.051978319783197835,
      "step": 959,
      "training_loss": 6.589485168457031
    },
    {
      "epoch": 0.05203252032520325,
      "grad_norm": 16.19386863708496,
      "learning_rate": 1e-05,
      "loss": 7.0487,
      "step": 960
    },
    {
      "epoch": 0.05203252032520325,
      "step": 960,
      "training_loss": 5.326778411865234
    },
    {
      "epoch": 0.05208672086720867,
      "step": 961,
      "training_loss": 7.784420490264893
    },
    {
      "epoch": 0.05214092140921409,
      "step": 962,
      "training_loss": 6.473048686981201
    },
    {
      "epoch": 0.05219512195121951,
      "step": 963,
      "training_loss": 7.905091762542725
    },
    {
      "epoch": 0.052249322493224934,
      "grad_norm": 16.46824073791504,
      "learning_rate": 1e-05,
      "loss": 6.8723,
      "step": 964
    },
    {
      "epoch": 0.052249322493224934,
      "step": 964,
      "training_loss": 6.160741806030273
    },
    {
      "epoch": 0.05230352303523035,
      "step": 965,
      "training_loss": 6.883854389190674
    },
    {
      "epoch": 0.052357723577235775,
      "step": 966,
      "training_loss": 6.336925983428955
    },
    {
      "epoch": 0.05241192411924119,
      "step": 967,
      "training_loss": 5.87310791015625
    },
    {
      "epoch": 0.052466124661246616,
      "grad_norm": 29.191425323486328,
      "learning_rate": 1e-05,
      "loss": 6.3137,
      "step": 968
    },
    {
      "epoch": 0.052466124661246616,
      "step": 968,
      "training_loss": 7.205433368682861
    },
    {
      "epoch": 0.05252032520325203,
      "step": 969,
      "training_loss": 7.132815837860107
    },
    {
      "epoch": 0.05257452574525745,
      "step": 970,
      "training_loss": 7.370950222015381
    },
    {
      "epoch": 0.052628726287262874,
      "step": 971,
      "training_loss": 7.025955677032471
    },
    {
      "epoch": 0.05268292682926829,
      "grad_norm": 12.859195709228516,
      "learning_rate": 1e-05,
      "loss": 7.1838,
      "step": 972
    },
    {
      "epoch": 0.05268292682926829,
      "step": 972,
      "training_loss": 7.627626895904541
    },
    {
      "epoch": 0.052737127371273715,
      "step": 973,
      "training_loss": 7.323227882385254
    },
    {
      "epoch": 0.05279132791327913,
      "step": 974,
      "training_loss": 7.576320171356201
    },
    {
      "epoch": 0.052845528455284556,
      "step": 975,
      "training_loss": 5.841156482696533
    },
    {
      "epoch": 0.05289972899728997,
      "grad_norm": 22.999853134155273,
      "learning_rate": 1e-05,
      "loss": 7.0921,
      "step": 976
    },
    {
      "epoch": 0.05289972899728997,
      "step": 976,
      "training_loss": 7.416243076324463
    },
    {
      "epoch": 0.05295392953929539,
      "step": 977,
      "training_loss": 7.807962417602539
    },
    {
      "epoch": 0.053008130081300814,
      "step": 978,
      "training_loss": 8.554228782653809
    },
    {
      "epoch": 0.05306233062330623,
      "step": 979,
      "training_loss": 7.037246227264404
    },
    {
      "epoch": 0.053116531165311655,
      "grad_norm": 20.322328567504883,
      "learning_rate": 1e-05,
      "loss": 7.7039,
      "step": 980
    },
    {
      "epoch": 0.053116531165311655,
      "step": 980,
      "training_loss": 6.495983600616455
    },
    {
      "epoch": 0.05317073170731707,
      "step": 981,
      "training_loss": 8.01674747467041
    },
    {
      "epoch": 0.053224932249322496,
      "step": 982,
      "training_loss": 6.966456413269043
    },
    {
      "epoch": 0.05327913279132791,
      "step": 983,
      "training_loss": 7.385901927947998
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 11.912016868591309,
      "learning_rate": 1e-05,
      "loss": 7.2163,
      "step": 984
    },
    {
      "epoch": 0.05333333333333334,
      "step": 984,
      "training_loss": 7.523409366607666
    },
    {
      "epoch": 0.053387533875338754,
      "step": 985,
      "training_loss": 8.920205116271973
    },
    {
      "epoch": 0.05344173441734417,
      "step": 986,
      "training_loss": 8.082175254821777
    },
    {
      "epoch": 0.053495934959349595,
      "step": 987,
      "training_loss": 8.559883117675781
    },
    {
      "epoch": 0.05355013550135501,
      "grad_norm": 26.502840042114258,
      "learning_rate": 1e-05,
      "loss": 8.2714,
      "step": 988
    },
    {
      "epoch": 0.05355013550135501,
      "step": 988,
      "training_loss": 6.126516819000244
    },
    {
      "epoch": 0.053604336043360436,
      "step": 989,
      "training_loss": 6.245349884033203
    },
    {
      "epoch": 0.05365853658536585,
      "step": 990,
      "training_loss": 8.391536712646484
    },
    {
      "epoch": 0.05371273712737128,
      "step": 991,
      "training_loss": 7.194152355194092
    },
    {
      "epoch": 0.053766937669376694,
      "grad_norm": 21.345285415649414,
      "learning_rate": 1e-05,
      "loss": 6.9894,
      "step": 992
    },
    {
      "epoch": 0.053766937669376694,
      "step": 992,
      "training_loss": 7.435711860656738
    },
    {
      "epoch": 0.05382113821138211,
      "step": 993,
      "training_loss": 6.294216632843018
    },
    {
      "epoch": 0.053875338753387535,
      "step": 994,
      "training_loss": 7.401108264923096
    },
    {
      "epoch": 0.05392953929539295,
      "step": 995,
      "training_loss": 6.909874439239502
    },
    {
      "epoch": 0.053983739837398376,
      "grad_norm": 12.897551536560059,
      "learning_rate": 1e-05,
      "loss": 7.0102,
      "step": 996
    },
    {
      "epoch": 0.053983739837398376,
      "step": 996,
      "training_loss": 6.611092567443848
    },
    {
      "epoch": 0.05403794037940379,
      "step": 997,
      "training_loss": 7.404722213745117
    },
    {
      "epoch": 0.05409214092140922,
      "step": 998,
      "training_loss": 7.886529922485352
    },
    {
      "epoch": 0.054146341463414634,
      "step": 999,
      "training_loss": 7.304025650024414
    },
    {
      "epoch": 0.05420054200542006,
      "grad_norm": 18.598155975341797,
      "learning_rate": 1e-05,
      "loss": 7.3016,
      "step": 1000
    },
    {
      "epoch": 0.05420054200542006,
      "step": 1000,
      "training_loss": 5.423346996307373
    },
    {
      "epoch": 0.054254742547425475,
      "step": 1001,
      "training_loss": 8.353638648986816
    },
    {
      "epoch": 0.05430894308943089,
      "step": 1002,
      "training_loss": 5.708527565002441
    },
    {
      "epoch": 0.054363143631436316,
      "step": 1003,
      "training_loss": 7.219461441040039
    },
    {
      "epoch": 0.05441734417344173,
      "grad_norm": 14.725149154663086,
      "learning_rate": 1e-05,
      "loss": 6.6762,
      "step": 1004
    },
    {
      "epoch": 0.05441734417344173,
      "step": 1004,
      "training_loss": 7.737529754638672
    },
    {
      "epoch": 0.05447154471544716,
      "step": 1005,
      "training_loss": 7.195572853088379
    },
    {
      "epoch": 0.054525745257452574,
      "step": 1006,
      "training_loss": 7.598361968994141
    },
    {
      "epoch": 0.054579945799458,
      "step": 1007,
      "training_loss": 6.809720516204834
    },
    {
      "epoch": 0.054634146341463415,
      "grad_norm": 14.658546447753906,
      "learning_rate": 1e-05,
      "loss": 7.3353,
      "step": 1008
    },
    {
      "epoch": 0.054634146341463415,
      "step": 1008,
      "training_loss": 8.26030445098877
    },
    {
      "epoch": 0.05468834688346883,
      "step": 1009,
      "training_loss": 8.589308738708496
    },
    {
      "epoch": 0.054742547425474256,
      "step": 1010,
      "training_loss": 7.513148784637451
    },
    {
      "epoch": 0.05479674796747967,
      "step": 1011,
      "training_loss": 8.2163667678833
    },
    {
      "epoch": 0.0548509485094851,
      "grad_norm": 23.300668716430664,
      "learning_rate": 1e-05,
      "loss": 8.1448,
      "step": 1012
    },
    {
      "epoch": 0.0548509485094851,
      "step": 1012,
      "training_loss": 7.554819583892822
    },
    {
      "epoch": 0.054905149051490514,
      "step": 1013,
      "training_loss": 8.300232887268066
    },
    {
      "epoch": 0.05495934959349594,
      "step": 1014,
      "training_loss": 7.958643913269043
    },
    {
      "epoch": 0.055013550135501355,
      "step": 1015,
      "training_loss": 6.952342510223389
    },
    {
      "epoch": 0.05506775067750678,
      "grad_norm": 10.139036178588867,
      "learning_rate": 1e-05,
      "loss": 7.6915,
      "step": 1016
    },
    {
      "epoch": 0.05506775067750678,
      "step": 1016,
      "training_loss": 7.0158371925354
    },
    {
      "epoch": 0.055121951219512196,
      "step": 1017,
      "training_loss": 6.726415157318115
    },
    {
      "epoch": 0.05517615176151761,
      "step": 1018,
      "training_loss": 6.993637561798096
    },
    {
      "epoch": 0.055230352303523036,
      "step": 1019,
      "training_loss": 7.459524631500244
    },
    {
      "epoch": 0.055284552845528454,
      "grad_norm": 16.256820678710938,
      "learning_rate": 1e-05,
      "loss": 7.0489,
      "step": 1020
    },
    {
      "epoch": 0.055284552845528454,
      "step": 1020,
      "training_loss": 6.971411228179932
    },
    {
      "epoch": 0.05533875338753388,
      "step": 1021,
      "training_loss": 8.128894805908203
    },
    {
      "epoch": 0.055392953929539294,
      "step": 1022,
      "training_loss": 7.179687976837158
    },
    {
      "epoch": 0.05544715447154472,
      "step": 1023,
      "training_loss": 7.043475151062012
    },
    {
      "epoch": 0.055501355013550135,
      "grad_norm": 14.338786125183105,
      "learning_rate": 1e-05,
      "loss": 7.3309,
      "step": 1024
    },
    {
      "epoch": 0.055501355013550135,
      "step": 1024,
      "training_loss": 5.971951961517334
    },
    {
      "epoch": 0.05555555555555555,
      "step": 1025,
      "training_loss": 6.324031352996826
    },
    {
      "epoch": 0.055609756097560976,
      "step": 1026,
      "training_loss": 8.277191162109375
    },
    {
      "epoch": 0.05566395663956639,
      "step": 1027,
      "training_loss": 7.003145694732666
    },
    {
      "epoch": 0.05571815718157182,
      "grad_norm": 15.781475067138672,
      "learning_rate": 1e-05,
      "loss": 6.8941,
      "step": 1028
    },
    {
      "epoch": 0.05571815718157182,
      "step": 1028,
      "training_loss": 8.174275398254395
    },
    {
      "epoch": 0.055772357723577234,
      "step": 1029,
      "training_loss": 7.52393102645874
    },
    {
      "epoch": 0.05582655826558266,
      "step": 1030,
      "training_loss": 6.024376392364502
    },
    {
      "epoch": 0.055880758807588075,
      "step": 1031,
      "training_loss": 6.678736209869385
    },
    {
      "epoch": 0.0559349593495935,
      "grad_norm": 22.665674209594727,
      "learning_rate": 1e-05,
      "loss": 7.1003,
      "step": 1032
    },
    {
      "epoch": 0.0559349593495935,
      "step": 1032,
      "training_loss": 7.96961784362793
    },
    {
      "epoch": 0.055989159891598916,
      "step": 1033,
      "training_loss": 8.026301383972168
    },
    {
      "epoch": 0.05604336043360433,
      "step": 1034,
      "training_loss": 7.402660369873047
    },
    {
      "epoch": 0.05609756097560976,
      "step": 1035,
      "training_loss": 6.123022556304932
    },
    {
      "epoch": 0.056151761517615174,
      "grad_norm": 25.17579460144043,
      "learning_rate": 1e-05,
      "loss": 7.3804,
      "step": 1036
    },
    {
      "epoch": 0.056151761517615174,
      "step": 1036,
      "training_loss": 7.847700595855713
    },
    {
      "epoch": 0.0562059620596206,
      "step": 1037,
      "training_loss": 7.650993824005127
    },
    {
      "epoch": 0.056260162601626015,
      "step": 1038,
      "training_loss": 7.159902572631836
    },
    {
      "epoch": 0.05631436314363144,
      "step": 1039,
      "training_loss": 6.795316696166992
    },
    {
      "epoch": 0.056368563685636856,
      "grad_norm": 19.09967613220215,
      "learning_rate": 1e-05,
      "loss": 7.3635,
      "step": 1040
    },
    {
      "epoch": 0.056368563685636856,
      "step": 1040,
      "training_loss": 7.257760047912598
    },
    {
      "epoch": 0.05642276422764227,
      "step": 1041,
      "training_loss": 6.9885077476501465
    },
    {
      "epoch": 0.0564769647696477,
      "step": 1042,
      "training_loss": 7.7369256019592285
    },
    {
      "epoch": 0.056531165311653114,
      "step": 1043,
      "training_loss": 8.525167465209961
    },
    {
      "epoch": 0.05658536585365854,
      "grad_norm": 15.99631118774414,
      "learning_rate": 1e-05,
      "loss": 7.6271,
      "step": 1044
    },
    {
      "epoch": 0.05658536585365854,
      "step": 1044,
      "training_loss": 5.7938737869262695
    },
    {
      "epoch": 0.056639566395663955,
      "step": 1045,
      "training_loss": 7.175351619720459
    },
    {
      "epoch": 0.05669376693766938,
      "step": 1046,
      "training_loss": 6.569903373718262
    },
    {
      "epoch": 0.056747967479674796,
      "step": 1047,
      "training_loss": 5.586272716522217
    },
    {
      "epoch": 0.05680216802168022,
      "grad_norm": 19.07814598083496,
      "learning_rate": 1e-05,
      "loss": 6.2814,
      "step": 1048
    },
    {
      "epoch": 0.05680216802168022,
      "step": 1048,
      "training_loss": 7.7877068519592285
    },
    {
      "epoch": 0.05685636856368564,
      "step": 1049,
      "training_loss": 7.5955400466918945
    },
    {
      "epoch": 0.056910569105691054,
      "step": 1050,
      "training_loss": 6.238462924957275
    },
    {
      "epoch": 0.05696476964769648,
      "step": 1051,
      "training_loss": 6.607945442199707
    },
    {
      "epoch": 0.057018970189701895,
      "grad_norm": 23.0045108795166,
      "learning_rate": 1e-05,
      "loss": 7.0574,
      "step": 1052
    },
    {
      "epoch": 0.057018970189701895,
      "step": 1052,
      "training_loss": 7.107295989990234
    },
    {
      "epoch": 0.05707317073170732,
      "step": 1053,
      "training_loss": 7.168727397918701
    },
    {
      "epoch": 0.057127371273712736,
      "step": 1054,
      "training_loss": 7.283388137817383
    },
    {
      "epoch": 0.05718157181571816,
      "step": 1055,
      "training_loss": 7.24202823638916
    },
    {
      "epoch": 0.05723577235772358,
      "grad_norm": 16.806169509887695,
      "learning_rate": 1e-05,
      "loss": 7.2004,
      "step": 1056
    },
    {
      "epoch": 0.05723577235772358,
      "step": 1056,
      "training_loss": 7.69818639755249
    },
    {
      "epoch": 0.057289972899728994,
      "step": 1057,
      "training_loss": 7.180434703826904
    },
    {
      "epoch": 0.05734417344173442,
      "step": 1058,
      "training_loss": 7.1734466552734375
    },
    {
      "epoch": 0.057398373983739835,
      "step": 1059,
      "training_loss": 7.024773597717285
    },
    {
      "epoch": 0.05745257452574526,
      "grad_norm": 13.300437927246094,
      "learning_rate": 1e-05,
      "loss": 7.2692,
      "step": 1060
    },
    {
      "epoch": 0.05745257452574526,
      "step": 1060,
      "training_loss": 6.418539524078369
    },
    {
      "epoch": 0.057506775067750676,
      "step": 1061,
      "training_loss": 7.101564407348633
    },
    {
      "epoch": 0.0575609756097561,
      "step": 1062,
      "training_loss": 6.283143997192383
    },
    {
      "epoch": 0.05761517615176152,
      "step": 1063,
      "training_loss": 8.322888374328613
    },
    {
      "epoch": 0.05766937669376694,
      "grad_norm": 20.906089782714844,
      "learning_rate": 1e-05,
      "loss": 7.0315,
      "step": 1064
    },
    {
      "epoch": 0.05766937669376694,
      "step": 1064,
      "training_loss": 8.54878044128418
    },
    {
      "epoch": 0.05772357723577236,
      "step": 1065,
      "training_loss": 7.914668560028076
    },
    {
      "epoch": 0.057777777777777775,
      "step": 1066,
      "training_loss": 5.867153167724609
    },
    {
      "epoch": 0.0578319783197832,
      "step": 1067,
      "training_loss": 7.627315521240234
    },
    {
      "epoch": 0.057886178861788616,
      "grad_norm": 14.542054176330566,
      "learning_rate": 1e-05,
      "loss": 7.4895,
      "step": 1068
    },
    {
      "epoch": 0.057886178861788616,
      "step": 1068,
      "training_loss": 7.8690290451049805
    },
    {
      "epoch": 0.05794037940379404,
      "step": 1069,
      "training_loss": 9.733686447143555
    },
    {
      "epoch": 0.05799457994579946,
      "step": 1070,
      "training_loss": 7.7432122230529785
    },
    {
      "epoch": 0.05804878048780488,
      "step": 1071,
      "training_loss": 7.2561235427856445
    },
    {
      "epoch": 0.0581029810298103,
      "grad_norm": 12.425250053405762,
      "learning_rate": 1e-05,
      "loss": 8.1505,
      "step": 1072
    },
    {
      "epoch": 0.0581029810298103,
      "step": 1072,
      "training_loss": 5.981330394744873
    },
    {
      "epoch": 0.058157181571815715,
      "step": 1073,
      "training_loss": 6.773984432220459
    },
    {
      "epoch": 0.05821138211382114,
      "step": 1074,
      "training_loss": 7.977208614349365
    },
    {
      "epoch": 0.058265582655826556,
      "step": 1075,
      "training_loss": 7.5596089363098145
    },
    {
      "epoch": 0.05831978319783198,
      "grad_norm": 21.413118362426758,
      "learning_rate": 1e-05,
      "loss": 7.073,
      "step": 1076
    },
    {
      "epoch": 0.05831978319783198,
      "step": 1076,
      "training_loss": 6.7426934242248535
    },
    {
      "epoch": 0.0583739837398374,
      "step": 1077,
      "training_loss": 7.0705342292785645
    },
    {
      "epoch": 0.05842818428184282,
      "step": 1078,
      "training_loss": 6.2885260581970215
    },
    {
      "epoch": 0.05848238482384824,
      "step": 1079,
      "training_loss": 7.129429817199707
    },
    {
      "epoch": 0.05853658536585366,
      "grad_norm": 19.159372329711914,
      "learning_rate": 1e-05,
      "loss": 6.8078,
      "step": 1080
    },
    {
      "epoch": 0.05853658536585366,
      "step": 1080,
      "training_loss": 8.059964179992676
    },
    {
      "epoch": 0.05859078590785908,
      "step": 1081,
      "training_loss": 7.319461345672607
    },
    {
      "epoch": 0.058644986449864496,
      "step": 1082,
      "training_loss": 4.8918776512146
    },
    {
      "epoch": 0.05869918699186992,
      "step": 1083,
      "training_loss": 6.573831558227539
    },
    {
      "epoch": 0.05875338753387534,
      "grad_norm": 19.735681533813477,
      "learning_rate": 1e-05,
      "loss": 6.7113,
      "step": 1084
    },
    {
      "epoch": 0.05875338753387534,
      "step": 1084,
      "training_loss": 8.980514526367188
    },
    {
      "epoch": 0.05880758807588076,
      "step": 1085,
      "training_loss": 7.188004970550537
    },
    {
      "epoch": 0.05886178861788618,
      "step": 1086,
      "training_loss": 5.788382053375244
    },
    {
      "epoch": 0.0589159891598916,
      "step": 1087,
      "training_loss": 5.593386650085449
    },
    {
      "epoch": 0.05897018970189702,
      "grad_norm": 15.628474235534668,
      "learning_rate": 1e-05,
      "loss": 6.8876,
      "step": 1088
    },
    {
      "epoch": 0.05897018970189702,
      "step": 1088,
      "training_loss": 7.7224016189575195
    },
    {
      "epoch": 0.059024390243902436,
      "step": 1089,
      "training_loss": 6.692620277404785
    },
    {
      "epoch": 0.05907859078590786,
      "step": 1090,
      "training_loss": 7.205183029174805
    },
    {
      "epoch": 0.05913279132791328,
      "step": 1091,
      "training_loss": 7.5396223068237305
    },
    {
      "epoch": 0.0591869918699187,
      "grad_norm": 25.3314266204834,
      "learning_rate": 1e-05,
      "loss": 7.29,
      "step": 1092
    },
    {
      "epoch": 0.0591869918699187,
      "step": 1092,
      "training_loss": 5.666321277618408
    },
    {
      "epoch": 0.05924119241192412,
      "step": 1093,
      "training_loss": 7.4689764976501465
    },
    {
      "epoch": 0.05929539295392954,
      "step": 1094,
      "training_loss": 7.5067644119262695
    },
    {
      "epoch": 0.05934959349593496,
      "step": 1095,
      "training_loss": 6.197824478149414
    },
    {
      "epoch": 0.05940379403794038,
      "grad_norm": 15.697505950927734,
      "learning_rate": 1e-05,
      "loss": 6.71,
      "step": 1096
    },
    {
      "epoch": 0.05940379403794038,
      "step": 1096,
      "training_loss": 7.322272300720215
    },
    {
      "epoch": 0.0594579945799458,
      "step": 1097,
      "training_loss": 8.127367973327637
    },
    {
      "epoch": 0.05951219512195122,
      "step": 1098,
      "training_loss": 6.487936973571777
    },
    {
      "epoch": 0.05956639566395664,
      "step": 1099,
      "training_loss": 6.665485382080078
    },
    {
      "epoch": 0.05962059620596206,
      "grad_norm": 48.31156539916992,
      "learning_rate": 1e-05,
      "loss": 7.1508,
      "step": 1100
    },
    {
      "epoch": 0.05962059620596206,
      "step": 1100,
      "training_loss": 7.958281993865967
    },
    {
      "epoch": 0.05967479674796748,
      "step": 1101,
      "training_loss": 7.2205810546875
    },
    {
      "epoch": 0.0597289972899729,
      "step": 1102,
      "training_loss": 6.436598777770996
    },
    {
      "epoch": 0.05978319783197832,
      "step": 1103,
      "training_loss": 7.073470115661621
    },
    {
      "epoch": 0.05983739837398374,
      "grad_norm": 15.994619369506836,
      "learning_rate": 1e-05,
      "loss": 7.1722,
      "step": 1104
    },
    {
      "epoch": 0.05983739837398374,
      "step": 1104,
      "training_loss": 6.58692741394043
    },
    {
      "epoch": 0.05989159891598916,
      "step": 1105,
      "training_loss": 7.67822790145874
    },
    {
      "epoch": 0.05994579945799458,
      "step": 1106,
      "training_loss": 5.738107204437256
    },
    {
      "epoch": 0.06,
      "step": 1107,
      "training_loss": 5.804506778717041
    },
    {
      "epoch": 0.06005420054200542,
      "grad_norm": 16.356191635131836,
      "learning_rate": 1e-05,
      "loss": 6.4519,
      "step": 1108
    },
    {
      "epoch": 0.06005420054200542,
      "step": 1108,
      "training_loss": 7.362017631530762
    },
    {
      "epoch": 0.06010840108401084,
      "step": 1109,
      "training_loss": 7.348652362823486
    },
    {
      "epoch": 0.06016260162601626,
      "step": 1110,
      "training_loss": 7.012127876281738
    },
    {
      "epoch": 0.06021680216802168,
      "step": 1111,
      "training_loss": 7.323354721069336
    },
    {
      "epoch": 0.060271002710027104,
      "grad_norm": 16.182912826538086,
      "learning_rate": 1e-05,
      "loss": 7.2615,
      "step": 1112
    },
    {
      "epoch": 0.060271002710027104,
      "step": 1112,
      "training_loss": 7.774411201477051
    },
    {
      "epoch": 0.06032520325203252,
      "step": 1113,
      "training_loss": 7.275075435638428
    },
    {
      "epoch": 0.06037940379403794,
      "step": 1114,
      "training_loss": 7.010379314422607
    },
    {
      "epoch": 0.06043360433604336,
      "step": 1115,
      "training_loss": 8.298691749572754
    },
    {
      "epoch": 0.06048780487804878,
      "grad_norm": 32.006248474121094,
      "learning_rate": 1e-05,
      "loss": 7.5896,
      "step": 1116
    },
    {
      "epoch": 0.06048780487804878,
      "step": 1116,
      "training_loss": 6.9379048347473145
    },
    {
      "epoch": 0.0605420054200542,
      "step": 1117,
      "training_loss": 7.320272445678711
    },
    {
      "epoch": 0.06059620596205962,
      "step": 1118,
      "training_loss": 7.9644975662231445
    },
    {
      "epoch": 0.060650406504065044,
      "step": 1119,
      "training_loss": 7.903962135314941
    },
    {
      "epoch": 0.06070460704607046,
      "grad_norm": 27.69833755493164,
      "learning_rate": 1e-05,
      "loss": 7.5317,
      "step": 1120
    },
    {
      "epoch": 0.06070460704607046,
      "step": 1120,
      "training_loss": 7.244228839874268
    },
    {
      "epoch": 0.06075880758807588,
      "step": 1121,
      "training_loss": 9.534761428833008
    },
    {
      "epoch": 0.0608130081300813,
      "step": 1122,
      "training_loss": 6.4127912521362305
    },
    {
      "epoch": 0.06086720867208672,
      "step": 1123,
      "training_loss": 7.024827003479004
    },
    {
      "epoch": 0.06092140921409214,
      "grad_norm": 14.304356575012207,
      "learning_rate": 1e-05,
      "loss": 7.5542,
      "step": 1124
    },
    {
      "epoch": 0.06092140921409214,
      "step": 1124,
      "training_loss": 7.087276935577393
    },
    {
      "epoch": 0.06097560975609756,
      "step": 1125,
      "training_loss": 8.39791202545166
    },
    {
      "epoch": 0.061029810298102984,
      "step": 1126,
      "training_loss": 7.025268077850342
    },
    {
      "epoch": 0.0610840108401084,
      "step": 1127,
      "training_loss": 6.795405864715576
    },
    {
      "epoch": 0.061138211382113825,
      "grad_norm": 20.11495590209961,
      "learning_rate": 1e-05,
      "loss": 7.3265,
      "step": 1128
    },
    {
      "epoch": 0.061138211382113825,
      "step": 1128,
      "training_loss": 7.0560994148254395
    },
    {
      "epoch": 0.06119241192411924,
      "step": 1129,
      "training_loss": 6.819112300872803
    },
    {
      "epoch": 0.06124661246612466,
      "step": 1130,
      "training_loss": 6.259636402130127
    },
    {
      "epoch": 0.06130081300813008,
      "step": 1131,
      "training_loss": 6.642788887023926
    },
    {
      "epoch": 0.0613550135501355,
      "grad_norm": 35.22883987426758,
      "learning_rate": 1e-05,
      "loss": 6.6944,
      "step": 1132
    },
    {
      "epoch": 0.0613550135501355,
      "step": 1132,
      "training_loss": 8.563484191894531
    },
    {
      "epoch": 0.061409214092140924,
      "step": 1133,
      "training_loss": 8.603116989135742
    },
    {
      "epoch": 0.06146341463414634,
      "step": 1134,
      "training_loss": 4.8863983154296875
    },
    {
      "epoch": 0.061517615176151765,
      "step": 1135,
      "training_loss": 7.302838325500488
    },
    {
      "epoch": 0.06157181571815718,
      "grad_norm": 24.517066955566406,
      "learning_rate": 1e-05,
      "loss": 7.339,
      "step": 1136
    },
    {
      "epoch": 0.06157181571815718,
      "step": 1136,
      "training_loss": 7.574068546295166
    },
    {
      "epoch": 0.0616260162601626,
      "step": 1137,
      "training_loss": 7.282171726226807
    },
    {
      "epoch": 0.06168021680216802,
      "step": 1138,
      "training_loss": 8.10904598236084
    },
    {
      "epoch": 0.06173441734417344,
      "step": 1139,
      "training_loss": 6.1899614334106445
    },
    {
      "epoch": 0.061788617886178863,
      "grad_norm": 35.59292984008789,
      "learning_rate": 1e-05,
      "loss": 7.2888,
      "step": 1140
    },
    {
      "epoch": 0.061788617886178863,
      "step": 1140,
      "training_loss": 7.364821434020996
    },
    {
      "epoch": 0.06184281842818428,
      "step": 1141,
      "training_loss": 7.42295503616333
    },
    {
      "epoch": 0.061897018970189704,
      "step": 1142,
      "training_loss": 7.42645263671875
    },
    {
      "epoch": 0.06195121951219512,
      "step": 1143,
      "training_loss": 7.762019157409668
    },
    {
      "epoch": 0.062005420054200545,
      "grad_norm": 21.304540634155273,
      "learning_rate": 1e-05,
      "loss": 7.4941,
      "step": 1144
    },
    {
      "epoch": 0.062005420054200545,
      "step": 1144,
      "training_loss": 7.9398603439331055
    },
    {
      "epoch": 0.06205962059620596,
      "step": 1145,
      "training_loss": 6.86506986618042
    },
    {
      "epoch": 0.06211382113821138,
      "step": 1146,
      "training_loss": 6.883852005004883
    },
    {
      "epoch": 0.0621680216802168,
      "step": 1147,
      "training_loss": 9.342705726623535
    },
    {
      "epoch": 0.06222222222222222,
      "grad_norm": 25.14708137512207,
      "learning_rate": 1e-05,
      "loss": 7.7579,
      "step": 1148
    },
    {
      "epoch": 0.06222222222222222,
      "step": 1148,
      "training_loss": 6.3046875
    },
    {
      "epoch": 0.062276422764227644,
      "step": 1149,
      "training_loss": 7.595922946929932
    },
    {
      "epoch": 0.06233062330623306,
      "step": 1150,
      "training_loss": 6.738684177398682
    },
    {
      "epoch": 0.062384823848238485,
      "step": 1151,
      "training_loss": 6.993240833282471
    },
    {
      "epoch": 0.0624390243902439,
      "grad_norm": 17.268102645874023,
      "learning_rate": 1e-05,
      "loss": 6.9081,
      "step": 1152
    },
    {
      "epoch": 0.0624390243902439,
      "step": 1152,
      "training_loss": 7.0030012130737305
    },
    {
      "epoch": 0.06249322493224932,
      "step": 1153,
      "training_loss": 7.204982757568359
    },
    {
      "epoch": 0.06254742547425474,
      "step": 1154,
      "training_loss": 5.619539260864258
    },
    {
      "epoch": 0.06260162601626017,
      "step": 1155,
      "training_loss": 8.045683860778809
    },
    {
      "epoch": 0.06265582655826558,
      "grad_norm": 24.4381160736084,
      "learning_rate": 1e-05,
      "loss": 6.9683,
      "step": 1156
    },
    {
      "epoch": 0.06265582655826558,
      "step": 1156,
      "training_loss": 7.545524597167969
    },
    {
      "epoch": 0.062710027100271,
      "step": 1157,
      "training_loss": 8.503303527832031
    },
    {
      "epoch": 0.06276422764227642,
      "step": 1158,
      "training_loss": 7.1374592781066895
    },
    {
      "epoch": 0.06281842818428185,
      "step": 1159,
      "training_loss": 8.5801420211792
    },
    {
      "epoch": 0.06287262872628727,
      "grad_norm": 22.50828742980957,
      "learning_rate": 1e-05,
      "loss": 7.9416,
      "step": 1160
    },
    {
      "epoch": 0.06287262872628727,
      "step": 1160,
      "training_loss": 6.086567401885986
    },
    {
      "epoch": 0.06292682926829268,
      "step": 1161,
      "training_loss": 5.9756245613098145
    },
    {
      "epoch": 0.0629810298102981,
      "step": 1162,
      "training_loss": 7.592171669006348
    },
    {
      "epoch": 0.06303523035230352,
      "step": 1163,
      "training_loss": 5.462793350219727
    },
    {
      "epoch": 0.06308943089430895,
      "grad_norm": 18.95348358154297,
      "learning_rate": 1e-05,
      "loss": 6.2793,
      "step": 1164
    },
    {
      "epoch": 0.06308943089430895,
      "step": 1164,
      "training_loss": 5.664377689361572
    },
    {
      "epoch": 0.06314363143631437,
      "step": 1165,
      "training_loss": 6.78983736038208
    },
    {
      "epoch": 0.06319783197831978,
      "step": 1166,
      "training_loss": 6.579728603363037
    },
    {
      "epoch": 0.0632520325203252,
      "step": 1167,
      "training_loss": 7.7860026359558105
    },
    {
      "epoch": 0.06330623306233063,
      "grad_norm": 20.34470558166504,
      "learning_rate": 1e-05,
      "loss": 6.705,
      "step": 1168
    },
    {
      "epoch": 0.06330623306233063,
      "step": 1168,
      "training_loss": 6.937973976135254
    },
    {
      "epoch": 0.06336043360433605,
      "step": 1169,
      "training_loss": 6.824165344238281
    },
    {
      "epoch": 0.06341463414634146,
      "step": 1170,
      "training_loss": 7.609042167663574
    },
    {
      "epoch": 0.06346883468834688,
      "step": 1171,
      "training_loss": 6.742302894592285
    },
    {
      "epoch": 0.0635230352303523,
      "grad_norm": 29.362123489379883,
      "learning_rate": 1e-05,
      "loss": 7.0284,
      "step": 1172
    },
    {
      "epoch": 0.0635230352303523,
      "step": 1172,
      "training_loss": 7.029359817504883
    },
    {
      "epoch": 0.06357723577235773,
      "step": 1173,
      "training_loss": 8.914628028869629
    },
    {
      "epoch": 0.06363143631436315,
      "step": 1174,
      "training_loss": 6.150058746337891
    },
    {
      "epoch": 0.06368563685636856,
      "step": 1175,
      "training_loss": 5.799278259277344
    },
    {
      "epoch": 0.06373983739837398,
      "grad_norm": 17.885576248168945,
      "learning_rate": 1e-05,
      "loss": 6.9733,
      "step": 1176
    },
    {
      "epoch": 0.06373983739837398,
      "step": 1176,
      "training_loss": 6.958876132965088
    },
    {
      "epoch": 0.0637940379403794,
      "step": 1177,
      "training_loss": 7.831454277038574
    },
    {
      "epoch": 0.06384823848238483,
      "step": 1178,
      "training_loss": 7.596394062042236
    },
    {
      "epoch": 0.06390243902439025,
      "step": 1179,
      "training_loss": 7.480057716369629
    },
    {
      "epoch": 0.06395663956639566,
      "grad_norm": 24.247356414794922,
      "learning_rate": 1e-05,
      "loss": 7.4667,
      "step": 1180
    },
    {
      "epoch": 0.06395663956639566,
      "step": 1180,
      "training_loss": 7.515820503234863
    },
    {
      "epoch": 0.06401084010840108,
      "step": 1181,
      "training_loss": 6.420022964477539
    },
    {
      "epoch": 0.06406504065040651,
      "step": 1182,
      "training_loss": 6.783469200134277
    },
    {
      "epoch": 0.06411924119241193,
      "step": 1183,
      "training_loss": 8.36957836151123
    },
    {
      "epoch": 0.06417344173441734,
      "grad_norm": 21.682851791381836,
      "learning_rate": 1e-05,
      "loss": 7.2722,
      "step": 1184
    },
    {
      "epoch": 0.06417344173441734,
      "step": 1184,
      "training_loss": 8.501017570495605
    },
    {
      "epoch": 0.06422764227642276,
      "step": 1185,
      "training_loss": 6.5178728103637695
    },
    {
      "epoch": 0.06428184281842818,
      "step": 1186,
      "training_loss": 5.253070831298828
    },
    {
      "epoch": 0.06433604336043361,
      "step": 1187,
      "training_loss": 7.070279598236084
    },
    {
      "epoch": 0.06439024390243903,
      "grad_norm": 15.383620262145996,
      "learning_rate": 1e-05,
      "loss": 6.8356,
      "step": 1188
    },
    {
      "epoch": 0.06439024390243903,
      "step": 1188,
      "training_loss": 6.496452331542969
    },
    {
      "epoch": 0.06444444444444444,
      "step": 1189,
      "training_loss": 7.044044017791748
    },
    {
      "epoch": 0.06449864498644986,
      "step": 1190,
      "training_loss": 6.293707370758057
    },
    {
      "epoch": 0.06455284552845529,
      "step": 1191,
      "training_loss": 7.0924153327941895
    },
    {
      "epoch": 0.06460704607046071,
      "grad_norm": 18.98362159729004,
      "learning_rate": 1e-05,
      "loss": 6.7317,
      "step": 1192
    },
    {
      "epoch": 0.06460704607046071,
      "step": 1192,
      "training_loss": 7.6227192878723145
    },
    {
      "epoch": 0.06466124661246613,
      "step": 1193,
      "training_loss": 6.203481197357178
    },
    {
      "epoch": 0.06471544715447154,
      "step": 1194,
      "training_loss": 7.169016361236572
    },
    {
      "epoch": 0.06476964769647696,
      "step": 1195,
      "training_loss": 7.543185710906982
    },
    {
      "epoch": 0.06482384823848239,
      "grad_norm": 11.688157081604004,
      "learning_rate": 1e-05,
      "loss": 7.1346,
      "step": 1196
    },
    {
      "epoch": 0.06482384823848239,
      "step": 1196,
      "training_loss": 7.258532524108887
    },
    {
      "epoch": 0.06487804878048781,
      "step": 1197,
      "training_loss": 7.11359977722168
    },
    {
      "epoch": 0.06493224932249322,
      "step": 1198,
      "training_loss": 6.661787986755371
    },
    {
      "epoch": 0.06498644986449864,
      "step": 1199,
      "training_loss": 6.991456508636475
    },
    {
      "epoch": 0.06504065040650407,
      "grad_norm": 17.794189453125,
      "learning_rate": 1e-05,
      "loss": 7.0063,
      "step": 1200
    },
    {
      "epoch": 0.06504065040650407,
      "step": 1200,
      "training_loss": 7.901155948638916
    },
    {
      "epoch": 0.06509485094850949,
      "step": 1201,
      "training_loss": 8.227991104125977
    },
    {
      "epoch": 0.0651490514905149,
      "step": 1202,
      "training_loss": 6.608217716217041
    },
    {
      "epoch": 0.06520325203252032,
      "step": 1203,
      "training_loss": 7.616850852966309
    },
    {
      "epoch": 0.06525745257452574,
      "grad_norm": 20.819608688354492,
      "learning_rate": 1e-05,
      "loss": 7.5886,
      "step": 1204
    },
    {
      "epoch": 0.06525745257452574,
      "step": 1204,
      "training_loss": 7.557468891143799
    },
    {
      "epoch": 0.06531165311653117,
      "step": 1205,
      "training_loss": 6.961863994598389
    },
    {
      "epoch": 0.06536585365853659,
      "step": 1206,
      "training_loss": 7.553560733795166
    },
    {
      "epoch": 0.065420054200542,
      "step": 1207,
      "training_loss": 6.905263423919678
    },
    {
      "epoch": 0.06547425474254742,
      "grad_norm": 13.821636199951172,
      "learning_rate": 1e-05,
      "loss": 7.2445,
      "step": 1208
    },
    {
      "epoch": 0.06547425474254742,
      "step": 1208,
      "training_loss": 7.615242958068848
    },
    {
      "epoch": 0.06552845528455284,
      "step": 1209,
      "training_loss": 6.089145183563232
    },
    {
      "epoch": 0.06558265582655827,
      "step": 1210,
      "training_loss": 7.811246871948242
    },
    {
      "epoch": 0.06563685636856369,
      "step": 1211,
      "training_loss": 7.233102321624756
    },
    {
      "epoch": 0.0656910569105691,
      "grad_norm": 17.266504287719727,
      "learning_rate": 1e-05,
      "loss": 7.1872,
      "step": 1212
    },
    {
      "epoch": 0.0656910569105691,
      "step": 1212,
      "training_loss": 6.4957594871521
    },
    {
      "epoch": 0.06574525745257452,
      "step": 1213,
      "training_loss": 7.054527282714844
    },
    {
      "epoch": 0.06579945799457995,
      "step": 1214,
      "training_loss": 6.697423934936523
    },
    {
      "epoch": 0.06585365853658537,
      "step": 1215,
      "training_loss": 6.35510778427124
    },
    {
      "epoch": 0.06590785907859079,
      "grad_norm": 24.541244506835938,
      "learning_rate": 1e-05,
      "loss": 6.6507,
      "step": 1216
    },
    {
      "epoch": 0.06590785907859079,
      "step": 1216,
      "training_loss": 6.923638343811035
    },
    {
      "epoch": 0.0659620596205962,
      "step": 1217,
      "training_loss": 6.537091255187988
    },
    {
      "epoch": 0.06601626016260162,
      "step": 1218,
      "training_loss": 7.848398208618164
    },
    {
      "epoch": 0.06607046070460705,
      "step": 1219,
      "training_loss": 7.84337043762207
    },
    {
      "epoch": 0.06612466124661247,
      "grad_norm": 29.432851791381836,
      "learning_rate": 1e-05,
      "loss": 7.2881,
      "step": 1220
    },
    {
      "epoch": 0.06612466124661247,
      "step": 1220,
      "training_loss": 7.562560081481934
    },
    {
      "epoch": 0.06617886178861788,
      "step": 1221,
      "training_loss": 7.682186126708984
    },
    {
      "epoch": 0.0662330623306233,
      "step": 1222,
      "training_loss": 7.396001815795898
    },
    {
      "epoch": 0.06628726287262873,
      "step": 1223,
      "training_loss": 7.484533309936523
    },
    {
      "epoch": 0.06634146341463415,
      "grad_norm": 18.112091064453125,
      "learning_rate": 1e-05,
      "loss": 7.5313,
      "step": 1224
    },
    {
      "epoch": 0.06634146341463415,
      "step": 1224,
      "training_loss": 7.489201068878174
    },
    {
      "epoch": 0.06639566395663957,
      "step": 1225,
      "training_loss": 7.112617492675781
    },
    {
      "epoch": 0.06644986449864498,
      "step": 1226,
      "training_loss": 7.391746520996094
    },
    {
      "epoch": 0.0665040650406504,
      "step": 1227,
      "training_loss": 7.2776618003845215
    },
    {
      "epoch": 0.06655826558265583,
      "grad_norm": 15.220815658569336,
      "learning_rate": 1e-05,
      "loss": 7.3178,
      "step": 1228
    },
    {
      "epoch": 0.06655826558265583,
      "step": 1228,
      "training_loss": 7.107320785522461
    },
    {
      "epoch": 0.06661246612466125,
      "step": 1229,
      "training_loss": 7.567112922668457
    },
    {
      "epoch": 0.06666666666666667,
      "step": 1230,
      "training_loss": 6.95797061920166
    },
    {
      "epoch": 0.06672086720867208,
      "step": 1231,
      "training_loss": 7.65869665145874
    },
    {
      "epoch": 0.06677506775067751,
      "grad_norm": 16.2276668548584,
      "learning_rate": 1e-05,
      "loss": 7.3228,
      "step": 1232
    },
    {
      "epoch": 0.06677506775067751,
      "step": 1232,
      "training_loss": 7.971121788024902
    },
    {
      "epoch": 0.06682926829268293,
      "step": 1233,
      "training_loss": 6.615100860595703
    },
    {
      "epoch": 0.06688346883468835,
      "step": 1234,
      "training_loss": 7.738020420074463
    },
    {
      "epoch": 0.06693766937669376,
      "step": 1235,
      "training_loss": 7.199188232421875
    },
    {
      "epoch": 0.06699186991869918,
      "grad_norm": 20.82866859436035,
      "learning_rate": 1e-05,
      "loss": 7.3809,
      "step": 1236
    },
    {
      "epoch": 0.06699186991869918,
      "step": 1236,
      "training_loss": 6.486534595489502
    },
    {
      "epoch": 0.06704607046070461,
      "step": 1237,
      "training_loss": 8.544031143188477
    },
    {
      "epoch": 0.06710027100271003,
      "step": 1238,
      "training_loss": 6.856101036071777
    },
    {
      "epoch": 0.06715447154471545,
      "step": 1239,
      "training_loss": 5.30283260345459
    },
    {
      "epoch": 0.06720867208672086,
      "grad_norm": 16.347774505615234,
      "learning_rate": 1e-05,
      "loss": 6.7974,
      "step": 1240
    },
    {
      "epoch": 0.06720867208672086,
      "step": 1240,
      "training_loss": 7.23225212097168
    },
    {
      "epoch": 0.06726287262872628,
      "step": 1241,
      "training_loss": 6.07886266708374
    },
    {
      "epoch": 0.06731707317073171,
      "step": 1242,
      "training_loss": 6.329558372497559
    },
    {
      "epoch": 0.06737127371273713,
      "step": 1243,
      "training_loss": 6.819217205047607
    },
    {
      "epoch": 0.06742547425474255,
      "grad_norm": 22.68744659423828,
      "learning_rate": 1e-05,
      "loss": 6.615,
      "step": 1244
    },
    {
      "epoch": 0.06742547425474255,
      "step": 1244,
      "training_loss": 7.869333744049072
    },
    {
      "epoch": 0.06747967479674796,
      "step": 1245,
      "training_loss": 5.674101829528809
    },
    {
      "epoch": 0.0675338753387534,
      "step": 1246,
      "training_loss": 6.418008327484131
    },
    {
      "epoch": 0.06758807588075881,
      "step": 1247,
      "training_loss": 7.838515281677246
    },
    {
      "epoch": 0.06764227642276423,
      "grad_norm": 14.113866806030273,
      "learning_rate": 1e-05,
      "loss": 6.95,
      "step": 1248
    },
    {
      "epoch": 0.06764227642276423,
      "step": 1248,
      "training_loss": 5.779331207275391
    },
    {
      "epoch": 0.06769647696476964,
      "step": 1249,
      "training_loss": 6.970399379730225
    },
    {
      "epoch": 0.06775067750677506,
      "step": 1250,
      "training_loss": 7.544131755828857
    },
    {
      "epoch": 0.06780487804878049,
      "step": 1251,
      "training_loss": 5.306141376495361
    },
    {
      "epoch": 0.06785907859078591,
      "grad_norm": 21.736698150634766,
      "learning_rate": 1e-05,
      "loss": 6.4,
      "step": 1252
    },
    {
      "epoch": 0.06785907859078591,
      "step": 1252,
      "training_loss": 6.99942684173584
    },
    {
      "epoch": 0.06791327913279133,
      "step": 1253,
      "training_loss": 6.643209457397461
    },
    {
      "epoch": 0.06796747967479674,
      "step": 1254,
      "training_loss": 7.06787633895874
    },
    {
      "epoch": 0.06802168021680217,
      "step": 1255,
      "training_loss": 7.495128631591797
    },
    {
      "epoch": 0.06807588075880759,
      "grad_norm": 19.095090866088867,
      "learning_rate": 1e-05,
      "loss": 7.0514,
      "step": 1256
    },
    {
      "epoch": 0.06807588075880759,
      "step": 1256,
      "training_loss": 7.004937171936035
    },
    {
      "epoch": 0.06813008130081301,
      "step": 1257,
      "training_loss": 7.208305358886719
    },
    {
      "epoch": 0.06818428184281843,
      "step": 1258,
      "training_loss": 7.423425197601318
    },
    {
      "epoch": 0.06823848238482384,
      "step": 1259,
      "training_loss": 6.997070789337158
    },
    {
      "epoch": 0.06829268292682927,
      "grad_norm": 23.80327033996582,
      "learning_rate": 1e-05,
      "loss": 7.1584,
      "step": 1260
    },
    {
      "epoch": 0.06829268292682927,
      "step": 1260,
      "training_loss": 6.68543004989624
    },
    {
      "epoch": 0.06834688346883469,
      "step": 1261,
      "training_loss": 6.052387714385986
    },
    {
      "epoch": 0.06840108401084011,
      "step": 1262,
      "training_loss": 6.052464962005615
    },
    {
      "epoch": 0.06845528455284552,
      "step": 1263,
      "training_loss": 6.403851509094238
    },
    {
      "epoch": 0.06850948509485096,
      "grad_norm": 18.446142196655273,
      "learning_rate": 1e-05,
      "loss": 6.2985,
      "step": 1264
    },
    {
      "epoch": 0.06850948509485096,
      "step": 1264,
      "training_loss": 7.03385066986084
    },
    {
      "epoch": 0.06856368563685637,
      "step": 1265,
      "training_loss": 8.072417259216309
    },
    {
      "epoch": 0.06861788617886179,
      "step": 1266,
      "training_loss": 7.442957401275635
    },
    {
      "epoch": 0.0686720867208672,
      "step": 1267,
      "training_loss": 6.9312052726745605
    },
    {
      "epoch": 0.06872628726287262,
      "grad_norm": 16.32529067993164,
      "learning_rate": 1e-05,
      "loss": 7.3701,
      "step": 1268
    },
    {
      "epoch": 0.06872628726287262,
      "step": 1268,
      "training_loss": 6.8176045417785645
    },
    {
      "epoch": 0.06878048780487805,
      "step": 1269,
      "training_loss": 7.9781880378723145
    },
    {
      "epoch": 0.06883468834688347,
      "step": 1270,
      "training_loss": 6.628145694732666
    },
    {
      "epoch": 0.06888888888888889,
      "step": 1271,
      "training_loss": 7.4174089431762695
    },
    {
      "epoch": 0.0689430894308943,
      "grad_norm": 19.21282386779785,
      "learning_rate": 1e-05,
      "loss": 7.2103,
      "step": 1272
    },
    {
      "epoch": 0.0689430894308943,
      "step": 1272,
      "training_loss": 5.894661903381348
    },
    {
      "epoch": 0.06899728997289972,
      "step": 1273,
      "training_loss": 8.32895278930664
    },
    {
      "epoch": 0.06905149051490515,
      "step": 1274,
      "training_loss": 7.910914421081543
    },
    {
      "epoch": 0.06910569105691057,
      "step": 1275,
      "training_loss": 6.963369846343994
    },
    {
      "epoch": 0.06915989159891599,
      "grad_norm": 17.936185836791992,
      "learning_rate": 1e-05,
      "loss": 7.2745,
      "step": 1276
    },
    {
      "epoch": 0.06915989159891599,
      "step": 1276,
      "training_loss": 9.25946044921875
    },
    {
      "epoch": 0.0692140921409214,
      "step": 1277,
      "training_loss": 7.969833850860596
    },
    {
      "epoch": 0.06926829268292684,
      "step": 1278,
      "training_loss": 7.078672885894775
    },
    {
      "epoch": 0.06932249322493225,
      "step": 1279,
      "training_loss": 7.192852020263672
    },
    {
      "epoch": 0.06937669376693767,
      "grad_norm": 12.04131031036377,
      "learning_rate": 1e-05,
      "loss": 7.8752,
      "step": 1280
    },
    {
      "epoch": 0.06937669376693767,
      "step": 1280,
      "training_loss": 7.375736713409424
    },
    {
      "epoch": 0.06943089430894309,
      "step": 1281,
      "training_loss": 6.965559005737305
    },
    {
      "epoch": 0.0694850948509485,
      "step": 1282,
      "training_loss": 11.058287620544434
    },
    {
      "epoch": 0.06953929539295393,
      "step": 1283,
      "training_loss": 7.674808979034424
    },
    {
      "epoch": 0.06959349593495935,
      "grad_norm": 13.566668510437012,
      "learning_rate": 1e-05,
      "loss": 8.2686,
      "step": 1284
    },
    {
      "epoch": 0.06959349593495935,
      "step": 1284,
      "training_loss": 9.452164649963379
    },
    {
      "epoch": 0.06964769647696477,
      "step": 1285,
      "training_loss": 8.833514213562012
    },
    {
      "epoch": 0.06970189701897019,
      "step": 1286,
      "training_loss": 6.692436218261719
    },
    {
      "epoch": 0.06975609756097562,
      "step": 1287,
      "training_loss": 6.7555742263793945
    },
    {
      "epoch": 0.06981029810298103,
      "grad_norm": 12.378443717956543,
      "learning_rate": 1e-05,
      "loss": 7.9334,
      "step": 1288
    },
    {
      "epoch": 0.06981029810298103,
      "step": 1288,
      "training_loss": 7.519520282745361
    },
    {
      "epoch": 0.06986449864498645,
      "step": 1289,
      "training_loss": 6.439953327178955
    },
    {
      "epoch": 0.06991869918699187,
      "step": 1290,
      "training_loss": 6.692440509796143
    },
    {
      "epoch": 0.06997289972899728,
      "step": 1291,
      "training_loss": 6.244106292724609
    },
    {
      "epoch": 0.07002710027100272,
      "grad_norm": 12.969138145446777,
      "learning_rate": 1e-05,
      "loss": 6.724,
      "step": 1292
    },
    {
      "epoch": 0.07002710027100272,
      "step": 1292,
      "training_loss": 8.146893501281738
    },
    {
      "epoch": 0.07008130081300813,
      "step": 1293,
      "training_loss": 6.87779426574707
    },
    {
      "epoch": 0.07013550135501355,
      "step": 1294,
      "training_loss": 5.506044387817383
    },
    {
      "epoch": 0.07018970189701897,
      "step": 1295,
      "training_loss": 7.793099880218506
    },
    {
      "epoch": 0.0702439024390244,
      "grad_norm": 18.20217514038086,
      "learning_rate": 1e-05,
      "loss": 7.081,
      "step": 1296
    },
    {
      "epoch": 0.0702439024390244,
      "step": 1296,
      "training_loss": 7.330090045928955
    },
    {
      "epoch": 0.07029810298102981,
      "step": 1297,
      "training_loss": 7.517210006713867
    },
    {
      "epoch": 0.07035230352303523,
      "step": 1298,
      "training_loss": 7.536279201507568
    },
    {
      "epoch": 0.07040650406504065,
      "step": 1299,
      "training_loss": 6.697569847106934
    },
    {
      "epoch": 0.07046070460704607,
      "grad_norm": 23.272714614868164,
      "learning_rate": 1e-05,
      "loss": 7.2703,
      "step": 1300
    },
    {
      "epoch": 0.07046070460704607,
      "step": 1300,
      "training_loss": 7.980480194091797
    },
    {
      "epoch": 0.0705149051490515,
      "step": 1301,
      "training_loss": 5.9181928634643555
    },
    {
      "epoch": 0.07056910569105691,
      "step": 1302,
      "training_loss": 8.12515640258789
    },
    {
      "epoch": 0.07062330623306233,
      "step": 1303,
      "training_loss": 7.587394714355469
    },
    {
      "epoch": 0.07067750677506775,
      "grad_norm": 14.558653831481934,
      "learning_rate": 1e-05,
      "loss": 7.4028,
      "step": 1304
    },
    {
      "epoch": 0.07067750677506775,
      "step": 1304,
      "training_loss": 7.514743804931641
    },
    {
      "epoch": 0.07073170731707316,
      "step": 1305,
      "training_loss": 8.084977149963379
    },
    {
      "epoch": 0.0707859078590786,
      "step": 1306,
      "training_loss": 6.610060214996338
    },
    {
      "epoch": 0.07084010840108401,
      "step": 1307,
      "training_loss": 7.934784889221191
    },
    {
      "epoch": 0.07089430894308943,
      "grad_norm": 25.570402145385742,
      "learning_rate": 1e-05,
      "loss": 7.5361,
      "step": 1308
    },
    {
      "epoch": 0.07089430894308943,
      "step": 1308,
      "training_loss": 8.580503463745117
    },
    {
      "epoch": 0.07094850948509485,
      "step": 1309,
      "training_loss": 7.062569618225098
    },
    {
      "epoch": 0.07100271002710028,
      "step": 1310,
      "training_loss": 7.262284278869629
    },
    {
      "epoch": 0.0710569105691057,
      "step": 1311,
      "training_loss": 6.484193325042725
    },
    {
      "epoch": 0.07111111111111111,
      "grad_norm": 19.427053451538086,
      "learning_rate": 1e-05,
      "loss": 7.3474,
      "step": 1312
    },
    {
      "epoch": 0.07111111111111111,
      "step": 1312,
      "training_loss": 5.9641008377075195
    },
    {
      "epoch": 0.07116531165311653,
      "step": 1313,
      "training_loss": 6.6972808837890625
    },
    {
      "epoch": 0.07121951219512195,
      "step": 1314,
      "training_loss": 6.620757102966309
    },
    {
      "epoch": 0.07127371273712738,
      "step": 1315,
      "training_loss": 6.24309778213501
    },
    {
      "epoch": 0.07132791327913279,
      "grad_norm": 27.212825775146484,
      "learning_rate": 1e-05,
      "loss": 6.3813,
      "step": 1316
    },
    {
      "epoch": 0.07132791327913279,
      "step": 1316,
      "training_loss": 6.594232082366943
    },
    {
      "epoch": 0.07138211382113821,
      "step": 1317,
      "training_loss": 7.6760029792785645
    },
    {
      "epoch": 0.07143631436314363,
      "step": 1318,
      "training_loss": 7.49739408493042
    },
    {
      "epoch": 0.07149051490514906,
      "step": 1319,
      "training_loss": 7.373599052429199
    },
    {
      "epoch": 0.07154471544715447,
      "grad_norm": 26.727933883666992,
      "learning_rate": 1e-05,
      "loss": 7.2853,
      "step": 1320
    },
    {
      "epoch": 0.07154471544715447,
      "step": 1320,
      "training_loss": 6.636991500854492
    },
    {
      "epoch": 0.07159891598915989,
      "step": 1321,
      "training_loss": 7.629459381103516
    },
    {
      "epoch": 0.07165311653116531,
      "step": 1322,
      "training_loss": 6.804813861846924
    },
    {
      "epoch": 0.07170731707317073,
      "step": 1323,
      "training_loss": 6.669712543487549
    },
    {
      "epoch": 0.07176151761517616,
      "grad_norm": 13.523297309875488,
      "learning_rate": 1e-05,
      "loss": 6.9352,
      "step": 1324
    },
    {
      "epoch": 0.07176151761517616,
      "step": 1324,
      "training_loss": 9.069578170776367
    },
    {
      "epoch": 0.07181571815718157,
      "step": 1325,
      "training_loss": 8.159207344055176
    },
    {
      "epoch": 0.07186991869918699,
      "step": 1326,
      "training_loss": 9.148430824279785
    },
    {
      "epoch": 0.07192411924119241,
      "step": 1327,
      "training_loss": 7.067361831665039
    },
    {
      "epoch": 0.07197831978319784,
      "grad_norm": 25.682552337646484,
      "learning_rate": 1e-05,
      "loss": 8.3611,
      "step": 1328
    },
    {
      "epoch": 0.07197831978319784,
      "step": 1328,
      "training_loss": 7.775543212890625
    },
    {
      "epoch": 0.07203252032520326,
      "step": 1329,
      "training_loss": 7.616308212280273
    },
    {
      "epoch": 0.07208672086720867,
      "step": 1330,
      "training_loss": 7.10444974899292
    },
    {
      "epoch": 0.07214092140921409,
      "step": 1331,
      "training_loss": 8.170500755310059
    },
    {
      "epoch": 0.0721951219512195,
      "grad_norm": 16.557764053344727,
      "learning_rate": 1e-05,
      "loss": 7.6667,
      "step": 1332
    },
    {
      "epoch": 0.0721951219512195,
      "step": 1332,
      "training_loss": 5.681288242340088
    },
    {
      "epoch": 0.07224932249322494,
      "step": 1333,
      "training_loss": 7.5265960693359375
    },
    {
      "epoch": 0.07230352303523035,
      "step": 1334,
      "training_loss": 7.6290283203125
    },
    {
      "epoch": 0.07235772357723577,
      "step": 1335,
      "training_loss": 8.207475662231445
    },
    {
      "epoch": 0.07241192411924119,
      "grad_norm": 19.8802547454834,
      "learning_rate": 1e-05,
      "loss": 7.2611,
      "step": 1336
    },
    {
      "epoch": 0.07241192411924119,
      "step": 1336,
      "training_loss": 7.050888538360596
    },
    {
      "epoch": 0.0724661246612466,
      "step": 1337,
      "training_loss": 5.191099166870117
    },
    {
      "epoch": 0.07252032520325204,
      "step": 1338,
      "training_loss": 7.669728755950928
    },
    {
      "epoch": 0.07257452574525745,
      "step": 1339,
      "training_loss": 8.126837730407715
    },
    {
      "epoch": 0.07262872628726287,
      "grad_norm": 13.511286735534668,
      "learning_rate": 1e-05,
      "loss": 7.0096,
      "step": 1340
    },
    {
      "epoch": 0.07262872628726287,
      "step": 1340,
      "training_loss": 6.420637607574463
    },
    {
      "epoch": 0.07268292682926829,
      "step": 1341,
      "training_loss": 5.995233535766602
    },
    {
      "epoch": 0.07273712737127372,
      "step": 1342,
      "training_loss": 7.170783042907715
    },
    {
      "epoch": 0.07279132791327914,
      "step": 1343,
      "training_loss": 7.402502536773682
    },
    {
      "epoch": 0.07284552845528455,
      "grad_norm": 15.774076461791992,
      "learning_rate": 1e-05,
      "loss": 6.7473,
      "step": 1344
    },
    {
      "epoch": 0.07284552845528455,
      "step": 1344,
      "training_loss": 8.018043518066406
    },
    {
      "epoch": 0.07289972899728997,
      "step": 1345,
      "training_loss": 7.46043586730957
    },
    {
      "epoch": 0.07295392953929539,
      "step": 1346,
      "training_loss": 7.304476261138916
    },
    {
      "epoch": 0.07300813008130082,
      "step": 1347,
      "training_loss": 7.7636189460754395
    },
    {
      "epoch": 0.07306233062330623,
      "grad_norm": 20.593793869018555,
      "learning_rate": 1e-05,
      "loss": 7.6366,
      "step": 1348
    },
    {
      "epoch": 0.07306233062330623,
      "step": 1348,
      "training_loss": 6.435647487640381
    },
    {
      "epoch": 0.07311653116531165,
      "step": 1349,
      "training_loss": 7.217040061950684
    },
    {
      "epoch": 0.07317073170731707,
      "step": 1350,
      "training_loss": 7.993233680725098
    },
    {
      "epoch": 0.0732249322493225,
      "step": 1351,
      "training_loss": 6.630616664886475
    },
    {
      "epoch": 0.07327913279132792,
      "grad_norm": 13.688066482543945,
      "learning_rate": 1e-05,
      "loss": 7.0691,
      "step": 1352
    },
    {
      "epoch": 0.07327913279132792,
      "step": 1352,
      "training_loss": 8.521154403686523
    },
    {
      "epoch": 0.07333333333333333,
      "step": 1353,
      "training_loss": 6.450411796569824
    },
    {
      "epoch": 0.07338753387533875,
      "step": 1354,
      "training_loss": 7.5537238121032715
    },
    {
      "epoch": 0.07344173441734417,
      "step": 1355,
      "training_loss": 7.611783504486084
    },
    {
      "epoch": 0.0734959349593496,
      "grad_norm": 52.94451904296875,
      "learning_rate": 1e-05,
      "loss": 7.5343,
      "step": 1356
    },
    {
      "epoch": 0.0734959349593496,
      "step": 1356,
      "training_loss": 5.538791179656982
    },
    {
      "epoch": 0.07355013550135502,
      "step": 1357,
      "training_loss": 7.1327128410339355
    },
    {
      "epoch": 0.07360433604336043,
      "step": 1358,
      "training_loss": 7.333003997802734
    },
    {
      "epoch": 0.07365853658536585,
      "step": 1359,
      "training_loss": 6.648930549621582
    },
    {
      "epoch": 0.07371273712737128,
      "grad_norm": 17.89681053161621,
      "learning_rate": 1e-05,
      "loss": 6.6634,
      "step": 1360
    },
    {
      "epoch": 0.07371273712737128,
      "step": 1360,
      "training_loss": 6.737459182739258
    },
    {
      "epoch": 0.0737669376693767,
      "step": 1361,
      "training_loss": 7.360080242156982
    },
    {
      "epoch": 0.07382113821138211,
      "step": 1362,
      "training_loss": 7.177248001098633
    },
    {
      "epoch": 0.07387533875338753,
      "step": 1363,
      "training_loss": 8.061264991760254
    },
    {
      "epoch": 0.07392953929539295,
      "grad_norm": 24.463987350463867,
      "learning_rate": 1e-05,
      "loss": 7.334,
      "step": 1364
    },
    {
      "epoch": 0.07392953929539295,
      "step": 1364,
      "training_loss": 7.587916851043701
    },
    {
      "epoch": 0.07398373983739838,
      "step": 1365,
      "training_loss": 7.355554580688477
    },
    {
      "epoch": 0.0740379403794038,
      "step": 1366,
      "training_loss": 6.500428199768066
    },
    {
      "epoch": 0.07409214092140921,
      "step": 1367,
      "training_loss": 7.597382068634033
    },
    {
      "epoch": 0.07414634146341463,
      "grad_norm": 15.095193862915039,
      "learning_rate": 1e-05,
      "loss": 7.2603,
      "step": 1368
    },
    {
      "epoch": 0.07414634146341463,
      "step": 1368,
      "training_loss": 6.5892229080200195
    },
    {
      "epoch": 0.07420054200542005,
      "step": 1369,
      "training_loss": 6.494722843170166
    },
    {
      "epoch": 0.07425474254742548,
      "step": 1370,
      "training_loss": 7.964849948883057
    },
    {
      "epoch": 0.0743089430894309,
      "step": 1371,
      "training_loss": 8.512529373168945
    },
    {
      "epoch": 0.07436314363143631,
      "grad_norm": 21.977903366088867,
      "learning_rate": 1e-05,
      "loss": 7.3903,
      "step": 1372
    },
    {
      "epoch": 0.07436314363143631,
      "step": 1372,
      "training_loss": 7.559330463409424
    },
    {
      "epoch": 0.07441734417344173,
      "step": 1373,
      "training_loss": 7.239760875701904
    },
    {
      "epoch": 0.07447154471544716,
      "step": 1374,
      "training_loss": 7.072460174560547
    },
    {
      "epoch": 0.07452574525745258,
      "step": 1375,
      "training_loss": 7.119217395782471
    },
    {
      "epoch": 0.074579945799458,
      "grad_norm": 18.09760856628418,
      "learning_rate": 1e-05,
      "loss": 7.2477,
      "step": 1376
    },
    {
      "epoch": 0.074579945799458,
      "step": 1376,
      "training_loss": 7.294633388519287
    },
    {
      "epoch": 0.07463414634146341,
      "step": 1377,
      "training_loss": 6.429388523101807
    },
    {
      "epoch": 0.07468834688346883,
      "step": 1378,
      "training_loss": 7.016976356506348
    },
    {
      "epoch": 0.07474254742547426,
      "step": 1379,
      "training_loss": 6.5732951164245605
    },
    {
      "epoch": 0.07479674796747968,
      "grad_norm": 14.66733455657959,
      "learning_rate": 1e-05,
      "loss": 6.8286,
      "step": 1380
    },
    {
      "epoch": 0.07479674796747968,
      "step": 1380,
      "training_loss": 7.496856212615967
    },
    {
      "epoch": 0.0748509485094851,
      "step": 1381,
      "training_loss": 7.9602952003479
    },
    {
      "epoch": 0.07490514905149051,
      "step": 1382,
      "training_loss": 6.639913558959961
    },
    {
      "epoch": 0.07495934959349594,
      "step": 1383,
      "training_loss": 7.5907392501831055
    },
    {
      "epoch": 0.07501355013550136,
      "grad_norm": 17.689172744750977,
      "learning_rate": 1e-05,
      "loss": 7.422,
      "step": 1384
    },
    {
      "epoch": 0.07501355013550136,
      "step": 1384,
      "training_loss": 7.628103733062744
    },
    {
      "epoch": 0.07506775067750678,
      "step": 1385,
      "training_loss": 8.529654502868652
    },
    {
      "epoch": 0.07512195121951219,
      "step": 1386,
      "training_loss": 5.507836818695068
    },
    {
      "epoch": 0.07517615176151761,
      "step": 1387,
      "training_loss": 7.2611494064331055
    },
    {
      "epoch": 0.07523035230352304,
      "grad_norm": 29.80289077758789,
      "learning_rate": 1e-05,
      "loss": 7.2317,
      "step": 1388
    },
    {
      "epoch": 0.07523035230352304,
      "step": 1388,
      "training_loss": 7.598761081695557
    },
    {
      "epoch": 0.07528455284552846,
      "step": 1389,
      "training_loss": 6.10706901550293
    },
    {
      "epoch": 0.07533875338753387,
      "step": 1390,
      "training_loss": 9.289928436279297
    },
    {
      "epoch": 0.07539295392953929,
      "step": 1391,
      "training_loss": 8.023929595947266
    },
    {
      "epoch": 0.07544715447154472,
      "grad_norm": 30.00541877746582,
      "learning_rate": 1e-05,
      "loss": 7.7549,
      "step": 1392
    },
    {
      "epoch": 0.07544715447154472,
      "step": 1392,
      "training_loss": 6.688792705535889
    },
    {
      "epoch": 0.07550135501355014,
      "step": 1393,
      "training_loss": 7.545599937438965
    },
    {
      "epoch": 0.07555555555555556,
      "step": 1394,
      "training_loss": 8.403003692626953
    },
    {
      "epoch": 0.07560975609756097,
      "step": 1395,
      "training_loss": 7.7653279304504395
    },
    {
      "epoch": 0.07566395663956639,
      "grad_norm": 23.372568130493164,
      "learning_rate": 1e-05,
      "loss": 7.6007,
      "step": 1396
    },
    {
      "epoch": 0.07566395663956639,
      "step": 1396,
      "training_loss": 7.430633068084717
    },
    {
      "epoch": 0.07571815718157182,
      "step": 1397,
      "training_loss": 6.868967056274414
    },
    {
      "epoch": 0.07577235772357724,
      "step": 1398,
      "training_loss": 6.976731300354004
    },
    {
      "epoch": 0.07582655826558266,
      "step": 1399,
      "training_loss": 7.246401309967041
    },
    {
      "epoch": 0.07588075880758807,
      "grad_norm": 15.635798454284668,
      "learning_rate": 1e-05,
      "loss": 7.1307,
      "step": 1400
    },
    {
      "epoch": 0.07588075880758807,
      "step": 1400,
      "training_loss": 6.245474338531494
    },
    {
      "epoch": 0.07593495934959349,
      "step": 1401,
      "training_loss": 6.546582221984863
    },
    {
      "epoch": 0.07598915989159892,
      "step": 1402,
      "training_loss": 8.040111541748047
    },
    {
      "epoch": 0.07604336043360434,
      "step": 1403,
      "training_loss": 7.86308479309082
    },
    {
      "epoch": 0.07609756097560975,
      "grad_norm": 15.693978309631348,
      "learning_rate": 1e-05,
      "loss": 7.1738,
      "step": 1404
    },
    {
      "epoch": 0.07609756097560975,
      "step": 1404,
      "training_loss": 5.040076732635498
    },
    {
      "epoch": 0.07615176151761517,
      "step": 1405,
      "training_loss": 7.231629371643066
    },
    {
      "epoch": 0.0762059620596206,
      "step": 1406,
      "training_loss": 7.01637077331543
    },
    {
      "epoch": 0.07626016260162602,
      "step": 1407,
      "training_loss": 6.504293918609619
    },
    {
      "epoch": 0.07631436314363144,
      "grad_norm": 18.494321823120117,
      "learning_rate": 1e-05,
      "loss": 6.4481,
      "step": 1408
    },
    {
      "epoch": 0.07631436314363144,
      "step": 1408,
      "training_loss": 6.78996467590332
    },
    {
      "epoch": 0.07636856368563685,
      "step": 1409,
      "training_loss": 6.724791526794434
    },
    {
      "epoch": 0.07642276422764227,
      "step": 1410,
      "training_loss": 6.271981239318848
    },
    {
      "epoch": 0.0764769647696477,
      "step": 1411,
      "training_loss": 7.091998100280762
    },
    {
      "epoch": 0.07653116531165312,
      "grad_norm": 13.725008964538574,
      "learning_rate": 1e-05,
      "loss": 6.7197,
      "step": 1412
    },
    {
      "epoch": 0.07653116531165312,
      "step": 1412,
      "training_loss": 7.8359785079956055
    },
    {
      "epoch": 0.07658536585365854,
      "step": 1413,
      "training_loss": 7.817992687225342
    },
    {
      "epoch": 0.07663956639566395,
      "step": 1414,
      "training_loss": 7.339719295501709
    },
    {
      "epoch": 0.07669376693766938,
      "step": 1415,
      "training_loss": 7.190313339233398
    },
    {
      "epoch": 0.0767479674796748,
      "grad_norm": 37.538291931152344,
      "learning_rate": 1e-05,
      "loss": 7.546,
      "step": 1416
    },
    {
      "epoch": 0.0767479674796748,
      "step": 1416,
      "training_loss": 7.485129356384277
    },
    {
      "epoch": 0.07680216802168022,
      "step": 1417,
      "training_loss": 6.963754653930664
    },
    {
      "epoch": 0.07685636856368563,
      "step": 1418,
      "training_loss": 6.3950676918029785
    },
    {
      "epoch": 0.07691056910569105,
      "step": 1419,
      "training_loss": 8.116442680358887
    },
    {
      "epoch": 0.07696476964769648,
      "grad_norm": 14.048554420471191,
      "learning_rate": 1e-05,
      "loss": 7.2401,
      "step": 1420
    },
    {
      "epoch": 0.07696476964769648,
      "step": 1420,
      "training_loss": 6.996037006378174
    },
    {
      "epoch": 0.0770189701897019,
      "step": 1421,
      "training_loss": 8.285539627075195
    },
    {
      "epoch": 0.07707317073170732,
      "step": 1422,
      "training_loss": 6.649250507354736
    },
    {
      "epoch": 0.07712737127371273,
      "step": 1423,
      "training_loss": 7.309784412384033
    },
    {
      "epoch": 0.07718157181571816,
      "grad_norm": 19.633821487426758,
      "learning_rate": 1e-05,
      "loss": 7.3102,
      "step": 1424
    },
    {
      "epoch": 0.07718157181571816,
      "step": 1424,
      "training_loss": 6.99269962310791
    },
    {
      "epoch": 0.07723577235772358,
      "step": 1425,
      "training_loss": 6.368566513061523
    },
    {
      "epoch": 0.077289972899729,
      "step": 1426,
      "training_loss": 7.817479610443115
    },
    {
      "epoch": 0.07734417344173442,
      "step": 1427,
      "training_loss": 6.435400009155273
    },
    {
      "epoch": 0.07739837398373983,
      "grad_norm": 21.16888999938965,
      "learning_rate": 1e-05,
      "loss": 6.9035,
      "step": 1428
    },
    {
      "epoch": 0.07739837398373983,
      "step": 1428,
      "training_loss": 6.521190166473389
    },
    {
      "epoch": 0.07745257452574526,
      "step": 1429,
      "training_loss": 7.970839977264404
    },
    {
      "epoch": 0.07750677506775068,
      "step": 1430,
      "training_loss": 7.6834917068481445
    },
    {
      "epoch": 0.0775609756097561,
      "step": 1431,
      "training_loss": 7.40251350402832
    },
    {
      "epoch": 0.07761517615176151,
      "grad_norm": 23.670787811279297,
      "learning_rate": 1e-05,
      "loss": 7.3945,
      "step": 1432
    },
    {
      "epoch": 0.07761517615176151,
      "step": 1432,
      "training_loss": 9.035991668701172
    },
    {
      "epoch": 0.07766937669376693,
      "step": 1433,
      "training_loss": 6.817252159118652
    },
    {
      "epoch": 0.07772357723577236,
      "step": 1434,
      "training_loss": 6.138806343078613
    },
    {
      "epoch": 0.07777777777777778,
      "step": 1435,
      "training_loss": 7.0732011795043945
    },
    {
      "epoch": 0.0778319783197832,
      "grad_norm": 14.223114967346191,
      "learning_rate": 1e-05,
      "loss": 7.2663,
      "step": 1436
    },
    {
      "epoch": 0.0778319783197832,
      "step": 1436,
      "training_loss": 7.20578145980835
    },
    {
      "epoch": 0.07788617886178861,
      "step": 1437,
      "training_loss": 8.071422576904297
    },
    {
      "epoch": 0.07794037940379404,
      "step": 1438,
      "training_loss": 7.480517387390137
    },
    {
      "epoch": 0.07799457994579946,
      "step": 1439,
      "training_loss": 7.467219829559326
    },
    {
      "epoch": 0.07804878048780488,
      "grad_norm": 24.664318084716797,
      "learning_rate": 1e-05,
      "loss": 7.5562,
      "step": 1440
    },
    {
      "epoch": 0.07804878048780488,
      "step": 1440,
      "training_loss": 7.579808712005615
    },
    {
      "epoch": 0.0781029810298103,
      "step": 1441,
      "training_loss": 6.848506450653076
    },
    {
      "epoch": 0.07815718157181571,
      "step": 1442,
      "training_loss": 7.306116580963135
    },
    {
      "epoch": 0.07821138211382114,
      "step": 1443,
      "training_loss": 6.494561672210693
    },
    {
      "epoch": 0.07826558265582656,
      "grad_norm": 42.137451171875,
      "learning_rate": 1e-05,
      "loss": 7.0572,
      "step": 1444
    },
    {
      "epoch": 0.07826558265582656,
      "step": 1444,
      "training_loss": 7.392282009124756
    },
    {
      "epoch": 0.07831978319783198,
      "step": 1445,
      "training_loss": 6.134659290313721
    },
    {
      "epoch": 0.0783739837398374,
      "step": 1446,
      "training_loss": 5.638955593109131
    },
    {
      "epoch": 0.07842818428184282,
      "step": 1447,
      "training_loss": 6.898643493652344
    },
    {
      "epoch": 0.07848238482384824,
      "grad_norm": 23.83702278137207,
      "learning_rate": 1e-05,
      "loss": 6.5161,
      "step": 1448
    },
    {
      "epoch": 0.07848238482384824,
      "step": 1448,
      "training_loss": 6.566101551055908
    },
    {
      "epoch": 0.07853658536585366,
      "step": 1449,
      "training_loss": 6.311508655548096
    },
    {
      "epoch": 0.07859078590785908,
      "step": 1450,
      "training_loss": 7.895265579223633
    },
    {
      "epoch": 0.07864498644986449,
      "step": 1451,
      "training_loss": 7.614598274230957
    },
    {
      "epoch": 0.07869918699186992,
      "grad_norm": 17.752578735351562,
      "learning_rate": 1e-05,
      "loss": 7.0969,
      "step": 1452
    },
    {
      "epoch": 0.07869918699186992,
      "step": 1452,
      "training_loss": 6.970240592956543
    },
    {
      "epoch": 0.07875338753387534,
      "step": 1453,
      "training_loss": 8.099026679992676
    },
    {
      "epoch": 0.07880758807588076,
      "step": 1454,
      "training_loss": 7.569939136505127
    },
    {
      "epoch": 0.07886178861788617,
      "step": 1455,
      "training_loss": 7.156240463256836
    },
    {
      "epoch": 0.0789159891598916,
      "grad_norm": 18.15254783630371,
      "learning_rate": 1e-05,
      "loss": 7.4489,
      "step": 1456
    },
    {
      "epoch": 0.0789159891598916,
      "step": 1456,
      "training_loss": 7.655505180358887
    },
    {
      "epoch": 0.07897018970189702,
      "step": 1457,
      "training_loss": 6.60431432723999
    },
    {
      "epoch": 0.07902439024390244,
      "step": 1458,
      "training_loss": 7.953263282775879
    },
    {
      "epoch": 0.07907859078590786,
      "step": 1459,
      "training_loss": 7.432521820068359
    },
    {
      "epoch": 0.07913279132791327,
      "grad_norm": 21.70386505126953,
      "learning_rate": 1e-05,
      "loss": 7.4114,
      "step": 1460
    },
    {
      "epoch": 0.07913279132791327,
      "step": 1460,
      "training_loss": 6.045256614685059
    },
    {
      "epoch": 0.0791869918699187,
      "step": 1461,
      "training_loss": 6.47216796875
    },
    {
      "epoch": 0.07924119241192412,
      "step": 1462,
      "training_loss": 8.049202919006348
    },
    {
      "epoch": 0.07929539295392954,
      "step": 1463,
      "training_loss": 7.122693061828613
    },
    {
      "epoch": 0.07934959349593496,
      "grad_norm": 15.110715866088867,
      "learning_rate": 1e-05,
      "loss": 6.9223,
      "step": 1464
    },
    {
      "epoch": 0.07934959349593496,
      "step": 1464,
      "training_loss": 6.9110589027404785
    },
    {
      "epoch": 0.07940379403794037,
      "step": 1465,
      "training_loss": 6.566760063171387
    },
    {
      "epoch": 0.0794579945799458,
      "step": 1466,
      "training_loss": 5.219304084777832
    },
    {
      "epoch": 0.07951219512195122,
      "step": 1467,
      "training_loss": 7.674643516540527
    },
    {
      "epoch": 0.07956639566395664,
      "grad_norm": 13.845264434814453,
      "learning_rate": 1e-05,
      "loss": 6.5929,
      "step": 1468
    },
    {
      "epoch": 0.07956639566395664,
      "step": 1468,
      "training_loss": 7.560214519500732
    },
    {
      "epoch": 0.07962059620596205,
      "step": 1469,
      "training_loss": 5.237473964691162
    },
    {
      "epoch": 0.07967479674796749,
      "step": 1470,
      "training_loss": 6.023062705993652
    },
    {
      "epoch": 0.0797289972899729,
      "step": 1471,
      "training_loss": 7.002035140991211
    },
    {
      "epoch": 0.07978319783197832,
      "grad_norm": 20.4355411529541,
      "learning_rate": 1e-05,
      "loss": 6.4557,
      "step": 1472
    },
    {
      "epoch": 0.07978319783197832,
      "step": 1472,
      "training_loss": 6.8937907218933105
    },
    {
      "epoch": 0.07983739837398374,
      "step": 1473,
      "training_loss": 6.148158073425293
    },
    {
      "epoch": 0.07989159891598915,
      "step": 1474,
      "training_loss": 8.882543563842773
    },
    {
      "epoch": 0.07994579945799458,
      "step": 1475,
      "training_loss": 6.937169075012207
    },
    {
      "epoch": 0.08,
      "grad_norm": 25.23741912841797,
      "learning_rate": 1e-05,
      "loss": 7.2154,
      "step": 1476
    },
    {
      "epoch": 0.08,
      "step": 1476,
      "training_loss": 7.165998935699463
    },
    {
      "epoch": 0.08005420054200542,
      "step": 1477,
      "training_loss": 7.397009372711182
    },
    {
      "epoch": 0.08010840108401084,
      "step": 1478,
      "training_loss": 5.513299465179443
    },
    {
      "epoch": 0.08016260162601627,
      "step": 1479,
      "training_loss": 6.627427577972412
    },
    {
      "epoch": 0.08021680216802168,
      "grad_norm": 14.394490242004395,
      "learning_rate": 1e-05,
      "loss": 6.6759,
      "step": 1480
    },
    {
      "epoch": 0.08021680216802168,
      "step": 1480,
      "training_loss": 6.956590175628662
    },
    {
      "epoch": 0.0802710027100271,
      "step": 1481,
      "training_loss": 6.424236297607422
    },
    {
      "epoch": 0.08032520325203252,
      "step": 1482,
      "training_loss": 7.15387487411499
    },
    {
      "epoch": 0.08037940379403793,
      "step": 1483,
      "training_loss": 6.621458053588867
    },
    {
      "epoch": 0.08043360433604337,
      "grad_norm": 16.38645362854004,
      "learning_rate": 1e-05,
      "loss": 6.789,
      "step": 1484
    },
    {
      "epoch": 0.08043360433604337,
      "step": 1484,
      "training_loss": 6.7173333168029785
    },
    {
      "epoch": 0.08048780487804878,
      "step": 1485,
      "training_loss": 5.35543155670166
    },
    {
      "epoch": 0.0805420054200542,
      "step": 1486,
      "training_loss": 8.152166366577148
    },
    {
      "epoch": 0.08059620596205962,
      "step": 1487,
      "training_loss": 7.406446933746338
    },
    {
      "epoch": 0.08065040650406505,
      "grad_norm": 20.081880569458008,
      "learning_rate": 1e-05,
      "loss": 6.9078,
      "step": 1488
    },
    {
      "epoch": 0.08065040650406505,
      "step": 1488,
      "training_loss": 7.458789348602295
    },
    {
      "epoch": 0.08070460704607046,
      "step": 1489,
      "training_loss": 6.390273571014404
    },
    {
      "epoch": 0.08075880758807588,
      "step": 1490,
      "training_loss": 7.265676975250244
    },
    {
      "epoch": 0.0808130081300813,
      "step": 1491,
      "training_loss": 8.377300262451172
    },
    {
      "epoch": 0.08086720867208672,
      "grad_norm": 22.48006820678711,
      "learning_rate": 1e-05,
      "loss": 7.373,
      "step": 1492
    },
    {
      "epoch": 0.08086720867208672,
      "step": 1492,
      "training_loss": 6.276495933532715
    },
    {
      "epoch": 0.08092140921409215,
      "step": 1493,
      "training_loss": 6.643661975860596
    },
    {
      "epoch": 0.08097560975609756,
      "step": 1494,
      "training_loss": 6.761312484741211
    },
    {
      "epoch": 0.08102981029810298,
      "step": 1495,
      "training_loss": 7.584204196929932
    },
    {
      "epoch": 0.0810840108401084,
      "grad_norm": 15.860672950744629,
      "learning_rate": 1e-05,
      "loss": 6.8164,
      "step": 1496
    },
    {
      "epoch": 0.0810840108401084,
      "step": 1496,
      "training_loss": 5.024026393890381
    },
    {
      "epoch": 0.08113821138211381,
      "step": 1497,
      "training_loss": 7.907277584075928
    },
    {
      "epoch": 0.08119241192411925,
      "step": 1498,
      "training_loss": 6.9513092041015625
    },
    {
      "epoch": 0.08124661246612466,
      "step": 1499,
      "training_loss": 6.9853410720825195
    },
    {
      "epoch": 0.08130081300813008,
      "grad_norm": 17.394542694091797,
      "learning_rate": 1e-05,
      "loss": 6.717,
      "step": 1500
    },
    {
      "epoch": 0.08130081300813008,
      "step": 1500,
      "training_loss": 6.105336666107178
    },
    {
      "epoch": 0.0813550135501355,
      "step": 1501,
      "training_loss": 7.092627048492432
    },
    {
      "epoch": 0.08140921409214093,
      "step": 1502,
      "training_loss": 6.558999538421631
    },
    {
      "epoch": 0.08146341463414634,
      "step": 1503,
      "training_loss": 8.096537590026855
    },
    {
      "epoch": 0.08151761517615176,
      "grad_norm": 22.192474365234375,
      "learning_rate": 1e-05,
      "loss": 6.9634,
      "step": 1504
    },
    {
      "epoch": 0.08151761517615176,
      "step": 1504,
      "training_loss": 7.470880508422852
    },
    {
      "epoch": 0.08157181571815718,
      "step": 1505,
      "training_loss": 6.671574115753174
    },
    {
      "epoch": 0.0816260162601626,
      "step": 1506,
      "training_loss": 6.071920394897461
    },
    {
      "epoch": 0.08168021680216803,
      "step": 1507,
      "training_loss": 6.956241130828857
    },
    {
      "epoch": 0.08173441734417344,
      "grad_norm": 26.47064781188965,
      "learning_rate": 1e-05,
      "loss": 6.7927,
      "step": 1508
    },
    {
      "epoch": 0.08173441734417344,
      "step": 1508,
      "training_loss": 6.8162970542907715
    },
    {
      "epoch": 0.08178861788617886,
      "step": 1509,
      "training_loss": 6.921463966369629
    },
    {
      "epoch": 0.08184281842818428,
      "step": 1510,
      "training_loss": 7.125483512878418
    },
    {
      "epoch": 0.08189701897018971,
      "step": 1511,
      "training_loss": 5.455466270446777
    },
    {
      "epoch": 0.08195121951219513,
      "grad_norm": 18.126893997192383,
      "learning_rate": 1e-05,
      "loss": 6.5797,
      "step": 1512
    },
    {
      "epoch": 0.08195121951219513,
      "step": 1512,
      "training_loss": 6.609006404876709
    },
    {
      "epoch": 0.08200542005420054,
      "step": 1513,
      "training_loss": 7.985594749450684
    },
    {
      "epoch": 0.08205962059620596,
      "step": 1514,
      "training_loss": 6.592886447906494
    },
    {
      "epoch": 0.08211382113821138,
      "step": 1515,
      "training_loss": 6.069246768951416
    },
    {
      "epoch": 0.08216802168021681,
      "grad_norm": 21.564937591552734,
      "learning_rate": 1e-05,
      "loss": 6.8142,
      "step": 1516
    },
    {
      "epoch": 0.08216802168021681,
      "step": 1516,
      "training_loss": 7.090270042419434
    },
    {
      "epoch": 0.08222222222222222,
      "step": 1517,
      "training_loss": 7.298571586608887
    },
    {
      "epoch": 0.08227642276422764,
      "step": 1518,
      "training_loss": 6.740000247955322
    },
    {
      "epoch": 0.08233062330623306,
      "step": 1519,
      "training_loss": 11.398785591125488
    },
    {
      "epoch": 0.08238482384823849,
      "grad_norm": 37.81239318847656,
      "learning_rate": 1e-05,
      "loss": 8.1319,
      "step": 1520
    },
    {
      "epoch": 0.08238482384823849,
      "step": 1520,
      "training_loss": 7.549437999725342
    },
    {
      "epoch": 0.0824390243902439,
      "step": 1521,
      "training_loss": 7.608947277069092
    },
    {
      "epoch": 0.08249322493224932,
      "step": 1522,
      "training_loss": 6.870312690734863
    },
    {
      "epoch": 0.08254742547425474,
      "step": 1523,
      "training_loss": 6.6610517501831055
    },
    {
      "epoch": 0.08260162601626016,
      "grad_norm": 25.696657180786133,
      "learning_rate": 1e-05,
      "loss": 7.1724,
      "step": 1524
    },
    {
      "epoch": 0.08260162601626016,
      "step": 1524,
      "training_loss": 7.4949750900268555
    },
    {
      "epoch": 0.08265582655826559,
      "step": 1525,
      "training_loss": 5.225578308105469
    },
    {
      "epoch": 0.082710027100271,
      "step": 1526,
      "training_loss": 7.049113750457764
    },
    {
      "epoch": 0.08276422764227642,
      "step": 1527,
      "training_loss": 7.2169189453125
    },
    {
      "epoch": 0.08281842818428184,
      "grad_norm": 24.93592071533203,
      "learning_rate": 1e-05,
      "loss": 6.7466,
      "step": 1528
    },
    {
      "epoch": 0.08281842818428184,
      "step": 1528,
      "training_loss": 7.409938335418701
    },
    {
      "epoch": 0.08287262872628726,
      "step": 1529,
      "training_loss": 7.165013790130615
    },
    {
      "epoch": 0.08292682926829269,
      "step": 1530,
      "training_loss": 8.702951431274414
    },
    {
      "epoch": 0.0829810298102981,
      "step": 1531,
      "training_loss": 8.115078926086426
    },
    {
      "epoch": 0.08303523035230352,
      "grad_norm": 23.805255889892578,
      "learning_rate": 1e-05,
      "loss": 7.8482,
      "step": 1532
    },
    {
      "epoch": 0.08303523035230352,
      "step": 1532,
      "training_loss": 7.938722610473633
    },
    {
      "epoch": 0.08308943089430894,
      "step": 1533,
      "training_loss": 5.741585731506348
    },
    {
      "epoch": 0.08314363143631437,
      "step": 1534,
      "training_loss": 7.104894638061523
    },
    {
      "epoch": 0.08319783197831979,
      "step": 1535,
      "training_loss": 6.0118327140808105
    },
    {
      "epoch": 0.0832520325203252,
      "grad_norm": 14.243522644042969,
      "learning_rate": 1e-05,
      "loss": 6.6993,
      "step": 1536
    },
    {
      "epoch": 0.0832520325203252,
      "step": 1536,
      "training_loss": 7.262784481048584
    },
    {
      "epoch": 0.08330623306233062,
      "step": 1537,
      "training_loss": 6.9191131591796875
    },
    {
      "epoch": 0.08336043360433604,
      "step": 1538,
      "training_loss": 7.988018989562988
    },
    {
      "epoch": 0.08341463414634147,
      "step": 1539,
      "training_loss": 8.090574264526367
    },
    {
      "epoch": 0.08346883468834689,
      "grad_norm": 19.91983413696289,
      "learning_rate": 1e-05,
      "loss": 7.5651,
      "step": 1540
    },
    {
      "epoch": 0.08346883468834689,
      "step": 1540,
      "training_loss": 8.881203651428223
    },
    {
      "epoch": 0.0835230352303523,
      "step": 1541,
      "training_loss": 7.409794330596924
    },
    {
      "epoch": 0.08357723577235772,
      "step": 1542,
      "training_loss": 8.56857967376709
    },
    {
      "epoch": 0.08363143631436315,
      "step": 1543,
      "training_loss": 6.943595886230469
    },
    {
      "epoch": 0.08368563685636857,
      "grad_norm": 15.645379066467285,
      "learning_rate": 1e-05,
      "loss": 7.9508,
      "step": 1544
    },
    {
      "epoch": 0.08368563685636857,
      "step": 1544,
      "training_loss": 7.356025218963623
    },
    {
      "epoch": 0.08373983739837398,
      "step": 1545,
      "training_loss": 7.733541488647461
    },
    {
      "epoch": 0.0837940379403794,
      "step": 1546,
      "training_loss": 7.200157642364502
    },
    {
      "epoch": 0.08384823848238482,
      "step": 1547,
      "training_loss": 7.594922065734863
    },
    {
      "epoch": 0.08390243902439025,
      "grad_norm": 13.607351303100586,
      "learning_rate": 1e-05,
      "loss": 7.4712,
      "step": 1548
    },
    {
      "epoch": 0.08390243902439025,
      "step": 1548,
      "training_loss": 6.70325231552124
    },
    {
      "epoch": 0.08395663956639567,
      "step": 1549,
      "training_loss": 7.938178539276123
    },
    {
      "epoch": 0.08401084010840108,
      "step": 1550,
      "training_loss": 8.155025482177734
    },
    {
      "epoch": 0.0840650406504065,
      "step": 1551,
      "training_loss": 7.428589344024658
    },
    {
      "epoch": 0.08411924119241193,
      "grad_norm": 31.541887283325195,
      "learning_rate": 1e-05,
      "loss": 7.5563,
      "step": 1552
    },
    {
      "epoch": 0.08411924119241193,
      "step": 1552,
      "training_loss": 6.888820648193359
    },
    {
      "epoch": 0.08417344173441735,
      "step": 1553,
      "training_loss": 6.773348331451416
    },
    {
      "epoch": 0.08422764227642277,
      "step": 1554,
      "training_loss": 7.2355217933654785
    },
    {
      "epoch": 0.08428184281842818,
      "step": 1555,
      "training_loss": 8.344488143920898
    },
    {
      "epoch": 0.0843360433604336,
      "grad_norm": 23.43170928955078,
      "learning_rate": 1e-05,
      "loss": 7.3105,
      "step": 1556
    },
    {
      "epoch": 0.0843360433604336,
      "step": 1556,
      "training_loss": 7.104752540588379
    },
    {
      "epoch": 0.08439024390243903,
      "step": 1557,
      "training_loss": 5.187212944030762
    },
    {
      "epoch": 0.08444444444444445,
      "step": 1558,
      "training_loss": 7.240516185760498
    },
    {
      "epoch": 0.08449864498644986,
      "step": 1559,
      "training_loss": 7.27593994140625
    },
    {
      "epoch": 0.08455284552845528,
      "grad_norm": 18.487150192260742,
      "learning_rate": 1e-05,
      "loss": 6.7021,
      "step": 1560
    },
    {
      "epoch": 0.08455284552845528,
      "step": 1560,
      "training_loss": 5.9194769859313965
    },
    {
      "epoch": 0.0846070460704607,
      "step": 1561,
      "training_loss": 7.973294258117676
    },
    {
      "epoch": 0.08466124661246613,
      "step": 1562,
      "training_loss": 7.3256096839904785
    },
    {
      "epoch": 0.08471544715447155,
      "step": 1563,
      "training_loss": 7.686769485473633
    },
    {
      "epoch": 0.08476964769647696,
      "grad_norm": 28.198734283447266,
      "learning_rate": 1e-05,
      "loss": 7.2263,
      "step": 1564
    },
    {
      "epoch": 0.08476964769647696,
      "step": 1564,
      "training_loss": 7.542346954345703
    },
    {
      "epoch": 0.08482384823848238,
      "step": 1565,
      "training_loss": 6.45596981048584
    },
    {
      "epoch": 0.08487804878048781,
      "step": 1566,
      "training_loss": 6.859743595123291
    },
    {
      "epoch": 0.08493224932249323,
      "step": 1567,
      "training_loss": 5.823577404022217
    },
    {
      "epoch": 0.08498644986449864,
      "grad_norm": 15.307823181152344,
      "learning_rate": 1e-05,
      "loss": 6.6704,
      "step": 1568
    },
    {
      "epoch": 0.08498644986449864,
      "step": 1568,
      "training_loss": 7.583731174468994
    },
    {
      "epoch": 0.08504065040650406,
      "step": 1569,
      "training_loss": 7.5811767578125
    },
    {
      "epoch": 0.08509485094850948,
      "step": 1570,
      "training_loss": 6.795912265777588
    },
    {
      "epoch": 0.08514905149051491,
      "step": 1571,
      "training_loss": 7.405450344085693
    },
    {
      "epoch": 0.08520325203252033,
      "grad_norm": 26.650653839111328,
      "learning_rate": 1e-05,
      "loss": 7.3416,
      "step": 1572
    },
    {
      "epoch": 0.08520325203252033,
      "step": 1572,
      "training_loss": 6.925144672393799
    },
    {
      "epoch": 0.08525745257452574,
      "step": 1573,
      "training_loss": 7.117498874664307
    },
    {
      "epoch": 0.08531165311653116,
      "step": 1574,
      "training_loss": 7.433722972869873
    },
    {
      "epoch": 0.08536585365853659,
      "step": 1575,
      "training_loss": 6.889317035675049
    },
    {
      "epoch": 0.08542005420054201,
      "grad_norm": 18.049654006958008,
      "learning_rate": 1e-05,
      "loss": 7.0914,
      "step": 1576
    },
    {
      "epoch": 0.08542005420054201,
      "step": 1576,
      "training_loss": 6.523434162139893
    },
    {
      "epoch": 0.08547425474254743,
      "step": 1577,
      "training_loss": 6.53915548324585
    },
    {
      "epoch": 0.08552845528455284,
      "step": 1578,
      "training_loss": 7.580345630645752
    },
    {
      "epoch": 0.08558265582655826,
      "step": 1579,
      "training_loss": 7.635035991668701
    },
    {
      "epoch": 0.08563685636856369,
      "grad_norm": 21.523439407348633,
      "learning_rate": 1e-05,
      "loss": 7.0695,
      "step": 1580
    },
    {
      "epoch": 0.08563685636856369,
      "step": 1580,
      "training_loss": 6.668606281280518
    },
    {
      "epoch": 0.08569105691056911,
      "step": 1581,
      "training_loss": 6.72382116317749
    },
    {
      "epoch": 0.08574525745257452,
      "step": 1582,
      "training_loss": 7.158984184265137
    },
    {
      "epoch": 0.08579945799457994,
      "step": 1583,
      "training_loss": 7.330589771270752
    },
    {
      "epoch": 0.08585365853658537,
      "grad_norm": 17.996816635131836,
      "learning_rate": 1e-05,
      "loss": 6.9705,
      "step": 1584
    },
    {
      "epoch": 0.08585365853658537,
      "step": 1584,
      "training_loss": 6.535168170928955
    },
    {
      "epoch": 0.08590785907859079,
      "step": 1585,
      "training_loss": 7.677383899688721
    },
    {
      "epoch": 0.0859620596205962,
      "step": 1586,
      "training_loss": 6.562088966369629
    },
    {
      "epoch": 0.08601626016260162,
      "step": 1587,
      "training_loss": 6.779745578765869
    },
    {
      "epoch": 0.08607046070460704,
      "grad_norm": 21.74555015563965,
      "learning_rate": 1e-05,
      "loss": 6.8886,
      "step": 1588
    },
    {
      "epoch": 0.08607046070460704,
      "step": 1588,
      "training_loss": 8.241918563842773
    },
    {
      "epoch": 0.08612466124661247,
      "step": 1589,
      "training_loss": 6.6531901359558105
    },
    {
      "epoch": 0.08617886178861789,
      "step": 1590,
      "training_loss": 7.717720985412598
    },
    {
      "epoch": 0.0862330623306233,
      "step": 1591,
      "training_loss": 6.020966053009033
    },
    {
      "epoch": 0.08628726287262872,
      "grad_norm": 16.044780731201172,
      "learning_rate": 1e-05,
      "loss": 7.1584,
      "step": 1592
    },
    {
      "epoch": 0.08628726287262872,
      "step": 1592,
      "training_loss": 9.817872047424316
    },
    {
      "epoch": 0.08634146341463414,
      "step": 1593,
      "training_loss": 7.6298723220825195
    },
    {
      "epoch": 0.08639566395663957,
      "step": 1594,
      "training_loss": 6.75858211517334
    },
    {
      "epoch": 0.08644986449864499,
      "step": 1595,
      "training_loss": 9.481396675109863
    },
    {
      "epoch": 0.0865040650406504,
      "grad_norm": 33.85790252685547,
      "learning_rate": 1e-05,
      "loss": 8.4219,
      "step": 1596
    },
    {
      "epoch": 0.0865040650406504,
      "step": 1596,
      "training_loss": 7.313276767730713
    },
    {
      "epoch": 0.08655826558265582,
      "step": 1597,
      "training_loss": 7.281013011932373
    },
    {
      "epoch": 0.08661246612466125,
      "step": 1598,
      "training_loss": 7.151163101196289
    },
    {
      "epoch": 0.08666666666666667,
      "step": 1599,
      "training_loss": 6.7644219398498535
    },
    {
      "epoch": 0.08672086720867209,
      "grad_norm": 17.21185302734375,
      "learning_rate": 1e-05,
      "loss": 7.1275,
      "step": 1600
    },
    {
      "epoch": 0.08672086720867209,
      "step": 1600,
      "training_loss": 7.123671531677246
    },
    {
      "epoch": 0.0867750677506775,
      "step": 1601,
      "training_loss": 6.964865207672119
    },
    {
      "epoch": 0.08682926829268292,
      "step": 1602,
      "training_loss": 5.422028064727783
    },
    {
      "epoch": 0.08688346883468835,
      "step": 1603,
      "training_loss": 5.575869560241699
    },
    {
      "epoch": 0.08693766937669377,
      "grad_norm": 36.85334396362305,
      "learning_rate": 1e-05,
      "loss": 6.2716,
      "step": 1604
    },
    {
      "epoch": 0.08693766937669377,
      "step": 1604,
      "training_loss": 7.80880880355835
    },
    {
      "epoch": 0.08699186991869919,
      "step": 1605,
      "training_loss": 7.339385509490967
    },
    {
      "epoch": 0.0870460704607046,
      "step": 1606,
      "training_loss": 7.033257007598877
    },
    {
      "epoch": 0.08710027100271003,
      "step": 1607,
      "training_loss": 7.587625503540039
    },
    {
      "epoch": 0.08715447154471545,
      "grad_norm": 26.551273345947266,
      "learning_rate": 1e-05,
      "loss": 7.4423,
      "step": 1608
    },
    {
      "epoch": 0.08715447154471545,
      "step": 1608,
      "training_loss": 6.939778804779053
    },
    {
      "epoch": 0.08720867208672087,
      "step": 1609,
      "training_loss": 5.784375190734863
    },
    {
      "epoch": 0.08726287262872628,
      "step": 1610,
      "training_loss": 6.988247394561768
    },
    {
      "epoch": 0.0873170731707317,
      "step": 1611,
      "training_loss": 9.202912330627441
    },
    {
      "epoch": 0.08737127371273713,
      "grad_norm": 47.047611236572266,
      "learning_rate": 1e-05,
      "loss": 7.2288,
      "step": 1612
    },
    {
      "epoch": 0.08737127371273713,
      "step": 1612,
      "training_loss": 6.002492904663086
    },
    {
      "epoch": 0.08742547425474255,
      "step": 1613,
      "training_loss": 6.229701519012451
    },
    {
      "epoch": 0.08747967479674797,
      "step": 1614,
      "training_loss": 7.681308269500732
    },
    {
      "epoch": 0.08753387533875338,
      "step": 1615,
      "training_loss": 6.27416467666626
    },
    {
      "epoch": 0.08758807588075881,
      "grad_norm": 22.022615432739258,
      "learning_rate": 1e-05,
      "loss": 6.5469,
      "step": 1616
    },
    {
      "epoch": 0.08758807588075881,
      "step": 1616,
      "training_loss": 5.585240840911865
    },
    {
      "epoch": 0.08764227642276423,
      "step": 1617,
      "training_loss": 7.478819370269775
    },
    {
      "epoch": 0.08769647696476965,
      "step": 1618,
      "training_loss": 7.32618522644043
    },
    {
      "epoch": 0.08775067750677507,
      "step": 1619,
      "training_loss": 6.169351577758789
    },
    {
      "epoch": 0.08780487804878048,
      "grad_norm": 14.736547470092773,
      "learning_rate": 1e-05,
      "loss": 6.6399,
      "step": 1620
    },
    {
      "epoch": 0.08780487804878048,
      "step": 1620,
      "training_loss": 7.288869380950928
    },
    {
      "epoch": 0.08785907859078591,
      "step": 1621,
      "training_loss": 4.688216209411621
    },
    {
      "epoch": 0.08791327913279133,
      "step": 1622,
      "training_loss": 5.676927089691162
    },
    {
      "epoch": 0.08796747967479675,
      "step": 1623,
      "training_loss": 6.484979629516602
    },
    {
      "epoch": 0.08802168021680216,
      "grad_norm": 15.032989501953125,
      "learning_rate": 1e-05,
      "loss": 6.0347,
      "step": 1624
    },
    {
      "epoch": 0.08802168021680216,
      "step": 1624,
      "training_loss": 6.748158931732178
    },
    {
      "epoch": 0.08807588075880758,
      "step": 1625,
      "training_loss": 8.676514625549316
    },
    {
      "epoch": 0.08813008130081301,
      "step": 1626,
      "training_loss": 6.736316680908203
    },
    {
      "epoch": 0.08818428184281843,
      "step": 1627,
      "training_loss": 8.372291564941406
    },
    {
      "epoch": 0.08823848238482385,
      "grad_norm": 22.17508888244629,
      "learning_rate": 1e-05,
      "loss": 7.6333,
      "step": 1628
    },
    {
      "epoch": 0.08823848238482385,
      "step": 1628,
      "training_loss": 7.854903697967529
    },
    {
      "epoch": 0.08829268292682926,
      "step": 1629,
      "training_loss": 8.114118576049805
    },
    {
      "epoch": 0.0883468834688347,
      "step": 1630,
      "training_loss": 7.274725914001465
    },
    {
      "epoch": 0.08840108401084011,
      "step": 1631,
      "training_loss": 7.442898273468018
    },
    {
      "epoch": 0.08845528455284553,
      "grad_norm": 20.545764923095703,
      "learning_rate": 1e-05,
      "loss": 7.6717,
      "step": 1632
    },
    {
      "epoch": 0.08845528455284553,
      "step": 1632,
      "training_loss": 5.580777168273926
    },
    {
      "epoch": 0.08850948509485095,
      "step": 1633,
      "training_loss": 8.175048828125
    },
    {
      "epoch": 0.08856368563685636,
      "step": 1634,
      "training_loss": 6.947646617889404
    },
    {
      "epoch": 0.0886178861788618,
      "step": 1635,
      "training_loss": 6.21968936920166
    },
    {
      "epoch": 0.08867208672086721,
      "grad_norm": 21.360780715942383,
      "learning_rate": 1e-05,
      "loss": 6.7308,
      "step": 1636
    },
    {
      "epoch": 0.08867208672086721,
      "step": 1636,
      "training_loss": 7.093921184539795
    },
    {
      "epoch": 0.08872628726287263,
      "step": 1637,
      "training_loss": 7.506170749664307
    },
    {
      "epoch": 0.08878048780487804,
      "step": 1638,
      "training_loss": 7.138119220733643
    },
    {
      "epoch": 0.08883468834688348,
      "step": 1639,
      "training_loss": 8.075820922851562
    },
    {
      "epoch": 0.08888888888888889,
      "grad_norm": 15.183390617370605,
      "learning_rate": 1e-05,
      "loss": 7.4535,
      "step": 1640
    },
    {
      "epoch": 0.08888888888888889,
      "step": 1640,
      "training_loss": 6.718434810638428
    },
    {
      "epoch": 0.08894308943089431,
      "step": 1641,
      "training_loss": 6.749777317047119
    },
    {
      "epoch": 0.08899728997289973,
      "step": 1642,
      "training_loss": 7.979598522186279
    },
    {
      "epoch": 0.08905149051490514,
      "step": 1643,
      "training_loss": 6.925064563751221
    },
    {
      "epoch": 0.08910569105691057,
      "grad_norm": 39.536964416503906,
      "learning_rate": 1e-05,
      "loss": 7.0932,
      "step": 1644
    },
    {
      "epoch": 0.08910569105691057,
      "step": 1644,
      "training_loss": 7.009535789489746
    },
    {
      "epoch": 0.08915989159891599,
      "step": 1645,
      "training_loss": 6.237033367156982
    },
    {
      "epoch": 0.08921409214092141,
      "step": 1646,
      "training_loss": 5.891733169555664
    },
    {
      "epoch": 0.08926829268292683,
      "step": 1647,
      "training_loss": 6.250555038452148
    },
    {
      "epoch": 0.08932249322493226,
      "grad_norm": 29.263757705688477,
      "learning_rate": 1e-05,
      "loss": 6.3472,
      "step": 1648
    },
    {
      "epoch": 0.08932249322493226,
      "step": 1648,
      "training_loss": 6.953632354736328
    },
    {
      "epoch": 0.08937669376693767,
      "step": 1649,
      "training_loss": 8.989429473876953
    },
    {
      "epoch": 0.08943089430894309,
      "step": 1650,
      "training_loss": 4.6601457595825195
    },
    {
      "epoch": 0.08948509485094851,
      "step": 1651,
      "training_loss": 7.617650032043457
    },
    {
      "epoch": 0.08953929539295392,
      "grad_norm": 20.728431701660156,
      "learning_rate": 1e-05,
      "loss": 7.0552,
      "step": 1652
    },
    {
      "epoch": 0.08953929539295392,
      "step": 1652,
      "training_loss": 6.447307586669922
    },
    {
      "epoch": 0.08959349593495936,
      "step": 1653,
      "training_loss": 7.548569202423096
    },
    {
      "epoch": 0.08964769647696477,
      "step": 1654,
      "training_loss": 7.182150840759277
    },
    {
      "epoch": 0.08970189701897019,
      "step": 1655,
      "training_loss": 8.147107124328613
    },
    {
      "epoch": 0.0897560975609756,
      "grad_norm": 17.337038040161133,
      "learning_rate": 1e-05,
      "loss": 7.3313,
      "step": 1656
    },
    {
      "epoch": 0.0897560975609756,
      "step": 1656,
      "training_loss": 7.160848140716553
    },
    {
      "epoch": 0.08981029810298102,
      "step": 1657,
      "training_loss": 7.219114780426025
    },
    {
      "epoch": 0.08986449864498645,
      "step": 1658,
      "training_loss": 7.852097034454346
    },
    {
      "epoch": 0.08991869918699187,
      "step": 1659,
      "training_loss": 7.2736616134643555
    },
    {
      "epoch": 0.08997289972899729,
      "grad_norm": 15.549280166625977,
      "learning_rate": 1e-05,
      "loss": 7.3764,
      "step": 1660
    },
    {
      "epoch": 0.08997289972899729,
      "step": 1660,
      "training_loss": 8.21684741973877
    },
    {
      "epoch": 0.0900271002710027,
      "step": 1661,
      "training_loss": 6.508275508880615
    },
    {
      "epoch": 0.09008130081300814,
      "step": 1662,
      "training_loss": 6.70107364654541
    },
    {
      "epoch": 0.09013550135501355,
      "step": 1663,
      "training_loss": 7.71552848815918
    },
    {
      "epoch": 0.09018970189701897,
      "grad_norm": 16.131174087524414,
      "learning_rate": 1e-05,
      "loss": 7.2854,
      "step": 1664
    },
    {
      "epoch": 0.09018970189701897,
      "step": 1664,
      "training_loss": 7.004007816314697
    },
    {
      "epoch": 0.09024390243902439,
      "step": 1665,
      "training_loss": 6.639325141906738
    },
    {
      "epoch": 0.0902981029810298,
      "step": 1666,
      "training_loss": 6.506496906280518
    },
    {
      "epoch": 0.09035230352303524,
      "step": 1667,
      "training_loss": 6.646972179412842
    },
    {
      "epoch": 0.09040650406504065,
      "grad_norm": 21.314233779907227,
      "learning_rate": 1e-05,
      "loss": 6.6992,
      "step": 1668
    },
    {
      "epoch": 0.09040650406504065,
      "step": 1668,
      "training_loss": 6.48427677154541
    },
    {
      "epoch": 0.09046070460704607,
      "step": 1669,
      "training_loss": 6.787038326263428
    },
    {
      "epoch": 0.09051490514905149,
      "step": 1670,
      "training_loss": 6.616793632507324
    },
    {
      "epoch": 0.09056910569105692,
      "step": 1671,
      "training_loss": 6.998480796813965
    },
    {
      "epoch": 0.09062330623306233,
      "grad_norm": 24.879770278930664,
      "learning_rate": 1e-05,
      "loss": 6.7216,
      "step": 1672
    },
    {
      "epoch": 0.09062330623306233,
      "step": 1672,
      "training_loss": 6.941429615020752
    },
    {
      "epoch": 0.09067750677506775,
      "step": 1673,
      "training_loss": 5.861907958984375
    },
    {
      "epoch": 0.09073170731707317,
      "step": 1674,
      "training_loss": 6.384670257568359
    },
    {
      "epoch": 0.09078590785907859,
      "step": 1675,
      "training_loss": 6.893835544586182
    },
    {
      "epoch": 0.09084010840108402,
      "grad_norm": 16.238502502441406,
      "learning_rate": 1e-05,
      "loss": 6.5205,
      "step": 1676
    },
    {
      "epoch": 0.09084010840108402,
      "step": 1676,
      "training_loss": 8.070765495300293
    },
    {
      "epoch": 0.09089430894308943,
      "step": 1677,
      "training_loss": 6.743760108947754
    },
    {
      "epoch": 0.09094850948509485,
      "step": 1678,
      "training_loss": 6.405947208404541
    },
    {
      "epoch": 0.09100271002710027,
      "step": 1679,
      "training_loss": 8.092854499816895
    },
    {
      "epoch": 0.0910569105691057,
      "grad_norm": 17.252174377441406,
      "learning_rate": 1e-05,
      "loss": 7.3283,
      "step": 1680
    },
    {
      "epoch": 0.0910569105691057,
      "step": 1680,
      "training_loss": 7.3212080001831055
    },
    {
      "epoch": 0.09111111111111111,
      "step": 1681,
      "training_loss": 8.87846565246582
    },
    {
      "epoch": 0.09116531165311653,
      "step": 1682,
      "training_loss": 7.089105129241943
    },
    {
      "epoch": 0.09121951219512195,
      "step": 1683,
      "training_loss": 6.897226810455322
    },
    {
      "epoch": 0.09127371273712737,
      "grad_norm": 22.430179595947266,
      "learning_rate": 1e-05,
      "loss": 7.5465,
      "step": 1684
    },
    {
      "epoch": 0.09127371273712737,
      "step": 1684,
      "training_loss": 6.415831089019775
    },
    {
      "epoch": 0.0913279132791328,
      "step": 1685,
      "training_loss": 6.781657695770264
    },
    {
      "epoch": 0.09138211382113821,
      "step": 1686,
      "training_loss": 5.891106128692627
    },
    {
      "epoch": 0.09143631436314363,
      "step": 1687,
      "training_loss": 5.012064456939697
    },
    {
      "epoch": 0.09149051490514905,
      "grad_norm": 19.26152801513672,
      "learning_rate": 1e-05,
      "loss": 6.0252,
      "step": 1688
    },
    {
      "epoch": 0.09149051490514905,
      "step": 1688,
      "training_loss": 8.217626571655273
    },
    {
      "epoch": 0.09154471544715446,
      "step": 1689,
      "training_loss": 7.038978099822998
    },
    {
      "epoch": 0.0915989159891599,
      "step": 1690,
      "training_loss": 6.29246711730957
    },
    {
      "epoch": 0.09165311653116531,
      "step": 1691,
      "training_loss": 7.3843865394592285
    },
    {
      "epoch": 0.09170731707317073,
      "grad_norm": 15.88426399230957,
      "learning_rate": 1e-05,
      "loss": 7.2334,
      "step": 1692
    },
    {
      "epoch": 0.09170731707317073,
      "step": 1692,
      "training_loss": 7.540465354919434
    },
    {
      "epoch": 0.09176151761517615,
      "step": 1693,
      "training_loss": 7.558072090148926
    },
    {
      "epoch": 0.09181571815718158,
      "step": 1694,
      "training_loss": 7.320772647857666
    },
    {
      "epoch": 0.091869918699187,
      "step": 1695,
      "training_loss": 7.443187236785889
    },
    {
      "epoch": 0.09192411924119241,
      "grad_norm": 17.922340393066406,
      "learning_rate": 1e-05,
      "loss": 7.4656,
      "step": 1696
    },
    {
      "epoch": 0.09192411924119241,
      "step": 1696,
      "training_loss": 7.122110366821289
    },
    {
      "epoch": 0.09197831978319783,
      "step": 1697,
      "training_loss": 5.091551303863525
    },
    {
      "epoch": 0.09203252032520325,
      "step": 1698,
      "training_loss": 6.835752487182617
    },
    {
      "epoch": 0.09208672086720868,
      "step": 1699,
      "training_loss": 6.449139595031738
    },
    {
      "epoch": 0.0921409214092141,
      "grad_norm": 15.705408096313477,
      "learning_rate": 1e-05,
      "loss": 6.3746,
      "step": 1700
    },
    {
      "epoch": 0.0921409214092141,
      "step": 1700,
      "training_loss": 9.28454875946045
    },
    {
      "epoch": 0.09219512195121951,
      "step": 1701,
      "training_loss": 6.343812465667725
    },
    {
      "epoch": 0.09224932249322493,
      "step": 1702,
      "training_loss": 4.7976813316345215
    },
    {
      "epoch": 0.09230352303523036,
      "step": 1703,
      "training_loss": 7.948275566101074
    },
    {
      "epoch": 0.09235772357723578,
      "grad_norm": 23.639522552490234,
      "learning_rate": 1e-05,
      "loss": 7.0936,
      "step": 1704
    },
    {
      "epoch": 0.09235772357723578,
      "step": 1704,
      "training_loss": 7.589580059051514
    },
    {
      "epoch": 0.09241192411924119,
      "step": 1705,
      "training_loss": 5.874301910400391
    },
    {
      "epoch": 0.09246612466124661,
      "step": 1706,
      "training_loss": 6.815415859222412
    },
    {
      "epoch": 0.09252032520325203,
      "step": 1707,
      "training_loss": 7.598508834838867
    },
    {
      "epoch": 0.09257452574525746,
      "grad_norm": 16.76819610595703,
      "learning_rate": 1e-05,
      "loss": 6.9695,
      "step": 1708
    },
    {
      "epoch": 0.09257452574525746,
      "step": 1708,
      "training_loss": 6.924911975860596
    },
    {
      "epoch": 0.09262872628726287,
      "step": 1709,
      "training_loss": 7.427004337310791
    },
    {
      "epoch": 0.09268292682926829,
      "step": 1710,
      "training_loss": 7.596051216125488
    },
    {
      "epoch": 0.09273712737127371,
      "step": 1711,
      "training_loss": 7.249311447143555
    },
    {
      "epoch": 0.09279132791327914,
      "grad_norm": 36.00410461425781,
      "learning_rate": 1e-05,
      "loss": 7.2993,
      "step": 1712
    },
    {
      "epoch": 0.09279132791327914,
      "step": 1712,
      "training_loss": 7.356167316436768
    },
    {
      "epoch": 0.09284552845528456,
      "step": 1713,
      "training_loss": 5.279910087585449
    },
    {
      "epoch": 0.09289972899728997,
      "step": 1714,
      "training_loss": 7.40649938583374
    },
    {
      "epoch": 0.09295392953929539,
      "step": 1715,
      "training_loss": 7.539501667022705
    },
    {
      "epoch": 0.09300813008130081,
      "grad_norm": 22.652135848999023,
      "learning_rate": 1e-05,
      "loss": 6.8955,
      "step": 1716
    },
    {
      "epoch": 0.09300813008130081,
      "step": 1716,
      "training_loss": 8.626862525939941
    },
    {
      "epoch": 0.09306233062330624,
      "step": 1717,
      "training_loss": 7.179759979248047
    },
    {
      "epoch": 0.09311653116531166,
      "step": 1718,
      "training_loss": 7.615808010101318
    },
    {
      "epoch": 0.09317073170731707,
      "step": 1719,
      "training_loss": 11.11100959777832
    },
    {
      "epoch": 0.09322493224932249,
      "grad_norm": 52.94569778442383,
      "learning_rate": 1e-05,
      "loss": 8.6334,
      "step": 1720
    },
    {
      "epoch": 0.09322493224932249,
      "step": 1720,
      "training_loss": 7.584996223449707
    },
    {
      "epoch": 0.0932791327913279,
      "step": 1721,
      "training_loss": 7.91416597366333
    },
    {
      "epoch": 0.09333333333333334,
      "step": 1722,
      "training_loss": 6.997749328613281
    },
    {
      "epoch": 0.09338753387533875,
      "step": 1723,
      "training_loss": 7.072558879852295
    },
    {
      "epoch": 0.09344173441734417,
      "grad_norm": 23.074668884277344,
      "learning_rate": 1e-05,
      "loss": 7.3924,
      "step": 1724
    },
    {
      "epoch": 0.09344173441734417,
      "step": 1724,
      "training_loss": 7.791093826293945
    },
    {
      "epoch": 0.09349593495934959,
      "step": 1725,
      "training_loss": 9.33739948272705
    },
    {
      "epoch": 0.09355013550135502,
      "step": 1726,
      "training_loss": 8.524396896362305
    },
    {
      "epoch": 0.09360433604336044,
      "step": 1727,
      "training_loss": 7.42374849319458
    },
    {
      "epoch": 0.09365853658536585,
      "grad_norm": 13.249479293823242,
      "learning_rate": 1e-05,
      "loss": 8.2692,
      "step": 1728
    },
    {
      "epoch": 0.09365853658536585,
      "step": 1728,
      "training_loss": 9.12932300567627
    },
    {
      "epoch": 0.09371273712737127,
      "step": 1729,
      "training_loss": 7.320218086242676
    },
    {
      "epoch": 0.09376693766937669,
      "step": 1730,
      "training_loss": 6.8992018699646
    },
    {
      "epoch": 0.09382113821138212,
      "step": 1731,
      "training_loss": 7.438861846923828
    },
    {
      "epoch": 0.09387533875338754,
      "grad_norm": 24.36119270324707,
      "learning_rate": 1e-05,
      "loss": 7.6969,
      "step": 1732
    },
    {
      "epoch": 0.09387533875338754,
      "step": 1732,
      "training_loss": 6.066250801086426
    },
    {
      "epoch": 0.09392953929539295,
      "step": 1733,
      "training_loss": 8.088546752929688
    },
    {
      "epoch": 0.09398373983739837,
      "step": 1734,
      "training_loss": 7.038322448730469
    },
    {
      "epoch": 0.0940379403794038,
      "step": 1735,
      "training_loss": 7.741424560546875
    },
    {
      "epoch": 0.09409214092140922,
      "grad_norm": 17.9685001373291,
      "learning_rate": 1e-05,
      "loss": 7.2336,
      "step": 1736
    },
    {
      "epoch": 0.09409214092140922,
      "step": 1736,
      "training_loss": 6.628321647644043
    },
    {
      "epoch": 0.09414634146341463,
      "step": 1737,
      "training_loss": 7.795683860778809
    },
    {
      "epoch": 0.09420054200542005,
      "step": 1738,
      "training_loss": 6.239549160003662
    },
    {
      "epoch": 0.09425474254742547,
      "step": 1739,
      "training_loss": 5.0602288246154785
    },
    {
      "epoch": 0.0943089430894309,
      "grad_norm": 17.41661262512207,
      "learning_rate": 1e-05,
      "loss": 6.4309,
      "step": 1740
    },
    {
      "epoch": 0.0943089430894309,
      "step": 1740,
      "training_loss": 8.33154582977295
    },
    {
      "epoch": 0.09436314363143632,
      "step": 1741,
      "training_loss": 7.251248359680176
    },
    {
      "epoch": 0.09441734417344173,
      "step": 1742,
      "training_loss": 7.42059850692749
    },
    {
      "epoch": 0.09447154471544715,
      "step": 1743,
      "training_loss": 6.009142875671387
    },
    {
      "epoch": 0.09452574525745258,
      "grad_norm": 24.224031448364258,
      "learning_rate": 1e-05,
      "loss": 7.2531,
      "step": 1744
    },
    {
      "epoch": 0.09452574525745258,
      "step": 1744,
      "training_loss": 8.310248374938965
    },
    {
      "epoch": 0.094579945799458,
      "step": 1745,
      "training_loss": 7.091411590576172
    },
    {
      "epoch": 0.09463414634146342,
      "step": 1746,
      "training_loss": 5.931295871734619
    },
    {
      "epoch": 0.09468834688346883,
      "step": 1747,
      "training_loss": 6.3198466300964355
    },
    {
      "epoch": 0.09474254742547425,
      "grad_norm": 12.753416061401367,
      "learning_rate": 1e-05,
      "loss": 6.9132,
      "step": 1748
    },
    {
      "epoch": 0.09474254742547425,
      "step": 1748,
      "training_loss": 7.669896125793457
    },
    {
      "epoch": 0.09479674796747968,
      "step": 1749,
      "training_loss": 7.236912250518799
    },
    {
      "epoch": 0.0948509485094851,
      "step": 1750,
      "training_loss": 4.905777454376221
    },
    {
      "epoch": 0.09490514905149051,
      "step": 1751,
      "training_loss": 6.838205814361572
    },
    {
      "epoch": 0.09495934959349593,
      "grad_norm": 17.959123611450195,
      "learning_rate": 1e-05,
      "loss": 6.6627,
      "step": 1752
    },
    {
      "epoch": 0.09495934959349593,
      "step": 1752,
      "training_loss": 6.276697635650635
    },
    {
      "epoch": 0.09501355013550135,
      "step": 1753,
      "training_loss": 6.445558071136475
    },
    {
      "epoch": 0.09506775067750678,
      "step": 1754,
      "training_loss": 7.154428005218506
    },
    {
      "epoch": 0.0951219512195122,
      "step": 1755,
      "training_loss": 5.825795650482178
    },
    {
      "epoch": 0.09517615176151761,
      "grad_norm": 21.541080474853516,
      "learning_rate": 1e-05,
      "loss": 6.4256,
      "step": 1756
    },
    {
      "epoch": 0.09517615176151761,
      "step": 1756,
      "training_loss": 7.253842353820801
    },
    {
      "epoch": 0.09523035230352303,
      "step": 1757,
      "training_loss": 7.106935024261475
    },
    {
      "epoch": 0.09528455284552846,
      "step": 1758,
      "training_loss": 5.761862277984619
    },
    {
      "epoch": 0.09533875338753388,
      "step": 1759,
      "training_loss": 6.92268180847168
    },
    {
      "epoch": 0.0953929539295393,
      "grad_norm": 16.41026496887207,
      "learning_rate": 1e-05,
      "loss": 6.7613,
      "step": 1760
    },
    {
      "epoch": 0.0953929539295393,
      "step": 1760,
      "training_loss": 6.975717067718506
    },
    {
      "epoch": 0.09544715447154471,
      "step": 1761,
      "training_loss": 6.2652740478515625
    },
    {
      "epoch": 0.09550135501355013,
      "step": 1762,
      "training_loss": 7.610599994659424
    },
    {
      "epoch": 0.09555555555555556,
      "step": 1763,
      "training_loss": 7.090829372406006
    },
    {
      "epoch": 0.09560975609756098,
      "grad_norm": 16.02631378173828,
      "learning_rate": 1e-05,
      "loss": 6.9856,
      "step": 1764
    },
    {
      "epoch": 0.09560975609756098,
      "step": 1764,
      "training_loss": 7.206759929656982
    },
    {
      "epoch": 0.0956639566395664,
      "step": 1765,
      "training_loss": 7.0517048835754395
    },
    {
      "epoch": 0.09571815718157181,
      "step": 1766,
      "training_loss": 7.74597692489624
    },
    {
      "epoch": 0.09577235772357724,
      "step": 1767,
      "training_loss": 6.3255510330200195
    },
    {
      "epoch": 0.09582655826558266,
      "grad_norm": 23.97528648376465,
      "learning_rate": 1e-05,
      "loss": 7.0825,
      "step": 1768
    },
    {
      "epoch": 0.09582655826558266,
      "step": 1768,
      "training_loss": 8.020119667053223
    },
    {
      "epoch": 0.09588075880758808,
      "step": 1769,
      "training_loss": 7.313936710357666
    },
    {
      "epoch": 0.0959349593495935,
      "step": 1770,
      "training_loss": 6.772221088409424
    },
    {
      "epoch": 0.09598915989159891,
      "step": 1771,
      "training_loss": 7.0258026123046875
    },
    {
      "epoch": 0.09604336043360434,
      "grad_norm": 17.637819290161133,
      "learning_rate": 1e-05,
      "loss": 7.283,
      "step": 1772
    },
    {
      "epoch": 0.09604336043360434,
      "step": 1772,
      "training_loss": 7.6663994789123535
    },
    {
      "epoch": 0.09609756097560976,
      "step": 1773,
      "training_loss": 7.6933112144470215
    },
    {
      "epoch": 0.09615176151761518,
      "step": 1774,
      "training_loss": 6.320790767669678
    },
    {
      "epoch": 0.09620596205962059,
      "step": 1775,
      "training_loss": 8.899639129638672
    },
    {
      "epoch": 0.09626016260162602,
      "grad_norm": 20.46413230895996,
      "learning_rate": 1e-05,
      "loss": 7.645,
      "step": 1776
    },
    {
      "epoch": 0.09626016260162602,
      "step": 1776,
      "training_loss": 7.159790515899658
    },
    {
      "epoch": 0.09631436314363144,
      "step": 1777,
      "training_loss": 7.281558990478516
    },
    {
      "epoch": 0.09636856368563686,
      "step": 1778,
      "training_loss": 6.548126697540283
    },
    {
      "epoch": 0.09642276422764227,
      "step": 1779,
      "training_loss": 7.986053466796875
    },
    {
      "epoch": 0.09647696476964769,
      "grad_norm": 24.946428298950195,
      "learning_rate": 1e-05,
      "loss": 7.2439,
      "step": 1780
    },
    {
      "epoch": 0.09647696476964769,
      "step": 1780,
      "training_loss": 7.789584636688232
    },
    {
      "epoch": 0.09653116531165312,
      "step": 1781,
      "training_loss": 7.309199810028076
    },
    {
      "epoch": 0.09658536585365854,
      "step": 1782,
      "training_loss": 7.847604274749756
    },
    {
      "epoch": 0.09663956639566396,
      "step": 1783,
      "training_loss": 5.337222576141357
    },
    {
      "epoch": 0.09669376693766937,
      "grad_norm": 17.567522048950195,
      "learning_rate": 1e-05,
      "loss": 7.0709,
      "step": 1784
    },
    {
      "epoch": 0.09669376693766937,
      "step": 1784,
      "training_loss": 7.6073126792907715
    },
    {
      "epoch": 0.09674796747967479,
      "step": 1785,
      "training_loss": 7.344090938568115
    },
    {
      "epoch": 0.09680216802168022,
      "step": 1786,
      "training_loss": 7.879390716552734
    },
    {
      "epoch": 0.09685636856368564,
      "step": 1787,
      "training_loss": 6.820295810699463
    },
    {
      "epoch": 0.09691056910569106,
      "grad_norm": 11.551276206970215,
      "learning_rate": 1e-05,
      "loss": 7.4128,
      "step": 1788
    },
    {
      "epoch": 0.09691056910569106,
      "step": 1788,
      "training_loss": 7.3769731521606445
    },
    {
      "epoch": 0.09696476964769647,
      "step": 1789,
      "training_loss": 6.810166358947754
    },
    {
      "epoch": 0.0970189701897019,
      "step": 1790,
      "training_loss": 6.943021297454834
    },
    {
      "epoch": 0.09707317073170732,
      "step": 1791,
      "training_loss": 6.193000316619873
    },
    {
      "epoch": 0.09712737127371274,
      "grad_norm": 13.75859546661377,
      "learning_rate": 1e-05,
      "loss": 6.8308,
      "step": 1792
    },
    {
      "epoch": 0.09712737127371274,
      "step": 1792,
      "training_loss": 7.501184463500977
    },
    {
      "epoch": 0.09718157181571815,
      "step": 1793,
      "training_loss": 6.744534015655518
    },
    {
      "epoch": 0.09723577235772357,
      "step": 1794,
      "training_loss": 8.088337898254395
    },
    {
      "epoch": 0.097289972899729,
      "step": 1795,
      "training_loss": 4.722229957580566
    },
    {
      "epoch": 0.09734417344173442,
      "grad_norm": 16.40852928161621,
      "learning_rate": 1e-05,
      "loss": 6.7641,
      "step": 1796
    },
    {
      "epoch": 0.09734417344173442,
      "step": 1796,
      "training_loss": 5.999045372009277
    },
    {
      "epoch": 0.09739837398373984,
      "step": 1797,
      "training_loss": 7.9346771240234375
    },
    {
      "epoch": 0.09745257452574525,
      "step": 1798,
      "training_loss": 4.874082565307617
    },
    {
      "epoch": 0.09750677506775068,
      "step": 1799,
      "training_loss": 7.72590970993042
    },
    {
      "epoch": 0.0975609756097561,
      "grad_norm": 17.744157791137695,
      "learning_rate": 1e-05,
      "loss": 6.6334,
      "step": 1800
    },
    {
      "epoch": 0.0975609756097561,
      "step": 1800,
      "training_loss": 6.864748954772949
    },
    {
      "epoch": 0.09761517615176152,
      "step": 1801,
      "training_loss": 7.550734996795654
    },
    {
      "epoch": 0.09766937669376693,
      "step": 1802,
      "training_loss": 6.66982889175415
    },
    {
      "epoch": 0.09772357723577235,
      "step": 1803,
      "training_loss": 8.060007095336914
    },
    {
      "epoch": 0.09777777777777778,
      "grad_norm": 28.389541625976562,
      "learning_rate": 1e-05,
      "loss": 7.2863,
      "step": 1804
    },
    {
      "epoch": 0.09777777777777778,
      "step": 1804,
      "training_loss": 7.176174163818359
    },
    {
      "epoch": 0.0978319783197832,
      "step": 1805,
      "training_loss": 7.384130001068115
    },
    {
      "epoch": 0.09788617886178862,
      "step": 1806,
      "training_loss": 7.144604682922363
    },
    {
      "epoch": 0.09794037940379403,
      "step": 1807,
      "training_loss": 6.732770919799805
    },
    {
      "epoch": 0.09799457994579946,
      "grad_norm": 20.322484970092773,
      "learning_rate": 1e-05,
      "loss": 7.1094,
      "step": 1808
    },
    {
      "epoch": 0.09799457994579946,
      "step": 1808,
      "training_loss": 7.012442111968994
    },
    {
      "epoch": 0.09804878048780488,
      "step": 1809,
      "training_loss": 7.252750873565674
    },
    {
      "epoch": 0.0981029810298103,
      "step": 1810,
      "training_loss": 7.621109485626221
    },
    {
      "epoch": 0.09815718157181572,
      "step": 1811,
      "training_loss": 7.669775009155273
    },
    {
      "epoch": 0.09821138211382113,
      "grad_norm": 20.952239990234375,
      "learning_rate": 1e-05,
      "loss": 7.389,
      "step": 1812
    },
    {
      "epoch": 0.09821138211382113,
      "step": 1812,
      "training_loss": 6.4778947830200195
    },
    {
      "epoch": 0.09826558265582656,
      "step": 1813,
      "training_loss": 7.008749008178711
    },
    {
      "epoch": 0.09831978319783198,
      "step": 1814,
      "training_loss": 4.594441890716553
    },
    {
      "epoch": 0.0983739837398374,
      "step": 1815,
      "training_loss": 6.74560546875
    },
    {
      "epoch": 0.09842818428184281,
      "grad_norm": 23.24747657775879,
      "learning_rate": 1e-05,
      "loss": 6.2067,
      "step": 1816
    },
    {
      "epoch": 0.09842818428184281,
      "step": 1816,
      "training_loss": 7.7325758934021
    },
    {
      "epoch": 0.09848238482384823,
      "step": 1817,
      "training_loss": 6.696701526641846
    },
    {
      "epoch": 0.09853658536585366,
      "step": 1818,
      "training_loss": 4.698680400848389
    },
    {
      "epoch": 0.09859078590785908,
      "step": 1819,
      "training_loss": 6.987692832946777
    },
    {
      "epoch": 0.0986449864498645,
      "grad_norm": 15.378469467163086,
      "learning_rate": 1e-05,
      "loss": 6.5289,
      "step": 1820
    },
    {
      "epoch": 0.0986449864498645,
      "step": 1820,
      "training_loss": 7.661124229431152
    },
    {
      "epoch": 0.09869918699186991,
      "step": 1821,
      "training_loss": 6.872621536254883
    },
    {
      "epoch": 0.09875338753387534,
      "step": 1822,
      "training_loss": 8.3343505859375
    },
    {
      "epoch": 0.09880758807588076,
      "step": 1823,
      "training_loss": 7.249980926513672
    },
    {
      "epoch": 0.09886178861788618,
      "grad_norm": 19.45137596130371,
      "learning_rate": 1e-05,
      "loss": 7.5295,
      "step": 1824
    },
    {
      "epoch": 0.09886178861788618,
      "step": 1824,
      "training_loss": 7.07426118850708
    },
    {
      "epoch": 0.0989159891598916,
      "step": 1825,
      "training_loss": 6.6371541023254395
    },
    {
      "epoch": 0.09897018970189701,
      "step": 1826,
      "training_loss": 6.09772253036499
    },
    {
      "epoch": 0.09902439024390244,
      "step": 1827,
      "training_loss": 7.62018346786499
    },
    {
      "epoch": 0.09907859078590786,
      "grad_norm": 19.541664123535156,
      "learning_rate": 1e-05,
      "loss": 6.8573,
      "step": 1828
    },
    {
      "epoch": 0.09907859078590786,
      "step": 1828,
      "training_loss": 7.037596225738525
    },
    {
      "epoch": 0.09913279132791328,
      "step": 1829,
      "training_loss": 5.411575794219971
    },
    {
      "epoch": 0.0991869918699187,
      "step": 1830,
      "training_loss": 5.918088436126709
    },
    {
      "epoch": 0.09924119241192413,
      "step": 1831,
      "training_loss": 6.0435404777526855
    },
    {
      "epoch": 0.09929539295392954,
      "grad_norm": 32.72734451293945,
      "learning_rate": 1e-05,
      "loss": 6.1027,
      "step": 1832
    },
    {
      "epoch": 0.09929539295392954,
      "step": 1832,
      "training_loss": 6.31008243560791
    },
    {
      "epoch": 0.09934959349593496,
      "step": 1833,
      "training_loss": 7.640936374664307
    },
    {
      "epoch": 0.09940379403794038,
      "step": 1834,
      "training_loss": 7.680691242218018
    },
    {
      "epoch": 0.0994579945799458,
      "step": 1835,
      "training_loss": 6.665595531463623
    },
    {
      "epoch": 0.09951219512195122,
      "grad_norm": 14.271285057067871,
      "learning_rate": 1e-05,
      "loss": 7.0743,
      "step": 1836
    },
    {
      "epoch": 0.09951219512195122,
      "step": 1836,
      "training_loss": 5.690708637237549
    },
    {
      "epoch": 0.09956639566395664,
      "step": 1837,
      "training_loss": 8.010293960571289
    },
    {
      "epoch": 0.09962059620596206,
      "step": 1838,
      "training_loss": 5.154156684875488
    },
    {
      "epoch": 0.09967479674796748,
      "step": 1839,
      "training_loss": 7.071045398712158
    },
    {
      "epoch": 0.0997289972899729,
      "grad_norm": 12.849090576171875,
      "learning_rate": 1e-05,
      "loss": 6.4816,
      "step": 1840
    },
    {
      "epoch": 0.0997289972899729,
      "step": 1840,
      "training_loss": 7.014519214630127
    },
    {
      "epoch": 0.09978319783197832,
      "step": 1841,
      "training_loss": 7.2970380783081055
    },
    {
      "epoch": 0.09983739837398374,
      "step": 1842,
      "training_loss": 6.379818916320801
    },
    {
      "epoch": 0.09989159891598916,
      "step": 1843,
      "training_loss": 5.890921592712402
    },
    {
      "epoch": 0.09994579945799457,
      "grad_norm": 22.724231719970703,
      "learning_rate": 1e-05,
      "loss": 6.6456,
      "step": 1844
    },
    {
      "epoch": 0.09994579945799457,
      "step": 1844,
      "training_loss": 6.573121070861816
    },
    {
      "epoch": 0.1,
      "step": 1845,
      "training_loss": 6.535424709320068
    },
    {
      "epoch": 0.10005420054200542,
      "step": 1846,
      "training_loss": 6.750287055969238
    },
    {
      "epoch": 0.10010840108401084,
      "step": 1847,
      "training_loss": 6.688355922698975
    },
    {
      "epoch": 0.10016260162601626,
      "grad_norm": 20.58989143371582,
      "learning_rate": 1e-05,
      "loss": 6.6368,
      "step": 1848
    },
    {
      "epoch": 0.10016260162601626,
      "step": 1848,
      "training_loss": 7.169182777404785
    },
    {
      "epoch": 0.10021680216802167,
      "step": 1849,
      "training_loss": 6.872289657592773
    },
    {
      "epoch": 0.1002710027100271,
      "step": 1850,
      "training_loss": 6.915380001068115
    },
    {
      "epoch": 0.10032520325203252,
      "step": 1851,
      "training_loss": 9.504040718078613
    },
    {
      "epoch": 0.10037940379403794,
      "grad_norm": 56.86107635498047,
      "learning_rate": 1e-05,
      "loss": 7.6152,
      "step": 1852
    },
    {
      "epoch": 0.10037940379403794,
      "step": 1852,
      "training_loss": 6.494965553283691
    },
    {
      "epoch": 0.10043360433604336,
      "step": 1853,
      "training_loss": 6.6870951652526855
    },
    {
      "epoch": 0.10048780487804879,
      "step": 1854,
      "training_loss": 8.392060279846191
    },
    {
      "epoch": 0.1005420054200542,
      "step": 1855,
      "training_loss": 7.3568434715271
    },
    {
      "epoch": 0.10059620596205962,
      "grad_norm": 49.58441925048828,
      "learning_rate": 1e-05,
      "loss": 7.2327,
      "step": 1856
    },
    {
      "epoch": 0.10059620596205962,
      "step": 1856,
      "training_loss": 6.857938289642334
    },
    {
      "epoch": 0.10065040650406504,
      "step": 1857,
      "training_loss": 7.114043235778809
    },
    {
      "epoch": 0.10070460704607045,
      "step": 1858,
      "training_loss": 7.980775833129883
    },
    {
      "epoch": 0.10075880758807589,
      "step": 1859,
      "training_loss": 7.852813720703125
    },
    {
      "epoch": 0.1008130081300813,
      "grad_norm": 53.052452087402344,
      "learning_rate": 1e-05,
      "loss": 7.4514,
      "step": 1860
    },
    {
      "epoch": 0.1008130081300813,
      "step": 1860,
      "training_loss": 7.443462371826172
    },
    {
      "epoch": 0.10086720867208672,
      "step": 1861,
      "training_loss": 6.19350004196167
    },
    {
      "epoch": 0.10092140921409214,
      "step": 1862,
      "training_loss": 7.3889031410217285
    },
    {
      "epoch": 0.10097560975609757,
      "step": 1863,
      "training_loss": 7.585278511047363
    },
    {
      "epoch": 0.10102981029810298,
      "grad_norm": 27.88114356994629,
      "learning_rate": 1e-05,
      "loss": 7.1528,
      "step": 1864
    },
    {
      "epoch": 0.10102981029810298,
      "step": 1864,
      "training_loss": 8.452298164367676
    },
    {
      "epoch": 0.1010840108401084,
      "step": 1865,
      "training_loss": 5.742522716522217
    },
    {
      "epoch": 0.10113821138211382,
      "step": 1866,
      "training_loss": 7.657788276672363
    },
    {
      "epoch": 0.10119241192411924,
      "step": 1867,
      "training_loss": 7.492915630340576
    },
    {
      "epoch": 0.10124661246612467,
      "grad_norm": 18.857528686523438,
      "learning_rate": 1e-05,
      "loss": 7.3364,
      "step": 1868
    },
    {
      "epoch": 0.10124661246612467,
      "step": 1868,
      "training_loss": 5.594577312469482
    },
    {
      "epoch": 0.10130081300813008,
      "step": 1869,
      "training_loss": 8.023194313049316
    },
    {
      "epoch": 0.1013550135501355,
      "step": 1870,
      "training_loss": 6.285765171051025
    },
    {
      "epoch": 0.10140921409214092,
      "step": 1871,
      "training_loss": 7.735846996307373
    },
    {
      "epoch": 0.10146341463414635,
      "grad_norm": 23.132484436035156,
      "learning_rate": 1e-05,
      "loss": 6.9098,
      "step": 1872
    },
    {
      "epoch": 0.10146341463414635,
      "step": 1872,
      "training_loss": 8.006044387817383
    },
    {
      "epoch": 0.10151761517615177,
      "step": 1873,
      "training_loss": 6.534153938293457
    },
    {
      "epoch": 0.10157181571815718,
      "step": 1874,
      "training_loss": 6.306844711303711
    },
    {
      "epoch": 0.1016260162601626,
      "step": 1875,
      "training_loss": 6.963500022888184
    },
    {
      "epoch": 0.10168021680216802,
      "grad_norm": 14.774385452270508,
      "learning_rate": 1e-05,
      "loss": 6.9526,
      "step": 1876
    },
    {
      "epoch": 0.10168021680216802,
      "step": 1876,
      "training_loss": 8.16310977935791
    },
    {
      "epoch": 0.10173441734417345,
      "step": 1877,
      "training_loss": 6.408857345581055
    },
    {
      "epoch": 0.10178861788617886,
      "step": 1878,
      "training_loss": 7.996594429016113
    },
    {
      "epoch": 0.10184281842818428,
      "step": 1879,
      "training_loss": 6.072828769683838
    },
    {
      "epoch": 0.1018970189701897,
      "grad_norm": 14.568513870239258,
      "learning_rate": 1e-05,
      "loss": 7.1603,
      "step": 1880
    },
    {
      "epoch": 0.1018970189701897,
      "step": 1880,
      "training_loss": 6.176990509033203
    },
    {
      "epoch": 0.10195121951219512,
      "step": 1881,
      "training_loss": 7.096137046813965
    },
    {
      "epoch": 0.10200542005420055,
      "step": 1882,
      "training_loss": 7.603650093078613
    },
    {
      "epoch": 0.10205962059620596,
      "step": 1883,
      "training_loss": 7.451887607574463
    },
    {
      "epoch": 0.10211382113821138,
      "grad_norm": 23.526025772094727,
      "learning_rate": 1e-05,
      "loss": 7.0822,
      "step": 1884
    },
    {
      "epoch": 0.10211382113821138,
      "step": 1884,
      "training_loss": 6.8904709815979
    },
    {
      "epoch": 0.1021680216802168,
      "step": 1885,
      "training_loss": 8.41188907623291
    },
    {
      "epoch": 0.10222222222222223,
      "step": 1886,
      "training_loss": 7.375153064727783
    },
    {
      "epoch": 0.10227642276422765,
      "step": 1887,
      "training_loss": 7.26914644241333
    },
    {
      "epoch": 0.10233062330623306,
      "grad_norm": 21.91248321533203,
      "learning_rate": 1e-05,
      "loss": 7.4867,
      "step": 1888
    },
    {
      "epoch": 0.10233062330623306,
      "step": 1888,
      "training_loss": 7.367809772491455
    },
    {
      "epoch": 0.10238482384823848,
      "step": 1889,
      "training_loss": 7.01220703125
    },
    {
      "epoch": 0.1024390243902439,
      "step": 1890,
      "training_loss": 5.942544460296631
    },
    {
      "epoch": 0.10249322493224933,
      "step": 1891,
      "training_loss": 7.407491207122803
    },
    {
      "epoch": 0.10254742547425474,
      "grad_norm": 23.44442367553711,
      "learning_rate": 1e-05,
      "loss": 6.9325,
      "step": 1892
    },
    {
      "epoch": 0.10254742547425474,
      "step": 1892,
      "training_loss": 6.786383628845215
    },
    {
      "epoch": 0.10260162601626016,
      "step": 1893,
      "training_loss": 7.513130187988281
    },
    {
      "epoch": 0.10265582655826558,
      "step": 1894,
      "training_loss": 8.256592750549316
    },
    {
      "epoch": 0.10271002710027101,
      "step": 1895,
      "training_loss": 7.2881760597229
    },
    {
      "epoch": 0.10276422764227643,
      "grad_norm": 32.483028411865234,
      "learning_rate": 1e-05,
      "loss": 7.4611,
      "step": 1896
    },
    {
      "epoch": 0.10276422764227643,
      "step": 1896,
      "training_loss": 4.818662166595459
    },
    {
      "epoch": 0.10281842818428184,
      "step": 1897,
      "training_loss": 6.713764667510986
    },
    {
      "epoch": 0.10287262872628726,
      "step": 1898,
      "training_loss": 6.064594268798828
    },
    {
      "epoch": 0.10292682926829268,
      "step": 1899,
      "training_loss": 8.250630378723145
    },
    {
      "epoch": 0.10298102981029811,
      "grad_norm": 29.14861297607422,
      "learning_rate": 1e-05,
      "loss": 6.4619,
      "step": 1900
    },
    {
      "epoch": 0.10298102981029811,
      "step": 1900,
      "training_loss": 5.687113285064697
    },
    {
      "epoch": 0.10303523035230353,
      "step": 1901,
      "training_loss": 7.016377925872803
    },
    {
      "epoch": 0.10308943089430894,
      "step": 1902,
      "training_loss": 7.7814531326293945
    },
    {
      "epoch": 0.10314363143631436,
      "step": 1903,
      "training_loss": 7.398102760314941
    },
    {
      "epoch": 0.10319783197831979,
      "grad_norm": 27.779050827026367,
      "learning_rate": 1e-05,
      "loss": 6.9708,
      "step": 1904
    },
    {
      "epoch": 0.10319783197831979,
      "step": 1904,
      "training_loss": 6.42487907409668
    },
    {
      "epoch": 0.10325203252032521,
      "step": 1905,
      "training_loss": 7.301225185394287
    },
    {
      "epoch": 0.10330623306233062,
      "step": 1906,
      "training_loss": 7.408722400665283
    },
    {
      "epoch": 0.10336043360433604,
      "step": 1907,
      "training_loss": 7.125107765197754
    },
    {
      "epoch": 0.10341463414634146,
      "grad_norm": 18.092239379882812,
      "learning_rate": 1e-05,
      "loss": 7.065,
      "step": 1908
    },
    {
      "epoch": 0.10341463414634146,
      "step": 1908,
      "training_loss": 7.588310241699219
    },
    {
      "epoch": 0.10346883468834689,
      "step": 1909,
      "training_loss": 7.085721015930176
    },
    {
      "epoch": 0.1035230352303523,
      "step": 1910,
      "training_loss": 7.9064435958862305
    },
    {
      "epoch": 0.10357723577235772,
      "step": 1911,
      "training_loss": 8.664698600769043
    },
    {
      "epoch": 0.10363143631436314,
      "grad_norm": 32.35499954223633,
      "learning_rate": 1e-05,
      "loss": 7.8113,
      "step": 1912
    },
    {
      "epoch": 0.10363143631436314,
      "step": 1912,
      "training_loss": 7.165069103240967
    },
    {
      "epoch": 0.10368563685636856,
      "step": 1913,
      "training_loss": 6.031279563903809
    },
    {
      "epoch": 0.10373983739837399,
      "step": 1914,
      "training_loss": 8.030135154724121
    },
    {
      "epoch": 0.1037940379403794,
      "step": 1915,
      "training_loss": 6.911118030548096
    },
    {
      "epoch": 0.10384823848238482,
      "grad_norm": 23.252477645874023,
      "learning_rate": 1e-05,
      "loss": 7.0344,
      "step": 1916
    },
    {
      "epoch": 0.10384823848238482,
      "step": 1916,
      "training_loss": 6.886211395263672
    },
    {
      "epoch": 0.10390243902439024,
      "step": 1917,
      "training_loss": 7.567008018493652
    },
    {
      "epoch": 0.10395663956639567,
      "step": 1918,
      "training_loss": 6.618258476257324
    },
    {
      "epoch": 0.10401084010840109,
      "step": 1919,
      "training_loss": 6.368618965148926
    },
    {
      "epoch": 0.1040650406504065,
      "grad_norm": 34.3057746887207,
      "learning_rate": 1e-05,
      "loss": 6.86,
      "step": 1920
    },
    {
      "epoch": 0.1040650406504065,
      "step": 1920,
      "training_loss": 5.036281108856201
    },
    {
      "epoch": 0.10411924119241192,
      "step": 1921,
      "training_loss": 7.437923431396484
    },
    {
      "epoch": 0.10417344173441734,
      "step": 1922,
      "training_loss": 5.69644832611084
    },
    {
      "epoch": 0.10422764227642277,
      "step": 1923,
      "training_loss": 7.384374618530273
    },
    {
      "epoch": 0.10428184281842819,
      "grad_norm": 25.513246536254883,
      "learning_rate": 1e-05,
      "loss": 6.3888,
      "step": 1924
    },
    {
      "epoch": 0.10428184281842819,
      "step": 1924,
      "training_loss": 7.976056098937988
    },
    {
      "epoch": 0.1043360433604336,
      "step": 1925,
      "training_loss": 7.0879740715026855
    },
    {
      "epoch": 0.10439024390243902,
      "step": 1926,
      "training_loss": 5.618669033050537
    },
    {
      "epoch": 0.10444444444444445,
      "step": 1927,
      "training_loss": 7.607583045959473
    },
    {
      "epoch": 0.10449864498644987,
      "grad_norm": 19.35965347290039,
      "learning_rate": 1e-05,
      "loss": 7.0726,
      "step": 1928
    },
    {
      "epoch": 0.10449864498644987,
      "step": 1928,
      "training_loss": 6.183177471160889
    },
    {
      "epoch": 0.10455284552845528,
      "step": 1929,
      "training_loss": 7.9468793869018555
    },
    {
      "epoch": 0.1046070460704607,
      "step": 1930,
      "training_loss": 6.587648868560791
    },
    {
      "epoch": 0.10466124661246612,
      "step": 1931,
      "training_loss": 5.988985061645508
    },
    {
      "epoch": 0.10471544715447155,
      "grad_norm": 15.967059135437012,
      "learning_rate": 1e-05,
      "loss": 6.6767,
      "step": 1932
    },
    {
      "epoch": 0.10471544715447155,
      "step": 1932,
      "training_loss": 6.778229236602783
    },
    {
      "epoch": 0.10476964769647697,
      "step": 1933,
      "training_loss": 7.788546562194824
    },
    {
      "epoch": 0.10482384823848238,
      "step": 1934,
      "training_loss": 5.470512866973877
    },
    {
      "epoch": 0.1048780487804878,
      "step": 1935,
      "training_loss": 7.287130832672119
    },
    {
      "epoch": 0.10493224932249323,
      "grad_norm": 18.69687271118164,
      "learning_rate": 1e-05,
      "loss": 6.8311,
      "step": 1936
    },
    {
      "epoch": 0.10493224932249323,
      "step": 1936,
      "training_loss": 7.2460432052612305
    },
    {
      "epoch": 0.10498644986449865,
      "step": 1937,
      "training_loss": 5.408318519592285
    },
    {
      "epoch": 0.10504065040650407,
      "step": 1938,
      "training_loss": 7.379900932312012
    },
    {
      "epoch": 0.10509485094850948,
      "step": 1939,
      "training_loss": 7.259235858917236
    },
    {
      "epoch": 0.1051490514905149,
      "grad_norm": 19.686471939086914,
      "learning_rate": 1e-05,
      "loss": 6.8234,
      "step": 1940
    },
    {
      "epoch": 0.1051490514905149,
      "step": 1940,
      "training_loss": 7.323550701141357
    },
    {
      "epoch": 0.10520325203252033,
      "step": 1941,
      "training_loss": 7.5910115242004395
    },
    {
      "epoch": 0.10525745257452575,
      "step": 1942,
      "training_loss": 6.2993879318237305
    },
    {
      "epoch": 0.10531165311653116,
      "step": 1943,
      "training_loss": 7.883515357971191
    },
    {
      "epoch": 0.10536585365853658,
      "grad_norm": 32.84680938720703,
      "learning_rate": 1e-05,
      "loss": 7.2744,
      "step": 1944
    },
    {
      "epoch": 0.10536585365853658,
      "step": 1944,
      "training_loss": 7.150788307189941
    },
    {
      "epoch": 0.105420054200542,
      "step": 1945,
      "training_loss": 6.813632965087891
    },
    {
      "epoch": 0.10547425474254743,
      "step": 1946,
      "training_loss": 6.969394207000732
    },
    {
      "epoch": 0.10552845528455285,
      "step": 1947,
      "training_loss": 7.042242527008057
    },
    {
      "epoch": 0.10558265582655826,
      "grad_norm": 14.737787246704102,
      "learning_rate": 1e-05,
      "loss": 6.994,
      "step": 1948
    },
    {
      "epoch": 0.10558265582655826,
      "step": 1948,
      "training_loss": 6.207190990447998
    },
    {
      "epoch": 0.10563685636856368,
      "step": 1949,
      "training_loss": 7.253708362579346
    },
    {
      "epoch": 0.10569105691056911,
      "step": 1950,
      "training_loss": 7.475819110870361
    },
    {
      "epoch": 0.10574525745257453,
      "step": 1951,
      "training_loss": 6.559276580810547
    },
    {
      "epoch": 0.10579945799457995,
      "grad_norm": 13.06650447845459,
      "learning_rate": 1e-05,
      "loss": 6.874,
      "step": 1952
    },
    {
      "epoch": 0.10579945799457995,
      "step": 1952,
      "training_loss": 7.1003923416137695
    },
    {
      "epoch": 0.10585365853658536,
      "step": 1953,
      "training_loss": 7.40924596786499
    },
    {
      "epoch": 0.10590785907859078,
      "step": 1954,
      "training_loss": 7.4768805503845215
    },
    {
      "epoch": 0.10596205962059621,
      "step": 1955,
      "training_loss": 6.6013569831848145
    },
    {
      "epoch": 0.10601626016260163,
      "grad_norm": 17.387710571289062,
      "learning_rate": 1e-05,
      "loss": 7.147,
      "step": 1956
    },
    {
      "epoch": 0.10601626016260163,
      "step": 1956,
      "training_loss": 6.421169281005859
    },
    {
      "epoch": 0.10607046070460704,
      "step": 1957,
      "training_loss": 7.327566623687744
    },
    {
      "epoch": 0.10612466124661246,
      "step": 1958,
      "training_loss": 7.779445171356201
    },
    {
      "epoch": 0.10617886178861789,
      "step": 1959,
      "training_loss": 6.880326271057129
    },
    {
      "epoch": 0.10623306233062331,
      "grad_norm": 15.404858589172363,
      "learning_rate": 1e-05,
      "loss": 7.1021,
      "step": 1960
    },
    {
      "epoch": 0.10623306233062331,
      "step": 1960,
      "training_loss": 6.2346577644348145
    },
    {
      "epoch": 0.10628726287262873,
      "step": 1961,
      "training_loss": 8.373960494995117
    },
    {
      "epoch": 0.10634146341463414,
      "step": 1962,
      "training_loss": 7.205341339111328
    },
    {
      "epoch": 0.10639566395663956,
      "step": 1963,
      "training_loss": 9.370823860168457
    },
    {
      "epoch": 0.10644986449864499,
      "grad_norm": 27.95480728149414,
      "learning_rate": 1e-05,
      "loss": 7.7962,
      "step": 1964
    },
    {
      "epoch": 0.10644986449864499,
      "step": 1964,
      "training_loss": 6.605642795562744
    },
    {
      "epoch": 0.10650406504065041,
      "step": 1965,
      "training_loss": 6.541418075561523
    },
    {
      "epoch": 0.10655826558265583,
      "step": 1966,
      "training_loss": 8.00596809387207
    },
    {
      "epoch": 0.10661246612466124,
      "step": 1967,
      "training_loss": 7.321264743804932
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 19.02974510192871,
      "learning_rate": 1e-05,
      "loss": 7.1186,
      "step": 1968
    },
    {
      "epoch": 0.10666666666666667,
      "step": 1968,
      "training_loss": 6.069786071777344
    },
    {
      "epoch": 0.10672086720867209,
      "step": 1969,
      "training_loss": 9.089376449584961
    },
    {
      "epoch": 0.10677506775067751,
      "step": 1970,
      "training_loss": 6.151271820068359
    },
    {
      "epoch": 0.10682926829268292,
      "step": 1971,
      "training_loss": 6.22847843170166
    },
    {
      "epoch": 0.10688346883468834,
      "grad_norm": 15.762473106384277,
      "learning_rate": 1e-05,
      "loss": 6.8847,
      "step": 1972
    },
    {
      "epoch": 0.10688346883468834,
      "step": 1972,
      "training_loss": 6.670057773590088
    },
    {
      "epoch": 0.10693766937669377,
      "step": 1973,
      "training_loss": 8.368659019470215
    },
    {
      "epoch": 0.10699186991869919,
      "step": 1974,
      "training_loss": 6.4137115478515625
    },
    {
      "epoch": 0.1070460704607046,
      "step": 1975,
      "training_loss": 7.357468128204346
    },
    {
      "epoch": 0.10710027100271002,
      "grad_norm": 15.742369651794434,
      "learning_rate": 1e-05,
      "loss": 7.2025,
      "step": 1976
    },
    {
      "epoch": 0.10710027100271002,
      "step": 1976,
      "training_loss": 6.975642681121826
    },
    {
      "epoch": 0.10715447154471544,
      "step": 1977,
      "training_loss": 5.945455074310303
    },
    {
      "epoch": 0.10720867208672087,
      "step": 1978,
      "training_loss": 6.926813125610352
    },
    {
      "epoch": 0.10726287262872629,
      "step": 1979,
      "training_loss": 7.6792144775390625
    },
    {
      "epoch": 0.1073170731707317,
      "grad_norm": 18.588665008544922,
      "learning_rate": 1e-05,
      "loss": 6.8818,
      "step": 1980
    },
    {
      "epoch": 0.1073170731707317,
      "step": 1980,
      "training_loss": 6.699853897094727
    },
    {
      "epoch": 0.10737127371273712,
      "step": 1981,
      "training_loss": 6.843050003051758
    },
    {
      "epoch": 0.10742547425474255,
      "step": 1982,
      "training_loss": 6.9632134437561035
    },
    {
      "epoch": 0.10747967479674797,
      "step": 1983,
      "training_loss": 7.026053428649902
    },
    {
      "epoch": 0.10753387533875339,
      "grad_norm": 14.146771430969238,
      "learning_rate": 1e-05,
      "loss": 6.883,
      "step": 1984
    },
    {
      "epoch": 0.10753387533875339,
      "step": 1984,
      "training_loss": 6.589237689971924
    },
    {
      "epoch": 0.1075880758807588,
      "step": 1985,
      "training_loss": 7.369289875030518
    },
    {
      "epoch": 0.10764227642276422,
      "step": 1986,
      "training_loss": 7.209839820861816
    },
    {
      "epoch": 0.10769647696476965,
      "step": 1987,
      "training_loss": 5.73673677444458
    },
    {
      "epoch": 0.10775067750677507,
      "grad_norm": 22.378562927246094,
      "learning_rate": 1e-05,
      "loss": 6.7263,
      "step": 1988
    },
    {
      "epoch": 0.10775067750677507,
      "step": 1988,
      "training_loss": 5.998231887817383
    },
    {
      "epoch": 0.10780487804878049,
      "step": 1989,
      "training_loss": 8.2850341796875
    },
    {
      "epoch": 0.1078590785907859,
      "step": 1990,
      "training_loss": 6.635718822479248
    },
    {
      "epoch": 0.10791327913279133,
      "step": 1991,
      "training_loss": 6.44923734664917
    },
    {
      "epoch": 0.10796747967479675,
      "grad_norm": 19.15929412841797,
      "learning_rate": 1e-05,
      "loss": 6.8421,
      "step": 1992
    },
    {
      "epoch": 0.10796747967479675,
      "step": 1992,
      "training_loss": 6.53092098236084
    },
    {
      "epoch": 0.10802168021680217,
      "step": 1993,
      "training_loss": 5.64456033706665
    },
    {
      "epoch": 0.10807588075880759,
      "step": 1994,
      "training_loss": 7.11775541305542
    },
    {
      "epoch": 0.108130081300813,
      "step": 1995,
      "training_loss": 7.091853618621826
    },
    {
      "epoch": 0.10818428184281843,
      "grad_norm": 12.573721885681152,
      "learning_rate": 1e-05,
      "loss": 6.5963,
      "step": 1996
    },
    {
      "epoch": 0.10818428184281843,
      "step": 1996,
      "training_loss": 8.003024101257324
    },
    {
      "epoch": 0.10823848238482385,
      "step": 1997,
      "training_loss": 7.554884433746338
    },
    {
      "epoch": 0.10829268292682927,
      "step": 1998,
      "training_loss": 6.000514507293701
    },
    {
      "epoch": 0.10834688346883468,
      "step": 1999,
      "training_loss": 7.263428688049316
    },
    {
      "epoch": 0.10840108401084012,
      "grad_norm": 29.47939682006836,
      "learning_rate": 1e-05,
      "loss": 7.2055,
      "step": 2000
    },
    {
      "epoch": 0.10840108401084012,
      "step": 2000,
      "training_loss": 4.917733669281006
    },
    {
      "epoch": 0.10845528455284553,
      "step": 2001,
      "training_loss": 5.301447868347168
    },
    {
      "epoch": 0.10850948509485095,
      "step": 2002,
      "training_loss": 7.338685035705566
    },
    {
      "epoch": 0.10856368563685637,
      "step": 2003,
      "training_loss": 6.105548858642578
    },
    {
      "epoch": 0.10861788617886178,
      "grad_norm": 38.752967834472656,
      "learning_rate": 1e-05,
      "loss": 5.9159,
      "step": 2004
    },
    {
      "epoch": 0.10861788617886178,
      "step": 2004,
      "training_loss": 7.486490249633789
    },
    {
      "epoch": 0.10867208672086721,
      "step": 2005,
      "training_loss": 6.366743087768555
    },
    {
      "epoch": 0.10872628726287263,
      "step": 2006,
      "training_loss": 7.139036178588867
    },
    {
      "epoch": 0.10878048780487805,
      "step": 2007,
      "training_loss": 7.025553226470947
    },
    {
      "epoch": 0.10883468834688347,
      "grad_norm": 20.09546661376953,
      "learning_rate": 1e-05,
      "loss": 7.0045,
      "step": 2008
    },
    {
      "epoch": 0.10883468834688347,
      "step": 2008,
      "training_loss": 7.152218818664551
    },
    {
      "epoch": 0.10888888888888888,
      "step": 2009,
      "training_loss": 7.274974822998047
    },
    {
      "epoch": 0.10894308943089431,
      "step": 2010,
      "training_loss": 5.628118991851807
    },
    {
      "epoch": 0.10899728997289973,
      "step": 2011,
      "training_loss": 5.626953125
    },
    {
      "epoch": 0.10905149051490515,
      "grad_norm": 21.96625518798828,
      "learning_rate": 1e-05,
      "loss": 6.4206,
      "step": 2012
    },
    {
      "epoch": 0.10905149051490515,
      "step": 2012,
      "training_loss": 7.25082540512085
    },
    {
      "epoch": 0.10910569105691056,
      "step": 2013,
      "training_loss": 7.4336066246032715
    },
    {
      "epoch": 0.109159891598916,
      "step": 2014,
      "training_loss": 7.142324447631836
    },
    {
      "epoch": 0.10921409214092141,
      "step": 2015,
      "training_loss": 6.022584915161133
    },
    {
      "epoch": 0.10926829268292683,
      "grad_norm": 18.266977310180664,
      "learning_rate": 1e-05,
      "loss": 6.9623,
      "step": 2016
    },
    {
      "epoch": 0.10926829268292683,
      "step": 2016,
      "training_loss": 6.441844463348389
    },
    {
      "epoch": 0.10932249322493225,
      "step": 2017,
      "training_loss": 7.817961692810059
    },
    {
      "epoch": 0.10937669376693766,
      "step": 2018,
      "training_loss": 5.661920070648193
    },
    {
      "epoch": 0.1094308943089431,
      "step": 2019,
      "training_loss": 6.9994378089904785
    },
    {
      "epoch": 0.10948509485094851,
      "grad_norm": 32.17500686645508,
      "learning_rate": 1e-05,
      "loss": 6.7303,
      "step": 2020
    },
    {
      "epoch": 0.10948509485094851,
      "step": 2020,
      "training_loss": 5.38447904586792
    },
    {
      "epoch": 0.10953929539295393,
      "step": 2021,
      "training_loss": 8.099637031555176
    },
    {
      "epoch": 0.10959349593495935,
      "step": 2022,
      "training_loss": 6.31291389465332
    },
    {
      "epoch": 0.10964769647696478,
      "step": 2023,
      "training_loss": 7.197179794311523
    },
    {
      "epoch": 0.1097018970189702,
      "grad_norm": 17.256973266601562,
      "learning_rate": 1e-05,
      "loss": 6.7486,
      "step": 2024
    },
    {
      "epoch": 0.1097018970189702,
      "step": 2024,
      "training_loss": 6.13470983505249
    },
    {
      "epoch": 0.10975609756097561,
      "step": 2025,
      "training_loss": 7.179380893707275
    },
    {
      "epoch": 0.10981029810298103,
      "step": 2026,
      "training_loss": 6.916756629943848
    },
    {
      "epoch": 0.10986449864498644,
      "step": 2027,
      "training_loss": 7.053202152252197
    },
    {
      "epoch": 0.10991869918699188,
      "grad_norm": 23.984786987304688,
      "learning_rate": 1e-05,
      "loss": 6.821,
      "step": 2028
    },
    {
      "epoch": 0.10991869918699188,
      "step": 2028,
      "training_loss": 7.695729732513428
    },
    {
      "epoch": 0.10997289972899729,
      "step": 2029,
      "training_loss": 7.657358646392822
    },
    {
      "epoch": 0.11002710027100271,
      "step": 2030,
      "training_loss": 6.535762786865234
    },
    {
      "epoch": 0.11008130081300813,
      "step": 2031,
      "training_loss": 6.6994500160217285
    },
    {
      "epoch": 0.11013550135501356,
      "grad_norm": 23.21292495727539,
      "learning_rate": 1e-05,
      "loss": 7.1471,
      "step": 2032
    },
    {
      "epoch": 0.11013550135501356,
      "step": 2032,
      "training_loss": 7.39978551864624
    },
    {
      "epoch": 0.11018970189701897,
      "step": 2033,
      "training_loss": 7.436779975891113
    },
    {
      "epoch": 0.11024390243902439,
      "step": 2034,
      "training_loss": 7.955730438232422
    },
    {
      "epoch": 0.11029810298102981,
      "step": 2035,
      "training_loss": 6.83694314956665
    },
    {
      "epoch": 0.11035230352303523,
      "grad_norm": 13.935140609741211,
      "learning_rate": 1e-05,
      "loss": 7.4073,
      "step": 2036
    },
    {
      "epoch": 0.11035230352303523,
      "step": 2036,
      "training_loss": 8.060543060302734
    },
    {
      "epoch": 0.11040650406504066,
      "step": 2037,
      "training_loss": 6.091076374053955
    },
    {
      "epoch": 0.11046070460704607,
      "step": 2038,
      "training_loss": 5.8447265625
    },
    {
      "epoch": 0.11051490514905149,
      "step": 2039,
      "training_loss": 6.937036991119385
    },
    {
      "epoch": 0.11056910569105691,
      "grad_norm": 18.470890045166016,
      "learning_rate": 1e-05,
      "loss": 6.7333,
      "step": 2040
    },
    {
      "epoch": 0.11056910569105691,
      "step": 2040,
      "training_loss": 7.043863773345947
    },
    {
      "epoch": 0.11062330623306232,
      "step": 2041,
      "training_loss": 6.365616321563721
    },
    {
      "epoch": 0.11067750677506775,
      "step": 2042,
      "training_loss": 7.37481164932251
    },
    {
      "epoch": 0.11073170731707317,
      "step": 2043,
      "training_loss": 6.091265678405762
    },
    {
      "epoch": 0.11078590785907859,
      "grad_norm": 18.24860191345215,
      "learning_rate": 1e-05,
      "loss": 6.7189,
      "step": 2044
    },
    {
      "epoch": 0.11078590785907859,
      "step": 2044,
      "training_loss": 7.589911937713623
    },
    {
      "epoch": 0.110840108401084,
      "step": 2045,
      "training_loss": 7.25822639465332
    },
    {
      "epoch": 0.11089430894308944,
      "step": 2046,
      "training_loss": 7.216233253479004
    },
    {
      "epoch": 0.11094850948509485,
      "step": 2047,
      "training_loss": 6.628027439117432
    },
    {
      "epoch": 0.11100271002710027,
      "grad_norm": 25.72601890563965,
      "learning_rate": 1e-05,
      "loss": 7.1731,
      "step": 2048
    },
    {
      "epoch": 0.11100271002710027,
      "step": 2048,
      "training_loss": 7.169719696044922
    },
    {
      "epoch": 0.11105691056910569,
      "step": 2049,
      "training_loss": 6.779860019683838
    },
    {
      "epoch": 0.1111111111111111,
      "step": 2050,
      "training_loss": 6.15336275100708
    },
    {
      "epoch": 0.11116531165311654,
      "step": 2051,
      "training_loss": 6.842559814453125
    },
    {
      "epoch": 0.11121951219512195,
      "grad_norm": 19.939983367919922,
      "learning_rate": 1e-05,
      "loss": 6.7364,
      "step": 2052
    },
    {
      "epoch": 0.11121951219512195,
      "step": 2052,
      "training_loss": 7.427682399749756
    },
    {
      "epoch": 0.11127371273712737,
      "step": 2053,
      "training_loss": 6.81777811050415
    },
    {
      "epoch": 0.11132791327913279,
      "step": 2054,
      "training_loss": 5.9099955558776855
    },
    {
      "epoch": 0.11138211382113822,
      "step": 2055,
      "training_loss": 4.812455177307129
    },
    {
      "epoch": 0.11143631436314363,
      "grad_norm": 30.833858489990234,
      "learning_rate": 1e-05,
      "loss": 6.242,
      "step": 2056
    },
    {
      "epoch": 0.11143631436314363,
      "step": 2056,
      "training_loss": 7.872157096862793
    },
    {
      "epoch": 0.11149051490514905,
      "step": 2057,
      "training_loss": 7.923331260681152
    },
    {
      "epoch": 0.11154471544715447,
      "step": 2058,
      "training_loss": 7.510112762451172
    },
    {
      "epoch": 0.11159891598915989,
      "step": 2059,
      "training_loss": 7.7613911628723145
    },
    {
      "epoch": 0.11165311653116532,
      "grad_norm": 16.247591018676758,
      "learning_rate": 1e-05,
      "loss": 7.7667,
      "step": 2060
    },
    {
      "epoch": 0.11165311653116532,
      "step": 2060,
      "training_loss": 5.250670909881592
    },
    {
      "epoch": 0.11170731707317073,
      "step": 2061,
      "training_loss": 7.279896259307861
    },
    {
      "epoch": 0.11176151761517615,
      "step": 2062,
      "training_loss": 6.21408224105835
    },
    {
      "epoch": 0.11181571815718157,
      "step": 2063,
      "training_loss": 7.445474624633789
    },
    {
      "epoch": 0.111869918699187,
      "grad_norm": 18.06972885131836,
      "learning_rate": 1e-05,
      "loss": 6.5475,
      "step": 2064
    },
    {
      "epoch": 0.111869918699187,
      "step": 2064,
      "training_loss": 6.311251163482666
    },
    {
      "epoch": 0.11192411924119242,
      "step": 2065,
      "training_loss": 5.578175067901611
    },
    {
      "epoch": 0.11197831978319783,
      "step": 2066,
      "training_loss": 7.900197982788086
    },
    {
      "epoch": 0.11203252032520325,
      "step": 2067,
      "training_loss": 7.469659328460693
    },
    {
      "epoch": 0.11208672086720867,
      "grad_norm": 22.75433921813965,
      "learning_rate": 1e-05,
      "loss": 6.8148,
      "step": 2068
    },
    {
      "epoch": 0.11208672086720867,
      "step": 2068,
      "training_loss": 6.8019514083862305
    },
    {
      "epoch": 0.1121409214092141,
      "step": 2069,
      "training_loss": 7.554027557373047
    },
    {
      "epoch": 0.11219512195121951,
      "step": 2070,
      "training_loss": 7.7410664558410645
    },
    {
      "epoch": 0.11224932249322493,
      "step": 2071,
      "training_loss": 7.443899631500244
    },
    {
      "epoch": 0.11230352303523035,
      "grad_norm": 28.041431427001953,
      "learning_rate": 1e-05,
      "loss": 7.3852,
      "step": 2072
    },
    {
      "epoch": 0.11230352303523035,
      "step": 2072,
      "training_loss": 8.143228530883789
    },
    {
      "epoch": 0.11235772357723577,
      "step": 2073,
      "training_loss": 6.881195545196533
    },
    {
      "epoch": 0.1124119241192412,
      "step": 2074,
      "training_loss": 6.679059028625488
    },
    {
      "epoch": 0.11246612466124661,
      "step": 2075,
      "training_loss": 7.1452250480651855
    },
    {
      "epoch": 0.11252032520325203,
      "grad_norm": 24.63528823852539,
      "learning_rate": 1e-05,
      "loss": 7.2122,
      "step": 2076
    },
    {
      "epoch": 0.11252032520325203,
      "step": 2076,
      "training_loss": 7.797855854034424
    },
    {
      "epoch": 0.11257452574525745,
      "step": 2077,
      "training_loss": 7.4600934982299805
    },
    {
      "epoch": 0.11262872628726288,
      "step": 2078,
      "training_loss": 4.757382392883301
    },
    {
      "epoch": 0.1126829268292683,
      "step": 2079,
      "training_loss": 7.314043045043945
    },
    {
      "epoch": 0.11273712737127371,
      "grad_norm": 14.911195755004883,
      "learning_rate": 1e-05,
      "loss": 6.8323,
      "step": 2080
    },
    {
      "epoch": 0.11273712737127371,
      "step": 2080,
      "training_loss": 7.941863536834717
    },
    {
      "epoch": 0.11279132791327913,
      "step": 2081,
      "training_loss": 6.773160934448242
    },
    {
      "epoch": 0.11284552845528455,
      "step": 2082,
      "training_loss": 8.481539726257324
    },
    {
      "epoch": 0.11289972899728998,
      "step": 2083,
      "training_loss": 6.505673408508301
    },
    {
      "epoch": 0.1129539295392954,
      "grad_norm": 18.554052352905273,
      "learning_rate": 1e-05,
      "loss": 7.4256,
      "step": 2084
    },
    {
      "epoch": 0.1129539295392954,
      "step": 2084,
      "training_loss": 6.93223762512207
    },
    {
      "epoch": 0.11300813008130081,
      "step": 2085,
      "training_loss": 7.408477306365967
    },
    {
      "epoch": 0.11306233062330623,
      "step": 2086,
      "training_loss": 7.136406898498535
    },
    {
      "epoch": 0.11311653116531166,
      "step": 2087,
      "training_loss": 11.030338287353516
    },
    {
      "epoch": 0.11317073170731708,
      "grad_norm": 63.35004806518555,
      "learning_rate": 1e-05,
      "loss": 8.1269,
      "step": 2088
    },
    {
      "epoch": 0.11317073170731708,
      "step": 2088,
      "training_loss": 7.2723541259765625
    },
    {
      "epoch": 0.1132249322493225,
      "step": 2089,
      "training_loss": 6.949575901031494
    },
    {
      "epoch": 0.11327913279132791,
      "step": 2090,
      "training_loss": 8.952471733093262
    },
    {
      "epoch": 0.11333333333333333,
      "step": 2091,
      "training_loss": 6.025363445281982
    },
    {
      "epoch": 0.11338753387533876,
      "grad_norm": 25.684778213500977,
      "learning_rate": 1e-05,
      "loss": 7.2999,
      "step": 2092
    },
    {
      "epoch": 0.11338753387533876,
      "step": 2092,
      "training_loss": 7.202154159545898
    },
    {
      "epoch": 0.11344173441734418,
      "step": 2093,
      "training_loss": 7.344385147094727
    },
    {
      "epoch": 0.11349593495934959,
      "step": 2094,
      "training_loss": 6.950316429138184
    },
    {
      "epoch": 0.11355013550135501,
      "step": 2095,
      "training_loss": 6.523730278015137
    },
    {
      "epoch": 0.11360433604336044,
      "grad_norm": 19.213626861572266,
      "learning_rate": 1e-05,
      "loss": 7.0051,
      "step": 2096
    },
    {
      "epoch": 0.11360433604336044,
      "step": 2096,
      "training_loss": 4.602724552154541
    },
    {
      "epoch": 0.11365853658536586,
      "step": 2097,
      "training_loss": 6.775113582611084
    },
    {
      "epoch": 0.11371273712737127,
      "step": 2098,
      "training_loss": 7.911966323852539
    },
    {
      "epoch": 0.11376693766937669,
      "step": 2099,
      "training_loss": 5.996678352355957
    },
    {
      "epoch": 0.11382113821138211,
      "grad_norm": 26.162574768066406,
      "learning_rate": 1e-05,
      "loss": 6.3216,
      "step": 2100
    },
    {
      "epoch": 0.11382113821138211,
      "step": 2100,
      "training_loss": 6.781576633453369
    },
    {
      "epoch": 0.11387533875338754,
      "step": 2101,
      "training_loss": 7.5383172035217285
    },
    {
      "epoch": 0.11392953929539296,
      "step": 2102,
      "training_loss": 7.247105121612549
    },
    {
      "epoch": 0.11398373983739837,
      "step": 2103,
      "training_loss": 5.870489120483398
    },
    {
      "epoch": 0.11403794037940379,
      "grad_norm": 27.816017150878906,
      "learning_rate": 1e-05,
      "loss": 6.8594,
      "step": 2104
    },
    {
      "epoch": 0.11403794037940379,
      "step": 2104,
      "training_loss": 7.518408298492432
    },
    {
      "epoch": 0.11409214092140921,
      "step": 2105,
      "training_loss": 8.428993225097656
    },
    {
      "epoch": 0.11414634146341464,
      "step": 2106,
      "training_loss": 6.692682266235352
    },
    {
      "epoch": 0.11420054200542006,
      "step": 2107,
      "training_loss": 8.433989524841309
    },
    {
      "epoch": 0.11425474254742547,
      "grad_norm": 32.873779296875,
      "learning_rate": 1e-05,
      "loss": 7.7685,
      "step": 2108
    },
    {
      "epoch": 0.11425474254742547,
      "step": 2108,
      "training_loss": 6.494830131530762
    },
    {
      "epoch": 0.11430894308943089,
      "step": 2109,
      "training_loss": 8.253833770751953
    },
    {
      "epoch": 0.11436314363143632,
      "step": 2110,
      "training_loss": 6.045570373535156
    },
    {
      "epoch": 0.11441734417344174,
      "step": 2111,
      "training_loss": 7.167105197906494
    },
    {
      "epoch": 0.11447154471544715,
      "grad_norm": 18.969377517700195,
      "learning_rate": 1e-05,
      "loss": 6.9903,
      "step": 2112
    },
    {
      "epoch": 0.11447154471544715,
      "step": 2112,
      "training_loss": 7.4354658126831055
    },
    {
      "epoch": 0.11452574525745257,
      "step": 2113,
      "training_loss": 7.504770278930664
    },
    {
      "epoch": 0.11457994579945799,
      "step": 2114,
      "training_loss": 5.098837375640869
    },
    {
      "epoch": 0.11463414634146342,
      "step": 2115,
      "training_loss": 7.09758996963501
    },
    {
      "epoch": 0.11468834688346884,
      "grad_norm": 17.852073669433594,
      "learning_rate": 1e-05,
      "loss": 6.7842,
      "step": 2116
    },
    {
      "epoch": 0.11468834688346884,
      "step": 2116,
      "training_loss": 6.416047096252441
    },
    {
      "epoch": 0.11474254742547425,
      "step": 2117,
      "training_loss": 5.340090751647949
    },
    {
      "epoch": 0.11479674796747967,
      "step": 2118,
      "training_loss": 6.885333061218262
    },
    {
      "epoch": 0.1148509485094851,
      "step": 2119,
      "training_loss": 6.9160051345825195
    },
    {
      "epoch": 0.11490514905149052,
      "grad_norm": 22.28732681274414,
      "learning_rate": 1e-05,
      "loss": 6.3894,
      "step": 2120
    },
    {
      "epoch": 0.11490514905149052,
      "step": 2120,
      "training_loss": 7.809572219848633
    },
    {
      "epoch": 0.11495934959349594,
      "step": 2121,
      "training_loss": 6.504796981811523
    },
    {
      "epoch": 0.11501355013550135,
      "step": 2122,
      "training_loss": 6.268870830535889
    },
    {
      "epoch": 0.11506775067750677,
      "step": 2123,
      "training_loss": 7.7261528968811035
    },
    {
      "epoch": 0.1151219512195122,
      "grad_norm": 26.758596420288086,
      "learning_rate": 1e-05,
      "loss": 7.0773,
      "step": 2124
    },
    {
      "epoch": 0.1151219512195122,
      "step": 2124,
      "training_loss": 7.725598335266113
    },
    {
      "epoch": 0.11517615176151762,
      "step": 2125,
      "training_loss": 7.527799129486084
    },
    {
      "epoch": 0.11523035230352303,
      "step": 2126,
      "training_loss": 7.7196478843688965
    },
    {
      "epoch": 0.11528455284552845,
      "step": 2127,
      "training_loss": 6.61648416519165
    },
    {
      "epoch": 0.11533875338753388,
      "grad_norm": 22.383005142211914,
      "learning_rate": 1e-05,
      "loss": 7.3974,
      "step": 2128
    },
    {
      "epoch": 0.11533875338753388,
      "step": 2128,
      "training_loss": 7.687233924865723
    },
    {
      "epoch": 0.1153929539295393,
      "step": 2129,
      "training_loss": 4.464258670806885
    },
    {
      "epoch": 0.11544715447154472,
      "step": 2130,
      "training_loss": 5.528926849365234
    },
    {
      "epoch": 0.11550135501355013,
      "step": 2131,
      "training_loss": 6.233382225036621
    },
    {
      "epoch": 0.11555555555555555,
      "grad_norm": 26.185707092285156,
      "learning_rate": 1e-05,
      "loss": 5.9785,
      "step": 2132
    },
    {
      "epoch": 0.11555555555555555,
      "step": 2132,
      "training_loss": 5.084183216094971
    },
    {
      "epoch": 0.11560975609756098,
      "step": 2133,
      "training_loss": 6.269379615783691
    },
    {
      "epoch": 0.1156639566395664,
      "step": 2134,
      "training_loss": 6.6263041496276855
    },
    {
      "epoch": 0.11571815718157182,
      "step": 2135,
      "training_loss": 7.871219158172607
    },
    {
      "epoch": 0.11577235772357723,
      "grad_norm": 22.49785614013672,
      "learning_rate": 1e-05,
      "loss": 6.4628,
      "step": 2136
    },
    {
      "epoch": 0.11577235772357723,
      "step": 2136,
      "training_loss": 7.294027805328369
    },
    {
      "epoch": 0.11582655826558265,
      "step": 2137,
      "training_loss": 6.727995872497559
    },
    {
      "epoch": 0.11588075880758808,
      "step": 2138,
      "training_loss": 5.219162464141846
    },
    {
      "epoch": 0.1159349593495935,
      "step": 2139,
      "training_loss": 8.14572525024414
    },
    {
      "epoch": 0.11598915989159891,
      "grad_norm": 19.990718841552734,
      "learning_rate": 1e-05,
      "loss": 6.8467,
      "step": 2140
    },
    {
      "epoch": 0.11598915989159891,
      "step": 2140,
      "training_loss": 6.7659759521484375
    },
    {
      "epoch": 0.11604336043360433,
      "step": 2141,
      "training_loss": 6.988767623901367
    },
    {
      "epoch": 0.11609756097560976,
      "step": 2142,
      "training_loss": 6.989974498748779
    },
    {
      "epoch": 0.11615176151761518,
      "step": 2143,
      "training_loss": 6.723678112030029
    },
    {
      "epoch": 0.1162059620596206,
      "grad_norm": 23.07312774658203,
      "learning_rate": 1e-05,
      "loss": 6.8671,
      "step": 2144
    },
    {
      "epoch": 0.1162059620596206,
      "step": 2144,
      "training_loss": 6.134854793548584
    },
    {
      "epoch": 0.11626016260162601,
      "step": 2145,
      "training_loss": 5.1014485359191895
    },
    {
      "epoch": 0.11631436314363143,
      "step": 2146,
      "training_loss": 5.724407196044922
    },
    {
      "epoch": 0.11636856368563686,
      "step": 2147,
      "training_loss": 7.357482433319092
    },
    {
      "epoch": 0.11642276422764228,
      "grad_norm": 15.279850006103516,
      "learning_rate": 1e-05,
      "loss": 6.0795,
      "step": 2148
    },
    {
      "epoch": 0.11642276422764228,
      "step": 2148,
      "training_loss": 5.9882097244262695
    },
    {
      "epoch": 0.1164769647696477,
      "step": 2149,
      "training_loss": 7.7010817527771
    },
    {
      "epoch": 0.11653116531165311,
      "step": 2150,
      "training_loss": 7.026291370391846
    },
    {
      "epoch": 0.11658536585365854,
      "step": 2151,
      "training_loss": 6.788961887359619
    },
    {
      "epoch": 0.11663956639566396,
      "grad_norm": 70.93637084960938,
      "learning_rate": 1e-05,
      "loss": 6.8761,
      "step": 2152
    },
    {
      "epoch": 0.11663956639566396,
      "step": 2152,
      "training_loss": 6.7039103507995605
    },
    {
      "epoch": 0.11669376693766938,
      "step": 2153,
      "training_loss": 7.3231120109558105
    },
    {
      "epoch": 0.1167479674796748,
      "step": 2154,
      "training_loss": 5.800763130187988
    },
    {
      "epoch": 0.11680216802168021,
      "step": 2155,
      "training_loss": 7.861323356628418
    },
    {
      "epoch": 0.11685636856368564,
      "grad_norm": 19.008670806884766,
      "learning_rate": 1e-05,
      "loss": 6.9223,
      "step": 2156
    },
    {
      "epoch": 0.11685636856368564,
      "step": 2156,
      "training_loss": 7.269955635070801
    },
    {
      "epoch": 0.11691056910569106,
      "step": 2157,
      "training_loss": 4.812743663787842
    },
    {
      "epoch": 0.11696476964769648,
      "step": 2158,
      "training_loss": 7.735233783721924
    },
    {
      "epoch": 0.11701897018970189,
      "step": 2159,
      "training_loss": 6.502394199371338
    },
    {
      "epoch": 0.11707317073170732,
      "grad_norm": 23.99907684326172,
      "learning_rate": 1e-05,
      "loss": 6.5801,
      "step": 2160
    },
    {
      "epoch": 0.11707317073170732,
      "step": 2160,
      "training_loss": 7.685892105102539
    },
    {
      "epoch": 0.11712737127371274,
      "step": 2161,
      "training_loss": 7.871717929840088
    },
    {
      "epoch": 0.11718157181571816,
      "step": 2162,
      "training_loss": 6.920495986938477
    },
    {
      "epoch": 0.11723577235772357,
      "step": 2163,
      "training_loss": 7.4070515632629395
    },
    {
      "epoch": 0.11728997289972899,
      "grad_norm": 20.27174186706543,
      "learning_rate": 1e-05,
      "loss": 7.4713,
      "step": 2164
    },
    {
      "epoch": 0.11728997289972899,
      "step": 2164,
      "training_loss": 7.945376396179199
    },
    {
      "epoch": 0.11734417344173442,
      "step": 2165,
      "training_loss": 4.502406120300293
    },
    {
      "epoch": 0.11739837398373984,
      "step": 2166,
      "training_loss": 7.04753303527832
    },
    {
      "epoch": 0.11745257452574526,
      "step": 2167,
      "training_loss": 7.5309343338012695
    },
    {
      "epoch": 0.11750677506775067,
      "grad_norm": 22.33063316345215,
      "learning_rate": 1e-05,
      "loss": 6.7566,
      "step": 2168
    },
    {
      "epoch": 0.11750677506775067,
      "step": 2168,
      "training_loss": 6.14025354385376
    },
    {
      "epoch": 0.11756097560975609,
      "step": 2169,
      "training_loss": 6.563918590545654
    },
    {
      "epoch": 0.11761517615176152,
      "step": 2170,
      "training_loss": 7.859302520751953
    },
    {
      "epoch": 0.11766937669376694,
      "step": 2171,
      "training_loss": 6.076991558074951
    },
    {
      "epoch": 0.11772357723577236,
      "grad_norm": 17.46418571472168,
      "learning_rate": 1e-05,
      "loss": 6.6601,
      "step": 2172
    },
    {
      "epoch": 0.11772357723577236,
      "step": 2172,
      "training_loss": 5.954078674316406
    },
    {
      "epoch": 0.11777777777777777,
      "step": 2173,
      "training_loss": 7.260891914367676
    },
    {
      "epoch": 0.1178319783197832,
      "step": 2174,
      "training_loss": 9.489696502685547
    },
    {
      "epoch": 0.11788617886178862,
      "step": 2175,
      "training_loss": 8.630245208740234
    },
    {
      "epoch": 0.11794037940379404,
      "grad_norm": 34.31849670410156,
      "learning_rate": 1e-05,
      "loss": 7.8337,
      "step": 2176
    },
    {
      "epoch": 0.11794037940379404,
      "step": 2176,
      "training_loss": 7.6533074378967285
    },
    {
      "epoch": 0.11799457994579945,
      "step": 2177,
      "training_loss": 5.671876430511475
    },
    {
      "epoch": 0.11804878048780487,
      "step": 2178,
      "training_loss": 6.775413990020752
    },
    {
      "epoch": 0.1181029810298103,
      "step": 2179,
      "training_loss": 5.70692253112793
    },
    {
      "epoch": 0.11815718157181572,
      "grad_norm": 18.061006546020508,
      "learning_rate": 1e-05,
      "loss": 6.4519,
      "step": 2180
    },
    {
      "epoch": 0.11815718157181572,
      "step": 2180,
      "training_loss": 7.063594818115234
    },
    {
      "epoch": 0.11821138211382114,
      "step": 2181,
      "training_loss": 7.9276885986328125
    },
    {
      "epoch": 0.11826558265582655,
      "step": 2182,
      "training_loss": 9.29543399810791
    },
    {
      "epoch": 0.11831978319783198,
      "step": 2183,
      "training_loss": 6.55404806137085
    },
    {
      "epoch": 0.1183739837398374,
      "grad_norm": 15.392293930053711,
      "learning_rate": 1e-05,
      "loss": 7.7102,
      "step": 2184
    },
    {
      "epoch": 0.1183739837398374,
      "step": 2184,
      "training_loss": 8.104308128356934
    },
    {
      "epoch": 0.11842818428184282,
      "step": 2185,
      "training_loss": 6.652906894683838
    },
    {
      "epoch": 0.11848238482384824,
      "step": 2186,
      "training_loss": 6.879006385803223
    },
    {
      "epoch": 0.11853658536585365,
      "step": 2187,
      "training_loss": 6.587248802185059
    },
    {
      "epoch": 0.11859078590785908,
      "grad_norm": 18.70789337158203,
      "learning_rate": 1e-05,
      "loss": 7.0559,
      "step": 2188
    },
    {
      "epoch": 0.11859078590785908,
      "step": 2188,
      "training_loss": 6.450517654418945
    },
    {
      "epoch": 0.1186449864498645,
      "step": 2189,
      "training_loss": 5.038748741149902
    },
    {
      "epoch": 0.11869918699186992,
      "step": 2190,
      "training_loss": 8.775609016418457
    },
    {
      "epoch": 0.11875338753387533,
      "step": 2191,
      "training_loss": 6.797126293182373
    },
    {
      "epoch": 0.11880758807588077,
      "grad_norm": 18.439281463623047,
      "learning_rate": 1e-05,
      "loss": 6.7655,
      "step": 2192
    },
    {
      "epoch": 0.11880758807588077,
      "step": 2192,
      "training_loss": 6.416903495788574
    },
    {
      "epoch": 0.11886178861788618,
      "step": 2193,
      "training_loss": 5.5691962242126465
    },
    {
      "epoch": 0.1189159891598916,
      "step": 2194,
      "training_loss": 6.2019944190979
    },
    {
      "epoch": 0.11897018970189702,
      "step": 2195,
      "training_loss": 6.4698991775512695
    },
    {
      "epoch": 0.11902439024390243,
      "grad_norm": 22.409799575805664,
      "learning_rate": 1e-05,
      "loss": 6.1645,
      "step": 2196
    },
    {
      "epoch": 0.11902439024390243,
      "step": 2196,
      "training_loss": 6.789050579071045
    },
    {
      "epoch": 0.11907859078590786,
      "step": 2197,
      "training_loss": 6.300307273864746
    },
    {
      "epoch": 0.11913279132791328,
      "step": 2198,
      "training_loss": 5.709183692932129
    },
    {
      "epoch": 0.1191869918699187,
      "step": 2199,
      "training_loss": 7.36451530456543
    },
    {
      "epoch": 0.11924119241192412,
      "grad_norm": 11.4341402053833,
      "learning_rate": 1e-05,
      "loss": 6.5408,
      "step": 2200
    },
    {
      "epoch": 0.11924119241192412,
      "step": 2200,
      "training_loss": 4.954929828643799
    },
    {
      "epoch": 0.11929539295392953,
      "step": 2201,
      "training_loss": 5.926755428314209
    },
    {
      "epoch": 0.11934959349593496,
      "step": 2202,
      "training_loss": 7.742156982421875
    },
    {
      "epoch": 0.11940379403794038,
      "step": 2203,
      "training_loss": 7.57988166809082
    },
    {
      "epoch": 0.1194579945799458,
      "grad_norm": 28.034393310546875,
      "learning_rate": 1e-05,
      "loss": 6.5509,
      "step": 2204
    },
    {
      "epoch": 0.1194579945799458,
      "step": 2204,
      "training_loss": 8.24657917022705
    },
    {
      "epoch": 0.11951219512195121,
      "step": 2205,
      "training_loss": 6.572117805480957
    },
    {
      "epoch": 0.11956639566395665,
      "step": 2206,
      "training_loss": 7.496987819671631
    },
    {
      "epoch": 0.11962059620596206,
      "step": 2207,
      "training_loss": 7.6184000968933105
    },
    {
      "epoch": 0.11967479674796748,
      "grad_norm": 31.050830841064453,
      "learning_rate": 1e-05,
      "loss": 7.4835,
      "step": 2208
    },
    {
      "epoch": 0.11967479674796748,
      "step": 2208,
      "training_loss": 4.887528419494629
    },
    {
      "epoch": 0.1197289972899729,
      "step": 2209,
      "training_loss": 6.160145282745361
    },
    {
      "epoch": 0.11978319783197831,
      "step": 2210,
      "training_loss": 7.176706790924072
    },
    {
      "epoch": 0.11983739837398374,
      "step": 2211,
      "training_loss": 6.819027423858643
    },
    {
      "epoch": 0.11989159891598916,
      "grad_norm": 19.507844924926758,
      "learning_rate": 1e-05,
      "loss": 6.2609,
      "step": 2212
    },
    {
      "epoch": 0.11989159891598916,
      "step": 2212,
      "training_loss": 7.826619625091553
    },
    {
      "epoch": 0.11994579945799458,
      "step": 2213,
      "training_loss": 7.772364616394043
    },
    {
      "epoch": 0.12,
      "step": 2214,
      "training_loss": 5.403945446014404
    },
    {
      "epoch": 0.12005420054200543,
      "step": 2215,
      "training_loss": 8.485770225524902
    },
    {
      "epoch": 0.12010840108401084,
      "grad_norm": 32.67414093017578,
      "learning_rate": 1e-05,
      "loss": 7.3722,
      "step": 2216
    },
    {
      "epoch": 0.12010840108401084,
      "step": 2216,
      "training_loss": 7.541272163391113
    },
    {
      "epoch": 0.12016260162601626,
      "step": 2217,
      "training_loss": 7.084115028381348
    },
    {
      "epoch": 0.12021680216802168,
      "step": 2218,
      "training_loss": 6.98788595199585
    },
    {
      "epoch": 0.1202710027100271,
      "step": 2219,
      "training_loss": 5.689939975738525
    },
    {
      "epoch": 0.12032520325203253,
      "grad_norm": 33.34212875366211,
      "learning_rate": 1e-05,
      "loss": 6.8258,
      "step": 2220
    },
    {
      "epoch": 0.12032520325203253,
      "step": 2220,
      "training_loss": 7.090573787689209
    },
    {
      "epoch": 0.12037940379403794,
      "step": 2221,
      "training_loss": 5.047291278839111
    },
    {
      "epoch": 0.12043360433604336,
      "step": 2222,
      "training_loss": 6.384059429168701
    },
    {
      "epoch": 0.12048780487804878,
      "step": 2223,
      "training_loss": 7.2189717292785645
    },
    {
      "epoch": 0.12054200542005421,
      "grad_norm": 18.934520721435547,
      "learning_rate": 1e-05,
      "loss": 6.4352,
      "step": 2224
    },
    {
      "epoch": 0.12054200542005421,
      "step": 2224,
      "training_loss": 6.766025066375732
    },
    {
      "epoch": 0.12059620596205962,
      "step": 2225,
      "training_loss": 7.633221626281738
    },
    {
      "epoch": 0.12065040650406504,
      "step": 2226,
      "training_loss": 7.633810997009277
    },
    {
      "epoch": 0.12070460704607046,
      "step": 2227,
      "training_loss": 7.482257843017578
    },
    {
      "epoch": 0.12075880758807588,
      "grad_norm": 23.372957229614258,
      "learning_rate": 1e-05,
      "loss": 7.3788,
      "step": 2228
    },
    {
      "epoch": 0.12075880758807588,
      "step": 2228,
      "training_loss": 6.6373291015625
    },
    {
      "epoch": 0.1208130081300813,
      "step": 2229,
      "training_loss": 5.038592338562012
    },
    {
      "epoch": 0.12086720867208672,
      "step": 2230,
      "training_loss": 8.039042472839355
    },
    {
      "epoch": 0.12092140921409214,
      "step": 2231,
      "training_loss": 6.464637756347656
    },
    {
      "epoch": 0.12097560975609756,
      "grad_norm": 33.631282806396484,
      "learning_rate": 1e-05,
      "loss": 6.5449,
      "step": 2232
    },
    {
      "epoch": 0.12097560975609756,
      "step": 2232,
      "training_loss": 7.694478988647461
    },
    {
      "epoch": 0.12102981029810297,
      "step": 2233,
      "training_loss": 6.953336238861084
    },
    {
      "epoch": 0.1210840108401084,
      "step": 2234,
      "training_loss": 6.987929821014404
    },
    {
      "epoch": 0.12113821138211382,
      "step": 2235,
      "training_loss": 7.201374053955078
    },
    {
      "epoch": 0.12119241192411924,
      "grad_norm": 24.31184959411621,
      "learning_rate": 1e-05,
      "loss": 7.2093,
      "step": 2236
    },
    {
      "epoch": 0.12119241192411924,
      "step": 2236,
      "training_loss": 6.796027660369873
    },
    {
      "epoch": 0.12124661246612466,
      "step": 2237,
      "training_loss": 7.678626537322998
    },
    {
      "epoch": 0.12130081300813009,
      "step": 2238,
      "training_loss": 6.350281715393066
    },
    {
      "epoch": 0.1213550135501355,
      "step": 2239,
      "training_loss": 7.147307872772217
    },
    {
      "epoch": 0.12140921409214092,
      "grad_norm": 18.23291015625,
      "learning_rate": 1e-05,
      "loss": 6.9931,
      "step": 2240
    },
    {
      "epoch": 0.12140921409214092,
      "step": 2240,
      "training_loss": 7.355422496795654
    },
    {
      "epoch": 0.12146341463414634,
      "step": 2241,
      "training_loss": 6.673851490020752
    },
    {
      "epoch": 0.12151761517615176,
      "step": 2242,
      "training_loss": 6.4347920417785645
    },
    {
      "epoch": 0.12157181571815719,
      "step": 2243,
      "training_loss": 6.456027984619141
    },
    {
      "epoch": 0.1216260162601626,
      "grad_norm": 16.660287857055664,
      "learning_rate": 1e-05,
      "loss": 6.73,
      "step": 2244
    },
    {
      "epoch": 0.1216260162601626,
      "step": 2244,
      "training_loss": 7.085278034210205
    },
    {
      "epoch": 0.12168021680216802,
      "step": 2245,
      "training_loss": 6.997116565704346
    },
    {
      "epoch": 0.12173441734417344,
      "step": 2246,
      "training_loss": 6.541682720184326
    },
    {
      "epoch": 0.12178861788617887,
      "step": 2247,
      "training_loss": 5.74325704574585
    },
    {
      "epoch": 0.12184281842818429,
      "grad_norm": 14.664073944091797,
      "learning_rate": 1e-05,
      "loss": 6.5918,
      "step": 2248
    },
    {
      "epoch": 0.12184281842818429,
      "step": 2248,
      "training_loss": 4.703540802001953
    },
    {
      "epoch": 0.1218970189701897,
      "step": 2249,
      "training_loss": 5.0067267417907715
    },
    {
      "epoch": 0.12195121951219512,
      "step": 2250,
      "training_loss": 5.738106727600098
    },
    {
      "epoch": 0.12200542005420054,
      "step": 2251,
      "training_loss": 7.200331211090088
    },
    {
      "epoch": 0.12205962059620597,
      "grad_norm": 19.849672317504883,
      "learning_rate": 1e-05,
      "loss": 5.6622,
      "step": 2252
    },
    {
      "epoch": 0.12205962059620597,
      "step": 2252,
      "training_loss": 7.078901767730713
    },
    {
      "epoch": 0.12211382113821138,
      "step": 2253,
      "training_loss": 7.500701427459717
    },
    {
      "epoch": 0.1221680216802168,
      "step": 2254,
      "training_loss": 5.436148643493652
    },
    {
      "epoch": 0.12222222222222222,
      "step": 2255,
      "training_loss": 7.110401153564453
    },
    {
      "epoch": 0.12227642276422765,
      "grad_norm": 29.79174041748047,
      "learning_rate": 1e-05,
      "loss": 6.7815,
      "step": 2256
    },
    {
      "epoch": 0.12227642276422765,
      "step": 2256,
      "training_loss": 7.0023369789123535
    },
    {
      "epoch": 0.12233062330623307,
      "step": 2257,
      "training_loss": 7.499988555908203
    },
    {
      "epoch": 0.12238482384823848,
      "step": 2258,
      "training_loss": 7.7180986404418945
    },
    {
      "epoch": 0.1224390243902439,
      "step": 2259,
      "training_loss": 7.783355236053467
    },
    {
      "epoch": 0.12249322493224932,
      "grad_norm": 21.479135513305664,
      "learning_rate": 1e-05,
      "loss": 7.5009,
      "step": 2260
    },
    {
      "epoch": 0.12249322493224932,
      "step": 2260,
      "training_loss": 7.4851884841918945
    },
    {
      "epoch": 0.12254742547425475,
      "step": 2261,
      "training_loss": 6.882392883300781
    },
    {
      "epoch": 0.12260162601626017,
      "step": 2262,
      "training_loss": 6.7331366539001465
    },
    {
      "epoch": 0.12265582655826558,
      "step": 2263,
      "training_loss": 9.494770050048828
    },
    {
      "epoch": 0.122710027100271,
      "grad_norm": 46.98485565185547,
      "learning_rate": 1e-05,
      "loss": 7.6489,
      "step": 2264
    },
    {
      "epoch": 0.122710027100271,
      "step": 2264,
      "training_loss": 7.01367712020874
    },
    {
      "epoch": 0.12276422764227642,
      "step": 2265,
      "training_loss": 6.715948104858398
    },
    {
      "epoch": 0.12281842818428185,
      "step": 2266,
      "training_loss": 6.553761005401611
    },
    {
      "epoch": 0.12287262872628726,
      "step": 2267,
      "training_loss": 4.769719123840332
    },
    {
      "epoch": 0.12292682926829268,
      "grad_norm": 17.23247528076172,
      "learning_rate": 1e-05,
      "loss": 6.2633,
      "step": 2268
    },
    {
      "epoch": 0.12292682926829268,
      "step": 2268,
      "training_loss": 6.697353363037109
    },
    {
      "epoch": 0.1229810298102981,
      "step": 2269,
      "training_loss": 4.626371383666992
    },
    {
      "epoch": 0.12303523035230353,
      "step": 2270,
      "training_loss": 7.181683540344238
    },
    {
      "epoch": 0.12308943089430895,
      "step": 2271,
      "training_loss": 8.144038200378418
    },
    {
      "epoch": 0.12314363143631436,
      "grad_norm": 20.310401916503906,
      "learning_rate": 1e-05,
      "loss": 6.6624,
      "step": 2272
    },
    {
      "epoch": 0.12314363143631436,
      "step": 2272,
      "training_loss": 7.420018672943115
    },
    {
      "epoch": 0.12319783197831978,
      "step": 2273,
      "training_loss": 7.878206729888916
    },
    {
      "epoch": 0.1232520325203252,
      "step": 2274,
      "training_loss": 6.313616752624512
    },
    {
      "epoch": 0.12330623306233063,
      "step": 2275,
      "training_loss": 7.545258522033691
    },
    {
      "epoch": 0.12336043360433604,
      "grad_norm": 32.66688919067383,
      "learning_rate": 1e-05,
      "loss": 7.2893,
      "step": 2276
    },
    {
      "epoch": 0.12336043360433604,
      "step": 2276,
      "training_loss": 7.857612609863281
    },
    {
      "epoch": 0.12341463414634146,
      "step": 2277,
      "training_loss": 6.832972526550293
    },
    {
      "epoch": 0.12346883468834688,
      "step": 2278,
      "training_loss": 7.7751851081848145
    },
    {
      "epoch": 0.12352303523035231,
      "step": 2279,
      "training_loss": 7.564754486083984
    },
    {
      "epoch": 0.12357723577235773,
      "grad_norm": 21.442054748535156,
      "learning_rate": 1e-05,
      "loss": 7.5076,
      "step": 2280
    },
    {
      "epoch": 0.12357723577235773,
      "step": 2280,
      "training_loss": 8.066852569580078
    },
    {
      "epoch": 0.12363143631436314,
      "step": 2281,
      "training_loss": 6.586187839508057
    },
    {
      "epoch": 0.12368563685636856,
      "step": 2282,
      "training_loss": 7.1680521965026855
    },
    {
      "epoch": 0.12373983739837398,
      "step": 2283,
      "training_loss": 4.69591760635376
    },
    {
      "epoch": 0.12379403794037941,
      "grad_norm": 18.26604461669922,
      "learning_rate": 1e-05,
      "loss": 6.6293,
      "step": 2284
    },
    {
      "epoch": 0.12379403794037941,
      "step": 2284,
      "training_loss": 6.899262428283691
    },
    {
      "epoch": 0.12384823848238483,
      "step": 2285,
      "training_loss": 8.297028541564941
    },
    {
      "epoch": 0.12390243902439024,
      "step": 2286,
      "training_loss": 6.748493671417236
    },
    {
      "epoch": 0.12395663956639566,
      "step": 2287,
      "training_loss": 5.633950710296631
    },
    {
      "epoch": 0.12401084010840109,
      "grad_norm": 21.80858039855957,
      "learning_rate": 1e-05,
      "loss": 6.8947,
      "step": 2288
    },
    {
      "epoch": 0.12401084010840109,
      "step": 2288,
      "training_loss": 7.935492992401123
    },
    {
      "epoch": 0.12406504065040651,
      "step": 2289,
      "training_loss": 6.318075656890869
    },
    {
      "epoch": 0.12411924119241192,
      "step": 2290,
      "training_loss": 8.735027313232422
    },
    {
      "epoch": 0.12417344173441734,
      "step": 2291,
      "training_loss": 7.181682109832764
    },
    {
      "epoch": 0.12422764227642276,
      "grad_norm": 25.777793884277344,
      "learning_rate": 1e-05,
      "loss": 7.5426,
      "step": 2292
    },
    {
      "epoch": 0.12422764227642276,
      "step": 2292,
      "training_loss": 6.749355792999268
    },
    {
      "epoch": 0.12428184281842819,
      "step": 2293,
      "training_loss": 7.916969299316406
    },
    {
      "epoch": 0.1243360433604336,
      "step": 2294,
      "training_loss": 7.639201641082764
    },
    {
      "epoch": 0.12439024390243902,
      "step": 2295,
      "training_loss": 5.085854530334473
    },
    {
      "epoch": 0.12444444444444444,
      "grad_norm": 23.33244514465332,
      "learning_rate": 1e-05,
      "loss": 6.8478,
      "step": 2296
    },
    {
      "epoch": 0.12444444444444444,
      "step": 2296,
      "training_loss": 7.673794269561768
    },
    {
      "epoch": 0.12449864498644986,
      "step": 2297,
      "training_loss": 7.263751983642578
    },
    {
      "epoch": 0.12455284552845529,
      "step": 2298,
      "training_loss": 7.893367290496826
    },
    {
      "epoch": 0.1246070460704607,
      "step": 2299,
      "training_loss": 6.223792552947998
    },
    {
      "epoch": 0.12466124661246612,
      "grad_norm": 26.077638626098633,
      "learning_rate": 1e-05,
      "loss": 7.2637,
      "step": 2300
    },
    {
      "epoch": 0.12466124661246612,
      "step": 2300,
      "training_loss": 7.192048072814941
    },
    {
      "epoch": 0.12471544715447154,
      "step": 2301,
      "training_loss": 5.456752300262451
    },
    {
      "epoch": 0.12476964769647697,
      "step": 2302,
      "training_loss": 7.792191028594971
    },
    {
      "epoch": 0.12482384823848239,
      "step": 2303,
      "training_loss": 6.6484270095825195
    },
    {
      "epoch": 0.1248780487804878,
      "grad_norm": 33.54572296142578,
      "learning_rate": 1e-05,
      "loss": 6.7724,
      "step": 2304
    },
    {
      "epoch": 0.1248780487804878,
      "step": 2304,
      "training_loss": 5.881302833557129
    },
    {
      "epoch": 0.12493224932249322,
      "step": 2305,
      "training_loss": 6.88964319229126
    },
    {
      "epoch": 0.12498644986449864,
      "step": 2306,
      "training_loss": 6.558231353759766
    },
    {
      "epoch": 0.12504065040650406,
      "step": 2307,
      "training_loss": 5.7404398918151855
    },
    {
      "epoch": 0.12509485094850947,
      "grad_norm": 16.859508514404297,
      "learning_rate": 1e-05,
      "loss": 6.2674,
      "step": 2308
    },
    {
      "epoch": 0.12509485094850947,
      "step": 2308,
      "training_loss": 5.9354681968688965
    },
    {
      "epoch": 0.12514905149051492,
      "step": 2309,
      "training_loss": 7.143780708312988
    },
    {
      "epoch": 0.12520325203252033,
      "step": 2310,
      "training_loss": 6.716307640075684
    },
    {
      "epoch": 0.12525745257452575,
      "step": 2311,
      "training_loss": 6.3847808837890625
    },
    {
      "epoch": 0.12531165311653117,
      "grad_norm": 15.5435152053833,
      "learning_rate": 1e-05,
      "loss": 6.5451,
      "step": 2312
    },
    {
      "epoch": 0.12531165311653117,
      "step": 2312,
      "training_loss": 5.398257732391357
    },
    {
      "epoch": 0.12536585365853659,
      "step": 2313,
      "training_loss": 6.784984111785889
    },
    {
      "epoch": 0.125420054200542,
      "step": 2314,
      "training_loss": 7.342296123504639
    },
    {
      "epoch": 0.12547425474254742,
      "step": 2315,
      "training_loss": 5.805182933807373
    },
    {
      "epoch": 0.12552845528455284,
      "grad_norm": 24.448890686035156,
      "learning_rate": 1e-05,
      "loss": 6.3327,
      "step": 2316
    },
    {
      "epoch": 0.12552845528455284,
      "step": 2316,
      "training_loss": 6.260063171386719
    },
    {
      "epoch": 0.12558265582655825,
      "step": 2317,
      "training_loss": 7.237641334533691
    },
    {
      "epoch": 0.1256368563685637,
      "step": 2318,
      "training_loss": 7.885072708129883
    },
    {
      "epoch": 0.12569105691056912,
      "step": 2319,
      "training_loss": 6.006633281707764
    },
    {
      "epoch": 0.12574525745257453,
      "grad_norm": 14.371999740600586,
      "learning_rate": 1e-05,
      "loss": 6.8474,
      "step": 2320
    },
    {
      "epoch": 0.12574525745257453,
      "step": 2320,
      "training_loss": 7.109259128570557
    },
    {
      "epoch": 0.12579945799457995,
      "step": 2321,
      "training_loss": 6.004148960113525
    },
    {
      "epoch": 0.12585365853658537,
      "step": 2322,
      "training_loss": 6.886720657348633
    },
    {
      "epoch": 0.12590785907859078,
      "step": 2323,
      "training_loss": 8.63757038116455
    },
    {
      "epoch": 0.1259620596205962,
      "grad_norm": 42.692726135253906,
      "learning_rate": 1e-05,
      "loss": 7.1594,
      "step": 2324
    },
    {
      "epoch": 0.1259620596205962,
      "step": 2324,
      "training_loss": 9.097541809082031
    },
    {
      "epoch": 0.12601626016260162,
      "step": 2325,
      "training_loss": 6.228115558624268
    },
    {
      "epoch": 0.12607046070460703,
      "step": 2326,
      "training_loss": 6.93276309967041
    },
    {
      "epoch": 0.12612466124661248,
      "step": 2327,
      "training_loss": 7.2412428855896
    },
    {
      "epoch": 0.1261788617886179,
      "grad_norm": 18.923810958862305,
      "learning_rate": 1e-05,
      "loss": 7.3749,
      "step": 2328
    },
    {
      "epoch": 0.1261788617886179,
      "step": 2328,
      "training_loss": 7.3738603591918945
    },
    {
      "epoch": 0.1262330623306233,
      "step": 2329,
      "training_loss": 6.653367042541504
    },
    {
      "epoch": 0.12628726287262873,
      "step": 2330,
      "training_loss": 7.742931842803955
    },
    {
      "epoch": 0.12634146341463415,
      "step": 2331,
      "training_loss": 7.738585472106934
    },
    {
      "epoch": 0.12639566395663956,
      "grad_norm": 20.050537109375,
      "learning_rate": 1e-05,
      "loss": 7.3772,
      "step": 2332
    },
    {
      "epoch": 0.12639566395663956,
      "step": 2332,
      "training_loss": 7.737110137939453
    },
    {
      "epoch": 0.12644986449864498,
      "step": 2333,
      "training_loss": 8.401562690734863
    },
    {
      "epoch": 0.1265040650406504,
      "step": 2334,
      "training_loss": 6.940257549285889
    },
    {
      "epoch": 0.12655826558265582,
      "step": 2335,
      "training_loss": 5.1582841873168945
    },
    {
      "epoch": 0.12661246612466126,
      "grad_norm": 25.309974670410156,
      "learning_rate": 1e-05,
      "loss": 7.0593,
      "step": 2336
    },
    {
      "epoch": 0.12661246612466126,
      "step": 2336,
      "training_loss": 6.9512810707092285
    },
    {
      "epoch": 0.12666666666666668,
      "step": 2337,
      "training_loss": 7.266663074493408
    },
    {
      "epoch": 0.1267208672086721,
      "step": 2338,
      "training_loss": 7.459133148193359
    },
    {
      "epoch": 0.1267750677506775,
      "step": 2339,
      "training_loss": 7.0086774826049805
    },
    {
      "epoch": 0.12682926829268293,
      "grad_norm": 23.231477737426758,
      "learning_rate": 1e-05,
      "loss": 7.1714,
      "step": 2340
    },
    {
      "epoch": 0.12682926829268293,
      "step": 2340,
      "training_loss": 6.045403957366943
    },
    {
      "epoch": 0.12688346883468835,
      "step": 2341,
      "training_loss": 7.154129981994629
    },
    {
      "epoch": 0.12693766937669376,
      "step": 2342,
      "training_loss": 7.1170148849487305
    },
    {
      "epoch": 0.12699186991869918,
      "step": 2343,
      "training_loss": 8.479242324829102
    },
    {
      "epoch": 0.1270460704607046,
      "grad_norm": 30.86034393310547,
      "learning_rate": 1e-05,
      "loss": 7.1989,
      "step": 2344
    },
    {
      "epoch": 0.1270460704607046,
      "step": 2344,
      "training_loss": 6.553088665008545
    },
    {
      "epoch": 0.12710027100271,
      "step": 2345,
      "training_loss": 6.300509929656982
    },
    {
      "epoch": 0.12715447154471546,
      "step": 2346,
      "training_loss": 8.175697326660156
    },
    {
      "epoch": 0.12720867208672088,
      "step": 2347,
      "training_loss": 7.6241679191589355
    },
    {
      "epoch": 0.1272628726287263,
      "grad_norm": 19.317899703979492,
      "learning_rate": 1e-05,
      "loss": 7.1634,
      "step": 2348
    },
    {
      "epoch": 0.1272628726287263,
      "step": 2348,
      "training_loss": 5.1277031898498535
    },
    {
      "epoch": 0.1273170731707317,
      "step": 2349,
      "training_loss": 7.588439464569092
    },
    {
      "epoch": 0.12737127371273713,
      "step": 2350,
      "training_loss": 7.3179473876953125
    },
    {
      "epoch": 0.12742547425474254,
      "step": 2351,
      "training_loss": 6.514327049255371
    },
    {
      "epoch": 0.12747967479674796,
      "grad_norm": 27.796995162963867,
      "learning_rate": 1e-05,
      "loss": 6.6371,
      "step": 2352
    },
    {
      "epoch": 0.12747967479674796,
      "step": 2352,
      "training_loss": 6.474440097808838
    },
    {
      "epoch": 0.12753387533875338,
      "step": 2353,
      "training_loss": 5.062373161315918
    },
    {
      "epoch": 0.1275880758807588,
      "step": 2354,
      "training_loss": 6.981166362762451
    },
    {
      "epoch": 0.12764227642276424,
      "step": 2355,
      "training_loss": 7.293150901794434
    },
    {
      "epoch": 0.12769647696476966,
      "grad_norm": 23.094104766845703,
      "learning_rate": 1e-05,
      "loss": 6.4528,
      "step": 2356
    },
    {
      "epoch": 0.12769647696476966,
      "step": 2356,
      "training_loss": 6.672297477722168
    },
    {
      "epoch": 0.12775067750677507,
      "step": 2357,
      "training_loss": 7.342505931854248
    },
    {
      "epoch": 0.1278048780487805,
      "step": 2358,
      "training_loss": 7.209371089935303
    },
    {
      "epoch": 0.1278590785907859,
      "step": 2359,
      "training_loss": 7.44959831237793
    },
    {
      "epoch": 0.12791327913279132,
      "grad_norm": 15.741528511047363,
      "learning_rate": 1e-05,
      "loss": 7.1684,
      "step": 2360
    },
    {
      "epoch": 0.12791327913279132,
      "step": 2360,
      "training_loss": 7.147550582885742
    },
    {
      "epoch": 0.12796747967479674,
      "step": 2361,
      "training_loss": 9.249164581298828
    },
    {
      "epoch": 0.12802168021680216,
      "step": 2362,
      "training_loss": 7.309779167175293
    },
    {
      "epoch": 0.12807588075880758,
      "step": 2363,
      "training_loss": 7.205150127410889
    },
    {
      "epoch": 0.12813008130081302,
      "grad_norm": 16.930511474609375,
      "learning_rate": 1e-05,
      "loss": 7.7279,
      "step": 2364
    },
    {
      "epoch": 0.12813008130081302,
      "step": 2364,
      "training_loss": 6.0998759269714355
    },
    {
      "epoch": 0.12818428184281844,
      "step": 2365,
      "training_loss": 8.268562316894531
    },
    {
      "epoch": 0.12823848238482385,
      "step": 2366,
      "training_loss": 8.989215850830078
    },
    {
      "epoch": 0.12829268292682927,
      "step": 2367,
      "training_loss": 7.231636047363281
    },
    {
      "epoch": 0.1283468834688347,
      "grad_norm": 19.490684509277344,
      "learning_rate": 1e-05,
      "loss": 7.6473,
      "step": 2368
    },
    {
      "epoch": 0.1283468834688347,
      "step": 2368,
      "training_loss": 6.891519546508789
    },
    {
      "epoch": 0.1284010840108401,
      "step": 2369,
      "training_loss": 7.0190229415893555
    },
    {
      "epoch": 0.12845528455284552,
      "step": 2370,
      "training_loss": 5.833025932312012
    },
    {
      "epoch": 0.12850948509485094,
      "step": 2371,
      "training_loss": 7.318814754486084
    },
    {
      "epoch": 0.12856368563685636,
      "grad_norm": 19.696821212768555,
      "learning_rate": 1e-05,
      "loss": 6.7656,
      "step": 2372
    },
    {
      "epoch": 0.12856368563685636,
      "step": 2372,
      "training_loss": 7.996345520019531
    },
    {
      "epoch": 0.1286178861788618,
      "step": 2373,
      "training_loss": 7.4722065925598145
    },
    {
      "epoch": 0.12867208672086722,
      "step": 2374,
      "training_loss": 7.2403950691223145
    },
    {
      "epoch": 0.12872628726287264,
      "step": 2375,
      "training_loss": 6.946951866149902
    },
    {
      "epoch": 0.12878048780487805,
      "grad_norm": 18.996192932128906,
      "learning_rate": 1e-05,
      "loss": 7.414,
      "step": 2376
    },
    {
      "epoch": 0.12878048780487805,
      "step": 2376,
      "training_loss": 6.9443559646606445
    },
    {
      "epoch": 0.12883468834688347,
      "step": 2377,
      "training_loss": 10.435005187988281
    },
    {
      "epoch": 0.1288888888888889,
      "step": 2378,
      "training_loss": 8.213976860046387
    },
    {
      "epoch": 0.1289430894308943,
      "step": 2379,
      "training_loss": 7.399564743041992
    },
    {
      "epoch": 0.12899728997289972,
      "grad_norm": 18.470012664794922,
      "learning_rate": 1e-05,
      "loss": 8.2482,
      "step": 2380
    },
    {
      "epoch": 0.12899728997289972,
      "step": 2380,
      "training_loss": 6.8458147048950195
    },
    {
      "epoch": 0.12905149051490514,
      "step": 2381,
      "training_loss": 7.100912570953369
    },
    {
      "epoch": 0.12910569105691058,
      "step": 2382,
      "training_loss": 8.331835746765137
    },
    {
      "epoch": 0.129159891598916,
      "step": 2383,
      "training_loss": 7.173605442047119
    },
    {
      "epoch": 0.12921409214092142,
      "grad_norm": 17.97764778137207,
      "learning_rate": 1e-05,
      "loss": 7.363,
      "step": 2384
    },
    {
      "epoch": 0.12921409214092142,
      "step": 2384,
      "training_loss": 8.817522048950195
    },
    {
      "epoch": 0.12926829268292683,
      "step": 2385,
      "training_loss": 7.092715263366699
    },
    {
      "epoch": 0.12932249322493225,
      "step": 2386,
      "training_loss": 5.736758232116699
    },
    {
      "epoch": 0.12937669376693767,
      "step": 2387,
      "training_loss": 7.840070724487305
    },
    {
      "epoch": 0.12943089430894308,
      "grad_norm": 16.82536506652832,
      "learning_rate": 1e-05,
      "loss": 7.3718,
      "step": 2388
    },
    {
      "epoch": 0.12943089430894308,
      "step": 2388,
      "training_loss": 6.460188865661621
    },
    {
      "epoch": 0.1294850948509485,
      "step": 2389,
      "training_loss": 7.762294292449951
    },
    {
      "epoch": 0.12953929539295392,
      "step": 2390,
      "training_loss": 7.433584213256836
    },
    {
      "epoch": 0.12959349593495936,
      "step": 2391,
      "training_loss": 6.45660400390625
    },
    {
      "epoch": 0.12964769647696478,
      "grad_norm": 16.68767738342285,
      "learning_rate": 1e-05,
      "loss": 7.0282,
      "step": 2392
    },
    {
      "epoch": 0.12964769647696478,
      "step": 2392,
      "training_loss": 5.8159308433532715
    },
    {
      "epoch": 0.1297018970189702,
      "step": 2393,
      "training_loss": 7.891869068145752
    },
    {
      "epoch": 0.12975609756097561,
      "step": 2394,
      "training_loss": 6.140820503234863
    },
    {
      "epoch": 0.12981029810298103,
      "step": 2395,
      "training_loss": 6.949145317077637
    },
    {
      "epoch": 0.12986449864498645,
      "grad_norm": 24.94117546081543,
      "learning_rate": 1e-05,
      "loss": 6.6994,
      "step": 2396
    },
    {
      "epoch": 0.12986449864498645,
      "step": 2396,
      "training_loss": 6.84550142288208
    },
    {
      "epoch": 0.12991869918699187,
      "step": 2397,
      "training_loss": 6.777259349822998
    },
    {
      "epoch": 0.12997289972899728,
      "step": 2398,
      "training_loss": 7.136823654174805
    },
    {
      "epoch": 0.1300271002710027,
      "step": 2399,
      "training_loss": 8.293983459472656
    },
    {
      "epoch": 0.13008130081300814,
      "grad_norm": 22.999238967895508,
      "learning_rate": 1e-05,
      "loss": 7.2634,
      "step": 2400
    },
    {
      "epoch": 0.13008130081300814,
      "step": 2400,
      "training_loss": 6.7133636474609375
    },
    {
      "epoch": 0.13013550135501356,
      "step": 2401,
      "training_loss": 7.599674224853516
    },
    {
      "epoch": 0.13018970189701898,
      "step": 2402,
      "training_loss": 7.431965351104736
    },
    {
      "epoch": 0.1302439024390244,
      "step": 2403,
      "training_loss": 6.959190845489502
    },
    {
      "epoch": 0.1302981029810298,
      "grad_norm": 17.997512817382812,
      "learning_rate": 1e-05,
      "loss": 7.176,
      "step": 2404
    },
    {
      "epoch": 0.1302981029810298,
      "step": 2404,
      "training_loss": 7.372251033782959
    },
    {
      "epoch": 0.13035230352303523,
      "step": 2405,
      "training_loss": 6.6398234367370605
    },
    {
      "epoch": 0.13040650406504065,
      "step": 2406,
      "training_loss": 7.129350662231445
    },
    {
      "epoch": 0.13046070460704606,
      "step": 2407,
      "training_loss": 6.397891044616699
    },
    {
      "epoch": 0.13051490514905148,
      "grad_norm": 33.74754333496094,
      "learning_rate": 1e-05,
      "loss": 6.8848,
      "step": 2408
    },
    {
      "epoch": 0.13051490514905148,
      "step": 2408,
      "training_loss": 5.21405029296875
    },
    {
      "epoch": 0.1305691056910569,
      "step": 2409,
      "training_loss": 6.430717468261719
    },
    {
      "epoch": 0.13062330623306234,
      "step": 2410,
      "training_loss": 5.143457889556885
    },
    {
      "epoch": 0.13067750677506776,
      "step": 2411,
      "training_loss": 7.825999736785889
    },
    {
      "epoch": 0.13073170731707318,
      "grad_norm": 24.811138153076172,
      "learning_rate": 1e-05,
      "loss": 6.1536,
      "step": 2412
    },
    {
      "epoch": 0.13073170731707318,
      "step": 2412,
      "training_loss": 8.10484504699707
    },
    {
      "epoch": 0.1307859078590786,
      "step": 2413,
      "training_loss": 6.983096122741699
    },
    {
      "epoch": 0.130840108401084,
      "step": 2414,
      "training_loss": 5.724494934082031
    },
    {
      "epoch": 0.13089430894308943,
      "step": 2415,
      "training_loss": 6.397602081298828
    },
    {
      "epoch": 0.13094850948509484,
      "grad_norm": 16.252355575561523,
      "learning_rate": 1e-05,
      "loss": 6.8025,
      "step": 2416
    },
    {
      "epoch": 0.13094850948509484,
      "step": 2416,
      "training_loss": 6.791269302368164
    },
    {
      "epoch": 0.13100271002710026,
      "step": 2417,
      "training_loss": 6.114009380340576
    },
    {
      "epoch": 0.13105691056910568,
      "step": 2418,
      "training_loss": 5.585865497589111
    },
    {
      "epoch": 0.13111111111111112,
      "step": 2419,
      "training_loss": 6.7568440437316895
    },
    {
      "epoch": 0.13116531165311654,
      "grad_norm": 17.54572296142578,
      "learning_rate": 1e-05,
      "loss": 6.312,
      "step": 2420
    },
    {
      "epoch": 0.13116531165311654,
      "step": 2420,
      "training_loss": 6.756948947906494
    },
    {
      "epoch": 0.13121951219512196,
      "step": 2421,
      "training_loss": 6.776217460632324
    },
    {
      "epoch": 0.13127371273712737,
      "step": 2422,
      "training_loss": 6.766881465911865
    },
    {
      "epoch": 0.1313279132791328,
      "step": 2423,
      "training_loss": 7.655012130737305
    },
    {
      "epoch": 0.1313821138211382,
      "grad_norm": 18.40644645690918,
      "learning_rate": 1e-05,
      "loss": 6.9888,
      "step": 2424
    },
    {
      "epoch": 0.1313821138211382,
      "step": 2424,
      "training_loss": 7.084962844848633
    },
    {
      "epoch": 0.13143631436314362,
      "step": 2425,
      "training_loss": 6.592479228973389
    },
    {
      "epoch": 0.13149051490514904,
      "step": 2426,
      "training_loss": 6.49119758605957
    },
    {
      "epoch": 0.13154471544715446,
      "step": 2427,
      "training_loss": 6.095357418060303
    },
    {
      "epoch": 0.1315989159891599,
      "grad_norm": 40.02559280395508,
      "learning_rate": 1e-05,
      "loss": 6.566,
      "step": 2428
    },
    {
      "epoch": 0.1315989159891599,
      "step": 2428,
      "training_loss": 7.425017833709717
    },
    {
      "epoch": 0.13165311653116532,
      "step": 2429,
      "training_loss": 7.837277889251709
    },
    {
      "epoch": 0.13170731707317074,
      "step": 2430,
      "training_loss": 7.50784969329834
    },
    {
      "epoch": 0.13176151761517615,
      "step": 2431,
      "training_loss": 7.3861846923828125
    },
    {
      "epoch": 0.13181571815718157,
      "grad_norm": 17.26953887939453,
      "learning_rate": 1e-05,
      "loss": 7.5391,
      "step": 2432
    },
    {
      "epoch": 0.13181571815718157,
      "step": 2432,
      "training_loss": 7.383130073547363
    },
    {
      "epoch": 0.131869918699187,
      "step": 2433,
      "training_loss": 7.492430686950684
    },
    {
      "epoch": 0.1319241192411924,
      "step": 2434,
      "training_loss": 7.4050679206848145
    },
    {
      "epoch": 0.13197831978319782,
      "step": 2435,
      "training_loss": 5.913609027862549
    },
    {
      "epoch": 0.13203252032520324,
      "grad_norm": 22.826536178588867,
      "learning_rate": 1e-05,
      "loss": 7.0486,
      "step": 2436
    },
    {
      "epoch": 0.13203252032520324,
      "step": 2436,
      "training_loss": 7.549715042114258
    },
    {
      "epoch": 0.13208672086720868,
      "step": 2437,
      "training_loss": 8.091358184814453
    },
    {
      "epoch": 0.1321409214092141,
      "step": 2438,
      "training_loss": 8.662674903869629
    },
    {
      "epoch": 0.13219512195121952,
      "step": 2439,
      "training_loss": 7.535335063934326
    },
    {
      "epoch": 0.13224932249322494,
      "grad_norm": 39.024600982666016,
      "learning_rate": 1e-05,
      "loss": 7.9598,
      "step": 2440
    },
    {
      "epoch": 0.13224932249322494,
      "step": 2440,
      "training_loss": 6.582455158233643
    },
    {
      "epoch": 0.13230352303523035,
      "step": 2441,
      "training_loss": 7.434326171875
    },
    {
      "epoch": 0.13235772357723577,
      "step": 2442,
      "training_loss": 6.895413875579834
    },
    {
      "epoch": 0.1324119241192412,
      "step": 2443,
      "training_loss": 6.560115814208984
    },
    {
      "epoch": 0.1324661246612466,
      "grad_norm": 28.078418731689453,
      "learning_rate": 1e-05,
      "loss": 6.8681,
      "step": 2444
    },
    {
      "epoch": 0.1324661246612466,
      "step": 2444,
      "training_loss": 7.337508678436279
    },
    {
      "epoch": 0.13252032520325202,
      "step": 2445,
      "training_loss": 6.848161220550537
    },
    {
      "epoch": 0.13257452574525747,
      "step": 2446,
      "training_loss": 6.6051764488220215
    },
    {
      "epoch": 0.13262872628726288,
      "step": 2447,
      "training_loss": 7.909592151641846
    },
    {
      "epoch": 0.1326829268292683,
      "grad_norm": 29.793598175048828,
      "learning_rate": 1e-05,
      "loss": 7.1751,
      "step": 2448
    },
    {
      "epoch": 0.1326829268292683,
      "step": 2448,
      "training_loss": 7.942841529846191
    },
    {
      "epoch": 0.13273712737127372,
      "step": 2449,
      "training_loss": 7.339183807373047
    },
    {
      "epoch": 0.13279132791327913,
      "step": 2450,
      "training_loss": 7.739979267120361
    },
    {
      "epoch": 0.13284552845528455,
      "step": 2451,
      "training_loss": 7.990419864654541
    },
    {
      "epoch": 0.13289972899728997,
      "grad_norm": 19.519268035888672,
      "learning_rate": 1e-05,
      "loss": 7.7531,
      "step": 2452
    },
    {
      "epoch": 0.13289972899728997,
      "step": 2452,
      "training_loss": 7.545776844024658
    },
    {
      "epoch": 0.13295392953929538,
      "step": 2453,
      "training_loss": 5.994248390197754
    },
    {
      "epoch": 0.1330081300813008,
      "step": 2454,
      "training_loss": 7.444095134735107
    },
    {
      "epoch": 0.13306233062330625,
      "step": 2455,
      "training_loss": 7.267855167388916
    },
    {
      "epoch": 0.13311653116531166,
      "grad_norm": 19.37725067138672,
      "learning_rate": 1e-05,
      "loss": 7.063,
      "step": 2456
    },
    {
      "epoch": 0.13311653116531166,
      "step": 2456,
      "training_loss": 7.487898826599121
    },
    {
      "epoch": 0.13317073170731708,
      "step": 2457,
      "training_loss": 7.750001430511475
    },
    {
      "epoch": 0.1332249322493225,
      "step": 2458,
      "training_loss": 5.965328693389893
    },
    {
      "epoch": 0.13327913279132791,
      "step": 2459,
      "training_loss": 7.28836727142334
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 28.248313903808594,
      "learning_rate": 1e-05,
      "loss": 7.1229,
      "step": 2460
    },
    {
      "epoch": 0.13333333333333333,
      "step": 2460,
      "training_loss": 6.611510753631592
    },
    {
      "epoch": 0.13338753387533875,
      "step": 2461,
      "training_loss": 8.610699653625488
    },
    {
      "epoch": 0.13344173441734417,
      "step": 2462,
      "training_loss": 7.95767068862915
    },
    {
      "epoch": 0.13349593495934958,
      "step": 2463,
      "training_loss": 6.774229526519775
    },
    {
      "epoch": 0.13355013550135503,
      "grad_norm": 19.879915237426758,
      "learning_rate": 1e-05,
      "loss": 7.4885,
      "step": 2464
    },
    {
      "epoch": 0.13355013550135503,
      "step": 2464,
      "training_loss": 7.73317289352417
    },
    {
      "epoch": 0.13360433604336044,
      "step": 2465,
      "training_loss": 7.757315635681152
    },
    {
      "epoch": 0.13365853658536586,
      "step": 2466,
      "training_loss": 7.437290191650391
    },
    {
      "epoch": 0.13371273712737128,
      "step": 2467,
      "training_loss": 7.686046600341797
    },
    {
      "epoch": 0.1337669376693767,
      "grad_norm": 31.695730209350586,
      "learning_rate": 1e-05,
      "loss": 7.6535,
      "step": 2468
    },
    {
      "epoch": 0.1337669376693767,
      "step": 2468,
      "training_loss": 7.093227863311768
    },
    {
      "epoch": 0.1338211382113821,
      "step": 2469,
      "training_loss": 8.419249534606934
    },
    {
      "epoch": 0.13387533875338753,
      "step": 2470,
      "training_loss": 8.040327072143555
    },
    {
      "epoch": 0.13392953929539295,
      "step": 2471,
      "training_loss": 7.246767044067383
    },
    {
      "epoch": 0.13398373983739836,
      "grad_norm": 20.5117130279541,
      "learning_rate": 1e-05,
      "loss": 7.6999,
      "step": 2472
    },
    {
      "epoch": 0.13398373983739836,
      "step": 2472,
      "training_loss": 6.817556381225586
    },
    {
      "epoch": 0.13403794037940378,
      "step": 2473,
      "training_loss": 5.766285419464111
    },
    {
      "epoch": 0.13409214092140923,
      "step": 2474,
      "training_loss": 7.6953816413879395
    },
    {
      "epoch": 0.13414634146341464,
      "step": 2475,
      "training_loss": 6.556519985198975
    },
    {
      "epoch": 0.13420054200542006,
      "grad_norm": 16.53655242919922,
      "learning_rate": 1e-05,
      "loss": 6.7089,
      "step": 2476
    },
    {
      "epoch": 0.13420054200542006,
      "step": 2476,
      "training_loss": 6.717647552490234
    },
    {
      "epoch": 0.13425474254742548,
      "step": 2477,
      "training_loss": 6.436633586883545
    },
    {
      "epoch": 0.1343089430894309,
      "step": 2478,
      "training_loss": 6.870391368865967
    },
    {
      "epoch": 0.1343631436314363,
      "step": 2479,
      "training_loss": 5.539710998535156
    },
    {
      "epoch": 0.13441734417344173,
      "grad_norm": 18.03468894958496,
      "learning_rate": 1e-05,
      "loss": 6.3911,
      "step": 2480
    },
    {
      "epoch": 0.13441734417344173,
      "step": 2480,
      "training_loss": 7.421394348144531
    },
    {
      "epoch": 0.13447154471544714,
      "step": 2481,
      "training_loss": 7.195505142211914
    },
    {
      "epoch": 0.13452574525745256,
      "step": 2482,
      "training_loss": 6.995494842529297
    },
    {
      "epoch": 0.134579945799458,
      "step": 2483,
      "training_loss": 7.791748046875
    },
    {
      "epoch": 0.13463414634146342,
      "grad_norm": 23.69862174987793,
      "learning_rate": 1e-05,
      "loss": 7.351,
      "step": 2484
    },
    {
      "epoch": 0.13463414634146342,
      "step": 2484,
      "training_loss": 7.868696212768555
    },
    {
      "epoch": 0.13468834688346884,
      "step": 2485,
      "training_loss": 7.784933090209961
    },
    {
      "epoch": 0.13474254742547426,
      "step": 2486,
      "training_loss": 4.909270763397217
    },
    {
      "epoch": 0.13479674796747967,
      "step": 2487,
      "training_loss": 5.411971569061279
    },
    {
      "epoch": 0.1348509485094851,
      "grad_norm": 19.184114456176758,
      "learning_rate": 1e-05,
      "loss": 6.4937,
      "step": 2488
    },
    {
      "epoch": 0.1348509485094851,
      "step": 2488,
      "training_loss": 6.562197685241699
    },
    {
      "epoch": 0.1349051490514905,
      "step": 2489,
      "training_loss": 5.837830066680908
    },
    {
      "epoch": 0.13495934959349593,
      "step": 2490,
      "training_loss": 7.1732096672058105
    },
    {
      "epoch": 0.13501355013550134,
      "step": 2491,
      "training_loss": 7.060583591461182
    },
    {
      "epoch": 0.1350677506775068,
      "grad_norm": 19.953832626342773,
      "learning_rate": 1e-05,
      "loss": 6.6585,
      "step": 2492
    },
    {
      "epoch": 0.1350677506775068,
      "step": 2492,
      "training_loss": 7.216676235198975
    },
    {
      "epoch": 0.1351219512195122,
      "step": 2493,
      "training_loss": 6.966799736022949
    },
    {
      "epoch": 0.13517615176151762,
      "step": 2494,
      "training_loss": 7.49847936630249
    },
    {
      "epoch": 0.13523035230352304,
      "step": 2495,
      "training_loss": 4.347060203552246
    },
    {
      "epoch": 0.13528455284552846,
      "grad_norm": 35.6468620300293,
      "learning_rate": 1e-05,
      "loss": 6.5073,
      "step": 2496
    },
    {
      "epoch": 0.13528455284552846,
      "step": 2496,
      "training_loss": 6.736688613891602
    },
    {
      "epoch": 0.13533875338753387,
      "step": 2497,
      "training_loss": 6.622025489807129
    },
    {
      "epoch": 0.1353929539295393,
      "step": 2498,
      "training_loss": 7.7705488204956055
    },
    {
      "epoch": 0.1354471544715447,
      "step": 2499,
      "training_loss": 6.626865863800049
    },
    {
      "epoch": 0.13550135501355012,
      "grad_norm": 20.903995513916016,
      "learning_rate": 1e-05,
      "loss": 6.939,
      "step": 2500
    },
    {
      "epoch": 0.13550135501355012,
      "step": 2500,
      "training_loss": 6.2944865226745605
    },
    {
      "epoch": 0.13555555555555557,
      "step": 2501,
      "training_loss": 7.82246732711792
    },
    {
      "epoch": 0.13560975609756099,
      "step": 2502,
      "training_loss": 7.1069254875183105
    },
    {
      "epoch": 0.1356639566395664,
      "step": 2503,
      "training_loss": 7.054758071899414
    },
    {
      "epoch": 0.13571815718157182,
      "grad_norm": 16.166383743286133,
      "learning_rate": 1e-05,
      "loss": 7.0697,
      "step": 2504
    },
    {
      "epoch": 0.13571815718157182,
      "step": 2504,
      "training_loss": 6.958922863006592
    },
    {
      "epoch": 0.13577235772357724,
      "step": 2505,
      "training_loss": 6.614577770233154
    },
    {
      "epoch": 0.13582655826558265,
      "step": 2506,
      "training_loss": 7.345183849334717
    },
    {
      "epoch": 0.13588075880758807,
      "step": 2507,
      "training_loss": 6.462575435638428
    },
    {
      "epoch": 0.1359349593495935,
      "grad_norm": 67.98273468017578,
      "learning_rate": 1e-05,
      "loss": 6.8453,
      "step": 2508
    },
    {
      "epoch": 0.1359349593495935,
      "step": 2508,
      "training_loss": 6.912648677825928
    },
    {
      "epoch": 0.1359891598915989,
      "step": 2509,
      "training_loss": 7.60536003112793
    },
    {
      "epoch": 0.13604336043360435,
      "step": 2510,
      "training_loss": 7.151080131530762
    },
    {
      "epoch": 0.13609756097560977,
      "step": 2511,
      "training_loss": 6.910722255706787
    },
    {
      "epoch": 0.13615176151761518,
      "grad_norm": 30.519861221313477,
      "learning_rate": 1e-05,
      "loss": 7.145,
      "step": 2512
    },
    {
      "epoch": 0.13615176151761518,
      "step": 2512,
      "training_loss": 7.603370189666748
    },
    {
      "epoch": 0.1362059620596206,
      "step": 2513,
      "training_loss": 6.863235950469971
    },
    {
      "epoch": 0.13626016260162602,
      "step": 2514,
      "training_loss": 7.850847244262695
    },
    {
      "epoch": 0.13631436314363143,
      "step": 2515,
      "training_loss": 6.552717208862305
    },
    {
      "epoch": 0.13636856368563685,
      "grad_norm": 20.339080810546875,
      "learning_rate": 1e-05,
      "loss": 7.2175,
      "step": 2516
    },
    {
      "epoch": 0.13636856368563685,
      "step": 2516,
      "training_loss": 7.756474494934082
    },
    {
      "epoch": 0.13642276422764227,
      "step": 2517,
      "training_loss": 7.400864601135254
    },
    {
      "epoch": 0.13647696476964769,
      "step": 2518,
      "training_loss": 7.056346893310547
    },
    {
      "epoch": 0.13653116531165313,
      "step": 2519,
      "training_loss": 7.149261951446533
    },
    {
      "epoch": 0.13658536585365855,
      "grad_norm": 17.49959373474121,
      "learning_rate": 1e-05,
      "loss": 7.3407,
      "step": 2520
    },
    {
      "epoch": 0.13658536585365855,
      "step": 2520,
      "training_loss": 7.862305641174316
    },
    {
      "epoch": 0.13663956639566396,
      "step": 2521,
      "training_loss": 4.291009902954102
    },
    {
      "epoch": 0.13669376693766938,
      "step": 2522,
      "training_loss": 6.7023115158081055
    },
    {
      "epoch": 0.1367479674796748,
      "step": 2523,
      "training_loss": 6.281989097595215
    },
    {
      "epoch": 0.13680216802168021,
      "grad_norm": 25.290563583374023,
      "learning_rate": 1e-05,
      "loss": 6.2844,
      "step": 2524
    },
    {
      "epoch": 0.13680216802168021,
      "step": 2524,
      "training_loss": 7.517651081085205
    },
    {
      "epoch": 0.13685636856368563,
      "step": 2525,
      "training_loss": 7.353532314300537
    },
    {
      "epoch": 0.13691056910569105,
      "step": 2526,
      "training_loss": 7.968490123748779
    },
    {
      "epoch": 0.13696476964769647,
      "step": 2527,
      "training_loss": 6.855169296264648
    },
    {
      "epoch": 0.1370189701897019,
      "grad_norm": 33.6573371887207,
      "learning_rate": 1e-05,
      "loss": 7.4237,
      "step": 2528
    },
    {
      "epoch": 0.1370189701897019,
      "step": 2528,
      "training_loss": 6.804192066192627
    },
    {
      "epoch": 0.13707317073170733,
      "step": 2529,
      "training_loss": 7.79111909866333
    },
    {
      "epoch": 0.13712737127371274,
      "step": 2530,
      "training_loss": 7.622513771057129
    },
    {
      "epoch": 0.13718157181571816,
      "step": 2531,
      "training_loss": 7.120431900024414
    },
    {
      "epoch": 0.13723577235772358,
      "grad_norm": 21.62851905822754,
      "learning_rate": 1e-05,
      "loss": 7.3346,
      "step": 2532
    },
    {
      "epoch": 0.13723577235772358,
      "step": 2532,
      "training_loss": 7.346731662750244
    },
    {
      "epoch": 0.137289972899729,
      "step": 2533,
      "training_loss": 7.026785373687744
    },
    {
      "epoch": 0.1373441734417344,
      "step": 2534,
      "training_loss": 6.719282150268555
    },
    {
      "epoch": 0.13739837398373983,
      "step": 2535,
      "training_loss": 7.658319473266602
    },
    {
      "epoch": 0.13745257452574525,
      "grad_norm": 18.880321502685547,
      "learning_rate": 1e-05,
      "loss": 7.1878,
      "step": 2536
    },
    {
      "epoch": 0.13745257452574525,
      "step": 2536,
      "training_loss": 5.669539451599121
    },
    {
      "epoch": 0.13750677506775066,
      "step": 2537,
      "training_loss": 6.377847671508789
    },
    {
      "epoch": 0.1375609756097561,
      "step": 2538,
      "training_loss": 6.079473972320557
    },
    {
      "epoch": 0.13761517615176153,
      "step": 2539,
      "training_loss": 6.654615879058838
    },
    {
      "epoch": 0.13766937669376694,
      "grad_norm": 21.901548385620117,
      "learning_rate": 1e-05,
      "loss": 6.1954,
      "step": 2540
    },
    {
      "epoch": 0.13766937669376694,
      "step": 2540,
      "training_loss": 5.615240097045898
    },
    {
      "epoch": 0.13772357723577236,
      "step": 2541,
      "training_loss": 6.780345439910889
    },
    {
      "epoch": 0.13777777777777778,
      "step": 2542,
      "training_loss": 7.1078267097473145
    },
    {
      "epoch": 0.1378319783197832,
      "step": 2543,
      "training_loss": 7.742656707763672
    },
    {
      "epoch": 0.1378861788617886,
      "grad_norm": 27.089338302612305,
      "learning_rate": 1e-05,
      "loss": 6.8115,
      "step": 2544
    },
    {
      "epoch": 0.1378861788617886,
      "step": 2544,
      "training_loss": 7.532241344451904
    },
    {
      "epoch": 0.13794037940379403,
      "step": 2545,
      "training_loss": 7.4307861328125
    },
    {
      "epoch": 0.13799457994579944,
      "step": 2546,
      "training_loss": 6.04208517074585
    },
    {
      "epoch": 0.1380487804878049,
      "step": 2547,
      "training_loss": 7.581478118896484
    },
    {
      "epoch": 0.1381029810298103,
      "grad_norm": 23.44907569885254,
      "learning_rate": 1e-05,
      "loss": 7.1466,
      "step": 2548
    },
    {
      "epoch": 0.1381029810298103,
      "step": 2548,
      "training_loss": 7.040570259094238
    },
    {
      "epoch": 0.13815718157181572,
      "step": 2549,
      "training_loss": 4.930980205535889
    },
    {
      "epoch": 0.13821138211382114,
      "step": 2550,
      "training_loss": 5.0986456871032715
    },
    {
      "epoch": 0.13826558265582656,
      "step": 2551,
      "training_loss": 9.067586898803711
    },
    {
      "epoch": 0.13831978319783197,
      "grad_norm": 44.085933685302734,
      "learning_rate": 1e-05,
      "loss": 6.5344,
      "step": 2552
    },
    {
      "epoch": 0.13831978319783197,
      "step": 2552,
      "training_loss": 7.21317195892334
    },
    {
      "epoch": 0.1383739837398374,
      "step": 2553,
      "training_loss": 6.495886325836182
    },
    {
      "epoch": 0.1384281842818428,
      "step": 2554,
      "training_loss": 8.241730690002441
    },
    {
      "epoch": 0.13848238482384823,
      "step": 2555,
      "training_loss": 5.781787395477295
    },
    {
      "epoch": 0.13853658536585367,
      "grad_norm": 16.974815368652344,
      "learning_rate": 1e-05,
      "loss": 6.9331,
      "step": 2556
    },
    {
      "epoch": 0.13853658536585367,
      "step": 2556,
      "training_loss": 6.670403957366943
    },
    {
      "epoch": 0.1385907859078591,
      "step": 2557,
      "training_loss": 6.396185874938965
    },
    {
      "epoch": 0.1386449864498645,
      "step": 2558,
      "training_loss": 7.178925514221191
    },
    {
      "epoch": 0.13869918699186992,
      "step": 2559,
      "training_loss": 7.673912525177002
    },
    {
      "epoch": 0.13875338753387534,
      "grad_norm": 30.165178298950195,
      "learning_rate": 1e-05,
      "loss": 6.9799,
      "step": 2560
    },
    {
      "epoch": 0.13875338753387534,
      "step": 2560,
      "training_loss": 7.843574047088623
    },
    {
      "epoch": 0.13880758807588076,
      "step": 2561,
      "training_loss": 6.451774597167969
    },
    {
      "epoch": 0.13886178861788617,
      "step": 2562,
      "training_loss": 6.670575141906738
    },
    {
      "epoch": 0.1389159891598916,
      "step": 2563,
      "training_loss": 8.523720741271973
    },
    {
      "epoch": 0.138970189701897,
      "grad_norm": 25.6728515625,
      "learning_rate": 1e-05,
      "loss": 7.3724,
      "step": 2564
    },
    {
      "epoch": 0.138970189701897,
      "step": 2564,
      "training_loss": 5.8212127685546875
    },
    {
      "epoch": 0.13902439024390245,
      "step": 2565,
      "training_loss": 4.848917007446289
    },
    {
      "epoch": 0.13907859078590787,
      "step": 2566,
      "training_loss": 7.615372657775879
    },
    {
      "epoch": 0.13913279132791329,
      "step": 2567,
      "training_loss": 7.4322733879089355
    },
    {
      "epoch": 0.1391869918699187,
      "grad_norm": 34.8857307434082,
      "learning_rate": 1e-05,
      "loss": 6.4294,
      "step": 2568
    },
    {
      "epoch": 0.1391869918699187,
      "step": 2568,
      "training_loss": 7.6629133224487305
    },
    {
      "epoch": 0.13924119241192412,
      "step": 2569,
      "training_loss": 7.003819465637207
    },
    {
      "epoch": 0.13929539295392954,
      "step": 2570,
      "training_loss": 7.350333213806152
    },
    {
      "epoch": 0.13934959349593495,
      "step": 2571,
      "training_loss": 6.4309587478637695
    },
    {
      "epoch": 0.13940379403794037,
      "grad_norm": 17.854223251342773,
      "learning_rate": 1e-05,
      "loss": 7.112,
      "step": 2572
    },
    {
      "epoch": 0.13940379403794037,
      "step": 2572,
      "training_loss": 6.125321865081787
    },
    {
      "epoch": 0.1394579945799458,
      "step": 2573,
      "training_loss": 6.790889739990234
    },
    {
      "epoch": 0.13951219512195123,
      "step": 2574,
      "training_loss": 7.109437465667725
    },
    {
      "epoch": 0.13956639566395665,
      "step": 2575,
      "training_loss": 6.869138240814209
    },
    {
      "epoch": 0.13962059620596207,
      "grad_norm": 17.120479583740234,
      "learning_rate": 1e-05,
      "loss": 6.7237,
      "step": 2576
    },
    {
      "epoch": 0.13962059620596207,
      "step": 2576,
      "training_loss": 7.03721809387207
    },
    {
      "epoch": 0.13967479674796748,
      "step": 2577,
      "training_loss": 7.747026443481445
    },
    {
      "epoch": 0.1397289972899729,
      "step": 2578,
      "training_loss": 6.946017742156982
    },
    {
      "epoch": 0.13978319783197832,
      "step": 2579,
      "training_loss": 6.172929286956787
    },
    {
      "epoch": 0.13983739837398373,
      "grad_norm": 21.055814743041992,
      "learning_rate": 1e-05,
      "loss": 6.9758,
      "step": 2580
    },
    {
      "epoch": 0.13983739837398373,
      "step": 2580,
      "training_loss": 6.768195629119873
    },
    {
      "epoch": 0.13989159891598915,
      "step": 2581,
      "training_loss": 6.940977096557617
    },
    {
      "epoch": 0.13994579945799457,
      "step": 2582,
      "training_loss": 5.660757541656494
    },
    {
      "epoch": 0.14,
      "step": 2583,
      "training_loss": 6.886831760406494
    },
    {
      "epoch": 0.14005420054200543,
      "grad_norm": 23.115251541137695,
      "learning_rate": 1e-05,
      "loss": 6.5642,
      "step": 2584
    },
    {
      "epoch": 0.14005420054200543,
      "step": 2584,
      "training_loss": 7.897873878479004
    },
    {
      "epoch": 0.14010840108401085,
      "step": 2585,
      "training_loss": 7.113914966583252
    },
    {
      "epoch": 0.14016260162601626,
      "step": 2586,
      "training_loss": 6.489286422729492
    },
    {
      "epoch": 0.14021680216802168,
      "step": 2587,
      "training_loss": 6.589053153991699
    },
    {
      "epoch": 0.1402710027100271,
      "grad_norm": 17.186826705932617,
      "learning_rate": 1e-05,
      "loss": 7.0225,
      "step": 2588
    },
    {
      "epoch": 0.1402710027100271,
      "step": 2588,
      "training_loss": 5.904541015625
    },
    {
      "epoch": 0.14032520325203252,
      "step": 2589,
      "training_loss": 7.777456283569336
    },
    {
      "epoch": 0.14037940379403793,
      "step": 2590,
      "training_loss": 6.57749080657959
    },
    {
      "epoch": 0.14043360433604335,
      "step": 2591,
      "training_loss": 7.421666145324707
    },
    {
      "epoch": 0.1404878048780488,
      "grad_norm": 24.025636672973633,
      "learning_rate": 1e-05,
      "loss": 6.9203,
      "step": 2592
    },
    {
      "epoch": 0.1404878048780488,
      "step": 2592,
      "training_loss": 6.866920471191406
    },
    {
      "epoch": 0.1405420054200542,
      "step": 2593,
      "training_loss": 6.356772422790527
    },
    {
      "epoch": 0.14059620596205963,
      "step": 2594,
      "training_loss": 7.865425109863281
    },
    {
      "epoch": 0.14065040650406505,
      "step": 2595,
      "training_loss": 6.310418605804443
    },
    {
      "epoch": 0.14070460704607046,
      "grad_norm": 16.37093162536621,
      "learning_rate": 1e-05,
      "loss": 6.8499,
      "step": 2596
    },
    {
      "epoch": 0.14070460704607046,
      "step": 2596,
      "training_loss": 7.986814975738525
    },
    {
      "epoch": 0.14075880758807588,
      "step": 2597,
      "training_loss": 7.093797206878662
    },
    {
      "epoch": 0.1408130081300813,
      "step": 2598,
      "training_loss": 7.266373634338379
    },
    {
      "epoch": 0.1408672086720867,
      "step": 2599,
      "training_loss": 7.495234966278076
    },
    {
      "epoch": 0.14092140921409213,
      "grad_norm": 24.399343490600586,
      "learning_rate": 1e-05,
      "loss": 7.4606,
      "step": 2600
    },
    {
      "epoch": 0.14092140921409213,
      "step": 2600,
      "training_loss": 6.410475730895996
    },
    {
      "epoch": 0.14097560975609755,
      "step": 2601,
      "training_loss": 6.86810302734375
    },
    {
      "epoch": 0.141029810298103,
      "step": 2602,
      "training_loss": 7.801304340362549
    },
    {
      "epoch": 0.1410840108401084,
      "step": 2603,
      "training_loss": 7.721095085144043
    },
    {
      "epoch": 0.14113821138211383,
      "grad_norm": 24.383296966552734,
      "learning_rate": 1e-05,
      "loss": 7.2002,
      "step": 2604
    },
    {
      "epoch": 0.14113821138211383,
      "step": 2604,
      "training_loss": 7.8185529708862305
    },
    {
      "epoch": 0.14119241192411924,
      "step": 2605,
      "training_loss": 7.029047012329102
    },
    {
      "epoch": 0.14124661246612466,
      "step": 2606,
      "training_loss": 7.460811614990234
    },
    {
      "epoch": 0.14130081300813008,
      "step": 2607,
      "training_loss": 7.480808734893799
    },
    {
      "epoch": 0.1413550135501355,
      "grad_norm": 15.486342430114746,
      "learning_rate": 1e-05,
      "loss": 7.4473,
      "step": 2608
    },
    {
      "epoch": 0.1413550135501355,
      "step": 2608,
      "training_loss": 7.195689678192139
    },
    {
      "epoch": 0.1414092140921409,
      "step": 2609,
      "training_loss": 7.599444389343262
    },
    {
      "epoch": 0.14146341463414633,
      "step": 2610,
      "training_loss": 6.489224910736084
    },
    {
      "epoch": 0.14151761517615177,
      "step": 2611,
      "training_loss": 5.484253406524658
    },
    {
      "epoch": 0.1415718157181572,
      "grad_norm": 20.400161743164062,
      "learning_rate": 1e-05,
      "loss": 6.6922,
      "step": 2612
    },
    {
      "epoch": 0.1415718157181572,
      "step": 2612,
      "training_loss": 7.4624247550964355
    },
    {
      "epoch": 0.1416260162601626,
      "step": 2613,
      "training_loss": 9.60594654083252
    },
    {
      "epoch": 0.14168021680216802,
      "step": 2614,
      "training_loss": 6.936070919036865
    },
    {
      "epoch": 0.14173441734417344,
      "step": 2615,
      "training_loss": 7.8860578536987305
    },
    {
      "epoch": 0.14178861788617886,
      "grad_norm": 14.954551696777344,
      "learning_rate": 1e-05,
      "loss": 7.9726,
      "step": 2616
    },
    {
      "epoch": 0.14178861788617886,
      "step": 2616,
      "training_loss": 6.348743438720703
    },
    {
      "epoch": 0.14184281842818428,
      "step": 2617,
      "training_loss": 7.02944278717041
    },
    {
      "epoch": 0.1418970189701897,
      "step": 2618,
      "training_loss": 7.17039680480957
    },
    {
      "epoch": 0.1419512195121951,
      "step": 2619,
      "training_loss": 8.11153793334961
    },
    {
      "epoch": 0.14200542005420055,
      "grad_norm": 19.25794219970703,
      "learning_rate": 1e-05,
      "loss": 7.165,
      "step": 2620
    },
    {
      "epoch": 0.14200542005420055,
      "step": 2620,
      "training_loss": 6.456801414489746
    },
    {
      "epoch": 0.14205962059620597,
      "step": 2621,
      "training_loss": 7.190435886383057
    },
    {
      "epoch": 0.1421138211382114,
      "step": 2622,
      "training_loss": 6.131196975708008
    },
    {
      "epoch": 0.1421680216802168,
      "step": 2623,
      "training_loss": 7.137503623962402
    },
    {
      "epoch": 0.14222222222222222,
      "grad_norm": 16.46312141418457,
      "learning_rate": 1e-05,
      "loss": 6.729,
      "step": 2624
    },
    {
      "epoch": 0.14222222222222222,
      "step": 2624,
      "training_loss": 7.21945333480835
    },
    {
      "epoch": 0.14227642276422764,
      "step": 2625,
      "training_loss": 6.304246425628662
    },
    {
      "epoch": 0.14233062330623306,
      "step": 2626,
      "training_loss": 6.86186408996582
    },
    {
      "epoch": 0.14238482384823847,
      "step": 2627,
      "training_loss": 8.161542892456055
    },
    {
      "epoch": 0.1424390243902439,
      "grad_norm": 24.044849395751953,
      "learning_rate": 1e-05,
      "loss": 7.1368,
      "step": 2628
    },
    {
      "epoch": 0.1424390243902439,
      "step": 2628,
      "training_loss": 6.6491618156433105
    },
    {
      "epoch": 0.14249322493224933,
      "step": 2629,
      "training_loss": 7.545411586761475
    },
    {
      "epoch": 0.14254742547425475,
      "step": 2630,
      "training_loss": 7.041926860809326
    },
    {
      "epoch": 0.14260162601626017,
      "step": 2631,
      "training_loss": 7.190550327301025
    },
    {
      "epoch": 0.14265582655826559,
      "grad_norm": 17.28243064880371,
      "learning_rate": 1e-05,
      "loss": 7.1068,
      "step": 2632
    },
    {
      "epoch": 0.14265582655826559,
      "step": 2632,
      "training_loss": 7.361677646636963
    },
    {
      "epoch": 0.142710027100271,
      "step": 2633,
      "training_loss": 7.30226469039917
    },
    {
      "epoch": 0.14276422764227642,
      "step": 2634,
      "training_loss": 8.460108757019043
    },
    {
      "epoch": 0.14281842818428184,
      "step": 2635,
      "training_loss": 7.010674953460693
    },
    {
      "epoch": 0.14287262872628725,
      "grad_norm": 17.456485748291016,
      "learning_rate": 1e-05,
      "loss": 7.5337,
      "step": 2636
    },
    {
      "epoch": 0.14287262872628725,
      "step": 2636,
      "training_loss": 7.053275108337402
    },
    {
      "epoch": 0.14292682926829267,
      "step": 2637,
      "training_loss": 7.076700687408447
    },
    {
      "epoch": 0.14298102981029812,
      "step": 2638,
      "training_loss": 7.728366851806641
    },
    {
      "epoch": 0.14303523035230353,
      "step": 2639,
      "training_loss": 7.305314540863037
    },
    {
      "epoch": 0.14308943089430895,
      "grad_norm": 21.654850006103516,
      "learning_rate": 1e-05,
      "loss": 7.2909,
      "step": 2640
    },
    {
      "epoch": 0.14308943089430895,
      "step": 2640,
      "training_loss": 6.181407928466797
    },
    {
      "epoch": 0.14314363143631437,
      "step": 2641,
      "training_loss": 7.536325454711914
    },
    {
      "epoch": 0.14319783197831978,
      "step": 2642,
      "training_loss": 7.40533971786499
    },
    {
      "epoch": 0.1432520325203252,
      "step": 2643,
      "training_loss": 6.647276878356934
    },
    {
      "epoch": 0.14330623306233062,
      "grad_norm": 14.121835708618164,
      "learning_rate": 1e-05,
      "loss": 6.9426,
      "step": 2644
    },
    {
      "epoch": 0.14330623306233062,
      "step": 2644,
      "training_loss": 7.893548488616943
    },
    {
      "epoch": 0.14336043360433603,
      "step": 2645,
      "training_loss": 6.1227240562438965
    },
    {
      "epoch": 0.14341463414634145,
      "step": 2646,
      "training_loss": 8.516410827636719
    },
    {
      "epoch": 0.1434688346883469,
      "step": 2647,
      "training_loss": 7.292142391204834
    },
    {
      "epoch": 0.1435230352303523,
      "grad_norm": 17.110750198364258,
      "learning_rate": 1e-05,
      "loss": 7.4562,
      "step": 2648
    },
    {
      "epoch": 0.1435230352303523,
      "step": 2648,
      "training_loss": 6.254172325134277
    },
    {
      "epoch": 0.14357723577235773,
      "step": 2649,
      "training_loss": 5.857818603515625
    },
    {
      "epoch": 0.14363143631436315,
      "step": 2650,
      "training_loss": 6.030587196350098
    },
    {
      "epoch": 0.14368563685636856,
      "step": 2651,
      "training_loss": 7.093649387359619
    },
    {
      "epoch": 0.14373983739837398,
      "grad_norm": 25.02707290649414,
      "learning_rate": 1e-05,
      "loss": 6.3091,
      "step": 2652
    },
    {
      "epoch": 0.14373983739837398,
      "step": 2652,
      "training_loss": 7.9951934814453125
    },
    {
      "epoch": 0.1437940379403794,
      "step": 2653,
      "training_loss": 6.886555194854736
    },
    {
      "epoch": 0.14384823848238482,
      "step": 2654,
      "training_loss": 7.544373989105225
    },
    {
      "epoch": 0.14390243902439023,
      "step": 2655,
      "training_loss": 5.216752052307129
    },
    {
      "epoch": 0.14395663956639568,
      "grad_norm": 17.857152938842773,
      "learning_rate": 1e-05,
      "loss": 6.9107,
      "step": 2656
    },
    {
      "epoch": 0.14395663956639568,
      "step": 2656,
      "training_loss": 6.886301040649414
    },
    {
      "epoch": 0.1440108401084011,
      "step": 2657,
      "training_loss": 7.307267665863037
    },
    {
      "epoch": 0.1440650406504065,
      "step": 2658,
      "training_loss": 7.07749080657959
    },
    {
      "epoch": 0.14411924119241193,
      "step": 2659,
      "training_loss": 5.898308753967285
    },
    {
      "epoch": 0.14417344173441735,
      "grad_norm": 21.145387649536133,
      "learning_rate": 1e-05,
      "loss": 6.7923,
      "step": 2660
    },
    {
      "epoch": 0.14417344173441735,
      "step": 2660,
      "training_loss": 8.123764038085938
    },
    {
      "epoch": 0.14422764227642276,
      "step": 2661,
      "training_loss": 6.961852073669434
    },
    {
      "epoch": 0.14428184281842818,
      "step": 2662,
      "training_loss": 6.3797736167907715
    },
    {
      "epoch": 0.1443360433604336,
      "step": 2663,
      "training_loss": 7.33880615234375
    },
    {
      "epoch": 0.144390243902439,
      "grad_norm": 23.6699275970459,
      "learning_rate": 1e-05,
      "loss": 7.201,
      "step": 2664
    },
    {
      "epoch": 0.144390243902439,
      "step": 2664,
      "training_loss": 6.75139045715332
    },
    {
      "epoch": 0.14444444444444443,
      "step": 2665,
      "training_loss": 6.73094367980957
    },
    {
      "epoch": 0.14449864498644988,
      "step": 2666,
      "training_loss": 6.6419501304626465
    },
    {
      "epoch": 0.1445528455284553,
      "step": 2667,
      "training_loss": 5.977971076965332
    },
    {
      "epoch": 0.1446070460704607,
      "grad_norm": 21.326229095458984,
      "learning_rate": 1e-05,
      "loss": 6.5256,
      "step": 2668
    },
    {
      "epoch": 0.1446070460704607,
      "step": 2668,
      "training_loss": 8.152270317077637
    },
    {
      "epoch": 0.14466124661246613,
      "step": 2669,
      "training_loss": 7.795559406280518
    },
    {
      "epoch": 0.14471544715447154,
      "step": 2670,
      "training_loss": 6.828698635101318
    },
    {
      "epoch": 0.14476964769647696,
      "step": 2671,
      "training_loss": 8.477325439453125
    },
    {
      "epoch": 0.14482384823848238,
      "grad_norm": 23.462650299072266,
      "learning_rate": 1e-05,
      "loss": 7.8135,
      "step": 2672
    },
    {
      "epoch": 0.14482384823848238,
      "step": 2672,
      "training_loss": 7.181950092315674
    },
    {
      "epoch": 0.1448780487804878,
      "step": 2673,
      "training_loss": 6.300014972686768
    },
    {
      "epoch": 0.1449322493224932,
      "step": 2674,
      "training_loss": 7.548744201660156
    },
    {
      "epoch": 0.14498644986449866,
      "step": 2675,
      "training_loss": 7.495262622833252
    },
    {
      "epoch": 0.14504065040650407,
      "grad_norm": 31.267736434936523,
      "learning_rate": 1e-05,
      "loss": 7.1315,
      "step": 2676
    },
    {
      "epoch": 0.14504065040650407,
      "step": 2676,
      "training_loss": 4.403129577636719
    },
    {
      "epoch": 0.1450948509485095,
      "step": 2677,
      "training_loss": 7.270368576049805
    },
    {
      "epoch": 0.1451490514905149,
      "step": 2678,
      "training_loss": 7.146984577178955
    },
    {
      "epoch": 0.14520325203252032,
      "step": 2679,
      "training_loss": 6.9243364334106445
    },
    {
      "epoch": 0.14525745257452574,
      "grad_norm": 26.08025550842285,
      "learning_rate": 1e-05,
      "loss": 6.4362,
      "step": 2680
    },
    {
      "epoch": 0.14525745257452574,
      "step": 2680,
      "training_loss": 8.023134231567383
    },
    {
      "epoch": 0.14531165311653116,
      "step": 2681,
      "training_loss": 8.00939655303955
    },
    {
      "epoch": 0.14536585365853658,
      "step": 2682,
      "training_loss": 7.32981014251709
    },
    {
      "epoch": 0.145420054200542,
      "step": 2683,
      "training_loss": 7.435822010040283
    },
    {
      "epoch": 0.14547425474254744,
      "grad_norm": 16.1890869140625,
      "learning_rate": 1e-05,
      "loss": 7.6995,
      "step": 2684
    },
    {
      "epoch": 0.14547425474254744,
      "step": 2684,
      "training_loss": 4.415499687194824
    },
    {
      "epoch": 0.14552845528455285,
      "step": 2685,
      "training_loss": 6.979267120361328
    },
    {
      "epoch": 0.14558265582655827,
      "step": 2686,
      "training_loss": 7.1596808433532715
    },
    {
      "epoch": 0.1456368563685637,
      "step": 2687,
      "training_loss": 6.932631492614746
    },
    {
      "epoch": 0.1456910569105691,
      "grad_norm": 46.051815032958984,
      "learning_rate": 1e-05,
      "loss": 6.3718,
      "step": 2688
    },
    {
      "epoch": 0.1456910569105691,
      "step": 2688,
      "training_loss": 7.968842506408691
    },
    {
      "epoch": 0.14574525745257452,
      "step": 2689,
      "training_loss": 5.563681125640869
    },
    {
      "epoch": 0.14579945799457994,
      "step": 2690,
      "training_loss": 5.70306396484375
    },
    {
      "epoch": 0.14585365853658536,
      "step": 2691,
      "training_loss": 7.031442165374756
    },
    {
      "epoch": 0.14590785907859077,
      "grad_norm": 30.15680503845215,
      "learning_rate": 1e-05,
      "loss": 6.5668,
      "step": 2692
    },
    {
      "epoch": 0.14590785907859077,
      "step": 2692,
      "training_loss": 5.624709129333496
    },
    {
      "epoch": 0.14596205962059622,
      "step": 2693,
      "training_loss": 6.83181095123291
    },
    {
      "epoch": 0.14601626016260164,
      "step": 2694,
      "training_loss": 7.501360893249512
    },
    {
      "epoch": 0.14607046070460705,
      "step": 2695,
      "training_loss": 5.888236999511719
    },
    {
      "epoch": 0.14612466124661247,
      "grad_norm": 28.95340919494629,
      "learning_rate": 1e-05,
      "loss": 6.4615,
      "step": 2696
    },
    {
      "epoch": 0.14612466124661247,
      "step": 2696,
      "training_loss": 7.543758869171143
    },
    {
      "epoch": 0.1461788617886179,
      "step": 2697,
      "training_loss": 6.650112628936768
    },
    {
      "epoch": 0.1462330623306233,
      "step": 2698,
      "training_loss": 6.910295486450195
    },
    {
      "epoch": 0.14628726287262872,
      "step": 2699,
      "training_loss": 6.898913383483887
    },
    {
      "epoch": 0.14634146341463414,
      "grad_norm": 29.399581909179688,
      "learning_rate": 1e-05,
      "loss": 7.0008,
      "step": 2700
    },
    {
      "epoch": 0.14634146341463414,
      "step": 2700,
      "training_loss": 5.190524101257324
    },
    {
      "epoch": 0.14639566395663955,
      "step": 2701,
      "training_loss": 7.194784164428711
    },
    {
      "epoch": 0.146449864498645,
      "step": 2702,
      "training_loss": 7.428859233856201
    },
    {
      "epoch": 0.14650406504065042,
      "step": 2703,
      "training_loss": 7.772037982940674
    },
    {
      "epoch": 0.14655826558265583,
      "grad_norm": 25.330265045166016,
      "learning_rate": 1e-05,
      "loss": 6.8966,
      "step": 2704
    },
    {
      "epoch": 0.14655826558265583,
      "step": 2704,
      "training_loss": 6.064313888549805
    },
    {
      "epoch": 0.14661246612466125,
      "step": 2705,
      "training_loss": 6.1325201988220215
    },
    {
      "epoch": 0.14666666666666667,
      "step": 2706,
      "training_loss": 6.746006011962891
    },
    {
      "epoch": 0.14672086720867208,
      "step": 2707,
      "training_loss": 7.027482032775879
    },
    {
      "epoch": 0.1467750677506775,
      "grad_norm": 29.07795524597168,
      "learning_rate": 1e-05,
      "loss": 6.4926,
      "step": 2708
    },
    {
      "epoch": 0.1467750677506775,
      "step": 2708,
      "training_loss": 6.280714511871338
    },
    {
      "epoch": 0.14682926829268292,
      "step": 2709,
      "training_loss": 6.554640769958496
    },
    {
      "epoch": 0.14688346883468834,
      "step": 2710,
      "training_loss": 7.091335773468018
    },
    {
      "epoch": 0.14693766937669378,
      "step": 2711,
      "training_loss": 7.897545337677002
    },
    {
      "epoch": 0.1469918699186992,
      "grad_norm": 23.965288162231445,
      "learning_rate": 1e-05,
      "loss": 6.9561,
      "step": 2712
    },
    {
      "epoch": 0.1469918699186992,
      "step": 2712,
      "training_loss": 8.26445198059082
    },
    {
      "epoch": 0.14704607046070461,
      "step": 2713,
      "training_loss": 6.800072193145752
    },
    {
      "epoch": 0.14710027100271003,
      "step": 2714,
      "training_loss": 6.746550559997559
    },
    {
      "epoch": 0.14715447154471545,
      "step": 2715,
      "training_loss": 4.743051052093506
    },
    {
      "epoch": 0.14720867208672087,
      "grad_norm": 26.738666534423828,
      "learning_rate": 1e-05,
      "loss": 6.6385,
      "step": 2716
    },
    {
      "epoch": 0.14720867208672087,
      "step": 2716,
      "training_loss": 6.969179153442383
    },
    {
      "epoch": 0.14726287262872628,
      "step": 2717,
      "training_loss": 7.300539493560791
    },
    {
      "epoch": 0.1473170731707317,
      "step": 2718,
      "training_loss": 7.881967067718506
    },
    {
      "epoch": 0.14737127371273712,
      "step": 2719,
      "training_loss": 8.212117195129395
    },
    {
      "epoch": 0.14742547425474256,
      "grad_norm": 43.5081901550293,
      "learning_rate": 1e-05,
      "loss": 7.591,
      "step": 2720
    },
    {
      "epoch": 0.14742547425474256,
      "step": 2720,
      "training_loss": 7.683688163757324
    },
    {
      "epoch": 0.14747967479674798,
      "step": 2721,
      "training_loss": 6.935351371765137
    },
    {
      "epoch": 0.1475338753387534,
      "step": 2722,
      "training_loss": 7.5208563804626465
    },
    {
      "epoch": 0.1475880758807588,
      "step": 2723,
      "training_loss": 6.421712398529053
    },
    {
      "epoch": 0.14764227642276423,
      "grad_norm": 15.367502212524414,
      "learning_rate": 1e-05,
      "loss": 7.1404,
      "step": 2724
    },
    {
      "epoch": 0.14764227642276423,
      "step": 2724,
      "training_loss": 8.721485137939453
    },
    {
      "epoch": 0.14769647696476965,
      "step": 2725,
      "training_loss": 7.487117290496826
    },
    {
      "epoch": 0.14775067750677506,
      "step": 2726,
      "training_loss": 6.056400775909424
    },
    {
      "epoch": 0.14780487804878048,
      "step": 2727,
      "training_loss": 8.04054069519043
    },
    {
      "epoch": 0.1478590785907859,
      "grad_norm": 24.12108612060547,
      "learning_rate": 1e-05,
      "loss": 7.5764,
      "step": 2728
    },
    {
      "epoch": 0.1478590785907859,
      "step": 2728,
      "training_loss": 5.131184101104736
    },
    {
      "epoch": 0.14791327913279131,
      "step": 2729,
      "training_loss": 7.3170342445373535
    },
    {
      "epoch": 0.14796747967479676,
      "step": 2730,
      "training_loss": 6.484695911407471
    },
    {
      "epoch": 0.14802168021680218,
      "step": 2731,
      "training_loss": 6.992718696594238
    },
    {
      "epoch": 0.1480758807588076,
      "grad_norm": 19.52605628967285,
      "learning_rate": 1e-05,
      "loss": 6.4814,
      "step": 2732
    },
    {
      "epoch": 0.1480758807588076,
      "step": 2732,
      "training_loss": 7.523624420166016
    },
    {
      "epoch": 0.148130081300813,
      "step": 2733,
      "training_loss": 7.454044818878174
    },
    {
      "epoch": 0.14818428184281843,
      "step": 2734,
      "training_loss": 6.95874547958374
    },
    {
      "epoch": 0.14823848238482384,
      "step": 2735,
      "training_loss": 8.376072883605957
    },
    {
      "epoch": 0.14829268292682926,
      "grad_norm": 26.37027931213379,
      "learning_rate": 1e-05,
      "loss": 7.5781,
      "step": 2736
    },
    {
      "epoch": 0.14829268292682926,
      "step": 2736,
      "training_loss": 7.0401201248168945
    },
    {
      "epoch": 0.14834688346883468,
      "step": 2737,
      "training_loss": 7.085273742675781
    },
    {
      "epoch": 0.1484010840108401,
      "step": 2738,
      "training_loss": 6.184514999389648
    },
    {
      "epoch": 0.14845528455284554,
      "step": 2739,
      "training_loss": 8.308947563171387
    },
    {
      "epoch": 0.14850948509485096,
      "grad_norm": 38.29178237915039,
      "learning_rate": 1e-05,
      "loss": 7.1547,
      "step": 2740
    },
    {
      "epoch": 0.14850948509485096,
      "step": 2740,
      "training_loss": 7.816874027252197
    },
    {
      "epoch": 0.14856368563685637,
      "step": 2741,
      "training_loss": 7.641136646270752
    },
    {
      "epoch": 0.1486178861788618,
      "step": 2742,
      "training_loss": 6.590243339538574
    },
    {
      "epoch": 0.1486720867208672,
      "step": 2743,
      "training_loss": 6.91975212097168
    },
    {
      "epoch": 0.14872628726287263,
      "grad_norm": 28.48296546936035,
      "learning_rate": 1e-05,
      "loss": 7.242,
      "step": 2744
    },
    {
      "epoch": 0.14872628726287263,
      "step": 2744,
      "training_loss": 7.07703971862793
    },
    {
      "epoch": 0.14878048780487804,
      "step": 2745,
      "training_loss": 5.6660261154174805
    },
    {
      "epoch": 0.14883468834688346,
      "step": 2746,
      "training_loss": 4.158545970916748
    },
    {
      "epoch": 0.14888888888888888,
      "step": 2747,
      "training_loss": 5.982562065124512
    },
    {
      "epoch": 0.14894308943089432,
      "grad_norm": 22.693960189819336,
      "learning_rate": 1e-05,
      "loss": 5.721,
      "step": 2748
    },
    {
      "epoch": 0.14894308943089432,
      "step": 2748,
      "training_loss": 4.633694648742676
    },
    {
      "epoch": 0.14899728997289974,
      "step": 2749,
      "training_loss": 7.384731292724609
    },
    {
      "epoch": 0.14905149051490515,
      "step": 2750,
      "training_loss": 6.685415744781494
    },
    {
      "epoch": 0.14910569105691057,
      "step": 2751,
      "training_loss": 5.716179847717285
    },
    {
      "epoch": 0.149159891598916,
      "grad_norm": 29.589332580566406,
      "learning_rate": 1e-05,
      "loss": 6.105,
      "step": 2752
    },
    {
      "epoch": 0.149159891598916,
      "step": 2752,
      "training_loss": 5.508753776550293
    },
    {
      "epoch": 0.1492140921409214,
      "step": 2753,
      "training_loss": 6.403130531311035
    },
    {
      "epoch": 0.14926829268292682,
      "step": 2754,
      "training_loss": 6.1644158363342285
    },
    {
      "epoch": 0.14932249322493224,
      "step": 2755,
      "training_loss": 7.2176713943481445
    },
    {
      "epoch": 0.14937669376693766,
      "grad_norm": 15.610504150390625,
      "learning_rate": 1e-05,
      "loss": 6.3235,
      "step": 2756
    },
    {
      "epoch": 0.14937669376693766,
      "step": 2756,
      "training_loss": 7.52488899230957
    },
    {
      "epoch": 0.1494308943089431,
      "step": 2757,
      "training_loss": 7.013801097869873
    },
    {
      "epoch": 0.14948509485094852,
      "step": 2758,
      "training_loss": 6.904720306396484
    },
    {
      "epoch": 0.14953929539295394,
      "step": 2759,
      "training_loss": 7.126418590545654
    },
    {
      "epoch": 0.14959349593495935,
      "grad_norm": 24.772092819213867,
      "learning_rate": 1e-05,
      "loss": 7.1425,
      "step": 2760
    },
    {
      "epoch": 0.14959349593495935,
      "step": 2760,
      "training_loss": 6.748104095458984
    },
    {
      "epoch": 0.14964769647696477,
      "step": 2761,
      "training_loss": 4.5430073738098145
    },
    {
      "epoch": 0.1497018970189702,
      "step": 2762,
      "training_loss": 6.75314474105835
    },
    {
      "epoch": 0.1497560975609756,
      "step": 2763,
      "training_loss": 6.927399158477783
    },
    {
      "epoch": 0.14981029810298102,
      "grad_norm": 26.12412452697754,
      "learning_rate": 1e-05,
      "loss": 6.2429,
      "step": 2764
    },
    {
      "epoch": 0.14981029810298102,
      "step": 2764,
      "training_loss": 6.580842971801758
    },
    {
      "epoch": 0.14986449864498644,
      "step": 2765,
      "training_loss": 7.397954940795898
    },
    {
      "epoch": 0.14991869918699188,
      "step": 2766,
      "training_loss": 7.401541233062744
    },
    {
      "epoch": 0.1499728997289973,
      "step": 2767,
      "training_loss": 7.023067474365234
    },
    {
      "epoch": 0.15002710027100272,
      "grad_norm": 15.717175483703613,
      "learning_rate": 1e-05,
      "loss": 7.1009,
      "step": 2768
    },
    {
      "epoch": 0.15002710027100272,
      "step": 2768,
      "training_loss": 5.815362930297852
    },
    {
      "epoch": 0.15008130081300813,
      "step": 2769,
      "training_loss": 4.521266937255859
    },
    {
      "epoch": 0.15013550135501355,
      "step": 2770,
      "training_loss": 7.441351890563965
    },
    {
      "epoch": 0.15018970189701897,
      "step": 2771,
      "training_loss": 5.383084774017334
    },
    {
      "epoch": 0.15024390243902438,
      "grad_norm": 21.512527465820312,
      "learning_rate": 1e-05,
      "loss": 5.7903,
      "step": 2772
    },
    {
      "epoch": 0.15024390243902438,
      "step": 2772,
      "training_loss": 5.500888347625732
    },
    {
      "epoch": 0.1502981029810298,
      "step": 2773,
      "training_loss": 6.990847110748291
    },
    {
      "epoch": 0.15035230352303522,
      "step": 2774,
      "training_loss": 6.203507900238037
    },
    {
      "epoch": 0.15040650406504066,
      "step": 2775,
      "training_loss": 6.280167579650879
    },
    {
      "epoch": 0.15046070460704608,
      "grad_norm": 17.69522476196289,
      "learning_rate": 1e-05,
      "loss": 6.2439,
      "step": 2776
    },
    {
      "epoch": 0.15046070460704608,
      "step": 2776,
      "training_loss": 7.131898880004883
    },
    {
      "epoch": 0.1505149051490515,
      "step": 2777,
      "training_loss": 8.264874458312988
    },
    {
      "epoch": 0.15056910569105691,
      "step": 2778,
      "training_loss": 6.983683109283447
    },
    {
      "epoch": 0.15062330623306233,
      "step": 2779,
      "training_loss": 6.052300930023193
    },
    {
      "epoch": 0.15067750677506775,
      "grad_norm": 15.918036460876465,
      "learning_rate": 1e-05,
      "loss": 7.1082,
      "step": 2780
    },
    {
      "epoch": 0.15067750677506775,
      "step": 2780,
      "training_loss": 7.474325180053711
    },
    {
      "epoch": 0.15073170731707317,
      "step": 2781,
      "training_loss": 7.750882148742676
    },
    {
      "epoch": 0.15078590785907858,
      "step": 2782,
      "training_loss": 7.630563735961914
    },
    {
      "epoch": 0.150840108401084,
      "step": 2783,
      "training_loss": 6.802700996398926
    },
    {
      "epoch": 0.15089430894308944,
      "grad_norm": 22.999780654907227,
      "learning_rate": 1e-05,
      "loss": 7.4146,
      "step": 2784
    },
    {
      "epoch": 0.15089430894308944,
      "step": 2784,
      "training_loss": 6.824918746948242
    },
    {
      "epoch": 0.15094850948509486,
      "step": 2785,
      "training_loss": 7.499577045440674
    },
    {
      "epoch": 0.15100271002710028,
      "step": 2786,
      "training_loss": 7.234065055847168
    },
    {
      "epoch": 0.1510569105691057,
      "step": 2787,
      "training_loss": 7.381229877471924
    },
    {
      "epoch": 0.1511111111111111,
      "grad_norm": 28.34613800048828,
      "learning_rate": 1e-05,
      "loss": 7.2349,
      "step": 2788
    },
    {
      "epoch": 0.1511111111111111,
      "step": 2788,
      "training_loss": 6.11315393447876
    },
    {
      "epoch": 0.15116531165311653,
      "step": 2789,
      "training_loss": 6.908787250518799
    },
    {
      "epoch": 0.15121951219512195,
      "step": 2790,
      "training_loss": 6.644617557525635
    },
    {
      "epoch": 0.15127371273712736,
      "step": 2791,
      "training_loss": 8.45887565612793
    },
    {
      "epoch": 0.15132791327913278,
      "grad_norm": 18.90997886657715,
      "learning_rate": 1e-05,
      "loss": 7.0314,
      "step": 2792
    },
    {
      "epoch": 0.15132791327913278,
      "step": 2792,
      "training_loss": 6.93646240234375
    },
    {
      "epoch": 0.1513821138211382,
      "step": 2793,
      "training_loss": 7.207126617431641
    },
    {
      "epoch": 0.15143631436314364,
      "step": 2794,
      "training_loss": 7.815772533416748
    },
    {
      "epoch": 0.15149051490514906,
      "step": 2795,
      "training_loss": 7.011143207550049
    },
    {
      "epoch": 0.15154471544715448,
      "grad_norm": 17.711400985717773,
      "learning_rate": 1e-05,
      "loss": 7.2426,
      "step": 2796
    },
    {
      "epoch": 0.15154471544715448,
      "step": 2796,
      "training_loss": 7.868875503540039
    },
    {
      "epoch": 0.1515989159891599,
      "step": 2797,
      "training_loss": 5.351524353027344
    },
    {
      "epoch": 0.1516531165311653,
      "step": 2798,
      "training_loss": 6.708816051483154
    },
    {
      "epoch": 0.15170731707317073,
      "step": 2799,
      "training_loss": 7.103667736053467
    },
    {
      "epoch": 0.15176151761517614,
      "grad_norm": 21.83403968811035,
      "learning_rate": 1e-05,
      "loss": 6.7582,
      "step": 2800
    },
    {
      "epoch": 0.15176151761517614,
      "step": 2800,
      "training_loss": 7.187362194061279
    },
    {
      "epoch": 0.15181571815718156,
      "step": 2801,
      "training_loss": 5.375797271728516
    },
    {
      "epoch": 0.15186991869918698,
      "step": 2802,
      "training_loss": 7.37620210647583
    },
    {
      "epoch": 0.15192411924119242,
      "step": 2803,
      "training_loss": 5.788379192352295
    },
    {
      "epoch": 0.15197831978319784,
      "grad_norm": 48.4687614440918,
      "learning_rate": 1e-05,
      "loss": 6.4319,
      "step": 2804
    },
    {
      "epoch": 0.15197831978319784,
      "step": 2804,
      "training_loss": 6.213126182556152
    },
    {
      "epoch": 0.15203252032520326,
      "step": 2805,
      "training_loss": 5.87962532043457
    },
    {
      "epoch": 0.15208672086720867,
      "step": 2806,
      "training_loss": 6.834182262420654
    },
    {
      "epoch": 0.1521409214092141,
      "step": 2807,
      "training_loss": 6.190089225769043
    },
    {
      "epoch": 0.1521951219512195,
      "grad_norm": 18.155990600585938,
      "learning_rate": 1e-05,
      "loss": 6.2793,
      "step": 2808
    },
    {
      "epoch": 0.1521951219512195,
      "step": 2808,
      "training_loss": 6.960704326629639
    },
    {
      "epoch": 0.15224932249322493,
      "step": 2809,
      "training_loss": 6.816217422485352
    },
    {
      "epoch": 0.15230352303523034,
      "step": 2810,
      "training_loss": 6.386111736297607
    },
    {
      "epoch": 0.15235772357723576,
      "step": 2811,
      "training_loss": 8.093706130981445
    },
    {
      "epoch": 0.1524119241192412,
      "grad_norm": 34.86417007446289,
      "learning_rate": 1e-05,
      "loss": 7.0642,
      "step": 2812
    },
    {
      "epoch": 0.1524119241192412,
      "step": 2812,
      "training_loss": 7.025763034820557
    },
    {
      "epoch": 0.15246612466124662,
      "step": 2813,
      "training_loss": 5.740941524505615
    },
    {
      "epoch": 0.15252032520325204,
      "step": 2814,
      "training_loss": 6.446156024932861
    },
    {
      "epoch": 0.15257452574525746,
      "step": 2815,
      "training_loss": 7.457784175872803
    },
    {
      "epoch": 0.15262872628726287,
      "grad_norm": 17.195314407348633,
      "learning_rate": 1e-05,
      "loss": 6.6677,
      "step": 2816
    },
    {
      "epoch": 0.15262872628726287,
      "step": 2816,
      "training_loss": 6.662289619445801
    },
    {
      "epoch": 0.1526829268292683,
      "step": 2817,
      "training_loss": 7.3425397872924805
    },
    {
      "epoch": 0.1527371273712737,
      "step": 2818,
      "training_loss": 6.903387069702148
    },
    {
      "epoch": 0.15279132791327912,
      "step": 2819,
      "training_loss": 7.31217098236084
    },
    {
      "epoch": 0.15284552845528454,
      "grad_norm": 23.215063095092773,
      "learning_rate": 1e-05,
      "loss": 7.0551,
      "step": 2820
    },
    {
      "epoch": 0.15284552845528454,
      "step": 2820,
      "training_loss": 7.486176490783691
    },
    {
      "epoch": 0.15289972899728999,
      "step": 2821,
      "training_loss": 7.081368446350098
    },
    {
      "epoch": 0.1529539295392954,
      "step": 2822,
      "training_loss": 6.873575210571289
    },
    {
      "epoch": 0.15300813008130082,
      "step": 2823,
      "training_loss": 7.563320636749268
    },
    {
      "epoch": 0.15306233062330624,
      "grad_norm": 17.66584587097168,
      "learning_rate": 1e-05,
      "loss": 7.2511,
      "step": 2824
    },
    {
      "epoch": 0.15306233062330624,
      "step": 2824,
      "training_loss": 5.7166924476623535
    },
    {
      "epoch": 0.15311653116531165,
      "step": 2825,
      "training_loss": 6.489023685455322
    },
    {
      "epoch": 0.15317073170731707,
      "step": 2826,
      "training_loss": 6.718019008636475
    },
    {
      "epoch": 0.1532249322493225,
      "step": 2827,
      "training_loss": 7.365563869476318
    },
    {
      "epoch": 0.1532791327913279,
      "grad_norm": 21.14395523071289,
      "learning_rate": 1e-05,
      "loss": 6.5723,
      "step": 2828
    },
    {
      "epoch": 0.1532791327913279,
      "step": 2828,
      "training_loss": 7.259206295013428
    },
    {
      "epoch": 0.15333333333333332,
      "step": 2829,
      "training_loss": 6.249456405639648
    },
    {
      "epoch": 0.15338753387533877,
      "step": 2830,
      "training_loss": 5.700502872467041
    },
    {
      "epoch": 0.15344173441734418,
      "step": 2831,
      "training_loss": 7.0566725730896
    },
    {
      "epoch": 0.1534959349593496,
      "grad_norm": 14.497147560119629,
      "learning_rate": 1e-05,
      "loss": 6.5665,
      "step": 2832
    },
    {
      "epoch": 0.1534959349593496,
      "step": 2832,
      "training_loss": 5.515836238861084
    },
    {
      "epoch": 0.15355013550135502,
      "step": 2833,
      "training_loss": 7.052157402038574
    },
    {
      "epoch": 0.15360433604336043,
      "step": 2834,
      "training_loss": 6.649300575256348
    },
    {
      "epoch": 0.15365853658536585,
      "step": 2835,
      "training_loss": 7.26254940032959
    },
    {
      "epoch": 0.15371273712737127,
      "grad_norm": 20.538881301879883,
      "learning_rate": 1e-05,
      "loss": 6.62,
      "step": 2836
    },
    {
      "epoch": 0.15371273712737127,
      "step": 2836,
      "training_loss": 7.1271796226501465
    },
    {
      "epoch": 0.15376693766937669,
      "step": 2837,
      "training_loss": 6.447570323944092
    },
    {
      "epoch": 0.1538211382113821,
      "step": 2838,
      "training_loss": 6.9930100440979
    },
    {
      "epoch": 0.15387533875338755,
      "step": 2839,
      "training_loss": 7.503795146942139
    },
    {
      "epoch": 0.15392953929539296,
      "grad_norm": 20.24793243408203,
      "learning_rate": 1e-05,
      "loss": 7.0179,
      "step": 2840
    },
    {
      "epoch": 0.15392953929539296,
      "step": 2840,
      "training_loss": 7.438872814178467
    },
    {
      "epoch": 0.15398373983739838,
      "step": 2841,
      "training_loss": 6.756739616394043
    },
    {
      "epoch": 0.1540379403794038,
      "step": 2842,
      "training_loss": 7.526775360107422
    },
    {
      "epoch": 0.15409214092140922,
      "step": 2843,
      "training_loss": 6.844085693359375
    },
    {
      "epoch": 0.15414634146341463,
      "grad_norm": 24.70453453063965,
      "learning_rate": 1e-05,
      "loss": 7.1416,
      "step": 2844
    },
    {
      "epoch": 0.15414634146341463,
      "step": 2844,
      "training_loss": 6.6657490730285645
    },
    {
      "epoch": 0.15420054200542005,
      "step": 2845,
      "training_loss": 6.371633529663086
    },
    {
      "epoch": 0.15425474254742547,
      "step": 2846,
      "training_loss": 6.308870315551758
    },
    {
      "epoch": 0.15430894308943088,
      "step": 2847,
      "training_loss": 6.957880020141602
    },
    {
      "epoch": 0.15436314363143633,
      "grad_norm": 36.64063262939453,
      "learning_rate": 1e-05,
      "loss": 6.576,
      "step": 2848
    },
    {
      "epoch": 0.15436314363143633,
      "step": 2848,
      "training_loss": 6.810107231140137
    },
    {
      "epoch": 0.15441734417344175,
      "step": 2849,
      "training_loss": 8.678108215332031
    },
    {
      "epoch": 0.15447154471544716,
      "step": 2850,
      "training_loss": 6.493203163146973
    },
    {
      "epoch": 0.15452574525745258,
      "step": 2851,
      "training_loss": 7.394636154174805
    },
    {
      "epoch": 0.154579945799458,
      "grad_norm": 21.090574264526367,
      "learning_rate": 1e-05,
      "loss": 7.344,
      "step": 2852
    },
    {
      "epoch": 0.154579945799458,
      "step": 2852,
      "training_loss": 6.893336772918701
    },
    {
      "epoch": 0.1546341463414634,
      "step": 2853,
      "training_loss": 8.11740493774414
    },
    {
      "epoch": 0.15468834688346883,
      "step": 2854,
      "training_loss": 7.34279727935791
    },
    {
      "epoch": 0.15474254742547425,
      "step": 2855,
      "training_loss": 7.38548469543457
    },
    {
      "epoch": 0.15479674796747966,
      "grad_norm": 18.32781410217285,
      "learning_rate": 1e-05,
      "loss": 7.4348,
      "step": 2856
    },
    {
      "epoch": 0.15479674796747966,
      "step": 2856,
      "training_loss": 7.451510429382324
    },
    {
      "epoch": 0.15485094850948508,
      "step": 2857,
      "training_loss": 3.8825275897979736
    },
    {
      "epoch": 0.15490514905149053,
      "step": 2858,
      "training_loss": 7.525318622589111
    },
    {
      "epoch": 0.15495934959349594,
      "step": 2859,
      "training_loss": 7.2286176681518555
    },
    {
      "epoch": 0.15501355013550136,
      "grad_norm": 19.2498779296875,
      "learning_rate": 1e-05,
      "loss": 6.522,
      "step": 2860
    },
    {
      "epoch": 0.15501355013550136,
      "step": 2860,
      "training_loss": 5.746329307556152
    },
    {
      "epoch": 0.15506775067750678,
      "step": 2861,
      "training_loss": 8.426191329956055
    },
    {
      "epoch": 0.1551219512195122,
      "step": 2862,
      "training_loss": 6.7994866371154785
    },
    {
      "epoch": 0.1551761517615176,
      "step": 2863,
      "training_loss": 5.427798271179199
    },
    {
      "epoch": 0.15523035230352303,
      "grad_norm": 46.90581130981445,
      "learning_rate": 1e-05,
      "loss": 6.6,
      "step": 2864
    },
    {
      "epoch": 0.15523035230352303,
      "step": 2864,
      "training_loss": 5.347766876220703
    },
    {
      "epoch": 0.15528455284552845,
      "step": 2865,
      "training_loss": 7.108022689819336
    },
    {
      "epoch": 0.15533875338753386,
      "step": 2866,
      "training_loss": 7.885133266448975
    },
    {
      "epoch": 0.1553929539295393,
      "step": 2867,
      "training_loss": 7.103589057922363
    },
    {
      "epoch": 0.15544715447154472,
      "grad_norm": 27.888778686523438,
      "learning_rate": 1e-05,
      "loss": 6.8611,
      "step": 2868
    },
    {
      "epoch": 0.15544715447154472,
      "step": 2868,
      "training_loss": 6.86094331741333
    },
    {
      "epoch": 0.15550135501355014,
      "step": 2869,
      "training_loss": 6.1615986824035645
    },
    {
      "epoch": 0.15555555555555556,
      "step": 2870,
      "training_loss": 5.908816337585449
    },
    {
      "epoch": 0.15560975609756098,
      "step": 2871,
      "training_loss": 7.663077354431152
    },
    {
      "epoch": 0.1556639566395664,
      "grad_norm": 34.526859283447266,
      "learning_rate": 1e-05,
      "loss": 6.6486,
      "step": 2872
    },
    {
      "epoch": 0.1556639566395664,
      "step": 2872,
      "training_loss": 6.351736068725586
    },
    {
      "epoch": 0.1557181571815718,
      "step": 2873,
      "training_loss": 5.865800380706787
    },
    {
      "epoch": 0.15577235772357723,
      "step": 2874,
      "training_loss": 6.7286224365234375
    },
    {
      "epoch": 0.15582655826558264,
      "step": 2875,
      "training_loss": 6.431277275085449
    },
    {
      "epoch": 0.1558807588075881,
      "grad_norm": 20.1003475189209,
      "learning_rate": 1e-05,
      "loss": 6.3444,
      "step": 2876
    },
    {
      "epoch": 0.1558807588075881,
      "step": 2876,
      "training_loss": 6.754427909851074
    },
    {
      "epoch": 0.1559349593495935,
      "step": 2877,
      "training_loss": 6.780934810638428
    },
    {
      "epoch": 0.15598915989159892,
      "step": 2878,
      "training_loss": 7.198507785797119
    },
    {
      "epoch": 0.15604336043360434,
      "step": 2879,
      "training_loss": 8.61989688873291
    },
    {
      "epoch": 0.15609756097560976,
      "grad_norm": 30.530954360961914,
      "learning_rate": 1e-05,
      "loss": 7.3384,
      "step": 2880
    },
    {
      "epoch": 0.15609756097560976,
      "step": 2880,
      "training_loss": 4.937347888946533
    },
    {
      "epoch": 0.15615176151761517,
      "step": 2881,
      "training_loss": 6.741906642913818
    },
    {
      "epoch": 0.1562059620596206,
      "step": 2882,
      "training_loss": 6.947394847869873
    },
    {
      "epoch": 0.156260162601626,
      "step": 2883,
      "training_loss": 7.205965518951416
    },
    {
      "epoch": 0.15631436314363142,
      "grad_norm": 19.211959838867188,
      "learning_rate": 1e-05,
      "loss": 6.4582,
      "step": 2884
    },
    {
      "epoch": 0.15631436314363142,
      "step": 2884,
      "training_loss": 6.8303608894348145
    },
    {
      "epoch": 0.15636856368563687,
      "step": 2885,
      "training_loss": 5.653949737548828
    },
    {
      "epoch": 0.15642276422764229,
      "step": 2886,
      "training_loss": 6.185618877410889
    },
    {
      "epoch": 0.1564769647696477,
      "step": 2887,
      "training_loss": 7.253644943237305
    },
    {
      "epoch": 0.15653116531165312,
      "grad_norm": 34.4869499206543,
      "learning_rate": 1e-05,
      "loss": 6.4809,
      "step": 2888
    },
    {
      "epoch": 0.15653116531165312,
      "step": 2888,
      "training_loss": 7.342574119567871
    },
    {
      "epoch": 0.15658536585365854,
      "step": 2889,
      "training_loss": 6.886693477630615
    },
    {
      "epoch": 0.15663956639566395,
      "step": 2890,
      "training_loss": 6.285322666168213
    },
    {
      "epoch": 0.15669376693766937,
      "step": 2891,
      "training_loss": 5.830822944641113
    },
    {
      "epoch": 0.1567479674796748,
      "grad_norm": 22.661685943603516,
      "learning_rate": 1e-05,
      "loss": 6.5864,
      "step": 2892
    },
    {
      "epoch": 0.1567479674796748,
      "step": 2892,
      "training_loss": 6.716421604156494
    },
    {
      "epoch": 0.1568021680216802,
      "step": 2893,
      "training_loss": 7.420016765594482
    },
    {
      "epoch": 0.15685636856368565,
      "step": 2894,
      "training_loss": 7.3783063888549805
    },
    {
      "epoch": 0.15691056910569107,
      "step": 2895,
      "training_loss": 5.9821038246154785
    },
    {
      "epoch": 0.15696476964769648,
      "grad_norm": 19.748342514038086,
      "learning_rate": 1e-05,
      "loss": 6.8742,
      "step": 2896
    },
    {
      "epoch": 0.15696476964769648,
      "step": 2896,
      "training_loss": 6.272614002227783
    },
    {
      "epoch": 0.1570189701897019,
      "step": 2897,
      "training_loss": 6.163705825805664
    },
    {
      "epoch": 0.15707317073170732,
      "step": 2898,
      "training_loss": 7.571549415588379
    },
    {
      "epoch": 0.15712737127371273,
      "step": 2899,
      "training_loss": 6.90871000289917
    },
    {
      "epoch": 0.15718157181571815,
      "grad_norm": 33.41717529296875,
      "learning_rate": 1e-05,
      "loss": 6.7291,
      "step": 2900
    },
    {
      "epoch": 0.15718157181571815,
      "step": 2900,
      "training_loss": 5.12941837310791
    },
    {
      "epoch": 0.15723577235772357,
      "step": 2901,
      "training_loss": 7.404757499694824
    },
    {
      "epoch": 0.15728997289972899,
      "step": 2902,
      "training_loss": 7.612482070922852
    },
    {
      "epoch": 0.15734417344173443,
      "step": 2903,
      "training_loss": 4.866520881652832
    },
    {
      "epoch": 0.15739837398373985,
      "grad_norm": 27.730783462524414,
      "learning_rate": 1e-05,
      "loss": 6.2533,
      "step": 2904
    },
    {
      "epoch": 0.15739837398373985,
      "step": 2904,
      "training_loss": 5.585017681121826
    },
    {
      "epoch": 0.15745257452574526,
      "step": 2905,
      "training_loss": 6.588736057281494
    },
    {
      "epoch": 0.15750677506775068,
      "step": 2906,
      "training_loss": 8.834848403930664
    },
    {
      "epoch": 0.1575609756097561,
      "step": 2907,
      "training_loss": 6.759108066558838
    },
    {
      "epoch": 0.15761517615176152,
      "grad_norm": 19.158987045288086,
      "learning_rate": 1e-05,
      "loss": 6.9419,
      "step": 2908
    },
    {
      "epoch": 0.15761517615176152,
      "step": 2908,
      "training_loss": 6.351749420166016
    },
    {
      "epoch": 0.15766937669376693,
      "step": 2909,
      "training_loss": 6.403289794921875
    },
    {
      "epoch": 0.15772357723577235,
      "step": 2910,
      "training_loss": 6.54787015914917
    },
    {
      "epoch": 0.15777777777777777,
      "step": 2911,
      "training_loss": 7.141826152801514
    },
    {
      "epoch": 0.1578319783197832,
      "grad_norm": 31.344697952270508,
      "learning_rate": 1e-05,
      "loss": 6.6112,
      "step": 2912
    },
    {
      "epoch": 0.1578319783197832,
      "step": 2912,
      "training_loss": 7.155374526977539
    },
    {
      "epoch": 0.15788617886178863,
      "step": 2913,
      "training_loss": 7.525177955627441
    },
    {
      "epoch": 0.15794037940379405,
      "step": 2914,
      "training_loss": 6.3772430419921875
    },
    {
      "epoch": 0.15799457994579946,
      "step": 2915,
      "training_loss": 7.657386779785156
    },
    {
      "epoch": 0.15804878048780488,
      "grad_norm": 24.08736801147461,
      "learning_rate": 1e-05,
      "loss": 7.1788,
      "step": 2916
    },
    {
      "epoch": 0.15804878048780488,
      "step": 2916,
      "training_loss": 7.312280654907227
    },
    {
      "epoch": 0.1581029810298103,
      "step": 2917,
      "training_loss": 7.298388481140137
    },
    {
      "epoch": 0.1581571815718157,
      "step": 2918,
      "training_loss": 7.257785320281982
    },
    {
      "epoch": 0.15821138211382113,
      "step": 2919,
      "training_loss": 6.690280437469482
    },
    {
      "epoch": 0.15826558265582655,
      "grad_norm": 19.093996047973633,
      "learning_rate": 1e-05,
      "loss": 7.1397,
      "step": 2920
    },
    {
      "epoch": 0.15826558265582655,
      "step": 2920,
      "training_loss": 7.184230327606201
    },
    {
      "epoch": 0.15831978319783196,
      "step": 2921,
      "training_loss": 6.872853755950928
    },
    {
      "epoch": 0.1583739837398374,
      "step": 2922,
      "training_loss": 6.759646892547607
    },
    {
      "epoch": 0.15842818428184283,
      "step": 2923,
      "training_loss": 8.62765121459961
    },
    {
      "epoch": 0.15848238482384824,
      "grad_norm": 20.097896575927734,
      "learning_rate": 1e-05,
      "loss": 7.3611,
      "step": 2924
    },
    {
      "epoch": 0.15848238482384824,
      "step": 2924,
      "training_loss": 8.058176040649414
    },
    {
      "epoch": 0.15853658536585366,
      "step": 2925,
      "training_loss": 6.972200870513916
    },
    {
      "epoch": 0.15859078590785908,
      "step": 2926,
      "training_loss": 7.216691493988037
    },
    {
      "epoch": 0.1586449864498645,
      "step": 2927,
      "training_loss": 8.45913028717041
    },
    {
      "epoch": 0.1586991869918699,
      "grad_norm": 60.1237678527832,
      "learning_rate": 1e-05,
      "loss": 7.6765,
      "step": 2928
    },
    {
      "epoch": 0.1586991869918699,
      "step": 2928,
      "training_loss": 7.066335678100586
    },
    {
      "epoch": 0.15875338753387533,
      "step": 2929,
      "training_loss": 6.921389102935791
    },
    {
      "epoch": 0.15880758807588075,
      "step": 2930,
      "training_loss": 6.835501194000244
    },
    {
      "epoch": 0.1588617886178862,
      "step": 2931,
      "training_loss": 7.324372291564941
    },
    {
      "epoch": 0.1589159891598916,
      "grad_norm": 18.133073806762695,
      "learning_rate": 1e-05,
      "loss": 7.0369,
      "step": 2932
    },
    {
      "epoch": 0.1589159891598916,
      "step": 2932,
      "training_loss": 6.630540370941162
    },
    {
      "epoch": 0.15897018970189702,
      "step": 2933,
      "training_loss": 5.975952625274658
    },
    {
      "epoch": 0.15902439024390244,
      "step": 2934,
      "training_loss": 8.555869102478027
    },
    {
      "epoch": 0.15907859078590786,
      "step": 2935,
      "training_loss": 5.589618682861328
    },
    {
      "epoch": 0.15913279132791328,
      "grad_norm": 27.929931640625,
      "learning_rate": 1e-05,
      "loss": 6.688,
      "step": 2936
    },
    {
      "epoch": 0.15913279132791328,
      "step": 2936,
      "training_loss": 7.464033126831055
    },
    {
      "epoch": 0.1591869918699187,
      "step": 2937,
      "training_loss": 7.311155796051025
    },
    {
      "epoch": 0.1592411924119241,
      "step": 2938,
      "training_loss": 6.561821937561035
    },
    {
      "epoch": 0.15929539295392953,
      "step": 2939,
      "training_loss": 7.215827465057373
    },
    {
      "epoch": 0.15934959349593497,
      "grad_norm": 31.612863540649414,
      "learning_rate": 1e-05,
      "loss": 7.1382,
      "step": 2940
    },
    {
      "epoch": 0.15934959349593497,
      "step": 2940,
      "training_loss": 7.76384973526001
    },
    {
      "epoch": 0.1594037940379404,
      "step": 2941,
      "training_loss": 5.759899616241455
    },
    {
      "epoch": 0.1594579945799458,
      "step": 2942,
      "training_loss": 6.909980773925781
    },
    {
      "epoch": 0.15951219512195122,
      "step": 2943,
      "training_loss": 5.849776744842529
    },
    {
      "epoch": 0.15956639566395664,
      "grad_norm": 23.873563766479492,
      "learning_rate": 1e-05,
      "loss": 6.5709,
      "step": 2944
    },
    {
      "epoch": 0.15956639566395664,
      "step": 2944,
      "training_loss": 7.537816524505615
    },
    {
      "epoch": 0.15962059620596206,
      "step": 2945,
      "training_loss": 6.690806865692139
    },
    {
      "epoch": 0.15967479674796747,
      "step": 2946,
      "training_loss": 6.7462849617004395
    },
    {
      "epoch": 0.1597289972899729,
      "step": 2947,
      "training_loss": 6.3421173095703125
    },
    {
      "epoch": 0.1597831978319783,
      "grad_norm": 28.810468673706055,
      "learning_rate": 1e-05,
      "loss": 6.8293,
      "step": 2948
    },
    {
      "epoch": 0.1597831978319783,
      "step": 2948,
      "training_loss": 6.755227088928223
    },
    {
      "epoch": 0.15983739837398375,
      "step": 2949,
      "training_loss": 6.545165538787842
    },
    {
      "epoch": 0.15989159891598917,
      "step": 2950,
      "training_loss": 7.795363426208496
    },
    {
      "epoch": 0.1599457994579946,
      "step": 2951,
      "training_loss": 6.14130163192749
    },
    {
      "epoch": 0.16,
      "grad_norm": 15.153792381286621,
      "learning_rate": 1e-05,
      "loss": 6.8093,
      "step": 2952
    },
    {
      "epoch": 0.16,
      "step": 2952,
      "training_loss": 7.396259784698486
    },
    {
      "epoch": 0.16005420054200542,
      "step": 2953,
      "training_loss": 7.645601272583008
    },
    {
      "epoch": 0.16010840108401084,
      "step": 2954,
      "training_loss": 7.682761192321777
    },
    {
      "epoch": 0.16016260162601625,
      "step": 2955,
      "training_loss": 6.86782693862915
    },
    {
      "epoch": 0.16021680216802167,
      "grad_norm": 22.44603157043457,
      "learning_rate": 1e-05,
      "loss": 7.3981,
      "step": 2956
    },
    {
      "epoch": 0.16021680216802167,
      "step": 2956,
      "training_loss": 6.718705177307129
    },
    {
      "epoch": 0.1602710027100271,
      "step": 2957,
      "training_loss": 7.335902214050293
    },
    {
      "epoch": 0.16032520325203253,
      "step": 2958,
      "training_loss": 5.903501510620117
    },
    {
      "epoch": 0.16037940379403795,
      "step": 2959,
      "training_loss": 7.022304058074951
    },
    {
      "epoch": 0.16043360433604337,
      "grad_norm": 16.520267486572266,
      "learning_rate": 1e-05,
      "loss": 6.7451,
      "step": 2960
    },
    {
      "epoch": 0.16043360433604337,
      "step": 2960,
      "training_loss": 6.148603439331055
    },
    {
      "epoch": 0.16048780487804878,
      "step": 2961,
      "training_loss": 6.776230812072754
    },
    {
      "epoch": 0.1605420054200542,
      "step": 2962,
      "training_loss": 7.218337535858154
    },
    {
      "epoch": 0.16059620596205962,
      "step": 2963,
      "training_loss": 5.7717790603637695
    },
    {
      "epoch": 0.16065040650406504,
      "grad_norm": 17.16210174560547,
      "learning_rate": 1e-05,
      "loss": 6.4787,
      "step": 2964
    },
    {
      "epoch": 0.16065040650406504,
      "step": 2964,
      "training_loss": 7.78719425201416
    },
    {
      "epoch": 0.16070460704607045,
      "step": 2965,
      "training_loss": 5.902966022491455
    },
    {
      "epoch": 0.16075880758807587,
      "step": 2966,
      "training_loss": 6.517284393310547
    },
    {
      "epoch": 0.16081300813008131,
      "step": 2967,
      "training_loss": 4.077859401702881
    },
    {
      "epoch": 0.16086720867208673,
      "grad_norm": 23.341978073120117,
      "learning_rate": 1e-05,
      "loss": 6.0713,
      "step": 2968
    },
    {
      "epoch": 0.16086720867208673,
      "step": 2968,
      "training_loss": 6.580926418304443
    },
    {
      "epoch": 0.16092140921409215,
      "step": 2969,
      "training_loss": 7.023199558258057
    },
    {
      "epoch": 0.16097560975609757,
      "step": 2970,
      "training_loss": 7.310860633850098
    },
    {
      "epoch": 0.16102981029810298,
      "step": 2971,
      "training_loss": 6.431434631347656
    },
    {
      "epoch": 0.1610840108401084,
      "grad_norm": 33.148582458496094,
      "learning_rate": 1e-05,
      "loss": 6.8366,
      "step": 2972
    },
    {
      "epoch": 0.1610840108401084,
      "step": 2972,
      "training_loss": 5.974183559417725
    },
    {
      "epoch": 0.16113821138211382,
      "step": 2973,
      "training_loss": 5.247602462768555
    },
    {
      "epoch": 0.16119241192411923,
      "step": 2974,
      "training_loss": 7.360989093780518
    },
    {
      "epoch": 0.16124661246612465,
      "step": 2975,
      "training_loss": 7.905933856964111
    },
    {
      "epoch": 0.1613008130081301,
      "grad_norm": 18.197437286376953,
      "learning_rate": 1e-05,
      "loss": 6.6222,
      "step": 2976
    },
    {
      "epoch": 0.1613008130081301,
      "step": 2976,
      "training_loss": 6.763075351715088
    },
    {
      "epoch": 0.1613550135501355,
      "step": 2977,
      "training_loss": 6.837372303009033
    },
    {
      "epoch": 0.16140921409214093,
      "step": 2978,
      "training_loss": 6.511440277099609
    },
    {
      "epoch": 0.16146341463414635,
      "step": 2979,
      "training_loss": 6.540027618408203
    },
    {
      "epoch": 0.16151761517615176,
      "grad_norm": 21.105897903442383,
      "learning_rate": 1e-05,
      "loss": 6.663,
      "step": 2980
    },
    {
      "epoch": 0.16151761517615176,
      "step": 2980,
      "training_loss": 7.701763153076172
    },
    {
      "epoch": 0.16157181571815718,
      "step": 2981,
      "training_loss": 6.073128700256348
    },
    {
      "epoch": 0.1616260162601626,
      "step": 2982,
      "training_loss": 6.560590744018555
    },
    {
      "epoch": 0.16168021680216801,
      "step": 2983,
      "training_loss": 7.808140277862549
    },
    {
      "epoch": 0.16173441734417343,
      "grad_norm": 30.810598373413086,
      "learning_rate": 1e-05,
      "loss": 7.0359,
      "step": 2984
    },
    {
      "epoch": 0.16173441734417343,
      "step": 2984,
      "training_loss": 7.014292240142822
    },
    {
      "epoch": 0.16178861788617885,
      "step": 2985,
      "training_loss": 7.17975378036499
    },
    {
      "epoch": 0.1618428184281843,
      "step": 2986,
      "training_loss": 6.069551944732666
    },
    {
      "epoch": 0.1618970189701897,
      "step": 2987,
      "training_loss": 7.328714847564697
    },
    {
      "epoch": 0.16195121951219513,
      "grad_norm": 14.939138412475586,
      "learning_rate": 1e-05,
      "loss": 6.8981,
      "step": 2988
    },
    {
      "epoch": 0.16195121951219513,
      "step": 2988,
      "training_loss": 7.748476982116699
    },
    {
      "epoch": 0.16200542005420054,
      "step": 2989,
      "training_loss": 5.465795040130615
    },
    {
      "epoch": 0.16205962059620596,
      "step": 2990,
      "training_loss": 7.206252574920654
    },
    {
      "epoch": 0.16211382113821138,
      "step": 2991,
      "training_loss": 6.936278820037842
    },
    {
      "epoch": 0.1621680216802168,
      "grad_norm": 15.578089714050293,
      "learning_rate": 1e-05,
      "loss": 6.8392,
      "step": 2992
    },
    {
      "epoch": 0.1621680216802168,
      "step": 2992,
      "training_loss": 7.004034996032715
    },
    {
      "epoch": 0.1622222222222222,
      "step": 2993,
      "training_loss": 7.50494909286499
    },
    {
      "epoch": 0.16227642276422763,
      "step": 2994,
      "training_loss": 7.0521392822265625
    },
    {
      "epoch": 0.16233062330623307,
      "step": 2995,
      "training_loss": 5.744019508361816
    },
    {
      "epoch": 0.1623848238482385,
      "grad_norm": 25.260517120361328,
      "learning_rate": 1e-05,
      "loss": 6.8263,
      "step": 2996
    },
    {
      "epoch": 0.1623848238482385,
      "step": 2996,
      "training_loss": 6.471571922302246
    },
    {
      "epoch": 0.1624390243902439,
      "step": 2997,
      "training_loss": 7.002123832702637
    },
    {
      "epoch": 0.16249322493224932,
      "step": 2998,
      "training_loss": 7.546938896179199
    },
    {
      "epoch": 0.16254742547425474,
      "step": 2999,
      "training_loss": 7.516677379608154
    },
    {
      "epoch": 0.16260162601626016,
      "grad_norm": 20.08682632446289,
      "learning_rate": 1e-05,
      "loss": 7.1343,
      "step": 3000
    },
    {
      "epoch": 0.16260162601626016,
      "step": 3000,
      "training_loss": 5.769819259643555
    },
    {
      "epoch": 0.16265582655826558,
      "step": 3001,
      "training_loss": 10.680994987487793
    },
    {
      "epoch": 0.162710027100271,
      "step": 3002,
      "training_loss": 6.731485843658447
    },
    {
      "epoch": 0.1627642276422764,
      "step": 3003,
      "training_loss": 6.923071384429932
    },
    {
      "epoch": 0.16281842818428185,
      "grad_norm": 14.367572784423828,
      "learning_rate": 1e-05,
      "loss": 7.5263,
      "step": 3004
    },
    {
      "epoch": 0.16281842818428185,
      "step": 3004,
      "training_loss": 7.621476173400879
    },
    {
      "epoch": 0.16287262872628727,
      "step": 3005,
      "training_loss": 7.175113201141357
    },
    {
      "epoch": 0.1629268292682927,
      "step": 3006,
      "training_loss": 7.4440999031066895
    },
    {
      "epoch": 0.1629810298102981,
      "step": 3007,
      "training_loss": 7.67173433303833
    },
    {
      "epoch": 0.16303523035230352,
      "grad_norm": 23.191003799438477,
      "learning_rate": 1e-05,
      "loss": 7.4781,
      "step": 3008
    },
    {
      "epoch": 0.16303523035230352,
      "step": 3008,
      "training_loss": 7.437249183654785
    },
    {
      "epoch": 0.16308943089430894,
      "step": 3009,
      "training_loss": 7.108950138092041
    },
    {
      "epoch": 0.16314363143631436,
      "step": 3010,
      "training_loss": 5.40097188949585
    },
    {
      "epoch": 0.16319783197831977,
      "step": 3011,
      "training_loss": 6.818798542022705
    },
    {
      "epoch": 0.1632520325203252,
      "grad_norm": 24.684803009033203,
      "learning_rate": 1e-05,
      "loss": 6.6915,
      "step": 3012
    },
    {
      "epoch": 0.1632520325203252,
      "step": 3012,
      "training_loss": 7.505446434020996
    },
    {
      "epoch": 0.16330623306233064,
      "step": 3013,
      "training_loss": 7.148746490478516
    },
    {
      "epoch": 0.16336043360433605,
      "step": 3014,
      "training_loss": 6.43250846862793
    },
    {
      "epoch": 0.16341463414634147,
      "step": 3015,
      "training_loss": 6.915080547332764
    },
    {
      "epoch": 0.1634688346883469,
      "grad_norm": 23.25904655456543,
      "learning_rate": 1e-05,
      "loss": 7.0004,
      "step": 3016
    },
    {
      "epoch": 0.1634688346883469,
      "step": 3016,
      "training_loss": 7.0306196212768555
    },
    {
      "epoch": 0.1635230352303523,
      "step": 3017,
      "training_loss": 6.743037700653076
    },
    {
      "epoch": 0.16357723577235772,
      "step": 3018,
      "training_loss": 6.103106498718262
    },
    {
      "epoch": 0.16363143631436314,
      "step": 3019,
      "training_loss": 6.64677619934082
    },
    {
      "epoch": 0.16368563685636855,
      "grad_norm": 20.784542083740234,
      "learning_rate": 1e-05,
      "loss": 6.6309,
      "step": 3020
    },
    {
      "epoch": 0.16368563685636855,
      "step": 3020,
      "training_loss": 7.416233539581299
    },
    {
      "epoch": 0.16373983739837397,
      "step": 3021,
      "training_loss": 7.746795177459717
    },
    {
      "epoch": 0.16379403794037942,
      "step": 3022,
      "training_loss": 7.151573657989502
    },
    {
      "epoch": 0.16384823848238483,
      "step": 3023,
      "training_loss": 6.856207370758057
    },
    {
      "epoch": 0.16390243902439025,
      "grad_norm": 15.727557182312012,
      "learning_rate": 1e-05,
      "loss": 7.2927,
      "step": 3024
    },
    {
      "epoch": 0.16390243902439025,
      "step": 3024,
      "training_loss": 6.165416240692139
    },
    {
      "epoch": 0.16395663956639567,
      "step": 3025,
      "training_loss": 8.213289260864258
    },
    {
      "epoch": 0.16401084010840108,
      "step": 3026,
      "training_loss": 6.973206996917725
    },
    {
      "epoch": 0.1640650406504065,
      "step": 3027,
      "training_loss": 6.507347106933594
    },
    {
      "epoch": 0.16411924119241192,
      "grad_norm": 27.191898345947266,
      "learning_rate": 1e-05,
      "loss": 6.9648,
      "step": 3028
    },
    {
      "epoch": 0.16411924119241192,
      "step": 3028,
      "training_loss": 7.474644184112549
    },
    {
      "epoch": 0.16417344173441734,
      "step": 3029,
      "training_loss": 6.960941314697266
    },
    {
      "epoch": 0.16422764227642275,
      "step": 3030,
      "training_loss": 7.219117641448975
    },
    {
      "epoch": 0.1642818428184282,
      "step": 3031,
      "training_loss": 7.206991195678711
    },
    {
      "epoch": 0.16433604336043361,
      "grad_norm": 30.17197608947754,
      "learning_rate": 1e-05,
      "loss": 7.2154,
      "step": 3032
    },
    {
      "epoch": 0.16433604336043361,
      "step": 3032,
      "training_loss": 6.730391025543213
    },
    {
      "epoch": 0.16439024390243903,
      "step": 3033,
      "training_loss": 4.737142086029053
    },
    {
      "epoch": 0.16444444444444445,
      "step": 3034,
      "training_loss": 6.7536725997924805
    },
    {
      "epoch": 0.16449864498644987,
      "step": 3035,
      "training_loss": 6.878812313079834
    },
    {
      "epoch": 0.16455284552845528,
      "grad_norm": 38.94382858276367,
      "learning_rate": 1e-05,
      "loss": 6.275,
      "step": 3036
    },
    {
      "epoch": 0.16455284552845528,
      "step": 3036,
      "training_loss": 6.4678215980529785
    },
    {
      "epoch": 0.1646070460704607,
      "step": 3037,
      "training_loss": 6.912356853485107
    },
    {
      "epoch": 0.16466124661246612,
      "step": 3038,
      "training_loss": 4.957960605621338
    },
    {
      "epoch": 0.16471544715447153,
      "step": 3039,
      "training_loss": 7.39825963973999
    },
    {
      "epoch": 0.16476964769647698,
      "grad_norm": 41.762451171875,
      "learning_rate": 1e-05,
      "loss": 6.4341,
      "step": 3040
    },
    {
      "epoch": 0.16476964769647698,
      "step": 3040,
      "training_loss": 7.542705535888672
    },
    {
      "epoch": 0.1648238482384824,
      "step": 3041,
      "training_loss": 5.586994171142578
    },
    {
      "epoch": 0.1648780487804878,
      "step": 3042,
      "training_loss": 6.544967174530029
    },
    {
      "epoch": 0.16493224932249323,
      "step": 3043,
      "training_loss": 7.472884178161621
    },
    {
      "epoch": 0.16498644986449865,
      "grad_norm": 33.550743103027344,
      "learning_rate": 1e-05,
      "loss": 6.7869,
      "step": 3044
    },
    {
      "epoch": 0.16498644986449865,
      "step": 3044,
      "training_loss": 7.897436141967773
    },
    {
      "epoch": 0.16504065040650406,
      "step": 3045,
      "training_loss": 7.175792694091797
    },
    {
      "epoch": 0.16509485094850948,
      "step": 3046,
      "training_loss": 4.800732135772705
    },
    {
      "epoch": 0.1651490514905149,
      "step": 3047,
      "training_loss": 6.889263153076172
    },
    {
      "epoch": 0.16520325203252031,
      "grad_norm": 20.25153923034668,
      "learning_rate": 1e-05,
      "loss": 6.6908,
      "step": 3048
    },
    {
      "epoch": 0.16520325203252031,
      "step": 3048,
      "training_loss": 7.55864953994751
    },
    {
      "epoch": 0.16525745257452573,
      "step": 3049,
      "training_loss": 4.602255821228027
    },
    {
      "epoch": 0.16531165311653118,
      "step": 3050,
      "training_loss": 6.225058555603027
    },
    {
      "epoch": 0.1653658536585366,
      "step": 3051,
      "training_loss": 6.786135673522949
    },
    {
      "epoch": 0.165420054200542,
      "grad_norm": 27.394487380981445,
      "learning_rate": 1e-05,
      "loss": 6.293,
      "step": 3052
    },
    {
      "epoch": 0.165420054200542,
      "step": 3052,
      "training_loss": 6.06257963180542
    },
    {
      "epoch": 0.16547425474254743,
      "step": 3053,
      "training_loss": 6.010027885437012
    },
    {
      "epoch": 0.16552845528455284,
      "step": 3054,
      "training_loss": 7.60477876663208
    },
    {
      "epoch": 0.16558265582655826,
      "step": 3055,
      "training_loss": 6.959472179412842
    },
    {
      "epoch": 0.16563685636856368,
      "grad_norm": 22.176677703857422,
      "learning_rate": 1e-05,
      "loss": 6.6592,
      "step": 3056
    },
    {
      "epoch": 0.16563685636856368,
      "step": 3056,
      "training_loss": 7.926078796386719
    },
    {
      "epoch": 0.1656910569105691,
      "step": 3057,
      "training_loss": 8.182756423950195
    },
    {
      "epoch": 0.1657452574525745,
      "step": 3058,
      "training_loss": 5.917239189147949
    },
    {
      "epoch": 0.16579945799457996,
      "step": 3059,
      "training_loss": 6.761448383331299
    },
    {
      "epoch": 0.16585365853658537,
      "grad_norm": 24.537160873413086,
      "learning_rate": 1e-05,
      "loss": 7.1969,
      "step": 3060
    },
    {
      "epoch": 0.16585365853658537,
      "step": 3060,
      "training_loss": 7.081878185272217
    },
    {
      "epoch": 0.1659078590785908,
      "step": 3061,
      "training_loss": 7.328185558319092
    },
    {
      "epoch": 0.1659620596205962,
      "step": 3062,
      "training_loss": 6.369275093078613
    },
    {
      "epoch": 0.16601626016260163,
      "step": 3063,
      "training_loss": 6.763922691345215
    },
    {
      "epoch": 0.16607046070460704,
      "grad_norm": 31.511489868164062,
      "learning_rate": 1e-05,
      "loss": 6.8858,
      "step": 3064
    },
    {
      "epoch": 0.16607046070460704,
      "step": 3064,
      "training_loss": 7.178269863128662
    },
    {
      "epoch": 0.16612466124661246,
      "step": 3065,
      "training_loss": 6.631086349487305
    },
    {
      "epoch": 0.16617886178861788,
      "step": 3066,
      "training_loss": 4.1674723625183105
    },
    {
      "epoch": 0.1662330623306233,
      "step": 3067,
      "training_loss": 6.83041524887085
    },
    {
      "epoch": 0.16628726287262874,
      "grad_norm": 18.849241256713867,
      "learning_rate": 1e-05,
      "loss": 6.2018,
      "step": 3068
    },
    {
      "epoch": 0.16628726287262874,
      "step": 3068,
      "training_loss": 7.932281494140625
    },
    {
      "epoch": 0.16634146341463416,
      "step": 3069,
      "training_loss": 6.995041370391846
    },
    {
      "epoch": 0.16639566395663957,
      "step": 3070,
      "training_loss": 4.595454216003418
    },
    {
      "epoch": 0.166449864498645,
      "step": 3071,
      "training_loss": 6.943033695220947
    },
    {
      "epoch": 0.1665040650406504,
      "grad_norm": 17.41122817993164,
      "learning_rate": 1e-05,
      "loss": 6.6165,
      "step": 3072
    },
    {
      "epoch": 0.1665040650406504,
      "step": 3072,
      "training_loss": 8.204055786132812
    },
    {
      "epoch": 0.16655826558265582,
      "step": 3073,
      "training_loss": 8.037402153015137
    },
    {
      "epoch": 0.16661246612466124,
      "step": 3074,
      "training_loss": 5.65858268737793
    },
    {
      "epoch": 0.16666666666666666,
      "step": 3075,
      "training_loss": 7.237144947052002
    },
    {
      "epoch": 0.16672086720867207,
      "grad_norm": 21.637319564819336,
      "learning_rate": 1e-05,
      "loss": 7.2843,
      "step": 3076
    },
    {
      "epoch": 0.16672086720867207,
      "step": 3076,
      "training_loss": 7.160490989685059
    },
    {
      "epoch": 0.16677506775067752,
      "step": 3077,
      "training_loss": 4.883201599121094
    },
    {
      "epoch": 0.16682926829268294,
      "step": 3078,
      "training_loss": 7.745425701141357
    },
    {
      "epoch": 0.16688346883468835,
      "step": 3079,
      "training_loss": 7.029091835021973
    },
    {
      "epoch": 0.16693766937669377,
      "grad_norm": 36.4002571105957,
      "learning_rate": 1e-05,
      "loss": 6.7046,
      "step": 3080
    },
    {
      "epoch": 0.16693766937669377,
      "step": 3080,
      "training_loss": 5.811226844787598
    },
    {
      "epoch": 0.1669918699186992,
      "step": 3081,
      "training_loss": 7.73484468460083
    },
    {
      "epoch": 0.1670460704607046,
      "step": 3082,
      "training_loss": 5.814998149871826
    },
    {
      "epoch": 0.16710027100271002,
      "step": 3083,
      "training_loss": 7.497524738311768
    },
    {
      "epoch": 0.16715447154471544,
      "grad_norm": 25.36642074584961,
      "learning_rate": 1e-05,
      "loss": 6.7146,
      "step": 3084
    },
    {
      "epoch": 0.16715447154471544,
      "step": 3084,
      "training_loss": 8.101901054382324
    },
    {
      "epoch": 0.16720867208672086,
      "step": 3085,
      "training_loss": 6.929892539978027
    },
    {
      "epoch": 0.1672628726287263,
      "step": 3086,
      "training_loss": 6.97938871383667
    },
    {
      "epoch": 0.16731707317073172,
      "step": 3087,
      "training_loss": 7.118065357208252
    },
    {
      "epoch": 0.16737127371273713,
      "grad_norm": 15.914959907531738,
      "learning_rate": 1e-05,
      "loss": 7.2823,
      "step": 3088
    },
    {
      "epoch": 0.16737127371273713,
      "step": 3088,
      "training_loss": 6.880626678466797
    },
    {
      "epoch": 0.16742547425474255,
      "step": 3089,
      "training_loss": 5.085703372955322
    },
    {
      "epoch": 0.16747967479674797,
      "step": 3090,
      "training_loss": 7.088844299316406
    },
    {
      "epoch": 0.16753387533875339,
      "step": 3091,
      "training_loss": 5.724661827087402
    },
    {
      "epoch": 0.1675880758807588,
      "grad_norm": 25.180042266845703,
      "learning_rate": 1e-05,
      "loss": 6.195,
      "step": 3092
    },
    {
      "epoch": 0.1675880758807588,
      "step": 3092,
      "training_loss": 6.92713737487793
    },
    {
      "epoch": 0.16764227642276422,
      "step": 3093,
      "training_loss": 7.261476039886475
    },
    {
      "epoch": 0.16769647696476964,
      "step": 3094,
      "training_loss": 6.161435127258301
    },
    {
      "epoch": 0.16775067750677508,
      "step": 3095,
      "training_loss": 6.982956409454346
    },
    {
      "epoch": 0.1678048780487805,
      "grad_norm": 17.577503204345703,
      "learning_rate": 1e-05,
      "loss": 6.8333,
      "step": 3096
    },
    {
      "epoch": 0.1678048780487805,
      "step": 3096,
      "training_loss": 7.264886856079102
    },
    {
      "epoch": 0.16785907859078592,
      "step": 3097,
      "training_loss": 6.866893291473389
    },
    {
      "epoch": 0.16791327913279133,
      "step": 3098,
      "training_loss": 7.713854789733887
    },
    {
      "epoch": 0.16796747967479675,
      "step": 3099,
      "training_loss": 5.279563903808594
    },
    {
      "epoch": 0.16802168021680217,
      "grad_norm": 47.223445892333984,
      "learning_rate": 1e-05,
      "loss": 6.7813,
      "step": 3100
    },
    {
      "epoch": 0.16802168021680217,
      "step": 3100,
      "training_loss": 5.647733211517334
    },
    {
      "epoch": 0.16807588075880758,
      "step": 3101,
      "training_loss": 7.685414791107178
    },
    {
      "epoch": 0.168130081300813,
      "step": 3102,
      "training_loss": 7.0499091148376465
    },
    {
      "epoch": 0.16818428184281842,
      "step": 3103,
      "training_loss": 7.747322082519531
    },
    {
      "epoch": 0.16823848238482386,
      "grad_norm": 16.89069175720215,
      "learning_rate": 1e-05,
      "loss": 7.0326,
      "step": 3104
    },
    {
      "epoch": 0.16823848238482386,
      "step": 3104,
      "training_loss": 6.98338508605957
    },
    {
      "epoch": 0.16829268292682928,
      "step": 3105,
      "training_loss": 6.849808216094971
    },
    {
      "epoch": 0.1683468834688347,
      "step": 3106,
      "training_loss": 6.071924686431885
    },
    {
      "epoch": 0.1684010840108401,
      "step": 3107,
      "training_loss": 6.734445095062256
    },
    {
      "epoch": 0.16845528455284553,
      "grad_norm": 23.535348892211914,
      "learning_rate": 1e-05,
      "loss": 6.6599,
      "step": 3108
    },
    {
      "epoch": 0.16845528455284553,
      "step": 3108,
      "training_loss": 7.023128032684326
    },
    {
      "epoch": 0.16850948509485095,
      "step": 3109,
      "training_loss": 4.155453681945801
    },
    {
      "epoch": 0.16856368563685636,
      "step": 3110,
      "training_loss": 4.943539619445801
    },
    {
      "epoch": 0.16861788617886178,
      "step": 3111,
      "training_loss": 7.421543121337891
    },
    {
      "epoch": 0.1686720867208672,
      "grad_norm": 20.524398803710938,
      "learning_rate": 1e-05,
      "loss": 5.8859,
      "step": 3112
    },
    {
      "epoch": 0.1686720867208672,
      "step": 3112,
      "training_loss": 5.883206844329834
    },
    {
      "epoch": 0.16872628726287262,
      "step": 3113,
      "training_loss": 5.795571327209473
    },
    {
      "epoch": 0.16878048780487806,
      "step": 3114,
      "training_loss": 6.883586406707764
    },
    {
      "epoch": 0.16883468834688348,
      "step": 3115,
      "training_loss": 7.044589519500732
    },
    {
      "epoch": 0.1688888888888889,
      "grad_norm": 17.307992935180664,
      "learning_rate": 1e-05,
      "loss": 6.4017,
      "step": 3116
    },
    {
      "epoch": 0.1688888888888889,
      "step": 3116,
      "training_loss": 7.030011177062988
    },
    {
      "epoch": 0.1689430894308943,
      "step": 3117,
      "training_loss": 6.61893367767334
    },
    {
      "epoch": 0.16899728997289973,
      "step": 3118,
      "training_loss": 6.592992305755615
    },
    {
      "epoch": 0.16905149051490515,
      "step": 3119,
      "training_loss": 7.057188034057617
    },
    {
      "epoch": 0.16910569105691056,
      "grad_norm": 32.32083511352539,
      "learning_rate": 1e-05,
      "loss": 6.8248,
      "step": 3120
    },
    {
      "epoch": 0.16910569105691056,
      "step": 3120,
      "training_loss": 7.2521491050720215
    },
    {
      "epoch": 0.16915989159891598,
      "step": 3121,
      "training_loss": 9.490034103393555
    },
    {
      "epoch": 0.1692140921409214,
      "step": 3122,
      "training_loss": 8.063058853149414
    },
    {
      "epoch": 0.16926829268292684,
      "step": 3123,
      "training_loss": 7.575556755065918
    },
    {
      "epoch": 0.16932249322493226,
      "grad_norm": 20.32110595703125,
      "learning_rate": 1e-05,
      "loss": 8.0952,
      "step": 3124
    },
    {
      "epoch": 0.16932249322493226,
      "step": 3124,
      "training_loss": 7.333987712860107
    },
    {
      "epoch": 0.16937669376693767,
      "step": 3125,
      "training_loss": 7.037868022918701
    },
    {
      "epoch": 0.1694308943089431,
      "step": 3126,
      "training_loss": 6.261382579803467
    },
    {
      "epoch": 0.1694850948509485,
      "step": 3127,
      "training_loss": 6.719054698944092
    },
    {
      "epoch": 0.16953929539295393,
      "grad_norm": 21.622461318969727,
      "learning_rate": 1e-05,
      "loss": 6.8381,
      "step": 3128
    },
    {
      "epoch": 0.16953929539295393,
      "step": 3128,
      "training_loss": 8.399873733520508
    },
    {
      "epoch": 0.16959349593495934,
      "step": 3129,
      "training_loss": 6.6672563552856445
    },
    {
      "epoch": 0.16964769647696476,
      "step": 3130,
      "training_loss": 7.172022342681885
    },
    {
      "epoch": 0.16970189701897018,
      "step": 3131,
      "training_loss": 8.93308162689209
    },
    {
      "epoch": 0.16975609756097562,
      "grad_norm": 51.62116622924805,
      "learning_rate": 1e-05,
      "loss": 7.7931,
      "step": 3132
    },
    {
      "epoch": 0.16975609756097562,
      "step": 3132,
      "training_loss": 7.328136444091797
    },
    {
      "epoch": 0.16981029810298104,
      "step": 3133,
      "training_loss": 7.744343280792236
    },
    {
      "epoch": 0.16986449864498646,
      "step": 3134,
      "training_loss": 6.7554240226745605
    },
    {
      "epoch": 0.16991869918699187,
      "step": 3135,
      "training_loss": 7.3132100105285645
    },
    {
      "epoch": 0.1699728997289973,
      "grad_norm": 22.913816452026367,
      "learning_rate": 1e-05,
      "loss": 7.2853,
      "step": 3136
    },
    {
      "epoch": 0.1699728997289973,
      "step": 3136,
      "training_loss": 7.068224906921387
    },
    {
      "epoch": 0.1700271002710027,
      "step": 3137,
      "training_loss": 6.088888168334961
    },
    {
      "epoch": 0.17008130081300812,
      "step": 3138,
      "training_loss": 7.17848014831543
    },
    {
      "epoch": 0.17013550135501354,
      "step": 3139,
      "training_loss": 7.391305446624756
    },
    {
      "epoch": 0.17018970189701896,
      "grad_norm": 18.83468246459961,
      "learning_rate": 1e-05,
      "loss": 6.9317,
      "step": 3140
    },
    {
      "epoch": 0.17018970189701896,
      "step": 3140,
      "training_loss": 7.276265621185303
    },
    {
      "epoch": 0.1702439024390244,
      "step": 3141,
      "training_loss": 7.125787734985352
    },
    {
      "epoch": 0.17029810298102982,
      "step": 3142,
      "training_loss": 7.342701435089111
    },
    {
      "epoch": 0.17035230352303524,
      "step": 3143,
      "training_loss": 6.469883918762207
    },
    {
      "epoch": 0.17040650406504065,
      "grad_norm": 29.855716705322266,
      "learning_rate": 1e-05,
      "loss": 7.0537,
      "step": 3144
    },
    {
      "epoch": 0.17040650406504065,
      "step": 3144,
      "training_loss": 7.030792236328125
    },
    {
      "epoch": 0.17046070460704607,
      "step": 3145,
      "training_loss": 7.1502685546875
    },
    {
      "epoch": 0.1705149051490515,
      "step": 3146,
      "training_loss": 8.052061080932617
    },
    {
      "epoch": 0.1705691056910569,
      "step": 3147,
      "training_loss": 6.9613847732543945
    },
    {
      "epoch": 0.17062330623306232,
      "grad_norm": 28.797874450683594,
      "learning_rate": 1e-05,
      "loss": 7.2986,
      "step": 3148
    },
    {
      "epoch": 0.17062330623306232,
      "step": 3148,
      "training_loss": 7.702326774597168
    },
    {
      "epoch": 0.17067750677506774,
      "step": 3149,
      "training_loss": 8.024182319641113
    },
    {
      "epoch": 0.17073170731707318,
      "step": 3150,
      "training_loss": 5.567826747894287
    },
    {
      "epoch": 0.1707859078590786,
      "step": 3151,
      "training_loss": 6.948361396789551
    },
    {
      "epoch": 0.17084010840108402,
      "grad_norm": 21.152616500854492,
      "learning_rate": 1e-05,
      "loss": 7.0607,
      "step": 3152
    },
    {
      "epoch": 0.17084010840108402,
      "step": 3152,
      "training_loss": 5.869195938110352
    },
    {
      "epoch": 0.17089430894308943,
      "step": 3153,
      "training_loss": 6.852989673614502
    },
    {
      "epoch": 0.17094850948509485,
      "step": 3154,
      "training_loss": 7.8581743240356445
    },
    {
      "epoch": 0.17100271002710027,
      "step": 3155,
      "training_loss": 7.477871417999268
    },
    {
      "epoch": 0.17105691056910569,
      "grad_norm": 23.43837547302246,
      "learning_rate": 1e-05,
      "loss": 7.0146,
      "step": 3156
    },
    {
      "epoch": 0.17105691056910569,
      "step": 3156,
      "training_loss": 7.0970330238342285
    },
    {
      "epoch": 0.1711111111111111,
      "step": 3157,
      "training_loss": 5.651440620422363
    },
    {
      "epoch": 0.17116531165311652,
      "step": 3158,
      "training_loss": 7.003447532653809
    },
    {
      "epoch": 0.17121951219512196,
      "step": 3159,
      "training_loss": 6.804032325744629
    },
    {
      "epoch": 0.17127371273712738,
      "grad_norm": 29.328453063964844,
      "learning_rate": 1e-05,
      "loss": 6.639,
      "step": 3160
    },
    {
      "epoch": 0.17127371273712738,
      "step": 3160,
      "training_loss": 6.867862701416016
    },
    {
      "epoch": 0.1713279132791328,
      "step": 3161,
      "training_loss": 7.63116455078125
    },
    {
      "epoch": 0.17138211382113822,
      "step": 3162,
      "training_loss": 6.925065994262695
    },
    {
      "epoch": 0.17143631436314363,
      "step": 3163,
      "training_loss": 6.144101619720459
    },
    {
      "epoch": 0.17149051490514905,
      "grad_norm": 35.97827911376953,
      "learning_rate": 1e-05,
      "loss": 6.892,
      "step": 3164
    },
    {
      "epoch": 0.17149051490514905,
      "step": 3164,
      "training_loss": 7.160562992095947
    },
    {
      "epoch": 0.17154471544715447,
      "step": 3165,
      "training_loss": 7.436864376068115
    },
    {
      "epoch": 0.17159891598915988,
      "step": 3166,
      "training_loss": 7.45028018951416
    },
    {
      "epoch": 0.1716531165311653,
      "step": 3167,
      "training_loss": 6.950116157531738
    },
    {
      "epoch": 0.17170731707317075,
      "grad_norm": 22.69205665588379,
      "learning_rate": 1e-05,
      "loss": 7.2495,
      "step": 3168
    },
    {
      "epoch": 0.17170731707317075,
      "step": 3168,
      "training_loss": 7.836371421813965
    },
    {
      "epoch": 0.17176151761517616,
      "step": 3169,
      "training_loss": 6.688650131225586
    },
    {
      "epoch": 0.17181571815718158,
      "step": 3170,
      "training_loss": 7.153052806854248
    },
    {
      "epoch": 0.171869918699187,
      "step": 3171,
      "training_loss": 7.338819980621338
    },
    {
      "epoch": 0.1719241192411924,
      "grad_norm": 29.253459930419922,
      "learning_rate": 1e-05,
      "loss": 7.2542,
      "step": 3172
    },
    {
      "epoch": 0.1719241192411924,
      "step": 3172,
      "training_loss": 7.612550735473633
    },
    {
      "epoch": 0.17197831978319783,
      "step": 3173,
      "training_loss": 5.6997575759887695
    },
    {
      "epoch": 0.17203252032520325,
      "step": 3174,
      "training_loss": 6.6111578941345215
    },
    {
      "epoch": 0.17208672086720866,
      "step": 3175,
      "training_loss": 7.370857238769531
    },
    {
      "epoch": 0.17214092140921408,
      "grad_norm": 40.226863861083984,
      "learning_rate": 1e-05,
      "loss": 6.8236,
      "step": 3176
    },
    {
      "epoch": 0.17214092140921408,
      "step": 3176,
      "training_loss": 7.253020763397217
    },
    {
      "epoch": 0.1721951219512195,
      "step": 3177,
      "training_loss": 6.341847896575928
    },
    {
      "epoch": 0.17224932249322494,
      "step": 3178,
      "training_loss": 7.532234191894531
    },
    {
      "epoch": 0.17230352303523036,
      "step": 3179,
      "training_loss": 6.378076553344727
    },
    {
      "epoch": 0.17235772357723578,
      "grad_norm": 21.195556640625,
      "learning_rate": 1e-05,
      "loss": 6.8763,
      "step": 3180
    },
    {
      "epoch": 0.17235772357723578,
      "step": 3180,
      "training_loss": 7.553975582122803
    },
    {
      "epoch": 0.1724119241192412,
      "step": 3181,
      "training_loss": 6.838134765625
    },
    {
      "epoch": 0.1724661246612466,
      "step": 3182,
      "training_loss": 7.186484336853027
    },
    {
      "epoch": 0.17252032520325203,
      "step": 3183,
      "training_loss": 4.762709140777588
    },
    {
      "epoch": 0.17257452574525745,
      "grad_norm": 27.985027313232422,
      "learning_rate": 1e-05,
      "loss": 6.5853,
      "step": 3184
    },
    {
      "epoch": 0.17257452574525745,
      "step": 3184,
      "training_loss": 7.077261447906494
    },
    {
      "epoch": 0.17262872628726286,
      "step": 3185,
      "training_loss": 6.913172245025635
    },
    {
      "epoch": 0.17268292682926828,
      "step": 3186,
      "training_loss": 6.7933526039123535
    },
    {
      "epoch": 0.17273712737127372,
      "step": 3187,
      "training_loss": 7.11514949798584
    },
    {
      "epoch": 0.17279132791327914,
      "grad_norm": 16.404638290405273,
      "learning_rate": 1e-05,
      "loss": 6.9747,
      "step": 3188
    },
    {
      "epoch": 0.17279132791327914,
      "step": 3188,
      "training_loss": 7.390235424041748
    },
    {
      "epoch": 0.17284552845528456,
      "step": 3189,
      "training_loss": 7.369512557983398
    },
    {
      "epoch": 0.17289972899728998,
      "step": 3190,
      "training_loss": 5.726130962371826
    },
    {
      "epoch": 0.1729539295392954,
      "step": 3191,
      "training_loss": 7.54180908203125
    },
    {
      "epoch": 0.1730081300813008,
      "grad_norm": 21.78600311279297,
      "learning_rate": 1e-05,
      "loss": 7.0069,
      "step": 3192
    },
    {
      "epoch": 0.1730081300813008,
      "step": 3192,
      "training_loss": 6.878592491149902
    },
    {
      "epoch": 0.17306233062330623,
      "step": 3193,
      "training_loss": 7.664971351623535
    },
    {
      "epoch": 0.17311653116531164,
      "step": 3194,
      "training_loss": 7.205855369567871
    },
    {
      "epoch": 0.17317073170731706,
      "step": 3195,
      "training_loss": 7.454861164093018
    },
    {
      "epoch": 0.1732249322493225,
      "grad_norm": 13.264493942260742,
      "learning_rate": 1e-05,
      "loss": 7.3011,
      "step": 3196
    },
    {
      "epoch": 0.1732249322493225,
      "step": 3196,
      "training_loss": 6.272412300109863
    },
    {
      "epoch": 0.17327913279132792,
      "step": 3197,
      "training_loss": 5.989239692687988
    },
    {
      "epoch": 0.17333333333333334,
      "step": 3198,
      "training_loss": 6.642259120941162
    },
    {
      "epoch": 0.17338753387533876,
      "step": 3199,
      "training_loss": 7.241647720336914
    },
    {
      "epoch": 0.17344173441734417,
      "grad_norm": 36.143707275390625,
      "learning_rate": 1e-05,
      "loss": 6.5364,
      "step": 3200
    },
    {
      "epoch": 0.17344173441734417,
      "step": 3200,
      "training_loss": 4.644891738891602
    },
    {
      "epoch": 0.1734959349593496,
      "step": 3201,
      "training_loss": 7.050055503845215
    },
    {
      "epoch": 0.173550135501355,
      "step": 3202,
      "training_loss": 6.916146755218506
    },
    {
      "epoch": 0.17360433604336042,
      "step": 3203,
      "training_loss": 5.252574920654297
    },
    {
      "epoch": 0.17365853658536584,
      "grad_norm": 37.01118087768555,
      "learning_rate": 1e-05,
      "loss": 5.9659,
      "step": 3204
    },
    {
      "epoch": 0.17365853658536584,
      "step": 3204,
      "training_loss": 6.228144645690918
    },
    {
      "epoch": 0.1737127371273713,
      "step": 3205,
      "training_loss": 7.36279821395874
    },
    {
      "epoch": 0.1737669376693767,
      "step": 3206,
      "training_loss": 7.684335708618164
    },
    {
      "epoch": 0.17382113821138212,
      "step": 3207,
      "training_loss": 7.533237934112549
    },
    {
      "epoch": 0.17387533875338754,
      "grad_norm": 29.32270050048828,
      "learning_rate": 1e-05,
      "loss": 7.2021,
      "step": 3208
    },
    {
      "epoch": 0.17387533875338754,
      "step": 3208,
      "training_loss": 7.269965171813965
    },
    {
      "epoch": 0.17392953929539295,
      "step": 3209,
      "training_loss": 6.206577777862549
    },
    {
      "epoch": 0.17398373983739837,
      "step": 3210,
      "training_loss": 7.829830169677734
    },
    {
      "epoch": 0.1740379403794038,
      "step": 3211,
      "training_loss": 5.941713333129883
    },
    {
      "epoch": 0.1740921409214092,
      "grad_norm": 35.47455596923828,
      "learning_rate": 1e-05,
      "loss": 6.812,
      "step": 3212
    },
    {
      "epoch": 0.1740921409214092,
      "step": 3212,
      "training_loss": 7.325170993804932
    },
    {
      "epoch": 0.17414634146341462,
      "step": 3213,
      "training_loss": 6.9300713539123535
    },
    {
      "epoch": 0.17420054200542007,
      "step": 3214,
      "training_loss": 7.543281078338623
    },
    {
      "epoch": 0.17425474254742548,
      "step": 3215,
      "training_loss": 7.318941116333008
    },
    {
      "epoch": 0.1743089430894309,
      "grad_norm": 21.31529998779297,
      "learning_rate": 1e-05,
      "loss": 7.2794,
      "step": 3216
    },
    {
      "epoch": 0.1743089430894309,
      "step": 3216,
      "training_loss": 7.178551197052002
    },
    {
      "epoch": 0.17436314363143632,
      "step": 3217,
      "training_loss": 7.263692855834961
    },
    {
      "epoch": 0.17441734417344174,
      "step": 3218,
      "training_loss": 7.215605735778809
    },
    {
      "epoch": 0.17447154471544715,
      "step": 3219,
      "training_loss": 6.388561725616455
    },
    {
      "epoch": 0.17452574525745257,
      "grad_norm": 17.529579162597656,
      "learning_rate": 1e-05,
      "loss": 7.0116,
      "step": 3220
    },
    {
      "epoch": 0.17452574525745257,
      "step": 3220,
      "training_loss": 7.307718753814697
    },
    {
      "epoch": 0.174579945799458,
      "step": 3221,
      "training_loss": 7.286611557006836
    },
    {
      "epoch": 0.1746341463414634,
      "step": 3222,
      "training_loss": 7.494560718536377
    },
    {
      "epoch": 0.17468834688346885,
      "step": 3223,
      "training_loss": 6.876011848449707
    },
    {
      "epoch": 0.17474254742547426,
      "grad_norm": 34.818321228027344,
      "learning_rate": 1e-05,
      "loss": 7.2412,
      "step": 3224
    },
    {
      "epoch": 0.17474254742547426,
      "step": 3224,
      "training_loss": 6.961209774017334
    },
    {
      "epoch": 0.17479674796747968,
      "step": 3225,
      "training_loss": 7.2172441482543945
    },
    {
      "epoch": 0.1748509485094851,
      "step": 3226,
      "training_loss": 6.1304707527160645
    },
    {
      "epoch": 0.17490514905149052,
      "step": 3227,
      "training_loss": 5.204795837402344
    },
    {
      "epoch": 0.17495934959349593,
      "grad_norm": 27.94009017944336,
      "learning_rate": 1e-05,
      "loss": 6.3784,
      "step": 3228
    },
    {
      "epoch": 0.17495934959349593,
      "step": 3228,
      "training_loss": 7.87689208984375
    },
    {
      "epoch": 0.17501355013550135,
      "step": 3229,
      "training_loss": 8.491917610168457
    },
    {
      "epoch": 0.17506775067750677,
      "step": 3230,
      "training_loss": 6.5653076171875
    },
    {
      "epoch": 0.17512195121951218,
      "step": 3231,
      "training_loss": 7.178850173950195
    },
    {
      "epoch": 0.17517615176151763,
      "grad_norm": 15.335867881774902,
      "learning_rate": 1e-05,
      "loss": 7.5282,
      "step": 3232
    },
    {
      "epoch": 0.17517615176151763,
      "step": 3232,
      "training_loss": 6.095301628112793
    },
    {
      "epoch": 0.17523035230352305,
      "step": 3233,
      "training_loss": 7.424407005310059
    },
    {
      "epoch": 0.17528455284552846,
      "step": 3234,
      "training_loss": 4.420038223266602
    },
    {
      "epoch": 0.17533875338753388,
      "step": 3235,
      "training_loss": 6.062870979309082
    },
    {
      "epoch": 0.1753929539295393,
      "grad_norm": 26.156465530395508,
      "learning_rate": 1e-05,
      "loss": 6.0007,
      "step": 3236
    },
    {
      "epoch": 0.1753929539295393,
      "step": 3236,
      "training_loss": 6.944444179534912
    },
    {
      "epoch": 0.17544715447154471,
      "step": 3237,
      "training_loss": 7.077493667602539
    },
    {
      "epoch": 0.17550135501355013,
      "step": 3238,
      "training_loss": 7.582773208618164
    },
    {
      "epoch": 0.17555555555555555,
      "step": 3239,
      "training_loss": 7.386801719665527
    },
    {
      "epoch": 0.17560975609756097,
      "grad_norm": 14.9144287109375,
      "learning_rate": 1e-05,
      "loss": 7.2479,
      "step": 3240
    },
    {
      "epoch": 0.17560975609756097,
      "step": 3240,
      "training_loss": 7.715625286102295
    },
    {
      "epoch": 0.17566395663956638,
      "step": 3241,
      "training_loss": 7.068658828735352
    },
    {
      "epoch": 0.17571815718157183,
      "step": 3242,
      "training_loss": 6.4366679191589355
    },
    {
      "epoch": 0.17577235772357724,
      "step": 3243,
      "training_loss": 7.5511698722839355
    },
    {
      "epoch": 0.17582655826558266,
      "grad_norm": 21.348920822143555,
      "learning_rate": 1e-05,
      "loss": 7.193,
      "step": 3244
    },
    {
      "epoch": 0.17582655826558266,
      "step": 3244,
      "training_loss": 4.703373432159424
    },
    {
      "epoch": 0.17588075880758808,
      "step": 3245,
      "training_loss": 7.087835788726807
    },
    {
      "epoch": 0.1759349593495935,
      "step": 3246,
      "training_loss": 3.859253168106079
    },
    {
      "epoch": 0.1759891598915989,
      "step": 3247,
      "training_loss": 6.577881336212158
    },
    {
      "epoch": 0.17604336043360433,
      "grad_norm": 22.96194839477539,
      "learning_rate": 1e-05,
      "loss": 5.5571,
      "step": 3248
    },
    {
      "epoch": 0.17604336043360433,
      "step": 3248,
      "training_loss": 6.751131057739258
    },
    {
      "epoch": 0.17609756097560975,
      "step": 3249,
      "training_loss": 6.933318614959717
    },
    {
      "epoch": 0.17615176151761516,
      "step": 3250,
      "training_loss": 6.484048843383789
    },
    {
      "epoch": 0.1762059620596206,
      "step": 3251,
      "training_loss": 8.3451566696167
    },
    {
      "epoch": 0.17626016260162602,
      "grad_norm": 30.493253707885742,
      "learning_rate": 1e-05,
      "loss": 7.1284,
      "step": 3252
    },
    {
      "epoch": 0.17626016260162602,
      "step": 3252,
      "training_loss": 6.663568019866943
    },
    {
      "epoch": 0.17631436314363144,
      "step": 3253,
      "training_loss": 7.993785381317139
    },
    {
      "epoch": 0.17636856368563686,
      "step": 3254,
      "training_loss": 5.558472156524658
    },
    {
      "epoch": 0.17642276422764228,
      "step": 3255,
      "training_loss": 6.624032497406006
    },
    {
      "epoch": 0.1764769647696477,
      "grad_norm": 22.208112716674805,
      "learning_rate": 1e-05,
      "loss": 6.71,
      "step": 3256
    },
    {
      "epoch": 0.1764769647696477,
      "step": 3256,
      "training_loss": 6.806300163269043
    },
    {
      "epoch": 0.1765311653116531,
      "step": 3257,
      "training_loss": 5.946181297302246
    },
    {
      "epoch": 0.17658536585365853,
      "step": 3258,
      "training_loss": 8.124853134155273
    },
    {
      "epoch": 0.17663956639566394,
      "step": 3259,
      "training_loss": 6.726989269256592
    },
    {
      "epoch": 0.1766937669376694,
      "grad_norm": 17.16012954711914,
      "learning_rate": 1e-05,
      "loss": 6.9011,
      "step": 3260
    },
    {
      "epoch": 0.1766937669376694,
      "step": 3260,
      "training_loss": 6.624676704406738
    },
    {
      "epoch": 0.1767479674796748,
      "step": 3261,
      "training_loss": 6.152055263519287
    },
    {
      "epoch": 0.17680216802168022,
      "step": 3262,
      "training_loss": 6.597365856170654
    },
    {
      "epoch": 0.17685636856368564,
      "step": 3263,
      "training_loss": 7.128118515014648
    },
    {
      "epoch": 0.17691056910569106,
      "grad_norm": 26.104700088500977,
      "learning_rate": 1e-05,
      "loss": 6.6256,
      "step": 3264
    },
    {
      "epoch": 0.17691056910569106,
      "step": 3264,
      "training_loss": 7.03904914855957
    },
    {
      "epoch": 0.17696476964769647,
      "step": 3265,
      "training_loss": 7.596627712249756
    },
    {
      "epoch": 0.1770189701897019,
      "step": 3266,
      "training_loss": 5.742495059967041
    },
    {
      "epoch": 0.1770731707317073,
      "step": 3267,
      "training_loss": 7.074875831604004
    },
    {
      "epoch": 0.17712737127371272,
      "grad_norm": 19.43735122680664,
      "learning_rate": 1e-05,
      "loss": 6.8633,
      "step": 3268
    },
    {
      "epoch": 0.17712737127371272,
      "step": 3268,
      "training_loss": 7.695891380310059
    },
    {
      "epoch": 0.17718157181571817,
      "step": 3269,
      "training_loss": 6.900552272796631
    },
    {
      "epoch": 0.1772357723577236,
      "step": 3270,
      "training_loss": 5.291436195373535
    },
    {
      "epoch": 0.177289972899729,
      "step": 3271,
      "training_loss": 6.013469696044922
    },
    {
      "epoch": 0.17734417344173442,
      "grad_norm": 16.803646087646484,
      "learning_rate": 1e-05,
      "loss": 6.4753,
      "step": 3272
    },
    {
      "epoch": 0.17734417344173442,
      "step": 3272,
      "training_loss": 7.2746663093566895
    },
    {
      "epoch": 0.17739837398373984,
      "step": 3273,
      "training_loss": 6.102732181549072
    },
    {
      "epoch": 0.17745257452574525,
      "step": 3274,
      "training_loss": 7.757063865661621
    },
    {
      "epoch": 0.17750677506775067,
      "step": 3275,
      "training_loss": 6.618338108062744
    },
    {
      "epoch": 0.1775609756097561,
      "grad_norm": 19.317054748535156,
      "learning_rate": 1e-05,
      "loss": 6.9382,
      "step": 3276
    },
    {
      "epoch": 0.1775609756097561,
      "step": 3276,
      "training_loss": 7.068641662597656
    },
    {
      "epoch": 0.1776151761517615,
      "step": 3277,
      "training_loss": 6.432555675506592
    },
    {
      "epoch": 0.17766937669376695,
      "step": 3278,
      "training_loss": 4.457352161407471
    },
    {
      "epoch": 0.17772357723577237,
      "step": 3279,
      "training_loss": 6.704146862030029
    },
    {
      "epoch": 0.17777777777777778,
      "grad_norm": 23.30915069580078,
      "learning_rate": 1e-05,
      "loss": 6.1657,
      "step": 3280
    },
    {
      "epoch": 0.17777777777777778,
      "step": 3280,
      "training_loss": 6.910881519317627
    },
    {
      "epoch": 0.1778319783197832,
      "step": 3281,
      "training_loss": 7.2862935066223145
    },
    {
      "epoch": 0.17788617886178862,
      "step": 3282,
      "training_loss": 7.420891284942627
    },
    {
      "epoch": 0.17794037940379404,
      "step": 3283,
      "training_loss": 6.523392200469971
    },
    {
      "epoch": 0.17799457994579945,
      "grad_norm": 28.09433937072754,
      "learning_rate": 1e-05,
      "loss": 7.0354,
      "step": 3284
    },
    {
      "epoch": 0.17799457994579945,
      "step": 3284,
      "training_loss": 8.242647171020508
    },
    {
      "epoch": 0.17804878048780487,
      "step": 3285,
      "training_loss": 6.408278465270996
    },
    {
      "epoch": 0.1781029810298103,
      "step": 3286,
      "training_loss": 6.981882095336914
    },
    {
      "epoch": 0.17815718157181573,
      "step": 3287,
      "training_loss": 7.152814865112305
    },
    {
      "epoch": 0.17821138211382115,
      "grad_norm": 21.75108528137207,
      "learning_rate": 1e-05,
      "loss": 7.1964,
      "step": 3288
    },
    {
      "epoch": 0.17821138211382115,
      "step": 3288,
      "training_loss": 6.5876264572143555
    },
    {
      "epoch": 0.17826558265582657,
      "step": 3289,
      "training_loss": 7.191148281097412
    },
    {
      "epoch": 0.17831978319783198,
      "step": 3290,
      "training_loss": 5.793615341186523
    },
    {
      "epoch": 0.1783739837398374,
      "step": 3291,
      "training_loss": 7.491645812988281
    },
    {
      "epoch": 0.17842818428184282,
      "grad_norm": 15.205506324768066,
      "learning_rate": 1e-05,
      "loss": 6.766,
      "step": 3292
    },
    {
      "epoch": 0.17842818428184282,
      "step": 3292,
      "training_loss": 7.271615505218506
    },
    {
      "epoch": 0.17848238482384823,
      "step": 3293,
      "training_loss": 8.042524337768555
    },
    {
      "epoch": 0.17853658536585365,
      "step": 3294,
      "training_loss": 5.623101711273193
    },
    {
      "epoch": 0.17859078590785907,
      "step": 3295,
      "training_loss": 3.649507522583008
    },
    {
      "epoch": 0.1786449864498645,
      "grad_norm": 22.416088104248047,
      "learning_rate": 1e-05,
      "loss": 6.1467,
      "step": 3296
    },
    {
      "epoch": 0.1786449864498645,
      "step": 3296,
      "training_loss": 6.499926567077637
    },
    {
      "epoch": 0.17869918699186993,
      "step": 3297,
      "training_loss": 7.30272912979126
    },
    {
      "epoch": 0.17875338753387535,
      "step": 3298,
      "training_loss": 7.140902519226074
    },
    {
      "epoch": 0.17880758807588076,
      "step": 3299,
      "training_loss": 5.435976505279541
    },
    {
      "epoch": 0.17886178861788618,
      "grad_norm": 15.576151847839355,
      "learning_rate": 1e-05,
      "loss": 6.5949,
      "step": 3300
    },
    {
      "epoch": 0.17886178861788618,
      "step": 3300,
      "training_loss": 7.834137439727783
    },
    {
      "epoch": 0.1789159891598916,
      "step": 3301,
      "training_loss": 6.638140678405762
    },
    {
      "epoch": 0.17897018970189701,
      "step": 3302,
      "training_loss": 6.912834644317627
    },
    {
      "epoch": 0.17902439024390243,
      "step": 3303,
      "training_loss": 6.344339370727539
    },
    {
      "epoch": 0.17907859078590785,
      "grad_norm": 18.460460662841797,
      "learning_rate": 1e-05,
      "loss": 6.9324,
      "step": 3304
    },
    {
      "epoch": 0.17907859078590785,
      "step": 3304,
      "training_loss": 7.457200050354004
    },
    {
      "epoch": 0.17913279132791327,
      "step": 3305,
      "training_loss": 6.275417327880859
    },
    {
      "epoch": 0.1791869918699187,
      "step": 3306,
      "training_loss": 7.31915807723999
    },
    {
      "epoch": 0.17924119241192413,
      "step": 3307,
      "training_loss": 7.382887840270996
    },
    {
      "epoch": 0.17929539295392954,
      "grad_norm": 31.38921546936035,
      "learning_rate": 1e-05,
      "loss": 7.1087,
      "step": 3308
    },
    {
      "epoch": 0.17929539295392954,
      "step": 3308,
      "training_loss": 7.389094829559326
    },
    {
      "epoch": 0.17934959349593496,
      "step": 3309,
      "training_loss": 7.320819854736328
    },
    {
      "epoch": 0.17940379403794038,
      "step": 3310,
      "training_loss": 7.33685302734375
    },
    {
      "epoch": 0.1794579945799458,
      "step": 3311,
      "training_loss": 5.047056674957275
    },
    {
      "epoch": 0.1795121951219512,
      "grad_norm": 26.34992027282715,
      "learning_rate": 1e-05,
      "loss": 6.7735,
      "step": 3312
    },
    {
      "epoch": 0.1795121951219512,
      "step": 3312,
      "training_loss": 6.802356719970703
    },
    {
      "epoch": 0.17956639566395663,
      "step": 3313,
      "training_loss": 7.65686559677124
    },
    {
      "epoch": 0.17962059620596205,
      "step": 3314,
      "training_loss": 7.394371509552002
    },
    {
      "epoch": 0.1796747967479675,
      "step": 3315,
      "training_loss": 7.776859760284424
    },
    {
      "epoch": 0.1797289972899729,
      "grad_norm": 32.12604904174805,
      "learning_rate": 1e-05,
      "loss": 7.4076,
      "step": 3316
    },
    {
      "epoch": 0.1797289972899729,
      "step": 3316,
      "training_loss": 7.0990495681762695
    },
    {
      "epoch": 0.17978319783197833,
      "step": 3317,
      "training_loss": 6.183715343475342
    },
    {
      "epoch": 0.17983739837398374,
      "step": 3318,
      "training_loss": 7.7558135986328125
    },
    {
      "epoch": 0.17989159891598916,
      "step": 3319,
      "training_loss": 6.838190078735352
    },
    {
      "epoch": 0.17994579945799458,
      "grad_norm": 26.998388290405273,
      "learning_rate": 1e-05,
      "loss": 6.9692,
      "step": 3320
    },
    {
      "epoch": 0.17994579945799458,
      "step": 3320,
      "training_loss": 7.519218921661377
    },
    {
      "epoch": 0.18,
      "step": 3321,
      "training_loss": 7.506350517272949
    },
    {
      "epoch": 0.1800542005420054,
      "step": 3322,
      "training_loss": 7.00709867477417
    },
    {
      "epoch": 0.18010840108401083,
      "step": 3323,
      "training_loss": 6.29004430770874
    },
    {
      "epoch": 0.18016260162601627,
      "grad_norm": 23.616519927978516,
      "learning_rate": 1e-05,
      "loss": 7.0807,
      "step": 3324
    },
    {
      "epoch": 0.18016260162601627,
      "step": 3324,
      "training_loss": 7.54437255859375
    },
    {
      "epoch": 0.1802168021680217,
      "step": 3325,
      "training_loss": 6.813650608062744
    },
    {
      "epoch": 0.1802710027100271,
      "step": 3326,
      "training_loss": 5.768085956573486
    },
    {
      "epoch": 0.18032520325203252,
      "step": 3327,
      "training_loss": 4.22550630569458
    },
    {
      "epoch": 0.18037940379403794,
      "grad_norm": 24.43640899658203,
      "learning_rate": 1e-05,
      "loss": 6.0879,
      "step": 3328
    },
    {
      "epoch": 0.18037940379403794,
      "step": 3328,
      "training_loss": 6.517227649688721
    },
    {
      "epoch": 0.18043360433604336,
      "step": 3329,
      "training_loss": 6.517622470855713
    },
    {
      "epoch": 0.18048780487804877,
      "step": 3330,
      "training_loss": 7.013121128082275
    },
    {
      "epoch": 0.1805420054200542,
      "step": 3331,
      "training_loss": 6.851014614105225
    },
    {
      "epoch": 0.1805962059620596,
      "grad_norm": 27.333852767944336,
      "learning_rate": 1e-05,
      "loss": 6.7247,
      "step": 3332
    },
    {
      "epoch": 0.1805962059620596,
      "step": 3332,
      "training_loss": 5.669787406921387
    },
    {
      "epoch": 0.18065040650406505,
      "step": 3333,
      "training_loss": 7.989651203155518
    },
    {
      "epoch": 0.18070460704607047,
      "step": 3334,
      "training_loss": 7.478070259094238
    },
    {
      "epoch": 0.1807588075880759,
      "step": 3335,
      "training_loss": 5.707287788391113
    },
    {
      "epoch": 0.1808130081300813,
      "grad_norm": 16.548011779785156,
      "learning_rate": 1e-05,
      "loss": 6.7112,
      "step": 3336
    },
    {
      "epoch": 0.1808130081300813,
      "step": 3336,
      "training_loss": 7.21728515625
    },
    {
      "epoch": 0.18086720867208672,
      "step": 3337,
      "training_loss": 4.729776382446289
    },
    {
      "epoch": 0.18092140921409214,
      "step": 3338,
      "training_loss": 7.253871917724609
    },
    {
      "epoch": 0.18097560975609756,
      "step": 3339,
      "training_loss": 7.343036651611328
    },
    {
      "epoch": 0.18102981029810297,
      "grad_norm": 20.118356704711914,
      "learning_rate": 1e-05,
      "loss": 6.636,
      "step": 3340
    },
    {
      "epoch": 0.18102981029810297,
      "step": 3340,
      "training_loss": 6.8309221267700195
    },
    {
      "epoch": 0.1810840108401084,
      "step": 3341,
      "training_loss": 5.049726486206055
    },
    {
      "epoch": 0.18113821138211383,
      "step": 3342,
      "training_loss": 7.507279396057129
    },
    {
      "epoch": 0.18119241192411925,
      "step": 3343,
      "training_loss": 6.627949237823486
    },
    {
      "epoch": 0.18124661246612467,
      "grad_norm": 35.261566162109375,
      "learning_rate": 1e-05,
      "loss": 6.504,
      "step": 3344
    },
    {
      "epoch": 0.18124661246612467,
      "step": 3344,
      "training_loss": 6.978277683258057
    },
    {
      "epoch": 0.18130081300813009,
      "step": 3345,
      "training_loss": 7.691988468170166
    },
    {
      "epoch": 0.1813550135501355,
      "step": 3346,
      "training_loss": 6.0778045654296875
    },
    {
      "epoch": 0.18140921409214092,
      "step": 3347,
      "training_loss": 6.906822681427002
    },
    {
      "epoch": 0.18146341463414634,
      "grad_norm": 20.806055068969727,
      "learning_rate": 1e-05,
      "loss": 6.9137,
      "step": 3348
    },
    {
      "epoch": 0.18146341463414634,
      "step": 3348,
      "training_loss": 7.204168319702148
    },
    {
      "epoch": 0.18151761517615175,
      "step": 3349,
      "training_loss": 7.180367469787598
    },
    {
      "epoch": 0.18157181571815717,
      "step": 3350,
      "training_loss": 6.33204984664917
    },
    {
      "epoch": 0.18162601626016261,
      "step": 3351,
      "training_loss": 8.198505401611328
    },
    {
      "epoch": 0.18168021680216803,
      "grad_norm": 39.35863494873047,
      "learning_rate": 1e-05,
      "loss": 7.2288,
      "step": 3352
    },
    {
      "epoch": 0.18168021680216803,
      "step": 3352,
      "training_loss": 5.649402141571045
    },
    {
      "epoch": 0.18173441734417345,
      "step": 3353,
      "training_loss": 6.981473922729492
    },
    {
      "epoch": 0.18178861788617887,
      "step": 3354,
      "training_loss": 6.505300998687744
    },
    {
      "epoch": 0.18184281842818428,
      "step": 3355,
      "training_loss": 7.557161808013916
    },
    {
      "epoch": 0.1818970189701897,
      "grad_norm": 24.307798385620117,
      "learning_rate": 1e-05,
      "loss": 6.6733,
      "step": 3356
    },
    {
      "epoch": 0.1818970189701897,
      "step": 3356,
      "training_loss": 8.475086212158203
    },
    {
      "epoch": 0.18195121951219512,
      "step": 3357,
      "training_loss": 7.619329929351807
    },
    {
      "epoch": 0.18200542005420053,
      "step": 3358,
      "training_loss": 6.289558410644531
    },
    {
      "epoch": 0.18205962059620595,
      "step": 3359,
      "training_loss": 7.4788618087768555
    },
    {
      "epoch": 0.1821138211382114,
      "grad_norm": 48.06265640258789,
      "learning_rate": 1e-05,
      "loss": 7.4657,
      "step": 3360
    },
    {
      "epoch": 0.1821138211382114,
      "step": 3360,
      "training_loss": 5.435580730438232
    },
    {
      "epoch": 0.1821680216802168,
      "step": 3361,
      "training_loss": 5.644412517547607
    },
    {
      "epoch": 0.18222222222222223,
      "step": 3362,
      "training_loss": 7.408228874206543
    },
    {
      "epoch": 0.18227642276422765,
      "step": 3363,
      "training_loss": 7.279889106750488
    },
    {
      "epoch": 0.18233062330623306,
      "grad_norm": 17.207822799682617,
      "learning_rate": 1e-05,
      "loss": 6.442,
      "step": 3364
    },
    {
      "epoch": 0.18233062330623306,
      "step": 3364,
      "training_loss": 6.933803558349609
    },
    {
      "epoch": 0.18238482384823848,
      "step": 3365,
      "training_loss": 4.771103382110596
    },
    {
      "epoch": 0.1824390243902439,
      "step": 3366,
      "training_loss": 7.1827473640441895
    },
    {
      "epoch": 0.18249322493224931,
      "step": 3367,
      "training_loss": 7.059759140014648
    },
    {
      "epoch": 0.18254742547425473,
      "grad_norm": 16.411314010620117,
      "learning_rate": 1e-05,
      "loss": 6.4869,
      "step": 3368
    },
    {
      "epoch": 0.18254742547425473,
      "step": 3368,
      "training_loss": 7.067473888397217
    },
    {
      "epoch": 0.18260162601626015,
      "step": 3369,
      "training_loss": 7.472407817840576
    },
    {
      "epoch": 0.1826558265582656,
      "step": 3370,
      "training_loss": 7.13031005859375
    },
    {
      "epoch": 0.182710027100271,
      "step": 3371,
      "training_loss": 7.098349094390869
    },
    {
      "epoch": 0.18276422764227643,
      "grad_norm": 31.09568214416504,
      "learning_rate": 1e-05,
      "loss": 7.1921,
      "step": 3372
    },
    {
      "epoch": 0.18276422764227643,
      "step": 3372,
      "training_loss": 6.898789882659912
    },
    {
      "epoch": 0.18281842818428184,
      "step": 3373,
      "training_loss": 7.289907455444336
    },
    {
      "epoch": 0.18287262872628726,
      "step": 3374,
      "training_loss": 7.373929500579834
    },
    {
      "epoch": 0.18292682926829268,
      "step": 3375,
      "training_loss": 7.536787033081055
    },
    {
      "epoch": 0.1829810298102981,
      "grad_norm": 18.382659912109375,
      "learning_rate": 1e-05,
      "loss": 7.2749,
      "step": 3376
    },
    {
      "epoch": 0.1829810298102981,
      "step": 3376,
      "training_loss": 8.545639991760254
    },
    {
      "epoch": 0.1830352303523035,
      "step": 3377,
      "training_loss": 7.308839321136475
    },
    {
      "epoch": 0.18308943089430893,
      "step": 3378,
      "training_loss": 6.600259304046631
    },
    {
      "epoch": 0.18314363143631437,
      "step": 3379,
      "training_loss": 7.262246131896973
    },
    {
      "epoch": 0.1831978319783198,
      "grad_norm": 37.13809585571289,
      "learning_rate": 1e-05,
      "loss": 7.4292,
      "step": 3380
    },
    {
      "epoch": 0.1831978319783198,
      "step": 3380,
      "training_loss": 4.7367048263549805
    },
    {
      "epoch": 0.1832520325203252,
      "step": 3381,
      "training_loss": 7.384112358093262
    },
    {
      "epoch": 0.18330623306233063,
      "step": 3382,
      "training_loss": 6.965579032897949
    },
    {
      "epoch": 0.18336043360433604,
      "step": 3383,
      "training_loss": 6.959291458129883
    },
    {
      "epoch": 0.18341463414634146,
      "grad_norm": 25.015670776367188,
      "learning_rate": 1e-05,
      "loss": 6.5114,
      "step": 3384
    },
    {
      "epoch": 0.18341463414634146,
      "step": 3384,
      "training_loss": 6.952235698699951
    },
    {
      "epoch": 0.18346883468834688,
      "step": 3385,
      "training_loss": 6.194606781005859
    },
    {
      "epoch": 0.1835230352303523,
      "step": 3386,
      "training_loss": 6.711449146270752
    },
    {
      "epoch": 0.1835772357723577,
      "step": 3387,
      "training_loss": 5.779006004333496
    },
    {
      "epoch": 0.18363143631436316,
      "grad_norm": 23.652664184570312,
      "learning_rate": 1e-05,
      "loss": 6.4093,
      "step": 3388
    },
    {
      "epoch": 0.18363143631436316,
      "step": 3388,
      "training_loss": 7.395166873931885
    },
    {
      "epoch": 0.18368563685636857,
      "step": 3389,
      "training_loss": 6.696549892425537
    },
    {
      "epoch": 0.183739837398374,
      "step": 3390,
      "training_loss": 5.127013683319092
    },
    {
      "epoch": 0.1837940379403794,
      "step": 3391,
      "training_loss": 6.125940799713135
    },
    {
      "epoch": 0.18384823848238482,
      "grad_norm": 25.875394821166992,
      "learning_rate": 1e-05,
      "loss": 6.3362,
      "step": 3392
    },
    {
      "epoch": 0.18384823848238482,
      "step": 3392,
      "training_loss": 7.43597412109375
    },
    {
      "epoch": 0.18390243902439024,
      "step": 3393,
      "training_loss": 6.802240371704102
    },
    {
      "epoch": 0.18395663956639566,
      "step": 3394,
      "training_loss": 6.806080341339111
    },
    {
      "epoch": 0.18401084010840107,
      "step": 3395,
      "training_loss": 6.915140151977539
    },
    {
      "epoch": 0.1840650406504065,
      "grad_norm": 20.619792938232422,
      "learning_rate": 1e-05,
      "loss": 6.9899,
      "step": 3396
    },
    {
      "epoch": 0.1840650406504065,
      "step": 3396,
      "training_loss": 6.334334373474121
    },
    {
      "epoch": 0.18411924119241194,
      "step": 3397,
      "training_loss": 7.213754653930664
    },
    {
      "epoch": 0.18417344173441735,
      "step": 3398,
      "training_loss": 6.9066925048828125
    },
    {
      "epoch": 0.18422764227642277,
      "step": 3399,
      "training_loss": 7.491916656494141
    },
    {
      "epoch": 0.1842818428184282,
      "grad_norm": 25.376258850097656,
      "learning_rate": 1e-05,
      "loss": 6.9867,
      "step": 3400
    },
    {
      "epoch": 0.1842818428184282,
      "step": 3400,
      "training_loss": 8.201772689819336
    },
    {
      "epoch": 0.1843360433604336,
      "step": 3401,
      "training_loss": 4.462465286254883
    },
    {
      "epoch": 0.18439024390243902,
      "step": 3402,
      "training_loss": 6.443544387817383
    },
    {
      "epoch": 0.18444444444444444,
      "step": 3403,
      "training_loss": 6.162970066070557
    },
    {
      "epoch": 0.18449864498644986,
      "grad_norm": 16.94151496887207,
      "learning_rate": 1e-05,
      "loss": 6.3177,
      "step": 3404
    },
    {
      "epoch": 0.18449864498644986,
      "step": 3404,
      "training_loss": 5.379667282104492
    },
    {
      "epoch": 0.18455284552845527,
      "step": 3405,
      "training_loss": 7.251896381378174
    },
    {
      "epoch": 0.18460704607046072,
      "step": 3406,
      "training_loss": 6.159588813781738
    },
    {
      "epoch": 0.18466124661246613,
      "step": 3407,
      "training_loss": 7.735924243927002
    },
    {
      "epoch": 0.18471544715447155,
      "grad_norm": 28.975675582885742,
      "learning_rate": 1e-05,
      "loss": 6.6318,
      "step": 3408
    },
    {
      "epoch": 0.18471544715447155,
      "step": 3408,
      "training_loss": 6.941023826599121
    },
    {
      "epoch": 0.18476964769647697,
      "step": 3409,
      "training_loss": 7.57454252243042
    },
    {
      "epoch": 0.18482384823848239,
      "step": 3410,
      "training_loss": 6.454051494598389
    },
    {
      "epoch": 0.1848780487804878,
      "step": 3411,
      "training_loss": 6.925812244415283
    },
    {
      "epoch": 0.18493224932249322,
      "grad_norm": 16.656503677368164,
      "learning_rate": 1e-05,
      "loss": 6.9739,
      "step": 3412
    },
    {
      "epoch": 0.18493224932249322,
      "step": 3412,
      "training_loss": 6.546768665313721
    },
    {
      "epoch": 0.18498644986449864,
      "step": 3413,
      "training_loss": 5.75166130065918
    },
    {
      "epoch": 0.18504065040650405,
      "step": 3414,
      "training_loss": 6.919075965881348
    },
    {
      "epoch": 0.1850948509485095,
      "step": 3415,
      "training_loss": 7.345009803771973
    },
    {
      "epoch": 0.18514905149051492,
      "grad_norm": 17.101131439208984,
      "learning_rate": 1e-05,
      "loss": 6.6406,
      "step": 3416
    },
    {
      "epoch": 0.18514905149051492,
      "step": 3416,
      "training_loss": 6.298130989074707
    },
    {
      "epoch": 0.18520325203252033,
      "step": 3417,
      "training_loss": 6.830080509185791
    },
    {
      "epoch": 0.18525745257452575,
      "step": 3418,
      "training_loss": 9.819458961486816
    },
    {
      "epoch": 0.18531165311653117,
      "step": 3419,
      "training_loss": 6.7071709632873535
    },
    {
      "epoch": 0.18536585365853658,
      "grad_norm": 20.146581649780273,
      "learning_rate": 1e-05,
      "loss": 7.4137,
      "step": 3420
    },
    {
      "epoch": 0.18536585365853658,
      "step": 3420,
      "training_loss": 7.384185791015625
    },
    {
      "epoch": 0.185420054200542,
      "step": 3421,
      "training_loss": 6.3185224533081055
    },
    {
      "epoch": 0.18547425474254742,
      "step": 3422,
      "training_loss": 5.7343010902404785
    },
    {
      "epoch": 0.18552845528455283,
      "step": 3423,
      "training_loss": 7.767590522766113
    },
    {
      "epoch": 0.18558265582655828,
      "grad_norm": 20.473827362060547,
      "learning_rate": 1e-05,
      "loss": 6.8012,
      "step": 3424
    },
    {
      "epoch": 0.18558265582655828,
      "step": 3424,
      "training_loss": 7.124728202819824
    },
    {
      "epoch": 0.1856368563685637,
      "step": 3425,
      "training_loss": 6.565446376800537
    },
    {
      "epoch": 0.1856910569105691,
      "step": 3426,
      "training_loss": 6.312071323394775
    },
    {
      "epoch": 0.18574525745257453,
      "step": 3427,
      "training_loss": 6.863083362579346
    },
    {
      "epoch": 0.18579945799457995,
      "grad_norm": 19.267183303833008,
      "learning_rate": 1e-05,
      "loss": 6.7163,
      "step": 3428
    },
    {
      "epoch": 0.18579945799457995,
      "step": 3428,
      "training_loss": 6.901342868804932
    },
    {
      "epoch": 0.18585365853658536,
      "step": 3429,
      "training_loss": 6.820026874542236
    },
    {
      "epoch": 0.18590785907859078,
      "step": 3430,
      "training_loss": 6.04063606262207
    },
    {
      "epoch": 0.1859620596205962,
      "step": 3431,
      "training_loss": 7.929154872894287
    },
    {
      "epoch": 0.18601626016260162,
      "grad_norm": 22.01738739013672,
      "learning_rate": 1e-05,
      "loss": 6.9228,
      "step": 3432
    },
    {
      "epoch": 0.18601626016260162,
      "step": 3432,
      "training_loss": 5.661021709442139
    },
    {
      "epoch": 0.18607046070460703,
      "step": 3433,
      "training_loss": 5.103138446807861
    },
    {
      "epoch": 0.18612466124661248,
      "step": 3434,
      "training_loss": 7.084292888641357
    },
    {
      "epoch": 0.1861788617886179,
      "step": 3435,
      "training_loss": 5.237099647521973
    },
    {
      "epoch": 0.1862330623306233,
      "grad_norm": 22.17915153503418,
      "learning_rate": 1e-05,
      "loss": 5.7714,
      "step": 3436
    },
    {
      "epoch": 0.1862330623306233,
      "step": 3436,
      "training_loss": 6.630657196044922
    },
    {
      "epoch": 0.18628726287262873,
      "step": 3437,
      "training_loss": 8.178718566894531
    },
    {
      "epoch": 0.18634146341463415,
      "step": 3438,
      "training_loss": 6.116684436798096
    },
    {
      "epoch": 0.18639566395663956,
      "step": 3439,
      "training_loss": 7.480030059814453
    },
    {
      "epoch": 0.18644986449864498,
      "grad_norm": 23.384624481201172,
      "learning_rate": 1e-05,
      "loss": 7.1015,
      "step": 3440
    },
    {
      "epoch": 0.18644986449864498,
      "step": 3440,
      "training_loss": 6.911051273345947
    },
    {
      "epoch": 0.1865040650406504,
      "step": 3441,
      "training_loss": 6.944309711456299
    },
    {
      "epoch": 0.1865582655826558,
      "step": 3442,
      "training_loss": 7.681085586547852
    },
    {
      "epoch": 0.18661246612466126,
      "step": 3443,
      "training_loss": 7.258789539337158
    },
    {
      "epoch": 0.18666666666666668,
      "grad_norm": 24.848281860351562,
      "learning_rate": 1e-05,
      "loss": 7.1988,
      "step": 3444
    },
    {
      "epoch": 0.18666666666666668,
      "step": 3444,
      "training_loss": 8.989505767822266
    },
    {
      "epoch": 0.1867208672086721,
      "step": 3445,
      "training_loss": 7.3001604080200195
    },
    {
      "epoch": 0.1867750677506775,
      "step": 3446,
      "training_loss": 6.821796417236328
    },
    {
      "epoch": 0.18682926829268293,
      "step": 3447,
      "training_loss": 6.781571388244629
    },
    {
      "epoch": 0.18688346883468834,
      "grad_norm": 40.569580078125,
      "learning_rate": 1e-05,
      "loss": 7.4733,
      "step": 3448
    },
    {
      "epoch": 0.18688346883468834,
      "step": 3448,
      "training_loss": 7.203474998474121
    },
    {
      "epoch": 0.18693766937669376,
      "step": 3449,
      "training_loss": 6.693836212158203
    },
    {
      "epoch": 0.18699186991869918,
      "step": 3450,
      "training_loss": 6.9434380531311035
    },
    {
      "epoch": 0.1870460704607046,
      "step": 3451,
      "training_loss": 6.708651065826416
    },
    {
      "epoch": 0.18710027100271004,
      "grad_norm": 51.20587158203125,
      "learning_rate": 1e-05,
      "loss": 6.8874,
      "step": 3452
    },
    {
      "epoch": 0.18710027100271004,
      "step": 3452,
      "training_loss": 7.080171585083008
    },
    {
      "epoch": 0.18715447154471546,
      "step": 3453,
      "training_loss": 7.378125190734863
    },
    {
      "epoch": 0.18720867208672087,
      "step": 3454,
      "training_loss": 5.928691387176514
    },
    {
      "epoch": 0.1872628726287263,
      "step": 3455,
      "training_loss": 6.236708641052246
    },
    {
      "epoch": 0.1873170731707317,
      "grad_norm": 21.638193130493164,
      "learning_rate": 1e-05,
      "loss": 6.6559,
      "step": 3456
    },
    {
      "epoch": 0.1873170731707317,
      "step": 3456,
      "training_loss": 7.058627128601074
    },
    {
      "epoch": 0.18737127371273712,
      "step": 3457,
      "training_loss": 6.924045562744141
    },
    {
      "epoch": 0.18742547425474254,
      "step": 3458,
      "training_loss": 6.858800411224365
    },
    {
      "epoch": 0.18747967479674796,
      "step": 3459,
      "training_loss": 6.867778778076172
    },
    {
      "epoch": 0.18753387533875338,
      "grad_norm": 24.784788131713867,
      "learning_rate": 1e-05,
      "loss": 6.9273,
      "step": 3460
    },
    {
      "epoch": 0.18753387533875338,
      "step": 3460,
      "training_loss": 6.287567138671875
    },
    {
      "epoch": 0.18758807588075882,
      "step": 3461,
      "training_loss": 6.86269474029541
    },
    {
      "epoch": 0.18764227642276424,
      "step": 3462,
      "training_loss": 6.133185863494873
    },
    {
      "epoch": 0.18769647696476965,
      "step": 3463,
      "training_loss": 6.548514366149902
    },
    {
      "epoch": 0.18775067750677507,
      "grad_norm": 20.304061889648438,
      "learning_rate": 1e-05,
      "loss": 6.458,
      "step": 3464
    },
    {
      "epoch": 0.18775067750677507,
      "step": 3464,
      "training_loss": 7.936813831329346
    },
    {
      "epoch": 0.1878048780487805,
      "step": 3465,
      "training_loss": 7.206125259399414
    },
    {
      "epoch": 0.1878590785907859,
      "step": 3466,
      "training_loss": 8.0089693069458
    },
    {
      "epoch": 0.18791327913279132,
      "step": 3467,
      "training_loss": 6.7452874183654785
    },
    {
      "epoch": 0.18796747967479674,
      "grad_norm": 19.755260467529297,
      "learning_rate": 1e-05,
      "loss": 7.4743,
      "step": 3468
    },
    {
      "epoch": 0.18796747967479674,
      "step": 3468,
      "training_loss": 5.797020435333252
    },
    {
      "epoch": 0.18802168021680216,
      "step": 3469,
      "training_loss": 7.387396812438965
    },
    {
      "epoch": 0.1880758807588076,
      "step": 3470,
      "training_loss": 6.846914768218994
    },
    {
      "epoch": 0.18813008130081302,
      "step": 3471,
      "training_loss": 7.254397392272949
    },
    {
      "epoch": 0.18818428184281843,
      "grad_norm": 25.550495147705078,
      "learning_rate": 1e-05,
      "loss": 6.8214,
      "step": 3472
    },
    {
      "epoch": 0.18818428184281843,
      "step": 3472,
      "training_loss": 7.41315221786499
    },
    {
      "epoch": 0.18823848238482385,
      "step": 3473,
      "training_loss": 8.467247009277344
    },
    {
      "epoch": 0.18829268292682927,
      "step": 3474,
      "training_loss": 7.787130832672119
    },
    {
      "epoch": 0.18834688346883469,
      "step": 3475,
      "training_loss": 6.269234657287598
    },
    {
      "epoch": 0.1884010840108401,
      "grad_norm": 17.83312225341797,
      "learning_rate": 1e-05,
      "loss": 7.4842,
      "step": 3476
    },
    {
      "epoch": 0.1884010840108401,
      "step": 3476,
      "training_loss": 6.103306293487549
    },
    {
      "epoch": 0.18845528455284552,
      "step": 3477,
      "training_loss": 7.817312717437744
    },
    {
      "epoch": 0.18850948509485094,
      "step": 3478,
      "training_loss": 6.307263374328613
    },
    {
      "epoch": 0.18856368563685638,
      "step": 3479,
      "training_loss": 7.446882247924805
    },
    {
      "epoch": 0.1886178861788618,
      "grad_norm": 21.80047035217285,
      "learning_rate": 1e-05,
      "loss": 6.9187,
      "step": 3480
    },
    {
      "epoch": 0.1886178861788618,
      "step": 3480,
      "training_loss": 6.416016101837158
    },
    {
      "epoch": 0.18867208672086722,
      "step": 3481,
      "training_loss": 7.96049165725708
    },
    {
      "epoch": 0.18872628726287263,
      "step": 3482,
      "training_loss": 7.121279239654541
    },
    {
      "epoch": 0.18878048780487805,
      "step": 3483,
      "training_loss": 7.533348560333252
    },
    {
      "epoch": 0.18883468834688347,
      "grad_norm": 20.86664581298828,
      "learning_rate": 1e-05,
      "loss": 7.2578,
      "step": 3484
    },
    {
      "epoch": 0.18883468834688347,
      "step": 3484,
      "training_loss": 6.258667945861816
    },
    {
      "epoch": 0.18888888888888888,
      "step": 3485,
      "training_loss": 6.708319187164307
    },
    {
      "epoch": 0.1889430894308943,
      "step": 3486,
      "training_loss": 5.572155952453613
    },
    {
      "epoch": 0.18899728997289972,
      "step": 3487,
      "training_loss": 6.677277088165283
    },
    {
      "epoch": 0.18905149051490516,
      "grad_norm": 46.096649169921875,
      "learning_rate": 1e-05,
      "loss": 6.3041,
      "step": 3488
    },
    {
      "epoch": 0.18905149051490516,
      "step": 3488,
      "training_loss": 5.042290210723877
    },
    {
      "epoch": 0.18910569105691058,
      "step": 3489,
      "training_loss": 6.765042304992676
    },
    {
      "epoch": 0.189159891598916,
      "step": 3490,
      "training_loss": 6.946923732757568
    },
    {
      "epoch": 0.1892140921409214,
      "step": 3491,
      "training_loss": 7.128706932067871
    },
    {
      "epoch": 0.18926829268292683,
      "grad_norm": 19.224590301513672,
      "learning_rate": 1e-05,
      "loss": 6.4707,
      "step": 3492
    },
    {
      "epoch": 0.18926829268292683,
      "step": 3492,
      "training_loss": 6.520163536071777
    },
    {
      "epoch": 0.18932249322493225,
      "step": 3493,
      "training_loss": 5.814317226409912
    },
    {
      "epoch": 0.18937669376693766,
      "step": 3494,
      "training_loss": 4.713820934295654
    },
    {
      "epoch": 0.18943089430894308,
      "step": 3495,
      "training_loss": 6.914499282836914
    },
    {
      "epoch": 0.1894850948509485,
      "grad_norm": 25.591083526611328,
      "learning_rate": 1e-05,
      "loss": 5.9907,
      "step": 3496
    },
    {
      "epoch": 0.1894850948509485,
      "step": 3496,
      "training_loss": 5.882189750671387
    },
    {
      "epoch": 0.18953929539295392,
      "step": 3497,
      "training_loss": 6.789541244506836
    },
    {
      "epoch": 0.18959349593495936,
      "step": 3498,
      "training_loss": 8.489331245422363
    },
    {
      "epoch": 0.18964769647696478,
      "step": 3499,
      "training_loss": 8.511167526245117
    },
    {
      "epoch": 0.1897018970189702,
      "grad_norm": 62.952980041503906,
      "learning_rate": 1e-05,
      "loss": 7.4181,
      "step": 3500
    },
    {
      "epoch": 0.1897018970189702,
      "step": 3500,
      "training_loss": 7.399101734161377
    },
    {
      "epoch": 0.1897560975609756,
      "step": 3501,
      "training_loss": 7.1829514503479
    },
    {
      "epoch": 0.18981029810298103,
      "step": 3502,
      "training_loss": 7.134843349456787
    },
    {
      "epoch": 0.18986449864498645,
      "step": 3503,
      "training_loss": 6.642043113708496
    },
    {
      "epoch": 0.18991869918699186,
      "grad_norm": 23.504793167114258,
      "learning_rate": 1e-05,
      "loss": 7.0897,
      "step": 3504
    },
    {
      "epoch": 0.18991869918699186,
      "step": 3504,
      "training_loss": 6.446897983551025
    },
    {
      "epoch": 0.18997289972899728,
      "step": 3505,
      "training_loss": 7.196646213531494
    },
    {
      "epoch": 0.1900271002710027,
      "step": 3506,
      "training_loss": 7.2335357666015625
    },
    {
      "epoch": 0.19008130081300814,
      "step": 3507,
      "training_loss": 3.8451244831085205
    },
    {
      "epoch": 0.19013550135501356,
      "grad_norm": 31.52877426147461,
      "learning_rate": 1e-05,
      "loss": 6.1806,
      "step": 3508
    },
    {
      "epoch": 0.19013550135501356,
      "step": 3508,
      "training_loss": 8.038898468017578
    },
    {
      "epoch": 0.19018970189701898,
      "step": 3509,
      "training_loss": 8.071480751037598
    },
    {
      "epoch": 0.1902439024390244,
      "step": 3510,
      "training_loss": 7.032698154449463
    },
    {
      "epoch": 0.1902981029810298,
      "step": 3511,
      "training_loss": 6.798803806304932
    },
    {
      "epoch": 0.19035230352303523,
      "grad_norm": 19.408241271972656,
      "learning_rate": 1e-05,
      "loss": 7.4855,
      "step": 3512
    },
    {
      "epoch": 0.19035230352303523,
      "step": 3512,
      "training_loss": 8.390660285949707
    },
    {
      "epoch": 0.19040650406504064,
      "step": 3513,
      "training_loss": 5.963978290557861
    },
    {
      "epoch": 0.19046070460704606,
      "step": 3514,
      "training_loss": 8.271682739257812
    },
    {
      "epoch": 0.19051490514905148,
      "step": 3515,
      "training_loss": 6.1907453536987305
    },
    {
      "epoch": 0.19056910569105692,
      "grad_norm": 29.050853729248047,
      "learning_rate": 1e-05,
      "loss": 7.2043,
      "step": 3516
    },
    {
      "epoch": 0.19056910569105692,
      "step": 3516,
      "training_loss": 6.836751461029053
    },
    {
      "epoch": 0.19062330623306234,
      "step": 3517,
      "training_loss": 6.775646209716797
    },
    {
      "epoch": 0.19067750677506776,
      "step": 3518,
      "training_loss": 6.142198085784912
    },
    {
      "epoch": 0.19073170731707317,
      "step": 3519,
      "training_loss": 7.033194065093994
    },
    {
      "epoch": 0.1907859078590786,
      "grad_norm": 19.76761817932129,
      "learning_rate": 1e-05,
      "loss": 6.6969,
      "step": 3520
    },
    {
      "epoch": 0.1907859078590786,
      "step": 3520,
      "training_loss": 7.350064754486084
    },
    {
      "epoch": 0.190840108401084,
      "step": 3521,
      "training_loss": 7.468199253082275
    },
    {
      "epoch": 0.19089430894308942,
      "step": 3522,
      "training_loss": 6.538405895233154
    },
    {
      "epoch": 0.19094850948509484,
      "step": 3523,
      "training_loss": 6.253392219543457
    },
    {
      "epoch": 0.19100271002710026,
      "grad_norm": 27.41691017150879,
      "learning_rate": 1e-05,
      "loss": 6.9025,
      "step": 3524
    },
    {
      "epoch": 0.19100271002710026,
      "step": 3524,
      "training_loss": 7.649094104766846
    },
    {
      "epoch": 0.1910569105691057,
      "step": 3525,
      "training_loss": 6.199626445770264
    },
    {
      "epoch": 0.19111111111111112,
      "step": 3526,
      "training_loss": 6.262503623962402
    },
    {
      "epoch": 0.19116531165311654,
      "step": 3527,
      "training_loss": 7.09645938873291
    },
    {
      "epoch": 0.19121951219512195,
      "grad_norm": 19.283796310424805,
      "learning_rate": 1e-05,
      "loss": 6.8019,
      "step": 3528
    },
    {
      "epoch": 0.19121951219512195,
      "step": 3528,
      "training_loss": 6.632265567779541
    },
    {
      "epoch": 0.19127371273712737,
      "step": 3529,
      "training_loss": 7.377540111541748
    },
    {
      "epoch": 0.1913279132791328,
      "step": 3530,
      "training_loss": 7.22407341003418
    },
    {
      "epoch": 0.1913821138211382,
      "step": 3531,
      "training_loss": 6.923447132110596
    },
    {
      "epoch": 0.19143631436314362,
      "grad_norm": 21.516965866088867,
      "learning_rate": 1e-05,
      "loss": 7.0393,
      "step": 3532
    },
    {
      "epoch": 0.19143631436314362,
      "step": 3532,
      "training_loss": 7.718166828155518
    },
    {
      "epoch": 0.19149051490514904,
      "step": 3533,
      "training_loss": 7.17830228805542
    },
    {
      "epoch": 0.19154471544715448,
      "step": 3534,
      "training_loss": 9.594426155090332
    },
    {
      "epoch": 0.1915989159891599,
      "step": 3535,
      "training_loss": 6.626454830169678
    },
    {
      "epoch": 0.19165311653116532,
      "grad_norm": 21.104188919067383,
      "learning_rate": 1e-05,
      "loss": 7.7793,
      "step": 3536
    },
    {
      "epoch": 0.19165311653116532,
      "step": 3536,
      "training_loss": 4.722585678100586
    },
    {
      "epoch": 0.19170731707317074,
      "step": 3537,
      "training_loss": 6.965334415435791
    },
    {
      "epoch": 0.19176151761517615,
      "step": 3538,
      "training_loss": 7.1152753829956055
    },
    {
      "epoch": 0.19181571815718157,
      "step": 3539,
      "training_loss": 6.453128814697266
    },
    {
      "epoch": 0.191869918699187,
      "grad_norm": 29.41900634765625,
      "learning_rate": 1e-05,
      "loss": 6.3141,
      "step": 3540
    },
    {
      "epoch": 0.191869918699187,
      "step": 3540,
      "training_loss": 7.118203639984131
    },
    {
      "epoch": 0.1919241192411924,
      "step": 3541,
      "training_loss": 5.378966808319092
    },
    {
      "epoch": 0.19197831978319782,
      "step": 3542,
      "training_loss": 7.774205207824707
    },
    {
      "epoch": 0.19203252032520327,
      "step": 3543,
      "training_loss": 7.703166961669922
    },
    {
      "epoch": 0.19208672086720868,
      "grad_norm": 16.592958450317383,
      "learning_rate": 1e-05,
      "loss": 6.9936,
      "step": 3544
    },
    {
      "epoch": 0.19208672086720868,
      "step": 3544,
      "training_loss": 6.540310382843018
    },
    {
      "epoch": 0.1921409214092141,
      "step": 3545,
      "training_loss": 5.651665210723877
    },
    {
      "epoch": 0.19219512195121952,
      "step": 3546,
      "training_loss": 5.747505187988281
    },
    {
      "epoch": 0.19224932249322493,
      "step": 3547,
      "training_loss": 7.715489387512207
    },
    {
      "epoch": 0.19230352303523035,
      "grad_norm": 23.626712799072266,
      "learning_rate": 1e-05,
      "loss": 6.4137,
      "step": 3548
    },
    {
      "epoch": 0.19230352303523035,
      "step": 3548,
      "training_loss": 7.46294641494751
    },
    {
      "epoch": 0.19235772357723577,
      "step": 3549,
      "training_loss": 6.186453342437744
    },
    {
      "epoch": 0.19241192411924118,
      "step": 3550,
      "training_loss": 8.385303497314453
    },
    {
      "epoch": 0.1924661246612466,
      "step": 3551,
      "training_loss": 7.523306369781494
    },
    {
      "epoch": 0.19252032520325205,
      "grad_norm": 42.851768493652344,
      "learning_rate": 1e-05,
      "loss": 7.3895,
      "step": 3552
    },
    {
      "epoch": 0.19252032520325205,
      "step": 3552,
      "training_loss": 6.682075500488281
    },
    {
      "epoch": 0.19257452574525746,
      "step": 3553,
      "training_loss": 7.262420654296875
    },
    {
      "epoch": 0.19262872628726288,
      "step": 3554,
      "training_loss": 7.950928211212158
    },
    {
      "epoch": 0.1926829268292683,
      "step": 3555,
      "training_loss": 6.828555583953857
    },
    {
      "epoch": 0.19273712737127371,
      "grad_norm": 18.830814361572266,
      "learning_rate": 1e-05,
      "loss": 7.181,
      "step": 3556
    },
    {
      "epoch": 0.19273712737127371,
      "step": 3556,
      "training_loss": 6.7854838371276855
    },
    {
      "epoch": 0.19279132791327913,
      "step": 3557,
      "training_loss": 6.101210594177246
    },
    {
      "epoch": 0.19284552845528455,
      "step": 3558,
      "training_loss": 5.956278324127197
    },
    {
      "epoch": 0.19289972899728997,
      "step": 3559,
      "training_loss": 7.574970245361328
    },
    {
      "epoch": 0.19295392953929538,
      "grad_norm": 33.00794982910156,
      "learning_rate": 1e-05,
      "loss": 6.6045,
      "step": 3560
    },
    {
      "epoch": 0.19295392953929538,
      "step": 3560,
      "training_loss": 6.590302467346191
    },
    {
      "epoch": 0.1930081300813008,
      "step": 3561,
      "training_loss": 4.756896495819092
    },
    {
      "epoch": 0.19306233062330624,
      "step": 3562,
      "training_loss": 7.241307258605957
    },
    {
      "epoch": 0.19311653116531166,
      "step": 3563,
      "training_loss": 7.090335369110107
    },
    {
      "epoch": 0.19317073170731708,
      "grad_norm": 19.086322784423828,
      "learning_rate": 1e-05,
      "loss": 6.4197,
      "step": 3564
    },
    {
      "epoch": 0.19317073170731708,
      "step": 3564,
      "training_loss": 7.436647891998291
    },
    {
      "epoch": 0.1932249322493225,
      "step": 3565,
      "training_loss": 8.379436492919922
    },
    {
      "epoch": 0.1932791327913279,
      "step": 3566,
      "training_loss": 6.523434638977051
    },
    {
      "epoch": 0.19333333333333333,
      "step": 3567,
      "training_loss": 6.130964279174805
    },
    {
      "epoch": 0.19338753387533875,
      "grad_norm": 38.70289611816406,
      "learning_rate": 1e-05,
      "loss": 7.1176,
      "step": 3568
    },
    {
      "epoch": 0.19338753387533875,
      "step": 3568,
      "training_loss": 7.204666614532471
    },
    {
      "epoch": 0.19344173441734416,
      "step": 3569,
      "training_loss": 7.106040000915527
    },
    {
      "epoch": 0.19349593495934958,
      "step": 3570,
      "training_loss": 6.819925308227539
    },
    {
      "epoch": 0.19355013550135503,
      "step": 3571,
      "training_loss": 6.470937728881836
    },
    {
      "epoch": 0.19360433604336044,
      "grad_norm": 19.954730987548828,
      "learning_rate": 1e-05,
      "loss": 6.9004,
      "step": 3572
    },
    {
      "epoch": 0.19360433604336044,
      "step": 3572,
      "training_loss": 6.625735759735107
    },
    {
      "epoch": 0.19365853658536586,
      "step": 3573,
      "training_loss": 7.299422740936279
    },
    {
      "epoch": 0.19371273712737128,
      "step": 3574,
      "training_loss": 7.502279758453369
    },
    {
      "epoch": 0.1937669376693767,
      "step": 3575,
      "training_loss": 7.361159324645996
    },
    {
      "epoch": 0.1938211382113821,
      "grad_norm": 27.219772338867188,
      "learning_rate": 1e-05,
      "loss": 7.1971,
      "step": 3576
    },
    {
      "epoch": 0.1938211382113821,
      "step": 3576,
      "training_loss": 6.324658393859863
    },
    {
      "epoch": 0.19387533875338753,
      "step": 3577,
      "training_loss": 6.833270072937012
    },
    {
      "epoch": 0.19392953929539294,
      "step": 3578,
      "training_loss": 7.275321006774902
    },
    {
      "epoch": 0.19398373983739836,
      "step": 3579,
      "training_loss": 7.868011951446533
    },
    {
      "epoch": 0.1940379403794038,
      "grad_norm": 59.7314453125,
      "learning_rate": 1e-05,
      "loss": 7.0753,
      "step": 3580
    },
    {
      "epoch": 0.1940379403794038,
      "step": 3580,
      "training_loss": 5.897635459899902
    },
    {
      "epoch": 0.19409214092140922,
      "step": 3581,
      "training_loss": 6.910755634307861
    },
    {
      "epoch": 0.19414634146341464,
      "step": 3582,
      "training_loss": 8.202655792236328
    },
    {
      "epoch": 0.19420054200542006,
      "step": 3583,
      "training_loss": 6.196660041809082
    },
    {
      "epoch": 0.19425474254742547,
      "grad_norm": 21.58075714111328,
      "learning_rate": 1e-05,
      "loss": 6.8019,
      "step": 3584
    },
    {
      "epoch": 0.19425474254742547,
      "step": 3584,
      "training_loss": 6.014052867889404
    },
    {
      "epoch": 0.1943089430894309,
      "step": 3585,
      "training_loss": 7.2250823974609375
    },
    {
      "epoch": 0.1943631436314363,
      "step": 3586,
      "training_loss": 5.885572910308838
    },
    {
      "epoch": 0.19441734417344173,
      "step": 3587,
      "training_loss": 7.1241350173950195
    },
    {
      "epoch": 0.19447154471544714,
      "grad_norm": 15.550854682922363,
      "learning_rate": 1e-05,
      "loss": 6.5622,
      "step": 3588
    },
    {
      "epoch": 0.19447154471544714,
      "step": 3588,
      "training_loss": 5.759889125823975
    },
    {
      "epoch": 0.1945257452574526,
      "step": 3589,
      "training_loss": 5.750014305114746
    },
    {
      "epoch": 0.194579945799458,
      "step": 3590,
      "training_loss": 6.830233573913574
    },
    {
      "epoch": 0.19463414634146342,
      "step": 3591,
      "training_loss": 7.1210479736328125
    },
    {
      "epoch": 0.19468834688346884,
      "grad_norm": 41.917236328125,
      "learning_rate": 1e-05,
      "loss": 6.3653,
      "step": 3592
    },
    {
      "epoch": 0.19468834688346884,
      "step": 3592,
      "training_loss": 6.646726608276367
    },
    {
      "epoch": 0.19474254742547426,
      "step": 3593,
      "training_loss": 6.808524131774902
    },
    {
      "epoch": 0.19479674796747967,
      "step": 3594,
      "training_loss": 7.074746131896973
    },
    {
      "epoch": 0.1948509485094851,
      "step": 3595,
      "training_loss": 7.535642623901367
    },
    {
      "epoch": 0.1949051490514905,
      "grad_norm": 15.21619987487793,
      "learning_rate": 1e-05,
      "loss": 7.0164,
      "step": 3596
    },
    {
      "epoch": 0.1949051490514905,
      "step": 3596,
      "training_loss": 7.188093662261963
    },
    {
      "epoch": 0.19495934959349592,
      "step": 3597,
      "training_loss": 6.375606536865234
    },
    {
      "epoch": 0.19501355013550137,
      "step": 3598,
      "training_loss": 6.8431077003479
    },
    {
      "epoch": 0.19506775067750678,
      "step": 3599,
      "training_loss": 6.619203090667725
    },
    {
      "epoch": 0.1951219512195122,
      "grad_norm": 17.445575714111328,
      "learning_rate": 1e-05,
      "loss": 6.7565,
      "step": 3600
    },
    {
      "epoch": 0.1951219512195122,
      "step": 3600,
      "training_loss": 6.647426605224609
    },
    {
      "epoch": 0.19517615176151762,
      "step": 3601,
      "training_loss": 7.542350769042969
    },
    {
      "epoch": 0.19523035230352304,
      "step": 3602,
      "training_loss": 5.918890476226807
    },
    {
      "epoch": 0.19528455284552845,
      "step": 3603,
      "training_loss": 7.953139781951904
    },
    {
      "epoch": 0.19533875338753387,
      "grad_norm": 26.28270149230957,
      "learning_rate": 1e-05,
      "loss": 7.0155,
      "step": 3604
    },
    {
      "epoch": 0.19533875338753387,
      "step": 3604,
      "training_loss": 5.173587799072266
    },
    {
      "epoch": 0.1953929539295393,
      "step": 3605,
      "training_loss": 6.5647196769714355
    },
    {
      "epoch": 0.1954471544715447,
      "step": 3606,
      "training_loss": 5.8073225021362305
    },
    {
      "epoch": 0.19550135501355015,
      "step": 3607,
      "training_loss": 7.742386817932129
    },
    {
      "epoch": 0.19555555555555557,
      "grad_norm": 30.476964950561523,
      "learning_rate": 1e-05,
      "loss": 6.322,
      "step": 3608
    },
    {
      "epoch": 0.19555555555555557,
      "step": 3608,
      "training_loss": 7.219214916229248
    },
    {
      "epoch": 0.19560975609756098,
      "step": 3609,
      "training_loss": 5.867938041687012
    },
    {
      "epoch": 0.1956639566395664,
      "step": 3610,
      "training_loss": 7.985525608062744
    },
    {
      "epoch": 0.19571815718157182,
      "step": 3611,
      "training_loss": 7.248887538909912
    },
    {
      "epoch": 0.19577235772357723,
      "grad_norm": 23.288326263427734,
      "learning_rate": 1e-05,
      "loss": 7.0804,
      "step": 3612
    },
    {
      "epoch": 0.19577235772357723,
      "step": 3612,
      "training_loss": 6.982251167297363
    },
    {
      "epoch": 0.19582655826558265,
      "step": 3613,
      "training_loss": 8.432421684265137
    },
    {
      "epoch": 0.19588075880758807,
      "step": 3614,
      "training_loss": 7.005619525909424
    },
    {
      "epoch": 0.19593495934959348,
      "step": 3615,
      "training_loss": 7.306734561920166
    },
    {
      "epoch": 0.19598915989159893,
      "grad_norm": 17.739580154418945,
      "learning_rate": 1e-05,
      "loss": 7.4318,
      "step": 3616
    },
    {
      "epoch": 0.19598915989159893,
      "step": 3616,
      "training_loss": 7.70357084274292
    },
    {
      "epoch": 0.19604336043360435,
      "step": 3617,
      "training_loss": 7.45521354675293
    },
    {
      "epoch": 0.19609756097560976,
      "step": 3618,
      "training_loss": 8.275249481201172
    },
    {
      "epoch": 0.19615176151761518,
      "step": 3619,
      "training_loss": 6.4157891273498535
    },
    {
      "epoch": 0.1962059620596206,
      "grad_norm": 25.05075454711914,
      "learning_rate": 1e-05,
      "loss": 7.4625,
      "step": 3620
    },
    {
      "epoch": 0.1962059620596206,
      "step": 3620,
      "training_loss": 8.002338409423828
    },
    {
      "epoch": 0.19626016260162601,
      "step": 3621,
      "training_loss": 7.151495456695557
    },
    {
      "epoch": 0.19631436314363143,
      "step": 3622,
      "training_loss": 7.340665340423584
    },
    {
      "epoch": 0.19636856368563685,
      "step": 3623,
      "training_loss": 5.787600994110107
    },
    {
      "epoch": 0.19642276422764227,
      "grad_norm": 21.339664459228516,
      "learning_rate": 1e-05,
      "loss": 7.0705,
      "step": 3624
    },
    {
      "epoch": 0.19642276422764227,
      "step": 3624,
      "training_loss": 5.194981098175049
    },
    {
      "epoch": 0.19647696476964768,
      "step": 3625,
      "training_loss": 7.2146382331848145
    },
    {
      "epoch": 0.19653116531165313,
      "step": 3626,
      "training_loss": 6.672301769256592
    },
    {
      "epoch": 0.19658536585365854,
      "step": 3627,
      "training_loss": 6.368736743927002
    },
    {
      "epoch": 0.19663956639566396,
      "grad_norm": 21.93890380859375,
      "learning_rate": 1e-05,
      "loss": 6.3627,
      "step": 3628
    },
    {
      "epoch": 0.19663956639566396,
      "step": 3628,
      "training_loss": 5.3946757316589355
    },
    {
      "epoch": 0.19669376693766938,
      "step": 3629,
      "training_loss": 6.938905715942383
    },
    {
      "epoch": 0.1967479674796748,
      "step": 3630,
      "training_loss": 6.957594871520996
    },
    {
      "epoch": 0.1968021680216802,
      "step": 3631,
      "training_loss": 4.491446018218994
    },
    {
      "epoch": 0.19685636856368563,
      "grad_norm": 21.67856788635254,
      "learning_rate": 1e-05,
      "loss": 5.9457,
      "step": 3632
    },
    {
      "epoch": 0.19685636856368563,
      "step": 3632,
      "training_loss": 10.10795783996582
    },
    {
      "epoch": 0.19691056910569105,
      "step": 3633,
      "training_loss": 7.300577640533447
    },
    {
      "epoch": 0.19696476964769646,
      "step": 3634,
      "training_loss": 9.514747619628906
    },
    {
      "epoch": 0.1970189701897019,
      "step": 3635,
      "training_loss": 5.756318092346191
    },
    {
      "epoch": 0.19707317073170733,
      "grad_norm": 24.290868759155273,
      "learning_rate": 1e-05,
      "loss": 8.1699,
      "step": 3636
    },
    {
      "epoch": 0.19707317073170733,
      "step": 3636,
      "training_loss": 5.519053936004639
    },
    {
      "epoch": 0.19712737127371274,
      "step": 3637,
      "training_loss": 6.770873546600342
    },
    {
      "epoch": 0.19718157181571816,
      "step": 3638,
      "training_loss": 7.256626605987549
    },
    {
      "epoch": 0.19723577235772358,
      "step": 3639,
      "training_loss": 8.113604545593262
    },
    {
      "epoch": 0.197289972899729,
      "grad_norm": 37.62157440185547,
      "learning_rate": 1e-05,
      "loss": 6.915,
      "step": 3640
    },
    {
      "epoch": 0.197289972899729,
      "step": 3640,
      "training_loss": 4.712960720062256
    },
    {
      "epoch": 0.1973441734417344,
      "step": 3641,
      "training_loss": 7.667952537536621
    },
    {
      "epoch": 0.19739837398373983,
      "step": 3642,
      "training_loss": 7.2596564292907715
    },
    {
      "epoch": 0.19745257452574524,
      "step": 3643,
      "training_loss": 6.276018142700195
    },
    {
      "epoch": 0.1975067750677507,
      "grad_norm": 16.380876541137695,
      "learning_rate": 1e-05,
      "loss": 6.4791,
      "step": 3644
    },
    {
      "epoch": 0.1975067750677507,
      "step": 3644,
      "training_loss": 6.901395320892334
    },
    {
      "epoch": 0.1975609756097561,
      "step": 3645,
      "training_loss": 7.820331573486328
    },
    {
      "epoch": 0.19761517615176152,
      "step": 3646,
      "training_loss": 6.393054008483887
    },
    {
      "epoch": 0.19766937669376694,
      "step": 3647,
      "training_loss": 6.307424068450928
    },
    {
      "epoch": 0.19772357723577236,
      "grad_norm": 28.34696388244629,
      "learning_rate": 1e-05,
      "loss": 6.8556,
      "step": 3648
    },
    {
      "epoch": 0.19772357723577236,
      "step": 3648,
      "training_loss": 7.298562526702881
    },
    {
      "epoch": 0.19777777777777777,
      "step": 3649,
      "training_loss": 7.6671342849731445
    },
    {
      "epoch": 0.1978319783197832,
      "step": 3650,
      "training_loss": 8.117452621459961
    },
    {
      "epoch": 0.1978861788617886,
      "step": 3651,
      "training_loss": 6.682476997375488
    },
    {
      "epoch": 0.19794037940379403,
      "grad_norm": 19.50032615661621,
      "learning_rate": 1e-05,
      "loss": 7.4414,
      "step": 3652
    },
    {
      "epoch": 0.19794037940379403,
      "step": 3652,
      "training_loss": 7.641303062438965
    },
    {
      "epoch": 0.19799457994579947,
      "step": 3653,
      "training_loss": 7.8183393478393555
    },
    {
      "epoch": 0.1980487804878049,
      "step": 3654,
      "training_loss": 7.477652072906494
    },
    {
      "epoch": 0.1981029810298103,
      "step": 3655,
      "training_loss": 5.818542003631592
    },
    {
      "epoch": 0.19815718157181572,
      "grad_norm": 27.886316299438477,
      "learning_rate": 1e-05,
      "loss": 7.189,
      "step": 3656
    },
    {
      "epoch": 0.19815718157181572,
      "step": 3656,
      "training_loss": 7.092739582061768
    },
    {
      "epoch": 0.19821138211382114,
      "step": 3657,
      "training_loss": 8.065757751464844
    },
    {
      "epoch": 0.19826558265582656,
      "step": 3658,
      "training_loss": 8.45667552947998
    },
    {
      "epoch": 0.19831978319783197,
      "step": 3659,
      "training_loss": 6.902286529541016
    },
    {
      "epoch": 0.1983739837398374,
      "grad_norm": 29.202165603637695,
      "learning_rate": 1e-05,
      "loss": 7.6294,
      "step": 3660
    },
    {
      "epoch": 0.1983739837398374,
      "step": 3660,
      "training_loss": 6.8144612312316895
    },
    {
      "epoch": 0.1984281842818428,
      "step": 3661,
      "training_loss": 7.959171772003174
    },
    {
      "epoch": 0.19848238482384825,
      "step": 3662,
      "training_loss": 7.68939208984375
    },
    {
      "epoch": 0.19853658536585367,
      "step": 3663,
      "training_loss": 7.173436641693115
    },
    {
      "epoch": 0.19859078590785909,
      "grad_norm": 19.999155044555664,
      "learning_rate": 1e-05,
      "loss": 7.4091,
      "step": 3664
    },
    {
      "epoch": 0.19859078590785909,
      "step": 3664,
      "training_loss": 7.908653736114502
    },
    {
      "epoch": 0.1986449864498645,
      "step": 3665,
      "training_loss": 6.844855785369873
    },
    {
      "epoch": 0.19869918699186992,
      "step": 3666,
      "training_loss": 7.200395584106445
    },
    {
      "epoch": 0.19875338753387534,
      "step": 3667,
      "training_loss": 7.60434627532959
    },
    {
      "epoch": 0.19880758807588075,
      "grad_norm": 23.82035255432129,
      "learning_rate": 1e-05,
      "loss": 7.3896,
      "step": 3668
    },
    {
      "epoch": 0.19880758807588075,
      "step": 3668,
      "training_loss": 6.539467811584473
    },
    {
      "epoch": 0.19886178861788617,
      "step": 3669,
      "training_loss": 7.424093723297119
    },
    {
      "epoch": 0.1989159891598916,
      "step": 3670,
      "training_loss": 6.190935134887695
    },
    {
      "epoch": 0.19897018970189703,
      "step": 3671,
      "training_loss": 6.709878444671631
    },
    {
      "epoch": 0.19902439024390245,
      "grad_norm": 17.562734603881836,
      "learning_rate": 1e-05,
      "loss": 6.7161,
      "step": 3672
    },
    {
      "epoch": 0.19902439024390245,
      "step": 3672,
      "training_loss": 6.6650800704956055
    },
    {
      "epoch": 0.19907859078590787,
      "step": 3673,
      "training_loss": 7.3142218589782715
    },
    {
      "epoch": 0.19913279132791328,
      "step": 3674,
      "training_loss": 7.112971782684326
    },
    {
      "epoch": 0.1991869918699187,
      "step": 3675,
      "training_loss": 7.9690775871276855
    },
    {
      "epoch": 0.19924119241192412,
      "grad_norm": 20.131502151489258,
      "learning_rate": 1e-05,
      "loss": 7.2653,
      "step": 3676
    },
    {
      "epoch": 0.19924119241192412,
      "step": 3676,
      "training_loss": 7.467028617858887
    },
    {
      "epoch": 0.19929539295392953,
      "step": 3677,
      "training_loss": 7.4162116050720215
    },
    {
      "epoch": 0.19934959349593495,
      "step": 3678,
      "training_loss": 6.486652851104736
    },
    {
      "epoch": 0.19940379403794037,
      "step": 3679,
      "training_loss": 7.118208408355713
    },
    {
      "epoch": 0.1994579945799458,
      "grad_norm": 22.733455657958984,
      "learning_rate": 1e-05,
      "loss": 7.122,
      "step": 3680
    },
    {
      "epoch": 0.1994579945799458,
      "step": 3680,
      "training_loss": 6.63466739654541
    },
    {
      "epoch": 0.19951219512195123,
      "step": 3681,
      "training_loss": 7.488204002380371
    },
    {
      "epoch": 0.19956639566395665,
      "step": 3682,
      "training_loss": 6.862729549407959
    },
    {
      "epoch": 0.19962059620596206,
      "step": 3683,
      "training_loss": 6.670866966247559
    },
    {
      "epoch": 0.19967479674796748,
      "grad_norm": 32.30569076538086,
      "learning_rate": 1e-05,
      "loss": 6.9141,
      "step": 3684
    },
    {
      "epoch": 0.19967479674796748,
      "step": 3684,
      "training_loss": 7.37429141998291
    },
    {
      "epoch": 0.1997289972899729,
      "step": 3685,
      "training_loss": 7.108931541442871
    },
    {
      "epoch": 0.19978319783197832,
      "step": 3686,
      "training_loss": 5.195807456970215
    },
    {
      "epoch": 0.19983739837398373,
      "step": 3687,
      "training_loss": 7.150634288787842
    },
    {
      "epoch": 0.19989159891598915,
      "grad_norm": 17.38572883605957,
      "learning_rate": 1e-05,
      "loss": 6.7074,
      "step": 3688
    },
    {
      "epoch": 0.19989159891598915,
      "step": 3688,
      "training_loss": 6.8793511390686035
    },
    {
      "epoch": 0.19994579945799457,
      "step": 3689,
      "training_loss": 6.780917167663574
    },
    {
      "epoch": 0.2,
      "step": 3690,
      "training_loss": 6.2888054847717285
    },
    {
      "epoch": 0.20005420054200543,
      "step": 3691,
      "training_loss": 5.793900966644287
    },
    {
      "epoch": 0.20010840108401085,
      "grad_norm": 27.417531967163086,
      "learning_rate": 1e-05,
      "loss": 6.4357,
      "step": 3692
    },
    {
      "epoch": 0.20010840108401085,
      "step": 3692,
      "training_loss": 7.059779167175293
    },
    {
      "epoch": 0.20016260162601626,
      "step": 3693,
      "training_loss": 7.7476959228515625
    },
    {
      "epoch": 0.20021680216802168,
      "step": 3694,
      "training_loss": 8.078887939453125
    },
    {
      "epoch": 0.2002710027100271,
      "step": 3695,
      "training_loss": 7.748600006103516
    },
    {
      "epoch": 0.2003252032520325,
      "grad_norm": 24.214841842651367,
      "learning_rate": 1e-05,
      "loss": 7.6587,
      "step": 3696
    },
    {
      "epoch": 0.2003252032520325,
      "step": 3696,
      "training_loss": 5.402561187744141
    },
    {
      "epoch": 0.20037940379403793,
      "step": 3697,
      "training_loss": 7.602196216583252
    },
    {
      "epoch": 0.20043360433604335,
      "step": 3698,
      "training_loss": 6.460982322692871
    },
    {
      "epoch": 0.2004878048780488,
      "step": 3699,
      "training_loss": 6.8257246017456055
    },
    {
      "epoch": 0.2005420054200542,
      "grad_norm": 35.73275375366211,
      "learning_rate": 1e-05,
      "loss": 6.5729,
      "step": 3700
    },
    {
      "epoch": 0.2005420054200542,
      "step": 3700,
      "training_loss": 7.636689186096191
    },
    {
      "epoch": 0.20059620596205963,
      "step": 3701,
      "training_loss": 5.736073970794678
    },
    {
      "epoch": 0.20065040650406504,
      "step": 3702,
      "training_loss": 6.629544258117676
    },
    {
      "epoch": 0.20070460704607046,
      "step": 3703,
      "training_loss": 6.984662055969238
    },
    {
      "epoch": 0.20075880758807588,
      "grad_norm": 26.969566345214844,
      "learning_rate": 1e-05,
      "loss": 6.7467,
      "step": 3704
    },
    {
      "epoch": 0.20075880758807588,
      "step": 3704,
      "training_loss": 7.584930419921875
    },
    {
      "epoch": 0.2008130081300813,
      "step": 3705,
      "training_loss": 6.989312171936035
    },
    {
      "epoch": 0.2008672086720867,
      "step": 3706,
      "training_loss": 8.103487968444824
    },
    {
      "epoch": 0.20092140921409213,
      "step": 3707,
      "training_loss": 5.944775104522705
    },
    {
      "epoch": 0.20097560975609757,
      "grad_norm": 20.99077606201172,
      "learning_rate": 1e-05,
      "loss": 7.1556,
      "step": 3708
    },
    {
      "epoch": 0.20097560975609757,
      "step": 3708,
      "training_loss": 7.127499103546143
    },
    {
      "epoch": 0.201029810298103,
      "step": 3709,
      "training_loss": 7.067489147186279
    },
    {
      "epoch": 0.2010840108401084,
      "step": 3710,
      "training_loss": 8.529799461364746
    },
    {
      "epoch": 0.20113821138211382,
      "step": 3711,
      "training_loss": 5.936741352081299
    },
    {
      "epoch": 0.20119241192411924,
      "grad_norm": 14.440022468566895,
      "learning_rate": 1e-05,
      "loss": 7.1654,
      "step": 3712
    },
    {
      "epoch": 0.20119241192411924,
      "step": 3712,
      "training_loss": 5.79723596572876
    },
    {
      "epoch": 0.20124661246612466,
      "step": 3713,
      "training_loss": 6.195626258850098
    },
    {
      "epoch": 0.20130081300813008,
      "step": 3714,
      "training_loss": 7.336697101593018
    },
    {
      "epoch": 0.2013550135501355,
      "step": 3715,
      "training_loss": 6.103996276855469
    },
    {
      "epoch": 0.2014092140921409,
      "grad_norm": 27.086442947387695,
      "learning_rate": 1e-05,
      "loss": 6.3584,
      "step": 3716
    },
    {
      "epoch": 0.2014092140921409,
      "step": 3716,
      "training_loss": 9.423589706420898
    },
    {
      "epoch": 0.20146341463414635,
      "step": 3717,
      "training_loss": 7.2984466552734375
    },
    {
      "epoch": 0.20151761517615177,
      "step": 3718,
      "training_loss": 7.39729642868042
    },
    {
      "epoch": 0.2015718157181572,
      "step": 3719,
      "training_loss": 6.507932186126709
    },
    {
      "epoch": 0.2016260162601626,
      "grad_norm": 21.38241195678711,
      "learning_rate": 1e-05,
      "loss": 7.6568,
      "step": 3720
    },
    {
      "epoch": 0.2016260162601626,
      "step": 3720,
      "training_loss": 6.14583683013916
    },
    {
      "epoch": 0.20168021680216802,
      "step": 3721,
      "training_loss": 7.773763656616211
    },
    {
      "epoch": 0.20173441734417344,
      "step": 3722,
      "training_loss": 6.346989154815674
    },
    {
      "epoch": 0.20178861788617886,
      "step": 3723,
      "training_loss": 7.759746074676514
    },
    {
      "epoch": 0.20184281842818427,
      "grad_norm": 28.2387638092041,
      "learning_rate": 1e-05,
      "loss": 7.0066,
      "step": 3724
    },
    {
      "epoch": 0.20184281842818427,
      "step": 3724,
      "training_loss": 6.691409111022949
    },
    {
      "epoch": 0.2018970189701897,
      "step": 3725,
      "training_loss": 7.230884552001953
    },
    {
      "epoch": 0.20195121951219513,
      "step": 3726,
      "training_loss": 7.2621283531188965
    },
    {
      "epoch": 0.20200542005420055,
      "step": 3727,
      "training_loss": 6.4293975830078125
    },
    {
      "epoch": 0.20205962059620597,
      "grad_norm": 18.04242706298828,
      "learning_rate": 1e-05,
      "loss": 6.9035,
      "step": 3728
    },
    {
      "epoch": 0.20205962059620597,
      "step": 3728,
      "training_loss": 6.093830585479736
    },
    {
      "epoch": 0.20211382113821139,
      "step": 3729,
      "training_loss": 7.133187294006348
    },
    {
      "epoch": 0.2021680216802168,
      "step": 3730,
      "training_loss": 7.707437515258789
    },
    {
      "epoch": 0.20222222222222222,
      "step": 3731,
      "training_loss": 6.288330078125
    },
    {
      "epoch": 0.20227642276422764,
      "grad_norm": 25.852039337158203,
      "learning_rate": 1e-05,
      "loss": 6.8057,
      "step": 3732
    },
    {
      "epoch": 0.20227642276422764,
      "step": 3732,
      "training_loss": 8.462844848632812
    },
    {
      "epoch": 0.20233062330623305,
      "step": 3733,
      "training_loss": 7.386443138122559
    },
    {
      "epoch": 0.20238482384823847,
      "step": 3734,
      "training_loss": 5.888491630554199
    },
    {
      "epoch": 0.20243902439024392,
      "step": 3735,
      "training_loss": 10.228243827819824
    },
    {
      "epoch": 0.20249322493224933,
      "grad_norm": 62.90739059448242,
      "learning_rate": 1e-05,
      "loss": 7.9915,
      "step": 3736
    },
    {
      "epoch": 0.20249322493224933,
      "step": 3736,
      "training_loss": 6.906584739685059
    },
    {
      "epoch": 0.20254742547425475,
      "step": 3737,
      "training_loss": 7.593590259552002
    },
    {
      "epoch": 0.20260162601626017,
      "step": 3738,
      "training_loss": 6.996849536895752
    },
    {
      "epoch": 0.20265582655826558,
      "step": 3739,
      "training_loss": 7.42687463760376
    },
    {
      "epoch": 0.202710027100271,
      "grad_norm": 32.043479919433594,
      "learning_rate": 1e-05,
      "loss": 7.231,
      "step": 3740
    },
    {
      "epoch": 0.202710027100271,
      "step": 3740,
      "training_loss": 6.641480922698975
    },
    {
      "epoch": 0.20276422764227642,
      "step": 3741,
      "training_loss": 5.6986470222473145
    },
    {
      "epoch": 0.20281842818428183,
      "step": 3742,
      "training_loss": 7.252570629119873
    },
    {
      "epoch": 0.20287262872628725,
      "step": 3743,
      "training_loss": 7.341577053070068
    },
    {
      "epoch": 0.2029268292682927,
      "grad_norm": 18.72827911376953,
      "learning_rate": 1e-05,
      "loss": 6.7336,
      "step": 3744
    },
    {
      "epoch": 0.2029268292682927,
      "step": 3744,
      "training_loss": 7.952272891998291
    },
    {
      "epoch": 0.2029810298102981,
      "step": 3745,
      "training_loss": 6.965378284454346
    },
    {
      "epoch": 0.20303523035230353,
      "step": 3746,
      "training_loss": 4.7070088386535645
    },
    {
      "epoch": 0.20308943089430895,
      "step": 3747,
      "training_loss": 7.248885154724121
    },
    {
      "epoch": 0.20314363143631436,
      "grad_norm": 25.276844024658203,
      "learning_rate": 1e-05,
      "loss": 6.7184,
      "step": 3748
    },
    {
      "epoch": 0.20314363143631436,
      "step": 3748,
      "training_loss": 6.406479835510254
    },
    {
      "epoch": 0.20319783197831978,
      "step": 3749,
      "training_loss": 7.403736591339111
    },
    {
      "epoch": 0.2032520325203252,
      "step": 3750,
      "training_loss": 5.089934825897217
    },
    {
      "epoch": 0.20330623306233062,
      "step": 3751,
      "training_loss": 7.201196670532227
    },
    {
      "epoch": 0.20336043360433603,
      "grad_norm": 23.481536865234375,
      "learning_rate": 1e-05,
      "loss": 6.5253,
      "step": 3752
    },
    {
      "epoch": 0.20336043360433603,
      "step": 3752,
      "training_loss": 4.735683917999268
    },
    {
      "epoch": 0.20341463414634145,
      "step": 3753,
      "training_loss": 7.275781154632568
    },
    {
      "epoch": 0.2034688346883469,
      "step": 3754,
      "training_loss": 6.5600810050964355
    },
    {
      "epoch": 0.2035230352303523,
      "step": 3755,
      "training_loss": 7.231378078460693
    },
    {
      "epoch": 0.20357723577235773,
      "grad_norm": 16.868249893188477,
      "learning_rate": 1e-05,
      "loss": 6.4507,
      "step": 3756
    },
    {
      "epoch": 0.20357723577235773,
      "step": 3756,
      "training_loss": 6.266883373260498
    },
    {
      "epoch": 0.20363143631436315,
      "step": 3757,
      "training_loss": 7.108072280883789
    },
    {
      "epoch": 0.20368563685636856,
      "step": 3758,
      "training_loss": 7.5145463943481445
    },
    {
      "epoch": 0.20373983739837398,
      "step": 3759,
      "training_loss": 6.8705573081970215
    },
    {
      "epoch": 0.2037940379403794,
      "grad_norm": 19.499956130981445,
      "learning_rate": 1e-05,
      "loss": 6.94,
      "step": 3760
    },
    {
      "epoch": 0.2037940379403794,
      "step": 3760,
      "training_loss": 7.254319190979004
    },
    {
      "epoch": 0.2038482384823848,
      "step": 3761,
      "training_loss": 7.747385501861572
    },
    {
      "epoch": 0.20390243902439023,
      "step": 3762,
      "training_loss": 6.521635055541992
    },
    {
      "epoch": 0.20395663956639568,
      "step": 3763,
      "training_loss": 7.14340353012085
    },
    {
      "epoch": 0.2040108401084011,
      "grad_norm": 15.998696327209473,
      "learning_rate": 1e-05,
      "loss": 7.1667,
      "step": 3764
    },
    {
      "epoch": 0.2040108401084011,
      "step": 3764,
      "training_loss": 6.375705718994141
    },
    {
      "epoch": 0.2040650406504065,
      "step": 3765,
      "training_loss": 6.8692307472229
    },
    {
      "epoch": 0.20411924119241193,
      "step": 3766,
      "training_loss": 5.7958879470825195
    },
    {
      "epoch": 0.20417344173441734,
      "step": 3767,
      "training_loss": 5.278838157653809
    },
    {
      "epoch": 0.20422764227642276,
      "grad_norm": 23.002012252807617,
      "learning_rate": 1e-05,
      "loss": 6.0799,
      "step": 3768
    },
    {
      "epoch": 0.20422764227642276,
      "step": 3768,
      "training_loss": 6.560371398925781
    },
    {
      "epoch": 0.20428184281842818,
      "step": 3769,
      "training_loss": 7.05556058883667
    },
    {
      "epoch": 0.2043360433604336,
      "step": 3770,
      "training_loss": 5.911425590515137
    },
    {
      "epoch": 0.204390243902439,
      "step": 3771,
      "training_loss": 6.529625415802002
    },
    {
      "epoch": 0.20444444444444446,
      "grad_norm": 32.05925750732422,
      "learning_rate": 1e-05,
      "loss": 6.5142,
      "step": 3772
    },
    {
      "epoch": 0.20444444444444446,
      "step": 3772,
      "training_loss": 7.186750888824463
    },
    {
      "epoch": 0.20449864498644987,
      "step": 3773,
      "training_loss": 7.254762172698975
    },
    {
      "epoch": 0.2045528455284553,
      "step": 3774,
      "training_loss": 7.112131595611572
    },
    {
      "epoch": 0.2046070460704607,
      "step": 3775,
      "training_loss": 4.544216632843018
    },
    {
      "epoch": 0.20466124661246612,
      "grad_norm": 30.24347686767578,
      "learning_rate": 1e-05,
      "loss": 6.5245,
      "step": 3776
    },
    {
      "epoch": 0.20466124661246612,
      "step": 3776,
      "training_loss": 5.960275650024414
    },
    {
      "epoch": 0.20471544715447154,
      "step": 3777,
      "training_loss": 5.438753604888916
    },
    {
      "epoch": 0.20476964769647696,
      "step": 3778,
      "training_loss": 6.213459491729736
    },
    {
      "epoch": 0.20482384823848238,
      "step": 3779,
      "training_loss": 7.096193313598633
    },
    {
      "epoch": 0.2048780487804878,
      "grad_norm": 26.780899047851562,
      "learning_rate": 1e-05,
      "loss": 6.1772,
      "step": 3780
    },
    {
      "epoch": 0.2048780487804878,
      "step": 3780,
      "training_loss": 6.912062168121338
    },
    {
      "epoch": 0.20493224932249324,
      "step": 3781,
      "training_loss": 6.910130500793457
    },
    {
      "epoch": 0.20498644986449865,
      "step": 3782,
      "training_loss": 5.577674388885498
    },
    {
      "epoch": 0.20504065040650407,
      "step": 3783,
      "training_loss": 6.965164661407471
    },
    {
      "epoch": 0.2050948509485095,
      "grad_norm": 27.676620483398438,
      "learning_rate": 1e-05,
      "loss": 6.5913,
      "step": 3784
    },
    {
      "epoch": 0.2050948509485095,
      "step": 3784,
      "training_loss": 6.469407081604004
    },
    {
      "epoch": 0.2051490514905149,
      "step": 3785,
      "training_loss": 4.501212120056152
    },
    {
      "epoch": 0.20520325203252032,
      "step": 3786,
      "training_loss": 7.512286186218262
    },
    {
      "epoch": 0.20525745257452574,
      "step": 3787,
      "training_loss": 6.7867560386657715
    },
    {
      "epoch": 0.20531165311653116,
      "grad_norm": 13.730051040649414,
      "learning_rate": 1e-05,
      "loss": 6.3174,
      "step": 3788
    },
    {
      "epoch": 0.20531165311653116,
      "step": 3788,
      "training_loss": 5.275069236755371
    },
    {
      "epoch": 0.20536585365853657,
      "step": 3789,
      "training_loss": 8.25909423828125
    },
    {
      "epoch": 0.20542005420054202,
      "step": 3790,
      "training_loss": 5.658388137817383
    },
    {
      "epoch": 0.20547425474254744,
      "step": 3791,
      "training_loss": 5.53835391998291
    },
    {
      "epoch": 0.20552845528455285,
      "grad_norm": 24.392990112304688,
      "learning_rate": 1e-05,
      "loss": 6.1827,
      "step": 3792
    },
    {
      "epoch": 0.20552845528455285,
      "step": 3792,
      "training_loss": 7.258354663848877
    },
    {
      "epoch": 0.20558265582655827,
      "step": 3793,
      "training_loss": 6.920764923095703
    },
    {
      "epoch": 0.2056368563685637,
      "step": 3794,
      "training_loss": 6.990993022918701
    },
    {
      "epoch": 0.2056910569105691,
      "step": 3795,
      "training_loss": 8.564592361450195
    },
    {
      "epoch": 0.20574525745257452,
      "grad_norm": 23.733346939086914,
      "learning_rate": 1e-05,
      "loss": 7.4337,
      "step": 3796
    },
    {
      "epoch": 0.20574525745257452,
      "step": 3796,
      "training_loss": 5.875835418701172
    },
    {
      "epoch": 0.20579945799457994,
      "step": 3797,
      "training_loss": 6.975363731384277
    },
    {
      "epoch": 0.20585365853658535,
      "step": 3798,
      "training_loss": 7.160928249359131
    },
    {
      "epoch": 0.2059078590785908,
      "step": 3799,
      "training_loss": 5.941561698913574
    },
    {
      "epoch": 0.20596205962059622,
      "grad_norm": 35.57673645019531,
      "learning_rate": 1e-05,
      "loss": 6.4884,
      "step": 3800
    },
    {
      "epoch": 0.20596205962059622,
      "step": 3800,
      "training_loss": 6.046288967132568
    },
    {
      "epoch": 0.20601626016260163,
      "step": 3801,
      "training_loss": 7.517364025115967
    },
    {
      "epoch": 0.20607046070460705,
      "step": 3802,
      "training_loss": 8.301523208618164
    },
    {
      "epoch": 0.20612466124661247,
      "step": 3803,
      "training_loss": 6.801531791687012
    },
    {
      "epoch": 0.20617886178861788,
      "grad_norm": 26.156278610229492,
      "learning_rate": 1e-05,
      "loss": 7.1667,
      "step": 3804
    },
    {
      "epoch": 0.20617886178861788,
      "step": 3804,
      "training_loss": 4.75673246383667
    },
    {
      "epoch": 0.2062330623306233,
      "step": 3805,
      "training_loss": 7.181939601898193
    },
    {
      "epoch": 0.20628726287262872,
      "step": 3806,
      "training_loss": 6.025169372558594
    },
    {
      "epoch": 0.20634146341463414,
      "step": 3807,
      "training_loss": 4.7658371925354
    },
    {
      "epoch": 0.20639566395663958,
      "grad_norm": 27.089412689208984,
      "learning_rate": 1e-05,
      "loss": 5.6824,
      "step": 3808
    },
    {
      "epoch": 0.20639566395663958,
      "step": 3808,
      "training_loss": 5.95260763168335
    },
    {
      "epoch": 0.206449864498645,
      "step": 3809,
      "training_loss": 7.122954845428467
    },
    {
      "epoch": 0.20650406504065041,
      "step": 3810,
      "training_loss": 7.892776012420654
    },
    {
      "epoch": 0.20655826558265583,
      "step": 3811,
      "training_loss": 6.233120918273926
    },
    {
      "epoch": 0.20661246612466125,
      "grad_norm": 22.90690803527832,
      "learning_rate": 1e-05,
      "loss": 6.8004,
      "step": 3812
    },
    {
      "epoch": 0.20661246612466125,
      "step": 3812,
      "training_loss": 8.103516578674316
    },
    {
      "epoch": 0.20666666666666667,
      "step": 3813,
      "training_loss": 6.692227363586426
    },
    {
      "epoch": 0.20672086720867208,
      "step": 3814,
      "training_loss": 7.558800220489502
    },
    {
      "epoch": 0.2067750677506775,
      "step": 3815,
      "training_loss": 5.164167881011963
    },
    {
      "epoch": 0.20682926829268292,
      "grad_norm": 19.71681022644043,
      "learning_rate": 1e-05,
      "loss": 6.8797,
      "step": 3816
    },
    {
      "epoch": 0.20682926829268292,
      "step": 3816,
      "training_loss": 7.053528785705566
    },
    {
      "epoch": 0.20688346883468833,
      "step": 3817,
      "training_loss": 7.225793361663818
    },
    {
      "epoch": 0.20693766937669378,
      "step": 3818,
      "training_loss": 6.050297260284424
    },
    {
      "epoch": 0.2069918699186992,
      "step": 3819,
      "training_loss": 5.7134222984313965
    },
    {
      "epoch": 0.2070460704607046,
      "grad_norm": 31.173913955688477,
      "learning_rate": 1e-05,
      "loss": 6.5108,
      "step": 3820
    },
    {
      "epoch": 0.2070460704607046,
      "step": 3820,
      "training_loss": 6.234885215759277
    },
    {
      "epoch": 0.20710027100271003,
      "step": 3821,
      "training_loss": 6.689486026763916
    },
    {
      "epoch": 0.20715447154471545,
      "step": 3822,
      "training_loss": 7.462898254394531
    },
    {
      "epoch": 0.20720867208672086,
      "step": 3823,
      "training_loss": 6.996607303619385
    },
    {
      "epoch": 0.20726287262872628,
      "grad_norm": 19.794387817382812,
      "learning_rate": 1e-05,
      "loss": 6.846,
      "step": 3824
    },
    {
      "epoch": 0.20726287262872628,
      "step": 3824,
      "training_loss": 5.971132755279541
    },
    {
      "epoch": 0.2073170731707317,
      "step": 3825,
      "training_loss": 6.921026706695557
    },
    {
      "epoch": 0.20737127371273711,
      "step": 3826,
      "training_loss": 8.004469871520996
    },
    {
      "epoch": 0.20742547425474256,
      "step": 3827,
      "training_loss": 7.335885047912598
    },
    {
      "epoch": 0.20747967479674798,
      "grad_norm": 15.814802169799805,
      "learning_rate": 1e-05,
      "loss": 7.0581,
      "step": 3828
    },
    {
      "epoch": 0.20747967479674798,
      "step": 3828,
      "training_loss": 5.919142246246338
    },
    {
      "epoch": 0.2075338753387534,
      "step": 3829,
      "training_loss": 8.005236625671387
    },
    {
      "epoch": 0.2075880758807588,
      "step": 3830,
      "training_loss": 6.948180675506592
    },
    {
      "epoch": 0.20764227642276423,
      "step": 3831,
      "training_loss": 6.789438247680664
    },
    {
      "epoch": 0.20769647696476964,
      "grad_norm": 17.76123809814453,
      "learning_rate": 1e-05,
      "loss": 6.9155,
      "step": 3832
    },
    {
      "epoch": 0.20769647696476964,
      "step": 3832,
      "training_loss": 5.358366012573242
    },
    {
      "epoch": 0.20775067750677506,
      "step": 3833,
      "training_loss": 7.674780368804932
    },
    {
      "epoch": 0.20780487804878048,
      "step": 3834,
      "training_loss": 5.386256217956543
    },
    {
      "epoch": 0.2078590785907859,
      "step": 3835,
      "training_loss": 8.076111793518066
    },
    {
      "epoch": 0.20791327913279134,
      "grad_norm": 39.06454849243164,
      "learning_rate": 1e-05,
      "loss": 6.6239,
      "step": 3836
    },
    {
      "epoch": 0.20791327913279134,
      "step": 3836,
      "training_loss": 7.03363561630249
    },
    {
      "epoch": 0.20796747967479676,
      "step": 3837,
      "training_loss": 7.025528907775879
    },
    {
      "epoch": 0.20802168021680217,
      "step": 3838,
      "training_loss": 6.799386978149414
    },
    {
      "epoch": 0.2080758807588076,
      "step": 3839,
      "training_loss": 7.1652607917785645
    },
    {
      "epoch": 0.208130081300813,
      "grad_norm": 21.7110652923584,
      "learning_rate": 1e-05,
      "loss": 7.006,
      "step": 3840
    },
    {
      "epoch": 0.208130081300813,
      "step": 3840,
      "training_loss": 6.302520275115967
    },
    {
      "epoch": 0.20818428184281842,
      "step": 3841,
      "training_loss": 6.851632118225098
    },
    {
      "epoch": 0.20823848238482384,
      "step": 3842,
      "training_loss": 7.327374458312988
    },
    {
      "epoch": 0.20829268292682926,
      "step": 3843,
      "training_loss": 8.052748680114746
    },
    {
      "epoch": 0.20834688346883468,
      "grad_norm": 24.734600067138672,
      "learning_rate": 1e-05,
      "loss": 7.1336,
      "step": 3844
    },
    {
      "epoch": 0.20834688346883468,
      "step": 3844,
      "training_loss": 7.117315769195557
    },
    {
      "epoch": 0.20840108401084012,
      "step": 3845,
      "training_loss": 6.951404571533203
    },
    {
      "epoch": 0.20845528455284554,
      "step": 3846,
      "training_loss": 7.251163005828857
    },
    {
      "epoch": 0.20850948509485095,
      "step": 3847,
      "training_loss": 5.117646217346191
    },
    {
      "epoch": 0.20856368563685637,
      "grad_norm": 20.883214950561523,
      "learning_rate": 1e-05,
      "loss": 6.6094,
      "step": 3848
    },
    {
      "epoch": 0.20856368563685637,
      "step": 3848,
      "training_loss": 6.260022163391113
    },
    {
      "epoch": 0.2086178861788618,
      "step": 3849,
      "training_loss": 7.564265727996826
    },
    {
      "epoch": 0.2086720867208672,
      "step": 3850,
      "training_loss": 7.024861812591553
    },
    {
      "epoch": 0.20872628726287262,
      "step": 3851,
      "training_loss": 7.861347675323486
    },
    {
      "epoch": 0.20878048780487804,
      "grad_norm": 22.853374481201172,
      "learning_rate": 1e-05,
      "loss": 7.1776,
      "step": 3852
    },
    {
      "epoch": 0.20878048780487804,
      "step": 3852,
      "training_loss": 6.137435436248779
    },
    {
      "epoch": 0.20883468834688346,
      "step": 3853,
      "training_loss": 6.759093284606934
    },
    {
      "epoch": 0.2088888888888889,
      "step": 3854,
      "training_loss": 7.878698825836182
    },
    {
      "epoch": 0.20894308943089432,
      "step": 3855,
      "training_loss": 8.156208992004395
    },
    {
      "epoch": 0.20899728997289974,
      "grad_norm": 31.50153350830078,
      "learning_rate": 1e-05,
      "loss": 7.2329,
      "step": 3856
    },
    {
      "epoch": 0.20899728997289974,
      "step": 3856,
      "training_loss": 7.417598247528076
    },
    {
      "epoch": 0.20905149051490515,
      "step": 3857,
      "training_loss": 6.221846103668213
    },
    {
      "epoch": 0.20910569105691057,
      "step": 3858,
      "training_loss": 7.76515007019043
    },
    {
      "epoch": 0.209159891598916,
      "step": 3859,
      "training_loss": 6.516568660736084
    },
    {
      "epoch": 0.2092140921409214,
      "grad_norm": 17.897579193115234,
      "learning_rate": 1e-05,
      "loss": 6.9803,
      "step": 3860
    },
    {
      "epoch": 0.2092140921409214,
      "step": 3860,
      "training_loss": 6.842835426330566
    },
    {
      "epoch": 0.20926829268292682,
      "step": 3861,
      "training_loss": 4.546152591705322
    },
    {
      "epoch": 0.20932249322493224,
      "step": 3862,
      "training_loss": 5.94821834564209
    },
    {
      "epoch": 0.20937669376693768,
      "step": 3863,
      "training_loss": 7.85129451751709
    },
    {
      "epoch": 0.2094308943089431,
      "grad_norm": 29.55127716064453,
      "learning_rate": 1e-05,
      "loss": 6.2971,
      "step": 3864
    },
    {
      "epoch": 0.2094308943089431,
      "step": 3864,
      "training_loss": 7.8184494972229
    },
    {
      "epoch": 0.20948509485094852,
      "step": 3865,
      "training_loss": 9.428171157836914
    },
    {
      "epoch": 0.20953929539295393,
      "step": 3866,
      "training_loss": 7.256155967712402
    },
    {
      "epoch": 0.20959349593495935,
      "step": 3867,
      "training_loss": 7.46991491317749
    },
    {
      "epoch": 0.20964769647696477,
      "grad_norm": 21.921478271484375,
      "learning_rate": 1e-05,
      "loss": 7.9932,
      "step": 3868
    },
    {
      "epoch": 0.20964769647696477,
      "step": 3868,
      "training_loss": 7.719578266143799
    },
    {
      "epoch": 0.20970189701897018,
      "step": 3869,
      "training_loss": 7.902423858642578
    },
    {
      "epoch": 0.2097560975609756,
      "step": 3870,
      "training_loss": 6.133706092834473
    },
    {
      "epoch": 0.20981029810298102,
      "step": 3871,
      "training_loss": 5.818116188049316
    },
    {
      "epoch": 0.20986449864498646,
      "grad_norm": 28.026477813720703,
      "learning_rate": 1e-05,
      "loss": 6.8935,
      "step": 3872
    },
    {
      "epoch": 0.20986449864498646,
      "step": 3872,
      "training_loss": 8.170235633850098
    },
    {
      "epoch": 0.20991869918699188,
      "step": 3873,
      "training_loss": 7.002233028411865
    },
    {
      "epoch": 0.2099728997289973,
      "step": 3874,
      "training_loss": 3.6572487354278564
    },
    {
      "epoch": 0.21002710027100271,
      "step": 3875,
      "training_loss": 7.142763137817383
    },
    {
      "epoch": 0.21008130081300813,
      "grad_norm": 18.906471252441406,
      "learning_rate": 1e-05,
      "loss": 6.4931,
      "step": 3876
    },
    {
      "epoch": 0.21008130081300813,
      "step": 3876,
      "training_loss": 4.376753807067871
    },
    {
      "epoch": 0.21013550135501355,
      "step": 3877,
      "training_loss": 5.4424920082092285
    },
    {
      "epoch": 0.21018970189701897,
      "step": 3878,
      "training_loss": 7.0227556228637695
    },
    {
      "epoch": 0.21024390243902438,
      "step": 3879,
      "training_loss": 7.278994083404541
    },
    {
      "epoch": 0.2102981029810298,
      "grad_norm": 35.929412841796875,
      "learning_rate": 1e-05,
      "loss": 6.0302,
      "step": 3880
    },
    {
      "epoch": 0.2102981029810298,
      "step": 3880,
      "training_loss": 5.761898040771484
    },
    {
      "epoch": 0.21035230352303522,
      "step": 3881,
      "training_loss": 6.787208080291748
    },
    {
      "epoch": 0.21040650406504066,
      "step": 3882,
      "training_loss": 6.820392608642578
    },
    {
      "epoch": 0.21046070460704608,
      "step": 3883,
      "training_loss": 6.706552982330322
    },
    {
      "epoch": 0.2105149051490515,
      "grad_norm": 30.171180725097656,
      "learning_rate": 1e-05,
      "loss": 6.519,
      "step": 3884
    },
    {
      "epoch": 0.2105149051490515,
      "step": 3884,
      "training_loss": 6.850915431976318
    },
    {
      "epoch": 0.2105691056910569,
      "step": 3885,
      "training_loss": 7.010969638824463
    },
    {
      "epoch": 0.21062330623306233,
      "step": 3886,
      "training_loss": 6.954214572906494
    },
    {
      "epoch": 0.21067750677506775,
      "step": 3887,
      "training_loss": 7.230890274047852
    },
    {
      "epoch": 0.21073170731707316,
      "grad_norm": 16.000232696533203,
      "learning_rate": 1e-05,
      "loss": 7.0117,
      "step": 3888
    },
    {
      "epoch": 0.21073170731707316,
      "step": 3888,
      "training_loss": 7.519984245300293
    },
    {
      "epoch": 0.21078590785907858,
      "step": 3889,
      "training_loss": 6.69010591506958
    },
    {
      "epoch": 0.210840108401084,
      "step": 3890,
      "training_loss": 6.395223140716553
    },
    {
      "epoch": 0.21089430894308944,
      "step": 3891,
      "training_loss": 8.101165771484375
    },
    {
      "epoch": 0.21094850948509486,
      "grad_norm": 26.299665451049805,
      "learning_rate": 1e-05,
      "loss": 7.1766,
      "step": 3892
    },
    {
      "epoch": 0.21094850948509486,
      "step": 3892,
      "training_loss": 4.784836769104004
    },
    {
      "epoch": 0.21100271002710028,
      "step": 3893,
      "training_loss": 6.6865057945251465
    },
    {
      "epoch": 0.2110569105691057,
      "step": 3894,
      "training_loss": 9.5493803024292
    },
    {
      "epoch": 0.2111111111111111,
      "step": 3895,
      "training_loss": 6.849838733673096
    },
    {
      "epoch": 0.21116531165311653,
      "grad_norm": 32.127017974853516,
      "learning_rate": 1e-05,
      "loss": 6.9676,
      "step": 3896
    },
    {
      "epoch": 0.21116531165311653,
      "step": 3896,
      "training_loss": 6.4462103843688965
    },
    {
      "epoch": 0.21121951219512194,
      "step": 3897,
      "training_loss": 5.083954811096191
    },
    {
      "epoch": 0.21127371273712736,
      "step": 3898,
      "training_loss": 6.025144577026367
    },
    {
      "epoch": 0.21132791327913278,
      "step": 3899,
      "training_loss": 6.726621627807617
    },
    {
      "epoch": 0.21138211382113822,
      "grad_norm": 29.129610061645508,
      "learning_rate": 1e-05,
      "loss": 6.0705,
      "step": 3900
    },
    {
      "epoch": 0.21138211382113822,
      "step": 3900,
      "training_loss": 4.559609413146973
    },
    {
      "epoch": 0.21143631436314364,
      "step": 3901,
      "training_loss": 8.209848403930664
    },
    {
      "epoch": 0.21149051490514906,
      "step": 3902,
      "training_loss": 6.339117050170898
    },
    {
      "epoch": 0.21154471544715447,
      "step": 3903,
      "training_loss": 7.030979156494141
    },
    {
      "epoch": 0.2115989159891599,
      "grad_norm": 25.902301788330078,
      "learning_rate": 1e-05,
      "loss": 6.5349,
      "step": 3904
    },
    {
      "epoch": 0.2115989159891599,
      "step": 3904,
      "training_loss": 7.418928623199463
    },
    {
      "epoch": 0.2116531165311653,
      "step": 3905,
      "training_loss": 4.027886867523193
    },
    {
      "epoch": 0.21170731707317073,
      "step": 3906,
      "training_loss": 6.433897972106934
    },
    {
      "epoch": 0.21176151761517614,
      "step": 3907,
      "training_loss": 7.607709884643555
    },
    {
      "epoch": 0.21181571815718156,
      "grad_norm": 36.51329803466797,
      "learning_rate": 1e-05,
      "loss": 6.3721,
      "step": 3908
    },
    {
      "epoch": 0.21181571815718156,
      "step": 3908,
      "training_loss": 4.887247562408447
    },
    {
      "epoch": 0.211869918699187,
      "step": 3909,
      "training_loss": 6.063617706298828
    },
    {
      "epoch": 0.21192411924119242,
      "step": 3910,
      "training_loss": 5.577686786651611
    },
    {
      "epoch": 0.21197831978319784,
      "step": 3911,
      "training_loss": 6.3955793380737305
    },
    {
      "epoch": 0.21203252032520326,
      "grad_norm": 17.389474868774414,
      "learning_rate": 1e-05,
      "loss": 5.731,
      "step": 3912
    },
    {
      "epoch": 0.21203252032520326,
      "step": 3912,
      "training_loss": 7.088480472564697
    },
    {
      "epoch": 0.21208672086720867,
      "step": 3913,
      "training_loss": 6.919827461242676
    },
    {
      "epoch": 0.2121409214092141,
      "step": 3914,
      "training_loss": 5.264483451843262
    },
    {
      "epoch": 0.2121951219512195,
      "step": 3915,
      "training_loss": 6.596003532409668
    },
    {
      "epoch": 0.21224932249322492,
      "grad_norm": 29.7912540435791,
      "learning_rate": 1e-05,
      "loss": 6.4672,
      "step": 3916
    },
    {
      "epoch": 0.21224932249322492,
      "step": 3916,
      "training_loss": 6.253208160400391
    },
    {
      "epoch": 0.21230352303523034,
      "step": 3917,
      "training_loss": 7.958639621734619
    },
    {
      "epoch": 0.21235772357723579,
      "step": 3918,
      "training_loss": 6.201200008392334
    },
    {
      "epoch": 0.2124119241192412,
      "step": 3919,
      "training_loss": 7.362308025360107
    },
    {
      "epoch": 0.21246612466124662,
      "grad_norm": 17.51653289794922,
      "learning_rate": 1e-05,
      "loss": 6.9438,
      "step": 3920
    },
    {
      "epoch": 0.21246612466124662,
      "step": 3920,
      "training_loss": 7.547051906585693
    },
    {
      "epoch": 0.21252032520325204,
      "step": 3921,
      "training_loss": 6.605013370513916
    },
    {
      "epoch": 0.21257452574525745,
      "step": 3922,
      "training_loss": 7.500350475311279
    },
    {
      "epoch": 0.21262872628726287,
      "step": 3923,
      "training_loss": 6.36594295501709
    },
    {
      "epoch": 0.2126829268292683,
      "grad_norm": 22.6464786529541,
      "learning_rate": 1e-05,
      "loss": 7.0046,
      "step": 3924
    },
    {
      "epoch": 0.2126829268292683,
      "step": 3924,
      "training_loss": 4.848431587219238
    },
    {
      "epoch": 0.2127371273712737,
      "step": 3925,
      "training_loss": 7.187383651733398
    },
    {
      "epoch": 0.21279132791327912,
      "step": 3926,
      "training_loss": 7.2261643409729
    },
    {
      "epoch": 0.21284552845528457,
      "step": 3927,
      "training_loss": 6.788715839385986
    },
    {
      "epoch": 0.21289972899728998,
      "grad_norm": 24.177133560180664,
      "learning_rate": 1e-05,
      "loss": 6.5127,
      "step": 3928
    },
    {
      "epoch": 0.21289972899728998,
      "step": 3928,
      "training_loss": 5.853002548217773
    },
    {
      "epoch": 0.2129539295392954,
      "step": 3929,
      "training_loss": 7.211835861206055
    },
    {
      "epoch": 0.21300813008130082,
      "step": 3930,
      "training_loss": 6.760707855224609
    },
    {
      "epoch": 0.21306233062330623,
      "step": 3931,
      "training_loss": 6.116564750671387
    },
    {
      "epoch": 0.21311653116531165,
      "grad_norm": 31.79603385925293,
      "learning_rate": 1e-05,
      "loss": 6.4855,
      "step": 3932
    },
    {
      "epoch": 0.21311653116531165,
      "step": 3932,
      "training_loss": 5.856260299682617
    },
    {
      "epoch": 0.21317073170731707,
      "step": 3933,
      "training_loss": 6.58408260345459
    },
    {
      "epoch": 0.21322493224932249,
      "step": 3934,
      "training_loss": 6.241202354431152
    },
    {
      "epoch": 0.2132791327913279,
      "step": 3935,
      "training_loss": 7.325661659240723
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 28.12739372253418,
      "learning_rate": 1e-05,
      "loss": 6.5018,
      "step": 3936
    },
    {
      "epoch": 0.21333333333333335,
      "step": 3936,
      "training_loss": 8.290103912353516
    },
    {
      "epoch": 0.21338753387533876,
      "step": 3937,
      "training_loss": 5.431718826293945
    },
    {
      "epoch": 0.21344173441734418,
      "step": 3938,
      "training_loss": 5.692765712738037
    },
    {
      "epoch": 0.2134959349593496,
      "step": 3939,
      "training_loss": 7.300207614898682
    },
    {
      "epoch": 0.21355013550135502,
      "grad_norm": 20.010469436645508,
      "learning_rate": 1e-05,
      "loss": 6.6787,
      "step": 3940
    },
    {
      "epoch": 0.21355013550135502,
      "step": 3940,
      "training_loss": 7.697545528411865
    },
    {
      "epoch": 0.21360433604336043,
      "step": 3941,
      "training_loss": 8.208379745483398
    },
    {
      "epoch": 0.21365853658536585,
      "step": 3942,
      "training_loss": 6.883516788482666
    },
    {
      "epoch": 0.21371273712737127,
      "step": 3943,
      "training_loss": 7.677276611328125
    },
    {
      "epoch": 0.21376693766937668,
      "grad_norm": 27.64386558532715,
      "learning_rate": 1e-05,
      "loss": 7.6167,
      "step": 3944
    },
    {
      "epoch": 0.21376693766937668,
      "step": 3944,
      "training_loss": 4.435361862182617
    },
    {
      "epoch": 0.2138211382113821,
      "step": 3945,
      "training_loss": 6.47470760345459
    },
    {
      "epoch": 0.21387533875338754,
      "step": 3946,
      "training_loss": 8.673595428466797
    },
    {
      "epoch": 0.21392953929539296,
      "step": 3947,
      "training_loss": 8.247069358825684
    },
    {
      "epoch": 0.21398373983739838,
      "grad_norm": 31.07786750793457,
      "learning_rate": 1e-05,
      "loss": 6.9577,
      "step": 3948
    },
    {
      "epoch": 0.21398373983739838,
      "step": 3948,
      "training_loss": 5.961287498474121
    },
    {
      "epoch": 0.2140379403794038,
      "step": 3949,
      "training_loss": 6.879069805145264
    },
    {
      "epoch": 0.2140921409214092,
      "step": 3950,
      "training_loss": 7.060001850128174
    },
    {
      "epoch": 0.21414634146341463,
      "step": 3951,
      "training_loss": 7.368741035461426
    },
    {
      "epoch": 0.21420054200542005,
      "grad_norm": 20.216026306152344,
      "learning_rate": 1e-05,
      "loss": 6.8173,
      "step": 3952
    },
    {
      "epoch": 0.21420054200542005,
      "step": 3952,
      "training_loss": 7.894190311431885
    },
    {
      "epoch": 0.21425474254742546,
      "step": 3953,
      "training_loss": 7.171477317810059
    },
    {
      "epoch": 0.21430894308943088,
      "step": 3954,
      "training_loss": 7.401179790496826
    },
    {
      "epoch": 0.21436314363143633,
      "step": 3955,
      "training_loss": 6.653152942657471
    },
    {
      "epoch": 0.21441734417344174,
      "grad_norm": 24.149723052978516,
      "learning_rate": 1e-05,
      "loss": 7.28,
      "step": 3956
    },
    {
      "epoch": 0.21441734417344174,
      "step": 3956,
      "training_loss": 6.920646667480469
    },
    {
      "epoch": 0.21447154471544716,
      "step": 3957,
      "training_loss": 7.3410725593566895
    },
    {
      "epoch": 0.21452574525745258,
      "step": 3958,
      "training_loss": 7.406496047973633
    },
    {
      "epoch": 0.214579945799458,
      "step": 3959,
      "training_loss": 6.755458354949951
    },
    {
      "epoch": 0.2146341463414634,
      "grad_norm": 16.520530700683594,
      "learning_rate": 1e-05,
      "loss": 7.1059,
      "step": 3960
    },
    {
      "epoch": 0.2146341463414634,
      "step": 3960,
      "training_loss": 7.261811256408691
    },
    {
      "epoch": 0.21468834688346883,
      "step": 3961,
      "training_loss": 6.761796474456787
    },
    {
      "epoch": 0.21474254742547425,
      "step": 3962,
      "training_loss": 7.253380298614502
    },
    {
      "epoch": 0.21479674796747966,
      "step": 3963,
      "training_loss": 6.8646087646484375
    },
    {
      "epoch": 0.2148509485094851,
      "grad_norm": 24.80393409729004,
      "learning_rate": 1e-05,
      "loss": 7.0354,
      "step": 3964
    },
    {
      "epoch": 0.2148509485094851,
      "step": 3964,
      "training_loss": 6.807260513305664
    },
    {
      "epoch": 0.21490514905149052,
      "step": 3965,
      "training_loss": 7.798518180847168
    },
    {
      "epoch": 0.21495934959349594,
      "step": 3966,
      "training_loss": 5.643454074859619
    },
    {
      "epoch": 0.21501355013550136,
      "step": 3967,
      "training_loss": 6.414785861968994
    },
    {
      "epoch": 0.21506775067750677,
      "grad_norm": 37.713035583496094,
      "learning_rate": 1e-05,
      "loss": 6.666,
      "step": 3968
    },
    {
      "epoch": 0.21506775067750677,
      "step": 3968,
      "training_loss": 4.464761257171631
    },
    {
      "epoch": 0.2151219512195122,
      "step": 3969,
      "training_loss": 6.2351884841918945
    },
    {
      "epoch": 0.2151761517615176,
      "step": 3970,
      "training_loss": 6.8583083152771
    },
    {
      "epoch": 0.21523035230352303,
      "step": 3971,
      "training_loss": 8.083370208740234
    },
    {
      "epoch": 0.21528455284552844,
      "grad_norm": 43.66646194458008,
      "learning_rate": 1e-05,
      "loss": 6.4104,
      "step": 3972
    },
    {
      "epoch": 0.21528455284552844,
      "step": 3972,
      "training_loss": 7.83139181137085
    },
    {
      "epoch": 0.2153387533875339,
      "step": 3973,
      "training_loss": 5.919060707092285
    },
    {
      "epoch": 0.2153929539295393,
      "step": 3974,
      "training_loss": 7.18191385269165
    },
    {
      "epoch": 0.21544715447154472,
      "step": 3975,
      "training_loss": 8.761238098144531
    },
    {
      "epoch": 0.21550135501355014,
      "grad_norm": 24.323101043701172,
      "learning_rate": 1e-05,
      "loss": 7.4234,
      "step": 3976
    },
    {
      "epoch": 0.21550135501355014,
      "step": 3976,
      "training_loss": 6.746639251708984
    },
    {
      "epoch": 0.21555555555555556,
      "step": 3977,
      "training_loss": 5.926162242889404
    },
    {
      "epoch": 0.21560975609756097,
      "step": 3978,
      "training_loss": 7.56758451461792
    },
    {
      "epoch": 0.2156639566395664,
      "step": 3979,
      "training_loss": 7.3034467697143555
    },
    {
      "epoch": 0.2157181571815718,
      "grad_norm": 27.82272720336914,
      "learning_rate": 1e-05,
      "loss": 6.886,
      "step": 3980
    },
    {
      "epoch": 0.2157181571815718,
      "step": 3980,
      "training_loss": 5.06129264831543
    },
    {
      "epoch": 0.21577235772357722,
      "step": 3981,
      "training_loss": 7.32918119430542
    },
    {
      "epoch": 0.21582655826558267,
      "step": 3982,
      "training_loss": 7.543288230895996
    },
    {
      "epoch": 0.21588075880758809,
      "step": 3983,
      "training_loss": 6.033237457275391
    },
    {
      "epoch": 0.2159349593495935,
      "grad_norm": 39.033084869384766,
      "learning_rate": 1e-05,
      "loss": 6.4917,
      "step": 3984
    },
    {
      "epoch": 0.2159349593495935,
      "step": 3984,
      "training_loss": 7.87290620803833
    },
    {
      "epoch": 0.21598915989159892,
      "step": 3985,
      "training_loss": 7.139349460601807
    },
    {
      "epoch": 0.21604336043360434,
      "step": 3986,
      "training_loss": 7.95499324798584
    },
    {
      "epoch": 0.21609756097560975,
      "step": 3987,
      "training_loss": 7.343323230743408
    },
    {
      "epoch": 0.21615176151761517,
      "grad_norm": 18.44797706604004,
      "learning_rate": 1e-05,
      "loss": 7.5776,
      "step": 3988
    },
    {
      "epoch": 0.21615176151761517,
      "step": 3988,
      "training_loss": 7.071167469024658
    },
    {
      "epoch": 0.2162059620596206,
      "step": 3989,
      "training_loss": 6.719727993011475
    },
    {
      "epoch": 0.216260162601626,
      "step": 3990,
      "training_loss": 7.5694708824157715
    },
    {
      "epoch": 0.21631436314363145,
      "step": 3991,
      "training_loss": 7.385441780090332
    },
    {
      "epoch": 0.21636856368563687,
      "grad_norm": 24.510669708251953,
      "learning_rate": 1e-05,
      "loss": 7.1865,
      "step": 3992
    },
    {
      "epoch": 0.21636856368563687,
      "step": 3992,
      "training_loss": 6.682980537414551
    },
    {
      "epoch": 0.21642276422764228,
      "step": 3993,
      "training_loss": 6.567015171051025
    },
    {
      "epoch": 0.2164769647696477,
      "step": 3994,
      "training_loss": 8.345881462097168
    },
    {
      "epoch": 0.21653116531165312,
      "step": 3995,
      "training_loss": 8.279413223266602
    },
    {
      "epoch": 0.21658536585365853,
      "grad_norm": 22.561193466186523,
      "learning_rate": 1e-05,
      "loss": 7.4688,
      "step": 3996
    },
    {
      "epoch": 0.21658536585365853,
      "step": 3996,
      "training_loss": 7.466126441955566
    },
    {
      "epoch": 0.21663956639566395,
      "step": 3997,
      "training_loss": 6.230143070220947
    },
    {
      "epoch": 0.21669376693766937,
      "step": 3998,
      "training_loss": 6.3392333984375
    },
    {
      "epoch": 0.21674796747967479,
      "step": 3999,
      "training_loss": 6.9574785232543945
    },
    {
      "epoch": 0.21680216802168023,
      "grad_norm": 33.403350830078125,
      "learning_rate": 1e-05,
      "loss": 6.7482,
      "step": 4000
    },
    {
      "epoch": 0.21680216802168023,
      "step": 4000,
      "training_loss": 7.1234331130981445
    },
    {
      "epoch": 0.21685636856368565,
      "step": 4001,
      "training_loss": 6.23089599609375
    },
    {
      "epoch": 0.21691056910569106,
      "step": 4002,
      "training_loss": 6.679632186889648
    },
    {
      "epoch": 0.21696476964769648,
      "step": 4003,
      "training_loss": 8.928170204162598
    },
    {
      "epoch": 0.2170189701897019,
      "grad_norm": 35.96221160888672,
      "learning_rate": 1e-05,
      "loss": 7.2405,
      "step": 4004
    },
    {
      "epoch": 0.2170189701897019,
      "step": 4004,
      "training_loss": 7.260451793670654
    },
    {
      "epoch": 0.21707317073170732,
      "step": 4005,
      "training_loss": 10.046250343322754
    },
    {
      "epoch": 0.21712737127371273,
      "step": 4006,
      "training_loss": 7.7572126388549805
    },
    {
      "epoch": 0.21718157181571815,
      "step": 4007,
      "training_loss": 7.303957462310791
    },
    {
      "epoch": 0.21723577235772357,
      "grad_norm": 35.875999450683594,
      "learning_rate": 1e-05,
      "loss": 8.092,
      "step": 4008
    },
    {
      "epoch": 0.21723577235772357,
      "step": 4008,
      "training_loss": 6.981508731842041
    },
    {
      "epoch": 0.21728997289972898,
      "step": 4009,
      "training_loss": 6.686259746551514
    },
    {
      "epoch": 0.21734417344173443,
      "step": 4010,
      "training_loss": 7.673064708709717
    },
    {
      "epoch": 0.21739837398373985,
      "step": 4011,
      "training_loss": 6.770108699798584
    },
    {
      "epoch": 0.21745257452574526,
      "grad_norm": 14.917753219604492,
      "learning_rate": 1e-05,
      "loss": 7.0277,
      "step": 4012
    },
    {
      "epoch": 0.21745257452574526,
      "step": 4012,
      "training_loss": 7.362985610961914
    },
    {
      "epoch": 0.21750677506775068,
      "step": 4013,
      "training_loss": 7.669528007507324
    },
    {
      "epoch": 0.2175609756097561,
      "step": 4014,
      "training_loss": 5.989074230194092
    },
    {
      "epoch": 0.2176151761517615,
      "step": 4015,
      "training_loss": 7.576344013214111
    },
    {
      "epoch": 0.21766937669376693,
      "grad_norm": 36.9124641418457,
      "learning_rate": 1e-05,
      "loss": 7.1495,
      "step": 4016
    },
    {
      "epoch": 0.21766937669376693,
      "step": 4016,
      "training_loss": 7.360233306884766
    },
    {
      "epoch": 0.21772357723577235,
      "step": 4017,
      "training_loss": 8.645576477050781
    },
    {
      "epoch": 0.21777777777777776,
      "step": 4018,
      "training_loss": 7.079346179962158
    },
    {
      "epoch": 0.2178319783197832,
      "step": 4019,
      "training_loss": 6.884378910064697
    },
    {
      "epoch": 0.21788617886178863,
      "grad_norm": 23.944272994995117,
      "learning_rate": 1e-05,
      "loss": 7.4924,
      "step": 4020
    },
    {
      "epoch": 0.21788617886178863,
      "step": 4020,
      "training_loss": 6.64524507522583
    },
    {
      "epoch": 0.21794037940379404,
      "step": 4021,
      "training_loss": 6.1658735275268555
    },
    {
      "epoch": 0.21799457994579946,
      "step": 4022,
      "training_loss": 5.417333126068115
    },
    {
      "epoch": 0.21804878048780488,
      "step": 4023,
      "training_loss": 5.909547328948975
    },
    {
      "epoch": 0.2181029810298103,
      "grad_norm": 24.732955932617188,
      "learning_rate": 1e-05,
      "loss": 6.0345,
      "step": 4024
    },
    {
      "epoch": 0.2181029810298103,
      "step": 4024,
      "training_loss": 6.178500652313232
    },
    {
      "epoch": 0.2181571815718157,
      "step": 4025,
      "training_loss": 7.989866733551025
    },
    {
      "epoch": 0.21821138211382113,
      "step": 4026,
      "training_loss": 4.387531757354736
    },
    {
      "epoch": 0.21826558265582655,
      "step": 4027,
      "training_loss": 7.167354583740234
    },
    {
      "epoch": 0.218319783197832,
      "grad_norm": 21.86565589904785,
      "learning_rate": 1e-05,
      "loss": 6.4308,
      "step": 4028
    },
    {
      "epoch": 0.218319783197832,
      "step": 4028,
      "training_loss": 6.924036979675293
    },
    {
      "epoch": 0.2183739837398374,
      "step": 4029,
      "training_loss": 7.1644134521484375
    },
    {
      "epoch": 0.21842818428184282,
      "step": 4030,
      "training_loss": 6.938899040222168
    },
    {
      "epoch": 0.21848238482384824,
      "step": 4031,
      "training_loss": 6.343527793884277
    },
    {
      "epoch": 0.21853658536585366,
      "grad_norm": 37.84022521972656,
      "learning_rate": 1e-05,
      "loss": 6.8427,
      "step": 4032
    },
    {
      "epoch": 0.21853658536585366,
      "step": 4032,
      "training_loss": 4.816824913024902
    },
    {
      "epoch": 0.21859078590785908,
      "step": 4033,
      "training_loss": 7.394515037536621
    },
    {
      "epoch": 0.2186449864498645,
      "step": 4034,
      "training_loss": 7.1368088722229
    },
    {
      "epoch": 0.2186991869918699,
      "step": 4035,
      "training_loss": 7.048422813415527
    },
    {
      "epoch": 0.21875338753387533,
      "grad_norm": 17.616886138916016,
      "learning_rate": 1e-05,
      "loss": 6.5991,
      "step": 4036
    },
    {
      "epoch": 0.21875338753387533,
      "step": 4036,
      "training_loss": 6.9381256103515625
    },
    {
      "epoch": 0.21880758807588077,
      "step": 4037,
      "training_loss": 7.411272048950195
    },
    {
      "epoch": 0.2188617886178862,
      "step": 4038,
      "training_loss": 7.592193126678467
    },
    {
      "epoch": 0.2189159891598916,
      "step": 4039,
      "training_loss": 6.4018235206604
    },
    {
      "epoch": 0.21897018970189702,
      "grad_norm": 35.38779067993164,
      "learning_rate": 1e-05,
      "loss": 7.0859,
      "step": 4040
    },
    {
      "epoch": 0.21897018970189702,
      "step": 4040,
      "training_loss": 6.574548244476318
    },
    {
      "epoch": 0.21902439024390244,
      "step": 4041,
      "training_loss": 4.18763542175293
    },
    {
      "epoch": 0.21907859078590786,
      "step": 4042,
      "training_loss": 7.29395055770874
    },
    {
      "epoch": 0.21913279132791327,
      "step": 4043,
      "training_loss": 7.049490928649902
    },
    {
      "epoch": 0.2191869918699187,
      "grad_norm": 17.02776336669922,
      "learning_rate": 1e-05,
      "loss": 6.2764,
      "step": 4044
    },
    {
      "epoch": 0.2191869918699187,
      "step": 4044,
      "training_loss": 6.527430057525635
    },
    {
      "epoch": 0.2192411924119241,
      "step": 4045,
      "training_loss": 7.195842742919922
    },
    {
      "epoch": 0.21929539295392955,
      "step": 4046,
      "training_loss": 6.855818748474121
    },
    {
      "epoch": 0.21934959349593497,
      "step": 4047,
      "training_loss": 7.281011581420898
    },
    {
      "epoch": 0.2194037940379404,
      "grad_norm": 49.759891510009766,
      "learning_rate": 1e-05,
      "loss": 6.965,
      "step": 4048
    },
    {
      "epoch": 0.2194037940379404,
      "step": 4048,
      "training_loss": 5.390628814697266
    },
    {
      "epoch": 0.2194579945799458,
      "step": 4049,
      "training_loss": 6.446857452392578
    },
    {
      "epoch": 0.21951219512195122,
      "step": 4050,
      "training_loss": 6.016637802124023
    },
    {
      "epoch": 0.21956639566395664,
      "step": 4051,
      "training_loss": 7.54254150390625
    },
    {
      "epoch": 0.21962059620596205,
      "grad_norm": 32.00193405151367,
      "learning_rate": 1e-05,
      "loss": 6.3492,
      "step": 4052
    },
    {
      "epoch": 0.21962059620596205,
      "step": 4052,
      "training_loss": 7.071527004241943
    },
    {
      "epoch": 0.21967479674796747,
      "step": 4053,
      "training_loss": 6.744954586029053
    },
    {
      "epoch": 0.2197289972899729,
      "step": 4054,
      "training_loss": 5.854695796966553
    },
    {
      "epoch": 0.21978319783197833,
      "step": 4055,
      "training_loss": 6.7136335372924805
    },
    {
      "epoch": 0.21983739837398375,
      "grad_norm": 20.965545654296875,
      "learning_rate": 1e-05,
      "loss": 6.5962,
      "step": 4056
    },
    {
      "epoch": 0.21983739837398375,
      "step": 4056,
      "training_loss": 5.615957736968994
    },
    {
      "epoch": 0.21989159891598917,
      "step": 4057,
      "training_loss": 6.525793552398682
    },
    {
      "epoch": 0.21994579945799458,
      "step": 4058,
      "training_loss": 6.771556377410889
    },
    {
      "epoch": 0.22,
      "step": 4059,
      "training_loss": 5.97222900390625
    },
    {
      "epoch": 0.22005420054200542,
      "grad_norm": 37.72552490234375,
      "learning_rate": 1e-05,
      "loss": 6.2214,
      "step": 4060
    },
    {
      "epoch": 0.22005420054200542,
      "step": 4060,
      "training_loss": 6.7936201095581055
    },
    {
      "epoch": 0.22010840108401084,
      "step": 4061,
      "training_loss": 6.342365741729736
    },
    {
      "epoch": 0.22016260162601625,
      "step": 4062,
      "training_loss": 6.820255756378174
    },
    {
      "epoch": 0.22021680216802167,
      "step": 4063,
      "training_loss": 5.9772467613220215
    },
    {
      "epoch": 0.22027100271002711,
      "grad_norm": 27.470779418945312,
      "learning_rate": 1e-05,
      "loss": 6.4834,
      "step": 4064
    },
    {
      "epoch": 0.22027100271002711,
      "step": 4064,
      "training_loss": 7.207183837890625
    },
    {
      "epoch": 0.22032520325203253,
      "step": 4065,
      "training_loss": 6.88875150680542
    },
    {
      "epoch": 0.22037940379403795,
      "step": 4066,
      "training_loss": 6.725322723388672
    },
    {
      "epoch": 0.22043360433604337,
      "step": 4067,
      "training_loss": 5.365711688995361
    },
    {
      "epoch": 0.22048780487804878,
      "grad_norm": 34.53125,
      "learning_rate": 1e-05,
      "loss": 6.5467,
      "step": 4068
    },
    {
      "epoch": 0.22048780487804878,
      "step": 4068,
      "training_loss": 7.22627592086792
    },
    {
      "epoch": 0.2205420054200542,
      "step": 4069,
      "training_loss": 7.478855609893799
    },
    {
      "epoch": 0.22059620596205962,
      "step": 4070,
      "training_loss": 5.36694860458374
    },
    {
      "epoch": 0.22065040650406503,
      "step": 4071,
      "training_loss": 6.9834699630737305
    },
    {
      "epoch": 0.22070460704607045,
      "grad_norm": 20.059154510498047,
      "learning_rate": 1e-05,
      "loss": 6.7639,
      "step": 4072
    },
    {
      "epoch": 0.22070460704607045,
      "step": 4072,
      "training_loss": 8.588367462158203
    },
    {
      "epoch": 0.22075880758807587,
      "step": 4073,
      "training_loss": 6.415325164794922
    },
    {
      "epoch": 0.2208130081300813,
      "step": 4074,
      "training_loss": 6.20147705078125
    },
    {
      "epoch": 0.22086720867208673,
      "step": 4075,
      "training_loss": 6.616713523864746
    },
    {
      "epoch": 0.22092140921409215,
      "grad_norm": 23.016292572021484,
      "learning_rate": 1e-05,
      "loss": 6.9555,
      "step": 4076
    },
    {
      "epoch": 0.22092140921409215,
      "step": 4076,
      "training_loss": 5.838983058929443
    },
    {
      "epoch": 0.22097560975609756,
      "step": 4077,
      "training_loss": 7.553499698638916
    },
    {
      "epoch": 0.22102981029810298,
      "step": 4078,
      "training_loss": 5.4524431228637695
    },
    {
      "epoch": 0.2210840108401084,
      "step": 4079,
      "training_loss": 6.952179908752441
    },
    {
      "epoch": 0.22113821138211381,
      "grad_norm": 21.014917373657227,
      "learning_rate": 1e-05,
      "loss": 6.4493,
      "step": 4080
    },
    {
      "epoch": 0.22113821138211381,
      "step": 4080,
      "training_loss": 6.931869029998779
    },
    {
      "epoch": 0.22119241192411923,
      "step": 4081,
      "training_loss": 7.305164337158203
    },
    {
      "epoch": 0.22124661246612465,
      "step": 4082,
      "training_loss": 8.35661506652832
    },
    {
      "epoch": 0.2213008130081301,
      "step": 4083,
      "training_loss": 7.171274185180664
    },
    {
      "epoch": 0.2213550135501355,
      "grad_norm": 21.008018493652344,
      "learning_rate": 1e-05,
      "loss": 7.4412,
      "step": 4084
    },
    {
      "epoch": 0.2213550135501355,
      "step": 4084,
      "training_loss": 8.232091903686523
    },
    {
      "epoch": 0.22140921409214093,
      "step": 4085,
      "training_loss": 4.439918518066406
    },
    {
      "epoch": 0.22146341463414634,
      "step": 4086,
      "training_loss": 7.365460395812988
    },
    {
      "epoch": 0.22151761517615176,
      "step": 4087,
      "training_loss": 7.116079330444336
    },
    {
      "epoch": 0.22157181571815718,
      "grad_norm": 21.859619140625,
      "learning_rate": 1e-05,
      "loss": 6.7884,
      "step": 4088
    },
    {
      "epoch": 0.22157181571815718,
      "step": 4088,
      "training_loss": 7.2144904136657715
    },
    {
      "epoch": 0.2216260162601626,
      "step": 4089,
      "training_loss": 7.732489585876465
    },
    {
      "epoch": 0.221680216802168,
      "step": 4090,
      "training_loss": 7.292977809906006
    },
    {
      "epoch": 0.22173441734417343,
      "step": 4091,
      "training_loss": 6.6245903968811035
    },
    {
      "epoch": 0.22178861788617887,
      "grad_norm": 20.061840057373047,
      "learning_rate": 1e-05,
      "loss": 7.2161,
      "step": 4092
    },
    {
      "epoch": 0.22178861788617887,
      "step": 4092,
      "training_loss": 6.798467636108398
    },
    {
      "epoch": 0.2218428184281843,
      "step": 4093,
      "training_loss": 6.682686805725098
    },
    {
      "epoch": 0.2218970189701897,
      "step": 4094,
      "training_loss": 6.902859210968018
    },
    {
      "epoch": 0.22195121951219512,
      "step": 4095,
      "training_loss": 6.949934005737305
    },
    {
      "epoch": 0.22200542005420054,
      "grad_norm": 20.005212783813477,
      "learning_rate": 1e-05,
      "loss": 6.8335,
      "step": 4096
    },
    {
      "epoch": 0.22200542005420054,
      "step": 4096,
      "training_loss": 4.53184175491333
    },
    {
      "epoch": 0.22205962059620596,
      "step": 4097,
      "training_loss": 6.054826259613037
    },
    {
      "epoch": 0.22211382113821138,
      "step": 4098,
      "training_loss": 6.120676517486572
    },
    {
      "epoch": 0.2221680216802168,
      "step": 4099,
      "training_loss": 6.467099666595459
    },
    {
      "epoch": 0.2222222222222222,
      "grad_norm": 22.558429718017578,
      "learning_rate": 1e-05,
      "loss": 5.7936,
      "step": 4100
    },
    {
      "epoch": 0.2222222222222222,
      "step": 4100,
      "training_loss": 6.183416843414307
    },
    {
      "epoch": 0.22227642276422765,
      "step": 4101,
      "training_loss": 6.821567535400391
    },
    {
      "epoch": 0.22233062330623307,
      "step": 4102,
      "training_loss": 6.113670349121094
    },
    {
      "epoch": 0.2223848238482385,
      "step": 4103,
      "training_loss": 6.852871894836426
    },
    {
      "epoch": 0.2224390243902439,
      "grad_norm": 18.4316349029541,
      "learning_rate": 1e-05,
      "loss": 6.4929,
      "step": 4104
    },
    {
      "epoch": 0.2224390243902439,
      "step": 4104,
      "training_loss": 8.151611328125
    },
    {
      "epoch": 0.22249322493224932,
      "step": 4105,
      "training_loss": 6.8867411613464355
    },
    {
      "epoch": 0.22254742547425474,
      "step": 4106,
      "training_loss": 7.003637313842773
    },
    {
      "epoch": 0.22260162601626016,
      "step": 4107,
      "training_loss": 7.180736064910889
    },
    {
      "epoch": 0.22265582655826557,
      "grad_norm": 24.627662658691406,
      "learning_rate": 1e-05,
      "loss": 7.3057,
      "step": 4108
    },
    {
      "epoch": 0.22265582655826557,
      "step": 4108,
      "training_loss": 6.648873329162598
    },
    {
      "epoch": 0.222710027100271,
      "step": 4109,
      "training_loss": 8.099369049072266
    },
    {
      "epoch": 0.22276422764227644,
      "step": 4110,
      "training_loss": 6.947166442871094
    },
    {
      "epoch": 0.22281842818428185,
      "step": 4111,
      "training_loss": 8.297039985656738
    },
    {
      "epoch": 0.22287262872628727,
      "grad_norm": 29.77553367614746,
      "learning_rate": 1e-05,
      "loss": 7.4981,
      "step": 4112
    },
    {
      "epoch": 0.22287262872628727,
      "step": 4112,
      "training_loss": 7.80922794342041
    },
    {
      "epoch": 0.2229268292682927,
      "step": 4113,
      "training_loss": 5.678173065185547
    },
    {
      "epoch": 0.2229810298102981,
      "step": 4114,
      "training_loss": 7.511624336242676
    },
    {
      "epoch": 0.22303523035230352,
      "step": 4115,
      "training_loss": 7.506175994873047
    },
    {
      "epoch": 0.22308943089430894,
      "grad_norm": 20.34319496154785,
      "learning_rate": 1e-05,
      "loss": 7.1263,
      "step": 4116
    },
    {
      "epoch": 0.22308943089430894,
      "step": 4116,
      "training_loss": 6.923946380615234
    },
    {
      "epoch": 0.22314363143631435,
      "step": 4117,
      "training_loss": 7.284884929656982
    },
    {
      "epoch": 0.22319783197831977,
      "step": 4118,
      "training_loss": 6.614790916442871
    },
    {
      "epoch": 0.22325203252032522,
      "step": 4119,
      "training_loss": 6.570936679840088
    },
    {
      "epoch": 0.22330623306233063,
      "grad_norm": 34.592376708984375,
      "learning_rate": 1e-05,
      "loss": 6.8486,
      "step": 4120
    },
    {
      "epoch": 0.22330623306233063,
      "step": 4120,
      "training_loss": 6.971793174743652
    },
    {
      "epoch": 0.22336043360433605,
      "step": 4121,
      "training_loss": 5.628452777862549
    },
    {
      "epoch": 0.22341463414634147,
      "step": 4122,
      "training_loss": 7.615406036376953
    },
    {
      "epoch": 0.22346883468834688,
      "step": 4123,
      "training_loss": 6.166704177856445
    },
    {
      "epoch": 0.2235230352303523,
      "grad_norm": 26.216625213623047,
      "learning_rate": 1e-05,
      "loss": 6.5956,
      "step": 4124
    },
    {
      "epoch": 0.2235230352303523,
      "step": 4124,
      "training_loss": 7.321887493133545
    },
    {
      "epoch": 0.22357723577235772,
      "step": 4125,
      "training_loss": 7.339171409606934
    },
    {
      "epoch": 0.22363143631436314,
      "step": 4126,
      "training_loss": 5.212711811065674
    },
    {
      "epoch": 0.22368563685636855,
      "step": 4127,
      "training_loss": 7.325892448425293
    },
    {
      "epoch": 0.223739837398374,
      "grad_norm": 18.301706314086914,
      "learning_rate": 1e-05,
      "loss": 6.7999,
      "step": 4128
    },
    {
      "epoch": 0.223739837398374,
      "step": 4128,
      "training_loss": 7.753897190093994
    },
    {
      "epoch": 0.22379403794037941,
      "step": 4129,
      "training_loss": 8.935059547424316
    },
    {
      "epoch": 0.22384823848238483,
      "step": 4130,
      "training_loss": 7.328277111053467
    },
    {
      "epoch": 0.22390243902439025,
      "step": 4131,
      "training_loss": 6.892980098724365
    },
    {
      "epoch": 0.22395663956639567,
      "grad_norm": 59.344520568847656,
      "learning_rate": 1e-05,
      "loss": 7.7276,
      "step": 4132
    },
    {
      "epoch": 0.22395663956639567,
      "step": 4132,
      "training_loss": 6.024771690368652
    },
    {
      "epoch": 0.22401084010840108,
      "step": 4133,
      "training_loss": 7.470433712005615
    },
    {
      "epoch": 0.2240650406504065,
      "step": 4134,
      "training_loss": 8.064765930175781
    },
    {
      "epoch": 0.22411924119241192,
      "step": 4135,
      "training_loss": 8.091643333435059
    },
    {
      "epoch": 0.22417344173441733,
      "grad_norm": 52.90707015991211,
      "learning_rate": 1e-05,
      "loss": 7.4129,
      "step": 4136
    },
    {
      "epoch": 0.22417344173441733,
      "step": 4136,
      "training_loss": 6.799311637878418
    },
    {
      "epoch": 0.22422764227642275,
      "step": 4137,
      "training_loss": 7.275129318237305
    },
    {
      "epoch": 0.2242818428184282,
      "step": 4138,
      "training_loss": 8.116592407226562
    },
    {
      "epoch": 0.2243360433604336,
      "step": 4139,
      "training_loss": 6.20548152923584
    },
    {
      "epoch": 0.22439024390243903,
      "grad_norm": 24.07093048095703,
      "learning_rate": 1e-05,
      "loss": 7.0991,
      "step": 4140
    },
    {
      "epoch": 0.22439024390243903,
      "step": 4140,
      "training_loss": 7.733544826507568
    },
    {
      "epoch": 0.22444444444444445,
      "step": 4141,
      "training_loss": 6.619667053222656
    },
    {
      "epoch": 0.22449864498644986,
      "step": 4142,
      "training_loss": 6.054275035858154
    },
    {
      "epoch": 0.22455284552845528,
      "step": 4143,
      "training_loss": 6.7412109375
    },
    {
      "epoch": 0.2246070460704607,
      "grad_norm": 16.543994903564453,
      "learning_rate": 1e-05,
      "loss": 6.7872,
      "step": 4144
    },
    {
      "epoch": 0.2246070460704607,
      "step": 4144,
      "training_loss": 4.418519496917725
    },
    {
      "epoch": 0.22466124661246611,
      "step": 4145,
      "training_loss": 6.084198951721191
    },
    {
      "epoch": 0.22471544715447153,
      "step": 4146,
      "training_loss": 6.8445658683776855
    },
    {
      "epoch": 0.22476964769647698,
      "step": 4147,
      "training_loss": 7.1573920249938965
    },
    {
      "epoch": 0.2248238482384824,
      "grad_norm": 20.34365463256836,
      "learning_rate": 1e-05,
      "loss": 6.1262,
      "step": 4148
    },
    {
      "epoch": 0.2248238482384824,
      "step": 4148,
      "training_loss": 7.604632377624512
    },
    {
      "epoch": 0.2248780487804878,
      "step": 4149,
      "training_loss": 7.170168399810791
    },
    {
      "epoch": 0.22493224932249323,
      "step": 4150,
      "training_loss": 11.59896183013916
    },
    {
      "epoch": 0.22498644986449864,
      "step": 4151,
      "training_loss": 5.432461261749268
    },
    {
      "epoch": 0.22504065040650406,
      "grad_norm": 45.05095291137695,
      "learning_rate": 1e-05,
      "loss": 7.9516,
      "step": 4152
    },
    {
      "epoch": 0.22504065040650406,
      "step": 4152,
      "training_loss": 7.285581588745117
    },
    {
      "epoch": 0.22509485094850948,
      "step": 4153,
      "training_loss": 6.110910892486572
    },
    {
      "epoch": 0.2251490514905149,
      "step": 4154,
      "training_loss": 6.8191118240356445
    },
    {
      "epoch": 0.2252032520325203,
      "step": 4155,
      "training_loss": 7.5815839767456055
    },
    {
      "epoch": 0.22525745257452576,
      "grad_norm": 16.72846794128418,
      "learning_rate": 1e-05,
      "loss": 6.9493,
      "step": 4156
    },
    {
      "epoch": 0.22525745257452576,
      "step": 4156,
      "training_loss": 6.840685844421387
    },
    {
      "epoch": 0.22531165311653117,
      "step": 4157,
      "training_loss": 6.894312858581543
    },
    {
      "epoch": 0.2253658536585366,
      "step": 4158,
      "training_loss": 7.066439151763916
    },
    {
      "epoch": 0.225420054200542,
      "step": 4159,
      "training_loss": 6.329594612121582
    },
    {
      "epoch": 0.22547425474254743,
      "grad_norm": 24.23981285095215,
      "learning_rate": 1e-05,
      "loss": 6.7828,
      "step": 4160
    },
    {
      "epoch": 0.22547425474254743,
      "step": 4160,
      "training_loss": 8.1886568069458
    },
    {
      "epoch": 0.22552845528455284,
      "step": 4161,
      "training_loss": 5.553374767303467
    },
    {
      "epoch": 0.22558265582655826,
      "step": 4162,
      "training_loss": 6.8200459480285645
    },
    {
      "epoch": 0.22563685636856368,
      "step": 4163,
      "training_loss": 7.016386985778809
    },
    {
      "epoch": 0.2256910569105691,
      "grad_norm": 24.214054107666016,
      "learning_rate": 1e-05,
      "loss": 6.8946,
      "step": 4164
    },
    {
      "epoch": 0.2256910569105691,
      "step": 4164,
      "training_loss": 5.9942708015441895
    },
    {
      "epoch": 0.22574525745257454,
      "step": 4165,
      "training_loss": 7.201555252075195
    },
    {
      "epoch": 0.22579945799457996,
      "step": 4166,
      "training_loss": 7.0515265464782715
    },
    {
      "epoch": 0.22585365853658537,
      "step": 4167,
      "training_loss": 6.194610595703125
    },
    {
      "epoch": 0.2259078590785908,
      "grad_norm": 16.93990707397461,
      "learning_rate": 1e-05,
      "loss": 6.6105,
      "step": 4168
    },
    {
      "epoch": 0.2259078590785908,
      "step": 4168,
      "training_loss": 6.100837230682373
    },
    {
      "epoch": 0.2259620596205962,
      "step": 4169,
      "training_loss": 7.079367637634277
    },
    {
      "epoch": 0.22601626016260162,
      "step": 4170,
      "training_loss": 6.511838912963867
    },
    {
      "epoch": 0.22607046070460704,
      "step": 4171,
      "training_loss": 7.363991737365723
    },
    {
      "epoch": 0.22612466124661246,
      "grad_norm": 19.55134391784668,
      "learning_rate": 1e-05,
      "loss": 6.764,
      "step": 4172
    },
    {
      "epoch": 0.22612466124661246,
      "step": 4172,
      "training_loss": 6.4290452003479
    },
    {
      "epoch": 0.22617886178861787,
      "step": 4173,
      "training_loss": 6.499814510345459
    },
    {
      "epoch": 0.22623306233062332,
      "step": 4174,
      "training_loss": 4.797901630401611
    },
    {
      "epoch": 0.22628726287262874,
      "step": 4175,
      "training_loss": 7.2690300941467285
    },
    {
      "epoch": 0.22634146341463415,
      "grad_norm": 41.34626388549805,
      "learning_rate": 1e-05,
      "loss": 6.2489,
      "step": 4176
    },
    {
      "epoch": 0.22634146341463415,
      "step": 4176,
      "training_loss": 5.624520301818848
    },
    {
      "epoch": 0.22639566395663957,
      "step": 4177,
      "training_loss": 7.718352317810059
    },
    {
      "epoch": 0.226449864498645,
      "step": 4178,
      "training_loss": 6.857205390930176
    },
    {
      "epoch": 0.2265040650406504,
      "step": 4179,
      "training_loss": 6.757772922515869
    },
    {
      "epoch": 0.22655826558265582,
      "grad_norm": 29.994068145751953,
      "learning_rate": 1e-05,
      "loss": 6.7395,
      "step": 4180
    },
    {
      "epoch": 0.22655826558265582,
      "step": 4180,
      "training_loss": 6.795315742492676
    },
    {
      "epoch": 0.22661246612466124,
      "step": 4181,
      "training_loss": 5.929982662200928
    },
    {
      "epoch": 0.22666666666666666,
      "step": 4182,
      "training_loss": 7.619246482849121
    },
    {
      "epoch": 0.2267208672086721,
      "step": 4183,
      "training_loss": 7.148619174957275
    },
    {
      "epoch": 0.22677506775067752,
      "grad_norm": 17.393035888671875,
      "learning_rate": 1e-05,
      "loss": 6.8733,
      "step": 4184
    },
    {
      "epoch": 0.22677506775067752,
      "step": 4184,
      "training_loss": 4.824831008911133
    },
    {
      "epoch": 0.22682926829268293,
      "step": 4185,
      "training_loss": 7.146673679351807
    },
    {
      "epoch": 0.22688346883468835,
      "step": 4186,
      "training_loss": 6.848747253417969
    },
    {
      "epoch": 0.22693766937669377,
      "step": 4187,
      "training_loss": 7.469182968139648
    },
    {
      "epoch": 0.22699186991869919,
      "grad_norm": 19.43671226501465,
      "learning_rate": 1e-05,
      "loss": 6.5724,
      "step": 4188
    },
    {
      "epoch": 0.22699186991869919,
      "step": 4188,
      "training_loss": 6.738481521606445
    },
    {
      "epoch": 0.2270460704607046,
      "step": 4189,
      "training_loss": 7.04539155960083
    },
    {
      "epoch": 0.22710027100271002,
      "step": 4190,
      "training_loss": 6.7117509841918945
    },
    {
      "epoch": 0.22715447154471544,
      "step": 4191,
      "training_loss": 6.724305629730225
    },
    {
      "epoch": 0.22720867208672088,
      "grad_norm": 31.04790496826172,
      "learning_rate": 1e-05,
      "loss": 6.805,
      "step": 4192
    },
    {
      "epoch": 0.22720867208672088,
      "step": 4192,
      "training_loss": 7.146434783935547
    },
    {
      "epoch": 0.2272628726287263,
      "step": 4193,
      "training_loss": 7.600211143493652
    },
    {
      "epoch": 0.22731707317073171,
      "step": 4194,
      "training_loss": 7.213775157928467
    },
    {
      "epoch": 0.22737127371273713,
      "step": 4195,
      "training_loss": 7.148360252380371
    },
    {
      "epoch": 0.22742547425474255,
      "grad_norm": 24.334524154663086,
      "learning_rate": 1e-05,
      "loss": 7.2772,
      "step": 4196
    },
    {
      "epoch": 0.22742547425474255,
      "step": 4196,
      "training_loss": 6.7052483558654785
    },
    {
      "epoch": 0.22747967479674797,
      "step": 4197,
      "training_loss": 6.868356704711914
    },
    {
      "epoch": 0.22753387533875338,
      "step": 4198,
      "training_loss": 7.4333882331848145
    },
    {
      "epoch": 0.2275880758807588,
      "step": 4199,
      "training_loss": 6.690549373626709
    },
    {
      "epoch": 0.22764227642276422,
      "grad_norm": 40.33662796020508,
      "learning_rate": 1e-05,
      "loss": 6.9244,
      "step": 4200
    },
    {
      "epoch": 0.22764227642276422,
      "step": 4200,
      "training_loss": 8.845355033874512
    },
    {
      "epoch": 0.22769647696476963,
      "step": 4201,
      "training_loss": 5.854869365692139
    },
    {
      "epoch": 0.22775067750677508,
      "step": 4202,
      "training_loss": 6.587584018707275
    },
    {
      "epoch": 0.2278048780487805,
      "step": 4203,
      "training_loss": 7.641119956970215
    },
    {
      "epoch": 0.2278590785907859,
      "grad_norm": 34.47628402709961,
      "learning_rate": 1e-05,
      "loss": 7.2322,
      "step": 4204
    },
    {
      "epoch": 0.2278590785907859,
      "step": 4204,
      "training_loss": 8.692529678344727
    },
    {
      "epoch": 0.22791327913279133,
      "step": 4205,
      "training_loss": 7.199920177459717
    },
    {
      "epoch": 0.22796747967479675,
      "step": 4206,
      "training_loss": 6.365113735198975
    },
    {
      "epoch": 0.22802168021680216,
      "step": 4207,
      "training_loss": 7.408240795135498
    },
    {
      "epoch": 0.22807588075880758,
      "grad_norm": 20.117053985595703,
      "learning_rate": 1e-05,
      "loss": 7.4165,
      "step": 4208
    },
    {
      "epoch": 0.22807588075880758,
      "step": 4208,
      "training_loss": 7.338084697723389
    },
    {
      "epoch": 0.228130081300813,
      "step": 4209,
      "training_loss": 6.603647708892822
    },
    {
      "epoch": 0.22818428184281841,
      "step": 4210,
      "training_loss": 6.460409641265869
    },
    {
      "epoch": 0.22823848238482386,
      "step": 4211,
      "training_loss": 7.310305595397949
    },
    {
      "epoch": 0.22829268292682928,
      "grad_norm": 29.24696922302246,
      "learning_rate": 1e-05,
      "loss": 6.9281,
      "step": 4212
    },
    {
      "epoch": 0.22829268292682928,
      "step": 4212,
      "training_loss": 6.535386562347412
    },
    {
      "epoch": 0.2283468834688347,
      "step": 4213,
      "training_loss": 6.860266208648682
    },
    {
      "epoch": 0.2284010840108401,
      "step": 4214,
      "training_loss": 7.141116142272949
    },
    {
      "epoch": 0.22845528455284553,
      "step": 4215,
      "training_loss": 7.002740383148193
    },
    {
      "epoch": 0.22850948509485094,
      "grad_norm": 31.001296997070312,
      "learning_rate": 1e-05,
      "loss": 6.8849,
      "step": 4216
    },
    {
      "epoch": 0.22850948509485094,
      "step": 4216,
      "training_loss": 6.864316463470459
    },
    {
      "epoch": 0.22856368563685636,
      "step": 4217,
      "training_loss": 7.001038551330566
    },
    {
      "epoch": 0.22861788617886178,
      "step": 4218,
      "training_loss": 5.335446357727051
    },
    {
      "epoch": 0.2286720867208672,
      "step": 4219,
      "training_loss": 6.358856201171875
    },
    {
      "epoch": 0.22872628726287264,
      "grad_norm": 24.977581024169922,
      "learning_rate": 1e-05,
      "loss": 6.3899,
      "step": 4220
    },
    {
      "epoch": 0.22872628726287264,
      "step": 4220,
      "training_loss": 7.6785454750061035
    },
    {
      "epoch": 0.22878048780487806,
      "step": 4221,
      "training_loss": 6.9177327156066895
    },
    {
      "epoch": 0.22883468834688347,
      "step": 4222,
      "training_loss": 7.76539421081543
    },
    {
      "epoch": 0.2288888888888889,
      "step": 4223,
      "training_loss": 7.284115314483643
    },
    {
      "epoch": 0.2289430894308943,
      "grad_norm": 24.794361114501953,
      "learning_rate": 1e-05,
      "loss": 7.4114,
      "step": 4224
    },
    {
      "epoch": 0.2289430894308943,
      "step": 4224,
      "training_loss": 7.619570732116699
    },
    {
      "epoch": 0.22899728997289973,
      "step": 4225,
      "training_loss": 5.207873344421387
    },
    {
      "epoch": 0.22905149051490514,
      "step": 4226,
      "training_loss": 4.472482681274414
    },
    {
      "epoch": 0.22910569105691056,
      "step": 4227,
      "training_loss": 6.558895111083984
    },
    {
      "epoch": 0.22915989159891598,
      "grad_norm": 33.103275299072266,
      "learning_rate": 1e-05,
      "loss": 5.9647,
      "step": 4228
    },
    {
      "epoch": 0.22915989159891598,
      "step": 4228,
      "training_loss": 6.773101329803467
    },
    {
      "epoch": 0.22921409214092142,
      "step": 4229,
      "training_loss": 6.778286457061768
    },
    {
      "epoch": 0.22926829268292684,
      "step": 4230,
      "training_loss": 7.39033842086792
    },
    {
      "epoch": 0.22932249322493226,
      "step": 4231,
      "training_loss": 6.01769495010376
    },
    {
      "epoch": 0.22937669376693767,
      "grad_norm": 25.01999855041504,
      "learning_rate": 1e-05,
      "loss": 6.7399,
      "step": 4232
    },
    {
      "epoch": 0.22937669376693767,
      "step": 4232,
      "training_loss": 8.08940601348877
    },
    {
      "epoch": 0.2294308943089431,
      "step": 4233,
      "training_loss": 7.029899597167969
    },
    {
      "epoch": 0.2294850948509485,
      "step": 4234,
      "training_loss": 4.804824352264404
    },
    {
      "epoch": 0.22953929539295392,
      "step": 4235,
      "training_loss": 6.4619927406311035
    },
    {
      "epoch": 0.22959349593495934,
      "grad_norm": 20.81642723083496,
      "learning_rate": 1e-05,
      "loss": 6.5965,
      "step": 4236
    },
    {
      "epoch": 0.22959349593495934,
      "step": 4236,
      "training_loss": 6.856471061706543
    },
    {
      "epoch": 0.22964769647696476,
      "step": 4237,
      "training_loss": 7.169760227203369
    },
    {
      "epoch": 0.2297018970189702,
      "step": 4238,
      "training_loss": 6.763638019561768
    },
    {
      "epoch": 0.22975609756097562,
      "step": 4239,
      "training_loss": 7.783140659332275
    },
    {
      "epoch": 0.22981029810298104,
      "grad_norm": 25.889514923095703,
      "learning_rate": 1e-05,
      "loss": 7.1433,
      "step": 4240
    },
    {
      "epoch": 0.22981029810298104,
      "step": 4240,
      "training_loss": 6.09543514251709
    },
    {
      "epoch": 0.22986449864498645,
      "step": 4241,
      "training_loss": 5.838420867919922
    },
    {
      "epoch": 0.22991869918699187,
      "step": 4242,
      "training_loss": 6.8158745765686035
    },
    {
      "epoch": 0.2299728997289973,
      "step": 4243,
      "training_loss": 5.121786594390869
    },
    {
      "epoch": 0.2300271002710027,
      "grad_norm": 19.348005294799805,
      "learning_rate": 1e-05,
      "loss": 5.9679,
      "step": 4244
    },
    {
      "epoch": 0.2300271002710027,
      "step": 4244,
      "training_loss": 5.525928020477295
    },
    {
      "epoch": 0.23008130081300812,
      "step": 4245,
      "training_loss": 7.455216407775879
    },
    {
      "epoch": 0.23013550135501354,
      "step": 4246,
      "training_loss": 6.978044033050537
    },
    {
      "epoch": 0.23018970189701898,
      "step": 4247,
      "training_loss": 6.53294038772583
    },
    {
      "epoch": 0.2302439024390244,
      "grad_norm": 18.038639068603516,
      "learning_rate": 1e-05,
      "loss": 6.623,
      "step": 4248
    },
    {
      "epoch": 0.2302439024390244,
      "step": 4248,
      "training_loss": 6.75203800201416
    },
    {
      "epoch": 0.23029810298102982,
      "step": 4249,
      "training_loss": 7.35309362411499
    },
    {
      "epoch": 0.23035230352303523,
      "step": 4250,
      "training_loss": 6.266088485717773
    },
    {
      "epoch": 0.23040650406504065,
      "step": 4251,
      "training_loss": 7.271257400512695
    },
    {
      "epoch": 0.23046070460704607,
      "grad_norm": 21.460468292236328,
      "learning_rate": 1e-05,
      "loss": 6.9106,
      "step": 4252
    },
    {
      "epoch": 0.23046070460704607,
      "step": 4252,
      "training_loss": 7.242105960845947
    },
    {
      "epoch": 0.23051490514905149,
      "step": 4253,
      "training_loss": 7.505939483642578
    },
    {
      "epoch": 0.2305691056910569,
      "step": 4254,
      "training_loss": 7.153629779815674
    },
    {
      "epoch": 0.23062330623306232,
      "step": 4255,
      "training_loss": 5.629159927368164
    },
    {
      "epoch": 0.23067750677506776,
      "grad_norm": 20.276254653930664,
      "learning_rate": 1e-05,
      "loss": 6.8827,
      "step": 4256
    },
    {
      "epoch": 0.23067750677506776,
      "step": 4256,
      "training_loss": 6.505120277404785
    },
    {
      "epoch": 0.23073170731707318,
      "step": 4257,
      "training_loss": 7.813936710357666
    },
    {
      "epoch": 0.2307859078590786,
      "step": 4258,
      "training_loss": 7.301130771636963
    },
    {
      "epoch": 0.23084010840108402,
      "step": 4259,
      "training_loss": 7.717890739440918
    },
    {
      "epoch": 0.23089430894308943,
      "grad_norm": 17.935928344726562,
      "learning_rate": 1e-05,
      "loss": 7.3345,
      "step": 4260
    },
    {
      "epoch": 0.23089430894308943,
      "step": 4260,
      "training_loss": 5.755926132202148
    },
    {
      "epoch": 0.23094850948509485,
      "step": 4261,
      "training_loss": 7.042089939117432
    },
    {
      "epoch": 0.23100271002710027,
      "step": 4262,
      "training_loss": 6.278707027435303
    },
    {
      "epoch": 0.23105691056910568,
      "step": 4263,
      "training_loss": 6.90524435043335
    },
    {
      "epoch": 0.2311111111111111,
      "grad_norm": 14.44227123260498,
      "learning_rate": 1e-05,
      "loss": 6.4955,
      "step": 4264
    },
    {
      "epoch": 0.2311111111111111,
      "step": 4264,
      "training_loss": 7.544012546539307
    },
    {
      "epoch": 0.23116531165311652,
      "step": 4265,
      "training_loss": 7.387238502502441
    },
    {
      "epoch": 0.23121951219512196,
      "step": 4266,
      "training_loss": 8.332276344299316
    },
    {
      "epoch": 0.23127371273712738,
      "step": 4267,
      "training_loss": 7.205306529998779
    },
    {
      "epoch": 0.2313279132791328,
      "grad_norm": 28.627113342285156,
      "learning_rate": 1e-05,
      "loss": 7.6172,
      "step": 4268
    },
    {
      "epoch": 0.2313279132791328,
      "step": 4268,
      "training_loss": 7.700301647186279
    },
    {
      "epoch": 0.2313821138211382,
      "step": 4269,
      "training_loss": 7.115381717681885
    },
    {
      "epoch": 0.23143631436314363,
      "step": 4270,
      "training_loss": 7.281911849975586
    },
    {
      "epoch": 0.23149051490514905,
      "step": 4271,
      "training_loss": 8.2998046875
    },
    {
      "epoch": 0.23154471544715446,
      "grad_norm": 23.420183181762695,
      "learning_rate": 1e-05,
      "loss": 7.5993,
      "step": 4272
    },
    {
      "epoch": 0.23154471544715446,
      "step": 4272,
      "training_loss": 7.440885543823242
    },
    {
      "epoch": 0.23159891598915988,
      "step": 4273,
      "training_loss": 6.438448429107666
    },
    {
      "epoch": 0.2316531165311653,
      "step": 4274,
      "training_loss": 6.482273578643799
    },
    {
      "epoch": 0.23170731707317074,
      "step": 4275,
      "training_loss": 6.972700119018555
    },
    {
      "epoch": 0.23176151761517616,
      "grad_norm": 29.951087951660156,
      "learning_rate": 1e-05,
      "loss": 6.8336,
      "step": 4276
    },
    {
      "epoch": 0.23176151761517616,
      "step": 4276,
      "training_loss": 6.996655464172363
    },
    {
      "epoch": 0.23181571815718158,
      "step": 4277,
      "training_loss": 6.064216613769531
    },
    {
      "epoch": 0.231869918699187,
      "step": 4278,
      "training_loss": 6.33924674987793
    },
    {
      "epoch": 0.2319241192411924,
      "step": 4279,
      "training_loss": 5.169548988342285
    },
    {
      "epoch": 0.23197831978319783,
      "grad_norm": 27.32291030883789,
      "learning_rate": 1e-05,
      "loss": 6.1424,
      "step": 4280
    },
    {
      "epoch": 0.23197831978319783,
      "step": 4280,
      "training_loss": 6.970345973968506
    },
    {
      "epoch": 0.23203252032520325,
      "step": 4281,
      "training_loss": 6.944186210632324
    },
    {
      "epoch": 0.23208672086720866,
      "step": 4282,
      "training_loss": 5.395129203796387
    },
    {
      "epoch": 0.23214092140921408,
      "step": 4283,
      "training_loss": 6.404702186584473
    },
    {
      "epoch": 0.23219512195121952,
      "grad_norm": 23.12282943725586,
      "learning_rate": 1e-05,
      "loss": 6.4286,
      "step": 4284
    },
    {
      "epoch": 0.23219512195121952,
      "step": 4284,
      "training_loss": 5.905412197113037
    },
    {
      "epoch": 0.23224932249322494,
      "step": 4285,
      "training_loss": 6.4146223068237305
    },
    {
      "epoch": 0.23230352303523036,
      "step": 4286,
      "training_loss": 6.402682304382324
    },
    {
      "epoch": 0.23235772357723578,
      "step": 4287,
      "training_loss": 7.227110385894775
    },
    {
      "epoch": 0.2324119241192412,
      "grad_norm": 27.11325454711914,
      "learning_rate": 1e-05,
      "loss": 6.4875,
      "step": 4288
    },
    {
      "epoch": 0.2324119241192412,
      "step": 4288,
      "training_loss": 6.795136451721191
    },
    {
      "epoch": 0.2324661246612466,
      "step": 4289,
      "training_loss": 5.492066860198975
    },
    {
      "epoch": 0.23252032520325203,
      "step": 4290,
      "training_loss": 6.959251880645752
    },
    {
      "epoch": 0.23257452574525744,
      "step": 4291,
      "training_loss": 7.216732501983643
    },
    {
      "epoch": 0.23262872628726286,
      "grad_norm": 32.04901885986328,
      "learning_rate": 1e-05,
      "loss": 6.6158,
      "step": 4292
    },
    {
      "epoch": 0.23262872628726286,
      "step": 4292,
      "training_loss": 8.127777099609375
    },
    {
      "epoch": 0.2326829268292683,
      "step": 4293,
      "training_loss": 6.613431930541992
    },
    {
      "epoch": 0.23273712737127372,
      "step": 4294,
      "training_loss": 7.988693714141846
    },
    {
      "epoch": 0.23279132791327914,
      "step": 4295,
      "training_loss": 6.850008487701416
    },
    {
      "epoch": 0.23284552845528456,
      "grad_norm": 16.248441696166992,
      "learning_rate": 1e-05,
      "loss": 7.395,
      "step": 4296
    },
    {
      "epoch": 0.23284552845528456,
      "step": 4296,
      "training_loss": 6.224175930023193
    },
    {
      "epoch": 0.23289972899728997,
      "step": 4297,
      "training_loss": 7.074512481689453
    },
    {
      "epoch": 0.2329539295392954,
      "step": 4298,
      "training_loss": 4.89918851852417
    },
    {
      "epoch": 0.2330081300813008,
      "step": 4299,
      "training_loss": 7.010467529296875
    },
    {
      "epoch": 0.23306233062330622,
      "grad_norm": 21.734590530395508,
      "learning_rate": 1e-05,
      "loss": 6.3021,
      "step": 4300
    },
    {
      "epoch": 0.23306233062330622,
      "step": 4300,
      "training_loss": 7.06980037689209
    },
    {
      "epoch": 0.23311653116531164,
      "step": 4301,
      "training_loss": 8.148900032043457
    },
    {
      "epoch": 0.23317073170731709,
      "step": 4302,
      "training_loss": 6.709314346313477
    },
    {
      "epoch": 0.2332249322493225,
      "step": 4303,
      "training_loss": 6.533067226409912
    },
    {
      "epoch": 0.23327913279132792,
      "grad_norm": 21.139089584350586,
      "learning_rate": 1e-05,
      "loss": 7.1153,
      "step": 4304
    },
    {
      "epoch": 0.23327913279132792,
      "step": 4304,
      "training_loss": 6.780628204345703
    },
    {
      "epoch": 0.23333333333333334,
      "step": 4305,
      "training_loss": 6.279298305511475
    },
    {
      "epoch": 0.23338753387533875,
      "step": 4306,
      "training_loss": 7.58099889755249
    },
    {
      "epoch": 0.23344173441734417,
      "step": 4307,
      "training_loss": 5.68433952331543
    },
    {
      "epoch": 0.2334959349593496,
      "grad_norm": 21.258888244628906,
      "learning_rate": 1e-05,
      "loss": 6.5813,
      "step": 4308
    },
    {
      "epoch": 0.2334959349593496,
      "step": 4308,
      "training_loss": 7.180887699127197
    },
    {
      "epoch": 0.233550135501355,
      "step": 4309,
      "training_loss": 6.030940055847168
    },
    {
      "epoch": 0.23360433604336042,
      "step": 4310,
      "training_loss": 6.864538669586182
    },
    {
      "epoch": 0.23365853658536587,
      "step": 4311,
      "training_loss": 5.611827373504639
    },
    {
      "epoch": 0.23371273712737128,
      "grad_norm": 26.106382369995117,
      "learning_rate": 1e-05,
      "loss": 6.422,
      "step": 4312
    },
    {
      "epoch": 0.23371273712737128,
      "step": 4312,
      "training_loss": 7.384819507598877
    },
    {
      "epoch": 0.2337669376693767,
      "step": 4313,
      "training_loss": 6.951534271240234
    },
    {
      "epoch": 0.23382113821138212,
      "step": 4314,
      "training_loss": 6.653585433959961
    },
    {
      "epoch": 0.23387533875338753,
      "step": 4315,
      "training_loss": 7.824566841125488
    },
    {
      "epoch": 0.23392953929539295,
      "grad_norm": 29.15214729309082,
      "learning_rate": 1e-05,
      "loss": 7.2036,
      "step": 4316
    },
    {
      "epoch": 0.23392953929539295,
      "step": 4316,
      "training_loss": 6.0780029296875
    },
    {
      "epoch": 0.23398373983739837,
      "step": 4317,
      "training_loss": 5.289434432983398
    },
    {
      "epoch": 0.23403794037940379,
      "step": 4318,
      "training_loss": 6.497130870819092
    },
    {
      "epoch": 0.2340921409214092,
      "step": 4319,
      "training_loss": 6.853566646575928
    },
    {
      "epoch": 0.23414634146341465,
      "grad_norm": 24.81565284729004,
      "learning_rate": 1e-05,
      "loss": 6.1795,
      "step": 4320
    },
    {
      "epoch": 0.23414634146341465,
      "step": 4320,
      "training_loss": 6.213203430175781
    },
    {
      "epoch": 0.23420054200542006,
      "step": 4321,
      "training_loss": 6.970090866088867
    },
    {
      "epoch": 0.23425474254742548,
      "step": 4322,
      "training_loss": 7.083038330078125
    },
    {
      "epoch": 0.2343089430894309,
      "step": 4323,
      "training_loss": 7.925377368927002
    },
    {
      "epoch": 0.23436314363143632,
      "grad_norm": 25.734222412109375,
      "learning_rate": 1e-05,
      "loss": 7.0479,
      "step": 4324
    },
    {
      "epoch": 0.23436314363143632,
      "step": 4324,
      "training_loss": 8.052496910095215
    },
    {
      "epoch": 0.23441734417344173,
      "step": 4325,
      "training_loss": 7.318731307983398
    },
    {
      "epoch": 0.23447154471544715,
      "step": 4326,
      "training_loss": 7.2207350730896
    },
    {
      "epoch": 0.23452574525745257,
      "step": 4327,
      "training_loss": 6.733564853668213
    },
    {
      "epoch": 0.23457994579945798,
      "grad_norm": 18.650577545166016,
      "learning_rate": 1e-05,
      "loss": 7.3314,
      "step": 4328
    },
    {
      "epoch": 0.23457994579945798,
      "step": 4328,
      "training_loss": 6.689419269561768
    },
    {
      "epoch": 0.2346341463414634,
      "step": 4329,
      "training_loss": 7.002178192138672
    },
    {
      "epoch": 0.23468834688346885,
      "step": 4330,
      "training_loss": 7.577493667602539
    },
    {
      "epoch": 0.23474254742547426,
      "step": 4331,
      "training_loss": 7.1103925704956055
    },
    {
      "epoch": 0.23479674796747968,
      "grad_norm": 21.00897979736328,
      "learning_rate": 1e-05,
      "loss": 7.0949,
      "step": 4332
    },
    {
      "epoch": 0.23479674796747968,
      "step": 4332,
      "training_loss": 7.373303413391113
    },
    {
      "epoch": 0.2348509485094851,
      "step": 4333,
      "training_loss": 7.176000118255615
    },
    {
      "epoch": 0.2349051490514905,
      "step": 4334,
      "training_loss": 7.78582239151001
    },
    {
      "epoch": 0.23495934959349593,
      "step": 4335,
      "training_loss": 7.943779468536377
    },
    {
      "epoch": 0.23501355013550135,
      "grad_norm": 16.026491165161133,
      "learning_rate": 1e-05,
      "loss": 7.5697,
      "step": 4336
    },
    {
      "epoch": 0.23501355013550135,
      "step": 4336,
      "training_loss": 6.4475202560424805
    },
    {
      "epoch": 0.23506775067750676,
      "step": 4337,
      "training_loss": 6.2717437744140625
    },
    {
      "epoch": 0.23512195121951218,
      "step": 4338,
      "training_loss": 5.523937702178955
    },
    {
      "epoch": 0.23517615176151763,
      "step": 4339,
      "training_loss": 4.958921909332275
    },
    {
      "epoch": 0.23523035230352304,
      "grad_norm": 19.618003845214844,
      "learning_rate": 1e-05,
      "loss": 5.8005,
      "step": 4340
    },
    {
      "epoch": 0.23523035230352304,
      "step": 4340,
      "training_loss": 6.8440327644348145
    },
    {
      "epoch": 0.23528455284552846,
      "step": 4341,
      "training_loss": 6.567181587219238
    },
    {
      "epoch": 0.23533875338753388,
      "step": 4342,
      "training_loss": 6.764875411987305
    },
    {
      "epoch": 0.2353929539295393,
      "step": 4343,
      "training_loss": 7.353209495544434
    },
    {
      "epoch": 0.2354471544715447,
      "grad_norm": 21.754444122314453,
      "learning_rate": 1e-05,
      "loss": 6.8823,
      "step": 4344
    },
    {
      "epoch": 0.2354471544715447,
      "step": 4344,
      "training_loss": 7.613955020904541
    },
    {
      "epoch": 0.23550135501355013,
      "step": 4345,
      "training_loss": 6.435595512390137
    },
    {
      "epoch": 0.23555555555555555,
      "step": 4346,
      "training_loss": 5.921867370605469
    },
    {
      "epoch": 0.23560975609756096,
      "step": 4347,
      "training_loss": 7.431410789489746
    },
    {
      "epoch": 0.2356639566395664,
      "grad_norm": 23.11284065246582,
      "learning_rate": 1e-05,
      "loss": 6.8507,
      "step": 4348
    },
    {
      "epoch": 0.2356639566395664,
      "step": 4348,
      "training_loss": 7.884244441986084
    },
    {
      "epoch": 0.23571815718157182,
      "step": 4349,
      "training_loss": 7.529660701751709
    },
    {
      "epoch": 0.23577235772357724,
      "step": 4350,
      "training_loss": 7.712798118591309
    },
    {
      "epoch": 0.23582655826558266,
      "step": 4351,
      "training_loss": 6.622200012207031
    },
    {
      "epoch": 0.23588075880758808,
      "grad_norm": 23.998422622680664,
      "learning_rate": 1e-05,
      "loss": 7.4372,
      "step": 4352
    },
    {
      "epoch": 0.23588075880758808,
      "step": 4352,
      "training_loss": 6.048445701599121
    },
    {
      "epoch": 0.2359349593495935,
      "step": 4353,
      "training_loss": 4.9421868324279785
    },
    {
      "epoch": 0.2359891598915989,
      "step": 4354,
      "training_loss": 6.4204792976379395
    },
    {
      "epoch": 0.23604336043360433,
      "step": 4355,
      "training_loss": 7.2341413497924805
    },
    {
      "epoch": 0.23609756097560974,
      "grad_norm": 14.75236988067627,
      "learning_rate": 1e-05,
      "loss": 6.1613,
      "step": 4356
    },
    {
      "epoch": 0.23609756097560974,
      "step": 4356,
      "training_loss": 7.485603332519531
    },
    {
      "epoch": 0.2361517615176152,
      "step": 4357,
      "training_loss": 6.263051509857178
    },
    {
      "epoch": 0.2362059620596206,
      "step": 4358,
      "training_loss": 8.23328685760498
    },
    {
      "epoch": 0.23626016260162602,
      "step": 4359,
      "training_loss": 7.664773941040039
    },
    {
      "epoch": 0.23631436314363144,
      "grad_norm": 24.57575798034668,
      "learning_rate": 1e-05,
      "loss": 7.4117,
      "step": 4360
    },
    {
      "epoch": 0.23631436314363144,
      "step": 4360,
      "training_loss": 8.359538078308105
    },
    {
      "epoch": 0.23636856368563686,
      "step": 4361,
      "training_loss": 7.356449127197266
    },
    {
      "epoch": 0.23642276422764227,
      "step": 4362,
      "training_loss": 6.2679524421691895
    },
    {
      "epoch": 0.2364769647696477,
      "step": 4363,
      "training_loss": 6.665120601654053
    },
    {
      "epoch": 0.2365311653116531,
      "grad_norm": 26.507112503051758,
      "learning_rate": 1e-05,
      "loss": 7.1623,
      "step": 4364
    },
    {
      "epoch": 0.2365311653116531,
      "step": 4364,
      "training_loss": 6.9254536628723145
    },
    {
      "epoch": 0.23658536585365852,
      "step": 4365,
      "training_loss": 6.877685070037842
    },
    {
      "epoch": 0.23663956639566397,
      "step": 4366,
      "training_loss": 7.397954940795898
    },
    {
      "epoch": 0.2366937669376694,
      "step": 4367,
      "training_loss": 7.783182621002197
    },
    {
      "epoch": 0.2367479674796748,
      "grad_norm": 21.223737716674805,
      "learning_rate": 1e-05,
      "loss": 7.2461,
      "step": 4368
    },
    {
      "epoch": 0.2367479674796748,
      "step": 4368,
      "training_loss": 7.627262592315674
    },
    {
      "epoch": 0.23680216802168022,
      "step": 4369,
      "training_loss": 7.412837028503418
    },
    {
      "epoch": 0.23685636856368564,
      "step": 4370,
      "training_loss": 7.338603973388672
    },
    {
      "epoch": 0.23691056910569105,
      "step": 4371,
      "training_loss": 5.434022903442383
    },
    {
      "epoch": 0.23696476964769647,
      "grad_norm": 22.177261352539062,
      "learning_rate": 1e-05,
      "loss": 6.9532,
      "step": 4372
    },
    {
      "epoch": 0.23696476964769647,
      "step": 4372,
      "training_loss": 7.146210193634033
    },
    {
      "epoch": 0.2370189701897019,
      "step": 4373,
      "training_loss": 5.160923004150391
    },
    {
      "epoch": 0.2370731707317073,
      "step": 4374,
      "training_loss": 7.406973838806152
    },
    {
      "epoch": 0.23712737127371275,
      "step": 4375,
      "training_loss": 5.317807674407959
    },
    {
      "epoch": 0.23718157181571817,
      "grad_norm": 53.280521392822266,
      "learning_rate": 1e-05,
      "loss": 6.258,
      "step": 4376
    },
    {
      "epoch": 0.23718157181571817,
      "step": 4376,
      "training_loss": 6.169683933258057
    },
    {
      "epoch": 0.23723577235772358,
      "step": 4377,
      "training_loss": 6.942132472991943
    },
    {
      "epoch": 0.237289972899729,
      "step": 4378,
      "training_loss": 6.819205284118652
    },
    {
      "epoch": 0.23734417344173442,
      "step": 4379,
      "training_loss": 7.4781413078308105
    },
    {
      "epoch": 0.23739837398373984,
      "grad_norm": 23.46261215209961,
      "learning_rate": 1e-05,
      "loss": 6.8523,
      "step": 4380
    },
    {
      "epoch": 0.23739837398373984,
      "step": 4380,
      "training_loss": 6.8815531730651855
    },
    {
      "epoch": 0.23745257452574525,
      "step": 4381,
      "training_loss": 7.065310001373291
    },
    {
      "epoch": 0.23750677506775067,
      "step": 4382,
      "training_loss": 7.8840789794921875
    },
    {
      "epoch": 0.2375609756097561,
      "step": 4383,
      "training_loss": 6.5218729972839355
    },
    {
      "epoch": 0.23761517615176153,
      "grad_norm": 56.67045974731445,
      "learning_rate": 1e-05,
      "loss": 7.0882,
      "step": 4384
    },
    {
      "epoch": 0.23761517615176153,
      "step": 4384,
      "training_loss": 7.386837959289551
    },
    {
      "epoch": 0.23766937669376695,
      "step": 4385,
      "training_loss": 6.462287425994873
    },
    {
      "epoch": 0.23772357723577237,
      "step": 4386,
      "training_loss": 5.866318225860596
    },
    {
      "epoch": 0.23777777777777778,
      "step": 4387,
      "training_loss": 6.120508670806885
    },
    {
      "epoch": 0.2378319783197832,
      "grad_norm": 20.200637817382812,
      "learning_rate": 1e-05,
      "loss": 6.459,
      "step": 4388
    },
    {
      "epoch": 0.2378319783197832,
      "step": 4388,
      "training_loss": 7.125944137573242
    },
    {
      "epoch": 0.23788617886178862,
      "step": 4389,
      "training_loss": 6.393800258636475
    },
    {
      "epoch": 0.23794037940379403,
      "step": 4390,
      "training_loss": 7.4474101066589355
    },
    {
      "epoch": 0.23799457994579945,
      "step": 4391,
      "training_loss": 8.869770050048828
    },
    {
      "epoch": 0.23804878048780487,
      "grad_norm": 24.917264938354492,
      "learning_rate": 1e-05,
      "loss": 7.4592,
      "step": 4392
    },
    {
      "epoch": 0.23804878048780487,
      "step": 4392,
      "training_loss": 7.319926738739014
    },
    {
      "epoch": 0.23810298102981028,
      "step": 4393,
      "training_loss": 6.81741189956665
    },
    {
      "epoch": 0.23815718157181573,
      "step": 4394,
      "training_loss": 7.273848056793213
    },
    {
      "epoch": 0.23821138211382115,
      "step": 4395,
      "training_loss": 6.321357250213623
    },
    {
      "epoch": 0.23826558265582656,
      "grad_norm": 26.68958282470703,
      "learning_rate": 1e-05,
      "loss": 6.9331,
      "step": 4396
    },
    {
      "epoch": 0.23826558265582656,
      "step": 4396,
      "training_loss": 7.1799211502075195
    },
    {
      "epoch": 0.23831978319783198,
      "step": 4397,
      "training_loss": 6.1893439292907715
    },
    {
      "epoch": 0.2383739837398374,
      "step": 4398,
      "training_loss": 6.818233966827393
    },
    {
      "epoch": 0.23842818428184281,
      "step": 4399,
      "training_loss": 7.8539018630981445
    },
    {
      "epoch": 0.23848238482384823,
      "grad_norm": 29.28624153137207,
      "learning_rate": 1e-05,
      "loss": 7.0104,
      "step": 4400
    },
    {
      "epoch": 0.23848238482384823,
      "step": 4400,
      "training_loss": 6.553771495819092
    },
    {
      "epoch": 0.23853658536585365,
      "step": 4401,
      "training_loss": 7.184660911560059
    },
    {
      "epoch": 0.23859078590785907,
      "step": 4402,
      "training_loss": 6.337023735046387
    },
    {
      "epoch": 0.2386449864498645,
      "step": 4403,
      "training_loss": 7.033659934997559
    },
    {
      "epoch": 0.23869918699186993,
      "grad_norm": 17.967388153076172,
      "learning_rate": 1e-05,
      "loss": 6.7773,
      "step": 4404
    },
    {
      "epoch": 0.23869918699186993,
      "step": 4404,
      "training_loss": 7.906445026397705
    },
    {
      "epoch": 0.23875338753387534,
      "step": 4405,
      "training_loss": 7.531702518463135
    },
    {
      "epoch": 0.23880758807588076,
      "step": 4406,
      "training_loss": 6.814608573913574
    },
    {
      "epoch": 0.23886178861788618,
      "step": 4407,
      "training_loss": 7.9899139404296875
    },
    {
      "epoch": 0.2389159891598916,
      "grad_norm": 17.45176887512207,
      "learning_rate": 1e-05,
      "loss": 7.5607,
      "step": 4408
    },
    {
      "epoch": 0.2389159891598916,
      "step": 4408,
      "training_loss": 4.271721839904785
    },
    {
      "epoch": 0.238970189701897,
      "step": 4409,
      "training_loss": 7.046867847442627
    },
    {
      "epoch": 0.23902439024390243,
      "step": 4410,
      "training_loss": 6.892483711242676
    },
    {
      "epoch": 0.23907859078590785,
      "step": 4411,
      "training_loss": 7.4565019607543945
    },
    {
      "epoch": 0.2391327913279133,
      "grad_norm": 19.478090286254883,
      "learning_rate": 1e-05,
      "loss": 6.4169,
      "step": 4412
    },
    {
      "epoch": 0.2391327913279133,
      "step": 4412,
      "training_loss": 6.644140720367432
    },
    {
      "epoch": 0.2391869918699187,
      "step": 4413,
      "training_loss": 7.1792426109313965
    },
    {
      "epoch": 0.23924119241192413,
      "step": 4414,
      "training_loss": 7.179388523101807
    },
    {
      "epoch": 0.23929539295392954,
      "step": 4415,
      "training_loss": 6.593448162078857
    },
    {
      "epoch": 0.23934959349593496,
      "grad_norm": 41.471309661865234,
      "learning_rate": 1e-05,
      "loss": 6.8991,
      "step": 4416
    },
    {
      "epoch": 0.23934959349593496,
      "step": 4416,
      "training_loss": 7.330967426300049
    },
    {
      "epoch": 0.23940379403794038,
      "step": 4417,
      "training_loss": 6.526386260986328
    },
    {
      "epoch": 0.2394579945799458,
      "step": 4418,
      "training_loss": 7.2749481201171875
    },
    {
      "epoch": 0.2395121951219512,
      "step": 4419,
      "training_loss": 6.88484525680542
    },
    {
      "epoch": 0.23956639566395663,
      "grad_norm": 24.583946228027344,
      "learning_rate": 1e-05,
      "loss": 7.0043,
      "step": 4420
    },
    {
      "epoch": 0.23956639566395663,
      "step": 4420,
      "training_loss": 7.430469512939453
    },
    {
      "epoch": 0.23962059620596207,
      "step": 4421,
      "training_loss": 6.7609968185424805
    },
    {
      "epoch": 0.2396747967479675,
      "step": 4422,
      "training_loss": 5.931519508361816
    },
    {
      "epoch": 0.2397289972899729,
      "step": 4423,
      "training_loss": 6.582832336425781
    },
    {
      "epoch": 0.23978319783197832,
      "grad_norm": 27.119464874267578,
      "learning_rate": 1e-05,
      "loss": 6.6765,
      "step": 4424
    },
    {
      "epoch": 0.23978319783197832,
      "step": 4424,
      "training_loss": 7.351327896118164
    },
    {
      "epoch": 0.23983739837398374,
      "step": 4425,
      "training_loss": 7.715068817138672
    },
    {
      "epoch": 0.23989159891598916,
      "step": 4426,
      "training_loss": 6.197259902954102
    },
    {
      "epoch": 0.23994579945799457,
      "step": 4427,
      "training_loss": 6.558686256408691
    },
    {
      "epoch": 0.24,
      "grad_norm": 18.540584564208984,
      "learning_rate": 1e-05,
      "loss": 6.9556,
      "step": 4428
    },
    {
      "epoch": 0.24,
      "step": 4428,
      "training_loss": 5.920137405395508
    },
    {
      "epoch": 0.2400542005420054,
      "step": 4429,
      "training_loss": 5.918654441833496
    },
    {
      "epoch": 0.24010840108401085,
      "step": 4430,
      "training_loss": 7.253365993499756
    },
    {
      "epoch": 0.24016260162601627,
      "step": 4431,
      "training_loss": 6.813449382781982
    },
    {
      "epoch": 0.2402168021680217,
      "grad_norm": 19.185752868652344,
      "learning_rate": 1e-05,
      "loss": 6.4764,
      "step": 4432
    },
    {
      "epoch": 0.2402168021680217,
      "step": 4432,
      "training_loss": 4.627323150634766
    },
    {
      "epoch": 0.2402710027100271,
      "step": 4433,
      "training_loss": 6.682458400726318
    },
    {
      "epoch": 0.24032520325203252,
      "step": 4434,
      "training_loss": 7.5768141746521
    },
    {
      "epoch": 0.24037940379403794,
      "step": 4435,
      "training_loss": 7.143962383270264
    },
    {
      "epoch": 0.24043360433604336,
      "grad_norm": 23.54817008972168,
      "learning_rate": 1e-05,
      "loss": 6.5076,
      "step": 4436
    },
    {
      "epoch": 0.24043360433604336,
      "step": 4436,
      "training_loss": 7.684629917144775
    },
    {
      "epoch": 0.24048780487804877,
      "step": 4437,
      "training_loss": 8.65621280670166
    },
    {
      "epoch": 0.2405420054200542,
      "step": 4438,
      "training_loss": 6.6017913818359375
    },
    {
      "epoch": 0.24059620596205963,
      "step": 4439,
      "training_loss": 7.6429338455200195
    },
    {
      "epoch": 0.24065040650406505,
      "grad_norm": 16.749080657958984,
      "learning_rate": 1e-05,
      "loss": 7.6464,
      "step": 4440
    },
    {
      "epoch": 0.24065040650406505,
      "step": 4440,
      "training_loss": 7.656450271606445
    },
    {
      "epoch": 0.24070460704607047,
      "step": 4441,
      "training_loss": 11.895524024963379
    },
    {
      "epoch": 0.24075880758807588,
      "step": 4442,
      "training_loss": 7.372775077819824
    },
    {
      "epoch": 0.2408130081300813,
      "step": 4443,
      "training_loss": 6.2542219161987305
    },
    {
      "epoch": 0.24086720867208672,
      "grad_norm": 21.486587524414062,
      "learning_rate": 1e-05,
      "loss": 8.2947,
      "step": 4444
    },
    {
      "epoch": 0.24086720867208672,
      "step": 4444,
      "training_loss": 5.528926372528076
    },
    {
      "epoch": 0.24092140921409214,
      "step": 4445,
      "training_loss": 7.64396858215332
    },
    {
      "epoch": 0.24097560975609755,
      "step": 4446,
      "training_loss": 6.5410332679748535
    },
    {
      "epoch": 0.24102981029810297,
      "step": 4447,
      "training_loss": 7.622759819030762
    },
    {
      "epoch": 0.24108401084010841,
      "grad_norm": 24.22739028930664,
      "learning_rate": 1e-05,
      "loss": 6.8342,
      "step": 4448
    },
    {
      "epoch": 0.24108401084010841,
      "step": 4448,
      "training_loss": 5.470860481262207
    },
    {
      "epoch": 0.24113821138211383,
      "step": 4449,
      "training_loss": 4.471888542175293
    },
    {
      "epoch": 0.24119241192411925,
      "step": 4450,
      "training_loss": 7.626521587371826
    },
    {
      "epoch": 0.24124661246612467,
      "step": 4451,
      "training_loss": 7.541464805603027
    },
    {
      "epoch": 0.24130081300813008,
      "grad_norm": 19.522865295410156,
      "learning_rate": 1e-05,
      "loss": 6.2777,
      "step": 4452
    },
    {
      "epoch": 0.24130081300813008,
      "step": 4452,
      "training_loss": 6.877355098724365
    },
    {
      "epoch": 0.2413550135501355,
      "step": 4453,
      "training_loss": 6.489975929260254
    },
    {
      "epoch": 0.24140921409214092,
      "step": 4454,
      "training_loss": 7.96221399307251
    },
    {
      "epoch": 0.24146341463414633,
      "step": 4455,
      "training_loss": 6.278899669647217
    },
    {
      "epoch": 0.24151761517615175,
      "grad_norm": 22.807878494262695,
      "learning_rate": 1e-05,
      "loss": 6.9021,
      "step": 4456
    },
    {
      "epoch": 0.24151761517615175,
      "step": 4456,
      "training_loss": 6.624127388000488
    },
    {
      "epoch": 0.24157181571815717,
      "step": 4457,
      "training_loss": 7.170284748077393
    },
    {
      "epoch": 0.2416260162601626,
      "step": 4458,
      "training_loss": 7.182557582855225
    },
    {
      "epoch": 0.24168021680216803,
      "step": 4459,
      "training_loss": 7.689818382263184
    },
    {
      "epoch": 0.24173441734417345,
      "grad_norm": 27.096721649169922,
      "learning_rate": 1e-05,
      "loss": 7.1667,
      "step": 4460
    },
    {
      "epoch": 0.24173441734417345,
      "step": 4460,
      "training_loss": 8.005291938781738
    },
    {
      "epoch": 0.24178861788617886,
      "step": 4461,
      "training_loss": 6.566344738006592
    },
    {
      "epoch": 0.24184281842818428,
      "step": 4462,
      "training_loss": 6.099650859832764
    },
    {
      "epoch": 0.2418970189701897,
      "step": 4463,
      "training_loss": 5.755043983459473
    },
    {
      "epoch": 0.24195121951219511,
      "grad_norm": 18.951732635498047,
      "learning_rate": 1e-05,
      "loss": 6.6066,
      "step": 4464
    },
    {
      "epoch": 0.24195121951219511,
      "step": 4464,
      "training_loss": 7.081868648529053
    },
    {
      "epoch": 0.24200542005420053,
      "step": 4465,
      "training_loss": 5.666322231292725
    },
    {
      "epoch": 0.24205962059620595,
      "step": 4466,
      "training_loss": 6.484644412994385
    },
    {
      "epoch": 0.2421138211382114,
      "step": 4467,
      "training_loss": 7.312374114990234
    },
    {
      "epoch": 0.2421680216802168,
      "grad_norm": 27.77928352355957,
      "learning_rate": 1e-05,
      "loss": 6.6363,
      "step": 4468
    },
    {
      "epoch": 0.2421680216802168,
      "step": 4468,
      "training_loss": 7.13197135925293
    },
    {
      "epoch": 0.24222222222222223,
      "step": 4469,
      "training_loss": 7.156121730804443
    },
    {
      "epoch": 0.24227642276422764,
      "step": 4470,
      "training_loss": 6.726797103881836
    },
    {
      "epoch": 0.24233062330623306,
      "step": 4471,
      "training_loss": 7.277840614318848
    },
    {
      "epoch": 0.24238482384823848,
      "grad_norm": 18.644439697265625,
      "learning_rate": 1e-05,
      "loss": 7.0732,
      "step": 4472
    },
    {
      "epoch": 0.24238482384823848,
      "step": 4472,
      "training_loss": 6.4107513427734375
    },
    {
      "epoch": 0.2424390243902439,
      "step": 4473,
      "training_loss": 5.821068286895752
    },
    {
      "epoch": 0.2424932249322493,
      "step": 4474,
      "training_loss": 6.864126682281494
    },
    {
      "epoch": 0.24254742547425473,
      "step": 4475,
      "training_loss": 5.94859504699707
    },
    {
      "epoch": 0.24260162601626017,
      "grad_norm": 61.698448181152344,
      "learning_rate": 1e-05,
      "loss": 6.2611,
      "step": 4476
    },
    {
      "epoch": 0.24260162601626017,
      "step": 4476,
      "training_loss": 4.097338676452637
    },
    {
      "epoch": 0.2426558265582656,
      "step": 4477,
      "training_loss": 5.324566841125488
    },
    {
      "epoch": 0.242710027100271,
      "step": 4478,
      "training_loss": 8.010733604431152
    },
    {
      "epoch": 0.24276422764227643,
      "step": 4479,
      "training_loss": 7.303486347198486
    },
    {
      "epoch": 0.24281842818428184,
      "grad_norm": 18.445661544799805,
      "learning_rate": 1e-05,
      "loss": 6.184,
      "step": 4480
    },
    {
      "epoch": 0.24281842818428184,
      "step": 4480,
      "training_loss": 6.996105194091797
    },
    {
      "epoch": 0.24287262872628726,
      "step": 4481,
      "training_loss": 6.765478610992432
    },
    {
      "epoch": 0.24292682926829268,
      "step": 4482,
      "training_loss": 6.995742321014404
    },
    {
      "epoch": 0.2429810298102981,
      "step": 4483,
      "training_loss": 6.537970066070557
    },
    {
      "epoch": 0.2430352303523035,
      "grad_norm": 21.474321365356445,
      "learning_rate": 1e-05,
      "loss": 6.8238,
      "step": 4484
    },
    {
      "epoch": 0.2430352303523035,
      "step": 4484,
      "training_loss": 6.791171550750732
    },
    {
      "epoch": 0.24308943089430896,
      "step": 4485,
      "training_loss": 6.430607318878174
    },
    {
      "epoch": 0.24314363143631437,
      "step": 4486,
      "training_loss": 7.579246997833252
    },
    {
      "epoch": 0.2431978319783198,
      "step": 4487,
      "training_loss": 7.821079254150391
    },
    {
      "epoch": 0.2432520325203252,
      "grad_norm": 28.302509307861328,
      "learning_rate": 1e-05,
      "loss": 7.1555,
      "step": 4488
    },
    {
      "epoch": 0.2432520325203252,
      "step": 4488,
      "training_loss": 7.388105392456055
    },
    {
      "epoch": 0.24330623306233062,
      "step": 4489,
      "training_loss": 4.482167720794678
    },
    {
      "epoch": 0.24336043360433604,
      "step": 4490,
      "training_loss": 6.577086925506592
    },
    {
      "epoch": 0.24341463414634146,
      "step": 4491,
      "training_loss": 7.3038434982299805
    },
    {
      "epoch": 0.24346883468834687,
      "grad_norm": 28.162363052368164,
      "learning_rate": 1e-05,
      "loss": 6.4378,
      "step": 4492
    },
    {
      "epoch": 0.24346883468834687,
      "step": 4492,
      "training_loss": 7.181783199310303
    },
    {
      "epoch": 0.2435230352303523,
      "step": 4493,
      "training_loss": 5.787737846374512
    },
    {
      "epoch": 0.24357723577235774,
      "step": 4494,
      "training_loss": 5.429065227508545
    },
    {
      "epoch": 0.24363143631436315,
      "step": 4495,
      "training_loss": 8.203770637512207
    },
    {
      "epoch": 0.24368563685636857,
      "grad_norm": 16.483322143554688,
      "learning_rate": 1e-05,
      "loss": 6.6506,
      "step": 4496
    },
    {
      "epoch": 0.24368563685636857,
      "step": 4496,
      "training_loss": 5.569873332977295
    },
    {
      "epoch": 0.243739837398374,
      "step": 4497,
      "training_loss": 6.18034553527832
    },
    {
      "epoch": 0.2437940379403794,
      "step": 4498,
      "training_loss": 7.83945894241333
    },
    {
      "epoch": 0.24384823848238482,
      "step": 4499,
      "training_loss": 7.164546012878418
    },
    {
      "epoch": 0.24390243902439024,
      "grad_norm": 41.87200927734375,
      "learning_rate": 1e-05,
      "loss": 6.6886,
      "step": 4500
    },
    {
      "epoch": 0.24390243902439024,
      "step": 4500,
      "training_loss": 7.404544830322266
    },
    {
      "epoch": 0.24395663956639566,
      "step": 4501,
      "training_loss": 7.072329998016357
    },
    {
      "epoch": 0.24401084010840107,
      "step": 4502,
      "training_loss": 5.771177291870117
    },
    {
      "epoch": 0.24406504065040652,
      "step": 4503,
      "training_loss": 8.163029670715332
    },
    {
      "epoch": 0.24411924119241193,
      "grad_norm": 36.2460823059082,
      "learning_rate": 1e-05,
      "loss": 7.1028,
      "step": 4504
    },
    {
      "epoch": 0.24411924119241193,
      "step": 4504,
      "training_loss": 5.552910804748535
    },
    {
      "epoch": 0.24417344173441735,
      "step": 4505,
      "training_loss": 6.729031562805176
    },
    {
      "epoch": 0.24422764227642277,
      "step": 4506,
      "training_loss": 7.443583011627197
    },
    {
      "epoch": 0.24428184281842819,
      "step": 4507,
      "training_loss": 7.498842716217041
    },
    {
      "epoch": 0.2443360433604336,
      "grad_norm": 20.700660705566406,
      "learning_rate": 1e-05,
      "loss": 6.8061,
      "step": 4508
    },
    {
      "epoch": 0.2443360433604336,
      "step": 4508,
      "training_loss": 7.813329219818115
    },
    {
      "epoch": 0.24439024390243902,
      "step": 4509,
      "training_loss": 5.609825611114502
    },
    {
      "epoch": 0.24444444444444444,
      "step": 4510,
      "training_loss": 6.069540500640869
    },
    {
      "epoch": 0.24449864498644985,
      "step": 4511,
      "training_loss": 7.16575288772583
    },
    {
      "epoch": 0.2445528455284553,
      "grad_norm": 21.64506721496582,
      "learning_rate": 1e-05,
      "loss": 6.6646,
      "step": 4512
    },
    {
      "epoch": 0.2445528455284553,
      "step": 4512,
      "training_loss": 5.135182857513428
    },
    {
      "epoch": 0.24460704607046072,
      "step": 4513,
      "training_loss": 6.346367359161377
    },
    {
      "epoch": 0.24466124661246613,
      "step": 4514,
      "training_loss": 7.509317398071289
    },
    {
      "epoch": 0.24471544715447155,
      "step": 4515,
      "training_loss": 6.550971508026123
    },
    {
      "epoch": 0.24476964769647697,
      "grad_norm": 20.282211303710938,
      "learning_rate": 1e-05,
      "loss": 6.3855,
      "step": 4516
    },
    {
      "epoch": 0.24476964769647697,
      "step": 4516,
      "training_loss": 8.341081619262695
    },
    {
      "epoch": 0.24482384823848238,
      "step": 4517,
      "training_loss": 6.284140110015869
    },
    {
      "epoch": 0.2448780487804878,
      "step": 4518,
      "training_loss": 4.640686511993408
    },
    {
      "epoch": 0.24493224932249322,
      "step": 4519,
      "training_loss": 4.598402976989746
    },
    {
      "epoch": 0.24498644986449863,
      "grad_norm": 26.01274299621582,
      "learning_rate": 1e-05,
      "loss": 5.9661,
      "step": 4520
    },
    {
      "epoch": 0.24498644986449863,
      "step": 4520,
      "training_loss": 6.042552471160889
    },
    {
      "epoch": 0.24504065040650405,
      "step": 4521,
      "training_loss": 6.234198570251465
    },
    {
      "epoch": 0.2450948509485095,
      "step": 4522,
      "training_loss": 7.533639907836914
    },
    {
      "epoch": 0.2451490514905149,
      "step": 4523,
      "training_loss": 7.145086288452148
    },
    {
      "epoch": 0.24520325203252033,
      "grad_norm": 16.01547622680664,
      "learning_rate": 1e-05,
      "loss": 6.7389,
      "step": 4524
    },
    {
      "epoch": 0.24520325203252033,
      "step": 4524,
      "training_loss": 3.889970064163208
    },
    {
      "epoch": 0.24525745257452575,
      "step": 4525,
      "training_loss": 6.775411128997803
    },
    {
      "epoch": 0.24531165311653116,
      "step": 4526,
      "training_loss": 7.044589996337891
    },
    {
      "epoch": 0.24536585365853658,
      "step": 4527,
      "training_loss": 7.3013434410095215
    },
    {
      "epoch": 0.245420054200542,
      "grad_norm": 15.554230690002441,
      "learning_rate": 1e-05,
      "loss": 6.2528,
      "step": 4528
    },
    {
      "epoch": 0.245420054200542,
      "step": 4528,
      "training_loss": 6.514624118804932
    },
    {
      "epoch": 0.24547425474254742,
      "step": 4529,
      "training_loss": 6.318849086761475
    },
    {
      "epoch": 0.24552845528455283,
      "step": 4530,
      "training_loss": 7.1814374923706055
    },
    {
      "epoch": 0.24558265582655828,
      "step": 4531,
      "training_loss": 7.397181987762451
    },
    {
      "epoch": 0.2456368563685637,
      "grad_norm": 16.00885009765625,
      "learning_rate": 1e-05,
      "loss": 6.853,
      "step": 4532
    },
    {
      "epoch": 0.2456368563685637,
      "step": 4532,
      "training_loss": 6.759978294372559
    },
    {
      "epoch": 0.2456910569105691,
      "step": 4533,
      "training_loss": 6.4628682136535645
    },
    {
      "epoch": 0.24574525745257453,
      "step": 4534,
      "training_loss": 5.1588969230651855
    },
    {
      "epoch": 0.24579945799457995,
      "step": 4535,
      "training_loss": 6.574718475341797
    },
    {
      "epoch": 0.24585365853658536,
      "grad_norm": 15.954670906066895,
      "learning_rate": 1e-05,
      "loss": 6.2391,
      "step": 4536
    },
    {
      "epoch": 0.24585365853658536,
      "step": 4536,
      "training_loss": 7.6104326248168945
    },
    {
      "epoch": 0.24590785907859078,
      "step": 4537,
      "training_loss": 7.137988090515137
    },
    {
      "epoch": 0.2459620596205962,
      "step": 4538,
      "training_loss": 7.186797142028809
    },
    {
      "epoch": 0.2460162601626016,
      "step": 4539,
      "training_loss": 7.093160629272461
    },
    {
      "epoch": 0.24607046070460706,
      "grad_norm": 22.790943145751953,
      "learning_rate": 1e-05,
      "loss": 7.2571,
      "step": 4540
    },
    {
      "epoch": 0.24607046070460706,
      "step": 4540,
      "training_loss": 7.440145969390869
    },
    {
      "epoch": 0.24612466124661248,
      "step": 4541,
      "training_loss": 6.835419178009033
    },
    {
      "epoch": 0.2461788617886179,
      "step": 4542,
      "training_loss": 7.7702741622924805
    },
    {
      "epoch": 0.2462330623306233,
      "step": 4543,
      "training_loss": 6.990754127502441
    },
    {
      "epoch": 0.24628726287262873,
      "grad_norm": 20.287410736083984,
      "learning_rate": 1e-05,
      "loss": 7.2591,
      "step": 4544
    },
    {
      "epoch": 0.24628726287262873,
      "step": 4544,
      "training_loss": 7.494838714599609
    },
    {
      "epoch": 0.24634146341463414,
      "step": 4545,
      "training_loss": 5.750039577484131
    },
    {
      "epoch": 0.24639566395663956,
      "step": 4546,
      "training_loss": 7.869279861450195
    },
    {
      "epoch": 0.24644986449864498,
      "step": 4547,
      "training_loss": 5.887230396270752
    },
    {
      "epoch": 0.2465040650406504,
      "grad_norm": 18.272802352905273,
      "learning_rate": 1e-05,
      "loss": 6.7503,
      "step": 4548
    },
    {
      "epoch": 0.2465040650406504,
      "step": 4548,
      "training_loss": 7.684829235076904
    },
    {
      "epoch": 0.24655826558265584,
      "step": 4549,
      "training_loss": 5.7760748863220215
    },
    {
      "epoch": 0.24661246612466126,
      "step": 4550,
      "training_loss": 5.750380039215088
    },
    {
      "epoch": 0.24666666666666667,
      "step": 4551,
      "training_loss": 6.819571495056152
    },
    {
      "epoch": 0.2467208672086721,
      "grad_norm": 24.018138885498047,
      "learning_rate": 1e-05,
      "loss": 6.5077,
      "step": 4552
    },
    {
      "epoch": 0.2467208672086721,
      "step": 4552,
      "training_loss": 6.890917778015137
    },
    {
      "epoch": 0.2467750677506775,
      "step": 4553,
      "training_loss": 7.182192802429199
    },
    {
      "epoch": 0.24682926829268292,
      "step": 4554,
      "training_loss": 6.704033374786377
    },
    {
      "epoch": 0.24688346883468834,
      "step": 4555,
      "training_loss": 6.5764007568359375
    },
    {
      "epoch": 0.24693766937669376,
      "grad_norm": 22.37562370300293,
      "learning_rate": 1e-05,
      "loss": 6.8384,
      "step": 4556
    },
    {
      "epoch": 0.24693766937669376,
      "step": 4556,
      "training_loss": 7.25836706161499
    },
    {
      "epoch": 0.24699186991869918,
      "step": 4557,
      "training_loss": 6.968473434448242
    },
    {
      "epoch": 0.24704607046070462,
      "step": 4558,
      "training_loss": 7.078373908996582
    },
    {
      "epoch": 0.24710027100271004,
      "step": 4559,
      "training_loss": 7.788518905639648
    },
    {
      "epoch": 0.24715447154471545,
      "grad_norm": 23.589799880981445,
      "learning_rate": 1e-05,
      "loss": 7.2734,
      "step": 4560
    },
    {
      "epoch": 0.24715447154471545,
      "step": 4560,
      "training_loss": 6.329687595367432
    },
    {
      "epoch": 0.24720867208672087,
      "step": 4561,
      "training_loss": 6.079620361328125
    },
    {
      "epoch": 0.2472628726287263,
      "step": 4562,
      "training_loss": 7.68562650680542
    },
    {
      "epoch": 0.2473170731707317,
      "step": 4563,
      "training_loss": 6.342005729675293
    },
    {
      "epoch": 0.24737127371273712,
      "grad_norm": 30.73809814453125,
      "learning_rate": 1e-05,
      "loss": 6.6092,
      "step": 4564
    },
    {
      "epoch": 0.24737127371273712,
      "step": 4564,
      "training_loss": 7.128328323364258
    },
    {
      "epoch": 0.24742547425474254,
      "step": 4565,
      "training_loss": 7.136937618255615
    },
    {
      "epoch": 0.24747967479674796,
      "step": 4566,
      "training_loss": 7.031460762023926
    },
    {
      "epoch": 0.2475338753387534,
      "step": 4567,
      "training_loss": 7.506978988647461
    },
    {
      "epoch": 0.24758807588075882,
      "grad_norm": 16.014278411865234,
      "learning_rate": 1e-05,
      "loss": 7.2009,
      "step": 4568
    },
    {
      "epoch": 0.24758807588075882,
      "step": 4568,
      "training_loss": 6.431039333343506
    },
    {
      "epoch": 0.24764227642276423,
      "step": 4569,
      "training_loss": 6.409792423248291
    },
    {
      "epoch": 0.24769647696476965,
      "step": 4570,
      "training_loss": 6.8296380043029785
    },
    {
      "epoch": 0.24775067750677507,
      "step": 4571,
      "training_loss": 6.548471927642822
    },
    {
      "epoch": 0.24780487804878049,
      "grad_norm": 19.125904083251953,
      "learning_rate": 1e-05,
      "loss": 6.5547,
      "step": 4572
    },
    {
      "epoch": 0.24780487804878049,
      "step": 4572,
      "training_loss": 6.9530930519104
    },
    {
      "epoch": 0.2478590785907859,
      "step": 4573,
      "training_loss": 7.27553129196167
    },
    {
      "epoch": 0.24791327913279132,
      "step": 4574,
      "training_loss": 6.520015716552734
    },
    {
      "epoch": 0.24796747967479674,
      "step": 4575,
      "training_loss": 7.90704345703125
    },
    {
      "epoch": 0.24802168021680218,
      "grad_norm": 17.368438720703125,
      "learning_rate": 1e-05,
      "loss": 7.1639,
      "step": 4576
    },
    {
      "epoch": 0.24802168021680218,
      "step": 4576,
      "training_loss": 6.481507778167725
    },
    {
      "epoch": 0.2480758807588076,
      "step": 4577,
      "training_loss": 8.354305267333984
    },
    {
      "epoch": 0.24813008130081302,
      "step": 4578,
      "training_loss": 5.429924488067627
    },
    {
      "epoch": 0.24818428184281843,
      "step": 4579,
      "training_loss": 7.604311466217041
    },
    {
      "epoch": 0.24823848238482385,
      "grad_norm": 20.3966007232666,
      "learning_rate": 1e-05,
      "loss": 6.9675,
      "step": 4580
    },
    {
      "epoch": 0.24823848238482385,
      "step": 4580,
      "training_loss": 6.9965291023254395
    },
    {
      "epoch": 0.24829268292682927,
      "step": 4581,
      "training_loss": 8.372895240783691
    },
    {
      "epoch": 0.24834688346883468,
      "step": 4582,
      "training_loss": 6.701887130737305
    },
    {
      "epoch": 0.2484010840108401,
      "step": 4583,
      "training_loss": 4.441675186157227
    },
    {
      "epoch": 0.24845528455284552,
      "grad_norm": 31.714881896972656,
      "learning_rate": 1e-05,
      "loss": 6.6282,
      "step": 4584
    },
    {
      "epoch": 0.24845528455284552,
      "step": 4584,
      "training_loss": 5.69799280166626
    },
    {
      "epoch": 0.24850948509485093,
      "step": 4585,
      "training_loss": 4.320451259613037
    },
    {
      "epoch": 0.24856368563685638,
      "step": 4586,
      "training_loss": 7.557811260223389
    },
    {
      "epoch": 0.2486178861788618,
      "step": 4587,
      "training_loss": 6.94527006149292
    },
    {
      "epoch": 0.2486720867208672,
      "grad_norm": 21.169239044189453,
      "learning_rate": 1e-05,
      "loss": 6.1304,
      "step": 4588
    },
    {
      "epoch": 0.2486720867208672,
      "step": 4588,
      "training_loss": 7.605837821960449
    },
    {
      "epoch": 0.24872628726287263,
      "step": 4589,
      "training_loss": 7.093896389007568
    },
    {
      "epoch": 0.24878048780487805,
      "step": 4590,
      "training_loss": 8.130249977111816
    },
    {
      "epoch": 0.24883468834688346,
      "step": 4591,
      "training_loss": 7.392412185668945
    },
    {
      "epoch": 0.24888888888888888,
      "grad_norm": 27.44292449951172,
      "learning_rate": 1e-05,
      "loss": 7.5556,
      "step": 4592
    },
    {
      "epoch": 0.24888888888888888,
      "step": 4592,
      "training_loss": 4.4372382164001465
    },
    {
      "epoch": 0.2489430894308943,
      "step": 4593,
      "training_loss": 7.897927284240723
    },
    {
      "epoch": 0.24899728997289972,
      "step": 4594,
      "training_loss": 7.018857955932617
    },
    {
      "epoch": 0.24905149051490516,
      "step": 4595,
      "training_loss": 7.199654579162598
    },
    {
      "epoch": 0.24910569105691058,
      "grad_norm": 24.1024112701416,
      "learning_rate": 1e-05,
      "loss": 6.6384,
      "step": 4596
    },
    {
      "epoch": 0.24910569105691058,
      "step": 4596,
      "training_loss": 7.103385925292969
    },
    {
      "epoch": 0.249159891598916,
      "step": 4597,
      "training_loss": 7.209924221038818
    },
    {
      "epoch": 0.2492140921409214,
      "step": 4598,
      "training_loss": 7.0638203620910645
    },
    {
      "epoch": 0.24926829268292683,
      "step": 4599,
      "training_loss": 7.41127347946167
    },
    {
      "epoch": 0.24932249322493225,
      "grad_norm": 22.770994186401367,
      "learning_rate": 1e-05,
      "loss": 7.1971,
      "step": 4600
    },
    {
      "epoch": 0.24932249322493225,
      "step": 4600,
      "training_loss": 6.04990816116333
    },
    {
      "epoch": 0.24937669376693766,
      "step": 4601,
      "training_loss": 4.893494606018066
    },
    {
      "epoch": 0.24943089430894308,
      "step": 4602,
      "training_loss": 5.409361839294434
    },
    {
      "epoch": 0.2494850948509485,
      "step": 4603,
      "training_loss": 6.867082118988037
    },
    {
      "epoch": 0.24953929539295394,
      "grad_norm": 18.806711196899414,
      "learning_rate": 1e-05,
      "loss": 5.805,
      "step": 4604
    },
    {
      "epoch": 0.24953929539295394,
      "step": 4604,
      "training_loss": 7.8328022956848145
    },
    {
      "epoch": 0.24959349593495936,
      "step": 4605,
      "training_loss": 7.59335994720459
    },
    {
      "epoch": 0.24964769647696478,
      "step": 4606,
      "training_loss": 6.457448482513428
    },
    {
      "epoch": 0.2497018970189702,
      "step": 4607,
      "training_loss": 7.907302379608154
    },
    {
      "epoch": 0.2497560975609756,
      "grad_norm": 41.81088638305664,
      "learning_rate": 1e-05,
      "loss": 7.4477,
      "step": 4608
    },
    {
      "epoch": 0.2497560975609756,
      "step": 4608,
      "training_loss": 6.2744669914245605
    },
    {
      "epoch": 0.24981029810298103,
      "step": 4609,
      "training_loss": 7.387753963470459
    },
    {
      "epoch": 0.24986449864498644,
      "step": 4610,
      "training_loss": 6.343862056732178
    },
    {
      "epoch": 0.24991869918699186,
      "step": 4611,
      "training_loss": 7.204009532928467
    },
    {
      "epoch": 0.24997289972899728,
      "grad_norm": 37.41841125488281,
      "learning_rate": 1e-05,
      "loss": 6.8025,
      "step": 4612
    },
    {
      "epoch": 0.24997289972899728,
      "step": 4612,
      "training_loss": 7.245847225189209
    },
    {
      "epoch": 0.2500271002710027,
      "step": 4613,
      "training_loss": 6.64047384262085
    },
    {
      "epoch": 0.2500813008130081,
      "step": 4614,
      "training_loss": 8.074647903442383
    },
    {
      "epoch": 0.25013550135501356,
      "step": 4615,
      "training_loss": 6.702983379364014
    },
    {
      "epoch": 0.25018970189701895,
      "grad_norm": 23.12486457824707,
      "learning_rate": 1e-05,
      "loss": 7.166,
      "step": 4616
    },
    {
      "epoch": 0.25018970189701895,
      "step": 4616,
      "training_loss": 6.4627685546875
    },
    {
      "epoch": 0.2502439024390244,
      "step": 4617,
      "training_loss": 5.977531433105469
    },
    {
      "epoch": 0.25029810298102984,
      "step": 4618,
      "training_loss": 7.506042003631592
    },
    {
      "epoch": 0.2503523035230352,
      "step": 4619,
      "training_loss": 6.419808387756348
    },
    {
      "epoch": 0.25040650406504067,
      "grad_norm": 82.96723175048828,
      "learning_rate": 1e-05,
      "loss": 6.5915,
      "step": 4620
    },
    {
      "epoch": 0.25040650406504067,
      "step": 4620,
      "training_loss": 6.733415603637695
    },
    {
      "epoch": 0.25046070460704606,
      "step": 4621,
      "training_loss": 6.802484035491943
    },
    {
      "epoch": 0.2505149051490515,
      "step": 4622,
      "training_loss": 8.205282211303711
    },
    {
      "epoch": 0.2505691056910569,
      "step": 4623,
      "training_loss": 5.581820964813232
    },
    {
      "epoch": 0.25062330623306234,
      "grad_norm": 26.381759643554688,
      "learning_rate": 1e-05,
      "loss": 6.8308,
      "step": 4624
    },
    {
      "epoch": 0.25062330623306234,
      "step": 4624,
      "training_loss": 4.65245246887207
    },
    {
      "epoch": 0.2506775067750677,
      "step": 4625,
      "training_loss": 7.4611663818359375
    },
    {
      "epoch": 0.25073170731707317,
      "step": 4626,
      "training_loss": 5.341056823730469
    },
    {
      "epoch": 0.2507859078590786,
      "step": 4627,
      "training_loss": 5.817070960998535
    },
    {
      "epoch": 0.250840108401084,
      "grad_norm": 20.74981117248535,
      "learning_rate": 1e-05,
      "loss": 5.8179,
      "step": 4628
    },
    {
      "epoch": 0.250840108401084,
      "step": 4628,
      "training_loss": 6.704030513763428
    },
    {
      "epoch": 0.25089430894308945,
      "step": 4629,
      "training_loss": 6.240573406219482
    },
    {
      "epoch": 0.25094850948509484,
      "step": 4630,
      "training_loss": 6.458856105804443
    },
    {
      "epoch": 0.2510027100271003,
      "step": 4631,
      "training_loss": 7.970684051513672
    },
    {
      "epoch": 0.2510569105691057,
      "grad_norm": 37.188724517822266,
      "learning_rate": 1e-05,
      "loss": 6.8435,
      "step": 4632
    },
    {
      "epoch": 0.2510569105691057,
      "step": 4632,
      "training_loss": 6.27128791809082
    },
    {
      "epoch": 0.2511111111111111,
      "step": 4633,
      "training_loss": 7.226425647735596
    },
    {
      "epoch": 0.2511653116531165,
      "step": 4634,
      "training_loss": 7.015195846557617
    },
    {
      "epoch": 0.25121951219512195,
      "step": 4635,
      "training_loss": 7.166135311126709
    },
    {
      "epoch": 0.2512737127371274,
      "grad_norm": 40.6094970703125,
      "learning_rate": 1e-05,
      "loss": 6.9198,
      "step": 4636
    },
    {
      "epoch": 0.2512737127371274,
      "step": 4636,
      "training_loss": 6.859950065612793
    },
    {
      "epoch": 0.2513279132791328,
      "step": 4637,
      "training_loss": 8.376907348632812
    },
    {
      "epoch": 0.25138211382113823,
      "step": 4638,
      "training_loss": 7.321602821350098
    },
    {
      "epoch": 0.2514363143631436,
      "step": 4639,
      "training_loss": 7.25193452835083
    },
    {
      "epoch": 0.25149051490514907,
      "grad_norm": 21.85409927368164,
      "learning_rate": 1e-05,
      "loss": 7.4526,
      "step": 4640
    },
    {
      "epoch": 0.25149051490514907,
      "step": 4640,
      "training_loss": 6.823029041290283
    },
    {
      "epoch": 0.25154471544715445,
      "step": 4641,
      "training_loss": 6.952274322509766
    },
    {
      "epoch": 0.2515989159891599,
      "step": 4642,
      "training_loss": 6.236060619354248
    },
    {
      "epoch": 0.2516531165311653,
      "step": 4643,
      "training_loss": 6.918301582336426
    },
    {
      "epoch": 0.25170731707317073,
      "grad_norm": 24.01155662536621,
      "learning_rate": 1e-05,
      "loss": 6.7324,
      "step": 4644
    },
    {
      "epoch": 0.25170731707317073,
      "step": 4644,
      "training_loss": 6.819944381713867
    },
    {
      "epoch": 0.2517615176151762,
      "step": 4645,
      "training_loss": 7.387278079986572
    },
    {
      "epoch": 0.25181571815718157,
      "step": 4646,
      "training_loss": 7.952418327331543
    },
    {
      "epoch": 0.251869918699187,
      "step": 4647,
      "training_loss": 7.534212112426758
    },
    {
      "epoch": 0.2519241192411924,
      "grad_norm": 26.80008316040039,
      "learning_rate": 1e-05,
      "loss": 7.4235,
      "step": 4648
    },
    {
      "epoch": 0.2519241192411924,
      "step": 4648,
      "training_loss": 5.65468168258667
    },
    {
      "epoch": 0.25197831978319785,
      "step": 4649,
      "training_loss": 4.691615104675293
    },
    {
      "epoch": 0.25203252032520324,
      "step": 4650,
      "training_loss": 8.580548286437988
    },
    {
      "epoch": 0.2520867208672087,
      "step": 4651,
      "training_loss": 7.169849872589111
    },
    {
      "epoch": 0.25214092140921407,
      "grad_norm": 27.231306076049805,
      "learning_rate": 1e-05,
      "loss": 6.5242,
      "step": 4652
    },
    {
      "epoch": 0.25214092140921407,
      "step": 4652,
      "training_loss": 6.833973407745361
    },
    {
      "epoch": 0.2521951219512195,
      "step": 4653,
      "training_loss": 5.172091960906982
    },
    {
      "epoch": 0.25224932249322496,
      "step": 4654,
      "training_loss": 6.423611164093018
    },
    {
      "epoch": 0.25230352303523035,
      "step": 4655,
      "training_loss": 7.619742393493652
    },
    {
      "epoch": 0.2523577235772358,
      "grad_norm": 36.05659484863281,
      "learning_rate": 1e-05,
      "loss": 6.5124,
      "step": 4656
    },
    {
      "epoch": 0.2523577235772358,
      "step": 4656,
      "training_loss": 6.813578128814697
    },
    {
      "epoch": 0.2524119241192412,
      "step": 4657,
      "training_loss": 7.633707523345947
    },
    {
      "epoch": 0.2524661246612466,
      "step": 4658,
      "training_loss": 6.711893558502197
    },
    {
      "epoch": 0.252520325203252,
      "step": 4659,
      "training_loss": 6.7235236167907715
    },
    {
      "epoch": 0.25257452574525746,
      "grad_norm": 35.05952835083008,
      "learning_rate": 1e-05,
      "loss": 6.9707,
      "step": 4660
    },
    {
      "epoch": 0.25257452574525746,
      "step": 4660,
      "training_loss": 6.633430480957031
    },
    {
      "epoch": 0.25262872628726285,
      "step": 4661,
      "training_loss": 5.657050609588623
    },
    {
      "epoch": 0.2526829268292683,
      "step": 4662,
      "training_loss": 7.177713394165039
    },
    {
      "epoch": 0.25273712737127374,
      "step": 4663,
      "training_loss": 7.371050834655762
    },
    {
      "epoch": 0.25279132791327913,
      "grad_norm": 16.79178237915039,
      "learning_rate": 1e-05,
      "loss": 6.7098,
      "step": 4664
    },
    {
      "epoch": 0.25279132791327913,
      "step": 4664,
      "training_loss": 7.751221656799316
    },
    {
      "epoch": 0.2528455284552846,
      "step": 4665,
      "training_loss": 6.124966621398926
    },
    {
      "epoch": 0.25289972899728996,
      "step": 4666,
      "training_loss": 6.888044834136963
    },
    {
      "epoch": 0.2529539295392954,
      "step": 4667,
      "training_loss": 7.544567584991455
    },
    {
      "epoch": 0.2530081300813008,
      "grad_norm": 32.01638412475586,
      "learning_rate": 1e-05,
      "loss": 7.0772,
      "step": 4668
    },
    {
      "epoch": 0.2530081300813008,
      "step": 4668,
      "training_loss": 6.50324010848999
    },
    {
      "epoch": 0.25306233062330624,
      "step": 4669,
      "training_loss": 11.355605125427246
    },
    {
      "epoch": 0.25311653116531163,
      "step": 4670,
      "training_loss": 7.093403339385986
    },
    {
      "epoch": 0.2531707317073171,
      "step": 4671,
      "training_loss": 7.122182846069336
    },
    {
      "epoch": 0.2532249322493225,
      "grad_norm": 23.703500747680664,
      "learning_rate": 1e-05,
      "loss": 8.0186,
      "step": 4672
    },
    {
      "epoch": 0.2532249322493225,
      "step": 4672,
      "training_loss": 7.069399356842041
    },
    {
      "epoch": 0.2532791327913279,
      "step": 4673,
      "training_loss": 6.60746431350708
    },
    {
      "epoch": 0.25333333333333335,
      "step": 4674,
      "training_loss": 7.07022762298584
    },
    {
      "epoch": 0.25338753387533874,
      "step": 4675,
      "training_loss": 7.068127632141113
    },
    {
      "epoch": 0.2534417344173442,
      "grad_norm": 22.649747848510742,
      "learning_rate": 1e-05,
      "loss": 6.9538,
      "step": 4676
    },
    {
      "epoch": 0.2534417344173442,
      "step": 4676,
      "training_loss": 6.763116359710693
    },
    {
      "epoch": 0.2534959349593496,
      "step": 4677,
      "training_loss": 7.2142133712768555
    },
    {
      "epoch": 0.253550135501355,
      "step": 4678,
      "training_loss": 6.372584819793701
    },
    {
      "epoch": 0.2536043360433604,
      "step": 4679,
      "training_loss": 7.094247341156006
    },
    {
      "epoch": 0.25365853658536586,
      "grad_norm": 19.366981506347656,
      "learning_rate": 1e-05,
      "loss": 6.861,
      "step": 4680
    },
    {
      "epoch": 0.25365853658536586,
      "step": 4680,
      "training_loss": 6.370521068572998
    },
    {
      "epoch": 0.25371273712737125,
      "step": 4681,
      "training_loss": 6.98710298538208
    },
    {
      "epoch": 0.2537669376693767,
      "step": 4682,
      "training_loss": 4.873260974884033
    },
    {
      "epoch": 0.25382113821138214,
      "step": 4683,
      "training_loss": 5.733206748962402
    },
    {
      "epoch": 0.2538753387533875,
      "grad_norm": 22.866430282592773,
      "learning_rate": 1e-05,
      "loss": 5.991,
      "step": 4684
    },
    {
      "epoch": 0.2538753387533875,
      "step": 4684,
      "training_loss": 6.733104228973389
    },
    {
      "epoch": 0.25392953929539297,
      "step": 4685,
      "training_loss": 9.011604309082031
    },
    {
      "epoch": 0.25398373983739836,
      "step": 4686,
      "training_loss": 6.0508904457092285
    },
    {
      "epoch": 0.2540379403794038,
      "step": 4687,
      "training_loss": 7.450942516326904
    },
    {
      "epoch": 0.2540921409214092,
      "grad_norm": 21.511890411376953,
      "learning_rate": 1e-05,
      "loss": 7.3116,
      "step": 4688
    },
    {
      "epoch": 0.2540921409214092,
      "step": 4688,
      "training_loss": 7.3003926277160645
    },
    {
      "epoch": 0.25414634146341464,
      "step": 4689,
      "training_loss": 6.508411407470703
    },
    {
      "epoch": 0.25420054200542,
      "step": 4690,
      "training_loss": 7.240006923675537
    },
    {
      "epoch": 0.25425474254742547,
      "step": 4691,
      "training_loss": 7.517923355102539
    },
    {
      "epoch": 0.2543089430894309,
      "grad_norm": 25.021516799926758,
      "learning_rate": 1e-05,
      "loss": 7.1417,
      "step": 4692
    },
    {
      "epoch": 0.2543089430894309,
      "step": 4692,
      "training_loss": 4.939881324768066
    },
    {
      "epoch": 0.2543631436314363,
      "step": 4693,
      "training_loss": 7.399707317352295
    },
    {
      "epoch": 0.25441734417344175,
      "step": 4694,
      "training_loss": 6.037477493286133
    },
    {
      "epoch": 0.25447154471544714,
      "step": 4695,
      "training_loss": 7.461540222167969
    },
    {
      "epoch": 0.2545257452574526,
      "grad_norm": 16.980327606201172,
      "learning_rate": 1e-05,
      "loss": 6.4597,
      "step": 4696
    },
    {
      "epoch": 0.2545257452574526,
      "step": 4696,
      "training_loss": 7.288647174835205
    },
    {
      "epoch": 0.254579945799458,
      "step": 4697,
      "training_loss": 7.834609031677246
    },
    {
      "epoch": 0.2546341463414634,
      "step": 4698,
      "training_loss": 7.004611015319824
    },
    {
      "epoch": 0.2546883468834688,
      "step": 4699,
      "training_loss": 5.878342151641846
    },
    {
      "epoch": 0.25474254742547425,
      "grad_norm": 23.178443908691406,
      "learning_rate": 1e-05,
      "loss": 7.0016,
      "step": 4700
    },
    {
      "epoch": 0.25474254742547425,
      "step": 4700,
      "training_loss": 6.801117420196533
    },
    {
      "epoch": 0.2547967479674797,
      "step": 4701,
      "training_loss": 7.014598369598389
    },
    {
      "epoch": 0.2548509485094851,
      "step": 4702,
      "training_loss": 6.798978328704834
    },
    {
      "epoch": 0.25490514905149053,
      "step": 4703,
      "training_loss": 6.22695779800415
    },
    {
      "epoch": 0.2549593495934959,
      "grad_norm": 20.569637298583984,
      "learning_rate": 1e-05,
      "loss": 6.7104,
      "step": 4704
    },
    {
      "epoch": 0.2549593495934959,
      "step": 4704,
      "training_loss": 7.608172416687012
    },
    {
      "epoch": 0.25501355013550137,
      "step": 4705,
      "training_loss": 7.65728759765625
    },
    {
      "epoch": 0.25506775067750675,
      "step": 4706,
      "training_loss": 6.621956825256348
    },
    {
      "epoch": 0.2551219512195122,
      "step": 4707,
      "training_loss": 6.784362316131592
    },
    {
      "epoch": 0.2551761517615176,
      "grad_norm": 33.51100158691406,
      "learning_rate": 1e-05,
      "loss": 7.1679,
      "step": 4708
    },
    {
      "epoch": 0.2551761517615176,
      "step": 4708,
      "training_loss": 6.368015766143799
    },
    {
      "epoch": 0.25523035230352303,
      "step": 4709,
      "training_loss": 7.996453285217285
    },
    {
      "epoch": 0.2552845528455285,
      "step": 4710,
      "training_loss": 6.792501926422119
    },
    {
      "epoch": 0.25533875338753387,
      "step": 4711,
      "training_loss": 5.792491436004639
    },
    {
      "epoch": 0.2553929539295393,
      "grad_norm": 29.033891677856445,
      "learning_rate": 1e-05,
      "loss": 6.7374,
      "step": 4712
    },
    {
      "epoch": 0.2553929539295393,
      "step": 4712,
      "training_loss": 7.160445213317871
    },
    {
      "epoch": 0.2554471544715447,
      "step": 4713,
      "training_loss": 6.366353511810303
    },
    {
      "epoch": 0.25550135501355015,
      "step": 4714,
      "training_loss": 7.013277530670166
    },
    {
      "epoch": 0.25555555555555554,
      "step": 4715,
      "training_loss": 7.063840389251709
    },
    {
      "epoch": 0.255609756097561,
      "grad_norm": 18.709552764892578,
      "learning_rate": 1e-05,
      "loss": 6.901,
      "step": 4716
    },
    {
      "epoch": 0.255609756097561,
      "step": 4716,
      "training_loss": 8.570260047912598
    },
    {
      "epoch": 0.25566395663956637,
      "step": 4717,
      "training_loss": 6.790302276611328
    },
    {
      "epoch": 0.2557181571815718,
      "step": 4718,
      "training_loss": 7.396368026733398
    },
    {
      "epoch": 0.25577235772357726,
      "step": 4719,
      "training_loss": 8.204614639282227
    },
    {
      "epoch": 0.25582655826558265,
      "grad_norm": 47.08232879638672,
      "learning_rate": 1e-05,
      "loss": 7.7404,
      "step": 4720
    },
    {
      "epoch": 0.25582655826558265,
      "step": 4720,
      "training_loss": 5.876017093658447
    },
    {
      "epoch": 0.2558807588075881,
      "step": 4721,
      "training_loss": 6.935137748718262
    },
    {
      "epoch": 0.2559349593495935,
      "step": 4722,
      "training_loss": 7.8045878410339355
    },
    {
      "epoch": 0.2559891598915989,
      "step": 4723,
      "training_loss": 6.336191654205322
    },
    {
      "epoch": 0.2560433604336043,
      "grad_norm": 20.57781219482422,
      "learning_rate": 1e-05,
      "loss": 6.738,
      "step": 4724
    },
    {
      "epoch": 0.2560433604336043,
      "step": 4724,
      "training_loss": 7.884202480316162
    },
    {
      "epoch": 0.25609756097560976,
      "step": 4725,
      "training_loss": 7.122417449951172
    },
    {
      "epoch": 0.25615176151761515,
      "step": 4726,
      "training_loss": 6.764018535614014
    },
    {
      "epoch": 0.2562059620596206,
      "step": 4727,
      "training_loss": 7.1979851722717285
    },
    {
      "epoch": 0.25626016260162604,
      "grad_norm": 16.583280563354492,
      "learning_rate": 1e-05,
      "loss": 7.2422,
      "step": 4728
    },
    {
      "epoch": 0.25626016260162604,
      "step": 4728,
      "training_loss": 7.041479587554932
    },
    {
      "epoch": 0.25631436314363143,
      "step": 4729,
      "training_loss": 7.053718090057373
    },
    {
      "epoch": 0.2563685636856369,
      "step": 4730,
      "training_loss": 6.89370584487915
    },
    {
      "epoch": 0.25642276422764226,
      "step": 4731,
      "training_loss": 7.866270542144775
    },
    {
      "epoch": 0.2564769647696477,
      "grad_norm": 30.00738525390625,
      "learning_rate": 1e-05,
      "loss": 7.2138,
      "step": 4732
    },
    {
      "epoch": 0.2564769647696477,
      "step": 4732,
      "training_loss": 4.692131519317627
    },
    {
      "epoch": 0.2565311653116531,
      "step": 4733,
      "training_loss": 8.504878997802734
    },
    {
      "epoch": 0.25658536585365854,
      "step": 4734,
      "training_loss": 8.180495262145996
    },
    {
      "epoch": 0.25663956639566393,
      "step": 4735,
      "training_loss": 6.914626598358154
    },
    {
      "epoch": 0.2566937669376694,
      "grad_norm": 23.290536880493164,
      "learning_rate": 1e-05,
      "loss": 7.073,
      "step": 4736
    },
    {
      "epoch": 0.2566937669376694,
      "step": 4736,
      "training_loss": 7.005627155303955
    },
    {
      "epoch": 0.2567479674796748,
      "step": 4737,
      "training_loss": 8.017702102661133
    },
    {
      "epoch": 0.2568021680216802,
      "step": 4738,
      "training_loss": 7.499058723449707
    },
    {
      "epoch": 0.25685636856368566,
      "step": 4739,
      "training_loss": 6.350644111633301
    },
    {
      "epoch": 0.25691056910569104,
      "grad_norm": 16.757465362548828,
      "learning_rate": 1e-05,
      "loss": 7.2183,
      "step": 4740
    },
    {
      "epoch": 0.25691056910569104,
      "step": 4740,
      "training_loss": 6.397661209106445
    },
    {
      "epoch": 0.2569647696476965,
      "step": 4741,
      "training_loss": 6.939577579498291
    },
    {
      "epoch": 0.2570189701897019,
      "step": 4742,
      "training_loss": 6.013177394866943
    },
    {
      "epoch": 0.2570731707317073,
      "step": 4743,
      "training_loss": 5.072497844696045
    },
    {
      "epoch": 0.2571273712737127,
      "grad_norm": 16.83380699157715,
      "learning_rate": 1e-05,
      "loss": 6.1057,
      "step": 4744
    },
    {
      "epoch": 0.2571273712737127,
      "step": 4744,
      "training_loss": 3.6848556995391846
    },
    {
      "epoch": 0.25718157181571816,
      "step": 4745,
      "training_loss": 5.314875602722168
    },
    {
      "epoch": 0.2572357723577236,
      "step": 4746,
      "training_loss": 7.135481357574463
    },
    {
      "epoch": 0.257289972899729,
      "step": 4747,
      "training_loss": 5.012190818786621
    },
    {
      "epoch": 0.25734417344173444,
      "grad_norm": 21.291763305664062,
      "learning_rate": 1e-05,
      "loss": 5.2869,
      "step": 4748
    },
    {
      "epoch": 0.25734417344173444,
      "step": 4748,
      "training_loss": 6.821893692016602
    },
    {
      "epoch": 0.2573983739837398,
      "step": 4749,
      "training_loss": 6.750768184661865
    },
    {
      "epoch": 0.25745257452574527,
      "step": 4750,
      "training_loss": 8.09833812713623
    },
    {
      "epoch": 0.25750677506775066,
      "step": 4751,
      "training_loss": 7.114133834838867
    },
    {
      "epoch": 0.2575609756097561,
      "grad_norm": 48.92202377319336,
      "learning_rate": 1e-05,
      "loss": 7.1963,
      "step": 4752
    },
    {
      "epoch": 0.2575609756097561,
      "step": 4752,
      "training_loss": 6.211601257324219
    },
    {
      "epoch": 0.2576151761517615,
      "step": 4753,
      "training_loss": 7.121237754821777
    },
    {
      "epoch": 0.25766937669376694,
      "step": 4754,
      "training_loss": 6.11627197265625
    },
    {
      "epoch": 0.2577235772357724,
      "step": 4755,
      "training_loss": 6.1874680519104
    },
    {
      "epoch": 0.2577777777777778,
      "grad_norm": 23.005651473999023,
      "learning_rate": 1e-05,
      "loss": 6.4091,
      "step": 4756
    },
    {
      "epoch": 0.2577777777777778,
      "step": 4756,
      "training_loss": 6.434803009033203
    },
    {
      "epoch": 0.2578319783197832,
      "step": 4757,
      "training_loss": 5.556206703186035
    },
    {
      "epoch": 0.2578861788617886,
      "step": 4758,
      "training_loss": 4.186215877532959
    },
    {
      "epoch": 0.25794037940379405,
      "step": 4759,
      "training_loss": 4.125746726989746
    },
    {
      "epoch": 0.25799457994579944,
      "grad_norm": 37.234100341796875,
      "learning_rate": 1e-05,
      "loss": 5.0757,
      "step": 4760
    },
    {
      "epoch": 0.25799457994579944,
      "step": 4760,
      "training_loss": 7.370724201202393
    },
    {
      "epoch": 0.2580487804878049,
      "step": 4761,
      "training_loss": 7.147902488708496
    },
    {
      "epoch": 0.2581029810298103,
      "step": 4762,
      "training_loss": 8.05123519897461
    },
    {
      "epoch": 0.2581571815718157,
      "step": 4763,
      "training_loss": 6.469763278961182
    },
    {
      "epoch": 0.25821138211382116,
      "grad_norm": 27.233915328979492,
      "learning_rate": 1e-05,
      "loss": 7.2599,
      "step": 4764
    },
    {
      "epoch": 0.25821138211382116,
      "step": 4764,
      "training_loss": 6.541208267211914
    },
    {
      "epoch": 0.25826558265582655,
      "step": 4765,
      "training_loss": 7.979613304138184
    },
    {
      "epoch": 0.258319783197832,
      "step": 4766,
      "training_loss": 8.0649995803833
    },
    {
      "epoch": 0.2583739837398374,
      "step": 4767,
      "training_loss": 7.176191329956055
    },
    {
      "epoch": 0.25842818428184283,
      "grad_norm": 29.848480224609375,
      "learning_rate": 1e-05,
      "loss": 7.4405,
      "step": 4768
    },
    {
      "epoch": 0.25842818428184283,
      "step": 4768,
      "training_loss": 6.6219587326049805
    },
    {
      "epoch": 0.2584823848238482,
      "step": 4769,
      "training_loss": 7.738934516906738
    },
    {
      "epoch": 0.25853658536585367,
      "step": 4770,
      "training_loss": 6.82429313659668
    },
    {
      "epoch": 0.25859078590785906,
      "step": 4771,
      "training_loss": 8.224771499633789
    },
    {
      "epoch": 0.2586449864498645,
      "grad_norm": 25.921215057373047,
      "learning_rate": 1e-05,
      "loss": 7.3525,
      "step": 4772
    },
    {
      "epoch": 0.2586449864498645,
      "step": 4772,
      "training_loss": 6.676982402801514
    },
    {
      "epoch": 0.25869918699186994,
      "step": 4773,
      "training_loss": 6.622319221496582
    },
    {
      "epoch": 0.25875338753387533,
      "step": 4774,
      "training_loss": 6.78183126449585
    },
    {
      "epoch": 0.2588075880758808,
      "step": 4775,
      "training_loss": 6.8583149909973145
    },
    {
      "epoch": 0.25886178861788617,
      "grad_norm": 24.91596031188965,
      "learning_rate": 1e-05,
      "loss": 6.7349,
      "step": 4776
    },
    {
      "epoch": 0.25886178861788617,
      "step": 4776,
      "training_loss": 8.347890853881836
    },
    {
      "epoch": 0.2589159891598916,
      "step": 4777,
      "training_loss": 5.515162467956543
    },
    {
      "epoch": 0.258970189701897,
      "step": 4778,
      "training_loss": 6.924659252166748
    },
    {
      "epoch": 0.25902439024390245,
      "step": 4779,
      "training_loss": 6.916841506958008
    },
    {
      "epoch": 0.25907859078590784,
      "grad_norm": 15.96618366241455,
      "learning_rate": 1e-05,
      "loss": 6.9261,
      "step": 4780
    },
    {
      "epoch": 0.25907859078590784,
      "step": 4780,
      "training_loss": 6.75818395614624
    },
    {
      "epoch": 0.2591327913279133,
      "step": 4781,
      "training_loss": 7.585190773010254
    },
    {
      "epoch": 0.2591869918699187,
      "step": 4782,
      "training_loss": 6.595373630523682
    },
    {
      "epoch": 0.2592411924119241,
      "step": 4783,
      "training_loss": 5.693598747253418
    },
    {
      "epoch": 0.25929539295392956,
      "grad_norm": 24.765138626098633,
      "learning_rate": 1e-05,
      "loss": 6.6581,
      "step": 4784
    },
    {
      "epoch": 0.25929539295392956,
      "step": 4784,
      "training_loss": 6.746213912963867
    },
    {
      "epoch": 0.25934959349593495,
      "step": 4785,
      "training_loss": 6.618460655212402
    },
    {
      "epoch": 0.2594037940379404,
      "step": 4786,
      "training_loss": 6.4064555168151855
    },
    {
      "epoch": 0.2594579945799458,
      "step": 4787,
      "training_loss": 7.501226425170898
    },
    {
      "epoch": 0.25951219512195123,
      "grad_norm": 22.372682571411133,
      "learning_rate": 1e-05,
      "loss": 6.8181,
      "step": 4788
    },
    {
      "epoch": 0.25951219512195123,
      "step": 4788,
      "training_loss": 6.6543779373168945
    },
    {
      "epoch": 0.2595663956639566,
      "step": 4789,
      "training_loss": 6.815804958343506
    },
    {
      "epoch": 0.25962059620596206,
      "step": 4790,
      "training_loss": 7.315384387969971
    },
    {
      "epoch": 0.2596747967479675,
      "step": 4791,
      "training_loss": 6.02483606338501
    },
    {
      "epoch": 0.2597289972899729,
      "grad_norm": 30.280776977539062,
      "learning_rate": 1e-05,
      "loss": 6.7026,
      "step": 4792
    },
    {
      "epoch": 0.2597289972899729,
      "step": 4792,
      "training_loss": 7.538172721862793
    },
    {
      "epoch": 0.25978319783197834,
      "step": 4793,
      "training_loss": 6.887235164642334
    },
    {
      "epoch": 0.25983739837398373,
      "step": 4794,
      "training_loss": 6.96810245513916
    },
    {
      "epoch": 0.2598915989159892,
      "step": 4795,
      "training_loss": 6.018955707550049
    },
    {
      "epoch": 0.25994579945799456,
      "grad_norm": 24.2818660736084,
      "learning_rate": 1e-05,
      "loss": 6.8531,
      "step": 4796
    },
    {
      "epoch": 0.25994579945799456,
      "step": 4796,
      "training_loss": 7.401679515838623
    },
    {
      "epoch": 0.26,
      "step": 4797,
      "training_loss": 7.00376558303833
    },
    {
      "epoch": 0.2600542005420054,
      "step": 4798,
      "training_loss": 7.407041072845459
    },
    {
      "epoch": 0.26010840108401084,
      "step": 4799,
      "training_loss": 5.800042629241943
    },
    {
      "epoch": 0.2601626016260163,
      "grad_norm": 29.239648818969727,
      "learning_rate": 1e-05,
      "loss": 6.9031,
      "step": 4800
    },
    {
      "epoch": 0.2601626016260163,
      "step": 4800,
      "training_loss": 7.03473424911499
    },
    {
      "epoch": 0.2602168021680217,
      "step": 4801,
      "training_loss": 7.587743759155273
    },
    {
      "epoch": 0.2602710027100271,
      "step": 4802,
      "training_loss": 6.96549654006958
    },
    {
      "epoch": 0.2603252032520325,
      "step": 4803,
      "training_loss": 7.239055633544922
    },
    {
      "epoch": 0.26037940379403796,
      "grad_norm": 19.655977249145508,
      "learning_rate": 1e-05,
      "loss": 7.2068,
      "step": 4804
    },
    {
      "epoch": 0.26037940379403796,
      "step": 4804,
      "training_loss": 6.653024196624756
    },
    {
      "epoch": 0.26043360433604335,
      "step": 4805,
      "training_loss": 5.323056221008301
    },
    {
      "epoch": 0.2604878048780488,
      "step": 4806,
      "training_loss": 6.282486438751221
    },
    {
      "epoch": 0.2605420054200542,
      "step": 4807,
      "training_loss": 7.995870113372803
    },
    {
      "epoch": 0.2605962059620596,
      "grad_norm": 55.86457443237305,
      "learning_rate": 1e-05,
      "loss": 6.5636,
      "step": 4808
    },
    {
      "epoch": 0.2605962059620596,
      "step": 4808,
      "training_loss": 7.903599739074707
    },
    {
      "epoch": 0.260650406504065,
      "step": 4809,
      "training_loss": 8.078314781188965
    },
    {
      "epoch": 0.26070460704607046,
      "step": 4810,
      "training_loss": 4.749776363372803
    },
    {
      "epoch": 0.2607588075880759,
      "step": 4811,
      "training_loss": 6.693337917327881
    },
    {
      "epoch": 0.2608130081300813,
      "grad_norm": 32.427337646484375,
      "learning_rate": 1e-05,
      "loss": 6.8563,
      "step": 4812
    },
    {
      "epoch": 0.2608130081300813,
      "step": 4812,
      "training_loss": 8.74120807647705
    },
    {
      "epoch": 0.26086720867208674,
      "step": 4813,
      "training_loss": 6.5647783279418945
    },
    {
      "epoch": 0.2609214092140921,
      "step": 4814,
      "training_loss": 7.683102607727051
    },
    {
      "epoch": 0.26097560975609757,
      "step": 4815,
      "training_loss": 6.456812381744385
    },
    {
      "epoch": 0.26102981029810296,
      "grad_norm": 24.26529884338379,
      "learning_rate": 1e-05,
      "loss": 7.3615,
      "step": 4816
    },
    {
      "epoch": 0.26102981029810296,
      "step": 4816,
      "training_loss": 6.7926177978515625
    },
    {
      "epoch": 0.2610840108401084,
      "step": 4817,
      "training_loss": 7.516434669494629
    },
    {
      "epoch": 0.2611382113821138,
      "step": 4818,
      "training_loss": 6.01669979095459
    },
    {
      "epoch": 0.26119241192411924,
      "step": 4819,
      "training_loss": 7.223236083984375
    },
    {
      "epoch": 0.2612466124661247,
      "grad_norm": 33.34225082397461,
      "learning_rate": 1e-05,
      "loss": 6.8872,
      "step": 4820
    },
    {
      "epoch": 0.2612466124661247,
      "step": 4820,
      "training_loss": 8.027338027954102
    },
    {
      "epoch": 0.2613008130081301,
      "step": 4821,
      "training_loss": 7.103915691375732
    },
    {
      "epoch": 0.2613550135501355,
      "step": 4822,
      "training_loss": 7.132882118225098
    },
    {
      "epoch": 0.2614092140921409,
      "step": 4823,
      "training_loss": 6.559634208679199
    },
    {
      "epoch": 0.26146341463414635,
      "grad_norm": 48.379241943359375,
      "learning_rate": 1e-05,
      "loss": 7.2059,
      "step": 4824
    },
    {
      "epoch": 0.26146341463414635,
      "step": 4824,
      "training_loss": 4.663775444030762
    },
    {
      "epoch": 0.26151761517615174,
      "step": 4825,
      "training_loss": 6.274765491485596
    },
    {
      "epoch": 0.2615718157181572,
      "step": 4826,
      "training_loss": 7.439170837402344
    },
    {
      "epoch": 0.2616260162601626,
      "step": 4827,
      "training_loss": 6.4873504638671875
    },
    {
      "epoch": 0.261680216802168,
      "grad_norm": 31.360361099243164,
      "learning_rate": 1e-05,
      "loss": 6.2163,
      "step": 4828
    },
    {
      "epoch": 0.261680216802168,
      "step": 4828,
      "training_loss": 6.543314456939697
    },
    {
      "epoch": 0.26173441734417346,
      "step": 4829,
      "training_loss": 4.955932140350342
    },
    {
      "epoch": 0.26178861788617885,
      "step": 4830,
      "training_loss": 6.999753475189209
    },
    {
      "epoch": 0.2618428184281843,
      "step": 4831,
      "training_loss": 7.164976119995117
    },
    {
      "epoch": 0.2618970189701897,
      "grad_norm": 19.549957275390625,
      "learning_rate": 1e-05,
      "loss": 6.416,
      "step": 4832
    },
    {
      "epoch": 0.2618970189701897,
      "step": 4832,
      "training_loss": 5.509737968444824
    },
    {
      "epoch": 0.26195121951219513,
      "step": 4833,
      "training_loss": 4.545966148376465
    },
    {
      "epoch": 0.2620054200542005,
      "step": 4834,
      "training_loss": 7.066369533538818
    },
    {
      "epoch": 0.26205962059620597,
      "step": 4835,
      "training_loss": 4.158344268798828
    },
    {
      "epoch": 0.26211382113821136,
      "grad_norm": 22.723669052124023,
      "learning_rate": 1e-05,
      "loss": 5.3201,
      "step": 4836
    },
    {
      "epoch": 0.26211382113821136,
      "step": 4836,
      "training_loss": 6.641505241394043
    },
    {
      "epoch": 0.2621680216802168,
      "step": 4837,
      "training_loss": 6.911890506744385
    },
    {
      "epoch": 0.26222222222222225,
      "step": 4838,
      "training_loss": 7.751863956451416
    },
    {
      "epoch": 0.26227642276422763,
      "step": 4839,
      "training_loss": 6.661981105804443
    },
    {
      "epoch": 0.2623306233062331,
      "grad_norm": 26.848857879638672,
      "learning_rate": 1e-05,
      "loss": 6.9918,
      "step": 4840
    },
    {
      "epoch": 0.2623306233062331,
      "step": 4840,
      "training_loss": 7.032006740570068
    },
    {
      "epoch": 0.26238482384823847,
      "step": 4841,
      "training_loss": 7.130910873413086
    },
    {
      "epoch": 0.2624390243902439,
      "step": 4842,
      "training_loss": 5.854705333709717
    },
    {
      "epoch": 0.2624932249322493,
      "step": 4843,
      "training_loss": 5.652467727661133
    },
    {
      "epoch": 0.26254742547425475,
      "grad_norm": 20.525835037231445,
      "learning_rate": 1e-05,
      "loss": 6.4175,
      "step": 4844
    },
    {
      "epoch": 0.26254742547425475,
      "step": 4844,
      "training_loss": 6.253573894500732
    },
    {
      "epoch": 0.26260162601626014,
      "step": 4845,
      "training_loss": 6.104823112487793
    },
    {
      "epoch": 0.2626558265582656,
      "step": 4846,
      "training_loss": 6.407424449920654
    },
    {
      "epoch": 0.262710027100271,
      "step": 4847,
      "training_loss": 6.159245491027832
    },
    {
      "epoch": 0.2627642276422764,
      "grad_norm": 32.34025955200195,
      "learning_rate": 1e-05,
      "loss": 6.2313,
      "step": 4848
    },
    {
      "epoch": 0.2627642276422764,
      "step": 4848,
      "training_loss": 6.343488693237305
    },
    {
      "epoch": 0.26281842818428186,
      "step": 4849,
      "training_loss": 3.463752269744873
    },
    {
      "epoch": 0.26287262872628725,
      "step": 4850,
      "training_loss": 7.3799662590026855
    },
    {
      "epoch": 0.2629268292682927,
      "step": 4851,
      "training_loss": 7.650152206420898
    },
    {
      "epoch": 0.2629810298102981,
      "grad_norm": 26.656349182128906,
      "learning_rate": 1e-05,
      "loss": 6.2093,
      "step": 4852
    },
    {
      "epoch": 0.2629810298102981,
      "step": 4852,
      "training_loss": 8.01840591430664
    },
    {
      "epoch": 0.26303523035230353,
      "step": 4853,
      "training_loss": 7.63856840133667
    },
    {
      "epoch": 0.2630894308943089,
      "step": 4854,
      "training_loss": 5.44691801071167
    },
    {
      "epoch": 0.26314363143631436,
      "step": 4855,
      "training_loss": 7.764342784881592
    },
    {
      "epoch": 0.2631978319783198,
      "grad_norm": 61.70840835571289,
      "learning_rate": 1e-05,
      "loss": 7.2171,
      "step": 4856
    },
    {
      "epoch": 0.2631978319783198,
      "step": 4856,
      "training_loss": 7.310907363891602
    },
    {
      "epoch": 0.2632520325203252,
      "step": 4857,
      "training_loss": 6.803649425506592
    },
    {
      "epoch": 0.26330623306233064,
      "step": 4858,
      "training_loss": 6.427469253540039
    },
    {
      "epoch": 0.26336043360433603,
      "step": 4859,
      "training_loss": 5.6680402755737305
    },
    {
      "epoch": 0.2634146341463415,
      "grad_norm": 29.596471786499023,
      "learning_rate": 1e-05,
      "loss": 6.5525,
      "step": 4860
    },
    {
      "epoch": 0.2634146341463415,
      "step": 4860,
      "training_loss": 6.297351837158203
    },
    {
      "epoch": 0.26346883468834686,
      "step": 4861,
      "training_loss": 6.702069282531738
    },
    {
      "epoch": 0.2635230352303523,
      "step": 4862,
      "training_loss": 6.743041515350342
    },
    {
      "epoch": 0.2635772357723577,
      "step": 4863,
      "training_loss": 5.9014506340026855
    },
    {
      "epoch": 0.26363143631436314,
      "grad_norm": 33.422752380371094,
      "learning_rate": 1e-05,
      "loss": 6.411,
      "step": 4864
    },
    {
      "epoch": 0.26363143631436314,
      "step": 4864,
      "training_loss": 6.9830169677734375
    },
    {
      "epoch": 0.2636856368563686,
      "step": 4865,
      "training_loss": 7.094808101654053
    },
    {
      "epoch": 0.263739837398374,
      "step": 4866,
      "training_loss": 6.969700336456299
    },
    {
      "epoch": 0.2637940379403794,
      "step": 4867,
      "training_loss": 7.193997859954834
    },
    {
      "epoch": 0.2638482384823848,
      "grad_norm": 25.80554962158203,
      "learning_rate": 1e-05,
      "loss": 7.0604,
      "step": 4868
    },
    {
      "epoch": 0.2638482384823848,
      "step": 4868,
      "training_loss": 7.023822784423828
    },
    {
      "epoch": 0.26390243902439026,
      "step": 4869,
      "training_loss": 6.904531478881836
    },
    {
      "epoch": 0.26395663956639565,
      "step": 4870,
      "training_loss": 6.532861232757568
    },
    {
      "epoch": 0.2640108401084011,
      "step": 4871,
      "training_loss": 7.518523693084717
    },
    {
      "epoch": 0.2640650406504065,
      "grad_norm": 16.098114013671875,
      "learning_rate": 1e-05,
      "loss": 6.9949,
      "step": 4872
    },
    {
      "epoch": 0.2640650406504065,
      "step": 4872,
      "training_loss": 5.525960445404053
    },
    {
      "epoch": 0.2641192411924119,
      "step": 4873,
      "training_loss": 6.4418745040893555
    },
    {
      "epoch": 0.26417344173441737,
      "step": 4874,
      "training_loss": 5.409494400024414
    },
    {
      "epoch": 0.26422764227642276,
      "step": 4875,
      "training_loss": 7.028253555297852
    },
    {
      "epoch": 0.2642818428184282,
      "grad_norm": 17.365798950195312,
      "learning_rate": 1e-05,
      "loss": 6.1014,
      "step": 4876
    },
    {
      "epoch": 0.2642818428184282,
      "step": 4876,
      "training_loss": 7.705293655395508
    },
    {
      "epoch": 0.2643360433604336,
      "step": 4877,
      "training_loss": 7.6135101318359375
    },
    {
      "epoch": 0.26439024390243904,
      "step": 4878,
      "training_loss": 6.538888454437256
    },
    {
      "epoch": 0.2644444444444444,
      "step": 4879,
      "training_loss": 6.8404860496521
    },
    {
      "epoch": 0.26449864498644987,
      "grad_norm": 22.497493743896484,
      "learning_rate": 1e-05,
      "loss": 7.1745,
      "step": 4880
    },
    {
      "epoch": 0.26449864498644987,
      "step": 4880,
      "training_loss": 6.473151683807373
    },
    {
      "epoch": 0.26455284552845526,
      "step": 4881,
      "training_loss": 7.773501873016357
    },
    {
      "epoch": 0.2646070460704607,
      "step": 4882,
      "training_loss": 7.117162227630615
    },
    {
      "epoch": 0.26466124661246615,
      "step": 4883,
      "training_loss": 4.842599391937256
    },
    {
      "epoch": 0.26471544715447154,
      "grad_norm": 20.025226593017578,
      "learning_rate": 1e-05,
      "loss": 6.5516,
      "step": 4884
    },
    {
      "epoch": 0.26471544715447154,
      "step": 4884,
      "training_loss": 8.028918266296387
    },
    {
      "epoch": 0.264769647696477,
      "step": 4885,
      "training_loss": 4.613664627075195
    },
    {
      "epoch": 0.2648238482384824,
      "step": 4886,
      "training_loss": 5.689857006072998
    },
    {
      "epoch": 0.2648780487804878,
      "step": 4887,
      "training_loss": 7.2086944580078125
    },
    {
      "epoch": 0.2649322493224932,
      "grad_norm": 16.11890983581543,
      "learning_rate": 1e-05,
      "loss": 6.3853,
      "step": 4888
    },
    {
      "epoch": 0.2649322493224932,
      "step": 4888,
      "training_loss": 6.974789142608643
    },
    {
      "epoch": 0.26498644986449865,
      "step": 4889,
      "training_loss": 6.038115501403809
    },
    {
      "epoch": 0.26504065040650404,
      "step": 4890,
      "training_loss": 7.399166107177734
    },
    {
      "epoch": 0.2650948509485095,
      "step": 4891,
      "training_loss": 9.039830207824707
    },
    {
      "epoch": 0.26514905149051493,
      "grad_norm": 28.489824295043945,
      "learning_rate": 1e-05,
      "loss": 7.363,
      "step": 4892
    },
    {
      "epoch": 0.26514905149051493,
      "step": 4892,
      "training_loss": 6.314671039581299
    },
    {
      "epoch": 0.2652032520325203,
      "step": 4893,
      "training_loss": 7.068612575531006
    },
    {
      "epoch": 0.26525745257452576,
      "step": 4894,
      "training_loss": 6.624389171600342
    },
    {
      "epoch": 0.26531165311653115,
      "step": 4895,
      "training_loss": 7.188756465911865
    },
    {
      "epoch": 0.2653658536585366,
      "grad_norm": 32.66558074951172,
      "learning_rate": 1e-05,
      "loss": 6.7991,
      "step": 4896
    },
    {
      "epoch": 0.2653658536585366,
      "step": 4896,
      "training_loss": 7.902585506439209
    },
    {
      "epoch": 0.265420054200542,
      "step": 4897,
      "training_loss": 7.601114273071289
    },
    {
      "epoch": 0.26547425474254743,
      "step": 4898,
      "training_loss": 6.281242847442627
    },
    {
      "epoch": 0.2655284552845528,
      "step": 4899,
      "training_loss": 6.994826316833496
    },
    {
      "epoch": 0.26558265582655827,
      "grad_norm": 24.744199752807617,
      "learning_rate": 1e-05,
      "loss": 7.1949,
      "step": 4900
    },
    {
      "epoch": 0.26558265582655827,
      "step": 4900,
      "training_loss": 4.899870872497559
    },
    {
      "epoch": 0.2656368563685637,
      "step": 4901,
      "training_loss": 5.6681294441223145
    },
    {
      "epoch": 0.2656910569105691,
      "step": 4902,
      "training_loss": 6.671302795410156
    },
    {
      "epoch": 0.26574525745257455,
      "step": 4903,
      "training_loss": 7.625035285949707
    },
    {
      "epoch": 0.26579945799457994,
      "grad_norm": 32.96500778198242,
      "learning_rate": 1e-05,
      "loss": 6.2161,
      "step": 4904
    },
    {
      "epoch": 0.26579945799457994,
      "step": 4904,
      "training_loss": 6.943455696105957
    },
    {
      "epoch": 0.2658536585365854,
      "step": 4905,
      "training_loss": 7.1414384841918945
    },
    {
      "epoch": 0.26590785907859077,
      "step": 4906,
      "training_loss": 6.6416425704956055
    },
    {
      "epoch": 0.2659620596205962,
      "step": 4907,
      "training_loss": 7.8988728523254395
    },
    {
      "epoch": 0.2660162601626016,
      "grad_norm": 33.226688385009766,
      "learning_rate": 1e-05,
      "loss": 7.1564,
      "step": 4908
    },
    {
      "epoch": 0.2660162601626016,
      "step": 4908,
      "training_loss": 6.752378463745117
    },
    {
      "epoch": 0.26607046070460705,
      "step": 4909,
      "training_loss": 6.614992618560791
    },
    {
      "epoch": 0.2661246612466125,
      "step": 4910,
      "training_loss": 7.080846786499023
    },
    {
      "epoch": 0.2661788617886179,
      "step": 4911,
      "training_loss": 6.1626763343811035
    },
    {
      "epoch": 0.2662330623306233,
      "grad_norm": 28.04447364807129,
      "learning_rate": 1e-05,
      "loss": 6.6527,
      "step": 4912
    },
    {
      "epoch": 0.2662330623306233,
      "step": 4912,
      "training_loss": 8.245169639587402
    },
    {
      "epoch": 0.2662872628726287,
      "step": 4913,
      "training_loss": 6.370792865753174
    },
    {
      "epoch": 0.26634146341463416,
      "step": 4914,
      "training_loss": 7.199748992919922
    },
    {
      "epoch": 0.26639566395663955,
      "step": 4915,
      "training_loss": 6.339925289154053
    },
    {
      "epoch": 0.266449864498645,
      "grad_norm": 22.966571807861328,
      "learning_rate": 1e-05,
      "loss": 7.0389,
      "step": 4916
    },
    {
      "epoch": 0.266449864498645,
      "step": 4916,
      "training_loss": 6.000926494598389
    },
    {
      "epoch": 0.2665040650406504,
      "step": 4917,
      "training_loss": 6.407894611358643
    },
    {
      "epoch": 0.26655826558265583,
      "step": 4918,
      "training_loss": 7.498404026031494
    },
    {
      "epoch": 0.2666124661246613,
      "step": 4919,
      "training_loss": 6.698840141296387
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 19.85947608947754,
      "learning_rate": 1e-05,
      "loss": 6.6515,
      "step": 4920
    },
    {
      "epoch": 0.26666666666666666,
      "step": 4920,
      "training_loss": 7.020698547363281
    },
    {
      "epoch": 0.2667208672086721,
      "step": 4921,
      "training_loss": 4.550604343414307
    },
    {
      "epoch": 0.2667750677506775,
      "step": 4922,
      "training_loss": 7.1332879066467285
    },
    {
      "epoch": 0.26682926829268294,
      "step": 4923,
      "training_loss": 6.6504740715026855
    },
    {
      "epoch": 0.26688346883468833,
      "grad_norm": 15.641388893127441,
      "learning_rate": 1e-05,
      "loss": 6.3388,
      "step": 4924
    },
    {
      "epoch": 0.26688346883468833,
      "step": 4924,
      "training_loss": 7.641758918762207
    },
    {
      "epoch": 0.2669376693766938,
      "step": 4925,
      "training_loss": 6.5288591384887695
    },
    {
      "epoch": 0.26699186991869917,
      "step": 4926,
      "training_loss": 6.759339809417725
    },
    {
      "epoch": 0.2670460704607046,
      "step": 4927,
      "training_loss": 6.272345066070557
    },
    {
      "epoch": 0.26710027100271005,
      "grad_norm": 24.616151809692383,
      "learning_rate": 1e-05,
      "loss": 6.8006,
      "step": 4928
    },
    {
      "epoch": 0.26710027100271005,
      "step": 4928,
      "training_loss": 7.08860445022583
    },
    {
      "epoch": 0.26715447154471544,
      "step": 4929,
      "training_loss": 7.004964828491211
    },
    {
      "epoch": 0.2672086720867209,
      "step": 4930,
      "training_loss": 4.900237560272217
    },
    {
      "epoch": 0.2672628726287263,
      "step": 4931,
      "training_loss": 6.592990875244141
    },
    {
      "epoch": 0.2673170731707317,
      "grad_norm": 21.048871994018555,
      "learning_rate": 1e-05,
      "loss": 6.3967,
      "step": 4932
    },
    {
      "epoch": 0.2673170731707317,
      "step": 4932,
      "training_loss": 8.386610984802246
    },
    {
      "epoch": 0.2673712737127371,
      "step": 4933,
      "training_loss": 7.453389644622803
    },
    {
      "epoch": 0.26742547425474256,
      "step": 4934,
      "training_loss": 6.80778694152832
    },
    {
      "epoch": 0.26747967479674795,
      "step": 4935,
      "training_loss": 5.854772567749023
    },
    {
      "epoch": 0.2675338753387534,
      "grad_norm": 24.39864730834961,
      "learning_rate": 1e-05,
      "loss": 7.1256,
      "step": 4936
    },
    {
      "epoch": 0.2675338753387534,
      "step": 4936,
      "training_loss": 5.845946788787842
    },
    {
      "epoch": 0.2675880758807588,
      "step": 4937,
      "training_loss": 5.502752304077148
    },
    {
      "epoch": 0.2676422764227642,
      "step": 4938,
      "training_loss": 7.993241786956787
    },
    {
      "epoch": 0.26769647696476967,
      "step": 4939,
      "training_loss": 7.720244407653809
    },
    {
      "epoch": 0.26775067750677506,
      "grad_norm": 22.063528060913086,
      "learning_rate": 1e-05,
      "loss": 6.7655,
      "step": 4940
    },
    {
      "epoch": 0.26775067750677506,
      "step": 4940,
      "training_loss": 8.788993835449219
    },
    {
      "epoch": 0.2678048780487805,
      "step": 4941,
      "training_loss": 6.705447673797607
    },
    {
      "epoch": 0.2678590785907859,
      "step": 4942,
      "training_loss": 6.750504970550537
    },
    {
      "epoch": 0.26791327913279134,
      "step": 4943,
      "training_loss": 7.156973361968994
    },
    {
      "epoch": 0.2679674796747967,
      "grad_norm": 25.818771362304688,
      "learning_rate": 1e-05,
      "loss": 7.3505,
      "step": 4944
    },
    {
      "epoch": 0.2679674796747967,
      "step": 4944,
      "training_loss": 6.248327732086182
    },
    {
      "epoch": 0.26802168021680217,
      "step": 4945,
      "training_loss": 4.493154048919678
    },
    {
      "epoch": 0.26807588075880756,
      "step": 4946,
      "training_loss": 7.335882663726807
    },
    {
      "epoch": 0.268130081300813,
      "step": 4947,
      "training_loss": 7.410265922546387
    },
    {
      "epoch": 0.26818428184281845,
      "grad_norm": 20.019559860229492,
      "learning_rate": 1e-05,
      "loss": 6.3719,
      "step": 4948
    },
    {
      "epoch": 0.26818428184281845,
      "step": 4948,
      "training_loss": 7.330765724182129
    },
    {
      "epoch": 0.26823848238482384,
      "step": 4949,
      "training_loss": 7.232378005981445
    },
    {
      "epoch": 0.2682926829268293,
      "step": 4950,
      "training_loss": 6.155293941497803
    },
    {
      "epoch": 0.2683468834688347,
      "step": 4951,
      "training_loss": 6.044593334197998
    },
    {
      "epoch": 0.2684010840108401,
      "grad_norm": 20.529251098632812,
      "learning_rate": 1e-05,
      "loss": 6.6908,
      "step": 4952
    },
    {
      "epoch": 0.2684010840108401,
      "step": 4952,
      "training_loss": 6.961282730102539
    },
    {
      "epoch": 0.2684552845528455,
      "step": 4953,
      "training_loss": 6.80707311630249
    },
    {
      "epoch": 0.26850948509485095,
      "step": 4954,
      "training_loss": 7.294464588165283
    },
    {
      "epoch": 0.26856368563685634,
      "step": 4955,
      "training_loss": 7.190250396728516
    },
    {
      "epoch": 0.2686178861788618,
      "grad_norm": 17.028221130371094,
      "learning_rate": 1e-05,
      "loss": 7.0633,
      "step": 4956
    },
    {
      "epoch": 0.2686178861788618,
      "step": 4956,
      "training_loss": 6.37131404876709
    },
    {
      "epoch": 0.26867208672086723,
      "step": 4957,
      "training_loss": 6.238094806671143
    },
    {
      "epoch": 0.2687262872628726,
      "step": 4958,
      "training_loss": 6.145900249481201
    },
    {
      "epoch": 0.26878048780487807,
      "step": 4959,
      "training_loss": 6.130329132080078
    },
    {
      "epoch": 0.26883468834688345,
      "grad_norm": 25.210399627685547,
      "learning_rate": 1e-05,
      "loss": 6.2214,
      "step": 4960
    },
    {
      "epoch": 0.26883468834688345,
      "step": 4960,
      "training_loss": 5.908989429473877
    },
    {
      "epoch": 0.2688888888888889,
      "step": 4961,
      "training_loss": 6.660285472869873
    },
    {
      "epoch": 0.2689430894308943,
      "step": 4962,
      "training_loss": 6.9891791343688965
    },
    {
      "epoch": 0.26899728997289973,
      "step": 4963,
      "training_loss": 5.959968566894531
    },
    {
      "epoch": 0.2690514905149051,
      "grad_norm": 24.21146011352539,
      "learning_rate": 1e-05,
      "loss": 6.3796,
      "step": 4964
    },
    {
      "epoch": 0.2690514905149051,
      "step": 4964,
      "training_loss": 6.107871055603027
    },
    {
      "epoch": 0.26910569105691057,
      "step": 4965,
      "training_loss": 7.278170585632324
    },
    {
      "epoch": 0.269159891598916,
      "step": 4966,
      "training_loss": 7.867782115936279
    },
    {
      "epoch": 0.2692140921409214,
      "step": 4967,
      "training_loss": 7.0565667152404785
    },
    {
      "epoch": 0.26926829268292685,
      "grad_norm": 17.299789428710938,
      "learning_rate": 1e-05,
      "loss": 7.0776,
      "step": 4968
    },
    {
      "epoch": 0.26926829268292685,
      "step": 4968,
      "training_loss": 6.16890811920166
    },
    {
      "epoch": 0.26932249322493224,
      "step": 4969,
      "training_loss": 7.701040267944336
    },
    {
      "epoch": 0.2693766937669377,
      "step": 4970,
      "training_loss": 4.338235855102539
    },
    {
      "epoch": 0.26943089430894307,
      "step": 4971,
      "training_loss": 6.553873062133789
    },
    {
      "epoch": 0.2694850948509485,
      "grad_norm": 23.4880428314209,
      "learning_rate": 1e-05,
      "loss": 6.1905,
      "step": 4972
    },
    {
      "epoch": 0.2694850948509485,
      "step": 4972,
      "training_loss": 5.885667324066162
    },
    {
      "epoch": 0.2695392953929539,
      "step": 4973,
      "training_loss": 6.927097797393799
    },
    {
      "epoch": 0.26959349593495935,
      "step": 4974,
      "training_loss": 6.96165132522583
    },
    {
      "epoch": 0.2696476964769648,
      "step": 4975,
      "training_loss": 4.967837333679199
    },
    {
      "epoch": 0.2697018970189702,
      "grad_norm": 21.30640411376953,
      "learning_rate": 1e-05,
      "loss": 6.1856,
      "step": 4976
    },
    {
      "epoch": 0.2697018970189702,
      "step": 4976,
      "training_loss": 6.866994857788086
    },
    {
      "epoch": 0.2697560975609756,
      "step": 4977,
      "training_loss": 6.377378463745117
    },
    {
      "epoch": 0.269810298102981,
      "step": 4978,
      "training_loss": 5.97785758972168
    },
    {
      "epoch": 0.26986449864498646,
      "step": 4979,
      "training_loss": 6.238738059997559
    },
    {
      "epoch": 0.26991869918699185,
      "grad_norm": 25.346500396728516,
      "learning_rate": 1e-05,
      "loss": 6.3652,
      "step": 4980
    },
    {
      "epoch": 0.26991869918699185,
      "step": 4980,
      "training_loss": 6.406407833099365
    },
    {
      "epoch": 0.2699728997289973,
      "step": 4981,
      "training_loss": 6.7348456382751465
    },
    {
      "epoch": 0.2700271002710027,
      "step": 4982,
      "training_loss": 5.688357353210449
    },
    {
      "epoch": 0.27008130081300813,
      "step": 4983,
      "training_loss": 6.648710250854492
    },
    {
      "epoch": 0.2701355013550136,
      "grad_norm": 55.215389251708984,
      "learning_rate": 1e-05,
      "loss": 6.3696,
      "step": 4984
    },
    {
      "epoch": 0.2701355013550136,
      "step": 4984,
      "training_loss": 8.202692985534668
    },
    {
      "epoch": 0.27018970189701896,
      "step": 4985,
      "training_loss": 7.671238899230957
    },
    {
      "epoch": 0.2702439024390244,
      "step": 4986,
      "training_loss": 7.342266082763672
    },
    {
      "epoch": 0.2702981029810298,
      "step": 4987,
      "training_loss": 7.1196393966674805
    },
    {
      "epoch": 0.27035230352303524,
      "grad_norm": 18.4271183013916,
      "learning_rate": 1e-05,
      "loss": 7.584,
      "step": 4988
    },
    {
      "epoch": 0.27035230352303524,
      "step": 4988,
      "training_loss": 5.983510494232178
    },
    {
      "epoch": 0.27040650406504063,
      "step": 4989,
      "training_loss": 4.742553234100342
    },
    {
      "epoch": 0.2704607046070461,
      "step": 4990,
      "training_loss": 7.378482341766357
    },
    {
      "epoch": 0.27051490514905147,
      "step": 4991,
      "training_loss": 5.532063007354736
    },
    {
      "epoch": 0.2705691056910569,
      "grad_norm": 33.3089485168457,
      "learning_rate": 1e-05,
      "loss": 5.9092,
      "step": 4992
    },
    {
      "epoch": 0.2705691056910569,
      "step": 4992,
      "training_loss": 6.453804016113281
    },
    {
      "epoch": 0.27062330623306236,
      "step": 4993,
      "training_loss": 6.779522895812988
    },
    {
      "epoch": 0.27067750677506774,
      "step": 4994,
      "training_loss": 6.931865692138672
    },
    {
      "epoch": 0.2707317073170732,
      "step": 4995,
      "training_loss": 6.8208909034729
    },
    {
      "epoch": 0.2707859078590786,
      "grad_norm": 28.626035690307617,
      "learning_rate": 1e-05,
      "loss": 6.7465,
      "step": 4996
    },
    {
      "epoch": 0.2707859078590786,
      "step": 4996,
      "training_loss": 7.275331974029541
    },
    {
      "epoch": 0.270840108401084,
      "step": 4997,
      "training_loss": 6.914759159088135
    },
    {
      "epoch": 0.2708943089430894,
      "step": 4998,
      "training_loss": 6.469493865966797
    },
    {
      "epoch": 0.27094850948509486,
      "step": 4999,
      "training_loss": 6.281454086303711
    },
    {
      "epoch": 0.27100271002710025,
      "grad_norm": 20.635196685791016,
      "learning_rate": 1e-05,
      "loss": 6.7353,
      "step": 5000
    },
    {
      "epoch": 0.27100271002710025,
      "step": 5000,
      "training_loss": 6.846871376037598
    },
    {
      "epoch": 0.2710569105691057,
      "step": 5001,
      "training_loss": 6.084092617034912
    },
    {
      "epoch": 0.27111111111111114,
      "step": 5002,
      "training_loss": 6.354703426361084
    },
    {
      "epoch": 0.2711653116531165,
      "step": 5003,
      "training_loss": 5.487571716308594
    },
    {
      "epoch": 0.27121951219512197,
      "grad_norm": 33.11050796508789,
      "learning_rate": 1e-05,
      "loss": 6.1933,
      "step": 5004
    },
    {
      "epoch": 0.27121951219512197,
      "step": 5004,
      "training_loss": 4.632855415344238
    },
    {
      "epoch": 0.27127371273712736,
      "step": 5005,
      "training_loss": 8.17302131652832
    },
    {
      "epoch": 0.2713279132791328,
      "step": 5006,
      "training_loss": 8.119634628295898
    },
    {
      "epoch": 0.2713821138211382,
      "step": 5007,
      "training_loss": 6.907395839691162
    },
    {
      "epoch": 0.27143631436314364,
      "grad_norm": 27.857711791992188,
      "learning_rate": 1e-05,
      "loss": 6.9582,
      "step": 5008
    },
    {
      "epoch": 0.27143631436314364,
      "step": 5008,
      "training_loss": 6.668277263641357
    },
    {
      "epoch": 0.271490514905149,
      "step": 5009,
      "training_loss": 6.824997901916504
    },
    {
      "epoch": 0.27154471544715447,
      "step": 5010,
      "training_loss": 6.480597496032715
    },
    {
      "epoch": 0.2715989159891599,
      "step": 5011,
      "training_loss": 6.35274600982666
    },
    {
      "epoch": 0.2716531165311653,
      "grad_norm": 29.897846221923828,
      "learning_rate": 1e-05,
      "loss": 6.5817,
      "step": 5012
    },
    {
      "epoch": 0.2716531165311653,
      "step": 5012,
      "training_loss": 5.531122207641602
    },
    {
      "epoch": 0.27170731707317075,
      "step": 5013,
      "training_loss": 6.113900184631348
    },
    {
      "epoch": 0.27176151761517614,
      "step": 5014,
      "training_loss": 6.961343765258789
    },
    {
      "epoch": 0.2718157181571816,
      "step": 5015,
      "training_loss": 7.088756084442139
    },
    {
      "epoch": 0.271869918699187,
      "grad_norm": 19.20054817199707,
      "learning_rate": 1e-05,
      "loss": 6.4238,
      "step": 5016
    },
    {
      "epoch": 0.271869918699187,
      "step": 5016,
      "training_loss": 7.603376388549805
    },
    {
      "epoch": 0.2719241192411924,
      "step": 5017,
      "training_loss": 6.75981330871582
    },
    {
      "epoch": 0.2719783197831978,
      "step": 5018,
      "training_loss": 6.7920966148376465
    },
    {
      "epoch": 0.27203252032520325,
      "step": 5019,
      "training_loss": 9.877098083496094
    },
    {
      "epoch": 0.2720867208672087,
      "grad_norm": 60.20651626586914,
      "learning_rate": 1e-05,
      "loss": 7.7581,
      "step": 5020
    },
    {
      "epoch": 0.2720867208672087,
      "step": 5020,
      "training_loss": 6.8385491371154785
    },
    {
      "epoch": 0.2721409214092141,
      "step": 5021,
      "training_loss": 6.908149719238281
    },
    {
      "epoch": 0.27219512195121953,
      "step": 5022,
      "training_loss": 4.465498924255371
    },
    {
      "epoch": 0.2722493224932249,
      "step": 5023,
      "training_loss": 7.869126319885254
    },
    {
      "epoch": 0.27230352303523037,
      "grad_norm": 25.243654251098633,
      "learning_rate": 1e-05,
      "loss": 6.5203,
      "step": 5024
    },
    {
      "epoch": 0.27230352303523037,
      "step": 5024,
      "training_loss": 7.12803316116333
    },
    {
      "epoch": 0.27235772357723576,
      "step": 5025,
      "training_loss": 7.4212870597839355
    },
    {
      "epoch": 0.2724119241192412,
      "step": 5026,
      "training_loss": 6.751155376434326
    },
    {
      "epoch": 0.2724661246612466,
      "step": 5027,
      "training_loss": 7.263567924499512
    },
    {
      "epoch": 0.27252032520325203,
      "grad_norm": 20.02451515197754,
      "learning_rate": 1e-05,
      "loss": 7.141,
      "step": 5028
    },
    {
      "epoch": 0.27252032520325203,
      "step": 5028,
      "training_loss": 7.132732391357422
    },
    {
      "epoch": 0.2725745257452575,
      "step": 5029,
      "training_loss": 6.012722492218018
    },
    {
      "epoch": 0.27262872628726287,
      "step": 5030,
      "training_loss": 6.621293067932129
    },
    {
      "epoch": 0.2726829268292683,
      "step": 5031,
      "training_loss": 7.5128068923950195
    },
    {
      "epoch": 0.2727371273712737,
      "grad_norm": 23.665990829467773,
      "learning_rate": 1e-05,
      "loss": 6.8199,
      "step": 5032
    },
    {
      "epoch": 0.2727371273712737,
      "step": 5032,
      "training_loss": 5.978063583374023
    },
    {
      "epoch": 0.27279132791327915,
      "step": 5033,
      "training_loss": 6.502889633178711
    },
    {
      "epoch": 0.27284552845528454,
      "step": 5034,
      "training_loss": 7.972695350646973
    },
    {
      "epoch": 0.27289972899729,
      "step": 5035,
      "training_loss": 5.6975884437561035
    },
    {
      "epoch": 0.27295392953929537,
      "grad_norm": 26.227331161499023,
      "learning_rate": 1e-05,
      "loss": 6.5378,
      "step": 5036
    },
    {
      "epoch": 0.27295392953929537,
      "step": 5036,
      "training_loss": 4.315586090087891
    },
    {
      "epoch": 0.2730081300813008,
      "step": 5037,
      "training_loss": 7.259707450866699
    },
    {
      "epoch": 0.27306233062330626,
      "step": 5038,
      "training_loss": 6.864482879638672
    },
    {
      "epoch": 0.27311653116531165,
      "step": 5039,
      "training_loss": 6.551335334777832
    },
    {
      "epoch": 0.2731707317073171,
      "grad_norm": 27.97097396850586,
      "learning_rate": 1e-05,
      "loss": 6.2478,
      "step": 5040
    },
    {
      "epoch": 0.2731707317073171,
      "step": 5040,
      "training_loss": 6.701514720916748
    },
    {
      "epoch": 0.2732249322493225,
      "step": 5041,
      "training_loss": 7.216390132904053
    },
    {
      "epoch": 0.27327913279132793,
      "step": 5042,
      "training_loss": 8.100756645202637
    },
    {
      "epoch": 0.2733333333333333,
      "step": 5043,
      "training_loss": 7.402855396270752
    },
    {
      "epoch": 0.27338753387533876,
      "grad_norm": 24.500633239746094,
      "learning_rate": 1e-05,
      "loss": 7.3554,
      "step": 5044
    },
    {
      "epoch": 0.27338753387533876,
      "step": 5044,
      "training_loss": 6.1606245040893555
    },
    {
      "epoch": 0.27344173441734415,
      "step": 5045,
      "training_loss": 5.402135372161865
    },
    {
      "epoch": 0.2734959349593496,
      "step": 5046,
      "training_loss": 6.702467918395996
    },
    {
      "epoch": 0.27355013550135504,
      "step": 5047,
      "training_loss": 6.352453231811523
    },
    {
      "epoch": 0.27360433604336043,
      "grad_norm": 21.45200538635254,
      "learning_rate": 1e-05,
      "loss": 6.1544,
      "step": 5048
    },
    {
      "epoch": 0.27360433604336043,
      "step": 5048,
      "training_loss": 6.0108489990234375
    },
    {
      "epoch": 0.2736585365853659,
      "step": 5049,
      "training_loss": 6.343145370483398
    },
    {
      "epoch": 0.27371273712737126,
      "step": 5050,
      "training_loss": 6.253802299499512
    },
    {
      "epoch": 0.2737669376693767,
      "step": 5051,
      "training_loss": 8.04179859161377
    },
    {
      "epoch": 0.2738211382113821,
      "grad_norm": 24.116987228393555,
      "learning_rate": 1e-05,
      "loss": 6.6624,
      "step": 5052
    },
    {
      "epoch": 0.2738211382113821,
      "step": 5052,
      "training_loss": 3.90155029296875
    },
    {
      "epoch": 0.27387533875338754,
      "step": 5053,
      "training_loss": 7.348795413970947
    },
    {
      "epoch": 0.27392953929539293,
      "step": 5054,
      "training_loss": 7.3843865394592285
    },
    {
      "epoch": 0.2739837398373984,
      "step": 5055,
      "training_loss": 8.598566055297852
    },
    {
      "epoch": 0.2740379403794038,
      "grad_norm": 27.270095825195312,
      "learning_rate": 1e-05,
      "loss": 6.8083,
      "step": 5056
    },
    {
      "epoch": 0.2740379403794038,
      "step": 5056,
      "training_loss": 7.274750709533691
    },
    {
      "epoch": 0.2740921409214092,
      "step": 5057,
      "training_loss": 5.564451694488525
    },
    {
      "epoch": 0.27414634146341466,
      "step": 5058,
      "training_loss": 6.425874710083008
    },
    {
      "epoch": 0.27420054200542004,
      "step": 5059,
      "training_loss": 8.992061614990234
    },
    {
      "epoch": 0.2742547425474255,
      "grad_norm": 42.347007751464844,
      "learning_rate": 1e-05,
      "loss": 7.0643,
      "step": 5060
    },
    {
      "epoch": 0.2742547425474255,
      "step": 5060,
      "training_loss": 7.033367156982422
    },
    {
      "epoch": 0.2743089430894309,
      "step": 5061,
      "training_loss": 6.919116020202637
    },
    {
      "epoch": 0.2743631436314363,
      "step": 5062,
      "training_loss": 7.683764934539795
    },
    {
      "epoch": 0.2744173441734417,
      "step": 5063,
      "training_loss": 6.566936492919922
    },
    {
      "epoch": 0.27447154471544716,
      "grad_norm": 34.346927642822266,
      "learning_rate": 1e-05,
      "loss": 7.0508,
      "step": 5064
    },
    {
      "epoch": 0.27447154471544716,
      "step": 5064,
      "training_loss": 6.86555814743042
    },
    {
      "epoch": 0.27452574525745255,
      "step": 5065,
      "training_loss": 7.374061584472656
    },
    {
      "epoch": 0.274579945799458,
      "step": 5066,
      "training_loss": 8.585213661193848
    },
    {
      "epoch": 0.27463414634146344,
      "step": 5067,
      "training_loss": 6.180380344390869
    },
    {
      "epoch": 0.2746883468834688,
      "grad_norm": 25.922157287597656,
      "learning_rate": 1e-05,
      "loss": 7.2513,
      "step": 5068
    },
    {
      "epoch": 0.2746883468834688,
      "step": 5068,
      "training_loss": 7.020946025848389
    },
    {
      "epoch": 0.27474254742547427,
      "step": 5069,
      "training_loss": 6.561461448669434
    },
    {
      "epoch": 0.27479674796747966,
      "step": 5070,
      "training_loss": 6.494003772735596
    },
    {
      "epoch": 0.2748509485094851,
      "step": 5071,
      "training_loss": 8.637481689453125
    },
    {
      "epoch": 0.2749051490514905,
      "grad_norm": 27.33583641052246,
      "learning_rate": 1e-05,
      "loss": 7.1785,
      "step": 5072
    },
    {
      "epoch": 0.2749051490514905,
      "step": 5072,
      "training_loss": 7.536584377288818
    },
    {
      "epoch": 0.27495934959349594,
      "step": 5073,
      "training_loss": 7.6446852684021
    },
    {
      "epoch": 0.27501355013550133,
      "step": 5074,
      "training_loss": 8.392212867736816
    },
    {
      "epoch": 0.2750677506775068,
      "step": 5075,
      "training_loss": 6.30781888961792
    },
    {
      "epoch": 0.2751219512195122,
      "grad_norm": 24.775413513183594,
      "learning_rate": 1e-05,
      "loss": 7.4703,
      "step": 5076
    },
    {
      "epoch": 0.2751219512195122,
      "step": 5076,
      "training_loss": 3.8457186222076416
    },
    {
      "epoch": 0.2751761517615176,
      "step": 5077,
      "training_loss": 6.927586555480957
    },
    {
      "epoch": 0.27523035230352305,
      "step": 5078,
      "training_loss": 5.750773906707764
    },
    {
      "epoch": 0.27528455284552844,
      "step": 5079,
      "training_loss": 6.9924211502075195
    },
    {
      "epoch": 0.2753387533875339,
      "grad_norm": 23.354942321777344,
      "learning_rate": 1e-05,
      "loss": 5.8791,
      "step": 5080
    },
    {
      "epoch": 0.2753387533875339,
      "step": 5080,
      "training_loss": 7.685932636260986
    },
    {
      "epoch": 0.2753929539295393,
      "step": 5081,
      "training_loss": 7.648141860961914
    },
    {
      "epoch": 0.2754471544715447,
      "step": 5082,
      "training_loss": 6.837948322296143
    },
    {
      "epoch": 0.2755013550135501,
      "step": 5083,
      "training_loss": 5.5679612159729
    },
    {
      "epoch": 0.27555555555555555,
      "grad_norm": 24.68473243713379,
      "learning_rate": 1e-05,
      "loss": 6.935,
      "step": 5084
    },
    {
      "epoch": 0.27555555555555555,
      "step": 5084,
      "training_loss": 7.605480194091797
    },
    {
      "epoch": 0.275609756097561,
      "step": 5085,
      "training_loss": 7.336971759796143
    },
    {
      "epoch": 0.2756639566395664,
      "step": 5086,
      "training_loss": 6.379064083099365
    },
    {
      "epoch": 0.27571815718157183,
      "step": 5087,
      "training_loss": 6.675513744354248
    },
    {
      "epoch": 0.2757723577235772,
      "grad_norm": 24.13417625427246,
      "learning_rate": 1e-05,
      "loss": 6.9993,
      "step": 5088
    },
    {
      "epoch": 0.2757723577235772,
      "step": 5088,
      "training_loss": 6.234110355377197
    },
    {
      "epoch": 0.27582655826558267,
      "step": 5089,
      "training_loss": 6.761823654174805
    },
    {
      "epoch": 0.27588075880758806,
      "step": 5090,
      "training_loss": 7.025148391723633
    },
    {
      "epoch": 0.2759349593495935,
      "step": 5091,
      "training_loss": 7.738255977630615
    },
    {
      "epoch": 0.2759891598915989,
      "grad_norm": 30.339004516601562,
      "learning_rate": 1e-05,
      "loss": 6.9398,
      "step": 5092
    },
    {
      "epoch": 0.2759891598915989,
      "step": 5092,
      "training_loss": 7.103665828704834
    },
    {
      "epoch": 0.27604336043360433,
      "step": 5093,
      "training_loss": 5.269614219665527
    },
    {
      "epoch": 0.2760975609756098,
      "step": 5094,
      "training_loss": 7.804317474365234
    },
    {
      "epoch": 0.27615176151761517,
      "step": 5095,
      "training_loss": 6.4863176345825195
    },
    {
      "epoch": 0.2762059620596206,
      "grad_norm": 27.882822036743164,
      "learning_rate": 1e-05,
      "loss": 6.666,
      "step": 5096
    },
    {
      "epoch": 0.2762059620596206,
      "step": 5096,
      "training_loss": 5.433704853057861
    },
    {
      "epoch": 0.276260162601626,
      "step": 5097,
      "training_loss": 6.801220417022705
    },
    {
      "epoch": 0.27631436314363145,
      "step": 5098,
      "training_loss": 7.806066036224365
    },
    {
      "epoch": 0.27636856368563684,
      "step": 5099,
      "training_loss": 4.798552989959717
    },
    {
      "epoch": 0.2764227642276423,
      "grad_norm": 42.81672668457031,
      "learning_rate": 1e-05,
      "loss": 6.2099,
      "step": 5100
    },
    {
      "epoch": 0.2764227642276423,
      "step": 5100,
      "training_loss": 5.2487263679504395
    },
    {
      "epoch": 0.27647696476964767,
      "step": 5101,
      "training_loss": 7.74701452255249
    },
    {
      "epoch": 0.2765311653116531,
      "step": 5102,
      "training_loss": 6.846710681915283
    },
    {
      "epoch": 0.27658536585365856,
      "step": 5103,
      "training_loss": 4.7576189041137695
    },
    {
      "epoch": 0.27663956639566395,
      "grad_norm": 22.564355850219727,
      "learning_rate": 1e-05,
      "loss": 6.15,
      "step": 5104
    },
    {
      "epoch": 0.27663956639566395,
      "step": 5104,
      "training_loss": 4.055253982543945
    },
    {
      "epoch": 0.2766937669376694,
      "step": 5105,
      "training_loss": 7.487155437469482
    },
    {
      "epoch": 0.2767479674796748,
      "step": 5106,
      "training_loss": 8.462461471557617
    },
    {
      "epoch": 0.27680216802168023,
      "step": 5107,
      "training_loss": 6.205909252166748
    },
    {
      "epoch": 0.2768563685636856,
      "grad_norm": 22.684247970581055,
      "learning_rate": 1e-05,
      "loss": 6.5527,
      "step": 5108
    },
    {
      "epoch": 0.2768563685636856,
      "step": 5108,
      "training_loss": 6.215158939361572
    },
    {
      "epoch": 0.27691056910569106,
      "step": 5109,
      "training_loss": 7.340853691101074
    },
    {
      "epoch": 0.27696476964769645,
      "step": 5110,
      "training_loss": 7.656935691833496
    },
    {
      "epoch": 0.2770189701897019,
      "step": 5111,
      "training_loss": 6.677061557769775
    },
    {
      "epoch": 0.27707317073170734,
      "grad_norm": 16.870849609375,
      "learning_rate": 1e-05,
      "loss": 6.9725,
      "step": 5112
    },
    {
      "epoch": 0.27707317073170734,
      "step": 5112,
      "training_loss": 7.626512050628662
    },
    {
      "epoch": 0.27712737127371273,
      "step": 5113,
      "training_loss": 4.525638103485107
    },
    {
      "epoch": 0.2771815718157182,
      "step": 5114,
      "training_loss": 6.450981140136719
    },
    {
      "epoch": 0.27723577235772356,
      "step": 5115,
      "training_loss": 7.713808059692383
    },
    {
      "epoch": 0.277289972899729,
      "grad_norm": 21.2799129486084,
      "learning_rate": 1e-05,
      "loss": 6.5792,
      "step": 5116
    },
    {
      "epoch": 0.277289972899729,
      "step": 5116,
      "training_loss": 7.57472562789917
    },
    {
      "epoch": 0.2773441734417344,
      "step": 5117,
      "training_loss": 7.269898891448975
    },
    {
      "epoch": 0.27739837398373984,
      "step": 5118,
      "training_loss": 7.2742390632629395
    },
    {
      "epoch": 0.27745257452574523,
      "step": 5119,
      "training_loss": 5.78804349899292
    },
    {
      "epoch": 0.2775067750677507,
      "grad_norm": 27.47576141357422,
      "learning_rate": 1e-05,
      "loss": 6.9767,
      "step": 5120
    },
    {
      "epoch": 0.2775067750677507,
      "step": 5120,
      "training_loss": 3.971553325653076
    },
    {
      "epoch": 0.2775609756097561,
      "step": 5121,
      "training_loss": 7.555501461029053
    },
    {
      "epoch": 0.2776151761517615,
      "step": 5122,
      "training_loss": 7.695818901062012
    },
    {
      "epoch": 0.27766937669376696,
      "step": 5123,
      "training_loss": 7.948122501373291
    },
    {
      "epoch": 0.27772357723577235,
      "grad_norm": 37.73394012451172,
      "learning_rate": 1e-05,
      "loss": 6.7927,
      "step": 5124
    },
    {
      "epoch": 0.27772357723577235,
      "step": 5124,
      "training_loss": 6.556992530822754
    },
    {
      "epoch": 0.2777777777777778,
      "step": 5125,
      "training_loss": 6.840548038482666
    },
    {
      "epoch": 0.2778319783197832,
      "step": 5126,
      "training_loss": 6.6039042472839355
    },
    {
      "epoch": 0.2778861788617886,
      "step": 5127,
      "training_loss": 6.574808120727539
    },
    {
      "epoch": 0.277940379403794,
      "grad_norm": 21.144800186157227,
      "learning_rate": 1e-05,
      "loss": 6.6441,
      "step": 5128
    },
    {
      "epoch": 0.277940379403794,
      "step": 5128,
      "training_loss": 6.023215293884277
    },
    {
      "epoch": 0.27799457994579946,
      "step": 5129,
      "training_loss": 6.987311363220215
    },
    {
      "epoch": 0.2780487804878049,
      "step": 5130,
      "training_loss": 7.173766613006592
    },
    {
      "epoch": 0.2781029810298103,
      "step": 5131,
      "training_loss": 6.386744499206543
    },
    {
      "epoch": 0.27815718157181574,
      "grad_norm": 21.99808692932129,
      "learning_rate": 1e-05,
      "loss": 6.6428,
      "step": 5132
    },
    {
      "epoch": 0.27815718157181574,
      "step": 5132,
      "training_loss": 3.740746259689331
    },
    {
      "epoch": 0.2782113821138211,
      "step": 5133,
      "training_loss": 6.378673076629639
    },
    {
      "epoch": 0.27826558265582657,
      "step": 5134,
      "training_loss": 7.0471954345703125
    },
    {
      "epoch": 0.27831978319783196,
      "step": 5135,
      "training_loss": 4.465422630310059
    },
    {
      "epoch": 0.2783739837398374,
      "grad_norm": 28.879209518432617,
      "learning_rate": 1e-05,
      "loss": 5.408,
      "step": 5136
    },
    {
      "epoch": 0.2783739837398374,
      "step": 5136,
      "training_loss": 7.169620990753174
    },
    {
      "epoch": 0.2784281842818428,
      "step": 5137,
      "training_loss": 6.835855960845947
    },
    {
      "epoch": 0.27848238482384824,
      "step": 5138,
      "training_loss": 6.894830226898193
    },
    {
      "epoch": 0.2785365853658537,
      "step": 5139,
      "training_loss": 6.159998893737793
    },
    {
      "epoch": 0.2785907859078591,
      "grad_norm": 24.268579483032227,
      "learning_rate": 1e-05,
      "loss": 6.7651,
      "step": 5140
    },
    {
      "epoch": 0.2785907859078591,
      "step": 5140,
      "training_loss": 5.784419059753418
    },
    {
      "epoch": 0.2786449864498645,
      "step": 5141,
      "training_loss": 7.549221515655518
    },
    {
      "epoch": 0.2786991869918699,
      "step": 5142,
      "training_loss": 8.798530578613281
    },
    {
      "epoch": 0.27875338753387535,
      "step": 5143,
      "training_loss": 6.7353291511535645
    },
    {
      "epoch": 0.27880758807588074,
      "grad_norm": 20.075687408447266,
      "learning_rate": 1e-05,
      "loss": 7.2169,
      "step": 5144
    },
    {
      "epoch": 0.27880758807588074,
      "step": 5144,
      "training_loss": 7.566021919250488
    },
    {
      "epoch": 0.2788617886178862,
      "step": 5145,
      "training_loss": 7.280628204345703
    },
    {
      "epoch": 0.2789159891598916,
      "step": 5146,
      "training_loss": 6.950271129608154
    },
    {
      "epoch": 0.278970189701897,
      "step": 5147,
      "training_loss": 5.411248683929443
    },
    {
      "epoch": 0.27902439024390246,
      "grad_norm": 21.45005989074707,
      "learning_rate": 1e-05,
      "loss": 6.802,
      "step": 5148
    },
    {
      "epoch": 0.27902439024390246,
      "step": 5148,
      "training_loss": 6.991097450256348
    },
    {
      "epoch": 0.27907859078590785,
      "step": 5149,
      "training_loss": 7.274267673492432
    },
    {
      "epoch": 0.2791327913279133,
      "step": 5150,
      "training_loss": 6.727305889129639
    },
    {
      "epoch": 0.2791869918699187,
      "step": 5151,
      "training_loss": 5.354216575622559
    },
    {
      "epoch": 0.27924119241192413,
      "grad_norm": 22.670394897460938,
      "learning_rate": 1e-05,
      "loss": 6.5867,
      "step": 5152
    },
    {
      "epoch": 0.27924119241192413,
      "step": 5152,
      "training_loss": 6.7358832359313965
    },
    {
      "epoch": 0.2792953929539295,
      "step": 5153,
      "training_loss": 6.445342540740967
    },
    {
      "epoch": 0.27934959349593497,
      "step": 5154,
      "training_loss": 7.842922687530518
    },
    {
      "epoch": 0.27940379403794036,
      "step": 5155,
      "training_loss": 7.114840984344482
    },
    {
      "epoch": 0.2794579945799458,
      "grad_norm": 17.341154098510742,
      "learning_rate": 1e-05,
      "loss": 7.0347,
      "step": 5156
    },
    {
      "epoch": 0.2794579945799458,
      "step": 5156,
      "training_loss": 7.133351802825928
    },
    {
      "epoch": 0.27951219512195125,
      "step": 5157,
      "training_loss": 5.844870090484619
    },
    {
      "epoch": 0.27956639566395663,
      "step": 5158,
      "training_loss": 7.485314846038818
    },
    {
      "epoch": 0.2796205962059621,
      "step": 5159,
      "training_loss": 6.714995384216309
    },
    {
      "epoch": 0.27967479674796747,
      "grad_norm": 16.67828941345215,
      "learning_rate": 1e-05,
      "loss": 6.7946,
      "step": 5160
    },
    {
      "epoch": 0.27967479674796747,
      "step": 5160,
      "training_loss": 6.3726372718811035
    },
    {
      "epoch": 0.2797289972899729,
      "step": 5161,
      "training_loss": 7.45034646987915
    },
    {
      "epoch": 0.2797831978319783,
      "step": 5162,
      "training_loss": 7.450637340545654
    },
    {
      "epoch": 0.27983739837398375,
      "step": 5163,
      "training_loss": 7.76308536529541
    },
    {
      "epoch": 0.27989159891598914,
      "grad_norm": 19.63082504272461,
      "learning_rate": 1e-05,
      "loss": 7.2592,
      "step": 5164
    },
    {
      "epoch": 0.27989159891598914,
      "step": 5164,
      "training_loss": 5.989553928375244
    },
    {
      "epoch": 0.2799457994579946,
      "step": 5165,
      "training_loss": 7.993453502655029
    },
    {
      "epoch": 0.28,
      "step": 5166,
      "training_loss": 7.123623371124268
    },
    {
      "epoch": 0.2800542005420054,
      "step": 5167,
      "training_loss": 7.363927364349365
    },
    {
      "epoch": 0.28010840108401086,
      "grad_norm": 41.5241813659668,
      "learning_rate": 1e-05,
      "loss": 7.1176,
      "step": 5168
    },
    {
      "epoch": 0.28010840108401086,
      "step": 5168,
      "training_loss": 6.673237323760986
    },
    {
      "epoch": 0.28016260162601625,
      "step": 5169,
      "training_loss": 7.022278785705566
    },
    {
      "epoch": 0.2802168021680217,
      "step": 5170,
      "training_loss": 5.6270599365234375
    },
    {
      "epoch": 0.2802710027100271,
      "step": 5171,
      "training_loss": 6.243344783782959
    },
    {
      "epoch": 0.28032520325203253,
      "grad_norm": 32.25327682495117,
      "learning_rate": 1e-05,
      "loss": 6.3915,
      "step": 5172
    },
    {
      "epoch": 0.28032520325203253,
      "step": 5172,
      "training_loss": 6.562553882598877
    },
    {
      "epoch": 0.2803794037940379,
      "step": 5173,
      "training_loss": 6.111568927764893
    },
    {
      "epoch": 0.28043360433604336,
      "step": 5174,
      "training_loss": 7.07467794418335
    },
    {
      "epoch": 0.2804878048780488,
      "step": 5175,
      "training_loss": 6.762960433959961
    },
    {
      "epoch": 0.2805420054200542,
      "grad_norm": 33.26948165893555,
      "learning_rate": 1e-05,
      "loss": 6.6279,
      "step": 5176
    },
    {
      "epoch": 0.2805420054200542,
      "step": 5176,
      "training_loss": 5.911686897277832
    },
    {
      "epoch": 0.28059620596205964,
      "step": 5177,
      "training_loss": 7.487447738647461
    },
    {
      "epoch": 0.28065040650406503,
      "step": 5178,
      "training_loss": 6.62183952331543
    },
    {
      "epoch": 0.2807046070460705,
      "step": 5179,
      "training_loss": 7.121008396148682
    },
    {
      "epoch": 0.28075880758807586,
      "grad_norm": 37.367462158203125,
      "learning_rate": 1e-05,
      "loss": 6.7855,
      "step": 5180
    },
    {
      "epoch": 0.28075880758807586,
      "step": 5180,
      "training_loss": 7.644127368927002
    },
    {
      "epoch": 0.2808130081300813,
      "step": 5181,
      "training_loss": 7.107113838195801
    },
    {
      "epoch": 0.2808672086720867,
      "step": 5182,
      "training_loss": 6.588601589202881
    },
    {
      "epoch": 0.28092140921409214,
      "step": 5183,
      "training_loss": 6.829239368438721
    },
    {
      "epoch": 0.2809756097560976,
      "grad_norm": 37.35757064819336,
      "learning_rate": 1e-05,
      "loss": 7.0423,
      "step": 5184
    },
    {
      "epoch": 0.2809756097560976,
      "step": 5184,
      "training_loss": 4.624415397644043
    },
    {
      "epoch": 0.281029810298103,
      "step": 5185,
      "training_loss": 6.896310806274414
    },
    {
      "epoch": 0.2810840108401084,
      "step": 5186,
      "training_loss": 7.160922527313232
    },
    {
      "epoch": 0.2811382113821138,
      "step": 5187,
      "training_loss": 5.826164245605469
    },
    {
      "epoch": 0.28119241192411926,
      "grad_norm": 22.8209228515625,
      "learning_rate": 1e-05,
      "loss": 6.127,
      "step": 5188
    },
    {
      "epoch": 0.28119241192411926,
      "step": 5188,
      "training_loss": 7.945351600646973
    },
    {
      "epoch": 0.28124661246612465,
      "step": 5189,
      "training_loss": 5.796289443969727
    },
    {
      "epoch": 0.2813008130081301,
      "step": 5190,
      "training_loss": 6.830871105194092
    },
    {
      "epoch": 0.2813550135501355,
      "step": 5191,
      "training_loss": 7.005458354949951
    },
    {
      "epoch": 0.2814092140921409,
      "grad_norm": 18.31974220275879,
      "learning_rate": 1e-05,
      "loss": 6.8945,
      "step": 5192
    },
    {
      "epoch": 0.2814092140921409,
      "step": 5192,
      "training_loss": 7.952748775482178
    },
    {
      "epoch": 0.2814634146341463,
      "step": 5193,
      "training_loss": 7.311100482940674
    },
    {
      "epoch": 0.28151761517615176,
      "step": 5194,
      "training_loss": 6.510183334350586
    },
    {
      "epoch": 0.2815718157181572,
      "step": 5195,
      "training_loss": 7.906668663024902
    },
    {
      "epoch": 0.2816260162601626,
      "grad_norm": 77.68608856201172,
      "learning_rate": 1e-05,
      "loss": 7.4202,
      "step": 5196
    },
    {
      "epoch": 0.2816260162601626,
      "step": 5196,
      "training_loss": 7.115444183349609
    },
    {
      "epoch": 0.28168021680216804,
      "step": 5197,
      "training_loss": 6.917321681976318
    },
    {
      "epoch": 0.2817344173441734,
      "step": 5198,
      "training_loss": 6.136656761169434
    },
    {
      "epoch": 0.28178861788617887,
      "step": 5199,
      "training_loss": 7.709949493408203
    },
    {
      "epoch": 0.28184281842818426,
      "grad_norm": 22.127769470214844,
      "learning_rate": 1e-05,
      "loss": 6.9698,
      "step": 5200
    },
    {
      "epoch": 0.28184281842818426,
      "step": 5200,
      "training_loss": 5.764556884765625
    },
    {
      "epoch": 0.2818970189701897,
      "step": 5201,
      "training_loss": 7.877197265625
    },
    {
      "epoch": 0.2819512195121951,
      "step": 5202,
      "training_loss": 6.7823805809021
    },
    {
      "epoch": 0.28200542005420054,
      "step": 5203,
      "training_loss": 8.165361404418945
    },
    {
      "epoch": 0.282059620596206,
      "grad_norm": 25.326120376586914,
      "learning_rate": 1e-05,
      "loss": 7.1474,
      "step": 5204
    },
    {
      "epoch": 0.282059620596206,
      "step": 5204,
      "training_loss": 6.388025760650635
    },
    {
      "epoch": 0.2821138211382114,
      "step": 5205,
      "training_loss": 6.351504325866699
    },
    {
      "epoch": 0.2821680216802168,
      "step": 5206,
      "training_loss": 7.861251354217529
    },
    {
      "epoch": 0.2822222222222222,
      "step": 5207,
      "training_loss": 7.390162944793701
    },
    {
      "epoch": 0.28227642276422765,
      "grad_norm": 16.987268447875977,
      "learning_rate": 1e-05,
      "loss": 6.9977,
      "step": 5208
    },
    {
      "epoch": 0.28227642276422765,
      "step": 5208,
      "training_loss": 7.283344268798828
    },
    {
      "epoch": 0.28233062330623304,
      "step": 5209,
      "training_loss": 7.125881671905518
    },
    {
      "epoch": 0.2823848238482385,
      "step": 5210,
      "training_loss": 7.1070075035095215
    },
    {
      "epoch": 0.2824390243902439,
      "step": 5211,
      "training_loss": 7.021738052368164
    },
    {
      "epoch": 0.2824932249322493,
      "grad_norm": 27.06585121154785,
      "learning_rate": 1e-05,
      "loss": 7.1345,
      "step": 5212
    },
    {
      "epoch": 0.2824932249322493,
      "step": 5212,
      "training_loss": 6.187038421630859
    },
    {
      "epoch": 0.28254742547425477,
      "step": 5213,
      "training_loss": 6.556807994842529
    },
    {
      "epoch": 0.28260162601626015,
      "step": 5214,
      "training_loss": 7.579919338226318
    },
    {
      "epoch": 0.2826558265582656,
      "step": 5215,
      "training_loss": 7.545155048370361
    },
    {
      "epoch": 0.282710027100271,
      "grad_norm": 31.178661346435547,
      "learning_rate": 1e-05,
      "loss": 6.9672,
      "step": 5216
    },
    {
      "epoch": 0.282710027100271,
      "step": 5216,
      "training_loss": 7.667713642120361
    },
    {
      "epoch": 0.28276422764227643,
      "step": 5217,
      "training_loss": 7.4069037437438965
    },
    {
      "epoch": 0.2828184281842818,
      "step": 5218,
      "training_loss": 4.635534763336182
    },
    {
      "epoch": 0.28287262872628727,
      "step": 5219,
      "training_loss": 5.5687665939331055
    },
    {
      "epoch": 0.28292682926829266,
      "grad_norm": 23.188932418823242,
      "learning_rate": 1e-05,
      "loss": 6.3197,
      "step": 5220
    },
    {
      "epoch": 0.28292682926829266,
      "step": 5220,
      "training_loss": 4.1709675788879395
    },
    {
      "epoch": 0.2829810298102981,
      "step": 5221,
      "training_loss": 6.814461708068848
    },
    {
      "epoch": 0.28303523035230355,
      "step": 5222,
      "training_loss": 7.102078437805176
    },
    {
      "epoch": 0.28308943089430894,
      "step": 5223,
      "training_loss": 7.401483058929443
    },
    {
      "epoch": 0.2831436314363144,
      "grad_norm": 31.887855529785156,
      "learning_rate": 1e-05,
      "loss": 6.3722,
      "step": 5224
    },
    {
      "epoch": 0.2831436314363144,
      "step": 5224,
      "training_loss": 7.350093364715576
    },
    {
      "epoch": 0.28319783197831977,
      "step": 5225,
      "training_loss": 6.1855034828186035
    },
    {
      "epoch": 0.2832520325203252,
      "step": 5226,
      "training_loss": 7.006816864013672
    },
    {
      "epoch": 0.2833062330623306,
      "step": 5227,
      "training_loss": 6.887613773345947
    },
    {
      "epoch": 0.28336043360433605,
      "grad_norm": 42.62064743041992,
      "learning_rate": 1e-05,
      "loss": 6.8575,
      "step": 5228
    },
    {
      "epoch": 0.28336043360433605,
      "step": 5228,
      "training_loss": 6.0656843185424805
    },
    {
      "epoch": 0.28341463414634144,
      "step": 5229,
      "training_loss": 7.7234110832214355
    },
    {
      "epoch": 0.2834688346883469,
      "step": 5230,
      "training_loss": 6.922615051269531
    },
    {
      "epoch": 0.2835230352303523,
      "step": 5231,
      "training_loss": 6.5160908699035645
    },
    {
      "epoch": 0.2835772357723577,
      "grad_norm": 17.85529136657715,
      "learning_rate": 1e-05,
      "loss": 6.807,
      "step": 5232
    },
    {
      "epoch": 0.2835772357723577,
      "step": 5232,
      "training_loss": 6.273229598999023
    },
    {
      "epoch": 0.28363143631436316,
      "step": 5233,
      "training_loss": 6.816527366638184
    },
    {
      "epoch": 0.28368563685636855,
      "step": 5234,
      "training_loss": 8.570627212524414
    },
    {
      "epoch": 0.283739837398374,
      "step": 5235,
      "training_loss": 6.764116287231445
    },
    {
      "epoch": 0.2837940379403794,
      "grad_norm": 21.80604362487793,
      "learning_rate": 1e-05,
      "loss": 7.1061,
      "step": 5236
    },
    {
      "epoch": 0.2837940379403794,
      "step": 5236,
      "training_loss": 6.635782241821289
    },
    {
      "epoch": 0.28384823848238483,
      "step": 5237,
      "training_loss": 6.256566524505615
    },
    {
      "epoch": 0.2839024390243902,
      "step": 5238,
      "training_loss": 5.38894510269165
    },
    {
      "epoch": 0.28395663956639566,
      "step": 5239,
      "training_loss": 7.5237908363342285
    },
    {
      "epoch": 0.2840108401084011,
      "grad_norm": 22.22565269470215,
      "learning_rate": 1e-05,
      "loss": 6.4513,
      "step": 5240
    },
    {
      "epoch": 0.2840108401084011,
      "step": 5240,
      "training_loss": 6.651532173156738
    },
    {
      "epoch": 0.2840650406504065,
      "step": 5241,
      "training_loss": 6.7688117027282715
    },
    {
      "epoch": 0.28411924119241194,
      "step": 5242,
      "training_loss": 7.012316703796387
    },
    {
      "epoch": 0.28417344173441733,
      "step": 5243,
      "training_loss": 7.42957878112793
    },
    {
      "epoch": 0.2842276422764228,
      "grad_norm": 18.078596115112305,
      "learning_rate": 1e-05,
      "loss": 6.9656,
      "step": 5244
    },
    {
      "epoch": 0.2842276422764228,
      "step": 5244,
      "training_loss": 7.1429762840271
    },
    {
      "epoch": 0.28428184281842817,
      "step": 5245,
      "training_loss": 6.842650413513184
    },
    {
      "epoch": 0.2843360433604336,
      "step": 5246,
      "training_loss": 7.20407772064209
    },
    {
      "epoch": 0.284390243902439,
      "step": 5247,
      "training_loss": 6.572466850280762
    },
    {
      "epoch": 0.28444444444444444,
      "grad_norm": 25.562299728393555,
      "learning_rate": 1e-05,
      "loss": 6.9405,
      "step": 5248
    },
    {
      "epoch": 0.28444444444444444,
      "step": 5248,
      "training_loss": 6.074676990509033
    },
    {
      "epoch": 0.2844986449864499,
      "step": 5249,
      "training_loss": 7.043507099151611
    },
    {
      "epoch": 0.2845528455284553,
      "step": 5250,
      "training_loss": 6.2238335609436035
    },
    {
      "epoch": 0.2846070460704607,
      "step": 5251,
      "training_loss": 6.5402727127075195
    },
    {
      "epoch": 0.2846612466124661,
      "grad_norm": 23.977296829223633,
      "learning_rate": 1e-05,
      "loss": 6.4706,
      "step": 5252
    },
    {
      "epoch": 0.2846612466124661,
      "step": 5252,
      "training_loss": 6.156836032867432
    },
    {
      "epoch": 0.28471544715447156,
      "step": 5253,
      "training_loss": 6.554589748382568
    },
    {
      "epoch": 0.28476964769647695,
      "step": 5254,
      "training_loss": 5.548050403594971
    },
    {
      "epoch": 0.2848238482384824,
      "step": 5255,
      "training_loss": 8.805357933044434
    },
    {
      "epoch": 0.2848780487804878,
      "grad_norm": 30.57151222229004,
      "learning_rate": 1e-05,
      "loss": 6.7662,
      "step": 5256
    },
    {
      "epoch": 0.2848780487804878,
      "step": 5256,
      "training_loss": 6.236405372619629
    },
    {
      "epoch": 0.2849322493224932,
      "step": 5257,
      "training_loss": 6.987600326538086
    },
    {
      "epoch": 0.28498644986449867,
      "step": 5258,
      "training_loss": 5.1495513916015625
    },
    {
      "epoch": 0.28504065040650406,
      "step": 5259,
      "training_loss": 7.092639446258545
    },
    {
      "epoch": 0.2850948509485095,
      "grad_norm": 36.871376037597656,
      "learning_rate": 1e-05,
      "loss": 6.3665,
      "step": 5260
    },
    {
      "epoch": 0.2850948509485095,
      "step": 5260,
      "training_loss": 8.083032608032227
    },
    {
      "epoch": 0.2851490514905149,
      "step": 5261,
      "training_loss": 4.392541885375977
    },
    {
      "epoch": 0.28520325203252034,
      "step": 5262,
      "training_loss": 7.419931888580322
    },
    {
      "epoch": 0.2852574525745257,
      "step": 5263,
      "training_loss": 5.5116682052612305
    },
    {
      "epoch": 0.28531165311653117,
      "grad_norm": 23.41350555419922,
      "learning_rate": 1e-05,
      "loss": 6.3518,
      "step": 5264
    },
    {
      "epoch": 0.28531165311653117,
      "step": 5264,
      "training_loss": 7.548313140869141
    },
    {
      "epoch": 0.28536585365853656,
      "step": 5265,
      "training_loss": 6.520968914031982
    },
    {
      "epoch": 0.285420054200542,
      "step": 5266,
      "training_loss": 6.9198174476623535
    },
    {
      "epoch": 0.28547425474254745,
      "step": 5267,
      "training_loss": 6.782238006591797
    },
    {
      "epoch": 0.28552845528455284,
      "grad_norm": 17.492534637451172,
      "learning_rate": 1e-05,
      "loss": 6.9428,
      "step": 5268
    },
    {
      "epoch": 0.28552845528455284,
      "step": 5268,
      "training_loss": 7.270510196685791
    },
    {
      "epoch": 0.2855826558265583,
      "step": 5269,
      "training_loss": 7.786079406738281
    },
    {
      "epoch": 0.2856368563685637,
      "step": 5270,
      "training_loss": 7.101654052734375
    },
    {
      "epoch": 0.2856910569105691,
      "step": 5271,
      "training_loss": 7.642735004425049
    },
    {
      "epoch": 0.2857452574525745,
      "grad_norm": 24.60482406616211,
      "learning_rate": 1e-05,
      "loss": 7.4502,
      "step": 5272
    },
    {
      "epoch": 0.2857452574525745,
      "step": 5272,
      "training_loss": 6.340524196624756
    },
    {
      "epoch": 0.28579945799457995,
      "step": 5273,
      "training_loss": 7.765787124633789
    },
    {
      "epoch": 0.28585365853658534,
      "step": 5274,
      "training_loss": 8.387922286987305
    },
    {
      "epoch": 0.2859078590785908,
      "step": 5275,
      "training_loss": 6.0428853034973145
    },
    {
      "epoch": 0.28596205962059623,
      "grad_norm": 19.092060089111328,
      "learning_rate": 1e-05,
      "loss": 7.1343,
      "step": 5276
    },
    {
      "epoch": 0.28596205962059623,
      "step": 5276,
      "training_loss": 7.413949966430664
    },
    {
      "epoch": 0.2860162601626016,
      "step": 5277,
      "training_loss": 6.7883172035217285
    },
    {
      "epoch": 0.28607046070460707,
      "step": 5278,
      "training_loss": 5.374650955200195
    },
    {
      "epoch": 0.28612466124661246,
      "step": 5279,
      "training_loss": 7.0377421379089355
    },
    {
      "epoch": 0.2861788617886179,
      "grad_norm": 20.909955978393555,
      "learning_rate": 1e-05,
      "loss": 6.6537,
      "step": 5280
    },
    {
      "epoch": 0.2861788617886179,
      "step": 5280,
      "training_loss": 7.689935684204102
    },
    {
      "epoch": 0.2862330623306233,
      "step": 5281,
      "training_loss": 6.01671838760376
    },
    {
      "epoch": 0.28628726287262873,
      "step": 5282,
      "training_loss": 7.1751227378845215
    },
    {
      "epoch": 0.2863414634146341,
      "step": 5283,
      "training_loss": 7.382654190063477
    },
    {
      "epoch": 0.28639566395663957,
      "grad_norm": 27.168909072875977,
      "learning_rate": 1e-05,
      "loss": 7.0661,
      "step": 5284
    },
    {
      "epoch": 0.28639566395663957,
      "step": 5284,
      "training_loss": 6.783395767211914
    },
    {
      "epoch": 0.286449864498645,
      "step": 5285,
      "training_loss": 6.651967525482178
    },
    {
      "epoch": 0.2865040650406504,
      "step": 5286,
      "training_loss": 6.406911373138428
    },
    {
      "epoch": 0.28655826558265585,
      "step": 5287,
      "training_loss": 6.967129230499268
    },
    {
      "epoch": 0.28661246612466124,
      "grad_norm": 19.180513381958008,
      "learning_rate": 1e-05,
      "loss": 6.7024,
      "step": 5288
    },
    {
      "epoch": 0.28661246612466124,
      "step": 5288,
      "training_loss": 6.628426551818848
    },
    {
      "epoch": 0.2866666666666667,
      "step": 5289,
      "training_loss": 6.829556465148926
    },
    {
      "epoch": 0.28672086720867207,
      "step": 5290,
      "training_loss": 9.073548316955566
    },
    {
      "epoch": 0.2867750677506775,
      "step": 5291,
      "training_loss": 6.309055328369141
    },
    {
      "epoch": 0.2868292682926829,
      "grad_norm": 21.336000442504883,
      "learning_rate": 1e-05,
      "loss": 7.2101,
      "step": 5292
    },
    {
      "epoch": 0.2868292682926829,
      "step": 5292,
      "training_loss": 7.1395792961120605
    },
    {
      "epoch": 0.28688346883468835,
      "step": 5293,
      "training_loss": 7.018993377685547
    },
    {
      "epoch": 0.2869376693766938,
      "step": 5294,
      "training_loss": 7.349099159240723
    },
    {
      "epoch": 0.2869918699186992,
      "step": 5295,
      "training_loss": 7.6808366775512695
    },
    {
      "epoch": 0.2870460704607046,
      "grad_norm": 27.94015884399414,
      "learning_rate": 1e-05,
      "loss": 7.2971,
      "step": 5296
    },
    {
      "epoch": 0.2870460704607046,
      "step": 5296,
      "training_loss": 6.661665916442871
    },
    {
      "epoch": 0.28710027100271,
      "step": 5297,
      "training_loss": 3.640577793121338
    },
    {
      "epoch": 0.28715447154471546,
      "step": 5298,
      "training_loss": 7.739356517791748
    },
    {
      "epoch": 0.28720867208672085,
      "step": 5299,
      "training_loss": 6.204237937927246
    },
    {
      "epoch": 0.2872628726287263,
      "grad_norm": 48.14756393432617,
      "learning_rate": 1e-05,
      "loss": 6.0615,
      "step": 5300
    },
    {
      "epoch": 0.2872628726287263,
      "step": 5300,
      "training_loss": 6.85186243057251
    },
    {
      "epoch": 0.2873170731707317,
      "step": 5301,
      "training_loss": 7.671169757843018
    },
    {
      "epoch": 0.28737127371273713,
      "step": 5302,
      "training_loss": 8.179996490478516
    },
    {
      "epoch": 0.2874254742547426,
      "step": 5303,
      "training_loss": 6.823071002960205
    },
    {
      "epoch": 0.28747967479674796,
      "grad_norm": 40.712867736816406,
      "learning_rate": 1e-05,
      "loss": 7.3815,
      "step": 5304
    },
    {
      "epoch": 0.28747967479674796,
      "step": 5304,
      "training_loss": 6.155437469482422
    },
    {
      "epoch": 0.2875338753387534,
      "step": 5305,
      "training_loss": 6.137989044189453
    },
    {
      "epoch": 0.2875880758807588,
      "step": 5306,
      "training_loss": 7.15587854385376
    },
    {
      "epoch": 0.28764227642276424,
      "step": 5307,
      "training_loss": 8.414701461791992
    },
    {
      "epoch": 0.28769647696476963,
      "grad_norm": 78.17243194580078,
      "learning_rate": 1e-05,
      "loss": 6.966,
      "step": 5308
    },
    {
      "epoch": 0.28769647696476963,
      "step": 5308,
      "training_loss": 6.297510147094727
    },
    {
      "epoch": 0.2877506775067751,
      "step": 5309,
      "training_loss": 6.956943988800049
    },
    {
      "epoch": 0.28780487804878047,
      "step": 5310,
      "training_loss": 7.547102928161621
    },
    {
      "epoch": 0.2878590785907859,
      "step": 5311,
      "training_loss": 3.8531954288482666
    },
    {
      "epoch": 0.28791327913279136,
      "grad_norm": 26.05545997619629,
      "learning_rate": 1e-05,
      "loss": 6.1637,
      "step": 5312
    },
    {
      "epoch": 0.28791327913279136,
      "step": 5312,
      "training_loss": 7.009917259216309
    },
    {
      "epoch": 0.28796747967479674,
      "step": 5313,
      "training_loss": 6.706622123718262
    },
    {
      "epoch": 0.2880216802168022,
      "step": 5314,
      "training_loss": 6.684818744659424
    },
    {
      "epoch": 0.2880758807588076,
      "step": 5315,
      "training_loss": 5.638955116271973
    },
    {
      "epoch": 0.288130081300813,
      "grad_norm": 21.3287353515625,
      "learning_rate": 1e-05,
      "loss": 6.5101,
      "step": 5316
    },
    {
      "epoch": 0.288130081300813,
      "step": 5316,
      "training_loss": 7.250980377197266
    },
    {
      "epoch": 0.2881842818428184,
      "step": 5317,
      "training_loss": 6.224137783050537
    },
    {
      "epoch": 0.28823848238482386,
      "step": 5318,
      "training_loss": 6.468808650970459
    },
    {
      "epoch": 0.28829268292682925,
      "step": 5319,
      "training_loss": 6.701232433319092
    },
    {
      "epoch": 0.2883468834688347,
      "grad_norm": 18.970199584960938,
      "learning_rate": 1e-05,
      "loss": 6.6613,
      "step": 5320
    },
    {
      "epoch": 0.2883468834688347,
      "step": 5320,
      "training_loss": 6.83026123046875
    },
    {
      "epoch": 0.2884010840108401,
      "step": 5321,
      "training_loss": 6.868050575256348
    },
    {
      "epoch": 0.2884552845528455,
      "step": 5322,
      "training_loss": 7.054967403411865
    },
    {
      "epoch": 0.28850948509485097,
      "step": 5323,
      "training_loss": 7.402952194213867
    },
    {
      "epoch": 0.28856368563685636,
      "grad_norm": 22.500350952148438,
      "learning_rate": 1e-05,
      "loss": 7.0391,
      "step": 5324
    },
    {
      "epoch": 0.28856368563685636,
      "step": 5324,
      "training_loss": 7.40770959854126
    },
    {
      "epoch": 0.2886178861788618,
      "step": 5325,
      "training_loss": 7.596284866333008
    },
    {
      "epoch": 0.2886720867208672,
      "step": 5326,
      "training_loss": 4.44111967086792
    },
    {
      "epoch": 0.28872628726287264,
      "step": 5327,
      "training_loss": 7.2621846199035645
    },
    {
      "epoch": 0.288780487804878,
      "grad_norm": 27.319278717041016,
      "learning_rate": 1e-05,
      "loss": 6.6768,
      "step": 5328
    },
    {
      "epoch": 0.288780487804878,
      "step": 5328,
      "training_loss": 6.463940620422363
    },
    {
      "epoch": 0.2888346883468835,
      "step": 5329,
      "training_loss": 8.231742858886719
    },
    {
      "epoch": 0.28888888888888886,
      "step": 5330,
      "training_loss": 7.187008857727051
    },
    {
      "epoch": 0.2889430894308943,
      "step": 5331,
      "training_loss": 6.863292217254639
    },
    {
      "epoch": 0.28899728997289975,
      "grad_norm": 17.677885055541992,
      "learning_rate": 1e-05,
      "loss": 7.1865,
      "step": 5332
    },
    {
      "epoch": 0.28899728997289975,
      "step": 5332,
      "training_loss": 6.664202690124512
    },
    {
      "epoch": 0.28905149051490514,
      "step": 5333,
      "training_loss": 6.469142436981201
    },
    {
      "epoch": 0.2891056910569106,
      "step": 5334,
      "training_loss": 6.693777561187744
    },
    {
      "epoch": 0.289159891598916,
      "step": 5335,
      "training_loss": 7.494684219360352
    },
    {
      "epoch": 0.2892140921409214,
      "grad_norm": 20.342342376708984,
      "learning_rate": 1e-05,
      "loss": 6.8305,
      "step": 5336
    },
    {
      "epoch": 0.2892140921409214,
      "step": 5336,
      "training_loss": 7.786673545837402
    },
    {
      "epoch": 0.2892682926829268,
      "step": 5337,
      "training_loss": 4.48774528503418
    },
    {
      "epoch": 0.28932249322493225,
      "step": 5338,
      "training_loss": 7.496335983276367
    },
    {
      "epoch": 0.28937669376693764,
      "step": 5339,
      "training_loss": 6.379673480987549
    },
    {
      "epoch": 0.2894308943089431,
      "grad_norm": 41.221282958984375,
      "learning_rate": 1e-05,
      "loss": 6.5376,
      "step": 5340
    },
    {
      "epoch": 0.2894308943089431,
      "step": 5340,
      "training_loss": 6.394215106964111
    },
    {
      "epoch": 0.28948509485094853,
      "step": 5341,
      "training_loss": 7.209780693054199
    },
    {
      "epoch": 0.2895392953929539,
      "step": 5342,
      "training_loss": 6.103243350982666
    },
    {
      "epoch": 0.28959349593495937,
      "step": 5343,
      "training_loss": 5.598082542419434
    },
    {
      "epoch": 0.28964769647696476,
      "grad_norm": 37.81312561035156,
      "learning_rate": 1e-05,
      "loss": 6.3263,
      "step": 5344
    },
    {
      "epoch": 0.28964769647696476,
      "step": 5344,
      "training_loss": 7.411798000335693
    },
    {
      "epoch": 0.2897018970189702,
      "step": 5345,
      "training_loss": 7.15424919128418
    },
    {
      "epoch": 0.2897560975609756,
      "step": 5346,
      "training_loss": 7.311041355133057
    },
    {
      "epoch": 0.28981029810298103,
      "step": 5347,
      "training_loss": 6.543461799621582
    },
    {
      "epoch": 0.2898644986449864,
      "grad_norm": 17.882768630981445,
      "learning_rate": 1e-05,
      "loss": 7.1051,
      "step": 5348
    },
    {
      "epoch": 0.2898644986449864,
      "step": 5348,
      "training_loss": 6.930213451385498
    },
    {
      "epoch": 0.28991869918699187,
      "step": 5349,
      "training_loss": 6.931403636932373
    },
    {
      "epoch": 0.2899728997289973,
      "step": 5350,
      "training_loss": 7.70989465713501
    },
    {
      "epoch": 0.2900271002710027,
      "step": 5351,
      "training_loss": 6.400733947753906
    },
    {
      "epoch": 0.29008130081300815,
      "grad_norm": 25.23141098022461,
      "learning_rate": 1e-05,
      "loss": 6.9931,
      "step": 5352
    },
    {
      "epoch": 0.29008130081300815,
      "step": 5352,
      "training_loss": 8.181900024414062
    },
    {
      "epoch": 0.29013550135501354,
      "step": 5353,
      "training_loss": 6.972903728485107
    },
    {
      "epoch": 0.290189701897019,
      "step": 5354,
      "training_loss": 6.9252400398254395
    },
    {
      "epoch": 0.29024390243902437,
      "step": 5355,
      "training_loss": 6.932626247406006
    },
    {
      "epoch": 0.2902981029810298,
      "grad_norm": 17.846702575683594,
      "learning_rate": 1e-05,
      "loss": 7.2532,
      "step": 5356
    },
    {
      "epoch": 0.2902981029810298,
      "step": 5356,
      "training_loss": 5.080899238586426
    },
    {
      "epoch": 0.2903523035230352,
      "step": 5357,
      "training_loss": 7.067171573638916
    },
    {
      "epoch": 0.29040650406504065,
      "step": 5358,
      "training_loss": 5.63410758972168
    },
    {
      "epoch": 0.2904607046070461,
      "step": 5359,
      "training_loss": 6.71927547454834
    },
    {
      "epoch": 0.2905149051490515,
      "grad_norm": 25.468624114990234,
      "learning_rate": 1e-05,
      "loss": 6.1254,
      "step": 5360
    },
    {
      "epoch": 0.2905149051490515,
      "step": 5360,
      "training_loss": 9.797452926635742
    },
    {
      "epoch": 0.29056910569105693,
      "step": 5361,
      "training_loss": 7.896750450134277
    },
    {
      "epoch": 0.2906233062330623,
      "step": 5362,
      "training_loss": 6.997118949890137
    },
    {
      "epoch": 0.29067750677506776,
      "step": 5363,
      "training_loss": 7.469674110412598
    },
    {
      "epoch": 0.29073170731707315,
      "grad_norm": 20.753643035888672,
      "learning_rate": 1e-05,
      "loss": 8.0402,
      "step": 5364
    },
    {
      "epoch": 0.29073170731707315,
      "step": 5364,
      "training_loss": 5.922784805297852
    },
    {
      "epoch": 0.2907859078590786,
      "step": 5365,
      "training_loss": 7.277478218078613
    },
    {
      "epoch": 0.290840108401084,
      "step": 5366,
      "training_loss": 6.92458438873291
    },
    {
      "epoch": 0.29089430894308943,
      "step": 5367,
      "training_loss": 6.937494277954102
    },
    {
      "epoch": 0.2909485094850949,
      "grad_norm": 17.402557373046875,
      "learning_rate": 1e-05,
      "loss": 6.7656,
      "step": 5368
    },
    {
      "epoch": 0.2909485094850949,
      "step": 5368,
      "training_loss": 7.587395668029785
    },
    {
      "epoch": 0.29100271002710026,
      "step": 5369,
      "training_loss": 8.406533241271973
    },
    {
      "epoch": 0.2910569105691057,
      "step": 5370,
      "training_loss": 6.470522880554199
    },
    {
      "epoch": 0.2911111111111111,
      "step": 5371,
      "training_loss": 6.343951225280762
    },
    {
      "epoch": 0.29116531165311654,
      "grad_norm": 22.623754501342773,
      "learning_rate": 1e-05,
      "loss": 7.2021,
      "step": 5372
    },
    {
      "epoch": 0.29116531165311654,
      "step": 5372,
      "training_loss": 7.1260223388671875
    },
    {
      "epoch": 0.29121951219512193,
      "step": 5373,
      "training_loss": 6.817702770233154
    },
    {
      "epoch": 0.2912737127371274,
      "step": 5374,
      "training_loss": 4.984694957733154
    },
    {
      "epoch": 0.29132791327913277,
      "step": 5375,
      "training_loss": 6.888352870941162
    },
    {
      "epoch": 0.2913821138211382,
      "grad_norm": 28.936830520629883,
      "learning_rate": 1e-05,
      "loss": 6.4542,
      "step": 5376
    },
    {
      "epoch": 0.2913821138211382,
      "step": 5376,
      "training_loss": 4.702929973602295
    },
    {
      "epoch": 0.29143631436314366,
      "step": 5377,
      "training_loss": 6.706864356994629
    },
    {
      "epoch": 0.29149051490514905,
      "step": 5378,
      "training_loss": 6.540095329284668
    },
    {
      "epoch": 0.2915447154471545,
      "step": 5379,
      "training_loss": 6.9532952308654785
    },
    {
      "epoch": 0.2915989159891599,
      "grad_norm": 44.36650085449219,
      "learning_rate": 1e-05,
      "loss": 6.2258,
      "step": 5380
    },
    {
      "epoch": 0.2915989159891599,
      "step": 5380,
      "training_loss": 6.387392520904541
    },
    {
      "epoch": 0.2916531165311653,
      "step": 5381,
      "training_loss": 7.2492170333862305
    },
    {
      "epoch": 0.2917073170731707,
      "step": 5382,
      "training_loss": 4.036409378051758
    },
    {
      "epoch": 0.29176151761517616,
      "step": 5383,
      "training_loss": 7.6299848556518555
    },
    {
      "epoch": 0.29181571815718155,
      "grad_norm": 24.41399574279785,
      "learning_rate": 1e-05,
      "loss": 6.3258,
      "step": 5384
    },
    {
      "epoch": 0.29181571815718155,
      "step": 5384,
      "training_loss": 6.632418632507324
    },
    {
      "epoch": 0.291869918699187,
      "step": 5385,
      "training_loss": 6.2361931800842285
    },
    {
      "epoch": 0.29192411924119244,
      "step": 5386,
      "training_loss": 7.371770858764648
    },
    {
      "epoch": 0.2919783197831978,
      "step": 5387,
      "training_loss": 6.970407962799072
    },
    {
      "epoch": 0.29203252032520327,
      "grad_norm": 33.87522506713867,
      "learning_rate": 1e-05,
      "loss": 6.8027,
      "step": 5388
    },
    {
      "epoch": 0.29203252032520327,
      "step": 5388,
      "training_loss": 6.624364376068115
    },
    {
      "epoch": 0.29208672086720866,
      "step": 5389,
      "training_loss": 6.542552471160889
    },
    {
      "epoch": 0.2921409214092141,
      "step": 5390,
      "training_loss": 7.375150680541992
    },
    {
      "epoch": 0.2921951219512195,
      "step": 5391,
      "training_loss": 6.9036641120910645
    },
    {
      "epoch": 0.29224932249322494,
      "grad_norm": 22.645727157592773,
      "learning_rate": 1e-05,
      "loss": 6.8614,
      "step": 5392
    },
    {
      "epoch": 0.29224932249322494,
      "step": 5392,
      "training_loss": 7.154378890991211
    },
    {
      "epoch": 0.29230352303523033,
      "step": 5393,
      "training_loss": 6.4620256423950195
    },
    {
      "epoch": 0.2923577235772358,
      "step": 5394,
      "training_loss": 6.520908832550049
    },
    {
      "epoch": 0.2924119241192412,
      "step": 5395,
      "training_loss": 4.332579135894775
    },
    {
      "epoch": 0.2924661246612466,
      "grad_norm": 24.443540573120117,
      "learning_rate": 1e-05,
      "loss": 6.1175,
      "step": 5396
    },
    {
      "epoch": 0.2924661246612466,
      "step": 5396,
      "training_loss": 6.589319705963135
    },
    {
      "epoch": 0.29252032520325205,
      "step": 5397,
      "training_loss": 6.474750518798828
    },
    {
      "epoch": 0.29257452574525744,
      "step": 5398,
      "training_loss": 7.461750507354736
    },
    {
      "epoch": 0.2926287262872629,
      "step": 5399,
      "training_loss": 7.071953773498535
    },
    {
      "epoch": 0.2926829268292683,
      "grad_norm": 39.68954086303711,
      "learning_rate": 1e-05,
      "loss": 6.8994,
      "step": 5400
    },
    {
      "epoch": 0.2926829268292683,
      "step": 5400,
      "training_loss": 9.482519149780273
    },
    {
      "epoch": 0.2927371273712737,
      "step": 5401,
      "training_loss": 7.418614864349365
    },
    {
      "epoch": 0.2927913279132791,
      "step": 5402,
      "training_loss": 6.50262975692749
    },
    {
      "epoch": 0.29284552845528455,
      "step": 5403,
      "training_loss": 6.139232158660889
    },
    {
      "epoch": 0.29289972899729,
      "grad_norm": 25.16407585144043,
      "learning_rate": 1e-05,
      "loss": 7.3857,
      "step": 5404
    },
    {
      "epoch": 0.29289972899729,
      "step": 5404,
      "training_loss": 6.533246040344238
    },
    {
      "epoch": 0.2929539295392954,
      "step": 5405,
      "training_loss": 7.16412353515625
    },
    {
      "epoch": 0.29300813008130083,
      "step": 5406,
      "training_loss": 7.42618989944458
    },
    {
      "epoch": 0.2930623306233062,
      "step": 5407,
      "training_loss": 7.0395989418029785
    },
    {
      "epoch": 0.29311653116531167,
      "grad_norm": 24.8836727142334,
      "learning_rate": 1e-05,
      "loss": 7.0408,
      "step": 5408
    },
    {
      "epoch": 0.29311653116531167,
      "step": 5408,
      "training_loss": 5.691144943237305
    },
    {
      "epoch": 0.29317073170731706,
      "step": 5409,
      "training_loss": 7.244941234588623
    },
    {
      "epoch": 0.2932249322493225,
      "step": 5410,
      "training_loss": 7.340455532073975
    },
    {
      "epoch": 0.2932791327913279,
      "step": 5411,
      "training_loss": 7.3873395919799805
    },
    {
      "epoch": 0.29333333333333333,
      "grad_norm": 27.80739402770996,
      "learning_rate": 1e-05,
      "loss": 6.916,
      "step": 5412
    },
    {
      "epoch": 0.29333333333333333,
      "step": 5412,
      "training_loss": 6.593052864074707
    },
    {
      "epoch": 0.2933875338753388,
      "step": 5413,
      "training_loss": 8.23981761932373
    },
    {
      "epoch": 0.29344173441734417,
      "step": 5414,
      "training_loss": 8.105524063110352
    },
    {
      "epoch": 0.2934959349593496,
      "step": 5415,
      "training_loss": 7.857624530792236
    },
    {
      "epoch": 0.293550135501355,
      "grad_norm": 33.67546463012695,
      "learning_rate": 1e-05,
      "loss": 7.699,
      "step": 5416
    },
    {
      "epoch": 0.293550135501355,
      "step": 5416,
      "training_loss": 7.672733783721924
    },
    {
      "epoch": 0.29360433604336045,
      "step": 5417,
      "training_loss": 6.510993957519531
    },
    {
      "epoch": 0.29365853658536584,
      "step": 5418,
      "training_loss": 6.485279083251953
    },
    {
      "epoch": 0.2937127371273713,
      "step": 5419,
      "training_loss": 6.819220542907715
    },
    {
      "epoch": 0.29376693766937667,
      "grad_norm": 23.00949478149414,
      "learning_rate": 1e-05,
      "loss": 6.8721,
      "step": 5420
    },
    {
      "epoch": 0.29376693766937667,
      "step": 5420,
      "training_loss": 4.885075569152832
    },
    {
      "epoch": 0.2938211382113821,
      "step": 5421,
      "training_loss": 6.473326683044434
    },
    {
      "epoch": 0.29387533875338756,
      "step": 5422,
      "training_loss": 7.925174713134766
    },
    {
      "epoch": 0.29392953929539295,
      "step": 5423,
      "training_loss": 7.347281455993652
    },
    {
      "epoch": 0.2939837398373984,
      "grad_norm": 32.032718658447266,
      "learning_rate": 1e-05,
      "loss": 6.6577,
      "step": 5424
    },
    {
      "epoch": 0.2939837398373984,
      "step": 5424,
      "training_loss": 7.257643699645996
    },
    {
      "epoch": 0.2940379403794038,
      "step": 5425,
      "training_loss": 5.469919681549072
    },
    {
      "epoch": 0.29409214092140923,
      "step": 5426,
      "training_loss": 8.010518074035645
    },
    {
      "epoch": 0.2941463414634146,
      "step": 5427,
      "training_loss": 6.140584945678711
    },
    {
      "epoch": 0.29420054200542006,
      "grad_norm": 24.184797286987305,
      "learning_rate": 1e-05,
      "loss": 6.7197,
      "step": 5428
    },
    {
      "epoch": 0.29420054200542006,
      "step": 5428,
      "training_loss": 6.17633581161499
    },
    {
      "epoch": 0.29425474254742545,
      "step": 5429,
      "training_loss": 6.6990885734558105
    },
    {
      "epoch": 0.2943089430894309,
      "step": 5430,
      "training_loss": 7.710830211639404
    },
    {
      "epoch": 0.29436314363143634,
      "step": 5431,
      "training_loss": 5.8875532150268555
    },
    {
      "epoch": 0.29441734417344173,
      "grad_norm": 24.823833465576172,
      "learning_rate": 1e-05,
      "loss": 6.6185,
      "step": 5432
    },
    {
      "epoch": 0.29441734417344173,
      "step": 5432,
      "training_loss": 7.469133377075195
    },
    {
      "epoch": 0.2944715447154472,
      "step": 5433,
      "training_loss": 6.711974620819092
    },
    {
      "epoch": 0.29452574525745256,
      "step": 5434,
      "training_loss": 7.265709400177002
    },
    {
      "epoch": 0.294579945799458,
      "step": 5435,
      "training_loss": 7.589774131774902
    },
    {
      "epoch": 0.2946341463414634,
      "grad_norm": 16.761837005615234,
      "learning_rate": 1e-05,
      "loss": 7.2591,
      "step": 5436
    },
    {
      "epoch": 0.2946341463414634,
      "step": 5436,
      "training_loss": 6.662492752075195
    },
    {
      "epoch": 0.29468834688346884,
      "step": 5437,
      "training_loss": 6.290999889373779
    },
    {
      "epoch": 0.29474254742547423,
      "step": 5438,
      "training_loss": 6.79396915435791
    },
    {
      "epoch": 0.2947967479674797,
      "step": 5439,
      "training_loss": 7.089818954467773
    },
    {
      "epoch": 0.2948509485094851,
      "grad_norm": 23.903217315673828,
      "learning_rate": 1e-05,
      "loss": 6.7093,
      "step": 5440
    },
    {
      "epoch": 0.2948509485094851,
      "step": 5440,
      "training_loss": 4.897316932678223
    },
    {
      "epoch": 0.2949051490514905,
      "step": 5441,
      "training_loss": 6.081754684448242
    },
    {
      "epoch": 0.29495934959349596,
      "step": 5442,
      "training_loss": 6.971445083618164
    },
    {
      "epoch": 0.29501355013550135,
      "step": 5443,
      "training_loss": 7.104244232177734
    },
    {
      "epoch": 0.2950677506775068,
      "grad_norm": 32.4334602355957,
      "learning_rate": 1e-05,
      "loss": 6.2637,
      "step": 5444
    },
    {
      "epoch": 0.2950677506775068,
      "step": 5444,
      "training_loss": 6.729641437530518
    },
    {
      "epoch": 0.2951219512195122,
      "step": 5445,
      "training_loss": 6.964099884033203
    },
    {
      "epoch": 0.2951761517615176,
      "step": 5446,
      "training_loss": 5.476464748382568
    },
    {
      "epoch": 0.295230352303523,
      "step": 5447,
      "training_loss": 7.421701431274414
    },
    {
      "epoch": 0.29528455284552846,
      "grad_norm": 36.65415573120117,
      "learning_rate": 1e-05,
      "loss": 6.648,
      "step": 5448
    },
    {
      "epoch": 0.29528455284552846,
      "step": 5448,
      "training_loss": 7.460057735443115
    },
    {
      "epoch": 0.29533875338753385,
      "step": 5449,
      "training_loss": 6.437042713165283
    },
    {
      "epoch": 0.2953929539295393,
      "step": 5450,
      "training_loss": 7.289834022521973
    },
    {
      "epoch": 0.29544715447154474,
      "step": 5451,
      "training_loss": 6.66705322265625
    },
    {
      "epoch": 0.2955013550135501,
      "grad_norm": 21.2213191986084,
      "learning_rate": 1e-05,
      "loss": 6.9635,
      "step": 5452
    },
    {
      "epoch": 0.2955013550135501,
      "step": 5452,
      "training_loss": 5.850754261016846
    },
    {
      "epoch": 0.29555555555555557,
      "step": 5453,
      "training_loss": 6.881796836853027
    },
    {
      "epoch": 0.29560975609756096,
      "step": 5454,
      "training_loss": 7.066958427429199
    },
    {
      "epoch": 0.2956639566395664,
      "step": 5455,
      "training_loss": 6.367775917053223
    },
    {
      "epoch": 0.2957181571815718,
      "grad_norm": 21.94205093383789,
      "learning_rate": 1e-05,
      "loss": 6.5418,
      "step": 5456
    },
    {
      "epoch": 0.2957181571815718,
      "step": 5456,
      "training_loss": 8.778267860412598
    },
    {
      "epoch": 0.29577235772357724,
      "step": 5457,
      "training_loss": 6.910949230194092
    },
    {
      "epoch": 0.29582655826558263,
      "step": 5458,
      "training_loss": 8.253515243530273
    },
    {
      "epoch": 0.2958807588075881,
      "step": 5459,
      "training_loss": 6.2879252433776855
    },
    {
      "epoch": 0.2959349593495935,
      "grad_norm": 24.878326416015625,
      "learning_rate": 1e-05,
      "loss": 7.5577,
      "step": 5460
    },
    {
      "epoch": 0.2959349593495935,
      "step": 5460,
      "training_loss": 6.262354850769043
    },
    {
      "epoch": 0.2959891598915989,
      "step": 5461,
      "training_loss": 6.437631130218506
    },
    {
      "epoch": 0.29604336043360435,
      "step": 5462,
      "training_loss": 7.129105567932129
    },
    {
      "epoch": 0.29609756097560974,
      "step": 5463,
      "training_loss": 7.29028844833374
    },
    {
      "epoch": 0.2961517615176152,
      "grad_norm": 18.525371551513672,
      "learning_rate": 1e-05,
      "loss": 6.7798,
      "step": 5464
    },
    {
      "epoch": 0.2961517615176152,
      "step": 5464,
      "training_loss": 7.168431758880615
    },
    {
      "epoch": 0.2962059620596206,
      "step": 5465,
      "training_loss": 5.124592304229736
    },
    {
      "epoch": 0.296260162601626,
      "step": 5466,
      "training_loss": 6.54768180847168
    },
    {
      "epoch": 0.2963143631436314,
      "step": 5467,
      "training_loss": 6.717886924743652
    },
    {
      "epoch": 0.29636856368563685,
      "grad_norm": 29.66290855407715,
      "learning_rate": 1e-05,
      "loss": 6.3896,
      "step": 5468
    },
    {
      "epoch": 0.29636856368563685,
      "step": 5468,
      "training_loss": 6.176183700561523
    },
    {
      "epoch": 0.2964227642276423,
      "step": 5469,
      "training_loss": 7.376357078552246
    },
    {
      "epoch": 0.2964769647696477,
      "step": 5470,
      "training_loss": 4.546340465545654
    },
    {
      "epoch": 0.29653116531165313,
      "step": 5471,
      "training_loss": 7.62912130355835
    },
    {
      "epoch": 0.2965853658536585,
      "grad_norm": 27.918865203857422,
      "learning_rate": 1e-05,
      "loss": 6.432,
      "step": 5472
    },
    {
      "epoch": 0.2965853658536585,
      "step": 5472,
      "training_loss": 6.5108489990234375
    },
    {
      "epoch": 0.29663956639566397,
      "step": 5473,
      "training_loss": 7.312901973724365
    },
    {
      "epoch": 0.29669376693766936,
      "step": 5474,
      "training_loss": 6.724462985992432
    },
    {
      "epoch": 0.2967479674796748,
      "step": 5475,
      "training_loss": 8.036532402038574
    },
    {
      "epoch": 0.2968021680216802,
      "grad_norm": 32.404720306396484,
      "learning_rate": 1e-05,
      "loss": 7.1462,
      "step": 5476
    },
    {
      "epoch": 0.2968021680216802,
      "step": 5476,
      "training_loss": 6.66433572769165
    },
    {
      "epoch": 0.29685636856368564,
      "step": 5477,
      "training_loss": 6.960991859436035
    },
    {
      "epoch": 0.2969105691056911,
      "step": 5478,
      "training_loss": 7.266385555267334
    },
    {
      "epoch": 0.29696476964769647,
      "step": 5479,
      "training_loss": 5.550258159637451
    },
    {
      "epoch": 0.2970189701897019,
      "grad_norm": 35.68106460571289,
      "learning_rate": 1e-05,
      "loss": 6.6105,
      "step": 5480
    },
    {
      "epoch": 0.2970189701897019,
      "step": 5480,
      "training_loss": 7.316122055053711
    },
    {
      "epoch": 0.2970731707317073,
      "step": 5481,
      "training_loss": 6.1057868003845215
    },
    {
      "epoch": 0.29712737127371275,
      "step": 5482,
      "training_loss": 6.1804375648498535
    },
    {
      "epoch": 0.29718157181571814,
      "step": 5483,
      "training_loss": 5.860891819000244
    },
    {
      "epoch": 0.2972357723577236,
      "grad_norm": 69.05267333984375,
      "learning_rate": 1e-05,
      "loss": 6.3658,
      "step": 5484
    },
    {
      "epoch": 0.2972357723577236,
      "step": 5484,
      "training_loss": 7.455819129943848
    },
    {
      "epoch": 0.29728997289972897,
      "step": 5485,
      "training_loss": 8.266287803649902
    },
    {
      "epoch": 0.2973441734417344,
      "step": 5486,
      "training_loss": 6.524265766143799
    },
    {
      "epoch": 0.29739837398373986,
      "step": 5487,
      "training_loss": 6.7133965492248535
    },
    {
      "epoch": 0.29745257452574525,
      "grad_norm": 25.6127872467041,
      "learning_rate": 1e-05,
      "loss": 7.2399,
      "step": 5488
    },
    {
      "epoch": 0.29745257452574525,
      "step": 5488,
      "training_loss": 6.975706577301025
    },
    {
      "epoch": 0.2975067750677507,
      "step": 5489,
      "training_loss": 4.948730945587158
    },
    {
      "epoch": 0.2975609756097561,
      "step": 5490,
      "training_loss": 6.9163360595703125
    },
    {
      "epoch": 0.29761517615176153,
      "step": 5491,
      "training_loss": 7.129118919372559
    },
    {
      "epoch": 0.2976693766937669,
      "grad_norm": 18.8866024017334,
      "learning_rate": 1e-05,
      "loss": 6.4925,
      "step": 5492
    },
    {
      "epoch": 0.2976693766937669,
      "step": 5492,
      "training_loss": 5.022350311279297
    },
    {
      "epoch": 0.29772357723577236,
      "step": 5493,
      "training_loss": 6.070078372955322
    },
    {
      "epoch": 0.29777777777777775,
      "step": 5494,
      "training_loss": 6.224246501922607
    },
    {
      "epoch": 0.2978319783197832,
      "step": 5495,
      "training_loss": 8.201390266418457
    },
    {
      "epoch": 0.29788617886178864,
      "grad_norm": 18.18089485168457,
      "learning_rate": 1e-05,
      "loss": 6.3795,
      "step": 5496
    },
    {
      "epoch": 0.29788617886178864,
      "step": 5496,
      "training_loss": 7.984183311462402
    },
    {
      "epoch": 0.29794037940379403,
      "step": 5497,
      "training_loss": 6.755579471588135
    },
    {
      "epoch": 0.2979945799457995,
      "step": 5498,
      "training_loss": 6.608581066131592
    },
    {
      "epoch": 0.29804878048780487,
      "step": 5499,
      "training_loss": 7.175459384918213
    },
    {
      "epoch": 0.2981029810298103,
      "grad_norm": 41.69963836669922,
      "learning_rate": 1e-05,
      "loss": 7.131,
      "step": 5500
    },
    {
      "epoch": 0.2981029810298103,
      "step": 5500,
      "training_loss": 7.328845024108887
    },
    {
      "epoch": 0.2981571815718157,
      "step": 5501,
      "training_loss": 4.071731090545654
    },
    {
      "epoch": 0.29821138211382114,
      "step": 5502,
      "training_loss": 6.575993061065674
    },
    {
      "epoch": 0.29826558265582653,
      "step": 5503,
      "training_loss": 4.091268539428711
    },
    {
      "epoch": 0.298319783197832,
      "grad_norm": 29.184398651123047,
      "learning_rate": 1e-05,
      "loss": 5.517,
      "step": 5504
    },
    {
      "epoch": 0.298319783197832,
      "step": 5504,
      "training_loss": 7.375997543334961
    },
    {
      "epoch": 0.2983739837398374,
      "step": 5505,
      "training_loss": 6.988004684448242
    },
    {
      "epoch": 0.2984281842818428,
      "step": 5506,
      "training_loss": 7.076620578765869
    },
    {
      "epoch": 0.29848238482384826,
      "step": 5507,
      "training_loss": 5.177403450012207
    },
    {
      "epoch": 0.29853658536585365,
      "grad_norm": 30.834688186645508,
      "learning_rate": 1e-05,
      "loss": 6.6545,
      "step": 5508
    },
    {
      "epoch": 0.29853658536585365,
      "step": 5508,
      "training_loss": 7.039413928985596
    },
    {
      "epoch": 0.2985907859078591,
      "step": 5509,
      "training_loss": 7.739176273345947
    },
    {
      "epoch": 0.2986449864498645,
      "step": 5510,
      "training_loss": 4.8256096839904785
    },
    {
      "epoch": 0.2986991869918699,
      "step": 5511,
      "training_loss": 6.697467803955078
    },
    {
      "epoch": 0.2987533875338753,
      "grad_norm": 19.76763343811035,
      "learning_rate": 1e-05,
      "loss": 6.5754,
      "step": 5512
    },
    {
      "epoch": 0.2987533875338753,
      "step": 5512,
      "training_loss": 6.049251556396484
    },
    {
      "epoch": 0.29880758807588076,
      "step": 5513,
      "training_loss": 4.8466668128967285
    },
    {
      "epoch": 0.2988617886178862,
      "step": 5514,
      "training_loss": 7.635298728942871
    },
    {
      "epoch": 0.2989159891598916,
      "step": 5515,
      "training_loss": 6.132689476013184
    },
    {
      "epoch": 0.29897018970189704,
      "grad_norm": 39.671871185302734,
      "learning_rate": 1e-05,
      "loss": 6.166,
      "step": 5516
    },
    {
      "epoch": 0.29897018970189704,
      "step": 5516,
      "training_loss": 6.974247455596924
    },
    {
      "epoch": 0.2990243902439024,
      "step": 5517,
      "training_loss": 7.380183696746826
    },
    {
      "epoch": 0.29907859078590787,
      "step": 5518,
      "training_loss": 7.232763290405273
    },
    {
      "epoch": 0.29913279132791326,
      "step": 5519,
      "training_loss": 7.783301830291748
    },
    {
      "epoch": 0.2991869918699187,
      "grad_norm": 26.479284286499023,
      "learning_rate": 1e-05,
      "loss": 7.3426,
      "step": 5520
    },
    {
      "epoch": 0.2991869918699187,
      "step": 5520,
      "training_loss": 6.702774524688721
    },
    {
      "epoch": 0.2992411924119241,
      "step": 5521,
      "training_loss": 7.255177974700928
    },
    {
      "epoch": 0.29929539295392954,
      "step": 5522,
      "training_loss": 8.21551513671875
    },
    {
      "epoch": 0.299349593495935,
      "step": 5523,
      "training_loss": 5.2431535720825195
    },
    {
      "epoch": 0.2994037940379404,
      "grad_norm": 31.80640411376953,
      "learning_rate": 1e-05,
      "loss": 6.8542,
      "step": 5524
    },
    {
      "epoch": 0.2994037940379404,
      "step": 5524,
      "training_loss": 7.038202285766602
    },
    {
      "epoch": 0.2994579945799458,
      "step": 5525,
      "training_loss": 7.91202449798584
    },
    {
      "epoch": 0.2995121951219512,
      "step": 5526,
      "training_loss": 6.446844577789307
    },
    {
      "epoch": 0.29956639566395665,
      "step": 5527,
      "training_loss": 7.139042377471924
    },
    {
      "epoch": 0.29962059620596204,
      "grad_norm": 20.05819320678711,
      "learning_rate": 1e-05,
      "loss": 7.134,
      "step": 5528
    },
    {
      "epoch": 0.29962059620596204,
      "step": 5528,
      "training_loss": 7.820009708404541
    },
    {
      "epoch": 0.2996747967479675,
      "step": 5529,
      "training_loss": 5.994859218597412
    },
    {
      "epoch": 0.2997289972899729,
      "step": 5530,
      "training_loss": 6.723250389099121
    },
    {
      "epoch": 0.2997831978319783,
      "step": 5531,
      "training_loss": 6.849370956420898
    },
    {
      "epoch": 0.29983739837398377,
      "grad_norm": 16.394268035888672,
      "learning_rate": 1e-05,
      "loss": 6.8469,
      "step": 5532
    },
    {
      "epoch": 0.29983739837398377,
      "step": 5532,
      "training_loss": 7.273368835449219
    },
    {
      "epoch": 0.29989159891598915,
      "step": 5533,
      "training_loss": 7.238754749298096
    },
    {
      "epoch": 0.2999457994579946,
      "step": 5534,
      "training_loss": 6.296139717102051
    },
    {
      "epoch": 0.3,
      "step": 5535,
      "training_loss": 6.499319553375244
    },
    {
      "epoch": 0.30005420054200543,
      "grad_norm": 34.7421875,
      "learning_rate": 1e-05,
      "loss": 6.8269,
      "step": 5536
    },
    {
      "epoch": 0.30005420054200543,
      "step": 5536,
      "training_loss": 7.484922885894775
    },
    {
      "epoch": 0.3001084010840108,
      "step": 5537,
      "training_loss": 6.314769744873047
    },
    {
      "epoch": 0.30016260162601627,
      "step": 5538,
      "training_loss": 6.730964183807373
    },
    {
      "epoch": 0.30021680216802166,
      "step": 5539,
      "training_loss": 6.804922580718994
    },
    {
      "epoch": 0.3002710027100271,
      "grad_norm": 21.85296058654785,
      "learning_rate": 1e-05,
      "loss": 6.8339,
      "step": 5540
    },
    {
      "epoch": 0.3002710027100271,
      "step": 5540,
      "training_loss": 7.180948734283447
    },
    {
      "epoch": 0.30032520325203255,
      "step": 5541,
      "training_loss": 6.544251918792725
    },
    {
      "epoch": 0.30037940379403794,
      "step": 5542,
      "training_loss": 6.958643436431885
    },
    {
      "epoch": 0.3004336043360434,
      "step": 5543,
      "training_loss": 6.361026287078857
    },
    {
      "epoch": 0.30048780487804877,
      "grad_norm": 36.71466064453125,
      "learning_rate": 1e-05,
      "loss": 6.7612,
      "step": 5544
    },
    {
      "epoch": 0.30048780487804877,
      "step": 5544,
      "training_loss": 6.8731818199157715
    },
    {
      "epoch": 0.3005420054200542,
      "step": 5545,
      "training_loss": 6.841780662536621
    },
    {
      "epoch": 0.3005962059620596,
      "step": 5546,
      "training_loss": 6.088544845581055
    },
    {
      "epoch": 0.30065040650406505,
      "step": 5547,
      "training_loss": 6.454706192016602
    },
    {
      "epoch": 0.30070460704607044,
      "grad_norm": 22.931716918945312,
      "learning_rate": 1e-05,
      "loss": 6.5646,
      "step": 5548
    },
    {
      "epoch": 0.30070460704607044,
      "step": 5548,
      "training_loss": 7.024929046630859
    },
    {
      "epoch": 0.3007588075880759,
      "step": 5549,
      "training_loss": 6.912380695343018
    },
    {
      "epoch": 0.3008130081300813,
      "step": 5550,
      "training_loss": 7.809717178344727
    },
    {
      "epoch": 0.3008672086720867,
      "step": 5551,
      "training_loss": 4.381865501403809
    },
    {
      "epoch": 0.30092140921409216,
      "grad_norm": 21.23476219177246,
      "learning_rate": 1e-05,
      "loss": 6.5322,
      "step": 5552
    },
    {
      "epoch": 0.30092140921409216,
      "step": 5552,
      "training_loss": 7.821357727050781
    },
    {
      "epoch": 0.30097560975609755,
      "step": 5553,
      "training_loss": 6.342700481414795
    },
    {
      "epoch": 0.301029810298103,
      "step": 5554,
      "training_loss": 5.828577518463135
    },
    {
      "epoch": 0.3010840108401084,
      "step": 5555,
      "training_loss": 6.992353916168213
    },
    {
      "epoch": 0.30113821138211383,
      "grad_norm": 19.846437454223633,
      "learning_rate": 1e-05,
      "loss": 6.7462,
      "step": 5556
    },
    {
      "epoch": 0.30113821138211383,
      "step": 5556,
      "training_loss": 6.98183012008667
    },
    {
      "epoch": 0.3011924119241192,
      "step": 5557,
      "training_loss": 6.840639114379883
    },
    {
      "epoch": 0.30124661246612466,
      "step": 5558,
      "training_loss": 6.654868125915527
    },
    {
      "epoch": 0.3013008130081301,
      "step": 5559,
      "training_loss": 6.637481689453125
    },
    {
      "epoch": 0.3013550135501355,
      "grad_norm": 29.733474731445312,
      "learning_rate": 1e-05,
      "loss": 6.7787,
      "step": 5560
    },
    {
      "epoch": 0.3013550135501355,
      "step": 5560,
      "training_loss": 5.09548807144165
    },
    {
      "epoch": 0.30140921409214094,
      "step": 5561,
      "training_loss": 7.296283721923828
    },
    {
      "epoch": 0.30146341463414633,
      "step": 5562,
      "training_loss": 8.032248497009277
    },
    {
      "epoch": 0.3015176151761518,
      "step": 5563,
      "training_loss": 5.314632415771484
    },
    {
      "epoch": 0.30157181571815717,
      "grad_norm": 25.621479034423828,
      "learning_rate": 1e-05,
      "loss": 6.4347,
      "step": 5564
    },
    {
      "epoch": 0.30157181571815717,
      "step": 5564,
      "training_loss": 7.616086006164551
    },
    {
      "epoch": 0.3016260162601626,
      "step": 5565,
      "training_loss": 8.321093559265137
    },
    {
      "epoch": 0.301680216802168,
      "step": 5566,
      "training_loss": 8.170673370361328
    },
    {
      "epoch": 0.30173441734417344,
      "step": 5567,
      "training_loss": 5.390249252319336
    },
    {
      "epoch": 0.3017886178861789,
      "grad_norm": 20.957359313964844,
      "learning_rate": 1e-05,
      "loss": 7.3745,
      "step": 5568
    },
    {
      "epoch": 0.3017886178861789,
      "step": 5568,
      "training_loss": 7.149376392364502
    },
    {
      "epoch": 0.3018428184281843,
      "step": 5569,
      "training_loss": 6.768954753875732
    },
    {
      "epoch": 0.3018970189701897,
      "step": 5570,
      "training_loss": 7.549567699432373
    },
    {
      "epoch": 0.3019512195121951,
      "step": 5571,
      "training_loss": 7.015838623046875
    },
    {
      "epoch": 0.30200542005420056,
      "grad_norm": 27.7188663482666,
      "learning_rate": 1e-05,
      "loss": 7.1209,
      "step": 5572
    },
    {
      "epoch": 0.30200542005420056,
      "step": 5572,
      "training_loss": 8.161271095275879
    },
    {
      "epoch": 0.30205962059620595,
      "step": 5573,
      "training_loss": 6.862929821014404
    },
    {
      "epoch": 0.3021138211382114,
      "step": 5574,
      "training_loss": 7.346306800842285
    },
    {
      "epoch": 0.3021680216802168,
      "step": 5575,
      "training_loss": 6.5987324714660645
    },
    {
      "epoch": 0.3022222222222222,
      "grad_norm": 29.505937576293945,
      "learning_rate": 1e-05,
      "loss": 7.2423,
      "step": 5576
    },
    {
      "epoch": 0.3022222222222222,
      "step": 5576,
      "training_loss": 8.413679122924805
    },
    {
      "epoch": 0.3022764227642276,
      "step": 5577,
      "training_loss": 6.430257797241211
    },
    {
      "epoch": 0.30233062330623306,
      "step": 5578,
      "training_loss": 6.31581974029541
    },
    {
      "epoch": 0.3023848238482385,
      "step": 5579,
      "training_loss": 5.180459976196289
    },
    {
      "epoch": 0.3024390243902439,
      "grad_norm": 32.86572265625,
      "learning_rate": 1e-05,
      "loss": 6.5851,
      "step": 5580
    },
    {
      "epoch": 0.3024390243902439,
      "step": 5580,
      "training_loss": 7.073565483093262
    },
    {
      "epoch": 0.30249322493224934,
      "step": 5581,
      "training_loss": 6.625865459442139
    },
    {
      "epoch": 0.3025474254742547,
      "step": 5582,
      "training_loss": 8.067943572998047
    },
    {
      "epoch": 0.3026016260162602,
      "step": 5583,
      "training_loss": 6.298433780670166
    },
    {
      "epoch": 0.30265582655826556,
      "grad_norm": 24.368162155151367,
      "learning_rate": 1e-05,
      "loss": 7.0165,
      "step": 5584
    },
    {
      "epoch": 0.30265582655826556,
      "step": 5584,
      "training_loss": 6.936344623565674
    },
    {
      "epoch": 0.302710027100271,
      "step": 5585,
      "training_loss": 7.3765387535095215
    },
    {
      "epoch": 0.3027642276422764,
      "step": 5586,
      "training_loss": 7.2839436531066895
    },
    {
      "epoch": 0.30281842818428184,
      "step": 5587,
      "training_loss": 8.006497383117676
    },
    {
      "epoch": 0.3028726287262873,
      "grad_norm": 53.2348518371582,
      "learning_rate": 1e-05,
      "loss": 7.4008,
      "step": 5588
    },
    {
      "epoch": 0.3028726287262873,
      "step": 5588,
      "training_loss": 7.6632609367370605
    },
    {
      "epoch": 0.3029268292682927,
      "step": 5589,
      "training_loss": 6.79746150970459
    },
    {
      "epoch": 0.3029810298102981,
      "step": 5590,
      "training_loss": 8.137222290039062
    },
    {
      "epoch": 0.3030352303523035,
      "step": 5591,
      "training_loss": 6.861934185028076
    },
    {
      "epoch": 0.30308943089430895,
      "grad_norm": 21.730628967285156,
      "learning_rate": 1e-05,
      "loss": 7.365,
      "step": 5592
    },
    {
      "epoch": 0.30308943089430895,
      "step": 5592,
      "training_loss": 8.058844566345215
    },
    {
      "epoch": 0.30314363143631434,
      "step": 5593,
      "training_loss": 7.757909297943115
    },
    {
      "epoch": 0.3031978319783198,
      "step": 5594,
      "training_loss": 5.605886459350586
    },
    {
      "epoch": 0.3032520325203252,
      "step": 5595,
      "training_loss": 6.188580513000488
    },
    {
      "epoch": 0.3033062330623306,
      "grad_norm": 18.49835968017578,
      "learning_rate": 1e-05,
      "loss": 6.9028,
      "step": 5596
    },
    {
      "epoch": 0.3033062330623306,
      "step": 5596,
      "training_loss": 6.364943981170654
    },
    {
      "epoch": 0.30336043360433607,
      "step": 5597,
      "training_loss": 6.770594120025635
    },
    {
      "epoch": 0.30341463414634146,
      "step": 5598,
      "training_loss": 7.060952186584473
    },
    {
      "epoch": 0.3034688346883469,
      "step": 5599,
      "training_loss": 6.430978298187256
    },
    {
      "epoch": 0.3035230352303523,
      "grad_norm": 33.04143142700195,
      "learning_rate": 1e-05,
      "loss": 6.6569,
      "step": 5600
    },
    {
      "epoch": 0.3035230352303523,
      "step": 5600,
      "training_loss": 6.1977949142456055
    },
    {
      "epoch": 0.30357723577235773,
      "step": 5601,
      "training_loss": 6.835841655731201
    },
    {
      "epoch": 0.3036314363143631,
      "step": 5602,
      "training_loss": 3.5345358848571777
    },
    {
      "epoch": 0.30368563685636857,
      "step": 5603,
      "training_loss": 7.248349666595459
    },
    {
      "epoch": 0.30373983739837396,
      "grad_norm": 18.628734588623047,
      "learning_rate": 1e-05,
      "loss": 5.9541,
      "step": 5604
    },
    {
      "epoch": 0.30373983739837396,
      "step": 5604,
      "training_loss": 6.899853229522705
    },
    {
      "epoch": 0.3037940379403794,
      "step": 5605,
      "training_loss": 7.664999008178711
    },
    {
      "epoch": 0.30384823848238485,
      "step": 5606,
      "training_loss": 5.464599609375
    },
    {
      "epoch": 0.30390243902439024,
      "step": 5607,
      "training_loss": 6.951932907104492
    },
    {
      "epoch": 0.3039566395663957,
      "grad_norm": 22.734699249267578,
      "learning_rate": 1e-05,
      "loss": 6.7453,
      "step": 5608
    },
    {
      "epoch": 0.3039566395663957,
      "step": 5608,
      "training_loss": 5.440035343170166
    },
    {
      "epoch": 0.30401084010840107,
      "step": 5609,
      "training_loss": 7.290704250335693
    },
    {
      "epoch": 0.3040650406504065,
      "step": 5610,
      "training_loss": 9.008588790893555
    },
    {
      "epoch": 0.3041192411924119,
      "step": 5611,
      "training_loss": 7.613358974456787
    },
    {
      "epoch": 0.30417344173441735,
      "grad_norm": 29.5846004486084,
      "learning_rate": 1e-05,
      "loss": 7.3382,
      "step": 5612
    },
    {
      "epoch": 0.30417344173441735,
      "step": 5612,
      "training_loss": 7.3649091720581055
    },
    {
      "epoch": 0.30422764227642274,
      "step": 5613,
      "training_loss": 6.522493362426758
    },
    {
      "epoch": 0.3042818428184282,
      "step": 5614,
      "training_loss": 7.117709159851074
    },
    {
      "epoch": 0.30433604336043363,
      "step": 5615,
      "training_loss": 4.888985633850098
    },
    {
      "epoch": 0.304390243902439,
      "grad_norm": 18.588655471801758,
      "learning_rate": 1e-05,
      "loss": 6.4735,
      "step": 5616
    },
    {
      "epoch": 0.304390243902439,
      "step": 5616,
      "training_loss": 7.766632080078125
    },
    {
      "epoch": 0.30444444444444446,
      "step": 5617,
      "training_loss": 5.686806678771973
    },
    {
      "epoch": 0.30449864498644985,
      "step": 5618,
      "training_loss": 7.811820030212402
    },
    {
      "epoch": 0.3045528455284553,
      "step": 5619,
      "training_loss": 7.102019786834717
    },
    {
      "epoch": 0.3046070460704607,
      "grad_norm": 20.60364532470703,
      "learning_rate": 1e-05,
      "loss": 7.0918,
      "step": 5620
    },
    {
      "epoch": 0.3046070460704607,
      "step": 5620,
      "training_loss": 7.803108215332031
    },
    {
      "epoch": 0.30466124661246613,
      "step": 5621,
      "training_loss": 4.884634494781494
    },
    {
      "epoch": 0.3047154471544715,
      "step": 5622,
      "training_loss": 5.482209205627441
    },
    {
      "epoch": 0.30476964769647696,
      "step": 5623,
      "training_loss": 7.288241386413574
    },
    {
      "epoch": 0.3048238482384824,
      "grad_norm": 22.279600143432617,
      "learning_rate": 1e-05,
      "loss": 6.3645,
      "step": 5624
    },
    {
      "epoch": 0.3048238482384824,
      "step": 5624,
      "training_loss": 6.632844924926758
    },
    {
      "epoch": 0.3048780487804878,
      "step": 5625,
      "training_loss": 6.580577373504639
    },
    {
      "epoch": 0.30493224932249324,
      "step": 5626,
      "training_loss": 7.92974853515625
    },
    {
      "epoch": 0.30498644986449863,
      "step": 5627,
      "training_loss": 7.063093662261963
    },
    {
      "epoch": 0.3050406504065041,
      "grad_norm": 18.670408248901367,
      "learning_rate": 1e-05,
      "loss": 7.0516,
      "step": 5628
    },
    {
      "epoch": 0.3050406504065041,
      "step": 5628,
      "training_loss": 7.953441619873047
    },
    {
      "epoch": 0.30509485094850947,
      "step": 5629,
      "training_loss": 7.060548782348633
    },
    {
      "epoch": 0.3051490514905149,
      "step": 5630,
      "training_loss": 7.255799293518066
    },
    {
      "epoch": 0.3052032520325203,
      "step": 5631,
      "training_loss": 8.333537101745605
    },
    {
      "epoch": 0.30525745257452574,
      "grad_norm": 40.10157012939453,
      "learning_rate": 1e-05,
      "loss": 7.6508,
      "step": 5632
    },
    {
      "epoch": 0.30525745257452574,
      "step": 5632,
      "training_loss": 5.497543811798096
    },
    {
      "epoch": 0.3053116531165312,
      "step": 5633,
      "training_loss": 7.591534614562988
    },
    {
      "epoch": 0.3053658536585366,
      "step": 5634,
      "training_loss": 5.894743919372559
    },
    {
      "epoch": 0.305420054200542,
      "step": 5635,
      "training_loss": 6.397545337677002
    },
    {
      "epoch": 0.3054742547425474,
      "grad_norm": 22.630298614501953,
      "learning_rate": 1e-05,
      "loss": 6.3453,
      "step": 5636
    },
    {
      "epoch": 0.3054742547425474,
      "step": 5636,
      "training_loss": 7.515269756317139
    },
    {
      "epoch": 0.30552845528455286,
      "step": 5637,
      "training_loss": 7.740232467651367
    },
    {
      "epoch": 0.30558265582655825,
      "step": 5638,
      "training_loss": 6.244409084320068
    },
    {
      "epoch": 0.3056368563685637,
      "step": 5639,
      "training_loss": 6.640351295471191
    },
    {
      "epoch": 0.3056910569105691,
      "grad_norm": 50.59546661376953,
      "learning_rate": 1e-05,
      "loss": 7.0351,
      "step": 5640
    },
    {
      "epoch": 0.3056910569105691,
      "step": 5640,
      "training_loss": 7.902849197387695
    },
    {
      "epoch": 0.3057452574525745,
      "step": 5641,
      "training_loss": 6.594899654388428
    },
    {
      "epoch": 0.30579945799457997,
      "step": 5642,
      "training_loss": 5.061570167541504
    },
    {
      "epoch": 0.30585365853658536,
      "step": 5643,
      "training_loss": 5.198919296264648
    },
    {
      "epoch": 0.3059078590785908,
      "grad_norm": 30.461374282836914,
      "learning_rate": 1e-05,
      "loss": 6.1896,
      "step": 5644
    },
    {
      "epoch": 0.3059078590785908,
      "step": 5644,
      "training_loss": 7.294280529022217
    },
    {
      "epoch": 0.3059620596205962,
      "step": 5645,
      "training_loss": 8.218485832214355
    },
    {
      "epoch": 0.30601626016260164,
      "step": 5646,
      "training_loss": 6.215823173522949
    },
    {
      "epoch": 0.30607046070460703,
      "step": 5647,
      "training_loss": 6.693709373474121
    },
    {
      "epoch": 0.3061246612466125,
      "grad_norm": 34.499656677246094,
      "learning_rate": 1e-05,
      "loss": 7.1056,
      "step": 5648
    },
    {
      "epoch": 0.3061246612466125,
      "step": 5648,
      "training_loss": 6.997509956359863
    },
    {
      "epoch": 0.30617886178861786,
      "step": 5649,
      "training_loss": 7.000695705413818
    },
    {
      "epoch": 0.3062330623306233,
      "step": 5650,
      "training_loss": 7.170607089996338
    },
    {
      "epoch": 0.30628726287262875,
      "step": 5651,
      "training_loss": 6.930575847625732
    },
    {
      "epoch": 0.30634146341463414,
      "grad_norm": 25.51556968688965,
      "learning_rate": 1e-05,
      "loss": 7.0248,
      "step": 5652
    },
    {
      "epoch": 0.30634146341463414,
      "step": 5652,
      "training_loss": 7.301340579986572
    },
    {
      "epoch": 0.3063956639566396,
      "step": 5653,
      "training_loss": 6.4238996505737305
    },
    {
      "epoch": 0.306449864498645,
      "step": 5654,
      "training_loss": 6.134068965911865
    },
    {
      "epoch": 0.3065040650406504,
      "step": 5655,
      "training_loss": 5.936971187591553
    },
    {
      "epoch": 0.3065582655826558,
      "grad_norm": 40.66069412231445,
      "learning_rate": 1e-05,
      "loss": 6.4491,
      "step": 5656
    },
    {
      "epoch": 0.3065582655826558,
      "step": 5656,
      "training_loss": 8.067761421203613
    },
    {
      "epoch": 0.30661246612466125,
      "step": 5657,
      "training_loss": 9.802783012390137
    },
    {
      "epoch": 0.30666666666666664,
      "step": 5658,
      "training_loss": 4.4486212730407715
    },
    {
      "epoch": 0.3067208672086721,
      "step": 5659,
      "training_loss": 7.168184280395508
    },
    {
      "epoch": 0.30677506775067753,
      "grad_norm": 29.660341262817383,
      "learning_rate": 1e-05,
      "loss": 7.3718,
      "step": 5660
    },
    {
      "epoch": 0.30677506775067753,
      "step": 5660,
      "training_loss": 3.9070136547088623
    },
    {
      "epoch": 0.3068292682926829,
      "step": 5661,
      "training_loss": 4.118085861206055
    },
    {
      "epoch": 0.30688346883468837,
      "step": 5662,
      "training_loss": 6.458305835723877
    },
    {
      "epoch": 0.30693766937669376,
      "step": 5663,
      "training_loss": 6.403006553649902
    },
    {
      "epoch": 0.3069918699186992,
      "grad_norm": 25.39635467529297,
      "learning_rate": 1e-05,
      "loss": 5.2216,
      "step": 5664
    },
    {
      "epoch": 0.3069918699186992,
      "step": 5664,
      "training_loss": 6.902246475219727
    },
    {
      "epoch": 0.3070460704607046,
      "step": 5665,
      "training_loss": 7.556827545166016
    },
    {
      "epoch": 0.30710027100271003,
      "step": 5666,
      "training_loss": 7.115594387054443
    },
    {
      "epoch": 0.3071544715447154,
      "step": 5667,
      "training_loss": 6.0891571044921875
    },
    {
      "epoch": 0.30720867208672087,
      "grad_norm": 32.9075927734375,
      "learning_rate": 1e-05,
      "loss": 6.916,
      "step": 5668
    },
    {
      "epoch": 0.30720867208672087,
      "step": 5668,
      "training_loss": 6.099724292755127
    },
    {
      "epoch": 0.3072628726287263,
      "step": 5669,
      "training_loss": 6.062244415283203
    },
    {
      "epoch": 0.3073170731707317,
      "step": 5670,
      "training_loss": 4.740251064300537
    },
    {
      "epoch": 0.30737127371273715,
      "step": 5671,
      "training_loss": 6.853412628173828
    },
    {
      "epoch": 0.30742547425474254,
      "grad_norm": 21.484861373901367,
      "learning_rate": 1e-05,
      "loss": 5.9389,
      "step": 5672
    },
    {
      "epoch": 0.30742547425474254,
      "step": 5672,
      "training_loss": 7.3955183029174805
    },
    {
      "epoch": 0.307479674796748,
      "step": 5673,
      "training_loss": 6.897545337677002
    },
    {
      "epoch": 0.30753387533875337,
      "step": 5674,
      "training_loss": 4.39104700088501
    },
    {
      "epoch": 0.3075880758807588,
      "step": 5675,
      "training_loss": 6.0901923179626465
    },
    {
      "epoch": 0.3076422764227642,
      "grad_norm": 46.783050537109375,
      "learning_rate": 1e-05,
      "loss": 6.1936,
      "step": 5676
    },
    {
      "epoch": 0.3076422764227642,
      "step": 5676,
      "training_loss": 6.80014705657959
    },
    {
      "epoch": 0.30769647696476965,
      "step": 5677,
      "training_loss": 8.72382926940918
    },
    {
      "epoch": 0.3077506775067751,
      "step": 5678,
      "training_loss": 5.271643161773682
    },
    {
      "epoch": 0.3078048780487805,
      "step": 5679,
      "training_loss": 5.660804271697998
    },
    {
      "epoch": 0.30785907859078593,
      "grad_norm": 23.02401351928711,
      "learning_rate": 1e-05,
      "loss": 6.6141,
      "step": 5680
    },
    {
      "epoch": 0.30785907859078593,
      "step": 5680,
      "training_loss": 6.784835338592529
    },
    {
      "epoch": 0.3079132791327913,
      "step": 5681,
      "training_loss": 6.966787338256836
    },
    {
      "epoch": 0.30796747967479676,
      "step": 5682,
      "training_loss": 6.085117816925049
    },
    {
      "epoch": 0.30802168021680215,
      "step": 5683,
      "training_loss": 5.332808017730713
    },
    {
      "epoch": 0.3080758807588076,
      "grad_norm": 22.09832000732422,
      "learning_rate": 1e-05,
      "loss": 6.2924,
      "step": 5684
    },
    {
      "epoch": 0.3080758807588076,
      "step": 5684,
      "training_loss": 5.999695777893066
    },
    {
      "epoch": 0.308130081300813,
      "step": 5685,
      "training_loss": 5.4123854637146
    },
    {
      "epoch": 0.30818428184281843,
      "step": 5686,
      "training_loss": 6.607239246368408
    },
    {
      "epoch": 0.3082384823848239,
      "step": 5687,
      "training_loss": 6.707234859466553
    },
    {
      "epoch": 0.30829268292682926,
      "grad_norm": 24.895450592041016,
      "learning_rate": 1e-05,
      "loss": 6.1816,
      "step": 5688
    },
    {
      "epoch": 0.30829268292682926,
      "step": 5688,
      "training_loss": 6.759856700897217
    },
    {
      "epoch": 0.3083468834688347,
      "step": 5689,
      "training_loss": 6.852887153625488
    },
    {
      "epoch": 0.3084010840108401,
      "step": 5690,
      "training_loss": 6.268793106079102
    },
    {
      "epoch": 0.30845528455284554,
      "step": 5691,
      "training_loss": 5.976069450378418
    },
    {
      "epoch": 0.30850948509485093,
      "grad_norm": 19.641803741455078,
      "learning_rate": 1e-05,
      "loss": 6.4644,
      "step": 5692
    },
    {
      "epoch": 0.30850948509485093,
      "step": 5692,
      "training_loss": 7.07320499420166
    },
    {
      "epoch": 0.3085636856368564,
      "step": 5693,
      "training_loss": 7.039528846740723
    },
    {
      "epoch": 0.30861788617886177,
      "step": 5694,
      "training_loss": 7.1125006675720215
    },
    {
      "epoch": 0.3086720867208672,
      "step": 5695,
      "training_loss": 6.930583953857422
    },
    {
      "epoch": 0.30872628726287266,
      "grad_norm": 17.801191329956055,
      "learning_rate": 1e-05,
      "loss": 7.039,
      "step": 5696
    },
    {
      "epoch": 0.30872628726287266,
      "step": 5696,
      "training_loss": 7.122097492218018
    },
    {
      "epoch": 0.30878048780487805,
      "step": 5697,
      "training_loss": 6.535782814025879
    },
    {
      "epoch": 0.3088346883468835,
      "step": 5698,
      "training_loss": 6.257598876953125
    },
    {
      "epoch": 0.3088888888888889,
      "step": 5699,
      "training_loss": 6.308672904968262
    },
    {
      "epoch": 0.3089430894308943,
      "grad_norm": 52.232933044433594,
      "learning_rate": 1e-05,
      "loss": 6.556,
      "step": 5700
    },
    {
      "epoch": 0.3089430894308943,
      "step": 5700,
      "training_loss": 5.993052959442139
    },
    {
      "epoch": 0.3089972899728997,
      "step": 5701,
      "training_loss": 7.701688289642334
    },
    {
      "epoch": 0.30905149051490516,
      "step": 5702,
      "training_loss": 8.550039291381836
    },
    {
      "epoch": 0.30910569105691055,
      "step": 5703,
      "training_loss": 6.700006008148193
    },
    {
      "epoch": 0.309159891598916,
      "grad_norm": 26.465267181396484,
      "learning_rate": 1e-05,
      "loss": 7.2362,
      "step": 5704
    },
    {
      "epoch": 0.309159891598916,
      "step": 5704,
      "training_loss": 6.991594314575195
    },
    {
      "epoch": 0.3092140921409214,
      "step": 5705,
      "training_loss": 7.836162090301514
    },
    {
      "epoch": 0.3092682926829268,
      "step": 5706,
      "training_loss": 6.7906341552734375
    },
    {
      "epoch": 0.30932249322493227,
      "step": 5707,
      "training_loss": 5.498788356781006
    },
    {
      "epoch": 0.30937669376693766,
      "grad_norm": 45.878257751464844,
      "learning_rate": 1e-05,
      "loss": 6.7793,
      "step": 5708
    },
    {
      "epoch": 0.30937669376693766,
      "step": 5708,
      "training_loss": 6.691109657287598
    },
    {
      "epoch": 0.3094308943089431,
      "step": 5709,
      "training_loss": 7.044139385223389
    },
    {
      "epoch": 0.3094850948509485,
      "step": 5710,
      "training_loss": 7.263749599456787
    },
    {
      "epoch": 0.30953929539295394,
      "step": 5711,
      "training_loss": 6.768485069274902
    },
    {
      "epoch": 0.30959349593495933,
      "grad_norm": 42.97751235961914,
      "learning_rate": 1e-05,
      "loss": 6.9419,
      "step": 5712
    },
    {
      "epoch": 0.30959349593495933,
      "step": 5712,
      "training_loss": 3.9524691104888916
    },
    {
      "epoch": 0.3096476964769648,
      "step": 5713,
      "training_loss": 7.4976325035095215
    },
    {
      "epoch": 0.30970189701897016,
      "step": 5714,
      "training_loss": 7.5047688484191895
    },
    {
      "epoch": 0.3097560975609756,
      "step": 5715,
      "training_loss": 6.800693035125732
    },
    {
      "epoch": 0.30981029810298105,
      "grad_norm": 15.038965225219727,
      "learning_rate": 1e-05,
      "loss": 6.4389,
      "step": 5716
    },
    {
      "epoch": 0.30981029810298105,
      "step": 5716,
      "training_loss": 7.289487838745117
    },
    {
      "epoch": 0.30986449864498644,
      "step": 5717,
      "training_loss": 7.239224910736084
    },
    {
      "epoch": 0.3099186991869919,
      "step": 5718,
      "training_loss": 7.38472318649292
    },
    {
      "epoch": 0.3099728997289973,
      "step": 5719,
      "training_loss": 8.070430755615234
    },
    {
      "epoch": 0.3100271002710027,
      "grad_norm": 82.74951934814453,
      "learning_rate": 1e-05,
      "loss": 7.496,
      "step": 5720
    },
    {
      "epoch": 0.3100271002710027,
      "step": 5720,
      "training_loss": 7.786215782165527
    },
    {
      "epoch": 0.3100813008130081,
      "step": 5721,
      "training_loss": 5.5870866775512695
    },
    {
      "epoch": 0.31013550135501355,
      "step": 5722,
      "training_loss": 4.916363716125488
    },
    {
      "epoch": 0.31018970189701894,
      "step": 5723,
      "training_loss": 6.648750305175781
    },
    {
      "epoch": 0.3102439024390244,
      "grad_norm": 21.794355392456055,
      "learning_rate": 1e-05,
      "loss": 6.2346,
      "step": 5724
    },
    {
      "epoch": 0.3102439024390244,
      "step": 5724,
      "training_loss": 7.2577805519104
    },
    {
      "epoch": 0.31029810298102983,
      "step": 5725,
      "training_loss": 6.099295616149902
    },
    {
      "epoch": 0.3103523035230352,
      "step": 5726,
      "training_loss": 6.274988651275635
    },
    {
      "epoch": 0.31040650406504067,
      "step": 5727,
      "training_loss": 6.728343963623047
    },
    {
      "epoch": 0.31046070460704606,
      "grad_norm": 23.212095260620117,
      "learning_rate": 1e-05,
      "loss": 6.5901,
      "step": 5728
    },
    {
      "epoch": 0.31046070460704606,
      "step": 5728,
      "training_loss": 7.5760979652404785
    },
    {
      "epoch": 0.3105149051490515,
      "step": 5729,
      "training_loss": 5.834286212921143
    },
    {
      "epoch": 0.3105691056910569,
      "step": 5730,
      "training_loss": 4.870362758636475
    },
    {
      "epoch": 0.31062330623306234,
      "step": 5731,
      "training_loss": 7.167490482330322
    },
    {
      "epoch": 0.3106775067750677,
      "grad_norm": 42.2354850769043,
      "learning_rate": 1e-05,
      "loss": 6.3621,
      "step": 5732
    },
    {
      "epoch": 0.3106775067750677,
      "step": 5732,
      "training_loss": 7.16312837600708
    },
    {
      "epoch": 0.31073170731707317,
      "step": 5733,
      "training_loss": 6.545947074890137
    },
    {
      "epoch": 0.3107859078590786,
      "step": 5734,
      "training_loss": 6.779641151428223
    },
    {
      "epoch": 0.310840108401084,
      "step": 5735,
      "training_loss": 5.822399139404297
    },
    {
      "epoch": 0.31089430894308945,
      "grad_norm": 27.367969512939453,
      "learning_rate": 1e-05,
      "loss": 6.5778,
      "step": 5736
    },
    {
      "epoch": 0.31089430894308945,
      "step": 5736,
      "training_loss": 6.414102554321289
    },
    {
      "epoch": 0.31094850948509484,
      "step": 5737,
      "training_loss": 7.356184959411621
    },
    {
      "epoch": 0.3110027100271003,
      "step": 5738,
      "training_loss": 6.262912750244141
    },
    {
      "epoch": 0.31105691056910567,
      "step": 5739,
      "training_loss": 7.064983367919922
    },
    {
      "epoch": 0.3111111111111111,
      "grad_norm": 41.36314392089844,
      "learning_rate": 1e-05,
      "loss": 6.7745,
      "step": 5740
    },
    {
      "epoch": 0.3111111111111111,
      "step": 5740,
      "training_loss": 6.285340785980225
    },
    {
      "epoch": 0.3111653116531165,
      "step": 5741,
      "training_loss": 6.940309047698975
    },
    {
      "epoch": 0.31121951219512195,
      "step": 5742,
      "training_loss": 6.396690368652344
    },
    {
      "epoch": 0.3112737127371274,
      "step": 5743,
      "training_loss": 7.294802188873291
    },
    {
      "epoch": 0.3113279132791328,
      "grad_norm": 31.526874542236328,
      "learning_rate": 1e-05,
      "loss": 6.7293,
      "step": 5744
    },
    {
      "epoch": 0.3113279132791328,
      "step": 5744,
      "training_loss": 7.212146759033203
    },
    {
      "epoch": 0.31138211382113823,
      "step": 5745,
      "training_loss": 7.118375301361084
    },
    {
      "epoch": 0.3114363143631436,
      "step": 5746,
      "training_loss": 6.7352142333984375
    },
    {
      "epoch": 0.31149051490514906,
      "step": 5747,
      "training_loss": 5.793942451477051
    },
    {
      "epoch": 0.31154471544715445,
      "grad_norm": 29.407386779785156,
      "learning_rate": 1e-05,
      "loss": 6.7149,
      "step": 5748
    },
    {
      "epoch": 0.31154471544715445,
      "step": 5748,
      "training_loss": 5.5595784187316895
    },
    {
      "epoch": 0.3115989159891599,
      "step": 5749,
      "training_loss": 4.273542404174805
    },
    {
      "epoch": 0.3116531165311653,
      "step": 5750,
      "training_loss": 6.081841945648193
    },
    {
      "epoch": 0.31170731707317073,
      "step": 5751,
      "training_loss": 7.299894332885742
    },
    {
      "epoch": 0.3117615176151762,
      "grad_norm": 21.688682556152344,
      "learning_rate": 1e-05,
      "loss": 5.8037,
      "step": 5752
    },
    {
      "epoch": 0.3117615176151762,
      "step": 5752,
      "training_loss": 6.1036481857299805
    },
    {
      "epoch": 0.31181571815718157,
      "step": 5753,
      "training_loss": 7.1180806159973145
    },
    {
      "epoch": 0.311869918699187,
      "step": 5754,
      "training_loss": 4.290517330169678
    },
    {
      "epoch": 0.3119241192411924,
      "step": 5755,
      "training_loss": 5.909097194671631
    },
    {
      "epoch": 0.31197831978319784,
      "grad_norm": 34.878910064697266,
      "learning_rate": 1e-05,
      "loss": 5.8553,
      "step": 5756
    },
    {
      "epoch": 0.31197831978319784,
      "step": 5756,
      "training_loss": 6.577840328216553
    },
    {
      "epoch": 0.31203252032520323,
      "step": 5757,
      "training_loss": 6.795447826385498
    },
    {
      "epoch": 0.3120867208672087,
      "step": 5758,
      "training_loss": 5.773514270782471
    },
    {
      "epoch": 0.31214092140921407,
      "step": 5759,
      "training_loss": 7.0548481941223145
    },
    {
      "epoch": 0.3121951219512195,
      "grad_norm": 25.343135833740234,
      "learning_rate": 1e-05,
      "loss": 6.5504,
      "step": 5760
    },
    {
      "epoch": 0.3121951219512195,
      "step": 5760,
      "training_loss": 6.703137397766113
    },
    {
      "epoch": 0.31224932249322496,
      "step": 5761,
      "training_loss": 6.282063961029053
    },
    {
      "epoch": 0.31230352303523035,
      "step": 5762,
      "training_loss": 6.596960544586182
    },
    {
      "epoch": 0.3123577235772358,
      "step": 5763,
      "training_loss": 6.0319504737854
    },
    {
      "epoch": 0.3124119241192412,
      "grad_norm": 24.636404037475586,
      "learning_rate": 1e-05,
      "loss": 6.4035,
      "step": 5764
    },
    {
      "epoch": 0.3124119241192412,
      "step": 5764,
      "training_loss": 7.311299800872803
    },
    {
      "epoch": 0.3124661246612466,
      "step": 5765,
      "training_loss": 6.8936920166015625
    },
    {
      "epoch": 0.312520325203252,
      "step": 5766,
      "training_loss": 5.310729026794434
    },
    {
      "epoch": 0.31257452574525746,
      "step": 5767,
      "training_loss": 6.615461349487305
    },
    {
      "epoch": 0.31262872628726285,
      "grad_norm": 47.350494384765625,
      "learning_rate": 1e-05,
      "loss": 6.5328,
      "step": 5768
    },
    {
      "epoch": 0.31262872628726285,
      "step": 5768,
      "training_loss": 7.56817102432251
    },
    {
      "epoch": 0.3126829268292683,
      "step": 5769,
      "training_loss": 5.7881646156311035
    },
    {
      "epoch": 0.31273712737127374,
      "step": 5770,
      "training_loss": 7.583795070648193
    },
    {
      "epoch": 0.3127913279132791,
      "step": 5771,
      "training_loss": 6.775852203369141
    },
    {
      "epoch": 0.31284552845528457,
      "grad_norm": 25.977781295776367,
      "learning_rate": 1e-05,
      "loss": 6.929,
      "step": 5772
    },
    {
      "epoch": 0.31284552845528457,
      "step": 5772,
      "training_loss": 5.840571403503418
    },
    {
      "epoch": 0.31289972899728996,
      "step": 5773,
      "training_loss": 6.1451416015625
    },
    {
      "epoch": 0.3129539295392954,
      "step": 5774,
      "training_loss": 7.260654926300049
    },
    {
      "epoch": 0.3130081300813008,
      "step": 5775,
      "training_loss": 6.209415435791016
    },
    {
      "epoch": 0.31306233062330624,
      "grad_norm": 42.23783874511719,
      "learning_rate": 1e-05,
      "loss": 6.3639,
      "step": 5776
    },
    {
      "epoch": 0.31306233062330624,
      "step": 5776,
      "training_loss": 6.256805896759033
    },
    {
      "epoch": 0.31311653116531163,
      "step": 5777,
      "training_loss": 7.376813888549805
    },
    {
      "epoch": 0.3131707317073171,
      "step": 5778,
      "training_loss": 5.702299118041992
    },
    {
      "epoch": 0.3132249322493225,
      "step": 5779,
      "training_loss": 7.403219223022461
    },
    {
      "epoch": 0.3132791327913279,
      "grad_norm": 19.175630569458008,
      "learning_rate": 1e-05,
      "loss": 6.6848,
      "step": 5780
    },
    {
      "epoch": 0.3132791327913279,
      "step": 5780,
      "training_loss": 6.411343097686768
    },
    {
      "epoch": 0.31333333333333335,
      "step": 5781,
      "training_loss": 6.798467636108398
    },
    {
      "epoch": 0.31338753387533874,
      "step": 5782,
      "training_loss": 4.88440465927124
    },
    {
      "epoch": 0.3134417344173442,
      "step": 5783,
      "training_loss": 5.670746326446533
    },
    {
      "epoch": 0.3134959349593496,
      "grad_norm": 28.41655921936035,
      "learning_rate": 1e-05,
      "loss": 5.9412,
      "step": 5784
    },
    {
      "epoch": 0.3134959349593496,
      "step": 5784,
      "training_loss": 5.592528343200684
    },
    {
      "epoch": 0.313550135501355,
      "step": 5785,
      "training_loss": 7.271027565002441
    },
    {
      "epoch": 0.3136043360433604,
      "step": 5786,
      "training_loss": 5.115762233734131
    },
    {
      "epoch": 0.31365853658536585,
      "step": 5787,
      "training_loss": 6.620197772979736
    },
    {
      "epoch": 0.3137127371273713,
      "grad_norm": 24.834152221679688,
      "learning_rate": 1e-05,
      "loss": 6.1499,
      "step": 5788
    },
    {
      "epoch": 0.3137127371273713,
      "step": 5788,
      "training_loss": 7.693042755126953
    },
    {
      "epoch": 0.3137669376693767,
      "step": 5789,
      "training_loss": 6.801689624786377
    },
    {
      "epoch": 0.31382113821138213,
      "step": 5790,
      "training_loss": 7.480729103088379
    },
    {
      "epoch": 0.3138753387533875,
      "step": 5791,
      "training_loss": 6.696623802185059
    },
    {
      "epoch": 0.31392953929539297,
      "grad_norm": 22.940128326416016,
      "learning_rate": 1e-05,
      "loss": 7.168,
      "step": 5792
    },
    {
      "epoch": 0.31392953929539297,
      "step": 5792,
      "training_loss": 7.058065414428711
    },
    {
      "epoch": 0.31398373983739836,
      "step": 5793,
      "training_loss": 6.728540897369385
    },
    {
      "epoch": 0.3140379403794038,
      "step": 5794,
      "training_loss": 8.452890396118164
    },
    {
      "epoch": 0.3140921409214092,
      "step": 5795,
      "training_loss": 6.7228264808654785
    },
    {
      "epoch": 0.31414634146341464,
      "grad_norm": 22.840595245361328,
      "learning_rate": 1e-05,
      "loss": 7.2406,
      "step": 5796
    },
    {
      "epoch": 0.31414634146341464,
      "step": 5796,
      "training_loss": 6.047886371612549
    },
    {
      "epoch": 0.3142005420054201,
      "step": 5797,
      "training_loss": 6.668405055999756
    },
    {
      "epoch": 0.31425474254742547,
      "step": 5798,
      "training_loss": 7.558332443237305
    },
    {
      "epoch": 0.3143089430894309,
      "step": 5799,
      "training_loss": 6.345000743865967
    },
    {
      "epoch": 0.3143631436314363,
      "grad_norm": 23.17375373840332,
      "learning_rate": 1e-05,
      "loss": 6.6549,
      "step": 5800
    },
    {
      "epoch": 0.3143631436314363,
      "step": 5800,
      "training_loss": 7.4203901290893555
    },
    {
      "epoch": 0.31441734417344175,
      "step": 5801,
      "training_loss": 5.373239517211914
    },
    {
      "epoch": 0.31447154471544714,
      "step": 5802,
      "training_loss": 4.321131229400635
    },
    {
      "epoch": 0.3145257452574526,
      "step": 5803,
      "training_loss": 6.611727714538574
    },
    {
      "epoch": 0.31457994579945797,
      "grad_norm": 16.8461971282959,
      "learning_rate": 1e-05,
      "loss": 5.9316,
      "step": 5804
    },
    {
      "epoch": 0.31457994579945797,
      "step": 5804,
      "training_loss": 6.4659504890441895
    },
    {
      "epoch": 0.3146341463414634,
      "step": 5805,
      "training_loss": 7.185008525848389
    },
    {
      "epoch": 0.31468834688346886,
      "step": 5806,
      "training_loss": 3.449525833129883
    },
    {
      "epoch": 0.31474254742547425,
      "step": 5807,
      "training_loss": 4.159731388092041
    },
    {
      "epoch": 0.3147967479674797,
      "grad_norm": 26.505292892456055,
      "learning_rate": 1e-05,
      "loss": 5.3151,
      "step": 5808
    },
    {
      "epoch": 0.3147967479674797,
      "step": 5808,
      "training_loss": 6.248623371124268
    },
    {
      "epoch": 0.3148509485094851,
      "step": 5809,
      "training_loss": 7.463626861572266
    },
    {
      "epoch": 0.31490514905149053,
      "step": 5810,
      "training_loss": 6.624380111694336
    },
    {
      "epoch": 0.3149593495934959,
      "step": 5811,
      "training_loss": 5.520007133483887
    },
    {
      "epoch": 0.31501355013550136,
      "grad_norm": 21.78594970703125,
      "learning_rate": 1e-05,
      "loss": 6.4642,
      "step": 5812
    },
    {
      "epoch": 0.31501355013550136,
      "step": 5812,
      "training_loss": 7.619346618652344
    },
    {
      "epoch": 0.31506775067750675,
      "step": 5813,
      "training_loss": 6.231497287750244
    },
    {
      "epoch": 0.3151219512195122,
      "step": 5814,
      "training_loss": 6.083370685577393
    },
    {
      "epoch": 0.31517615176151764,
      "step": 5815,
      "training_loss": 6.920437812805176
    },
    {
      "epoch": 0.31523035230352303,
      "grad_norm": 35.20348358154297,
      "learning_rate": 1e-05,
      "loss": 6.7137,
      "step": 5816
    },
    {
      "epoch": 0.31523035230352303,
      "step": 5816,
      "training_loss": 6.842915058135986
    },
    {
      "epoch": 0.3152845528455285,
      "step": 5817,
      "training_loss": 5.498743534088135
    },
    {
      "epoch": 0.31533875338753387,
      "step": 5818,
      "training_loss": 7.944902420043945
    },
    {
      "epoch": 0.3153929539295393,
      "step": 5819,
      "training_loss": 4.770860195159912
    },
    {
      "epoch": 0.3154471544715447,
      "grad_norm": 72.4189224243164,
      "learning_rate": 1e-05,
      "loss": 6.2644,
      "step": 5820
    },
    {
      "epoch": 0.3154471544715447,
      "step": 5820,
      "training_loss": 6.06373929977417
    },
    {
      "epoch": 0.31550135501355014,
      "step": 5821,
      "training_loss": 5.523509979248047
    },
    {
      "epoch": 0.31555555555555553,
      "step": 5822,
      "training_loss": 7.252439975738525
    },
    {
      "epoch": 0.315609756097561,
      "step": 5823,
      "training_loss": 6.740824222564697
    },
    {
      "epoch": 0.3156639566395664,
      "grad_norm": 26.553478240966797,
      "learning_rate": 1e-05,
      "loss": 6.3951,
      "step": 5824
    },
    {
      "epoch": 0.3156639566395664,
      "step": 5824,
      "training_loss": 11.267218589782715
    },
    {
      "epoch": 0.3157181571815718,
      "step": 5825,
      "training_loss": 6.311057090759277
    },
    {
      "epoch": 0.31577235772357726,
      "step": 5826,
      "training_loss": 6.704049110412598
    },
    {
      "epoch": 0.31582655826558265,
      "step": 5827,
      "training_loss": 6.219981670379639
    },
    {
      "epoch": 0.3158807588075881,
      "grad_norm": 26.473173141479492,
      "learning_rate": 1e-05,
      "loss": 7.6256,
      "step": 5828
    },
    {
      "epoch": 0.3158807588075881,
      "step": 5828,
      "training_loss": 7.710574150085449
    },
    {
      "epoch": 0.3159349593495935,
      "step": 5829,
      "training_loss": 6.91347074508667
    },
    {
      "epoch": 0.3159891598915989,
      "step": 5830,
      "training_loss": 7.192059516906738
    },
    {
      "epoch": 0.3160433604336043,
      "step": 5831,
      "training_loss": 6.113874435424805
    },
    {
      "epoch": 0.31609756097560976,
      "grad_norm": 29.36288070678711,
      "learning_rate": 1e-05,
      "loss": 6.9825,
      "step": 5832
    },
    {
      "epoch": 0.31609756097560976,
      "step": 5832,
      "training_loss": 4.775993824005127
    },
    {
      "epoch": 0.31615176151761515,
      "step": 5833,
      "training_loss": 7.3392486572265625
    },
    {
      "epoch": 0.3162059620596206,
      "step": 5834,
      "training_loss": 6.62311315536499
    },
    {
      "epoch": 0.31626016260162604,
      "step": 5835,
      "training_loss": 7.8624267578125
    },
    {
      "epoch": 0.3163143631436314,
      "grad_norm": 30.216135025024414,
      "learning_rate": 1e-05,
      "loss": 6.6502,
      "step": 5836
    },
    {
      "epoch": 0.3163143631436314,
      "step": 5836,
      "training_loss": 4.78987979888916
    },
    {
      "epoch": 0.31636856368563687,
      "step": 5837,
      "training_loss": 7.154413223266602
    },
    {
      "epoch": 0.31642276422764226,
      "step": 5838,
      "training_loss": 6.834336280822754
    },
    {
      "epoch": 0.3164769647696477,
      "step": 5839,
      "training_loss": 4.923222064971924
    },
    {
      "epoch": 0.3165311653116531,
      "grad_norm": 69.00007629394531,
      "learning_rate": 1e-05,
      "loss": 5.9255,
      "step": 5840
    },
    {
      "epoch": 0.3165311653116531,
      "step": 5840,
      "training_loss": 7.296465873718262
    },
    {
      "epoch": 0.31658536585365854,
      "step": 5841,
      "training_loss": 7.121496677398682
    },
    {
      "epoch": 0.31663956639566393,
      "step": 5842,
      "training_loss": 5.672130107879639
    },
    {
      "epoch": 0.3166937669376694,
      "step": 5843,
      "training_loss": 6.694658279418945
    },
    {
      "epoch": 0.3167479674796748,
      "grad_norm": 24.16042137145996,
      "learning_rate": 1e-05,
      "loss": 6.6962,
      "step": 5844
    },
    {
      "epoch": 0.3167479674796748,
      "step": 5844,
      "training_loss": 5.461847305297852
    },
    {
      "epoch": 0.3168021680216802,
      "step": 5845,
      "training_loss": 6.7026238441467285
    },
    {
      "epoch": 0.31685636856368565,
      "step": 5846,
      "training_loss": 6.57052755355835
    },
    {
      "epoch": 0.31691056910569104,
      "step": 5847,
      "training_loss": 6.900490760803223
    },
    {
      "epoch": 0.3169647696476965,
      "grad_norm": 17.34479331970215,
      "learning_rate": 1e-05,
      "loss": 6.4089,
      "step": 5848
    },
    {
      "epoch": 0.3169647696476965,
      "step": 5848,
      "training_loss": 6.513444423675537
    },
    {
      "epoch": 0.3170189701897019,
      "step": 5849,
      "training_loss": 6.666845798492432
    },
    {
      "epoch": 0.3170731707317073,
      "step": 5850,
      "training_loss": 7.298574924468994
    },
    {
      "epoch": 0.3171273712737127,
      "step": 5851,
      "training_loss": 5.02549409866333
    },
    {
      "epoch": 0.31718157181571816,
      "grad_norm": 58.74439239501953,
      "learning_rate": 1e-05,
      "loss": 6.3761,
      "step": 5852
    },
    {
      "epoch": 0.31718157181571816,
      "step": 5852,
      "training_loss": 8.06676959991455
    },
    {
      "epoch": 0.3172357723577236,
      "step": 5853,
      "training_loss": 7.628046989440918
    },
    {
      "epoch": 0.317289972899729,
      "step": 5854,
      "training_loss": 8.051897048950195
    },
    {
      "epoch": 0.31734417344173443,
      "step": 5855,
      "training_loss": 6.780163764953613
    },
    {
      "epoch": 0.3173983739837398,
      "grad_norm": 17.77842903137207,
      "learning_rate": 1e-05,
      "loss": 7.6317,
      "step": 5856
    },
    {
      "epoch": 0.3173983739837398,
      "step": 5856,
      "training_loss": 4.569359302520752
    },
    {
      "epoch": 0.31745257452574527,
      "step": 5857,
      "training_loss": 5.626171112060547
    },
    {
      "epoch": 0.31750677506775066,
      "step": 5858,
      "training_loss": 6.286734580993652
    },
    {
      "epoch": 0.3175609756097561,
      "step": 5859,
      "training_loss": 6.929113388061523
    },
    {
      "epoch": 0.3176151761517615,
      "grad_norm": 23.13702964782715,
      "learning_rate": 1e-05,
      "loss": 5.8528,
      "step": 5860
    },
    {
      "epoch": 0.3176151761517615,
      "step": 5860,
      "training_loss": 5.806484222412109
    },
    {
      "epoch": 0.31766937669376694,
      "step": 5861,
      "training_loss": 5.913209915161133
    },
    {
      "epoch": 0.3177235772357724,
      "step": 5862,
      "training_loss": 7.151899814605713
    },
    {
      "epoch": 0.31777777777777777,
      "step": 5863,
      "training_loss": 6.612955570220947
    },
    {
      "epoch": 0.3178319783197832,
      "grad_norm": 21.074583053588867,
      "learning_rate": 1e-05,
      "loss": 6.3711,
      "step": 5864
    },
    {
      "epoch": 0.3178319783197832,
      "step": 5864,
      "training_loss": 8.574155807495117
    },
    {
      "epoch": 0.3178861788617886,
      "step": 5865,
      "training_loss": 6.501039028167725
    },
    {
      "epoch": 0.31794037940379405,
      "step": 5866,
      "training_loss": 7.505618572235107
    },
    {
      "epoch": 0.31799457994579944,
      "step": 5867,
      "training_loss": 5.319931983947754
    },
    {
      "epoch": 0.3180487804878049,
      "grad_norm": 20.204391479492188,
      "learning_rate": 1e-05,
      "loss": 6.9752,
      "step": 5868
    },
    {
      "epoch": 0.3180487804878049,
      "step": 5868,
      "training_loss": 5.152632713317871
    },
    {
      "epoch": 0.31810298102981027,
      "step": 5869,
      "training_loss": 5.940826892852783
    },
    {
      "epoch": 0.3181571815718157,
      "step": 5870,
      "training_loss": 7.489578723907471
    },
    {
      "epoch": 0.31821138211382116,
      "step": 5871,
      "training_loss": 7.042148590087891
    },
    {
      "epoch": 0.31826558265582655,
      "grad_norm": 22.611831665039062,
      "learning_rate": 1e-05,
      "loss": 6.4063,
      "step": 5872
    },
    {
      "epoch": 0.31826558265582655,
      "step": 5872,
      "training_loss": 7.6668314933776855
    },
    {
      "epoch": 0.318319783197832,
      "step": 5873,
      "training_loss": 7.248950481414795
    },
    {
      "epoch": 0.3183739837398374,
      "step": 5874,
      "training_loss": 6.897092819213867
    },
    {
      "epoch": 0.31842818428184283,
      "step": 5875,
      "training_loss": 7.705967903137207
    },
    {
      "epoch": 0.3184823848238482,
      "grad_norm": 17.765077590942383,
      "learning_rate": 1e-05,
      "loss": 7.3797,
      "step": 5876
    },
    {
      "epoch": 0.3184823848238482,
      "step": 5876,
      "training_loss": 5.937889575958252
    },
    {
      "epoch": 0.31853658536585366,
      "step": 5877,
      "training_loss": 6.413964748382568
    },
    {
      "epoch": 0.31859078590785905,
      "step": 5878,
      "training_loss": 7.2751922607421875
    },
    {
      "epoch": 0.3186449864498645,
      "step": 5879,
      "training_loss": 5.699212551116943
    },
    {
      "epoch": 0.31869918699186994,
      "grad_norm": 40.64456558227539,
      "learning_rate": 1e-05,
      "loss": 6.3316,
      "step": 5880
    },
    {
      "epoch": 0.31869918699186994,
      "step": 5880,
      "training_loss": 7.298428058624268
    },
    {
      "epoch": 0.31875338753387533,
      "step": 5881,
      "training_loss": 6.805619239807129
    },
    {
      "epoch": 0.3188075880758808,
      "step": 5882,
      "training_loss": 7.731278419494629
    },
    {
      "epoch": 0.31886178861788617,
      "step": 5883,
      "training_loss": 7.825931072235107
    },
    {
      "epoch": 0.3189159891598916,
      "grad_norm": 68.83385467529297,
      "learning_rate": 1e-05,
      "loss": 7.4153,
      "step": 5884
    },
    {
      "epoch": 0.3189159891598916,
      "step": 5884,
      "training_loss": 6.804432392120361
    },
    {
      "epoch": 0.318970189701897,
      "step": 5885,
      "training_loss": 6.3274359703063965
    },
    {
      "epoch": 0.31902439024390244,
      "step": 5886,
      "training_loss": 6.979660511016846
    },
    {
      "epoch": 0.31907859078590783,
      "step": 5887,
      "training_loss": 7.149112224578857
    },
    {
      "epoch": 0.3191327913279133,
      "grad_norm": 21.761354446411133,
      "learning_rate": 1e-05,
      "loss": 6.8152,
      "step": 5888
    },
    {
      "epoch": 0.3191327913279133,
      "step": 5888,
      "training_loss": 8.22008991241455
    },
    {
      "epoch": 0.3191869918699187,
      "step": 5889,
      "training_loss": 5.448006629943848
    },
    {
      "epoch": 0.3192411924119241,
      "step": 5890,
      "training_loss": 8.100433349609375
    },
    {
      "epoch": 0.31929539295392956,
      "step": 5891,
      "training_loss": 6.4691362380981445
    },
    {
      "epoch": 0.31934959349593495,
      "grad_norm": 23.328535079956055,
      "learning_rate": 1e-05,
      "loss": 7.0594,
      "step": 5892
    },
    {
      "epoch": 0.31934959349593495,
      "step": 5892,
      "training_loss": 6.8321123123168945
    },
    {
      "epoch": 0.3194037940379404,
      "step": 5893,
      "training_loss": 6.7137932777404785
    },
    {
      "epoch": 0.3194579945799458,
      "step": 5894,
      "training_loss": 6.974546432495117
    },
    {
      "epoch": 0.3195121951219512,
      "step": 5895,
      "training_loss": 6.467870235443115
    },
    {
      "epoch": 0.3195663956639566,
      "grad_norm": 22.96050453186035,
      "learning_rate": 1e-05,
      "loss": 6.7471,
      "step": 5896
    },
    {
      "epoch": 0.3195663956639566,
      "step": 5896,
      "training_loss": 5.113171577453613
    },
    {
      "epoch": 0.31962059620596206,
      "step": 5897,
      "training_loss": 6.435691833496094
    },
    {
      "epoch": 0.3196747967479675,
      "step": 5898,
      "training_loss": 6.466738224029541
    },
    {
      "epoch": 0.3197289972899729,
      "step": 5899,
      "training_loss": 7.712418556213379
    },
    {
      "epoch": 0.31978319783197834,
      "grad_norm": 35.82182312011719,
      "learning_rate": 1e-05,
      "loss": 6.432,
      "step": 5900
    },
    {
      "epoch": 0.31978319783197834,
      "step": 5900,
      "training_loss": 6.475142955780029
    },
    {
      "epoch": 0.31983739837398373,
      "step": 5901,
      "training_loss": 7.726949691772461
    },
    {
      "epoch": 0.3198915989159892,
      "step": 5902,
      "training_loss": 8.509922981262207
    },
    {
      "epoch": 0.31994579945799456,
      "step": 5903,
      "training_loss": 6.493100643157959
    },
    {
      "epoch": 0.32,
      "grad_norm": 19.61806297302246,
      "learning_rate": 1e-05,
      "loss": 7.3013,
      "step": 5904
    },
    {
      "epoch": 0.32,
      "step": 5904,
      "training_loss": 7.08864688873291
    },
    {
      "epoch": 0.3200542005420054,
      "step": 5905,
      "training_loss": 7.151829242706299
    },
    {
      "epoch": 0.32010840108401084,
      "step": 5906,
      "training_loss": 6.58980131149292
    },
    {
      "epoch": 0.3201626016260163,
      "step": 5907,
      "training_loss": 7.5547895431518555
    },
    {
      "epoch": 0.3202168021680217,
      "grad_norm": 45.1436767578125,
      "learning_rate": 1e-05,
      "loss": 7.0963,
      "step": 5908
    },
    {
      "epoch": 0.3202168021680217,
      "step": 5908,
      "training_loss": 5.922755718231201
    },
    {
      "epoch": 0.3202710027100271,
      "step": 5909,
      "training_loss": 7.355559825897217
    },
    {
      "epoch": 0.3203252032520325,
      "step": 5910,
      "training_loss": 6.828775405883789
    },
    {
      "epoch": 0.32037940379403795,
      "step": 5911,
      "training_loss": 6.866682529449463
    },
    {
      "epoch": 0.32043360433604334,
      "grad_norm": 31.493366241455078,
      "learning_rate": 1e-05,
      "loss": 6.7434,
      "step": 5912
    },
    {
      "epoch": 0.32043360433604334,
      "step": 5912,
      "training_loss": 4.105734348297119
    },
    {
      "epoch": 0.3204878048780488,
      "step": 5913,
      "training_loss": 7.107813835144043
    },
    {
      "epoch": 0.3205420054200542,
      "step": 5914,
      "training_loss": 5.569887638092041
    },
    {
      "epoch": 0.3205962059620596,
      "step": 5915,
      "training_loss": 6.881999492645264
    },
    {
      "epoch": 0.32065040650406507,
      "grad_norm": 19.699813842773438,
      "learning_rate": 1e-05,
      "loss": 5.9164,
      "step": 5916
    },
    {
      "epoch": 0.32065040650406507,
      "step": 5916,
      "training_loss": 7.782650470733643
    },
    {
      "epoch": 0.32070460704607046,
      "step": 5917,
      "training_loss": 8.119726181030273
    },
    {
      "epoch": 0.3207588075880759,
      "step": 5918,
      "training_loss": 6.7699785232543945
    },
    {
      "epoch": 0.3208130081300813,
      "step": 5919,
      "training_loss": 6.616827487945557
    },
    {
      "epoch": 0.32086720867208673,
      "grad_norm": 20.829360961914062,
      "learning_rate": 1e-05,
      "loss": 7.3223,
      "step": 5920
    },
    {
      "epoch": 0.32086720867208673,
      "step": 5920,
      "training_loss": 7.041062355041504
    },
    {
      "epoch": 0.3209214092140921,
      "step": 5921,
      "training_loss": 8.50377082824707
    },
    {
      "epoch": 0.32097560975609757,
      "step": 5922,
      "training_loss": 7.007730007171631
    },
    {
      "epoch": 0.32102981029810296,
      "step": 5923,
      "training_loss": 9.507293701171875
    },
    {
      "epoch": 0.3210840108401084,
      "grad_norm": 62.89863204956055,
      "learning_rate": 1e-05,
      "loss": 8.015,
      "step": 5924
    },
    {
      "epoch": 0.3210840108401084,
      "step": 5924,
      "training_loss": 7.601363658905029
    },
    {
      "epoch": 0.32113821138211385,
      "step": 5925,
      "training_loss": 5.908513069152832
    },
    {
      "epoch": 0.32119241192411924,
      "step": 5926,
      "training_loss": 6.979511260986328
    },
    {
      "epoch": 0.3212466124661247,
      "step": 5927,
      "training_loss": 7.225335597991943
    },
    {
      "epoch": 0.32130081300813007,
      "grad_norm": 19.092966079711914,
      "learning_rate": 1e-05,
      "loss": 6.9287,
      "step": 5928
    },
    {
      "epoch": 0.32130081300813007,
      "step": 5928,
      "training_loss": 6.8022541999816895
    },
    {
      "epoch": 0.3213550135501355,
      "step": 5929,
      "training_loss": 7.5333404541015625
    },
    {
      "epoch": 0.3214092140921409,
      "step": 5930,
      "training_loss": 5.556742191314697
    },
    {
      "epoch": 0.32146341463414635,
      "step": 5931,
      "training_loss": 7.535274505615234
    },
    {
      "epoch": 0.32151761517615174,
      "grad_norm": 38.99094772338867,
      "learning_rate": 1e-05,
      "loss": 6.8569,
      "step": 5932
    },
    {
      "epoch": 0.32151761517615174,
      "step": 5932,
      "training_loss": 8.799294471740723
    },
    {
      "epoch": 0.3215718157181572,
      "step": 5933,
      "training_loss": 6.37860107421875
    },
    {
      "epoch": 0.32162601626016263,
      "step": 5934,
      "training_loss": 5.1690568923950195
    },
    {
      "epoch": 0.321680216802168,
      "step": 5935,
      "training_loss": 7.376804351806641
    },
    {
      "epoch": 0.32173441734417346,
      "grad_norm": 43.752742767333984,
      "learning_rate": 1e-05,
      "loss": 6.9309,
      "step": 5936
    },
    {
      "epoch": 0.32173441734417346,
      "step": 5936,
      "training_loss": 6.840096473693848
    },
    {
      "epoch": 0.32178861788617885,
      "step": 5937,
      "training_loss": 5.280869960784912
    },
    {
      "epoch": 0.3218428184281843,
      "step": 5938,
      "training_loss": 6.969644069671631
    },
    {
      "epoch": 0.3218970189701897,
      "step": 5939,
      "training_loss": 6.431957244873047
    },
    {
      "epoch": 0.32195121951219513,
      "grad_norm": 47.9928092956543,
      "learning_rate": 1e-05,
      "loss": 6.3806,
      "step": 5940
    },
    {
      "epoch": 0.32195121951219513,
      "step": 5940,
      "training_loss": 7.174919605255127
    },
    {
      "epoch": 0.3220054200542005,
      "step": 5941,
      "training_loss": 6.889699935913086
    },
    {
      "epoch": 0.32205962059620596,
      "step": 5942,
      "training_loss": 6.3323655128479
    },
    {
      "epoch": 0.3221138211382114,
      "step": 5943,
      "training_loss": 7.808323383331299
    },
    {
      "epoch": 0.3221680216802168,
      "grad_norm": 43.655914306640625,
      "learning_rate": 1e-05,
      "loss": 7.0513,
      "step": 5944
    },
    {
      "epoch": 0.3221680216802168,
      "step": 5944,
      "training_loss": 7.0808634757995605
    },
    {
      "epoch": 0.32222222222222224,
      "step": 5945,
      "training_loss": 6.415621757507324
    },
    {
      "epoch": 0.32227642276422763,
      "step": 5946,
      "training_loss": 5.71527624130249
    },
    {
      "epoch": 0.3223306233062331,
      "step": 5947,
      "training_loss": 6.594751358032227
    },
    {
      "epoch": 0.32238482384823847,
      "grad_norm": 33.094947814941406,
      "learning_rate": 1e-05,
      "loss": 6.4516,
      "step": 5948
    },
    {
      "epoch": 0.32238482384823847,
      "step": 5948,
      "training_loss": 6.581784725189209
    },
    {
      "epoch": 0.3224390243902439,
      "step": 5949,
      "training_loss": 6.839021682739258
    },
    {
      "epoch": 0.3224932249322493,
      "step": 5950,
      "training_loss": 7.4682793617248535
    },
    {
      "epoch": 0.32254742547425475,
      "step": 5951,
      "training_loss": 6.239390850067139
    },
    {
      "epoch": 0.3226016260162602,
      "grad_norm": 27.238805770874023,
      "learning_rate": 1e-05,
      "loss": 6.7821,
      "step": 5952
    },
    {
      "epoch": 0.3226016260162602,
      "step": 5952,
      "training_loss": 7.336747646331787
    },
    {
      "epoch": 0.3226558265582656,
      "step": 5953,
      "training_loss": 5.941938877105713
    },
    {
      "epoch": 0.322710027100271,
      "step": 5954,
      "training_loss": 3.9204800128936768
    },
    {
      "epoch": 0.3227642276422764,
      "step": 5955,
      "training_loss": 6.811975955963135
    },
    {
      "epoch": 0.32281842818428186,
      "grad_norm": 31.048606872558594,
      "learning_rate": 1e-05,
      "loss": 6.0028,
      "step": 5956
    },
    {
      "epoch": 0.32281842818428186,
      "step": 5956,
      "training_loss": 6.369053840637207
    },
    {
      "epoch": 0.32287262872628725,
      "step": 5957,
      "training_loss": 6.846630573272705
    },
    {
      "epoch": 0.3229268292682927,
      "step": 5958,
      "training_loss": 5.370584964752197
    },
    {
      "epoch": 0.3229810298102981,
      "step": 5959,
      "training_loss": 5.332876682281494
    },
    {
      "epoch": 0.3230352303523035,
      "grad_norm": 30.726064682006836,
      "learning_rate": 1e-05,
      "loss": 5.9798,
      "step": 5960
    },
    {
      "epoch": 0.3230352303523035,
      "step": 5960,
      "training_loss": 7.109951972961426
    },
    {
      "epoch": 0.3230894308943089,
      "step": 5961,
      "training_loss": 9.305908203125
    },
    {
      "epoch": 0.32314363143631436,
      "step": 5962,
      "training_loss": 5.186137676239014
    },
    {
      "epoch": 0.3231978319783198,
      "step": 5963,
      "training_loss": 6.467779636383057
    },
    {
      "epoch": 0.3232520325203252,
      "grad_norm": 22.865385055541992,
      "learning_rate": 1e-05,
      "loss": 7.0174,
      "step": 5964
    },
    {
      "epoch": 0.3232520325203252,
      "step": 5964,
      "training_loss": 7.366249084472656
    },
    {
      "epoch": 0.32330623306233064,
      "step": 5965,
      "training_loss": 7.091031551361084
    },
    {
      "epoch": 0.32336043360433603,
      "step": 5966,
      "training_loss": 5.874504566192627
    },
    {
      "epoch": 0.3234146341463415,
      "step": 5967,
      "training_loss": 5.9192728996276855
    },
    {
      "epoch": 0.32346883468834686,
      "grad_norm": 25.73246955871582,
      "learning_rate": 1e-05,
      "loss": 6.5628,
      "step": 5968
    },
    {
      "epoch": 0.32346883468834686,
      "step": 5968,
      "training_loss": 6.849892616271973
    },
    {
      "epoch": 0.3235230352303523,
      "step": 5969,
      "training_loss": 6.4188456535339355
    },
    {
      "epoch": 0.3235772357723577,
      "step": 5970,
      "training_loss": 7.833365440368652
    },
    {
      "epoch": 0.32363143631436314,
      "step": 5971,
      "training_loss": 6.230817794799805
    },
    {
      "epoch": 0.3236856368563686,
      "grad_norm": 24.091510772705078,
      "learning_rate": 1e-05,
      "loss": 6.8332,
      "step": 5972
    },
    {
      "epoch": 0.3236856368563686,
      "step": 5972,
      "training_loss": 6.90035343170166
    },
    {
      "epoch": 0.323739837398374,
      "step": 5973,
      "training_loss": 5.66616678237915
    },
    {
      "epoch": 0.3237940379403794,
      "step": 5974,
      "training_loss": 7.194713592529297
    },
    {
      "epoch": 0.3238482384823848,
      "step": 5975,
      "training_loss": 6.297544956207275
    },
    {
      "epoch": 0.32390243902439025,
      "grad_norm": 19.37042999267578,
      "learning_rate": 1e-05,
      "loss": 6.5147,
      "step": 5976
    },
    {
      "epoch": 0.32390243902439025,
      "step": 5976,
      "training_loss": 5.767319202423096
    },
    {
      "epoch": 0.32395663956639564,
      "step": 5977,
      "training_loss": 7.182741641998291
    },
    {
      "epoch": 0.3240108401084011,
      "step": 5978,
      "training_loss": 6.630466461181641
    },
    {
      "epoch": 0.3240650406504065,
      "step": 5979,
      "training_loss": 7.34871768951416
    },
    {
      "epoch": 0.3241192411924119,
      "grad_norm": 20.015714645385742,
      "learning_rate": 1e-05,
      "loss": 6.7323,
      "step": 5980
    },
    {
      "epoch": 0.3241192411924119,
      "step": 5980,
      "training_loss": 7.975666046142578
    },
    {
      "epoch": 0.32417344173441737,
      "step": 5981,
      "training_loss": 6.6924262046813965
    },
    {
      "epoch": 0.32422764227642276,
      "step": 5982,
      "training_loss": 5.228850841522217
    },
    {
      "epoch": 0.3242818428184282,
      "step": 5983,
      "training_loss": 8.242481231689453
    },
    {
      "epoch": 0.3243360433604336,
      "grad_norm": 24.73629379272461,
      "learning_rate": 1e-05,
      "loss": 7.0349,
      "step": 5984
    },
    {
      "epoch": 0.3243360433604336,
      "step": 5984,
      "training_loss": 6.960901737213135
    },
    {
      "epoch": 0.32439024390243903,
      "step": 5985,
      "training_loss": 8.45429515838623
    },
    {
      "epoch": 0.3244444444444444,
      "step": 5986,
      "training_loss": 6.409183025360107
    },
    {
      "epoch": 0.32449864498644987,
      "step": 5987,
      "training_loss": 6.450237274169922
    },
    {
      "epoch": 0.32455284552845526,
      "grad_norm": 19.96232795715332,
      "learning_rate": 1e-05,
      "loss": 7.0687,
      "step": 5988
    },
    {
      "epoch": 0.32455284552845526,
      "step": 5988,
      "training_loss": 7.871325492858887
    },
    {
      "epoch": 0.3246070460704607,
      "step": 5989,
      "training_loss": 7.060184001922607
    },
    {
      "epoch": 0.32466124661246615,
      "step": 5990,
      "training_loss": 6.10964298248291
    },
    {
      "epoch": 0.32471544715447154,
      "step": 5991,
      "training_loss": 6.8624677658081055
    },
    {
      "epoch": 0.324769647696477,
      "grad_norm": 29.2587890625,
      "learning_rate": 1e-05,
      "loss": 6.9759,
      "step": 5992
    },
    {
      "epoch": 0.324769647696477,
      "step": 5992,
      "training_loss": 6.718069553375244
    },
    {
      "epoch": 0.32482384823848237,
      "step": 5993,
      "training_loss": 5.554644584655762
    },
    {
      "epoch": 0.3248780487804878,
      "step": 5994,
      "training_loss": 7.989099502563477
    },
    {
      "epoch": 0.3249322493224932,
      "step": 5995,
      "training_loss": 6.593599319458008
    },
    {
      "epoch": 0.32498644986449865,
      "grad_norm": 32.938838958740234,
      "learning_rate": 1e-05,
      "loss": 6.7139,
      "step": 5996
    },
    {
      "epoch": 0.32498644986449865,
      "step": 5996,
      "training_loss": 6.432736873626709
    },
    {
      "epoch": 0.32504065040650404,
      "step": 5997,
      "training_loss": 6.925594329833984
    },
    {
      "epoch": 0.3250948509485095,
      "step": 5998,
      "training_loss": 7.199916839599609
    },
    {
      "epoch": 0.32514905149051493,
      "step": 5999,
      "training_loss": 7.935614109039307
    },
    {
      "epoch": 0.3252032520325203,
      "grad_norm": 36.047637939453125,
      "learning_rate": 1e-05,
      "loss": 7.1235,
      "step": 6000
    },
    {
      "epoch": 0.3252032520325203,
      "eval_runtime": 462.0402,
      "eval_samples_per_second": 4.437,
      "eval_steps_per_second": 4.437,
      "step": 6000
    },
    {
      "epoch": 0.3252032520325203,
      "step": 6000,
      "training_loss": 9.160839080810547
    },
    {
      "epoch": 0.32525745257452576,
      "step": 6001,
      "training_loss": 6.0318169593811035
    },
    {
      "epoch": 0.32531165311653115,
      "step": 6002,
      "training_loss": 6.709136009216309
    },
    {
      "epoch": 0.3253658536585366,
      "step": 6003,
      "training_loss": 6.994519233703613
    },
    {
      "epoch": 0.325420054200542,
      "grad_norm": 38.72222900390625,
      "learning_rate": 1e-05,
      "loss": 7.2241,
      "step": 6004
    },
    {
      "epoch": 0.325420054200542,
      "step": 6004,
      "training_loss": 6.354831218719482
    },
    {
      "epoch": 0.32547425474254743,
      "step": 6005,
      "training_loss": 7.853728771209717
    },
    {
      "epoch": 0.3255284552845528,
      "step": 6006,
      "training_loss": 7.050440311431885
    },
    {
      "epoch": 0.32558265582655826,
      "step": 6007,
      "training_loss": 6.668951511383057
    },
    {
      "epoch": 0.3256368563685637,
      "grad_norm": 17.151147842407227,
      "learning_rate": 1e-05,
      "loss": 6.982,
      "step": 6008
    },
    {
      "epoch": 0.3256368563685637,
      "step": 6008,
      "training_loss": 5.970897674560547
    },
    {
      "epoch": 0.3256910569105691,
      "step": 6009,
      "training_loss": 7.851711750030518
    },
    {
      "epoch": 0.32574525745257454,
      "step": 6010,
      "training_loss": 7.019566059112549
    },
    {
      "epoch": 0.32579945799457993,
      "step": 6011,
      "training_loss": 7.562386989593506
    },
    {
      "epoch": 0.3258536585365854,
      "grad_norm": 20.577287673950195,
      "learning_rate": 1e-05,
      "loss": 7.1011,
      "step": 6012
    },
    {
      "epoch": 0.3258536585365854,
      "step": 6012,
      "training_loss": 6.392899990081787
    },
    {
      "epoch": 0.32590785907859077,
      "step": 6013,
      "training_loss": 7.421629905700684
    },
    {
      "epoch": 0.3259620596205962,
      "step": 6014,
      "training_loss": 7.114896774291992
    },
    {
      "epoch": 0.3260162601626016,
      "step": 6015,
      "training_loss": 6.064645767211914
    },
    {
      "epoch": 0.32607046070460705,
      "grad_norm": 20.492578506469727,
      "learning_rate": 1e-05,
      "loss": 6.7485,
      "step": 6016
    },
    {
      "epoch": 0.32607046070460705,
      "step": 6016,
      "training_loss": 7.142755031585693
    },
    {
      "epoch": 0.3261246612466125,
      "step": 6017,
      "training_loss": 6.3799147605896
    },
    {
      "epoch": 0.3261788617886179,
      "step": 6018,
      "training_loss": 6.955055236816406
    },
    {
      "epoch": 0.3262330623306233,
      "step": 6019,
      "training_loss": 6.577277660369873
    },
    {
      "epoch": 0.3262872628726287,
      "grad_norm": 30.657283782958984,
      "learning_rate": 1e-05,
      "loss": 6.7638,
      "step": 6020
    },
    {
      "epoch": 0.3262872628726287,
      "step": 6020,
      "training_loss": 7.560022354125977
    },
    {
      "epoch": 0.32634146341463416,
      "step": 6021,
      "training_loss": 6.239037990570068
    },
    {
      "epoch": 0.32639566395663955,
      "step": 6022,
      "training_loss": 8.130775451660156
    },
    {
      "epoch": 0.326449864498645,
      "step": 6023,
      "training_loss": 6.192019939422607
    },
    {
      "epoch": 0.3265040650406504,
      "grad_norm": 33.06449890136719,
      "learning_rate": 1e-05,
      "loss": 7.0305,
      "step": 6024
    },
    {
      "epoch": 0.3265040650406504,
      "step": 6024,
      "training_loss": 7.011870861053467
    },
    {
      "epoch": 0.3265582655826558,
      "step": 6025,
      "training_loss": 7.367755889892578
    },
    {
      "epoch": 0.32661246612466127,
      "step": 6026,
      "training_loss": 6.772421836853027
    },
    {
      "epoch": 0.32666666666666666,
      "step": 6027,
      "training_loss": 6.223995208740234
    },
    {
      "epoch": 0.3267208672086721,
      "grad_norm": 29.26898956298828,
      "learning_rate": 1e-05,
      "loss": 6.844,
      "step": 6028
    },
    {
      "epoch": 0.3267208672086721,
      "step": 6028,
      "training_loss": 4.111407279968262
    },
    {
      "epoch": 0.3267750677506775,
      "step": 6029,
      "training_loss": 7.714766502380371
    },
    {
      "epoch": 0.32682926829268294,
      "step": 6030,
      "training_loss": 7.272806644439697
    },
    {
      "epoch": 0.32688346883468833,
      "step": 6031,
      "training_loss": 6.584993839263916
    },
    {
      "epoch": 0.3269376693766938,
      "grad_norm": 17.790342330932617,
      "learning_rate": 1e-05,
      "loss": 6.421,
      "step": 6032
    },
    {
      "epoch": 0.3269376693766938,
      "step": 6032,
      "training_loss": 8.264686584472656
    },
    {
      "epoch": 0.32699186991869916,
      "step": 6033,
      "training_loss": 7.229796886444092
    },
    {
      "epoch": 0.3270460704607046,
      "step": 6034,
      "training_loss": 7.030047416687012
    },
    {
      "epoch": 0.32710027100271005,
      "step": 6035,
      "training_loss": 6.969878196716309
    },
    {
      "epoch": 0.32715447154471544,
      "grad_norm": 25.69417953491211,
      "learning_rate": 1e-05,
      "loss": 7.3736,
      "step": 6036
    },
    {
      "epoch": 0.32715447154471544,
      "step": 6036,
      "training_loss": 6.56243896484375
    },
    {
      "epoch": 0.3272086720867209,
      "step": 6037,
      "training_loss": 7.2165679931640625
    },
    {
      "epoch": 0.3272628726287263,
      "step": 6038,
      "training_loss": 7.3723249435424805
    },
    {
      "epoch": 0.3273170731707317,
      "step": 6039,
      "training_loss": 5.181469440460205
    },
    {
      "epoch": 0.3273712737127371,
      "grad_norm": 26.747291564941406,
      "learning_rate": 1e-05,
      "loss": 6.5832,
      "step": 6040
    },
    {
      "epoch": 0.3273712737127371,
      "step": 6040,
      "training_loss": 6.4947190284729
    },
    {
      "epoch": 0.32742547425474255,
      "step": 6041,
      "training_loss": 5.804882526397705
    },
    {
      "epoch": 0.32747967479674794,
      "step": 6042,
      "training_loss": 6.973016262054443
    },
    {
      "epoch": 0.3275338753387534,
      "step": 6043,
      "training_loss": 6.664291858673096
    },
    {
      "epoch": 0.32758807588075883,
      "grad_norm": 19.21213150024414,
      "learning_rate": 1e-05,
      "loss": 6.4842,
      "step": 6044
    },
    {
      "epoch": 0.32758807588075883,
      "step": 6044,
      "training_loss": 6.914392471313477
    },
    {
      "epoch": 0.3276422764227642,
      "step": 6045,
      "training_loss": 6.264156818389893
    },
    {
      "epoch": 0.32769647696476967,
      "step": 6046,
      "training_loss": 6.05776309967041
    },
    {
      "epoch": 0.32775067750677506,
      "step": 6047,
      "training_loss": 8.079551696777344
    },
    {
      "epoch": 0.3278048780487805,
      "grad_norm": 32.0974235534668,
      "learning_rate": 1e-05,
      "loss": 6.829,
      "step": 6048
    },
    {
      "epoch": 0.3278048780487805,
      "step": 6048,
      "training_loss": 6.706146240234375
    },
    {
      "epoch": 0.3278590785907859,
      "step": 6049,
      "training_loss": 6.695825099945068
    },
    {
      "epoch": 0.32791327913279134,
      "step": 6050,
      "training_loss": 6.148055076599121
    },
    {
      "epoch": 0.3279674796747967,
      "step": 6051,
      "training_loss": 3.6847305297851562
    },
    {
      "epoch": 0.32802168021680217,
      "grad_norm": 21.700101852416992,
      "learning_rate": 1e-05,
      "loss": 5.8087,
      "step": 6052
    },
    {
      "epoch": 0.32802168021680217,
      "step": 6052,
      "training_loss": 6.291018009185791
    },
    {
      "epoch": 0.3280758807588076,
      "step": 6053,
      "training_loss": 3.9489262104034424
    },
    {
      "epoch": 0.328130081300813,
      "step": 6054,
      "training_loss": 4.274266242980957
    },
    {
      "epoch": 0.32818428184281845,
      "step": 6055,
      "training_loss": 6.29048490524292
    },
    {
      "epoch": 0.32823848238482384,
      "grad_norm": 58.803810119628906,
      "learning_rate": 1e-05,
      "loss": 5.2012,
      "step": 6056
    },
    {
      "epoch": 0.32823848238482384,
      "step": 6056,
      "training_loss": 6.841683387756348
    },
    {
      "epoch": 0.3282926829268293,
      "step": 6057,
      "training_loss": 6.871067523956299
    },
    {
      "epoch": 0.32834688346883467,
      "step": 6058,
      "training_loss": 7.559925556182861
    },
    {
      "epoch": 0.3284010840108401,
      "step": 6059,
      "training_loss": 6.814112663269043
    },
    {
      "epoch": 0.3284552845528455,
      "grad_norm": 23.055683135986328,
      "learning_rate": 1e-05,
      "loss": 7.0217,
      "step": 6060
    },
    {
      "epoch": 0.3284552845528455,
      "step": 6060,
      "training_loss": 8.226845741271973
    },
    {
      "epoch": 0.32850948509485095,
      "step": 6061,
      "training_loss": 6.052009105682373
    },
    {
      "epoch": 0.3285636856368564,
      "step": 6062,
      "training_loss": 6.900029182434082
    },
    {
      "epoch": 0.3286178861788618,
      "step": 6063,
      "training_loss": 7.5286054611206055
    },
    {
      "epoch": 0.32867208672086723,
      "grad_norm": 23.521949768066406,
      "learning_rate": 1e-05,
      "loss": 7.1769,
      "step": 6064
    },
    {
      "epoch": 0.32867208672086723,
      "step": 6064,
      "training_loss": 6.062361717224121
    },
    {
      "epoch": 0.3287262872628726,
      "step": 6065,
      "training_loss": 5.941549301147461
    },
    {
      "epoch": 0.32878048780487806,
      "step": 6066,
      "training_loss": 6.98619270324707
    },
    {
      "epoch": 0.32883468834688345,
      "step": 6067,
      "training_loss": 6.45131254196167
    },
    {
      "epoch": 0.3288888888888889,
      "grad_norm": 18.573753356933594,
      "learning_rate": 1e-05,
      "loss": 6.3604,
      "step": 6068
    },
    {
      "epoch": 0.3288888888888889,
      "step": 6068,
      "training_loss": 7.468347549438477
    },
    {
      "epoch": 0.3289430894308943,
      "step": 6069,
      "training_loss": 6.230598449707031
    },
    {
      "epoch": 0.32899728997289973,
      "step": 6070,
      "training_loss": 6.230242729187012
    },
    {
      "epoch": 0.3290514905149052,
      "step": 6071,
      "training_loss": 4.528853416442871
    },
    {
      "epoch": 0.32910569105691057,
      "grad_norm": 21.2990779876709,
      "learning_rate": 1e-05,
      "loss": 6.1145,
      "step": 6072
    },
    {
      "epoch": 0.32910569105691057,
      "step": 6072,
      "training_loss": 7.562550067901611
    },
    {
      "epoch": 0.329159891598916,
      "step": 6073,
      "training_loss": 7.978912830352783
    },
    {
      "epoch": 0.3292140921409214,
      "step": 6074,
      "training_loss": 7.186305999755859
    },
    {
      "epoch": 0.32926829268292684,
      "step": 6075,
      "training_loss": 8.679457664489746
    },
    {
      "epoch": 0.32932249322493223,
      "grad_norm": 59.07727813720703,
      "learning_rate": 1e-05,
      "loss": 7.8518,
      "step": 6076
    },
    {
      "epoch": 0.32932249322493223,
      "step": 6076,
      "training_loss": 6.0610198974609375
    },
    {
      "epoch": 0.3293766937669377,
      "step": 6077,
      "training_loss": 8.520455360412598
    },
    {
      "epoch": 0.32943089430894307,
      "step": 6078,
      "training_loss": 6.28860330581665
    },
    {
      "epoch": 0.3294850948509485,
      "step": 6079,
      "training_loss": 6.553339004516602
    },
    {
      "epoch": 0.32953929539295396,
      "grad_norm": 25.102073669433594,
      "learning_rate": 1e-05,
      "loss": 6.8559,
      "step": 6080
    },
    {
      "epoch": 0.32953929539295396,
      "step": 6080,
      "training_loss": 6.766594409942627
    },
    {
      "epoch": 0.32959349593495935,
      "step": 6081,
      "training_loss": 3.795463800430298
    },
    {
      "epoch": 0.3296476964769648,
      "step": 6082,
      "training_loss": 6.956119537353516
    },
    {
      "epoch": 0.3297018970189702,
      "step": 6083,
      "training_loss": 6.565354824066162
    },
    {
      "epoch": 0.3297560975609756,
      "grad_norm": 22.942792892456055,
      "learning_rate": 1e-05,
      "loss": 6.0209,
      "step": 6084
    },
    {
      "epoch": 0.3297560975609756,
      "step": 6084,
      "training_loss": 7.632557392120361
    },
    {
      "epoch": 0.329810298102981,
      "step": 6085,
      "training_loss": 6.586256504058838
    },
    {
      "epoch": 0.32986449864498646,
      "step": 6086,
      "training_loss": 6.793607711791992
    },
    {
      "epoch": 0.32991869918699185,
      "step": 6087,
      "training_loss": 7.933783054351807
    },
    {
      "epoch": 0.3299728997289973,
      "grad_norm": 29.44334602355957,
      "learning_rate": 1e-05,
      "loss": 7.2366,
      "step": 6088
    },
    {
      "epoch": 0.3299728997289973,
      "step": 6088,
      "training_loss": 5.980306625366211
    },
    {
      "epoch": 0.3300271002710027,
      "step": 6089,
      "training_loss": 5.875192642211914
    },
    {
      "epoch": 0.3300813008130081,
      "step": 6090,
      "training_loss": 5.071262359619141
    },
    {
      "epoch": 0.33013550135501357,
      "step": 6091,
      "training_loss": 6.268278121948242
    },
    {
      "epoch": 0.33018970189701896,
      "grad_norm": 41.24275588989258,
      "learning_rate": 1e-05,
      "loss": 5.7988,
      "step": 6092
    },
    {
      "epoch": 0.33018970189701896,
      "step": 6092,
      "training_loss": 7.030759334564209
    },
    {
      "epoch": 0.3302439024390244,
      "step": 6093,
      "training_loss": 7.274022102355957
    },
    {
      "epoch": 0.3302981029810298,
      "step": 6094,
      "training_loss": 7.371537208557129
    },
    {
      "epoch": 0.33035230352303524,
      "step": 6095,
      "training_loss": 7.2573442459106445
    },
    {
      "epoch": 0.33040650406504063,
      "grad_norm": 21.680194854736328,
      "learning_rate": 1e-05,
      "loss": 7.2334,
      "step": 6096
    },
    {
      "epoch": 0.33040650406504063,
      "step": 6096,
      "training_loss": 7.941184997558594
    },
    {
      "epoch": 0.3304607046070461,
      "step": 6097,
      "training_loss": 6.85949182510376
    },
    {
      "epoch": 0.33051490514905146,
      "step": 6098,
      "training_loss": 5.887777805328369
    },
    {
      "epoch": 0.3305691056910569,
      "step": 6099,
      "training_loss": 7.169404983520508
    },
    {
      "epoch": 0.33062330623306235,
      "grad_norm": 31.8612003326416,
      "learning_rate": 1e-05,
      "loss": 6.9645,
      "step": 6100
    },
    {
      "epoch": 0.33062330623306235,
      "step": 6100,
      "training_loss": 5.658972263336182
    },
    {
      "epoch": 0.33067750677506774,
      "step": 6101,
      "training_loss": 6.256481170654297
    },
    {
      "epoch": 0.3307317073170732,
      "step": 6102,
      "training_loss": 7.9775390625
    },
    {
      "epoch": 0.3307859078590786,
      "step": 6103,
      "training_loss": 5.874622344970703
    },
    {
      "epoch": 0.330840108401084,
      "grad_norm": 30.732078552246094,
      "learning_rate": 1e-05,
      "loss": 6.4419,
      "step": 6104
    },
    {
      "epoch": 0.330840108401084,
      "step": 6104,
      "training_loss": 5.991870880126953
    },
    {
      "epoch": 0.3308943089430894,
      "step": 6105,
      "training_loss": 6.834616184234619
    },
    {
      "epoch": 0.33094850948509485,
      "step": 6106,
      "training_loss": 7.7075066566467285
    },
    {
      "epoch": 0.33100271002710024,
      "step": 6107,
      "training_loss": 6.826347827911377
    },
    {
      "epoch": 0.3310569105691057,
      "grad_norm": 37.61049270629883,
      "learning_rate": 1e-05,
      "loss": 6.8401,
      "step": 6108
    },
    {
      "epoch": 0.3310569105691057,
      "step": 6108,
      "training_loss": 7.375263214111328
    },
    {
      "epoch": 0.33111111111111113,
      "step": 6109,
      "training_loss": 8.290824890136719
    },
    {
      "epoch": 0.3311653116531165,
      "step": 6110,
      "training_loss": 6.585933685302734
    },
    {
      "epoch": 0.33121951219512197,
      "step": 6111,
      "training_loss": 6.1245036125183105
    },
    {
      "epoch": 0.33127371273712736,
      "grad_norm": 47.73459243774414,
      "learning_rate": 1e-05,
      "loss": 7.0941,
      "step": 6112
    },
    {
      "epoch": 0.33127371273712736,
      "step": 6112,
      "training_loss": 6.375006198883057
    },
    {
      "epoch": 0.3313279132791328,
      "step": 6113,
      "training_loss": 6.083980083465576
    },
    {
      "epoch": 0.3313821138211382,
      "step": 6114,
      "training_loss": 7.898725509643555
    },
    {
      "epoch": 0.33143631436314364,
      "step": 6115,
      "training_loss": 6.128098964691162
    },
    {
      "epoch": 0.331490514905149,
      "grad_norm": 36.51874923706055,
      "learning_rate": 1e-05,
      "loss": 6.6215,
      "step": 6116
    },
    {
      "epoch": 0.331490514905149,
      "step": 6116,
      "training_loss": 4.41322135925293
    },
    {
      "epoch": 0.33154471544715447,
      "step": 6117,
      "training_loss": 5.974276542663574
    },
    {
      "epoch": 0.3315989159891599,
      "step": 6118,
      "training_loss": 6.485373497009277
    },
    {
      "epoch": 0.3316531165311653,
      "step": 6119,
      "training_loss": 6.562717437744141
    },
    {
      "epoch": 0.33170731707317075,
      "grad_norm": 27.013286590576172,
      "learning_rate": 1e-05,
      "loss": 5.8589,
      "step": 6120
    },
    {
      "epoch": 0.33170731707317075,
      "step": 6120,
      "training_loss": 5.377558708190918
    },
    {
      "epoch": 0.33176151761517614,
      "step": 6121,
      "training_loss": 7.218767166137695
    },
    {
      "epoch": 0.3318157181571816,
      "step": 6122,
      "training_loss": 6.564325332641602
    },
    {
      "epoch": 0.33186991869918697,
      "step": 6123,
      "training_loss": 6.526294231414795
    },
    {
      "epoch": 0.3319241192411924,
      "grad_norm": 32.11281967163086,
      "learning_rate": 1e-05,
      "loss": 6.4217,
      "step": 6124
    },
    {
      "epoch": 0.3319241192411924,
      "step": 6124,
      "training_loss": 7.776324272155762
    },
    {
      "epoch": 0.3319783197831978,
      "step": 6125,
      "training_loss": 8.954216957092285
    },
    {
      "epoch": 0.33203252032520325,
      "step": 6126,
      "training_loss": 7.273709774017334
    },
    {
      "epoch": 0.3320867208672087,
      "step": 6127,
      "training_loss": 4.183901786804199
    },
    {
      "epoch": 0.3321409214092141,
      "grad_norm": 33.349021911621094,
      "learning_rate": 1e-05,
      "loss": 7.047,
      "step": 6128
    },
    {
      "epoch": 0.3321409214092141,
      "step": 6128,
      "training_loss": 6.319303035736084
    },
    {
      "epoch": 0.33219512195121953,
      "step": 6129,
      "training_loss": 6.9585089683532715
    },
    {
      "epoch": 0.3322493224932249,
      "step": 6130,
      "training_loss": 5.5633225440979
    },
    {
      "epoch": 0.33230352303523036,
      "step": 6131,
      "training_loss": 5.92814826965332
    },
    {
      "epoch": 0.33235772357723575,
      "grad_norm": 38.04792404174805,
      "learning_rate": 1e-05,
      "loss": 6.1923,
      "step": 6132
    },
    {
      "epoch": 0.33235772357723575,
      "step": 6132,
      "training_loss": 7.857516765594482
    },
    {
      "epoch": 0.3324119241192412,
      "step": 6133,
      "training_loss": 6.13900089263916
    },
    {
      "epoch": 0.3324661246612466,
      "step": 6134,
      "training_loss": 6.953981876373291
    },
    {
      "epoch": 0.33252032520325203,
      "step": 6135,
      "training_loss": 7.04850959777832
    },
    {
      "epoch": 0.3325745257452575,
      "grad_norm": 27.794755935668945,
      "learning_rate": 1e-05,
      "loss": 6.9998,
      "step": 6136
    },
    {
      "epoch": 0.3325745257452575,
      "step": 6136,
      "training_loss": 7.179985046386719
    },
    {
      "epoch": 0.33262872628726287,
      "step": 6137,
      "training_loss": 7.9226579666137695
    },
    {
      "epoch": 0.3326829268292683,
      "step": 6138,
      "training_loss": 7.998851776123047
    },
    {
      "epoch": 0.3327371273712737,
      "step": 6139,
      "training_loss": 6.931368350982666
    },
    {
      "epoch": 0.33279132791327914,
      "grad_norm": 17.695892333984375,
      "learning_rate": 1e-05,
      "loss": 7.5082,
      "step": 6140
    },
    {
      "epoch": 0.33279132791327914,
      "step": 6140,
      "training_loss": 7.828348636627197
    },
    {
      "epoch": 0.33284552845528453,
      "step": 6141,
      "training_loss": 6.47308349609375
    },
    {
      "epoch": 0.33289972899729,
      "step": 6142,
      "training_loss": 6.592516899108887
    },
    {
      "epoch": 0.33295392953929537,
      "step": 6143,
      "training_loss": 7.57607889175415
    },
    {
      "epoch": 0.3330081300813008,
      "grad_norm": 20.42711067199707,
      "learning_rate": 1e-05,
      "loss": 7.1175,
      "step": 6144
    },
    {
      "epoch": 0.3330081300813008,
      "step": 6144,
      "training_loss": 7.127450942993164
    },
    {
      "epoch": 0.33306233062330626,
      "step": 6145,
      "training_loss": 7.646691799163818
    },
    {
      "epoch": 0.33311653116531165,
      "step": 6146,
      "training_loss": 5.962753772735596
    },
    {
      "epoch": 0.3331707317073171,
      "step": 6147,
      "training_loss": 7.7739033699035645
    },
    {
      "epoch": 0.3332249322493225,
      "grad_norm": 27.880332946777344,
      "learning_rate": 1e-05,
      "loss": 7.1277,
      "step": 6148
    },
    {
      "epoch": 0.3332249322493225,
      "step": 6148,
      "training_loss": 7.841975688934326
    },
    {
      "epoch": 0.3332791327913279,
      "step": 6149,
      "training_loss": 7.010959625244141
    },
    {
      "epoch": 0.3333333333333333,
      "step": 6150,
      "training_loss": 3.8230860233306885
    },
    {
      "epoch": 0.33338753387533876,
      "step": 6151,
      "training_loss": 6.926908493041992
    },
    {
      "epoch": 0.33344173441734415,
      "grad_norm": 22.2352294921875,
      "learning_rate": 1e-05,
      "loss": 6.4007,
      "step": 6152
    },
    {
      "epoch": 0.33344173441734415,
      "step": 6152,
      "training_loss": 6.993409633636475
    },
    {
      "epoch": 0.3334959349593496,
      "step": 6153,
      "training_loss": 7.440922737121582
    },
    {
      "epoch": 0.33355013550135504,
      "step": 6154,
      "training_loss": 7.906304359436035
    },
    {
      "epoch": 0.3336043360433604,
      "step": 6155,
      "training_loss": 7.4522833824157715
    },
    {
      "epoch": 0.3336585365853659,
      "grad_norm": 17.200021743774414,
      "learning_rate": 1e-05,
      "loss": 7.4482,
      "step": 6156
    },
    {
      "epoch": 0.3336585365853659,
      "step": 6156,
      "training_loss": 5.15447998046875
    },
    {
      "epoch": 0.33371273712737126,
      "step": 6157,
      "training_loss": 6.700315475463867
    },
    {
      "epoch": 0.3337669376693767,
      "step": 6158,
      "training_loss": 6.954917907714844
    },
    {
      "epoch": 0.3338211382113821,
      "step": 6159,
      "training_loss": 7.6882805824279785
    },
    {
      "epoch": 0.33387533875338754,
      "grad_norm": 37.928504943847656,
      "learning_rate": 1e-05,
      "loss": 6.6245,
      "step": 6160
    },
    {
      "epoch": 0.33387533875338754,
      "step": 6160,
      "training_loss": 7.104310035705566
    },
    {
      "epoch": 0.33392953929539293,
      "step": 6161,
      "training_loss": 5.2627153396606445
    },
    {
      "epoch": 0.3339837398373984,
      "step": 6162,
      "training_loss": 6.747410774230957
    },
    {
      "epoch": 0.3340379403794038,
      "step": 6163,
      "training_loss": 6.612609386444092
    },
    {
      "epoch": 0.3340921409214092,
      "grad_norm": 18.128185272216797,
      "learning_rate": 1e-05,
      "loss": 6.4318,
      "step": 6164
    },
    {
      "epoch": 0.3340921409214092,
      "step": 6164,
      "training_loss": 6.1662068367004395
    },
    {
      "epoch": 0.33414634146341465,
      "step": 6165,
      "training_loss": 6.795995235443115
    },
    {
      "epoch": 0.33420054200542004,
      "step": 6166,
      "training_loss": 8.333433151245117
    },
    {
      "epoch": 0.3342547425474255,
      "step": 6167,
      "training_loss": 6.5260844230651855
    },
    {
      "epoch": 0.3343089430894309,
      "grad_norm": 24.31267547607422,
      "learning_rate": 1e-05,
      "loss": 6.9554,
      "step": 6168
    },
    {
      "epoch": 0.3343089430894309,
      "step": 6168,
      "training_loss": 6.771424770355225
    },
    {
      "epoch": 0.3343631436314363,
      "step": 6169,
      "training_loss": 6.805393695831299
    },
    {
      "epoch": 0.3344173441734417,
      "step": 6170,
      "training_loss": 7.123603343963623
    },
    {
      "epoch": 0.33447154471544716,
      "step": 6171,
      "training_loss": 6.897218227386475
    },
    {
      "epoch": 0.3345257452574526,
      "grad_norm": 27.308931350708008,
      "learning_rate": 1e-05,
      "loss": 6.8994,
      "step": 6172
    },
    {
      "epoch": 0.3345257452574526,
      "step": 6172,
      "training_loss": 7.599579334259033
    },
    {
      "epoch": 0.334579945799458,
      "step": 6173,
      "training_loss": 7.338629722595215
    },
    {
      "epoch": 0.33463414634146343,
      "step": 6174,
      "training_loss": 11.898221969604492
    },
    {
      "epoch": 0.3346883468834688,
      "step": 6175,
      "training_loss": 6.9452362060546875
    },
    {
      "epoch": 0.33474254742547427,
      "grad_norm": 17.879440307617188,
      "learning_rate": 1e-05,
      "loss": 8.4454,
      "step": 6176
    },
    {
      "epoch": 0.33474254742547427,
      "step": 6176,
      "training_loss": 6.82297420501709
    },
    {
      "epoch": 0.33479674796747966,
      "step": 6177,
      "training_loss": 5.958806991577148
    },
    {
      "epoch": 0.3348509485094851,
      "step": 6178,
      "training_loss": 6.297227382659912
    },
    {
      "epoch": 0.3349051490514905,
      "step": 6179,
      "training_loss": 6.768820762634277
    },
    {
      "epoch": 0.33495934959349594,
      "grad_norm": 23.706445693969727,
      "learning_rate": 1e-05,
      "loss": 6.462,
      "step": 6180
    },
    {
      "epoch": 0.33495934959349594,
      "step": 6180,
      "training_loss": 7.067298412322998
    },
    {
      "epoch": 0.3350135501355014,
      "step": 6181,
      "training_loss": 7.136777877807617
    },
    {
      "epoch": 0.33506775067750677,
      "step": 6182,
      "training_loss": 6.195103168487549
    },
    {
      "epoch": 0.3351219512195122,
      "step": 6183,
      "training_loss": 7.233058452606201
    },
    {
      "epoch": 0.3351761517615176,
      "grad_norm": 21.996257781982422,
      "learning_rate": 1e-05,
      "loss": 6.9081,
      "step": 6184
    },
    {
      "epoch": 0.3351761517615176,
      "step": 6184,
      "training_loss": 6.938024044036865
    },
    {
      "epoch": 0.33523035230352305,
      "step": 6185,
      "training_loss": 6.842136859893799
    },
    {
      "epoch": 0.33528455284552844,
      "step": 6186,
      "training_loss": 7.203336715698242
    },
    {
      "epoch": 0.3353387533875339,
      "step": 6187,
      "training_loss": 7.8568925857543945
    },
    {
      "epoch": 0.3353929539295393,
      "grad_norm": 25.69923210144043,
      "learning_rate": 1e-05,
      "loss": 7.2101,
      "step": 6188
    },
    {
      "epoch": 0.3353929539295393,
      "step": 6188,
      "training_loss": 8.085953712463379
    },
    {
      "epoch": 0.3354471544715447,
      "step": 6189,
      "training_loss": 7.253179550170898
    },
    {
      "epoch": 0.33550135501355016,
      "step": 6190,
      "training_loss": 3.991499900817871
    },
    {
      "epoch": 0.33555555555555555,
      "step": 6191,
      "training_loss": 6.688772201538086
    },
    {
      "epoch": 0.335609756097561,
      "grad_norm": 24.464445114135742,
      "learning_rate": 1e-05,
      "loss": 6.5049,
      "step": 6192
    },
    {
      "epoch": 0.335609756097561,
      "step": 6192,
      "training_loss": 7.338552474975586
    },
    {
      "epoch": 0.3356639566395664,
      "step": 6193,
      "training_loss": 7.698235034942627
    },
    {
      "epoch": 0.33571815718157183,
      "step": 6194,
      "training_loss": 5.894566059112549
    },
    {
      "epoch": 0.3357723577235772,
      "step": 6195,
      "training_loss": 6.558442115783691
    },
    {
      "epoch": 0.33582655826558266,
      "grad_norm": 22.334644317626953,
      "learning_rate": 1e-05,
      "loss": 6.8724,
      "step": 6196
    },
    {
      "epoch": 0.33582655826558266,
      "step": 6196,
      "training_loss": 6.157556056976318
    },
    {
      "epoch": 0.33588075880758805,
      "step": 6197,
      "training_loss": 6.071434020996094
    },
    {
      "epoch": 0.3359349593495935,
      "step": 6198,
      "training_loss": 5.844330787658691
    },
    {
      "epoch": 0.33598915989159894,
      "step": 6199,
      "training_loss": 5.77523946762085
    },
    {
      "epoch": 0.33604336043360433,
      "grad_norm": 55.71149444580078,
      "learning_rate": 1e-05,
      "loss": 5.9621,
      "step": 6200
    },
    {
      "epoch": 0.33604336043360433,
      "step": 6200,
      "training_loss": 7.233241558074951
    },
    {
      "epoch": 0.3360975609756098,
      "step": 6201,
      "training_loss": 4.227163314819336
    },
    {
      "epoch": 0.33615176151761517,
      "step": 6202,
      "training_loss": 7.727740287780762
    },
    {
      "epoch": 0.3362059620596206,
      "step": 6203,
      "training_loss": 5.794949054718018
    },
    {
      "epoch": 0.336260162601626,
      "grad_norm": 26.29555320739746,
      "learning_rate": 1e-05,
      "loss": 6.2458,
      "step": 6204
    },
    {
      "epoch": 0.336260162601626,
      "step": 6204,
      "training_loss": 7.4484148025512695
    },
    {
      "epoch": 0.33631436314363145,
      "step": 6205,
      "training_loss": 6.523226737976074
    },
    {
      "epoch": 0.33636856368563683,
      "step": 6206,
      "training_loss": 5.7302656173706055
    },
    {
      "epoch": 0.3364227642276423,
      "step": 6207,
      "training_loss": 6.061172962188721
    },
    {
      "epoch": 0.3364769647696477,
      "grad_norm": 28.170644760131836,
      "learning_rate": 1e-05,
      "loss": 6.4408,
      "step": 6208
    },
    {
      "epoch": 0.3364769647696477,
      "step": 6208,
      "training_loss": 6.270484447479248
    },
    {
      "epoch": 0.3365311653116531,
      "step": 6209,
      "training_loss": 6.536577224731445
    },
    {
      "epoch": 0.33658536585365856,
      "step": 6210,
      "training_loss": 9.014437675476074
    },
    {
      "epoch": 0.33663956639566395,
      "step": 6211,
      "training_loss": 7.247890949249268
    },
    {
      "epoch": 0.3366937669376694,
      "grad_norm": 26.602968215942383,
      "learning_rate": 1e-05,
      "loss": 7.2673,
      "step": 6212
    },
    {
      "epoch": 0.3366937669376694,
      "step": 6212,
      "training_loss": 7.876255989074707
    },
    {
      "epoch": 0.3367479674796748,
      "step": 6213,
      "training_loss": 6.675606727600098
    },
    {
      "epoch": 0.3368021680216802,
      "step": 6214,
      "training_loss": 7.3839945793151855
    },
    {
      "epoch": 0.3368563685636856,
      "step": 6215,
      "training_loss": 7.683138847351074
    },
    {
      "epoch": 0.33691056910569106,
      "grad_norm": 24.209110260009766,
      "learning_rate": 1e-05,
      "loss": 7.4047,
      "step": 6216
    },
    {
      "epoch": 0.33691056910569106,
      "step": 6216,
      "training_loss": 5.777529239654541
    },
    {
      "epoch": 0.33696476964769645,
      "step": 6217,
      "training_loss": 7.475255489349365
    },
    {
      "epoch": 0.3370189701897019,
      "step": 6218,
      "training_loss": 6.166440963745117
    },
    {
      "epoch": 0.33707317073170734,
      "step": 6219,
      "training_loss": 4.769937038421631
    },
    {
      "epoch": 0.33712737127371273,
      "grad_norm": 28.528160095214844,
      "learning_rate": 1e-05,
      "loss": 6.0473,
      "step": 6220
    },
    {
      "epoch": 0.33712737127371273,
      "step": 6220,
      "training_loss": 7.386740207672119
    },
    {
      "epoch": 0.3371815718157182,
      "step": 6221,
      "training_loss": 6.214078903198242
    },
    {
      "epoch": 0.33723577235772356,
      "step": 6222,
      "training_loss": 7.182382583618164
    },
    {
      "epoch": 0.337289972899729,
      "step": 6223,
      "training_loss": 5.146989345550537
    },
    {
      "epoch": 0.3373441734417344,
      "grad_norm": 19.061487197875977,
      "learning_rate": 1e-05,
      "loss": 6.4825,
      "step": 6224
    },
    {
      "epoch": 0.3373441734417344,
      "step": 6224,
      "training_loss": 7.438662528991699
    },
    {
      "epoch": 0.33739837398373984,
      "step": 6225,
      "training_loss": 7.487960338592529
    },
    {
      "epoch": 0.33745257452574523,
      "step": 6226,
      "training_loss": 6.586421489715576
    },
    {
      "epoch": 0.3375067750677507,
      "step": 6227,
      "training_loss": 6.215594291687012
    },
    {
      "epoch": 0.3375609756097561,
      "grad_norm": 26.209808349609375,
      "learning_rate": 1e-05,
      "loss": 6.9322,
      "step": 6228
    },
    {
      "epoch": 0.3375609756097561,
      "step": 6228,
      "training_loss": 5.630683898925781
    },
    {
      "epoch": 0.3376151761517615,
      "step": 6229,
      "training_loss": 8.044766426086426
    },
    {
      "epoch": 0.33766937669376695,
      "step": 6230,
      "training_loss": 7.62612247467041
    },
    {
      "epoch": 0.33772357723577234,
      "step": 6231,
      "training_loss": 7.572805404663086
    },
    {
      "epoch": 0.3377777777777778,
      "grad_norm": 24.84682273864746,
      "learning_rate": 1e-05,
      "loss": 7.2186,
      "step": 6232
    },
    {
      "epoch": 0.3377777777777778,
      "step": 6232,
      "training_loss": 6.66788387298584
    },
    {
      "epoch": 0.3378319783197832,
      "step": 6233,
      "training_loss": 7.191273212432861
    },
    {
      "epoch": 0.3378861788617886,
      "step": 6234,
      "training_loss": 6.784770965576172
    },
    {
      "epoch": 0.337940379403794,
      "step": 6235,
      "training_loss": 6.673260688781738
    },
    {
      "epoch": 0.33799457994579946,
      "grad_norm": 19.669052124023438,
      "learning_rate": 1e-05,
      "loss": 6.8293,
      "step": 6236
    },
    {
      "epoch": 0.33799457994579946,
      "step": 6236,
      "training_loss": 6.683608055114746
    },
    {
      "epoch": 0.3380487804878049,
      "step": 6237,
      "training_loss": 7.305322647094727
    },
    {
      "epoch": 0.3381029810298103,
      "step": 6238,
      "training_loss": 7.7246994972229
    },
    {
      "epoch": 0.33815718157181573,
      "step": 6239,
      "training_loss": 7.430775165557861
    },
    {
      "epoch": 0.3382113821138211,
      "grad_norm": 22.578697204589844,
      "learning_rate": 1e-05,
      "loss": 7.2861,
      "step": 6240
    },
    {
      "epoch": 0.3382113821138211,
      "step": 6240,
      "training_loss": 6.159229755401611
    },
    {
      "epoch": 0.33826558265582657,
      "step": 6241,
      "training_loss": 4.06314754486084
    },
    {
      "epoch": 0.33831978319783196,
      "step": 6242,
      "training_loss": 6.517626762390137
    },
    {
      "epoch": 0.3383739837398374,
      "step": 6243,
      "training_loss": 6.066029071807861
    },
    {
      "epoch": 0.3384281842818428,
      "grad_norm": 30.909727096557617,
      "learning_rate": 1e-05,
      "loss": 5.7015,
      "step": 6244
    },
    {
      "epoch": 0.3384281842818428,
      "step": 6244,
      "training_loss": 7.360775470733643
    },
    {
      "epoch": 0.33848238482384824,
      "step": 6245,
      "training_loss": 6.947822570800781
    },
    {
      "epoch": 0.3385365853658537,
      "step": 6246,
      "training_loss": 7.8587822914123535
    },
    {
      "epoch": 0.33859078590785907,
      "step": 6247,
      "training_loss": 6.570969104766846
    },
    {
      "epoch": 0.3386449864498645,
      "grad_norm": 20.239120483398438,
      "learning_rate": 1e-05,
      "loss": 7.1846,
      "step": 6248
    },
    {
      "epoch": 0.3386449864498645,
      "step": 6248,
      "training_loss": 5.227697849273682
    },
    {
      "epoch": 0.3386991869918699,
      "step": 6249,
      "training_loss": 5.796826362609863
    },
    {
      "epoch": 0.33875338753387535,
      "step": 6250,
      "training_loss": 6.5591206550598145
    },
    {
      "epoch": 0.33880758807588074,
      "step": 6251,
      "training_loss": 6.236870288848877
    },
    {
      "epoch": 0.3388617886178862,
      "grad_norm": 20.923749923706055,
      "learning_rate": 1e-05,
      "loss": 5.9551,
      "step": 6252
    },
    {
      "epoch": 0.3388617886178862,
      "step": 6252,
      "training_loss": 6.665060043334961
    },
    {
      "epoch": 0.3389159891598916,
      "step": 6253,
      "training_loss": 6.9359822273254395
    },
    {
      "epoch": 0.338970189701897,
      "step": 6254,
      "training_loss": 3.8466999530792236
    },
    {
      "epoch": 0.33902439024390246,
      "step": 6255,
      "training_loss": 7.620935916900635
    },
    {
      "epoch": 0.33907859078590785,
      "grad_norm": 30.189735412597656,
      "learning_rate": 1e-05,
      "loss": 6.2672,
      "step": 6256
    },
    {
      "epoch": 0.33907859078590785,
      "step": 6256,
      "training_loss": 7.299194812774658
    },
    {
      "epoch": 0.3391327913279133,
      "step": 6257,
      "training_loss": 6.425755977630615
    },
    {
      "epoch": 0.3391869918699187,
      "step": 6258,
      "training_loss": 7.058590412139893
    },
    {
      "epoch": 0.33924119241192413,
      "step": 6259,
      "training_loss": 7.213994026184082
    },
    {
      "epoch": 0.3392953929539295,
      "grad_norm": 23.50928497314453,
      "learning_rate": 1e-05,
      "loss": 6.9994,
      "step": 6260
    },
    {
      "epoch": 0.3392953929539295,
      "step": 6260,
      "training_loss": 7.470625400543213
    },
    {
      "epoch": 0.33934959349593496,
      "step": 6261,
      "training_loss": 7.199131011962891
    },
    {
      "epoch": 0.33940379403794035,
      "step": 6262,
      "training_loss": 6.769397735595703
    },
    {
      "epoch": 0.3394579945799458,
      "step": 6263,
      "training_loss": 7.480622291564941
    },
    {
      "epoch": 0.33951219512195124,
      "grad_norm": 25.745649337768555,
      "learning_rate": 1e-05,
      "loss": 7.2299,
      "step": 6264
    },
    {
      "epoch": 0.33951219512195124,
      "step": 6264,
      "training_loss": 7.802855968475342
    },
    {
      "epoch": 0.33956639566395663,
      "step": 6265,
      "training_loss": 4.918275833129883
    },
    {
      "epoch": 0.3396205962059621,
      "step": 6266,
      "training_loss": 7.788371562957764
    },
    {
      "epoch": 0.33967479674796747,
      "step": 6267,
      "training_loss": 6.661841869354248
    },
    {
      "epoch": 0.3397289972899729,
      "grad_norm": 22.658252716064453,
      "learning_rate": 1e-05,
      "loss": 6.7928,
      "step": 6268
    },
    {
      "epoch": 0.3397289972899729,
      "step": 6268,
      "training_loss": 6.988081455230713
    },
    {
      "epoch": 0.3397831978319783,
      "step": 6269,
      "training_loss": 8.133706092834473
    },
    {
      "epoch": 0.33983739837398375,
      "step": 6270,
      "training_loss": 6.829588413238525
    },
    {
      "epoch": 0.33989159891598913,
      "step": 6271,
      "training_loss": 7.274930477142334
    },
    {
      "epoch": 0.3399457994579946,
      "grad_norm": 36.80942153930664,
      "learning_rate": 1e-05,
      "loss": 7.3066,
      "step": 6272
    },
    {
      "epoch": 0.3399457994579946,
      "step": 6272,
      "training_loss": 6.629634380340576
    },
    {
      "epoch": 0.34,
      "step": 6273,
      "training_loss": 6.556410789489746
    },
    {
      "epoch": 0.3400542005420054,
      "step": 6274,
      "training_loss": 7.391041278839111
    },
    {
      "epoch": 0.34010840108401086,
      "step": 6275,
      "training_loss": 5.841085433959961
    },
    {
      "epoch": 0.34016260162601625,
      "grad_norm": 29.57818603515625,
      "learning_rate": 1e-05,
      "loss": 6.6045,
      "step": 6276
    },
    {
      "epoch": 0.34016260162601625,
      "step": 6276,
      "training_loss": 6.68731689453125
    },
    {
      "epoch": 0.3402168021680217,
      "step": 6277,
      "training_loss": 5.5486578941345215
    },
    {
      "epoch": 0.3402710027100271,
      "step": 6278,
      "training_loss": 6.568977355957031
    },
    {
      "epoch": 0.3403252032520325,
      "step": 6279,
      "training_loss": 7.54710578918457
    },
    {
      "epoch": 0.3403794037940379,
      "grad_norm": 48.415626525878906,
      "learning_rate": 1e-05,
      "loss": 6.588,
      "step": 6280
    },
    {
      "epoch": 0.3403794037940379,
      "step": 6280,
      "training_loss": 6.83445930480957
    },
    {
      "epoch": 0.34043360433604336,
      "step": 6281,
      "training_loss": 8.232970237731934
    },
    {
      "epoch": 0.3404878048780488,
      "step": 6282,
      "training_loss": 7.239229202270508
    },
    {
      "epoch": 0.3405420054200542,
      "step": 6283,
      "training_loss": 6.068480968475342
    },
    {
      "epoch": 0.34059620596205964,
      "grad_norm": 30.897146224975586,
      "learning_rate": 1e-05,
      "loss": 7.0938,
      "step": 6284
    },
    {
      "epoch": 0.34059620596205964,
      "step": 6284,
      "training_loss": 5.931920051574707
    },
    {
      "epoch": 0.34065040650406503,
      "step": 6285,
      "training_loss": 7.7988176345825195
    },
    {
      "epoch": 0.3407046070460705,
      "step": 6286,
      "training_loss": 6.812915802001953
    },
    {
      "epoch": 0.34075880758807586,
      "step": 6287,
      "training_loss": 6.962589740753174
    },
    {
      "epoch": 0.3408130081300813,
      "grad_norm": 16.623821258544922,
      "learning_rate": 1e-05,
      "loss": 6.8766,
      "step": 6288
    },
    {
      "epoch": 0.3408130081300813,
      "step": 6288,
      "training_loss": 5.665805816650391
    },
    {
      "epoch": 0.3408672086720867,
      "step": 6289,
      "training_loss": 7.028487205505371
    },
    {
      "epoch": 0.34092140921409214,
      "step": 6290,
      "training_loss": 5.810740947723389
    },
    {
      "epoch": 0.3409756097560976,
      "step": 6291,
      "training_loss": 6.595656871795654
    },
    {
      "epoch": 0.341029810298103,
      "grad_norm": 17.11758804321289,
      "learning_rate": 1e-05,
      "loss": 6.2752,
      "step": 6292
    },
    {
      "epoch": 0.341029810298103,
      "step": 6292,
      "training_loss": 6.100787162780762
    },
    {
      "epoch": 0.3410840108401084,
      "step": 6293,
      "training_loss": 5.8056182861328125
    },
    {
      "epoch": 0.3411382113821138,
      "step": 6294,
      "training_loss": 6.904923439025879
    },
    {
      "epoch": 0.34119241192411925,
      "step": 6295,
      "training_loss": 7.197823524475098
    },
    {
      "epoch": 0.34124661246612464,
      "grad_norm": 35.751522064208984,
      "learning_rate": 1e-05,
      "loss": 6.5023,
      "step": 6296
    },
    {
      "epoch": 0.34124661246612464,
      "step": 6296,
      "training_loss": 6.193079471588135
    },
    {
      "epoch": 0.3413008130081301,
      "step": 6297,
      "training_loss": 6.1868414878845215
    },
    {
      "epoch": 0.3413550135501355,
      "step": 6298,
      "training_loss": 6.884649753570557
    },
    {
      "epoch": 0.3414092140921409,
      "step": 6299,
      "training_loss": 7.053680896759033
    },
    {
      "epoch": 0.34146341463414637,
      "grad_norm": 32.37422561645508,
      "learning_rate": 1e-05,
      "loss": 6.5796,
      "step": 6300
    },
    {
      "epoch": 0.34146341463414637,
      "step": 6300,
      "training_loss": 6.588493824005127
    },
    {
      "epoch": 0.34151761517615176,
      "step": 6301,
      "training_loss": 7.3429274559021
    },
    {
      "epoch": 0.3415718157181572,
      "step": 6302,
      "training_loss": 6.193696022033691
    },
    {
      "epoch": 0.3416260162601626,
      "step": 6303,
      "training_loss": 6.615804672241211
    },
    {
      "epoch": 0.34168021680216804,
      "grad_norm": 15.850741386413574,
      "learning_rate": 1e-05,
      "loss": 6.6852,
      "step": 6304
    },
    {
      "epoch": 0.34168021680216804,
      "step": 6304,
      "training_loss": 7.167088508605957
    },
    {
      "epoch": 0.3417344173441734,
      "step": 6305,
      "training_loss": 6.255301475524902
    },
    {
      "epoch": 0.34178861788617887,
      "step": 6306,
      "training_loss": 5.172689914703369
    },
    {
      "epoch": 0.34184281842818426,
      "step": 6307,
      "training_loss": 7.683225631713867
    },
    {
      "epoch": 0.3418970189701897,
      "grad_norm": 38.6815185546875,
      "learning_rate": 1e-05,
      "loss": 6.5696,
      "step": 6308
    },
    {
      "epoch": 0.3418970189701897,
      "step": 6308,
      "training_loss": 7.059417247772217
    },
    {
      "epoch": 0.34195121951219515,
      "step": 6309,
      "training_loss": 6.555639266967773
    },
    {
      "epoch": 0.34200542005420054,
      "step": 6310,
      "training_loss": 6.982062816619873
    },
    {
      "epoch": 0.342059620596206,
      "step": 6311,
      "training_loss": 7.009784698486328
    },
    {
      "epoch": 0.34211382113821137,
      "grad_norm": 24.712234497070312,
      "learning_rate": 1e-05,
      "loss": 6.9017,
      "step": 6312
    },
    {
      "epoch": 0.34211382113821137,
      "step": 6312,
      "training_loss": 6.574111461639404
    },
    {
      "epoch": 0.3421680216802168,
      "step": 6313,
      "training_loss": 6.435896396636963
    },
    {
      "epoch": 0.3422222222222222,
      "step": 6314,
      "training_loss": 8.776126861572266
    },
    {
      "epoch": 0.34227642276422765,
      "step": 6315,
      "training_loss": 6.814724922180176
    },
    {
      "epoch": 0.34233062330623304,
      "grad_norm": 27.375497817993164,
      "learning_rate": 1e-05,
      "loss": 7.1502,
      "step": 6316
    },
    {
      "epoch": 0.34233062330623304,
      "step": 6316,
      "training_loss": 7.433483600616455
    },
    {
      "epoch": 0.3423848238482385,
      "step": 6317,
      "training_loss": 6.974978923797607
    },
    {
      "epoch": 0.34243902439024393,
      "step": 6318,
      "training_loss": 7.055800914764404
    },
    {
      "epoch": 0.3424932249322493,
      "step": 6319,
      "training_loss": 6.516908168792725
    },
    {
      "epoch": 0.34254742547425476,
      "grad_norm": 33.944053649902344,
      "learning_rate": 1e-05,
      "loss": 6.9953,
      "step": 6320
    },
    {
      "epoch": 0.34254742547425476,
      "step": 6320,
      "training_loss": 6.417811870574951
    },
    {
      "epoch": 0.34260162601626015,
      "step": 6321,
      "training_loss": 6.41992712020874
    },
    {
      "epoch": 0.3426558265582656,
      "step": 6322,
      "training_loss": 4.023117542266846
    },
    {
      "epoch": 0.342710027100271,
      "step": 6323,
      "training_loss": 7.8327155113220215
    },
    {
      "epoch": 0.34276422764227643,
      "grad_norm": 51.310062408447266,
      "learning_rate": 1e-05,
      "loss": 6.1734,
      "step": 6324
    },
    {
      "epoch": 0.34276422764227643,
      "step": 6324,
      "training_loss": 6.891510009765625
    },
    {
      "epoch": 0.3428184281842818,
      "step": 6325,
      "training_loss": 6.572791576385498
    },
    {
      "epoch": 0.34287262872628727,
      "step": 6326,
      "training_loss": 7.270503997802734
    },
    {
      "epoch": 0.3429268292682927,
      "step": 6327,
      "training_loss": 6.707022666931152
    },
    {
      "epoch": 0.3429810298102981,
      "grad_norm": 18.20306968688965,
      "learning_rate": 1e-05,
      "loss": 6.8605,
      "step": 6328
    },
    {
      "epoch": 0.3429810298102981,
      "step": 6328,
      "training_loss": 7.015679836273193
    },
    {
      "epoch": 0.34303523035230354,
      "step": 6329,
      "training_loss": 7.118435382843018
    },
    {
      "epoch": 0.34308943089430893,
      "step": 6330,
      "training_loss": 6.549257278442383
    },
    {
      "epoch": 0.3431436314363144,
      "step": 6331,
      "training_loss": 6.592155456542969
    },
    {
      "epoch": 0.34319783197831977,
      "grad_norm": 19.940004348754883,
      "learning_rate": 1e-05,
      "loss": 6.8189,
      "step": 6332
    },
    {
      "epoch": 0.34319783197831977,
      "step": 6332,
      "training_loss": 6.274853706359863
    },
    {
      "epoch": 0.3432520325203252,
      "step": 6333,
      "training_loss": 6.803891658782959
    },
    {
      "epoch": 0.3433062330623306,
      "step": 6334,
      "training_loss": 7.480794429779053
    },
    {
      "epoch": 0.34336043360433605,
      "step": 6335,
      "training_loss": 7.574959754943848
    },
    {
      "epoch": 0.3434146341463415,
      "grad_norm": 28.403823852539062,
      "learning_rate": 1e-05,
      "loss": 7.0336,
      "step": 6336
    },
    {
      "epoch": 0.3434146341463415,
      "step": 6336,
      "training_loss": 7.383708477020264
    },
    {
      "epoch": 0.3434688346883469,
      "step": 6337,
      "training_loss": 6.4098358154296875
    },
    {
      "epoch": 0.3435230352303523,
      "step": 6338,
      "training_loss": 4.747142314910889
    },
    {
      "epoch": 0.3435772357723577,
      "step": 6339,
      "training_loss": 5.660088539123535
    },
    {
      "epoch": 0.34363143631436316,
      "grad_norm": 30.0875301361084,
      "learning_rate": 1e-05,
      "loss": 6.0502,
      "step": 6340
    },
    {
      "epoch": 0.34363143631436316,
      "step": 6340,
      "training_loss": 6.206982612609863
    },
    {
      "epoch": 0.34368563685636855,
      "step": 6341,
      "training_loss": 6.0471696853637695
    },
    {
      "epoch": 0.343739837398374,
      "step": 6342,
      "training_loss": 6.864224910736084
    },
    {
      "epoch": 0.3437940379403794,
      "step": 6343,
      "training_loss": 6.505196571350098
    },
    {
      "epoch": 0.3438482384823848,
      "grad_norm": 28.83859634399414,
      "learning_rate": 1e-05,
      "loss": 6.4059,
      "step": 6344
    },
    {
      "epoch": 0.3438482384823848,
      "step": 6344,
      "training_loss": 5.554030418395996
    },
    {
      "epoch": 0.3439024390243902,
      "step": 6345,
      "training_loss": 6.716000080108643
    },
    {
      "epoch": 0.34395663956639566,
      "step": 6346,
      "training_loss": 5.258601665496826
    },
    {
      "epoch": 0.3440108401084011,
      "step": 6347,
      "training_loss": 4.468751430511475
    },
    {
      "epoch": 0.3440650406504065,
      "grad_norm": 37.80878829956055,
      "learning_rate": 1e-05,
      "loss": 5.4993,
      "step": 6348
    },
    {
      "epoch": 0.3440650406504065,
      "step": 6348,
      "training_loss": 3.577669620513916
    },
    {
      "epoch": 0.34411924119241194,
      "step": 6349,
      "training_loss": 5.637235164642334
    },
    {
      "epoch": 0.34417344173441733,
      "step": 6350,
      "training_loss": 6.908473968505859
    },
    {
      "epoch": 0.3442276422764228,
      "step": 6351,
      "training_loss": 6.321985244750977
    },
    {
      "epoch": 0.34428184281842816,
      "grad_norm": 22.651966094970703,
      "learning_rate": 1e-05,
      "loss": 5.6113,
      "step": 6352
    },
    {
      "epoch": 0.34428184281842816,
      "step": 6352,
      "training_loss": 5.345940113067627
    },
    {
      "epoch": 0.3443360433604336,
      "step": 6353,
      "training_loss": 6.9783101081848145
    },
    {
      "epoch": 0.344390243902439,
      "step": 6354,
      "training_loss": 5.252970218658447
    },
    {
      "epoch": 0.34444444444444444,
      "step": 6355,
      "training_loss": 6.822729587554932
    },
    {
      "epoch": 0.3444986449864499,
      "grad_norm": 17.766305923461914,
      "learning_rate": 1e-05,
      "loss": 6.1,
      "step": 6356
    },
    {
      "epoch": 0.3444986449864499,
      "step": 6356,
      "training_loss": 6.931325912475586
    },
    {
      "epoch": 0.3445528455284553,
      "step": 6357,
      "training_loss": 6.276830673217773
    },
    {
      "epoch": 0.3446070460704607,
      "step": 6358,
      "training_loss": 6.746407985687256
    },
    {
      "epoch": 0.3446612466124661,
      "step": 6359,
      "training_loss": 6.2693281173706055
    },
    {
      "epoch": 0.34471544715447155,
      "grad_norm": 19.62092399597168,
      "learning_rate": 1e-05,
      "loss": 6.556,
      "step": 6360
    },
    {
      "epoch": 0.34471544715447155,
      "step": 6360,
      "training_loss": 5.792294979095459
    },
    {
      "epoch": 0.34476964769647694,
      "step": 6361,
      "training_loss": 5.426393508911133
    },
    {
      "epoch": 0.3448238482384824,
      "step": 6362,
      "training_loss": 5.8724045753479
    },
    {
      "epoch": 0.3448780487804878,
      "step": 6363,
      "training_loss": 7.157192230224609
    },
    {
      "epoch": 0.3449322493224932,
      "grad_norm": 30.85821533203125,
      "learning_rate": 1e-05,
      "loss": 6.0621,
      "step": 6364
    },
    {
      "epoch": 0.3449322493224932,
      "step": 6364,
      "training_loss": 5.154865264892578
    },
    {
      "epoch": 0.34498644986449867,
      "step": 6365,
      "training_loss": 7.48168420791626
    },
    {
      "epoch": 0.34504065040650406,
      "step": 6366,
      "training_loss": 6.065273284912109
    },
    {
      "epoch": 0.3450948509485095,
      "step": 6367,
      "training_loss": 3.888328790664673
    },
    {
      "epoch": 0.3451490514905149,
      "grad_norm": 30.438852310180664,
      "learning_rate": 1e-05,
      "loss": 5.6475,
      "step": 6368
    },
    {
      "epoch": 0.3451490514905149,
      "step": 6368,
      "training_loss": 5.263718128204346
    },
    {
      "epoch": 0.34520325203252034,
      "step": 6369,
      "training_loss": 6.911509990692139
    },
    {
      "epoch": 0.3452574525745257,
      "step": 6370,
      "training_loss": 5.7375946044921875
    },
    {
      "epoch": 0.34531165311653117,
      "step": 6371,
      "training_loss": 7.393380165100098
    },
    {
      "epoch": 0.34536585365853656,
      "grad_norm": 17.46609115600586,
      "learning_rate": 1e-05,
      "loss": 6.3266,
      "step": 6372
    },
    {
      "epoch": 0.34536585365853656,
      "step": 6372,
      "training_loss": 4.853726387023926
    },
    {
      "epoch": 0.345420054200542,
      "step": 6373,
      "training_loss": 8.755541801452637
    },
    {
      "epoch": 0.34547425474254745,
      "step": 6374,
      "training_loss": 8.4414701461792
    },
    {
      "epoch": 0.34552845528455284,
      "step": 6375,
      "training_loss": 7.024077892303467
    },
    {
      "epoch": 0.3455826558265583,
      "grad_norm": 23.34258460998535,
      "learning_rate": 1e-05,
      "loss": 7.2687,
      "step": 6376
    },
    {
      "epoch": 0.3455826558265583,
      "step": 6376,
      "training_loss": 7.268304824829102
    },
    {
      "epoch": 0.34563685636856367,
      "step": 6377,
      "training_loss": 7.174885272979736
    },
    {
      "epoch": 0.3456910569105691,
      "step": 6378,
      "training_loss": 6.465145111083984
    },
    {
      "epoch": 0.3457452574525745,
      "step": 6379,
      "training_loss": 6.263390064239502
    },
    {
      "epoch": 0.34579945799457995,
      "grad_norm": 20.906435012817383,
      "learning_rate": 1e-05,
      "loss": 6.7929,
      "step": 6380
    },
    {
      "epoch": 0.34579945799457995,
      "step": 6380,
      "training_loss": 6.5118489265441895
    },
    {
      "epoch": 0.34585365853658534,
      "step": 6381,
      "training_loss": 3.9549622535705566
    },
    {
      "epoch": 0.3459078590785908,
      "step": 6382,
      "training_loss": 7.238216876983643
    },
    {
      "epoch": 0.34596205962059623,
      "step": 6383,
      "training_loss": 6.18924617767334
    },
    {
      "epoch": 0.3460162601626016,
      "grad_norm": 29.61685562133789,
      "learning_rate": 1e-05,
      "loss": 5.9736,
      "step": 6384
    },
    {
      "epoch": 0.3460162601626016,
      "step": 6384,
      "training_loss": 6.159909248352051
    },
    {
      "epoch": 0.34607046070460706,
      "step": 6385,
      "training_loss": 7.016173839569092
    },
    {
      "epoch": 0.34612466124661245,
      "step": 6386,
      "training_loss": 6.183467864990234
    },
    {
      "epoch": 0.3461788617886179,
      "step": 6387,
      "training_loss": 6.584869384765625
    },
    {
      "epoch": 0.3462330623306233,
      "grad_norm": 29.38451385498047,
      "learning_rate": 1e-05,
      "loss": 6.4861,
      "step": 6388
    },
    {
      "epoch": 0.3462330623306233,
      "step": 6388,
      "training_loss": 7.184560775756836
    },
    {
      "epoch": 0.34628726287262873,
      "step": 6389,
      "training_loss": 7.541079521179199
    },
    {
      "epoch": 0.3463414634146341,
      "step": 6390,
      "training_loss": 7.149069786071777
    },
    {
      "epoch": 0.34639566395663957,
      "step": 6391,
      "training_loss": 6.927952289581299
    },
    {
      "epoch": 0.346449864498645,
      "grad_norm": 21.234243392944336,
      "learning_rate": 1e-05,
      "loss": 7.2007,
      "step": 6392
    },
    {
      "epoch": 0.346449864498645,
      "step": 6392,
      "training_loss": 3.6518797874450684
    },
    {
      "epoch": 0.3465040650406504,
      "step": 6393,
      "training_loss": 6.322025299072266
    },
    {
      "epoch": 0.34655826558265584,
      "step": 6394,
      "training_loss": 6.943960666656494
    },
    {
      "epoch": 0.34661246612466123,
      "step": 6395,
      "training_loss": 7.612777233123779
    },
    {
      "epoch": 0.3466666666666667,
      "grad_norm": 32.93720245361328,
      "learning_rate": 1e-05,
      "loss": 6.1327,
      "step": 6396
    },
    {
      "epoch": 0.3466666666666667,
      "step": 6396,
      "training_loss": 7.2560553550720215
    },
    {
      "epoch": 0.34672086720867207,
      "step": 6397,
      "training_loss": 7.412034034729004
    },
    {
      "epoch": 0.3467750677506775,
      "step": 6398,
      "training_loss": 5.251104354858398
    },
    {
      "epoch": 0.3468292682926829,
      "step": 6399,
      "training_loss": 7.030162811279297
    },
    {
      "epoch": 0.34688346883468835,
      "grad_norm": 51.68379211425781,
      "learning_rate": 1e-05,
      "loss": 6.7373,
      "step": 6400
    },
    {
      "epoch": 0.34688346883468835,
      "step": 6400,
      "training_loss": 7.134955406188965
    },
    {
      "epoch": 0.3469376693766938,
      "step": 6401,
      "training_loss": 7.668304443359375
    },
    {
      "epoch": 0.3469918699186992,
      "step": 6402,
      "training_loss": 7.617717742919922
    },
    {
      "epoch": 0.3470460704607046,
      "step": 6403,
      "training_loss": 7.762385845184326
    },
    {
      "epoch": 0.34710027100271,
      "grad_norm": 48.46792984008789,
      "learning_rate": 1e-05,
      "loss": 7.5458,
      "step": 6404
    },
    {
      "epoch": 0.34710027100271,
      "step": 6404,
      "training_loss": 3.51908278465271
    },
    {
      "epoch": 0.34715447154471546,
      "step": 6405,
      "training_loss": 7.57853364944458
    },
    {
      "epoch": 0.34720867208672085,
      "step": 6406,
      "training_loss": 6.9521403312683105
    },
    {
      "epoch": 0.3472628726287263,
      "step": 6407,
      "training_loss": 8.014122009277344
    },
    {
      "epoch": 0.3473170731707317,
      "grad_norm": 61.57145309448242,
      "learning_rate": 1e-05,
      "loss": 6.516,
      "step": 6408
    },
    {
      "epoch": 0.3473170731707317,
      "step": 6408,
      "training_loss": 8.788102149963379
    },
    {
      "epoch": 0.3473712737127371,
      "step": 6409,
      "training_loss": 7.140857696533203
    },
    {
      "epoch": 0.3474254742547426,
      "step": 6410,
      "training_loss": 5.22325325012207
    },
    {
      "epoch": 0.34747967479674796,
      "step": 6411,
      "training_loss": 6.377359867095947
    },
    {
      "epoch": 0.3475338753387534,
      "grad_norm": 21.359596252441406,
      "learning_rate": 1e-05,
      "loss": 6.8824,
      "step": 6412
    },
    {
      "epoch": 0.3475338753387534,
      "step": 6412,
      "training_loss": 7.033940315246582
    },
    {
      "epoch": 0.3475880758807588,
      "step": 6413,
      "training_loss": 7.1536760330200195
    },
    {
      "epoch": 0.34764227642276424,
      "step": 6414,
      "training_loss": 7.402554988861084
    },
    {
      "epoch": 0.34769647696476963,
      "step": 6415,
      "training_loss": 5.5572075843811035
    },
    {
      "epoch": 0.3477506775067751,
      "grad_norm": 28.90810203552246,
      "learning_rate": 1e-05,
      "loss": 6.7868,
      "step": 6416
    },
    {
      "epoch": 0.3477506775067751,
      "step": 6416,
      "training_loss": 5.926836967468262
    },
    {
      "epoch": 0.34780487804878046,
      "step": 6417,
      "training_loss": 7.357771396636963
    },
    {
      "epoch": 0.3478590785907859,
      "step": 6418,
      "training_loss": 6.9245476722717285
    },
    {
      "epoch": 0.34791327913279135,
      "step": 6419,
      "training_loss": 6.13192892074585
    },
    {
      "epoch": 0.34796747967479674,
      "grad_norm": 25.033769607543945,
      "learning_rate": 1e-05,
      "loss": 6.5853,
      "step": 6420
    },
    {
      "epoch": 0.34796747967479674,
      "step": 6420,
      "training_loss": 7.449134349822998
    },
    {
      "epoch": 0.3480216802168022,
      "step": 6421,
      "training_loss": 7.6546950340271
    },
    {
      "epoch": 0.3480758807588076,
      "step": 6422,
      "training_loss": 5.249676704406738
    },
    {
      "epoch": 0.348130081300813,
      "step": 6423,
      "training_loss": 5.042140007019043
    },
    {
      "epoch": 0.3481842818428184,
      "grad_norm": 20.077943801879883,
      "learning_rate": 1e-05,
      "loss": 6.3489,
      "step": 6424
    },
    {
      "epoch": 0.3481842818428184,
      "step": 6424,
      "training_loss": 7.76718807220459
    },
    {
      "epoch": 0.34823848238482386,
      "step": 6425,
      "training_loss": 7.090789318084717
    },
    {
      "epoch": 0.34829268292682924,
      "step": 6426,
      "training_loss": 6.740455150604248
    },
    {
      "epoch": 0.3483468834688347,
      "step": 6427,
      "training_loss": 7.795472145080566
    },
    {
      "epoch": 0.34840108401084013,
      "grad_norm": 20.24775505065918,
      "learning_rate": 1e-05,
      "loss": 7.3485,
      "step": 6428
    },
    {
      "epoch": 0.34840108401084013,
      "step": 6428,
      "training_loss": 6.601312160491943
    },
    {
      "epoch": 0.3484552845528455,
      "step": 6429,
      "training_loss": 4.186994552612305
    },
    {
      "epoch": 0.34850948509485097,
      "step": 6430,
      "training_loss": 5.905497074127197
    },
    {
      "epoch": 0.34856368563685636,
      "step": 6431,
      "training_loss": 7.365419387817383
    },
    {
      "epoch": 0.3486178861788618,
      "grad_norm": 24.495412826538086,
      "learning_rate": 1e-05,
      "loss": 6.0148,
      "step": 6432
    },
    {
      "epoch": 0.3486178861788618,
      "step": 6432,
      "training_loss": 7.950517177581787
    },
    {
      "epoch": 0.3486720867208672,
      "step": 6433,
      "training_loss": 8.329634666442871
    },
    {
      "epoch": 0.34872628726287264,
      "step": 6434,
      "training_loss": 6.296882152557373
    },
    {
      "epoch": 0.348780487804878,
      "step": 6435,
      "training_loss": 5.508078575134277
    },
    {
      "epoch": 0.34883468834688347,
      "grad_norm": 24.41019630432129,
      "learning_rate": 1e-05,
      "loss": 7.0213,
      "step": 6436
    },
    {
      "epoch": 0.34883468834688347,
      "step": 6436,
      "training_loss": 7.198047637939453
    },
    {
      "epoch": 0.3488888888888889,
      "step": 6437,
      "training_loss": 6.18460750579834
    },
    {
      "epoch": 0.3489430894308943,
      "step": 6438,
      "training_loss": 7.135682582855225
    },
    {
      "epoch": 0.34899728997289975,
      "step": 6439,
      "training_loss": 8.664691925048828
    },
    {
      "epoch": 0.34905149051490514,
      "grad_norm": 44.99592971801758,
      "learning_rate": 1e-05,
      "loss": 7.2958,
      "step": 6440
    },
    {
      "epoch": 0.34905149051490514,
      "step": 6440,
      "training_loss": 6.6530046463012695
    },
    {
      "epoch": 0.3491056910569106,
      "step": 6441,
      "training_loss": 7.8564229011535645
    },
    {
      "epoch": 0.349159891598916,
      "step": 6442,
      "training_loss": 7.168155670166016
    },
    {
      "epoch": 0.3492140921409214,
      "step": 6443,
      "training_loss": 7.756242752075195
    },
    {
      "epoch": 0.3492682926829268,
      "grad_norm": 23.935251235961914,
      "learning_rate": 1e-05,
      "loss": 7.3585,
      "step": 6444
    },
    {
      "epoch": 0.3492682926829268,
      "step": 6444,
      "training_loss": 8.0775728225708
    },
    {
      "epoch": 0.34932249322493225,
      "step": 6445,
      "training_loss": 4.193600177764893
    },
    {
      "epoch": 0.3493766937669377,
      "step": 6446,
      "training_loss": 6.157063007354736
    },
    {
      "epoch": 0.3494308943089431,
      "step": 6447,
      "training_loss": 3.142308473587036
    },
    {
      "epoch": 0.34948509485094853,
      "grad_norm": 31.063631057739258,
      "learning_rate": 1e-05,
      "loss": 5.3926,
      "step": 6448
    },
    {
      "epoch": 0.34948509485094853,
      "step": 6448,
      "training_loss": 6.327059268951416
    },
    {
      "epoch": 0.3495392953929539,
      "step": 6449,
      "training_loss": 6.8822736740112305
    },
    {
      "epoch": 0.34959349593495936,
      "step": 6450,
      "training_loss": 7.231649398803711
    },
    {
      "epoch": 0.34964769647696475,
      "step": 6451,
      "training_loss": 6.717939853668213
    },
    {
      "epoch": 0.3497018970189702,
      "grad_norm": 18.255142211914062,
      "learning_rate": 1e-05,
      "loss": 6.7897,
      "step": 6452
    },
    {
      "epoch": 0.3497018970189702,
      "step": 6452,
      "training_loss": 6.970008373260498
    },
    {
      "epoch": 0.3497560975609756,
      "step": 6453,
      "training_loss": 6.352059841156006
    },
    {
      "epoch": 0.34981029810298103,
      "step": 6454,
      "training_loss": 6.724672317504883
    },
    {
      "epoch": 0.3498644986449865,
      "step": 6455,
      "training_loss": 6.855196952819824
    },
    {
      "epoch": 0.34991869918699187,
      "grad_norm": 25.51972770690918,
      "learning_rate": 1e-05,
      "loss": 6.7255,
      "step": 6456
    },
    {
      "epoch": 0.34991869918699187,
      "step": 6456,
      "training_loss": 6.3620429039001465
    },
    {
      "epoch": 0.3499728997289973,
      "step": 6457,
      "training_loss": 6.815540313720703
    },
    {
      "epoch": 0.3500271002710027,
      "step": 6458,
      "training_loss": 8.159809112548828
    },
    {
      "epoch": 0.35008130081300814,
      "step": 6459,
      "training_loss": 6.666754245758057
    },
    {
      "epoch": 0.35013550135501353,
      "grad_norm": 34.65127182006836,
      "learning_rate": 1e-05,
      "loss": 7.001,
      "step": 6460
    },
    {
      "epoch": 0.35013550135501353,
      "step": 6460,
      "training_loss": 7.194031238555908
    },
    {
      "epoch": 0.350189701897019,
      "step": 6461,
      "training_loss": 7.0448408126831055
    },
    {
      "epoch": 0.35024390243902437,
      "step": 6462,
      "training_loss": 6.766623497009277
    },
    {
      "epoch": 0.3502981029810298,
      "step": 6463,
      "training_loss": 6.958500385284424
    },
    {
      "epoch": 0.35035230352303526,
      "grad_norm": 27.05451202392578,
      "learning_rate": 1e-05,
      "loss": 6.991,
      "step": 6464
    },
    {
      "epoch": 0.35035230352303526,
      "step": 6464,
      "training_loss": 7.106897830963135
    },
    {
      "epoch": 0.35040650406504065,
      "step": 6465,
      "training_loss": 5.8979315757751465
    },
    {
      "epoch": 0.3504607046070461,
      "step": 6466,
      "training_loss": 6.844369411468506
    },
    {
      "epoch": 0.3505149051490515,
      "step": 6467,
      "training_loss": 7.53814697265625
    },
    {
      "epoch": 0.3505691056910569,
      "grad_norm": 28.694955825805664,
      "learning_rate": 1e-05,
      "loss": 6.8468,
      "step": 6468
    },
    {
      "epoch": 0.3505691056910569,
      "step": 6468,
      "training_loss": 7.22803258895874
    },
    {
      "epoch": 0.3506233062330623,
      "step": 6469,
      "training_loss": 6.723931312561035
    },
    {
      "epoch": 0.35067750677506776,
      "step": 6470,
      "training_loss": 6.840158939361572
    },
    {
      "epoch": 0.35073170731707315,
      "step": 6471,
      "training_loss": 6.889123439788818
    },
    {
      "epoch": 0.3507859078590786,
      "grad_norm": 19.702560424804688,
      "learning_rate": 1e-05,
      "loss": 6.9203,
      "step": 6472
    },
    {
      "epoch": 0.3507859078590786,
      "step": 6472,
      "training_loss": 6.7496747970581055
    },
    {
      "epoch": 0.350840108401084,
      "step": 6473,
      "training_loss": 6.82058048248291
    },
    {
      "epoch": 0.35089430894308943,
      "step": 6474,
      "training_loss": 6.2145209312438965
    },
    {
      "epoch": 0.3509485094850949,
      "step": 6475,
      "training_loss": 7.215917587280273
    },
    {
      "epoch": 0.35100271002710026,
      "grad_norm": 21.98192024230957,
      "learning_rate": 1e-05,
      "loss": 6.7502,
      "step": 6476
    },
    {
      "epoch": 0.35100271002710026,
      "step": 6476,
      "training_loss": 6.793153762817383
    },
    {
      "epoch": 0.3510569105691057,
      "step": 6477,
      "training_loss": 5.49573278427124
    },
    {
      "epoch": 0.3511111111111111,
      "step": 6478,
      "training_loss": 6.498250961303711
    },
    {
      "epoch": 0.35116531165311654,
      "step": 6479,
      "training_loss": 5.6609907150268555
    },
    {
      "epoch": 0.35121951219512193,
      "grad_norm": 26.256183624267578,
      "learning_rate": 1e-05,
      "loss": 6.112,
      "step": 6480
    },
    {
      "epoch": 0.35121951219512193,
      "step": 6480,
      "training_loss": 7.18055534362793
    },
    {
      "epoch": 0.3512737127371274,
      "step": 6481,
      "training_loss": 6.98810338973999
    },
    {
      "epoch": 0.35132791327913276,
      "step": 6482,
      "training_loss": 6.633907318115234
    },
    {
      "epoch": 0.3513821138211382,
      "step": 6483,
      "training_loss": 7.726110935211182
    },
    {
      "epoch": 0.35143631436314365,
      "grad_norm": 31.432846069335938,
      "learning_rate": 1e-05,
      "loss": 7.1322,
      "step": 6484
    },
    {
      "epoch": 0.35143631436314365,
      "step": 6484,
      "training_loss": 6.207346439361572
    },
    {
      "epoch": 0.35149051490514904,
      "step": 6485,
      "training_loss": 5.941495418548584
    },
    {
      "epoch": 0.3515447154471545,
      "step": 6486,
      "training_loss": 6.837113857269287
    },
    {
      "epoch": 0.3515989159891599,
      "step": 6487,
      "training_loss": 6.8106184005737305
    },
    {
      "epoch": 0.3516531165311653,
      "grad_norm": 21.931997299194336,
      "learning_rate": 1e-05,
      "loss": 6.4491,
      "step": 6488
    },
    {
      "epoch": 0.3516531165311653,
      "step": 6488,
      "training_loss": 7.900659084320068
    },
    {
      "epoch": 0.3517073170731707,
      "step": 6489,
      "training_loss": 7.028137683868408
    },
    {
      "epoch": 0.35176151761517616,
      "step": 6490,
      "training_loss": 6.054415702819824
    },
    {
      "epoch": 0.35181571815718155,
      "step": 6491,
      "training_loss": 6.251192569732666
    },
    {
      "epoch": 0.351869918699187,
      "grad_norm": 35.419822692871094,
      "learning_rate": 1e-05,
      "loss": 6.8086,
      "step": 6492
    },
    {
      "epoch": 0.351869918699187,
      "step": 6492,
      "training_loss": 8.992440223693848
    },
    {
      "epoch": 0.35192411924119243,
      "step": 6493,
      "training_loss": 7.572983264923096
    },
    {
      "epoch": 0.3519783197831978,
      "step": 6494,
      "training_loss": 6.121156692504883
    },
    {
      "epoch": 0.35203252032520327,
      "step": 6495,
      "training_loss": 6.091341972351074
    },
    {
      "epoch": 0.35208672086720866,
      "grad_norm": 23.047616958618164,
      "learning_rate": 1e-05,
      "loss": 7.1945,
      "step": 6496
    },
    {
      "epoch": 0.35208672086720866,
      "step": 6496,
      "training_loss": 6.352682590484619
    },
    {
      "epoch": 0.3521409214092141,
      "step": 6497,
      "training_loss": 8.131243705749512
    },
    {
      "epoch": 0.3521951219512195,
      "step": 6498,
      "training_loss": 7.261756420135498
    },
    {
      "epoch": 0.35224932249322494,
      "step": 6499,
      "training_loss": 5.151254653930664
    },
    {
      "epoch": 0.3523035230352303,
      "grad_norm": 15.848348617553711,
      "learning_rate": 1e-05,
      "loss": 6.7242,
      "step": 6500
    },
    {
      "epoch": 0.3523035230352303,
      "step": 6500,
      "training_loss": 6.934019565582275
    },
    {
      "epoch": 0.35235772357723577,
      "step": 6501,
      "training_loss": 6.709251880645752
    },
    {
      "epoch": 0.3524119241192412,
      "step": 6502,
      "training_loss": 6.347320556640625
    },
    {
      "epoch": 0.3524661246612466,
      "step": 6503,
      "training_loss": 6.379341125488281
    },
    {
      "epoch": 0.35252032520325205,
      "grad_norm": 19.88292121887207,
      "learning_rate": 1e-05,
      "loss": 6.5925,
      "step": 6504
    },
    {
      "epoch": 0.35252032520325205,
      "step": 6504,
      "training_loss": 6.851846694946289
    },
    {
      "epoch": 0.35257452574525744,
      "step": 6505,
      "training_loss": 5.624385356903076
    },
    {
      "epoch": 0.3526287262872629,
      "step": 6506,
      "training_loss": 7.497167110443115
    },
    {
      "epoch": 0.3526829268292683,
      "step": 6507,
      "training_loss": 8.078025817871094
    },
    {
      "epoch": 0.3527371273712737,
      "grad_norm": 22.41208267211914,
      "learning_rate": 1e-05,
      "loss": 7.0129,
      "step": 6508
    },
    {
      "epoch": 0.3527371273712737,
      "step": 6508,
      "training_loss": 5.640442848205566
    },
    {
      "epoch": 0.3527913279132791,
      "step": 6509,
      "training_loss": 6.012951374053955
    },
    {
      "epoch": 0.35284552845528455,
      "step": 6510,
      "training_loss": 6.702563285827637
    },
    {
      "epoch": 0.35289972899729,
      "step": 6511,
      "training_loss": 4.389449119567871
    },
    {
      "epoch": 0.3529539295392954,
      "grad_norm": 25.36896514892578,
      "learning_rate": 1e-05,
      "loss": 5.6864,
      "step": 6512
    },
    {
      "epoch": 0.3529539295392954,
      "step": 6512,
      "training_loss": 5.750751495361328
    },
    {
      "epoch": 0.35300813008130083,
      "step": 6513,
      "training_loss": 7.013433456420898
    },
    {
      "epoch": 0.3530623306233062,
      "step": 6514,
      "training_loss": 6.85248327255249
    },
    {
      "epoch": 0.35311653116531166,
      "step": 6515,
      "training_loss": 6.790577411651611
    },
    {
      "epoch": 0.35317073170731705,
      "grad_norm": 19.346010208129883,
      "learning_rate": 1e-05,
      "loss": 6.6018,
      "step": 6516
    },
    {
      "epoch": 0.35317073170731705,
      "step": 6516,
      "training_loss": 6.430017948150635
    },
    {
      "epoch": 0.3532249322493225,
      "step": 6517,
      "training_loss": 7.1868486404418945
    },
    {
      "epoch": 0.3532791327913279,
      "step": 6518,
      "training_loss": 6.961609363555908
    },
    {
      "epoch": 0.35333333333333333,
      "step": 6519,
      "training_loss": 6.645561695098877
    },
    {
      "epoch": 0.3533875338753388,
      "grad_norm": 46.601112365722656,
      "learning_rate": 1e-05,
      "loss": 6.806,
      "step": 6520
    },
    {
      "epoch": 0.3533875338753388,
      "step": 6520,
      "training_loss": 7.943562984466553
    },
    {
      "epoch": 0.35344173441734417,
      "step": 6521,
      "training_loss": 7.042295455932617
    },
    {
      "epoch": 0.3534959349593496,
      "step": 6522,
      "training_loss": 7.252364158630371
    },
    {
      "epoch": 0.353550135501355,
      "step": 6523,
      "training_loss": 6.625558853149414
    },
    {
      "epoch": 0.35360433604336045,
      "grad_norm": 28.456083297729492,
      "learning_rate": 1e-05,
      "loss": 7.2159,
      "step": 6524
    },
    {
      "epoch": 0.35360433604336045,
      "step": 6524,
      "training_loss": 6.941740036010742
    },
    {
      "epoch": 0.35365853658536583,
      "step": 6525,
      "training_loss": 6.7842230796813965
    },
    {
      "epoch": 0.3537127371273713,
      "step": 6526,
      "training_loss": 7.718615531921387
    },
    {
      "epoch": 0.35376693766937667,
      "step": 6527,
      "training_loss": 6.078638076782227
    },
    {
      "epoch": 0.3538211382113821,
      "grad_norm": 26.433977127075195,
      "learning_rate": 1e-05,
      "loss": 6.8808,
      "step": 6528
    },
    {
      "epoch": 0.3538211382113821,
      "step": 6528,
      "training_loss": 5.8262457847595215
    },
    {
      "epoch": 0.35387533875338756,
      "step": 6529,
      "training_loss": 6.717292785644531
    },
    {
      "epoch": 0.35392953929539295,
      "step": 6530,
      "training_loss": 8.42414379119873
    },
    {
      "epoch": 0.3539837398373984,
      "step": 6531,
      "training_loss": 6.256785869598389
    },
    {
      "epoch": 0.3540379403794038,
      "grad_norm": 25.16558265686035,
      "learning_rate": 1e-05,
      "loss": 6.8061,
      "step": 6532
    },
    {
      "epoch": 0.3540379403794038,
      "step": 6532,
      "training_loss": 6.753457546234131
    },
    {
      "epoch": 0.3540921409214092,
      "step": 6533,
      "training_loss": 6.796695232391357
    },
    {
      "epoch": 0.3541463414634146,
      "step": 6534,
      "training_loss": 6.236731052398682
    },
    {
      "epoch": 0.35420054200542006,
      "step": 6535,
      "training_loss": 7.136058330535889
    },
    {
      "epoch": 0.35425474254742545,
      "grad_norm": 22.554927825927734,
      "learning_rate": 1e-05,
      "loss": 6.7307,
      "step": 6536
    },
    {
      "epoch": 0.35425474254742545,
      "step": 6536,
      "training_loss": 6.645693302154541
    },
    {
      "epoch": 0.3543089430894309,
      "step": 6537,
      "training_loss": 8.671792030334473
    },
    {
      "epoch": 0.35436314363143634,
      "step": 6538,
      "training_loss": 7.5245232582092285
    },
    {
      "epoch": 0.35441734417344173,
      "step": 6539,
      "training_loss": 6.451324462890625
    },
    {
      "epoch": 0.3544715447154472,
      "grad_norm": 33.81816864013672,
      "learning_rate": 1e-05,
      "loss": 7.3233,
      "step": 6540
    },
    {
      "epoch": 0.3544715447154472,
      "step": 6540,
      "training_loss": 6.783705234527588
    },
    {
      "epoch": 0.35452574525745256,
      "step": 6541,
      "training_loss": 7.050596714019775
    },
    {
      "epoch": 0.354579945799458,
      "step": 6542,
      "training_loss": 5.492567539215088
    },
    {
      "epoch": 0.3546341463414634,
      "step": 6543,
      "training_loss": 6.251776218414307
    },
    {
      "epoch": 0.35468834688346884,
      "grad_norm": 40.04548645019531,
      "learning_rate": 1e-05,
      "loss": 6.3947,
      "step": 6544
    },
    {
      "epoch": 0.35468834688346884,
      "step": 6544,
      "training_loss": 7.276416778564453
    },
    {
      "epoch": 0.35474254742547423,
      "step": 6545,
      "training_loss": 6.972763538360596
    },
    {
      "epoch": 0.3547967479674797,
      "step": 6546,
      "training_loss": 7.257777214050293
    },
    {
      "epoch": 0.3548509485094851,
      "step": 6547,
      "training_loss": 7.038117408752441
    },
    {
      "epoch": 0.3549051490514905,
      "grad_norm": 26.332014083862305,
      "learning_rate": 1e-05,
      "loss": 7.1363,
      "step": 6548
    },
    {
      "epoch": 0.3549051490514905,
      "step": 6548,
      "training_loss": 6.012943744659424
    },
    {
      "epoch": 0.35495934959349595,
      "step": 6549,
      "training_loss": 6.8040032386779785
    },
    {
      "epoch": 0.35501355013550134,
      "step": 6550,
      "training_loss": 6.11003303527832
    },
    {
      "epoch": 0.3550677506775068,
      "step": 6551,
      "training_loss": 7.689798355102539
    },
    {
      "epoch": 0.3551219512195122,
      "grad_norm": 18.958255767822266,
      "learning_rate": 1e-05,
      "loss": 6.6542,
      "step": 6552
    },
    {
      "epoch": 0.3551219512195122,
      "step": 6552,
      "training_loss": 6.791553497314453
    },
    {
      "epoch": 0.3551761517615176,
      "step": 6553,
      "training_loss": 7.195243835449219
    },
    {
      "epoch": 0.355230352303523,
      "step": 6554,
      "training_loss": 7.433091163635254
    },
    {
      "epoch": 0.35528455284552846,
      "step": 6555,
      "training_loss": 6.381130695343018
    },
    {
      "epoch": 0.3553387533875339,
      "grad_norm": 27.3834285736084,
      "learning_rate": 1e-05,
      "loss": 6.9503,
      "step": 6556
    },
    {
      "epoch": 0.3553387533875339,
      "step": 6556,
      "training_loss": 6.04202938079834
    },
    {
      "epoch": 0.3553929539295393,
      "step": 6557,
      "training_loss": 5.835136413574219
    },
    {
      "epoch": 0.35544715447154474,
      "step": 6558,
      "training_loss": 6.483502388000488
    },
    {
      "epoch": 0.3555013550135501,
      "step": 6559,
      "training_loss": 6.330499172210693
    },
    {
      "epoch": 0.35555555555555557,
      "grad_norm": 17.806198120117188,
      "learning_rate": 1e-05,
      "loss": 6.1728,
      "step": 6560
    },
    {
      "epoch": 0.35555555555555557,
      "step": 6560,
      "training_loss": 6.41159200668335
    },
    {
      "epoch": 0.35560975609756096,
      "step": 6561,
      "training_loss": 6.28641939163208
    },
    {
      "epoch": 0.3556639566395664,
      "step": 6562,
      "training_loss": 5.946593284606934
    },
    {
      "epoch": 0.3557181571815718,
      "step": 6563,
      "training_loss": 6.830678462982178
    },
    {
      "epoch": 0.35577235772357724,
      "grad_norm": 39.57390213012695,
      "learning_rate": 1e-05,
      "loss": 6.3688,
      "step": 6564
    },
    {
      "epoch": 0.35577235772357724,
      "step": 6564,
      "training_loss": 6.561222076416016
    },
    {
      "epoch": 0.3558265582655827,
      "step": 6565,
      "training_loss": 6.943983554840088
    },
    {
      "epoch": 0.35588075880758807,
      "step": 6566,
      "training_loss": 8.732025146484375
    },
    {
      "epoch": 0.3559349593495935,
      "step": 6567,
      "training_loss": 4.949669361114502
    },
    {
      "epoch": 0.3559891598915989,
      "grad_norm": 22.656999588012695,
      "learning_rate": 1e-05,
      "loss": 6.7967,
      "step": 6568
    },
    {
      "epoch": 0.3559891598915989,
      "step": 6568,
      "training_loss": 5.791256904602051
    },
    {
      "epoch": 0.35604336043360435,
      "step": 6569,
      "training_loss": 6.283870697021484
    },
    {
      "epoch": 0.35609756097560974,
      "step": 6570,
      "training_loss": 6.374179363250732
    },
    {
      "epoch": 0.3561517615176152,
      "step": 6571,
      "training_loss": 8.693872451782227
    },
    {
      "epoch": 0.3562059620596206,
      "grad_norm": 84.82489776611328,
      "learning_rate": 1e-05,
      "loss": 6.7858,
      "step": 6572
    },
    {
      "epoch": 0.3562059620596206,
      "step": 6572,
      "training_loss": 7.1978302001953125
    },
    {
      "epoch": 0.356260162601626,
      "step": 6573,
      "training_loss": 7.4014410972595215
    },
    {
      "epoch": 0.35631436314363146,
      "step": 6574,
      "training_loss": 7.026317596435547
    },
    {
      "epoch": 0.35636856368563685,
      "step": 6575,
      "training_loss": 7.362362384796143
    },
    {
      "epoch": 0.3564227642276423,
      "grad_norm": 27.864118576049805,
      "learning_rate": 1e-05,
      "loss": 7.247,
      "step": 6576
    },
    {
      "epoch": 0.3564227642276423,
      "step": 6576,
      "training_loss": 8.226922035217285
    },
    {
      "epoch": 0.3564769647696477,
      "step": 6577,
      "training_loss": 6.880699634552002
    },
    {
      "epoch": 0.35653116531165313,
      "step": 6578,
      "training_loss": 7.206475734710693
    },
    {
      "epoch": 0.3565853658536585,
      "step": 6579,
      "training_loss": 4.034158706665039
    },
    {
      "epoch": 0.35663956639566397,
      "grad_norm": 31.604604721069336,
      "learning_rate": 1e-05,
      "loss": 6.5871,
      "step": 6580
    },
    {
      "epoch": 0.35663956639566397,
      "step": 6580,
      "training_loss": 6.650999069213867
    },
    {
      "epoch": 0.35669376693766935,
      "step": 6581,
      "training_loss": 6.727376461029053
    },
    {
      "epoch": 0.3567479674796748,
      "step": 6582,
      "training_loss": 7.5861968994140625
    },
    {
      "epoch": 0.35680216802168024,
      "step": 6583,
      "training_loss": 5.644196510314941
    },
    {
      "epoch": 0.35685636856368563,
      "grad_norm": 29.147356033325195,
      "learning_rate": 1e-05,
      "loss": 6.6522,
      "step": 6584
    },
    {
      "epoch": 0.35685636856368563,
      "step": 6584,
      "training_loss": 6.707025527954102
    },
    {
      "epoch": 0.3569105691056911,
      "step": 6585,
      "training_loss": 5.895820140838623
    },
    {
      "epoch": 0.35696476964769647,
      "step": 6586,
      "training_loss": 6.917744159698486
    },
    {
      "epoch": 0.3570189701897019,
      "step": 6587,
      "training_loss": 6.311798572540283
    },
    {
      "epoch": 0.3570731707317073,
      "grad_norm": 24.130203247070312,
      "learning_rate": 1e-05,
      "loss": 6.4581,
      "step": 6588
    },
    {
      "epoch": 0.3570731707317073,
      "step": 6588,
      "training_loss": 6.603028774261475
    },
    {
      "epoch": 0.35712737127371275,
      "step": 6589,
      "training_loss": 7.141239643096924
    },
    {
      "epoch": 0.35718157181571814,
      "step": 6590,
      "training_loss": 6.748022079467773
    },
    {
      "epoch": 0.3572357723577236,
      "step": 6591,
      "training_loss": 7.846853256225586
    },
    {
      "epoch": 0.357289972899729,
      "grad_norm": 35.37565994262695,
      "learning_rate": 1e-05,
      "loss": 7.0848,
      "step": 6592
    },
    {
      "epoch": 0.357289972899729,
      "step": 6592,
      "training_loss": 3.6515891551971436
    },
    {
      "epoch": 0.3573441734417344,
      "step": 6593,
      "training_loss": 5.9548234939575195
    },
    {
      "epoch": 0.35739837398373986,
      "step": 6594,
      "training_loss": 6.32733154296875
    },
    {
      "epoch": 0.35745257452574525,
      "step": 6595,
      "training_loss": 7.395484447479248
    },
    {
      "epoch": 0.3575067750677507,
      "grad_norm": 22.880794525146484,
      "learning_rate": 1e-05,
      "loss": 5.8323,
      "step": 6596
    },
    {
      "epoch": 0.3575067750677507,
      "step": 6596,
      "training_loss": 5.944004058837891
    },
    {
      "epoch": 0.3575609756097561,
      "step": 6597,
      "training_loss": 6.788647651672363
    },
    {
      "epoch": 0.3576151761517615,
      "step": 6598,
      "training_loss": 7.4386305809021
    },
    {
      "epoch": 0.3576693766937669,
      "step": 6599,
      "training_loss": 7.686117172241211
    },
    {
      "epoch": 0.35772357723577236,
      "grad_norm": 32.799400329589844,
      "learning_rate": 1e-05,
      "loss": 6.9643,
      "step": 6600
    },
    {
      "epoch": 0.35772357723577236,
      "step": 6600,
      "training_loss": 7.800227642059326
    },
    {
      "epoch": 0.35777777777777775,
      "step": 6601,
      "training_loss": 7.147615432739258
    },
    {
      "epoch": 0.3578319783197832,
      "step": 6602,
      "training_loss": 8.404892921447754
    },
    {
      "epoch": 0.35788617886178864,
      "step": 6603,
      "training_loss": 6.739175796508789
    },
    {
      "epoch": 0.35794037940379403,
      "grad_norm": 17.39862060546875,
      "learning_rate": 1e-05,
      "loss": 7.523,
      "step": 6604
    },
    {
      "epoch": 0.35794037940379403,
      "step": 6604,
      "training_loss": 6.225778579711914
    },
    {
      "epoch": 0.3579945799457995,
      "step": 6605,
      "training_loss": 7.851912498474121
    },
    {
      "epoch": 0.35804878048780486,
      "step": 6606,
      "training_loss": 6.117790699005127
    },
    {
      "epoch": 0.3581029810298103,
      "step": 6607,
      "training_loss": 6.37053918838501
    },
    {
      "epoch": 0.3581571815718157,
      "grad_norm": 27.76675796508789,
      "learning_rate": 1e-05,
      "loss": 6.6415,
      "step": 6608
    },
    {
      "epoch": 0.3581571815718157,
      "step": 6608,
      "training_loss": 7.1695404052734375
    },
    {
      "epoch": 0.35821138211382114,
      "step": 6609,
      "training_loss": 8.025186538696289
    },
    {
      "epoch": 0.35826558265582653,
      "step": 6610,
      "training_loss": 6.044534683227539
    },
    {
      "epoch": 0.358319783197832,
      "step": 6611,
      "training_loss": 7.138655185699463
    },
    {
      "epoch": 0.3583739837398374,
      "grad_norm": 23.687734603881836,
      "learning_rate": 1e-05,
      "loss": 7.0945,
      "step": 6612
    },
    {
      "epoch": 0.3583739837398374,
      "step": 6612,
      "training_loss": 7.242292881011963
    },
    {
      "epoch": 0.3584281842818428,
      "step": 6613,
      "training_loss": 6.584967613220215
    },
    {
      "epoch": 0.35848238482384825,
      "step": 6614,
      "training_loss": 3.986062526702881
    },
    {
      "epoch": 0.35853658536585364,
      "step": 6615,
      "training_loss": 6.82820463180542
    },
    {
      "epoch": 0.3585907859078591,
      "grad_norm": 21.697168350219727,
      "learning_rate": 1e-05,
      "loss": 6.1604,
      "step": 6616
    },
    {
      "epoch": 0.3585907859078591,
      "step": 6616,
      "training_loss": 6.476848125457764
    },
    {
      "epoch": 0.3586449864498645,
      "step": 6617,
      "training_loss": 7.731224060058594
    },
    {
      "epoch": 0.3586991869918699,
      "step": 6618,
      "training_loss": 7.288486957550049
    },
    {
      "epoch": 0.3587533875338753,
      "step": 6619,
      "training_loss": 6.764151096343994
    },
    {
      "epoch": 0.35880758807588076,
      "grad_norm": 19.322246551513672,
      "learning_rate": 1e-05,
      "loss": 7.0652,
      "step": 6620
    },
    {
      "epoch": 0.35880758807588076,
      "step": 6620,
      "training_loss": 6.795987606048584
    },
    {
      "epoch": 0.3588617886178862,
      "step": 6621,
      "training_loss": 7.9339823722839355
    },
    {
      "epoch": 0.3589159891598916,
      "step": 6622,
      "training_loss": 6.560136795043945
    },
    {
      "epoch": 0.35897018970189704,
      "step": 6623,
      "training_loss": 6.005332946777344
    },
    {
      "epoch": 0.3590243902439024,
      "grad_norm": 51.278133392333984,
      "learning_rate": 1e-05,
      "loss": 6.8239,
      "step": 6624
    },
    {
      "epoch": 0.3590243902439024,
      "step": 6624,
      "training_loss": 5.838364124298096
    },
    {
      "epoch": 0.35907859078590787,
      "step": 6625,
      "training_loss": 6.7544755935668945
    },
    {
      "epoch": 0.35913279132791326,
      "step": 6626,
      "training_loss": 7.977322101593018
    },
    {
      "epoch": 0.3591869918699187,
      "step": 6627,
      "training_loss": 7.809516429901123
    },
    {
      "epoch": 0.3592411924119241,
      "grad_norm": 29.406211853027344,
      "learning_rate": 1e-05,
      "loss": 7.0949,
      "step": 6628
    },
    {
      "epoch": 0.3592411924119241,
      "step": 6628,
      "training_loss": 6.667437553405762
    },
    {
      "epoch": 0.35929539295392954,
      "step": 6629,
      "training_loss": 6.6871724128723145
    },
    {
      "epoch": 0.359349593495935,
      "step": 6630,
      "training_loss": 6.931609153747559
    },
    {
      "epoch": 0.35940379403794037,
      "step": 6631,
      "training_loss": 6.593209266662598
    },
    {
      "epoch": 0.3594579945799458,
      "grad_norm": 35.384315490722656,
      "learning_rate": 1e-05,
      "loss": 6.7199,
      "step": 6632
    },
    {
      "epoch": 0.3594579945799458,
      "step": 6632,
      "training_loss": 7.370378494262695
    },
    {
      "epoch": 0.3595121951219512,
      "step": 6633,
      "training_loss": 4.936680316925049
    },
    {
      "epoch": 0.35956639566395665,
      "step": 6634,
      "training_loss": 4.471035480499268
    },
    {
      "epoch": 0.35962059620596204,
      "step": 6635,
      "training_loss": 7.070708274841309
    },
    {
      "epoch": 0.3596747967479675,
      "grad_norm": 23.633358001708984,
      "learning_rate": 1e-05,
      "loss": 5.9622,
      "step": 6636
    },
    {
      "epoch": 0.3596747967479675,
      "step": 6636,
      "training_loss": 7.493342399597168
    },
    {
      "epoch": 0.3597289972899729,
      "step": 6637,
      "training_loss": 7.881983757019043
    },
    {
      "epoch": 0.3597831978319783,
      "step": 6638,
      "training_loss": 7.717954158782959
    },
    {
      "epoch": 0.35983739837398376,
      "step": 6639,
      "training_loss": 6.855457782745361
    },
    {
      "epoch": 0.35989159891598915,
      "grad_norm": 23.71262550354004,
      "learning_rate": 1e-05,
      "loss": 7.4872,
      "step": 6640
    },
    {
      "epoch": 0.35989159891598915,
      "step": 6640,
      "training_loss": 9.158839225769043
    },
    {
      "epoch": 0.3599457994579946,
      "step": 6641,
      "training_loss": 7.485986709594727
    },
    {
      "epoch": 0.36,
      "step": 6642,
      "training_loss": 7.120582103729248
    },
    {
      "epoch": 0.36005420054200543,
      "step": 6643,
      "training_loss": 7.058218955993652
    },
    {
      "epoch": 0.3601084010840108,
      "grad_norm": 24.864469528198242,
      "learning_rate": 1e-05,
      "loss": 7.7059,
      "step": 6644
    },
    {
      "epoch": 0.3601084010840108,
      "step": 6644,
      "training_loss": 6.97179651260376
    },
    {
      "epoch": 0.36016260162601627,
      "step": 6645,
      "training_loss": 7.249072074890137
    },
    {
      "epoch": 0.36021680216802165,
      "step": 6646,
      "training_loss": 7.753809928894043
    },
    {
      "epoch": 0.3602710027100271,
      "step": 6647,
      "training_loss": 7.822756290435791
    },
    {
      "epoch": 0.36032520325203254,
      "grad_norm": 35.412174224853516,
      "learning_rate": 1e-05,
      "loss": 7.4494,
      "step": 6648
    },
    {
      "epoch": 0.36032520325203254,
      "step": 6648,
      "training_loss": 7.037697792053223
    },
    {
      "epoch": 0.36037940379403793,
      "step": 6649,
      "training_loss": 7.180531978607178
    },
    {
      "epoch": 0.3604336043360434,
      "step": 6650,
      "training_loss": 6.320361137390137
    },
    {
      "epoch": 0.36048780487804877,
      "step": 6651,
      "training_loss": 5.740286827087402
    },
    {
      "epoch": 0.3605420054200542,
      "grad_norm": 23.632341384887695,
      "learning_rate": 1e-05,
      "loss": 6.5697,
      "step": 6652
    },
    {
      "epoch": 0.3605420054200542,
      "step": 6652,
      "training_loss": 7.08156156539917
    },
    {
      "epoch": 0.3605962059620596,
      "step": 6653,
      "training_loss": 6.70101261138916
    },
    {
      "epoch": 0.36065040650406505,
      "step": 6654,
      "training_loss": 7.9522175788879395
    },
    {
      "epoch": 0.36070460704607044,
      "step": 6655,
      "training_loss": 4.22745418548584
    },
    {
      "epoch": 0.3607588075880759,
      "grad_norm": 27.385648727416992,
      "learning_rate": 1e-05,
      "loss": 6.4906,
      "step": 6656
    },
    {
      "epoch": 0.3607588075880759,
      "step": 6656,
      "training_loss": 7.737420558929443
    },
    {
      "epoch": 0.3608130081300813,
      "step": 6657,
      "training_loss": 6.960633277893066
    },
    {
      "epoch": 0.3608672086720867,
      "step": 6658,
      "training_loss": 7.778233528137207
    },
    {
      "epoch": 0.36092140921409216,
      "step": 6659,
      "training_loss": 5.794010162353516
    },
    {
      "epoch": 0.36097560975609755,
      "grad_norm": 20.79888153076172,
      "learning_rate": 1e-05,
      "loss": 7.0676,
      "step": 6660
    },
    {
      "epoch": 0.36097560975609755,
      "step": 6660,
      "training_loss": 8.366907119750977
    },
    {
      "epoch": 0.361029810298103,
      "step": 6661,
      "training_loss": 7.61062479019165
    },
    {
      "epoch": 0.3610840108401084,
      "step": 6662,
      "training_loss": 6.52549934387207
    },
    {
      "epoch": 0.3611382113821138,
      "step": 6663,
      "training_loss": 7.001402378082275
    },
    {
      "epoch": 0.3611924119241192,
      "grad_norm": 17.053295135498047,
      "learning_rate": 1e-05,
      "loss": 7.3761,
      "step": 6664
    },
    {
      "epoch": 0.3611924119241192,
      "step": 6664,
      "training_loss": 8.22494125366211
    },
    {
      "epoch": 0.36124661246612466,
      "step": 6665,
      "training_loss": 5.110729217529297
    },
    {
      "epoch": 0.3613008130081301,
      "step": 6666,
      "training_loss": 5.9787445068359375
    },
    {
      "epoch": 0.3613550135501355,
      "step": 6667,
      "training_loss": 7.021988868713379
    },
    {
      "epoch": 0.36140921409214094,
      "grad_norm": 15.664604187011719,
      "learning_rate": 1e-05,
      "loss": 6.5841,
      "step": 6668
    },
    {
      "epoch": 0.36140921409214094,
      "step": 6668,
      "training_loss": 7.798600673675537
    },
    {
      "epoch": 0.36146341463414633,
      "step": 6669,
      "training_loss": 7.749302387237549
    },
    {
      "epoch": 0.3615176151761518,
      "step": 6670,
      "training_loss": 7.375100135803223
    },
    {
      "epoch": 0.36157181571815716,
      "step": 6671,
      "training_loss": 6.219274520874023
    },
    {
      "epoch": 0.3616260162601626,
      "grad_norm": 23.646451950073242,
      "learning_rate": 1e-05,
      "loss": 7.2856,
      "step": 6672
    },
    {
      "epoch": 0.3616260162601626,
      "step": 6672,
      "training_loss": 7.066266059875488
    },
    {
      "epoch": 0.361680216802168,
      "step": 6673,
      "training_loss": 7.089017391204834
    },
    {
      "epoch": 0.36173441734417344,
      "step": 6674,
      "training_loss": 6.417721748352051
    },
    {
      "epoch": 0.3617886178861789,
      "step": 6675,
      "training_loss": 6.931331634521484
    },
    {
      "epoch": 0.3618428184281843,
      "grad_norm": 25.17891502380371,
      "learning_rate": 1e-05,
      "loss": 6.8761,
      "step": 6676
    },
    {
      "epoch": 0.3618428184281843,
      "step": 6676,
      "training_loss": 6.903581142425537
    },
    {
      "epoch": 0.3618970189701897,
      "step": 6677,
      "training_loss": 5.983407497406006
    },
    {
      "epoch": 0.3619512195121951,
      "step": 6678,
      "training_loss": 7.686522006988525
    },
    {
      "epoch": 0.36200542005420056,
      "step": 6679,
      "training_loss": 7.130943775177002
    },
    {
      "epoch": 0.36205962059620594,
      "grad_norm": 27.061866760253906,
      "learning_rate": 1e-05,
      "loss": 6.9261,
      "step": 6680
    },
    {
      "epoch": 0.36205962059620594,
      "step": 6680,
      "training_loss": 6.149702072143555
    },
    {
      "epoch": 0.3621138211382114,
      "step": 6681,
      "training_loss": 6.88123893737793
    },
    {
      "epoch": 0.3621680216802168,
      "step": 6682,
      "training_loss": 5.089265823364258
    },
    {
      "epoch": 0.3622222222222222,
      "step": 6683,
      "training_loss": 7.3995041847229
    },
    {
      "epoch": 0.36227642276422767,
      "grad_norm": 19.45016098022461,
      "learning_rate": 1e-05,
      "loss": 6.3799,
      "step": 6684
    },
    {
      "epoch": 0.36227642276422767,
      "step": 6684,
      "training_loss": 7.041067600250244
    },
    {
      "epoch": 0.36233062330623306,
      "step": 6685,
      "training_loss": 4.74206018447876
    },
    {
      "epoch": 0.3623848238482385,
      "step": 6686,
      "training_loss": 7.135110378265381
    },
    {
      "epoch": 0.3624390243902439,
      "step": 6687,
      "training_loss": 7.174950122833252
    },
    {
      "epoch": 0.36249322493224934,
      "grad_norm": 34.810611724853516,
      "learning_rate": 1e-05,
      "loss": 6.5233,
      "step": 6688
    },
    {
      "epoch": 0.36249322493224934,
      "step": 6688,
      "training_loss": 6.597105503082275
    },
    {
      "epoch": 0.3625474254742547,
      "step": 6689,
      "training_loss": 6.432358264923096
    },
    {
      "epoch": 0.36260162601626017,
      "step": 6690,
      "training_loss": 5.8985676765441895
    },
    {
      "epoch": 0.36265582655826556,
      "step": 6691,
      "training_loss": 6.018073081970215
    },
    {
      "epoch": 0.362710027100271,
      "grad_norm": 22.57537078857422,
      "learning_rate": 1e-05,
      "loss": 6.2365,
      "step": 6692
    },
    {
      "epoch": 0.362710027100271,
      "step": 6692,
      "training_loss": 7.021673202514648
    },
    {
      "epoch": 0.36276422764227645,
      "step": 6693,
      "training_loss": 6.465336322784424
    },
    {
      "epoch": 0.36281842818428184,
      "step": 6694,
      "training_loss": 7.193671703338623
    },
    {
      "epoch": 0.3628726287262873,
      "step": 6695,
      "training_loss": 6.585031509399414
    },
    {
      "epoch": 0.36292682926829267,
      "grad_norm": 25.06048583984375,
      "learning_rate": 1e-05,
      "loss": 6.8164,
      "step": 6696
    },
    {
      "epoch": 0.36292682926829267,
      "step": 6696,
      "training_loss": 5.111147403717041
    },
    {
      "epoch": 0.3629810298102981,
      "step": 6697,
      "training_loss": 5.392783164978027
    },
    {
      "epoch": 0.3630352303523035,
      "step": 6698,
      "training_loss": 3.973543882369995
    },
    {
      "epoch": 0.36308943089430895,
      "step": 6699,
      "training_loss": 7.306787967681885
    },
    {
      "epoch": 0.36314363143631434,
      "grad_norm": 21.112424850463867,
      "learning_rate": 1e-05,
      "loss": 5.4461,
      "step": 6700
    },
    {
      "epoch": 0.36314363143631434,
      "step": 6700,
      "training_loss": 6.74818229675293
    },
    {
      "epoch": 0.3631978319783198,
      "step": 6701,
      "training_loss": 5.934051036834717
    },
    {
      "epoch": 0.36325203252032523,
      "step": 6702,
      "training_loss": 6.7948198318481445
    },
    {
      "epoch": 0.3633062330623306,
      "step": 6703,
      "training_loss": 7.35202693939209
    },
    {
      "epoch": 0.36336043360433606,
      "grad_norm": 22.114002227783203,
      "learning_rate": 1e-05,
      "loss": 6.7073,
      "step": 6704
    },
    {
      "epoch": 0.36336043360433606,
      "step": 6704,
      "training_loss": 4.292536735534668
    },
    {
      "epoch": 0.36341463414634145,
      "step": 6705,
      "training_loss": 9.15273380279541
    },
    {
      "epoch": 0.3634688346883469,
      "step": 6706,
      "training_loss": 5.590038299560547
    },
    {
      "epoch": 0.3635230352303523,
      "step": 6707,
      "training_loss": 6.882701396942139
    },
    {
      "epoch": 0.36357723577235773,
      "grad_norm": 33.0245361328125,
      "learning_rate": 1e-05,
      "loss": 6.4795,
      "step": 6708
    },
    {
      "epoch": 0.36357723577235773,
      "step": 6708,
      "training_loss": 6.53597354888916
    },
    {
      "epoch": 0.3636314363143631,
      "step": 6709,
      "training_loss": 4.852235794067383
    },
    {
      "epoch": 0.36368563685636857,
      "step": 6710,
      "training_loss": 6.206319332122803
    },
    {
      "epoch": 0.363739837398374,
      "step": 6711,
      "training_loss": 7.210384368896484
    },
    {
      "epoch": 0.3637940379403794,
      "grad_norm": 21.534828186035156,
      "learning_rate": 1e-05,
      "loss": 6.2012,
      "step": 6712
    },
    {
      "epoch": 0.3637940379403794,
      "step": 6712,
      "training_loss": 6.929117202758789
    },
    {
      "epoch": 0.36384823848238484,
      "step": 6713,
      "training_loss": 7.908634185791016
    },
    {
      "epoch": 0.36390243902439023,
      "step": 6714,
      "training_loss": 7.8359694480896
    },
    {
      "epoch": 0.3639566395663957,
      "step": 6715,
      "training_loss": 7.291739463806152
    },
    {
      "epoch": 0.36401084010840107,
      "grad_norm": 24.981536865234375,
      "learning_rate": 1e-05,
      "loss": 7.4914,
      "step": 6716
    },
    {
      "epoch": 0.36401084010840107,
      "step": 6716,
      "training_loss": 6.887685298919678
    },
    {
      "epoch": 0.3640650406504065,
      "step": 6717,
      "training_loss": 6.627788066864014
    },
    {
      "epoch": 0.3641192411924119,
      "step": 6718,
      "training_loss": 6.988602161407471
    },
    {
      "epoch": 0.36417344173441735,
      "step": 6719,
      "training_loss": 7.591480731964111
    },
    {
      "epoch": 0.3642276422764228,
      "grad_norm": 29.34467887878418,
      "learning_rate": 1e-05,
      "loss": 7.0239,
      "step": 6720
    },
    {
      "epoch": 0.3642276422764228,
      "step": 6720,
      "training_loss": 7.016384601593018
    },
    {
      "epoch": 0.3642818428184282,
      "step": 6721,
      "training_loss": 7.458726406097412
    },
    {
      "epoch": 0.3643360433604336,
      "step": 6722,
      "training_loss": 4.909348964691162
    },
    {
      "epoch": 0.364390243902439,
      "step": 6723,
      "training_loss": 6.565119743347168
    },
    {
      "epoch": 0.36444444444444446,
      "grad_norm": 20.13692283630371,
      "learning_rate": 1e-05,
      "loss": 6.4874,
      "step": 6724
    },
    {
      "epoch": 0.36444444444444446,
      "step": 6724,
      "training_loss": 5.827091217041016
    },
    {
      "epoch": 0.36449864498644985,
      "step": 6725,
      "training_loss": 6.301394939422607
    },
    {
      "epoch": 0.3645528455284553,
      "step": 6726,
      "training_loss": 6.67828369140625
    },
    {
      "epoch": 0.3646070460704607,
      "step": 6727,
      "training_loss": 4.039531707763672
    },
    {
      "epoch": 0.36466124661246613,
      "grad_norm": 29.05623435974121,
      "learning_rate": 1e-05,
      "loss": 5.7116,
      "step": 6728
    },
    {
      "epoch": 0.36466124661246613,
      "step": 6728,
      "training_loss": 8.12631893157959
    },
    {
      "epoch": 0.3647154471544715,
      "step": 6729,
      "training_loss": 6.795499801635742
    },
    {
      "epoch": 0.36476964769647696,
      "step": 6730,
      "training_loss": 7.026108741760254
    },
    {
      "epoch": 0.3648238482384824,
      "step": 6731,
      "training_loss": 4.592558860778809
    },
    {
      "epoch": 0.3648780487804878,
      "grad_norm": 21.79595375061035,
      "learning_rate": 1e-05,
      "loss": 6.6351,
      "step": 6732
    },
    {
      "epoch": 0.3648780487804878,
      "step": 6732,
      "training_loss": 6.885189056396484
    },
    {
      "epoch": 0.36493224932249324,
      "step": 6733,
      "training_loss": 7.703930854797363
    },
    {
      "epoch": 0.36498644986449863,
      "step": 6734,
      "training_loss": 5.878296852111816
    },
    {
      "epoch": 0.3650406504065041,
      "step": 6735,
      "training_loss": 7.824390888214111
    },
    {
      "epoch": 0.36509485094850946,
      "grad_norm": 38.18163299560547,
      "learning_rate": 1e-05,
      "loss": 7.073,
      "step": 6736
    },
    {
      "epoch": 0.36509485094850946,
      "step": 6736,
      "training_loss": 7.054073333740234
    },
    {
      "epoch": 0.3651490514905149,
      "step": 6737,
      "training_loss": 8.036407470703125
    },
    {
      "epoch": 0.3652032520325203,
      "step": 6738,
      "training_loss": 4.896052360534668
    },
    {
      "epoch": 0.36525745257452574,
      "step": 6739,
      "training_loss": 6.565099716186523
    },
    {
      "epoch": 0.3653116531165312,
      "grad_norm": 18.107717514038086,
      "learning_rate": 1e-05,
      "loss": 6.6379,
      "step": 6740
    },
    {
      "epoch": 0.3653116531165312,
      "step": 6740,
      "training_loss": 6.9062066078186035
    },
    {
      "epoch": 0.3653658536585366,
      "step": 6741,
      "training_loss": 6.298330783843994
    },
    {
      "epoch": 0.365420054200542,
      "step": 6742,
      "training_loss": 6.414133071899414
    },
    {
      "epoch": 0.3654742547425474,
      "step": 6743,
      "training_loss": 7.23737096786499
    },
    {
      "epoch": 0.36552845528455286,
      "grad_norm": 20.151622772216797,
      "learning_rate": 1e-05,
      "loss": 6.714,
      "step": 6744
    },
    {
      "epoch": 0.36552845528455286,
      "step": 6744,
      "training_loss": 6.075956344604492
    },
    {
      "epoch": 0.36558265582655824,
      "step": 6745,
      "training_loss": 7.299052715301514
    },
    {
      "epoch": 0.3656368563685637,
      "step": 6746,
      "training_loss": 7.298227310180664
    },
    {
      "epoch": 0.3656910569105691,
      "step": 6747,
      "training_loss": 7.304922103881836
    },
    {
      "epoch": 0.3657452574525745,
      "grad_norm": 22.623878479003906,
      "learning_rate": 1e-05,
      "loss": 6.9945,
      "step": 6748
    },
    {
      "epoch": 0.3657452574525745,
      "step": 6748,
      "training_loss": 6.745022296905518
    },
    {
      "epoch": 0.36579945799457997,
      "step": 6749,
      "training_loss": 7.697759628295898
    },
    {
      "epoch": 0.36585365853658536,
      "step": 6750,
      "training_loss": 6.813319683074951
    },
    {
      "epoch": 0.3659078590785908,
      "step": 6751,
      "training_loss": 6.4053144454956055
    },
    {
      "epoch": 0.3659620596205962,
      "grad_norm": 26.744638442993164,
      "learning_rate": 1e-05,
      "loss": 6.9154,
      "step": 6752
    },
    {
      "epoch": 0.3659620596205962,
      "step": 6752,
      "training_loss": 7.501651287078857
    },
    {
      "epoch": 0.36601626016260164,
      "step": 6753,
      "training_loss": 7.261375904083252
    },
    {
      "epoch": 0.366070460704607,
      "step": 6754,
      "training_loss": 5.917473793029785
    },
    {
      "epoch": 0.36612466124661247,
      "step": 6755,
      "training_loss": 7.64984655380249
    },
    {
      "epoch": 0.36617886178861786,
      "grad_norm": 24.154245376586914,
      "learning_rate": 1e-05,
      "loss": 7.0826,
      "step": 6756
    },
    {
      "epoch": 0.36617886178861786,
      "step": 6756,
      "training_loss": 7.442232608795166
    },
    {
      "epoch": 0.3662330623306233,
      "step": 6757,
      "training_loss": 6.661323547363281
    },
    {
      "epoch": 0.36628726287262875,
      "step": 6758,
      "training_loss": 6.757535934448242
    },
    {
      "epoch": 0.36634146341463414,
      "step": 6759,
      "training_loss": 7.127504348754883
    },
    {
      "epoch": 0.3663956639566396,
      "grad_norm": 32.74821853637695,
      "learning_rate": 1e-05,
      "loss": 6.9971,
      "step": 6760
    },
    {
      "epoch": 0.3663956639566396,
      "step": 6760,
      "training_loss": 6.966625213623047
    },
    {
      "epoch": 0.366449864498645,
      "step": 6761,
      "training_loss": 5.206571578979492
    },
    {
      "epoch": 0.3665040650406504,
      "step": 6762,
      "training_loss": 6.789125442504883
    },
    {
      "epoch": 0.3665582655826558,
      "step": 6763,
      "training_loss": 6.485213279724121
    },
    {
      "epoch": 0.36661246612466125,
      "grad_norm": 21.758634567260742,
      "learning_rate": 1e-05,
      "loss": 6.3619,
      "step": 6764
    },
    {
      "epoch": 0.36661246612466125,
      "step": 6764,
      "training_loss": 6.233237266540527
    },
    {
      "epoch": 0.36666666666666664,
      "step": 6765,
      "training_loss": 6.991031646728516
    },
    {
      "epoch": 0.3667208672086721,
      "step": 6766,
      "training_loss": 7.814845561981201
    },
    {
      "epoch": 0.36677506775067753,
      "step": 6767,
      "training_loss": 5.649831771850586
    },
    {
      "epoch": 0.3668292682926829,
      "grad_norm": 19.371904373168945,
      "learning_rate": 1e-05,
      "loss": 6.6722,
      "step": 6768
    },
    {
      "epoch": 0.3668292682926829,
      "step": 6768,
      "training_loss": 7.369756698608398
    },
    {
      "epoch": 0.36688346883468836,
      "step": 6769,
      "training_loss": 5.927788734436035
    },
    {
      "epoch": 0.36693766937669375,
      "step": 6770,
      "training_loss": 6.069366455078125
    },
    {
      "epoch": 0.3669918699186992,
      "step": 6771,
      "training_loss": 7.739939212799072
    },
    {
      "epoch": 0.3670460704607046,
      "grad_norm": 22.200273513793945,
      "learning_rate": 1e-05,
      "loss": 6.7767,
      "step": 6772
    },
    {
      "epoch": 0.3670460704607046,
      "step": 6772,
      "training_loss": 7.795598983764648
    },
    {
      "epoch": 0.36710027100271003,
      "step": 6773,
      "training_loss": 6.92857551574707
    },
    {
      "epoch": 0.3671544715447154,
      "step": 6774,
      "training_loss": 6.844959735870361
    },
    {
      "epoch": 0.36720867208672087,
      "step": 6775,
      "training_loss": 6.533808708190918
    },
    {
      "epoch": 0.3672628726287263,
      "grad_norm": 32.68263244628906,
      "learning_rate": 1e-05,
      "loss": 7.0257,
      "step": 6776
    },
    {
      "epoch": 0.3672628726287263,
      "step": 6776,
      "training_loss": 6.974741458892822
    },
    {
      "epoch": 0.3673170731707317,
      "step": 6777,
      "training_loss": 5.645468235015869
    },
    {
      "epoch": 0.36737127371273715,
      "step": 6778,
      "training_loss": 6.546815872192383
    },
    {
      "epoch": 0.36742547425474253,
      "step": 6779,
      "training_loss": 6.084687232971191
    },
    {
      "epoch": 0.367479674796748,
      "grad_norm": 75.1548080444336,
      "learning_rate": 1e-05,
      "loss": 6.3129,
      "step": 6780
    },
    {
      "epoch": 0.367479674796748,
      "step": 6780,
      "training_loss": 6.3757734298706055
    },
    {
      "epoch": 0.36753387533875337,
      "step": 6781,
      "training_loss": 6.240445137023926
    },
    {
      "epoch": 0.3675880758807588,
      "step": 6782,
      "training_loss": 5.396172523498535
    },
    {
      "epoch": 0.3676422764227642,
      "step": 6783,
      "training_loss": 6.43947696685791
    },
    {
      "epoch": 0.36769647696476965,
      "grad_norm": 17.55197525024414,
      "learning_rate": 1e-05,
      "loss": 6.113,
      "step": 6784
    },
    {
      "epoch": 0.36769647696476965,
      "step": 6784,
      "training_loss": 6.200657844543457
    },
    {
      "epoch": 0.3677506775067751,
      "step": 6785,
      "training_loss": 6.638781547546387
    },
    {
      "epoch": 0.3678048780487805,
      "step": 6786,
      "training_loss": 7.130745887756348
    },
    {
      "epoch": 0.3678590785907859,
      "step": 6787,
      "training_loss": 6.123559474945068
    },
    {
      "epoch": 0.3679132791327913,
      "grad_norm": 30.300151824951172,
      "learning_rate": 1e-05,
      "loss": 6.5234,
      "step": 6788
    },
    {
      "epoch": 0.3679132791327913,
      "step": 6788,
      "training_loss": 6.444717884063721
    },
    {
      "epoch": 0.36796747967479676,
      "step": 6789,
      "training_loss": 6.568289756774902
    },
    {
      "epoch": 0.36802168021680215,
      "step": 6790,
      "training_loss": 7.748286247253418
    },
    {
      "epoch": 0.3680758807588076,
      "step": 6791,
      "training_loss": 7.622835636138916
    },
    {
      "epoch": 0.368130081300813,
      "grad_norm": 29.85880470275879,
      "learning_rate": 1e-05,
      "loss": 7.096,
      "step": 6792
    },
    {
      "epoch": 0.368130081300813,
      "step": 6792,
      "training_loss": 6.876766681671143
    },
    {
      "epoch": 0.36818428184281843,
      "step": 6793,
      "training_loss": 6.488679885864258
    },
    {
      "epoch": 0.3682384823848239,
      "step": 6794,
      "training_loss": 6.872808456420898
    },
    {
      "epoch": 0.36829268292682926,
      "step": 6795,
      "training_loss": 8.18111515045166
    },
    {
      "epoch": 0.3683468834688347,
      "grad_norm": 22.92772102355957,
      "learning_rate": 1e-05,
      "loss": 7.1048,
      "step": 6796
    },
    {
      "epoch": 0.3683468834688347,
      "step": 6796,
      "training_loss": 7.965398788452148
    },
    {
      "epoch": 0.3684010840108401,
      "step": 6797,
      "training_loss": 7.715510845184326
    },
    {
      "epoch": 0.36845528455284554,
      "step": 6798,
      "training_loss": 7.56705904006958
    },
    {
      "epoch": 0.36850948509485093,
      "step": 6799,
      "training_loss": 5.723963260650635
    },
    {
      "epoch": 0.3685636856368564,
      "grad_norm": 27.400339126586914,
      "learning_rate": 1e-05,
      "loss": 7.243,
      "step": 6800
    },
    {
      "epoch": 0.3685636856368564,
      "step": 6800,
      "training_loss": 7.046982288360596
    },
    {
      "epoch": 0.36861788617886176,
      "step": 6801,
      "training_loss": 5.494089603424072
    },
    {
      "epoch": 0.3686720867208672,
      "step": 6802,
      "training_loss": 6.962437152862549
    },
    {
      "epoch": 0.36872628726287265,
      "step": 6803,
      "training_loss": 5.9609527587890625
    },
    {
      "epoch": 0.36878048780487804,
      "grad_norm": 47.23640823364258,
      "learning_rate": 1e-05,
      "loss": 6.3661,
      "step": 6804
    },
    {
      "epoch": 0.36878048780487804,
      "step": 6804,
      "training_loss": 6.790295124053955
    },
    {
      "epoch": 0.3688346883468835,
      "step": 6805,
      "training_loss": 6.528672218322754
    },
    {
      "epoch": 0.3688888888888889,
      "step": 6806,
      "training_loss": 7.175172328948975
    },
    {
      "epoch": 0.3689430894308943,
      "step": 6807,
      "training_loss": 6.989517688751221
    },
    {
      "epoch": 0.3689972899728997,
      "grad_norm": 61.4882926940918,
      "learning_rate": 1e-05,
      "loss": 6.8709,
      "step": 6808
    },
    {
      "epoch": 0.3689972899728997,
      "step": 6808,
      "training_loss": 4.13694429397583
    },
    {
      "epoch": 0.36905149051490516,
      "step": 6809,
      "training_loss": 6.957061767578125
    },
    {
      "epoch": 0.36910569105691055,
      "step": 6810,
      "training_loss": 6.906156539916992
    },
    {
      "epoch": 0.369159891598916,
      "step": 6811,
      "training_loss": 5.973457336425781
    },
    {
      "epoch": 0.36921409214092143,
      "grad_norm": 33.22321319580078,
      "learning_rate": 1e-05,
      "loss": 5.9934,
      "step": 6812
    },
    {
      "epoch": 0.36921409214092143,
      "step": 6812,
      "training_loss": 8.5363130569458
    },
    {
      "epoch": 0.3692682926829268,
      "step": 6813,
      "training_loss": 6.565407752990723
    },
    {
      "epoch": 0.36932249322493227,
      "step": 6814,
      "training_loss": 6.747478485107422
    },
    {
      "epoch": 0.36937669376693766,
      "step": 6815,
      "training_loss": 6.970669746398926
    },
    {
      "epoch": 0.3694308943089431,
      "grad_norm": 16.165372848510742,
      "learning_rate": 1e-05,
      "loss": 7.205,
      "step": 6816
    },
    {
      "epoch": 0.3694308943089431,
      "step": 6816,
      "training_loss": 5.975772857666016
    },
    {
      "epoch": 0.3694850948509485,
      "step": 6817,
      "training_loss": 8.459402084350586
    },
    {
      "epoch": 0.36953929539295394,
      "step": 6818,
      "training_loss": 7.200088024139404
    },
    {
      "epoch": 0.3695934959349593,
      "step": 6819,
      "training_loss": 6.465674877166748
    },
    {
      "epoch": 0.36964769647696477,
      "grad_norm": 21.088457107543945,
      "learning_rate": 1e-05,
      "loss": 7.0252,
      "step": 6820
    },
    {
      "epoch": 0.36964769647696477,
      "step": 6820,
      "training_loss": 6.927291393280029
    },
    {
      "epoch": 0.3697018970189702,
      "step": 6821,
      "training_loss": 7.5679779052734375
    },
    {
      "epoch": 0.3697560975609756,
      "step": 6822,
      "training_loss": 6.632697105407715
    },
    {
      "epoch": 0.36981029810298105,
      "step": 6823,
      "training_loss": 4.862186431884766
    },
    {
      "epoch": 0.36986449864498644,
      "grad_norm": 24.927433013916016,
      "learning_rate": 1e-05,
      "loss": 6.4975,
      "step": 6824
    },
    {
      "epoch": 0.36986449864498644,
      "step": 6824,
      "training_loss": 5.953434944152832
    },
    {
      "epoch": 0.3699186991869919,
      "step": 6825,
      "training_loss": 5.243303298950195
    },
    {
      "epoch": 0.3699728997289973,
      "step": 6826,
      "training_loss": 5.775541305541992
    },
    {
      "epoch": 0.3700271002710027,
      "step": 6827,
      "training_loss": 6.817763805389404
    },
    {
      "epoch": 0.3700813008130081,
      "grad_norm": 21.141420364379883,
      "learning_rate": 1e-05,
      "loss": 5.9475,
      "step": 6828
    },
    {
      "epoch": 0.3700813008130081,
      "step": 6828,
      "training_loss": 7.217027187347412
    },
    {
      "epoch": 0.37013550135501355,
      "step": 6829,
      "training_loss": 3.7050178050994873
    },
    {
      "epoch": 0.370189701897019,
      "step": 6830,
      "training_loss": 6.3599348068237305
    },
    {
      "epoch": 0.3702439024390244,
      "step": 6831,
      "training_loss": 5.644771575927734
    },
    {
      "epoch": 0.37029810298102983,
      "grad_norm": 35.42595291137695,
      "learning_rate": 1e-05,
      "loss": 5.7317,
      "step": 6832
    },
    {
      "epoch": 0.37029810298102983,
      "step": 6832,
      "training_loss": 8.770609855651855
    },
    {
      "epoch": 0.3703523035230352,
      "step": 6833,
      "training_loss": 6.070414066314697
    },
    {
      "epoch": 0.37040650406504066,
      "step": 6834,
      "training_loss": 6.930695533752441
    },
    {
      "epoch": 0.37046070460704605,
      "step": 6835,
      "training_loss": 6.632177829742432
    },
    {
      "epoch": 0.3705149051490515,
      "grad_norm": 25.22545623779297,
      "learning_rate": 1e-05,
      "loss": 7.101,
      "step": 6836
    },
    {
      "epoch": 0.3705149051490515,
      "step": 6836,
      "training_loss": 6.6497416496276855
    },
    {
      "epoch": 0.3705691056910569,
      "step": 6837,
      "training_loss": 7.688798427581787
    },
    {
      "epoch": 0.37062330623306233,
      "step": 6838,
      "training_loss": 7.151877403259277
    },
    {
      "epoch": 0.3706775067750678,
      "step": 6839,
      "training_loss": 5.180576801300049
    },
    {
      "epoch": 0.37073170731707317,
      "grad_norm": 27.149742126464844,
      "learning_rate": 1e-05,
      "loss": 6.6677,
      "step": 6840
    },
    {
      "epoch": 0.37073170731707317,
      "step": 6840,
      "training_loss": 7.272748947143555
    },
    {
      "epoch": 0.3707859078590786,
      "step": 6841,
      "training_loss": 7.059701919555664
    },
    {
      "epoch": 0.370840108401084,
      "step": 6842,
      "training_loss": 7.083401203155518
    },
    {
      "epoch": 0.37089430894308945,
      "step": 6843,
      "training_loss": 8.88982105255127
    },
    {
      "epoch": 0.37094850948509484,
      "grad_norm": 75.68962860107422,
      "learning_rate": 1e-05,
      "loss": 7.5764,
      "step": 6844
    },
    {
      "epoch": 0.37094850948509484,
      "step": 6844,
      "training_loss": 7.181268692016602
    },
    {
      "epoch": 0.3710027100271003,
      "step": 6845,
      "training_loss": 6.405343055725098
    },
    {
      "epoch": 0.37105691056910567,
      "step": 6846,
      "training_loss": 7.350888729095459
    },
    {
      "epoch": 0.3711111111111111,
      "step": 6847,
      "training_loss": 5.979510307312012
    },
    {
      "epoch": 0.37116531165311656,
      "grad_norm": 24.528013229370117,
      "learning_rate": 1e-05,
      "loss": 6.7293,
      "step": 6848
    },
    {
      "epoch": 0.37116531165311656,
      "step": 6848,
      "training_loss": 8.914355278015137
    },
    {
      "epoch": 0.37121951219512195,
      "step": 6849,
      "training_loss": 6.736927509307861
    },
    {
      "epoch": 0.3712737127371274,
      "step": 6850,
      "training_loss": 6.911015033721924
    },
    {
      "epoch": 0.3713279132791328,
      "step": 6851,
      "training_loss": 6.463784694671631
    },
    {
      "epoch": 0.3713821138211382,
      "grad_norm": 23.68647003173828,
      "learning_rate": 1e-05,
      "loss": 7.2565,
      "step": 6852
    },
    {
      "epoch": 0.3713821138211382,
      "step": 6852,
      "training_loss": 6.687892436981201
    },
    {
      "epoch": 0.3714363143631436,
      "step": 6853,
      "training_loss": 7.963924407958984
    },
    {
      "epoch": 0.37149051490514906,
      "step": 6854,
      "training_loss": 7.048602104187012
    },
    {
      "epoch": 0.37154471544715445,
      "step": 6855,
      "training_loss": 5.67565393447876
    },
    {
      "epoch": 0.3715989159891599,
      "grad_norm": 24.851526260375977,
      "learning_rate": 1e-05,
      "loss": 6.844,
      "step": 6856
    },
    {
      "epoch": 0.3715989159891599,
      "step": 6856,
      "training_loss": 7.239675521850586
    },
    {
      "epoch": 0.3716531165311653,
      "step": 6857,
      "training_loss": 7.354898929595947
    },
    {
      "epoch": 0.37170731707317073,
      "step": 6858,
      "training_loss": 7.929262638092041
    },
    {
      "epoch": 0.3717615176151762,
      "step": 6859,
      "training_loss": 6.294778347015381
    },
    {
      "epoch": 0.37181571815718156,
      "grad_norm": 29.15407371520996,
      "learning_rate": 1e-05,
      "loss": 7.2047,
      "step": 6860
    },
    {
      "epoch": 0.37181571815718156,
      "step": 6860,
      "training_loss": 7.820303440093994
    },
    {
      "epoch": 0.371869918699187,
      "step": 6861,
      "training_loss": 7.092202186584473
    },
    {
      "epoch": 0.3719241192411924,
      "step": 6862,
      "training_loss": 7.563591480255127
    },
    {
      "epoch": 0.37197831978319784,
      "step": 6863,
      "training_loss": 6.547972679138184
    },
    {
      "epoch": 0.37203252032520323,
      "grad_norm": 27.72176170349121,
      "learning_rate": 1e-05,
      "loss": 7.256,
      "step": 6864
    },
    {
      "epoch": 0.37203252032520323,
      "step": 6864,
      "training_loss": 6.878727912902832
    },
    {
      "epoch": 0.3720867208672087,
      "step": 6865,
      "training_loss": 5.142728328704834
    },
    {
      "epoch": 0.37214092140921406,
      "step": 6866,
      "training_loss": 6.1219892501831055
    },
    {
      "epoch": 0.3721951219512195,
      "step": 6867,
      "training_loss": 6.9098615646362305
    },
    {
      "epoch": 0.37224932249322495,
      "grad_norm": 21.048173904418945,
      "learning_rate": 1e-05,
      "loss": 6.2633,
      "step": 6868
    },
    {
      "epoch": 0.37224932249322495,
      "step": 6868,
      "training_loss": 4.924676895141602
    },
    {
      "epoch": 0.37230352303523034,
      "step": 6869,
      "training_loss": 6.4606032371521
    },
    {
      "epoch": 0.3723577235772358,
      "step": 6870,
      "training_loss": 7.6628570556640625
    },
    {
      "epoch": 0.3724119241192412,
      "step": 6871,
      "training_loss": 5.038130760192871
    },
    {
      "epoch": 0.3724661246612466,
      "grad_norm": 27.587322235107422,
      "learning_rate": 1e-05,
      "loss": 6.0216,
      "step": 6872
    },
    {
      "epoch": 0.3724661246612466,
      "step": 6872,
      "training_loss": 5.401768207550049
    },
    {
      "epoch": 0.372520325203252,
      "step": 6873,
      "training_loss": 6.673570156097412
    },
    {
      "epoch": 0.37257452574525746,
      "step": 6874,
      "training_loss": 5.682947158813477
    },
    {
      "epoch": 0.37262872628726285,
      "step": 6875,
      "training_loss": 6.720434665679932
    },
    {
      "epoch": 0.3726829268292683,
      "grad_norm": 38.62486267089844,
      "learning_rate": 1e-05,
      "loss": 6.1197,
      "step": 6876
    },
    {
      "epoch": 0.3726829268292683,
      "step": 6876,
      "training_loss": 5.143740177154541
    },
    {
      "epoch": 0.37273712737127374,
      "step": 6877,
      "training_loss": 7.220987796783447
    },
    {
      "epoch": 0.3727913279132791,
      "step": 6878,
      "training_loss": 5.637367248535156
    },
    {
      "epoch": 0.37284552845528457,
      "step": 6879,
      "training_loss": 5.494185924530029
    },
    {
      "epoch": 0.37289972899728996,
      "grad_norm": 21.472431182861328,
      "learning_rate": 1e-05,
      "loss": 5.8741,
      "step": 6880
    },
    {
      "epoch": 0.37289972899728996,
      "step": 6880,
      "training_loss": 7.5033979415893555
    },
    {
      "epoch": 0.3729539295392954,
      "step": 6881,
      "training_loss": 4.0686845779418945
    },
    {
      "epoch": 0.3730081300813008,
      "step": 6882,
      "training_loss": 6.945824146270752
    },
    {
      "epoch": 0.37306233062330624,
      "step": 6883,
      "training_loss": 5.229550361633301
    },
    {
      "epoch": 0.3731165311653116,
      "grad_norm": 25.975069046020508,
      "learning_rate": 1e-05,
      "loss": 5.9369,
      "step": 6884
    },
    {
      "epoch": 0.3731165311653116,
      "step": 6884,
      "training_loss": 5.839024543762207
    },
    {
      "epoch": 0.37317073170731707,
      "step": 6885,
      "training_loss": 6.237910270690918
    },
    {
      "epoch": 0.3732249322493225,
      "step": 6886,
      "training_loss": 6.733879566192627
    },
    {
      "epoch": 0.3732791327913279,
      "step": 6887,
      "training_loss": 7.25178861618042
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 24.036773681640625,
      "learning_rate": 1e-05,
      "loss": 6.5157,
      "step": 6888
    },
    {
      "epoch": 0.37333333333333335,
      "step": 6888,
      "training_loss": 6.338096618652344
    },
    {
      "epoch": 0.37338753387533874,
      "step": 6889,
      "training_loss": 6.493306636810303
    },
    {
      "epoch": 0.3734417344173442,
      "step": 6890,
      "training_loss": 6.3550333976745605
    },
    {
      "epoch": 0.3734959349593496,
      "step": 6891,
      "training_loss": 6.903665542602539
    },
    {
      "epoch": 0.373550135501355,
      "grad_norm": 19.937255859375,
      "learning_rate": 1e-05,
      "loss": 6.5225,
      "step": 6892
    },
    {
      "epoch": 0.373550135501355,
      "step": 6892,
      "training_loss": 6.752291202545166
    },
    {
      "epoch": 0.3736043360433604,
      "step": 6893,
      "training_loss": 6.099608421325684
    },
    {
      "epoch": 0.37365853658536585,
      "step": 6894,
      "training_loss": 6.531599521636963
    },
    {
      "epoch": 0.3737127371273713,
      "step": 6895,
      "training_loss": 6.82792329788208
    },
    {
      "epoch": 0.3737669376693767,
      "grad_norm": 21.293384552001953,
      "learning_rate": 1e-05,
      "loss": 6.5529,
      "step": 6896
    },
    {
      "epoch": 0.3737669376693767,
      "step": 6896,
      "training_loss": 7.067358493804932
    },
    {
      "epoch": 0.37382113821138213,
      "step": 6897,
      "training_loss": 6.670102596282959
    },
    {
      "epoch": 0.3738753387533875,
      "step": 6898,
      "training_loss": 5.264103412628174
    },
    {
      "epoch": 0.37392953929539297,
      "step": 6899,
      "training_loss": 7.865945816040039
    },
    {
      "epoch": 0.37398373983739835,
      "grad_norm": 25.025325775146484,
      "learning_rate": 1e-05,
      "loss": 6.7169,
      "step": 6900
    },
    {
      "epoch": 0.37398373983739835,
      "step": 6900,
      "training_loss": 7.781487941741943
    },
    {
      "epoch": 0.3740379403794038,
      "step": 6901,
      "training_loss": 7.00124454498291
    },
    {
      "epoch": 0.3740921409214092,
      "step": 6902,
      "training_loss": 6.308883190155029
    },
    {
      "epoch": 0.37414634146341463,
      "step": 6903,
      "training_loss": 5.531224250793457
    },
    {
      "epoch": 0.3742005420054201,
      "grad_norm": 30.041797637939453,
      "learning_rate": 1e-05,
      "loss": 6.6557,
      "step": 6904
    },
    {
      "epoch": 0.3742005420054201,
      "step": 6904,
      "training_loss": 7.472126483917236
    },
    {
      "epoch": 0.37425474254742547,
      "step": 6905,
      "training_loss": 6.438598155975342
    },
    {
      "epoch": 0.3743089430894309,
      "step": 6906,
      "training_loss": 6.97121000289917
    },
    {
      "epoch": 0.3743631436314363,
      "step": 6907,
      "training_loss": 5.774769306182861
    },
    {
      "epoch": 0.37441734417344175,
      "grad_norm": 28.620033264160156,
      "learning_rate": 1e-05,
      "loss": 6.6642,
      "step": 6908
    },
    {
      "epoch": 0.37441734417344175,
      "step": 6908,
      "training_loss": 7.1214518547058105
    },
    {
      "epoch": 0.37447154471544714,
      "step": 6909,
      "training_loss": 7.0958170890808105
    },
    {
      "epoch": 0.3745257452574526,
      "step": 6910,
      "training_loss": 7.243278503417969
    },
    {
      "epoch": 0.37457994579945797,
      "step": 6911,
      "training_loss": 4.821236610412598
    },
    {
      "epoch": 0.3746341463414634,
      "grad_norm": 23.034090042114258,
      "learning_rate": 1e-05,
      "loss": 6.5704,
      "step": 6912
    },
    {
      "epoch": 0.3746341463414634,
      "step": 6912,
      "training_loss": 5.890326023101807
    },
    {
      "epoch": 0.37468834688346886,
      "step": 6913,
      "training_loss": 7.8511271476745605
    },
    {
      "epoch": 0.37474254742547425,
      "step": 6914,
      "training_loss": 7.6506171226501465
    },
    {
      "epoch": 0.3747967479674797,
      "step": 6915,
      "training_loss": 5.778909683227539
    },
    {
      "epoch": 0.3748509485094851,
      "grad_norm": 72.24595642089844,
      "learning_rate": 1e-05,
      "loss": 6.7927,
      "step": 6916
    },
    {
      "epoch": 0.3748509485094851,
      "step": 6916,
      "training_loss": 5.794347763061523
    },
    {
      "epoch": 0.3749051490514905,
      "step": 6917,
      "training_loss": 7.7222137451171875
    },
    {
      "epoch": 0.3749593495934959,
      "step": 6918,
      "training_loss": 7.152623176574707
    },
    {
      "epoch": 0.37501355013550136,
      "step": 6919,
      "training_loss": 7.111287593841553
    },
    {
      "epoch": 0.37506775067750675,
      "grad_norm": 24.77666664123535,
      "learning_rate": 1e-05,
      "loss": 6.9451,
      "step": 6920
    },
    {
      "epoch": 0.37506775067750675,
      "step": 6920,
      "training_loss": 5.216424942016602
    },
    {
      "epoch": 0.3751219512195122,
      "step": 6921,
      "training_loss": 7.57433557510376
    },
    {
      "epoch": 0.37517615176151764,
      "step": 6922,
      "training_loss": 6.028023719787598
    },
    {
      "epoch": 0.37523035230352303,
      "step": 6923,
      "training_loss": 7.33193826675415
    },
    {
      "epoch": 0.3752845528455285,
      "grad_norm": 32.39814376831055,
      "learning_rate": 1e-05,
      "loss": 6.5377,
      "step": 6924
    },
    {
      "epoch": 0.3752845528455285,
      "step": 6924,
      "training_loss": 6.123784065246582
    },
    {
      "epoch": 0.37533875338753386,
      "step": 6925,
      "training_loss": 5.674304008483887
    },
    {
      "epoch": 0.3753929539295393,
      "step": 6926,
      "training_loss": 5.834909915924072
    },
    {
      "epoch": 0.3754471544715447,
      "step": 6927,
      "training_loss": 7.063019752502441
    },
    {
      "epoch": 0.37550135501355014,
      "grad_norm": 34.39344787597656,
      "learning_rate": 1e-05,
      "loss": 6.174,
      "step": 6928
    },
    {
      "epoch": 0.37550135501355014,
      "step": 6928,
      "training_loss": 7.728333473205566
    },
    {
      "epoch": 0.37555555555555553,
      "step": 6929,
      "training_loss": 5.6715288162231445
    },
    {
      "epoch": 0.375609756097561,
      "step": 6930,
      "training_loss": 5.14422082901001
    },
    {
      "epoch": 0.3756639566395664,
      "step": 6931,
      "training_loss": 6.802100658416748
    },
    {
      "epoch": 0.3757181571815718,
      "grad_norm": 35.41592025756836,
      "learning_rate": 1e-05,
      "loss": 6.3365,
      "step": 6932
    },
    {
      "epoch": 0.3757181571815718,
      "step": 6932,
      "training_loss": 6.689964771270752
    },
    {
      "epoch": 0.37577235772357725,
      "step": 6933,
      "training_loss": 6.44517183303833
    },
    {
      "epoch": 0.37582655826558264,
      "step": 6934,
      "training_loss": 8.385297775268555
    },
    {
      "epoch": 0.3758807588075881,
      "step": 6935,
      "training_loss": 7.4003987312316895
    },
    {
      "epoch": 0.3759349593495935,
      "grad_norm": 41.561336517333984,
      "learning_rate": 1e-05,
      "loss": 7.2302,
      "step": 6936
    },
    {
      "epoch": 0.3759349593495935,
      "step": 6936,
      "training_loss": 7.193836212158203
    },
    {
      "epoch": 0.3759891598915989,
      "step": 6937,
      "training_loss": 5.886139392852783
    },
    {
      "epoch": 0.3760433604336043,
      "step": 6938,
      "training_loss": 8.148164749145508
    },
    {
      "epoch": 0.37609756097560976,
      "step": 6939,
      "training_loss": 7.377781867980957
    },
    {
      "epoch": 0.3761517615176152,
      "grad_norm": 17.984594345092773,
      "learning_rate": 1e-05,
      "loss": 7.1515,
      "step": 6940
    },
    {
      "epoch": 0.3761517615176152,
      "step": 6940,
      "training_loss": 6.662384986877441
    },
    {
      "epoch": 0.3762059620596206,
      "step": 6941,
      "training_loss": 7.781246662139893
    },
    {
      "epoch": 0.37626016260162604,
      "step": 6942,
      "training_loss": 7.035322666168213
    },
    {
      "epoch": 0.3763143631436314,
      "step": 6943,
      "training_loss": 7.372323036193848
    },
    {
      "epoch": 0.37636856368563687,
      "grad_norm": 18.083641052246094,
      "learning_rate": 1e-05,
      "loss": 7.2128,
      "step": 6944
    },
    {
      "epoch": 0.37636856368563687,
      "step": 6944,
      "training_loss": 6.547684669494629
    },
    {
      "epoch": 0.37642276422764226,
      "step": 6945,
      "training_loss": 7.0902323722839355
    },
    {
      "epoch": 0.3764769647696477,
      "step": 6946,
      "training_loss": 7.084469318389893
    },
    {
      "epoch": 0.3765311653116531,
      "step": 6947,
      "training_loss": 6.828534126281738
    },
    {
      "epoch": 0.37658536585365854,
      "grad_norm": 21.573535919189453,
      "learning_rate": 1e-05,
      "loss": 6.8877,
      "step": 6948
    },
    {
      "epoch": 0.37658536585365854,
      "step": 6948,
      "training_loss": 6.243709564208984
    },
    {
      "epoch": 0.376639566395664,
      "step": 6949,
      "training_loss": 7.974456310272217
    },
    {
      "epoch": 0.37669376693766937,
      "step": 6950,
      "training_loss": 7.4833083152771
    },
    {
      "epoch": 0.3767479674796748,
      "step": 6951,
      "training_loss": 7.167367935180664
    },
    {
      "epoch": 0.3768021680216802,
      "grad_norm": 25.37021255493164,
      "learning_rate": 1e-05,
      "loss": 7.2172,
      "step": 6952
    },
    {
      "epoch": 0.3768021680216802,
      "step": 6952,
      "training_loss": 6.176992893218994
    },
    {
      "epoch": 0.37685636856368565,
      "step": 6953,
      "training_loss": 5.94606351852417
    },
    {
      "epoch": 0.37691056910569104,
      "step": 6954,
      "training_loss": 6.92100191116333
    },
    {
      "epoch": 0.3769647696476965,
      "step": 6955,
      "training_loss": 8.480734825134277
    },
    {
      "epoch": 0.3770189701897019,
      "grad_norm": 48.028018951416016,
      "learning_rate": 1e-05,
      "loss": 6.8812,
      "step": 6956
    },
    {
      "epoch": 0.3770189701897019,
      "step": 6956,
      "training_loss": 7.07943868637085
    },
    {
      "epoch": 0.3770731707317073,
      "step": 6957,
      "training_loss": 7.695176601409912
    },
    {
      "epoch": 0.37712737127371276,
      "step": 6958,
      "training_loss": 6.114814281463623
    },
    {
      "epoch": 0.37718157181571815,
      "step": 6959,
      "training_loss": 6.503608703613281
    },
    {
      "epoch": 0.3772357723577236,
      "grad_norm": 31.18461799621582,
      "learning_rate": 1e-05,
      "loss": 6.8483,
      "step": 6960
    },
    {
      "epoch": 0.3772357723577236,
      "step": 6960,
      "training_loss": 6.385972499847412
    },
    {
      "epoch": 0.377289972899729,
      "step": 6961,
      "training_loss": 6.116177082061768
    },
    {
      "epoch": 0.37734417344173443,
      "step": 6962,
      "training_loss": 6.850268363952637
    },
    {
      "epoch": 0.3773983739837398,
      "step": 6963,
      "training_loss": 7.2313337326049805
    },
    {
      "epoch": 0.37745257452574527,
      "grad_norm": 17.435041427612305,
      "learning_rate": 1e-05,
      "loss": 6.6459,
      "step": 6964
    },
    {
      "epoch": 0.37745257452574527,
      "step": 6964,
      "training_loss": 6.413352012634277
    },
    {
      "epoch": 0.37750677506775066,
      "step": 6965,
      "training_loss": 6.467925548553467
    },
    {
      "epoch": 0.3775609756097561,
      "step": 6966,
      "training_loss": 7.061180591583252
    },
    {
      "epoch": 0.37761517615176154,
      "step": 6967,
      "training_loss": 6.575417518615723
    },
    {
      "epoch": 0.37766937669376693,
      "grad_norm": 24.980825424194336,
      "learning_rate": 1e-05,
      "loss": 6.6295,
      "step": 6968
    },
    {
      "epoch": 0.37766937669376693,
      "step": 6968,
      "training_loss": 6.991919040679932
    },
    {
      "epoch": 0.3777235772357724,
      "step": 6969,
      "training_loss": 8.193552017211914
    },
    {
      "epoch": 0.37777777777777777,
      "step": 6970,
      "training_loss": 6.673854827880859
    },
    {
      "epoch": 0.3778319783197832,
      "step": 6971,
      "training_loss": 7.221465587615967
    },
    {
      "epoch": 0.3778861788617886,
      "grad_norm": 30.335176467895508,
      "learning_rate": 1e-05,
      "loss": 7.2702,
      "step": 6972
    },
    {
      "epoch": 0.3778861788617886,
      "step": 6972,
      "training_loss": 6.829179286956787
    },
    {
      "epoch": 0.37794037940379405,
      "step": 6973,
      "training_loss": 8.896193504333496
    },
    {
      "epoch": 0.37799457994579944,
      "step": 6974,
      "training_loss": 6.193957328796387
    },
    {
      "epoch": 0.3780487804878049,
      "step": 6975,
      "training_loss": 6.675784111022949
    },
    {
      "epoch": 0.3781029810298103,
      "grad_norm": 38.64054489135742,
      "learning_rate": 1e-05,
      "loss": 7.1488,
      "step": 6976
    },
    {
      "epoch": 0.3781029810298103,
      "step": 6976,
      "training_loss": 7.090846538543701
    },
    {
      "epoch": 0.3781571815718157,
      "step": 6977,
      "training_loss": 5.796453952789307
    },
    {
      "epoch": 0.37821138211382116,
      "step": 6978,
      "training_loss": 6.686551570892334
    },
    {
      "epoch": 0.37826558265582655,
      "step": 6979,
      "training_loss": 6.606532573699951
    },
    {
      "epoch": 0.378319783197832,
      "grad_norm": 29.0800724029541,
      "learning_rate": 1e-05,
      "loss": 6.5451,
      "step": 6980
    },
    {
      "epoch": 0.378319783197832,
      "step": 6980,
      "training_loss": 7.920861721038818
    },
    {
      "epoch": 0.3783739837398374,
      "step": 6981,
      "training_loss": 6.985860824584961
    },
    {
      "epoch": 0.3784281842818428,
      "step": 6982,
      "training_loss": 6.178020000457764
    },
    {
      "epoch": 0.3784823848238482,
      "step": 6983,
      "training_loss": 4.912003993988037
    },
    {
      "epoch": 0.37853658536585366,
      "grad_norm": 28.89142608642578,
      "learning_rate": 1e-05,
      "loss": 6.4992,
      "step": 6984
    },
    {
      "epoch": 0.37853658536585366,
      "step": 6984,
      "training_loss": 5.839718818664551
    },
    {
      "epoch": 0.37859078590785905,
      "step": 6985,
      "training_loss": 5.980053424835205
    },
    {
      "epoch": 0.3786449864498645,
      "step": 6986,
      "training_loss": 5.657026290893555
    },
    {
      "epoch": 0.37869918699186994,
      "step": 6987,
      "training_loss": 7.776857376098633
    },
    {
      "epoch": 0.37875338753387533,
      "grad_norm": 24.705909729003906,
      "learning_rate": 1e-05,
      "loss": 6.3134,
      "step": 6988
    },
    {
      "epoch": 0.37875338753387533,
      "step": 6988,
      "training_loss": 7.482309341430664
    },
    {
      "epoch": 0.3788075880758808,
      "step": 6989,
      "training_loss": 8.239633560180664
    },
    {
      "epoch": 0.37886178861788616,
      "step": 6990,
      "training_loss": 6.156041145324707
    },
    {
      "epoch": 0.3789159891598916,
      "step": 6991,
      "training_loss": 6.611276149749756
    },
    {
      "epoch": 0.378970189701897,
      "grad_norm": 34.19172286987305,
      "learning_rate": 1e-05,
      "loss": 7.1223,
      "step": 6992
    },
    {
      "epoch": 0.378970189701897,
      "step": 6992,
      "training_loss": 3.745753049850464
    },
    {
      "epoch": 0.37902439024390244,
      "step": 6993,
      "training_loss": 4.5110673904418945
    },
    {
      "epoch": 0.37907859078590783,
      "step": 6994,
      "training_loss": 7.233354091644287
    },
    {
      "epoch": 0.3791327913279133,
      "step": 6995,
      "training_loss": 6.747306823730469
    },
    {
      "epoch": 0.3791869918699187,
      "grad_norm": 20.945871353149414,
      "learning_rate": 1e-05,
      "loss": 5.5594,
      "step": 6996
    },
    {
      "epoch": 0.3791869918699187,
      "step": 6996,
      "training_loss": 7.376743316650391
    },
    {
      "epoch": 0.3792411924119241,
      "step": 6997,
      "training_loss": 6.429549694061279
    },
    {
      "epoch": 0.37929539295392956,
      "step": 6998,
      "training_loss": 5.022339820861816
    },
    {
      "epoch": 0.37934959349593494,
      "step": 6999,
      "training_loss": 6.892563343048096
    },
    {
      "epoch": 0.3794037940379404,
      "grad_norm": 17.53384780883789,
      "learning_rate": 1e-05,
      "loss": 6.4303,
      "step": 7000
    },
    {
      "epoch": 0.3794037940379404,
      "step": 7000,
      "training_loss": 5.177480697631836
    },
    {
      "epoch": 0.3794579945799458,
      "step": 7001,
      "training_loss": 6.339449882507324
    },
    {
      "epoch": 0.3795121951219512,
      "step": 7002,
      "training_loss": 7.285297393798828
    },
    {
      "epoch": 0.3795663956639566,
      "step": 7003,
      "training_loss": 7.527730464935303
    },
    {
      "epoch": 0.37962059620596206,
      "grad_norm": 77.96324157714844,
      "learning_rate": 1e-05,
      "loss": 6.5825,
      "step": 7004
    },
    {
      "epoch": 0.37962059620596206,
      "step": 7004,
      "training_loss": 7.225308895111084
    },
    {
      "epoch": 0.3796747967479675,
      "step": 7005,
      "training_loss": 8.112135887145996
    },
    {
      "epoch": 0.3797289972899729,
      "step": 7006,
      "training_loss": 6.732477188110352
    },
    {
      "epoch": 0.37978319783197834,
      "step": 7007,
      "training_loss": 6.273375511169434
    },
    {
      "epoch": 0.3798373983739837,
      "grad_norm": 23.756715774536133,
      "learning_rate": 1e-05,
      "loss": 7.0858,
      "step": 7008
    },
    {
      "epoch": 0.3798373983739837,
      "step": 7008,
      "training_loss": 7.904748916625977
    },
    {
      "epoch": 0.37989159891598917,
      "step": 7009,
      "training_loss": 6.0537543296813965
    },
    {
      "epoch": 0.37994579945799456,
      "step": 7010,
      "training_loss": 7.560460090637207
    },
    {
      "epoch": 0.38,
      "step": 7011,
      "training_loss": 5.120344638824463
    },
    {
      "epoch": 0.3800542005420054,
      "grad_norm": 19.902671813964844,
      "learning_rate": 1e-05,
      "loss": 6.6598,
      "step": 7012
    },
    {
      "epoch": 0.3800542005420054,
      "step": 7012,
      "training_loss": 6.735270977020264
    },
    {
      "epoch": 0.38010840108401084,
      "step": 7013,
      "training_loss": 7.266846656799316
    },
    {
      "epoch": 0.3801626016260163,
      "step": 7014,
      "training_loss": 6.5916361808776855
    },
    {
      "epoch": 0.3802168021680217,
      "step": 7015,
      "training_loss": 7.46246337890625
    },
    {
      "epoch": 0.3802710027100271,
      "grad_norm": 20.970083236694336,
      "learning_rate": 1e-05,
      "loss": 7.0141,
      "step": 7016
    },
    {
      "epoch": 0.3802710027100271,
      "step": 7016,
      "training_loss": 8.255684852600098
    },
    {
      "epoch": 0.3803252032520325,
      "step": 7017,
      "training_loss": 6.838104724884033
    },
    {
      "epoch": 0.38037940379403795,
      "step": 7018,
      "training_loss": 7.682542324066162
    },
    {
      "epoch": 0.38043360433604334,
      "step": 7019,
      "training_loss": 6.734335899353027
    },
    {
      "epoch": 0.3804878048780488,
      "grad_norm": 26.6297607421875,
      "learning_rate": 1e-05,
      "loss": 7.3777,
      "step": 7020
    },
    {
      "epoch": 0.3804878048780488,
      "step": 7020,
      "training_loss": 7.326518535614014
    },
    {
      "epoch": 0.3805420054200542,
      "step": 7021,
      "training_loss": 7.080309867858887
    },
    {
      "epoch": 0.3805962059620596,
      "step": 7022,
      "training_loss": 6.590254306793213
    },
    {
      "epoch": 0.38065040650406506,
      "step": 7023,
      "training_loss": 6.783657550811768
    },
    {
      "epoch": 0.38070460704607045,
      "grad_norm": 19.218843460083008,
      "learning_rate": 1e-05,
      "loss": 6.9452,
      "step": 7024
    },
    {
      "epoch": 0.38070460704607045,
      "step": 7024,
      "training_loss": 7.167970180511475
    },
    {
      "epoch": 0.3807588075880759,
      "step": 7025,
      "training_loss": 6.984647274017334
    },
    {
      "epoch": 0.3808130081300813,
      "step": 7026,
      "training_loss": 7.39645528793335
    },
    {
      "epoch": 0.38086720867208673,
      "step": 7027,
      "training_loss": 7.023360252380371
    },
    {
      "epoch": 0.3809214092140921,
      "grad_norm": 20.75128936767578,
      "learning_rate": 1e-05,
      "loss": 7.1431,
      "step": 7028
    },
    {
      "epoch": 0.3809214092140921,
      "step": 7028,
      "training_loss": 8.100582122802734
    },
    {
      "epoch": 0.38097560975609757,
      "step": 7029,
      "training_loss": 3.9163806438446045
    },
    {
      "epoch": 0.38102981029810296,
      "step": 7030,
      "training_loss": 6.983893871307373
    },
    {
      "epoch": 0.3810840108401084,
      "step": 7031,
      "training_loss": 6.429473876953125
    },
    {
      "epoch": 0.38113821138211385,
      "grad_norm": 36.644187927246094,
      "learning_rate": 1e-05,
      "loss": 6.3576,
      "step": 7032
    },
    {
      "epoch": 0.38113821138211385,
      "step": 7032,
      "training_loss": 7.713173866271973
    },
    {
      "epoch": 0.38119241192411923,
      "step": 7033,
      "training_loss": 6.461673736572266
    },
    {
      "epoch": 0.3812466124661247,
      "step": 7034,
      "training_loss": 6.545942306518555
    },
    {
      "epoch": 0.38130081300813007,
      "step": 7035,
      "training_loss": 5.925700664520264
    },
    {
      "epoch": 0.3813550135501355,
      "grad_norm": 62.06793975830078,
      "learning_rate": 1e-05,
      "loss": 6.6616,
      "step": 7036
    },
    {
      "epoch": 0.3813550135501355,
      "step": 7036,
      "training_loss": 6.793792247772217
    },
    {
      "epoch": 0.3814092140921409,
      "step": 7037,
      "training_loss": 7.787769794464111
    },
    {
      "epoch": 0.38146341463414635,
      "step": 7038,
      "training_loss": 7.887321949005127
    },
    {
      "epoch": 0.38151761517615174,
      "step": 7039,
      "training_loss": 6.972893714904785
    },
    {
      "epoch": 0.3815718157181572,
      "grad_norm": 23.081924438476562,
      "learning_rate": 1e-05,
      "loss": 7.3604,
      "step": 7040
    },
    {
      "epoch": 0.3815718157181572,
      "step": 7040,
      "training_loss": 9.51031494140625
    },
    {
      "epoch": 0.3816260162601626,
      "step": 7041,
      "training_loss": 7.893771171569824
    },
    {
      "epoch": 0.381680216802168,
      "step": 7042,
      "training_loss": 6.915109634399414
    },
    {
      "epoch": 0.38173441734417346,
      "step": 7043,
      "training_loss": 6.83690881729126
    },
    {
      "epoch": 0.38178861788617885,
      "grad_norm": 18.65372085571289,
      "learning_rate": 1e-05,
      "loss": 7.789,
      "step": 7044
    },
    {
      "epoch": 0.38178861788617885,
      "step": 7044,
      "training_loss": 5.353602409362793
    },
    {
      "epoch": 0.3818428184281843,
      "step": 7045,
      "training_loss": 6.347422122955322
    },
    {
      "epoch": 0.3818970189701897,
      "step": 7046,
      "training_loss": 7.765645980834961
    },
    {
      "epoch": 0.38195121951219513,
      "step": 7047,
      "training_loss": 7.1378021240234375
    },
    {
      "epoch": 0.3820054200542005,
      "grad_norm": 28.868698120117188,
      "learning_rate": 1e-05,
      "loss": 6.6511,
      "step": 7048
    },
    {
      "epoch": 0.3820054200542005,
      "step": 7048,
      "training_loss": 6.761176109313965
    },
    {
      "epoch": 0.38205962059620596,
      "step": 7049,
      "training_loss": 7.318777084350586
    },
    {
      "epoch": 0.3821138211382114,
      "step": 7050,
      "training_loss": 6.04600715637207
    },
    {
      "epoch": 0.3821680216802168,
      "step": 7051,
      "training_loss": 8.267666816711426
    },
    {
      "epoch": 0.38222222222222224,
      "grad_norm": 37.791378021240234,
      "learning_rate": 1e-05,
      "loss": 7.0984,
      "step": 7052
    },
    {
      "epoch": 0.38222222222222224,
      "step": 7052,
      "training_loss": 7.575191497802734
    },
    {
      "epoch": 0.38227642276422763,
      "step": 7053,
      "training_loss": 8.199450492858887
    },
    {
      "epoch": 0.3823306233062331,
      "step": 7054,
      "training_loss": 8.266864776611328
    },
    {
      "epoch": 0.38238482384823846,
      "step": 7055,
      "training_loss": 6.2708258628845215
    },
    {
      "epoch": 0.3824390243902439,
      "grad_norm": 27.27001953125,
      "learning_rate": 1e-05,
      "loss": 7.5781,
      "step": 7056
    },
    {
      "epoch": 0.3824390243902439,
      "step": 7056,
      "training_loss": 6.888470649719238
    },
    {
      "epoch": 0.3824932249322493,
      "step": 7057,
      "training_loss": 6.8533616065979
    },
    {
      "epoch": 0.38254742547425474,
      "step": 7058,
      "training_loss": 7.567269802093506
    },
    {
      "epoch": 0.3826016260162602,
      "step": 7059,
      "training_loss": 4.1193013191223145
    },
    {
      "epoch": 0.3826558265582656,
      "grad_norm": 29.250009536743164,
      "learning_rate": 1e-05,
      "loss": 6.3571,
      "step": 7060
    },
    {
      "epoch": 0.3826558265582656,
      "step": 7060,
      "training_loss": 8.55776309967041
    },
    {
      "epoch": 0.382710027100271,
      "step": 7061,
      "training_loss": 8.104905128479004
    },
    {
      "epoch": 0.3827642276422764,
      "step": 7062,
      "training_loss": 3.6460318565368652
    },
    {
      "epoch": 0.38281842818428186,
      "step": 7063,
      "training_loss": 5.640207767486572
    },
    {
      "epoch": 0.38287262872628725,
      "grad_norm": 29.779216766357422,
      "learning_rate": 1e-05,
      "loss": 6.4872,
      "step": 7064
    },
    {
      "epoch": 0.38287262872628725,
      "step": 7064,
      "training_loss": 6.90346622467041
    },
    {
      "epoch": 0.3829268292682927,
      "step": 7065,
      "training_loss": 7.562420845031738
    },
    {
      "epoch": 0.3829810298102981,
      "step": 7066,
      "training_loss": 6.635329723358154
    },
    {
      "epoch": 0.3830352303523035,
      "step": 7067,
      "training_loss": 6.6666717529296875
    },
    {
      "epoch": 0.38308943089430897,
      "grad_norm": 20.099647521972656,
      "learning_rate": 1e-05,
      "loss": 6.942,
      "step": 7068
    },
    {
      "epoch": 0.38308943089430897,
      "step": 7068,
      "training_loss": 6.324173450469971
    },
    {
      "epoch": 0.38314363143631436,
      "step": 7069,
      "training_loss": 7.1776227951049805
    },
    {
      "epoch": 0.3831978319783198,
      "step": 7070,
      "training_loss": 6.88174295425415
    },
    {
      "epoch": 0.3832520325203252,
      "step": 7071,
      "training_loss": 6.654806613922119
    },
    {
      "epoch": 0.38330623306233064,
      "grad_norm": 25.1617488861084,
      "learning_rate": 1e-05,
      "loss": 6.7596,
      "step": 7072
    },
    {
      "epoch": 0.38330623306233064,
      "step": 7072,
      "training_loss": 7.823658466339111
    },
    {
      "epoch": 0.383360433604336,
      "step": 7073,
      "training_loss": 6.415987968444824
    },
    {
      "epoch": 0.38341463414634147,
      "step": 7074,
      "training_loss": 6.592568874359131
    },
    {
      "epoch": 0.38346883468834686,
      "step": 7075,
      "training_loss": 6.367554664611816
    },
    {
      "epoch": 0.3835230352303523,
      "grad_norm": 21.560293197631836,
      "learning_rate": 1e-05,
      "loss": 6.7999,
      "step": 7076
    },
    {
      "epoch": 0.3835230352303523,
      "step": 7076,
      "training_loss": 5.465493202209473
    },
    {
      "epoch": 0.38357723577235775,
      "step": 7077,
      "training_loss": 8.197659492492676
    },
    {
      "epoch": 0.38363143631436314,
      "step": 7078,
      "training_loss": 7.104294776916504
    },
    {
      "epoch": 0.3836856368563686,
      "step": 7079,
      "training_loss": 8.175268173217773
    },
    {
      "epoch": 0.383739837398374,
      "grad_norm": 41.79127502441406,
      "learning_rate": 1e-05,
      "loss": 7.2357,
      "step": 7080
    },
    {
      "epoch": 0.383739837398374,
      "step": 7080,
      "training_loss": 7.196562767028809
    },
    {
      "epoch": 0.3837940379403794,
      "step": 7081,
      "training_loss": 7.050652027130127
    },
    {
      "epoch": 0.3838482384823848,
      "step": 7082,
      "training_loss": 5.047696113586426
    },
    {
      "epoch": 0.38390243902439025,
      "step": 7083,
      "training_loss": 5.769705772399902
    },
    {
      "epoch": 0.38395663956639564,
      "grad_norm": 29.4522762298584,
      "learning_rate": 1e-05,
      "loss": 6.2662,
      "step": 7084
    },
    {
      "epoch": 0.38395663956639564,
      "step": 7084,
      "training_loss": 4.7792863845825195
    },
    {
      "epoch": 0.3840108401084011,
      "step": 7085,
      "training_loss": 6.5062994956970215
    },
    {
      "epoch": 0.38406504065040653,
      "step": 7086,
      "training_loss": 7.165081024169922
    },
    {
      "epoch": 0.3841192411924119,
      "step": 7087,
      "training_loss": 7.759383201599121
    },
    {
      "epoch": 0.38417344173441736,
      "grad_norm": 33.67479705810547,
      "learning_rate": 1e-05,
      "loss": 6.5525,
      "step": 7088
    },
    {
      "epoch": 0.38417344173441736,
      "step": 7088,
      "training_loss": 8.302288055419922
    },
    {
      "epoch": 0.38422764227642275,
      "step": 7089,
      "training_loss": 6.962663173675537
    },
    {
      "epoch": 0.3842818428184282,
      "step": 7090,
      "training_loss": 6.256582736968994
    },
    {
      "epoch": 0.3843360433604336,
      "step": 7091,
      "training_loss": 4.256584167480469
    },
    {
      "epoch": 0.38439024390243903,
      "grad_norm": 23.458147048950195,
      "learning_rate": 1e-05,
      "loss": 6.4445,
      "step": 7092
    },
    {
      "epoch": 0.38439024390243903,
      "step": 7092,
      "training_loss": 6.977562427520752
    },
    {
      "epoch": 0.3844444444444444,
      "step": 7093,
      "training_loss": 5.416940212249756
    },
    {
      "epoch": 0.38449864498644987,
      "step": 7094,
      "training_loss": 7.115813255310059
    },
    {
      "epoch": 0.3845528455284553,
      "step": 7095,
      "training_loss": 5.8627610206604
    },
    {
      "epoch": 0.3846070460704607,
      "grad_norm": 30.01268196105957,
      "learning_rate": 1e-05,
      "loss": 6.3433,
      "step": 7096
    },
    {
      "epoch": 0.3846070460704607,
      "step": 7096,
      "training_loss": 6.83416223526001
    },
    {
      "epoch": 0.38466124661246615,
      "step": 7097,
      "training_loss": 5.741281986236572
    },
    {
      "epoch": 0.38471544715447153,
      "step": 7098,
      "training_loss": 7.141754150390625
    },
    {
      "epoch": 0.384769647696477,
      "step": 7099,
      "training_loss": 5.847227573394775
    },
    {
      "epoch": 0.38482384823848237,
      "grad_norm": 25.33218002319336,
      "learning_rate": 1e-05,
      "loss": 6.3911,
      "step": 7100
    },
    {
      "epoch": 0.38482384823848237,
      "step": 7100,
      "training_loss": 7.708542346954346
    },
    {
      "epoch": 0.3848780487804878,
      "step": 7101,
      "training_loss": 7.526235580444336
    },
    {
      "epoch": 0.3849322493224932,
      "step": 7102,
      "training_loss": 6.5382490158081055
    },
    {
      "epoch": 0.38498644986449865,
      "step": 7103,
      "training_loss": 6.273085594177246
    },
    {
      "epoch": 0.3850406504065041,
      "grad_norm": 23.974437713623047,
      "learning_rate": 1e-05,
      "loss": 7.0115,
      "step": 7104
    },
    {
      "epoch": 0.3850406504065041,
      "step": 7104,
      "training_loss": 5.990479946136475
    },
    {
      "epoch": 0.3850948509485095,
      "step": 7105,
      "training_loss": 7.216933250427246
    },
    {
      "epoch": 0.3851490514905149,
      "step": 7106,
      "training_loss": 7.141512870788574
    },
    {
      "epoch": 0.3852032520325203,
      "step": 7107,
      "training_loss": 7.3510565757751465
    },
    {
      "epoch": 0.38525745257452576,
      "grad_norm": 35.274810791015625,
      "learning_rate": 1e-05,
      "loss": 6.925,
      "step": 7108
    },
    {
      "epoch": 0.38525745257452576,
      "step": 7108,
      "training_loss": 6.773981094360352
    },
    {
      "epoch": 0.38531165311653115,
      "step": 7109,
      "training_loss": 6.099858283996582
    },
    {
      "epoch": 0.3853658536585366,
      "step": 7110,
      "training_loss": 5.826756000518799
    },
    {
      "epoch": 0.385420054200542,
      "step": 7111,
      "training_loss": 7.819217205047607
    },
    {
      "epoch": 0.38547425474254743,
      "grad_norm": 50.94823455810547,
      "learning_rate": 1e-05,
      "loss": 6.63,
      "step": 7112
    },
    {
      "epoch": 0.38547425474254743,
      "step": 7112,
      "training_loss": 6.3717193603515625
    },
    {
      "epoch": 0.3855284552845528,
      "step": 7113,
      "training_loss": 6.3132524490356445
    },
    {
      "epoch": 0.38558265582655826,
      "step": 7114,
      "training_loss": 6.13682746887207
    },
    {
      "epoch": 0.3856368563685637,
      "step": 7115,
      "training_loss": 4.867013931274414
    },
    {
      "epoch": 0.3856910569105691,
      "grad_norm": 27.019866943359375,
      "learning_rate": 1e-05,
      "loss": 5.9222,
      "step": 7116
    },
    {
      "epoch": 0.3856910569105691,
      "step": 7116,
      "training_loss": 7.737082004547119
    },
    {
      "epoch": 0.38574525745257454,
      "step": 7117,
      "training_loss": 7.859206199645996
    },
    {
      "epoch": 0.38579945799457993,
      "step": 7118,
      "training_loss": 6.267508029937744
    },
    {
      "epoch": 0.3858536585365854,
      "step": 7119,
      "training_loss": 5.819750785827637
    },
    {
      "epoch": 0.38590785907859076,
      "grad_norm": 25.674495697021484,
      "learning_rate": 1e-05,
      "loss": 6.9209,
      "step": 7120
    },
    {
      "epoch": 0.38590785907859076,
      "step": 7120,
      "training_loss": 6.302713871002197
    },
    {
      "epoch": 0.3859620596205962,
      "step": 7121,
      "training_loss": 6.5911407470703125
    },
    {
      "epoch": 0.3860162601626016,
      "step": 7122,
      "training_loss": 6.508857250213623
    },
    {
      "epoch": 0.38607046070460704,
      "step": 7123,
      "training_loss": 7.679287433624268
    },
    {
      "epoch": 0.3861246612466125,
      "grad_norm": 37.010318756103516,
      "learning_rate": 1e-05,
      "loss": 6.7705,
      "step": 7124
    },
    {
      "epoch": 0.3861246612466125,
      "step": 7124,
      "training_loss": 6.628364086151123
    },
    {
      "epoch": 0.3861788617886179,
      "step": 7125,
      "training_loss": 7.396568775177002
    },
    {
      "epoch": 0.3862330623306233,
      "step": 7126,
      "training_loss": 3.566180944442749
    },
    {
      "epoch": 0.3862872628726287,
      "step": 7127,
      "training_loss": 6.2705817222595215
    },
    {
      "epoch": 0.38634146341463416,
      "grad_norm": 21.087549209594727,
      "learning_rate": 1e-05,
      "loss": 5.9654,
      "step": 7128
    },
    {
      "epoch": 0.38634146341463416,
      "step": 7128,
      "training_loss": 5.352570056915283
    },
    {
      "epoch": 0.38639566395663955,
      "step": 7129,
      "training_loss": 7.066267013549805
    },
    {
      "epoch": 0.386449864498645,
      "step": 7130,
      "training_loss": 7.076781272888184
    },
    {
      "epoch": 0.3865040650406504,
      "step": 7131,
      "training_loss": 5.623025417327881
    },
    {
      "epoch": 0.3865582655826558,
      "grad_norm": 24.84597396850586,
      "learning_rate": 1e-05,
      "loss": 6.2797,
      "step": 7132
    },
    {
      "epoch": 0.3865582655826558,
      "step": 7132,
      "training_loss": 6.652166843414307
    },
    {
      "epoch": 0.38661246612466127,
      "step": 7133,
      "training_loss": 7.703428268432617
    },
    {
      "epoch": 0.38666666666666666,
      "step": 7134,
      "training_loss": 4.070091724395752
    },
    {
      "epoch": 0.3867208672086721,
      "step": 7135,
      "training_loss": 7.266104221343994
    },
    {
      "epoch": 0.3867750677506775,
      "grad_norm": 30.040740966796875,
      "learning_rate": 1e-05,
      "loss": 6.4229,
      "step": 7136
    },
    {
      "epoch": 0.3867750677506775,
      "step": 7136,
      "training_loss": 6.819209575653076
    },
    {
      "epoch": 0.38682926829268294,
      "step": 7137,
      "training_loss": 7.510324954986572
    },
    {
      "epoch": 0.3868834688346883,
      "step": 7138,
      "training_loss": 6.822201251983643
    },
    {
      "epoch": 0.38693766937669377,
      "step": 7139,
      "training_loss": 6.81137752532959
    },
    {
      "epoch": 0.38699186991869916,
      "grad_norm": 20.07114028930664,
      "learning_rate": 1e-05,
      "loss": 6.9908,
      "step": 7140
    },
    {
      "epoch": 0.38699186991869916,
      "step": 7140,
      "training_loss": 6.814160346984863
    },
    {
      "epoch": 0.3870460704607046,
      "step": 7141,
      "training_loss": 6.439782619476318
    },
    {
      "epoch": 0.38710027100271005,
      "step": 7142,
      "training_loss": 7.413494110107422
    },
    {
      "epoch": 0.38715447154471544,
      "step": 7143,
      "training_loss": 7.8104448318481445
    },
    {
      "epoch": 0.3872086720867209,
      "grad_norm": 22.67823028564453,
      "learning_rate": 1e-05,
      "loss": 7.1195,
      "step": 7144
    },
    {
      "epoch": 0.3872086720867209,
      "step": 7144,
      "training_loss": 7.116803169250488
    },
    {
      "epoch": 0.3872628726287263,
      "step": 7145,
      "training_loss": 7.5484819412231445
    },
    {
      "epoch": 0.3873170731707317,
      "step": 7146,
      "training_loss": 7.248539924621582
    },
    {
      "epoch": 0.3873712737127371,
      "step": 7147,
      "training_loss": 7.524375915527344
    },
    {
      "epoch": 0.38742547425474255,
      "grad_norm": 30.859834671020508,
      "learning_rate": 1e-05,
      "loss": 7.3596,
      "step": 7148
    },
    {
      "epoch": 0.38742547425474255,
      "step": 7148,
      "training_loss": 6.697455883026123
    },
    {
      "epoch": 0.38747967479674794,
      "step": 7149,
      "training_loss": 4.546640872955322
    },
    {
      "epoch": 0.3875338753387534,
      "step": 7150,
      "training_loss": 3.83514404296875
    },
    {
      "epoch": 0.38758807588075883,
      "step": 7151,
      "training_loss": 6.55875301361084
    },
    {
      "epoch": 0.3876422764227642,
      "grad_norm": 26.476627349853516,
      "learning_rate": 1e-05,
      "loss": 5.4095,
      "step": 7152
    },
    {
      "epoch": 0.3876422764227642,
      "step": 7152,
      "training_loss": 7.069206714630127
    },
    {
      "epoch": 0.38769647696476967,
      "step": 7153,
      "training_loss": 6.985457420349121
    },
    {
      "epoch": 0.38775067750677505,
      "step": 7154,
      "training_loss": 5.840420722961426
    },
    {
      "epoch": 0.3878048780487805,
      "step": 7155,
      "training_loss": 5.005209445953369
    },
    {
      "epoch": 0.3878590785907859,
      "grad_norm": 21.148929595947266,
      "learning_rate": 1e-05,
      "loss": 6.2251,
      "step": 7156
    },
    {
      "epoch": 0.3878590785907859,
      "step": 7156,
      "training_loss": 6.490980625152588
    },
    {
      "epoch": 0.38791327913279133,
      "step": 7157,
      "training_loss": 7.995773792266846
    },
    {
      "epoch": 0.3879674796747967,
      "step": 7158,
      "training_loss": 5.3172783851623535
    },
    {
      "epoch": 0.38802168021680217,
      "step": 7159,
      "training_loss": 7.878077507019043
    },
    {
      "epoch": 0.3880758807588076,
      "grad_norm": 72.77122497558594,
      "learning_rate": 1e-05,
      "loss": 6.9205,
      "step": 7160
    },
    {
      "epoch": 0.3880758807588076,
      "step": 7160,
      "training_loss": 6.004993438720703
    },
    {
      "epoch": 0.388130081300813,
      "step": 7161,
      "training_loss": 6.582783222198486
    },
    {
      "epoch": 0.38818428184281845,
      "step": 7162,
      "training_loss": 8.035286903381348
    },
    {
      "epoch": 0.38823848238482384,
      "step": 7163,
      "training_loss": 5.9724202156066895
    },
    {
      "epoch": 0.3882926829268293,
      "grad_norm": 25.0375919342041,
      "learning_rate": 1e-05,
      "loss": 6.6489,
      "step": 7164
    },
    {
      "epoch": 0.3882926829268293,
      "step": 7164,
      "training_loss": 6.46744966506958
    },
    {
      "epoch": 0.38834688346883467,
      "step": 7165,
      "training_loss": 6.625386714935303
    },
    {
      "epoch": 0.3884010840108401,
      "step": 7166,
      "training_loss": 7.119786262512207
    },
    {
      "epoch": 0.3884552845528455,
      "step": 7167,
      "training_loss": 6.090149879455566
    },
    {
      "epoch": 0.38850948509485095,
      "grad_norm": 41.773136138916016,
      "learning_rate": 1e-05,
      "loss": 6.5757,
      "step": 7168
    },
    {
      "epoch": 0.38850948509485095,
      "step": 7168,
      "training_loss": 6.895901679992676
    },
    {
      "epoch": 0.3885636856368564,
      "step": 7169,
      "training_loss": 7.377534866333008
    },
    {
      "epoch": 0.3886178861788618,
      "step": 7170,
      "training_loss": 5.847145080566406
    },
    {
      "epoch": 0.3886720867208672,
      "step": 7171,
      "training_loss": 6.945122718811035
    },
    {
      "epoch": 0.3887262872628726,
      "grad_norm": 43.454071044921875,
      "learning_rate": 1e-05,
      "loss": 6.7664,
      "step": 7172
    },
    {
      "epoch": 0.3887262872628726,
      "step": 7172,
      "training_loss": 6.568477153778076
    },
    {
      "epoch": 0.38878048780487806,
      "step": 7173,
      "training_loss": 6.198281288146973
    },
    {
      "epoch": 0.38883468834688345,
      "step": 7174,
      "training_loss": 5.458138942718506
    },
    {
      "epoch": 0.3888888888888889,
      "step": 7175,
      "training_loss": 6.990591526031494
    },
    {
      "epoch": 0.3889430894308943,
      "grad_norm": 37.78620147705078,
      "learning_rate": 1e-05,
      "loss": 6.3039,
      "step": 7176
    },
    {
      "epoch": 0.3889430894308943,
      "step": 7176,
      "training_loss": 7.640005111694336
    },
    {
      "epoch": 0.38899728997289973,
      "step": 7177,
      "training_loss": 8.182488441467285
    },
    {
      "epoch": 0.3890514905149052,
      "step": 7178,
      "training_loss": 6.071622371673584
    },
    {
      "epoch": 0.38910569105691056,
      "step": 7179,
      "training_loss": 4.285754680633545
    },
    {
      "epoch": 0.389159891598916,
      "grad_norm": 22.143321990966797,
      "learning_rate": 1e-05,
      "loss": 6.545,
      "step": 7180
    },
    {
      "epoch": 0.389159891598916,
      "step": 7180,
      "training_loss": 6.829681873321533
    },
    {
      "epoch": 0.3892140921409214,
      "step": 7181,
      "training_loss": 3.704315423965454
    },
    {
      "epoch": 0.38926829268292684,
      "step": 7182,
      "training_loss": 7.3520426750183105
    },
    {
      "epoch": 0.38932249322493223,
      "step": 7183,
      "training_loss": 5.984243392944336
    },
    {
      "epoch": 0.3893766937669377,
      "grad_norm": 32.94315719604492,
      "learning_rate": 1e-05,
      "loss": 5.9676,
      "step": 7184
    },
    {
      "epoch": 0.3893766937669377,
      "step": 7184,
      "training_loss": 4.721563816070557
    },
    {
      "epoch": 0.38943089430894307,
      "step": 7185,
      "training_loss": 6.792606830596924
    },
    {
      "epoch": 0.3894850948509485,
      "step": 7186,
      "training_loss": 6.720722198486328
    },
    {
      "epoch": 0.38953929539295395,
      "step": 7187,
      "training_loss": 7.143580913543701
    },
    {
      "epoch": 0.38959349593495934,
      "grad_norm": 17.131990432739258,
      "learning_rate": 1e-05,
      "loss": 6.3446,
      "step": 7188
    },
    {
      "epoch": 0.38959349593495934,
      "step": 7188,
      "training_loss": 5.108522415161133
    },
    {
      "epoch": 0.3896476964769648,
      "step": 7189,
      "training_loss": 7.1016058921813965
    },
    {
      "epoch": 0.3897018970189702,
      "step": 7190,
      "training_loss": 6.99229621887207
    },
    {
      "epoch": 0.3897560975609756,
      "step": 7191,
      "training_loss": 7.112305164337158
    },
    {
      "epoch": 0.389810298102981,
      "grad_norm": 16.230833053588867,
      "learning_rate": 1e-05,
      "loss": 6.5787,
      "step": 7192
    },
    {
      "epoch": 0.389810298102981,
      "step": 7192,
      "training_loss": 6.8870930671691895
    },
    {
      "epoch": 0.38986449864498646,
      "step": 7193,
      "training_loss": 7.065417289733887
    },
    {
      "epoch": 0.38991869918699185,
      "step": 7194,
      "training_loss": 6.198197364807129
    },
    {
      "epoch": 0.3899728997289973,
      "step": 7195,
      "training_loss": 6.249063968658447
    },
    {
      "epoch": 0.39002710027100274,
      "grad_norm": 77.17227935791016,
      "learning_rate": 1e-05,
      "loss": 6.5999,
      "step": 7196
    },
    {
      "epoch": 0.39002710027100274,
      "step": 7196,
      "training_loss": 5.3426666259765625
    },
    {
      "epoch": 0.3900813008130081,
      "step": 7197,
      "training_loss": 6.1127543449401855
    },
    {
      "epoch": 0.39013550135501357,
      "step": 7198,
      "training_loss": 7.192173004150391
    },
    {
      "epoch": 0.39018970189701896,
      "step": 7199,
      "training_loss": 5.544871807098389
    },
    {
      "epoch": 0.3902439024390244,
      "grad_norm": 24.7218017578125,
      "learning_rate": 1e-05,
      "loss": 6.0481,
      "step": 7200
    },
    {
      "epoch": 0.3902439024390244,
      "step": 7200,
      "training_loss": 9.453819274902344
    },
    {
      "epoch": 0.3902981029810298,
      "step": 7201,
      "training_loss": 6.18978214263916
    },
    {
      "epoch": 0.39035230352303524,
      "step": 7202,
      "training_loss": 7.173003196716309
    },
    {
      "epoch": 0.3904065040650406,
      "step": 7203,
      "training_loss": 5.796618461608887
    },
    {
      "epoch": 0.39046070460704607,
      "grad_norm": 30.9619197845459,
      "learning_rate": 1e-05,
      "loss": 7.1533,
      "step": 7204
    },
    {
      "epoch": 0.39046070460704607,
      "step": 7204,
      "training_loss": 5.957486629486084
    },
    {
      "epoch": 0.3905149051490515,
      "step": 7205,
      "training_loss": 7.013499736785889
    },
    {
      "epoch": 0.3905691056910569,
      "step": 7206,
      "training_loss": 6.1304097175598145
    },
    {
      "epoch": 0.39062330623306235,
      "step": 7207,
      "training_loss": 6.570291042327881
    },
    {
      "epoch": 0.39067750677506774,
      "grad_norm": 26.175399780273438,
      "learning_rate": 1e-05,
      "loss": 6.4179,
      "step": 7208
    },
    {
      "epoch": 0.39067750677506774,
      "step": 7208,
      "training_loss": 7.06631326675415
    },
    {
      "epoch": 0.3907317073170732,
      "step": 7209,
      "training_loss": 6.449318885803223
    },
    {
      "epoch": 0.3907859078590786,
      "step": 7210,
      "training_loss": 3.528834104537964
    },
    {
      "epoch": 0.390840108401084,
      "step": 7211,
      "training_loss": 3.9679970741271973
    },
    {
      "epoch": 0.3908943089430894,
      "grad_norm": 27.135347366333008,
      "learning_rate": 1e-05,
      "loss": 5.2531,
      "step": 7212
    },
    {
      "epoch": 0.3908943089430894,
      "step": 7212,
      "training_loss": 6.844478607177734
    },
    {
      "epoch": 0.39094850948509485,
      "step": 7213,
      "training_loss": 4.188588619232178
    },
    {
      "epoch": 0.3910027100271003,
      "step": 7214,
      "training_loss": 7.214710235595703
    },
    {
      "epoch": 0.3910569105691057,
      "step": 7215,
      "training_loss": 6.626529216766357
    },
    {
      "epoch": 0.39111111111111113,
      "grad_norm": 25.24302864074707,
      "learning_rate": 1e-05,
      "loss": 6.2186,
      "step": 7216
    },
    {
      "epoch": 0.39111111111111113,
      "step": 7216,
      "training_loss": 5.598705291748047
    },
    {
      "epoch": 0.3911653116531165,
      "step": 7217,
      "training_loss": 6.993618488311768
    },
    {
      "epoch": 0.39121951219512197,
      "step": 7218,
      "training_loss": 6.189815998077393
    },
    {
      "epoch": 0.39127371273712735,
      "step": 7219,
      "training_loss": 5.92515754699707
    },
    {
      "epoch": 0.3913279132791328,
      "grad_norm": 27.367774963378906,
      "learning_rate": 1e-05,
      "loss": 6.1768,
      "step": 7220
    },
    {
      "epoch": 0.3913279132791328,
      "step": 7220,
      "training_loss": 7.392482280731201
    },
    {
      "epoch": 0.3913821138211382,
      "step": 7221,
      "training_loss": 8.238670349121094
    },
    {
      "epoch": 0.39143631436314363,
      "step": 7222,
      "training_loss": 7.06462287902832
    },
    {
      "epoch": 0.3914905149051491,
      "step": 7223,
      "training_loss": 7.397127628326416
    },
    {
      "epoch": 0.39154471544715447,
      "grad_norm": 17.120445251464844,
      "learning_rate": 1e-05,
      "loss": 7.5232,
      "step": 7224
    },
    {
      "epoch": 0.39154471544715447,
      "step": 7224,
      "training_loss": 6.983788967132568
    },
    {
      "epoch": 0.3915989159891599,
      "step": 7225,
      "training_loss": 6.674283981323242
    },
    {
      "epoch": 0.3916531165311653,
      "step": 7226,
      "training_loss": 7.19673490524292
    },
    {
      "epoch": 0.39170731707317075,
      "step": 7227,
      "training_loss": 6.918562889099121
    },
    {
      "epoch": 0.39176151761517614,
      "grad_norm": 36.7235107421875,
      "learning_rate": 1e-05,
      "loss": 6.9433,
      "step": 7228
    },
    {
      "epoch": 0.39176151761517614,
      "step": 7228,
      "training_loss": 4.583689212799072
    },
    {
      "epoch": 0.3918157181571816,
      "step": 7229,
      "training_loss": 5.335278034210205
    },
    {
      "epoch": 0.39186991869918697,
      "step": 7230,
      "training_loss": 6.767180442810059
    },
    {
      "epoch": 0.3919241192411924,
      "step": 7231,
      "training_loss": 4.521335601806641
    },
    {
      "epoch": 0.39197831978319786,
      "grad_norm": 33.95069122314453,
      "learning_rate": 1e-05,
      "loss": 5.3019,
      "step": 7232
    },
    {
      "epoch": 0.39197831978319786,
      "step": 7232,
      "training_loss": 5.326591968536377
    },
    {
      "epoch": 0.39203252032520325,
      "step": 7233,
      "training_loss": 8.124847412109375
    },
    {
      "epoch": 0.3920867208672087,
      "step": 7234,
      "training_loss": 7.016376495361328
    },
    {
      "epoch": 0.3921409214092141,
      "step": 7235,
      "training_loss": 5.946435928344727
    },
    {
      "epoch": 0.3921951219512195,
      "grad_norm": 23.922719955444336,
      "learning_rate": 1e-05,
      "loss": 6.6036,
      "step": 7236
    },
    {
      "epoch": 0.3921951219512195,
      "step": 7236,
      "training_loss": 4.291472911834717
    },
    {
      "epoch": 0.3922493224932249,
      "step": 7237,
      "training_loss": 6.269893646240234
    },
    {
      "epoch": 0.39230352303523036,
      "step": 7238,
      "training_loss": 6.298527717590332
    },
    {
      "epoch": 0.39235772357723575,
      "step": 7239,
      "training_loss": 5.088912487030029
    },
    {
      "epoch": 0.3924119241192412,
      "grad_norm": 26.979183197021484,
      "learning_rate": 1e-05,
      "loss": 5.4872,
      "step": 7240
    },
    {
      "epoch": 0.3924119241192412,
      "step": 7240,
      "training_loss": 7.033139228820801
    },
    {
      "epoch": 0.3924661246612466,
      "step": 7241,
      "training_loss": 7.724609375
    },
    {
      "epoch": 0.39252032520325203,
      "step": 7242,
      "training_loss": 7.436332702636719
    },
    {
      "epoch": 0.3925745257452575,
      "step": 7243,
      "training_loss": 7.669039726257324
    },
    {
      "epoch": 0.39262872628726286,
      "grad_norm": 33.29457473754883,
      "learning_rate": 1e-05,
      "loss": 7.4658,
      "step": 7244
    },
    {
      "epoch": 0.39262872628726286,
      "step": 7244,
      "training_loss": 6.7587432861328125
    },
    {
      "epoch": 0.3926829268292683,
      "step": 7245,
      "training_loss": 5.71501350402832
    },
    {
      "epoch": 0.3927371273712737,
      "step": 7246,
      "training_loss": 6.241184711456299
    },
    {
      "epoch": 0.39279132791327914,
      "step": 7247,
      "training_loss": 7.172502040863037
    },
    {
      "epoch": 0.39284552845528453,
      "grad_norm": 36.7473258972168,
      "learning_rate": 1e-05,
      "loss": 6.4719,
      "step": 7248
    },
    {
      "epoch": 0.39284552845528453,
      "step": 7248,
      "training_loss": 6.405222415924072
    },
    {
      "epoch": 0.39289972899729,
      "step": 7249,
      "training_loss": 6.823419570922852
    },
    {
      "epoch": 0.39295392953929537,
      "step": 7250,
      "training_loss": 5.7808756828308105
    },
    {
      "epoch": 0.3930081300813008,
      "step": 7251,
      "training_loss": 4.9163641929626465
    },
    {
      "epoch": 0.39306233062330626,
      "grad_norm": 26.109453201293945,
      "learning_rate": 1e-05,
      "loss": 5.9815,
      "step": 7252
    },
    {
      "epoch": 0.39306233062330626,
      "step": 7252,
      "training_loss": 3.8838093280792236
    },
    {
      "epoch": 0.39311653116531164,
      "step": 7253,
      "training_loss": 7.522944927215576
    },
    {
      "epoch": 0.3931707317073171,
      "step": 7254,
      "training_loss": 6.136791706085205
    },
    {
      "epoch": 0.3932249322493225,
      "step": 7255,
      "training_loss": 6.974949359893799
    },
    {
      "epoch": 0.3932791327913279,
      "grad_norm": 20.697620391845703,
      "learning_rate": 1e-05,
      "loss": 6.1296,
      "step": 7256
    },
    {
      "epoch": 0.3932791327913279,
      "step": 7256,
      "training_loss": 6.97199821472168
    },
    {
      "epoch": 0.3933333333333333,
      "step": 7257,
      "training_loss": 7.135804653167725
    },
    {
      "epoch": 0.39338753387533876,
      "step": 7258,
      "training_loss": 7.255858421325684
    },
    {
      "epoch": 0.39344173441734415,
      "step": 7259,
      "training_loss": 7.082683086395264
    },
    {
      "epoch": 0.3934959349593496,
      "grad_norm": 25.632648468017578,
      "learning_rate": 1e-05,
      "loss": 7.1116,
      "step": 7260
    },
    {
      "epoch": 0.3934959349593496,
      "step": 7260,
      "training_loss": 6.5511274337768555
    },
    {
      "epoch": 0.39355013550135504,
      "step": 7261,
      "training_loss": 7.262543678283691
    },
    {
      "epoch": 0.3936043360433604,
      "step": 7262,
      "training_loss": 6.653410911560059
    },
    {
      "epoch": 0.39365853658536587,
      "step": 7263,
      "training_loss": 6.457334041595459
    },
    {
      "epoch": 0.39371273712737126,
      "grad_norm": 25.54412269592285,
      "learning_rate": 1e-05,
      "loss": 6.7311,
      "step": 7264
    },
    {
      "epoch": 0.39371273712737126,
      "step": 7264,
      "training_loss": 6.483891010284424
    },
    {
      "epoch": 0.3937669376693767,
      "step": 7265,
      "training_loss": 6.885862350463867
    },
    {
      "epoch": 0.3938211382113821,
      "step": 7266,
      "training_loss": 6.126129627227783
    },
    {
      "epoch": 0.39387533875338754,
      "step": 7267,
      "training_loss": 7.241342544555664
    },
    {
      "epoch": 0.3939295392953929,
      "grad_norm": 30.65334129333496,
      "learning_rate": 1e-05,
      "loss": 6.6843,
      "step": 7268
    },
    {
      "epoch": 0.3939295392953929,
      "step": 7268,
      "training_loss": 5.025345325469971
    },
    {
      "epoch": 0.3939837398373984,
      "step": 7269,
      "training_loss": 6.698938846588135
    },
    {
      "epoch": 0.3940379403794038,
      "step": 7270,
      "training_loss": 6.963595867156982
    },
    {
      "epoch": 0.3940921409214092,
      "step": 7271,
      "training_loss": 5.72355318069458
    },
    {
      "epoch": 0.39414634146341465,
      "grad_norm": 39.23273468017578,
      "learning_rate": 1e-05,
      "loss": 6.1029,
      "step": 7272
    },
    {
      "epoch": 0.39414634146341465,
      "step": 7272,
      "training_loss": 6.7022504806518555
    },
    {
      "epoch": 0.39420054200542004,
      "step": 7273,
      "training_loss": 6.737138271331787
    },
    {
      "epoch": 0.3942547425474255,
      "step": 7274,
      "training_loss": 5.322238445281982
    },
    {
      "epoch": 0.3943089430894309,
      "step": 7275,
      "training_loss": 6.842942237854004
    },
    {
      "epoch": 0.3943631436314363,
      "grad_norm": 21.921091079711914,
      "learning_rate": 1e-05,
      "loss": 6.4011,
      "step": 7276
    },
    {
      "epoch": 0.3943631436314363,
      "step": 7276,
      "training_loss": 7.191795825958252
    },
    {
      "epoch": 0.3944173441734417,
      "step": 7277,
      "training_loss": 7.238072395324707
    },
    {
      "epoch": 0.39447154471544715,
      "step": 7278,
      "training_loss": 6.669567108154297
    },
    {
      "epoch": 0.3945257452574526,
      "step": 7279,
      "training_loss": 6.9191789627075195
    },
    {
      "epoch": 0.394579945799458,
      "grad_norm": 31.638837814331055,
      "learning_rate": 1e-05,
      "loss": 7.0047,
      "step": 7280
    },
    {
      "epoch": 0.394579945799458,
      "step": 7280,
      "training_loss": 6.992020130157471
    },
    {
      "epoch": 0.39463414634146343,
      "step": 7281,
      "training_loss": 7.933511734008789
    },
    {
      "epoch": 0.3946883468834688,
      "step": 7282,
      "training_loss": 6.96674108505249
    },
    {
      "epoch": 0.39474254742547427,
      "step": 7283,
      "training_loss": 6.522523403167725
    },
    {
      "epoch": 0.39479674796747966,
      "grad_norm": 61.30598831176758,
      "learning_rate": 1e-05,
      "loss": 7.1037,
      "step": 7284
    },
    {
      "epoch": 0.39479674796747966,
      "step": 7284,
      "training_loss": 7.404857635498047
    },
    {
      "epoch": 0.3948509485094851,
      "step": 7285,
      "training_loss": 7.099971771240234
    },
    {
      "epoch": 0.3949051490514905,
      "step": 7286,
      "training_loss": 6.643045425415039
    },
    {
      "epoch": 0.39495934959349593,
      "step": 7287,
      "training_loss": 6.5434980392456055
    },
    {
      "epoch": 0.3950135501355014,
      "grad_norm": 24.670265197753906,
      "learning_rate": 1e-05,
      "loss": 6.9228,
      "step": 7288
    },
    {
      "epoch": 0.3950135501355014,
      "step": 7288,
      "training_loss": 6.121450901031494
    },
    {
      "epoch": 0.39506775067750677,
      "step": 7289,
      "training_loss": 6.402304649353027
    },
    {
      "epoch": 0.3951219512195122,
      "step": 7290,
      "training_loss": 5.614499568939209
    },
    {
      "epoch": 0.3951761517615176,
      "step": 7291,
      "training_loss": 7.06988525390625
    },
    {
      "epoch": 0.39523035230352305,
      "grad_norm": 26.73685646057129,
      "learning_rate": 1e-05,
      "loss": 6.302,
      "step": 7292
    },
    {
      "epoch": 0.39523035230352305,
      "step": 7292,
      "training_loss": 7.894477367401123
    },
    {
      "epoch": 0.39528455284552844,
      "step": 7293,
      "training_loss": 5.689913272857666
    },
    {
      "epoch": 0.3953387533875339,
      "step": 7294,
      "training_loss": 6.85118293762207
    },
    {
      "epoch": 0.39539295392953927,
      "step": 7295,
      "training_loss": 8.092354774475098
    },
    {
      "epoch": 0.3954471544715447,
      "grad_norm": 18.5408992767334,
      "learning_rate": 1e-05,
      "loss": 7.132,
      "step": 7296
    },
    {
      "epoch": 0.3954471544715447,
      "step": 7296,
      "training_loss": 6.977597236633301
    },
    {
      "epoch": 0.39550135501355016,
      "step": 7297,
      "training_loss": 6.828029632568359
    },
    {
      "epoch": 0.39555555555555555,
      "step": 7298,
      "training_loss": 6.391794681549072
    },
    {
      "epoch": 0.395609756097561,
      "step": 7299,
      "training_loss": 6.6960296630859375
    },
    {
      "epoch": 0.3956639566395664,
      "grad_norm": 27.985801696777344,
      "learning_rate": 1e-05,
      "loss": 6.7234,
      "step": 7300
    },
    {
      "epoch": 0.3956639566395664,
      "step": 7300,
      "training_loss": 7.109903335571289
    },
    {
      "epoch": 0.39571815718157183,
      "step": 7301,
      "training_loss": 8.00784969329834
    },
    {
      "epoch": 0.3957723577235772,
      "step": 7302,
      "training_loss": 6.81566047668457
    },
    {
      "epoch": 0.39582655826558266,
      "step": 7303,
      "training_loss": 7.019426345825195
    },
    {
      "epoch": 0.39588075880758805,
      "grad_norm": 17.655521392822266,
      "learning_rate": 1e-05,
      "loss": 7.2382,
      "step": 7304
    },
    {
      "epoch": 0.39588075880758805,
      "step": 7304,
      "training_loss": 4.829493045806885
    },
    {
      "epoch": 0.3959349593495935,
      "step": 7305,
      "training_loss": 7.201014041900635
    },
    {
      "epoch": 0.39598915989159894,
      "step": 7306,
      "training_loss": 6.693699836730957
    },
    {
      "epoch": 0.39604336043360433,
      "step": 7307,
      "training_loss": 6.979384899139404
    },
    {
      "epoch": 0.3960975609756098,
      "grad_norm": 25.571557998657227,
      "learning_rate": 1e-05,
      "loss": 6.4259,
      "step": 7308
    },
    {
      "epoch": 0.3960975609756098,
      "step": 7308,
      "training_loss": 4.292574882507324
    },
    {
      "epoch": 0.39615176151761516,
      "step": 7309,
      "training_loss": 7.081496715545654
    },
    {
      "epoch": 0.3962059620596206,
      "step": 7310,
      "training_loss": 5.462164878845215
    },
    {
      "epoch": 0.396260162601626,
      "step": 7311,
      "training_loss": 7.985743522644043
    },
    {
      "epoch": 0.39631436314363144,
      "grad_norm": 24.79670524597168,
      "learning_rate": 1e-05,
      "loss": 6.2055,
      "step": 7312
    },
    {
      "epoch": 0.39631436314363144,
      "step": 7312,
      "training_loss": 7.359299182891846
    },
    {
      "epoch": 0.39636856368563683,
      "step": 7313,
      "training_loss": 5.131252288818359
    },
    {
      "epoch": 0.3964227642276423,
      "step": 7314,
      "training_loss": 6.9708709716796875
    },
    {
      "epoch": 0.3964769647696477,
      "step": 7315,
      "training_loss": 7.989283084869385
    },
    {
      "epoch": 0.3965311653116531,
      "grad_norm": 31.335405349731445,
      "learning_rate": 1e-05,
      "loss": 6.8627,
      "step": 7316
    },
    {
      "epoch": 0.3965311653116531,
      "step": 7316,
      "training_loss": 3.3177218437194824
    },
    {
      "epoch": 0.39658536585365856,
      "step": 7317,
      "training_loss": 6.762340545654297
    },
    {
      "epoch": 0.39663956639566395,
      "step": 7318,
      "training_loss": 7.651504039764404
    },
    {
      "epoch": 0.3966937669376694,
      "step": 7319,
      "training_loss": 6.416579246520996
    },
    {
      "epoch": 0.3967479674796748,
      "grad_norm": 26.656461715698242,
      "learning_rate": 1e-05,
      "loss": 6.037,
      "step": 7320
    },
    {
      "epoch": 0.3967479674796748,
      "step": 7320,
      "training_loss": 6.778052806854248
    },
    {
      "epoch": 0.3968021680216802,
      "step": 7321,
      "training_loss": 7.008183002471924
    },
    {
      "epoch": 0.3968563685636856,
      "step": 7322,
      "training_loss": 6.681262969970703
    },
    {
      "epoch": 0.39691056910569106,
      "step": 7323,
      "training_loss": 4.86995267868042
    },
    {
      "epoch": 0.3969647696476965,
      "grad_norm": 22.04300880432129,
      "learning_rate": 1e-05,
      "loss": 6.3344,
      "step": 7324
    },
    {
      "epoch": 0.3969647696476965,
      "step": 7324,
      "training_loss": 7.169356346130371
    },
    {
      "epoch": 0.3970189701897019,
      "step": 7325,
      "training_loss": 7.0016679763793945
    },
    {
      "epoch": 0.39707317073170734,
      "step": 7326,
      "training_loss": 5.969143867492676
    },
    {
      "epoch": 0.3971273712737127,
      "step": 7327,
      "training_loss": 6.476751327514648
    },
    {
      "epoch": 0.39718157181571817,
      "grad_norm": 32.02535629272461,
      "learning_rate": 1e-05,
      "loss": 6.6542,
      "step": 7328
    },
    {
      "epoch": 0.39718157181571817,
      "step": 7328,
      "training_loss": 3.8866219520568848
    },
    {
      "epoch": 0.39723577235772356,
      "step": 7329,
      "training_loss": 5.749519348144531
    },
    {
      "epoch": 0.397289972899729,
      "step": 7330,
      "training_loss": 6.690897464752197
    },
    {
      "epoch": 0.3973441734417344,
      "step": 7331,
      "training_loss": 5.992074489593506
    },
    {
      "epoch": 0.39739837398373984,
      "grad_norm": 37.677635192871094,
      "learning_rate": 1e-05,
      "loss": 5.5798,
      "step": 7332
    },
    {
      "epoch": 0.39739837398373984,
      "step": 7332,
      "training_loss": 6.92940616607666
    },
    {
      "epoch": 0.3974525745257453,
      "step": 7333,
      "training_loss": 7.113687038421631
    },
    {
      "epoch": 0.3975067750677507,
      "step": 7334,
      "training_loss": 6.60088586807251
    },
    {
      "epoch": 0.3975609756097561,
      "step": 7335,
      "training_loss": 3.6883890628814697
    },
    {
      "epoch": 0.3976151761517615,
      "grad_norm": 27.29520034790039,
      "learning_rate": 1e-05,
      "loss": 6.0831,
      "step": 7336
    },
    {
      "epoch": 0.3976151761517615,
      "step": 7336,
      "training_loss": 7.269008636474609
    },
    {
      "epoch": 0.39766937669376695,
      "step": 7337,
      "training_loss": 7.3705244064331055
    },
    {
      "epoch": 0.39772357723577234,
      "step": 7338,
      "training_loss": 3.540541172027588
    },
    {
      "epoch": 0.3977777777777778,
      "step": 7339,
      "training_loss": 6.76015043258667
    },
    {
      "epoch": 0.3978319783197832,
      "grad_norm": 40.09764862060547,
      "learning_rate": 1e-05,
      "loss": 6.2351,
      "step": 7340
    },
    {
      "epoch": 0.3978319783197832,
      "step": 7340,
      "training_loss": 5.959716796875
    },
    {
      "epoch": 0.3978861788617886,
      "step": 7341,
      "training_loss": 6.545450687408447
    },
    {
      "epoch": 0.39794037940379406,
      "step": 7342,
      "training_loss": 6.627559661865234
    },
    {
      "epoch": 0.39799457994579945,
      "step": 7343,
      "training_loss": 6.82268762588501
    },
    {
      "epoch": 0.3980487804878049,
      "grad_norm": 16.78015899658203,
      "learning_rate": 1e-05,
      "loss": 6.4889,
      "step": 7344
    },
    {
      "epoch": 0.3980487804878049,
      "step": 7344,
      "training_loss": 5.926809787750244
    },
    {
      "epoch": 0.3981029810298103,
      "step": 7345,
      "training_loss": 6.047359466552734
    },
    {
      "epoch": 0.39815718157181573,
      "step": 7346,
      "training_loss": 8.183247566223145
    },
    {
      "epoch": 0.3982113821138211,
      "step": 7347,
      "training_loss": 3.657886505126953
    },
    {
      "epoch": 0.39826558265582657,
      "grad_norm": 25.536479949951172,
      "learning_rate": 1e-05,
      "loss": 5.9538,
      "step": 7348
    },
    {
      "epoch": 0.39826558265582657,
      "step": 7348,
      "training_loss": 7.245607852935791
    },
    {
      "epoch": 0.39831978319783196,
      "step": 7349,
      "training_loss": 7.6331915855407715
    },
    {
      "epoch": 0.3983739837398374,
      "step": 7350,
      "training_loss": 6.968524932861328
    },
    {
      "epoch": 0.39842818428184285,
      "step": 7351,
      "training_loss": 6.999598026275635
    },
    {
      "epoch": 0.39848238482384823,
      "grad_norm": 21.028060913085938,
      "learning_rate": 1e-05,
      "loss": 7.2117,
      "step": 7352
    },
    {
      "epoch": 0.39848238482384823,
      "step": 7352,
      "training_loss": 7.225113868713379
    },
    {
      "epoch": 0.3985365853658537,
      "step": 7353,
      "training_loss": 8.877124786376953
    },
    {
      "epoch": 0.39859078590785907,
      "step": 7354,
      "training_loss": 7.192966938018799
    },
    {
      "epoch": 0.3986449864498645,
      "step": 7355,
      "training_loss": 6.830648899078369
    },
    {
      "epoch": 0.3986991869918699,
      "grad_norm": 41.05656433105469,
      "learning_rate": 1e-05,
      "loss": 7.5315,
      "step": 7356
    },
    {
      "epoch": 0.3986991869918699,
      "step": 7356,
      "training_loss": 6.001355171203613
    },
    {
      "epoch": 0.39875338753387535,
      "step": 7357,
      "training_loss": 6.262204170227051
    },
    {
      "epoch": 0.39880758807588074,
      "step": 7358,
      "training_loss": 7.808730602264404
    },
    {
      "epoch": 0.3988617886178862,
      "step": 7359,
      "training_loss": 7.192479133605957
    },
    {
      "epoch": 0.3989159891598916,
      "grad_norm": 24.263992309570312,
      "learning_rate": 1e-05,
      "loss": 6.8162,
      "step": 7360
    },
    {
      "epoch": 0.3989159891598916,
      "step": 7360,
      "training_loss": 6.0843634605407715
    },
    {
      "epoch": 0.398970189701897,
      "step": 7361,
      "training_loss": 7.028557300567627
    },
    {
      "epoch": 0.39902439024390246,
      "step": 7362,
      "training_loss": 7.362267971038818
    },
    {
      "epoch": 0.39907859078590785,
      "step": 7363,
      "training_loss": 6.474125385284424
    },
    {
      "epoch": 0.3991327913279133,
      "grad_norm": 25.149072647094727,
      "learning_rate": 1e-05,
      "loss": 6.7373,
      "step": 7364
    },
    {
      "epoch": 0.3991327913279133,
      "step": 7364,
      "training_loss": 4.477811336517334
    },
    {
      "epoch": 0.3991869918699187,
      "step": 7365,
      "training_loss": 7.7790207862854
    },
    {
      "epoch": 0.39924119241192413,
      "step": 7366,
      "training_loss": 7.475703716278076
    },
    {
      "epoch": 0.3992953929539295,
      "step": 7367,
      "training_loss": 5.6587419509887695
    },
    {
      "epoch": 0.39934959349593496,
      "grad_norm": 24.038265228271484,
      "learning_rate": 1e-05,
      "loss": 6.3478,
      "step": 7368
    },
    {
      "epoch": 0.39934959349593496,
      "step": 7368,
      "training_loss": 7.339603900909424
    },
    {
      "epoch": 0.39940379403794035,
      "step": 7369,
      "training_loss": 6.317002296447754
    },
    {
      "epoch": 0.3994579945799458,
      "step": 7370,
      "training_loss": 5.453810691833496
    },
    {
      "epoch": 0.39951219512195124,
      "step": 7371,
      "training_loss": 6.473306179046631
    },
    {
      "epoch": 0.39956639566395663,
      "grad_norm": 27.66282844543457,
      "learning_rate": 1e-05,
      "loss": 6.3959,
      "step": 7372
    },
    {
      "epoch": 0.39956639566395663,
      "step": 7372,
      "training_loss": 6.748651504516602
    },
    {
      "epoch": 0.3996205962059621,
      "step": 7373,
      "training_loss": 3.7664647102355957
    },
    {
      "epoch": 0.39967479674796746,
      "step": 7374,
      "training_loss": 7.226869106292725
    },
    {
      "epoch": 0.3997289972899729,
      "step": 7375,
      "training_loss": 6.296019554138184
    },
    {
      "epoch": 0.3997831978319783,
      "grad_norm": 16.738239288330078,
      "learning_rate": 1e-05,
      "loss": 6.0095,
      "step": 7376
    },
    {
      "epoch": 0.3997831978319783,
      "step": 7376,
      "training_loss": 4.110815525054932
    },
    {
      "epoch": 0.39983739837398374,
      "step": 7377,
      "training_loss": 10.25929069519043
    },
    {
      "epoch": 0.39989159891598913,
      "step": 7378,
      "training_loss": 5.433449745178223
    },
    {
      "epoch": 0.3999457994579946,
      "step": 7379,
      "training_loss": 7.112864971160889
    },
    {
      "epoch": 0.4,
      "grad_norm": 28.803207397460938,
      "learning_rate": 1e-05,
      "loss": 6.7291,
      "step": 7380
    },
    {
      "epoch": 0.4,
      "step": 7380,
      "training_loss": 7.437262058258057
    },
    {
      "epoch": 0.4000542005420054,
      "step": 7381,
      "training_loss": 8.013895988464355
    },
    {
      "epoch": 0.40010840108401086,
      "step": 7382,
      "training_loss": 5.8267717361450195
    },
    {
      "epoch": 0.40016260162601625,
      "step": 7383,
      "training_loss": 7.112789630889893
    },
    {
      "epoch": 0.4002168021680217,
      "grad_norm": 24.080347061157227,
      "learning_rate": 1e-05,
      "loss": 7.0977,
      "step": 7384
    },
    {
      "epoch": 0.4002168021680217,
      "step": 7384,
      "training_loss": 7.029842853546143
    },
    {
      "epoch": 0.4002710027100271,
      "step": 7385,
      "training_loss": 6.331428527832031
    },
    {
      "epoch": 0.4003252032520325,
      "step": 7386,
      "training_loss": 7.162086009979248
    },
    {
      "epoch": 0.4003794037940379,
      "step": 7387,
      "training_loss": 6.277514457702637
    },
    {
      "epoch": 0.40043360433604336,
      "grad_norm": 24.457334518432617,
      "learning_rate": 1e-05,
      "loss": 6.7002,
      "step": 7388
    },
    {
      "epoch": 0.40043360433604336,
      "step": 7388,
      "training_loss": 7.688632488250732
    },
    {
      "epoch": 0.4004878048780488,
      "step": 7389,
      "training_loss": 7.458871841430664
    },
    {
      "epoch": 0.4005420054200542,
      "step": 7390,
      "training_loss": 7.298785209655762
    },
    {
      "epoch": 0.40059620596205964,
      "step": 7391,
      "training_loss": 5.574996471405029
    },
    {
      "epoch": 0.400650406504065,
      "grad_norm": 37.31958770751953,
      "learning_rate": 1e-05,
      "loss": 7.0053,
      "step": 7392
    },
    {
      "epoch": 0.400650406504065,
      "step": 7392,
      "training_loss": 7.234678745269775
    },
    {
      "epoch": 0.40070460704607047,
      "step": 7393,
      "training_loss": 7.044847011566162
    },
    {
      "epoch": 0.40075880758807586,
      "step": 7394,
      "training_loss": 8.167634963989258
    },
    {
      "epoch": 0.4008130081300813,
      "step": 7395,
      "training_loss": 6.769653797149658
    },
    {
      "epoch": 0.4008672086720867,
      "grad_norm": 17.359811782836914,
      "learning_rate": 1e-05,
      "loss": 7.3042,
      "step": 7396
    },
    {
      "epoch": 0.4008672086720867,
      "step": 7396,
      "training_loss": 7.4705328941345215
    },
    {
      "epoch": 0.40092140921409214,
      "step": 7397,
      "training_loss": 5.2361321449279785
    },
    {
      "epoch": 0.4009756097560976,
      "step": 7398,
      "training_loss": 6.2078471183776855
    },
    {
      "epoch": 0.401029810298103,
      "step": 7399,
      "training_loss": 6.42321252822876
    },
    {
      "epoch": 0.4010840108401084,
      "grad_norm": 26.187379837036133,
      "learning_rate": 1e-05,
      "loss": 6.3344,
      "step": 7400
    },
    {
      "epoch": 0.4010840108401084,
      "step": 7400,
      "training_loss": 4.865018367767334
    },
    {
      "epoch": 0.4011382113821138,
      "step": 7401,
      "training_loss": 7.882285118103027
    },
    {
      "epoch": 0.40119241192411925,
      "step": 7402,
      "training_loss": 7.1632304191589355
    },
    {
      "epoch": 0.40124661246612464,
      "step": 7403,
      "training_loss": 6.33237361907959
    },
    {
      "epoch": 0.4013008130081301,
      "grad_norm": 44.111602783203125,
      "learning_rate": 1e-05,
      "loss": 6.5607,
      "step": 7404
    },
    {
      "epoch": 0.4013008130081301,
      "step": 7404,
      "training_loss": 5.606115818023682
    },
    {
      "epoch": 0.4013550135501355,
      "step": 7405,
      "training_loss": 6.474436283111572
    },
    {
      "epoch": 0.4014092140921409,
      "step": 7406,
      "training_loss": 6.879089832305908
    },
    {
      "epoch": 0.40146341463414636,
      "step": 7407,
      "training_loss": 5.770050525665283
    },
    {
      "epoch": 0.40151761517615175,
      "grad_norm": 32.62046813964844,
      "learning_rate": 1e-05,
      "loss": 6.1824,
      "step": 7408
    },
    {
      "epoch": 0.40151761517615175,
      "step": 7408,
      "training_loss": 6.669522285461426
    },
    {
      "epoch": 0.4015718157181572,
      "step": 7409,
      "training_loss": 6.704941272735596
    },
    {
      "epoch": 0.4016260162601626,
      "step": 7410,
      "training_loss": 5.7418131828308105
    },
    {
      "epoch": 0.40168021680216803,
      "step": 7411,
      "training_loss": 6.275989532470703
    },
    {
      "epoch": 0.4017344173441734,
      "grad_norm": 32.490882873535156,
      "learning_rate": 1e-05,
      "loss": 6.3481,
      "step": 7412
    },
    {
      "epoch": 0.4017344173441734,
      "step": 7412,
      "training_loss": 7.609532833099365
    },
    {
      "epoch": 0.40178861788617887,
      "step": 7413,
      "training_loss": 3.3349764347076416
    },
    {
      "epoch": 0.40184281842818426,
      "step": 7414,
      "training_loss": 6.838066101074219
    },
    {
      "epoch": 0.4018970189701897,
      "step": 7415,
      "training_loss": 6.61293888092041
    },
    {
      "epoch": 0.40195121951219515,
      "grad_norm": 46.75132751464844,
      "learning_rate": 1e-05,
      "loss": 6.0989,
      "step": 7416
    },
    {
      "epoch": 0.40195121951219515,
      "step": 7416,
      "training_loss": 8.08866024017334
    },
    {
      "epoch": 0.40200542005420054,
      "step": 7417,
      "training_loss": 7.122399806976318
    },
    {
      "epoch": 0.402059620596206,
      "step": 7418,
      "training_loss": 7.000319480895996
    },
    {
      "epoch": 0.40211382113821137,
      "step": 7419,
      "training_loss": 5.642205715179443
    },
    {
      "epoch": 0.4021680216802168,
      "grad_norm": 29.134618759155273,
      "learning_rate": 1e-05,
      "loss": 6.9634,
      "step": 7420
    },
    {
      "epoch": 0.4021680216802168,
      "step": 7420,
      "training_loss": 6.323511600494385
    },
    {
      "epoch": 0.4022222222222222,
      "step": 7421,
      "training_loss": 6.635869979858398
    },
    {
      "epoch": 0.40227642276422765,
      "step": 7422,
      "training_loss": 6.260538101196289
    },
    {
      "epoch": 0.40233062330623304,
      "step": 7423,
      "training_loss": 7.201754570007324
    },
    {
      "epoch": 0.4023848238482385,
      "grad_norm": 27.76097869873047,
      "learning_rate": 1e-05,
      "loss": 6.6054,
      "step": 7424
    },
    {
      "epoch": 0.4023848238482385,
      "step": 7424,
      "training_loss": 4.763306140899658
    },
    {
      "epoch": 0.4024390243902439,
      "step": 7425,
      "training_loss": 6.536773681640625
    },
    {
      "epoch": 0.4024932249322493,
      "step": 7426,
      "training_loss": 6.637755870819092
    },
    {
      "epoch": 0.40254742547425476,
      "step": 7427,
      "training_loss": 7.184900760650635
    },
    {
      "epoch": 0.40260162601626015,
      "grad_norm": 25.642045974731445,
      "learning_rate": 1e-05,
      "loss": 6.2807,
      "step": 7428
    },
    {
      "epoch": 0.40260162601626015,
      "step": 7428,
      "training_loss": 6.619565010070801
    },
    {
      "epoch": 0.4026558265582656,
      "step": 7429,
      "training_loss": 3.7323598861694336
    },
    {
      "epoch": 0.402710027100271,
      "step": 7430,
      "training_loss": 6.798029899597168
    },
    {
      "epoch": 0.40276422764227643,
      "step": 7431,
      "training_loss": 7.171594142913818
    },
    {
      "epoch": 0.4028184281842818,
      "grad_norm": 14.626795768737793,
      "learning_rate": 1e-05,
      "loss": 6.0804,
      "step": 7432
    },
    {
      "epoch": 0.4028184281842818,
      "step": 7432,
      "training_loss": 5.980007171630859
    },
    {
      "epoch": 0.40287262872628726,
      "step": 7433,
      "training_loss": 6.652684688568115
    },
    {
      "epoch": 0.4029268292682927,
      "step": 7434,
      "training_loss": 6.856192111968994
    },
    {
      "epoch": 0.4029810298102981,
      "step": 7435,
      "training_loss": 8.120460510253906
    },
    {
      "epoch": 0.40303523035230354,
      "grad_norm": 30.154891967773438,
      "learning_rate": 1e-05,
      "loss": 6.9023,
      "step": 7436
    },
    {
      "epoch": 0.40303523035230354,
      "step": 7436,
      "training_loss": 6.243505954742432
    },
    {
      "epoch": 0.40308943089430893,
      "step": 7437,
      "training_loss": 7.81321907043457
    },
    {
      "epoch": 0.4031436314363144,
      "step": 7438,
      "training_loss": 5.07675838470459
    },
    {
      "epoch": 0.40319783197831977,
      "step": 7439,
      "training_loss": 6.419912815093994
    },
    {
      "epoch": 0.4032520325203252,
      "grad_norm": 26.764118194580078,
      "learning_rate": 1e-05,
      "loss": 6.3883,
      "step": 7440
    },
    {
      "epoch": 0.4032520325203252,
      "step": 7440,
      "training_loss": 6.468867778778076
    },
    {
      "epoch": 0.4033062330623306,
      "step": 7441,
      "training_loss": 5.954531669616699
    },
    {
      "epoch": 0.40336043360433604,
      "step": 7442,
      "training_loss": 7.764697074890137
    },
    {
      "epoch": 0.4034146341463415,
      "step": 7443,
      "training_loss": 7.8901166915893555
    },
    {
      "epoch": 0.4034688346883469,
      "grad_norm": 21.989757537841797,
      "learning_rate": 1e-05,
      "loss": 7.0196,
      "step": 7444
    },
    {
      "epoch": 0.4034688346883469,
      "step": 7444,
      "training_loss": 5.704701900482178
    },
    {
      "epoch": 0.4035230352303523,
      "step": 7445,
      "training_loss": 8.285834312438965
    },
    {
      "epoch": 0.4035772357723577,
      "step": 7446,
      "training_loss": 6.945742607116699
    },
    {
      "epoch": 0.40363143631436316,
      "step": 7447,
      "training_loss": 4.747708797454834
    },
    {
      "epoch": 0.40368563685636855,
      "grad_norm": 41.07554626464844,
      "learning_rate": 1e-05,
      "loss": 6.421,
      "step": 7448
    },
    {
      "epoch": 0.40368563685636855,
      "step": 7448,
      "training_loss": 3.6572353839874268
    },
    {
      "epoch": 0.403739837398374,
      "step": 7449,
      "training_loss": 6.796742916107178
    },
    {
      "epoch": 0.4037940379403794,
      "step": 7450,
      "training_loss": 6.617387771606445
    },
    {
      "epoch": 0.4038482384823848,
      "step": 7451,
      "training_loss": 7.46129035949707
    },
    {
      "epoch": 0.40390243902439027,
      "grad_norm": 39.1595344543457,
      "learning_rate": 1e-05,
      "loss": 6.1332,
      "step": 7452
    },
    {
      "epoch": 0.40390243902439027,
      "step": 7452,
      "training_loss": 7.006686210632324
    },
    {
      "epoch": 0.40395663956639566,
      "step": 7453,
      "training_loss": 7.102314472198486
    },
    {
      "epoch": 0.4040108401084011,
      "step": 7454,
      "training_loss": 6.949713230133057
    },
    {
      "epoch": 0.4040650406504065,
      "step": 7455,
      "training_loss": 6.390828609466553
    },
    {
      "epoch": 0.40411924119241194,
      "grad_norm": 21.086828231811523,
      "learning_rate": 1e-05,
      "loss": 6.8624,
      "step": 7456
    },
    {
      "epoch": 0.40411924119241194,
      "step": 7456,
      "training_loss": 7.533724784851074
    },
    {
      "epoch": 0.4041734417344173,
      "step": 7457,
      "training_loss": 7.049105167388916
    },
    {
      "epoch": 0.40422764227642277,
      "step": 7458,
      "training_loss": 6.475299835205078
    },
    {
      "epoch": 0.40428184281842816,
      "step": 7459,
      "training_loss": 6.361251354217529
    },
    {
      "epoch": 0.4043360433604336,
      "grad_norm": 24.51497459411621,
      "learning_rate": 1e-05,
      "loss": 6.8548,
      "step": 7460
    },
    {
      "epoch": 0.4043360433604336,
      "step": 7460,
      "training_loss": 6.417996883392334
    },
    {
      "epoch": 0.40439024390243905,
      "step": 7461,
      "training_loss": 7.534511566162109
    },
    {
      "epoch": 0.40444444444444444,
      "step": 7462,
      "training_loss": 6.735986232757568
    },
    {
      "epoch": 0.4044986449864499,
      "step": 7463,
      "training_loss": 5.835306644439697
    },
    {
      "epoch": 0.4045528455284553,
      "grad_norm": 36.81172180175781,
      "learning_rate": 1e-05,
      "loss": 6.631,
      "step": 7464
    },
    {
      "epoch": 0.4045528455284553,
      "step": 7464,
      "training_loss": 7.058969497680664
    },
    {
      "epoch": 0.4046070460704607,
      "step": 7465,
      "training_loss": 7.153674125671387
    },
    {
      "epoch": 0.4046612466124661,
      "step": 7466,
      "training_loss": 7.226945877075195
    },
    {
      "epoch": 0.40471544715447155,
      "step": 7467,
      "training_loss": 5.56262731552124
    },
    {
      "epoch": 0.40476964769647694,
      "grad_norm": 26.00107192993164,
      "learning_rate": 1e-05,
      "loss": 6.7506,
      "step": 7468
    },
    {
      "epoch": 0.40476964769647694,
      "step": 7468,
      "training_loss": 7.287125587463379
    },
    {
      "epoch": 0.4048238482384824,
      "step": 7469,
      "training_loss": 7.150584697723389
    },
    {
      "epoch": 0.40487804878048783,
      "step": 7470,
      "training_loss": 6.644893169403076
    },
    {
      "epoch": 0.4049322493224932,
      "step": 7471,
      "training_loss": 7.798174858093262
    },
    {
      "epoch": 0.40498644986449867,
      "grad_norm": 46.114559173583984,
      "learning_rate": 1e-05,
      "loss": 7.2202,
      "step": 7472
    },
    {
      "epoch": 0.40498644986449867,
      "step": 7472,
      "training_loss": 7.009734630584717
    },
    {
      "epoch": 0.40504065040650405,
      "step": 7473,
      "training_loss": 7.050102233886719
    },
    {
      "epoch": 0.4050948509485095,
      "step": 7474,
      "training_loss": 5.927144527435303
    },
    {
      "epoch": 0.4051490514905149,
      "step": 7475,
      "training_loss": 6.248011589050293
    },
    {
      "epoch": 0.40520325203252033,
      "grad_norm": 24.11564064025879,
      "learning_rate": 1e-05,
      "loss": 6.5587,
      "step": 7476
    },
    {
      "epoch": 0.40520325203252033,
      "step": 7476,
      "training_loss": 7.512349605560303
    },
    {
      "epoch": 0.4052574525745257,
      "step": 7477,
      "training_loss": 8.90215015411377
    },
    {
      "epoch": 0.40531165311653117,
      "step": 7478,
      "training_loss": 6.358344554901123
    },
    {
      "epoch": 0.4053658536585366,
      "step": 7479,
      "training_loss": 7.246358871459961
    },
    {
      "epoch": 0.405420054200542,
      "grad_norm": 30.141935348510742,
      "learning_rate": 1e-05,
      "loss": 7.5048,
      "step": 7480
    },
    {
      "epoch": 0.405420054200542,
      "step": 7480,
      "training_loss": 7.628805637359619
    },
    {
      "epoch": 0.40547425474254745,
      "step": 7481,
      "training_loss": 7.281904697418213
    },
    {
      "epoch": 0.40552845528455284,
      "step": 7482,
      "training_loss": 7.667950630187988
    },
    {
      "epoch": 0.4055826558265583,
      "step": 7483,
      "training_loss": 6.001323699951172
    },
    {
      "epoch": 0.40563685636856367,
      "grad_norm": 34.845855712890625,
      "learning_rate": 1e-05,
      "loss": 7.145,
      "step": 7484
    },
    {
      "epoch": 0.40563685636856367,
      "step": 7484,
      "training_loss": 8.017752647399902
    },
    {
      "epoch": 0.4056910569105691,
      "step": 7485,
      "training_loss": 7.230794906616211
    },
    {
      "epoch": 0.4057452574525745,
      "step": 7486,
      "training_loss": 6.33327054977417
    },
    {
      "epoch": 0.40579945799457995,
      "step": 7487,
      "training_loss": 5.868180751800537
    },
    {
      "epoch": 0.4058536585365854,
      "grad_norm": 41.90886306762695,
      "learning_rate": 1e-05,
      "loss": 6.8625,
      "step": 7488
    },
    {
      "epoch": 0.4058536585365854,
      "step": 7488,
      "training_loss": 7.267489433288574
    },
    {
      "epoch": 0.4059078590785908,
      "step": 7489,
      "training_loss": 7.316134929656982
    },
    {
      "epoch": 0.4059620596205962,
      "step": 7490,
      "training_loss": 6.470063209533691
    },
    {
      "epoch": 0.4060162601626016,
      "step": 7491,
      "training_loss": 8.446951866149902
    },
    {
      "epoch": 0.40607046070460706,
      "grad_norm": 30.742128372192383,
      "learning_rate": 1e-05,
      "loss": 7.3752,
      "step": 7492
    },
    {
      "epoch": 0.40607046070460706,
      "step": 7492,
      "training_loss": 7.64586877822876
    },
    {
      "epoch": 0.40612466124661245,
      "step": 7493,
      "training_loss": 6.988495349884033
    },
    {
      "epoch": 0.4061788617886179,
      "step": 7494,
      "training_loss": 6.530984878540039
    },
    {
      "epoch": 0.4062330623306233,
      "step": 7495,
      "training_loss": 7.333242416381836
    },
    {
      "epoch": 0.40628726287262873,
      "grad_norm": 17.867395401000977,
      "learning_rate": 1e-05,
      "loss": 7.1246,
      "step": 7496
    },
    {
      "epoch": 0.40628726287262873,
      "step": 7496,
      "training_loss": 5.992990970611572
    },
    {
      "epoch": 0.4063414634146341,
      "step": 7497,
      "training_loss": 6.0768723487854
    },
    {
      "epoch": 0.40639566395663956,
      "step": 7498,
      "training_loss": 6.7906999588012695
    },
    {
      "epoch": 0.406449864498645,
      "step": 7499,
      "training_loss": 6.622633934020996
    },
    {
      "epoch": 0.4065040650406504,
      "grad_norm": 24.533267974853516,
      "learning_rate": 1e-05,
      "loss": 6.3708,
      "step": 7500
    },
    {
      "epoch": 0.4065040650406504,
      "step": 7500,
      "training_loss": 6.442449569702148
    },
    {
      "epoch": 0.40655826558265584,
      "step": 7501,
      "training_loss": 7.711370468139648
    },
    {
      "epoch": 0.40661246612466123,
      "step": 7502,
      "training_loss": 6.396764278411865
    },
    {
      "epoch": 0.4066666666666667,
      "step": 7503,
      "training_loss": 6.579486846923828
    },
    {
      "epoch": 0.40672086720867207,
      "grad_norm": 25.275550842285156,
      "learning_rate": 1e-05,
      "loss": 6.7825,
      "step": 7504
    },
    {
      "epoch": 0.40672086720867207,
      "step": 7504,
      "training_loss": 7.038330078125
    },
    {
      "epoch": 0.4067750677506775,
      "step": 7505,
      "training_loss": 6.205780029296875
    },
    {
      "epoch": 0.4068292682926829,
      "step": 7506,
      "training_loss": 6.487898349761963
    },
    {
      "epoch": 0.40688346883468834,
      "step": 7507,
      "training_loss": 6.441143035888672
    },
    {
      "epoch": 0.4069376693766938,
      "grad_norm": 30.29231071472168,
      "learning_rate": 1e-05,
      "loss": 6.5433,
      "step": 7508
    },
    {
      "epoch": 0.4069376693766938,
      "step": 7508,
      "training_loss": 6.6501593589782715
    },
    {
      "epoch": 0.4069918699186992,
      "step": 7509,
      "training_loss": 6.241630554199219
    },
    {
      "epoch": 0.4070460704607046,
      "step": 7510,
      "training_loss": 6.548931121826172
    },
    {
      "epoch": 0.40710027100271,
      "step": 7511,
      "training_loss": 6.286919593811035
    },
    {
      "epoch": 0.40715447154471546,
      "grad_norm": 45.81669235229492,
      "learning_rate": 1e-05,
      "loss": 6.4319,
      "step": 7512
    },
    {
      "epoch": 0.40715447154471546,
      "step": 7512,
      "training_loss": 6.183393955230713
    },
    {
      "epoch": 0.40720867208672085,
      "step": 7513,
      "training_loss": 5.941897869110107
    },
    {
      "epoch": 0.4072628726287263,
      "step": 7514,
      "training_loss": 7.010492324829102
    },
    {
      "epoch": 0.4073170731707317,
      "step": 7515,
      "training_loss": 6.752414226531982
    },
    {
      "epoch": 0.4073712737127371,
      "grad_norm": 19.60426902770996,
      "learning_rate": 1e-05,
      "loss": 6.472,
      "step": 7516
    },
    {
      "epoch": 0.4073712737127371,
      "step": 7516,
      "training_loss": 6.843292713165283
    },
    {
      "epoch": 0.40742547425474257,
      "step": 7517,
      "training_loss": 5.969305038452148
    },
    {
      "epoch": 0.40747967479674796,
      "step": 7518,
      "training_loss": 6.493997097015381
    },
    {
      "epoch": 0.4075338753387534,
      "step": 7519,
      "training_loss": 7.543930530548096
    },
    {
      "epoch": 0.4075880758807588,
      "grad_norm": 28.272472381591797,
      "learning_rate": 1e-05,
      "loss": 6.7126,
      "step": 7520
    },
    {
      "epoch": 0.4075880758807588,
      "step": 7520,
      "training_loss": 6.417809009552002
    },
    {
      "epoch": 0.40764227642276424,
      "step": 7521,
      "training_loss": 7.123584747314453
    },
    {
      "epoch": 0.4076964769647696,
      "step": 7522,
      "training_loss": 7.501959323883057
    },
    {
      "epoch": 0.40775067750677507,
      "step": 7523,
      "training_loss": 5.55683708190918
    },
    {
      "epoch": 0.40780487804878046,
      "grad_norm": 30.163503646850586,
      "learning_rate": 1e-05,
      "loss": 6.65,
      "step": 7524
    },
    {
      "epoch": 0.40780487804878046,
      "step": 7524,
      "training_loss": 5.506644248962402
    },
    {
      "epoch": 0.4078590785907859,
      "step": 7525,
      "training_loss": 8.393240928649902
    },
    {
      "epoch": 0.40791327913279135,
      "step": 7526,
      "training_loss": 6.59257698059082
    },
    {
      "epoch": 0.40796747967479674,
      "step": 7527,
      "training_loss": 5.559625148773193
    },
    {
      "epoch": 0.4080216802168022,
      "grad_norm": 34.25416946411133,
      "learning_rate": 1e-05,
      "loss": 6.513,
      "step": 7528
    },
    {
      "epoch": 0.4080216802168022,
      "step": 7528,
      "training_loss": 7.121633052825928
    },
    {
      "epoch": 0.4080758807588076,
      "step": 7529,
      "training_loss": 6.004499912261963
    },
    {
      "epoch": 0.408130081300813,
      "step": 7530,
      "training_loss": 6.415772438049316
    },
    {
      "epoch": 0.4081842818428184,
      "step": 7531,
      "training_loss": 6.161742687225342
    },
    {
      "epoch": 0.40823848238482385,
      "grad_norm": 31.83953857421875,
      "learning_rate": 1e-05,
      "loss": 6.4259,
      "step": 7532
    },
    {
      "epoch": 0.40823848238482385,
      "step": 7532,
      "training_loss": 8.941690444946289
    },
    {
      "epoch": 0.40829268292682924,
      "step": 7533,
      "training_loss": 5.295707702636719
    },
    {
      "epoch": 0.4083468834688347,
      "step": 7534,
      "training_loss": 7.214529514312744
    },
    {
      "epoch": 0.40840108401084013,
      "step": 7535,
      "training_loss": 6.876858711242676
    },
    {
      "epoch": 0.4084552845528455,
      "grad_norm": 35.78632354736328,
      "learning_rate": 1e-05,
      "loss": 7.0822,
      "step": 7536
    },
    {
      "epoch": 0.4084552845528455,
      "step": 7536,
      "training_loss": 6.399169445037842
    },
    {
      "epoch": 0.40850948509485097,
      "step": 7537,
      "training_loss": 4.144837856292725
    },
    {
      "epoch": 0.40856368563685636,
      "step": 7538,
      "training_loss": 7.82336950302124
    },
    {
      "epoch": 0.4086178861788618,
      "step": 7539,
      "training_loss": 3.744335889816284
    },
    {
      "epoch": 0.4086720867208672,
      "grad_norm": 23.24009895324707,
      "learning_rate": 1e-05,
      "loss": 5.5279,
      "step": 7540
    },
    {
      "epoch": 0.4086720867208672,
      "step": 7540,
      "training_loss": 6.392261981964111
    },
    {
      "epoch": 0.40872628726287263,
      "step": 7541,
      "training_loss": 7.120906829833984
    },
    {
      "epoch": 0.408780487804878,
      "step": 7542,
      "training_loss": 7.016310214996338
    },
    {
      "epoch": 0.40883468834688347,
      "step": 7543,
      "training_loss": 6.9040656089782715
    },
    {
      "epoch": 0.4088888888888889,
      "grad_norm": 29.42048454284668,
      "learning_rate": 1e-05,
      "loss": 6.8584,
      "step": 7544
    },
    {
      "epoch": 0.4088888888888889,
      "step": 7544,
      "training_loss": 3.4533839225769043
    },
    {
      "epoch": 0.4089430894308943,
      "step": 7545,
      "training_loss": 7.75993013381958
    },
    {
      "epoch": 0.40899728997289975,
      "step": 7546,
      "training_loss": 7.061952114105225
    },
    {
      "epoch": 0.40905149051490514,
      "step": 7547,
      "training_loss": 7.111529350280762
    },
    {
      "epoch": 0.4091056910569106,
      "grad_norm": 18.866273880004883,
      "learning_rate": 1e-05,
      "loss": 6.3467,
      "step": 7548
    },
    {
      "epoch": 0.4091056910569106,
      "step": 7548,
      "training_loss": 7.2341718673706055
    },
    {
      "epoch": 0.40915989159891597,
      "step": 7549,
      "training_loss": 4.71633768081665
    },
    {
      "epoch": 0.4092140921409214,
      "step": 7550,
      "training_loss": 6.947551727294922
    },
    {
      "epoch": 0.4092682926829268,
      "step": 7551,
      "training_loss": 7.85488224029541
    },
    {
      "epoch": 0.40932249322493225,
      "grad_norm": 36.7707405090332,
      "learning_rate": 1e-05,
      "loss": 6.6882,
      "step": 7552
    },
    {
      "epoch": 0.40932249322493225,
      "step": 7552,
      "training_loss": 6.872740268707275
    },
    {
      "epoch": 0.4093766937669377,
      "step": 7553,
      "training_loss": 7.306842803955078
    },
    {
      "epoch": 0.4094308943089431,
      "step": 7554,
      "training_loss": 6.848151683807373
    },
    {
      "epoch": 0.40948509485094853,
      "step": 7555,
      "training_loss": 6.6508402824401855
    },
    {
      "epoch": 0.4095392953929539,
      "grad_norm": 35.8945198059082,
      "learning_rate": 1e-05,
      "loss": 6.9196,
      "step": 7556
    },
    {
      "epoch": 0.4095392953929539,
      "step": 7556,
      "training_loss": 6.4541521072387695
    },
    {
      "epoch": 0.40959349593495936,
      "step": 7557,
      "training_loss": 2.893548011779785
    },
    {
      "epoch": 0.40964769647696475,
      "step": 7558,
      "training_loss": 6.393303394317627
    },
    {
      "epoch": 0.4097018970189702,
      "step": 7559,
      "training_loss": 4.960347652435303
    },
    {
      "epoch": 0.4097560975609756,
      "grad_norm": 32.13814163208008,
      "learning_rate": 1e-05,
      "loss": 5.1753,
      "step": 7560
    },
    {
      "epoch": 0.4097560975609756,
      "step": 7560,
      "training_loss": 6.598443984985352
    },
    {
      "epoch": 0.40981029810298103,
      "step": 7561,
      "training_loss": 7.411984443664551
    },
    {
      "epoch": 0.4098644986449865,
      "step": 7562,
      "training_loss": 6.720041275024414
    },
    {
      "epoch": 0.40991869918699186,
      "step": 7563,
      "training_loss": 7.453019618988037
    },
    {
      "epoch": 0.4099728997289973,
      "grad_norm": 42.159942626953125,
      "learning_rate": 1e-05,
      "loss": 7.0459,
      "step": 7564
    },
    {
      "epoch": 0.4099728997289973,
      "step": 7564,
      "training_loss": 7.610309600830078
    },
    {
      "epoch": 0.4100271002710027,
      "step": 7565,
      "training_loss": 7.25598669052124
    },
    {
      "epoch": 0.41008130081300814,
      "step": 7566,
      "training_loss": 6.925579071044922
    },
    {
      "epoch": 0.41013550135501353,
      "step": 7567,
      "training_loss": 7.815921783447266
    },
    {
      "epoch": 0.410189701897019,
      "grad_norm": 20.12781524658203,
      "learning_rate": 1e-05,
      "loss": 7.4019,
      "step": 7568
    },
    {
      "epoch": 0.410189701897019,
      "step": 7568,
      "training_loss": 6.31447696685791
    },
    {
      "epoch": 0.41024390243902437,
      "step": 7569,
      "training_loss": 6.952073097229004
    },
    {
      "epoch": 0.4102981029810298,
      "step": 7570,
      "training_loss": 6.979824542999268
    },
    {
      "epoch": 0.41035230352303526,
      "step": 7571,
      "training_loss": 8.275704383850098
    },
    {
      "epoch": 0.41040650406504064,
      "grad_norm": 26.623676300048828,
      "learning_rate": 1e-05,
      "loss": 7.1305,
      "step": 7572
    },
    {
      "epoch": 0.41040650406504064,
      "step": 7572,
      "training_loss": 5.033967971801758
    },
    {
      "epoch": 0.4104607046070461,
      "step": 7573,
      "training_loss": 5.7358903884887695
    },
    {
      "epoch": 0.4105149051490515,
      "step": 7574,
      "training_loss": 6.239565849304199
    },
    {
      "epoch": 0.4105691056910569,
      "step": 7575,
      "training_loss": 6.692193984985352
    },
    {
      "epoch": 0.4106233062330623,
      "grad_norm": 22.690155029296875,
      "learning_rate": 1e-05,
      "loss": 5.9254,
      "step": 7576
    },
    {
      "epoch": 0.4106233062330623,
      "step": 7576,
      "training_loss": 5.894453048706055
    },
    {
      "epoch": 0.41067750677506776,
      "step": 7577,
      "training_loss": 4.829363822937012
    },
    {
      "epoch": 0.41073170731707315,
      "step": 7578,
      "training_loss": 7.173053741455078
    },
    {
      "epoch": 0.4107859078590786,
      "step": 7579,
      "training_loss": 6.492788314819336
    },
    {
      "epoch": 0.41084010840108404,
      "grad_norm": 25.815793991088867,
      "learning_rate": 1e-05,
      "loss": 6.0974,
      "step": 7580
    },
    {
      "epoch": 0.41084010840108404,
      "step": 7580,
      "training_loss": 6.788500785827637
    },
    {
      "epoch": 0.4108943089430894,
      "step": 7581,
      "training_loss": 4.85092830657959
    },
    {
      "epoch": 0.41094850948509487,
      "step": 7582,
      "training_loss": 7.767731666564941
    },
    {
      "epoch": 0.41100271002710026,
      "step": 7583,
      "training_loss": 6.510496616363525
    },
    {
      "epoch": 0.4110569105691057,
      "grad_norm": 22.326005935668945,
      "learning_rate": 1e-05,
      "loss": 6.4794,
      "step": 7584
    },
    {
      "epoch": 0.4110569105691057,
      "step": 7584,
      "training_loss": 8.472265243530273
    },
    {
      "epoch": 0.4111111111111111,
      "step": 7585,
      "training_loss": 6.632848739624023
    },
    {
      "epoch": 0.41116531165311654,
      "step": 7586,
      "training_loss": 7.271413803100586
    },
    {
      "epoch": 0.41121951219512193,
      "step": 7587,
      "training_loss": 6.864455699920654
    },
    {
      "epoch": 0.4112737127371274,
      "grad_norm": 16.983320236206055,
      "learning_rate": 1e-05,
      "loss": 7.3102,
      "step": 7588
    },
    {
      "epoch": 0.4112737127371274,
      "step": 7588,
      "training_loss": 6.765711784362793
    },
    {
      "epoch": 0.4113279132791328,
      "step": 7589,
      "training_loss": 7.027331352233887
    },
    {
      "epoch": 0.4113821138211382,
      "step": 7590,
      "training_loss": 4.427988052368164
    },
    {
      "epoch": 0.41143631436314365,
      "step": 7591,
      "training_loss": 6.656741142272949
    },
    {
      "epoch": 0.41149051490514904,
      "grad_norm": 36.57645034790039,
      "learning_rate": 1e-05,
      "loss": 6.2194,
      "step": 7592
    },
    {
      "epoch": 0.41149051490514904,
      "step": 7592,
      "training_loss": 7.095437526702881
    },
    {
      "epoch": 0.4115447154471545,
      "step": 7593,
      "training_loss": 7.31374454498291
    },
    {
      "epoch": 0.4115989159891599,
      "step": 7594,
      "training_loss": 4.30722188949585
    },
    {
      "epoch": 0.4116531165311653,
      "step": 7595,
      "training_loss": 6.884979248046875
    },
    {
      "epoch": 0.4117073170731707,
      "grad_norm": 24.18291473388672,
      "learning_rate": 1e-05,
      "loss": 6.4003,
      "step": 7596
    },
    {
      "epoch": 0.4117073170731707,
      "step": 7596,
      "training_loss": 7.566868305206299
    },
    {
      "epoch": 0.41176151761517615,
      "step": 7597,
      "training_loss": 6.885537147521973
    },
    {
      "epoch": 0.4118157181571816,
      "step": 7598,
      "training_loss": 5.43121337890625
    },
    {
      "epoch": 0.411869918699187,
      "step": 7599,
      "training_loss": 7.013779163360596
    },
    {
      "epoch": 0.41192411924119243,
      "grad_norm": 36.04652786254883,
      "learning_rate": 1e-05,
      "loss": 6.7243,
      "step": 7600
    },
    {
      "epoch": 0.41192411924119243,
      "step": 7600,
      "training_loss": 6.875563144683838
    },
    {
      "epoch": 0.4119783197831978,
      "step": 7601,
      "training_loss": 7.806459426879883
    },
    {
      "epoch": 0.41203252032520327,
      "step": 7602,
      "training_loss": 6.4596052169799805
    },
    {
      "epoch": 0.41208672086720866,
      "step": 7603,
      "training_loss": 7.178589344024658
    },
    {
      "epoch": 0.4121409214092141,
      "grad_norm": 47.16157913208008,
      "learning_rate": 1e-05,
      "loss": 7.0801,
      "step": 7604
    },
    {
      "epoch": 0.4121409214092141,
      "step": 7604,
      "training_loss": 5.8933844566345215
    },
    {
      "epoch": 0.4121951219512195,
      "step": 7605,
      "training_loss": 7.996598720550537
    },
    {
      "epoch": 0.41224932249322493,
      "step": 7606,
      "training_loss": 6.012012958526611
    },
    {
      "epoch": 0.4123035230352304,
      "step": 7607,
      "training_loss": 6.668570041656494
    },
    {
      "epoch": 0.41235772357723577,
      "grad_norm": 42.55768966674805,
      "learning_rate": 1e-05,
      "loss": 6.6426,
      "step": 7608
    },
    {
      "epoch": 0.41235772357723577,
      "step": 7608,
      "training_loss": 6.931301116943359
    },
    {
      "epoch": 0.4124119241192412,
      "step": 7609,
      "training_loss": 4.988126277923584
    },
    {
      "epoch": 0.4124661246612466,
      "step": 7610,
      "training_loss": 6.968217372894287
    },
    {
      "epoch": 0.41252032520325205,
      "step": 7611,
      "training_loss": 7.113757610321045
    },
    {
      "epoch": 0.41257452574525744,
      "grad_norm": 33.930511474609375,
      "learning_rate": 1e-05,
      "loss": 6.5004,
      "step": 7612
    },
    {
      "epoch": 0.41257452574525744,
      "step": 7612,
      "training_loss": 6.427861213684082
    },
    {
      "epoch": 0.4126287262872629,
      "step": 7613,
      "training_loss": 6.7452778816223145
    },
    {
      "epoch": 0.41268292682926827,
      "step": 7614,
      "training_loss": 7.8344831466674805
    },
    {
      "epoch": 0.4127371273712737,
      "step": 7615,
      "training_loss": 6.758591175079346
    },
    {
      "epoch": 0.41279132791327916,
      "grad_norm": 16.198381423950195,
      "learning_rate": 1e-05,
      "loss": 6.9416,
      "step": 7616
    },
    {
      "epoch": 0.41279132791327916,
      "step": 7616,
      "training_loss": 5.910443305969238
    },
    {
      "epoch": 0.41284552845528455,
      "step": 7617,
      "training_loss": 7.357443332672119
    },
    {
      "epoch": 0.41289972899729,
      "step": 7618,
      "training_loss": 8.13483715057373
    },
    {
      "epoch": 0.4129539295392954,
      "step": 7619,
      "training_loss": 6.969118595123291
    },
    {
      "epoch": 0.41300813008130083,
      "grad_norm": 22.43235206604004,
      "learning_rate": 1e-05,
      "loss": 7.093,
      "step": 7620
    },
    {
      "epoch": 0.41300813008130083,
      "step": 7620,
      "training_loss": 6.6251020431518555
    },
    {
      "epoch": 0.4130623306233062,
      "step": 7621,
      "training_loss": 6.787398338317871
    },
    {
      "epoch": 0.41311653116531166,
      "step": 7622,
      "training_loss": 7.2068610191345215
    },
    {
      "epoch": 0.41317073170731705,
      "step": 7623,
      "training_loss": 7.060799598693848
    },
    {
      "epoch": 0.4132249322493225,
      "grad_norm": 24.364521026611328,
      "learning_rate": 1e-05,
      "loss": 6.92,
      "step": 7624
    },
    {
      "epoch": 0.4132249322493225,
      "step": 7624,
      "training_loss": 5.213955402374268
    },
    {
      "epoch": 0.4132791327913279,
      "step": 7625,
      "training_loss": 6.611571788787842
    },
    {
      "epoch": 0.41333333333333333,
      "step": 7626,
      "training_loss": 6.433046340942383
    },
    {
      "epoch": 0.4133875338753388,
      "step": 7627,
      "training_loss": 7.013207912445068
    },
    {
      "epoch": 0.41344173441734416,
      "grad_norm": 21.436769485473633,
      "learning_rate": 1e-05,
      "loss": 6.3179,
      "step": 7628
    },
    {
      "epoch": 0.41344173441734416,
      "step": 7628,
      "training_loss": 7.0864763259887695
    },
    {
      "epoch": 0.4134959349593496,
      "step": 7629,
      "training_loss": 5.753509044647217
    },
    {
      "epoch": 0.413550135501355,
      "step": 7630,
      "training_loss": 6.438249588012695
    },
    {
      "epoch": 0.41360433604336044,
      "step": 7631,
      "training_loss": 5.513089179992676
    },
    {
      "epoch": 0.41365853658536583,
      "grad_norm": 42.7889404296875,
      "learning_rate": 1e-05,
      "loss": 6.1978,
      "step": 7632
    },
    {
      "epoch": 0.41365853658536583,
      "step": 7632,
      "training_loss": 6.941665172576904
    },
    {
      "epoch": 0.4137127371273713,
      "step": 7633,
      "training_loss": 7.755363941192627
    },
    {
      "epoch": 0.41376693766937667,
      "step": 7634,
      "training_loss": 6.83052921295166
    },
    {
      "epoch": 0.4138211382113821,
      "step": 7635,
      "training_loss": 6.940684795379639
    },
    {
      "epoch": 0.41387533875338756,
      "grad_norm": 19.51777458190918,
      "learning_rate": 1e-05,
      "loss": 7.1171,
      "step": 7636
    },
    {
      "epoch": 0.41387533875338756,
      "step": 7636,
      "training_loss": 7.1064324378967285
    },
    {
      "epoch": 0.41392953929539295,
      "step": 7637,
      "training_loss": 5.916669845581055
    },
    {
      "epoch": 0.4139837398373984,
      "step": 7638,
      "training_loss": 6.750951290130615
    },
    {
      "epoch": 0.4140379403794038,
      "step": 7639,
      "training_loss": 5.846860885620117
    },
    {
      "epoch": 0.4140921409214092,
      "grad_norm": 24.934825897216797,
      "learning_rate": 1e-05,
      "loss": 6.4052,
      "step": 7640
    },
    {
      "epoch": 0.4140921409214092,
      "step": 7640,
      "training_loss": 6.219931602478027
    },
    {
      "epoch": 0.4141463414634146,
      "step": 7641,
      "training_loss": 6.644257068634033
    },
    {
      "epoch": 0.41420054200542006,
      "step": 7642,
      "training_loss": 7.870326042175293
    },
    {
      "epoch": 0.41425474254742545,
      "step": 7643,
      "training_loss": 7.550781726837158
    },
    {
      "epoch": 0.4143089430894309,
      "grad_norm": 27.732107162475586,
      "learning_rate": 1e-05,
      "loss": 7.0713,
      "step": 7644
    },
    {
      "epoch": 0.4143089430894309,
      "step": 7644,
      "training_loss": 7.81325101852417
    },
    {
      "epoch": 0.41436314363143634,
      "step": 7645,
      "training_loss": 7.6058173179626465
    },
    {
      "epoch": 0.4144173441734417,
      "step": 7646,
      "training_loss": 7.706308841705322
    },
    {
      "epoch": 0.41447154471544717,
      "step": 7647,
      "training_loss": 7.1765360832214355
    },
    {
      "epoch": 0.41452574525745256,
      "grad_norm": 32.484703063964844,
      "learning_rate": 1e-05,
      "loss": 7.5755,
      "step": 7648
    },
    {
      "epoch": 0.41452574525745256,
      "step": 7648,
      "training_loss": 7.0763678550720215
    },
    {
      "epoch": 0.414579945799458,
      "step": 7649,
      "training_loss": 6.750057697296143
    },
    {
      "epoch": 0.4146341463414634,
      "step": 7650,
      "training_loss": 6.668651103973389
    },
    {
      "epoch": 0.41468834688346884,
      "step": 7651,
      "training_loss": 8.400938034057617
    },
    {
      "epoch": 0.41474254742547423,
      "grad_norm": 36.185794830322266,
      "learning_rate": 1e-05,
      "loss": 7.224,
      "step": 7652
    },
    {
      "epoch": 0.41474254742547423,
      "step": 7652,
      "training_loss": 6.989224910736084
    },
    {
      "epoch": 0.4147967479674797,
      "step": 7653,
      "training_loss": 6.33795166015625
    },
    {
      "epoch": 0.4148509485094851,
      "step": 7654,
      "training_loss": 6.491886615753174
    },
    {
      "epoch": 0.4149051490514905,
      "step": 7655,
      "training_loss": 7.014217376708984
    },
    {
      "epoch": 0.41495934959349595,
      "grad_norm": 23.8441219329834,
      "learning_rate": 1e-05,
      "loss": 6.7083,
      "step": 7656
    },
    {
      "epoch": 0.41495934959349595,
      "step": 7656,
      "training_loss": 7.2931342124938965
    },
    {
      "epoch": 0.41501355013550134,
      "step": 7657,
      "training_loss": 5.423446178436279
    },
    {
      "epoch": 0.4150677506775068,
      "step": 7658,
      "training_loss": 6.834499835968018
    },
    {
      "epoch": 0.4151219512195122,
      "step": 7659,
      "training_loss": 6.018067359924316
    },
    {
      "epoch": 0.4151761517615176,
      "grad_norm": 69.72260284423828,
      "learning_rate": 1e-05,
      "loss": 6.3923,
      "step": 7660
    },
    {
      "epoch": 0.4151761517615176,
      "step": 7660,
      "training_loss": 7.7315287590026855
    },
    {
      "epoch": 0.415230352303523,
      "step": 7661,
      "training_loss": 5.9689483642578125
    },
    {
      "epoch": 0.41528455284552845,
      "step": 7662,
      "training_loss": 6.386828422546387
    },
    {
      "epoch": 0.4153387533875339,
      "step": 7663,
      "training_loss": 4.380887985229492
    },
    {
      "epoch": 0.4153929539295393,
      "grad_norm": 38.38799285888672,
      "learning_rate": 1e-05,
      "loss": 6.117,
      "step": 7664
    },
    {
      "epoch": 0.4153929539295393,
      "step": 7664,
      "training_loss": 7.384108543395996
    },
    {
      "epoch": 0.41544715447154473,
      "step": 7665,
      "training_loss": 6.036690711975098
    },
    {
      "epoch": 0.4155013550135501,
      "step": 7666,
      "training_loss": 7.038891792297363
    },
    {
      "epoch": 0.41555555555555557,
      "step": 7667,
      "training_loss": 6.383690357208252
    },
    {
      "epoch": 0.41560975609756096,
      "grad_norm": 22.98211669921875,
      "learning_rate": 1e-05,
      "loss": 6.7108,
      "step": 7668
    },
    {
      "epoch": 0.41560975609756096,
      "step": 7668,
      "training_loss": 7.812686920166016
    },
    {
      "epoch": 0.4156639566395664,
      "step": 7669,
      "training_loss": 4.707923889160156
    },
    {
      "epoch": 0.4157181571815718,
      "step": 7670,
      "training_loss": 7.300449848175049
    },
    {
      "epoch": 0.41577235772357723,
      "step": 7671,
      "training_loss": 7.871554374694824
    },
    {
      "epoch": 0.4158265582655827,
      "grad_norm": 20.65654754638672,
      "learning_rate": 1e-05,
      "loss": 6.9232,
      "step": 7672
    },
    {
      "epoch": 0.4158265582655827,
      "step": 7672,
      "training_loss": 7.270224094390869
    },
    {
      "epoch": 0.41588075880758807,
      "step": 7673,
      "training_loss": 4.510278224945068
    },
    {
      "epoch": 0.4159349593495935,
      "step": 7674,
      "training_loss": 7.776300430297852
    },
    {
      "epoch": 0.4159891598915989,
      "step": 7675,
      "training_loss": 6.464915752410889
    },
    {
      "epoch": 0.41604336043360435,
      "grad_norm": 39.84014892578125,
      "learning_rate": 1e-05,
      "loss": 6.5054,
      "step": 7676
    },
    {
      "epoch": 0.41604336043360435,
      "step": 7676,
      "training_loss": 7.654420375823975
    },
    {
      "epoch": 0.41609756097560974,
      "step": 7677,
      "training_loss": 8.03846263885498
    },
    {
      "epoch": 0.4161517615176152,
      "step": 7678,
      "training_loss": 7.218215465545654
    },
    {
      "epoch": 0.41620596205962057,
      "step": 7679,
      "training_loss": 6.210945129394531
    },
    {
      "epoch": 0.416260162601626,
      "grad_norm": 30.878751754760742,
      "learning_rate": 1e-05,
      "loss": 7.2805,
      "step": 7680
    },
    {
      "epoch": 0.416260162601626,
      "step": 7680,
      "training_loss": 3.4332821369171143
    },
    {
      "epoch": 0.41631436314363146,
      "step": 7681,
      "training_loss": 4.148479461669922
    },
    {
      "epoch": 0.41636856368563685,
      "step": 7682,
      "training_loss": 4.945489883422852
    },
    {
      "epoch": 0.4164227642276423,
      "step": 7683,
      "training_loss": 6.244405746459961
    },
    {
      "epoch": 0.4164769647696477,
      "grad_norm": 31.305910110473633,
      "learning_rate": 1e-05,
      "loss": 4.6929,
      "step": 7684
    },
    {
      "epoch": 0.4164769647696477,
      "step": 7684,
      "training_loss": 8.475407600402832
    },
    {
      "epoch": 0.41653116531165313,
      "step": 7685,
      "training_loss": 6.495334148406982
    },
    {
      "epoch": 0.4165853658536585,
      "step": 7686,
      "training_loss": 6.649301052093506
    },
    {
      "epoch": 0.41663956639566396,
      "step": 7687,
      "training_loss": 6.935438632965088
    },
    {
      "epoch": 0.41669376693766935,
      "grad_norm": 26.120376586914062,
      "learning_rate": 1e-05,
      "loss": 7.1389,
      "step": 7688
    },
    {
      "epoch": 0.41669376693766935,
      "step": 7688,
      "training_loss": 7.586895942687988
    },
    {
      "epoch": 0.4167479674796748,
      "step": 7689,
      "training_loss": 6.86264181137085
    },
    {
      "epoch": 0.41680216802168024,
      "step": 7690,
      "training_loss": 7.43306827545166
    },
    {
      "epoch": 0.41685636856368563,
      "step": 7691,
      "training_loss": 6.586820125579834
    },
    {
      "epoch": 0.4169105691056911,
      "grad_norm": 19.97188949584961,
      "learning_rate": 1e-05,
      "loss": 7.1174,
      "step": 7692
    },
    {
      "epoch": 0.4169105691056911,
      "step": 7692,
      "training_loss": 7.036323070526123
    },
    {
      "epoch": 0.41696476964769646,
      "step": 7693,
      "training_loss": 8.661930084228516
    },
    {
      "epoch": 0.4170189701897019,
      "step": 7694,
      "training_loss": 7.762115478515625
    },
    {
      "epoch": 0.4170731707317073,
      "step": 7695,
      "training_loss": 7.310917377471924
    },
    {
      "epoch": 0.41712737127371274,
      "grad_norm": 25.1306209564209,
      "learning_rate": 1e-05,
      "loss": 7.6928,
      "step": 7696
    },
    {
      "epoch": 0.41712737127371274,
      "step": 7696,
      "training_loss": 6.466590881347656
    },
    {
      "epoch": 0.41718157181571813,
      "step": 7697,
      "training_loss": 7.857304573059082
    },
    {
      "epoch": 0.4172357723577236,
      "step": 7698,
      "training_loss": 6.022629261016846
    },
    {
      "epoch": 0.417289972899729,
      "step": 7699,
      "training_loss": 6.896627902984619
    },
    {
      "epoch": 0.4173441734417344,
      "grad_norm": 51.49489974975586,
      "learning_rate": 1e-05,
      "loss": 6.8108,
      "step": 7700
    },
    {
      "epoch": 0.4173441734417344,
      "step": 7700,
      "training_loss": 6.398451805114746
    },
    {
      "epoch": 0.41739837398373986,
      "step": 7701,
      "training_loss": 6.712703704833984
    },
    {
      "epoch": 0.41745257452574525,
      "step": 7702,
      "training_loss": 7.223179817199707
    },
    {
      "epoch": 0.4175067750677507,
      "step": 7703,
      "training_loss": 4.347097396850586
    },
    {
      "epoch": 0.4175609756097561,
      "grad_norm": 33.080162048339844,
      "learning_rate": 1e-05,
      "loss": 6.1704,
      "step": 7704
    },
    {
      "epoch": 0.4175609756097561,
      "step": 7704,
      "training_loss": 6.631043910980225
    },
    {
      "epoch": 0.4176151761517615,
      "step": 7705,
      "training_loss": 6.889456748962402
    },
    {
      "epoch": 0.4176693766937669,
      "step": 7706,
      "training_loss": 7.3994059562683105
    },
    {
      "epoch": 0.41772357723577236,
      "step": 7707,
      "training_loss": 7.503105163574219
    },
    {
      "epoch": 0.4177777777777778,
      "grad_norm": 24.463356018066406,
      "learning_rate": 1e-05,
      "loss": 7.1058,
      "step": 7708
    },
    {
      "epoch": 0.4177777777777778,
      "step": 7708,
      "training_loss": 6.267416954040527
    },
    {
      "epoch": 0.4178319783197832,
      "step": 7709,
      "training_loss": 6.874973773956299
    },
    {
      "epoch": 0.41788617886178864,
      "step": 7710,
      "training_loss": 7.348937511444092
    },
    {
      "epoch": 0.417940379403794,
      "step": 7711,
      "training_loss": 7.14518404006958
    },
    {
      "epoch": 0.41799457994579947,
      "grad_norm": 21.063989639282227,
      "learning_rate": 1e-05,
      "loss": 6.9091,
      "step": 7712
    },
    {
      "epoch": 0.41799457994579947,
      "step": 7712,
      "training_loss": 4.143589973449707
    },
    {
      "epoch": 0.41804878048780486,
      "step": 7713,
      "training_loss": 7.567941665649414
    },
    {
      "epoch": 0.4181029810298103,
      "step": 7714,
      "training_loss": 5.759700298309326
    },
    {
      "epoch": 0.4181571815718157,
      "step": 7715,
      "training_loss": 6.612008094787598
    },
    {
      "epoch": 0.41821138211382114,
      "grad_norm": 24.390485763549805,
      "learning_rate": 1e-05,
      "loss": 6.0208,
      "step": 7716
    },
    {
      "epoch": 0.41821138211382114,
      "step": 7716,
      "training_loss": 8.00241470336914
    },
    {
      "epoch": 0.4182655826558266,
      "step": 7717,
      "training_loss": 6.343388557434082
    },
    {
      "epoch": 0.418319783197832,
      "step": 7718,
      "training_loss": 7.205050468444824
    },
    {
      "epoch": 0.4183739837398374,
      "step": 7719,
      "training_loss": 5.87721061706543
    },
    {
      "epoch": 0.4184281842818428,
      "grad_norm": 32.57801055908203,
      "learning_rate": 1e-05,
      "loss": 6.857,
      "step": 7720
    },
    {
      "epoch": 0.4184281842818428,
      "step": 7720,
      "training_loss": 8.068928718566895
    },
    {
      "epoch": 0.41848238482384825,
      "step": 7721,
      "training_loss": 7.034666538238525
    },
    {
      "epoch": 0.41853658536585364,
      "step": 7722,
      "training_loss": 8.419090270996094
    },
    {
      "epoch": 0.4185907859078591,
      "step": 7723,
      "training_loss": 8.118354797363281
    },
    {
      "epoch": 0.4186449864498645,
      "grad_norm": 18.819799423217773,
      "learning_rate": 1e-05,
      "loss": 7.9103,
      "step": 7724
    },
    {
      "epoch": 0.4186449864498645,
      "step": 7724,
      "training_loss": 6.401317596435547
    },
    {
      "epoch": 0.4186991869918699,
      "step": 7725,
      "training_loss": 5.383295059204102
    },
    {
      "epoch": 0.41875338753387537,
      "step": 7726,
      "training_loss": 7.197575569152832
    },
    {
      "epoch": 0.41880758807588075,
      "step": 7727,
      "training_loss": 7.412471771240234
    },
    {
      "epoch": 0.4188617886178862,
      "grad_norm": 24.57183074951172,
      "learning_rate": 1e-05,
      "loss": 6.5987,
      "step": 7728
    },
    {
      "epoch": 0.4188617886178862,
      "step": 7728,
      "training_loss": 5.229171276092529
    },
    {
      "epoch": 0.4189159891598916,
      "step": 7729,
      "training_loss": 5.852801322937012
    },
    {
      "epoch": 0.41897018970189703,
      "step": 7730,
      "training_loss": 6.702392578125
    },
    {
      "epoch": 0.4190243902439024,
      "step": 7731,
      "training_loss": 6.61341667175293
    },
    {
      "epoch": 0.41907859078590787,
      "grad_norm": 41.63737487792969,
      "learning_rate": 1e-05,
      "loss": 6.0994,
      "step": 7732
    },
    {
      "epoch": 0.41907859078590787,
      "step": 7732,
      "training_loss": 7.58092737197876
    },
    {
      "epoch": 0.41913279132791326,
      "step": 7733,
      "training_loss": 7.096378326416016
    },
    {
      "epoch": 0.4191869918699187,
      "step": 7734,
      "training_loss": 8.424118041992188
    },
    {
      "epoch": 0.41924119241192415,
      "step": 7735,
      "training_loss": 6.827807426452637
    },
    {
      "epoch": 0.41929539295392954,
      "grad_norm": 18.581838607788086,
      "learning_rate": 1e-05,
      "loss": 7.4823,
      "step": 7736
    },
    {
      "epoch": 0.41929539295392954,
      "step": 7736,
      "training_loss": 5.171513080596924
    },
    {
      "epoch": 0.419349593495935,
      "step": 7737,
      "training_loss": 6.37436056137085
    },
    {
      "epoch": 0.41940379403794037,
      "step": 7738,
      "training_loss": 6.864777088165283
    },
    {
      "epoch": 0.4194579945799458,
      "step": 7739,
      "training_loss": 6.690176010131836
    },
    {
      "epoch": 0.4195121951219512,
      "grad_norm": 31.065086364746094,
      "learning_rate": 1e-05,
      "loss": 6.2752,
      "step": 7740
    },
    {
      "epoch": 0.4195121951219512,
      "step": 7740,
      "training_loss": 4.859755992889404
    },
    {
      "epoch": 0.41956639566395665,
      "step": 7741,
      "training_loss": 7.427501678466797
    },
    {
      "epoch": 0.41962059620596204,
      "step": 7742,
      "training_loss": 7.524186611175537
    },
    {
      "epoch": 0.4196747967479675,
      "step": 7743,
      "training_loss": 5.945630073547363
    },
    {
      "epoch": 0.4197289972899729,
      "grad_norm": 27.332609176635742,
      "learning_rate": 1e-05,
      "loss": 6.4393,
      "step": 7744
    },
    {
      "epoch": 0.4197289972899729,
      "step": 7744,
      "training_loss": 6.4751057624816895
    },
    {
      "epoch": 0.4197831978319783,
      "step": 7745,
      "training_loss": 7.686435222625732
    },
    {
      "epoch": 0.41983739837398376,
      "step": 7746,
      "training_loss": 7.237191200256348
    },
    {
      "epoch": 0.41989159891598915,
      "step": 7747,
      "training_loss": 6.08746862411499
    },
    {
      "epoch": 0.4199457994579946,
      "grad_norm": 20.220949172973633,
      "learning_rate": 1e-05,
      "loss": 6.8716,
      "step": 7748
    },
    {
      "epoch": 0.4199457994579946,
      "step": 7748,
      "training_loss": 7.124852657318115
    },
    {
      "epoch": 0.42,
      "step": 7749,
      "training_loss": 6.569269180297852
    },
    {
      "epoch": 0.42005420054200543,
      "step": 7750,
      "training_loss": 6.674663066864014
    },
    {
      "epoch": 0.4201084010840108,
      "step": 7751,
      "training_loss": 6.976460933685303
    },
    {
      "epoch": 0.42016260162601626,
      "grad_norm": 58.44417190551758,
      "learning_rate": 1e-05,
      "loss": 6.8363,
      "step": 7752
    },
    {
      "epoch": 0.42016260162601626,
      "step": 7752,
      "training_loss": 5.834338665008545
    },
    {
      "epoch": 0.42021680216802165,
      "step": 7753,
      "training_loss": 4.24915075302124
    },
    {
      "epoch": 0.4202710027100271,
      "step": 7754,
      "training_loss": 6.2704854011535645
    },
    {
      "epoch": 0.42032520325203254,
      "step": 7755,
      "training_loss": 5.295060634613037
    },
    {
      "epoch": 0.42037940379403793,
      "grad_norm": 19.675804138183594,
      "learning_rate": 1e-05,
      "loss": 5.4123,
      "step": 7756
    },
    {
      "epoch": 0.42037940379403793,
      "step": 7756,
      "training_loss": 6.337949752807617
    },
    {
      "epoch": 0.4204336043360434,
      "step": 7757,
      "training_loss": 7.099472522735596
    },
    {
      "epoch": 0.42048780487804877,
      "step": 7758,
      "training_loss": 6.383653163909912
    },
    {
      "epoch": 0.4205420054200542,
      "step": 7759,
      "training_loss": 6.251302242279053
    },
    {
      "epoch": 0.4205962059620596,
      "grad_norm": 25.72007942199707,
      "learning_rate": 1e-05,
      "loss": 6.5181,
      "step": 7760
    },
    {
      "epoch": 0.4205962059620596,
      "step": 7760,
      "training_loss": 7.1793599128723145
    },
    {
      "epoch": 0.42065040650406504,
      "step": 7761,
      "training_loss": 6.289941310882568
    },
    {
      "epoch": 0.42070460704607043,
      "step": 7762,
      "training_loss": 7.241664886474609
    },
    {
      "epoch": 0.4207588075880759,
      "step": 7763,
      "training_loss": 5.625668525695801
    },
    {
      "epoch": 0.4208130081300813,
      "grad_norm": 25.290437698364258,
      "learning_rate": 1e-05,
      "loss": 6.5842,
      "step": 7764
    },
    {
      "epoch": 0.4208130081300813,
      "step": 7764,
      "training_loss": 6.682417869567871
    },
    {
      "epoch": 0.4208672086720867,
      "step": 7765,
      "training_loss": 7.283472061157227
    },
    {
      "epoch": 0.42092140921409216,
      "step": 7766,
      "training_loss": 7.340407371520996
    },
    {
      "epoch": 0.42097560975609755,
      "step": 7767,
      "training_loss": 6.91591739654541
    },
    {
      "epoch": 0.421029810298103,
      "grad_norm": 69.92362976074219,
      "learning_rate": 1e-05,
      "loss": 7.0556,
      "step": 7768
    },
    {
      "epoch": 0.421029810298103,
      "step": 7768,
      "training_loss": 7.431829929351807
    },
    {
      "epoch": 0.4210840108401084,
      "step": 7769,
      "training_loss": 6.807079315185547
    },
    {
      "epoch": 0.4211382113821138,
      "step": 7770,
      "training_loss": 6.779428482055664
    },
    {
      "epoch": 0.4211924119241192,
      "step": 7771,
      "training_loss": 7.829164505004883
    },
    {
      "epoch": 0.42124661246612466,
      "grad_norm": 20.84551429748535,
      "learning_rate": 1e-05,
      "loss": 7.2119,
      "step": 7772
    },
    {
      "epoch": 0.42124661246612466,
      "step": 7772,
      "training_loss": 6.96693229675293
    },
    {
      "epoch": 0.4213008130081301,
      "step": 7773,
      "training_loss": 7.318663120269775
    },
    {
      "epoch": 0.4213550135501355,
      "step": 7774,
      "training_loss": 6.508758068084717
    },
    {
      "epoch": 0.42140921409214094,
      "step": 7775,
      "training_loss": 7.212369441986084
    },
    {
      "epoch": 0.4214634146341463,
      "grad_norm": 22.10368537902832,
      "learning_rate": 1e-05,
      "loss": 7.0017,
      "step": 7776
    },
    {
      "epoch": 0.4214634146341463,
      "step": 7776,
      "training_loss": 6.729766368865967
    },
    {
      "epoch": 0.42151761517615177,
      "step": 7777,
      "training_loss": 6.919695854187012
    },
    {
      "epoch": 0.42157181571815716,
      "step": 7778,
      "training_loss": 7.701744556427002
    },
    {
      "epoch": 0.4216260162601626,
      "step": 7779,
      "training_loss": 6.534665107727051
    },
    {
      "epoch": 0.421680216802168,
      "grad_norm": 35.078487396240234,
      "learning_rate": 1e-05,
      "loss": 6.9715,
      "step": 7780
    },
    {
      "epoch": 0.421680216802168,
      "step": 7780,
      "training_loss": 5.732327938079834
    },
    {
      "epoch": 0.42173441734417344,
      "step": 7781,
      "training_loss": 7.564126014709473
    },
    {
      "epoch": 0.4217886178861789,
      "step": 7782,
      "training_loss": 6.052468776702881
    },
    {
      "epoch": 0.4218428184281843,
      "step": 7783,
      "training_loss": 6.4098310470581055
    },
    {
      "epoch": 0.4218970189701897,
      "grad_norm": 26.640438079833984,
      "learning_rate": 1e-05,
      "loss": 6.4397,
      "step": 7784
    },
    {
      "epoch": 0.4218970189701897,
      "step": 7784,
      "training_loss": 7.502034664154053
    },
    {
      "epoch": 0.4219512195121951,
      "step": 7785,
      "training_loss": 6.510094165802002
    },
    {
      "epoch": 0.42200542005420055,
      "step": 7786,
      "training_loss": 7.684813499450684
    },
    {
      "epoch": 0.42205962059620594,
      "step": 7787,
      "training_loss": 6.871053695678711
    },
    {
      "epoch": 0.4221138211382114,
      "grad_norm": 39.67859649658203,
      "learning_rate": 1e-05,
      "loss": 7.142,
      "step": 7788
    },
    {
      "epoch": 0.4221138211382114,
      "step": 7788,
      "training_loss": 6.864894390106201
    },
    {
      "epoch": 0.4221680216802168,
      "step": 7789,
      "training_loss": 6.309174537658691
    },
    {
      "epoch": 0.4222222222222222,
      "step": 7790,
      "training_loss": 7.228411674499512
    },
    {
      "epoch": 0.42227642276422767,
      "step": 7791,
      "training_loss": 6.949085712432861
    },
    {
      "epoch": 0.42233062330623306,
      "grad_norm": 24.76970100402832,
      "learning_rate": 1e-05,
      "loss": 6.8379,
      "step": 7792
    },
    {
      "epoch": 0.42233062330623306,
      "step": 7792,
      "training_loss": 7.406891345977783
    },
    {
      "epoch": 0.4223848238482385,
      "step": 7793,
      "training_loss": 7.076571941375732
    },
    {
      "epoch": 0.4224390243902439,
      "step": 7794,
      "training_loss": 9.45516300201416
    },
    {
      "epoch": 0.42249322493224933,
      "step": 7795,
      "training_loss": 4.166803359985352
    },
    {
      "epoch": 0.4225474254742547,
      "grad_norm": 31.0424747467041,
      "learning_rate": 1e-05,
      "loss": 7.0264,
      "step": 7796
    },
    {
      "epoch": 0.4225474254742547,
      "step": 7796,
      "training_loss": 7.301784515380859
    },
    {
      "epoch": 0.42260162601626017,
      "step": 7797,
      "training_loss": 7.484076976776123
    },
    {
      "epoch": 0.42265582655826556,
      "step": 7798,
      "training_loss": 6.873716354370117
    },
    {
      "epoch": 0.422710027100271,
      "step": 7799,
      "training_loss": 5.6030683517456055
    },
    {
      "epoch": 0.42276422764227645,
      "grad_norm": 24.62680435180664,
      "learning_rate": 1e-05,
      "loss": 6.8157,
      "step": 7800
    },
    {
      "epoch": 0.42276422764227645,
      "step": 7800,
      "training_loss": 7.3858819007873535
    },
    {
      "epoch": 0.42281842818428184,
      "step": 7801,
      "training_loss": 6.986471176147461
    },
    {
      "epoch": 0.4228726287262873,
      "step": 7802,
      "training_loss": 7.074446678161621
    },
    {
      "epoch": 0.42292682926829267,
      "step": 7803,
      "training_loss": 6.9121317863464355
    },
    {
      "epoch": 0.4229810298102981,
      "grad_norm": 17.55544662475586,
      "learning_rate": 1e-05,
      "loss": 7.0897,
      "step": 7804
    },
    {
      "epoch": 0.4229810298102981,
      "step": 7804,
      "training_loss": 6.8040876388549805
    },
    {
      "epoch": 0.4230352303523035,
      "step": 7805,
      "training_loss": 5.387664318084717
    },
    {
      "epoch": 0.42308943089430895,
      "step": 7806,
      "training_loss": 3.782909631729126
    },
    {
      "epoch": 0.42314363143631434,
      "step": 7807,
      "training_loss": 6.057889461517334
    },
    {
      "epoch": 0.4231978319783198,
      "grad_norm": 38.88338088989258,
      "learning_rate": 1e-05,
      "loss": 5.5081,
      "step": 7808
    },
    {
      "epoch": 0.4231978319783198,
      "step": 7808,
      "training_loss": 5.052989482879639
    },
    {
      "epoch": 0.4232520325203252,
      "step": 7809,
      "training_loss": 6.862192153930664
    },
    {
      "epoch": 0.4233062330623306,
      "step": 7810,
      "training_loss": 7.688868522644043
    },
    {
      "epoch": 0.42336043360433606,
      "step": 7811,
      "training_loss": 5.7483229637146
    },
    {
      "epoch": 0.42341463414634145,
      "grad_norm": 24.028352737426758,
      "learning_rate": 1e-05,
      "loss": 6.3381,
      "step": 7812
    },
    {
      "epoch": 0.42341463414634145,
      "step": 7812,
      "training_loss": 7.488256931304932
    },
    {
      "epoch": 0.4234688346883469,
      "step": 7813,
      "training_loss": 6.254744529724121
    },
    {
      "epoch": 0.4235230352303523,
      "step": 7814,
      "training_loss": 7.998236179351807
    },
    {
      "epoch": 0.42357723577235773,
      "step": 7815,
      "training_loss": 7.260943412780762
    },
    {
      "epoch": 0.4236314363143631,
      "grad_norm": 19.27342987060547,
      "learning_rate": 1e-05,
      "loss": 7.2505,
      "step": 7816
    },
    {
      "epoch": 0.4236314363143631,
      "step": 7816,
      "training_loss": 6.924376964569092
    },
    {
      "epoch": 0.42368563685636856,
      "step": 7817,
      "training_loss": 7.069772243499756
    },
    {
      "epoch": 0.423739837398374,
      "step": 7818,
      "training_loss": 7.2915849685668945
    },
    {
      "epoch": 0.4237940379403794,
      "step": 7819,
      "training_loss": 6.922695636749268
    },
    {
      "epoch": 0.42384823848238484,
      "grad_norm": 30.71107292175293,
      "learning_rate": 1e-05,
      "loss": 7.0521,
      "step": 7820
    },
    {
      "epoch": 0.42384823848238484,
      "step": 7820,
      "training_loss": 6.5576605796813965
    },
    {
      "epoch": 0.42390243902439023,
      "step": 7821,
      "training_loss": 3.973170280456543
    },
    {
      "epoch": 0.4239566395663957,
      "step": 7822,
      "training_loss": 5.320155620574951
    },
    {
      "epoch": 0.42401084010840107,
      "step": 7823,
      "training_loss": 7.016238212585449
    },
    {
      "epoch": 0.4240650406504065,
      "grad_norm": 53.354278564453125,
      "learning_rate": 1e-05,
      "loss": 5.7168,
      "step": 7824
    },
    {
      "epoch": 0.4240650406504065,
      "step": 7824,
      "training_loss": 7.819061756134033
    },
    {
      "epoch": 0.4241192411924119,
      "step": 7825,
      "training_loss": 4.881776332855225
    },
    {
      "epoch": 0.42417344173441734,
      "step": 7826,
      "training_loss": 6.622987747192383
    },
    {
      "epoch": 0.4242276422764228,
      "step": 7827,
      "training_loss": 7.3280110359191895
    },
    {
      "epoch": 0.4242818428184282,
      "grad_norm": 27.74319076538086,
      "learning_rate": 1e-05,
      "loss": 6.663,
      "step": 7828
    },
    {
      "epoch": 0.4242818428184282,
      "step": 7828,
      "training_loss": 6.421045780181885
    },
    {
      "epoch": 0.4243360433604336,
      "step": 7829,
      "training_loss": 6.118679523468018
    },
    {
      "epoch": 0.424390243902439,
      "step": 7830,
      "training_loss": 5.826162815093994
    },
    {
      "epoch": 0.42444444444444446,
      "step": 7831,
      "training_loss": 8.043225288391113
    },
    {
      "epoch": 0.42449864498644985,
      "grad_norm": 25.917627334594727,
      "learning_rate": 1e-05,
      "loss": 6.6023,
      "step": 7832
    },
    {
      "epoch": 0.42449864498644985,
      "step": 7832,
      "training_loss": 5.8083038330078125
    },
    {
      "epoch": 0.4245528455284553,
      "step": 7833,
      "training_loss": 6.805800914764404
    },
    {
      "epoch": 0.4246070460704607,
      "step": 7834,
      "training_loss": 4.261364936828613
    },
    {
      "epoch": 0.4246612466124661,
      "step": 7835,
      "training_loss": 6.186458110809326
    },
    {
      "epoch": 0.42471544715447157,
      "grad_norm": 33.43647003173828,
      "learning_rate": 1e-05,
      "loss": 5.7655,
      "step": 7836
    },
    {
      "epoch": 0.42471544715447157,
      "step": 7836,
      "training_loss": 3.1403656005859375
    },
    {
      "epoch": 0.42476964769647696,
      "step": 7837,
      "training_loss": 7.637777328491211
    },
    {
      "epoch": 0.4248238482384824,
      "step": 7838,
      "training_loss": 8.215968132019043
    },
    {
      "epoch": 0.4248780487804878,
      "step": 7839,
      "training_loss": 6.735254287719727
    },
    {
      "epoch": 0.42493224932249324,
      "grad_norm": 27.3978214263916,
      "learning_rate": 1e-05,
      "loss": 6.4323,
      "step": 7840
    },
    {
      "epoch": 0.42493224932249324,
      "step": 7840,
      "training_loss": 6.73409366607666
    },
    {
      "epoch": 0.4249864498644986,
      "step": 7841,
      "training_loss": 6.653261661529541
    },
    {
      "epoch": 0.4250406504065041,
      "step": 7842,
      "training_loss": 7.277663707733154
    },
    {
      "epoch": 0.42509485094850946,
      "step": 7843,
      "training_loss": 6.559392929077148
    },
    {
      "epoch": 0.4251490514905149,
      "grad_norm": 24.54143524169922,
      "learning_rate": 1e-05,
      "loss": 6.8061,
      "step": 7844
    },
    {
      "epoch": 0.4251490514905149,
      "step": 7844,
      "training_loss": 7.472454071044922
    },
    {
      "epoch": 0.42520325203252035,
      "step": 7845,
      "training_loss": 3.719482660293579
    },
    {
      "epoch": 0.42525745257452574,
      "step": 7846,
      "training_loss": 7.452087879180908
    },
    {
      "epoch": 0.4253116531165312,
      "step": 7847,
      "training_loss": 6.6780900955200195
    },
    {
      "epoch": 0.4253658536585366,
      "grad_norm": 26.08942985534668,
      "learning_rate": 1e-05,
      "loss": 6.3305,
      "step": 7848
    },
    {
      "epoch": 0.4253658536585366,
      "step": 7848,
      "training_loss": 6.078473091125488
    },
    {
      "epoch": 0.425420054200542,
      "step": 7849,
      "training_loss": 7.153534889221191
    },
    {
      "epoch": 0.4254742547425474,
      "step": 7850,
      "training_loss": 6.3733744621276855
    },
    {
      "epoch": 0.42552845528455285,
      "step": 7851,
      "training_loss": 6.332329750061035
    },
    {
      "epoch": 0.42558265582655824,
      "grad_norm": 23.064266204833984,
      "learning_rate": 1e-05,
      "loss": 6.4844,
      "step": 7852
    },
    {
      "epoch": 0.42558265582655824,
      "step": 7852,
      "training_loss": 7.513256072998047
    },
    {
      "epoch": 0.4256368563685637,
      "step": 7853,
      "training_loss": 6.436367034912109
    },
    {
      "epoch": 0.42569105691056913,
      "step": 7854,
      "training_loss": 7.7619948387146
    },
    {
      "epoch": 0.4257452574525745,
      "step": 7855,
      "training_loss": 7.905157566070557
    },
    {
      "epoch": 0.42579945799457997,
      "grad_norm": 28.574052810668945,
      "learning_rate": 1e-05,
      "loss": 7.4042,
      "step": 7856
    },
    {
      "epoch": 0.42579945799457997,
      "step": 7856,
      "training_loss": 5.589784145355225
    },
    {
      "epoch": 0.42585365853658536,
      "step": 7857,
      "training_loss": 6.623550891876221
    },
    {
      "epoch": 0.4259078590785908,
      "step": 7858,
      "training_loss": 7.031798839569092
    },
    {
      "epoch": 0.4259620596205962,
      "step": 7859,
      "training_loss": 7.843588829040527
    },
    {
      "epoch": 0.42601626016260163,
      "grad_norm": 16.350257873535156,
      "learning_rate": 1e-05,
      "loss": 6.7722,
      "step": 7860
    },
    {
      "epoch": 0.42601626016260163,
      "step": 7860,
      "training_loss": 6.851053714752197
    },
    {
      "epoch": 0.426070460704607,
      "step": 7861,
      "training_loss": 6.944736003875732
    },
    {
      "epoch": 0.42612466124661247,
      "step": 7862,
      "training_loss": 5.240780830383301
    },
    {
      "epoch": 0.4261788617886179,
      "step": 7863,
      "training_loss": 7.138175964355469
    },
    {
      "epoch": 0.4262330623306233,
      "grad_norm": 31.178518295288086,
      "learning_rate": 1e-05,
      "loss": 6.5437,
      "step": 7864
    },
    {
      "epoch": 0.4262330623306233,
      "step": 7864,
      "training_loss": 3.722895622253418
    },
    {
      "epoch": 0.42628726287262875,
      "step": 7865,
      "training_loss": 7.654791355133057
    },
    {
      "epoch": 0.42634146341463414,
      "step": 7866,
      "training_loss": 7.186412811279297
    },
    {
      "epoch": 0.4263956639566396,
      "step": 7867,
      "training_loss": 8.30303955078125
    },
    {
      "epoch": 0.42644986449864497,
      "grad_norm": 43.84754180908203,
      "learning_rate": 1e-05,
      "loss": 6.7168,
      "step": 7868
    },
    {
      "epoch": 0.42644986449864497,
      "step": 7868,
      "training_loss": 6.818877696990967
    },
    {
      "epoch": 0.4265040650406504,
      "step": 7869,
      "training_loss": 7.9246506690979
    },
    {
      "epoch": 0.4265582655826558,
      "step": 7870,
      "training_loss": 6.510461330413818
    },
    {
      "epoch": 0.42661246612466125,
      "step": 7871,
      "training_loss": 7.190768718719482
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 16.411664962768555,
      "learning_rate": 1e-05,
      "loss": 7.1112,
      "step": 7872
    },
    {
      "epoch": 0.4266666666666667,
      "step": 7872,
      "training_loss": 6.504652500152588
    },
    {
      "epoch": 0.4267208672086721,
      "step": 7873,
      "training_loss": 7.5367431640625
    },
    {
      "epoch": 0.42677506775067753,
      "step": 7874,
      "training_loss": 8.598840713500977
    },
    {
      "epoch": 0.4268292682926829,
      "step": 7875,
      "training_loss": 6.934776782989502
    },
    {
      "epoch": 0.42688346883468836,
      "grad_norm": 23.010074615478516,
      "learning_rate": 1e-05,
      "loss": 7.3938,
      "step": 7876
    },
    {
      "epoch": 0.42688346883468836,
      "step": 7876,
      "training_loss": 5.5561747550964355
    },
    {
      "epoch": 0.42693766937669375,
      "step": 7877,
      "training_loss": 6.491856098175049
    },
    {
      "epoch": 0.4269918699186992,
      "step": 7878,
      "training_loss": 7.779551029205322
    },
    {
      "epoch": 0.4270460704607046,
      "step": 7879,
      "training_loss": 7.290308475494385
    },
    {
      "epoch": 0.42710027100271003,
      "grad_norm": 38.83766555786133,
      "learning_rate": 1e-05,
      "loss": 6.7795,
      "step": 7880
    },
    {
      "epoch": 0.42710027100271003,
      "step": 7880,
      "training_loss": 7.408020973205566
    },
    {
      "epoch": 0.4271544715447154,
      "step": 7881,
      "training_loss": 6.287638187408447
    },
    {
      "epoch": 0.42720867208672086,
      "step": 7882,
      "training_loss": 6.474941730499268
    },
    {
      "epoch": 0.4272628726287263,
      "step": 7883,
      "training_loss": 6.014929294586182
    },
    {
      "epoch": 0.4273170731707317,
      "grad_norm": 23.2979736328125,
      "learning_rate": 1e-05,
      "loss": 6.5464,
      "step": 7884
    },
    {
      "epoch": 0.4273170731707317,
      "step": 7884,
      "training_loss": 7.660056114196777
    },
    {
      "epoch": 0.42737127371273714,
      "step": 7885,
      "training_loss": 7.805360317230225
    },
    {
      "epoch": 0.42742547425474253,
      "step": 7886,
      "training_loss": 6.847892761230469
    },
    {
      "epoch": 0.427479674796748,
      "step": 7887,
      "training_loss": 6.875881671905518
    },
    {
      "epoch": 0.42753387533875337,
      "grad_norm": 24.01569938659668,
      "learning_rate": 1e-05,
      "loss": 7.2973,
      "step": 7888
    },
    {
      "epoch": 0.42753387533875337,
      "step": 7888,
      "training_loss": 4.630331993103027
    },
    {
      "epoch": 0.4275880758807588,
      "step": 7889,
      "training_loss": 7.3683953285217285
    },
    {
      "epoch": 0.4276422764227642,
      "step": 7890,
      "training_loss": 7.6905293464660645
    },
    {
      "epoch": 0.42769647696476965,
      "step": 7891,
      "training_loss": 7.223028659820557
    },
    {
      "epoch": 0.4277506775067751,
      "grad_norm": 26.99675178527832,
      "learning_rate": 1e-05,
      "loss": 6.7281,
      "step": 7892
    },
    {
      "epoch": 0.4277506775067751,
      "step": 7892,
      "training_loss": 7.698811054229736
    },
    {
      "epoch": 0.4278048780487805,
      "step": 7893,
      "training_loss": 6.224588871002197
    },
    {
      "epoch": 0.4278590785907859,
      "step": 7894,
      "training_loss": 6.086231708526611
    },
    {
      "epoch": 0.4279132791327913,
      "step": 7895,
      "training_loss": 8.603888511657715
    },
    {
      "epoch": 0.42796747967479676,
      "grad_norm": 28.59384536743164,
      "learning_rate": 1e-05,
      "loss": 7.1534,
      "step": 7896
    },
    {
      "epoch": 0.42796747967479676,
      "step": 7896,
      "training_loss": 8.454004287719727
    },
    {
      "epoch": 0.42802168021680215,
      "step": 7897,
      "training_loss": 6.575347423553467
    },
    {
      "epoch": 0.4280758807588076,
      "step": 7898,
      "training_loss": 6.996267318725586
    },
    {
      "epoch": 0.428130081300813,
      "step": 7899,
      "training_loss": 6.466290473937988
    },
    {
      "epoch": 0.4281842818428184,
      "grad_norm": 40.723506927490234,
      "learning_rate": 1e-05,
      "loss": 7.123,
      "step": 7900
    },
    {
      "epoch": 0.4281842818428184,
      "step": 7900,
      "training_loss": 6.253154277801514
    },
    {
      "epoch": 0.42823848238482387,
      "step": 7901,
      "training_loss": 7.334366321563721
    },
    {
      "epoch": 0.42829268292682926,
      "step": 7902,
      "training_loss": 6.912310600280762
    },
    {
      "epoch": 0.4283468834688347,
      "step": 7903,
      "training_loss": 6.532101154327393
    },
    {
      "epoch": 0.4284010840108401,
      "grad_norm": 23.885108947753906,
      "learning_rate": 1e-05,
      "loss": 6.758,
      "step": 7904
    },
    {
      "epoch": 0.4284010840108401,
      "step": 7904,
      "training_loss": 7.468868255615234
    },
    {
      "epoch": 0.42845528455284554,
      "step": 7905,
      "training_loss": 4.04206657409668
    },
    {
      "epoch": 0.42850948509485093,
      "step": 7906,
      "training_loss": 7.502621650695801
    },
    {
      "epoch": 0.4285636856368564,
      "step": 7907,
      "training_loss": 6.590845584869385
    },
    {
      "epoch": 0.42861788617886176,
      "grad_norm": 22.217010498046875,
      "learning_rate": 1e-05,
      "loss": 6.4011,
      "step": 7908
    },
    {
      "epoch": 0.42861788617886176,
      "step": 7908,
      "training_loss": 7.216558933258057
    },
    {
      "epoch": 0.4286720867208672,
      "step": 7909,
      "training_loss": 6.062840461730957
    },
    {
      "epoch": 0.42872628726287265,
      "step": 7910,
      "training_loss": 6.886486053466797
    },
    {
      "epoch": 0.42878048780487804,
      "step": 7911,
      "training_loss": 7.08745813369751
    },
    {
      "epoch": 0.4288346883468835,
      "grad_norm": 29.40376091003418,
      "learning_rate": 1e-05,
      "loss": 6.8133,
      "step": 7912
    },
    {
      "epoch": 0.4288346883468835,
      "step": 7912,
      "training_loss": 6.243493556976318
    },
    {
      "epoch": 0.4288888888888889,
      "step": 7913,
      "training_loss": 8.489448547363281
    },
    {
      "epoch": 0.4289430894308943,
      "step": 7914,
      "training_loss": 7.203141212463379
    },
    {
      "epoch": 0.4289972899728997,
      "step": 7915,
      "training_loss": 7.3962507247924805
    },
    {
      "epoch": 0.42905149051490515,
      "grad_norm": 18.158260345458984,
      "learning_rate": 1e-05,
      "loss": 7.3331,
      "step": 7916
    },
    {
      "epoch": 0.42905149051490515,
      "step": 7916,
      "training_loss": 6.859111309051514
    },
    {
      "epoch": 0.42910569105691054,
      "step": 7917,
      "training_loss": 6.645291328430176
    },
    {
      "epoch": 0.429159891598916,
      "step": 7918,
      "training_loss": 7.990327835083008
    },
    {
      "epoch": 0.42921409214092143,
      "step": 7919,
      "training_loss": 7.8530802726745605
    },
    {
      "epoch": 0.4292682926829268,
      "grad_norm": 35.186649322509766,
      "learning_rate": 1e-05,
      "loss": 7.337,
      "step": 7920
    },
    {
      "epoch": 0.4292682926829268,
      "step": 7920,
      "training_loss": 7.138495922088623
    },
    {
      "epoch": 0.42932249322493227,
      "step": 7921,
      "training_loss": 7.996737957000732
    },
    {
      "epoch": 0.42937669376693766,
      "step": 7922,
      "training_loss": 6.548360347747803
    },
    {
      "epoch": 0.4294308943089431,
      "step": 7923,
      "training_loss": 6.094964027404785
    },
    {
      "epoch": 0.4294850948509485,
      "grad_norm": 33.6556510925293,
      "learning_rate": 1e-05,
      "loss": 6.9446,
      "step": 7924
    },
    {
      "epoch": 0.4294850948509485,
      "step": 7924,
      "training_loss": 6.421669960021973
    },
    {
      "epoch": 0.42953929539295393,
      "step": 7925,
      "training_loss": 10.902554512023926
    },
    {
      "epoch": 0.4295934959349593,
      "step": 7926,
      "training_loss": 6.148611545562744
    },
    {
      "epoch": 0.42964769647696477,
      "step": 7927,
      "training_loss": 6.972262382507324
    },
    {
      "epoch": 0.4297018970189702,
      "grad_norm": 20.820514678955078,
      "learning_rate": 1e-05,
      "loss": 7.6113,
      "step": 7928
    },
    {
      "epoch": 0.4297018970189702,
      "step": 7928,
      "training_loss": 7.231574535369873
    },
    {
      "epoch": 0.4297560975609756,
      "step": 7929,
      "training_loss": 6.92225980758667
    },
    {
      "epoch": 0.42981029810298105,
      "step": 7930,
      "training_loss": 4.361027717590332
    },
    {
      "epoch": 0.42986449864498644,
      "step": 7931,
      "training_loss": 8.107158660888672
    },
    {
      "epoch": 0.4299186991869919,
      "grad_norm": 44.94450378417969,
      "learning_rate": 1e-05,
      "loss": 6.6555,
      "step": 7932
    },
    {
      "epoch": 0.4299186991869919,
      "step": 7932,
      "training_loss": 6.405442237854004
    },
    {
      "epoch": 0.42997289972899727,
      "step": 7933,
      "training_loss": 6.563603401184082
    },
    {
      "epoch": 0.4300271002710027,
      "step": 7934,
      "training_loss": 4.243002414703369
    },
    {
      "epoch": 0.4300813008130081,
      "step": 7935,
      "training_loss": 5.330835819244385
    },
    {
      "epoch": 0.43013550135501355,
      "grad_norm": 66.8744888305664,
      "learning_rate": 1e-05,
      "loss": 5.6357,
      "step": 7936
    },
    {
      "epoch": 0.43013550135501355,
      "step": 7936,
      "training_loss": 6.758170127868652
    },
    {
      "epoch": 0.430189701897019,
      "step": 7937,
      "training_loss": 5.99799919128418
    },
    {
      "epoch": 0.4302439024390244,
      "step": 7938,
      "training_loss": 7.698978900909424
    },
    {
      "epoch": 0.43029810298102983,
      "step": 7939,
      "training_loss": 7.033547401428223
    },
    {
      "epoch": 0.4303523035230352,
      "grad_norm": 43.1019287109375,
      "learning_rate": 1e-05,
      "loss": 6.8722,
      "step": 7940
    },
    {
      "epoch": 0.4303523035230352,
      "step": 7940,
      "training_loss": 7.562690734863281
    },
    {
      "epoch": 0.43040650406504066,
      "step": 7941,
      "training_loss": 7.807316303253174
    },
    {
      "epoch": 0.43046070460704605,
      "step": 7942,
      "training_loss": 6.903937816619873
    },
    {
      "epoch": 0.4305149051490515,
      "step": 7943,
      "training_loss": 7.120881080627441
    },
    {
      "epoch": 0.4305691056910569,
      "grad_norm": 51.586753845214844,
      "learning_rate": 1e-05,
      "loss": 7.3487,
      "step": 7944
    },
    {
      "epoch": 0.4305691056910569,
      "step": 7944,
      "training_loss": 7.627832412719727
    },
    {
      "epoch": 0.43062330623306233,
      "step": 7945,
      "training_loss": 7.282935619354248
    },
    {
      "epoch": 0.4306775067750678,
      "step": 7946,
      "training_loss": 5.468019485473633
    },
    {
      "epoch": 0.43073170731707316,
      "step": 7947,
      "training_loss": 7.221094131469727
    },
    {
      "epoch": 0.4307859078590786,
      "grad_norm": 39.89152526855469,
      "learning_rate": 1e-05,
      "loss": 6.9,
      "step": 7948
    },
    {
      "epoch": 0.4307859078590786,
      "step": 7948,
      "training_loss": 4.248379707336426
    },
    {
      "epoch": 0.430840108401084,
      "step": 7949,
      "training_loss": 4.469666957855225
    },
    {
      "epoch": 0.43089430894308944,
      "step": 7950,
      "training_loss": 6.927561283111572
    },
    {
      "epoch": 0.43094850948509483,
      "step": 7951,
      "training_loss": 5.923295497894287
    },
    {
      "epoch": 0.4310027100271003,
      "grad_norm": 31.073457717895508,
      "learning_rate": 1e-05,
      "loss": 5.3922,
      "step": 7952
    },
    {
      "epoch": 0.4310027100271003,
      "step": 7952,
      "training_loss": 6.5247297286987305
    },
    {
      "epoch": 0.43105691056910567,
      "step": 7953,
      "training_loss": 8.831812858581543
    },
    {
      "epoch": 0.4311111111111111,
      "step": 7954,
      "training_loss": 6.306443214416504
    },
    {
      "epoch": 0.43116531165311656,
      "step": 7955,
      "training_loss": 7.538034439086914
    },
    {
      "epoch": 0.43121951219512195,
      "grad_norm": 20.317026138305664,
      "learning_rate": 1e-05,
      "loss": 7.3003,
      "step": 7956
    },
    {
      "epoch": 0.43121951219512195,
      "step": 7956,
      "training_loss": 6.213636875152588
    },
    {
      "epoch": 0.4312737127371274,
      "step": 7957,
      "training_loss": 4.86830997467041
    },
    {
      "epoch": 0.4313279132791328,
      "step": 7958,
      "training_loss": 6.714923858642578
    },
    {
      "epoch": 0.4313821138211382,
      "step": 7959,
      "training_loss": 6.665684700012207
    },
    {
      "epoch": 0.4314363143631436,
      "grad_norm": 28.620092391967773,
      "learning_rate": 1e-05,
      "loss": 6.1156,
      "step": 7960
    },
    {
      "epoch": 0.4314363143631436,
      "step": 7960,
      "training_loss": 7.437958717346191
    },
    {
      "epoch": 0.43149051490514906,
      "step": 7961,
      "training_loss": 5.095729351043701
    },
    {
      "epoch": 0.43154471544715445,
      "step": 7962,
      "training_loss": 7.9966206550598145
    },
    {
      "epoch": 0.4315989159891599,
      "step": 7963,
      "training_loss": 4.688431262969971
    },
    {
      "epoch": 0.43165311653116534,
      "grad_norm": 27.08428192138672,
      "learning_rate": 1e-05,
      "loss": 6.3047,
      "step": 7964
    },
    {
      "epoch": 0.43165311653116534,
      "step": 7964,
      "training_loss": 6.895338535308838
    },
    {
      "epoch": 0.4317073170731707,
      "step": 7965,
      "training_loss": 5.416995525360107
    },
    {
      "epoch": 0.43176151761517617,
      "step": 7966,
      "training_loss": 5.734230041503906
    },
    {
      "epoch": 0.43181571815718156,
      "step": 7967,
      "training_loss": 5.033278942108154
    },
    {
      "epoch": 0.431869918699187,
      "grad_norm": 20.754316329956055,
      "learning_rate": 1e-05,
      "loss": 5.77,
      "step": 7968
    },
    {
      "epoch": 0.431869918699187,
      "step": 7968,
      "training_loss": 6.681022644042969
    },
    {
      "epoch": 0.4319241192411924,
      "step": 7969,
      "training_loss": 7.865976333618164
    },
    {
      "epoch": 0.43197831978319784,
      "step": 7970,
      "training_loss": 6.387457847595215
    },
    {
      "epoch": 0.43203252032520323,
      "step": 7971,
      "training_loss": 2.791217565536499
    },
    {
      "epoch": 0.4320867208672087,
      "grad_norm": 42.58533477783203,
      "learning_rate": 1e-05,
      "loss": 5.9314,
      "step": 7972
    },
    {
      "epoch": 0.4320867208672087,
      "step": 7972,
      "training_loss": 6.121706962585449
    },
    {
      "epoch": 0.4321409214092141,
      "step": 7973,
      "training_loss": 6.674696922302246
    },
    {
      "epoch": 0.4321951219512195,
      "step": 7974,
      "training_loss": 6.191391944885254
    },
    {
      "epoch": 0.43224932249322495,
      "step": 7975,
      "training_loss": 7.826810359954834
    },
    {
      "epoch": 0.43230352303523034,
      "grad_norm": 39.68452835083008,
      "learning_rate": 1e-05,
      "loss": 6.7037,
      "step": 7976
    },
    {
      "epoch": 0.43230352303523034,
      "step": 7976,
      "training_loss": 6.7538323402404785
    },
    {
      "epoch": 0.4323577235772358,
      "step": 7977,
      "training_loss": 6.157514572143555
    },
    {
      "epoch": 0.4324119241192412,
      "step": 7978,
      "training_loss": 5.959238529205322
    },
    {
      "epoch": 0.4324661246612466,
      "step": 7979,
      "training_loss": 6.996457576751709
    },
    {
      "epoch": 0.432520325203252,
      "grad_norm": 22.432294845581055,
      "learning_rate": 1e-05,
      "loss": 6.4668,
      "step": 7980
    },
    {
      "epoch": 0.432520325203252,
      "step": 7980,
      "training_loss": 6.992592811584473
    },
    {
      "epoch": 0.43257452574525745,
      "step": 7981,
      "training_loss": 7.517174243927002
    },
    {
      "epoch": 0.4326287262872629,
      "step": 7982,
      "training_loss": 6.266642093658447
    },
    {
      "epoch": 0.4326829268292683,
      "step": 7983,
      "training_loss": 7.073917388916016
    },
    {
      "epoch": 0.43273712737127373,
      "grad_norm": 22.27263069152832,
      "learning_rate": 1e-05,
      "loss": 6.9626,
      "step": 7984
    },
    {
      "epoch": 0.43273712737127373,
      "step": 7984,
      "training_loss": 7.249133110046387
    },
    {
      "epoch": 0.4327913279132791,
      "step": 7985,
      "training_loss": 5.01076078414917
    },
    {
      "epoch": 0.43284552845528457,
      "step": 7986,
      "training_loss": 5.503969669342041
    },
    {
      "epoch": 0.43289972899728996,
      "step": 7987,
      "training_loss": 6.74653959274292
    },
    {
      "epoch": 0.4329539295392954,
      "grad_norm": 25.746185302734375,
      "learning_rate": 1e-05,
      "loss": 6.1276,
      "step": 7988
    },
    {
      "epoch": 0.4329539295392954,
      "step": 7988,
      "training_loss": 4.085809230804443
    },
    {
      "epoch": 0.4330081300813008,
      "step": 7989,
      "training_loss": 7.237697601318359
    },
    {
      "epoch": 0.43306233062330624,
      "step": 7990,
      "training_loss": 7.2290778160095215
    },
    {
      "epoch": 0.4331165311653117,
      "step": 7991,
      "training_loss": 8.123595237731934
    },
    {
      "epoch": 0.43317073170731707,
      "grad_norm": 33.10724639892578,
      "learning_rate": 1e-05,
      "loss": 6.669,
      "step": 7992
    },
    {
      "epoch": 0.43317073170731707,
      "step": 7992,
      "training_loss": 7.207988739013672
    },
    {
      "epoch": 0.4332249322493225,
      "step": 7993,
      "training_loss": 6.079499244689941
    },
    {
      "epoch": 0.4332791327913279,
      "step": 7994,
      "training_loss": 6.798069477081299
    },
    {
      "epoch": 0.43333333333333335,
      "step": 7995,
      "training_loss": 4.722879886627197
    },
    {
      "epoch": 0.43338753387533874,
      "grad_norm": 42.41131591796875,
      "learning_rate": 1e-05,
      "loss": 6.2021,
      "step": 7996
    },
    {
      "epoch": 0.43338753387533874,
      "step": 7996,
      "training_loss": 7.1649298667907715
    },
    {
      "epoch": 0.4334417344173442,
      "step": 7997,
      "training_loss": 6.499204158782959
    },
    {
      "epoch": 0.43349593495934957,
      "step": 7998,
      "training_loss": 4.678892135620117
    },
    {
      "epoch": 0.433550135501355,
      "step": 7999,
      "training_loss": 7.8891801834106445
    },
    {
      "epoch": 0.43360433604336046,
      "grad_norm": 18.75175666809082,
      "learning_rate": 1e-05,
      "loss": 6.5581,
      "step": 8000
    },
    {
      "epoch": 0.43360433604336046,
      "step": 8000,
      "training_loss": 6.491148471832275
    },
    {
      "epoch": 0.43365853658536585,
      "step": 8001,
      "training_loss": 6.857872486114502
    },
    {
      "epoch": 0.4337127371273713,
      "step": 8002,
      "training_loss": 8.746089935302734
    },
    {
      "epoch": 0.4337669376693767,
      "step": 8003,
      "training_loss": 7.354485988616943
    },
    {
      "epoch": 0.43382113821138213,
      "grad_norm": 25.85138511657715,
      "learning_rate": 1e-05,
      "loss": 7.3624,
      "step": 8004
    },
    {
      "epoch": 0.43382113821138213,
      "step": 8004,
      "training_loss": 5.944742679595947
    },
    {
      "epoch": 0.4338753387533875,
      "step": 8005,
      "training_loss": 7.313472270965576
    },
    {
      "epoch": 0.43392953929539296,
      "step": 8006,
      "training_loss": 7.8097333908081055
    },
    {
      "epoch": 0.43398373983739835,
      "step": 8007,
      "training_loss": 7.2130513191223145
    },
    {
      "epoch": 0.4340379403794038,
      "grad_norm": 31.135974884033203,
      "learning_rate": 1e-05,
      "loss": 7.0702,
      "step": 8008
    },
    {
      "epoch": 0.4340379403794038,
      "step": 8008,
      "training_loss": 6.919013977050781
    },
    {
      "epoch": 0.4340921409214092,
      "step": 8009,
      "training_loss": 6.9677734375
    },
    {
      "epoch": 0.43414634146341463,
      "step": 8010,
      "training_loss": 7.121743202209473
    },
    {
      "epoch": 0.4342005420054201,
      "step": 8011,
      "training_loss": 5.0575361251831055
    },
    {
      "epoch": 0.43425474254742547,
      "grad_norm": 36.47465515136719,
      "learning_rate": 1e-05,
      "loss": 6.5165,
      "step": 8012
    },
    {
      "epoch": 0.43425474254742547,
      "step": 8012,
      "training_loss": 6.952902793884277
    },
    {
      "epoch": 0.4343089430894309,
      "step": 8013,
      "training_loss": 6.135051250457764
    },
    {
      "epoch": 0.4343631436314363,
      "step": 8014,
      "training_loss": 6.774745464324951
    },
    {
      "epoch": 0.43441734417344174,
      "step": 8015,
      "training_loss": 6.561902046203613
    },
    {
      "epoch": 0.43447154471544713,
      "grad_norm": 21.59077262878418,
      "learning_rate": 1e-05,
      "loss": 6.6062,
      "step": 8016
    },
    {
      "epoch": 0.43447154471544713,
      "step": 8016,
      "training_loss": 5.320347309112549
    },
    {
      "epoch": 0.4345257452574526,
      "step": 8017,
      "training_loss": 7.04622220993042
    },
    {
      "epoch": 0.43457994579945797,
      "step": 8018,
      "training_loss": 6.629886627197266
    },
    {
      "epoch": 0.4346341463414634,
      "step": 8019,
      "training_loss": 7.459859848022461
    },
    {
      "epoch": 0.43468834688346886,
      "grad_norm": 29.126413345336914,
      "learning_rate": 1e-05,
      "loss": 6.6141,
      "step": 8020
    },
    {
      "epoch": 0.43468834688346886,
      "step": 8020,
      "training_loss": 5.973500728607178
    },
    {
      "epoch": 0.43474254742547425,
      "step": 8021,
      "training_loss": 7.839420318603516
    },
    {
      "epoch": 0.4347967479674797,
      "step": 8022,
      "training_loss": 5.878750324249268
    },
    {
      "epoch": 0.4348509485094851,
      "step": 8023,
      "training_loss": 6.197791576385498
    },
    {
      "epoch": 0.4349051490514905,
      "grad_norm": 22.514442443847656,
      "learning_rate": 1e-05,
      "loss": 6.4724,
      "step": 8024
    },
    {
      "epoch": 0.4349051490514905,
      "step": 8024,
      "training_loss": 6.760502815246582
    },
    {
      "epoch": 0.4349593495934959,
      "step": 8025,
      "training_loss": 5.593503952026367
    },
    {
      "epoch": 0.43501355013550136,
      "step": 8026,
      "training_loss": 6.654840469360352
    },
    {
      "epoch": 0.43506775067750675,
      "step": 8027,
      "training_loss": 6.4170989990234375
    },
    {
      "epoch": 0.4351219512195122,
      "grad_norm": 26.443241119384766,
      "learning_rate": 1e-05,
      "loss": 6.3565,
      "step": 8028
    },
    {
      "epoch": 0.4351219512195122,
      "step": 8028,
      "training_loss": 6.9142560958862305
    },
    {
      "epoch": 0.43517615176151764,
      "step": 8029,
      "training_loss": 6.963478088378906
    },
    {
      "epoch": 0.435230352303523,
      "step": 8030,
      "training_loss": 7.527954578399658
    },
    {
      "epoch": 0.43528455284552847,
      "step": 8031,
      "training_loss": 6.708647727966309
    },
    {
      "epoch": 0.43533875338753386,
      "grad_norm": 52.63113784790039,
      "learning_rate": 1e-05,
      "loss": 7.0286,
      "step": 8032
    },
    {
      "epoch": 0.43533875338753386,
      "step": 8032,
      "training_loss": 5.956773281097412
    },
    {
      "epoch": 0.4353929539295393,
      "step": 8033,
      "training_loss": 5.173198223114014
    },
    {
      "epoch": 0.4354471544715447,
      "step": 8034,
      "training_loss": 6.864476203918457
    },
    {
      "epoch": 0.43550135501355014,
      "step": 8035,
      "training_loss": 4.877102375030518
    },
    {
      "epoch": 0.43555555555555553,
      "grad_norm": 23.433223724365234,
      "learning_rate": 1e-05,
      "loss": 5.7179,
      "step": 8036
    },
    {
      "epoch": 0.43555555555555553,
      "step": 8036,
      "training_loss": 7.472510814666748
    },
    {
      "epoch": 0.435609756097561,
      "step": 8037,
      "training_loss": 7.395421504974365
    },
    {
      "epoch": 0.4356639566395664,
      "step": 8038,
      "training_loss": 5.979814529418945
    },
    {
      "epoch": 0.4357181571815718,
      "step": 8039,
      "training_loss": 5.44082498550415
    },
    {
      "epoch": 0.43577235772357725,
      "grad_norm": 22.260299682617188,
      "learning_rate": 1e-05,
      "loss": 6.5721,
      "step": 8040
    },
    {
      "epoch": 0.43577235772357725,
      "step": 8040,
      "training_loss": 6.331615924835205
    },
    {
      "epoch": 0.43582655826558264,
      "step": 8041,
      "training_loss": 6.457884311676025
    },
    {
      "epoch": 0.4358807588075881,
      "step": 8042,
      "training_loss": 7.436496257781982
    },
    {
      "epoch": 0.4359349593495935,
      "step": 8043,
      "training_loss": 4.397522926330566
    },
    {
      "epoch": 0.4359891598915989,
      "grad_norm": 28.271507263183594,
      "learning_rate": 1e-05,
      "loss": 6.1559,
      "step": 8044
    },
    {
      "epoch": 0.4359891598915989,
      "step": 8044,
      "training_loss": 6.987164497375488
    },
    {
      "epoch": 0.4360433604336043,
      "step": 8045,
      "training_loss": 6.241919040679932
    },
    {
      "epoch": 0.43609756097560975,
      "step": 8046,
      "training_loss": 6.579577445983887
    },
    {
      "epoch": 0.4361517615176152,
      "step": 8047,
      "training_loss": 6.7520036697387695
    },
    {
      "epoch": 0.4362059620596206,
      "grad_norm": 20.480525970458984,
      "learning_rate": 1e-05,
      "loss": 6.6402,
      "step": 8048
    },
    {
      "epoch": 0.4362059620596206,
      "step": 8048,
      "training_loss": 6.558703422546387
    },
    {
      "epoch": 0.43626016260162603,
      "step": 8049,
      "training_loss": 6.347782135009766
    },
    {
      "epoch": 0.4363143631436314,
      "step": 8050,
      "training_loss": 6.666445255279541
    },
    {
      "epoch": 0.43636856368563687,
      "step": 8051,
      "training_loss": 7.52255392074585
    },
    {
      "epoch": 0.43642276422764226,
      "grad_norm": 53.80347442626953,
      "learning_rate": 1e-05,
      "loss": 6.7739,
      "step": 8052
    },
    {
      "epoch": 0.43642276422764226,
      "step": 8052,
      "training_loss": 6.696251392364502
    },
    {
      "epoch": 0.4364769647696477,
      "step": 8053,
      "training_loss": 7.723170280456543
    },
    {
      "epoch": 0.4365311653116531,
      "step": 8054,
      "training_loss": 8.150884628295898
    },
    {
      "epoch": 0.43658536585365854,
      "step": 8055,
      "training_loss": 4.972743988037109
    },
    {
      "epoch": 0.436639566395664,
      "grad_norm": 40.29854965209961,
      "learning_rate": 1e-05,
      "loss": 6.8858,
      "step": 8056
    },
    {
      "epoch": 0.436639566395664,
      "step": 8056,
      "training_loss": 8.055663108825684
    },
    {
      "epoch": 0.43669376693766937,
      "step": 8057,
      "training_loss": 6.5953779220581055
    },
    {
      "epoch": 0.4367479674796748,
      "step": 8058,
      "training_loss": 6.466728687286377
    },
    {
      "epoch": 0.4368021680216802,
      "step": 8059,
      "training_loss": 8.05805778503418
    },
    {
      "epoch": 0.43685636856368565,
      "grad_norm": 56.46544647216797,
      "learning_rate": 1e-05,
      "loss": 7.294,
      "step": 8060
    },
    {
      "epoch": 0.43685636856368565,
      "step": 8060,
      "training_loss": 6.653526306152344
    },
    {
      "epoch": 0.43691056910569104,
      "step": 8061,
      "training_loss": 7.404813289642334
    },
    {
      "epoch": 0.4369647696476965,
      "step": 8062,
      "training_loss": 7.352372646331787
    },
    {
      "epoch": 0.43701897018970187,
      "step": 8063,
      "training_loss": 6.006574630737305
    },
    {
      "epoch": 0.4370731707317073,
      "grad_norm": 44.22993087768555,
      "learning_rate": 1e-05,
      "loss": 6.8543,
      "step": 8064
    },
    {
      "epoch": 0.4370731707317073,
      "step": 8064,
      "training_loss": 4.831226348876953
    },
    {
      "epoch": 0.43712737127371276,
      "step": 8065,
      "training_loss": 6.399166107177734
    },
    {
      "epoch": 0.43718157181571815,
      "step": 8066,
      "training_loss": 6.972890853881836
    },
    {
      "epoch": 0.4372357723577236,
      "step": 8067,
      "training_loss": 7.203618049621582
    },
    {
      "epoch": 0.437289972899729,
      "grad_norm": 29.057003021240234,
      "learning_rate": 1e-05,
      "loss": 6.3517,
      "step": 8068
    },
    {
      "epoch": 0.437289972899729,
      "step": 8068,
      "training_loss": 7.344972133636475
    },
    {
      "epoch": 0.43734417344173443,
      "step": 8069,
      "training_loss": 3.8141400814056396
    },
    {
      "epoch": 0.4373983739837398,
      "step": 8070,
      "training_loss": 7.113799571990967
    },
    {
      "epoch": 0.43745257452574526,
      "step": 8071,
      "training_loss": 7.195051193237305
    },
    {
      "epoch": 0.43750677506775065,
      "grad_norm": 15.21441650390625,
      "learning_rate": 1e-05,
      "loss": 6.367,
      "step": 8072
    },
    {
      "epoch": 0.43750677506775065,
      "step": 8072,
      "training_loss": 6.561731338500977
    },
    {
      "epoch": 0.4375609756097561,
      "step": 8073,
      "training_loss": 7.0827836990356445
    },
    {
      "epoch": 0.43761517615176154,
      "step": 8074,
      "training_loss": 6.698850631713867
    },
    {
      "epoch": 0.43766937669376693,
      "step": 8075,
      "training_loss": 7.628745079040527
    },
    {
      "epoch": 0.4377235772357724,
      "grad_norm": 46.076385498046875,
      "learning_rate": 1e-05,
      "loss": 6.993,
      "step": 8076
    },
    {
      "epoch": 0.4377235772357724,
      "step": 8076,
      "training_loss": 7.619403839111328
    },
    {
      "epoch": 0.43777777777777777,
      "step": 8077,
      "training_loss": 6.043525695800781
    },
    {
      "epoch": 0.4378319783197832,
      "step": 8078,
      "training_loss": 5.889252662658691
    },
    {
      "epoch": 0.4378861788617886,
      "step": 8079,
      "training_loss": 6.90635347366333
    },
    {
      "epoch": 0.43794037940379404,
      "grad_norm": 33.11886978149414,
      "learning_rate": 1e-05,
      "loss": 6.6146,
      "step": 8080
    },
    {
      "epoch": 0.43794037940379404,
      "step": 8080,
      "training_loss": 6.879099369049072
    },
    {
      "epoch": 0.43799457994579943,
      "step": 8081,
      "training_loss": 6.914022445678711
    },
    {
      "epoch": 0.4380487804878049,
      "step": 8082,
      "training_loss": 7.30277156829834
    },
    {
      "epoch": 0.4381029810298103,
      "step": 8083,
      "training_loss": 3.15483021736145
    },
    {
      "epoch": 0.4381571815718157,
      "grad_norm": 48.24003982543945,
      "learning_rate": 1e-05,
      "loss": 6.0627,
      "step": 8084
    },
    {
      "epoch": 0.4381571815718157,
      "step": 8084,
      "training_loss": 7.098682403564453
    },
    {
      "epoch": 0.43821138211382116,
      "step": 8085,
      "training_loss": 7.360342979431152
    },
    {
      "epoch": 0.43826558265582655,
      "step": 8086,
      "training_loss": 6.93558406829834
    },
    {
      "epoch": 0.438319783197832,
      "step": 8087,
      "training_loss": 7.0218048095703125
    },
    {
      "epoch": 0.4383739837398374,
      "grad_norm": 25.111650466918945,
      "learning_rate": 1e-05,
      "loss": 7.1041,
      "step": 8088
    },
    {
      "epoch": 0.4383739837398374,
      "step": 8088,
      "training_loss": 6.314109802246094
    },
    {
      "epoch": 0.4384281842818428,
      "step": 8089,
      "training_loss": 6.847649097442627
    },
    {
      "epoch": 0.4384823848238482,
      "step": 8090,
      "training_loss": 7.315694332122803
    },
    {
      "epoch": 0.43853658536585366,
      "step": 8091,
      "training_loss": 7.1537346839904785
    },
    {
      "epoch": 0.4385907859078591,
      "grad_norm": 25.711997985839844,
      "learning_rate": 1e-05,
      "loss": 6.9078,
      "step": 8092
    },
    {
      "epoch": 0.4385907859078591,
      "step": 8092,
      "training_loss": 7.03372859954834
    },
    {
      "epoch": 0.4386449864498645,
      "step": 8093,
      "training_loss": 7.475039958953857
    },
    {
      "epoch": 0.43869918699186994,
      "step": 8094,
      "training_loss": 4.425059795379639
    },
    {
      "epoch": 0.4387533875338753,
      "step": 8095,
      "training_loss": 7.222886562347412
    },
    {
      "epoch": 0.4388075880758808,
      "grad_norm": 20.799072265625,
      "learning_rate": 1e-05,
      "loss": 6.5392,
      "step": 8096
    },
    {
      "epoch": 0.4388075880758808,
      "step": 8096,
      "training_loss": 6.204232692718506
    },
    {
      "epoch": 0.43886178861788616,
      "step": 8097,
      "training_loss": 6.747182846069336
    },
    {
      "epoch": 0.4389159891598916,
      "step": 8098,
      "training_loss": 7.314514636993408
    },
    {
      "epoch": 0.438970189701897,
      "step": 8099,
      "training_loss": 7.91782808303833
    },
    {
      "epoch": 0.43902439024390244,
      "grad_norm": 49.8494758605957,
      "learning_rate": 1e-05,
      "loss": 7.0459,
      "step": 8100
    },
    {
      "epoch": 0.43902439024390244,
      "step": 8100,
      "training_loss": 6.443227767944336
    },
    {
      "epoch": 0.4390785907859079,
      "step": 8101,
      "training_loss": 6.3512959480285645
    },
    {
      "epoch": 0.4391327913279133,
      "step": 8102,
      "training_loss": 6.7849650382995605
    },
    {
      "epoch": 0.4391869918699187,
      "step": 8103,
      "training_loss": 7.086388111114502
    },
    {
      "epoch": 0.4392411924119241,
      "grad_norm": 28.797439575195312,
      "learning_rate": 1e-05,
      "loss": 6.6665,
      "step": 8104
    },
    {
      "epoch": 0.4392411924119241,
      "step": 8104,
      "training_loss": 7.197363376617432
    },
    {
      "epoch": 0.43929539295392955,
      "step": 8105,
      "training_loss": 6.200630187988281
    },
    {
      "epoch": 0.43934959349593494,
      "step": 8106,
      "training_loss": 8.825495719909668
    },
    {
      "epoch": 0.4394037940379404,
      "step": 8107,
      "training_loss": 5.867044448852539
    },
    {
      "epoch": 0.4394579945799458,
      "grad_norm": 39.245113372802734,
      "learning_rate": 1e-05,
      "loss": 7.0226,
      "step": 8108
    },
    {
      "epoch": 0.4394579945799458,
      "step": 8108,
      "training_loss": 6.907960414886475
    },
    {
      "epoch": 0.4395121951219512,
      "step": 8109,
      "training_loss": 6.875208854675293
    },
    {
      "epoch": 0.43956639566395667,
      "step": 8110,
      "training_loss": 6.869296073913574
    },
    {
      "epoch": 0.43962059620596206,
      "step": 8111,
      "training_loss": 6.540830612182617
    },
    {
      "epoch": 0.4396747967479675,
      "grad_norm": 19.95665168762207,
      "learning_rate": 1e-05,
      "loss": 6.7983,
      "step": 8112
    },
    {
      "epoch": 0.4396747967479675,
      "step": 8112,
      "training_loss": 6.472819805145264
    },
    {
      "epoch": 0.4397289972899729,
      "step": 8113,
      "training_loss": 6.584656238555908
    },
    {
      "epoch": 0.43978319783197833,
      "step": 8114,
      "training_loss": 6.223148822784424
    },
    {
      "epoch": 0.4398373983739837,
      "step": 8115,
      "training_loss": 6.822268486022949
    },
    {
      "epoch": 0.43989159891598917,
      "grad_norm": 28.752727508544922,
      "learning_rate": 1e-05,
      "loss": 6.5257,
      "step": 8116
    },
    {
      "epoch": 0.43989159891598917,
      "step": 8116,
      "training_loss": 4.676452159881592
    },
    {
      "epoch": 0.43994579945799456,
      "step": 8117,
      "training_loss": 7.168167591094971
    },
    {
      "epoch": 0.44,
      "step": 8118,
      "training_loss": 7.390233516693115
    },
    {
      "epoch": 0.44005420054200545,
      "step": 8119,
      "training_loss": 6.480104923248291
    },
    {
      "epoch": 0.44010840108401084,
      "grad_norm": 27.44974136352539,
      "learning_rate": 1e-05,
      "loss": 6.4287,
      "step": 8120
    },
    {
      "epoch": 0.44010840108401084,
      "step": 8120,
      "training_loss": 7.3272929191589355
    },
    {
      "epoch": 0.4401626016260163,
      "step": 8121,
      "training_loss": 7.769532203674316
    },
    {
      "epoch": 0.44021680216802167,
      "step": 8122,
      "training_loss": 7.237548828125
    },
    {
      "epoch": 0.4402710027100271,
      "step": 8123,
      "training_loss": 6.784505844116211
    },
    {
      "epoch": 0.4403252032520325,
      "grad_norm": 29.170581817626953,
      "learning_rate": 1e-05,
      "loss": 7.2797,
      "step": 8124
    },
    {
      "epoch": 0.4403252032520325,
      "step": 8124,
      "training_loss": 6.659965515136719
    },
    {
      "epoch": 0.44037940379403795,
      "step": 8125,
      "training_loss": 6.068385601043701
    },
    {
      "epoch": 0.44043360433604334,
      "step": 8126,
      "training_loss": 6.036505222320557
    },
    {
      "epoch": 0.4404878048780488,
      "step": 8127,
      "training_loss": 6.8506059646606445
    },
    {
      "epoch": 0.44054200542005423,
      "grad_norm": 15.526226997375488,
      "learning_rate": 1e-05,
      "loss": 6.4039,
      "step": 8128
    },
    {
      "epoch": 0.44054200542005423,
      "step": 8128,
      "training_loss": 7.306703567504883
    },
    {
      "epoch": 0.4405962059620596,
      "step": 8129,
      "training_loss": 6.8415303230285645
    },
    {
      "epoch": 0.44065040650406506,
      "step": 8130,
      "training_loss": 5.363858222961426
    },
    {
      "epoch": 0.44070460704607045,
      "step": 8131,
      "training_loss": 7.655520439147949
    },
    {
      "epoch": 0.4407588075880759,
      "grad_norm": 48.354251861572266,
      "learning_rate": 1e-05,
      "loss": 6.7919,
      "step": 8132
    },
    {
      "epoch": 0.4407588075880759,
      "step": 8132,
      "training_loss": 7.036459445953369
    },
    {
      "epoch": 0.4408130081300813,
      "step": 8133,
      "training_loss": 6.970671653747559
    },
    {
      "epoch": 0.44086720867208673,
      "step": 8134,
      "training_loss": 6.421081066131592
    },
    {
      "epoch": 0.4409214092140921,
      "step": 8135,
      "training_loss": 6.09926700592041
    },
    {
      "epoch": 0.44097560975609756,
      "grad_norm": 30.214513778686523,
      "learning_rate": 1e-05,
      "loss": 6.6319,
      "step": 8136
    },
    {
      "epoch": 0.44097560975609756,
      "step": 8136,
      "training_loss": 7.180535793304443
    },
    {
      "epoch": 0.44102981029810295,
      "step": 8137,
      "training_loss": 3.787065267562866
    },
    {
      "epoch": 0.4410840108401084,
      "step": 8138,
      "training_loss": 6.655517578125
    },
    {
      "epoch": 0.44113821138211384,
      "step": 8139,
      "training_loss": 6.9928436279296875
    },
    {
      "epoch": 0.44119241192411923,
      "grad_norm": 17.013643264770508,
      "learning_rate": 1e-05,
      "loss": 6.154,
      "step": 8140
    },
    {
      "epoch": 0.44119241192411923,
      "step": 8140,
      "training_loss": 5.6493916511535645
    },
    {
      "epoch": 0.4412466124661247,
      "step": 8141,
      "training_loss": 4.6769514083862305
    },
    {
      "epoch": 0.44130081300813007,
      "step": 8142,
      "training_loss": 7.8416290283203125
    },
    {
      "epoch": 0.4413550135501355,
      "step": 8143,
      "training_loss": 7.771835803985596
    },
    {
      "epoch": 0.4414092140921409,
      "grad_norm": 18.77234649658203,
      "learning_rate": 1e-05,
      "loss": 6.485,
      "step": 8144
    },
    {
      "epoch": 0.4414092140921409,
      "step": 8144,
      "training_loss": 7.553965091705322
    },
    {
      "epoch": 0.44146341463414634,
      "step": 8145,
      "training_loss": 6.578408241271973
    },
    {
      "epoch": 0.44151761517615173,
      "step": 8146,
      "training_loss": 5.737755298614502
    },
    {
      "epoch": 0.4415718157181572,
      "step": 8147,
      "training_loss": 5.589134693145752
    },
    {
      "epoch": 0.4416260162601626,
      "grad_norm": 54.36440658569336,
      "learning_rate": 1e-05,
      "loss": 6.3648,
      "step": 8148
    },
    {
      "epoch": 0.4416260162601626,
      "step": 8148,
      "training_loss": 5.267343997955322
    },
    {
      "epoch": 0.441680216802168,
      "step": 8149,
      "training_loss": 6.932278633117676
    },
    {
      "epoch": 0.44173441734417346,
      "step": 8150,
      "training_loss": 7.254813194274902
    },
    {
      "epoch": 0.44178861788617885,
      "step": 8151,
      "training_loss": 7.013768672943115
    },
    {
      "epoch": 0.4418428184281843,
      "grad_norm": 26.472793579101562,
      "learning_rate": 1e-05,
      "loss": 6.6171,
      "step": 8152
    },
    {
      "epoch": 0.4418428184281843,
      "step": 8152,
      "training_loss": 7.228199005126953
    },
    {
      "epoch": 0.4418970189701897,
      "step": 8153,
      "training_loss": 6.300885200500488
    },
    {
      "epoch": 0.4419512195121951,
      "step": 8154,
      "training_loss": 6.710201740264893
    },
    {
      "epoch": 0.4420054200542005,
      "step": 8155,
      "training_loss": 6.93809700012207
    },
    {
      "epoch": 0.44205962059620596,
      "grad_norm": 24.103425979614258,
      "learning_rate": 1e-05,
      "loss": 6.7943,
      "step": 8156
    },
    {
      "epoch": 0.44205962059620596,
      "step": 8156,
      "training_loss": 6.90388298034668
    },
    {
      "epoch": 0.4421138211382114,
      "step": 8157,
      "training_loss": 7.9935383796691895
    },
    {
      "epoch": 0.4421680216802168,
      "step": 8158,
      "training_loss": 7.669425964355469
    },
    {
      "epoch": 0.44222222222222224,
      "step": 8159,
      "training_loss": 7.1226983070373535
    },
    {
      "epoch": 0.44227642276422763,
      "grad_norm": 17.125736236572266,
      "learning_rate": 1e-05,
      "loss": 7.4224,
      "step": 8160
    },
    {
      "epoch": 0.44227642276422763,
      "step": 8160,
      "training_loss": 4.831562519073486
    },
    {
      "epoch": 0.4423306233062331,
      "step": 8161,
      "training_loss": 6.538742542266846
    },
    {
      "epoch": 0.44238482384823846,
      "step": 8162,
      "training_loss": 7.386579990386963
    },
    {
      "epoch": 0.4424390243902439,
      "step": 8163,
      "training_loss": 7.159160614013672
    },
    {
      "epoch": 0.4424932249322493,
      "grad_norm": 18.467973709106445,
      "learning_rate": 1e-05,
      "loss": 6.479,
      "step": 8164
    },
    {
      "epoch": 0.4424932249322493,
      "step": 8164,
      "training_loss": 6.9222588539123535
    },
    {
      "epoch": 0.44254742547425474,
      "step": 8165,
      "training_loss": 7.142856121063232
    },
    {
      "epoch": 0.4426016260162602,
      "step": 8166,
      "training_loss": 7.582573413848877
    },
    {
      "epoch": 0.4426558265582656,
      "step": 8167,
      "training_loss": 6.911417007446289
    },
    {
      "epoch": 0.442710027100271,
      "grad_norm": 54.247867584228516,
      "learning_rate": 1e-05,
      "loss": 7.1398,
      "step": 8168
    },
    {
      "epoch": 0.442710027100271,
      "step": 8168,
      "training_loss": 6.572088718414307
    },
    {
      "epoch": 0.4427642276422764,
      "step": 8169,
      "training_loss": 6.839517116546631
    },
    {
      "epoch": 0.44281842818428185,
      "step": 8170,
      "training_loss": 7.600141525268555
    },
    {
      "epoch": 0.44287262872628724,
      "step": 8171,
      "training_loss": 7.540291786193848
    },
    {
      "epoch": 0.4429268292682927,
      "grad_norm": 28.57255744934082,
      "learning_rate": 1e-05,
      "loss": 7.138,
      "step": 8172
    },
    {
      "epoch": 0.4429268292682927,
      "step": 8172,
      "training_loss": 6.309783935546875
    },
    {
      "epoch": 0.4429810298102981,
      "step": 8173,
      "training_loss": 5.4573750495910645
    },
    {
      "epoch": 0.4430352303523035,
      "step": 8174,
      "training_loss": 6.5311431884765625
    },
    {
      "epoch": 0.44308943089430897,
      "step": 8175,
      "training_loss": 5.687403202056885
    },
    {
      "epoch": 0.44314363143631436,
      "grad_norm": 24.89129638671875,
      "learning_rate": 1e-05,
      "loss": 5.9964,
      "step": 8176
    },
    {
      "epoch": 0.44314363143631436,
      "step": 8176,
      "training_loss": 7.100203037261963
    },
    {
      "epoch": 0.4431978319783198,
      "step": 8177,
      "training_loss": 8.453295707702637
    },
    {
      "epoch": 0.4432520325203252,
      "step": 8178,
      "training_loss": 7.566751956939697
    },
    {
      "epoch": 0.44330623306233063,
      "step": 8179,
      "training_loss": 7.42940616607666
    },
    {
      "epoch": 0.443360433604336,
      "grad_norm": 21.013362884521484,
      "learning_rate": 1e-05,
      "loss": 7.6374,
      "step": 8180
    },
    {
      "epoch": 0.443360433604336,
      "step": 8180,
      "training_loss": 7.902832984924316
    },
    {
      "epoch": 0.44341463414634147,
      "step": 8181,
      "training_loss": 7.723849296569824
    },
    {
      "epoch": 0.44346883468834686,
      "step": 8182,
      "training_loss": 7.25770378112793
    },
    {
      "epoch": 0.4435230352303523,
      "step": 8183,
      "training_loss": 6.292951583862305
    },
    {
      "epoch": 0.44357723577235775,
      "grad_norm": 18.623764038085938,
      "learning_rate": 1e-05,
      "loss": 7.2943,
      "step": 8184
    },
    {
      "epoch": 0.44357723577235775,
      "step": 8184,
      "training_loss": 7.029231548309326
    },
    {
      "epoch": 0.44363143631436314,
      "step": 8185,
      "training_loss": 6.862909317016602
    },
    {
      "epoch": 0.4436856368563686,
      "step": 8186,
      "training_loss": 7.210009574890137
    },
    {
      "epoch": 0.44373983739837397,
      "step": 8187,
      "training_loss": 5.853819370269775
    },
    {
      "epoch": 0.4437940379403794,
      "grad_norm": 57.440704345703125,
      "learning_rate": 1e-05,
      "loss": 6.739,
      "step": 8188
    },
    {
      "epoch": 0.4437940379403794,
      "step": 8188,
      "training_loss": 6.676497936248779
    },
    {
      "epoch": 0.4438482384823848,
      "step": 8189,
      "training_loss": 7.410459041595459
    },
    {
      "epoch": 0.44390243902439025,
      "step": 8190,
      "training_loss": 7.046413898468018
    },
    {
      "epoch": 0.44395663956639564,
      "step": 8191,
      "training_loss": 6.905586242675781
    },
    {
      "epoch": 0.4440108401084011,
      "grad_norm": 29.175430297851562,
      "learning_rate": 1e-05,
      "loss": 7.0097,
      "step": 8192
    },
    {
      "epoch": 0.4440108401084011,
      "step": 8192,
      "training_loss": 5.767580509185791
    },
    {
      "epoch": 0.44406504065040653,
      "step": 8193,
      "training_loss": 6.354806900024414
    },
    {
      "epoch": 0.4441192411924119,
      "step": 8194,
      "training_loss": 7.549503326416016
    },
    {
      "epoch": 0.44417344173441736,
      "step": 8195,
      "training_loss": 6.023799896240234
    },
    {
      "epoch": 0.44422764227642275,
      "grad_norm": 34.522457122802734,
      "learning_rate": 1e-05,
      "loss": 6.4239,
      "step": 8196
    },
    {
      "epoch": 0.44422764227642275,
      "step": 8196,
      "training_loss": 6.857411861419678
    },
    {
      "epoch": 0.4442818428184282,
      "step": 8197,
      "training_loss": 6.554429054260254
    },
    {
      "epoch": 0.4443360433604336,
      "step": 8198,
      "training_loss": 6.600257396697998
    },
    {
      "epoch": 0.44439024390243903,
      "step": 8199,
      "training_loss": 5.6609320640563965
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 36.98326110839844,
      "learning_rate": 1e-05,
      "loss": 6.4183,
      "step": 8200
    },
    {
      "epoch": 0.4444444444444444,
      "step": 8200,
      "training_loss": 7.102816581726074
    },
    {
      "epoch": 0.44449864498644986,
      "step": 8201,
      "training_loss": 3.1403660774230957
    },
    {
      "epoch": 0.4445528455284553,
      "step": 8202,
      "training_loss": 4.788891792297363
    },
    {
      "epoch": 0.4446070460704607,
      "step": 8203,
      "training_loss": 7.092482566833496
    },
    {
      "epoch": 0.44466124661246614,
      "grad_norm": 20.07781410217285,
      "learning_rate": 1e-05,
      "loss": 5.5311,
      "step": 8204
    },
    {
      "epoch": 0.44466124661246614,
      "step": 8204,
      "training_loss": 5.1446852684021
    },
    {
      "epoch": 0.44471544715447153,
      "step": 8205,
      "training_loss": 6.1360955238342285
    },
    {
      "epoch": 0.444769647696477,
      "step": 8206,
      "training_loss": 6.527866840362549
    },
    {
      "epoch": 0.44482384823848237,
      "step": 8207,
      "training_loss": 7.53373384475708
    },
    {
      "epoch": 0.4448780487804878,
      "grad_norm": 34.486106872558594,
      "learning_rate": 1e-05,
      "loss": 6.3356,
      "step": 8208
    },
    {
      "epoch": 0.4448780487804878,
      "step": 8208,
      "training_loss": 7.392029285430908
    },
    {
      "epoch": 0.4449322493224932,
      "step": 8209,
      "training_loss": 5.3082075119018555
    },
    {
      "epoch": 0.44498644986449865,
      "step": 8210,
      "training_loss": 6.7643232345581055
    },
    {
      "epoch": 0.4450406504065041,
      "step": 8211,
      "training_loss": 7.937789440155029
    },
    {
      "epoch": 0.4450948509485095,
      "grad_norm": 32.49357223510742,
      "learning_rate": 1e-05,
      "loss": 6.8506,
      "step": 8212
    },
    {
      "epoch": 0.4450948509485095,
      "step": 8212,
      "training_loss": 6.789936065673828
    },
    {
      "epoch": 0.4451490514905149,
      "step": 8213,
      "training_loss": 6.307281494140625
    },
    {
      "epoch": 0.4452032520325203,
      "step": 8214,
      "training_loss": 5.377074718475342
    },
    {
      "epoch": 0.44525745257452576,
      "step": 8215,
      "training_loss": 7.269587516784668
    },
    {
      "epoch": 0.44531165311653115,
      "grad_norm": 24.066511154174805,
      "learning_rate": 1e-05,
      "loss": 6.436,
      "step": 8216
    },
    {
      "epoch": 0.44531165311653115,
      "step": 8216,
      "training_loss": 5.982344150543213
    },
    {
      "epoch": 0.4453658536585366,
      "step": 8217,
      "training_loss": 6.782807350158691
    },
    {
      "epoch": 0.445420054200542,
      "step": 8218,
      "training_loss": 6.717530727386475
    },
    {
      "epoch": 0.4454742547425474,
      "step": 8219,
      "training_loss": 6.565794944763184
    },
    {
      "epoch": 0.44552845528455287,
      "grad_norm": 38.848304748535156,
      "learning_rate": 1e-05,
      "loss": 6.5121,
      "step": 8220
    },
    {
      "epoch": 0.44552845528455287,
      "step": 8220,
      "training_loss": 7.041417598724365
    },
    {
      "epoch": 0.44558265582655826,
      "step": 8221,
      "training_loss": 5.972350597381592
    },
    {
      "epoch": 0.4456368563685637,
      "step": 8222,
      "training_loss": 6.938588619232178
    },
    {
      "epoch": 0.4456910569105691,
      "step": 8223,
      "training_loss": 6.389080047607422
    },
    {
      "epoch": 0.44574525745257454,
      "grad_norm": 20.76525115966797,
      "learning_rate": 1e-05,
      "loss": 6.5854,
      "step": 8224
    },
    {
      "epoch": 0.44574525745257454,
      "step": 8224,
      "training_loss": 7.286797046661377
    },
    {
      "epoch": 0.44579945799457993,
      "step": 8225,
      "training_loss": 7.1662397384643555
    },
    {
      "epoch": 0.4458536585365854,
      "step": 8226,
      "training_loss": 4.744441032409668
    },
    {
      "epoch": 0.44590785907859076,
      "step": 8227,
      "training_loss": 7.361050605773926
    },
    {
      "epoch": 0.4459620596205962,
      "grad_norm": 22.57117462158203,
      "learning_rate": 1e-05,
      "loss": 6.6396,
      "step": 8228
    },
    {
      "epoch": 0.4459620596205962,
      "step": 8228,
      "training_loss": 5.827388763427734
    },
    {
      "epoch": 0.44601626016260165,
      "step": 8229,
      "training_loss": 6.5393829345703125
    },
    {
      "epoch": 0.44607046070460704,
      "step": 8230,
      "training_loss": 7.567068099975586
    },
    {
      "epoch": 0.4461246612466125,
      "step": 8231,
      "training_loss": 6.90751838684082
    },
    {
      "epoch": 0.4461788617886179,
      "grad_norm": 17.129789352416992,
      "learning_rate": 1e-05,
      "loss": 6.7103,
      "step": 8232
    },
    {
      "epoch": 0.4461788617886179,
      "step": 8232,
      "training_loss": 5.1618428230285645
    },
    {
      "epoch": 0.4462330623306233,
      "step": 8233,
      "training_loss": 5.606719493865967
    },
    {
      "epoch": 0.4462872628726287,
      "step": 8234,
      "training_loss": 6.816987037658691
    },
    {
      "epoch": 0.44634146341463415,
      "step": 8235,
      "training_loss": 6.330997943878174
    },
    {
      "epoch": 0.44639566395663954,
      "grad_norm": 27.952207565307617,
      "learning_rate": 1e-05,
      "loss": 5.9791,
      "step": 8236
    },
    {
      "epoch": 0.44639566395663954,
      "step": 8236,
      "training_loss": 5.060422897338867
    },
    {
      "epoch": 0.446449864498645,
      "step": 8237,
      "training_loss": 6.887959957122803
    },
    {
      "epoch": 0.44650406504065043,
      "step": 8238,
      "training_loss": 5.841223239898682
    },
    {
      "epoch": 0.4465582655826558,
      "step": 8239,
      "training_loss": 6.646287441253662
    },
    {
      "epoch": 0.44661246612466127,
      "grad_norm": 28.656160354614258,
      "learning_rate": 1e-05,
      "loss": 6.109,
      "step": 8240
    },
    {
      "epoch": 0.44661246612466127,
      "step": 8240,
      "training_loss": 7.319875717163086
    },
    {
      "epoch": 0.44666666666666666,
      "step": 8241,
      "training_loss": 6.489490509033203
    },
    {
      "epoch": 0.4467208672086721,
      "step": 8242,
      "training_loss": 6.87009859085083
    },
    {
      "epoch": 0.4467750677506775,
      "step": 8243,
      "training_loss": 7.0185346603393555
    },
    {
      "epoch": 0.44682926829268294,
      "grad_norm": 25.861936569213867,
      "learning_rate": 1e-05,
      "loss": 6.9245,
      "step": 8244
    },
    {
      "epoch": 0.44682926829268294,
      "step": 8244,
      "training_loss": 6.2918314933776855
    },
    {
      "epoch": 0.4468834688346883,
      "step": 8245,
      "training_loss": 7.287998199462891
    },
    {
      "epoch": 0.44693766937669377,
      "step": 8246,
      "training_loss": 7.5790510177612305
    },
    {
      "epoch": 0.4469918699186992,
      "step": 8247,
      "training_loss": 6.072106838226318
    },
    {
      "epoch": 0.4470460704607046,
      "grad_norm": 24.019615173339844,
      "learning_rate": 1e-05,
      "loss": 6.8077,
      "step": 8248
    },
    {
      "epoch": 0.4470460704607046,
      "step": 8248,
      "training_loss": 5.217564105987549
    },
    {
      "epoch": 0.44710027100271005,
      "step": 8249,
      "training_loss": 5.346512794494629
    },
    {
      "epoch": 0.44715447154471544,
      "step": 8250,
      "training_loss": 7.792701244354248
    },
    {
      "epoch": 0.4472086720867209,
      "step": 8251,
      "training_loss": 5.785630702972412
    },
    {
      "epoch": 0.44726287262872627,
      "grad_norm": 27.7298583984375,
      "learning_rate": 1e-05,
      "loss": 6.0356,
      "step": 8252
    },
    {
      "epoch": 0.44726287262872627,
      "step": 8252,
      "training_loss": 6.708921432495117
    },
    {
      "epoch": 0.4473170731707317,
      "step": 8253,
      "training_loss": 7.645541191101074
    },
    {
      "epoch": 0.4473712737127371,
      "step": 8254,
      "training_loss": 8.149529457092285
    },
    {
      "epoch": 0.44742547425474255,
      "step": 8255,
      "training_loss": 6.516108989715576
    },
    {
      "epoch": 0.447479674796748,
      "grad_norm": 27.343725204467773,
      "learning_rate": 1e-05,
      "loss": 7.255,
      "step": 8256
    },
    {
      "epoch": 0.447479674796748,
      "step": 8256,
      "training_loss": 6.945442199707031
    },
    {
      "epoch": 0.4475338753387534,
      "step": 8257,
      "training_loss": 6.6772661209106445
    },
    {
      "epoch": 0.44758807588075883,
      "step": 8258,
      "training_loss": 3.742710590362549
    },
    {
      "epoch": 0.4476422764227642,
      "step": 8259,
      "training_loss": 7.096062183380127
    },
    {
      "epoch": 0.44769647696476966,
      "grad_norm": 27.297866821289062,
      "learning_rate": 1e-05,
      "loss": 6.1154,
      "step": 8260
    },
    {
      "epoch": 0.44769647696476966,
      "step": 8260,
      "training_loss": 7.160518646240234
    },
    {
      "epoch": 0.44775067750677505,
      "step": 8261,
      "training_loss": 6.232922077178955
    },
    {
      "epoch": 0.4478048780487805,
      "step": 8262,
      "training_loss": 6.908133029937744
    },
    {
      "epoch": 0.4478590785907859,
      "step": 8263,
      "training_loss": 7.013347625732422
    },
    {
      "epoch": 0.44791327913279133,
      "grad_norm": 19.177486419677734,
      "learning_rate": 1e-05,
      "loss": 6.8287,
      "step": 8264
    },
    {
      "epoch": 0.44791327913279133,
      "step": 8264,
      "training_loss": 6.8296051025390625
    },
    {
      "epoch": 0.4479674796747967,
      "step": 8265,
      "training_loss": 7.352970123291016
    },
    {
      "epoch": 0.44802168021680217,
      "step": 8266,
      "training_loss": 5.382256984710693
    },
    {
      "epoch": 0.4480758807588076,
      "step": 8267,
      "training_loss": 7.453564643859863
    },
    {
      "epoch": 0.448130081300813,
      "grad_norm": 27.723094940185547,
      "learning_rate": 1e-05,
      "loss": 6.7546,
      "step": 8268
    },
    {
      "epoch": 0.448130081300813,
      "step": 8268,
      "training_loss": 6.012807846069336
    },
    {
      "epoch": 0.44818428184281844,
      "step": 8269,
      "training_loss": 5.982337474822998
    },
    {
      "epoch": 0.44823848238482383,
      "step": 8270,
      "training_loss": 7.248868465423584
    },
    {
      "epoch": 0.4482926829268293,
      "step": 8271,
      "training_loss": 6.603532791137695
    },
    {
      "epoch": 0.44834688346883467,
      "grad_norm": 20.641427993774414,
      "learning_rate": 1e-05,
      "loss": 6.4619,
      "step": 8272
    },
    {
      "epoch": 0.44834688346883467,
      "step": 8272,
      "training_loss": 7.770524501800537
    },
    {
      "epoch": 0.4484010840108401,
      "step": 8273,
      "training_loss": 6.855900287628174
    },
    {
      "epoch": 0.4484552845528455,
      "step": 8274,
      "training_loss": 7.0307464599609375
    },
    {
      "epoch": 0.44850948509485095,
      "step": 8275,
      "training_loss": 6.790366172790527
    },
    {
      "epoch": 0.4485636856368564,
      "grad_norm": 39.7060432434082,
      "learning_rate": 1e-05,
      "loss": 7.1119,
      "step": 8276
    },
    {
      "epoch": 0.4485636856368564,
      "step": 8276,
      "training_loss": 6.740177631378174
    },
    {
      "epoch": 0.4486178861788618,
      "step": 8277,
      "training_loss": 6.939429759979248
    },
    {
      "epoch": 0.4486720867208672,
      "step": 8278,
      "training_loss": 5.429165363311768
    },
    {
      "epoch": 0.4487262872628726,
      "step": 8279,
      "training_loss": 6.407424449920654
    },
    {
      "epoch": 0.44878048780487806,
      "grad_norm": 25.967195510864258,
      "learning_rate": 1e-05,
      "loss": 6.379,
      "step": 8280
    },
    {
      "epoch": 0.44878048780487806,
      "step": 8280,
      "training_loss": 8.744620323181152
    },
    {
      "epoch": 0.44883468834688345,
      "step": 8281,
      "training_loss": 6.08967399597168
    },
    {
      "epoch": 0.4488888888888889,
      "step": 8282,
      "training_loss": 8.598761558532715
    },
    {
      "epoch": 0.4489430894308943,
      "step": 8283,
      "training_loss": 6.3061676025390625
    },
    {
      "epoch": 0.4489972899728997,
      "grad_norm": 30.03961181640625,
      "learning_rate": 1e-05,
      "loss": 7.4348,
      "step": 8284
    },
    {
      "epoch": 0.4489972899728997,
      "step": 8284,
      "training_loss": 6.428534984588623
    },
    {
      "epoch": 0.44905149051490517,
      "step": 8285,
      "training_loss": 6.940668106079102
    },
    {
      "epoch": 0.44910569105691056,
      "step": 8286,
      "training_loss": 7.069773197174072
    },
    {
      "epoch": 0.449159891598916,
      "step": 8287,
      "training_loss": 6.705940246582031
    },
    {
      "epoch": 0.4492140921409214,
      "grad_norm": 67.25321197509766,
      "learning_rate": 1e-05,
      "loss": 6.7862,
      "step": 8288
    },
    {
      "epoch": 0.4492140921409214,
      "step": 8288,
      "training_loss": 5.420377731323242
    },
    {
      "epoch": 0.44926829268292684,
      "step": 8289,
      "training_loss": 8.372438430786133
    },
    {
      "epoch": 0.44932249322493223,
      "step": 8290,
      "training_loss": 7.55040168762207
    },
    {
      "epoch": 0.4493766937669377,
      "step": 8291,
      "training_loss": 3.9648425579071045
    },
    {
      "epoch": 0.44943089430894306,
      "grad_norm": 29.43997573852539,
      "learning_rate": 1e-05,
      "loss": 6.327,
      "step": 8292
    },
    {
      "epoch": 0.44943089430894306,
      "step": 8292,
      "training_loss": 6.286276817321777
    },
    {
      "epoch": 0.4494850948509485,
      "step": 8293,
      "training_loss": 6.195817947387695
    },
    {
      "epoch": 0.44953929539295395,
      "step": 8294,
      "training_loss": 3.7231767177581787
    },
    {
      "epoch": 0.44959349593495934,
      "step": 8295,
      "training_loss": 7.069536209106445
    },
    {
      "epoch": 0.4496476964769648,
      "grad_norm": 44.9501838684082,
      "learning_rate": 1e-05,
      "loss": 5.8187,
      "step": 8296
    },
    {
      "epoch": 0.4496476964769648,
      "step": 8296,
      "training_loss": 5.568358898162842
    },
    {
      "epoch": 0.4497018970189702,
      "step": 8297,
      "training_loss": 7.187375068664551
    },
    {
      "epoch": 0.4497560975609756,
      "step": 8298,
      "training_loss": 8.846266746520996
    },
    {
      "epoch": 0.449810298102981,
      "step": 8299,
      "training_loss": 6.468688488006592
    },
    {
      "epoch": 0.44986449864498645,
      "grad_norm": 28.8133602142334,
      "learning_rate": 1e-05,
      "loss": 7.0177,
      "step": 8300
    },
    {
      "epoch": 0.44986449864498645,
      "step": 8300,
      "training_loss": 6.543020725250244
    },
    {
      "epoch": 0.44991869918699184,
      "step": 8301,
      "training_loss": 6.140039443969727
    },
    {
      "epoch": 0.4499728997289973,
      "step": 8302,
      "training_loss": 7.160146236419678
    },
    {
      "epoch": 0.45002710027100273,
      "step": 8303,
      "training_loss": 7.515230655670166
    },
    {
      "epoch": 0.4500813008130081,
      "grad_norm": 61.47321701049805,
      "learning_rate": 1e-05,
      "loss": 6.8396,
      "step": 8304
    },
    {
      "epoch": 0.4500813008130081,
      "step": 8304,
      "training_loss": 7.048649787902832
    },
    {
      "epoch": 0.45013550135501357,
      "step": 8305,
      "training_loss": 6.6897053718566895
    },
    {
      "epoch": 0.45018970189701896,
      "step": 8306,
      "training_loss": 6.140384674072266
    },
    {
      "epoch": 0.4502439024390244,
      "step": 8307,
      "training_loss": 6.273830890655518
    },
    {
      "epoch": 0.4502981029810298,
      "grad_norm": 20.982810974121094,
      "learning_rate": 1e-05,
      "loss": 6.5381,
      "step": 8308
    },
    {
      "epoch": 0.4502981029810298,
      "step": 8308,
      "training_loss": 7.121150970458984
    },
    {
      "epoch": 0.45035230352303524,
      "step": 8309,
      "training_loss": 6.366192817687988
    },
    {
      "epoch": 0.4504065040650406,
      "step": 8310,
      "training_loss": 4.83152437210083
    },
    {
      "epoch": 0.45046070460704607,
      "step": 8311,
      "training_loss": 7.691376209259033
    },
    {
      "epoch": 0.4505149051490515,
      "grad_norm": 19.27788543701172,
      "learning_rate": 1e-05,
      "loss": 6.5026,
      "step": 8312
    },
    {
      "epoch": 0.4505149051490515,
      "step": 8312,
      "training_loss": 5.241520881652832
    },
    {
      "epoch": 0.4505691056910569,
      "step": 8313,
      "training_loss": 8.549886703491211
    },
    {
      "epoch": 0.45062330623306235,
      "step": 8314,
      "training_loss": 6.146019458770752
    },
    {
      "epoch": 0.45067750677506774,
      "step": 8315,
      "training_loss": 6.334733009338379
    },
    {
      "epoch": 0.4507317073170732,
      "grad_norm": 34.375980377197266,
      "learning_rate": 1e-05,
      "loss": 6.568,
      "step": 8316
    },
    {
      "epoch": 0.4507317073170732,
      "step": 8316,
      "training_loss": 7.217872619628906
    },
    {
      "epoch": 0.45078590785907857,
      "step": 8317,
      "training_loss": 6.3045525550842285
    },
    {
      "epoch": 0.450840108401084,
      "step": 8318,
      "training_loss": 6.822809219360352
    },
    {
      "epoch": 0.4508943089430894,
      "step": 8319,
      "training_loss": 6.828903675079346
    },
    {
      "epoch": 0.45094850948509485,
      "grad_norm": 23.80942726135254,
      "learning_rate": 1e-05,
      "loss": 6.7935,
      "step": 8320
    },
    {
      "epoch": 0.45094850948509485,
      "step": 8320,
      "training_loss": 6.860592365264893
    },
    {
      "epoch": 0.4510027100271003,
      "step": 8321,
      "training_loss": 7.049927234649658
    },
    {
      "epoch": 0.4510569105691057,
      "step": 8322,
      "training_loss": 6.330461025238037
    },
    {
      "epoch": 0.45111111111111113,
      "step": 8323,
      "training_loss": 6.600176811218262
    },
    {
      "epoch": 0.4511653116531165,
      "grad_norm": 21.501041412353516,
      "learning_rate": 1e-05,
      "loss": 6.7103,
      "step": 8324
    },
    {
      "epoch": 0.4511653116531165,
      "step": 8324,
      "training_loss": 6.9441022872924805
    },
    {
      "epoch": 0.45121951219512196,
      "step": 8325,
      "training_loss": 8.680023193359375
    },
    {
      "epoch": 0.45127371273712735,
      "step": 8326,
      "training_loss": 9.475342750549316
    },
    {
      "epoch": 0.4513279132791328,
      "step": 8327,
      "training_loss": 4.175700664520264
    },
    {
      "epoch": 0.4513821138211382,
      "grad_norm": 41.92763900756836,
      "learning_rate": 1e-05,
      "loss": 7.3188,
      "step": 8328
    },
    {
      "epoch": 0.4513821138211382,
      "step": 8328,
      "training_loss": 7.985508441925049
    },
    {
      "epoch": 0.45143631436314363,
      "step": 8329,
      "training_loss": 5.986815929412842
    },
    {
      "epoch": 0.4514905149051491,
      "step": 8330,
      "training_loss": 8.043913841247559
    },
    {
      "epoch": 0.45154471544715447,
      "step": 8331,
      "training_loss": 7.422543048858643
    },
    {
      "epoch": 0.4515989159891599,
      "grad_norm": 18.03217124938965,
      "learning_rate": 1e-05,
      "loss": 7.3597,
      "step": 8332
    },
    {
      "epoch": 0.4515989159891599,
      "step": 8332,
      "training_loss": 7.7730278968811035
    },
    {
      "epoch": 0.4516531165311653,
      "step": 8333,
      "training_loss": 5.8201680183410645
    },
    {
      "epoch": 0.45170731707317074,
      "step": 8334,
      "training_loss": 5.934267997741699
    },
    {
      "epoch": 0.45176151761517613,
      "step": 8335,
      "training_loss": 6.675990581512451
    },
    {
      "epoch": 0.4518157181571816,
      "grad_norm": 18.760656356811523,
      "learning_rate": 1e-05,
      "loss": 6.5509,
      "step": 8336
    },
    {
      "epoch": 0.4518157181571816,
      "step": 8336,
      "training_loss": 7.863875389099121
    },
    {
      "epoch": 0.45186991869918697,
      "step": 8337,
      "training_loss": 4.683748245239258
    },
    {
      "epoch": 0.4519241192411924,
      "step": 8338,
      "training_loss": 5.736080646514893
    },
    {
      "epoch": 0.45197831978319786,
      "step": 8339,
      "training_loss": 7.63052225112915
    },
    {
      "epoch": 0.45203252032520325,
      "grad_norm": 27.134845733642578,
      "learning_rate": 1e-05,
      "loss": 6.4786,
      "step": 8340
    },
    {
      "epoch": 0.45203252032520325,
      "step": 8340,
      "training_loss": 5.960819721221924
    },
    {
      "epoch": 0.4520867208672087,
      "step": 8341,
      "training_loss": 6.404706001281738
    },
    {
      "epoch": 0.4521409214092141,
      "step": 8342,
      "training_loss": 6.767082214355469
    },
    {
      "epoch": 0.4521951219512195,
      "step": 8343,
      "training_loss": 6.387109279632568
    },
    {
      "epoch": 0.4522493224932249,
      "grad_norm": 27.968244552612305,
      "learning_rate": 1e-05,
      "loss": 6.3799,
      "step": 8344
    },
    {
      "epoch": 0.4522493224932249,
      "step": 8344,
      "training_loss": 7.056804656982422
    },
    {
      "epoch": 0.45230352303523036,
      "step": 8345,
      "training_loss": 6.9597697257995605
    },
    {
      "epoch": 0.45235772357723575,
      "step": 8346,
      "training_loss": 6.462277412414551
    },
    {
      "epoch": 0.4524119241192412,
      "step": 8347,
      "training_loss": 7.472935199737549
    },
    {
      "epoch": 0.45246612466124664,
      "grad_norm": 27.11907386779785,
      "learning_rate": 1e-05,
      "loss": 6.9879,
      "step": 8348
    },
    {
      "epoch": 0.45246612466124664,
      "step": 8348,
      "training_loss": 3.6941616535186768
    },
    {
      "epoch": 0.452520325203252,
      "step": 8349,
      "training_loss": 7.044703006744385
    },
    {
      "epoch": 0.45257452574525747,
      "step": 8350,
      "training_loss": 7.067488193511963
    },
    {
      "epoch": 0.45262872628726286,
      "step": 8351,
      "training_loss": 6.657807350158691
    },
    {
      "epoch": 0.4526829268292683,
      "grad_norm": 27.897432327270508,
      "learning_rate": 1e-05,
      "loss": 6.116,
      "step": 8352
    },
    {
      "epoch": 0.4526829268292683,
      "step": 8352,
      "training_loss": 7.950738906860352
    },
    {
      "epoch": 0.4527371273712737,
      "step": 8353,
      "training_loss": 6.811282157897949
    },
    {
      "epoch": 0.45279132791327914,
      "step": 8354,
      "training_loss": 7.459352016448975
    },
    {
      "epoch": 0.45284552845528453,
      "step": 8355,
      "training_loss": 7.488129615783691
    },
    {
      "epoch": 0.45289972899729,
      "grad_norm": 21.766653060913086,
      "learning_rate": 1e-05,
      "loss": 7.4274,
      "step": 8356
    },
    {
      "epoch": 0.45289972899729,
      "step": 8356,
      "training_loss": 6.761117458343506
    },
    {
      "epoch": 0.4529539295392954,
      "step": 8357,
      "training_loss": 5.52523946762085
    },
    {
      "epoch": 0.4530081300813008,
      "step": 8358,
      "training_loss": 6.109141826629639
    },
    {
      "epoch": 0.45306233062330625,
      "step": 8359,
      "training_loss": 7.901735782623291
    },
    {
      "epoch": 0.45311653116531164,
      "grad_norm": 20.852689743041992,
      "learning_rate": 1e-05,
      "loss": 6.5743,
      "step": 8360
    },
    {
      "epoch": 0.45311653116531164,
      "step": 8360,
      "training_loss": 6.6889119148254395
    },
    {
      "epoch": 0.4531707317073171,
      "step": 8361,
      "training_loss": 6.573680877685547
    },
    {
      "epoch": 0.4532249322493225,
      "step": 8362,
      "training_loss": 7.065075874328613
    },
    {
      "epoch": 0.4532791327913279,
      "step": 8363,
      "training_loss": 4.416660308837891
    },
    {
      "epoch": 0.4533333333333333,
      "grad_norm": 26.6219539642334,
      "learning_rate": 1e-05,
      "loss": 6.1861,
      "step": 8364
    },
    {
      "epoch": 0.4533333333333333,
      "step": 8364,
      "training_loss": 5.010110855102539
    },
    {
      "epoch": 0.45338753387533876,
      "step": 8365,
      "training_loss": 8.336398124694824
    },
    {
      "epoch": 0.4534417344173442,
      "step": 8366,
      "training_loss": 6.635228157043457
    },
    {
      "epoch": 0.4534959349593496,
      "step": 8367,
      "training_loss": 7.008676528930664
    },
    {
      "epoch": 0.45355013550135503,
      "grad_norm": 18.3210506439209,
      "learning_rate": 1e-05,
      "loss": 6.7476,
      "step": 8368
    },
    {
      "epoch": 0.45355013550135503,
      "step": 8368,
      "training_loss": 7.671557903289795
    },
    {
      "epoch": 0.4536043360433604,
      "step": 8369,
      "training_loss": 6.8732686042785645
    },
    {
      "epoch": 0.45365853658536587,
      "step": 8370,
      "training_loss": 6.054126739501953
    },
    {
      "epoch": 0.45371273712737126,
      "step": 8371,
      "training_loss": 5.921885013580322
    },
    {
      "epoch": 0.4537669376693767,
      "grad_norm": 26.038331985473633,
      "learning_rate": 1e-05,
      "loss": 6.6302,
      "step": 8372
    },
    {
      "epoch": 0.4537669376693767,
      "step": 8372,
      "training_loss": 6.3074564933776855
    },
    {
      "epoch": 0.4538211382113821,
      "step": 8373,
      "training_loss": 6.657670974731445
    },
    {
      "epoch": 0.45387533875338754,
      "step": 8374,
      "training_loss": 8.092293739318848
    },
    {
      "epoch": 0.453929539295393,
      "step": 8375,
      "training_loss": 7.491418361663818
    },
    {
      "epoch": 0.45398373983739837,
      "grad_norm": 18.062284469604492,
      "learning_rate": 1e-05,
      "loss": 7.1372,
      "step": 8376
    },
    {
      "epoch": 0.45398373983739837,
      "step": 8376,
      "training_loss": 7.581141948699951
    },
    {
      "epoch": 0.4540379403794038,
      "step": 8377,
      "training_loss": 5.510718822479248
    },
    {
      "epoch": 0.4540921409214092,
      "step": 8378,
      "training_loss": 6.8154754638671875
    },
    {
      "epoch": 0.45414634146341465,
      "step": 8379,
      "training_loss": 6.733699321746826
    },
    {
      "epoch": 0.45420054200542004,
      "grad_norm": 16.075634002685547,
      "learning_rate": 1e-05,
      "loss": 6.6603,
      "step": 8380
    },
    {
      "epoch": 0.45420054200542004,
      "step": 8380,
      "training_loss": 7.565511703491211
    },
    {
      "epoch": 0.4542547425474255,
      "step": 8381,
      "training_loss": 6.113533020019531
    },
    {
      "epoch": 0.45430894308943087,
      "step": 8382,
      "training_loss": 5.811766624450684
    },
    {
      "epoch": 0.4543631436314363,
      "step": 8383,
      "training_loss": 6.566676616668701
    },
    {
      "epoch": 0.45441734417344176,
      "grad_norm": 20.505207061767578,
      "learning_rate": 1e-05,
      "loss": 6.5144,
      "step": 8384
    },
    {
      "epoch": 0.45441734417344176,
      "step": 8384,
      "training_loss": 7.3166375160217285
    },
    {
      "epoch": 0.45447154471544715,
      "step": 8385,
      "training_loss": 5.826606273651123
    },
    {
      "epoch": 0.4545257452574526,
      "step": 8386,
      "training_loss": 6.133037567138672
    },
    {
      "epoch": 0.454579945799458,
      "step": 8387,
      "training_loss": 5.661354064941406
    },
    {
      "epoch": 0.45463414634146343,
      "grad_norm": 32.76298522949219,
      "learning_rate": 1e-05,
      "loss": 6.2344,
      "step": 8388
    },
    {
      "epoch": 0.45463414634146343,
      "step": 8388,
      "training_loss": 6.4790191650390625
    },
    {
      "epoch": 0.4546883468834688,
      "step": 8389,
      "training_loss": 7.182042121887207
    },
    {
      "epoch": 0.45474254742547426,
      "step": 8390,
      "training_loss": 7.720861911773682
    },
    {
      "epoch": 0.45479674796747965,
      "step": 8391,
      "training_loss": 7.728996276855469
    },
    {
      "epoch": 0.4548509485094851,
      "grad_norm": 37.652835845947266,
      "learning_rate": 1e-05,
      "loss": 7.2777,
      "step": 8392
    },
    {
      "epoch": 0.4548509485094851,
      "step": 8392,
      "training_loss": 7.454465389251709
    },
    {
      "epoch": 0.4549051490514905,
      "step": 8393,
      "training_loss": 6.644428730010986
    },
    {
      "epoch": 0.45495934959349593,
      "step": 8394,
      "training_loss": 7.561052322387695
    },
    {
      "epoch": 0.4550135501355014,
      "step": 8395,
      "training_loss": 7.230408668518066
    },
    {
      "epoch": 0.45506775067750677,
      "grad_norm": 24.340932846069336,
      "learning_rate": 1e-05,
      "loss": 7.2226,
      "step": 8396
    },
    {
      "epoch": 0.45506775067750677,
      "step": 8396,
      "training_loss": 5.292126655578613
    },
    {
      "epoch": 0.4551219512195122,
      "step": 8397,
      "training_loss": 6.304010391235352
    },
    {
      "epoch": 0.4551761517615176,
      "step": 8398,
      "training_loss": 7.611096382141113
    },
    {
      "epoch": 0.45523035230352304,
      "step": 8399,
      "training_loss": 6.934770107269287
    },
    {
      "epoch": 0.45528455284552843,
      "grad_norm": 72.18305969238281,
      "learning_rate": 1e-05,
      "loss": 6.5355,
      "step": 8400
    },
    {
      "epoch": 0.45528455284552843,
      "step": 8400,
      "training_loss": 6.478612899780273
    },
    {
      "epoch": 0.4553387533875339,
      "step": 8401,
      "training_loss": 8.07593822479248
    },
    {
      "epoch": 0.45539295392953927,
      "step": 8402,
      "training_loss": 6.929280757904053
    },
    {
      "epoch": 0.4554471544715447,
      "step": 8403,
      "training_loss": 8.559234619140625
    },
    {
      "epoch": 0.45550135501355016,
      "grad_norm": 37.78125762939453,
      "learning_rate": 1e-05,
      "loss": 7.5108,
      "step": 8404
    },
    {
      "epoch": 0.45550135501355016,
      "step": 8404,
      "training_loss": 7.740141868591309
    },
    {
      "epoch": 0.45555555555555555,
      "step": 8405,
      "training_loss": 7.249494552612305
    },
    {
      "epoch": 0.455609756097561,
      "step": 8406,
      "training_loss": 4.718895435333252
    },
    {
      "epoch": 0.4556639566395664,
      "step": 8407,
      "training_loss": 7.00748872756958
    },
    {
      "epoch": 0.4557181571815718,
      "grad_norm": 14.744338989257812,
      "learning_rate": 1e-05,
      "loss": 6.679,
      "step": 8408
    },
    {
      "epoch": 0.4557181571815718,
      "step": 8408,
      "training_loss": 6.185537338256836
    },
    {
      "epoch": 0.4557723577235772,
      "step": 8409,
      "training_loss": 5.822697639465332
    },
    {
      "epoch": 0.45582655826558266,
      "step": 8410,
      "training_loss": 6.313354015350342
    },
    {
      "epoch": 0.45588075880758805,
      "step": 8411,
      "training_loss": 7.830173492431641
    },
    {
      "epoch": 0.4559349593495935,
      "grad_norm": 25.54146957397461,
      "learning_rate": 1e-05,
      "loss": 6.5379,
      "step": 8412
    },
    {
      "epoch": 0.4559349593495935,
      "step": 8412,
      "training_loss": 7.008570194244385
    },
    {
      "epoch": 0.45598915989159894,
      "step": 8413,
      "training_loss": 8.162601470947266
    },
    {
      "epoch": 0.45604336043360433,
      "step": 8414,
      "training_loss": 6.653845310211182
    },
    {
      "epoch": 0.4560975609756098,
      "step": 8415,
      "training_loss": 7.085592269897461
    },
    {
      "epoch": 0.45615176151761516,
      "grad_norm": 35.83754348754883,
      "learning_rate": 1e-05,
      "loss": 7.2277,
      "step": 8416
    },
    {
      "epoch": 0.45615176151761516,
      "step": 8416,
      "training_loss": 7.176225185394287
    },
    {
      "epoch": 0.4562059620596206,
      "step": 8417,
      "training_loss": 4.808078765869141
    },
    {
      "epoch": 0.456260162601626,
      "step": 8418,
      "training_loss": 7.416661262512207
    },
    {
      "epoch": 0.45631436314363144,
      "step": 8419,
      "training_loss": 7.084159851074219
    },
    {
      "epoch": 0.45636856368563683,
      "grad_norm": 18.835622787475586,
      "learning_rate": 1e-05,
      "loss": 6.6213,
      "step": 8420
    },
    {
      "epoch": 0.45636856368563683,
      "step": 8420,
      "training_loss": 6.252806186676025
    },
    {
      "epoch": 0.4564227642276423,
      "step": 8421,
      "training_loss": 7.361830711364746
    },
    {
      "epoch": 0.4564769647696477,
      "step": 8422,
      "training_loss": 5.932641506195068
    },
    {
      "epoch": 0.4565311653116531,
      "step": 8423,
      "training_loss": 6.086440563201904
    },
    {
      "epoch": 0.45658536585365855,
      "grad_norm": 49.76703643798828,
      "learning_rate": 1e-05,
      "loss": 6.4084,
      "step": 8424
    },
    {
      "epoch": 0.45658536585365855,
      "step": 8424,
      "training_loss": 6.124665260314941
    },
    {
      "epoch": 0.45663956639566394,
      "step": 8425,
      "training_loss": 5.835714817047119
    },
    {
      "epoch": 0.4566937669376694,
      "step": 8426,
      "training_loss": 6.481194496154785
    },
    {
      "epoch": 0.4567479674796748,
      "step": 8427,
      "training_loss": 7.2050461769104
    },
    {
      "epoch": 0.4568021680216802,
      "grad_norm": 23.946561813354492,
      "learning_rate": 1e-05,
      "loss": 6.4117,
      "step": 8428
    },
    {
      "epoch": 0.4568021680216802,
      "step": 8428,
      "training_loss": 5.769561767578125
    },
    {
      "epoch": 0.4568563685636856,
      "step": 8429,
      "training_loss": 4.779145240783691
    },
    {
      "epoch": 0.45691056910569106,
      "step": 8430,
      "training_loss": 6.554823875427246
    },
    {
      "epoch": 0.4569647696476965,
      "step": 8431,
      "training_loss": 4.604783535003662
    },
    {
      "epoch": 0.4570189701897019,
      "grad_norm": 37.633358001708984,
      "learning_rate": 1e-05,
      "loss": 5.4271,
      "step": 8432
    },
    {
      "epoch": 0.4570189701897019,
      "step": 8432,
      "training_loss": 7.92233419418335
    },
    {
      "epoch": 0.45707317073170733,
      "step": 8433,
      "training_loss": 6.397341251373291
    },
    {
      "epoch": 0.4571273712737127,
      "step": 8434,
      "training_loss": 5.66489315032959
    },
    {
      "epoch": 0.45718157181571817,
      "step": 8435,
      "training_loss": 6.529414176940918
    },
    {
      "epoch": 0.45723577235772356,
      "grad_norm": 24.724143981933594,
      "learning_rate": 1e-05,
      "loss": 6.6285,
      "step": 8436
    },
    {
      "epoch": 0.45723577235772356,
      "step": 8436,
      "training_loss": 6.183351516723633
    },
    {
      "epoch": 0.457289972899729,
      "step": 8437,
      "training_loss": 3.657766103744507
    },
    {
      "epoch": 0.4573441734417344,
      "step": 8438,
      "training_loss": 5.95081901550293
    },
    {
      "epoch": 0.45739837398373984,
      "step": 8439,
      "training_loss": 6.172309398651123
    },
    {
      "epoch": 0.4574525745257453,
      "grad_norm": 24.52756690979004,
      "learning_rate": 1e-05,
      "loss": 5.4911,
      "step": 8440
    },
    {
      "epoch": 0.4574525745257453,
      "step": 8440,
      "training_loss": 5.014325141906738
    },
    {
      "epoch": 0.45750677506775067,
      "step": 8441,
      "training_loss": 8.20913028717041
    },
    {
      "epoch": 0.4575609756097561,
      "step": 8442,
      "training_loss": 6.353869438171387
    },
    {
      "epoch": 0.4576151761517615,
      "step": 8443,
      "training_loss": 4.612145900726318
    },
    {
      "epoch": 0.45766937669376695,
      "grad_norm": 24.708412170410156,
      "learning_rate": 1e-05,
      "loss": 6.0474,
      "step": 8444
    },
    {
      "epoch": 0.45766937669376695,
      "step": 8444,
      "training_loss": 7.192948341369629
    },
    {
      "epoch": 0.45772357723577234,
      "step": 8445,
      "training_loss": 5.17155122756958
    },
    {
      "epoch": 0.4577777777777778,
      "step": 8446,
      "training_loss": 6.558570384979248
    },
    {
      "epoch": 0.4578319783197832,
      "step": 8447,
      "training_loss": 6.437488555908203
    },
    {
      "epoch": 0.4578861788617886,
      "grad_norm": 33.15867233276367,
      "learning_rate": 1e-05,
      "loss": 6.3401,
      "step": 8448
    },
    {
      "epoch": 0.4578861788617886,
      "step": 8448,
      "training_loss": 6.6680588722229
    },
    {
      "epoch": 0.45794037940379406,
      "step": 8449,
      "training_loss": 4.302699565887451
    },
    {
      "epoch": 0.45799457994579945,
      "step": 8450,
      "training_loss": 5.915868759155273
    },
    {
      "epoch": 0.4580487804878049,
      "step": 8451,
      "training_loss": 6.142035961151123
    },
    {
      "epoch": 0.4581029810298103,
      "grad_norm": 30.78451156616211,
      "learning_rate": 1e-05,
      "loss": 5.7572,
      "step": 8452
    },
    {
      "epoch": 0.4581029810298103,
      "step": 8452,
      "training_loss": 7.569204807281494
    },
    {
      "epoch": 0.45815718157181573,
      "step": 8453,
      "training_loss": 5.888049602508545
    },
    {
      "epoch": 0.4582113821138211,
      "step": 8454,
      "training_loss": 7.243458271026611
    },
    {
      "epoch": 0.45826558265582656,
      "step": 8455,
      "training_loss": 6.3824462890625
    },
    {
      "epoch": 0.45831978319783195,
      "grad_norm": 39.07431411743164,
      "learning_rate": 1e-05,
      "loss": 6.7708,
      "step": 8456
    },
    {
      "epoch": 0.45831978319783195,
      "step": 8456,
      "training_loss": 5.219546794891357
    },
    {
      "epoch": 0.4583739837398374,
      "step": 8457,
      "training_loss": 6.777557849884033
    },
    {
      "epoch": 0.45842818428184284,
      "step": 8458,
      "training_loss": 6.7231574058532715
    },
    {
      "epoch": 0.45848238482384823,
      "step": 8459,
      "training_loss": 3.4781954288482666
    },
    {
      "epoch": 0.4585365853658537,
      "grad_norm": 31.654659271240234,
      "learning_rate": 1e-05,
      "loss": 5.5496,
      "step": 8460
    },
    {
      "epoch": 0.4585365853658537,
      "step": 8460,
      "training_loss": 5.372468948364258
    },
    {
      "epoch": 0.45859078590785907,
      "step": 8461,
      "training_loss": 6.765272617340088
    },
    {
      "epoch": 0.4586449864498645,
      "step": 8462,
      "training_loss": 7.336417198181152
    },
    {
      "epoch": 0.4586991869918699,
      "step": 8463,
      "training_loss": 7.030848026275635
    },
    {
      "epoch": 0.45875338753387535,
      "grad_norm": 21.43985366821289,
      "learning_rate": 1e-05,
      "loss": 6.6263,
      "step": 8464
    },
    {
      "epoch": 0.45875338753387535,
      "step": 8464,
      "training_loss": 6.7674880027771
    },
    {
      "epoch": 0.45880758807588073,
      "step": 8465,
      "training_loss": 7.138282775878906
    },
    {
      "epoch": 0.4588617886178862,
      "step": 8466,
      "training_loss": 5.9202561378479
    },
    {
      "epoch": 0.4589159891598916,
      "step": 8467,
      "training_loss": 7.012590408325195
    },
    {
      "epoch": 0.458970189701897,
      "grad_norm": 32.85957717895508,
      "learning_rate": 1e-05,
      "loss": 6.7097,
      "step": 8468
    },
    {
      "epoch": 0.458970189701897,
      "step": 8468,
      "training_loss": 7.371553421020508
    },
    {
      "epoch": 0.45902439024390246,
      "step": 8469,
      "training_loss": 7.5112481117248535
    },
    {
      "epoch": 0.45907859078590785,
      "step": 8470,
      "training_loss": 6.438809394836426
    },
    {
      "epoch": 0.4591327913279133,
      "step": 8471,
      "training_loss": 7.826340675354004
    },
    {
      "epoch": 0.4591869918699187,
      "grad_norm": 22.654726028442383,
      "learning_rate": 1e-05,
      "loss": 7.287,
      "step": 8472
    },
    {
      "epoch": 0.4591869918699187,
      "step": 8472,
      "training_loss": 8.873156547546387
    },
    {
      "epoch": 0.4592411924119241,
      "step": 8473,
      "training_loss": 6.676289081573486
    },
    {
      "epoch": 0.4592953929539295,
      "step": 8474,
      "training_loss": 5.105082988739014
    },
    {
      "epoch": 0.45934959349593496,
      "step": 8475,
      "training_loss": 5.923914432525635
    },
    {
      "epoch": 0.4594037940379404,
      "grad_norm": 35.274112701416016,
      "learning_rate": 1e-05,
      "loss": 6.6446,
      "step": 8476
    },
    {
      "epoch": 0.4594037940379404,
      "step": 8476,
      "training_loss": 5.908389568328857
    },
    {
      "epoch": 0.4594579945799458,
      "step": 8477,
      "training_loss": 3.248802900314331
    },
    {
      "epoch": 0.45951219512195124,
      "step": 8478,
      "training_loss": 6.365559101104736
    },
    {
      "epoch": 0.45956639566395663,
      "step": 8479,
      "training_loss": 5.463208198547363
    },
    {
      "epoch": 0.4596205962059621,
      "grad_norm": 53.78577423095703,
      "learning_rate": 1e-05,
      "loss": 5.2465,
      "step": 8480
    },
    {
      "epoch": 0.4596205962059621,
      "step": 8480,
      "training_loss": 7.317103862762451
    },
    {
      "epoch": 0.45967479674796746,
      "step": 8481,
      "training_loss": 7.028984069824219
    },
    {
      "epoch": 0.4597289972899729,
      "step": 8482,
      "training_loss": 7.5140862464904785
    },
    {
      "epoch": 0.4597831978319783,
      "step": 8483,
      "training_loss": 6.746005535125732
    },
    {
      "epoch": 0.45983739837398374,
      "grad_norm": 21.463171005249023,
      "learning_rate": 1e-05,
      "loss": 7.1515,
      "step": 8484
    },
    {
      "epoch": 0.45983739837398374,
      "step": 8484,
      "training_loss": 7.468868255615234
    },
    {
      "epoch": 0.4598915989159892,
      "step": 8485,
      "training_loss": 7.394130229949951
    },
    {
      "epoch": 0.4599457994579946,
      "step": 8486,
      "training_loss": 4.515610694885254
    },
    {
      "epoch": 0.46,
      "step": 8487,
      "training_loss": 6.136929512023926
    },
    {
      "epoch": 0.4600542005420054,
      "grad_norm": 27.053024291992188,
      "learning_rate": 1e-05,
      "loss": 6.3789,
      "step": 8488
    },
    {
      "epoch": 0.4600542005420054,
      "step": 8488,
      "training_loss": 6.512476444244385
    },
    {
      "epoch": 0.46010840108401085,
      "step": 8489,
      "training_loss": 6.828607559204102
    },
    {
      "epoch": 0.46016260162601624,
      "step": 8490,
      "training_loss": 3.3929450511932373
    },
    {
      "epoch": 0.4602168021680217,
      "step": 8491,
      "training_loss": 7.026132106781006
    },
    {
      "epoch": 0.4602710027100271,
      "grad_norm": 35.406578063964844,
      "learning_rate": 1e-05,
      "loss": 5.94,
      "step": 8492
    },
    {
      "epoch": 0.4602710027100271,
      "step": 8492,
      "training_loss": 6.365007400512695
    },
    {
      "epoch": 0.4603252032520325,
      "step": 8493,
      "training_loss": 7.0618486404418945
    },
    {
      "epoch": 0.46037940379403797,
      "step": 8494,
      "training_loss": 7.258636951446533
    },
    {
      "epoch": 0.46043360433604336,
      "step": 8495,
      "training_loss": 6.21249532699585
    },
    {
      "epoch": 0.4604878048780488,
      "grad_norm": 38.1852912902832,
      "learning_rate": 1e-05,
      "loss": 6.7245,
      "step": 8496
    },
    {
      "epoch": 0.4604878048780488,
      "step": 8496,
      "training_loss": 5.334940433502197
    },
    {
      "epoch": 0.4605420054200542,
      "step": 8497,
      "training_loss": 5.650568962097168
    },
    {
      "epoch": 0.46059620596205963,
      "step": 8498,
      "training_loss": 4.020259857177734
    },
    {
      "epoch": 0.460650406504065,
      "step": 8499,
      "training_loss": 6.019588947296143
    },
    {
      "epoch": 0.46070460704607047,
      "grad_norm": 30.821977615356445,
      "learning_rate": 1e-05,
      "loss": 5.2563,
      "step": 8500
    },
    {
      "epoch": 0.46070460704607047,
      "step": 8500,
      "training_loss": 6.94798469543457
    },
    {
      "epoch": 0.46075880758807586,
      "step": 8501,
      "training_loss": 6.5714030265808105
    },
    {
      "epoch": 0.4608130081300813,
      "step": 8502,
      "training_loss": 6.798056602478027
    },
    {
      "epoch": 0.46086720867208675,
      "step": 8503,
      "training_loss": 7.37143087387085
    },
    {
      "epoch": 0.46092140921409214,
      "grad_norm": 19.84378433227539,
      "learning_rate": 1e-05,
      "loss": 6.9222,
      "step": 8504
    },
    {
      "epoch": 0.46092140921409214,
      "step": 8504,
      "training_loss": 7.9041643142700195
    },
    {
      "epoch": 0.4609756097560976,
      "step": 8505,
      "training_loss": 7.2931036949157715
    },
    {
      "epoch": 0.46102981029810297,
      "step": 8506,
      "training_loss": 7.065906047821045
    },
    {
      "epoch": 0.4610840108401084,
      "step": 8507,
      "training_loss": 5.112383842468262
    },
    {
      "epoch": 0.4611382113821138,
      "grad_norm": 22.71832275390625,
      "learning_rate": 1e-05,
      "loss": 6.8439,
      "step": 8508
    },
    {
      "epoch": 0.4611382113821138,
      "step": 8508,
      "training_loss": 5.570858955383301
    },
    {
      "epoch": 0.46119241192411925,
      "step": 8509,
      "training_loss": 7.087912559509277
    },
    {
      "epoch": 0.46124661246612464,
      "step": 8510,
      "training_loss": 6.872337818145752
    },
    {
      "epoch": 0.4613008130081301,
      "step": 8511,
      "training_loss": 6.001792907714844
    },
    {
      "epoch": 0.46135501355013553,
      "grad_norm": 28.49165153503418,
      "learning_rate": 1e-05,
      "loss": 6.3832,
      "step": 8512
    },
    {
      "epoch": 0.46135501355013553,
      "step": 8512,
      "training_loss": 6.102360248565674
    },
    {
      "epoch": 0.4614092140921409,
      "step": 8513,
      "training_loss": 6.1168975830078125
    },
    {
      "epoch": 0.46146341463414636,
      "step": 8514,
      "training_loss": 6.576451301574707
    },
    {
      "epoch": 0.46151761517615175,
      "step": 8515,
      "training_loss": 9.17586898803711
    },
    {
      "epoch": 0.4615718157181572,
      "grad_norm": 34.819026947021484,
      "learning_rate": 1e-05,
      "loss": 6.9929,
      "step": 8516
    },
    {
      "epoch": 0.4615718157181572,
      "step": 8516,
      "training_loss": 5.884393215179443
    },
    {
      "epoch": 0.4616260162601626,
      "step": 8517,
      "training_loss": 7.537317752838135
    },
    {
      "epoch": 0.46168021680216803,
      "step": 8518,
      "training_loss": 6.1886067390441895
    },
    {
      "epoch": 0.4617344173441734,
      "step": 8519,
      "training_loss": 5.967593193054199
    },
    {
      "epoch": 0.46178861788617886,
      "grad_norm": 26.896198272705078,
      "learning_rate": 1e-05,
      "loss": 6.3945,
      "step": 8520
    },
    {
      "epoch": 0.46178861788617886,
      "step": 8520,
      "training_loss": 6.77141809463501
    },
    {
      "epoch": 0.46184281842818425,
      "step": 8521,
      "training_loss": 6.872450828552246
    },
    {
      "epoch": 0.4618970189701897,
      "step": 8522,
      "training_loss": 5.975635528564453
    },
    {
      "epoch": 0.46195121951219514,
      "step": 8523,
      "training_loss": 7.094635009765625
    },
    {
      "epoch": 0.46200542005420053,
      "grad_norm": 40.893428802490234,
      "learning_rate": 1e-05,
      "loss": 6.6785,
      "step": 8524
    },
    {
      "epoch": 0.46200542005420053,
      "step": 8524,
      "training_loss": 7.321335792541504
    },
    {
      "epoch": 0.462059620596206,
      "step": 8525,
      "training_loss": 6.038415908813477
    },
    {
      "epoch": 0.46211382113821137,
      "step": 8526,
      "training_loss": 6.049764633178711
    },
    {
      "epoch": 0.4621680216802168,
      "step": 8527,
      "training_loss": 5.918550491333008
    },
    {
      "epoch": 0.4622222222222222,
      "grad_norm": 49.22575378417969,
      "learning_rate": 1e-05,
      "loss": 6.332,
      "step": 8528
    },
    {
      "epoch": 0.4622222222222222,
      "step": 8528,
      "training_loss": 8.072705268859863
    },
    {
      "epoch": 0.46227642276422765,
      "step": 8529,
      "training_loss": 6.833769798278809
    },
    {
      "epoch": 0.46233062330623304,
      "step": 8530,
      "training_loss": 6.88467264175415
    },
    {
      "epoch": 0.4623848238482385,
      "step": 8531,
      "training_loss": 7.5770649909973145
    },
    {
      "epoch": 0.4624390243902439,
      "grad_norm": 21.763357162475586,
      "learning_rate": 1e-05,
      "loss": 7.3421,
      "step": 8532
    },
    {
      "epoch": 0.4624390243902439,
      "step": 8532,
      "training_loss": 9.130842208862305
    },
    {
      "epoch": 0.4624932249322493,
      "step": 8533,
      "training_loss": 6.766871929168701
    },
    {
      "epoch": 0.46254742547425476,
      "step": 8534,
      "training_loss": 7.990833282470703
    },
    {
      "epoch": 0.46260162601626015,
      "step": 8535,
      "training_loss": 7.064733982086182
    },
    {
      "epoch": 0.4626558265582656,
      "grad_norm": 31.822816848754883,
      "learning_rate": 1e-05,
      "loss": 7.7383,
      "step": 8536
    },
    {
      "epoch": 0.4626558265582656,
      "step": 8536,
      "training_loss": 7.80715274810791
    },
    {
      "epoch": 0.462710027100271,
      "step": 8537,
      "training_loss": 6.525784015655518
    },
    {
      "epoch": 0.4627642276422764,
      "step": 8538,
      "training_loss": 6.7311296463012695
    },
    {
      "epoch": 0.4628184281842818,
      "step": 8539,
      "training_loss": 6.797152996063232
    },
    {
      "epoch": 0.46287262872628726,
      "grad_norm": 54.77821350097656,
      "learning_rate": 1e-05,
      "loss": 6.9653,
      "step": 8540
    },
    {
      "epoch": 0.46287262872628726,
      "step": 8540,
      "training_loss": 6.643914222717285
    },
    {
      "epoch": 0.4629268292682927,
      "step": 8541,
      "training_loss": 6.978574752807617
    },
    {
      "epoch": 0.4629810298102981,
      "step": 8542,
      "training_loss": 6.99643087387085
    },
    {
      "epoch": 0.46303523035230354,
      "step": 8543,
      "training_loss": 6.467453956604004
    },
    {
      "epoch": 0.46308943089430893,
      "grad_norm": 21.548940658569336,
      "learning_rate": 1e-05,
      "loss": 6.7716,
      "step": 8544
    },
    {
      "epoch": 0.46308943089430893,
      "step": 8544,
      "training_loss": 5.906507968902588
    },
    {
      "epoch": 0.4631436314363144,
      "step": 8545,
      "training_loss": 6.773350238800049
    },
    {
      "epoch": 0.46319783197831976,
      "step": 8546,
      "training_loss": 6.72076940536499
    },
    {
      "epoch": 0.4632520325203252,
      "step": 8547,
      "training_loss": 6.5645036697387695
    },
    {
      "epoch": 0.4633062330623306,
      "grad_norm": 30.533859252929688,
      "learning_rate": 1e-05,
      "loss": 6.4913,
      "step": 8548
    },
    {
      "epoch": 0.4633062330623306,
      "step": 8548,
      "training_loss": 8.055243492126465
    },
    {
      "epoch": 0.46336043360433604,
      "step": 8549,
      "training_loss": 7.054098606109619
    },
    {
      "epoch": 0.4634146341463415,
      "step": 8550,
      "training_loss": 6.2277069091796875
    },
    {
      "epoch": 0.4634688346883469,
      "step": 8551,
      "training_loss": 6.668924331665039
    },
    {
      "epoch": 0.4635230352303523,
      "grad_norm": 20.28780746459961,
      "learning_rate": 1e-05,
      "loss": 7.0015,
      "step": 8552
    },
    {
      "epoch": 0.4635230352303523,
      "step": 8552,
      "training_loss": 6.273198127746582
    },
    {
      "epoch": 0.4635772357723577,
      "step": 8553,
      "training_loss": 7.2938947677612305
    },
    {
      "epoch": 0.46363143631436315,
      "step": 8554,
      "training_loss": 7.115691184997559
    },
    {
      "epoch": 0.46368563685636854,
      "step": 8555,
      "training_loss": 6.6697893142700195
    },
    {
      "epoch": 0.463739837398374,
      "grad_norm": 31.922800064086914,
      "learning_rate": 1e-05,
      "loss": 6.8381,
      "step": 8556
    },
    {
      "epoch": 0.463739837398374,
      "step": 8556,
      "training_loss": 5.0353522300720215
    },
    {
      "epoch": 0.4637940379403794,
      "step": 8557,
      "training_loss": 7.86930513381958
    },
    {
      "epoch": 0.4638482384823848,
      "step": 8558,
      "training_loss": 3.6697776317596436
    },
    {
      "epoch": 0.46390243902439027,
      "step": 8559,
      "training_loss": 7.924859046936035
    },
    {
      "epoch": 0.46395663956639566,
      "grad_norm": 43.71812057495117,
      "learning_rate": 1e-05,
      "loss": 6.1248,
      "step": 8560
    },
    {
      "epoch": 0.46395663956639566,
      "step": 8560,
      "training_loss": 6.81377649307251
    },
    {
      "epoch": 0.4640108401084011,
      "step": 8561,
      "training_loss": 6.989928722381592
    },
    {
      "epoch": 0.4640650406504065,
      "step": 8562,
      "training_loss": 8.277199745178223
    },
    {
      "epoch": 0.46411924119241194,
      "step": 8563,
      "training_loss": 7.145013809204102
    },
    {
      "epoch": 0.4641734417344173,
      "grad_norm": 20.483856201171875,
      "learning_rate": 1e-05,
      "loss": 7.3065,
      "step": 8564
    },
    {
      "epoch": 0.4641734417344173,
      "step": 8564,
      "training_loss": 6.292242527008057
    },
    {
      "epoch": 0.46422764227642277,
      "step": 8565,
      "training_loss": 6.426300048828125
    },
    {
      "epoch": 0.46428184281842816,
      "step": 8566,
      "training_loss": 7.638297080993652
    },
    {
      "epoch": 0.4643360433604336,
      "step": 8567,
      "training_loss": 6.956747055053711
    },
    {
      "epoch": 0.46439024390243905,
      "grad_norm": 34.66873550415039,
      "learning_rate": 1e-05,
      "loss": 6.8284,
      "step": 8568
    },
    {
      "epoch": 0.46439024390243905,
      "step": 8568,
      "training_loss": 6.180942535400391
    },
    {
      "epoch": 0.46444444444444444,
      "step": 8569,
      "training_loss": 3.585958480834961
    },
    {
      "epoch": 0.4644986449864499,
      "step": 8570,
      "training_loss": 7.735703468322754
    },
    {
      "epoch": 0.46455284552845527,
      "step": 8571,
      "training_loss": 4.153733253479004
    },
    {
      "epoch": 0.4646070460704607,
      "grad_norm": 38.79208755493164,
      "learning_rate": 1e-05,
      "loss": 5.4141,
      "step": 8572
    },
    {
      "epoch": 0.4646070460704607,
      "step": 8572,
      "training_loss": 5.978084564208984
    },
    {
      "epoch": 0.4646612466124661,
      "step": 8573,
      "training_loss": 7.2192254066467285
    },
    {
      "epoch": 0.46471544715447155,
      "step": 8574,
      "training_loss": 7.480886459350586
    },
    {
      "epoch": 0.46476964769647694,
      "step": 8575,
      "training_loss": 6.196849346160889
    },
    {
      "epoch": 0.4648238482384824,
      "grad_norm": 35.37839126586914,
      "learning_rate": 1e-05,
      "loss": 6.7188,
      "step": 8576
    },
    {
      "epoch": 0.4648238482384824,
      "step": 8576,
      "training_loss": 7.4422831535339355
    },
    {
      "epoch": 0.46487804878048783,
      "step": 8577,
      "training_loss": 6.8868021965026855
    },
    {
      "epoch": 0.4649322493224932,
      "step": 8578,
      "training_loss": 3.6717631816864014
    },
    {
      "epoch": 0.46498644986449866,
      "step": 8579,
      "training_loss": 6.409389495849609
    },
    {
      "epoch": 0.46504065040650405,
      "grad_norm": 23.002132415771484,
      "learning_rate": 1e-05,
      "loss": 6.1026,
      "step": 8580
    },
    {
      "epoch": 0.46504065040650405,
      "step": 8580,
      "training_loss": 7.8093390464782715
    },
    {
      "epoch": 0.4650948509485095,
      "step": 8581,
      "training_loss": 7.974227428436279
    },
    {
      "epoch": 0.4651490514905149,
      "step": 8582,
      "training_loss": 8.1430025100708
    },
    {
      "epoch": 0.46520325203252033,
      "step": 8583,
      "training_loss": 5.116078853607178
    },
    {
      "epoch": 0.4652574525745257,
      "grad_norm": 43.27751541137695,
      "learning_rate": 1e-05,
      "loss": 7.2607,
      "step": 8584
    },
    {
      "epoch": 0.4652574525745257,
      "step": 8584,
      "training_loss": 5.507007122039795
    },
    {
      "epoch": 0.46531165311653117,
      "step": 8585,
      "training_loss": 5.968656539916992
    },
    {
      "epoch": 0.4653658536585366,
      "step": 8586,
      "training_loss": 5.933338165283203
    },
    {
      "epoch": 0.465420054200542,
      "step": 8587,
      "training_loss": 7.475781440734863
    },
    {
      "epoch": 0.46547425474254744,
      "grad_norm": 25.0750732421875,
      "learning_rate": 1e-05,
      "loss": 6.2212,
      "step": 8588
    },
    {
      "epoch": 0.46547425474254744,
      "step": 8588,
      "training_loss": 5.780303478240967
    },
    {
      "epoch": 0.46552845528455283,
      "step": 8589,
      "training_loss": 4.539261817932129
    },
    {
      "epoch": 0.4655826558265583,
      "step": 8590,
      "training_loss": 6.117464542388916
    },
    {
      "epoch": 0.46563685636856367,
      "step": 8591,
      "training_loss": 6.94151496887207
    },
    {
      "epoch": 0.4656910569105691,
      "grad_norm": 29.821840286254883,
      "learning_rate": 1e-05,
      "loss": 5.8446,
      "step": 8592
    },
    {
      "epoch": 0.4656910569105691,
      "step": 8592,
      "training_loss": 6.707578659057617
    },
    {
      "epoch": 0.4657452574525745,
      "step": 8593,
      "training_loss": 3.934690237045288
    },
    {
      "epoch": 0.46579945799457995,
      "step": 8594,
      "training_loss": 4.446638107299805
    },
    {
      "epoch": 0.4658536585365854,
      "step": 8595,
      "training_loss": 7.732671737670898
    },
    {
      "epoch": 0.4659078590785908,
      "grad_norm": 17.625490188598633,
      "learning_rate": 1e-05,
      "loss": 5.7054,
      "step": 8596
    },
    {
      "epoch": 0.4659078590785908,
      "step": 8596,
      "training_loss": 6.815301895141602
    },
    {
      "epoch": 0.4659620596205962,
      "step": 8597,
      "training_loss": 6.730497360229492
    },
    {
      "epoch": 0.4660162601626016,
      "step": 8598,
      "training_loss": 7.3247246742248535
    },
    {
      "epoch": 0.46607046070460706,
      "step": 8599,
      "training_loss": 6.212113380432129
    },
    {
      "epoch": 0.46612466124661245,
      "grad_norm": 37.60476303100586,
      "learning_rate": 1e-05,
      "loss": 6.7707,
      "step": 8600
    },
    {
      "epoch": 0.46612466124661245,
      "step": 8600,
      "training_loss": 5.159328937530518
    },
    {
      "epoch": 0.4661788617886179,
      "step": 8601,
      "training_loss": 6.279941082000732
    },
    {
      "epoch": 0.4662330623306233,
      "step": 8602,
      "training_loss": 6.097900867462158
    },
    {
      "epoch": 0.4662872628726287,
      "step": 8603,
      "training_loss": 7.815320014953613
    },
    {
      "epoch": 0.46634146341463417,
      "grad_norm": 31.801382064819336,
      "learning_rate": 1e-05,
      "loss": 6.3381,
      "step": 8604
    },
    {
      "epoch": 0.46634146341463417,
      "step": 8604,
      "training_loss": 7.113962650299072
    },
    {
      "epoch": 0.46639566395663956,
      "step": 8605,
      "training_loss": 6.084290981292725
    },
    {
      "epoch": 0.466449864498645,
      "step": 8606,
      "training_loss": 6.223576545715332
    },
    {
      "epoch": 0.4665040650406504,
      "step": 8607,
      "training_loss": 7.393270969390869
    },
    {
      "epoch": 0.46655826558265584,
      "grad_norm": 23.85835838317871,
      "learning_rate": 1e-05,
      "loss": 6.7038,
      "step": 8608
    },
    {
      "epoch": 0.46655826558265584,
      "step": 8608,
      "training_loss": 7.386989116668701
    },
    {
      "epoch": 0.46661246612466123,
      "step": 8609,
      "training_loss": 7.202570915222168
    },
    {
      "epoch": 0.4666666666666667,
      "step": 8610,
      "training_loss": 7.2412109375
    },
    {
      "epoch": 0.46672086720867206,
      "step": 8611,
      "training_loss": 6.610138416290283
    },
    {
      "epoch": 0.4667750677506775,
      "grad_norm": 28.416152954101562,
      "learning_rate": 1e-05,
      "loss": 7.1102,
      "step": 8612
    },
    {
      "epoch": 0.4667750677506775,
      "step": 8612,
      "training_loss": 6.7035393714904785
    },
    {
      "epoch": 0.46682926829268295,
      "step": 8613,
      "training_loss": 7.739098072052002
    },
    {
      "epoch": 0.46688346883468834,
      "step": 8614,
      "training_loss": 7.476682186126709
    },
    {
      "epoch": 0.4669376693766938,
      "step": 8615,
      "training_loss": 6.520513534545898
    },
    {
      "epoch": 0.4669918699186992,
      "grad_norm": 21.152376174926758,
      "learning_rate": 1e-05,
      "loss": 7.11,
      "step": 8616
    },
    {
      "epoch": 0.4669918699186992,
      "step": 8616,
      "training_loss": 6.327324867248535
    },
    {
      "epoch": 0.4670460704607046,
      "step": 8617,
      "training_loss": 8.215302467346191
    },
    {
      "epoch": 0.46710027100271,
      "step": 8618,
      "training_loss": 8.076729774475098
    },
    {
      "epoch": 0.46715447154471545,
      "step": 8619,
      "training_loss": 7.17300271987915
    },
    {
      "epoch": 0.46720867208672084,
      "grad_norm": 36.71858215332031,
      "learning_rate": 1e-05,
      "loss": 7.4481,
      "step": 8620
    },
    {
      "epoch": 0.46720867208672084,
      "step": 8620,
      "training_loss": 7.422115802764893
    },
    {
      "epoch": 0.4672628726287263,
      "step": 8621,
      "training_loss": 6.101724147796631
    },
    {
      "epoch": 0.46731707317073173,
      "step": 8622,
      "training_loss": 7.22337532043457
    },
    {
      "epoch": 0.4673712737127371,
      "step": 8623,
      "training_loss": 4.641124725341797
    },
    {
      "epoch": 0.46742547425474257,
      "grad_norm": 26.472517013549805,
      "learning_rate": 1e-05,
      "loss": 6.3471,
      "step": 8624
    },
    {
      "epoch": 0.46742547425474257,
      "step": 8624,
      "training_loss": 6.414724826812744
    },
    {
      "epoch": 0.46747967479674796,
      "step": 8625,
      "training_loss": 7.502845287322998
    },
    {
      "epoch": 0.4675338753387534,
      "step": 8626,
      "training_loss": 5.964104175567627
    },
    {
      "epoch": 0.4675880758807588,
      "step": 8627,
      "training_loss": 6.581466197967529
    },
    {
      "epoch": 0.46764227642276424,
      "grad_norm": 21.83155632019043,
      "learning_rate": 1e-05,
      "loss": 6.6158,
      "step": 8628
    },
    {
      "epoch": 0.46764227642276424,
      "step": 8628,
      "training_loss": 3.115542411804199
    },
    {
      "epoch": 0.4676964769647696,
      "step": 8629,
      "training_loss": 7.028499603271484
    },
    {
      "epoch": 0.46775067750677507,
      "step": 8630,
      "training_loss": 6.995840549468994
    },
    {
      "epoch": 0.4678048780487805,
      "step": 8631,
      "training_loss": 5.899604797363281
    },
    {
      "epoch": 0.4678590785907859,
      "grad_norm": 39.433345794677734,
      "learning_rate": 1e-05,
      "loss": 5.7599,
      "step": 8632
    },
    {
      "epoch": 0.4678590785907859,
      "step": 8632,
      "training_loss": 7.41007661819458
    },
    {
      "epoch": 0.46791327913279135,
      "step": 8633,
      "training_loss": 6.320960521697998
    },
    {
      "epoch": 0.46796747967479674,
      "step": 8634,
      "training_loss": 6.391262054443359
    },
    {
      "epoch": 0.4680216802168022,
      "step": 8635,
      "training_loss": 3.8357062339782715
    },
    {
      "epoch": 0.46807588075880757,
      "grad_norm": 29.33900260925293,
      "learning_rate": 1e-05,
      "loss": 5.9895,
      "step": 8636
    },
    {
      "epoch": 0.46807588075880757,
      "step": 8636,
      "training_loss": 6.930405139923096
    },
    {
      "epoch": 0.468130081300813,
      "step": 8637,
      "training_loss": 6.475010871887207
    },
    {
      "epoch": 0.4681842818428184,
      "step": 8638,
      "training_loss": 7.142306327819824
    },
    {
      "epoch": 0.46823848238482385,
      "step": 8639,
      "training_loss": 5.883336067199707
    },
    {
      "epoch": 0.4682926829268293,
      "grad_norm": 30.191373825073242,
      "learning_rate": 1e-05,
      "loss": 6.6078,
      "step": 8640
    },
    {
      "epoch": 0.4682926829268293,
      "step": 8640,
      "training_loss": 5.692075252532959
    },
    {
      "epoch": 0.4683468834688347,
      "step": 8641,
      "training_loss": 6.378225326538086
    },
    {
      "epoch": 0.46840108401084013,
      "step": 8642,
      "training_loss": 7.863348007202148
    },
    {
      "epoch": 0.4684552845528455,
      "step": 8643,
      "training_loss": 6.402125835418701
    },
    {
      "epoch": 0.46850948509485096,
      "grad_norm": 20.056215286254883,
      "learning_rate": 1e-05,
      "loss": 6.5839,
      "step": 8644
    },
    {
      "epoch": 0.46850948509485096,
      "step": 8644,
      "training_loss": 5.916999816894531
    },
    {
      "epoch": 0.46856368563685635,
      "step": 8645,
      "training_loss": 6.745124816894531
    },
    {
      "epoch": 0.4686178861788618,
      "step": 8646,
      "training_loss": 6.499805450439453
    },
    {
      "epoch": 0.4686720867208672,
      "step": 8647,
      "training_loss": 7.4791131019592285
    },
    {
      "epoch": 0.46872628726287263,
      "grad_norm": 42.851722717285156,
      "learning_rate": 1e-05,
      "loss": 6.6603,
      "step": 8648
    },
    {
      "epoch": 0.46872628726287263,
      "step": 8648,
      "training_loss": 7.381659030914307
    },
    {
      "epoch": 0.468780487804878,
      "step": 8649,
      "training_loss": 6.790375232696533
    },
    {
      "epoch": 0.46883468834688347,
      "step": 8650,
      "training_loss": 6.982457160949707
    },
    {
      "epoch": 0.4688888888888889,
      "step": 8651,
      "training_loss": 7.54249382019043
    },
    {
      "epoch": 0.4689430894308943,
      "grad_norm": 44.048187255859375,
      "learning_rate": 1e-05,
      "loss": 7.1742,
      "step": 8652
    },
    {
      "epoch": 0.4689430894308943,
      "step": 8652,
      "training_loss": 7.2123517990112305
    },
    {
      "epoch": 0.46899728997289974,
      "step": 8653,
      "training_loss": 4.911919593811035
    },
    {
      "epoch": 0.46905149051490513,
      "step": 8654,
      "training_loss": 7.718087196350098
    },
    {
      "epoch": 0.4691056910569106,
      "step": 8655,
      "training_loss": 8.062870979309082
    },
    {
      "epoch": 0.46915989159891597,
      "grad_norm": 23.22121238708496,
      "learning_rate": 1e-05,
      "loss": 6.9763,
      "step": 8656
    },
    {
      "epoch": 0.46915989159891597,
      "step": 8656,
      "training_loss": 7.439856052398682
    },
    {
      "epoch": 0.4692140921409214,
      "step": 8657,
      "training_loss": 6.0522003173828125
    },
    {
      "epoch": 0.4692682926829268,
      "step": 8658,
      "training_loss": 7.7125091552734375
    },
    {
      "epoch": 0.46932249322493225,
      "step": 8659,
      "training_loss": 7.214272499084473
    },
    {
      "epoch": 0.4693766937669377,
      "grad_norm": 19.825796127319336,
      "learning_rate": 1e-05,
      "loss": 7.1047,
      "step": 8660
    },
    {
      "epoch": 0.4693766937669377,
      "step": 8660,
      "training_loss": 5.923830986022949
    },
    {
      "epoch": 0.4694308943089431,
      "step": 8661,
      "training_loss": 6.521286487579346
    },
    {
      "epoch": 0.4694850948509485,
      "step": 8662,
      "training_loss": 6.038105010986328
    },
    {
      "epoch": 0.4695392953929539,
      "step": 8663,
      "training_loss": 6.662823677062988
    },
    {
      "epoch": 0.46959349593495936,
      "grad_norm": 22.464780807495117,
      "learning_rate": 1e-05,
      "loss": 6.2865,
      "step": 8664
    },
    {
      "epoch": 0.46959349593495936,
      "step": 8664,
      "training_loss": 6.864611625671387
    },
    {
      "epoch": 0.46964769647696475,
      "step": 8665,
      "training_loss": 6.272677898406982
    },
    {
      "epoch": 0.4697018970189702,
      "step": 8666,
      "training_loss": 6.4762725830078125
    },
    {
      "epoch": 0.4697560975609756,
      "step": 8667,
      "training_loss": 3.74747896194458
    },
    {
      "epoch": 0.469810298102981,
      "grad_norm": 29.769132614135742,
      "learning_rate": 1e-05,
      "loss": 5.8403,
      "step": 8668
    },
    {
      "epoch": 0.469810298102981,
      "step": 8668,
      "training_loss": 7.112630844116211
    },
    {
      "epoch": 0.4698644986449865,
      "step": 8669,
      "training_loss": 6.855426788330078
    },
    {
      "epoch": 0.46991869918699186,
      "step": 8670,
      "training_loss": 3.3026342391967773
    },
    {
      "epoch": 0.4699728997289973,
      "step": 8671,
      "training_loss": 5.98288106918335
    },
    {
      "epoch": 0.4700271002710027,
      "grad_norm": 39.04652786254883,
      "learning_rate": 1e-05,
      "loss": 5.8134,
      "step": 8672
    },
    {
      "epoch": 0.4700271002710027,
      "step": 8672,
      "training_loss": 6.556191921234131
    },
    {
      "epoch": 0.47008130081300814,
      "step": 8673,
      "training_loss": 6.0853495597839355
    },
    {
      "epoch": 0.47013550135501353,
      "step": 8674,
      "training_loss": 7.320001602172852
    },
    {
      "epoch": 0.470189701897019,
      "step": 8675,
      "training_loss": 5.3924174308776855
    },
    {
      "epoch": 0.47024390243902436,
      "grad_norm": 47.62234878540039,
      "learning_rate": 1e-05,
      "loss": 6.3385,
      "step": 8676
    },
    {
      "epoch": 0.47024390243902436,
      "step": 8676,
      "training_loss": 5.716021537780762
    },
    {
      "epoch": 0.4702981029810298,
      "step": 8677,
      "training_loss": 6.370461463928223
    },
    {
      "epoch": 0.47035230352303525,
      "step": 8678,
      "training_loss": 7.755607604980469
    },
    {
      "epoch": 0.47040650406504064,
      "step": 8679,
      "training_loss": 6.985742092132568
    },
    {
      "epoch": 0.4704607046070461,
      "grad_norm": 23.86858367919922,
      "learning_rate": 1e-05,
      "loss": 6.707,
      "step": 8680
    },
    {
      "epoch": 0.4704607046070461,
      "step": 8680,
      "training_loss": 7.509665489196777
    },
    {
      "epoch": 0.4705149051490515,
      "step": 8681,
      "training_loss": 6.731199741363525
    },
    {
      "epoch": 0.4705691056910569,
      "step": 8682,
      "training_loss": 7.700690746307373
    },
    {
      "epoch": 0.4706233062330623,
      "step": 8683,
      "training_loss": 7.787207126617432
    },
    {
      "epoch": 0.47067750677506776,
      "grad_norm": 30.10032844543457,
      "learning_rate": 1e-05,
      "loss": 7.4322,
      "step": 8684
    },
    {
      "epoch": 0.47067750677506776,
      "step": 8684,
      "training_loss": 7.749781131744385
    },
    {
      "epoch": 0.47073170731707314,
      "step": 8685,
      "training_loss": 6.955041885375977
    },
    {
      "epoch": 0.4707859078590786,
      "step": 8686,
      "training_loss": 5.730529308319092
    },
    {
      "epoch": 0.47084010840108403,
      "step": 8687,
      "training_loss": 5.895066738128662
    },
    {
      "epoch": 0.4708943089430894,
      "grad_norm": 57.95079803466797,
      "learning_rate": 1e-05,
      "loss": 6.5826,
      "step": 8688
    },
    {
      "epoch": 0.4708943089430894,
      "step": 8688,
      "training_loss": 6.033708572387695
    },
    {
      "epoch": 0.47094850948509487,
      "step": 8689,
      "training_loss": 6.940702438354492
    },
    {
      "epoch": 0.47100271002710026,
      "step": 8690,
      "training_loss": 7.261291980743408
    },
    {
      "epoch": 0.4710569105691057,
      "step": 8691,
      "training_loss": 6.546548843383789
    },
    {
      "epoch": 0.4711111111111111,
      "grad_norm": 21.850107192993164,
      "learning_rate": 1e-05,
      "loss": 6.6956,
      "step": 8692
    },
    {
      "epoch": 0.4711111111111111,
      "step": 8692,
      "training_loss": 5.593597888946533
    },
    {
      "epoch": 0.47116531165311654,
      "step": 8693,
      "training_loss": 6.772270679473877
    },
    {
      "epoch": 0.4712195121951219,
      "step": 8694,
      "training_loss": 7.062253475189209
    },
    {
      "epoch": 0.47127371273712737,
      "step": 8695,
      "training_loss": 7.0160040855407715
    },
    {
      "epoch": 0.4713279132791328,
      "grad_norm": 22.64872169494629,
      "learning_rate": 1e-05,
      "loss": 6.611,
      "step": 8696
    },
    {
      "epoch": 0.4713279132791328,
      "step": 8696,
      "training_loss": 5.76266622543335
    },
    {
      "epoch": 0.4713821138211382,
      "step": 8697,
      "training_loss": 6.062202453613281
    },
    {
      "epoch": 0.47143631436314365,
      "step": 8698,
      "training_loss": 7.76332950592041
    },
    {
      "epoch": 0.47149051490514904,
      "step": 8699,
      "training_loss": 8.213022232055664
    },
    {
      "epoch": 0.4715447154471545,
      "grad_norm": 30.958864212036133,
      "learning_rate": 1e-05,
      "loss": 6.9503,
      "step": 8700
    },
    {
      "epoch": 0.4715447154471545,
      "step": 8700,
      "training_loss": 6.7914886474609375
    },
    {
      "epoch": 0.4715989159891599,
      "step": 8701,
      "training_loss": 6.961594581604004
    },
    {
      "epoch": 0.4716531165311653,
      "step": 8702,
      "training_loss": 7.585513591766357
    },
    {
      "epoch": 0.4717073170731707,
      "step": 8703,
      "training_loss": 7.117781639099121
    },
    {
      "epoch": 0.47176151761517615,
      "grad_norm": 38.1373405456543,
      "learning_rate": 1e-05,
      "loss": 7.1141,
      "step": 8704
    },
    {
      "epoch": 0.47176151761517615,
      "step": 8704,
      "training_loss": 6.420679569244385
    },
    {
      "epoch": 0.4718157181571816,
      "step": 8705,
      "training_loss": 8.451550483703613
    },
    {
      "epoch": 0.471869918699187,
      "step": 8706,
      "training_loss": 6.703178405761719
    },
    {
      "epoch": 0.47192411924119243,
      "step": 8707,
      "training_loss": 3.3368821144104004
    },
    {
      "epoch": 0.4719783197831978,
      "grad_norm": 40.77399826049805,
      "learning_rate": 1e-05,
      "loss": 6.2281,
      "step": 8708
    },
    {
      "epoch": 0.4719783197831978,
      "step": 8708,
      "training_loss": 6.6677021980285645
    },
    {
      "epoch": 0.47203252032520326,
      "step": 8709,
      "training_loss": 6.954292297363281
    },
    {
      "epoch": 0.47208672086720865,
      "step": 8710,
      "training_loss": 6.652273654937744
    },
    {
      "epoch": 0.4721409214092141,
      "step": 8711,
      "training_loss": 6.608066082000732
    },
    {
      "epoch": 0.4721951219512195,
      "grad_norm": 27.399381637573242,
      "learning_rate": 1e-05,
      "loss": 6.7206,
      "step": 8712
    },
    {
      "epoch": 0.4721951219512195,
      "step": 8712,
      "training_loss": 6.775742053985596
    },
    {
      "epoch": 0.47224932249322493,
      "step": 8713,
      "training_loss": 7.231801986694336
    },
    {
      "epoch": 0.4723035230352304,
      "step": 8714,
      "training_loss": 6.355579376220703
    },
    {
      "epoch": 0.47235772357723577,
      "step": 8715,
      "training_loss": 7.822214126586914
    },
    {
      "epoch": 0.4724119241192412,
      "grad_norm": 42.243839263916016,
      "learning_rate": 1e-05,
      "loss": 7.0463,
      "step": 8716
    },
    {
      "epoch": 0.4724119241192412,
      "step": 8716,
      "training_loss": 6.531036376953125
    },
    {
      "epoch": 0.4724661246612466,
      "step": 8717,
      "training_loss": 5.136106967926025
    },
    {
      "epoch": 0.47252032520325205,
      "step": 8718,
      "training_loss": 6.449653148651123
    },
    {
      "epoch": 0.47257452574525743,
      "step": 8719,
      "training_loss": 6.7311882972717285
    },
    {
      "epoch": 0.4726287262872629,
      "grad_norm": 31.606645584106445,
      "learning_rate": 1e-05,
      "loss": 6.212,
      "step": 8720
    },
    {
      "epoch": 0.4726287262872629,
      "step": 8720,
      "training_loss": 6.576032638549805
    },
    {
      "epoch": 0.47268292682926827,
      "step": 8721,
      "training_loss": 6.013205528259277
    },
    {
      "epoch": 0.4727371273712737,
      "step": 8722,
      "training_loss": 7.869683742523193
    },
    {
      "epoch": 0.47279132791327916,
      "step": 8723,
      "training_loss": 8.748190879821777
    },
    {
      "epoch": 0.47284552845528455,
      "grad_norm": 35.611083984375,
      "learning_rate": 1e-05,
      "loss": 7.3018,
      "step": 8724
    },
    {
      "epoch": 0.47284552845528455,
      "step": 8724,
      "training_loss": 7.102457523345947
    },
    {
      "epoch": 0.47289972899729,
      "step": 8725,
      "training_loss": 6.6374640464782715
    },
    {
      "epoch": 0.4729539295392954,
      "step": 8726,
      "training_loss": 7.40946102142334
    },
    {
      "epoch": 0.4730081300813008,
      "step": 8727,
      "training_loss": 6.145172595977783
    },
    {
      "epoch": 0.4730623306233062,
      "grad_norm": 36.29380416870117,
      "learning_rate": 1e-05,
      "loss": 6.8236,
      "step": 8728
    },
    {
      "epoch": 0.4730623306233062,
      "step": 8728,
      "training_loss": 5.568206787109375
    },
    {
      "epoch": 0.47311653116531166,
      "step": 8729,
      "training_loss": 6.184704780578613
    },
    {
      "epoch": 0.47317073170731705,
      "step": 8730,
      "training_loss": 8.18642807006836
    },
    {
      "epoch": 0.4732249322493225,
      "step": 8731,
      "training_loss": 6.82732629776001
    },
    {
      "epoch": 0.47327913279132794,
      "grad_norm": 21.478429794311523,
      "learning_rate": 1e-05,
      "loss": 6.6917,
      "step": 8732
    },
    {
      "epoch": 0.47327913279132794,
      "step": 8732,
      "training_loss": 6.843959808349609
    },
    {
      "epoch": 0.47333333333333333,
      "step": 8733,
      "training_loss": 7.046823024749756
    },
    {
      "epoch": 0.4733875338753388,
      "step": 8734,
      "training_loss": 6.247045516967773
    },
    {
      "epoch": 0.47344173441734416,
      "step": 8735,
      "training_loss": 7.06622314453125
    },
    {
      "epoch": 0.4734959349593496,
      "grad_norm": 34.67776870727539,
      "learning_rate": 1e-05,
      "loss": 6.801,
      "step": 8736
    },
    {
      "epoch": 0.4734959349593496,
      "step": 8736,
      "training_loss": 6.784939765930176
    },
    {
      "epoch": 0.473550135501355,
      "step": 8737,
      "training_loss": 7.088143348693848
    },
    {
      "epoch": 0.47360433604336044,
      "step": 8738,
      "training_loss": 6.908008575439453
    },
    {
      "epoch": 0.47365853658536583,
      "step": 8739,
      "training_loss": 6.625329494476318
    },
    {
      "epoch": 0.4737127371273713,
      "grad_norm": 28.95180892944336,
      "learning_rate": 1e-05,
      "loss": 6.8516,
      "step": 8740
    },
    {
      "epoch": 0.4737127371273713,
      "step": 8740,
      "training_loss": 6.6881103515625
    },
    {
      "epoch": 0.4737669376693767,
      "step": 8741,
      "training_loss": 6.202998161315918
    },
    {
      "epoch": 0.4738211382113821,
      "step": 8742,
      "training_loss": 5.767998695373535
    },
    {
      "epoch": 0.47387533875338755,
      "step": 8743,
      "training_loss": 6.476691246032715
    },
    {
      "epoch": 0.47392953929539294,
      "grad_norm": 32.542144775390625,
      "learning_rate": 1e-05,
      "loss": 6.2839,
      "step": 8744
    },
    {
      "epoch": 0.47392953929539294,
      "step": 8744,
      "training_loss": 5.902226448059082
    },
    {
      "epoch": 0.4739837398373984,
      "step": 8745,
      "training_loss": 6.46298360824585
    },
    {
      "epoch": 0.4740379403794038,
      "step": 8746,
      "training_loss": 8.297635078430176
    },
    {
      "epoch": 0.4740921409214092,
      "step": 8747,
      "training_loss": 7.369076251983643
    },
    {
      "epoch": 0.4741463414634146,
      "grad_norm": 19.012176513671875,
      "learning_rate": 1e-05,
      "loss": 7.008,
      "step": 8748
    },
    {
      "epoch": 0.4741463414634146,
      "step": 8748,
      "training_loss": 6.769769191741943
    },
    {
      "epoch": 0.47420054200542006,
      "step": 8749,
      "training_loss": 6.675316333770752
    },
    {
      "epoch": 0.4742547425474255,
      "step": 8750,
      "training_loss": 7.558692455291748
    },
    {
      "epoch": 0.4743089430894309,
      "step": 8751,
      "training_loss": 5.63127326965332
    },
    {
      "epoch": 0.47436314363143633,
      "grad_norm": 38.909217834472656,
      "learning_rate": 1e-05,
      "loss": 6.6588,
      "step": 8752
    },
    {
      "epoch": 0.47436314363143633,
      "step": 8752,
      "training_loss": 5.299503326416016
    },
    {
      "epoch": 0.4744173441734417,
      "step": 8753,
      "training_loss": 5.239810466766357
    },
    {
      "epoch": 0.47447154471544717,
      "step": 8754,
      "training_loss": 6.506971836090088
    },
    {
      "epoch": 0.47452574525745256,
      "step": 8755,
      "training_loss": 6.450228691101074
    },
    {
      "epoch": 0.474579945799458,
      "grad_norm": 23.002342224121094,
      "learning_rate": 1e-05,
      "loss": 5.8741,
      "step": 8756
    },
    {
      "epoch": 0.474579945799458,
      "step": 8756,
      "training_loss": 7.766998767852783
    },
    {
      "epoch": 0.4746341463414634,
      "step": 8757,
      "training_loss": 7.820295333862305
    },
    {
      "epoch": 0.47468834688346884,
      "step": 8758,
      "training_loss": 7.205555438995361
    },
    {
      "epoch": 0.4747425474254743,
      "step": 8759,
      "training_loss": 8.483386993408203
    },
    {
      "epoch": 0.47479674796747967,
      "grad_norm": 47.67266082763672,
      "learning_rate": 1e-05,
      "loss": 7.8191,
      "step": 8760
    },
    {
      "epoch": 0.47479674796747967,
      "step": 8760,
      "training_loss": 3.733051300048828
    },
    {
      "epoch": 0.4748509485094851,
      "step": 8761,
      "training_loss": 7.7333807945251465
    },
    {
      "epoch": 0.4749051490514905,
      "step": 8762,
      "training_loss": 6.830978870391846
    },
    {
      "epoch": 0.47495934959349595,
      "step": 8763,
      "training_loss": 6.573931694030762
    },
    {
      "epoch": 0.47501355013550134,
      "grad_norm": 26.713071823120117,
      "learning_rate": 1e-05,
      "loss": 6.2178,
      "step": 8764
    },
    {
      "epoch": 0.47501355013550134,
      "step": 8764,
      "training_loss": 6.924048900604248
    },
    {
      "epoch": 0.4750677506775068,
      "step": 8765,
      "training_loss": 7.117854118347168
    },
    {
      "epoch": 0.4751219512195122,
      "step": 8766,
      "training_loss": 7.250119686126709
    },
    {
      "epoch": 0.4751761517615176,
      "step": 8767,
      "training_loss": 7.215385437011719
    },
    {
      "epoch": 0.47523035230352306,
      "grad_norm": 24.751157760620117,
      "learning_rate": 1e-05,
      "loss": 7.1269,
      "step": 8768
    },
    {
      "epoch": 0.47523035230352306,
      "step": 8768,
      "training_loss": 6.365550518035889
    },
    {
      "epoch": 0.47528455284552845,
      "step": 8769,
      "training_loss": 5.940555572509766
    },
    {
      "epoch": 0.4753387533875339,
      "step": 8770,
      "training_loss": 5.074552536010742
    },
    {
      "epoch": 0.4753929539295393,
      "step": 8771,
      "training_loss": 6.424692153930664
    },
    {
      "epoch": 0.47544715447154473,
      "grad_norm": 31.172080993652344,
      "learning_rate": 1e-05,
      "loss": 5.9513,
      "step": 8772
    },
    {
      "epoch": 0.47544715447154473,
      "step": 8772,
      "training_loss": 6.843995571136475
    },
    {
      "epoch": 0.4755013550135501,
      "step": 8773,
      "training_loss": 6.668908596038818
    },
    {
      "epoch": 0.47555555555555556,
      "step": 8774,
      "training_loss": 6.52249002456665
    },
    {
      "epoch": 0.47560975609756095,
      "step": 8775,
      "training_loss": 6.615206241607666
    },
    {
      "epoch": 0.4756639566395664,
      "grad_norm": 25.31827163696289,
      "learning_rate": 1e-05,
      "loss": 6.6627,
      "step": 8776
    },
    {
      "epoch": 0.4756639566395664,
      "step": 8776,
      "training_loss": 5.230291843414307
    },
    {
      "epoch": 0.4757181571815718,
      "step": 8777,
      "training_loss": 6.387392520904541
    },
    {
      "epoch": 0.47577235772357723,
      "step": 8778,
      "training_loss": 7.162108898162842
    },
    {
      "epoch": 0.4758265582655827,
      "step": 8779,
      "training_loss": 6.620734691619873
    },
    {
      "epoch": 0.47588075880758807,
      "grad_norm": 18.415634155273438,
      "learning_rate": 1e-05,
      "loss": 6.3501,
      "step": 8780
    },
    {
      "epoch": 0.47588075880758807,
      "step": 8780,
      "training_loss": 6.956798553466797
    },
    {
      "epoch": 0.4759349593495935,
      "step": 8781,
      "training_loss": 8.101387023925781
    },
    {
      "epoch": 0.4759891598915989,
      "step": 8782,
      "training_loss": 6.47579288482666
    },
    {
      "epoch": 0.47604336043360435,
      "step": 8783,
      "training_loss": 8.033929824829102
    },
    {
      "epoch": 0.47609756097560973,
      "grad_norm": 26.342304229736328,
      "learning_rate": 1e-05,
      "loss": 7.392,
      "step": 8784
    },
    {
      "epoch": 0.47609756097560973,
      "step": 8784,
      "training_loss": 3.227618455886841
    },
    {
      "epoch": 0.4761517615176152,
      "step": 8785,
      "training_loss": 4.3746185302734375
    },
    {
      "epoch": 0.47620596205962057,
      "step": 8786,
      "training_loss": 6.8831281661987305
    },
    {
      "epoch": 0.476260162601626,
      "step": 8787,
      "training_loss": 5.654993534088135
    },
    {
      "epoch": 0.47631436314363146,
      "grad_norm": 22.567249298095703,
      "learning_rate": 1e-05,
      "loss": 5.0351,
      "step": 8788
    },
    {
      "epoch": 0.47631436314363146,
      "step": 8788,
      "training_loss": 6.305398941040039
    },
    {
      "epoch": 0.47636856368563685,
      "step": 8789,
      "training_loss": 6.250289440155029
    },
    {
      "epoch": 0.4764227642276423,
      "step": 8790,
      "training_loss": 7.146369457244873
    },
    {
      "epoch": 0.4764769647696477,
      "step": 8791,
      "training_loss": 6.169522762298584
    },
    {
      "epoch": 0.4765311653116531,
      "grad_norm": 19.841699600219727,
      "learning_rate": 1e-05,
      "loss": 6.4679,
      "step": 8792
    },
    {
      "epoch": 0.4765311653116531,
      "step": 8792,
      "training_loss": 7.9433979988098145
    },
    {
      "epoch": 0.4765853658536585,
      "step": 8793,
      "training_loss": 9.458259582519531
    },
    {
      "epoch": 0.47663956639566396,
      "step": 8794,
      "training_loss": 7.722391128540039
    },
    {
      "epoch": 0.47669376693766935,
      "step": 8795,
      "training_loss": 7.397755146026611
    },
    {
      "epoch": 0.4767479674796748,
      "grad_norm": 21.563644409179688,
      "learning_rate": 1e-05,
      "loss": 8.1305,
      "step": 8796
    },
    {
      "epoch": 0.4767479674796748,
      "step": 8796,
      "training_loss": 6.64775276184082
    },
    {
      "epoch": 0.47680216802168024,
      "step": 8797,
      "training_loss": 5.638513088226318
    },
    {
      "epoch": 0.47685636856368563,
      "step": 8798,
      "training_loss": 4.850424766540527
    },
    {
      "epoch": 0.4769105691056911,
      "step": 8799,
      "training_loss": 6.442920684814453
    },
    {
      "epoch": 0.47696476964769646,
      "grad_norm": 28.643798828125,
      "learning_rate": 1e-05,
      "loss": 5.8949,
      "step": 8800
    },
    {
      "epoch": 0.47696476964769646,
      "step": 8800,
      "training_loss": 7.0121893882751465
    },
    {
      "epoch": 0.4770189701897019,
      "step": 8801,
      "training_loss": 6.859425067901611
    },
    {
      "epoch": 0.4770731707317073,
      "step": 8802,
      "training_loss": 7.197177410125732
    },
    {
      "epoch": 0.47712737127371274,
      "step": 8803,
      "training_loss": 5.899493217468262
    },
    {
      "epoch": 0.47718157181571813,
      "grad_norm": 21.07969856262207,
      "learning_rate": 1e-05,
      "loss": 6.7421,
      "step": 8804
    },
    {
      "epoch": 0.47718157181571813,
      "step": 8804,
      "training_loss": 6.766119003295898
    },
    {
      "epoch": 0.4772357723577236,
      "step": 8805,
      "training_loss": 6.813048839569092
    },
    {
      "epoch": 0.477289972899729,
      "step": 8806,
      "training_loss": 7.76570987701416
    },
    {
      "epoch": 0.4773441734417344,
      "step": 8807,
      "training_loss": 6.60589599609375
    },
    {
      "epoch": 0.47739837398373985,
      "grad_norm": 29.333566665649414,
      "learning_rate": 1e-05,
      "loss": 6.9877,
      "step": 8808
    },
    {
      "epoch": 0.47739837398373985,
      "step": 8808,
      "training_loss": 7.622342586517334
    },
    {
      "epoch": 0.47745257452574524,
      "step": 8809,
      "training_loss": 6.590959072113037
    },
    {
      "epoch": 0.4775067750677507,
      "step": 8810,
      "training_loss": 6.975590229034424
    },
    {
      "epoch": 0.4775609756097561,
      "step": 8811,
      "training_loss": 5.2090864181518555
    },
    {
      "epoch": 0.4776151761517615,
      "grad_norm": 20.340665817260742,
      "learning_rate": 1e-05,
      "loss": 6.5995,
      "step": 8812
    },
    {
      "epoch": 0.4776151761517615,
      "step": 8812,
      "training_loss": 5.7463226318359375
    },
    {
      "epoch": 0.4776693766937669,
      "step": 8813,
      "training_loss": 6.36509895324707
    },
    {
      "epoch": 0.47772357723577236,
      "step": 8814,
      "training_loss": 7.382028579711914
    },
    {
      "epoch": 0.4777777777777778,
      "step": 8815,
      "training_loss": 7.809224605560303
    },
    {
      "epoch": 0.4778319783197832,
      "grad_norm": 20.736373901367188,
      "learning_rate": 1e-05,
      "loss": 6.8257,
      "step": 8816
    },
    {
      "epoch": 0.4778319783197832,
      "step": 8816,
      "training_loss": 3.534034013748169
    },
    {
      "epoch": 0.47788617886178864,
      "step": 8817,
      "training_loss": 7.384912967681885
    },
    {
      "epoch": 0.477940379403794,
      "step": 8818,
      "training_loss": 6.5061869621276855
    },
    {
      "epoch": 0.47799457994579947,
      "step": 8819,
      "training_loss": 6.688403606414795
    },
    {
      "epoch": 0.47804878048780486,
      "grad_norm": 33.206565856933594,
      "learning_rate": 1e-05,
      "loss": 6.0284,
      "step": 8820
    },
    {
      "epoch": 0.47804878048780486,
      "step": 8820,
      "training_loss": 5.8532395362854
    },
    {
      "epoch": 0.4781029810298103,
      "step": 8821,
      "training_loss": 4.454146862030029
    },
    {
      "epoch": 0.4781571815718157,
      "step": 8822,
      "training_loss": 7.356954097747803
    },
    {
      "epoch": 0.47821138211382114,
      "step": 8823,
      "training_loss": 5.035706043243408
    },
    {
      "epoch": 0.4782655826558266,
      "grad_norm": 27.481599807739258,
      "learning_rate": 1e-05,
      "loss": 5.675,
      "step": 8824
    },
    {
      "epoch": 0.4782655826558266,
      "step": 8824,
      "training_loss": 7.229004383087158
    },
    {
      "epoch": 0.47831978319783197,
      "step": 8825,
      "training_loss": 6.932861328125
    },
    {
      "epoch": 0.4783739837398374,
      "step": 8826,
      "training_loss": 3.975011110305786
    },
    {
      "epoch": 0.4784281842818428,
      "step": 8827,
      "training_loss": 5.7658371925354
    },
    {
      "epoch": 0.47848238482384825,
      "grad_norm": 28.180753707885742,
      "learning_rate": 1e-05,
      "loss": 5.9757,
      "step": 8828
    },
    {
      "epoch": 0.47848238482384825,
      "step": 8828,
      "training_loss": 6.184800148010254
    },
    {
      "epoch": 0.47853658536585364,
      "step": 8829,
      "training_loss": 6.154801368713379
    },
    {
      "epoch": 0.4785907859078591,
      "step": 8830,
      "training_loss": 6.420965194702148
    },
    {
      "epoch": 0.4786449864498645,
      "step": 8831,
      "training_loss": 7.134716987609863
    },
    {
      "epoch": 0.4786991869918699,
      "grad_norm": 30.857315063476562,
      "learning_rate": 1e-05,
      "loss": 6.4738,
      "step": 8832
    },
    {
      "epoch": 0.4786991869918699,
      "step": 8832,
      "training_loss": 6.055235385894775
    },
    {
      "epoch": 0.47875338753387536,
      "step": 8833,
      "training_loss": 7.178245544433594
    },
    {
      "epoch": 0.47880758807588075,
      "step": 8834,
      "training_loss": 6.105220317840576
    },
    {
      "epoch": 0.4788617886178862,
      "step": 8835,
      "training_loss": 7.445420265197754
    },
    {
      "epoch": 0.4789159891598916,
      "grad_norm": 27.689594268798828,
      "learning_rate": 1e-05,
      "loss": 6.696,
      "step": 8836
    },
    {
      "epoch": 0.4789159891598916,
      "step": 8836,
      "training_loss": 5.635678291320801
    },
    {
      "epoch": 0.47897018970189703,
      "step": 8837,
      "training_loss": 6.7795820236206055
    },
    {
      "epoch": 0.4790243902439024,
      "step": 8838,
      "training_loss": 5.87316370010376
    },
    {
      "epoch": 0.47907859078590787,
      "step": 8839,
      "training_loss": 8.226849555969238
    },
    {
      "epoch": 0.47913279132791325,
      "grad_norm": 46.6641731262207,
      "learning_rate": 1e-05,
      "loss": 6.6288,
      "step": 8840
    },
    {
      "epoch": 0.47913279132791325,
      "step": 8840,
      "training_loss": 6.299983024597168
    },
    {
      "epoch": 0.4791869918699187,
      "step": 8841,
      "training_loss": 4.951182842254639
    },
    {
      "epoch": 0.47924119241192414,
      "step": 8842,
      "training_loss": 7.474847793579102
    },
    {
      "epoch": 0.47929539295392953,
      "step": 8843,
      "training_loss": 6.5591325759887695
    },
    {
      "epoch": 0.479349593495935,
      "grad_norm": 18.284961700439453,
      "learning_rate": 1e-05,
      "loss": 6.3213,
      "step": 8844
    },
    {
      "epoch": 0.479349593495935,
      "step": 8844,
      "training_loss": 5.744426250457764
    },
    {
      "epoch": 0.47940379403794037,
      "step": 8845,
      "training_loss": 6.01138162612915
    },
    {
      "epoch": 0.4794579945799458,
      "step": 8846,
      "training_loss": 7.191774845123291
    },
    {
      "epoch": 0.4795121951219512,
      "step": 8847,
      "training_loss": 6.814273357391357
    },
    {
      "epoch": 0.47956639566395665,
      "grad_norm": 16.893808364868164,
      "learning_rate": 1e-05,
      "loss": 6.4405,
      "step": 8848
    },
    {
      "epoch": 0.47956639566395665,
      "step": 8848,
      "training_loss": 7.210080146789551
    },
    {
      "epoch": 0.47962059620596204,
      "step": 8849,
      "training_loss": 6.066402912139893
    },
    {
      "epoch": 0.4796747967479675,
      "step": 8850,
      "training_loss": 6.1848063468933105
    },
    {
      "epoch": 0.4797289972899729,
      "step": 8851,
      "training_loss": 7.017498970031738
    },
    {
      "epoch": 0.4797831978319783,
      "grad_norm": 24.132061004638672,
      "learning_rate": 1e-05,
      "loss": 6.6197,
      "step": 8852
    },
    {
      "epoch": 0.4797831978319783,
      "step": 8852,
      "training_loss": 5.860453128814697
    },
    {
      "epoch": 0.47983739837398376,
      "step": 8853,
      "training_loss": 4.681447982788086
    },
    {
      "epoch": 0.47989159891598915,
      "step": 8854,
      "training_loss": 6.175044536590576
    },
    {
      "epoch": 0.4799457994579946,
      "step": 8855,
      "training_loss": 7.442633628845215
    },
    {
      "epoch": 0.48,
      "grad_norm": 27.961538314819336,
      "learning_rate": 1e-05,
      "loss": 6.0399,
      "step": 8856
    },
    {
      "epoch": 0.48,
      "step": 8856,
      "training_loss": 6.32319450378418
    },
    {
      "epoch": 0.4800542005420054,
      "step": 8857,
      "training_loss": 6.850034713745117
    },
    {
      "epoch": 0.4801084010840108,
      "step": 8858,
      "training_loss": 6.996112823486328
    },
    {
      "epoch": 0.48016260162601626,
      "step": 8859,
      "training_loss": 6.6716742515563965
    },
    {
      "epoch": 0.4802168021680217,
      "grad_norm": 35.995819091796875,
      "learning_rate": 1e-05,
      "loss": 6.7103,
      "step": 8860
    },
    {
      "epoch": 0.4802168021680217,
      "step": 8860,
      "training_loss": 7.316413402557373
    },
    {
      "epoch": 0.4802710027100271,
      "step": 8861,
      "training_loss": 4.848312854766846
    },
    {
      "epoch": 0.48032520325203254,
      "step": 8862,
      "training_loss": 6.952425956726074
    },
    {
      "epoch": 0.48037940379403793,
      "step": 8863,
      "training_loss": 6.888877868652344
    },
    {
      "epoch": 0.4804336043360434,
      "grad_norm": 24.0025577545166,
      "learning_rate": 1e-05,
      "loss": 6.5015,
      "step": 8864
    },
    {
      "epoch": 0.4804336043360434,
      "step": 8864,
      "training_loss": 6.4479169845581055
    },
    {
      "epoch": 0.48048780487804876,
      "step": 8865,
      "training_loss": 6.192218780517578
    },
    {
      "epoch": 0.4805420054200542,
      "step": 8866,
      "training_loss": 7.284388065338135
    },
    {
      "epoch": 0.4805962059620596,
      "step": 8867,
      "training_loss": 6.524163246154785
    },
    {
      "epoch": 0.48065040650406504,
      "grad_norm": 31.674741744995117,
      "learning_rate": 1e-05,
      "loss": 6.6122,
      "step": 8868
    },
    {
      "epoch": 0.48065040650406504,
      "step": 8868,
      "training_loss": 6.096820831298828
    },
    {
      "epoch": 0.4807046070460705,
      "step": 8869,
      "training_loss": 3.947031259536743
    },
    {
      "epoch": 0.4807588075880759,
      "step": 8870,
      "training_loss": 6.527018070220947
    },
    {
      "epoch": 0.4808130081300813,
      "step": 8871,
      "training_loss": 6.7894487380981445
    },
    {
      "epoch": 0.4808672086720867,
      "grad_norm": 46.60946273803711,
      "learning_rate": 1e-05,
      "loss": 5.8401,
      "step": 8872
    },
    {
      "epoch": 0.4808672086720867,
      "step": 8872,
      "training_loss": 5.6705403327941895
    },
    {
      "epoch": 0.48092140921409215,
      "step": 8873,
      "training_loss": 5.503927230834961
    },
    {
      "epoch": 0.48097560975609754,
      "step": 8874,
      "training_loss": 7.493914604187012
    },
    {
      "epoch": 0.481029810298103,
      "step": 8875,
      "training_loss": 5.308805465698242
    },
    {
      "epoch": 0.4810840108401084,
      "grad_norm": 34.570045471191406,
      "learning_rate": 1e-05,
      "loss": 5.9943,
      "step": 8876
    },
    {
      "epoch": 0.4810840108401084,
      "step": 8876,
      "training_loss": 8.09554672241211
    },
    {
      "epoch": 0.4811382113821138,
      "step": 8877,
      "training_loss": 7.276968479156494
    },
    {
      "epoch": 0.48119241192411927,
      "step": 8878,
      "training_loss": 7.028543472290039
    },
    {
      "epoch": 0.48124661246612466,
      "step": 8879,
      "training_loss": 5.171832084655762
    },
    {
      "epoch": 0.4813008130081301,
      "grad_norm": 43.98770523071289,
      "learning_rate": 1e-05,
      "loss": 6.8932,
      "step": 8880
    },
    {
      "epoch": 0.4813008130081301,
      "step": 8880,
      "training_loss": 7.378412246704102
    },
    {
      "epoch": 0.4813550135501355,
      "step": 8881,
      "training_loss": 6.444746494293213
    },
    {
      "epoch": 0.48140921409214094,
      "step": 8882,
      "training_loss": 6.8459086418151855
    },
    {
      "epoch": 0.4814634146341463,
      "step": 8883,
      "training_loss": 6.308406352996826
    },
    {
      "epoch": 0.48151761517615177,
      "grad_norm": 25.283098220825195,
      "learning_rate": 1e-05,
      "loss": 6.7444,
      "step": 8884
    },
    {
      "epoch": 0.48151761517615177,
      "step": 8884,
      "training_loss": 6.412582874298096
    },
    {
      "epoch": 0.48157181571815716,
      "step": 8885,
      "training_loss": 7.507418632507324
    },
    {
      "epoch": 0.4816260162601626,
      "step": 8886,
      "training_loss": 6.95683479309082
    },
    {
      "epoch": 0.48168021680216805,
      "step": 8887,
      "training_loss": 7.323493957519531
    },
    {
      "epoch": 0.48173441734417344,
      "grad_norm": 37.64848327636719,
      "learning_rate": 1e-05,
      "loss": 7.0501,
      "step": 8888
    },
    {
      "epoch": 0.48173441734417344,
      "step": 8888,
      "training_loss": 3.6900949478149414
    },
    {
      "epoch": 0.4817886178861789,
      "step": 8889,
      "training_loss": 6.984001159667969
    },
    {
      "epoch": 0.48184281842818427,
      "step": 8890,
      "training_loss": 7.962331295013428
    },
    {
      "epoch": 0.4818970189701897,
      "step": 8891,
      "training_loss": 5.778239727020264
    },
    {
      "epoch": 0.4819512195121951,
      "grad_norm": 29.00046157836914,
      "learning_rate": 1e-05,
      "loss": 6.1037,
      "step": 8892
    },
    {
      "epoch": 0.4819512195121951,
      "step": 8892,
      "training_loss": 5.331688404083252
    },
    {
      "epoch": 0.48200542005420055,
      "step": 8893,
      "training_loss": 6.8526482582092285
    },
    {
      "epoch": 0.48205962059620594,
      "step": 8894,
      "training_loss": 7.547780513763428
    },
    {
      "epoch": 0.4821138211382114,
      "step": 8895,
      "training_loss": 5.605044364929199
    },
    {
      "epoch": 0.48216802168021683,
      "grad_norm": 28.239479064941406,
      "learning_rate": 1e-05,
      "loss": 6.3343,
      "step": 8896
    },
    {
      "epoch": 0.48216802168021683,
      "step": 8896,
      "training_loss": 7.46440315246582
    },
    {
      "epoch": 0.4822222222222222,
      "step": 8897,
      "training_loss": 5.5891547203063965
    },
    {
      "epoch": 0.48227642276422766,
      "step": 8898,
      "training_loss": 7.387080669403076
    },
    {
      "epoch": 0.48233062330623305,
      "step": 8899,
      "training_loss": 7.2711591720581055
    },
    {
      "epoch": 0.4823848238482385,
      "grad_norm": 24.548261642456055,
      "learning_rate": 1e-05,
      "loss": 6.9279,
      "step": 8900
    },
    {
      "epoch": 0.4823848238482385,
      "step": 8900,
      "training_loss": 6.821017742156982
    },
    {
      "epoch": 0.4824390243902439,
      "step": 8901,
      "training_loss": 7.3133649826049805
    },
    {
      "epoch": 0.48249322493224933,
      "step": 8902,
      "training_loss": 5.19185209274292
    },
    {
      "epoch": 0.4825474254742547,
      "step": 8903,
      "training_loss": 6.828348159790039
    },
    {
      "epoch": 0.48260162601626017,
      "grad_norm": 25.60003089904785,
      "learning_rate": 1e-05,
      "loss": 6.5386,
      "step": 8904
    },
    {
      "epoch": 0.48260162601626017,
      "step": 8904,
      "training_loss": 8.165124893188477
    },
    {
      "epoch": 0.48265582655826555,
      "step": 8905,
      "training_loss": 6.218834400177002
    },
    {
      "epoch": 0.482710027100271,
      "step": 8906,
      "training_loss": 6.809776782989502
    },
    {
      "epoch": 0.48276422764227644,
      "step": 8907,
      "training_loss": 6.523629188537598
    },
    {
      "epoch": 0.48281842818428183,
      "grad_norm": 21.730079650878906,
      "learning_rate": 1e-05,
      "loss": 6.9293,
      "step": 8908
    },
    {
      "epoch": 0.48281842818428183,
      "step": 8908,
      "training_loss": 7.963545799255371
    },
    {
      "epoch": 0.4828726287262873,
      "step": 8909,
      "training_loss": 7.679685592651367
    },
    {
      "epoch": 0.48292682926829267,
      "step": 8910,
      "training_loss": 6.6245012283325195
    },
    {
      "epoch": 0.4829810298102981,
      "step": 8911,
      "training_loss": 6.892821788787842
    },
    {
      "epoch": 0.4830352303523035,
      "grad_norm": 21.655101776123047,
      "learning_rate": 1e-05,
      "loss": 7.2901,
      "step": 8912
    },
    {
      "epoch": 0.4830352303523035,
      "step": 8912,
      "training_loss": 6.231719970703125
    },
    {
      "epoch": 0.48308943089430895,
      "step": 8913,
      "training_loss": 6.977208137512207
    },
    {
      "epoch": 0.48314363143631434,
      "step": 8914,
      "training_loss": 7.04428243637085
    },
    {
      "epoch": 0.4831978319783198,
      "step": 8915,
      "training_loss": 5.996301651000977
    },
    {
      "epoch": 0.4832520325203252,
      "grad_norm": 24.78044891357422,
      "learning_rate": 1e-05,
      "loss": 6.5624,
      "step": 8916
    },
    {
      "epoch": 0.4832520325203252,
      "step": 8916,
      "training_loss": 5.717746734619141
    },
    {
      "epoch": 0.4833062330623306,
      "step": 8917,
      "training_loss": 6.166818618774414
    },
    {
      "epoch": 0.48336043360433606,
      "step": 8918,
      "training_loss": 6.918550968170166
    },
    {
      "epoch": 0.48341463414634145,
      "step": 8919,
      "training_loss": 6.049692153930664
    },
    {
      "epoch": 0.4834688346883469,
      "grad_norm": 29.95161247253418,
      "learning_rate": 1e-05,
      "loss": 6.2132,
      "step": 8920
    },
    {
      "epoch": 0.4834688346883469,
      "step": 8920,
      "training_loss": 7.7176079750061035
    },
    {
      "epoch": 0.4835230352303523,
      "step": 8921,
      "training_loss": 6.805065155029297
    },
    {
      "epoch": 0.4835772357723577,
      "step": 8922,
      "training_loss": 6.894712924957275
    },
    {
      "epoch": 0.4836314363143631,
      "step": 8923,
      "training_loss": 7.581709861755371
    },
    {
      "epoch": 0.48368563685636856,
      "grad_norm": 37.61424255371094,
      "learning_rate": 1e-05,
      "loss": 7.2498,
      "step": 8924
    },
    {
      "epoch": 0.48368563685636856,
      "step": 8924,
      "training_loss": 6.200612545013428
    },
    {
      "epoch": 0.483739837398374,
      "step": 8925,
      "training_loss": 4.083110332489014
    },
    {
      "epoch": 0.4837940379403794,
      "step": 8926,
      "training_loss": 5.279655933380127
    },
    {
      "epoch": 0.48384823848238484,
      "step": 8927,
      "training_loss": 5.652878761291504
    },
    {
      "epoch": 0.48390243902439023,
      "grad_norm": 23.751522064208984,
      "learning_rate": 1e-05,
      "loss": 5.3041,
      "step": 8928
    },
    {
      "epoch": 0.48390243902439023,
      "step": 8928,
      "training_loss": 6.751635551452637
    },
    {
      "epoch": 0.4839566395663957,
      "step": 8929,
      "training_loss": 7.439648151397705
    },
    {
      "epoch": 0.48401084010840106,
      "step": 8930,
      "training_loss": 6.658985137939453
    },
    {
      "epoch": 0.4840650406504065,
      "step": 8931,
      "training_loss": 9.676735877990723
    },
    {
      "epoch": 0.4841192411924119,
      "grad_norm": 40.969451904296875,
      "learning_rate": 1e-05,
      "loss": 7.6318,
      "step": 8932
    },
    {
      "epoch": 0.4841192411924119,
      "step": 8932,
      "training_loss": 7.38181734085083
    },
    {
      "epoch": 0.48417344173441734,
      "step": 8933,
      "training_loss": 7.839977264404297
    },
    {
      "epoch": 0.4842276422764228,
      "step": 8934,
      "training_loss": 7.318114280700684
    },
    {
      "epoch": 0.4842818428184282,
      "step": 8935,
      "training_loss": 6.29379415512085
    },
    {
      "epoch": 0.4843360433604336,
      "grad_norm": 29.59566879272461,
      "learning_rate": 1e-05,
      "loss": 7.2084,
      "step": 8936
    },
    {
      "epoch": 0.4843360433604336,
      "step": 8936,
      "training_loss": 6.636831283569336
    },
    {
      "epoch": 0.484390243902439,
      "step": 8937,
      "training_loss": 6.400794982910156
    },
    {
      "epoch": 0.48444444444444446,
      "step": 8938,
      "training_loss": 5.351787567138672
    },
    {
      "epoch": 0.48449864498644984,
      "step": 8939,
      "training_loss": 7.034098148345947
    },
    {
      "epoch": 0.4845528455284553,
      "grad_norm": 19.452600479125977,
      "learning_rate": 1e-05,
      "loss": 6.3559,
      "step": 8940
    },
    {
      "epoch": 0.4845528455284553,
      "step": 8940,
      "training_loss": 6.814493656158447
    },
    {
      "epoch": 0.4846070460704607,
      "step": 8941,
      "training_loss": 6.033960819244385
    },
    {
      "epoch": 0.4846612466124661,
      "step": 8942,
      "training_loss": 6.951189041137695
    },
    {
      "epoch": 0.48471544715447157,
      "step": 8943,
      "training_loss": 8.422263145446777
    },
    {
      "epoch": 0.48476964769647696,
      "grad_norm": 42.1578254699707,
      "learning_rate": 1e-05,
      "loss": 7.0555,
      "step": 8944
    },
    {
      "epoch": 0.48476964769647696,
      "step": 8944,
      "training_loss": 7.6651997566223145
    },
    {
      "epoch": 0.4848238482384824,
      "step": 8945,
      "training_loss": 7.701375484466553
    },
    {
      "epoch": 0.4848780487804878,
      "step": 8946,
      "training_loss": 6.653533935546875
    },
    {
      "epoch": 0.48493224932249324,
      "step": 8947,
      "training_loss": 6.82818078994751
    },
    {
      "epoch": 0.4849864498644986,
      "grad_norm": 14.794265747070312,
      "learning_rate": 1e-05,
      "loss": 7.2121,
      "step": 8948
    },
    {
      "epoch": 0.4849864498644986,
      "step": 8948,
      "training_loss": 6.569118976593018
    },
    {
      "epoch": 0.48504065040650407,
      "step": 8949,
      "training_loss": 5.136620044708252
    },
    {
      "epoch": 0.48509485094850946,
      "step": 8950,
      "training_loss": 6.920426368713379
    },
    {
      "epoch": 0.4851490514905149,
      "step": 8951,
      "training_loss": 5.695066452026367
    },
    {
      "epoch": 0.48520325203252035,
      "grad_norm": 25.692445755004883,
      "learning_rate": 1e-05,
      "loss": 6.0803,
      "step": 8952
    },
    {
      "epoch": 0.48520325203252035,
      "step": 8952,
      "training_loss": 7.026833534240723
    },
    {
      "epoch": 0.48525745257452574,
      "step": 8953,
      "training_loss": 5.443542003631592
    },
    {
      "epoch": 0.4853116531165312,
      "step": 8954,
      "training_loss": 6.681685447692871
    },
    {
      "epoch": 0.4853658536585366,
      "step": 8955,
      "training_loss": 6.7163004875183105
    },
    {
      "epoch": 0.485420054200542,
      "grad_norm": 36.96062088012695,
      "learning_rate": 1e-05,
      "loss": 6.4671,
      "step": 8956
    },
    {
      "epoch": 0.485420054200542,
      "step": 8956,
      "training_loss": 8.157255172729492
    },
    {
      "epoch": 0.4854742547425474,
      "step": 8957,
      "training_loss": 7.324065685272217
    },
    {
      "epoch": 0.48552845528455285,
      "step": 8958,
      "training_loss": 6.51101016998291
    },
    {
      "epoch": 0.48558265582655824,
      "step": 8959,
      "training_loss": 7.022419452667236
    },
    {
      "epoch": 0.4856368563685637,
      "grad_norm": 23.2686767578125,
      "learning_rate": 1e-05,
      "loss": 7.2537,
      "step": 8960
    },
    {
      "epoch": 0.4856368563685637,
      "step": 8960,
      "training_loss": 6.868050575256348
    },
    {
      "epoch": 0.48569105691056913,
      "step": 8961,
      "training_loss": 7.230299949645996
    },
    {
      "epoch": 0.4857452574525745,
      "step": 8962,
      "training_loss": 6.4818010330200195
    },
    {
      "epoch": 0.48579945799457996,
      "step": 8963,
      "training_loss": 7.742203235626221
    },
    {
      "epoch": 0.48585365853658535,
      "grad_norm": 28.08232307434082,
      "learning_rate": 1e-05,
      "loss": 7.0806,
      "step": 8964
    },
    {
      "epoch": 0.48585365853658535,
      "step": 8964,
      "training_loss": 6.799591541290283
    },
    {
      "epoch": 0.4859078590785908,
      "step": 8965,
      "training_loss": 6.836517333984375
    },
    {
      "epoch": 0.4859620596205962,
      "step": 8966,
      "training_loss": 6.027851104736328
    },
    {
      "epoch": 0.48601626016260163,
      "step": 8967,
      "training_loss": 6.296720504760742
    },
    {
      "epoch": 0.486070460704607,
      "grad_norm": 27.50383758544922,
      "learning_rate": 1e-05,
      "loss": 6.4902,
      "step": 8968
    },
    {
      "epoch": 0.486070460704607,
      "step": 8968,
      "training_loss": 5.985729217529297
    },
    {
      "epoch": 0.48612466124661247,
      "step": 8969,
      "training_loss": 7.120241165161133
    },
    {
      "epoch": 0.4861788617886179,
      "step": 8970,
      "training_loss": 6.883100509643555
    },
    {
      "epoch": 0.4862330623306233,
      "step": 8971,
      "training_loss": 6.217647552490234
    },
    {
      "epoch": 0.48628726287262874,
      "grad_norm": 22.85759925842285,
      "learning_rate": 1e-05,
      "loss": 6.5517,
      "step": 8972
    },
    {
      "epoch": 0.48628726287262874,
      "step": 8972,
      "training_loss": 7.659177780151367
    },
    {
      "epoch": 0.48634146341463413,
      "step": 8973,
      "training_loss": 7.330987453460693
    },
    {
      "epoch": 0.4863956639566396,
      "step": 8974,
      "training_loss": 6.453711032867432
    },
    {
      "epoch": 0.48644986449864497,
      "step": 8975,
      "training_loss": 4.380496025085449
    },
    {
      "epoch": 0.4865040650406504,
      "grad_norm": 81.31118774414062,
      "learning_rate": 1e-05,
      "loss": 6.4561,
      "step": 8976
    },
    {
      "epoch": 0.4865040650406504,
      "step": 8976,
      "training_loss": 6.417230606079102
    },
    {
      "epoch": 0.4865582655826558,
      "step": 8977,
      "training_loss": 5.965674877166748
    },
    {
      "epoch": 0.48661246612466125,
      "step": 8978,
      "training_loss": 6.823566436767578
    },
    {
      "epoch": 0.4866666666666667,
      "step": 8979,
      "training_loss": 5.553826808929443
    },
    {
      "epoch": 0.4867208672086721,
      "grad_norm": 24.152860641479492,
      "learning_rate": 1e-05,
      "loss": 6.1901,
      "step": 8980
    },
    {
      "epoch": 0.4867208672086721,
      "step": 8980,
      "training_loss": 7.024099826812744
    },
    {
      "epoch": 0.4867750677506775,
      "step": 8981,
      "training_loss": 5.752742290496826
    },
    {
      "epoch": 0.4868292682926829,
      "step": 8982,
      "training_loss": 7.351658821105957
    },
    {
      "epoch": 0.48688346883468836,
      "step": 8983,
      "training_loss": 3.898818016052246
    },
    {
      "epoch": 0.48693766937669375,
      "grad_norm": 37.06403732299805,
      "learning_rate": 1e-05,
      "loss": 6.0068,
      "step": 8984
    },
    {
      "epoch": 0.48693766937669375,
      "step": 8984,
      "training_loss": 5.605304718017578
    },
    {
      "epoch": 0.4869918699186992,
      "step": 8985,
      "training_loss": 6.29272985458374
    },
    {
      "epoch": 0.4870460704607046,
      "step": 8986,
      "training_loss": 7.5559539794921875
    },
    {
      "epoch": 0.48710027100271003,
      "step": 8987,
      "training_loss": 5.026524543762207
    },
    {
      "epoch": 0.4871544715447155,
      "grad_norm": 43.95143127441406,
      "learning_rate": 1e-05,
      "loss": 6.1201,
      "step": 8988
    },
    {
      "epoch": 0.4871544715447155,
      "step": 8988,
      "training_loss": 6.382904529571533
    },
    {
      "epoch": 0.48720867208672086,
      "step": 8989,
      "training_loss": 6.946070671081543
    },
    {
      "epoch": 0.4872628726287263,
      "step": 8990,
      "training_loss": 7.629547595977783
    },
    {
      "epoch": 0.4873170731707317,
      "step": 8991,
      "training_loss": 6.605504035949707
    },
    {
      "epoch": 0.48737127371273714,
      "grad_norm": 25.543476104736328,
      "learning_rate": 1e-05,
      "loss": 6.891,
      "step": 8992
    },
    {
      "epoch": 0.48737127371273714,
      "step": 8992,
      "training_loss": 7.312286376953125
    },
    {
      "epoch": 0.48742547425474253,
      "step": 8993,
      "training_loss": 6.2604804039001465
    },
    {
      "epoch": 0.487479674796748,
      "step": 8994,
      "training_loss": 8.216614723205566
    },
    {
      "epoch": 0.48753387533875336,
      "step": 8995,
      "training_loss": 6.713452339172363
    },
    {
      "epoch": 0.4875880758807588,
      "grad_norm": 41.05596923828125,
      "learning_rate": 1e-05,
      "loss": 7.1257,
      "step": 8996
    },
    {
      "epoch": 0.4875880758807588,
      "step": 8996,
      "training_loss": 7.573590278625488
    },
    {
      "epoch": 0.48764227642276425,
      "step": 8997,
      "training_loss": 6.263689041137695
    },
    {
      "epoch": 0.48769647696476964,
      "step": 8998,
      "training_loss": 6.014900207519531
    },
    {
      "epoch": 0.4877506775067751,
      "step": 8999,
      "training_loss": 5.735995769500732
    },
    {
      "epoch": 0.4878048780487805,
      "grad_norm": 28.354393005371094,
      "learning_rate": 1e-05,
      "loss": 6.397,
      "step": 9000
    },
    {
      "epoch": 0.4878048780487805,
      "step": 9000,
      "training_loss": 5.411006450653076
    },
    {
      "epoch": 0.4878590785907859,
      "step": 9001,
      "training_loss": 6.419164180755615
    },
    {
      "epoch": 0.4879132791327913,
      "step": 9002,
      "training_loss": 7.2653608322143555
    },
    {
      "epoch": 0.48796747967479676,
      "step": 9003,
      "training_loss": 7.829456329345703
    },
    {
      "epoch": 0.48802168021680215,
      "grad_norm": 27.69310188293457,
      "learning_rate": 1e-05,
      "loss": 6.7312,
      "step": 9004
    },
    {
      "epoch": 0.48802168021680215,
      "step": 9004,
      "training_loss": 5.48872184753418
    },
    {
      "epoch": 0.4880758807588076,
      "step": 9005,
      "training_loss": 6.120251178741455
    },
    {
      "epoch": 0.48813008130081303,
      "step": 9006,
      "training_loss": 8.017509460449219
    },
    {
      "epoch": 0.4881842818428184,
      "step": 9007,
      "training_loss": 6.924993991851807
    },
    {
      "epoch": 0.48823848238482387,
      "grad_norm": 21.918413162231445,
      "learning_rate": 1e-05,
      "loss": 6.6379,
      "step": 9008
    },
    {
      "epoch": 0.48823848238482387,
      "step": 9008,
      "training_loss": 5.7804341316223145
    },
    {
      "epoch": 0.48829268292682926,
      "step": 9009,
      "training_loss": 6.97171688079834
    },
    {
      "epoch": 0.4883468834688347,
      "step": 9010,
      "training_loss": 7.161952495574951
    },
    {
      "epoch": 0.4884010840108401,
      "step": 9011,
      "training_loss": 6.133023262023926
    },
    {
      "epoch": 0.48845528455284554,
      "grad_norm": 39.06163024902344,
      "learning_rate": 1e-05,
      "loss": 6.5118,
      "step": 9012
    },
    {
      "epoch": 0.48845528455284554,
      "step": 9012,
      "training_loss": 7.415543556213379
    },
    {
      "epoch": 0.4885094850948509,
      "step": 9013,
      "training_loss": 6.4871416091918945
    },
    {
      "epoch": 0.48856368563685637,
      "step": 9014,
      "training_loss": 6.876533031463623
    },
    {
      "epoch": 0.4886178861788618,
      "step": 9015,
      "training_loss": 5.862724304199219
    },
    {
      "epoch": 0.4886720867208672,
      "grad_norm": 27.88146209716797,
      "learning_rate": 1e-05,
      "loss": 6.6605,
      "step": 9016
    },
    {
      "epoch": 0.4886720867208672,
      "step": 9016,
      "training_loss": 4.2598795890808105
    },
    {
      "epoch": 0.48872628726287265,
      "step": 9017,
      "training_loss": 6.7691802978515625
    },
    {
      "epoch": 0.48878048780487804,
      "step": 9018,
      "training_loss": 6.4207587242126465
    },
    {
      "epoch": 0.4888346883468835,
      "step": 9019,
      "training_loss": 7.269820690155029
    },
    {
      "epoch": 0.4888888888888889,
      "grad_norm": 37.92618942260742,
      "learning_rate": 1e-05,
      "loss": 6.1799,
      "step": 9020
    },
    {
      "epoch": 0.4888888888888889,
      "step": 9020,
      "training_loss": 9.203091621398926
    },
    {
      "epoch": 0.4889430894308943,
      "step": 9021,
      "training_loss": 5.785216808319092
    },
    {
      "epoch": 0.4889972899728997,
      "step": 9022,
      "training_loss": 5.017086505889893
    },
    {
      "epoch": 0.48905149051490515,
      "step": 9023,
      "training_loss": 5.163398265838623
    },
    {
      "epoch": 0.4891056910569106,
      "grad_norm": 30.051837921142578,
      "learning_rate": 1e-05,
      "loss": 6.2922,
      "step": 9024
    },
    {
      "epoch": 0.4891056910569106,
      "step": 9024,
      "training_loss": 6.376031875610352
    },
    {
      "epoch": 0.489159891598916,
      "step": 9025,
      "training_loss": 6.919240474700928
    },
    {
      "epoch": 0.48921409214092143,
      "step": 9026,
      "training_loss": 5.74926233291626
    },
    {
      "epoch": 0.4892682926829268,
      "step": 9027,
      "training_loss": 6.136116981506348
    },
    {
      "epoch": 0.48932249322493226,
      "grad_norm": 19.560897827148438,
      "learning_rate": 1e-05,
      "loss": 6.2952,
      "step": 9028
    },
    {
      "epoch": 0.48932249322493226,
      "step": 9028,
      "training_loss": 6.721078872680664
    },
    {
      "epoch": 0.48937669376693765,
      "step": 9029,
      "training_loss": 5.249762535095215
    },
    {
      "epoch": 0.4894308943089431,
      "step": 9030,
      "training_loss": 7.096804141998291
    },
    {
      "epoch": 0.4894850948509485,
      "step": 9031,
      "training_loss": 5.672016143798828
    },
    {
      "epoch": 0.48953929539295393,
      "grad_norm": 37.36707305908203,
      "learning_rate": 1e-05,
      "loss": 6.1849,
      "step": 9032
    },
    {
      "epoch": 0.48953929539295393,
      "step": 9032,
      "training_loss": 6.955079555511475
    },
    {
      "epoch": 0.4895934959349593,
      "step": 9033,
      "training_loss": 6.838584899902344
    },
    {
      "epoch": 0.48964769647696477,
      "step": 9034,
      "training_loss": 7.567968845367432
    },
    {
      "epoch": 0.4897018970189702,
      "step": 9035,
      "training_loss": 5.391059398651123
    },
    {
      "epoch": 0.4897560975609756,
      "grad_norm": 23.337017059326172,
      "learning_rate": 1e-05,
      "loss": 6.6882,
      "step": 9036
    },
    {
      "epoch": 0.4897560975609756,
      "step": 9036,
      "training_loss": 6.905191421508789
    },
    {
      "epoch": 0.48981029810298105,
      "step": 9037,
      "training_loss": 6.5586748123168945
    },
    {
      "epoch": 0.48986449864498643,
      "step": 9038,
      "training_loss": 4.580617427825928
    },
    {
      "epoch": 0.4899186991869919,
      "step": 9039,
      "training_loss": 6.062633037567139
    },
    {
      "epoch": 0.48997289972899727,
      "grad_norm": 32.888450622558594,
      "learning_rate": 1e-05,
      "loss": 6.0268,
      "step": 9040
    },
    {
      "epoch": 0.48997289972899727,
      "step": 9040,
      "training_loss": 5.858994483947754
    },
    {
      "epoch": 0.4900271002710027,
      "step": 9041,
      "training_loss": 6.20511531829834
    },
    {
      "epoch": 0.4900813008130081,
      "step": 9042,
      "training_loss": 7.104310512542725
    },
    {
      "epoch": 0.49013550135501355,
      "step": 9043,
      "training_loss": 7.063283920288086
    },
    {
      "epoch": 0.490189701897019,
      "grad_norm": 36.94001388549805,
      "learning_rate": 1e-05,
      "loss": 6.5579,
      "step": 9044
    },
    {
      "epoch": 0.490189701897019,
      "step": 9044,
      "training_loss": 7.728265285491943
    },
    {
      "epoch": 0.4902439024390244,
      "step": 9045,
      "training_loss": 5.4694600105285645
    },
    {
      "epoch": 0.4902981029810298,
      "step": 9046,
      "training_loss": 5.401524066925049
    },
    {
      "epoch": 0.4903523035230352,
      "step": 9047,
      "training_loss": 8.578848838806152
    },
    {
      "epoch": 0.49040650406504066,
      "grad_norm": 48.0347785949707,
      "learning_rate": 1e-05,
      "loss": 6.7945,
      "step": 9048
    },
    {
      "epoch": 0.49040650406504066,
      "step": 9048,
      "training_loss": 6.557613372802734
    },
    {
      "epoch": 0.49046070460704605,
      "step": 9049,
      "training_loss": 6.09209680557251
    },
    {
      "epoch": 0.4905149051490515,
      "step": 9050,
      "training_loss": 6.269066333770752
    },
    {
      "epoch": 0.4905691056910569,
      "step": 9051,
      "training_loss": 6.624342918395996
    },
    {
      "epoch": 0.49062330623306233,
      "grad_norm": 31.532155990600586,
      "learning_rate": 1e-05,
      "loss": 6.3858,
      "step": 9052
    },
    {
      "epoch": 0.49062330623306233,
      "step": 9052,
      "training_loss": 6.4948859214782715
    },
    {
      "epoch": 0.4906775067750678,
      "step": 9053,
      "training_loss": 6.735886096954346
    },
    {
      "epoch": 0.49073170731707316,
      "step": 9054,
      "training_loss": 6.97871732711792
    },
    {
      "epoch": 0.4907859078590786,
      "step": 9055,
      "training_loss": 5.869349002838135
    },
    {
      "epoch": 0.490840108401084,
      "grad_norm": 25.419734954833984,
      "learning_rate": 1e-05,
      "loss": 6.5197,
      "step": 9056
    },
    {
      "epoch": 0.490840108401084,
      "step": 9056,
      "training_loss": 6.482371807098389
    },
    {
      "epoch": 0.49089430894308944,
      "step": 9057,
      "training_loss": 8.055088996887207
    },
    {
      "epoch": 0.49094850948509483,
      "step": 9058,
      "training_loss": 5.6500563621521
    },
    {
      "epoch": 0.4910027100271003,
      "step": 9059,
      "training_loss": 6.825674533843994
    },
    {
      "epoch": 0.49105691056910566,
      "grad_norm": 31.765199661254883,
      "learning_rate": 1e-05,
      "loss": 6.7533,
      "step": 9060
    },
    {
      "epoch": 0.49105691056910566,
      "step": 9060,
      "training_loss": 6.816110134124756
    },
    {
      "epoch": 0.4911111111111111,
      "step": 9061,
      "training_loss": 8.611560821533203
    },
    {
      "epoch": 0.49116531165311655,
      "step": 9062,
      "training_loss": 7.0665764808654785
    },
    {
      "epoch": 0.49121951219512194,
      "step": 9063,
      "training_loss": 6.8648552894592285
    },
    {
      "epoch": 0.4912737127371274,
      "grad_norm": 27.961091995239258,
      "learning_rate": 1e-05,
      "loss": 7.3398,
      "step": 9064
    },
    {
      "epoch": 0.4912737127371274,
      "step": 9064,
      "training_loss": 6.9767560958862305
    },
    {
      "epoch": 0.4913279132791328,
      "step": 9065,
      "training_loss": 5.305400371551514
    },
    {
      "epoch": 0.4913821138211382,
      "step": 9066,
      "training_loss": 9.235082626342773
    },
    {
      "epoch": 0.4914363143631436,
      "step": 9067,
      "training_loss": 6.795554161071777
    },
    {
      "epoch": 0.49149051490514906,
      "grad_norm": 19.755842208862305,
      "learning_rate": 1e-05,
      "loss": 7.0782,
      "step": 9068
    },
    {
      "epoch": 0.49149051490514906,
      "step": 9068,
      "training_loss": 6.6036295890808105
    },
    {
      "epoch": 0.49154471544715445,
      "step": 9069,
      "training_loss": 7.223876953125
    },
    {
      "epoch": 0.4915989159891599,
      "step": 9070,
      "training_loss": 7.157607555389404
    },
    {
      "epoch": 0.49165311653116534,
      "step": 9071,
      "training_loss": 7.024926662445068
    },
    {
      "epoch": 0.4917073170731707,
      "grad_norm": 18.185091018676758,
      "learning_rate": 1e-05,
      "loss": 7.0025,
      "step": 9072
    },
    {
      "epoch": 0.4917073170731707,
      "step": 9072,
      "training_loss": 6.475006580352783
    },
    {
      "epoch": 0.49176151761517617,
      "step": 9073,
      "training_loss": 6.652662754058838
    },
    {
      "epoch": 0.49181571815718156,
      "step": 9074,
      "training_loss": 6.968447208404541
    },
    {
      "epoch": 0.491869918699187,
      "step": 9075,
      "training_loss": 5.104265213012695
    },
    {
      "epoch": 0.4919241192411924,
      "grad_norm": 23.952850341796875,
      "learning_rate": 1e-05,
      "loss": 6.3001,
      "step": 9076
    },
    {
      "epoch": 0.4919241192411924,
      "step": 9076,
      "training_loss": 6.99242639541626
    },
    {
      "epoch": 0.49197831978319784,
      "step": 9077,
      "training_loss": 7.268124103546143
    },
    {
      "epoch": 0.4920325203252032,
      "step": 9078,
      "training_loss": 5.964552402496338
    },
    {
      "epoch": 0.49208672086720867,
      "step": 9079,
      "training_loss": 5.946582317352295
    },
    {
      "epoch": 0.4921409214092141,
      "grad_norm": 23.223888397216797,
      "learning_rate": 1e-05,
      "loss": 6.5429,
      "step": 9080
    },
    {
      "epoch": 0.4921409214092141,
      "step": 9080,
      "training_loss": 7.938473224639893
    },
    {
      "epoch": 0.4921951219512195,
      "step": 9081,
      "training_loss": 6.668980121612549
    },
    {
      "epoch": 0.49224932249322495,
      "step": 9082,
      "training_loss": 7.404988765716553
    },
    {
      "epoch": 0.49230352303523034,
      "step": 9083,
      "training_loss": 6.359721660614014
    },
    {
      "epoch": 0.4923577235772358,
      "grad_norm": 18.656591415405273,
      "learning_rate": 1e-05,
      "loss": 7.093,
      "step": 9084
    },
    {
      "epoch": 0.4923577235772358,
      "step": 9084,
      "training_loss": 7.618412017822266
    },
    {
      "epoch": 0.4924119241192412,
      "step": 9085,
      "training_loss": 7.394871711730957
    },
    {
      "epoch": 0.4924661246612466,
      "step": 9086,
      "training_loss": 8.060304641723633
    },
    {
      "epoch": 0.492520325203252,
      "step": 9087,
      "training_loss": 5.649713039398193
    },
    {
      "epoch": 0.49257452574525745,
      "grad_norm": 77.21041870117188,
      "learning_rate": 1e-05,
      "loss": 7.1808,
      "step": 9088
    },
    {
      "epoch": 0.49257452574525745,
      "step": 9088,
      "training_loss": 3.6114327907562256
    },
    {
      "epoch": 0.4926287262872629,
      "step": 9089,
      "training_loss": 4.510251998901367
    },
    {
      "epoch": 0.4926829268292683,
      "step": 9090,
      "training_loss": 7.016951560974121
    },
    {
      "epoch": 0.49273712737127373,
      "step": 9091,
      "training_loss": 6.459357261657715
    },
    {
      "epoch": 0.4927913279132791,
      "grad_norm": 25.942686080932617,
      "learning_rate": 1e-05,
      "loss": 5.3995,
      "step": 9092
    },
    {
      "epoch": 0.4927913279132791,
      "step": 9092,
      "training_loss": 6.570151329040527
    },
    {
      "epoch": 0.49284552845528456,
      "step": 9093,
      "training_loss": 5.466946125030518
    },
    {
      "epoch": 0.49289972899728995,
      "step": 9094,
      "training_loss": 4.5115966796875
    },
    {
      "epoch": 0.4929539295392954,
      "step": 9095,
      "training_loss": 6.3815083503723145
    },
    {
      "epoch": 0.4930081300813008,
      "grad_norm": 44.74559020996094,
      "learning_rate": 1e-05,
      "loss": 5.7326,
      "step": 9096
    },
    {
      "epoch": 0.4930081300813008,
      "step": 9096,
      "training_loss": 5.243258476257324
    },
    {
      "epoch": 0.49306233062330623,
      "step": 9097,
      "training_loss": 5.716365337371826
    },
    {
      "epoch": 0.4931165311653117,
      "step": 9098,
      "training_loss": 5.391549587249756
    },
    {
      "epoch": 0.49317073170731707,
      "step": 9099,
      "training_loss": 6.355703830718994
    },
    {
      "epoch": 0.4932249322493225,
      "grad_norm": 51.289527893066406,
      "learning_rate": 1e-05,
      "loss": 5.6767,
      "step": 9100
    },
    {
      "epoch": 0.4932249322493225,
      "step": 9100,
      "training_loss": 4.863423824310303
    },
    {
      "epoch": 0.4932791327913279,
      "step": 9101,
      "training_loss": 6.463311195373535
    },
    {
      "epoch": 0.49333333333333335,
      "step": 9102,
      "training_loss": 7.7676310539245605
    },
    {
      "epoch": 0.49338753387533874,
      "step": 9103,
      "training_loss": 6.705563068389893
    },
    {
      "epoch": 0.4934417344173442,
      "grad_norm": 18.264423370361328,
      "learning_rate": 1e-05,
      "loss": 6.45,
      "step": 9104
    },
    {
      "epoch": 0.4934417344173442,
      "step": 9104,
      "training_loss": 7.06267786026001
    },
    {
      "epoch": 0.49349593495934957,
      "step": 9105,
      "training_loss": 7.220362186431885
    },
    {
      "epoch": 0.493550135501355,
      "step": 9106,
      "training_loss": 5.640773296356201
    },
    {
      "epoch": 0.49360433604336046,
      "step": 9107,
      "training_loss": 6.4965009689331055
    },
    {
      "epoch": 0.49365853658536585,
      "grad_norm": 27.234582901000977,
      "learning_rate": 1e-05,
      "loss": 6.6051,
      "step": 9108
    },
    {
      "epoch": 0.49365853658536585,
      "step": 9108,
      "training_loss": 11.565091133117676
    },
    {
      "epoch": 0.4937127371273713,
      "step": 9109,
      "training_loss": 8.376219749450684
    },
    {
      "epoch": 0.4937669376693767,
      "step": 9110,
      "training_loss": 7.49989128112793
    },
    {
      "epoch": 0.4938211382113821,
      "step": 9111,
      "training_loss": 7.604022979736328
    },
    {
      "epoch": 0.4938753387533875,
      "grad_norm": 24.352190017700195,
      "learning_rate": 1e-05,
      "loss": 8.7613,
      "step": 9112
    },
    {
      "epoch": 0.4938753387533875,
      "step": 9112,
      "training_loss": 7.546797752380371
    },
    {
      "epoch": 0.49392953929539296,
      "step": 9113,
      "training_loss": 7.2345051765441895
    },
    {
      "epoch": 0.49398373983739835,
      "step": 9114,
      "training_loss": 6.8963236808776855
    },
    {
      "epoch": 0.4940379403794038,
      "step": 9115,
      "training_loss": 7.246772289276123
    },
    {
      "epoch": 0.49409214092140924,
      "grad_norm": 17.484376907348633,
      "learning_rate": 1e-05,
      "loss": 7.2311,
      "step": 9116
    },
    {
      "epoch": 0.49409214092140924,
      "step": 9116,
      "training_loss": 8.382390022277832
    },
    {
      "epoch": 0.49414634146341463,
      "step": 9117,
      "training_loss": 6.5643696784973145
    },
    {
      "epoch": 0.4942005420054201,
      "step": 9118,
      "training_loss": 8.056949615478516
    },
    {
      "epoch": 0.49425474254742546,
      "step": 9119,
      "training_loss": 6.044797897338867
    },
    {
      "epoch": 0.4943089430894309,
      "grad_norm": 24.046031951904297,
      "learning_rate": 1e-05,
      "loss": 7.2621,
      "step": 9120
    },
    {
      "epoch": 0.4943089430894309,
      "step": 9120,
      "training_loss": 6.492442607879639
    },
    {
      "epoch": 0.4943631436314363,
      "step": 9121,
      "training_loss": 6.567230701446533
    },
    {
      "epoch": 0.49441734417344174,
      "step": 9122,
      "training_loss": 6.695854663848877
    },
    {
      "epoch": 0.49447154471544713,
      "step": 9123,
      "training_loss": 4.919946193695068
    },
    {
      "epoch": 0.4945257452574526,
      "grad_norm": 21.186017990112305,
      "learning_rate": 1e-05,
      "loss": 6.1689,
      "step": 9124
    },
    {
      "epoch": 0.4945257452574526,
      "step": 9124,
      "training_loss": 5.519497394561768
    },
    {
      "epoch": 0.494579945799458,
      "step": 9125,
      "training_loss": 6.48455286026001
    },
    {
      "epoch": 0.4946341463414634,
      "step": 9126,
      "training_loss": 5.458519458770752
    },
    {
      "epoch": 0.49468834688346885,
      "step": 9127,
      "training_loss": 7.73160457611084
    },
    {
      "epoch": 0.49474254742547424,
      "grad_norm": 22.027450561523438,
      "learning_rate": 1e-05,
      "loss": 6.2985,
      "step": 9128
    },
    {
      "epoch": 0.49474254742547424,
      "step": 9128,
      "training_loss": 7.11393404006958
    },
    {
      "epoch": 0.4947967479674797,
      "step": 9129,
      "training_loss": 6.130556583404541
    },
    {
      "epoch": 0.4948509485094851,
      "step": 9130,
      "training_loss": 6.759035110473633
    },
    {
      "epoch": 0.4949051490514905,
      "step": 9131,
      "training_loss": 6.211678504943848
    },
    {
      "epoch": 0.4949593495934959,
      "grad_norm": 35.221168518066406,
      "learning_rate": 1e-05,
      "loss": 6.5538,
      "step": 9132
    },
    {
      "epoch": 0.4949593495934959,
      "step": 9132,
      "training_loss": 6.851840496063232
    },
    {
      "epoch": 0.49501355013550136,
      "step": 9133,
      "training_loss": 6.279191017150879
    },
    {
      "epoch": 0.4950677506775068,
      "step": 9134,
      "training_loss": 7.413212299346924
    },
    {
      "epoch": 0.4951219512195122,
      "step": 9135,
      "training_loss": 7.021441459655762
    },
    {
      "epoch": 0.49517615176151764,
      "grad_norm": 22.1071720123291,
      "learning_rate": 1e-05,
      "loss": 6.8914,
      "step": 9136
    },
    {
      "epoch": 0.49517615176151764,
      "step": 9136,
      "training_loss": 7.069622039794922
    },
    {
      "epoch": 0.495230352303523,
      "step": 9137,
      "training_loss": 5.1128315925598145
    },
    {
      "epoch": 0.49528455284552847,
      "step": 9138,
      "training_loss": 6.743166446685791
    },
    {
      "epoch": 0.49533875338753386,
      "step": 9139,
      "training_loss": 6.581573963165283
    },
    {
      "epoch": 0.4953929539295393,
      "grad_norm": 31.762407302856445,
      "learning_rate": 1e-05,
      "loss": 6.3768,
      "step": 9140
    },
    {
      "epoch": 0.4953929539295393,
      "step": 9140,
      "training_loss": 6.296514987945557
    },
    {
      "epoch": 0.4954471544715447,
      "step": 9141,
      "training_loss": 5.681942462921143
    },
    {
      "epoch": 0.49550135501355014,
      "step": 9142,
      "training_loss": 7.35014009475708
    },
    {
      "epoch": 0.4955555555555556,
      "step": 9143,
      "training_loss": 5.24214506149292
    },
    {
      "epoch": 0.49560975609756097,
      "grad_norm": 25.039804458618164,
      "learning_rate": 1e-05,
      "loss": 6.1427,
      "step": 9144
    },
    {
      "epoch": 0.49560975609756097,
      "step": 9144,
      "training_loss": 7.263769626617432
    },
    {
      "epoch": 0.4956639566395664,
      "step": 9145,
      "training_loss": 6.380624294281006
    },
    {
      "epoch": 0.4957181571815718,
      "step": 9146,
      "training_loss": 5.870656967163086
    },
    {
      "epoch": 0.49577235772357725,
      "step": 9147,
      "training_loss": 7.387397289276123
    },
    {
      "epoch": 0.49582655826558264,
      "grad_norm": 37.08298110961914,
      "learning_rate": 1e-05,
      "loss": 6.7256,
      "step": 9148
    },
    {
      "epoch": 0.49582655826558264,
      "step": 9148,
      "training_loss": 6.882152080535889
    },
    {
      "epoch": 0.4958807588075881,
      "step": 9149,
      "training_loss": 5.440623760223389
    },
    {
      "epoch": 0.4959349593495935,
      "step": 9150,
      "training_loss": 4.091825485229492
    },
    {
      "epoch": 0.4959891598915989,
      "step": 9151,
      "training_loss": 5.969109058380127
    },
    {
      "epoch": 0.49604336043360436,
      "grad_norm": 24.545886993408203,
      "learning_rate": 1e-05,
      "loss": 5.5959,
      "step": 9152
    },
    {
      "epoch": 0.49604336043360436,
      "step": 9152,
      "training_loss": 6.964231967926025
    },
    {
      "epoch": 0.49609756097560975,
      "step": 9153,
      "training_loss": 7.133962631225586
    },
    {
      "epoch": 0.4961517615176152,
      "step": 9154,
      "training_loss": 6.957623481750488
    },
    {
      "epoch": 0.4962059620596206,
      "step": 9155,
      "training_loss": 6.957652568817139
    },
    {
      "epoch": 0.49626016260162603,
      "grad_norm": 18.678415298461914,
      "learning_rate": 1e-05,
      "loss": 7.0034,
      "step": 9156
    },
    {
      "epoch": 0.49626016260162603,
      "step": 9156,
      "training_loss": 5.553287982940674
    },
    {
      "epoch": 0.4963143631436314,
      "step": 9157,
      "training_loss": 7.024933338165283
    },
    {
      "epoch": 0.49636856368563687,
      "step": 9158,
      "training_loss": 6.133265972137451
    },
    {
      "epoch": 0.49642276422764225,
      "step": 9159,
      "training_loss": 7.2564311027526855
    },
    {
      "epoch": 0.4964769647696477,
      "grad_norm": 22.57634735107422,
      "learning_rate": 1e-05,
      "loss": 6.492,
      "step": 9160
    },
    {
      "epoch": 0.4964769647696477,
      "step": 9160,
      "training_loss": 5.0311079025268555
    },
    {
      "epoch": 0.4965311653116531,
      "step": 9161,
      "training_loss": 5.824268817901611
    },
    {
      "epoch": 0.49658536585365853,
      "step": 9162,
      "training_loss": 6.4595537185668945
    },
    {
      "epoch": 0.496639566395664,
      "step": 9163,
      "training_loss": 2.998002290725708
    },
    {
      "epoch": 0.49669376693766937,
      "grad_norm": 33.8668098449707,
      "learning_rate": 1e-05,
      "loss": 5.0782,
      "step": 9164
    },
    {
      "epoch": 0.49669376693766937,
      "step": 9164,
      "training_loss": 6.875458240509033
    },
    {
      "epoch": 0.4967479674796748,
      "step": 9165,
      "training_loss": 6.460803508758545
    },
    {
      "epoch": 0.4968021680216802,
      "step": 9166,
      "training_loss": 7.472646713256836
    },
    {
      "epoch": 0.49685636856368565,
      "step": 9167,
      "training_loss": 7.019392013549805
    },
    {
      "epoch": 0.49691056910569104,
      "grad_norm": 34.137474060058594,
      "learning_rate": 1e-05,
      "loss": 6.9571,
      "step": 9168
    },
    {
      "epoch": 0.49691056910569104,
      "step": 9168,
      "training_loss": 4.379890441894531
    },
    {
      "epoch": 0.4969647696476965,
      "step": 9169,
      "training_loss": 7.300665378570557
    },
    {
      "epoch": 0.49701897018970187,
      "step": 9170,
      "training_loss": 6.906923294067383
    },
    {
      "epoch": 0.4970731707317073,
      "step": 9171,
      "training_loss": 5.049377918243408
    },
    {
      "epoch": 0.49712737127371276,
      "grad_norm": 29.656536102294922,
      "learning_rate": 1e-05,
      "loss": 5.9092,
      "step": 9172
    },
    {
      "epoch": 0.49712737127371276,
      "step": 9172,
      "training_loss": 3.6292219161987305
    },
    {
      "epoch": 0.49718157181571815,
      "step": 9173,
      "training_loss": 6.345781326293945
    },
    {
      "epoch": 0.4972357723577236,
      "step": 9174,
      "training_loss": 4.076997756958008
    },
    {
      "epoch": 0.497289972899729,
      "step": 9175,
      "training_loss": 6.911808490753174
    },
    {
      "epoch": 0.4973441734417344,
      "grad_norm": 24.494810104370117,
      "learning_rate": 1e-05,
      "loss": 5.241,
      "step": 9176
    },
    {
      "epoch": 0.4973441734417344,
      "step": 9176,
      "training_loss": 6.7435808181762695
    },
    {
      "epoch": 0.4973983739837398,
      "step": 9177,
      "training_loss": 7.651881694793701
    },
    {
      "epoch": 0.49745257452574526,
      "step": 9178,
      "training_loss": 6.893044471740723
    },
    {
      "epoch": 0.49750677506775065,
      "step": 9179,
      "training_loss": 6.833942413330078
    },
    {
      "epoch": 0.4975609756097561,
      "grad_norm": 51.26289749145508,
      "learning_rate": 1e-05,
      "loss": 7.0306,
      "step": 9180
    },
    {
      "epoch": 0.4975609756097561,
      "step": 9180,
      "training_loss": 5.616428852081299
    },
    {
      "epoch": 0.49761517615176154,
      "step": 9181,
      "training_loss": 4.57697868347168
    },
    {
      "epoch": 0.49766937669376693,
      "step": 9182,
      "training_loss": 5.932775497436523
    },
    {
      "epoch": 0.4977235772357724,
      "step": 9183,
      "training_loss": 6.989792823791504
    },
    {
      "epoch": 0.49777777777777776,
      "grad_norm": 19.8890380859375,
      "learning_rate": 1e-05,
      "loss": 5.779,
      "step": 9184
    },
    {
      "epoch": 0.49777777777777776,
      "step": 9184,
      "training_loss": 6.257388591766357
    },
    {
      "epoch": 0.4978319783197832,
      "step": 9185,
      "training_loss": 6.887912750244141
    },
    {
      "epoch": 0.4978861788617886,
      "step": 9186,
      "training_loss": 6.671661853790283
    },
    {
      "epoch": 0.49794037940379404,
      "step": 9187,
      "training_loss": 7.047162055969238
    },
    {
      "epoch": 0.49799457994579943,
      "grad_norm": 18.283021926879883,
      "learning_rate": 1e-05,
      "loss": 6.716,
      "step": 9188
    },
    {
      "epoch": 0.49799457994579943,
      "step": 9188,
      "training_loss": 6.967996597290039
    },
    {
      "epoch": 0.4980487804878049,
      "step": 9189,
      "training_loss": 6.6343607902526855
    },
    {
      "epoch": 0.4981029810298103,
      "step": 9190,
      "training_loss": 5.446115016937256
    },
    {
      "epoch": 0.4981571815718157,
      "step": 9191,
      "training_loss": 6.760519027709961
    },
    {
      "epoch": 0.49821138211382116,
      "grad_norm": 42.9332275390625,
      "learning_rate": 1e-05,
      "loss": 6.4522,
      "step": 9192
    },
    {
      "epoch": 0.49821138211382116,
      "step": 9192,
      "training_loss": 6.060924530029297
    },
    {
      "epoch": 0.49826558265582654,
      "step": 9193,
      "training_loss": 6.676138401031494
    },
    {
      "epoch": 0.498319783197832,
      "step": 9194,
      "training_loss": 6.987652778625488
    },
    {
      "epoch": 0.4983739837398374,
      "step": 9195,
      "training_loss": 6.2613115310668945
    },
    {
      "epoch": 0.4984281842818428,
      "grad_norm": 25.993776321411133,
      "learning_rate": 1e-05,
      "loss": 6.4965,
      "step": 9196
    },
    {
      "epoch": 0.4984281842818428,
      "step": 9196,
      "training_loss": 6.07361364364624
    },
    {
      "epoch": 0.4984823848238482,
      "step": 9197,
      "training_loss": 6.046566009521484
    },
    {
      "epoch": 0.49853658536585366,
      "step": 9198,
      "training_loss": 6.080285549163818
    },
    {
      "epoch": 0.4985907859078591,
      "step": 9199,
      "training_loss": 3.2620999813079834
    },
    {
      "epoch": 0.4986449864498645,
      "grad_norm": 27.76082420349121,
      "learning_rate": 1e-05,
      "loss": 5.3656,
      "step": 9200
    },
    {
      "epoch": 0.4986449864498645,
      "step": 9200,
      "training_loss": 7.097308158874512
    },
    {
      "epoch": 0.49869918699186994,
      "step": 9201,
      "training_loss": 6.8357720375061035
    },
    {
      "epoch": 0.4987533875338753,
      "step": 9202,
      "training_loss": 6.578258514404297
    },
    {
      "epoch": 0.49880758807588077,
      "step": 9203,
      "training_loss": 7.316877841949463
    },
    {
      "epoch": 0.49886178861788616,
      "grad_norm": 41.04412078857422,
      "learning_rate": 1e-05,
      "loss": 6.9571,
      "step": 9204
    },
    {
      "epoch": 0.49886178861788616,
      "step": 9204,
      "training_loss": 7.700393199920654
    },
    {
      "epoch": 0.4989159891598916,
      "step": 9205,
      "training_loss": 6.810184001922607
    },
    {
      "epoch": 0.498970189701897,
      "step": 9206,
      "training_loss": 6.656343936920166
    },
    {
      "epoch": 0.49902439024390244,
      "step": 9207,
      "training_loss": 7.13769006729126
    },
    {
      "epoch": 0.4990785907859079,
      "grad_norm": 21.182106018066406,
      "learning_rate": 1e-05,
      "loss": 7.0762,
      "step": 9208
    },
    {
      "epoch": 0.4990785907859079,
      "step": 9208,
      "training_loss": 8.261460304260254
    },
    {
      "epoch": 0.49913279132791327,
      "step": 9209,
      "training_loss": 6.81929874420166
    },
    {
      "epoch": 0.4991869918699187,
      "step": 9210,
      "training_loss": 6.219162464141846
    },
    {
      "epoch": 0.4992411924119241,
      "step": 9211,
      "training_loss": 6.756204605102539
    },
    {
      "epoch": 0.49929539295392955,
      "grad_norm": 20.97350311279297,
      "learning_rate": 1e-05,
      "loss": 7.014,
      "step": 9212
    },
    {
      "epoch": 0.49929539295392955,
      "step": 9212,
      "training_loss": 6.730951309204102
    },
    {
      "epoch": 0.49934959349593494,
      "step": 9213,
      "training_loss": 7.754195213317871
    },
    {
      "epoch": 0.4994037940379404,
      "step": 9214,
      "training_loss": 6.657818794250488
    },
    {
      "epoch": 0.4994579945799458,
      "step": 9215,
      "training_loss": 7.003376007080078
    },
    {
      "epoch": 0.4995121951219512,
      "grad_norm": 21.31661033630371,
      "learning_rate": 1e-05,
      "loss": 7.0366,
      "step": 9216
    },
    {
      "epoch": 0.4995121951219512,
      "step": 9216,
      "training_loss": 6.3646135330200195
    },
    {
      "epoch": 0.49956639566395666,
      "step": 9217,
      "training_loss": 5.2583818435668945
    },
    {
      "epoch": 0.49962059620596205,
      "step": 9218,
      "training_loss": 5.581732749938965
    },
    {
      "epoch": 0.4996747967479675,
      "step": 9219,
      "training_loss": 5.717523574829102
    },
    {
      "epoch": 0.4997289972899729,
      "grad_norm": 59.78232955932617,
      "learning_rate": 1e-05,
      "loss": 5.7306,
      "step": 9220
    },
    {
      "epoch": 0.4997289972899729,
      "step": 9220,
      "training_loss": 7.037298202514648
    },
    {
      "epoch": 0.49978319783197833,
      "step": 9221,
      "training_loss": 6.452640056610107
    },
    {
      "epoch": 0.4998373983739837,
      "step": 9222,
      "training_loss": 7.118925094604492
    },
    {
      "epoch": 0.49989159891598917,
      "step": 9223,
      "training_loss": 5.772362232208252
    },
    {
      "epoch": 0.49994579945799456,
      "grad_norm": 28.74480628967285,
      "learning_rate": 1e-05,
      "loss": 6.5953,
      "step": 9224
    },
    {
      "epoch": 0.49994579945799456,
      "step": 9224,
      "training_loss": 7.91542387008667
    },
    {
      "epoch": 0.5,
      "step": 9225,
      "training_loss": 6.796013355255127
    },
    {
      "epoch": 0.5000542005420054,
      "step": 9226,
      "training_loss": 6.085362911224365
    },
    {
      "epoch": 0.5001084010840109,
      "step": 9227,
      "training_loss": 4.1684889793396
    },
    {
      "epoch": 0.5001626016260162,
      "grad_norm": 30.2279052734375,
      "learning_rate": 1e-05,
      "loss": 6.2413,
      "step": 9228
    },
    {
      "epoch": 0.5001626016260162,
      "step": 9228,
      "training_loss": 7.405529975891113
    },
    {
      "epoch": 0.5002168021680217,
      "step": 9229,
      "training_loss": 5.722873210906982
    },
    {
      "epoch": 0.5002710027100271,
      "step": 9230,
      "training_loss": 7.19019079208374
    },
    {
      "epoch": 0.5003252032520326,
      "step": 9231,
      "training_loss": 6.17974328994751
    },
    {
      "epoch": 0.5003794037940379,
      "grad_norm": 32.24415588378906,
      "learning_rate": 1e-05,
      "loss": 6.6246,
      "step": 9232
    },
    {
      "epoch": 0.5003794037940379,
      "step": 9232,
      "training_loss": 6.88792610168457
    },
    {
      "epoch": 0.5004336043360433,
      "step": 9233,
      "training_loss": 7.298182487487793
    },
    {
      "epoch": 0.5004878048780488,
      "step": 9234,
      "training_loss": 7.61370849609375
    },
    {
      "epoch": 0.5005420054200542,
      "step": 9235,
      "training_loss": 6.225897312164307
    },
    {
      "epoch": 0.5005962059620597,
      "grad_norm": 41.76133728027344,
      "learning_rate": 1e-05,
      "loss": 7.0064,
      "step": 9236
    },
    {
      "epoch": 0.5005962059620597,
      "step": 9236,
      "training_loss": 6.672258377075195
    },
    {
      "epoch": 0.500650406504065,
      "step": 9237,
      "training_loss": 5.966427803039551
    },
    {
      "epoch": 0.5007046070460704,
      "step": 9238,
      "training_loss": 6.014954566955566
    },
    {
      "epoch": 0.5007588075880759,
      "step": 9239,
      "training_loss": 5.720435619354248
    },
    {
      "epoch": 0.5008130081300813,
      "grad_norm": 23.23637580871582,
      "learning_rate": 1e-05,
      "loss": 6.0935,
      "step": 9240
    },
    {
      "epoch": 0.5008130081300813,
      "step": 9240,
      "training_loss": 6.28398323059082
    },
    {
      "epoch": 0.5008672086720867,
      "step": 9241,
      "training_loss": 6.95640230178833
    },
    {
      "epoch": 0.5009214092140921,
      "step": 9242,
      "training_loss": 7.4750823974609375
    },
    {
      "epoch": 0.5009756097560976,
      "step": 9243,
      "training_loss": 7.1204514503479
    },
    {
      "epoch": 0.501029810298103,
      "grad_norm": 25.249927520751953,
      "learning_rate": 1e-05,
      "loss": 6.959,
      "step": 9244
    },
    {
      "epoch": 0.501029810298103,
      "step": 9244,
      "training_loss": 7.180030345916748
    },
    {
      "epoch": 0.5010840108401085,
      "step": 9245,
      "training_loss": 7.007579803466797
    },
    {
      "epoch": 0.5011382113821138,
      "step": 9246,
      "training_loss": 6.847432613372803
    },
    {
      "epoch": 0.5011924119241192,
      "step": 9247,
      "training_loss": 6.7306952476501465
    },
    {
      "epoch": 0.5012466124661247,
      "grad_norm": 23.90400505065918,
      "learning_rate": 1e-05,
      "loss": 6.9414,
      "step": 9248
    },
    {
      "epoch": 0.5012466124661247,
      "step": 9248,
      "training_loss": 7.464193344116211
    },
    {
      "epoch": 0.5013008130081301,
      "step": 9249,
      "training_loss": 2.9250478744506836
    },
    {
      "epoch": 0.5013550135501355,
      "step": 9250,
      "training_loss": 4.727477073669434
    },
    {
      "epoch": 0.5014092140921409,
      "step": 9251,
      "training_loss": 5.7468180656433105
    },
    {
      "epoch": 0.5014634146341463,
      "grad_norm": 40.029972076416016,
      "learning_rate": 1e-05,
      "loss": 5.2159,
      "step": 9252
    },
    {
      "epoch": 0.5014634146341463,
      "step": 9252,
      "training_loss": 6.878419876098633
    },
    {
      "epoch": 0.5015176151761518,
      "step": 9253,
      "training_loss": 5.944953441619873
    },
    {
      "epoch": 0.5015718157181572,
      "step": 9254,
      "training_loss": 6.680044174194336
    },
    {
      "epoch": 0.5016260162601626,
      "step": 9255,
      "training_loss": 6.949342727661133
    },
    {
      "epoch": 0.501680216802168,
      "grad_norm": 21.25746726989746,
      "learning_rate": 1e-05,
      "loss": 6.6132,
      "step": 9256
    },
    {
      "epoch": 0.501680216802168,
      "step": 9256,
      "training_loss": 6.953127384185791
    },
    {
      "epoch": 0.5017344173441735,
      "step": 9257,
      "training_loss": 6.543498992919922
    },
    {
      "epoch": 0.5017886178861789,
      "step": 9258,
      "training_loss": 6.009528636932373
    },
    {
      "epoch": 0.5018428184281842,
      "step": 9259,
      "training_loss": 6.935858249664307
    },
    {
      "epoch": 0.5018970189701897,
      "grad_norm": 23.894760131835938,
      "learning_rate": 1e-05,
      "loss": 6.6105,
      "step": 9260
    },
    {
      "epoch": 0.5018970189701897,
      "step": 9260,
      "training_loss": 7.696237564086914
    },
    {
      "epoch": 0.5019512195121951,
      "step": 9261,
      "training_loss": 7.3132710456848145
    },
    {
      "epoch": 0.5020054200542006,
      "step": 9262,
      "training_loss": 7.7938642501831055
    },
    {
      "epoch": 0.502059620596206,
      "step": 9263,
      "training_loss": 7.65399169921875
    },
    {
      "epoch": 0.5021138211382113,
      "grad_norm": 36.8054313659668,
      "learning_rate": 1e-05,
      "loss": 7.6143,
      "step": 9264
    },
    {
      "epoch": 0.5021138211382113,
      "step": 9264,
      "training_loss": 6.467344284057617
    },
    {
      "epoch": 0.5021680216802168,
      "step": 9265,
      "training_loss": 7.40678596496582
    },
    {
      "epoch": 0.5022222222222222,
      "step": 9266,
      "training_loss": 7.417160511016846
    },
    {
      "epoch": 0.5022764227642277,
      "step": 9267,
      "training_loss": 7.382201671600342
    },
    {
      "epoch": 0.502330623306233,
      "grad_norm": 54.224388122558594,
      "learning_rate": 1e-05,
      "loss": 7.1684,
      "step": 9268
    },
    {
      "epoch": 0.502330623306233,
      "step": 9268,
      "training_loss": 6.83365535736084
    },
    {
      "epoch": 0.5023848238482385,
      "step": 9269,
      "training_loss": 5.109129428863525
    },
    {
      "epoch": 0.5024390243902439,
      "step": 9270,
      "training_loss": 7.6540327072143555
    },
    {
      "epoch": 0.5024932249322493,
      "step": 9271,
      "training_loss": 5.41087007522583
    },
    {
      "epoch": 0.5025474254742548,
      "grad_norm": 26.68635368347168,
      "learning_rate": 1e-05,
      "loss": 6.2519,
      "step": 9272
    },
    {
      "epoch": 0.5025474254742548,
      "step": 9272,
      "training_loss": 5.438775062561035
    },
    {
      "epoch": 0.5026016260162601,
      "step": 9273,
      "training_loss": 5.489713668823242
    },
    {
      "epoch": 0.5026558265582656,
      "step": 9274,
      "training_loss": 5.3606343269348145
    },
    {
      "epoch": 0.502710027100271,
      "step": 9275,
      "training_loss": 6.941457748413086
    },
    {
      "epoch": 0.5027642276422765,
      "grad_norm": 33.9127311706543,
      "learning_rate": 1e-05,
      "loss": 5.8076,
      "step": 9276
    },
    {
      "epoch": 0.5027642276422765,
      "step": 9276,
      "training_loss": 6.594932556152344
    },
    {
      "epoch": 0.5028184281842818,
      "step": 9277,
      "training_loss": 6.74439811706543
    },
    {
      "epoch": 0.5028726287262872,
      "step": 9278,
      "training_loss": 6.94791841506958
    },
    {
      "epoch": 0.5029268292682927,
      "step": 9279,
      "training_loss": 7.697303295135498
    },
    {
      "epoch": 0.5029810298102981,
      "grad_norm": 40.984222412109375,
      "learning_rate": 1e-05,
      "loss": 6.9961,
      "step": 9280
    },
    {
      "epoch": 0.5029810298102981,
      "step": 9280,
      "training_loss": 6.13950777053833
    },
    {
      "epoch": 0.5030352303523036,
      "step": 9281,
      "training_loss": 8.564407348632812
    },
    {
      "epoch": 0.5030894308943089,
      "step": 9282,
      "training_loss": 7.297156810760498
    },
    {
      "epoch": 0.5031436314363144,
      "step": 9283,
      "training_loss": 6.592316627502441
    },
    {
      "epoch": 0.5031978319783198,
      "grad_norm": 24.069000244140625,
      "learning_rate": 1e-05,
      "loss": 7.1483,
      "step": 9284
    },
    {
      "epoch": 0.5031978319783198,
      "step": 9284,
      "training_loss": 7.2291579246521
    },
    {
      "epoch": 0.5032520325203252,
      "step": 9285,
      "training_loss": 7.275142192840576
    },
    {
      "epoch": 0.5033062330623306,
      "step": 9286,
      "training_loss": 6.371780872344971
    },
    {
      "epoch": 0.503360433604336,
      "step": 9287,
      "training_loss": 6.18124532699585
    },
    {
      "epoch": 0.5034146341463415,
      "grad_norm": 38.2996711730957,
      "learning_rate": 1e-05,
      "loss": 6.7643,
      "step": 9288
    },
    {
      "epoch": 0.5034146341463415,
      "step": 9288,
      "training_loss": 6.850641250610352
    },
    {
      "epoch": 0.5034688346883469,
      "step": 9289,
      "training_loss": 6.387641429901123
    },
    {
      "epoch": 0.5035230352303524,
      "step": 9290,
      "training_loss": 8.045665740966797
    },
    {
      "epoch": 0.5035772357723577,
      "step": 9291,
      "training_loss": 7.114301681518555
    },
    {
      "epoch": 0.5036314363143631,
      "grad_norm": 19.01157569885254,
      "learning_rate": 1e-05,
      "loss": 7.0996,
      "step": 9292
    },
    {
      "epoch": 0.5036314363143631,
      "step": 9292,
      "training_loss": 6.628777980804443
    },
    {
      "epoch": 0.5036856368563686,
      "step": 9293,
      "training_loss": 6.732247829437256
    },
    {
      "epoch": 0.503739837398374,
      "step": 9294,
      "training_loss": 5.480654716491699
    },
    {
      "epoch": 0.5037940379403794,
      "step": 9295,
      "training_loss": 6.671843528747559
    },
    {
      "epoch": 0.5038482384823848,
      "grad_norm": 22.01521110534668,
      "learning_rate": 1e-05,
      "loss": 6.3784,
      "step": 9296
    },
    {
      "epoch": 0.5038482384823848,
      "step": 9296,
      "training_loss": 7.700998783111572
    },
    {
      "epoch": 0.5039024390243902,
      "step": 9297,
      "training_loss": 7.851490497589111
    },
    {
      "epoch": 0.5039566395663957,
      "step": 9298,
      "training_loss": 6.895702362060547
    },
    {
      "epoch": 0.5040108401084011,
      "step": 9299,
      "training_loss": 6.650139808654785
    },
    {
      "epoch": 0.5040650406504065,
      "grad_norm": 25.017419815063477,
      "learning_rate": 1e-05,
      "loss": 7.2746,
      "step": 9300
    },
    {
      "epoch": 0.5040650406504065,
      "step": 9300,
      "training_loss": 7.591976165771484
    },
    {
      "epoch": 0.5041192411924119,
      "step": 9301,
      "training_loss": 7.49928092956543
    },
    {
      "epoch": 0.5041734417344174,
      "step": 9302,
      "training_loss": 5.035922527313232
    },
    {
      "epoch": 0.5042276422764228,
      "step": 9303,
      "training_loss": 9.189040184020996
    },
    {
      "epoch": 0.5042818428184281,
      "grad_norm": 78.20269012451172,
      "learning_rate": 1e-05,
      "loss": 7.3291,
      "step": 9304
    },
    {
      "epoch": 0.5042818428184281,
      "step": 9304,
      "training_loss": 4.524158000946045
    },
    {
      "epoch": 0.5043360433604336,
      "step": 9305,
      "training_loss": 7.402740001678467
    },
    {
      "epoch": 0.504390243902439,
      "step": 9306,
      "training_loss": 5.117555141448975
    },
    {
      "epoch": 0.5044444444444445,
      "step": 9307,
      "training_loss": 4.965198993682861
    },
    {
      "epoch": 0.5044986449864499,
      "grad_norm": 26.847505569458008,
      "learning_rate": 1e-05,
      "loss": 5.5024,
      "step": 9308
    },
    {
      "epoch": 0.5044986449864499,
      "step": 9308,
      "training_loss": 7.566585540771484
    },
    {
      "epoch": 0.5045528455284553,
      "step": 9309,
      "training_loss": 6.356234550476074
    },
    {
      "epoch": 0.5046070460704607,
      "step": 9310,
      "training_loss": 6.9276814460754395
    },
    {
      "epoch": 0.5046612466124661,
      "step": 9311,
      "training_loss": 6.948678493499756
    },
    {
      "epoch": 0.5047154471544716,
      "grad_norm": 27.49725341796875,
      "learning_rate": 1e-05,
      "loss": 6.9498,
      "step": 9312
    },
    {
      "epoch": 0.5047154471544716,
      "step": 9312,
      "training_loss": 5.2508344650268555
    },
    {
      "epoch": 0.5047696476964769,
      "step": 9313,
      "training_loss": 7.699601173400879
    },
    {
      "epoch": 0.5048238482384824,
      "step": 9314,
      "training_loss": 6.326183795928955
    },
    {
      "epoch": 0.5048780487804878,
      "step": 9315,
      "training_loss": 6.598257064819336
    },
    {
      "epoch": 0.5049322493224933,
      "grad_norm": 24.21921730041504,
      "learning_rate": 1e-05,
      "loss": 6.4687,
      "step": 9316
    },
    {
      "epoch": 0.5049322493224933,
      "step": 9316,
      "training_loss": 5.979980945587158
    },
    {
      "epoch": 0.5049864498644987,
      "step": 9317,
      "training_loss": 6.85888671875
    },
    {
      "epoch": 0.505040650406504,
      "step": 9318,
      "training_loss": 7.4621429443359375
    },
    {
      "epoch": 0.5050948509485095,
      "step": 9319,
      "training_loss": 6.135166168212891
    },
    {
      "epoch": 0.5051490514905149,
      "grad_norm": 28.18268585205078,
      "learning_rate": 1e-05,
      "loss": 6.609,
      "step": 9320
    },
    {
      "epoch": 0.5051490514905149,
      "step": 9320,
      "training_loss": 6.883684158325195
    },
    {
      "epoch": 0.5052032520325204,
      "step": 9321,
      "training_loss": 4.416332244873047
    },
    {
      "epoch": 0.5052574525745257,
      "step": 9322,
      "training_loss": 7.626893997192383
    },
    {
      "epoch": 0.5053116531165311,
      "step": 9323,
      "training_loss": 6.203800678253174
    },
    {
      "epoch": 0.5053658536585366,
      "grad_norm": 25.84231948852539,
      "learning_rate": 1e-05,
      "loss": 6.2827,
      "step": 9324
    },
    {
      "epoch": 0.5053658536585366,
      "step": 9324,
      "training_loss": 5.287230491638184
    },
    {
      "epoch": 0.505420054200542,
      "step": 9325,
      "training_loss": 7.376324653625488
    },
    {
      "epoch": 0.5054742547425475,
      "step": 9326,
      "training_loss": 4.685263633728027
    },
    {
      "epoch": 0.5055284552845528,
      "step": 9327,
      "training_loss": 7.270073890686035
    },
    {
      "epoch": 0.5055826558265583,
      "grad_norm": 36.456520080566406,
      "learning_rate": 1e-05,
      "loss": 6.1547,
      "step": 9328
    },
    {
      "epoch": 0.5055826558265583,
      "step": 9328,
      "training_loss": 6.6220808029174805
    },
    {
      "epoch": 0.5056368563685637,
      "step": 9329,
      "training_loss": 6.662378787994385
    },
    {
      "epoch": 0.5056910569105691,
      "step": 9330,
      "training_loss": 6.471219539642334
    },
    {
      "epoch": 0.5057452574525745,
      "step": 9331,
      "training_loss": 6.070008754730225
    },
    {
      "epoch": 0.5057994579945799,
      "grad_norm": 19.189910888671875,
      "learning_rate": 1e-05,
      "loss": 6.4564,
      "step": 9332
    },
    {
      "epoch": 0.5057994579945799,
      "step": 9332,
      "training_loss": 6.335573196411133
    },
    {
      "epoch": 0.5058536585365854,
      "step": 9333,
      "training_loss": 7.450869560241699
    },
    {
      "epoch": 0.5059078590785908,
      "step": 9334,
      "training_loss": 6.555703639984131
    },
    {
      "epoch": 0.5059620596205963,
      "step": 9335,
      "training_loss": 5.7164082527160645
    },
    {
      "epoch": 0.5060162601626016,
      "grad_norm": 25.747835159301758,
      "learning_rate": 1e-05,
      "loss": 6.5146,
      "step": 9336
    },
    {
      "epoch": 0.5060162601626016,
      "step": 9336,
      "training_loss": 7.931277751922607
    },
    {
      "epoch": 0.506070460704607,
      "step": 9337,
      "training_loss": 6.726268768310547
    },
    {
      "epoch": 0.5061246612466125,
      "step": 9338,
      "training_loss": 10.825084686279297
    },
    {
      "epoch": 0.5061788617886179,
      "step": 9339,
      "training_loss": 6.826751708984375
    },
    {
      "epoch": 0.5062330623306233,
      "grad_norm": 29.48639678955078,
      "learning_rate": 1e-05,
      "loss": 8.0773,
      "step": 9340
    },
    {
      "epoch": 0.5062330623306233,
      "step": 9340,
      "training_loss": 6.641903400421143
    },
    {
      "epoch": 0.5062872628726287,
      "step": 9341,
      "training_loss": 5.60133171081543
    },
    {
      "epoch": 0.5063414634146342,
      "step": 9342,
      "training_loss": 6.005939483642578
    },
    {
      "epoch": 0.5063956639566396,
      "step": 9343,
      "training_loss": 7.464890956878662
    },
    {
      "epoch": 0.506449864498645,
      "grad_norm": 24.916744232177734,
      "learning_rate": 1e-05,
      "loss": 6.4285,
      "step": 9344
    },
    {
      "epoch": 0.506449864498645,
      "step": 9344,
      "training_loss": 4.864725112915039
    },
    {
      "epoch": 0.5065040650406504,
      "step": 9345,
      "training_loss": 6.475304126739502
    },
    {
      "epoch": 0.5065582655826558,
      "step": 9346,
      "training_loss": 7.305524826049805
    },
    {
      "epoch": 0.5066124661246613,
      "step": 9347,
      "training_loss": 6.956238269805908
    },
    {
      "epoch": 0.5066666666666667,
      "grad_norm": 18.60189437866211,
      "learning_rate": 1e-05,
      "loss": 6.4004,
      "step": 9348
    },
    {
      "epoch": 0.5066666666666667,
      "step": 9348,
      "training_loss": 6.900885105133057
    },
    {
      "epoch": 0.506720867208672,
      "step": 9349,
      "training_loss": 7.510798931121826
    },
    {
      "epoch": 0.5067750677506775,
      "step": 9350,
      "training_loss": 6.7894487380981445
    },
    {
      "epoch": 0.5068292682926829,
      "step": 9351,
      "training_loss": 6.58537483215332
    },
    {
      "epoch": 0.5068834688346884,
      "grad_norm": 28.024919509887695,
      "learning_rate": 1e-05,
      "loss": 6.9466,
      "step": 9352
    },
    {
      "epoch": 0.5068834688346884,
      "step": 9352,
      "training_loss": 7.0175065994262695
    },
    {
      "epoch": 0.5069376693766938,
      "step": 9353,
      "training_loss": 5.763477802276611
    },
    {
      "epoch": 0.5069918699186992,
      "step": 9354,
      "training_loss": 4.779074192047119
    },
    {
      "epoch": 0.5070460704607046,
      "step": 9355,
      "training_loss": 6.7921247482299805
    },
    {
      "epoch": 0.50710027100271,
      "grad_norm": 57.80131149291992,
      "learning_rate": 1e-05,
      "loss": 6.088,
      "step": 9356
    },
    {
      "epoch": 0.50710027100271,
      "step": 9356,
      "training_loss": 7.164523124694824
    },
    {
      "epoch": 0.5071544715447155,
      "step": 9357,
      "training_loss": 7.945720672607422
    },
    {
      "epoch": 0.5072086720867208,
      "step": 9358,
      "training_loss": 6.462282657623291
    },
    {
      "epoch": 0.5072628726287263,
      "step": 9359,
      "training_loss": 5.828577518463135
    },
    {
      "epoch": 0.5073170731707317,
      "grad_norm": 55.56242752075195,
      "learning_rate": 1e-05,
      "loss": 6.8503,
      "step": 9360
    },
    {
      "epoch": 0.5073170731707317,
      "step": 9360,
      "training_loss": 6.005712509155273
    },
    {
      "epoch": 0.5073712737127372,
      "step": 9361,
      "training_loss": 5.586267471313477
    },
    {
      "epoch": 0.5074254742547425,
      "step": 9362,
      "training_loss": 5.9809465408325195
    },
    {
      "epoch": 0.5074796747967479,
      "step": 9363,
      "training_loss": 5.5904340744018555
    },
    {
      "epoch": 0.5075338753387534,
      "grad_norm": 42.794803619384766,
      "learning_rate": 1e-05,
      "loss": 5.7908,
      "step": 9364
    },
    {
      "epoch": 0.5075338753387534,
      "step": 9364,
      "training_loss": 6.534442901611328
    },
    {
      "epoch": 0.5075880758807588,
      "step": 9365,
      "training_loss": 5.909269332885742
    },
    {
      "epoch": 0.5076422764227643,
      "step": 9366,
      "training_loss": 7.585550308227539
    },
    {
      "epoch": 0.5076964769647696,
      "step": 9367,
      "training_loss": 7.080704689025879
    },
    {
      "epoch": 0.507750677506775,
      "grad_norm": 16.407014846801758,
      "learning_rate": 1e-05,
      "loss": 6.7775,
      "step": 9368
    },
    {
      "epoch": 0.507750677506775,
      "step": 9368,
      "training_loss": 3.4285476207733154
    },
    {
      "epoch": 0.5078048780487805,
      "step": 9369,
      "training_loss": 5.117558002471924
    },
    {
      "epoch": 0.5078590785907859,
      "step": 9370,
      "training_loss": 7.25670862197876
    },
    {
      "epoch": 0.5079132791327913,
      "step": 9371,
      "training_loss": 6.4113450050354
    },
    {
      "epoch": 0.5079674796747967,
      "grad_norm": 30.095380783081055,
      "learning_rate": 1e-05,
      "loss": 5.5535,
      "step": 9372
    },
    {
      "epoch": 0.5079674796747967,
      "step": 9372,
      "training_loss": 7.988765716552734
    },
    {
      "epoch": 0.5080216802168022,
      "step": 9373,
      "training_loss": 8.178577423095703
    },
    {
      "epoch": 0.5080758807588076,
      "step": 9374,
      "training_loss": 6.087115287780762
    },
    {
      "epoch": 0.508130081300813,
      "step": 9375,
      "training_loss": 7.213921546936035
    },
    {
      "epoch": 0.5081842818428184,
      "grad_norm": 23.360273361206055,
      "learning_rate": 1e-05,
      "loss": 7.3671,
      "step": 9376
    },
    {
      "epoch": 0.5081842818428184,
      "step": 9376,
      "training_loss": 7.728024005889893
    },
    {
      "epoch": 0.5082384823848238,
      "step": 9377,
      "training_loss": 6.800257205963135
    },
    {
      "epoch": 0.5082926829268293,
      "step": 9378,
      "training_loss": 6.109385013580322
    },
    {
      "epoch": 0.5083468834688347,
      "step": 9379,
      "training_loss": 7.501580715179443
    },
    {
      "epoch": 0.50840108401084,
      "grad_norm": 26.626985549926758,
      "learning_rate": 1e-05,
      "loss": 7.0348,
      "step": 9380
    },
    {
      "epoch": 0.50840108401084,
      "step": 9380,
      "training_loss": 7.180726528167725
    },
    {
      "epoch": 0.5084552845528455,
      "step": 9381,
      "training_loss": 7.238770008087158
    },
    {
      "epoch": 0.5085094850948509,
      "step": 9382,
      "training_loss": 7.853950500488281
    },
    {
      "epoch": 0.5085636856368564,
      "step": 9383,
      "training_loss": 6.99188232421875
    },
    {
      "epoch": 0.5086178861788618,
      "grad_norm": 34.85055160522461,
      "learning_rate": 1e-05,
      "loss": 7.3163,
      "step": 9384
    },
    {
      "epoch": 0.5086178861788618,
      "step": 9384,
      "training_loss": 6.633785247802734
    },
    {
      "epoch": 0.5086720867208672,
      "step": 9385,
      "training_loss": 7.195978164672852
    },
    {
      "epoch": 0.5087262872628726,
      "step": 9386,
      "training_loss": 7.041106224060059
    },
    {
      "epoch": 0.5087804878048781,
      "step": 9387,
      "training_loss": 8.544382095336914
    },
    {
      "epoch": 0.5088346883468835,
      "grad_norm": 48.63591003417969,
      "learning_rate": 1e-05,
      "loss": 7.3538,
      "step": 9388
    },
    {
      "epoch": 0.5088346883468835,
      "step": 9388,
      "training_loss": 6.42794942855835
    },
    {
      "epoch": 0.5088888888888888,
      "step": 9389,
      "training_loss": 7.115901470184326
    },
    {
      "epoch": 0.5089430894308943,
      "step": 9390,
      "training_loss": 6.532139778137207
    },
    {
      "epoch": 0.5089972899728997,
      "step": 9391,
      "training_loss": 4.846798896789551
    },
    {
      "epoch": 0.5090514905149052,
      "grad_norm": 47.7508659362793,
      "learning_rate": 1e-05,
      "loss": 6.2307,
      "step": 9392
    },
    {
      "epoch": 0.5090514905149052,
      "step": 9392,
      "training_loss": 7.3045759201049805
    },
    {
      "epoch": 0.5091056910569106,
      "step": 9393,
      "training_loss": 7.540050506591797
    },
    {
      "epoch": 0.509159891598916,
      "step": 9394,
      "training_loss": 6.420684337615967
    },
    {
      "epoch": 0.5092140921409214,
      "step": 9395,
      "training_loss": 5.783837795257568
    },
    {
      "epoch": 0.5092682926829268,
      "grad_norm": 39.45790100097656,
      "learning_rate": 1e-05,
      "loss": 6.7623,
      "step": 9396
    },
    {
      "epoch": 0.5092682926829268,
      "step": 9396,
      "training_loss": 6.995947360992432
    },
    {
      "epoch": 0.5093224932249323,
      "step": 9397,
      "training_loss": 6.224783420562744
    },
    {
      "epoch": 0.5093766937669376,
      "step": 9398,
      "training_loss": 7.60929536819458
    },
    {
      "epoch": 0.5094308943089431,
      "step": 9399,
      "training_loss": 6.936363697052002
    },
    {
      "epoch": 0.5094850948509485,
      "grad_norm": 32.61698532104492,
      "learning_rate": 1e-05,
      "loss": 6.9416,
      "step": 9400
    },
    {
      "epoch": 0.5094850948509485,
      "step": 9400,
      "training_loss": 6.748865127563477
    },
    {
      "epoch": 0.509539295392954,
      "step": 9401,
      "training_loss": 6.621376037597656
    },
    {
      "epoch": 0.5095934959349594,
      "step": 9402,
      "training_loss": 6.649735450744629
    },
    {
      "epoch": 0.5096476964769647,
      "step": 9403,
      "training_loss": 6.082646369934082
    },
    {
      "epoch": 0.5097018970189702,
      "grad_norm": 36.135589599609375,
      "learning_rate": 1e-05,
      "loss": 6.5257,
      "step": 9404
    },
    {
      "epoch": 0.5097018970189702,
      "step": 9404,
      "training_loss": 6.506075382232666
    },
    {
      "epoch": 0.5097560975609756,
      "step": 9405,
      "training_loss": 7.326719284057617
    },
    {
      "epoch": 0.5098102981029811,
      "step": 9406,
      "training_loss": 6.81652307510376
    },
    {
      "epoch": 0.5098644986449864,
      "step": 9407,
      "training_loss": 6.363936424255371
    },
    {
      "epoch": 0.5099186991869918,
      "grad_norm": 40.95758819580078,
      "learning_rate": 1e-05,
      "loss": 6.7533,
      "step": 9408
    },
    {
      "epoch": 0.5099186991869918,
      "step": 9408,
      "training_loss": 7.09166955947876
    },
    {
      "epoch": 0.5099728997289973,
      "step": 9409,
      "training_loss": 7.997737407684326
    },
    {
      "epoch": 0.5100271002710027,
      "step": 9410,
      "training_loss": 6.5144548416137695
    },
    {
      "epoch": 0.5100813008130082,
      "step": 9411,
      "training_loss": 6.1622514724731445
    },
    {
      "epoch": 0.5101355013550135,
      "grad_norm": 26.985763549804688,
      "learning_rate": 1e-05,
      "loss": 6.9415,
      "step": 9412
    },
    {
      "epoch": 0.5101355013550135,
      "step": 9412,
      "training_loss": 5.837831020355225
    },
    {
      "epoch": 0.510189701897019,
      "step": 9413,
      "training_loss": 7.470285415649414
    },
    {
      "epoch": 0.5102439024390244,
      "step": 9414,
      "training_loss": 6.062154293060303
    },
    {
      "epoch": 0.5102981029810298,
      "step": 9415,
      "training_loss": 6.263950824737549
    },
    {
      "epoch": 0.5103523035230352,
      "grad_norm": 23.17583465576172,
      "learning_rate": 1e-05,
      "loss": 6.4086,
      "step": 9416
    },
    {
      "epoch": 0.5103523035230352,
      "step": 9416,
      "training_loss": 7.062836647033691
    },
    {
      "epoch": 0.5104065040650406,
      "step": 9417,
      "training_loss": 8.644229888916016
    },
    {
      "epoch": 0.5104607046070461,
      "step": 9418,
      "training_loss": 5.683413982391357
    },
    {
      "epoch": 0.5105149051490515,
      "step": 9419,
      "training_loss": 8.732364654541016
    },
    {
      "epoch": 0.510569105691057,
      "grad_norm": 36.65022659301758,
      "learning_rate": 1e-05,
      "loss": 7.5307,
      "step": 9420
    },
    {
      "epoch": 0.510569105691057,
      "step": 9420,
      "training_loss": 6.777005672454834
    },
    {
      "epoch": 0.5106233062330623,
      "step": 9421,
      "training_loss": 5.165324687957764
    },
    {
      "epoch": 0.5106775067750677,
      "step": 9422,
      "training_loss": 7.176806449890137
    },
    {
      "epoch": 0.5107317073170732,
      "step": 9423,
      "training_loss": 7.422308444976807
    },
    {
      "epoch": 0.5107859078590786,
      "grad_norm": 44.1083869934082,
      "learning_rate": 1e-05,
      "loss": 6.6354,
      "step": 9424
    },
    {
      "epoch": 0.5107859078590786,
      "step": 9424,
      "training_loss": 5.607591152191162
    },
    {
      "epoch": 0.510840108401084,
      "step": 9425,
      "training_loss": 6.204714775085449
    },
    {
      "epoch": 0.5108943089430894,
      "step": 9426,
      "training_loss": 7.5787272453308105
    },
    {
      "epoch": 0.5109485094850948,
      "step": 9427,
      "training_loss": 6.63130521774292
    },
    {
      "epoch": 0.5110027100271003,
      "grad_norm": 32.82887268066406,
      "learning_rate": 1e-05,
      "loss": 6.5056,
      "step": 9428
    },
    {
      "epoch": 0.5110027100271003,
      "step": 9428,
      "training_loss": 7.114188194274902
    },
    {
      "epoch": 0.5110569105691057,
      "step": 9429,
      "training_loss": 5.148246765136719
    },
    {
      "epoch": 0.5111111111111111,
      "step": 9430,
      "training_loss": 6.857637882232666
    },
    {
      "epoch": 0.5111653116531165,
      "step": 9431,
      "training_loss": 5.485462188720703
    },
    {
      "epoch": 0.511219512195122,
      "grad_norm": 27.076702117919922,
      "learning_rate": 1e-05,
      "loss": 6.1514,
      "step": 9432
    },
    {
      "epoch": 0.511219512195122,
      "step": 9432,
      "training_loss": 6.562795162200928
    },
    {
      "epoch": 0.5112737127371274,
      "step": 9433,
      "training_loss": 6.5331711769104
    },
    {
      "epoch": 0.5113279132791327,
      "step": 9434,
      "training_loss": 7.745797634124756
    },
    {
      "epoch": 0.5113821138211382,
      "step": 9435,
      "training_loss": 5.58876371383667
    },
    {
      "epoch": 0.5114363143631436,
      "grad_norm": 49.34455108642578,
      "learning_rate": 1e-05,
      "loss": 6.6076,
      "step": 9436
    },
    {
      "epoch": 0.5114363143631436,
      "step": 9436,
      "training_loss": 6.233401775360107
    },
    {
      "epoch": 0.5114905149051491,
      "step": 9437,
      "training_loss": 6.853105068206787
    },
    {
      "epoch": 0.5115447154471545,
      "step": 9438,
      "training_loss": 6.86221170425415
    },
    {
      "epoch": 0.5115989159891599,
      "step": 9439,
      "training_loss": 6.541162014007568
    },
    {
      "epoch": 0.5116531165311653,
      "grad_norm": 31.21217918395996,
      "learning_rate": 1e-05,
      "loss": 6.6225,
      "step": 9440
    },
    {
      "epoch": 0.5116531165311653,
      "step": 9440,
      "training_loss": 5.65451717376709
    },
    {
      "epoch": 0.5117073170731707,
      "step": 9441,
      "training_loss": 6.563840389251709
    },
    {
      "epoch": 0.5117615176151762,
      "step": 9442,
      "training_loss": 3.678635835647583
    },
    {
      "epoch": 0.5118157181571815,
      "step": 9443,
      "training_loss": 6.4943156242370605
    },
    {
      "epoch": 0.511869918699187,
      "grad_norm": 32.11185073852539,
      "learning_rate": 1e-05,
      "loss": 5.5978,
      "step": 9444
    },
    {
      "epoch": 0.511869918699187,
      "step": 9444,
      "training_loss": 6.4007110595703125
    },
    {
      "epoch": 0.5119241192411924,
      "step": 9445,
      "training_loss": 6.852844715118408
    },
    {
      "epoch": 0.5119783197831979,
      "step": 9446,
      "training_loss": 6.931565284729004
    },
    {
      "epoch": 0.5120325203252033,
      "step": 9447,
      "training_loss": 8.416762351989746
    },
    {
      "epoch": 0.5120867208672086,
      "grad_norm": 49.863346099853516,
      "learning_rate": 1e-05,
      "loss": 7.1505,
      "step": 9448
    },
    {
      "epoch": 0.5120867208672086,
      "step": 9448,
      "training_loss": 6.584115505218506
    },
    {
      "epoch": 0.5121409214092141,
      "step": 9449,
      "training_loss": 7.1199822425842285
    },
    {
      "epoch": 0.5121951219512195,
      "step": 9450,
      "training_loss": 4.982180118560791
    },
    {
      "epoch": 0.512249322493225,
      "step": 9451,
      "training_loss": 7.955127716064453
    },
    {
      "epoch": 0.5123035230352303,
      "grad_norm": 43.33005905151367,
      "learning_rate": 1e-05,
      "loss": 6.6604,
      "step": 9452
    },
    {
      "epoch": 0.5123035230352303,
      "step": 9452,
      "training_loss": 6.521570205688477
    },
    {
      "epoch": 0.5123577235772357,
      "step": 9453,
      "training_loss": 6.458561897277832
    },
    {
      "epoch": 0.5124119241192412,
      "step": 9454,
      "training_loss": 6.973254680633545
    },
    {
      "epoch": 0.5124661246612466,
      "step": 9455,
      "training_loss": 6.849236488342285
    },
    {
      "epoch": 0.5125203252032521,
      "grad_norm": 26.45845603942871,
      "learning_rate": 1e-05,
      "loss": 6.7007,
      "step": 9456
    },
    {
      "epoch": 0.5125203252032521,
      "step": 9456,
      "training_loss": 5.489564895629883
    },
    {
      "epoch": 0.5125745257452574,
      "step": 9457,
      "training_loss": 7.037575721740723
    },
    {
      "epoch": 0.5126287262872629,
      "step": 9458,
      "training_loss": 6.7680158615112305
    },
    {
      "epoch": 0.5126829268292683,
      "step": 9459,
      "training_loss": 4.938263893127441
    },
    {
      "epoch": 0.5127371273712737,
      "grad_norm": 22.01645278930664,
      "learning_rate": 1e-05,
      "loss": 6.0584,
      "step": 9460
    },
    {
      "epoch": 0.5127371273712737,
      "step": 9460,
      "training_loss": 7.107806205749512
    },
    {
      "epoch": 0.5127913279132791,
      "step": 9461,
      "training_loss": 7.156266212463379
    },
    {
      "epoch": 0.5128455284552845,
      "step": 9462,
      "training_loss": 6.758171081542969
    },
    {
      "epoch": 0.51289972899729,
      "step": 9463,
      "training_loss": 7.133074760437012
    },
    {
      "epoch": 0.5129539295392954,
      "grad_norm": 23.483701705932617,
      "learning_rate": 1e-05,
      "loss": 7.0388,
      "step": 9464
    },
    {
      "epoch": 0.5129539295392954,
      "step": 9464,
      "training_loss": 6.875300407409668
    },
    {
      "epoch": 0.5130081300813009,
      "step": 9465,
      "training_loss": 6.552558898925781
    },
    {
      "epoch": 0.5130623306233062,
      "step": 9466,
      "training_loss": 6.695173263549805
    },
    {
      "epoch": 0.5131165311653116,
      "step": 9467,
      "training_loss": 7.415503025054932
    },
    {
      "epoch": 0.5131707317073171,
      "grad_norm": 39.57554244995117,
      "learning_rate": 1e-05,
      "loss": 6.8846,
      "step": 9468
    },
    {
      "epoch": 0.5131707317073171,
      "step": 9468,
      "training_loss": 5.662585735321045
    },
    {
      "epoch": 0.5132249322493225,
      "step": 9469,
      "training_loss": 3.76889967918396
    },
    {
      "epoch": 0.5132791327913279,
      "step": 9470,
      "training_loss": 6.607550144195557
    },
    {
      "epoch": 0.5133333333333333,
      "step": 9471,
      "training_loss": 7.1940412521362305
    },
    {
      "epoch": 0.5133875338753388,
      "grad_norm": 20.17822265625,
      "learning_rate": 1e-05,
      "loss": 5.8083,
      "step": 9472
    },
    {
      "epoch": 0.5133875338753388,
      "step": 9472,
      "training_loss": 4.771574020385742
    },
    {
      "epoch": 0.5134417344173442,
      "step": 9473,
      "training_loss": 6.561086654663086
    },
    {
      "epoch": 0.5134959349593496,
      "step": 9474,
      "training_loss": 8.23315715789795
    },
    {
      "epoch": 0.513550135501355,
      "step": 9475,
      "training_loss": 8.050536155700684
    },
    {
      "epoch": 0.5136043360433604,
      "grad_norm": 28.78531837463379,
      "learning_rate": 1e-05,
      "loss": 6.9041,
      "step": 9476
    },
    {
      "epoch": 0.5136043360433604,
      "step": 9476,
      "training_loss": 4.24435567855835
    },
    {
      "epoch": 0.5136585365853659,
      "step": 9477,
      "training_loss": 4.323211193084717
    },
    {
      "epoch": 0.5137127371273713,
      "step": 9478,
      "training_loss": 7.024258136749268
    },
    {
      "epoch": 0.5137669376693766,
      "step": 9479,
      "training_loss": 7.261996746063232
    },
    {
      "epoch": 0.5138211382113821,
      "grad_norm": 29.264427185058594,
      "learning_rate": 1e-05,
      "loss": 5.7135,
      "step": 9480
    },
    {
      "epoch": 0.5138211382113821,
      "step": 9480,
      "training_loss": 5.780209541320801
    },
    {
      "epoch": 0.5138753387533875,
      "step": 9481,
      "training_loss": 5.989528656005859
    },
    {
      "epoch": 0.513929539295393,
      "step": 9482,
      "training_loss": 7.4964399337768555
    },
    {
      "epoch": 0.5139837398373984,
      "step": 9483,
      "training_loss": 7.240569591522217
    },
    {
      "epoch": 0.5140379403794038,
      "grad_norm": 27.297069549560547,
      "learning_rate": 1e-05,
      "loss": 6.6267,
      "step": 9484
    },
    {
      "epoch": 0.5140379403794038,
      "step": 9484,
      "training_loss": 6.39105224609375
    },
    {
      "epoch": 0.5140921409214092,
      "step": 9485,
      "training_loss": 7.382957935333252
    },
    {
      "epoch": 0.5141463414634146,
      "step": 9486,
      "training_loss": 6.044888973236084
    },
    {
      "epoch": 0.5142005420054201,
      "step": 9487,
      "training_loss": 7.064299583435059
    },
    {
      "epoch": 0.5142547425474254,
      "grad_norm": 18.486177444458008,
      "learning_rate": 1e-05,
      "loss": 6.7208,
      "step": 9488
    },
    {
      "epoch": 0.5142547425474254,
      "step": 9488,
      "training_loss": 3.8143200874328613
    },
    {
      "epoch": 0.5143089430894309,
      "step": 9489,
      "training_loss": 7.006397724151611
    },
    {
      "epoch": 0.5143631436314363,
      "step": 9490,
      "training_loss": 6.006895542144775
    },
    {
      "epoch": 0.5144173441734418,
      "step": 9491,
      "training_loss": 6.530871391296387
    },
    {
      "epoch": 0.5144715447154472,
      "grad_norm": 29.28832244873047,
      "learning_rate": 1e-05,
      "loss": 5.8396,
      "step": 9492
    },
    {
      "epoch": 0.5144715447154472,
      "step": 9492,
      "training_loss": 6.897173881530762
    },
    {
      "epoch": 0.5145257452574525,
      "step": 9493,
      "training_loss": 6.640283107757568
    },
    {
      "epoch": 0.514579945799458,
      "step": 9494,
      "training_loss": 6.329916477203369
    },
    {
      "epoch": 0.5146341463414634,
      "step": 9495,
      "training_loss": 6.6031036376953125
    },
    {
      "epoch": 0.5146883468834689,
      "grad_norm": 35.833770751953125,
      "learning_rate": 1e-05,
      "loss": 6.6176,
      "step": 9496
    },
    {
      "epoch": 0.5146883468834689,
      "step": 9496,
      "training_loss": 9.456435203552246
    },
    {
      "epoch": 0.5147425474254742,
      "step": 9497,
      "training_loss": 3.6830384731292725
    },
    {
      "epoch": 0.5147967479674797,
      "step": 9498,
      "training_loss": 4.376260280609131
    },
    {
      "epoch": 0.5148509485094851,
      "step": 9499,
      "training_loss": 6.693746566772461
    },
    {
      "epoch": 0.5149051490514905,
      "grad_norm": 20.414159774780273,
      "learning_rate": 1e-05,
      "loss": 6.0524,
      "step": 9500
    },
    {
      "epoch": 0.5149051490514905,
      "step": 9500,
      "training_loss": 7.095373153686523
    },
    {
      "epoch": 0.514959349593496,
      "step": 9501,
      "training_loss": 6.960213661193848
    },
    {
      "epoch": 0.5150135501355013,
      "step": 9502,
      "training_loss": 6.820555210113525
    },
    {
      "epoch": 0.5150677506775068,
      "step": 9503,
      "training_loss": 7.382510662078857
    },
    {
      "epoch": 0.5151219512195122,
      "grad_norm": 27.323131561279297,
      "learning_rate": 1e-05,
      "loss": 7.0647,
      "step": 9504
    },
    {
      "epoch": 0.5151219512195122,
      "step": 9504,
      "training_loss": 5.765367031097412
    },
    {
      "epoch": 0.5151761517615177,
      "step": 9505,
      "training_loss": 6.856478214263916
    },
    {
      "epoch": 0.515230352303523,
      "step": 9506,
      "training_loss": 3.692814826965332
    },
    {
      "epoch": 0.5152845528455284,
      "step": 9507,
      "training_loss": 7.497641086578369
    },
    {
      "epoch": 0.5153387533875339,
      "grad_norm": 31.94963264465332,
      "learning_rate": 1e-05,
      "loss": 5.9531,
      "step": 9508
    },
    {
      "epoch": 0.5153387533875339,
      "step": 9508,
      "training_loss": 4.534415245056152
    },
    {
      "epoch": 0.5153929539295393,
      "step": 9509,
      "training_loss": 7.087235927581787
    },
    {
      "epoch": 0.5154471544715448,
      "step": 9510,
      "training_loss": 7.372198104858398
    },
    {
      "epoch": 0.5155013550135501,
      "step": 9511,
      "training_loss": 5.571072578430176
    },
    {
      "epoch": 0.5155555555555555,
      "grad_norm": 21.580663681030273,
      "learning_rate": 1e-05,
      "loss": 6.1412,
      "step": 9512
    },
    {
      "epoch": 0.5155555555555555,
      "step": 9512,
      "training_loss": 7.5760498046875
    },
    {
      "epoch": 0.515609756097561,
      "step": 9513,
      "training_loss": 7.972783088684082
    },
    {
      "epoch": 0.5156639566395664,
      "step": 9514,
      "training_loss": 7.472857475280762
    },
    {
      "epoch": 0.5157181571815718,
      "step": 9515,
      "training_loss": 6.352828502655029
    },
    {
      "epoch": 0.5157723577235772,
      "grad_norm": 22.601829528808594,
      "learning_rate": 1e-05,
      "loss": 7.3436,
      "step": 9516
    },
    {
      "epoch": 0.5157723577235772,
      "step": 9516,
      "training_loss": 6.949321746826172
    },
    {
      "epoch": 0.5158265582655827,
      "step": 9517,
      "training_loss": 6.393242359161377
    },
    {
      "epoch": 0.5158807588075881,
      "step": 9518,
      "training_loss": 6.211468696594238
    },
    {
      "epoch": 0.5159349593495935,
      "step": 9519,
      "training_loss": 5.505151748657227
    },
    {
      "epoch": 0.5159891598915989,
      "grad_norm": 28.875953674316406,
      "learning_rate": 1e-05,
      "loss": 6.2648,
      "step": 9520
    },
    {
      "epoch": 0.5159891598915989,
      "step": 9520,
      "training_loss": 7.29638671875
    },
    {
      "epoch": 0.5160433604336043,
      "step": 9521,
      "training_loss": 6.103770732879639
    },
    {
      "epoch": 0.5160975609756098,
      "step": 9522,
      "training_loss": 7.639422416687012
    },
    {
      "epoch": 0.5161517615176152,
      "step": 9523,
      "training_loss": 6.350490570068359
    },
    {
      "epoch": 0.5162059620596205,
      "grad_norm": 53.114994049072266,
      "learning_rate": 1e-05,
      "loss": 6.8475,
      "step": 9524
    },
    {
      "epoch": 0.5162059620596205,
      "step": 9524,
      "training_loss": 8.406533241271973
    },
    {
      "epoch": 0.516260162601626,
      "step": 9525,
      "training_loss": 6.042542457580566
    },
    {
      "epoch": 0.5163143631436314,
      "step": 9526,
      "training_loss": 5.346029758453369
    },
    {
      "epoch": 0.5163685636856369,
      "step": 9527,
      "training_loss": 5.899007320404053
    },
    {
      "epoch": 0.5164227642276423,
      "grad_norm": 43.10411071777344,
      "learning_rate": 1e-05,
      "loss": 6.4235,
      "step": 9528
    },
    {
      "epoch": 0.5164227642276423,
      "step": 9528,
      "training_loss": 4.499537944793701
    },
    {
      "epoch": 0.5164769647696477,
      "step": 9529,
      "training_loss": 7.274683475494385
    },
    {
      "epoch": 0.5165311653116531,
      "step": 9530,
      "training_loss": 7.495638847351074
    },
    {
      "epoch": 0.5165853658536586,
      "step": 9531,
      "training_loss": 6.56251859664917
    },
    {
      "epoch": 0.516639566395664,
      "grad_norm": 22.486217498779297,
      "learning_rate": 1e-05,
      "loss": 6.4581,
      "step": 9532
    },
    {
      "epoch": 0.516639566395664,
      "step": 9532,
      "training_loss": 7.549342632293701
    },
    {
      "epoch": 0.5166937669376693,
      "step": 9533,
      "training_loss": 7.533498764038086
    },
    {
      "epoch": 0.5167479674796748,
      "step": 9534,
      "training_loss": 7.145816802978516
    },
    {
      "epoch": 0.5168021680216802,
      "step": 9535,
      "training_loss": 6.983182430267334
    },
    {
      "epoch": 0.5168563685636857,
      "grad_norm": 25.577091217041016,
      "learning_rate": 1e-05,
      "loss": 7.303,
      "step": 9536
    },
    {
      "epoch": 0.5168563685636857,
      "step": 9536,
      "training_loss": 6.354353427886963
    },
    {
      "epoch": 0.5169105691056911,
      "step": 9537,
      "training_loss": 5.597435474395752
    },
    {
      "epoch": 0.5169647696476964,
      "step": 9538,
      "training_loss": 7.40962028503418
    },
    {
      "epoch": 0.5170189701897019,
      "step": 9539,
      "training_loss": 5.182443618774414
    },
    {
      "epoch": 0.5170731707317073,
      "grad_norm": 25.789791107177734,
      "learning_rate": 1e-05,
      "loss": 6.136,
      "step": 9540
    },
    {
      "epoch": 0.5170731707317073,
      "step": 9540,
      "training_loss": 7.066013813018799
    },
    {
      "epoch": 0.5171273712737128,
      "step": 9541,
      "training_loss": 5.481747627258301
    },
    {
      "epoch": 0.5171815718157181,
      "step": 9542,
      "training_loss": 7.156299114227295
    },
    {
      "epoch": 0.5172357723577236,
      "step": 9543,
      "training_loss": 6.628240585327148
    },
    {
      "epoch": 0.517289972899729,
      "grad_norm": 19.227689743041992,
      "learning_rate": 1e-05,
      "loss": 6.5831,
      "step": 9544
    },
    {
      "epoch": 0.517289972899729,
      "step": 9544,
      "training_loss": 5.819274425506592
    },
    {
      "epoch": 0.5173441734417344,
      "step": 9545,
      "training_loss": 7.508223056793213
    },
    {
      "epoch": 0.5173983739837399,
      "step": 9546,
      "training_loss": 5.6643290519714355
    },
    {
      "epoch": 0.5174525745257452,
      "step": 9547,
      "training_loss": 7.488202095031738
    },
    {
      "epoch": 0.5175067750677507,
      "grad_norm": 24.096357345581055,
      "learning_rate": 1e-05,
      "loss": 6.62,
      "step": 9548
    },
    {
      "epoch": 0.5175067750677507,
      "step": 9548,
      "training_loss": 4.595670700073242
    },
    {
      "epoch": 0.5175609756097561,
      "step": 9549,
      "training_loss": 6.47482442855835
    },
    {
      "epoch": 0.5176151761517616,
      "step": 9550,
      "training_loss": 6.4585676193237305
    },
    {
      "epoch": 0.5176693766937669,
      "step": 9551,
      "training_loss": 5.926566123962402
    },
    {
      "epoch": 0.5177235772357723,
      "grad_norm": 27.6567325592041,
      "learning_rate": 1e-05,
      "loss": 5.8639,
      "step": 9552
    },
    {
      "epoch": 0.5177235772357723,
      "step": 9552,
      "training_loss": 8.11286735534668
    },
    {
      "epoch": 0.5177777777777778,
      "step": 9553,
      "training_loss": 7.22021484375
    },
    {
      "epoch": 0.5178319783197832,
      "step": 9554,
      "training_loss": 8.530810356140137
    },
    {
      "epoch": 0.5178861788617887,
      "step": 9555,
      "training_loss": 8.01332950592041
    },
    {
      "epoch": 0.517940379403794,
      "grad_norm": 31.49946403503418,
      "learning_rate": 1e-05,
      "loss": 7.9693,
      "step": 9556
    },
    {
      "epoch": 0.517940379403794,
      "step": 9556,
      "training_loss": 6.397794723510742
    },
    {
      "epoch": 0.5179945799457994,
      "step": 9557,
      "training_loss": 6.019030570983887
    },
    {
      "epoch": 0.5180487804878049,
      "step": 9558,
      "training_loss": 5.41861629486084
    },
    {
      "epoch": 0.5181029810298103,
      "step": 9559,
      "training_loss": 4.300378322601318
    },
    {
      "epoch": 0.5181571815718157,
      "grad_norm": 30.522510528564453,
      "learning_rate": 1e-05,
      "loss": 5.534,
      "step": 9560
    },
    {
      "epoch": 0.5181571815718157,
      "step": 9560,
      "training_loss": 5.141674041748047
    },
    {
      "epoch": 0.5182113821138211,
      "step": 9561,
      "training_loss": 6.686334609985352
    },
    {
      "epoch": 0.5182655826558266,
      "step": 9562,
      "training_loss": 5.648861408233643
    },
    {
      "epoch": 0.518319783197832,
      "step": 9563,
      "training_loss": 7.442631244659424
    },
    {
      "epoch": 0.5183739837398375,
      "grad_norm": 32.27928924560547,
      "learning_rate": 1e-05,
      "loss": 6.2299,
      "step": 9564
    },
    {
      "epoch": 0.5183739837398375,
      "step": 9564,
      "training_loss": 8.064446449279785
    },
    {
      "epoch": 0.5184281842818428,
      "step": 9565,
      "training_loss": 8.043489456176758
    },
    {
      "epoch": 0.5184823848238482,
      "step": 9566,
      "training_loss": 7.73532772064209
    },
    {
      "epoch": 0.5185365853658537,
      "step": 9567,
      "training_loss": 6.7775373458862305
    },
    {
      "epoch": 0.5185907859078591,
      "grad_norm": 39.60411071777344,
      "learning_rate": 1e-05,
      "loss": 7.6552,
      "step": 9568
    },
    {
      "epoch": 0.5185907859078591,
      "step": 9568,
      "training_loss": 5.9825119972229
    },
    {
      "epoch": 0.5186449864498645,
      "step": 9569,
      "training_loss": 3.666909694671631
    },
    {
      "epoch": 0.5186991869918699,
      "step": 9570,
      "training_loss": 6.62580680847168
    },
    {
      "epoch": 0.5187533875338753,
      "step": 9571,
      "training_loss": 3.3958096504211426
    },
    {
      "epoch": 0.5188075880758808,
      "grad_norm": 31.188283920288086,
      "learning_rate": 1e-05,
      "loss": 4.9178,
      "step": 9572
    },
    {
      "epoch": 0.5188075880758808,
      "step": 9572,
      "training_loss": 6.954967498779297
    },
    {
      "epoch": 0.5188617886178862,
      "step": 9573,
      "training_loss": 7.771423816680908
    },
    {
      "epoch": 0.5189159891598916,
      "step": 9574,
      "training_loss": 6.8738250732421875
    },
    {
      "epoch": 0.518970189701897,
      "step": 9575,
      "training_loss": 6.395634174346924
    },
    {
      "epoch": 0.5190243902439025,
      "grad_norm": 27.62102699279785,
      "learning_rate": 1e-05,
      "loss": 6.999,
      "step": 9576
    },
    {
      "epoch": 0.5190243902439025,
      "step": 9576,
      "training_loss": 7.419196128845215
    },
    {
      "epoch": 0.5190785907859079,
      "step": 9577,
      "training_loss": 7.099149227142334
    },
    {
      "epoch": 0.5191327913279132,
      "step": 9578,
      "training_loss": 6.668705463409424
    },
    {
      "epoch": 0.5191869918699187,
      "step": 9579,
      "training_loss": 6.182611465454102
    },
    {
      "epoch": 0.5192411924119241,
      "grad_norm": 27.755712509155273,
      "learning_rate": 1e-05,
      "loss": 6.8424,
      "step": 9580
    },
    {
      "epoch": 0.5192411924119241,
      "step": 9580,
      "training_loss": 6.632083892822266
    },
    {
      "epoch": 0.5192953929539296,
      "step": 9581,
      "training_loss": 6.170544147491455
    },
    {
      "epoch": 0.519349593495935,
      "step": 9582,
      "training_loss": 7.04189395904541
    },
    {
      "epoch": 0.5194037940379403,
      "step": 9583,
      "training_loss": 6.3949456214904785
    },
    {
      "epoch": 0.5194579945799458,
      "grad_norm": 37.065025329589844,
      "learning_rate": 1e-05,
      "loss": 6.5599,
      "step": 9584
    },
    {
      "epoch": 0.5194579945799458,
      "step": 9584,
      "training_loss": 5.372675895690918
    },
    {
      "epoch": 0.5195121951219512,
      "step": 9585,
      "training_loss": 7.056602478027344
    },
    {
      "epoch": 0.5195663956639567,
      "step": 9586,
      "training_loss": 7.631365776062012
    },
    {
      "epoch": 0.519620596205962,
      "step": 9587,
      "training_loss": 6.429041862487793
    },
    {
      "epoch": 0.5196747967479675,
      "grad_norm": 28.535446166992188,
      "learning_rate": 1e-05,
      "loss": 6.6224,
      "step": 9588
    },
    {
      "epoch": 0.5196747967479675,
      "step": 9588,
      "training_loss": 6.265780925750732
    },
    {
      "epoch": 0.5197289972899729,
      "step": 9589,
      "training_loss": 6.8834991455078125
    },
    {
      "epoch": 0.5197831978319783,
      "step": 9590,
      "training_loss": 8.202510833740234
    },
    {
      "epoch": 0.5198373983739838,
      "step": 9591,
      "training_loss": 7.0856475830078125
    },
    {
      "epoch": 0.5198915989159891,
      "grad_norm": 16.67130470275879,
      "learning_rate": 1e-05,
      "loss": 7.1094,
      "step": 9592
    },
    {
      "epoch": 0.5198915989159891,
      "step": 9592,
      "training_loss": 6.087097644805908
    },
    {
      "epoch": 0.5199457994579946,
      "step": 9593,
      "training_loss": 7.215664386749268
    },
    {
      "epoch": 0.52,
      "step": 9594,
      "training_loss": 5.42456579208374
    },
    {
      "epoch": 0.5200542005420055,
      "step": 9595,
      "training_loss": 6.9215593338012695
    },
    {
      "epoch": 0.5201084010840108,
      "grad_norm": 33.887535095214844,
      "learning_rate": 1e-05,
      "loss": 6.4122,
      "step": 9596
    },
    {
      "epoch": 0.5201084010840108,
      "step": 9596,
      "training_loss": 8.394469261169434
    },
    {
      "epoch": 0.5201626016260162,
      "step": 9597,
      "training_loss": 4.7310919761657715
    },
    {
      "epoch": 0.5202168021680217,
      "step": 9598,
      "training_loss": 7.701591491699219
    },
    {
      "epoch": 0.5202710027100271,
      "step": 9599,
      "training_loss": 6.983083724975586
    },
    {
      "epoch": 0.5203252032520326,
      "grad_norm": 27.470186233520508,
      "learning_rate": 1e-05,
      "loss": 6.9526,
      "step": 9600
    },
    {
      "epoch": 0.5203252032520326,
      "step": 9600,
      "training_loss": 3.4848012924194336
    },
    {
      "epoch": 0.5203794037940379,
      "step": 9601,
      "training_loss": 6.586922645568848
    },
    {
      "epoch": 0.5204336043360434,
      "step": 9602,
      "training_loss": 9.147187232971191
    },
    {
      "epoch": 0.5204878048780488,
      "step": 9603,
      "training_loss": 3.03908634185791
    },
    {
      "epoch": 0.5205420054200542,
      "grad_norm": 32.49177551269531,
      "learning_rate": 1e-05,
      "loss": 5.5645,
      "step": 9604
    },
    {
      "epoch": 0.5205420054200542,
      "step": 9604,
      "training_loss": 7.006771564483643
    },
    {
      "epoch": 0.5205962059620596,
      "step": 9605,
      "training_loss": 5.991774559020996
    },
    {
      "epoch": 0.520650406504065,
      "step": 9606,
      "training_loss": 6.844423294067383
    },
    {
      "epoch": 0.5207046070460705,
      "step": 9607,
      "training_loss": 6.780031204223633
    },
    {
      "epoch": 0.5207588075880759,
      "grad_norm": 25.39093017578125,
      "learning_rate": 1e-05,
      "loss": 6.6558,
      "step": 9608
    },
    {
      "epoch": 0.5207588075880759,
      "step": 9608,
      "training_loss": 7.920829772949219
    },
    {
      "epoch": 0.5208130081300814,
      "step": 9609,
      "training_loss": 6.904693603515625
    },
    {
      "epoch": 0.5208672086720867,
      "step": 9610,
      "training_loss": 6.594028949737549
    },
    {
      "epoch": 0.5209214092140921,
      "step": 9611,
      "training_loss": 8.180171966552734
    },
    {
      "epoch": 0.5209756097560976,
      "grad_norm": 22.603078842163086,
      "learning_rate": 1e-05,
      "loss": 7.3999,
      "step": 9612
    },
    {
      "epoch": 0.5209756097560976,
      "step": 9612,
      "training_loss": 7.204494953155518
    },
    {
      "epoch": 0.521029810298103,
      "step": 9613,
      "training_loss": 4.3880462646484375
    },
    {
      "epoch": 0.5210840108401084,
      "step": 9614,
      "training_loss": 7.846563339233398
    },
    {
      "epoch": 0.5211382113821138,
      "step": 9615,
      "training_loss": 2.868795394897461
    },
    {
      "epoch": 0.5211924119241192,
      "grad_norm": 30.956119537353516,
      "learning_rate": 1e-05,
      "loss": 5.577,
      "step": 9616
    },
    {
      "epoch": 0.5211924119241192,
      "step": 9616,
      "training_loss": 7.776705265045166
    },
    {
      "epoch": 0.5212466124661247,
      "step": 9617,
      "training_loss": 7.379814624786377
    },
    {
      "epoch": 0.52130081300813,
      "step": 9618,
      "training_loss": 4.246032238006592
    },
    {
      "epoch": 0.5213550135501355,
      "step": 9619,
      "training_loss": 5.720118999481201
    },
    {
      "epoch": 0.5214092140921409,
      "grad_norm": 63.882389068603516,
      "learning_rate": 1e-05,
      "loss": 6.2807,
      "step": 9620
    },
    {
      "epoch": 0.5214092140921409,
      "step": 9620,
      "training_loss": 5.8913960456848145
    },
    {
      "epoch": 0.5214634146341464,
      "step": 9621,
      "training_loss": 6.697641372680664
    },
    {
      "epoch": 0.5215176151761518,
      "step": 9622,
      "training_loss": 7.028530597686768
    },
    {
      "epoch": 0.5215718157181571,
      "step": 9623,
      "training_loss": 6.734933376312256
    },
    {
      "epoch": 0.5216260162601626,
      "grad_norm": 24.382793426513672,
      "learning_rate": 1e-05,
      "loss": 6.5881,
      "step": 9624
    },
    {
      "epoch": 0.5216260162601626,
      "step": 9624,
      "training_loss": 6.387518882751465
    },
    {
      "epoch": 0.521680216802168,
      "step": 9625,
      "training_loss": 6.314081192016602
    },
    {
      "epoch": 0.5217344173441735,
      "step": 9626,
      "training_loss": 6.17048454284668
    },
    {
      "epoch": 0.5217886178861788,
      "step": 9627,
      "training_loss": 6.722458839416504
    },
    {
      "epoch": 0.5218428184281843,
      "grad_norm": 29.06219482421875,
      "learning_rate": 1e-05,
      "loss": 6.3986,
      "step": 9628
    },
    {
      "epoch": 0.5218428184281843,
      "step": 9628,
      "training_loss": 6.889704704284668
    },
    {
      "epoch": 0.5218970189701897,
      "step": 9629,
      "training_loss": 2.5911002159118652
    },
    {
      "epoch": 0.5219512195121951,
      "step": 9630,
      "training_loss": 4.760660648345947
    },
    {
      "epoch": 0.5220054200542006,
      "step": 9631,
      "training_loss": 5.509957790374756
    },
    {
      "epoch": 0.5220596205962059,
      "grad_norm": 31.314525604248047,
      "learning_rate": 1e-05,
      "loss": 4.9379,
      "step": 9632
    },
    {
      "epoch": 0.5220596205962059,
      "step": 9632,
      "training_loss": 6.636502742767334
    },
    {
      "epoch": 0.5221138211382114,
      "step": 9633,
      "training_loss": 6.586602210998535
    },
    {
      "epoch": 0.5221680216802168,
      "step": 9634,
      "training_loss": 7.2720561027526855
    },
    {
      "epoch": 0.5222222222222223,
      "step": 9635,
      "training_loss": 6.866654872894287
    },
    {
      "epoch": 0.5222764227642276,
      "grad_norm": 21.112987518310547,
      "learning_rate": 1e-05,
      "loss": 6.8405,
      "step": 9636
    },
    {
      "epoch": 0.5222764227642276,
      "step": 9636,
      "training_loss": 6.83845329284668
    },
    {
      "epoch": 0.522330623306233,
      "step": 9637,
      "training_loss": 7.1938796043396
    },
    {
      "epoch": 0.5223848238482385,
      "step": 9638,
      "training_loss": 5.256102085113525
    },
    {
      "epoch": 0.5224390243902439,
      "step": 9639,
      "training_loss": 6.570268154144287
    },
    {
      "epoch": 0.5224932249322494,
      "grad_norm": 30.790847778320312,
      "learning_rate": 1e-05,
      "loss": 6.4647,
      "step": 9640
    },
    {
      "epoch": 0.5224932249322494,
      "step": 9640,
      "training_loss": 5.579808235168457
    },
    {
      "epoch": 0.5225474254742547,
      "step": 9641,
      "training_loss": 4.80472993850708
    },
    {
      "epoch": 0.5226016260162601,
      "step": 9642,
      "training_loss": 7.458771228790283
    },
    {
      "epoch": 0.5226558265582656,
      "step": 9643,
      "training_loss": 3.927602767944336
    },
    {
      "epoch": 0.522710027100271,
      "grad_norm": 40.53163146972656,
      "learning_rate": 1e-05,
      "loss": 5.4427,
      "step": 9644
    },
    {
      "epoch": 0.522710027100271,
      "step": 9644,
      "training_loss": 6.3685526847839355
    },
    {
      "epoch": 0.5227642276422764,
      "step": 9645,
      "training_loss": 8.113601684570312
    },
    {
      "epoch": 0.5228184281842818,
      "step": 9646,
      "training_loss": 7.103826522827148
    },
    {
      "epoch": 0.5228726287262873,
      "step": 9647,
      "training_loss": 5.38153600692749
    },
    {
      "epoch": 0.5229268292682927,
      "grad_norm": 67.52688598632812,
      "learning_rate": 1e-05,
      "loss": 6.7419,
      "step": 9648
    },
    {
      "epoch": 0.5229268292682927,
      "step": 9648,
      "training_loss": 6.582376956939697
    },
    {
      "epoch": 0.5229810298102981,
      "step": 9649,
      "training_loss": 6.586444854736328
    },
    {
      "epoch": 0.5230352303523035,
      "step": 9650,
      "training_loss": 6.427563190460205
    },
    {
      "epoch": 0.5230894308943089,
      "step": 9651,
      "training_loss": 7.578298568725586
    },
    {
      "epoch": 0.5231436314363144,
      "grad_norm": 26.01799964904785,
      "learning_rate": 1e-05,
      "loss": 6.7937,
      "step": 9652
    },
    {
      "epoch": 0.5231436314363144,
      "step": 9652,
      "training_loss": 6.927414894104004
    },
    {
      "epoch": 0.5231978319783198,
      "step": 9653,
      "training_loss": 6.890048027038574
    },
    {
      "epoch": 0.5232520325203251,
      "step": 9654,
      "training_loss": 5.574130535125732
    },
    {
      "epoch": 0.5233062330623306,
      "step": 9655,
      "training_loss": 6.705051898956299
    },
    {
      "epoch": 0.523360433604336,
      "grad_norm": 26.894975662231445,
      "learning_rate": 1e-05,
      "loss": 6.5242,
      "step": 9656
    },
    {
      "epoch": 0.523360433604336,
      "step": 9656,
      "training_loss": 6.418520927429199
    },
    {
      "epoch": 0.5234146341463415,
      "step": 9657,
      "training_loss": 7.832317352294922
    },
    {
      "epoch": 0.5234688346883469,
      "step": 9658,
      "training_loss": 6.84884786605835
    },
    {
      "epoch": 0.5235230352303523,
      "step": 9659,
      "training_loss": 5.470003128051758
    },
    {
      "epoch": 0.5235772357723577,
      "grad_norm": 30.486373901367188,
      "learning_rate": 1e-05,
      "loss": 6.6424,
      "step": 9660
    },
    {
      "epoch": 0.5235772357723577,
      "step": 9660,
      "training_loss": 5.983697414398193
    },
    {
      "epoch": 0.5236314363143632,
      "step": 9661,
      "training_loss": 5.778126239776611
    },
    {
      "epoch": 0.5236856368563686,
      "step": 9662,
      "training_loss": 6.304009437561035
    },
    {
      "epoch": 0.5237398373983739,
      "step": 9663,
      "training_loss": 6.833927154541016
    },
    {
      "epoch": 0.5237940379403794,
      "grad_norm": 27.157337188720703,
      "learning_rate": 1e-05,
      "loss": 6.2249,
      "step": 9664
    },
    {
      "epoch": 0.5237940379403794,
      "step": 9664,
      "training_loss": 7.100186347961426
    },
    {
      "epoch": 0.5238482384823848,
      "step": 9665,
      "training_loss": 5.85746431350708
    },
    {
      "epoch": 0.5239024390243903,
      "step": 9666,
      "training_loss": 6.146460056304932
    },
    {
      "epoch": 0.5239566395663957,
      "step": 9667,
      "training_loss": 6.722309112548828
    },
    {
      "epoch": 0.524010840108401,
      "grad_norm": 21.341167449951172,
      "learning_rate": 1e-05,
      "loss": 6.4566,
      "step": 9668
    },
    {
      "epoch": 0.524010840108401,
      "step": 9668,
      "training_loss": 6.05898904800415
    },
    {
      "epoch": 0.5240650406504065,
      "step": 9669,
      "training_loss": 5.439909934997559
    },
    {
      "epoch": 0.5241192411924119,
      "step": 9670,
      "training_loss": 7.274693965911865
    },
    {
      "epoch": 0.5241734417344174,
      "step": 9671,
      "training_loss": 6.688558578491211
    },
    {
      "epoch": 0.5242276422764227,
      "grad_norm": 44.50340270996094,
      "learning_rate": 1e-05,
      "loss": 6.3655,
      "step": 9672
    },
    {
      "epoch": 0.5242276422764227,
      "step": 9672,
      "training_loss": 5.974871635437012
    },
    {
      "epoch": 0.5242818428184282,
      "step": 9673,
      "training_loss": 7.014312744140625
    },
    {
      "epoch": 0.5243360433604336,
      "step": 9674,
      "training_loss": 6.901167392730713
    },
    {
      "epoch": 0.524390243902439,
      "step": 9675,
      "training_loss": 5.453174114227295
    },
    {
      "epoch": 0.5244444444444445,
      "grad_norm": 32.079532623291016,
      "learning_rate": 1e-05,
      "loss": 6.3359,
      "step": 9676
    },
    {
      "epoch": 0.5244444444444445,
      "step": 9676,
      "training_loss": 6.05565881729126
    },
    {
      "epoch": 0.5244986449864498,
      "step": 9677,
      "training_loss": 6.635852813720703
    },
    {
      "epoch": 0.5245528455284553,
      "step": 9678,
      "training_loss": 4.932492256164551
    },
    {
      "epoch": 0.5246070460704607,
      "step": 9679,
      "training_loss": 6.617843151092529
    },
    {
      "epoch": 0.5246612466124662,
      "grad_norm": 46.430320739746094,
      "learning_rate": 1e-05,
      "loss": 6.0605,
      "step": 9680
    },
    {
      "epoch": 0.5246612466124662,
      "step": 9680,
      "training_loss": 6.877277851104736
    },
    {
      "epoch": 0.5247154471544715,
      "step": 9681,
      "training_loss": 5.003950595855713
    },
    {
      "epoch": 0.5247696476964769,
      "step": 9682,
      "training_loss": 5.407961845397949
    },
    {
      "epoch": 0.5248238482384824,
      "step": 9683,
      "training_loss": 7.6607842445373535
    },
    {
      "epoch": 0.5248780487804878,
      "grad_norm": 33.99900436401367,
      "learning_rate": 1e-05,
      "loss": 6.2375,
      "step": 9684
    },
    {
      "epoch": 0.5248780487804878,
      "step": 9684,
      "training_loss": 5.89048433303833
    },
    {
      "epoch": 0.5249322493224933,
      "step": 9685,
      "training_loss": 5.228835582733154
    },
    {
      "epoch": 0.5249864498644986,
      "step": 9686,
      "training_loss": 6.33176851272583
    },
    {
      "epoch": 0.525040650406504,
      "step": 9687,
      "training_loss": 6.454526424407959
    },
    {
      "epoch": 0.5250948509485095,
      "grad_norm": 25.195188522338867,
      "learning_rate": 1e-05,
      "loss": 5.9764,
      "step": 9688
    },
    {
      "epoch": 0.5250948509485095,
      "step": 9688,
      "training_loss": 6.679466724395752
    },
    {
      "epoch": 0.5251490514905149,
      "step": 9689,
      "training_loss": 6.349140167236328
    },
    {
      "epoch": 0.5252032520325203,
      "step": 9690,
      "training_loss": 5.725728988647461
    },
    {
      "epoch": 0.5252574525745257,
      "step": 9691,
      "training_loss": 6.940592288970947
    },
    {
      "epoch": 0.5253116531165312,
      "grad_norm": 31.50779914855957,
      "learning_rate": 1e-05,
      "loss": 6.4237,
      "step": 9692
    },
    {
      "epoch": 0.5253116531165312,
      "step": 9692,
      "training_loss": 5.845620632171631
    },
    {
      "epoch": 0.5253658536585366,
      "step": 9693,
      "training_loss": 8.164358139038086
    },
    {
      "epoch": 0.525420054200542,
      "step": 9694,
      "training_loss": 7.1963372230529785
    },
    {
      "epoch": 0.5254742547425474,
      "step": 9695,
      "training_loss": 6.760117053985596
    },
    {
      "epoch": 0.5255284552845528,
      "grad_norm": 30.486108779907227,
      "learning_rate": 1e-05,
      "loss": 6.9916,
      "step": 9696
    },
    {
      "epoch": 0.5255284552845528,
      "step": 9696,
      "training_loss": 6.008039951324463
    },
    {
      "epoch": 0.5255826558265583,
      "step": 9697,
      "training_loss": 6.178077697753906
    },
    {
      "epoch": 0.5256368563685637,
      "step": 9698,
      "training_loss": 6.614447116851807
    },
    {
      "epoch": 0.525691056910569,
      "step": 9699,
      "training_loss": 7.07179069519043
    },
    {
      "epoch": 0.5257452574525745,
      "grad_norm": 44.29544448852539,
      "learning_rate": 1e-05,
      "loss": 6.4681,
      "step": 9700
    },
    {
      "epoch": 0.5257452574525745,
      "step": 9700,
      "training_loss": 7.418595314025879
    },
    {
      "epoch": 0.5257994579945799,
      "step": 9701,
      "training_loss": 7.151765823364258
    },
    {
      "epoch": 0.5258536585365854,
      "step": 9702,
      "training_loss": 7.136828422546387
    },
    {
      "epoch": 0.5259078590785908,
      "step": 9703,
      "training_loss": 5.085784912109375
    },
    {
      "epoch": 0.5259620596205962,
      "grad_norm": 20.269027709960938,
      "learning_rate": 1e-05,
      "loss": 6.6982,
      "step": 9704
    },
    {
      "epoch": 0.5259620596205962,
      "step": 9704,
      "training_loss": 7.6161980628967285
    },
    {
      "epoch": 0.5260162601626016,
      "step": 9705,
      "training_loss": 4.164944171905518
    },
    {
      "epoch": 0.5260704607046071,
      "step": 9706,
      "training_loss": 6.020112991333008
    },
    {
      "epoch": 0.5261246612466125,
      "step": 9707,
      "training_loss": 5.833414554595947
    },
    {
      "epoch": 0.5261788617886178,
      "grad_norm": 41.543365478515625,
      "learning_rate": 1e-05,
      "loss": 5.9087,
      "step": 9708
    },
    {
      "epoch": 0.5261788617886178,
      "step": 9708,
      "training_loss": 5.291521072387695
    },
    {
      "epoch": 0.5262330623306233,
      "step": 9709,
      "training_loss": 6.4018754959106445
    },
    {
      "epoch": 0.5262872628726287,
      "step": 9710,
      "training_loss": 6.701310634613037
    },
    {
      "epoch": 0.5263414634146342,
      "step": 9711,
      "training_loss": 6.609109878540039
    },
    {
      "epoch": 0.5263956639566396,
      "grad_norm": 17.47312355041504,
      "learning_rate": 1e-05,
      "loss": 6.251,
      "step": 9712
    },
    {
      "epoch": 0.5263956639566396,
      "step": 9712,
      "training_loss": 6.7333807945251465
    },
    {
      "epoch": 0.526449864498645,
      "step": 9713,
      "training_loss": 7.494507312774658
    },
    {
      "epoch": 0.5265040650406504,
      "step": 9714,
      "training_loss": 7.139697074890137
    },
    {
      "epoch": 0.5265582655826558,
      "step": 9715,
      "training_loss": 5.9849348068237305
    },
    {
      "epoch": 0.5266124661246613,
      "grad_norm": 35.972591400146484,
      "learning_rate": 1e-05,
      "loss": 6.8381,
      "step": 9716
    },
    {
      "epoch": 0.5266124661246613,
      "step": 9716,
      "training_loss": 7.232282638549805
    },
    {
      "epoch": 0.5266666666666666,
      "step": 9717,
      "training_loss": 6.660080432891846
    },
    {
      "epoch": 0.5267208672086721,
      "step": 9718,
      "training_loss": 5.686517238616943
    },
    {
      "epoch": 0.5267750677506775,
      "step": 9719,
      "training_loss": 5.274191856384277
    },
    {
      "epoch": 0.526829268292683,
      "grad_norm": 21.49020004272461,
      "learning_rate": 1e-05,
      "loss": 6.2133,
      "step": 9720
    },
    {
      "epoch": 0.526829268292683,
      "step": 9720,
      "training_loss": 3.7950568199157715
    },
    {
      "epoch": 0.5268834688346884,
      "step": 9721,
      "training_loss": 6.681818008422852
    },
    {
      "epoch": 0.5269376693766937,
      "step": 9722,
      "training_loss": 7.523965358734131
    },
    {
      "epoch": 0.5269918699186992,
      "step": 9723,
      "training_loss": 5.245815753936768
    },
    {
      "epoch": 0.5270460704607046,
      "grad_norm": 23.577775955200195,
      "learning_rate": 1e-05,
      "loss": 5.8117,
      "step": 9724
    },
    {
      "epoch": 0.5270460704607046,
      "step": 9724,
      "training_loss": 6.516645908355713
    },
    {
      "epoch": 0.5271002710027101,
      "step": 9725,
      "training_loss": 7.303182125091553
    },
    {
      "epoch": 0.5271544715447154,
      "step": 9726,
      "training_loss": 6.558983325958252
    },
    {
      "epoch": 0.5272086720867208,
      "step": 9727,
      "training_loss": 5.911112308502197
    },
    {
      "epoch": 0.5272628726287263,
      "grad_norm": 16.086668014526367,
      "learning_rate": 1e-05,
      "loss": 6.5725,
      "step": 9728
    },
    {
      "epoch": 0.5272628726287263,
      "step": 9728,
      "training_loss": 6.034470081329346
    },
    {
      "epoch": 0.5273170731707317,
      "step": 9729,
      "training_loss": 5.053601264953613
    },
    {
      "epoch": 0.5273712737127372,
      "step": 9730,
      "training_loss": 6.832926273345947
    },
    {
      "epoch": 0.5274254742547425,
      "step": 9731,
      "training_loss": 8.81865406036377
    },
    {
      "epoch": 0.527479674796748,
      "grad_norm": 40.678855895996094,
      "learning_rate": 1e-05,
      "loss": 6.6849,
      "step": 9732
    },
    {
      "epoch": 0.527479674796748,
      "step": 9732,
      "training_loss": 4.554589748382568
    },
    {
      "epoch": 0.5275338753387534,
      "step": 9733,
      "training_loss": 6.046540260314941
    },
    {
      "epoch": 0.5275880758807588,
      "step": 9734,
      "training_loss": 8.450616836547852
    },
    {
      "epoch": 0.5276422764227642,
      "step": 9735,
      "training_loss": 7.576934337615967
    },
    {
      "epoch": 0.5276964769647696,
      "grad_norm": 37.34990692138672,
      "learning_rate": 1e-05,
      "loss": 6.6572,
      "step": 9736
    },
    {
      "epoch": 0.5276964769647696,
      "step": 9736,
      "training_loss": 5.4718241691589355
    },
    {
      "epoch": 0.5277506775067751,
      "step": 9737,
      "training_loss": 6.915360927581787
    },
    {
      "epoch": 0.5278048780487805,
      "step": 9738,
      "training_loss": 5.53808069229126
    },
    {
      "epoch": 0.527859078590786,
      "step": 9739,
      "training_loss": 4.89952278137207
    },
    {
      "epoch": 0.5279132791327913,
      "grad_norm": 37.9547233581543,
      "learning_rate": 1e-05,
      "loss": 5.7062,
      "step": 9740
    },
    {
      "epoch": 0.5279132791327913,
      "step": 9740,
      "training_loss": 7.221657752990723
    },
    {
      "epoch": 0.5279674796747967,
      "step": 9741,
      "training_loss": 6.366672992706299
    },
    {
      "epoch": 0.5280216802168022,
      "step": 9742,
      "training_loss": 7.261107921600342
    },
    {
      "epoch": 0.5280758807588076,
      "step": 9743,
      "training_loss": 4.595312118530273
    },
    {
      "epoch": 0.528130081300813,
      "grad_norm": 24.95877456665039,
      "learning_rate": 1e-05,
      "loss": 6.3612,
      "step": 9744
    },
    {
      "epoch": 0.528130081300813,
      "step": 9744,
      "training_loss": 4.557188510894775
    },
    {
      "epoch": 0.5281842818428184,
      "step": 9745,
      "training_loss": 4.928098678588867
    },
    {
      "epoch": 0.5282384823848238,
      "step": 9746,
      "training_loss": 7.5500078201293945
    },
    {
      "epoch": 0.5282926829268293,
      "step": 9747,
      "training_loss": 5.677328109741211
    },
    {
      "epoch": 0.5283468834688347,
      "grad_norm": 39.5737419128418,
      "learning_rate": 1e-05,
      "loss": 5.6782,
      "step": 9748
    },
    {
      "epoch": 0.5283468834688347,
      "step": 9748,
      "training_loss": 6.978540420532227
    },
    {
      "epoch": 0.5284010840108401,
      "step": 9749,
      "training_loss": 6.822380542755127
    },
    {
      "epoch": 0.5284552845528455,
      "step": 9750,
      "training_loss": 6.080850124359131
    },
    {
      "epoch": 0.528509485094851,
      "step": 9751,
      "training_loss": 7.650949478149414
    },
    {
      "epoch": 0.5285636856368564,
      "grad_norm": 27.771467208862305,
      "learning_rate": 1e-05,
      "loss": 6.8832,
      "step": 9752
    },
    {
      "epoch": 0.5285636856368564,
      "step": 9752,
      "training_loss": 6.827139377593994
    },
    {
      "epoch": 0.5286178861788617,
      "step": 9753,
      "training_loss": 7.04806661605835
    },
    {
      "epoch": 0.5286720867208672,
      "step": 9754,
      "training_loss": 7.032912731170654
    },
    {
      "epoch": 0.5287262872628726,
      "step": 9755,
      "training_loss": 6.9453864097595215
    },
    {
      "epoch": 0.5287804878048781,
      "grad_norm": 21.933883666992188,
      "learning_rate": 1e-05,
      "loss": 6.9634,
      "step": 9756
    },
    {
      "epoch": 0.5287804878048781,
      "step": 9756,
      "training_loss": 4.915214538574219
    },
    {
      "epoch": 0.5288346883468835,
      "step": 9757,
      "training_loss": 2.669625759124756
    },
    {
      "epoch": 0.5288888888888889,
      "step": 9758,
      "training_loss": 4.163006782531738
    },
    {
      "epoch": 0.5289430894308943,
      "step": 9759,
      "training_loss": 7.2818169593811035
    },
    {
      "epoch": 0.5289972899728997,
      "grad_norm": 41.63895797729492,
      "learning_rate": 1e-05,
      "loss": 4.7574,
      "step": 9760
    },
    {
      "epoch": 0.5289972899728997,
      "step": 9760,
      "training_loss": 6.5558881759643555
    },
    {
      "epoch": 0.5290514905149052,
      "step": 9761,
      "training_loss": 7.326479434967041
    },
    {
      "epoch": 0.5291056910569105,
      "step": 9762,
      "training_loss": 6.412082195281982
    },
    {
      "epoch": 0.529159891598916,
      "step": 9763,
      "training_loss": 7.675337314605713
    },
    {
      "epoch": 0.5292140921409214,
      "grad_norm": 24.761594772338867,
      "learning_rate": 1e-05,
      "loss": 6.9924,
      "step": 9764
    },
    {
      "epoch": 0.5292140921409214,
      "step": 9764,
      "training_loss": 7.148303985595703
    },
    {
      "epoch": 0.5292682926829269,
      "step": 9765,
      "training_loss": 6.593478202819824
    },
    {
      "epoch": 0.5293224932249323,
      "step": 9766,
      "training_loss": 6.848930358886719
    },
    {
      "epoch": 0.5293766937669376,
      "step": 9767,
      "training_loss": 5.9249491691589355
    },
    {
      "epoch": 0.5294308943089431,
      "grad_norm": 36.052799224853516,
      "learning_rate": 1e-05,
      "loss": 6.6289,
      "step": 9768
    },
    {
      "epoch": 0.5294308943089431,
      "step": 9768,
      "training_loss": 5.190903663635254
    },
    {
      "epoch": 0.5294850948509485,
      "step": 9769,
      "training_loss": 7.078761100769043
    },
    {
      "epoch": 0.529539295392954,
      "step": 9770,
      "training_loss": 7.779417514801025
    },
    {
      "epoch": 0.5295934959349593,
      "step": 9771,
      "training_loss": 6.294119358062744
    },
    {
      "epoch": 0.5296476964769647,
      "grad_norm": 25.51953125,
      "learning_rate": 1e-05,
      "loss": 6.5858,
      "step": 9772
    },
    {
      "epoch": 0.5296476964769647,
      "step": 9772,
      "training_loss": 7.372880935668945
    },
    {
      "epoch": 0.5297018970189702,
      "step": 9773,
      "training_loss": 7.0222649574279785
    },
    {
      "epoch": 0.5297560975609756,
      "step": 9774,
      "training_loss": 7.1469268798828125
    },
    {
      "epoch": 0.5298102981029811,
      "step": 9775,
      "training_loss": 5.029252529144287
    },
    {
      "epoch": 0.5298644986449864,
      "grad_norm": 37.86753845214844,
      "learning_rate": 1e-05,
      "loss": 6.6428,
      "step": 9776
    },
    {
      "epoch": 0.5298644986449864,
      "step": 9776,
      "training_loss": 7.096391201019287
    },
    {
      "epoch": 0.5299186991869919,
      "step": 9777,
      "training_loss": 4.582314968109131
    },
    {
      "epoch": 0.5299728997289973,
      "step": 9778,
      "training_loss": 6.623559951782227
    },
    {
      "epoch": 0.5300271002710027,
      "step": 9779,
      "training_loss": 6.287086009979248
    },
    {
      "epoch": 0.5300813008130081,
      "grad_norm": 45.93681716918945,
      "learning_rate": 1e-05,
      "loss": 6.1473,
      "step": 9780
    },
    {
      "epoch": 0.5300813008130081,
      "step": 9780,
      "training_loss": 6.915719509124756
    },
    {
      "epoch": 0.5301355013550135,
      "step": 9781,
      "training_loss": 6.325526714324951
    },
    {
      "epoch": 0.530189701897019,
      "step": 9782,
      "training_loss": 6.542122840881348
    },
    {
      "epoch": 0.5302439024390244,
      "step": 9783,
      "training_loss": 8.024178504943848
    },
    {
      "epoch": 0.5302981029810299,
      "grad_norm": 26.14285659790039,
      "learning_rate": 1e-05,
      "loss": 6.9519,
      "step": 9784
    },
    {
      "epoch": 0.5302981029810299,
      "step": 9784,
      "training_loss": 7.132741451263428
    },
    {
      "epoch": 0.5303523035230352,
      "step": 9785,
      "training_loss": 6.237795352935791
    },
    {
      "epoch": 0.5304065040650406,
      "step": 9786,
      "training_loss": 6.1982574462890625
    },
    {
      "epoch": 0.5304607046070461,
      "step": 9787,
      "training_loss": 7.729539394378662
    },
    {
      "epoch": 0.5305149051490515,
      "grad_norm": 28.002195358276367,
      "learning_rate": 1e-05,
      "loss": 6.8246,
      "step": 9788
    },
    {
      "epoch": 0.5305149051490515,
      "step": 9788,
      "training_loss": 6.75046968460083
    },
    {
      "epoch": 0.5305691056910569,
      "step": 9789,
      "training_loss": 5.093532085418701
    },
    {
      "epoch": 0.5306233062330623,
      "step": 9790,
      "training_loss": 6.298792362213135
    },
    {
      "epoch": 0.5306775067750678,
      "step": 9791,
      "training_loss": 7.655940532684326
    },
    {
      "epoch": 0.5307317073170732,
      "grad_norm": 40.006900787353516,
      "learning_rate": 1e-05,
      "loss": 6.4497,
      "step": 9792
    },
    {
      "epoch": 0.5307317073170732,
      "step": 9792,
      "training_loss": 7.363857746124268
    },
    {
      "epoch": 0.5307859078590786,
      "step": 9793,
      "training_loss": 5.628309726715088
    },
    {
      "epoch": 0.530840108401084,
      "step": 9794,
      "training_loss": 6.7273850440979
    },
    {
      "epoch": 0.5308943089430894,
      "step": 9795,
      "training_loss": 6.092717170715332
    },
    {
      "epoch": 0.5309485094850949,
      "grad_norm": 23.12781524658203,
      "learning_rate": 1e-05,
      "loss": 6.4531,
      "step": 9796
    },
    {
      "epoch": 0.5309485094850949,
      "step": 9796,
      "training_loss": 6.0570268630981445
    },
    {
      "epoch": 0.5310027100271003,
      "step": 9797,
      "training_loss": 7.038976669311523
    },
    {
      "epoch": 0.5310569105691056,
      "step": 9798,
      "training_loss": 6.61879301071167
    },
    {
      "epoch": 0.5311111111111111,
      "step": 9799,
      "training_loss": 6.9529266357421875
    },
    {
      "epoch": 0.5311653116531165,
      "grad_norm": 35.08141326904297,
      "learning_rate": 1e-05,
      "loss": 6.6669,
      "step": 9800
    },
    {
      "epoch": 0.5311653116531165,
      "step": 9800,
      "training_loss": 7.215754508972168
    },
    {
      "epoch": 0.531219512195122,
      "step": 9801,
      "training_loss": 6.791914939880371
    },
    {
      "epoch": 0.5312737127371274,
      "step": 9802,
      "training_loss": 5.619354248046875
    },
    {
      "epoch": 0.5313279132791328,
      "step": 9803,
      "training_loss": 6.70796012878418
    },
    {
      "epoch": 0.5313821138211382,
      "grad_norm": 21.581464767456055,
      "learning_rate": 1e-05,
      "loss": 6.5837,
      "step": 9804
    },
    {
      "epoch": 0.5313821138211382,
      "step": 9804,
      "training_loss": 5.7322163581848145
    },
    {
      "epoch": 0.5314363143631436,
      "step": 9805,
      "training_loss": 7.172445297241211
    },
    {
      "epoch": 0.5314905149051491,
      "step": 9806,
      "training_loss": 6.846028804779053
    },
    {
      "epoch": 0.5315447154471544,
      "step": 9807,
      "training_loss": 5.61526346206665
    },
    {
      "epoch": 0.5315989159891599,
      "grad_norm": 78.49889373779297,
      "learning_rate": 1e-05,
      "loss": 6.3415,
      "step": 9808
    },
    {
      "epoch": 0.5315989159891599,
      "step": 9808,
      "training_loss": 6.415689945220947
    },
    {
      "epoch": 0.5316531165311653,
      "step": 9809,
      "training_loss": 6.555326461791992
    },
    {
      "epoch": 0.5317073170731708,
      "step": 9810,
      "training_loss": 5.537311553955078
    },
    {
      "epoch": 0.5317615176151762,
      "step": 9811,
      "training_loss": 7.13571310043335
    },
    {
      "epoch": 0.5318157181571815,
      "grad_norm": 19.921419143676758,
      "learning_rate": 1e-05,
      "loss": 6.411,
      "step": 9812
    },
    {
      "epoch": 0.5318157181571815,
      "step": 9812,
      "training_loss": 5.748772144317627
    },
    {
      "epoch": 0.531869918699187,
      "step": 9813,
      "training_loss": 6.741727828979492
    },
    {
      "epoch": 0.5319241192411924,
      "step": 9814,
      "training_loss": 6.827011585235596
    },
    {
      "epoch": 0.5319783197831979,
      "step": 9815,
      "training_loss": 6.9182610511779785
    },
    {
      "epoch": 0.5320325203252032,
      "grad_norm": 28.5490779876709,
      "learning_rate": 1e-05,
      "loss": 6.5589,
      "step": 9816
    },
    {
      "epoch": 0.5320325203252032,
      "step": 9816,
      "training_loss": 6.700838565826416
    },
    {
      "epoch": 0.5320867208672087,
      "step": 9817,
      "training_loss": 6.931272506713867
    },
    {
      "epoch": 0.5321409214092141,
      "step": 9818,
      "training_loss": 7.420947551727295
    },
    {
      "epoch": 0.5321951219512195,
      "step": 9819,
      "training_loss": 7.660480499267578
    },
    {
      "epoch": 0.532249322493225,
      "grad_norm": 31.328405380249023,
      "learning_rate": 1e-05,
      "loss": 7.1784,
      "step": 9820
    },
    {
      "epoch": 0.532249322493225,
      "step": 9820,
      "training_loss": 7.276173114776611
    },
    {
      "epoch": 0.5323035230352303,
      "step": 9821,
      "training_loss": 5.660027980804443
    },
    {
      "epoch": 0.5323577235772358,
      "step": 9822,
      "training_loss": 7.155582904815674
    },
    {
      "epoch": 0.5324119241192412,
      "step": 9823,
      "training_loss": 5.3317742347717285
    },
    {
      "epoch": 0.5324661246612467,
      "grad_norm": 40.54351043701172,
      "learning_rate": 1e-05,
      "loss": 6.3559,
      "step": 9824
    },
    {
      "epoch": 0.5324661246612467,
      "step": 9824,
      "training_loss": 5.289310932159424
    },
    {
      "epoch": 0.532520325203252,
      "step": 9825,
      "training_loss": 5.733797073364258
    },
    {
      "epoch": 0.5325745257452574,
      "step": 9826,
      "training_loss": 5.900796890258789
    },
    {
      "epoch": 0.5326287262872629,
      "step": 9827,
      "training_loss": 6.175434112548828
    },
    {
      "epoch": 0.5326829268292683,
      "grad_norm": 23.005462646484375,
      "learning_rate": 1e-05,
      "loss": 5.7748,
      "step": 9828
    },
    {
      "epoch": 0.5326829268292683,
      "step": 9828,
      "training_loss": 4.084205150604248
    },
    {
      "epoch": 0.5327371273712738,
      "step": 9829,
      "training_loss": 7.967818260192871
    },
    {
      "epoch": 0.5327913279132791,
      "step": 9830,
      "training_loss": 5.70588493347168
    },
    {
      "epoch": 0.5328455284552845,
      "step": 9831,
      "training_loss": 3.4190192222595215
    },
    {
      "epoch": 0.53289972899729,
      "grad_norm": 32.79330062866211,
      "learning_rate": 1e-05,
      "loss": 5.2942,
      "step": 9832
    },
    {
      "epoch": 0.53289972899729,
      "step": 9832,
      "training_loss": 7.082696437835693
    },
    {
      "epoch": 0.5329539295392954,
      "step": 9833,
      "training_loss": 6.75284481048584
    },
    {
      "epoch": 0.5330081300813008,
      "step": 9834,
      "training_loss": 6.766963481903076
    },
    {
      "epoch": 0.5330623306233062,
      "step": 9835,
      "training_loss": 7.3140034675598145
    },
    {
      "epoch": 0.5331165311653117,
      "grad_norm": 25.66160011291504,
      "learning_rate": 1e-05,
      "loss": 6.9791,
      "step": 9836
    },
    {
      "epoch": 0.5331165311653117,
      "step": 9836,
      "training_loss": 6.443922519683838
    },
    {
      "epoch": 0.5331707317073171,
      "step": 9837,
      "training_loss": 6.9173078536987305
    },
    {
      "epoch": 0.5332249322493225,
      "step": 9838,
      "training_loss": 6.309428691864014
    },
    {
      "epoch": 0.5332791327913279,
      "step": 9839,
      "training_loss": 5.243016719818115
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 32.09250259399414,
      "learning_rate": 1e-05,
      "loss": 6.2284,
      "step": 9840
    },
    {
      "epoch": 0.5333333333333333,
      "step": 9840,
      "training_loss": 6.450216770172119
    },
    {
      "epoch": 0.5333875338753388,
      "step": 9841,
      "training_loss": 6.5905351638793945
    },
    {
      "epoch": 0.5334417344173442,
      "step": 9842,
      "training_loss": 5.704099655151367
    },
    {
      "epoch": 0.5334959349593495,
      "step": 9843,
      "training_loss": 6.667447090148926
    },
    {
      "epoch": 0.533550135501355,
      "grad_norm": 17.557647705078125,
      "learning_rate": 1e-05,
      "loss": 6.3531,
      "step": 9844
    },
    {
      "epoch": 0.533550135501355,
      "step": 9844,
      "training_loss": 6.5966315269470215
    },
    {
      "epoch": 0.5336043360433604,
      "step": 9845,
      "training_loss": 7.1627092361450195
    },
    {
      "epoch": 0.5336585365853659,
      "step": 9846,
      "training_loss": 7.255829811096191
    },
    {
      "epoch": 0.5337127371273713,
      "step": 9847,
      "training_loss": 7.394549369812012
    },
    {
      "epoch": 0.5337669376693767,
      "grad_norm": 21.572961807250977,
      "learning_rate": 1e-05,
      "loss": 7.1024,
      "step": 9848
    },
    {
      "epoch": 0.5337669376693767,
      "step": 9848,
      "training_loss": 8.073174476623535
    },
    {
      "epoch": 0.5338211382113821,
      "step": 9849,
      "training_loss": 6.3889312744140625
    },
    {
      "epoch": 0.5338753387533876,
      "step": 9850,
      "training_loss": 7.379858493804932
    },
    {
      "epoch": 0.533929539295393,
      "step": 9851,
      "training_loss": 5.32395601272583
    },
    {
      "epoch": 0.5339837398373983,
      "grad_norm": 40.63267517089844,
      "learning_rate": 1e-05,
      "loss": 6.7915,
      "step": 9852
    },
    {
      "epoch": 0.5339837398373983,
      "step": 9852,
      "training_loss": 7.22347354888916
    },
    {
      "epoch": 0.5340379403794038,
      "step": 9853,
      "training_loss": 6.0697197914123535
    },
    {
      "epoch": 0.5340921409214092,
      "step": 9854,
      "training_loss": 7.641605377197266
    },
    {
      "epoch": 0.5341463414634147,
      "step": 9855,
      "training_loss": 6.770161151885986
    },
    {
      "epoch": 0.5342005420054201,
      "grad_norm": 26.06795883178711,
      "learning_rate": 1e-05,
      "loss": 6.9262,
      "step": 9856
    },
    {
      "epoch": 0.5342005420054201,
      "step": 9856,
      "training_loss": 7.574159622192383
    },
    {
      "epoch": 0.5342547425474254,
      "step": 9857,
      "training_loss": 3.351170539855957
    },
    {
      "epoch": 0.5343089430894309,
      "step": 9858,
      "training_loss": 7.295527458190918
    },
    {
      "epoch": 0.5343631436314363,
      "step": 9859,
      "training_loss": 5.228156566619873
    },
    {
      "epoch": 0.5344173441734418,
      "grad_norm": 37.07360076904297,
      "learning_rate": 1e-05,
      "loss": 5.8623,
      "step": 9860
    },
    {
      "epoch": 0.5344173441734418,
      "step": 9860,
      "training_loss": 6.640567302703857
    },
    {
      "epoch": 0.5344715447154471,
      "step": 9861,
      "training_loss": 5.569455146789551
    },
    {
      "epoch": 0.5345257452574526,
      "step": 9862,
      "training_loss": 6.262401580810547
    },
    {
      "epoch": 0.534579945799458,
      "step": 9863,
      "training_loss": 6.860971450805664
    },
    {
      "epoch": 0.5346341463414634,
      "grad_norm": 17.40635871887207,
      "learning_rate": 1e-05,
      "loss": 6.3333,
      "step": 9864
    },
    {
      "epoch": 0.5346341463414634,
      "step": 9864,
      "training_loss": 5.793179035186768
    },
    {
      "epoch": 0.5346883468834689,
      "step": 9865,
      "training_loss": 3.697145938873291
    },
    {
      "epoch": 0.5347425474254742,
      "step": 9866,
      "training_loss": 4.356195449829102
    },
    {
      "epoch": 0.5347967479674797,
      "step": 9867,
      "training_loss": 6.0913872718811035
    },
    {
      "epoch": 0.5348509485094851,
      "grad_norm": 29.16472816467285,
      "learning_rate": 1e-05,
      "loss": 4.9845,
      "step": 9868
    },
    {
      "epoch": 0.5348509485094851,
      "step": 9868,
      "training_loss": 5.5994110107421875
    },
    {
      "epoch": 0.5349051490514906,
      "step": 9869,
      "training_loss": 5.746710777282715
    },
    {
      "epoch": 0.5349593495934959,
      "step": 9870,
      "training_loss": 8.261406898498535
    },
    {
      "epoch": 0.5350135501355013,
      "step": 9871,
      "training_loss": 5.681939601898193
    },
    {
      "epoch": 0.5350677506775068,
      "grad_norm": 23.448200225830078,
      "learning_rate": 1e-05,
      "loss": 6.3224,
      "step": 9872
    },
    {
      "epoch": 0.5350677506775068,
      "step": 9872,
      "training_loss": 6.04384183883667
    },
    {
      "epoch": 0.5351219512195122,
      "step": 9873,
      "training_loss": 4.67739725112915
    },
    {
      "epoch": 0.5351761517615176,
      "step": 9874,
      "training_loss": 8.41773509979248
    },
    {
      "epoch": 0.535230352303523,
      "step": 9875,
      "training_loss": 7.14284086227417
    },
    {
      "epoch": 0.5352845528455284,
      "grad_norm": 24.77376937866211,
      "learning_rate": 1e-05,
      "loss": 6.5705,
      "step": 9876
    },
    {
      "epoch": 0.5352845528455284,
      "step": 9876,
      "training_loss": 7.265745162963867
    },
    {
      "epoch": 0.5353387533875339,
      "step": 9877,
      "training_loss": 5.663590431213379
    },
    {
      "epoch": 0.5353929539295393,
      "step": 9878,
      "training_loss": 6.709957599639893
    },
    {
      "epoch": 0.5354471544715447,
      "step": 9879,
      "training_loss": 5.012781143188477
    },
    {
      "epoch": 0.5355013550135501,
      "grad_norm": 31.676528930664062,
      "learning_rate": 1e-05,
      "loss": 6.163,
      "step": 9880
    },
    {
      "epoch": 0.5355013550135501,
      "step": 9880,
      "training_loss": 6.339178562164307
    },
    {
      "epoch": 0.5355555555555556,
      "step": 9881,
      "training_loss": 6.283968925476074
    },
    {
      "epoch": 0.535609756097561,
      "step": 9882,
      "training_loss": 3.6937263011932373
    },
    {
      "epoch": 0.5356639566395663,
      "step": 9883,
      "training_loss": 7.289591312408447
    },
    {
      "epoch": 0.5357181571815718,
      "grad_norm": 22.73908805847168,
      "learning_rate": 1e-05,
      "loss": 5.9016,
      "step": 9884
    },
    {
      "epoch": 0.5357181571815718,
      "step": 9884,
      "training_loss": 7.069984436035156
    },
    {
      "epoch": 0.5357723577235772,
      "step": 9885,
      "training_loss": 7.635213375091553
    },
    {
      "epoch": 0.5358265582655827,
      "step": 9886,
      "training_loss": 5.379856109619141
    },
    {
      "epoch": 0.5358807588075881,
      "step": 9887,
      "training_loss": 2.8397598266601562
    },
    {
      "epoch": 0.5359349593495935,
      "grad_norm": 30.59885025024414,
      "learning_rate": 1e-05,
      "loss": 5.7312,
      "step": 9888
    },
    {
      "epoch": 0.5359349593495935,
      "step": 9888,
      "training_loss": 7.923654079437256
    },
    {
      "epoch": 0.5359891598915989,
      "step": 9889,
      "training_loss": 6.487328052520752
    },
    {
      "epoch": 0.5360433604336043,
      "step": 9890,
      "training_loss": 5.283884525299072
    },
    {
      "epoch": 0.5360975609756098,
      "step": 9891,
      "training_loss": 6.0763678550720215
    },
    {
      "epoch": 0.5361517615176151,
      "grad_norm": 34.290069580078125,
      "learning_rate": 1e-05,
      "loss": 6.4428,
      "step": 9892
    },
    {
      "epoch": 0.5361517615176151,
      "step": 9892,
      "training_loss": 6.426182746887207
    },
    {
      "epoch": 0.5362059620596206,
      "step": 9893,
      "training_loss": 6.433197021484375
    },
    {
      "epoch": 0.536260162601626,
      "step": 9894,
      "training_loss": 6.839251518249512
    },
    {
      "epoch": 0.5363143631436315,
      "step": 9895,
      "training_loss": 7.133983135223389
    },
    {
      "epoch": 0.5363685636856369,
      "grad_norm": 49.072227478027344,
      "learning_rate": 1e-05,
      "loss": 6.7082,
      "step": 9896
    },
    {
      "epoch": 0.5363685636856369,
      "step": 9896,
      "training_loss": 6.554077625274658
    },
    {
      "epoch": 0.5364227642276422,
      "step": 9897,
      "training_loss": 7.144684791564941
    },
    {
      "epoch": 0.5364769647696477,
      "step": 9898,
      "training_loss": 4.295400142669678
    },
    {
      "epoch": 0.5365311653116531,
      "step": 9899,
      "training_loss": 8.578272819519043
    },
    {
      "epoch": 0.5365853658536586,
      "grad_norm": 42.19497299194336,
      "learning_rate": 1e-05,
      "loss": 6.6431,
      "step": 9900
    },
    {
      "epoch": 0.5365853658536586,
      "step": 9900,
      "training_loss": 8.062020301818848
    },
    {
      "epoch": 0.5366395663956639,
      "step": 9901,
      "training_loss": 7.124002933502197
    },
    {
      "epoch": 0.5366937669376693,
      "step": 9902,
      "training_loss": 6.361248016357422
    },
    {
      "epoch": 0.5367479674796748,
      "step": 9903,
      "training_loss": 3.0151174068450928
    },
    {
      "epoch": 0.5368021680216802,
      "grad_norm": 34.68681716918945,
      "learning_rate": 1e-05,
      "loss": 6.1406,
      "step": 9904
    },
    {
      "epoch": 0.5368021680216802,
      "step": 9904,
      "training_loss": 6.755093097686768
    },
    {
      "epoch": 0.5368563685636857,
      "step": 9905,
      "training_loss": 6.605825901031494
    },
    {
      "epoch": 0.536910569105691,
      "step": 9906,
      "training_loss": 6.923766136169434
    },
    {
      "epoch": 0.5369647696476965,
      "step": 9907,
      "training_loss": 8.251556396484375
    },
    {
      "epoch": 0.5370189701897019,
      "grad_norm": 31.092206954956055,
      "learning_rate": 1e-05,
      "loss": 7.1341,
      "step": 9908
    },
    {
      "epoch": 0.5370189701897019,
      "step": 9908,
      "training_loss": 5.3045220375061035
    },
    {
      "epoch": 0.5370731707317074,
      "step": 9909,
      "training_loss": 6.172341823577881
    },
    {
      "epoch": 0.5371273712737127,
      "step": 9910,
      "training_loss": 6.797322750091553
    },
    {
      "epoch": 0.5371815718157181,
      "step": 9911,
      "training_loss": 7.418909072875977
    },
    {
      "epoch": 0.5372357723577236,
      "grad_norm": 21.826478958129883,
      "learning_rate": 1e-05,
      "loss": 6.4233,
      "step": 9912
    },
    {
      "epoch": 0.5372357723577236,
      "step": 9912,
      "training_loss": 6.699124813079834
    },
    {
      "epoch": 0.537289972899729,
      "step": 9913,
      "training_loss": 7.2060322761535645
    },
    {
      "epoch": 0.5373441734417345,
      "step": 9914,
      "training_loss": 6.131251335144043
    },
    {
      "epoch": 0.5373983739837398,
      "step": 9915,
      "training_loss": 4.434967517852783
    },
    {
      "epoch": 0.5374525745257452,
      "grad_norm": 35.79872131347656,
      "learning_rate": 1e-05,
      "loss": 6.1178,
      "step": 9916
    },
    {
      "epoch": 0.5374525745257452,
      "step": 9916,
      "training_loss": 5.244661331176758
    },
    {
      "epoch": 0.5375067750677507,
      "step": 9917,
      "training_loss": 3.5527005195617676
    },
    {
      "epoch": 0.5375609756097561,
      "step": 9918,
      "training_loss": 4.5836896896362305
    },
    {
      "epoch": 0.5376151761517615,
      "step": 9919,
      "training_loss": 7.14425802230835
    },
    {
      "epoch": 0.5376693766937669,
      "grad_norm": 19.326005935668945,
      "learning_rate": 1e-05,
      "loss": 5.1313,
      "step": 9920
    },
    {
      "epoch": 0.5376693766937669,
      "step": 9920,
      "training_loss": 6.681807994842529
    },
    {
      "epoch": 0.5377235772357724,
      "step": 9921,
      "training_loss": 6.2459211349487305
    },
    {
      "epoch": 0.5377777777777778,
      "step": 9922,
      "training_loss": 6.567477226257324
    },
    {
      "epoch": 0.5378319783197832,
      "step": 9923,
      "training_loss": 8.611211776733398
    },
    {
      "epoch": 0.5378861788617886,
      "grad_norm": 40.313011169433594,
      "learning_rate": 1e-05,
      "loss": 7.0266,
      "step": 9924
    },
    {
      "epoch": 0.5378861788617886,
      "step": 9924,
      "training_loss": 7.779663562774658
    },
    {
      "epoch": 0.537940379403794,
      "step": 9925,
      "training_loss": 7.336366176605225
    },
    {
      "epoch": 0.5379945799457995,
      "step": 9926,
      "training_loss": 6.8769850730896
    },
    {
      "epoch": 0.5380487804878049,
      "step": 9927,
      "training_loss": 7.05618143081665
    },
    {
      "epoch": 0.5381029810298102,
      "grad_norm": 21.146820068359375,
      "learning_rate": 1e-05,
      "loss": 7.2623,
      "step": 9928
    },
    {
      "epoch": 0.5381029810298102,
      "step": 9928,
      "training_loss": 7.275004863739014
    },
    {
      "epoch": 0.5381571815718157,
      "step": 9929,
      "training_loss": 7.4874958992004395
    },
    {
      "epoch": 0.5382113821138211,
      "step": 9930,
      "training_loss": 7.837551116943359
    },
    {
      "epoch": 0.5382655826558266,
      "step": 9931,
      "training_loss": 7.047699928283691
    },
    {
      "epoch": 0.538319783197832,
      "grad_norm": 66.56848907470703,
      "learning_rate": 1e-05,
      "loss": 7.4119,
      "step": 9932
    },
    {
      "epoch": 0.538319783197832,
      "step": 9932,
      "training_loss": 7.695091724395752
    },
    {
      "epoch": 0.5383739837398374,
      "step": 9933,
      "training_loss": 6.653684139251709
    },
    {
      "epoch": 0.5384281842818428,
      "step": 9934,
      "training_loss": 4.989891529083252
    },
    {
      "epoch": 0.5384823848238482,
      "step": 9935,
      "training_loss": 6.925408840179443
    },
    {
      "epoch": 0.5385365853658537,
      "grad_norm": 22.311817169189453,
      "learning_rate": 1e-05,
      "loss": 6.566,
      "step": 9936
    },
    {
      "epoch": 0.5385365853658537,
      "step": 9936,
      "training_loss": 6.746735095977783
    },
    {
      "epoch": 0.538590785907859,
      "step": 9937,
      "training_loss": 6.906179904937744
    },
    {
      "epoch": 0.5386449864498645,
      "step": 9938,
      "training_loss": 6.316070079803467
    },
    {
      "epoch": 0.5386991869918699,
      "step": 9939,
      "training_loss": 6.48468017578125
    },
    {
      "epoch": 0.5387533875338754,
      "grad_norm": 30.486621856689453,
      "learning_rate": 1e-05,
      "loss": 6.6134,
      "step": 9940
    },
    {
      "epoch": 0.5387533875338754,
      "step": 9940,
      "training_loss": 5.462783336639404
    },
    {
      "epoch": 0.5388075880758808,
      "step": 9941,
      "training_loss": 6.332703113555908
    },
    {
      "epoch": 0.5388617886178861,
      "step": 9942,
      "training_loss": 5.517433166503906
    },
    {
      "epoch": 0.5389159891598916,
      "step": 9943,
      "training_loss": 7.656428813934326
    },
    {
      "epoch": 0.538970189701897,
      "grad_norm": 42.87921142578125,
      "learning_rate": 1e-05,
      "loss": 6.2423,
      "step": 9944
    },
    {
      "epoch": 0.538970189701897,
      "step": 9944,
      "training_loss": 9.0454683303833
    },
    {
      "epoch": 0.5390243902439025,
      "step": 9945,
      "training_loss": 3.714951753616333
    },
    {
      "epoch": 0.5390785907859078,
      "step": 9946,
      "training_loss": 6.784860610961914
    },
    {
      "epoch": 0.5391327913279133,
      "step": 9947,
      "training_loss": 6.6773810386657715
    },
    {
      "epoch": 0.5391869918699187,
      "grad_norm": 16.471837997436523,
      "learning_rate": 1e-05,
      "loss": 6.5557,
      "step": 9948
    },
    {
      "epoch": 0.5391869918699187,
      "step": 9948,
      "training_loss": 6.895878791809082
    },
    {
      "epoch": 0.5392411924119241,
      "step": 9949,
      "training_loss": 6.438842296600342
    },
    {
      "epoch": 0.5392953929539296,
      "step": 9950,
      "training_loss": 6.361146450042725
    },
    {
      "epoch": 0.5393495934959349,
      "step": 9951,
      "training_loss": 5.073774814605713
    },
    {
      "epoch": 0.5394037940379404,
      "grad_norm": 49.93552017211914,
      "learning_rate": 1e-05,
      "loss": 6.1924,
      "step": 9952
    },
    {
      "epoch": 0.5394037940379404,
      "step": 9952,
      "training_loss": 5.8960137367248535
    },
    {
      "epoch": 0.5394579945799458,
      "step": 9953,
      "training_loss": 6.577657222747803
    },
    {
      "epoch": 0.5395121951219513,
      "step": 9954,
      "training_loss": 7.736851215362549
    },
    {
      "epoch": 0.5395663956639566,
      "step": 9955,
      "training_loss": 7.173737525939941
    },
    {
      "epoch": 0.539620596205962,
      "grad_norm": 18.2580623626709,
      "learning_rate": 1e-05,
      "loss": 6.8461,
      "step": 9956
    },
    {
      "epoch": 0.539620596205962,
      "step": 9956,
      "training_loss": 6.9529314041137695
    },
    {
      "epoch": 0.5396747967479675,
      "step": 9957,
      "training_loss": 6.1090497970581055
    },
    {
      "epoch": 0.5397289972899729,
      "step": 9958,
      "training_loss": 6.670487880706787
    },
    {
      "epoch": 0.5397831978319784,
      "step": 9959,
      "training_loss": 8.414233207702637
    },
    {
      "epoch": 0.5398373983739837,
      "grad_norm": 22.02490234375,
      "learning_rate": 1e-05,
      "loss": 7.0367,
      "step": 9960
    },
    {
      "epoch": 0.5398373983739837,
      "step": 9960,
      "training_loss": 6.04464864730835
    },
    {
      "epoch": 0.5398915989159891,
      "step": 9961,
      "training_loss": 7.906925201416016
    },
    {
      "epoch": 0.5399457994579946,
      "step": 9962,
      "training_loss": 6.6887688636779785
    },
    {
      "epoch": 0.54,
      "step": 9963,
      "training_loss": 6.653687000274658
    },
    {
      "epoch": 0.5400542005420054,
      "grad_norm": 26.270418167114258,
      "learning_rate": 1e-05,
      "loss": 6.8235,
      "step": 9964
    },
    {
      "epoch": 0.5400542005420054,
      "step": 9964,
      "training_loss": 7.271444797515869
    },
    {
      "epoch": 0.5401084010840108,
      "step": 9965,
      "training_loss": 8.174966812133789
    },
    {
      "epoch": 0.5401626016260163,
      "step": 9966,
      "training_loss": 7.290604591369629
    },
    {
      "epoch": 0.5402168021680217,
      "step": 9967,
      "training_loss": 7.553721904754639
    },
    {
      "epoch": 0.5402710027100271,
      "grad_norm": 25.669944763183594,
      "learning_rate": 1e-05,
      "loss": 7.5727,
      "step": 9968
    },
    {
      "epoch": 0.5402710027100271,
      "step": 9968,
      "training_loss": 6.498220920562744
    },
    {
      "epoch": 0.5403252032520325,
      "step": 9969,
      "training_loss": 6.59105920791626
    },
    {
      "epoch": 0.5403794037940379,
      "step": 9970,
      "training_loss": 6.357550621032715
    },
    {
      "epoch": 0.5404336043360434,
      "step": 9971,
      "training_loss": 5.698193550109863
    },
    {
      "epoch": 0.5404878048780488,
      "grad_norm": 44.089324951171875,
      "learning_rate": 1e-05,
      "loss": 6.2863,
      "step": 9972
    },
    {
      "epoch": 0.5404878048780488,
      "step": 9972,
      "training_loss": 6.107305526733398
    },
    {
      "epoch": 0.5405420054200542,
      "step": 9973,
      "training_loss": 7.288496494293213
    },
    {
      "epoch": 0.5405962059620596,
      "step": 9974,
      "training_loss": 6.204261302947998
    },
    {
      "epoch": 0.540650406504065,
      "step": 9975,
      "training_loss": 7.425544261932373
    },
    {
      "epoch": 0.5407046070460705,
      "grad_norm": 26.760719299316406,
      "learning_rate": 1e-05,
      "loss": 6.7564,
      "step": 9976
    },
    {
      "epoch": 0.5407046070460705,
      "step": 9976,
      "training_loss": 6.640613079071045
    },
    {
      "epoch": 0.5407588075880759,
      "step": 9977,
      "training_loss": 7.055013656616211
    },
    {
      "epoch": 0.5408130081300813,
      "step": 9978,
      "training_loss": 4.063788414001465
    },
    {
      "epoch": 0.5408672086720867,
      "step": 9979,
      "training_loss": 5.873685836791992
    },
    {
      "epoch": 0.5409214092140922,
      "grad_norm": 34.32584762573242,
      "learning_rate": 1e-05,
      "loss": 5.9083,
      "step": 9980
    },
    {
      "epoch": 0.5409214092140922,
      "step": 9980,
      "training_loss": 7.459442138671875
    },
    {
      "epoch": 0.5409756097560976,
      "step": 9981,
      "training_loss": 8.187084197998047
    },
    {
      "epoch": 0.5410298102981029,
      "step": 9982,
      "training_loss": 2.989003896713257
    },
    {
      "epoch": 0.5410840108401084,
      "step": 9983,
      "training_loss": 7.333758354187012
    },
    {
      "epoch": 0.5411382113821138,
      "grad_norm": 19.299537658691406,
      "learning_rate": 1e-05,
      "loss": 6.4923,
      "step": 9984
    },
    {
      "epoch": 0.5411382113821138,
      "step": 9984,
      "training_loss": 7.040680408477783
    },
    {
      "epoch": 0.5411924119241193,
      "step": 9985,
      "training_loss": 5.895277976989746
    },
    {
      "epoch": 0.5412466124661247,
      "step": 9986,
      "training_loss": 6.79386568069458
    },
    {
      "epoch": 0.54130081300813,
      "step": 9987,
      "training_loss": 5.774025917053223
    },
    {
      "epoch": 0.5413550135501355,
      "grad_norm": 78.65231323242188,
      "learning_rate": 1e-05,
      "loss": 6.376,
      "step": 9988
    },
    {
      "epoch": 0.5413550135501355,
      "step": 9988,
      "training_loss": 7.22315788269043
    },
    {
      "epoch": 0.5414092140921409,
      "step": 9989,
      "training_loss": 7.270186901092529
    },
    {
      "epoch": 0.5414634146341464,
      "step": 9990,
      "training_loss": 6.539029121398926
    },
    {
      "epoch": 0.5415176151761517,
      "step": 9991,
      "training_loss": 6.486447811126709
    },
    {
      "epoch": 0.5415718157181572,
      "grad_norm": 30.79615020751953,
      "learning_rate": 1e-05,
      "loss": 6.8797,
      "step": 9992
    },
    {
      "epoch": 0.5415718157181572,
      "step": 9992,
      "training_loss": 6.067354202270508
    },
    {
      "epoch": 0.5416260162601626,
      "step": 9993,
      "training_loss": 5.780787467956543
    },
    {
      "epoch": 0.541680216802168,
      "step": 9994,
      "training_loss": 7.488144874572754
    },
    {
      "epoch": 0.5417344173441735,
      "step": 9995,
      "training_loss": 6.879703998565674
    },
    {
      "epoch": 0.5417886178861788,
      "grad_norm": 16.613862991333008,
      "learning_rate": 1e-05,
      "loss": 6.554,
      "step": 9996
    },
    {
      "epoch": 0.5417886178861788,
      "step": 9996,
      "training_loss": 6.685351371765137
    },
    {
      "epoch": 0.5418428184281843,
      "step": 9997,
      "training_loss": 7.517923831939697
    },
    {
      "epoch": 0.5418970189701897,
      "step": 9998,
      "training_loss": 5.4897780418396
    },
    {
      "epoch": 0.5419512195121952,
      "step": 9999,
      "training_loss": 4.835322380065918
    },
    {
      "epoch": 0.5420054200542005,
      "grad_norm": 40.81881332397461,
      "learning_rate": 1e-05,
      "loss": 6.1321,
      "step": 10000
    },
    {
      "epoch": 0.5420054200542005,
      "step": 10000,
      "training_loss": 4.0884108543396
    },
    {
      "epoch": 0.5420596205962059,
      "step": 10001,
      "training_loss": 6.750410556793213
    },
    {
      "epoch": 0.5421138211382114,
      "step": 10002,
      "training_loss": 7.639892101287842
    },
    {
      "epoch": 0.5421680216802168,
      "step": 10003,
      "training_loss": 5.268014907836914
    },
    {
      "epoch": 0.5422222222222223,
      "grad_norm": 33.051025390625,
      "learning_rate": 1e-05,
      "loss": 5.9367,
      "step": 10004
    },
    {
      "epoch": 0.5422222222222223,
      "step": 10004,
      "training_loss": 6.657711982727051
    },
    {
      "epoch": 0.5422764227642276,
      "step": 10005,
      "training_loss": 7.018234729766846
    },
    {
      "epoch": 0.542330623306233,
      "step": 10006,
      "training_loss": 7.420594692230225
    },
    {
      "epoch": 0.5423848238482385,
      "step": 10007,
      "training_loss": 7.7137064933776855
    },
    {
      "epoch": 0.5424390243902439,
      "grad_norm": 37.660064697265625,
      "learning_rate": 1e-05,
      "loss": 7.2026,
      "step": 10008
    },
    {
      "epoch": 0.5424390243902439,
      "step": 10008,
      "training_loss": 7.852132797241211
    },
    {
      "epoch": 0.5424932249322493,
      "step": 10009,
      "training_loss": 7.288126468658447
    },
    {
      "epoch": 0.5425474254742547,
      "step": 10010,
      "training_loss": 7.922852516174316
    },
    {
      "epoch": 0.5426016260162602,
      "step": 10011,
      "training_loss": 7.039330959320068
    },
    {
      "epoch": 0.5426558265582656,
      "grad_norm": 27.83021354675293,
      "learning_rate": 1e-05,
      "loss": 7.5256,
      "step": 10012
    },
    {
      "epoch": 0.5426558265582656,
      "step": 10012,
      "training_loss": 7.426022052764893
    },
    {
      "epoch": 0.542710027100271,
      "step": 10013,
      "training_loss": 7.427744388580322
    },
    {
      "epoch": 0.5427642276422764,
      "step": 10014,
      "training_loss": 7.829416275024414
    },
    {
      "epoch": 0.5428184281842818,
      "step": 10015,
      "training_loss": 6.795017719268799
    },
    {
      "epoch": 0.5428726287262873,
      "grad_norm": 22.158002853393555,
      "learning_rate": 1e-05,
      "loss": 7.3695,
      "step": 10016
    },
    {
      "epoch": 0.5428726287262873,
      "step": 10016,
      "training_loss": 7.046565532684326
    },
    {
      "epoch": 0.5429268292682927,
      "step": 10017,
      "training_loss": 7.026900291442871
    },
    {
      "epoch": 0.542981029810298,
      "step": 10018,
      "training_loss": 7.159027576446533
    },
    {
      "epoch": 0.5430352303523035,
      "step": 10019,
      "training_loss": 6.293629169464111
    },
    {
      "epoch": 0.5430894308943089,
      "grad_norm": 24.717090606689453,
      "learning_rate": 1e-05,
      "loss": 6.8815,
      "step": 10020
    },
    {
      "epoch": 0.5430894308943089,
      "step": 10020,
      "training_loss": 6.986725330352783
    },
    {
      "epoch": 0.5431436314363144,
      "step": 10021,
      "training_loss": 5.778526782989502
    },
    {
      "epoch": 0.5431978319783198,
      "step": 10022,
      "training_loss": 6.682635307312012
    },
    {
      "epoch": 0.5432520325203252,
      "step": 10023,
      "training_loss": 5.512078762054443
    },
    {
      "epoch": 0.5433062330623306,
      "grad_norm": 31.04959487915039,
      "learning_rate": 1e-05,
      "loss": 6.24,
      "step": 10024
    },
    {
      "epoch": 0.5433062330623306,
      "step": 10024,
      "training_loss": 8.391281127929688
    },
    {
      "epoch": 0.5433604336043361,
      "step": 10025,
      "training_loss": 6.921921253204346
    },
    {
      "epoch": 0.5434146341463415,
      "step": 10026,
      "training_loss": 4.995527267456055
    },
    {
      "epoch": 0.5434688346883468,
      "step": 10027,
      "training_loss": 7.648502349853516
    },
    {
      "epoch": 0.5435230352303523,
      "grad_norm": 33.54485321044922,
      "learning_rate": 1e-05,
      "loss": 6.9893,
      "step": 10028
    },
    {
      "epoch": 0.5435230352303523,
      "step": 10028,
      "training_loss": 7.266848087310791
    },
    {
      "epoch": 0.5435772357723577,
      "step": 10029,
      "training_loss": 7.940197944641113
    },
    {
      "epoch": 0.5436314363143632,
      "step": 10030,
      "training_loss": 6.533794403076172
    },
    {
      "epoch": 0.5436856368563686,
      "step": 10031,
      "training_loss": 7.225090026855469
    },
    {
      "epoch": 0.543739837398374,
      "grad_norm": 31.14344596862793,
      "learning_rate": 1e-05,
      "loss": 7.2415,
      "step": 10032
    },
    {
      "epoch": 0.543739837398374,
      "step": 10032,
      "training_loss": 6.463386058807373
    },
    {
      "epoch": 0.5437940379403794,
      "step": 10033,
      "training_loss": 6.60625696182251
    },
    {
      "epoch": 0.5438482384823848,
      "step": 10034,
      "training_loss": 7.070001125335693
    },
    {
      "epoch": 0.5439024390243903,
      "step": 10035,
      "training_loss": 6.468708038330078
    },
    {
      "epoch": 0.5439566395663956,
      "grad_norm": 28.165328979492188,
      "learning_rate": 1e-05,
      "loss": 6.6521,
      "step": 10036
    },
    {
      "epoch": 0.5439566395663956,
      "step": 10036,
      "training_loss": 6.722848892211914
    },
    {
      "epoch": 0.5440108401084011,
      "step": 10037,
      "training_loss": 7.615884304046631
    },
    {
      "epoch": 0.5440650406504065,
      "step": 10038,
      "training_loss": 4.189123630523682
    },
    {
      "epoch": 0.544119241192412,
      "step": 10039,
      "training_loss": 7.20990514755249
    },
    {
      "epoch": 0.5441734417344174,
      "grad_norm": 26.16131019592285,
      "learning_rate": 1e-05,
      "loss": 6.4344,
      "step": 10040
    },
    {
      "epoch": 0.5441734417344174,
      "step": 10040,
      "training_loss": 7.9553046226501465
    },
    {
      "epoch": 0.5442276422764227,
      "step": 10041,
      "training_loss": 7.361443996429443
    },
    {
      "epoch": 0.5442818428184282,
      "step": 10042,
      "training_loss": 7.553104400634766
    },
    {
      "epoch": 0.5443360433604336,
      "step": 10043,
      "training_loss": 6.917912006378174
    },
    {
      "epoch": 0.5443902439024391,
      "grad_norm": 24.955175399780273,
      "learning_rate": 1e-05,
      "loss": 7.4469,
      "step": 10044
    },
    {
      "epoch": 0.5443902439024391,
      "step": 10044,
      "training_loss": 6.581033229827881
    },
    {
      "epoch": 0.5444444444444444,
      "step": 10045,
      "training_loss": 5.750679016113281
    },
    {
      "epoch": 0.5444986449864498,
      "step": 10046,
      "training_loss": 6.733935356140137
    },
    {
      "epoch": 0.5445528455284553,
      "step": 10047,
      "training_loss": 7.197137832641602
    },
    {
      "epoch": 0.5446070460704607,
      "grad_norm": 22.902759552001953,
      "learning_rate": 1e-05,
      "loss": 6.5657,
      "step": 10048
    },
    {
      "epoch": 0.5446070460704607,
      "step": 10048,
      "training_loss": 4.994439601898193
    },
    {
      "epoch": 0.5446612466124662,
      "step": 10049,
      "training_loss": 7.041933536529541
    },
    {
      "epoch": 0.5447154471544715,
      "step": 10050,
      "training_loss": 6.725452899932861
    },
    {
      "epoch": 0.544769647696477,
      "step": 10051,
      "training_loss": 6.142440319061279
    },
    {
      "epoch": 0.5448238482384824,
      "grad_norm": 23.10369300842285,
      "learning_rate": 1e-05,
      "loss": 6.2261,
      "step": 10052
    },
    {
      "epoch": 0.5448238482384824,
      "step": 10052,
      "training_loss": 6.819488048553467
    },
    {
      "epoch": 0.5448780487804878,
      "step": 10053,
      "training_loss": 7.86004114151001
    },
    {
      "epoch": 0.5449322493224932,
      "step": 10054,
      "training_loss": 6.900211334228516
    },
    {
      "epoch": 0.5449864498644986,
      "step": 10055,
      "training_loss": 7.859254837036133
    },
    {
      "epoch": 0.5450406504065041,
      "grad_norm": 29.47810935974121,
      "learning_rate": 1e-05,
      "loss": 7.3597,
      "step": 10056
    },
    {
      "epoch": 0.5450406504065041,
      "step": 10056,
      "training_loss": 4.89538049697876
    },
    {
      "epoch": 0.5450948509485095,
      "step": 10057,
      "training_loss": 6.578735828399658
    },
    {
      "epoch": 0.545149051490515,
      "step": 10058,
      "training_loss": 5.637573719024658
    },
    {
      "epoch": 0.5452032520325203,
      "step": 10059,
      "training_loss": 5.635173320770264
    },
    {
      "epoch": 0.5452574525745257,
      "grad_norm": 29.29116439819336,
      "learning_rate": 1e-05,
      "loss": 5.6867,
      "step": 10060
    },
    {
      "epoch": 0.5452574525745257,
      "step": 10060,
      "training_loss": 4.795928955078125
    },
    {
      "epoch": 0.5453116531165312,
      "step": 10061,
      "training_loss": 7.154339790344238
    },
    {
      "epoch": 0.5453658536585366,
      "step": 10062,
      "training_loss": 6.635884761810303
    },
    {
      "epoch": 0.545420054200542,
      "step": 10063,
      "training_loss": 7.31048059463501
    },
    {
      "epoch": 0.5454742547425474,
      "grad_norm": 25.97119903564453,
      "learning_rate": 1e-05,
      "loss": 6.4742,
      "step": 10064
    },
    {
      "epoch": 0.5454742547425474,
      "step": 10064,
      "training_loss": 7.184084415435791
    },
    {
      "epoch": 0.5455284552845528,
      "step": 10065,
      "training_loss": 4.914015769958496
    },
    {
      "epoch": 0.5455826558265583,
      "step": 10066,
      "training_loss": 6.093289852142334
    },
    {
      "epoch": 0.5456368563685637,
      "step": 10067,
      "training_loss": 7.331298828125
    },
    {
      "epoch": 0.5456910569105691,
      "grad_norm": 50.051719665527344,
      "learning_rate": 1e-05,
      "loss": 6.3807,
      "step": 10068
    },
    {
      "epoch": 0.5456910569105691,
      "step": 10068,
      "training_loss": 6.358314514160156
    },
    {
      "epoch": 0.5457452574525745,
      "step": 10069,
      "training_loss": 5.825453281402588
    },
    {
      "epoch": 0.54579945799458,
      "step": 10070,
      "training_loss": 7.387664318084717
    },
    {
      "epoch": 0.5458536585365854,
      "step": 10071,
      "training_loss": 6.281922340393066
    },
    {
      "epoch": 0.5459078590785907,
      "grad_norm": 30.047426223754883,
      "learning_rate": 1e-05,
      "loss": 6.4633,
      "step": 10072
    },
    {
      "epoch": 0.5459078590785907,
      "step": 10072,
      "training_loss": 5.997745990753174
    },
    {
      "epoch": 0.5459620596205962,
      "step": 10073,
      "training_loss": 6.074693202972412
    },
    {
      "epoch": 0.5460162601626016,
      "step": 10074,
      "training_loss": 3.476099967956543
    },
    {
      "epoch": 0.5460704607046071,
      "step": 10075,
      "training_loss": 6.434319019317627
    },
    {
      "epoch": 0.5461246612466125,
      "grad_norm": 36.030845642089844,
      "learning_rate": 1e-05,
      "loss": 5.4957,
      "step": 10076
    },
    {
      "epoch": 0.5461246612466125,
      "step": 10076,
      "training_loss": 8.21731948852539
    },
    {
      "epoch": 0.5461788617886179,
      "step": 10077,
      "training_loss": 6.536273956298828
    },
    {
      "epoch": 0.5462330623306233,
      "step": 10078,
      "training_loss": 6.892163276672363
    },
    {
      "epoch": 0.5462872628726287,
      "step": 10079,
      "training_loss": 6.673051834106445
    },
    {
      "epoch": 0.5463414634146342,
      "grad_norm": 20.899208068847656,
      "learning_rate": 1e-05,
      "loss": 7.0797,
      "step": 10080
    },
    {
      "epoch": 0.5463414634146342,
      "step": 10080,
      "training_loss": 8.329710960388184
    },
    {
      "epoch": 0.5463956639566395,
      "step": 10081,
      "training_loss": 4.647604465484619
    },
    {
      "epoch": 0.546449864498645,
      "step": 10082,
      "training_loss": 6.620713233947754
    },
    {
      "epoch": 0.5465040650406504,
      "step": 10083,
      "training_loss": 5.733615398406982
    },
    {
      "epoch": 0.5465582655826559,
      "grad_norm": 23.439481735229492,
      "learning_rate": 1e-05,
      "loss": 6.3329,
      "step": 10084
    },
    {
      "epoch": 0.5465582655826559,
      "step": 10084,
      "training_loss": 5.327526569366455
    },
    {
      "epoch": 0.5466124661246613,
      "step": 10085,
      "training_loss": 6.6201171875
    },
    {
      "epoch": 0.5466666666666666,
      "step": 10086,
      "training_loss": 7.086401462554932
    },
    {
      "epoch": 0.5467208672086721,
      "step": 10087,
      "training_loss": 3.394253969192505
    },
    {
      "epoch": 0.5467750677506775,
      "grad_norm": 33.04712677001953,
      "learning_rate": 1e-05,
      "loss": 5.6071,
      "step": 10088
    },
    {
      "epoch": 0.5467750677506775,
      "step": 10088,
      "training_loss": 5.180321216583252
    },
    {
      "epoch": 0.546829268292683,
      "step": 10089,
      "training_loss": 6.785147666931152
    },
    {
      "epoch": 0.5468834688346883,
      "step": 10090,
      "training_loss": 7.585442543029785
    },
    {
      "epoch": 0.5469376693766937,
      "step": 10091,
      "training_loss": 6.096438407897949
    },
    {
      "epoch": 0.5469918699186992,
      "grad_norm": 31.262996673583984,
      "learning_rate": 1e-05,
      "loss": 6.4118,
      "step": 10092
    },
    {
      "epoch": 0.5469918699186992,
      "step": 10092,
      "training_loss": 6.926726818084717
    },
    {
      "epoch": 0.5470460704607046,
      "step": 10093,
      "training_loss": 6.76611328125
    },
    {
      "epoch": 0.5471002710027101,
      "step": 10094,
      "training_loss": 6.626926898956299
    },
    {
      "epoch": 0.5471544715447154,
      "step": 10095,
      "training_loss": 6.363091945648193
    },
    {
      "epoch": 0.5472086720867209,
      "grad_norm": 30.304609298706055,
      "learning_rate": 1e-05,
      "loss": 6.6707,
      "step": 10096
    },
    {
      "epoch": 0.5472086720867209,
      "step": 10096,
      "training_loss": 6.668227195739746
    },
    {
      "epoch": 0.5472628726287263,
      "step": 10097,
      "training_loss": 6.8100266456604
    },
    {
      "epoch": 0.5473170731707317,
      "step": 10098,
      "training_loss": 6.807910919189453
    },
    {
      "epoch": 0.5473712737127371,
      "step": 10099,
      "training_loss": 7.265864372253418
    },
    {
      "epoch": 0.5474254742547425,
      "grad_norm": 33.08173370361328,
      "learning_rate": 1e-05,
      "loss": 6.888,
      "step": 10100
    },
    {
      "epoch": 0.5474254742547425,
      "step": 10100,
      "training_loss": 7.020654201507568
    },
    {
      "epoch": 0.547479674796748,
      "step": 10101,
      "training_loss": 6.44590950012207
    },
    {
      "epoch": 0.5475338753387534,
      "step": 10102,
      "training_loss": 7.288801193237305
    },
    {
      "epoch": 0.5475880758807589,
      "step": 10103,
      "training_loss": 3.7571403980255127
    },
    {
      "epoch": 0.5476422764227642,
      "grad_norm": 40.87198257446289,
      "learning_rate": 1e-05,
      "loss": 6.1281,
      "step": 10104
    },
    {
      "epoch": 0.5476422764227642,
      "step": 10104,
      "training_loss": 7.07740592956543
    },
    {
      "epoch": 0.5476964769647696,
      "step": 10105,
      "training_loss": 6.812315464019775
    },
    {
      "epoch": 0.5477506775067751,
      "step": 10106,
      "training_loss": 6.57818603515625
    },
    {
      "epoch": 0.5478048780487805,
      "step": 10107,
      "training_loss": 6.513089656829834
    },
    {
      "epoch": 0.5478590785907859,
      "grad_norm": 72.26758575439453,
      "learning_rate": 1e-05,
      "loss": 6.7452,
      "step": 10108
    },
    {
      "epoch": 0.5478590785907859,
      "step": 10108,
      "training_loss": 6.9896321296691895
    },
    {
      "epoch": 0.5479132791327913,
      "step": 10109,
      "training_loss": 7.000930309295654
    },
    {
      "epoch": 0.5479674796747968,
      "step": 10110,
      "training_loss": 5.165087699890137
    },
    {
      "epoch": 0.5480216802168022,
      "step": 10111,
      "training_loss": 5.7934417724609375
    },
    {
      "epoch": 0.5480758807588076,
      "grad_norm": 38.297157287597656,
      "learning_rate": 1e-05,
      "loss": 6.2373,
      "step": 10112
    },
    {
      "epoch": 0.5480758807588076,
      "step": 10112,
      "training_loss": 5.819374084472656
    },
    {
      "epoch": 0.548130081300813,
      "step": 10113,
      "training_loss": 7.454476356506348
    },
    {
      "epoch": 0.5481842818428184,
      "step": 10114,
      "training_loss": 7.066938877105713
    },
    {
      "epoch": 0.5482384823848239,
      "step": 10115,
      "training_loss": 6.4179816246032715
    },
    {
      "epoch": 0.5482926829268293,
      "grad_norm": 42.08332443237305,
      "learning_rate": 1e-05,
      "loss": 6.6897,
      "step": 10116
    },
    {
      "epoch": 0.5482926829268293,
      "step": 10116,
      "training_loss": 7.091528415679932
    },
    {
      "epoch": 0.5483468834688346,
      "step": 10117,
      "training_loss": 6.917365550994873
    },
    {
      "epoch": 0.5484010840108401,
      "step": 10118,
      "training_loss": 6.854114532470703
    },
    {
      "epoch": 0.5484552845528455,
      "step": 10119,
      "training_loss": 7.7164764404296875
    },
    {
      "epoch": 0.548509485094851,
      "grad_norm": 52.095115661621094,
      "learning_rate": 1e-05,
      "loss": 7.1449,
      "step": 10120
    },
    {
      "epoch": 0.548509485094851,
      "step": 10120,
      "training_loss": 7.402437686920166
    },
    {
      "epoch": 0.5485636856368564,
      "step": 10121,
      "training_loss": 6.829827308654785
    },
    {
      "epoch": 0.5486178861788618,
      "step": 10122,
      "training_loss": 7.291697025299072
    },
    {
      "epoch": 0.5486720867208672,
      "step": 10123,
      "training_loss": 7.239345073699951
    },
    {
      "epoch": 0.5487262872628726,
      "grad_norm": 22.859304428100586,
      "learning_rate": 1e-05,
      "loss": 7.1908,
      "step": 10124
    },
    {
      "epoch": 0.5487262872628726,
      "step": 10124,
      "training_loss": 5.323678016662598
    },
    {
      "epoch": 0.5487804878048781,
      "step": 10125,
      "training_loss": 7.209267616271973
    },
    {
      "epoch": 0.5488346883468834,
      "step": 10126,
      "training_loss": 7.533291339874268
    },
    {
      "epoch": 0.5488888888888889,
      "step": 10127,
      "training_loss": 8.132050514221191
    },
    {
      "epoch": 0.5489430894308943,
      "grad_norm": 25.332365036010742,
      "learning_rate": 1e-05,
      "loss": 7.0496,
      "step": 10128
    },
    {
      "epoch": 0.5489430894308943,
      "step": 10128,
      "training_loss": 4.716519355773926
    },
    {
      "epoch": 0.5489972899728998,
      "step": 10129,
      "training_loss": 6.123331069946289
    },
    {
      "epoch": 0.5490514905149051,
      "step": 10130,
      "training_loss": 3.5836381912231445
    },
    {
      "epoch": 0.5491056910569105,
      "step": 10131,
      "training_loss": 7.803130149841309
    },
    {
      "epoch": 0.549159891598916,
      "grad_norm": 27.697948455810547,
      "learning_rate": 1e-05,
      "loss": 5.5567,
      "step": 10132
    },
    {
      "epoch": 0.549159891598916,
      "step": 10132,
      "training_loss": 2.977940559387207
    },
    {
      "epoch": 0.5492140921409214,
      "step": 10133,
      "training_loss": 6.47655725479126
    },
    {
      "epoch": 0.5492682926829269,
      "step": 10134,
      "training_loss": 7.011092185974121
    },
    {
      "epoch": 0.5493224932249322,
      "step": 10135,
      "training_loss": 7.331273078918457
    },
    {
      "epoch": 0.5493766937669377,
      "grad_norm": 20.23733139038086,
      "learning_rate": 1e-05,
      "loss": 5.9492,
      "step": 10136
    },
    {
      "epoch": 0.5493766937669377,
      "step": 10136,
      "training_loss": 5.708622455596924
    },
    {
      "epoch": 0.5494308943089431,
      "step": 10137,
      "training_loss": 6.706066131591797
    },
    {
      "epoch": 0.5494850948509485,
      "step": 10138,
      "training_loss": 5.452096939086914
    },
    {
      "epoch": 0.5495392953929539,
      "step": 10139,
      "training_loss": 3.6669535636901855
    },
    {
      "epoch": 0.5495934959349593,
      "grad_norm": 33.21770095825195,
      "learning_rate": 1e-05,
      "loss": 5.3834,
      "step": 10140
    },
    {
      "epoch": 0.5495934959349593,
      "step": 10140,
      "training_loss": 6.714145183563232
    },
    {
      "epoch": 0.5496476964769648,
      "step": 10141,
      "training_loss": 5.791875839233398
    },
    {
      "epoch": 0.5497018970189702,
      "step": 10142,
      "training_loss": 5.705471515655518
    },
    {
      "epoch": 0.5497560975609757,
      "step": 10143,
      "training_loss": 8.106451988220215
    },
    {
      "epoch": 0.549810298102981,
      "grad_norm": 24.004688262939453,
      "learning_rate": 1e-05,
      "loss": 6.5795,
      "step": 10144
    },
    {
      "epoch": 0.549810298102981,
      "step": 10144,
      "training_loss": 6.899320602416992
    },
    {
      "epoch": 0.5498644986449864,
      "step": 10145,
      "training_loss": 6.917535305023193
    },
    {
      "epoch": 0.5499186991869919,
      "step": 10146,
      "training_loss": 7.3010969161987305
    },
    {
      "epoch": 0.5499728997289973,
      "step": 10147,
      "training_loss": 7.187376499176025
    },
    {
      "epoch": 0.5500271002710027,
      "grad_norm": 36.7945671081543,
      "learning_rate": 1e-05,
      "loss": 7.0763,
      "step": 10148
    },
    {
      "epoch": 0.5500271002710027,
      "step": 10148,
      "training_loss": 6.143848419189453
    },
    {
      "epoch": 0.5500813008130081,
      "step": 10149,
      "training_loss": 6.353063583374023
    },
    {
      "epoch": 0.5501355013550135,
      "step": 10150,
      "training_loss": 6.639336585998535
    },
    {
      "epoch": 0.550189701897019,
      "step": 10151,
      "training_loss": 7.526693344116211
    },
    {
      "epoch": 0.5502439024390244,
      "grad_norm": 30.859962463378906,
      "learning_rate": 1e-05,
      "loss": 6.6657,
      "step": 10152
    },
    {
      "epoch": 0.5502439024390244,
      "step": 10152,
      "training_loss": 7.201488494873047
    },
    {
      "epoch": 0.5502981029810298,
      "step": 10153,
      "training_loss": 6.628012657165527
    },
    {
      "epoch": 0.5503523035230352,
      "step": 10154,
      "training_loss": 5.558785438537598
    },
    {
      "epoch": 0.5504065040650407,
      "step": 10155,
      "training_loss": 6.957876682281494
    },
    {
      "epoch": 0.5504607046070461,
      "grad_norm": 31.963502883911133,
      "learning_rate": 1e-05,
      "loss": 6.5865,
      "step": 10156
    },
    {
      "epoch": 0.5504607046070461,
      "step": 10156,
      "training_loss": 5.725583076477051
    },
    {
      "epoch": 0.5505149051490514,
      "step": 10157,
      "training_loss": 6.074495792388916
    },
    {
      "epoch": 0.5505691056910569,
      "step": 10158,
      "training_loss": 7.196175575256348
    },
    {
      "epoch": 0.5506233062330623,
      "step": 10159,
      "training_loss": 7.350490570068359
    },
    {
      "epoch": 0.5506775067750678,
      "grad_norm": 48.960201263427734,
      "learning_rate": 1e-05,
      "loss": 6.5867,
      "step": 10160
    },
    {
      "epoch": 0.5506775067750678,
      "step": 10160,
      "training_loss": 6.710953712463379
    },
    {
      "epoch": 0.5507317073170732,
      "step": 10161,
      "training_loss": 8.08621597290039
    },
    {
      "epoch": 0.5507859078590785,
      "step": 10162,
      "training_loss": 6.506458759307861
    },
    {
      "epoch": 0.550840108401084,
      "step": 10163,
      "training_loss": 7.005580425262451
    },
    {
      "epoch": 0.5508943089430894,
      "grad_norm": 42.82406997680664,
      "learning_rate": 1e-05,
      "loss": 7.0773,
      "step": 10164
    },
    {
      "epoch": 0.5508943089430894,
      "step": 10164,
      "training_loss": 6.3521294593811035
    },
    {
      "epoch": 0.5509485094850949,
      "step": 10165,
      "training_loss": 6.678192138671875
    },
    {
      "epoch": 0.5510027100271002,
      "step": 10166,
      "training_loss": 3.2141847610473633
    },
    {
      "epoch": 0.5510569105691057,
      "step": 10167,
      "training_loss": 6.859452247619629
    },
    {
      "epoch": 0.5511111111111111,
      "grad_norm": 18.35820198059082,
      "learning_rate": 1e-05,
      "loss": 5.776,
      "step": 10168
    },
    {
      "epoch": 0.5511111111111111,
      "step": 10168,
      "training_loss": 7.446741104125977
    },
    {
      "epoch": 0.5511653116531166,
      "step": 10169,
      "training_loss": 7.28303337097168
    },
    {
      "epoch": 0.551219512195122,
      "step": 10170,
      "training_loss": 7.552099227905273
    },
    {
      "epoch": 0.5512737127371273,
      "step": 10171,
      "training_loss": 6.942210674285889
    },
    {
      "epoch": 0.5513279132791328,
      "grad_norm": 22.66388702392578,
      "learning_rate": 1e-05,
      "loss": 7.306,
      "step": 10172
    },
    {
      "epoch": 0.5513279132791328,
      "step": 10172,
      "training_loss": 6.207724094390869
    },
    {
      "epoch": 0.5513821138211382,
      "step": 10173,
      "training_loss": 7.081358909606934
    },
    {
      "epoch": 0.5514363143631437,
      "step": 10174,
      "training_loss": 7.242644309997559
    },
    {
      "epoch": 0.551490514905149,
      "step": 10175,
      "training_loss": 6.4917802810668945
    },
    {
      "epoch": 0.5515447154471544,
      "grad_norm": 24.03855323791504,
      "learning_rate": 1e-05,
      "loss": 6.7559,
      "step": 10176
    },
    {
      "epoch": 0.5515447154471544,
      "step": 10176,
      "training_loss": 6.4303717613220215
    },
    {
      "epoch": 0.5515989159891599,
      "step": 10177,
      "training_loss": 3.129624366760254
    },
    {
      "epoch": 0.5516531165311653,
      "step": 10178,
      "training_loss": 7.365009307861328
    },
    {
      "epoch": 0.5517073170731708,
      "step": 10179,
      "training_loss": 5.969769477844238
    },
    {
      "epoch": 0.5517615176151761,
      "grad_norm": 27.74235725402832,
      "learning_rate": 1e-05,
      "loss": 5.7237,
      "step": 10180
    },
    {
      "epoch": 0.5517615176151761,
      "step": 10180,
      "training_loss": 6.267726421356201
    },
    {
      "epoch": 0.5518157181571816,
      "step": 10181,
      "training_loss": 6.862852573394775
    },
    {
      "epoch": 0.551869918699187,
      "step": 10182,
      "training_loss": 5.598127365112305
    },
    {
      "epoch": 0.5519241192411924,
      "step": 10183,
      "training_loss": 5.738584041595459
    },
    {
      "epoch": 0.5519783197831978,
      "grad_norm": 18.586952209472656,
      "learning_rate": 1e-05,
      "loss": 6.1168,
      "step": 10184
    },
    {
      "epoch": 0.5519783197831978,
      "step": 10184,
      "training_loss": 7.6404290199279785
    },
    {
      "epoch": 0.5520325203252032,
      "step": 10185,
      "training_loss": 7.730036735534668
    },
    {
      "epoch": 0.5520867208672087,
      "step": 10186,
      "training_loss": 6.995104789733887
    },
    {
      "epoch": 0.5521409214092141,
      "step": 10187,
      "training_loss": 5.86696195602417
    },
    {
      "epoch": 0.5521951219512196,
      "grad_norm": 36.53228759765625,
      "learning_rate": 1e-05,
      "loss": 7.0581,
      "step": 10188
    },
    {
      "epoch": 0.5521951219512196,
      "step": 10188,
      "training_loss": 6.770162105560303
    },
    {
      "epoch": 0.5522493224932249,
      "step": 10189,
      "training_loss": 5.670939922332764
    },
    {
      "epoch": 0.5523035230352303,
      "step": 10190,
      "training_loss": 7.17880392074585
    },
    {
      "epoch": 0.5523577235772358,
      "step": 10191,
      "training_loss": 8.075448036193848
    },
    {
      "epoch": 0.5524119241192412,
      "grad_norm": 30.022159576416016,
      "learning_rate": 1e-05,
      "loss": 6.9238,
      "step": 10192
    },
    {
      "epoch": 0.5524119241192412,
      "step": 10192,
      "training_loss": 7.173426628112793
    },
    {
      "epoch": 0.5524661246612466,
      "step": 10193,
      "training_loss": 7.467563629150391
    },
    {
      "epoch": 0.552520325203252,
      "step": 10194,
      "training_loss": 7.242402076721191
    },
    {
      "epoch": 0.5525745257452574,
      "step": 10195,
      "training_loss": 7.853885173797607
    },
    {
      "epoch": 0.5526287262872629,
      "grad_norm": 35.13573455810547,
      "learning_rate": 1e-05,
      "loss": 7.4343,
      "step": 10196
    },
    {
      "epoch": 0.5526287262872629,
      "step": 10196,
      "training_loss": 4.591731071472168
    },
    {
      "epoch": 0.5526829268292683,
      "step": 10197,
      "training_loss": 4.850244045257568
    },
    {
      "epoch": 0.5527371273712737,
      "step": 10198,
      "training_loss": 6.763851642608643
    },
    {
      "epoch": 0.5527913279132791,
      "step": 10199,
      "training_loss": 7.114096641540527
    },
    {
      "epoch": 0.5528455284552846,
      "grad_norm": 35.83285140991211,
      "learning_rate": 1e-05,
      "loss": 5.83,
      "step": 10200
    },
    {
      "epoch": 0.5528455284552846,
      "step": 10200,
      "training_loss": 5.474304676055908
    },
    {
      "epoch": 0.55289972899729,
      "step": 10201,
      "training_loss": 7.591050148010254
    },
    {
      "epoch": 0.5529539295392953,
      "step": 10202,
      "training_loss": 5.960597991943359
    },
    {
      "epoch": 0.5530081300813008,
      "step": 10203,
      "training_loss": 8.88579273223877
    },
    {
      "epoch": 0.5530623306233062,
      "grad_norm": 60.52781677246094,
      "learning_rate": 1e-05,
      "loss": 6.9779,
      "step": 10204
    },
    {
      "epoch": 0.5530623306233062,
      "step": 10204,
      "training_loss": 6.438190460205078
    },
    {
      "epoch": 0.5531165311653117,
      "step": 10205,
      "training_loss": 5.049415111541748
    },
    {
      "epoch": 0.5531707317073171,
      "step": 10206,
      "training_loss": 7.946346759796143
    },
    {
      "epoch": 0.5532249322493225,
      "step": 10207,
      "training_loss": 6.730273246765137
    },
    {
      "epoch": 0.5532791327913279,
      "grad_norm": 20.127161026000977,
      "learning_rate": 1e-05,
      "loss": 6.5411,
      "step": 10208
    },
    {
      "epoch": 0.5532791327913279,
      "step": 10208,
      "training_loss": 6.131056785583496
    },
    {
      "epoch": 0.5533333333333333,
      "step": 10209,
      "training_loss": 7.221526622772217
    },
    {
      "epoch": 0.5533875338753388,
      "step": 10210,
      "training_loss": 6.1739301681518555
    },
    {
      "epoch": 0.5534417344173441,
      "step": 10211,
      "training_loss": 6.847011089324951
    },
    {
      "epoch": 0.5534959349593496,
      "grad_norm": 38.90782165527344,
      "learning_rate": 1e-05,
      "loss": 6.5934,
      "step": 10212
    },
    {
      "epoch": 0.5534959349593496,
      "step": 10212,
      "training_loss": 7.951044082641602
    },
    {
      "epoch": 0.553550135501355,
      "step": 10213,
      "training_loss": 7.127629280090332
    },
    {
      "epoch": 0.5536043360433605,
      "step": 10214,
      "training_loss": 6.188093185424805
    },
    {
      "epoch": 0.5536585365853659,
      "step": 10215,
      "training_loss": 7.174495697021484
    },
    {
      "epoch": 0.5537127371273712,
      "grad_norm": 26.495628356933594,
      "learning_rate": 1e-05,
      "loss": 7.1103,
      "step": 10216
    },
    {
      "epoch": 0.5537127371273712,
      "step": 10216,
      "training_loss": 6.144722938537598
    },
    {
      "epoch": 0.5537669376693767,
      "step": 10217,
      "training_loss": 5.825077056884766
    },
    {
      "epoch": 0.5538211382113821,
      "step": 10218,
      "training_loss": 7.872002601623535
    },
    {
      "epoch": 0.5538753387533876,
      "step": 10219,
      "training_loss": 7.355203628540039
    },
    {
      "epoch": 0.5539295392953929,
      "grad_norm": 22.19012451171875,
      "learning_rate": 1e-05,
      "loss": 6.7993,
      "step": 10220
    },
    {
      "epoch": 0.5539295392953929,
      "step": 10220,
      "training_loss": 6.8510236740112305
    },
    {
      "epoch": 0.5539837398373983,
      "step": 10221,
      "training_loss": 7.213933944702148
    },
    {
      "epoch": 0.5540379403794038,
      "step": 10222,
      "training_loss": 5.649324417114258
    },
    {
      "epoch": 0.5540921409214092,
      "step": 10223,
      "training_loss": 6.15371036529541
    },
    {
      "epoch": 0.5541463414634147,
      "grad_norm": 25.639068603515625,
      "learning_rate": 1e-05,
      "loss": 6.467,
      "step": 10224
    },
    {
      "epoch": 0.5541463414634147,
      "step": 10224,
      "training_loss": 8.149883270263672
    },
    {
      "epoch": 0.55420054200542,
      "step": 10225,
      "training_loss": 6.974882125854492
    },
    {
      "epoch": 0.5542547425474255,
      "step": 10226,
      "training_loss": 6.7808756828308105
    },
    {
      "epoch": 0.5543089430894309,
      "step": 10227,
      "training_loss": 6.1211371421813965
    },
    {
      "epoch": 0.5543631436314364,
      "grad_norm": 30.42864227294922,
      "learning_rate": 1e-05,
      "loss": 7.0067,
      "step": 10228
    },
    {
      "epoch": 0.5543631436314364,
      "step": 10228,
      "training_loss": 7.667081356048584
    },
    {
      "epoch": 0.5544173441734417,
      "step": 10229,
      "training_loss": 7.3695387840271
    },
    {
      "epoch": 0.5544715447154471,
      "step": 10230,
      "training_loss": 7.2501959800720215
    },
    {
      "epoch": 0.5545257452574526,
      "step": 10231,
      "training_loss": 6.532341957092285
    },
    {
      "epoch": 0.554579945799458,
      "grad_norm": 38.46168899536133,
      "learning_rate": 1e-05,
      "loss": 7.2048,
      "step": 10232
    },
    {
      "epoch": 0.554579945799458,
      "step": 10232,
      "training_loss": 5.366608142852783
    },
    {
      "epoch": 0.5546341463414635,
      "step": 10233,
      "training_loss": 6.731403827667236
    },
    {
      "epoch": 0.5546883468834688,
      "step": 10234,
      "training_loss": 5.537346839904785
    },
    {
      "epoch": 0.5547425474254742,
      "step": 10235,
      "training_loss": 7.446096897125244
    },
    {
      "epoch": 0.5547967479674797,
      "grad_norm": 24.86814308166504,
      "learning_rate": 1e-05,
      "loss": 6.2704,
      "step": 10236
    },
    {
      "epoch": 0.5547967479674797,
      "step": 10236,
      "training_loss": 7.42139196395874
    },
    {
      "epoch": 0.5548509485094851,
      "step": 10237,
      "training_loss": 6.238451957702637
    },
    {
      "epoch": 0.5549051490514905,
      "step": 10238,
      "training_loss": 5.147407531738281
    },
    {
      "epoch": 0.5549593495934959,
      "step": 10239,
      "training_loss": 6.117430210113525
    },
    {
      "epoch": 0.5550135501355014,
      "grad_norm": 25.291309356689453,
      "learning_rate": 1e-05,
      "loss": 6.2312,
      "step": 10240
    },
    {
      "epoch": 0.5550135501355014,
      "step": 10240,
      "training_loss": 6.143322944641113
    },
    {
      "epoch": 0.5550677506775068,
      "step": 10241,
      "training_loss": 6.7071943283081055
    },
    {
      "epoch": 0.5551219512195122,
      "step": 10242,
      "training_loss": 6.950464248657227
    },
    {
      "epoch": 0.5551761517615176,
      "step": 10243,
      "training_loss": 6.092950344085693
    },
    {
      "epoch": 0.555230352303523,
      "grad_norm": 27.62071418762207,
      "learning_rate": 1e-05,
      "loss": 6.4735,
      "step": 10244
    },
    {
      "epoch": 0.555230352303523,
      "step": 10244,
      "training_loss": 7.2164201736450195
    },
    {
      "epoch": 0.5552845528455285,
      "step": 10245,
      "training_loss": 7.302879333496094
    },
    {
      "epoch": 0.5553387533875339,
      "step": 10246,
      "training_loss": 7.927268028259277
    },
    {
      "epoch": 0.5553929539295392,
      "step": 10247,
      "training_loss": 5.859294891357422
    },
    {
      "epoch": 0.5554471544715447,
      "grad_norm": 21.62462043762207,
      "learning_rate": 1e-05,
      "loss": 7.0765,
      "step": 10248
    },
    {
      "epoch": 0.5554471544715447,
      "step": 10248,
      "training_loss": 6.66030216217041
    },
    {
      "epoch": 0.5555013550135501,
      "step": 10249,
      "training_loss": 7.778261661529541
    },
    {
      "epoch": 0.5555555555555556,
      "step": 10250,
      "training_loss": 5.894133567810059
    },
    {
      "epoch": 0.555609756097561,
      "step": 10251,
      "training_loss": 6.013963222503662
    },
    {
      "epoch": 0.5556639566395664,
      "grad_norm": 44.12455749511719,
      "learning_rate": 1e-05,
      "loss": 6.5867,
      "step": 10252
    },
    {
      "epoch": 0.5556639566395664,
      "step": 10252,
      "training_loss": 7.91405725479126
    },
    {
      "epoch": 0.5557181571815718,
      "step": 10253,
      "training_loss": 5.891777515411377
    },
    {
      "epoch": 0.5557723577235772,
      "step": 10254,
      "training_loss": 6.656589031219482
    },
    {
      "epoch": 0.5558265582655827,
      "step": 10255,
      "training_loss": 6.6174092292785645
    },
    {
      "epoch": 0.555880758807588,
      "grad_norm": 17.71992301940918,
      "learning_rate": 1e-05,
      "loss": 6.77,
      "step": 10256
    },
    {
      "epoch": 0.555880758807588,
      "step": 10256,
      "training_loss": 6.714356899261475
    },
    {
      "epoch": 0.5559349593495935,
      "step": 10257,
      "training_loss": 5.259883880615234
    },
    {
      "epoch": 0.5559891598915989,
      "step": 10258,
      "training_loss": 7.333856105804443
    },
    {
      "epoch": 0.5560433604336044,
      "step": 10259,
      "training_loss": 6.59744119644165
    },
    {
      "epoch": 0.5560975609756098,
      "grad_norm": 21.574060440063477,
      "learning_rate": 1e-05,
      "loss": 6.4764,
      "step": 10260
    },
    {
      "epoch": 0.5560975609756098,
      "step": 10260,
      "training_loss": 8.33283519744873
    },
    {
      "epoch": 0.5561517615176151,
      "step": 10261,
      "training_loss": 7.186168670654297
    },
    {
      "epoch": 0.5562059620596206,
      "step": 10262,
      "training_loss": 5.201061248779297
    },
    {
      "epoch": 0.556260162601626,
      "step": 10263,
      "training_loss": 5.112277984619141
    },
    {
      "epoch": 0.5563143631436315,
      "grad_norm": 20.4532527923584,
      "learning_rate": 1e-05,
      "loss": 6.4581,
      "step": 10264
    },
    {
      "epoch": 0.5563143631436315,
      "step": 10264,
      "training_loss": 6.803074359893799
    },
    {
      "epoch": 0.5563685636856368,
      "step": 10265,
      "training_loss": 7.433464527130127
    },
    {
      "epoch": 0.5564227642276423,
      "step": 10266,
      "training_loss": 6.7972025871276855
    },
    {
      "epoch": 0.5564769647696477,
      "step": 10267,
      "training_loss": 7.178928852081299
    },
    {
      "epoch": 0.5565311653116531,
      "grad_norm": 22.50616455078125,
      "learning_rate": 1e-05,
      "loss": 7.0532,
      "step": 10268
    },
    {
      "epoch": 0.5565311653116531,
      "step": 10268,
      "training_loss": 7.319755554199219
    },
    {
      "epoch": 0.5565853658536586,
      "step": 10269,
      "training_loss": 5.821589469909668
    },
    {
      "epoch": 0.5566395663956639,
      "step": 10270,
      "training_loss": 6.731960773468018
    },
    {
      "epoch": 0.5566937669376694,
      "step": 10271,
      "training_loss": 5.361306667327881
    },
    {
      "epoch": 0.5567479674796748,
      "grad_norm": 30.69222640991211,
      "learning_rate": 1e-05,
      "loss": 6.3087,
      "step": 10272
    },
    {
      "epoch": 0.5567479674796748,
      "step": 10272,
      "training_loss": 6.04269552230835
    },
    {
      "epoch": 0.5568021680216803,
      "step": 10273,
      "training_loss": 7.087437152862549
    },
    {
      "epoch": 0.5568563685636856,
      "step": 10274,
      "training_loss": 7.310868263244629
    },
    {
      "epoch": 0.556910569105691,
      "step": 10275,
      "training_loss": 5.809119701385498
    },
    {
      "epoch": 0.5569647696476965,
      "grad_norm": 26.33839225769043,
      "learning_rate": 1e-05,
      "loss": 6.5625,
      "step": 10276
    },
    {
      "epoch": 0.5569647696476965,
      "step": 10276,
      "training_loss": 6.614248275756836
    },
    {
      "epoch": 0.5570189701897019,
      "step": 10277,
      "training_loss": 6.851939678192139
    },
    {
      "epoch": 0.5570731707317074,
      "step": 10278,
      "training_loss": 7.000573635101318
    },
    {
      "epoch": 0.5571273712737127,
      "step": 10279,
      "training_loss": 6.464057445526123
    },
    {
      "epoch": 0.5571815718157181,
      "grad_norm": 19.93478012084961,
      "learning_rate": 1e-05,
      "loss": 6.7327,
      "step": 10280
    },
    {
      "epoch": 0.5571815718157181,
      "step": 10280,
      "training_loss": 7.106313705444336
    },
    {
      "epoch": 0.5572357723577236,
      "step": 10281,
      "training_loss": 6.940623760223389
    },
    {
      "epoch": 0.557289972899729,
      "step": 10282,
      "training_loss": 6.147639751434326
    },
    {
      "epoch": 0.5573441734417344,
      "step": 10283,
      "training_loss": 7.097616195678711
    },
    {
      "epoch": 0.5573983739837398,
      "grad_norm": 28.331865310668945,
      "learning_rate": 1e-05,
      "loss": 6.823,
      "step": 10284
    },
    {
      "epoch": 0.5573983739837398,
      "step": 10284,
      "training_loss": 4.437939643859863
    },
    {
      "epoch": 0.5574525745257453,
      "step": 10285,
      "training_loss": 7.572988510131836
    },
    {
      "epoch": 0.5575067750677507,
      "step": 10286,
      "training_loss": 3.9536550045013428
    },
    {
      "epoch": 0.5575609756097561,
      "step": 10287,
      "training_loss": 6.76115608215332
    },
    {
      "epoch": 0.5576151761517615,
      "grad_norm": 25.983631134033203,
      "learning_rate": 1e-05,
      "loss": 5.6814,
      "step": 10288
    },
    {
      "epoch": 0.5576151761517615,
      "step": 10288,
      "training_loss": 6.645658016204834
    },
    {
      "epoch": 0.5576693766937669,
      "step": 10289,
      "training_loss": 5.441540241241455
    },
    {
      "epoch": 0.5577235772357724,
      "step": 10290,
      "training_loss": 7.815933704376221
    },
    {
      "epoch": 0.5577777777777778,
      "step": 10291,
      "training_loss": 6.126594543457031
    },
    {
      "epoch": 0.5578319783197832,
      "grad_norm": 28.869827270507812,
      "learning_rate": 1e-05,
      "loss": 6.5074,
      "step": 10292
    },
    {
      "epoch": 0.5578319783197832,
      "step": 10292,
      "training_loss": 5.940779685974121
    },
    {
      "epoch": 0.5578861788617886,
      "step": 10293,
      "training_loss": 6.427518844604492
    },
    {
      "epoch": 0.557940379403794,
      "step": 10294,
      "training_loss": 5.394731521606445
    },
    {
      "epoch": 0.5579945799457995,
      "step": 10295,
      "training_loss": 8.041586875915527
    },
    {
      "epoch": 0.5580487804878049,
      "grad_norm": 37.89105224609375,
      "learning_rate": 1e-05,
      "loss": 6.4512,
      "step": 10296
    },
    {
      "epoch": 0.5580487804878049,
      "step": 10296,
      "training_loss": 6.161736011505127
    },
    {
      "epoch": 0.5581029810298103,
      "step": 10297,
      "training_loss": 5.8794121742248535
    },
    {
      "epoch": 0.5581571815718157,
      "step": 10298,
      "training_loss": 4.607844352722168
    },
    {
      "epoch": 0.5582113821138212,
      "step": 10299,
      "training_loss": 8.115015029907227
    },
    {
      "epoch": 0.5582655826558266,
      "grad_norm": 40.2039794921875,
      "learning_rate": 1e-05,
      "loss": 6.191,
      "step": 10300
    },
    {
      "epoch": 0.5582655826558266,
      "step": 10300,
      "training_loss": 4.77564001083374
    },
    {
      "epoch": 0.5583197831978319,
      "step": 10301,
      "training_loss": 6.473740577697754
    },
    {
      "epoch": 0.5583739837398374,
      "step": 10302,
      "training_loss": 3.1783080101013184
    },
    {
      "epoch": 0.5584281842818428,
      "step": 10303,
      "training_loss": 7.223475456237793
    },
    {
      "epoch": 0.5584823848238483,
      "grad_norm": 49.873130798339844,
      "learning_rate": 1e-05,
      "loss": 5.4128,
      "step": 10304
    },
    {
      "epoch": 0.5584823848238483,
      "step": 10304,
      "training_loss": 3.4612081050872803
    },
    {
      "epoch": 0.5585365853658537,
      "step": 10305,
      "training_loss": 6.840085506439209
    },
    {
      "epoch": 0.558590785907859,
      "step": 10306,
      "training_loss": 6.98634147644043
    },
    {
      "epoch": 0.5586449864498645,
      "step": 10307,
      "training_loss": 5.260972023010254
    },
    {
      "epoch": 0.5586991869918699,
      "grad_norm": 52.3125,
      "learning_rate": 1e-05,
      "loss": 5.6372,
      "step": 10308
    },
    {
      "epoch": 0.5586991869918699,
      "step": 10308,
      "training_loss": 6.815856456756592
    },
    {
      "epoch": 0.5587533875338754,
      "step": 10309,
      "training_loss": 7.400420188903809
    },
    {
      "epoch": 0.5588075880758807,
      "step": 10310,
      "training_loss": 6.3393731117248535
    },
    {
      "epoch": 0.5588617886178862,
      "step": 10311,
      "training_loss": 4.9536848068237305
    },
    {
      "epoch": 0.5589159891598916,
      "grad_norm": 24.86861228942871,
      "learning_rate": 1e-05,
      "loss": 6.3773,
      "step": 10312
    },
    {
      "epoch": 0.5589159891598916,
      "step": 10312,
      "training_loss": 7.243371486663818
    },
    {
      "epoch": 0.558970189701897,
      "step": 10313,
      "training_loss": 6.373915672302246
    },
    {
      "epoch": 0.5590243902439025,
      "step": 10314,
      "training_loss": 7.870254039764404
    },
    {
      "epoch": 0.5590785907859078,
      "step": 10315,
      "training_loss": 6.8140130043029785
    },
    {
      "epoch": 0.5591327913279133,
      "grad_norm": 28.60189437866211,
      "learning_rate": 1e-05,
      "loss": 7.0754,
      "step": 10316
    },
    {
      "epoch": 0.5591327913279133,
      "step": 10316,
      "training_loss": 6.8944091796875
    },
    {
      "epoch": 0.5591869918699187,
      "step": 10317,
      "training_loss": 5.962188243865967
    },
    {
      "epoch": 0.5592411924119242,
      "step": 10318,
      "training_loss": 6.62021017074585
    },
    {
      "epoch": 0.5592953929539295,
      "step": 10319,
      "training_loss": 7.207704067230225
    },
    {
      "epoch": 0.5593495934959349,
      "grad_norm": 29.086078643798828,
      "learning_rate": 1e-05,
      "loss": 6.6711,
      "step": 10320
    },
    {
      "epoch": 0.5593495934959349,
      "step": 10320,
      "training_loss": 8.283299446105957
    },
    {
      "epoch": 0.5594037940379404,
      "step": 10321,
      "training_loss": 7.898664951324463
    },
    {
      "epoch": 0.5594579945799458,
      "step": 10322,
      "training_loss": 6.7285003662109375
    },
    {
      "epoch": 0.5595121951219513,
      "step": 10323,
      "training_loss": 7.0810441970825195
    },
    {
      "epoch": 0.5595663956639566,
      "grad_norm": 16.439058303833008,
      "learning_rate": 1e-05,
      "loss": 7.4979,
      "step": 10324
    },
    {
      "epoch": 0.5595663956639566,
      "step": 10324,
      "training_loss": 7.337147235870361
    },
    {
      "epoch": 0.559620596205962,
      "step": 10325,
      "training_loss": 7.247504234313965
    },
    {
      "epoch": 0.5596747967479675,
      "step": 10326,
      "training_loss": 4.712551593780518
    },
    {
      "epoch": 0.5597289972899729,
      "step": 10327,
      "training_loss": 7.041409015655518
    },
    {
      "epoch": 0.5597831978319783,
      "grad_norm": 18.58390235900879,
      "learning_rate": 1e-05,
      "loss": 6.5847,
      "step": 10328
    },
    {
      "epoch": 0.5597831978319783,
      "step": 10328,
      "training_loss": 5.730121612548828
    },
    {
      "epoch": 0.5598373983739837,
      "step": 10329,
      "training_loss": 6.646227836608887
    },
    {
      "epoch": 0.5598915989159892,
      "step": 10330,
      "training_loss": 6.430820465087891
    },
    {
      "epoch": 0.5599457994579946,
      "step": 10331,
      "training_loss": 6.761920928955078
    },
    {
      "epoch": 0.56,
      "grad_norm": 28.716543197631836,
      "learning_rate": 1e-05,
      "loss": 6.3923,
      "step": 10332
    },
    {
      "epoch": 0.56,
      "step": 10332,
      "training_loss": 5.394380569458008
    },
    {
      "epoch": 0.5600542005420054,
      "step": 10333,
      "training_loss": 5.263134479522705
    },
    {
      "epoch": 0.5601084010840108,
      "step": 10334,
      "training_loss": 7.283895969390869
    },
    {
      "epoch": 0.5601626016260163,
      "step": 10335,
      "training_loss": 5.085348129272461
    },
    {
      "epoch": 0.5602168021680217,
      "grad_norm": 36.284420013427734,
      "learning_rate": 1e-05,
      "loss": 5.7567,
      "step": 10336
    },
    {
      "epoch": 0.5602168021680217,
      "step": 10336,
      "training_loss": 4.186120986938477
    },
    {
      "epoch": 0.560271002710027,
      "step": 10337,
      "training_loss": 6.635748863220215
    },
    {
      "epoch": 0.5603252032520325,
      "step": 10338,
      "training_loss": 8.433313369750977
    },
    {
      "epoch": 0.560379403794038,
      "step": 10339,
      "training_loss": 7.835522651672363
    },
    {
      "epoch": 0.5604336043360434,
      "grad_norm": 30.4494571685791,
      "learning_rate": 1e-05,
      "loss": 6.7727,
      "step": 10340
    },
    {
      "epoch": 0.5604336043360434,
      "step": 10340,
      "training_loss": 7.07396936416626
    },
    {
      "epoch": 0.5604878048780488,
      "step": 10341,
      "training_loss": 7.971859455108643
    },
    {
      "epoch": 0.5605420054200542,
      "step": 10342,
      "training_loss": 7.957704544067383
    },
    {
      "epoch": 0.5605962059620596,
      "step": 10343,
      "training_loss": 6.5305047035217285
    },
    {
      "epoch": 0.5606504065040651,
      "grad_norm": 31.92664337158203,
      "learning_rate": 1e-05,
      "loss": 7.3835,
      "step": 10344
    },
    {
      "epoch": 0.5606504065040651,
      "step": 10344,
      "training_loss": 7.814095973968506
    },
    {
      "epoch": 0.5607046070460705,
      "step": 10345,
      "training_loss": 8.150463104248047
    },
    {
      "epoch": 0.5607588075880758,
      "step": 10346,
      "training_loss": 6.864407062530518
    },
    {
      "epoch": 0.5608130081300813,
      "step": 10347,
      "training_loss": 6.665713310241699
    },
    {
      "epoch": 0.5608672086720867,
      "grad_norm": 37.638275146484375,
      "learning_rate": 1e-05,
      "loss": 7.3737,
      "step": 10348
    },
    {
      "epoch": 0.5608672086720867,
      "step": 10348,
      "training_loss": 6.750881195068359
    },
    {
      "epoch": 0.5609214092140922,
      "step": 10349,
      "training_loss": 7.614865779876709
    },
    {
      "epoch": 0.5609756097560976,
      "step": 10350,
      "training_loss": 7.700051784515381
    },
    {
      "epoch": 0.561029810298103,
      "step": 10351,
      "training_loss": 6.725773811340332
    },
    {
      "epoch": 0.5610840108401084,
      "grad_norm": 39.430599212646484,
      "learning_rate": 1e-05,
      "loss": 7.1979,
      "step": 10352
    },
    {
      "epoch": 0.5610840108401084,
      "step": 10352,
      "training_loss": 5.405179500579834
    },
    {
      "epoch": 0.5611382113821138,
      "step": 10353,
      "training_loss": 6.43522834777832
    },
    {
      "epoch": 0.5611924119241193,
      "step": 10354,
      "training_loss": 3.227720022201538
    },
    {
      "epoch": 0.5612466124661246,
      "step": 10355,
      "training_loss": 6.707723140716553
    },
    {
      "epoch": 0.5613008130081301,
      "grad_norm": 20.10828971862793,
      "learning_rate": 1e-05,
      "loss": 5.444,
      "step": 10356
    },
    {
      "epoch": 0.5613008130081301,
      "step": 10356,
      "training_loss": 6.630204200744629
    },
    {
      "epoch": 0.5613550135501355,
      "step": 10357,
      "training_loss": 8.013134956359863
    },
    {
      "epoch": 0.561409214092141,
      "step": 10358,
      "training_loss": 5.9237260818481445
    },
    {
      "epoch": 0.5614634146341464,
      "step": 10359,
      "training_loss": 8.197988510131836
    },
    {
      "epoch": 0.5615176151761517,
      "grad_norm": 33.13504409790039,
      "learning_rate": 1e-05,
      "loss": 7.1913,
      "step": 10360
    },
    {
      "epoch": 0.5615176151761517,
      "step": 10360,
      "training_loss": 7.879618167877197
    },
    {
      "epoch": 0.5615718157181572,
      "step": 10361,
      "training_loss": 5.70021390914917
    },
    {
      "epoch": 0.5616260162601626,
      "step": 10362,
      "training_loss": 7.540599822998047
    },
    {
      "epoch": 0.5616802168021681,
      "step": 10363,
      "training_loss": 7.180606365203857
    },
    {
      "epoch": 0.5617344173441734,
      "grad_norm": 22.75647735595703,
      "learning_rate": 1e-05,
      "loss": 7.0753,
      "step": 10364
    },
    {
      "epoch": 0.5617344173441734,
      "step": 10364,
      "training_loss": 5.69468879699707
    },
    {
      "epoch": 0.5617886178861788,
      "step": 10365,
      "training_loss": 7.299725532531738
    },
    {
      "epoch": 0.5618428184281843,
      "step": 10366,
      "training_loss": 7.982301712036133
    },
    {
      "epoch": 0.5618970189701897,
      "step": 10367,
      "training_loss": 6.618002414703369
    },
    {
      "epoch": 0.5619512195121952,
      "grad_norm": 32.529258728027344,
      "learning_rate": 1e-05,
      "loss": 6.8987,
      "step": 10368
    },
    {
      "epoch": 0.5619512195121952,
      "step": 10368,
      "training_loss": 6.781437397003174
    },
    {
      "epoch": 0.5620054200542005,
      "step": 10369,
      "training_loss": 7.068377494812012
    },
    {
      "epoch": 0.562059620596206,
      "step": 10370,
      "training_loss": 7.275594234466553
    },
    {
      "epoch": 0.5621138211382114,
      "step": 10371,
      "training_loss": 6.190382957458496
    },
    {
      "epoch": 0.5621680216802168,
      "grad_norm": 24.4677791595459,
      "learning_rate": 1e-05,
      "loss": 6.8289,
      "step": 10372
    },
    {
      "epoch": 0.5621680216802168,
      "step": 10372,
      "training_loss": 5.95583438873291
    },
    {
      "epoch": 0.5622222222222222,
      "step": 10373,
      "training_loss": 4.694339752197266
    },
    {
      "epoch": 0.5622764227642276,
      "step": 10374,
      "training_loss": 6.972382068634033
    },
    {
      "epoch": 0.5623306233062331,
      "step": 10375,
      "training_loss": 6.637528896331787
    },
    {
      "epoch": 0.5623848238482385,
      "grad_norm": 18.927595138549805,
      "learning_rate": 1e-05,
      "loss": 6.065,
      "step": 10376
    },
    {
      "epoch": 0.5623848238482385,
      "step": 10376,
      "training_loss": 6.6817426681518555
    },
    {
      "epoch": 0.562439024390244,
      "step": 10377,
      "training_loss": 6.825987339019775
    },
    {
      "epoch": 0.5624932249322493,
      "step": 10378,
      "training_loss": 7.085963726043701
    },
    {
      "epoch": 0.5625474254742547,
      "step": 10379,
      "training_loss": 6.702401161193848
    },
    {
      "epoch": 0.5626016260162602,
      "grad_norm": 23.961448669433594,
      "learning_rate": 1e-05,
      "loss": 6.824,
      "step": 10380
    },
    {
      "epoch": 0.5626016260162602,
      "step": 10380,
      "training_loss": 5.9597601890563965
    },
    {
      "epoch": 0.5626558265582656,
      "step": 10381,
      "training_loss": 5.536291599273682
    },
    {
      "epoch": 0.562710027100271,
      "step": 10382,
      "training_loss": 7.566895008087158
    },
    {
      "epoch": 0.5627642276422764,
      "step": 10383,
      "training_loss": 5.717249393463135
    },
    {
      "epoch": 0.5628184281842818,
      "grad_norm": 46.94078063964844,
      "learning_rate": 1e-05,
      "loss": 6.195,
      "step": 10384
    },
    {
      "epoch": 0.5628184281842818,
      "step": 10384,
      "training_loss": 6.556564807891846
    },
    {
      "epoch": 0.5628726287262873,
      "step": 10385,
      "training_loss": 7.057911396026611
    },
    {
      "epoch": 0.5629268292682926,
      "step": 10386,
      "training_loss": 7.658633232116699
    },
    {
      "epoch": 0.5629810298102981,
      "step": 10387,
      "training_loss": 6.767714977264404
    },
    {
      "epoch": 0.5630352303523035,
      "grad_norm": 22.365461349487305,
      "learning_rate": 1e-05,
      "loss": 7.0102,
      "step": 10388
    },
    {
      "epoch": 0.5630352303523035,
      "step": 10388,
      "training_loss": 7.538250923156738
    },
    {
      "epoch": 0.563089430894309,
      "step": 10389,
      "training_loss": 7.427778720855713
    },
    {
      "epoch": 0.5631436314363144,
      "step": 10390,
      "training_loss": 7.978275775909424
    },
    {
      "epoch": 0.5631978319783197,
      "step": 10391,
      "training_loss": 5.972560405731201
    },
    {
      "epoch": 0.5632520325203252,
      "grad_norm": 42.645843505859375,
      "learning_rate": 1e-05,
      "loss": 7.2292,
      "step": 10392
    },
    {
      "epoch": 0.5632520325203252,
      "step": 10392,
      "training_loss": 4.064203262329102
    },
    {
      "epoch": 0.5633062330623306,
      "step": 10393,
      "training_loss": 7.886270999908447
    },
    {
      "epoch": 0.5633604336043361,
      "step": 10394,
      "training_loss": 5.913489818572998
    },
    {
      "epoch": 0.5634146341463414,
      "step": 10395,
      "training_loss": 6.433365345001221
    },
    {
      "epoch": 0.5634688346883469,
      "grad_norm": 32.329742431640625,
      "learning_rate": 1e-05,
      "loss": 6.0743,
      "step": 10396
    },
    {
      "epoch": 0.5634688346883469,
      "step": 10396,
      "training_loss": 5.8368706703186035
    },
    {
      "epoch": 0.5635230352303523,
      "step": 10397,
      "training_loss": 7.36315393447876
    },
    {
      "epoch": 0.5635772357723577,
      "step": 10398,
      "training_loss": 3.6648662090301514
    },
    {
      "epoch": 0.5636314363143632,
      "step": 10399,
      "training_loss": 7.265148639678955
    },
    {
      "epoch": 0.5636856368563685,
      "grad_norm": 39.64714050292969,
      "learning_rate": 1e-05,
      "loss": 6.0325,
      "step": 10400
    },
    {
      "epoch": 0.5636856368563685,
      "step": 10400,
      "training_loss": 7.673219680786133
    },
    {
      "epoch": 0.563739837398374,
      "step": 10401,
      "training_loss": 7.2148332595825195
    },
    {
      "epoch": 0.5637940379403794,
      "step": 10402,
      "training_loss": 6.981037139892578
    },
    {
      "epoch": 0.5638482384823849,
      "step": 10403,
      "training_loss": 6.217399597167969
    },
    {
      "epoch": 0.5639024390243902,
      "grad_norm": 26.633861541748047,
      "learning_rate": 1e-05,
      "loss": 7.0216,
      "step": 10404
    },
    {
      "epoch": 0.5639024390243902,
      "step": 10404,
      "training_loss": 6.522485256195068
    },
    {
      "epoch": 0.5639566395663956,
      "step": 10405,
      "training_loss": 4.179061412811279
    },
    {
      "epoch": 0.5640108401084011,
      "step": 10406,
      "training_loss": 6.629424571990967
    },
    {
      "epoch": 0.5640650406504065,
      "step": 10407,
      "training_loss": 6.741423606872559
    },
    {
      "epoch": 0.564119241192412,
      "grad_norm": 32.168426513671875,
      "learning_rate": 1e-05,
      "loss": 6.0181,
      "step": 10408
    },
    {
      "epoch": 0.564119241192412,
      "step": 10408,
      "training_loss": 5.9414191246032715
    },
    {
      "epoch": 0.5641734417344173,
      "step": 10409,
      "training_loss": 7.573800086975098
    },
    {
      "epoch": 0.5642276422764227,
      "step": 10410,
      "training_loss": 7.1949687004089355
    },
    {
      "epoch": 0.5642818428184282,
      "step": 10411,
      "training_loss": 7.035340309143066
    },
    {
      "epoch": 0.5643360433604336,
      "grad_norm": 21.518634796142578,
      "learning_rate": 1e-05,
      "loss": 6.9364,
      "step": 10412
    },
    {
      "epoch": 0.5643360433604336,
      "step": 10412,
      "training_loss": 8.18941879272461
    },
    {
      "epoch": 0.564390243902439,
      "step": 10413,
      "training_loss": 7.229745388031006
    },
    {
      "epoch": 0.5644444444444444,
      "step": 10414,
      "training_loss": 5.3968281745910645
    },
    {
      "epoch": 0.5644986449864499,
      "step": 10415,
      "training_loss": 6.646704196929932
    },
    {
      "epoch": 0.5645528455284553,
      "grad_norm": 31.12897491455078,
      "learning_rate": 1e-05,
      "loss": 6.8657,
      "step": 10416
    },
    {
      "epoch": 0.5645528455284553,
      "step": 10416,
      "training_loss": 6.756365776062012
    },
    {
      "epoch": 0.5646070460704607,
      "step": 10417,
      "training_loss": 5.986629009246826
    },
    {
      "epoch": 0.5646612466124661,
      "step": 10418,
      "training_loss": 6.939435005187988
    },
    {
      "epoch": 0.5647154471544715,
      "step": 10419,
      "training_loss": 6.759681224822998
    },
    {
      "epoch": 0.564769647696477,
      "grad_norm": 19.461666107177734,
      "learning_rate": 1e-05,
      "loss": 6.6105,
      "step": 10420
    },
    {
      "epoch": 0.564769647696477,
      "step": 10420,
      "training_loss": 5.450059413909912
    },
    {
      "epoch": 0.5648238482384824,
      "step": 10421,
      "training_loss": 7.388752460479736
    },
    {
      "epoch": 0.5648780487804878,
      "step": 10422,
      "training_loss": 5.600449562072754
    },
    {
      "epoch": 0.5649322493224932,
      "step": 10423,
      "training_loss": 7.095954895019531
    },
    {
      "epoch": 0.5649864498644986,
      "grad_norm": 22.00181770324707,
      "learning_rate": 1e-05,
      "loss": 6.3838,
      "step": 10424
    },
    {
      "epoch": 0.5649864498644986,
      "step": 10424,
      "training_loss": 4.999387741088867
    },
    {
      "epoch": 0.5650406504065041,
      "step": 10425,
      "training_loss": 6.52104377746582
    },
    {
      "epoch": 0.5650948509485095,
      "step": 10426,
      "training_loss": 5.35963249206543
    },
    {
      "epoch": 0.5651490514905149,
      "step": 10427,
      "training_loss": 5.984370708465576
    },
    {
      "epoch": 0.5652032520325203,
      "grad_norm": 46.571388244628906,
      "learning_rate": 1e-05,
      "loss": 5.7161,
      "step": 10428
    },
    {
      "epoch": 0.5652032520325203,
      "step": 10428,
      "training_loss": 5.870128154754639
    },
    {
      "epoch": 0.5652574525745258,
      "step": 10429,
      "training_loss": 7.1796464920043945
    },
    {
      "epoch": 0.5653116531165312,
      "step": 10430,
      "training_loss": 6.232906818389893
    },
    {
      "epoch": 0.5653658536585365,
      "step": 10431,
      "training_loss": 6.696777820587158
    },
    {
      "epoch": 0.565420054200542,
      "grad_norm": 69.65573120117188,
      "learning_rate": 1e-05,
      "loss": 6.4949,
      "step": 10432
    },
    {
      "epoch": 0.565420054200542,
      "step": 10432,
      "training_loss": 6.906976699829102
    },
    {
      "epoch": 0.5654742547425474,
      "step": 10433,
      "training_loss": 8.144091606140137
    },
    {
      "epoch": 0.5655284552845529,
      "step": 10434,
      "training_loss": 6.827754020690918
    },
    {
      "epoch": 0.5655826558265583,
      "step": 10435,
      "training_loss": 6.301621437072754
    },
    {
      "epoch": 0.5656368563685636,
      "grad_norm": 24.571645736694336,
      "learning_rate": 1e-05,
      "loss": 7.0451,
      "step": 10436
    },
    {
      "epoch": 0.5656368563685636,
      "step": 10436,
      "training_loss": 4.974908351898193
    },
    {
      "epoch": 0.5656910569105691,
      "step": 10437,
      "training_loss": 5.791353225708008
    },
    {
      "epoch": 0.5657452574525745,
      "step": 10438,
      "training_loss": 6.790146350860596
    },
    {
      "epoch": 0.56579945799458,
      "step": 10439,
      "training_loss": 6.753128528594971
    },
    {
      "epoch": 0.5658536585365853,
      "grad_norm": 51.39850997924805,
      "learning_rate": 1e-05,
      "loss": 6.0774,
      "step": 10440
    },
    {
      "epoch": 0.5658536585365853,
      "step": 10440,
      "training_loss": 6.426671028137207
    },
    {
      "epoch": 0.5659078590785908,
      "step": 10441,
      "training_loss": 6.776679992675781
    },
    {
      "epoch": 0.5659620596205962,
      "step": 10442,
      "training_loss": 9.42129135131836
    },
    {
      "epoch": 0.5660162601626016,
      "step": 10443,
      "training_loss": 5.896242618560791
    },
    {
      "epoch": 0.5660704607046071,
      "grad_norm": 20.94673728942871,
      "learning_rate": 1e-05,
      "loss": 7.1302,
      "step": 10444
    },
    {
      "epoch": 0.5660704607046071,
      "step": 10444,
      "training_loss": 9.137641906738281
    },
    {
      "epoch": 0.5661246612466124,
      "step": 10445,
      "training_loss": 6.299225807189941
    },
    {
      "epoch": 0.5661788617886179,
      "step": 10446,
      "training_loss": 7.010969638824463
    },
    {
      "epoch": 0.5662330623306233,
      "step": 10447,
      "training_loss": 6.680572032928467
    },
    {
      "epoch": 0.5662872628726288,
      "grad_norm": 29.59135627746582,
      "learning_rate": 1e-05,
      "loss": 7.2821,
      "step": 10448
    },
    {
      "epoch": 0.5662872628726288,
      "step": 10448,
      "training_loss": 7.052921295166016
    },
    {
      "epoch": 0.5663414634146341,
      "step": 10449,
      "training_loss": 5.650815486907959
    },
    {
      "epoch": 0.5663956639566395,
      "step": 10450,
      "training_loss": 7.237578392028809
    },
    {
      "epoch": 0.566449864498645,
      "step": 10451,
      "training_loss": 7.322469711303711
    },
    {
      "epoch": 0.5665040650406504,
      "grad_norm": 24.010461807250977,
      "learning_rate": 1e-05,
      "loss": 6.8159,
      "step": 10452
    },
    {
      "epoch": 0.5665040650406504,
      "step": 10452,
      "training_loss": 8.60381031036377
    },
    {
      "epoch": 0.5665582655826559,
      "step": 10453,
      "training_loss": 7.898327827453613
    },
    {
      "epoch": 0.5666124661246612,
      "step": 10454,
      "training_loss": 5.680578708648682
    },
    {
      "epoch": 0.5666666666666667,
      "step": 10455,
      "training_loss": 6.40401029586792
    },
    {
      "epoch": 0.5667208672086721,
      "grad_norm": 19.194520950317383,
      "learning_rate": 1e-05,
      "loss": 7.1467,
      "step": 10456
    },
    {
      "epoch": 0.5667208672086721,
      "step": 10456,
      "training_loss": 3.1037139892578125
    },
    {
      "epoch": 0.5667750677506775,
      "step": 10457,
      "training_loss": 6.411881923675537
    },
    {
      "epoch": 0.5668292682926829,
      "step": 10458,
      "training_loss": 7.057480812072754
    },
    {
      "epoch": 0.5668834688346883,
      "step": 10459,
      "training_loss": 7.030624866485596
    },
    {
      "epoch": 0.5669376693766938,
      "grad_norm": 31.712692260742188,
      "learning_rate": 1e-05,
      "loss": 5.9009,
      "step": 10460
    },
    {
      "epoch": 0.5669376693766938,
      "step": 10460,
      "training_loss": 7.456546783447266
    },
    {
      "epoch": 0.5669918699186992,
      "step": 10461,
      "training_loss": 6.622799396514893
    },
    {
      "epoch": 0.5670460704607047,
      "step": 10462,
      "training_loss": 7.469241619110107
    },
    {
      "epoch": 0.56710027100271,
      "step": 10463,
      "training_loss": 5.684375286102295
    },
    {
      "epoch": 0.5671544715447154,
      "grad_norm": 28.455039978027344,
      "learning_rate": 1e-05,
      "loss": 6.8082,
      "step": 10464
    },
    {
      "epoch": 0.5671544715447154,
      "step": 10464,
      "training_loss": 8.160179138183594
    },
    {
      "epoch": 0.5672086720867209,
      "step": 10465,
      "training_loss": 3.3394057750701904
    },
    {
      "epoch": 0.5672628726287263,
      "step": 10466,
      "training_loss": 7.681423664093018
    },
    {
      "epoch": 0.5673170731707317,
      "step": 10467,
      "training_loss": 7.133221626281738
    },
    {
      "epoch": 0.5673712737127371,
      "grad_norm": 35.92129898071289,
      "learning_rate": 1e-05,
      "loss": 6.5786,
      "step": 10468
    },
    {
      "epoch": 0.5673712737127371,
      "step": 10468,
      "training_loss": 5.347471714019775
    },
    {
      "epoch": 0.5674254742547425,
      "step": 10469,
      "training_loss": 6.953397750854492
    },
    {
      "epoch": 0.567479674796748,
      "step": 10470,
      "training_loss": 7.2033586502075195
    },
    {
      "epoch": 0.5675338753387534,
      "step": 10471,
      "training_loss": 6.839695453643799
    },
    {
      "epoch": 0.5675880758807588,
      "grad_norm": 28.850046157836914,
      "learning_rate": 1e-05,
      "loss": 6.586,
      "step": 10472
    },
    {
      "epoch": 0.5675880758807588,
      "step": 10472,
      "training_loss": 6.2420783042907715
    },
    {
      "epoch": 0.5676422764227642,
      "step": 10473,
      "training_loss": 5.8736114501953125
    },
    {
      "epoch": 0.5676964769647697,
      "step": 10474,
      "training_loss": 7.9209303855896
    },
    {
      "epoch": 0.5677506775067751,
      "step": 10475,
      "training_loss": 5.673335075378418
    },
    {
      "epoch": 0.5678048780487804,
      "grad_norm": 25.874454498291016,
      "learning_rate": 1e-05,
      "loss": 6.4275,
      "step": 10476
    },
    {
      "epoch": 0.5678048780487804,
      "step": 10476,
      "training_loss": 5.909512519836426
    },
    {
      "epoch": 0.5678590785907859,
      "step": 10477,
      "training_loss": 5.998401165008545
    },
    {
      "epoch": 0.5679132791327913,
      "step": 10478,
      "training_loss": 6.960227966308594
    },
    {
      "epoch": 0.5679674796747968,
      "step": 10479,
      "training_loss": 6.182552337646484
    },
    {
      "epoch": 0.5680216802168022,
      "grad_norm": 25.8763370513916,
      "learning_rate": 1e-05,
      "loss": 6.2627,
      "step": 10480
    },
    {
      "epoch": 0.5680216802168022,
      "step": 10480,
      "training_loss": 7.431725025177002
    },
    {
      "epoch": 0.5680758807588075,
      "step": 10481,
      "training_loss": 5.8312554359436035
    },
    {
      "epoch": 0.568130081300813,
      "step": 10482,
      "training_loss": 7.5887041091918945
    },
    {
      "epoch": 0.5681842818428184,
      "step": 10483,
      "training_loss": 7.061915397644043
    },
    {
      "epoch": 0.5682384823848239,
      "grad_norm": 27.83323097229004,
      "learning_rate": 1e-05,
      "loss": 6.9784,
      "step": 10484
    },
    {
      "epoch": 0.5682384823848239,
      "step": 10484,
      "training_loss": 7.232338905334473
    },
    {
      "epoch": 0.5682926829268292,
      "step": 10485,
      "training_loss": 7.461278915405273
    },
    {
      "epoch": 0.5683468834688347,
      "step": 10486,
      "training_loss": 5.010290622711182
    },
    {
      "epoch": 0.5684010840108401,
      "step": 10487,
      "training_loss": 5.070282936096191
    },
    {
      "epoch": 0.5684552845528456,
      "grad_norm": 37.67283630371094,
      "learning_rate": 1e-05,
      "loss": 6.1935,
      "step": 10488
    },
    {
      "epoch": 0.5684552845528456,
      "step": 10488,
      "training_loss": 6.656632423400879
    },
    {
      "epoch": 0.568509485094851,
      "step": 10489,
      "training_loss": 7.278044700622559
    },
    {
      "epoch": 0.5685636856368563,
      "step": 10490,
      "training_loss": 6.175861835479736
    },
    {
      "epoch": 0.5686178861788618,
      "step": 10491,
      "training_loss": 5.3690571784973145
    },
    {
      "epoch": 0.5686720867208672,
      "grad_norm": 26.928462982177734,
      "learning_rate": 1e-05,
      "loss": 6.3699,
      "step": 10492
    },
    {
      "epoch": 0.5686720867208672,
      "step": 10492,
      "training_loss": 6.855340480804443
    },
    {
      "epoch": 0.5687262872628727,
      "step": 10493,
      "training_loss": 6.507912635803223
    },
    {
      "epoch": 0.568780487804878,
      "step": 10494,
      "training_loss": 7.2865471839904785
    },
    {
      "epoch": 0.5688346883468834,
      "step": 10495,
      "training_loss": 5.990206241607666
    },
    {
      "epoch": 0.5688888888888889,
      "grad_norm": 36.29056167602539,
      "learning_rate": 1e-05,
      "loss": 6.66,
      "step": 10496
    },
    {
      "epoch": 0.5688888888888889,
      "step": 10496,
      "training_loss": 7.183734893798828
    },
    {
      "epoch": 0.5689430894308943,
      "step": 10497,
      "training_loss": 7.284801006317139
    },
    {
      "epoch": 0.5689972899728998,
      "step": 10498,
      "training_loss": 5.9004435539245605
    },
    {
      "epoch": 0.5690514905149051,
      "step": 10499,
      "training_loss": 6.147570610046387
    },
    {
      "epoch": 0.5691056910569106,
      "grad_norm": 45.59368133544922,
      "learning_rate": 1e-05,
      "loss": 6.6291,
      "step": 10500
    },
    {
      "epoch": 0.5691056910569106,
      "step": 10500,
      "training_loss": 6.850982189178467
    },
    {
      "epoch": 0.569159891598916,
      "step": 10501,
      "training_loss": 6.295035362243652
    },
    {
      "epoch": 0.5692140921409214,
      "step": 10502,
      "training_loss": 7.011207103729248
    },
    {
      "epoch": 0.5692682926829268,
      "step": 10503,
      "training_loss": 7.248258590698242
    },
    {
      "epoch": 0.5693224932249322,
      "grad_norm": 38.847816467285156,
      "learning_rate": 1e-05,
      "loss": 6.8514,
      "step": 10504
    },
    {
      "epoch": 0.5693224932249322,
      "step": 10504,
      "training_loss": 7.67783260345459
    },
    {
      "epoch": 0.5693766937669377,
      "step": 10505,
      "training_loss": 6.64480447769165
    },
    {
      "epoch": 0.5694308943089431,
      "step": 10506,
      "training_loss": 7.602640151977539
    },
    {
      "epoch": 0.5694850948509486,
      "step": 10507,
      "training_loss": 5.6373138427734375
    },
    {
      "epoch": 0.5695392953929539,
      "grad_norm": 34.23382568359375,
      "learning_rate": 1e-05,
      "loss": 6.8906,
      "step": 10508
    },
    {
      "epoch": 0.5695392953929539,
      "step": 10508,
      "training_loss": 6.1576128005981445
    },
    {
      "epoch": 0.5695934959349593,
      "step": 10509,
      "training_loss": 7.0882568359375
    },
    {
      "epoch": 0.5696476964769648,
      "step": 10510,
      "training_loss": 3.475395917892456
    },
    {
      "epoch": 0.5697018970189702,
      "step": 10511,
      "training_loss": 4.5720906257629395
    },
    {
      "epoch": 0.5697560975609756,
      "grad_norm": 37.844112396240234,
      "learning_rate": 1e-05,
      "loss": 5.3233,
      "step": 10512
    },
    {
      "epoch": 0.5697560975609756,
      "step": 10512,
      "training_loss": 5.329232692718506
    },
    {
      "epoch": 0.569810298102981,
      "step": 10513,
      "training_loss": 6.35496187210083
    },
    {
      "epoch": 0.5698644986449865,
      "step": 10514,
      "training_loss": 3.429819107055664
    },
    {
      "epoch": 0.5699186991869919,
      "step": 10515,
      "training_loss": 6.715415000915527
    },
    {
      "epoch": 0.5699728997289973,
      "grad_norm": 16.603713989257812,
      "learning_rate": 1e-05,
      "loss": 5.4574,
      "step": 10516
    },
    {
      "epoch": 0.5699728997289973,
      "step": 10516,
      "training_loss": 6.758861541748047
    },
    {
      "epoch": 0.5700271002710027,
      "step": 10517,
      "training_loss": 6.5050835609436035
    },
    {
      "epoch": 0.5700813008130081,
      "step": 10518,
      "training_loss": 5.053009033203125
    },
    {
      "epoch": 0.5701355013550136,
      "step": 10519,
      "training_loss": 5.4364776611328125
    },
    {
      "epoch": 0.570189701897019,
      "grad_norm": 69.40625,
      "learning_rate": 1e-05,
      "loss": 5.9384,
      "step": 10520
    },
    {
      "epoch": 0.570189701897019,
      "step": 10520,
      "training_loss": 5.00968599319458
    },
    {
      "epoch": 0.5702439024390243,
      "step": 10521,
      "training_loss": 6.065668106079102
    },
    {
      "epoch": 0.5702981029810298,
      "step": 10522,
      "training_loss": 7.0172576904296875
    },
    {
      "epoch": 0.5703523035230352,
      "step": 10523,
      "training_loss": 3.3851706981658936
    },
    {
      "epoch": 0.5704065040650407,
      "grad_norm": 28.489748001098633,
      "learning_rate": 1e-05,
      "loss": 5.3694,
      "step": 10524
    },
    {
      "epoch": 0.5704065040650407,
      "step": 10524,
      "training_loss": 7.173994064331055
    },
    {
      "epoch": 0.5704607046070461,
      "step": 10525,
      "training_loss": 7.2607550621032715
    },
    {
      "epoch": 0.5705149051490515,
      "step": 10526,
      "training_loss": 6.684391975402832
    },
    {
      "epoch": 0.5705691056910569,
      "step": 10527,
      "training_loss": 4.758415699005127
    },
    {
      "epoch": 0.5706233062330623,
      "grad_norm": 25.086833953857422,
      "learning_rate": 1e-05,
      "loss": 6.4694,
      "step": 10528
    },
    {
      "epoch": 0.5706233062330623,
      "step": 10528,
      "training_loss": 6.459214210510254
    },
    {
      "epoch": 0.5706775067750678,
      "step": 10529,
      "training_loss": 7.970938205718994
    },
    {
      "epoch": 0.5707317073170731,
      "step": 10530,
      "training_loss": 3.895643711090088
    },
    {
      "epoch": 0.5707859078590786,
      "step": 10531,
      "training_loss": 5.3581767082214355
    },
    {
      "epoch": 0.570840108401084,
      "grad_norm": 62.46110916137695,
      "learning_rate": 1e-05,
      "loss": 5.921,
      "step": 10532
    },
    {
      "epoch": 0.570840108401084,
      "step": 10532,
      "training_loss": 6.9544172286987305
    },
    {
      "epoch": 0.5708943089430895,
      "step": 10533,
      "training_loss": 5.943472385406494
    },
    {
      "epoch": 0.5709485094850949,
      "step": 10534,
      "training_loss": 6.55913782119751
    },
    {
      "epoch": 0.5710027100271002,
      "step": 10535,
      "training_loss": 8.16065502166748
    },
    {
      "epoch": 0.5710569105691057,
      "grad_norm": 30.675100326538086,
      "learning_rate": 1e-05,
      "loss": 6.9044,
      "step": 10536
    },
    {
      "epoch": 0.5710569105691057,
      "step": 10536,
      "training_loss": 5.2714762687683105
    },
    {
      "epoch": 0.5711111111111111,
      "step": 10537,
      "training_loss": 5.983870029449463
    },
    {
      "epoch": 0.5711653116531166,
      "step": 10538,
      "training_loss": 5.520933151245117
    },
    {
      "epoch": 0.5712195121951219,
      "step": 10539,
      "training_loss": 8.593856811523438
    },
    {
      "epoch": 0.5712737127371273,
      "grad_norm": 34.95151138305664,
      "learning_rate": 1e-05,
      "loss": 6.3425,
      "step": 10540
    },
    {
      "epoch": 0.5712737127371273,
      "step": 10540,
      "training_loss": 7.236551284790039
    },
    {
      "epoch": 0.5713279132791328,
      "step": 10541,
      "training_loss": 7.9229278564453125
    },
    {
      "epoch": 0.5713821138211382,
      "step": 10542,
      "training_loss": 7.311246395111084
    },
    {
      "epoch": 0.5714363143631437,
      "step": 10543,
      "training_loss": 6.956802845001221
    },
    {
      "epoch": 0.571490514905149,
      "grad_norm": 40.0702018737793,
      "learning_rate": 1e-05,
      "loss": 7.3569,
      "step": 10544
    },
    {
      "epoch": 0.571490514905149,
      "step": 10544,
      "training_loss": 7.872537136077881
    },
    {
      "epoch": 0.5715447154471545,
      "step": 10545,
      "training_loss": 8.152029037475586
    },
    {
      "epoch": 0.5715989159891599,
      "step": 10546,
      "training_loss": 5.886329174041748
    },
    {
      "epoch": 0.5716531165311654,
      "step": 10547,
      "training_loss": 7.07614803314209
    },
    {
      "epoch": 0.5717073170731707,
      "grad_norm": 19.07423973083496,
      "learning_rate": 1e-05,
      "loss": 7.2468,
      "step": 10548
    },
    {
      "epoch": 0.5717073170731707,
      "step": 10548,
      "training_loss": 6.822338581085205
    },
    {
      "epoch": 0.5717615176151761,
      "step": 10549,
      "training_loss": 4.595409393310547
    },
    {
      "epoch": 0.5718157181571816,
      "step": 10550,
      "training_loss": 7.129879474639893
    },
    {
      "epoch": 0.571869918699187,
      "step": 10551,
      "training_loss": 5.790482044219971
    },
    {
      "epoch": 0.5719241192411925,
      "grad_norm": 35.56757354736328,
      "learning_rate": 1e-05,
      "loss": 6.0845,
      "step": 10552
    },
    {
      "epoch": 0.5719241192411925,
      "step": 10552,
      "training_loss": 5.787370204925537
    },
    {
      "epoch": 0.5719783197831978,
      "step": 10553,
      "training_loss": 6.075467109680176
    },
    {
      "epoch": 0.5720325203252032,
      "step": 10554,
      "training_loss": 5.729782581329346
    },
    {
      "epoch": 0.5720867208672087,
      "step": 10555,
      "training_loss": 8.041638374328613
    },
    {
      "epoch": 0.5721409214092141,
      "grad_norm": 72.20128631591797,
      "learning_rate": 1e-05,
      "loss": 6.4086,
      "step": 10556
    },
    {
      "epoch": 0.5721409214092141,
      "step": 10556,
      "training_loss": 3.375519275665283
    },
    {
      "epoch": 0.5721951219512195,
      "step": 10557,
      "training_loss": 7.40240478515625
    },
    {
      "epoch": 0.5722493224932249,
      "step": 10558,
      "training_loss": 7.610381126403809
    },
    {
      "epoch": 0.5723035230352304,
      "step": 10559,
      "training_loss": 6.635237693786621
    },
    {
      "epoch": 0.5723577235772358,
      "grad_norm": 19.871496200561523,
      "learning_rate": 1e-05,
      "loss": 6.2559,
      "step": 10560
    },
    {
      "epoch": 0.5723577235772358,
      "step": 10560,
      "training_loss": 6.4174957275390625
    },
    {
      "epoch": 0.5724119241192412,
      "step": 10561,
      "training_loss": 6.763468265533447
    },
    {
      "epoch": 0.5724661246612466,
      "step": 10562,
      "training_loss": 6.876895904541016
    },
    {
      "epoch": 0.572520325203252,
      "step": 10563,
      "training_loss": 7.192476272583008
    },
    {
      "epoch": 0.5725745257452575,
      "grad_norm": 31.02653694152832,
      "learning_rate": 1e-05,
      "loss": 6.8126,
      "step": 10564
    },
    {
      "epoch": 0.5725745257452575,
      "step": 10564,
      "training_loss": 7.295247554779053
    },
    {
      "epoch": 0.5726287262872629,
      "step": 10565,
      "training_loss": 8.640125274658203
    },
    {
      "epoch": 0.5726829268292682,
      "step": 10566,
      "training_loss": 6.03096342086792
    },
    {
      "epoch": 0.5727371273712737,
      "step": 10567,
      "training_loss": 6.882867813110352
    },
    {
      "epoch": 0.5727913279132791,
      "grad_norm": 24.53270721435547,
      "learning_rate": 1e-05,
      "loss": 7.2123,
      "step": 10568
    },
    {
      "epoch": 0.5727913279132791,
      "step": 10568,
      "training_loss": 7.176701068878174
    },
    {
      "epoch": 0.5728455284552846,
      "step": 10569,
      "training_loss": 7.336175441741943
    },
    {
      "epoch": 0.57289972899729,
      "step": 10570,
      "training_loss": 5.254494667053223
    },
    {
      "epoch": 0.5729539295392954,
      "step": 10571,
      "training_loss": 7.570196628570557
    },
    {
      "epoch": 0.5730081300813008,
      "grad_norm": 45.330955505371094,
      "learning_rate": 1e-05,
      "loss": 6.8344,
      "step": 10572
    },
    {
      "epoch": 0.5730081300813008,
      "step": 10572,
      "training_loss": 5.216699123382568
    },
    {
      "epoch": 0.5730623306233062,
      "step": 10573,
      "training_loss": 6.560548305511475
    },
    {
      "epoch": 0.5731165311653117,
      "step": 10574,
      "training_loss": 6.4349846839904785
    },
    {
      "epoch": 0.573170731707317,
      "step": 10575,
      "training_loss": 5.823992729187012
    },
    {
      "epoch": 0.5732249322493225,
      "grad_norm": 36.7844352722168,
      "learning_rate": 1e-05,
      "loss": 6.0091,
      "step": 10576
    },
    {
      "epoch": 0.5732249322493225,
      "step": 10576,
      "training_loss": 5.716328144073486
    },
    {
      "epoch": 0.5732791327913279,
      "step": 10577,
      "training_loss": 7.599839210510254
    },
    {
      "epoch": 0.5733333333333334,
      "step": 10578,
      "training_loss": 7.059874534606934
    },
    {
      "epoch": 0.5733875338753388,
      "step": 10579,
      "training_loss": 6.523560523986816
    },
    {
      "epoch": 0.5734417344173441,
      "grad_norm": 33.5770263671875,
      "learning_rate": 1e-05,
      "loss": 6.7249,
      "step": 10580
    },
    {
      "epoch": 0.5734417344173441,
      "step": 10580,
      "training_loss": 6.647366523742676
    },
    {
      "epoch": 0.5734959349593496,
      "step": 10581,
      "training_loss": 5.824982166290283
    },
    {
      "epoch": 0.573550135501355,
      "step": 10582,
      "training_loss": 5.792875289916992
    },
    {
      "epoch": 0.5736043360433605,
      "step": 10583,
      "training_loss": 7.551311492919922
    },
    {
      "epoch": 0.5736585365853658,
      "grad_norm": 25.30124855041504,
      "learning_rate": 1e-05,
      "loss": 6.4541,
      "step": 10584
    },
    {
      "epoch": 0.5736585365853658,
      "step": 10584,
      "training_loss": 7.424960136413574
    },
    {
      "epoch": 0.5737127371273713,
      "step": 10585,
      "training_loss": 7.693682670593262
    },
    {
      "epoch": 0.5737669376693767,
      "step": 10586,
      "training_loss": 6.725932598114014
    },
    {
      "epoch": 0.5738211382113821,
      "step": 10587,
      "training_loss": 6.767826080322266
    },
    {
      "epoch": 0.5738753387533876,
      "grad_norm": 26.18610191345215,
      "learning_rate": 1e-05,
      "loss": 7.1531,
      "step": 10588
    },
    {
      "epoch": 0.5738753387533876,
      "step": 10588,
      "training_loss": 6.939011573791504
    },
    {
      "epoch": 0.5739295392953929,
      "step": 10589,
      "training_loss": 6.360716342926025
    },
    {
      "epoch": 0.5739837398373984,
      "step": 10590,
      "training_loss": 5.485673904418945
    },
    {
      "epoch": 0.5740379403794038,
      "step": 10591,
      "training_loss": 6.834218978881836
    },
    {
      "epoch": 0.5740921409214093,
      "grad_norm": 22.520095825195312,
      "learning_rate": 1e-05,
      "loss": 6.4049,
      "step": 10592
    },
    {
      "epoch": 0.5740921409214093,
      "step": 10592,
      "training_loss": 7.0736517906188965
    },
    {
      "epoch": 0.5741463414634146,
      "step": 10593,
      "training_loss": 6.569272041320801
    },
    {
      "epoch": 0.57420054200542,
      "step": 10594,
      "training_loss": 7.111419200897217
    },
    {
      "epoch": 0.5742547425474255,
      "step": 10595,
      "training_loss": 6.391386985778809
    },
    {
      "epoch": 0.5743089430894309,
      "grad_norm": 42.96262741088867,
      "learning_rate": 1e-05,
      "loss": 6.7864,
      "step": 10596
    },
    {
      "epoch": 0.5743089430894309,
      "step": 10596,
      "training_loss": 7.030332088470459
    },
    {
      "epoch": 0.5743631436314364,
      "step": 10597,
      "training_loss": 4.78100061416626
    },
    {
      "epoch": 0.5744173441734417,
      "step": 10598,
      "training_loss": 5.533414840698242
    },
    {
      "epoch": 0.5744715447154471,
      "step": 10599,
      "training_loss": 5.995053291320801
    },
    {
      "epoch": 0.5745257452574526,
      "grad_norm": 24.81258201599121,
      "learning_rate": 1e-05,
      "loss": 5.835,
      "step": 10600
    },
    {
      "epoch": 0.5745257452574526,
      "step": 10600,
      "training_loss": 5.860249996185303
    },
    {
      "epoch": 0.574579945799458,
      "step": 10601,
      "training_loss": 7.313304901123047
    },
    {
      "epoch": 0.5746341463414634,
      "step": 10602,
      "training_loss": 6.771667957305908
    },
    {
      "epoch": 0.5746883468834688,
      "step": 10603,
      "training_loss": 7.507078170776367
    },
    {
      "epoch": 0.5747425474254743,
      "grad_norm": 23.650869369506836,
      "learning_rate": 1e-05,
      "loss": 6.8631,
      "step": 10604
    },
    {
      "epoch": 0.5747425474254743,
      "step": 10604,
      "training_loss": 6.816234111785889
    },
    {
      "epoch": 0.5747967479674797,
      "step": 10605,
      "training_loss": 6.600253582000732
    },
    {
      "epoch": 0.5748509485094851,
      "step": 10606,
      "training_loss": 5.4933953285217285
    },
    {
      "epoch": 0.5749051490514905,
      "step": 10607,
      "training_loss": 7.080843448638916
    },
    {
      "epoch": 0.5749593495934959,
      "grad_norm": 18.050216674804688,
      "learning_rate": 1e-05,
      "loss": 6.4977,
      "step": 10608
    },
    {
      "epoch": 0.5749593495934959,
      "step": 10608,
      "training_loss": 6.964140892028809
    },
    {
      "epoch": 0.5750135501355014,
      "step": 10609,
      "training_loss": 7.287193775177002
    },
    {
      "epoch": 0.5750677506775068,
      "step": 10610,
      "training_loss": 7.242587089538574
    },
    {
      "epoch": 0.5751219512195122,
      "step": 10611,
      "training_loss": 6.203110218048096
    },
    {
      "epoch": 0.5751761517615176,
      "grad_norm": 17.725860595703125,
      "learning_rate": 1e-05,
      "loss": 6.9243,
      "step": 10612
    },
    {
      "epoch": 0.5751761517615176,
      "step": 10612,
      "training_loss": 3.426819324493408
    },
    {
      "epoch": 0.575230352303523,
      "step": 10613,
      "training_loss": 7.33378267288208
    },
    {
      "epoch": 0.5752845528455285,
      "step": 10614,
      "training_loss": 7.0966033935546875
    },
    {
      "epoch": 0.5753387533875339,
      "step": 10615,
      "training_loss": 7.10952615737915
    },
    {
      "epoch": 0.5753929539295393,
      "grad_norm": 31.506847381591797,
      "learning_rate": 1e-05,
      "loss": 6.2417,
      "step": 10616
    },
    {
      "epoch": 0.5753929539295393,
      "step": 10616,
      "training_loss": 5.876652240753174
    },
    {
      "epoch": 0.5754471544715447,
      "step": 10617,
      "training_loss": 8.180251121520996
    },
    {
      "epoch": 0.5755013550135502,
      "step": 10618,
      "training_loss": 7.973162651062012
    },
    {
      "epoch": 0.5755555555555556,
      "step": 10619,
      "training_loss": 7.34768533706665
    },
    {
      "epoch": 0.5756097560975609,
      "grad_norm": 32.46738052368164,
      "learning_rate": 1e-05,
      "loss": 7.3444,
      "step": 10620
    },
    {
      "epoch": 0.5756097560975609,
      "step": 10620,
      "training_loss": 6.551015853881836
    },
    {
      "epoch": 0.5756639566395664,
      "step": 10621,
      "training_loss": 8.177851676940918
    },
    {
      "epoch": 0.5757181571815718,
      "step": 10622,
      "training_loss": 4.532387733459473
    },
    {
      "epoch": 0.5757723577235773,
      "step": 10623,
      "training_loss": 7.63257360458374
    },
    {
      "epoch": 0.5758265582655827,
      "grad_norm": 43.52940368652344,
      "learning_rate": 1e-05,
      "loss": 6.7235,
      "step": 10624
    },
    {
      "epoch": 0.5758265582655827,
      "step": 10624,
      "training_loss": 6.77771520614624
    },
    {
      "epoch": 0.575880758807588,
      "step": 10625,
      "training_loss": 6.1318840980529785
    },
    {
      "epoch": 0.5759349593495935,
      "step": 10626,
      "training_loss": 8.183489799499512
    },
    {
      "epoch": 0.5759891598915989,
      "step": 10627,
      "training_loss": 7.72284460067749
    },
    {
      "epoch": 0.5760433604336044,
      "grad_norm": 19.815536499023438,
      "learning_rate": 1e-05,
      "loss": 7.204,
      "step": 10628
    },
    {
      "epoch": 0.5760433604336044,
      "step": 10628,
      "training_loss": 6.936280727386475
    },
    {
      "epoch": 0.5760975609756097,
      "step": 10629,
      "training_loss": 7.31749963760376
    },
    {
      "epoch": 0.5761517615176152,
      "step": 10630,
      "training_loss": 6.945127010345459
    },
    {
      "epoch": 0.5762059620596206,
      "step": 10631,
      "training_loss": 7.4888529777526855
    },
    {
      "epoch": 0.576260162601626,
      "grad_norm": 49.41292190551758,
      "learning_rate": 1e-05,
      "loss": 7.1719,
      "step": 10632
    },
    {
      "epoch": 0.576260162601626,
      "step": 10632,
      "training_loss": 6.840275287628174
    },
    {
      "epoch": 0.5763143631436315,
      "step": 10633,
      "training_loss": 4.870210647583008
    },
    {
      "epoch": 0.5763685636856368,
      "step": 10634,
      "training_loss": 7.106381416320801
    },
    {
      "epoch": 0.5764227642276423,
      "step": 10635,
      "training_loss": 6.958207130432129
    },
    {
      "epoch": 0.5764769647696477,
      "grad_norm": 27.040481567382812,
      "learning_rate": 1e-05,
      "loss": 6.4438,
      "step": 10636
    },
    {
      "epoch": 0.5764769647696477,
      "step": 10636,
      "training_loss": 5.539544582366943
    },
    {
      "epoch": 0.5765311653116532,
      "step": 10637,
      "training_loss": 7.76188850402832
    },
    {
      "epoch": 0.5765853658536585,
      "step": 10638,
      "training_loss": 6.657186031341553
    },
    {
      "epoch": 0.5766395663956639,
      "step": 10639,
      "training_loss": 7.9906721115112305
    },
    {
      "epoch": 0.5766937669376694,
      "grad_norm": 22.708715438842773,
      "learning_rate": 1e-05,
      "loss": 6.9873,
      "step": 10640
    },
    {
      "epoch": 0.5766937669376694,
      "step": 10640,
      "training_loss": 6.829059600830078
    },
    {
      "epoch": 0.5767479674796748,
      "step": 10641,
      "training_loss": 7.424344539642334
    },
    {
      "epoch": 0.5768021680216802,
      "step": 10642,
      "training_loss": 4.163827896118164
    },
    {
      "epoch": 0.5768563685636856,
      "step": 10643,
      "training_loss": 6.828400135040283
    },
    {
      "epoch": 0.576910569105691,
      "grad_norm": 19.8441162109375,
      "learning_rate": 1e-05,
      "loss": 6.3114,
      "step": 10644
    },
    {
      "epoch": 0.576910569105691,
      "step": 10644,
      "training_loss": 7.311088562011719
    },
    {
      "epoch": 0.5769647696476965,
      "step": 10645,
      "training_loss": 4.874318599700928
    },
    {
      "epoch": 0.5770189701897019,
      "step": 10646,
      "training_loss": 4.507870197296143
    },
    {
      "epoch": 0.5770731707317073,
      "step": 10647,
      "training_loss": 6.579845428466797
    },
    {
      "epoch": 0.5771273712737127,
      "grad_norm": 32.448486328125,
      "learning_rate": 1e-05,
      "loss": 5.8183,
      "step": 10648
    },
    {
      "epoch": 0.5771273712737127,
      "step": 10648,
      "training_loss": 6.6008172035217285
    },
    {
      "epoch": 0.5771815718157182,
      "step": 10649,
      "training_loss": 6.8361358642578125
    },
    {
      "epoch": 0.5772357723577236,
      "step": 10650,
      "training_loss": 7.288567066192627
    },
    {
      "epoch": 0.5772899728997289,
      "step": 10651,
      "training_loss": 6.086943626403809
    },
    {
      "epoch": 0.5773441734417344,
      "grad_norm": 50.081077575683594,
      "learning_rate": 1e-05,
      "loss": 6.7031,
      "step": 10652
    },
    {
      "epoch": 0.5773441734417344,
      "step": 10652,
      "training_loss": 6.642703056335449
    },
    {
      "epoch": 0.5773983739837398,
      "step": 10653,
      "training_loss": 7.738502502441406
    },
    {
      "epoch": 0.5774525745257453,
      "step": 10654,
      "training_loss": 7.410053730010986
    },
    {
      "epoch": 0.5775067750677507,
      "step": 10655,
      "training_loss": 8.437114715576172
    },
    {
      "epoch": 0.577560975609756,
      "grad_norm": 34.47589111328125,
      "learning_rate": 1e-05,
      "loss": 7.5571,
      "step": 10656
    },
    {
      "epoch": 0.577560975609756,
      "step": 10656,
      "training_loss": 6.24958610534668
    },
    {
      "epoch": 0.5776151761517615,
      "step": 10657,
      "training_loss": 6.79618501663208
    },
    {
      "epoch": 0.577669376693767,
      "step": 10658,
      "training_loss": 8.561102867126465
    },
    {
      "epoch": 0.5777235772357724,
      "step": 10659,
      "training_loss": 3.384182929992676
    },
    {
      "epoch": 0.5777777777777777,
      "grad_norm": 38.68779754638672,
      "learning_rate": 1e-05,
      "loss": 6.2478,
      "step": 10660
    },
    {
      "epoch": 0.5777777777777777,
      "step": 10660,
      "training_loss": 6.243158340454102
    },
    {
      "epoch": 0.5778319783197832,
      "step": 10661,
      "training_loss": 6.7504377365112305
    },
    {
      "epoch": 0.5778861788617886,
      "step": 10662,
      "training_loss": 9.509748458862305
    },
    {
      "epoch": 0.5779403794037941,
      "step": 10663,
      "training_loss": 6.807366371154785
    },
    {
      "epoch": 0.5779945799457995,
      "grad_norm": 20.623502731323242,
      "learning_rate": 1e-05,
      "loss": 7.3277,
      "step": 10664
    },
    {
      "epoch": 0.5779945799457995,
      "step": 10664,
      "training_loss": 7.638394355773926
    },
    {
      "epoch": 0.5780487804878048,
      "step": 10665,
      "training_loss": 5.109130859375
    },
    {
      "epoch": 0.5781029810298103,
      "step": 10666,
      "training_loss": 7.828357696533203
    },
    {
      "epoch": 0.5781571815718157,
      "step": 10667,
      "training_loss": 7.52353572845459
    },
    {
      "epoch": 0.5782113821138212,
      "grad_norm": 22.122730255126953,
      "learning_rate": 1e-05,
      "loss": 7.0249,
      "step": 10668
    },
    {
      "epoch": 0.5782113821138212,
      "step": 10668,
      "training_loss": 7.150613307952881
    },
    {
      "epoch": 0.5782655826558265,
      "step": 10669,
      "training_loss": 6.507049083709717
    },
    {
      "epoch": 0.578319783197832,
      "step": 10670,
      "training_loss": 5.935432434082031
    },
    {
      "epoch": 0.5783739837398374,
      "step": 10671,
      "training_loss": 6.691883563995361
    },
    {
      "epoch": 0.5784281842818428,
      "grad_norm": 24.03052520751953,
      "learning_rate": 1e-05,
      "loss": 6.5712,
      "step": 10672
    },
    {
      "epoch": 0.5784281842818428,
      "step": 10672,
      "training_loss": 7.552915573120117
    },
    {
      "epoch": 0.5784823848238483,
      "step": 10673,
      "training_loss": 6.6719584465026855
    },
    {
      "epoch": 0.5785365853658536,
      "step": 10674,
      "training_loss": 6.330356121063232
    },
    {
      "epoch": 0.5785907859078591,
      "step": 10675,
      "training_loss": 7.782793998718262
    },
    {
      "epoch": 0.5786449864498645,
      "grad_norm": 57.11579895019531,
      "learning_rate": 1e-05,
      "loss": 7.0845,
      "step": 10676
    },
    {
      "epoch": 0.5786449864498645,
      "step": 10676,
      "training_loss": 5.922139644622803
    },
    {
      "epoch": 0.57869918699187,
      "step": 10677,
      "training_loss": 7.121176719665527
    },
    {
      "epoch": 0.5787533875338753,
      "step": 10678,
      "training_loss": 7.65575647354126
    },
    {
      "epoch": 0.5788075880758807,
      "step": 10679,
      "training_loss": 7.041973114013672
    },
    {
      "epoch": 0.5788617886178862,
      "grad_norm": 30.661161422729492,
      "learning_rate": 1e-05,
      "loss": 6.9353,
      "step": 10680
    },
    {
      "epoch": 0.5788617886178862,
      "step": 10680,
      "training_loss": 6.32002592086792
    },
    {
      "epoch": 0.5789159891598916,
      "step": 10681,
      "training_loss": 6.730837345123291
    },
    {
      "epoch": 0.5789701897018971,
      "step": 10682,
      "training_loss": 5.292316436767578
    },
    {
      "epoch": 0.5790243902439024,
      "step": 10683,
      "training_loss": 5.24578857421875
    },
    {
      "epoch": 0.5790785907859078,
      "grad_norm": 31.41204833984375,
      "learning_rate": 1e-05,
      "loss": 5.8972,
      "step": 10684
    },
    {
      "epoch": 0.5790785907859078,
      "step": 10684,
      "training_loss": 7.303308010101318
    },
    {
      "epoch": 0.5791327913279133,
      "step": 10685,
      "training_loss": 8.017271041870117
    },
    {
      "epoch": 0.5791869918699187,
      "step": 10686,
      "training_loss": 7.230189800262451
    },
    {
      "epoch": 0.5792411924119241,
      "step": 10687,
      "training_loss": 5.121545314788818
    },
    {
      "epoch": 0.5792953929539295,
      "grad_norm": 32.54673385620117,
      "learning_rate": 1e-05,
      "loss": 6.9181,
      "step": 10688
    },
    {
      "epoch": 0.5792953929539295,
      "step": 10688,
      "training_loss": 3.1690587997436523
    },
    {
      "epoch": 0.579349593495935,
      "step": 10689,
      "training_loss": 6.8542680740356445
    },
    {
      "epoch": 0.5794037940379404,
      "step": 10690,
      "training_loss": 7.94221305847168
    },
    {
      "epoch": 0.5794579945799458,
      "step": 10691,
      "training_loss": 6.747151851654053
    },
    {
      "epoch": 0.5795121951219512,
      "grad_norm": 24.096202850341797,
      "learning_rate": 1e-05,
      "loss": 6.1782,
      "step": 10692
    },
    {
      "epoch": 0.5795121951219512,
      "step": 10692,
      "training_loss": 6.873372554779053
    },
    {
      "epoch": 0.5795663956639566,
      "step": 10693,
      "training_loss": 7.672433376312256
    },
    {
      "epoch": 0.5796205962059621,
      "step": 10694,
      "training_loss": 5.493031024932861
    },
    {
      "epoch": 0.5796747967479675,
      "step": 10695,
      "training_loss": 6.084115505218506
    },
    {
      "epoch": 0.5797289972899728,
      "grad_norm": 22.863571166992188,
      "learning_rate": 1e-05,
      "loss": 6.5307,
      "step": 10696
    },
    {
      "epoch": 0.5797289972899728,
      "step": 10696,
      "training_loss": 7.586788654327393
    },
    {
      "epoch": 0.5797831978319783,
      "step": 10697,
      "training_loss": 6.478992462158203
    },
    {
      "epoch": 0.5798373983739837,
      "step": 10698,
      "training_loss": 7.263560771942139
    },
    {
      "epoch": 0.5798915989159892,
      "step": 10699,
      "training_loss": 7.127529621124268
    },
    {
      "epoch": 0.5799457994579946,
      "grad_norm": 39.41047668457031,
      "learning_rate": 1e-05,
      "loss": 7.1142,
      "step": 10700
    },
    {
      "epoch": 0.5799457994579946,
      "step": 10700,
      "training_loss": 6.615542888641357
    },
    {
      "epoch": 0.58,
      "step": 10701,
      "training_loss": 8.019083023071289
    },
    {
      "epoch": 0.5800542005420054,
      "step": 10702,
      "training_loss": 5.787032127380371
    },
    {
      "epoch": 0.5801084010840108,
      "step": 10703,
      "training_loss": 6.420877933502197
    },
    {
      "epoch": 0.5801626016260163,
      "grad_norm": 33.18110275268555,
      "learning_rate": 1e-05,
      "loss": 6.7106,
      "step": 10704
    },
    {
      "epoch": 0.5801626016260163,
      "step": 10704,
      "training_loss": 3.395906686782837
    },
    {
      "epoch": 0.5802168021680216,
      "step": 10705,
      "training_loss": 7.172216892242432
    },
    {
      "epoch": 0.5802710027100271,
      "step": 10706,
      "training_loss": 6.948159694671631
    },
    {
      "epoch": 0.5803252032520325,
      "step": 10707,
      "training_loss": 7.189533233642578
    },
    {
      "epoch": 0.580379403794038,
      "grad_norm": 23.817886352539062,
      "learning_rate": 1e-05,
      "loss": 6.1765,
      "step": 10708
    },
    {
      "epoch": 0.580379403794038,
      "step": 10708,
      "training_loss": 6.645956516265869
    },
    {
      "epoch": 0.5804336043360434,
      "step": 10709,
      "training_loss": 6.893399238586426
    },
    {
      "epoch": 0.5804878048780487,
      "step": 10710,
      "training_loss": 6.814138889312744
    },
    {
      "epoch": 0.5805420054200542,
      "step": 10711,
      "training_loss": 5.252736568450928
    },
    {
      "epoch": 0.5805962059620596,
      "grad_norm": 30.222328186035156,
      "learning_rate": 1e-05,
      "loss": 6.4016,
      "step": 10712
    },
    {
      "epoch": 0.5805962059620596,
      "step": 10712,
      "training_loss": 6.563167572021484
    },
    {
      "epoch": 0.5806504065040651,
      "step": 10713,
      "training_loss": 5.328951835632324
    },
    {
      "epoch": 0.5807046070460704,
      "step": 10714,
      "training_loss": 6.6717000007629395
    },
    {
      "epoch": 0.5807588075880759,
      "step": 10715,
      "training_loss": 3.4001104831695557
    },
    {
      "epoch": 0.5808130081300813,
      "grad_norm": 34.13042068481445,
      "learning_rate": 1e-05,
      "loss": 5.491,
      "step": 10716
    },
    {
      "epoch": 0.5808130081300813,
      "step": 10716,
      "training_loss": 6.79336404800415
    },
    {
      "epoch": 0.5808672086720867,
      "step": 10717,
      "training_loss": 6.952476501464844
    },
    {
      "epoch": 0.5809214092140922,
      "step": 10718,
      "training_loss": 7.824494361877441
    },
    {
      "epoch": 0.5809756097560975,
      "step": 10719,
      "training_loss": 7.143118381500244
    },
    {
      "epoch": 0.581029810298103,
      "grad_norm": 19.207599639892578,
      "learning_rate": 1e-05,
      "loss": 7.1784,
      "step": 10720
    },
    {
      "epoch": 0.581029810298103,
      "step": 10720,
      "training_loss": 6.89040470123291
    },
    {
      "epoch": 0.5810840108401084,
      "step": 10721,
      "training_loss": 8.750863075256348
    },
    {
      "epoch": 0.5811382113821139,
      "step": 10722,
      "training_loss": 3.7774901390075684
    },
    {
      "epoch": 0.5811924119241192,
      "step": 10723,
      "training_loss": 6.755903244018555
    },
    {
      "epoch": 0.5812466124661246,
      "grad_norm": 24.326583862304688,
      "learning_rate": 1e-05,
      "loss": 6.5437,
      "step": 10724
    },
    {
      "epoch": 0.5812466124661246,
      "step": 10724,
      "training_loss": 6.888076305389404
    },
    {
      "epoch": 0.5813008130081301,
      "step": 10725,
      "training_loss": 5.851171493530273
    },
    {
      "epoch": 0.5813550135501355,
      "step": 10726,
      "training_loss": 7.104116439819336
    },
    {
      "epoch": 0.581409214092141,
      "step": 10727,
      "training_loss": 9.426363945007324
    },
    {
      "epoch": 0.5814634146341463,
      "grad_norm": 66.2968521118164,
      "learning_rate": 1e-05,
      "loss": 7.3174,
      "step": 10728
    },
    {
      "epoch": 0.5814634146341463,
      "step": 10728,
      "training_loss": 7.729812145233154
    },
    {
      "epoch": 0.5815176151761517,
      "step": 10729,
      "training_loss": 3.5189573764801025
    },
    {
      "epoch": 0.5815718157181572,
      "step": 10730,
      "training_loss": 7.089015483856201
    },
    {
      "epoch": 0.5816260162601626,
      "step": 10731,
      "training_loss": 7.926569938659668
    },
    {
      "epoch": 0.581680216802168,
      "grad_norm": 38.33790588378906,
      "learning_rate": 1e-05,
      "loss": 6.5661,
      "step": 10732
    },
    {
      "epoch": 0.581680216802168,
      "step": 10732,
      "training_loss": 5.506194591522217
    },
    {
      "epoch": 0.5817344173441734,
      "step": 10733,
      "training_loss": 5.781031131744385
    },
    {
      "epoch": 0.5817886178861789,
      "step": 10734,
      "training_loss": 5.881262302398682
    },
    {
      "epoch": 0.5818428184281843,
      "step": 10735,
      "training_loss": 6.821425437927246
    },
    {
      "epoch": 0.5818970189701897,
      "grad_norm": 28.07146644592285,
      "learning_rate": 1e-05,
      "loss": 5.9975,
      "step": 10736
    },
    {
      "epoch": 0.5818970189701897,
      "step": 10736,
      "training_loss": 5.791848659515381
    },
    {
      "epoch": 0.5819512195121951,
      "step": 10737,
      "training_loss": 6.648776531219482
    },
    {
      "epoch": 0.5820054200542005,
      "step": 10738,
      "training_loss": 8.215598106384277
    },
    {
      "epoch": 0.582059620596206,
      "step": 10739,
      "training_loss": 6.210207939147949
    },
    {
      "epoch": 0.5821138211382114,
      "grad_norm": 50.478111267089844,
      "learning_rate": 1e-05,
      "loss": 6.7166,
      "step": 10740
    },
    {
      "epoch": 0.5821138211382114,
      "step": 10740,
      "training_loss": 5.432884693145752
    },
    {
      "epoch": 0.5821680216802168,
      "step": 10741,
      "training_loss": 6.6900739669799805
    },
    {
      "epoch": 0.5822222222222222,
      "step": 10742,
      "training_loss": 5.338370323181152
    },
    {
      "epoch": 0.5822764227642276,
      "step": 10743,
      "training_loss": 5.872139930725098
    },
    {
      "epoch": 0.5823306233062331,
      "grad_norm": 27.67526626586914,
      "learning_rate": 1e-05,
      "loss": 5.8334,
      "step": 10744
    },
    {
      "epoch": 0.5823306233062331,
      "step": 10744,
      "training_loss": 5.314620018005371
    },
    {
      "epoch": 0.5823848238482385,
      "step": 10745,
      "training_loss": 4.881579875946045
    },
    {
      "epoch": 0.5824390243902439,
      "step": 10746,
      "training_loss": 4.5221052169799805
    },
    {
      "epoch": 0.5824932249322493,
      "step": 10747,
      "training_loss": 7.036464214324951
    },
    {
      "epoch": 0.5825474254742548,
      "grad_norm": 27.047433853149414,
      "learning_rate": 1e-05,
      "loss": 5.4387,
      "step": 10748
    },
    {
      "epoch": 0.5825474254742548,
      "step": 10748,
      "training_loss": 7.2198381423950195
    },
    {
      "epoch": 0.5826016260162602,
      "step": 10749,
      "training_loss": 4.533426284790039
    },
    {
      "epoch": 0.5826558265582655,
      "step": 10750,
      "training_loss": 7.0246477127075195
    },
    {
      "epoch": 0.582710027100271,
      "step": 10751,
      "training_loss": 5.7280755043029785
    },
    {
      "epoch": 0.5827642276422764,
      "grad_norm": 26.241666793823242,
      "learning_rate": 1e-05,
      "loss": 6.1265,
      "step": 10752
    },
    {
      "epoch": 0.5827642276422764,
      "step": 10752,
      "training_loss": 8.094752311706543
    },
    {
      "epoch": 0.5828184281842819,
      "step": 10753,
      "training_loss": 6.278922080993652
    },
    {
      "epoch": 0.5828726287262873,
      "step": 10754,
      "training_loss": 7.595825672149658
    },
    {
      "epoch": 0.5829268292682926,
      "step": 10755,
      "training_loss": 4.088876247406006
    },
    {
      "epoch": 0.5829810298102981,
      "grad_norm": 37.85654830932617,
      "learning_rate": 1e-05,
      "loss": 6.5146,
      "step": 10756
    },
    {
      "epoch": 0.5829810298102981,
      "step": 10756,
      "training_loss": 5.554528713226318
    },
    {
      "epoch": 0.5830352303523035,
      "step": 10757,
      "training_loss": 8.47893238067627
    },
    {
      "epoch": 0.583089430894309,
      "step": 10758,
      "training_loss": 7.561848163604736
    },
    {
      "epoch": 0.5831436314363143,
      "step": 10759,
      "training_loss": 7.4334211349487305
    },
    {
      "epoch": 0.5831978319783198,
      "grad_norm": 41.7794075012207,
      "learning_rate": 1e-05,
      "loss": 7.2572,
      "step": 10760
    },
    {
      "epoch": 0.5831978319783198,
      "step": 10760,
      "training_loss": 6.936636447906494
    },
    {
      "epoch": 0.5832520325203252,
      "step": 10761,
      "training_loss": 6.9557390213012695
    },
    {
      "epoch": 0.5833062330623306,
      "step": 10762,
      "training_loss": 7.106219291687012
    },
    {
      "epoch": 0.5833604336043361,
      "step": 10763,
      "training_loss": 6.178569793701172
    },
    {
      "epoch": 0.5834146341463414,
      "grad_norm": 35.035484313964844,
      "learning_rate": 1e-05,
      "loss": 6.7943,
      "step": 10764
    },
    {
      "epoch": 0.5834146341463414,
      "step": 10764,
      "training_loss": 5.497514724731445
    },
    {
      "epoch": 0.5834688346883469,
      "step": 10765,
      "training_loss": 7.275423049926758
    },
    {
      "epoch": 0.5835230352303523,
      "step": 10766,
      "training_loss": 7.254736423492432
    },
    {
      "epoch": 0.5835772357723578,
      "step": 10767,
      "training_loss": 6.762832164764404
    },
    {
      "epoch": 0.5836314363143631,
      "grad_norm": 28.320152282714844,
      "learning_rate": 1e-05,
      "loss": 6.6976,
      "step": 10768
    },
    {
      "epoch": 0.5836314363143631,
      "step": 10768,
      "training_loss": 6.5917558670043945
    },
    {
      "epoch": 0.5836856368563685,
      "step": 10769,
      "training_loss": 5.617501258850098
    },
    {
      "epoch": 0.583739837398374,
      "step": 10770,
      "training_loss": 7.910782814025879
    },
    {
      "epoch": 0.5837940379403794,
      "step": 10771,
      "training_loss": 5.9974470138549805
    },
    {
      "epoch": 0.5838482384823849,
      "grad_norm": 21.32032585144043,
      "learning_rate": 1e-05,
      "loss": 6.5294,
      "step": 10772
    },
    {
      "epoch": 0.5838482384823849,
      "step": 10772,
      "training_loss": 6.191165447235107
    },
    {
      "epoch": 0.5839024390243902,
      "step": 10773,
      "training_loss": 7.189765453338623
    },
    {
      "epoch": 0.5839566395663957,
      "step": 10774,
      "training_loss": 5.528679370880127
    },
    {
      "epoch": 0.5840108401084011,
      "step": 10775,
      "training_loss": 7.746912956237793
    },
    {
      "epoch": 0.5840650406504065,
      "grad_norm": 38.54292678833008,
      "learning_rate": 1e-05,
      "loss": 6.6641,
      "step": 10776
    },
    {
      "epoch": 0.5840650406504065,
      "step": 10776,
      "training_loss": 6.296053409576416
    },
    {
      "epoch": 0.5841192411924119,
      "step": 10777,
      "training_loss": 6.402445316314697
    },
    {
      "epoch": 0.5841734417344173,
      "step": 10778,
      "training_loss": 6.0141801834106445
    },
    {
      "epoch": 0.5842276422764228,
      "step": 10779,
      "training_loss": 6.684527397155762
    },
    {
      "epoch": 0.5842818428184282,
      "grad_norm": 23.75824737548828,
      "learning_rate": 1e-05,
      "loss": 6.3493,
      "step": 10780
    },
    {
      "epoch": 0.5842818428184282,
      "step": 10780,
      "training_loss": 7.14809513092041
    },
    {
      "epoch": 0.5843360433604337,
      "step": 10781,
      "training_loss": 6.241199493408203
    },
    {
      "epoch": 0.584390243902439,
      "step": 10782,
      "training_loss": 6.945287227630615
    },
    {
      "epoch": 0.5844444444444444,
      "step": 10783,
      "training_loss": 5.626240253448486
    },
    {
      "epoch": 0.5844986449864499,
      "grad_norm": 45.01643753051758,
      "learning_rate": 1e-05,
      "loss": 6.4902,
      "step": 10784
    },
    {
      "epoch": 0.5844986449864499,
      "step": 10784,
      "training_loss": 6.398335933685303
    },
    {
      "epoch": 0.5845528455284553,
      "step": 10785,
      "training_loss": 7.155322551727295
    },
    {
      "epoch": 0.5846070460704607,
      "step": 10786,
      "training_loss": 7.956261157989502
    },
    {
      "epoch": 0.5846612466124661,
      "step": 10787,
      "training_loss": 7.964447975158691
    },
    {
      "epoch": 0.5847154471544715,
      "grad_norm": 32.76669692993164,
      "learning_rate": 1e-05,
      "loss": 7.3686,
      "step": 10788
    },
    {
      "epoch": 0.5847154471544715,
      "step": 10788,
      "training_loss": 5.176779747009277
    },
    {
      "epoch": 0.584769647696477,
      "step": 10789,
      "training_loss": 6.98179817199707
    },
    {
      "epoch": 0.5848238482384824,
      "step": 10790,
      "training_loss": 6.0992817878723145
    },
    {
      "epoch": 0.5848780487804878,
      "step": 10791,
      "training_loss": 8.20240306854248
    },
    {
      "epoch": 0.5849322493224932,
      "grad_norm": 18.977731704711914,
      "learning_rate": 1e-05,
      "loss": 6.6151,
      "step": 10792
    },
    {
      "epoch": 0.5849322493224932,
      "step": 10792,
      "training_loss": 8.350988388061523
    },
    {
      "epoch": 0.5849864498644987,
      "step": 10793,
      "training_loss": 5.289259910583496
    },
    {
      "epoch": 0.5850406504065041,
      "step": 10794,
      "training_loss": 6.769769191741943
    },
    {
      "epoch": 0.5850948509485094,
      "step": 10795,
      "training_loss": 4.688404560089111
    },
    {
      "epoch": 0.5851490514905149,
      "grad_norm": 38.34771728515625,
      "learning_rate": 1e-05,
      "loss": 6.2746,
      "step": 10796
    },
    {
      "epoch": 0.5851490514905149,
      "step": 10796,
      "training_loss": 6.963413715362549
    },
    {
      "epoch": 0.5852032520325203,
      "step": 10797,
      "training_loss": 5.193933010101318
    },
    {
      "epoch": 0.5852574525745258,
      "step": 10798,
      "training_loss": 5.595315456390381
    },
    {
      "epoch": 0.5853116531165312,
      "step": 10799,
      "training_loss": 6.668007850646973
    },
    {
      "epoch": 0.5853658536585366,
      "grad_norm": 29.41156005859375,
      "learning_rate": 1e-05,
      "loss": 6.1052,
      "step": 10800
    },
    {
      "epoch": 0.5853658536585366,
      "step": 10800,
      "training_loss": 3.9791009426116943
    },
    {
      "epoch": 0.585420054200542,
      "step": 10801,
      "training_loss": 6.788079738616943
    },
    {
      "epoch": 0.5854742547425474,
      "step": 10802,
      "training_loss": 7.332408428192139
    },
    {
      "epoch": 0.5855284552845529,
      "step": 10803,
      "training_loss": 7.771402359008789
    },
    {
      "epoch": 0.5855826558265582,
      "grad_norm": 35.53688049316406,
      "learning_rate": 1e-05,
      "loss": 6.4677,
      "step": 10804
    },
    {
      "epoch": 0.5855826558265582,
      "step": 10804,
      "training_loss": 4.527338981628418
    },
    {
      "epoch": 0.5856368563685637,
      "step": 10805,
      "training_loss": 7.055393695831299
    },
    {
      "epoch": 0.5856910569105691,
      "step": 10806,
      "training_loss": 5.661281585693359
    },
    {
      "epoch": 0.5857452574525746,
      "step": 10807,
      "training_loss": 6.472621440887451
    },
    {
      "epoch": 0.58579945799458,
      "grad_norm": 23.06268882751465,
      "learning_rate": 1e-05,
      "loss": 5.9292,
      "step": 10808
    },
    {
      "epoch": 0.58579945799458,
      "step": 10808,
      "training_loss": 7.377439022064209
    },
    {
      "epoch": 0.5858536585365853,
      "step": 10809,
      "training_loss": 6.953929901123047
    },
    {
      "epoch": 0.5859078590785908,
      "step": 10810,
      "training_loss": 7.758964538574219
    },
    {
      "epoch": 0.5859620596205962,
      "step": 10811,
      "training_loss": 5.9151763916015625
    },
    {
      "epoch": 0.5860162601626017,
      "grad_norm": 23.693817138671875,
      "learning_rate": 1e-05,
      "loss": 7.0014,
      "step": 10812
    },
    {
      "epoch": 0.5860162601626017,
      "step": 10812,
      "training_loss": 7.025575637817383
    },
    {
      "epoch": 0.586070460704607,
      "step": 10813,
      "training_loss": 7.075145721435547
    },
    {
      "epoch": 0.5861246612466124,
      "step": 10814,
      "training_loss": 6.905951023101807
    },
    {
      "epoch": 0.5861788617886179,
      "step": 10815,
      "training_loss": 7.1766862869262695
    },
    {
      "epoch": 0.5862330623306233,
      "grad_norm": 21.635770797729492,
      "learning_rate": 1e-05,
      "loss": 7.0458,
      "step": 10816
    },
    {
      "epoch": 0.5862330623306233,
      "step": 10816,
      "training_loss": 7.380486011505127
    },
    {
      "epoch": 0.5862872628726288,
      "step": 10817,
      "training_loss": 6.919126987457275
    },
    {
      "epoch": 0.5863414634146341,
      "step": 10818,
      "training_loss": 4.435816764831543
    },
    {
      "epoch": 0.5863956639566396,
      "step": 10819,
      "training_loss": 7.17893123626709
    },
    {
      "epoch": 0.586449864498645,
      "grad_norm": 18.837255477905273,
      "learning_rate": 1e-05,
      "loss": 6.4786,
      "step": 10820
    },
    {
      "epoch": 0.586449864498645,
      "step": 10820,
      "training_loss": 7.522952556610107
    },
    {
      "epoch": 0.5865040650406504,
      "step": 10821,
      "training_loss": 6.437590599060059
    },
    {
      "epoch": 0.5865582655826558,
      "step": 10822,
      "training_loss": 6.453891277313232
    },
    {
      "epoch": 0.5866124661246612,
      "step": 10823,
      "training_loss": 7.8945183753967285
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 25.07820701599121,
      "learning_rate": 1e-05,
      "loss": 7.0772,
      "step": 10824
    },
    {
      "epoch": 0.5866666666666667,
      "step": 10824,
      "training_loss": 6.708160877227783
    },
    {
      "epoch": 0.5867208672086721,
      "step": 10825,
      "training_loss": 6.499424457550049
    },
    {
      "epoch": 0.5867750677506776,
      "step": 10826,
      "training_loss": 6.619016647338867
    },
    {
      "epoch": 0.5868292682926829,
      "step": 10827,
      "training_loss": 6.592101573944092
    },
    {
      "epoch": 0.5868834688346883,
      "grad_norm": 25.250526428222656,
      "learning_rate": 1e-05,
      "loss": 6.6047,
      "step": 10828
    },
    {
      "epoch": 0.5868834688346883,
      "step": 10828,
      "training_loss": 5.591179847717285
    },
    {
      "epoch": 0.5869376693766938,
      "step": 10829,
      "training_loss": 7.40268611907959
    },
    {
      "epoch": 0.5869918699186992,
      "step": 10830,
      "training_loss": 6.768598556518555
    },
    {
      "epoch": 0.5870460704607046,
      "step": 10831,
      "training_loss": 6.797879695892334
    },
    {
      "epoch": 0.58710027100271,
      "grad_norm": 15.76722526550293,
      "learning_rate": 1e-05,
      "loss": 6.6401,
      "step": 10832
    },
    {
      "epoch": 0.58710027100271,
      "step": 10832,
      "training_loss": 7.415195941925049
    },
    {
      "epoch": 0.5871544715447155,
      "step": 10833,
      "training_loss": 6.868375301361084
    },
    {
      "epoch": 0.5872086720867209,
      "step": 10834,
      "training_loss": 6.6106791496276855
    },
    {
      "epoch": 0.5872628726287263,
      "step": 10835,
      "training_loss": 6.313642978668213
    },
    {
      "epoch": 0.5873170731707317,
      "grad_norm": 25.991504669189453,
      "learning_rate": 1e-05,
      "loss": 6.802,
      "step": 10836
    },
    {
      "epoch": 0.5873170731707317,
      "step": 10836,
      "training_loss": 7.378246307373047
    },
    {
      "epoch": 0.5873712737127371,
      "step": 10837,
      "training_loss": 7.908206939697266
    },
    {
      "epoch": 0.5874254742547426,
      "step": 10838,
      "training_loss": 6.948360443115234
    },
    {
      "epoch": 0.587479674796748,
      "step": 10839,
      "training_loss": 5.744277477264404
    },
    {
      "epoch": 0.5875338753387533,
      "grad_norm": 37.295936584472656,
      "learning_rate": 1e-05,
      "loss": 6.9948,
      "step": 10840
    },
    {
      "epoch": 0.5875338753387533,
      "step": 10840,
      "training_loss": 6.242033004760742
    },
    {
      "epoch": 0.5875880758807588,
      "step": 10841,
      "training_loss": 6.12424898147583
    },
    {
      "epoch": 0.5876422764227642,
      "step": 10842,
      "training_loss": 6.988702774047852
    },
    {
      "epoch": 0.5876964769647697,
      "step": 10843,
      "training_loss": 5.298373222351074
    },
    {
      "epoch": 0.5877506775067751,
      "grad_norm": 26.0887451171875,
      "learning_rate": 1e-05,
      "loss": 6.1633,
      "step": 10844
    },
    {
      "epoch": 0.5877506775067751,
      "step": 10844,
      "training_loss": 7.762077808380127
    },
    {
      "epoch": 0.5878048780487805,
      "step": 10845,
      "training_loss": 6.441713333129883
    },
    {
      "epoch": 0.5878590785907859,
      "step": 10846,
      "training_loss": 5.997550964355469
    },
    {
      "epoch": 0.5879132791327913,
      "step": 10847,
      "training_loss": 6.809454917907715
    },
    {
      "epoch": 0.5879674796747968,
      "grad_norm": 28.445541381835938,
      "learning_rate": 1e-05,
      "loss": 6.7527,
      "step": 10848
    },
    {
      "epoch": 0.5879674796747968,
      "step": 10848,
      "training_loss": 6.041167736053467
    },
    {
      "epoch": 0.5880216802168021,
      "step": 10849,
      "training_loss": 6.331148624420166
    },
    {
      "epoch": 0.5880758807588076,
      "step": 10850,
      "training_loss": 6.954871654510498
    },
    {
      "epoch": 0.588130081300813,
      "step": 10851,
      "training_loss": 7.082447052001953
    },
    {
      "epoch": 0.5881842818428185,
      "grad_norm": 20.744476318359375,
      "learning_rate": 1e-05,
      "loss": 6.6024,
      "step": 10852
    },
    {
      "epoch": 0.5881842818428185,
      "step": 10852,
      "training_loss": 3.7333059310913086
    },
    {
      "epoch": 0.5882384823848239,
      "step": 10853,
      "training_loss": 6.351018905639648
    },
    {
      "epoch": 0.5882926829268292,
      "step": 10854,
      "training_loss": 6.401708602905273
    },
    {
      "epoch": 0.5883468834688347,
      "step": 10855,
      "training_loss": 6.0004754066467285
    },
    {
      "epoch": 0.5884010840108401,
      "grad_norm": 21.86050033569336,
      "learning_rate": 1e-05,
      "loss": 5.6216,
      "step": 10856
    },
    {
      "epoch": 0.5884010840108401,
      "step": 10856,
      "training_loss": 6.679627895355225
    },
    {
      "epoch": 0.5884552845528456,
      "step": 10857,
      "training_loss": 7.575232982635498
    },
    {
      "epoch": 0.5885094850948509,
      "step": 10858,
      "training_loss": 6.357011318206787
    },
    {
      "epoch": 0.5885636856368563,
      "step": 10859,
      "training_loss": 7.600066184997559
    },
    {
      "epoch": 0.5886178861788618,
      "grad_norm": 24.37816619873047,
      "learning_rate": 1e-05,
      "loss": 7.053,
      "step": 10860
    },
    {
      "epoch": 0.5886178861788618,
      "step": 10860,
      "training_loss": 8.496696472167969
    },
    {
      "epoch": 0.5886720867208672,
      "step": 10861,
      "training_loss": 6.611633777618408
    },
    {
      "epoch": 0.5887262872628727,
      "step": 10862,
      "training_loss": 5.805522441864014
    },
    {
      "epoch": 0.588780487804878,
      "step": 10863,
      "training_loss": 8.25708293914795
    },
    {
      "epoch": 0.5888346883468835,
      "grad_norm": 39.84707260131836,
      "learning_rate": 1e-05,
      "loss": 7.2927,
      "step": 10864
    },
    {
      "epoch": 0.5888346883468835,
      "step": 10864,
      "training_loss": 6.600860118865967
    },
    {
      "epoch": 0.5888888888888889,
      "step": 10865,
      "training_loss": 6.194369792938232
    },
    {
      "epoch": 0.5889430894308944,
      "step": 10866,
      "training_loss": 5.367364883422852
    },
    {
      "epoch": 0.5889972899728997,
      "step": 10867,
      "training_loss": 8.00243091583252
    },
    {
      "epoch": 0.5890514905149051,
      "grad_norm": 24.986318588256836,
      "learning_rate": 1e-05,
      "loss": 6.5413,
      "step": 10868
    },
    {
      "epoch": 0.5890514905149051,
      "step": 10868,
      "training_loss": 7.399446964263916
    },
    {
      "epoch": 0.5891056910569106,
      "step": 10869,
      "training_loss": 6.008026123046875
    },
    {
      "epoch": 0.589159891598916,
      "step": 10870,
      "training_loss": 6.302243709564209
    },
    {
      "epoch": 0.5892140921409215,
      "step": 10871,
      "training_loss": 6.882573127746582
    },
    {
      "epoch": 0.5892682926829268,
      "grad_norm": 30.16670799255371,
      "learning_rate": 1e-05,
      "loss": 6.6481,
      "step": 10872
    },
    {
      "epoch": 0.5892682926829268,
      "step": 10872,
      "training_loss": 7.302220821380615
    },
    {
      "epoch": 0.5893224932249322,
      "step": 10873,
      "training_loss": 6.109467506408691
    },
    {
      "epoch": 0.5893766937669377,
      "step": 10874,
      "training_loss": 5.90629243850708
    },
    {
      "epoch": 0.5894308943089431,
      "step": 10875,
      "training_loss": 6.722400188446045
    },
    {
      "epoch": 0.5894850948509485,
      "grad_norm": 21.680017471313477,
      "learning_rate": 1e-05,
      "loss": 6.5101,
      "step": 10876
    },
    {
      "epoch": 0.5894850948509485,
      "step": 10876,
      "training_loss": 6.612547874450684
    },
    {
      "epoch": 0.5895392953929539,
      "step": 10877,
      "training_loss": 7.70654821395874
    },
    {
      "epoch": 0.5895934959349594,
      "step": 10878,
      "training_loss": 6.561415195465088
    },
    {
      "epoch": 0.5896476964769648,
      "step": 10879,
      "training_loss": 5.846531867980957
    },
    {
      "epoch": 0.5897018970189702,
      "grad_norm": 26.68223762512207,
      "learning_rate": 1e-05,
      "loss": 6.6818,
      "step": 10880
    },
    {
      "epoch": 0.5897018970189702,
      "step": 10880,
      "training_loss": 6.198846817016602
    },
    {
      "epoch": 0.5897560975609756,
      "step": 10881,
      "training_loss": 4.2323994636535645
    },
    {
      "epoch": 0.589810298102981,
      "step": 10882,
      "training_loss": 6.901535987854004
    },
    {
      "epoch": 0.5898644986449865,
      "step": 10883,
      "training_loss": 6.53735876083374
    },
    {
      "epoch": 0.5899186991869919,
      "grad_norm": 25.285293579101562,
      "learning_rate": 1e-05,
      "loss": 5.9675,
      "step": 10884
    },
    {
      "epoch": 0.5899186991869919,
      "step": 10884,
      "training_loss": 7.098373889923096
    },
    {
      "epoch": 0.5899728997289972,
      "step": 10885,
      "training_loss": 6.911816596984863
    },
    {
      "epoch": 0.5900271002710027,
      "step": 10886,
      "training_loss": 6.398226737976074
    },
    {
      "epoch": 0.5900813008130081,
      "step": 10887,
      "training_loss": 7.497204780578613
    },
    {
      "epoch": 0.5901355013550136,
      "grad_norm": 22.433027267456055,
      "learning_rate": 1e-05,
      "loss": 6.9764,
      "step": 10888
    },
    {
      "epoch": 0.5901355013550136,
      "step": 10888,
      "training_loss": 6.67626953125
    },
    {
      "epoch": 0.590189701897019,
      "step": 10889,
      "training_loss": 7.2457075119018555
    },
    {
      "epoch": 0.5902439024390244,
      "step": 10890,
      "training_loss": 6.280462741851807
    },
    {
      "epoch": 0.5902981029810298,
      "step": 10891,
      "training_loss": 6.938139915466309
    },
    {
      "epoch": 0.5903523035230352,
      "grad_norm": 22.010112762451172,
      "learning_rate": 1e-05,
      "loss": 6.7851,
      "step": 10892
    },
    {
      "epoch": 0.5903523035230352,
      "step": 10892,
      "training_loss": 6.0924973487854
    },
    {
      "epoch": 0.5904065040650407,
      "step": 10893,
      "training_loss": 7.857004165649414
    },
    {
      "epoch": 0.590460704607046,
      "step": 10894,
      "training_loss": 7.597013473510742
    },
    {
      "epoch": 0.5905149051490515,
      "step": 10895,
      "training_loss": 7.647669315338135
    },
    {
      "epoch": 0.5905691056910569,
      "grad_norm": 27.210851669311523,
      "learning_rate": 1e-05,
      "loss": 7.2985,
      "step": 10896
    },
    {
      "epoch": 0.5905691056910569,
      "step": 10896,
      "training_loss": 6.2586894035339355
    },
    {
      "epoch": 0.5906233062330624,
      "step": 10897,
      "training_loss": 7.443748474121094
    },
    {
      "epoch": 0.5906775067750677,
      "step": 10898,
      "training_loss": 7.3205671310424805
    },
    {
      "epoch": 0.5907317073170731,
      "step": 10899,
      "training_loss": 4.82878303527832
    },
    {
      "epoch": 0.5907859078590786,
      "grad_norm": 29.55854606628418,
      "learning_rate": 1e-05,
      "loss": 6.4629,
      "step": 10900
    },
    {
      "epoch": 0.5907859078590786,
      "step": 10900,
      "training_loss": 7.292073726654053
    },
    {
      "epoch": 0.590840108401084,
      "step": 10901,
      "training_loss": 6.155125617980957
    },
    {
      "epoch": 0.5908943089430895,
      "step": 10902,
      "training_loss": 6.722681522369385
    },
    {
      "epoch": 0.5909485094850948,
      "step": 10903,
      "training_loss": 6.93394136428833
    },
    {
      "epoch": 0.5910027100271003,
      "grad_norm": 62.040504455566406,
      "learning_rate": 1e-05,
      "loss": 6.776,
      "step": 10904
    },
    {
      "epoch": 0.5910027100271003,
      "step": 10904,
      "training_loss": 7.2113261222839355
    },
    {
      "epoch": 0.5910569105691057,
      "step": 10905,
      "training_loss": 6.015541076660156
    },
    {
      "epoch": 0.5911111111111111,
      "step": 10906,
      "training_loss": 7.038764476776123
    },
    {
      "epoch": 0.5911653116531165,
      "step": 10907,
      "training_loss": 4.677352428436279
    },
    {
      "epoch": 0.5912195121951219,
      "grad_norm": 33.5398063659668,
      "learning_rate": 1e-05,
      "loss": 6.2357,
      "step": 10908
    },
    {
      "epoch": 0.5912195121951219,
      "step": 10908,
      "training_loss": 5.128088474273682
    },
    {
      "epoch": 0.5912737127371274,
      "step": 10909,
      "training_loss": 7.327917575836182
    },
    {
      "epoch": 0.5913279132791328,
      "step": 10910,
      "training_loss": 6.537975311279297
    },
    {
      "epoch": 0.5913821138211383,
      "step": 10911,
      "training_loss": 5.84335994720459
    },
    {
      "epoch": 0.5914363143631436,
      "grad_norm": 22.84670639038086,
      "learning_rate": 1e-05,
      "loss": 6.2093,
      "step": 10912
    },
    {
      "epoch": 0.5914363143631436,
      "step": 10912,
      "training_loss": 3.219191551208496
    },
    {
      "epoch": 0.591490514905149,
      "step": 10913,
      "training_loss": 7.6177978515625
    },
    {
      "epoch": 0.5915447154471545,
      "step": 10914,
      "training_loss": 3.22597599029541
    },
    {
      "epoch": 0.5915989159891599,
      "step": 10915,
      "training_loss": 7.665921211242676
    },
    {
      "epoch": 0.5916531165311653,
      "grad_norm": 27.438894271850586,
      "learning_rate": 1e-05,
      "loss": 5.4322,
      "step": 10916
    },
    {
      "epoch": 0.5916531165311653,
      "step": 10916,
      "training_loss": 6.249122142791748
    },
    {
      "epoch": 0.5917073170731707,
      "step": 10917,
      "training_loss": 6.149415969848633
    },
    {
      "epoch": 0.5917615176151761,
      "step": 10918,
      "training_loss": 5.801778316497803
    },
    {
      "epoch": 0.5918157181571816,
      "step": 10919,
      "training_loss": 6.929904937744141
    },
    {
      "epoch": 0.591869918699187,
      "grad_norm": 21.524887084960938,
      "learning_rate": 1e-05,
      "loss": 6.2826,
      "step": 10920
    },
    {
      "epoch": 0.591869918699187,
      "step": 10920,
      "training_loss": 7.962332248687744
    },
    {
      "epoch": 0.5919241192411924,
      "step": 10921,
      "training_loss": 5.983090400695801
    },
    {
      "epoch": 0.5919783197831978,
      "step": 10922,
      "training_loss": 3.48252534866333
    },
    {
      "epoch": 0.5920325203252033,
      "step": 10923,
      "training_loss": 6.151144027709961
    },
    {
      "epoch": 0.5920867208672087,
      "grad_norm": 37.84500503540039,
      "learning_rate": 1e-05,
      "loss": 5.8948,
      "step": 10924
    },
    {
      "epoch": 0.5920867208672087,
      "step": 10924,
      "training_loss": 5.23261022567749
    },
    {
      "epoch": 0.592140921409214,
      "step": 10925,
      "training_loss": 6.688906192779541
    },
    {
      "epoch": 0.5921951219512195,
      "step": 10926,
      "training_loss": 6.244410037994385
    },
    {
      "epoch": 0.5922493224932249,
      "step": 10927,
      "training_loss": 7.374927043914795
    },
    {
      "epoch": 0.5923035230352304,
      "grad_norm": 36.99298858642578,
      "learning_rate": 1e-05,
      "loss": 6.3852,
      "step": 10928
    },
    {
      "epoch": 0.5923035230352304,
      "step": 10928,
      "training_loss": 7.135001182556152
    },
    {
      "epoch": 0.5923577235772358,
      "step": 10929,
      "training_loss": 7.6407647132873535
    },
    {
      "epoch": 0.5924119241192412,
      "step": 10930,
      "training_loss": 6.753104209899902
    },
    {
      "epoch": 0.5924661246612466,
      "step": 10931,
      "training_loss": 6.60596227645874
    },
    {
      "epoch": 0.592520325203252,
      "grad_norm": 28.54059410095215,
      "learning_rate": 1e-05,
      "loss": 7.0337,
      "step": 10932
    },
    {
      "epoch": 0.592520325203252,
      "step": 10932,
      "training_loss": 7.648454666137695
    },
    {
      "epoch": 0.5925745257452575,
      "step": 10933,
      "training_loss": 7.68911075592041
    },
    {
      "epoch": 0.5926287262872628,
      "step": 10934,
      "training_loss": 6.529126167297363
    },
    {
      "epoch": 0.5926829268292683,
      "step": 10935,
      "training_loss": 6.601788520812988
    },
    {
      "epoch": 0.5927371273712737,
      "grad_norm": 28.263595581054688,
      "learning_rate": 1e-05,
      "loss": 7.1171,
      "step": 10936
    },
    {
      "epoch": 0.5927371273712737,
      "step": 10936,
      "training_loss": 6.242354869842529
    },
    {
      "epoch": 0.5927913279132792,
      "step": 10937,
      "training_loss": 6.810308456420898
    },
    {
      "epoch": 0.5928455284552846,
      "step": 10938,
      "training_loss": 7.256829261779785
    },
    {
      "epoch": 0.5928997289972899,
      "step": 10939,
      "training_loss": 5.84932804107666
    },
    {
      "epoch": 0.5929539295392954,
      "grad_norm": 31.298978805541992,
      "learning_rate": 1e-05,
      "loss": 6.5397,
      "step": 10940
    },
    {
      "epoch": 0.5929539295392954,
      "step": 10940,
      "training_loss": 7.0377516746521
    },
    {
      "epoch": 0.5930081300813008,
      "step": 10941,
      "training_loss": 4.510262489318848
    },
    {
      "epoch": 0.5930623306233063,
      "step": 10942,
      "training_loss": 7.654019355773926
    },
    {
      "epoch": 0.5931165311653116,
      "step": 10943,
      "training_loss": 6.205503940582275
    },
    {
      "epoch": 0.593170731707317,
      "grad_norm": 49.37314224243164,
      "learning_rate": 1e-05,
      "loss": 6.3519,
      "step": 10944
    },
    {
      "epoch": 0.593170731707317,
      "step": 10944,
      "training_loss": 3.5934720039367676
    },
    {
      "epoch": 0.5932249322493225,
      "step": 10945,
      "training_loss": 6.514315128326416
    },
    {
      "epoch": 0.5932791327913279,
      "step": 10946,
      "training_loss": 5.6994428634643555
    },
    {
      "epoch": 0.5933333333333334,
      "step": 10947,
      "training_loss": 7.260229587554932
    },
    {
      "epoch": 0.5933875338753387,
      "grad_norm": 17.406232833862305,
      "learning_rate": 1e-05,
      "loss": 5.7669,
      "step": 10948
    },
    {
      "epoch": 0.5933875338753387,
      "step": 10948,
      "training_loss": 6.4792633056640625
    },
    {
      "epoch": 0.5934417344173442,
      "step": 10949,
      "training_loss": 7.935939311981201
    },
    {
      "epoch": 0.5934959349593496,
      "step": 10950,
      "training_loss": 5.978304386138916
    },
    {
      "epoch": 0.593550135501355,
      "step": 10951,
      "training_loss": 7.221642971038818
    },
    {
      "epoch": 0.5936043360433604,
      "grad_norm": 18.652280807495117,
      "learning_rate": 1e-05,
      "loss": 6.9038,
      "step": 10952
    },
    {
      "epoch": 0.5936043360433604,
      "step": 10952,
      "training_loss": 7.20927619934082
    },
    {
      "epoch": 0.5936585365853658,
      "step": 10953,
      "training_loss": 6.49690055847168
    },
    {
      "epoch": 0.5937127371273713,
      "step": 10954,
      "training_loss": 6.986031532287598
    },
    {
      "epoch": 0.5937669376693767,
      "step": 10955,
      "training_loss": 7.815383434295654
    },
    {
      "epoch": 0.5938211382113822,
      "grad_norm": 23.462730407714844,
      "learning_rate": 1e-05,
      "loss": 7.1269,
      "step": 10956
    },
    {
      "epoch": 0.5938211382113822,
      "step": 10956,
      "training_loss": 6.597806453704834
    },
    {
      "epoch": 0.5938753387533875,
      "step": 10957,
      "training_loss": 7.182574272155762
    },
    {
      "epoch": 0.5939295392953929,
      "step": 10958,
      "training_loss": 6.640136241912842
    },
    {
      "epoch": 0.5939837398373984,
      "step": 10959,
      "training_loss": 7.992475986480713
    },
    {
      "epoch": 0.5940379403794038,
      "grad_norm": 40.409481048583984,
      "learning_rate": 1e-05,
      "loss": 7.1032,
      "step": 10960
    },
    {
      "epoch": 0.5940379403794038,
      "step": 10960,
      "training_loss": 6.069347858428955
    },
    {
      "epoch": 0.5940921409214092,
      "step": 10961,
      "training_loss": 7.157155990600586
    },
    {
      "epoch": 0.5941463414634146,
      "step": 10962,
      "training_loss": 6.566418647766113
    },
    {
      "epoch": 0.59420054200542,
      "step": 10963,
      "training_loss": 6.959513187408447
    },
    {
      "epoch": 0.5942547425474255,
      "grad_norm": 33.699974060058594,
      "learning_rate": 1e-05,
      "loss": 6.6881,
      "step": 10964
    },
    {
      "epoch": 0.5942547425474255,
      "step": 10964,
      "training_loss": 7.070240497589111
    },
    {
      "epoch": 0.5943089430894309,
      "step": 10965,
      "training_loss": 7.371450424194336
    },
    {
      "epoch": 0.5943631436314363,
      "step": 10966,
      "training_loss": 6.8828277587890625
    },
    {
      "epoch": 0.5944173441734417,
      "step": 10967,
      "training_loss": 7.327083110809326
    },
    {
      "epoch": 0.5944715447154472,
      "grad_norm": 47.91764450073242,
      "learning_rate": 1e-05,
      "loss": 7.1629,
      "step": 10968
    },
    {
      "epoch": 0.5944715447154472,
      "step": 10968,
      "training_loss": 7.086299419403076
    },
    {
      "epoch": 0.5945257452574526,
      "step": 10969,
      "training_loss": 7.2729878425598145
    },
    {
      "epoch": 0.5945799457994579,
      "step": 10970,
      "training_loss": 7.045570373535156
    },
    {
      "epoch": 0.5946341463414634,
      "step": 10971,
      "training_loss": 5.8894429206848145
    },
    {
      "epoch": 0.5946883468834688,
      "grad_norm": 47.57496643066406,
      "learning_rate": 1e-05,
      "loss": 6.8236,
      "step": 10972
    },
    {
      "epoch": 0.5946883468834688,
      "step": 10972,
      "training_loss": 8.676595687866211
    },
    {
      "epoch": 0.5947425474254743,
      "step": 10973,
      "training_loss": 6.637314796447754
    },
    {
      "epoch": 0.5947967479674797,
      "step": 10974,
      "training_loss": 6.684154510498047
    },
    {
      "epoch": 0.5948509485094851,
      "step": 10975,
      "training_loss": 7.113494396209717
    },
    {
      "epoch": 0.5949051490514905,
      "grad_norm": 34.73836898803711,
      "learning_rate": 1e-05,
      "loss": 7.2779,
      "step": 10976
    },
    {
      "epoch": 0.5949051490514905,
      "step": 10976,
      "training_loss": 6.757529258728027
    },
    {
      "epoch": 0.594959349593496,
      "step": 10977,
      "training_loss": 5.902159214019775
    },
    {
      "epoch": 0.5950135501355014,
      "step": 10978,
      "training_loss": 7.659835338592529
    },
    {
      "epoch": 0.5950677506775067,
      "step": 10979,
      "training_loss": 6.680417537689209
    },
    {
      "epoch": 0.5951219512195122,
      "grad_norm": 18.70823097229004,
      "learning_rate": 1e-05,
      "loss": 6.75,
      "step": 10980
    },
    {
      "epoch": 0.5951219512195122,
      "step": 10980,
      "training_loss": 6.5368971824646
    },
    {
      "epoch": 0.5951761517615176,
      "step": 10981,
      "training_loss": 5.995997428894043
    },
    {
      "epoch": 0.5952303523035231,
      "step": 10982,
      "training_loss": 5.486804962158203
    },
    {
      "epoch": 0.5952845528455285,
      "step": 10983,
      "training_loss": 7.321106433868408
    },
    {
      "epoch": 0.5953387533875338,
      "grad_norm": 17.287126541137695,
      "learning_rate": 1e-05,
      "loss": 6.3352,
      "step": 10984
    },
    {
      "epoch": 0.5953387533875338,
      "step": 10984,
      "training_loss": 7.109471797943115
    },
    {
      "epoch": 0.5953929539295393,
      "step": 10985,
      "training_loss": 7.795657634735107
    },
    {
      "epoch": 0.5954471544715447,
      "step": 10986,
      "training_loss": 6.176992416381836
    },
    {
      "epoch": 0.5955013550135502,
      "step": 10987,
      "training_loss": 7.018503189086914
    },
    {
      "epoch": 0.5955555555555555,
      "grad_norm": 21.34609603881836,
      "learning_rate": 1e-05,
      "loss": 7.0252,
      "step": 10988
    },
    {
      "epoch": 0.5955555555555555,
      "step": 10988,
      "training_loss": 7.313769340515137
    },
    {
      "epoch": 0.595609756097561,
      "step": 10989,
      "training_loss": 5.759042739868164
    },
    {
      "epoch": 0.5956639566395664,
      "step": 10990,
      "training_loss": 7.361340045928955
    },
    {
      "epoch": 0.5957181571815718,
      "step": 10991,
      "training_loss": 7.464496612548828
    },
    {
      "epoch": 0.5957723577235773,
      "grad_norm": 21.616439819335938,
      "learning_rate": 1e-05,
      "loss": 6.9747,
      "step": 10992
    },
    {
      "epoch": 0.5957723577235773,
      "step": 10992,
      "training_loss": 7.053995132446289
    },
    {
      "epoch": 0.5958265582655826,
      "step": 10993,
      "training_loss": 6.485004425048828
    },
    {
      "epoch": 0.5958807588075881,
      "step": 10994,
      "training_loss": 6.759740352630615
    },
    {
      "epoch": 0.5959349593495935,
      "step": 10995,
      "training_loss": 6.217282772064209
    },
    {
      "epoch": 0.595989159891599,
      "grad_norm": 31.431102752685547,
      "learning_rate": 1e-05,
      "loss": 6.629,
      "step": 10996
    },
    {
      "epoch": 0.595989159891599,
      "step": 10996,
      "training_loss": 7.2114763259887695
    },
    {
      "epoch": 0.5960433604336043,
      "step": 10997,
      "training_loss": 7.711504936218262
    },
    {
      "epoch": 0.5960975609756097,
      "step": 10998,
      "training_loss": 7.071123123168945
    },
    {
      "epoch": 0.5961517615176152,
      "step": 10999,
      "training_loss": 7.637585639953613
    },
    {
      "epoch": 0.5962059620596206,
      "grad_norm": 35.33336639404297,
      "learning_rate": 1e-05,
      "loss": 7.4079,
      "step": 11000
    },
    {
      "epoch": 0.5962059620596206,
      "step": 11000,
      "training_loss": 6.624011993408203
    },
    {
      "epoch": 0.5962601626016261,
      "step": 11001,
      "training_loss": 7.715563774108887
    },
    {
      "epoch": 0.5963143631436314,
      "step": 11002,
      "training_loss": 6.0952229499816895
    },
    {
      "epoch": 0.5963685636856368,
      "step": 11003,
      "training_loss": 6.628664970397949
    },
    {
      "epoch": 0.5964227642276423,
      "grad_norm": 20.86016845703125,
      "learning_rate": 1e-05,
      "loss": 6.7659,
      "step": 11004
    },
    {
      "epoch": 0.5964227642276423,
      "step": 11004,
      "training_loss": 4.4166131019592285
    },
    {
      "epoch": 0.5964769647696477,
      "step": 11005,
      "training_loss": 6.440060138702393
    },
    {
      "epoch": 0.5965311653116531,
      "step": 11006,
      "training_loss": 6.8351969718933105
    },
    {
      "epoch": 0.5965853658536585,
      "step": 11007,
      "training_loss": 7.09214448928833
    },
    {
      "epoch": 0.596639566395664,
      "grad_norm": 28.267255783081055,
      "learning_rate": 1e-05,
      "loss": 6.196,
      "step": 11008
    },
    {
      "epoch": 0.596639566395664,
      "step": 11008,
      "training_loss": 5.976308345794678
    },
    {
      "epoch": 0.5966937669376694,
      "step": 11009,
      "training_loss": 5.982761859893799
    },
    {
      "epoch": 0.5967479674796748,
      "step": 11010,
      "training_loss": 6.092014312744141
    },
    {
      "epoch": 0.5968021680216802,
      "step": 11011,
      "training_loss": 6.877490043640137
    },
    {
      "epoch": 0.5968563685636856,
      "grad_norm": 18.726083755493164,
      "learning_rate": 1e-05,
      "loss": 6.2321,
      "step": 11012
    },
    {
      "epoch": 0.5968563685636856,
      "step": 11012,
      "training_loss": 6.883813381195068
    },
    {
      "epoch": 0.5969105691056911,
      "step": 11013,
      "training_loss": 4.491900444030762
    },
    {
      "epoch": 0.5969647696476965,
      "step": 11014,
      "training_loss": 7.4127936363220215
    },
    {
      "epoch": 0.5970189701897018,
      "step": 11015,
      "training_loss": 5.638383865356445
    },
    {
      "epoch": 0.5970731707317073,
      "grad_norm": 37.2763557434082,
      "learning_rate": 1e-05,
      "loss": 6.1067,
      "step": 11016
    },
    {
      "epoch": 0.5970731707317073,
      "step": 11016,
      "training_loss": 7.073550701141357
    },
    {
      "epoch": 0.5971273712737127,
      "step": 11017,
      "training_loss": 5.962313652038574
    },
    {
      "epoch": 0.5971815718157182,
      "step": 11018,
      "training_loss": 8.091687202453613
    },
    {
      "epoch": 0.5972357723577236,
      "step": 11019,
      "training_loss": 6.509961128234863
    },
    {
      "epoch": 0.597289972899729,
      "grad_norm": 16.785369873046875,
      "learning_rate": 1e-05,
      "loss": 6.9094,
      "step": 11020
    },
    {
      "epoch": 0.597289972899729,
      "step": 11020,
      "training_loss": 7.65781831741333
    },
    {
      "epoch": 0.5973441734417344,
      "step": 11021,
      "training_loss": 6.519495487213135
    },
    {
      "epoch": 0.5973983739837398,
      "step": 11022,
      "training_loss": 4.5997490882873535
    },
    {
      "epoch": 0.5974525745257453,
      "step": 11023,
      "training_loss": 8.772398948669434
    },
    {
      "epoch": 0.5975067750677506,
      "grad_norm": 43.56674575805664,
      "learning_rate": 1e-05,
      "loss": 6.8874,
      "step": 11024
    },
    {
      "epoch": 0.5975067750677506,
      "step": 11024,
      "training_loss": 6.5361223220825195
    },
    {
      "epoch": 0.5975609756097561,
      "step": 11025,
      "training_loss": 7.049206733703613
    },
    {
      "epoch": 0.5976151761517615,
      "step": 11026,
      "training_loss": 4.235907554626465
    },
    {
      "epoch": 0.597669376693767,
      "step": 11027,
      "training_loss": 5.437300682067871
    },
    {
      "epoch": 0.5977235772357724,
      "grad_norm": 28.03268814086914,
      "learning_rate": 1e-05,
      "loss": 5.8146,
      "step": 11028
    },
    {
      "epoch": 0.5977235772357724,
      "step": 11028,
      "training_loss": 6.493498802185059
    },
    {
      "epoch": 0.5977777777777777,
      "step": 11029,
      "training_loss": 6.8048319816589355
    },
    {
      "epoch": 0.5978319783197832,
      "step": 11030,
      "training_loss": 6.525485992431641
    },
    {
      "epoch": 0.5978861788617886,
      "step": 11031,
      "training_loss": 6.761744022369385
    },
    {
      "epoch": 0.5979403794037941,
      "grad_norm": 28.689022064208984,
      "learning_rate": 1e-05,
      "loss": 6.6464,
      "step": 11032
    },
    {
      "epoch": 0.5979403794037941,
      "step": 11032,
      "training_loss": 7.260319709777832
    },
    {
      "epoch": 0.5979945799457994,
      "step": 11033,
      "training_loss": 6.692844867706299
    },
    {
      "epoch": 0.5980487804878049,
      "step": 11034,
      "training_loss": 6.4098358154296875
    },
    {
      "epoch": 0.5981029810298103,
      "step": 11035,
      "training_loss": 7.184148788452148
    },
    {
      "epoch": 0.5981571815718157,
      "grad_norm": 34.189781188964844,
      "learning_rate": 1e-05,
      "loss": 6.8868,
      "step": 11036
    },
    {
      "epoch": 0.5981571815718157,
      "step": 11036,
      "training_loss": 7.063739776611328
    },
    {
      "epoch": 0.5982113821138212,
      "step": 11037,
      "training_loss": 6.28643798828125
    },
    {
      "epoch": 0.5982655826558265,
      "step": 11038,
      "training_loss": 4.544436454772949
    },
    {
      "epoch": 0.598319783197832,
      "step": 11039,
      "training_loss": 7.697327136993408
    },
    {
      "epoch": 0.5983739837398374,
      "grad_norm": 25.265592575073242,
      "learning_rate": 1e-05,
      "loss": 6.398,
      "step": 11040
    },
    {
      "epoch": 0.5983739837398374,
      "step": 11040,
      "training_loss": 6.535763263702393
    },
    {
      "epoch": 0.5984281842818429,
      "step": 11041,
      "training_loss": 7.774016380310059
    },
    {
      "epoch": 0.5984823848238482,
      "step": 11042,
      "training_loss": 6.844910144805908
    },
    {
      "epoch": 0.5985365853658536,
      "step": 11043,
      "training_loss": 7.001053333282471
    },
    {
      "epoch": 0.5985907859078591,
      "grad_norm": 39.059715270996094,
      "learning_rate": 1e-05,
      "loss": 7.0389,
      "step": 11044
    },
    {
      "epoch": 0.5985907859078591,
      "step": 11044,
      "training_loss": 6.449651718139648
    },
    {
      "epoch": 0.5986449864498645,
      "step": 11045,
      "training_loss": 6.382030963897705
    },
    {
      "epoch": 0.59869918699187,
      "step": 11046,
      "training_loss": 6.947986125946045
    },
    {
      "epoch": 0.5987533875338753,
      "step": 11047,
      "training_loss": 7.240787506103516
    },
    {
      "epoch": 0.5988075880758807,
      "grad_norm": 24.085723876953125,
      "learning_rate": 1e-05,
      "loss": 6.7551,
      "step": 11048
    },
    {
      "epoch": 0.5988075880758807,
      "step": 11048,
      "training_loss": 5.980581760406494
    },
    {
      "epoch": 0.5988617886178862,
      "step": 11049,
      "training_loss": 5.424380779266357
    },
    {
      "epoch": 0.5989159891598916,
      "step": 11050,
      "training_loss": 6.069179534912109
    },
    {
      "epoch": 0.598970189701897,
      "step": 11051,
      "training_loss": 6.845284461975098
    },
    {
      "epoch": 0.5990243902439024,
      "grad_norm": 26.171842575073242,
      "learning_rate": 1e-05,
      "loss": 6.0799,
      "step": 11052
    },
    {
      "epoch": 0.5990243902439024,
      "step": 11052,
      "training_loss": 6.222631931304932
    },
    {
      "epoch": 0.5990785907859079,
      "step": 11053,
      "training_loss": 7.399404525756836
    },
    {
      "epoch": 0.5991327913279133,
      "step": 11054,
      "training_loss": 7.3995466232299805
    },
    {
      "epoch": 0.5991869918699188,
      "step": 11055,
      "training_loss": 5.721418380737305
    },
    {
      "epoch": 0.5992411924119241,
      "grad_norm": 31.462398529052734,
      "learning_rate": 1e-05,
      "loss": 6.6858,
      "step": 11056
    },
    {
      "epoch": 0.5992411924119241,
      "step": 11056,
      "training_loss": 7.463067054748535
    },
    {
      "epoch": 0.5992953929539295,
      "step": 11057,
      "training_loss": 6.362881183624268
    },
    {
      "epoch": 0.599349593495935,
      "step": 11058,
      "training_loss": 7.3611555099487305
    },
    {
      "epoch": 0.5994037940379404,
      "step": 11059,
      "training_loss": 4.4443182945251465
    },
    {
      "epoch": 0.5994579945799458,
      "grad_norm": 24.091516494750977,
      "learning_rate": 1e-05,
      "loss": 6.4079,
      "step": 11060
    },
    {
      "epoch": 0.5994579945799458,
      "step": 11060,
      "training_loss": 6.266279220581055
    },
    {
      "epoch": 0.5995121951219512,
      "step": 11061,
      "training_loss": 6.749885082244873
    },
    {
      "epoch": 0.5995663956639566,
      "step": 11062,
      "training_loss": 7.556498050689697
    },
    {
      "epoch": 0.5996205962059621,
      "step": 11063,
      "training_loss": 7.574294567108154
    },
    {
      "epoch": 0.5996747967479675,
      "grad_norm": 27.47272491455078,
      "learning_rate": 1e-05,
      "loss": 7.0367,
      "step": 11064
    },
    {
      "epoch": 0.5996747967479675,
      "step": 11064,
      "training_loss": 5.9072265625
    },
    {
      "epoch": 0.5997289972899729,
      "step": 11065,
      "training_loss": 3.950883626937866
    },
    {
      "epoch": 0.5997831978319783,
      "step": 11066,
      "training_loss": 6.7181596755981445
    },
    {
      "epoch": 0.5998373983739838,
      "step": 11067,
      "training_loss": 6.299131393432617
    },
    {
      "epoch": 0.5998915989159892,
      "grad_norm": 32.96095657348633,
      "learning_rate": 1e-05,
      "loss": 5.7189,
      "step": 11068
    },
    {
      "epoch": 0.5998915989159892,
      "step": 11068,
      "training_loss": 6.8476243019104
    },
    {
      "epoch": 0.5999457994579945,
      "step": 11069,
      "training_loss": 7.038668155670166
    },
    {
      "epoch": 0.6,
      "step": 11070,
      "training_loss": 5.633530616760254
    },
    {
      "epoch": 0.6000542005420054,
      "step": 11071,
      "training_loss": 6.157073497772217
    },
    {
      "epoch": 0.6001084010840109,
      "grad_norm": 29.113750457763672,
      "learning_rate": 1e-05,
      "loss": 6.4192,
      "step": 11072
    },
    {
      "epoch": 0.6001084010840109,
      "step": 11072,
      "training_loss": 3.9853029251098633
    },
    {
      "epoch": 0.6001626016260163,
      "step": 11073,
      "training_loss": 7.395498752593994
    },
    {
      "epoch": 0.6002168021680216,
      "step": 11074,
      "training_loss": 5.8743510246276855
    },
    {
      "epoch": 0.6002710027100271,
      "step": 11075,
      "training_loss": 5.958527565002441
    },
    {
      "epoch": 0.6003252032520325,
      "grad_norm": 22.25904083251953,
      "learning_rate": 1e-05,
      "loss": 5.8034,
      "step": 11076
    },
    {
      "epoch": 0.6003252032520325,
      "step": 11076,
      "training_loss": 6.335699081420898
    },
    {
      "epoch": 0.600379403794038,
      "step": 11077,
      "training_loss": 5.265078544616699
    },
    {
      "epoch": 0.6004336043360433,
      "step": 11078,
      "training_loss": 7.262122631072998
    },
    {
      "epoch": 0.6004878048780488,
      "step": 11079,
      "training_loss": 5.960559368133545
    },
    {
      "epoch": 0.6005420054200542,
      "grad_norm": 21.0752010345459,
      "learning_rate": 1e-05,
      "loss": 6.2059,
      "step": 11080
    },
    {
      "epoch": 0.6005420054200542,
      "step": 11080,
      "training_loss": 7.588852405548096
    },
    {
      "epoch": 0.6005962059620596,
      "step": 11081,
      "training_loss": 6.321976184844971
    },
    {
      "epoch": 0.6006504065040651,
      "step": 11082,
      "training_loss": 5.871057033538818
    },
    {
      "epoch": 0.6007046070460704,
      "step": 11083,
      "training_loss": 7.118670463562012
    },
    {
      "epoch": 0.6007588075880759,
      "grad_norm": 35.98613357543945,
      "learning_rate": 1e-05,
      "loss": 6.7251,
      "step": 11084
    },
    {
      "epoch": 0.6007588075880759,
      "step": 11084,
      "training_loss": 7.7578277587890625
    },
    {
      "epoch": 0.6008130081300813,
      "step": 11085,
      "training_loss": 6.739837169647217
    },
    {
      "epoch": 0.6008672086720868,
      "step": 11086,
      "training_loss": 3.545435667037964
    },
    {
      "epoch": 0.6009214092140921,
      "step": 11087,
      "training_loss": 7.071723937988281
    },
    {
      "epoch": 0.6009756097560975,
      "grad_norm": 26.478544235229492,
      "learning_rate": 1e-05,
      "loss": 6.2787,
      "step": 11088
    },
    {
      "epoch": 0.6009756097560975,
      "step": 11088,
      "training_loss": 5.812066078186035
    },
    {
      "epoch": 0.601029810298103,
      "step": 11089,
      "training_loss": 5.181218147277832
    },
    {
      "epoch": 0.6010840108401084,
      "step": 11090,
      "training_loss": 5.86263370513916
    },
    {
      "epoch": 0.6011382113821139,
      "step": 11091,
      "training_loss": 6.072546005249023
    },
    {
      "epoch": 0.6011924119241192,
      "grad_norm": 36.03868865966797,
      "learning_rate": 1e-05,
      "loss": 5.7321,
      "step": 11092
    },
    {
      "epoch": 0.6011924119241192,
      "step": 11092,
      "training_loss": 7.373278617858887
    },
    {
      "epoch": 0.6012466124661247,
      "step": 11093,
      "training_loss": 6.928121566772461
    },
    {
      "epoch": 0.6013008130081301,
      "step": 11094,
      "training_loss": 6.857038497924805
    },
    {
      "epoch": 0.6013550135501355,
      "step": 11095,
      "training_loss": 7.491692066192627
    },
    {
      "epoch": 0.6014092140921409,
      "grad_norm": 32.940216064453125,
      "learning_rate": 1e-05,
      "loss": 7.1625,
      "step": 11096
    },
    {
      "epoch": 0.6014092140921409,
      "step": 11096,
      "training_loss": 6.818441867828369
    },
    {
      "epoch": 0.6014634146341463,
      "step": 11097,
      "training_loss": 5.059744834899902
    },
    {
      "epoch": 0.6015176151761518,
      "step": 11098,
      "training_loss": 7.71392822265625
    },
    {
      "epoch": 0.6015718157181572,
      "step": 11099,
      "training_loss": 7.2998456954956055
    },
    {
      "epoch": 0.6016260162601627,
      "grad_norm": 28.199628829956055,
      "learning_rate": 1e-05,
      "loss": 6.723,
      "step": 11100
    },
    {
      "epoch": 0.6016260162601627,
      "step": 11100,
      "training_loss": 7.26662015914917
    },
    {
      "epoch": 0.601680216802168,
      "step": 11101,
      "training_loss": 5.4526190757751465
    },
    {
      "epoch": 0.6017344173441734,
      "step": 11102,
      "training_loss": 6.835565567016602
    },
    {
      "epoch": 0.6017886178861789,
      "step": 11103,
      "training_loss": 4.026360988616943
    },
    {
      "epoch": 0.6018428184281843,
      "grad_norm": 25.205020904541016,
      "learning_rate": 1e-05,
      "loss": 5.8953,
      "step": 11104
    },
    {
      "epoch": 0.6018428184281843,
      "step": 11104,
      "training_loss": 3.431600570678711
    },
    {
      "epoch": 0.6018970189701897,
      "step": 11105,
      "training_loss": 6.6712799072265625
    },
    {
      "epoch": 0.6019512195121951,
      "step": 11106,
      "training_loss": 7.544037818908691
    },
    {
      "epoch": 0.6020054200542005,
      "step": 11107,
      "training_loss": 7.106781959533691
    },
    {
      "epoch": 0.602059620596206,
      "grad_norm": 38.598323822021484,
      "learning_rate": 1e-05,
      "loss": 6.1884,
      "step": 11108
    },
    {
      "epoch": 0.602059620596206,
      "step": 11108,
      "training_loss": 7.289297580718994
    },
    {
      "epoch": 0.6021138211382114,
      "step": 11109,
      "training_loss": 6.159316062927246
    },
    {
      "epoch": 0.6021680216802168,
      "step": 11110,
      "training_loss": 8.767621040344238
    },
    {
      "epoch": 0.6022222222222222,
      "step": 11111,
      "training_loss": 8.07129955291748
    },
    {
      "epoch": 0.6022764227642277,
      "grad_norm": 19.872697830200195,
      "learning_rate": 1e-05,
      "loss": 7.5719,
      "step": 11112
    },
    {
      "epoch": 0.6022764227642277,
      "step": 11112,
      "training_loss": 7.1027512550354
    },
    {
      "epoch": 0.6023306233062331,
      "step": 11113,
      "training_loss": 6.727883338928223
    },
    {
      "epoch": 0.6023848238482384,
      "step": 11114,
      "training_loss": 3.0002238750457764
    },
    {
      "epoch": 0.6024390243902439,
      "step": 11115,
      "training_loss": 5.1880784034729
    },
    {
      "epoch": 0.6024932249322493,
      "grad_norm": 22.234895706176758,
      "learning_rate": 1e-05,
      "loss": 5.5047,
      "step": 11116
    },
    {
      "epoch": 0.6024932249322493,
      "step": 11116,
      "training_loss": 5.815372467041016
    },
    {
      "epoch": 0.6025474254742548,
      "step": 11117,
      "training_loss": 7.154808521270752
    },
    {
      "epoch": 0.6026016260162602,
      "step": 11118,
      "training_loss": 7.020277976989746
    },
    {
      "epoch": 0.6026558265582656,
      "step": 11119,
      "training_loss": 6.808940410614014
    },
    {
      "epoch": 0.602710027100271,
      "grad_norm": 23.4995174407959,
      "learning_rate": 1e-05,
      "loss": 6.6999,
      "step": 11120
    },
    {
      "epoch": 0.602710027100271,
      "step": 11120,
      "training_loss": 6.451155185699463
    },
    {
      "epoch": 0.6027642276422764,
      "step": 11121,
      "training_loss": 7.615228176116943
    },
    {
      "epoch": 0.6028184281842819,
      "step": 11122,
      "training_loss": 7.08967399597168
    },
    {
      "epoch": 0.6028726287262872,
      "step": 11123,
      "training_loss": 5.846789360046387
    },
    {
      "epoch": 0.6029268292682927,
      "grad_norm": 27.270578384399414,
      "learning_rate": 1e-05,
      "loss": 6.7507,
      "step": 11124
    },
    {
      "epoch": 0.6029268292682927,
      "step": 11124,
      "training_loss": 7.180241107940674
    },
    {
      "epoch": 0.6029810298102981,
      "step": 11125,
      "training_loss": 7.126986026763916
    },
    {
      "epoch": 0.6030352303523036,
      "step": 11126,
      "training_loss": 6.95031213760376
    },
    {
      "epoch": 0.603089430894309,
      "step": 11127,
      "training_loss": 6.971716403961182
    },
    {
      "epoch": 0.6031436314363143,
      "grad_norm": 32.0150146484375,
      "learning_rate": 1e-05,
      "loss": 7.0573,
      "step": 11128
    },
    {
      "epoch": 0.6031436314363143,
      "step": 11128,
      "training_loss": 6.687315464019775
    },
    {
      "epoch": 0.6031978319783198,
      "step": 11129,
      "training_loss": 7.483044624328613
    },
    {
      "epoch": 0.6032520325203252,
      "step": 11130,
      "training_loss": 5.853895664215088
    },
    {
      "epoch": 0.6033062330623307,
      "step": 11131,
      "training_loss": 7.476398944854736
    },
    {
      "epoch": 0.603360433604336,
      "grad_norm": 31.813541412353516,
      "learning_rate": 1e-05,
      "loss": 6.8752,
      "step": 11132
    },
    {
      "epoch": 0.603360433604336,
      "step": 11132,
      "training_loss": 7.440887928009033
    },
    {
      "epoch": 0.6034146341463414,
      "step": 11133,
      "training_loss": 6.7304816246032715
    },
    {
      "epoch": 0.6034688346883469,
      "step": 11134,
      "training_loss": 5.310020446777344
    },
    {
      "epoch": 0.6035230352303523,
      "step": 11135,
      "training_loss": 7.876435279846191
    },
    {
      "epoch": 0.6035772357723578,
      "grad_norm": 32.63596725463867,
      "learning_rate": 1e-05,
      "loss": 6.8395,
      "step": 11136
    },
    {
      "epoch": 0.6035772357723578,
      "step": 11136,
      "training_loss": 5.4183030128479
    },
    {
      "epoch": 0.6036314363143631,
      "step": 11137,
      "training_loss": 4.978416442871094
    },
    {
      "epoch": 0.6036856368563686,
      "step": 11138,
      "training_loss": 5.870357036590576
    },
    {
      "epoch": 0.603739837398374,
      "step": 11139,
      "training_loss": 7.300804138183594
    },
    {
      "epoch": 0.6037940379403794,
      "grad_norm": 32.62084197998047,
      "learning_rate": 1e-05,
      "loss": 5.892,
      "step": 11140
    },
    {
      "epoch": 0.6037940379403794,
      "step": 11140,
      "training_loss": 7.719554901123047
    },
    {
      "epoch": 0.6038482384823848,
      "step": 11141,
      "training_loss": 5.946413040161133
    },
    {
      "epoch": 0.6039024390243902,
      "step": 11142,
      "training_loss": 6.110853672027588
    },
    {
      "epoch": 0.6039566395663957,
      "step": 11143,
      "training_loss": 6.8098249435424805
    },
    {
      "epoch": 0.6040108401084011,
      "grad_norm": 41.03069305419922,
      "learning_rate": 1e-05,
      "loss": 6.6467,
      "step": 11144
    },
    {
      "epoch": 0.6040108401084011,
      "step": 11144,
      "training_loss": 4.484838962554932
    },
    {
      "epoch": 0.6040650406504066,
      "step": 11145,
      "training_loss": 7.318984031677246
    },
    {
      "epoch": 0.6041192411924119,
      "step": 11146,
      "training_loss": 7.205708980560303
    },
    {
      "epoch": 0.6041734417344173,
      "step": 11147,
      "training_loss": 7.346485614776611
    },
    {
      "epoch": 0.6042276422764228,
      "grad_norm": 17.971315383911133,
      "learning_rate": 1e-05,
      "loss": 6.589,
      "step": 11148
    },
    {
      "epoch": 0.6042276422764228,
      "step": 11148,
      "training_loss": 6.546396732330322
    },
    {
      "epoch": 0.6042818428184282,
      "step": 11149,
      "training_loss": 5.486326217651367
    },
    {
      "epoch": 0.6043360433604336,
      "step": 11150,
      "training_loss": 6.443658828735352
    },
    {
      "epoch": 0.604390243902439,
      "step": 11151,
      "training_loss": 6.884119987487793
    },
    {
      "epoch": 0.6044444444444445,
      "grad_norm": 21.91139793395996,
      "learning_rate": 1e-05,
      "loss": 6.3401,
      "step": 11152
    },
    {
      "epoch": 0.6044444444444445,
      "step": 11152,
      "training_loss": 6.504848003387451
    },
    {
      "epoch": 0.6044986449864499,
      "step": 11153,
      "training_loss": 6.050110340118408
    },
    {
      "epoch": 0.6045528455284552,
      "step": 11154,
      "training_loss": 7.203690528869629
    },
    {
      "epoch": 0.6046070460704607,
      "step": 11155,
      "training_loss": 6.312140941619873
    },
    {
      "epoch": 0.6046612466124661,
      "grad_norm": 37.18186569213867,
      "learning_rate": 1e-05,
      "loss": 6.5177,
      "step": 11156
    },
    {
      "epoch": 0.6046612466124661,
      "step": 11156,
      "training_loss": 7.906692028045654
    },
    {
      "epoch": 0.6047154471544716,
      "step": 11157,
      "training_loss": 5.654050827026367
    },
    {
      "epoch": 0.604769647696477,
      "step": 11158,
      "training_loss": 5.984521865844727
    },
    {
      "epoch": 0.6048238482384823,
      "step": 11159,
      "training_loss": 4.27946662902832
    },
    {
      "epoch": 0.6048780487804878,
      "grad_norm": 33.48423767089844,
      "learning_rate": 1e-05,
      "loss": 5.9562,
      "step": 11160
    },
    {
      "epoch": 0.6048780487804878,
      "step": 11160,
      "training_loss": 7.672090530395508
    },
    {
      "epoch": 0.6049322493224932,
      "step": 11161,
      "training_loss": 4.441939830780029
    },
    {
      "epoch": 0.6049864498644987,
      "step": 11162,
      "training_loss": 6.9736738204956055
    },
    {
      "epoch": 0.605040650406504,
      "step": 11163,
      "training_loss": 6.742969989776611
    },
    {
      "epoch": 0.6050948509485095,
      "grad_norm": 28.160686492919922,
      "learning_rate": 1e-05,
      "loss": 6.4577,
      "step": 11164
    },
    {
      "epoch": 0.6050948509485095,
      "step": 11164,
      "training_loss": 6.33933162689209
    },
    {
      "epoch": 0.6051490514905149,
      "step": 11165,
      "training_loss": 7.207388877868652
    },
    {
      "epoch": 0.6052032520325203,
      "step": 11166,
      "training_loss": 5.688606262207031
    },
    {
      "epoch": 0.6052574525745258,
      "step": 11167,
      "training_loss": 6.219055652618408
    },
    {
      "epoch": 0.6053116531165311,
      "grad_norm": 34.580291748046875,
      "learning_rate": 1e-05,
      "loss": 6.3636,
      "step": 11168
    },
    {
      "epoch": 0.6053116531165311,
      "step": 11168,
      "training_loss": 7.385870933532715
    },
    {
      "epoch": 0.6053658536585366,
      "step": 11169,
      "training_loss": 5.501719951629639
    },
    {
      "epoch": 0.605420054200542,
      "step": 11170,
      "training_loss": 5.8818769454956055
    },
    {
      "epoch": 0.6054742547425475,
      "step": 11171,
      "training_loss": 6.710631370544434
    },
    {
      "epoch": 0.6055284552845528,
      "grad_norm": 24.408185958862305,
      "learning_rate": 1e-05,
      "loss": 6.37,
      "step": 11172
    },
    {
      "epoch": 0.6055284552845528,
      "step": 11172,
      "training_loss": 5.818498611450195
    },
    {
      "epoch": 0.6055826558265582,
      "step": 11173,
      "training_loss": 7.234785079956055
    },
    {
      "epoch": 0.6056368563685637,
      "step": 11174,
      "training_loss": 5.418095588684082
    },
    {
      "epoch": 0.6056910569105691,
      "step": 11175,
      "training_loss": 6.615765571594238
    },
    {
      "epoch": 0.6057452574525746,
      "grad_norm": 53.746063232421875,
      "learning_rate": 1e-05,
      "loss": 6.2718,
      "step": 11176
    },
    {
      "epoch": 0.6057452574525746,
      "step": 11176,
      "training_loss": 5.2076029777526855
    },
    {
      "epoch": 0.6057994579945799,
      "step": 11177,
      "training_loss": 6.290706634521484
    },
    {
      "epoch": 0.6058536585365853,
      "step": 11178,
      "training_loss": 5.801238536834717
    },
    {
      "epoch": 0.6059078590785908,
      "step": 11179,
      "training_loss": 5.6558990478515625
    },
    {
      "epoch": 0.6059620596205962,
      "grad_norm": 28.075519561767578,
      "learning_rate": 1e-05,
      "loss": 5.7389,
      "step": 11180
    },
    {
      "epoch": 0.6059620596205962,
      "step": 11180,
      "training_loss": 6.400613784790039
    },
    {
      "epoch": 0.6060162601626016,
      "step": 11181,
      "training_loss": 5.955056190490723
    },
    {
      "epoch": 0.606070460704607,
      "step": 11182,
      "training_loss": 6.921067237854004
    },
    {
      "epoch": 0.6061246612466125,
      "step": 11183,
      "training_loss": 3.492403984069824
    },
    {
      "epoch": 0.6061788617886179,
      "grad_norm": 32.03846740722656,
      "learning_rate": 1e-05,
      "loss": 5.6923,
      "step": 11184
    },
    {
      "epoch": 0.6061788617886179,
      "step": 11184,
      "training_loss": 5.043980598449707
    },
    {
      "epoch": 0.6062330623306234,
      "step": 11185,
      "training_loss": 6.554940223693848
    },
    {
      "epoch": 0.6062872628726287,
      "step": 11186,
      "training_loss": 7.4737772941589355
    },
    {
      "epoch": 0.6063414634146341,
      "step": 11187,
      "training_loss": 5.502895355224609
    },
    {
      "epoch": 0.6063956639566396,
      "grad_norm": 23.64360237121582,
      "learning_rate": 1e-05,
      "loss": 6.1439,
      "step": 11188
    },
    {
      "epoch": 0.6063956639566396,
      "step": 11188,
      "training_loss": 7.110880374908447
    },
    {
      "epoch": 0.606449864498645,
      "step": 11189,
      "training_loss": 6.012363433837891
    },
    {
      "epoch": 0.6065040650406504,
      "step": 11190,
      "training_loss": 3.9696340560913086
    },
    {
      "epoch": 0.6065582655826558,
      "step": 11191,
      "training_loss": 5.86651611328125
    },
    {
      "epoch": 0.6066124661246612,
      "grad_norm": 29.538890838623047,
      "learning_rate": 1e-05,
      "loss": 5.7398,
      "step": 11192
    },
    {
      "epoch": 0.6066124661246612,
      "step": 11192,
      "training_loss": 7.111260414123535
    },
    {
      "epoch": 0.6066666666666667,
      "step": 11193,
      "training_loss": 6.243556976318359
    },
    {
      "epoch": 0.6067208672086721,
      "step": 11194,
      "training_loss": 3.752134084701538
    },
    {
      "epoch": 0.6067750677506775,
      "step": 11195,
      "training_loss": 5.929152488708496
    },
    {
      "epoch": 0.6068292682926829,
      "grad_norm": 30.68099594116211,
      "learning_rate": 1e-05,
      "loss": 5.759,
      "step": 11196
    },
    {
      "epoch": 0.6068292682926829,
      "step": 11196,
      "training_loss": 6.936254024505615
    },
    {
      "epoch": 0.6068834688346884,
      "step": 11197,
      "training_loss": 6.843874454498291
    },
    {
      "epoch": 0.6069376693766938,
      "step": 11198,
      "training_loss": 6.964770317077637
    },
    {
      "epoch": 0.6069918699186991,
      "step": 11199,
      "training_loss": 7.376611232757568
    },
    {
      "epoch": 0.6070460704607046,
      "grad_norm": 46.50511169433594,
      "learning_rate": 1e-05,
      "loss": 7.0304,
      "step": 11200
    },
    {
      "epoch": 0.6070460704607046,
      "step": 11200,
      "training_loss": 7.183384895324707
    },
    {
      "epoch": 0.60710027100271,
      "step": 11201,
      "training_loss": 7.443504333496094
    },
    {
      "epoch": 0.6071544715447155,
      "step": 11202,
      "training_loss": 6.876457691192627
    },
    {
      "epoch": 0.6072086720867209,
      "step": 11203,
      "training_loss": 6.16873836517334
    },
    {
      "epoch": 0.6072628726287262,
      "grad_norm": 45.977745056152344,
      "learning_rate": 1e-05,
      "loss": 6.918,
      "step": 11204
    },
    {
      "epoch": 0.6072628726287262,
      "step": 11204,
      "training_loss": 6.689276218414307
    },
    {
      "epoch": 0.6073170731707317,
      "step": 11205,
      "training_loss": 4.460056304931641
    },
    {
      "epoch": 0.6073712737127371,
      "step": 11206,
      "training_loss": 6.421581268310547
    },
    {
      "epoch": 0.6074254742547426,
      "step": 11207,
      "training_loss": 7.392500877380371
    },
    {
      "epoch": 0.6074796747967479,
      "grad_norm": 30.580488204956055,
      "learning_rate": 1e-05,
      "loss": 6.2409,
      "step": 11208
    },
    {
      "epoch": 0.6074796747967479,
      "step": 11208,
      "training_loss": 5.146912574768066
    },
    {
      "epoch": 0.6075338753387534,
      "step": 11209,
      "training_loss": 5.7571306228637695
    },
    {
      "epoch": 0.6075880758807588,
      "step": 11210,
      "training_loss": 9.073530197143555
    },
    {
      "epoch": 0.6076422764227642,
      "step": 11211,
      "training_loss": 3.5741138458251953
    },
    {
      "epoch": 0.6076964769647697,
      "grad_norm": 32.482051849365234,
      "learning_rate": 1e-05,
      "loss": 5.8879,
      "step": 11212
    },
    {
      "epoch": 0.6076964769647697,
      "step": 11212,
      "training_loss": 6.732844829559326
    },
    {
      "epoch": 0.607750677506775,
      "step": 11213,
      "training_loss": 7.417181491851807
    },
    {
      "epoch": 0.6078048780487805,
      "step": 11214,
      "training_loss": 6.815167427062988
    },
    {
      "epoch": 0.6078590785907859,
      "step": 11215,
      "training_loss": 6.950540542602539
    },
    {
      "epoch": 0.6079132791327914,
      "grad_norm": 17.182924270629883,
      "learning_rate": 1e-05,
      "loss": 6.9789,
      "step": 11216
    },
    {
      "epoch": 0.6079132791327914,
      "step": 11216,
      "training_loss": 6.477718353271484
    },
    {
      "epoch": 0.6079674796747967,
      "step": 11217,
      "training_loss": 6.643585681915283
    },
    {
      "epoch": 0.6080216802168021,
      "step": 11218,
      "training_loss": 7.812399864196777
    },
    {
      "epoch": 0.6080758807588076,
      "step": 11219,
      "training_loss": 6.245311737060547
    },
    {
      "epoch": 0.608130081300813,
      "grad_norm": 28.52439308166504,
      "learning_rate": 1e-05,
      "loss": 6.7948,
      "step": 11220
    },
    {
      "epoch": 0.608130081300813,
      "step": 11220,
      "training_loss": 6.402613162994385
    },
    {
      "epoch": 0.6081842818428185,
      "step": 11221,
      "training_loss": 6.639903545379639
    },
    {
      "epoch": 0.6082384823848238,
      "step": 11222,
      "training_loss": 5.796301364898682
    },
    {
      "epoch": 0.6082926829268293,
      "step": 11223,
      "training_loss": 7.1629319190979
    },
    {
      "epoch": 0.6083468834688347,
      "grad_norm": 32.73838806152344,
      "learning_rate": 1e-05,
      "loss": 6.5004,
      "step": 11224
    },
    {
      "epoch": 0.6083468834688347,
      "step": 11224,
      "training_loss": 8.80589485168457
    },
    {
      "epoch": 0.6084010840108401,
      "step": 11225,
      "training_loss": 8.146546363830566
    },
    {
      "epoch": 0.6084552845528455,
      "step": 11226,
      "training_loss": 7.365365505218506
    },
    {
      "epoch": 0.6085094850948509,
      "step": 11227,
      "training_loss": 5.216592311859131
    },
    {
      "epoch": 0.6085636856368564,
      "grad_norm": 34.2731819152832,
      "learning_rate": 1e-05,
      "loss": 7.3836,
      "step": 11228
    },
    {
      "epoch": 0.6085636856368564,
      "step": 11228,
      "training_loss": 7.014476299285889
    },
    {
      "epoch": 0.6086178861788618,
      "step": 11229,
      "training_loss": 7.646817207336426
    },
    {
      "epoch": 0.6086720867208673,
      "step": 11230,
      "training_loss": 4.6355366706848145
    },
    {
      "epoch": 0.6087262872628726,
      "step": 11231,
      "training_loss": 5.363553047180176
    },
    {
      "epoch": 0.608780487804878,
      "grad_norm": 27.26681900024414,
      "learning_rate": 1e-05,
      "loss": 6.1651,
      "step": 11232
    },
    {
      "epoch": 0.608780487804878,
      "step": 11232,
      "training_loss": 6.6459808349609375
    },
    {
      "epoch": 0.6088346883468835,
      "step": 11233,
      "training_loss": 5.915040493011475
    },
    {
      "epoch": 0.6088888888888889,
      "step": 11234,
      "training_loss": 6.742127895355225
    },
    {
      "epoch": 0.6089430894308943,
      "step": 11235,
      "training_loss": 7.124571800231934
    },
    {
      "epoch": 0.6089972899728997,
      "grad_norm": 27.36038589477539,
      "learning_rate": 1e-05,
      "loss": 6.6069,
      "step": 11236
    },
    {
      "epoch": 0.6089972899728997,
      "step": 11236,
      "training_loss": 5.662044048309326
    },
    {
      "epoch": 0.6090514905149051,
      "step": 11237,
      "training_loss": 6.8114237785339355
    },
    {
      "epoch": 0.6091056910569106,
      "step": 11238,
      "training_loss": 8.750863075256348
    },
    {
      "epoch": 0.609159891598916,
      "step": 11239,
      "training_loss": 7.9501729011535645
    },
    {
      "epoch": 0.6092140921409214,
      "grad_norm": 28.623126983642578,
      "learning_rate": 1e-05,
      "loss": 7.2936,
      "step": 11240
    },
    {
      "epoch": 0.6092140921409214,
      "step": 11240,
      "training_loss": 5.928312301635742
    },
    {
      "epoch": 0.6092682926829268,
      "step": 11241,
      "training_loss": 7.001163959503174
    },
    {
      "epoch": 0.6093224932249323,
      "step": 11242,
      "training_loss": 6.664824485778809
    },
    {
      "epoch": 0.6093766937669377,
      "step": 11243,
      "training_loss": 6.761682987213135
    },
    {
      "epoch": 0.609430894308943,
      "grad_norm": 61.847042083740234,
      "learning_rate": 1e-05,
      "loss": 6.589,
      "step": 11244
    },
    {
      "epoch": 0.609430894308943,
      "step": 11244,
      "training_loss": 6.911077499389648
    },
    {
      "epoch": 0.6094850948509485,
      "step": 11245,
      "training_loss": 5.638232707977295
    },
    {
      "epoch": 0.6095392953929539,
      "step": 11246,
      "training_loss": 6.52617073059082
    },
    {
      "epoch": 0.6095934959349594,
      "step": 11247,
      "training_loss": 6.392491817474365
    },
    {
      "epoch": 0.6096476964769648,
      "grad_norm": 29.211414337158203,
      "learning_rate": 1e-05,
      "loss": 6.367,
      "step": 11248
    },
    {
      "epoch": 0.6096476964769648,
      "step": 11248,
      "training_loss": 5.275373458862305
    },
    {
      "epoch": 0.6097018970189702,
      "step": 11249,
      "training_loss": 6.5089921951293945
    },
    {
      "epoch": 0.6097560975609756,
      "step": 11250,
      "training_loss": 7.675992488861084
    },
    {
      "epoch": 0.609810298102981,
      "step": 11251,
      "training_loss": 6.947501182556152
    },
    {
      "epoch": 0.6098644986449865,
      "grad_norm": 17.48905372619629,
      "learning_rate": 1e-05,
      "loss": 6.602,
      "step": 11252
    },
    {
      "epoch": 0.6098644986449865,
      "step": 11252,
      "training_loss": 7.220412254333496
    },
    {
      "epoch": 0.6099186991869918,
      "step": 11253,
      "training_loss": 6.699283123016357
    },
    {
      "epoch": 0.6099728997289973,
      "step": 11254,
      "training_loss": 6.433595180511475
    },
    {
      "epoch": 0.6100271002710027,
      "step": 11255,
      "training_loss": 4.99775505065918
    },
    {
      "epoch": 0.6100813008130082,
      "grad_norm": 21.1135311126709,
      "learning_rate": 1e-05,
      "loss": 6.3378,
      "step": 11256
    },
    {
      "epoch": 0.6100813008130082,
      "step": 11256,
      "training_loss": 7.210528373718262
    },
    {
      "epoch": 0.6101355013550136,
      "step": 11257,
      "training_loss": 6.70083475112915
    },
    {
      "epoch": 0.6101897018970189,
      "step": 11258,
      "training_loss": 6.478548049926758
    },
    {
      "epoch": 0.6102439024390244,
      "step": 11259,
      "training_loss": 6.95396089553833
    },
    {
      "epoch": 0.6102981029810298,
      "grad_norm": 48.264259338378906,
      "learning_rate": 1e-05,
      "loss": 6.836,
      "step": 11260
    },
    {
      "epoch": 0.6102981029810298,
      "step": 11260,
      "training_loss": 6.467312335968018
    },
    {
      "epoch": 0.6103523035230353,
      "step": 11261,
      "training_loss": 6.568975448608398
    },
    {
      "epoch": 0.6104065040650406,
      "step": 11262,
      "training_loss": 7.7906084060668945
    },
    {
      "epoch": 0.610460704607046,
      "step": 11263,
      "training_loss": 7.278379917144775
    },
    {
      "epoch": 0.6105149051490515,
      "grad_norm": 42.62302780151367,
      "learning_rate": 1e-05,
      "loss": 7.0263,
      "step": 11264
    },
    {
      "epoch": 0.6105149051490515,
      "step": 11264,
      "training_loss": 8.276134490966797
    },
    {
      "epoch": 0.6105691056910569,
      "step": 11265,
      "training_loss": 6.663234233856201
    },
    {
      "epoch": 0.6106233062330624,
      "step": 11266,
      "training_loss": 6.4208149909973145
    },
    {
      "epoch": 0.6106775067750677,
      "step": 11267,
      "training_loss": 6.809808254241943
    },
    {
      "epoch": 0.6107317073170732,
      "grad_norm": 35.38264465332031,
      "learning_rate": 1e-05,
      "loss": 7.0425,
      "step": 11268
    },
    {
      "epoch": 0.6107317073170732,
      "step": 11268,
      "training_loss": 8.35557746887207
    },
    {
      "epoch": 0.6107859078590786,
      "step": 11269,
      "training_loss": 6.543444633483887
    },
    {
      "epoch": 0.610840108401084,
      "step": 11270,
      "training_loss": 7.7660441398620605
    },
    {
      "epoch": 0.6108943089430894,
      "step": 11271,
      "training_loss": 5.178247928619385
    },
    {
      "epoch": 0.6109485094850948,
      "grad_norm": 25.233232498168945,
      "learning_rate": 1e-05,
      "loss": 6.9608,
      "step": 11272
    },
    {
      "epoch": 0.6109485094850948,
      "step": 11272,
      "training_loss": 7.078662872314453
    },
    {
      "epoch": 0.6110027100271003,
      "step": 11273,
      "training_loss": 6.627451419830322
    },
    {
      "epoch": 0.6110569105691057,
      "step": 11274,
      "training_loss": 6.655632495880127
    },
    {
      "epoch": 0.6111111111111112,
      "step": 11275,
      "training_loss": 7.80885648727417
    },
    {
      "epoch": 0.6111653116531165,
      "grad_norm": 21.70892906188965,
      "learning_rate": 1e-05,
      "loss": 7.0427,
      "step": 11276
    },
    {
      "epoch": 0.6111653116531165,
      "step": 11276,
      "training_loss": 6.768720626831055
    },
    {
      "epoch": 0.6112195121951219,
      "step": 11277,
      "training_loss": 6.723751544952393
    },
    {
      "epoch": 0.6112737127371274,
      "step": 11278,
      "training_loss": 7.539618492126465
    },
    {
      "epoch": 0.6113279132791328,
      "step": 11279,
      "training_loss": 6.713935375213623
    },
    {
      "epoch": 0.6113821138211382,
      "grad_norm": 47.264644622802734,
      "learning_rate": 1e-05,
      "loss": 6.9365,
      "step": 11280
    },
    {
      "epoch": 0.6113821138211382,
      "step": 11280,
      "training_loss": 6.303622245788574
    },
    {
      "epoch": 0.6114363143631436,
      "step": 11281,
      "training_loss": 7.595890045166016
    },
    {
      "epoch": 0.611490514905149,
      "step": 11282,
      "training_loss": 8.824200630187988
    },
    {
      "epoch": 0.6115447154471545,
      "step": 11283,
      "training_loss": 7.023214340209961
    },
    {
      "epoch": 0.6115989159891599,
      "grad_norm": 23.584877014160156,
      "learning_rate": 1e-05,
      "loss": 7.4367,
      "step": 11284
    },
    {
      "epoch": 0.6115989159891599,
      "step": 11284,
      "training_loss": 7.041236400604248
    },
    {
      "epoch": 0.6116531165311653,
      "step": 11285,
      "training_loss": 5.738922119140625
    },
    {
      "epoch": 0.6117073170731707,
      "step": 11286,
      "training_loss": 7.04403829574585
    },
    {
      "epoch": 0.6117615176151762,
      "step": 11287,
      "training_loss": 6.475838661193848
    },
    {
      "epoch": 0.6118157181571816,
      "grad_norm": 39.53059768676758,
      "learning_rate": 1e-05,
      "loss": 6.575,
      "step": 11288
    },
    {
      "epoch": 0.6118157181571816,
      "step": 11288,
      "training_loss": 6.496584892272949
    },
    {
      "epoch": 0.6118699186991869,
      "step": 11289,
      "training_loss": 6.644057750701904
    },
    {
      "epoch": 0.6119241192411924,
      "step": 11290,
      "training_loss": 6.992630481719971
    },
    {
      "epoch": 0.6119783197831978,
      "step": 11291,
      "training_loss": 7.3552350997924805
    },
    {
      "epoch": 0.6120325203252033,
      "grad_norm": 26.70940589904785,
      "learning_rate": 1e-05,
      "loss": 6.8721,
      "step": 11292
    },
    {
      "epoch": 0.6120325203252033,
      "step": 11292,
      "training_loss": 6.8657989501953125
    },
    {
      "epoch": 0.6120867208672087,
      "step": 11293,
      "training_loss": 5.419997692108154
    },
    {
      "epoch": 0.6121409214092141,
      "step": 11294,
      "training_loss": 5.801185607910156
    },
    {
      "epoch": 0.6121951219512195,
      "step": 11295,
      "training_loss": 5.838531970977783
    },
    {
      "epoch": 0.612249322493225,
      "grad_norm": 23.09028434753418,
      "learning_rate": 1e-05,
      "loss": 5.9814,
      "step": 11296
    },
    {
      "epoch": 0.612249322493225,
      "step": 11296,
      "training_loss": 7.647808074951172
    },
    {
      "epoch": 0.6123035230352304,
      "step": 11297,
      "training_loss": 6.8108391761779785
    },
    {
      "epoch": 0.6123577235772357,
      "step": 11298,
      "training_loss": 8.02306079864502
    },
    {
      "epoch": 0.6124119241192412,
      "step": 11299,
      "training_loss": 5.723076343536377
    },
    {
      "epoch": 0.6124661246612466,
      "grad_norm": 33.0677604675293,
      "learning_rate": 1e-05,
      "loss": 7.0512,
      "step": 11300
    },
    {
      "epoch": 0.6124661246612466,
      "step": 11300,
      "training_loss": 7.636668682098389
    },
    {
      "epoch": 0.6125203252032521,
      "step": 11301,
      "training_loss": 6.755480766296387
    },
    {
      "epoch": 0.6125745257452575,
      "step": 11302,
      "training_loss": 5.89705753326416
    },
    {
      "epoch": 0.6126287262872628,
      "step": 11303,
      "training_loss": 6.818777561187744
    },
    {
      "epoch": 0.6126829268292683,
      "grad_norm": 29.24814224243164,
      "learning_rate": 1e-05,
      "loss": 6.777,
      "step": 11304
    },
    {
      "epoch": 0.6126829268292683,
      "step": 11304,
      "training_loss": 7.734655857086182
    },
    {
      "epoch": 0.6127371273712737,
      "step": 11305,
      "training_loss": 6.335875034332275
    },
    {
      "epoch": 0.6127913279132792,
      "step": 11306,
      "training_loss": 6.5225830078125
    },
    {
      "epoch": 0.6128455284552845,
      "step": 11307,
      "training_loss": 6.633366107940674
    },
    {
      "epoch": 0.61289972899729,
      "grad_norm": 20.891796112060547,
      "learning_rate": 1e-05,
      "loss": 6.8066,
      "step": 11308
    },
    {
      "epoch": 0.61289972899729,
      "step": 11308,
      "training_loss": 5.564530372619629
    },
    {
      "epoch": 0.6129539295392954,
      "step": 11309,
      "training_loss": 6.226563453674316
    },
    {
      "epoch": 0.6130081300813008,
      "step": 11310,
      "training_loss": 6.35615348815918
    },
    {
      "epoch": 0.6130623306233063,
      "step": 11311,
      "training_loss": 6.56524658203125
    },
    {
      "epoch": 0.6131165311653116,
      "grad_norm": 29.677932739257812,
      "learning_rate": 1e-05,
      "loss": 6.1781,
      "step": 11312
    },
    {
      "epoch": 0.6131165311653116,
      "step": 11312,
      "training_loss": 7.740209102630615
    },
    {
      "epoch": 0.6131707317073171,
      "step": 11313,
      "training_loss": 6.042128086090088
    },
    {
      "epoch": 0.6132249322493225,
      "step": 11314,
      "training_loss": 4.316457748413086
    },
    {
      "epoch": 0.613279132791328,
      "step": 11315,
      "training_loss": 5.491030693054199
    },
    {
      "epoch": 0.6133333333333333,
      "grad_norm": 29.576078414916992,
      "learning_rate": 1e-05,
      "loss": 5.8975,
      "step": 11316
    },
    {
      "epoch": 0.6133333333333333,
      "step": 11316,
      "training_loss": 6.699967861175537
    },
    {
      "epoch": 0.6133875338753387,
      "step": 11317,
      "training_loss": 6.452723503112793
    },
    {
      "epoch": 0.6134417344173442,
      "step": 11318,
      "training_loss": 7.387274265289307
    },
    {
      "epoch": 0.6134959349593496,
      "step": 11319,
      "training_loss": 6.888917922973633
    },
    {
      "epoch": 0.6135501355013551,
      "grad_norm": 32.25921630859375,
      "learning_rate": 1e-05,
      "loss": 6.8572,
      "step": 11320
    },
    {
      "epoch": 0.6135501355013551,
      "step": 11320,
      "training_loss": 7.7271599769592285
    },
    {
      "epoch": 0.6136043360433604,
      "step": 11321,
      "training_loss": 6.869986057281494
    },
    {
      "epoch": 0.6136585365853658,
      "step": 11322,
      "training_loss": 7.058345317840576
    },
    {
      "epoch": 0.6137127371273713,
      "step": 11323,
      "training_loss": 6.711979866027832
    },
    {
      "epoch": 0.6137669376693767,
      "grad_norm": 26.176889419555664,
      "learning_rate": 1e-05,
      "loss": 7.0919,
      "step": 11324
    },
    {
      "epoch": 0.6137669376693767,
      "step": 11324,
      "training_loss": 5.441460132598877
    },
    {
      "epoch": 0.6138211382113821,
      "step": 11325,
      "training_loss": 6.555063247680664
    },
    {
      "epoch": 0.6138753387533875,
      "step": 11326,
      "training_loss": 6.407551288604736
    },
    {
      "epoch": 0.613929539295393,
      "step": 11327,
      "training_loss": 8.479093551635742
    },
    {
      "epoch": 0.6139837398373984,
      "grad_norm": 41.8017692565918,
      "learning_rate": 1e-05,
      "loss": 6.7208,
      "step": 11328
    },
    {
      "epoch": 0.6139837398373984,
      "step": 11328,
      "training_loss": 8.638534545898438
    },
    {
      "epoch": 0.6140379403794038,
      "step": 11329,
      "training_loss": 6.704812049865723
    },
    {
      "epoch": 0.6140921409214092,
      "step": 11330,
      "training_loss": 5.16819953918457
    },
    {
      "epoch": 0.6141463414634146,
      "step": 11331,
      "training_loss": 5.4704203605651855
    },
    {
      "epoch": 0.6142005420054201,
      "grad_norm": 39.048866271972656,
      "learning_rate": 1e-05,
      "loss": 6.4955,
      "step": 11332
    },
    {
      "epoch": 0.6142005420054201,
      "step": 11332,
      "training_loss": 5.413432598114014
    },
    {
      "epoch": 0.6142547425474255,
      "step": 11333,
      "training_loss": 7.168967247009277
    },
    {
      "epoch": 0.6143089430894308,
      "step": 11334,
      "training_loss": 6.714488983154297
    },
    {
      "epoch": 0.6143631436314363,
      "step": 11335,
      "training_loss": 5.86995792388916
    },
    {
      "epoch": 0.6144173441734417,
      "grad_norm": 64.10388946533203,
      "learning_rate": 1e-05,
      "loss": 6.2917,
      "step": 11336
    },
    {
      "epoch": 0.6144173441734417,
      "step": 11336,
      "training_loss": 7.32692289352417
    },
    {
      "epoch": 0.6144715447154472,
      "step": 11337,
      "training_loss": 6.773665428161621
    },
    {
      "epoch": 0.6145257452574526,
      "step": 11338,
      "training_loss": 6.876765727996826
    },
    {
      "epoch": 0.614579945799458,
      "step": 11339,
      "training_loss": 6.3032755851745605
    },
    {
      "epoch": 0.6146341463414634,
      "grad_norm": 28.352487564086914,
      "learning_rate": 1e-05,
      "loss": 6.8202,
      "step": 11340
    },
    {
      "epoch": 0.6146341463414634,
      "step": 11340,
      "training_loss": 7.550464153289795
    },
    {
      "epoch": 0.6146883468834689,
      "step": 11341,
      "training_loss": 5.043428897857666
    },
    {
      "epoch": 0.6147425474254743,
      "step": 11342,
      "training_loss": 7.385724067687988
    },
    {
      "epoch": 0.6147967479674796,
      "step": 11343,
      "training_loss": 3.5865566730499268
    },
    {
      "epoch": 0.6148509485094851,
      "grad_norm": 34.023834228515625,
      "learning_rate": 1e-05,
      "loss": 5.8915,
      "step": 11344
    },
    {
      "epoch": 0.6148509485094851,
      "step": 11344,
      "training_loss": 5.4101409912109375
    },
    {
      "epoch": 0.6149051490514905,
      "step": 11345,
      "training_loss": 5.848629474639893
    },
    {
      "epoch": 0.614959349593496,
      "step": 11346,
      "training_loss": 6.231936931610107
    },
    {
      "epoch": 0.6150135501355014,
      "step": 11347,
      "training_loss": 6.972702980041504
    },
    {
      "epoch": 0.6150677506775067,
      "grad_norm": 23.260929107666016,
      "learning_rate": 1e-05,
      "loss": 6.1159,
      "step": 11348
    },
    {
      "epoch": 0.6150677506775067,
      "step": 11348,
      "training_loss": 5.41292667388916
    },
    {
      "epoch": 0.6151219512195122,
      "step": 11349,
      "training_loss": 7.562378406524658
    },
    {
      "epoch": 0.6151761517615176,
      "step": 11350,
      "training_loss": 6.329529285430908
    },
    {
      "epoch": 0.6152303523035231,
      "step": 11351,
      "training_loss": 7.996321201324463
    },
    {
      "epoch": 0.6152845528455284,
      "grad_norm": 38.92446517944336,
      "learning_rate": 1e-05,
      "loss": 6.8253,
      "step": 11352
    },
    {
      "epoch": 0.6152845528455284,
      "step": 11352,
      "training_loss": 7.149829387664795
    },
    {
      "epoch": 0.6153387533875339,
      "step": 11353,
      "training_loss": 5.866180896759033
    },
    {
      "epoch": 0.6153929539295393,
      "step": 11354,
      "training_loss": 6.5238447189331055
    },
    {
      "epoch": 0.6154471544715447,
      "step": 11355,
      "training_loss": 6.789317607879639
    },
    {
      "epoch": 0.6155013550135502,
      "grad_norm": 19.80809211730957,
      "learning_rate": 1e-05,
      "loss": 6.5823,
      "step": 11356
    },
    {
      "epoch": 0.6155013550135502,
      "step": 11356,
      "training_loss": 7.2432050704956055
    },
    {
      "epoch": 0.6155555555555555,
      "step": 11357,
      "training_loss": 7.723573207855225
    },
    {
      "epoch": 0.615609756097561,
      "step": 11358,
      "training_loss": 6.470171928405762
    },
    {
      "epoch": 0.6156639566395664,
      "step": 11359,
      "training_loss": 7.880538463592529
    },
    {
      "epoch": 0.6157181571815719,
      "grad_norm": 39.955596923828125,
      "learning_rate": 1e-05,
      "loss": 7.3294,
      "step": 11360
    },
    {
      "epoch": 0.6157181571815719,
      "step": 11360,
      "training_loss": 7.375846862792969
    },
    {
      "epoch": 0.6157723577235772,
      "step": 11361,
      "training_loss": 6.149341106414795
    },
    {
      "epoch": 0.6158265582655826,
      "step": 11362,
      "training_loss": 6.489469528198242
    },
    {
      "epoch": 0.6158807588075881,
      "step": 11363,
      "training_loss": 7.371108055114746
    },
    {
      "epoch": 0.6159349593495935,
      "grad_norm": 22.78110694885254,
      "learning_rate": 1e-05,
      "loss": 6.8464,
      "step": 11364
    },
    {
      "epoch": 0.6159349593495935,
      "step": 11364,
      "training_loss": 7.466135025024414
    },
    {
      "epoch": 0.615989159891599,
      "step": 11365,
      "training_loss": 6.901870250701904
    },
    {
      "epoch": 0.6160433604336043,
      "step": 11366,
      "training_loss": 7.144099712371826
    },
    {
      "epoch": 0.6160975609756097,
      "step": 11367,
      "training_loss": 6.562309741973877
    },
    {
      "epoch": 0.6161517615176152,
      "grad_norm": 26.26930046081543,
      "learning_rate": 1e-05,
      "loss": 7.0186,
      "step": 11368
    },
    {
      "epoch": 0.6161517615176152,
      "step": 11368,
      "training_loss": 6.659481048583984
    },
    {
      "epoch": 0.6162059620596206,
      "step": 11369,
      "training_loss": 6.7514190673828125
    },
    {
      "epoch": 0.616260162601626,
      "step": 11370,
      "training_loss": 6.889050483703613
    },
    {
      "epoch": 0.6163143631436314,
      "step": 11371,
      "training_loss": 7.457887649536133
    },
    {
      "epoch": 0.6163685636856369,
      "grad_norm": 32.93944549560547,
      "learning_rate": 1e-05,
      "loss": 6.9395,
      "step": 11372
    },
    {
      "epoch": 0.6163685636856369,
      "step": 11372,
      "training_loss": 6.600778102874756
    },
    {
      "epoch": 0.6164227642276423,
      "step": 11373,
      "training_loss": 6.660166263580322
    },
    {
      "epoch": 0.6164769647696478,
      "step": 11374,
      "training_loss": 5.915328502655029
    },
    {
      "epoch": 0.6165311653116531,
      "step": 11375,
      "training_loss": 6.538428783416748
    },
    {
      "epoch": 0.6165853658536585,
      "grad_norm": 19.925966262817383,
      "learning_rate": 1e-05,
      "loss": 6.4287,
      "step": 11376
    },
    {
      "epoch": 0.6165853658536585,
      "step": 11376,
      "training_loss": 6.400457859039307
    },
    {
      "epoch": 0.616639566395664,
      "step": 11377,
      "training_loss": 6.171314716339111
    },
    {
      "epoch": 0.6166937669376694,
      "step": 11378,
      "training_loss": 5.315052509307861
    },
    {
      "epoch": 0.6167479674796748,
      "step": 11379,
      "training_loss": 5.3396897315979
    },
    {
      "epoch": 0.6168021680216802,
      "grad_norm": 24.247392654418945,
      "learning_rate": 1e-05,
      "loss": 5.8066,
      "step": 11380
    },
    {
      "epoch": 0.6168021680216802,
      "step": 11380,
      "training_loss": 6.730795860290527
    },
    {
      "epoch": 0.6168563685636856,
      "step": 11381,
      "training_loss": 5.539433479309082
    },
    {
      "epoch": 0.6169105691056911,
      "step": 11382,
      "training_loss": 6.604463577270508
    },
    {
      "epoch": 0.6169647696476965,
      "step": 11383,
      "training_loss": 7.860656261444092
    },
    {
      "epoch": 0.6170189701897019,
      "grad_norm": 26.743085861206055,
      "learning_rate": 1e-05,
      "loss": 6.6838,
      "step": 11384
    },
    {
      "epoch": 0.6170189701897019,
      "step": 11384,
      "training_loss": 6.905025959014893
    },
    {
      "epoch": 0.6170731707317073,
      "step": 11385,
      "training_loss": 7.385547161102295
    },
    {
      "epoch": 0.6171273712737128,
      "step": 11386,
      "training_loss": 4.919545650482178
    },
    {
      "epoch": 0.6171815718157182,
      "step": 11387,
      "training_loss": 6.684041500091553
    },
    {
      "epoch": 0.6172357723577235,
      "grad_norm": 34.95923614501953,
      "learning_rate": 1e-05,
      "loss": 6.4735,
      "step": 11388
    },
    {
      "epoch": 0.6172357723577235,
      "step": 11388,
      "training_loss": 6.086111068725586
    },
    {
      "epoch": 0.617289972899729,
      "step": 11389,
      "training_loss": 10.608139991760254
    },
    {
      "epoch": 0.6173441734417344,
      "step": 11390,
      "training_loss": 8.819283485412598
    },
    {
      "epoch": 0.6173983739837399,
      "step": 11391,
      "training_loss": 6.525148391723633
    },
    {
      "epoch": 0.6174525745257453,
      "grad_norm": 32.84817886352539,
      "learning_rate": 1e-05,
      "loss": 8.0097,
      "step": 11392
    },
    {
      "epoch": 0.6174525745257453,
      "step": 11392,
      "training_loss": 5.800529479980469
    },
    {
      "epoch": 0.6175067750677506,
      "step": 11393,
      "training_loss": 5.222017765045166
    },
    {
      "epoch": 0.6175609756097561,
      "step": 11394,
      "training_loss": 7.031182289123535
    },
    {
      "epoch": 0.6176151761517615,
      "step": 11395,
      "training_loss": 5.246772766113281
    },
    {
      "epoch": 0.617669376693767,
      "grad_norm": 24.74188804626465,
      "learning_rate": 1e-05,
      "loss": 5.8251,
      "step": 11396
    },
    {
      "epoch": 0.617669376693767,
      "step": 11396,
      "training_loss": 7.470844268798828
    },
    {
      "epoch": 0.6177235772357723,
      "step": 11397,
      "training_loss": 7.243969917297363
    },
    {
      "epoch": 0.6177777777777778,
      "step": 11398,
      "training_loss": 5.692907810211182
    },
    {
      "epoch": 0.6178319783197832,
      "step": 11399,
      "training_loss": 6.4959235191345215
    },
    {
      "epoch": 0.6178861788617886,
      "grad_norm": 22.123445510864258,
      "learning_rate": 1e-05,
      "loss": 6.7259,
      "step": 11400
    },
    {
      "epoch": 0.6178861788617886,
      "step": 11400,
      "training_loss": 5.66618537902832
    },
    {
      "epoch": 0.6179403794037941,
      "step": 11401,
      "training_loss": 5.611722469329834
    },
    {
      "epoch": 0.6179945799457994,
      "step": 11402,
      "training_loss": 7.868722915649414
    },
    {
      "epoch": 0.6180487804878049,
      "step": 11403,
      "training_loss": 6.5907158851623535
    },
    {
      "epoch": 0.6181029810298103,
      "grad_norm": 32.301387786865234,
      "learning_rate": 1e-05,
      "loss": 6.4343,
      "step": 11404
    },
    {
      "epoch": 0.6181029810298103,
      "step": 11404,
      "training_loss": 6.536569118499756
    },
    {
      "epoch": 0.6181571815718158,
      "step": 11405,
      "training_loss": 7.067713260650635
    },
    {
      "epoch": 0.6182113821138211,
      "step": 11406,
      "training_loss": 7.110666275024414
    },
    {
      "epoch": 0.6182655826558265,
      "step": 11407,
      "training_loss": 6.465622901916504
    },
    {
      "epoch": 0.618319783197832,
      "grad_norm": 21.40811538696289,
      "learning_rate": 1e-05,
      "loss": 6.7951,
      "step": 11408
    },
    {
      "epoch": 0.618319783197832,
      "step": 11408,
      "training_loss": 6.641366958618164
    },
    {
      "epoch": 0.6183739837398374,
      "step": 11409,
      "training_loss": 7.417462348937988
    },
    {
      "epoch": 0.6184281842818428,
      "step": 11410,
      "training_loss": 4.870222091674805
    },
    {
      "epoch": 0.6184823848238482,
      "step": 11411,
      "training_loss": 6.760310173034668
    },
    {
      "epoch": 0.6185365853658537,
      "grad_norm": 31.947463989257812,
      "learning_rate": 1e-05,
      "loss": 6.4223,
      "step": 11412
    },
    {
      "epoch": 0.6185365853658537,
      "step": 11412,
      "training_loss": 6.739415645599365
    },
    {
      "epoch": 0.6185907859078591,
      "step": 11413,
      "training_loss": 6.626222133636475
    },
    {
      "epoch": 0.6186449864498645,
      "step": 11414,
      "training_loss": 6.477309226989746
    },
    {
      "epoch": 0.6186991869918699,
      "step": 11415,
      "training_loss": 6.343752861022949
    },
    {
      "epoch": 0.6187533875338753,
      "grad_norm": 23.466960906982422,
      "learning_rate": 1e-05,
      "loss": 6.5467,
      "step": 11416
    },
    {
      "epoch": 0.6187533875338753,
      "step": 11416,
      "training_loss": 7.153983116149902
    },
    {
      "epoch": 0.6188075880758808,
      "step": 11417,
      "training_loss": 6.6438307762146
    },
    {
      "epoch": 0.6188617886178862,
      "step": 11418,
      "training_loss": 5.20931339263916
    },
    {
      "epoch": 0.6189159891598915,
      "step": 11419,
      "training_loss": 5.469620227813721
    },
    {
      "epoch": 0.618970189701897,
      "grad_norm": 19.306188583374023,
      "learning_rate": 1e-05,
      "loss": 6.1192,
      "step": 11420
    },
    {
      "epoch": 0.618970189701897,
      "step": 11420,
      "training_loss": 7.655539512634277
    },
    {
      "epoch": 0.6190243902439024,
      "step": 11421,
      "training_loss": 6.905930519104004
    },
    {
      "epoch": 0.6190785907859079,
      "step": 11422,
      "training_loss": 5.962951183319092
    },
    {
      "epoch": 0.6191327913279133,
      "step": 11423,
      "training_loss": 8.2465238571167
    },
    {
      "epoch": 0.6191869918699187,
      "grad_norm": 29.833955764770508,
      "learning_rate": 1e-05,
      "loss": 7.1927,
      "step": 11424
    },
    {
      "epoch": 0.6191869918699187,
      "step": 11424,
      "training_loss": 7.494314193725586
    },
    {
      "epoch": 0.6192411924119241,
      "step": 11425,
      "training_loss": 5.940314769744873
    },
    {
      "epoch": 0.6192953929539295,
      "step": 11426,
      "training_loss": 7.606244087219238
    },
    {
      "epoch": 0.619349593495935,
      "step": 11427,
      "training_loss": 4.957748889923096
    },
    {
      "epoch": 0.6194037940379403,
      "grad_norm": 43.71128463745117,
      "learning_rate": 1e-05,
      "loss": 6.4997,
      "step": 11428
    },
    {
      "epoch": 0.6194037940379403,
      "step": 11428,
      "training_loss": 6.904810905456543
    },
    {
      "epoch": 0.6194579945799458,
      "step": 11429,
      "training_loss": 7.07820987701416
    },
    {
      "epoch": 0.6195121951219512,
      "step": 11430,
      "training_loss": 7.20499324798584
    },
    {
      "epoch": 0.6195663956639567,
      "step": 11431,
      "training_loss": 2.9351491928100586
    },
    {
      "epoch": 0.6196205962059621,
      "grad_norm": NaN,
      "learning_rate": 1e-05,
      "loss": 6.0308,
      "step": 11432
    },
    {
      "epoch": 0.6196205962059621,
      "step": 11432,
      "training_loss": 6.9413042068481445
    },
    {
      "epoch": 0.6196747967479674,
      "step": 11433,
      "training_loss": 6.2411699295043945
    },
    {
      "epoch": 0.6197289972899729,
      "step": 11434,
      "training_loss": 6.051945686340332
    },
    {
      "epoch": 0.6197831978319783,
      "step": 11435,
      "training_loss": 5.8712568283081055
    },
    {
      "epoch": 0.6198373983739838,
      "grad_norm": 31.292396545410156,
      "learning_rate": 1e-05,
      "loss": 6.2764,
      "step": 11436
    },
    {
      "epoch": 0.6198373983739838,
      "step": 11436,
      "training_loss": 7.141268730163574
    },
    {
      "epoch": 0.6198915989159891,
      "step": 11437,
      "training_loss": 6.752528667449951
    },
    {
      "epoch": 0.6199457994579946,
      "step": 11438,
      "training_loss": 6.805010795593262
    },
    {
      "epoch": 0.62,
      "step": 11439,
      "training_loss": 6.319882392883301
    },
    {
      "epoch": 0.6200542005420054,
      "grad_norm": 41.46897506713867,
      "learning_rate": 1e-05,
      "loss": 6.7547,
      "step": 11440
    },
    {
      "epoch": 0.6200542005420054,
      "step": 11440,
      "training_loss": 6.824941158294678
    },
    {
      "epoch": 0.6201084010840109,
      "step": 11441,
      "training_loss": 7.562222957611084
    },
    {
      "epoch": 0.6201626016260162,
      "step": 11442,
      "training_loss": 7.414915561676025
    },
    {
      "epoch": 0.6202168021680217,
      "step": 11443,
      "training_loss": 4.78086519241333
    },
    {
      "epoch": 0.6202710027100271,
      "grad_norm": 21.519500732421875,
      "learning_rate": 1e-05,
      "loss": 6.6457,
      "step": 11444
    },
    {
      "epoch": 0.6202710027100271,
      "step": 11444,
      "training_loss": 5.422993183135986
    },
    {
      "epoch": 0.6203252032520326,
      "step": 11445,
      "training_loss": 7.029477596282959
    },
    {
      "epoch": 0.6203794037940379,
      "step": 11446,
      "training_loss": 6.905835151672363
    },
    {
      "epoch": 0.6204336043360433,
      "step": 11447,
      "training_loss": 6.697786331176758
    },
    {
      "epoch": 0.6204878048780488,
      "grad_norm": 21.17983627319336,
      "learning_rate": 1e-05,
      "loss": 6.514,
      "step": 11448
    },
    {
      "epoch": 0.6204878048780488,
      "step": 11448,
      "training_loss": 3.2138311862945557
    },
    {
      "epoch": 0.6205420054200542,
      "step": 11449,
      "training_loss": 7.633329391479492
    },
    {
      "epoch": 0.6205962059620597,
      "step": 11450,
      "training_loss": 6.486152648925781
    },
    {
      "epoch": 0.620650406504065,
      "step": 11451,
      "training_loss": 7.725139617919922
    },
    {
      "epoch": 0.6207046070460704,
      "grad_norm": 63.305728912353516,
      "learning_rate": 1e-05,
      "loss": 6.2646,
      "step": 11452
    },
    {
      "epoch": 0.6207046070460704,
      "step": 11452,
      "training_loss": 7.839945316314697
    },
    {
      "epoch": 0.6207588075880759,
      "step": 11453,
      "training_loss": 6.783031463623047
    },
    {
      "epoch": 0.6208130081300813,
      "step": 11454,
      "training_loss": 5.130895614624023
    },
    {
      "epoch": 0.6208672086720867,
      "step": 11455,
      "training_loss": 8.278023719787598
    },
    {
      "epoch": 0.6209214092140921,
      "grad_norm": 31.493574142456055,
      "learning_rate": 1e-05,
      "loss": 7.008,
      "step": 11456
    },
    {
      "epoch": 0.6209214092140921,
      "step": 11456,
      "training_loss": 6.257554054260254
    },
    {
      "epoch": 0.6209756097560976,
      "step": 11457,
      "training_loss": 8.233624458312988
    },
    {
      "epoch": 0.621029810298103,
      "step": 11458,
      "training_loss": 8.270806312561035
    },
    {
      "epoch": 0.6210840108401084,
      "step": 11459,
      "training_loss": 5.534177303314209
    },
    {
      "epoch": 0.6211382113821138,
      "grad_norm": 28.535558700561523,
      "learning_rate": 1e-05,
      "loss": 7.074,
      "step": 11460
    },
    {
      "epoch": 0.6211382113821138,
      "step": 11460,
      "training_loss": 7.57560396194458
    },
    {
      "epoch": 0.6211924119241192,
      "step": 11461,
      "training_loss": 3.835627555847168
    },
    {
      "epoch": 0.6212466124661247,
      "step": 11462,
      "training_loss": 6.86619758605957
    },
    {
      "epoch": 0.6213008130081301,
      "step": 11463,
      "training_loss": 7.141982555389404
    },
    {
      "epoch": 0.6213550135501354,
      "grad_norm": 37.255828857421875,
      "learning_rate": 1e-05,
      "loss": 6.3549,
      "step": 11464
    },
    {
      "epoch": 0.6213550135501354,
      "step": 11464,
      "training_loss": 7.2726731300354
    },
    {
      "epoch": 0.6214092140921409,
      "step": 11465,
      "training_loss": 8.405336380004883
    },
    {
      "epoch": 0.6214634146341463,
      "step": 11466,
      "training_loss": 5.582237243652344
    },
    {
      "epoch": 0.6215176151761518,
      "step": 11467,
      "training_loss": 6.579826831817627
    },
    {
      "epoch": 0.6215718157181572,
      "grad_norm": 27.302186965942383,
      "learning_rate": 1e-05,
      "loss": 6.96,
      "step": 11468
    },
    {
      "epoch": 0.6215718157181572,
      "step": 11468,
      "training_loss": 6.643796920776367
    },
    {
      "epoch": 0.6216260162601626,
      "step": 11469,
      "training_loss": 6.271905899047852
    },
    {
      "epoch": 0.621680216802168,
      "step": 11470,
      "training_loss": 5.607721328735352
    },
    {
      "epoch": 0.6217344173441735,
      "step": 11471,
      "training_loss": 6.187300682067871
    },
    {
      "epoch": 0.6217886178861789,
      "grad_norm": 22.900409698486328,
      "learning_rate": 1e-05,
      "loss": 6.1777,
      "step": 11472
    },
    {
      "epoch": 0.6217886178861789,
      "step": 11472,
      "training_loss": 7.10373067855835
    },
    {
      "epoch": 0.6218428184281842,
      "step": 11473,
      "training_loss": 5.039839267730713
    },
    {
      "epoch": 0.6218970189701897,
      "step": 11474,
      "training_loss": 7.690942764282227
    },
    {
      "epoch": 0.6219512195121951,
      "step": 11475,
      "training_loss": 7.6645636558532715
    },
    {
      "epoch": 0.6220054200542006,
      "grad_norm": 38.05363845825195,
      "learning_rate": 1e-05,
      "loss": 6.8748,
      "step": 11476
    },
    {
      "epoch": 0.6220054200542006,
      "step": 11476,
      "training_loss": 5.859551906585693
    },
    {
      "epoch": 0.622059620596206,
      "step": 11477,
      "training_loss": 6.342041492462158
    },
    {
      "epoch": 0.6221138211382113,
      "step": 11478,
      "training_loss": 6.994059085845947
    },
    {
      "epoch": 0.6221680216802168,
      "step": 11479,
      "training_loss": 8.002752304077148
    },
    {
      "epoch": 0.6222222222222222,
      "grad_norm": 58.92805862426758,
      "learning_rate": 1e-05,
      "loss": 6.7996,
      "step": 11480
    },
    {
      "epoch": 0.6222222222222222,
      "step": 11480,
      "training_loss": 7.096755027770996
    },
    {
      "epoch": 0.6222764227642277,
      "step": 11481,
      "training_loss": 6.611555576324463
    },
    {
      "epoch": 0.622330623306233,
      "step": 11482,
      "training_loss": 7.363749027252197
    },
    {
      "epoch": 0.6223848238482385,
      "step": 11483,
      "training_loss": 6.256092071533203
    },
    {
      "epoch": 0.6224390243902439,
      "grad_norm": 26.842313766479492,
      "learning_rate": 1e-05,
      "loss": 6.832,
      "step": 11484
    },
    {
      "epoch": 0.6224390243902439,
      "step": 11484,
      "training_loss": 7.664494037628174
    },
    {
      "epoch": 0.6224932249322493,
      "step": 11485,
      "training_loss": 7.355506420135498
    },
    {
      "epoch": 0.6225474254742548,
      "step": 11486,
      "training_loss": 7.130784034729004
    },
    {
      "epoch": 0.6226016260162601,
      "step": 11487,
      "training_loss": 6.589031219482422
    },
    {
      "epoch": 0.6226558265582656,
      "grad_norm": 31.331823348999023,
      "learning_rate": 1e-05,
      "loss": 7.185,
      "step": 11488
    },
    {
      "epoch": 0.6226558265582656,
      "step": 11488,
      "training_loss": 6.542294979095459
    },
    {
      "epoch": 0.622710027100271,
      "step": 11489,
      "training_loss": 6.080667018890381
    },
    {
      "epoch": 0.6227642276422765,
      "step": 11490,
      "training_loss": 7.438118934631348
    },
    {
      "epoch": 0.6228184281842818,
      "step": 11491,
      "training_loss": 6.18211555480957
    },
    {
      "epoch": 0.6228726287262872,
      "grad_norm": 43.16499328613281,
      "learning_rate": 1e-05,
      "loss": 6.5608,
      "step": 11492
    },
    {
      "epoch": 0.6228726287262872,
      "step": 11492,
      "training_loss": 7.45780086517334
    },
    {
      "epoch": 0.6229268292682927,
      "step": 11493,
      "training_loss": 6.926546096801758
    },
    {
      "epoch": 0.6229810298102981,
      "step": 11494,
      "training_loss": 5.870646953582764
    },
    {
      "epoch": 0.6230352303523036,
      "step": 11495,
      "training_loss": 6.8589630126953125
    },
    {
      "epoch": 0.6230894308943089,
      "grad_norm": 15.570048332214355,
      "learning_rate": 1e-05,
      "loss": 6.7785,
      "step": 11496
    },
    {
      "epoch": 0.6230894308943089,
      "step": 11496,
      "training_loss": 6.724920749664307
    },
    {
      "epoch": 0.6231436314363143,
      "step": 11497,
      "training_loss": 6.9341278076171875
    },
    {
      "epoch": 0.6231978319783198,
      "step": 11498,
      "training_loss": 7.493228435516357
    },
    {
      "epoch": 0.6232520325203252,
      "step": 11499,
      "training_loss": 5.454426288604736
    },
    {
      "epoch": 0.6233062330623306,
      "grad_norm": 31.830642700195312,
      "learning_rate": 1e-05,
      "loss": 6.6517,
      "step": 11500
    },
    {
      "epoch": 0.6233062330623306,
      "step": 11500,
      "training_loss": 7.373132228851318
    },
    {
      "epoch": 0.623360433604336,
      "step": 11501,
      "training_loss": 5.351102352142334
    },
    {
      "epoch": 0.6234146341463415,
      "step": 11502,
      "training_loss": 6.826417446136475
    },
    {
      "epoch": 0.6234688346883469,
      "step": 11503,
      "training_loss": 7.135560989379883
    },
    {
      "epoch": 0.6235230352303524,
      "grad_norm": 24.611099243164062,
      "learning_rate": 1e-05,
      "loss": 6.6716,
      "step": 11504
    },
    {
      "epoch": 0.6235230352303524,
      "step": 11504,
      "training_loss": 5.470585823059082
    },
    {
      "epoch": 0.6235772357723577,
      "step": 11505,
      "training_loss": 5.930881500244141
    },
    {
      "epoch": 0.6236314363143631,
      "step": 11506,
      "training_loss": 6.559390544891357
    },
    {
      "epoch": 0.6236856368563686,
      "step": 11507,
      "training_loss": 7.606136798858643
    },
    {
      "epoch": 0.623739837398374,
      "grad_norm": 20.358566284179688,
      "learning_rate": 1e-05,
      "loss": 6.3917,
      "step": 11508
    },
    {
      "epoch": 0.623739837398374,
      "step": 11508,
      "training_loss": 7.260361671447754
    },
    {
      "epoch": 0.6237940379403794,
      "step": 11509,
      "training_loss": 7.117171287536621
    },
    {
      "epoch": 0.6238482384823848,
      "step": 11510,
      "training_loss": 6.310906410217285
    },
    {
      "epoch": 0.6239024390243902,
      "step": 11511,
      "training_loss": 6.793302059173584
    },
    {
      "epoch": 0.6239566395663957,
      "grad_norm": 18.46077537536621,
      "learning_rate": 1e-05,
      "loss": 6.8704,
      "step": 11512
    },
    {
      "epoch": 0.6239566395663957,
      "step": 11512,
      "training_loss": 6.983683109283447
    },
    {
      "epoch": 0.6240108401084011,
      "step": 11513,
      "training_loss": 7.3154120445251465
    },
    {
      "epoch": 0.6240650406504065,
      "step": 11514,
      "training_loss": 5.861965179443359
    },
    {
      "epoch": 0.6241192411924119,
      "step": 11515,
      "training_loss": 5.4448137283325195
    },
    {
      "epoch": 0.6241734417344174,
      "grad_norm": 23.00214195251465,
      "learning_rate": 1e-05,
      "loss": 6.4015,
      "step": 11516
    },
    {
      "epoch": 0.6241734417344174,
      "step": 11516,
      "training_loss": 6.342292785644531
    },
    {
      "epoch": 0.6242276422764228,
      "step": 11517,
      "training_loss": 5.050547122955322
    },
    {
      "epoch": 0.6242818428184281,
      "step": 11518,
      "training_loss": 6.624389171600342
    },
    {
      "epoch": 0.6243360433604336,
      "step": 11519,
      "training_loss": 7.28161096572876
    },
    {
      "epoch": 0.624390243902439,
      "grad_norm": 21.312402725219727,
      "learning_rate": 1e-05,
      "loss": 6.3247,
      "step": 11520
    },
    {
      "epoch": 0.624390243902439,
      "step": 11520,
      "training_loss": 6.81438684463501
    },
    {
      "epoch": 0.6244444444444445,
      "step": 11521,
      "training_loss": 6.046383857727051
    },
    {
      "epoch": 0.6244986449864499,
      "step": 11522,
      "training_loss": 7.120192527770996
    },
    {
      "epoch": 0.6245528455284552,
      "step": 11523,
      "training_loss": 6.566410064697266
    },
    {
      "epoch": 0.6246070460704607,
      "grad_norm": 27.912090301513672,
      "learning_rate": 1e-05,
      "loss": 6.6368,
      "step": 11524
    },
    {
      "epoch": 0.6246070460704607,
      "step": 11524,
      "training_loss": 3.386979818344116
    },
    {
      "epoch": 0.6246612466124661,
      "step": 11525,
      "training_loss": 5.407517433166504
    },
    {
      "epoch": 0.6247154471544716,
      "step": 11526,
      "training_loss": 6.758056640625
    },
    {
      "epoch": 0.6247696476964769,
      "step": 11527,
      "training_loss": 5.635386943817139
    },
    {
      "epoch": 0.6248238482384824,
      "grad_norm": 27.263317108154297,
      "learning_rate": 1e-05,
      "loss": 5.297,
      "step": 11528
    },
    {
      "epoch": 0.6248238482384824,
      "step": 11528,
      "training_loss": 5.52052116394043
    },
    {
      "epoch": 0.6248780487804878,
      "step": 11529,
      "training_loss": 6.978020191192627
    },
    {
      "epoch": 0.6249322493224932,
      "step": 11530,
      "training_loss": 6.656538963317871
    },
    {
      "epoch": 0.6249864498644987,
      "step": 11531,
      "training_loss": 3.4523122310638428
    },
    {
      "epoch": 0.625040650406504,
      "grad_norm": 34.59577178955078,
      "learning_rate": 1e-05,
      "loss": 5.6518,
      "step": 11532
    },
    {
      "epoch": 0.625040650406504,
      "step": 11532,
      "training_loss": 6.216303825378418
    },
    {
      "epoch": 0.6250948509485095,
      "step": 11533,
      "training_loss": 7.570906639099121
    },
    {
      "epoch": 0.6251490514905149,
      "step": 11534,
      "training_loss": 8.015138626098633
    },
    {
      "epoch": 0.6252032520325204,
      "step": 11535,
      "training_loss": 4.882587909698486
    },
    {
      "epoch": 0.6252574525745257,
      "grad_norm": 26.511127471923828,
      "learning_rate": 1e-05,
      "loss": 6.6712,
      "step": 11536
    },
    {
      "epoch": 0.6252574525745257,
      "step": 11536,
      "training_loss": 6.577565670013428
    },
    {
      "epoch": 0.6253116531165311,
      "step": 11537,
      "training_loss": 7.361900806427002
    },
    {
      "epoch": 0.6253658536585366,
      "step": 11538,
      "training_loss": 5.673123836517334
    },
    {
      "epoch": 0.625420054200542,
      "step": 11539,
      "training_loss": 6.523476600646973
    },
    {
      "epoch": 0.6254742547425475,
      "grad_norm": 29.904674530029297,
      "learning_rate": 1e-05,
      "loss": 6.534,
      "step": 11540
    },
    {
      "epoch": 0.6254742547425475,
      "step": 11540,
      "training_loss": 6.66992712020874
    },
    {
      "epoch": 0.6255284552845528,
      "step": 11541,
      "training_loss": 7.4056525230407715
    },
    {
      "epoch": 0.6255826558265583,
      "step": 11542,
      "training_loss": 6.377288341522217
    },
    {
      "epoch": 0.6256368563685637,
      "step": 11543,
      "training_loss": 6.185977935791016
    },
    {
      "epoch": 0.6256910569105691,
      "grad_norm": 33.30143356323242,
      "learning_rate": 1e-05,
      "loss": 6.6597,
      "step": 11544
    },
    {
      "epoch": 0.6256910569105691,
      "step": 11544,
      "training_loss": 6.797186374664307
    },
    {
      "epoch": 0.6257452574525745,
      "step": 11545,
      "training_loss": 7.276741981506348
    },
    {
      "epoch": 0.6257994579945799,
      "step": 11546,
      "training_loss": 7.194684028625488
    },
    {
      "epoch": 0.6258536585365854,
      "step": 11547,
      "training_loss": 5.834744930267334
    },
    {
      "epoch": 0.6259078590785908,
      "grad_norm": 21.539209365844727,
      "learning_rate": 1e-05,
      "loss": 6.7758,
      "step": 11548
    },
    {
      "epoch": 0.6259078590785908,
      "step": 11548,
      "training_loss": 5.381436824798584
    },
    {
      "epoch": 0.6259620596205963,
      "step": 11549,
      "training_loss": 5.592673301696777
    },
    {
      "epoch": 0.6260162601626016,
      "step": 11550,
      "training_loss": 6.813393592834473
    },
    {
      "epoch": 0.626070460704607,
      "step": 11551,
      "training_loss": 5.485158920288086
    },
    {
      "epoch": 0.6261246612466125,
      "grad_norm": 39.37210464477539,
      "learning_rate": 1e-05,
      "loss": 5.8182,
      "step": 11552
    },
    {
      "epoch": 0.6261246612466125,
      "step": 11552,
      "training_loss": 7.044721603393555
    },
    {
      "epoch": 0.6261788617886179,
      "step": 11553,
      "training_loss": 7.11760139465332
    },
    {
      "epoch": 0.6262330623306233,
      "step": 11554,
      "training_loss": 3.812392234802246
    },
    {
      "epoch": 0.6262872628726287,
      "step": 11555,
      "training_loss": 6.235739231109619
    },
    {
      "epoch": 0.6263414634146341,
      "grad_norm": 27.37174415588379,
      "learning_rate": 1e-05,
      "loss": 6.0526,
      "step": 11556
    },
    {
      "epoch": 0.6263414634146341,
      "step": 11556,
      "training_loss": 3.905977964401245
    },
    {
      "epoch": 0.6263956639566396,
      "step": 11557,
      "training_loss": 5.846651554107666
    },
    {
      "epoch": 0.626449864498645,
      "step": 11558,
      "training_loss": 8.077717781066895
    },
    {
      "epoch": 0.6265040650406504,
      "step": 11559,
      "training_loss": 5.686764240264893
    },
    {
      "epoch": 0.6265582655826558,
      "grad_norm": 46.82603073120117,
      "learning_rate": 1e-05,
      "loss": 5.8793,
      "step": 11560
    },
    {
      "epoch": 0.6265582655826558,
      "step": 11560,
      "training_loss": 7.305984020233154
    },
    {
      "epoch": 0.6266124661246613,
      "step": 11561,
      "training_loss": 7.545787811279297
    },
    {
      "epoch": 0.6266666666666667,
      "step": 11562,
      "training_loss": 7.195873260498047
    },
    {
      "epoch": 0.626720867208672,
      "step": 11563,
      "training_loss": 6.886990070343018
    },
    {
      "epoch": 0.6267750677506775,
      "grad_norm": 22.626081466674805,
      "learning_rate": 1e-05,
      "loss": 7.2337,
      "step": 11564
    },
    {
      "epoch": 0.6267750677506775,
      "step": 11564,
      "training_loss": 6.424182415008545
    },
    {
      "epoch": 0.6268292682926829,
      "step": 11565,
      "training_loss": 4.638602256774902
    },
    {
      "epoch": 0.6268834688346884,
      "step": 11566,
      "training_loss": 4.892691135406494
    },
    {
      "epoch": 0.6269376693766938,
      "step": 11567,
      "training_loss": 7.570157527923584
    },
    {
      "epoch": 0.6269918699186992,
      "grad_norm": 39.512123107910156,
      "learning_rate": 1e-05,
      "loss": 5.8814,
      "step": 11568
    },
    {
      "epoch": 0.6269918699186992,
      "step": 11568,
      "training_loss": 7.293168544769287
    },
    {
      "epoch": 0.6270460704607046,
      "step": 11569,
      "training_loss": 6.736209392547607
    },
    {
      "epoch": 0.62710027100271,
      "step": 11570,
      "training_loss": 7.775758266448975
    },
    {
      "epoch": 0.6271544715447155,
      "step": 11571,
      "training_loss": 6.9426045417785645
    },
    {
      "epoch": 0.6272086720867208,
      "grad_norm": 18.165447235107422,
      "learning_rate": 1e-05,
      "loss": 7.1869,
      "step": 11572
    },
    {
      "epoch": 0.6272086720867208,
      "step": 11572,
      "training_loss": 6.463873386383057
    },
    {
      "epoch": 0.6272628726287263,
      "step": 11573,
      "training_loss": 5.1314616203308105
    },
    {
      "epoch": 0.6273170731707317,
      "step": 11574,
      "training_loss": 6.907630443572998
    },
    {
      "epoch": 0.6273712737127372,
      "step": 11575,
      "training_loss": 4.854385852813721
    },
    {
      "epoch": 0.6274254742547426,
      "grad_norm": 26.0987548828125,
      "learning_rate": 1e-05,
      "loss": 5.8393,
      "step": 11576
    },
    {
      "epoch": 0.6274254742547426,
      "step": 11576,
      "training_loss": 6.606950283050537
    },
    {
      "epoch": 0.6274796747967479,
      "step": 11577,
      "training_loss": 7.254753112792969
    },
    {
      "epoch": 0.6275338753387534,
      "step": 11578,
      "training_loss": 4.366433620452881
    },
    {
      "epoch": 0.6275880758807588,
      "step": 11579,
      "training_loss": 3.7083940505981445
    },
    {
      "epoch": 0.6276422764227643,
      "grad_norm": 35.94243621826172,
      "learning_rate": 1e-05,
      "loss": 5.4841,
      "step": 11580
    },
    {
      "epoch": 0.6276422764227643,
      "step": 11580,
      "training_loss": 6.555232524871826
    },
    {
      "epoch": 0.6276964769647696,
      "step": 11581,
      "training_loss": 6.649840354919434
    },
    {
      "epoch": 0.627750677506775,
      "step": 11582,
      "training_loss": 6.795307636260986
    },
    {
      "epoch": 0.6278048780487805,
      "step": 11583,
      "training_loss": 6.3592529296875
    },
    {
      "epoch": 0.6278590785907859,
      "grad_norm": 30.199569702148438,
      "learning_rate": 1e-05,
      "loss": 6.5899,
      "step": 11584
    },
    {
      "epoch": 0.6278590785907859,
      "step": 11584,
      "training_loss": 6.327897548675537
    },
    {
      "epoch": 0.6279132791327914,
      "step": 11585,
      "training_loss": 7.387509346008301
    },
    {
      "epoch": 0.6279674796747967,
      "step": 11586,
      "training_loss": 6.040432453155518
    },
    {
      "epoch": 0.6280216802168022,
      "step": 11587,
      "training_loss": 7.09779691696167
    },
    {
      "epoch": 0.6280758807588076,
      "grad_norm": 18.437496185302734,
      "learning_rate": 1e-05,
      "loss": 6.7134,
      "step": 11588
    },
    {
      "epoch": 0.6280758807588076,
      "step": 11588,
      "training_loss": 7.080665111541748
    },
    {
      "epoch": 0.628130081300813,
      "step": 11589,
      "training_loss": 3.5524089336395264
    },
    {
      "epoch": 0.6281842818428184,
      "step": 11590,
      "training_loss": 7.7062668800354
    },
    {
      "epoch": 0.6282384823848238,
      "step": 11591,
      "training_loss": 7.710129737854004
    },
    {
      "epoch": 0.6282926829268293,
      "grad_norm": 54.00621795654297,
      "learning_rate": 1e-05,
      "loss": 6.5124,
      "step": 11592
    },
    {
      "epoch": 0.6282926829268293,
      "step": 11592,
      "training_loss": 6.0756754875183105
    },
    {
      "epoch": 0.6283468834688347,
      "step": 11593,
      "training_loss": 7.00514030456543
    },
    {
      "epoch": 0.6284010840108402,
      "step": 11594,
      "training_loss": 7.718172073364258
    },
    {
      "epoch": 0.6284552845528455,
      "step": 11595,
      "training_loss": 6.967085361480713
    },
    {
      "epoch": 0.6285094850948509,
      "grad_norm": 66.07813262939453,
      "learning_rate": 1e-05,
      "loss": 6.9415,
      "step": 11596
    },
    {
      "epoch": 0.6285094850948509,
      "step": 11596,
      "training_loss": 6.384727954864502
    },
    {
      "epoch": 0.6285636856368564,
      "step": 11597,
      "training_loss": 7.478121280670166
    },
    {
      "epoch": 0.6286178861788618,
      "step": 11598,
      "training_loss": 5.857002258300781
    },
    {
      "epoch": 0.6286720867208672,
      "step": 11599,
      "training_loss": 5.9838361740112305
    },
    {
      "epoch": 0.6287262872628726,
      "grad_norm": 23.66663932800293,
      "learning_rate": 1e-05,
      "loss": 6.4259,
      "step": 11600
    },
    {
      "epoch": 0.6287262872628726,
      "step": 11600,
      "training_loss": 6.611574172973633
    },
    {
      "epoch": 0.628780487804878,
      "step": 11601,
      "training_loss": 7.8097028732299805
    },
    {
      "epoch": 0.6288346883468835,
      "step": 11602,
      "training_loss": 6.945863723754883
    },
    {
      "epoch": 0.6288888888888889,
      "step": 11603,
      "training_loss": 7.657835483551025
    },
    {
      "epoch": 0.6289430894308943,
      "grad_norm": 27.820571899414062,
      "learning_rate": 1e-05,
      "loss": 7.2562,
      "step": 11604
    },
    {
      "epoch": 0.6289430894308943,
      "step": 11604,
      "training_loss": 3.6481354236602783
    },
    {
      "epoch": 0.6289972899728997,
      "step": 11605,
      "training_loss": 6.85271692276001
    },
    {
      "epoch": 0.6290514905149052,
      "step": 11606,
      "training_loss": 6.524293422698975
    },
    {
      "epoch": 0.6291056910569106,
      "step": 11607,
      "training_loss": 7.013737201690674
    },
    {
      "epoch": 0.6291598915989159,
      "grad_norm": 43.887699127197266,
      "learning_rate": 1e-05,
      "loss": 6.0097,
      "step": 11608
    },
    {
      "epoch": 0.6291598915989159,
      "step": 11608,
      "training_loss": 6.72916841506958
    },
    {
      "epoch": 0.6292140921409214,
      "step": 11609,
      "training_loss": 6.311586380004883
    },
    {
      "epoch": 0.6292682926829268,
      "step": 11610,
      "training_loss": 6.676720142364502
    },
    {
      "epoch": 0.6293224932249323,
      "step": 11611,
      "training_loss": 6.554199695587158
    },
    {
      "epoch": 0.6293766937669377,
      "grad_norm": 18.239688873291016,
      "learning_rate": 1e-05,
      "loss": 6.5679,
      "step": 11612
    },
    {
      "epoch": 0.6293766937669377,
      "step": 11612,
      "training_loss": 4.410006523132324
    },
    {
      "epoch": 0.6294308943089431,
      "step": 11613,
      "training_loss": 6.81241512298584
    },
    {
      "epoch": 0.6294850948509485,
      "step": 11614,
      "training_loss": 7.148622989654541
    },
    {
      "epoch": 0.629539295392954,
      "step": 11615,
      "training_loss": 6.720794677734375
    },
    {
      "epoch": 0.6295934959349594,
      "grad_norm": 36.45005416870117,
      "learning_rate": 1e-05,
      "loss": 6.273,
      "step": 11616
    },
    {
      "epoch": 0.6295934959349594,
      "step": 11616,
      "training_loss": 5.949161529541016
    },
    {
      "epoch": 0.6296476964769647,
      "step": 11617,
      "training_loss": 7.372554779052734
    },
    {
      "epoch": 0.6297018970189702,
      "step": 11618,
      "training_loss": 5.681575775146484
    },
    {
      "epoch": 0.6297560975609756,
      "step": 11619,
      "training_loss": 6.336404323577881
    },
    {
      "epoch": 0.6298102981029811,
      "grad_norm": 24.481412887573242,
      "learning_rate": 1e-05,
      "loss": 6.3349,
      "step": 11620
    },
    {
      "epoch": 0.6298102981029811,
      "step": 11620,
      "training_loss": 6.894272804260254
    },
    {
      "epoch": 0.6298644986449865,
      "step": 11621,
      "training_loss": 5.140134811401367
    },
    {
      "epoch": 0.6299186991869918,
      "step": 11622,
      "training_loss": 5.383610725402832
    },
    {
      "epoch": 0.6299728997289973,
      "step": 11623,
      "training_loss": 6.823522567749023
    },
    {
      "epoch": 0.6300271002710027,
      "grad_norm": 42.01029586791992,
      "learning_rate": 1e-05,
      "loss": 6.0604,
      "step": 11624
    },
    {
      "epoch": 0.6300271002710027,
      "step": 11624,
      "training_loss": 5.790747165679932
    },
    {
      "epoch": 0.6300813008130082,
      "step": 11625,
      "training_loss": 6.917034149169922
    },
    {
      "epoch": 0.6301355013550135,
      "step": 11626,
      "training_loss": 6.89534330368042
    },
    {
      "epoch": 0.630189701897019,
      "step": 11627,
      "training_loss": 6.376233100891113
    },
    {
      "epoch": 0.6302439024390244,
      "grad_norm": 31.143367767333984,
      "learning_rate": 1e-05,
      "loss": 6.4948,
      "step": 11628
    },
    {
      "epoch": 0.6302439024390244,
      "step": 11628,
      "training_loss": 5.658337593078613
    },
    {
      "epoch": 0.6302981029810298,
      "step": 11629,
      "training_loss": 5.785068035125732
    },
    {
      "epoch": 0.6303523035230353,
      "step": 11630,
      "training_loss": 7.965154647827148
    },
    {
      "epoch": 0.6304065040650406,
      "step": 11631,
      "training_loss": 7.410191059112549
    },
    {
      "epoch": 0.6304607046070461,
      "grad_norm": 19.80173683166504,
      "learning_rate": 1e-05,
      "loss": 6.7047,
      "step": 11632
    },
    {
      "epoch": 0.6304607046070461,
      "step": 11632,
      "training_loss": 8.085923194885254
    },
    {
      "epoch": 0.6305149051490515,
      "step": 11633,
      "training_loss": 6.220264911651611
    },
    {
      "epoch": 0.630569105691057,
      "step": 11634,
      "training_loss": 3.5195350646972656
    },
    {
      "epoch": 0.6306233062330623,
      "step": 11635,
      "training_loss": 6.221568584442139
    },
    {
      "epoch": 0.6306775067750677,
      "grad_norm": 25.804710388183594,
      "learning_rate": 1e-05,
      "loss": 6.0118,
      "step": 11636
    },
    {
      "epoch": 0.6306775067750677,
      "step": 11636,
      "training_loss": 6.255904197692871
    },
    {
      "epoch": 0.6307317073170732,
      "step": 11637,
      "training_loss": 8.479000091552734
    },
    {
      "epoch": 0.6307859078590786,
      "step": 11638,
      "training_loss": 4.102428436279297
    },
    {
      "epoch": 0.6308401084010841,
      "step": 11639,
      "training_loss": 7.9386444091796875
    },
    {
      "epoch": 0.6308943089430894,
      "grad_norm": 26.386075973510742,
      "learning_rate": 1e-05,
      "loss": 6.694,
      "step": 11640
    },
    {
      "epoch": 0.6308943089430894,
      "step": 11640,
      "training_loss": 8.030597686767578
    },
    {
      "epoch": 0.6309485094850948,
      "step": 11641,
      "training_loss": 7.7212815284729
    },
    {
      "epoch": 0.6310027100271003,
      "step": 11642,
      "training_loss": 4.727000713348389
    },
    {
      "epoch": 0.6310569105691057,
      "step": 11643,
      "training_loss": 6.0104475021362305
    },
    {
      "epoch": 0.6311111111111111,
      "grad_norm": 19.268030166625977,
      "learning_rate": 1e-05,
      "loss": 6.6223,
      "step": 11644
    },
    {
      "epoch": 0.6311111111111111,
      "step": 11644,
      "training_loss": 7.127519130706787
    },
    {
      "epoch": 0.6311653116531165,
      "step": 11645,
      "training_loss": 6.405059814453125
    },
    {
      "epoch": 0.631219512195122,
      "step": 11646,
      "training_loss": 5.94141149520874
    },
    {
      "epoch": 0.6312737127371274,
      "step": 11647,
      "training_loss": 6.937768459320068
    },
    {
      "epoch": 0.6313279132791328,
      "grad_norm": 54.654483795166016,
      "learning_rate": 1e-05,
      "loss": 6.6029,
      "step": 11648
    },
    {
      "epoch": 0.6313279132791328,
      "step": 11648,
      "training_loss": 7.256430625915527
    },
    {
      "epoch": 0.6313821138211382,
      "step": 11649,
      "training_loss": 6.822581768035889
    },
    {
      "epoch": 0.6314363143631436,
      "step": 11650,
      "training_loss": 6.714725017547607
    },
    {
      "epoch": 0.6314905149051491,
      "step": 11651,
      "training_loss": 6.1743974685668945
    },
    {
      "epoch": 0.6315447154471545,
      "grad_norm": 29.773014068603516,
      "learning_rate": 1e-05,
      "loss": 6.742,
      "step": 11652
    },
    {
      "epoch": 0.6315447154471545,
      "step": 11652,
      "training_loss": 6.724294662475586
    },
    {
      "epoch": 0.6315989159891598,
      "step": 11653,
      "training_loss": 7.506894588470459
    },
    {
      "epoch": 0.6316531165311653,
      "step": 11654,
      "training_loss": 6.914860725402832
    },
    {
      "epoch": 0.6317073170731707,
      "step": 11655,
      "training_loss": 6.707242965698242
    },
    {
      "epoch": 0.6317615176151762,
      "grad_norm": 38.323368072509766,
      "learning_rate": 1e-05,
      "loss": 6.9633,
      "step": 11656
    },
    {
      "epoch": 0.6317615176151762,
      "step": 11656,
      "training_loss": 6.667076110839844
    },
    {
      "epoch": 0.6318157181571816,
      "step": 11657,
      "training_loss": 7.746479034423828
    },
    {
      "epoch": 0.631869918699187,
      "step": 11658,
      "training_loss": 7.974804401397705
    },
    {
      "epoch": 0.6319241192411924,
      "step": 11659,
      "training_loss": 6.819611072540283
    },
    {
      "epoch": 0.6319783197831979,
      "grad_norm": 16.144044876098633,
      "learning_rate": 1e-05,
      "loss": 7.302,
      "step": 11660
    },
    {
      "epoch": 0.6319783197831979,
      "step": 11660,
      "training_loss": 6.979854583740234
    },
    {
      "epoch": 0.6320325203252033,
      "step": 11661,
      "training_loss": 7.035589694976807
    },
    {
      "epoch": 0.6320867208672086,
      "step": 11662,
      "training_loss": 7.221772193908691
    },
    {
      "epoch": 0.6321409214092141,
      "step": 11663,
      "training_loss": 7.47969913482666
    },
    {
      "epoch": 0.6321951219512195,
      "grad_norm": 32.67722702026367,
      "learning_rate": 1e-05,
      "loss": 7.1792,
      "step": 11664
    },
    {
      "epoch": 0.6321951219512195,
      "step": 11664,
      "training_loss": 7.136134624481201
    },
    {
      "epoch": 0.632249322493225,
      "step": 11665,
      "training_loss": 7.7142229080200195
    },
    {
      "epoch": 0.6323035230352303,
      "step": 11666,
      "training_loss": 5.06520938873291
    },
    {
      "epoch": 0.6323577235772357,
      "step": 11667,
      "training_loss": 6.69002103805542
    },
    {
      "epoch": 0.6324119241192412,
      "grad_norm": 22.38074493408203,
      "learning_rate": 1e-05,
      "loss": 6.6514,
      "step": 11668
    },
    {
      "epoch": 0.6324119241192412,
      "step": 11668,
      "training_loss": 7.0084123611450195
    },
    {
      "epoch": 0.6324661246612466,
      "step": 11669,
      "training_loss": 6.734184265136719
    },
    {
      "epoch": 0.6325203252032521,
      "step": 11670,
      "training_loss": 7.029689788818359
    },
    {
      "epoch": 0.6325745257452574,
      "step": 11671,
      "training_loss": 6.659478187561035
    },
    {
      "epoch": 0.6326287262872629,
      "grad_norm": 25.739595413208008,
      "learning_rate": 1e-05,
      "loss": 6.8579,
      "step": 11672
    },
    {
      "epoch": 0.6326287262872629,
      "step": 11672,
      "training_loss": 6.569239616394043
    },
    {
      "epoch": 0.6326829268292683,
      "step": 11673,
      "training_loss": 8.39320182800293
    },
    {
      "epoch": 0.6327371273712737,
      "step": 11674,
      "training_loss": 7.990665912628174
    },
    {
      "epoch": 0.6327913279132791,
      "step": 11675,
      "training_loss": 6.517584323883057
    },
    {
      "epoch": 0.6328455284552845,
      "grad_norm": 45.860252380371094,
      "learning_rate": 1e-05,
      "loss": 7.3677,
      "step": 11676
    },
    {
      "epoch": 0.6328455284552845,
      "step": 11676,
      "training_loss": 6.648385524749756
    },
    {
      "epoch": 0.63289972899729,
      "step": 11677,
      "training_loss": 4.377110481262207
    },
    {
      "epoch": 0.6329539295392954,
      "step": 11678,
      "training_loss": 7.3983354568481445
    },
    {
      "epoch": 0.6330081300813009,
      "step": 11679,
      "training_loss": 5.613166809082031
    },
    {
      "epoch": 0.6330623306233062,
      "grad_norm": 20.875978469848633,
      "learning_rate": 1e-05,
      "loss": 6.0092,
      "step": 11680
    },
    {
      "epoch": 0.6330623306233062,
      "step": 11680,
      "training_loss": 4.522249698638916
    },
    {
      "epoch": 0.6331165311653116,
      "step": 11681,
      "training_loss": 7.305893421173096
    },
    {
      "epoch": 0.6331707317073171,
      "step": 11682,
      "training_loss": 5.9431023597717285
    },
    {
      "epoch": 0.6332249322493225,
      "step": 11683,
      "training_loss": 6.128513336181641
    },
    {
      "epoch": 0.6332791327913279,
      "grad_norm": 26.87824058532715,
      "learning_rate": 1e-05,
      "loss": 5.9749,
      "step": 11684
    },
    {
      "epoch": 0.6332791327913279,
      "step": 11684,
      "training_loss": 5.659332752227783
    },
    {
      "epoch": 0.6333333333333333,
      "step": 11685,
      "training_loss": 6.884258270263672
    },
    {
      "epoch": 0.6333875338753387,
      "step": 11686,
      "training_loss": 3.4895217418670654
    },
    {
      "epoch": 0.6334417344173442,
      "step": 11687,
      "training_loss": 4.940441608428955
    },
    {
      "epoch": 0.6334959349593496,
      "grad_norm": 42.662940979003906,
      "learning_rate": 1e-05,
      "loss": 5.2434,
      "step": 11688
    },
    {
      "epoch": 0.6334959349593496,
      "step": 11688,
      "training_loss": 7.430475234985352
    },
    {
      "epoch": 0.633550135501355,
      "step": 11689,
      "training_loss": 7.532782077789307
    },
    {
      "epoch": 0.6336043360433604,
      "step": 11690,
      "training_loss": 7.517321586608887
    },
    {
      "epoch": 0.6336585365853659,
      "step": 11691,
      "training_loss": 4.066052436828613
    },
    {
      "epoch": 0.6337127371273713,
      "grad_norm": 39.604698181152344,
      "learning_rate": 1e-05,
      "loss": 6.6367,
      "step": 11692
    },
    {
      "epoch": 0.6337127371273713,
      "step": 11692,
      "training_loss": 6.0279059410095215
    },
    {
      "epoch": 0.6337669376693766,
      "step": 11693,
      "training_loss": 3.7552337646484375
    },
    {
      "epoch": 0.6338211382113821,
      "step": 11694,
      "training_loss": 7.163933277130127
    },
    {
      "epoch": 0.6338753387533875,
      "step": 11695,
      "training_loss": 6.144022464752197
    },
    {
      "epoch": 0.633929539295393,
      "grad_norm": 22.08597755432129,
      "learning_rate": 1e-05,
      "loss": 5.7728,
      "step": 11696
    },
    {
      "epoch": 0.633929539295393,
      "step": 11696,
      "training_loss": 6.475762367248535
    },
    {
      "epoch": 0.6339837398373984,
      "step": 11697,
      "training_loss": 5.780971050262451
    },
    {
      "epoch": 0.6340379403794038,
      "step": 11698,
      "training_loss": 6.725395679473877
    },
    {
      "epoch": 0.6340921409214092,
      "step": 11699,
      "training_loss": 6.117815971374512
    },
    {
      "epoch": 0.6341463414634146,
      "grad_norm": 19.625146865844727,
      "learning_rate": 1e-05,
      "loss": 6.275,
      "step": 11700
    },
    {
      "epoch": 0.6341463414634146,
      "step": 11700,
      "training_loss": 6.444201469421387
    },
    {
      "epoch": 0.6342005420054201,
      "step": 11701,
      "training_loss": 6.693032264709473
    },
    {
      "epoch": 0.6342547425474254,
      "step": 11702,
      "training_loss": 7.175809860229492
    },
    {
      "epoch": 0.6343089430894309,
      "step": 11703,
      "training_loss": 6.874169826507568
    },
    {
      "epoch": 0.6343631436314363,
      "grad_norm": 15.825380325317383,
      "learning_rate": 1e-05,
      "loss": 6.7968,
      "step": 11704
    },
    {
      "epoch": 0.6343631436314363,
      "step": 11704,
      "training_loss": 6.741199970245361
    },
    {
      "epoch": 0.6344173441734418,
      "step": 11705,
      "training_loss": 6.2052435874938965
    },
    {
      "epoch": 0.6344715447154472,
      "step": 11706,
      "training_loss": 6.079782962799072
    },
    {
      "epoch": 0.6345257452574525,
      "step": 11707,
      "training_loss": 7.934900760650635
    },
    {
      "epoch": 0.634579945799458,
      "grad_norm": 23.901031494140625,
      "learning_rate": 1e-05,
      "loss": 6.7403,
      "step": 11708
    },
    {
      "epoch": 0.634579945799458,
      "step": 11708,
      "training_loss": 5.862795829772949
    },
    {
      "epoch": 0.6346341463414634,
      "step": 11709,
      "training_loss": 5.977489948272705
    },
    {
      "epoch": 0.6346883468834689,
      "step": 11710,
      "training_loss": 6.695567607879639
    },
    {
      "epoch": 0.6347425474254742,
      "step": 11711,
      "training_loss": 7.68466329574585
    },
    {
      "epoch": 0.6347967479674796,
      "grad_norm": 26.541675567626953,
      "learning_rate": 1e-05,
      "loss": 6.5551,
      "step": 11712
    },
    {
      "epoch": 0.6347967479674796,
      "step": 11712,
      "training_loss": 7.305777549743652
    },
    {
      "epoch": 0.6348509485094851,
      "step": 11713,
      "training_loss": 6.191231727600098
    },
    {
      "epoch": 0.6349051490514905,
      "step": 11714,
      "training_loss": 6.837710380554199
    },
    {
      "epoch": 0.634959349593496,
      "step": 11715,
      "training_loss": 6.954896450042725
    },
    {
      "epoch": 0.6350135501355013,
      "grad_norm": 27.34204864501953,
      "learning_rate": 1e-05,
      "loss": 6.8224,
      "step": 11716
    },
    {
      "epoch": 0.6350135501355013,
      "step": 11716,
      "training_loss": 6.949438571929932
    },
    {
      "epoch": 0.6350677506775068,
      "step": 11717,
      "training_loss": 6.889804363250732
    },
    {
      "epoch": 0.6351219512195122,
      "step": 11718,
      "training_loss": 6.775296211242676
    },
    {
      "epoch": 0.6351761517615176,
      "step": 11719,
      "training_loss": 7.026795864105225
    },
    {
      "epoch": 0.635230352303523,
      "grad_norm": 19.746185302734375,
      "learning_rate": 1e-05,
      "loss": 6.9103,
      "step": 11720
    },
    {
      "epoch": 0.635230352303523,
      "step": 11720,
      "training_loss": 6.710719585418701
    },
    {
      "epoch": 0.6352845528455284,
      "step": 11721,
      "training_loss": 6.804909706115723
    },
    {
      "epoch": 0.6353387533875339,
      "step": 11722,
      "training_loss": 5.590270519256592
    },
    {
      "epoch": 0.6353929539295393,
      "step": 11723,
      "training_loss": 5.760349273681641
    },
    {
      "epoch": 0.6354471544715448,
      "grad_norm": 24.40620231628418,
      "learning_rate": 1e-05,
      "loss": 6.2166,
      "step": 11724
    },
    {
      "epoch": 0.6354471544715448,
      "step": 11724,
      "training_loss": 7.572494983673096
    },
    {
      "epoch": 0.6355013550135501,
      "step": 11725,
      "training_loss": 5.940310001373291
    },
    {
      "epoch": 0.6355555555555555,
      "step": 11726,
      "training_loss": 6.008162021636963
    },
    {
      "epoch": 0.635609756097561,
      "step": 11727,
      "training_loss": 6.520827770233154
    },
    {
      "epoch": 0.6356639566395664,
      "grad_norm": 24.63962745666504,
      "learning_rate": 1e-05,
      "loss": 6.5104,
      "step": 11728
    },
    {
      "epoch": 0.6356639566395664,
      "step": 11728,
      "training_loss": 7.429903507232666
    },
    {
      "epoch": 0.6357181571815718,
      "step": 11729,
      "training_loss": 5.872063159942627
    },
    {
      "epoch": 0.6357723577235772,
      "step": 11730,
      "training_loss": 4.338019847869873
    },
    {
      "epoch": 0.6358265582655827,
      "step": 11731,
      "training_loss": 3.4421839714050293
    },
    {
      "epoch": 0.6358807588075881,
      "grad_norm": 25.453720092773438,
      "learning_rate": 1e-05,
      "loss": 5.2705,
      "step": 11732
    },
    {
      "epoch": 0.6358807588075881,
      "step": 11732,
      "training_loss": 5.598824977874756
    },
    {
      "epoch": 0.6359349593495935,
      "step": 11733,
      "training_loss": 7.044096946716309
    },
    {
      "epoch": 0.6359891598915989,
      "step": 11734,
      "training_loss": 7.043619155883789
    },
    {
      "epoch": 0.6360433604336043,
      "step": 11735,
      "training_loss": 6.81726598739624
    },
    {
      "epoch": 0.6360975609756098,
      "grad_norm": 44.56647872924805,
      "learning_rate": 1e-05,
      "loss": 6.626,
      "step": 11736
    },
    {
      "epoch": 0.6360975609756098,
      "step": 11736,
      "training_loss": 7.236033916473389
    },
    {
      "epoch": 0.6361517615176152,
      "step": 11737,
      "training_loss": 7.205228328704834
    },
    {
      "epoch": 0.6362059620596205,
      "step": 11738,
      "training_loss": 4.139251232147217
    },
    {
      "epoch": 0.636260162601626,
      "step": 11739,
      "training_loss": 6.156902313232422
    },
    {
      "epoch": 0.6363143631436314,
      "grad_norm": 22.999982833862305,
      "learning_rate": 1e-05,
      "loss": 6.1844,
      "step": 11740
    },
    {
      "epoch": 0.6363143631436314,
      "step": 11740,
      "training_loss": 5.819483280181885
    },
    {
      "epoch": 0.6363685636856369,
      "step": 11741,
      "training_loss": 4.239505767822266
    },
    {
      "epoch": 0.6364227642276423,
      "step": 11742,
      "training_loss": 7.3598952293396
    },
    {
      "epoch": 0.6364769647696477,
      "step": 11743,
      "training_loss": 7.417023658752441
    },
    {
      "epoch": 0.6365311653116531,
      "grad_norm": 17.71719741821289,
      "learning_rate": 1e-05,
      "loss": 6.209,
      "step": 11744
    },
    {
      "epoch": 0.6365311653116531,
      "step": 11744,
      "training_loss": 6.419147968292236
    },
    {
      "epoch": 0.6365853658536585,
      "step": 11745,
      "training_loss": 6.946335792541504
    },
    {
      "epoch": 0.636639566395664,
      "step": 11746,
      "training_loss": 7.785714149475098
    },
    {
      "epoch": 0.6366937669376693,
      "step": 11747,
      "training_loss": 7.373483180999756
    },
    {
      "epoch": 0.6367479674796748,
      "grad_norm": 26.279903411865234,
      "learning_rate": 1e-05,
      "loss": 7.1312,
      "step": 11748
    },
    {
      "epoch": 0.6367479674796748,
      "step": 11748,
      "training_loss": 7.18766450881958
    },
    {
      "epoch": 0.6368021680216802,
      "step": 11749,
      "training_loss": 5.495625972747803
    },
    {
      "epoch": 0.6368563685636857,
      "step": 11750,
      "training_loss": 7.119259834289551
    },
    {
      "epoch": 0.6369105691056911,
      "step": 11751,
      "training_loss": 7.361983299255371
    },
    {
      "epoch": 0.6369647696476964,
      "grad_norm": 25.906429290771484,
      "learning_rate": 1e-05,
      "loss": 6.7911,
      "step": 11752
    },
    {
      "epoch": 0.6369647696476964,
      "step": 11752,
      "training_loss": 7.486239433288574
    },
    {
      "epoch": 0.6370189701897019,
      "step": 11753,
      "training_loss": 5.895930767059326
    },
    {
      "epoch": 0.6370731707317073,
      "step": 11754,
      "training_loss": 7.850953578948975
    },
    {
      "epoch": 0.6371273712737128,
      "step": 11755,
      "training_loss": 7.484862327575684
    },
    {
      "epoch": 0.6371815718157181,
      "grad_norm": 42.48828887939453,
      "learning_rate": 1e-05,
      "loss": 7.1795,
      "step": 11756
    },
    {
      "epoch": 0.6371815718157181,
      "step": 11756,
      "training_loss": 6.9748992919921875
    },
    {
      "epoch": 0.6372357723577236,
      "step": 11757,
      "training_loss": 6.988945007324219
    },
    {
      "epoch": 0.637289972899729,
      "step": 11758,
      "training_loss": 6.914846897125244
    },
    {
      "epoch": 0.6373441734417344,
      "step": 11759,
      "training_loss": 8.259796142578125
    },
    {
      "epoch": 0.6373983739837399,
      "grad_norm": 35.92946243286133,
      "learning_rate": 1e-05,
      "loss": 7.2846,
      "step": 11760
    },
    {
      "epoch": 0.6373983739837399,
      "step": 11760,
      "training_loss": 4.874603748321533
    },
    {
      "epoch": 0.6374525745257452,
      "step": 11761,
      "training_loss": 7.001924991607666
    },
    {
      "epoch": 0.6375067750677507,
      "step": 11762,
      "training_loss": 6.921316623687744
    },
    {
      "epoch": 0.6375609756097561,
      "step": 11763,
      "training_loss": 7.5758562088012695
    },
    {
      "epoch": 0.6376151761517616,
      "grad_norm": 21.15620231628418,
      "learning_rate": 1e-05,
      "loss": 6.5934,
      "step": 11764
    },
    {
      "epoch": 0.6376151761517616,
      "step": 11764,
      "training_loss": 6.060793876647949
    },
    {
      "epoch": 0.6376693766937669,
      "step": 11765,
      "training_loss": 7.04704475402832
    },
    {
      "epoch": 0.6377235772357723,
      "step": 11766,
      "training_loss": 6.634149551391602
    },
    {
      "epoch": 0.6377777777777778,
      "step": 11767,
      "training_loss": 5.65764856338501
    },
    {
      "epoch": 0.6378319783197832,
      "grad_norm": 66.58346557617188,
      "learning_rate": 1e-05,
      "loss": 6.3499,
      "step": 11768
    },
    {
      "epoch": 0.6378319783197832,
      "step": 11768,
      "training_loss": 5.884496212005615
    },
    {
      "epoch": 0.6378861788617887,
      "step": 11769,
      "training_loss": 7.3259477615356445
    },
    {
      "epoch": 0.637940379403794,
      "step": 11770,
      "training_loss": 7.619277000427246
    },
    {
      "epoch": 0.6379945799457994,
      "step": 11771,
      "training_loss": 5.663491249084473
    },
    {
      "epoch": 0.6380487804878049,
      "grad_norm": 40.1063117980957,
      "learning_rate": 1e-05,
      "loss": 6.6233,
      "step": 11772
    },
    {
      "epoch": 0.6380487804878049,
      "step": 11772,
      "training_loss": 7.4705095291137695
    },
    {
      "epoch": 0.6381029810298103,
      "step": 11773,
      "training_loss": 6.099836349487305
    },
    {
      "epoch": 0.6381571815718157,
      "step": 11774,
      "training_loss": 4.1025848388671875
    },
    {
      "epoch": 0.6382113821138211,
      "step": 11775,
      "training_loss": 8.479107856750488
    },
    {
      "epoch": 0.6382655826558266,
      "grad_norm": 46.72176742553711,
      "learning_rate": 1e-05,
      "loss": 6.538,
      "step": 11776
    },
    {
      "epoch": 0.6382655826558266,
      "step": 11776,
      "training_loss": 7.6117939949035645
    },
    {
      "epoch": 0.638319783197832,
      "step": 11777,
      "training_loss": 6.635526180267334
    },
    {
      "epoch": 0.6383739837398374,
      "step": 11778,
      "training_loss": 5.817753314971924
    },
    {
      "epoch": 0.6384281842818428,
      "step": 11779,
      "training_loss": 6.167905330657959
    },
    {
      "epoch": 0.6384823848238482,
      "grad_norm": 31.73203468322754,
      "learning_rate": 1e-05,
      "loss": 6.5582,
      "step": 11780
    },
    {
      "epoch": 0.6384823848238482,
      "step": 11780,
      "training_loss": 6.953126430511475
    },
    {
      "epoch": 0.6385365853658537,
      "step": 11781,
      "training_loss": 5.373810768127441
    },
    {
      "epoch": 0.6385907859078591,
      "step": 11782,
      "training_loss": 8.17015552520752
    },
    {
      "epoch": 0.6386449864498644,
      "step": 11783,
      "training_loss": 6.689027786254883
    },
    {
      "epoch": 0.6386991869918699,
      "grad_norm": 22.225540161132812,
      "learning_rate": 1e-05,
      "loss": 6.7965,
      "step": 11784
    },
    {
      "epoch": 0.6386991869918699,
      "step": 11784,
      "training_loss": 7.75863790512085
    },
    {
      "epoch": 0.6387533875338753,
      "step": 11785,
      "training_loss": 7.970412254333496
    },
    {
      "epoch": 0.6388075880758808,
      "step": 11786,
      "training_loss": 8.168437957763672
    },
    {
      "epoch": 0.6388617886178862,
      "step": 11787,
      "training_loss": 5.54536771774292
    },
    {
      "epoch": 0.6389159891598916,
      "grad_norm": 26.10710334777832,
      "learning_rate": 1e-05,
      "loss": 7.3607,
      "step": 11788
    },
    {
      "epoch": 0.6389159891598916,
      "step": 11788,
      "training_loss": 7.753032207489014
    },
    {
      "epoch": 0.638970189701897,
      "step": 11789,
      "training_loss": 7.159919261932373
    },
    {
      "epoch": 0.6390243902439025,
      "step": 11790,
      "training_loss": 6.742947101593018
    },
    {
      "epoch": 0.6390785907859079,
      "step": 11791,
      "training_loss": 7.732846736907959
    },
    {
      "epoch": 0.6391327913279132,
      "grad_norm": 28.897537231445312,
      "learning_rate": 1e-05,
      "loss": 7.3472,
      "step": 11792
    },
    {
      "epoch": 0.6391327913279132,
      "step": 11792,
      "training_loss": 7.201237201690674
    },
    {
      "epoch": 0.6391869918699187,
      "step": 11793,
      "training_loss": 8.196982383728027
    },
    {
      "epoch": 0.6392411924119241,
      "step": 11794,
      "training_loss": 5.97195291519165
    },
    {
      "epoch": 0.6392953929539296,
      "step": 11795,
      "training_loss": 5.831193923950195
    },
    {
      "epoch": 0.639349593495935,
      "grad_norm": 81.0438461303711,
      "learning_rate": 1e-05,
      "loss": 6.8003,
      "step": 11796
    },
    {
      "epoch": 0.639349593495935,
      "step": 11796,
      "training_loss": 5.963351249694824
    },
    {
      "epoch": 0.6394037940379403,
      "step": 11797,
      "training_loss": 6.524927616119385
    },
    {
      "epoch": 0.6394579945799458,
      "step": 11798,
      "training_loss": 7.802496910095215
    },
    {
      "epoch": 0.6395121951219512,
      "step": 11799,
      "training_loss": 6.944415092468262
    },
    {
      "epoch": 0.6395663956639567,
      "grad_norm": 16.417818069458008,
      "learning_rate": 1e-05,
      "loss": 6.8088,
      "step": 11800
    },
    {
      "epoch": 0.6395663956639567,
      "step": 11800,
      "training_loss": 6.963167667388916
    },
    {
      "epoch": 0.639620596205962,
      "step": 11801,
      "training_loss": 6.6446733474731445
    },
    {
      "epoch": 0.6396747967479675,
      "step": 11802,
      "training_loss": 5.663712978363037
    },
    {
      "epoch": 0.6397289972899729,
      "step": 11803,
      "training_loss": 5.549039363861084
    },
    {
      "epoch": 0.6397831978319783,
      "grad_norm": 33.5757942199707,
      "learning_rate": 1e-05,
      "loss": 6.2051,
      "step": 11804
    },
    {
      "epoch": 0.6397831978319783,
      "step": 11804,
      "training_loss": 6.667770862579346
    },
    {
      "epoch": 0.6398373983739838,
      "step": 11805,
      "training_loss": 7.990084648132324
    },
    {
      "epoch": 0.6398915989159891,
      "step": 11806,
      "training_loss": 6.831103324890137
    },
    {
      "epoch": 0.6399457994579946,
      "step": 11807,
      "training_loss": 7.380618572235107
    },
    {
      "epoch": 0.64,
      "grad_norm": 30.96148109436035,
      "learning_rate": 1e-05,
      "loss": 7.2174,
      "step": 11808
    },
    {
      "epoch": 0.64,
      "step": 11808,
      "training_loss": 6.660581111907959
    },
    {
      "epoch": 0.6400542005420055,
      "step": 11809,
      "training_loss": 7.011509895324707
    },
    {
      "epoch": 0.6401084010840108,
      "step": 11810,
      "training_loss": 6.695528507232666
    },
    {
      "epoch": 0.6401626016260162,
      "step": 11811,
      "training_loss": 6.533024311065674
    },
    {
      "epoch": 0.6402168021680217,
      "grad_norm": 33.58344650268555,
      "learning_rate": 1e-05,
      "loss": 6.7252,
      "step": 11812
    },
    {
      "epoch": 0.6402168021680217,
      "step": 11812,
      "training_loss": 5.5199432373046875
    },
    {
      "epoch": 0.6402710027100271,
      "step": 11813,
      "training_loss": 7.830564498901367
    },
    {
      "epoch": 0.6403252032520326,
      "step": 11814,
      "training_loss": 7.0081987380981445
    },
    {
      "epoch": 0.6403794037940379,
      "step": 11815,
      "training_loss": 6.179981708526611
    },
    {
      "epoch": 0.6404336043360433,
      "grad_norm": 22.96920394897461,
      "learning_rate": 1e-05,
      "loss": 6.6347,
      "step": 11816
    },
    {
      "epoch": 0.6404336043360433,
      "step": 11816,
      "training_loss": 6.373022556304932
    },
    {
      "epoch": 0.6404878048780488,
      "step": 11817,
      "training_loss": 7.956639766693115
    },
    {
      "epoch": 0.6405420054200542,
      "step": 11818,
      "training_loss": 6.732541561126709
    },
    {
      "epoch": 0.6405962059620596,
      "step": 11819,
      "training_loss": 8.358450889587402
    },
    {
      "epoch": 0.640650406504065,
      "grad_norm": 26.891782760620117,
      "learning_rate": 1e-05,
      "loss": 7.3552,
      "step": 11820
    },
    {
      "epoch": 0.640650406504065,
      "step": 11820,
      "training_loss": 6.82148551940918
    },
    {
      "epoch": 0.6407046070460705,
      "step": 11821,
      "training_loss": 7.12058162689209
    },
    {
      "epoch": 0.6407588075880759,
      "step": 11822,
      "training_loss": 5.983980655670166
    },
    {
      "epoch": 0.6408130081300814,
      "step": 11823,
      "training_loss": 4.4183759689331055
    },
    {
      "epoch": 0.6408672086720867,
      "grad_norm": 27.60979652404785,
      "learning_rate": 1e-05,
      "loss": 6.0861,
      "step": 11824
    },
    {
      "epoch": 0.6408672086720867,
      "step": 11824,
      "training_loss": 5.476255893707275
    },
    {
      "epoch": 0.6409214092140921,
      "step": 11825,
      "training_loss": 6.912441253662109
    },
    {
      "epoch": 0.6409756097560976,
      "step": 11826,
      "training_loss": 6.2231926918029785
    },
    {
      "epoch": 0.641029810298103,
      "step": 11827,
      "training_loss": 6.241001129150391
    },
    {
      "epoch": 0.6410840108401084,
      "grad_norm": 25.337453842163086,
      "learning_rate": 1e-05,
      "loss": 6.2132,
      "step": 11828
    },
    {
      "epoch": 0.6410840108401084,
      "step": 11828,
      "training_loss": 6.6960768699646
    },
    {
      "epoch": 0.6411382113821138,
      "step": 11829,
      "training_loss": 6.58093786239624
    },
    {
      "epoch": 0.6411924119241192,
      "step": 11830,
      "training_loss": 6.708583831787109
    },
    {
      "epoch": 0.6412466124661247,
      "step": 11831,
      "training_loss": 5.729235649108887
    },
    {
      "epoch": 0.6413008130081301,
      "grad_norm": 27.689130783081055,
      "learning_rate": 1e-05,
      "loss": 6.4287,
      "step": 11832
    },
    {
      "epoch": 0.6413008130081301,
      "step": 11832,
      "training_loss": 7.5200042724609375
    },
    {
      "epoch": 0.6413550135501355,
      "step": 11833,
      "training_loss": 6.380545139312744
    },
    {
      "epoch": 0.6414092140921409,
      "step": 11834,
      "training_loss": 5.71992301940918
    },
    {
      "epoch": 0.6414634146341464,
      "step": 11835,
      "training_loss": 6.835394382476807
    },
    {
      "epoch": 0.6415176151761518,
      "grad_norm": 22.56804084777832,
      "learning_rate": 1e-05,
      "loss": 6.614,
      "step": 11836
    },
    {
      "epoch": 0.6415176151761518,
      "step": 11836,
      "training_loss": 7.745523452758789
    },
    {
      "epoch": 0.6415718157181571,
      "step": 11837,
      "training_loss": 3.949768543243408
    },
    {
      "epoch": 0.6416260162601626,
      "step": 11838,
      "training_loss": 7.293341159820557
    },
    {
      "epoch": 0.641680216802168,
      "step": 11839,
      "training_loss": 3.02119779586792
    },
    {
      "epoch": 0.6417344173441735,
      "grad_norm": 35.28905487060547,
      "learning_rate": 1e-05,
      "loss": 5.5025,
      "step": 11840
    },
    {
      "epoch": 0.6417344173441735,
      "step": 11840,
      "training_loss": 4.934745788574219
    },
    {
      "epoch": 0.6417886178861789,
      "step": 11841,
      "training_loss": 6.059588432312012
    },
    {
      "epoch": 0.6418428184281842,
      "step": 11842,
      "training_loss": 6.838918209075928
    },
    {
      "epoch": 0.6418970189701897,
      "step": 11843,
      "training_loss": 5.856301784515381
    },
    {
      "epoch": 0.6419512195121951,
      "grad_norm": 29.937002182006836,
      "learning_rate": 1e-05,
      "loss": 5.9224,
      "step": 11844
    },
    {
      "epoch": 0.6419512195121951,
      "step": 11844,
      "training_loss": 8.127507209777832
    },
    {
      "epoch": 0.6420054200542006,
      "step": 11845,
      "training_loss": 7.262648582458496
    },
    {
      "epoch": 0.6420596205962059,
      "step": 11846,
      "training_loss": 6.526034832000732
    },
    {
      "epoch": 0.6421138211382114,
      "step": 11847,
      "training_loss": 6.471045017242432
    },
    {
      "epoch": 0.6421680216802168,
      "grad_norm": 23.569730758666992,
      "learning_rate": 1e-05,
      "loss": 7.0968,
      "step": 11848
    },
    {
      "epoch": 0.6421680216802168,
      "step": 11848,
      "training_loss": 7.302966594696045
    },
    {
      "epoch": 0.6422222222222222,
      "step": 11849,
      "training_loss": 6.8825249671936035
    },
    {
      "epoch": 0.6422764227642277,
      "step": 11850,
      "training_loss": 3.445009469985962
    },
    {
      "epoch": 0.642330623306233,
      "step": 11851,
      "training_loss": 6.747885227203369
    },
    {
      "epoch": 0.6423848238482385,
      "grad_norm": 16.361467361450195,
      "learning_rate": 1e-05,
      "loss": 6.0946,
      "step": 11852
    },
    {
      "epoch": 0.6423848238482385,
      "step": 11852,
      "training_loss": 5.405698776245117
    },
    {
      "epoch": 0.6424390243902439,
      "step": 11853,
      "training_loss": 7.781442642211914
    },
    {
      "epoch": 0.6424932249322494,
      "step": 11854,
      "training_loss": 5.882194995880127
    },
    {
      "epoch": 0.6425474254742547,
      "step": 11855,
      "training_loss": 7.070438861846924
    },
    {
      "epoch": 0.6426016260162601,
      "grad_norm": 26.05781364440918,
      "learning_rate": 1e-05,
      "loss": 6.5349,
      "step": 11856
    },
    {
      "epoch": 0.6426016260162601,
      "step": 11856,
      "training_loss": 6.473283290863037
    },
    {
      "epoch": 0.6426558265582656,
      "step": 11857,
      "training_loss": 6.262012958526611
    },
    {
      "epoch": 0.642710027100271,
      "step": 11858,
      "training_loss": 5.919254779815674
    },
    {
      "epoch": 0.6427642276422765,
      "step": 11859,
      "training_loss": 7.330028533935547
    },
    {
      "epoch": 0.6428184281842818,
      "grad_norm": 26.813093185424805,
      "learning_rate": 1e-05,
      "loss": 6.4961,
      "step": 11860
    },
    {
      "epoch": 0.6428184281842818,
      "step": 11860,
      "training_loss": 6.538963794708252
    },
    {
      "epoch": 0.6428726287262873,
      "step": 11861,
      "training_loss": 7.152466297149658
    },
    {
      "epoch": 0.6429268292682927,
      "step": 11862,
      "training_loss": 5.21635627746582
    },
    {
      "epoch": 0.6429810298102981,
      "step": 11863,
      "training_loss": 6.97518253326416
    },
    {
      "epoch": 0.6430352303523035,
      "grad_norm": 39.43296432495117,
      "learning_rate": 1e-05,
      "loss": 6.4707,
      "step": 11864
    },
    {
      "epoch": 0.6430352303523035,
      "step": 11864,
      "training_loss": 6.67764949798584
    },
    {
      "epoch": 0.6430894308943089,
      "step": 11865,
      "training_loss": 6.962160587310791
    },
    {
      "epoch": 0.6431436314363144,
      "step": 11866,
      "training_loss": 6.39415168762207
    },
    {
      "epoch": 0.6431978319783198,
      "step": 11867,
      "training_loss": 3.1356611251831055
    },
    {
      "epoch": 0.6432520325203253,
      "grad_norm": 67.6036148071289,
      "learning_rate": 1e-05,
      "loss": 5.7924,
      "step": 11868
    },
    {
      "epoch": 0.6432520325203253,
      "step": 11868,
      "training_loss": 6.069421291351318
    },
    {
      "epoch": 0.6433062330623306,
      "step": 11869,
      "training_loss": 4.696389675140381
    },
    {
      "epoch": 0.643360433604336,
      "step": 11870,
      "training_loss": 4.216884613037109
    },
    {
      "epoch": 0.6434146341463415,
      "step": 11871,
      "training_loss": 7.614371299743652
    },
    {
      "epoch": 0.6434688346883469,
      "grad_norm": 30.48450469970703,
      "learning_rate": 1e-05,
      "loss": 5.6493,
      "step": 11872
    },
    {
      "epoch": 0.6434688346883469,
      "step": 11872,
      "training_loss": 6.382742881774902
    },
    {
      "epoch": 0.6435230352303523,
      "step": 11873,
      "training_loss": 4.572136878967285
    },
    {
      "epoch": 0.6435772357723577,
      "step": 11874,
      "training_loss": 6.749756813049316
    },
    {
      "epoch": 0.6436314363143631,
      "step": 11875,
      "training_loss": 6.727668762207031
    },
    {
      "epoch": 0.6436856368563686,
      "grad_norm": 19.412260055541992,
      "learning_rate": 1e-05,
      "loss": 6.1081,
      "step": 11876
    },
    {
      "epoch": 0.6436856368563686,
      "step": 11876,
      "training_loss": 6.474536895751953
    },
    {
      "epoch": 0.643739837398374,
      "step": 11877,
      "training_loss": 8.762940406799316
    },
    {
      "epoch": 0.6437940379403794,
      "step": 11878,
      "training_loss": 4.966619968414307
    },
    {
      "epoch": 0.6438482384823848,
      "step": 11879,
      "training_loss": 5.80113410949707
    },
    {
      "epoch": 0.6439024390243903,
      "grad_norm": 50.98820877075195,
      "learning_rate": 1e-05,
      "loss": 6.5013,
      "step": 11880
    },
    {
      "epoch": 0.6439024390243903,
      "step": 11880,
      "training_loss": 2.641249179840088
    },
    {
      "epoch": 0.6439566395663957,
      "step": 11881,
      "training_loss": 6.995940208435059
    },
    {
      "epoch": 0.644010840108401,
      "step": 11882,
      "training_loss": 6.628015041351318
    },
    {
      "epoch": 0.6440650406504065,
      "step": 11883,
      "training_loss": 5.377385139465332
    },
    {
      "epoch": 0.6441192411924119,
      "grad_norm": 45.76964569091797,
      "learning_rate": 1e-05,
      "loss": 5.4106,
      "step": 11884
    },
    {
      "epoch": 0.6441192411924119,
      "step": 11884,
      "training_loss": 7.308255672454834
    },
    {
      "epoch": 0.6441734417344174,
      "step": 11885,
      "training_loss": 6.799653053283691
    },
    {
      "epoch": 0.6442276422764228,
      "step": 11886,
      "training_loss": 4.042882919311523
    },
    {
      "epoch": 0.6442818428184282,
      "step": 11887,
      "training_loss": 6.648024559020996
    },
    {
      "epoch": 0.6443360433604336,
      "grad_norm": 22.467308044433594,
      "learning_rate": 1e-05,
      "loss": 6.1997,
      "step": 11888
    },
    {
      "epoch": 0.6443360433604336,
      "step": 11888,
      "training_loss": 6.91936731338501
    },
    {
      "epoch": 0.644390243902439,
      "step": 11889,
      "training_loss": 5.929586887359619
    },
    {
      "epoch": 0.6444444444444445,
      "step": 11890,
      "training_loss": 5.611338138580322
    },
    {
      "epoch": 0.6444986449864498,
      "step": 11891,
      "training_loss": 8.371760368347168
    },
    {
      "epoch": 0.6445528455284553,
      "grad_norm": 87.06768035888672,
      "learning_rate": 1e-05,
      "loss": 6.708,
      "step": 11892
    },
    {
      "epoch": 0.6445528455284553,
      "step": 11892,
      "training_loss": 7.430912971496582
    },
    {
      "epoch": 0.6446070460704607,
      "step": 11893,
      "training_loss": 5.674572467803955
    },
    {
      "epoch": 0.6446612466124662,
      "step": 11894,
      "training_loss": 7.040356636047363
    },
    {
      "epoch": 0.6447154471544716,
      "step": 11895,
      "training_loss": 2.7456724643707275
    },
    {
      "epoch": 0.6447696476964769,
      "grad_norm": 27.834793090820312,
      "learning_rate": 1e-05,
      "loss": 5.7229,
      "step": 11896
    },
    {
      "epoch": 0.6447696476964769,
      "step": 11896,
      "training_loss": 6.912219047546387
    },
    {
      "epoch": 0.6448238482384824,
      "step": 11897,
      "training_loss": 8.134971618652344
    },
    {
      "epoch": 0.6448780487804878,
      "step": 11898,
      "training_loss": 7.639001369476318
    },
    {
      "epoch": 0.6449322493224933,
      "step": 11899,
      "training_loss": 6.857874870300293
    },
    {
      "epoch": 0.6449864498644986,
      "grad_norm": 31.655555725097656,
      "learning_rate": 1e-05,
      "loss": 7.386,
      "step": 11900
    },
    {
      "epoch": 0.6449864498644986,
      "step": 11900,
      "training_loss": 5.70486307144165
    },
    {
      "epoch": 0.645040650406504,
      "step": 11901,
      "training_loss": 7.902177333831787
    },
    {
      "epoch": 0.6450948509485095,
      "step": 11902,
      "training_loss": 7.049992561340332
    },
    {
      "epoch": 0.6451490514905149,
      "step": 11903,
      "training_loss": 6.520066738128662
    },
    {
      "epoch": 0.6452032520325204,
      "grad_norm": 28.344585418701172,
      "learning_rate": 1e-05,
      "loss": 6.7943,
      "step": 11904
    },
    {
      "epoch": 0.6452032520325204,
      "step": 11904,
      "training_loss": 7.096060752868652
    },
    {
      "epoch": 0.6452574525745257,
      "step": 11905,
      "training_loss": 6.620462417602539
    },
    {
      "epoch": 0.6453116531165312,
      "step": 11906,
      "training_loss": 6.0918965339660645
    },
    {
      "epoch": 0.6453658536585366,
      "step": 11907,
      "training_loss": 5.817670822143555
    },
    {
      "epoch": 0.645420054200542,
      "grad_norm": 33.12303161621094,
      "learning_rate": 1e-05,
      "loss": 6.4065,
      "step": 11908
    },
    {
      "epoch": 0.645420054200542,
      "step": 11908,
      "training_loss": 7.314813137054443
    },
    {
      "epoch": 0.6454742547425474,
      "step": 11909,
      "training_loss": 7.436476230621338
    },
    {
      "epoch": 0.6455284552845528,
      "step": 11910,
      "training_loss": 6.081169128417969
    },
    {
      "epoch": 0.6455826558265583,
      "step": 11911,
      "training_loss": 6.136452674865723
    },
    {
      "epoch": 0.6456368563685637,
      "grad_norm": 33.9500617980957,
      "learning_rate": 1e-05,
      "loss": 6.7422,
      "step": 11912
    },
    {
      "epoch": 0.6456368563685637,
      "step": 11912,
      "training_loss": 3.009551763534546
    },
    {
      "epoch": 0.6456910569105692,
      "step": 11913,
      "training_loss": 5.5132222175598145
    },
    {
      "epoch": 0.6457452574525745,
      "step": 11914,
      "training_loss": 7.98861026763916
    },
    {
      "epoch": 0.6457994579945799,
      "step": 11915,
      "training_loss": 4.910616397857666
    },
    {
      "epoch": 0.6458536585365854,
      "grad_norm": 37.1708869934082,
      "learning_rate": 1e-05,
      "loss": 5.3555,
      "step": 11916
    },
    {
      "epoch": 0.6458536585365854,
      "step": 11916,
      "training_loss": 8.322675704956055
    },
    {
      "epoch": 0.6459078590785908,
      "step": 11917,
      "training_loss": 5.943985939025879
    },
    {
      "epoch": 0.6459620596205962,
      "step": 11918,
      "training_loss": 6.725347995758057
    },
    {
      "epoch": 0.6460162601626016,
      "step": 11919,
      "training_loss": 7.139859199523926
    },
    {
      "epoch": 0.646070460704607,
      "grad_norm": 37.475276947021484,
      "learning_rate": 1e-05,
      "loss": 7.033,
      "step": 11920
    },
    {
      "epoch": 0.646070460704607,
      "step": 11920,
      "training_loss": 5.0640549659729
    },
    {
      "epoch": 0.6461246612466125,
      "step": 11921,
      "training_loss": 6.191675186157227
    },
    {
      "epoch": 0.6461788617886178,
      "step": 11922,
      "training_loss": 6.214646339416504
    },
    {
      "epoch": 0.6462330623306233,
      "step": 11923,
      "training_loss": 5.983818531036377
    },
    {
      "epoch": 0.6462872628726287,
      "grad_norm": 21.875823974609375,
      "learning_rate": 1e-05,
      "loss": 5.8635,
      "step": 11924
    },
    {
      "epoch": 0.6462872628726287,
      "step": 11924,
      "training_loss": 6.764224529266357
    },
    {
      "epoch": 0.6463414634146342,
      "step": 11925,
      "training_loss": 5.673083782196045
    },
    {
      "epoch": 0.6463956639566396,
      "step": 11926,
      "training_loss": 5.05077600479126
    },
    {
      "epoch": 0.6464498644986449,
      "step": 11927,
      "training_loss": 7.808804988861084
    },
    {
      "epoch": 0.6465040650406504,
      "grad_norm": 18.78481101989746,
      "learning_rate": 1e-05,
      "loss": 6.3242,
      "step": 11928
    },
    {
      "epoch": 0.6465040650406504,
      "step": 11928,
      "training_loss": 6.325571537017822
    },
    {
      "epoch": 0.6465582655826558,
      "step": 11929,
      "training_loss": 6.882275104522705
    },
    {
      "epoch": 0.6466124661246613,
      "step": 11930,
      "training_loss": 6.62170934677124
    },
    {
      "epoch": 0.6466666666666666,
      "step": 11931,
      "training_loss": 4.186068058013916
    },
    {
      "epoch": 0.6467208672086721,
      "grad_norm": 26.583127975463867,
      "learning_rate": 1e-05,
      "loss": 6.0039,
      "step": 11932
    },
    {
      "epoch": 0.6467208672086721,
      "step": 11932,
      "training_loss": 6.612649917602539
    },
    {
      "epoch": 0.6467750677506775,
      "step": 11933,
      "training_loss": 6.041675567626953
    },
    {
      "epoch": 0.646829268292683,
      "step": 11934,
      "training_loss": 7.036031246185303
    },
    {
      "epoch": 0.6468834688346884,
      "step": 11935,
      "training_loss": 6.56500768661499
    },
    {
      "epoch": 0.6469376693766937,
      "grad_norm": 25.519733428955078,
      "learning_rate": 1e-05,
      "loss": 6.5638,
      "step": 11936
    },
    {
      "epoch": 0.6469376693766937,
      "step": 11936,
      "training_loss": 7.578536033630371
    },
    {
      "epoch": 0.6469918699186992,
      "step": 11937,
      "training_loss": 7.2944207191467285
    },
    {
      "epoch": 0.6470460704607046,
      "step": 11938,
      "training_loss": 6.79395055770874
    },
    {
      "epoch": 0.6471002710027101,
      "step": 11939,
      "training_loss": 7.038455963134766
    },
    {
      "epoch": 0.6471544715447154,
      "grad_norm": 21.195419311523438,
      "learning_rate": 1e-05,
      "loss": 7.1763,
      "step": 11940
    },
    {
      "epoch": 0.6471544715447154,
      "step": 11940,
      "training_loss": 5.6197075843811035
    },
    {
      "epoch": 0.6472086720867208,
      "step": 11941,
      "training_loss": 6.055943489074707
    },
    {
      "epoch": 0.6472628726287263,
      "step": 11942,
      "training_loss": 5.2097392082214355
    },
    {
      "epoch": 0.6473170731707317,
      "step": 11943,
      "training_loss": 6.003670692443848
    },
    {
      "epoch": 0.6473712737127372,
      "grad_norm": 23.210094451904297,
      "learning_rate": 1e-05,
      "loss": 5.7223,
      "step": 11944
    },
    {
      "epoch": 0.6473712737127372,
      "step": 11944,
      "training_loss": 7.174233913421631
    },
    {
      "epoch": 0.6474254742547425,
      "step": 11945,
      "training_loss": 7.748323440551758
    },
    {
      "epoch": 0.647479674796748,
      "step": 11946,
      "training_loss": 6.7774338722229
    },
    {
      "epoch": 0.6475338753387534,
      "step": 11947,
      "training_loss": 11.180807113647461
    },
    {
      "epoch": 0.6475880758807588,
      "grad_norm": 57.24665832519531,
      "learning_rate": 1e-05,
      "loss": 8.2202,
      "step": 11948
    },
    {
      "epoch": 0.6475880758807588,
      "step": 11948,
      "training_loss": 6.879744529724121
    },
    {
      "epoch": 0.6476422764227642,
      "step": 11949,
      "training_loss": 6.050050258636475
    },
    {
      "epoch": 0.6476964769647696,
      "step": 11950,
      "training_loss": 7.754326343536377
    },
    {
      "epoch": 0.6477506775067751,
      "step": 11951,
      "training_loss": 5.277713775634766
    },
    {
      "epoch": 0.6478048780487805,
      "grad_norm": 68.2000961303711,
      "learning_rate": 1e-05,
      "loss": 6.4905,
      "step": 11952
    },
    {
      "epoch": 0.6478048780487805,
      "step": 11952,
      "training_loss": 4.172652721405029
    },
    {
      "epoch": 0.647859078590786,
      "step": 11953,
      "training_loss": 6.8420586585998535
    },
    {
      "epoch": 0.6479132791327913,
      "step": 11954,
      "training_loss": 4.916158676147461
    },
    {
      "epoch": 0.6479674796747967,
      "step": 11955,
      "training_loss": 6.646322727203369
    },
    {
      "epoch": 0.6480216802168022,
      "grad_norm": 25.062705993652344,
      "learning_rate": 1e-05,
      "loss": 5.6443,
      "step": 11956
    },
    {
      "epoch": 0.6480216802168022,
      "step": 11956,
      "training_loss": 5.263750076293945
    },
    {
      "epoch": 0.6480758807588076,
      "step": 11957,
      "training_loss": 6.946170806884766
    },
    {
      "epoch": 0.648130081300813,
      "step": 11958,
      "training_loss": 6.453798770904541
    },
    {
      "epoch": 0.6481842818428184,
      "step": 11959,
      "training_loss": 7.20506477355957
    },
    {
      "epoch": 0.6482384823848238,
      "grad_norm": 18.37923240661621,
      "learning_rate": 1e-05,
      "loss": 6.4672,
      "step": 11960
    },
    {
      "epoch": 0.6482384823848238,
      "step": 11960,
      "training_loss": 7.2243475914001465
    },
    {
      "epoch": 0.6482926829268293,
      "step": 11961,
      "training_loss": 6.264101505279541
    },
    {
      "epoch": 0.6483468834688347,
      "step": 11962,
      "training_loss": 8.126676559448242
    },
    {
      "epoch": 0.6484010840108401,
      "step": 11963,
      "training_loss": 6.253096103668213
    },
    {
      "epoch": 0.6484552845528455,
      "grad_norm": 64.3786849975586,
      "learning_rate": 1e-05,
      "loss": 6.9671,
      "step": 11964
    },
    {
      "epoch": 0.6484552845528455,
      "step": 11964,
      "training_loss": 6.523728370666504
    },
    {
      "epoch": 0.648509485094851,
      "step": 11965,
      "training_loss": 6.005191326141357
    },
    {
      "epoch": 0.6485636856368564,
      "step": 11966,
      "training_loss": 9.96554183959961
    },
    {
      "epoch": 0.6486178861788617,
      "step": 11967,
      "training_loss": 5.962465763092041
    },
    {
      "epoch": 0.6486720867208672,
      "grad_norm": 42.47715377807617,
      "learning_rate": 1e-05,
      "loss": 7.1142,
      "step": 11968
    },
    {
      "epoch": 0.6486720867208672,
      "step": 11968,
      "training_loss": 6.7910919189453125
    },
    {
      "epoch": 0.6487262872628726,
      "step": 11969,
      "training_loss": 5.62821626663208
    },
    {
      "epoch": 0.6487804878048781,
      "step": 11970,
      "training_loss": 7.380094051361084
    },
    {
      "epoch": 0.6488346883468835,
      "step": 11971,
      "training_loss": 7.019751071929932
    },
    {
      "epoch": 0.6488888888888888,
      "grad_norm": 23.649253845214844,
      "learning_rate": 1e-05,
      "loss": 6.7048,
      "step": 11972
    },
    {
      "epoch": 0.6488888888888888,
      "step": 11972,
      "training_loss": 6.740675449371338
    },
    {
      "epoch": 0.6489430894308943,
      "step": 11973,
      "training_loss": 7.918353080749512
    },
    {
      "epoch": 0.6489972899728997,
      "step": 11974,
      "training_loss": 5.506171703338623
    },
    {
      "epoch": 0.6490514905149052,
      "step": 11975,
      "training_loss": 5.549627304077148
    },
    {
      "epoch": 0.6491056910569105,
      "grad_norm": 28.639619827270508,
      "learning_rate": 1e-05,
      "loss": 6.4287,
      "step": 11976
    },
    {
      "epoch": 0.6491056910569105,
      "step": 11976,
      "training_loss": 6.812075614929199
    },
    {
      "epoch": 0.649159891598916,
      "step": 11977,
      "training_loss": 6.5759758949279785
    },
    {
      "epoch": 0.6492140921409214,
      "step": 11978,
      "training_loss": 7.969862937927246
    },
    {
      "epoch": 0.6492682926829269,
      "step": 11979,
      "training_loss": 7.252748489379883
    },
    {
      "epoch": 0.6493224932249323,
      "grad_norm": 45.380470275878906,
      "learning_rate": 1e-05,
      "loss": 7.1527,
      "step": 11980
    },
    {
      "epoch": 0.6493224932249323,
      "step": 11980,
      "training_loss": 6.215418338775635
    },
    {
      "epoch": 0.6493766937669376,
      "step": 11981,
      "training_loss": 7.428092956542969
    },
    {
      "epoch": 0.6494308943089431,
      "step": 11982,
      "training_loss": 6.44018030166626
    },
    {
      "epoch": 0.6494850948509485,
      "step": 11983,
      "training_loss": 7.412359714508057
    },
    {
      "epoch": 0.649539295392954,
      "grad_norm": 32.92805480957031,
      "learning_rate": 1e-05,
      "loss": 6.874,
      "step": 11984
    },
    {
      "epoch": 0.649539295392954,
      "step": 11984,
      "training_loss": 5.736299991607666
    },
    {
      "epoch": 0.6495934959349593,
      "step": 11985,
      "training_loss": 6.835362434387207
    },
    {
      "epoch": 0.6496476964769647,
      "step": 11986,
      "training_loss": 6.833380222320557
    },
    {
      "epoch": 0.6497018970189702,
      "step": 11987,
      "training_loss": 7.190483093261719
    },
    {
      "epoch": 0.6497560975609756,
      "grad_norm": 23.961429595947266,
      "learning_rate": 1e-05,
      "loss": 6.6489,
      "step": 11988
    },
    {
      "epoch": 0.6497560975609756,
      "step": 11988,
      "training_loss": 6.382753849029541
    },
    {
      "epoch": 0.6498102981029811,
      "step": 11989,
      "training_loss": 7.890622615814209
    },
    {
      "epoch": 0.6498644986449864,
      "step": 11990,
      "training_loss": 7.289433002471924
    },
    {
      "epoch": 0.6499186991869919,
      "step": 11991,
      "training_loss": 7.053399085998535
    },
    {
      "epoch": 0.6499728997289973,
      "grad_norm": 33.770484924316406,
      "learning_rate": 1e-05,
      "loss": 7.1541,
      "step": 11992
    },
    {
      "epoch": 0.6499728997289973,
      "step": 11992,
      "training_loss": 6.940412998199463
    },
    {
      "epoch": 0.6500271002710027,
      "step": 11993,
      "training_loss": 8.138921737670898
    },
    {
      "epoch": 0.6500813008130081,
      "step": 11994,
      "training_loss": 6.370700359344482
    },
    {
      "epoch": 0.6501355013550135,
      "step": 11995,
      "training_loss": 7.468482971191406
    },
    {
      "epoch": 0.650189701897019,
      "grad_norm": 19.394664764404297,
      "learning_rate": 1e-05,
      "loss": 7.2296,
      "step": 11996
    },
    {
      "epoch": 0.650189701897019,
      "step": 11996,
      "training_loss": 5.515185832977295
    },
    {
      "epoch": 0.6502439024390244,
      "step": 11997,
      "training_loss": 6.362451076507568
    },
    {
      "epoch": 0.6502981029810299,
      "step": 11998,
      "training_loss": 4.16831111907959
    },
    {
      "epoch": 0.6503523035230352,
      "step": 11999,
      "training_loss": 6.698981761932373
    },
    {
      "epoch": 0.6504065040650406,
      "grad_norm": 19.40988540649414,
      "learning_rate": 1e-05,
      "loss": 5.6862,
      "step": 12000
    },
    {
      "epoch": 0.6504065040650406,
      "eval_runtime": 457.4745,
      "eval_samples_per_second": 4.481,
      "eval_steps_per_second": 4.481,
      "step": 12000
    }
  ],
  "logging_steps": 4,
  "max_steps": 18450,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 6000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 6.0818741710848e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
