{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 6000,
  "global_step": 18450,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0,
      "step": 0,
      "training_loss": 11.111725807189941
    },
    {
      "epoch": 5.4200542005420054e-05,
      "step": 1,
      "training_loss": 13.243904113769531
    },
    {
      "epoch": 0.00010840108401084011,
      "step": 2,
      "training_loss": 12.016393661499023
    },
    {
      "epoch": 0.00016260162601626016,
      "step": 3,
      "training_loss": 12.524957656860352
    },
    {
      "epoch": 0.00021680216802168022,
      "grad_norm": NaN,
      "learning_rate": 1e-05,
      "loss": 12.2242,
      "step": 4
    },
    {
      "epoch": 0.00021680216802168022,
      "step": 4,
      "training_loss": 12.043158531188965
    },
    {
      "epoch": 0.00027100271002710027,
      "step": 5,
      "training_loss": 12.434259414672852
    },
    {
      "epoch": 0.0003252032520325203,
      "step": 6,
      "training_loss": 14.311711311340332
    },
    {
      "epoch": 0.0003794037940379404,
      "step": 7,
      "training_loss": 11.889445304870605
    },
    {
      "epoch": 0.00043360433604336043,
      "grad_norm": NaN,
      "learning_rate": 1e-05,
      "loss": 12.6696,
      "step": 8
    },
    {
      "epoch": 0.00043360433604336043,
      "step": 8,
      "training_loss": 11.8179292678833
    },
    {
      "epoch": 0.0004878048780487805,
      "step": 9,
      "training_loss": 12.8345308303833
    },
    {
      "epoch": 0.0005420054200542005,
      "step": 10,
      "training_loss": 12.50240707397461
    },
    {
      "epoch": 0.0005962059620596206,
      "step": 11,
      "training_loss": 12.181693077087402
    },
    {
      "epoch": 0.0006504065040650406,
      "grad_norm": 31.525789260864258,
      "learning_rate": 1e-05,
      "loss": 12.3341,
      "step": 12
    },
    {
      "epoch": 0.0006504065040650406,
      "step": 12,
      "training_loss": 13.70712661743164
    },
    {
      "epoch": 0.0007046070460704607,
      "step": 13,
      "training_loss": 11.968474388122559
    },
    {
      "epoch": 0.0007588075880758808,
      "step": 14,
      "training_loss": 11.70648193359375
    },
    {
      "epoch": 0.0008130081300813008,
      "step": 15,
      "training_loss": 12.483585357666016
    },
    {
      "epoch": 0.0008672086720867209,
      "grad_norm": 109.42294311523438,
      "learning_rate": 1e-05,
      "loss": 12.4664,
      "step": 16
    },
    {
      "epoch": 0.0008672086720867209,
      "step": 16,
      "training_loss": 14.522932052612305
    },
    {
      "epoch": 0.0009214092140921409,
      "step": 17,
      "training_loss": 12.172041893005371
    },
    {
      "epoch": 0.000975609756097561,
      "step": 18,
      "training_loss": 11.292916297912598
    },
    {
      "epoch": 0.001029810298102981,
      "step": 19,
      "training_loss": 11.767900466918945
    },
    {
      "epoch": 0.001084010840108401,
      "grad_norm": 64.20651245117188,
      "learning_rate": 1e-05,
      "loss": 12.4389,
      "step": 20
    },
    {
      "epoch": 0.001084010840108401,
      "step": 20,
      "training_loss": 12.513604164123535
    },
    {
      "epoch": 0.0011382113821138211,
      "step": 21,
      "training_loss": 11.945090293884277
    },
    {
      "epoch": 0.0011924119241192412,
      "step": 22,
      "training_loss": 12.44311237335205
    },
    {
      "epoch": 0.0012466124661246612,
      "step": 23,
      "training_loss": 23.766647338867188
    },
    {
      "epoch": 0.0013008130081300813,
      "grad_norm": NaN,
      "learning_rate": 1e-05,
      "loss": 15.1671,
      "step": 24
    },
    {
      "epoch": 0.0013008130081300813,
      "step": 24,
      "training_loss": 12.468871116638184
    },
    {
      "epoch": 0.0013550135501355014,
      "step": 25,
      "training_loss": 11.110321044921875
    },
    {
      "epoch": 0.0014092140921409214,
      "step": 26,
      "training_loss": 12.302184104919434
    },
    {
      "epoch": 0.0014634146341463415,
      "step": 27,
      "training_loss": 10.503989219665527
    },
    {
      "epoch": 0.0015176151761517615,
      "grad_norm": 98.07079315185547,
      "learning_rate": 1e-05,
      "loss": 11.5963,
      "step": 28
    },
    {
      "epoch": 0.0015176151761517615,
      "step": 28,
      "training_loss": 12.163992881774902
    },
    {
      "epoch": 0.0015718157181571816,
      "step": 29,
      "training_loss": 11.964346885681152
    },
    {
      "epoch": 0.0016260162601626016,
      "step": 30,
      "training_loss": 18.407920837402344
    },
    {
      "epoch": 0.0016802168021680217,
      "step": 31,
      "training_loss": 11.686904907226562
    },
    {
      "epoch": 0.0017344173441734417,
      "grad_norm": 36.55788040161133,
      "learning_rate": 1e-05,
      "loss": 13.5558,
      "step": 32
    },
    {
      "epoch": 0.0017344173441734417,
      "step": 32,
      "training_loss": 12.702886581420898
    },
    {
      "epoch": 0.0017886178861788618,
      "step": 33,
      "training_loss": 12.914487838745117
    },
    {
      "epoch": 0.0018428184281842818,
      "step": 34,
      "training_loss": 10.897786140441895
    },
    {
      "epoch": 0.001897018970189702,
      "step": 35,
      "training_loss": 11.767158508300781
    },
    {
      "epoch": 0.001951219512195122,
      "grad_norm": 65.15444946289062,
      "learning_rate": 1e-05,
      "loss": 12.0706,
      "step": 36
    },
    {
      "epoch": 0.001951219512195122,
      "step": 36,
      "training_loss": 12.365867614746094
    },
    {
      "epoch": 0.002005420054200542,
      "step": 37,
      "training_loss": 11.822869300842285
    },
    {
      "epoch": 0.002059620596205962,
      "step": 38,
      "training_loss": 12.36075496673584
    },
    {
      "epoch": 0.002113821138211382,
      "step": 39,
      "training_loss": 12.555594444274902
    },
    {
      "epoch": 0.002168021680216802,
      "grad_norm": 37.7735710144043,
      "learning_rate": 1e-05,
      "loss": 12.2763,
      "step": 40
    },
    {
      "epoch": 0.002168021680216802,
      "step": 40,
      "training_loss": 11.8652925491333
    },
    {
      "epoch": 0.0022222222222222222,
      "step": 41,
      "training_loss": 11.969447135925293
    },
    {
      "epoch": 0.0022764227642276423,
      "step": 42,
      "training_loss": 12.587841033935547
    },
    {
      "epoch": 0.0023306233062330623,
      "step": 43,
      "training_loss": 12.134407997131348
    },
    {
      "epoch": 0.0023848238482384824,
      "grad_norm": 16.635469436645508,
      "learning_rate": 1e-05,
      "loss": 12.1392,
      "step": 44
    },
    {
      "epoch": 0.0023848238482384824,
      "step": 44,
      "training_loss": 11.510613441467285
    },
    {
      "epoch": 0.0024390243902439024,
      "step": 45,
      "training_loss": 12.909424781799316
    },
    {
      "epoch": 0.0024932249322493225,
      "step": 46,
      "training_loss": 11.108716011047363
    },
    {
      "epoch": 0.0025474254742547425,
      "step": 47,
      "training_loss": 11.486334800720215
    },
    {
      "epoch": 0.0026016260162601626,
      "grad_norm": 115.90557861328125,
      "learning_rate": 1e-05,
      "loss": 11.7538,
      "step": 48
    },
    {
      "epoch": 0.0026016260162601626,
      "step": 48,
      "training_loss": 10.617513656616211
    },
    {
      "epoch": 0.0026558265582655827,
      "step": 49,
      "training_loss": 11.362805366516113
    },
    {
      "epoch": 0.0027100271002710027,
      "step": 50,
      "training_loss": 13.34550666809082
    },
    {
      "epoch": 0.0027642276422764228,
      "step": 51,
      "training_loss": 13.236893653869629
    },
    {
      "epoch": 0.002818428184281843,
      "grad_norm": NaN,
      "learning_rate": 1e-05,
      "loss": 12.1407,
      "step": 52
    },
    {
      "epoch": 0.002818428184281843,
      "step": 52,
      "training_loss": 11.147956848144531
    },
    {
      "epoch": 0.002872628726287263,
      "step": 53,
      "training_loss": 11.111977577209473
    },
    {
      "epoch": 0.002926829268292683,
      "step": 54,
      "training_loss": 11.819609642028809
    },
    {
      "epoch": 0.002981029810298103,
      "step": 55,
      "training_loss": 18.253034591674805
    },
    {
      "epoch": 0.003035230352303523,
      "grad_norm": 3510.15576171875,
      "learning_rate": 1e-05,
      "loss": 13.0831,
      "step": 56
    },
    {
      "epoch": 0.003035230352303523,
      "step": 56,
      "training_loss": 11.89055347442627
    },
    {
      "epoch": 0.003089430894308943,
      "step": 57,
      "training_loss": 12.646760940551758
    },
    {
      "epoch": 0.003143631436314363,
      "step": 58,
      "training_loss": 12.024698257446289
    },
    {
      "epoch": 0.003197831978319783,
      "step": 59,
      "training_loss": 10.462615013122559
    },
    {
      "epoch": 0.0032520325203252032,
      "grad_norm": 66.9910659790039,
      "learning_rate": 1e-05,
      "loss": 11.7562,
      "step": 60
    },
    {
      "epoch": 0.0032520325203252032,
      "step": 60,
      "training_loss": 12.814921379089355
    },
    {
      "epoch": 0.0033062330623306233,
      "step": 61,
      "training_loss": 12.584982872009277
    },
    {
      "epoch": 0.0033604336043360434,
      "step": 62,
      "training_loss": 16.339969635009766
    },
    {
      "epoch": 0.0034146341463414634,
      "step": 63,
      "training_loss": 12.473222732543945
    },
    {
      "epoch": 0.0034688346883468835,
      "grad_norm": 144.7232208251953,
      "learning_rate": 1e-05,
      "loss": 13.5533,
      "step": 64
    },
    {
      "epoch": 0.0034688346883468835,
      "step": 64,
      "training_loss": 11.977269172668457
    },
    {
      "epoch": 0.0035230352303523035,
      "step": 65,
      "training_loss": 11.061274528503418
    },
    {
      "epoch": 0.0035772357723577236,
      "step": 66,
      "training_loss": 11.066757202148438
    },
    {
      "epoch": 0.0036314363143631436,
      "step": 67,
      "training_loss": 11.54782485961914
    },
    {
      "epoch": 0.0036856368563685637,
      "grad_norm": 30.801361083984375,
      "learning_rate": 1e-05,
      "loss": 11.4133,
      "step": 68
    },
    {
      "epoch": 0.0036856368563685637,
      "step": 68,
      "training_loss": 16.31871795654297
    },
    {
      "epoch": 0.0037398373983739837,
      "step": 69,
      "training_loss": 11.002864837646484
    },
    {
      "epoch": 0.003794037940379404,
      "step": 70,
      "training_loss": 10.380319595336914
    },
    {
      "epoch": 0.003848238482384824,
      "step": 71,
      "training_loss": 13.15201473236084
    },
    {
      "epoch": 0.003902439024390244,
      "grad_norm": 26.781593322753906,
      "learning_rate": 1e-05,
      "loss": 12.7135,
      "step": 72
    },
    {
      "epoch": 0.003902439024390244,
      "step": 72,
      "training_loss": 12.929259300231934
    },
    {
      "epoch": 0.003956639566395664,
      "step": 73,
      "training_loss": 12.527139663696289
    },
    {
      "epoch": 0.004010840108401084,
      "step": 74,
      "training_loss": 13.343894004821777
    },
    {
      "epoch": 0.0040650406504065045,
      "step": 75,
      "training_loss": 11.778653144836426
    },
    {
      "epoch": 0.004119241192411924,
      "grad_norm": 13.673876762390137,
      "learning_rate": 1e-05,
      "loss": 12.6447,
      "step": 76
    },
    {
      "epoch": 0.004119241192411924,
      "step": 76,
      "training_loss": 11.3930082321167
    },
    {
      "epoch": 0.004173441734417345,
      "step": 77,
      "training_loss": 11.176904678344727
    },
    {
      "epoch": 0.004227642276422764,
      "step": 78,
      "training_loss": 11.507253646850586
    },
    {
      "epoch": 0.004281842818428185,
      "step": 79,
      "training_loss": 18.210491180419922
    },
    {
      "epoch": 0.004336043360433604,
      "grad_norm": 2676.913818359375,
      "learning_rate": 1e-05,
      "loss": 13.0719,
      "step": 80
    },
    {
      "epoch": 0.004336043360433604,
      "step": 80,
      "training_loss": 11.651453018188477
    },
    {
      "epoch": 0.004390243902439025,
      "step": 81,
      "training_loss": 11.206361770629883
    },
    {
      "epoch": 0.0044444444444444444,
      "step": 82,
      "training_loss": 11.949674606323242
    },
    {
      "epoch": 0.004498644986449865,
      "step": 83,
      "training_loss": 11.430155754089355
    },
    {
      "epoch": 0.0045528455284552845,
      "grad_norm": 51.11741638183594,
      "learning_rate": 1e-05,
      "loss": 11.5594,
      "step": 84
    },
    {
      "epoch": 0.0045528455284552845,
      "step": 84,
      "training_loss": 11.974573135375977
    },
    {
      "epoch": 0.004607046070460705,
      "step": 85,
      "training_loss": 9.924421310424805
    },
    {
      "epoch": 0.004661246612466125,
      "step": 86,
      "training_loss": 11.799753189086914
    },
    {
      "epoch": 0.004715447154471545,
      "step": 87,
      "training_loss": 11.262210845947266
    },
    {
      "epoch": 0.004769647696476965,
      "grad_norm": 27.579992294311523,
      "learning_rate": 1e-05,
      "loss": 11.2402,
      "step": 88
    },
    {
      "epoch": 0.004769647696476965,
      "step": 88,
      "training_loss": 9.01122760772705
    },
    {
      "epoch": 0.004823848238482385,
      "step": 89,
      "training_loss": 11.549857139587402
    },
    {
      "epoch": 0.004878048780487805,
      "step": 90,
      "training_loss": 11.19095516204834
    },
    {
      "epoch": 0.004932249322493225,
      "step": 91,
      "training_loss": 10.911154747009277
    },
    {
      "epoch": 0.004986449864498645,
      "grad_norm": 42.25879669189453,
      "learning_rate": 1e-05,
      "loss": 10.6658,
      "step": 92
    },
    {
      "epoch": 0.004986449864498645,
      "step": 92,
      "training_loss": 10.490349769592285
    },
    {
      "epoch": 0.0050406504065040655,
      "step": 93,
      "training_loss": 12.937756538391113
    },
    {
      "epoch": 0.005094850948509485,
      "step": 94,
      "training_loss": 13.500983238220215
    },
    {
      "epoch": 0.005149051490514906,
      "step": 95,
      "training_loss": 12.888493537902832
    },
    {
      "epoch": 0.005203252032520325,
      "grad_norm": 46.61079788208008,
      "learning_rate": 1e-05,
      "loss": 12.4544,
      "step": 96
    },
    {
      "epoch": 0.005203252032520325,
      "step": 96,
      "training_loss": 11.705912590026855
    },
    {
      "epoch": 0.005257452574525746,
      "step": 97,
      "training_loss": 12.569985389709473
    },
    {
      "epoch": 0.005311653116531165,
      "step": 98,
      "training_loss": 11.238210678100586
    },
    {
      "epoch": 0.005365853658536586,
      "step": 99,
      "training_loss": 11.309539794921875
    },
    {
      "epoch": 0.005420054200542005,
      "grad_norm": 19.945053100585938,
      "learning_rate": 1e-05,
      "loss": 11.7059,
      "step": 100
    },
    {
      "epoch": 0.005420054200542005,
      "step": 100,
      "training_loss": 10.453259468078613
    },
    {
      "epoch": 0.005474254742547426,
      "step": 101,
      "training_loss": 9.973780632019043
    },
    {
      "epoch": 0.0055284552845528455,
      "step": 102,
      "training_loss": 10.217357635498047
    },
    {
      "epoch": 0.005582655826558266,
      "step": 103,
      "training_loss": 12.519991874694824
    },
    {
      "epoch": 0.005636856368563686,
      "grad_norm": 290.6909484863281,
      "learning_rate": 1e-05,
      "loss": 10.7911,
      "step": 104
    },
    {
      "epoch": 0.005636856368563686,
      "step": 104,
      "training_loss": 11.5534029006958
    },
    {
      "epoch": 0.005691056910569106,
      "step": 105,
      "training_loss": 11.72472095489502
    },
    {
      "epoch": 0.005745257452574526,
      "step": 106,
      "training_loss": 11.177977561950684
    },
    {
      "epoch": 0.005799457994579946,
      "step": 107,
      "training_loss": 9.76594066619873
    },
    {
      "epoch": 0.005853658536585366,
      "grad_norm": 65.16187286376953,
      "learning_rate": 1e-05,
      "loss": 11.0555,
      "step": 108
    },
    {
      "epoch": 0.005853658536585366,
      "step": 108,
      "training_loss": 10.47020435333252
    },
    {
      "epoch": 0.005907859078590786,
      "step": 109,
      "training_loss": 11.763017654418945
    },
    {
      "epoch": 0.005962059620596206,
      "step": 110,
      "training_loss": 10.09343147277832
    },
    {
      "epoch": 0.0060162601626016264,
      "step": 111,
      "training_loss": 12.833083152770996
    },
    {
      "epoch": 0.006070460704607046,
      "grad_norm": 416.72174072265625,
      "learning_rate": 1e-05,
      "loss": 11.2899,
      "step": 112
    },
    {
      "epoch": 0.006070460704607046,
      "step": 112,
      "training_loss": 10.665101051330566
    },
    {
      "epoch": 0.0061246612466124666,
      "step": 113,
      "training_loss": 11.124534606933594
    },
    {
      "epoch": 0.006178861788617886,
      "step": 114,
      "training_loss": 11.358909606933594
    },
    {
      "epoch": 0.006233062330623307,
      "step": 115,
      "training_loss": 11.18017292022705
    },
    {
      "epoch": 0.006287262872628726,
      "grad_norm": 65.0464096069336,
      "learning_rate": 1e-05,
      "loss": 11.0822,
      "step": 116
    },
    {
      "epoch": 0.006287262872628726,
      "step": 116,
      "training_loss": 12.137036323547363
    },
    {
      "epoch": 0.006341463414634147,
      "step": 117,
      "training_loss": 11.771164894104004
    },
    {
      "epoch": 0.006395663956639566,
      "step": 118,
      "training_loss": 10.574539184570312
    },
    {
      "epoch": 0.006449864498644987,
      "step": 119,
      "training_loss": 11.43765926361084
    },
    {
      "epoch": 0.0065040650406504065,
      "grad_norm": 124.70130920410156,
      "learning_rate": 1e-05,
      "loss": 11.4801,
      "step": 120
    },
    {
      "epoch": 0.0065040650406504065,
      "step": 120,
      "training_loss": 11.914714813232422
    },
    {
      "epoch": 0.006558265582655827,
      "step": 121,
      "training_loss": 9.085593223571777
    },
    {
      "epoch": 0.006612466124661247,
      "step": 122,
      "training_loss": 11.181285858154297
    },
    {
      "epoch": 0.006666666666666667,
      "step": 123,
      "training_loss": 10.497374534606934
    },
    {
      "epoch": 0.006720867208672087,
      "grad_norm": 57.40266418457031,
      "learning_rate": 1e-05,
      "loss": 10.6697,
      "step": 124
    },
    {
      "epoch": 0.006720867208672087,
      "step": 124,
      "training_loss": 11.198966026306152
    },
    {
      "epoch": 0.006775067750677507,
      "step": 125,
      "training_loss": 11.80430793762207
    },
    {
      "epoch": 0.006829268292682927,
      "step": 126,
      "training_loss": 10.973591804504395
    },
    {
      "epoch": 0.006883468834688347,
      "step": 127,
      "training_loss": 9.583660125732422
    },
    {
      "epoch": 0.006937669376693767,
      "grad_norm": 47.901145935058594,
      "learning_rate": 1e-05,
      "loss": 10.8901,
      "step": 128
    },
    {
      "epoch": 0.006937669376693767,
      "step": 128,
      "training_loss": 11.993356704711914
    },
    {
      "epoch": 0.006991869918699187,
      "step": 129,
      "training_loss": 10.64016342163086
    },
    {
      "epoch": 0.007046070460704607,
      "step": 130,
      "training_loss": 10.88064956665039
    },
    {
      "epoch": 0.0071002710027100275,
      "step": 131,
      "training_loss": 10.638195991516113
    },
    {
      "epoch": 0.007154471544715447,
      "grad_norm": 25.196043014526367,
      "learning_rate": 1e-05,
      "loss": 11.0381,
      "step": 132
    },
    {
      "epoch": 0.007154471544715447,
      "step": 132,
      "training_loss": 12.720953941345215
    },
    {
      "epoch": 0.007208672086720868,
      "step": 133,
      "training_loss": 11.219255447387695
    },
    {
      "epoch": 0.007262872628726287,
      "step": 134,
      "training_loss": 10.449835777282715
    },
    {
      "epoch": 0.007317073170731708,
      "step": 135,
      "training_loss": 11.332894325256348
    },
    {
      "epoch": 0.007371273712737127,
      "grad_norm": 35.33205795288086,
      "learning_rate": 1e-05,
      "loss": 11.4307,
      "step": 136
    },
    {
      "epoch": 0.007371273712737127,
      "step": 136,
      "training_loss": 8.992963790893555
    },
    {
      "epoch": 0.007425474254742548,
      "step": 137,
      "training_loss": 10.61905288696289
    },
    {
      "epoch": 0.0074796747967479675,
      "step": 138,
      "training_loss": 10.478516578674316
    },
    {
      "epoch": 0.007533875338753388,
      "step": 139,
      "training_loss": 10.031811714172363
    },
    {
      "epoch": 0.007588075880758808,
      "grad_norm": 56.115909576416016,
      "learning_rate": 1e-05,
      "loss": 10.0306,
      "step": 140
    },
    {
      "epoch": 0.007588075880758808,
      "step": 140,
      "training_loss": 10.708312034606934
    },
    {
      "epoch": 0.007642276422764228,
      "step": 141,
      "training_loss": 12.18755054473877
    },
    {
      "epoch": 0.007696476964769648,
      "step": 142,
      "training_loss": 10.794925689697266
    },
    {
      "epoch": 0.007750677506775068,
      "step": 143,
      "training_loss": 10.226948738098145
    },
    {
      "epoch": 0.007804878048780488,
      "grad_norm": 35.30724334716797,
      "learning_rate": 1e-05,
      "loss": 10.9794,
      "step": 144
    },
    {
      "epoch": 0.007804878048780488,
      "step": 144,
      "training_loss": 11.62662124633789
    },
    {
      "epoch": 0.007859078590785908,
      "step": 145,
      "training_loss": 10.435996055603027
    },
    {
      "epoch": 0.007913279132791329,
      "step": 146,
      "training_loss": 11.219482421875
    },
    {
      "epoch": 0.007967479674796748,
      "step": 147,
      "training_loss": 10.211543083190918
    },
    {
      "epoch": 0.008021680216802168,
      "grad_norm": 66.42169952392578,
      "learning_rate": 1e-05,
      "loss": 10.8734,
      "step": 148
    },
    {
      "epoch": 0.008021680216802168,
      "step": 148,
      "training_loss": 9.528517723083496
    },
    {
      "epoch": 0.008075880758807589,
      "step": 149,
      "training_loss": 10.306105613708496
    },
    {
      "epoch": 0.008130081300813009,
      "step": 150,
      "training_loss": 11.306597709655762
    },
    {
      "epoch": 0.008184281842818428,
      "step": 151,
      "training_loss": 10.273051261901855
    },
    {
      "epoch": 0.008238482384823848,
      "grad_norm": 25.634309768676758,
      "learning_rate": 1e-05,
      "loss": 10.3536,
      "step": 152
    },
    {
      "epoch": 0.008238482384823848,
      "step": 152,
      "training_loss": 11.826397895812988
    },
    {
      "epoch": 0.008292682926829269,
      "step": 153,
      "training_loss": 10.880592346191406
    },
    {
      "epoch": 0.00834688346883469,
      "step": 154,
      "training_loss": 10.433236122131348
    },
    {
      "epoch": 0.008401084010840108,
      "step": 155,
      "training_loss": 11.639178276062012
    },
    {
      "epoch": 0.008455284552845528,
      "grad_norm": 57.76725769042969,
      "learning_rate": 1e-05,
      "loss": 11.1949,
      "step": 156
    },
    {
      "epoch": 0.008455284552845528,
      "step": 156,
      "training_loss": 10.858449935913086
    },
    {
      "epoch": 0.008509485094850949,
      "step": 157,
      "training_loss": 11.179259300231934
    },
    {
      "epoch": 0.00856368563685637,
      "step": 158,
      "training_loss": 9.746270179748535
    },
    {
      "epoch": 0.008617886178861788,
      "step": 159,
      "training_loss": 10.675082206726074
    },
    {
      "epoch": 0.008672086720867209,
      "grad_norm": 43.02224349975586,
      "learning_rate": 1e-05,
      "loss": 10.6148,
      "step": 160
    },
    {
      "epoch": 0.008672086720867209,
      "step": 160,
      "training_loss": 12.17440414428711
    },
    {
      "epoch": 0.00872628726287263,
      "step": 161,
      "training_loss": 9.399737358093262
    },
    {
      "epoch": 0.00878048780487805,
      "step": 162,
      "training_loss": 11.896190643310547
    },
    {
      "epoch": 0.008834688346883468,
      "step": 163,
      "training_loss": 9.154244422912598
    },
    {
      "epoch": 0.008888888888888889,
      "grad_norm": 24.27657127380371,
      "learning_rate": 1e-05,
      "loss": 10.6561,
      "step": 164
    },
    {
      "epoch": 0.008888888888888889,
      "step": 164,
      "training_loss": 10.401182174682617
    },
    {
      "epoch": 0.00894308943089431,
      "step": 165,
      "training_loss": 11.158220291137695
    },
    {
      "epoch": 0.00899728997289973,
      "step": 166,
      "training_loss": 10.03610610961914
    },
    {
      "epoch": 0.009051490514905149,
      "step": 167,
      "training_loss": 10.533648490905762
    },
    {
      "epoch": 0.009105691056910569,
      "grad_norm": 27.298343658447266,
      "learning_rate": 1e-05,
      "loss": 10.5323,
      "step": 168
    },
    {
      "epoch": 0.009105691056910569,
      "step": 168,
      "training_loss": 10.831672668457031
    },
    {
      "epoch": 0.00915989159891599,
      "step": 169,
      "training_loss": 9.928610801696777
    },
    {
      "epoch": 0.00921409214092141,
      "step": 170,
      "training_loss": 9.534505844116211
    },
    {
      "epoch": 0.009268292682926829,
      "step": 171,
      "training_loss": 12.548449516296387
    },
    {
      "epoch": 0.00932249322493225,
      "grad_norm": 33.712833404541016,
      "learning_rate": 1e-05,
      "loss": 10.7108,
      "step": 172
    },
    {
      "epoch": 0.00932249322493225,
      "step": 172,
      "training_loss": 9.055968284606934
    },
    {
      "epoch": 0.00937669376693767,
      "step": 173,
      "training_loss": 10.076143264770508
    },
    {
      "epoch": 0.00943089430894309,
      "step": 174,
      "training_loss": 9.921630859375
    },
    {
      "epoch": 0.009485094850948509,
      "step": 175,
      "training_loss": 11.052468299865723
    },
    {
      "epoch": 0.00953929539295393,
      "grad_norm": 29.249650955200195,
      "learning_rate": 1e-05,
      "loss": 10.0266,
      "step": 176
    },
    {
      "epoch": 0.00953929539295393,
      "step": 176,
      "training_loss": 11.840372085571289
    },
    {
      "epoch": 0.00959349593495935,
      "step": 177,
      "training_loss": 9.16879940032959
    },
    {
      "epoch": 0.00964769647696477,
      "step": 178,
      "training_loss": 9.26314926147461
    },
    {
      "epoch": 0.00970189701897019,
      "step": 179,
      "training_loss": 10.310220718383789
    },
    {
      "epoch": 0.00975609756097561,
      "grad_norm": 57.88793182373047,
      "learning_rate": 1e-05,
      "loss": 10.1456,
      "step": 180
    },
    {
      "epoch": 0.00975609756097561,
      "step": 180,
      "training_loss": 11.384123802185059
    },
    {
      "epoch": 0.00981029810298103,
      "step": 181,
      "training_loss": 11.630406379699707
    },
    {
      "epoch": 0.00986449864498645,
      "step": 182,
      "training_loss": 10.63655948638916
    },
    {
      "epoch": 0.00991869918699187,
      "step": 183,
      "training_loss": 10.648456573486328
    },
    {
      "epoch": 0.00997289972899729,
      "grad_norm": 88.28411102294922,
      "learning_rate": 1e-05,
      "loss": 11.0749,
      "step": 184
    },
    {
      "epoch": 0.00997289972899729,
      "step": 184,
      "training_loss": 10.726205825805664
    },
    {
      "epoch": 0.01002710027100271,
      "step": 185,
      "training_loss": 12.421470642089844
    },
    {
      "epoch": 0.010081300813008131,
      "step": 186,
      "training_loss": 11.398855209350586
    },
    {
      "epoch": 0.01013550135501355,
      "step": 187,
      "training_loss": 10.761733055114746
    },
    {
      "epoch": 0.01018970189701897,
      "grad_norm": 40.832733154296875,
      "learning_rate": 1e-05,
      "loss": 11.3271,
      "step": 188
    },
    {
      "epoch": 0.01018970189701897,
      "step": 188,
      "training_loss": 11.110318183898926
    },
    {
      "epoch": 0.01024390243902439,
      "step": 189,
      "training_loss": 9.783599853515625
    },
    {
      "epoch": 0.010298102981029811,
      "step": 190,
      "training_loss": 10.568955421447754
    },
    {
      "epoch": 0.01035230352303523,
      "step": 191,
      "training_loss": 10.305909156799316
    },
    {
      "epoch": 0.01040650406504065,
      "grad_norm": 28.829519271850586,
      "learning_rate": 1e-05,
      "loss": 10.4422,
      "step": 192
    },
    {
      "epoch": 0.01040650406504065,
      "step": 192,
      "training_loss": 10.539288520812988
    },
    {
      "epoch": 0.010460704607046071,
      "step": 193,
      "training_loss": 10.521219253540039
    },
    {
      "epoch": 0.010514905149051491,
      "step": 194,
      "training_loss": 11.271158218383789
    },
    {
      "epoch": 0.01056910569105691,
      "step": 195,
      "training_loss": 9.358236312866211
    },
    {
      "epoch": 0.01062330623306233,
      "grad_norm": 30.441970825195312,
      "learning_rate": 1e-05,
      "loss": 10.4225,
      "step": 196
    },
    {
      "epoch": 0.01062330623306233,
      "step": 196,
      "training_loss": 11.183821678161621
    },
    {
      "epoch": 0.010677506775067751,
      "step": 197,
      "training_loss": 9.7353515625
    },
    {
      "epoch": 0.010731707317073172,
      "step": 198,
      "training_loss": 9.843719482421875
    },
    {
      "epoch": 0.01078590785907859,
      "step": 199,
      "training_loss": 9.85303783416748
    },
    {
      "epoch": 0.01084010840108401,
      "grad_norm": 27.371170043945312,
      "learning_rate": 1e-05,
      "loss": 10.154,
      "step": 200
    },
    {
      "epoch": 0.01084010840108401,
      "step": 200,
      "training_loss": 11.018733978271484
    },
    {
      "epoch": 0.010894308943089431,
      "step": 201,
      "training_loss": 8.939705848693848
    },
    {
      "epoch": 0.010948509485094852,
      "step": 202,
      "training_loss": 9.353633880615234
    },
    {
      "epoch": 0.01100271002710027,
      "step": 203,
      "training_loss": 10.075800895690918
    },
    {
      "epoch": 0.011056910569105691,
      "grad_norm": 32.45064163208008,
      "learning_rate": 1e-05,
      "loss": 9.847,
      "step": 204
    },
    {
      "epoch": 0.011056910569105691,
      "step": 204,
      "training_loss": 9.363607406616211
    },
    {
      "epoch": 0.011111111111111112,
      "step": 205,
      "training_loss": 10.482160568237305
    },
    {
      "epoch": 0.011165311653116532,
      "step": 206,
      "training_loss": 9.114072799682617
    },
    {
      "epoch": 0.01121951219512195,
      "step": 207,
      "training_loss": 10.320244789123535
    },
    {
      "epoch": 0.011273712737127371,
      "grad_norm": 42.16585159301758,
      "learning_rate": 1e-05,
      "loss": 9.82,
      "step": 208
    },
    {
      "epoch": 0.011273712737127371,
      "step": 208,
      "training_loss": 8.485520362854004
    },
    {
      "epoch": 0.011327913279132792,
      "step": 209,
      "training_loss": 10.149017333984375
    },
    {
      "epoch": 0.011382113821138212,
      "step": 210,
      "training_loss": 9.366436958312988
    },
    {
      "epoch": 0.011436314363143631,
      "step": 211,
      "training_loss": 9.488317489624023
    },
    {
      "epoch": 0.011490514905149051,
      "grad_norm": 76.35527801513672,
      "learning_rate": 1e-05,
      "loss": 9.3723,
      "step": 212
    },
    {
      "epoch": 0.011490514905149051,
      "step": 212,
      "training_loss": 9.656798362731934
    },
    {
      "epoch": 0.011544715447154472,
      "step": 213,
      "training_loss": 8.110672950744629
    },
    {
      "epoch": 0.011598915989159892,
      "step": 214,
      "training_loss": 9.679655075073242
    },
    {
      "epoch": 0.011653116531165311,
      "step": 215,
      "training_loss": 10.348111152648926
    },
    {
      "epoch": 0.011707317073170732,
      "grad_norm": 28.596580505371094,
      "learning_rate": 1e-05,
      "loss": 9.4488,
      "step": 216
    },
    {
      "epoch": 0.011707317073170732,
      "step": 216,
      "training_loss": 9.542818069458008
    },
    {
      "epoch": 0.011761517615176152,
      "step": 217,
      "training_loss": 10.631009101867676
    },
    {
      "epoch": 0.011815718157181573,
      "step": 218,
      "training_loss": 9.405866622924805
    },
    {
      "epoch": 0.011869918699186991,
      "step": 219,
      "training_loss": 11.114250183105469
    },
    {
      "epoch": 0.011924119241192412,
      "grad_norm": 57.84479904174805,
      "learning_rate": 1e-05,
      "loss": 10.1735,
      "step": 220
    },
    {
      "epoch": 0.011924119241192412,
      "step": 220,
      "training_loss": 10.114420890808105
    },
    {
      "epoch": 0.011978319783197832,
      "step": 221,
      "training_loss": 8.765615463256836
    },
    {
      "epoch": 0.012032520325203253,
      "step": 222,
      "training_loss": 8.557538986206055
    },
    {
      "epoch": 0.012086720867208672,
      "step": 223,
      "training_loss": 8.861695289611816
    },
    {
      "epoch": 0.012140921409214092,
      "grad_norm": 16.74322509765625,
      "learning_rate": 1e-05,
      "loss": 9.0748,
      "step": 224
    },
    {
      "epoch": 0.012140921409214092,
      "step": 224,
      "training_loss": 9.108297348022461
    },
    {
      "epoch": 0.012195121951219513,
      "step": 225,
      "training_loss": 9.796586036682129
    },
    {
      "epoch": 0.012249322493224933,
      "step": 226,
      "training_loss": 10.130340576171875
    },
    {
      "epoch": 0.012303523035230352,
      "step": 227,
      "training_loss": 8.97702407836914
    },
    {
      "epoch": 0.012357723577235772,
      "grad_norm": 27.27787208557129,
      "learning_rate": 1e-05,
      "loss": 9.5031,
      "step": 228
    },
    {
      "epoch": 0.012357723577235772,
      "step": 228,
      "training_loss": 11.316548347473145
    },
    {
      "epoch": 0.012411924119241193,
      "step": 229,
      "training_loss": 10.519478797912598
    },
    {
      "epoch": 0.012466124661246613,
      "step": 230,
      "training_loss": 8.426501274108887
    },
    {
      "epoch": 0.012520325203252032,
      "step": 231,
      "training_loss": 10.85566234588623
    },
    {
      "epoch": 0.012574525745257453,
      "grad_norm": 46.42202377319336,
      "learning_rate": 1e-05,
      "loss": 10.2795,
      "step": 232
    },
    {
      "epoch": 0.012574525745257453,
      "step": 232,
      "training_loss": 8.400527000427246
    },
    {
      "epoch": 0.012628726287262873,
      "step": 233,
      "training_loss": 8.767032623291016
    },
    {
      "epoch": 0.012682926829268294,
      "step": 234,
      "training_loss": 8.553905487060547
    },
    {
      "epoch": 0.012737127371273712,
      "step": 235,
      "training_loss": 9.756759643554688
    },
    {
      "epoch": 0.012791327913279133,
      "grad_norm": 17.56345558166504,
      "learning_rate": 1e-05,
      "loss": 8.8696,
      "step": 236
    },
    {
      "epoch": 0.012791327913279133,
      "step": 236,
      "training_loss": 8.415847778320312
    },
    {
      "epoch": 0.012845528455284553,
      "step": 237,
      "training_loss": 8.690194129943848
    },
    {
      "epoch": 0.012899728997289974,
      "step": 238,
      "training_loss": 10.289337158203125
    },
    {
      "epoch": 0.012953929539295393,
      "step": 239,
      "training_loss": 10.482305526733398
    },
    {
      "epoch": 0.013008130081300813,
      "grad_norm": 48.825138092041016,
      "learning_rate": 1e-05,
      "loss": 9.4694,
      "step": 240
    },
    {
      "epoch": 0.013008130081300813,
      "step": 240,
      "training_loss": 9.455025672912598
    },
    {
      "epoch": 0.013062330623306233,
      "step": 241,
      "training_loss": 7.950691223144531
    },
    {
      "epoch": 0.013116531165311654,
      "step": 242,
      "training_loss": 10.397004127502441
    },
    {
      "epoch": 0.013170731707317073,
      "step": 243,
      "training_loss": 9.911083221435547
    },
    {
      "epoch": 0.013224932249322493,
      "grad_norm": 71.18138122558594,
      "learning_rate": 1e-05,
      "loss": 9.4285,
      "step": 244
    },
    {
      "epoch": 0.013224932249322493,
      "step": 244,
      "training_loss": 10.34853458404541
    },
    {
      "epoch": 0.013279132791327914,
      "step": 245,
      "training_loss": 9.728510856628418
    },
    {
      "epoch": 0.013333333333333334,
      "step": 246,
      "training_loss": 9.254651069641113
    },
    {
      "epoch": 0.013387533875338753,
      "step": 247,
      "training_loss": 10.217768669128418
    },
    {
      "epoch": 0.013441734417344173,
      "grad_norm": 36.83623504638672,
      "learning_rate": 1e-05,
      "loss": 9.8874,
      "step": 248
    },
    {
      "epoch": 0.013441734417344173,
      "step": 248,
      "training_loss": 8.915474891662598
    },
    {
      "epoch": 0.013495934959349594,
      "step": 249,
      "training_loss": 10.080000877380371
    },
    {
      "epoch": 0.013550135501355014,
      "step": 250,
      "training_loss": 9.276690483093262
    },
    {
      "epoch": 0.013604336043360433,
      "step": 251,
      "training_loss": 8.97238540649414
    },
    {
      "epoch": 0.013658536585365854,
      "grad_norm": 27.018537521362305,
      "learning_rate": 1e-05,
      "loss": 9.3111,
      "step": 252
    },
    {
      "epoch": 0.013658536585365854,
      "step": 252,
      "training_loss": 8.47642707824707
    },
    {
      "epoch": 0.013712737127371274,
      "step": 253,
      "training_loss": 9.077317237854004
    },
    {
      "epoch": 0.013766937669376695,
      "step": 254,
      "training_loss": 8.320564270019531
    },
    {
      "epoch": 0.013821138211382113,
      "step": 255,
      "training_loss": 9.3147611618042
    },
    {
      "epoch": 0.013875338753387534,
      "grad_norm": 16.86305809020996,
      "learning_rate": 1e-05,
      "loss": 8.7973,
      "step": 256
    },
    {
      "epoch": 0.013875338753387534,
      "step": 256,
      "training_loss": 8.058910369873047
    },
    {
      "epoch": 0.013929539295392954,
      "step": 257,
      "training_loss": 8.54357624053955
    },
    {
      "epoch": 0.013983739837398375,
      "step": 258,
      "training_loss": 8.083870887756348
    },
    {
      "epoch": 0.014037940379403794,
      "step": 259,
      "training_loss": 9.136470794677734
    },
    {
      "epoch": 0.014092140921409214,
      "grad_norm": 91.17194366455078,
      "learning_rate": 1e-05,
      "loss": 8.4557,
      "step": 260
    },
    {
      "epoch": 0.014092140921409214,
      "step": 260,
      "training_loss": 10.743507385253906
    },
    {
      "epoch": 0.014146341463414635,
      "step": 261,
      "training_loss": 9.20523738861084
    },
    {
      "epoch": 0.014200542005420055,
      "step": 262,
      "training_loss": 10.02578353881836
    },
    {
      "epoch": 0.014254742547425474,
      "step": 263,
      "training_loss": 9.480998992919922
    },
    {
      "epoch": 0.014308943089430894,
      "grad_norm": 46.80637741088867,
      "learning_rate": 1e-05,
      "loss": 9.8639,
      "step": 264
    },
    {
      "epoch": 0.014308943089430894,
      "step": 264,
      "training_loss": 8.286177635192871
    },
    {
      "epoch": 0.014363143631436315,
      "step": 265,
      "training_loss": 9.340065002441406
    },
    {
      "epoch": 0.014417344173441735,
      "step": 266,
      "training_loss": 10.482244491577148
    },
    {
      "epoch": 0.014471544715447154,
      "step": 267,
      "training_loss": 9.251486778259277
    },
    {
      "epoch": 0.014525745257452575,
      "grad_norm": 27.2625732421875,
      "learning_rate": 1e-05,
      "loss": 9.34,
      "step": 268
    },
    {
      "epoch": 0.014525745257452575,
      "step": 268,
      "training_loss": 10.094221115112305
    },
    {
      "epoch": 0.014579945799457995,
      "step": 269,
      "training_loss": 8.98193073272705
    },
    {
      "epoch": 0.014634146341463415,
      "step": 270,
      "training_loss": 8.341784477233887
    },
    {
      "epoch": 0.014688346883468834,
      "step": 271,
      "training_loss": 9.468655586242676
    },
    {
      "epoch": 0.014742547425474255,
      "grad_norm": 20.20656967163086,
      "learning_rate": 1e-05,
      "loss": 9.2216,
      "step": 272
    },
    {
      "epoch": 0.014742547425474255,
      "step": 272,
      "training_loss": 11.27656364440918
    },
    {
      "epoch": 0.014796747967479675,
      "step": 273,
      "training_loss": 10.032068252563477
    },
    {
      "epoch": 0.014850948509485096,
      "step": 274,
      "training_loss": 8.886849403381348
    },
    {
      "epoch": 0.014905149051490514,
      "step": 275,
      "training_loss": 8.74573802947998
    },
    {
      "epoch": 0.014959349593495935,
      "grad_norm": 19.699695587158203,
      "learning_rate": 1e-05,
      "loss": 9.7353,
      "step": 276
    },
    {
      "epoch": 0.014959349593495935,
      "step": 276,
      "training_loss": 8.745194435119629
    },
    {
      "epoch": 0.015013550135501355,
      "step": 277,
      "training_loss": 8.684534072875977
    },
    {
      "epoch": 0.015067750677506776,
      "step": 278,
      "training_loss": 9.80054759979248
    },
    {
      "epoch": 0.015121951219512195,
      "step": 279,
      "training_loss": 8.774774551391602
    },
    {
      "epoch": 0.015176151761517615,
      "grad_norm": 21.437641143798828,
      "learning_rate": 1e-05,
      "loss": 9.0013,
      "step": 280
    },
    {
      "epoch": 0.015176151761517615,
      "step": 280,
      "training_loss": 10.099853515625
    },
    {
      "epoch": 0.015230352303523036,
      "step": 281,
      "training_loss": 9.792943954467773
    },
    {
      "epoch": 0.015284552845528456,
      "step": 282,
      "training_loss": 9.225262641906738
    },
    {
      "epoch": 0.015338753387533875,
      "step": 283,
      "training_loss": 9.37082290649414
    },
    {
      "epoch": 0.015392953929539295,
      "grad_norm": 30.1685848236084,
      "learning_rate": 1e-05,
      "loss": 9.6222,
      "step": 284
    },
    {
      "epoch": 0.015392953929539295,
      "step": 284,
      "training_loss": 9.023386001586914
    },
    {
      "epoch": 0.015447154471544716,
      "step": 285,
      "training_loss": 10.123665809631348
    },
    {
      "epoch": 0.015501355013550136,
      "step": 286,
      "training_loss": 9.353898048400879
    },
    {
      "epoch": 0.015555555555555555,
      "step": 287,
      "training_loss": 7.633894920349121
    },
    {
      "epoch": 0.015609756097560976,
      "grad_norm": 21.047245025634766,
      "learning_rate": 1e-05,
      "loss": 9.0337,
      "step": 288
    },
    {
      "epoch": 0.015609756097560976,
      "step": 288,
      "training_loss": 8.956399917602539
    },
    {
      "epoch": 0.015663956639566396,
      "step": 289,
      "training_loss": 8.501402854919434
    },
    {
      "epoch": 0.015718157181571817,
      "step": 290,
      "training_loss": 9.322672843933105
    },
    {
      "epoch": 0.015772357723577237,
      "step": 291,
      "training_loss": 8.748307228088379
    },
    {
      "epoch": 0.015826558265582658,
      "grad_norm": 29.912433624267578,
      "learning_rate": 1e-05,
      "loss": 8.8822,
      "step": 292
    },
    {
      "epoch": 0.015826558265582658,
      "step": 292,
      "training_loss": 9.276658058166504
    },
    {
      "epoch": 0.015880758807588075,
      "step": 293,
      "training_loss": 11.696881294250488
    },
    {
      "epoch": 0.015934959349593495,
      "step": 294,
      "training_loss": 9.638446807861328
    },
    {
      "epoch": 0.015989159891598916,
      "step": 295,
      "training_loss": 7.8476033210754395
    },
    {
      "epoch": 0.016043360433604336,
      "grad_norm": 17.330045700073242,
      "learning_rate": 1e-05,
      "loss": 9.6149,
      "step": 296
    },
    {
      "epoch": 0.016043360433604336,
      "step": 296,
      "training_loss": 9.440367698669434
    },
    {
      "epoch": 0.016097560975609757,
      "step": 297,
      "training_loss": 7.9831438064575195
    },
    {
      "epoch": 0.016151761517615177,
      "step": 298,
      "training_loss": 7.866391181945801
    },
    {
      "epoch": 0.016205962059620597,
      "step": 299,
      "training_loss": 9.351469039916992
    },
    {
      "epoch": 0.016260162601626018,
      "grad_norm": 15.587423324584961,
      "learning_rate": 1e-05,
      "loss": 8.6603,
      "step": 300
    },
    {
      "epoch": 0.016260162601626018,
      "step": 300,
      "training_loss": 8.607007026672363
    },
    {
      "epoch": 0.016314363143631435,
      "step": 301,
      "training_loss": 9.57178783416748
    },
    {
      "epoch": 0.016368563685636855,
      "step": 302,
      "training_loss": 9.801836013793945
    },
    {
      "epoch": 0.016422764227642276,
      "step": 303,
      "training_loss": 8.356504440307617
    },
    {
      "epoch": 0.016476964769647696,
      "grad_norm": 14.989166259765625,
      "learning_rate": 1e-05,
      "loss": 9.0843,
      "step": 304
    },
    {
      "epoch": 0.016476964769647696,
      "step": 304,
      "training_loss": 6.732490539550781
    },
    {
      "epoch": 0.016531165311653117,
      "step": 305,
      "training_loss": 9.923141479492188
    },
    {
      "epoch": 0.016585365853658537,
      "step": 306,
      "training_loss": 9.289495468139648
    },
    {
      "epoch": 0.016639566395663958,
      "step": 307,
      "training_loss": 8.725504875183105
    },
    {
      "epoch": 0.01669376693766938,
      "grad_norm": 16.531253814697266,
      "learning_rate": 1e-05,
      "loss": 8.6677,
      "step": 308
    },
    {
      "epoch": 0.01669376693766938,
      "step": 308,
      "training_loss": 9.91627311706543
    },
    {
      "epoch": 0.016747967479674795,
      "step": 309,
      "training_loss": 9.295805931091309
    },
    {
      "epoch": 0.016802168021680216,
      "step": 310,
      "training_loss": 8.279821395874023
    },
    {
      "epoch": 0.016856368563685636,
      "step": 311,
      "training_loss": 11.07154369354248
    },
    {
      "epoch": 0.016910569105691057,
      "grad_norm": 27.21591567993164,
      "learning_rate": 1e-05,
      "loss": 9.6409,
      "step": 312
    },
    {
      "epoch": 0.016910569105691057,
      "step": 312,
      "training_loss": 8.62761116027832
    },
    {
      "epoch": 0.016964769647696477,
      "step": 313,
      "training_loss": 8.079964637756348
    },
    {
      "epoch": 0.017018970189701898,
      "step": 314,
      "training_loss": 6.985485553741455
    },
    {
      "epoch": 0.01707317073170732,
      "step": 315,
      "training_loss": 9.265228271484375
    },
    {
      "epoch": 0.01712737127371274,
      "grad_norm": 23.863388061523438,
      "learning_rate": 1e-05,
      "loss": 8.2396,
      "step": 316
    },
    {
      "epoch": 0.01712737127371274,
      "step": 316,
      "training_loss": 9.071197509765625
    },
    {
      "epoch": 0.017181571815718156,
      "step": 317,
      "training_loss": 9.31923770904541
    },
    {
      "epoch": 0.017235772357723576,
      "step": 318,
      "training_loss": 9.46812915802002
    },
    {
      "epoch": 0.017289972899728997,
      "step": 319,
      "training_loss": 8.964067459106445
    },
    {
      "epoch": 0.017344173441734417,
      "grad_norm": 22.54400062561035,
      "learning_rate": 1e-05,
      "loss": 9.2057,
      "step": 320
    },
    {
      "epoch": 0.017344173441734417,
      "step": 320,
      "training_loss": 9.166092872619629
    },
    {
      "epoch": 0.017398373983739838,
      "step": 321,
      "training_loss": 8.846397399902344
    },
    {
      "epoch": 0.01745257452574526,
      "step": 322,
      "training_loss": 9.016765594482422
    },
    {
      "epoch": 0.01750677506775068,
      "step": 323,
      "training_loss": 8.721442222595215
    },
    {
      "epoch": 0.0175609756097561,
      "grad_norm": 24.970245361328125,
      "learning_rate": 1e-05,
      "loss": 8.9377,
      "step": 324
    },
    {
      "epoch": 0.0175609756097561,
      "step": 324,
      "training_loss": 8.666658401489258
    },
    {
      "epoch": 0.017615176151761516,
      "step": 325,
      "training_loss": 8.0747652053833
    },
    {
      "epoch": 0.017669376693766937,
      "step": 326,
      "training_loss": 9.385220527648926
    },
    {
      "epoch": 0.017723577235772357,
      "step": 327,
      "training_loss": 7.595514297485352
    },
    {
      "epoch": 0.017777777777777778,
      "grad_norm": 15.149890899658203,
      "learning_rate": 1e-05,
      "loss": 8.4305,
      "step": 328
    },
    {
      "epoch": 0.017777777777777778,
      "step": 328,
      "training_loss": 7.909492492675781
    },
    {
      "epoch": 0.017831978319783198,
      "step": 329,
      "training_loss": 8.959549903869629
    },
    {
      "epoch": 0.01788617886178862,
      "step": 330,
      "training_loss": 10.01013469696045
    },
    {
      "epoch": 0.01794037940379404,
      "step": 331,
      "training_loss": 10.243875503540039
    },
    {
      "epoch": 0.01799457994579946,
      "grad_norm": 29.232423782348633,
      "learning_rate": 1e-05,
      "loss": 9.2808,
      "step": 332
    },
    {
      "epoch": 0.01799457994579946,
      "step": 332,
      "training_loss": 8.528722763061523
    },
    {
      "epoch": 0.018048780487804877,
      "step": 333,
      "training_loss": 9.731560707092285
    },
    {
      "epoch": 0.018102981029810297,
      "step": 334,
      "training_loss": 9.827203750610352
    },
    {
      "epoch": 0.018157181571815718,
      "step": 335,
      "training_loss": 8.408852577209473
    },
    {
      "epoch": 0.018211382113821138,
      "grad_norm": 116.2842788696289,
      "learning_rate": 1e-05,
      "loss": 9.1241,
      "step": 336
    },
    {
      "epoch": 0.018211382113821138,
      "step": 336,
      "training_loss": 8.360906600952148
    },
    {
      "epoch": 0.01826558265582656,
      "step": 337,
      "training_loss": 8.78665542602539
    },
    {
      "epoch": 0.01831978319783198,
      "step": 338,
      "training_loss": 8.43526554107666
    },
    {
      "epoch": 0.0183739837398374,
      "step": 339,
      "training_loss": 12.098740577697754
    },
    {
      "epoch": 0.01842818428184282,
      "grad_norm": 325.4991455078125,
      "learning_rate": 1e-05,
      "loss": 9.4204,
      "step": 340
    },
    {
      "epoch": 0.01842818428184282,
      "step": 340,
      "training_loss": 7.886733055114746
    },
    {
      "epoch": 0.018482384823848237,
      "step": 341,
      "training_loss": 7.866455554962158
    },
    {
      "epoch": 0.018536585365853658,
      "step": 342,
      "training_loss": 8.546805381774902
    },
    {
      "epoch": 0.018590785907859078,
      "step": 343,
      "training_loss": 8.790390968322754
    },
    {
      "epoch": 0.0186449864498645,
      "grad_norm": 24.537687301635742,
      "learning_rate": 1e-05,
      "loss": 8.2726,
      "step": 344
    },
    {
      "epoch": 0.0186449864498645,
      "step": 344,
      "training_loss": 8.041144371032715
    },
    {
      "epoch": 0.01869918699186992,
      "step": 345,
      "training_loss": 9.099125862121582
    },
    {
      "epoch": 0.01875338753387534,
      "step": 346,
      "training_loss": 8.649168968200684
    },
    {
      "epoch": 0.01880758807588076,
      "step": 347,
      "training_loss": 8.315924644470215
    },
    {
      "epoch": 0.01886178861788618,
      "grad_norm": 19.284896850585938,
      "learning_rate": 1e-05,
      "loss": 8.5263,
      "step": 348
    },
    {
      "epoch": 0.01886178861788618,
      "step": 348,
      "training_loss": 8.381394386291504
    },
    {
      "epoch": 0.018915989159891598,
      "step": 349,
      "training_loss": 9.773293495178223
    },
    {
      "epoch": 0.018970189701897018,
      "step": 350,
      "training_loss": 9.729175567626953
    },
    {
      "epoch": 0.01902439024390244,
      "step": 351,
      "training_loss": 9.061675071716309
    },
    {
      "epoch": 0.01907859078590786,
      "grad_norm": 22.570791244506836,
      "learning_rate": 1e-05,
      "loss": 9.2364,
      "step": 352
    },
    {
      "epoch": 0.01907859078590786,
      "step": 352,
      "training_loss": 9.205097198486328
    },
    {
      "epoch": 0.01913279132791328,
      "step": 353,
      "training_loss": 8.28036880493164
    },
    {
      "epoch": 0.0191869918699187,
      "step": 354,
      "training_loss": 7.077770233154297
    },
    {
      "epoch": 0.01924119241192412,
      "step": 355,
      "training_loss": 9.249635696411133
    },
    {
      "epoch": 0.01929539295392954,
      "grad_norm": 20.977657318115234,
      "learning_rate": 1e-05,
      "loss": 8.4532,
      "step": 356
    },
    {
      "epoch": 0.01929539295392954,
      "step": 356,
      "training_loss": 7.114163875579834
    },
    {
      "epoch": 0.019349593495934958,
      "step": 357,
      "training_loss": 9.17654800415039
    },
    {
      "epoch": 0.01940379403794038,
      "step": 358,
      "training_loss": 8.6029691696167
    },
    {
      "epoch": 0.0194579945799458,
      "step": 359,
      "training_loss": 8.269427299499512
    },
    {
      "epoch": 0.01951219512195122,
      "grad_norm": 19.475473403930664,
      "learning_rate": 1e-05,
      "loss": 8.2908,
      "step": 360
    },
    {
      "epoch": 0.01951219512195122,
      "step": 360,
      "training_loss": 7.7233123779296875
    },
    {
      "epoch": 0.01956639566395664,
      "step": 361,
      "training_loss": 7.430850505828857
    },
    {
      "epoch": 0.01962059620596206,
      "step": 362,
      "training_loss": 8.918601989746094
    },
    {
      "epoch": 0.01967479674796748,
      "step": 363,
      "training_loss": 8.447885513305664
    },
    {
      "epoch": 0.0197289972899729,
      "grad_norm": 74.73638916015625,
      "learning_rate": 1e-05,
      "loss": 8.1302,
      "step": 364
    },
    {
      "epoch": 0.0197289972899729,
      "step": 364,
      "training_loss": 8.260132789611816
    },
    {
      "epoch": 0.01978319783197832,
      "step": 365,
      "training_loss": 8.113731384277344
    },
    {
      "epoch": 0.01983739837398374,
      "step": 366,
      "training_loss": 8.079197883605957
    },
    {
      "epoch": 0.01989159891598916,
      "step": 367,
      "training_loss": 8.11274242401123
    },
    {
      "epoch": 0.01994579945799458,
      "grad_norm": 11.171524047851562,
      "learning_rate": 1e-05,
      "loss": 8.1415,
      "step": 368
    },
    {
      "epoch": 0.01994579945799458,
      "step": 368,
      "training_loss": 9.181455612182617
    },
    {
      "epoch": 0.02,
      "step": 369,
      "training_loss": 8.42985725402832
    },
    {
      "epoch": 0.02005420054200542,
      "step": 370,
      "training_loss": 8.581767082214355
    },
    {
      "epoch": 0.02010840108401084,
      "step": 371,
      "training_loss": 8.306815147399902
    },
    {
      "epoch": 0.020162601626016262,
      "grad_norm": 15.505874633789062,
      "learning_rate": 1e-05,
      "loss": 8.625,
      "step": 372
    },
    {
      "epoch": 0.020162601626016262,
      "step": 372,
      "training_loss": 8.882105827331543
    },
    {
      "epoch": 0.02021680216802168,
      "step": 373,
      "training_loss": 7.933631420135498
    },
    {
      "epoch": 0.0202710027100271,
      "step": 374,
      "training_loss": 7.909082412719727
    },
    {
      "epoch": 0.02032520325203252,
      "step": 375,
      "training_loss": 8.631102561950684
    },
    {
      "epoch": 0.02037940379403794,
      "grad_norm": 16.70805549621582,
      "learning_rate": 1e-05,
      "loss": 8.339,
      "step": 376
    },
    {
      "epoch": 0.02037940379403794,
      "step": 376,
      "training_loss": 6.825157165527344
    },
    {
      "epoch": 0.02043360433604336,
      "step": 377,
      "training_loss": 8.17951488494873
    },
    {
      "epoch": 0.02048780487804878,
      "step": 378,
      "training_loss": 7.624025344848633
    },
    {
      "epoch": 0.020542005420054202,
      "step": 379,
      "training_loss": 7.595602989196777
    },
    {
      "epoch": 0.020596205962059622,
      "grad_norm": 9.132671356201172,
      "learning_rate": 1e-05,
      "loss": 7.5561,
      "step": 380
    },
    {
      "epoch": 0.020596205962059622,
      "step": 380,
      "training_loss": 8.969121932983398
    },
    {
      "epoch": 0.02065040650406504,
      "step": 381,
      "training_loss": 9.680608749389648
    },
    {
      "epoch": 0.02070460704607046,
      "step": 382,
      "training_loss": 10.503753662109375
    },
    {
      "epoch": 0.02075880758807588,
      "step": 383,
      "training_loss": 7.353460788726807
    },
    {
      "epoch": 0.0208130081300813,
      "grad_norm": 16.1645565032959,
      "learning_rate": 1e-05,
      "loss": 9.1267,
      "step": 384
    },
    {
      "epoch": 0.0208130081300813,
      "step": 384,
      "training_loss": 8.351395606994629
    },
    {
      "epoch": 0.02086720867208672,
      "step": 385,
      "training_loss": 7.837363243103027
    },
    {
      "epoch": 0.020921409214092142,
      "step": 386,
      "training_loss": 8.236838340759277
    },
    {
      "epoch": 0.020975609756097562,
      "step": 387,
      "training_loss": 6.893348693847656
    },
    {
      "epoch": 0.021029810298102983,
      "grad_norm": 161.31109619140625,
      "learning_rate": 1e-05,
      "loss": 7.8297,
      "step": 388
    },
    {
      "epoch": 0.021029810298102983,
      "step": 388,
      "training_loss": 7.9343085289001465
    },
    {
      "epoch": 0.0210840108401084,
      "step": 389,
      "training_loss": 8.428815841674805
    },
    {
      "epoch": 0.02113821138211382,
      "step": 390,
      "training_loss": 9.046061515808105
    },
    {
      "epoch": 0.02119241192411924,
      "step": 391,
      "training_loss": 8.30547046661377
    },
    {
      "epoch": 0.02124661246612466,
      "grad_norm": 22.251420974731445,
      "learning_rate": 1e-05,
      "loss": 8.4287,
      "step": 392
    },
    {
      "epoch": 0.02124661246612466,
      "step": 392,
      "training_loss": 8.12428092956543
    },
    {
      "epoch": 0.02130081300813008,
      "step": 393,
      "training_loss": 7.956228256225586
    },
    {
      "epoch": 0.021355013550135502,
      "step": 394,
      "training_loss": 7.874785423278809
    },
    {
      "epoch": 0.021409214092140923,
      "step": 395,
      "training_loss": 8.476790428161621
    },
    {
      "epoch": 0.021463414634146343,
      "grad_norm": 11.951824188232422,
      "learning_rate": 1e-05,
      "loss": 8.108,
      "step": 396
    },
    {
      "epoch": 0.021463414634146343,
      "step": 396,
      "training_loss": 8.065881729125977
    },
    {
      "epoch": 0.02151761517615176,
      "step": 397,
      "training_loss": 7.801947116851807
    },
    {
      "epoch": 0.02157181571815718,
      "step": 398,
      "training_loss": 7.0356669425964355
    },
    {
      "epoch": 0.0216260162601626,
      "step": 399,
      "training_loss": 7.137936115264893
    },
    {
      "epoch": 0.02168021680216802,
      "grad_norm": 15.099337577819824,
      "learning_rate": 1e-05,
      "loss": 7.5104,
      "step": 400
    },
    {
      "epoch": 0.02168021680216802,
      "step": 400,
      "training_loss": 7.382430076599121
    },
    {
      "epoch": 0.021734417344173442,
      "step": 401,
      "training_loss": 7.641194820404053
    },
    {
      "epoch": 0.021788617886178863,
      "step": 402,
      "training_loss": 8.275113105773926
    },
    {
      "epoch": 0.021842818428184283,
      "step": 403,
      "training_loss": 7.928378105163574
    },
    {
      "epoch": 0.021897018970189704,
      "grad_norm": 22.510404586791992,
      "learning_rate": 1e-05,
      "loss": 7.8068,
      "step": 404
    },
    {
      "epoch": 0.021897018970189704,
      "step": 404,
      "training_loss": 7.857913494110107
    },
    {
      "epoch": 0.02195121951219512,
      "step": 405,
      "training_loss": 7.97804594039917
    },
    {
      "epoch": 0.02200542005420054,
      "step": 406,
      "training_loss": 6.97877836227417
    },
    {
      "epoch": 0.02205962059620596,
      "step": 407,
      "training_loss": 8.23125171661377
    },
    {
      "epoch": 0.022113821138211382,
      "grad_norm": 12.540722846984863,
      "learning_rate": 1e-05,
      "loss": 7.7615,
      "step": 408
    },
    {
      "epoch": 0.022113821138211382,
      "step": 408,
      "training_loss": 8.743800163269043
    },
    {
      "epoch": 0.022168021680216803,
      "step": 409,
      "training_loss": 8.306242942810059
    },
    {
      "epoch": 0.022222222222222223,
      "step": 410,
      "training_loss": 10.09003734588623
    },
    {
      "epoch": 0.022276422764227644,
      "step": 411,
      "training_loss": 8.354555130004883
    },
    {
      "epoch": 0.022330623306233064,
      "grad_norm": 17.296802520751953,
      "learning_rate": 1e-05,
      "loss": 8.8737,
      "step": 412
    },
    {
      "epoch": 0.022330623306233064,
      "step": 412,
      "training_loss": 8.873902320861816
    },
    {
      "epoch": 0.02238482384823848,
      "step": 413,
      "training_loss": 7.847009181976318
    },
    {
      "epoch": 0.0224390243902439,
      "step": 414,
      "training_loss": 7.264033794403076
    },
    {
      "epoch": 0.022493224932249322,
      "step": 415,
      "training_loss": 7.698939800262451
    },
    {
      "epoch": 0.022547425474254743,
      "grad_norm": 44.4743537902832,
      "learning_rate": 1e-05,
      "loss": 7.921,
      "step": 416
    },
    {
      "epoch": 0.022547425474254743,
      "step": 416,
      "training_loss": 8.31988525390625
    },
    {
      "epoch": 0.022601626016260163,
      "step": 417,
      "training_loss": 7.558592319488525
    },
    {
      "epoch": 0.022655826558265584,
      "step": 418,
      "training_loss": 8.039164543151855
    },
    {
      "epoch": 0.022710027100271004,
      "step": 419,
      "training_loss": 7.301027774810791
    },
    {
      "epoch": 0.022764227642276424,
      "grad_norm": 14.116792678833008,
      "learning_rate": 1e-05,
      "loss": 7.8047,
      "step": 420
    },
    {
      "epoch": 0.022764227642276424,
      "step": 420,
      "training_loss": 9.309996604919434
    },
    {
      "epoch": 0.02281842818428184,
      "step": 421,
      "training_loss": 8.074880599975586
    },
    {
      "epoch": 0.022872628726287262,
      "step": 422,
      "training_loss": 6.638279914855957
    },
    {
      "epoch": 0.022926829268292682,
      "step": 423,
      "training_loss": 8.46131420135498
    },
    {
      "epoch": 0.022981029810298103,
      "grad_norm": 13.438453674316406,
      "learning_rate": 1e-05,
      "loss": 8.1211,
      "step": 424
    },
    {
      "epoch": 0.022981029810298103,
      "step": 424,
      "training_loss": 7.890506267547607
    },
    {
      "epoch": 0.023035230352303523,
      "step": 425,
      "training_loss": 7.3360209465026855
    },
    {
      "epoch": 0.023089430894308944,
      "step": 426,
      "training_loss": 7.732000350952148
    },
    {
      "epoch": 0.023143631436314364,
      "step": 427,
      "training_loss": 6.986161708831787
    },
    {
      "epoch": 0.023197831978319785,
      "grad_norm": 22.73676872253418,
      "learning_rate": 1e-05,
      "loss": 7.4862,
      "step": 428
    },
    {
      "epoch": 0.023197831978319785,
      "step": 428,
      "training_loss": 6.317811489105225
    },
    {
      "epoch": 0.023252032520325202,
      "step": 429,
      "training_loss": 7.956981182098389
    },
    {
      "epoch": 0.023306233062330622,
      "step": 430,
      "training_loss": 7.851401329040527
    },
    {
      "epoch": 0.023360433604336043,
      "step": 431,
      "training_loss": 7.931685447692871
    },
    {
      "epoch": 0.023414634146341463,
      "grad_norm": 19.4801025390625,
      "learning_rate": 1e-05,
      "loss": 7.5145,
      "step": 432
    },
    {
      "epoch": 0.023414634146341463,
      "step": 432,
      "training_loss": 8.250307083129883
    },
    {
      "epoch": 0.023468834688346884,
      "step": 433,
      "training_loss": 6.365569591522217
    },
    {
      "epoch": 0.023523035230352304,
      "step": 434,
      "training_loss": 8.734480857849121
    },
    {
      "epoch": 0.023577235772357725,
      "step": 435,
      "training_loss": 7.921696186065674
    },
    {
      "epoch": 0.023631436314363145,
      "grad_norm": 49.99014663696289,
      "learning_rate": 1e-05,
      "loss": 7.818,
      "step": 436
    },
    {
      "epoch": 0.023631436314363145,
      "step": 436,
      "training_loss": 9.406049728393555
    },
    {
      "epoch": 0.023685636856368562,
      "step": 437,
      "training_loss": 8.261984825134277
    },
    {
      "epoch": 0.023739837398373983,
      "step": 438,
      "training_loss": 7.801459312438965
    },
    {
      "epoch": 0.023794037940379403,
      "step": 439,
      "training_loss": 6.366605281829834
    },
    {
      "epoch": 0.023848238482384824,
      "grad_norm": 14.054723739624023,
      "learning_rate": 1e-05,
      "loss": 7.959,
      "step": 440
    },
    {
      "epoch": 0.023848238482384824,
      "step": 440,
      "training_loss": 8.463953018188477
    },
    {
      "epoch": 0.023902439024390244,
      "step": 441,
      "training_loss": 11.978558540344238
    },
    {
      "epoch": 0.023956639566395665,
      "step": 442,
      "training_loss": 7.241355895996094
    },
    {
      "epoch": 0.024010840108401085,
      "step": 443,
      "training_loss": 7.96688985824585
    },
    {
      "epoch": 0.024065040650406506,
      "grad_norm": 20.506898880004883,
      "learning_rate": 1e-05,
      "loss": 8.9127,
      "step": 444
    },
    {
      "epoch": 0.024065040650406506,
      "step": 444,
      "training_loss": 8.147042274475098
    },
    {
      "epoch": 0.024119241192411923,
      "step": 445,
      "training_loss": 8.120525360107422
    },
    {
      "epoch": 0.024173441734417343,
      "step": 446,
      "training_loss": 7.316007614135742
    },
    {
      "epoch": 0.024227642276422764,
      "step": 447,
      "training_loss": 7.652131080627441
    },
    {
      "epoch": 0.024281842818428184,
      "grad_norm": 13.76490592956543,
      "learning_rate": 1e-05,
      "loss": 7.8089,
      "step": 448
    },
    {
      "epoch": 0.024281842818428184,
      "step": 448,
      "training_loss": 7.5631890296936035
    },
    {
      "epoch": 0.024336043360433605,
      "step": 449,
      "training_loss": 8.733774185180664
    },
    {
      "epoch": 0.024390243902439025,
      "step": 450,
      "training_loss": 7.819546699523926
    },
    {
      "epoch": 0.024444444444444446,
      "step": 451,
      "training_loss": 8.358509063720703
    },
    {
      "epoch": 0.024498644986449866,
      "grad_norm": 11.693607330322266,
      "learning_rate": 1e-05,
      "loss": 8.1188,
      "step": 452
    },
    {
      "epoch": 0.024498644986449866,
      "step": 452,
      "training_loss": 8.049361228942871
    },
    {
      "epoch": 0.024552845528455283,
      "step": 453,
      "training_loss": 7.477034568786621
    },
    {
      "epoch": 0.024607046070460704,
      "step": 454,
      "training_loss": 8.694418907165527
    },
    {
      "epoch": 0.024661246612466124,
      "step": 455,
      "training_loss": 7.679545879364014
    },
    {
      "epoch": 0.024715447154471545,
      "grad_norm": 14.152242660522461,
      "learning_rate": 1e-05,
      "loss": 7.9751,
      "step": 456
    },
    {
      "epoch": 0.024715447154471545,
      "step": 456,
      "training_loss": 6.727057456970215
    },
    {
      "epoch": 0.024769647696476965,
      "step": 457,
      "training_loss": 8.258109092712402
    },
    {
      "epoch": 0.024823848238482386,
      "step": 458,
      "training_loss": 6.571463108062744
    },
    {
      "epoch": 0.024878048780487806,
      "step": 459,
      "training_loss": 9.265181541442871
    },
    {
      "epoch": 0.024932249322493227,
      "grad_norm": 95.29155731201172,
      "learning_rate": 1e-05,
      "loss": 7.7055,
      "step": 460
    },
    {
      "epoch": 0.024932249322493227,
      "step": 460,
      "training_loss": 7.959630012512207
    },
    {
      "epoch": 0.024986449864498644,
      "step": 461,
      "training_loss": 8.809687614440918
    },
    {
      "epoch": 0.025040650406504064,
      "step": 462,
      "training_loss": 8.034317970275879
    },
    {
      "epoch": 0.025094850948509485,
      "step": 463,
      "training_loss": 9.111335754394531
    },
    {
      "epoch": 0.025149051490514905,
      "grad_norm": 17.28402328491211,
      "learning_rate": 1e-05,
      "loss": 8.4787,
      "step": 464
    },
    {
      "epoch": 0.025149051490514905,
      "step": 464,
      "training_loss": 7.907818794250488
    },
    {
      "epoch": 0.025203252032520326,
      "step": 465,
      "training_loss": 6.366112232208252
    },
    {
      "epoch": 0.025257452574525746,
      "step": 466,
      "training_loss": 8.446928977966309
    },
    {
      "epoch": 0.025311653116531167,
      "step": 467,
      "training_loss": 8.859402656555176
    },
    {
      "epoch": 0.025365853658536587,
      "grad_norm": 25.955493927001953,
      "learning_rate": 1e-05,
      "loss": 7.8951,
      "step": 468
    },
    {
      "epoch": 0.025365853658536587,
      "step": 468,
      "training_loss": 7.934590816497803
    },
    {
      "epoch": 0.025420054200542004,
      "step": 469,
      "training_loss": 7.631979942321777
    },
    {
      "epoch": 0.025474254742547425,
      "step": 470,
      "training_loss": 9.382837295532227
    },
    {
      "epoch": 0.025528455284552845,
      "step": 471,
      "training_loss": 7.657494068145752
    },
    {
      "epoch": 0.025582655826558266,
      "grad_norm": 12.350764274597168,
      "learning_rate": 1e-05,
      "loss": 8.1517,
      "step": 472
    },
    {
      "epoch": 0.025582655826558266,
      "step": 472,
      "training_loss": 7.057915210723877
    },
    {
      "epoch": 0.025636856368563686,
      "step": 473,
      "training_loss": 8.141605377197266
    },
    {
      "epoch": 0.025691056910569107,
      "step": 474,
      "training_loss": 6.964909553527832
    },
    {
      "epoch": 0.025745257452574527,
      "step": 475,
      "training_loss": 8.538036346435547
    },
    {
      "epoch": 0.025799457994579948,
      "grad_norm": 11.41733169555664,
      "learning_rate": 1e-05,
      "loss": 7.6756,
      "step": 476
    },
    {
      "epoch": 0.025799457994579948,
      "step": 476,
      "training_loss": 6.934912204742432
    },
    {
      "epoch": 0.025853658536585365,
      "step": 477,
      "training_loss": 6.558340549468994
    },
    {
      "epoch": 0.025907859078590785,
      "step": 478,
      "training_loss": 8.120965003967285
    },
    {
      "epoch": 0.025962059620596206,
      "step": 479,
      "training_loss": 8.37429428100586
    },
    {
      "epoch": 0.026016260162601626,
      "grad_norm": 29.7208194732666,
      "learning_rate": 1e-05,
      "loss": 7.4971,
      "step": 480
    },
    {
      "epoch": 0.026016260162601626,
      "step": 480,
      "training_loss": 6.973459243774414
    },
    {
      "epoch": 0.026070460704607046,
      "step": 481,
      "training_loss": 7.689476490020752
    },
    {
      "epoch": 0.026124661246612467,
      "step": 482,
      "training_loss": 7.858756065368652
    },
    {
      "epoch": 0.026178861788617887,
      "step": 483,
      "training_loss": 7.241107940673828
    },
    {
      "epoch": 0.026233062330623308,
      "grad_norm": 24.06954002380371,
      "learning_rate": 1e-05,
      "loss": 7.4407,
      "step": 484
    },
    {
      "epoch": 0.026233062330623308,
      "step": 484,
      "training_loss": 8.792257308959961
    },
    {
      "epoch": 0.026287262872628725,
      "step": 485,
      "training_loss": 8.03976058959961
    },
    {
      "epoch": 0.026341463414634145,
      "step": 486,
      "training_loss": 7.354308605194092
    },
    {
      "epoch": 0.026395663956639566,
      "step": 487,
      "training_loss": 8.887027740478516
    },
    {
      "epoch": 0.026449864498644986,
      "grad_norm": 20.02303695678711,
      "learning_rate": 1e-05,
      "loss": 8.2683,
      "step": 488
    },
    {
      "epoch": 0.026449864498644986,
      "step": 488,
      "training_loss": 8.766927719116211
    },
    {
      "epoch": 0.026504065040650407,
      "step": 489,
      "training_loss": 9.024206161499023
    },
    {
      "epoch": 0.026558265582655827,
      "step": 490,
      "training_loss": 6.299105644226074
    },
    {
      "epoch": 0.026612466124661248,
      "step": 491,
      "training_loss": 6.3097405433654785
    },
    {
      "epoch": 0.02666666666666667,
      "grad_norm": 16.101600646972656,
      "learning_rate": 1e-05,
      "loss": 7.6,
      "step": 492
    },
    {
      "epoch": 0.02666666666666667,
      "step": 492,
      "training_loss": 8.434672355651855
    },
    {
      "epoch": 0.026720867208672085,
      "step": 493,
      "training_loss": 7.963082313537598
    },
    {
      "epoch": 0.026775067750677506,
      "step": 494,
      "training_loss": 8.284965515136719
    },
    {
      "epoch": 0.026829268292682926,
      "step": 495,
      "training_loss": 7.380144119262695
    },
    {
      "epoch": 0.026883468834688347,
      "grad_norm": 13.190058708190918,
      "learning_rate": 1e-05,
      "loss": 8.0157,
      "step": 496
    },
    {
      "epoch": 0.026883468834688347,
      "step": 496,
      "training_loss": 8.209112167358398
    },
    {
      "epoch": 0.026937669376693767,
      "step": 497,
      "training_loss": 6.986357688903809
    },
    {
      "epoch": 0.026991869918699188,
      "step": 498,
      "training_loss": 8.515549659729004
    },
    {
      "epoch": 0.02704607046070461,
      "step": 499,
      "training_loss": 7.201557159423828
    },
    {
      "epoch": 0.02710027100271003,
      "grad_norm": 12.486780166625977,
      "learning_rate": 1e-05,
      "loss": 7.7281,
      "step": 500
    },
    {
      "epoch": 0.02710027100271003,
      "step": 500,
      "training_loss": 7.13062858581543
    },
    {
      "epoch": 0.027154471544715446,
      "step": 501,
      "training_loss": 8.09872055053711
    },
    {
      "epoch": 0.027208672086720866,
      "step": 502,
      "training_loss": 8.761409759521484
    },
    {
      "epoch": 0.027262872628726287,
      "step": 503,
      "training_loss": 6.616372108459473
    },
    {
      "epoch": 0.027317073170731707,
      "grad_norm": 25.115819931030273,
      "learning_rate": 1e-05,
      "loss": 7.6518,
      "step": 504
    },
    {
      "epoch": 0.027317073170731707,
      "step": 504,
      "training_loss": 8.176163673400879
    },
    {
      "epoch": 0.027371273712737128,
      "step": 505,
      "training_loss": 7.771926403045654
    },
    {
      "epoch": 0.02742547425474255,
      "step": 506,
      "training_loss": 7.734491348266602
    },
    {
      "epoch": 0.02747967479674797,
      "step": 507,
      "training_loss": 7.998220443725586
    },
    {
      "epoch": 0.02753387533875339,
      "grad_norm": 13.558465957641602,
      "learning_rate": 1e-05,
      "loss": 7.9202,
      "step": 508
    },
    {
      "epoch": 0.02753387533875339,
      "step": 508,
      "training_loss": 7.738372802734375
    },
    {
      "epoch": 0.027588075880758806,
      "step": 509,
      "training_loss": 8.241597175598145
    },
    {
      "epoch": 0.027642276422764227,
      "step": 510,
      "training_loss": 8.918270111083984
    },
    {
      "epoch": 0.027696476964769647,
      "step": 511,
      "training_loss": 6.77186918258667
    },
    {
      "epoch": 0.027750677506775068,
      "grad_norm": 10.977042198181152,
      "learning_rate": 1e-05,
      "loss": 7.9175,
      "step": 512
    },
    {
      "epoch": 0.027750677506775068,
      "step": 512,
      "training_loss": 7.243427276611328
    },
    {
      "epoch": 0.027804878048780488,
      "step": 513,
      "training_loss": 7.145732402801514
    },
    {
      "epoch": 0.02785907859078591,
      "step": 514,
      "training_loss": 7.725897789001465
    },
    {
      "epoch": 0.02791327913279133,
      "step": 515,
      "training_loss": 7.917025566101074
    },
    {
      "epoch": 0.02796747967479675,
      "grad_norm": 51.873714447021484,
      "learning_rate": 1e-05,
      "loss": 7.508,
      "step": 516
    },
    {
      "epoch": 0.02796747967479675,
      "step": 516,
      "training_loss": 8.662243843078613
    },
    {
      "epoch": 0.028021680216802167,
      "step": 517,
      "training_loss": 7.13602876663208
    },
    {
      "epoch": 0.028075880758807587,
      "step": 518,
      "training_loss": 7.190653324127197
    },
    {
      "epoch": 0.028130081300813008,
      "step": 519,
      "training_loss": 7.300667762756348
    },
    {
      "epoch": 0.028184281842818428,
      "grad_norm": 83.92121887207031,
      "learning_rate": 1e-05,
      "loss": 7.5724,
      "step": 520
    },
    {
      "epoch": 0.028184281842818428,
      "step": 520,
      "training_loss": 7.369582653045654
    },
    {
      "epoch": 0.02823848238482385,
      "step": 521,
      "training_loss": 8.244224548339844
    },
    {
      "epoch": 0.02829268292682927,
      "step": 522,
      "training_loss": 7.870123386383057
    },
    {
      "epoch": 0.02834688346883469,
      "step": 523,
      "training_loss": 8.35583209991455
    },
    {
      "epoch": 0.02840108401084011,
      "grad_norm": 14.924042701721191,
      "learning_rate": 1e-05,
      "loss": 7.9599,
      "step": 524
    },
    {
      "epoch": 0.02840108401084011,
      "step": 524,
      "training_loss": 7.588839054107666
    },
    {
      "epoch": 0.028455284552845527,
      "step": 525,
      "training_loss": 7.083045482635498
    },
    {
      "epoch": 0.028509485094850948,
      "step": 526,
      "training_loss": 7.160036563873291
    },
    {
      "epoch": 0.028563685636856368,
      "step": 527,
      "training_loss": 7.558352947235107
    },
    {
      "epoch": 0.02861788617886179,
      "grad_norm": 10.282278060913086,
      "learning_rate": 1e-05,
      "loss": 7.3476,
      "step": 528
    },
    {
      "epoch": 0.02861788617886179,
      "step": 528,
      "training_loss": 7.837279796600342
    },
    {
      "epoch": 0.02867208672086721,
      "step": 529,
      "training_loss": 8.573434829711914
    },
    {
      "epoch": 0.02872628726287263,
      "step": 530,
      "training_loss": 7.385232925415039
    },
    {
      "epoch": 0.02878048780487805,
      "step": 531,
      "training_loss": 8.93227481842041
    },
    {
      "epoch": 0.02883468834688347,
      "grad_norm": 28.24067497253418,
      "learning_rate": 1e-05,
      "loss": 8.1821,
      "step": 532
    },
    {
      "epoch": 0.02883468834688347,
      "step": 532,
      "training_loss": 8.903108596801758
    },
    {
      "epoch": 0.028888888888888888,
      "step": 533,
      "training_loss": 8.694533348083496
    },
    {
      "epoch": 0.028943089430894308,
      "step": 534,
      "training_loss": 12.607704162597656
    },
    {
      "epoch": 0.02899728997289973,
      "step": 535,
      "training_loss": 7.098885536193848
    },
    {
      "epoch": 0.02905149051490515,
      "grad_norm": 19.555856704711914,
      "learning_rate": 1e-05,
      "loss": 9.3261,
      "step": 536
    },
    {
      "epoch": 0.02905149051490515,
      "step": 536,
      "training_loss": 7.145205974578857
    },
    {
      "epoch": 0.02910569105691057,
      "step": 537,
      "training_loss": 7.424733638763428
    },
    {
      "epoch": 0.02915989159891599,
      "step": 538,
      "training_loss": 9.446101188659668
    },
    {
      "epoch": 0.02921409214092141,
      "step": 539,
      "training_loss": 8.077714920043945
    },
    {
      "epoch": 0.02926829268292683,
      "grad_norm": 13.626842498779297,
      "learning_rate": 1e-05,
      "loss": 8.0234,
      "step": 540
    },
    {
      "epoch": 0.02926829268292683,
      "step": 540,
      "training_loss": 8.232004165649414
    },
    {
      "epoch": 0.029322493224932248,
      "step": 541,
      "training_loss": 8.675039291381836
    },
    {
      "epoch": 0.02937669376693767,
      "step": 542,
      "training_loss": 8.267992973327637
    },
    {
      "epoch": 0.02943089430894309,
      "step": 543,
      "training_loss": 7.682991981506348
    },
    {
      "epoch": 0.02948509485094851,
      "grad_norm": 13.600375175476074,
      "learning_rate": 1e-05,
      "loss": 8.2145,
      "step": 544
    },
    {
      "epoch": 0.02948509485094851,
      "step": 544,
      "training_loss": 7.752343654632568
    },
    {
      "epoch": 0.02953929539295393,
      "step": 545,
      "training_loss": 7.010008811950684
    },
    {
      "epoch": 0.02959349593495935,
      "step": 546,
      "training_loss": 7.402151584625244
    },
    {
      "epoch": 0.02964769647696477,
      "step": 547,
      "training_loss": 8.647198677062988
    },
    {
      "epoch": 0.02970189701897019,
      "grad_norm": 18.925647735595703,
      "learning_rate": 1e-05,
      "loss": 7.7029,
      "step": 548
    },
    {
      "epoch": 0.02970189701897019,
      "step": 548,
      "training_loss": 7.286717891693115
    },
    {
      "epoch": 0.02975609756097561,
      "step": 549,
      "training_loss": 8.617945671081543
    },
    {
      "epoch": 0.02981029810298103,
      "step": 550,
      "training_loss": 6.48945426940918
    },
    {
      "epoch": 0.02986449864498645,
      "step": 551,
      "training_loss": 7.536012172698975
    },
    {
      "epoch": 0.02991869918699187,
      "grad_norm": 8.868014335632324,
      "learning_rate": 1e-05,
      "loss": 7.4825,
      "step": 552
    },
    {
      "epoch": 0.02991869918699187,
      "step": 552,
      "training_loss": 7.655765533447266
    },
    {
      "epoch": 0.02997289972899729,
      "step": 553,
      "training_loss": 8.01198673248291
    },
    {
      "epoch": 0.03002710027100271,
      "step": 554,
      "training_loss": 8.47567367553711
    },
    {
      "epoch": 0.03008130081300813,
      "step": 555,
      "training_loss": 7.649114608764648
    },
    {
      "epoch": 0.030135501355013552,
      "grad_norm": 19.67048454284668,
      "learning_rate": 1e-05,
      "loss": 7.9481,
      "step": 556
    },
    {
      "epoch": 0.030135501355013552,
      "step": 556,
      "training_loss": 8.006077766418457
    },
    {
      "epoch": 0.03018970189701897,
      "step": 557,
      "training_loss": 7.4127960205078125
    },
    {
      "epoch": 0.03024390243902439,
      "step": 558,
      "training_loss": 7.562286853790283
    },
    {
      "epoch": 0.03029810298102981,
      "step": 559,
      "training_loss": 6.267316818237305
    },
    {
      "epoch": 0.03035230352303523,
      "grad_norm": 13.241568565368652,
      "learning_rate": 1e-05,
      "loss": 7.3121,
      "step": 560
    },
    {
      "epoch": 0.03035230352303523,
      "step": 560,
      "training_loss": 8.27786636352539
    },
    {
      "epoch": 0.03040650406504065,
      "step": 561,
      "training_loss": 7.505637168884277
    },
    {
      "epoch": 0.03046070460704607,
      "step": 562,
      "training_loss": 6.8319244384765625
    },
    {
      "epoch": 0.030514905149051492,
      "step": 563,
      "training_loss": 6.262115955352783
    },
    {
      "epoch": 0.030569105691056912,
      "grad_norm": 13.091245651245117,
      "learning_rate": 1e-05,
      "loss": 7.2194,
      "step": 564
    },
    {
      "epoch": 0.030569105691056912,
      "step": 564,
      "training_loss": 7.486822605133057
    },
    {
      "epoch": 0.03062330623306233,
      "step": 565,
      "training_loss": 6.506314754486084
    },
    {
      "epoch": 0.03067750677506775,
      "step": 566,
      "training_loss": 7.852730751037598
    },
    {
      "epoch": 0.03073170731707317,
      "step": 567,
      "training_loss": 9.516322135925293
    },
    {
      "epoch": 0.03078590785907859,
      "grad_norm": 22.927791595458984,
      "learning_rate": 1e-05,
      "loss": 7.8405,
      "step": 568
    },
    {
      "epoch": 0.03078590785907859,
      "step": 568,
      "training_loss": 7.596273899078369
    },
    {
      "epoch": 0.03084010840108401,
      "step": 569,
      "training_loss": 6.8946309089660645
    },
    {
      "epoch": 0.030894308943089432,
      "step": 570,
      "training_loss": 7.808889865875244
    },
    {
      "epoch": 0.030948509485094852,
      "step": 571,
      "training_loss": 7.22052001953125
    },
    {
      "epoch": 0.031002710027100273,
      "grad_norm": 19.072341918945312,
      "learning_rate": 1e-05,
      "loss": 7.3801,
      "step": 572
    },
    {
      "epoch": 0.031002710027100273,
      "step": 572,
      "training_loss": 7.628958225250244
    },
    {
      "epoch": 0.03105691056910569,
      "step": 573,
      "training_loss": 7.715534687042236
    },
    {
      "epoch": 0.03111111111111111,
      "step": 574,
      "training_loss": 9.171743392944336
    },
    {
      "epoch": 0.03116531165311653,
      "step": 575,
      "training_loss": 6.987898349761963
    },
    {
      "epoch": 0.03121951219512195,
      "grad_norm": 13.227346420288086,
      "learning_rate": 1e-05,
      "loss": 7.876,
      "step": 576
    },
    {
      "epoch": 0.03121951219512195,
      "step": 576,
      "training_loss": 7.856621742248535
    },
    {
      "epoch": 0.03127371273712737,
      "step": 577,
      "training_loss": 8.118185043334961
    },
    {
      "epoch": 0.03132791327913279,
      "step": 578,
      "training_loss": 7.513347625732422
    },
    {
      "epoch": 0.03138211382113821,
      "step": 579,
      "training_loss": 6.811148166656494
    },
    {
      "epoch": 0.03143631436314363,
      "grad_norm": 21.458024978637695,
      "learning_rate": 1e-05,
      "loss": 7.5748,
      "step": 580
    },
    {
      "epoch": 0.03143631436314363,
      "step": 580,
      "training_loss": 6.786680698394775
    },
    {
      "epoch": 0.03149051490514905,
      "step": 581,
      "training_loss": 7.159631252288818
    },
    {
      "epoch": 0.031544715447154474,
      "step": 582,
      "training_loss": 8.047096252441406
    },
    {
      "epoch": 0.03159891598915989,
      "step": 583,
      "training_loss": 7.907321929931641
    },
    {
      "epoch": 0.031653116531165315,
      "grad_norm": 21.504545211791992,
      "learning_rate": 1e-05,
      "loss": 7.4752,
      "step": 584
    },
    {
      "epoch": 0.031653116531165315,
      "step": 584,
      "training_loss": 7.784085273742676
    },
    {
      "epoch": 0.03170731707317073,
      "step": 585,
      "training_loss": 8.942150115966797
    },
    {
      "epoch": 0.03176151761517615,
      "step": 586,
      "training_loss": 9.160422325134277
    },
    {
      "epoch": 0.03181571815718157,
      "step": 587,
      "training_loss": 8.868200302124023
    },
    {
      "epoch": 0.03186991869918699,
      "grad_norm": 29.23548126220703,
      "learning_rate": 1e-05,
      "loss": 8.6887,
      "step": 588
    },
    {
      "epoch": 0.03186991869918699,
      "step": 588,
      "training_loss": 6.81386661529541
    },
    {
      "epoch": 0.031924119241192414,
      "step": 589,
      "training_loss": 8.087862968444824
    },
    {
      "epoch": 0.03197831978319783,
      "step": 590,
      "training_loss": 7.766304016113281
    },
    {
      "epoch": 0.032032520325203255,
      "step": 591,
      "training_loss": 7.51026725769043
    },
    {
      "epoch": 0.03208672086720867,
      "grad_norm": 10.99610710144043,
      "learning_rate": 1e-05,
      "loss": 7.5446,
      "step": 592
    },
    {
      "epoch": 0.03208672086720867,
      "step": 592,
      "training_loss": 7.143747806549072
    },
    {
      "epoch": 0.03214092140921409,
      "step": 593,
      "training_loss": 7.672973155975342
    },
    {
      "epoch": 0.03219512195121951,
      "step": 594,
      "training_loss": 7.325706481933594
    },
    {
      "epoch": 0.03224932249322493,
      "step": 595,
      "training_loss": 8.01272201538086
    },
    {
      "epoch": 0.032303523035230354,
      "grad_norm": 19.675317764282227,
      "learning_rate": 1e-05,
      "loss": 7.5388,
      "step": 596
    },
    {
      "epoch": 0.032303523035230354,
      "step": 596,
      "training_loss": 7.754485607147217
    },
    {
      "epoch": 0.03235772357723577,
      "step": 597,
      "training_loss": 8.303577423095703
    },
    {
      "epoch": 0.032411924119241195,
      "step": 598,
      "training_loss": 7.519500255584717
    },
    {
      "epoch": 0.03246612466124661,
      "step": 599,
      "training_loss": 7.545088291168213
    },
    {
      "epoch": 0.032520325203252036,
      "grad_norm": 11.03748607635498,
      "learning_rate": 1e-05,
      "loss": 7.7807,
      "step": 600
    },
    {
      "epoch": 0.032520325203252036,
      "step": 600,
      "training_loss": 8.318832397460938
    },
    {
      "epoch": 0.03257452574525745,
      "step": 601,
      "training_loss": 8.336999893188477
    },
    {
      "epoch": 0.03262872628726287,
      "step": 602,
      "training_loss": 8.142541885375977
    },
    {
      "epoch": 0.032682926829268294,
      "step": 603,
      "training_loss": 7.395812511444092
    },
    {
      "epoch": 0.03273712737127371,
      "grad_norm": 13.745853424072266,
      "learning_rate": 1e-05,
      "loss": 8.0485,
      "step": 604
    },
    {
      "epoch": 0.03273712737127371,
      "step": 604,
      "training_loss": 7.020951747894287
    },
    {
      "epoch": 0.032791327913279135,
      "step": 605,
      "training_loss": 6.944255828857422
    },
    {
      "epoch": 0.03284552845528455,
      "step": 606,
      "training_loss": 7.477126121520996
    },
    {
      "epoch": 0.032899728997289976,
      "step": 607,
      "training_loss": 5.148961544036865
    },
    {
      "epoch": 0.03295392953929539,
      "grad_norm": 27.703140258789062,
      "learning_rate": 1e-05,
      "loss": 6.6478,
      "step": 608
    },
    {
      "epoch": 0.03295392953929539,
      "step": 608,
      "training_loss": 7.075716972351074
    },
    {
      "epoch": 0.03300813008130081,
      "step": 609,
      "training_loss": 7.870292663574219
    },
    {
      "epoch": 0.033062330623306234,
      "step": 610,
      "training_loss": 7.562296390533447
    },
    {
      "epoch": 0.03311653116531165,
      "step": 611,
      "training_loss": 6.005953788757324
    },
    {
      "epoch": 0.033170731707317075,
      "grad_norm": 16.46178436279297,
      "learning_rate": 1e-05,
      "loss": 7.1286,
      "step": 612
    },
    {
      "epoch": 0.033170731707317075,
      "step": 612,
      "training_loss": 7.661654472351074
    },
    {
      "epoch": 0.03322493224932249,
      "step": 613,
      "training_loss": 8.430442810058594
    },
    {
      "epoch": 0.033279132791327916,
      "step": 614,
      "training_loss": 8.001923561096191
    },
    {
      "epoch": 0.03333333333333333,
      "step": 615,
      "training_loss": 6.9261674880981445
    },
    {
      "epoch": 0.03338753387533876,
      "grad_norm": 14.56048583984375,
      "learning_rate": 1e-05,
      "loss": 7.755,
      "step": 616
    },
    {
      "epoch": 0.03338753387533876,
      "step": 616,
      "training_loss": 7.840579509735107
    },
    {
      "epoch": 0.033441734417344174,
      "step": 617,
      "training_loss": 7.342605113983154
    },
    {
      "epoch": 0.03349593495934959,
      "step": 618,
      "training_loss": 6.175075531005859
    },
    {
      "epoch": 0.033550135501355015,
      "step": 619,
      "training_loss": 7.016615867614746
    },
    {
      "epoch": 0.03360433604336043,
      "grad_norm": 14.835762023925781,
      "learning_rate": 1e-05,
      "loss": 7.0937,
      "step": 620
    },
    {
      "epoch": 0.03360433604336043,
      "step": 620,
      "training_loss": 8.156594276428223
    },
    {
      "epoch": 0.033658536585365856,
      "step": 621,
      "training_loss": 7.633522987365723
    },
    {
      "epoch": 0.03371273712737127,
      "step": 622,
      "training_loss": 7.895918846130371
    },
    {
      "epoch": 0.0337669376693767,
      "step": 623,
      "training_loss": 8.076791763305664
    },
    {
      "epoch": 0.033821138211382114,
      "grad_norm": 18.83048439025879,
      "learning_rate": 1e-05,
      "loss": 7.9407,
      "step": 624
    },
    {
      "epoch": 0.033821138211382114,
      "step": 624,
      "training_loss": 8.693800926208496
    },
    {
      "epoch": 0.03387533875338753,
      "step": 625,
      "training_loss": 8.160466194152832
    },
    {
      "epoch": 0.033929539295392955,
      "step": 626,
      "training_loss": 7.449671745300293
    },
    {
      "epoch": 0.03398373983739837,
      "step": 627,
      "training_loss": 7.530853271484375
    },
    {
      "epoch": 0.034037940379403796,
      "grad_norm": 16.453369140625,
      "learning_rate": 1e-05,
      "loss": 7.9587,
      "step": 628
    },
    {
      "epoch": 0.034037940379403796,
      "step": 628,
      "training_loss": 6.371591567993164
    },
    {
      "epoch": 0.03409214092140921,
      "step": 629,
      "training_loss": 7.984294891357422
    },
    {
      "epoch": 0.03414634146341464,
      "step": 630,
      "training_loss": 6.843656063079834
    },
    {
      "epoch": 0.034200542005420054,
      "step": 631,
      "training_loss": 7.089364528656006
    },
    {
      "epoch": 0.03425474254742548,
      "grad_norm": 18.147918701171875,
      "learning_rate": 1e-05,
      "loss": 7.0722,
      "step": 632
    },
    {
      "epoch": 0.03425474254742548,
      "step": 632,
      "training_loss": 5.8620734214782715
    },
    {
      "epoch": 0.034308943089430895,
      "step": 633,
      "training_loss": 7.434935092926025
    },
    {
      "epoch": 0.03436314363143631,
      "step": 634,
      "training_loss": 7.697505474090576
    },
    {
      "epoch": 0.034417344173441736,
      "step": 635,
      "training_loss": 8.205038070678711
    },
    {
      "epoch": 0.03447154471544715,
      "grad_norm": 14.699790954589844,
      "learning_rate": 1e-05,
      "loss": 7.2999,
      "step": 636
    },
    {
      "epoch": 0.03447154471544715,
      "step": 636,
      "training_loss": 8.378556251525879
    },
    {
      "epoch": 0.03452574525745258,
      "step": 637,
      "training_loss": 7.40627908706665
    },
    {
      "epoch": 0.034579945799457994,
      "step": 638,
      "training_loss": 6.95586633682251
    },
    {
      "epoch": 0.03463414634146342,
      "step": 639,
      "training_loss": 8.133492469787598
    },
    {
      "epoch": 0.034688346883468835,
      "grad_norm": 18.504623413085938,
      "learning_rate": 1e-05,
      "loss": 7.7185,
      "step": 640
    },
    {
      "epoch": 0.034688346883468835,
      "step": 640,
      "training_loss": 6.613019943237305
    },
    {
      "epoch": 0.03474254742547425,
      "step": 641,
      "training_loss": 6.14841890335083
    },
    {
      "epoch": 0.034796747967479676,
      "step": 642,
      "training_loss": 6.311915874481201
    },
    {
      "epoch": 0.03485094850948509,
      "step": 643,
      "training_loss": 7.22858190536499
    },
    {
      "epoch": 0.03490514905149052,
      "grad_norm": 15.910090446472168,
      "learning_rate": 1e-05,
      "loss": 6.5755,
      "step": 644
    },
    {
      "epoch": 0.03490514905149052,
      "step": 644,
      "training_loss": 8.16611099243164
    },
    {
      "epoch": 0.034959349593495934,
      "step": 645,
      "training_loss": 8.402572631835938
    },
    {
      "epoch": 0.03501355013550136,
      "step": 646,
      "training_loss": 6.247028827667236
    },
    {
      "epoch": 0.035067750677506775,
      "step": 647,
      "training_loss": 7.658950328826904
    },
    {
      "epoch": 0.0351219512195122,
      "grad_norm": 14.55771255493164,
      "learning_rate": 1e-05,
      "loss": 7.6187,
      "step": 648
    },
    {
      "epoch": 0.0351219512195122,
      "step": 648,
      "training_loss": 8.1662015914917
    },
    {
      "epoch": 0.035176151761517616,
      "step": 649,
      "training_loss": 5.8511457443237305
    },
    {
      "epoch": 0.03523035230352303,
      "step": 650,
      "training_loss": 7.341268539428711
    },
    {
      "epoch": 0.03528455284552846,
      "step": 651,
      "training_loss": 9.425777435302734
    },
    {
      "epoch": 0.035338753387533874,
      "grad_norm": 22.843904495239258,
      "learning_rate": 1e-05,
      "loss": 7.6961,
      "step": 652
    },
    {
      "epoch": 0.035338753387533874,
      "step": 652,
      "training_loss": 7.36356782913208
    },
    {
      "epoch": 0.0353929539295393,
      "step": 653,
      "training_loss": 7.418573379516602
    },
    {
      "epoch": 0.035447154471544715,
      "step": 654,
      "training_loss": 8.25290298461914
    },
    {
      "epoch": 0.03550135501355014,
      "step": 655,
      "training_loss": 7.1481451988220215
    },
    {
      "epoch": 0.035555555555555556,
      "grad_norm": 16.31294822692871,
      "learning_rate": 1e-05,
      "loss": 7.5458,
      "step": 656
    },
    {
      "epoch": 0.035555555555555556,
      "step": 656,
      "training_loss": 7.939426422119141
    },
    {
      "epoch": 0.03560975609756097,
      "step": 657,
      "training_loss": 6.82961893081665
    },
    {
      "epoch": 0.035663956639566397,
      "step": 658,
      "training_loss": 8.04493236541748
    },
    {
      "epoch": 0.035718157181571814,
      "step": 659,
      "training_loss": 6.600935459136963
    },
    {
      "epoch": 0.03577235772357724,
      "grad_norm": 11.468917846679688,
      "learning_rate": 1e-05,
      "loss": 7.3537,
      "step": 660
    },
    {
      "epoch": 0.03577235772357724,
      "step": 660,
      "training_loss": 5.9979729652404785
    },
    {
      "epoch": 0.035826558265582654,
      "step": 661,
      "training_loss": 7.510555267333984
    },
    {
      "epoch": 0.03588075880758808,
      "step": 662,
      "training_loss": 6.637672424316406
    },
    {
      "epoch": 0.035934959349593495,
      "step": 663,
      "training_loss": 6.011204719543457
    },
    {
      "epoch": 0.03598915989159892,
      "grad_norm": 21.313053131103516,
      "learning_rate": 1e-05,
      "loss": 6.5394,
      "step": 664
    },
    {
      "epoch": 0.03598915989159892,
      "step": 664,
      "training_loss": 7.330417156219482
    },
    {
      "epoch": 0.036043360433604336,
      "step": 665,
      "training_loss": 6.95147705078125
    },
    {
      "epoch": 0.03609756097560975,
      "step": 666,
      "training_loss": 8.35532283782959
    },
    {
      "epoch": 0.03615176151761518,
      "step": 667,
      "training_loss": 8.489896774291992
    },
    {
      "epoch": 0.036205962059620594,
      "grad_norm": 14.010684967041016,
      "learning_rate": 1e-05,
      "loss": 7.7818,
      "step": 668
    },
    {
      "epoch": 0.036205962059620594,
      "step": 668,
      "training_loss": 7.910189151763916
    },
    {
      "epoch": 0.03626016260162602,
      "step": 669,
      "training_loss": 7.940420150756836
    },
    {
      "epoch": 0.036314363143631435,
      "step": 670,
      "training_loss": 7.8779449462890625
    },
    {
      "epoch": 0.03636856368563686,
      "step": 671,
      "training_loss": 6.968067646026611
    },
    {
      "epoch": 0.036422764227642276,
      "grad_norm": 16.165576934814453,
      "learning_rate": 1e-05,
      "loss": 7.6742,
      "step": 672
    },
    {
      "epoch": 0.036422764227642276,
      "step": 672,
      "training_loss": 7.153119087219238
    },
    {
      "epoch": 0.03647696476964769,
      "step": 673,
      "training_loss": 8.627752304077148
    },
    {
      "epoch": 0.03653116531165312,
      "step": 674,
      "training_loss": 5.915846347808838
    },
    {
      "epoch": 0.036585365853658534,
      "step": 675,
      "training_loss": 8.214350700378418
    },
    {
      "epoch": 0.03663956639566396,
      "grad_norm": 18.81600570678711,
      "learning_rate": 1e-05,
      "loss": 7.4778,
      "step": 676
    },
    {
      "epoch": 0.03663956639566396,
      "step": 676,
      "training_loss": 8.202044486999512
    },
    {
      "epoch": 0.036693766937669375,
      "step": 677,
      "training_loss": 7.135343074798584
    },
    {
      "epoch": 0.0367479674796748,
      "step": 678,
      "training_loss": 6.442089080810547
    },
    {
      "epoch": 0.036802168021680216,
      "step": 679,
      "training_loss": 5.712493419647217
    },
    {
      "epoch": 0.03685636856368564,
      "grad_norm": 29.244075775146484,
      "learning_rate": 1e-05,
      "loss": 6.873,
      "step": 680
    },
    {
      "epoch": 0.03685636856368564,
      "step": 680,
      "training_loss": 8.153648376464844
    },
    {
      "epoch": 0.03691056910569106,
      "step": 681,
      "training_loss": 8.160945892333984
    },
    {
      "epoch": 0.036964769647696474,
      "step": 682,
      "training_loss": 8.450806617736816
    },
    {
      "epoch": 0.0370189701897019,
      "step": 683,
      "training_loss": 7.582562446594238
    },
    {
      "epoch": 0.037073170731707315,
      "grad_norm": 12.198261260986328,
      "learning_rate": 1e-05,
      "loss": 8.087,
      "step": 684
    },
    {
      "epoch": 0.037073170731707315,
      "step": 684,
      "training_loss": 7.470959663391113
    },
    {
      "epoch": 0.03712737127371274,
      "step": 685,
      "training_loss": 7.416088104248047
    },
    {
      "epoch": 0.037181571815718156,
      "step": 686,
      "training_loss": 9.539102554321289
    },
    {
      "epoch": 0.03723577235772358,
      "step": 687,
      "training_loss": 7.1583147048950195
    },
    {
      "epoch": 0.037289972899729,
      "grad_norm": 25.945592880249023,
      "learning_rate": 1e-05,
      "loss": 7.8961,
      "step": 688
    },
    {
      "epoch": 0.037289972899729,
      "step": 688,
      "training_loss": 8.239164352416992
    },
    {
      "epoch": 0.037344173441734414,
      "step": 689,
      "training_loss": 7.099133491516113
    },
    {
      "epoch": 0.03739837398373984,
      "step": 690,
      "training_loss": 8.468503952026367
    },
    {
      "epoch": 0.037452574525745255,
      "step": 691,
      "training_loss": 7.67402982711792
    },
    {
      "epoch": 0.03750677506775068,
      "grad_norm": 11.716397285461426,
      "learning_rate": 1e-05,
      "loss": 7.8702,
      "step": 692
    },
    {
      "epoch": 0.03750677506775068,
      "step": 692,
      "training_loss": 7.378066062927246
    },
    {
      "epoch": 0.037560975609756096,
      "step": 693,
      "training_loss": 6.541484832763672
    },
    {
      "epoch": 0.03761517615176152,
      "step": 694,
      "training_loss": 7.551252365112305
    },
    {
      "epoch": 0.03766937669376694,
      "step": 695,
      "training_loss": 8.089284896850586
    },
    {
      "epoch": 0.03772357723577236,
      "grad_norm": 12.184758186340332,
      "learning_rate": 1e-05,
      "loss": 7.39,
      "step": 696
    },
    {
      "epoch": 0.03772357723577236,
      "step": 696,
      "training_loss": 8.257721900939941
    },
    {
      "epoch": 0.03777777777777778,
      "step": 697,
      "training_loss": 6.292238712310791
    },
    {
      "epoch": 0.037831978319783195,
      "step": 698,
      "training_loss": 6.569148540496826
    },
    {
      "epoch": 0.03788617886178862,
      "step": 699,
      "training_loss": 7.420546531677246
    },
    {
      "epoch": 0.037940379403794036,
      "grad_norm": 14.983966827392578,
      "learning_rate": 1e-05,
      "loss": 7.1349,
      "step": 700
    },
    {
      "epoch": 0.037940379403794036,
      "step": 700,
      "training_loss": 6.714663982391357
    },
    {
      "epoch": 0.03799457994579946,
      "step": 701,
      "training_loss": 6.584252834320068
    },
    {
      "epoch": 0.03804878048780488,
      "step": 702,
      "training_loss": 6.620905876159668
    },
    {
      "epoch": 0.0381029810298103,
      "step": 703,
      "training_loss": 6.902586460113525
    },
    {
      "epoch": 0.03815718157181572,
      "grad_norm": 17.378864288330078,
      "learning_rate": 1e-05,
      "loss": 6.7056,
      "step": 704
    },
    {
      "epoch": 0.03815718157181572,
      "step": 704,
      "training_loss": 6.1832756996154785
    },
    {
      "epoch": 0.038211382113821135,
      "step": 705,
      "training_loss": 5.44205379486084
    },
    {
      "epoch": 0.03826558265582656,
      "step": 706,
      "training_loss": 8.830633163452148
    },
    {
      "epoch": 0.038319783197831976,
      "step": 707,
      "training_loss": 7.2137064933776855
    },
    {
      "epoch": 0.0383739837398374,
      "grad_norm": 22.71533966064453,
      "learning_rate": 1e-05,
      "loss": 6.9174,
      "step": 708
    },
    {
      "epoch": 0.0383739837398374,
      "step": 708,
      "training_loss": 8.376605033874512
    },
    {
      "epoch": 0.03842818428184282,
      "step": 709,
      "training_loss": 7.329748153686523
    },
    {
      "epoch": 0.03848238482384824,
      "step": 710,
      "training_loss": 7.6830878257751465
    },
    {
      "epoch": 0.03853658536585366,
      "step": 711,
      "training_loss": 7.433039665222168
    },
    {
      "epoch": 0.03859078590785908,
      "grad_norm": 15.80206298828125,
      "learning_rate": 1e-05,
      "loss": 7.7056,
      "step": 712
    },
    {
      "epoch": 0.03859078590785908,
      "step": 712,
      "training_loss": 7.516097545623779
    },
    {
      "epoch": 0.0386449864498645,
      "step": 713,
      "training_loss": 6.4643330574035645
    },
    {
      "epoch": 0.038699186991869916,
      "step": 714,
      "training_loss": 5.9585723876953125
    },
    {
      "epoch": 0.03875338753387534,
      "step": 715,
      "training_loss": 8.432138442993164
    },
    {
      "epoch": 0.03880758807588076,
      "grad_norm": 18.99262237548828,
      "learning_rate": 1e-05,
      "loss": 7.0928,
      "step": 716
    },
    {
      "epoch": 0.03880758807588076,
      "step": 716,
      "training_loss": 8.328255653381348
    },
    {
      "epoch": 0.03886178861788618,
      "step": 717,
      "training_loss": 8.119059562683105
    },
    {
      "epoch": 0.0389159891598916,
      "step": 718,
      "training_loss": 6.933791637420654
    },
    {
      "epoch": 0.03897018970189702,
      "step": 719,
      "training_loss": 7.218845844268799
    },
    {
      "epoch": 0.03902439024390244,
      "grad_norm": 18.767086029052734,
      "learning_rate": 1e-05,
      "loss": 7.65,
      "step": 720
    },
    {
      "epoch": 0.03902439024390244,
      "step": 720,
      "training_loss": 6.973489761352539
    },
    {
      "epoch": 0.039078590785907856,
      "step": 721,
      "training_loss": 7.563920021057129
    },
    {
      "epoch": 0.03913279132791328,
      "step": 722,
      "training_loss": 7.615703582763672
    },
    {
      "epoch": 0.0391869918699187,
      "step": 723,
      "training_loss": 7.2648091316223145
    },
    {
      "epoch": 0.03924119241192412,
      "grad_norm": 12.964652061462402,
      "learning_rate": 1e-05,
      "loss": 7.3545,
      "step": 724
    },
    {
      "epoch": 0.03924119241192412,
      "step": 724,
      "training_loss": 7.467070579528809
    },
    {
      "epoch": 0.03929539295392954,
      "step": 725,
      "training_loss": 8.62618637084961
    },
    {
      "epoch": 0.03934959349593496,
      "step": 726,
      "training_loss": 6.898422718048096
    },
    {
      "epoch": 0.03940379403794038,
      "step": 727,
      "training_loss": 7.6373610496521
    },
    {
      "epoch": 0.0394579945799458,
      "grad_norm": 16.635196685791016,
      "learning_rate": 1e-05,
      "loss": 7.6573,
      "step": 728
    },
    {
      "epoch": 0.0394579945799458,
      "step": 728,
      "training_loss": 7.55942964553833
    },
    {
      "epoch": 0.03951219512195122,
      "step": 729,
      "training_loss": 7.1468329429626465
    },
    {
      "epoch": 0.03956639566395664,
      "step": 730,
      "training_loss": 7.739222049713135
    },
    {
      "epoch": 0.03962059620596206,
      "step": 731,
      "training_loss": 7.401004314422607
    },
    {
      "epoch": 0.03967479674796748,
      "grad_norm": 14.708269119262695,
      "learning_rate": 1e-05,
      "loss": 7.4616,
      "step": 732
    },
    {
      "epoch": 0.03967479674796748,
      "step": 732,
      "training_loss": 7.24678373336792
    },
    {
      "epoch": 0.0397289972899729,
      "step": 733,
      "training_loss": 5.748908519744873
    },
    {
      "epoch": 0.03978319783197832,
      "step": 734,
      "training_loss": 6.049829483032227
    },
    {
      "epoch": 0.03983739837398374,
      "step": 735,
      "training_loss": 6.159605026245117
    },
    {
      "epoch": 0.03989159891598916,
      "grad_norm": 13.45759391784668,
      "learning_rate": 1e-05,
      "loss": 6.3013,
      "step": 736
    },
    {
      "epoch": 0.03989159891598916,
      "step": 736,
      "training_loss": 5.773135662078857
    },
    {
      "epoch": 0.03994579945799458,
      "step": 737,
      "training_loss": 7.824631214141846
    },
    {
      "epoch": 0.04,
      "step": 738,
      "training_loss": 7.847968578338623
    },
    {
      "epoch": 0.04005420054200542,
      "step": 739,
      "training_loss": 7.686949253082275
    },
    {
      "epoch": 0.04010840108401084,
      "grad_norm": 16.186946868896484,
      "learning_rate": 1e-05,
      "loss": 7.2832,
      "step": 740
    },
    {
      "epoch": 0.04010840108401084,
      "step": 740,
      "training_loss": 7.519392490386963
    },
    {
      "epoch": 0.04016260162601626,
      "step": 741,
      "training_loss": 7.683074474334717
    },
    {
      "epoch": 0.04021680216802168,
      "step": 742,
      "training_loss": 7.522810459136963
    },
    {
      "epoch": 0.0402710027100271,
      "step": 743,
      "training_loss": 8.632411003112793
    },
    {
      "epoch": 0.040325203252032524,
      "grad_norm": 15.69361686706543,
      "learning_rate": 1e-05,
      "loss": 7.8394,
      "step": 744
    },
    {
      "epoch": 0.040325203252032524,
      "step": 744,
      "training_loss": 7.715400695800781
    },
    {
      "epoch": 0.04037940379403794,
      "step": 745,
      "training_loss": 7.8688578605651855
    },
    {
      "epoch": 0.04043360433604336,
      "step": 746,
      "training_loss": 7.59503173828125
    },
    {
      "epoch": 0.04048780487804878,
      "step": 747,
      "training_loss": 8.285539627075195
    },
    {
      "epoch": 0.0405420054200542,
      "grad_norm": 11.197763442993164,
      "learning_rate": 1e-05,
      "loss": 7.8662,
      "step": 748
    },
    {
      "epoch": 0.0405420054200542,
      "step": 748,
      "training_loss": 7.2277512550354
    },
    {
      "epoch": 0.04059620596205962,
      "step": 749,
      "training_loss": 8.54175090789795
    },
    {
      "epoch": 0.04065040650406504,
      "step": 750,
      "training_loss": 6.748481750488281
    },
    {
      "epoch": 0.040704607046070464,
      "step": 751,
      "training_loss": 7.085055351257324
    },
    {
      "epoch": 0.04075880758807588,
      "grad_norm": 64.28804016113281,
      "learning_rate": 1e-05,
      "loss": 7.4008,
      "step": 752
    },
    {
      "epoch": 0.04075880758807588,
      "step": 752,
      "training_loss": 7.578892230987549
    },
    {
      "epoch": 0.0408130081300813,
      "step": 753,
      "training_loss": 6.7549824714660645
    },
    {
      "epoch": 0.04086720867208672,
      "step": 754,
      "training_loss": 7.358738422393799
    },
    {
      "epoch": 0.04092140921409214,
      "step": 755,
      "training_loss": 8.599678993225098
    },
    {
      "epoch": 0.04097560975609756,
      "grad_norm": 29.437355041503906,
      "learning_rate": 1e-05,
      "loss": 7.5731,
      "step": 756
    },
    {
      "epoch": 0.04097560975609756,
      "step": 756,
      "training_loss": 6.446199893951416
    },
    {
      "epoch": 0.04102981029810298,
      "step": 757,
      "training_loss": 7.97297477722168
    },
    {
      "epoch": 0.041084010840108404,
      "step": 758,
      "training_loss": 8.171884536743164
    },
    {
      "epoch": 0.04113821138211382,
      "step": 759,
      "training_loss": 7.620614051818848
    },
    {
      "epoch": 0.041192411924119245,
      "grad_norm": 15.04110050201416,
      "learning_rate": 1e-05,
      "loss": 7.5529,
      "step": 760
    },
    {
      "epoch": 0.041192411924119245,
      "step": 760,
      "training_loss": 7.398055076599121
    },
    {
      "epoch": 0.04124661246612466,
      "step": 761,
      "training_loss": 7.762007236480713
    },
    {
      "epoch": 0.04130081300813008,
      "step": 762,
      "training_loss": 7.657063007354736
    },
    {
      "epoch": 0.0413550135501355,
      "step": 763,
      "training_loss": 6.44507360458374
    },
    {
      "epoch": 0.04140921409214092,
      "grad_norm": 19.448123931884766,
      "learning_rate": 1e-05,
      "loss": 7.3155,
      "step": 764
    },
    {
      "epoch": 0.04140921409214092,
      "step": 764,
      "training_loss": 8.893157005310059
    },
    {
      "epoch": 0.041463414634146344,
      "step": 765,
      "training_loss": 8.004855155944824
    },
    {
      "epoch": 0.04151761517615176,
      "step": 766,
      "training_loss": 8.95726490020752
    },
    {
      "epoch": 0.041571815718157185,
      "step": 767,
      "training_loss": 7.944192409515381
    },
    {
      "epoch": 0.0416260162601626,
      "grad_norm": 12.517049789428711,
      "learning_rate": 1e-05,
      "loss": 8.4499,
      "step": 768
    },
    {
      "epoch": 0.0416260162601626,
      "step": 768,
      "training_loss": 7.222275257110596
    },
    {
      "epoch": 0.04168021680216802,
      "step": 769,
      "training_loss": 6.584643363952637
    },
    {
      "epoch": 0.04173441734417344,
      "step": 770,
      "training_loss": 7.481154918670654
    },
    {
      "epoch": 0.04178861788617886,
      "step": 771,
      "training_loss": 7.401394367218018
    },
    {
      "epoch": 0.041842818428184284,
      "grad_norm": 19.670612335205078,
      "learning_rate": 1e-05,
      "loss": 7.1724,
      "step": 772
    },
    {
      "epoch": 0.041842818428184284,
      "step": 772,
      "training_loss": 6.846065521240234
    },
    {
      "epoch": 0.0418970189701897,
      "step": 773,
      "training_loss": 7.272346019744873
    },
    {
      "epoch": 0.041951219512195125,
      "step": 774,
      "training_loss": 7.539729595184326
    },
    {
      "epoch": 0.04200542005420054,
      "step": 775,
      "training_loss": 7.643473148345947
    },
    {
      "epoch": 0.042059620596205965,
      "grad_norm": 15.767237663269043,
      "learning_rate": 1e-05,
      "loss": 7.3254,
      "step": 776
    },
    {
      "epoch": 0.042059620596205965,
      "step": 776,
      "training_loss": 7.768119812011719
    },
    {
      "epoch": 0.04211382113821138,
      "step": 777,
      "training_loss": 8.141231536865234
    },
    {
      "epoch": 0.0421680216802168,
      "step": 778,
      "training_loss": 7.020600318908691
    },
    {
      "epoch": 0.042222222222222223,
      "step": 779,
      "training_loss": 7.396282196044922
    },
    {
      "epoch": 0.04227642276422764,
      "grad_norm": 13.881558418273926,
      "learning_rate": 1e-05,
      "loss": 7.5816,
      "step": 780
    },
    {
      "epoch": 0.04227642276422764,
      "step": 780,
      "training_loss": 7.871460437774658
    },
    {
      "epoch": 0.042330623306233064,
      "step": 781,
      "training_loss": 7.26619291305542
    },
    {
      "epoch": 0.04238482384823848,
      "step": 782,
      "training_loss": 7.747412204742432
    },
    {
      "epoch": 0.042439024390243905,
      "step": 783,
      "training_loss": 9.669589042663574
    },
    {
      "epoch": 0.04249322493224932,
      "grad_norm": 50.625267028808594,
      "learning_rate": 1e-05,
      "loss": 8.1387,
      "step": 784
    },
    {
      "epoch": 0.04249322493224932,
      "step": 784,
      "training_loss": 7.909923553466797
    },
    {
      "epoch": 0.04254742547425474,
      "step": 785,
      "training_loss": 7.110124588012695
    },
    {
      "epoch": 0.04260162601626016,
      "step": 786,
      "training_loss": 8.00576400756836
    },
    {
      "epoch": 0.04265582655826558,
      "step": 787,
      "training_loss": 7.681179523468018
    },
    {
      "epoch": 0.042710027100271004,
      "grad_norm": 11.550066947937012,
      "learning_rate": 1e-05,
      "loss": 7.6767,
      "step": 788
    },
    {
      "epoch": 0.042710027100271004,
      "step": 788,
      "training_loss": 7.251941680908203
    },
    {
      "epoch": 0.04276422764227642,
      "step": 789,
      "training_loss": 6.5937700271606445
    },
    {
      "epoch": 0.042818428184281845,
      "step": 790,
      "training_loss": 6.456654071807861
    },
    {
      "epoch": 0.04287262872628726,
      "step": 791,
      "training_loss": 7.992836952209473
    },
    {
      "epoch": 0.042926829268292686,
      "grad_norm": 25.286224365234375,
      "learning_rate": 1e-05,
      "loss": 7.0738,
      "step": 792
    },
    {
      "epoch": 0.042926829268292686,
      "step": 792,
      "training_loss": 7.279108047485352
    },
    {
      "epoch": 0.0429810298102981,
      "step": 793,
      "training_loss": 7.841796398162842
    },
    {
      "epoch": 0.04303523035230352,
      "step": 794,
      "training_loss": 7.987884521484375
    },
    {
      "epoch": 0.043089430894308944,
      "step": 795,
      "training_loss": 7.5241570472717285
    },
    {
      "epoch": 0.04314363143631436,
      "grad_norm": 20.65587615966797,
      "learning_rate": 1e-05,
      "loss": 7.6582,
      "step": 796
    },
    {
      "epoch": 0.04314363143631436,
      "step": 796,
      "training_loss": 6.852330684661865
    },
    {
      "epoch": 0.043197831978319785,
      "step": 797,
      "training_loss": 6.557071208953857
    },
    {
      "epoch": 0.0432520325203252,
      "step": 798,
      "training_loss": 5.717689514160156
    },
    {
      "epoch": 0.043306233062330626,
      "step": 799,
      "training_loss": 6.757059574127197
    },
    {
      "epoch": 0.04336043360433604,
      "grad_norm": 24.131460189819336,
      "learning_rate": 1e-05,
      "loss": 6.471,
      "step": 800
    },
    {
      "epoch": 0.04336043360433604,
      "step": 800,
      "training_loss": 7.316311836242676
    },
    {
      "epoch": 0.04341463414634146,
      "step": 801,
      "training_loss": 8.068388938903809
    },
    {
      "epoch": 0.043468834688346884,
      "step": 802,
      "training_loss": 6.338033199310303
    },
    {
      "epoch": 0.0435230352303523,
      "step": 803,
      "training_loss": 7.67354154586792
    },
    {
      "epoch": 0.043577235772357725,
      "grad_norm": 30.85428237915039,
      "learning_rate": 1e-05,
      "loss": 7.3491,
      "step": 804
    },
    {
      "epoch": 0.043577235772357725,
      "step": 804,
      "training_loss": 6.531081199645996
    },
    {
      "epoch": 0.04363143631436314,
      "step": 805,
      "training_loss": 7.523169040679932
    },
    {
      "epoch": 0.043685636856368566,
      "step": 806,
      "training_loss": 7.332772254943848
    },
    {
      "epoch": 0.04373983739837398,
      "step": 807,
      "training_loss": 7.115194320678711
    },
    {
      "epoch": 0.04379403794037941,
      "grad_norm": 19.423254013061523,
      "learning_rate": 1e-05,
      "loss": 7.1256,
      "step": 808
    },
    {
      "epoch": 0.04379403794037941,
      "step": 808,
      "training_loss": 7.355279922485352
    },
    {
      "epoch": 0.043848238482384824,
      "step": 809,
      "training_loss": 7.15977144241333
    },
    {
      "epoch": 0.04390243902439024,
      "step": 810,
      "training_loss": 7.304504871368408
    },
    {
      "epoch": 0.043956639566395665,
      "step": 811,
      "training_loss": 8.165520668029785
    },
    {
      "epoch": 0.04401084010840108,
      "grad_norm": 15.658426284790039,
      "learning_rate": 1e-05,
      "loss": 7.4963,
      "step": 812
    },
    {
      "epoch": 0.04401084010840108,
      "step": 812,
      "training_loss": 7.243791103363037
    },
    {
      "epoch": 0.044065040650406506,
      "step": 813,
      "training_loss": 7.818070411682129
    },
    {
      "epoch": 0.04411924119241192,
      "step": 814,
      "training_loss": 7.665558338165283
    },
    {
      "epoch": 0.04417344173441735,
      "step": 815,
      "training_loss": 8.180334091186523
    },
    {
      "epoch": 0.044227642276422764,
      "grad_norm": 27.220943450927734,
      "learning_rate": 1e-05,
      "loss": 7.7269,
      "step": 816
    },
    {
      "epoch": 0.044227642276422764,
      "step": 816,
      "training_loss": 6.488495349884033
    },
    {
      "epoch": 0.04428184281842818,
      "step": 817,
      "training_loss": 5.744063854217529
    },
    {
      "epoch": 0.044336043360433605,
      "step": 818,
      "training_loss": 7.816483497619629
    },
    {
      "epoch": 0.04439024390243902,
      "step": 819,
      "training_loss": 6.1043829917907715
    },
    {
      "epoch": 0.044444444444444446,
      "grad_norm": 20.663820266723633,
      "learning_rate": 1e-05,
      "loss": 6.5384,
      "step": 820
    },
    {
      "epoch": 0.044444444444444446,
      "step": 820,
      "training_loss": 7.831073760986328
    },
    {
      "epoch": 0.04449864498644986,
      "step": 821,
      "training_loss": 6.76140022277832
    },
    {
      "epoch": 0.04455284552845529,
      "step": 822,
      "training_loss": 6.868954658508301
    },
    {
      "epoch": 0.044607046070460704,
      "step": 823,
      "training_loss": 9.232460975646973
    },
    {
      "epoch": 0.04466124661246613,
      "grad_norm": 26.163970947265625,
      "learning_rate": 1e-05,
      "loss": 7.6735,
      "step": 824
    },
    {
      "epoch": 0.04466124661246613,
      "step": 824,
      "training_loss": 8.625916481018066
    },
    {
      "epoch": 0.044715447154471545,
      "step": 825,
      "training_loss": 7.054008483886719
    },
    {
      "epoch": 0.04476964769647696,
      "step": 826,
      "training_loss": 7.3619561195373535
    },
    {
      "epoch": 0.044823848238482386,
      "step": 827,
      "training_loss": 7.113012790679932
    },
    {
      "epoch": 0.0448780487804878,
      "grad_norm": 14.996211051940918,
      "learning_rate": 1e-05,
      "loss": 7.5387,
      "step": 828
    },
    {
      "epoch": 0.0448780487804878,
      "step": 828,
      "training_loss": 7.43806266784668
    },
    {
      "epoch": 0.04493224932249323,
      "step": 829,
      "training_loss": 6.783214569091797
    },
    {
      "epoch": 0.044986449864498644,
      "step": 830,
      "training_loss": 7.515467643737793
    },
    {
      "epoch": 0.04504065040650407,
      "step": 831,
      "training_loss": 7.85584831237793
    },
    {
      "epoch": 0.045094850948509485,
      "grad_norm": 30.54755973815918,
      "learning_rate": 1e-05,
      "loss": 7.3981,
      "step": 832
    },
    {
      "epoch": 0.045094850948509485,
      "step": 832,
      "training_loss": 7.160261631011963
    },
    {
      "epoch": 0.0451490514905149,
      "step": 833,
      "training_loss": 7.93576717376709
    },
    {
      "epoch": 0.045203252032520326,
      "step": 834,
      "training_loss": 7.352416515350342
    },
    {
      "epoch": 0.04525745257452574,
      "step": 835,
      "training_loss": 6.516341209411621
    },
    {
      "epoch": 0.04531165311653117,
      "grad_norm": 17.588171005249023,
      "learning_rate": 1e-05,
      "loss": 7.2412,
      "step": 836
    },
    {
      "epoch": 0.04531165311653117,
      "step": 836,
      "training_loss": 6.1782073974609375
    },
    {
      "epoch": 0.045365853658536584,
      "step": 837,
      "training_loss": 7.5690598487854
    },
    {
      "epoch": 0.04542005420054201,
      "step": 838,
      "training_loss": 5.679376125335693
    },
    {
      "epoch": 0.045474254742547425,
      "step": 839,
      "training_loss": 8.154483795166016
    },
    {
      "epoch": 0.04552845528455285,
      "grad_norm": 19.348909378051758,
      "learning_rate": 1e-05,
      "loss": 6.8953,
      "step": 840
    },
    {
      "epoch": 0.04552845528455285,
      "step": 840,
      "training_loss": 6.571814060211182
    },
    {
      "epoch": 0.045582655826558266,
      "step": 841,
      "training_loss": 7.2438530921936035
    },
    {
      "epoch": 0.04563685636856368,
      "step": 842,
      "training_loss": 8.407193183898926
    },
    {
      "epoch": 0.04569105691056911,
      "step": 843,
      "training_loss": 7.19254207611084
    },
    {
      "epoch": 0.045745257452574524,
      "grad_norm": 13.141278266906738,
      "learning_rate": 1e-05,
      "loss": 7.3539,
      "step": 844
    },
    {
      "epoch": 0.045745257452574524,
      "step": 844,
      "training_loss": 7.549839019775391
    },
    {
      "epoch": 0.04579945799457995,
      "step": 845,
      "training_loss": 7.3195881843566895
    },
    {
      "epoch": 0.045853658536585365,
      "step": 846,
      "training_loss": 6.036137104034424
    },
    {
      "epoch": 0.04590785907859079,
      "step": 847,
      "training_loss": 7.111788272857666
    },
    {
      "epoch": 0.045962059620596206,
      "grad_norm": 16.057788848876953,
      "learning_rate": 1e-05,
      "loss": 7.0043,
      "step": 848
    },
    {
      "epoch": 0.045962059620596206,
      "step": 848,
      "training_loss": 6.979340553283691
    },
    {
      "epoch": 0.04601626016260162,
      "step": 849,
      "training_loss": 7.636937618255615
    },
    {
      "epoch": 0.04607046070460705,
      "step": 850,
      "training_loss": 7.127559185028076
    },
    {
      "epoch": 0.046124661246612464,
      "step": 851,
      "training_loss": 6.829360008239746
    },
    {
      "epoch": 0.04617886178861789,
      "grad_norm": 20.51664924621582,
      "learning_rate": 1e-05,
      "loss": 7.1433,
      "step": 852
    },
    {
      "epoch": 0.04617886178861789,
      "step": 852,
      "training_loss": 6.6409101486206055
    },
    {
      "epoch": 0.046233062330623305,
      "step": 853,
      "training_loss": 6.708644866943359
    },
    {
      "epoch": 0.04628726287262873,
      "step": 854,
      "training_loss": 7.283260822296143
    },
    {
      "epoch": 0.046341463414634146,
      "step": 855,
      "training_loss": 7.675302982330322
    },
    {
      "epoch": 0.04639566395663957,
      "grad_norm": 25.710466384887695,
      "learning_rate": 1e-05,
      "loss": 7.077,
      "step": 856
    },
    {
      "epoch": 0.04639566395663957,
      "step": 856,
      "training_loss": 6.524420738220215
    },
    {
      "epoch": 0.04644986449864499,
      "step": 857,
      "training_loss": 6.3211822509765625
    },
    {
      "epoch": 0.046504065040650404,
      "step": 858,
      "training_loss": 8.711405754089355
    },
    {
      "epoch": 0.04655826558265583,
      "step": 859,
      "training_loss": 7.500962257385254
    },
    {
      "epoch": 0.046612466124661245,
      "grad_norm": 16.647005081176758,
      "learning_rate": 1e-05,
      "loss": 7.2645,
      "step": 860
    },
    {
      "epoch": 0.046612466124661245,
      "step": 860,
      "training_loss": 7.2642974853515625
    },
    {
      "epoch": 0.04666666666666667,
      "step": 861,
      "training_loss": 8.148364067077637
    },
    {
      "epoch": 0.046720867208672086,
      "step": 862,
      "training_loss": 7.357884407043457
    },
    {
      "epoch": 0.04677506775067751,
      "step": 863,
      "training_loss": 6.77963924407959
    },
    {
      "epoch": 0.04682926829268293,
      "grad_norm": 18.547786712646484,
      "learning_rate": 1e-05,
      "loss": 7.3875,
      "step": 864
    },
    {
      "epoch": 0.04682926829268293,
      "step": 864,
      "training_loss": 7.273497104644775
    },
    {
      "epoch": 0.046883468834688344,
      "step": 865,
      "training_loss": 7.8357439041137695
    },
    {
      "epoch": 0.04693766937669377,
      "step": 866,
      "training_loss": 7.008145809173584
    },
    {
      "epoch": 0.046991869918699185,
      "step": 867,
      "training_loss": 7.876757621765137
    },
    {
      "epoch": 0.04704607046070461,
      "grad_norm": 29.133487701416016,
      "learning_rate": 1e-05,
      "loss": 7.4985,
      "step": 868
    },
    {
      "epoch": 0.04704607046070461,
      "step": 868,
      "training_loss": 7.528341770172119
    },
    {
      "epoch": 0.047100271002710026,
      "step": 869,
      "training_loss": 8.349176406860352
    },
    {
      "epoch": 0.04715447154471545,
      "step": 870,
      "training_loss": 8.014127731323242
    },
    {
      "epoch": 0.04720867208672087,
      "step": 871,
      "training_loss": 6.581873416900635
    },
    {
      "epoch": 0.04726287262872629,
      "grad_norm": 13.748699188232422,
      "learning_rate": 1e-05,
      "loss": 7.6184,
      "step": 872
    },
    {
      "epoch": 0.04726287262872629,
      "step": 872,
      "training_loss": 6.829333305358887
    },
    {
      "epoch": 0.04731707317073171,
      "step": 873,
      "training_loss": 7.062126159667969
    },
    {
      "epoch": 0.047371273712737125,
      "step": 874,
      "training_loss": 7.849270820617676
    },
    {
      "epoch": 0.04742547425474255,
      "step": 875,
      "training_loss": 7.982462406158447
    },
    {
      "epoch": 0.047479674796747966,
      "grad_norm": 13.907047271728516,
      "learning_rate": 1e-05,
      "loss": 7.4308,
      "step": 876
    },
    {
      "epoch": 0.047479674796747966,
      "step": 876,
      "training_loss": 9.103348731994629
    },
    {
      "epoch": 0.04753387533875339,
      "step": 877,
      "training_loss": 7.604116916656494
    },
    {
      "epoch": 0.04758807588075881,
      "step": 878,
      "training_loss": 7.614312171936035
    },
    {
      "epoch": 0.04764227642276423,
      "step": 879,
      "training_loss": 5.669027328491211
    },
    {
      "epoch": 0.04769647696476965,
      "grad_norm": 14.9942626953125,
      "learning_rate": 1e-05,
      "loss": 7.4977,
      "step": 880
    },
    {
      "epoch": 0.04769647696476965,
      "step": 880,
      "training_loss": 6.862196922302246
    },
    {
      "epoch": 0.047750677506775065,
      "step": 881,
      "training_loss": 6.237283706665039
    },
    {
      "epoch": 0.04780487804878049,
      "step": 882,
      "training_loss": 8.14236831665039
    },
    {
      "epoch": 0.047859078590785906,
      "step": 883,
      "training_loss": 7.211472511291504
    },
    {
      "epoch": 0.04791327913279133,
      "grad_norm": 22.17084503173828,
      "learning_rate": 1e-05,
      "loss": 7.1133,
      "step": 884
    },
    {
      "epoch": 0.04791327913279133,
      "step": 884,
      "training_loss": 7.483644008636475
    },
    {
      "epoch": 0.04796747967479675,
      "step": 885,
      "training_loss": 9.229742050170898
    },
    {
      "epoch": 0.04802168021680217,
      "step": 886,
      "training_loss": 6.443379878997803
    },
    {
      "epoch": 0.04807588075880759,
      "step": 887,
      "training_loss": 5.85428524017334
    },
    {
      "epoch": 0.04813008130081301,
      "grad_norm": 18.950946807861328,
      "learning_rate": 1e-05,
      "loss": 7.2528,
      "step": 888
    },
    {
      "epoch": 0.04813008130081301,
      "step": 888,
      "training_loss": 6.936169147491455
    },
    {
      "epoch": 0.04818428184281843,
      "step": 889,
      "training_loss": 6.417416095733643
    },
    {
      "epoch": 0.048238482384823846,
      "step": 890,
      "training_loss": 8.328697204589844
    },
    {
      "epoch": 0.04829268292682927,
      "step": 891,
      "training_loss": 6.93245792388916
    },
    {
      "epoch": 0.04834688346883469,
      "grad_norm": 15.778785705566406,
      "learning_rate": 1e-05,
      "loss": 7.1537,
      "step": 892
    },
    {
      "epoch": 0.04834688346883469,
      "step": 892,
      "training_loss": 7.177065372467041
    },
    {
      "epoch": 0.04840108401084011,
      "step": 893,
      "training_loss": 6.413909435272217
    },
    {
      "epoch": 0.04845528455284553,
      "step": 894,
      "training_loss": 7.5944600105285645
    },
    {
      "epoch": 0.04850948509485095,
      "step": 895,
      "training_loss": 7.347728252410889
    },
    {
      "epoch": 0.04856368563685637,
      "grad_norm": 17.56531524658203,
      "learning_rate": 1e-05,
      "loss": 7.1333,
      "step": 896
    },
    {
      "epoch": 0.04856368563685637,
      "step": 896,
      "training_loss": 7.512213706970215
    },
    {
      "epoch": 0.048617886178861786,
      "step": 897,
      "training_loss": 6.147216796875
    },
    {
      "epoch": 0.04867208672086721,
      "step": 898,
      "training_loss": 7.0170135498046875
    },
    {
      "epoch": 0.048726287262872627,
      "step": 899,
      "training_loss": 7.627912521362305
    },
    {
      "epoch": 0.04878048780487805,
      "grad_norm": 15.260225296020508,
      "learning_rate": 1e-05,
      "loss": 7.0761,
      "step": 900
    },
    {
      "epoch": 0.04878048780487805,
      "step": 900,
      "training_loss": 7.364727973937988
    },
    {
      "epoch": 0.04883468834688347,
      "step": 901,
      "training_loss": 7.467303276062012
    },
    {
      "epoch": 0.04888888888888889,
      "step": 902,
      "training_loss": 5.502925395965576
    },
    {
      "epoch": 0.04894308943089431,
      "step": 903,
      "training_loss": 6.91231107711792
    },
    {
      "epoch": 0.04899728997289973,
      "grad_norm": 19.97547721862793,
      "learning_rate": 1e-05,
      "loss": 6.8118,
      "step": 904
    },
    {
      "epoch": 0.04899728997289973,
      "step": 904,
      "training_loss": 6.865436553955078
    },
    {
      "epoch": 0.04905149051490515,
      "step": 905,
      "training_loss": 7.5608296394348145
    },
    {
      "epoch": 0.049105691056910566,
      "step": 906,
      "training_loss": 6.88442325592041
    },
    {
      "epoch": 0.04915989159891599,
      "step": 907,
      "training_loss": 7.847963333129883
    },
    {
      "epoch": 0.04921409214092141,
      "grad_norm": 18.614635467529297,
      "learning_rate": 1e-05,
      "loss": 7.2897,
      "step": 908
    },
    {
      "epoch": 0.04921409214092141,
      "step": 908,
      "training_loss": 7.936404705047607
    },
    {
      "epoch": 0.04926829268292683,
      "step": 909,
      "training_loss": 5.243791580200195
    },
    {
      "epoch": 0.04932249322493225,
      "step": 910,
      "training_loss": 5.497528076171875
    },
    {
      "epoch": 0.04937669376693767,
      "step": 911,
      "training_loss": 5.459578990936279
    },
    {
      "epoch": 0.04943089430894309,
      "grad_norm": 12.529241561889648,
      "learning_rate": 1e-05,
      "loss": 6.0343,
      "step": 912
    },
    {
      "epoch": 0.04943089430894309,
      "step": 912,
      "training_loss": 6.455092430114746
    },
    {
      "epoch": 0.049485094850948506,
      "step": 913,
      "training_loss": 7.590631008148193
    },
    {
      "epoch": 0.04953929539295393,
      "step": 914,
      "training_loss": 6.278628826141357
    },
    {
      "epoch": 0.04959349593495935,
      "step": 915,
      "training_loss": 7.92773962020874
    },
    {
      "epoch": 0.04964769647696477,
      "grad_norm": 17.33404541015625,
      "learning_rate": 1e-05,
      "loss": 7.063,
      "step": 916
    },
    {
      "epoch": 0.04964769647696477,
      "step": 916,
      "training_loss": 7.086029529571533
    },
    {
      "epoch": 0.04970189701897019,
      "step": 917,
      "training_loss": 7.635897636413574
    },
    {
      "epoch": 0.04975609756097561,
      "step": 918,
      "training_loss": 7.731043815612793
    },
    {
      "epoch": 0.04981029810298103,
      "step": 919,
      "training_loss": 7.639596939086914
    },
    {
      "epoch": 0.04986449864498645,
      "grad_norm": 18.917165756225586,
      "learning_rate": 1e-05,
      "loss": 7.5231,
      "step": 920
    },
    {
      "epoch": 0.04986449864498645,
      "step": 920,
      "training_loss": 6.5616278648376465
    },
    {
      "epoch": 0.04991869918699187,
      "step": 921,
      "training_loss": 6.927579402923584
    },
    {
      "epoch": 0.04997289972899729,
      "step": 922,
      "training_loss": 9.750565528869629
    },
    {
      "epoch": 0.05002710027100271,
      "step": 923,
      "training_loss": 8.237527847290039
    },
    {
      "epoch": 0.05008130081300813,
      "grad_norm": 25.651338577270508,
      "learning_rate": 1e-05,
      "loss": 7.8693,
      "step": 924
    },
    {
      "epoch": 0.05008130081300813,
      "step": 924,
      "training_loss": 8.394700050354004
    },
    {
      "epoch": 0.05013550135501355,
      "step": 925,
      "training_loss": 6.714446544647217
    },
    {
      "epoch": 0.05018970189701897,
      "step": 926,
      "training_loss": 7.629763126373291
    },
    {
      "epoch": 0.05024390243902439,
      "step": 927,
      "training_loss": 7.59296178817749
    },
    {
      "epoch": 0.05029810298102981,
      "grad_norm": 25.557334899902344,
      "learning_rate": 1e-05,
      "loss": 7.583,
      "step": 928
    },
    {
      "epoch": 0.05029810298102981,
      "step": 928,
      "training_loss": 6.919543266296387
    },
    {
      "epoch": 0.05035230352303523,
      "step": 929,
      "training_loss": 6.082066535949707
    },
    {
      "epoch": 0.05040650406504065,
      "step": 930,
      "training_loss": 5.740822792053223
    },
    {
      "epoch": 0.05046070460704607,
      "step": 931,
      "training_loss": 5.476573944091797
    },
    {
      "epoch": 0.05051490514905149,
      "grad_norm": 22.110902786254883,
      "learning_rate": 1e-05,
      "loss": 6.0548,
      "step": 932
    },
    {
      "epoch": 0.05051490514905149,
      "step": 932,
      "training_loss": 7.100552082061768
    },
    {
      "epoch": 0.05056910569105691,
      "step": 933,
      "training_loss": 7.1756815910339355
    },
    {
      "epoch": 0.05062330623306233,
      "step": 934,
      "training_loss": 6.261508941650391
    },
    {
      "epoch": 0.05067750677506775,
      "step": 935,
      "training_loss": 7.468195915222168
    },
    {
      "epoch": 0.050731707317073174,
      "grad_norm": 24.568561553955078,
      "learning_rate": 1e-05,
      "loss": 7.0015,
      "step": 936
    },
    {
      "epoch": 0.050731707317073174,
      "step": 936,
      "training_loss": 7.972352504730225
    },
    {
      "epoch": 0.05078590785907859,
      "step": 937,
      "training_loss": 5.584224700927734
    },
    {
      "epoch": 0.05084010840108401,
      "step": 938,
      "training_loss": 7.157180309295654
    },
    {
      "epoch": 0.05089430894308943,
      "step": 939,
      "training_loss": 7.206649303436279
    },
    {
      "epoch": 0.05094850948509485,
      "grad_norm": 16.646568298339844,
      "learning_rate": 1e-05,
      "loss": 6.9801,
      "step": 940
    },
    {
      "epoch": 0.05094850948509485,
      "step": 940,
      "training_loss": 7.693033695220947
    },
    {
      "epoch": 0.05100271002710027,
      "step": 941,
      "training_loss": 6.436394691467285
    },
    {
      "epoch": 0.05105691056910569,
      "step": 942,
      "training_loss": 7.506513595581055
    },
    {
      "epoch": 0.051111111111111114,
      "step": 943,
      "training_loss": 7.7202067375183105
    },
    {
      "epoch": 0.05116531165311653,
      "grad_norm": 24.43022346496582,
      "learning_rate": 1e-05,
      "loss": 7.339,
      "step": 944
    },
    {
      "epoch": 0.05116531165311653,
      "step": 944,
      "training_loss": 6.399364948272705
    },
    {
      "epoch": 0.05121951219512195,
      "step": 945,
      "training_loss": 7.6773271560668945
    },
    {
      "epoch": 0.05127371273712737,
      "step": 946,
      "training_loss": 8.39827823638916
    },
    {
      "epoch": 0.05132791327913279,
      "step": 947,
      "training_loss": 7.55624532699585
    },
    {
      "epoch": 0.05138211382113821,
      "grad_norm": 15.871807098388672,
      "learning_rate": 1e-05,
      "loss": 7.5078,
      "step": 948
    },
    {
      "epoch": 0.05138211382113821,
      "step": 948,
      "training_loss": 6.345094203948975
    },
    {
      "epoch": 0.05143631436314363,
      "step": 949,
      "training_loss": 7.883281707763672
    },
    {
      "epoch": 0.051490514905149054,
      "step": 950,
      "training_loss": 7.269168376922607
    },
    {
      "epoch": 0.05154471544715447,
      "step": 951,
      "training_loss": 6.733432292938232
    },
    {
      "epoch": 0.051598915989159895,
      "grad_norm": 22.68878746032715,
      "learning_rate": 1e-05,
      "loss": 7.0577,
      "step": 952
    },
    {
      "epoch": 0.051598915989159895,
      "step": 952,
      "training_loss": 5.5579047203063965
    },
    {
      "epoch": 0.05165311653116531,
      "step": 953,
      "training_loss": 7.477138042449951
    },
    {
      "epoch": 0.05170731707317073,
      "step": 954,
      "training_loss": 7.50747013092041
    },
    {
      "epoch": 0.05176151761517615,
      "step": 955,
      "training_loss": 9.118658065795898
    },
    {
      "epoch": 0.05181571815718157,
      "grad_norm": 31.263294219970703,
      "learning_rate": 1e-05,
      "loss": 7.4153,
      "step": 956
    },
    {
      "epoch": 0.05181571815718157,
      "step": 956,
      "training_loss": 6.553242206573486
    },
    {
      "epoch": 0.051869918699186994,
      "step": 957,
      "training_loss": 7.7325310707092285
    },
    {
      "epoch": 0.05192411924119241,
      "step": 958,
      "training_loss": 7.319624423980713
    },
    {
      "epoch": 0.051978319783197835,
      "step": 959,
      "training_loss": 6.589485168457031
    },
    {
      "epoch": 0.05203252032520325,
      "grad_norm": 16.19386863708496,
      "learning_rate": 1e-05,
      "loss": 7.0487,
      "step": 960
    },
    {
      "epoch": 0.05203252032520325,
      "step": 960,
      "training_loss": 5.326778411865234
    },
    {
      "epoch": 0.05208672086720867,
      "step": 961,
      "training_loss": 7.784420490264893
    },
    {
      "epoch": 0.05214092140921409,
      "step": 962,
      "training_loss": 6.473048686981201
    },
    {
      "epoch": 0.05219512195121951,
      "step": 963,
      "training_loss": 7.905091762542725
    },
    {
      "epoch": 0.052249322493224934,
      "grad_norm": 16.46824073791504,
      "learning_rate": 1e-05,
      "loss": 6.8723,
      "step": 964
    },
    {
      "epoch": 0.052249322493224934,
      "step": 964,
      "training_loss": 6.160741806030273
    },
    {
      "epoch": 0.05230352303523035,
      "step": 965,
      "training_loss": 6.883854389190674
    },
    {
      "epoch": 0.052357723577235775,
      "step": 966,
      "training_loss": 6.336925983428955
    },
    {
      "epoch": 0.05241192411924119,
      "step": 967,
      "training_loss": 5.87310791015625
    },
    {
      "epoch": 0.052466124661246616,
      "grad_norm": 29.191425323486328,
      "learning_rate": 1e-05,
      "loss": 6.3137,
      "step": 968
    },
    {
      "epoch": 0.052466124661246616,
      "step": 968,
      "training_loss": 7.205433368682861
    },
    {
      "epoch": 0.05252032520325203,
      "step": 969,
      "training_loss": 7.132815837860107
    },
    {
      "epoch": 0.05257452574525745,
      "step": 970,
      "training_loss": 7.370950222015381
    },
    {
      "epoch": 0.052628726287262874,
      "step": 971,
      "training_loss": 7.025955677032471
    },
    {
      "epoch": 0.05268292682926829,
      "grad_norm": 12.859195709228516,
      "learning_rate": 1e-05,
      "loss": 7.1838,
      "step": 972
    },
    {
      "epoch": 0.05268292682926829,
      "step": 972,
      "training_loss": 7.627626895904541
    },
    {
      "epoch": 0.052737127371273715,
      "step": 973,
      "training_loss": 7.323227882385254
    },
    {
      "epoch": 0.05279132791327913,
      "step": 974,
      "training_loss": 7.576320171356201
    },
    {
      "epoch": 0.052845528455284556,
      "step": 975,
      "training_loss": 5.841156482696533
    },
    {
      "epoch": 0.05289972899728997,
      "grad_norm": 22.999853134155273,
      "learning_rate": 1e-05,
      "loss": 7.0921,
      "step": 976
    },
    {
      "epoch": 0.05289972899728997,
      "step": 976,
      "training_loss": 7.416243076324463
    },
    {
      "epoch": 0.05295392953929539,
      "step": 977,
      "training_loss": 7.807962417602539
    },
    {
      "epoch": 0.053008130081300814,
      "step": 978,
      "training_loss": 8.554228782653809
    },
    {
      "epoch": 0.05306233062330623,
      "step": 979,
      "training_loss": 7.037246227264404
    },
    {
      "epoch": 0.053116531165311655,
      "grad_norm": 20.322328567504883,
      "learning_rate": 1e-05,
      "loss": 7.7039,
      "step": 980
    },
    {
      "epoch": 0.053116531165311655,
      "step": 980,
      "training_loss": 6.495983600616455
    },
    {
      "epoch": 0.05317073170731707,
      "step": 981,
      "training_loss": 8.01674747467041
    },
    {
      "epoch": 0.053224932249322496,
      "step": 982,
      "training_loss": 6.966456413269043
    },
    {
      "epoch": 0.05327913279132791,
      "step": 983,
      "training_loss": 7.385901927947998
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 11.912016868591309,
      "learning_rate": 1e-05,
      "loss": 7.2163,
      "step": 984
    },
    {
      "epoch": 0.05333333333333334,
      "step": 984,
      "training_loss": 7.523409366607666
    },
    {
      "epoch": 0.053387533875338754,
      "step": 985,
      "training_loss": 8.920205116271973
    },
    {
      "epoch": 0.05344173441734417,
      "step": 986,
      "training_loss": 8.082175254821777
    },
    {
      "epoch": 0.053495934959349595,
      "step": 987,
      "training_loss": 8.559883117675781
    },
    {
      "epoch": 0.05355013550135501,
      "grad_norm": 26.502840042114258,
      "learning_rate": 1e-05,
      "loss": 8.2714,
      "step": 988
    },
    {
      "epoch": 0.05355013550135501,
      "step": 988,
      "training_loss": 6.126516819000244
    },
    {
      "epoch": 0.053604336043360436,
      "step": 989,
      "training_loss": 6.245349884033203
    },
    {
      "epoch": 0.05365853658536585,
      "step": 990,
      "training_loss": 8.391536712646484
    },
    {
      "epoch": 0.05371273712737128,
      "step": 991,
      "training_loss": 7.194152355194092
    },
    {
      "epoch": 0.053766937669376694,
      "grad_norm": 21.345285415649414,
      "learning_rate": 1e-05,
      "loss": 6.9894,
      "step": 992
    },
    {
      "epoch": 0.053766937669376694,
      "step": 992,
      "training_loss": 7.435711860656738
    },
    {
      "epoch": 0.05382113821138211,
      "step": 993,
      "training_loss": 6.294216632843018
    },
    {
      "epoch": 0.053875338753387535,
      "step": 994,
      "training_loss": 7.401108264923096
    },
    {
      "epoch": 0.05392953929539295,
      "step": 995,
      "training_loss": 6.909874439239502
    },
    {
      "epoch": 0.053983739837398376,
      "grad_norm": 12.897551536560059,
      "learning_rate": 1e-05,
      "loss": 7.0102,
      "step": 996
    },
    {
      "epoch": 0.053983739837398376,
      "step": 996,
      "training_loss": 6.611092567443848
    },
    {
      "epoch": 0.05403794037940379,
      "step": 997,
      "training_loss": 7.404722213745117
    },
    {
      "epoch": 0.05409214092140922,
      "step": 998,
      "training_loss": 7.886529922485352
    },
    {
      "epoch": 0.054146341463414634,
      "step": 999,
      "training_loss": 7.304025650024414
    },
    {
      "epoch": 0.05420054200542006,
      "grad_norm": 18.598155975341797,
      "learning_rate": 1e-05,
      "loss": 7.3016,
      "step": 1000
    },
    {
      "epoch": 0.05420054200542006,
      "step": 1000,
      "training_loss": 5.423346996307373
    },
    {
      "epoch": 0.054254742547425475,
      "step": 1001,
      "training_loss": 8.353638648986816
    },
    {
      "epoch": 0.05430894308943089,
      "step": 1002,
      "training_loss": 5.708527565002441
    },
    {
      "epoch": 0.054363143631436316,
      "step": 1003,
      "training_loss": 7.219461441040039
    },
    {
      "epoch": 0.05441734417344173,
      "grad_norm": 14.725149154663086,
      "learning_rate": 1e-05,
      "loss": 6.6762,
      "step": 1004
    },
    {
      "epoch": 0.05441734417344173,
      "step": 1004,
      "training_loss": 7.737529754638672
    },
    {
      "epoch": 0.05447154471544716,
      "step": 1005,
      "training_loss": 7.195572853088379
    },
    {
      "epoch": 0.054525745257452574,
      "step": 1006,
      "training_loss": 7.598361968994141
    },
    {
      "epoch": 0.054579945799458,
      "step": 1007,
      "training_loss": 6.809720516204834
    },
    {
      "epoch": 0.054634146341463415,
      "grad_norm": 14.658546447753906,
      "learning_rate": 1e-05,
      "loss": 7.3353,
      "step": 1008
    },
    {
      "epoch": 0.054634146341463415,
      "step": 1008,
      "training_loss": 8.26030445098877
    },
    {
      "epoch": 0.05468834688346883,
      "step": 1009,
      "training_loss": 8.589308738708496
    },
    {
      "epoch": 0.054742547425474256,
      "step": 1010,
      "training_loss": 7.513148784637451
    },
    {
      "epoch": 0.05479674796747967,
      "step": 1011,
      "training_loss": 8.2163667678833
    },
    {
      "epoch": 0.0548509485094851,
      "grad_norm": 23.300668716430664,
      "learning_rate": 1e-05,
      "loss": 8.1448,
      "step": 1012
    },
    {
      "epoch": 0.0548509485094851,
      "step": 1012,
      "training_loss": 7.554819583892822
    },
    {
      "epoch": 0.054905149051490514,
      "step": 1013,
      "training_loss": 8.300232887268066
    },
    {
      "epoch": 0.05495934959349594,
      "step": 1014,
      "training_loss": 7.958643913269043
    },
    {
      "epoch": 0.055013550135501355,
      "step": 1015,
      "training_loss": 6.952342510223389
    },
    {
      "epoch": 0.05506775067750678,
      "grad_norm": 10.139036178588867,
      "learning_rate": 1e-05,
      "loss": 7.6915,
      "step": 1016
    },
    {
      "epoch": 0.05506775067750678,
      "step": 1016,
      "training_loss": 7.0158371925354
    },
    {
      "epoch": 0.055121951219512196,
      "step": 1017,
      "training_loss": 6.726415157318115
    },
    {
      "epoch": 0.05517615176151761,
      "step": 1018,
      "training_loss": 6.993637561798096
    },
    {
      "epoch": 0.055230352303523036,
      "step": 1019,
      "training_loss": 7.459524631500244
    },
    {
      "epoch": 0.055284552845528454,
      "grad_norm": 16.256820678710938,
      "learning_rate": 1e-05,
      "loss": 7.0489,
      "step": 1020
    },
    {
      "epoch": 0.055284552845528454,
      "step": 1020,
      "training_loss": 6.971411228179932
    },
    {
      "epoch": 0.05533875338753388,
      "step": 1021,
      "training_loss": 8.128894805908203
    },
    {
      "epoch": 0.055392953929539294,
      "step": 1022,
      "training_loss": 7.179687976837158
    },
    {
      "epoch": 0.05544715447154472,
      "step": 1023,
      "training_loss": 7.043475151062012
    },
    {
      "epoch": 0.055501355013550135,
      "grad_norm": 14.338786125183105,
      "learning_rate": 1e-05,
      "loss": 7.3309,
      "step": 1024
    },
    {
      "epoch": 0.055501355013550135,
      "step": 1024,
      "training_loss": 5.971951961517334
    },
    {
      "epoch": 0.05555555555555555,
      "step": 1025,
      "training_loss": 6.324031352996826
    },
    {
      "epoch": 0.055609756097560976,
      "step": 1026,
      "training_loss": 8.277191162109375
    },
    {
      "epoch": 0.05566395663956639,
      "step": 1027,
      "training_loss": 7.003145694732666
    },
    {
      "epoch": 0.05571815718157182,
      "grad_norm": 15.781475067138672,
      "learning_rate": 1e-05,
      "loss": 6.8941,
      "step": 1028
    },
    {
      "epoch": 0.05571815718157182,
      "step": 1028,
      "training_loss": 8.174275398254395
    },
    {
      "epoch": 0.055772357723577234,
      "step": 1029,
      "training_loss": 7.52393102645874
    },
    {
      "epoch": 0.05582655826558266,
      "step": 1030,
      "training_loss": 6.024376392364502
    },
    {
      "epoch": 0.055880758807588075,
      "step": 1031,
      "training_loss": 6.678736209869385
    },
    {
      "epoch": 0.0559349593495935,
      "grad_norm": 22.665674209594727,
      "learning_rate": 1e-05,
      "loss": 7.1003,
      "step": 1032
    },
    {
      "epoch": 0.0559349593495935,
      "step": 1032,
      "training_loss": 7.96961784362793
    },
    {
      "epoch": 0.055989159891598916,
      "step": 1033,
      "training_loss": 8.026301383972168
    },
    {
      "epoch": 0.05604336043360433,
      "step": 1034,
      "training_loss": 7.402660369873047
    },
    {
      "epoch": 0.05609756097560976,
      "step": 1035,
      "training_loss": 6.123022556304932
    },
    {
      "epoch": 0.056151761517615174,
      "grad_norm": 25.17579460144043,
      "learning_rate": 1e-05,
      "loss": 7.3804,
      "step": 1036
    },
    {
      "epoch": 0.056151761517615174,
      "step": 1036,
      "training_loss": 7.847700595855713
    },
    {
      "epoch": 0.0562059620596206,
      "step": 1037,
      "training_loss": 7.650993824005127
    },
    {
      "epoch": 0.056260162601626015,
      "step": 1038,
      "training_loss": 7.159902572631836
    },
    {
      "epoch": 0.05631436314363144,
      "step": 1039,
      "training_loss": 6.795316696166992
    },
    {
      "epoch": 0.056368563685636856,
      "grad_norm": 19.09967613220215,
      "learning_rate": 1e-05,
      "loss": 7.3635,
      "step": 1040
    },
    {
      "epoch": 0.056368563685636856,
      "step": 1040,
      "training_loss": 7.257760047912598
    },
    {
      "epoch": 0.05642276422764227,
      "step": 1041,
      "training_loss": 6.9885077476501465
    },
    {
      "epoch": 0.0564769647696477,
      "step": 1042,
      "training_loss": 7.7369256019592285
    },
    {
      "epoch": 0.056531165311653114,
      "step": 1043,
      "training_loss": 8.525167465209961
    },
    {
      "epoch": 0.05658536585365854,
      "grad_norm": 15.99631118774414,
      "learning_rate": 1e-05,
      "loss": 7.6271,
      "step": 1044
    },
    {
      "epoch": 0.05658536585365854,
      "step": 1044,
      "training_loss": 5.7938737869262695
    },
    {
      "epoch": 0.056639566395663955,
      "step": 1045,
      "training_loss": 7.175351619720459
    },
    {
      "epoch": 0.05669376693766938,
      "step": 1046,
      "training_loss": 6.569903373718262
    },
    {
      "epoch": 0.056747967479674796,
      "step": 1047,
      "training_loss": 5.586272716522217
    },
    {
      "epoch": 0.05680216802168022,
      "grad_norm": 19.07814598083496,
      "learning_rate": 1e-05,
      "loss": 6.2814,
      "step": 1048
    },
    {
      "epoch": 0.05680216802168022,
      "step": 1048,
      "training_loss": 7.7877068519592285
    },
    {
      "epoch": 0.05685636856368564,
      "step": 1049,
      "training_loss": 7.5955400466918945
    },
    {
      "epoch": 0.056910569105691054,
      "step": 1050,
      "training_loss": 6.238462924957275
    },
    {
      "epoch": 0.05696476964769648,
      "step": 1051,
      "training_loss": 6.607945442199707
    },
    {
      "epoch": 0.057018970189701895,
      "grad_norm": 23.0045108795166,
      "learning_rate": 1e-05,
      "loss": 7.0574,
      "step": 1052
    },
    {
      "epoch": 0.057018970189701895,
      "step": 1052,
      "training_loss": 7.107295989990234
    },
    {
      "epoch": 0.05707317073170732,
      "step": 1053,
      "training_loss": 7.168727397918701
    },
    {
      "epoch": 0.057127371273712736,
      "step": 1054,
      "training_loss": 7.283388137817383
    },
    {
      "epoch": 0.05718157181571816,
      "step": 1055,
      "training_loss": 7.24202823638916
    },
    {
      "epoch": 0.05723577235772358,
      "grad_norm": 16.806169509887695,
      "learning_rate": 1e-05,
      "loss": 7.2004,
      "step": 1056
    },
    {
      "epoch": 0.05723577235772358,
      "step": 1056,
      "training_loss": 7.69818639755249
    },
    {
      "epoch": 0.057289972899728994,
      "step": 1057,
      "training_loss": 7.180434703826904
    },
    {
      "epoch": 0.05734417344173442,
      "step": 1058,
      "training_loss": 7.1734466552734375
    },
    {
      "epoch": 0.057398373983739835,
      "step": 1059,
      "training_loss": 7.024773597717285
    },
    {
      "epoch": 0.05745257452574526,
      "grad_norm": 13.300437927246094,
      "learning_rate": 1e-05,
      "loss": 7.2692,
      "step": 1060
    },
    {
      "epoch": 0.05745257452574526,
      "step": 1060,
      "training_loss": 6.418539524078369
    },
    {
      "epoch": 0.057506775067750676,
      "step": 1061,
      "training_loss": 7.101564407348633
    },
    {
      "epoch": 0.0575609756097561,
      "step": 1062,
      "training_loss": 6.283143997192383
    },
    {
      "epoch": 0.05761517615176152,
      "step": 1063,
      "training_loss": 8.322888374328613
    },
    {
      "epoch": 0.05766937669376694,
      "grad_norm": 20.906089782714844,
      "learning_rate": 1e-05,
      "loss": 7.0315,
      "step": 1064
    },
    {
      "epoch": 0.05766937669376694,
      "step": 1064,
      "training_loss": 8.54878044128418
    },
    {
      "epoch": 0.05772357723577236,
      "step": 1065,
      "training_loss": 7.914668560028076
    },
    {
      "epoch": 0.057777777777777775,
      "step": 1066,
      "training_loss": 5.867153167724609
    },
    {
      "epoch": 0.0578319783197832,
      "step": 1067,
      "training_loss": 7.627315521240234
    },
    {
      "epoch": 0.057886178861788616,
      "grad_norm": 14.542054176330566,
      "learning_rate": 1e-05,
      "loss": 7.4895,
      "step": 1068
    },
    {
      "epoch": 0.057886178861788616,
      "step": 1068,
      "training_loss": 7.8690290451049805
    },
    {
      "epoch": 0.05794037940379404,
      "step": 1069,
      "training_loss": 9.733686447143555
    },
    {
      "epoch": 0.05799457994579946,
      "step": 1070,
      "training_loss": 7.7432122230529785
    },
    {
      "epoch": 0.05804878048780488,
      "step": 1071,
      "training_loss": 7.2561235427856445
    },
    {
      "epoch": 0.0581029810298103,
      "grad_norm": 12.425250053405762,
      "learning_rate": 1e-05,
      "loss": 8.1505,
      "step": 1072
    },
    {
      "epoch": 0.0581029810298103,
      "step": 1072,
      "training_loss": 5.981330394744873
    },
    {
      "epoch": 0.058157181571815715,
      "step": 1073,
      "training_loss": 6.773984432220459
    },
    {
      "epoch": 0.05821138211382114,
      "step": 1074,
      "training_loss": 7.977208614349365
    },
    {
      "epoch": 0.058265582655826556,
      "step": 1075,
      "training_loss": 7.5596089363098145
    },
    {
      "epoch": 0.05831978319783198,
      "grad_norm": 21.413118362426758,
      "learning_rate": 1e-05,
      "loss": 7.073,
      "step": 1076
    },
    {
      "epoch": 0.05831978319783198,
      "step": 1076,
      "training_loss": 6.7426934242248535
    },
    {
      "epoch": 0.0583739837398374,
      "step": 1077,
      "training_loss": 7.0705342292785645
    },
    {
      "epoch": 0.05842818428184282,
      "step": 1078,
      "training_loss": 6.2885260581970215
    },
    {
      "epoch": 0.05848238482384824,
      "step": 1079,
      "training_loss": 7.129429817199707
    },
    {
      "epoch": 0.05853658536585366,
      "grad_norm": 19.159372329711914,
      "learning_rate": 1e-05,
      "loss": 6.8078,
      "step": 1080
    },
    {
      "epoch": 0.05853658536585366,
      "step": 1080,
      "training_loss": 8.059964179992676
    },
    {
      "epoch": 0.05859078590785908,
      "step": 1081,
      "training_loss": 7.319461345672607
    },
    {
      "epoch": 0.058644986449864496,
      "step": 1082,
      "training_loss": 4.8918776512146
    },
    {
      "epoch": 0.05869918699186992,
      "step": 1083,
      "training_loss": 6.573831558227539
    },
    {
      "epoch": 0.05875338753387534,
      "grad_norm": 19.735681533813477,
      "learning_rate": 1e-05,
      "loss": 6.7113,
      "step": 1084
    },
    {
      "epoch": 0.05875338753387534,
      "step": 1084,
      "training_loss": 8.980514526367188
    },
    {
      "epoch": 0.05880758807588076,
      "step": 1085,
      "training_loss": 7.188004970550537
    },
    {
      "epoch": 0.05886178861788618,
      "step": 1086,
      "training_loss": 5.788382053375244
    },
    {
      "epoch": 0.0589159891598916,
      "step": 1087,
      "training_loss": 5.593386650085449
    },
    {
      "epoch": 0.05897018970189702,
      "grad_norm": 15.628474235534668,
      "learning_rate": 1e-05,
      "loss": 6.8876,
      "step": 1088
    },
    {
      "epoch": 0.05897018970189702,
      "step": 1088,
      "training_loss": 7.7224016189575195
    },
    {
      "epoch": 0.059024390243902436,
      "step": 1089,
      "training_loss": 6.692620277404785
    },
    {
      "epoch": 0.05907859078590786,
      "step": 1090,
      "training_loss": 7.205183029174805
    },
    {
      "epoch": 0.05913279132791328,
      "step": 1091,
      "training_loss": 7.5396223068237305
    },
    {
      "epoch": 0.0591869918699187,
      "grad_norm": 25.3314266204834,
      "learning_rate": 1e-05,
      "loss": 7.29,
      "step": 1092
    },
    {
      "epoch": 0.0591869918699187,
      "step": 1092,
      "training_loss": 5.666321277618408
    },
    {
      "epoch": 0.05924119241192412,
      "step": 1093,
      "training_loss": 7.4689764976501465
    },
    {
      "epoch": 0.05929539295392954,
      "step": 1094,
      "training_loss": 7.5067644119262695
    },
    {
      "epoch": 0.05934959349593496,
      "step": 1095,
      "training_loss": 6.197824478149414
    },
    {
      "epoch": 0.05940379403794038,
      "grad_norm": 15.697505950927734,
      "learning_rate": 1e-05,
      "loss": 6.71,
      "step": 1096
    },
    {
      "epoch": 0.05940379403794038,
      "step": 1096,
      "training_loss": 7.322272300720215
    },
    {
      "epoch": 0.0594579945799458,
      "step": 1097,
      "training_loss": 8.127367973327637
    },
    {
      "epoch": 0.05951219512195122,
      "step": 1098,
      "training_loss": 6.487936973571777
    },
    {
      "epoch": 0.05956639566395664,
      "step": 1099,
      "training_loss": 6.665485382080078
    },
    {
      "epoch": 0.05962059620596206,
      "grad_norm": 48.31156539916992,
      "learning_rate": 1e-05,
      "loss": 7.1508,
      "step": 1100
    },
    {
      "epoch": 0.05962059620596206,
      "step": 1100,
      "training_loss": 7.958281993865967
    },
    {
      "epoch": 0.05967479674796748,
      "step": 1101,
      "training_loss": 7.2205810546875
    },
    {
      "epoch": 0.0597289972899729,
      "step": 1102,
      "training_loss": 6.436598777770996
    },
    {
      "epoch": 0.05978319783197832,
      "step": 1103,
      "training_loss": 7.073470115661621
    },
    {
      "epoch": 0.05983739837398374,
      "grad_norm": 15.994619369506836,
      "learning_rate": 1e-05,
      "loss": 7.1722,
      "step": 1104
    },
    {
      "epoch": 0.05983739837398374,
      "step": 1104,
      "training_loss": 6.58692741394043
    },
    {
      "epoch": 0.05989159891598916,
      "step": 1105,
      "training_loss": 7.67822790145874
    },
    {
      "epoch": 0.05994579945799458,
      "step": 1106,
      "training_loss": 5.738107204437256
    },
    {
      "epoch": 0.06,
      "step": 1107,
      "training_loss": 5.804506778717041
    },
    {
      "epoch": 0.06005420054200542,
      "grad_norm": 16.356191635131836,
      "learning_rate": 1e-05,
      "loss": 6.4519,
      "step": 1108
    },
    {
      "epoch": 0.06005420054200542,
      "step": 1108,
      "training_loss": 7.362017631530762
    },
    {
      "epoch": 0.06010840108401084,
      "step": 1109,
      "training_loss": 7.348652362823486
    },
    {
      "epoch": 0.06016260162601626,
      "step": 1110,
      "training_loss": 7.012127876281738
    },
    {
      "epoch": 0.06021680216802168,
      "step": 1111,
      "training_loss": 7.323354721069336
    },
    {
      "epoch": 0.060271002710027104,
      "grad_norm": 16.182912826538086,
      "learning_rate": 1e-05,
      "loss": 7.2615,
      "step": 1112
    },
    {
      "epoch": 0.060271002710027104,
      "step": 1112,
      "training_loss": 7.774411201477051
    },
    {
      "epoch": 0.06032520325203252,
      "step": 1113,
      "training_loss": 7.275075435638428
    },
    {
      "epoch": 0.06037940379403794,
      "step": 1114,
      "training_loss": 7.010379314422607
    },
    {
      "epoch": 0.06043360433604336,
      "step": 1115,
      "training_loss": 8.298691749572754
    },
    {
      "epoch": 0.06048780487804878,
      "grad_norm": 32.006248474121094,
      "learning_rate": 1e-05,
      "loss": 7.5896,
      "step": 1116
    },
    {
      "epoch": 0.06048780487804878,
      "step": 1116,
      "training_loss": 6.9379048347473145
    },
    {
      "epoch": 0.0605420054200542,
      "step": 1117,
      "training_loss": 7.320272445678711
    },
    {
      "epoch": 0.06059620596205962,
      "step": 1118,
      "training_loss": 7.9644975662231445
    },
    {
      "epoch": 0.060650406504065044,
      "step": 1119,
      "training_loss": 7.903962135314941
    },
    {
      "epoch": 0.06070460704607046,
      "grad_norm": 27.69833755493164,
      "learning_rate": 1e-05,
      "loss": 7.5317,
      "step": 1120
    },
    {
      "epoch": 0.06070460704607046,
      "step": 1120,
      "training_loss": 7.244228839874268
    },
    {
      "epoch": 0.06075880758807588,
      "step": 1121,
      "training_loss": 9.534761428833008
    },
    {
      "epoch": 0.0608130081300813,
      "step": 1122,
      "training_loss": 6.4127912521362305
    },
    {
      "epoch": 0.06086720867208672,
      "step": 1123,
      "training_loss": 7.024827003479004
    },
    {
      "epoch": 0.06092140921409214,
      "grad_norm": 14.304356575012207,
      "learning_rate": 1e-05,
      "loss": 7.5542,
      "step": 1124
    },
    {
      "epoch": 0.06092140921409214,
      "step": 1124,
      "training_loss": 7.087276935577393
    },
    {
      "epoch": 0.06097560975609756,
      "step": 1125,
      "training_loss": 8.39791202545166
    },
    {
      "epoch": 0.061029810298102984,
      "step": 1126,
      "training_loss": 7.025268077850342
    },
    {
      "epoch": 0.0610840108401084,
      "step": 1127,
      "training_loss": 6.795405864715576
    },
    {
      "epoch": 0.061138211382113825,
      "grad_norm": 20.11495590209961,
      "learning_rate": 1e-05,
      "loss": 7.3265,
      "step": 1128
    },
    {
      "epoch": 0.061138211382113825,
      "step": 1128,
      "training_loss": 7.0560994148254395
    },
    {
      "epoch": 0.06119241192411924,
      "step": 1129,
      "training_loss": 6.819112300872803
    },
    {
      "epoch": 0.06124661246612466,
      "step": 1130,
      "training_loss": 6.259636402130127
    },
    {
      "epoch": 0.06130081300813008,
      "step": 1131,
      "training_loss": 6.642788887023926
    },
    {
      "epoch": 0.0613550135501355,
      "grad_norm": 35.22883987426758,
      "learning_rate": 1e-05,
      "loss": 6.6944,
      "step": 1132
    },
    {
      "epoch": 0.0613550135501355,
      "step": 1132,
      "training_loss": 8.563484191894531
    },
    {
      "epoch": 0.061409214092140924,
      "step": 1133,
      "training_loss": 8.603116989135742
    },
    {
      "epoch": 0.06146341463414634,
      "step": 1134,
      "training_loss": 4.8863983154296875
    },
    {
      "epoch": 0.061517615176151765,
      "step": 1135,
      "training_loss": 7.302838325500488
    },
    {
      "epoch": 0.06157181571815718,
      "grad_norm": 24.517066955566406,
      "learning_rate": 1e-05,
      "loss": 7.339,
      "step": 1136
    },
    {
      "epoch": 0.06157181571815718,
      "step": 1136,
      "training_loss": 7.574068546295166
    },
    {
      "epoch": 0.0616260162601626,
      "step": 1137,
      "training_loss": 7.282171726226807
    },
    {
      "epoch": 0.06168021680216802,
      "step": 1138,
      "training_loss": 8.10904598236084
    },
    {
      "epoch": 0.06173441734417344,
      "step": 1139,
      "training_loss": 6.1899614334106445
    },
    {
      "epoch": 0.061788617886178863,
      "grad_norm": 35.59292984008789,
      "learning_rate": 1e-05,
      "loss": 7.2888,
      "step": 1140
    },
    {
      "epoch": 0.061788617886178863,
      "step": 1140,
      "training_loss": 7.364821434020996
    },
    {
      "epoch": 0.06184281842818428,
      "step": 1141,
      "training_loss": 7.42295503616333
    },
    {
      "epoch": 0.061897018970189704,
      "step": 1142,
      "training_loss": 7.42645263671875
    },
    {
      "epoch": 0.06195121951219512,
      "step": 1143,
      "training_loss": 7.762019157409668
    },
    {
      "epoch": 0.062005420054200545,
      "grad_norm": 21.304540634155273,
      "learning_rate": 1e-05,
      "loss": 7.4941,
      "step": 1144
    },
    {
      "epoch": 0.062005420054200545,
      "step": 1144,
      "training_loss": 7.9398603439331055
    },
    {
      "epoch": 0.06205962059620596,
      "step": 1145,
      "training_loss": 6.86506986618042
    },
    {
      "epoch": 0.06211382113821138,
      "step": 1146,
      "training_loss": 6.883852005004883
    },
    {
      "epoch": 0.0621680216802168,
      "step": 1147,
      "training_loss": 9.342705726623535
    },
    {
      "epoch": 0.06222222222222222,
      "grad_norm": 25.14708137512207,
      "learning_rate": 1e-05,
      "loss": 7.7579,
      "step": 1148
    },
    {
      "epoch": 0.06222222222222222,
      "step": 1148,
      "training_loss": 6.3046875
    },
    {
      "epoch": 0.062276422764227644,
      "step": 1149,
      "training_loss": 7.595922946929932
    },
    {
      "epoch": 0.06233062330623306,
      "step": 1150,
      "training_loss": 6.738684177398682
    },
    {
      "epoch": 0.062384823848238485,
      "step": 1151,
      "training_loss": 6.993240833282471
    },
    {
      "epoch": 0.0624390243902439,
      "grad_norm": 17.268102645874023,
      "learning_rate": 1e-05,
      "loss": 6.9081,
      "step": 1152
    },
    {
      "epoch": 0.0624390243902439,
      "step": 1152,
      "training_loss": 7.0030012130737305
    },
    {
      "epoch": 0.06249322493224932,
      "step": 1153,
      "training_loss": 7.204982757568359
    },
    {
      "epoch": 0.06254742547425474,
      "step": 1154,
      "training_loss": 5.619539260864258
    },
    {
      "epoch": 0.06260162601626017,
      "step": 1155,
      "training_loss": 8.045683860778809
    },
    {
      "epoch": 0.06265582655826558,
      "grad_norm": 24.4381160736084,
      "learning_rate": 1e-05,
      "loss": 6.9683,
      "step": 1156
    },
    {
      "epoch": 0.06265582655826558,
      "step": 1156,
      "training_loss": 7.545524597167969
    },
    {
      "epoch": 0.062710027100271,
      "step": 1157,
      "training_loss": 8.503303527832031
    },
    {
      "epoch": 0.06276422764227642,
      "step": 1158,
      "training_loss": 7.1374592781066895
    },
    {
      "epoch": 0.06281842818428185,
      "step": 1159,
      "training_loss": 8.5801420211792
    },
    {
      "epoch": 0.06287262872628727,
      "grad_norm": 22.50828742980957,
      "learning_rate": 1e-05,
      "loss": 7.9416,
      "step": 1160
    },
    {
      "epoch": 0.06287262872628727,
      "step": 1160,
      "training_loss": 6.086567401885986
    },
    {
      "epoch": 0.06292682926829268,
      "step": 1161,
      "training_loss": 5.9756245613098145
    },
    {
      "epoch": 0.0629810298102981,
      "step": 1162,
      "training_loss": 7.592171669006348
    },
    {
      "epoch": 0.06303523035230352,
      "step": 1163,
      "training_loss": 5.462793350219727
    },
    {
      "epoch": 0.06308943089430895,
      "grad_norm": 18.95348358154297,
      "learning_rate": 1e-05,
      "loss": 6.2793,
      "step": 1164
    },
    {
      "epoch": 0.06308943089430895,
      "step": 1164,
      "training_loss": 5.664377689361572
    },
    {
      "epoch": 0.06314363143631437,
      "step": 1165,
      "training_loss": 6.78983736038208
    },
    {
      "epoch": 0.06319783197831978,
      "step": 1166,
      "training_loss": 6.579728603363037
    },
    {
      "epoch": 0.0632520325203252,
      "step": 1167,
      "training_loss": 7.7860026359558105
    },
    {
      "epoch": 0.06330623306233063,
      "grad_norm": 20.34470558166504,
      "learning_rate": 1e-05,
      "loss": 6.705,
      "step": 1168
    },
    {
      "epoch": 0.06330623306233063,
      "step": 1168,
      "training_loss": 6.937973976135254
    },
    {
      "epoch": 0.06336043360433605,
      "step": 1169,
      "training_loss": 6.824165344238281
    },
    {
      "epoch": 0.06341463414634146,
      "step": 1170,
      "training_loss": 7.609042167663574
    },
    {
      "epoch": 0.06346883468834688,
      "step": 1171,
      "training_loss": 6.742302894592285
    },
    {
      "epoch": 0.0635230352303523,
      "grad_norm": 29.362123489379883,
      "learning_rate": 1e-05,
      "loss": 7.0284,
      "step": 1172
    },
    {
      "epoch": 0.0635230352303523,
      "step": 1172,
      "training_loss": 7.029359817504883
    },
    {
      "epoch": 0.06357723577235773,
      "step": 1173,
      "training_loss": 8.914628028869629
    },
    {
      "epoch": 0.06363143631436315,
      "step": 1174,
      "training_loss": 6.150058746337891
    },
    {
      "epoch": 0.06368563685636856,
      "step": 1175,
      "training_loss": 5.799278259277344
    },
    {
      "epoch": 0.06373983739837398,
      "grad_norm": 17.885576248168945,
      "learning_rate": 1e-05,
      "loss": 6.9733,
      "step": 1176
    },
    {
      "epoch": 0.06373983739837398,
      "step": 1176,
      "training_loss": 6.958876132965088
    },
    {
      "epoch": 0.0637940379403794,
      "step": 1177,
      "training_loss": 7.831454277038574
    },
    {
      "epoch": 0.06384823848238483,
      "step": 1178,
      "training_loss": 7.596394062042236
    },
    {
      "epoch": 0.06390243902439025,
      "step": 1179,
      "training_loss": 7.480057716369629
    },
    {
      "epoch": 0.06395663956639566,
      "grad_norm": 24.247356414794922,
      "learning_rate": 1e-05,
      "loss": 7.4667,
      "step": 1180
    },
    {
      "epoch": 0.06395663956639566,
      "step": 1180,
      "training_loss": 7.515820503234863
    },
    {
      "epoch": 0.06401084010840108,
      "step": 1181,
      "training_loss": 6.420022964477539
    },
    {
      "epoch": 0.06406504065040651,
      "step": 1182,
      "training_loss": 6.783469200134277
    },
    {
      "epoch": 0.06411924119241193,
      "step": 1183,
      "training_loss": 8.36957836151123
    },
    {
      "epoch": 0.06417344173441734,
      "grad_norm": 21.682851791381836,
      "learning_rate": 1e-05,
      "loss": 7.2722,
      "step": 1184
    },
    {
      "epoch": 0.06417344173441734,
      "step": 1184,
      "training_loss": 8.501017570495605
    },
    {
      "epoch": 0.06422764227642276,
      "step": 1185,
      "training_loss": 6.5178728103637695
    },
    {
      "epoch": 0.06428184281842818,
      "step": 1186,
      "training_loss": 5.253070831298828
    },
    {
      "epoch": 0.06433604336043361,
      "step": 1187,
      "training_loss": 7.070279598236084
    },
    {
      "epoch": 0.06439024390243903,
      "grad_norm": 15.383620262145996,
      "learning_rate": 1e-05,
      "loss": 6.8356,
      "step": 1188
    },
    {
      "epoch": 0.06439024390243903,
      "step": 1188,
      "training_loss": 6.496452331542969
    },
    {
      "epoch": 0.06444444444444444,
      "step": 1189,
      "training_loss": 7.044044017791748
    },
    {
      "epoch": 0.06449864498644986,
      "step": 1190,
      "training_loss": 6.293707370758057
    },
    {
      "epoch": 0.06455284552845529,
      "step": 1191,
      "training_loss": 7.0924153327941895
    },
    {
      "epoch": 0.06460704607046071,
      "grad_norm": 18.98362159729004,
      "learning_rate": 1e-05,
      "loss": 6.7317,
      "step": 1192
    },
    {
      "epoch": 0.06460704607046071,
      "step": 1192,
      "training_loss": 7.6227192878723145
    },
    {
      "epoch": 0.06466124661246613,
      "step": 1193,
      "training_loss": 6.203481197357178
    },
    {
      "epoch": 0.06471544715447154,
      "step": 1194,
      "training_loss": 7.169016361236572
    },
    {
      "epoch": 0.06476964769647696,
      "step": 1195,
      "training_loss": 7.543185710906982
    },
    {
      "epoch": 0.06482384823848239,
      "grad_norm": 11.688157081604004,
      "learning_rate": 1e-05,
      "loss": 7.1346,
      "step": 1196
    },
    {
      "epoch": 0.06482384823848239,
      "step": 1196,
      "training_loss": 7.258532524108887
    },
    {
      "epoch": 0.06487804878048781,
      "step": 1197,
      "training_loss": 7.11359977722168
    },
    {
      "epoch": 0.06493224932249322,
      "step": 1198,
      "training_loss": 6.661787986755371
    },
    {
      "epoch": 0.06498644986449864,
      "step": 1199,
      "training_loss": 6.991456508636475
    },
    {
      "epoch": 0.06504065040650407,
      "grad_norm": 17.794189453125,
      "learning_rate": 1e-05,
      "loss": 7.0063,
      "step": 1200
    },
    {
      "epoch": 0.06504065040650407,
      "step": 1200,
      "training_loss": 7.901155948638916
    },
    {
      "epoch": 0.06509485094850949,
      "step": 1201,
      "training_loss": 8.227991104125977
    },
    {
      "epoch": 0.0651490514905149,
      "step": 1202,
      "training_loss": 6.608217716217041
    },
    {
      "epoch": 0.06520325203252032,
      "step": 1203,
      "training_loss": 7.616850852966309
    },
    {
      "epoch": 0.06525745257452574,
      "grad_norm": 20.819608688354492,
      "learning_rate": 1e-05,
      "loss": 7.5886,
      "step": 1204
    },
    {
      "epoch": 0.06525745257452574,
      "step": 1204,
      "training_loss": 7.557468891143799
    },
    {
      "epoch": 0.06531165311653117,
      "step": 1205,
      "training_loss": 6.961863994598389
    },
    {
      "epoch": 0.06536585365853659,
      "step": 1206,
      "training_loss": 7.553560733795166
    },
    {
      "epoch": 0.065420054200542,
      "step": 1207,
      "training_loss": 6.905263423919678
    },
    {
      "epoch": 0.06547425474254742,
      "grad_norm": 13.821636199951172,
      "learning_rate": 1e-05,
      "loss": 7.2445,
      "step": 1208
    },
    {
      "epoch": 0.06547425474254742,
      "step": 1208,
      "training_loss": 7.615242958068848
    },
    {
      "epoch": 0.06552845528455284,
      "step": 1209,
      "training_loss": 6.089145183563232
    },
    {
      "epoch": 0.06558265582655827,
      "step": 1210,
      "training_loss": 7.811246871948242
    },
    {
      "epoch": 0.06563685636856369,
      "step": 1211,
      "training_loss": 7.233102321624756
    },
    {
      "epoch": 0.0656910569105691,
      "grad_norm": 17.266504287719727,
      "learning_rate": 1e-05,
      "loss": 7.1872,
      "step": 1212
    },
    {
      "epoch": 0.0656910569105691,
      "step": 1212,
      "training_loss": 6.4957594871521
    },
    {
      "epoch": 0.06574525745257452,
      "step": 1213,
      "training_loss": 7.054527282714844
    },
    {
      "epoch": 0.06579945799457995,
      "step": 1214,
      "training_loss": 6.697423934936523
    },
    {
      "epoch": 0.06585365853658537,
      "step": 1215,
      "training_loss": 6.35510778427124
    },
    {
      "epoch": 0.06590785907859079,
      "grad_norm": 24.541244506835938,
      "learning_rate": 1e-05,
      "loss": 6.6507,
      "step": 1216
    },
    {
      "epoch": 0.06590785907859079,
      "step": 1216,
      "training_loss": 6.923638343811035
    },
    {
      "epoch": 0.0659620596205962,
      "step": 1217,
      "training_loss": 6.537091255187988
    },
    {
      "epoch": 0.06601626016260162,
      "step": 1218,
      "training_loss": 7.848398208618164
    },
    {
      "epoch": 0.06607046070460705,
      "step": 1219,
      "training_loss": 7.84337043762207
    },
    {
      "epoch": 0.06612466124661247,
      "grad_norm": 29.432851791381836,
      "learning_rate": 1e-05,
      "loss": 7.2881,
      "step": 1220
    },
    {
      "epoch": 0.06612466124661247,
      "step": 1220,
      "training_loss": 7.562560081481934
    },
    {
      "epoch": 0.06617886178861788,
      "step": 1221,
      "training_loss": 7.682186126708984
    },
    {
      "epoch": 0.0662330623306233,
      "step": 1222,
      "training_loss": 7.396001815795898
    },
    {
      "epoch": 0.06628726287262873,
      "step": 1223,
      "training_loss": 7.484533309936523
    },
    {
      "epoch": 0.06634146341463415,
      "grad_norm": 18.112091064453125,
      "learning_rate": 1e-05,
      "loss": 7.5313,
      "step": 1224
    },
    {
      "epoch": 0.06634146341463415,
      "step": 1224,
      "training_loss": 7.489201068878174
    },
    {
      "epoch": 0.06639566395663957,
      "step": 1225,
      "training_loss": 7.112617492675781
    },
    {
      "epoch": 0.06644986449864498,
      "step": 1226,
      "training_loss": 7.391746520996094
    },
    {
      "epoch": 0.0665040650406504,
      "step": 1227,
      "training_loss": 7.2776618003845215
    },
    {
      "epoch": 0.06655826558265583,
      "grad_norm": 15.220815658569336,
      "learning_rate": 1e-05,
      "loss": 7.3178,
      "step": 1228
    },
    {
      "epoch": 0.06655826558265583,
      "step": 1228,
      "training_loss": 7.107320785522461
    },
    {
      "epoch": 0.06661246612466125,
      "step": 1229,
      "training_loss": 7.567112922668457
    },
    {
      "epoch": 0.06666666666666667,
      "step": 1230,
      "training_loss": 6.95797061920166
    },
    {
      "epoch": 0.06672086720867208,
      "step": 1231,
      "training_loss": 7.65869665145874
    },
    {
      "epoch": 0.06677506775067751,
      "grad_norm": 16.2276668548584,
      "learning_rate": 1e-05,
      "loss": 7.3228,
      "step": 1232
    },
    {
      "epoch": 0.06677506775067751,
      "step": 1232,
      "training_loss": 7.971121788024902
    },
    {
      "epoch": 0.06682926829268293,
      "step": 1233,
      "training_loss": 6.615100860595703
    },
    {
      "epoch": 0.06688346883468835,
      "step": 1234,
      "training_loss": 7.738020420074463
    },
    {
      "epoch": 0.06693766937669376,
      "step": 1235,
      "training_loss": 7.199188232421875
    },
    {
      "epoch": 0.06699186991869918,
      "grad_norm": 20.82866859436035,
      "learning_rate": 1e-05,
      "loss": 7.3809,
      "step": 1236
    },
    {
      "epoch": 0.06699186991869918,
      "step": 1236,
      "training_loss": 6.486534595489502
    },
    {
      "epoch": 0.06704607046070461,
      "step": 1237,
      "training_loss": 8.544031143188477
    },
    {
      "epoch": 0.06710027100271003,
      "step": 1238,
      "training_loss": 6.856101036071777
    },
    {
      "epoch": 0.06715447154471545,
      "step": 1239,
      "training_loss": 5.30283260345459
    },
    {
      "epoch": 0.06720867208672086,
      "grad_norm": 16.347774505615234,
      "learning_rate": 1e-05,
      "loss": 6.7974,
      "step": 1240
    },
    {
      "epoch": 0.06720867208672086,
      "step": 1240,
      "training_loss": 7.23225212097168
    },
    {
      "epoch": 0.06726287262872628,
      "step": 1241,
      "training_loss": 6.07886266708374
    },
    {
      "epoch": 0.06731707317073171,
      "step": 1242,
      "training_loss": 6.329558372497559
    },
    {
      "epoch": 0.06737127371273713,
      "step": 1243,
      "training_loss": 6.819217205047607
    },
    {
      "epoch": 0.06742547425474255,
      "grad_norm": 22.68744659423828,
      "learning_rate": 1e-05,
      "loss": 6.615,
      "step": 1244
    },
    {
      "epoch": 0.06742547425474255,
      "step": 1244,
      "training_loss": 7.869333744049072
    },
    {
      "epoch": 0.06747967479674796,
      "step": 1245,
      "training_loss": 5.674101829528809
    },
    {
      "epoch": 0.0675338753387534,
      "step": 1246,
      "training_loss": 6.418008327484131
    },
    {
      "epoch": 0.06758807588075881,
      "step": 1247,
      "training_loss": 7.838515281677246
    },
    {
      "epoch": 0.06764227642276423,
      "grad_norm": 14.113866806030273,
      "learning_rate": 1e-05,
      "loss": 6.95,
      "step": 1248
    },
    {
      "epoch": 0.06764227642276423,
      "step": 1248,
      "training_loss": 5.779331207275391
    },
    {
      "epoch": 0.06769647696476964,
      "step": 1249,
      "training_loss": 6.970399379730225
    },
    {
      "epoch": 0.06775067750677506,
      "step": 1250,
      "training_loss": 7.544131755828857
    },
    {
      "epoch": 0.06780487804878049,
      "step": 1251,
      "training_loss": 5.306141376495361
    },
    {
      "epoch": 0.06785907859078591,
      "grad_norm": 21.736698150634766,
      "learning_rate": 1e-05,
      "loss": 6.4,
      "step": 1252
    },
    {
      "epoch": 0.06785907859078591,
      "step": 1252,
      "training_loss": 6.99942684173584
    },
    {
      "epoch": 0.06791327913279133,
      "step": 1253,
      "training_loss": 6.643209457397461
    },
    {
      "epoch": 0.06796747967479674,
      "step": 1254,
      "training_loss": 7.06787633895874
    },
    {
      "epoch": 0.06802168021680217,
      "step": 1255,
      "training_loss": 7.495128631591797
    },
    {
      "epoch": 0.06807588075880759,
      "grad_norm": 19.095090866088867,
      "learning_rate": 1e-05,
      "loss": 7.0514,
      "step": 1256
    },
    {
      "epoch": 0.06807588075880759,
      "step": 1256,
      "training_loss": 7.004937171936035
    },
    {
      "epoch": 0.06813008130081301,
      "step": 1257,
      "training_loss": 7.208305358886719
    },
    {
      "epoch": 0.06818428184281843,
      "step": 1258,
      "training_loss": 7.423425197601318
    },
    {
      "epoch": 0.06823848238482384,
      "step": 1259,
      "training_loss": 6.997070789337158
    },
    {
      "epoch": 0.06829268292682927,
      "grad_norm": 23.80327033996582,
      "learning_rate": 1e-05,
      "loss": 7.1584,
      "step": 1260
    },
    {
      "epoch": 0.06829268292682927,
      "step": 1260,
      "training_loss": 6.68543004989624
    },
    {
      "epoch": 0.06834688346883469,
      "step": 1261,
      "training_loss": 6.052387714385986
    },
    {
      "epoch": 0.06840108401084011,
      "step": 1262,
      "training_loss": 6.052464962005615
    },
    {
      "epoch": 0.06845528455284552,
      "step": 1263,
      "training_loss": 6.403851509094238
    },
    {
      "epoch": 0.06850948509485096,
      "grad_norm": 18.446142196655273,
      "learning_rate": 1e-05,
      "loss": 6.2985,
      "step": 1264
    },
    {
      "epoch": 0.06850948509485096,
      "step": 1264,
      "training_loss": 7.03385066986084
    },
    {
      "epoch": 0.06856368563685637,
      "step": 1265,
      "training_loss": 8.072417259216309
    },
    {
      "epoch": 0.06861788617886179,
      "step": 1266,
      "training_loss": 7.442957401275635
    },
    {
      "epoch": 0.0686720867208672,
      "step": 1267,
      "training_loss": 6.9312052726745605
    },
    {
      "epoch": 0.06872628726287262,
      "grad_norm": 16.32529067993164,
      "learning_rate": 1e-05,
      "loss": 7.3701,
      "step": 1268
    },
    {
      "epoch": 0.06872628726287262,
      "step": 1268,
      "training_loss": 6.8176045417785645
    },
    {
      "epoch": 0.06878048780487805,
      "step": 1269,
      "training_loss": 7.9781880378723145
    },
    {
      "epoch": 0.06883468834688347,
      "step": 1270,
      "training_loss": 6.628145694732666
    },
    {
      "epoch": 0.06888888888888889,
      "step": 1271,
      "training_loss": 7.4174089431762695
    },
    {
      "epoch": 0.0689430894308943,
      "grad_norm": 19.21282386779785,
      "learning_rate": 1e-05,
      "loss": 7.2103,
      "step": 1272
    },
    {
      "epoch": 0.0689430894308943,
      "step": 1272,
      "training_loss": 5.894661903381348
    },
    {
      "epoch": 0.06899728997289972,
      "step": 1273,
      "training_loss": 8.32895278930664
    },
    {
      "epoch": 0.06905149051490515,
      "step": 1274,
      "training_loss": 7.910914421081543
    },
    {
      "epoch": 0.06910569105691057,
      "step": 1275,
      "training_loss": 6.963369846343994
    },
    {
      "epoch": 0.06915989159891599,
      "grad_norm": 17.936185836791992,
      "learning_rate": 1e-05,
      "loss": 7.2745,
      "step": 1276
    },
    {
      "epoch": 0.06915989159891599,
      "step": 1276,
      "training_loss": 9.25946044921875
    },
    {
      "epoch": 0.0692140921409214,
      "step": 1277,
      "training_loss": 7.969833850860596
    },
    {
      "epoch": 0.06926829268292684,
      "step": 1278,
      "training_loss": 7.078672885894775
    },
    {
      "epoch": 0.06932249322493225,
      "step": 1279,
      "training_loss": 7.192852020263672
    },
    {
      "epoch": 0.06937669376693767,
      "grad_norm": 12.04131031036377,
      "learning_rate": 1e-05,
      "loss": 7.8752,
      "step": 1280
    },
    {
      "epoch": 0.06937669376693767,
      "step": 1280,
      "training_loss": 7.375736713409424
    },
    {
      "epoch": 0.06943089430894309,
      "step": 1281,
      "training_loss": 6.965559005737305
    },
    {
      "epoch": 0.0694850948509485,
      "step": 1282,
      "training_loss": 11.058287620544434
    },
    {
      "epoch": 0.06953929539295393,
      "step": 1283,
      "training_loss": 7.674808979034424
    },
    {
      "epoch": 0.06959349593495935,
      "grad_norm": 13.566668510437012,
      "learning_rate": 1e-05,
      "loss": 8.2686,
      "step": 1284
    },
    {
      "epoch": 0.06959349593495935,
      "step": 1284,
      "training_loss": 9.452164649963379
    },
    {
      "epoch": 0.06964769647696477,
      "step": 1285,
      "training_loss": 8.833514213562012
    },
    {
      "epoch": 0.06970189701897019,
      "step": 1286,
      "training_loss": 6.692436218261719
    },
    {
      "epoch": 0.06975609756097562,
      "step": 1287,
      "training_loss": 6.7555742263793945
    },
    {
      "epoch": 0.06981029810298103,
      "grad_norm": 12.378443717956543,
      "learning_rate": 1e-05,
      "loss": 7.9334,
      "step": 1288
    },
    {
      "epoch": 0.06981029810298103,
      "step": 1288,
      "training_loss": 7.519520282745361
    },
    {
      "epoch": 0.06986449864498645,
      "step": 1289,
      "training_loss": 6.439953327178955
    },
    {
      "epoch": 0.06991869918699187,
      "step": 1290,
      "training_loss": 6.692440509796143
    },
    {
      "epoch": 0.06997289972899728,
      "step": 1291,
      "training_loss": 6.244106292724609
    },
    {
      "epoch": 0.07002710027100272,
      "grad_norm": 12.969138145446777,
      "learning_rate": 1e-05,
      "loss": 6.724,
      "step": 1292
    },
    {
      "epoch": 0.07002710027100272,
      "step": 1292,
      "training_loss": 8.146893501281738
    },
    {
      "epoch": 0.07008130081300813,
      "step": 1293,
      "training_loss": 6.87779426574707
    },
    {
      "epoch": 0.07013550135501355,
      "step": 1294,
      "training_loss": 5.506044387817383
    },
    {
      "epoch": 0.07018970189701897,
      "step": 1295,
      "training_loss": 7.793099880218506
    },
    {
      "epoch": 0.0702439024390244,
      "grad_norm": 18.20217514038086,
      "learning_rate": 1e-05,
      "loss": 7.081,
      "step": 1296
    },
    {
      "epoch": 0.0702439024390244,
      "step": 1296,
      "training_loss": 7.330090045928955
    },
    {
      "epoch": 0.07029810298102981,
      "step": 1297,
      "training_loss": 7.517210006713867
    },
    {
      "epoch": 0.07035230352303523,
      "step": 1298,
      "training_loss": 7.536279201507568
    },
    {
      "epoch": 0.07040650406504065,
      "step": 1299,
      "training_loss": 6.697569847106934
    },
    {
      "epoch": 0.07046070460704607,
      "grad_norm": 23.272714614868164,
      "learning_rate": 1e-05,
      "loss": 7.2703,
      "step": 1300
    },
    {
      "epoch": 0.07046070460704607,
      "step": 1300,
      "training_loss": 7.980480194091797
    },
    {
      "epoch": 0.0705149051490515,
      "step": 1301,
      "training_loss": 5.9181928634643555
    },
    {
      "epoch": 0.07056910569105691,
      "step": 1302,
      "training_loss": 8.12515640258789
    },
    {
      "epoch": 0.07062330623306233,
      "step": 1303,
      "training_loss": 7.587394714355469
    },
    {
      "epoch": 0.07067750677506775,
      "grad_norm": 14.558653831481934,
      "learning_rate": 1e-05,
      "loss": 7.4028,
      "step": 1304
    },
    {
      "epoch": 0.07067750677506775,
      "step": 1304,
      "training_loss": 7.514743804931641
    },
    {
      "epoch": 0.07073170731707316,
      "step": 1305,
      "training_loss": 8.084977149963379
    },
    {
      "epoch": 0.0707859078590786,
      "step": 1306,
      "training_loss": 6.610060214996338
    },
    {
      "epoch": 0.07084010840108401,
      "step": 1307,
      "training_loss": 7.934784889221191
    },
    {
      "epoch": 0.07089430894308943,
      "grad_norm": 25.570402145385742,
      "learning_rate": 1e-05,
      "loss": 7.5361,
      "step": 1308
    },
    {
      "epoch": 0.07089430894308943,
      "step": 1308,
      "training_loss": 8.580503463745117
    },
    {
      "epoch": 0.07094850948509485,
      "step": 1309,
      "training_loss": 7.062569618225098
    },
    {
      "epoch": 0.07100271002710028,
      "step": 1310,
      "training_loss": 7.262284278869629
    },
    {
      "epoch": 0.0710569105691057,
      "step": 1311,
      "training_loss": 6.484193325042725
    },
    {
      "epoch": 0.07111111111111111,
      "grad_norm": 19.427053451538086,
      "learning_rate": 1e-05,
      "loss": 7.3474,
      "step": 1312
    },
    {
      "epoch": 0.07111111111111111,
      "step": 1312,
      "training_loss": 5.9641008377075195
    },
    {
      "epoch": 0.07116531165311653,
      "step": 1313,
      "training_loss": 6.6972808837890625
    },
    {
      "epoch": 0.07121951219512195,
      "step": 1314,
      "training_loss": 6.620757102966309
    },
    {
      "epoch": 0.07127371273712738,
      "step": 1315,
      "training_loss": 6.24309778213501
    },
    {
      "epoch": 0.07132791327913279,
      "grad_norm": 27.212825775146484,
      "learning_rate": 1e-05,
      "loss": 6.3813,
      "step": 1316
    },
    {
      "epoch": 0.07132791327913279,
      "step": 1316,
      "training_loss": 6.594232082366943
    },
    {
      "epoch": 0.07138211382113821,
      "step": 1317,
      "training_loss": 7.6760029792785645
    },
    {
      "epoch": 0.07143631436314363,
      "step": 1318,
      "training_loss": 7.49739408493042
    },
    {
      "epoch": 0.07149051490514906,
      "step": 1319,
      "training_loss": 7.373599052429199
    },
    {
      "epoch": 0.07154471544715447,
      "grad_norm": 26.727933883666992,
      "learning_rate": 1e-05,
      "loss": 7.2853,
      "step": 1320
    },
    {
      "epoch": 0.07154471544715447,
      "step": 1320,
      "training_loss": 6.636991500854492
    },
    {
      "epoch": 0.07159891598915989,
      "step": 1321,
      "training_loss": 7.629459381103516
    },
    {
      "epoch": 0.07165311653116531,
      "step": 1322,
      "training_loss": 6.804813861846924
    },
    {
      "epoch": 0.07170731707317073,
      "step": 1323,
      "training_loss": 6.669712543487549
    },
    {
      "epoch": 0.07176151761517616,
      "grad_norm": 13.523297309875488,
      "learning_rate": 1e-05,
      "loss": 6.9352,
      "step": 1324
    },
    {
      "epoch": 0.07176151761517616,
      "step": 1324,
      "training_loss": 9.069578170776367
    },
    {
      "epoch": 0.07181571815718157,
      "step": 1325,
      "training_loss": 8.159207344055176
    },
    {
      "epoch": 0.07186991869918699,
      "step": 1326,
      "training_loss": 9.148430824279785
    },
    {
      "epoch": 0.07192411924119241,
      "step": 1327,
      "training_loss": 7.067361831665039
    },
    {
      "epoch": 0.07197831978319784,
      "grad_norm": 25.682552337646484,
      "learning_rate": 1e-05,
      "loss": 8.3611,
      "step": 1328
    },
    {
      "epoch": 0.07197831978319784,
      "step": 1328,
      "training_loss": 7.775543212890625
    },
    {
      "epoch": 0.07203252032520326,
      "step": 1329,
      "training_loss": 7.616308212280273
    },
    {
      "epoch": 0.07208672086720867,
      "step": 1330,
      "training_loss": 7.10444974899292
    },
    {
      "epoch": 0.07214092140921409,
      "step": 1331,
      "training_loss": 8.170500755310059
    },
    {
      "epoch": 0.0721951219512195,
      "grad_norm": 16.557764053344727,
      "learning_rate": 1e-05,
      "loss": 7.6667,
      "step": 1332
    },
    {
      "epoch": 0.0721951219512195,
      "step": 1332,
      "training_loss": 5.681288242340088
    },
    {
      "epoch": 0.07224932249322494,
      "step": 1333,
      "training_loss": 7.5265960693359375
    },
    {
      "epoch": 0.07230352303523035,
      "step": 1334,
      "training_loss": 7.6290283203125
    },
    {
      "epoch": 0.07235772357723577,
      "step": 1335,
      "training_loss": 8.207475662231445
    },
    {
      "epoch": 0.07241192411924119,
      "grad_norm": 19.8802547454834,
      "learning_rate": 1e-05,
      "loss": 7.2611,
      "step": 1336
    },
    {
      "epoch": 0.07241192411924119,
      "step": 1336,
      "training_loss": 7.050888538360596
    },
    {
      "epoch": 0.0724661246612466,
      "step": 1337,
      "training_loss": 5.191099166870117
    },
    {
      "epoch": 0.07252032520325204,
      "step": 1338,
      "training_loss": 7.669728755950928
    },
    {
      "epoch": 0.07257452574525745,
      "step": 1339,
      "training_loss": 8.126837730407715
    },
    {
      "epoch": 0.07262872628726287,
      "grad_norm": 13.511286735534668,
      "learning_rate": 1e-05,
      "loss": 7.0096,
      "step": 1340
    },
    {
      "epoch": 0.07262872628726287,
      "step": 1340,
      "training_loss": 6.420637607574463
    },
    {
      "epoch": 0.07268292682926829,
      "step": 1341,
      "training_loss": 5.995233535766602
    },
    {
      "epoch": 0.07273712737127372,
      "step": 1342,
      "training_loss": 7.170783042907715
    },
    {
      "epoch": 0.07279132791327914,
      "step": 1343,
      "training_loss": 7.402502536773682
    },
    {
      "epoch": 0.07284552845528455,
      "grad_norm": 15.774076461791992,
      "learning_rate": 1e-05,
      "loss": 6.7473,
      "step": 1344
    },
    {
      "epoch": 0.07284552845528455,
      "step": 1344,
      "training_loss": 8.018043518066406
    },
    {
      "epoch": 0.07289972899728997,
      "step": 1345,
      "training_loss": 7.46043586730957
    },
    {
      "epoch": 0.07295392953929539,
      "step": 1346,
      "training_loss": 7.304476261138916
    },
    {
      "epoch": 0.07300813008130082,
      "step": 1347,
      "training_loss": 7.7636189460754395
    },
    {
      "epoch": 0.07306233062330623,
      "grad_norm": 20.593793869018555,
      "learning_rate": 1e-05,
      "loss": 7.6366,
      "step": 1348
    },
    {
      "epoch": 0.07306233062330623,
      "step": 1348,
      "training_loss": 6.435647487640381
    },
    {
      "epoch": 0.07311653116531165,
      "step": 1349,
      "training_loss": 7.217040061950684
    },
    {
      "epoch": 0.07317073170731707,
      "step": 1350,
      "training_loss": 7.993233680725098
    },
    {
      "epoch": 0.0732249322493225,
      "step": 1351,
      "training_loss": 6.630616664886475
    },
    {
      "epoch": 0.07327913279132792,
      "grad_norm": 13.688066482543945,
      "learning_rate": 1e-05,
      "loss": 7.0691,
      "step": 1352
    },
    {
      "epoch": 0.07327913279132792,
      "step": 1352,
      "training_loss": 8.521154403686523
    },
    {
      "epoch": 0.07333333333333333,
      "step": 1353,
      "training_loss": 6.450411796569824
    },
    {
      "epoch": 0.07338753387533875,
      "step": 1354,
      "training_loss": 7.5537238121032715
    },
    {
      "epoch": 0.07344173441734417,
      "step": 1355,
      "training_loss": 7.611783504486084
    },
    {
      "epoch": 0.0734959349593496,
      "grad_norm": 52.94451904296875,
      "learning_rate": 1e-05,
      "loss": 7.5343,
      "step": 1356
    },
    {
      "epoch": 0.0734959349593496,
      "step": 1356,
      "training_loss": 5.538791179656982
    },
    {
      "epoch": 0.07355013550135502,
      "step": 1357,
      "training_loss": 7.1327128410339355
    },
    {
      "epoch": 0.07360433604336043,
      "step": 1358,
      "training_loss": 7.333003997802734
    },
    {
      "epoch": 0.07365853658536585,
      "step": 1359,
      "training_loss": 6.648930549621582
    },
    {
      "epoch": 0.07371273712737128,
      "grad_norm": 17.89681053161621,
      "learning_rate": 1e-05,
      "loss": 6.6634,
      "step": 1360
    },
    {
      "epoch": 0.07371273712737128,
      "step": 1360,
      "training_loss": 6.737459182739258
    },
    {
      "epoch": 0.0737669376693767,
      "step": 1361,
      "training_loss": 7.360080242156982
    },
    {
      "epoch": 0.07382113821138211,
      "step": 1362,
      "training_loss": 7.177248001098633
    },
    {
      "epoch": 0.07387533875338753,
      "step": 1363,
      "training_loss": 8.061264991760254
    },
    {
      "epoch": 0.07392953929539295,
      "grad_norm": 24.463987350463867,
      "learning_rate": 1e-05,
      "loss": 7.334,
      "step": 1364
    },
    {
      "epoch": 0.07392953929539295,
      "step": 1364,
      "training_loss": 7.587916851043701
    },
    {
      "epoch": 0.07398373983739838,
      "step": 1365,
      "training_loss": 7.355554580688477
    },
    {
      "epoch": 0.0740379403794038,
      "step": 1366,
      "training_loss": 6.500428199768066
    },
    {
      "epoch": 0.07409214092140921,
      "step": 1367,
      "training_loss": 7.597382068634033
    },
    {
      "epoch": 0.07414634146341463,
      "grad_norm": 15.095193862915039,
      "learning_rate": 1e-05,
      "loss": 7.2603,
      "step": 1368
    },
    {
      "epoch": 0.07414634146341463,
      "step": 1368,
      "training_loss": 6.5892229080200195
    },
    {
      "epoch": 0.07420054200542005,
      "step": 1369,
      "training_loss": 6.494722843170166
    },
    {
      "epoch": 0.07425474254742548,
      "step": 1370,
      "training_loss": 7.964849948883057
    },
    {
      "epoch": 0.0743089430894309,
      "step": 1371,
      "training_loss": 8.512529373168945
    },
    {
      "epoch": 0.07436314363143631,
      "grad_norm": 21.977903366088867,
      "learning_rate": 1e-05,
      "loss": 7.3903,
      "step": 1372
    },
    {
      "epoch": 0.07436314363143631,
      "step": 1372,
      "training_loss": 7.559330463409424
    },
    {
      "epoch": 0.07441734417344173,
      "step": 1373,
      "training_loss": 7.239760875701904
    },
    {
      "epoch": 0.07447154471544716,
      "step": 1374,
      "training_loss": 7.072460174560547
    },
    {
      "epoch": 0.07452574525745258,
      "step": 1375,
      "training_loss": 7.119217395782471
    },
    {
      "epoch": 0.074579945799458,
      "grad_norm": 18.09760856628418,
      "learning_rate": 1e-05,
      "loss": 7.2477,
      "step": 1376
    },
    {
      "epoch": 0.074579945799458,
      "step": 1376,
      "training_loss": 7.294633388519287
    },
    {
      "epoch": 0.07463414634146341,
      "step": 1377,
      "training_loss": 6.429388523101807
    },
    {
      "epoch": 0.07468834688346883,
      "step": 1378,
      "training_loss": 7.016976356506348
    },
    {
      "epoch": 0.07474254742547426,
      "step": 1379,
      "training_loss": 6.5732951164245605
    },
    {
      "epoch": 0.07479674796747968,
      "grad_norm": 14.66733455657959,
      "learning_rate": 1e-05,
      "loss": 6.8286,
      "step": 1380
    },
    {
      "epoch": 0.07479674796747968,
      "step": 1380,
      "training_loss": 7.496856212615967
    },
    {
      "epoch": 0.0748509485094851,
      "step": 1381,
      "training_loss": 7.9602952003479
    },
    {
      "epoch": 0.07490514905149051,
      "step": 1382,
      "training_loss": 6.639913558959961
    },
    {
      "epoch": 0.07495934959349594,
      "step": 1383,
      "training_loss": 7.5907392501831055
    },
    {
      "epoch": 0.07501355013550136,
      "grad_norm": 17.689172744750977,
      "learning_rate": 1e-05,
      "loss": 7.422,
      "step": 1384
    },
    {
      "epoch": 0.07501355013550136,
      "step": 1384,
      "training_loss": 7.628103733062744
    },
    {
      "epoch": 0.07506775067750678,
      "step": 1385,
      "training_loss": 8.529654502868652
    },
    {
      "epoch": 0.07512195121951219,
      "step": 1386,
      "training_loss": 5.507836818695068
    },
    {
      "epoch": 0.07517615176151761,
      "step": 1387,
      "training_loss": 7.2611494064331055
    },
    {
      "epoch": 0.07523035230352304,
      "grad_norm": 29.80289077758789,
      "learning_rate": 1e-05,
      "loss": 7.2317,
      "step": 1388
    },
    {
      "epoch": 0.07523035230352304,
      "step": 1388,
      "training_loss": 7.598761081695557
    },
    {
      "epoch": 0.07528455284552846,
      "step": 1389,
      "training_loss": 6.10706901550293
    },
    {
      "epoch": 0.07533875338753387,
      "step": 1390,
      "training_loss": 9.289928436279297
    },
    {
      "epoch": 0.07539295392953929,
      "step": 1391,
      "training_loss": 8.023929595947266
    },
    {
      "epoch": 0.07544715447154472,
      "grad_norm": 30.00541877746582,
      "learning_rate": 1e-05,
      "loss": 7.7549,
      "step": 1392
    },
    {
      "epoch": 0.07544715447154472,
      "step": 1392,
      "training_loss": 6.688792705535889
    },
    {
      "epoch": 0.07550135501355014,
      "step": 1393,
      "training_loss": 7.545599937438965
    },
    {
      "epoch": 0.07555555555555556,
      "step": 1394,
      "training_loss": 8.403003692626953
    },
    {
      "epoch": 0.07560975609756097,
      "step": 1395,
      "training_loss": 7.7653279304504395
    },
    {
      "epoch": 0.07566395663956639,
      "grad_norm": 23.372568130493164,
      "learning_rate": 1e-05,
      "loss": 7.6007,
      "step": 1396
    },
    {
      "epoch": 0.07566395663956639,
      "step": 1396,
      "training_loss": 7.430633068084717
    },
    {
      "epoch": 0.07571815718157182,
      "step": 1397,
      "training_loss": 6.868967056274414
    },
    {
      "epoch": 0.07577235772357724,
      "step": 1398,
      "training_loss": 6.976731300354004
    },
    {
      "epoch": 0.07582655826558266,
      "step": 1399,
      "training_loss": 7.246401309967041
    },
    {
      "epoch": 0.07588075880758807,
      "grad_norm": 15.635798454284668,
      "learning_rate": 1e-05,
      "loss": 7.1307,
      "step": 1400
    },
    {
      "epoch": 0.07588075880758807,
      "step": 1400,
      "training_loss": 6.245474338531494
    },
    {
      "epoch": 0.07593495934959349,
      "step": 1401,
      "training_loss": 6.546582221984863
    },
    {
      "epoch": 0.07598915989159892,
      "step": 1402,
      "training_loss": 8.040111541748047
    },
    {
      "epoch": 0.07604336043360434,
      "step": 1403,
      "training_loss": 7.86308479309082
    },
    {
      "epoch": 0.07609756097560975,
      "grad_norm": 15.693978309631348,
      "learning_rate": 1e-05,
      "loss": 7.1738,
      "step": 1404
    },
    {
      "epoch": 0.07609756097560975,
      "step": 1404,
      "training_loss": 5.040076732635498
    },
    {
      "epoch": 0.07615176151761517,
      "step": 1405,
      "training_loss": 7.231629371643066
    },
    {
      "epoch": 0.0762059620596206,
      "step": 1406,
      "training_loss": 7.01637077331543
    },
    {
      "epoch": 0.07626016260162602,
      "step": 1407,
      "training_loss": 6.504293918609619
    },
    {
      "epoch": 0.07631436314363144,
      "grad_norm": 18.494321823120117,
      "learning_rate": 1e-05,
      "loss": 6.4481,
      "step": 1408
    },
    {
      "epoch": 0.07631436314363144,
      "step": 1408,
      "training_loss": 6.78996467590332
    },
    {
      "epoch": 0.07636856368563685,
      "step": 1409,
      "training_loss": 6.724791526794434
    },
    {
      "epoch": 0.07642276422764227,
      "step": 1410,
      "training_loss": 6.271981239318848
    },
    {
      "epoch": 0.0764769647696477,
      "step": 1411,
      "training_loss": 7.091998100280762
    },
    {
      "epoch": 0.07653116531165312,
      "grad_norm": 13.725008964538574,
      "learning_rate": 1e-05,
      "loss": 6.7197,
      "step": 1412
    },
    {
      "epoch": 0.07653116531165312,
      "step": 1412,
      "training_loss": 7.8359785079956055
    },
    {
      "epoch": 0.07658536585365854,
      "step": 1413,
      "training_loss": 7.817992687225342
    },
    {
      "epoch": 0.07663956639566395,
      "step": 1414,
      "training_loss": 7.339719295501709
    },
    {
      "epoch": 0.07669376693766938,
      "step": 1415,
      "training_loss": 7.190313339233398
    },
    {
      "epoch": 0.0767479674796748,
      "grad_norm": 37.538291931152344,
      "learning_rate": 1e-05,
      "loss": 7.546,
      "step": 1416
    },
    {
      "epoch": 0.0767479674796748,
      "step": 1416,
      "training_loss": 7.485129356384277
    },
    {
      "epoch": 0.07680216802168022,
      "step": 1417,
      "training_loss": 6.963754653930664
    },
    {
      "epoch": 0.07685636856368563,
      "step": 1418,
      "training_loss": 6.3950676918029785
    },
    {
      "epoch": 0.07691056910569105,
      "step": 1419,
      "training_loss": 8.116442680358887
    },
    {
      "epoch": 0.07696476964769648,
      "grad_norm": 14.048554420471191,
      "learning_rate": 1e-05,
      "loss": 7.2401,
      "step": 1420
    },
    {
      "epoch": 0.07696476964769648,
      "step": 1420,
      "training_loss": 6.996037006378174
    },
    {
      "epoch": 0.0770189701897019,
      "step": 1421,
      "training_loss": 8.285539627075195
    },
    {
      "epoch": 0.07707317073170732,
      "step": 1422,
      "training_loss": 6.649250507354736
    },
    {
      "epoch": 0.07712737127371273,
      "step": 1423,
      "training_loss": 7.309784412384033
    },
    {
      "epoch": 0.07718157181571816,
      "grad_norm": 19.633821487426758,
      "learning_rate": 1e-05,
      "loss": 7.3102,
      "step": 1424
    },
    {
      "epoch": 0.07718157181571816,
      "step": 1424,
      "training_loss": 6.99269962310791
    },
    {
      "epoch": 0.07723577235772358,
      "step": 1425,
      "training_loss": 6.368566513061523
    },
    {
      "epoch": 0.077289972899729,
      "step": 1426,
      "training_loss": 7.817479610443115
    },
    {
      "epoch": 0.07734417344173442,
      "step": 1427,
      "training_loss": 6.435400009155273
    },
    {
      "epoch": 0.07739837398373983,
      "grad_norm": 21.16888999938965,
      "learning_rate": 1e-05,
      "loss": 6.9035,
      "step": 1428
    },
    {
      "epoch": 0.07739837398373983,
      "step": 1428,
      "training_loss": 6.521190166473389
    },
    {
      "epoch": 0.07745257452574526,
      "step": 1429,
      "training_loss": 7.970839977264404
    },
    {
      "epoch": 0.07750677506775068,
      "step": 1430,
      "training_loss": 7.6834917068481445
    },
    {
      "epoch": 0.0775609756097561,
      "step": 1431,
      "training_loss": 7.40251350402832
    },
    {
      "epoch": 0.07761517615176151,
      "grad_norm": 23.670787811279297,
      "learning_rate": 1e-05,
      "loss": 7.3945,
      "step": 1432
    },
    {
      "epoch": 0.07761517615176151,
      "step": 1432,
      "training_loss": 9.035991668701172
    },
    {
      "epoch": 0.07766937669376693,
      "step": 1433,
      "training_loss": 6.817252159118652
    },
    {
      "epoch": 0.07772357723577236,
      "step": 1434,
      "training_loss": 6.138806343078613
    },
    {
      "epoch": 0.07777777777777778,
      "step": 1435,
      "training_loss": 7.0732011795043945
    },
    {
      "epoch": 0.0778319783197832,
      "grad_norm": 14.223114967346191,
      "learning_rate": 1e-05,
      "loss": 7.2663,
      "step": 1436
    },
    {
      "epoch": 0.0778319783197832,
      "step": 1436,
      "training_loss": 7.20578145980835
    },
    {
      "epoch": 0.07788617886178861,
      "step": 1437,
      "training_loss": 8.071422576904297
    },
    {
      "epoch": 0.07794037940379404,
      "step": 1438,
      "training_loss": 7.480517387390137
    },
    {
      "epoch": 0.07799457994579946,
      "step": 1439,
      "training_loss": 7.467219829559326
    },
    {
      "epoch": 0.07804878048780488,
      "grad_norm": 24.664318084716797,
      "learning_rate": 1e-05,
      "loss": 7.5562,
      "step": 1440
    },
    {
      "epoch": 0.07804878048780488,
      "step": 1440,
      "training_loss": 7.579808712005615
    },
    {
      "epoch": 0.0781029810298103,
      "step": 1441,
      "training_loss": 6.848506450653076
    },
    {
      "epoch": 0.07815718157181571,
      "step": 1442,
      "training_loss": 7.306116580963135
    },
    {
      "epoch": 0.07821138211382114,
      "step": 1443,
      "training_loss": 6.494561672210693
    },
    {
      "epoch": 0.07826558265582656,
      "grad_norm": 42.137451171875,
      "learning_rate": 1e-05,
      "loss": 7.0572,
      "step": 1444
    },
    {
      "epoch": 0.07826558265582656,
      "step": 1444,
      "training_loss": 7.392282009124756
    },
    {
      "epoch": 0.07831978319783198,
      "step": 1445,
      "training_loss": 6.134659290313721
    },
    {
      "epoch": 0.0783739837398374,
      "step": 1446,
      "training_loss": 5.638955593109131
    },
    {
      "epoch": 0.07842818428184282,
      "step": 1447,
      "training_loss": 6.898643493652344
    },
    {
      "epoch": 0.07848238482384824,
      "grad_norm": 23.83702278137207,
      "learning_rate": 1e-05,
      "loss": 6.5161,
      "step": 1448
    },
    {
      "epoch": 0.07848238482384824,
      "step": 1448,
      "training_loss": 6.566101551055908
    },
    {
      "epoch": 0.07853658536585366,
      "step": 1449,
      "training_loss": 6.311508655548096
    },
    {
      "epoch": 0.07859078590785908,
      "step": 1450,
      "training_loss": 7.895265579223633
    },
    {
      "epoch": 0.07864498644986449,
      "step": 1451,
      "training_loss": 7.614598274230957
    },
    {
      "epoch": 0.07869918699186992,
      "grad_norm": 17.752578735351562,
      "learning_rate": 1e-05,
      "loss": 7.0969,
      "step": 1452
    },
    {
      "epoch": 0.07869918699186992,
      "step": 1452,
      "training_loss": 6.970240592956543
    },
    {
      "epoch": 0.07875338753387534,
      "step": 1453,
      "training_loss": 8.099026679992676
    },
    {
      "epoch": 0.07880758807588076,
      "step": 1454,
      "training_loss": 7.569939136505127
    },
    {
      "epoch": 0.07886178861788617,
      "step": 1455,
      "training_loss": 7.156240463256836
    },
    {
      "epoch": 0.0789159891598916,
      "grad_norm": 18.15254783630371,
      "learning_rate": 1e-05,
      "loss": 7.4489,
      "step": 1456
    },
    {
      "epoch": 0.0789159891598916,
      "step": 1456,
      "training_loss": 7.655505180358887
    },
    {
      "epoch": 0.07897018970189702,
      "step": 1457,
      "training_loss": 6.60431432723999
    },
    {
      "epoch": 0.07902439024390244,
      "step": 1458,
      "training_loss": 7.953263282775879
    },
    {
      "epoch": 0.07907859078590786,
      "step": 1459,
      "training_loss": 7.432521820068359
    },
    {
      "epoch": 0.07913279132791327,
      "grad_norm": 21.70386505126953,
      "learning_rate": 1e-05,
      "loss": 7.4114,
      "step": 1460
    },
    {
      "epoch": 0.07913279132791327,
      "step": 1460,
      "training_loss": 6.045256614685059
    },
    {
      "epoch": 0.0791869918699187,
      "step": 1461,
      "training_loss": 6.47216796875
    },
    {
      "epoch": 0.07924119241192412,
      "step": 1462,
      "training_loss": 8.049202919006348
    },
    {
      "epoch": 0.07929539295392954,
      "step": 1463,
      "training_loss": 7.122693061828613
    },
    {
      "epoch": 0.07934959349593496,
      "grad_norm": 15.110715866088867,
      "learning_rate": 1e-05,
      "loss": 6.9223,
      "step": 1464
    },
    {
      "epoch": 0.07934959349593496,
      "step": 1464,
      "training_loss": 6.9110589027404785
    },
    {
      "epoch": 0.07940379403794037,
      "step": 1465,
      "training_loss": 6.566760063171387
    },
    {
      "epoch": 0.0794579945799458,
      "step": 1466,
      "training_loss": 5.219304084777832
    },
    {
      "epoch": 0.07951219512195122,
      "step": 1467,
      "training_loss": 7.674643516540527
    },
    {
      "epoch": 0.07956639566395664,
      "grad_norm": 13.845264434814453,
      "learning_rate": 1e-05,
      "loss": 6.5929,
      "step": 1468
    },
    {
      "epoch": 0.07956639566395664,
      "step": 1468,
      "training_loss": 7.560214519500732
    },
    {
      "epoch": 0.07962059620596205,
      "step": 1469,
      "training_loss": 5.237473964691162
    },
    {
      "epoch": 0.07967479674796749,
      "step": 1470,
      "training_loss": 6.023062705993652
    },
    {
      "epoch": 0.0797289972899729,
      "step": 1471,
      "training_loss": 7.002035140991211
    },
    {
      "epoch": 0.07978319783197832,
      "grad_norm": 20.4355411529541,
      "learning_rate": 1e-05,
      "loss": 6.4557,
      "step": 1472
    },
    {
      "epoch": 0.07978319783197832,
      "step": 1472,
      "training_loss": 6.8937907218933105
    },
    {
      "epoch": 0.07983739837398374,
      "step": 1473,
      "training_loss": 6.148158073425293
    },
    {
      "epoch": 0.07989159891598915,
      "step": 1474,
      "training_loss": 8.882543563842773
    },
    {
      "epoch": 0.07994579945799458,
      "step": 1475,
      "training_loss": 6.937169075012207
    },
    {
      "epoch": 0.08,
      "grad_norm": 25.23741912841797,
      "learning_rate": 1e-05,
      "loss": 7.2154,
      "step": 1476
    },
    {
      "epoch": 0.08,
      "step": 1476,
      "training_loss": 7.165998935699463
    },
    {
      "epoch": 0.08005420054200542,
      "step": 1477,
      "training_loss": 7.397009372711182
    },
    {
      "epoch": 0.08010840108401084,
      "step": 1478,
      "training_loss": 5.513299465179443
    },
    {
      "epoch": 0.08016260162601627,
      "step": 1479,
      "training_loss": 6.627427577972412
    },
    {
      "epoch": 0.08021680216802168,
      "grad_norm": 14.394490242004395,
      "learning_rate": 1e-05,
      "loss": 6.6759,
      "step": 1480
    },
    {
      "epoch": 0.08021680216802168,
      "step": 1480,
      "training_loss": 6.956590175628662
    },
    {
      "epoch": 0.0802710027100271,
      "step": 1481,
      "training_loss": 6.424236297607422
    },
    {
      "epoch": 0.08032520325203252,
      "step": 1482,
      "training_loss": 7.15387487411499
    },
    {
      "epoch": 0.08037940379403793,
      "step": 1483,
      "training_loss": 6.621458053588867
    },
    {
      "epoch": 0.08043360433604337,
      "grad_norm": 16.38645362854004,
      "learning_rate": 1e-05,
      "loss": 6.789,
      "step": 1484
    },
    {
      "epoch": 0.08043360433604337,
      "step": 1484,
      "training_loss": 6.7173333168029785
    },
    {
      "epoch": 0.08048780487804878,
      "step": 1485,
      "training_loss": 5.35543155670166
    },
    {
      "epoch": 0.0805420054200542,
      "step": 1486,
      "training_loss": 8.152166366577148
    },
    {
      "epoch": 0.08059620596205962,
      "step": 1487,
      "training_loss": 7.406446933746338
    },
    {
      "epoch": 0.08065040650406505,
      "grad_norm": 20.081880569458008,
      "learning_rate": 1e-05,
      "loss": 6.9078,
      "step": 1488
    },
    {
      "epoch": 0.08065040650406505,
      "step": 1488,
      "training_loss": 7.458789348602295
    },
    {
      "epoch": 0.08070460704607046,
      "step": 1489,
      "training_loss": 6.390273571014404
    },
    {
      "epoch": 0.08075880758807588,
      "step": 1490,
      "training_loss": 7.265676975250244
    },
    {
      "epoch": 0.0808130081300813,
      "step": 1491,
      "training_loss": 8.377300262451172
    },
    {
      "epoch": 0.08086720867208672,
      "grad_norm": 22.48006820678711,
      "learning_rate": 1e-05,
      "loss": 7.373,
      "step": 1492
    },
    {
      "epoch": 0.08086720867208672,
      "step": 1492,
      "training_loss": 6.276495933532715
    },
    {
      "epoch": 0.08092140921409215,
      "step": 1493,
      "training_loss": 6.643661975860596
    },
    {
      "epoch": 0.08097560975609756,
      "step": 1494,
      "training_loss": 6.761312484741211
    },
    {
      "epoch": 0.08102981029810298,
      "step": 1495,
      "training_loss": 7.584204196929932
    },
    {
      "epoch": 0.0810840108401084,
      "grad_norm": 15.860672950744629,
      "learning_rate": 1e-05,
      "loss": 6.8164,
      "step": 1496
    },
    {
      "epoch": 0.0810840108401084,
      "step": 1496,
      "training_loss": 5.024026393890381
    },
    {
      "epoch": 0.08113821138211381,
      "step": 1497,
      "training_loss": 7.907277584075928
    },
    {
      "epoch": 0.08119241192411925,
      "step": 1498,
      "training_loss": 6.9513092041015625
    },
    {
      "epoch": 0.08124661246612466,
      "step": 1499,
      "training_loss": 6.9853410720825195
    },
    {
      "epoch": 0.08130081300813008,
      "grad_norm": 17.394542694091797,
      "learning_rate": 1e-05,
      "loss": 6.717,
      "step": 1500
    },
    {
      "epoch": 0.08130081300813008,
      "step": 1500,
      "training_loss": 6.105336666107178
    },
    {
      "epoch": 0.0813550135501355,
      "step": 1501,
      "training_loss": 7.092627048492432
    },
    {
      "epoch": 0.08140921409214093,
      "step": 1502,
      "training_loss": 6.558999538421631
    },
    {
      "epoch": 0.08146341463414634,
      "step": 1503,
      "training_loss": 8.096537590026855
    },
    {
      "epoch": 0.08151761517615176,
      "grad_norm": 22.192474365234375,
      "learning_rate": 1e-05,
      "loss": 6.9634,
      "step": 1504
    },
    {
      "epoch": 0.08151761517615176,
      "step": 1504,
      "training_loss": 7.470880508422852
    },
    {
      "epoch": 0.08157181571815718,
      "step": 1505,
      "training_loss": 6.671574115753174
    },
    {
      "epoch": 0.0816260162601626,
      "step": 1506,
      "training_loss": 6.071920394897461
    },
    {
      "epoch": 0.08168021680216803,
      "step": 1507,
      "training_loss": 6.956241130828857
    },
    {
      "epoch": 0.08173441734417344,
      "grad_norm": 26.47064781188965,
      "learning_rate": 1e-05,
      "loss": 6.7927,
      "step": 1508
    },
    {
      "epoch": 0.08173441734417344,
      "step": 1508,
      "training_loss": 6.8162970542907715
    },
    {
      "epoch": 0.08178861788617886,
      "step": 1509,
      "training_loss": 6.921463966369629
    },
    {
      "epoch": 0.08184281842818428,
      "step": 1510,
      "training_loss": 7.125483512878418
    },
    {
      "epoch": 0.08189701897018971,
      "step": 1511,
      "training_loss": 5.455466270446777
    },
    {
      "epoch": 0.08195121951219513,
      "grad_norm": 18.126893997192383,
      "learning_rate": 1e-05,
      "loss": 6.5797,
      "step": 1512
    },
    {
      "epoch": 0.08195121951219513,
      "step": 1512,
      "training_loss": 6.609006404876709
    },
    {
      "epoch": 0.08200542005420054,
      "step": 1513,
      "training_loss": 7.985594749450684
    },
    {
      "epoch": 0.08205962059620596,
      "step": 1514,
      "training_loss": 6.592886447906494
    },
    {
      "epoch": 0.08211382113821138,
      "step": 1515,
      "training_loss": 6.069246768951416
    },
    {
      "epoch": 0.08216802168021681,
      "grad_norm": 21.564937591552734,
      "learning_rate": 1e-05,
      "loss": 6.8142,
      "step": 1516
    },
    {
      "epoch": 0.08216802168021681,
      "step": 1516,
      "training_loss": 7.090270042419434
    },
    {
      "epoch": 0.08222222222222222,
      "step": 1517,
      "training_loss": 7.298571586608887
    },
    {
      "epoch": 0.08227642276422764,
      "step": 1518,
      "training_loss": 6.740000247955322
    },
    {
      "epoch": 0.08233062330623306,
      "step": 1519,
      "training_loss": 11.398785591125488
    },
    {
      "epoch": 0.08238482384823849,
      "grad_norm": 37.81239318847656,
      "learning_rate": 1e-05,
      "loss": 8.1319,
      "step": 1520
    },
    {
      "epoch": 0.08238482384823849,
      "step": 1520,
      "training_loss": 7.549437999725342
    },
    {
      "epoch": 0.0824390243902439,
      "step": 1521,
      "training_loss": 7.608947277069092
    },
    {
      "epoch": 0.08249322493224932,
      "step": 1522,
      "training_loss": 6.870312690734863
    },
    {
      "epoch": 0.08254742547425474,
      "step": 1523,
      "training_loss": 6.6610517501831055
    },
    {
      "epoch": 0.08260162601626016,
      "grad_norm": 25.696657180786133,
      "learning_rate": 1e-05,
      "loss": 7.1724,
      "step": 1524
    },
    {
      "epoch": 0.08260162601626016,
      "step": 1524,
      "training_loss": 7.4949750900268555
    },
    {
      "epoch": 0.08265582655826559,
      "step": 1525,
      "training_loss": 5.225578308105469
    },
    {
      "epoch": 0.082710027100271,
      "step": 1526,
      "training_loss": 7.049113750457764
    },
    {
      "epoch": 0.08276422764227642,
      "step": 1527,
      "training_loss": 7.2169189453125
    },
    {
      "epoch": 0.08281842818428184,
      "grad_norm": 24.93592071533203,
      "learning_rate": 1e-05,
      "loss": 6.7466,
      "step": 1528
    },
    {
      "epoch": 0.08281842818428184,
      "step": 1528,
      "training_loss": 7.409938335418701
    },
    {
      "epoch": 0.08287262872628726,
      "step": 1529,
      "training_loss": 7.165013790130615
    },
    {
      "epoch": 0.08292682926829269,
      "step": 1530,
      "training_loss": 8.702951431274414
    },
    {
      "epoch": 0.0829810298102981,
      "step": 1531,
      "training_loss": 8.115078926086426
    },
    {
      "epoch": 0.08303523035230352,
      "grad_norm": 23.805255889892578,
      "learning_rate": 1e-05,
      "loss": 7.8482,
      "step": 1532
    },
    {
      "epoch": 0.08303523035230352,
      "step": 1532,
      "training_loss": 7.938722610473633
    },
    {
      "epoch": 0.08308943089430894,
      "step": 1533,
      "training_loss": 5.741585731506348
    },
    {
      "epoch": 0.08314363143631437,
      "step": 1534,
      "training_loss": 7.104894638061523
    },
    {
      "epoch": 0.08319783197831979,
      "step": 1535,
      "training_loss": 6.0118327140808105
    },
    {
      "epoch": 0.0832520325203252,
      "grad_norm": 14.243522644042969,
      "learning_rate": 1e-05,
      "loss": 6.6993,
      "step": 1536
    },
    {
      "epoch": 0.0832520325203252,
      "step": 1536,
      "training_loss": 7.262784481048584
    },
    {
      "epoch": 0.08330623306233062,
      "step": 1537,
      "training_loss": 6.9191131591796875
    },
    {
      "epoch": 0.08336043360433604,
      "step": 1538,
      "training_loss": 7.988018989562988
    },
    {
      "epoch": 0.08341463414634147,
      "step": 1539,
      "training_loss": 8.090574264526367
    },
    {
      "epoch": 0.08346883468834689,
      "grad_norm": 19.91983413696289,
      "learning_rate": 1e-05,
      "loss": 7.5651,
      "step": 1540
    },
    {
      "epoch": 0.08346883468834689,
      "step": 1540,
      "training_loss": 8.881203651428223
    },
    {
      "epoch": 0.0835230352303523,
      "step": 1541,
      "training_loss": 7.409794330596924
    },
    {
      "epoch": 0.08357723577235772,
      "step": 1542,
      "training_loss": 8.56857967376709
    },
    {
      "epoch": 0.08363143631436315,
      "step": 1543,
      "training_loss": 6.943595886230469
    },
    {
      "epoch": 0.08368563685636857,
      "grad_norm": 15.645379066467285,
      "learning_rate": 1e-05,
      "loss": 7.9508,
      "step": 1544
    },
    {
      "epoch": 0.08368563685636857,
      "step": 1544,
      "training_loss": 7.356025218963623
    },
    {
      "epoch": 0.08373983739837398,
      "step": 1545,
      "training_loss": 7.733541488647461
    },
    {
      "epoch": 0.0837940379403794,
      "step": 1546,
      "training_loss": 7.200157642364502
    },
    {
      "epoch": 0.08384823848238482,
      "step": 1547,
      "training_loss": 7.594922065734863
    },
    {
      "epoch": 0.08390243902439025,
      "grad_norm": 13.607351303100586,
      "learning_rate": 1e-05,
      "loss": 7.4712,
      "step": 1548
    },
    {
      "epoch": 0.08390243902439025,
      "step": 1548,
      "training_loss": 6.70325231552124
    },
    {
      "epoch": 0.08395663956639567,
      "step": 1549,
      "training_loss": 7.938178539276123
    },
    {
      "epoch": 0.08401084010840108,
      "step": 1550,
      "training_loss": 8.155025482177734
    },
    {
      "epoch": 0.0840650406504065,
      "step": 1551,
      "training_loss": 7.428589344024658
    },
    {
      "epoch": 0.08411924119241193,
      "grad_norm": 31.541887283325195,
      "learning_rate": 1e-05,
      "loss": 7.5563,
      "step": 1552
    },
    {
      "epoch": 0.08411924119241193,
      "step": 1552,
      "training_loss": 6.888820648193359
    },
    {
      "epoch": 0.08417344173441735,
      "step": 1553,
      "training_loss": 6.773348331451416
    },
    {
      "epoch": 0.08422764227642277,
      "step": 1554,
      "training_loss": 7.2355217933654785
    },
    {
      "epoch": 0.08428184281842818,
      "step": 1555,
      "training_loss": 8.344488143920898
    },
    {
      "epoch": 0.0843360433604336,
      "grad_norm": 23.43170928955078,
      "learning_rate": 1e-05,
      "loss": 7.3105,
      "step": 1556
    },
    {
      "epoch": 0.0843360433604336,
      "step": 1556,
      "training_loss": 7.104752540588379
    },
    {
      "epoch": 0.08439024390243903,
      "step": 1557,
      "training_loss": 5.187212944030762
    },
    {
      "epoch": 0.08444444444444445,
      "step": 1558,
      "training_loss": 7.240516185760498
    },
    {
      "epoch": 0.08449864498644986,
      "step": 1559,
      "training_loss": 7.27593994140625
    },
    {
      "epoch": 0.08455284552845528,
      "grad_norm": 18.487150192260742,
      "learning_rate": 1e-05,
      "loss": 6.7021,
      "step": 1560
    },
    {
      "epoch": 0.08455284552845528,
      "step": 1560,
      "training_loss": 5.9194769859313965
    },
    {
      "epoch": 0.0846070460704607,
      "step": 1561,
      "training_loss": 7.973294258117676
    },
    {
      "epoch": 0.08466124661246613,
      "step": 1562,
      "training_loss": 7.3256096839904785
    },
    {
      "epoch": 0.08471544715447155,
      "step": 1563,
      "training_loss": 7.686769485473633
    },
    {
      "epoch": 0.08476964769647696,
      "grad_norm": 28.198734283447266,
      "learning_rate": 1e-05,
      "loss": 7.2263,
      "step": 1564
    },
    {
      "epoch": 0.08476964769647696,
      "step": 1564,
      "training_loss": 7.542346954345703
    },
    {
      "epoch": 0.08482384823848238,
      "step": 1565,
      "training_loss": 6.45596981048584
    },
    {
      "epoch": 0.08487804878048781,
      "step": 1566,
      "training_loss": 6.859743595123291
    },
    {
      "epoch": 0.08493224932249323,
      "step": 1567,
      "training_loss": 5.823577404022217
    },
    {
      "epoch": 0.08498644986449864,
      "grad_norm": 15.307823181152344,
      "learning_rate": 1e-05,
      "loss": 6.6704,
      "step": 1568
    },
    {
      "epoch": 0.08498644986449864,
      "step": 1568,
      "training_loss": 7.583731174468994
    },
    {
      "epoch": 0.08504065040650406,
      "step": 1569,
      "training_loss": 7.5811767578125
    },
    {
      "epoch": 0.08509485094850948,
      "step": 1570,
      "training_loss": 6.795912265777588
    },
    {
      "epoch": 0.08514905149051491,
      "step": 1571,
      "training_loss": 7.405450344085693
    },
    {
      "epoch": 0.08520325203252033,
      "grad_norm": 26.650653839111328,
      "learning_rate": 1e-05,
      "loss": 7.3416,
      "step": 1572
    },
    {
      "epoch": 0.08520325203252033,
      "step": 1572,
      "training_loss": 6.925144672393799
    },
    {
      "epoch": 0.08525745257452574,
      "step": 1573,
      "training_loss": 7.117498874664307
    },
    {
      "epoch": 0.08531165311653116,
      "step": 1574,
      "training_loss": 7.433722972869873
    },
    {
      "epoch": 0.08536585365853659,
      "step": 1575,
      "training_loss": 6.889317035675049
    },
    {
      "epoch": 0.08542005420054201,
      "grad_norm": 18.049654006958008,
      "learning_rate": 1e-05,
      "loss": 7.0914,
      "step": 1576
    },
    {
      "epoch": 0.08542005420054201,
      "step": 1576,
      "training_loss": 6.523434162139893
    },
    {
      "epoch": 0.08547425474254743,
      "step": 1577,
      "training_loss": 6.53915548324585
    },
    {
      "epoch": 0.08552845528455284,
      "step": 1578,
      "training_loss": 7.580345630645752
    },
    {
      "epoch": 0.08558265582655826,
      "step": 1579,
      "training_loss": 7.635035991668701
    },
    {
      "epoch": 0.08563685636856369,
      "grad_norm": 21.523439407348633,
      "learning_rate": 1e-05,
      "loss": 7.0695,
      "step": 1580
    },
    {
      "epoch": 0.08563685636856369,
      "step": 1580,
      "training_loss": 6.668606281280518
    },
    {
      "epoch": 0.08569105691056911,
      "step": 1581,
      "training_loss": 6.72382116317749
    },
    {
      "epoch": 0.08574525745257452,
      "step": 1582,
      "training_loss": 7.158984184265137
    },
    {
      "epoch": 0.08579945799457994,
      "step": 1583,
      "training_loss": 7.330589771270752
    },
    {
      "epoch": 0.08585365853658537,
      "grad_norm": 17.996816635131836,
      "learning_rate": 1e-05,
      "loss": 6.9705,
      "step": 1584
    },
    {
      "epoch": 0.08585365853658537,
      "step": 1584,
      "training_loss": 6.535168170928955
    },
    {
      "epoch": 0.08590785907859079,
      "step": 1585,
      "training_loss": 7.677383899688721
    },
    {
      "epoch": 0.0859620596205962,
      "step": 1586,
      "training_loss": 6.562088966369629
    },
    {
      "epoch": 0.08601626016260162,
      "step": 1587,
      "training_loss": 6.779745578765869
    },
    {
      "epoch": 0.08607046070460704,
      "grad_norm": 21.74555015563965,
      "learning_rate": 1e-05,
      "loss": 6.8886,
      "step": 1588
    },
    {
      "epoch": 0.08607046070460704,
      "step": 1588,
      "training_loss": 8.241918563842773
    },
    {
      "epoch": 0.08612466124661247,
      "step": 1589,
      "training_loss": 6.6531901359558105
    },
    {
      "epoch": 0.08617886178861789,
      "step": 1590,
      "training_loss": 7.717720985412598
    },
    {
      "epoch": 0.0862330623306233,
      "step": 1591,
      "training_loss": 6.020966053009033
    },
    {
      "epoch": 0.08628726287262872,
      "grad_norm": 16.044780731201172,
      "learning_rate": 1e-05,
      "loss": 7.1584,
      "step": 1592
    },
    {
      "epoch": 0.08628726287262872,
      "step": 1592,
      "training_loss": 9.817872047424316
    },
    {
      "epoch": 0.08634146341463414,
      "step": 1593,
      "training_loss": 7.6298723220825195
    },
    {
      "epoch": 0.08639566395663957,
      "step": 1594,
      "training_loss": 6.75858211517334
    },
    {
      "epoch": 0.08644986449864499,
      "step": 1595,
      "training_loss": 9.481396675109863
    },
    {
      "epoch": 0.0865040650406504,
      "grad_norm": 33.85790252685547,
      "learning_rate": 1e-05,
      "loss": 8.4219,
      "step": 1596
    },
    {
      "epoch": 0.0865040650406504,
      "step": 1596,
      "training_loss": 7.313276767730713
    },
    {
      "epoch": 0.08655826558265582,
      "step": 1597,
      "training_loss": 7.281013011932373
    },
    {
      "epoch": 0.08661246612466125,
      "step": 1598,
      "training_loss": 7.151163101196289
    },
    {
      "epoch": 0.08666666666666667,
      "step": 1599,
      "training_loss": 6.7644219398498535
    },
    {
      "epoch": 0.08672086720867209,
      "grad_norm": 17.21185302734375,
      "learning_rate": 1e-05,
      "loss": 7.1275,
      "step": 1600
    },
    {
      "epoch": 0.08672086720867209,
      "step": 1600,
      "training_loss": 7.123671531677246
    },
    {
      "epoch": 0.0867750677506775,
      "step": 1601,
      "training_loss": 6.964865207672119
    },
    {
      "epoch": 0.08682926829268292,
      "step": 1602,
      "training_loss": 5.422028064727783
    },
    {
      "epoch": 0.08688346883468835,
      "step": 1603,
      "training_loss": 5.575869560241699
    },
    {
      "epoch": 0.08693766937669377,
      "grad_norm": 36.85334396362305,
      "learning_rate": 1e-05,
      "loss": 6.2716,
      "step": 1604
    },
    {
      "epoch": 0.08693766937669377,
      "step": 1604,
      "training_loss": 7.80880880355835
    },
    {
      "epoch": 0.08699186991869919,
      "step": 1605,
      "training_loss": 7.339385509490967
    },
    {
      "epoch": 0.0870460704607046,
      "step": 1606,
      "training_loss": 7.033257007598877
    },
    {
      "epoch": 0.08710027100271003,
      "step": 1607,
      "training_loss": 7.587625503540039
    },
    {
      "epoch": 0.08715447154471545,
      "grad_norm": 26.551273345947266,
      "learning_rate": 1e-05,
      "loss": 7.4423,
      "step": 1608
    },
    {
      "epoch": 0.08715447154471545,
      "step": 1608,
      "training_loss": 6.939778804779053
    },
    {
      "epoch": 0.08720867208672087,
      "step": 1609,
      "training_loss": 5.784375190734863
    },
    {
      "epoch": 0.08726287262872628,
      "step": 1610,
      "training_loss": 6.988247394561768
    },
    {
      "epoch": 0.0873170731707317,
      "step": 1611,
      "training_loss": 9.202912330627441
    },
    {
      "epoch": 0.08737127371273713,
      "grad_norm": 47.047611236572266,
      "learning_rate": 1e-05,
      "loss": 7.2288,
      "step": 1612
    },
    {
      "epoch": 0.08737127371273713,
      "step": 1612,
      "training_loss": 6.002492904663086
    },
    {
      "epoch": 0.08742547425474255,
      "step": 1613,
      "training_loss": 6.229701519012451
    },
    {
      "epoch": 0.08747967479674797,
      "step": 1614,
      "training_loss": 7.681308269500732
    },
    {
      "epoch": 0.08753387533875338,
      "step": 1615,
      "training_loss": 6.27416467666626
    },
    {
      "epoch": 0.08758807588075881,
      "grad_norm": 22.022615432739258,
      "learning_rate": 1e-05,
      "loss": 6.5469,
      "step": 1616
    },
    {
      "epoch": 0.08758807588075881,
      "step": 1616,
      "training_loss": 5.585240840911865
    },
    {
      "epoch": 0.08764227642276423,
      "step": 1617,
      "training_loss": 7.478819370269775
    },
    {
      "epoch": 0.08769647696476965,
      "step": 1618,
      "training_loss": 7.32618522644043
    },
    {
      "epoch": 0.08775067750677507,
      "step": 1619,
      "training_loss": 6.169351577758789
    },
    {
      "epoch": 0.08780487804878048,
      "grad_norm": 14.736547470092773,
      "learning_rate": 1e-05,
      "loss": 6.6399,
      "step": 1620
    },
    {
      "epoch": 0.08780487804878048,
      "step": 1620,
      "training_loss": 7.288869380950928
    },
    {
      "epoch": 0.08785907859078591,
      "step": 1621,
      "training_loss": 4.688216209411621
    },
    {
      "epoch": 0.08791327913279133,
      "step": 1622,
      "training_loss": 5.676927089691162
    },
    {
      "epoch": 0.08796747967479675,
      "step": 1623,
      "training_loss": 6.484979629516602
    },
    {
      "epoch": 0.08802168021680216,
      "grad_norm": 15.032989501953125,
      "learning_rate": 1e-05,
      "loss": 6.0347,
      "step": 1624
    },
    {
      "epoch": 0.08802168021680216,
      "step": 1624,
      "training_loss": 6.748158931732178
    },
    {
      "epoch": 0.08807588075880758,
      "step": 1625,
      "training_loss": 8.676514625549316
    },
    {
      "epoch": 0.08813008130081301,
      "step": 1626,
      "training_loss": 6.736316680908203
    },
    {
      "epoch": 0.08818428184281843,
      "step": 1627,
      "training_loss": 8.372291564941406
    },
    {
      "epoch": 0.08823848238482385,
      "grad_norm": 22.17508888244629,
      "learning_rate": 1e-05,
      "loss": 7.6333,
      "step": 1628
    },
    {
      "epoch": 0.08823848238482385,
      "step": 1628,
      "training_loss": 7.854903697967529
    },
    {
      "epoch": 0.08829268292682926,
      "step": 1629,
      "training_loss": 8.114118576049805
    },
    {
      "epoch": 0.0883468834688347,
      "step": 1630,
      "training_loss": 7.274725914001465
    },
    {
      "epoch": 0.08840108401084011,
      "step": 1631,
      "training_loss": 7.442898273468018
    },
    {
      "epoch": 0.08845528455284553,
      "grad_norm": 20.545764923095703,
      "learning_rate": 1e-05,
      "loss": 7.6717,
      "step": 1632
    },
    {
      "epoch": 0.08845528455284553,
      "step": 1632,
      "training_loss": 5.580777168273926
    },
    {
      "epoch": 0.08850948509485095,
      "step": 1633,
      "training_loss": 8.175048828125
    },
    {
      "epoch": 0.08856368563685636,
      "step": 1634,
      "training_loss": 6.947646617889404
    },
    {
      "epoch": 0.0886178861788618,
      "step": 1635,
      "training_loss": 6.21968936920166
    },
    {
      "epoch": 0.08867208672086721,
      "grad_norm": 21.360780715942383,
      "learning_rate": 1e-05,
      "loss": 6.7308,
      "step": 1636
    },
    {
      "epoch": 0.08867208672086721,
      "step": 1636,
      "training_loss": 7.093921184539795
    },
    {
      "epoch": 0.08872628726287263,
      "step": 1637,
      "training_loss": 7.506170749664307
    },
    {
      "epoch": 0.08878048780487804,
      "step": 1638,
      "training_loss": 7.138119220733643
    },
    {
      "epoch": 0.08883468834688348,
      "step": 1639,
      "training_loss": 8.075820922851562
    },
    {
      "epoch": 0.08888888888888889,
      "grad_norm": 15.183390617370605,
      "learning_rate": 1e-05,
      "loss": 7.4535,
      "step": 1640
    },
    {
      "epoch": 0.08888888888888889,
      "step": 1640,
      "training_loss": 6.718434810638428
    },
    {
      "epoch": 0.08894308943089431,
      "step": 1641,
      "training_loss": 6.749777317047119
    },
    {
      "epoch": 0.08899728997289973,
      "step": 1642,
      "training_loss": 7.979598522186279
    },
    {
      "epoch": 0.08905149051490514,
      "step": 1643,
      "training_loss": 6.925064563751221
    },
    {
      "epoch": 0.08910569105691057,
      "grad_norm": 39.536964416503906,
      "learning_rate": 1e-05,
      "loss": 7.0932,
      "step": 1644
    },
    {
      "epoch": 0.08910569105691057,
      "step": 1644,
      "training_loss": 7.009535789489746
    },
    {
      "epoch": 0.08915989159891599,
      "step": 1645,
      "training_loss": 6.237033367156982
    },
    {
      "epoch": 0.08921409214092141,
      "step": 1646,
      "training_loss": 5.891733169555664
    },
    {
      "epoch": 0.08926829268292683,
      "step": 1647,
      "training_loss": 6.250555038452148
    },
    {
      "epoch": 0.08932249322493226,
      "grad_norm": 29.263757705688477,
      "learning_rate": 1e-05,
      "loss": 6.3472,
      "step": 1648
    },
    {
      "epoch": 0.08932249322493226,
      "step": 1648,
      "training_loss": 6.953632354736328
    },
    {
      "epoch": 0.08937669376693767,
      "step": 1649,
      "training_loss": 8.989429473876953
    },
    {
      "epoch": 0.08943089430894309,
      "step": 1650,
      "training_loss": 4.6601457595825195
    },
    {
      "epoch": 0.08948509485094851,
      "step": 1651,
      "training_loss": 7.617650032043457
    },
    {
      "epoch": 0.08953929539295392,
      "grad_norm": 20.728431701660156,
      "learning_rate": 1e-05,
      "loss": 7.0552,
      "step": 1652
    },
    {
      "epoch": 0.08953929539295392,
      "step": 1652,
      "training_loss": 6.447307586669922
    },
    {
      "epoch": 0.08959349593495936,
      "step": 1653,
      "training_loss": 7.548569202423096
    },
    {
      "epoch": 0.08964769647696477,
      "step": 1654,
      "training_loss": 7.182150840759277
    },
    {
      "epoch": 0.08970189701897019,
      "step": 1655,
      "training_loss": 8.147107124328613
    },
    {
      "epoch": 0.0897560975609756,
      "grad_norm": 17.337038040161133,
      "learning_rate": 1e-05,
      "loss": 7.3313,
      "step": 1656
    },
    {
      "epoch": 0.0897560975609756,
      "step": 1656,
      "training_loss": 7.160848140716553
    },
    {
      "epoch": 0.08981029810298102,
      "step": 1657,
      "training_loss": 7.219114780426025
    },
    {
      "epoch": 0.08986449864498645,
      "step": 1658,
      "training_loss": 7.852097034454346
    },
    {
      "epoch": 0.08991869918699187,
      "step": 1659,
      "training_loss": 7.2736616134643555
    },
    {
      "epoch": 0.08997289972899729,
      "grad_norm": 15.549280166625977,
      "learning_rate": 1e-05,
      "loss": 7.3764,
      "step": 1660
    },
    {
      "epoch": 0.08997289972899729,
      "step": 1660,
      "training_loss": 8.21684741973877
    },
    {
      "epoch": 0.0900271002710027,
      "step": 1661,
      "training_loss": 6.508275508880615
    },
    {
      "epoch": 0.09008130081300814,
      "step": 1662,
      "training_loss": 6.70107364654541
    },
    {
      "epoch": 0.09013550135501355,
      "step": 1663,
      "training_loss": 7.71552848815918
    },
    {
      "epoch": 0.09018970189701897,
      "grad_norm": 16.131174087524414,
      "learning_rate": 1e-05,
      "loss": 7.2854,
      "step": 1664
    },
    {
      "epoch": 0.09018970189701897,
      "step": 1664,
      "training_loss": 7.004007816314697
    },
    {
      "epoch": 0.09024390243902439,
      "step": 1665,
      "training_loss": 6.639325141906738
    },
    {
      "epoch": 0.0902981029810298,
      "step": 1666,
      "training_loss": 6.506496906280518
    },
    {
      "epoch": 0.09035230352303524,
      "step": 1667,
      "training_loss": 6.646972179412842
    },
    {
      "epoch": 0.09040650406504065,
      "grad_norm": 21.314233779907227,
      "learning_rate": 1e-05,
      "loss": 6.6992,
      "step": 1668
    },
    {
      "epoch": 0.09040650406504065,
      "step": 1668,
      "training_loss": 6.48427677154541
    },
    {
      "epoch": 0.09046070460704607,
      "step": 1669,
      "training_loss": 6.787038326263428
    },
    {
      "epoch": 0.09051490514905149,
      "step": 1670,
      "training_loss": 6.616793632507324
    },
    {
      "epoch": 0.09056910569105692,
      "step": 1671,
      "training_loss": 6.998480796813965
    },
    {
      "epoch": 0.09062330623306233,
      "grad_norm": 24.879770278930664,
      "learning_rate": 1e-05,
      "loss": 6.7216,
      "step": 1672
    },
    {
      "epoch": 0.09062330623306233,
      "step": 1672,
      "training_loss": 6.941429615020752
    },
    {
      "epoch": 0.09067750677506775,
      "step": 1673,
      "training_loss": 5.861907958984375
    },
    {
      "epoch": 0.09073170731707317,
      "step": 1674,
      "training_loss": 6.384670257568359
    },
    {
      "epoch": 0.09078590785907859,
      "step": 1675,
      "training_loss": 6.893835544586182
    },
    {
      "epoch": 0.09084010840108402,
      "grad_norm": 16.238502502441406,
      "learning_rate": 1e-05,
      "loss": 6.5205,
      "step": 1676
    },
    {
      "epoch": 0.09084010840108402,
      "step": 1676,
      "training_loss": 8.070765495300293
    },
    {
      "epoch": 0.09089430894308943,
      "step": 1677,
      "training_loss": 6.743760108947754
    },
    {
      "epoch": 0.09094850948509485,
      "step": 1678,
      "training_loss": 6.405947208404541
    },
    {
      "epoch": 0.09100271002710027,
      "step": 1679,
      "training_loss": 8.092854499816895
    },
    {
      "epoch": 0.0910569105691057,
      "grad_norm": 17.252174377441406,
      "learning_rate": 1e-05,
      "loss": 7.3283,
      "step": 1680
    },
    {
      "epoch": 0.0910569105691057,
      "step": 1680,
      "training_loss": 7.3212080001831055
    },
    {
      "epoch": 0.09111111111111111,
      "step": 1681,
      "training_loss": 8.87846565246582
    },
    {
      "epoch": 0.09116531165311653,
      "step": 1682,
      "training_loss": 7.089105129241943
    },
    {
      "epoch": 0.09121951219512195,
      "step": 1683,
      "training_loss": 6.897226810455322
    },
    {
      "epoch": 0.09127371273712737,
      "grad_norm": 22.430179595947266,
      "learning_rate": 1e-05,
      "loss": 7.5465,
      "step": 1684
    },
    {
      "epoch": 0.09127371273712737,
      "step": 1684,
      "training_loss": 6.415831089019775
    },
    {
      "epoch": 0.0913279132791328,
      "step": 1685,
      "training_loss": 6.781657695770264
    },
    {
      "epoch": 0.09138211382113821,
      "step": 1686,
      "training_loss": 5.891106128692627
    },
    {
      "epoch": 0.09143631436314363,
      "step": 1687,
      "training_loss": 5.012064456939697
    },
    {
      "epoch": 0.09149051490514905,
      "grad_norm": 19.26152801513672,
      "learning_rate": 1e-05,
      "loss": 6.0252,
      "step": 1688
    },
    {
      "epoch": 0.09149051490514905,
      "step": 1688,
      "training_loss": 8.217626571655273
    },
    {
      "epoch": 0.09154471544715446,
      "step": 1689,
      "training_loss": 7.038978099822998
    },
    {
      "epoch": 0.0915989159891599,
      "step": 1690,
      "training_loss": 6.29246711730957
    },
    {
      "epoch": 0.09165311653116531,
      "step": 1691,
      "training_loss": 7.3843865394592285
    },
    {
      "epoch": 0.09170731707317073,
      "grad_norm": 15.88426399230957,
      "learning_rate": 1e-05,
      "loss": 7.2334,
      "step": 1692
    },
    {
      "epoch": 0.09170731707317073,
      "step": 1692,
      "training_loss": 7.540465354919434
    },
    {
      "epoch": 0.09176151761517615,
      "step": 1693,
      "training_loss": 7.558072090148926
    },
    {
      "epoch": 0.09181571815718158,
      "step": 1694,
      "training_loss": 7.320772647857666
    },
    {
      "epoch": 0.091869918699187,
      "step": 1695,
      "training_loss": 7.443187236785889
    },
    {
      "epoch": 0.09192411924119241,
      "grad_norm": 17.922340393066406,
      "learning_rate": 1e-05,
      "loss": 7.4656,
      "step": 1696
    },
    {
      "epoch": 0.09192411924119241,
      "step": 1696,
      "training_loss": 7.122110366821289
    },
    {
      "epoch": 0.09197831978319783,
      "step": 1697,
      "training_loss": 5.091551303863525
    },
    {
      "epoch": 0.09203252032520325,
      "step": 1698,
      "training_loss": 6.835752487182617
    },
    {
      "epoch": 0.09208672086720868,
      "step": 1699,
      "training_loss": 6.449139595031738
    },
    {
      "epoch": 0.0921409214092141,
      "grad_norm": 15.705408096313477,
      "learning_rate": 1e-05,
      "loss": 6.3746,
      "step": 1700
    },
    {
      "epoch": 0.0921409214092141,
      "step": 1700,
      "training_loss": 9.28454875946045
    },
    {
      "epoch": 0.09219512195121951,
      "step": 1701,
      "training_loss": 6.343812465667725
    },
    {
      "epoch": 0.09224932249322493,
      "step": 1702,
      "training_loss": 4.7976813316345215
    },
    {
      "epoch": 0.09230352303523036,
      "step": 1703,
      "training_loss": 7.948275566101074
    },
    {
      "epoch": 0.09235772357723578,
      "grad_norm": 23.639522552490234,
      "learning_rate": 1e-05,
      "loss": 7.0936,
      "step": 1704
    },
    {
      "epoch": 0.09235772357723578,
      "step": 1704,
      "training_loss": 7.589580059051514
    },
    {
      "epoch": 0.09241192411924119,
      "step": 1705,
      "training_loss": 5.874301910400391
    },
    {
      "epoch": 0.09246612466124661,
      "step": 1706,
      "training_loss": 6.815415859222412
    },
    {
      "epoch": 0.09252032520325203,
      "step": 1707,
      "training_loss": 7.598508834838867
    },
    {
      "epoch": 0.09257452574525746,
      "grad_norm": 16.76819610595703,
      "learning_rate": 1e-05,
      "loss": 6.9695,
      "step": 1708
    },
    {
      "epoch": 0.09257452574525746,
      "step": 1708,
      "training_loss": 6.924911975860596
    },
    {
      "epoch": 0.09262872628726287,
      "step": 1709,
      "training_loss": 7.427004337310791
    },
    {
      "epoch": 0.09268292682926829,
      "step": 1710,
      "training_loss": 7.596051216125488
    },
    {
      "epoch": 0.09273712737127371,
      "step": 1711,
      "training_loss": 7.249311447143555
    },
    {
      "epoch": 0.09279132791327914,
      "grad_norm": 36.00410461425781,
      "learning_rate": 1e-05,
      "loss": 7.2993,
      "step": 1712
    },
    {
      "epoch": 0.09279132791327914,
      "step": 1712,
      "training_loss": 7.356167316436768
    },
    {
      "epoch": 0.09284552845528456,
      "step": 1713,
      "training_loss": 5.279910087585449
    },
    {
      "epoch": 0.09289972899728997,
      "step": 1714,
      "training_loss": 7.40649938583374
    },
    {
      "epoch": 0.09295392953929539,
      "step": 1715,
      "training_loss": 7.539501667022705
    },
    {
      "epoch": 0.09300813008130081,
      "grad_norm": 22.652135848999023,
      "learning_rate": 1e-05,
      "loss": 6.8955,
      "step": 1716
    },
    {
      "epoch": 0.09300813008130081,
      "step": 1716,
      "training_loss": 8.626862525939941
    },
    {
      "epoch": 0.09306233062330624,
      "step": 1717,
      "training_loss": 7.179759979248047
    },
    {
      "epoch": 0.09311653116531166,
      "step": 1718,
      "training_loss": 7.615808010101318
    },
    {
      "epoch": 0.09317073170731707,
      "step": 1719,
      "training_loss": 11.11100959777832
    },
    {
      "epoch": 0.09322493224932249,
      "grad_norm": 52.94569778442383,
      "learning_rate": 1e-05,
      "loss": 8.6334,
      "step": 1720
    },
    {
      "epoch": 0.09322493224932249,
      "step": 1720,
      "training_loss": 7.584996223449707
    },
    {
      "epoch": 0.0932791327913279,
      "step": 1721,
      "training_loss": 7.91416597366333
    },
    {
      "epoch": 0.09333333333333334,
      "step": 1722,
      "training_loss": 6.997749328613281
    },
    {
      "epoch": 0.09338753387533875,
      "step": 1723,
      "training_loss": 7.072558879852295
    },
    {
      "epoch": 0.09344173441734417,
      "grad_norm": 23.074668884277344,
      "learning_rate": 1e-05,
      "loss": 7.3924,
      "step": 1724
    },
    {
      "epoch": 0.09344173441734417,
      "step": 1724,
      "training_loss": 7.791093826293945
    },
    {
      "epoch": 0.09349593495934959,
      "step": 1725,
      "training_loss": 9.33739948272705
    },
    {
      "epoch": 0.09355013550135502,
      "step": 1726,
      "training_loss": 8.524396896362305
    },
    {
      "epoch": 0.09360433604336044,
      "step": 1727,
      "training_loss": 7.42374849319458
    },
    {
      "epoch": 0.09365853658536585,
      "grad_norm": 13.249479293823242,
      "learning_rate": 1e-05,
      "loss": 8.2692,
      "step": 1728
    },
    {
      "epoch": 0.09365853658536585,
      "step": 1728,
      "training_loss": 9.12932300567627
    },
    {
      "epoch": 0.09371273712737127,
      "step": 1729,
      "training_loss": 7.320218086242676
    },
    {
      "epoch": 0.09376693766937669,
      "step": 1730,
      "training_loss": 6.8992018699646
    },
    {
      "epoch": 0.09382113821138212,
      "step": 1731,
      "training_loss": 7.438861846923828
    },
    {
      "epoch": 0.09387533875338754,
      "grad_norm": 24.36119270324707,
      "learning_rate": 1e-05,
      "loss": 7.6969,
      "step": 1732
    },
    {
      "epoch": 0.09387533875338754,
      "step": 1732,
      "training_loss": 6.066250801086426
    },
    {
      "epoch": 0.09392953929539295,
      "step": 1733,
      "training_loss": 8.088546752929688
    },
    {
      "epoch": 0.09398373983739837,
      "step": 1734,
      "training_loss": 7.038322448730469
    },
    {
      "epoch": 0.0940379403794038,
      "step": 1735,
      "training_loss": 7.741424560546875
    },
    {
      "epoch": 0.09409214092140922,
      "grad_norm": 17.9685001373291,
      "learning_rate": 1e-05,
      "loss": 7.2336,
      "step": 1736
    },
    {
      "epoch": 0.09409214092140922,
      "step": 1736,
      "training_loss": 6.628321647644043
    },
    {
      "epoch": 0.09414634146341463,
      "step": 1737,
      "training_loss": 7.795683860778809
    },
    {
      "epoch": 0.09420054200542005,
      "step": 1738,
      "training_loss": 6.239549160003662
    },
    {
      "epoch": 0.09425474254742547,
      "step": 1739,
      "training_loss": 5.0602288246154785
    },
    {
      "epoch": 0.0943089430894309,
      "grad_norm": 17.41661262512207,
      "learning_rate": 1e-05,
      "loss": 6.4309,
      "step": 1740
    },
    {
      "epoch": 0.0943089430894309,
      "step": 1740,
      "training_loss": 8.33154582977295
    },
    {
      "epoch": 0.09436314363143632,
      "step": 1741,
      "training_loss": 7.251248359680176
    },
    {
      "epoch": 0.09441734417344173,
      "step": 1742,
      "training_loss": 7.42059850692749
    },
    {
      "epoch": 0.09447154471544715,
      "step": 1743,
      "training_loss": 6.009142875671387
    },
    {
      "epoch": 0.09452574525745258,
      "grad_norm": 24.224031448364258,
      "learning_rate": 1e-05,
      "loss": 7.2531,
      "step": 1744
    },
    {
      "epoch": 0.09452574525745258,
      "step": 1744,
      "training_loss": 8.310248374938965
    },
    {
      "epoch": 0.094579945799458,
      "step": 1745,
      "training_loss": 7.091411590576172
    },
    {
      "epoch": 0.09463414634146342,
      "step": 1746,
      "training_loss": 5.931295871734619
    },
    {
      "epoch": 0.09468834688346883,
      "step": 1747,
      "training_loss": 6.3198466300964355
    },
    {
      "epoch": 0.09474254742547425,
      "grad_norm": 12.753416061401367,
      "learning_rate": 1e-05,
      "loss": 6.9132,
      "step": 1748
    },
    {
      "epoch": 0.09474254742547425,
      "step": 1748,
      "training_loss": 7.669896125793457
    },
    {
      "epoch": 0.09479674796747968,
      "step": 1749,
      "training_loss": 7.236912250518799
    },
    {
      "epoch": 0.0948509485094851,
      "step": 1750,
      "training_loss": 4.905777454376221
    },
    {
      "epoch": 0.09490514905149051,
      "step": 1751,
      "training_loss": 6.838205814361572
    },
    {
      "epoch": 0.09495934959349593,
      "grad_norm": 17.959123611450195,
      "learning_rate": 1e-05,
      "loss": 6.6627,
      "step": 1752
    },
    {
      "epoch": 0.09495934959349593,
      "step": 1752,
      "training_loss": 6.276697635650635
    },
    {
      "epoch": 0.09501355013550135,
      "step": 1753,
      "training_loss": 6.445558071136475
    },
    {
      "epoch": 0.09506775067750678,
      "step": 1754,
      "training_loss": 7.154428005218506
    },
    {
      "epoch": 0.0951219512195122,
      "step": 1755,
      "training_loss": 5.825795650482178
    },
    {
      "epoch": 0.09517615176151761,
      "grad_norm": 21.541080474853516,
      "learning_rate": 1e-05,
      "loss": 6.4256,
      "step": 1756
    },
    {
      "epoch": 0.09517615176151761,
      "step": 1756,
      "training_loss": 7.253842353820801
    },
    {
      "epoch": 0.09523035230352303,
      "step": 1757,
      "training_loss": 7.106935024261475
    },
    {
      "epoch": 0.09528455284552846,
      "step": 1758,
      "training_loss": 5.761862277984619
    },
    {
      "epoch": 0.09533875338753388,
      "step": 1759,
      "training_loss": 6.92268180847168
    },
    {
      "epoch": 0.0953929539295393,
      "grad_norm": 16.41026496887207,
      "learning_rate": 1e-05,
      "loss": 6.7613,
      "step": 1760
    },
    {
      "epoch": 0.0953929539295393,
      "step": 1760,
      "training_loss": 6.975717067718506
    },
    {
      "epoch": 0.09544715447154471,
      "step": 1761,
      "training_loss": 6.2652740478515625
    },
    {
      "epoch": 0.09550135501355013,
      "step": 1762,
      "training_loss": 7.610599994659424
    },
    {
      "epoch": 0.09555555555555556,
      "step": 1763,
      "training_loss": 7.090829372406006
    },
    {
      "epoch": 0.09560975609756098,
      "grad_norm": 16.02631378173828,
      "learning_rate": 1e-05,
      "loss": 6.9856,
      "step": 1764
    },
    {
      "epoch": 0.09560975609756098,
      "step": 1764,
      "training_loss": 7.206759929656982
    },
    {
      "epoch": 0.0956639566395664,
      "step": 1765,
      "training_loss": 7.0517048835754395
    },
    {
      "epoch": 0.09571815718157181,
      "step": 1766,
      "training_loss": 7.74597692489624
    },
    {
      "epoch": 0.09577235772357724,
      "step": 1767,
      "training_loss": 6.3255510330200195
    },
    {
      "epoch": 0.09582655826558266,
      "grad_norm": 23.97528648376465,
      "learning_rate": 1e-05,
      "loss": 7.0825,
      "step": 1768
    },
    {
      "epoch": 0.09582655826558266,
      "step": 1768,
      "training_loss": 8.020119667053223
    },
    {
      "epoch": 0.09588075880758808,
      "step": 1769,
      "training_loss": 7.313936710357666
    },
    {
      "epoch": 0.0959349593495935,
      "step": 1770,
      "training_loss": 6.772221088409424
    },
    {
      "epoch": 0.09598915989159891,
      "step": 1771,
      "training_loss": 7.0258026123046875
    },
    {
      "epoch": 0.09604336043360434,
      "grad_norm": 17.637819290161133,
      "learning_rate": 1e-05,
      "loss": 7.283,
      "step": 1772
    },
    {
      "epoch": 0.09604336043360434,
      "step": 1772,
      "training_loss": 7.6663994789123535
    },
    {
      "epoch": 0.09609756097560976,
      "step": 1773,
      "training_loss": 7.6933112144470215
    },
    {
      "epoch": 0.09615176151761518,
      "step": 1774,
      "training_loss": 6.320790767669678
    },
    {
      "epoch": 0.09620596205962059,
      "step": 1775,
      "training_loss": 8.899639129638672
    },
    {
      "epoch": 0.09626016260162602,
      "grad_norm": 20.46413230895996,
      "learning_rate": 1e-05,
      "loss": 7.645,
      "step": 1776
    },
    {
      "epoch": 0.09626016260162602,
      "step": 1776,
      "training_loss": 7.159790515899658
    },
    {
      "epoch": 0.09631436314363144,
      "step": 1777,
      "training_loss": 7.281558990478516
    },
    {
      "epoch": 0.09636856368563686,
      "step": 1778,
      "training_loss": 6.548126697540283
    },
    {
      "epoch": 0.09642276422764227,
      "step": 1779,
      "training_loss": 7.986053466796875
    },
    {
      "epoch": 0.09647696476964769,
      "grad_norm": 24.946428298950195,
      "learning_rate": 1e-05,
      "loss": 7.2439,
      "step": 1780
    },
    {
      "epoch": 0.09647696476964769,
      "step": 1780,
      "training_loss": 7.789584636688232
    },
    {
      "epoch": 0.09653116531165312,
      "step": 1781,
      "training_loss": 7.309199810028076
    },
    {
      "epoch": 0.09658536585365854,
      "step": 1782,
      "training_loss": 7.847604274749756
    },
    {
      "epoch": 0.09663956639566396,
      "step": 1783,
      "training_loss": 5.337222576141357
    },
    {
      "epoch": 0.09669376693766937,
      "grad_norm": 17.567522048950195,
      "learning_rate": 1e-05,
      "loss": 7.0709,
      "step": 1784
    },
    {
      "epoch": 0.09669376693766937,
      "step": 1784,
      "training_loss": 7.6073126792907715
    },
    {
      "epoch": 0.09674796747967479,
      "step": 1785,
      "training_loss": 7.344090938568115
    },
    {
      "epoch": 0.09680216802168022,
      "step": 1786,
      "training_loss": 7.879390716552734
    },
    {
      "epoch": 0.09685636856368564,
      "step": 1787,
      "training_loss": 6.820295810699463
    },
    {
      "epoch": 0.09691056910569106,
      "grad_norm": 11.551276206970215,
      "learning_rate": 1e-05,
      "loss": 7.4128,
      "step": 1788
    },
    {
      "epoch": 0.09691056910569106,
      "step": 1788,
      "training_loss": 7.3769731521606445
    },
    {
      "epoch": 0.09696476964769647,
      "step": 1789,
      "training_loss": 6.810166358947754
    },
    {
      "epoch": 0.0970189701897019,
      "step": 1790,
      "training_loss": 6.943021297454834
    },
    {
      "epoch": 0.09707317073170732,
      "step": 1791,
      "training_loss": 6.193000316619873
    },
    {
      "epoch": 0.09712737127371274,
      "grad_norm": 13.75859546661377,
      "learning_rate": 1e-05,
      "loss": 6.8308,
      "step": 1792
    },
    {
      "epoch": 0.09712737127371274,
      "step": 1792,
      "training_loss": 7.501184463500977
    },
    {
      "epoch": 0.09718157181571815,
      "step": 1793,
      "training_loss": 6.744534015655518
    },
    {
      "epoch": 0.09723577235772357,
      "step": 1794,
      "training_loss": 8.088337898254395
    },
    {
      "epoch": 0.097289972899729,
      "step": 1795,
      "training_loss": 4.722229957580566
    },
    {
      "epoch": 0.09734417344173442,
      "grad_norm": 16.40852928161621,
      "learning_rate": 1e-05,
      "loss": 6.7641,
      "step": 1796
    },
    {
      "epoch": 0.09734417344173442,
      "step": 1796,
      "training_loss": 5.999045372009277
    },
    {
      "epoch": 0.09739837398373984,
      "step": 1797,
      "training_loss": 7.9346771240234375
    },
    {
      "epoch": 0.09745257452574525,
      "step": 1798,
      "training_loss": 4.874082565307617
    },
    {
      "epoch": 0.09750677506775068,
      "step": 1799,
      "training_loss": 7.72590970993042
    },
    {
      "epoch": 0.0975609756097561,
      "grad_norm": 17.744157791137695,
      "learning_rate": 1e-05,
      "loss": 6.6334,
      "step": 1800
    },
    {
      "epoch": 0.0975609756097561,
      "step": 1800,
      "training_loss": 6.864748954772949
    },
    {
      "epoch": 0.09761517615176152,
      "step": 1801,
      "training_loss": 7.550734996795654
    },
    {
      "epoch": 0.09766937669376693,
      "step": 1802,
      "training_loss": 6.66982889175415
    },
    {
      "epoch": 0.09772357723577235,
      "step": 1803,
      "training_loss": 8.060007095336914
    },
    {
      "epoch": 0.09777777777777778,
      "grad_norm": 28.389541625976562,
      "learning_rate": 1e-05,
      "loss": 7.2863,
      "step": 1804
    },
    {
      "epoch": 0.09777777777777778,
      "step": 1804,
      "training_loss": 7.176174163818359
    },
    {
      "epoch": 0.0978319783197832,
      "step": 1805,
      "training_loss": 7.384130001068115
    },
    {
      "epoch": 0.09788617886178862,
      "step": 1806,
      "training_loss": 7.144604682922363
    },
    {
      "epoch": 0.09794037940379403,
      "step": 1807,
      "training_loss": 6.732770919799805
    },
    {
      "epoch": 0.09799457994579946,
      "grad_norm": 20.322484970092773,
      "learning_rate": 1e-05,
      "loss": 7.1094,
      "step": 1808
    },
    {
      "epoch": 0.09799457994579946,
      "step": 1808,
      "training_loss": 7.012442111968994
    },
    {
      "epoch": 0.09804878048780488,
      "step": 1809,
      "training_loss": 7.252750873565674
    },
    {
      "epoch": 0.0981029810298103,
      "step": 1810,
      "training_loss": 7.621109485626221
    },
    {
      "epoch": 0.09815718157181572,
      "step": 1811,
      "training_loss": 7.669775009155273
    },
    {
      "epoch": 0.09821138211382113,
      "grad_norm": 20.952239990234375,
      "learning_rate": 1e-05,
      "loss": 7.389,
      "step": 1812
    },
    {
      "epoch": 0.09821138211382113,
      "step": 1812,
      "training_loss": 6.4778947830200195
    },
    {
      "epoch": 0.09826558265582656,
      "step": 1813,
      "training_loss": 7.008749008178711
    },
    {
      "epoch": 0.09831978319783198,
      "step": 1814,
      "training_loss": 4.594441890716553
    },
    {
      "epoch": 0.0983739837398374,
      "step": 1815,
      "training_loss": 6.74560546875
    },
    {
      "epoch": 0.09842818428184281,
      "grad_norm": 23.24747657775879,
      "learning_rate": 1e-05,
      "loss": 6.2067,
      "step": 1816
    },
    {
      "epoch": 0.09842818428184281,
      "step": 1816,
      "training_loss": 7.7325758934021
    },
    {
      "epoch": 0.09848238482384823,
      "step": 1817,
      "training_loss": 6.696701526641846
    },
    {
      "epoch": 0.09853658536585366,
      "step": 1818,
      "training_loss": 4.698680400848389
    },
    {
      "epoch": 0.09859078590785908,
      "step": 1819,
      "training_loss": 6.987692832946777
    },
    {
      "epoch": 0.0986449864498645,
      "grad_norm": 15.378469467163086,
      "learning_rate": 1e-05,
      "loss": 6.5289,
      "step": 1820
    },
    {
      "epoch": 0.0986449864498645,
      "step": 1820,
      "training_loss": 7.661124229431152
    },
    {
      "epoch": 0.09869918699186991,
      "step": 1821,
      "training_loss": 6.872621536254883
    },
    {
      "epoch": 0.09875338753387534,
      "step": 1822,
      "training_loss": 8.3343505859375
    },
    {
      "epoch": 0.09880758807588076,
      "step": 1823,
      "training_loss": 7.249980926513672
    },
    {
      "epoch": 0.09886178861788618,
      "grad_norm": 19.45137596130371,
      "learning_rate": 1e-05,
      "loss": 7.5295,
      "step": 1824
    },
    {
      "epoch": 0.09886178861788618,
      "step": 1824,
      "training_loss": 7.07426118850708
    },
    {
      "epoch": 0.0989159891598916,
      "step": 1825,
      "training_loss": 6.6371541023254395
    },
    {
      "epoch": 0.09897018970189701,
      "step": 1826,
      "training_loss": 6.09772253036499
    },
    {
      "epoch": 0.09902439024390244,
      "step": 1827,
      "training_loss": 7.62018346786499
    },
    {
      "epoch": 0.09907859078590786,
      "grad_norm": 19.541664123535156,
      "learning_rate": 1e-05,
      "loss": 6.8573,
      "step": 1828
    },
    {
      "epoch": 0.09907859078590786,
      "step": 1828,
      "training_loss": 7.037596225738525
    },
    {
      "epoch": 0.09913279132791328,
      "step": 1829,
      "training_loss": 5.411575794219971
    },
    {
      "epoch": 0.0991869918699187,
      "step": 1830,
      "training_loss": 5.918088436126709
    },
    {
      "epoch": 0.09924119241192413,
      "step": 1831,
      "training_loss": 6.0435404777526855
    },
    {
      "epoch": 0.09929539295392954,
      "grad_norm": 32.72734451293945,
      "learning_rate": 1e-05,
      "loss": 6.1027,
      "step": 1832
    },
    {
      "epoch": 0.09929539295392954,
      "step": 1832,
      "training_loss": 6.31008243560791
    },
    {
      "epoch": 0.09934959349593496,
      "step": 1833,
      "training_loss": 7.640936374664307
    },
    {
      "epoch": 0.09940379403794038,
      "step": 1834,
      "training_loss": 7.680691242218018
    },
    {
      "epoch": 0.0994579945799458,
      "step": 1835,
      "training_loss": 6.665595531463623
    },
    {
      "epoch": 0.09951219512195122,
      "grad_norm": 14.271285057067871,
      "learning_rate": 1e-05,
      "loss": 7.0743,
      "step": 1836
    },
    {
      "epoch": 0.09951219512195122,
      "step": 1836,
      "training_loss": 5.690708637237549
    },
    {
      "epoch": 0.09956639566395664,
      "step": 1837,
      "training_loss": 8.010293960571289
    },
    {
      "epoch": 0.09962059620596206,
      "step": 1838,
      "training_loss": 5.154156684875488
    },
    {
      "epoch": 0.09967479674796748,
      "step": 1839,
      "training_loss": 7.071045398712158
    },
    {
      "epoch": 0.0997289972899729,
      "grad_norm": 12.849090576171875,
      "learning_rate": 1e-05,
      "loss": 6.4816,
      "step": 1840
    },
    {
      "epoch": 0.0997289972899729,
      "step": 1840,
      "training_loss": 7.014519214630127
    },
    {
      "epoch": 0.09978319783197832,
      "step": 1841,
      "training_loss": 7.2970380783081055
    },
    {
      "epoch": 0.09983739837398374,
      "step": 1842,
      "training_loss": 6.379818916320801
    },
    {
      "epoch": 0.09989159891598916,
      "step": 1843,
      "training_loss": 5.890921592712402
    },
    {
      "epoch": 0.09994579945799457,
      "grad_norm": 22.724231719970703,
      "learning_rate": 1e-05,
      "loss": 6.6456,
      "step": 1844
    },
    {
      "epoch": 0.09994579945799457,
      "step": 1844,
      "training_loss": 6.573121070861816
    },
    {
      "epoch": 0.1,
      "step": 1845,
      "training_loss": 6.535424709320068
    },
    {
      "epoch": 0.10005420054200542,
      "step": 1846,
      "training_loss": 6.750287055969238
    },
    {
      "epoch": 0.10010840108401084,
      "step": 1847,
      "training_loss": 6.688355922698975
    },
    {
      "epoch": 0.10016260162601626,
      "grad_norm": 20.58989143371582,
      "learning_rate": 1e-05,
      "loss": 6.6368,
      "step": 1848
    },
    {
      "epoch": 0.10016260162601626,
      "step": 1848,
      "training_loss": 7.169182777404785
    },
    {
      "epoch": 0.10021680216802167,
      "step": 1849,
      "training_loss": 6.872289657592773
    },
    {
      "epoch": 0.1002710027100271,
      "step": 1850,
      "training_loss": 6.915380001068115
    },
    {
      "epoch": 0.10032520325203252,
      "step": 1851,
      "training_loss": 9.504040718078613
    },
    {
      "epoch": 0.10037940379403794,
      "grad_norm": 56.86107635498047,
      "learning_rate": 1e-05,
      "loss": 7.6152,
      "step": 1852
    },
    {
      "epoch": 0.10037940379403794,
      "step": 1852,
      "training_loss": 6.494965553283691
    },
    {
      "epoch": 0.10043360433604336,
      "step": 1853,
      "training_loss": 6.6870951652526855
    },
    {
      "epoch": 0.10048780487804879,
      "step": 1854,
      "training_loss": 8.392060279846191
    },
    {
      "epoch": 0.1005420054200542,
      "step": 1855,
      "training_loss": 7.3568434715271
    },
    {
      "epoch": 0.10059620596205962,
      "grad_norm": 49.58441925048828,
      "learning_rate": 1e-05,
      "loss": 7.2327,
      "step": 1856
    },
    {
      "epoch": 0.10059620596205962,
      "step": 1856,
      "training_loss": 6.857938289642334
    },
    {
      "epoch": 0.10065040650406504,
      "step": 1857,
      "training_loss": 7.114043235778809
    },
    {
      "epoch": 0.10070460704607045,
      "step": 1858,
      "training_loss": 7.980775833129883
    },
    {
      "epoch": 0.10075880758807589,
      "step": 1859,
      "training_loss": 7.852813720703125
    },
    {
      "epoch": 0.1008130081300813,
      "grad_norm": 53.052452087402344,
      "learning_rate": 1e-05,
      "loss": 7.4514,
      "step": 1860
    },
    {
      "epoch": 0.1008130081300813,
      "step": 1860,
      "training_loss": 7.443462371826172
    },
    {
      "epoch": 0.10086720867208672,
      "step": 1861,
      "training_loss": 6.19350004196167
    },
    {
      "epoch": 0.10092140921409214,
      "step": 1862,
      "training_loss": 7.3889031410217285
    },
    {
      "epoch": 0.10097560975609757,
      "step": 1863,
      "training_loss": 7.585278511047363
    },
    {
      "epoch": 0.10102981029810298,
      "grad_norm": 27.88114356994629,
      "learning_rate": 1e-05,
      "loss": 7.1528,
      "step": 1864
    },
    {
      "epoch": 0.10102981029810298,
      "step": 1864,
      "training_loss": 8.452298164367676
    },
    {
      "epoch": 0.1010840108401084,
      "step": 1865,
      "training_loss": 5.742522716522217
    },
    {
      "epoch": 0.10113821138211382,
      "step": 1866,
      "training_loss": 7.657788276672363
    },
    {
      "epoch": 0.10119241192411924,
      "step": 1867,
      "training_loss": 7.492915630340576
    },
    {
      "epoch": 0.10124661246612467,
      "grad_norm": 18.857528686523438,
      "learning_rate": 1e-05,
      "loss": 7.3364,
      "step": 1868
    },
    {
      "epoch": 0.10124661246612467,
      "step": 1868,
      "training_loss": 5.594577312469482
    },
    {
      "epoch": 0.10130081300813008,
      "step": 1869,
      "training_loss": 8.023194313049316
    },
    {
      "epoch": 0.1013550135501355,
      "step": 1870,
      "training_loss": 6.285765171051025
    },
    {
      "epoch": 0.10140921409214092,
      "step": 1871,
      "training_loss": 7.735846996307373
    },
    {
      "epoch": 0.10146341463414635,
      "grad_norm": 23.132484436035156,
      "learning_rate": 1e-05,
      "loss": 6.9098,
      "step": 1872
    },
    {
      "epoch": 0.10146341463414635,
      "step": 1872,
      "training_loss": 8.006044387817383
    },
    {
      "epoch": 0.10151761517615177,
      "step": 1873,
      "training_loss": 6.534153938293457
    },
    {
      "epoch": 0.10157181571815718,
      "step": 1874,
      "training_loss": 6.306844711303711
    },
    {
      "epoch": 0.1016260162601626,
      "step": 1875,
      "training_loss": 6.963500022888184
    },
    {
      "epoch": 0.10168021680216802,
      "grad_norm": 14.774385452270508,
      "learning_rate": 1e-05,
      "loss": 6.9526,
      "step": 1876
    },
    {
      "epoch": 0.10168021680216802,
      "step": 1876,
      "training_loss": 8.16310977935791
    },
    {
      "epoch": 0.10173441734417345,
      "step": 1877,
      "training_loss": 6.408857345581055
    },
    {
      "epoch": 0.10178861788617886,
      "step": 1878,
      "training_loss": 7.996594429016113
    },
    {
      "epoch": 0.10184281842818428,
      "step": 1879,
      "training_loss": 6.072828769683838
    },
    {
      "epoch": 0.1018970189701897,
      "grad_norm": 14.568513870239258,
      "learning_rate": 1e-05,
      "loss": 7.1603,
      "step": 1880
    },
    {
      "epoch": 0.1018970189701897,
      "step": 1880,
      "training_loss": 6.176990509033203
    },
    {
      "epoch": 0.10195121951219512,
      "step": 1881,
      "training_loss": 7.096137046813965
    },
    {
      "epoch": 0.10200542005420055,
      "step": 1882,
      "training_loss": 7.603650093078613
    },
    {
      "epoch": 0.10205962059620596,
      "step": 1883,
      "training_loss": 7.451887607574463
    },
    {
      "epoch": 0.10211382113821138,
      "grad_norm": 23.526025772094727,
      "learning_rate": 1e-05,
      "loss": 7.0822,
      "step": 1884
    },
    {
      "epoch": 0.10211382113821138,
      "step": 1884,
      "training_loss": 6.8904709815979
    },
    {
      "epoch": 0.1021680216802168,
      "step": 1885,
      "training_loss": 8.41188907623291
    },
    {
      "epoch": 0.10222222222222223,
      "step": 1886,
      "training_loss": 7.375153064727783
    },
    {
      "epoch": 0.10227642276422765,
      "step": 1887,
      "training_loss": 7.26914644241333
    },
    {
      "epoch": 0.10233062330623306,
      "grad_norm": 21.91248321533203,
      "learning_rate": 1e-05,
      "loss": 7.4867,
      "step": 1888
    },
    {
      "epoch": 0.10233062330623306,
      "step": 1888,
      "training_loss": 7.367809772491455
    },
    {
      "epoch": 0.10238482384823848,
      "step": 1889,
      "training_loss": 7.01220703125
    },
    {
      "epoch": 0.1024390243902439,
      "step": 1890,
      "training_loss": 5.942544460296631
    },
    {
      "epoch": 0.10249322493224933,
      "step": 1891,
      "training_loss": 7.407491207122803
    },
    {
      "epoch": 0.10254742547425474,
      "grad_norm": 23.44442367553711,
      "learning_rate": 1e-05,
      "loss": 6.9325,
      "step": 1892
    },
    {
      "epoch": 0.10254742547425474,
      "step": 1892,
      "training_loss": 6.786383628845215
    },
    {
      "epoch": 0.10260162601626016,
      "step": 1893,
      "training_loss": 7.513130187988281
    },
    {
      "epoch": 0.10265582655826558,
      "step": 1894,
      "training_loss": 8.256592750549316
    },
    {
      "epoch": 0.10271002710027101,
      "step": 1895,
      "training_loss": 7.2881760597229
    },
    {
      "epoch": 0.10276422764227643,
      "grad_norm": 32.483028411865234,
      "learning_rate": 1e-05,
      "loss": 7.4611,
      "step": 1896
    },
    {
      "epoch": 0.10276422764227643,
      "step": 1896,
      "training_loss": 4.818662166595459
    },
    {
      "epoch": 0.10281842818428184,
      "step": 1897,
      "training_loss": 6.713764667510986
    },
    {
      "epoch": 0.10287262872628726,
      "step": 1898,
      "training_loss": 6.064594268798828
    },
    {
      "epoch": 0.10292682926829268,
      "step": 1899,
      "training_loss": 8.250630378723145
    },
    {
      "epoch": 0.10298102981029811,
      "grad_norm": 29.14861297607422,
      "learning_rate": 1e-05,
      "loss": 6.4619,
      "step": 1900
    },
    {
      "epoch": 0.10298102981029811,
      "step": 1900,
      "training_loss": 5.687113285064697
    },
    {
      "epoch": 0.10303523035230353,
      "step": 1901,
      "training_loss": 7.016377925872803
    },
    {
      "epoch": 0.10308943089430894,
      "step": 1902,
      "training_loss": 7.7814531326293945
    },
    {
      "epoch": 0.10314363143631436,
      "step": 1903,
      "training_loss": 7.398102760314941
    },
    {
      "epoch": 0.10319783197831979,
      "grad_norm": 27.779050827026367,
      "learning_rate": 1e-05,
      "loss": 6.9708,
      "step": 1904
    },
    {
      "epoch": 0.10319783197831979,
      "step": 1904,
      "training_loss": 6.42487907409668
    },
    {
      "epoch": 0.10325203252032521,
      "step": 1905,
      "training_loss": 7.301225185394287
    },
    {
      "epoch": 0.10330623306233062,
      "step": 1906,
      "training_loss": 7.408722400665283
    },
    {
      "epoch": 0.10336043360433604,
      "step": 1907,
      "training_loss": 7.125107765197754
    },
    {
      "epoch": 0.10341463414634146,
      "grad_norm": 18.092239379882812,
      "learning_rate": 1e-05,
      "loss": 7.065,
      "step": 1908
    },
    {
      "epoch": 0.10341463414634146,
      "step": 1908,
      "training_loss": 7.588310241699219
    },
    {
      "epoch": 0.10346883468834689,
      "step": 1909,
      "training_loss": 7.085721015930176
    },
    {
      "epoch": 0.1035230352303523,
      "step": 1910,
      "training_loss": 7.9064435958862305
    },
    {
      "epoch": 0.10357723577235772,
      "step": 1911,
      "training_loss": 8.664698600769043
    },
    {
      "epoch": 0.10363143631436314,
      "grad_norm": 32.35499954223633,
      "learning_rate": 1e-05,
      "loss": 7.8113,
      "step": 1912
    },
    {
      "epoch": 0.10363143631436314,
      "step": 1912,
      "training_loss": 7.165069103240967
    },
    {
      "epoch": 0.10368563685636856,
      "step": 1913,
      "training_loss": 6.031279563903809
    },
    {
      "epoch": 0.10373983739837399,
      "step": 1914,
      "training_loss": 8.030135154724121
    },
    {
      "epoch": 0.1037940379403794,
      "step": 1915,
      "training_loss": 6.911118030548096
    },
    {
      "epoch": 0.10384823848238482,
      "grad_norm": 23.252477645874023,
      "learning_rate": 1e-05,
      "loss": 7.0344,
      "step": 1916
    },
    {
      "epoch": 0.10384823848238482,
      "step": 1916,
      "training_loss": 6.886211395263672
    },
    {
      "epoch": 0.10390243902439024,
      "step": 1917,
      "training_loss": 7.567008018493652
    },
    {
      "epoch": 0.10395663956639567,
      "step": 1918,
      "training_loss": 6.618258476257324
    },
    {
      "epoch": 0.10401084010840109,
      "step": 1919,
      "training_loss": 6.368618965148926
    },
    {
      "epoch": 0.1040650406504065,
      "grad_norm": 34.3057746887207,
      "learning_rate": 1e-05,
      "loss": 6.86,
      "step": 1920
    },
    {
      "epoch": 0.1040650406504065,
      "step": 1920,
      "training_loss": 5.036281108856201
    },
    {
      "epoch": 0.10411924119241192,
      "step": 1921,
      "training_loss": 7.437923431396484
    },
    {
      "epoch": 0.10417344173441734,
      "step": 1922,
      "training_loss": 5.69644832611084
    },
    {
      "epoch": 0.10422764227642277,
      "step": 1923,
      "training_loss": 7.384374618530273
    },
    {
      "epoch": 0.10428184281842819,
      "grad_norm": 25.513246536254883,
      "learning_rate": 1e-05,
      "loss": 6.3888,
      "step": 1924
    },
    {
      "epoch": 0.10428184281842819,
      "step": 1924,
      "training_loss": 7.976056098937988
    },
    {
      "epoch": 0.1043360433604336,
      "step": 1925,
      "training_loss": 7.0879740715026855
    },
    {
      "epoch": 0.10439024390243902,
      "step": 1926,
      "training_loss": 5.618669033050537
    },
    {
      "epoch": 0.10444444444444445,
      "step": 1927,
      "training_loss": 7.607583045959473
    },
    {
      "epoch": 0.10449864498644987,
      "grad_norm": 19.35965347290039,
      "learning_rate": 1e-05,
      "loss": 7.0726,
      "step": 1928
    },
    {
      "epoch": 0.10449864498644987,
      "step": 1928,
      "training_loss": 6.183177471160889
    },
    {
      "epoch": 0.10455284552845528,
      "step": 1929,
      "training_loss": 7.9468793869018555
    },
    {
      "epoch": 0.1046070460704607,
      "step": 1930,
      "training_loss": 6.587648868560791
    },
    {
      "epoch": 0.10466124661246612,
      "step": 1931,
      "training_loss": 5.988985061645508
    },
    {
      "epoch": 0.10471544715447155,
      "grad_norm": 15.967059135437012,
      "learning_rate": 1e-05,
      "loss": 6.6767,
      "step": 1932
    },
    {
      "epoch": 0.10471544715447155,
      "step": 1932,
      "training_loss": 6.778229236602783
    },
    {
      "epoch": 0.10476964769647697,
      "step": 1933,
      "training_loss": 7.788546562194824
    },
    {
      "epoch": 0.10482384823848238,
      "step": 1934,
      "training_loss": 5.470512866973877
    },
    {
      "epoch": 0.1048780487804878,
      "step": 1935,
      "training_loss": 7.287130832672119
    },
    {
      "epoch": 0.10493224932249323,
      "grad_norm": 18.69687271118164,
      "learning_rate": 1e-05,
      "loss": 6.8311,
      "step": 1936
    },
    {
      "epoch": 0.10493224932249323,
      "step": 1936,
      "training_loss": 7.2460432052612305
    },
    {
      "epoch": 0.10498644986449865,
      "step": 1937,
      "training_loss": 5.408318519592285
    },
    {
      "epoch": 0.10504065040650407,
      "step": 1938,
      "training_loss": 7.379900932312012
    },
    {
      "epoch": 0.10509485094850948,
      "step": 1939,
      "training_loss": 7.259235858917236
    },
    {
      "epoch": 0.1051490514905149,
      "grad_norm": 19.686471939086914,
      "learning_rate": 1e-05,
      "loss": 6.8234,
      "step": 1940
    },
    {
      "epoch": 0.1051490514905149,
      "step": 1940,
      "training_loss": 7.323550701141357
    },
    {
      "epoch": 0.10520325203252033,
      "step": 1941,
      "training_loss": 7.5910115242004395
    },
    {
      "epoch": 0.10525745257452575,
      "step": 1942,
      "training_loss": 6.2993879318237305
    },
    {
      "epoch": 0.10531165311653116,
      "step": 1943,
      "training_loss": 7.883515357971191
    },
    {
      "epoch": 0.10536585365853658,
      "grad_norm": 32.84680938720703,
      "learning_rate": 1e-05,
      "loss": 7.2744,
      "step": 1944
    },
    {
      "epoch": 0.10536585365853658,
      "step": 1944,
      "training_loss": 7.150788307189941
    },
    {
      "epoch": 0.105420054200542,
      "step": 1945,
      "training_loss": 6.813632965087891
    },
    {
      "epoch": 0.10547425474254743,
      "step": 1946,
      "training_loss": 6.969394207000732
    },
    {
      "epoch": 0.10552845528455285,
      "step": 1947,
      "training_loss": 7.042242527008057
    },
    {
      "epoch": 0.10558265582655826,
      "grad_norm": 14.737787246704102,
      "learning_rate": 1e-05,
      "loss": 6.994,
      "step": 1948
    },
    {
      "epoch": 0.10558265582655826,
      "step": 1948,
      "training_loss": 6.207190990447998
    },
    {
      "epoch": 0.10563685636856368,
      "step": 1949,
      "training_loss": 7.253708362579346
    },
    {
      "epoch": 0.10569105691056911,
      "step": 1950,
      "training_loss": 7.475819110870361
    },
    {
      "epoch": 0.10574525745257453,
      "step": 1951,
      "training_loss": 6.559276580810547
    },
    {
      "epoch": 0.10579945799457995,
      "grad_norm": 13.06650447845459,
      "learning_rate": 1e-05,
      "loss": 6.874,
      "step": 1952
    },
    {
      "epoch": 0.10579945799457995,
      "step": 1952,
      "training_loss": 7.1003923416137695
    },
    {
      "epoch": 0.10585365853658536,
      "step": 1953,
      "training_loss": 7.40924596786499
    },
    {
      "epoch": 0.10590785907859078,
      "step": 1954,
      "training_loss": 7.4768805503845215
    },
    {
      "epoch": 0.10596205962059621,
      "step": 1955,
      "training_loss": 6.6013569831848145
    },
    {
      "epoch": 0.10601626016260163,
      "grad_norm": 17.387710571289062,
      "learning_rate": 1e-05,
      "loss": 7.147,
      "step": 1956
    },
    {
      "epoch": 0.10601626016260163,
      "step": 1956,
      "training_loss": 6.421169281005859
    },
    {
      "epoch": 0.10607046070460704,
      "step": 1957,
      "training_loss": 7.327566623687744
    },
    {
      "epoch": 0.10612466124661246,
      "step": 1958,
      "training_loss": 7.779445171356201
    },
    {
      "epoch": 0.10617886178861789,
      "step": 1959,
      "training_loss": 6.880326271057129
    },
    {
      "epoch": 0.10623306233062331,
      "grad_norm": 15.404858589172363,
      "learning_rate": 1e-05,
      "loss": 7.1021,
      "step": 1960
    },
    {
      "epoch": 0.10623306233062331,
      "step": 1960,
      "training_loss": 6.2346577644348145
    },
    {
      "epoch": 0.10628726287262873,
      "step": 1961,
      "training_loss": 8.373960494995117
    },
    {
      "epoch": 0.10634146341463414,
      "step": 1962,
      "training_loss": 7.205341339111328
    },
    {
      "epoch": 0.10639566395663956,
      "step": 1963,
      "training_loss": 9.370823860168457
    },
    {
      "epoch": 0.10644986449864499,
      "grad_norm": 27.95480728149414,
      "learning_rate": 1e-05,
      "loss": 7.7962,
      "step": 1964
    },
    {
      "epoch": 0.10644986449864499,
      "step": 1964,
      "training_loss": 6.605642795562744
    },
    {
      "epoch": 0.10650406504065041,
      "step": 1965,
      "training_loss": 6.541418075561523
    },
    {
      "epoch": 0.10655826558265583,
      "step": 1966,
      "training_loss": 8.00596809387207
    },
    {
      "epoch": 0.10661246612466124,
      "step": 1967,
      "training_loss": 7.321264743804932
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 19.02974510192871,
      "learning_rate": 1e-05,
      "loss": 7.1186,
      "step": 1968
    },
    {
      "epoch": 0.10666666666666667,
      "step": 1968,
      "training_loss": 6.069786071777344
    },
    {
      "epoch": 0.10672086720867209,
      "step": 1969,
      "training_loss": 9.089376449584961
    },
    {
      "epoch": 0.10677506775067751,
      "step": 1970,
      "training_loss": 6.151271820068359
    },
    {
      "epoch": 0.10682926829268292,
      "step": 1971,
      "training_loss": 6.22847843170166
    },
    {
      "epoch": 0.10688346883468834,
      "grad_norm": 15.762473106384277,
      "learning_rate": 1e-05,
      "loss": 6.8847,
      "step": 1972
    },
    {
      "epoch": 0.10688346883468834,
      "step": 1972,
      "training_loss": 6.670057773590088
    },
    {
      "epoch": 0.10693766937669377,
      "step": 1973,
      "training_loss": 8.368659019470215
    },
    {
      "epoch": 0.10699186991869919,
      "step": 1974,
      "training_loss": 6.4137115478515625
    },
    {
      "epoch": 0.1070460704607046,
      "step": 1975,
      "training_loss": 7.357468128204346
    },
    {
      "epoch": 0.10710027100271002,
      "grad_norm": 15.742369651794434,
      "learning_rate": 1e-05,
      "loss": 7.2025,
      "step": 1976
    },
    {
      "epoch": 0.10710027100271002,
      "step": 1976,
      "training_loss": 6.975642681121826
    },
    {
      "epoch": 0.10715447154471544,
      "step": 1977,
      "training_loss": 5.945455074310303
    },
    {
      "epoch": 0.10720867208672087,
      "step": 1978,
      "training_loss": 6.926813125610352
    },
    {
      "epoch": 0.10726287262872629,
      "step": 1979,
      "training_loss": 7.6792144775390625
    },
    {
      "epoch": 0.1073170731707317,
      "grad_norm": 18.588665008544922,
      "learning_rate": 1e-05,
      "loss": 6.8818,
      "step": 1980
    },
    {
      "epoch": 0.1073170731707317,
      "step": 1980,
      "training_loss": 6.699853897094727
    },
    {
      "epoch": 0.10737127371273712,
      "step": 1981,
      "training_loss": 6.843050003051758
    },
    {
      "epoch": 0.10742547425474255,
      "step": 1982,
      "training_loss": 6.9632134437561035
    },
    {
      "epoch": 0.10747967479674797,
      "step": 1983,
      "training_loss": 7.026053428649902
    },
    {
      "epoch": 0.10753387533875339,
      "grad_norm": 14.146771430969238,
      "learning_rate": 1e-05,
      "loss": 6.883,
      "step": 1984
    },
    {
      "epoch": 0.10753387533875339,
      "step": 1984,
      "training_loss": 6.589237689971924
    },
    {
      "epoch": 0.1075880758807588,
      "step": 1985,
      "training_loss": 7.369289875030518
    },
    {
      "epoch": 0.10764227642276422,
      "step": 1986,
      "training_loss": 7.209839820861816
    },
    {
      "epoch": 0.10769647696476965,
      "step": 1987,
      "training_loss": 5.73673677444458
    },
    {
      "epoch": 0.10775067750677507,
      "grad_norm": 22.378562927246094,
      "learning_rate": 1e-05,
      "loss": 6.7263,
      "step": 1988
    },
    {
      "epoch": 0.10775067750677507,
      "step": 1988,
      "training_loss": 5.998231887817383
    },
    {
      "epoch": 0.10780487804878049,
      "step": 1989,
      "training_loss": 8.2850341796875
    },
    {
      "epoch": 0.1078590785907859,
      "step": 1990,
      "training_loss": 6.635718822479248
    },
    {
      "epoch": 0.10791327913279133,
      "step": 1991,
      "training_loss": 6.44923734664917
    },
    {
      "epoch": 0.10796747967479675,
      "grad_norm": 19.15929412841797,
      "learning_rate": 1e-05,
      "loss": 6.8421,
      "step": 1992
    },
    {
      "epoch": 0.10796747967479675,
      "step": 1992,
      "training_loss": 6.53092098236084
    },
    {
      "epoch": 0.10802168021680217,
      "step": 1993,
      "training_loss": 5.64456033706665
    },
    {
      "epoch": 0.10807588075880759,
      "step": 1994,
      "training_loss": 7.11775541305542
    },
    {
      "epoch": 0.108130081300813,
      "step": 1995,
      "training_loss": 7.091853618621826
    },
    {
      "epoch": 0.10818428184281843,
      "grad_norm": 12.573721885681152,
      "learning_rate": 1e-05,
      "loss": 6.5963,
      "step": 1996
    },
    {
      "epoch": 0.10818428184281843,
      "step": 1996,
      "training_loss": 8.003024101257324
    },
    {
      "epoch": 0.10823848238482385,
      "step": 1997,
      "training_loss": 7.554884433746338
    },
    {
      "epoch": 0.10829268292682927,
      "step": 1998,
      "training_loss": 6.000514507293701
    },
    {
      "epoch": 0.10834688346883468,
      "step": 1999,
      "training_loss": 7.263428688049316
    },
    {
      "epoch": 0.10840108401084012,
      "grad_norm": 29.47939682006836,
      "learning_rate": 1e-05,
      "loss": 7.2055,
      "step": 2000
    },
    {
      "epoch": 0.10840108401084012,
      "step": 2000,
      "training_loss": 4.917733669281006
    },
    {
      "epoch": 0.10845528455284553,
      "step": 2001,
      "training_loss": 5.301447868347168
    },
    {
      "epoch": 0.10850948509485095,
      "step": 2002,
      "training_loss": 7.338685035705566
    },
    {
      "epoch": 0.10856368563685637,
      "step": 2003,
      "training_loss": 6.105548858642578
    },
    {
      "epoch": 0.10861788617886178,
      "grad_norm": 38.752967834472656,
      "learning_rate": 1e-05,
      "loss": 5.9159,
      "step": 2004
    },
    {
      "epoch": 0.10861788617886178,
      "step": 2004,
      "training_loss": 7.486490249633789
    },
    {
      "epoch": 0.10867208672086721,
      "step": 2005,
      "training_loss": 6.366743087768555
    },
    {
      "epoch": 0.10872628726287263,
      "step": 2006,
      "training_loss": 7.139036178588867
    },
    {
      "epoch": 0.10878048780487805,
      "step": 2007,
      "training_loss": 7.025553226470947
    },
    {
      "epoch": 0.10883468834688347,
      "grad_norm": 20.09546661376953,
      "learning_rate": 1e-05,
      "loss": 7.0045,
      "step": 2008
    },
    {
      "epoch": 0.10883468834688347,
      "step": 2008,
      "training_loss": 7.152218818664551
    },
    {
      "epoch": 0.10888888888888888,
      "step": 2009,
      "training_loss": 7.274974822998047
    },
    {
      "epoch": 0.10894308943089431,
      "step": 2010,
      "training_loss": 5.628118991851807
    },
    {
      "epoch": 0.10899728997289973,
      "step": 2011,
      "training_loss": 5.626953125
    },
    {
      "epoch": 0.10905149051490515,
      "grad_norm": 21.96625518798828,
      "learning_rate": 1e-05,
      "loss": 6.4206,
      "step": 2012
    },
    {
      "epoch": 0.10905149051490515,
      "step": 2012,
      "training_loss": 7.25082540512085
    },
    {
      "epoch": 0.10910569105691056,
      "step": 2013,
      "training_loss": 7.4336066246032715
    },
    {
      "epoch": 0.109159891598916,
      "step": 2014,
      "training_loss": 7.142324447631836
    },
    {
      "epoch": 0.10921409214092141,
      "step": 2015,
      "training_loss": 6.022584915161133
    },
    {
      "epoch": 0.10926829268292683,
      "grad_norm": 18.266977310180664,
      "learning_rate": 1e-05,
      "loss": 6.9623,
      "step": 2016
    },
    {
      "epoch": 0.10926829268292683,
      "step": 2016,
      "training_loss": 6.441844463348389
    },
    {
      "epoch": 0.10932249322493225,
      "step": 2017,
      "training_loss": 7.817961692810059
    },
    {
      "epoch": 0.10937669376693766,
      "step": 2018,
      "training_loss": 5.661920070648193
    },
    {
      "epoch": 0.1094308943089431,
      "step": 2019,
      "training_loss": 6.9994378089904785
    },
    {
      "epoch": 0.10948509485094851,
      "grad_norm": 32.17500686645508,
      "learning_rate": 1e-05,
      "loss": 6.7303,
      "step": 2020
    },
    {
      "epoch": 0.10948509485094851,
      "step": 2020,
      "training_loss": 5.38447904586792
    },
    {
      "epoch": 0.10953929539295393,
      "step": 2021,
      "training_loss": 8.099637031555176
    },
    {
      "epoch": 0.10959349593495935,
      "step": 2022,
      "training_loss": 6.31291389465332
    },
    {
      "epoch": 0.10964769647696478,
      "step": 2023,
      "training_loss": 7.197179794311523
    },
    {
      "epoch": 0.1097018970189702,
      "grad_norm": 17.256973266601562,
      "learning_rate": 1e-05,
      "loss": 6.7486,
      "step": 2024
    },
    {
      "epoch": 0.1097018970189702,
      "step": 2024,
      "training_loss": 6.13470983505249
    },
    {
      "epoch": 0.10975609756097561,
      "step": 2025,
      "training_loss": 7.179380893707275
    },
    {
      "epoch": 0.10981029810298103,
      "step": 2026,
      "training_loss": 6.916756629943848
    },
    {
      "epoch": 0.10986449864498644,
      "step": 2027,
      "training_loss": 7.053202152252197
    },
    {
      "epoch": 0.10991869918699188,
      "grad_norm": 23.984786987304688,
      "learning_rate": 1e-05,
      "loss": 6.821,
      "step": 2028
    },
    {
      "epoch": 0.10991869918699188,
      "step": 2028,
      "training_loss": 7.695729732513428
    },
    {
      "epoch": 0.10997289972899729,
      "step": 2029,
      "training_loss": 7.657358646392822
    },
    {
      "epoch": 0.11002710027100271,
      "step": 2030,
      "training_loss": 6.535762786865234
    },
    {
      "epoch": 0.11008130081300813,
      "step": 2031,
      "training_loss": 6.6994500160217285
    },
    {
      "epoch": 0.11013550135501356,
      "grad_norm": 23.21292495727539,
      "learning_rate": 1e-05,
      "loss": 7.1471,
      "step": 2032
    },
    {
      "epoch": 0.11013550135501356,
      "step": 2032,
      "training_loss": 7.39978551864624
    },
    {
      "epoch": 0.11018970189701897,
      "step": 2033,
      "training_loss": 7.436779975891113
    },
    {
      "epoch": 0.11024390243902439,
      "step": 2034,
      "training_loss": 7.955730438232422
    },
    {
      "epoch": 0.11029810298102981,
      "step": 2035,
      "training_loss": 6.83694314956665
    },
    {
      "epoch": 0.11035230352303523,
      "grad_norm": 13.935140609741211,
      "learning_rate": 1e-05,
      "loss": 7.4073,
      "step": 2036
    },
    {
      "epoch": 0.11035230352303523,
      "step": 2036,
      "training_loss": 8.060543060302734
    },
    {
      "epoch": 0.11040650406504066,
      "step": 2037,
      "training_loss": 6.091076374053955
    },
    {
      "epoch": 0.11046070460704607,
      "step": 2038,
      "training_loss": 5.8447265625
    },
    {
      "epoch": 0.11051490514905149,
      "step": 2039,
      "training_loss": 6.937036991119385
    },
    {
      "epoch": 0.11056910569105691,
      "grad_norm": 18.470890045166016,
      "learning_rate": 1e-05,
      "loss": 6.7333,
      "step": 2040
    },
    {
      "epoch": 0.11056910569105691,
      "step": 2040,
      "training_loss": 7.043863773345947
    },
    {
      "epoch": 0.11062330623306232,
      "step": 2041,
      "training_loss": 6.365616321563721
    },
    {
      "epoch": 0.11067750677506775,
      "step": 2042,
      "training_loss": 7.37481164932251
    },
    {
      "epoch": 0.11073170731707317,
      "step": 2043,
      "training_loss": 6.091265678405762
    },
    {
      "epoch": 0.11078590785907859,
      "grad_norm": 18.24860191345215,
      "learning_rate": 1e-05,
      "loss": 6.7189,
      "step": 2044
    },
    {
      "epoch": 0.11078590785907859,
      "step": 2044,
      "training_loss": 7.589911937713623
    },
    {
      "epoch": 0.110840108401084,
      "step": 2045,
      "training_loss": 7.25822639465332
    },
    {
      "epoch": 0.11089430894308944,
      "step": 2046,
      "training_loss": 7.216233253479004
    },
    {
      "epoch": 0.11094850948509485,
      "step": 2047,
      "training_loss": 6.628027439117432
    },
    {
      "epoch": 0.11100271002710027,
      "grad_norm": 25.72601890563965,
      "learning_rate": 1e-05,
      "loss": 7.1731,
      "step": 2048
    },
    {
      "epoch": 0.11100271002710027,
      "step": 2048,
      "training_loss": 7.169719696044922
    },
    {
      "epoch": 0.11105691056910569,
      "step": 2049,
      "training_loss": 6.779860019683838
    },
    {
      "epoch": 0.1111111111111111,
      "step": 2050,
      "training_loss": 6.15336275100708
    },
    {
      "epoch": 0.11116531165311654,
      "step": 2051,
      "training_loss": 6.842559814453125
    },
    {
      "epoch": 0.11121951219512195,
      "grad_norm": 19.939983367919922,
      "learning_rate": 1e-05,
      "loss": 6.7364,
      "step": 2052
    },
    {
      "epoch": 0.11121951219512195,
      "step": 2052,
      "training_loss": 7.427682399749756
    },
    {
      "epoch": 0.11127371273712737,
      "step": 2053,
      "training_loss": 6.81777811050415
    },
    {
      "epoch": 0.11132791327913279,
      "step": 2054,
      "training_loss": 5.9099955558776855
    },
    {
      "epoch": 0.11138211382113822,
      "step": 2055,
      "training_loss": 4.812455177307129
    },
    {
      "epoch": 0.11143631436314363,
      "grad_norm": 30.833858489990234,
      "learning_rate": 1e-05,
      "loss": 6.242,
      "step": 2056
    },
    {
      "epoch": 0.11143631436314363,
      "step": 2056,
      "training_loss": 7.872157096862793
    },
    {
      "epoch": 0.11149051490514905,
      "step": 2057,
      "training_loss": 7.923331260681152
    },
    {
      "epoch": 0.11154471544715447,
      "step": 2058,
      "training_loss": 7.510112762451172
    },
    {
      "epoch": 0.11159891598915989,
      "step": 2059,
      "training_loss": 7.7613911628723145
    },
    {
      "epoch": 0.11165311653116532,
      "grad_norm": 16.247591018676758,
      "learning_rate": 1e-05,
      "loss": 7.7667,
      "step": 2060
    },
    {
      "epoch": 0.11165311653116532,
      "step": 2060,
      "training_loss": 5.250670909881592
    },
    {
      "epoch": 0.11170731707317073,
      "step": 2061,
      "training_loss": 7.279896259307861
    },
    {
      "epoch": 0.11176151761517615,
      "step": 2062,
      "training_loss": 6.21408224105835
    },
    {
      "epoch": 0.11181571815718157,
      "step": 2063,
      "training_loss": 7.445474624633789
    },
    {
      "epoch": 0.111869918699187,
      "grad_norm": 18.06972885131836,
      "learning_rate": 1e-05,
      "loss": 6.5475,
      "step": 2064
    },
    {
      "epoch": 0.111869918699187,
      "step": 2064,
      "training_loss": 6.311251163482666
    },
    {
      "epoch": 0.11192411924119242,
      "step": 2065,
      "training_loss": 5.578175067901611
    },
    {
      "epoch": 0.11197831978319783,
      "step": 2066,
      "training_loss": 7.900197982788086
    },
    {
      "epoch": 0.11203252032520325,
      "step": 2067,
      "training_loss": 7.469659328460693
    },
    {
      "epoch": 0.11208672086720867,
      "grad_norm": 22.75433921813965,
      "learning_rate": 1e-05,
      "loss": 6.8148,
      "step": 2068
    },
    {
      "epoch": 0.11208672086720867,
      "step": 2068,
      "training_loss": 6.8019514083862305
    },
    {
      "epoch": 0.1121409214092141,
      "step": 2069,
      "training_loss": 7.554027557373047
    },
    {
      "epoch": 0.11219512195121951,
      "step": 2070,
      "training_loss": 7.7410664558410645
    },
    {
      "epoch": 0.11224932249322493,
      "step": 2071,
      "training_loss": 7.443899631500244
    },
    {
      "epoch": 0.11230352303523035,
      "grad_norm": 28.041431427001953,
      "learning_rate": 1e-05,
      "loss": 7.3852,
      "step": 2072
    },
    {
      "epoch": 0.11230352303523035,
      "step": 2072,
      "training_loss": 8.143228530883789
    },
    {
      "epoch": 0.11235772357723577,
      "step": 2073,
      "training_loss": 6.881195545196533
    },
    {
      "epoch": 0.1124119241192412,
      "step": 2074,
      "training_loss": 6.679059028625488
    },
    {
      "epoch": 0.11246612466124661,
      "step": 2075,
      "training_loss": 7.1452250480651855
    },
    {
      "epoch": 0.11252032520325203,
      "grad_norm": 24.63528823852539,
      "learning_rate": 1e-05,
      "loss": 7.2122,
      "step": 2076
    },
    {
      "epoch": 0.11252032520325203,
      "step": 2076,
      "training_loss": 7.797855854034424
    },
    {
      "epoch": 0.11257452574525745,
      "step": 2077,
      "training_loss": 7.4600934982299805
    },
    {
      "epoch": 0.11262872628726288,
      "step": 2078,
      "training_loss": 4.757382392883301
    },
    {
      "epoch": 0.1126829268292683,
      "step": 2079,
      "training_loss": 7.314043045043945
    },
    {
      "epoch": 0.11273712737127371,
      "grad_norm": 14.911195755004883,
      "learning_rate": 1e-05,
      "loss": 6.8323,
      "step": 2080
    },
    {
      "epoch": 0.11273712737127371,
      "step": 2080,
      "training_loss": 7.941863536834717
    },
    {
      "epoch": 0.11279132791327913,
      "step": 2081,
      "training_loss": 6.773160934448242
    },
    {
      "epoch": 0.11284552845528455,
      "step": 2082,
      "training_loss": 8.481539726257324
    },
    {
      "epoch": 0.11289972899728998,
      "step": 2083,
      "training_loss": 6.505673408508301
    },
    {
      "epoch": 0.1129539295392954,
      "grad_norm": 18.554052352905273,
      "learning_rate": 1e-05,
      "loss": 7.4256,
      "step": 2084
    },
    {
      "epoch": 0.1129539295392954,
      "step": 2084,
      "training_loss": 6.93223762512207
    },
    {
      "epoch": 0.11300813008130081,
      "step": 2085,
      "training_loss": 7.408477306365967
    },
    {
      "epoch": 0.11306233062330623,
      "step": 2086,
      "training_loss": 7.136406898498535
    },
    {
      "epoch": 0.11311653116531166,
      "step": 2087,
      "training_loss": 11.030338287353516
    },
    {
      "epoch": 0.11317073170731708,
      "grad_norm": 63.35004806518555,
      "learning_rate": 1e-05,
      "loss": 8.1269,
      "step": 2088
    },
    {
      "epoch": 0.11317073170731708,
      "step": 2088,
      "training_loss": 7.2723541259765625
    },
    {
      "epoch": 0.1132249322493225,
      "step": 2089,
      "training_loss": 6.949575901031494
    },
    {
      "epoch": 0.11327913279132791,
      "step": 2090,
      "training_loss": 8.952471733093262
    },
    {
      "epoch": 0.11333333333333333,
      "step": 2091,
      "training_loss": 6.025363445281982
    },
    {
      "epoch": 0.11338753387533876,
      "grad_norm": 25.684778213500977,
      "learning_rate": 1e-05,
      "loss": 7.2999,
      "step": 2092
    },
    {
      "epoch": 0.11338753387533876,
      "step": 2092,
      "training_loss": 7.202154159545898
    },
    {
      "epoch": 0.11344173441734418,
      "step": 2093,
      "training_loss": 7.344385147094727
    },
    {
      "epoch": 0.11349593495934959,
      "step": 2094,
      "training_loss": 6.950316429138184
    },
    {
      "epoch": 0.11355013550135501,
      "step": 2095,
      "training_loss": 6.523730278015137
    },
    {
      "epoch": 0.11360433604336044,
      "grad_norm": 19.213626861572266,
      "learning_rate": 1e-05,
      "loss": 7.0051,
      "step": 2096
    },
    {
      "epoch": 0.11360433604336044,
      "step": 2096,
      "training_loss": 4.602724552154541
    },
    {
      "epoch": 0.11365853658536586,
      "step": 2097,
      "training_loss": 6.775113582611084
    },
    {
      "epoch": 0.11371273712737127,
      "step": 2098,
      "training_loss": 7.911966323852539
    },
    {
      "epoch": 0.11376693766937669,
      "step": 2099,
      "training_loss": 5.996678352355957
    },
    {
      "epoch": 0.11382113821138211,
      "grad_norm": 26.162574768066406,
      "learning_rate": 1e-05,
      "loss": 6.3216,
      "step": 2100
    },
    {
      "epoch": 0.11382113821138211,
      "step": 2100,
      "training_loss": 6.781576633453369
    },
    {
      "epoch": 0.11387533875338754,
      "step": 2101,
      "training_loss": 7.5383172035217285
    },
    {
      "epoch": 0.11392953929539296,
      "step": 2102,
      "training_loss": 7.247105121612549
    },
    {
      "epoch": 0.11398373983739837,
      "step": 2103,
      "training_loss": 5.870489120483398
    },
    {
      "epoch": 0.11403794037940379,
      "grad_norm": 27.816017150878906,
      "learning_rate": 1e-05,
      "loss": 6.8594,
      "step": 2104
    },
    {
      "epoch": 0.11403794037940379,
      "step": 2104,
      "training_loss": 7.518408298492432
    },
    {
      "epoch": 0.11409214092140921,
      "step": 2105,
      "training_loss": 8.428993225097656
    },
    {
      "epoch": 0.11414634146341464,
      "step": 2106,
      "training_loss": 6.692682266235352
    },
    {
      "epoch": 0.11420054200542006,
      "step": 2107,
      "training_loss": 8.433989524841309
    },
    {
      "epoch": 0.11425474254742547,
      "grad_norm": 32.873779296875,
      "learning_rate": 1e-05,
      "loss": 7.7685,
      "step": 2108
    },
    {
      "epoch": 0.11425474254742547,
      "step": 2108,
      "training_loss": 6.494830131530762
    },
    {
      "epoch": 0.11430894308943089,
      "step": 2109,
      "training_loss": 8.253833770751953
    },
    {
      "epoch": 0.11436314363143632,
      "step": 2110,
      "training_loss": 6.045570373535156
    },
    {
      "epoch": 0.11441734417344174,
      "step": 2111,
      "training_loss": 7.167105197906494
    },
    {
      "epoch": 0.11447154471544715,
      "grad_norm": 18.969377517700195,
      "learning_rate": 1e-05,
      "loss": 6.9903,
      "step": 2112
    },
    {
      "epoch": 0.11447154471544715,
      "step": 2112,
      "training_loss": 7.4354658126831055
    },
    {
      "epoch": 0.11452574525745257,
      "step": 2113,
      "training_loss": 7.504770278930664
    },
    {
      "epoch": 0.11457994579945799,
      "step": 2114,
      "training_loss": 5.098837375640869
    },
    {
      "epoch": 0.11463414634146342,
      "step": 2115,
      "training_loss": 7.09758996963501
    },
    {
      "epoch": 0.11468834688346884,
      "grad_norm": 17.852073669433594,
      "learning_rate": 1e-05,
      "loss": 6.7842,
      "step": 2116
    },
    {
      "epoch": 0.11468834688346884,
      "step": 2116,
      "training_loss": 6.416047096252441
    },
    {
      "epoch": 0.11474254742547425,
      "step": 2117,
      "training_loss": 5.340090751647949
    },
    {
      "epoch": 0.11479674796747967,
      "step": 2118,
      "training_loss": 6.885333061218262
    },
    {
      "epoch": 0.1148509485094851,
      "step": 2119,
      "training_loss": 6.9160051345825195
    },
    {
      "epoch": 0.11490514905149052,
      "grad_norm": 22.28732681274414,
      "learning_rate": 1e-05,
      "loss": 6.3894,
      "step": 2120
    },
    {
      "epoch": 0.11490514905149052,
      "step": 2120,
      "training_loss": 7.809572219848633
    },
    {
      "epoch": 0.11495934959349594,
      "step": 2121,
      "training_loss": 6.504796981811523
    },
    {
      "epoch": 0.11501355013550135,
      "step": 2122,
      "training_loss": 6.268870830535889
    },
    {
      "epoch": 0.11506775067750677,
      "step": 2123,
      "training_loss": 7.7261528968811035
    },
    {
      "epoch": 0.1151219512195122,
      "grad_norm": 26.758596420288086,
      "learning_rate": 1e-05,
      "loss": 7.0773,
      "step": 2124
    },
    {
      "epoch": 0.1151219512195122,
      "step": 2124,
      "training_loss": 7.725598335266113
    },
    {
      "epoch": 0.11517615176151762,
      "step": 2125,
      "training_loss": 7.527799129486084
    },
    {
      "epoch": 0.11523035230352303,
      "step": 2126,
      "training_loss": 7.7196478843688965
    },
    {
      "epoch": 0.11528455284552845,
      "step": 2127,
      "training_loss": 6.61648416519165
    },
    {
      "epoch": 0.11533875338753388,
      "grad_norm": 22.383005142211914,
      "learning_rate": 1e-05,
      "loss": 7.3974,
      "step": 2128
    },
    {
      "epoch": 0.11533875338753388,
      "step": 2128,
      "training_loss": 7.687233924865723
    },
    {
      "epoch": 0.1153929539295393,
      "step": 2129,
      "training_loss": 4.464258670806885
    },
    {
      "epoch": 0.11544715447154472,
      "step": 2130,
      "training_loss": 5.528926849365234
    },
    {
      "epoch": 0.11550135501355013,
      "step": 2131,
      "training_loss": 6.233382225036621
    },
    {
      "epoch": 0.11555555555555555,
      "grad_norm": 26.185707092285156,
      "learning_rate": 1e-05,
      "loss": 5.9785,
      "step": 2132
    },
    {
      "epoch": 0.11555555555555555,
      "step": 2132,
      "training_loss": 5.084183216094971
    },
    {
      "epoch": 0.11560975609756098,
      "step": 2133,
      "training_loss": 6.269379615783691
    },
    {
      "epoch": 0.1156639566395664,
      "step": 2134,
      "training_loss": 6.6263041496276855
    },
    {
      "epoch": 0.11571815718157182,
      "step": 2135,
      "training_loss": 7.871219158172607
    },
    {
      "epoch": 0.11577235772357723,
      "grad_norm": 22.49785614013672,
      "learning_rate": 1e-05,
      "loss": 6.4628,
      "step": 2136
    },
    {
      "epoch": 0.11577235772357723,
      "step": 2136,
      "training_loss": 7.294027805328369
    },
    {
      "epoch": 0.11582655826558265,
      "step": 2137,
      "training_loss": 6.727995872497559
    },
    {
      "epoch": 0.11588075880758808,
      "step": 2138,
      "training_loss": 5.219162464141846
    },
    {
      "epoch": 0.1159349593495935,
      "step": 2139,
      "training_loss": 8.14572525024414
    },
    {
      "epoch": 0.11598915989159891,
      "grad_norm": 19.990718841552734,
      "learning_rate": 1e-05,
      "loss": 6.8467,
      "step": 2140
    },
    {
      "epoch": 0.11598915989159891,
      "step": 2140,
      "training_loss": 6.7659759521484375
    },
    {
      "epoch": 0.11604336043360433,
      "step": 2141,
      "training_loss": 6.988767623901367
    },
    {
      "epoch": 0.11609756097560976,
      "step": 2142,
      "training_loss": 6.989974498748779
    },
    {
      "epoch": 0.11615176151761518,
      "step": 2143,
      "training_loss": 6.723678112030029
    },
    {
      "epoch": 0.1162059620596206,
      "grad_norm": 23.07312774658203,
      "learning_rate": 1e-05,
      "loss": 6.8671,
      "step": 2144
    },
    {
      "epoch": 0.1162059620596206,
      "step": 2144,
      "training_loss": 6.134854793548584
    },
    {
      "epoch": 0.11626016260162601,
      "step": 2145,
      "training_loss": 5.1014485359191895
    },
    {
      "epoch": 0.11631436314363143,
      "step": 2146,
      "training_loss": 5.724407196044922
    },
    {
      "epoch": 0.11636856368563686,
      "step": 2147,
      "training_loss": 7.357482433319092
    },
    {
      "epoch": 0.11642276422764228,
      "grad_norm": 15.279850006103516,
      "learning_rate": 1e-05,
      "loss": 6.0795,
      "step": 2148
    },
    {
      "epoch": 0.11642276422764228,
      "step": 2148,
      "training_loss": 5.9882097244262695
    },
    {
      "epoch": 0.1164769647696477,
      "step": 2149,
      "training_loss": 7.7010817527771
    },
    {
      "epoch": 0.11653116531165311,
      "step": 2150,
      "training_loss": 7.026291370391846
    },
    {
      "epoch": 0.11658536585365854,
      "step": 2151,
      "training_loss": 6.788961887359619
    },
    {
      "epoch": 0.11663956639566396,
      "grad_norm": 70.93637084960938,
      "learning_rate": 1e-05,
      "loss": 6.8761,
      "step": 2152
    },
    {
      "epoch": 0.11663956639566396,
      "step": 2152,
      "training_loss": 6.7039103507995605
    },
    {
      "epoch": 0.11669376693766938,
      "step": 2153,
      "training_loss": 7.3231120109558105
    },
    {
      "epoch": 0.1167479674796748,
      "step": 2154,
      "training_loss": 5.800763130187988
    },
    {
      "epoch": 0.11680216802168021,
      "step": 2155,
      "training_loss": 7.861323356628418
    },
    {
      "epoch": 0.11685636856368564,
      "grad_norm": 19.008670806884766,
      "learning_rate": 1e-05,
      "loss": 6.9223,
      "step": 2156
    },
    {
      "epoch": 0.11685636856368564,
      "step": 2156,
      "training_loss": 7.269955635070801
    },
    {
      "epoch": 0.11691056910569106,
      "step": 2157,
      "training_loss": 4.812743663787842
    },
    {
      "epoch": 0.11696476964769648,
      "step": 2158,
      "training_loss": 7.735233783721924
    },
    {
      "epoch": 0.11701897018970189,
      "step": 2159,
      "training_loss": 6.502394199371338
    },
    {
      "epoch": 0.11707317073170732,
      "grad_norm": 23.99907684326172,
      "learning_rate": 1e-05,
      "loss": 6.5801,
      "step": 2160
    },
    {
      "epoch": 0.11707317073170732,
      "step": 2160,
      "training_loss": 7.685892105102539
    },
    {
      "epoch": 0.11712737127371274,
      "step": 2161,
      "training_loss": 7.871717929840088
    },
    {
      "epoch": 0.11718157181571816,
      "step": 2162,
      "training_loss": 6.920495986938477
    },
    {
      "epoch": 0.11723577235772357,
      "step": 2163,
      "training_loss": 7.4070515632629395
    },
    {
      "epoch": 0.11728997289972899,
      "grad_norm": 20.27174186706543,
      "learning_rate": 1e-05,
      "loss": 7.4713,
      "step": 2164
    },
    {
      "epoch": 0.11728997289972899,
      "step": 2164,
      "training_loss": 7.945376396179199
    },
    {
      "epoch": 0.11734417344173442,
      "step": 2165,
      "training_loss": 4.502406120300293
    },
    {
      "epoch": 0.11739837398373984,
      "step": 2166,
      "training_loss": 7.04753303527832
    },
    {
      "epoch": 0.11745257452574526,
      "step": 2167,
      "training_loss": 7.5309343338012695
    },
    {
      "epoch": 0.11750677506775067,
      "grad_norm": 22.33063316345215,
      "learning_rate": 1e-05,
      "loss": 6.7566,
      "step": 2168
    },
    {
      "epoch": 0.11750677506775067,
      "step": 2168,
      "training_loss": 6.14025354385376
    },
    {
      "epoch": 0.11756097560975609,
      "step": 2169,
      "training_loss": 6.563918590545654
    },
    {
      "epoch": 0.11761517615176152,
      "step": 2170,
      "training_loss": 7.859302520751953
    },
    {
      "epoch": 0.11766937669376694,
      "step": 2171,
      "training_loss": 6.076991558074951
    },
    {
      "epoch": 0.11772357723577236,
      "grad_norm": 17.46418571472168,
      "learning_rate": 1e-05,
      "loss": 6.6601,
      "step": 2172
    },
    {
      "epoch": 0.11772357723577236,
      "step": 2172,
      "training_loss": 5.954078674316406
    },
    {
      "epoch": 0.11777777777777777,
      "step": 2173,
      "training_loss": 7.260891914367676
    },
    {
      "epoch": 0.1178319783197832,
      "step": 2174,
      "training_loss": 9.489696502685547
    },
    {
      "epoch": 0.11788617886178862,
      "step": 2175,
      "training_loss": 8.630245208740234
    },
    {
      "epoch": 0.11794037940379404,
      "grad_norm": 34.31849670410156,
      "learning_rate": 1e-05,
      "loss": 7.8337,
      "step": 2176
    },
    {
      "epoch": 0.11794037940379404,
      "step": 2176,
      "training_loss": 7.6533074378967285
    },
    {
      "epoch": 0.11799457994579945,
      "step": 2177,
      "training_loss": 5.671876430511475
    },
    {
      "epoch": 0.11804878048780487,
      "step": 2178,
      "training_loss": 6.775413990020752
    },
    {
      "epoch": 0.1181029810298103,
      "step": 2179,
      "training_loss": 5.70692253112793
    },
    {
      "epoch": 0.11815718157181572,
      "grad_norm": 18.061006546020508,
      "learning_rate": 1e-05,
      "loss": 6.4519,
      "step": 2180
    },
    {
      "epoch": 0.11815718157181572,
      "step": 2180,
      "training_loss": 7.063594818115234
    },
    {
      "epoch": 0.11821138211382114,
      "step": 2181,
      "training_loss": 7.9276885986328125
    },
    {
      "epoch": 0.11826558265582655,
      "step": 2182,
      "training_loss": 9.29543399810791
    },
    {
      "epoch": 0.11831978319783198,
      "step": 2183,
      "training_loss": 6.55404806137085
    },
    {
      "epoch": 0.1183739837398374,
      "grad_norm": 15.392293930053711,
      "learning_rate": 1e-05,
      "loss": 7.7102,
      "step": 2184
    },
    {
      "epoch": 0.1183739837398374,
      "step": 2184,
      "training_loss": 8.104308128356934
    },
    {
      "epoch": 0.11842818428184282,
      "step": 2185,
      "training_loss": 6.652906894683838
    },
    {
      "epoch": 0.11848238482384824,
      "step": 2186,
      "training_loss": 6.879006385803223
    },
    {
      "epoch": 0.11853658536585365,
      "step": 2187,
      "training_loss": 6.587248802185059
    },
    {
      "epoch": 0.11859078590785908,
      "grad_norm": 18.70789337158203,
      "learning_rate": 1e-05,
      "loss": 7.0559,
      "step": 2188
    },
    {
      "epoch": 0.11859078590785908,
      "step": 2188,
      "training_loss": 6.450517654418945
    },
    {
      "epoch": 0.1186449864498645,
      "step": 2189,
      "training_loss": 5.038748741149902
    },
    {
      "epoch": 0.11869918699186992,
      "step": 2190,
      "training_loss": 8.775609016418457
    },
    {
      "epoch": 0.11875338753387533,
      "step": 2191,
      "training_loss": 6.797126293182373
    },
    {
      "epoch": 0.11880758807588077,
      "grad_norm": 18.439281463623047,
      "learning_rate": 1e-05,
      "loss": 6.7655,
      "step": 2192
    },
    {
      "epoch": 0.11880758807588077,
      "step": 2192,
      "training_loss": 6.416903495788574
    },
    {
      "epoch": 0.11886178861788618,
      "step": 2193,
      "training_loss": 5.5691962242126465
    },
    {
      "epoch": 0.1189159891598916,
      "step": 2194,
      "training_loss": 6.2019944190979
    },
    {
      "epoch": 0.11897018970189702,
      "step": 2195,
      "training_loss": 6.4698991775512695
    },
    {
      "epoch": 0.11902439024390243,
      "grad_norm": 22.409799575805664,
      "learning_rate": 1e-05,
      "loss": 6.1645,
      "step": 2196
    },
    {
      "epoch": 0.11902439024390243,
      "step": 2196,
      "training_loss": 6.789050579071045
    },
    {
      "epoch": 0.11907859078590786,
      "step": 2197,
      "training_loss": 6.300307273864746
    },
    {
      "epoch": 0.11913279132791328,
      "step": 2198,
      "training_loss": 5.709183692932129
    },
    {
      "epoch": 0.1191869918699187,
      "step": 2199,
      "training_loss": 7.36451530456543
    },
    {
      "epoch": 0.11924119241192412,
      "grad_norm": 11.4341402053833,
      "learning_rate": 1e-05,
      "loss": 6.5408,
      "step": 2200
    },
    {
      "epoch": 0.11924119241192412,
      "step": 2200,
      "training_loss": 4.954929828643799
    },
    {
      "epoch": 0.11929539295392953,
      "step": 2201,
      "training_loss": 5.926755428314209
    },
    {
      "epoch": 0.11934959349593496,
      "step": 2202,
      "training_loss": 7.742156982421875
    },
    {
      "epoch": 0.11940379403794038,
      "step": 2203,
      "training_loss": 7.57988166809082
    },
    {
      "epoch": 0.1194579945799458,
      "grad_norm": 28.034393310546875,
      "learning_rate": 1e-05,
      "loss": 6.5509,
      "step": 2204
    },
    {
      "epoch": 0.1194579945799458,
      "step": 2204,
      "training_loss": 8.24657917022705
    },
    {
      "epoch": 0.11951219512195121,
      "step": 2205,
      "training_loss": 6.572117805480957
    },
    {
      "epoch": 0.11956639566395665,
      "step": 2206,
      "training_loss": 7.496987819671631
    },
    {
      "epoch": 0.11962059620596206,
      "step": 2207,
      "training_loss": 7.6184000968933105
    },
    {
      "epoch": 0.11967479674796748,
      "grad_norm": 31.050830841064453,
      "learning_rate": 1e-05,
      "loss": 7.4835,
      "step": 2208
    },
    {
      "epoch": 0.11967479674796748,
      "step": 2208,
      "training_loss": 4.887528419494629
    },
    {
      "epoch": 0.1197289972899729,
      "step": 2209,
      "training_loss": 6.160145282745361
    },
    {
      "epoch": 0.11978319783197831,
      "step": 2210,
      "training_loss": 7.176706790924072
    },
    {
      "epoch": 0.11983739837398374,
      "step": 2211,
      "training_loss": 6.819027423858643
    },
    {
      "epoch": 0.11989159891598916,
      "grad_norm": 19.507844924926758,
      "learning_rate": 1e-05,
      "loss": 6.2609,
      "step": 2212
    },
    {
      "epoch": 0.11989159891598916,
      "step": 2212,
      "training_loss": 7.826619625091553
    },
    {
      "epoch": 0.11994579945799458,
      "step": 2213,
      "training_loss": 7.772364616394043
    },
    {
      "epoch": 0.12,
      "step": 2214,
      "training_loss": 5.403945446014404
    },
    {
      "epoch": 0.12005420054200543,
      "step": 2215,
      "training_loss": 8.485770225524902
    },
    {
      "epoch": 0.12010840108401084,
      "grad_norm": 32.67414093017578,
      "learning_rate": 1e-05,
      "loss": 7.3722,
      "step": 2216
    },
    {
      "epoch": 0.12010840108401084,
      "step": 2216,
      "training_loss": 7.541272163391113
    },
    {
      "epoch": 0.12016260162601626,
      "step": 2217,
      "training_loss": 7.084115028381348
    },
    {
      "epoch": 0.12021680216802168,
      "step": 2218,
      "training_loss": 6.98788595199585
    },
    {
      "epoch": 0.1202710027100271,
      "step": 2219,
      "training_loss": 5.689939975738525
    },
    {
      "epoch": 0.12032520325203253,
      "grad_norm": 33.34212875366211,
      "learning_rate": 1e-05,
      "loss": 6.8258,
      "step": 2220
    },
    {
      "epoch": 0.12032520325203253,
      "step": 2220,
      "training_loss": 7.090573787689209
    },
    {
      "epoch": 0.12037940379403794,
      "step": 2221,
      "training_loss": 5.047291278839111
    },
    {
      "epoch": 0.12043360433604336,
      "step": 2222,
      "training_loss": 6.384059429168701
    },
    {
      "epoch": 0.12048780487804878,
      "step": 2223,
      "training_loss": 7.2189717292785645
    },
    {
      "epoch": 0.12054200542005421,
      "grad_norm": 18.934520721435547,
      "learning_rate": 1e-05,
      "loss": 6.4352,
      "step": 2224
    },
    {
      "epoch": 0.12054200542005421,
      "step": 2224,
      "training_loss": 6.766025066375732
    },
    {
      "epoch": 0.12059620596205962,
      "step": 2225,
      "training_loss": 7.633221626281738
    },
    {
      "epoch": 0.12065040650406504,
      "step": 2226,
      "training_loss": 7.633810997009277
    },
    {
      "epoch": 0.12070460704607046,
      "step": 2227,
      "training_loss": 7.482257843017578
    },
    {
      "epoch": 0.12075880758807588,
      "grad_norm": 23.372957229614258,
      "learning_rate": 1e-05,
      "loss": 7.3788,
      "step": 2228
    },
    {
      "epoch": 0.12075880758807588,
      "step": 2228,
      "training_loss": 6.6373291015625
    },
    {
      "epoch": 0.1208130081300813,
      "step": 2229,
      "training_loss": 5.038592338562012
    },
    {
      "epoch": 0.12086720867208672,
      "step": 2230,
      "training_loss": 8.039042472839355
    },
    {
      "epoch": 0.12092140921409214,
      "step": 2231,
      "training_loss": 6.464637756347656
    },
    {
      "epoch": 0.12097560975609756,
      "grad_norm": 33.631282806396484,
      "learning_rate": 1e-05,
      "loss": 6.5449,
      "step": 2232
    },
    {
      "epoch": 0.12097560975609756,
      "step": 2232,
      "training_loss": 7.694478988647461
    },
    {
      "epoch": 0.12102981029810297,
      "step": 2233,
      "training_loss": 6.953336238861084
    },
    {
      "epoch": 0.1210840108401084,
      "step": 2234,
      "training_loss": 6.987929821014404
    },
    {
      "epoch": 0.12113821138211382,
      "step": 2235,
      "training_loss": 7.201374053955078
    },
    {
      "epoch": 0.12119241192411924,
      "grad_norm": 24.31184959411621,
      "learning_rate": 1e-05,
      "loss": 7.2093,
      "step": 2236
    },
    {
      "epoch": 0.12119241192411924,
      "step": 2236,
      "training_loss": 6.796027660369873
    },
    {
      "epoch": 0.12124661246612466,
      "step": 2237,
      "training_loss": 7.678626537322998
    },
    {
      "epoch": 0.12130081300813009,
      "step": 2238,
      "training_loss": 6.350281715393066
    },
    {
      "epoch": 0.1213550135501355,
      "step": 2239,
      "training_loss": 7.147307872772217
    },
    {
      "epoch": 0.12140921409214092,
      "grad_norm": 18.23291015625,
      "learning_rate": 1e-05,
      "loss": 6.9931,
      "step": 2240
    },
    {
      "epoch": 0.12140921409214092,
      "step": 2240,
      "training_loss": 7.355422496795654
    },
    {
      "epoch": 0.12146341463414634,
      "step": 2241,
      "training_loss": 6.673851490020752
    },
    {
      "epoch": 0.12151761517615176,
      "step": 2242,
      "training_loss": 6.4347920417785645
    },
    {
      "epoch": 0.12157181571815719,
      "step": 2243,
      "training_loss": 6.456027984619141
    },
    {
      "epoch": 0.1216260162601626,
      "grad_norm": 16.660287857055664,
      "learning_rate": 1e-05,
      "loss": 6.73,
      "step": 2244
    },
    {
      "epoch": 0.1216260162601626,
      "step": 2244,
      "training_loss": 7.085278034210205
    },
    {
      "epoch": 0.12168021680216802,
      "step": 2245,
      "training_loss": 6.997116565704346
    },
    {
      "epoch": 0.12173441734417344,
      "step": 2246,
      "training_loss": 6.541682720184326
    },
    {
      "epoch": 0.12178861788617887,
      "step": 2247,
      "training_loss": 5.74325704574585
    },
    {
      "epoch": 0.12184281842818429,
      "grad_norm": 14.664073944091797,
      "learning_rate": 1e-05,
      "loss": 6.5918,
      "step": 2248
    },
    {
      "epoch": 0.12184281842818429,
      "step": 2248,
      "training_loss": 4.703540802001953
    },
    {
      "epoch": 0.1218970189701897,
      "step": 2249,
      "training_loss": 5.0067267417907715
    },
    {
      "epoch": 0.12195121951219512,
      "step": 2250,
      "training_loss": 5.738106727600098
    },
    {
      "epoch": 0.12200542005420054,
      "step": 2251,
      "training_loss": 7.200331211090088
    },
    {
      "epoch": 0.12205962059620597,
      "grad_norm": 19.849672317504883,
      "learning_rate": 1e-05,
      "loss": 5.6622,
      "step": 2252
    },
    {
      "epoch": 0.12205962059620597,
      "step": 2252,
      "training_loss": 7.078901767730713
    },
    {
      "epoch": 0.12211382113821138,
      "step": 2253,
      "training_loss": 7.500701427459717
    },
    {
      "epoch": 0.1221680216802168,
      "step": 2254,
      "training_loss": 5.436148643493652
    },
    {
      "epoch": 0.12222222222222222,
      "step": 2255,
      "training_loss": 7.110401153564453
    },
    {
      "epoch": 0.12227642276422765,
      "grad_norm": 29.79174041748047,
      "learning_rate": 1e-05,
      "loss": 6.7815,
      "step": 2256
    },
    {
      "epoch": 0.12227642276422765,
      "step": 2256,
      "training_loss": 7.0023369789123535
    },
    {
      "epoch": 0.12233062330623307,
      "step": 2257,
      "training_loss": 7.499988555908203
    },
    {
      "epoch": 0.12238482384823848,
      "step": 2258,
      "training_loss": 7.7180986404418945
    },
    {
      "epoch": 0.1224390243902439,
      "step": 2259,
      "training_loss": 7.783355236053467
    },
    {
      "epoch": 0.12249322493224932,
      "grad_norm": 21.479135513305664,
      "learning_rate": 1e-05,
      "loss": 7.5009,
      "step": 2260
    },
    {
      "epoch": 0.12249322493224932,
      "step": 2260,
      "training_loss": 7.4851884841918945
    },
    {
      "epoch": 0.12254742547425475,
      "step": 2261,
      "training_loss": 6.882392883300781
    },
    {
      "epoch": 0.12260162601626017,
      "step": 2262,
      "training_loss": 6.7331366539001465
    },
    {
      "epoch": 0.12265582655826558,
      "step": 2263,
      "training_loss": 9.494770050048828
    },
    {
      "epoch": 0.122710027100271,
      "grad_norm": 46.98485565185547,
      "learning_rate": 1e-05,
      "loss": 7.6489,
      "step": 2264
    },
    {
      "epoch": 0.122710027100271,
      "step": 2264,
      "training_loss": 7.01367712020874
    },
    {
      "epoch": 0.12276422764227642,
      "step": 2265,
      "training_loss": 6.715948104858398
    },
    {
      "epoch": 0.12281842818428185,
      "step": 2266,
      "training_loss": 6.553761005401611
    },
    {
      "epoch": 0.12287262872628726,
      "step": 2267,
      "training_loss": 4.769719123840332
    },
    {
      "epoch": 0.12292682926829268,
      "grad_norm": 17.23247528076172,
      "learning_rate": 1e-05,
      "loss": 6.2633,
      "step": 2268
    },
    {
      "epoch": 0.12292682926829268,
      "step": 2268,
      "training_loss": 6.697353363037109
    },
    {
      "epoch": 0.1229810298102981,
      "step": 2269,
      "training_loss": 4.626371383666992
    },
    {
      "epoch": 0.12303523035230353,
      "step": 2270,
      "training_loss": 7.181683540344238
    },
    {
      "epoch": 0.12308943089430895,
      "step": 2271,
      "training_loss": 8.144038200378418
    },
    {
      "epoch": 0.12314363143631436,
      "grad_norm": 20.310401916503906,
      "learning_rate": 1e-05,
      "loss": 6.6624,
      "step": 2272
    },
    {
      "epoch": 0.12314363143631436,
      "step": 2272,
      "training_loss": 7.420018672943115
    },
    {
      "epoch": 0.12319783197831978,
      "step": 2273,
      "training_loss": 7.878206729888916
    },
    {
      "epoch": 0.1232520325203252,
      "step": 2274,
      "training_loss": 6.313616752624512
    },
    {
      "epoch": 0.12330623306233063,
      "step": 2275,
      "training_loss": 7.545258522033691
    },
    {
      "epoch": 0.12336043360433604,
      "grad_norm": 32.66688919067383,
      "learning_rate": 1e-05,
      "loss": 7.2893,
      "step": 2276
    },
    {
      "epoch": 0.12336043360433604,
      "step": 2276,
      "training_loss": 7.857612609863281
    },
    {
      "epoch": 0.12341463414634146,
      "step": 2277,
      "training_loss": 6.832972526550293
    },
    {
      "epoch": 0.12346883468834688,
      "step": 2278,
      "training_loss": 7.7751851081848145
    },
    {
      "epoch": 0.12352303523035231,
      "step": 2279,
      "training_loss": 7.564754486083984
    },
    {
      "epoch": 0.12357723577235773,
      "grad_norm": 21.442054748535156,
      "learning_rate": 1e-05,
      "loss": 7.5076,
      "step": 2280
    },
    {
      "epoch": 0.12357723577235773,
      "step": 2280,
      "training_loss": 8.066852569580078
    },
    {
      "epoch": 0.12363143631436314,
      "step": 2281,
      "training_loss": 6.586187839508057
    },
    {
      "epoch": 0.12368563685636856,
      "step": 2282,
      "training_loss": 7.1680521965026855
    },
    {
      "epoch": 0.12373983739837398,
      "step": 2283,
      "training_loss": 4.69591760635376
    },
    {
      "epoch": 0.12379403794037941,
      "grad_norm": 18.26604461669922,
      "learning_rate": 1e-05,
      "loss": 6.6293,
      "step": 2284
    },
    {
      "epoch": 0.12379403794037941,
      "step": 2284,
      "training_loss": 6.899262428283691
    },
    {
      "epoch": 0.12384823848238483,
      "step": 2285,
      "training_loss": 8.297028541564941
    },
    {
      "epoch": 0.12390243902439024,
      "step": 2286,
      "training_loss": 6.748493671417236
    },
    {
      "epoch": 0.12395663956639566,
      "step": 2287,
      "training_loss": 5.633950710296631
    },
    {
      "epoch": 0.12401084010840109,
      "grad_norm": 21.80858039855957,
      "learning_rate": 1e-05,
      "loss": 6.8947,
      "step": 2288
    },
    {
      "epoch": 0.12401084010840109,
      "step": 2288,
      "training_loss": 7.935492992401123
    },
    {
      "epoch": 0.12406504065040651,
      "step": 2289,
      "training_loss": 6.318075656890869
    },
    {
      "epoch": 0.12411924119241192,
      "step": 2290,
      "training_loss": 8.735027313232422
    },
    {
      "epoch": 0.12417344173441734,
      "step": 2291,
      "training_loss": 7.181682109832764
    },
    {
      "epoch": 0.12422764227642276,
      "grad_norm": 25.777793884277344,
      "learning_rate": 1e-05,
      "loss": 7.5426,
      "step": 2292
    },
    {
      "epoch": 0.12422764227642276,
      "step": 2292,
      "training_loss": 6.749355792999268
    },
    {
      "epoch": 0.12428184281842819,
      "step": 2293,
      "training_loss": 7.916969299316406
    },
    {
      "epoch": 0.1243360433604336,
      "step": 2294,
      "training_loss": 7.639201641082764
    },
    {
      "epoch": 0.12439024390243902,
      "step": 2295,
      "training_loss": 5.085854530334473
    },
    {
      "epoch": 0.12444444444444444,
      "grad_norm": 23.33244514465332,
      "learning_rate": 1e-05,
      "loss": 6.8478,
      "step": 2296
    },
    {
      "epoch": 0.12444444444444444,
      "step": 2296,
      "training_loss": 7.673794269561768
    },
    {
      "epoch": 0.12449864498644986,
      "step": 2297,
      "training_loss": 7.263751983642578
    },
    {
      "epoch": 0.12455284552845529,
      "step": 2298,
      "training_loss": 7.893367290496826
    },
    {
      "epoch": 0.1246070460704607,
      "step": 2299,
      "training_loss": 6.223792552947998
    },
    {
      "epoch": 0.12466124661246612,
      "grad_norm": 26.077638626098633,
      "learning_rate": 1e-05,
      "loss": 7.2637,
      "step": 2300
    },
    {
      "epoch": 0.12466124661246612,
      "step": 2300,
      "training_loss": 7.192048072814941
    },
    {
      "epoch": 0.12471544715447154,
      "step": 2301,
      "training_loss": 5.456752300262451
    },
    {
      "epoch": 0.12476964769647697,
      "step": 2302,
      "training_loss": 7.792191028594971
    },
    {
      "epoch": 0.12482384823848239,
      "step": 2303,
      "training_loss": 6.6484270095825195
    },
    {
      "epoch": 0.1248780487804878,
      "grad_norm": 33.54572296142578,
      "learning_rate": 1e-05,
      "loss": 6.7724,
      "step": 2304
    },
    {
      "epoch": 0.1248780487804878,
      "step": 2304,
      "training_loss": 5.881302833557129
    },
    {
      "epoch": 0.12493224932249322,
      "step": 2305,
      "training_loss": 6.88964319229126
    },
    {
      "epoch": 0.12498644986449864,
      "step": 2306,
      "training_loss": 6.558231353759766
    },
    {
      "epoch": 0.12504065040650406,
      "step": 2307,
      "training_loss": 5.7404398918151855
    },
    {
      "epoch": 0.12509485094850947,
      "grad_norm": 16.859508514404297,
      "learning_rate": 1e-05,
      "loss": 6.2674,
      "step": 2308
    },
    {
      "epoch": 0.12509485094850947,
      "step": 2308,
      "training_loss": 5.9354681968688965
    },
    {
      "epoch": 0.12514905149051492,
      "step": 2309,
      "training_loss": 7.143780708312988
    },
    {
      "epoch": 0.12520325203252033,
      "step": 2310,
      "training_loss": 6.716307640075684
    },
    {
      "epoch": 0.12525745257452575,
      "step": 2311,
      "training_loss": 6.3847808837890625
    },
    {
      "epoch": 0.12531165311653117,
      "grad_norm": 15.5435152053833,
      "learning_rate": 1e-05,
      "loss": 6.5451,
      "step": 2312
    },
    {
      "epoch": 0.12531165311653117,
      "step": 2312,
      "training_loss": 5.398257732391357
    },
    {
      "epoch": 0.12536585365853659,
      "step": 2313,
      "training_loss": 6.784984111785889
    },
    {
      "epoch": 0.125420054200542,
      "step": 2314,
      "training_loss": 7.342296123504639
    },
    {
      "epoch": 0.12547425474254742,
      "step": 2315,
      "training_loss": 5.805182933807373
    },
    {
      "epoch": 0.12552845528455284,
      "grad_norm": 24.448890686035156,
      "learning_rate": 1e-05,
      "loss": 6.3327,
      "step": 2316
    },
    {
      "epoch": 0.12552845528455284,
      "step": 2316,
      "training_loss": 6.260063171386719
    },
    {
      "epoch": 0.12558265582655825,
      "step": 2317,
      "training_loss": 7.237641334533691
    },
    {
      "epoch": 0.1256368563685637,
      "step": 2318,
      "training_loss": 7.885072708129883
    },
    {
      "epoch": 0.12569105691056912,
      "step": 2319,
      "training_loss": 6.006633281707764
    },
    {
      "epoch": 0.12574525745257453,
      "grad_norm": 14.371999740600586,
      "learning_rate": 1e-05,
      "loss": 6.8474,
      "step": 2320
    },
    {
      "epoch": 0.12574525745257453,
      "step": 2320,
      "training_loss": 7.109259128570557
    },
    {
      "epoch": 0.12579945799457995,
      "step": 2321,
      "training_loss": 6.004148960113525
    },
    {
      "epoch": 0.12585365853658537,
      "step": 2322,
      "training_loss": 6.886720657348633
    },
    {
      "epoch": 0.12590785907859078,
      "step": 2323,
      "training_loss": 8.63757038116455
    },
    {
      "epoch": 0.1259620596205962,
      "grad_norm": 42.692726135253906,
      "learning_rate": 1e-05,
      "loss": 7.1594,
      "step": 2324
    },
    {
      "epoch": 0.1259620596205962,
      "step": 2324,
      "training_loss": 9.097541809082031
    },
    {
      "epoch": 0.12601626016260162,
      "step": 2325,
      "training_loss": 6.228115558624268
    },
    {
      "epoch": 0.12607046070460703,
      "step": 2326,
      "training_loss": 6.93276309967041
    },
    {
      "epoch": 0.12612466124661248,
      "step": 2327,
      "training_loss": 7.2412428855896
    },
    {
      "epoch": 0.1261788617886179,
      "grad_norm": 18.923810958862305,
      "learning_rate": 1e-05,
      "loss": 7.3749,
      "step": 2328
    },
    {
      "epoch": 0.1261788617886179,
      "step": 2328,
      "training_loss": 7.3738603591918945
    },
    {
      "epoch": 0.1262330623306233,
      "step": 2329,
      "training_loss": 6.653367042541504
    },
    {
      "epoch": 0.12628726287262873,
      "step": 2330,
      "training_loss": 7.742931842803955
    },
    {
      "epoch": 0.12634146341463415,
      "step": 2331,
      "training_loss": 7.738585472106934
    },
    {
      "epoch": 0.12639566395663956,
      "grad_norm": 20.050537109375,
      "learning_rate": 1e-05,
      "loss": 7.3772,
      "step": 2332
    },
    {
      "epoch": 0.12639566395663956,
      "step": 2332,
      "training_loss": 7.737110137939453
    },
    {
      "epoch": 0.12644986449864498,
      "step": 2333,
      "training_loss": 8.401562690734863
    },
    {
      "epoch": 0.1265040650406504,
      "step": 2334,
      "training_loss": 6.940257549285889
    },
    {
      "epoch": 0.12655826558265582,
      "step": 2335,
      "training_loss": 5.1582841873168945
    },
    {
      "epoch": 0.12661246612466126,
      "grad_norm": 25.309974670410156,
      "learning_rate": 1e-05,
      "loss": 7.0593,
      "step": 2336
    },
    {
      "epoch": 0.12661246612466126,
      "step": 2336,
      "training_loss": 6.9512810707092285
    },
    {
      "epoch": 0.12666666666666668,
      "step": 2337,
      "training_loss": 7.266663074493408
    },
    {
      "epoch": 0.1267208672086721,
      "step": 2338,
      "training_loss": 7.459133148193359
    },
    {
      "epoch": 0.1267750677506775,
      "step": 2339,
      "training_loss": 7.0086774826049805
    },
    {
      "epoch": 0.12682926829268293,
      "grad_norm": 23.231477737426758,
      "learning_rate": 1e-05,
      "loss": 7.1714,
      "step": 2340
    },
    {
      "epoch": 0.12682926829268293,
      "step": 2340,
      "training_loss": 6.045403957366943
    },
    {
      "epoch": 0.12688346883468835,
      "step": 2341,
      "training_loss": 7.154129981994629
    },
    {
      "epoch": 0.12693766937669376,
      "step": 2342,
      "training_loss": 7.1170148849487305
    },
    {
      "epoch": 0.12699186991869918,
      "step": 2343,
      "training_loss": 8.479242324829102
    },
    {
      "epoch": 0.1270460704607046,
      "grad_norm": 30.86034393310547,
      "learning_rate": 1e-05,
      "loss": 7.1989,
      "step": 2344
    },
    {
      "epoch": 0.1270460704607046,
      "step": 2344,
      "training_loss": 6.553088665008545
    },
    {
      "epoch": 0.12710027100271,
      "step": 2345,
      "training_loss": 6.300509929656982
    },
    {
      "epoch": 0.12715447154471546,
      "step": 2346,
      "training_loss": 8.175697326660156
    },
    {
      "epoch": 0.12720867208672088,
      "step": 2347,
      "training_loss": 7.6241679191589355
    },
    {
      "epoch": 0.1272628726287263,
      "grad_norm": 19.317899703979492,
      "learning_rate": 1e-05,
      "loss": 7.1634,
      "step": 2348
    },
    {
      "epoch": 0.1272628726287263,
      "step": 2348,
      "training_loss": 5.1277031898498535
    },
    {
      "epoch": 0.1273170731707317,
      "step": 2349,
      "training_loss": 7.588439464569092
    },
    {
      "epoch": 0.12737127371273713,
      "step": 2350,
      "training_loss": 7.3179473876953125
    },
    {
      "epoch": 0.12742547425474254,
      "step": 2351,
      "training_loss": 6.514327049255371
    },
    {
      "epoch": 0.12747967479674796,
      "grad_norm": 27.796995162963867,
      "learning_rate": 1e-05,
      "loss": 6.6371,
      "step": 2352
    },
    {
      "epoch": 0.12747967479674796,
      "step": 2352,
      "training_loss": 6.474440097808838
    },
    {
      "epoch": 0.12753387533875338,
      "step": 2353,
      "training_loss": 5.062373161315918
    },
    {
      "epoch": 0.1275880758807588,
      "step": 2354,
      "training_loss": 6.981166362762451
    },
    {
      "epoch": 0.12764227642276424,
      "step": 2355,
      "training_loss": 7.293150901794434
    },
    {
      "epoch": 0.12769647696476966,
      "grad_norm": 23.094104766845703,
      "learning_rate": 1e-05,
      "loss": 6.4528,
      "step": 2356
    },
    {
      "epoch": 0.12769647696476966,
      "step": 2356,
      "training_loss": 6.672297477722168
    },
    {
      "epoch": 0.12775067750677507,
      "step": 2357,
      "training_loss": 7.342505931854248
    },
    {
      "epoch": 0.1278048780487805,
      "step": 2358,
      "training_loss": 7.209371089935303
    },
    {
      "epoch": 0.1278590785907859,
      "step": 2359,
      "training_loss": 7.44959831237793
    },
    {
      "epoch": 0.12791327913279132,
      "grad_norm": 15.741528511047363,
      "learning_rate": 1e-05,
      "loss": 7.1684,
      "step": 2360
    },
    {
      "epoch": 0.12791327913279132,
      "step": 2360,
      "training_loss": 7.147550582885742
    },
    {
      "epoch": 0.12796747967479674,
      "step": 2361,
      "training_loss": 9.249164581298828
    },
    {
      "epoch": 0.12802168021680216,
      "step": 2362,
      "training_loss": 7.309779167175293
    },
    {
      "epoch": 0.12807588075880758,
      "step": 2363,
      "training_loss": 7.205150127410889
    },
    {
      "epoch": 0.12813008130081302,
      "grad_norm": 16.930511474609375,
      "learning_rate": 1e-05,
      "loss": 7.7279,
      "step": 2364
    },
    {
      "epoch": 0.12813008130081302,
      "step": 2364,
      "training_loss": 6.0998759269714355
    },
    {
      "epoch": 0.12818428184281844,
      "step": 2365,
      "training_loss": 8.268562316894531
    },
    {
      "epoch": 0.12823848238482385,
      "step": 2366,
      "training_loss": 8.989215850830078
    },
    {
      "epoch": 0.12829268292682927,
      "step": 2367,
      "training_loss": 7.231636047363281
    },
    {
      "epoch": 0.1283468834688347,
      "grad_norm": 19.490684509277344,
      "learning_rate": 1e-05,
      "loss": 7.6473,
      "step": 2368
    },
    {
      "epoch": 0.1283468834688347,
      "step": 2368,
      "training_loss": 6.891519546508789
    },
    {
      "epoch": 0.1284010840108401,
      "step": 2369,
      "training_loss": 7.0190229415893555
    },
    {
      "epoch": 0.12845528455284552,
      "step": 2370,
      "training_loss": 5.833025932312012
    },
    {
      "epoch": 0.12850948509485094,
      "step": 2371,
      "training_loss": 7.318814754486084
    },
    {
      "epoch": 0.12856368563685636,
      "grad_norm": 19.696821212768555,
      "learning_rate": 1e-05,
      "loss": 6.7656,
      "step": 2372
    },
    {
      "epoch": 0.12856368563685636,
      "step": 2372,
      "training_loss": 7.996345520019531
    },
    {
      "epoch": 0.1286178861788618,
      "step": 2373,
      "training_loss": 7.4722065925598145
    },
    {
      "epoch": 0.12867208672086722,
      "step": 2374,
      "training_loss": 7.2403950691223145
    },
    {
      "epoch": 0.12872628726287264,
      "step": 2375,
      "training_loss": 6.946951866149902
    },
    {
      "epoch": 0.12878048780487805,
      "grad_norm": 18.996192932128906,
      "learning_rate": 1e-05,
      "loss": 7.414,
      "step": 2376
    },
    {
      "epoch": 0.12878048780487805,
      "step": 2376,
      "training_loss": 6.9443559646606445
    },
    {
      "epoch": 0.12883468834688347,
      "step": 2377,
      "training_loss": 10.435005187988281
    },
    {
      "epoch": 0.1288888888888889,
      "step": 2378,
      "training_loss": 8.213976860046387
    },
    {
      "epoch": 0.1289430894308943,
      "step": 2379,
      "training_loss": 7.399564743041992
    },
    {
      "epoch": 0.12899728997289972,
      "grad_norm": 18.470012664794922,
      "learning_rate": 1e-05,
      "loss": 8.2482,
      "step": 2380
    },
    {
      "epoch": 0.12899728997289972,
      "step": 2380,
      "training_loss": 6.8458147048950195
    },
    {
      "epoch": 0.12905149051490514,
      "step": 2381,
      "training_loss": 7.100912570953369
    },
    {
      "epoch": 0.12910569105691058,
      "step": 2382,
      "training_loss": 8.331835746765137
    },
    {
      "epoch": 0.129159891598916,
      "step": 2383,
      "training_loss": 7.173605442047119
    },
    {
      "epoch": 0.12921409214092142,
      "grad_norm": 17.97764778137207,
      "learning_rate": 1e-05,
      "loss": 7.363,
      "step": 2384
    },
    {
      "epoch": 0.12921409214092142,
      "step": 2384,
      "training_loss": 8.817522048950195
    },
    {
      "epoch": 0.12926829268292683,
      "step": 2385,
      "training_loss": 7.092715263366699
    },
    {
      "epoch": 0.12932249322493225,
      "step": 2386,
      "training_loss": 5.736758232116699
    },
    {
      "epoch": 0.12937669376693767,
      "step": 2387,
      "training_loss": 7.840070724487305
    },
    {
      "epoch": 0.12943089430894308,
      "grad_norm": 16.82536506652832,
      "learning_rate": 1e-05,
      "loss": 7.3718,
      "step": 2388
    },
    {
      "epoch": 0.12943089430894308,
      "step": 2388,
      "training_loss": 6.460188865661621
    },
    {
      "epoch": 0.1294850948509485,
      "step": 2389,
      "training_loss": 7.762294292449951
    },
    {
      "epoch": 0.12953929539295392,
      "step": 2390,
      "training_loss": 7.433584213256836
    },
    {
      "epoch": 0.12959349593495936,
      "step": 2391,
      "training_loss": 6.45660400390625
    },
    {
      "epoch": 0.12964769647696478,
      "grad_norm": 16.68767738342285,
      "learning_rate": 1e-05,
      "loss": 7.0282,
      "step": 2392
    },
    {
      "epoch": 0.12964769647696478,
      "step": 2392,
      "training_loss": 5.8159308433532715
    },
    {
      "epoch": 0.1297018970189702,
      "step": 2393,
      "training_loss": 7.891869068145752
    },
    {
      "epoch": 0.12975609756097561,
      "step": 2394,
      "training_loss": 6.140820503234863
    },
    {
      "epoch": 0.12981029810298103,
      "step": 2395,
      "training_loss": 6.949145317077637
    },
    {
      "epoch": 0.12986449864498645,
      "grad_norm": 24.94117546081543,
      "learning_rate": 1e-05,
      "loss": 6.6994,
      "step": 2396
    },
    {
      "epoch": 0.12986449864498645,
      "step": 2396,
      "training_loss": 6.84550142288208
    },
    {
      "epoch": 0.12991869918699187,
      "step": 2397,
      "training_loss": 6.777259349822998
    },
    {
      "epoch": 0.12997289972899728,
      "step": 2398,
      "training_loss": 7.136823654174805
    },
    {
      "epoch": 0.1300271002710027,
      "step": 2399,
      "training_loss": 8.293983459472656
    },
    {
      "epoch": 0.13008130081300814,
      "grad_norm": 22.999238967895508,
      "learning_rate": 1e-05,
      "loss": 7.2634,
      "step": 2400
    },
    {
      "epoch": 0.13008130081300814,
      "step": 2400,
      "training_loss": 6.7133636474609375
    },
    {
      "epoch": 0.13013550135501356,
      "step": 2401,
      "training_loss": 7.599674224853516
    },
    {
      "epoch": 0.13018970189701898,
      "step": 2402,
      "training_loss": 7.431965351104736
    },
    {
      "epoch": 0.1302439024390244,
      "step": 2403,
      "training_loss": 6.959190845489502
    },
    {
      "epoch": 0.1302981029810298,
      "grad_norm": 17.997512817382812,
      "learning_rate": 1e-05,
      "loss": 7.176,
      "step": 2404
    },
    {
      "epoch": 0.1302981029810298,
      "step": 2404,
      "training_loss": 7.372251033782959
    },
    {
      "epoch": 0.13035230352303523,
      "step": 2405,
      "training_loss": 6.6398234367370605
    },
    {
      "epoch": 0.13040650406504065,
      "step": 2406,
      "training_loss": 7.129350662231445
    },
    {
      "epoch": 0.13046070460704606,
      "step": 2407,
      "training_loss": 6.397891044616699
    },
    {
      "epoch": 0.13051490514905148,
      "grad_norm": 33.74754333496094,
      "learning_rate": 1e-05,
      "loss": 6.8848,
      "step": 2408
    },
    {
      "epoch": 0.13051490514905148,
      "step": 2408,
      "training_loss": 5.21405029296875
    },
    {
      "epoch": 0.1305691056910569,
      "step": 2409,
      "training_loss": 6.430717468261719
    },
    {
      "epoch": 0.13062330623306234,
      "step": 2410,
      "training_loss": 5.143457889556885
    },
    {
      "epoch": 0.13067750677506776,
      "step": 2411,
      "training_loss": 7.825999736785889
    },
    {
      "epoch": 0.13073170731707318,
      "grad_norm": 24.811138153076172,
      "learning_rate": 1e-05,
      "loss": 6.1536,
      "step": 2412
    },
    {
      "epoch": 0.13073170731707318,
      "step": 2412,
      "training_loss": 8.10484504699707
    },
    {
      "epoch": 0.1307859078590786,
      "step": 2413,
      "training_loss": 6.983096122741699
    },
    {
      "epoch": 0.130840108401084,
      "step": 2414,
      "training_loss": 5.724494934082031
    },
    {
      "epoch": 0.13089430894308943,
      "step": 2415,
      "training_loss": 6.397602081298828
    },
    {
      "epoch": 0.13094850948509484,
      "grad_norm": 16.252355575561523,
      "learning_rate": 1e-05,
      "loss": 6.8025,
      "step": 2416
    },
    {
      "epoch": 0.13094850948509484,
      "step": 2416,
      "training_loss": 6.791269302368164
    },
    {
      "epoch": 0.13100271002710026,
      "step": 2417,
      "training_loss": 6.114009380340576
    },
    {
      "epoch": 0.13105691056910568,
      "step": 2418,
      "training_loss": 5.585865497589111
    },
    {
      "epoch": 0.13111111111111112,
      "step": 2419,
      "training_loss": 6.7568440437316895
    },
    {
      "epoch": 0.13116531165311654,
      "grad_norm": 17.54572296142578,
      "learning_rate": 1e-05,
      "loss": 6.312,
      "step": 2420
    },
    {
      "epoch": 0.13116531165311654,
      "step": 2420,
      "training_loss": 6.756948947906494
    },
    {
      "epoch": 0.13121951219512196,
      "step": 2421,
      "training_loss": 6.776217460632324
    },
    {
      "epoch": 0.13127371273712737,
      "step": 2422,
      "training_loss": 6.766881465911865
    },
    {
      "epoch": 0.1313279132791328,
      "step": 2423,
      "training_loss": 7.655012130737305
    },
    {
      "epoch": 0.1313821138211382,
      "grad_norm": 18.40644645690918,
      "learning_rate": 1e-05,
      "loss": 6.9888,
      "step": 2424
    },
    {
      "epoch": 0.1313821138211382,
      "step": 2424,
      "training_loss": 7.084962844848633
    },
    {
      "epoch": 0.13143631436314362,
      "step": 2425,
      "training_loss": 6.592479228973389
    },
    {
      "epoch": 0.13149051490514904,
      "step": 2426,
      "training_loss": 6.49119758605957
    },
    {
      "epoch": 0.13154471544715446,
      "step": 2427,
      "training_loss": 6.095357418060303
    },
    {
      "epoch": 0.1315989159891599,
      "grad_norm": 40.02559280395508,
      "learning_rate": 1e-05,
      "loss": 6.566,
      "step": 2428
    },
    {
      "epoch": 0.1315989159891599,
      "step": 2428,
      "training_loss": 7.425017833709717
    },
    {
      "epoch": 0.13165311653116532,
      "step": 2429,
      "training_loss": 7.837277889251709
    },
    {
      "epoch": 0.13170731707317074,
      "step": 2430,
      "training_loss": 7.50784969329834
    },
    {
      "epoch": 0.13176151761517615,
      "step": 2431,
      "training_loss": 7.3861846923828125
    },
    {
      "epoch": 0.13181571815718157,
      "grad_norm": 17.26953887939453,
      "learning_rate": 1e-05,
      "loss": 7.5391,
      "step": 2432
    },
    {
      "epoch": 0.13181571815718157,
      "step": 2432,
      "training_loss": 7.383130073547363
    },
    {
      "epoch": 0.131869918699187,
      "step": 2433,
      "training_loss": 7.492430686950684
    },
    {
      "epoch": 0.1319241192411924,
      "step": 2434,
      "training_loss": 7.4050679206848145
    },
    {
      "epoch": 0.13197831978319782,
      "step": 2435,
      "training_loss": 5.913609027862549
    },
    {
      "epoch": 0.13203252032520324,
      "grad_norm": 22.826536178588867,
      "learning_rate": 1e-05,
      "loss": 7.0486,
      "step": 2436
    },
    {
      "epoch": 0.13203252032520324,
      "step": 2436,
      "training_loss": 7.549715042114258
    },
    {
      "epoch": 0.13208672086720868,
      "step": 2437,
      "training_loss": 8.091358184814453
    },
    {
      "epoch": 0.1321409214092141,
      "step": 2438,
      "training_loss": 8.662674903869629
    },
    {
      "epoch": 0.13219512195121952,
      "step": 2439,
      "training_loss": 7.535335063934326
    },
    {
      "epoch": 0.13224932249322494,
      "grad_norm": 39.024600982666016,
      "learning_rate": 1e-05,
      "loss": 7.9598,
      "step": 2440
    },
    {
      "epoch": 0.13224932249322494,
      "step": 2440,
      "training_loss": 6.582455158233643
    },
    {
      "epoch": 0.13230352303523035,
      "step": 2441,
      "training_loss": 7.434326171875
    },
    {
      "epoch": 0.13235772357723577,
      "step": 2442,
      "training_loss": 6.895413875579834
    },
    {
      "epoch": 0.1324119241192412,
      "step": 2443,
      "training_loss": 6.560115814208984
    },
    {
      "epoch": 0.1324661246612466,
      "grad_norm": 28.078418731689453,
      "learning_rate": 1e-05,
      "loss": 6.8681,
      "step": 2444
    },
    {
      "epoch": 0.1324661246612466,
      "step": 2444,
      "training_loss": 7.337508678436279
    },
    {
      "epoch": 0.13252032520325202,
      "step": 2445,
      "training_loss": 6.848161220550537
    },
    {
      "epoch": 0.13257452574525747,
      "step": 2446,
      "training_loss": 6.6051764488220215
    },
    {
      "epoch": 0.13262872628726288,
      "step": 2447,
      "training_loss": 7.909592151641846
    },
    {
      "epoch": 0.1326829268292683,
      "grad_norm": 29.793598175048828,
      "learning_rate": 1e-05,
      "loss": 7.1751,
      "step": 2448
    },
    {
      "epoch": 0.1326829268292683,
      "step": 2448,
      "training_loss": 7.942841529846191
    },
    {
      "epoch": 0.13273712737127372,
      "step": 2449,
      "training_loss": 7.339183807373047
    },
    {
      "epoch": 0.13279132791327913,
      "step": 2450,
      "training_loss": 7.739979267120361
    },
    {
      "epoch": 0.13284552845528455,
      "step": 2451,
      "training_loss": 7.990419864654541
    },
    {
      "epoch": 0.13289972899728997,
      "grad_norm": 19.519268035888672,
      "learning_rate": 1e-05,
      "loss": 7.7531,
      "step": 2452
    },
    {
      "epoch": 0.13289972899728997,
      "step": 2452,
      "training_loss": 7.545776844024658
    },
    {
      "epoch": 0.13295392953929538,
      "step": 2453,
      "training_loss": 5.994248390197754
    },
    {
      "epoch": 0.1330081300813008,
      "step": 2454,
      "training_loss": 7.444095134735107
    },
    {
      "epoch": 0.13306233062330625,
      "step": 2455,
      "training_loss": 7.267855167388916
    },
    {
      "epoch": 0.13311653116531166,
      "grad_norm": 19.37725067138672,
      "learning_rate": 1e-05,
      "loss": 7.063,
      "step": 2456
    },
    {
      "epoch": 0.13311653116531166,
      "step": 2456,
      "training_loss": 7.487898826599121
    },
    {
      "epoch": 0.13317073170731708,
      "step": 2457,
      "training_loss": 7.750001430511475
    },
    {
      "epoch": 0.1332249322493225,
      "step": 2458,
      "training_loss": 5.965328693389893
    },
    {
      "epoch": 0.13327913279132791,
      "step": 2459,
      "training_loss": 7.28836727142334
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 28.248313903808594,
      "learning_rate": 1e-05,
      "loss": 7.1229,
      "step": 2460
    },
    {
      "epoch": 0.13333333333333333,
      "step": 2460,
      "training_loss": 6.611510753631592
    },
    {
      "epoch": 0.13338753387533875,
      "step": 2461,
      "training_loss": 8.610699653625488
    },
    {
      "epoch": 0.13344173441734417,
      "step": 2462,
      "training_loss": 7.95767068862915
    },
    {
      "epoch": 0.13349593495934958,
      "step": 2463,
      "training_loss": 6.774229526519775
    },
    {
      "epoch": 0.13355013550135503,
      "grad_norm": 19.879915237426758,
      "learning_rate": 1e-05,
      "loss": 7.4885,
      "step": 2464
    },
    {
      "epoch": 0.13355013550135503,
      "step": 2464,
      "training_loss": 7.73317289352417
    },
    {
      "epoch": 0.13360433604336044,
      "step": 2465,
      "training_loss": 7.757315635681152
    },
    {
      "epoch": 0.13365853658536586,
      "step": 2466,
      "training_loss": 7.437290191650391
    },
    {
      "epoch": 0.13371273712737128,
      "step": 2467,
      "training_loss": 7.686046600341797
    },
    {
      "epoch": 0.1337669376693767,
      "grad_norm": 31.695730209350586,
      "learning_rate": 1e-05,
      "loss": 7.6535,
      "step": 2468
    },
    {
      "epoch": 0.1337669376693767,
      "step": 2468,
      "training_loss": 7.093227863311768
    },
    {
      "epoch": 0.1338211382113821,
      "step": 2469,
      "training_loss": 8.419249534606934
    },
    {
      "epoch": 0.13387533875338753,
      "step": 2470,
      "training_loss": 8.040327072143555
    },
    {
      "epoch": 0.13392953929539295,
      "step": 2471,
      "training_loss": 7.246767044067383
    },
    {
      "epoch": 0.13398373983739836,
      "grad_norm": 20.5117130279541,
      "learning_rate": 1e-05,
      "loss": 7.6999,
      "step": 2472
    },
    {
      "epoch": 0.13398373983739836,
      "step": 2472,
      "training_loss": 6.817556381225586
    },
    {
      "epoch": 0.13403794037940378,
      "step": 2473,
      "training_loss": 5.766285419464111
    },
    {
      "epoch": 0.13409214092140923,
      "step": 2474,
      "training_loss": 7.6953816413879395
    },
    {
      "epoch": 0.13414634146341464,
      "step": 2475,
      "training_loss": 6.556519985198975
    },
    {
      "epoch": 0.13420054200542006,
      "grad_norm": 16.53655242919922,
      "learning_rate": 1e-05,
      "loss": 6.7089,
      "step": 2476
    },
    {
      "epoch": 0.13420054200542006,
      "step": 2476,
      "training_loss": 6.717647552490234
    },
    {
      "epoch": 0.13425474254742548,
      "step": 2477,
      "training_loss": 6.436633586883545
    },
    {
      "epoch": 0.1343089430894309,
      "step": 2478,
      "training_loss": 6.870391368865967
    },
    {
      "epoch": 0.1343631436314363,
      "step": 2479,
      "training_loss": 5.539710998535156
    },
    {
      "epoch": 0.13441734417344173,
      "grad_norm": 18.03468894958496,
      "learning_rate": 1e-05,
      "loss": 6.3911,
      "step": 2480
    },
    {
      "epoch": 0.13441734417344173,
      "step": 2480,
      "training_loss": 7.421394348144531
    },
    {
      "epoch": 0.13447154471544714,
      "step": 2481,
      "training_loss": 7.195505142211914
    },
    {
      "epoch": 0.13452574525745256,
      "step": 2482,
      "training_loss": 6.995494842529297
    },
    {
      "epoch": 0.134579945799458,
      "step": 2483,
      "training_loss": 7.791748046875
    },
    {
      "epoch": 0.13463414634146342,
      "grad_norm": 23.69862174987793,
      "learning_rate": 1e-05,
      "loss": 7.351,
      "step": 2484
    },
    {
      "epoch": 0.13463414634146342,
      "step": 2484,
      "training_loss": 7.868696212768555
    },
    {
      "epoch": 0.13468834688346884,
      "step": 2485,
      "training_loss": 7.784933090209961
    },
    {
      "epoch": 0.13474254742547426,
      "step": 2486,
      "training_loss": 4.909270763397217
    },
    {
      "epoch": 0.13479674796747967,
      "step": 2487,
      "training_loss": 5.411971569061279
    },
    {
      "epoch": 0.1348509485094851,
      "grad_norm": 19.184114456176758,
      "learning_rate": 1e-05,
      "loss": 6.4937,
      "step": 2488
    },
    {
      "epoch": 0.1348509485094851,
      "step": 2488,
      "training_loss": 6.562197685241699
    },
    {
      "epoch": 0.1349051490514905,
      "step": 2489,
      "training_loss": 5.837830066680908
    },
    {
      "epoch": 0.13495934959349593,
      "step": 2490,
      "training_loss": 7.1732096672058105
    },
    {
      "epoch": 0.13501355013550134,
      "step": 2491,
      "training_loss": 7.060583591461182
    },
    {
      "epoch": 0.1350677506775068,
      "grad_norm": 19.953832626342773,
      "learning_rate": 1e-05,
      "loss": 6.6585,
      "step": 2492
    },
    {
      "epoch": 0.1350677506775068,
      "step": 2492,
      "training_loss": 7.216676235198975
    },
    {
      "epoch": 0.1351219512195122,
      "step": 2493,
      "training_loss": 6.966799736022949
    },
    {
      "epoch": 0.13517615176151762,
      "step": 2494,
      "training_loss": 7.49847936630249
    },
    {
      "epoch": 0.13523035230352304,
      "step": 2495,
      "training_loss": 4.347060203552246
    },
    {
      "epoch": 0.13528455284552846,
      "grad_norm": 35.6468620300293,
      "learning_rate": 1e-05,
      "loss": 6.5073,
      "step": 2496
    },
    {
      "epoch": 0.13528455284552846,
      "step": 2496,
      "training_loss": 6.736688613891602
    },
    {
      "epoch": 0.13533875338753387,
      "step": 2497,
      "training_loss": 6.622025489807129
    },
    {
      "epoch": 0.1353929539295393,
      "step": 2498,
      "training_loss": 7.7705488204956055
    },
    {
      "epoch": 0.1354471544715447,
      "step": 2499,
      "training_loss": 6.626865863800049
    },
    {
      "epoch": 0.13550135501355012,
      "grad_norm": 20.903995513916016,
      "learning_rate": 1e-05,
      "loss": 6.939,
      "step": 2500
    },
    {
      "epoch": 0.13550135501355012,
      "step": 2500,
      "training_loss": 6.2944865226745605
    },
    {
      "epoch": 0.13555555555555557,
      "step": 2501,
      "training_loss": 7.82246732711792
    },
    {
      "epoch": 0.13560975609756099,
      "step": 2502,
      "training_loss": 7.1069254875183105
    },
    {
      "epoch": 0.1356639566395664,
      "step": 2503,
      "training_loss": 7.054758071899414
    },
    {
      "epoch": 0.13571815718157182,
      "grad_norm": 16.166383743286133,
      "learning_rate": 1e-05,
      "loss": 7.0697,
      "step": 2504
    },
    {
      "epoch": 0.13571815718157182,
      "step": 2504,
      "training_loss": 6.958922863006592
    },
    {
      "epoch": 0.13577235772357724,
      "step": 2505,
      "training_loss": 6.614577770233154
    },
    {
      "epoch": 0.13582655826558265,
      "step": 2506,
      "training_loss": 7.345183849334717
    },
    {
      "epoch": 0.13588075880758807,
      "step": 2507,
      "training_loss": 6.462575435638428
    },
    {
      "epoch": 0.1359349593495935,
      "grad_norm": 67.98273468017578,
      "learning_rate": 1e-05,
      "loss": 6.8453,
      "step": 2508
    },
    {
      "epoch": 0.1359349593495935,
      "step": 2508,
      "training_loss": 6.912648677825928
    },
    {
      "epoch": 0.1359891598915989,
      "step": 2509,
      "training_loss": 7.60536003112793
    },
    {
      "epoch": 0.13604336043360435,
      "step": 2510,
      "training_loss": 7.151080131530762
    },
    {
      "epoch": 0.13609756097560977,
      "step": 2511,
      "training_loss": 6.910722255706787
    },
    {
      "epoch": 0.13615176151761518,
      "grad_norm": 30.519861221313477,
      "learning_rate": 1e-05,
      "loss": 7.145,
      "step": 2512
    },
    {
      "epoch": 0.13615176151761518,
      "step": 2512,
      "training_loss": 7.603370189666748
    },
    {
      "epoch": 0.1362059620596206,
      "step": 2513,
      "training_loss": 6.863235950469971
    },
    {
      "epoch": 0.13626016260162602,
      "step": 2514,
      "training_loss": 7.850847244262695
    },
    {
      "epoch": 0.13631436314363143,
      "step": 2515,
      "training_loss": 6.552717208862305
    },
    {
      "epoch": 0.13636856368563685,
      "grad_norm": 20.339080810546875,
      "learning_rate": 1e-05,
      "loss": 7.2175,
      "step": 2516
    },
    {
      "epoch": 0.13636856368563685,
      "step": 2516,
      "training_loss": 7.756474494934082
    },
    {
      "epoch": 0.13642276422764227,
      "step": 2517,
      "training_loss": 7.400864601135254
    },
    {
      "epoch": 0.13647696476964769,
      "step": 2518,
      "training_loss": 7.056346893310547
    },
    {
      "epoch": 0.13653116531165313,
      "step": 2519,
      "training_loss": 7.149261951446533
    },
    {
      "epoch": 0.13658536585365855,
      "grad_norm": 17.49959373474121,
      "learning_rate": 1e-05,
      "loss": 7.3407,
      "step": 2520
    },
    {
      "epoch": 0.13658536585365855,
      "step": 2520,
      "training_loss": 7.862305641174316
    },
    {
      "epoch": 0.13663956639566396,
      "step": 2521,
      "training_loss": 4.291009902954102
    },
    {
      "epoch": 0.13669376693766938,
      "step": 2522,
      "training_loss": 6.7023115158081055
    },
    {
      "epoch": 0.1367479674796748,
      "step": 2523,
      "training_loss": 6.281989097595215
    },
    {
      "epoch": 0.13680216802168021,
      "grad_norm": 25.290563583374023,
      "learning_rate": 1e-05,
      "loss": 6.2844,
      "step": 2524
    },
    {
      "epoch": 0.13680216802168021,
      "step": 2524,
      "training_loss": 7.517651081085205
    },
    {
      "epoch": 0.13685636856368563,
      "step": 2525,
      "training_loss": 7.353532314300537
    },
    {
      "epoch": 0.13691056910569105,
      "step": 2526,
      "training_loss": 7.968490123748779
    },
    {
      "epoch": 0.13696476964769647,
      "step": 2527,
      "training_loss": 6.855169296264648
    },
    {
      "epoch": 0.1370189701897019,
      "grad_norm": 33.6573371887207,
      "learning_rate": 1e-05,
      "loss": 7.4237,
      "step": 2528
    },
    {
      "epoch": 0.1370189701897019,
      "step": 2528,
      "training_loss": 6.804192066192627
    },
    {
      "epoch": 0.13707317073170733,
      "step": 2529,
      "training_loss": 7.79111909866333
    },
    {
      "epoch": 0.13712737127371274,
      "step": 2530,
      "training_loss": 7.622513771057129
    },
    {
      "epoch": 0.13718157181571816,
      "step": 2531,
      "training_loss": 7.120431900024414
    },
    {
      "epoch": 0.13723577235772358,
      "grad_norm": 21.62851905822754,
      "learning_rate": 1e-05,
      "loss": 7.3346,
      "step": 2532
    },
    {
      "epoch": 0.13723577235772358,
      "step": 2532,
      "training_loss": 7.346731662750244
    },
    {
      "epoch": 0.137289972899729,
      "step": 2533,
      "training_loss": 7.026785373687744
    },
    {
      "epoch": 0.1373441734417344,
      "step": 2534,
      "training_loss": 6.719282150268555
    },
    {
      "epoch": 0.13739837398373983,
      "step": 2535,
      "training_loss": 7.658319473266602
    },
    {
      "epoch": 0.13745257452574525,
      "grad_norm": 18.880321502685547,
      "learning_rate": 1e-05,
      "loss": 7.1878,
      "step": 2536
    },
    {
      "epoch": 0.13745257452574525,
      "step": 2536,
      "training_loss": 5.669539451599121
    },
    {
      "epoch": 0.13750677506775066,
      "step": 2537,
      "training_loss": 6.377847671508789
    },
    {
      "epoch": 0.1375609756097561,
      "step": 2538,
      "training_loss": 6.079473972320557
    },
    {
      "epoch": 0.13761517615176153,
      "step": 2539,
      "training_loss": 6.654615879058838
    },
    {
      "epoch": 0.13766937669376694,
      "grad_norm": 21.901548385620117,
      "learning_rate": 1e-05,
      "loss": 6.1954,
      "step": 2540
    },
    {
      "epoch": 0.13766937669376694,
      "step": 2540,
      "training_loss": 5.615240097045898
    },
    {
      "epoch": 0.13772357723577236,
      "step": 2541,
      "training_loss": 6.780345439910889
    },
    {
      "epoch": 0.13777777777777778,
      "step": 2542,
      "training_loss": 7.1078267097473145
    },
    {
      "epoch": 0.1378319783197832,
      "step": 2543,
      "training_loss": 7.742656707763672
    },
    {
      "epoch": 0.1378861788617886,
      "grad_norm": 27.089338302612305,
      "learning_rate": 1e-05,
      "loss": 6.8115,
      "step": 2544
    },
    {
      "epoch": 0.1378861788617886,
      "step": 2544,
      "training_loss": 7.532241344451904
    },
    {
      "epoch": 0.13794037940379403,
      "step": 2545,
      "training_loss": 7.4307861328125
    },
    {
      "epoch": 0.13799457994579944,
      "step": 2546,
      "training_loss": 6.04208517074585
    },
    {
      "epoch": 0.1380487804878049,
      "step": 2547,
      "training_loss": 7.581478118896484
    },
    {
      "epoch": 0.1381029810298103,
      "grad_norm": 23.44907569885254,
      "learning_rate": 1e-05,
      "loss": 7.1466,
      "step": 2548
    },
    {
      "epoch": 0.1381029810298103,
      "step": 2548,
      "training_loss": 7.040570259094238
    },
    {
      "epoch": 0.13815718157181572,
      "step": 2549,
      "training_loss": 4.930980205535889
    },
    {
      "epoch": 0.13821138211382114,
      "step": 2550,
      "training_loss": 5.0986456871032715
    },
    {
      "epoch": 0.13826558265582656,
      "step": 2551,
      "training_loss": 9.067586898803711
    },
    {
      "epoch": 0.13831978319783197,
      "grad_norm": 44.085933685302734,
      "learning_rate": 1e-05,
      "loss": 6.5344,
      "step": 2552
    },
    {
      "epoch": 0.13831978319783197,
      "step": 2552,
      "training_loss": 7.21317195892334
    },
    {
      "epoch": 0.1383739837398374,
      "step": 2553,
      "training_loss": 6.495886325836182
    },
    {
      "epoch": 0.1384281842818428,
      "step": 2554,
      "training_loss": 8.241730690002441
    },
    {
      "epoch": 0.13848238482384823,
      "step": 2555,
      "training_loss": 5.781787395477295
    },
    {
      "epoch": 0.13853658536585367,
      "grad_norm": 16.974815368652344,
      "learning_rate": 1e-05,
      "loss": 6.9331,
      "step": 2556
    },
    {
      "epoch": 0.13853658536585367,
      "step": 2556,
      "training_loss": 6.670403957366943
    },
    {
      "epoch": 0.1385907859078591,
      "step": 2557,
      "training_loss": 6.396185874938965
    },
    {
      "epoch": 0.1386449864498645,
      "step": 2558,
      "training_loss": 7.178925514221191
    },
    {
      "epoch": 0.13869918699186992,
      "step": 2559,
      "training_loss": 7.673912525177002
    },
    {
      "epoch": 0.13875338753387534,
      "grad_norm": 30.165178298950195,
      "learning_rate": 1e-05,
      "loss": 6.9799,
      "step": 2560
    },
    {
      "epoch": 0.13875338753387534,
      "step": 2560,
      "training_loss": 7.843574047088623
    },
    {
      "epoch": 0.13880758807588076,
      "step": 2561,
      "training_loss": 6.451774597167969
    },
    {
      "epoch": 0.13886178861788617,
      "step": 2562,
      "training_loss": 6.670575141906738
    },
    {
      "epoch": 0.1389159891598916,
      "step": 2563,
      "training_loss": 8.523720741271973
    },
    {
      "epoch": 0.138970189701897,
      "grad_norm": 25.6728515625,
      "learning_rate": 1e-05,
      "loss": 7.3724,
      "step": 2564
    },
    {
      "epoch": 0.138970189701897,
      "step": 2564,
      "training_loss": 5.8212127685546875
    },
    {
      "epoch": 0.13902439024390245,
      "step": 2565,
      "training_loss": 4.848917007446289
    },
    {
      "epoch": 0.13907859078590787,
      "step": 2566,
      "training_loss": 7.615372657775879
    },
    {
      "epoch": 0.13913279132791329,
      "step": 2567,
      "training_loss": 7.4322733879089355
    },
    {
      "epoch": 0.1391869918699187,
      "grad_norm": 34.8857307434082,
      "learning_rate": 1e-05,
      "loss": 6.4294,
      "step": 2568
    },
    {
      "epoch": 0.1391869918699187,
      "step": 2568,
      "training_loss": 7.6629133224487305
    },
    {
      "epoch": 0.13924119241192412,
      "step": 2569,
      "training_loss": 7.003819465637207
    },
    {
      "epoch": 0.13929539295392954,
      "step": 2570,
      "training_loss": 7.350333213806152
    },
    {
      "epoch": 0.13934959349593495,
      "step": 2571,
      "training_loss": 6.4309587478637695
    },
    {
      "epoch": 0.13940379403794037,
      "grad_norm": 17.854223251342773,
      "learning_rate": 1e-05,
      "loss": 7.112,
      "step": 2572
    },
    {
      "epoch": 0.13940379403794037,
      "step": 2572,
      "training_loss": 6.125321865081787
    },
    {
      "epoch": 0.1394579945799458,
      "step": 2573,
      "training_loss": 6.790889739990234
    },
    {
      "epoch": 0.13951219512195123,
      "step": 2574,
      "training_loss": 7.109437465667725
    },
    {
      "epoch": 0.13956639566395665,
      "step": 2575,
      "training_loss": 6.869138240814209
    },
    {
      "epoch": 0.13962059620596207,
      "grad_norm": 17.120479583740234,
      "learning_rate": 1e-05,
      "loss": 6.7237,
      "step": 2576
    },
    {
      "epoch": 0.13962059620596207,
      "step": 2576,
      "training_loss": 7.03721809387207
    },
    {
      "epoch": 0.13967479674796748,
      "step": 2577,
      "training_loss": 7.747026443481445
    },
    {
      "epoch": 0.1397289972899729,
      "step": 2578,
      "training_loss": 6.946017742156982
    },
    {
      "epoch": 0.13978319783197832,
      "step": 2579,
      "training_loss": 6.172929286956787
    },
    {
      "epoch": 0.13983739837398373,
      "grad_norm": 21.055814743041992,
      "learning_rate": 1e-05,
      "loss": 6.9758,
      "step": 2580
    },
    {
      "epoch": 0.13983739837398373,
      "step": 2580,
      "training_loss": 6.768195629119873
    },
    {
      "epoch": 0.13989159891598915,
      "step": 2581,
      "training_loss": 6.940977096557617
    },
    {
      "epoch": 0.13994579945799457,
      "step": 2582,
      "training_loss": 5.660757541656494
    },
    {
      "epoch": 0.14,
      "step": 2583,
      "training_loss": 6.886831760406494
    },
    {
      "epoch": 0.14005420054200543,
      "grad_norm": 23.115251541137695,
      "learning_rate": 1e-05,
      "loss": 6.5642,
      "step": 2584
    },
    {
      "epoch": 0.14005420054200543,
      "step": 2584,
      "training_loss": 7.897873878479004
    },
    {
      "epoch": 0.14010840108401085,
      "step": 2585,
      "training_loss": 7.113914966583252
    },
    {
      "epoch": 0.14016260162601626,
      "step": 2586,
      "training_loss": 6.489286422729492
    },
    {
      "epoch": 0.14021680216802168,
      "step": 2587,
      "training_loss": 6.589053153991699
    },
    {
      "epoch": 0.1402710027100271,
      "grad_norm": 17.186826705932617,
      "learning_rate": 1e-05,
      "loss": 7.0225,
      "step": 2588
    },
    {
      "epoch": 0.1402710027100271,
      "step": 2588,
      "training_loss": 5.904541015625
    },
    {
      "epoch": 0.14032520325203252,
      "step": 2589,
      "training_loss": 7.777456283569336
    },
    {
      "epoch": 0.14037940379403793,
      "step": 2590,
      "training_loss": 6.57749080657959
    },
    {
      "epoch": 0.14043360433604335,
      "step": 2591,
      "training_loss": 7.421666145324707
    },
    {
      "epoch": 0.1404878048780488,
      "grad_norm": 24.025636672973633,
      "learning_rate": 1e-05,
      "loss": 6.9203,
      "step": 2592
    },
    {
      "epoch": 0.1404878048780488,
      "step": 2592,
      "training_loss": 6.866920471191406
    },
    {
      "epoch": 0.1405420054200542,
      "step": 2593,
      "training_loss": 6.356772422790527
    },
    {
      "epoch": 0.14059620596205963,
      "step": 2594,
      "training_loss": 7.865425109863281
    },
    {
      "epoch": 0.14065040650406505,
      "step": 2595,
      "training_loss": 6.310418605804443
    },
    {
      "epoch": 0.14070460704607046,
      "grad_norm": 16.37093162536621,
      "learning_rate": 1e-05,
      "loss": 6.8499,
      "step": 2596
    },
    {
      "epoch": 0.14070460704607046,
      "step": 2596,
      "training_loss": 7.986814975738525
    },
    {
      "epoch": 0.14075880758807588,
      "step": 2597,
      "training_loss": 7.093797206878662
    },
    {
      "epoch": 0.1408130081300813,
      "step": 2598,
      "training_loss": 7.266373634338379
    },
    {
      "epoch": 0.1408672086720867,
      "step": 2599,
      "training_loss": 7.495234966278076
    },
    {
      "epoch": 0.14092140921409213,
      "grad_norm": 24.399343490600586,
      "learning_rate": 1e-05,
      "loss": 7.4606,
      "step": 2600
    },
    {
      "epoch": 0.14092140921409213,
      "step": 2600,
      "training_loss": 6.410475730895996
    },
    {
      "epoch": 0.14097560975609755,
      "step": 2601,
      "training_loss": 6.86810302734375
    },
    {
      "epoch": 0.141029810298103,
      "step": 2602,
      "training_loss": 7.801304340362549
    },
    {
      "epoch": 0.1410840108401084,
      "step": 2603,
      "training_loss": 7.721095085144043
    },
    {
      "epoch": 0.14113821138211383,
      "grad_norm": 24.383296966552734,
      "learning_rate": 1e-05,
      "loss": 7.2002,
      "step": 2604
    },
    {
      "epoch": 0.14113821138211383,
      "step": 2604,
      "training_loss": 7.8185529708862305
    },
    {
      "epoch": 0.14119241192411924,
      "step": 2605,
      "training_loss": 7.029047012329102
    },
    {
      "epoch": 0.14124661246612466,
      "step": 2606,
      "training_loss": 7.460811614990234
    },
    {
      "epoch": 0.14130081300813008,
      "step": 2607,
      "training_loss": 7.480808734893799
    },
    {
      "epoch": 0.1413550135501355,
      "grad_norm": 15.486342430114746,
      "learning_rate": 1e-05,
      "loss": 7.4473,
      "step": 2608
    },
    {
      "epoch": 0.1413550135501355,
      "step": 2608,
      "training_loss": 7.195689678192139
    },
    {
      "epoch": 0.1414092140921409,
      "step": 2609,
      "training_loss": 7.599444389343262
    },
    {
      "epoch": 0.14146341463414633,
      "step": 2610,
      "training_loss": 6.489224910736084
    },
    {
      "epoch": 0.14151761517615177,
      "step": 2611,
      "training_loss": 5.484253406524658
    },
    {
      "epoch": 0.1415718157181572,
      "grad_norm": 20.400161743164062,
      "learning_rate": 1e-05,
      "loss": 6.6922,
      "step": 2612
    },
    {
      "epoch": 0.1415718157181572,
      "step": 2612,
      "training_loss": 7.4624247550964355
    },
    {
      "epoch": 0.1416260162601626,
      "step": 2613,
      "training_loss": 9.60594654083252
    },
    {
      "epoch": 0.14168021680216802,
      "step": 2614,
      "training_loss": 6.936070919036865
    },
    {
      "epoch": 0.14173441734417344,
      "step": 2615,
      "training_loss": 7.8860578536987305
    },
    {
      "epoch": 0.14178861788617886,
      "grad_norm": 14.954551696777344,
      "learning_rate": 1e-05,
      "loss": 7.9726,
      "step": 2616
    },
    {
      "epoch": 0.14178861788617886,
      "step": 2616,
      "training_loss": 6.348743438720703
    },
    {
      "epoch": 0.14184281842818428,
      "step": 2617,
      "training_loss": 7.02944278717041
    },
    {
      "epoch": 0.1418970189701897,
      "step": 2618,
      "training_loss": 7.17039680480957
    },
    {
      "epoch": 0.1419512195121951,
      "step": 2619,
      "training_loss": 8.11153793334961
    },
    {
      "epoch": 0.14200542005420055,
      "grad_norm": 19.25794219970703,
      "learning_rate": 1e-05,
      "loss": 7.165,
      "step": 2620
    },
    {
      "epoch": 0.14200542005420055,
      "step": 2620,
      "training_loss": 6.456801414489746
    },
    {
      "epoch": 0.14205962059620597,
      "step": 2621,
      "training_loss": 7.190435886383057
    },
    {
      "epoch": 0.1421138211382114,
      "step": 2622,
      "training_loss": 6.131196975708008
    },
    {
      "epoch": 0.1421680216802168,
      "step": 2623,
      "training_loss": 7.137503623962402
    },
    {
      "epoch": 0.14222222222222222,
      "grad_norm": 16.46312141418457,
      "learning_rate": 1e-05,
      "loss": 6.729,
      "step": 2624
    },
    {
      "epoch": 0.14222222222222222,
      "step": 2624,
      "training_loss": 7.21945333480835
    },
    {
      "epoch": 0.14227642276422764,
      "step": 2625,
      "training_loss": 6.304246425628662
    },
    {
      "epoch": 0.14233062330623306,
      "step": 2626,
      "training_loss": 6.86186408996582
    },
    {
      "epoch": 0.14238482384823847,
      "step": 2627,
      "training_loss": 8.161542892456055
    },
    {
      "epoch": 0.1424390243902439,
      "grad_norm": 24.044849395751953,
      "learning_rate": 1e-05,
      "loss": 7.1368,
      "step": 2628
    },
    {
      "epoch": 0.1424390243902439,
      "step": 2628,
      "training_loss": 6.6491618156433105
    },
    {
      "epoch": 0.14249322493224933,
      "step": 2629,
      "training_loss": 7.545411586761475
    },
    {
      "epoch": 0.14254742547425475,
      "step": 2630,
      "training_loss": 7.041926860809326
    },
    {
      "epoch": 0.14260162601626017,
      "step": 2631,
      "training_loss": 7.190550327301025
    },
    {
      "epoch": 0.14265582655826559,
      "grad_norm": 17.28243064880371,
      "learning_rate": 1e-05,
      "loss": 7.1068,
      "step": 2632
    },
    {
      "epoch": 0.14265582655826559,
      "step": 2632,
      "training_loss": 7.361677646636963
    },
    {
      "epoch": 0.142710027100271,
      "step": 2633,
      "training_loss": 7.30226469039917
    },
    {
      "epoch": 0.14276422764227642,
      "step": 2634,
      "training_loss": 8.460108757019043
    },
    {
      "epoch": 0.14281842818428184,
      "step": 2635,
      "training_loss": 7.010674953460693
    },
    {
      "epoch": 0.14287262872628725,
      "grad_norm": 17.456485748291016,
      "learning_rate": 1e-05,
      "loss": 7.5337,
      "step": 2636
    },
    {
      "epoch": 0.14287262872628725,
      "step": 2636,
      "training_loss": 7.053275108337402
    },
    {
      "epoch": 0.14292682926829267,
      "step": 2637,
      "training_loss": 7.076700687408447
    },
    {
      "epoch": 0.14298102981029812,
      "step": 2638,
      "training_loss": 7.728366851806641
    },
    {
      "epoch": 0.14303523035230353,
      "step": 2639,
      "training_loss": 7.305314540863037
    },
    {
      "epoch": 0.14308943089430895,
      "grad_norm": 21.654850006103516,
      "learning_rate": 1e-05,
      "loss": 7.2909,
      "step": 2640
    },
    {
      "epoch": 0.14308943089430895,
      "step": 2640,
      "training_loss": 6.181407928466797
    },
    {
      "epoch": 0.14314363143631437,
      "step": 2641,
      "training_loss": 7.536325454711914
    },
    {
      "epoch": 0.14319783197831978,
      "step": 2642,
      "training_loss": 7.40533971786499
    },
    {
      "epoch": 0.1432520325203252,
      "step": 2643,
      "training_loss": 6.647276878356934
    },
    {
      "epoch": 0.14330623306233062,
      "grad_norm": 14.121835708618164,
      "learning_rate": 1e-05,
      "loss": 6.9426,
      "step": 2644
    },
    {
      "epoch": 0.14330623306233062,
      "step": 2644,
      "training_loss": 7.893548488616943
    },
    {
      "epoch": 0.14336043360433603,
      "step": 2645,
      "training_loss": 6.1227240562438965
    },
    {
      "epoch": 0.14341463414634145,
      "step": 2646,
      "training_loss": 8.516410827636719
    },
    {
      "epoch": 0.1434688346883469,
      "step": 2647,
      "training_loss": 7.292142391204834
    },
    {
      "epoch": 0.1435230352303523,
      "grad_norm": 17.110750198364258,
      "learning_rate": 1e-05,
      "loss": 7.4562,
      "step": 2648
    },
    {
      "epoch": 0.1435230352303523,
      "step": 2648,
      "training_loss": 6.254172325134277
    },
    {
      "epoch": 0.14357723577235773,
      "step": 2649,
      "training_loss": 5.857818603515625
    },
    {
      "epoch": 0.14363143631436315,
      "step": 2650,
      "training_loss": 6.030587196350098
    },
    {
      "epoch": 0.14368563685636856,
      "step": 2651,
      "training_loss": 7.093649387359619
    },
    {
      "epoch": 0.14373983739837398,
      "grad_norm": 25.02707290649414,
      "learning_rate": 1e-05,
      "loss": 6.3091,
      "step": 2652
    },
    {
      "epoch": 0.14373983739837398,
      "step": 2652,
      "training_loss": 7.9951934814453125
    },
    {
      "epoch": 0.1437940379403794,
      "step": 2653,
      "training_loss": 6.886555194854736
    },
    {
      "epoch": 0.14384823848238482,
      "step": 2654,
      "training_loss": 7.544373989105225
    },
    {
      "epoch": 0.14390243902439023,
      "step": 2655,
      "training_loss": 5.216752052307129
    },
    {
      "epoch": 0.14395663956639568,
      "grad_norm": 17.857152938842773,
      "learning_rate": 1e-05,
      "loss": 6.9107,
      "step": 2656
    },
    {
      "epoch": 0.14395663956639568,
      "step": 2656,
      "training_loss": 6.886301040649414
    },
    {
      "epoch": 0.1440108401084011,
      "step": 2657,
      "training_loss": 7.307267665863037
    },
    {
      "epoch": 0.1440650406504065,
      "step": 2658,
      "training_loss": 7.07749080657959
    },
    {
      "epoch": 0.14411924119241193,
      "step": 2659,
      "training_loss": 5.898308753967285
    },
    {
      "epoch": 0.14417344173441735,
      "grad_norm": 21.145387649536133,
      "learning_rate": 1e-05,
      "loss": 6.7923,
      "step": 2660
    },
    {
      "epoch": 0.14417344173441735,
      "step": 2660,
      "training_loss": 8.123764038085938
    },
    {
      "epoch": 0.14422764227642276,
      "step": 2661,
      "training_loss": 6.961852073669434
    },
    {
      "epoch": 0.14428184281842818,
      "step": 2662,
      "training_loss": 6.3797736167907715
    },
    {
      "epoch": 0.1443360433604336,
      "step": 2663,
      "training_loss": 7.33880615234375
    },
    {
      "epoch": 0.144390243902439,
      "grad_norm": 23.6699275970459,
      "learning_rate": 1e-05,
      "loss": 7.201,
      "step": 2664
    },
    {
      "epoch": 0.144390243902439,
      "step": 2664,
      "training_loss": 6.75139045715332
    },
    {
      "epoch": 0.14444444444444443,
      "step": 2665,
      "training_loss": 6.73094367980957
    },
    {
      "epoch": 0.14449864498644988,
      "step": 2666,
      "training_loss": 6.6419501304626465
    },
    {
      "epoch": 0.1445528455284553,
      "step": 2667,
      "training_loss": 5.977971076965332
    },
    {
      "epoch": 0.1446070460704607,
      "grad_norm": 21.326229095458984,
      "learning_rate": 1e-05,
      "loss": 6.5256,
      "step": 2668
    },
    {
      "epoch": 0.1446070460704607,
      "step": 2668,
      "training_loss": 8.152270317077637
    },
    {
      "epoch": 0.14466124661246613,
      "step": 2669,
      "training_loss": 7.795559406280518
    },
    {
      "epoch": 0.14471544715447154,
      "step": 2670,
      "training_loss": 6.828698635101318
    },
    {
      "epoch": 0.14476964769647696,
      "step": 2671,
      "training_loss": 8.477325439453125
    },
    {
      "epoch": 0.14482384823848238,
      "grad_norm": 23.462650299072266,
      "learning_rate": 1e-05,
      "loss": 7.8135,
      "step": 2672
    },
    {
      "epoch": 0.14482384823848238,
      "step": 2672,
      "training_loss": 7.181950092315674
    },
    {
      "epoch": 0.1448780487804878,
      "step": 2673,
      "training_loss": 6.300014972686768
    },
    {
      "epoch": 0.1449322493224932,
      "step": 2674,
      "training_loss": 7.548744201660156
    },
    {
      "epoch": 0.14498644986449866,
      "step": 2675,
      "training_loss": 7.495262622833252
    },
    {
      "epoch": 0.14504065040650407,
      "grad_norm": 31.267736434936523,
      "learning_rate": 1e-05,
      "loss": 7.1315,
      "step": 2676
    },
    {
      "epoch": 0.14504065040650407,
      "step": 2676,
      "training_loss": 4.403129577636719
    },
    {
      "epoch": 0.1450948509485095,
      "step": 2677,
      "training_loss": 7.270368576049805
    },
    {
      "epoch": 0.1451490514905149,
      "step": 2678,
      "training_loss": 7.146984577178955
    },
    {
      "epoch": 0.14520325203252032,
      "step": 2679,
      "training_loss": 6.9243364334106445
    },
    {
      "epoch": 0.14525745257452574,
      "grad_norm": 26.08025550842285,
      "learning_rate": 1e-05,
      "loss": 6.4362,
      "step": 2680
    },
    {
      "epoch": 0.14525745257452574,
      "step": 2680,
      "training_loss": 8.023134231567383
    },
    {
      "epoch": 0.14531165311653116,
      "step": 2681,
      "training_loss": 8.00939655303955
    },
    {
      "epoch": 0.14536585365853658,
      "step": 2682,
      "training_loss": 7.32981014251709
    },
    {
      "epoch": 0.145420054200542,
      "step": 2683,
      "training_loss": 7.435822010040283
    },
    {
      "epoch": 0.14547425474254744,
      "grad_norm": 16.1890869140625,
      "learning_rate": 1e-05,
      "loss": 7.6995,
      "step": 2684
    },
    {
      "epoch": 0.14547425474254744,
      "step": 2684,
      "training_loss": 4.415499687194824
    },
    {
      "epoch": 0.14552845528455285,
      "step": 2685,
      "training_loss": 6.979267120361328
    },
    {
      "epoch": 0.14558265582655827,
      "step": 2686,
      "training_loss": 7.1596808433532715
    },
    {
      "epoch": 0.1456368563685637,
      "step": 2687,
      "training_loss": 6.932631492614746
    },
    {
      "epoch": 0.1456910569105691,
      "grad_norm": 46.051815032958984,
      "learning_rate": 1e-05,
      "loss": 6.3718,
      "step": 2688
    },
    {
      "epoch": 0.1456910569105691,
      "step": 2688,
      "training_loss": 7.968842506408691
    },
    {
      "epoch": 0.14574525745257452,
      "step": 2689,
      "training_loss": 5.563681125640869
    },
    {
      "epoch": 0.14579945799457994,
      "step": 2690,
      "training_loss": 5.70306396484375
    },
    {
      "epoch": 0.14585365853658536,
      "step": 2691,
      "training_loss": 7.031442165374756
    },
    {
      "epoch": 0.14590785907859077,
      "grad_norm": 30.15680503845215,
      "learning_rate": 1e-05,
      "loss": 6.5668,
      "step": 2692
    },
    {
      "epoch": 0.14590785907859077,
      "step": 2692,
      "training_loss": 5.624709129333496
    },
    {
      "epoch": 0.14596205962059622,
      "step": 2693,
      "training_loss": 6.83181095123291
    },
    {
      "epoch": 0.14601626016260164,
      "step": 2694,
      "training_loss": 7.501360893249512
    },
    {
      "epoch": 0.14607046070460705,
      "step": 2695,
      "training_loss": 5.888236999511719
    },
    {
      "epoch": 0.14612466124661247,
      "grad_norm": 28.95340919494629,
      "learning_rate": 1e-05,
      "loss": 6.4615,
      "step": 2696
    },
    {
      "epoch": 0.14612466124661247,
      "step": 2696,
      "training_loss": 7.543758869171143
    },
    {
      "epoch": 0.1461788617886179,
      "step": 2697,
      "training_loss": 6.650112628936768
    },
    {
      "epoch": 0.1462330623306233,
      "step": 2698,
      "training_loss": 6.910295486450195
    },
    {
      "epoch": 0.14628726287262872,
      "step": 2699,
      "training_loss": 6.898913383483887
    },
    {
      "epoch": 0.14634146341463414,
      "grad_norm": 29.399581909179688,
      "learning_rate": 1e-05,
      "loss": 7.0008,
      "step": 2700
    },
    {
      "epoch": 0.14634146341463414,
      "step": 2700,
      "training_loss": 5.190524101257324
    },
    {
      "epoch": 0.14639566395663955,
      "step": 2701,
      "training_loss": 7.194784164428711
    },
    {
      "epoch": 0.146449864498645,
      "step": 2702,
      "training_loss": 7.428859233856201
    },
    {
      "epoch": 0.14650406504065042,
      "step": 2703,
      "training_loss": 7.772037982940674
    },
    {
      "epoch": 0.14655826558265583,
      "grad_norm": 25.330265045166016,
      "learning_rate": 1e-05,
      "loss": 6.8966,
      "step": 2704
    },
    {
      "epoch": 0.14655826558265583,
      "step": 2704,
      "training_loss": 6.064313888549805
    },
    {
      "epoch": 0.14661246612466125,
      "step": 2705,
      "training_loss": 6.1325201988220215
    },
    {
      "epoch": 0.14666666666666667,
      "step": 2706,
      "training_loss": 6.746006011962891
    },
    {
      "epoch": 0.14672086720867208,
      "step": 2707,
      "training_loss": 7.027482032775879
    },
    {
      "epoch": 0.1467750677506775,
      "grad_norm": 29.07795524597168,
      "learning_rate": 1e-05,
      "loss": 6.4926,
      "step": 2708
    },
    {
      "epoch": 0.1467750677506775,
      "step": 2708,
      "training_loss": 6.280714511871338
    },
    {
      "epoch": 0.14682926829268292,
      "step": 2709,
      "training_loss": 6.554640769958496
    },
    {
      "epoch": 0.14688346883468834,
      "step": 2710,
      "training_loss": 7.091335773468018
    },
    {
      "epoch": 0.14693766937669378,
      "step": 2711,
      "training_loss": 7.897545337677002
    },
    {
      "epoch": 0.1469918699186992,
      "grad_norm": 23.965288162231445,
      "learning_rate": 1e-05,
      "loss": 6.9561,
      "step": 2712
    },
    {
      "epoch": 0.1469918699186992,
      "step": 2712,
      "training_loss": 8.26445198059082
    },
    {
      "epoch": 0.14704607046070461,
      "step": 2713,
      "training_loss": 6.800072193145752
    },
    {
      "epoch": 0.14710027100271003,
      "step": 2714,
      "training_loss": 6.746550559997559
    },
    {
      "epoch": 0.14715447154471545,
      "step": 2715,
      "training_loss": 4.743051052093506
    },
    {
      "epoch": 0.14720867208672087,
      "grad_norm": 26.738666534423828,
      "learning_rate": 1e-05,
      "loss": 6.6385,
      "step": 2716
    },
    {
      "epoch": 0.14720867208672087,
      "step": 2716,
      "training_loss": 6.969179153442383
    },
    {
      "epoch": 0.14726287262872628,
      "step": 2717,
      "training_loss": 7.300539493560791
    },
    {
      "epoch": 0.1473170731707317,
      "step": 2718,
      "training_loss": 7.881967067718506
    },
    {
      "epoch": 0.14737127371273712,
      "step": 2719,
      "training_loss": 8.212117195129395
    },
    {
      "epoch": 0.14742547425474256,
      "grad_norm": 43.5081901550293,
      "learning_rate": 1e-05,
      "loss": 7.591,
      "step": 2720
    },
    {
      "epoch": 0.14742547425474256,
      "step": 2720,
      "training_loss": 7.683688163757324
    },
    {
      "epoch": 0.14747967479674798,
      "step": 2721,
      "training_loss": 6.935351371765137
    },
    {
      "epoch": 0.1475338753387534,
      "step": 2722,
      "training_loss": 7.5208563804626465
    },
    {
      "epoch": 0.1475880758807588,
      "step": 2723,
      "training_loss": 6.421712398529053
    },
    {
      "epoch": 0.14764227642276423,
      "grad_norm": 15.367502212524414,
      "learning_rate": 1e-05,
      "loss": 7.1404,
      "step": 2724
    },
    {
      "epoch": 0.14764227642276423,
      "step": 2724,
      "training_loss": 8.721485137939453
    },
    {
      "epoch": 0.14769647696476965,
      "step": 2725,
      "training_loss": 7.487117290496826
    },
    {
      "epoch": 0.14775067750677506,
      "step": 2726,
      "training_loss": 6.056400775909424
    },
    {
      "epoch": 0.14780487804878048,
      "step": 2727,
      "training_loss": 8.04054069519043
    },
    {
      "epoch": 0.1478590785907859,
      "grad_norm": 24.12108612060547,
      "learning_rate": 1e-05,
      "loss": 7.5764,
      "step": 2728
    },
    {
      "epoch": 0.1478590785907859,
      "step": 2728,
      "training_loss": 5.131184101104736
    },
    {
      "epoch": 0.14791327913279131,
      "step": 2729,
      "training_loss": 7.3170342445373535
    },
    {
      "epoch": 0.14796747967479676,
      "step": 2730,
      "training_loss": 6.484695911407471
    },
    {
      "epoch": 0.14802168021680218,
      "step": 2731,
      "training_loss": 6.992718696594238
    },
    {
      "epoch": 0.1480758807588076,
      "grad_norm": 19.52605628967285,
      "learning_rate": 1e-05,
      "loss": 6.4814,
      "step": 2732
    },
    {
      "epoch": 0.1480758807588076,
      "step": 2732,
      "training_loss": 7.523624420166016
    },
    {
      "epoch": 0.148130081300813,
      "step": 2733,
      "training_loss": 7.454044818878174
    },
    {
      "epoch": 0.14818428184281843,
      "step": 2734,
      "training_loss": 6.95874547958374
    },
    {
      "epoch": 0.14823848238482384,
      "step": 2735,
      "training_loss": 8.376072883605957
    },
    {
      "epoch": 0.14829268292682926,
      "grad_norm": 26.37027931213379,
      "learning_rate": 1e-05,
      "loss": 7.5781,
      "step": 2736
    },
    {
      "epoch": 0.14829268292682926,
      "step": 2736,
      "training_loss": 7.0401201248168945
    },
    {
      "epoch": 0.14834688346883468,
      "step": 2737,
      "training_loss": 7.085273742675781
    },
    {
      "epoch": 0.1484010840108401,
      "step": 2738,
      "training_loss": 6.184514999389648
    },
    {
      "epoch": 0.14845528455284554,
      "step": 2739,
      "training_loss": 8.308947563171387
    },
    {
      "epoch": 0.14850948509485096,
      "grad_norm": 38.29178237915039,
      "learning_rate": 1e-05,
      "loss": 7.1547,
      "step": 2740
    },
    {
      "epoch": 0.14850948509485096,
      "step": 2740,
      "training_loss": 7.816874027252197
    },
    {
      "epoch": 0.14856368563685637,
      "step": 2741,
      "training_loss": 7.641136646270752
    },
    {
      "epoch": 0.1486178861788618,
      "step": 2742,
      "training_loss": 6.590243339538574
    },
    {
      "epoch": 0.1486720867208672,
      "step": 2743,
      "training_loss": 6.91975212097168
    },
    {
      "epoch": 0.14872628726287263,
      "grad_norm": 28.48296546936035,
      "learning_rate": 1e-05,
      "loss": 7.242,
      "step": 2744
    },
    {
      "epoch": 0.14872628726287263,
      "step": 2744,
      "training_loss": 7.07703971862793
    },
    {
      "epoch": 0.14878048780487804,
      "step": 2745,
      "training_loss": 5.6660261154174805
    },
    {
      "epoch": 0.14883468834688346,
      "step": 2746,
      "training_loss": 4.158545970916748
    },
    {
      "epoch": 0.14888888888888888,
      "step": 2747,
      "training_loss": 5.982562065124512
    },
    {
      "epoch": 0.14894308943089432,
      "grad_norm": 22.693960189819336,
      "learning_rate": 1e-05,
      "loss": 5.721,
      "step": 2748
    },
    {
      "epoch": 0.14894308943089432,
      "step": 2748,
      "training_loss": 4.633694648742676
    },
    {
      "epoch": 0.14899728997289974,
      "step": 2749,
      "training_loss": 7.384731292724609
    },
    {
      "epoch": 0.14905149051490515,
      "step": 2750,
      "training_loss": 6.685415744781494
    },
    {
      "epoch": 0.14910569105691057,
      "step": 2751,
      "training_loss": 5.716179847717285
    },
    {
      "epoch": 0.149159891598916,
      "grad_norm": 29.589332580566406,
      "learning_rate": 1e-05,
      "loss": 6.105,
      "step": 2752
    },
    {
      "epoch": 0.149159891598916,
      "step": 2752,
      "training_loss": 5.508753776550293
    },
    {
      "epoch": 0.1492140921409214,
      "step": 2753,
      "training_loss": 6.403130531311035
    },
    {
      "epoch": 0.14926829268292682,
      "step": 2754,
      "training_loss": 6.1644158363342285
    },
    {
      "epoch": 0.14932249322493224,
      "step": 2755,
      "training_loss": 7.2176713943481445
    },
    {
      "epoch": 0.14937669376693766,
      "grad_norm": 15.610504150390625,
      "learning_rate": 1e-05,
      "loss": 6.3235,
      "step": 2756
    },
    {
      "epoch": 0.14937669376693766,
      "step": 2756,
      "training_loss": 7.52488899230957
    },
    {
      "epoch": 0.1494308943089431,
      "step": 2757,
      "training_loss": 7.013801097869873
    },
    {
      "epoch": 0.14948509485094852,
      "step": 2758,
      "training_loss": 6.904720306396484
    },
    {
      "epoch": 0.14953929539295394,
      "step": 2759,
      "training_loss": 7.126418590545654
    },
    {
      "epoch": 0.14959349593495935,
      "grad_norm": 24.772092819213867,
      "learning_rate": 1e-05,
      "loss": 7.1425,
      "step": 2760
    },
    {
      "epoch": 0.14959349593495935,
      "step": 2760,
      "training_loss": 6.748104095458984
    },
    {
      "epoch": 0.14964769647696477,
      "step": 2761,
      "training_loss": 4.5430073738098145
    },
    {
      "epoch": 0.1497018970189702,
      "step": 2762,
      "training_loss": 6.75314474105835
    },
    {
      "epoch": 0.1497560975609756,
      "step": 2763,
      "training_loss": 6.927399158477783
    },
    {
      "epoch": 0.14981029810298102,
      "grad_norm": 26.12412452697754,
      "learning_rate": 1e-05,
      "loss": 6.2429,
      "step": 2764
    },
    {
      "epoch": 0.14981029810298102,
      "step": 2764,
      "training_loss": 6.580842971801758
    },
    {
      "epoch": 0.14986449864498644,
      "step": 2765,
      "training_loss": 7.397954940795898
    },
    {
      "epoch": 0.14991869918699188,
      "step": 2766,
      "training_loss": 7.401541233062744
    },
    {
      "epoch": 0.1499728997289973,
      "step": 2767,
      "training_loss": 7.023067474365234
    },
    {
      "epoch": 0.15002710027100272,
      "grad_norm": 15.717175483703613,
      "learning_rate": 1e-05,
      "loss": 7.1009,
      "step": 2768
    },
    {
      "epoch": 0.15002710027100272,
      "step": 2768,
      "training_loss": 5.815362930297852
    },
    {
      "epoch": 0.15008130081300813,
      "step": 2769,
      "training_loss": 4.521266937255859
    },
    {
      "epoch": 0.15013550135501355,
      "step": 2770,
      "training_loss": 7.441351890563965
    },
    {
      "epoch": 0.15018970189701897,
      "step": 2771,
      "training_loss": 5.383084774017334
    },
    {
      "epoch": 0.15024390243902438,
      "grad_norm": 21.512527465820312,
      "learning_rate": 1e-05,
      "loss": 5.7903,
      "step": 2772
    },
    {
      "epoch": 0.15024390243902438,
      "step": 2772,
      "training_loss": 5.500888347625732
    },
    {
      "epoch": 0.1502981029810298,
      "step": 2773,
      "training_loss": 6.990847110748291
    },
    {
      "epoch": 0.15035230352303522,
      "step": 2774,
      "training_loss": 6.203507900238037
    },
    {
      "epoch": 0.15040650406504066,
      "step": 2775,
      "training_loss": 6.280167579650879
    },
    {
      "epoch": 0.15046070460704608,
      "grad_norm": 17.69522476196289,
      "learning_rate": 1e-05,
      "loss": 6.2439,
      "step": 2776
    },
    {
      "epoch": 0.15046070460704608,
      "step": 2776,
      "training_loss": 7.131898880004883
    },
    {
      "epoch": 0.1505149051490515,
      "step": 2777,
      "training_loss": 8.264874458312988
    },
    {
      "epoch": 0.15056910569105691,
      "step": 2778,
      "training_loss": 6.983683109283447
    },
    {
      "epoch": 0.15062330623306233,
      "step": 2779,
      "training_loss": 6.052300930023193
    },
    {
      "epoch": 0.15067750677506775,
      "grad_norm": 15.918036460876465,
      "learning_rate": 1e-05,
      "loss": 7.1082,
      "step": 2780
    },
    {
      "epoch": 0.15067750677506775,
      "step": 2780,
      "training_loss": 7.474325180053711
    },
    {
      "epoch": 0.15073170731707317,
      "step": 2781,
      "training_loss": 7.750882148742676
    },
    {
      "epoch": 0.15078590785907858,
      "step": 2782,
      "training_loss": 7.630563735961914
    },
    {
      "epoch": 0.150840108401084,
      "step": 2783,
      "training_loss": 6.802700996398926
    },
    {
      "epoch": 0.15089430894308944,
      "grad_norm": 22.999780654907227,
      "learning_rate": 1e-05,
      "loss": 7.4146,
      "step": 2784
    },
    {
      "epoch": 0.15089430894308944,
      "step": 2784,
      "training_loss": 6.824918746948242
    },
    {
      "epoch": 0.15094850948509486,
      "step": 2785,
      "training_loss": 7.499577045440674
    },
    {
      "epoch": 0.15100271002710028,
      "step": 2786,
      "training_loss": 7.234065055847168
    },
    {
      "epoch": 0.1510569105691057,
      "step": 2787,
      "training_loss": 7.381229877471924
    },
    {
      "epoch": 0.1511111111111111,
      "grad_norm": 28.34613800048828,
      "learning_rate": 1e-05,
      "loss": 7.2349,
      "step": 2788
    },
    {
      "epoch": 0.1511111111111111,
      "step": 2788,
      "training_loss": 6.11315393447876
    },
    {
      "epoch": 0.15116531165311653,
      "step": 2789,
      "training_loss": 6.908787250518799
    },
    {
      "epoch": 0.15121951219512195,
      "step": 2790,
      "training_loss": 6.644617557525635
    },
    {
      "epoch": 0.15127371273712736,
      "step": 2791,
      "training_loss": 8.45887565612793
    },
    {
      "epoch": 0.15132791327913278,
      "grad_norm": 18.90997886657715,
      "learning_rate": 1e-05,
      "loss": 7.0314,
      "step": 2792
    },
    {
      "epoch": 0.15132791327913278,
      "step": 2792,
      "training_loss": 6.93646240234375
    },
    {
      "epoch": 0.1513821138211382,
      "step": 2793,
      "training_loss": 7.207126617431641
    },
    {
      "epoch": 0.15143631436314364,
      "step": 2794,
      "training_loss": 7.815772533416748
    },
    {
      "epoch": 0.15149051490514906,
      "step": 2795,
      "training_loss": 7.011143207550049
    },
    {
      "epoch": 0.15154471544715448,
      "grad_norm": 17.711400985717773,
      "learning_rate": 1e-05,
      "loss": 7.2426,
      "step": 2796
    },
    {
      "epoch": 0.15154471544715448,
      "step": 2796,
      "training_loss": 7.868875503540039
    },
    {
      "epoch": 0.1515989159891599,
      "step": 2797,
      "training_loss": 5.351524353027344
    },
    {
      "epoch": 0.1516531165311653,
      "step": 2798,
      "training_loss": 6.708816051483154
    },
    {
      "epoch": 0.15170731707317073,
      "step": 2799,
      "training_loss": 7.103667736053467
    },
    {
      "epoch": 0.15176151761517614,
      "grad_norm": 21.83403968811035,
      "learning_rate": 1e-05,
      "loss": 6.7582,
      "step": 2800
    },
    {
      "epoch": 0.15176151761517614,
      "step": 2800,
      "training_loss": 7.187362194061279
    },
    {
      "epoch": 0.15181571815718156,
      "step": 2801,
      "training_loss": 5.375797271728516
    },
    {
      "epoch": 0.15186991869918698,
      "step": 2802,
      "training_loss": 7.37620210647583
    },
    {
      "epoch": 0.15192411924119242,
      "step": 2803,
      "training_loss": 5.788379192352295
    },
    {
      "epoch": 0.15197831978319784,
      "grad_norm": 48.4687614440918,
      "learning_rate": 1e-05,
      "loss": 6.4319,
      "step": 2804
    },
    {
      "epoch": 0.15197831978319784,
      "step": 2804,
      "training_loss": 6.213126182556152
    },
    {
      "epoch": 0.15203252032520326,
      "step": 2805,
      "training_loss": 5.87962532043457
    },
    {
      "epoch": 0.15208672086720867,
      "step": 2806,
      "training_loss": 6.834182262420654
    },
    {
      "epoch": 0.1521409214092141,
      "step": 2807,
      "training_loss": 6.190089225769043
    },
    {
      "epoch": 0.1521951219512195,
      "grad_norm": 18.155990600585938,
      "learning_rate": 1e-05,
      "loss": 6.2793,
      "step": 2808
    },
    {
      "epoch": 0.1521951219512195,
      "step": 2808,
      "training_loss": 6.960704326629639
    },
    {
      "epoch": 0.15224932249322493,
      "step": 2809,
      "training_loss": 6.816217422485352
    },
    {
      "epoch": 0.15230352303523034,
      "step": 2810,
      "training_loss": 6.386111736297607
    },
    {
      "epoch": 0.15235772357723576,
      "step": 2811,
      "training_loss": 8.093706130981445
    },
    {
      "epoch": 0.1524119241192412,
      "grad_norm": 34.86417007446289,
      "learning_rate": 1e-05,
      "loss": 7.0642,
      "step": 2812
    },
    {
      "epoch": 0.1524119241192412,
      "step": 2812,
      "training_loss": 7.025763034820557
    },
    {
      "epoch": 0.15246612466124662,
      "step": 2813,
      "training_loss": 5.740941524505615
    },
    {
      "epoch": 0.15252032520325204,
      "step": 2814,
      "training_loss": 6.446156024932861
    },
    {
      "epoch": 0.15257452574525746,
      "step": 2815,
      "training_loss": 7.457784175872803
    },
    {
      "epoch": 0.15262872628726287,
      "grad_norm": 17.195314407348633,
      "learning_rate": 1e-05,
      "loss": 6.6677,
      "step": 2816
    },
    {
      "epoch": 0.15262872628726287,
      "step": 2816,
      "training_loss": 6.662289619445801
    },
    {
      "epoch": 0.1526829268292683,
      "step": 2817,
      "training_loss": 7.3425397872924805
    },
    {
      "epoch": 0.1527371273712737,
      "step": 2818,
      "training_loss": 6.903387069702148
    },
    {
      "epoch": 0.15279132791327912,
      "step": 2819,
      "training_loss": 7.31217098236084
    },
    {
      "epoch": 0.15284552845528454,
      "grad_norm": 23.215063095092773,
      "learning_rate": 1e-05,
      "loss": 7.0551,
      "step": 2820
    },
    {
      "epoch": 0.15284552845528454,
      "step": 2820,
      "training_loss": 7.486176490783691
    },
    {
      "epoch": 0.15289972899728999,
      "step": 2821,
      "training_loss": 7.081368446350098
    },
    {
      "epoch": 0.1529539295392954,
      "step": 2822,
      "training_loss": 6.873575210571289
    },
    {
      "epoch": 0.15300813008130082,
      "step": 2823,
      "training_loss": 7.563320636749268
    },
    {
      "epoch": 0.15306233062330624,
      "grad_norm": 17.66584587097168,
      "learning_rate": 1e-05,
      "loss": 7.2511,
      "step": 2824
    },
    {
      "epoch": 0.15306233062330624,
      "step": 2824,
      "training_loss": 5.7166924476623535
    },
    {
      "epoch": 0.15311653116531165,
      "step": 2825,
      "training_loss": 6.489023685455322
    },
    {
      "epoch": 0.15317073170731707,
      "step": 2826,
      "training_loss": 6.718019008636475
    },
    {
      "epoch": 0.1532249322493225,
      "step": 2827,
      "training_loss": 7.365563869476318
    },
    {
      "epoch": 0.1532791327913279,
      "grad_norm": 21.14395523071289,
      "learning_rate": 1e-05,
      "loss": 6.5723,
      "step": 2828
    },
    {
      "epoch": 0.1532791327913279,
      "step": 2828,
      "training_loss": 7.259206295013428
    },
    {
      "epoch": 0.15333333333333332,
      "step": 2829,
      "training_loss": 6.249456405639648
    },
    {
      "epoch": 0.15338753387533877,
      "step": 2830,
      "training_loss": 5.700502872467041
    },
    {
      "epoch": 0.15344173441734418,
      "step": 2831,
      "training_loss": 7.0566725730896
    },
    {
      "epoch": 0.1534959349593496,
      "grad_norm": 14.497147560119629,
      "learning_rate": 1e-05,
      "loss": 6.5665,
      "step": 2832
    },
    {
      "epoch": 0.1534959349593496,
      "step": 2832,
      "training_loss": 5.515836238861084
    },
    {
      "epoch": 0.15355013550135502,
      "step": 2833,
      "training_loss": 7.052157402038574
    },
    {
      "epoch": 0.15360433604336043,
      "step": 2834,
      "training_loss": 6.649300575256348
    },
    {
      "epoch": 0.15365853658536585,
      "step": 2835,
      "training_loss": 7.26254940032959
    },
    {
      "epoch": 0.15371273712737127,
      "grad_norm": 20.538881301879883,
      "learning_rate": 1e-05,
      "loss": 6.62,
      "step": 2836
    },
    {
      "epoch": 0.15371273712737127,
      "step": 2836,
      "training_loss": 7.1271796226501465
    },
    {
      "epoch": 0.15376693766937669,
      "step": 2837,
      "training_loss": 6.447570323944092
    },
    {
      "epoch": 0.1538211382113821,
      "step": 2838,
      "training_loss": 6.9930100440979
    },
    {
      "epoch": 0.15387533875338755,
      "step": 2839,
      "training_loss": 7.503795146942139
    },
    {
      "epoch": 0.15392953929539296,
      "grad_norm": 20.24793243408203,
      "learning_rate": 1e-05,
      "loss": 7.0179,
      "step": 2840
    },
    {
      "epoch": 0.15392953929539296,
      "step": 2840,
      "training_loss": 7.438872814178467
    },
    {
      "epoch": 0.15398373983739838,
      "step": 2841,
      "training_loss": 6.756739616394043
    },
    {
      "epoch": 0.1540379403794038,
      "step": 2842,
      "training_loss": 7.526775360107422
    },
    {
      "epoch": 0.15409214092140922,
      "step": 2843,
      "training_loss": 6.844085693359375
    },
    {
      "epoch": 0.15414634146341463,
      "grad_norm": 24.70453453063965,
      "learning_rate": 1e-05,
      "loss": 7.1416,
      "step": 2844
    },
    {
      "epoch": 0.15414634146341463,
      "step": 2844,
      "training_loss": 6.6657490730285645
    },
    {
      "epoch": 0.15420054200542005,
      "step": 2845,
      "training_loss": 6.371633529663086
    },
    {
      "epoch": 0.15425474254742547,
      "step": 2846,
      "training_loss": 6.308870315551758
    },
    {
      "epoch": 0.15430894308943088,
      "step": 2847,
      "training_loss": 6.957880020141602
    },
    {
      "epoch": 0.15436314363143633,
      "grad_norm": 36.64063262939453,
      "learning_rate": 1e-05,
      "loss": 6.576,
      "step": 2848
    },
    {
      "epoch": 0.15436314363143633,
      "step": 2848,
      "training_loss": 6.810107231140137
    },
    {
      "epoch": 0.15441734417344175,
      "step": 2849,
      "training_loss": 8.678108215332031
    },
    {
      "epoch": 0.15447154471544716,
      "step": 2850,
      "training_loss": 6.493203163146973
    },
    {
      "epoch": 0.15452574525745258,
      "step": 2851,
      "training_loss": 7.394636154174805
    },
    {
      "epoch": 0.154579945799458,
      "grad_norm": 21.090574264526367,
      "learning_rate": 1e-05,
      "loss": 7.344,
      "step": 2852
    },
    {
      "epoch": 0.154579945799458,
      "step": 2852,
      "training_loss": 6.893336772918701
    },
    {
      "epoch": 0.1546341463414634,
      "step": 2853,
      "training_loss": 8.11740493774414
    },
    {
      "epoch": 0.15468834688346883,
      "step": 2854,
      "training_loss": 7.34279727935791
    },
    {
      "epoch": 0.15474254742547425,
      "step": 2855,
      "training_loss": 7.38548469543457
    },
    {
      "epoch": 0.15479674796747966,
      "grad_norm": 18.32781410217285,
      "learning_rate": 1e-05,
      "loss": 7.4348,
      "step": 2856
    },
    {
      "epoch": 0.15479674796747966,
      "step": 2856,
      "training_loss": 7.451510429382324
    },
    {
      "epoch": 0.15485094850948508,
      "step": 2857,
      "training_loss": 3.8825275897979736
    },
    {
      "epoch": 0.15490514905149053,
      "step": 2858,
      "training_loss": 7.525318622589111
    },
    {
      "epoch": 0.15495934959349594,
      "step": 2859,
      "training_loss": 7.2286176681518555
    },
    {
      "epoch": 0.15501355013550136,
      "grad_norm": 19.2498779296875,
      "learning_rate": 1e-05,
      "loss": 6.522,
      "step": 2860
    },
    {
      "epoch": 0.15501355013550136,
      "step": 2860,
      "training_loss": 5.746329307556152
    },
    {
      "epoch": 0.15506775067750678,
      "step": 2861,
      "training_loss": 8.426191329956055
    },
    {
      "epoch": 0.1551219512195122,
      "step": 2862,
      "training_loss": 6.7994866371154785
    },
    {
      "epoch": 0.1551761517615176,
      "step": 2863,
      "training_loss": 5.427798271179199
    },
    {
      "epoch": 0.15523035230352303,
      "grad_norm": 46.90581130981445,
      "learning_rate": 1e-05,
      "loss": 6.6,
      "step": 2864
    },
    {
      "epoch": 0.15523035230352303,
      "step": 2864,
      "training_loss": 5.347766876220703
    },
    {
      "epoch": 0.15528455284552845,
      "step": 2865,
      "training_loss": 7.108022689819336
    },
    {
      "epoch": 0.15533875338753386,
      "step": 2866,
      "training_loss": 7.885133266448975
    },
    {
      "epoch": 0.1553929539295393,
      "step": 2867,
      "training_loss": 7.103589057922363
    },
    {
      "epoch": 0.15544715447154472,
      "grad_norm": 27.888778686523438,
      "learning_rate": 1e-05,
      "loss": 6.8611,
      "step": 2868
    },
    {
      "epoch": 0.15544715447154472,
      "step": 2868,
      "training_loss": 6.86094331741333
    },
    {
      "epoch": 0.15550135501355014,
      "step": 2869,
      "training_loss": 6.1615986824035645
    },
    {
      "epoch": 0.15555555555555556,
      "step": 2870,
      "training_loss": 5.908816337585449
    },
    {
      "epoch": 0.15560975609756098,
      "step": 2871,
      "training_loss": 7.663077354431152
    },
    {
      "epoch": 0.1556639566395664,
      "grad_norm": 34.526859283447266,
      "learning_rate": 1e-05,
      "loss": 6.6486,
      "step": 2872
    },
    {
      "epoch": 0.1556639566395664,
      "step": 2872,
      "training_loss": 6.351736068725586
    },
    {
      "epoch": 0.1557181571815718,
      "step": 2873,
      "training_loss": 5.865800380706787
    },
    {
      "epoch": 0.15577235772357723,
      "step": 2874,
      "training_loss": 6.7286224365234375
    },
    {
      "epoch": 0.15582655826558264,
      "step": 2875,
      "training_loss": 6.431277275085449
    },
    {
      "epoch": 0.1558807588075881,
      "grad_norm": 20.1003475189209,
      "learning_rate": 1e-05,
      "loss": 6.3444,
      "step": 2876
    },
    {
      "epoch": 0.1558807588075881,
      "step": 2876,
      "training_loss": 6.754427909851074
    },
    {
      "epoch": 0.1559349593495935,
      "step": 2877,
      "training_loss": 6.780934810638428
    },
    {
      "epoch": 0.15598915989159892,
      "step": 2878,
      "training_loss": 7.198507785797119
    },
    {
      "epoch": 0.15604336043360434,
      "step": 2879,
      "training_loss": 8.61989688873291
    },
    {
      "epoch": 0.15609756097560976,
      "grad_norm": 30.530954360961914,
      "learning_rate": 1e-05,
      "loss": 7.3384,
      "step": 2880
    },
    {
      "epoch": 0.15609756097560976,
      "step": 2880,
      "training_loss": 4.937347888946533
    },
    {
      "epoch": 0.15615176151761517,
      "step": 2881,
      "training_loss": 6.741906642913818
    },
    {
      "epoch": 0.1562059620596206,
      "step": 2882,
      "training_loss": 6.947394847869873
    },
    {
      "epoch": 0.156260162601626,
      "step": 2883,
      "training_loss": 7.205965518951416
    },
    {
      "epoch": 0.15631436314363142,
      "grad_norm": 19.211959838867188,
      "learning_rate": 1e-05,
      "loss": 6.4582,
      "step": 2884
    },
    {
      "epoch": 0.15631436314363142,
      "step": 2884,
      "training_loss": 6.8303608894348145
    },
    {
      "epoch": 0.15636856368563687,
      "step": 2885,
      "training_loss": 5.653949737548828
    },
    {
      "epoch": 0.15642276422764229,
      "step": 2886,
      "training_loss": 6.185618877410889
    },
    {
      "epoch": 0.1564769647696477,
      "step": 2887,
      "training_loss": 7.253644943237305
    },
    {
      "epoch": 0.15653116531165312,
      "grad_norm": 34.4869499206543,
      "learning_rate": 1e-05,
      "loss": 6.4809,
      "step": 2888
    },
    {
      "epoch": 0.15653116531165312,
      "step": 2888,
      "training_loss": 7.342574119567871
    },
    {
      "epoch": 0.15658536585365854,
      "step": 2889,
      "training_loss": 6.886693477630615
    },
    {
      "epoch": 0.15663956639566395,
      "step": 2890,
      "training_loss": 6.285322666168213
    },
    {
      "epoch": 0.15669376693766937,
      "step": 2891,
      "training_loss": 5.830822944641113
    },
    {
      "epoch": 0.1567479674796748,
      "grad_norm": 22.661685943603516,
      "learning_rate": 1e-05,
      "loss": 6.5864,
      "step": 2892
    },
    {
      "epoch": 0.1567479674796748,
      "step": 2892,
      "training_loss": 6.716421604156494
    },
    {
      "epoch": 0.1568021680216802,
      "step": 2893,
      "training_loss": 7.420016765594482
    },
    {
      "epoch": 0.15685636856368565,
      "step": 2894,
      "training_loss": 7.3783063888549805
    },
    {
      "epoch": 0.15691056910569107,
      "step": 2895,
      "training_loss": 5.9821038246154785
    },
    {
      "epoch": 0.15696476964769648,
      "grad_norm": 19.748342514038086,
      "learning_rate": 1e-05,
      "loss": 6.8742,
      "step": 2896
    },
    {
      "epoch": 0.15696476964769648,
      "step": 2896,
      "training_loss": 6.272614002227783
    },
    {
      "epoch": 0.1570189701897019,
      "step": 2897,
      "training_loss": 6.163705825805664
    },
    {
      "epoch": 0.15707317073170732,
      "step": 2898,
      "training_loss": 7.571549415588379
    },
    {
      "epoch": 0.15712737127371273,
      "step": 2899,
      "training_loss": 6.90871000289917
    },
    {
      "epoch": 0.15718157181571815,
      "grad_norm": 33.41717529296875,
      "learning_rate": 1e-05,
      "loss": 6.7291,
      "step": 2900
    },
    {
      "epoch": 0.15718157181571815,
      "step": 2900,
      "training_loss": 5.12941837310791
    },
    {
      "epoch": 0.15723577235772357,
      "step": 2901,
      "training_loss": 7.404757499694824
    },
    {
      "epoch": 0.15728997289972899,
      "step": 2902,
      "training_loss": 7.612482070922852
    },
    {
      "epoch": 0.15734417344173443,
      "step": 2903,
      "training_loss": 4.866520881652832
    },
    {
      "epoch": 0.15739837398373985,
      "grad_norm": 27.730783462524414,
      "learning_rate": 1e-05,
      "loss": 6.2533,
      "step": 2904
    },
    {
      "epoch": 0.15739837398373985,
      "step": 2904,
      "training_loss": 5.585017681121826
    },
    {
      "epoch": 0.15745257452574526,
      "step": 2905,
      "training_loss": 6.588736057281494
    },
    {
      "epoch": 0.15750677506775068,
      "step": 2906,
      "training_loss": 8.834848403930664
    },
    {
      "epoch": 0.1575609756097561,
      "step": 2907,
      "training_loss": 6.759108066558838
    },
    {
      "epoch": 0.15761517615176152,
      "grad_norm": 19.158987045288086,
      "learning_rate": 1e-05,
      "loss": 6.9419,
      "step": 2908
    },
    {
      "epoch": 0.15761517615176152,
      "step": 2908,
      "training_loss": 6.351749420166016
    },
    {
      "epoch": 0.15766937669376693,
      "step": 2909,
      "training_loss": 6.403289794921875
    },
    {
      "epoch": 0.15772357723577235,
      "step": 2910,
      "training_loss": 6.54787015914917
    },
    {
      "epoch": 0.15777777777777777,
      "step": 2911,
      "training_loss": 7.141826152801514
    },
    {
      "epoch": 0.1578319783197832,
      "grad_norm": 31.344697952270508,
      "learning_rate": 1e-05,
      "loss": 6.6112,
      "step": 2912
    },
    {
      "epoch": 0.1578319783197832,
      "step": 2912,
      "training_loss": 7.155374526977539
    },
    {
      "epoch": 0.15788617886178863,
      "step": 2913,
      "training_loss": 7.525177955627441
    },
    {
      "epoch": 0.15794037940379405,
      "step": 2914,
      "training_loss": 6.3772430419921875
    },
    {
      "epoch": 0.15799457994579946,
      "step": 2915,
      "training_loss": 7.657386779785156
    },
    {
      "epoch": 0.15804878048780488,
      "grad_norm": 24.08736801147461,
      "learning_rate": 1e-05,
      "loss": 7.1788,
      "step": 2916
    },
    {
      "epoch": 0.15804878048780488,
      "step": 2916,
      "training_loss": 7.312280654907227
    },
    {
      "epoch": 0.1581029810298103,
      "step": 2917,
      "training_loss": 7.298388481140137
    },
    {
      "epoch": 0.1581571815718157,
      "step": 2918,
      "training_loss": 7.257785320281982
    },
    {
      "epoch": 0.15821138211382113,
      "step": 2919,
      "training_loss": 6.690280437469482
    },
    {
      "epoch": 0.15826558265582655,
      "grad_norm": 19.093996047973633,
      "learning_rate": 1e-05,
      "loss": 7.1397,
      "step": 2920
    },
    {
      "epoch": 0.15826558265582655,
      "step": 2920,
      "training_loss": 7.184230327606201
    },
    {
      "epoch": 0.15831978319783196,
      "step": 2921,
      "training_loss": 6.872853755950928
    },
    {
      "epoch": 0.1583739837398374,
      "step": 2922,
      "training_loss": 6.759646892547607
    },
    {
      "epoch": 0.15842818428184283,
      "step": 2923,
      "training_loss": 8.62765121459961
    },
    {
      "epoch": 0.15848238482384824,
      "grad_norm": 20.097896575927734,
      "learning_rate": 1e-05,
      "loss": 7.3611,
      "step": 2924
    },
    {
      "epoch": 0.15848238482384824,
      "step": 2924,
      "training_loss": 8.058176040649414
    },
    {
      "epoch": 0.15853658536585366,
      "step": 2925,
      "training_loss": 6.972200870513916
    },
    {
      "epoch": 0.15859078590785908,
      "step": 2926,
      "training_loss": 7.216691493988037
    },
    {
      "epoch": 0.1586449864498645,
      "step": 2927,
      "training_loss": 8.45913028717041
    },
    {
      "epoch": 0.1586991869918699,
      "grad_norm": 60.1237678527832,
      "learning_rate": 1e-05,
      "loss": 7.6765,
      "step": 2928
    },
    {
      "epoch": 0.1586991869918699,
      "step": 2928,
      "training_loss": 7.066335678100586
    },
    {
      "epoch": 0.15875338753387533,
      "step": 2929,
      "training_loss": 6.921389102935791
    },
    {
      "epoch": 0.15880758807588075,
      "step": 2930,
      "training_loss": 6.835501194000244
    },
    {
      "epoch": 0.1588617886178862,
      "step": 2931,
      "training_loss": 7.324372291564941
    },
    {
      "epoch": 0.1589159891598916,
      "grad_norm": 18.133073806762695,
      "learning_rate": 1e-05,
      "loss": 7.0369,
      "step": 2932
    },
    {
      "epoch": 0.1589159891598916,
      "step": 2932,
      "training_loss": 6.630540370941162
    },
    {
      "epoch": 0.15897018970189702,
      "step": 2933,
      "training_loss": 5.975952625274658
    },
    {
      "epoch": 0.15902439024390244,
      "step": 2934,
      "training_loss": 8.555869102478027
    },
    {
      "epoch": 0.15907859078590786,
      "step": 2935,
      "training_loss": 5.589618682861328
    },
    {
      "epoch": 0.15913279132791328,
      "grad_norm": 27.929931640625,
      "learning_rate": 1e-05,
      "loss": 6.688,
      "step": 2936
    },
    {
      "epoch": 0.15913279132791328,
      "step": 2936,
      "training_loss": 7.464033126831055
    },
    {
      "epoch": 0.1591869918699187,
      "step": 2937,
      "training_loss": 7.311155796051025
    },
    {
      "epoch": 0.1592411924119241,
      "step": 2938,
      "training_loss": 6.561821937561035
    },
    {
      "epoch": 0.15929539295392953,
      "step": 2939,
      "training_loss": 7.215827465057373
    },
    {
      "epoch": 0.15934959349593497,
      "grad_norm": 31.612863540649414,
      "learning_rate": 1e-05,
      "loss": 7.1382,
      "step": 2940
    },
    {
      "epoch": 0.15934959349593497,
      "step": 2940,
      "training_loss": 7.76384973526001
    },
    {
      "epoch": 0.1594037940379404,
      "step": 2941,
      "training_loss": 5.759899616241455
    },
    {
      "epoch": 0.1594579945799458,
      "step": 2942,
      "training_loss": 6.909980773925781
    },
    {
      "epoch": 0.15951219512195122,
      "step": 2943,
      "training_loss": 5.849776744842529
    },
    {
      "epoch": 0.15956639566395664,
      "grad_norm": 23.873563766479492,
      "learning_rate": 1e-05,
      "loss": 6.5709,
      "step": 2944
    },
    {
      "epoch": 0.15956639566395664,
      "step": 2944,
      "training_loss": 7.537816524505615
    },
    {
      "epoch": 0.15962059620596206,
      "step": 2945,
      "training_loss": 6.690806865692139
    },
    {
      "epoch": 0.15967479674796747,
      "step": 2946,
      "training_loss": 6.7462849617004395
    },
    {
      "epoch": 0.1597289972899729,
      "step": 2947,
      "training_loss": 6.3421173095703125
    },
    {
      "epoch": 0.1597831978319783,
      "grad_norm": 28.810468673706055,
      "learning_rate": 1e-05,
      "loss": 6.8293,
      "step": 2948
    },
    {
      "epoch": 0.1597831978319783,
      "step": 2948,
      "training_loss": 6.755227088928223
    },
    {
      "epoch": 0.15983739837398375,
      "step": 2949,
      "training_loss": 6.545165538787842
    },
    {
      "epoch": 0.15989159891598917,
      "step": 2950,
      "training_loss": 7.795363426208496
    },
    {
      "epoch": 0.1599457994579946,
      "step": 2951,
      "training_loss": 6.14130163192749
    },
    {
      "epoch": 0.16,
      "grad_norm": 15.153792381286621,
      "learning_rate": 1e-05,
      "loss": 6.8093,
      "step": 2952
    },
    {
      "epoch": 0.16,
      "step": 2952,
      "training_loss": 7.396259784698486
    },
    {
      "epoch": 0.16005420054200542,
      "step": 2953,
      "training_loss": 7.645601272583008
    },
    {
      "epoch": 0.16010840108401084,
      "step": 2954,
      "training_loss": 7.682761192321777
    },
    {
      "epoch": 0.16016260162601625,
      "step": 2955,
      "training_loss": 6.86782693862915
    },
    {
      "epoch": 0.16021680216802167,
      "grad_norm": 22.44603157043457,
      "learning_rate": 1e-05,
      "loss": 7.3981,
      "step": 2956
    },
    {
      "epoch": 0.16021680216802167,
      "step": 2956,
      "training_loss": 6.718705177307129
    },
    {
      "epoch": 0.1602710027100271,
      "step": 2957,
      "training_loss": 7.335902214050293
    },
    {
      "epoch": 0.16032520325203253,
      "step": 2958,
      "training_loss": 5.903501510620117
    },
    {
      "epoch": 0.16037940379403795,
      "step": 2959,
      "training_loss": 7.022304058074951
    },
    {
      "epoch": 0.16043360433604337,
      "grad_norm": 16.520267486572266,
      "learning_rate": 1e-05,
      "loss": 6.7451,
      "step": 2960
    },
    {
      "epoch": 0.16043360433604337,
      "step": 2960,
      "training_loss": 6.148603439331055
    },
    {
      "epoch": 0.16048780487804878,
      "step": 2961,
      "training_loss": 6.776230812072754
    },
    {
      "epoch": 0.1605420054200542,
      "step": 2962,
      "training_loss": 7.218337535858154
    },
    {
      "epoch": 0.16059620596205962,
      "step": 2963,
      "training_loss": 5.7717790603637695
    },
    {
      "epoch": 0.16065040650406504,
      "grad_norm": 17.16210174560547,
      "learning_rate": 1e-05,
      "loss": 6.4787,
      "step": 2964
    },
    {
      "epoch": 0.16065040650406504,
      "step": 2964,
      "training_loss": 7.78719425201416
    },
    {
      "epoch": 0.16070460704607045,
      "step": 2965,
      "training_loss": 5.902966022491455
    },
    {
      "epoch": 0.16075880758807587,
      "step": 2966,
      "training_loss": 6.517284393310547
    },
    {
      "epoch": 0.16081300813008131,
      "step": 2967,
      "training_loss": 4.077859401702881
    },
    {
      "epoch": 0.16086720867208673,
      "grad_norm": 23.341978073120117,
      "learning_rate": 1e-05,
      "loss": 6.0713,
      "step": 2968
    },
    {
      "epoch": 0.16086720867208673,
      "step": 2968,
      "training_loss": 6.580926418304443
    },
    {
      "epoch": 0.16092140921409215,
      "step": 2969,
      "training_loss": 7.023199558258057
    },
    {
      "epoch": 0.16097560975609757,
      "step": 2970,
      "training_loss": 7.310860633850098
    },
    {
      "epoch": 0.16102981029810298,
      "step": 2971,
      "training_loss": 6.431434631347656
    },
    {
      "epoch": 0.1610840108401084,
      "grad_norm": 33.148582458496094,
      "learning_rate": 1e-05,
      "loss": 6.8366,
      "step": 2972
    },
    {
      "epoch": 0.1610840108401084,
      "step": 2972,
      "training_loss": 5.974183559417725
    },
    {
      "epoch": 0.16113821138211382,
      "step": 2973,
      "training_loss": 5.247602462768555
    },
    {
      "epoch": 0.16119241192411923,
      "step": 2974,
      "training_loss": 7.360989093780518
    },
    {
      "epoch": 0.16124661246612465,
      "step": 2975,
      "training_loss": 7.905933856964111
    },
    {
      "epoch": 0.1613008130081301,
      "grad_norm": 18.197437286376953,
      "learning_rate": 1e-05,
      "loss": 6.6222,
      "step": 2976
    },
    {
      "epoch": 0.1613008130081301,
      "step": 2976,
      "training_loss": 6.763075351715088
    },
    {
      "epoch": 0.1613550135501355,
      "step": 2977,
      "training_loss": 6.837372303009033
    },
    {
      "epoch": 0.16140921409214093,
      "step": 2978,
      "training_loss": 6.511440277099609
    },
    {
      "epoch": 0.16146341463414635,
      "step": 2979,
      "training_loss": 6.540027618408203
    },
    {
      "epoch": 0.16151761517615176,
      "grad_norm": 21.105897903442383,
      "learning_rate": 1e-05,
      "loss": 6.663,
      "step": 2980
    },
    {
      "epoch": 0.16151761517615176,
      "step": 2980,
      "training_loss": 7.701763153076172
    },
    {
      "epoch": 0.16157181571815718,
      "step": 2981,
      "training_loss": 6.073128700256348
    },
    {
      "epoch": 0.1616260162601626,
      "step": 2982,
      "training_loss": 6.560590744018555
    },
    {
      "epoch": 0.16168021680216801,
      "step": 2983,
      "training_loss": 7.808140277862549
    },
    {
      "epoch": 0.16173441734417343,
      "grad_norm": 30.810598373413086,
      "learning_rate": 1e-05,
      "loss": 7.0359,
      "step": 2984
    },
    {
      "epoch": 0.16173441734417343,
      "step": 2984,
      "training_loss": 7.014292240142822
    },
    {
      "epoch": 0.16178861788617885,
      "step": 2985,
      "training_loss": 7.17975378036499
    },
    {
      "epoch": 0.1618428184281843,
      "step": 2986,
      "training_loss": 6.069551944732666
    },
    {
      "epoch": 0.1618970189701897,
      "step": 2987,
      "training_loss": 7.328714847564697
    },
    {
      "epoch": 0.16195121951219513,
      "grad_norm": 14.939138412475586,
      "learning_rate": 1e-05,
      "loss": 6.8981,
      "step": 2988
    },
    {
      "epoch": 0.16195121951219513,
      "step": 2988,
      "training_loss": 7.748476982116699
    },
    {
      "epoch": 0.16200542005420054,
      "step": 2989,
      "training_loss": 5.465795040130615
    },
    {
      "epoch": 0.16205962059620596,
      "step": 2990,
      "training_loss": 7.206252574920654
    },
    {
      "epoch": 0.16211382113821138,
      "step": 2991,
      "training_loss": 6.936278820037842
    },
    {
      "epoch": 0.1621680216802168,
      "grad_norm": 15.578089714050293,
      "learning_rate": 1e-05,
      "loss": 6.8392,
      "step": 2992
    },
    {
      "epoch": 0.1621680216802168,
      "step": 2992,
      "training_loss": 7.004034996032715
    },
    {
      "epoch": 0.1622222222222222,
      "step": 2993,
      "training_loss": 7.50494909286499
    },
    {
      "epoch": 0.16227642276422763,
      "step": 2994,
      "training_loss": 7.0521392822265625
    },
    {
      "epoch": 0.16233062330623307,
      "step": 2995,
      "training_loss": 5.744019508361816
    },
    {
      "epoch": 0.1623848238482385,
      "grad_norm": 25.260517120361328,
      "learning_rate": 1e-05,
      "loss": 6.8263,
      "step": 2996
    },
    {
      "epoch": 0.1623848238482385,
      "step": 2996,
      "training_loss": 6.471571922302246
    },
    {
      "epoch": 0.1624390243902439,
      "step": 2997,
      "training_loss": 7.002123832702637
    },
    {
      "epoch": 0.16249322493224932,
      "step": 2998,
      "training_loss": 7.546938896179199
    },
    {
      "epoch": 0.16254742547425474,
      "step": 2999,
      "training_loss": 7.516677379608154
    },
    {
      "epoch": 0.16260162601626016,
      "grad_norm": 20.08682632446289,
      "learning_rate": 1e-05,
      "loss": 7.1343,
      "step": 3000
    },
    {
      "epoch": 0.16260162601626016,
      "step": 3000,
      "training_loss": 5.769819259643555
    },
    {
      "epoch": 0.16265582655826558,
      "step": 3001,
      "training_loss": 10.680994987487793
    },
    {
      "epoch": 0.162710027100271,
      "step": 3002,
      "training_loss": 6.731485843658447
    },
    {
      "epoch": 0.1627642276422764,
      "step": 3003,
      "training_loss": 6.923071384429932
    },
    {
      "epoch": 0.16281842818428185,
      "grad_norm": 14.367572784423828,
      "learning_rate": 1e-05,
      "loss": 7.5263,
      "step": 3004
    },
    {
      "epoch": 0.16281842818428185,
      "step": 3004,
      "training_loss": 7.621476173400879
    },
    {
      "epoch": 0.16287262872628727,
      "step": 3005,
      "training_loss": 7.175113201141357
    },
    {
      "epoch": 0.1629268292682927,
      "step": 3006,
      "training_loss": 7.4440999031066895
    },
    {
      "epoch": 0.1629810298102981,
      "step": 3007,
      "training_loss": 7.67173433303833
    },
    {
      "epoch": 0.16303523035230352,
      "grad_norm": 23.191003799438477,
      "learning_rate": 1e-05,
      "loss": 7.4781,
      "step": 3008
    },
    {
      "epoch": 0.16303523035230352,
      "step": 3008,
      "training_loss": 7.437249183654785
    },
    {
      "epoch": 0.16308943089430894,
      "step": 3009,
      "training_loss": 7.108950138092041
    },
    {
      "epoch": 0.16314363143631436,
      "step": 3010,
      "training_loss": 5.40097188949585
    },
    {
      "epoch": 0.16319783197831977,
      "step": 3011,
      "training_loss": 6.818798542022705
    },
    {
      "epoch": 0.1632520325203252,
      "grad_norm": 24.684803009033203,
      "learning_rate": 1e-05,
      "loss": 6.6915,
      "step": 3012
    },
    {
      "epoch": 0.1632520325203252,
      "step": 3012,
      "training_loss": 7.505446434020996
    },
    {
      "epoch": 0.16330623306233064,
      "step": 3013,
      "training_loss": 7.148746490478516
    },
    {
      "epoch": 0.16336043360433605,
      "step": 3014,
      "training_loss": 6.43250846862793
    },
    {
      "epoch": 0.16341463414634147,
      "step": 3015,
      "training_loss": 6.915080547332764
    },
    {
      "epoch": 0.1634688346883469,
      "grad_norm": 23.25904655456543,
      "learning_rate": 1e-05,
      "loss": 7.0004,
      "step": 3016
    },
    {
      "epoch": 0.1634688346883469,
      "step": 3016,
      "training_loss": 7.0306196212768555
    },
    {
      "epoch": 0.1635230352303523,
      "step": 3017,
      "training_loss": 6.743037700653076
    },
    {
      "epoch": 0.16357723577235772,
      "step": 3018,
      "training_loss": 6.103106498718262
    },
    {
      "epoch": 0.16363143631436314,
      "step": 3019,
      "training_loss": 6.64677619934082
    },
    {
      "epoch": 0.16368563685636855,
      "grad_norm": 20.784542083740234,
      "learning_rate": 1e-05,
      "loss": 6.6309,
      "step": 3020
    },
    {
      "epoch": 0.16368563685636855,
      "step": 3020,
      "training_loss": 7.416233539581299
    },
    {
      "epoch": 0.16373983739837397,
      "step": 3021,
      "training_loss": 7.746795177459717
    },
    {
      "epoch": 0.16379403794037942,
      "step": 3022,
      "training_loss": 7.151573657989502
    },
    {
      "epoch": 0.16384823848238483,
      "step": 3023,
      "training_loss": 6.856207370758057
    },
    {
      "epoch": 0.16390243902439025,
      "grad_norm": 15.727557182312012,
      "learning_rate": 1e-05,
      "loss": 7.2927,
      "step": 3024
    },
    {
      "epoch": 0.16390243902439025,
      "step": 3024,
      "training_loss": 6.165416240692139
    },
    {
      "epoch": 0.16395663956639567,
      "step": 3025,
      "training_loss": 8.213289260864258
    },
    {
      "epoch": 0.16401084010840108,
      "step": 3026,
      "training_loss": 6.973206996917725
    },
    {
      "epoch": 0.1640650406504065,
      "step": 3027,
      "training_loss": 6.507347106933594
    },
    {
      "epoch": 0.16411924119241192,
      "grad_norm": 27.191898345947266,
      "learning_rate": 1e-05,
      "loss": 6.9648,
      "step": 3028
    },
    {
      "epoch": 0.16411924119241192,
      "step": 3028,
      "training_loss": 7.474644184112549
    },
    {
      "epoch": 0.16417344173441734,
      "step": 3029,
      "training_loss": 6.960941314697266
    },
    {
      "epoch": 0.16422764227642275,
      "step": 3030,
      "training_loss": 7.219117641448975
    },
    {
      "epoch": 0.1642818428184282,
      "step": 3031,
      "training_loss": 7.206991195678711
    },
    {
      "epoch": 0.16433604336043361,
      "grad_norm": 30.17197608947754,
      "learning_rate": 1e-05,
      "loss": 7.2154,
      "step": 3032
    },
    {
      "epoch": 0.16433604336043361,
      "step": 3032,
      "training_loss": 6.730391025543213
    },
    {
      "epoch": 0.16439024390243903,
      "step": 3033,
      "training_loss": 4.737142086029053
    },
    {
      "epoch": 0.16444444444444445,
      "step": 3034,
      "training_loss": 6.7536725997924805
    },
    {
      "epoch": 0.16449864498644987,
      "step": 3035,
      "training_loss": 6.878812313079834
    },
    {
      "epoch": 0.16455284552845528,
      "grad_norm": 38.94382858276367,
      "learning_rate": 1e-05,
      "loss": 6.275,
      "step": 3036
    },
    {
      "epoch": 0.16455284552845528,
      "step": 3036,
      "training_loss": 6.4678215980529785
    },
    {
      "epoch": 0.1646070460704607,
      "step": 3037,
      "training_loss": 6.912356853485107
    },
    {
      "epoch": 0.16466124661246612,
      "step": 3038,
      "training_loss": 4.957960605621338
    },
    {
      "epoch": 0.16471544715447153,
      "step": 3039,
      "training_loss": 7.39825963973999
    },
    {
      "epoch": 0.16476964769647698,
      "grad_norm": 41.762451171875,
      "learning_rate": 1e-05,
      "loss": 6.4341,
      "step": 3040
    },
    {
      "epoch": 0.16476964769647698,
      "step": 3040,
      "training_loss": 7.542705535888672
    },
    {
      "epoch": 0.1648238482384824,
      "step": 3041,
      "training_loss": 5.586994171142578
    },
    {
      "epoch": 0.1648780487804878,
      "step": 3042,
      "training_loss": 6.544967174530029
    },
    {
      "epoch": 0.16493224932249323,
      "step": 3043,
      "training_loss": 7.472884178161621
    },
    {
      "epoch": 0.16498644986449865,
      "grad_norm": 33.550743103027344,
      "learning_rate": 1e-05,
      "loss": 6.7869,
      "step": 3044
    },
    {
      "epoch": 0.16498644986449865,
      "step": 3044,
      "training_loss": 7.897436141967773
    },
    {
      "epoch": 0.16504065040650406,
      "step": 3045,
      "training_loss": 7.175792694091797
    },
    {
      "epoch": 0.16509485094850948,
      "step": 3046,
      "training_loss": 4.800732135772705
    },
    {
      "epoch": 0.1651490514905149,
      "step": 3047,
      "training_loss": 6.889263153076172
    },
    {
      "epoch": 0.16520325203252031,
      "grad_norm": 20.25153923034668,
      "learning_rate": 1e-05,
      "loss": 6.6908,
      "step": 3048
    },
    {
      "epoch": 0.16520325203252031,
      "step": 3048,
      "training_loss": 7.55864953994751
    },
    {
      "epoch": 0.16525745257452573,
      "step": 3049,
      "training_loss": 4.602255821228027
    },
    {
      "epoch": 0.16531165311653118,
      "step": 3050,
      "training_loss": 6.225058555603027
    },
    {
      "epoch": 0.1653658536585366,
      "step": 3051,
      "training_loss": 6.786135673522949
    },
    {
      "epoch": 0.165420054200542,
      "grad_norm": 27.394487380981445,
      "learning_rate": 1e-05,
      "loss": 6.293,
      "step": 3052
    },
    {
      "epoch": 0.165420054200542,
      "step": 3052,
      "training_loss": 6.06257963180542
    },
    {
      "epoch": 0.16547425474254743,
      "step": 3053,
      "training_loss": 6.010027885437012
    },
    {
      "epoch": 0.16552845528455284,
      "step": 3054,
      "training_loss": 7.60477876663208
    },
    {
      "epoch": 0.16558265582655826,
      "step": 3055,
      "training_loss": 6.959472179412842
    },
    {
      "epoch": 0.16563685636856368,
      "grad_norm": 22.176677703857422,
      "learning_rate": 1e-05,
      "loss": 6.6592,
      "step": 3056
    },
    {
      "epoch": 0.16563685636856368,
      "step": 3056,
      "training_loss": 7.926078796386719
    },
    {
      "epoch": 0.1656910569105691,
      "step": 3057,
      "training_loss": 8.182756423950195
    },
    {
      "epoch": 0.1657452574525745,
      "step": 3058,
      "training_loss": 5.917239189147949
    },
    {
      "epoch": 0.16579945799457996,
      "step": 3059,
      "training_loss": 6.761448383331299
    },
    {
      "epoch": 0.16585365853658537,
      "grad_norm": 24.537160873413086,
      "learning_rate": 1e-05,
      "loss": 7.1969,
      "step": 3060
    },
    {
      "epoch": 0.16585365853658537,
      "step": 3060,
      "training_loss": 7.081878185272217
    },
    {
      "epoch": 0.1659078590785908,
      "step": 3061,
      "training_loss": 7.328185558319092
    },
    {
      "epoch": 0.1659620596205962,
      "step": 3062,
      "training_loss": 6.369275093078613
    },
    {
      "epoch": 0.16601626016260163,
      "step": 3063,
      "training_loss": 6.763922691345215
    },
    {
      "epoch": 0.16607046070460704,
      "grad_norm": 31.511489868164062,
      "learning_rate": 1e-05,
      "loss": 6.8858,
      "step": 3064
    },
    {
      "epoch": 0.16607046070460704,
      "step": 3064,
      "training_loss": 7.178269863128662
    },
    {
      "epoch": 0.16612466124661246,
      "step": 3065,
      "training_loss": 6.631086349487305
    },
    {
      "epoch": 0.16617886178861788,
      "step": 3066,
      "training_loss": 4.1674723625183105
    },
    {
      "epoch": 0.1662330623306233,
      "step": 3067,
      "training_loss": 6.83041524887085
    },
    {
      "epoch": 0.16628726287262874,
      "grad_norm": 18.849241256713867,
      "learning_rate": 1e-05,
      "loss": 6.2018,
      "step": 3068
    },
    {
      "epoch": 0.16628726287262874,
      "step": 3068,
      "training_loss": 7.932281494140625
    },
    {
      "epoch": 0.16634146341463416,
      "step": 3069,
      "training_loss": 6.995041370391846
    },
    {
      "epoch": 0.16639566395663957,
      "step": 3070,
      "training_loss": 4.595454216003418
    },
    {
      "epoch": 0.166449864498645,
      "step": 3071,
      "training_loss": 6.943033695220947
    },
    {
      "epoch": 0.1665040650406504,
      "grad_norm": 17.41122817993164,
      "learning_rate": 1e-05,
      "loss": 6.6165,
      "step": 3072
    },
    {
      "epoch": 0.1665040650406504,
      "step": 3072,
      "training_loss": 8.204055786132812
    },
    {
      "epoch": 0.16655826558265582,
      "step": 3073,
      "training_loss": 8.037402153015137
    },
    {
      "epoch": 0.16661246612466124,
      "step": 3074,
      "training_loss": 5.65858268737793
    },
    {
      "epoch": 0.16666666666666666,
      "step": 3075,
      "training_loss": 7.237144947052002
    },
    {
      "epoch": 0.16672086720867207,
      "grad_norm": 21.637319564819336,
      "learning_rate": 1e-05,
      "loss": 7.2843,
      "step": 3076
    },
    {
      "epoch": 0.16672086720867207,
      "step": 3076,
      "training_loss": 7.160490989685059
    },
    {
      "epoch": 0.16677506775067752,
      "step": 3077,
      "training_loss": 4.883201599121094
    },
    {
      "epoch": 0.16682926829268294,
      "step": 3078,
      "training_loss": 7.745425701141357
    },
    {
      "epoch": 0.16688346883468835,
      "step": 3079,
      "training_loss": 7.029091835021973
    },
    {
      "epoch": 0.16693766937669377,
      "grad_norm": 36.4002571105957,
      "learning_rate": 1e-05,
      "loss": 6.7046,
      "step": 3080
    },
    {
      "epoch": 0.16693766937669377,
      "step": 3080,
      "training_loss": 5.811226844787598
    },
    {
      "epoch": 0.1669918699186992,
      "step": 3081,
      "training_loss": 7.73484468460083
    },
    {
      "epoch": 0.1670460704607046,
      "step": 3082,
      "training_loss": 5.814998149871826
    },
    {
      "epoch": 0.16710027100271002,
      "step": 3083,
      "training_loss": 7.497524738311768
    },
    {
      "epoch": 0.16715447154471544,
      "grad_norm": 25.36642074584961,
      "learning_rate": 1e-05,
      "loss": 6.7146,
      "step": 3084
    },
    {
      "epoch": 0.16715447154471544,
      "step": 3084,
      "training_loss": 8.101901054382324
    },
    {
      "epoch": 0.16720867208672086,
      "step": 3085,
      "training_loss": 6.929892539978027
    },
    {
      "epoch": 0.1672628726287263,
      "step": 3086,
      "training_loss": 6.97938871383667
    },
    {
      "epoch": 0.16731707317073172,
      "step": 3087,
      "training_loss": 7.118065357208252
    },
    {
      "epoch": 0.16737127371273713,
      "grad_norm": 15.914959907531738,
      "learning_rate": 1e-05,
      "loss": 7.2823,
      "step": 3088
    },
    {
      "epoch": 0.16737127371273713,
      "step": 3088,
      "training_loss": 6.880626678466797
    },
    {
      "epoch": 0.16742547425474255,
      "step": 3089,
      "training_loss": 5.085703372955322
    },
    {
      "epoch": 0.16747967479674797,
      "step": 3090,
      "training_loss": 7.088844299316406
    },
    {
      "epoch": 0.16753387533875339,
      "step": 3091,
      "training_loss": 5.724661827087402
    },
    {
      "epoch": 0.1675880758807588,
      "grad_norm": 25.180042266845703,
      "learning_rate": 1e-05,
      "loss": 6.195,
      "step": 3092
    },
    {
      "epoch": 0.1675880758807588,
      "step": 3092,
      "training_loss": 6.92713737487793
    },
    {
      "epoch": 0.16764227642276422,
      "step": 3093,
      "training_loss": 7.261476039886475
    },
    {
      "epoch": 0.16769647696476964,
      "step": 3094,
      "training_loss": 6.161435127258301
    },
    {
      "epoch": 0.16775067750677508,
      "step": 3095,
      "training_loss": 6.982956409454346
    },
    {
      "epoch": 0.1678048780487805,
      "grad_norm": 17.577503204345703,
      "learning_rate": 1e-05,
      "loss": 6.8333,
      "step": 3096
    },
    {
      "epoch": 0.1678048780487805,
      "step": 3096,
      "training_loss": 7.264886856079102
    },
    {
      "epoch": 0.16785907859078592,
      "step": 3097,
      "training_loss": 6.866893291473389
    },
    {
      "epoch": 0.16791327913279133,
      "step": 3098,
      "training_loss": 7.713854789733887
    },
    {
      "epoch": 0.16796747967479675,
      "step": 3099,
      "training_loss": 5.279563903808594
    },
    {
      "epoch": 0.16802168021680217,
      "grad_norm": 47.223445892333984,
      "learning_rate": 1e-05,
      "loss": 6.7813,
      "step": 3100
    },
    {
      "epoch": 0.16802168021680217,
      "step": 3100,
      "training_loss": 5.647733211517334
    },
    {
      "epoch": 0.16807588075880758,
      "step": 3101,
      "training_loss": 7.685414791107178
    },
    {
      "epoch": 0.168130081300813,
      "step": 3102,
      "training_loss": 7.0499091148376465
    },
    {
      "epoch": 0.16818428184281842,
      "step": 3103,
      "training_loss": 7.747322082519531
    },
    {
      "epoch": 0.16823848238482386,
      "grad_norm": 16.89069175720215,
      "learning_rate": 1e-05,
      "loss": 7.0326,
      "step": 3104
    },
    {
      "epoch": 0.16823848238482386,
      "step": 3104,
      "training_loss": 6.98338508605957
    },
    {
      "epoch": 0.16829268292682928,
      "step": 3105,
      "training_loss": 6.849808216094971
    },
    {
      "epoch": 0.1683468834688347,
      "step": 3106,
      "training_loss": 6.071924686431885
    },
    {
      "epoch": 0.1684010840108401,
      "step": 3107,
      "training_loss": 6.734445095062256
    },
    {
      "epoch": 0.16845528455284553,
      "grad_norm": 23.535348892211914,
      "learning_rate": 1e-05,
      "loss": 6.6599,
      "step": 3108
    },
    {
      "epoch": 0.16845528455284553,
      "step": 3108,
      "training_loss": 7.023128032684326
    },
    {
      "epoch": 0.16850948509485095,
      "step": 3109,
      "training_loss": 4.155453681945801
    },
    {
      "epoch": 0.16856368563685636,
      "step": 3110,
      "training_loss": 4.943539619445801
    },
    {
      "epoch": 0.16861788617886178,
      "step": 3111,
      "training_loss": 7.421543121337891
    },
    {
      "epoch": 0.1686720867208672,
      "grad_norm": 20.524398803710938,
      "learning_rate": 1e-05,
      "loss": 5.8859,
      "step": 3112
    },
    {
      "epoch": 0.1686720867208672,
      "step": 3112,
      "training_loss": 5.883206844329834
    },
    {
      "epoch": 0.16872628726287262,
      "step": 3113,
      "training_loss": 5.795571327209473
    },
    {
      "epoch": 0.16878048780487806,
      "step": 3114,
      "training_loss": 6.883586406707764
    },
    {
      "epoch": 0.16883468834688348,
      "step": 3115,
      "training_loss": 7.044589519500732
    },
    {
      "epoch": 0.1688888888888889,
      "grad_norm": 17.307992935180664,
      "learning_rate": 1e-05,
      "loss": 6.4017,
      "step": 3116
    },
    {
      "epoch": 0.1688888888888889,
      "step": 3116,
      "training_loss": 7.030011177062988
    },
    {
      "epoch": 0.1689430894308943,
      "step": 3117,
      "training_loss": 6.61893367767334
    },
    {
      "epoch": 0.16899728997289973,
      "step": 3118,
      "training_loss": 6.592992305755615
    },
    {
      "epoch": 0.16905149051490515,
      "step": 3119,
      "training_loss": 7.057188034057617
    },
    {
      "epoch": 0.16910569105691056,
      "grad_norm": 32.32083511352539,
      "learning_rate": 1e-05,
      "loss": 6.8248,
      "step": 3120
    },
    {
      "epoch": 0.16910569105691056,
      "step": 3120,
      "training_loss": 7.2521491050720215
    },
    {
      "epoch": 0.16915989159891598,
      "step": 3121,
      "training_loss": 9.490034103393555
    },
    {
      "epoch": 0.1692140921409214,
      "step": 3122,
      "training_loss": 8.063058853149414
    },
    {
      "epoch": 0.16926829268292684,
      "step": 3123,
      "training_loss": 7.575556755065918
    },
    {
      "epoch": 0.16932249322493226,
      "grad_norm": 20.32110595703125,
      "learning_rate": 1e-05,
      "loss": 8.0952,
      "step": 3124
    },
    {
      "epoch": 0.16932249322493226,
      "step": 3124,
      "training_loss": 7.333987712860107
    },
    {
      "epoch": 0.16937669376693767,
      "step": 3125,
      "training_loss": 7.037868022918701
    },
    {
      "epoch": 0.1694308943089431,
      "step": 3126,
      "training_loss": 6.261382579803467
    },
    {
      "epoch": 0.1694850948509485,
      "step": 3127,
      "training_loss": 6.719054698944092
    },
    {
      "epoch": 0.16953929539295393,
      "grad_norm": 21.622461318969727,
      "learning_rate": 1e-05,
      "loss": 6.8381,
      "step": 3128
    },
    {
      "epoch": 0.16953929539295393,
      "step": 3128,
      "training_loss": 8.399873733520508
    },
    {
      "epoch": 0.16959349593495934,
      "step": 3129,
      "training_loss": 6.6672563552856445
    },
    {
      "epoch": 0.16964769647696476,
      "step": 3130,
      "training_loss": 7.172022342681885
    },
    {
      "epoch": 0.16970189701897018,
      "step": 3131,
      "training_loss": 8.93308162689209
    },
    {
      "epoch": 0.16975609756097562,
      "grad_norm": 51.62116622924805,
      "learning_rate": 1e-05,
      "loss": 7.7931,
      "step": 3132
    },
    {
      "epoch": 0.16975609756097562,
      "step": 3132,
      "training_loss": 7.328136444091797
    },
    {
      "epoch": 0.16981029810298104,
      "step": 3133,
      "training_loss": 7.744343280792236
    },
    {
      "epoch": 0.16986449864498646,
      "step": 3134,
      "training_loss": 6.7554240226745605
    },
    {
      "epoch": 0.16991869918699187,
      "step": 3135,
      "training_loss": 7.3132100105285645
    },
    {
      "epoch": 0.1699728997289973,
      "grad_norm": 22.913816452026367,
      "learning_rate": 1e-05,
      "loss": 7.2853,
      "step": 3136
    },
    {
      "epoch": 0.1699728997289973,
      "step": 3136,
      "training_loss": 7.068224906921387
    },
    {
      "epoch": 0.1700271002710027,
      "step": 3137,
      "training_loss": 6.088888168334961
    },
    {
      "epoch": 0.17008130081300812,
      "step": 3138,
      "training_loss": 7.17848014831543
    },
    {
      "epoch": 0.17013550135501354,
      "step": 3139,
      "training_loss": 7.391305446624756
    },
    {
      "epoch": 0.17018970189701896,
      "grad_norm": 18.83468246459961,
      "learning_rate": 1e-05,
      "loss": 6.9317,
      "step": 3140
    },
    {
      "epoch": 0.17018970189701896,
      "step": 3140,
      "training_loss": 7.276265621185303
    },
    {
      "epoch": 0.1702439024390244,
      "step": 3141,
      "training_loss": 7.125787734985352
    },
    {
      "epoch": 0.17029810298102982,
      "step": 3142,
      "training_loss": 7.342701435089111
    },
    {
      "epoch": 0.17035230352303524,
      "step": 3143,
      "training_loss": 6.469883918762207
    },
    {
      "epoch": 0.17040650406504065,
      "grad_norm": 29.855716705322266,
      "learning_rate": 1e-05,
      "loss": 7.0537,
      "step": 3144
    },
    {
      "epoch": 0.17040650406504065,
      "step": 3144,
      "training_loss": 7.030792236328125
    },
    {
      "epoch": 0.17046070460704607,
      "step": 3145,
      "training_loss": 7.1502685546875
    },
    {
      "epoch": 0.1705149051490515,
      "step": 3146,
      "training_loss": 8.052061080932617
    },
    {
      "epoch": 0.1705691056910569,
      "step": 3147,
      "training_loss": 6.9613847732543945
    },
    {
      "epoch": 0.17062330623306232,
      "grad_norm": 28.797874450683594,
      "learning_rate": 1e-05,
      "loss": 7.2986,
      "step": 3148
    },
    {
      "epoch": 0.17062330623306232,
      "step": 3148,
      "training_loss": 7.702326774597168
    },
    {
      "epoch": 0.17067750677506774,
      "step": 3149,
      "training_loss": 8.024182319641113
    },
    {
      "epoch": 0.17073170731707318,
      "step": 3150,
      "training_loss": 5.567826747894287
    },
    {
      "epoch": 0.1707859078590786,
      "step": 3151,
      "training_loss": 6.948361396789551
    },
    {
      "epoch": 0.17084010840108402,
      "grad_norm": 21.152616500854492,
      "learning_rate": 1e-05,
      "loss": 7.0607,
      "step": 3152
    },
    {
      "epoch": 0.17084010840108402,
      "step": 3152,
      "training_loss": 5.869195938110352
    },
    {
      "epoch": 0.17089430894308943,
      "step": 3153,
      "training_loss": 6.852989673614502
    },
    {
      "epoch": 0.17094850948509485,
      "step": 3154,
      "training_loss": 7.8581743240356445
    },
    {
      "epoch": 0.17100271002710027,
      "step": 3155,
      "training_loss": 7.477871417999268
    },
    {
      "epoch": 0.17105691056910569,
      "grad_norm": 23.43837547302246,
      "learning_rate": 1e-05,
      "loss": 7.0146,
      "step": 3156
    },
    {
      "epoch": 0.17105691056910569,
      "step": 3156,
      "training_loss": 7.0970330238342285
    },
    {
      "epoch": 0.1711111111111111,
      "step": 3157,
      "training_loss": 5.651440620422363
    },
    {
      "epoch": 0.17116531165311652,
      "step": 3158,
      "training_loss": 7.003447532653809
    },
    {
      "epoch": 0.17121951219512196,
      "step": 3159,
      "training_loss": 6.804032325744629
    },
    {
      "epoch": 0.17127371273712738,
      "grad_norm": 29.328453063964844,
      "learning_rate": 1e-05,
      "loss": 6.639,
      "step": 3160
    },
    {
      "epoch": 0.17127371273712738,
      "step": 3160,
      "training_loss": 6.867862701416016
    },
    {
      "epoch": 0.1713279132791328,
      "step": 3161,
      "training_loss": 7.63116455078125
    },
    {
      "epoch": 0.17138211382113822,
      "step": 3162,
      "training_loss": 6.925065994262695
    },
    {
      "epoch": 0.17143631436314363,
      "step": 3163,
      "training_loss": 6.144101619720459
    },
    {
      "epoch": 0.17149051490514905,
      "grad_norm": 35.97827911376953,
      "learning_rate": 1e-05,
      "loss": 6.892,
      "step": 3164
    },
    {
      "epoch": 0.17149051490514905,
      "step": 3164,
      "training_loss": 7.160562992095947
    },
    {
      "epoch": 0.17154471544715447,
      "step": 3165,
      "training_loss": 7.436864376068115
    },
    {
      "epoch": 0.17159891598915988,
      "step": 3166,
      "training_loss": 7.45028018951416
    },
    {
      "epoch": 0.1716531165311653,
      "step": 3167,
      "training_loss": 6.950116157531738
    },
    {
      "epoch": 0.17170731707317075,
      "grad_norm": 22.69205665588379,
      "learning_rate": 1e-05,
      "loss": 7.2495,
      "step": 3168
    },
    {
      "epoch": 0.17170731707317075,
      "step": 3168,
      "training_loss": 7.836371421813965
    },
    {
      "epoch": 0.17176151761517616,
      "step": 3169,
      "training_loss": 6.688650131225586
    },
    {
      "epoch": 0.17181571815718158,
      "step": 3170,
      "training_loss": 7.153052806854248
    },
    {
      "epoch": 0.171869918699187,
      "step": 3171,
      "training_loss": 7.338819980621338
    },
    {
      "epoch": 0.1719241192411924,
      "grad_norm": 29.253459930419922,
      "learning_rate": 1e-05,
      "loss": 7.2542,
      "step": 3172
    },
    {
      "epoch": 0.1719241192411924,
      "step": 3172,
      "training_loss": 7.612550735473633
    },
    {
      "epoch": 0.17197831978319783,
      "step": 3173,
      "training_loss": 5.6997575759887695
    },
    {
      "epoch": 0.17203252032520325,
      "step": 3174,
      "training_loss": 6.6111578941345215
    },
    {
      "epoch": 0.17208672086720866,
      "step": 3175,
      "training_loss": 7.370857238769531
    },
    {
      "epoch": 0.17214092140921408,
      "grad_norm": 40.226863861083984,
      "learning_rate": 1e-05,
      "loss": 6.8236,
      "step": 3176
    },
    {
      "epoch": 0.17214092140921408,
      "step": 3176,
      "training_loss": 7.253020763397217
    },
    {
      "epoch": 0.1721951219512195,
      "step": 3177,
      "training_loss": 6.341847896575928
    },
    {
      "epoch": 0.17224932249322494,
      "step": 3178,
      "training_loss": 7.532234191894531
    },
    {
      "epoch": 0.17230352303523036,
      "step": 3179,
      "training_loss": 6.378076553344727
    },
    {
      "epoch": 0.17235772357723578,
      "grad_norm": 21.195556640625,
      "learning_rate": 1e-05,
      "loss": 6.8763,
      "step": 3180
    },
    {
      "epoch": 0.17235772357723578,
      "step": 3180,
      "training_loss": 7.553975582122803
    },
    {
      "epoch": 0.1724119241192412,
      "step": 3181,
      "training_loss": 6.838134765625
    },
    {
      "epoch": 0.1724661246612466,
      "step": 3182,
      "training_loss": 7.186484336853027
    },
    {
      "epoch": 0.17252032520325203,
      "step": 3183,
      "training_loss": 4.762709140777588
    },
    {
      "epoch": 0.17257452574525745,
      "grad_norm": 27.985027313232422,
      "learning_rate": 1e-05,
      "loss": 6.5853,
      "step": 3184
    },
    {
      "epoch": 0.17257452574525745,
      "step": 3184,
      "training_loss": 7.077261447906494
    },
    {
      "epoch": 0.17262872628726286,
      "step": 3185,
      "training_loss": 6.913172245025635
    },
    {
      "epoch": 0.17268292682926828,
      "step": 3186,
      "training_loss": 6.7933526039123535
    },
    {
      "epoch": 0.17273712737127372,
      "step": 3187,
      "training_loss": 7.11514949798584
    },
    {
      "epoch": 0.17279132791327914,
      "grad_norm": 16.404638290405273,
      "learning_rate": 1e-05,
      "loss": 6.9747,
      "step": 3188
    },
    {
      "epoch": 0.17279132791327914,
      "step": 3188,
      "training_loss": 7.390235424041748
    },
    {
      "epoch": 0.17284552845528456,
      "step": 3189,
      "training_loss": 7.369512557983398
    },
    {
      "epoch": 0.17289972899728998,
      "step": 3190,
      "training_loss": 5.726130962371826
    },
    {
      "epoch": 0.1729539295392954,
      "step": 3191,
      "training_loss": 7.54180908203125
    },
    {
      "epoch": 0.1730081300813008,
      "grad_norm": 21.78600311279297,
      "learning_rate": 1e-05,
      "loss": 7.0069,
      "step": 3192
    },
    {
      "epoch": 0.1730081300813008,
      "step": 3192,
      "training_loss": 6.878592491149902
    },
    {
      "epoch": 0.17306233062330623,
      "step": 3193,
      "training_loss": 7.664971351623535
    },
    {
      "epoch": 0.17311653116531164,
      "step": 3194,
      "training_loss": 7.205855369567871
    },
    {
      "epoch": 0.17317073170731706,
      "step": 3195,
      "training_loss": 7.454861164093018
    },
    {
      "epoch": 0.1732249322493225,
      "grad_norm": 13.264493942260742,
      "learning_rate": 1e-05,
      "loss": 7.3011,
      "step": 3196
    },
    {
      "epoch": 0.1732249322493225,
      "step": 3196,
      "training_loss": 6.272412300109863
    },
    {
      "epoch": 0.17327913279132792,
      "step": 3197,
      "training_loss": 5.989239692687988
    },
    {
      "epoch": 0.17333333333333334,
      "step": 3198,
      "training_loss": 6.642259120941162
    },
    {
      "epoch": 0.17338753387533876,
      "step": 3199,
      "training_loss": 7.241647720336914
    },
    {
      "epoch": 0.17344173441734417,
      "grad_norm": 36.143707275390625,
      "learning_rate": 1e-05,
      "loss": 6.5364,
      "step": 3200
    },
    {
      "epoch": 0.17344173441734417,
      "step": 3200,
      "training_loss": 4.644891738891602
    },
    {
      "epoch": 0.1734959349593496,
      "step": 3201,
      "training_loss": 7.050055503845215
    },
    {
      "epoch": 0.173550135501355,
      "step": 3202,
      "training_loss": 6.916146755218506
    },
    {
      "epoch": 0.17360433604336042,
      "step": 3203,
      "training_loss": 5.252574920654297
    },
    {
      "epoch": 0.17365853658536584,
      "grad_norm": 37.01118087768555,
      "learning_rate": 1e-05,
      "loss": 5.9659,
      "step": 3204
    },
    {
      "epoch": 0.17365853658536584,
      "step": 3204,
      "training_loss": 6.228144645690918
    },
    {
      "epoch": 0.1737127371273713,
      "step": 3205,
      "training_loss": 7.36279821395874
    },
    {
      "epoch": 0.1737669376693767,
      "step": 3206,
      "training_loss": 7.684335708618164
    },
    {
      "epoch": 0.17382113821138212,
      "step": 3207,
      "training_loss": 7.533237934112549
    },
    {
      "epoch": 0.17387533875338754,
      "grad_norm": 29.32270050048828,
      "learning_rate": 1e-05,
      "loss": 7.2021,
      "step": 3208
    },
    {
      "epoch": 0.17387533875338754,
      "step": 3208,
      "training_loss": 7.269965171813965
    },
    {
      "epoch": 0.17392953929539295,
      "step": 3209,
      "training_loss": 6.206577777862549
    },
    {
      "epoch": 0.17398373983739837,
      "step": 3210,
      "training_loss": 7.829830169677734
    },
    {
      "epoch": 0.1740379403794038,
      "step": 3211,
      "training_loss": 5.941713333129883
    },
    {
      "epoch": 0.1740921409214092,
      "grad_norm": 35.47455596923828,
      "learning_rate": 1e-05,
      "loss": 6.812,
      "step": 3212
    },
    {
      "epoch": 0.1740921409214092,
      "step": 3212,
      "training_loss": 7.325170993804932
    },
    {
      "epoch": 0.17414634146341462,
      "step": 3213,
      "training_loss": 6.9300713539123535
    },
    {
      "epoch": 0.17420054200542007,
      "step": 3214,
      "training_loss": 7.543281078338623
    },
    {
      "epoch": 0.17425474254742548,
      "step": 3215,
      "training_loss": 7.318941116333008
    },
    {
      "epoch": 0.1743089430894309,
      "grad_norm": 21.31529998779297,
      "learning_rate": 1e-05,
      "loss": 7.2794,
      "step": 3216
    },
    {
      "epoch": 0.1743089430894309,
      "step": 3216,
      "training_loss": 7.178551197052002
    },
    {
      "epoch": 0.17436314363143632,
      "step": 3217,
      "training_loss": 7.263692855834961
    },
    {
      "epoch": 0.17441734417344174,
      "step": 3218,
      "training_loss": 7.215605735778809
    },
    {
      "epoch": 0.17447154471544715,
      "step": 3219,
      "training_loss": 6.388561725616455
    },
    {
      "epoch": 0.17452574525745257,
      "grad_norm": 17.529579162597656,
      "learning_rate": 1e-05,
      "loss": 7.0116,
      "step": 3220
    },
    {
      "epoch": 0.17452574525745257,
      "step": 3220,
      "training_loss": 7.307718753814697
    },
    {
      "epoch": 0.174579945799458,
      "step": 3221,
      "training_loss": 7.286611557006836
    },
    {
      "epoch": 0.1746341463414634,
      "step": 3222,
      "training_loss": 7.494560718536377
    },
    {
      "epoch": 0.17468834688346885,
      "step": 3223,
      "training_loss": 6.876011848449707
    },
    {
      "epoch": 0.17474254742547426,
      "grad_norm": 34.818321228027344,
      "learning_rate": 1e-05,
      "loss": 7.2412,
      "step": 3224
    },
    {
      "epoch": 0.17474254742547426,
      "step": 3224,
      "training_loss": 6.961209774017334
    },
    {
      "epoch": 0.17479674796747968,
      "step": 3225,
      "training_loss": 7.2172441482543945
    },
    {
      "epoch": 0.1748509485094851,
      "step": 3226,
      "training_loss": 6.1304707527160645
    },
    {
      "epoch": 0.17490514905149052,
      "step": 3227,
      "training_loss": 5.204795837402344
    },
    {
      "epoch": 0.17495934959349593,
      "grad_norm": 27.94009017944336,
      "learning_rate": 1e-05,
      "loss": 6.3784,
      "step": 3228
    },
    {
      "epoch": 0.17495934959349593,
      "step": 3228,
      "training_loss": 7.87689208984375
    },
    {
      "epoch": 0.17501355013550135,
      "step": 3229,
      "training_loss": 8.491917610168457
    },
    {
      "epoch": 0.17506775067750677,
      "step": 3230,
      "training_loss": 6.5653076171875
    },
    {
      "epoch": 0.17512195121951218,
      "step": 3231,
      "training_loss": 7.178850173950195
    },
    {
      "epoch": 0.17517615176151763,
      "grad_norm": 15.335867881774902,
      "learning_rate": 1e-05,
      "loss": 7.5282,
      "step": 3232
    },
    {
      "epoch": 0.17517615176151763,
      "step": 3232,
      "training_loss": 6.095301628112793
    },
    {
      "epoch": 0.17523035230352305,
      "step": 3233,
      "training_loss": 7.424407005310059
    },
    {
      "epoch": 0.17528455284552846,
      "step": 3234,
      "training_loss": 4.420038223266602
    },
    {
      "epoch": 0.17533875338753388,
      "step": 3235,
      "training_loss": 6.062870979309082
    },
    {
      "epoch": 0.1753929539295393,
      "grad_norm": 26.156465530395508,
      "learning_rate": 1e-05,
      "loss": 6.0007,
      "step": 3236
    },
    {
      "epoch": 0.1753929539295393,
      "step": 3236,
      "training_loss": 6.944444179534912
    },
    {
      "epoch": 0.17544715447154471,
      "step": 3237,
      "training_loss": 7.077493667602539
    },
    {
      "epoch": 0.17550135501355013,
      "step": 3238,
      "training_loss": 7.582773208618164
    },
    {
      "epoch": 0.17555555555555555,
      "step": 3239,
      "training_loss": 7.386801719665527
    },
    {
      "epoch": 0.17560975609756097,
      "grad_norm": 14.9144287109375,
      "learning_rate": 1e-05,
      "loss": 7.2479,
      "step": 3240
    },
    {
      "epoch": 0.17560975609756097,
      "step": 3240,
      "training_loss": 7.715625286102295
    },
    {
      "epoch": 0.17566395663956638,
      "step": 3241,
      "training_loss": 7.068658828735352
    },
    {
      "epoch": 0.17571815718157183,
      "step": 3242,
      "training_loss": 6.4366679191589355
    },
    {
      "epoch": 0.17577235772357724,
      "step": 3243,
      "training_loss": 7.5511698722839355
    },
    {
      "epoch": 0.17582655826558266,
      "grad_norm": 21.348920822143555,
      "learning_rate": 1e-05,
      "loss": 7.193,
      "step": 3244
    },
    {
      "epoch": 0.17582655826558266,
      "step": 3244,
      "training_loss": 4.703373432159424
    },
    {
      "epoch": 0.17588075880758808,
      "step": 3245,
      "training_loss": 7.087835788726807
    },
    {
      "epoch": 0.1759349593495935,
      "step": 3246,
      "training_loss": 3.859253168106079
    },
    {
      "epoch": 0.1759891598915989,
      "step": 3247,
      "training_loss": 6.577881336212158
    },
    {
      "epoch": 0.17604336043360433,
      "grad_norm": 22.96194839477539,
      "learning_rate": 1e-05,
      "loss": 5.5571,
      "step": 3248
    },
    {
      "epoch": 0.17604336043360433,
      "step": 3248,
      "training_loss": 6.751131057739258
    },
    {
      "epoch": 0.17609756097560975,
      "step": 3249,
      "training_loss": 6.933318614959717
    },
    {
      "epoch": 0.17615176151761516,
      "step": 3250,
      "training_loss": 6.484048843383789
    },
    {
      "epoch": 0.1762059620596206,
      "step": 3251,
      "training_loss": 8.3451566696167
    },
    {
      "epoch": 0.17626016260162602,
      "grad_norm": 30.493253707885742,
      "learning_rate": 1e-05,
      "loss": 7.1284,
      "step": 3252
    },
    {
      "epoch": 0.17626016260162602,
      "step": 3252,
      "training_loss": 6.663568019866943
    },
    {
      "epoch": 0.17631436314363144,
      "step": 3253,
      "training_loss": 7.993785381317139
    },
    {
      "epoch": 0.17636856368563686,
      "step": 3254,
      "training_loss": 5.558472156524658
    },
    {
      "epoch": 0.17642276422764228,
      "step": 3255,
      "training_loss": 6.624032497406006
    },
    {
      "epoch": 0.1764769647696477,
      "grad_norm": 22.208112716674805,
      "learning_rate": 1e-05,
      "loss": 6.71,
      "step": 3256
    },
    {
      "epoch": 0.1764769647696477,
      "step": 3256,
      "training_loss": 6.806300163269043
    },
    {
      "epoch": 0.1765311653116531,
      "step": 3257,
      "training_loss": 5.946181297302246
    },
    {
      "epoch": 0.17658536585365853,
      "step": 3258,
      "training_loss": 8.124853134155273
    },
    {
      "epoch": 0.17663956639566394,
      "step": 3259,
      "training_loss": 6.726989269256592
    },
    {
      "epoch": 0.1766937669376694,
      "grad_norm": 17.16012954711914,
      "learning_rate": 1e-05,
      "loss": 6.9011,
      "step": 3260
    },
    {
      "epoch": 0.1766937669376694,
      "step": 3260,
      "training_loss": 6.624676704406738
    },
    {
      "epoch": 0.1767479674796748,
      "step": 3261,
      "training_loss": 6.152055263519287
    },
    {
      "epoch": 0.17680216802168022,
      "step": 3262,
      "training_loss": 6.597365856170654
    },
    {
      "epoch": 0.17685636856368564,
      "step": 3263,
      "training_loss": 7.128118515014648
    },
    {
      "epoch": 0.17691056910569106,
      "grad_norm": 26.104700088500977,
      "learning_rate": 1e-05,
      "loss": 6.6256,
      "step": 3264
    },
    {
      "epoch": 0.17691056910569106,
      "step": 3264,
      "training_loss": 7.03904914855957
    },
    {
      "epoch": 0.17696476964769647,
      "step": 3265,
      "training_loss": 7.596627712249756
    },
    {
      "epoch": 0.1770189701897019,
      "step": 3266,
      "training_loss": 5.742495059967041
    },
    {
      "epoch": 0.1770731707317073,
      "step": 3267,
      "training_loss": 7.074875831604004
    },
    {
      "epoch": 0.17712737127371272,
      "grad_norm": 19.43735122680664,
      "learning_rate": 1e-05,
      "loss": 6.8633,
      "step": 3268
    },
    {
      "epoch": 0.17712737127371272,
      "step": 3268,
      "training_loss": 7.695891380310059
    },
    {
      "epoch": 0.17718157181571817,
      "step": 3269,
      "training_loss": 6.900552272796631
    },
    {
      "epoch": 0.1772357723577236,
      "step": 3270,
      "training_loss": 5.291436195373535
    },
    {
      "epoch": 0.177289972899729,
      "step": 3271,
      "training_loss": 6.013469696044922
    },
    {
      "epoch": 0.17734417344173442,
      "grad_norm": 16.803646087646484,
      "learning_rate": 1e-05,
      "loss": 6.4753,
      "step": 3272
    },
    {
      "epoch": 0.17734417344173442,
      "step": 3272,
      "training_loss": 7.2746663093566895
    },
    {
      "epoch": 0.17739837398373984,
      "step": 3273,
      "training_loss": 6.102732181549072
    },
    {
      "epoch": 0.17745257452574525,
      "step": 3274,
      "training_loss": 7.757063865661621
    },
    {
      "epoch": 0.17750677506775067,
      "step": 3275,
      "training_loss": 6.618338108062744
    },
    {
      "epoch": 0.1775609756097561,
      "grad_norm": 19.317054748535156,
      "learning_rate": 1e-05,
      "loss": 6.9382,
      "step": 3276
    },
    {
      "epoch": 0.1775609756097561,
      "step": 3276,
      "training_loss": 7.068641662597656
    },
    {
      "epoch": 0.1776151761517615,
      "step": 3277,
      "training_loss": 6.432555675506592
    },
    {
      "epoch": 0.17766937669376695,
      "step": 3278,
      "training_loss": 4.457352161407471
    },
    {
      "epoch": 0.17772357723577237,
      "step": 3279,
      "training_loss": 6.704146862030029
    },
    {
      "epoch": 0.17777777777777778,
      "grad_norm": 23.30915069580078,
      "learning_rate": 1e-05,
      "loss": 6.1657,
      "step": 3280
    },
    {
      "epoch": 0.17777777777777778,
      "step": 3280,
      "training_loss": 6.910881519317627
    },
    {
      "epoch": 0.1778319783197832,
      "step": 3281,
      "training_loss": 7.2862935066223145
    },
    {
      "epoch": 0.17788617886178862,
      "step": 3282,
      "training_loss": 7.420891284942627
    },
    {
      "epoch": 0.17794037940379404,
      "step": 3283,
      "training_loss": 6.523392200469971
    },
    {
      "epoch": 0.17799457994579945,
      "grad_norm": 28.09433937072754,
      "learning_rate": 1e-05,
      "loss": 7.0354,
      "step": 3284
    },
    {
      "epoch": 0.17799457994579945,
      "step": 3284,
      "training_loss": 8.242647171020508
    },
    {
      "epoch": 0.17804878048780487,
      "step": 3285,
      "training_loss": 6.408278465270996
    },
    {
      "epoch": 0.1781029810298103,
      "step": 3286,
      "training_loss": 6.981882095336914
    },
    {
      "epoch": 0.17815718157181573,
      "step": 3287,
      "training_loss": 7.152814865112305
    },
    {
      "epoch": 0.17821138211382115,
      "grad_norm": 21.75108528137207,
      "learning_rate": 1e-05,
      "loss": 7.1964,
      "step": 3288
    },
    {
      "epoch": 0.17821138211382115,
      "step": 3288,
      "training_loss": 6.5876264572143555
    },
    {
      "epoch": 0.17826558265582657,
      "step": 3289,
      "training_loss": 7.191148281097412
    },
    {
      "epoch": 0.17831978319783198,
      "step": 3290,
      "training_loss": 5.793615341186523
    },
    {
      "epoch": 0.1783739837398374,
      "step": 3291,
      "training_loss": 7.491645812988281
    },
    {
      "epoch": 0.17842818428184282,
      "grad_norm": 15.205506324768066,
      "learning_rate": 1e-05,
      "loss": 6.766,
      "step": 3292
    },
    {
      "epoch": 0.17842818428184282,
      "step": 3292,
      "training_loss": 7.271615505218506
    },
    {
      "epoch": 0.17848238482384823,
      "step": 3293,
      "training_loss": 8.042524337768555
    },
    {
      "epoch": 0.17853658536585365,
      "step": 3294,
      "training_loss": 5.623101711273193
    },
    {
      "epoch": 0.17859078590785907,
      "step": 3295,
      "training_loss": 3.649507522583008
    },
    {
      "epoch": 0.1786449864498645,
      "grad_norm": 22.416088104248047,
      "learning_rate": 1e-05,
      "loss": 6.1467,
      "step": 3296
    },
    {
      "epoch": 0.1786449864498645,
      "step": 3296,
      "training_loss": 6.499926567077637
    },
    {
      "epoch": 0.17869918699186993,
      "step": 3297,
      "training_loss": 7.30272912979126
    },
    {
      "epoch": 0.17875338753387535,
      "step": 3298,
      "training_loss": 7.140902519226074
    },
    {
      "epoch": 0.17880758807588076,
      "step": 3299,
      "training_loss": 5.435976505279541
    },
    {
      "epoch": 0.17886178861788618,
      "grad_norm": 15.576151847839355,
      "learning_rate": 1e-05,
      "loss": 6.5949,
      "step": 3300
    },
    {
      "epoch": 0.17886178861788618,
      "step": 3300,
      "training_loss": 7.834137439727783
    },
    {
      "epoch": 0.1789159891598916,
      "step": 3301,
      "training_loss": 6.638140678405762
    },
    {
      "epoch": 0.17897018970189701,
      "step": 3302,
      "training_loss": 6.912834644317627
    },
    {
      "epoch": 0.17902439024390243,
      "step": 3303,
      "training_loss": 6.344339370727539
    },
    {
      "epoch": 0.17907859078590785,
      "grad_norm": 18.460460662841797,
      "learning_rate": 1e-05,
      "loss": 6.9324,
      "step": 3304
    },
    {
      "epoch": 0.17907859078590785,
      "step": 3304,
      "training_loss": 7.457200050354004
    },
    {
      "epoch": 0.17913279132791327,
      "step": 3305,
      "training_loss": 6.275417327880859
    },
    {
      "epoch": 0.1791869918699187,
      "step": 3306,
      "training_loss": 7.31915807723999
    },
    {
      "epoch": 0.17924119241192413,
      "step": 3307,
      "training_loss": 7.382887840270996
    },
    {
      "epoch": 0.17929539295392954,
      "grad_norm": 31.38921546936035,
      "learning_rate": 1e-05,
      "loss": 7.1087,
      "step": 3308
    },
    {
      "epoch": 0.17929539295392954,
      "step": 3308,
      "training_loss": 7.389094829559326
    },
    {
      "epoch": 0.17934959349593496,
      "step": 3309,
      "training_loss": 7.320819854736328
    },
    {
      "epoch": 0.17940379403794038,
      "step": 3310,
      "training_loss": 7.33685302734375
    },
    {
      "epoch": 0.1794579945799458,
      "step": 3311,
      "training_loss": 5.047056674957275
    },
    {
      "epoch": 0.1795121951219512,
      "grad_norm": 26.34992027282715,
      "learning_rate": 1e-05,
      "loss": 6.7735,
      "step": 3312
    },
    {
      "epoch": 0.1795121951219512,
      "step": 3312,
      "training_loss": 6.802356719970703
    },
    {
      "epoch": 0.17956639566395663,
      "step": 3313,
      "training_loss": 7.65686559677124
    },
    {
      "epoch": 0.17962059620596205,
      "step": 3314,
      "training_loss": 7.394371509552002
    },
    {
      "epoch": 0.1796747967479675,
      "step": 3315,
      "training_loss": 7.776859760284424
    },
    {
      "epoch": 0.1797289972899729,
      "grad_norm": 32.12604904174805,
      "learning_rate": 1e-05,
      "loss": 7.4076,
      "step": 3316
    },
    {
      "epoch": 0.1797289972899729,
      "step": 3316,
      "training_loss": 7.0990495681762695
    },
    {
      "epoch": 0.17978319783197833,
      "step": 3317,
      "training_loss": 6.183715343475342
    },
    {
      "epoch": 0.17983739837398374,
      "step": 3318,
      "training_loss": 7.7558135986328125
    },
    {
      "epoch": 0.17989159891598916,
      "step": 3319,
      "training_loss": 6.838190078735352
    },
    {
      "epoch": 0.17994579945799458,
      "grad_norm": 26.998388290405273,
      "learning_rate": 1e-05,
      "loss": 6.9692,
      "step": 3320
    },
    {
      "epoch": 0.17994579945799458,
      "step": 3320,
      "training_loss": 7.519218921661377
    },
    {
      "epoch": 0.18,
      "step": 3321,
      "training_loss": 7.506350517272949
    },
    {
      "epoch": 0.1800542005420054,
      "step": 3322,
      "training_loss": 7.00709867477417
    },
    {
      "epoch": 0.18010840108401083,
      "step": 3323,
      "training_loss": 6.29004430770874
    },
    {
      "epoch": 0.18016260162601627,
      "grad_norm": 23.616519927978516,
      "learning_rate": 1e-05,
      "loss": 7.0807,
      "step": 3324
    },
    {
      "epoch": 0.18016260162601627,
      "step": 3324,
      "training_loss": 7.54437255859375
    },
    {
      "epoch": 0.1802168021680217,
      "step": 3325,
      "training_loss": 6.813650608062744
    },
    {
      "epoch": 0.1802710027100271,
      "step": 3326,
      "training_loss": 5.768085956573486
    },
    {
      "epoch": 0.18032520325203252,
      "step": 3327,
      "training_loss": 4.22550630569458
    },
    {
      "epoch": 0.18037940379403794,
      "grad_norm": 24.43640899658203,
      "learning_rate": 1e-05,
      "loss": 6.0879,
      "step": 3328
    },
    {
      "epoch": 0.18037940379403794,
      "step": 3328,
      "training_loss": 6.517227649688721
    },
    {
      "epoch": 0.18043360433604336,
      "step": 3329,
      "training_loss": 6.517622470855713
    },
    {
      "epoch": 0.18048780487804877,
      "step": 3330,
      "training_loss": 7.013121128082275
    },
    {
      "epoch": 0.1805420054200542,
      "step": 3331,
      "training_loss": 6.851014614105225
    },
    {
      "epoch": 0.1805962059620596,
      "grad_norm": 27.333852767944336,
      "learning_rate": 1e-05,
      "loss": 6.7247,
      "step": 3332
    },
    {
      "epoch": 0.1805962059620596,
      "step": 3332,
      "training_loss": 5.669787406921387
    },
    {
      "epoch": 0.18065040650406505,
      "step": 3333,
      "training_loss": 7.989651203155518
    },
    {
      "epoch": 0.18070460704607047,
      "step": 3334,
      "training_loss": 7.478070259094238
    },
    {
      "epoch": 0.1807588075880759,
      "step": 3335,
      "training_loss": 5.707287788391113
    },
    {
      "epoch": 0.1808130081300813,
      "grad_norm": 16.548011779785156,
      "learning_rate": 1e-05,
      "loss": 6.7112,
      "step": 3336
    },
    {
      "epoch": 0.1808130081300813,
      "step": 3336,
      "training_loss": 7.21728515625
    },
    {
      "epoch": 0.18086720867208672,
      "step": 3337,
      "training_loss": 4.729776382446289
    },
    {
      "epoch": 0.18092140921409214,
      "step": 3338,
      "training_loss": 7.253871917724609
    },
    {
      "epoch": 0.18097560975609756,
      "step": 3339,
      "training_loss": 7.343036651611328
    },
    {
      "epoch": 0.18102981029810297,
      "grad_norm": 20.118356704711914,
      "learning_rate": 1e-05,
      "loss": 6.636,
      "step": 3340
    },
    {
      "epoch": 0.18102981029810297,
      "step": 3340,
      "training_loss": 6.8309221267700195
    },
    {
      "epoch": 0.1810840108401084,
      "step": 3341,
      "training_loss": 5.049726486206055
    },
    {
      "epoch": 0.18113821138211383,
      "step": 3342,
      "training_loss": 7.507279396057129
    },
    {
      "epoch": 0.18119241192411925,
      "step": 3343,
      "training_loss": 6.627949237823486
    },
    {
      "epoch": 0.18124661246612467,
      "grad_norm": 35.261566162109375,
      "learning_rate": 1e-05,
      "loss": 6.504,
      "step": 3344
    },
    {
      "epoch": 0.18124661246612467,
      "step": 3344,
      "training_loss": 6.978277683258057
    },
    {
      "epoch": 0.18130081300813009,
      "step": 3345,
      "training_loss": 7.691988468170166
    },
    {
      "epoch": 0.1813550135501355,
      "step": 3346,
      "training_loss": 6.0778045654296875
    },
    {
      "epoch": 0.18140921409214092,
      "step": 3347,
      "training_loss": 6.906822681427002
    },
    {
      "epoch": 0.18146341463414634,
      "grad_norm": 20.806055068969727,
      "learning_rate": 1e-05,
      "loss": 6.9137,
      "step": 3348
    },
    {
      "epoch": 0.18146341463414634,
      "step": 3348,
      "training_loss": 7.204168319702148
    },
    {
      "epoch": 0.18151761517615175,
      "step": 3349,
      "training_loss": 7.180367469787598
    },
    {
      "epoch": 0.18157181571815717,
      "step": 3350,
      "training_loss": 6.33204984664917
    },
    {
      "epoch": 0.18162601626016261,
      "step": 3351,
      "training_loss": 8.198505401611328
    },
    {
      "epoch": 0.18168021680216803,
      "grad_norm": 39.35863494873047,
      "learning_rate": 1e-05,
      "loss": 7.2288,
      "step": 3352
    },
    {
      "epoch": 0.18168021680216803,
      "step": 3352,
      "training_loss": 5.649402141571045
    },
    {
      "epoch": 0.18173441734417345,
      "step": 3353,
      "training_loss": 6.981473922729492
    },
    {
      "epoch": 0.18178861788617887,
      "step": 3354,
      "training_loss": 6.505300998687744
    },
    {
      "epoch": 0.18184281842818428,
      "step": 3355,
      "training_loss": 7.557161808013916
    },
    {
      "epoch": 0.1818970189701897,
      "grad_norm": 24.307798385620117,
      "learning_rate": 1e-05,
      "loss": 6.6733,
      "step": 3356
    },
    {
      "epoch": 0.1818970189701897,
      "step": 3356,
      "training_loss": 8.475086212158203
    },
    {
      "epoch": 0.18195121951219512,
      "step": 3357,
      "training_loss": 7.619329929351807
    },
    {
      "epoch": 0.18200542005420053,
      "step": 3358,
      "training_loss": 6.289558410644531
    },
    {
      "epoch": 0.18205962059620595,
      "step": 3359,
      "training_loss": 7.4788618087768555
    },
    {
      "epoch": 0.1821138211382114,
      "grad_norm": 48.06265640258789,
      "learning_rate": 1e-05,
      "loss": 7.4657,
      "step": 3360
    },
    {
      "epoch": 0.1821138211382114,
      "step": 3360,
      "training_loss": 5.435580730438232
    },
    {
      "epoch": 0.1821680216802168,
      "step": 3361,
      "training_loss": 5.644412517547607
    },
    {
      "epoch": 0.18222222222222223,
      "step": 3362,
      "training_loss": 7.408228874206543
    },
    {
      "epoch": 0.18227642276422765,
      "step": 3363,
      "training_loss": 7.279889106750488
    },
    {
      "epoch": 0.18233062330623306,
      "grad_norm": 17.207822799682617,
      "learning_rate": 1e-05,
      "loss": 6.442,
      "step": 3364
    },
    {
      "epoch": 0.18233062330623306,
      "step": 3364,
      "training_loss": 6.933803558349609
    },
    {
      "epoch": 0.18238482384823848,
      "step": 3365,
      "training_loss": 4.771103382110596
    },
    {
      "epoch": 0.1824390243902439,
      "step": 3366,
      "training_loss": 7.1827473640441895
    },
    {
      "epoch": 0.18249322493224931,
      "step": 3367,
      "training_loss": 7.059759140014648
    },
    {
      "epoch": 0.18254742547425473,
      "grad_norm": 16.411314010620117,
      "learning_rate": 1e-05,
      "loss": 6.4869,
      "step": 3368
    },
    {
      "epoch": 0.18254742547425473,
      "step": 3368,
      "training_loss": 7.067473888397217
    },
    {
      "epoch": 0.18260162601626015,
      "step": 3369,
      "training_loss": 7.472407817840576
    },
    {
      "epoch": 0.1826558265582656,
      "step": 3370,
      "training_loss": 7.13031005859375
    },
    {
      "epoch": 0.182710027100271,
      "step": 3371,
      "training_loss": 7.098349094390869
    },
    {
      "epoch": 0.18276422764227643,
      "grad_norm": 31.09568214416504,
      "learning_rate": 1e-05,
      "loss": 7.1921,
      "step": 3372
    },
    {
      "epoch": 0.18276422764227643,
      "step": 3372,
      "training_loss": 6.898789882659912
    },
    {
      "epoch": 0.18281842818428184,
      "step": 3373,
      "training_loss": 7.289907455444336
    },
    {
      "epoch": 0.18287262872628726,
      "step": 3374,
      "training_loss": 7.373929500579834
    },
    {
      "epoch": 0.18292682926829268,
      "step": 3375,
      "training_loss": 7.536787033081055
    },
    {
      "epoch": 0.1829810298102981,
      "grad_norm": 18.382659912109375,
      "learning_rate": 1e-05,
      "loss": 7.2749,
      "step": 3376
    },
    {
      "epoch": 0.1829810298102981,
      "step": 3376,
      "training_loss": 8.545639991760254
    },
    {
      "epoch": 0.1830352303523035,
      "step": 3377,
      "training_loss": 7.308839321136475
    },
    {
      "epoch": 0.18308943089430893,
      "step": 3378,
      "training_loss": 6.600259304046631
    },
    {
      "epoch": 0.18314363143631437,
      "step": 3379,
      "training_loss": 7.262246131896973
    },
    {
      "epoch": 0.1831978319783198,
      "grad_norm": 37.13809585571289,
      "learning_rate": 1e-05,
      "loss": 7.4292,
      "step": 3380
    },
    {
      "epoch": 0.1831978319783198,
      "step": 3380,
      "training_loss": 4.7367048263549805
    },
    {
      "epoch": 0.1832520325203252,
      "step": 3381,
      "training_loss": 7.384112358093262
    },
    {
      "epoch": 0.18330623306233063,
      "step": 3382,
      "training_loss": 6.965579032897949
    },
    {
      "epoch": 0.18336043360433604,
      "step": 3383,
      "training_loss": 6.959291458129883
    },
    {
      "epoch": 0.18341463414634146,
      "grad_norm": 25.015670776367188,
      "learning_rate": 1e-05,
      "loss": 6.5114,
      "step": 3384
    },
    {
      "epoch": 0.18341463414634146,
      "step": 3384,
      "training_loss": 6.952235698699951
    },
    {
      "epoch": 0.18346883468834688,
      "step": 3385,
      "training_loss": 6.194606781005859
    },
    {
      "epoch": 0.1835230352303523,
      "step": 3386,
      "training_loss": 6.711449146270752
    },
    {
      "epoch": 0.1835772357723577,
      "step": 3387,
      "training_loss": 5.779006004333496
    },
    {
      "epoch": 0.18363143631436316,
      "grad_norm": 23.652664184570312,
      "learning_rate": 1e-05,
      "loss": 6.4093,
      "step": 3388
    },
    {
      "epoch": 0.18363143631436316,
      "step": 3388,
      "training_loss": 7.395166873931885
    },
    {
      "epoch": 0.18368563685636857,
      "step": 3389,
      "training_loss": 6.696549892425537
    },
    {
      "epoch": 0.183739837398374,
      "step": 3390,
      "training_loss": 5.127013683319092
    },
    {
      "epoch": 0.1837940379403794,
      "step": 3391,
      "training_loss": 6.125940799713135
    },
    {
      "epoch": 0.18384823848238482,
      "grad_norm": 25.875394821166992,
      "learning_rate": 1e-05,
      "loss": 6.3362,
      "step": 3392
    },
    {
      "epoch": 0.18384823848238482,
      "step": 3392,
      "training_loss": 7.43597412109375
    },
    {
      "epoch": 0.18390243902439024,
      "step": 3393,
      "training_loss": 6.802240371704102
    },
    {
      "epoch": 0.18395663956639566,
      "step": 3394,
      "training_loss": 6.806080341339111
    },
    {
      "epoch": 0.18401084010840107,
      "step": 3395,
      "training_loss": 6.915140151977539
    },
    {
      "epoch": 0.1840650406504065,
      "grad_norm": 20.619792938232422,
      "learning_rate": 1e-05,
      "loss": 6.9899,
      "step": 3396
    },
    {
      "epoch": 0.1840650406504065,
      "step": 3396,
      "training_loss": 6.334334373474121
    },
    {
      "epoch": 0.18411924119241194,
      "step": 3397,
      "training_loss": 7.213754653930664
    },
    {
      "epoch": 0.18417344173441735,
      "step": 3398,
      "training_loss": 6.9066925048828125
    },
    {
      "epoch": 0.18422764227642277,
      "step": 3399,
      "training_loss": 7.491916656494141
    },
    {
      "epoch": 0.1842818428184282,
      "grad_norm": 25.376258850097656,
      "learning_rate": 1e-05,
      "loss": 6.9867,
      "step": 3400
    },
    {
      "epoch": 0.1842818428184282,
      "step": 3400,
      "training_loss": 8.201772689819336
    },
    {
      "epoch": 0.1843360433604336,
      "step": 3401,
      "training_loss": 4.462465286254883
    },
    {
      "epoch": 0.18439024390243902,
      "step": 3402,
      "training_loss": 6.443544387817383
    },
    {
      "epoch": 0.18444444444444444,
      "step": 3403,
      "training_loss": 6.162970066070557
    },
    {
      "epoch": 0.18449864498644986,
      "grad_norm": 16.94151496887207,
      "learning_rate": 1e-05,
      "loss": 6.3177,
      "step": 3404
    },
    {
      "epoch": 0.18449864498644986,
      "step": 3404,
      "training_loss": 5.379667282104492
    },
    {
      "epoch": 0.18455284552845527,
      "step": 3405,
      "training_loss": 7.251896381378174
    },
    {
      "epoch": 0.18460704607046072,
      "step": 3406,
      "training_loss": 6.159588813781738
    },
    {
      "epoch": 0.18466124661246613,
      "step": 3407,
      "training_loss": 7.735924243927002
    },
    {
      "epoch": 0.18471544715447155,
      "grad_norm": 28.975675582885742,
      "learning_rate": 1e-05,
      "loss": 6.6318,
      "step": 3408
    },
    {
      "epoch": 0.18471544715447155,
      "step": 3408,
      "training_loss": 6.941023826599121
    },
    {
      "epoch": 0.18476964769647697,
      "step": 3409,
      "training_loss": 7.57454252243042
    },
    {
      "epoch": 0.18482384823848239,
      "step": 3410,
      "training_loss": 6.454051494598389
    },
    {
      "epoch": 0.1848780487804878,
      "step": 3411,
      "training_loss": 6.925812244415283
    },
    {
      "epoch": 0.18493224932249322,
      "grad_norm": 16.656503677368164,
      "learning_rate": 1e-05,
      "loss": 6.9739,
      "step": 3412
    },
    {
      "epoch": 0.18493224932249322,
      "step": 3412,
      "training_loss": 6.546768665313721
    },
    {
      "epoch": 0.18498644986449864,
      "step": 3413,
      "training_loss": 5.75166130065918
    },
    {
      "epoch": 0.18504065040650405,
      "step": 3414,
      "training_loss": 6.919075965881348
    },
    {
      "epoch": 0.1850948509485095,
      "step": 3415,
      "training_loss": 7.345009803771973
    },
    {
      "epoch": 0.18514905149051492,
      "grad_norm": 17.101131439208984,
      "learning_rate": 1e-05,
      "loss": 6.6406,
      "step": 3416
    },
    {
      "epoch": 0.18514905149051492,
      "step": 3416,
      "training_loss": 6.298130989074707
    },
    {
      "epoch": 0.18520325203252033,
      "step": 3417,
      "training_loss": 6.830080509185791
    },
    {
      "epoch": 0.18525745257452575,
      "step": 3418,
      "training_loss": 9.819458961486816
    },
    {
      "epoch": 0.18531165311653117,
      "step": 3419,
      "training_loss": 6.7071709632873535
    },
    {
      "epoch": 0.18536585365853658,
      "grad_norm": 20.146581649780273,
      "learning_rate": 1e-05,
      "loss": 7.4137,
      "step": 3420
    },
    {
      "epoch": 0.18536585365853658,
      "step": 3420,
      "training_loss": 7.384185791015625
    },
    {
      "epoch": 0.185420054200542,
      "step": 3421,
      "training_loss": 6.3185224533081055
    },
    {
      "epoch": 0.18547425474254742,
      "step": 3422,
      "training_loss": 5.7343010902404785
    },
    {
      "epoch": 0.18552845528455283,
      "step": 3423,
      "training_loss": 7.767590522766113
    },
    {
      "epoch": 0.18558265582655828,
      "grad_norm": 20.473827362060547,
      "learning_rate": 1e-05,
      "loss": 6.8012,
      "step": 3424
    },
    {
      "epoch": 0.18558265582655828,
      "step": 3424,
      "training_loss": 7.124728202819824
    },
    {
      "epoch": 0.1856368563685637,
      "step": 3425,
      "training_loss": 6.565446376800537
    },
    {
      "epoch": 0.1856910569105691,
      "step": 3426,
      "training_loss": 6.312071323394775
    },
    {
      "epoch": 0.18574525745257453,
      "step": 3427,
      "training_loss": 6.863083362579346
    },
    {
      "epoch": 0.18579945799457995,
      "grad_norm": 19.267183303833008,
      "learning_rate": 1e-05,
      "loss": 6.7163,
      "step": 3428
    },
    {
      "epoch": 0.18579945799457995,
      "step": 3428,
      "training_loss": 6.901342868804932
    },
    {
      "epoch": 0.18585365853658536,
      "step": 3429,
      "training_loss": 6.820026874542236
    },
    {
      "epoch": 0.18590785907859078,
      "step": 3430,
      "training_loss": 6.04063606262207
    },
    {
      "epoch": 0.1859620596205962,
      "step": 3431,
      "training_loss": 7.929154872894287
    },
    {
      "epoch": 0.18601626016260162,
      "grad_norm": 22.01738739013672,
      "learning_rate": 1e-05,
      "loss": 6.9228,
      "step": 3432
    },
    {
      "epoch": 0.18601626016260162,
      "step": 3432,
      "training_loss": 5.661021709442139
    },
    {
      "epoch": 0.18607046070460703,
      "step": 3433,
      "training_loss": 5.103138446807861
    },
    {
      "epoch": 0.18612466124661248,
      "step": 3434,
      "training_loss": 7.084292888641357
    },
    {
      "epoch": 0.1861788617886179,
      "step": 3435,
      "training_loss": 5.237099647521973
    },
    {
      "epoch": 0.1862330623306233,
      "grad_norm": 22.17915153503418,
      "learning_rate": 1e-05,
      "loss": 5.7714,
      "step": 3436
    },
    {
      "epoch": 0.1862330623306233,
      "step": 3436,
      "training_loss": 6.630657196044922
    },
    {
      "epoch": 0.18628726287262873,
      "step": 3437,
      "training_loss": 8.178718566894531
    },
    {
      "epoch": 0.18634146341463415,
      "step": 3438,
      "training_loss": 6.116684436798096
    },
    {
      "epoch": 0.18639566395663956,
      "step": 3439,
      "training_loss": 7.480030059814453
    },
    {
      "epoch": 0.18644986449864498,
      "grad_norm": 23.384624481201172,
      "learning_rate": 1e-05,
      "loss": 7.1015,
      "step": 3440
    },
    {
      "epoch": 0.18644986449864498,
      "step": 3440,
      "training_loss": 6.911051273345947
    },
    {
      "epoch": 0.1865040650406504,
      "step": 3441,
      "training_loss": 6.944309711456299
    },
    {
      "epoch": 0.1865582655826558,
      "step": 3442,
      "training_loss": 7.681085586547852
    },
    {
      "epoch": 0.18661246612466126,
      "step": 3443,
      "training_loss": 7.258789539337158
    },
    {
      "epoch": 0.18666666666666668,
      "grad_norm": 24.848281860351562,
      "learning_rate": 1e-05,
      "loss": 7.1988,
      "step": 3444
    },
    {
      "epoch": 0.18666666666666668,
      "step": 3444,
      "training_loss": 8.989505767822266
    },
    {
      "epoch": 0.1867208672086721,
      "step": 3445,
      "training_loss": 7.3001604080200195
    },
    {
      "epoch": 0.1867750677506775,
      "step": 3446,
      "training_loss": 6.821796417236328
    },
    {
      "epoch": 0.18682926829268293,
      "step": 3447,
      "training_loss": 6.781571388244629
    },
    {
      "epoch": 0.18688346883468834,
      "grad_norm": 40.569580078125,
      "learning_rate": 1e-05,
      "loss": 7.4733,
      "step": 3448
    },
    {
      "epoch": 0.18688346883468834,
      "step": 3448,
      "training_loss": 7.203474998474121
    },
    {
      "epoch": 0.18693766937669376,
      "step": 3449,
      "training_loss": 6.693836212158203
    },
    {
      "epoch": 0.18699186991869918,
      "step": 3450,
      "training_loss": 6.9434380531311035
    },
    {
      "epoch": 0.1870460704607046,
      "step": 3451,
      "training_loss": 6.708651065826416
    },
    {
      "epoch": 0.18710027100271004,
      "grad_norm": 51.20587158203125,
      "learning_rate": 1e-05,
      "loss": 6.8874,
      "step": 3452
    },
    {
      "epoch": 0.18710027100271004,
      "step": 3452,
      "training_loss": 7.080171585083008
    },
    {
      "epoch": 0.18715447154471546,
      "step": 3453,
      "training_loss": 7.378125190734863
    },
    {
      "epoch": 0.18720867208672087,
      "step": 3454,
      "training_loss": 5.928691387176514
    },
    {
      "epoch": 0.1872628726287263,
      "step": 3455,
      "training_loss": 6.236708641052246
    },
    {
      "epoch": 0.1873170731707317,
      "grad_norm": 21.638193130493164,
      "learning_rate": 1e-05,
      "loss": 6.6559,
      "step": 3456
    },
    {
      "epoch": 0.1873170731707317,
      "step": 3456,
      "training_loss": 7.058627128601074
    },
    {
      "epoch": 0.18737127371273712,
      "step": 3457,
      "training_loss": 6.924045562744141
    },
    {
      "epoch": 0.18742547425474254,
      "step": 3458,
      "training_loss": 6.858800411224365
    },
    {
      "epoch": 0.18747967479674796,
      "step": 3459,
      "training_loss": 6.867778778076172
    },
    {
      "epoch": 0.18753387533875338,
      "grad_norm": 24.784788131713867,
      "learning_rate": 1e-05,
      "loss": 6.9273,
      "step": 3460
    },
    {
      "epoch": 0.18753387533875338,
      "step": 3460,
      "training_loss": 6.287567138671875
    },
    {
      "epoch": 0.18758807588075882,
      "step": 3461,
      "training_loss": 6.86269474029541
    },
    {
      "epoch": 0.18764227642276424,
      "step": 3462,
      "training_loss": 6.133185863494873
    },
    {
      "epoch": 0.18769647696476965,
      "step": 3463,
      "training_loss": 6.548514366149902
    },
    {
      "epoch": 0.18775067750677507,
      "grad_norm": 20.304061889648438,
      "learning_rate": 1e-05,
      "loss": 6.458,
      "step": 3464
    },
    {
      "epoch": 0.18775067750677507,
      "step": 3464,
      "training_loss": 7.936813831329346
    },
    {
      "epoch": 0.1878048780487805,
      "step": 3465,
      "training_loss": 7.206125259399414
    },
    {
      "epoch": 0.1878590785907859,
      "step": 3466,
      "training_loss": 8.0089693069458
    },
    {
      "epoch": 0.18791327913279132,
      "step": 3467,
      "training_loss": 6.7452874183654785
    },
    {
      "epoch": 0.18796747967479674,
      "grad_norm": 19.755260467529297,
      "learning_rate": 1e-05,
      "loss": 7.4743,
      "step": 3468
    },
    {
      "epoch": 0.18796747967479674,
      "step": 3468,
      "training_loss": 5.797020435333252
    },
    {
      "epoch": 0.18802168021680216,
      "step": 3469,
      "training_loss": 7.387396812438965
    },
    {
      "epoch": 0.1880758807588076,
      "step": 3470,
      "training_loss": 6.846914768218994
    },
    {
      "epoch": 0.18813008130081302,
      "step": 3471,
      "training_loss": 7.254397392272949
    },
    {
      "epoch": 0.18818428184281843,
      "grad_norm": 25.550495147705078,
      "learning_rate": 1e-05,
      "loss": 6.8214,
      "step": 3472
    },
    {
      "epoch": 0.18818428184281843,
      "step": 3472,
      "training_loss": 7.41315221786499
    },
    {
      "epoch": 0.18823848238482385,
      "step": 3473,
      "training_loss": 8.467247009277344
    },
    {
      "epoch": 0.18829268292682927,
      "step": 3474,
      "training_loss": 7.787130832672119
    },
    {
      "epoch": 0.18834688346883469,
      "step": 3475,
      "training_loss": 6.269234657287598
    },
    {
      "epoch": 0.1884010840108401,
      "grad_norm": 17.83312225341797,
      "learning_rate": 1e-05,
      "loss": 7.4842,
      "step": 3476
    },
    {
      "epoch": 0.1884010840108401,
      "step": 3476,
      "training_loss": 6.103306293487549
    },
    {
      "epoch": 0.18845528455284552,
      "step": 3477,
      "training_loss": 7.817312717437744
    },
    {
      "epoch": 0.18850948509485094,
      "step": 3478,
      "training_loss": 6.307263374328613
    },
    {
      "epoch": 0.18856368563685638,
      "step": 3479,
      "training_loss": 7.446882247924805
    },
    {
      "epoch": 0.1886178861788618,
      "grad_norm": 21.80047035217285,
      "learning_rate": 1e-05,
      "loss": 6.9187,
      "step": 3480
    },
    {
      "epoch": 0.1886178861788618,
      "step": 3480,
      "training_loss": 6.416016101837158
    },
    {
      "epoch": 0.18867208672086722,
      "step": 3481,
      "training_loss": 7.96049165725708
    },
    {
      "epoch": 0.18872628726287263,
      "step": 3482,
      "training_loss": 7.121279239654541
    },
    {
      "epoch": 0.18878048780487805,
      "step": 3483,
      "training_loss": 7.533348560333252
    },
    {
      "epoch": 0.18883468834688347,
      "grad_norm": 20.86664581298828,
      "learning_rate": 1e-05,
      "loss": 7.2578,
      "step": 3484
    },
    {
      "epoch": 0.18883468834688347,
      "step": 3484,
      "training_loss": 6.258667945861816
    },
    {
      "epoch": 0.18888888888888888,
      "step": 3485,
      "training_loss": 6.708319187164307
    },
    {
      "epoch": 0.1889430894308943,
      "step": 3486,
      "training_loss": 5.572155952453613
    },
    {
      "epoch": 0.18899728997289972,
      "step": 3487,
      "training_loss": 6.677277088165283
    },
    {
      "epoch": 0.18905149051490516,
      "grad_norm": 46.096649169921875,
      "learning_rate": 1e-05,
      "loss": 6.3041,
      "step": 3488
    },
    {
      "epoch": 0.18905149051490516,
      "step": 3488,
      "training_loss": 5.042290210723877
    },
    {
      "epoch": 0.18910569105691058,
      "step": 3489,
      "training_loss": 6.765042304992676
    },
    {
      "epoch": 0.189159891598916,
      "step": 3490,
      "training_loss": 6.946923732757568
    },
    {
      "epoch": 0.1892140921409214,
      "step": 3491,
      "training_loss": 7.128706932067871
    },
    {
      "epoch": 0.18926829268292683,
      "grad_norm": 19.224590301513672,
      "learning_rate": 1e-05,
      "loss": 6.4707,
      "step": 3492
    },
    {
      "epoch": 0.18926829268292683,
      "step": 3492,
      "training_loss": 6.520163536071777
    },
    {
      "epoch": 0.18932249322493225,
      "step": 3493,
      "training_loss": 5.814317226409912
    },
    {
      "epoch": 0.18937669376693766,
      "step": 3494,
      "training_loss": 4.713820934295654
    },
    {
      "epoch": 0.18943089430894308,
      "step": 3495,
      "training_loss": 6.914499282836914
    },
    {
      "epoch": 0.1894850948509485,
      "grad_norm": 25.591083526611328,
      "learning_rate": 1e-05,
      "loss": 5.9907,
      "step": 3496
    },
    {
      "epoch": 0.1894850948509485,
      "step": 3496,
      "training_loss": 5.882189750671387
    },
    {
      "epoch": 0.18953929539295392,
      "step": 3497,
      "training_loss": 6.789541244506836
    },
    {
      "epoch": 0.18959349593495936,
      "step": 3498,
      "training_loss": 8.489331245422363
    },
    {
      "epoch": 0.18964769647696478,
      "step": 3499,
      "training_loss": 8.511167526245117
    },
    {
      "epoch": 0.1897018970189702,
      "grad_norm": 62.952980041503906,
      "learning_rate": 1e-05,
      "loss": 7.4181,
      "step": 3500
    },
    {
      "epoch": 0.1897018970189702,
      "step": 3500,
      "training_loss": 7.399101734161377
    },
    {
      "epoch": 0.1897560975609756,
      "step": 3501,
      "training_loss": 7.1829514503479
    },
    {
      "epoch": 0.18981029810298103,
      "step": 3502,
      "training_loss": 7.134843349456787
    },
    {
      "epoch": 0.18986449864498645,
      "step": 3503,
      "training_loss": 6.642043113708496
    },
    {
      "epoch": 0.18991869918699186,
      "grad_norm": 23.504793167114258,
      "learning_rate": 1e-05,
      "loss": 7.0897,
      "step": 3504
    },
    {
      "epoch": 0.18991869918699186,
      "step": 3504,
      "training_loss": 6.446897983551025
    },
    {
      "epoch": 0.18997289972899728,
      "step": 3505,
      "training_loss": 7.196646213531494
    },
    {
      "epoch": 0.1900271002710027,
      "step": 3506,
      "training_loss": 7.2335357666015625
    },
    {
      "epoch": 0.19008130081300814,
      "step": 3507,
      "training_loss": 3.8451244831085205
    },
    {
      "epoch": 0.19013550135501356,
      "grad_norm": 31.52877426147461,
      "learning_rate": 1e-05,
      "loss": 6.1806,
      "step": 3508
    },
    {
      "epoch": 0.19013550135501356,
      "step": 3508,
      "training_loss": 8.038898468017578
    },
    {
      "epoch": 0.19018970189701898,
      "step": 3509,
      "training_loss": 8.071480751037598
    },
    {
      "epoch": 0.1902439024390244,
      "step": 3510,
      "training_loss": 7.032698154449463
    },
    {
      "epoch": 0.1902981029810298,
      "step": 3511,
      "training_loss": 6.798803806304932
    },
    {
      "epoch": 0.19035230352303523,
      "grad_norm": 19.408241271972656,
      "learning_rate": 1e-05,
      "loss": 7.4855,
      "step": 3512
    },
    {
      "epoch": 0.19035230352303523,
      "step": 3512,
      "training_loss": 8.390660285949707
    },
    {
      "epoch": 0.19040650406504064,
      "step": 3513,
      "training_loss": 5.963978290557861
    },
    {
      "epoch": 0.19046070460704606,
      "step": 3514,
      "training_loss": 8.271682739257812
    },
    {
      "epoch": 0.19051490514905148,
      "step": 3515,
      "training_loss": 6.1907453536987305
    },
    {
      "epoch": 0.19056910569105692,
      "grad_norm": 29.050853729248047,
      "learning_rate": 1e-05,
      "loss": 7.2043,
      "step": 3516
    },
    {
      "epoch": 0.19056910569105692,
      "step": 3516,
      "training_loss": 6.836751461029053
    },
    {
      "epoch": 0.19062330623306234,
      "step": 3517,
      "training_loss": 6.775646209716797
    },
    {
      "epoch": 0.19067750677506776,
      "step": 3518,
      "training_loss": 6.142198085784912
    },
    {
      "epoch": 0.19073170731707317,
      "step": 3519,
      "training_loss": 7.033194065093994
    },
    {
      "epoch": 0.1907859078590786,
      "grad_norm": 19.76761817932129,
      "learning_rate": 1e-05,
      "loss": 6.6969,
      "step": 3520
    },
    {
      "epoch": 0.1907859078590786,
      "step": 3520,
      "training_loss": 7.350064754486084
    },
    {
      "epoch": 0.190840108401084,
      "step": 3521,
      "training_loss": 7.468199253082275
    },
    {
      "epoch": 0.19089430894308942,
      "step": 3522,
      "training_loss": 6.538405895233154
    },
    {
      "epoch": 0.19094850948509484,
      "step": 3523,
      "training_loss": 6.253392219543457
    },
    {
      "epoch": 0.19100271002710026,
      "grad_norm": 27.41691017150879,
      "learning_rate": 1e-05,
      "loss": 6.9025,
      "step": 3524
    },
    {
      "epoch": 0.19100271002710026,
      "step": 3524,
      "training_loss": 7.649094104766846
    },
    {
      "epoch": 0.1910569105691057,
      "step": 3525,
      "training_loss": 6.199626445770264
    },
    {
      "epoch": 0.19111111111111112,
      "step": 3526,
      "training_loss": 6.262503623962402
    },
    {
      "epoch": 0.19116531165311654,
      "step": 3527,
      "training_loss": 7.09645938873291
    },
    {
      "epoch": 0.19121951219512195,
      "grad_norm": 19.283796310424805,
      "learning_rate": 1e-05,
      "loss": 6.8019,
      "step": 3528
    },
    {
      "epoch": 0.19121951219512195,
      "step": 3528,
      "training_loss": 6.632265567779541
    },
    {
      "epoch": 0.19127371273712737,
      "step": 3529,
      "training_loss": 7.377540111541748
    },
    {
      "epoch": 0.1913279132791328,
      "step": 3530,
      "training_loss": 7.22407341003418
    },
    {
      "epoch": 0.1913821138211382,
      "step": 3531,
      "training_loss": 6.923447132110596
    },
    {
      "epoch": 0.19143631436314362,
      "grad_norm": 21.516965866088867,
      "learning_rate": 1e-05,
      "loss": 7.0393,
      "step": 3532
    },
    {
      "epoch": 0.19143631436314362,
      "step": 3532,
      "training_loss": 7.718166828155518
    },
    {
      "epoch": 0.19149051490514904,
      "step": 3533,
      "training_loss": 7.17830228805542
    },
    {
      "epoch": 0.19154471544715448,
      "step": 3534,
      "training_loss": 9.594426155090332
    },
    {
      "epoch": 0.1915989159891599,
      "step": 3535,
      "training_loss": 6.626454830169678
    },
    {
      "epoch": 0.19165311653116532,
      "grad_norm": 21.104188919067383,
      "learning_rate": 1e-05,
      "loss": 7.7793,
      "step": 3536
    },
    {
      "epoch": 0.19165311653116532,
      "step": 3536,
      "training_loss": 4.722585678100586
    },
    {
      "epoch": 0.19170731707317074,
      "step": 3537,
      "training_loss": 6.965334415435791
    },
    {
      "epoch": 0.19176151761517615,
      "step": 3538,
      "training_loss": 7.1152753829956055
    },
    {
      "epoch": 0.19181571815718157,
      "step": 3539,
      "training_loss": 6.453128814697266
    },
    {
      "epoch": 0.191869918699187,
      "grad_norm": 29.41900634765625,
      "learning_rate": 1e-05,
      "loss": 6.3141,
      "step": 3540
    },
    {
      "epoch": 0.191869918699187,
      "step": 3540,
      "training_loss": 7.118203639984131
    },
    {
      "epoch": 0.1919241192411924,
      "step": 3541,
      "training_loss": 5.378966808319092
    },
    {
      "epoch": 0.19197831978319782,
      "step": 3542,
      "training_loss": 7.774205207824707
    },
    {
      "epoch": 0.19203252032520327,
      "step": 3543,
      "training_loss": 7.703166961669922
    },
    {
      "epoch": 0.19208672086720868,
      "grad_norm": 16.592958450317383,
      "learning_rate": 1e-05,
      "loss": 6.9936,
      "step": 3544
    },
    {
      "epoch": 0.19208672086720868,
      "step": 3544,
      "training_loss": 6.540310382843018
    },
    {
      "epoch": 0.1921409214092141,
      "step": 3545,
      "training_loss": 5.651665210723877
    },
    {
      "epoch": 0.19219512195121952,
      "step": 3546,
      "training_loss": 5.747505187988281
    },
    {
      "epoch": 0.19224932249322493,
      "step": 3547,
      "training_loss": 7.715489387512207
    },
    {
      "epoch": 0.19230352303523035,
      "grad_norm": 23.626712799072266,
      "learning_rate": 1e-05,
      "loss": 6.4137,
      "step": 3548
    },
    {
      "epoch": 0.19230352303523035,
      "step": 3548,
      "training_loss": 7.46294641494751
    },
    {
      "epoch": 0.19235772357723577,
      "step": 3549,
      "training_loss": 6.186453342437744
    },
    {
      "epoch": 0.19241192411924118,
      "step": 3550,
      "training_loss": 8.385303497314453
    },
    {
      "epoch": 0.1924661246612466,
      "step": 3551,
      "training_loss": 7.523306369781494
    },
    {
      "epoch": 0.19252032520325205,
      "grad_norm": 42.851768493652344,
      "learning_rate": 1e-05,
      "loss": 7.3895,
      "step": 3552
    },
    {
      "epoch": 0.19252032520325205,
      "step": 3552,
      "training_loss": 6.682075500488281
    },
    {
      "epoch": 0.19257452574525746,
      "step": 3553,
      "training_loss": 7.262420654296875
    },
    {
      "epoch": 0.19262872628726288,
      "step": 3554,
      "training_loss": 7.950928211212158
    },
    {
      "epoch": 0.1926829268292683,
      "step": 3555,
      "training_loss": 6.828555583953857
    },
    {
      "epoch": 0.19273712737127371,
      "grad_norm": 18.830814361572266,
      "learning_rate": 1e-05,
      "loss": 7.181,
      "step": 3556
    },
    {
      "epoch": 0.19273712737127371,
      "step": 3556,
      "training_loss": 6.7854838371276855
    },
    {
      "epoch": 0.19279132791327913,
      "step": 3557,
      "training_loss": 6.101210594177246
    },
    {
      "epoch": 0.19284552845528455,
      "step": 3558,
      "training_loss": 5.956278324127197
    },
    {
      "epoch": 0.19289972899728997,
      "step": 3559,
      "training_loss": 7.574970245361328
    },
    {
      "epoch": 0.19295392953929538,
      "grad_norm": 33.00794982910156,
      "learning_rate": 1e-05,
      "loss": 6.6045,
      "step": 3560
    },
    {
      "epoch": 0.19295392953929538,
      "step": 3560,
      "training_loss": 6.590302467346191
    },
    {
      "epoch": 0.1930081300813008,
      "step": 3561,
      "training_loss": 4.756896495819092
    },
    {
      "epoch": 0.19306233062330624,
      "step": 3562,
      "training_loss": 7.241307258605957
    },
    {
      "epoch": 0.19311653116531166,
      "step": 3563,
      "training_loss": 7.090335369110107
    },
    {
      "epoch": 0.19317073170731708,
      "grad_norm": 19.086322784423828,
      "learning_rate": 1e-05,
      "loss": 6.4197,
      "step": 3564
    },
    {
      "epoch": 0.19317073170731708,
      "step": 3564,
      "training_loss": 7.436647891998291
    },
    {
      "epoch": 0.1932249322493225,
      "step": 3565,
      "training_loss": 8.379436492919922
    },
    {
      "epoch": 0.1932791327913279,
      "step": 3566,
      "training_loss": 6.523434638977051
    },
    {
      "epoch": 0.19333333333333333,
      "step": 3567,
      "training_loss": 6.130964279174805
    },
    {
      "epoch": 0.19338753387533875,
      "grad_norm": 38.70289611816406,
      "learning_rate": 1e-05,
      "loss": 7.1176,
      "step": 3568
    },
    {
      "epoch": 0.19338753387533875,
      "step": 3568,
      "training_loss": 7.204666614532471
    },
    {
      "epoch": 0.19344173441734416,
      "step": 3569,
      "training_loss": 7.106040000915527
    },
    {
      "epoch": 0.19349593495934958,
      "step": 3570,
      "training_loss": 6.819925308227539
    },
    {
      "epoch": 0.19355013550135503,
      "step": 3571,
      "training_loss": 6.470937728881836
    },
    {
      "epoch": 0.19360433604336044,
      "grad_norm": 19.954730987548828,
      "learning_rate": 1e-05,
      "loss": 6.9004,
      "step": 3572
    },
    {
      "epoch": 0.19360433604336044,
      "step": 3572,
      "training_loss": 6.625735759735107
    },
    {
      "epoch": 0.19365853658536586,
      "step": 3573,
      "training_loss": 7.299422740936279
    },
    {
      "epoch": 0.19371273712737128,
      "step": 3574,
      "training_loss": 7.502279758453369
    },
    {
      "epoch": 0.1937669376693767,
      "step": 3575,
      "training_loss": 7.361159324645996
    },
    {
      "epoch": 0.1938211382113821,
      "grad_norm": 27.219772338867188,
      "learning_rate": 1e-05,
      "loss": 7.1971,
      "step": 3576
    },
    {
      "epoch": 0.1938211382113821,
      "step": 3576,
      "training_loss": 6.324658393859863
    },
    {
      "epoch": 0.19387533875338753,
      "step": 3577,
      "training_loss": 6.833270072937012
    },
    {
      "epoch": 0.19392953929539294,
      "step": 3578,
      "training_loss": 7.275321006774902
    },
    {
      "epoch": 0.19398373983739836,
      "step": 3579,
      "training_loss": 7.868011951446533
    },
    {
      "epoch": 0.1940379403794038,
      "grad_norm": 59.7314453125,
      "learning_rate": 1e-05,
      "loss": 7.0753,
      "step": 3580
    },
    {
      "epoch": 0.1940379403794038,
      "step": 3580,
      "training_loss": 5.897635459899902
    },
    {
      "epoch": 0.19409214092140922,
      "step": 3581,
      "training_loss": 6.910755634307861
    },
    {
      "epoch": 0.19414634146341464,
      "step": 3582,
      "training_loss": 8.202655792236328
    },
    {
      "epoch": 0.19420054200542006,
      "step": 3583,
      "training_loss": 6.196660041809082
    },
    {
      "epoch": 0.19425474254742547,
      "grad_norm": 21.58075714111328,
      "learning_rate": 1e-05,
      "loss": 6.8019,
      "step": 3584
    },
    {
      "epoch": 0.19425474254742547,
      "step": 3584,
      "training_loss": 6.014052867889404
    },
    {
      "epoch": 0.1943089430894309,
      "step": 3585,
      "training_loss": 7.2250823974609375
    },
    {
      "epoch": 0.1943631436314363,
      "step": 3586,
      "training_loss": 5.885572910308838
    },
    {
      "epoch": 0.19441734417344173,
      "step": 3587,
      "training_loss": 7.1241350173950195
    },
    {
      "epoch": 0.19447154471544714,
      "grad_norm": 15.550854682922363,
      "learning_rate": 1e-05,
      "loss": 6.5622,
      "step": 3588
    },
    {
      "epoch": 0.19447154471544714,
      "step": 3588,
      "training_loss": 5.759889125823975
    },
    {
      "epoch": 0.1945257452574526,
      "step": 3589,
      "training_loss": 5.750014305114746
    },
    {
      "epoch": 0.194579945799458,
      "step": 3590,
      "training_loss": 6.830233573913574
    },
    {
      "epoch": 0.19463414634146342,
      "step": 3591,
      "training_loss": 7.1210479736328125
    },
    {
      "epoch": 0.19468834688346884,
      "grad_norm": 41.917236328125,
      "learning_rate": 1e-05,
      "loss": 6.3653,
      "step": 3592
    },
    {
      "epoch": 0.19468834688346884,
      "step": 3592,
      "training_loss": 6.646726608276367
    },
    {
      "epoch": 0.19474254742547426,
      "step": 3593,
      "training_loss": 6.808524131774902
    },
    {
      "epoch": 0.19479674796747967,
      "step": 3594,
      "training_loss": 7.074746131896973
    },
    {
      "epoch": 0.1948509485094851,
      "step": 3595,
      "training_loss": 7.535642623901367
    },
    {
      "epoch": 0.1949051490514905,
      "grad_norm": 15.21619987487793,
      "learning_rate": 1e-05,
      "loss": 7.0164,
      "step": 3596
    },
    {
      "epoch": 0.1949051490514905,
      "step": 3596,
      "training_loss": 7.188093662261963
    },
    {
      "epoch": 0.19495934959349592,
      "step": 3597,
      "training_loss": 6.375606536865234
    },
    {
      "epoch": 0.19501355013550137,
      "step": 3598,
      "training_loss": 6.8431077003479
    },
    {
      "epoch": 0.19506775067750678,
      "step": 3599,
      "training_loss": 6.619203090667725
    },
    {
      "epoch": 0.1951219512195122,
      "grad_norm": 17.445575714111328,
      "learning_rate": 1e-05,
      "loss": 6.7565,
      "step": 3600
    },
    {
      "epoch": 0.1951219512195122,
      "step": 3600,
      "training_loss": 6.647426605224609
    },
    {
      "epoch": 0.19517615176151762,
      "step": 3601,
      "training_loss": 7.542350769042969
    },
    {
      "epoch": 0.19523035230352304,
      "step": 3602,
      "training_loss": 5.918890476226807
    },
    {
      "epoch": 0.19528455284552845,
      "step": 3603,
      "training_loss": 7.953139781951904
    },
    {
      "epoch": 0.19533875338753387,
      "grad_norm": 26.28270149230957,
      "learning_rate": 1e-05,
      "loss": 7.0155,
      "step": 3604
    },
    {
      "epoch": 0.19533875338753387,
      "step": 3604,
      "training_loss": 5.173587799072266
    },
    {
      "epoch": 0.1953929539295393,
      "step": 3605,
      "training_loss": 6.5647196769714355
    },
    {
      "epoch": 0.1954471544715447,
      "step": 3606,
      "training_loss": 5.8073225021362305
    },
    {
      "epoch": 0.19550135501355015,
      "step": 3607,
      "training_loss": 7.742386817932129
    },
    {
      "epoch": 0.19555555555555557,
      "grad_norm": 30.476964950561523,
      "learning_rate": 1e-05,
      "loss": 6.322,
      "step": 3608
    },
    {
      "epoch": 0.19555555555555557,
      "step": 3608,
      "training_loss": 7.219214916229248
    },
    {
      "epoch": 0.19560975609756098,
      "step": 3609,
      "training_loss": 5.867938041687012
    },
    {
      "epoch": 0.1956639566395664,
      "step": 3610,
      "training_loss": 7.985525608062744
    },
    {
      "epoch": 0.19571815718157182,
      "step": 3611,
      "training_loss": 7.248887538909912
    },
    {
      "epoch": 0.19577235772357723,
      "grad_norm": 23.288326263427734,
      "learning_rate": 1e-05,
      "loss": 7.0804,
      "step": 3612
    },
    {
      "epoch": 0.19577235772357723,
      "step": 3612,
      "training_loss": 6.982251167297363
    },
    {
      "epoch": 0.19582655826558265,
      "step": 3613,
      "training_loss": 8.432421684265137
    },
    {
      "epoch": 0.19588075880758807,
      "step": 3614,
      "training_loss": 7.005619525909424
    },
    {
      "epoch": 0.19593495934959348,
      "step": 3615,
      "training_loss": 7.306734561920166
    },
    {
      "epoch": 0.19598915989159893,
      "grad_norm": 17.739580154418945,
      "learning_rate": 1e-05,
      "loss": 7.4318,
      "step": 3616
    },
    {
      "epoch": 0.19598915989159893,
      "step": 3616,
      "training_loss": 7.70357084274292
    },
    {
      "epoch": 0.19604336043360435,
      "step": 3617,
      "training_loss": 7.45521354675293
    },
    {
      "epoch": 0.19609756097560976,
      "step": 3618,
      "training_loss": 8.275249481201172
    },
    {
      "epoch": 0.19615176151761518,
      "step": 3619,
      "training_loss": 6.4157891273498535
    },
    {
      "epoch": 0.1962059620596206,
      "grad_norm": 25.05075454711914,
      "learning_rate": 1e-05,
      "loss": 7.4625,
      "step": 3620
    },
    {
      "epoch": 0.1962059620596206,
      "step": 3620,
      "training_loss": 8.002338409423828
    },
    {
      "epoch": 0.19626016260162601,
      "step": 3621,
      "training_loss": 7.151495456695557
    },
    {
      "epoch": 0.19631436314363143,
      "step": 3622,
      "training_loss": 7.340665340423584
    },
    {
      "epoch": 0.19636856368563685,
      "step": 3623,
      "training_loss": 5.787600994110107
    },
    {
      "epoch": 0.19642276422764227,
      "grad_norm": 21.339664459228516,
      "learning_rate": 1e-05,
      "loss": 7.0705,
      "step": 3624
    },
    {
      "epoch": 0.19642276422764227,
      "step": 3624,
      "training_loss": 5.194981098175049
    },
    {
      "epoch": 0.19647696476964768,
      "step": 3625,
      "training_loss": 7.2146382331848145
    },
    {
      "epoch": 0.19653116531165313,
      "step": 3626,
      "training_loss": 6.672301769256592
    },
    {
      "epoch": 0.19658536585365854,
      "step": 3627,
      "training_loss": 6.368736743927002
    },
    {
      "epoch": 0.19663956639566396,
      "grad_norm": 21.93890380859375,
      "learning_rate": 1e-05,
      "loss": 6.3627,
      "step": 3628
    },
    {
      "epoch": 0.19663956639566396,
      "step": 3628,
      "training_loss": 5.3946757316589355
    },
    {
      "epoch": 0.19669376693766938,
      "step": 3629,
      "training_loss": 6.938905715942383
    },
    {
      "epoch": 0.1967479674796748,
      "step": 3630,
      "training_loss": 6.957594871520996
    },
    {
      "epoch": 0.1968021680216802,
      "step": 3631,
      "training_loss": 4.491446018218994
    },
    {
      "epoch": 0.19685636856368563,
      "grad_norm": 21.67856788635254,
      "learning_rate": 1e-05,
      "loss": 5.9457,
      "step": 3632
    },
    {
      "epoch": 0.19685636856368563,
      "step": 3632,
      "training_loss": 10.10795783996582
    },
    {
      "epoch": 0.19691056910569105,
      "step": 3633,
      "training_loss": 7.300577640533447
    },
    {
      "epoch": 0.19696476964769646,
      "step": 3634,
      "training_loss": 9.514747619628906
    },
    {
      "epoch": 0.1970189701897019,
      "step": 3635,
      "training_loss": 5.756318092346191
    },
    {
      "epoch": 0.19707317073170733,
      "grad_norm": 24.290868759155273,
      "learning_rate": 1e-05,
      "loss": 8.1699,
      "step": 3636
    },
    {
      "epoch": 0.19707317073170733,
      "step": 3636,
      "training_loss": 5.519053936004639
    },
    {
      "epoch": 0.19712737127371274,
      "step": 3637,
      "training_loss": 6.770873546600342
    },
    {
      "epoch": 0.19718157181571816,
      "step": 3638,
      "training_loss": 7.256626605987549
    },
    {
      "epoch": 0.19723577235772358,
      "step": 3639,
      "training_loss": 8.113604545593262
    },
    {
      "epoch": 0.197289972899729,
      "grad_norm": 37.62157440185547,
      "learning_rate": 1e-05,
      "loss": 6.915,
      "step": 3640
    },
    {
      "epoch": 0.197289972899729,
      "step": 3640,
      "training_loss": 4.712960720062256
    },
    {
      "epoch": 0.1973441734417344,
      "step": 3641,
      "training_loss": 7.667952537536621
    },
    {
      "epoch": 0.19739837398373983,
      "step": 3642,
      "training_loss": 7.2596564292907715
    },
    {
      "epoch": 0.19745257452574524,
      "step": 3643,
      "training_loss": 6.276018142700195
    },
    {
      "epoch": 0.1975067750677507,
      "grad_norm": 16.380876541137695,
      "learning_rate": 1e-05,
      "loss": 6.4791,
      "step": 3644
    },
    {
      "epoch": 0.1975067750677507,
      "step": 3644,
      "training_loss": 6.901395320892334
    },
    {
      "epoch": 0.1975609756097561,
      "step": 3645,
      "training_loss": 7.820331573486328
    },
    {
      "epoch": 0.19761517615176152,
      "step": 3646,
      "training_loss": 6.393054008483887
    },
    {
      "epoch": 0.19766937669376694,
      "step": 3647,
      "training_loss": 6.307424068450928
    },
    {
      "epoch": 0.19772357723577236,
      "grad_norm": 28.34696388244629,
      "learning_rate": 1e-05,
      "loss": 6.8556,
      "step": 3648
    },
    {
      "epoch": 0.19772357723577236,
      "step": 3648,
      "training_loss": 7.298562526702881
    },
    {
      "epoch": 0.19777777777777777,
      "step": 3649,
      "training_loss": 7.6671342849731445
    },
    {
      "epoch": 0.1978319783197832,
      "step": 3650,
      "training_loss": 8.117452621459961
    },
    {
      "epoch": 0.1978861788617886,
      "step": 3651,
      "training_loss": 6.682476997375488
    },
    {
      "epoch": 0.19794037940379403,
      "grad_norm": 19.50032615661621,
      "learning_rate": 1e-05,
      "loss": 7.4414,
      "step": 3652
    },
    {
      "epoch": 0.19794037940379403,
      "step": 3652,
      "training_loss": 7.641303062438965
    },
    {
      "epoch": 0.19799457994579947,
      "step": 3653,
      "training_loss": 7.8183393478393555
    },
    {
      "epoch": 0.1980487804878049,
      "step": 3654,
      "training_loss": 7.477652072906494
    },
    {
      "epoch": 0.1981029810298103,
      "step": 3655,
      "training_loss": 5.818542003631592
    },
    {
      "epoch": 0.19815718157181572,
      "grad_norm": 27.886316299438477,
      "learning_rate": 1e-05,
      "loss": 7.189,
      "step": 3656
    },
    {
      "epoch": 0.19815718157181572,
      "step": 3656,
      "training_loss": 7.092739582061768
    },
    {
      "epoch": 0.19821138211382114,
      "step": 3657,
      "training_loss": 8.065757751464844
    },
    {
      "epoch": 0.19826558265582656,
      "step": 3658,
      "training_loss": 8.45667552947998
    },
    {
      "epoch": 0.19831978319783197,
      "step": 3659,
      "training_loss": 6.902286529541016
    },
    {
      "epoch": 0.1983739837398374,
      "grad_norm": 29.202165603637695,
      "learning_rate": 1e-05,
      "loss": 7.6294,
      "step": 3660
    },
    {
      "epoch": 0.1983739837398374,
      "step": 3660,
      "training_loss": 6.8144612312316895
    },
    {
      "epoch": 0.1984281842818428,
      "step": 3661,
      "training_loss": 7.959171772003174
    },
    {
      "epoch": 0.19848238482384825,
      "step": 3662,
      "training_loss": 7.68939208984375
    },
    {
      "epoch": 0.19853658536585367,
      "step": 3663,
      "training_loss": 7.173436641693115
    },
    {
      "epoch": 0.19859078590785909,
      "grad_norm": 19.999155044555664,
      "learning_rate": 1e-05,
      "loss": 7.4091,
      "step": 3664
    },
    {
      "epoch": 0.19859078590785909,
      "step": 3664,
      "training_loss": 7.908653736114502
    },
    {
      "epoch": 0.1986449864498645,
      "step": 3665,
      "training_loss": 6.844855785369873
    },
    {
      "epoch": 0.19869918699186992,
      "step": 3666,
      "training_loss": 7.200395584106445
    },
    {
      "epoch": 0.19875338753387534,
      "step": 3667,
      "training_loss": 7.60434627532959
    },
    {
      "epoch": 0.19880758807588075,
      "grad_norm": 23.82035255432129,
      "learning_rate": 1e-05,
      "loss": 7.3896,
      "step": 3668
    },
    {
      "epoch": 0.19880758807588075,
      "step": 3668,
      "training_loss": 6.539467811584473
    },
    {
      "epoch": 0.19886178861788617,
      "step": 3669,
      "training_loss": 7.424093723297119
    },
    {
      "epoch": 0.1989159891598916,
      "step": 3670,
      "training_loss": 6.190935134887695
    },
    {
      "epoch": 0.19897018970189703,
      "step": 3671,
      "training_loss": 6.709878444671631
    },
    {
      "epoch": 0.19902439024390245,
      "grad_norm": 17.562734603881836,
      "learning_rate": 1e-05,
      "loss": 6.7161,
      "step": 3672
    },
    {
      "epoch": 0.19902439024390245,
      "step": 3672,
      "training_loss": 6.6650800704956055
    },
    {
      "epoch": 0.19907859078590787,
      "step": 3673,
      "training_loss": 7.3142218589782715
    },
    {
      "epoch": 0.19913279132791328,
      "step": 3674,
      "training_loss": 7.112971782684326
    },
    {
      "epoch": 0.1991869918699187,
      "step": 3675,
      "training_loss": 7.9690775871276855
    },
    {
      "epoch": 0.19924119241192412,
      "grad_norm": 20.131502151489258,
      "learning_rate": 1e-05,
      "loss": 7.2653,
      "step": 3676
    },
    {
      "epoch": 0.19924119241192412,
      "step": 3676,
      "training_loss": 7.467028617858887
    },
    {
      "epoch": 0.19929539295392953,
      "step": 3677,
      "training_loss": 7.4162116050720215
    },
    {
      "epoch": 0.19934959349593495,
      "step": 3678,
      "training_loss": 6.486652851104736
    },
    {
      "epoch": 0.19940379403794037,
      "step": 3679,
      "training_loss": 7.118208408355713
    },
    {
      "epoch": 0.1994579945799458,
      "grad_norm": 22.733455657958984,
      "learning_rate": 1e-05,
      "loss": 7.122,
      "step": 3680
    },
    {
      "epoch": 0.1994579945799458,
      "step": 3680,
      "training_loss": 6.63466739654541
    },
    {
      "epoch": 0.19951219512195123,
      "step": 3681,
      "training_loss": 7.488204002380371
    },
    {
      "epoch": 0.19956639566395665,
      "step": 3682,
      "training_loss": 6.862729549407959
    },
    {
      "epoch": 0.19962059620596206,
      "step": 3683,
      "training_loss": 6.670866966247559
    },
    {
      "epoch": 0.19967479674796748,
      "grad_norm": 32.30569076538086,
      "learning_rate": 1e-05,
      "loss": 6.9141,
      "step": 3684
    },
    {
      "epoch": 0.19967479674796748,
      "step": 3684,
      "training_loss": 7.37429141998291
    },
    {
      "epoch": 0.1997289972899729,
      "step": 3685,
      "training_loss": 7.108931541442871
    },
    {
      "epoch": 0.19978319783197832,
      "step": 3686,
      "training_loss": 5.195807456970215
    },
    {
      "epoch": 0.19983739837398373,
      "step": 3687,
      "training_loss": 7.150634288787842
    },
    {
      "epoch": 0.19989159891598915,
      "grad_norm": 17.38572883605957,
      "learning_rate": 1e-05,
      "loss": 6.7074,
      "step": 3688
    },
    {
      "epoch": 0.19989159891598915,
      "step": 3688,
      "training_loss": 6.8793511390686035
    },
    {
      "epoch": 0.19994579945799457,
      "step": 3689,
      "training_loss": 6.780917167663574
    },
    {
      "epoch": 0.2,
      "step": 3690,
      "training_loss": 6.2888054847717285
    },
    {
      "epoch": 0.20005420054200543,
      "step": 3691,
      "training_loss": 5.793900966644287
    },
    {
      "epoch": 0.20010840108401085,
      "grad_norm": 27.417531967163086,
      "learning_rate": 1e-05,
      "loss": 6.4357,
      "step": 3692
    },
    {
      "epoch": 0.20010840108401085,
      "step": 3692,
      "training_loss": 7.059779167175293
    },
    {
      "epoch": 0.20016260162601626,
      "step": 3693,
      "training_loss": 7.7476959228515625
    },
    {
      "epoch": 0.20021680216802168,
      "step": 3694,
      "training_loss": 8.078887939453125
    },
    {
      "epoch": 0.2002710027100271,
      "step": 3695,
      "training_loss": 7.748600006103516
    },
    {
      "epoch": 0.2003252032520325,
      "grad_norm": 24.214841842651367,
      "learning_rate": 1e-05,
      "loss": 7.6587,
      "step": 3696
    },
    {
      "epoch": 0.2003252032520325,
      "step": 3696,
      "training_loss": 5.402561187744141
    },
    {
      "epoch": 0.20037940379403793,
      "step": 3697,
      "training_loss": 7.602196216583252
    },
    {
      "epoch": 0.20043360433604335,
      "step": 3698,
      "training_loss": 6.460982322692871
    },
    {
      "epoch": 0.2004878048780488,
      "step": 3699,
      "training_loss": 6.8257246017456055
    },
    {
      "epoch": 0.2005420054200542,
      "grad_norm": 35.73275375366211,
      "learning_rate": 1e-05,
      "loss": 6.5729,
      "step": 3700
    },
    {
      "epoch": 0.2005420054200542,
      "step": 3700,
      "training_loss": 7.636689186096191
    },
    {
      "epoch": 0.20059620596205963,
      "step": 3701,
      "training_loss": 5.736073970794678
    },
    {
      "epoch": 0.20065040650406504,
      "step": 3702,
      "training_loss": 6.629544258117676
    },
    {
      "epoch": 0.20070460704607046,
      "step": 3703,
      "training_loss": 6.984662055969238
    },
    {
      "epoch": 0.20075880758807588,
      "grad_norm": 26.969566345214844,
      "learning_rate": 1e-05,
      "loss": 6.7467,
      "step": 3704
    },
    {
      "epoch": 0.20075880758807588,
      "step": 3704,
      "training_loss": 7.584930419921875
    },
    {
      "epoch": 0.2008130081300813,
      "step": 3705,
      "training_loss": 6.989312171936035
    },
    {
      "epoch": 0.2008672086720867,
      "step": 3706,
      "training_loss": 8.103487968444824
    },
    {
      "epoch": 0.20092140921409213,
      "step": 3707,
      "training_loss": 5.944775104522705
    },
    {
      "epoch": 0.20097560975609757,
      "grad_norm": 20.99077606201172,
      "learning_rate": 1e-05,
      "loss": 7.1556,
      "step": 3708
    },
    {
      "epoch": 0.20097560975609757,
      "step": 3708,
      "training_loss": 7.127499103546143
    },
    {
      "epoch": 0.201029810298103,
      "step": 3709,
      "training_loss": 7.067489147186279
    },
    {
      "epoch": 0.2010840108401084,
      "step": 3710,
      "training_loss": 8.529799461364746
    },
    {
      "epoch": 0.20113821138211382,
      "step": 3711,
      "training_loss": 5.936741352081299
    },
    {
      "epoch": 0.20119241192411924,
      "grad_norm": 14.440022468566895,
      "learning_rate": 1e-05,
      "loss": 7.1654,
      "step": 3712
    },
    {
      "epoch": 0.20119241192411924,
      "step": 3712,
      "training_loss": 5.79723596572876
    },
    {
      "epoch": 0.20124661246612466,
      "step": 3713,
      "training_loss": 6.195626258850098
    },
    {
      "epoch": 0.20130081300813008,
      "step": 3714,
      "training_loss": 7.336697101593018
    },
    {
      "epoch": 0.2013550135501355,
      "step": 3715,
      "training_loss": 6.103996276855469
    },
    {
      "epoch": 0.2014092140921409,
      "grad_norm": 27.086442947387695,
      "learning_rate": 1e-05,
      "loss": 6.3584,
      "step": 3716
    },
    {
      "epoch": 0.2014092140921409,
      "step": 3716,
      "training_loss": 9.423589706420898
    },
    {
      "epoch": 0.20146341463414635,
      "step": 3717,
      "training_loss": 7.2984466552734375
    },
    {
      "epoch": 0.20151761517615177,
      "step": 3718,
      "training_loss": 7.39729642868042
    },
    {
      "epoch": 0.2015718157181572,
      "step": 3719,
      "training_loss": 6.507932186126709
    },
    {
      "epoch": 0.2016260162601626,
      "grad_norm": 21.38241195678711,
      "learning_rate": 1e-05,
      "loss": 7.6568,
      "step": 3720
    },
    {
      "epoch": 0.2016260162601626,
      "step": 3720,
      "training_loss": 6.14583683013916
    },
    {
      "epoch": 0.20168021680216802,
      "step": 3721,
      "training_loss": 7.773763656616211
    },
    {
      "epoch": 0.20173441734417344,
      "step": 3722,
      "training_loss": 6.346989154815674
    },
    {
      "epoch": 0.20178861788617886,
      "step": 3723,
      "training_loss": 7.759746074676514
    },
    {
      "epoch": 0.20184281842818427,
      "grad_norm": 28.2387638092041,
      "learning_rate": 1e-05,
      "loss": 7.0066,
      "step": 3724
    },
    {
      "epoch": 0.20184281842818427,
      "step": 3724,
      "training_loss": 6.691409111022949
    },
    {
      "epoch": 0.2018970189701897,
      "step": 3725,
      "training_loss": 7.230884552001953
    },
    {
      "epoch": 0.20195121951219513,
      "step": 3726,
      "training_loss": 7.2621283531188965
    },
    {
      "epoch": 0.20200542005420055,
      "step": 3727,
      "training_loss": 6.4293975830078125
    },
    {
      "epoch": 0.20205962059620597,
      "grad_norm": 18.04242706298828,
      "learning_rate": 1e-05,
      "loss": 6.9035,
      "step": 3728
    },
    {
      "epoch": 0.20205962059620597,
      "step": 3728,
      "training_loss": 6.093830585479736
    },
    {
      "epoch": 0.20211382113821139,
      "step": 3729,
      "training_loss": 7.133187294006348
    },
    {
      "epoch": 0.2021680216802168,
      "step": 3730,
      "training_loss": 7.707437515258789
    },
    {
      "epoch": 0.20222222222222222,
      "step": 3731,
      "training_loss": 6.288330078125
    },
    {
      "epoch": 0.20227642276422764,
      "grad_norm": 25.852039337158203,
      "learning_rate": 1e-05,
      "loss": 6.8057,
      "step": 3732
    },
    {
      "epoch": 0.20227642276422764,
      "step": 3732,
      "training_loss": 8.462844848632812
    },
    {
      "epoch": 0.20233062330623305,
      "step": 3733,
      "training_loss": 7.386443138122559
    },
    {
      "epoch": 0.20238482384823847,
      "step": 3734,
      "training_loss": 5.888491630554199
    },
    {
      "epoch": 0.20243902439024392,
      "step": 3735,
      "training_loss": 10.228243827819824
    },
    {
      "epoch": 0.20249322493224933,
      "grad_norm": 62.90739059448242,
      "learning_rate": 1e-05,
      "loss": 7.9915,
      "step": 3736
    },
    {
      "epoch": 0.20249322493224933,
      "step": 3736,
      "training_loss": 6.906584739685059
    },
    {
      "epoch": 0.20254742547425475,
      "step": 3737,
      "training_loss": 7.593590259552002
    },
    {
      "epoch": 0.20260162601626017,
      "step": 3738,
      "training_loss": 6.996849536895752
    },
    {
      "epoch": 0.20265582655826558,
      "step": 3739,
      "training_loss": 7.42687463760376
    },
    {
      "epoch": 0.202710027100271,
      "grad_norm": 32.043479919433594,
      "learning_rate": 1e-05,
      "loss": 7.231,
      "step": 3740
    },
    {
      "epoch": 0.202710027100271,
      "step": 3740,
      "training_loss": 6.641480922698975
    },
    {
      "epoch": 0.20276422764227642,
      "step": 3741,
      "training_loss": 5.6986470222473145
    },
    {
      "epoch": 0.20281842818428183,
      "step": 3742,
      "training_loss": 7.252570629119873
    },
    {
      "epoch": 0.20287262872628725,
      "step": 3743,
      "training_loss": 7.341577053070068
    },
    {
      "epoch": 0.2029268292682927,
      "grad_norm": 18.72827911376953,
      "learning_rate": 1e-05,
      "loss": 6.7336,
      "step": 3744
    },
    {
      "epoch": 0.2029268292682927,
      "step": 3744,
      "training_loss": 7.952272891998291
    },
    {
      "epoch": 0.2029810298102981,
      "step": 3745,
      "training_loss": 6.965378284454346
    },
    {
      "epoch": 0.20303523035230353,
      "step": 3746,
      "training_loss": 4.7070088386535645
    },
    {
      "epoch": 0.20308943089430895,
      "step": 3747,
      "training_loss": 7.248885154724121
    },
    {
      "epoch": 0.20314363143631436,
      "grad_norm": 25.276844024658203,
      "learning_rate": 1e-05,
      "loss": 6.7184,
      "step": 3748
    },
    {
      "epoch": 0.20314363143631436,
      "step": 3748,
      "training_loss": 6.406479835510254
    },
    {
      "epoch": 0.20319783197831978,
      "step": 3749,
      "training_loss": 7.403736591339111
    },
    {
      "epoch": 0.2032520325203252,
      "step": 3750,
      "training_loss": 5.089934825897217
    },
    {
      "epoch": 0.20330623306233062,
      "step": 3751,
      "training_loss": 7.201196670532227
    },
    {
      "epoch": 0.20336043360433603,
      "grad_norm": 23.481536865234375,
      "learning_rate": 1e-05,
      "loss": 6.5253,
      "step": 3752
    },
    {
      "epoch": 0.20336043360433603,
      "step": 3752,
      "training_loss": 4.735683917999268
    },
    {
      "epoch": 0.20341463414634145,
      "step": 3753,
      "training_loss": 7.275781154632568
    },
    {
      "epoch": 0.2034688346883469,
      "step": 3754,
      "training_loss": 6.5600810050964355
    },
    {
      "epoch": 0.2035230352303523,
      "step": 3755,
      "training_loss": 7.231378078460693
    },
    {
      "epoch": 0.20357723577235773,
      "grad_norm": 16.868249893188477,
      "learning_rate": 1e-05,
      "loss": 6.4507,
      "step": 3756
    },
    {
      "epoch": 0.20357723577235773,
      "step": 3756,
      "training_loss": 6.266883373260498
    },
    {
      "epoch": 0.20363143631436315,
      "step": 3757,
      "training_loss": 7.108072280883789
    },
    {
      "epoch": 0.20368563685636856,
      "step": 3758,
      "training_loss": 7.5145463943481445
    },
    {
      "epoch": 0.20373983739837398,
      "step": 3759,
      "training_loss": 6.8705573081970215
    },
    {
      "epoch": 0.2037940379403794,
      "grad_norm": 19.499956130981445,
      "learning_rate": 1e-05,
      "loss": 6.94,
      "step": 3760
    },
    {
      "epoch": 0.2037940379403794,
      "step": 3760,
      "training_loss": 7.254319190979004
    },
    {
      "epoch": 0.2038482384823848,
      "step": 3761,
      "training_loss": 7.747385501861572
    },
    {
      "epoch": 0.20390243902439023,
      "step": 3762,
      "training_loss": 6.521635055541992
    },
    {
      "epoch": 0.20395663956639568,
      "step": 3763,
      "training_loss": 7.14340353012085
    },
    {
      "epoch": 0.2040108401084011,
      "grad_norm": 15.998696327209473,
      "learning_rate": 1e-05,
      "loss": 7.1667,
      "step": 3764
    },
    {
      "epoch": 0.2040108401084011,
      "step": 3764,
      "training_loss": 6.375705718994141
    },
    {
      "epoch": 0.2040650406504065,
      "step": 3765,
      "training_loss": 6.8692307472229
    },
    {
      "epoch": 0.20411924119241193,
      "step": 3766,
      "training_loss": 5.7958879470825195
    },
    {
      "epoch": 0.20417344173441734,
      "step": 3767,
      "training_loss": 5.278838157653809
    },
    {
      "epoch": 0.20422764227642276,
      "grad_norm": 23.002012252807617,
      "learning_rate": 1e-05,
      "loss": 6.0799,
      "step": 3768
    },
    {
      "epoch": 0.20422764227642276,
      "step": 3768,
      "training_loss": 6.560371398925781
    },
    {
      "epoch": 0.20428184281842818,
      "step": 3769,
      "training_loss": 7.05556058883667
    },
    {
      "epoch": 0.2043360433604336,
      "step": 3770,
      "training_loss": 5.911425590515137
    },
    {
      "epoch": 0.204390243902439,
      "step": 3771,
      "training_loss": 6.529625415802002
    },
    {
      "epoch": 0.20444444444444446,
      "grad_norm": 32.05925750732422,
      "learning_rate": 1e-05,
      "loss": 6.5142,
      "step": 3772
    },
    {
      "epoch": 0.20444444444444446,
      "step": 3772,
      "training_loss": 7.186750888824463
    },
    {
      "epoch": 0.20449864498644987,
      "step": 3773,
      "training_loss": 7.254762172698975
    },
    {
      "epoch": 0.2045528455284553,
      "step": 3774,
      "training_loss": 7.112131595611572
    },
    {
      "epoch": 0.2046070460704607,
      "step": 3775,
      "training_loss": 4.544216632843018
    },
    {
      "epoch": 0.20466124661246612,
      "grad_norm": 30.24347686767578,
      "learning_rate": 1e-05,
      "loss": 6.5245,
      "step": 3776
    },
    {
      "epoch": 0.20466124661246612,
      "step": 3776,
      "training_loss": 5.960275650024414
    },
    {
      "epoch": 0.20471544715447154,
      "step": 3777,
      "training_loss": 5.438753604888916
    },
    {
      "epoch": 0.20476964769647696,
      "step": 3778,
      "training_loss": 6.213459491729736
    },
    {
      "epoch": 0.20482384823848238,
      "step": 3779,
      "training_loss": 7.096193313598633
    },
    {
      "epoch": 0.2048780487804878,
      "grad_norm": 26.780899047851562,
      "learning_rate": 1e-05,
      "loss": 6.1772,
      "step": 3780
    },
    {
      "epoch": 0.2048780487804878,
      "step": 3780,
      "training_loss": 6.912062168121338
    },
    {
      "epoch": 0.20493224932249324,
      "step": 3781,
      "training_loss": 6.910130500793457
    },
    {
      "epoch": 0.20498644986449865,
      "step": 3782,
      "training_loss": 5.577674388885498
    },
    {
      "epoch": 0.20504065040650407,
      "step": 3783,
      "training_loss": 6.965164661407471
    },
    {
      "epoch": 0.2050948509485095,
      "grad_norm": 27.676620483398438,
      "learning_rate": 1e-05,
      "loss": 6.5913,
      "step": 3784
    },
    {
      "epoch": 0.2050948509485095,
      "step": 3784,
      "training_loss": 6.469407081604004
    },
    {
      "epoch": 0.2051490514905149,
      "step": 3785,
      "training_loss": 4.501212120056152
    },
    {
      "epoch": 0.20520325203252032,
      "step": 3786,
      "training_loss": 7.512286186218262
    },
    {
      "epoch": 0.20525745257452574,
      "step": 3787,
      "training_loss": 6.7867560386657715
    },
    {
      "epoch": 0.20531165311653116,
      "grad_norm": 13.730051040649414,
      "learning_rate": 1e-05,
      "loss": 6.3174,
      "step": 3788
    },
    {
      "epoch": 0.20531165311653116,
      "step": 3788,
      "training_loss": 5.275069236755371
    },
    {
      "epoch": 0.20536585365853657,
      "step": 3789,
      "training_loss": 8.25909423828125
    },
    {
      "epoch": 0.20542005420054202,
      "step": 3790,
      "training_loss": 5.658388137817383
    },
    {
      "epoch": 0.20547425474254744,
      "step": 3791,
      "training_loss": 5.53835391998291
    },
    {
      "epoch": 0.20552845528455285,
      "grad_norm": 24.392990112304688,
      "learning_rate": 1e-05,
      "loss": 6.1827,
      "step": 3792
    },
    {
      "epoch": 0.20552845528455285,
      "step": 3792,
      "training_loss": 7.258354663848877
    },
    {
      "epoch": 0.20558265582655827,
      "step": 3793,
      "training_loss": 6.920764923095703
    },
    {
      "epoch": 0.2056368563685637,
      "step": 3794,
      "training_loss": 6.990993022918701
    },
    {
      "epoch": 0.2056910569105691,
      "step": 3795,
      "training_loss": 8.564592361450195
    },
    {
      "epoch": 0.20574525745257452,
      "grad_norm": 23.733346939086914,
      "learning_rate": 1e-05,
      "loss": 7.4337,
      "step": 3796
    },
    {
      "epoch": 0.20574525745257452,
      "step": 3796,
      "training_loss": 5.875835418701172
    },
    {
      "epoch": 0.20579945799457994,
      "step": 3797,
      "training_loss": 6.975363731384277
    },
    {
      "epoch": 0.20585365853658535,
      "step": 3798,
      "training_loss": 7.160928249359131
    },
    {
      "epoch": 0.2059078590785908,
      "step": 3799,
      "training_loss": 5.941561698913574
    },
    {
      "epoch": 0.20596205962059622,
      "grad_norm": 35.57673645019531,
      "learning_rate": 1e-05,
      "loss": 6.4884,
      "step": 3800
    },
    {
      "epoch": 0.20596205962059622,
      "step": 3800,
      "training_loss": 6.046288967132568
    },
    {
      "epoch": 0.20601626016260163,
      "step": 3801,
      "training_loss": 7.517364025115967
    },
    {
      "epoch": 0.20607046070460705,
      "step": 3802,
      "training_loss": 8.301523208618164
    },
    {
      "epoch": 0.20612466124661247,
      "step": 3803,
      "training_loss": 6.801531791687012
    },
    {
      "epoch": 0.20617886178861788,
      "grad_norm": 26.156278610229492,
      "learning_rate": 1e-05,
      "loss": 7.1667,
      "step": 3804
    },
    {
      "epoch": 0.20617886178861788,
      "step": 3804,
      "training_loss": 4.75673246383667
    },
    {
      "epoch": 0.2062330623306233,
      "step": 3805,
      "training_loss": 7.181939601898193
    },
    {
      "epoch": 0.20628726287262872,
      "step": 3806,
      "training_loss": 6.025169372558594
    },
    {
      "epoch": 0.20634146341463414,
      "step": 3807,
      "training_loss": 4.7658371925354
    },
    {
      "epoch": 0.20639566395663958,
      "grad_norm": 27.089412689208984,
      "learning_rate": 1e-05,
      "loss": 5.6824,
      "step": 3808
    },
    {
      "epoch": 0.20639566395663958,
      "step": 3808,
      "training_loss": 5.95260763168335
    },
    {
      "epoch": 0.206449864498645,
      "step": 3809,
      "training_loss": 7.122954845428467
    },
    {
      "epoch": 0.20650406504065041,
      "step": 3810,
      "training_loss": 7.892776012420654
    },
    {
      "epoch": 0.20655826558265583,
      "step": 3811,
      "training_loss": 6.233120918273926
    },
    {
      "epoch": 0.20661246612466125,
      "grad_norm": 22.90690803527832,
      "learning_rate": 1e-05,
      "loss": 6.8004,
      "step": 3812
    },
    {
      "epoch": 0.20661246612466125,
      "step": 3812,
      "training_loss": 8.103516578674316
    },
    {
      "epoch": 0.20666666666666667,
      "step": 3813,
      "training_loss": 6.692227363586426
    },
    {
      "epoch": 0.20672086720867208,
      "step": 3814,
      "training_loss": 7.558800220489502
    },
    {
      "epoch": 0.2067750677506775,
      "step": 3815,
      "training_loss": 5.164167881011963
    },
    {
      "epoch": 0.20682926829268292,
      "grad_norm": 19.71681022644043,
      "learning_rate": 1e-05,
      "loss": 6.8797,
      "step": 3816
    },
    {
      "epoch": 0.20682926829268292,
      "step": 3816,
      "training_loss": 7.053528785705566
    },
    {
      "epoch": 0.20688346883468833,
      "step": 3817,
      "training_loss": 7.225793361663818
    },
    {
      "epoch": 0.20693766937669378,
      "step": 3818,
      "training_loss": 6.050297260284424
    },
    {
      "epoch": 0.2069918699186992,
      "step": 3819,
      "training_loss": 5.7134222984313965
    },
    {
      "epoch": 0.2070460704607046,
      "grad_norm": 31.173913955688477,
      "learning_rate": 1e-05,
      "loss": 6.5108,
      "step": 3820
    },
    {
      "epoch": 0.2070460704607046,
      "step": 3820,
      "training_loss": 6.234885215759277
    },
    {
      "epoch": 0.20710027100271003,
      "step": 3821,
      "training_loss": 6.689486026763916
    },
    {
      "epoch": 0.20715447154471545,
      "step": 3822,
      "training_loss": 7.462898254394531
    },
    {
      "epoch": 0.20720867208672086,
      "step": 3823,
      "training_loss": 6.996607303619385
    },
    {
      "epoch": 0.20726287262872628,
      "grad_norm": 19.794387817382812,
      "learning_rate": 1e-05,
      "loss": 6.846,
      "step": 3824
    },
    {
      "epoch": 0.20726287262872628,
      "step": 3824,
      "training_loss": 5.971132755279541
    },
    {
      "epoch": 0.2073170731707317,
      "step": 3825,
      "training_loss": 6.921026706695557
    },
    {
      "epoch": 0.20737127371273711,
      "step": 3826,
      "training_loss": 8.004469871520996
    },
    {
      "epoch": 0.20742547425474256,
      "step": 3827,
      "training_loss": 7.335885047912598
    },
    {
      "epoch": 0.20747967479674798,
      "grad_norm": 15.814802169799805,
      "learning_rate": 1e-05,
      "loss": 7.0581,
      "step": 3828
    },
    {
      "epoch": 0.20747967479674798,
      "step": 3828,
      "training_loss": 5.919142246246338
    },
    {
      "epoch": 0.2075338753387534,
      "step": 3829,
      "training_loss": 8.005236625671387
    },
    {
      "epoch": 0.2075880758807588,
      "step": 3830,
      "training_loss": 6.948180675506592
    },
    {
      "epoch": 0.20764227642276423,
      "step": 3831,
      "training_loss": 6.789438247680664
    },
    {
      "epoch": 0.20769647696476964,
      "grad_norm": 17.76123809814453,
      "learning_rate": 1e-05,
      "loss": 6.9155,
      "step": 3832
    },
    {
      "epoch": 0.20769647696476964,
      "step": 3832,
      "training_loss": 5.358366012573242
    },
    {
      "epoch": 0.20775067750677506,
      "step": 3833,
      "training_loss": 7.674780368804932
    },
    {
      "epoch": 0.20780487804878048,
      "step": 3834,
      "training_loss": 5.386256217956543
    },
    {
      "epoch": 0.2078590785907859,
      "step": 3835,
      "training_loss": 8.076111793518066
    },
    {
      "epoch": 0.20791327913279134,
      "grad_norm": 39.06454849243164,
      "learning_rate": 1e-05,
      "loss": 6.6239,
      "step": 3836
    },
    {
      "epoch": 0.20791327913279134,
      "step": 3836,
      "training_loss": 7.03363561630249
    },
    {
      "epoch": 0.20796747967479676,
      "step": 3837,
      "training_loss": 7.025528907775879
    },
    {
      "epoch": 0.20802168021680217,
      "step": 3838,
      "training_loss": 6.799386978149414
    },
    {
      "epoch": 0.2080758807588076,
      "step": 3839,
      "training_loss": 7.1652607917785645
    },
    {
      "epoch": 0.208130081300813,
      "grad_norm": 21.7110652923584,
      "learning_rate": 1e-05,
      "loss": 7.006,
      "step": 3840
    },
    {
      "epoch": 0.208130081300813,
      "step": 3840,
      "training_loss": 6.302520275115967
    },
    {
      "epoch": 0.20818428184281842,
      "step": 3841,
      "training_loss": 6.851632118225098
    },
    {
      "epoch": 0.20823848238482384,
      "step": 3842,
      "training_loss": 7.327374458312988
    },
    {
      "epoch": 0.20829268292682926,
      "step": 3843,
      "training_loss": 8.052748680114746
    },
    {
      "epoch": 0.20834688346883468,
      "grad_norm": 24.734600067138672,
      "learning_rate": 1e-05,
      "loss": 7.1336,
      "step": 3844
    },
    {
      "epoch": 0.20834688346883468,
      "step": 3844,
      "training_loss": 7.117315769195557
    },
    {
      "epoch": 0.20840108401084012,
      "step": 3845,
      "training_loss": 6.951404571533203
    },
    {
      "epoch": 0.20845528455284554,
      "step": 3846,
      "training_loss": 7.251163005828857
    },
    {
      "epoch": 0.20850948509485095,
      "step": 3847,
      "training_loss": 5.117646217346191
    },
    {
      "epoch": 0.20856368563685637,
      "grad_norm": 20.883214950561523,
      "learning_rate": 1e-05,
      "loss": 6.6094,
      "step": 3848
    },
    {
      "epoch": 0.20856368563685637,
      "step": 3848,
      "training_loss": 6.260022163391113
    },
    {
      "epoch": 0.2086178861788618,
      "step": 3849,
      "training_loss": 7.564265727996826
    },
    {
      "epoch": 0.2086720867208672,
      "step": 3850,
      "training_loss": 7.024861812591553
    },
    {
      "epoch": 0.20872628726287262,
      "step": 3851,
      "training_loss": 7.861347675323486
    },
    {
      "epoch": 0.20878048780487804,
      "grad_norm": 22.853374481201172,
      "learning_rate": 1e-05,
      "loss": 7.1776,
      "step": 3852
    },
    {
      "epoch": 0.20878048780487804,
      "step": 3852,
      "training_loss": 6.137435436248779
    },
    {
      "epoch": 0.20883468834688346,
      "step": 3853,
      "training_loss": 6.759093284606934
    },
    {
      "epoch": 0.2088888888888889,
      "step": 3854,
      "training_loss": 7.878698825836182
    },
    {
      "epoch": 0.20894308943089432,
      "step": 3855,
      "training_loss": 8.156208992004395
    },
    {
      "epoch": 0.20899728997289974,
      "grad_norm": 31.50153350830078,
      "learning_rate": 1e-05,
      "loss": 7.2329,
      "step": 3856
    },
    {
      "epoch": 0.20899728997289974,
      "step": 3856,
      "training_loss": 7.417598247528076
    },
    {
      "epoch": 0.20905149051490515,
      "step": 3857,
      "training_loss": 6.221846103668213
    },
    {
      "epoch": 0.20910569105691057,
      "step": 3858,
      "training_loss": 7.76515007019043
    },
    {
      "epoch": 0.209159891598916,
      "step": 3859,
      "training_loss": 6.516568660736084
    },
    {
      "epoch": 0.2092140921409214,
      "grad_norm": 17.897579193115234,
      "learning_rate": 1e-05,
      "loss": 6.9803,
      "step": 3860
    },
    {
      "epoch": 0.2092140921409214,
      "step": 3860,
      "training_loss": 6.842835426330566
    },
    {
      "epoch": 0.20926829268292682,
      "step": 3861,
      "training_loss": 4.546152591705322
    },
    {
      "epoch": 0.20932249322493224,
      "step": 3862,
      "training_loss": 5.94821834564209
    },
    {
      "epoch": 0.20937669376693768,
      "step": 3863,
      "training_loss": 7.85129451751709
    },
    {
      "epoch": 0.2094308943089431,
      "grad_norm": 29.55127716064453,
      "learning_rate": 1e-05,
      "loss": 6.2971,
      "step": 3864
    },
    {
      "epoch": 0.2094308943089431,
      "step": 3864,
      "training_loss": 7.8184494972229
    },
    {
      "epoch": 0.20948509485094852,
      "step": 3865,
      "training_loss": 9.428171157836914
    },
    {
      "epoch": 0.20953929539295393,
      "step": 3866,
      "training_loss": 7.256155967712402
    },
    {
      "epoch": 0.20959349593495935,
      "step": 3867,
      "training_loss": 7.46991491317749
    },
    {
      "epoch": 0.20964769647696477,
      "grad_norm": 21.921478271484375,
      "learning_rate": 1e-05,
      "loss": 7.9932,
      "step": 3868
    },
    {
      "epoch": 0.20964769647696477,
      "step": 3868,
      "training_loss": 7.719578266143799
    },
    {
      "epoch": 0.20970189701897018,
      "step": 3869,
      "training_loss": 7.902423858642578
    },
    {
      "epoch": 0.2097560975609756,
      "step": 3870,
      "training_loss": 6.133706092834473
    },
    {
      "epoch": 0.20981029810298102,
      "step": 3871,
      "training_loss": 5.818116188049316
    },
    {
      "epoch": 0.20986449864498646,
      "grad_norm": 28.026477813720703,
      "learning_rate": 1e-05,
      "loss": 6.8935,
      "step": 3872
    },
    {
      "epoch": 0.20986449864498646,
      "step": 3872,
      "training_loss": 8.170235633850098
    },
    {
      "epoch": 0.20991869918699188,
      "step": 3873,
      "training_loss": 7.002233028411865
    },
    {
      "epoch": 0.2099728997289973,
      "step": 3874,
      "training_loss": 3.6572487354278564
    },
    {
      "epoch": 0.21002710027100271,
      "step": 3875,
      "training_loss": 7.142763137817383
    },
    {
      "epoch": 0.21008130081300813,
      "grad_norm": 18.906471252441406,
      "learning_rate": 1e-05,
      "loss": 6.4931,
      "step": 3876
    },
    {
      "epoch": 0.21008130081300813,
      "step": 3876,
      "training_loss": 4.376753807067871
    },
    {
      "epoch": 0.21013550135501355,
      "step": 3877,
      "training_loss": 5.4424920082092285
    },
    {
      "epoch": 0.21018970189701897,
      "step": 3878,
      "training_loss": 7.0227556228637695
    },
    {
      "epoch": 0.21024390243902438,
      "step": 3879,
      "training_loss": 7.278994083404541
    },
    {
      "epoch": 0.2102981029810298,
      "grad_norm": 35.929412841796875,
      "learning_rate": 1e-05,
      "loss": 6.0302,
      "step": 3880
    },
    {
      "epoch": 0.2102981029810298,
      "step": 3880,
      "training_loss": 5.761898040771484
    },
    {
      "epoch": 0.21035230352303522,
      "step": 3881,
      "training_loss": 6.787208080291748
    },
    {
      "epoch": 0.21040650406504066,
      "step": 3882,
      "training_loss": 6.820392608642578
    },
    {
      "epoch": 0.21046070460704608,
      "step": 3883,
      "training_loss": 6.706552982330322
    },
    {
      "epoch": 0.2105149051490515,
      "grad_norm": 30.171180725097656,
      "learning_rate": 1e-05,
      "loss": 6.519,
      "step": 3884
    },
    {
      "epoch": 0.2105149051490515,
      "step": 3884,
      "training_loss": 6.850915431976318
    },
    {
      "epoch": 0.2105691056910569,
      "step": 3885,
      "training_loss": 7.010969638824463
    },
    {
      "epoch": 0.21062330623306233,
      "step": 3886,
      "training_loss": 6.954214572906494
    },
    {
      "epoch": 0.21067750677506775,
      "step": 3887,
      "training_loss": 7.230890274047852
    },
    {
      "epoch": 0.21073170731707316,
      "grad_norm": 16.000232696533203,
      "learning_rate": 1e-05,
      "loss": 7.0117,
      "step": 3888
    },
    {
      "epoch": 0.21073170731707316,
      "step": 3888,
      "training_loss": 7.519984245300293
    },
    {
      "epoch": 0.21078590785907858,
      "step": 3889,
      "training_loss": 6.69010591506958
    },
    {
      "epoch": 0.210840108401084,
      "step": 3890,
      "training_loss": 6.395223140716553
    },
    {
      "epoch": 0.21089430894308944,
      "step": 3891,
      "training_loss": 8.101165771484375
    },
    {
      "epoch": 0.21094850948509486,
      "grad_norm": 26.299665451049805,
      "learning_rate": 1e-05,
      "loss": 7.1766,
      "step": 3892
    },
    {
      "epoch": 0.21094850948509486,
      "step": 3892,
      "training_loss": 4.784836769104004
    },
    {
      "epoch": 0.21100271002710028,
      "step": 3893,
      "training_loss": 6.6865057945251465
    },
    {
      "epoch": 0.2110569105691057,
      "step": 3894,
      "training_loss": 9.5493803024292
    },
    {
      "epoch": 0.2111111111111111,
      "step": 3895,
      "training_loss": 6.849838733673096
    },
    {
      "epoch": 0.21116531165311653,
      "grad_norm": 32.127017974853516,
      "learning_rate": 1e-05,
      "loss": 6.9676,
      "step": 3896
    },
    {
      "epoch": 0.21116531165311653,
      "step": 3896,
      "training_loss": 6.4462103843688965
    },
    {
      "epoch": 0.21121951219512194,
      "step": 3897,
      "training_loss": 5.083954811096191
    },
    {
      "epoch": 0.21127371273712736,
      "step": 3898,
      "training_loss": 6.025144577026367
    },
    {
      "epoch": 0.21132791327913278,
      "step": 3899,
      "training_loss": 6.726621627807617
    },
    {
      "epoch": 0.21138211382113822,
      "grad_norm": 29.129610061645508,
      "learning_rate": 1e-05,
      "loss": 6.0705,
      "step": 3900
    },
    {
      "epoch": 0.21138211382113822,
      "step": 3900,
      "training_loss": 4.559609413146973
    },
    {
      "epoch": 0.21143631436314364,
      "step": 3901,
      "training_loss": 8.209848403930664
    },
    {
      "epoch": 0.21149051490514906,
      "step": 3902,
      "training_loss": 6.339117050170898
    },
    {
      "epoch": 0.21154471544715447,
      "step": 3903,
      "training_loss": 7.030979156494141
    },
    {
      "epoch": 0.2115989159891599,
      "grad_norm": 25.902301788330078,
      "learning_rate": 1e-05,
      "loss": 6.5349,
      "step": 3904
    },
    {
      "epoch": 0.2115989159891599,
      "step": 3904,
      "training_loss": 7.418928623199463
    },
    {
      "epoch": 0.2116531165311653,
      "step": 3905,
      "training_loss": 4.027886867523193
    },
    {
      "epoch": 0.21170731707317073,
      "step": 3906,
      "training_loss": 6.433897972106934
    },
    {
      "epoch": 0.21176151761517614,
      "step": 3907,
      "training_loss": 7.607709884643555
    },
    {
      "epoch": 0.21181571815718156,
      "grad_norm": 36.51329803466797,
      "learning_rate": 1e-05,
      "loss": 6.3721,
      "step": 3908
    },
    {
      "epoch": 0.21181571815718156,
      "step": 3908,
      "training_loss": 4.887247562408447
    },
    {
      "epoch": 0.211869918699187,
      "step": 3909,
      "training_loss": 6.063617706298828
    },
    {
      "epoch": 0.21192411924119242,
      "step": 3910,
      "training_loss": 5.577686786651611
    },
    {
      "epoch": 0.21197831978319784,
      "step": 3911,
      "training_loss": 6.3955793380737305
    },
    {
      "epoch": 0.21203252032520326,
      "grad_norm": 17.389474868774414,
      "learning_rate": 1e-05,
      "loss": 5.731,
      "step": 3912
    },
    {
      "epoch": 0.21203252032520326,
      "step": 3912,
      "training_loss": 7.088480472564697
    },
    {
      "epoch": 0.21208672086720867,
      "step": 3913,
      "training_loss": 6.919827461242676
    },
    {
      "epoch": 0.2121409214092141,
      "step": 3914,
      "training_loss": 5.264483451843262
    },
    {
      "epoch": 0.2121951219512195,
      "step": 3915,
      "training_loss": 6.596003532409668
    },
    {
      "epoch": 0.21224932249322492,
      "grad_norm": 29.7912540435791,
      "learning_rate": 1e-05,
      "loss": 6.4672,
      "step": 3916
    },
    {
      "epoch": 0.21224932249322492,
      "step": 3916,
      "training_loss": 6.253208160400391
    },
    {
      "epoch": 0.21230352303523034,
      "step": 3917,
      "training_loss": 7.958639621734619
    },
    {
      "epoch": 0.21235772357723579,
      "step": 3918,
      "training_loss": 6.201200008392334
    },
    {
      "epoch": 0.2124119241192412,
      "step": 3919,
      "training_loss": 7.362308025360107
    },
    {
      "epoch": 0.21246612466124662,
      "grad_norm": 17.51653289794922,
      "learning_rate": 1e-05,
      "loss": 6.9438,
      "step": 3920
    },
    {
      "epoch": 0.21246612466124662,
      "step": 3920,
      "training_loss": 7.547051906585693
    },
    {
      "epoch": 0.21252032520325204,
      "step": 3921,
      "training_loss": 6.605013370513916
    },
    {
      "epoch": 0.21257452574525745,
      "step": 3922,
      "training_loss": 7.500350475311279
    },
    {
      "epoch": 0.21262872628726287,
      "step": 3923,
      "training_loss": 6.36594295501709
    },
    {
      "epoch": 0.2126829268292683,
      "grad_norm": 22.6464786529541,
      "learning_rate": 1e-05,
      "loss": 7.0046,
      "step": 3924
    },
    {
      "epoch": 0.2126829268292683,
      "step": 3924,
      "training_loss": 4.848431587219238
    },
    {
      "epoch": 0.2127371273712737,
      "step": 3925,
      "training_loss": 7.187383651733398
    },
    {
      "epoch": 0.21279132791327912,
      "step": 3926,
      "training_loss": 7.2261643409729
    },
    {
      "epoch": 0.21284552845528457,
      "step": 3927,
      "training_loss": 6.788715839385986
    },
    {
      "epoch": 0.21289972899728998,
      "grad_norm": 24.177133560180664,
      "learning_rate": 1e-05,
      "loss": 6.5127,
      "step": 3928
    },
    {
      "epoch": 0.21289972899728998,
      "step": 3928,
      "training_loss": 5.853002548217773
    },
    {
      "epoch": 0.2129539295392954,
      "step": 3929,
      "training_loss": 7.211835861206055
    },
    {
      "epoch": 0.21300813008130082,
      "step": 3930,
      "training_loss": 6.760707855224609
    },
    {
      "epoch": 0.21306233062330623,
      "step": 3931,
      "training_loss": 6.116564750671387
    },
    {
      "epoch": 0.21311653116531165,
      "grad_norm": 31.79603385925293,
      "learning_rate": 1e-05,
      "loss": 6.4855,
      "step": 3932
    },
    {
      "epoch": 0.21311653116531165,
      "step": 3932,
      "training_loss": 5.856260299682617
    },
    {
      "epoch": 0.21317073170731707,
      "step": 3933,
      "training_loss": 6.58408260345459
    },
    {
      "epoch": 0.21322493224932249,
      "step": 3934,
      "training_loss": 6.241202354431152
    },
    {
      "epoch": 0.2132791327913279,
      "step": 3935,
      "training_loss": 7.325661659240723
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 28.12739372253418,
      "learning_rate": 1e-05,
      "loss": 6.5018,
      "step": 3936
    },
    {
      "epoch": 0.21333333333333335,
      "step": 3936,
      "training_loss": 8.290103912353516
    },
    {
      "epoch": 0.21338753387533876,
      "step": 3937,
      "training_loss": 5.431718826293945
    },
    {
      "epoch": 0.21344173441734418,
      "step": 3938,
      "training_loss": 5.692765712738037
    },
    {
      "epoch": 0.2134959349593496,
      "step": 3939,
      "training_loss": 7.300207614898682
    },
    {
      "epoch": 0.21355013550135502,
      "grad_norm": 20.010469436645508,
      "learning_rate": 1e-05,
      "loss": 6.6787,
      "step": 3940
    },
    {
      "epoch": 0.21355013550135502,
      "step": 3940,
      "training_loss": 7.697545528411865
    },
    {
      "epoch": 0.21360433604336043,
      "step": 3941,
      "training_loss": 8.208379745483398
    },
    {
      "epoch": 0.21365853658536585,
      "step": 3942,
      "training_loss": 6.883516788482666
    },
    {
      "epoch": 0.21371273712737127,
      "step": 3943,
      "training_loss": 7.677276611328125
    },
    {
      "epoch": 0.21376693766937668,
      "grad_norm": 27.64386558532715,
      "learning_rate": 1e-05,
      "loss": 7.6167,
      "step": 3944
    },
    {
      "epoch": 0.21376693766937668,
      "step": 3944,
      "training_loss": 4.435361862182617
    },
    {
      "epoch": 0.2138211382113821,
      "step": 3945,
      "training_loss": 6.47470760345459
    },
    {
      "epoch": 0.21387533875338754,
      "step": 3946,
      "training_loss": 8.673595428466797
    },
    {
      "epoch": 0.21392953929539296,
      "step": 3947,
      "training_loss": 8.247069358825684
    },
    {
      "epoch": 0.21398373983739838,
      "grad_norm": 31.07786750793457,
      "learning_rate": 1e-05,
      "loss": 6.9577,
      "step": 3948
    },
    {
      "epoch": 0.21398373983739838,
      "step": 3948,
      "training_loss": 5.961287498474121
    },
    {
      "epoch": 0.2140379403794038,
      "step": 3949,
      "training_loss": 6.879069805145264
    },
    {
      "epoch": 0.2140921409214092,
      "step": 3950,
      "training_loss": 7.060001850128174
    },
    {
      "epoch": 0.21414634146341463,
      "step": 3951,
      "training_loss": 7.368741035461426
    },
    {
      "epoch": 0.21420054200542005,
      "grad_norm": 20.216026306152344,
      "learning_rate": 1e-05,
      "loss": 6.8173,
      "step": 3952
    },
    {
      "epoch": 0.21420054200542005,
      "step": 3952,
      "training_loss": 7.894190311431885
    },
    {
      "epoch": 0.21425474254742546,
      "step": 3953,
      "training_loss": 7.171477317810059
    },
    {
      "epoch": 0.21430894308943088,
      "step": 3954,
      "training_loss": 7.401179790496826
    },
    {
      "epoch": 0.21436314363143633,
      "step": 3955,
      "training_loss": 6.653152942657471
    },
    {
      "epoch": 0.21441734417344174,
      "grad_norm": 24.149723052978516,
      "learning_rate": 1e-05,
      "loss": 7.28,
      "step": 3956
    },
    {
      "epoch": 0.21441734417344174,
      "step": 3956,
      "training_loss": 6.920646667480469
    },
    {
      "epoch": 0.21447154471544716,
      "step": 3957,
      "training_loss": 7.3410725593566895
    },
    {
      "epoch": 0.21452574525745258,
      "step": 3958,
      "training_loss": 7.406496047973633
    },
    {
      "epoch": 0.214579945799458,
      "step": 3959,
      "training_loss": 6.755458354949951
    },
    {
      "epoch": 0.2146341463414634,
      "grad_norm": 16.520530700683594,
      "learning_rate": 1e-05,
      "loss": 7.1059,
      "step": 3960
    },
    {
      "epoch": 0.2146341463414634,
      "step": 3960,
      "training_loss": 7.261811256408691
    },
    {
      "epoch": 0.21468834688346883,
      "step": 3961,
      "training_loss": 6.761796474456787
    },
    {
      "epoch": 0.21474254742547425,
      "step": 3962,
      "training_loss": 7.253380298614502
    },
    {
      "epoch": 0.21479674796747966,
      "step": 3963,
      "training_loss": 6.8646087646484375
    },
    {
      "epoch": 0.2148509485094851,
      "grad_norm": 24.80393409729004,
      "learning_rate": 1e-05,
      "loss": 7.0354,
      "step": 3964
    },
    {
      "epoch": 0.2148509485094851,
      "step": 3964,
      "training_loss": 6.807260513305664
    },
    {
      "epoch": 0.21490514905149052,
      "step": 3965,
      "training_loss": 7.798518180847168
    },
    {
      "epoch": 0.21495934959349594,
      "step": 3966,
      "training_loss": 5.643454074859619
    },
    {
      "epoch": 0.21501355013550136,
      "step": 3967,
      "training_loss": 6.414785861968994
    },
    {
      "epoch": 0.21506775067750677,
      "grad_norm": 37.713035583496094,
      "learning_rate": 1e-05,
      "loss": 6.666,
      "step": 3968
    },
    {
      "epoch": 0.21506775067750677,
      "step": 3968,
      "training_loss": 4.464761257171631
    },
    {
      "epoch": 0.2151219512195122,
      "step": 3969,
      "training_loss": 6.2351884841918945
    },
    {
      "epoch": 0.2151761517615176,
      "step": 3970,
      "training_loss": 6.8583083152771
    },
    {
      "epoch": 0.21523035230352303,
      "step": 3971,
      "training_loss": 8.083370208740234
    },
    {
      "epoch": 0.21528455284552844,
      "grad_norm": 43.66646194458008,
      "learning_rate": 1e-05,
      "loss": 6.4104,
      "step": 3972
    },
    {
      "epoch": 0.21528455284552844,
      "step": 3972,
      "training_loss": 7.83139181137085
    },
    {
      "epoch": 0.2153387533875339,
      "step": 3973,
      "training_loss": 5.919060707092285
    },
    {
      "epoch": 0.2153929539295393,
      "step": 3974,
      "training_loss": 7.18191385269165
    },
    {
      "epoch": 0.21544715447154472,
      "step": 3975,
      "training_loss": 8.761238098144531
    },
    {
      "epoch": 0.21550135501355014,
      "grad_norm": 24.323101043701172,
      "learning_rate": 1e-05,
      "loss": 7.4234,
      "step": 3976
    },
    {
      "epoch": 0.21550135501355014,
      "step": 3976,
      "training_loss": 6.746639251708984
    },
    {
      "epoch": 0.21555555555555556,
      "step": 3977,
      "training_loss": 5.926162242889404
    },
    {
      "epoch": 0.21560975609756097,
      "step": 3978,
      "training_loss": 7.56758451461792
    },
    {
      "epoch": 0.2156639566395664,
      "step": 3979,
      "training_loss": 7.3034467697143555
    },
    {
      "epoch": 0.2157181571815718,
      "grad_norm": 27.82272720336914,
      "learning_rate": 1e-05,
      "loss": 6.886,
      "step": 3980
    },
    {
      "epoch": 0.2157181571815718,
      "step": 3980,
      "training_loss": 5.06129264831543
    },
    {
      "epoch": 0.21577235772357722,
      "step": 3981,
      "training_loss": 7.32918119430542
    },
    {
      "epoch": 0.21582655826558267,
      "step": 3982,
      "training_loss": 7.543288230895996
    },
    {
      "epoch": 0.21588075880758809,
      "step": 3983,
      "training_loss": 6.033237457275391
    },
    {
      "epoch": 0.2159349593495935,
      "grad_norm": 39.033084869384766,
      "learning_rate": 1e-05,
      "loss": 6.4917,
      "step": 3984
    },
    {
      "epoch": 0.2159349593495935,
      "step": 3984,
      "training_loss": 7.87290620803833
    },
    {
      "epoch": 0.21598915989159892,
      "step": 3985,
      "training_loss": 7.139349460601807
    },
    {
      "epoch": 0.21604336043360434,
      "step": 3986,
      "training_loss": 7.95499324798584
    },
    {
      "epoch": 0.21609756097560975,
      "step": 3987,
      "training_loss": 7.343323230743408
    },
    {
      "epoch": 0.21615176151761517,
      "grad_norm": 18.44797706604004,
      "learning_rate": 1e-05,
      "loss": 7.5776,
      "step": 3988
    },
    {
      "epoch": 0.21615176151761517,
      "step": 3988,
      "training_loss": 7.071167469024658
    },
    {
      "epoch": 0.2162059620596206,
      "step": 3989,
      "training_loss": 6.719727993011475
    },
    {
      "epoch": 0.216260162601626,
      "step": 3990,
      "training_loss": 7.5694708824157715
    },
    {
      "epoch": 0.21631436314363145,
      "step": 3991,
      "training_loss": 7.385441780090332
    },
    {
      "epoch": 0.21636856368563687,
      "grad_norm": 24.510669708251953,
      "learning_rate": 1e-05,
      "loss": 7.1865,
      "step": 3992
    },
    {
      "epoch": 0.21636856368563687,
      "step": 3992,
      "training_loss": 6.682980537414551
    },
    {
      "epoch": 0.21642276422764228,
      "step": 3993,
      "training_loss": 6.567015171051025
    },
    {
      "epoch": 0.2164769647696477,
      "step": 3994,
      "training_loss": 8.345881462097168
    },
    {
      "epoch": 0.21653116531165312,
      "step": 3995,
      "training_loss": 8.279413223266602
    },
    {
      "epoch": 0.21658536585365853,
      "grad_norm": 22.561193466186523,
      "learning_rate": 1e-05,
      "loss": 7.4688,
      "step": 3996
    },
    {
      "epoch": 0.21658536585365853,
      "step": 3996,
      "training_loss": 7.466126441955566
    },
    {
      "epoch": 0.21663956639566395,
      "step": 3997,
      "training_loss": 6.230143070220947
    },
    {
      "epoch": 0.21669376693766937,
      "step": 3998,
      "training_loss": 6.3392333984375
    },
    {
      "epoch": 0.21674796747967479,
      "step": 3999,
      "training_loss": 6.9574785232543945
    },
    {
      "epoch": 0.21680216802168023,
      "grad_norm": 33.403350830078125,
      "learning_rate": 1e-05,
      "loss": 6.7482,
      "step": 4000
    },
    {
      "epoch": 0.21680216802168023,
      "step": 4000,
      "training_loss": 7.1234331130981445
    },
    {
      "epoch": 0.21685636856368565,
      "step": 4001,
      "training_loss": 6.23089599609375
    },
    {
      "epoch": 0.21691056910569106,
      "step": 4002,
      "training_loss": 6.679632186889648
    },
    {
      "epoch": 0.21696476964769648,
      "step": 4003,
      "training_loss": 8.928170204162598
    },
    {
      "epoch": 0.2170189701897019,
      "grad_norm": 35.96221160888672,
      "learning_rate": 1e-05,
      "loss": 7.2405,
      "step": 4004
    },
    {
      "epoch": 0.2170189701897019,
      "step": 4004,
      "training_loss": 7.260451793670654
    },
    {
      "epoch": 0.21707317073170732,
      "step": 4005,
      "training_loss": 10.046250343322754
    },
    {
      "epoch": 0.21712737127371273,
      "step": 4006,
      "training_loss": 7.7572126388549805
    },
    {
      "epoch": 0.21718157181571815,
      "step": 4007,
      "training_loss": 7.303957462310791
    },
    {
      "epoch": 0.21723577235772357,
      "grad_norm": 35.875999450683594,
      "learning_rate": 1e-05,
      "loss": 8.092,
      "step": 4008
    },
    {
      "epoch": 0.21723577235772357,
      "step": 4008,
      "training_loss": 6.981508731842041
    },
    {
      "epoch": 0.21728997289972898,
      "step": 4009,
      "training_loss": 6.686259746551514
    },
    {
      "epoch": 0.21734417344173443,
      "step": 4010,
      "training_loss": 7.673064708709717
    },
    {
      "epoch": 0.21739837398373985,
      "step": 4011,
      "training_loss": 6.770108699798584
    },
    {
      "epoch": 0.21745257452574526,
      "grad_norm": 14.917753219604492,
      "learning_rate": 1e-05,
      "loss": 7.0277,
      "step": 4012
    },
    {
      "epoch": 0.21745257452574526,
      "step": 4012,
      "training_loss": 7.362985610961914
    },
    {
      "epoch": 0.21750677506775068,
      "step": 4013,
      "training_loss": 7.669528007507324
    },
    {
      "epoch": 0.2175609756097561,
      "step": 4014,
      "training_loss": 5.989074230194092
    },
    {
      "epoch": 0.2176151761517615,
      "step": 4015,
      "training_loss": 7.576344013214111
    },
    {
      "epoch": 0.21766937669376693,
      "grad_norm": 36.9124641418457,
      "learning_rate": 1e-05,
      "loss": 7.1495,
      "step": 4016
    },
    {
      "epoch": 0.21766937669376693,
      "step": 4016,
      "training_loss": 7.360233306884766
    },
    {
      "epoch": 0.21772357723577235,
      "step": 4017,
      "training_loss": 8.645576477050781
    },
    {
      "epoch": 0.21777777777777776,
      "step": 4018,
      "training_loss": 7.079346179962158
    },
    {
      "epoch": 0.2178319783197832,
      "step": 4019,
      "training_loss": 6.884378910064697
    },
    {
      "epoch": 0.21788617886178863,
      "grad_norm": 23.944272994995117,
      "learning_rate": 1e-05,
      "loss": 7.4924,
      "step": 4020
    },
    {
      "epoch": 0.21788617886178863,
      "step": 4020,
      "training_loss": 6.64524507522583
    },
    {
      "epoch": 0.21794037940379404,
      "step": 4021,
      "training_loss": 6.1658735275268555
    },
    {
      "epoch": 0.21799457994579946,
      "step": 4022,
      "training_loss": 5.417333126068115
    },
    {
      "epoch": 0.21804878048780488,
      "step": 4023,
      "training_loss": 5.909547328948975
    },
    {
      "epoch": 0.2181029810298103,
      "grad_norm": 24.732955932617188,
      "learning_rate": 1e-05,
      "loss": 6.0345,
      "step": 4024
    },
    {
      "epoch": 0.2181029810298103,
      "step": 4024,
      "training_loss": 6.178500652313232
    },
    {
      "epoch": 0.2181571815718157,
      "step": 4025,
      "training_loss": 7.989866733551025
    },
    {
      "epoch": 0.21821138211382113,
      "step": 4026,
      "training_loss": 4.387531757354736
    },
    {
      "epoch": 0.21826558265582655,
      "step": 4027,
      "training_loss": 7.167354583740234
    },
    {
      "epoch": 0.218319783197832,
      "grad_norm": 21.86565589904785,
      "learning_rate": 1e-05,
      "loss": 6.4308,
      "step": 4028
    },
    {
      "epoch": 0.218319783197832,
      "step": 4028,
      "training_loss": 6.924036979675293
    },
    {
      "epoch": 0.2183739837398374,
      "step": 4029,
      "training_loss": 7.1644134521484375
    },
    {
      "epoch": 0.21842818428184282,
      "step": 4030,
      "training_loss": 6.938899040222168
    },
    {
      "epoch": 0.21848238482384824,
      "step": 4031,
      "training_loss": 6.343527793884277
    },
    {
      "epoch": 0.21853658536585366,
      "grad_norm": 37.84022521972656,
      "learning_rate": 1e-05,
      "loss": 6.8427,
      "step": 4032
    },
    {
      "epoch": 0.21853658536585366,
      "step": 4032,
      "training_loss": 4.816824913024902
    },
    {
      "epoch": 0.21859078590785908,
      "step": 4033,
      "training_loss": 7.394515037536621
    },
    {
      "epoch": 0.2186449864498645,
      "step": 4034,
      "training_loss": 7.1368088722229
    },
    {
      "epoch": 0.2186991869918699,
      "step": 4035,
      "training_loss": 7.048422813415527
    },
    {
      "epoch": 0.21875338753387533,
      "grad_norm": 17.616886138916016,
      "learning_rate": 1e-05,
      "loss": 6.5991,
      "step": 4036
    },
    {
      "epoch": 0.21875338753387533,
      "step": 4036,
      "training_loss": 6.9381256103515625
    },
    {
      "epoch": 0.21880758807588077,
      "step": 4037,
      "training_loss": 7.411272048950195
    },
    {
      "epoch": 0.2188617886178862,
      "step": 4038,
      "training_loss": 7.592193126678467
    },
    {
      "epoch": 0.2189159891598916,
      "step": 4039,
      "training_loss": 6.4018235206604
    },
    {
      "epoch": 0.21897018970189702,
      "grad_norm": 35.38779067993164,
      "learning_rate": 1e-05,
      "loss": 7.0859,
      "step": 4040
    },
    {
      "epoch": 0.21897018970189702,
      "step": 4040,
      "training_loss": 6.574548244476318
    },
    {
      "epoch": 0.21902439024390244,
      "step": 4041,
      "training_loss": 4.18763542175293
    },
    {
      "epoch": 0.21907859078590786,
      "step": 4042,
      "training_loss": 7.29395055770874
    },
    {
      "epoch": 0.21913279132791327,
      "step": 4043,
      "training_loss": 7.049490928649902
    },
    {
      "epoch": 0.2191869918699187,
      "grad_norm": 17.02776336669922,
      "learning_rate": 1e-05,
      "loss": 6.2764,
      "step": 4044
    },
    {
      "epoch": 0.2191869918699187,
      "step": 4044,
      "training_loss": 6.527430057525635
    },
    {
      "epoch": 0.2192411924119241,
      "step": 4045,
      "training_loss": 7.195842742919922
    },
    {
      "epoch": 0.21929539295392955,
      "step": 4046,
      "training_loss": 6.855818748474121
    },
    {
      "epoch": 0.21934959349593497,
      "step": 4047,
      "training_loss": 7.281011581420898
    },
    {
      "epoch": 0.2194037940379404,
      "grad_norm": 49.759891510009766,
      "learning_rate": 1e-05,
      "loss": 6.965,
      "step": 4048
    },
    {
      "epoch": 0.2194037940379404,
      "step": 4048,
      "training_loss": 5.390628814697266
    },
    {
      "epoch": 0.2194579945799458,
      "step": 4049,
      "training_loss": 6.446857452392578
    },
    {
      "epoch": 0.21951219512195122,
      "step": 4050,
      "training_loss": 6.016637802124023
    },
    {
      "epoch": 0.21956639566395664,
      "step": 4051,
      "training_loss": 7.54254150390625
    },
    {
      "epoch": 0.21962059620596205,
      "grad_norm": 32.00193405151367,
      "learning_rate": 1e-05,
      "loss": 6.3492,
      "step": 4052
    },
    {
      "epoch": 0.21962059620596205,
      "step": 4052,
      "training_loss": 7.071527004241943
    },
    {
      "epoch": 0.21967479674796747,
      "step": 4053,
      "training_loss": 6.744954586029053
    },
    {
      "epoch": 0.2197289972899729,
      "step": 4054,
      "training_loss": 5.854695796966553
    },
    {
      "epoch": 0.21978319783197833,
      "step": 4055,
      "training_loss": 6.7136335372924805
    },
    {
      "epoch": 0.21983739837398375,
      "grad_norm": 20.965545654296875,
      "learning_rate": 1e-05,
      "loss": 6.5962,
      "step": 4056
    },
    {
      "epoch": 0.21983739837398375,
      "step": 4056,
      "training_loss": 5.615957736968994
    },
    {
      "epoch": 0.21989159891598917,
      "step": 4057,
      "training_loss": 6.525793552398682
    },
    {
      "epoch": 0.21994579945799458,
      "step": 4058,
      "training_loss": 6.771556377410889
    },
    {
      "epoch": 0.22,
      "step": 4059,
      "training_loss": 5.97222900390625
    },
    {
      "epoch": 0.22005420054200542,
      "grad_norm": 37.72552490234375,
      "learning_rate": 1e-05,
      "loss": 6.2214,
      "step": 4060
    },
    {
      "epoch": 0.22005420054200542,
      "step": 4060,
      "training_loss": 6.7936201095581055
    },
    {
      "epoch": 0.22010840108401084,
      "step": 4061,
      "training_loss": 6.342365741729736
    },
    {
      "epoch": 0.22016260162601625,
      "step": 4062,
      "training_loss": 6.820255756378174
    },
    {
      "epoch": 0.22021680216802167,
      "step": 4063,
      "training_loss": 5.9772467613220215
    },
    {
      "epoch": 0.22027100271002711,
      "grad_norm": 27.470779418945312,
      "learning_rate": 1e-05,
      "loss": 6.4834,
      "step": 4064
    },
    {
      "epoch": 0.22027100271002711,
      "step": 4064,
      "training_loss": 7.207183837890625
    },
    {
      "epoch": 0.22032520325203253,
      "step": 4065,
      "training_loss": 6.88875150680542
    },
    {
      "epoch": 0.22037940379403795,
      "step": 4066,
      "training_loss": 6.725322723388672
    },
    {
      "epoch": 0.22043360433604337,
      "step": 4067,
      "training_loss": 5.365711688995361
    },
    {
      "epoch": 0.22048780487804878,
      "grad_norm": 34.53125,
      "learning_rate": 1e-05,
      "loss": 6.5467,
      "step": 4068
    },
    {
      "epoch": 0.22048780487804878,
      "step": 4068,
      "training_loss": 7.22627592086792
    },
    {
      "epoch": 0.2205420054200542,
      "step": 4069,
      "training_loss": 7.478855609893799
    },
    {
      "epoch": 0.22059620596205962,
      "step": 4070,
      "training_loss": 5.36694860458374
    },
    {
      "epoch": 0.22065040650406503,
      "step": 4071,
      "training_loss": 6.9834699630737305
    },
    {
      "epoch": 0.22070460704607045,
      "grad_norm": 20.059154510498047,
      "learning_rate": 1e-05,
      "loss": 6.7639,
      "step": 4072
    },
    {
      "epoch": 0.22070460704607045,
      "step": 4072,
      "training_loss": 8.588367462158203
    },
    {
      "epoch": 0.22075880758807587,
      "step": 4073,
      "training_loss": 6.415325164794922
    },
    {
      "epoch": 0.2208130081300813,
      "step": 4074,
      "training_loss": 6.20147705078125
    },
    {
      "epoch": 0.22086720867208673,
      "step": 4075,
      "training_loss": 6.616713523864746
    },
    {
      "epoch": 0.22092140921409215,
      "grad_norm": 23.016292572021484,
      "learning_rate": 1e-05,
      "loss": 6.9555,
      "step": 4076
    },
    {
      "epoch": 0.22092140921409215,
      "step": 4076,
      "training_loss": 5.838983058929443
    },
    {
      "epoch": 0.22097560975609756,
      "step": 4077,
      "training_loss": 7.553499698638916
    },
    {
      "epoch": 0.22102981029810298,
      "step": 4078,
      "training_loss": 5.4524431228637695
    },
    {
      "epoch": 0.2210840108401084,
      "step": 4079,
      "training_loss": 6.952179908752441
    },
    {
      "epoch": 0.22113821138211381,
      "grad_norm": 21.014917373657227,
      "learning_rate": 1e-05,
      "loss": 6.4493,
      "step": 4080
    },
    {
      "epoch": 0.22113821138211381,
      "step": 4080,
      "training_loss": 6.931869029998779
    },
    {
      "epoch": 0.22119241192411923,
      "step": 4081,
      "training_loss": 7.305164337158203
    },
    {
      "epoch": 0.22124661246612465,
      "step": 4082,
      "training_loss": 8.35661506652832
    },
    {
      "epoch": 0.2213008130081301,
      "step": 4083,
      "training_loss": 7.171274185180664
    },
    {
      "epoch": 0.2213550135501355,
      "grad_norm": 21.008018493652344,
      "learning_rate": 1e-05,
      "loss": 7.4412,
      "step": 4084
    },
    {
      "epoch": 0.2213550135501355,
      "step": 4084,
      "training_loss": 8.232091903686523
    },
    {
      "epoch": 0.22140921409214093,
      "step": 4085,
      "training_loss": 4.439918518066406
    },
    {
      "epoch": 0.22146341463414634,
      "step": 4086,
      "training_loss": 7.365460395812988
    },
    {
      "epoch": 0.22151761517615176,
      "step": 4087,
      "training_loss": 7.116079330444336
    },
    {
      "epoch": 0.22157181571815718,
      "grad_norm": 21.859619140625,
      "learning_rate": 1e-05,
      "loss": 6.7884,
      "step": 4088
    },
    {
      "epoch": 0.22157181571815718,
      "step": 4088,
      "training_loss": 7.2144904136657715
    },
    {
      "epoch": 0.2216260162601626,
      "step": 4089,
      "training_loss": 7.732489585876465
    },
    {
      "epoch": 0.221680216802168,
      "step": 4090,
      "training_loss": 7.292977809906006
    },
    {
      "epoch": 0.22173441734417343,
      "step": 4091,
      "training_loss": 6.6245903968811035
    },
    {
      "epoch": 0.22178861788617887,
      "grad_norm": 20.061840057373047,
      "learning_rate": 1e-05,
      "loss": 7.2161,
      "step": 4092
    },
    {
      "epoch": 0.22178861788617887,
      "step": 4092,
      "training_loss": 6.798467636108398
    },
    {
      "epoch": 0.2218428184281843,
      "step": 4093,
      "training_loss": 6.682686805725098
    },
    {
      "epoch": 0.2218970189701897,
      "step": 4094,
      "training_loss": 6.902859210968018
    },
    {
      "epoch": 0.22195121951219512,
      "step": 4095,
      "training_loss": 6.949934005737305
    },
    {
      "epoch": 0.22200542005420054,
      "grad_norm": 20.005212783813477,
      "learning_rate": 1e-05,
      "loss": 6.8335,
      "step": 4096
    },
    {
      "epoch": 0.22200542005420054,
      "step": 4096,
      "training_loss": 4.53184175491333
    },
    {
      "epoch": 0.22205962059620596,
      "step": 4097,
      "training_loss": 6.054826259613037
    },
    {
      "epoch": 0.22211382113821138,
      "step": 4098,
      "training_loss": 6.120676517486572
    },
    {
      "epoch": 0.2221680216802168,
      "step": 4099,
      "training_loss": 6.467099666595459
    },
    {
      "epoch": 0.2222222222222222,
      "grad_norm": 22.558429718017578,
      "learning_rate": 1e-05,
      "loss": 5.7936,
      "step": 4100
    },
    {
      "epoch": 0.2222222222222222,
      "step": 4100,
      "training_loss": 6.183416843414307
    },
    {
      "epoch": 0.22227642276422765,
      "step": 4101,
      "training_loss": 6.821567535400391
    },
    {
      "epoch": 0.22233062330623307,
      "step": 4102,
      "training_loss": 6.113670349121094
    },
    {
      "epoch": 0.2223848238482385,
      "step": 4103,
      "training_loss": 6.852871894836426
    },
    {
      "epoch": 0.2224390243902439,
      "grad_norm": 18.4316349029541,
      "learning_rate": 1e-05,
      "loss": 6.4929,
      "step": 4104
    },
    {
      "epoch": 0.2224390243902439,
      "step": 4104,
      "training_loss": 8.151611328125
    },
    {
      "epoch": 0.22249322493224932,
      "step": 4105,
      "training_loss": 6.8867411613464355
    },
    {
      "epoch": 0.22254742547425474,
      "step": 4106,
      "training_loss": 7.003637313842773
    },
    {
      "epoch": 0.22260162601626016,
      "step": 4107,
      "training_loss": 7.180736064910889
    },
    {
      "epoch": 0.22265582655826557,
      "grad_norm": 24.627662658691406,
      "learning_rate": 1e-05,
      "loss": 7.3057,
      "step": 4108
    },
    {
      "epoch": 0.22265582655826557,
      "step": 4108,
      "training_loss": 6.648873329162598
    },
    {
      "epoch": 0.222710027100271,
      "step": 4109,
      "training_loss": 8.099369049072266
    },
    {
      "epoch": 0.22276422764227644,
      "step": 4110,
      "training_loss": 6.947166442871094
    },
    {
      "epoch": 0.22281842818428185,
      "step": 4111,
      "training_loss": 8.297039985656738
    },
    {
      "epoch": 0.22287262872628727,
      "grad_norm": 29.77553367614746,
      "learning_rate": 1e-05,
      "loss": 7.4981,
      "step": 4112
    },
    {
      "epoch": 0.22287262872628727,
      "step": 4112,
      "training_loss": 7.80922794342041
    },
    {
      "epoch": 0.2229268292682927,
      "step": 4113,
      "training_loss": 5.678173065185547
    },
    {
      "epoch": 0.2229810298102981,
      "step": 4114,
      "training_loss": 7.511624336242676
    },
    {
      "epoch": 0.22303523035230352,
      "step": 4115,
      "training_loss": 7.506175994873047
    },
    {
      "epoch": 0.22308943089430894,
      "grad_norm": 20.34319496154785,
      "learning_rate": 1e-05,
      "loss": 7.1263,
      "step": 4116
    },
    {
      "epoch": 0.22308943089430894,
      "step": 4116,
      "training_loss": 6.923946380615234
    },
    {
      "epoch": 0.22314363143631435,
      "step": 4117,
      "training_loss": 7.284884929656982
    },
    {
      "epoch": 0.22319783197831977,
      "step": 4118,
      "training_loss": 6.614790916442871
    },
    {
      "epoch": 0.22325203252032522,
      "step": 4119,
      "training_loss": 6.570936679840088
    },
    {
      "epoch": 0.22330623306233063,
      "grad_norm": 34.592376708984375,
      "learning_rate": 1e-05,
      "loss": 6.8486,
      "step": 4120
    },
    {
      "epoch": 0.22330623306233063,
      "step": 4120,
      "training_loss": 6.971793174743652
    },
    {
      "epoch": 0.22336043360433605,
      "step": 4121,
      "training_loss": 5.628452777862549
    },
    {
      "epoch": 0.22341463414634147,
      "step": 4122,
      "training_loss": 7.615406036376953
    },
    {
      "epoch": 0.22346883468834688,
      "step": 4123,
      "training_loss": 6.166704177856445
    },
    {
      "epoch": 0.2235230352303523,
      "grad_norm": 26.216625213623047,
      "learning_rate": 1e-05,
      "loss": 6.5956,
      "step": 4124
    },
    {
      "epoch": 0.2235230352303523,
      "step": 4124,
      "training_loss": 7.321887493133545
    },
    {
      "epoch": 0.22357723577235772,
      "step": 4125,
      "training_loss": 7.339171409606934
    },
    {
      "epoch": 0.22363143631436314,
      "step": 4126,
      "training_loss": 5.212711811065674
    },
    {
      "epoch": 0.22368563685636855,
      "step": 4127,
      "training_loss": 7.325892448425293
    },
    {
      "epoch": 0.223739837398374,
      "grad_norm": 18.301706314086914,
      "learning_rate": 1e-05,
      "loss": 6.7999,
      "step": 4128
    },
    {
      "epoch": 0.223739837398374,
      "step": 4128,
      "training_loss": 7.753897190093994
    },
    {
      "epoch": 0.22379403794037941,
      "step": 4129,
      "training_loss": 8.935059547424316
    },
    {
      "epoch": 0.22384823848238483,
      "step": 4130,
      "training_loss": 7.328277111053467
    },
    {
      "epoch": 0.22390243902439025,
      "step": 4131,
      "training_loss": 6.892980098724365
    },
    {
      "epoch": 0.22395663956639567,
      "grad_norm": 59.344520568847656,
      "learning_rate": 1e-05,
      "loss": 7.7276,
      "step": 4132
    },
    {
      "epoch": 0.22395663956639567,
      "step": 4132,
      "training_loss": 6.024771690368652
    },
    {
      "epoch": 0.22401084010840108,
      "step": 4133,
      "training_loss": 7.470433712005615
    },
    {
      "epoch": 0.2240650406504065,
      "step": 4134,
      "training_loss": 8.064765930175781
    },
    {
      "epoch": 0.22411924119241192,
      "step": 4135,
      "training_loss": 8.091643333435059
    },
    {
      "epoch": 0.22417344173441733,
      "grad_norm": 52.90707015991211,
      "learning_rate": 1e-05,
      "loss": 7.4129,
      "step": 4136
    },
    {
      "epoch": 0.22417344173441733,
      "step": 4136,
      "training_loss": 6.799311637878418
    },
    {
      "epoch": 0.22422764227642275,
      "step": 4137,
      "training_loss": 7.275129318237305
    },
    {
      "epoch": 0.2242818428184282,
      "step": 4138,
      "training_loss": 8.116592407226562
    },
    {
      "epoch": 0.2243360433604336,
      "step": 4139,
      "training_loss": 6.20548152923584
    },
    {
      "epoch": 0.22439024390243903,
      "grad_norm": 24.07093048095703,
      "learning_rate": 1e-05,
      "loss": 7.0991,
      "step": 4140
    },
    {
      "epoch": 0.22439024390243903,
      "step": 4140,
      "training_loss": 7.733544826507568
    },
    {
      "epoch": 0.22444444444444445,
      "step": 4141,
      "training_loss": 6.619667053222656
    },
    {
      "epoch": 0.22449864498644986,
      "step": 4142,
      "training_loss": 6.054275035858154
    },
    {
      "epoch": 0.22455284552845528,
      "step": 4143,
      "training_loss": 6.7412109375
    },
    {
      "epoch": 0.2246070460704607,
      "grad_norm": 16.543994903564453,
      "learning_rate": 1e-05,
      "loss": 6.7872,
      "step": 4144
    },
    {
      "epoch": 0.2246070460704607,
      "step": 4144,
      "training_loss": 4.418519496917725
    },
    {
      "epoch": 0.22466124661246611,
      "step": 4145,
      "training_loss": 6.084198951721191
    },
    {
      "epoch": 0.22471544715447153,
      "step": 4146,
      "training_loss": 6.8445658683776855
    },
    {
      "epoch": 0.22476964769647698,
      "step": 4147,
      "training_loss": 7.1573920249938965
    },
    {
      "epoch": 0.2248238482384824,
      "grad_norm": 20.34365463256836,
      "learning_rate": 1e-05,
      "loss": 6.1262,
      "step": 4148
    },
    {
      "epoch": 0.2248238482384824,
      "step": 4148,
      "training_loss": 7.604632377624512
    },
    {
      "epoch": 0.2248780487804878,
      "step": 4149,
      "training_loss": 7.170168399810791
    },
    {
      "epoch": 0.22493224932249323,
      "step": 4150,
      "training_loss": 11.59896183013916
    },
    {
      "epoch": 0.22498644986449864,
      "step": 4151,
      "training_loss": 5.432461261749268
    },
    {
      "epoch": 0.22504065040650406,
      "grad_norm": 45.05095291137695,
      "learning_rate": 1e-05,
      "loss": 7.9516,
      "step": 4152
    },
    {
      "epoch": 0.22504065040650406,
      "step": 4152,
      "training_loss": 7.285581588745117
    },
    {
      "epoch": 0.22509485094850948,
      "step": 4153,
      "training_loss": 6.110910892486572
    },
    {
      "epoch": 0.2251490514905149,
      "step": 4154,
      "training_loss": 6.8191118240356445
    },
    {
      "epoch": 0.2252032520325203,
      "step": 4155,
      "training_loss": 7.5815839767456055
    },
    {
      "epoch": 0.22525745257452576,
      "grad_norm": 16.72846794128418,
      "learning_rate": 1e-05,
      "loss": 6.9493,
      "step": 4156
    },
    {
      "epoch": 0.22525745257452576,
      "step": 4156,
      "training_loss": 6.840685844421387
    },
    {
      "epoch": 0.22531165311653117,
      "step": 4157,
      "training_loss": 6.894312858581543
    },
    {
      "epoch": 0.2253658536585366,
      "step": 4158,
      "training_loss": 7.066439151763916
    },
    {
      "epoch": 0.225420054200542,
      "step": 4159,
      "training_loss": 6.329594612121582
    },
    {
      "epoch": 0.22547425474254743,
      "grad_norm": 24.23981285095215,
      "learning_rate": 1e-05,
      "loss": 6.7828,
      "step": 4160
    },
    {
      "epoch": 0.22547425474254743,
      "step": 4160,
      "training_loss": 8.1886568069458
    },
    {
      "epoch": 0.22552845528455284,
      "step": 4161,
      "training_loss": 5.553374767303467
    },
    {
      "epoch": 0.22558265582655826,
      "step": 4162,
      "training_loss": 6.8200459480285645
    },
    {
      "epoch": 0.22563685636856368,
      "step": 4163,
      "training_loss": 7.016386985778809
    },
    {
      "epoch": 0.2256910569105691,
      "grad_norm": 24.214054107666016,
      "learning_rate": 1e-05,
      "loss": 6.8946,
      "step": 4164
    },
    {
      "epoch": 0.2256910569105691,
      "step": 4164,
      "training_loss": 5.9942708015441895
    },
    {
      "epoch": 0.22574525745257454,
      "step": 4165,
      "training_loss": 7.201555252075195
    },
    {
      "epoch": 0.22579945799457996,
      "step": 4166,
      "training_loss": 7.0515265464782715
    },
    {
      "epoch": 0.22585365853658537,
      "step": 4167,
      "training_loss": 6.194610595703125
    },
    {
      "epoch": 0.2259078590785908,
      "grad_norm": 16.93990707397461,
      "learning_rate": 1e-05,
      "loss": 6.6105,
      "step": 4168
    },
    {
      "epoch": 0.2259078590785908,
      "step": 4168,
      "training_loss": 6.100837230682373
    },
    {
      "epoch": 0.2259620596205962,
      "step": 4169,
      "training_loss": 7.079367637634277
    },
    {
      "epoch": 0.22601626016260162,
      "step": 4170,
      "training_loss": 6.511838912963867
    },
    {
      "epoch": 0.22607046070460704,
      "step": 4171,
      "training_loss": 7.363991737365723
    },
    {
      "epoch": 0.22612466124661246,
      "grad_norm": 19.55134391784668,
      "learning_rate": 1e-05,
      "loss": 6.764,
      "step": 4172
    },
    {
      "epoch": 0.22612466124661246,
      "step": 4172,
      "training_loss": 6.4290452003479
    },
    {
      "epoch": 0.22617886178861787,
      "step": 4173,
      "training_loss": 6.499814510345459
    },
    {
      "epoch": 0.22623306233062332,
      "step": 4174,
      "training_loss": 4.797901630401611
    },
    {
      "epoch": 0.22628726287262874,
      "step": 4175,
      "training_loss": 7.2690300941467285
    },
    {
      "epoch": 0.22634146341463415,
      "grad_norm": 41.34626388549805,
      "learning_rate": 1e-05,
      "loss": 6.2489,
      "step": 4176
    },
    {
      "epoch": 0.22634146341463415,
      "step": 4176,
      "training_loss": 5.624520301818848
    },
    {
      "epoch": 0.22639566395663957,
      "step": 4177,
      "training_loss": 7.718352317810059
    },
    {
      "epoch": 0.226449864498645,
      "step": 4178,
      "training_loss": 6.857205390930176
    },
    {
      "epoch": 0.2265040650406504,
      "step": 4179,
      "training_loss": 6.757772922515869
    },
    {
      "epoch": 0.22655826558265582,
      "grad_norm": 29.994068145751953,
      "learning_rate": 1e-05,
      "loss": 6.7395,
      "step": 4180
    },
    {
      "epoch": 0.22655826558265582,
      "step": 4180,
      "training_loss": 6.795315742492676
    },
    {
      "epoch": 0.22661246612466124,
      "step": 4181,
      "training_loss": 5.929982662200928
    },
    {
      "epoch": 0.22666666666666666,
      "step": 4182,
      "training_loss": 7.619246482849121
    },
    {
      "epoch": 0.2267208672086721,
      "step": 4183,
      "training_loss": 7.148619174957275
    },
    {
      "epoch": 0.22677506775067752,
      "grad_norm": 17.393035888671875,
      "learning_rate": 1e-05,
      "loss": 6.8733,
      "step": 4184
    },
    {
      "epoch": 0.22677506775067752,
      "step": 4184,
      "training_loss": 4.824831008911133
    },
    {
      "epoch": 0.22682926829268293,
      "step": 4185,
      "training_loss": 7.146673679351807
    },
    {
      "epoch": 0.22688346883468835,
      "step": 4186,
      "training_loss": 6.848747253417969
    },
    {
      "epoch": 0.22693766937669377,
      "step": 4187,
      "training_loss": 7.469182968139648
    },
    {
      "epoch": 0.22699186991869919,
      "grad_norm": 19.43671226501465,
      "learning_rate": 1e-05,
      "loss": 6.5724,
      "step": 4188
    },
    {
      "epoch": 0.22699186991869919,
      "step": 4188,
      "training_loss": 6.738481521606445
    },
    {
      "epoch": 0.2270460704607046,
      "step": 4189,
      "training_loss": 7.04539155960083
    },
    {
      "epoch": 0.22710027100271002,
      "step": 4190,
      "training_loss": 6.7117509841918945
    },
    {
      "epoch": 0.22715447154471544,
      "step": 4191,
      "training_loss": 6.724305629730225
    },
    {
      "epoch": 0.22720867208672088,
      "grad_norm": 31.04790496826172,
      "learning_rate": 1e-05,
      "loss": 6.805,
      "step": 4192
    },
    {
      "epoch": 0.22720867208672088,
      "step": 4192,
      "training_loss": 7.146434783935547
    },
    {
      "epoch": 0.2272628726287263,
      "step": 4193,
      "training_loss": 7.600211143493652
    },
    {
      "epoch": 0.22731707317073171,
      "step": 4194,
      "training_loss": 7.213775157928467
    },
    {
      "epoch": 0.22737127371273713,
      "step": 4195,
      "training_loss": 7.148360252380371
    },
    {
      "epoch": 0.22742547425474255,
      "grad_norm": 24.334524154663086,
      "learning_rate": 1e-05,
      "loss": 7.2772,
      "step": 4196
    },
    {
      "epoch": 0.22742547425474255,
      "step": 4196,
      "training_loss": 6.7052483558654785
    },
    {
      "epoch": 0.22747967479674797,
      "step": 4197,
      "training_loss": 6.868356704711914
    },
    {
      "epoch": 0.22753387533875338,
      "step": 4198,
      "training_loss": 7.4333882331848145
    },
    {
      "epoch": 0.2275880758807588,
      "step": 4199,
      "training_loss": 6.690549373626709
    },
    {
      "epoch": 0.22764227642276422,
      "grad_norm": 40.33662796020508,
      "learning_rate": 1e-05,
      "loss": 6.9244,
      "step": 4200
    },
    {
      "epoch": 0.22764227642276422,
      "step": 4200,
      "training_loss": 8.845355033874512
    },
    {
      "epoch": 0.22769647696476963,
      "step": 4201,
      "training_loss": 5.854869365692139
    },
    {
      "epoch": 0.22775067750677508,
      "step": 4202,
      "training_loss": 6.587584018707275
    },
    {
      "epoch": 0.2278048780487805,
      "step": 4203,
      "training_loss": 7.641119956970215
    },
    {
      "epoch": 0.2278590785907859,
      "grad_norm": 34.47628402709961,
      "learning_rate": 1e-05,
      "loss": 7.2322,
      "step": 4204
    },
    {
      "epoch": 0.2278590785907859,
      "step": 4204,
      "training_loss": 8.692529678344727
    },
    {
      "epoch": 0.22791327913279133,
      "step": 4205,
      "training_loss": 7.199920177459717
    },
    {
      "epoch": 0.22796747967479675,
      "step": 4206,
      "training_loss": 6.365113735198975
    },
    {
      "epoch": 0.22802168021680216,
      "step": 4207,
      "training_loss": 7.408240795135498
    },
    {
      "epoch": 0.22807588075880758,
      "grad_norm": 20.117053985595703,
      "learning_rate": 1e-05,
      "loss": 7.4165,
      "step": 4208
    },
    {
      "epoch": 0.22807588075880758,
      "step": 4208,
      "training_loss": 7.338084697723389
    },
    {
      "epoch": 0.228130081300813,
      "step": 4209,
      "training_loss": 6.603647708892822
    },
    {
      "epoch": 0.22818428184281841,
      "step": 4210,
      "training_loss": 6.460409641265869
    },
    {
      "epoch": 0.22823848238482386,
      "step": 4211,
      "training_loss": 7.310305595397949
    },
    {
      "epoch": 0.22829268292682928,
      "grad_norm": 29.24696922302246,
      "learning_rate": 1e-05,
      "loss": 6.9281,
      "step": 4212
    },
    {
      "epoch": 0.22829268292682928,
      "step": 4212,
      "training_loss": 6.535386562347412
    },
    {
      "epoch": 0.2283468834688347,
      "step": 4213,
      "training_loss": 6.860266208648682
    },
    {
      "epoch": 0.2284010840108401,
      "step": 4214,
      "training_loss": 7.141116142272949
    },
    {
      "epoch": 0.22845528455284553,
      "step": 4215,
      "training_loss": 7.002740383148193
    },
    {
      "epoch": 0.22850948509485094,
      "grad_norm": 31.001296997070312,
      "learning_rate": 1e-05,
      "loss": 6.8849,
      "step": 4216
    },
    {
      "epoch": 0.22850948509485094,
      "step": 4216,
      "training_loss": 6.864316463470459
    },
    {
      "epoch": 0.22856368563685636,
      "step": 4217,
      "training_loss": 7.001038551330566
    },
    {
      "epoch": 0.22861788617886178,
      "step": 4218,
      "training_loss": 5.335446357727051
    },
    {
      "epoch": 0.2286720867208672,
      "step": 4219,
      "training_loss": 6.358856201171875
    },
    {
      "epoch": 0.22872628726287264,
      "grad_norm": 24.977581024169922,
      "learning_rate": 1e-05,
      "loss": 6.3899,
      "step": 4220
    },
    {
      "epoch": 0.22872628726287264,
      "step": 4220,
      "training_loss": 7.6785454750061035
    },
    {
      "epoch": 0.22878048780487806,
      "step": 4221,
      "training_loss": 6.9177327156066895
    },
    {
      "epoch": 0.22883468834688347,
      "step": 4222,
      "training_loss": 7.76539421081543
    },
    {
      "epoch": 0.2288888888888889,
      "step": 4223,
      "training_loss": 7.284115314483643
    },
    {
      "epoch": 0.2289430894308943,
      "grad_norm": 24.794361114501953,
      "learning_rate": 1e-05,
      "loss": 7.4114,
      "step": 4224
    },
    {
      "epoch": 0.2289430894308943,
      "step": 4224,
      "training_loss": 7.619570732116699
    },
    {
      "epoch": 0.22899728997289973,
      "step": 4225,
      "training_loss": 5.207873344421387
    },
    {
      "epoch": 0.22905149051490514,
      "step": 4226,
      "training_loss": 4.472482681274414
    },
    {
      "epoch": 0.22910569105691056,
      "step": 4227,
      "training_loss": 6.558895111083984
    },
    {
      "epoch": 0.22915989159891598,
      "grad_norm": 33.103275299072266,
      "learning_rate": 1e-05,
      "loss": 5.9647,
      "step": 4228
    },
    {
      "epoch": 0.22915989159891598,
      "step": 4228,
      "training_loss": 6.773101329803467
    },
    {
      "epoch": 0.22921409214092142,
      "step": 4229,
      "training_loss": 6.778286457061768
    },
    {
      "epoch": 0.22926829268292684,
      "step": 4230,
      "training_loss": 7.39033842086792
    },
    {
      "epoch": 0.22932249322493226,
      "step": 4231,
      "training_loss": 6.01769495010376
    },
    {
      "epoch": 0.22937669376693767,
      "grad_norm": 25.01999855041504,
      "learning_rate": 1e-05,
      "loss": 6.7399,
      "step": 4232
    },
    {
      "epoch": 0.22937669376693767,
      "step": 4232,
      "training_loss": 8.08940601348877
    },
    {
      "epoch": 0.2294308943089431,
      "step": 4233,
      "training_loss": 7.029899597167969
    },
    {
      "epoch": 0.2294850948509485,
      "step": 4234,
      "training_loss": 4.804824352264404
    },
    {
      "epoch": 0.22953929539295392,
      "step": 4235,
      "training_loss": 6.4619927406311035
    },
    {
      "epoch": 0.22959349593495934,
      "grad_norm": 20.81642723083496,
      "learning_rate": 1e-05,
      "loss": 6.5965,
      "step": 4236
    },
    {
      "epoch": 0.22959349593495934,
      "step": 4236,
      "training_loss": 6.856471061706543
    },
    {
      "epoch": 0.22964769647696476,
      "step": 4237,
      "training_loss": 7.169760227203369
    },
    {
      "epoch": 0.2297018970189702,
      "step": 4238,
      "training_loss": 6.763638019561768
    },
    {
      "epoch": 0.22975609756097562,
      "step": 4239,
      "training_loss": 7.783140659332275
    },
    {
      "epoch": 0.22981029810298104,
      "grad_norm": 25.889514923095703,
      "learning_rate": 1e-05,
      "loss": 7.1433,
      "step": 4240
    },
    {
      "epoch": 0.22981029810298104,
      "step": 4240,
      "training_loss": 6.09543514251709
    },
    {
      "epoch": 0.22986449864498645,
      "step": 4241,
      "training_loss": 5.838420867919922
    },
    {
      "epoch": 0.22991869918699187,
      "step": 4242,
      "training_loss": 6.8158745765686035
    },
    {
      "epoch": 0.2299728997289973,
      "step": 4243,
      "training_loss": 5.121786594390869
    },
    {
      "epoch": 0.2300271002710027,
      "grad_norm": 19.348005294799805,
      "learning_rate": 1e-05,
      "loss": 5.9679,
      "step": 4244
    },
    {
      "epoch": 0.2300271002710027,
      "step": 4244,
      "training_loss": 5.525928020477295
    },
    {
      "epoch": 0.23008130081300812,
      "step": 4245,
      "training_loss": 7.455216407775879
    },
    {
      "epoch": 0.23013550135501354,
      "step": 4246,
      "training_loss": 6.978044033050537
    },
    {
      "epoch": 0.23018970189701898,
      "step": 4247,
      "training_loss": 6.53294038772583
    },
    {
      "epoch": 0.2302439024390244,
      "grad_norm": 18.038639068603516,
      "learning_rate": 1e-05,
      "loss": 6.623,
      "step": 4248
    },
    {
      "epoch": 0.2302439024390244,
      "step": 4248,
      "training_loss": 6.75203800201416
    },
    {
      "epoch": 0.23029810298102982,
      "step": 4249,
      "training_loss": 7.35309362411499
    },
    {
      "epoch": 0.23035230352303523,
      "step": 4250,
      "training_loss": 6.266088485717773
    },
    {
      "epoch": 0.23040650406504065,
      "step": 4251,
      "training_loss": 7.271257400512695
    },
    {
      "epoch": 0.23046070460704607,
      "grad_norm": 21.460468292236328,
      "learning_rate": 1e-05,
      "loss": 6.9106,
      "step": 4252
    },
    {
      "epoch": 0.23046070460704607,
      "step": 4252,
      "training_loss": 7.242105960845947
    },
    {
      "epoch": 0.23051490514905149,
      "step": 4253,
      "training_loss": 7.505939483642578
    },
    {
      "epoch": 0.2305691056910569,
      "step": 4254,
      "training_loss": 7.153629779815674
    },
    {
      "epoch": 0.23062330623306232,
      "step": 4255,
      "training_loss": 5.629159927368164
    },
    {
      "epoch": 0.23067750677506776,
      "grad_norm": 20.276254653930664,
      "learning_rate": 1e-05,
      "loss": 6.8827,
      "step": 4256
    },
    {
      "epoch": 0.23067750677506776,
      "step": 4256,
      "training_loss": 6.505120277404785
    },
    {
      "epoch": 0.23073170731707318,
      "step": 4257,
      "training_loss": 7.813936710357666
    },
    {
      "epoch": 0.2307859078590786,
      "step": 4258,
      "training_loss": 7.301130771636963
    },
    {
      "epoch": 0.23084010840108402,
      "step": 4259,
      "training_loss": 7.717890739440918
    },
    {
      "epoch": 0.23089430894308943,
      "grad_norm": 17.935928344726562,
      "learning_rate": 1e-05,
      "loss": 7.3345,
      "step": 4260
    },
    {
      "epoch": 0.23089430894308943,
      "step": 4260,
      "training_loss": 5.755926132202148
    },
    {
      "epoch": 0.23094850948509485,
      "step": 4261,
      "training_loss": 7.042089939117432
    },
    {
      "epoch": 0.23100271002710027,
      "step": 4262,
      "training_loss": 6.278707027435303
    },
    {
      "epoch": 0.23105691056910568,
      "step": 4263,
      "training_loss": 6.90524435043335
    },
    {
      "epoch": 0.2311111111111111,
      "grad_norm": 14.44227123260498,
      "learning_rate": 1e-05,
      "loss": 6.4955,
      "step": 4264
    },
    {
      "epoch": 0.2311111111111111,
      "step": 4264,
      "training_loss": 7.544012546539307
    },
    {
      "epoch": 0.23116531165311652,
      "step": 4265,
      "training_loss": 7.387238502502441
    },
    {
      "epoch": 0.23121951219512196,
      "step": 4266,
      "training_loss": 8.332276344299316
    },
    {
      "epoch": 0.23127371273712738,
      "step": 4267,
      "training_loss": 7.205306529998779
    },
    {
      "epoch": 0.2313279132791328,
      "grad_norm": 28.627113342285156,
      "learning_rate": 1e-05,
      "loss": 7.6172,
      "step": 4268
    },
    {
      "epoch": 0.2313279132791328,
      "step": 4268,
      "training_loss": 7.700301647186279
    },
    {
      "epoch": 0.2313821138211382,
      "step": 4269,
      "training_loss": 7.115381717681885
    },
    {
      "epoch": 0.23143631436314363,
      "step": 4270,
      "training_loss": 7.281911849975586
    },
    {
      "epoch": 0.23149051490514905,
      "step": 4271,
      "training_loss": 8.2998046875
    },
    {
      "epoch": 0.23154471544715446,
      "grad_norm": 23.420183181762695,
      "learning_rate": 1e-05,
      "loss": 7.5993,
      "step": 4272
    },
    {
      "epoch": 0.23154471544715446,
      "step": 4272,
      "training_loss": 7.440885543823242
    },
    {
      "epoch": 0.23159891598915988,
      "step": 4273,
      "training_loss": 6.438448429107666
    },
    {
      "epoch": 0.2316531165311653,
      "step": 4274,
      "training_loss": 6.482273578643799
    },
    {
      "epoch": 0.23170731707317074,
      "step": 4275,
      "training_loss": 6.972700119018555
    },
    {
      "epoch": 0.23176151761517616,
      "grad_norm": 29.951087951660156,
      "learning_rate": 1e-05,
      "loss": 6.8336,
      "step": 4276
    },
    {
      "epoch": 0.23176151761517616,
      "step": 4276,
      "training_loss": 6.996655464172363
    },
    {
      "epoch": 0.23181571815718158,
      "step": 4277,
      "training_loss": 6.064216613769531
    },
    {
      "epoch": 0.231869918699187,
      "step": 4278,
      "training_loss": 6.33924674987793
    },
    {
      "epoch": 0.2319241192411924,
      "step": 4279,
      "training_loss": 5.169548988342285
    },
    {
      "epoch": 0.23197831978319783,
      "grad_norm": 27.32291030883789,
      "learning_rate": 1e-05,
      "loss": 6.1424,
      "step": 4280
    },
    {
      "epoch": 0.23197831978319783,
      "step": 4280,
      "training_loss": 6.970345973968506
    },
    {
      "epoch": 0.23203252032520325,
      "step": 4281,
      "training_loss": 6.944186210632324
    },
    {
      "epoch": 0.23208672086720866,
      "step": 4282,
      "training_loss": 5.395129203796387
    },
    {
      "epoch": 0.23214092140921408,
      "step": 4283,
      "training_loss": 6.404702186584473
    },
    {
      "epoch": 0.23219512195121952,
      "grad_norm": 23.12282943725586,
      "learning_rate": 1e-05,
      "loss": 6.4286,
      "step": 4284
    },
    {
      "epoch": 0.23219512195121952,
      "step": 4284,
      "training_loss": 5.905412197113037
    },
    {
      "epoch": 0.23224932249322494,
      "step": 4285,
      "training_loss": 6.4146223068237305
    },
    {
      "epoch": 0.23230352303523036,
      "step": 4286,
      "training_loss": 6.402682304382324
    },
    {
      "epoch": 0.23235772357723578,
      "step": 4287,
      "training_loss": 7.227110385894775
    },
    {
      "epoch": 0.2324119241192412,
      "grad_norm": 27.11325454711914,
      "learning_rate": 1e-05,
      "loss": 6.4875,
      "step": 4288
    },
    {
      "epoch": 0.2324119241192412,
      "step": 4288,
      "training_loss": 6.795136451721191
    },
    {
      "epoch": 0.2324661246612466,
      "step": 4289,
      "training_loss": 5.492066860198975
    },
    {
      "epoch": 0.23252032520325203,
      "step": 4290,
      "training_loss": 6.959251880645752
    },
    {
      "epoch": 0.23257452574525744,
      "step": 4291,
      "training_loss": 7.216732501983643
    },
    {
      "epoch": 0.23262872628726286,
      "grad_norm": 32.04901885986328,
      "learning_rate": 1e-05,
      "loss": 6.6158,
      "step": 4292
    },
    {
      "epoch": 0.23262872628726286,
      "step": 4292,
      "training_loss": 8.127777099609375
    },
    {
      "epoch": 0.2326829268292683,
      "step": 4293,
      "training_loss": 6.613431930541992
    },
    {
      "epoch": 0.23273712737127372,
      "step": 4294,
      "training_loss": 7.988693714141846
    },
    {
      "epoch": 0.23279132791327914,
      "step": 4295,
      "training_loss": 6.850008487701416
    },
    {
      "epoch": 0.23284552845528456,
      "grad_norm": 16.248441696166992,
      "learning_rate": 1e-05,
      "loss": 7.395,
      "step": 4296
    },
    {
      "epoch": 0.23284552845528456,
      "step": 4296,
      "training_loss": 6.224175930023193
    },
    {
      "epoch": 0.23289972899728997,
      "step": 4297,
      "training_loss": 7.074512481689453
    },
    {
      "epoch": 0.2329539295392954,
      "step": 4298,
      "training_loss": 4.89918851852417
    },
    {
      "epoch": 0.2330081300813008,
      "step": 4299,
      "training_loss": 7.010467529296875
    },
    {
      "epoch": 0.23306233062330622,
      "grad_norm": 21.734590530395508,
      "learning_rate": 1e-05,
      "loss": 6.3021,
      "step": 4300
    },
    {
      "epoch": 0.23306233062330622,
      "step": 4300,
      "training_loss": 7.06980037689209
    },
    {
      "epoch": 0.23311653116531164,
      "step": 4301,
      "training_loss": 8.148900032043457
    },
    {
      "epoch": 0.23317073170731709,
      "step": 4302,
      "training_loss": 6.709314346313477
    },
    {
      "epoch": 0.2332249322493225,
      "step": 4303,
      "training_loss": 6.533067226409912
    },
    {
      "epoch": 0.23327913279132792,
      "grad_norm": 21.139089584350586,
      "learning_rate": 1e-05,
      "loss": 7.1153,
      "step": 4304
    },
    {
      "epoch": 0.23327913279132792,
      "step": 4304,
      "training_loss": 6.780628204345703
    },
    {
      "epoch": 0.23333333333333334,
      "step": 4305,
      "training_loss": 6.279298305511475
    },
    {
      "epoch": 0.23338753387533875,
      "step": 4306,
      "training_loss": 7.58099889755249
    },
    {
      "epoch": 0.23344173441734417,
      "step": 4307,
      "training_loss": 5.68433952331543
    },
    {
      "epoch": 0.2334959349593496,
      "grad_norm": 21.258888244628906,
      "learning_rate": 1e-05,
      "loss": 6.5813,
      "step": 4308
    },
    {
      "epoch": 0.2334959349593496,
      "step": 4308,
      "training_loss": 7.180887699127197
    },
    {
      "epoch": 0.233550135501355,
      "step": 4309,
      "training_loss": 6.030940055847168
    },
    {
      "epoch": 0.23360433604336042,
      "step": 4310,
      "training_loss": 6.864538669586182
    },
    {
      "epoch": 0.23365853658536587,
      "step": 4311,
      "training_loss": 5.611827373504639
    },
    {
      "epoch": 0.23371273712737128,
      "grad_norm": 26.106382369995117,
      "learning_rate": 1e-05,
      "loss": 6.422,
      "step": 4312
    },
    {
      "epoch": 0.23371273712737128,
      "step": 4312,
      "training_loss": 7.384819507598877
    },
    {
      "epoch": 0.2337669376693767,
      "step": 4313,
      "training_loss": 6.951534271240234
    },
    {
      "epoch": 0.23382113821138212,
      "step": 4314,
      "training_loss": 6.653585433959961
    },
    {
      "epoch": 0.23387533875338753,
      "step": 4315,
      "training_loss": 7.824566841125488
    },
    {
      "epoch": 0.23392953929539295,
      "grad_norm": 29.15214729309082,
      "learning_rate": 1e-05,
      "loss": 7.2036,
      "step": 4316
    },
    {
      "epoch": 0.23392953929539295,
      "step": 4316,
      "training_loss": 6.0780029296875
    },
    {
      "epoch": 0.23398373983739837,
      "step": 4317,
      "training_loss": 5.289434432983398
    },
    {
      "epoch": 0.23403794037940379,
      "step": 4318,
      "training_loss": 6.497130870819092
    },
    {
      "epoch": 0.2340921409214092,
      "step": 4319,
      "training_loss": 6.853566646575928
    },
    {
      "epoch": 0.23414634146341465,
      "grad_norm": 24.81565284729004,
      "learning_rate": 1e-05,
      "loss": 6.1795,
      "step": 4320
    },
    {
      "epoch": 0.23414634146341465,
      "step": 4320,
      "training_loss": 6.213203430175781
    },
    {
      "epoch": 0.23420054200542006,
      "step": 4321,
      "training_loss": 6.970090866088867
    },
    {
      "epoch": 0.23425474254742548,
      "step": 4322,
      "training_loss": 7.083038330078125
    },
    {
      "epoch": 0.2343089430894309,
      "step": 4323,
      "training_loss": 7.925377368927002
    },
    {
      "epoch": 0.23436314363143632,
      "grad_norm": 25.734222412109375,
      "learning_rate": 1e-05,
      "loss": 7.0479,
      "step": 4324
    },
    {
      "epoch": 0.23436314363143632,
      "step": 4324,
      "training_loss": 8.052496910095215
    },
    {
      "epoch": 0.23441734417344173,
      "step": 4325,
      "training_loss": 7.318731307983398
    },
    {
      "epoch": 0.23447154471544715,
      "step": 4326,
      "training_loss": 7.2207350730896
    },
    {
      "epoch": 0.23452574525745257,
      "step": 4327,
      "training_loss": 6.733564853668213
    },
    {
      "epoch": 0.23457994579945798,
      "grad_norm": 18.650577545166016,
      "learning_rate": 1e-05,
      "loss": 7.3314,
      "step": 4328
    },
    {
      "epoch": 0.23457994579945798,
      "step": 4328,
      "training_loss": 6.689419269561768
    },
    {
      "epoch": 0.2346341463414634,
      "step": 4329,
      "training_loss": 7.002178192138672
    },
    {
      "epoch": 0.23468834688346885,
      "step": 4330,
      "training_loss": 7.577493667602539
    },
    {
      "epoch": 0.23474254742547426,
      "step": 4331,
      "training_loss": 7.1103925704956055
    },
    {
      "epoch": 0.23479674796747968,
      "grad_norm": 21.00897979736328,
      "learning_rate": 1e-05,
      "loss": 7.0949,
      "step": 4332
    },
    {
      "epoch": 0.23479674796747968,
      "step": 4332,
      "training_loss": 7.373303413391113
    },
    {
      "epoch": 0.2348509485094851,
      "step": 4333,
      "training_loss": 7.176000118255615
    },
    {
      "epoch": 0.2349051490514905,
      "step": 4334,
      "training_loss": 7.78582239151001
    },
    {
      "epoch": 0.23495934959349593,
      "step": 4335,
      "training_loss": 7.943779468536377
    },
    {
      "epoch": 0.23501355013550135,
      "grad_norm": 16.026491165161133,
      "learning_rate": 1e-05,
      "loss": 7.5697,
      "step": 4336
    },
    {
      "epoch": 0.23501355013550135,
      "step": 4336,
      "training_loss": 6.4475202560424805
    },
    {
      "epoch": 0.23506775067750676,
      "step": 4337,
      "training_loss": 6.2717437744140625
    },
    {
      "epoch": 0.23512195121951218,
      "step": 4338,
      "training_loss": 5.523937702178955
    },
    {
      "epoch": 0.23517615176151763,
      "step": 4339,
      "training_loss": 4.958921909332275
    },
    {
      "epoch": 0.23523035230352304,
      "grad_norm": 19.618003845214844,
      "learning_rate": 1e-05,
      "loss": 5.8005,
      "step": 4340
    },
    {
      "epoch": 0.23523035230352304,
      "step": 4340,
      "training_loss": 6.8440327644348145
    },
    {
      "epoch": 0.23528455284552846,
      "step": 4341,
      "training_loss": 6.567181587219238
    },
    {
      "epoch": 0.23533875338753388,
      "step": 4342,
      "training_loss": 6.764875411987305
    },
    {
      "epoch": 0.2353929539295393,
      "step": 4343,
      "training_loss": 7.353209495544434
    },
    {
      "epoch": 0.2354471544715447,
      "grad_norm": 21.754444122314453,
      "learning_rate": 1e-05,
      "loss": 6.8823,
      "step": 4344
    },
    {
      "epoch": 0.2354471544715447,
      "step": 4344,
      "training_loss": 7.613955020904541
    },
    {
      "epoch": 0.23550135501355013,
      "step": 4345,
      "training_loss": 6.435595512390137
    },
    {
      "epoch": 0.23555555555555555,
      "step": 4346,
      "training_loss": 5.921867370605469
    },
    {
      "epoch": 0.23560975609756096,
      "step": 4347,
      "training_loss": 7.431410789489746
    },
    {
      "epoch": 0.2356639566395664,
      "grad_norm": 23.11284065246582,
      "learning_rate": 1e-05,
      "loss": 6.8507,
      "step": 4348
    },
    {
      "epoch": 0.2356639566395664,
      "step": 4348,
      "training_loss": 7.884244441986084
    },
    {
      "epoch": 0.23571815718157182,
      "step": 4349,
      "training_loss": 7.529660701751709
    },
    {
      "epoch": 0.23577235772357724,
      "step": 4350,
      "training_loss": 7.712798118591309
    },
    {
      "epoch": 0.23582655826558266,
      "step": 4351,
      "training_loss": 6.622200012207031
    },
    {
      "epoch": 0.23588075880758808,
      "grad_norm": 23.998422622680664,
      "learning_rate": 1e-05,
      "loss": 7.4372,
      "step": 4352
    },
    {
      "epoch": 0.23588075880758808,
      "step": 4352,
      "training_loss": 6.048445701599121
    },
    {
      "epoch": 0.2359349593495935,
      "step": 4353,
      "training_loss": 4.9421868324279785
    },
    {
      "epoch": 0.2359891598915989,
      "step": 4354,
      "training_loss": 6.4204792976379395
    },
    {
      "epoch": 0.23604336043360433,
      "step": 4355,
      "training_loss": 7.2341413497924805
    },
    {
      "epoch": 0.23609756097560974,
      "grad_norm": 14.75236988067627,
      "learning_rate": 1e-05,
      "loss": 6.1613,
      "step": 4356
    },
    {
      "epoch": 0.23609756097560974,
      "step": 4356,
      "training_loss": 7.485603332519531
    },
    {
      "epoch": 0.2361517615176152,
      "step": 4357,
      "training_loss": 6.263051509857178
    },
    {
      "epoch": 0.2362059620596206,
      "step": 4358,
      "training_loss": 8.23328685760498
    },
    {
      "epoch": 0.23626016260162602,
      "step": 4359,
      "training_loss": 7.664773941040039
    },
    {
      "epoch": 0.23631436314363144,
      "grad_norm": 24.57575798034668,
      "learning_rate": 1e-05,
      "loss": 7.4117,
      "step": 4360
    },
    {
      "epoch": 0.23631436314363144,
      "step": 4360,
      "training_loss": 8.359538078308105
    },
    {
      "epoch": 0.23636856368563686,
      "step": 4361,
      "training_loss": 7.356449127197266
    },
    {
      "epoch": 0.23642276422764227,
      "step": 4362,
      "training_loss": 6.2679524421691895
    },
    {
      "epoch": 0.2364769647696477,
      "step": 4363,
      "training_loss": 6.665120601654053
    },
    {
      "epoch": 0.2365311653116531,
      "grad_norm": 26.507112503051758,
      "learning_rate": 1e-05,
      "loss": 7.1623,
      "step": 4364
    },
    {
      "epoch": 0.2365311653116531,
      "step": 4364,
      "training_loss": 6.9254536628723145
    },
    {
      "epoch": 0.23658536585365852,
      "step": 4365,
      "training_loss": 6.877685070037842
    },
    {
      "epoch": 0.23663956639566397,
      "step": 4366,
      "training_loss": 7.397954940795898
    },
    {
      "epoch": 0.2366937669376694,
      "step": 4367,
      "training_loss": 7.783182621002197
    },
    {
      "epoch": 0.2367479674796748,
      "grad_norm": 21.223737716674805,
      "learning_rate": 1e-05,
      "loss": 7.2461,
      "step": 4368
    },
    {
      "epoch": 0.2367479674796748,
      "step": 4368,
      "training_loss": 7.627262592315674
    },
    {
      "epoch": 0.23680216802168022,
      "step": 4369,
      "training_loss": 7.412837028503418
    },
    {
      "epoch": 0.23685636856368564,
      "step": 4370,
      "training_loss": 7.338603973388672
    },
    {
      "epoch": 0.23691056910569105,
      "step": 4371,
      "training_loss": 5.434022903442383
    },
    {
      "epoch": 0.23696476964769647,
      "grad_norm": 22.177261352539062,
      "learning_rate": 1e-05,
      "loss": 6.9532,
      "step": 4372
    },
    {
      "epoch": 0.23696476964769647,
      "step": 4372,
      "training_loss": 7.146210193634033
    },
    {
      "epoch": 0.2370189701897019,
      "step": 4373,
      "training_loss": 5.160923004150391
    },
    {
      "epoch": 0.2370731707317073,
      "step": 4374,
      "training_loss": 7.406973838806152
    },
    {
      "epoch": 0.23712737127371275,
      "step": 4375,
      "training_loss": 5.317807674407959
    },
    {
      "epoch": 0.23718157181571817,
      "grad_norm": 53.280521392822266,
      "learning_rate": 1e-05,
      "loss": 6.258,
      "step": 4376
    },
    {
      "epoch": 0.23718157181571817,
      "step": 4376,
      "training_loss": 6.169683933258057
    },
    {
      "epoch": 0.23723577235772358,
      "step": 4377,
      "training_loss": 6.942132472991943
    },
    {
      "epoch": 0.237289972899729,
      "step": 4378,
      "training_loss": 6.819205284118652
    },
    {
      "epoch": 0.23734417344173442,
      "step": 4379,
      "training_loss": 7.4781413078308105
    },
    {
      "epoch": 0.23739837398373984,
      "grad_norm": 23.46261215209961,
      "learning_rate": 1e-05,
      "loss": 6.8523,
      "step": 4380
    },
    {
      "epoch": 0.23739837398373984,
      "step": 4380,
      "training_loss": 6.8815531730651855
    },
    {
      "epoch": 0.23745257452574525,
      "step": 4381,
      "training_loss": 7.065310001373291
    },
    {
      "epoch": 0.23750677506775067,
      "step": 4382,
      "training_loss": 7.8840789794921875
    },
    {
      "epoch": 0.2375609756097561,
      "step": 4383,
      "training_loss": 6.5218729972839355
    },
    {
      "epoch": 0.23761517615176153,
      "grad_norm": 56.67045974731445,
      "learning_rate": 1e-05,
      "loss": 7.0882,
      "step": 4384
    },
    {
      "epoch": 0.23761517615176153,
      "step": 4384,
      "training_loss": 7.386837959289551
    },
    {
      "epoch": 0.23766937669376695,
      "step": 4385,
      "training_loss": 6.462287425994873
    },
    {
      "epoch": 0.23772357723577237,
      "step": 4386,
      "training_loss": 5.866318225860596
    },
    {
      "epoch": 0.23777777777777778,
      "step": 4387,
      "training_loss": 6.120508670806885
    },
    {
      "epoch": 0.2378319783197832,
      "grad_norm": 20.200637817382812,
      "learning_rate": 1e-05,
      "loss": 6.459,
      "step": 4388
    },
    {
      "epoch": 0.2378319783197832,
      "step": 4388,
      "training_loss": 7.125944137573242
    },
    {
      "epoch": 0.23788617886178862,
      "step": 4389,
      "training_loss": 6.393800258636475
    },
    {
      "epoch": 0.23794037940379403,
      "step": 4390,
      "training_loss": 7.4474101066589355
    },
    {
      "epoch": 0.23799457994579945,
      "step": 4391,
      "training_loss": 8.869770050048828
    },
    {
      "epoch": 0.23804878048780487,
      "grad_norm": 24.917264938354492,
      "learning_rate": 1e-05,
      "loss": 7.4592,
      "step": 4392
    },
    {
      "epoch": 0.23804878048780487,
      "step": 4392,
      "training_loss": 7.319926738739014
    },
    {
      "epoch": 0.23810298102981028,
      "step": 4393,
      "training_loss": 6.81741189956665
    },
    {
      "epoch": 0.23815718157181573,
      "step": 4394,
      "training_loss": 7.273848056793213
    },
    {
      "epoch": 0.23821138211382115,
      "step": 4395,
      "training_loss": 6.321357250213623
    },
    {
      "epoch": 0.23826558265582656,
      "grad_norm": 26.68958282470703,
      "learning_rate": 1e-05,
      "loss": 6.9331,
      "step": 4396
    },
    {
      "epoch": 0.23826558265582656,
      "step": 4396,
      "training_loss": 7.1799211502075195
    },
    {
      "epoch": 0.23831978319783198,
      "step": 4397,
      "training_loss": 6.1893439292907715
    },
    {
      "epoch": 0.2383739837398374,
      "step": 4398,
      "training_loss": 6.818233966827393
    },
    {
      "epoch": 0.23842818428184281,
      "step": 4399,
      "training_loss": 7.8539018630981445
    },
    {
      "epoch": 0.23848238482384823,
      "grad_norm": 29.28624153137207,
      "learning_rate": 1e-05,
      "loss": 7.0104,
      "step": 4400
    },
    {
      "epoch": 0.23848238482384823,
      "step": 4400,
      "training_loss": 6.553771495819092
    },
    {
      "epoch": 0.23853658536585365,
      "step": 4401,
      "training_loss": 7.184660911560059
    },
    {
      "epoch": 0.23859078590785907,
      "step": 4402,
      "training_loss": 6.337023735046387
    },
    {
      "epoch": 0.2386449864498645,
      "step": 4403,
      "training_loss": 7.033659934997559
    },
    {
      "epoch": 0.23869918699186993,
      "grad_norm": 17.967388153076172,
      "learning_rate": 1e-05,
      "loss": 6.7773,
      "step": 4404
    },
    {
      "epoch": 0.23869918699186993,
      "step": 4404,
      "training_loss": 7.906445026397705
    },
    {
      "epoch": 0.23875338753387534,
      "step": 4405,
      "training_loss": 7.531702518463135
    },
    {
      "epoch": 0.23880758807588076,
      "step": 4406,
      "training_loss": 6.814608573913574
    },
    {
      "epoch": 0.23886178861788618,
      "step": 4407,
      "training_loss": 7.9899139404296875
    },
    {
      "epoch": 0.2389159891598916,
      "grad_norm": 17.45176887512207,
      "learning_rate": 1e-05,
      "loss": 7.5607,
      "step": 4408
    },
    {
      "epoch": 0.2389159891598916,
      "step": 4408,
      "training_loss": 4.271721839904785
    },
    {
      "epoch": 0.238970189701897,
      "step": 4409,
      "training_loss": 7.046867847442627
    },
    {
      "epoch": 0.23902439024390243,
      "step": 4410,
      "training_loss": 6.892483711242676
    },
    {
      "epoch": 0.23907859078590785,
      "step": 4411,
      "training_loss": 7.4565019607543945
    },
    {
      "epoch": 0.2391327913279133,
      "grad_norm": 19.478090286254883,
      "learning_rate": 1e-05,
      "loss": 6.4169,
      "step": 4412
    },
    {
      "epoch": 0.2391327913279133,
      "step": 4412,
      "training_loss": 6.644140720367432
    },
    {
      "epoch": 0.2391869918699187,
      "step": 4413,
      "training_loss": 7.1792426109313965
    },
    {
      "epoch": 0.23924119241192413,
      "step": 4414,
      "training_loss": 7.179388523101807
    },
    {
      "epoch": 0.23929539295392954,
      "step": 4415,
      "training_loss": 6.593448162078857
    },
    {
      "epoch": 0.23934959349593496,
      "grad_norm": 41.471309661865234,
      "learning_rate": 1e-05,
      "loss": 6.8991,
      "step": 4416
    },
    {
      "epoch": 0.23934959349593496,
      "step": 4416,
      "training_loss": 7.330967426300049
    },
    {
      "epoch": 0.23940379403794038,
      "step": 4417,
      "training_loss": 6.526386260986328
    },
    {
      "epoch": 0.2394579945799458,
      "step": 4418,
      "training_loss": 7.2749481201171875
    },
    {
      "epoch": 0.2395121951219512,
      "step": 4419,
      "training_loss": 6.88484525680542
    },
    {
      "epoch": 0.23956639566395663,
      "grad_norm": 24.583946228027344,
      "learning_rate": 1e-05,
      "loss": 7.0043,
      "step": 4420
    },
    {
      "epoch": 0.23956639566395663,
      "step": 4420,
      "training_loss": 7.430469512939453
    },
    {
      "epoch": 0.23962059620596207,
      "step": 4421,
      "training_loss": 6.7609968185424805
    },
    {
      "epoch": 0.2396747967479675,
      "step": 4422,
      "training_loss": 5.931519508361816
    },
    {
      "epoch": 0.2397289972899729,
      "step": 4423,
      "training_loss": 6.582832336425781
    },
    {
      "epoch": 0.23978319783197832,
      "grad_norm": 27.119464874267578,
      "learning_rate": 1e-05,
      "loss": 6.6765,
      "step": 4424
    },
    {
      "epoch": 0.23978319783197832,
      "step": 4424,
      "training_loss": 7.351327896118164
    },
    {
      "epoch": 0.23983739837398374,
      "step": 4425,
      "training_loss": 7.715068817138672
    },
    {
      "epoch": 0.23989159891598916,
      "step": 4426,
      "training_loss": 6.197259902954102
    },
    {
      "epoch": 0.23994579945799457,
      "step": 4427,
      "training_loss": 6.558686256408691
    },
    {
      "epoch": 0.24,
      "grad_norm": 18.540584564208984,
      "learning_rate": 1e-05,
      "loss": 6.9556,
      "step": 4428
    },
    {
      "epoch": 0.24,
      "step": 4428,
      "training_loss": 5.920137405395508
    },
    {
      "epoch": 0.2400542005420054,
      "step": 4429,
      "training_loss": 5.918654441833496
    },
    {
      "epoch": 0.24010840108401085,
      "step": 4430,
      "training_loss": 7.253365993499756
    },
    {
      "epoch": 0.24016260162601627,
      "step": 4431,
      "training_loss": 6.813449382781982
    },
    {
      "epoch": 0.2402168021680217,
      "grad_norm": 19.185752868652344,
      "learning_rate": 1e-05,
      "loss": 6.4764,
      "step": 4432
    },
    {
      "epoch": 0.2402168021680217,
      "step": 4432,
      "training_loss": 4.627323150634766
    },
    {
      "epoch": 0.2402710027100271,
      "step": 4433,
      "training_loss": 6.682458400726318
    },
    {
      "epoch": 0.24032520325203252,
      "step": 4434,
      "training_loss": 7.5768141746521
    },
    {
      "epoch": 0.24037940379403794,
      "step": 4435,
      "training_loss": 7.143962383270264
    },
    {
      "epoch": 0.24043360433604336,
      "grad_norm": 23.54817008972168,
      "learning_rate": 1e-05,
      "loss": 6.5076,
      "step": 4436
    },
    {
      "epoch": 0.24043360433604336,
      "step": 4436,
      "training_loss": 7.684629917144775
    },
    {
      "epoch": 0.24048780487804877,
      "step": 4437,
      "training_loss": 8.65621280670166
    },
    {
      "epoch": 0.2405420054200542,
      "step": 4438,
      "training_loss": 6.6017913818359375
    },
    {
      "epoch": 0.24059620596205963,
      "step": 4439,
      "training_loss": 7.6429338455200195
    },
    {
      "epoch": 0.24065040650406505,
      "grad_norm": 16.749080657958984,
      "learning_rate": 1e-05,
      "loss": 7.6464,
      "step": 4440
    },
    {
      "epoch": 0.24065040650406505,
      "step": 4440,
      "training_loss": 7.656450271606445
    },
    {
      "epoch": 0.24070460704607047,
      "step": 4441,
      "training_loss": 11.895524024963379
    },
    {
      "epoch": 0.24075880758807588,
      "step": 4442,
      "training_loss": 7.372775077819824
    },
    {
      "epoch": 0.2408130081300813,
      "step": 4443,
      "training_loss": 6.2542219161987305
    },
    {
      "epoch": 0.24086720867208672,
      "grad_norm": 21.486587524414062,
      "learning_rate": 1e-05,
      "loss": 8.2947,
      "step": 4444
    },
    {
      "epoch": 0.24086720867208672,
      "step": 4444,
      "training_loss": 5.528926372528076
    },
    {
      "epoch": 0.24092140921409214,
      "step": 4445,
      "training_loss": 7.64396858215332
    },
    {
      "epoch": 0.24097560975609755,
      "step": 4446,
      "training_loss": 6.5410332679748535
    },
    {
      "epoch": 0.24102981029810297,
      "step": 4447,
      "training_loss": 7.622759819030762
    },
    {
      "epoch": 0.24108401084010841,
      "grad_norm": 24.22739028930664,
      "learning_rate": 1e-05,
      "loss": 6.8342,
      "step": 4448
    },
    {
      "epoch": 0.24108401084010841,
      "step": 4448,
      "training_loss": 5.470860481262207
    },
    {
      "epoch": 0.24113821138211383,
      "step": 4449,
      "training_loss": 4.471888542175293
    },
    {
      "epoch": 0.24119241192411925,
      "step": 4450,
      "training_loss": 7.626521587371826
    },
    {
      "epoch": 0.24124661246612467,
      "step": 4451,
      "training_loss": 7.541464805603027
    },
    {
      "epoch": 0.24130081300813008,
      "grad_norm": 19.522865295410156,
      "learning_rate": 1e-05,
      "loss": 6.2777,
      "step": 4452
    },
    {
      "epoch": 0.24130081300813008,
      "step": 4452,
      "training_loss": 6.877355098724365
    },
    {
      "epoch": 0.2413550135501355,
      "step": 4453,
      "training_loss": 6.489975929260254
    },
    {
      "epoch": 0.24140921409214092,
      "step": 4454,
      "training_loss": 7.96221399307251
    },
    {
      "epoch": 0.24146341463414633,
      "step": 4455,
      "training_loss": 6.278899669647217
    },
    {
      "epoch": 0.24151761517615175,
      "grad_norm": 22.807878494262695,
      "learning_rate": 1e-05,
      "loss": 6.9021,
      "step": 4456
    },
    {
      "epoch": 0.24151761517615175,
      "step": 4456,
      "training_loss": 6.624127388000488
    },
    {
      "epoch": 0.24157181571815717,
      "step": 4457,
      "training_loss": 7.170284748077393
    },
    {
      "epoch": 0.2416260162601626,
      "step": 4458,
      "training_loss": 7.182557582855225
    },
    {
      "epoch": 0.24168021680216803,
      "step": 4459,
      "training_loss": 7.689818382263184
    },
    {
      "epoch": 0.24173441734417345,
      "grad_norm": 27.096721649169922,
      "learning_rate": 1e-05,
      "loss": 7.1667,
      "step": 4460
    },
    {
      "epoch": 0.24173441734417345,
      "step": 4460,
      "training_loss": 8.005291938781738
    },
    {
      "epoch": 0.24178861788617886,
      "step": 4461,
      "training_loss": 6.566344738006592
    },
    {
      "epoch": 0.24184281842818428,
      "step": 4462,
      "training_loss": 6.099650859832764
    },
    {
      "epoch": 0.2418970189701897,
      "step": 4463,
      "training_loss": 5.755043983459473
    },
    {
      "epoch": 0.24195121951219511,
      "grad_norm": 18.951732635498047,
      "learning_rate": 1e-05,
      "loss": 6.6066,
      "step": 4464
    },
    {
      "epoch": 0.24195121951219511,
      "step": 4464,
      "training_loss": 7.081868648529053
    },
    {
      "epoch": 0.24200542005420053,
      "step": 4465,
      "training_loss": 5.666322231292725
    },
    {
      "epoch": 0.24205962059620595,
      "step": 4466,
      "training_loss": 6.484644412994385
    },
    {
      "epoch": 0.2421138211382114,
      "step": 4467,
      "training_loss": 7.312374114990234
    },
    {
      "epoch": 0.2421680216802168,
      "grad_norm": 27.77928352355957,
      "learning_rate": 1e-05,
      "loss": 6.6363,
      "step": 4468
    },
    {
      "epoch": 0.2421680216802168,
      "step": 4468,
      "training_loss": 7.13197135925293
    },
    {
      "epoch": 0.24222222222222223,
      "step": 4469,
      "training_loss": 7.156121730804443
    },
    {
      "epoch": 0.24227642276422764,
      "step": 4470,
      "training_loss": 6.726797103881836
    },
    {
      "epoch": 0.24233062330623306,
      "step": 4471,
      "training_loss": 7.277840614318848
    },
    {
      "epoch": 0.24238482384823848,
      "grad_norm": 18.644439697265625,
      "learning_rate": 1e-05,
      "loss": 7.0732,
      "step": 4472
    },
    {
      "epoch": 0.24238482384823848,
      "step": 4472,
      "training_loss": 6.4107513427734375
    },
    {
      "epoch": 0.2424390243902439,
      "step": 4473,
      "training_loss": 5.821068286895752
    },
    {
      "epoch": 0.2424932249322493,
      "step": 4474,
      "training_loss": 6.864126682281494
    },
    {
      "epoch": 0.24254742547425473,
      "step": 4475,
      "training_loss": 5.94859504699707
    },
    {
      "epoch": 0.24260162601626017,
      "grad_norm": 61.698448181152344,
      "learning_rate": 1e-05,
      "loss": 6.2611,
      "step": 4476
    },
    {
      "epoch": 0.24260162601626017,
      "step": 4476,
      "training_loss": 4.097338676452637
    },
    {
      "epoch": 0.2426558265582656,
      "step": 4477,
      "training_loss": 5.324566841125488
    },
    {
      "epoch": 0.242710027100271,
      "step": 4478,
      "training_loss": 8.010733604431152
    },
    {
      "epoch": 0.24276422764227643,
      "step": 4479,
      "training_loss": 7.303486347198486
    },
    {
      "epoch": 0.24281842818428184,
      "grad_norm": 18.445661544799805,
      "learning_rate": 1e-05,
      "loss": 6.184,
      "step": 4480
    },
    {
      "epoch": 0.24281842818428184,
      "step": 4480,
      "training_loss": 6.996105194091797
    },
    {
      "epoch": 0.24287262872628726,
      "step": 4481,
      "training_loss": 6.765478610992432
    },
    {
      "epoch": 0.24292682926829268,
      "step": 4482,
      "training_loss": 6.995742321014404
    },
    {
      "epoch": 0.2429810298102981,
      "step": 4483,
      "training_loss": 6.537970066070557
    },
    {
      "epoch": 0.2430352303523035,
      "grad_norm": 21.474321365356445,
      "learning_rate": 1e-05,
      "loss": 6.8238,
      "step": 4484
    },
    {
      "epoch": 0.2430352303523035,
      "step": 4484,
      "training_loss": 6.791171550750732
    },
    {
      "epoch": 0.24308943089430896,
      "step": 4485,
      "training_loss": 6.430607318878174
    },
    {
      "epoch": 0.24314363143631437,
      "step": 4486,
      "training_loss": 7.579246997833252
    },
    {
      "epoch": 0.2431978319783198,
      "step": 4487,
      "training_loss": 7.821079254150391
    },
    {
      "epoch": 0.2432520325203252,
      "grad_norm": 28.302509307861328,
      "learning_rate": 1e-05,
      "loss": 7.1555,
      "step": 4488
    },
    {
      "epoch": 0.2432520325203252,
      "step": 4488,
      "training_loss": 7.388105392456055
    },
    {
      "epoch": 0.24330623306233062,
      "step": 4489,
      "training_loss": 4.482167720794678
    },
    {
      "epoch": 0.24336043360433604,
      "step": 4490,
      "training_loss": 6.577086925506592
    },
    {
      "epoch": 0.24341463414634146,
      "step": 4491,
      "training_loss": 7.3038434982299805
    },
    {
      "epoch": 0.24346883468834687,
      "grad_norm": 28.162363052368164,
      "learning_rate": 1e-05,
      "loss": 6.4378,
      "step": 4492
    },
    {
      "epoch": 0.24346883468834687,
      "step": 4492,
      "training_loss": 7.181783199310303
    },
    {
      "epoch": 0.2435230352303523,
      "step": 4493,
      "training_loss": 5.787737846374512
    },
    {
      "epoch": 0.24357723577235774,
      "step": 4494,
      "training_loss": 5.429065227508545
    },
    {
      "epoch": 0.24363143631436315,
      "step": 4495,
      "training_loss": 8.203770637512207
    },
    {
      "epoch": 0.24368563685636857,
      "grad_norm": 16.483322143554688,
      "learning_rate": 1e-05,
      "loss": 6.6506,
      "step": 4496
    },
    {
      "epoch": 0.24368563685636857,
      "step": 4496,
      "training_loss": 5.569873332977295
    },
    {
      "epoch": 0.243739837398374,
      "step": 4497,
      "training_loss": 6.18034553527832
    },
    {
      "epoch": 0.2437940379403794,
      "step": 4498,
      "training_loss": 7.83945894241333
    },
    {
      "epoch": 0.24384823848238482,
      "step": 4499,
      "training_loss": 7.164546012878418
    },
    {
      "epoch": 0.24390243902439024,
      "grad_norm": 41.87200927734375,
      "learning_rate": 1e-05,
      "loss": 6.6886,
      "step": 4500
    },
    {
      "epoch": 0.24390243902439024,
      "step": 4500,
      "training_loss": 7.404544830322266
    },
    {
      "epoch": 0.24395663956639566,
      "step": 4501,
      "training_loss": 7.072329998016357
    },
    {
      "epoch": 0.24401084010840107,
      "step": 4502,
      "training_loss": 5.771177291870117
    },
    {
      "epoch": 0.24406504065040652,
      "step": 4503,
      "training_loss": 8.163029670715332
    },
    {
      "epoch": 0.24411924119241193,
      "grad_norm": 36.2460823059082,
      "learning_rate": 1e-05,
      "loss": 7.1028,
      "step": 4504
    },
    {
      "epoch": 0.24411924119241193,
      "step": 4504,
      "training_loss": 5.552910804748535
    },
    {
      "epoch": 0.24417344173441735,
      "step": 4505,
      "training_loss": 6.729031562805176
    },
    {
      "epoch": 0.24422764227642277,
      "step": 4506,
      "training_loss": 7.443583011627197
    },
    {
      "epoch": 0.24428184281842819,
      "step": 4507,
      "training_loss": 7.498842716217041
    },
    {
      "epoch": 0.2443360433604336,
      "grad_norm": 20.700660705566406,
      "learning_rate": 1e-05,
      "loss": 6.8061,
      "step": 4508
    },
    {
      "epoch": 0.2443360433604336,
      "step": 4508,
      "training_loss": 7.813329219818115
    },
    {
      "epoch": 0.24439024390243902,
      "step": 4509,
      "training_loss": 5.609825611114502
    },
    {
      "epoch": 0.24444444444444444,
      "step": 4510,
      "training_loss": 6.069540500640869
    },
    {
      "epoch": 0.24449864498644985,
      "step": 4511,
      "training_loss": 7.16575288772583
    },
    {
      "epoch": 0.2445528455284553,
      "grad_norm": 21.64506721496582,
      "learning_rate": 1e-05,
      "loss": 6.6646,
      "step": 4512
    },
    {
      "epoch": 0.2445528455284553,
      "step": 4512,
      "training_loss": 5.135182857513428
    },
    {
      "epoch": 0.24460704607046072,
      "step": 4513,
      "training_loss": 6.346367359161377
    },
    {
      "epoch": 0.24466124661246613,
      "step": 4514,
      "training_loss": 7.509317398071289
    },
    {
      "epoch": 0.24471544715447155,
      "step": 4515,
      "training_loss": 6.550971508026123
    },
    {
      "epoch": 0.24476964769647697,
      "grad_norm": 20.282211303710938,
      "learning_rate": 1e-05,
      "loss": 6.3855,
      "step": 4516
    },
    {
      "epoch": 0.24476964769647697,
      "step": 4516,
      "training_loss": 8.341081619262695
    },
    {
      "epoch": 0.24482384823848238,
      "step": 4517,
      "training_loss": 6.284140110015869
    },
    {
      "epoch": 0.2448780487804878,
      "step": 4518,
      "training_loss": 4.640686511993408
    },
    {
      "epoch": 0.24493224932249322,
      "step": 4519,
      "training_loss": 4.598402976989746
    },
    {
      "epoch": 0.24498644986449863,
      "grad_norm": 26.01274299621582,
      "learning_rate": 1e-05,
      "loss": 5.9661,
      "step": 4520
    },
    {
      "epoch": 0.24498644986449863,
      "step": 4520,
      "training_loss": 6.042552471160889
    },
    {
      "epoch": 0.24504065040650405,
      "step": 4521,
      "training_loss": 6.234198570251465
    },
    {
      "epoch": 0.2450948509485095,
      "step": 4522,
      "training_loss": 7.533639907836914
    },
    {
      "epoch": 0.2451490514905149,
      "step": 4523,
      "training_loss": 7.145086288452148
    },
    {
      "epoch": 0.24520325203252033,
      "grad_norm": 16.01547622680664,
      "learning_rate": 1e-05,
      "loss": 6.7389,
      "step": 4524
    },
    {
      "epoch": 0.24520325203252033,
      "step": 4524,
      "training_loss": 3.889970064163208
    },
    {
      "epoch": 0.24525745257452575,
      "step": 4525,
      "training_loss": 6.775411128997803
    },
    {
      "epoch": 0.24531165311653116,
      "step": 4526,
      "training_loss": 7.044589996337891
    },
    {
      "epoch": 0.24536585365853658,
      "step": 4527,
      "training_loss": 7.3013434410095215
    },
    {
      "epoch": 0.245420054200542,
      "grad_norm": 15.554230690002441,
      "learning_rate": 1e-05,
      "loss": 6.2528,
      "step": 4528
    },
    {
      "epoch": 0.245420054200542,
      "step": 4528,
      "training_loss": 6.514624118804932
    },
    {
      "epoch": 0.24547425474254742,
      "step": 4529,
      "training_loss": 6.318849086761475
    },
    {
      "epoch": 0.24552845528455283,
      "step": 4530,
      "training_loss": 7.1814374923706055
    },
    {
      "epoch": 0.24558265582655828,
      "step": 4531,
      "training_loss": 7.397181987762451
    },
    {
      "epoch": 0.2456368563685637,
      "grad_norm": 16.00885009765625,
      "learning_rate": 1e-05,
      "loss": 6.853,
      "step": 4532
    },
    {
      "epoch": 0.2456368563685637,
      "step": 4532,
      "training_loss": 6.759978294372559
    },
    {
      "epoch": 0.2456910569105691,
      "step": 4533,
      "training_loss": 6.4628682136535645
    },
    {
      "epoch": 0.24574525745257453,
      "step": 4534,
      "training_loss": 5.1588969230651855
    },
    {
      "epoch": 0.24579945799457995,
      "step": 4535,
      "training_loss": 6.574718475341797
    },
    {
      "epoch": 0.24585365853658536,
      "grad_norm": 15.954670906066895,
      "learning_rate": 1e-05,
      "loss": 6.2391,
      "step": 4536
    },
    {
      "epoch": 0.24585365853658536,
      "step": 4536,
      "training_loss": 7.6104326248168945
    },
    {
      "epoch": 0.24590785907859078,
      "step": 4537,
      "training_loss": 7.137988090515137
    },
    {
      "epoch": 0.2459620596205962,
      "step": 4538,
      "training_loss": 7.186797142028809
    },
    {
      "epoch": 0.2460162601626016,
      "step": 4539,
      "training_loss": 7.093160629272461
    },
    {
      "epoch": 0.24607046070460706,
      "grad_norm": 22.790943145751953,
      "learning_rate": 1e-05,
      "loss": 7.2571,
      "step": 4540
    },
    {
      "epoch": 0.24607046070460706,
      "step": 4540,
      "training_loss": 7.440145969390869
    },
    {
      "epoch": 0.24612466124661248,
      "step": 4541,
      "training_loss": 6.835419178009033
    },
    {
      "epoch": 0.2461788617886179,
      "step": 4542,
      "training_loss": 7.7702741622924805
    },
    {
      "epoch": 0.2462330623306233,
      "step": 4543,
      "training_loss": 6.990754127502441
    },
    {
      "epoch": 0.24628726287262873,
      "grad_norm": 20.287410736083984,
      "learning_rate": 1e-05,
      "loss": 7.2591,
      "step": 4544
    },
    {
      "epoch": 0.24628726287262873,
      "step": 4544,
      "training_loss": 7.494838714599609
    },
    {
      "epoch": 0.24634146341463414,
      "step": 4545,
      "training_loss": 5.750039577484131
    },
    {
      "epoch": 0.24639566395663956,
      "step": 4546,
      "training_loss": 7.869279861450195
    },
    {
      "epoch": 0.24644986449864498,
      "step": 4547,
      "training_loss": 5.887230396270752
    },
    {
      "epoch": 0.2465040650406504,
      "grad_norm": 18.272802352905273,
      "learning_rate": 1e-05,
      "loss": 6.7503,
      "step": 4548
    },
    {
      "epoch": 0.2465040650406504,
      "step": 4548,
      "training_loss": 7.684829235076904
    },
    {
      "epoch": 0.24655826558265584,
      "step": 4549,
      "training_loss": 5.7760748863220215
    },
    {
      "epoch": 0.24661246612466126,
      "step": 4550,
      "training_loss": 5.750380039215088
    },
    {
      "epoch": 0.24666666666666667,
      "step": 4551,
      "training_loss": 6.819571495056152
    },
    {
      "epoch": 0.2467208672086721,
      "grad_norm": 24.018138885498047,
      "learning_rate": 1e-05,
      "loss": 6.5077,
      "step": 4552
    },
    {
      "epoch": 0.2467208672086721,
      "step": 4552,
      "training_loss": 6.890917778015137
    },
    {
      "epoch": 0.2467750677506775,
      "step": 4553,
      "training_loss": 7.182192802429199
    },
    {
      "epoch": 0.24682926829268292,
      "step": 4554,
      "training_loss": 6.704033374786377
    },
    {
      "epoch": 0.24688346883468834,
      "step": 4555,
      "training_loss": 6.5764007568359375
    },
    {
      "epoch": 0.24693766937669376,
      "grad_norm": 22.37562370300293,
      "learning_rate": 1e-05,
      "loss": 6.8384,
      "step": 4556
    },
    {
      "epoch": 0.24693766937669376,
      "step": 4556,
      "training_loss": 7.25836706161499
    },
    {
      "epoch": 0.24699186991869918,
      "step": 4557,
      "training_loss": 6.968473434448242
    },
    {
      "epoch": 0.24704607046070462,
      "step": 4558,
      "training_loss": 7.078373908996582
    },
    {
      "epoch": 0.24710027100271004,
      "step": 4559,
      "training_loss": 7.788518905639648
    },
    {
      "epoch": 0.24715447154471545,
      "grad_norm": 23.589799880981445,
      "learning_rate": 1e-05,
      "loss": 7.2734,
      "step": 4560
    },
    {
      "epoch": 0.24715447154471545,
      "step": 4560,
      "training_loss": 6.329687595367432
    },
    {
      "epoch": 0.24720867208672087,
      "step": 4561,
      "training_loss": 6.079620361328125
    },
    {
      "epoch": 0.2472628726287263,
      "step": 4562,
      "training_loss": 7.68562650680542
    },
    {
      "epoch": 0.2473170731707317,
      "step": 4563,
      "training_loss": 6.342005729675293
    },
    {
      "epoch": 0.24737127371273712,
      "grad_norm": 30.73809814453125,
      "learning_rate": 1e-05,
      "loss": 6.6092,
      "step": 4564
    },
    {
      "epoch": 0.24737127371273712,
      "step": 4564,
      "training_loss": 7.128328323364258
    },
    {
      "epoch": 0.24742547425474254,
      "step": 4565,
      "training_loss": 7.136937618255615
    },
    {
      "epoch": 0.24747967479674796,
      "step": 4566,
      "training_loss": 7.031460762023926
    },
    {
      "epoch": 0.2475338753387534,
      "step": 4567,
      "training_loss": 7.506978988647461
    },
    {
      "epoch": 0.24758807588075882,
      "grad_norm": 16.014278411865234,
      "learning_rate": 1e-05,
      "loss": 7.2009,
      "step": 4568
    },
    {
      "epoch": 0.24758807588075882,
      "step": 4568,
      "training_loss": 6.431039333343506
    },
    {
      "epoch": 0.24764227642276423,
      "step": 4569,
      "training_loss": 6.409792423248291
    },
    {
      "epoch": 0.24769647696476965,
      "step": 4570,
      "training_loss": 6.8296380043029785
    },
    {
      "epoch": 0.24775067750677507,
      "step": 4571,
      "training_loss": 6.548471927642822
    },
    {
      "epoch": 0.24780487804878049,
      "grad_norm": 19.125904083251953,
      "learning_rate": 1e-05,
      "loss": 6.5547,
      "step": 4572
    },
    {
      "epoch": 0.24780487804878049,
      "step": 4572,
      "training_loss": 6.9530930519104
    },
    {
      "epoch": 0.2478590785907859,
      "step": 4573,
      "training_loss": 7.27553129196167
    },
    {
      "epoch": 0.24791327913279132,
      "step": 4574,
      "training_loss": 6.520015716552734
    },
    {
      "epoch": 0.24796747967479674,
      "step": 4575,
      "training_loss": 7.90704345703125
    },
    {
      "epoch": 0.24802168021680218,
      "grad_norm": 17.368438720703125,
      "learning_rate": 1e-05,
      "loss": 7.1639,
      "step": 4576
    },
    {
      "epoch": 0.24802168021680218,
      "step": 4576,
      "training_loss": 6.481507778167725
    },
    {
      "epoch": 0.2480758807588076,
      "step": 4577,
      "training_loss": 8.354305267333984
    },
    {
      "epoch": 0.24813008130081302,
      "step": 4578,
      "training_loss": 5.429924488067627
    },
    {
      "epoch": 0.24818428184281843,
      "step": 4579,
      "training_loss": 7.604311466217041
    },
    {
      "epoch": 0.24823848238482385,
      "grad_norm": 20.3966007232666,
      "learning_rate": 1e-05,
      "loss": 6.9675,
      "step": 4580
    },
    {
      "epoch": 0.24823848238482385,
      "step": 4580,
      "training_loss": 6.9965291023254395
    },
    {
      "epoch": 0.24829268292682927,
      "step": 4581,
      "training_loss": 8.372895240783691
    },
    {
      "epoch": 0.24834688346883468,
      "step": 4582,
      "training_loss": 6.701887130737305
    },
    {
      "epoch": 0.2484010840108401,
      "step": 4583,
      "training_loss": 4.441675186157227
    },
    {
      "epoch": 0.24845528455284552,
      "grad_norm": 31.714881896972656,
      "learning_rate": 1e-05,
      "loss": 6.6282,
      "step": 4584
    },
    {
      "epoch": 0.24845528455284552,
      "step": 4584,
      "training_loss": 5.69799280166626
    },
    {
      "epoch": 0.24850948509485093,
      "step": 4585,
      "training_loss": 4.320451259613037
    },
    {
      "epoch": 0.24856368563685638,
      "step": 4586,
      "training_loss": 7.557811260223389
    },
    {
      "epoch": 0.2486178861788618,
      "step": 4587,
      "training_loss": 6.94527006149292
    },
    {
      "epoch": 0.2486720867208672,
      "grad_norm": 21.169239044189453,
      "learning_rate": 1e-05,
      "loss": 6.1304,
      "step": 4588
    },
    {
      "epoch": 0.2486720867208672,
      "step": 4588,
      "training_loss": 7.605837821960449
    },
    {
      "epoch": 0.24872628726287263,
      "step": 4589,
      "training_loss": 7.093896389007568
    },
    {
      "epoch": 0.24878048780487805,
      "step": 4590,
      "training_loss": 8.130249977111816
    },
    {
      "epoch": 0.24883468834688346,
      "step": 4591,
      "training_loss": 7.392412185668945
    },
    {
      "epoch": 0.24888888888888888,
      "grad_norm": 27.44292449951172,
      "learning_rate": 1e-05,
      "loss": 7.5556,
      "step": 4592
    },
    {
      "epoch": 0.24888888888888888,
      "step": 4592,
      "training_loss": 4.4372382164001465
    },
    {
      "epoch": 0.2489430894308943,
      "step": 4593,
      "training_loss": 7.897927284240723
    },
    {
      "epoch": 0.24899728997289972,
      "step": 4594,
      "training_loss": 7.018857955932617
    },
    {
      "epoch": 0.24905149051490516,
      "step": 4595,
      "training_loss": 7.199654579162598
    },
    {
      "epoch": 0.24910569105691058,
      "grad_norm": 24.1024112701416,
      "learning_rate": 1e-05,
      "loss": 6.6384,
      "step": 4596
    },
    {
      "epoch": 0.24910569105691058,
      "step": 4596,
      "training_loss": 7.103385925292969
    },
    {
      "epoch": 0.249159891598916,
      "step": 4597,
      "training_loss": 7.209924221038818
    },
    {
      "epoch": 0.2492140921409214,
      "step": 4598,
      "training_loss": 7.0638203620910645
    },
    {
      "epoch": 0.24926829268292683,
      "step": 4599,
      "training_loss": 7.41127347946167
    },
    {
      "epoch": 0.24932249322493225,
      "grad_norm": 22.770994186401367,
      "learning_rate": 1e-05,
      "loss": 7.1971,
      "step": 4600
    },
    {
      "epoch": 0.24932249322493225,
      "step": 4600,
      "training_loss": 6.04990816116333
    },
    {
      "epoch": 0.24937669376693766,
      "step": 4601,
      "training_loss": 4.893494606018066
    },
    {
      "epoch": 0.24943089430894308,
      "step": 4602,
      "training_loss": 5.409361839294434
    },
    {
      "epoch": 0.2494850948509485,
      "step": 4603,
      "training_loss": 6.867082118988037
    },
    {
      "epoch": 0.24953929539295394,
      "grad_norm": 18.806711196899414,
      "learning_rate": 1e-05,
      "loss": 5.805,
      "step": 4604
    },
    {
      "epoch": 0.24953929539295394,
      "step": 4604,
      "training_loss": 7.8328022956848145
    },
    {
      "epoch": 0.24959349593495936,
      "step": 4605,
      "training_loss": 7.59335994720459
    },
    {
      "epoch": 0.24964769647696478,
      "step": 4606,
      "training_loss": 6.457448482513428
    },
    {
      "epoch": 0.2497018970189702,
      "step": 4607,
      "training_loss": 7.907302379608154
    },
    {
      "epoch": 0.2497560975609756,
      "grad_norm": 41.81088638305664,
      "learning_rate": 1e-05,
      "loss": 7.4477,
      "step": 4608
    },
    {
      "epoch": 0.2497560975609756,
      "step": 4608,
      "training_loss": 6.2744669914245605
    },
    {
      "epoch": 0.24981029810298103,
      "step": 4609,
      "training_loss": 7.387753963470459
    },
    {
      "epoch": 0.24986449864498644,
      "step": 4610,
      "training_loss": 6.343862056732178
    },
    {
      "epoch": 0.24991869918699186,
      "step": 4611,
      "training_loss": 7.204009532928467
    },
    {
      "epoch": 0.24997289972899728,
      "grad_norm": 37.41841125488281,
      "learning_rate": 1e-05,
      "loss": 6.8025,
      "step": 4612
    },
    {
      "epoch": 0.24997289972899728,
      "step": 4612,
      "training_loss": 7.245847225189209
    },
    {
      "epoch": 0.2500271002710027,
      "step": 4613,
      "training_loss": 6.64047384262085
    },
    {
      "epoch": 0.2500813008130081,
      "step": 4614,
      "training_loss": 8.074647903442383
    },
    {
      "epoch": 0.25013550135501356,
      "step": 4615,
      "training_loss": 6.702983379364014
    },
    {
      "epoch": 0.25018970189701895,
      "grad_norm": 23.12486457824707,
      "learning_rate": 1e-05,
      "loss": 7.166,
      "step": 4616
    },
    {
      "epoch": 0.25018970189701895,
      "step": 4616,
      "training_loss": 6.4627685546875
    },
    {
      "epoch": 0.2502439024390244,
      "step": 4617,
      "training_loss": 5.977531433105469
    },
    {
      "epoch": 0.25029810298102984,
      "step": 4618,
      "training_loss": 7.506042003631592
    },
    {
      "epoch": 0.2503523035230352,
      "step": 4619,
      "training_loss": 6.419808387756348
    },
    {
      "epoch": 0.25040650406504067,
      "grad_norm": 82.96723175048828,
      "learning_rate": 1e-05,
      "loss": 6.5915,
      "step": 4620
    },
    {
      "epoch": 0.25040650406504067,
      "step": 4620,
      "training_loss": 6.733415603637695
    },
    {
      "epoch": 0.25046070460704606,
      "step": 4621,
      "training_loss": 6.802484035491943
    },
    {
      "epoch": 0.2505149051490515,
      "step": 4622,
      "training_loss": 8.205282211303711
    },
    {
      "epoch": 0.2505691056910569,
      "step": 4623,
      "training_loss": 5.581820964813232
    },
    {
      "epoch": 0.25062330623306234,
      "grad_norm": 26.381759643554688,
      "learning_rate": 1e-05,
      "loss": 6.8308,
      "step": 4624
    },
    {
      "epoch": 0.25062330623306234,
      "step": 4624,
      "training_loss": 4.65245246887207
    },
    {
      "epoch": 0.2506775067750677,
      "step": 4625,
      "training_loss": 7.4611663818359375
    },
    {
      "epoch": 0.25073170731707317,
      "step": 4626,
      "training_loss": 5.341056823730469
    },
    {
      "epoch": 0.2507859078590786,
      "step": 4627,
      "training_loss": 5.817070960998535
    },
    {
      "epoch": 0.250840108401084,
      "grad_norm": 20.74981117248535,
      "learning_rate": 1e-05,
      "loss": 5.8179,
      "step": 4628
    },
    {
      "epoch": 0.250840108401084,
      "step": 4628,
      "training_loss": 6.704030513763428
    },
    {
      "epoch": 0.25089430894308945,
      "step": 4629,
      "training_loss": 6.240573406219482
    },
    {
      "epoch": 0.25094850948509484,
      "step": 4630,
      "training_loss": 6.458856105804443
    },
    {
      "epoch": 0.2510027100271003,
      "step": 4631,
      "training_loss": 7.970684051513672
    },
    {
      "epoch": 0.2510569105691057,
      "grad_norm": 37.188724517822266,
      "learning_rate": 1e-05,
      "loss": 6.8435,
      "step": 4632
    },
    {
      "epoch": 0.2510569105691057,
      "step": 4632,
      "training_loss": 6.27128791809082
    },
    {
      "epoch": 0.2511111111111111,
      "step": 4633,
      "training_loss": 7.226425647735596
    },
    {
      "epoch": 0.2511653116531165,
      "step": 4634,
      "training_loss": 7.015195846557617
    },
    {
      "epoch": 0.25121951219512195,
      "step": 4635,
      "training_loss": 7.166135311126709
    },
    {
      "epoch": 0.2512737127371274,
      "grad_norm": 40.6094970703125,
      "learning_rate": 1e-05,
      "loss": 6.9198,
      "step": 4636
    },
    {
      "epoch": 0.2512737127371274,
      "step": 4636,
      "training_loss": 6.859950065612793
    },
    {
      "epoch": 0.2513279132791328,
      "step": 4637,
      "training_loss": 8.376907348632812
    },
    {
      "epoch": 0.25138211382113823,
      "step": 4638,
      "training_loss": 7.321602821350098
    },
    {
      "epoch": 0.2514363143631436,
      "step": 4639,
      "training_loss": 7.25193452835083
    },
    {
      "epoch": 0.25149051490514907,
      "grad_norm": 21.85409927368164,
      "learning_rate": 1e-05,
      "loss": 7.4526,
      "step": 4640
    },
    {
      "epoch": 0.25149051490514907,
      "step": 4640,
      "training_loss": 6.823029041290283
    },
    {
      "epoch": 0.25154471544715445,
      "step": 4641,
      "training_loss": 6.952274322509766
    },
    {
      "epoch": 0.2515989159891599,
      "step": 4642,
      "training_loss": 6.236060619354248
    },
    {
      "epoch": 0.2516531165311653,
      "step": 4643,
      "training_loss": 6.918301582336426
    },
    {
      "epoch": 0.25170731707317073,
      "grad_norm": 24.01155662536621,
      "learning_rate": 1e-05,
      "loss": 6.7324,
      "step": 4644
    },
    {
      "epoch": 0.25170731707317073,
      "step": 4644,
      "training_loss": 6.819944381713867
    },
    {
      "epoch": 0.2517615176151762,
      "step": 4645,
      "training_loss": 7.387278079986572
    },
    {
      "epoch": 0.25181571815718157,
      "step": 4646,
      "training_loss": 7.952418327331543
    },
    {
      "epoch": 0.251869918699187,
      "step": 4647,
      "training_loss": 7.534212112426758
    },
    {
      "epoch": 0.2519241192411924,
      "grad_norm": 26.80008316040039,
      "learning_rate": 1e-05,
      "loss": 7.4235,
      "step": 4648
    },
    {
      "epoch": 0.2519241192411924,
      "step": 4648,
      "training_loss": 5.65468168258667
    },
    {
      "epoch": 0.25197831978319785,
      "step": 4649,
      "training_loss": 4.691615104675293
    },
    {
      "epoch": 0.25203252032520324,
      "step": 4650,
      "training_loss": 8.580548286437988
    },
    {
      "epoch": 0.2520867208672087,
      "step": 4651,
      "training_loss": 7.169849872589111
    },
    {
      "epoch": 0.25214092140921407,
      "grad_norm": 27.231306076049805,
      "learning_rate": 1e-05,
      "loss": 6.5242,
      "step": 4652
    },
    {
      "epoch": 0.25214092140921407,
      "step": 4652,
      "training_loss": 6.833973407745361
    },
    {
      "epoch": 0.2521951219512195,
      "step": 4653,
      "training_loss": 5.172091960906982
    },
    {
      "epoch": 0.25224932249322496,
      "step": 4654,
      "training_loss": 6.423611164093018
    },
    {
      "epoch": 0.25230352303523035,
      "step": 4655,
      "training_loss": 7.619742393493652
    },
    {
      "epoch": 0.2523577235772358,
      "grad_norm": 36.05659484863281,
      "learning_rate": 1e-05,
      "loss": 6.5124,
      "step": 4656
    },
    {
      "epoch": 0.2523577235772358,
      "step": 4656,
      "training_loss": 6.813578128814697
    },
    {
      "epoch": 0.2524119241192412,
      "step": 4657,
      "training_loss": 7.633707523345947
    },
    {
      "epoch": 0.2524661246612466,
      "step": 4658,
      "training_loss": 6.711893558502197
    },
    {
      "epoch": 0.252520325203252,
      "step": 4659,
      "training_loss": 6.7235236167907715
    },
    {
      "epoch": 0.25257452574525746,
      "grad_norm": 35.05952835083008,
      "learning_rate": 1e-05,
      "loss": 6.9707,
      "step": 4660
    },
    {
      "epoch": 0.25257452574525746,
      "step": 4660,
      "training_loss": 6.633430480957031
    },
    {
      "epoch": 0.25262872628726285,
      "step": 4661,
      "training_loss": 5.657050609588623
    },
    {
      "epoch": 0.2526829268292683,
      "step": 4662,
      "training_loss": 7.177713394165039
    },
    {
      "epoch": 0.25273712737127374,
      "step": 4663,
      "training_loss": 7.371050834655762
    },
    {
      "epoch": 0.25279132791327913,
      "grad_norm": 16.79178237915039,
      "learning_rate": 1e-05,
      "loss": 6.7098,
      "step": 4664
    },
    {
      "epoch": 0.25279132791327913,
      "step": 4664,
      "training_loss": 7.751221656799316
    },
    {
      "epoch": 0.2528455284552846,
      "step": 4665,
      "training_loss": 6.124966621398926
    },
    {
      "epoch": 0.25289972899728996,
      "step": 4666,
      "training_loss": 6.888044834136963
    },
    {
      "epoch": 0.2529539295392954,
      "step": 4667,
      "training_loss": 7.544567584991455
    },
    {
      "epoch": 0.2530081300813008,
      "grad_norm": 32.01638412475586,
      "learning_rate": 1e-05,
      "loss": 7.0772,
      "step": 4668
    },
    {
      "epoch": 0.2530081300813008,
      "step": 4668,
      "training_loss": 6.50324010848999
    },
    {
      "epoch": 0.25306233062330624,
      "step": 4669,
      "training_loss": 11.355605125427246
    },
    {
      "epoch": 0.25311653116531163,
      "step": 4670,
      "training_loss": 7.093403339385986
    },
    {
      "epoch": 0.2531707317073171,
      "step": 4671,
      "training_loss": 7.122182846069336
    },
    {
      "epoch": 0.2532249322493225,
      "grad_norm": 23.703500747680664,
      "learning_rate": 1e-05,
      "loss": 8.0186,
      "step": 4672
    },
    {
      "epoch": 0.2532249322493225,
      "step": 4672,
      "training_loss": 7.069399356842041
    },
    {
      "epoch": 0.2532791327913279,
      "step": 4673,
      "training_loss": 6.60746431350708
    },
    {
      "epoch": 0.25333333333333335,
      "step": 4674,
      "training_loss": 7.07022762298584
    },
    {
      "epoch": 0.25338753387533874,
      "step": 4675,
      "training_loss": 7.068127632141113
    },
    {
      "epoch": 0.2534417344173442,
      "grad_norm": 22.649747848510742,
      "learning_rate": 1e-05,
      "loss": 6.9538,
      "step": 4676
    },
    {
      "epoch": 0.2534417344173442,
      "step": 4676,
      "training_loss": 6.763116359710693
    },
    {
      "epoch": 0.2534959349593496,
      "step": 4677,
      "training_loss": 7.2142133712768555
    },
    {
      "epoch": 0.253550135501355,
      "step": 4678,
      "training_loss": 6.372584819793701
    },
    {
      "epoch": 0.2536043360433604,
      "step": 4679,
      "training_loss": 7.094247341156006
    },
    {
      "epoch": 0.25365853658536586,
      "grad_norm": 19.366981506347656,
      "learning_rate": 1e-05,
      "loss": 6.861,
      "step": 4680
    },
    {
      "epoch": 0.25365853658536586,
      "step": 4680,
      "training_loss": 6.370521068572998
    },
    {
      "epoch": 0.25371273712737125,
      "step": 4681,
      "training_loss": 6.98710298538208
    },
    {
      "epoch": 0.2537669376693767,
      "step": 4682,
      "training_loss": 4.873260974884033
    },
    {
      "epoch": 0.25382113821138214,
      "step": 4683,
      "training_loss": 5.733206748962402
    },
    {
      "epoch": 0.2538753387533875,
      "grad_norm": 22.866430282592773,
      "learning_rate": 1e-05,
      "loss": 5.991,
      "step": 4684
    },
    {
      "epoch": 0.2538753387533875,
      "step": 4684,
      "training_loss": 6.733104228973389
    },
    {
      "epoch": 0.25392953929539297,
      "step": 4685,
      "training_loss": 9.011604309082031
    },
    {
      "epoch": 0.25398373983739836,
      "step": 4686,
      "training_loss": 6.0508904457092285
    },
    {
      "epoch": 0.2540379403794038,
      "step": 4687,
      "training_loss": 7.450942516326904
    },
    {
      "epoch": 0.2540921409214092,
      "grad_norm": 21.511890411376953,
      "learning_rate": 1e-05,
      "loss": 7.3116,
      "step": 4688
    },
    {
      "epoch": 0.2540921409214092,
      "step": 4688,
      "training_loss": 7.3003926277160645
    },
    {
      "epoch": 0.25414634146341464,
      "step": 4689,
      "training_loss": 6.508411407470703
    },
    {
      "epoch": 0.25420054200542,
      "step": 4690,
      "training_loss": 7.240006923675537
    },
    {
      "epoch": 0.25425474254742547,
      "step": 4691,
      "training_loss": 7.517923355102539
    },
    {
      "epoch": 0.2543089430894309,
      "grad_norm": 25.021516799926758,
      "learning_rate": 1e-05,
      "loss": 7.1417,
      "step": 4692
    },
    {
      "epoch": 0.2543089430894309,
      "step": 4692,
      "training_loss": 4.939881324768066
    },
    {
      "epoch": 0.2543631436314363,
      "step": 4693,
      "training_loss": 7.399707317352295
    },
    {
      "epoch": 0.25441734417344175,
      "step": 4694,
      "training_loss": 6.037477493286133
    },
    {
      "epoch": 0.25447154471544714,
      "step": 4695,
      "training_loss": 7.461540222167969
    },
    {
      "epoch": 0.2545257452574526,
      "grad_norm": 16.980327606201172,
      "learning_rate": 1e-05,
      "loss": 6.4597,
      "step": 4696
    },
    {
      "epoch": 0.2545257452574526,
      "step": 4696,
      "training_loss": 7.288647174835205
    },
    {
      "epoch": 0.254579945799458,
      "step": 4697,
      "training_loss": 7.834609031677246
    },
    {
      "epoch": 0.2546341463414634,
      "step": 4698,
      "training_loss": 7.004611015319824
    },
    {
      "epoch": 0.2546883468834688,
      "step": 4699,
      "training_loss": 5.878342151641846
    },
    {
      "epoch": 0.25474254742547425,
      "grad_norm": 23.178443908691406,
      "learning_rate": 1e-05,
      "loss": 7.0016,
      "step": 4700
    },
    {
      "epoch": 0.25474254742547425,
      "step": 4700,
      "training_loss": 6.801117420196533
    },
    {
      "epoch": 0.2547967479674797,
      "step": 4701,
      "training_loss": 7.014598369598389
    },
    {
      "epoch": 0.2548509485094851,
      "step": 4702,
      "training_loss": 6.798978328704834
    },
    {
      "epoch": 0.25490514905149053,
      "step": 4703,
      "training_loss": 6.22695779800415
    },
    {
      "epoch": 0.2549593495934959,
      "grad_norm": 20.569637298583984,
      "learning_rate": 1e-05,
      "loss": 6.7104,
      "step": 4704
    },
    {
      "epoch": 0.2549593495934959,
      "step": 4704,
      "training_loss": 7.608172416687012
    },
    {
      "epoch": 0.25501355013550137,
      "step": 4705,
      "training_loss": 7.65728759765625
    },
    {
      "epoch": 0.25506775067750675,
      "step": 4706,
      "training_loss": 6.621956825256348
    },
    {
      "epoch": 0.2551219512195122,
      "step": 4707,
      "training_loss": 6.784362316131592
    },
    {
      "epoch": 0.2551761517615176,
      "grad_norm": 33.51100158691406,
      "learning_rate": 1e-05,
      "loss": 7.1679,
      "step": 4708
    },
    {
      "epoch": 0.2551761517615176,
      "step": 4708,
      "training_loss": 6.368015766143799
    },
    {
      "epoch": 0.25523035230352303,
      "step": 4709,
      "training_loss": 7.996453285217285
    },
    {
      "epoch": 0.2552845528455285,
      "step": 4710,
      "training_loss": 6.792501926422119
    },
    {
      "epoch": 0.25533875338753387,
      "step": 4711,
      "training_loss": 5.792491436004639
    },
    {
      "epoch": 0.2553929539295393,
      "grad_norm": 29.033891677856445,
      "learning_rate": 1e-05,
      "loss": 6.7374,
      "step": 4712
    },
    {
      "epoch": 0.2553929539295393,
      "step": 4712,
      "training_loss": 7.160445213317871
    },
    {
      "epoch": 0.2554471544715447,
      "step": 4713,
      "training_loss": 6.366353511810303
    },
    {
      "epoch": 0.25550135501355015,
      "step": 4714,
      "training_loss": 7.013277530670166
    },
    {
      "epoch": 0.25555555555555554,
      "step": 4715,
      "training_loss": 7.063840389251709
    },
    {
      "epoch": 0.255609756097561,
      "grad_norm": 18.709552764892578,
      "learning_rate": 1e-05,
      "loss": 6.901,
      "step": 4716
    },
    {
      "epoch": 0.255609756097561,
      "step": 4716,
      "training_loss": 8.570260047912598
    },
    {
      "epoch": 0.25566395663956637,
      "step": 4717,
      "training_loss": 6.790302276611328
    },
    {
      "epoch": 0.2557181571815718,
      "step": 4718,
      "training_loss": 7.396368026733398
    },
    {
      "epoch": 0.25577235772357726,
      "step": 4719,
      "training_loss": 8.204614639282227
    },
    {
      "epoch": 0.25582655826558265,
      "grad_norm": 47.08232879638672,
      "learning_rate": 1e-05,
      "loss": 7.7404,
      "step": 4720
    },
    {
      "epoch": 0.25582655826558265,
      "step": 4720,
      "training_loss": 5.876017093658447
    },
    {
      "epoch": 0.2558807588075881,
      "step": 4721,
      "training_loss": 6.935137748718262
    },
    {
      "epoch": 0.2559349593495935,
      "step": 4722,
      "training_loss": 7.8045878410339355
    },
    {
      "epoch": 0.2559891598915989,
      "step": 4723,
      "training_loss": 6.336191654205322
    },
    {
      "epoch": 0.2560433604336043,
      "grad_norm": 20.57781219482422,
      "learning_rate": 1e-05,
      "loss": 6.738,
      "step": 4724
    },
    {
      "epoch": 0.2560433604336043,
      "step": 4724,
      "training_loss": 7.884202480316162
    },
    {
      "epoch": 0.25609756097560976,
      "step": 4725,
      "training_loss": 7.122417449951172
    },
    {
      "epoch": 0.25615176151761515,
      "step": 4726,
      "training_loss": 6.764018535614014
    },
    {
      "epoch": 0.2562059620596206,
      "step": 4727,
      "training_loss": 7.1979851722717285
    },
    {
      "epoch": 0.25626016260162604,
      "grad_norm": 16.583280563354492,
      "learning_rate": 1e-05,
      "loss": 7.2422,
      "step": 4728
    },
    {
      "epoch": 0.25626016260162604,
      "step": 4728,
      "training_loss": 7.041479587554932
    },
    {
      "epoch": 0.25631436314363143,
      "step": 4729,
      "training_loss": 7.053718090057373
    },
    {
      "epoch": 0.2563685636856369,
      "step": 4730,
      "training_loss": 6.89370584487915
    },
    {
      "epoch": 0.25642276422764226,
      "step": 4731,
      "training_loss": 7.866270542144775
    },
    {
      "epoch": 0.2564769647696477,
      "grad_norm": 30.00738525390625,
      "learning_rate": 1e-05,
      "loss": 7.2138,
      "step": 4732
    },
    {
      "epoch": 0.2564769647696477,
      "step": 4732,
      "training_loss": 4.692131519317627
    },
    {
      "epoch": 0.2565311653116531,
      "step": 4733,
      "training_loss": 8.504878997802734
    },
    {
      "epoch": 0.25658536585365854,
      "step": 4734,
      "training_loss": 8.180495262145996
    },
    {
      "epoch": 0.25663956639566393,
      "step": 4735,
      "training_loss": 6.914626598358154
    },
    {
      "epoch": 0.2566937669376694,
      "grad_norm": 23.290536880493164,
      "learning_rate": 1e-05,
      "loss": 7.073,
      "step": 4736
    },
    {
      "epoch": 0.2566937669376694,
      "step": 4736,
      "training_loss": 7.005627155303955
    },
    {
      "epoch": 0.2567479674796748,
      "step": 4737,
      "training_loss": 8.017702102661133
    },
    {
      "epoch": 0.2568021680216802,
      "step": 4738,
      "training_loss": 7.499058723449707
    },
    {
      "epoch": 0.25685636856368566,
      "step": 4739,
      "training_loss": 6.350644111633301
    },
    {
      "epoch": 0.25691056910569104,
      "grad_norm": 16.757465362548828,
      "learning_rate": 1e-05,
      "loss": 7.2183,
      "step": 4740
    },
    {
      "epoch": 0.25691056910569104,
      "step": 4740,
      "training_loss": 6.397661209106445
    },
    {
      "epoch": 0.2569647696476965,
      "step": 4741,
      "training_loss": 6.939577579498291
    },
    {
      "epoch": 0.2570189701897019,
      "step": 4742,
      "training_loss": 6.013177394866943
    },
    {
      "epoch": 0.2570731707317073,
      "step": 4743,
      "training_loss": 5.072497844696045
    },
    {
      "epoch": 0.2571273712737127,
      "grad_norm": 16.83380699157715,
      "learning_rate": 1e-05,
      "loss": 6.1057,
      "step": 4744
    },
    {
      "epoch": 0.2571273712737127,
      "step": 4744,
      "training_loss": 3.6848556995391846
    },
    {
      "epoch": 0.25718157181571816,
      "step": 4745,
      "training_loss": 5.314875602722168
    },
    {
      "epoch": 0.2572357723577236,
      "step": 4746,
      "training_loss": 7.135481357574463
    },
    {
      "epoch": 0.257289972899729,
      "step": 4747,
      "training_loss": 5.012190818786621
    },
    {
      "epoch": 0.25734417344173444,
      "grad_norm": 21.291763305664062,
      "learning_rate": 1e-05,
      "loss": 5.2869,
      "step": 4748
    },
    {
      "epoch": 0.25734417344173444,
      "step": 4748,
      "training_loss": 6.821893692016602
    },
    {
      "epoch": 0.2573983739837398,
      "step": 4749,
      "training_loss": 6.750768184661865
    },
    {
      "epoch": 0.25745257452574527,
      "step": 4750,
      "training_loss": 8.09833812713623
    },
    {
      "epoch": 0.25750677506775066,
      "step": 4751,
      "training_loss": 7.114133834838867
    },
    {
      "epoch": 0.2575609756097561,
      "grad_norm": 48.92202377319336,
      "learning_rate": 1e-05,
      "loss": 7.1963,
      "step": 4752
    },
    {
      "epoch": 0.2575609756097561,
      "step": 4752,
      "training_loss": 6.211601257324219
    },
    {
      "epoch": 0.2576151761517615,
      "step": 4753,
      "training_loss": 7.121237754821777
    },
    {
      "epoch": 0.25766937669376694,
      "step": 4754,
      "training_loss": 6.11627197265625
    },
    {
      "epoch": 0.2577235772357724,
      "step": 4755,
      "training_loss": 6.1874680519104
    },
    {
      "epoch": 0.2577777777777778,
      "grad_norm": 23.005651473999023,
      "learning_rate": 1e-05,
      "loss": 6.4091,
      "step": 4756
    },
    {
      "epoch": 0.2577777777777778,
      "step": 4756,
      "training_loss": 6.434803009033203
    },
    {
      "epoch": 0.2578319783197832,
      "step": 4757,
      "training_loss": 5.556206703186035
    },
    {
      "epoch": 0.2578861788617886,
      "step": 4758,
      "training_loss": 4.186215877532959
    },
    {
      "epoch": 0.25794037940379405,
      "step": 4759,
      "training_loss": 4.125746726989746
    },
    {
      "epoch": 0.25799457994579944,
      "grad_norm": 37.234100341796875,
      "learning_rate": 1e-05,
      "loss": 5.0757,
      "step": 4760
    },
    {
      "epoch": 0.25799457994579944,
      "step": 4760,
      "training_loss": 7.370724201202393
    },
    {
      "epoch": 0.2580487804878049,
      "step": 4761,
      "training_loss": 7.147902488708496
    },
    {
      "epoch": 0.2581029810298103,
      "step": 4762,
      "training_loss": 8.05123519897461
    },
    {
      "epoch": 0.2581571815718157,
      "step": 4763,
      "training_loss": 6.469763278961182
    },
    {
      "epoch": 0.25821138211382116,
      "grad_norm": 27.233915328979492,
      "learning_rate": 1e-05,
      "loss": 7.2599,
      "step": 4764
    },
    {
      "epoch": 0.25821138211382116,
      "step": 4764,
      "training_loss": 6.541208267211914
    },
    {
      "epoch": 0.25826558265582655,
      "step": 4765,
      "training_loss": 7.979613304138184
    },
    {
      "epoch": 0.258319783197832,
      "step": 4766,
      "training_loss": 8.0649995803833
    },
    {
      "epoch": 0.2583739837398374,
      "step": 4767,
      "training_loss": 7.176191329956055
    },
    {
      "epoch": 0.25842818428184283,
      "grad_norm": 29.848480224609375,
      "learning_rate": 1e-05,
      "loss": 7.4405,
      "step": 4768
    },
    {
      "epoch": 0.25842818428184283,
      "step": 4768,
      "training_loss": 6.6219587326049805
    },
    {
      "epoch": 0.2584823848238482,
      "step": 4769,
      "training_loss": 7.738934516906738
    },
    {
      "epoch": 0.25853658536585367,
      "step": 4770,
      "training_loss": 6.82429313659668
    },
    {
      "epoch": 0.25859078590785906,
      "step": 4771,
      "training_loss": 8.224771499633789
    },
    {
      "epoch": 0.2586449864498645,
      "grad_norm": 25.921215057373047,
      "learning_rate": 1e-05,
      "loss": 7.3525,
      "step": 4772
    },
    {
      "epoch": 0.2586449864498645,
      "step": 4772,
      "training_loss": 6.676982402801514
    },
    {
      "epoch": 0.25869918699186994,
      "step": 4773,
      "training_loss": 6.622319221496582
    },
    {
      "epoch": 0.25875338753387533,
      "step": 4774,
      "training_loss": 6.78183126449585
    },
    {
      "epoch": 0.2588075880758808,
      "step": 4775,
      "training_loss": 6.8583149909973145
    },
    {
      "epoch": 0.25886178861788617,
      "grad_norm": 24.91596031188965,
      "learning_rate": 1e-05,
      "loss": 6.7349,
      "step": 4776
    },
    {
      "epoch": 0.25886178861788617,
      "step": 4776,
      "training_loss": 8.347890853881836
    },
    {
      "epoch": 0.2589159891598916,
      "step": 4777,
      "training_loss": 5.515162467956543
    },
    {
      "epoch": 0.258970189701897,
      "step": 4778,
      "training_loss": 6.924659252166748
    },
    {
      "epoch": 0.25902439024390245,
      "step": 4779,
      "training_loss": 6.916841506958008
    },
    {
      "epoch": 0.25907859078590784,
      "grad_norm": 15.96618366241455,
      "learning_rate": 1e-05,
      "loss": 6.9261,
      "step": 4780
    },
    {
      "epoch": 0.25907859078590784,
      "step": 4780,
      "training_loss": 6.75818395614624
    },
    {
      "epoch": 0.2591327913279133,
      "step": 4781,
      "training_loss": 7.585190773010254
    },
    {
      "epoch": 0.2591869918699187,
      "step": 4782,
      "training_loss": 6.595373630523682
    },
    {
      "epoch": 0.2592411924119241,
      "step": 4783,
      "training_loss": 5.693598747253418
    },
    {
      "epoch": 0.25929539295392956,
      "grad_norm": 24.765138626098633,
      "learning_rate": 1e-05,
      "loss": 6.6581,
      "step": 4784
    },
    {
      "epoch": 0.25929539295392956,
      "step": 4784,
      "training_loss": 6.746213912963867
    },
    {
      "epoch": 0.25934959349593495,
      "step": 4785,
      "training_loss": 6.618460655212402
    },
    {
      "epoch": 0.2594037940379404,
      "step": 4786,
      "training_loss": 6.4064555168151855
    },
    {
      "epoch": 0.2594579945799458,
      "step": 4787,
      "training_loss": 7.501226425170898
    },
    {
      "epoch": 0.25951219512195123,
      "grad_norm": 22.372682571411133,
      "learning_rate": 1e-05,
      "loss": 6.8181,
      "step": 4788
    },
    {
      "epoch": 0.25951219512195123,
      "step": 4788,
      "training_loss": 6.6543779373168945
    },
    {
      "epoch": 0.2595663956639566,
      "step": 4789,
      "training_loss": 6.815804958343506
    },
    {
      "epoch": 0.25962059620596206,
      "step": 4790,
      "training_loss": 7.315384387969971
    },
    {
      "epoch": 0.2596747967479675,
      "step": 4791,
      "training_loss": 6.02483606338501
    },
    {
      "epoch": 0.2597289972899729,
      "grad_norm": 30.280776977539062,
      "learning_rate": 1e-05,
      "loss": 6.7026,
      "step": 4792
    },
    {
      "epoch": 0.2597289972899729,
      "step": 4792,
      "training_loss": 7.538172721862793
    },
    {
      "epoch": 0.25978319783197834,
      "step": 4793,
      "training_loss": 6.887235164642334
    },
    {
      "epoch": 0.25983739837398373,
      "step": 4794,
      "training_loss": 6.96810245513916
    },
    {
      "epoch": 0.2598915989159892,
      "step": 4795,
      "training_loss": 6.018955707550049
    },
    {
      "epoch": 0.25994579945799456,
      "grad_norm": 24.2818660736084,
      "learning_rate": 1e-05,
      "loss": 6.8531,
      "step": 4796
    },
    {
      "epoch": 0.25994579945799456,
      "step": 4796,
      "training_loss": 7.401679515838623
    },
    {
      "epoch": 0.26,
      "step": 4797,
      "training_loss": 7.00376558303833
    },
    {
      "epoch": 0.2600542005420054,
      "step": 4798,
      "training_loss": 7.407041072845459
    },
    {
      "epoch": 0.26010840108401084,
      "step": 4799,
      "training_loss": 5.800042629241943
    },
    {
      "epoch": 0.2601626016260163,
      "grad_norm": 29.239648818969727,
      "learning_rate": 1e-05,
      "loss": 6.9031,
      "step": 4800
    },
    {
      "epoch": 0.2601626016260163,
      "step": 4800,
      "training_loss": 7.03473424911499
    },
    {
      "epoch": 0.2602168021680217,
      "step": 4801,
      "training_loss": 7.587743759155273
    },
    {
      "epoch": 0.2602710027100271,
      "step": 4802,
      "training_loss": 6.96549654006958
    },
    {
      "epoch": 0.2603252032520325,
      "step": 4803,
      "training_loss": 7.239055633544922
    },
    {
      "epoch": 0.26037940379403796,
      "grad_norm": 19.655977249145508,
      "learning_rate": 1e-05,
      "loss": 7.2068,
      "step": 4804
    },
    {
      "epoch": 0.26037940379403796,
      "step": 4804,
      "training_loss": 6.653024196624756
    },
    {
      "epoch": 0.26043360433604335,
      "step": 4805,
      "training_loss": 5.323056221008301
    },
    {
      "epoch": 0.2604878048780488,
      "step": 4806,
      "training_loss": 6.282486438751221
    },
    {
      "epoch": 0.2605420054200542,
      "step": 4807,
      "training_loss": 7.995870113372803
    },
    {
      "epoch": 0.2605962059620596,
      "grad_norm": 55.86457443237305,
      "learning_rate": 1e-05,
      "loss": 6.5636,
      "step": 4808
    },
    {
      "epoch": 0.2605962059620596,
      "step": 4808,
      "training_loss": 7.903599739074707
    },
    {
      "epoch": 0.260650406504065,
      "step": 4809,
      "training_loss": 8.078314781188965
    },
    {
      "epoch": 0.26070460704607046,
      "step": 4810,
      "training_loss": 4.749776363372803
    },
    {
      "epoch": 0.2607588075880759,
      "step": 4811,
      "training_loss": 6.693337917327881
    },
    {
      "epoch": 0.2608130081300813,
      "grad_norm": 32.427337646484375,
      "learning_rate": 1e-05,
      "loss": 6.8563,
      "step": 4812
    },
    {
      "epoch": 0.2608130081300813,
      "step": 4812,
      "training_loss": 8.74120807647705
    },
    {
      "epoch": 0.26086720867208674,
      "step": 4813,
      "training_loss": 6.5647783279418945
    },
    {
      "epoch": 0.2609214092140921,
      "step": 4814,
      "training_loss": 7.683102607727051
    },
    {
      "epoch": 0.26097560975609757,
      "step": 4815,
      "training_loss": 6.456812381744385
    },
    {
      "epoch": 0.26102981029810296,
      "grad_norm": 24.26529884338379,
      "learning_rate": 1e-05,
      "loss": 7.3615,
      "step": 4816
    },
    {
      "epoch": 0.26102981029810296,
      "step": 4816,
      "training_loss": 6.7926177978515625
    },
    {
      "epoch": 0.2610840108401084,
      "step": 4817,
      "training_loss": 7.516434669494629
    },
    {
      "epoch": 0.2611382113821138,
      "step": 4818,
      "training_loss": 6.01669979095459
    },
    {
      "epoch": 0.26119241192411924,
      "step": 4819,
      "training_loss": 7.223236083984375
    },
    {
      "epoch": 0.2612466124661247,
      "grad_norm": 33.34225082397461,
      "learning_rate": 1e-05,
      "loss": 6.8872,
      "step": 4820
    },
    {
      "epoch": 0.2612466124661247,
      "step": 4820,
      "training_loss": 8.027338027954102
    },
    {
      "epoch": 0.2613008130081301,
      "step": 4821,
      "training_loss": 7.103915691375732
    },
    {
      "epoch": 0.2613550135501355,
      "step": 4822,
      "training_loss": 7.132882118225098
    },
    {
      "epoch": 0.2614092140921409,
      "step": 4823,
      "training_loss": 6.559634208679199
    },
    {
      "epoch": 0.26146341463414635,
      "grad_norm": 48.379241943359375,
      "learning_rate": 1e-05,
      "loss": 7.2059,
      "step": 4824
    },
    {
      "epoch": 0.26146341463414635,
      "step": 4824,
      "training_loss": 4.663775444030762
    },
    {
      "epoch": 0.26151761517615174,
      "step": 4825,
      "training_loss": 6.274765491485596
    },
    {
      "epoch": 0.2615718157181572,
      "step": 4826,
      "training_loss": 7.439170837402344
    },
    {
      "epoch": 0.2616260162601626,
      "step": 4827,
      "training_loss": 6.4873504638671875
    },
    {
      "epoch": 0.261680216802168,
      "grad_norm": 31.360361099243164,
      "learning_rate": 1e-05,
      "loss": 6.2163,
      "step": 4828
    },
    {
      "epoch": 0.261680216802168,
      "step": 4828,
      "training_loss": 6.543314456939697
    },
    {
      "epoch": 0.26173441734417346,
      "step": 4829,
      "training_loss": 4.955932140350342
    },
    {
      "epoch": 0.26178861788617885,
      "step": 4830,
      "training_loss": 6.999753475189209
    },
    {
      "epoch": 0.2618428184281843,
      "step": 4831,
      "training_loss": 7.164976119995117
    },
    {
      "epoch": 0.2618970189701897,
      "grad_norm": 19.549957275390625,
      "learning_rate": 1e-05,
      "loss": 6.416,
      "step": 4832
    },
    {
      "epoch": 0.2618970189701897,
      "step": 4832,
      "training_loss": 5.509737968444824
    },
    {
      "epoch": 0.26195121951219513,
      "step": 4833,
      "training_loss": 4.545966148376465
    },
    {
      "epoch": 0.2620054200542005,
      "step": 4834,
      "training_loss": 7.066369533538818
    },
    {
      "epoch": 0.26205962059620597,
      "step": 4835,
      "training_loss": 4.158344268798828
    },
    {
      "epoch": 0.26211382113821136,
      "grad_norm": 22.723669052124023,
      "learning_rate": 1e-05,
      "loss": 5.3201,
      "step": 4836
    },
    {
      "epoch": 0.26211382113821136,
      "step": 4836,
      "training_loss": 6.641505241394043
    },
    {
      "epoch": 0.2621680216802168,
      "step": 4837,
      "training_loss": 6.911890506744385
    },
    {
      "epoch": 0.26222222222222225,
      "step": 4838,
      "training_loss": 7.751863956451416
    },
    {
      "epoch": 0.26227642276422763,
      "step": 4839,
      "training_loss": 6.661981105804443
    },
    {
      "epoch": 0.2623306233062331,
      "grad_norm": 26.848857879638672,
      "learning_rate": 1e-05,
      "loss": 6.9918,
      "step": 4840
    },
    {
      "epoch": 0.2623306233062331,
      "step": 4840,
      "training_loss": 7.032006740570068
    },
    {
      "epoch": 0.26238482384823847,
      "step": 4841,
      "training_loss": 7.130910873413086
    },
    {
      "epoch": 0.2624390243902439,
      "step": 4842,
      "training_loss": 5.854705333709717
    },
    {
      "epoch": 0.2624932249322493,
      "step": 4843,
      "training_loss": 5.652467727661133
    },
    {
      "epoch": 0.26254742547425475,
      "grad_norm": 20.525835037231445,
      "learning_rate": 1e-05,
      "loss": 6.4175,
      "step": 4844
    },
    {
      "epoch": 0.26254742547425475,
      "step": 4844,
      "training_loss": 6.253573894500732
    },
    {
      "epoch": 0.26260162601626014,
      "step": 4845,
      "training_loss": 6.104823112487793
    },
    {
      "epoch": 0.2626558265582656,
      "step": 4846,
      "training_loss": 6.407424449920654
    },
    {
      "epoch": 0.262710027100271,
      "step": 4847,
      "training_loss": 6.159245491027832
    },
    {
      "epoch": 0.2627642276422764,
      "grad_norm": 32.34025955200195,
      "learning_rate": 1e-05,
      "loss": 6.2313,
      "step": 4848
    },
    {
      "epoch": 0.2627642276422764,
      "step": 4848,
      "training_loss": 6.343488693237305
    },
    {
      "epoch": 0.26281842818428186,
      "step": 4849,
      "training_loss": 3.463752269744873
    },
    {
      "epoch": 0.26287262872628725,
      "step": 4850,
      "training_loss": 7.3799662590026855
    },
    {
      "epoch": 0.2629268292682927,
      "step": 4851,
      "training_loss": 7.650152206420898
    },
    {
      "epoch": 0.2629810298102981,
      "grad_norm": 26.656349182128906,
      "learning_rate": 1e-05,
      "loss": 6.2093,
      "step": 4852
    },
    {
      "epoch": 0.2629810298102981,
      "step": 4852,
      "training_loss": 8.01840591430664
    },
    {
      "epoch": 0.26303523035230353,
      "step": 4853,
      "training_loss": 7.63856840133667
    },
    {
      "epoch": 0.2630894308943089,
      "step": 4854,
      "training_loss": 5.44691801071167
    },
    {
      "epoch": 0.26314363143631436,
      "step": 4855,
      "training_loss": 7.764342784881592
    },
    {
      "epoch": 0.2631978319783198,
      "grad_norm": 61.70840835571289,
      "learning_rate": 1e-05,
      "loss": 7.2171,
      "step": 4856
    },
    {
      "epoch": 0.2631978319783198,
      "step": 4856,
      "training_loss": 7.310907363891602
    },
    {
      "epoch": 0.2632520325203252,
      "step": 4857,
      "training_loss": 6.803649425506592
    },
    {
      "epoch": 0.26330623306233064,
      "step": 4858,
      "training_loss": 6.427469253540039
    },
    {
      "epoch": 0.26336043360433603,
      "step": 4859,
      "training_loss": 5.6680402755737305
    },
    {
      "epoch": 0.2634146341463415,
      "grad_norm": 29.596471786499023,
      "learning_rate": 1e-05,
      "loss": 6.5525,
      "step": 4860
    },
    {
      "epoch": 0.2634146341463415,
      "step": 4860,
      "training_loss": 6.297351837158203
    },
    {
      "epoch": 0.26346883468834686,
      "step": 4861,
      "training_loss": 6.702069282531738
    },
    {
      "epoch": 0.2635230352303523,
      "step": 4862,
      "training_loss": 6.743041515350342
    },
    {
      "epoch": 0.2635772357723577,
      "step": 4863,
      "training_loss": 5.9014506340026855
    },
    {
      "epoch": 0.26363143631436314,
      "grad_norm": 33.422752380371094,
      "learning_rate": 1e-05,
      "loss": 6.411,
      "step": 4864
    },
    {
      "epoch": 0.26363143631436314,
      "step": 4864,
      "training_loss": 6.9830169677734375
    },
    {
      "epoch": 0.2636856368563686,
      "step": 4865,
      "training_loss": 7.094808101654053
    },
    {
      "epoch": 0.263739837398374,
      "step": 4866,
      "training_loss": 6.969700336456299
    },
    {
      "epoch": 0.2637940379403794,
      "step": 4867,
      "training_loss": 7.193997859954834
    },
    {
      "epoch": 0.2638482384823848,
      "grad_norm": 25.80554962158203,
      "learning_rate": 1e-05,
      "loss": 7.0604,
      "step": 4868
    },
    {
      "epoch": 0.2638482384823848,
      "step": 4868,
      "training_loss": 7.023822784423828
    },
    {
      "epoch": 0.26390243902439026,
      "step": 4869,
      "training_loss": 6.904531478881836
    },
    {
      "epoch": 0.26395663956639565,
      "step": 4870,
      "training_loss": 6.532861232757568
    },
    {
      "epoch": 0.2640108401084011,
      "step": 4871,
      "training_loss": 7.518523693084717
    },
    {
      "epoch": 0.2640650406504065,
      "grad_norm": 16.098114013671875,
      "learning_rate": 1e-05,
      "loss": 6.9949,
      "step": 4872
    },
    {
      "epoch": 0.2640650406504065,
      "step": 4872,
      "training_loss": 5.525960445404053
    },
    {
      "epoch": 0.2641192411924119,
      "step": 4873,
      "training_loss": 6.4418745040893555
    },
    {
      "epoch": 0.26417344173441737,
      "step": 4874,
      "training_loss": 5.409494400024414
    },
    {
      "epoch": 0.26422764227642276,
      "step": 4875,
      "training_loss": 7.028253555297852
    },
    {
      "epoch": 0.2642818428184282,
      "grad_norm": 17.365798950195312,
      "learning_rate": 1e-05,
      "loss": 6.1014,
      "step": 4876
    },
    {
      "epoch": 0.2642818428184282,
      "step": 4876,
      "training_loss": 7.705293655395508
    },
    {
      "epoch": 0.2643360433604336,
      "step": 4877,
      "training_loss": 7.6135101318359375
    },
    {
      "epoch": 0.26439024390243904,
      "step": 4878,
      "training_loss": 6.538888454437256
    },
    {
      "epoch": 0.2644444444444444,
      "step": 4879,
      "training_loss": 6.8404860496521
    },
    {
      "epoch": 0.26449864498644987,
      "grad_norm": 22.497493743896484,
      "learning_rate": 1e-05,
      "loss": 7.1745,
      "step": 4880
    },
    {
      "epoch": 0.26449864498644987,
      "step": 4880,
      "training_loss": 6.473151683807373
    },
    {
      "epoch": 0.26455284552845526,
      "step": 4881,
      "training_loss": 7.773501873016357
    },
    {
      "epoch": 0.2646070460704607,
      "step": 4882,
      "training_loss": 7.117162227630615
    },
    {
      "epoch": 0.26466124661246615,
      "step": 4883,
      "training_loss": 4.842599391937256
    },
    {
      "epoch": 0.26471544715447154,
      "grad_norm": 20.025226593017578,
      "learning_rate": 1e-05,
      "loss": 6.5516,
      "step": 4884
    },
    {
      "epoch": 0.26471544715447154,
      "step": 4884,
      "training_loss": 8.028918266296387
    },
    {
      "epoch": 0.264769647696477,
      "step": 4885,
      "training_loss": 4.613664627075195
    },
    {
      "epoch": 0.2648238482384824,
      "step": 4886,
      "training_loss": 5.689857006072998
    },
    {
      "epoch": 0.2648780487804878,
      "step": 4887,
      "training_loss": 7.2086944580078125
    },
    {
      "epoch": 0.2649322493224932,
      "grad_norm": 16.11890983581543,
      "learning_rate": 1e-05,
      "loss": 6.3853,
      "step": 4888
    },
    {
      "epoch": 0.2649322493224932,
      "step": 4888,
      "training_loss": 6.974789142608643
    },
    {
      "epoch": 0.26498644986449865,
      "step": 4889,
      "training_loss": 6.038115501403809
    },
    {
      "epoch": 0.26504065040650404,
      "step": 4890,
      "training_loss": 7.399166107177734
    },
    {
      "epoch": 0.2650948509485095,
      "step": 4891,
      "training_loss": 9.039830207824707
    },
    {
      "epoch": 0.26514905149051493,
      "grad_norm": 28.489824295043945,
      "learning_rate": 1e-05,
      "loss": 7.363,
      "step": 4892
    },
    {
      "epoch": 0.26514905149051493,
      "step": 4892,
      "training_loss": 6.314671039581299
    },
    {
      "epoch": 0.2652032520325203,
      "step": 4893,
      "training_loss": 7.068612575531006
    },
    {
      "epoch": 0.26525745257452576,
      "step": 4894,
      "training_loss": 6.624389171600342
    },
    {
      "epoch": 0.26531165311653115,
      "step": 4895,
      "training_loss": 7.188756465911865
    },
    {
      "epoch": 0.2653658536585366,
      "grad_norm": 32.66558074951172,
      "learning_rate": 1e-05,
      "loss": 6.7991,
      "step": 4896
    },
    {
      "epoch": 0.2653658536585366,
      "step": 4896,
      "training_loss": 7.902585506439209
    },
    {
      "epoch": 0.265420054200542,
      "step": 4897,
      "training_loss": 7.601114273071289
    },
    {
      "epoch": 0.26547425474254743,
      "step": 4898,
      "training_loss": 6.281242847442627
    },
    {
      "epoch": 0.2655284552845528,
      "step": 4899,
      "training_loss": 6.994826316833496
    },
    {
      "epoch": 0.26558265582655827,
      "grad_norm": 24.744199752807617,
      "learning_rate": 1e-05,
      "loss": 7.1949,
      "step": 4900
    },
    {
      "epoch": 0.26558265582655827,
      "step": 4900,
      "training_loss": 4.899870872497559
    },
    {
      "epoch": 0.2656368563685637,
      "step": 4901,
      "training_loss": 5.6681294441223145
    },
    {
      "epoch": 0.2656910569105691,
      "step": 4902,
      "training_loss": 6.671302795410156
    },
    {
      "epoch": 0.26574525745257455,
      "step": 4903,
      "training_loss": 7.625035285949707
    },
    {
      "epoch": 0.26579945799457994,
      "grad_norm": 32.96500778198242,
      "learning_rate": 1e-05,
      "loss": 6.2161,
      "step": 4904
    },
    {
      "epoch": 0.26579945799457994,
      "step": 4904,
      "training_loss": 6.943455696105957
    },
    {
      "epoch": 0.2658536585365854,
      "step": 4905,
      "training_loss": 7.1414384841918945
    },
    {
      "epoch": 0.26590785907859077,
      "step": 4906,
      "training_loss": 6.6416425704956055
    },
    {
      "epoch": 0.2659620596205962,
      "step": 4907,
      "training_loss": 7.8988728523254395
    },
    {
      "epoch": 0.2660162601626016,
      "grad_norm": 33.226688385009766,
      "learning_rate": 1e-05,
      "loss": 7.1564,
      "step": 4908
    },
    {
      "epoch": 0.2660162601626016,
      "step": 4908,
      "training_loss": 6.752378463745117
    },
    {
      "epoch": 0.26607046070460705,
      "step": 4909,
      "training_loss": 6.614992618560791
    },
    {
      "epoch": 0.2661246612466125,
      "step": 4910,
      "training_loss": 7.080846786499023
    },
    {
      "epoch": 0.2661788617886179,
      "step": 4911,
      "training_loss": 6.1626763343811035
    },
    {
      "epoch": 0.2662330623306233,
      "grad_norm": 28.04447364807129,
      "learning_rate": 1e-05,
      "loss": 6.6527,
      "step": 4912
    },
    {
      "epoch": 0.2662330623306233,
      "step": 4912,
      "training_loss": 8.245169639587402
    },
    {
      "epoch": 0.2662872628726287,
      "step": 4913,
      "training_loss": 6.370792865753174
    },
    {
      "epoch": 0.26634146341463416,
      "step": 4914,
      "training_loss": 7.199748992919922
    },
    {
      "epoch": 0.26639566395663955,
      "step": 4915,
      "training_loss": 6.339925289154053
    },
    {
      "epoch": 0.266449864498645,
      "grad_norm": 22.966571807861328,
      "learning_rate": 1e-05,
      "loss": 7.0389,
      "step": 4916
    },
    {
      "epoch": 0.266449864498645,
      "step": 4916,
      "training_loss": 6.000926494598389
    },
    {
      "epoch": 0.2665040650406504,
      "step": 4917,
      "training_loss": 6.407894611358643
    },
    {
      "epoch": 0.26655826558265583,
      "step": 4918,
      "training_loss": 7.498404026031494
    },
    {
      "epoch": 0.2666124661246613,
      "step": 4919,
      "training_loss": 6.698840141296387
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 19.85947608947754,
      "learning_rate": 1e-05,
      "loss": 6.6515,
      "step": 4920
    },
    {
      "epoch": 0.26666666666666666,
      "step": 4920,
      "training_loss": 7.020698547363281
    },
    {
      "epoch": 0.2667208672086721,
      "step": 4921,
      "training_loss": 4.550604343414307
    },
    {
      "epoch": 0.2667750677506775,
      "step": 4922,
      "training_loss": 7.1332879066467285
    },
    {
      "epoch": 0.26682926829268294,
      "step": 4923,
      "training_loss": 6.6504740715026855
    },
    {
      "epoch": 0.26688346883468833,
      "grad_norm": 15.641388893127441,
      "learning_rate": 1e-05,
      "loss": 6.3388,
      "step": 4924
    },
    {
      "epoch": 0.26688346883468833,
      "step": 4924,
      "training_loss": 7.641758918762207
    },
    {
      "epoch": 0.2669376693766938,
      "step": 4925,
      "training_loss": 6.5288591384887695
    },
    {
      "epoch": 0.26699186991869917,
      "step": 4926,
      "training_loss": 6.759339809417725
    },
    {
      "epoch": 0.2670460704607046,
      "step": 4927,
      "training_loss": 6.272345066070557
    },
    {
      "epoch": 0.26710027100271005,
      "grad_norm": 24.616151809692383,
      "learning_rate": 1e-05,
      "loss": 6.8006,
      "step": 4928
    },
    {
      "epoch": 0.26710027100271005,
      "step": 4928,
      "training_loss": 7.08860445022583
    },
    {
      "epoch": 0.26715447154471544,
      "step": 4929,
      "training_loss": 7.004964828491211
    },
    {
      "epoch": 0.2672086720867209,
      "step": 4930,
      "training_loss": 4.900237560272217
    },
    {
      "epoch": 0.2672628726287263,
      "step": 4931,
      "training_loss": 6.592990875244141
    },
    {
      "epoch": 0.2673170731707317,
      "grad_norm": 21.048871994018555,
      "learning_rate": 1e-05,
      "loss": 6.3967,
      "step": 4932
    },
    {
      "epoch": 0.2673170731707317,
      "step": 4932,
      "training_loss": 8.386610984802246
    },
    {
      "epoch": 0.2673712737127371,
      "step": 4933,
      "training_loss": 7.453389644622803
    },
    {
      "epoch": 0.26742547425474256,
      "step": 4934,
      "training_loss": 6.80778694152832
    },
    {
      "epoch": 0.26747967479674795,
      "step": 4935,
      "training_loss": 5.854772567749023
    },
    {
      "epoch": 0.2675338753387534,
      "grad_norm": 24.39864730834961,
      "learning_rate": 1e-05,
      "loss": 7.1256,
      "step": 4936
    },
    {
      "epoch": 0.2675338753387534,
      "step": 4936,
      "training_loss": 5.845946788787842
    },
    {
      "epoch": 0.2675880758807588,
      "step": 4937,
      "training_loss": 5.502752304077148
    },
    {
      "epoch": 0.2676422764227642,
      "step": 4938,
      "training_loss": 7.993241786956787
    },
    {
      "epoch": 0.26769647696476967,
      "step": 4939,
      "training_loss": 7.720244407653809
    },
    {
      "epoch": 0.26775067750677506,
      "grad_norm": 22.063528060913086,
      "learning_rate": 1e-05,
      "loss": 6.7655,
      "step": 4940
    },
    {
      "epoch": 0.26775067750677506,
      "step": 4940,
      "training_loss": 8.788993835449219
    },
    {
      "epoch": 0.2678048780487805,
      "step": 4941,
      "training_loss": 6.705447673797607
    },
    {
      "epoch": 0.2678590785907859,
      "step": 4942,
      "training_loss": 6.750504970550537
    },
    {
      "epoch": 0.26791327913279134,
      "step": 4943,
      "training_loss": 7.156973361968994
    },
    {
      "epoch": 0.2679674796747967,
      "grad_norm": 25.818771362304688,
      "learning_rate": 1e-05,
      "loss": 7.3505,
      "step": 4944
    },
    {
      "epoch": 0.2679674796747967,
      "step": 4944,
      "training_loss": 6.248327732086182
    },
    {
      "epoch": 0.26802168021680217,
      "step": 4945,
      "training_loss": 4.493154048919678
    },
    {
      "epoch": 0.26807588075880756,
      "step": 4946,
      "training_loss": 7.335882663726807
    },
    {
      "epoch": 0.268130081300813,
      "step": 4947,
      "training_loss": 7.410265922546387
    },
    {
      "epoch": 0.26818428184281845,
      "grad_norm": 20.019559860229492,
      "learning_rate": 1e-05,
      "loss": 6.3719,
      "step": 4948
    },
    {
      "epoch": 0.26818428184281845,
      "step": 4948,
      "training_loss": 7.330765724182129
    },
    {
      "epoch": 0.26823848238482384,
      "step": 4949,
      "training_loss": 7.232378005981445
    },
    {
      "epoch": 0.2682926829268293,
      "step": 4950,
      "training_loss": 6.155293941497803
    },
    {
      "epoch": 0.2683468834688347,
      "step": 4951,
      "training_loss": 6.044593334197998
    },
    {
      "epoch": 0.2684010840108401,
      "grad_norm": 20.529251098632812,
      "learning_rate": 1e-05,
      "loss": 6.6908,
      "step": 4952
    },
    {
      "epoch": 0.2684010840108401,
      "step": 4952,
      "training_loss": 6.961282730102539
    },
    {
      "epoch": 0.2684552845528455,
      "step": 4953,
      "training_loss": 6.80707311630249
    },
    {
      "epoch": 0.26850948509485095,
      "step": 4954,
      "training_loss": 7.294464588165283
    },
    {
      "epoch": 0.26856368563685634,
      "step": 4955,
      "training_loss": 7.190250396728516
    },
    {
      "epoch": 0.2686178861788618,
      "grad_norm": 17.028221130371094,
      "learning_rate": 1e-05,
      "loss": 7.0633,
      "step": 4956
    },
    {
      "epoch": 0.2686178861788618,
      "step": 4956,
      "training_loss": 6.37131404876709
    },
    {
      "epoch": 0.26867208672086723,
      "step": 4957,
      "training_loss": 6.238094806671143
    },
    {
      "epoch": 0.2687262872628726,
      "step": 4958,
      "training_loss": 6.145900249481201
    },
    {
      "epoch": 0.26878048780487807,
      "step": 4959,
      "training_loss": 6.130329132080078
    },
    {
      "epoch": 0.26883468834688345,
      "grad_norm": 25.210399627685547,
      "learning_rate": 1e-05,
      "loss": 6.2214,
      "step": 4960
    },
    {
      "epoch": 0.26883468834688345,
      "step": 4960,
      "training_loss": 5.908989429473877
    },
    {
      "epoch": 0.2688888888888889,
      "step": 4961,
      "training_loss": 6.660285472869873
    },
    {
      "epoch": 0.2689430894308943,
      "step": 4962,
      "training_loss": 6.9891791343688965
    },
    {
      "epoch": 0.26899728997289973,
      "step": 4963,
      "training_loss": 5.959968566894531
    },
    {
      "epoch": 0.2690514905149051,
      "grad_norm": 24.21146011352539,
      "learning_rate": 1e-05,
      "loss": 6.3796,
      "step": 4964
    },
    {
      "epoch": 0.2690514905149051,
      "step": 4964,
      "training_loss": 6.107871055603027
    },
    {
      "epoch": 0.26910569105691057,
      "step": 4965,
      "training_loss": 7.278170585632324
    },
    {
      "epoch": 0.269159891598916,
      "step": 4966,
      "training_loss": 7.867782115936279
    },
    {
      "epoch": 0.2692140921409214,
      "step": 4967,
      "training_loss": 7.0565667152404785
    },
    {
      "epoch": 0.26926829268292685,
      "grad_norm": 17.299789428710938,
      "learning_rate": 1e-05,
      "loss": 7.0776,
      "step": 4968
    },
    {
      "epoch": 0.26926829268292685,
      "step": 4968,
      "training_loss": 6.16890811920166
    },
    {
      "epoch": 0.26932249322493224,
      "step": 4969,
      "training_loss": 7.701040267944336
    },
    {
      "epoch": 0.2693766937669377,
      "step": 4970,
      "training_loss": 4.338235855102539
    },
    {
      "epoch": 0.26943089430894307,
      "step": 4971,
      "training_loss": 6.553873062133789
    },
    {
      "epoch": 0.2694850948509485,
      "grad_norm": 23.4880428314209,
      "learning_rate": 1e-05,
      "loss": 6.1905,
      "step": 4972
    },
    {
      "epoch": 0.2694850948509485,
      "step": 4972,
      "training_loss": 5.885667324066162
    },
    {
      "epoch": 0.2695392953929539,
      "step": 4973,
      "training_loss": 6.927097797393799
    },
    {
      "epoch": 0.26959349593495935,
      "step": 4974,
      "training_loss": 6.96165132522583
    },
    {
      "epoch": 0.2696476964769648,
      "step": 4975,
      "training_loss": 4.967837333679199
    },
    {
      "epoch": 0.2697018970189702,
      "grad_norm": 21.30640411376953,
      "learning_rate": 1e-05,
      "loss": 6.1856,
      "step": 4976
    },
    {
      "epoch": 0.2697018970189702,
      "step": 4976,
      "training_loss": 6.866994857788086
    },
    {
      "epoch": 0.2697560975609756,
      "step": 4977,
      "training_loss": 6.377378463745117
    },
    {
      "epoch": 0.269810298102981,
      "step": 4978,
      "training_loss": 5.97785758972168
    },
    {
      "epoch": 0.26986449864498646,
      "step": 4979,
      "training_loss": 6.238738059997559
    },
    {
      "epoch": 0.26991869918699185,
      "grad_norm": 25.346500396728516,
      "learning_rate": 1e-05,
      "loss": 6.3652,
      "step": 4980
    },
    {
      "epoch": 0.26991869918699185,
      "step": 4980,
      "training_loss": 6.406407833099365
    },
    {
      "epoch": 0.2699728997289973,
      "step": 4981,
      "training_loss": 6.7348456382751465
    },
    {
      "epoch": 0.2700271002710027,
      "step": 4982,
      "training_loss": 5.688357353210449
    },
    {
      "epoch": 0.27008130081300813,
      "step": 4983,
      "training_loss": 6.648710250854492
    },
    {
      "epoch": 0.2701355013550136,
      "grad_norm": 55.215389251708984,
      "learning_rate": 1e-05,
      "loss": 6.3696,
      "step": 4984
    },
    {
      "epoch": 0.2701355013550136,
      "step": 4984,
      "training_loss": 8.202692985534668
    },
    {
      "epoch": 0.27018970189701896,
      "step": 4985,
      "training_loss": 7.671238899230957
    },
    {
      "epoch": 0.2702439024390244,
      "step": 4986,
      "training_loss": 7.342266082763672
    },
    {
      "epoch": 0.2702981029810298,
      "step": 4987,
      "training_loss": 7.1196393966674805
    },
    {
      "epoch": 0.27035230352303524,
      "grad_norm": 18.4271183013916,
      "learning_rate": 1e-05,
      "loss": 7.584,
      "step": 4988
    },
    {
      "epoch": 0.27035230352303524,
      "step": 4988,
      "training_loss": 5.983510494232178
    },
    {
      "epoch": 0.27040650406504063,
      "step": 4989,
      "training_loss": 4.742553234100342
    },
    {
      "epoch": 0.2704607046070461,
      "step": 4990,
      "training_loss": 7.378482341766357
    },
    {
      "epoch": 0.27051490514905147,
      "step": 4991,
      "training_loss": 5.532063007354736
    },
    {
      "epoch": 0.2705691056910569,
      "grad_norm": 33.3089485168457,
      "learning_rate": 1e-05,
      "loss": 5.9092,
      "step": 4992
    },
    {
      "epoch": 0.2705691056910569,
      "step": 4992,
      "training_loss": 6.453804016113281
    },
    {
      "epoch": 0.27062330623306236,
      "step": 4993,
      "training_loss": 6.779522895812988
    },
    {
      "epoch": 0.27067750677506774,
      "step": 4994,
      "training_loss": 6.931865692138672
    },
    {
      "epoch": 0.2707317073170732,
      "step": 4995,
      "training_loss": 6.8208909034729
    },
    {
      "epoch": 0.2707859078590786,
      "grad_norm": 28.626035690307617,
      "learning_rate": 1e-05,
      "loss": 6.7465,
      "step": 4996
    },
    {
      "epoch": 0.2707859078590786,
      "step": 4996,
      "training_loss": 7.275331974029541
    },
    {
      "epoch": 0.270840108401084,
      "step": 4997,
      "training_loss": 6.914759159088135
    },
    {
      "epoch": 0.2708943089430894,
      "step": 4998,
      "training_loss": 6.469493865966797
    },
    {
      "epoch": 0.27094850948509486,
      "step": 4999,
      "training_loss": 6.281454086303711
    },
    {
      "epoch": 0.27100271002710025,
      "grad_norm": 20.635196685791016,
      "learning_rate": 1e-05,
      "loss": 6.7353,
      "step": 5000
    },
    {
      "epoch": 0.27100271002710025,
      "step": 5000,
      "training_loss": 6.846871376037598
    },
    {
      "epoch": 0.2710569105691057,
      "step": 5001,
      "training_loss": 6.084092617034912
    },
    {
      "epoch": 0.27111111111111114,
      "step": 5002,
      "training_loss": 6.354703426361084
    },
    {
      "epoch": 0.2711653116531165,
      "step": 5003,
      "training_loss": 5.487571716308594
    },
    {
      "epoch": 0.27121951219512197,
      "grad_norm": 33.11050796508789,
      "learning_rate": 1e-05,
      "loss": 6.1933,
      "step": 5004
    },
    {
      "epoch": 0.27121951219512197,
      "step": 5004,
      "training_loss": 4.632855415344238
    },
    {
      "epoch": 0.27127371273712736,
      "step": 5005,
      "training_loss": 8.17302131652832
    },
    {
      "epoch": 0.2713279132791328,
      "step": 5006,
      "training_loss": 8.119634628295898
    },
    {
      "epoch": 0.2713821138211382,
      "step": 5007,
      "training_loss": 6.907395839691162
    },
    {
      "epoch": 0.27143631436314364,
      "grad_norm": 27.857711791992188,
      "learning_rate": 1e-05,
      "loss": 6.9582,
      "step": 5008
    },
    {
      "epoch": 0.27143631436314364,
      "step": 5008,
      "training_loss": 6.668277263641357
    },
    {
      "epoch": 0.271490514905149,
      "step": 5009,
      "training_loss": 6.824997901916504
    },
    {
      "epoch": 0.27154471544715447,
      "step": 5010,
      "training_loss": 6.480597496032715
    },
    {
      "epoch": 0.2715989159891599,
      "step": 5011,
      "training_loss": 6.35274600982666
    },
    {
      "epoch": 0.2716531165311653,
      "grad_norm": 29.897846221923828,
      "learning_rate": 1e-05,
      "loss": 6.5817,
      "step": 5012
    },
    {
      "epoch": 0.2716531165311653,
      "step": 5012,
      "training_loss": 5.531122207641602
    },
    {
      "epoch": 0.27170731707317075,
      "step": 5013,
      "training_loss": 6.113900184631348
    },
    {
      "epoch": 0.27176151761517614,
      "step": 5014,
      "training_loss": 6.961343765258789
    },
    {
      "epoch": 0.2718157181571816,
      "step": 5015,
      "training_loss": 7.088756084442139
    },
    {
      "epoch": 0.271869918699187,
      "grad_norm": 19.20054817199707,
      "learning_rate": 1e-05,
      "loss": 6.4238,
      "step": 5016
    },
    {
      "epoch": 0.271869918699187,
      "step": 5016,
      "training_loss": 7.603376388549805
    },
    {
      "epoch": 0.2719241192411924,
      "step": 5017,
      "training_loss": 6.75981330871582
    },
    {
      "epoch": 0.2719783197831978,
      "step": 5018,
      "training_loss": 6.7920966148376465
    },
    {
      "epoch": 0.27203252032520325,
      "step": 5019,
      "training_loss": 9.877098083496094
    },
    {
      "epoch": 0.2720867208672087,
      "grad_norm": 60.20651626586914,
      "learning_rate": 1e-05,
      "loss": 7.7581,
      "step": 5020
    },
    {
      "epoch": 0.2720867208672087,
      "step": 5020,
      "training_loss": 6.8385491371154785
    },
    {
      "epoch": 0.2721409214092141,
      "step": 5021,
      "training_loss": 6.908149719238281
    },
    {
      "epoch": 0.27219512195121953,
      "step": 5022,
      "training_loss": 4.465498924255371
    },
    {
      "epoch": 0.2722493224932249,
      "step": 5023,
      "training_loss": 7.869126319885254
    },
    {
      "epoch": 0.27230352303523037,
      "grad_norm": 25.243654251098633,
      "learning_rate": 1e-05,
      "loss": 6.5203,
      "step": 5024
    },
    {
      "epoch": 0.27230352303523037,
      "step": 5024,
      "training_loss": 7.12803316116333
    },
    {
      "epoch": 0.27235772357723576,
      "step": 5025,
      "training_loss": 7.4212870597839355
    },
    {
      "epoch": 0.2724119241192412,
      "step": 5026,
      "training_loss": 6.751155376434326
    },
    {
      "epoch": 0.2724661246612466,
      "step": 5027,
      "training_loss": 7.263567924499512
    },
    {
      "epoch": 0.27252032520325203,
      "grad_norm": 20.02451515197754,
      "learning_rate": 1e-05,
      "loss": 7.141,
      "step": 5028
    },
    {
      "epoch": 0.27252032520325203,
      "step": 5028,
      "training_loss": 7.132732391357422
    },
    {
      "epoch": 0.2725745257452575,
      "step": 5029,
      "training_loss": 6.012722492218018
    },
    {
      "epoch": 0.27262872628726287,
      "step": 5030,
      "training_loss": 6.621293067932129
    },
    {
      "epoch": 0.2726829268292683,
      "step": 5031,
      "training_loss": 7.5128068923950195
    },
    {
      "epoch": 0.2727371273712737,
      "grad_norm": 23.665990829467773,
      "learning_rate": 1e-05,
      "loss": 6.8199,
      "step": 5032
    },
    {
      "epoch": 0.2727371273712737,
      "step": 5032,
      "training_loss": 5.978063583374023
    },
    {
      "epoch": 0.27279132791327915,
      "step": 5033,
      "training_loss": 6.502889633178711
    },
    {
      "epoch": 0.27284552845528454,
      "step": 5034,
      "training_loss": 7.972695350646973
    },
    {
      "epoch": 0.27289972899729,
      "step": 5035,
      "training_loss": 5.6975884437561035
    },
    {
      "epoch": 0.27295392953929537,
      "grad_norm": 26.227331161499023,
      "learning_rate": 1e-05,
      "loss": 6.5378,
      "step": 5036
    },
    {
      "epoch": 0.27295392953929537,
      "step": 5036,
      "training_loss": 4.315586090087891
    },
    {
      "epoch": 0.2730081300813008,
      "step": 5037,
      "training_loss": 7.259707450866699
    },
    {
      "epoch": 0.27306233062330626,
      "step": 5038,
      "training_loss": 6.864482879638672
    },
    {
      "epoch": 0.27311653116531165,
      "step": 5039,
      "training_loss": 6.551335334777832
    },
    {
      "epoch": 0.2731707317073171,
      "grad_norm": 27.97097396850586,
      "learning_rate": 1e-05,
      "loss": 6.2478,
      "step": 5040
    },
    {
      "epoch": 0.2731707317073171,
      "step": 5040,
      "training_loss": 6.701514720916748
    },
    {
      "epoch": 0.2732249322493225,
      "step": 5041,
      "training_loss": 7.216390132904053
    },
    {
      "epoch": 0.27327913279132793,
      "step": 5042,
      "training_loss": 8.100756645202637
    },
    {
      "epoch": 0.2733333333333333,
      "step": 5043,
      "training_loss": 7.402855396270752
    },
    {
      "epoch": 0.27338753387533876,
      "grad_norm": 24.500633239746094,
      "learning_rate": 1e-05,
      "loss": 7.3554,
      "step": 5044
    },
    {
      "epoch": 0.27338753387533876,
      "step": 5044,
      "training_loss": 6.1606245040893555
    },
    {
      "epoch": 0.27344173441734415,
      "step": 5045,
      "training_loss": 5.402135372161865
    },
    {
      "epoch": 0.2734959349593496,
      "step": 5046,
      "training_loss": 6.702467918395996
    },
    {
      "epoch": 0.27355013550135504,
      "step": 5047,
      "training_loss": 6.352453231811523
    },
    {
      "epoch": 0.27360433604336043,
      "grad_norm": 21.45200538635254,
      "learning_rate": 1e-05,
      "loss": 6.1544,
      "step": 5048
    },
    {
      "epoch": 0.27360433604336043,
      "step": 5048,
      "training_loss": 6.0108489990234375
    },
    {
      "epoch": 0.2736585365853659,
      "step": 5049,
      "training_loss": 6.343145370483398
    },
    {
      "epoch": 0.27371273712737126,
      "step": 5050,
      "training_loss": 6.253802299499512
    },
    {
      "epoch": 0.2737669376693767,
      "step": 5051,
      "training_loss": 8.04179859161377
    },
    {
      "epoch": 0.2738211382113821,
      "grad_norm": 24.116987228393555,
      "learning_rate": 1e-05,
      "loss": 6.6624,
      "step": 5052
    },
    {
      "epoch": 0.2738211382113821,
      "step": 5052,
      "training_loss": 3.90155029296875
    },
    {
      "epoch": 0.27387533875338754,
      "step": 5053,
      "training_loss": 7.348795413970947
    },
    {
      "epoch": 0.27392953929539293,
      "step": 5054,
      "training_loss": 7.3843865394592285
    },
    {
      "epoch": 0.2739837398373984,
      "step": 5055,
      "training_loss": 8.598566055297852
    },
    {
      "epoch": 0.2740379403794038,
      "grad_norm": 27.270095825195312,
      "learning_rate": 1e-05,
      "loss": 6.8083,
      "step": 5056
    },
    {
      "epoch": 0.2740379403794038,
      "step": 5056,
      "training_loss": 7.274750709533691
    },
    {
      "epoch": 0.2740921409214092,
      "step": 5057,
      "training_loss": 5.564451694488525
    },
    {
      "epoch": 0.27414634146341466,
      "step": 5058,
      "training_loss": 6.425874710083008
    },
    {
      "epoch": 0.27420054200542004,
      "step": 5059,
      "training_loss": 8.992061614990234
    },
    {
      "epoch": 0.2742547425474255,
      "grad_norm": 42.347007751464844,
      "learning_rate": 1e-05,
      "loss": 7.0643,
      "step": 5060
    },
    {
      "epoch": 0.2742547425474255,
      "step": 5060,
      "training_loss": 7.033367156982422
    },
    {
      "epoch": 0.2743089430894309,
      "step": 5061,
      "training_loss": 6.919116020202637
    },
    {
      "epoch": 0.2743631436314363,
      "step": 5062,
      "training_loss": 7.683764934539795
    },
    {
      "epoch": 0.2744173441734417,
      "step": 5063,
      "training_loss": 6.566936492919922
    },
    {
      "epoch": 0.27447154471544716,
      "grad_norm": 34.346927642822266,
      "learning_rate": 1e-05,
      "loss": 7.0508,
      "step": 5064
    },
    {
      "epoch": 0.27447154471544716,
      "step": 5064,
      "training_loss": 6.86555814743042
    },
    {
      "epoch": 0.27452574525745255,
      "step": 5065,
      "training_loss": 7.374061584472656
    },
    {
      "epoch": 0.274579945799458,
      "step": 5066,
      "training_loss": 8.585213661193848
    },
    {
      "epoch": 0.27463414634146344,
      "step": 5067,
      "training_loss": 6.180380344390869
    },
    {
      "epoch": 0.2746883468834688,
      "grad_norm": 25.922157287597656,
      "learning_rate": 1e-05,
      "loss": 7.2513,
      "step": 5068
    },
    {
      "epoch": 0.2746883468834688,
      "step": 5068,
      "training_loss": 7.020946025848389
    },
    {
      "epoch": 0.27474254742547427,
      "step": 5069,
      "training_loss": 6.561461448669434
    },
    {
      "epoch": 0.27479674796747966,
      "step": 5070,
      "training_loss": 6.494003772735596
    },
    {
      "epoch": 0.2748509485094851,
      "step": 5071,
      "training_loss": 8.637481689453125
    },
    {
      "epoch": 0.2749051490514905,
      "grad_norm": 27.33583641052246,
      "learning_rate": 1e-05,
      "loss": 7.1785,
      "step": 5072
    },
    {
      "epoch": 0.2749051490514905,
      "step": 5072,
      "training_loss": 7.536584377288818
    },
    {
      "epoch": 0.27495934959349594,
      "step": 5073,
      "training_loss": 7.6446852684021
    },
    {
      "epoch": 0.27501355013550133,
      "step": 5074,
      "training_loss": 8.392212867736816
    },
    {
      "epoch": 0.2750677506775068,
      "step": 5075,
      "training_loss": 6.30781888961792
    },
    {
      "epoch": 0.2751219512195122,
      "grad_norm": 24.775413513183594,
      "learning_rate": 1e-05,
      "loss": 7.4703,
      "step": 5076
    },
    {
      "epoch": 0.2751219512195122,
      "step": 5076,
      "training_loss": 3.8457186222076416
    },
    {
      "epoch": 0.2751761517615176,
      "step": 5077,
      "training_loss": 6.927586555480957
    },
    {
      "epoch": 0.27523035230352305,
      "step": 5078,
      "training_loss": 5.750773906707764
    },
    {
      "epoch": 0.27528455284552844,
      "step": 5079,
      "training_loss": 6.9924211502075195
    },
    {
      "epoch": 0.2753387533875339,
      "grad_norm": 23.354942321777344,
      "learning_rate": 1e-05,
      "loss": 5.8791,
      "step": 5080
    },
    {
      "epoch": 0.2753387533875339,
      "step": 5080,
      "training_loss": 7.685932636260986
    },
    {
      "epoch": 0.2753929539295393,
      "step": 5081,
      "training_loss": 7.648141860961914
    },
    {
      "epoch": 0.2754471544715447,
      "step": 5082,
      "training_loss": 6.837948322296143
    },
    {
      "epoch": 0.2755013550135501,
      "step": 5083,
      "training_loss": 5.5679612159729
    },
    {
      "epoch": 0.27555555555555555,
      "grad_norm": 24.68473243713379,
      "learning_rate": 1e-05,
      "loss": 6.935,
      "step": 5084
    },
    {
      "epoch": 0.27555555555555555,
      "step": 5084,
      "training_loss": 7.605480194091797
    },
    {
      "epoch": 0.275609756097561,
      "step": 5085,
      "training_loss": 7.336971759796143
    },
    {
      "epoch": 0.2756639566395664,
      "step": 5086,
      "training_loss": 6.379064083099365
    },
    {
      "epoch": 0.27571815718157183,
      "step": 5087,
      "training_loss": 6.675513744354248
    },
    {
      "epoch": 0.2757723577235772,
      "grad_norm": 24.13417625427246,
      "learning_rate": 1e-05,
      "loss": 6.9993,
      "step": 5088
    },
    {
      "epoch": 0.2757723577235772,
      "step": 5088,
      "training_loss": 6.234110355377197
    },
    {
      "epoch": 0.27582655826558267,
      "step": 5089,
      "training_loss": 6.761823654174805
    },
    {
      "epoch": 0.27588075880758806,
      "step": 5090,
      "training_loss": 7.025148391723633
    },
    {
      "epoch": 0.2759349593495935,
      "step": 5091,
      "training_loss": 7.738255977630615
    },
    {
      "epoch": 0.2759891598915989,
      "grad_norm": 30.339004516601562,
      "learning_rate": 1e-05,
      "loss": 6.9398,
      "step": 5092
    },
    {
      "epoch": 0.2759891598915989,
      "step": 5092,
      "training_loss": 7.103665828704834
    },
    {
      "epoch": 0.27604336043360433,
      "step": 5093,
      "training_loss": 5.269614219665527
    },
    {
      "epoch": 0.2760975609756098,
      "step": 5094,
      "training_loss": 7.804317474365234
    },
    {
      "epoch": 0.27615176151761517,
      "step": 5095,
      "training_loss": 6.4863176345825195
    },
    {
      "epoch": 0.2762059620596206,
      "grad_norm": 27.882822036743164,
      "learning_rate": 1e-05,
      "loss": 6.666,
      "step": 5096
    },
    {
      "epoch": 0.2762059620596206,
      "step": 5096,
      "training_loss": 5.433704853057861
    },
    {
      "epoch": 0.276260162601626,
      "step": 5097,
      "training_loss": 6.801220417022705
    },
    {
      "epoch": 0.27631436314363145,
      "step": 5098,
      "training_loss": 7.806066036224365
    },
    {
      "epoch": 0.27636856368563684,
      "step": 5099,
      "training_loss": 4.798552989959717
    },
    {
      "epoch": 0.2764227642276423,
      "grad_norm": 42.81672668457031,
      "learning_rate": 1e-05,
      "loss": 6.2099,
      "step": 5100
    },
    {
      "epoch": 0.2764227642276423,
      "step": 5100,
      "training_loss": 5.2487263679504395
    },
    {
      "epoch": 0.27647696476964767,
      "step": 5101,
      "training_loss": 7.74701452255249
    },
    {
      "epoch": 0.2765311653116531,
      "step": 5102,
      "training_loss": 6.846710681915283
    },
    {
      "epoch": 0.27658536585365856,
      "step": 5103,
      "training_loss": 4.7576189041137695
    },
    {
      "epoch": 0.27663956639566395,
      "grad_norm": 22.564355850219727,
      "learning_rate": 1e-05,
      "loss": 6.15,
      "step": 5104
    },
    {
      "epoch": 0.27663956639566395,
      "step": 5104,
      "training_loss": 4.055253982543945
    },
    {
      "epoch": 0.2766937669376694,
      "step": 5105,
      "training_loss": 7.487155437469482
    },
    {
      "epoch": 0.2767479674796748,
      "step": 5106,
      "training_loss": 8.462461471557617
    },
    {
      "epoch": 0.27680216802168023,
      "step": 5107,
      "training_loss": 6.205909252166748
    },
    {
      "epoch": 0.2768563685636856,
      "grad_norm": 22.684247970581055,
      "learning_rate": 1e-05,
      "loss": 6.5527,
      "step": 5108
    },
    {
      "epoch": 0.2768563685636856,
      "step": 5108,
      "training_loss": 6.215158939361572
    },
    {
      "epoch": 0.27691056910569106,
      "step": 5109,
      "training_loss": 7.340853691101074
    },
    {
      "epoch": 0.27696476964769645,
      "step": 5110,
      "training_loss": 7.656935691833496
    },
    {
      "epoch": 0.2770189701897019,
      "step": 5111,
      "training_loss": 6.677061557769775
    },
    {
      "epoch": 0.27707317073170734,
      "grad_norm": 16.870849609375,
      "learning_rate": 1e-05,
      "loss": 6.9725,
      "step": 5112
    },
    {
      "epoch": 0.27707317073170734,
      "step": 5112,
      "training_loss": 7.626512050628662
    },
    {
      "epoch": 0.27712737127371273,
      "step": 5113,
      "training_loss": 4.525638103485107
    },
    {
      "epoch": 0.2771815718157182,
      "step": 5114,
      "training_loss": 6.450981140136719
    },
    {
      "epoch": 0.27723577235772356,
      "step": 5115,
      "training_loss": 7.713808059692383
    },
    {
      "epoch": 0.277289972899729,
      "grad_norm": 21.2799129486084,
      "learning_rate": 1e-05,
      "loss": 6.5792,
      "step": 5116
    },
    {
      "epoch": 0.277289972899729,
      "step": 5116,
      "training_loss": 7.57472562789917
    },
    {
      "epoch": 0.2773441734417344,
      "step": 5117,
      "training_loss": 7.269898891448975
    },
    {
      "epoch": 0.27739837398373984,
      "step": 5118,
      "training_loss": 7.2742390632629395
    },
    {
      "epoch": 0.27745257452574523,
      "step": 5119,
      "training_loss": 5.78804349899292
    },
    {
      "epoch": 0.2775067750677507,
      "grad_norm": 27.47576141357422,
      "learning_rate": 1e-05,
      "loss": 6.9767,
      "step": 5120
    },
    {
      "epoch": 0.2775067750677507,
      "step": 5120,
      "training_loss": 3.971553325653076
    },
    {
      "epoch": 0.2775609756097561,
      "step": 5121,
      "training_loss": 7.555501461029053
    },
    {
      "epoch": 0.2776151761517615,
      "step": 5122,
      "training_loss": 7.695818901062012
    },
    {
      "epoch": 0.27766937669376696,
      "step": 5123,
      "training_loss": 7.948122501373291
    },
    {
      "epoch": 0.27772357723577235,
      "grad_norm": 37.73394012451172,
      "learning_rate": 1e-05,
      "loss": 6.7927,
      "step": 5124
    },
    {
      "epoch": 0.27772357723577235,
      "step": 5124,
      "training_loss": 6.556992530822754
    },
    {
      "epoch": 0.2777777777777778,
      "step": 5125,
      "training_loss": 6.840548038482666
    },
    {
      "epoch": 0.2778319783197832,
      "step": 5126,
      "training_loss": 6.6039042472839355
    },
    {
      "epoch": 0.2778861788617886,
      "step": 5127,
      "training_loss": 6.574808120727539
    },
    {
      "epoch": 0.277940379403794,
      "grad_norm": 21.144800186157227,
      "learning_rate": 1e-05,
      "loss": 6.6441,
      "step": 5128
    },
    {
      "epoch": 0.277940379403794,
      "step": 5128,
      "training_loss": 6.023215293884277
    },
    {
      "epoch": 0.27799457994579946,
      "step": 5129,
      "training_loss": 6.987311363220215
    },
    {
      "epoch": 0.2780487804878049,
      "step": 5130,
      "training_loss": 7.173766613006592
    },
    {
      "epoch": 0.2781029810298103,
      "step": 5131,
      "training_loss": 6.386744499206543
    },
    {
      "epoch": 0.27815718157181574,
      "grad_norm": 21.99808692932129,
      "learning_rate": 1e-05,
      "loss": 6.6428,
      "step": 5132
    },
    {
      "epoch": 0.27815718157181574,
      "step": 5132,
      "training_loss": 3.740746259689331
    },
    {
      "epoch": 0.2782113821138211,
      "step": 5133,
      "training_loss": 6.378673076629639
    },
    {
      "epoch": 0.27826558265582657,
      "step": 5134,
      "training_loss": 7.0471954345703125
    },
    {
      "epoch": 0.27831978319783196,
      "step": 5135,
      "training_loss": 4.465422630310059
    },
    {
      "epoch": 0.2783739837398374,
      "grad_norm": 28.879209518432617,
      "learning_rate": 1e-05,
      "loss": 5.408,
      "step": 5136
    },
    {
      "epoch": 0.2783739837398374,
      "step": 5136,
      "training_loss": 7.169620990753174
    },
    {
      "epoch": 0.2784281842818428,
      "step": 5137,
      "training_loss": 6.835855960845947
    },
    {
      "epoch": 0.27848238482384824,
      "step": 5138,
      "training_loss": 6.894830226898193
    },
    {
      "epoch": 0.2785365853658537,
      "step": 5139,
      "training_loss": 6.159998893737793
    },
    {
      "epoch": 0.2785907859078591,
      "grad_norm": 24.268579483032227,
      "learning_rate": 1e-05,
      "loss": 6.7651,
      "step": 5140
    },
    {
      "epoch": 0.2785907859078591,
      "step": 5140,
      "training_loss": 5.784419059753418
    },
    {
      "epoch": 0.2786449864498645,
      "step": 5141,
      "training_loss": 7.549221515655518
    },
    {
      "epoch": 0.2786991869918699,
      "step": 5142,
      "training_loss": 8.798530578613281
    },
    {
      "epoch": 0.27875338753387535,
      "step": 5143,
      "training_loss": 6.7353291511535645
    },
    {
      "epoch": 0.27880758807588074,
      "grad_norm": 20.075687408447266,
      "learning_rate": 1e-05,
      "loss": 7.2169,
      "step": 5144
    },
    {
      "epoch": 0.27880758807588074,
      "step": 5144,
      "training_loss": 7.566021919250488
    },
    {
      "epoch": 0.2788617886178862,
      "step": 5145,
      "training_loss": 7.280628204345703
    },
    {
      "epoch": 0.2789159891598916,
      "step": 5146,
      "training_loss": 6.950271129608154
    },
    {
      "epoch": 0.278970189701897,
      "step": 5147,
      "training_loss": 5.411248683929443
    },
    {
      "epoch": 0.27902439024390246,
      "grad_norm": 21.45005989074707,
      "learning_rate": 1e-05,
      "loss": 6.802,
      "step": 5148
    },
    {
      "epoch": 0.27902439024390246,
      "step": 5148,
      "training_loss": 6.991097450256348
    },
    {
      "epoch": 0.27907859078590785,
      "step": 5149,
      "training_loss": 7.274267673492432
    },
    {
      "epoch": 0.2791327913279133,
      "step": 5150,
      "training_loss": 6.727305889129639
    },
    {
      "epoch": 0.2791869918699187,
      "step": 5151,
      "training_loss": 5.354216575622559
    },
    {
      "epoch": 0.27924119241192413,
      "grad_norm": 22.670394897460938,
      "learning_rate": 1e-05,
      "loss": 6.5867,
      "step": 5152
    },
    {
      "epoch": 0.27924119241192413,
      "step": 5152,
      "training_loss": 6.7358832359313965
    },
    {
      "epoch": 0.2792953929539295,
      "step": 5153,
      "training_loss": 6.445342540740967
    },
    {
      "epoch": 0.27934959349593497,
      "step": 5154,
      "training_loss": 7.842922687530518
    },
    {
      "epoch": 0.27940379403794036,
      "step": 5155,
      "training_loss": 7.114840984344482
    },
    {
      "epoch": 0.2794579945799458,
      "grad_norm": 17.341154098510742,
      "learning_rate": 1e-05,
      "loss": 7.0347,
      "step": 5156
    },
    {
      "epoch": 0.2794579945799458,
      "step": 5156,
      "training_loss": 7.133351802825928
    },
    {
      "epoch": 0.27951219512195125,
      "step": 5157,
      "training_loss": 5.844870090484619
    },
    {
      "epoch": 0.27956639566395663,
      "step": 5158,
      "training_loss": 7.485314846038818
    },
    {
      "epoch": 0.2796205962059621,
      "step": 5159,
      "training_loss": 6.714995384216309
    },
    {
      "epoch": 0.27967479674796747,
      "grad_norm": 16.67828941345215,
      "learning_rate": 1e-05,
      "loss": 6.7946,
      "step": 5160
    },
    {
      "epoch": 0.27967479674796747,
      "step": 5160,
      "training_loss": 6.3726372718811035
    },
    {
      "epoch": 0.2797289972899729,
      "step": 5161,
      "training_loss": 7.45034646987915
    },
    {
      "epoch": 0.2797831978319783,
      "step": 5162,
      "training_loss": 7.450637340545654
    },
    {
      "epoch": 0.27983739837398375,
      "step": 5163,
      "training_loss": 7.76308536529541
    },
    {
      "epoch": 0.27989159891598914,
      "grad_norm": 19.63082504272461,
      "learning_rate": 1e-05,
      "loss": 7.2592,
      "step": 5164
    },
    {
      "epoch": 0.27989159891598914,
      "step": 5164,
      "training_loss": 5.989553928375244
    },
    {
      "epoch": 0.2799457994579946,
      "step": 5165,
      "training_loss": 7.993453502655029
    },
    {
      "epoch": 0.28,
      "step": 5166,
      "training_loss": 7.123623371124268
    },
    {
      "epoch": 0.2800542005420054,
      "step": 5167,
      "training_loss": 7.363927364349365
    },
    {
      "epoch": 0.28010840108401086,
      "grad_norm": 41.5241813659668,
      "learning_rate": 1e-05,
      "loss": 7.1176,
      "step": 5168
    },
    {
      "epoch": 0.28010840108401086,
      "step": 5168,
      "training_loss": 6.673237323760986
    },
    {
      "epoch": 0.28016260162601625,
      "step": 5169,
      "training_loss": 7.022278785705566
    },
    {
      "epoch": 0.2802168021680217,
      "step": 5170,
      "training_loss": 5.6270599365234375
    },
    {
      "epoch": 0.2802710027100271,
      "step": 5171,
      "training_loss": 6.243344783782959
    },
    {
      "epoch": 0.28032520325203253,
      "grad_norm": 32.25327682495117,
      "learning_rate": 1e-05,
      "loss": 6.3915,
      "step": 5172
    },
    {
      "epoch": 0.28032520325203253,
      "step": 5172,
      "training_loss": 6.562553882598877
    },
    {
      "epoch": 0.2803794037940379,
      "step": 5173,
      "training_loss": 6.111568927764893
    },
    {
      "epoch": 0.28043360433604336,
      "step": 5174,
      "training_loss": 7.07467794418335
    },
    {
      "epoch": 0.2804878048780488,
      "step": 5175,
      "training_loss": 6.762960433959961
    },
    {
      "epoch": 0.2805420054200542,
      "grad_norm": 33.26948165893555,
      "learning_rate": 1e-05,
      "loss": 6.6279,
      "step": 5176
    },
    {
      "epoch": 0.2805420054200542,
      "step": 5176,
      "training_loss": 5.911686897277832
    },
    {
      "epoch": 0.28059620596205964,
      "step": 5177,
      "training_loss": 7.487447738647461
    },
    {
      "epoch": 0.28065040650406503,
      "step": 5178,
      "training_loss": 6.62183952331543
    },
    {
      "epoch": 0.2807046070460705,
      "step": 5179,
      "training_loss": 7.121008396148682
    },
    {
      "epoch": 0.28075880758807586,
      "grad_norm": 37.367462158203125,
      "learning_rate": 1e-05,
      "loss": 6.7855,
      "step": 5180
    },
    {
      "epoch": 0.28075880758807586,
      "step": 5180,
      "training_loss": 7.644127368927002
    },
    {
      "epoch": 0.2808130081300813,
      "step": 5181,
      "training_loss": 7.107113838195801
    },
    {
      "epoch": 0.2808672086720867,
      "step": 5182,
      "training_loss": 6.588601589202881
    },
    {
      "epoch": 0.28092140921409214,
      "step": 5183,
      "training_loss": 6.829239368438721
    },
    {
      "epoch": 0.2809756097560976,
      "grad_norm": 37.35757064819336,
      "learning_rate": 1e-05,
      "loss": 7.0423,
      "step": 5184
    },
    {
      "epoch": 0.2809756097560976,
      "step": 5184,
      "training_loss": 4.624415397644043
    },
    {
      "epoch": 0.281029810298103,
      "step": 5185,
      "training_loss": 6.896310806274414
    },
    {
      "epoch": 0.2810840108401084,
      "step": 5186,
      "training_loss": 7.160922527313232
    },
    {
      "epoch": 0.2811382113821138,
      "step": 5187,
      "training_loss": 5.826164245605469
    },
    {
      "epoch": 0.28119241192411926,
      "grad_norm": 22.8209228515625,
      "learning_rate": 1e-05,
      "loss": 6.127,
      "step": 5188
    },
    {
      "epoch": 0.28119241192411926,
      "step": 5188,
      "training_loss": 7.945351600646973
    },
    {
      "epoch": 0.28124661246612465,
      "step": 5189,
      "training_loss": 5.796289443969727
    },
    {
      "epoch": 0.2813008130081301,
      "step": 5190,
      "training_loss": 6.830871105194092
    },
    {
      "epoch": 0.2813550135501355,
      "step": 5191,
      "training_loss": 7.005458354949951
    },
    {
      "epoch": 0.2814092140921409,
      "grad_norm": 18.31974220275879,
      "learning_rate": 1e-05,
      "loss": 6.8945,
      "step": 5192
    },
    {
      "epoch": 0.2814092140921409,
      "step": 5192,
      "training_loss": 7.952748775482178
    },
    {
      "epoch": 0.2814634146341463,
      "step": 5193,
      "training_loss": 7.311100482940674
    },
    {
      "epoch": 0.28151761517615176,
      "step": 5194,
      "training_loss": 6.510183334350586
    },
    {
      "epoch": 0.2815718157181572,
      "step": 5195,
      "training_loss": 7.906668663024902
    },
    {
      "epoch": 0.2816260162601626,
      "grad_norm": 77.68608856201172,
      "learning_rate": 1e-05,
      "loss": 7.4202,
      "step": 5196
    },
    {
      "epoch": 0.2816260162601626,
      "step": 5196,
      "training_loss": 7.115444183349609
    },
    {
      "epoch": 0.28168021680216804,
      "step": 5197,
      "training_loss": 6.917321681976318
    },
    {
      "epoch": 0.2817344173441734,
      "step": 5198,
      "training_loss": 6.136656761169434
    },
    {
      "epoch": 0.28178861788617887,
      "step": 5199,
      "training_loss": 7.709949493408203
    },
    {
      "epoch": 0.28184281842818426,
      "grad_norm": 22.127769470214844,
      "learning_rate": 1e-05,
      "loss": 6.9698,
      "step": 5200
    },
    {
      "epoch": 0.28184281842818426,
      "step": 5200,
      "training_loss": 5.764556884765625
    },
    {
      "epoch": 0.2818970189701897,
      "step": 5201,
      "training_loss": 7.877197265625
    },
    {
      "epoch": 0.2819512195121951,
      "step": 5202,
      "training_loss": 6.7823805809021
    },
    {
      "epoch": 0.28200542005420054,
      "step": 5203,
      "training_loss": 8.165361404418945
    },
    {
      "epoch": 0.282059620596206,
      "grad_norm": 25.326120376586914,
      "learning_rate": 1e-05,
      "loss": 7.1474,
      "step": 5204
    },
    {
      "epoch": 0.282059620596206,
      "step": 5204,
      "training_loss": 6.388025760650635
    },
    {
      "epoch": 0.2821138211382114,
      "step": 5205,
      "training_loss": 6.351504325866699
    },
    {
      "epoch": 0.2821680216802168,
      "step": 5206,
      "training_loss": 7.861251354217529
    },
    {
      "epoch": 0.2822222222222222,
      "step": 5207,
      "training_loss": 7.390162944793701
    },
    {
      "epoch": 0.28227642276422765,
      "grad_norm": 16.987268447875977,
      "learning_rate": 1e-05,
      "loss": 6.9977,
      "step": 5208
    },
    {
      "epoch": 0.28227642276422765,
      "step": 5208,
      "training_loss": 7.283344268798828
    },
    {
      "epoch": 0.28233062330623304,
      "step": 5209,
      "training_loss": 7.125881671905518
    },
    {
      "epoch": 0.2823848238482385,
      "step": 5210,
      "training_loss": 7.1070075035095215
    },
    {
      "epoch": 0.2824390243902439,
      "step": 5211,
      "training_loss": 7.021738052368164
    },
    {
      "epoch": 0.2824932249322493,
      "grad_norm": 27.06585121154785,
      "learning_rate": 1e-05,
      "loss": 7.1345,
      "step": 5212
    },
    {
      "epoch": 0.2824932249322493,
      "step": 5212,
      "training_loss": 6.187038421630859
    },
    {
      "epoch": 0.28254742547425477,
      "step": 5213,
      "training_loss": 6.556807994842529
    },
    {
      "epoch": 0.28260162601626015,
      "step": 5214,
      "training_loss": 7.579919338226318
    },
    {
      "epoch": 0.2826558265582656,
      "step": 5215,
      "training_loss": 7.545155048370361
    },
    {
      "epoch": 0.282710027100271,
      "grad_norm": 31.178661346435547,
      "learning_rate": 1e-05,
      "loss": 6.9672,
      "step": 5216
    },
    {
      "epoch": 0.282710027100271,
      "step": 5216,
      "training_loss": 7.667713642120361
    },
    {
      "epoch": 0.28276422764227643,
      "step": 5217,
      "training_loss": 7.4069037437438965
    },
    {
      "epoch": 0.2828184281842818,
      "step": 5218,
      "training_loss": 4.635534763336182
    },
    {
      "epoch": 0.28287262872628727,
      "step": 5219,
      "training_loss": 5.5687665939331055
    },
    {
      "epoch": 0.28292682926829266,
      "grad_norm": 23.188932418823242,
      "learning_rate": 1e-05,
      "loss": 6.3197,
      "step": 5220
    },
    {
      "epoch": 0.28292682926829266,
      "step": 5220,
      "training_loss": 4.1709675788879395
    },
    {
      "epoch": 0.2829810298102981,
      "step": 5221,
      "training_loss": 6.814461708068848
    },
    {
      "epoch": 0.28303523035230355,
      "step": 5222,
      "training_loss": 7.102078437805176
    },
    {
      "epoch": 0.28308943089430894,
      "step": 5223,
      "training_loss": 7.401483058929443
    },
    {
      "epoch": 0.2831436314363144,
      "grad_norm": 31.887855529785156,
      "learning_rate": 1e-05,
      "loss": 6.3722,
      "step": 5224
    },
    {
      "epoch": 0.2831436314363144,
      "step": 5224,
      "training_loss": 7.350093364715576
    },
    {
      "epoch": 0.28319783197831977,
      "step": 5225,
      "training_loss": 6.1855034828186035
    },
    {
      "epoch": 0.2832520325203252,
      "step": 5226,
      "training_loss": 7.006816864013672
    },
    {
      "epoch": 0.2833062330623306,
      "step": 5227,
      "training_loss": 6.887613773345947
    },
    {
      "epoch": 0.28336043360433605,
      "grad_norm": 42.62064743041992,
      "learning_rate": 1e-05,
      "loss": 6.8575,
      "step": 5228
    },
    {
      "epoch": 0.28336043360433605,
      "step": 5228,
      "training_loss": 6.0656843185424805
    },
    {
      "epoch": 0.28341463414634144,
      "step": 5229,
      "training_loss": 7.7234110832214355
    },
    {
      "epoch": 0.2834688346883469,
      "step": 5230,
      "training_loss": 6.922615051269531
    },
    {
      "epoch": 0.2835230352303523,
      "step": 5231,
      "training_loss": 6.5160908699035645
    },
    {
      "epoch": 0.2835772357723577,
      "grad_norm": 17.85529136657715,
      "learning_rate": 1e-05,
      "loss": 6.807,
      "step": 5232
    },
    {
      "epoch": 0.2835772357723577,
      "step": 5232,
      "training_loss": 6.273229598999023
    },
    {
      "epoch": 0.28363143631436316,
      "step": 5233,
      "training_loss": 6.816527366638184
    },
    {
      "epoch": 0.28368563685636855,
      "step": 5234,
      "training_loss": 8.570627212524414
    },
    {
      "epoch": 0.283739837398374,
      "step": 5235,
      "training_loss": 6.764116287231445
    },
    {
      "epoch": 0.2837940379403794,
      "grad_norm": 21.80604362487793,
      "learning_rate": 1e-05,
      "loss": 7.1061,
      "step": 5236
    },
    {
      "epoch": 0.2837940379403794,
      "step": 5236,
      "training_loss": 6.635782241821289
    },
    {
      "epoch": 0.28384823848238483,
      "step": 5237,
      "training_loss": 6.256566524505615
    },
    {
      "epoch": 0.2839024390243902,
      "step": 5238,
      "training_loss": 5.38894510269165
    },
    {
      "epoch": 0.28395663956639566,
      "step": 5239,
      "training_loss": 7.5237908363342285
    },
    {
      "epoch": 0.2840108401084011,
      "grad_norm": 22.22565269470215,
      "learning_rate": 1e-05,
      "loss": 6.4513,
      "step": 5240
    },
    {
      "epoch": 0.2840108401084011,
      "step": 5240,
      "training_loss": 6.651532173156738
    },
    {
      "epoch": 0.2840650406504065,
      "step": 5241,
      "training_loss": 6.7688117027282715
    },
    {
      "epoch": 0.28411924119241194,
      "step": 5242,
      "training_loss": 7.012316703796387
    },
    {
      "epoch": 0.28417344173441733,
      "step": 5243,
      "training_loss": 7.42957878112793
    },
    {
      "epoch": 0.2842276422764228,
      "grad_norm": 18.078596115112305,
      "learning_rate": 1e-05,
      "loss": 6.9656,
      "step": 5244
    },
    {
      "epoch": 0.2842276422764228,
      "step": 5244,
      "training_loss": 7.1429762840271
    },
    {
      "epoch": 0.28428184281842817,
      "step": 5245,
      "training_loss": 6.842650413513184
    },
    {
      "epoch": 0.2843360433604336,
      "step": 5246,
      "training_loss": 7.20407772064209
    },
    {
      "epoch": 0.284390243902439,
      "step": 5247,
      "training_loss": 6.572466850280762
    },
    {
      "epoch": 0.28444444444444444,
      "grad_norm": 25.562299728393555,
      "learning_rate": 1e-05,
      "loss": 6.9405,
      "step": 5248
    },
    {
      "epoch": 0.28444444444444444,
      "step": 5248,
      "training_loss": 6.074676990509033
    },
    {
      "epoch": 0.2844986449864499,
      "step": 5249,
      "training_loss": 7.043507099151611
    },
    {
      "epoch": 0.2845528455284553,
      "step": 5250,
      "training_loss": 6.2238335609436035
    },
    {
      "epoch": 0.2846070460704607,
      "step": 5251,
      "training_loss": 6.5402727127075195
    },
    {
      "epoch": 0.2846612466124661,
      "grad_norm": 23.977296829223633,
      "learning_rate": 1e-05,
      "loss": 6.4706,
      "step": 5252
    },
    {
      "epoch": 0.2846612466124661,
      "step": 5252,
      "training_loss": 6.156836032867432
    },
    {
      "epoch": 0.28471544715447156,
      "step": 5253,
      "training_loss": 6.554589748382568
    },
    {
      "epoch": 0.28476964769647695,
      "step": 5254,
      "training_loss": 5.548050403594971
    },
    {
      "epoch": 0.2848238482384824,
      "step": 5255,
      "training_loss": 8.805357933044434
    },
    {
      "epoch": 0.2848780487804878,
      "grad_norm": 30.57151222229004,
      "learning_rate": 1e-05,
      "loss": 6.7662,
      "step": 5256
    },
    {
      "epoch": 0.2848780487804878,
      "step": 5256,
      "training_loss": 6.236405372619629
    },
    {
      "epoch": 0.2849322493224932,
      "step": 5257,
      "training_loss": 6.987600326538086
    },
    {
      "epoch": 0.28498644986449867,
      "step": 5258,
      "training_loss": 5.1495513916015625
    },
    {
      "epoch": 0.28504065040650406,
      "step": 5259,
      "training_loss": 7.092639446258545
    },
    {
      "epoch": 0.2850948509485095,
      "grad_norm": 36.871376037597656,
      "learning_rate": 1e-05,
      "loss": 6.3665,
      "step": 5260
    },
    {
      "epoch": 0.2850948509485095,
      "step": 5260,
      "training_loss": 8.083032608032227
    },
    {
      "epoch": 0.2851490514905149,
      "step": 5261,
      "training_loss": 4.392541885375977
    },
    {
      "epoch": 0.28520325203252034,
      "step": 5262,
      "training_loss": 7.419931888580322
    },
    {
      "epoch": 0.2852574525745257,
      "step": 5263,
      "training_loss": 5.5116682052612305
    },
    {
      "epoch": 0.28531165311653117,
      "grad_norm": 23.41350555419922,
      "learning_rate": 1e-05,
      "loss": 6.3518,
      "step": 5264
    },
    {
      "epoch": 0.28531165311653117,
      "step": 5264,
      "training_loss": 7.548313140869141
    },
    {
      "epoch": 0.28536585365853656,
      "step": 5265,
      "training_loss": 6.520968914031982
    },
    {
      "epoch": 0.285420054200542,
      "step": 5266,
      "training_loss": 6.9198174476623535
    },
    {
      "epoch": 0.28547425474254745,
      "step": 5267,
      "training_loss": 6.782238006591797
    },
    {
      "epoch": 0.28552845528455284,
      "grad_norm": 17.492534637451172,
      "learning_rate": 1e-05,
      "loss": 6.9428,
      "step": 5268
    },
    {
      "epoch": 0.28552845528455284,
      "step": 5268,
      "training_loss": 7.270510196685791
    },
    {
      "epoch": 0.2855826558265583,
      "step": 5269,
      "training_loss": 7.786079406738281
    },
    {
      "epoch": 0.2856368563685637,
      "step": 5270,
      "training_loss": 7.101654052734375
    },
    {
      "epoch": 0.2856910569105691,
      "step": 5271,
      "training_loss": 7.642735004425049
    },
    {
      "epoch": 0.2857452574525745,
      "grad_norm": 24.60482406616211,
      "learning_rate": 1e-05,
      "loss": 7.4502,
      "step": 5272
    },
    {
      "epoch": 0.2857452574525745,
      "step": 5272,
      "training_loss": 6.340524196624756
    },
    {
      "epoch": 0.28579945799457995,
      "step": 5273,
      "training_loss": 7.765787124633789
    },
    {
      "epoch": 0.28585365853658534,
      "step": 5274,
      "training_loss": 8.387922286987305
    },
    {
      "epoch": 0.2859078590785908,
      "step": 5275,
      "training_loss": 6.0428853034973145
    },
    {
      "epoch": 0.28596205962059623,
      "grad_norm": 19.092060089111328,
      "learning_rate": 1e-05,
      "loss": 7.1343,
      "step": 5276
    },
    {
      "epoch": 0.28596205962059623,
      "step": 5276,
      "training_loss": 7.413949966430664
    },
    {
      "epoch": 0.2860162601626016,
      "step": 5277,
      "training_loss": 6.7883172035217285
    },
    {
      "epoch": 0.28607046070460707,
      "step": 5278,
      "training_loss": 5.374650955200195
    },
    {
      "epoch": 0.28612466124661246,
      "step": 5279,
      "training_loss": 7.0377421379089355
    },
    {
      "epoch": 0.2861788617886179,
      "grad_norm": 20.909955978393555,
      "learning_rate": 1e-05,
      "loss": 6.6537,
      "step": 5280
    },
    {
      "epoch": 0.2861788617886179,
      "step": 5280,
      "training_loss": 7.689935684204102
    },
    {
      "epoch": 0.2862330623306233,
      "step": 5281,
      "training_loss": 6.01671838760376
    },
    {
      "epoch": 0.28628726287262873,
      "step": 5282,
      "training_loss": 7.1751227378845215
    },
    {
      "epoch": 0.2863414634146341,
      "step": 5283,
      "training_loss": 7.382654190063477
    },
    {
      "epoch": 0.28639566395663957,
      "grad_norm": 27.168909072875977,
      "learning_rate": 1e-05,
      "loss": 7.0661,
      "step": 5284
    },
    {
      "epoch": 0.28639566395663957,
      "step": 5284,
      "training_loss": 6.783395767211914
    },
    {
      "epoch": 0.286449864498645,
      "step": 5285,
      "training_loss": 6.651967525482178
    },
    {
      "epoch": 0.2865040650406504,
      "step": 5286,
      "training_loss": 6.406911373138428
    },
    {
      "epoch": 0.28655826558265585,
      "step": 5287,
      "training_loss": 6.967129230499268
    },
    {
      "epoch": 0.28661246612466124,
      "grad_norm": 19.180513381958008,
      "learning_rate": 1e-05,
      "loss": 6.7024,
      "step": 5288
    },
    {
      "epoch": 0.28661246612466124,
      "step": 5288,
      "training_loss": 6.628426551818848
    },
    {
      "epoch": 0.2866666666666667,
      "step": 5289,
      "training_loss": 6.829556465148926
    },
    {
      "epoch": 0.28672086720867207,
      "step": 5290,
      "training_loss": 9.073548316955566
    },
    {
      "epoch": 0.2867750677506775,
      "step": 5291,
      "training_loss": 6.309055328369141
    },
    {
      "epoch": 0.2868292682926829,
      "grad_norm": 21.336000442504883,
      "learning_rate": 1e-05,
      "loss": 7.2101,
      "step": 5292
    },
    {
      "epoch": 0.2868292682926829,
      "step": 5292,
      "training_loss": 7.1395792961120605
    },
    {
      "epoch": 0.28688346883468835,
      "step": 5293,
      "training_loss": 7.018993377685547
    },
    {
      "epoch": 0.2869376693766938,
      "step": 5294,
      "training_loss": 7.349099159240723
    },
    {
      "epoch": 0.2869918699186992,
      "step": 5295,
      "training_loss": 7.6808366775512695
    },
    {
      "epoch": 0.2870460704607046,
      "grad_norm": 27.94015884399414,
      "learning_rate": 1e-05,
      "loss": 7.2971,
      "step": 5296
    },
    {
      "epoch": 0.2870460704607046,
      "step": 5296,
      "training_loss": 6.661665916442871
    },
    {
      "epoch": 0.28710027100271,
      "step": 5297,
      "training_loss": 3.640577793121338
    },
    {
      "epoch": 0.28715447154471546,
      "step": 5298,
      "training_loss": 7.739356517791748
    },
    {
      "epoch": 0.28720867208672085,
      "step": 5299,
      "training_loss": 6.204237937927246
    },
    {
      "epoch": 0.2872628726287263,
      "grad_norm": 48.14756393432617,
      "learning_rate": 1e-05,
      "loss": 6.0615,
      "step": 5300
    },
    {
      "epoch": 0.2872628726287263,
      "step": 5300,
      "training_loss": 6.85186243057251
    },
    {
      "epoch": 0.2873170731707317,
      "step": 5301,
      "training_loss": 7.671169757843018
    },
    {
      "epoch": 0.28737127371273713,
      "step": 5302,
      "training_loss": 8.179996490478516
    },
    {
      "epoch": 0.2874254742547426,
      "step": 5303,
      "training_loss": 6.823071002960205
    },
    {
      "epoch": 0.28747967479674796,
      "grad_norm": 40.712867736816406,
      "learning_rate": 1e-05,
      "loss": 7.3815,
      "step": 5304
    },
    {
      "epoch": 0.28747967479674796,
      "step": 5304,
      "training_loss": 6.155437469482422
    },
    {
      "epoch": 0.2875338753387534,
      "step": 5305,
      "training_loss": 6.137989044189453
    },
    {
      "epoch": 0.2875880758807588,
      "step": 5306,
      "training_loss": 7.15587854385376
    },
    {
      "epoch": 0.28764227642276424,
      "step": 5307,
      "training_loss": 8.414701461791992
    },
    {
      "epoch": 0.28769647696476963,
      "grad_norm": 78.17243194580078,
      "learning_rate": 1e-05,
      "loss": 6.966,
      "step": 5308
    },
    {
      "epoch": 0.28769647696476963,
      "step": 5308,
      "training_loss": 6.297510147094727
    },
    {
      "epoch": 0.2877506775067751,
      "step": 5309,
      "training_loss": 6.956943988800049
    },
    {
      "epoch": 0.28780487804878047,
      "step": 5310,
      "training_loss": 7.547102928161621
    },
    {
      "epoch": 0.2878590785907859,
      "step": 5311,
      "training_loss": 3.8531954288482666
    },
    {
      "epoch": 0.28791327913279136,
      "grad_norm": 26.05545997619629,
      "learning_rate": 1e-05,
      "loss": 6.1637,
      "step": 5312
    },
    {
      "epoch": 0.28791327913279136,
      "step": 5312,
      "training_loss": 7.009917259216309
    },
    {
      "epoch": 0.28796747967479674,
      "step": 5313,
      "training_loss": 6.706622123718262
    },
    {
      "epoch": 0.2880216802168022,
      "step": 5314,
      "training_loss": 6.684818744659424
    },
    {
      "epoch": 0.2880758807588076,
      "step": 5315,
      "training_loss": 5.638955116271973
    },
    {
      "epoch": 0.288130081300813,
      "grad_norm": 21.3287353515625,
      "learning_rate": 1e-05,
      "loss": 6.5101,
      "step": 5316
    },
    {
      "epoch": 0.288130081300813,
      "step": 5316,
      "training_loss": 7.250980377197266
    },
    {
      "epoch": 0.2881842818428184,
      "step": 5317,
      "training_loss": 6.224137783050537
    },
    {
      "epoch": 0.28823848238482386,
      "step": 5318,
      "training_loss": 6.468808650970459
    },
    {
      "epoch": 0.28829268292682925,
      "step": 5319,
      "training_loss": 6.701232433319092
    },
    {
      "epoch": 0.2883468834688347,
      "grad_norm": 18.970199584960938,
      "learning_rate": 1e-05,
      "loss": 6.6613,
      "step": 5320
    },
    {
      "epoch": 0.2883468834688347,
      "step": 5320,
      "training_loss": 6.83026123046875
    },
    {
      "epoch": 0.2884010840108401,
      "step": 5321,
      "training_loss": 6.868050575256348
    },
    {
      "epoch": 0.2884552845528455,
      "step": 5322,
      "training_loss": 7.054967403411865
    },
    {
      "epoch": 0.28850948509485097,
      "step": 5323,
      "training_loss": 7.402952194213867
    },
    {
      "epoch": 0.28856368563685636,
      "grad_norm": 22.500350952148438,
      "learning_rate": 1e-05,
      "loss": 7.0391,
      "step": 5324
    },
    {
      "epoch": 0.28856368563685636,
      "step": 5324,
      "training_loss": 7.40770959854126
    },
    {
      "epoch": 0.2886178861788618,
      "step": 5325,
      "training_loss": 7.596284866333008
    },
    {
      "epoch": 0.2886720867208672,
      "step": 5326,
      "training_loss": 4.44111967086792
    },
    {
      "epoch": 0.28872628726287264,
      "step": 5327,
      "training_loss": 7.2621846199035645
    },
    {
      "epoch": 0.288780487804878,
      "grad_norm": 27.319278717041016,
      "learning_rate": 1e-05,
      "loss": 6.6768,
      "step": 5328
    },
    {
      "epoch": 0.288780487804878,
      "step": 5328,
      "training_loss": 6.463940620422363
    },
    {
      "epoch": 0.2888346883468835,
      "step": 5329,
      "training_loss": 8.231742858886719
    },
    {
      "epoch": 0.28888888888888886,
      "step": 5330,
      "training_loss": 7.187008857727051
    },
    {
      "epoch": 0.2889430894308943,
      "step": 5331,
      "training_loss": 6.863292217254639
    },
    {
      "epoch": 0.28899728997289975,
      "grad_norm": 17.677885055541992,
      "learning_rate": 1e-05,
      "loss": 7.1865,
      "step": 5332
    },
    {
      "epoch": 0.28899728997289975,
      "step": 5332,
      "training_loss": 6.664202690124512
    },
    {
      "epoch": 0.28905149051490514,
      "step": 5333,
      "training_loss": 6.469142436981201
    },
    {
      "epoch": 0.2891056910569106,
      "step": 5334,
      "training_loss": 6.693777561187744
    },
    {
      "epoch": 0.289159891598916,
      "step": 5335,
      "training_loss": 7.494684219360352
    },
    {
      "epoch": 0.2892140921409214,
      "grad_norm": 20.342342376708984,
      "learning_rate": 1e-05,
      "loss": 6.8305,
      "step": 5336
    },
    {
      "epoch": 0.2892140921409214,
      "step": 5336,
      "training_loss": 7.786673545837402
    },
    {
      "epoch": 0.2892682926829268,
      "step": 5337,
      "training_loss": 4.48774528503418
    },
    {
      "epoch": 0.28932249322493225,
      "step": 5338,
      "training_loss": 7.496335983276367
    },
    {
      "epoch": 0.28937669376693764,
      "step": 5339,
      "training_loss": 6.379673480987549
    },
    {
      "epoch": 0.2894308943089431,
      "grad_norm": 41.221282958984375,
      "learning_rate": 1e-05,
      "loss": 6.5376,
      "step": 5340
    },
    {
      "epoch": 0.2894308943089431,
      "step": 5340,
      "training_loss": 6.394215106964111
    },
    {
      "epoch": 0.28948509485094853,
      "step": 5341,
      "training_loss": 7.209780693054199
    },
    {
      "epoch": 0.2895392953929539,
      "step": 5342,
      "training_loss": 6.103243350982666
    },
    {
      "epoch": 0.28959349593495937,
      "step": 5343,
      "training_loss": 5.598082542419434
    },
    {
      "epoch": 0.28964769647696476,
      "grad_norm": 37.81312561035156,
      "learning_rate": 1e-05,
      "loss": 6.3263,
      "step": 5344
    },
    {
      "epoch": 0.28964769647696476,
      "step": 5344,
      "training_loss": 7.411798000335693
    },
    {
      "epoch": 0.2897018970189702,
      "step": 5345,
      "training_loss": 7.15424919128418
    },
    {
      "epoch": 0.2897560975609756,
      "step": 5346,
      "training_loss": 7.311041355133057
    },
    {
      "epoch": 0.28981029810298103,
      "step": 5347,
      "training_loss": 6.543461799621582
    },
    {
      "epoch": 0.2898644986449864,
      "grad_norm": 17.882768630981445,
      "learning_rate": 1e-05,
      "loss": 7.1051,
      "step": 5348
    },
    {
      "epoch": 0.2898644986449864,
      "step": 5348,
      "training_loss": 6.930213451385498
    },
    {
      "epoch": 0.28991869918699187,
      "step": 5349,
      "training_loss": 6.931403636932373
    },
    {
      "epoch": 0.2899728997289973,
      "step": 5350,
      "training_loss": 7.70989465713501
    },
    {
      "epoch": 0.2900271002710027,
      "step": 5351,
      "training_loss": 6.400733947753906
    },
    {
      "epoch": 0.29008130081300815,
      "grad_norm": 25.23141098022461,
      "learning_rate": 1e-05,
      "loss": 6.9931,
      "step": 5352
    },
    {
      "epoch": 0.29008130081300815,
      "step": 5352,
      "training_loss": 8.181900024414062
    },
    {
      "epoch": 0.29013550135501354,
      "step": 5353,
      "training_loss": 6.972903728485107
    },
    {
      "epoch": 0.290189701897019,
      "step": 5354,
      "training_loss": 6.9252400398254395
    },
    {
      "epoch": 0.29024390243902437,
      "step": 5355,
      "training_loss": 6.932626247406006
    },
    {
      "epoch": 0.2902981029810298,
      "grad_norm": 17.846702575683594,
      "learning_rate": 1e-05,
      "loss": 7.2532,
      "step": 5356
    },
    {
      "epoch": 0.2902981029810298,
      "step": 5356,
      "training_loss": 5.080899238586426
    },
    {
      "epoch": 0.2903523035230352,
      "step": 5357,
      "training_loss": 7.067171573638916
    },
    {
      "epoch": 0.29040650406504065,
      "step": 5358,
      "training_loss": 5.63410758972168
    },
    {
      "epoch": 0.2904607046070461,
      "step": 5359,
      "training_loss": 6.71927547454834
    },
    {
      "epoch": 0.2905149051490515,
      "grad_norm": 25.468624114990234,
      "learning_rate": 1e-05,
      "loss": 6.1254,
      "step": 5360
    },
    {
      "epoch": 0.2905149051490515,
      "step": 5360,
      "training_loss": 9.797452926635742
    },
    {
      "epoch": 0.29056910569105693,
      "step": 5361,
      "training_loss": 7.896750450134277
    },
    {
      "epoch": 0.2906233062330623,
      "step": 5362,
      "training_loss": 6.997118949890137
    },
    {
      "epoch": 0.29067750677506776,
      "step": 5363,
      "training_loss": 7.469674110412598
    },
    {
      "epoch": 0.29073170731707315,
      "grad_norm": 20.753643035888672,
      "learning_rate": 1e-05,
      "loss": 8.0402,
      "step": 5364
    },
    {
      "epoch": 0.29073170731707315,
      "step": 5364,
      "training_loss": 5.922784805297852
    },
    {
      "epoch": 0.2907859078590786,
      "step": 5365,
      "training_loss": 7.277478218078613
    },
    {
      "epoch": 0.290840108401084,
      "step": 5366,
      "training_loss": 6.92458438873291
    },
    {
      "epoch": 0.29089430894308943,
      "step": 5367,
      "training_loss": 6.937494277954102
    },
    {
      "epoch": 0.2909485094850949,
      "grad_norm": 17.402557373046875,
      "learning_rate": 1e-05,
      "loss": 6.7656,
      "step": 5368
    },
    {
      "epoch": 0.2909485094850949,
      "step": 5368,
      "training_loss": 7.587395668029785
    },
    {
      "epoch": 0.29100271002710026,
      "step": 5369,
      "training_loss": 8.406533241271973
    },
    {
      "epoch": 0.2910569105691057,
      "step": 5370,
      "training_loss": 6.470522880554199
    },
    {
      "epoch": 0.2911111111111111,
      "step": 5371,
      "training_loss": 6.343951225280762
    },
    {
      "epoch": 0.29116531165311654,
      "grad_norm": 22.623754501342773,
      "learning_rate": 1e-05,
      "loss": 7.2021,
      "step": 5372
    },
    {
      "epoch": 0.29116531165311654,
      "step": 5372,
      "training_loss": 7.1260223388671875
    },
    {
      "epoch": 0.29121951219512193,
      "step": 5373,
      "training_loss": 6.817702770233154
    },
    {
      "epoch": 0.2912737127371274,
      "step": 5374,
      "training_loss": 4.984694957733154
    },
    {
      "epoch": 0.29132791327913277,
      "step": 5375,
      "training_loss": 6.888352870941162
    },
    {
      "epoch": 0.2913821138211382,
      "grad_norm": 28.936830520629883,
      "learning_rate": 1e-05,
      "loss": 6.4542,
      "step": 5376
    },
    {
      "epoch": 0.2913821138211382,
      "step": 5376,
      "training_loss": 4.702929973602295
    },
    {
      "epoch": 0.29143631436314366,
      "step": 5377,
      "training_loss": 6.706864356994629
    },
    {
      "epoch": 0.29149051490514905,
      "step": 5378,
      "training_loss": 6.540095329284668
    },
    {
      "epoch": 0.2915447154471545,
      "step": 5379,
      "training_loss": 6.9532952308654785
    },
    {
      "epoch": 0.2915989159891599,
      "grad_norm": 44.36650085449219,
      "learning_rate": 1e-05,
      "loss": 6.2258,
      "step": 5380
    },
    {
      "epoch": 0.2915989159891599,
      "step": 5380,
      "training_loss": 6.387392520904541
    },
    {
      "epoch": 0.2916531165311653,
      "step": 5381,
      "training_loss": 7.2492170333862305
    },
    {
      "epoch": 0.2917073170731707,
      "step": 5382,
      "training_loss": 4.036409378051758
    },
    {
      "epoch": 0.29176151761517616,
      "step": 5383,
      "training_loss": 7.6299848556518555
    },
    {
      "epoch": 0.29181571815718155,
      "grad_norm": 24.41399574279785,
      "learning_rate": 1e-05,
      "loss": 6.3258,
      "step": 5384
    },
    {
      "epoch": 0.29181571815718155,
      "step": 5384,
      "training_loss": 6.632418632507324
    },
    {
      "epoch": 0.291869918699187,
      "step": 5385,
      "training_loss": 6.2361931800842285
    },
    {
      "epoch": 0.29192411924119244,
      "step": 5386,
      "training_loss": 7.371770858764648
    },
    {
      "epoch": 0.2919783197831978,
      "step": 5387,
      "training_loss": 6.970407962799072
    },
    {
      "epoch": 0.29203252032520327,
      "grad_norm": 33.87522506713867,
      "learning_rate": 1e-05,
      "loss": 6.8027,
      "step": 5388
    },
    {
      "epoch": 0.29203252032520327,
      "step": 5388,
      "training_loss": 6.624364376068115
    },
    {
      "epoch": 0.29208672086720866,
      "step": 5389,
      "training_loss": 6.542552471160889
    },
    {
      "epoch": 0.2921409214092141,
      "step": 5390,
      "training_loss": 7.375150680541992
    },
    {
      "epoch": 0.2921951219512195,
      "step": 5391,
      "training_loss": 6.9036641120910645
    },
    {
      "epoch": 0.29224932249322494,
      "grad_norm": 22.645727157592773,
      "learning_rate": 1e-05,
      "loss": 6.8614,
      "step": 5392
    },
    {
      "epoch": 0.29224932249322494,
      "step": 5392,
      "training_loss": 7.154378890991211
    },
    {
      "epoch": 0.29230352303523033,
      "step": 5393,
      "training_loss": 6.4620256423950195
    },
    {
      "epoch": 0.2923577235772358,
      "step": 5394,
      "training_loss": 6.520908832550049
    },
    {
      "epoch": 0.2924119241192412,
      "step": 5395,
      "training_loss": 4.332579135894775
    },
    {
      "epoch": 0.2924661246612466,
      "grad_norm": 24.443540573120117,
      "learning_rate": 1e-05,
      "loss": 6.1175,
      "step": 5396
    },
    {
      "epoch": 0.2924661246612466,
      "step": 5396,
      "training_loss": 6.589319705963135
    },
    {
      "epoch": 0.29252032520325205,
      "step": 5397,
      "training_loss": 6.474750518798828
    },
    {
      "epoch": 0.29257452574525744,
      "step": 5398,
      "training_loss": 7.461750507354736
    },
    {
      "epoch": 0.2926287262872629,
      "step": 5399,
      "training_loss": 7.071953773498535
    },
    {
      "epoch": 0.2926829268292683,
      "grad_norm": 39.68954086303711,
      "learning_rate": 1e-05,
      "loss": 6.8994,
      "step": 5400
    },
    {
      "epoch": 0.2926829268292683,
      "step": 5400,
      "training_loss": 9.482519149780273
    },
    {
      "epoch": 0.2927371273712737,
      "step": 5401,
      "training_loss": 7.418614864349365
    },
    {
      "epoch": 0.2927913279132791,
      "step": 5402,
      "training_loss": 6.50262975692749
    },
    {
      "epoch": 0.29284552845528455,
      "step": 5403,
      "training_loss": 6.139232158660889
    },
    {
      "epoch": 0.29289972899729,
      "grad_norm": 25.16407585144043,
      "learning_rate": 1e-05,
      "loss": 7.3857,
      "step": 5404
    },
    {
      "epoch": 0.29289972899729,
      "step": 5404,
      "training_loss": 6.533246040344238
    },
    {
      "epoch": 0.2929539295392954,
      "step": 5405,
      "training_loss": 7.16412353515625
    },
    {
      "epoch": 0.29300813008130083,
      "step": 5406,
      "training_loss": 7.42618989944458
    },
    {
      "epoch": 0.2930623306233062,
      "step": 5407,
      "training_loss": 7.0395989418029785
    },
    {
      "epoch": 0.29311653116531167,
      "grad_norm": 24.8836727142334,
      "learning_rate": 1e-05,
      "loss": 7.0408,
      "step": 5408
    },
    {
      "epoch": 0.29311653116531167,
      "step": 5408,
      "training_loss": 5.691144943237305
    },
    {
      "epoch": 0.29317073170731706,
      "step": 5409,
      "training_loss": 7.244941234588623
    },
    {
      "epoch": 0.2932249322493225,
      "step": 5410,
      "training_loss": 7.340455532073975
    },
    {
      "epoch": 0.2932791327913279,
      "step": 5411,
      "training_loss": 7.3873395919799805
    },
    {
      "epoch": 0.29333333333333333,
      "grad_norm": 27.80739402770996,
      "learning_rate": 1e-05,
      "loss": 6.916,
      "step": 5412
    },
    {
      "epoch": 0.29333333333333333,
      "step": 5412,
      "training_loss": 6.593052864074707
    },
    {
      "epoch": 0.2933875338753388,
      "step": 5413,
      "training_loss": 8.23981761932373
    },
    {
      "epoch": 0.29344173441734417,
      "step": 5414,
      "training_loss": 8.105524063110352
    },
    {
      "epoch": 0.2934959349593496,
      "step": 5415,
      "training_loss": 7.857624530792236
    },
    {
      "epoch": 0.293550135501355,
      "grad_norm": 33.67546463012695,
      "learning_rate": 1e-05,
      "loss": 7.699,
      "step": 5416
    },
    {
      "epoch": 0.293550135501355,
      "step": 5416,
      "training_loss": 7.672733783721924
    },
    {
      "epoch": 0.29360433604336045,
      "step": 5417,
      "training_loss": 6.510993957519531
    },
    {
      "epoch": 0.29365853658536584,
      "step": 5418,
      "training_loss": 6.485279083251953
    },
    {
      "epoch": 0.2937127371273713,
      "step": 5419,
      "training_loss": 6.819220542907715
    },
    {
      "epoch": 0.29376693766937667,
      "grad_norm": 23.00949478149414,
      "learning_rate": 1e-05,
      "loss": 6.8721,
      "step": 5420
    },
    {
      "epoch": 0.29376693766937667,
      "step": 5420,
      "training_loss": 4.885075569152832
    },
    {
      "epoch": 0.2938211382113821,
      "step": 5421,
      "training_loss": 6.473326683044434
    },
    {
      "epoch": 0.29387533875338756,
      "step": 5422,
      "training_loss": 7.925174713134766
    },
    {
      "epoch": 0.29392953929539295,
      "step": 5423,
      "training_loss": 7.347281455993652
    },
    {
      "epoch": 0.2939837398373984,
      "grad_norm": 32.032718658447266,
      "learning_rate": 1e-05,
      "loss": 6.6577,
      "step": 5424
    },
    {
      "epoch": 0.2939837398373984,
      "step": 5424,
      "training_loss": 7.257643699645996
    },
    {
      "epoch": 0.2940379403794038,
      "step": 5425,
      "training_loss": 5.469919681549072
    },
    {
      "epoch": 0.29409214092140923,
      "step": 5426,
      "training_loss": 8.010518074035645
    },
    {
      "epoch": 0.2941463414634146,
      "step": 5427,
      "training_loss": 6.140584945678711
    },
    {
      "epoch": 0.29420054200542006,
      "grad_norm": 24.184797286987305,
      "learning_rate": 1e-05,
      "loss": 6.7197,
      "step": 5428
    },
    {
      "epoch": 0.29420054200542006,
      "step": 5428,
      "training_loss": 6.17633581161499
    },
    {
      "epoch": 0.29425474254742545,
      "step": 5429,
      "training_loss": 6.6990885734558105
    },
    {
      "epoch": 0.2943089430894309,
      "step": 5430,
      "training_loss": 7.710830211639404
    },
    {
      "epoch": 0.29436314363143634,
      "step": 5431,
      "training_loss": 5.8875532150268555
    },
    {
      "epoch": 0.29441734417344173,
      "grad_norm": 24.823833465576172,
      "learning_rate": 1e-05,
      "loss": 6.6185,
      "step": 5432
    },
    {
      "epoch": 0.29441734417344173,
      "step": 5432,
      "training_loss": 7.469133377075195
    },
    {
      "epoch": 0.2944715447154472,
      "step": 5433,
      "training_loss": 6.711974620819092
    },
    {
      "epoch": 0.29452574525745256,
      "step": 5434,
      "training_loss": 7.265709400177002
    },
    {
      "epoch": 0.294579945799458,
      "step": 5435,
      "training_loss": 7.589774131774902
    },
    {
      "epoch": 0.2946341463414634,
      "grad_norm": 16.761837005615234,
      "learning_rate": 1e-05,
      "loss": 7.2591,
      "step": 5436
    },
    {
      "epoch": 0.2946341463414634,
      "step": 5436,
      "training_loss": 6.662492752075195
    },
    {
      "epoch": 0.29468834688346884,
      "step": 5437,
      "training_loss": 6.290999889373779
    },
    {
      "epoch": 0.29474254742547423,
      "step": 5438,
      "training_loss": 6.79396915435791
    },
    {
      "epoch": 0.2947967479674797,
      "step": 5439,
      "training_loss": 7.089818954467773
    },
    {
      "epoch": 0.2948509485094851,
      "grad_norm": 23.903217315673828,
      "learning_rate": 1e-05,
      "loss": 6.7093,
      "step": 5440
    },
    {
      "epoch": 0.2948509485094851,
      "step": 5440,
      "training_loss": 4.897316932678223
    },
    {
      "epoch": 0.2949051490514905,
      "step": 5441,
      "training_loss": 6.081754684448242
    },
    {
      "epoch": 0.29495934959349596,
      "step": 5442,
      "training_loss": 6.971445083618164
    },
    {
      "epoch": 0.29501355013550135,
      "step": 5443,
      "training_loss": 7.104244232177734
    },
    {
      "epoch": 0.2950677506775068,
      "grad_norm": 32.4334602355957,
      "learning_rate": 1e-05,
      "loss": 6.2637,
      "step": 5444
    },
    {
      "epoch": 0.2950677506775068,
      "step": 5444,
      "training_loss": 6.729641437530518
    },
    {
      "epoch": 0.2951219512195122,
      "step": 5445,
      "training_loss": 6.964099884033203
    },
    {
      "epoch": 0.2951761517615176,
      "step": 5446,
      "training_loss": 5.476464748382568
    },
    {
      "epoch": 0.295230352303523,
      "step": 5447,
      "training_loss": 7.421701431274414
    },
    {
      "epoch": 0.29528455284552846,
      "grad_norm": 36.65415573120117,
      "learning_rate": 1e-05,
      "loss": 6.648,
      "step": 5448
    },
    {
      "epoch": 0.29528455284552846,
      "step": 5448,
      "training_loss": 7.460057735443115
    },
    {
      "epoch": 0.29533875338753385,
      "step": 5449,
      "training_loss": 6.437042713165283
    },
    {
      "epoch": 0.2953929539295393,
      "step": 5450,
      "training_loss": 7.289834022521973
    },
    {
      "epoch": 0.29544715447154474,
      "step": 5451,
      "training_loss": 6.66705322265625
    },
    {
      "epoch": 0.2955013550135501,
      "grad_norm": 21.2213191986084,
      "learning_rate": 1e-05,
      "loss": 6.9635,
      "step": 5452
    },
    {
      "epoch": 0.2955013550135501,
      "step": 5452,
      "training_loss": 5.850754261016846
    },
    {
      "epoch": 0.29555555555555557,
      "step": 5453,
      "training_loss": 6.881796836853027
    },
    {
      "epoch": 0.29560975609756096,
      "step": 5454,
      "training_loss": 7.066958427429199
    },
    {
      "epoch": 0.2956639566395664,
      "step": 5455,
      "training_loss": 6.367775917053223
    },
    {
      "epoch": 0.2957181571815718,
      "grad_norm": 21.94205093383789,
      "learning_rate": 1e-05,
      "loss": 6.5418,
      "step": 5456
    },
    {
      "epoch": 0.2957181571815718,
      "step": 5456,
      "training_loss": 8.778267860412598
    },
    {
      "epoch": 0.29577235772357724,
      "step": 5457,
      "training_loss": 6.910949230194092
    },
    {
      "epoch": 0.29582655826558263,
      "step": 5458,
      "training_loss": 8.253515243530273
    },
    {
      "epoch": 0.2958807588075881,
      "step": 5459,
      "training_loss": 6.2879252433776855
    },
    {
      "epoch": 0.2959349593495935,
      "grad_norm": 24.878326416015625,
      "learning_rate": 1e-05,
      "loss": 7.5577,
      "step": 5460
    },
    {
      "epoch": 0.2959349593495935,
      "step": 5460,
      "training_loss": 6.262354850769043
    },
    {
      "epoch": 0.2959891598915989,
      "step": 5461,
      "training_loss": 6.437631130218506
    },
    {
      "epoch": 0.29604336043360435,
      "step": 5462,
      "training_loss": 7.129105567932129
    },
    {
      "epoch": 0.29609756097560974,
      "step": 5463,
      "training_loss": 7.29028844833374
    },
    {
      "epoch": 0.2961517615176152,
      "grad_norm": 18.525371551513672,
      "learning_rate": 1e-05,
      "loss": 6.7798,
      "step": 5464
    },
    {
      "epoch": 0.2961517615176152,
      "step": 5464,
      "training_loss": 7.168431758880615
    },
    {
      "epoch": 0.2962059620596206,
      "step": 5465,
      "training_loss": 5.124592304229736
    },
    {
      "epoch": 0.296260162601626,
      "step": 5466,
      "training_loss": 6.54768180847168
    },
    {
      "epoch": 0.2963143631436314,
      "step": 5467,
      "training_loss": 6.717886924743652
    },
    {
      "epoch": 0.29636856368563685,
      "grad_norm": 29.66290855407715,
      "learning_rate": 1e-05,
      "loss": 6.3896,
      "step": 5468
    },
    {
      "epoch": 0.29636856368563685,
      "step": 5468,
      "training_loss": 6.176183700561523
    },
    {
      "epoch": 0.2964227642276423,
      "step": 5469,
      "training_loss": 7.376357078552246
    },
    {
      "epoch": 0.2964769647696477,
      "step": 5470,
      "training_loss": 4.546340465545654
    },
    {
      "epoch": 0.29653116531165313,
      "step": 5471,
      "training_loss": 7.62912130355835
    },
    {
      "epoch": 0.2965853658536585,
      "grad_norm": 27.918865203857422,
      "learning_rate": 1e-05,
      "loss": 6.432,
      "step": 5472
    },
    {
      "epoch": 0.2965853658536585,
      "step": 5472,
      "training_loss": 6.5108489990234375
    },
    {
      "epoch": 0.29663956639566397,
      "step": 5473,
      "training_loss": 7.312901973724365
    },
    {
      "epoch": 0.29669376693766936,
      "step": 5474,
      "training_loss": 6.724462985992432
    },
    {
      "epoch": 0.2967479674796748,
      "step": 5475,
      "training_loss": 8.036532402038574
    },
    {
      "epoch": 0.2968021680216802,
      "grad_norm": 32.404720306396484,
      "learning_rate": 1e-05,
      "loss": 7.1462,
      "step": 5476
    },
    {
      "epoch": 0.2968021680216802,
      "step": 5476,
      "training_loss": 6.66433572769165
    },
    {
      "epoch": 0.29685636856368564,
      "step": 5477,
      "training_loss": 6.960991859436035
    },
    {
      "epoch": 0.2969105691056911,
      "step": 5478,
      "training_loss": 7.266385555267334
    },
    {
      "epoch": 0.29696476964769647,
      "step": 5479,
      "training_loss": 5.550258159637451
    },
    {
      "epoch": 0.2970189701897019,
      "grad_norm": 35.68106460571289,
      "learning_rate": 1e-05,
      "loss": 6.6105,
      "step": 5480
    },
    {
      "epoch": 0.2970189701897019,
      "step": 5480,
      "training_loss": 7.316122055053711
    },
    {
      "epoch": 0.2970731707317073,
      "step": 5481,
      "training_loss": 6.1057868003845215
    },
    {
      "epoch": 0.29712737127371275,
      "step": 5482,
      "training_loss": 6.1804375648498535
    },
    {
      "epoch": 0.29718157181571814,
      "step": 5483,
      "training_loss": 5.860891819000244
    },
    {
      "epoch": 0.2972357723577236,
      "grad_norm": 69.05267333984375,
      "learning_rate": 1e-05,
      "loss": 6.3658,
      "step": 5484
    },
    {
      "epoch": 0.2972357723577236,
      "step": 5484,
      "training_loss": 7.455819129943848
    },
    {
      "epoch": 0.29728997289972897,
      "step": 5485,
      "training_loss": 8.266287803649902
    },
    {
      "epoch": 0.2973441734417344,
      "step": 5486,
      "training_loss": 6.524265766143799
    },
    {
      "epoch": 0.29739837398373986,
      "step": 5487,
      "training_loss": 6.7133965492248535
    },
    {
      "epoch": 0.29745257452574525,
      "grad_norm": 25.6127872467041,
      "learning_rate": 1e-05,
      "loss": 7.2399,
      "step": 5488
    },
    {
      "epoch": 0.29745257452574525,
      "step": 5488,
      "training_loss": 6.975706577301025
    },
    {
      "epoch": 0.2975067750677507,
      "step": 5489,
      "training_loss": 4.948730945587158
    },
    {
      "epoch": 0.2975609756097561,
      "step": 5490,
      "training_loss": 6.9163360595703125
    },
    {
      "epoch": 0.29761517615176153,
      "step": 5491,
      "training_loss": 7.129118919372559
    },
    {
      "epoch": 0.2976693766937669,
      "grad_norm": 18.8866024017334,
      "learning_rate": 1e-05,
      "loss": 6.4925,
      "step": 5492
    },
    {
      "epoch": 0.2976693766937669,
      "step": 5492,
      "training_loss": 5.022350311279297
    },
    {
      "epoch": 0.29772357723577236,
      "step": 5493,
      "training_loss": 6.070078372955322
    },
    {
      "epoch": 0.29777777777777775,
      "step": 5494,
      "training_loss": 6.224246501922607
    },
    {
      "epoch": 0.2978319783197832,
      "step": 5495,
      "training_loss": 8.201390266418457
    },
    {
      "epoch": 0.29788617886178864,
      "grad_norm": 18.18089485168457,
      "learning_rate": 1e-05,
      "loss": 6.3795,
      "step": 5496
    },
    {
      "epoch": 0.29788617886178864,
      "step": 5496,
      "training_loss": 7.984183311462402
    },
    {
      "epoch": 0.29794037940379403,
      "step": 5497,
      "training_loss": 6.755579471588135
    },
    {
      "epoch": 0.2979945799457995,
      "step": 5498,
      "training_loss": 6.608581066131592
    },
    {
      "epoch": 0.29804878048780487,
      "step": 5499,
      "training_loss": 7.175459384918213
    },
    {
      "epoch": 0.2981029810298103,
      "grad_norm": 41.69963836669922,
      "learning_rate": 1e-05,
      "loss": 7.131,
      "step": 5500
    },
    {
      "epoch": 0.2981029810298103,
      "step": 5500,
      "training_loss": 7.328845024108887
    },
    {
      "epoch": 0.2981571815718157,
      "step": 5501,
      "training_loss": 4.071731090545654
    },
    {
      "epoch": 0.29821138211382114,
      "step": 5502,
      "training_loss": 6.575993061065674
    },
    {
      "epoch": 0.29826558265582653,
      "step": 5503,
      "training_loss": 4.091268539428711
    },
    {
      "epoch": 0.298319783197832,
      "grad_norm": 29.184398651123047,
      "learning_rate": 1e-05,
      "loss": 5.517,
      "step": 5504
    },
    {
      "epoch": 0.298319783197832,
      "step": 5504,
      "training_loss": 7.375997543334961
    },
    {
      "epoch": 0.2983739837398374,
      "step": 5505,
      "training_loss": 6.988004684448242
    },
    {
      "epoch": 0.2984281842818428,
      "step": 5506,
      "training_loss": 7.076620578765869
    },
    {
      "epoch": 0.29848238482384826,
      "step": 5507,
      "training_loss": 5.177403450012207
    },
    {
      "epoch": 0.29853658536585365,
      "grad_norm": 30.834688186645508,
      "learning_rate": 1e-05,
      "loss": 6.6545,
      "step": 5508
    },
    {
      "epoch": 0.29853658536585365,
      "step": 5508,
      "training_loss": 7.039413928985596
    },
    {
      "epoch": 0.2985907859078591,
      "step": 5509,
      "training_loss": 7.739176273345947
    },
    {
      "epoch": 0.2986449864498645,
      "step": 5510,
      "training_loss": 4.8256096839904785
    },
    {
      "epoch": 0.2986991869918699,
      "step": 5511,
      "training_loss": 6.697467803955078
    },
    {
      "epoch": 0.2987533875338753,
      "grad_norm": 19.76763343811035,
      "learning_rate": 1e-05,
      "loss": 6.5754,
      "step": 5512
    },
    {
      "epoch": 0.2987533875338753,
      "step": 5512,
      "training_loss": 6.049251556396484
    },
    {
      "epoch": 0.29880758807588076,
      "step": 5513,
      "training_loss": 4.8466668128967285
    },
    {
      "epoch": 0.2988617886178862,
      "step": 5514,
      "training_loss": 7.635298728942871
    },
    {
      "epoch": 0.2989159891598916,
      "step": 5515,
      "training_loss": 6.132689476013184
    },
    {
      "epoch": 0.29897018970189704,
      "grad_norm": 39.671871185302734,
      "learning_rate": 1e-05,
      "loss": 6.166,
      "step": 5516
    },
    {
      "epoch": 0.29897018970189704,
      "step": 5516,
      "training_loss": 6.974247455596924
    },
    {
      "epoch": 0.2990243902439024,
      "step": 5517,
      "training_loss": 7.380183696746826
    },
    {
      "epoch": 0.29907859078590787,
      "step": 5518,
      "training_loss": 7.232763290405273
    },
    {
      "epoch": 0.29913279132791326,
      "step": 5519,
      "training_loss": 7.783301830291748
    },
    {
      "epoch": 0.2991869918699187,
      "grad_norm": 26.479284286499023,
      "learning_rate": 1e-05,
      "loss": 7.3426,
      "step": 5520
    },
    {
      "epoch": 0.2991869918699187,
      "step": 5520,
      "training_loss": 6.702774524688721
    },
    {
      "epoch": 0.2992411924119241,
      "step": 5521,
      "training_loss": 7.255177974700928
    },
    {
      "epoch": 0.29929539295392954,
      "step": 5522,
      "training_loss": 8.21551513671875
    },
    {
      "epoch": 0.299349593495935,
      "step": 5523,
      "training_loss": 5.2431535720825195
    },
    {
      "epoch": 0.2994037940379404,
      "grad_norm": 31.80640411376953,
      "learning_rate": 1e-05,
      "loss": 6.8542,
      "step": 5524
    },
    {
      "epoch": 0.2994037940379404,
      "step": 5524,
      "training_loss": 7.038202285766602
    },
    {
      "epoch": 0.2994579945799458,
      "step": 5525,
      "training_loss": 7.91202449798584
    },
    {
      "epoch": 0.2995121951219512,
      "step": 5526,
      "training_loss": 6.446844577789307
    },
    {
      "epoch": 0.29956639566395665,
      "step": 5527,
      "training_loss": 7.139042377471924
    },
    {
      "epoch": 0.29962059620596204,
      "grad_norm": 20.05819320678711,
      "learning_rate": 1e-05,
      "loss": 7.134,
      "step": 5528
    },
    {
      "epoch": 0.29962059620596204,
      "step": 5528,
      "training_loss": 7.820009708404541
    },
    {
      "epoch": 0.2996747967479675,
      "step": 5529,
      "training_loss": 5.994859218597412
    },
    {
      "epoch": 0.2997289972899729,
      "step": 5530,
      "training_loss": 6.723250389099121
    },
    {
      "epoch": 0.2997831978319783,
      "step": 5531,
      "training_loss": 6.849370956420898
    },
    {
      "epoch": 0.29983739837398377,
      "grad_norm": 16.394268035888672,
      "learning_rate": 1e-05,
      "loss": 6.8469,
      "step": 5532
    },
    {
      "epoch": 0.29983739837398377,
      "step": 5532,
      "training_loss": 7.273368835449219
    },
    {
      "epoch": 0.29989159891598915,
      "step": 5533,
      "training_loss": 7.238754749298096
    },
    {
      "epoch": 0.2999457994579946,
      "step": 5534,
      "training_loss": 6.296139717102051
    },
    {
      "epoch": 0.3,
      "step": 5535,
      "training_loss": 6.499319553375244
    },
    {
      "epoch": 0.30005420054200543,
      "grad_norm": 34.7421875,
      "learning_rate": 1e-05,
      "loss": 6.8269,
      "step": 5536
    },
    {
      "epoch": 0.30005420054200543,
      "step": 5536,
      "training_loss": 7.484922885894775
    },
    {
      "epoch": 0.3001084010840108,
      "step": 5537,
      "training_loss": 6.314769744873047
    },
    {
      "epoch": 0.30016260162601627,
      "step": 5538,
      "training_loss": 6.730964183807373
    },
    {
      "epoch": 0.30021680216802166,
      "step": 5539,
      "training_loss": 6.804922580718994
    },
    {
      "epoch": 0.3002710027100271,
      "grad_norm": 21.85296058654785,
      "learning_rate": 1e-05,
      "loss": 6.8339,
      "step": 5540
    },
    {
      "epoch": 0.3002710027100271,
      "step": 5540,
      "training_loss": 7.180948734283447
    },
    {
      "epoch": 0.30032520325203255,
      "step": 5541,
      "training_loss": 6.544251918792725
    },
    {
      "epoch": 0.30037940379403794,
      "step": 5542,
      "training_loss": 6.958643436431885
    },
    {
      "epoch": 0.3004336043360434,
      "step": 5543,
      "training_loss": 6.361026287078857
    },
    {
      "epoch": 0.30048780487804877,
      "grad_norm": 36.71466064453125,
      "learning_rate": 1e-05,
      "loss": 6.7612,
      "step": 5544
    },
    {
      "epoch": 0.30048780487804877,
      "step": 5544,
      "training_loss": 6.8731818199157715
    },
    {
      "epoch": 0.3005420054200542,
      "step": 5545,
      "training_loss": 6.841780662536621
    },
    {
      "epoch": 0.3005962059620596,
      "step": 5546,
      "training_loss": 6.088544845581055
    },
    {
      "epoch": 0.30065040650406505,
      "step": 5547,
      "training_loss": 6.454706192016602
    },
    {
      "epoch": 0.30070460704607044,
      "grad_norm": 22.931716918945312,
      "learning_rate": 1e-05,
      "loss": 6.5646,
      "step": 5548
    },
    {
      "epoch": 0.30070460704607044,
      "step": 5548,
      "training_loss": 7.024929046630859
    },
    {
      "epoch": 0.3007588075880759,
      "step": 5549,
      "training_loss": 6.912380695343018
    },
    {
      "epoch": 0.3008130081300813,
      "step": 5550,
      "training_loss": 7.809717178344727
    },
    {
      "epoch": 0.3008672086720867,
      "step": 5551,
      "training_loss": 4.381865501403809
    },
    {
      "epoch": 0.30092140921409216,
      "grad_norm": 21.23476219177246,
      "learning_rate": 1e-05,
      "loss": 6.5322,
      "step": 5552
    },
    {
      "epoch": 0.30092140921409216,
      "step": 5552,
      "training_loss": 7.821357727050781
    },
    {
      "epoch": 0.30097560975609755,
      "step": 5553,
      "training_loss": 6.342700481414795
    },
    {
      "epoch": 0.301029810298103,
      "step": 5554,
      "training_loss": 5.828577518463135
    },
    {
      "epoch": 0.3010840108401084,
      "step": 5555,
      "training_loss": 6.992353916168213
    },
    {
      "epoch": 0.30113821138211383,
      "grad_norm": 19.846437454223633,
      "learning_rate": 1e-05,
      "loss": 6.7462,
      "step": 5556
    },
    {
      "epoch": 0.30113821138211383,
      "step": 5556,
      "training_loss": 6.98183012008667
    },
    {
      "epoch": 0.3011924119241192,
      "step": 5557,
      "training_loss": 6.840639114379883
    },
    {
      "epoch": 0.30124661246612466,
      "step": 5558,
      "training_loss": 6.654868125915527
    },
    {
      "epoch": 0.3013008130081301,
      "step": 5559,
      "training_loss": 6.637481689453125
    },
    {
      "epoch": 0.3013550135501355,
      "grad_norm": 29.733474731445312,
      "learning_rate": 1e-05,
      "loss": 6.7787,
      "step": 5560
    },
    {
      "epoch": 0.3013550135501355,
      "step": 5560,
      "training_loss": 5.09548807144165
    },
    {
      "epoch": 0.30140921409214094,
      "step": 5561,
      "training_loss": 7.296283721923828
    },
    {
      "epoch": 0.30146341463414633,
      "step": 5562,
      "training_loss": 8.032248497009277
    },
    {
      "epoch": 0.3015176151761518,
      "step": 5563,
      "training_loss": 5.314632415771484
    },
    {
      "epoch": 0.30157181571815717,
      "grad_norm": 25.621479034423828,
      "learning_rate": 1e-05,
      "loss": 6.4347,
      "step": 5564
    },
    {
      "epoch": 0.30157181571815717,
      "step": 5564,
      "training_loss": 7.616086006164551
    },
    {
      "epoch": 0.3016260162601626,
      "step": 5565,
      "training_loss": 8.321093559265137
    },
    {
      "epoch": 0.301680216802168,
      "step": 5566,
      "training_loss": 8.170673370361328
    },
    {
      "epoch": 0.30173441734417344,
      "step": 5567,
      "training_loss": 5.390249252319336
    },
    {
      "epoch": 0.3017886178861789,
      "grad_norm": 20.957359313964844,
      "learning_rate": 1e-05,
      "loss": 7.3745,
      "step": 5568
    },
    {
      "epoch": 0.3017886178861789,
      "step": 5568,
      "training_loss": 7.149376392364502
    },
    {
      "epoch": 0.3018428184281843,
      "step": 5569,
      "training_loss": 6.768954753875732
    },
    {
      "epoch": 0.3018970189701897,
      "step": 5570,
      "training_loss": 7.549567699432373
    },
    {
      "epoch": 0.3019512195121951,
      "step": 5571,
      "training_loss": 7.015838623046875
    },
    {
      "epoch": 0.30200542005420056,
      "grad_norm": 27.7188663482666,
      "learning_rate": 1e-05,
      "loss": 7.1209,
      "step": 5572
    },
    {
      "epoch": 0.30200542005420056,
      "step": 5572,
      "training_loss": 8.161271095275879
    },
    {
      "epoch": 0.30205962059620595,
      "step": 5573,
      "training_loss": 6.862929821014404
    },
    {
      "epoch": 0.3021138211382114,
      "step": 5574,
      "training_loss": 7.346306800842285
    },
    {
      "epoch": 0.3021680216802168,
      "step": 5575,
      "training_loss": 6.5987324714660645
    },
    {
      "epoch": 0.3022222222222222,
      "grad_norm": 29.505937576293945,
      "learning_rate": 1e-05,
      "loss": 7.2423,
      "step": 5576
    },
    {
      "epoch": 0.3022222222222222,
      "step": 5576,
      "training_loss": 8.413679122924805
    },
    {
      "epoch": 0.3022764227642276,
      "step": 5577,
      "training_loss": 6.430257797241211
    },
    {
      "epoch": 0.30233062330623306,
      "step": 5578,
      "training_loss": 6.31581974029541
    },
    {
      "epoch": 0.3023848238482385,
      "step": 5579,
      "training_loss": 5.180459976196289
    },
    {
      "epoch": 0.3024390243902439,
      "grad_norm": 32.86572265625,
      "learning_rate": 1e-05,
      "loss": 6.5851,
      "step": 5580
    },
    {
      "epoch": 0.3024390243902439,
      "step": 5580,
      "training_loss": 7.073565483093262
    },
    {
      "epoch": 0.30249322493224934,
      "step": 5581,
      "training_loss": 6.625865459442139
    },
    {
      "epoch": 0.3025474254742547,
      "step": 5582,
      "training_loss": 8.067943572998047
    },
    {
      "epoch": 0.3026016260162602,
      "step": 5583,
      "training_loss": 6.298433780670166
    },
    {
      "epoch": 0.30265582655826556,
      "grad_norm": 24.368162155151367,
      "learning_rate": 1e-05,
      "loss": 7.0165,
      "step": 5584
    },
    {
      "epoch": 0.30265582655826556,
      "step": 5584,
      "training_loss": 6.936344623565674
    },
    {
      "epoch": 0.302710027100271,
      "step": 5585,
      "training_loss": 7.3765387535095215
    },
    {
      "epoch": 0.3027642276422764,
      "step": 5586,
      "training_loss": 7.2839436531066895
    },
    {
      "epoch": 0.30281842818428184,
      "step": 5587,
      "training_loss": 8.006497383117676
    },
    {
      "epoch": 0.3028726287262873,
      "grad_norm": 53.2348518371582,
      "learning_rate": 1e-05,
      "loss": 7.4008,
      "step": 5588
    },
    {
      "epoch": 0.3028726287262873,
      "step": 5588,
      "training_loss": 7.6632609367370605
    },
    {
      "epoch": 0.3029268292682927,
      "step": 5589,
      "training_loss": 6.79746150970459
    },
    {
      "epoch": 0.3029810298102981,
      "step": 5590,
      "training_loss": 8.137222290039062
    },
    {
      "epoch": 0.3030352303523035,
      "step": 5591,
      "training_loss": 6.861934185028076
    },
    {
      "epoch": 0.30308943089430895,
      "grad_norm": 21.730628967285156,
      "learning_rate": 1e-05,
      "loss": 7.365,
      "step": 5592
    },
    {
      "epoch": 0.30308943089430895,
      "step": 5592,
      "training_loss": 8.058844566345215
    },
    {
      "epoch": 0.30314363143631434,
      "step": 5593,
      "training_loss": 7.757909297943115
    },
    {
      "epoch": 0.3031978319783198,
      "step": 5594,
      "training_loss": 5.605886459350586
    },
    {
      "epoch": 0.3032520325203252,
      "step": 5595,
      "training_loss": 6.188580513000488
    },
    {
      "epoch": 0.3033062330623306,
      "grad_norm": 18.49835968017578,
      "learning_rate": 1e-05,
      "loss": 6.9028,
      "step": 5596
    },
    {
      "epoch": 0.3033062330623306,
      "step": 5596,
      "training_loss": 6.364943981170654
    },
    {
      "epoch": 0.30336043360433607,
      "step": 5597,
      "training_loss": 6.770594120025635
    },
    {
      "epoch": 0.30341463414634146,
      "step": 5598,
      "training_loss": 7.060952186584473
    },
    {
      "epoch": 0.3034688346883469,
      "step": 5599,
      "training_loss": 6.430978298187256
    },
    {
      "epoch": 0.3035230352303523,
      "grad_norm": 33.04143142700195,
      "learning_rate": 1e-05,
      "loss": 6.6569,
      "step": 5600
    },
    {
      "epoch": 0.3035230352303523,
      "step": 5600,
      "training_loss": 6.1977949142456055
    },
    {
      "epoch": 0.30357723577235773,
      "step": 5601,
      "training_loss": 6.835841655731201
    },
    {
      "epoch": 0.3036314363143631,
      "step": 5602,
      "training_loss": 3.5345358848571777
    },
    {
      "epoch": 0.30368563685636857,
      "step": 5603,
      "training_loss": 7.248349666595459
    },
    {
      "epoch": 0.30373983739837396,
      "grad_norm": 18.628734588623047,
      "learning_rate": 1e-05,
      "loss": 5.9541,
      "step": 5604
    },
    {
      "epoch": 0.30373983739837396,
      "step": 5604,
      "training_loss": 6.899853229522705
    },
    {
      "epoch": 0.3037940379403794,
      "step": 5605,
      "training_loss": 7.664999008178711
    },
    {
      "epoch": 0.30384823848238485,
      "step": 5606,
      "training_loss": 5.464599609375
    },
    {
      "epoch": 0.30390243902439024,
      "step": 5607,
      "training_loss": 6.951932907104492
    },
    {
      "epoch": 0.3039566395663957,
      "grad_norm": 22.734699249267578,
      "learning_rate": 1e-05,
      "loss": 6.7453,
      "step": 5608
    },
    {
      "epoch": 0.3039566395663957,
      "step": 5608,
      "training_loss": 5.440035343170166
    },
    {
      "epoch": 0.30401084010840107,
      "step": 5609,
      "training_loss": 7.290704250335693
    },
    {
      "epoch": 0.3040650406504065,
      "step": 5610,
      "training_loss": 9.008588790893555
    },
    {
      "epoch": 0.3041192411924119,
      "step": 5611,
      "training_loss": 7.613358974456787
    },
    {
      "epoch": 0.30417344173441735,
      "grad_norm": 29.5846004486084,
      "learning_rate": 1e-05,
      "loss": 7.3382,
      "step": 5612
    },
    {
      "epoch": 0.30417344173441735,
      "step": 5612,
      "training_loss": 7.3649091720581055
    },
    {
      "epoch": 0.30422764227642274,
      "step": 5613,
      "training_loss": 6.522493362426758
    },
    {
      "epoch": 0.3042818428184282,
      "step": 5614,
      "training_loss": 7.117709159851074
    },
    {
      "epoch": 0.30433604336043363,
      "step": 5615,
      "training_loss": 4.888985633850098
    },
    {
      "epoch": 0.304390243902439,
      "grad_norm": 18.588655471801758,
      "learning_rate": 1e-05,
      "loss": 6.4735,
      "step": 5616
    },
    {
      "epoch": 0.304390243902439,
      "step": 5616,
      "training_loss": 7.766632080078125
    },
    {
      "epoch": 0.30444444444444446,
      "step": 5617,
      "training_loss": 5.686806678771973
    },
    {
      "epoch": 0.30449864498644985,
      "step": 5618,
      "training_loss": 7.811820030212402
    },
    {
      "epoch": 0.3045528455284553,
      "step": 5619,
      "training_loss": 7.102019786834717
    },
    {
      "epoch": 0.3046070460704607,
      "grad_norm": 20.60364532470703,
      "learning_rate": 1e-05,
      "loss": 7.0918,
      "step": 5620
    },
    {
      "epoch": 0.3046070460704607,
      "step": 5620,
      "training_loss": 7.803108215332031
    },
    {
      "epoch": 0.30466124661246613,
      "step": 5621,
      "training_loss": 4.884634494781494
    },
    {
      "epoch": 0.3047154471544715,
      "step": 5622,
      "training_loss": 5.482209205627441
    },
    {
      "epoch": 0.30476964769647696,
      "step": 5623,
      "training_loss": 7.288241386413574
    },
    {
      "epoch": 0.3048238482384824,
      "grad_norm": 22.279600143432617,
      "learning_rate": 1e-05,
      "loss": 6.3645,
      "step": 5624
    },
    {
      "epoch": 0.3048238482384824,
      "step": 5624,
      "training_loss": 6.632844924926758
    },
    {
      "epoch": 0.3048780487804878,
      "step": 5625,
      "training_loss": 6.580577373504639
    },
    {
      "epoch": 0.30493224932249324,
      "step": 5626,
      "training_loss": 7.92974853515625
    },
    {
      "epoch": 0.30498644986449863,
      "step": 5627,
      "training_loss": 7.063093662261963
    },
    {
      "epoch": 0.3050406504065041,
      "grad_norm": 18.670408248901367,
      "learning_rate": 1e-05,
      "loss": 7.0516,
      "step": 5628
    },
    {
      "epoch": 0.3050406504065041,
      "step": 5628,
      "training_loss": 7.953441619873047
    },
    {
      "epoch": 0.30509485094850947,
      "step": 5629,
      "training_loss": 7.060548782348633
    },
    {
      "epoch": 0.3051490514905149,
      "step": 5630,
      "training_loss": 7.255799293518066
    },
    {
      "epoch": 0.3052032520325203,
      "step": 5631,
      "training_loss": 8.333537101745605
    },
    {
      "epoch": 0.30525745257452574,
      "grad_norm": 40.10157012939453,
      "learning_rate": 1e-05,
      "loss": 7.6508,
      "step": 5632
    },
    {
      "epoch": 0.30525745257452574,
      "step": 5632,
      "training_loss": 5.497543811798096
    },
    {
      "epoch": 0.3053116531165312,
      "step": 5633,
      "training_loss": 7.591534614562988
    },
    {
      "epoch": 0.3053658536585366,
      "step": 5634,
      "training_loss": 5.894743919372559
    },
    {
      "epoch": 0.305420054200542,
      "step": 5635,
      "training_loss": 6.397545337677002
    },
    {
      "epoch": 0.3054742547425474,
      "grad_norm": 22.630298614501953,
      "learning_rate": 1e-05,
      "loss": 6.3453,
      "step": 5636
    },
    {
      "epoch": 0.3054742547425474,
      "step": 5636,
      "training_loss": 7.515269756317139
    },
    {
      "epoch": 0.30552845528455286,
      "step": 5637,
      "training_loss": 7.740232467651367
    },
    {
      "epoch": 0.30558265582655825,
      "step": 5638,
      "training_loss": 6.244409084320068
    },
    {
      "epoch": 0.3056368563685637,
      "step": 5639,
      "training_loss": 6.640351295471191
    },
    {
      "epoch": 0.3056910569105691,
      "grad_norm": 50.59546661376953,
      "learning_rate": 1e-05,
      "loss": 7.0351,
      "step": 5640
    },
    {
      "epoch": 0.3056910569105691,
      "step": 5640,
      "training_loss": 7.902849197387695
    },
    {
      "epoch": 0.3057452574525745,
      "step": 5641,
      "training_loss": 6.594899654388428
    },
    {
      "epoch": 0.30579945799457997,
      "step": 5642,
      "training_loss": 5.061570167541504
    },
    {
      "epoch": 0.30585365853658536,
      "step": 5643,
      "training_loss": 5.198919296264648
    },
    {
      "epoch": 0.3059078590785908,
      "grad_norm": 30.461374282836914,
      "learning_rate": 1e-05,
      "loss": 6.1896,
      "step": 5644
    },
    {
      "epoch": 0.3059078590785908,
      "step": 5644,
      "training_loss": 7.294280529022217
    },
    {
      "epoch": 0.3059620596205962,
      "step": 5645,
      "training_loss": 8.218485832214355
    },
    {
      "epoch": 0.30601626016260164,
      "step": 5646,
      "training_loss": 6.215823173522949
    },
    {
      "epoch": 0.30607046070460703,
      "step": 5647,
      "training_loss": 6.693709373474121
    },
    {
      "epoch": 0.3061246612466125,
      "grad_norm": 34.499656677246094,
      "learning_rate": 1e-05,
      "loss": 7.1056,
      "step": 5648
    },
    {
      "epoch": 0.3061246612466125,
      "step": 5648,
      "training_loss": 6.997509956359863
    },
    {
      "epoch": 0.30617886178861786,
      "step": 5649,
      "training_loss": 7.000695705413818
    },
    {
      "epoch": 0.3062330623306233,
      "step": 5650,
      "training_loss": 7.170607089996338
    },
    {
      "epoch": 0.30628726287262875,
      "step": 5651,
      "training_loss": 6.930575847625732
    },
    {
      "epoch": 0.30634146341463414,
      "grad_norm": 25.51556968688965,
      "learning_rate": 1e-05,
      "loss": 7.0248,
      "step": 5652
    },
    {
      "epoch": 0.30634146341463414,
      "step": 5652,
      "training_loss": 7.301340579986572
    },
    {
      "epoch": 0.3063956639566396,
      "step": 5653,
      "training_loss": 6.4238996505737305
    },
    {
      "epoch": 0.306449864498645,
      "step": 5654,
      "training_loss": 6.134068965911865
    },
    {
      "epoch": 0.3065040650406504,
      "step": 5655,
      "training_loss": 5.936971187591553
    },
    {
      "epoch": 0.3065582655826558,
      "grad_norm": 40.66069412231445,
      "learning_rate": 1e-05,
      "loss": 6.4491,
      "step": 5656
    },
    {
      "epoch": 0.3065582655826558,
      "step": 5656,
      "training_loss": 8.067761421203613
    },
    {
      "epoch": 0.30661246612466125,
      "step": 5657,
      "training_loss": 9.802783012390137
    },
    {
      "epoch": 0.30666666666666664,
      "step": 5658,
      "training_loss": 4.4486212730407715
    },
    {
      "epoch": 0.3067208672086721,
      "step": 5659,
      "training_loss": 7.168184280395508
    },
    {
      "epoch": 0.30677506775067753,
      "grad_norm": 29.660341262817383,
      "learning_rate": 1e-05,
      "loss": 7.3718,
      "step": 5660
    },
    {
      "epoch": 0.30677506775067753,
      "step": 5660,
      "training_loss": 3.9070136547088623
    },
    {
      "epoch": 0.3068292682926829,
      "step": 5661,
      "training_loss": 4.118085861206055
    },
    {
      "epoch": 0.30688346883468837,
      "step": 5662,
      "training_loss": 6.458305835723877
    },
    {
      "epoch": 0.30693766937669376,
      "step": 5663,
      "training_loss": 6.403006553649902
    },
    {
      "epoch": 0.3069918699186992,
      "grad_norm": 25.39635467529297,
      "learning_rate": 1e-05,
      "loss": 5.2216,
      "step": 5664
    },
    {
      "epoch": 0.3069918699186992,
      "step": 5664,
      "training_loss": 6.902246475219727
    },
    {
      "epoch": 0.3070460704607046,
      "step": 5665,
      "training_loss": 7.556827545166016
    },
    {
      "epoch": 0.30710027100271003,
      "step": 5666,
      "training_loss": 7.115594387054443
    },
    {
      "epoch": 0.3071544715447154,
      "step": 5667,
      "training_loss": 6.0891571044921875
    },
    {
      "epoch": 0.30720867208672087,
      "grad_norm": 32.9075927734375,
      "learning_rate": 1e-05,
      "loss": 6.916,
      "step": 5668
    },
    {
      "epoch": 0.30720867208672087,
      "step": 5668,
      "training_loss": 6.099724292755127
    },
    {
      "epoch": 0.3072628726287263,
      "step": 5669,
      "training_loss": 6.062244415283203
    },
    {
      "epoch": 0.3073170731707317,
      "step": 5670,
      "training_loss": 4.740251064300537
    },
    {
      "epoch": 0.30737127371273715,
      "step": 5671,
      "training_loss": 6.853412628173828
    },
    {
      "epoch": 0.30742547425474254,
      "grad_norm": 21.484861373901367,
      "learning_rate": 1e-05,
      "loss": 5.9389,
      "step": 5672
    },
    {
      "epoch": 0.30742547425474254,
      "step": 5672,
      "training_loss": 7.3955183029174805
    },
    {
      "epoch": 0.307479674796748,
      "step": 5673,
      "training_loss": 6.897545337677002
    },
    {
      "epoch": 0.30753387533875337,
      "step": 5674,
      "training_loss": 4.39104700088501
    },
    {
      "epoch": 0.3075880758807588,
      "step": 5675,
      "training_loss": 6.0901923179626465
    },
    {
      "epoch": 0.3076422764227642,
      "grad_norm": 46.783050537109375,
      "learning_rate": 1e-05,
      "loss": 6.1936,
      "step": 5676
    },
    {
      "epoch": 0.3076422764227642,
      "step": 5676,
      "training_loss": 6.80014705657959
    },
    {
      "epoch": 0.30769647696476965,
      "step": 5677,
      "training_loss": 8.72382926940918
    },
    {
      "epoch": 0.3077506775067751,
      "step": 5678,
      "training_loss": 5.271643161773682
    },
    {
      "epoch": 0.3078048780487805,
      "step": 5679,
      "training_loss": 5.660804271697998
    },
    {
      "epoch": 0.30785907859078593,
      "grad_norm": 23.02401351928711,
      "learning_rate": 1e-05,
      "loss": 6.6141,
      "step": 5680
    },
    {
      "epoch": 0.30785907859078593,
      "step": 5680,
      "training_loss": 6.784835338592529
    },
    {
      "epoch": 0.3079132791327913,
      "step": 5681,
      "training_loss": 6.966787338256836
    },
    {
      "epoch": 0.30796747967479676,
      "step": 5682,
      "training_loss": 6.085117816925049
    },
    {
      "epoch": 0.30802168021680215,
      "step": 5683,
      "training_loss": 5.332808017730713
    },
    {
      "epoch": 0.3080758807588076,
      "grad_norm": 22.09832000732422,
      "learning_rate": 1e-05,
      "loss": 6.2924,
      "step": 5684
    },
    {
      "epoch": 0.3080758807588076,
      "step": 5684,
      "training_loss": 5.999695777893066
    },
    {
      "epoch": 0.308130081300813,
      "step": 5685,
      "training_loss": 5.4123854637146
    },
    {
      "epoch": 0.30818428184281843,
      "step": 5686,
      "training_loss": 6.607239246368408
    },
    {
      "epoch": 0.3082384823848239,
      "step": 5687,
      "training_loss": 6.707234859466553
    },
    {
      "epoch": 0.30829268292682926,
      "grad_norm": 24.895450592041016,
      "learning_rate": 1e-05,
      "loss": 6.1816,
      "step": 5688
    },
    {
      "epoch": 0.30829268292682926,
      "step": 5688,
      "training_loss": 6.759856700897217
    },
    {
      "epoch": 0.3083468834688347,
      "step": 5689,
      "training_loss": 6.852887153625488
    },
    {
      "epoch": 0.3084010840108401,
      "step": 5690,
      "training_loss": 6.268793106079102
    },
    {
      "epoch": 0.30845528455284554,
      "step": 5691,
      "training_loss": 5.976069450378418
    },
    {
      "epoch": 0.30850948509485093,
      "grad_norm": 19.641803741455078,
      "learning_rate": 1e-05,
      "loss": 6.4644,
      "step": 5692
    },
    {
      "epoch": 0.30850948509485093,
      "step": 5692,
      "training_loss": 7.07320499420166
    },
    {
      "epoch": 0.3085636856368564,
      "step": 5693,
      "training_loss": 7.039528846740723
    },
    {
      "epoch": 0.30861788617886177,
      "step": 5694,
      "training_loss": 7.1125006675720215
    },
    {
      "epoch": 0.3086720867208672,
      "step": 5695,
      "training_loss": 6.930583953857422
    },
    {
      "epoch": 0.30872628726287266,
      "grad_norm": 17.801191329956055,
      "learning_rate": 1e-05,
      "loss": 7.039,
      "step": 5696
    },
    {
      "epoch": 0.30872628726287266,
      "step": 5696,
      "training_loss": 7.122097492218018
    },
    {
      "epoch": 0.30878048780487805,
      "step": 5697,
      "training_loss": 6.535782814025879
    },
    {
      "epoch": 0.3088346883468835,
      "step": 5698,
      "training_loss": 6.257598876953125
    },
    {
      "epoch": 0.3088888888888889,
      "step": 5699,
      "training_loss": 6.308672904968262
    },
    {
      "epoch": 0.3089430894308943,
      "grad_norm": 52.232933044433594,
      "learning_rate": 1e-05,
      "loss": 6.556,
      "step": 5700
    },
    {
      "epoch": 0.3089430894308943,
      "step": 5700,
      "training_loss": 5.993052959442139
    },
    {
      "epoch": 0.3089972899728997,
      "step": 5701,
      "training_loss": 7.701688289642334
    },
    {
      "epoch": 0.30905149051490516,
      "step": 5702,
      "training_loss": 8.550039291381836
    },
    {
      "epoch": 0.30910569105691055,
      "step": 5703,
      "training_loss": 6.700006008148193
    },
    {
      "epoch": 0.309159891598916,
      "grad_norm": 26.465267181396484,
      "learning_rate": 1e-05,
      "loss": 7.2362,
      "step": 5704
    },
    {
      "epoch": 0.309159891598916,
      "step": 5704,
      "training_loss": 6.991594314575195
    },
    {
      "epoch": 0.3092140921409214,
      "step": 5705,
      "training_loss": 7.836162090301514
    },
    {
      "epoch": 0.3092682926829268,
      "step": 5706,
      "training_loss": 6.7906341552734375
    },
    {
      "epoch": 0.30932249322493227,
      "step": 5707,
      "training_loss": 5.498788356781006
    },
    {
      "epoch": 0.30937669376693766,
      "grad_norm": 45.878257751464844,
      "learning_rate": 1e-05,
      "loss": 6.7793,
      "step": 5708
    },
    {
      "epoch": 0.30937669376693766,
      "step": 5708,
      "training_loss": 6.691109657287598
    },
    {
      "epoch": 0.3094308943089431,
      "step": 5709,
      "training_loss": 7.044139385223389
    },
    {
      "epoch": 0.3094850948509485,
      "step": 5710,
      "training_loss": 7.263749599456787
    },
    {
      "epoch": 0.30953929539295394,
      "step": 5711,
      "training_loss": 6.768485069274902
    },
    {
      "epoch": 0.30959349593495933,
      "grad_norm": 42.97751235961914,
      "learning_rate": 1e-05,
      "loss": 6.9419,
      "step": 5712
    },
    {
      "epoch": 0.30959349593495933,
      "step": 5712,
      "training_loss": 3.9524691104888916
    },
    {
      "epoch": 0.3096476964769648,
      "step": 5713,
      "training_loss": 7.4976325035095215
    },
    {
      "epoch": 0.30970189701897016,
      "step": 5714,
      "training_loss": 7.5047688484191895
    },
    {
      "epoch": 0.3097560975609756,
      "step": 5715,
      "training_loss": 6.800693035125732
    },
    {
      "epoch": 0.30981029810298105,
      "grad_norm": 15.038965225219727,
      "learning_rate": 1e-05,
      "loss": 6.4389,
      "step": 5716
    },
    {
      "epoch": 0.30981029810298105,
      "step": 5716,
      "training_loss": 7.289487838745117
    },
    {
      "epoch": 0.30986449864498644,
      "step": 5717,
      "training_loss": 7.239224910736084
    },
    {
      "epoch": 0.3099186991869919,
      "step": 5718,
      "training_loss": 7.38472318649292
    },
    {
      "epoch": 0.3099728997289973,
      "step": 5719,
      "training_loss": 8.070430755615234
    },
    {
      "epoch": 0.3100271002710027,
      "grad_norm": 82.74951934814453,
      "learning_rate": 1e-05,
      "loss": 7.496,
      "step": 5720
    },
    {
      "epoch": 0.3100271002710027,
      "step": 5720,
      "training_loss": 7.786215782165527
    },
    {
      "epoch": 0.3100813008130081,
      "step": 5721,
      "training_loss": 5.5870866775512695
    },
    {
      "epoch": 0.31013550135501355,
      "step": 5722,
      "training_loss": 4.916363716125488
    },
    {
      "epoch": 0.31018970189701894,
      "step": 5723,
      "training_loss": 6.648750305175781
    },
    {
      "epoch": 0.3102439024390244,
      "grad_norm": 21.794355392456055,
      "learning_rate": 1e-05,
      "loss": 6.2346,
      "step": 5724
    },
    {
      "epoch": 0.3102439024390244,
      "step": 5724,
      "training_loss": 7.2577805519104
    },
    {
      "epoch": 0.31029810298102983,
      "step": 5725,
      "training_loss": 6.099295616149902
    },
    {
      "epoch": 0.3103523035230352,
      "step": 5726,
      "training_loss": 6.274988651275635
    },
    {
      "epoch": 0.31040650406504067,
      "step": 5727,
      "training_loss": 6.728343963623047
    },
    {
      "epoch": 0.31046070460704606,
      "grad_norm": 23.212095260620117,
      "learning_rate": 1e-05,
      "loss": 6.5901,
      "step": 5728
    },
    {
      "epoch": 0.31046070460704606,
      "step": 5728,
      "training_loss": 7.5760979652404785
    },
    {
      "epoch": 0.3105149051490515,
      "step": 5729,
      "training_loss": 5.834286212921143
    },
    {
      "epoch": 0.3105691056910569,
      "step": 5730,
      "training_loss": 4.870362758636475
    },
    {
      "epoch": 0.31062330623306234,
      "step": 5731,
      "training_loss": 7.167490482330322
    },
    {
      "epoch": 0.3106775067750677,
      "grad_norm": 42.2354850769043,
      "learning_rate": 1e-05,
      "loss": 6.3621,
      "step": 5732
    },
    {
      "epoch": 0.3106775067750677,
      "step": 5732,
      "training_loss": 7.16312837600708
    },
    {
      "epoch": 0.31073170731707317,
      "step": 5733,
      "training_loss": 6.545947074890137
    },
    {
      "epoch": 0.3107859078590786,
      "step": 5734,
      "training_loss": 6.779641151428223
    },
    {
      "epoch": 0.310840108401084,
      "step": 5735,
      "training_loss": 5.822399139404297
    },
    {
      "epoch": 0.31089430894308945,
      "grad_norm": 27.367969512939453,
      "learning_rate": 1e-05,
      "loss": 6.5778,
      "step": 5736
    },
    {
      "epoch": 0.31089430894308945,
      "step": 5736,
      "training_loss": 6.414102554321289
    },
    {
      "epoch": 0.31094850948509484,
      "step": 5737,
      "training_loss": 7.356184959411621
    },
    {
      "epoch": 0.3110027100271003,
      "step": 5738,
      "training_loss": 6.262912750244141
    },
    {
      "epoch": 0.31105691056910567,
      "step": 5739,
      "training_loss": 7.064983367919922
    },
    {
      "epoch": 0.3111111111111111,
      "grad_norm": 41.36314392089844,
      "learning_rate": 1e-05,
      "loss": 6.7745,
      "step": 5740
    },
    {
      "epoch": 0.3111111111111111,
      "step": 5740,
      "training_loss": 6.285340785980225
    },
    {
      "epoch": 0.3111653116531165,
      "step": 5741,
      "training_loss": 6.940309047698975
    },
    {
      "epoch": 0.31121951219512195,
      "step": 5742,
      "training_loss": 6.396690368652344
    },
    {
      "epoch": 0.3112737127371274,
      "step": 5743,
      "training_loss": 7.294802188873291
    },
    {
      "epoch": 0.3113279132791328,
      "grad_norm": 31.526874542236328,
      "learning_rate": 1e-05,
      "loss": 6.7293,
      "step": 5744
    },
    {
      "epoch": 0.3113279132791328,
      "step": 5744,
      "training_loss": 7.212146759033203
    },
    {
      "epoch": 0.31138211382113823,
      "step": 5745,
      "training_loss": 7.118375301361084
    },
    {
      "epoch": 0.3114363143631436,
      "step": 5746,
      "training_loss": 6.7352142333984375
    },
    {
      "epoch": 0.31149051490514906,
      "step": 5747,
      "training_loss": 5.793942451477051
    },
    {
      "epoch": 0.31154471544715445,
      "grad_norm": 29.407386779785156,
      "learning_rate": 1e-05,
      "loss": 6.7149,
      "step": 5748
    },
    {
      "epoch": 0.31154471544715445,
      "step": 5748,
      "training_loss": 5.5595784187316895
    },
    {
      "epoch": 0.3115989159891599,
      "step": 5749,
      "training_loss": 4.273542404174805
    },
    {
      "epoch": 0.3116531165311653,
      "step": 5750,
      "training_loss": 6.081841945648193
    },
    {
      "epoch": 0.31170731707317073,
      "step": 5751,
      "training_loss": 7.299894332885742
    },
    {
      "epoch": 0.3117615176151762,
      "grad_norm": 21.688682556152344,
      "learning_rate": 1e-05,
      "loss": 5.8037,
      "step": 5752
    },
    {
      "epoch": 0.3117615176151762,
      "step": 5752,
      "training_loss": 6.1036481857299805
    },
    {
      "epoch": 0.31181571815718157,
      "step": 5753,
      "training_loss": 7.1180806159973145
    },
    {
      "epoch": 0.311869918699187,
      "step": 5754,
      "training_loss": 4.290517330169678
    },
    {
      "epoch": 0.3119241192411924,
      "step": 5755,
      "training_loss": 5.909097194671631
    },
    {
      "epoch": 0.31197831978319784,
      "grad_norm": 34.878910064697266,
      "learning_rate": 1e-05,
      "loss": 5.8553,
      "step": 5756
    },
    {
      "epoch": 0.31197831978319784,
      "step": 5756,
      "training_loss": 6.577840328216553
    },
    {
      "epoch": 0.31203252032520323,
      "step": 5757,
      "training_loss": 6.795447826385498
    },
    {
      "epoch": 0.3120867208672087,
      "step": 5758,
      "training_loss": 5.773514270782471
    },
    {
      "epoch": 0.31214092140921407,
      "step": 5759,
      "training_loss": 7.0548481941223145
    },
    {
      "epoch": 0.3121951219512195,
      "grad_norm": 25.343135833740234,
      "learning_rate": 1e-05,
      "loss": 6.5504,
      "step": 5760
    },
    {
      "epoch": 0.3121951219512195,
      "step": 5760,
      "training_loss": 6.703137397766113
    },
    {
      "epoch": 0.31224932249322496,
      "step": 5761,
      "training_loss": 6.282063961029053
    },
    {
      "epoch": 0.31230352303523035,
      "step": 5762,
      "training_loss": 6.596960544586182
    },
    {
      "epoch": 0.3123577235772358,
      "step": 5763,
      "training_loss": 6.0319504737854
    },
    {
      "epoch": 0.3124119241192412,
      "grad_norm": 24.636404037475586,
      "learning_rate": 1e-05,
      "loss": 6.4035,
      "step": 5764
    },
    {
      "epoch": 0.3124119241192412,
      "step": 5764,
      "training_loss": 7.311299800872803
    },
    {
      "epoch": 0.3124661246612466,
      "step": 5765,
      "training_loss": 6.8936920166015625
    },
    {
      "epoch": 0.312520325203252,
      "step": 5766,
      "training_loss": 5.310729026794434
    },
    {
      "epoch": 0.31257452574525746,
      "step": 5767,
      "training_loss": 6.615461349487305
    },
    {
      "epoch": 0.31262872628726285,
      "grad_norm": 47.350494384765625,
      "learning_rate": 1e-05,
      "loss": 6.5328,
      "step": 5768
    },
    {
      "epoch": 0.31262872628726285,
      "step": 5768,
      "training_loss": 7.56817102432251
    },
    {
      "epoch": 0.3126829268292683,
      "step": 5769,
      "training_loss": 5.7881646156311035
    },
    {
      "epoch": 0.31273712737127374,
      "step": 5770,
      "training_loss": 7.583795070648193
    },
    {
      "epoch": 0.3127913279132791,
      "step": 5771,
      "training_loss": 6.775852203369141
    },
    {
      "epoch": 0.31284552845528457,
      "grad_norm": 25.977781295776367,
      "learning_rate": 1e-05,
      "loss": 6.929,
      "step": 5772
    },
    {
      "epoch": 0.31284552845528457,
      "step": 5772,
      "training_loss": 5.840571403503418
    },
    {
      "epoch": 0.31289972899728996,
      "step": 5773,
      "training_loss": 6.1451416015625
    },
    {
      "epoch": 0.3129539295392954,
      "step": 5774,
      "training_loss": 7.260654926300049
    },
    {
      "epoch": 0.3130081300813008,
      "step": 5775,
      "training_loss": 6.209415435791016
    },
    {
      "epoch": 0.31306233062330624,
      "grad_norm": 42.23783874511719,
      "learning_rate": 1e-05,
      "loss": 6.3639,
      "step": 5776
    },
    {
      "epoch": 0.31306233062330624,
      "step": 5776,
      "training_loss": 6.256805896759033
    },
    {
      "epoch": 0.31311653116531163,
      "step": 5777,
      "training_loss": 7.376813888549805
    },
    {
      "epoch": 0.3131707317073171,
      "step": 5778,
      "training_loss": 5.702299118041992
    },
    {
      "epoch": 0.3132249322493225,
      "step": 5779,
      "training_loss": 7.403219223022461
    },
    {
      "epoch": 0.3132791327913279,
      "grad_norm": 19.175630569458008,
      "learning_rate": 1e-05,
      "loss": 6.6848,
      "step": 5780
    },
    {
      "epoch": 0.3132791327913279,
      "step": 5780,
      "training_loss": 6.411343097686768
    },
    {
      "epoch": 0.31333333333333335,
      "step": 5781,
      "training_loss": 6.798467636108398
    },
    {
      "epoch": 0.31338753387533874,
      "step": 5782,
      "training_loss": 4.88440465927124
    },
    {
      "epoch": 0.3134417344173442,
      "step": 5783,
      "training_loss": 5.670746326446533
    },
    {
      "epoch": 0.3134959349593496,
      "grad_norm": 28.41655921936035,
      "learning_rate": 1e-05,
      "loss": 5.9412,
      "step": 5784
    },
    {
      "epoch": 0.3134959349593496,
      "step": 5784,
      "training_loss": 5.592528343200684
    },
    {
      "epoch": 0.313550135501355,
      "step": 5785,
      "training_loss": 7.271027565002441
    },
    {
      "epoch": 0.3136043360433604,
      "step": 5786,
      "training_loss": 5.115762233734131
    },
    {
      "epoch": 0.31365853658536585,
      "step": 5787,
      "training_loss": 6.620197772979736
    },
    {
      "epoch": 0.3137127371273713,
      "grad_norm": 24.834152221679688,
      "learning_rate": 1e-05,
      "loss": 6.1499,
      "step": 5788
    },
    {
      "epoch": 0.3137127371273713,
      "step": 5788,
      "training_loss": 7.693042755126953
    },
    {
      "epoch": 0.3137669376693767,
      "step": 5789,
      "training_loss": 6.801689624786377
    },
    {
      "epoch": 0.31382113821138213,
      "step": 5790,
      "training_loss": 7.480729103088379
    },
    {
      "epoch": 0.3138753387533875,
      "step": 5791,
      "training_loss": 6.696623802185059
    },
    {
      "epoch": 0.31392953929539297,
      "grad_norm": 22.940128326416016,
      "learning_rate": 1e-05,
      "loss": 7.168,
      "step": 5792
    },
    {
      "epoch": 0.31392953929539297,
      "step": 5792,
      "training_loss": 7.058065414428711
    },
    {
      "epoch": 0.31398373983739836,
      "step": 5793,
      "training_loss": 6.728540897369385
    },
    {
      "epoch": 0.3140379403794038,
      "step": 5794,
      "training_loss": 8.452890396118164
    },
    {
      "epoch": 0.3140921409214092,
      "step": 5795,
      "training_loss": 6.7228264808654785
    },
    {
      "epoch": 0.31414634146341464,
      "grad_norm": 22.840595245361328,
      "learning_rate": 1e-05,
      "loss": 7.2406,
      "step": 5796
    },
    {
      "epoch": 0.31414634146341464,
      "step": 5796,
      "training_loss": 6.047886371612549
    },
    {
      "epoch": 0.3142005420054201,
      "step": 5797,
      "training_loss": 6.668405055999756
    },
    {
      "epoch": 0.31425474254742547,
      "step": 5798,
      "training_loss": 7.558332443237305
    },
    {
      "epoch": 0.3143089430894309,
      "step": 5799,
      "training_loss": 6.345000743865967
    },
    {
      "epoch": 0.3143631436314363,
      "grad_norm": 23.17375373840332,
      "learning_rate": 1e-05,
      "loss": 6.6549,
      "step": 5800
    },
    {
      "epoch": 0.3143631436314363,
      "step": 5800,
      "training_loss": 7.4203901290893555
    },
    {
      "epoch": 0.31441734417344175,
      "step": 5801,
      "training_loss": 5.373239517211914
    },
    {
      "epoch": 0.31447154471544714,
      "step": 5802,
      "training_loss": 4.321131229400635
    },
    {
      "epoch": 0.3145257452574526,
      "step": 5803,
      "training_loss": 6.611727714538574
    },
    {
      "epoch": 0.31457994579945797,
      "grad_norm": 16.8461971282959,
      "learning_rate": 1e-05,
      "loss": 5.9316,
      "step": 5804
    },
    {
      "epoch": 0.31457994579945797,
      "step": 5804,
      "training_loss": 6.4659504890441895
    },
    {
      "epoch": 0.3146341463414634,
      "step": 5805,
      "training_loss": 7.185008525848389
    },
    {
      "epoch": 0.31468834688346886,
      "step": 5806,
      "training_loss": 3.449525833129883
    },
    {
      "epoch": 0.31474254742547425,
      "step": 5807,
      "training_loss": 4.159731388092041
    },
    {
      "epoch": 0.3147967479674797,
      "grad_norm": 26.505292892456055,
      "learning_rate": 1e-05,
      "loss": 5.3151,
      "step": 5808
    },
    {
      "epoch": 0.3147967479674797,
      "step": 5808,
      "training_loss": 6.248623371124268
    },
    {
      "epoch": 0.3148509485094851,
      "step": 5809,
      "training_loss": 7.463626861572266
    },
    {
      "epoch": 0.31490514905149053,
      "step": 5810,
      "training_loss": 6.624380111694336
    },
    {
      "epoch": 0.3149593495934959,
      "step": 5811,
      "training_loss": 5.520007133483887
    },
    {
      "epoch": 0.31501355013550136,
      "grad_norm": 21.78594970703125,
      "learning_rate": 1e-05,
      "loss": 6.4642,
      "step": 5812
    },
    {
      "epoch": 0.31501355013550136,
      "step": 5812,
      "training_loss": 7.619346618652344
    },
    {
      "epoch": 0.31506775067750675,
      "step": 5813,
      "training_loss": 6.231497287750244
    },
    {
      "epoch": 0.3151219512195122,
      "step": 5814,
      "training_loss": 6.083370685577393
    },
    {
      "epoch": 0.31517615176151764,
      "step": 5815,
      "training_loss": 6.920437812805176
    },
    {
      "epoch": 0.31523035230352303,
      "grad_norm": 35.20348358154297,
      "learning_rate": 1e-05,
      "loss": 6.7137,
      "step": 5816
    },
    {
      "epoch": 0.31523035230352303,
      "step": 5816,
      "training_loss": 6.842915058135986
    },
    {
      "epoch": 0.3152845528455285,
      "step": 5817,
      "training_loss": 5.498743534088135
    },
    {
      "epoch": 0.31533875338753387,
      "step": 5818,
      "training_loss": 7.944902420043945
    },
    {
      "epoch": 0.3153929539295393,
      "step": 5819,
      "training_loss": 4.770860195159912
    },
    {
      "epoch": 0.3154471544715447,
      "grad_norm": 72.4189224243164,
      "learning_rate": 1e-05,
      "loss": 6.2644,
      "step": 5820
    },
    {
      "epoch": 0.3154471544715447,
      "step": 5820,
      "training_loss": 6.06373929977417
    },
    {
      "epoch": 0.31550135501355014,
      "step": 5821,
      "training_loss": 5.523509979248047
    },
    {
      "epoch": 0.31555555555555553,
      "step": 5822,
      "training_loss": 7.252439975738525
    },
    {
      "epoch": 0.315609756097561,
      "step": 5823,
      "training_loss": 6.740824222564697
    },
    {
      "epoch": 0.3156639566395664,
      "grad_norm": 26.553478240966797,
      "learning_rate": 1e-05,
      "loss": 6.3951,
      "step": 5824
    },
    {
      "epoch": 0.3156639566395664,
      "step": 5824,
      "training_loss": 11.267218589782715
    },
    {
      "epoch": 0.3157181571815718,
      "step": 5825,
      "training_loss": 6.311057090759277
    },
    {
      "epoch": 0.31577235772357726,
      "step": 5826,
      "training_loss": 6.704049110412598
    },
    {
      "epoch": 0.31582655826558265,
      "step": 5827,
      "training_loss": 6.219981670379639
    },
    {
      "epoch": 0.3158807588075881,
      "grad_norm": 26.473173141479492,
      "learning_rate": 1e-05,
      "loss": 7.6256,
      "step": 5828
    },
    {
      "epoch": 0.3158807588075881,
      "step": 5828,
      "training_loss": 7.710574150085449
    },
    {
      "epoch": 0.3159349593495935,
      "step": 5829,
      "training_loss": 6.91347074508667
    },
    {
      "epoch": 0.3159891598915989,
      "step": 5830,
      "training_loss": 7.192059516906738
    },
    {
      "epoch": 0.3160433604336043,
      "step": 5831,
      "training_loss": 6.113874435424805
    },
    {
      "epoch": 0.31609756097560976,
      "grad_norm": 29.36288070678711,
      "learning_rate": 1e-05,
      "loss": 6.9825,
      "step": 5832
    },
    {
      "epoch": 0.31609756097560976,
      "step": 5832,
      "training_loss": 4.775993824005127
    },
    {
      "epoch": 0.31615176151761515,
      "step": 5833,
      "training_loss": 7.3392486572265625
    },
    {
      "epoch": 0.3162059620596206,
      "step": 5834,
      "training_loss": 6.62311315536499
    },
    {
      "epoch": 0.31626016260162604,
      "step": 5835,
      "training_loss": 7.8624267578125
    },
    {
      "epoch": 0.3163143631436314,
      "grad_norm": 30.216135025024414,
      "learning_rate": 1e-05,
      "loss": 6.6502,
      "step": 5836
    },
    {
      "epoch": 0.3163143631436314,
      "step": 5836,
      "training_loss": 4.78987979888916
    },
    {
      "epoch": 0.31636856368563687,
      "step": 5837,
      "training_loss": 7.154413223266602
    },
    {
      "epoch": 0.31642276422764226,
      "step": 5838,
      "training_loss": 6.834336280822754
    },
    {
      "epoch": 0.3164769647696477,
      "step": 5839,
      "training_loss": 4.923222064971924
    },
    {
      "epoch": 0.3165311653116531,
      "grad_norm": 69.00007629394531,
      "learning_rate": 1e-05,
      "loss": 5.9255,
      "step": 5840
    },
    {
      "epoch": 0.3165311653116531,
      "step": 5840,
      "training_loss": 7.296465873718262
    },
    {
      "epoch": 0.31658536585365854,
      "step": 5841,
      "training_loss": 7.121496677398682
    },
    {
      "epoch": 0.31663956639566393,
      "step": 5842,
      "training_loss": 5.672130107879639
    },
    {
      "epoch": 0.3166937669376694,
      "step": 5843,
      "training_loss": 6.694658279418945
    },
    {
      "epoch": 0.3167479674796748,
      "grad_norm": 24.16042137145996,
      "learning_rate": 1e-05,
      "loss": 6.6962,
      "step": 5844
    },
    {
      "epoch": 0.3167479674796748,
      "step": 5844,
      "training_loss": 5.461847305297852
    },
    {
      "epoch": 0.3168021680216802,
      "step": 5845,
      "training_loss": 6.7026238441467285
    },
    {
      "epoch": 0.31685636856368565,
      "step": 5846,
      "training_loss": 6.57052755355835
    },
    {
      "epoch": 0.31691056910569104,
      "step": 5847,
      "training_loss": 6.900490760803223
    },
    {
      "epoch": 0.3169647696476965,
      "grad_norm": 17.34479331970215,
      "learning_rate": 1e-05,
      "loss": 6.4089,
      "step": 5848
    },
    {
      "epoch": 0.3169647696476965,
      "step": 5848,
      "training_loss": 6.513444423675537
    },
    {
      "epoch": 0.3170189701897019,
      "step": 5849,
      "training_loss": 6.666845798492432
    },
    {
      "epoch": 0.3170731707317073,
      "step": 5850,
      "training_loss": 7.298574924468994
    },
    {
      "epoch": 0.3171273712737127,
      "step": 5851,
      "training_loss": 5.02549409866333
    },
    {
      "epoch": 0.31718157181571816,
      "grad_norm": 58.74439239501953,
      "learning_rate": 1e-05,
      "loss": 6.3761,
      "step": 5852
    },
    {
      "epoch": 0.31718157181571816,
      "step": 5852,
      "training_loss": 8.06676959991455
    },
    {
      "epoch": 0.3172357723577236,
      "step": 5853,
      "training_loss": 7.628046989440918
    },
    {
      "epoch": 0.317289972899729,
      "step": 5854,
      "training_loss": 8.051897048950195
    },
    {
      "epoch": 0.31734417344173443,
      "step": 5855,
      "training_loss": 6.780163764953613
    },
    {
      "epoch": 0.3173983739837398,
      "grad_norm": 17.77842903137207,
      "learning_rate": 1e-05,
      "loss": 7.6317,
      "step": 5856
    },
    {
      "epoch": 0.3173983739837398,
      "step": 5856,
      "training_loss": 4.569359302520752
    },
    {
      "epoch": 0.31745257452574527,
      "step": 5857,
      "training_loss": 5.626171112060547
    },
    {
      "epoch": 0.31750677506775066,
      "step": 5858,
      "training_loss": 6.286734580993652
    },
    {
      "epoch": 0.3175609756097561,
      "step": 5859,
      "training_loss": 6.929113388061523
    },
    {
      "epoch": 0.3176151761517615,
      "grad_norm": 23.13702964782715,
      "learning_rate": 1e-05,
      "loss": 5.8528,
      "step": 5860
    },
    {
      "epoch": 0.3176151761517615,
      "step": 5860,
      "training_loss": 5.806484222412109
    },
    {
      "epoch": 0.31766937669376694,
      "step": 5861,
      "training_loss": 5.913209915161133
    },
    {
      "epoch": 0.3177235772357724,
      "step": 5862,
      "training_loss": 7.151899814605713
    },
    {
      "epoch": 0.31777777777777777,
      "step": 5863,
      "training_loss": 6.612955570220947
    },
    {
      "epoch": 0.3178319783197832,
      "grad_norm": 21.074583053588867,
      "learning_rate": 1e-05,
      "loss": 6.3711,
      "step": 5864
    },
    {
      "epoch": 0.3178319783197832,
      "step": 5864,
      "training_loss": 8.574155807495117
    },
    {
      "epoch": 0.3178861788617886,
      "step": 5865,
      "training_loss": 6.501039028167725
    },
    {
      "epoch": 0.31794037940379405,
      "step": 5866,
      "training_loss": 7.505618572235107
    },
    {
      "epoch": 0.31799457994579944,
      "step": 5867,
      "training_loss": 5.319931983947754
    },
    {
      "epoch": 0.3180487804878049,
      "grad_norm": 20.204391479492188,
      "learning_rate": 1e-05,
      "loss": 6.9752,
      "step": 5868
    },
    {
      "epoch": 0.3180487804878049,
      "step": 5868,
      "training_loss": 5.152632713317871
    },
    {
      "epoch": 0.31810298102981027,
      "step": 5869,
      "training_loss": 5.940826892852783
    },
    {
      "epoch": 0.3181571815718157,
      "step": 5870,
      "training_loss": 7.489578723907471
    },
    {
      "epoch": 0.31821138211382116,
      "step": 5871,
      "training_loss": 7.042148590087891
    },
    {
      "epoch": 0.31826558265582655,
      "grad_norm": 22.611831665039062,
      "learning_rate": 1e-05,
      "loss": 6.4063,
      "step": 5872
    },
    {
      "epoch": 0.31826558265582655,
      "step": 5872,
      "training_loss": 7.6668314933776855
    },
    {
      "epoch": 0.318319783197832,
      "step": 5873,
      "training_loss": 7.248950481414795
    },
    {
      "epoch": 0.3183739837398374,
      "step": 5874,
      "training_loss": 6.897092819213867
    },
    {
      "epoch": 0.31842818428184283,
      "step": 5875,
      "training_loss": 7.705967903137207
    },
    {
      "epoch": 0.3184823848238482,
      "grad_norm": 17.765077590942383,
      "learning_rate": 1e-05,
      "loss": 7.3797,
      "step": 5876
    },
    {
      "epoch": 0.3184823848238482,
      "step": 5876,
      "training_loss": 5.937889575958252
    },
    {
      "epoch": 0.31853658536585366,
      "step": 5877,
      "training_loss": 6.413964748382568
    },
    {
      "epoch": 0.31859078590785905,
      "step": 5878,
      "training_loss": 7.2751922607421875
    },
    {
      "epoch": 0.3186449864498645,
      "step": 5879,
      "training_loss": 5.699212551116943
    },
    {
      "epoch": 0.31869918699186994,
      "grad_norm": 40.64456558227539,
      "learning_rate": 1e-05,
      "loss": 6.3316,
      "step": 5880
    },
    {
      "epoch": 0.31869918699186994,
      "step": 5880,
      "training_loss": 7.298428058624268
    },
    {
      "epoch": 0.31875338753387533,
      "step": 5881,
      "training_loss": 6.805619239807129
    },
    {
      "epoch": 0.3188075880758808,
      "step": 5882,
      "training_loss": 7.731278419494629
    },
    {
      "epoch": 0.31886178861788617,
      "step": 5883,
      "training_loss": 7.825931072235107
    },
    {
      "epoch": 0.3189159891598916,
      "grad_norm": 68.83385467529297,
      "learning_rate": 1e-05,
      "loss": 7.4153,
      "step": 5884
    },
    {
      "epoch": 0.3189159891598916,
      "step": 5884,
      "training_loss": 6.804432392120361
    },
    {
      "epoch": 0.318970189701897,
      "step": 5885,
      "training_loss": 6.3274359703063965
    },
    {
      "epoch": 0.31902439024390244,
      "step": 5886,
      "training_loss": 6.979660511016846
    },
    {
      "epoch": 0.31907859078590783,
      "step": 5887,
      "training_loss": 7.149112224578857
    },
    {
      "epoch": 0.3191327913279133,
      "grad_norm": 21.761354446411133,
      "learning_rate": 1e-05,
      "loss": 6.8152,
      "step": 5888
    },
    {
      "epoch": 0.3191327913279133,
      "step": 5888,
      "training_loss": 8.22008991241455
    },
    {
      "epoch": 0.3191869918699187,
      "step": 5889,
      "training_loss": 5.448006629943848
    },
    {
      "epoch": 0.3192411924119241,
      "step": 5890,
      "training_loss": 8.100433349609375
    },
    {
      "epoch": 0.31929539295392956,
      "step": 5891,
      "training_loss": 6.4691362380981445
    },
    {
      "epoch": 0.31934959349593495,
      "grad_norm": 23.328535079956055,
      "learning_rate": 1e-05,
      "loss": 7.0594,
      "step": 5892
    },
    {
      "epoch": 0.31934959349593495,
      "step": 5892,
      "training_loss": 6.8321123123168945
    },
    {
      "epoch": 0.3194037940379404,
      "step": 5893,
      "training_loss": 6.7137932777404785
    },
    {
      "epoch": 0.3194579945799458,
      "step": 5894,
      "training_loss": 6.974546432495117
    },
    {
      "epoch": 0.3195121951219512,
      "step": 5895,
      "training_loss": 6.467870235443115
    },
    {
      "epoch": 0.3195663956639566,
      "grad_norm": 22.96050453186035,
      "learning_rate": 1e-05,
      "loss": 6.7471,
      "step": 5896
    },
    {
      "epoch": 0.3195663956639566,
      "step": 5896,
      "training_loss": 5.113171577453613
    },
    {
      "epoch": 0.31962059620596206,
      "step": 5897,
      "training_loss": 6.435691833496094
    },
    {
      "epoch": 0.3196747967479675,
      "step": 5898,
      "training_loss": 6.466738224029541
    },
    {
      "epoch": 0.3197289972899729,
      "step": 5899,
      "training_loss": 7.712418556213379
    },
    {
      "epoch": 0.31978319783197834,
      "grad_norm": 35.82182312011719,
      "learning_rate": 1e-05,
      "loss": 6.432,
      "step": 5900
    },
    {
      "epoch": 0.31978319783197834,
      "step": 5900,
      "training_loss": 6.475142955780029
    },
    {
      "epoch": 0.31983739837398373,
      "step": 5901,
      "training_loss": 7.726949691772461
    },
    {
      "epoch": 0.3198915989159892,
      "step": 5902,
      "training_loss": 8.509922981262207
    },
    {
      "epoch": 0.31994579945799456,
      "step": 5903,
      "training_loss": 6.493100643157959
    },
    {
      "epoch": 0.32,
      "grad_norm": 19.61806297302246,
      "learning_rate": 1e-05,
      "loss": 7.3013,
      "step": 5904
    },
    {
      "epoch": 0.32,
      "step": 5904,
      "training_loss": 7.08864688873291
    },
    {
      "epoch": 0.3200542005420054,
      "step": 5905,
      "training_loss": 7.151829242706299
    },
    {
      "epoch": 0.32010840108401084,
      "step": 5906,
      "training_loss": 6.58980131149292
    },
    {
      "epoch": 0.3201626016260163,
      "step": 5907,
      "training_loss": 7.5547895431518555
    },
    {
      "epoch": 0.3202168021680217,
      "grad_norm": 45.1436767578125,
      "learning_rate": 1e-05,
      "loss": 7.0963,
      "step": 5908
    },
    {
      "epoch": 0.3202168021680217,
      "step": 5908,
      "training_loss": 5.922755718231201
    },
    {
      "epoch": 0.3202710027100271,
      "step": 5909,
      "training_loss": 7.355559825897217
    },
    {
      "epoch": 0.3203252032520325,
      "step": 5910,
      "training_loss": 6.828775405883789
    },
    {
      "epoch": 0.32037940379403795,
      "step": 5911,
      "training_loss": 6.866682529449463
    },
    {
      "epoch": 0.32043360433604334,
      "grad_norm": 31.493366241455078,
      "learning_rate": 1e-05,
      "loss": 6.7434,
      "step": 5912
    },
    {
      "epoch": 0.32043360433604334,
      "step": 5912,
      "training_loss": 4.105734348297119
    },
    {
      "epoch": 0.3204878048780488,
      "step": 5913,
      "training_loss": 7.107813835144043
    },
    {
      "epoch": 0.3205420054200542,
      "step": 5914,
      "training_loss": 5.569887638092041
    },
    {
      "epoch": 0.3205962059620596,
      "step": 5915,
      "training_loss": 6.881999492645264
    },
    {
      "epoch": 0.32065040650406507,
      "grad_norm": 19.699813842773438,
      "learning_rate": 1e-05,
      "loss": 5.9164,
      "step": 5916
    },
    {
      "epoch": 0.32065040650406507,
      "step": 5916,
      "training_loss": 7.782650470733643
    },
    {
      "epoch": 0.32070460704607046,
      "step": 5917,
      "training_loss": 8.119726181030273
    },
    {
      "epoch": 0.3207588075880759,
      "step": 5918,
      "training_loss": 6.7699785232543945
    },
    {
      "epoch": 0.3208130081300813,
      "step": 5919,
      "training_loss": 6.616827487945557
    },
    {
      "epoch": 0.32086720867208673,
      "grad_norm": 20.829360961914062,
      "learning_rate": 1e-05,
      "loss": 7.3223,
      "step": 5920
    },
    {
      "epoch": 0.32086720867208673,
      "step": 5920,
      "training_loss": 7.041062355041504
    },
    {
      "epoch": 0.3209214092140921,
      "step": 5921,
      "training_loss": 8.50377082824707
    },
    {
      "epoch": 0.32097560975609757,
      "step": 5922,
      "training_loss": 7.007730007171631
    },
    {
      "epoch": 0.32102981029810296,
      "step": 5923,
      "training_loss": 9.507293701171875
    },
    {
      "epoch": 0.3210840108401084,
      "grad_norm": 62.89863204956055,
      "learning_rate": 1e-05,
      "loss": 8.015,
      "step": 5924
    },
    {
      "epoch": 0.3210840108401084,
      "step": 5924,
      "training_loss": 7.601363658905029
    },
    {
      "epoch": 0.32113821138211385,
      "step": 5925,
      "training_loss": 5.908513069152832
    },
    {
      "epoch": 0.32119241192411924,
      "step": 5926,
      "training_loss": 6.979511260986328
    },
    {
      "epoch": 0.3212466124661247,
      "step": 5927,
      "training_loss": 7.225335597991943
    },
    {
      "epoch": 0.32130081300813007,
      "grad_norm": 19.092966079711914,
      "learning_rate": 1e-05,
      "loss": 6.9287,
      "step": 5928
    },
    {
      "epoch": 0.32130081300813007,
      "step": 5928,
      "training_loss": 6.8022541999816895
    },
    {
      "epoch": 0.3213550135501355,
      "step": 5929,
      "training_loss": 7.5333404541015625
    },
    {
      "epoch": 0.3214092140921409,
      "step": 5930,
      "training_loss": 5.556742191314697
    },
    {
      "epoch": 0.32146341463414635,
      "step": 5931,
      "training_loss": 7.535274505615234
    },
    {
      "epoch": 0.32151761517615174,
      "grad_norm": 38.99094772338867,
      "learning_rate": 1e-05,
      "loss": 6.8569,
      "step": 5932
    },
    {
      "epoch": 0.32151761517615174,
      "step": 5932,
      "training_loss": 8.799294471740723
    },
    {
      "epoch": 0.3215718157181572,
      "step": 5933,
      "training_loss": 6.37860107421875
    },
    {
      "epoch": 0.32162601626016263,
      "step": 5934,
      "training_loss": 5.1690568923950195
    },
    {
      "epoch": 0.321680216802168,
      "step": 5935,
      "training_loss": 7.376804351806641
    },
    {
      "epoch": 0.32173441734417346,
      "grad_norm": 43.752742767333984,
      "learning_rate": 1e-05,
      "loss": 6.9309,
      "step": 5936
    },
    {
      "epoch": 0.32173441734417346,
      "step": 5936,
      "training_loss": 6.840096473693848
    },
    {
      "epoch": 0.32178861788617885,
      "step": 5937,
      "training_loss": 5.280869960784912
    },
    {
      "epoch": 0.3218428184281843,
      "step": 5938,
      "training_loss": 6.969644069671631
    },
    {
      "epoch": 0.3218970189701897,
      "step": 5939,
      "training_loss": 6.431957244873047
    },
    {
      "epoch": 0.32195121951219513,
      "grad_norm": 47.9928092956543,
      "learning_rate": 1e-05,
      "loss": 6.3806,
      "step": 5940
    },
    {
      "epoch": 0.32195121951219513,
      "step": 5940,
      "training_loss": 7.174919605255127
    },
    {
      "epoch": 0.3220054200542005,
      "step": 5941,
      "training_loss": 6.889699935913086
    },
    {
      "epoch": 0.32205962059620596,
      "step": 5942,
      "training_loss": 6.3323655128479
    },
    {
      "epoch": 0.3221138211382114,
      "step": 5943,
      "training_loss": 7.808323383331299
    },
    {
      "epoch": 0.3221680216802168,
      "grad_norm": 43.655914306640625,
      "learning_rate": 1e-05,
      "loss": 7.0513,
      "step": 5944
    },
    {
      "epoch": 0.3221680216802168,
      "step": 5944,
      "training_loss": 7.0808634757995605
    },
    {
      "epoch": 0.32222222222222224,
      "step": 5945,
      "training_loss": 6.415621757507324
    },
    {
      "epoch": 0.32227642276422763,
      "step": 5946,
      "training_loss": 5.71527624130249
    },
    {
      "epoch": 0.3223306233062331,
      "step": 5947,
      "training_loss": 6.594751358032227
    },
    {
      "epoch": 0.32238482384823847,
      "grad_norm": 33.094947814941406,
      "learning_rate": 1e-05,
      "loss": 6.4516,
      "step": 5948
    },
    {
      "epoch": 0.32238482384823847,
      "step": 5948,
      "training_loss": 6.581784725189209
    },
    {
      "epoch": 0.3224390243902439,
      "step": 5949,
      "training_loss": 6.839021682739258
    },
    {
      "epoch": 0.3224932249322493,
      "step": 5950,
      "training_loss": 7.4682793617248535
    },
    {
      "epoch": 0.32254742547425475,
      "step": 5951,
      "training_loss": 6.239390850067139
    },
    {
      "epoch": 0.3226016260162602,
      "grad_norm": 27.238805770874023,
      "learning_rate": 1e-05,
      "loss": 6.7821,
      "step": 5952
    },
    {
      "epoch": 0.3226016260162602,
      "step": 5952,
      "training_loss": 7.336747646331787
    },
    {
      "epoch": 0.3226558265582656,
      "step": 5953,
      "training_loss": 5.941938877105713
    },
    {
      "epoch": 0.322710027100271,
      "step": 5954,
      "training_loss": 3.9204800128936768
    },
    {
      "epoch": 0.3227642276422764,
      "step": 5955,
      "training_loss": 6.811975955963135
    },
    {
      "epoch": 0.32281842818428186,
      "grad_norm": 31.048606872558594,
      "learning_rate": 1e-05,
      "loss": 6.0028,
      "step": 5956
    },
    {
      "epoch": 0.32281842818428186,
      "step": 5956,
      "training_loss": 6.369053840637207
    },
    {
      "epoch": 0.32287262872628725,
      "step": 5957,
      "training_loss": 6.846630573272705
    },
    {
      "epoch": 0.3229268292682927,
      "step": 5958,
      "training_loss": 5.370584964752197
    },
    {
      "epoch": 0.3229810298102981,
      "step": 5959,
      "training_loss": 5.332876682281494
    },
    {
      "epoch": 0.3230352303523035,
      "grad_norm": 30.726064682006836,
      "learning_rate": 1e-05,
      "loss": 5.9798,
      "step": 5960
    },
    {
      "epoch": 0.3230352303523035,
      "step": 5960,
      "training_loss": 7.109951972961426
    },
    {
      "epoch": 0.3230894308943089,
      "step": 5961,
      "training_loss": 9.305908203125
    },
    {
      "epoch": 0.32314363143631436,
      "step": 5962,
      "training_loss": 5.186137676239014
    },
    {
      "epoch": 0.3231978319783198,
      "step": 5963,
      "training_loss": 6.467779636383057
    },
    {
      "epoch": 0.3232520325203252,
      "grad_norm": 22.865385055541992,
      "learning_rate": 1e-05,
      "loss": 7.0174,
      "step": 5964
    },
    {
      "epoch": 0.3232520325203252,
      "step": 5964,
      "training_loss": 7.366249084472656
    },
    {
      "epoch": 0.32330623306233064,
      "step": 5965,
      "training_loss": 7.091031551361084
    },
    {
      "epoch": 0.32336043360433603,
      "step": 5966,
      "training_loss": 5.874504566192627
    },
    {
      "epoch": 0.3234146341463415,
      "step": 5967,
      "training_loss": 5.9192728996276855
    },
    {
      "epoch": 0.32346883468834686,
      "grad_norm": 25.73246955871582,
      "learning_rate": 1e-05,
      "loss": 6.5628,
      "step": 5968
    },
    {
      "epoch": 0.32346883468834686,
      "step": 5968,
      "training_loss": 6.849892616271973
    },
    {
      "epoch": 0.3235230352303523,
      "step": 5969,
      "training_loss": 6.4188456535339355
    },
    {
      "epoch": 0.3235772357723577,
      "step": 5970,
      "training_loss": 7.833365440368652
    },
    {
      "epoch": 0.32363143631436314,
      "step": 5971,
      "training_loss": 6.230817794799805
    },
    {
      "epoch": 0.3236856368563686,
      "grad_norm": 24.091510772705078,
      "learning_rate": 1e-05,
      "loss": 6.8332,
      "step": 5972
    },
    {
      "epoch": 0.3236856368563686,
      "step": 5972,
      "training_loss": 6.90035343170166
    },
    {
      "epoch": 0.323739837398374,
      "step": 5973,
      "training_loss": 5.66616678237915
    },
    {
      "epoch": 0.3237940379403794,
      "step": 5974,
      "training_loss": 7.194713592529297
    },
    {
      "epoch": 0.3238482384823848,
      "step": 5975,
      "training_loss": 6.297544956207275
    },
    {
      "epoch": 0.32390243902439025,
      "grad_norm": 19.37042999267578,
      "learning_rate": 1e-05,
      "loss": 6.5147,
      "step": 5976
    },
    {
      "epoch": 0.32390243902439025,
      "step": 5976,
      "training_loss": 5.767319202423096
    },
    {
      "epoch": 0.32395663956639564,
      "step": 5977,
      "training_loss": 7.182741641998291
    },
    {
      "epoch": 0.3240108401084011,
      "step": 5978,
      "training_loss": 6.630466461181641
    },
    {
      "epoch": 0.3240650406504065,
      "step": 5979,
      "training_loss": 7.34871768951416
    },
    {
      "epoch": 0.3241192411924119,
      "grad_norm": 20.015714645385742,
      "learning_rate": 1e-05,
      "loss": 6.7323,
      "step": 5980
    },
    {
      "epoch": 0.3241192411924119,
      "step": 5980,
      "training_loss": 7.975666046142578
    },
    {
      "epoch": 0.32417344173441737,
      "step": 5981,
      "training_loss": 6.6924262046813965
    },
    {
      "epoch": 0.32422764227642276,
      "step": 5982,
      "training_loss": 5.228850841522217
    },
    {
      "epoch": 0.3242818428184282,
      "step": 5983,
      "training_loss": 8.242481231689453
    },
    {
      "epoch": 0.3243360433604336,
      "grad_norm": 24.73629379272461,
      "learning_rate": 1e-05,
      "loss": 7.0349,
      "step": 5984
    },
    {
      "epoch": 0.3243360433604336,
      "step": 5984,
      "training_loss": 6.960901737213135
    },
    {
      "epoch": 0.32439024390243903,
      "step": 5985,
      "training_loss": 8.45429515838623
    },
    {
      "epoch": 0.3244444444444444,
      "step": 5986,
      "training_loss": 6.409183025360107
    },
    {
      "epoch": 0.32449864498644987,
      "step": 5987,
      "training_loss": 6.450237274169922
    },
    {
      "epoch": 0.32455284552845526,
      "grad_norm": 19.96232795715332,
      "learning_rate": 1e-05,
      "loss": 7.0687,
      "step": 5988
    },
    {
      "epoch": 0.32455284552845526,
      "step": 5988,
      "training_loss": 7.871325492858887
    },
    {
      "epoch": 0.3246070460704607,
      "step": 5989,
      "training_loss": 7.060184001922607
    },
    {
      "epoch": 0.32466124661246615,
      "step": 5990,
      "training_loss": 6.10964298248291
    },
    {
      "epoch": 0.32471544715447154,
      "step": 5991,
      "training_loss": 6.8624677658081055
    },
    {
      "epoch": 0.324769647696477,
      "grad_norm": 29.2587890625,
      "learning_rate": 1e-05,
      "loss": 6.9759,
      "step": 5992
    },
    {
      "epoch": 0.324769647696477,
      "step": 5992,
      "training_loss": 6.718069553375244
    },
    {
      "epoch": 0.32482384823848237,
      "step": 5993,
      "training_loss": 5.554644584655762
    },
    {
      "epoch": 0.3248780487804878,
      "step": 5994,
      "training_loss": 7.989099502563477
    },
    {
      "epoch": 0.3249322493224932,
      "step": 5995,
      "training_loss": 6.593599319458008
    },
    {
      "epoch": 0.32498644986449865,
      "grad_norm": 32.938838958740234,
      "learning_rate": 1e-05,
      "loss": 6.7139,
      "step": 5996
    },
    {
      "epoch": 0.32498644986449865,
      "step": 5996,
      "training_loss": 6.432736873626709
    },
    {
      "epoch": 0.32504065040650404,
      "step": 5997,
      "training_loss": 6.925594329833984
    },
    {
      "epoch": 0.3250948509485095,
      "step": 5998,
      "training_loss": 7.199916839599609
    },
    {
      "epoch": 0.32514905149051493,
      "step": 5999,
      "training_loss": 7.935614109039307
    },
    {
      "epoch": 0.3252032520325203,
      "grad_norm": 36.047637939453125,
      "learning_rate": 1e-05,
      "loss": 7.1235,
      "step": 6000
    },
    {
      "epoch": 0.3252032520325203,
      "eval_runtime": 462.0402,
      "eval_samples_per_second": 4.437,
      "eval_steps_per_second": 4.437,
      "step": 6000
    },
    {
      "epoch": 0.3252032520325203,
      "step": 6000,
      "training_loss": 9.160839080810547
    },
    {
      "epoch": 0.32525745257452576,
      "step": 6001,
      "training_loss": 6.0318169593811035
    },
    {
      "epoch": 0.32531165311653115,
      "step": 6002,
      "training_loss": 6.709136009216309
    },
    {
      "epoch": 0.3253658536585366,
      "step": 6003,
      "training_loss": 6.994519233703613
    },
    {
      "epoch": 0.325420054200542,
      "grad_norm": 38.72222900390625,
      "learning_rate": 1e-05,
      "loss": 7.2241,
      "step": 6004
    },
    {
      "epoch": 0.325420054200542,
      "step": 6004,
      "training_loss": 6.354831218719482
    },
    {
      "epoch": 0.32547425474254743,
      "step": 6005,
      "training_loss": 7.853728771209717
    },
    {
      "epoch": 0.3255284552845528,
      "step": 6006,
      "training_loss": 7.050440311431885
    },
    {
      "epoch": 0.32558265582655826,
      "step": 6007,
      "training_loss": 6.668951511383057
    },
    {
      "epoch": 0.3256368563685637,
      "grad_norm": 17.151147842407227,
      "learning_rate": 1e-05,
      "loss": 6.982,
      "step": 6008
    },
    {
      "epoch": 0.3256368563685637,
      "step": 6008,
      "training_loss": 5.970897674560547
    },
    {
      "epoch": 0.3256910569105691,
      "step": 6009,
      "training_loss": 7.851711750030518
    },
    {
      "epoch": 0.32574525745257454,
      "step": 6010,
      "training_loss": 7.019566059112549
    },
    {
      "epoch": 0.32579945799457993,
      "step": 6011,
      "training_loss": 7.562386989593506
    },
    {
      "epoch": 0.3258536585365854,
      "grad_norm": 20.577287673950195,
      "learning_rate": 1e-05,
      "loss": 7.1011,
      "step": 6012
    },
    {
      "epoch": 0.3258536585365854,
      "step": 6012,
      "training_loss": 6.392899990081787
    },
    {
      "epoch": 0.32590785907859077,
      "step": 6013,
      "training_loss": 7.421629905700684
    },
    {
      "epoch": 0.3259620596205962,
      "step": 6014,
      "training_loss": 7.114896774291992
    },
    {
      "epoch": 0.3260162601626016,
      "step": 6015,
      "training_loss": 6.064645767211914
    },
    {
      "epoch": 0.32607046070460705,
      "grad_norm": 20.492578506469727,
      "learning_rate": 1e-05,
      "loss": 6.7485,
      "step": 6016
    },
    {
      "epoch": 0.32607046070460705,
      "step": 6016,
      "training_loss": 7.142755031585693
    },
    {
      "epoch": 0.3261246612466125,
      "step": 6017,
      "training_loss": 6.3799147605896
    },
    {
      "epoch": 0.3261788617886179,
      "step": 6018,
      "training_loss": 6.955055236816406
    },
    {
      "epoch": 0.3262330623306233,
      "step": 6019,
      "training_loss": 6.577277660369873
    },
    {
      "epoch": 0.3262872628726287,
      "grad_norm": 30.657283782958984,
      "learning_rate": 1e-05,
      "loss": 6.7638,
      "step": 6020
    },
    {
      "epoch": 0.3262872628726287,
      "step": 6020,
      "training_loss": 7.560022354125977
    },
    {
      "epoch": 0.32634146341463416,
      "step": 6021,
      "training_loss": 6.239037990570068
    },
    {
      "epoch": 0.32639566395663955,
      "step": 6022,
      "training_loss": 8.130775451660156
    },
    {
      "epoch": 0.326449864498645,
      "step": 6023,
      "training_loss": 6.192019939422607
    },
    {
      "epoch": 0.3265040650406504,
      "grad_norm": 33.06449890136719,
      "learning_rate": 1e-05,
      "loss": 7.0305,
      "step": 6024
    },
    {
      "epoch": 0.3265040650406504,
      "step": 6024,
      "training_loss": 7.011870861053467
    },
    {
      "epoch": 0.3265582655826558,
      "step": 6025,
      "training_loss": 7.367755889892578
    },
    {
      "epoch": 0.32661246612466127,
      "step": 6026,
      "training_loss": 6.772421836853027
    },
    {
      "epoch": 0.32666666666666666,
      "step": 6027,
      "training_loss": 6.223995208740234
    },
    {
      "epoch": 0.3267208672086721,
      "grad_norm": 29.26898956298828,
      "learning_rate": 1e-05,
      "loss": 6.844,
      "step": 6028
    },
    {
      "epoch": 0.3267208672086721,
      "step": 6028,
      "training_loss": 4.111407279968262
    },
    {
      "epoch": 0.3267750677506775,
      "step": 6029,
      "training_loss": 7.714766502380371
    },
    {
      "epoch": 0.32682926829268294,
      "step": 6030,
      "training_loss": 7.272806644439697
    },
    {
      "epoch": 0.32688346883468833,
      "step": 6031,
      "training_loss": 6.584993839263916
    },
    {
      "epoch": 0.3269376693766938,
      "grad_norm": 17.790342330932617,
      "learning_rate": 1e-05,
      "loss": 6.421,
      "step": 6032
    },
    {
      "epoch": 0.3269376693766938,
      "step": 6032,
      "training_loss": 8.264686584472656
    },
    {
      "epoch": 0.32699186991869916,
      "step": 6033,
      "training_loss": 7.229796886444092
    },
    {
      "epoch": 0.3270460704607046,
      "step": 6034,
      "training_loss": 7.030047416687012
    },
    {
      "epoch": 0.32710027100271005,
      "step": 6035,
      "training_loss": 6.969878196716309
    },
    {
      "epoch": 0.32715447154471544,
      "grad_norm": 25.69417953491211,
      "learning_rate": 1e-05,
      "loss": 7.3736,
      "step": 6036
    },
    {
      "epoch": 0.32715447154471544,
      "step": 6036,
      "training_loss": 6.56243896484375
    },
    {
      "epoch": 0.3272086720867209,
      "step": 6037,
      "training_loss": 7.2165679931640625
    },
    {
      "epoch": 0.3272628726287263,
      "step": 6038,
      "training_loss": 7.3723249435424805
    },
    {
      "epoch": 0.3273170731707317,
      "step": 6039,
      "training_loss": 5.181469440460205
    },
    {
      "epoch": 0.3273712737127371,
      "grad_norm": 26.747291564941406,
      "learning_rate": 1e-05,
      "loss": 6.5832,
      "step": 6040
    },
    {
      "epoch": 0.3273712737127371,
      "step": 6040,
      "training_loss": 6.4947190284729
    },
    {
      "epoch": 0.32742547425474255,
      "step": 6041,
      "training_loss": 5.804882526397705
    },
    {
      "epoch": 0.32747967479674794,
      "step": 6042,
      "training_loss": 6.973016262054443
    },
    {
      "epoch": 0.3275338753387534,
      "step": 6043,
      "training_loss": 6.664291858673096
    },
    {
      "epoch": 0.32758807588075883,
      "grad_norm": 19.21213150024414,
      "learning_rate": 1e-05,
      "loss": 6.4842,
      "step": 6044
    },
    {
      "epoch": 0.32758807588075883,
      "step": 6044,
      "training_loss": 6.914392471313477
    },
    {
      "epoch": 0.3276422764227642,
      "step": 6045,
      "training_loss": 6.264156818389893
    },
    {
      "epoch": 0.32769647696476967,
      "step": 6046,
      "training_loss": 6.05776309967041
    },
    {
      "epoch": 0.32775067750677506,
      "step": 6047,
      "training_loss": 8.079551696777344
    },
    {
      "epoch": 0.3278048780487805,
      "grad_norm": 32.0974235534668,
      "learning_rate": 1e-05,
      "loss": 6.829,
      "step": 6048
    },
    {
      "epoch": 0.3278048780487805,
      "step": 6048,
      "training_loss": 6.706146240234375
    },
    {
      "epoch": 0.3278590785907859,
      "step": 6049,
      "training_loss": 6.695825099945068
    },
    {
      "epoch": 0.32791327913279134,
      "step": 6050,
      "training_loss": 6.148055076599121
    },
    {
      "epoch": 0.3279674796747967,
      "step": 6051,
      "training_loss": 3.6847305297851562
    },
    {
      "epoch": 0.32802168021680217,
      "grad_norm": 21.700101852416992,
      "learning_rate": 1e-05,
      "loss": 5.8087,
      "step": 6052
    },
    {
      "epoch": 0.32802168021680217,
      "step": 6052,
      "training_loss": 6.291018009185791
    },
    {
      "epoch": 0.3280758807588076,
      "step": 6053,
      "training_loss": 3.9489262104034424
    },
    {
      "epoch": 0.328130081300813,
      "step": 6054,
      "training_loss": 4.274266242980957
    },
    {
      "epoch": 0.32818428184281845,
      "step": 6055,
      "training_loss": 6.29048490524292
    },
    {
      "epoch": 0.32823848238482384,
      "grad_norm": 58.803810119628906,
      "learning_rate": 1e-05,
      "loss": 5.2012,
      "step": 6056
    },
    {
      "epoch": 0.32823848238482384,
      "step": 6056,
      "training_loss": 6.841683387756348
    },
    {
      "epoch": 0.3282926829268293,
      "step": 6057,
      "training_loss": 6.871067523956299
    },
    {
      "epoch": 0.32834688346883467,
      "step": 6058,
      "training_loss": 7.559925556182861
    },
    {
      "epoch": 0.3284010840108401,
      "step": 6059,
      "training_loss": 6.814112663269043
    },
    {
      "epoch": 0.3284552845528455,
      "grad_norm": 23.055683135986328,
      "learning_rate": 1e-05,
      "loss": 7.0217,
      "step": 6060
    },
    {
      "epoch": 0.3284552845528455,
      "step": 6060,
      "training_loss": 8.226845741271973
    },
    {
      "epoch": 0.32850948509485095,
      "step": 6061,
      "training_loss": 6.052009105682373
    },
    {
      "epoch": 0.3285636856368564,
      "step": 6062,
      "training_loss": 6.900029182434082
    },
    {
      "epoch": 0.3286178861788618,
      "step": 6063,
      "training_loss": 7.5286054611206055
    },
    {
      "epoch": 0.32867208672086723,
      "grad_norm": 23.521949768066406,
      "learning_rate": 1e-05,
      "loss": 7.1769,
      "step": 6064
    },
    {
      "epoch": 0.32867208672086723,
      "step": 6064,
      "training_loss": 6.062361717224121
    },
    {
      "epoch": 0.3287262872628726,
      "step": 6065,
      "training_loss": 5.941549301147461
    },
    {
      "epoch": 0.32878048780487806,
      "step": 6066,
      "training_loss": 6.98619270324707
    },
    {
      "epoch": 0.32883468834688345,
      "step": 6067,
      "training_loss": 6.45131254196167
    },
    {
      "epoch": 0.3288888888888889,
      "grad_norm": 18.573753356933594,
      "learning_rate": 1e-05,
      "loss": 6.3604,
      "step": 6068
    },
    {
      "epoch": 0.3288888888888889,
      "step": 6068,
      "training_loss": 7.468347549438477
    },
    {
      "epoch": 0.3289430894308943,
      "step": 6069,
      "training_loss": 6.230598449707031
    },
    {
      "epoch": 0.32899728997289973,
      "step": 6070,
      "training_loss": 6.230242729187012
    },
    {
      "epoch": 0.3290514905149052,
      "step": 6071,
      "training_loss": 4.528853416442871
    },
    {
      "epoch": 0.32910569105691057,
      "grad_norm": 21.2990779876709,
      "learning_rate": 1e-05,
      "loss": 6.1145,
      "step": 6072
    },
    {
      "epoch": 0.32910569105691057,
      "step": 6072,
      "training_loss": 7.562550067901611
    },
    {
      "epoch": 0.329159891598916,
      "step": 6073,
      "training_loss": 7.978912830352783
    },
    {
      "epoch": 0.3292140921409214,
      "step": 6074,
      "training_loss": 7.186305999755859
    },
    {
      "epoch": 0.32926829268292684,
      "step": 6075,
      "training_loss": 8.679457664489746
    },
    {
      "epoch": 0.32932249322493223,
      "grad_norm": 59.07727813720703,
      "learning_rate": 1e-05,
      "loss": 7.8518,
      "step": 6076
    },
    {
      "epoch": 0.32932249322493223,
      "step": 6076,
      "training_loss": 6.0610198974609375
    },
    {
      "epoch": 0.3293766937669377,
      "step": 6077,
      "training_loss": 8.520455360412598
    },
    {
      "epoch": 0.32943089430894307,
      "step": 6078,
      "training_loss": 6.28860330581665
    },
    {
      "epoch": 0.3294850948509485,
      "step": 6079,
      "training_loss": 6.553339004516602
    },
    {
      "epoch": 0.32953929539295396,
      "grad_norm": 25.102073669433594,
      "learning_rate": 1e-05,
      "loss": 6.8559,
      "step": 6080
    },
    {
      "epoch": 0.32953929539295396,
      "step": 6080,
      "training_loss": 6.766594409942627
    },
    {
      "epoch": 0.32959349593495935,
      "step": 6081,
      "training_loss": 3.795463800430298
    },
    {
      "epoch": 0.3296476964769648,
      "step": 6082,
      "training_loss": 6.956119537353516
    },
    {
      "epoch": 0.3297018970189702,
      "step": 6083,
      "training_loss": 6.565354824066162
    },
    {
      "epoch": 0.3297560975609756,
      "grad_norm": 22.942792892456055,
      "learning_rate": 1e-05,
      "loss": 6.0209,
      "step": 6084
    },
    {
      "epoch": 0.3297560975609756,
      "step": 6084,
      "training_loss": 7.632557392120361
    },
    {
      "epoch": 0.329810298102981,
      "step": 6085,
      "training_loss": 6.586256504058838
    },
    {
      "epoch": 0.32986449864498646,
      "step": 6086,
      "training_loss": 6.793607711791992
    },
    {
      "epoch": 0.32991869918699185,
      "step": 6087,
      "training_loss": 7.933783054351807
    },
    {
      "epoch": 0.3299728997289973,
      "grad_norm": 29.44334602355957,
      "learning_rate": 1e-05,
      "loss": 7.2366,
      "step": 6088
    },
    {
      "epoch": 0.3299728997289973,
      "step": 6088,
      "training_loss": 5.980306625366211
    },
    {
      "epoch": 0.3300271002710027,
      "step": 6089,
      "training_loss": 5.875192642211914
    },
    {
      "epoch": 0.3300813008130081,
      "step": 6090,
      "training_loss": 5.071262359619141
    },
    {
      "epoch": 0.33013550135501357,
      "step": 6091,
      "training_loss": 6.268278121948242
    },
    {
      "epoch": 0.33018970189701896,
      "grad_norm": 41.24275588989258,
      "learning_rate": 1e-05,
      "loss": 5.7988,
      "step": 6092
    },
    {
      "epoch": 0.33018970189701896,
      "step": 6092,
      "training_loss": 7.030759334564209
    },
    {
      "epoch": 0.3302439024390244,
      "step": 6093,
      "training_loss": 7.274022102355957
    },
    {
      "epoch": 0.3302981029810298,
      "step": 6094,
      "training_loss": 7.371537208557129
    },
    {
      "epoch": 0.33035230352303524,
      "step": 6095,
      "training_loss": 7.2573442459106445
    },
    {
      "epoch": 0.33040650406504063,
      "grad_norm": 21.680194854736328,
      "learning_rate": 1e-05,
      "loss": 7.2334,
      "step": 6096
    },
    {
      "epoch": 0.33040650406504063,
      "step": 6096,
      "training_loss": 7.941184997558594
    },
    {
      "epoch": 0.3304607046070461,
      "step": 6097,
      "training_loss": 6.85949182510376
    },
    {
      "epoch": 0.33051490514905146,
      "step": 6098,
      "training_loss": 5.887777805328369
    },
    {
      "epoch": 0.3305691056910569,
      "step": 6099,
      "training_loss": 7.169404983520508
    },
    {
      "epoch": 0.33062330623306235,
      "grad_norm": 31.8612003326416,
      "learning_rate": 1e-05,
      "loss": 6.9645,
      "step": 6100
    },
    {
      "epoch": 0.33062330623306235,
      "step": 6100,
      "training_loss": 5.658972263336182
    },
    {
      "epoch": 0.33067750677506774,
      "step": 6101,
      "training_loss": 6.256481170654297
    },
    {
      "epoch": 0.3307317073170732,
      "step": 6102,
      "training_loss": 7.9775390625
    },
    {
      "epoch": 0.3307859078590786,
      "step": 6103,
      "training_loss": 5.874622344970703
    },
    {
      "epoch": 0.330840108401084,
      "grad_norm": 30.732078552246094,
      "learning_rate": 1e-05,
      "loss": 6.4419,
      "step": 6104
    },
    {
      "epoch": 0.330840108401084,
      "step": 6104,
      "training_loss": 5.991870880126953
    },
    {
      "epoch": 0.3308943089430894,
      "step": 6105,
      "training_loss": 6.834616184234619
    },
    {
      "epoch": 0.33094850948509485,
      "step": 6106,
      "training_loss": 7.7075066566467285
    },
    {
      "epoch": 0.33100271002710024,
      "step": 6107,
      "training_loss": 6.826347827911377
    },
    {
      "epoch": 0.3310569105691057,
      "grad_norm": 37.61049270629883,
      "learning_rate": 1e-05,
      "loss": 6.8401,
      "step": 6108
    },
    {
      "epoch": 0.3310569105691057,
      "step": 6108,
      "training_loss": 7.375263214111328
    },
    {
      "epoch": 0.33111111111111113,
      "step": 6109,
      "training_loss": 8.290824890136719
    },
    {
      "epoch": 0.3311653116531165,
      "step": 6110,
      "training_loss": 6.585933685302734
    },
    {
      "epoch": 0.33121951219512197,
      "step": 6111,
      "training_loss": 6.1245036125183105
    },
    {
      "epoch": 0.33127371273712736,
      "grad_norm": 47.73459243774414,
      "learning_rate": 1e-05,
      "loss": 7.0941,
      "step": 6112
    },
    {
      "epoch": 0.33127371273712736,
      "step": 6112,
      "training_loss": 6.375006198883057
    },
    {
      "epoch": 0.3313279132791328,
      "step": 6113,
      "training_loss": 6.083980083465576
    },
    {
      "epoch": 0.3313821138211382,
      "step": 6114,
      "training_loss": 7.898725509643555
    },
    {
      "epoch": 0.33143631436314364,
      "step": 6115,
      "training_loss": 6.128098964691162
    },
    {
      "epoch": 0.331490514905149,
      "grad_norm": 36.51874923706055,
      "learning_rate": 1e-05,
      "loss": 6.6215,
      "step": 6116
    },
    {
      "epoch": 0.331490514905149,
      "step": 6116,
      "training_loss": 4.41322135925293
    },
    {
      "epoch": 0.33154471544715447,
      "step": 6117,
      "training_loss": 5.974276542663574
    },
    {
      "epoch": 0.3315989159891599,
      "step": 6118,
      "training_loss": 6.485373497009277
    },
    {
      "epoch": 0.3316531165311653,
      "step": 6119,
      "training_loss": 6.562717437744141
    },
    {
      "epoch": 0.33170731707317075,
      "grad_norm": 27.013286590576172,
      "learning_rate": 1e-05,
      "loss": 5.8589,
      "step": 6120
    },
    {
      "epoch": 0.33170731707317075,
      "step": 6120,
      "training_loss": 5.377558708190918
    },
    {
      "epoch": 0.33176151761517614,
      "step": 6121,
      "training_loss": 7.218767166137695
    },
    {
      "epoch": 0.3318157181571816,
      "step": 6122,
      "training_loss": 6.564325332641602
    },
    {
      "epoch": 0.33186991869918697,
      "step": 6123,
      "training_loss": 6.526294231414795
    },
    {
      "epoch": 0.3319241192411924,
      "grad_norm": 32.11281967163086,
      "learning_rate": 1e-05,
      "loss": 6.4217,
      "step": 6124
    },
    {
      "epoch": 0.3319241192411924,
      "step": 6124,
      "training_loss": 7.776324272155762
    },
    {
      "epoch": 0.3319783197831978,
      "step": 6125,
      "training_loss": 8.954216957092285
    },
    {
      "epoch": 0.33203252032520325,
      "step": 6126,
      "training_loss": 7.273709774017334
    },
    {
      "epoch": 0.3320867208672087,
      "step": 6127,
      "training_loss": 4.183901786804199
    },
    {
      "epoch": 0.3321409214092141,
      "grad_norm": 33.349021911621094,
      "learning_rate": 1e-05,
      "loss": 7.047,
      "step": 6128
    },
    {
      "epoch": 0.3321409214092141,
      "step": 6128,
      "training_loss": 6.319303035736084
    },
    {
      "epoch": 0.33219512195121953,
      "step": 6129,
      "training_loss": 6.9585089683532715
    },
    {
      "epoch": 0.3322493224932249,
      "step": 6130,
      "training_loss": 5.5633225440979
    },
    {
      "epoch": 0.33230352303523036,
      "step": 6131,
      "training_loss": 5.92814826965332
    },
    {
      "epoch": 0.33235772357723575,
      "grad_norm": 38.04792404174805,
      "learning_rate": 1e-05,
      "loss": 6.1923,
      "step": 6132
    },
    {
      "epoch": 0.33235772357723575,
      "step": 6132,
      "training_loss": 7.857516765594482
    },
    {
      "epoch": 0.3324119241192412,
      "step": 6133,
      "training_loss": 6.13900089263916
    },
    {
      "epoch": 0.3324661246612466,
      "step": 6134,
      "training_loss": 6.953981876373291
    },
    {
      "epoch": 0.33252032520325203,
      "step": 6135,
      "training_loss": 7.04850959777832
    },
    {
      "epoch": 0.3325745257452575,
      "grad_norm": 27.794755935668945,
      "learning_rate": 1e-05,
      "loss": 6.9998,
      "step": 6136
    },
    {
      "epoch": 0.3325745257452575,
      "step": 6136,
      "training_loss": 7.179985046386719
    },
    {
      "epoch": 0.33262872628726287,
      "step": 6137,
      "training_loss": 7.9226579666137695
    },
    {
      "epoch": 0.3326829268292683,
      "step": 6138,
      "training_loss": 7.998851776123047
    },
    {
      "epoch": 0.3327371273712737,
      "step": 6139,
      "training_loss": 6.931368350982666
    },
    {
      "epoch": 0.33279132791327914,
      "grad_norm": 17.695892333984375,
      "learning_rate": 1e-05,
      "loss": 7.5082,
      "step": 6140
    },
    {
      "epoch": 0.33279132791327914,
      "step": 6140,
      "training_loss": 7.828348636627197
    },
    {
      "epoch": 0.33284552845528453,
      "step": 6141,
      "training_loss": 6.47308349609375
    },
    {
      "epoch": 0.33289972899729,
      "step": 6142,
      "training_loss": 6.592516899108887
    },
    {
      "epoch": 0.33295392953929537,
      "step": 6143,
      "training_loss": 7.57607889175415
    },
    {
      "epoch": 0.3330081300813008,
      "grad_norm": 20.42711067199707,
      "learning_rate": 1e-05,
      "loss": 7.1175,
      "step": 6144
    },
    {
      "epoch": 0.3330081300813008,
      "step": 6144,
      "training_loss": 7.127450942993164
    },
    {
      "epoch": 0.33306233062330626,
      "step": 6145,
      "training_loss": 7.646691799163818
    },
    {
      "epoch": 0.33311653116531165,
      "step": 6146,
      "training_loss": 5.962753772735596
    },
    {
      "epoch": 0.3331707317073171,
      "step": 6147,
      "training_loss": 7.7739033699035645
    },
    {
      "epoch": 0.3332249322493225,
      "grad_norm": 27.880332946777344,
      "learning_rate": 1e-05,
      "loss": 7.1277,
      "step": 6148
    },
    {
      "epoch": 0.3332249322493225,
      "step": 6148,
      "training_loss": 7.841975688934326
    },
    {
      "epoch": 0.3332791327913279,
      "step": 6149,
      "training_loss": 7.010959625244141
    },
    {
      "epoch": 0.3333333333333333,
      "step": 6150,
      "training_loss": 3.8230860233306885
    },
    {
      "epoch": 0.33338753387533876,
      "step": 6151,
      "training_loss": 6.926908493041992
    },
    {
      "epoch": 0.33344173441734415,
      "grad_norm": 22.2352294921875,
      "learning_rate": 1e-05,
      "loss": 6.4007,
      "step": 6152
    },
    {
      "epoch": 0.33344173441734415,
      "step": 6152,
      "training_loss": 6.993409633636475
    },
    {
      "epoch": 0.3334959349593496,
      "step": 6153,
      "training_loss": 7.440922737121582
    },
    {
      "epoch": 0.33355013550135504,
      "step": 6154,
      "training_loss": 7.906304359436035
    },
    {
      "epoch": 0.3336043360433604,
      "step": 6155,
      "training_loss": 7.4522833824157715
    },
    {
      "epoch": 0.3336585365853659,
      "grad_norm": 17.200021743774414,
      "learning_rate": 1e-05,
      "loss": 7.4482,
      "step": 6156
    },
    {
      "epoch": 0.3336585365853659,
      "step": 6156,
      "training_loss": 5.15447998046875
    },
    {
      "epoch": 0.33371273712737126,
      "step": 6157,
      "training_loss": 6.700315475463867
    },
    {
      "epoch": 0.3337669376693767,
      "step": 6158,
      "training_loss": 6.954917907714844
    },
    {
      "epoch": 0.3338211382113821,
      "step": 6159,
      "training_loss": 7.6882805824279785
    },
    {
      "epoch": 0.33387533875338754,
      "grad_norm": 37.928504943847656,
      "learning_rate": 1e-05,
      "loss": 6.6245,
      "step": 6160
    },
    {
      "epoch": 0.33387533875338754,
      "step": 6160,
      "training_loss": 7.104310035705566
    },
    {
      "epoch": 0.33392953929539293,
      "step": 6161,
      "training_loss": 5.2627153396606445
    },
    {
      "epoch": 0.3339837398373984,
      "step": 6162,
      "training_loss": 6.747410774230957
    },
    {
      "epoch": 0.3340379403794038,
      "step": 6163,
      "training_loss": 6.612609386444092
    },
    {
      "epoch": 0.3340921409214092,
      "grad_norm": 18.128185272216797,
      "learning_rate": 1e-05,
      "loss": 6.4318,
      "step": 6164
    },
    {
      "epoch": 0.3340921409214092,
      "step": 6164,
      "training_loss": 6.1662068367004395
    },
    {
      "epoch": 0.33414634146341465,
      "step": 6165,
      "training_loss": 6.795995235443115
    },
    {
      "epoch": 0.33420054200542004,
      "step": 6166,
      "training_loss": 8.333433151245117
    },
    {
      "epoch": 0.3342547425474255,
      "step": 6167,
      "training_loss": 6.5260844230651855
    },
    {
      "epoch": 0.3343089430894309,
      "grad_norm": 24.31267547607422,
      "learning_rate": 1e-05,
      "loss": 6.9554,
      "step": 6168
    },
    {
      "epoch": 0.3343089430894309,
      "step": 6168,
      "training_loss": 6.771424770355225
    },
    {
      "epoch": 0.3343631436314363,
      "step": 6169,
      "training_loss": 6.805393695831299
    },
    {
      "epoch": 0.3344173441734417,
      "step": 6170,
      "training_loss": 7.123603343963623
    },
    {
      "epoch": 0.33447154471544716,
      "step": 6171,
      "training_loss": 6.897218227386475
    },
    {
      "epoch": 0.3345257452574526,
      "grad_norm": 27.308931350708008,
      "learning_rate": 1e-05,
      "loss": 6.8994,
      "step": 6172
    },
    {
      "epoch": 0.3345257452574526,
      "step": 6172,
      "training_loss": 7.599579334259033
    },
    {
      "epoch": 0.334579945799458,
      "step": 6173,
      "training_loss": 7.338629722595215
    },
    {
      "epoch": 0.33463414634146343,
      "step": 6174,
      "training_loss": 11.898221969604492
    },
    {
      "epoch": 0.3346883468834688,
      "step": 6175,
      "training_loss": 6.9452362060546875
    },
    {
      "epoch": 0.33474254742547427,
      "grad_norm": 17.879440307617188,
      "learning_rate": 1e-05,
      "loss": 8.4454,
      "step": 6176
    },
    {
      "epoch": 0.33474254742547427,
      "step": 6176,
      "training_loss": 6.82297420501709
    },
    {
      "epoch": 0.33479674796747966,
      "step": 6177,
      "training_loss": 5.958806991577148
    },
    {
      "epoch": 0.3348509485094851,
      "step": 6178,
      "training_loss": 6.297227382659912
    },
    {
      "epoch": 0.3349051490514905,
      "step": 6179,
      "training_loss": 6.768820762634277
    },
    {
      "epoch": 0.33495934959349594,
      "grad_norm": 23.706445693969727,
      "learning_rate": 1e-05,
      "loss": 6.462,
      "step": 6180
    },
    {
      "epoch": 0.33495934959349594,
      "step": 6180,
      "training_loss": 7.067298412322998
    },
    {
      "epoch": 0.3350135501355014,
      "step": 6181,
      "training_loss": 7.136777877807617
    },
    {
      "epoch": 0.33506775067750677,
      "step": 6182,
      "training_loss": 6.195103168487549
    },
    {
      "epoch": 0.3351219512195122,
      "step": 6183,
      "training_loss": 7.233058452606201
    },
    {
      "epoch": 0.3351761517615176,
      "grad_norm": 21.996257781982422,
      "learning_rate": 1e-05,
      "loss": 6.9081,
      "step": 6184
    },
    {
      "epoch": 0.3351761517615176,
      "step": 6184,
      "training_loss": 6.938024044036865
    },
    {
      "epoch": 0.33523035230352305,
      "step": 6185,
      "training_loss": 6.842136859893799
    },
    {
      "epoch": 0.33528455284552844,
      "step": 6186,
      "training_loss": 7.203336715698242
    },
    {
      "epoch": 0.3353387533875339,
      "step": 6187,
      "training_loss": 7.8568925857543945
    },
    {
      "epoch": 0.3353929539295393,
      "grad_norm": 25.69923210144043,
      "learning_rate": 1e-05,
      "loss": 7.2101,
      "step": 6188
    },
    {
      "epoch": 0.3353929539295393,
      "step": 6188,
      "training_loss": 8.085953712463379
    },
    {
      "epoch": 0.3354471544715447,
      "step": 6189,
      "training_loss": 7.253179550170898
    },
    {
      "epoch": 0.33550135501355016,
      "step": 6190,
      "training_loss": 3.991499900817871
    },
    {
      "epoch": 0.33555555555555555,
      "step": 6191,
      "training_loss": 6.688772201538086
    },
    {
      "epoch": 0.335609756097561,
      "grad_norm": 24.464445114135742,
      "learning_rate": 1e-05,
      "loss": 6.5049,
      "step": 6192
    },
    {
      "epoch": 0.335609756097561,
      "step": 6192,
      "training_loss": 7.338552474975586
    },
    {
      "epoch": 0.3356639566395664,
      "step": 6193,
      "training_loss": 7.698235034942627
    },
    {
      "epoch": 0.33571815718157183,
      "step": 6194,
      "training_loss": 5.894566059112549
    },
    {
      "epoch": 0.3357723577235772,
      "step": 6195,
      "training_loss": 6.558442115783691
    },
    {
      "epoch": 0.33582655826558266,
      "grad_norm": 22.334644317626953,
      "learning_rate": 1e-05,
      "loss": 6.8724,
      "step": 6196
    },
    {
      "epoch": 0.33582655826558266,
      "step": 6196,
      "training_loss": 6.157556056976318
    },
    {
      "epoch": 0.33588075880758805,
      "step": 6197,
      "training_loss": 6.071434020996094
    },
    {
      "epoch": 0.3359349593495935,
      "step": 6198,
      "training_loss": 5.844330787658691
    },
    {
      "epoch": 0.33598915989159894,
      "step": 6199,
      "training_loss": 5.77523946762085
    },
    {
      "epoch": 0.33604336043360433,
      "grad_norm": 55.71149444580078,
      "learning_rate": 1e-05,
      "loss": 5.9621,
      "step": 6200
    },
    {
      "epoch": 0.33604336043360433,
      "step": 6200,
      "training_loss": 7.233241558074951
    },
    {
      "epoch": 0.3360975609756098,
      "step": 6201,
      "training_loss": 4.227163314819336
    },
    {
      "epoch": 0.33615176151761517,
      "step": 6202,
      "training_loss": 7.727740287780762
    },
    {
      "epoch": 0.3362059620596206,
      "step": 6203,
      "training_loss": 5.794949054718018
    },
    {
      "epoch": 0.336260162601626,
      "grad_norm": 26.29555320739746,
      "learning_rate": 1e-05,
      "loss": 6.2458,
      "step": 6204
    },
    {
      "epoch": 0.336260162601626,
      "step": 6204,
      "training_loss": 7.4484148025512695
    },
    {
      "epoch": 0.33631436314363145,
      "step": 6205,
      "training_loss": 6.523226737976074
    },
    {
      "epoch": 0.33636856368563683,
      "step": 6206,
      "training_loss": 5.7302656173706055
    },
    {
      "epoch": 0.3364227642276423,
      "step": 6207,
      "training_loss": 6.061172962188721
    },
    {
      "epoch": 0.3364769647696477,
      "grad_norm": 28.170644760131836,
      "learning_rate": 1e-05,
      "loss": 6.4408,
      "step": 6208
    },
    {
      "epoch": 0.3364769647696477,
      "step": 6208,
      "training_loss": 6.270484447479248
    },
    {
      "epoch": 0.3365311653116531,
      "step": 6209,
      "training_loss": 6.536577224731445
    },
    {
      "epoch": 0.33658536585365856,
      "step": 6210,
      "training_loss": 9.014437675476074
    },
    {
      "epoch": 0.33663956639566395,
      "step": 6211,
      "training_loss": 7.247890949249268
    },
    {
      "epoch": 0.3366937669376694,
      "grad_norm": 26.602968215942383,
      "learning_rate": 1e-05,
      "loss": 7.2673,
      "step": 6212
    },
    {
      "epoch": 0.3366937669376694,
      "step": 6212,
      "training_loss": 7.876255989074707
    },
    {
      "epoch": 0.3367479674796748,
      "step": 6213,
      "training_loss": 6.675606727600098
    },
    {
      "epoch": 0.3368021680216802,
      "step": 6214,
      "training_loss": 7.3839945793151855
    },
    {
      "epoch": 0.3368563685636856,
      "step": 6215,
      "training_loss": 7.683138847351074
    },
    {
      "epoch": 0.33691056910569106,
      "grad_norm": 24.209110260009766,
      "learning_rate": 1e-05,
      "loss": 7.4047,
      "step": 6216
    },
    {
      "epoch": 0.33691056910569106,
      "step": 6216,
      "training_loss": 5.777529239654541
    },
    {
      "epoch": 0.33696476964769645,
      "step": 6217,
      "training_loss": 7.475255489349365
    },
    {
      "epoch": 0.3370189701897019,
      "step": 6218,
      "training_loss": 6.166440963745117
    },
    {
      "epoch": 0.33707317073170734,
      "step": 6219,
      "training_loss": 4.769937038421631
    },
    {
      "epoch": 0.33712737127371273,
      "grad_norm": 28.528160095214844,
      "learning_rate": 1e-05,
      "loss": 6.0473,
      "step": 6220
    },
    {
      "epoch": 0.33712737127371273,
      "step": 6220,
      "training_loss": 7.386740207672119
    },
    {
      "epoch": 0.3371815718157182,
      "step": 6221,
      "training_loss": 6.214078903198242
    },
    {
      "epoch": 0.33723577235772356,
      "step": 6222,
      "training_loss": 7.182382583618164
    },
    {
      "epoch": 0.337289972899729,
      "step": 6223,
      "training_loss": 5.146989345550537
    },
    {
      "epoch": 0.3373441734417344,
      "grad_norm": 19.061487197875977,
      "learning_rate": 1e-05,
      "loss": 6.4825,
      "step": 6224
    },
    {
      "epoch": 0.3373441734417344,
      "step": 6224,
      "training_loss": 7.438662528991699
    },
    {
      "epoch": 0.33739837398373984,
      "step": 6225,
      "training_loss": 7.487960338592529
    },
    {
      "epoch": 0.33745257452574523,
      "step": 6226,
      "training_loss": 6.586421489715576
    },
    {
      "epoch": 0.3375067750677507,
      "step": 6227,
      "training_loss": 6.215594291687012
    },
    {
      "epoch": 0.3375609756097561,
      "grad_norm": 26.209808349609375,
      "learning_rate": 1e-05,
      "loss": 6.9322,
      "step": 6228
    },
    {
      "epoch": 0.3375609756097561,
      "step": 6228,
      "training_loss": 5.630683898925781
    },
    {
      "epoch": 0.3376151761517615,
      "step": 6229,
      "training_loss": 8.044766426086426
    },
    {
      "epoch": 0.33766937669376695,
      "step": 6230,
      "training_loss": 7.62612247467041
    },
    {
      "epoch": 0.33772357723577234,
      "step": 6231,
      "training_loss": 7.572805404663086
    },
    {
      "epoch": 0.3377777777777778,
      "grad_norm": 24.84682273864746,
      "learning_rate": 1e-05,
      "loss": 7.2186,
      "step": 6232
    },
    {
      "epoch": 0.3377777777777778,
      "step": 6232,
      "training_loss": 6.66788387298584
    },
    {
      "epoch": 0.3378319783197832,
      "step": 6233,
      "training_loss": 7.191273212432861
    },
    {
      "epoch": 0.3378861788617886,
      "step": 6234,
      "training_loss": 6.784770965576172
    },
    {
      "epoch": 0.337940379403794,
      "step": 6235,
      "training_loss": 6.673260688781738
    },
    {
      "epoch": 0.33799457994579946,
      "grad_norm": 19.669052124023438,
      "learning_rate": 1e-05,
      "loss": 6.8293,
      "step": 6236
    },
    {
      "epoch": 0.33799457994579946,
      "step": 6236,
      "training_loss": 6.683608055114746
    },
    {
      "epoch": 0.3380487804878049,
      "step": 6237,
      "training_loss": 7.305322647094727
    },
    {
      "epoch": 0.3381029810298103,
      "step": 6238,
      "training_loss": 7.7246994972229
    },
    {
      "epoch": 0.33815718157181573,
      "step": 6239,
      "training_loss": 7.430775165557861
    },
    {
      "epoch": 0.3382113821138211,
      "grad_norm": 22.578697204589844,
      "learning_rate": 1e-05,
      "loss": 7.2861,
      "step": 6240
    },
    {
      "epoch": 0.3382113821138211,
      "step": 6240,
      "training_loss": 6.159229755401611
    },
    {
      "epoch": 0.33826558265582657,
      "step": 6241,
      "training_loss": 4.06314754486084
    },
    {
      "epoch": 0.33831978319783196,
      "step": 6242,
      "training_loss": 6.517626762390137
    },
    {
      "epoch": 0.3383739837398374,
      "step": 6243,
      "training_loss": 6.066029071807861
    },
    {
      "epoch": 0.3384281842818428,
      "grad_norm": 30.909727096557617,
      "learning_rate": 1e-05,
      "loss": 5.7015,
      "step": 6244
    },
    {
      "epoch": 0.3384281842818428,
      "step": 6244,
      "training_loss": 7.360775470733643
    },
    {
      "epoch": 0.33848238482384824,
      "step": 6245,
      "training_loss": 6.947822570800781
    },
    {
      "epoch": 0.3385365853658537,
      "step": 6246,
      "training_loss": 7.8587822914123535
    },
    {
      "epoch": 0.33859078590785907,
      "step": 6247,
      "training_loss": 6.570969104766846
    },
    {
      "epoch": 0.3386449864498645,
      "grad_norm": 20.239120483398438,
      "learning_rate": 1e-05,
      "loss": 7.1846,
      "step": 6248
    },
    {
      "epoch": 0.3386449864498645,
      "step": 6248,
      "training_loss": 5.227697849273682
    },
    {
      "epoch": 0.3386991869918699,
      "step": 6249,
      "training_loss": 5.796826362609863
    },
    {
      "epoch": 0.33875338753387535,
      "step": 6250,
      "training_loss": 6.5591206550598145
    },
    {
      "epoch": 0.33880758807588074,
      "step": 6251,
      "training_loss": 6.236870288848877
    },
    {
      "epoch": 0.3388617886178862,
      "grad_norm": 20.923749923706055,
      "learning_rate": 1e-05,
      "loss": 5.9551,
      "step": 6252
    },
    {
      "epoch": 0.3388617886178862,
      "step": 6252,
      "training_loss": 6.665060043334961
    },
    {
      "epoch": 0.3389159891598916,
      "step": 6253,
      "training_loss": 6.9359822273254395
    },
    {
      "epoch": 0.338970189701897,
      "step": 6254,
      "training_loss": 3.8466999530792236
    },
    {
      "epoch": 0.33902439024390246,
      "step": 6255,
      "training_loss": 7.620935916900635
    },
    {
      "epoch": 0.33907859078590785,
      "grad_norm": 30.189735412597656,
      "learning_rate": 1e-05,
      "loss": 6.2672,
      "step": 6256
    },
    {
      "epoch": 0.33907859078590785,
      "step": 6256,
      "training_loss": 7.299194812774658
    },
    {
      "epoch": 0.3391327913279133,
      "step": 6257,
      "training_loss": 6.425755977630615
    },
    {
      "epoch": 0.3391869918699187,
      "step": 6258,
      "training_loss": 7.058590412139893
    },
    {
      "epoch": 0.33924119241192413,
      "step": 6259,
      "training_loss": 7.213994026184082
    },
    {
      "epoch": 0.3392953929539295,
      "grad_norm": 23.50928497314453,
      "learning_rate": 1e-05,
      "loss": 6.9994,
      "step": 6260
    },
    {
      "epoch": 0.3392953929539295,
      "step": 6260,
      "training_loss": 7.470625400543213
    },
    {
      "epoch": 0.33934959349593496,
      "step": 6261,
      "training_loss": 7.199131011962891
    },
    {
      "epoch": 0.33940379403794035,
      "step": 6262,
      "training_loss": 6.769397735595703
    },
    {
      "epoch": 0.3394579945799458,
      "step": 6263,
      "training_loss": 7.480622291564941
    },
    {
      "epoch": 0.33951219512195124,
      "grad_norm": 25.745649337768555,
      "learning_rate": 1e-05,
      "loss": 7.2299,
      "step": 6264
    },
    {
      "epoch": 0.33951219512195124,
      "step": 6264,
      "training_loss": 7.802855968475342
    },
    {
      "epoch": 0.33956639566395663,
      "step": 6265,
      "training_loss": 4.918275833129883
    },
    {
      "epoch": 0.3396205962059621,
      "step": 6266,
      "training_loss": 7.788371562957764
    },
    {
      "epoch": 0.33967479674796747,
      "step": 6267,
      "training_loss": 6.661841869354248
    },
    {
      "epoch": 0.3397289972899729,
      "grad_norm": 22.658252716064453,
      "learning_rate": 1e-05,
      "loss": 6.7928,
      "step": 6268
    },
    {
      "epoch": 0.3397289972899729,
      "step": 6268,
      "training_loss": 6.988081455230713
    },
    {
      "epoch": 0.3397831978319783,
      "step": 6269,
      "training_loss": 8.133706092834473
    },
    {
      "epoch": 0.33983739837398375,
      "step": 6270,
      "training_loss": 6.829588413238525
    },
    {
      "epoch": 0.33989159891598913,
      "step": 6271,
      "training_loss": 7.274930477142334
    },
    {
      "epoch": 0.3399457994579946,
      "grad_norm": 36.80942153930664,
      "learning_rate": 1e-05,
      "loss": 7.3066,
      "step": 6272
    },
    {
      "epoch": 0.3399457994579946,
      "step": 6272,
      "training_loss": 6.629634380340576
    },
    {
      "epoch": 0.34,
      "step": 6273,
      "training_loss": 6.556410789489746
    },
    {
      "epoch": 0.3400542005420054,
      "step": 6274,
      "training_loss": 7.391041278839111
    },
    {
      "epoch": 0.34010840108401086,
      "step": 6275,
      "training_loss": 5.841085433959961
    },
    {
      "epoch": 0.34016260162601625,
      "grad_norm": 29.57818603515625,
      "learning_rate": 1e-05,
      "loss": 6.6045,
      "step": 6276
    },
    {
      "epoch": 0.34016260162601625,
      "step": 6276,
      "training_loss": 6.68731689453125
    },
    {
      "epoch": 0.3402168021680217,
      "step": 6277,
      "training_loss": 5.5486578941345215
    },
    {
      "epoch": 0.3402710027100271,
      "step": 6278,
      "training_loss": 6.568977355957031
    },
    {
      "epoch": 0.3403252032520325,
      "step": 6279,
      "training_loss": 7.54710578918457
    },
    {
      "epoch": 0.3403794037940379,
      "grad_norm": 48.415626525878906,
      "learning_rate": 1e-05,
      "loss": 6.588,
      "step": 6280
    },
    {
      "epoch": 0.3403794037940379,
      "step": 6280,
      "training_loss": 6.83445930480957
    },
    {
      "epoch": 0.34043360433604336,
      "step": 6281,
      "training_loss": 8.232970237731934
    },
    {
      "epoch": 0.3404878048780488,
      "step": 6282,
      "training_loss": 7.239229202270508
    },
    {
      "epoch": 0.3405420054200542,
      "step": 6283,
      "training_loss": 6.068480968475342
    },
    {
      "epoch": 0.34059620596205964,
      "grad_norm": 30.897146224975586,
      "learning_rate": 1e-05,
      "loss": 7.0938,
      "step": 6284
    },
    {
      "epoch": 0.34059620596205964,
      "step": 6284,
      "training_loss": 5.931920051574707
    },
    {
      "epoch": 0.34065040650406503,
      "step": 6285,
      "training_loss": 7.7988176345825195
    },
    {
      "epoch": 0.3407046070460705,
      "step": 6286,
      "training_loss": 6.812915802001953
    },
    {
      "epoch": 0.34075880758807586,
      "step": 6287,
      "training_loss": 6.962589740753174
    },
    {
      "epoch": 0.3408130081300813,
      "grad_norm": 16.623821258544922,
      "learning_rate": 1e-05,
      "loss": 6.8766,
      "step": 6288
    },
    {
      "epoch": 0.3408130081300813,
      "step": 6288,
      "training_loss": 5.665805816650391
    },
    {
      "epoch": 0.3408672086720867,
      "step": 6289,
      "training_loss": 7.028487205505371
    },
    {
      "epoch": 0.34092140921409214,
      "step": 6290,
      "training_loss": 5.810740947723389
    },
    {
      "epoch": 0.3409756097560976,
      "step": 6291,
      "training_loss": 6.595656871795654
    },
    {
      "epoch": 0.341029810298103,
      "grad_norm": 17.11758804321289,
      "learning_rate": 1e-05,
      "loss": 6.2752,
      "step": 6292
    },
    {
      "epoch": 0.341029810298103,
      "step": 6292,
      "training_loss": 6.100787162780762
    },
    {
      "epoch": 0.3410840108401084,
      "step": 6293,
      "training_loss": 5.8056182861328125
    },
    {
      "epoch": 0.3411382113821138,
      "step": 6294,
      "training_loss": 6.904923439025879
    },
    {
      "epoch": 0.34119241192411925,
      "step": 6295,
      "training_loss": 7.197823524475098
    },
    {
      "epoch": 0.34124661246612464,
      "grad_norm": 35.751522064208984,
      "learning_rate": 1e-05,
      "loss": 6.5023,
      "step": 6296
    },
    {
      "epoch": 0.34124661246612464,
      "step": 6296,
      "training_loss": 6.193079471588135
    },
    {
      "epoch": 0.3413008130081301,
      "step": 6297,
      "training_loss": 6.1868414878845215
    },
    {
      "epoch": 0.3413550135501355,
      "step": 6298,
      "training_loss": 6.884649753570557
    },
    {
      "epoch": 0.3414092140921409,
      "step": 6299,
      "training_loss": 7.053680896759033
    },
    {
      "epoch": 0.34146341463414637,
      "grad_norm": 32.37422561645508,
      "learning_rate": 1e-05,
      "loss": 6.5796,
      "step": 6300
    },
    {
      "epoch": 0.34146341463414637,
      "step": 6300,
      "training_loss": 6.588493824005127
    },
    {
      "epoch": 0.34151761517615176,
      "step": 6301,
      "training_loss": 7.3429274559021
    },
    {
      "epoch": 0.3415718157181572,
      "step": 6302,
      "training_loss": 6.193696022033691
    },
    {
      "epoch": 0.3416260162601626,
      "step": 6303,
      "training_loss": 6.615804672241211
    },
    {
      "epoch": 0.34168021680216804,
      "grad_norm": 15.850741386413574,
      "learning_rate": 1e-05,
      "loss": 6.6852,
      "step": 6304
    },
    {
      "epoch": 0.34168021680216804,
      "step": 6304,
      "training_loss": 7.167088508605957
    },
    {
      "epoch": 0.3417344173441734,
      "step": 6305,
      "training_loss": 6.255301475524902
    },
    {
      "epoch": 0.34178861788617887,
      "step": 6306,
      "training_loss": 5.172689914703369
    },
    {
      "epoch": 0.34184281842818426,
      "step": 6307,
      "training_loss": 7.683225631713867
    },
    {
      "epoch": 0.3418970189701897,
      "grad_norm": 38.6815185546875,
      "learning_rate": 1e-05,
      "loss": 6.5696,
      "step": 6308
    },
    {
      "epoch": 0.3418970189701897,
      "step": 6308,
      "training_loss": 7.059417247772217
    },
    {
      "epoch": 0.34195121951219515,
      "step": 6309,
      "training_loss": 6.555639266967773
    },
    {
      "epoch": 0.34200542005420054,
      "step": 6310,
      "training_loss": 6.982062816619873
    },
    {
      "epoch": 0.342059620596206,
      "step": 6311,
      "training_loss": 7.009784698486328
    },
    {
      "epoch": 0.34211382113821137,
      "grad_norm": 24.712234497070312,
      "learning_rate": 1e-05,
      "loss": 6.9017,
      "step": 6312
    },
    {
      "epoch": 0.34211382113821137,
      "step": 6312,
      "training_loss": 6.574111461639404
    },
    {
      "epoch": 0.3421680216802168,
      "step": 6313,
      "training_loss": 6.435896396636963
    },
    {
      "epoch": 0.3422222222222222,
      "step": 6314,
      "training_loss": 8.776126861572266
    },
    {
      "epoch": 0.34227642276422765,
      "step": 6315,
      "training_loss": 6.814724922180176
    },
    {
      "epoch": 0.34233062330623304,
      "grad_norm": 27.375497817993164,
      "learning_rate": 1e-05,
      "loss": 7.1502,
      "step": 6316
    },
    {
      "epoch": 0.34233062330623304,
      "step": 6316,
      "training_loss": 7.433483600616455
    },
    {
      "epoch": 0.3423848238482385,
      "step": 6317,
      "training_loss": 6.974978923797607
    },
    {
      "epoch": 0.34243902439024393,
      "step": 6318,
      "training_loss": 7.055800914764404
    },
    {
      "epoch": 0.3424932249322493,
      "step": 6319,
      "training_loss": 6.516908168792725
    },
    {
      "epoch": 0.34254742547425476,
      "grad_norm": 33.944053649902344,
      "learning_rate": 1e-05,
      "loss": 6.9953,
      "step": 6320
    },
    {
      "epoch": 0.34254742547425476,
      "step": 6320,
      "training_loss": 6.417811870574951
    },
    {
      "epoch": 0.34260162601626015,
      "step": 6321,
      "training_loss": 6.41992712020874
    },
    {
      "epoch": 0.3426558265582656,
      "step": 6322,
      "training_loss": 4.023117542266846
    },
    {
      "epoch": 0.342710027100271,
      "step": 6323,
      "training_loss": 7.8327155113220215
    },
    {
      "epoch": 0.34276422764227643,
      "grad_norm": 51.310062408447266,
      "learning_rate": 1e-05,
      "loss": 6.1734,
      "step": 6324
    },
    {
      "epoch": 0.34276422764227643,
      "step": 6324,
      "training_loss": 6.891510009765625
    },
    {
      "epoch": 0.3428184281842818,
      "step": 6325,
      "training_loss": 6.572791576385498
    },
    {
      "epoch": 0.34287262872628727,
      "step": 6326,
      "training_loss": 7.270503997802734
    },
    {
      "epoch": 0.3429268292682927,
      "step": 6327,
      "training_loss": 6.707022666931152
    },
    {
      "epoch": 0.3429810298102981,
      "grad_norm": 18.20306968688965,
      "learning_rate": 1e-05,
      "loss": 6.8605,
      "step": 6328
    },
    {
      "epoch": 0.3429810298102981,
      "step": 6328,
      "training_loss": 7.015679836273193
    },
    {
      "epoch": 0.34303523035230354,
      "step": 6329,
      "training_loss": 7.118435382843018
    },
    {
      "epoch": 0.34308943089430893,
      "step": 6330,
      "training_loss": 6.549257278442383
    },
    {
      "epoch": 0.3431436314363144,
      "step": 6331,
      "training_loss": 6.592155456542969
    },
    {
      "epoch": 0.34319783197831977,
      "grad_norm": 19.940004348754883,
      "learning_rate": 1e-05,
      "loss": 6.8189,
      "step": 6332
    },
    {
      "epoch": 0.34319783197831977,
      "step": 6332,
      "training_loss": 6.274853706359863
    },
    {
      "epoch": 0.3432520325203252,
      "step": 6333,
      "training_loss": 6.803891658782959
    },
    {
      "epoch": 0.3433062330623306,
      "step": 6334,
      "training_loss": 7.480794429779053
    },
    {
      "epoch": 0.34336043360433605,
      "step": 6335,
      "training_loss": 7.574959754943848
    },
    {
      "epoch": 0.3434146341463415,
      "grad_norm": 28.403823852539062,
      "learning_rate": 1e-05,
      "loss": 7.0336,
      "step": 6336
    },
    {
      "epoch": 0.3434146341463415,
      "step": 6336,
      "training_loss": 7.383708477020264
    },
    {
      "epoch": 0.3434688346883469,
      "step": 6337,
      "training_loss": 6.4098358154296875
    },
    {
      "epoch": 0.3435230352303523,
      "step": 6338,
      "training_loss": 4.747142314910889
    },
    {
      "epoch": 0.3435772357723577,
      "step": 6339,
      "training_loss": 5.660088539123535
    },
    {
      "epoch": 0.34363143631436316,
      "grad_norm": 30.0875301361084,
      "learning_rate": 1e-05,
      "loss": 6.0502,
      "step": 6340
    },
    {
      "epoch": 0.34363143631436316,
      "step": 6340,
      "training_loss": 6.206982612609863
    },
    {
      "epoch": 0.34368563685636855,
      "step": 6341,
      "training_loss": 6.0471696853637695
    },
    {
      "epoch": 0.343739837398374,
      "step": 6342,
      "training_loss": 6.864224910736084
    },
    {
      "epoch": 0.3437940379403794,
      "step": 6343,
      "training_loss": 6.505196571350098
    },
    {
      "epoch": 0.3438482384823848,
      "grad_norm": 28.83859634399414,
      "learning_rate": 1e-05,
      "loss": 6.4059,
      "step": 6344
    },
    {
      "epoch": 0.3438482384823848,
      "step": 6344,
      "training_loss": 5.554030418395996
    },
    {
      "epoch": 0.3439024390243902,
      "step": 6345,
      "training_loss": 6.716000080108643
    },
    {
      "epoch": 0.34395663956639566,
      "step": 6346,
      "training_loss": 5.258601665496826
    },
    {
      "epoch": 0.3440108401084011,
      "step": 6347,
      "training_loss": 4.468751430511475
    },
    {
      "epoch": 0.3440650406504065,
      "grad_norm": 37.80878829956055,
      "learning_rate": 1e-05,
      "loss": 5.4993,
      "step": 6348
    },
    {
      "epoch": 0.3440650406504065,
      "step": 6348,
      "training_loss": 3.577669620513916
    },
    {
      "epoch": 0.34411924119241194,
      "step": 6349,
      "training_loss": 5.637235164642334
    },
    {
      "epoch": 0.34417344173441733,
      "step": 6350,
      "training_loss": 6.908473968505859
    },
    {
      "epoch": 0.3442276422764228,
      "step": 6351,
      "training_loss": 6.321985244750977
    },
    {
      "epoch": 0.34428184281842816,
      "grad_norm": 22.651966094970703,
      "learning_rate": 1e-05,
      "loss": 5.6113,
      "step": 6352
    },
    {
      "epoch": 0.34428184281842816,
      "step": 6352,
      "training_loss": 5.345940113067627
    },
    {
      "epoch": 0.3443360433604336,
      "step": 6353,
      "training_loss": 6.9783101081848145
    },
    {
      "epoch": 0.344390243902439,
      "step": 6354,
      "training_loss": 5.252970218658447
    },
    {
      "epoch": 0.34444444444444444,
      "step": 6355,
      "training_loss": 6.822729587554932
    },
    {
      "epoch": 0.3444986449864499,
      "grad_norm": 17.766305923461914,
      "learning_rate": 1e-05,
      "loss": 6.1,
      "step": 6356
    },
    {
      "epoch": 0.3444986449864499,
      "step": 6356,
      "training_loss": 6.931325912475586
    },
    {
      "epoch": 0.3445528455284553,
      "step": 6357,
      "training_loss": 6.276830673217773
    },
    {
      "epoch": 0.3446070460704607,
      "step": 6358,
      "training_loss": 6.746407985687256
    },
    {
      "epoch": 0.3446612466124661,
      "step": 6359,
      "training_loss": 6.2693281173706055
    },
    {
      "epoch": 0.34471544715447155,
      "grad_norm": 19.62092399597168,
      "learning_rate": 1e-05,
      "loss": 6.556,
      "step": 6360
    },
    {
      "epoch": 0.34471544715447155,
      "step": 6360,
      "training_loss": 5.792294979095459
    },
    {
      "epoch": 0.34476964769647694,
      "step": 6361,
      "training_loss": 5.426393508911133
    },
    {
      "epoch": 0.3448238482384824,
      "step": 6362,
      "training_loss": 5.8724045753479
    },
    {
      "epoch": 0.3448780487804878,
      "step": 6363,
      "training_loss": 7.157192230224609
    },
    {
      "epoch": 0.3449322493224932,
      "grad_norm": 30.85821533203125,
      "learning_rate": 1e-05,
      "loss": 6.0621,
      "step": 6364
    },
    {
      "epoch": 0.3449322493224932,
      "step": 6364,
      "training_loss": 5.154865264892578
    },
    {
      "epoch": 0.34498644986449867,
      "step": 6365,
      "training_loss": 7.48168420791626
    },
    {
      "epoch": 0.34504065040650406,
      "step": 6366,
      "training_loss": 6.065273284912109
    },
    {
      "epoch": 0.3450948509485095,
      "step": 6367,
      "training_loss": 3.888328790664673
    },
    {
      "epoch": 0.3451490514905149,
      "grad_norm": 30.438852310180664,
      "learning_rate": 1e-05,
      "loss": 5.6475,
      "step": 6368
    },
    {
      "epoch": 0.3451490514905149,
      "step": 6368,
      "training_loss": 5.263718128204346
    },
    {
      "epoch": 0.34520325203252034,
      "step": 6369,
      "training_loss": 6.911509990692139
    },
    {
      "epoch": 0.3452574525745257,
      "step": 6370,
      "training_loss": 5.7375946044921875
    },
    {
      "epoch": 0.34531165311653117,
      "step": 6371,
      "training_loss": 7.393380165100098
    },
    {
      "epoch": 0.34536585365853656,
      "grad_norm": 17.46609115600586,
      "learning_rate": 1e-05,
      "loss": 6.3266,
      "step": 6372
    },
    {
      "epoch": 0.34536585365853656,
      "step": 6372,
      "training_loss": 4.853726387023926
    },
    {
      "epoch": 0.345420054200542,
      "step": 6373,
      "training_loss": 8.755541801452637
    },
    {
      "epoch": 0.34547425474254745,
      "step": 6374,
      "training_loss": 8.4414701461792
    },
    {
      "epoch": 0.34552845528455284,
      "step": 6375,
      "training_loss": 7.024077892303467
    },
    {
      "epoch": 0.3455826558265583,
      "grad_norm": 23.34258460998535,
      "learning_rate": 1e-05,
      "loss": 7.2687,
      "step": 6376
    },
    {
      "epoch": 0.3455826558265583,
      "step": 6376,
      "training_loss": 7.268304824829102
    },
    {
      "epoch": 0.34563685636856367,
      "step": 6377,
      "training_loss": 7.174885272979736
    },
    {
      "epoch": 0.3456910569105691,
      "step": 6378,
      "training_loss": 6.465145111083984
    },
    {
      "epoch": 0.3457452574525745,
      "step": 6379,
      "training_loss": 6.263390064239502
    },
    {
      "epoch": 0.34579945799457995,
      "grad_norm": 20.906435012817383,
      "learning_rate": 1e-05,
      "loss": 6.7929,
      "step": 6380
    },
    {
      "epoch": 0.34579945799457995,
      "step": 6380,
      "training_loss": 6.5118489265441895
    },
    {
      "epoch": 0.34585365853658534,
      "step": 6381,
      "training_loss": 3.9549622535705566
    },
    {
      "epoch": 0.3459078590785908,
      "step": 6382,
      "training_loss": 7.238216876983643
    },
    {
      "epoch": 0.34596205962059623,
      "step": 6383,
      "training_loss": 6.18924617767334
    },
    {
      "epoch": 0.3460162601626016,
      "grad_norm": 29.61685562133789,
      "learning_rate": 1e-05,
      "loss": 5.9736,
      "step": 6384
    },
    {
      "epoch": 0.3460162601626016,
      "step": 6384,
      "training_loss": 6.159909248352051
    },
    {
      "epoch": 0.34607046070460706,
      "step": 6385,
      "training_loss": 7.016173839569092
    },
    {
      "epoch": 0.34612466124661245,
      "step": 6386,
      "training_loss": 6.183467864990234
    },
    {
      "epoch": 0.3461788617886179,
      "step": 6387,
      "training_loss": 6.584869384765625
    },
    {
      "epoch": 0.3462330623306233,
      "grad_norm": 29.38451385498047,
      "learning_rate": 1e-05,
      "loss": 6.4861,
      "step": 6388
    },
    {
      "epoch": 0.3462330623306233,
      "step": 6388,
      "training_loss": 7.184560775756836
    },
    {
      "epoch": 0.34628726287262873,
      "step": 6389,
      "training_loss": 7.541079521179199
    },
    {
      "epoch": 0.3463414634146341,
      "step": 6390,
      "training_loss": 7.149069786071777
    },
    {
      "epoch": 0.34639566395663957,
      "step": 6391,
      "training_loss": 6.927952289581299
    },
    {
      "epoch": 0.346449864498645,
      "grad_norm": 21.234243392944336,
      "learning_rate": 1e-05,
      "loss": 7.2007,
      "step": 6392
    },
    {
      "epoch": 0.346449864498645,
      "step": 6392,
      "training_loss": 3.6518797874450684
    },
    {
      "epoch": 0.3465040650406504,
      "step": 6393,
      "training_loss": 6.322025299072266
    },
    {
      "epoch": 0.34655826558265584,
      "step": 6394,
      "training_loss": 6.943960666656494
    },
    {
      "epoch": 0.34661246612466123,
      "step": 6395,
      "training_loss": 7.612777233123779
    },
    {
      "epoch": 0.3466666666666667,
      "grad_norm": 32.93720245361328,
      "learning_rate": 1e-05,
      "loss": 6.1327,
      "step": 6396
    },
    {
      "epoch": 0.3466666666666667,
      "step": 6396,
      "training_loss": 7.2560553550720215
    },
    {
      "epoch": 0.34672086720867207,
      "step": 6397,
      "training_loss": 7.412034034729004
    },
    {
      "epoch": 0.3467750677506775,
      "step": 6398,
      "training_loss": 5.251104354858398
    },
    {
      "epoch": 0.3468292682926829,
      "step": 6399,
      "training_loss": 7.030162811279297
    },
    {
      "epoch": 0.34688346883468835,
      "grad_norm": 51.68379211425781,
      "learning_rate": 1e-05,
      "loss": 6.7373,
      "step": 6400
    },
    {
      "epoch": 0.34688346883468835,
      "step": 6400,
      "training_loss": 7.134955406188965
    },
    {
      "epoch": 0.3469376693766938,
      "step": 6401,
      "training_loss": 7.668304443359375
    },
    {
      "epoch": 0.3469918699186992,
      "step": 6402,
      "training_loss": 7.617717742919922
    },
    {
      "epoch": 0.3470460704607046,
      "step": 6403,
      "training_loss": 7.762385845184326
    },
    {
      "epoch": 0.34710027100271,
      "grad_norm": 48.46792984008789,
      "learning_rate": 1e-05,
      "loss": 7.5458,
      "step": 6404
    },
    {
      "epoch": 0.34710027100271,
      "step": 6404,
      "training_loss": 3.51908278465271
    },
    {
      "epoch": 0.34715447154471546,
      "step": 6405,
      "training_loss": 7.57853364944458
    },
    {
      "epoch": 0.34720867208672085,
      "step": 6406,
      "training_loss": 6.9521403312683105
    },
    {
      "epoch": 0.3472628726287263,
      "step": 6407,
      "training_loss": 8.014122009277344
    },
    {
      "epoch": 0.3473170731707317,
      "grad_norm": 61.57145309448242,
      "learning_rate": 1e-05,
      "loss": 6.516,
      "step": 6408
    },
    {
      "epoch": 0.3473170731707317,
      "step": 6408,
      "training_loss": 8.788102149963379
    },
    {
      "epoch": 0.3473712737127371,
      "step": 6409,
      "training_loss": 7.140857696533203
    },
    {
      "epoch": 0.3474254742547426,
      "step": 6410,
      "training_loss": 5.22325325012207
    },
    {
      "epoch": 0.34747967479674796,
      "step": 6411,
      "training_loss": 6.377359867095947
    },
    {
      "epoch": 0.3475338753387534,
      "grad_norm": 21.359596252441406,
      "learning_rate": 1e-05,
      "loss": 6.8824,
      "step": 6412
    },
    {
      "epoch": 0.3475338753387534,
      "step": 6412,
      "training_loss": 7.033940315246582
    },
    {
      "epoch": 0.3475880758807588,
      "step": 6413,
      "training_loss": 7.1536760330200195
    },
    {
      "epoch": 0.34764227642276424,
      "step": 6414,
      "training_loss": 7.402554988861084
    },
    {
      "epoch": 0.34769647696476963,
      "step": 6415,
      "training_loss": 5.5572075843811035
    },
    {
      "epoch": 0.3477506775067751,
      "grad_norm": 28.90810203552246,
      "learning_rate": 1e-05,
      "loss": 6.7868,
      "step": 6416
    },
    {
      "epoch": 0.3477506775067751,
      "step": 6416,
      "training_loss": 5.926836967468262
    },
    {
      "epoch": 0.34780487804878046,
      "step": 6417,
      "training_loss": 7.357771396636963
    },
    {
      "epoch": 0.3478590785907859,
      "step": 6418,
      "training_loss": 6.9245476722717285
    },
    {
      "epoch": 0.34791327913279135,
      "step": 6419,
      "training_loss": 6.13192892074585
    },
    {
      "epoch": 0.34796747967479674,
      "grad_norm": 25.033769607543945,
      "learning_rate": 1e-05,
      "loss": 6.5853,
      "step": 6420
    },
    {
      "epoch": 0.34796747967479674,
      "step": 6420,
      "training_loss": 7.449134349822998
    },
    {
      "epoch": 0.3480216802168022,
      "step": 6421,
      "training_loss": 7.6546950340271
    },
    {
      "epoch": 0.3480758807588076,
      "step": 6422,
      "training_loss": 5.249676704406738
    },
    {
      "epoch": 0.348130081300813,
      "step": 6423,
      "training_loss": 5.042140007019043
    },
    {
      "epoch": 0.3481842818428184,
      "grad_norm": 20.077943801879883,
      "learning_rate": 1e-05,
      "loss": 6.3489,
      "step": 6424
    },
    {
      "epoch": 0.3481842818428184,
      "step": 6424,
      "training_loss": 7.76718807220459
    },
    {
      "epoch": 0.34823848238482386,
      "step": 6425,
      "training_loss": 7.090789318084717
    },
    {
      "epoch": 0.34829268292682924,
      "step": 6426,
      "training_loss": 6.740455150604248
    },
    {
      "epoch": 0.3483468834688347,
      "step": 6427,
      "training_loss": 7.795472145080566
    },
    {
      "epoch": 0.34840108401084013,
      "grad_norm": 20.24775505065918,
      "learning_rate": 1e-05,
      "loss": 7.3485,
      "step": 6428
    },
    {
      "epoch": 0.34840108401084013,
      "step": 6428,
      "training_loss": 6.601312160491943
    },
    {
      "epoch": 0.3484552845528455,
      "step": 6429,
      "training_loss": 4.186994552612305
    },
    {
      "epoch": 0.34850948509485097,
      "step": 6430,
      "training_loss": 5.905497074127197
    },
    {
      "epoch": 0.34856368563685636,
      "step": 6431,
      "training_loss": 7.365419387817383
    },
    {
      "epoch": 0.3486178861788618,
      "grad_norm": 24.495412826538086,
      "learning_rate": 1e-05,
      "loss": 6.0148,
      "step": 6432
    },
    {
      "epoch": 0.3486178861788618,
      "step": 6432,
      "training_loss": 7.950517177581787
    },
    {
      "epoch": 0.3486720867208672,
      "step": 6433,
      "training_loss": 8.329634666442871
    },
    {
      "epoch": 0.34872628726287264,
      "step": 6434,
      "training_loss": 6.296882152557373
    },
    {
      "epoch": 0.348780487804878,
      "step": 6435,
      "training_loss": 5.508078575134277
    },
    {
      "epoch": 0.34883468834688347,
      "grad_norm": 24.41019630432129,
      "learning_rate": 1e-05,
      "loss": 7.0213,
      "step": 6436
    },
    {
      "epoch": 0.34883468834688347,
      "step": 6436,
      "training_loss": 7.198047637939453
    },
    {
      "epoch": 0.3488888888888889,
      "step": 6437,
      "training_loss": 6.18460750579834
    },
    {
      "epoch": 0.3489430894308943,
      "step": 6438,
      "training_loss": 7.135682582855225
    },
    {
      "epoch": 0.34899728997289975,
      "step": 6439,
      "training_loss": 8.664691925048828
    },
    {
      "epoch": 0.34905149051490514,
      "grad_norm": 44.99592971801758,
      "learning_rate": 1e-05,
      "loss": 7.2958,
      "step": 6440
    },
    {
      "epoch": 0.34905149051490514,
      "step": 6440,
      "training_loss": 6.6530046463012695
    },
    {
      "epoch": 0.3491056910569106,
      "step": 6441,
      "training_loss": 7.8564229011535645
    },
    {
      "epoch": 0.349159891598916,
      "step": 6442,
      "training_loss": 7.168155670166016
    },
    {
      "epoch": 0.3492140921409214,
      "step": 6443,
      "training_loss": 7.756242752075195
    },
    {
      "epoch": 0.3492682926829268,
      "grad_norm": 23.935251235961914,
      "learning_rate": 1e-05,
      "loss": 7.3585,
      "step": 6444
    },
    {
      "epoch": 0.3492682926829268,
      "step": 6444,
      "training_loss": 8.0775728225708
    },
    {
      "epoch": 0.34932249322493225,
      "step": 6445,
      "training_loss": 4.193600177764893
    },
    {
      "epoch": 0.3493766937669377,
      "step": 6446,
      "training_loss": 6.157063007354736
    },
    {
      "epoch": 0.3494308943089431,
      "step": 6447,
      "training_loss": 3.142308473587036
    },
    {
      "epoch": 0.34948509485094853,
      "grad_norm": 31.063631057739258,
      "learning_rate": 1e-05,
      "loss": 5.3926,
      "step": 6448
    },
    {
      "epoch": 0.34948509485094853,
      "step": 6448,
      "training_loss": 6.327059268951416
    },
    {
      "epoch": 0.3495392953929539,
      "step": 6449,
      "training_loss": 6.8822736740112305
    },
    {
      "epoch": 0.34959349593495936,
      "step": 6450,
      "training_loss": 7.231649398803711
    },
    {
      "epoch": 0.34964769647696475,
      "step": 6451,
      "training_loss": 6.717939853668213
    },
    {
      "epoch": 0.3497018970189702,
      "grad_norm": 18.255142211914062,
      "learning_rate": 1e-05,
      "loss": 6.7897,
      "step": 6452
    },
    {
      "epoch": 0.3497018970189702,
      "step": 6452,
      "training_loss": 6.970008373260498
    },
    {
      "epoch": 0.3497560975609756,
      "step": 6453,
      "training_loss": 6.352059841156006
    },
    {
      "epoch": 0.34981029810298103,
      "step": 6454,
      "training_loss": 6.724672317504883
    },
    {
      "epoch": 0.3498644986449865,
      "step": 6455,
      "training_loss": 6.855196952819824
    },
    {
      "epoch": 0.34991869918699187,
      "grad_norm": 25.51972770690918,
      "learning_rate": 1e-05,
      "loss": 6.7255,
      "step": 6456
    },
    {
      "epoch": 0.34991869918699187,
      "step": 6456,
      "training_loss": 6.3620429039001465
    },
    {
      "epoch": 0.3499728997289973,
      "step": 6457,
      "training_loss": 6.815540313720703
    },
    {
      "epoch": 0.3500271002710027,
      "step": 6458,
      "training_loss": 8.159809112548828
    },
    {
      "epoch": 0.35008130081300814,
      "step": 6459,
      "training_loss": 6.666754245758057
    },
    {
      "epoch": 0.35013550135501353,
      "grad_norm": 34.65127182006836,
      "learning_rate": 1e-05,
      "loss": 7.001,
      "step": 6460
    },
    {
      "epoch": 0.35013550135501353,
      "step": 6460,
      "training_loss": 7.194031238555908
    },
    {
      "epoch": 0.350189701897019,
      "step": 6461,
      "training_loss": 7.0448408126831055
    },
    {
      "epoch": 0.35024390243902437,
      "step": 6462,
      "training_loss": 6.766623497009277
    },
    {
      "epoch": 0.3502981029810298,
      "step": 6463,
      "training_loss": 6.958500385284424
    },
    {
      "epoch": 0.35035230352303526,
      "grad_norm": 27.05451202392578,
      "learning_rate": 1e-05,
      "loss": 6.991,
      "step": 6464
    },
    {
      "epoch": 0.35035230352303526,
      "step": 6464,
      "training_loss": 7.106897830963135
    },
    {
      "epoch": 0.35040650406504065,
      "step": 6465,
      "training_loss": 5.8979315757751465
    },
    {
      "epoch": 0.3504607046070461,
      "step": 6466,
      "training_loss": 6.844369411468506
    },
    {
      "epoch": 0.3505149051490515,
      "step": 6467,
      "training_loss": 7.53814697265625
    },
    {
      "epoch": 0.3505691056910569,
      "grad_norm": 28.694955825805664,
      "learning_rate": 1e-05,
      "loss": 6.8468,
      "step": 6468
    },
    {
      "epoch": 0.3505691056910569,
      "step": 6468,
      "training_loss": 7.22803258895874
    },
    {
      "epoch": 0.3506233062330623,
      "step": 6469,
      "training_loss": 6.723931312561035
    },
    {
      "epoch": 0.35067750677506776,
      "step": 6470,
      "training_loss": 6.840158939361572
    },
    {
      "epoch": 0.35073170731707315,
      "step": 6471,
      "training_loss": 6.889123439788818
    },
    {
      "epoch": 0.3507859078590786,
      "grad_norm": 19.702560424804688,
      "learning_rate": 1e-05,
      "loss": 6.9203,
      "step": 6472
    },
    {
      "epoch": 0.3507859078590786,
      "step": 6472,
      "training_loss": 6.7496747970581055
    },
    {
      "epoch": 0.350840108401084,
      "step": 6473,
      "training_loss": 6.82058048248291
    },
    {
      "epoch": 0.35089430894308943,
      "step": 6474,
      "training_loss": 6.2145209312438965
    },
    {
      "epoch": 0.3509485094850949,
      "step": 6475,
      "training_loss": 7.215917587280273
    },
    {
      "epoch": 0.35100271002710026,
      "grad_norm": 21.98192024230957,
      "learning_rate": 1e-05,
      "loss": 6.7502,
      "step": 6476
    },
    {
      "epoch": 0.35100271002710026,
      "step": 6476,
      "training_loss": 6.793153762817383
    },
    {
      "epoch": 0.3510569105691057,
      "step": 6477,
      "training_loss": 5.49573278427124
    },
    {
      "epoch": 0.3511111111111111,
      "step": 6478,
      "training_loss": 6.498250961303711
    },
    {
      "epoch": 0.35116531165311654,
      "step": 6479,
      "training_loss": 5.6609907150268555
    },
    {
      "epoch": 0.35121951219512193,
      "grad_norm": 26.256183624267578,
      "learning_rate": 1e-05,
      "loss": 6.112,
      "step": 6480
    },
    {
      "epoch": 0.35121951219512193,
      "step": 6480,
      "training_loss": 7.18055534362793
    },
    {
      "epoch": 0.3512737127371274,
      "step": 6481,
      "training_loss": 6.98810338973999
    },
    {
      "epoch": 0.35132791327913276,
      "step": 6482,
      "training_loss": 6.633907318115234
    },
    {
      "epoch": 0.3513821138211382,
      "step": 6483,
      "training_loss": 7.726110935211182
    },
    {
      "epoch": 0.35143631436314365,
      "grad_norm": 31.432846069335938,
      "learning_rate": 1e-05,
      "loss": 7.1322,
      "step": 6484
    },
    {
      "epoch": 0.35143631436314365,
      "step": 6484,
      "training_loss": 6.207346439361572
    },
    {
      "epoch": 0.35149051490514904,
      "step": 6485,
      "training_loss": 5.941495418548584
    },
    {
      "epoch": 0.3515447154471545,
      "step": 6486,
      "training_loss": 6.837113857269287
    },
    {
      "epoch": 0.3515989159891599,
      "step": 6487,
      "training_loss": 6.8106184005737305
    },
    {
      "epoch": 0.3516531165311653,
      "grad_norm": 21.931997299194336,
      "learning_rate": 1e-05,
      "loss": 6.4491,
      "step": 6488
    },
    {
      "epoch": 0.3516531165311653,
      "step": 6488,
      "training_loss": 7.900659084320068
    },
    {
      "epoch": 0.3517073170731707,
      "step": 6489,
      "training_loss": 7.028137683868408
    },
    {
      "epoch": 0.35176151761517616,
      "step": 6490,
      "training_loss": 6.054415702819824
    },
    {
      "epoch": 0.35181571815718155,
      "step": 6491,
      "training_loss": 6.251192569732666
    },
    {
      "epoch": 0.351869918699187,
      "grad_norm": 35.419822692871094,
      "learning_rate": 1e-05,
      "loss": 6.8086,
      "step": 6492
    },
    {
      "epoch": 0.351869918699187,
      "step": 6492,
      "training_loss": 8.992440223693848
    },
    {
      "epoch": 0.35192411924119243,
      "step": 6493,
      "training_loss": 7.572983264923096
    },
    {
      "epoch": 0.3519783197831978,
      "step": 6494,
      "training_loss": 6.121156692504883
    },
    {
      "epoch": 0.35203252032520327,
      "step": 6495,
      "training_loss": 6.091341972351074
    },
    {
      "epoch": 0.35208672086720866,
      "grad_norm": 23.047616958618164,
      "learning_rate": 1e-05,
      "loss": 7.1945,
      "step": 6496
    },
    {
      "epoch": 0.35208672086720866,
      "step": 6496,
      "training_loss": 6.352682590484619
    },
    {
      "epoch": 0.3521409214092141,
      "step": 6497,
      "training_loss": 8.131243705749512
    },
    {
      "epoch": 0.3521951219512195,
      "step": 6498,
      "training_loss": 7.261756420135498
    },
    {
      "epoch": 0.35224932249322494,
      "step": 6499,
      "training_loss": 5.151254653930664
    },
    {
      "epoch": 0.3523035230352303,
      "grad_norm": 15.848348617553711,
      "learning_rate": 1e-05,
      "loss": 6.7242,
      "step": 6500
    },
    {
      "epoch": 0.3523035230352303,
      "step": 6500,
      "training_loss": 6.934019565582275
    },
    {
      "epoch": 0.35235772357723577,
      "step": 6501,
      "training_loss": 6.709251880645752
    },
    {
      "epoch": 0.3524119241192412,
      "step": 6502,
      "training_loss": 6.347320556640625
    },
    {
      "epoch": 0.3524661246612466,
      "step": 6503,
      "training_loss": 6.379341125488281
    },
    {
      "epoch": 0.35252032520325205,
      "grad_norm": 19.88292121887207,
      "learning_rate": 1e-05,
      "loss": 6.5925,
      "step": 6504
    },
    {
      "epoch": 0.35252032520325205,
      "step": 6504,
      "training_loss": 6.851846694946289
    },
    {
      "epoch": 0.35257452574525744,
      "step": 6505,
      "training_loss": 5.624385356903076
    },
    {
      "epoch": 0.3526287262872629,
      "step": 6506,
      "training_loss": 7.497167110443115
    },
    {
      "epoch": 0.3526829268292683,
      "step": 6507,
      "training_loss": 8.078025817871094
    },
    {
      "epoch": 0.3527371273712737,
      "grad_norm": 22.41208267211914,
      "learning_rate": 1e-05,
      "loss": 7.0129,
      "step": 6508
    },
    {
      "epoch": 0.3527371273712737,
      "step": 6508,
      "training_loss": 5.640442848205566
    },
    {
      "epoch": 0.3527913279132791,
      "step": 6509,
      "training_loss": 6.012951374053955
    },
    {
      "epoch": 0.35284552845528455,
      "step": 6510,
      "training_loss": 6.702563285827637
    },
    {
      "epoch": 0.35289972899729,
      "step": 6511,
      "training_loss": 4.389449119567871
    },
    {
      "epoch": 0.3529539295392954,
      "grad_norm": 25.36896514892578,
      "learning_rate": 1e-05,
      "loss": 5.6864,
      "step": 6512
    },
    {
      "epoch": 0.3529539295392954,
      "step": 6512,
      "training_loss": 5.750751495361328
    },
    {
      "epoch": 0.35300813008130083,
      "step": 6513,
      "training_loss": 7.013433456420898
    },
    {
      "epoch": 0.3530623306233062,
      "step": 6514,
      "training_loss": 6.85248327255249
    },
    {
      "epoch": 0.35311653116531166,
      "step": 6515,
      "training_loss": 6.790577411651611
    },
    {
      "epoch": 0.35317073170731705,
      "grad_norm": 19.346010208129883,
      "learning_rate": 1e-05,
      "loss": 6.6018,
      "step": 6516
    },
    {
      "epoch": 0.35317073170731705,
      "step": 6516,
      "training_loss": 6.430017948150635
    },
    {
      "epoch": 0.3532249322493225,
      "step": 6517,
      "training_loss": 7.1868486404418945
    },
    {
      "epoch": 0.3532791327913279,
      "step": 6518,
      "training_loss": 6.961609363555908
    },
    {
      "epoch": 0.35333333333333333,
      "step": 6519,
      "training_loss": 6.645561695098877
    },
    {
      "epoch": 0.3533875338753388,
      "grad_norm": 46.601112365722656,
      "learning_rate": 1e-05,
      "loss": 6.806,
      "step": 6520
    },
    {
      "epoch": 0.3533875338753388,
      "step": 6520,
      "training_loss": 7.943562984466553
    },
    {
      "epoch": 0.35344173441734417,
      "step": 6521,
      "training_loss": 7.042295455932617
    },
    {
      "epoch": 0.3534959349593496,
      "step": 6522,
      "training_loss": 7.252364158630371
    },
    {
      "epoch": 0.353550135501355,
      "step": 6523,
      "training_loss": 6.625558853149414
    },
    {
      "epoch": 0.35360433604336045,
      "grad_norm": 28.456083297729492,
      "learning_rate": 1e-05,
      "loss": 7.2159,
      "step": 6524
    },
    {
      "epoch": 0.35360433604336045,
      "step": 6524,
      "training_loss": 6.941740036010742
    },
    {
      "epoch": 0.35365853658536583,
      "step": 6525,
      "training_loss": 6.7842230796813965
    },
    {
      "epoch": 0.3537127371273713,
      "step": 6526,
      "training_loss": 7.718615531921387
    },
    {
      "epoch": 0.35376693766937667,
      "step": 6527,
      "training_loss": 6.078638076782227
    },
    {
      "epoch": 0.3538211382113821,
      "grad_norm": 26.433977127075195,
      "learning_rate": 1e-05,
      "loss": 6.8808,
      "step": 6528
    },
    {
      "epoch": 0.3538211382113821,
      "step": 6528,
      "training_loss": 5.8262457847595215
    },
    {
      "epoch": 0.35387533875338756,
      "step": 6529,
      "training_loss": 6.717292785644531
    },
    {
      "epoch": 0.35392953929539295,
      "step": 6530,
      "training_loss": 8.42414379119873
    },
    {
      "epoch": 0.3539837398373984,
      "step": 6531,
      "training_loss": 6.256785869598389
    },
    {
      "epoch": 0.3540379403794038,
      "grad_norm": 25.16558265686035,
      "learning_rate": 1e-05,
      "loss": 6.8061,
      "step": 6532
    },
    {
      "epoch": 0.3540379403794038,
      "step": 6532,
      "training_loss": 6.753457546234131
    },
    {
      "epoch": 0.3540921409214092,
      "step": 6533,
      "training_loss": 6.796695232391357
    },
    {
      "epoch": 0.3541463414634146,
      "step": 6534,
      "training_loss": 6.236731052398682
    },
    {
      "epoch": 0.35420054200542006,
      "step": 6535,
      "training_loss": 7.136058330535889
    },
    {
      "epoch": 0.35425474254742545,
      "grad_norm": 22.554927825927734,
      "learning_rate": 1e-05,
      "loss": 6.7307,
      "step": 6536
    },
    {
      "epoch": 0.35425474254742545,
      "step": 6536,
      "training_loss": 6.645693302154541
    },
    {
      "epoch": 0.3543089430894309,
      "step": 6537,
      "training_loss": 8.671792030334473
    },
    {
      "epoch": 0.35436314363143634,
      "step": 6538,
      "training_loss": 7.5245232582092285
    },
    {
      "epoch": 0.35441734417344173,
      "step": 6539,
      "training_loss": 6.451324462890625
    },
    {
      "epoch": 0.3544715447154472,
      "grad_norm": 33.81816864013672,
      "learning_rate": 1e-05,
      "loss": 7.3233,
      "step": 6540
    },
    {
      "epoch": 0.3544715447154472,
      "step": 6540,
      "training_loss": 6.783705234527588
    },
    {
      "epoch": 0.35452574525745256,
      "step": 6541,
      "training_loss": 7.050596714019775
    },
    {
      "epoch": 0.354579945799458,
      "step": 6542,
      "training_loss": 5.492567539215088
    },
    {
      "epoch": 0.3546341463414634,
      "step": 6543,
      "training_loss": 6.251776218414307
    },
    {
      "epoch": 0.35468834688346884,
      "grad_norm": 40.04548645019531,
      "learning_rate": 1e-05,
      "loss": 6.3947,
      "step": 6544
    },
    {
      "epoch": 0.35468834688346884,
      "step": 6544,
      "training_loss": 7.276416778564453
    },
    {
      "epoch": 0.35474254742547423,
      "step": 6545,
      "training_loss": 6.972763538360596
    },
    {
      "epoch": 0.3547967479674797,
      "step": 6546,
      "training_loss": 7.257777214050293
    },
    {
      "epoch": 0.3548509485094851,
      "step": 6547,
      "training_loss": 7.038117408752441
    },
    {
      "epoch": 0.3549051490514905,
      "grad_norm": 26.332014083862305,
      "learning_rate": 1e-05,
      "loss": 7.1363,
      "step": 6548
    },
    {
      "epoch": 0.3549051490514905,
      "step": 6548,
      "training_loss": 6.012943744659424
    },
    {
      "epoch": 0.35495934959349595,
      "step": 6549,
      "training_loss": 6.8040032386779785
    },
    {
      "epoch": 0.35501355013550134,
      "step": 6550,
      "training_loss": 6.11003303527832
    },
    {
      "epoch": 0.3550677506775068,
      "step": 6551,
      "training_loss": 7.689798355102539
    },
    {
      "epoch": 0.3551219512195122,
      "grad_norm": 18.958255767822266,
      "learning_rate": 1e-05,
      "loss": 6.6542,
      "step": 6552
    },
    {
      "epoch": 0.3551219512195122,
      "step": 6552,
      "training_loss": 6.791553497314453
    },
    {
      "epoch": 0.3551761517615176,
      "step": 6553,
      "training_loss": 7.195243835449219
    },
    {
      "epoch": 0.355230352303523,
      "step": 6554,
      "training_loss": 7.433091163635254
    },
    {
      "epoch": 0.35528455284552846,
      "step": 6555,
      "training_loss": 6.381130695343018
    },
    {
      "epoch": 0.3553387533875339,
      "grad_norm": 27.3834285736084,
      "learning_rate": 1e-05,
      "loss": 6.9503,
      "step": 6556
    },
    {
      "epoch": 0.3553387533875339,
      "step": 6556,
      "training_loss": 6.04202938079834
    },
    {
      "epoch": 0.3553929539295393,
      "step": 6557,
      "training_loss": 5.835136413574219
    },
    {
      "epoch": 0.35544715447154474,
      "step": 6558,
      "training_loss": 6.483502388000488
    },
    {
      "epoch": 0.3555013550135501,
      "step": 6559,
      "training_loss": 6.330499172210693
    },
    {
      "epoch": 0.35555555555555557,
      "grad_norm": 17.806198120117188,
      "learning_rate": 1e-05,
      "loss": 6.1728,
      "step": 6560
    },
    {
      "epoch": 0.35555555555555557,
      "step": 6560,
      "training_loss": 6.41159200668335
    },
    {
      "epoch": 0.35560975609756096,
      "step": 6561,
      "training_loss": 6.28641939163208
    },
    {
      "epoch": 0.3556639566395664,
      "step": 6562,
      "training_loss": 5.946593284606934
    },
    {
      "epoch": 0.3557181571815718,
      "step": 6563,
      "training_loss": 6.830678462982178
    },
    {
      "epoch": 0.35577235772357724,
      "grad_norm": 39.57390213012695,
      "learning_rate": 1e-05,
      "loss": 6.3688,
      "step": 6564
    },
    {
      "epoch": 0.35577235772357724,
      "step": 6564,
      "training_loss": 6.561222076416016
    },
    {
      "epoch": 0.3558265582655827,
      "step": 6565,
      "training_loss": 6.943983554840088
    },
    {
      "epoch": 0.35588075880758807,
      "step": 6566,
      "training_loss": 8.732025146484375
    },
    {
      "epoch": 0.3559349593495935,
      "step": 6567,
      "training_loss": 4.949669361114502
    },
    {
      "epoch": 0.3559891598915989,
      "grad_norm": 22.656999588012695,
      "learning_rate": 1e-05,
      "loss": 6.7967,
      "step": 6568
    },
    {
      "epoch": 0.3559891598915989,
      "step": 6568,
      "training_loss": 5.791256904602051
    },
    {
      "epoch": 0.35604336043360435,
      "step": 6569,
      "training_loss": 6.283870697021484
    },
    {
      "epoch": 0.35609756097560974,
      "step": 6570,
      "training_loss": 6.374179363250732
    },
    {
      "epoch": 0.3561517615176152,
      "step": 6571,
      "training_loss": 8.693872451782227
    },
    {
      "epoch": 0.3562059620596206,
      "grad_norm": 84.82489776611328,
      "learning_rate": 1e-05,
      "loss": 6.7858,
      "step": 6572
    },
    {
      "epoch": 0.3562059620596206,
      "step": 6572,
      "training_loss": 7.1978302001953125
    },
    {
      "epoch": 0.356260162601626,
      "step": 6573,
      "training_loss": 7.4014410972595215
    },
    {
      "epoch": 0.35631436314363146,
      "step": 6574,
      "training_loss": 7.026317596435547
    },
    {
      "epoch": 0.35636856368563685,
      "step": 6575,
      "training_loss": 7.362362384796143
    },
    {
      "epoch": 0.3564227642276423,
      "grad_norm": 27.864118576049805,
      "learning_rate": 1e-05,
      "loss": 7.247,
      "step": 6576
    },
    {
      "epoch": 0.3564227642276423,
      "step": 6576,
      "training_loss": 8.226922035217285
    },
    {
      "epoch": 0.3564769647696477,
      "step": 6577,
      "training_loss": 6.880699634552002
    },
    {
      "epoch": 0.35653116531165313,
      "step": 6578,
      "training_loss": 7.206475734710693
    },
    {
      "epoch": 0.3565853658536585,
      "step": 6579,
      "training_loss": 4.034158706665039
    },
    {
      "epoch": 0.35663956639566397,
      "grad_norm": 31.604604721069336,
      "learning_rate": 1e-05,
      "loss": 6.5871,
      "step": 6580
    },
    {
      "epoch": 0.35663956639566397,
      "step": 6580,
      "training_loss": 6.650999069213867
    },
    {
      "epoch": 0.35669376693766935,
      "step": 6581,
      "training_loss": 6.727376461029053
    },
    {
      "epoch": 0.3567479674796748,
      "step": 6582,
      "training_loss": 7.5861968994140625
    },
    {
      "epoch": 0.35680216802168024,
      "step": 6583,
      "training_loss": 5.644196510314941
    },
    {
      "epoch": 0.35685636856368563,
      "grad_norm": 29.147356033325195,
      "learning_rate": 1e-05,
      "loss": 6.6522,
      "step": 6584
    },
    {
      "epoch": 0.35685636856368563,
      "step": 6584,
      "training_loss": 6.707025527954102
    },
    {
      "epoch": 0.3569105691056911,
      "step": 6585,
      "training_loss": 5.895820140838623
    },
    {
      "epoch": 0.35696476964769647,
      "step": 6586,
      "training_loss": 6.917744159698486
    },
    {
      "epoch": 0.3570189701897019,
      "step": 6587,
      "training_loss": 6.311798572540283
    },
    {
      "epoch": 0.3570731707317073,
      "grad_norm": 24.130203247070312,
      "learning_rate": 1e-05,
      "loss": 6.4581,
      "step": 6588
    },
    {
      "epoch": 0.3570731707317073,
      "step": 6588,
      "training_loss": 6.603028774261475
    },
    {
      "epoch": 0.35712737127371275,
      "step": 6589,
      "training_loss": 7.141239643096924
    },
    {
      "epoch": 0.35718157181571814,
      "step": 6590,
      "training_loss": 6.748022079467773
    },
    {
      "epoch": 0.3572357723577236,
      "step": 6591,
      "training_loss": 7.846853256225586
    },
    {
      "epoch": 0.357289972899729,
      "grad_norm": 35.37565994262695,
      "learning_rate": 1e-05,
      "loss": 7.0848,
      "step": 6592
    },
    {
      "epoch": 0.357289972899729,
      "step": 6592,
      "training_loss": 3.6515891551971436
    },
    {
      "epoch": 0.3573441734417344,
      "step": 6593,
      "training_loss": 5.9548234939575195
    },
    {
      "epoch": 0.35739837398373986,
      "step": 6594,
      "training_loss": 6.32733154296875
    },
    {
      "epoch": 0.35745257452574525,
      "step": 6595,
      "training_loss": 7.395484447479248
    },
    {
      "epoch": 0.3575067750677507,
      "grad_norm": 22.880794525146484,
      "learning_rate": 1e-05,
      "loss": 5.8323,
      "step": 6596
    },
    {
      "epoch": 0.3575067750677507,
      "step": 6596,
      "training_loss": 5.944004058837891
    },
    {
      "epoch": 0.3575609756097561,
      "step": 6597,
      "training_loss": 6.788647651672363
    },
    {
      "epoch": 0.3576151761517615,
      "step": 6598,
      "training_loss": 7.4386305809021
    },
    {
      "epoch": 0.3576693766937669,
      "step": 6599,
      "training_loss": 7.686117172241211
    },
    {
      "epoch": 0.35772357723577236,
      "grad_norm": 32.799400329589844,
      "learning_rate": 1e-05,
      "loss": 6.9643,
      "step": 6600
    },
    {
      "epoch": 0.35772357723577236,
      "step": 6600,
      "training_loss": 7.800227642059326
    },
    {
      "epoch": 0.35777777777777775,
      "step": 6601,
      "training_loss": 7.147615432739258
    },
    {
      "epoch": 0.3578319783197832,
      "step": 6602,
      "training_loss": 8.404892921447754
    },
    {
      "epoch": 0.35788617886178864,
      "step": 6603,
      "training_loss": 6.739175796508789
    },
    {
      "epoch": 0.35794037940379403,
      "grad_norm": 17.39862060546875,
      "learning_rate": 1e-05,
      "loss": 7.523,
      "step": 6604
    },
    {
      "epoch": 0.35794037940379403,
      "step": 6604,
      "training_loss": 6.225778579711914
    },
    {
      "epoch": 0.3579945799457995,
      "step": 6605,
      "training_loss": 7.851912498474121
    },
    {
      "epoch": 0.35804878048780486,
      "step": 6606,
      "training_loss": 6.117790699005127
    },
    {
      "epoch": 0.3581029810298103,
      "step": 6607,
      "training_loss": 6.37053918838501
    },
    {
      "epoch": 0.3581571815718157,
      "grad_norm": 27.76675796508789,
      "learning_rate": 1e-05,
      "loss": 6.6415,
      "step": 6608
    },
    {
      "epoch": 0.3581571815718157,
      "step": 6608,
      "training_loss": 7.1695404052734375
    },
    {
      "epoch": 0.35821138211382114,
      "step": 6609,
      "training_loss": 8.025186538696289
    },
    {
      "epoch": 0.35826558265582653,
      "step": 6610,
      "training_loss": 6.044534683227539
    },
    {
      "epoch": 0.358319783197832,
      "step": 6611,
      "training_loss": 7.138655185699463
    },
    {
      "epoch": 0.3583739837398374,
      "grad_norm": 23.687734603881836,
      "learning_rate": 1e-05,
      "loss": 7.0945,
      "step": 6612
    },
    {
      "epoch": 0.3583739837398374,
      "step": 6612,
      "training_loss": 7.242292881011963
    },
    {
      "epoch": 0.3584281842818428,
      "step": 6613,
      "training_loss": 6.584967613220215
    },
    {
      "epoch": 0.35848238482384825,
      "step": 6614,
      "training_loss": 3.986062526702881
    },
    {
      "epoch": 0.35853658536585364,
      "step": 6615,
      "training_loss": 6.82820463180542
    },
    {
      "epoch": 0.3585907859078591,
      "grad_norm": 21.697168350219727,
      "learning_rate": 1e-05,
      "loss": 6.1604,
      "step": 6616
    },
    {
      "epoch": 0.3585907859078591,
      "step": 6616,
      "training_loss": 6.476848125457764
    },
    {
      "epoch": 0.3586449864498645,
      "step": 6617,
      "training_loss": 7.731224060058594
    },
    {
      "epoch": 0.3586991869918699,
      "step": 6618,
      "training_loss": 7.288486957550049
    },
    {
      "epoch": 0.3587533875338753,
      "step": 6619,
      "training_loss": 6.764151096343994
    },
    {
      "epoch": 0.35880758807588076,
      "grad_norm": 19.322246551513672,
      "learning_rate": 1e-05,
      "loss": 7.0652,
      "step": 6620
    },
    {
      "epoch": 0.35880758807588076,
      "step": 6620,
      "training_loss": 6.795987606048584
    },
    {
      "epoch": 0.3588617886178862,
      "step": 6621,
      "training_loss": 7.9339823722839355
    },
    {
      "epoch": 0.3589159891598916,
      "step": 6622,
      "training_loss": 6.560136795043945
    },
    {
      "epoch": 0.35897018970189704,
      "step": 6623,
      "training_loss": 6.005332946777344
    },
    {
      "epoch": 0.3590243902439024,
      "grad_norm": 51.278133392333984,
      "learning_rate": 1e-05,
      "loss": 6.8239,
      "step": 6624
    },
    {
      "epoch": 0.3590243902439024,
      "step": 6624,
      "training_loss": 5.838364124298096
    },
    {
      "epoch": 0.35907859078590787,
      "step": 6625,
      "training_loss": 6.7544755935668945
    },
    {
      "epoch": 0.35913279132791326,
      "step": 6626,
      "training_loss": 7.977322101593018
    },
    {
      "epoch": 0.3591869918699187,
      "step": 6627,
      "training_loss": 7.809516429901123
    },
    {
      "epoch": 0.3592411924119241,
      "grad_norm": 29.406211853027344,
      "learning_rate": 1e-05,
      "loss": 7.0949,
      "step": 6628
    },
    {
      "epoch": 0.3592411924119241,
      "step": 6628,
      "training_loss": 6.667437553405762
    },
    {
      "epoch": 0.35929539295392954,
      "step": 6629,
      "training_loss": 6.6871724128723145
    },
    {
      "epoch": 0.359349593495935,
      "step": 6630,
      "training_loss": 6.931609153747559
    },
    {
      "epoch": 0.35940379403794037,
      "step": 6631,
      "training_loss": 6.593209266662598
    },
    {
      "epoch": 0.3594579945799458,
      "grad_norm": 35.384315490722656,
      "learning_rate": 1e-05,
      "loss": 6.7199,
      "step": 6632
    },
    {
      "epoch": 0.3594579945799458,
      "step": 6632,
      "training_loss": 7.370378494262695
    },
    {
      "epoch": 0.3595121951219512,
      "step": 6633,
      "training_loss": 4.936680316925049
    },
    {
      "epoch": 0.35956639566395665,
      "step": 6634,
      "training_loss": 4.471035480499268
    },
    {
      "epoch": 0.35962059620596204,
      "step": 6635,
      "training_loss": 7.070708274841309
    },
    {
      "epoch": 0.3596747967479675,
      "grad_norm": 23.633358001708984,
      "learning_rate": 1e-05,
      "loss": 5.9622,
      "step": 6636
    },
    {
      "epoch": 0.3596747967479675,
      "step": 6636,
      "training_loss": 7.493342399597168
    },
    {
      "epoch": 0.3597289972899729,
      "step": 6637,
      "training_loss": 7.881983757019043
    },
    {
      "epoch": 0.3597831978319783,
      "step": 6638,
      "training_loss": 7.717954158782959
    },
    {
      "epoch": 0.35983739837398376,
      "step": 6639,
      "training_loss": 6.855457782745361
    },
    {
      "epoch": 0.35989159891598915,
      "grad_norm": 23.71262550354004,
      "learning_rate": 1e-05,
      "loss": 7.4872,
      "step": 6640
    },
    {
      "epoch": 0.35989159891598915,
      "step": 6640,
      "training_loss": 9.158839225769043
    },
    {
      "epoch": 0.3599457994579946,
      "step": 6641,
      "training_loss": 7.485986709594727
    },
    {
      "epoch": 0.36,
      "step": 6642,
      "training_loss": 7.120582103729248
    },
    {
      "epoch": 0.36005420054200543,
      "step": 6643,
      "training_loss": 7.058218955993652
    },
    {
      "epoch": 0.3601084010840108,
      "grad_norm": 24.864469528198242,
      "learning_rate": 1e-05,
      "loss": 7.7059,
      "step": 6644
    },
    {
      "epoch": 0.3601084010840108,
      "step": 6644,
      "training_loss": 6.97179651260376
    },
    {
      "epoch": 0.36016260162601627,
      "step": 6645,
      "training_loss": 7.249072074890137
    },
    {
      "epoch": 0.36021680216802165,
      "step": 6646,
      "training_loss": 7.753809928894043
    },
    {
      "epoch": 0.3602710027100271,
      "step": 6647,
      "training_loss": 7.822756290435791
    },
    {
      "epoch": 0.36032520325203254,
      "grad_norm": 35.412174224853516,
      "learning_rate": 1e-05,
      "loss": 7.4494,
      "step": 6648
    },
    {
      "epoch": 0.36032520325203254,
      "step": 6648,
      "training_loss": 7.037697792053223
    },
    {
      "epoch": 0.36037940379403793,
      "step": 6649,
      "training_loss": 7.180531978607178
    },
    {
      "epoch": 0.3604336043360434,
      "step": 6650,
      "training_loss": 6.320361137390137
    },
    {
      "epoch": 0.36048780487804877,
      "step": 6651,
      "training_loss": 5.740286827087402
    },
    {
      "epoch": 0.3605420054200542,
      "grad_norm": 23.632341384887695,
      "learning_rate": 1e-05,
      "loss": 6.5697,
      "step": 6652
    },
    {
      "epoch": 0.3605420054200542,
      "step": 6652,
      "training_loss": 7.08156156539917
    },
    {
      "epoch": 0.3605962059620596,
      "step": 6653,
      "training_loss": 6.70101261138916
    },
    {
      "epoch": 0.36065040650406505,
      "step": 6654,
      "training_loss": 7.9522175788879395
    },
    {
      "epoch": 0.36070460704607044,
      "step": 6655,
      "training_loss": 4.22745418548584
    },
    {
      "epoch": 0.3607588075880759,
      "grad_norm": 27.385648727416992,
      "learning_rate": 1e-05,
      "loss": 6.4906,
      "step": 6656
    },
    {
      "epoch": 0.3607588075880759,
      "step": 6656,
      "training_loss": 7.737420558929443
    },
    {
      "epoch": 0.3608130081300813,
      "step": 6657,
      "training_loss": 6.960633277893066
    },
    {
      "epoch": 0.3608672086720867,
      "step": 6658,
      "training_loss": 7.778233528137207
    },
    {
      "epoch": 0.36092140921409216,
      "step": 6659,
      "training_loss": 5.794010162353516
    },
    {
      "epoch": 0.36097560975609755,
      "grad_norm": 20.79888153076172,
      "learning_rate": 1e-05,
      "loss": 7.0676,
      "step": 6660
    },
    {
      "epoch": 0.36097560975609755,
      "step": 6660,
      "training_loss": 8.366907119750977
    },
    {
      "epoch": 0.361029810298103,
      "step": 6661,
      "training_loss": 7.61062479019165
    },
    {
      "epoch": 0.3610840108401084,
      "step": 6662,
      "training_loss": 6.52549934387207
    },
    {
      "epoch": 0.3611382113821138,
      "step": 6663,
      "training_loss": 7.001402378082275
    },
    {
      "epoch": 0.3611924119241192,
      "grad_norm": 17.053295135498047,
      "learning_rate": 1e-05,
      "loss": 7.3761,
      "step": 6664
    },
    {
      "epoch": 0.3611924119241192,
      "step": 6664,
      "training_loss": 8.22494125366211
    },
    {
      "epoch": 0.36124661246612466,
      "step": 6665,
      "training_loss": 5.110729217529297
    },
    {
      "epoch": 0.3613008130081301,
      "step": 6666,
      "training_loss": 5.9787445068359375
    },
    {
      "epoch": 0.3613550135501355,
      "step": 6667,
      "training_loss": 7.021988868713379
    },
    {
      "epoch": 0.36140921409214094,
      "grad_norm": 15.664604187011719,
      "learning_rate": 1e-05,
      "loss": 6.5841,
      "step": 6668
    },
    {
      "epoch": 0.36140921409214094,
      "step": 6668,
      "training_loss": 7.798600673675537
    },
    {
      "epoch": 0.36146341463414633,
      "step": 6669,
      "training_loss": 7.749302387237549
    },
    {
      "epoch": 0.3615176151761518,
      "step": 6670,
      "training_loss": 7.375100135803223
    },
    {
      "epoch": 0.36157181571815716,
      "step": 6671,
      "training_loss": 6.219274520874023
    },
    {
      "epoch": 0.3616260162601626,
      "grad_norm": 23.646451950073242,
      "learning_rate": 1e-05,
      "loss": 7.2856,
      "step": 6672
    },
    {
      "epoch": 0.3616260162601626,
      "step": 6672,
      "training_loss": 7.066266059875488
    },
    {
      "epoch": 0.361680216802168,
      "step": 6673,
      "training_loss": 7.089017391204834
    },
    {
      "epoch": 0.36173441734417344,
      "step": 6674,
      "training_loss": 6.417721748352051
    },
    {
      "epoch": 0.3617886178861789,
      "step": 6675,
      "training_loss": 6.931331634521484
    },
    {
      "epoch": 0.3618428184281843,
      "grad_norm": 25.17891502380371,
      "learning_rate": 1e-05,
      "loss": 6.8761,
      "step": 6676
    },
    {
      "epoch": 0.3618428184281843,
      "step": 6676,
      "training_loss": 6.903581142425537
    },
    {
      "epoch": 0.3618970189701897,
      "step": 6677,
      "training_loss": 5.983407497406006
    },
    {
      "epoch": 0.3619512195121951,
      "step": 6678,
      "training_loss": 7.686522006988525
    },
    {
      "epoch": 0.36200542005420056,
      "step": 6679,
      "training_loss": 7.130943775177002
    },
    {
      "epoch": 0.36205962059620594,
      "grad_norm": 27.061866760253906,
      "learning_rate": 1e-05,
      "loss": 6.9261,
      "step": 6680
    },
    {
      "epoch": 0.36205962059620594,
      "step": 6680,
      "training_loss": 6.149702072143555
    },
    {
      "epoch": 0.3621138211382114,
      "step": 6681,
      "training_loss": 6.88123893737793
    },
    {
      "epoch": 0.3621680216802168,
      "step": 6682,
      "training_loss": 5.089265823364258
    },
    {
      "epoch": 0.3622222222222222,
      "step": 6683,
      "training_loss": 7.3995041847229
    },
    {
      "epoch": 0.36227642276422767,
      "grad_norm": 19.45016098022461,
      "learning_rate": 1e-05,
      "loss": 6.3799,
      "step": 6684
    },
    {
      "epoch": 0.36227642276422767,
      "step": 6684,
      "training_loss": 7.041067600250244
    },
    {
      "epoch": 0.36233062330623306,
      "step": 6685,
      "training_loss": 4.74206018447876
    },
    {
      "epoch": 0.3623848238482385,
      "step": 6686,
      "training_loss": 7.135110378265381
    },
    {
      "epoch": 0.3624390243902439,
      "step": 6687,
      "training_loss": 7.174950122833252
    },
    {
      "epoch": 0.36249322493224934,
      "grad_norm": 34.810611724853516,
      "learning_rate": 1e-05,
      "loss": 6.5233,
      "step": 6688
    },
    {
      "epoch": 0.36249322493224934,
      "step": 6688,
      "training_loss": 6.597105503082275
    },
    {
      "epoch": 0.3625474254742547,
      "step": 6689,
      "training_loss": 6.432358264923096
    },
    {
      "epoch": 0.36260162601626017,
      "step": 6690,
      "training_loss": 5.8985676765441895
    },
    {
      "epoch": 0.36265582655826556,
      "step": 6691,
      "training_loss": 6.018073081970215
    },
    {
      "epoch": 0.362710027100271,
      "grad_norm": 22.57537078857422,
      "learning_rate": 1e-05,
      "loss": 6.2365,
      "step": 6692
    },
    {
      "epoch": 0.362710027100271,
      "step": 6692,
      "training_loss": 7.021673202514648
    },
    {
      "epoch": 0.36276422764227645,
      "step": 6693,
      "training_loss": 6.465336322784424
    },
    {
      "epoch": 0.36281842818428184,
      "step": 6694,
      "training_loss": 7.193671703338623
    },
    {
      "epoch": 0.3628726287262873,
      "step": 6695,
      "training_loss": 6.585031509399414
    },
    {
      "epoch": 0.36292682926829267,
      "grad_norm": 25.06048583984375,
      "learning_rate": 1e-05,
      "loss": 6.8164,
      "step": 6696
    },
    {
      "epoch": 0.36292682926829267,
      "step": 6696,
      "training_loss": 5.111147403717041
    },
    {
      "epoch": 0.3629810298102981,
      "step": 6697,
      "training_loss": 5.392783164978027
    },
    {
      "epoch": 0.3630352303523035,
      "step": 6698,
      "training_loss": 3.973543882369995
    },
    {
      "epoch": 0.36308943089430895,
      "step": 6699,
      "training_loss": 7.306787967681885
    },
    {
      "epoch": 0.36314363143631434,
      "grad_norm": 21.112424850463867,
      "learning_rate": 1e-05,
      "loss": 5.4461,
      "step": 6700
    },
    {
      "epoch": 0.36314363143631434,
      "step": 6700,
      "training_loss": 6.74818229675293
    },
    {
      "epoch": 0.3631978319783198,
      "step": 6701,
      "training_loss": 5.934051036834717
    },
    {
      "epoch": 0.36325203252032523,
      "step": 6702,
      "training_loss": 6.7948198318481445
    },
    {
      "epoch": 0.3633062330623306,
      "step": 6703,
      "training_loss": 7.35202693939209
    },
    {
      "epoch": 0.36336043360433606,
      "grad_norm": 22.114002227783203,
      "learning_rate": 1e-05,
      "loss": 6.7073,
      "step": 6704
    },
    {
      "epoch": 0.36336043360433606,
      "step": 6704,
      "training_loss": 4.292536735534668
    },
    {
      "epoch": 0.36341463414634145,
      "step": 6705,
      "training_loss": 9.15273380279541
    },
    {
      "epoch": 0.3634688346883469,
      "step": 6706,
      "training_loss": 5.590038299560547
    },
    {
      "epoch": 0.3635230352303523,
      "step": 6707,
      "training_loss": 6.882701396942139
    },
    {
      "epoch": 0.36357723577235773,
      "grad_norm": 33.0245361328125,
      "learning_rate": 1e-05,
      "loss": 6.4795,
      "step": 6708
    },
    {
      "epoch": 0.36357723577235773,
      "step": 6708,
      "training_loss": 6.53597354888916
    },
    {
      "epoch": 0.3636314363143631,
      "step": 6709,
      "training_loss": 4.852235794067383
    },
    {
      "epoch": 0.36368563685636857,
      "step": 6710,
      "training_loss": 6.206319332122803
    },
    {
      "epoch": 0.363739837398374,
      "step": 6711,
      "training_loss": 7.210384368896484
    },
    {
      "epoch": 0.3637940379403794,
      "grad_norm": 21.534828186035156,
      "learning_rate": 1e-05,
      "loss": 6.2012,
      "step": 6712
    },
    {
      "epoch": 0.3637940379403794,
      "step": 6712,
      "training_loss": 6.929117202758789
    },
    {
      "epoch": 0.36384823848238484,
      "step": 6713,
      "training_loss": 7.908634185791016
    },
    {
      "epoch": 0.36390243902439023,
      "step": 6714,
      "training_loss": 7.8359694480896
    },
    {
      "epoch": 0.3639566395663957,
      "step": 6715,
      "training_loss": 7.291739463806152
    },
    {
      "epoch": 0.36401084010840107,
      "grad_norm": 24.981536865234375,
      "learning_rate": 1e-05,
      "loss": 7.4914,
      "step": 6716
    },
    {
      "epoch": 0.36401084010840107,
      "step": 6716,
      "training_loss": 6.887685298919678
    },
    {
      "epoch": 0.3640650406504065,
      "step": 6717,
      "training_loss": 6.627788066864014
    },
    {
      "epoch": 0.3641192411924119,
      "step": 6718,
      "training_loss": 6.988602161407471
    },
    {
      "epoch": 0.36417344173441735,
      "step": 6719,
      "training_loss": 7.591480731964111
    },
    {
      "epoch": 0.3642276422764228,
      "grad_norm": 29.34467887878418,
      "learning_rate": 1e-05,
      "loss": 7.0239,
      "step": 6720
    },
    {
      "epoch": 0.3642276422764228,
      "step": 6720,
      "training_loss": 7.016384601593018
    },
    {
      "epoch": 0.3642818428184282,
      "step": 6721,
      "training_loss": 7.458726406097412
    },
    {
      "epoch": 0.3643360433604336,
      "step": 6722,
      "training_loss": 4.909348964691162
    },
    {
      "epoch": 0.364390243902439,
      "step": 6723,
      "training_loss": 6.565119743347168
    },
    {
      "epoch": 0.36444444444444446,
      "grad_norm": 20.13692283630371,
      "learning_rate": 1e-05,
      "loss": 6.4874,
      "step": 6724
    },
    {
      "epoch": 0.36444444444444446,
      "step": 6724,
      "training_loss": 5.827091217041016
    },
    {
      "epoch": 0.36449864498644985,
      "step": 6725,
      "training_loss": 6.301394939422607
    },
    {
      "epoch": 0.3645528455284553,
      "step": 6726,
      "training_loss": 6.67828369140625
    },
    {
      "epoch": 0.3646070460704607,
      "step": 6727,
      "training_loss": 4.039531707763672
    },
    {
      "epoch": 0.36466124661246613,
      "grad_norm": 29.05623435974121,
      "learning_rate": 1e-05,
      "loss": 5.7116,
      "step": 6728
    },
    {
      "epoch": 0.36466124661246613,
      "step": 6728,
      "training_loss": 8.12631893157959
    },
    {
      "epoch": 0.3647154471544715,
      "step": 6729,
      "training_loss": 6.795499801635742
    },
    {
      "epoch": 0.36476964769647696,
      "step": 6730,
      "training_loss": 7.026108741760254
    },
    {
      "epoch": 0.3648238482384824,
      "step": 6731,
      "training_loss": 4.592558860778809
    },
    {
      "epoch": 0.3648780487804878,
      "grad_norm": 21.79595375061035,
      "learning_rate": 1e-05,
      "loss": 6.6351,
      "step": 6732
    },
    {
      "epoch": 0.3648780487804878,
      "step": 6732,
      "training_loss": 6.885189056396484
    },
    {
      "epoch": 0.36493224932249324,
      "step": 6733,
      "training_loss": 7.703930854797363
    },
    {
      "epoch": 0.36498644986449863,
      "step": 6734,
      "training_loss": 5.878296852111816
    },
    {
      "epoch": 0.3650406504065041,
      "step": 6735,
      "training_loss": 7.824390888214111
    },
    {
      "epoch": 0.36509485094850946,
      "grad_norm": 38.18163299560547,
      "learning_rate": 1e-05,
      "loss": 7.073,
      "step": 6736
    },
    {
      "epoch": 0.36509485094850946,
      "step": 6736,
      "training_loss": 7.054073333740234
    },
    {
      "epoch": 0.3651490514905149,
      "step": 6737,
      "training_loss": 8.036407470703125
    },
    {
      "epoch": 0.3652032520325203,
      "step": 6738,
      "training_loss": 4.896052360534668
    },
    {
      "epoch": 0.36525745257452574,
      "step": 6739,
      "training_loss": 6.565099716186523
    },
    {
      "epoch": 0.3653116531165312,
      "grad_norm": 18.107717514038086,
      "learning_rate": 1e-05,
      "loss": 6.6379,
      "step": 6740
    },
    {
      "epoch": 0.3653116531165312,
      "step": 6740,
      "training_loss": 6.9062066078186035
    },
    {
      "epoch": 0.3653658536585366,
      "step": 6741,
      "training_loss": 6.298330783843994
    },
    {
      "epoch": 0.365420054200542,
      "step": 6742,
      "training_loss": 6.414133071899414
    },
    {
      "epoch": 0.3654742547425474,
      "step": 6743,
      "training_loss": 7.23737096786499
    },
    {
      "epoch": 0.36552845528455286,
      "grad_norm": 20.151622772216797,
      "learning_rate": 1e-05,
      "loss": 6.714,
      "step": 6744
    },
    {
      "epoch": 0.36552845528455286,
      "step": 6744,
      "training_loss": 6.075956344604492
    },
    {
      "epoch": 0.36558265582655824,
      "step": 6745,
      "training_loss": 7.299052715301514
    },
    {
      "epoch": 0.3656368563685637,
      "step": 6746,
      "training_loss": 7.298227310180664
    },
    {
      "epoch": 0.3656910569105691,
      "step": 6747,
      "training_loss": 7.304922103881836
    },
    {
      "epoch": 0.3657452574525745,
      "grad_norm": 22.623878479003906,
      "learning_rate": 1e-05,
      "loss": 6.9945,
      "step": 6748
    },
    {
      "epoch": 0.3657452574525745,
      "step": 6748,
      "training_loss": 6.745022296905518
    },
    {
      "epoch": 0.36579945799457997,
      "step": 6749,
      "training_loss": 7.697759628295898
    },
    {
      "epoch": 0.36585365853658536,
      "step": 6750,
      "training_loss": 6.813319683074951
    },
    {
      "epoch": 0.3659078590785908,
      "step": 6751,
      "training_loss": 6.4053144454956055
    },
    {
      "epoch": 0.3659620596205962,
      "grad_norm": 26.744638442993164,
      "learning_rate": 1e-05,
      "loss": 6.9154,
      "step": 6752
    },
    {
      "epoch": 0.3659620596205962,
      "step": 6752,
      "training_loss": 7.501651287078857
    },
    {
      "epoch": 0.36601626016260164,
      "step": 6753,
      "training_loss": 7.261375904083252
    },
    {
      "epoch": 0.366070460704607,
      "step": 6754,
      "training_loss": 5.917473793029785
    },
    {
      "epoch": 0.36612466124661247,
      "step": 6755,
      "training_loss": 7.64984655380249
    },
    {
      "epoch": 0.36617886178861786,
      "grad_norm": 24.154245376586914,
      "learning_rate": 1e-05,
      "loss": 7.0826,
      "step": 6756
    },
    {
      "epoch": 0.36617886178861786,
      "step": 6756,
      "training_loss": 7.442232608795166
    },
    {
      "epoch": 0.3662330623306233,
      "step": 6757,
      "training_loss": 6.661323547363281
    },
    {
      "epoch": 0.36628726287262875,
      "step": 6758,
      "training_loss": 6.757535934448242
    },
    {
      "epoch": 0.36634146341463414,
      "step": 6759,
      "training_loss": 7.127504348754883
    },
    {
      "epoch": 0.3663956639566396,
      "grad_norm": 32.74821853637695,
      "learning_rate": 1e-05,
      "loss": 6.9971,
      "step": 6760
    },
    {
      "epoch": 0.3663956639566396,
      "step": 6760,
      "training_loss": 6.966625213623047
    },
    {
      "epoch": 0.366449864498645,
      "step": 6761,
      "training_loss": 5.206571578979492
    },
    {
      "epoch": 0.3665040650406504,
      "step": 6762,
      "training_loss": 6.789125442504883
    },
    {
      "epoch": 0.3665582655826558,
      "step": 6763,
      "training_loss": 6.485213279724121
    },
    {
      "epoch": 0.36661246612466125,
      "grad_norm": 21.758634567260742,
      "learning_rate": 1e-05,
      "loss": 6.3619,
      "step": 6764
    },
    {
      "epoch": 0.36661246612466125,
      "step": 6764,
      "training_loss": 6.233237266540527
    },
    {
      "epoch": 0.36666666666666664,
      "step": 6765,
      "training_loss": 6.991031646728516
    },
    {
      "epoch": 0.3667208672086721,
      "step": 6766,
      "training_loss": 7.814845561981201
    },
    {
      "epoch": 0.36677506775067753,
      "step": 6767,
      "training_loss": 5.649831771850586
    },
    {
      "epoch": 0.3668292682926829,
      "grad_norm": 19.371904373168945,
      "learning_rate": 1e-05,
      "loss": 6.6722,
      "step": 6768
    },
    {
      "epoch": 0.3668292682926829,
      "step": 6768,
      "training_loss": 7.369756698608398
    },
    {
      "epoch": 0.36688346883468836,
      "step": 6769,
      "training_loss": 5.927788734436035
    },
    {
      "epoch": 0.36693766937669375,
      "step": 6770,
      "training_loss": 6.069366455078125
    },
    {
      "epoch": 0.3669918699186992,
      "step": 6771,
      "training_loss": 7.739939212799072
    },
    {
      "epoch": 0.3670460704607046,
      "grad_norm": 22.200273513793945,
      "learning_rate": 1e-05,
      "loss": 6.7767,
      "step": 6772
    },
    {
      "epoch": 0.3670460704607046,
      "step": 6772,
      "training_loss": 7.795598983764648
    },
    {
      "epoch": 0.36710027100271003,
      "step": 6773,
      "training_loss": 6.92857551574707
    },
    {
      "epoch": 0.3671544715447154,
      "step": 6774,
      "training_loss": 6.844959735870361
    },
    {
      "epoch": 0.36720867208672087,
      "step": 6775,
      "training_loss": 6.533808708190918
    },
    {
      "epoch": 0.3672628726287263,
      "grad_norm": 32.68263244628906,
      "learning_rate": 1e-05,
      "loss": 7.0257,
      "step": 6776
    },
    {
      "epoch": 0.3672628726287263,
      "step": 6776,
      "training_loss": 6.974741458892822
    },
    {
      "epoch": 0.3673170731707317,
      "step": 6777,
      "training_loss": 5.645468235015869
    },
    {
      "epoch": 0.36737127371273715,
      "step": 6778,
      "training_loss": 6.546815872192383
    },
    {
      "epoch": 0.36742547425474253,
      "step": 6779,
      "training_loss": 6.084687232971191
    },
    {
      "epoch": 0.367479674796748,
      "grad_norm": 75.1548080444336,
      "learning_rate": 1e-05,
      "loss": 6.3129,
      "step": 6780
    },
    {
      "epoch": 0.367479674796748,
      "step": 6780,
      "training_loss": 6.3757734298706055
    },
    {
      "epoch": 0.36753387533875337,
      "step": 6781,
      "training_loss": 6.240445137023926
    },
    {
      "epoch": 0.3675880758807588,
      "step": 6782,
      "training_loss": 5.396172523498535
    },
    {
      "epoch": 0.3676422764227642,
      "step": 6783,
      "training_loss": 6.43947696685791
    },
    {
      "epoch": 0.36769647696476965,
      "grad_norm": 17.55197525024414,
      "learning_rate": 1e-05,
      "loss": 6.113,
      "step": 6784
    },
    {
      "epoch": 0.36769647696476965,
      "step": 6784,
      "training_loss": 6.200657844543457
    },
    {
      "epoch": 0.3677506775067751,
      "step": 6785,
      "training_loss": 6.638781547546387
    },
    {
      "epoch": 0.3678048780487805,
      "step": 6786,
      "training_loss": 7.130745887756348
    },
    {
      "epoch": 0.3678590785907859,
      "step": 6787,
      "training_loss": 6.123559474945068
    },
    {
      "epoch": 0.3679132791327913,
      "grad_norm": 30.300151824951172,
      "learning_rate": 1e-05,
      "loss": 6.5234,
      "step": 6788
    },
    {
      "epoch": 0.3679132791327913,
      "step": 6788,
      "training_loss": 6.444717884063721
    },
    {
      "epoch": 0.36796747967479676,
      "step": 6789,
      "training_loss": 6.568289756774902
    },
    {
      "epoch": 0.36802168021680215,
      "step": 6790,
      "training_loss": 7.748286247253418
    },
    {
      "epoch": 0.3680758807588076,
      "step": 6791,
      "training_loss": 7.622835636138916
    },
    {
      "epoch": 0.368130081300813,
      "grad_norm": 29.85880470275879,
      "learning_rate": 1e-05,
      "loss": 7.096,
      "step": 6792
    },
    {
      "epoch": 0.368130081300813,
      "step": 6792,
      "training_loss": 6.876766681671143
    },
    {
      "epoch": 0.36818428184281843,
      "step": 6793,
      "training_loss": 6.488679885864258
    },
    {
      "epoch": 0.3682384823848239,
      "step": 6794,
      "training_loss": 6.872808456420898
    },
    {
      "epoch": 0.36829268292682926,
      "step": 6795,
      "training_loss": 8.18111515045166
    },
    {
      "epoch": 0.3683468834688347,
      "grad_norm": 22.92772102355957,
      "learning_rate": 1e-05,
      "loss": 7.1048,
      "step": 6796
    },
    {
      "epoch": 0.3683468834688347,
      "step": 6796,
      "training_loss": 7.965398788452148
    },
    {
      "epoch": 0.3684010840108401,
      "step": 6797,
      "training_loss": 7.715510845184326
    },
    {
      "epoch": 0.36845528455284554,
      "step": 6798,
      "training_loss": 7.56705904006958
    },
    {
      "epoch": 0.36850948509485093,
      "step": 6799,
      "training_loss": 5.723963260650635
    },
    {
      "epoch": 0.3685636856368564,
      "grad_norm": 27.400339126586914,
      "learning_rate": 1e-05,
      "loss": 7.243,
      "step": 6800
    },
    {
      "epoch": 0.3685636856368564,
      "step": 6800,
      "training_loss": 7.046982288360596
    },
    {
      "epoch": 0.36861788617886176,
      "step": 6801,
      "training_loss": 5.494089603424072
    },
    {
      "epoch": 0.3686720867208672,
      "step": 6802,
      "training_loss": 6.962437152862549
    },
    {
      "epoch": 0.36872628726287265,
      "step": 6803,
      "training_loss": 5.9609527587890625
    },
    {
      "epoch": 0.36878048780487804,
      "grad_norm": 47.23640823364258,
      "learning_rate": 1e-05,
      "loss": 6.3661,
      "step": 6804
    },
    {
      "epoch": 0.36878048780487804,
      "step": 6804,
      "training_loss": 6.790295124053955
    },
    {
      "epoch": 0.3688346883468835,
      "step": 6805,
      "training_loss": 6.528672218322754
    },
    {
      "epoch": 0.3688888888888889,
      "step": 6806,
      "training_loss": 7.175172328948975
    },
    {
      "epoch": 0.3689430894308943,
      "step": 6807,
      "training_loss": 6.989517688751221
    },
    {
      "epoch": 0.3689972899728997,
      "grad_norm": 61.4882926940918,
      "learning_rate": 1e-05,
      "loss": 6.8709,
      "step": 6808
    },
    {
      "epoch": 0.3689972899728997,
      "step": 6808,
      "training_loss": 4.13694429397583
    },
    {
      "epoch": 0.36905149051490516,
      "step": 6809,
      "training_loss": 6.957061767578125
    },
    {
      "epoch": 0.36910569105691055,
      "step": 6810,
      "training_loss": 6.906156539916992
    },
    {
      "epoch": 0.369159891598916,
      "step": 6811,
      "training_loss": 5.973457336425781
    },
    {
      "epoch": 0.36921409214092143,
      "grad_norm": 33.22321319580078,
      "learning_rate": 1e-05,
      "loss": 5.9934,
      "step": 6812
    },
    {
      "epoch": 0.36921409214092143,
      "step": 6812,
      "training_loss": 8.5363130569458
    },
    {
      "epoch": 0.3692682926829268,
      "step": 6813,
      "training_loss": 6.565407752990723
    },
    {
      "epoch": 0.36932249322493227,
      "step": 6814,
      "training_loss": 6.747478485107422
    },
    {
      "epoch": 0.36937669376693766,
      "step": 6815,
      "training_loss": 6.970669746398926
    },
    {
      "epoch": 0.3694308943089431,
      "grad_norm": 16.165372848510742,
      "learning_rate": 1e-05,
      "loss": 7.205,
      "step": 6816
    },
    {
      "epoch": 0.3694308943089431,
      "step": 6816,
      "training_loss": 5.975772857666016
    },
    {
      "epoch": 0.3694850948509485,
      "step": 6817,
      "training_loss": 8.459402084350586
    },
    {
      "epoch": 0.36953929539295394,
      "step": 6818,
      "training_loss": 7.200088024139404
    },
    {
      "epoch": 0.3695934959349593,
      "step": 6819,
      "training_loss": 6.465674877166748
    },
    {
      "epoch": 0.36964769647696477,
      "grad_norm": 21.088457107543945,
      "learning_rate": 1e-05,
      "loss": 7.0252,
      "step": 6820
    },
    {
      "epoch": 0.36964769647696477,
      "step": 6820,
      "training_loss": 6.927291393280029
    },
    {
      "epoch": 0.3697018970189702,
      "step": 6821,
      "training_loss": 7.5679779052734375
    },
    {
      "epoch": 0.3697560975609756,
      "step": 6822,
      "training_loss": 6.632697105407715
    },
    {
      "epoch": 0.36981029810298105,
      "step": 6823,
      "training_loss": 4.862186431884766
    },
    {
      "epoch": 0.36986449864498644,
      "grad_norm": 24.927433013916016,
      "learning_rate": 1e-05,
      "loss": 6.4975,
      "step": 6824
    },
    {
      "epoch": 0.36986449864498644,
      "step": 6824,
      "training_loss": 5.953434944152832
    },
    {
      "epoch": 0.3699186991869919,
      "step": 6825,
      "training_loss": 5.243303298950195
    },
    {
      "epoch": 0.3699728997289973,
      "step": 6826,
      "training_loss": 5.775541305541992
    },
    {
      "epoch": 0.3700271002710027,
      "step": 6827,
      "training_loss": 6.817763805389404
    },
    {
      "epoch": 0.3700813008130081,
      "grad_norm": 21.141420364379883,
      "learning_rate": 1e-05,
      "loss": 5.9475,
      "step": 6828
    },
    {
      "epoch": 0.3700813008130081,
      "step": 6828,
      "training_loss": 7.217027187347412
    },
    {
      "epoch": 0.37013550135501355,
      "step": 6829,
      "training_loss": 3.7050178050994873
    },
    {
      "epoch": 0.370189701897019,
      "step": 6830,
      "training_loss": 6.3599348068237305
    },
    {
      "epoch": 0.3702439024390244,
      "step": 6831,
      "training_loss": 5.644771575927734
    },
    {
      "epoch": 0.37029810298102983,
      "grad_norm": 35.42595291137695,
      "learning_rate": 1e-05,
      "loss": 5.7317,
      "step": 6832
    },
    {
      "epoch": 0.37029810298102983,
      "step": 6832,
      "training_loss": 8.770609855651855
    },
    {
      "epoch": 0.3703523035230352,
      "step": 6833,
      "training_loss": 6.070414066314697
    },
    {
      "epoch": 0.37040650406504066,
      "step": 6834,
      "training_loss": 6.930695533752441
    },
    {
      "epoch": 0.37046070460704605,
      "step": 6835,
      "training_loss": 6.632177829742432
    },
    {
      "epoch": 0.3705149051490515,
      "grad_norm": 25.22545623779297,
      "learning_rate": 1e-05,
      "loss": 7.101,
      "step": 6836
    },
    {
      "epoch": 0.3705149051490515,
      "step": 6836,
      "training_loss": 6.6497416496276855
    },
    {
      "epoch": 0.3705691056910569,
      "step": 6837,
      "training_loss": 7.688798427581787
    },
    {
      "epoch": 0.37062330623306233,
      "step": 6838,
      "training_loss": 7.151877403259277
    },
    {
      "epoch": 0.3706775067750678,
      "step": 6839,
      "training_loss": 5.180576801300049
    },
    {
      "epoch": 0.37073170731707317,
      "grad_norm": 27.149742126464844,
      "learning_rate": 1e-05,
      "loss": 6.6677,
      "step": 6840
    },
    {
      "epoch": 0.37073170731707317,
      "step": 6840,
      "training_loss": 7.272748947143555
    },
    {
      "epoch": 0.3707859078590786,
      "step": 6841,
      "training_loss": 7.059701919555664
    },
    {
      "epoch": 0.370840108401084,
      "step": 6842,
      "training_loss": 7.083401203155518
    },
    {
      "epoch": 0.37089430894308945,
      "step": 6843,
      "training_loss": 8.88982105255127
    },
    {
      "epoch": 0.37094850948509484,
      "grad_norm": 75.68962860107422,
      "learning_rate": 1e-05,
      "loss": 7.5764,
      "step": 6844
    },
    {
      "epoch": 0.37094850948509484,
      "step": 6844,
      "training_loss": 7.181268692016602
    },
    {
      "epoch": 0.3710027100271003,
      "step": 6845,
      "training_loss": 6.405343055725098
    },
    {
      "epoch": 0.37105691056910567,
      "step": 6846,
      "training_loss": 7.350888729095459
    },
    {
      "epoch": 0.3711111111111111,
      "step": 6847,
      "training_loss": 5.979510307312012
    },
    {
      "epoch": 0.37116531165311656,
      "grad_norm": 24.528013229370117,
      "learning_rate": 1e-05,
      "loss": 6.7293,
      "step": 6848
    },
    {
      "epoch": 0.37116531165311656,
      "step": 6848,
      "training_loss": 8.914355278015137
    },
    {
      "epoch": 0.37121951219512195,
      "step": 6849,
      "training_loss": 6.736927509307861
    },
    {
      "epoch": 0.3712737127371274,
      "step": 6850,
      "training_loss": 6.911015033721924
    },
    {
      "epoch": 0.3713279132791328,
      "step": 6851,
      "training_loss": 6.463784694671631
    },
    {
      "epoch": 0.3713821138211382,
      "grad_norm": 23.68647003173828,
      "learning_rate": 1e-05,
      "loss": 7.2565,
      "step": 6852
    },
    {
      "epoch": 0.3713821138211382,
      "step": 6852,
      "training_loss": 6.687892436981201
    },
    {
      "epoch": 0.3714363143631436,
      "step": 6853,
      "training_loss": 7.963924407958984
    },
    {
      "epoch": 0.37149051490514906,
      "step": 6854,
      "training_loss": 7.048602104187012
    },
    {
      "epoch": 0.37154471544715445,
      "step": 6855,
      "training_loss": 5.67565393447876
    },
    {
      "epoch": 0.3715989159891599,
      "grad_norm": 24.851526260375977,
      "learning_rate": 1e-05,
      "loss": 6.844,
      "step": 6856
    },
    {
      "epoch": 0.3715989159891599,
      "step": 6856,
      "training_loss": 7.239675521850586
    },
    {
      "epoch": 0.3716531165311653,
      "step": 6857,
      "training_loss": 7.354898929595947
    },
    {
      "epoch": 0.37170731707317073,
      "step": 6858,
      "training_loss": 7.929262638092041
    },
    {
      "epoch": 0.3717615176151762,
      "step": 6859,
      "training_loss": 6.294778347015381
    },
    {
      "epoch": 0.37181571815718156,
      "grad_norm": 29.15407371520996,
      "learning_rate": 1e-05,
      "loss": 7.2047,
      "step": 6860
    },
    {
      "epoch": 0.37181571815718156,
      "step": 6860,
      "training_loss": 7.820303440093994
    },
    {
      "epoch": 0.371869918699187,
      "step": 6861,
      "training_loss": 7.092202186584473
    },
    {
      "epoch": 0.3719241192411924,
      "step": 6862,
      "training_loss": 7.563591480255127
    },
    {
      "epoch": 0.37197831978319784,
      "step": 6863,
      "training_loss": 6.547972679138184
    },
    {
      "epoch": 0.37203252032520323,
      "grad_norm": 27.72176170349121,
      "learning_rate": 1e-05,
      "loss": 7.256,
      "step": 6864
    },
    {
      "epoch": 0.37203252032520323,
      "step": 6864,
      "training_loss": 6.878727912902832
    },
    {
      "epoch": 0.3720867208672087,
      "step": 6865,
      "training_loss": 5.142728328704834
    },
    {
      "epoch": 0.37214092140921406,
      "step": 6866,
      "training_loss": 6.1219892501831055
    },
    {
      "epoch": 0.3721951219512195,
      "step": 6867,
      "training_loss": 6.9098615646362305
    },
    {
      "epoch": 0.37224932249322495,
      "grad_norm": 21.048173904418945,
      "learning_rate": 1e-05,
      "loss": 6.2633,
      "step": 6868
    },
    {
      "epoch": 0.37224932249322495,
      "step": 6868,
      "training_loss": 4.924676895141602
    },
    {
      "epoch": 0.37230352303523034,
      "step": 6869,
      "training_loss": 6.4606032371521
    },
    {
      "epoch": 0.3723577235772358,
      "step": 6870,
      "training_loss": 7.6628570556640625
    },
    {
      "epoch": 0.3724119241192412,
      "step": 6871,
      "training_loss": 5.038130760192871
    },
    {
      "epoch": 0.3724661246612466,
      "grad_norm": 27.587322235107422,
      "learning_rate": 1e-05,
      "loss": 6.0216,
      "step": 6872
    },
    {
      "epoch": 0.3724661246612466,
      "step": 6872,
      "training_loss": 5.401768207550049
    },
    {
      "epoch": 0.372520325203252,
      "step": 6873,
      "training_loss": 6.673570156097412
    },
    {
      "epoch": 0.37257452574525746,
      "step": 6874,
      "training_loss": 5.682947158813477
    },
    {
      "epoch": 0.37262872628726285,
      "step": 6875,
      "training_loss": 6.720434665679932
    },
    {
      "epoch": 0.3726829268292683,
      "grad_norm": 38.62486267089844,
      "learning_rate": 1e-05,
      "loss": 6.1197,
      "step": 6876
    },
    {
      "epoch": 0.3726829268292683,
      "step": 6876,
      "training_loss": 5.143740177154541
    },
    {
      "epoch": 0.37273712737127374,
      "step": 6877,
      "training_loss": 7.220987796783447
    },
    {
      "epoch": 0.3727913279132791,
      "step": 6878,
      "training_loss": 5.637367248535156
    },
    {
      "epoch": 0.37284552845528457,
      "step": 6879,
      "training_loss": 5.494185924530029
    },
    {
      "epoch": 0.37289972899728996,
      "grad_norm": 21.472431182861328,
      "learning_rate": 1e-05,
      "loss": 5.8741,
      "step": 6880
    },
    {
      "epoch": 0.37289972899728996,
      "step": 6880,
      "training_loss": 7.5033979415893555
    },
    {
      "epoch": 0.3729539295392954,
      "step": 6881,
      "training_loss": 4.0686845779418945
    },
    {
      "epoch": 0.3730081300813008,
      "step": 6882,
      "training_loss": 6.945824146270752
    },
    {
      "epoch": 0.37306233062330624,
      "step": 6883,
      "training_loss": 5.229550361633301
    },
    {
      "epoch": 0.3731165311653116,
      "grad_norm": 25.975069046020508,
      "learning_rate": 1e-05,
      "loss": 5.9369,
      "step": 6884
    },
    {
      "epoch": 0.3731165311653116,
      "step": 6884,
      "training_loss": 5.839024543762207
    },
    {
      "epoch": 0.37317073170731707,
      "step": 6885,
      "training_loss": 6.237910270690918
    },
    {
      "epoch": 0.3732249322493225,
      "step": 6886,
      "training_loss": 6.733879566192627
    },
    {
      "epoch": 0.3732791327913279,
      "step": 6887,
      "training_loss": 7.25178861618042
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 24.036773681640625,
      "learning_rate": 1e-05,
      "loss": 6.5157,
      "step": 6888
    },
    {
      "epoch": 0.37333333333333335,
      "step": 6888,
      "training_loss": 6.338096618652344
    },
    {
      "epoch": 0.37338753387533874,
      "step": 6889,
      "training_loss": 6.493306636810303
    },
    {
      "epoch": 0.3734417344173442,
      "step": 6890,
      "training_loss": 6.3550333976745605
    },
    {
      "epoch": 0.3734959349593496,
      "step": 6891,
      "training_loss": 6.903665542602539
    },
    {
      "epoch": 0.373550135501355,
      "grad_norm": 19.937255859375,
      "learning_rate": 1e-05,
      "loss": 6.5225,
      "step": 6892
    },
    {
      "epoch": 0.373550135501355,
      "step": 6892,
      "training_loss": 6.752291202545166
    },
    {
      "epoch": 0.3736043360433604,
      "step": 6893,
      "training_loss": 6.099608421325684
    },
    {
      "epoch": 0.37365853658536585,
      "step": 6894,
      "training_loss": 6.531599521636963
    },
    {
      "epoch": 0.3737127371273713,
      "step": 6895,
      "training_loss": 6.82792329788208
    },
    {
      "epoch": 0.3737669376693767,
      "grad_norm": 21.293384552001953,
      "learning_rate": 1e-05,
      "loss": 6.5529,
      "step": 6896
    },
    {
      "epoch": 0.3737669376693767,
      "step": 6896,
      "training_loss": 7.067358493804932
    },
    {
      "epoch": 0.37382113821138213,
      "step": 6897,
      "training_loss": 6.670102596282959
    },
    {
      "epoch": 0.3738753387533875,
      "step": 6898,
      "training_loss": 5.264103412628174
    },
    {
      "epoch": 0.37392953929539297,
      "step": 6899,
      "training_loss": 7.865945816040039
    },
    {
      "epoch": 0.37398373983739835,
      "grad_norm": 25.025325775146484,
      "learning_rate": 1e-05,
      "loss": 6.7169,
      "step": 6900
    },
    {
      "epoch": 0.37398373983739835,
      "step": 6900,
      "training_loss": 7.781487941741943
    },
    {
      "epoch": 0.3740379403794038,
      "step": 6901,
      "training_loss": 7.00124454498291
    },
    {
      "epoch": 0.3740921409214092,
      "step": 6902,
      "training_loss": 6.308883190155029
    },
    {
      "epoch": 0.37414634146341463,
      "step": 6903,
      "training_loss": 5.531224250793457
    },
    {
      "epoch": 0.3742005420054201,
      "grad_norm": 30.041797637939453,
      "learning_rate": 1e-05,
      "loss": 6.6557,
      "step": 6904
    },
    {
      "epoch": 0.3742005420054201,
      "step": 6904,
      "training_loss": 7.472126483917236
    },
    {
      "epoch": 0.37425474254742547,
      "step": 6905,
      "training_loss": 6.438598155975342
    },
    {
      "epoch": 0.3743089430894309,
      "step": 6906,
      "training_loss": 6.97121000289917
    },
    {
      "epoch": 0.3743631436314363,
      "step": 6907,
      "training_loss": 5.774769306182861
    },
    {
      "epoch": 0.37441734417344175,
      "grad_norm": 28.620033264160156,
      "learning_rate": 1e-05,
      "loss": 6.6642,
      "step": 6908
    },
    {
      "epoch": 0.37441734417344175,
      "step": 6908,
      "training_loss": 7.1214518547058105
    },
    {
      "epoch": 0.37447154471544714,
      "step": 6909,
      "training_loss": 7.0958170890808105
    },
    {
      "epoch": 0.3745257452574526,
      "step": 6910,
      "training_loss": 7.243278503417969
    },
    {
      "epoch": 0.37457994579945797,
      "step": 6911,
      "training_loss": 4.821236610412598
    },
    {
      "epoch": 0.3746341463414634,
      "grad_norm": 23.034090042114258,
      "learning_rate": 1e-05,
      "loss": 6.5704,
      "step": 6912
    },
    {
      "epoch": 0.3746341463414634,
      "step": 6912,
      "training_loss": 5.890326023101807
    },
    {
      "epoch": 0.37468834688346886,
      "step": 6913,
      "training_loss": 7.8511271476745605
    },
    {
      "epoch": 0.37474254742547425,
      "step": 6914,
      "training_loss": 7.6506171226501465
    },
    {
      "epoch": 0.3747967479674797,
      "step": 6915,
      "training_loss": 5.778909683227539
    },
    {
      "epoch": 0.3748509485094851,
      "grad_norm": 72.24595642089844,
      "learning_rate": 1e-05,
      "loss": 6.7927,
      "step": 6916
    },
    {
      "epoch": 0.3748509485094851,
      "step": 6916,
      "training_loss": 5.794347763061523
    },
    {
      "epoch": 0.3749051490514905,
      "step": 6917,
      "training_loss": 7.7222137451171875
    },
    {
      "epoch": 0.3749593495934959,
      "step": 6918,
      "training_loss": 7.152623176574707
    },
    {
      "epoch": 0.37501355013550136,
      "step": 6919,
      "training_loss": 7.111287593841553
    },
    {
      "epoch": 0.37506775067750675,
      "grad_norm": 24.77666664123535,
      "learning_rate": 1e-05,
      "loss": 6.9451,
      "step": 6920
    },
    {
      "epoch": 0.37506775067750675,
      "step": 6920,
      "training_loss": 5.216424942016602
    },
    {
      "epoch": 0.3751219512195122,
      "step": 6921,
      "training_loss": 7.57433557510376
    },
    {
      "epoch": 0.37517615176151764,
      "step": 6922,
      "training_loss": 6.028023719787598
    },
    {
      "epoch": 0.37523035230352303,
      "step": 6923,
      "training_loss": 7.33193826675415
    },
    {
      "epoch": 0.3752845528455285,
      "grad_norm": 32.39814376831055,
      "learning_rate": 1e-05,
      "loss": 6.5377,
      "step": 6924
    },
    {
      "epoch": 0.3752845528455285,
      "step": 6924,
      "training_loss": 6.123784065246582
    },
    {
      "epoch": 0.37533875338753386,
      "step": 6925,
      "training_loss": 5.674304008483887
    },
    {
      "epoch": 0.3753929539295393,
      "step": 6926,
      "training_loss": 5.834909915924072
    },
    {
      "epoch": 0.3754471544715447,
      "step": 6927,
      "training_loss": 7.063019752502441
    },
    {
      "epoch": 0.37550135501355014,
      "grad_norm": 34.39344787597656,
      "learning_rate": 1e-05,
      "loss": 6.174,
      "step": 6928
    },
    {
      "epoch": 0.37550135501355014,
      "step": 6928,
      "training_loss": 7.728333473205566
    },
    {
      "epoch": 0.37555555555555553,
      "step": 6929,
      "training_loss": 5.6715288162231445
    },
    {
      "epoch": 0.375609756097561,
      "step": 6930,
      "training_loss": 5.14422082901001
    },
    {
      "epoch": 0.3756639566395664,
      "step": 6931,
      "training_loss": 6.802100658416748
    },
    {
      "epoch": 0.3757181571815718,
      "grad_norm": 35.41592025756836,
      "learning_rate": 1e-05,
      "loss": 6.3365,
      "step": 6932
    },
    {
      "epoch": 0.3757181571815718,
      "step": 6932,
      "training_loss": 6.689964771270752
    },
    {
      "epoch": 0.37577235772357725,
      "step": 6933,
      "training_loss": 6.44517183303833
    },
    {
      "epoch": 0.37582655826558264,
      "step": 6934,
      "training_loss": 8.385297775268555
    },
    {
      "epoch": 0.3758807588075881,
      "step": 6935,
      "training_loss": 7.4003987312316895
    },
    {
      "epoch": 0.3759349593495935,
      "grad_norm": 41.561336517333984,
      "learning_rate": 1e-05,
      "loss": 7.2302,
      "step": 6936
    },
    {
      "epoch": 0.3759349593495935,
      "step": 6936,
      "training_loss": 7.193836212158203
    },
    {
      "epoch": 0.3759891598915989,
      "step": 6937,
      "training_loss": 5.886139392852783
    },
    {
      "epoch": 0.3760433604336043,
      "step": 6938,
      "training_loss": 8.148164749145508
    },
    {
      "epoch": 0.37609756097560976,
      "step": 6939,
      "training_loss": 7.377781867980957
    },
    {
      "epoch": 0.3761517615176152,
      "grad_norm": 17.984594345092773,
      "learning_rate": 1e-05,
      "loss": 7.1515,
      "step": 6940
    },
    {
      "epoch": 0.3761517615176152,
      "step": 6940,
      "training_loss": 6.662384986877441
    },
    {
      "epoch": 0.3762059620596206,
      "step": 6941,
      "training_loss": 7.781246662139893
    },
    {
      "epoch": 0.37626016260162604,
      "step": 6942,
      "training_loss": 7.035322666168213
    },
    {
      "epoch": 0.3763143631436314,
      "step": 6943,
      "training_loss": 7.372323036193848
    },
    {
      "epoch": 0.37636856368563687,
      "grad_norm": 18.083641052246094,
      "learning_rate": 1e-05,
      "loss": 7.2128,
      "step": 6944
    },
    {
      "epoch": 0.37636856368563687,
      "step": 6944,
      "training_loss": 6.547684669494629
    },
    {
      "epoch": 0.37642276422764226,
      "step": 6945,
      "training_loss": 7.0902323722839355
    },
    {
      "epoch": 0.3764769647696477,
      "step": 6946,
      "training_loss": 7.084469318389893
    },
    {
      "epoch": 0.3765311653116531,
      "step": 6947,
      "training_loss": 6.828534126281738
    },
    {
      "epoch": 0.37658536585365854,
      "grad_norm": 21.573535919189453,
      "learning_rate": 1e-05,
      "loss": 6.8877,
      "step": 6948
    },
    {
      "epoch": 0.37658536585365854,
      "step": 6948,
      "training_loss": 6.243709564208984
    },
    {
      "epoch": 0.376639566395664,
      "step": 6949,
      "training_loss": 7.974456310272217
    },
    {
      "epoch": 0.37669376693766937,
      "step": 6950,
      "training_loss": 7.4833083152771
    },
    {
      "epoch": 0.3767479674796748,
      "step": 6951,
      "training_loss": 7.167367935180664
    },
    {
      "epoch": 0.3768021680216802,
      "grad_norm": 25.37021255493164,
      "learning_rate": 1e-05,
      "loss": 7.2172,
      "step": 6952
    },
    {
      "epoch": 0.3768021680216802,
      "step": 6952,
      "training_loss": 6.176992893218994
    },
    {
      "epoch": 0.37685636856368565,
      "step": 6953,
      "training_loss": 5.94606351852417
    },
    {
      "epoch": 0.37691056910569104,
      "step": 6954,
      "training_loss": 6.92100191116333
    },
    {
      "epoch": 0.3769647696476965,
      "step": 6955,
      "training_loss": 8.480734825134277
    },
    {
      "epoch": 0.3770189701897019,
      "grad_norm": 48.028018951416016,
      "learning_rate": 1e-05,
      "loss": 6.8812,
      "step": 6956
    },
    {
      "epoch": 0.3770189701897019,
      "step": 6956,
      "training_loss": 7.07943868637085
    },
    {
      "epoch": 0.3770731707317073,
      "step": 6957,
      "training_loss": 7.695176601409912
    },
    {
      "epoch": 0.37712737127371276,
      "step": 6958,
      "training_loss": 6.114814281463623
    },
    {
      "epoch": 0.37718157181571815,
      "step": 6959,
      "training_loss": 6.503608703613281
    },
    {
      "epoch": 0.3772357723577236,
      "grad_norm": 31.18461799621582,
      "learning_rate": 1e-05,
      "loss": 6.8483,
      "step": 6960
    },
    {
      "epoch": 0.3772357723577236,
      "step": 6960,
      "training_loss": 6.385972499847412
    },
    {
      "epoch": 0.377289972899729,
      "step": 6961,
      "training_loss": 6.116177082061768
    },
    {
      "epoch": 0.37734417344173443,
      "step": 6962,
      "training_loss": 6.850268363952637
    },
    {
      "epoch": 0.3773983739837398,
      "step": 6963,
      "training_loss": 7.2313337326049805
    },
    {
      "epoch": 0.37745257452574527,
      "grad_norm": 17.435041427612305,
      "learning_rate": 1e-05,
      "loss": 6.6459,
      "step": 6964
    },
    {
      "epoch": 0.37745257452574527,
      "step": 6964,
      "training_loss": 6.413352012634277
    },
    {
      "epoch": 0.37750677506775066,
      "step": 6965,
      "training_loss": 6.467925548553467
    },
    {
      "epoch": 0.3775609756097561,
      "step": 6966,
      "training_loss": 7.061180591583252
    },
    {
      "epoch": 0.37761517615176154,
      "step": 6967,
      "training_loss": 6.575417518615723
    },
    {
      "epoch": 0.37766937669376693,
      "grad_norm": 24.980825424194336,
      "learning_rate": 1e-05,
      "loss": 6.6295,
      "step": 6968
    },
    {
      "epoch": 0.37766937669376693,
      "step": 6968,
      "training_loss": 6.991919040679932
    },
    {
      "epoch": 0.3777235772357724,
      "step": 6969,
      "training_loss": 8.193552017211914
    },
    {
      "epoch": 0.37777777777777777,
      "step": 6970,
      "training_loss": 6.673854827880859
    },
    {
      "epoch": 0.3778319783197832,
      "step": 6971,
      "training_loss": 7.221465587615967
    },
    {
      "epoch": 0.3778861788617886,
      "grad_norm": 30.335176467895508,
      "learning_rate": 1e-05,
      "loss": 7.2702,
      "step": 6972
    },
    {
      "epoch": 0.3778861788617886,
      "step": 6972,
      "training_loss": 6.829179286956787
    },
    {
      "epoch": 0.37794037940379405,
      "step": 6973,
      "training_loss": 8.896193504333496
    },
    {
      "epoch": 0.37799457994579944,
      "step": 6974,
      "training_loss": 6.193957328796387
    },
    {
      "epoch": 0.3780487804878049,
      "step": 6975,
      "training_loss": 6.675784111022949
    },
    {
      "epoch": 0.3781029810298103,
      "grad_norm": 38.64054489135742,
      "learning_rate": 1e-05,
      "loss": 7.1488,
      "step": 6976
    },
    {
      "epoch": 0.3781029810298103,
      "step": 6976,
      "training_loss": 7.090846538543701
    },
    {
      "epoch": 0.3781571815718157,
      "step": 6977,
      "training_loss": 5.796453952789307
    },
    {
      "epoch": 0.37821138211382116,
      "step": 6978,
      "training_loss": 6.686551570892334
    },
    {
      "epoch": 0.37826558265582655,
      "step": 6979,
      "training_loss": 6.606532573699951
    },
    {
      "epoch": 0.378319783197832,
      "grad_norm": 29.0800724029541,
      "learning_rate": 1e-05,
      "loss": 6.5451,
      "step": 6980
    },
    {
      "epoch": 0.378319783197832,
      "step": 6980,
      "training_loss": 7.920861721038818
    },
    {
      "epoch": 0.3783739837398374,
      "step": 6981,
      "training_loss": 6.985860824584961
    },
    {
      "epoch": 0.3784281842818428,
      "step": 6982,
      "training_loss": 6.178020000457764
    },
    {
      "epoch": 0.3784823848238482,
      "step": 6983,
      "training_loss": 4.912003993988037
    },
    {
      "epoch": 0.37853658536585366,
      "grad_norm": 28.89142608642578,
      "learning_rate": 1e-05,
      "loss": 6.4992,
      "step": 6984
    },
    {
      "epoch": 0.37853658536585366,
      "step": 6984,
      "training_loss": 5.839718818664551
    },
    {
      "epoch": 0.37859078590785905,
      "step": 6985,
      "training_loss": 5.980053424835205
    },
    {
      "epoch": 0.3786449864498645,
      "step": 6986,
      "training_loss": 5.657026290893555
    },
    {
      "epoch": 0.37869918699186994,
      "step": 6987,
      "training_loss": 7.776857376098633
    },
    {
      "epoch": 0.37875338753387533,
      "grad_norm": 24.705909729003906,
      "learning_rate": 1e-05,
      "loss": 6.3134,
      "step": 6988
    },
    {
      "epoch": 0.37875338753387533,
      "step": 6988,
      "training_loss": 7.482309341430664
    },
    {
      "epoch": 0.3788075880758808,
      "step": 6989,
      "training_loss": 8.239633560180664
    },
    {
      "epoch": 0.37886178861788616,
      "step": 6990,
      "training_loss": 6.156041145324707
    },
    {
      "epoch": 0.3789159891598916,
      "step": 6991,
      "training_loss": 6.611276149749756
    },
    {
      "epoch": 0.378970189701897,
      "grad_norm": 34.19172286987305,
      "learning_rate": 1e-05,
      "loss": 7.1223,
      "step": 6992
    },
    {
      "epoch": 0.378970189701897,
      "step": 6992,
      "training_loss": 3.745753049850464
    },
    {
      "epoch": 0.37902439024390244,
      "step": 6993,
      "training_loss": 4.5110673904418945
    },
    {
      "epoch": 0.37907859078590783,
      "step": 6994,
      "training_loss": 7.233354091644287
    },
    {
      "epoch": 0.3791327913279133,
      "step": 6995,
      "training_loss": 6.747306823730469
    },
    {
      "epoch": 0.3791869918699187,
      "grad_norm": 20.945871353149414,
      "learning_rate": 1e-05,
      "loss": 5.5594,
      "step": 6996
    },
    {
      "epoch": 0.3791869918699187,
      "step": 6996,
      "training_loss": 7.376743316650391
    },
    {
      "epoch": 0.3792411924119241,
      "step": 6997,
      "training_loss": 6.429549694061279
    },
    {
      "epoch": 0.37929539295392956,
      "step": 6998,
      "training_loss": 5.022339820861816
    },
    {
      "epoch": 0.37934959349593494,
      "step": 6999,
      "training_loss": 6.892563343048096
    },
    {
      "epoch": 0.3794037940379404,
      "grad_norm": 17.53384780883789,
      "learning_rate": 1e-05,
      "loss": 6.4303,
      "step": 7000
    },
    {
      "epoch": 0.3794037940379404,
      "step": 7000,
      "training_loss": 5.177480697631836
    },
    {
      "epoch": 0.3794579945799458,
      "step": 7001,
      "training_loss": 6.339449882507324
    },
    {
      "epoch": 0.3795121951219512,
      "step": 7002,
      "training_loss": 7.285297393798828
    },
    {
      "epoch": 0.3795663956639566,
      "step": 7003,
      "training_loss": 7.527730464935303
    },
    {
      "epoch": 0.37962059620596206,
      "grad_norm": 77.96324157714844,
      "learning_rate": 1e-05,
      "loss": 6.5825,
      "step": 7004
    },
    {
      "epoch": 0.37962059620596206,
      "step": 7004,
      "training_loss": 7.225308895111084
    },
    {
      "epoch": 0.3796747967479675,
      "step": 7005,
      "training_loss": 8.112135887145996
    },
    {
      "epoch": 0.3797289972899729,
      "step": 7006,
      "training_loss": 6.732477188110352
    },
    {
      "epoch": 0.37978319783197834,
      "step": 7007,
      "training_loss": 6.273375511169434
    },
    {
      "epoch": 0.3798373983739837,
      "grad_norm": 23.756715774536133,
      "learning_rate": 1e-05,
      "loss": 7.0858,
      "step": 7008
    },
    {
      "epoch": 0.3798373983739837,
      "step": 7008,
      "training_loss": 7.904748916625977
    },
    {
      "epoch": 0.37989159891598917,
      "step": 7009,
      "training_loss": 6.0537543296813965
    },
    {
      "epoch": 0.37994579945799456,
      "step": 7010,
      "training_loss": 7.560460090637207
    },
    {
      "epoch": 0.38,
      "step": 7011,
      "training_loss": 5.120344638824463
    },
    {
      "epoch": 0.3800542005420054,
      "grad_norm": 19.902671813964844,
      "learning_rate": 1e-05,
      "loss": 6.6598,
      "step": 7012
    },
    {
      "epoch": 0.3800542005420054,
      "step": 7012,
      "training_loss": 6.735270977020264
    },
    {
      "epoch": 0.38010840108401084,
      "step": 7013,
      "training_loss": 7.266846656799316
    },
    {
      "epoch": 0.3801626016260163,
      "step": 7014,
      "training_loss": 6.5916361808776855
    },
    {
      "epoch": 0.3802168021680217,
      "step": 7015,
      "training_loss": 7.46246337890625
    },
    {
      "epoch": 0.3802710027100271,
      "grad_norm": 20.970083236694336,
      "learning_rate": 1e-05,
      "loss": 7.0141,
      "step": 7016
    },
    {
      "epoch": 0.3802710027100271,
      "step": 7016,
      "training_loss": 8.255684852600098
    },
    {
      "epoch": 0.3803252032520325,
      "step": 7017,
      "training_loss": 6.838104724884033
    },
    {
      "epoch": 0.38037940379403795,
      "step": 7018,
      "training_loss": 7.682542324066162
    },
    {
      "epoch": 0.38043360433604334,
      "step": 7019,
      "training_loss": 6.734335899353027
    },
    {
      "epoch": 0.3804878048780488,
      "grad_norm": 26.6297607421875,
      "learning_rate": 1e-05,
      "loss": 7.3777,
      "step": 7020
    },
    {
      "epoch": 0.3804878048780488,
      "step": 7020,
      "training_loss": 7.326518535614014
    },
    {
      "epoch": 0.3805420054200542,
      "step": 7021,
      "training_loss": 7.080309867858887
    },
    {
      "epoch": 0.3805962059620596,
      "step": 7022,
      "training_loss": 6.590254306793213
    },
    {
      "epoch": 0.38065040650406506,
      "step": 7023,
      "training_loss": 6.783657550811768
    },
    {
      "epoch": 0.38070460704607045,
      "grad_norm": 19.218843460083008,
      "learning_rate": 1e-05,
      "loss": 6.9452,
      "step": 7024
    },
    {
      "epoch": 0.38070460704607045,
      "step": 7024,
      "training_loss": 7.167970180511475
    },
    {
      "epoch": 0.3807588075880759,
      "step": 7025,
      "training_loss": 6.984647274017334
    },
    {
      "epoch": 0.3808130081300813,
      "step": 7026,
      "training_loss": 7.39645528793335
    },
    {
      "epoch": 0.38086720867208673,
      "step": 7027,
      "training_loss": 7.023360252380371
    },
    {
      "epoch": 0.3809214092140921,
      "grad_norm": 20.75128936767578,
      "learning_rate": 1e-05,
      "loss": 7.1431,
      "step": 7028
    },
    {
      "epoch": 0.3809214092140921,
      "step": 7028,
      "training_loss": 8.100582122802734
    },
    {
      "epoch": 0.38097560975609757,
      "step": 7029,
      "training_loss": 3.9163806438446045
    },
    {
      "epoch": 0.38102981029810296,
      "step": 7030,
      "training_loss": 6.983893871307373
    },
    {
      "epoch": 0.3810840108401084,
      "step": 7031,
      "training_loss": 6.429473876953125
    },
    {
      "epoch": 0.38113821138211385,
      "grad_norm": 36.644187927246094,
      "learning_rate": 1e-05,
      "loss": 6.3576,
      "step": 7032
    },
    {
      "epoch": 0.38113821138211385,
      "step": 7032,
      "training_loss": 7.713173866271973
    },
    {
      "epoch": 0.38119241192411923,
      "step": 7033,
      "training_loss": 6.461673736572266
    },
    {
      "epoch": 0.3812466124661247,
      "step": 7034,
      "training_loss": 6.545942306518555
    },
    {
      "epoch": 0.38130081300813007,
      "step": 7035,
      "training_loss": 5.925700664520264
    },
    {
      "epoch": 0.3813550135501355,
      "grad_norm": 62.06793975830078,
      "learning_rate": 1e-05,
      "loss": 6.6616,
      "step": 7036
    },
    {
      "epoch": 0.3813550135501355,
      "step": 7036,
      "training_loss": 6.793792247772217
    },
    {
      "epoch": 0.3814092140921409,
      "step": 7037,
      "training_loss": 7.787769794464111
    },
    {
      "epoch": 0.38146341463414635,
      "step": 7038,
      "training_loss": 7.887321949005127
    },
    {
      "epoch": 0.38151761517615174,
      "step": 7039,
      "training_loss": 6.972893714904785
    },
    {
      "epoch": 0.3815718157181572,
      "grad_norm": 23.081924438476562,
      "learning_rate": 1e-05,
      "loss": 7.3604,
      "step": 7040
    },
    {
      "epoch": 0.3815718157181572,
      "step": 7040,
      "training_loss": 9.51031494140625
    },
    {
      "epoch": 0.3816260162601626,
      "step": 7041,
      "training_loss": 7.893771171569824
    },
    {
      "epoch": 0.381680216802168,
      "step": 7042,
      "training_loss": 6.915109634399414
    },
    {
      "epoch": 0.38173441734417346,
      "step": 7043,
      "training_loss": 6.83690881729126
    },
    {
      "epoch": 0.38178861788617885,
      "grad_norm": 18.65372085571289,
      "learning_rate": 1e-05,
      "loss": 7.789,
      "step": 7044
    },
    {
      "epoch": 0.38178861788617885,
      "step": 7044,
      "training_loss": 5.353602409362793
    },
    {
      "epoch": 0.3818428184281843,
      "step": 7045,
      "training_loss": 6.347422122955322
    },
    {
      "epoch": 0.3818970189701897,
      "step": 7046,
      "training_loss": 7.765645980834961
    },
    {
      "epoch": 0.38195121951219513,
      "step": 7047,
      "training_loss": 7.1378021240234375
    },
    {
      "epoch": 0.3820054200542005,
      "grad_norm": 28.868698120117188,
      "learning_rate": 1e-05,
      "loss": 6.6511,
      "step": 7048
    },
    {
      "epoch": 0.3820054200542005,
      "step": 7048,
      "training_loss": 6.761176109313965
    },
    {
      "epoch": 0.38205962059620596,
      "step": 7049,
      "training_loss": 7.318777084350586
    },
    {
      "epoch": 0.3821138211382114,
      "step": 7050,
      "training_loss": 6.04600715637207
    },
    {
      "epoch": 0.3821680216802168,
      "step": 7051,
      "training_loss": 8.267666816711426
    },
    {
      "epoch": 0.38222222222222224,
      "grad_norm": 37.791378021240234,
      "learning_rate": 1e-05,
      "loss": 7.0984,
      "step": 7052
    },
    {
      "epoch": 0.38222222222222224,
      "step": 7052,
      "training_loss": 7.575191497802734
    },
    {
      "epoch": 0.38227642276422763,
      "step": 7053,
      "training_loss": 8.199450492858887
    },
    {
      "epoch": 0.3823306233062331,
      "step": 7054,
      "training_loss": 8.266864776611328
    },
    {
      "epoch": 0.38238482384823846,
      "step": 7055,
      "training_loss": 6.2708258628845215
    },
    {
      "epoch": 0.3824390243902439,
      "grad_norm": 27.27001953125,
      "learning_rate": 1e-05,
      "loss": 7.5781,
      "step": 7056
    },
    {
      "epoch": 0.3824390243902439,
      "step": 7056,
      "training_loss": 6.888470649719238
    },
    {
      "epoch": 0.3824932249322493,
      "step": 7057,
      "training_loss": 6.8533616065979
    },
    {
      "epoch": 0.38254742547425474,
      "step": 7058,
      "training_loss": 7.567269802093506
    },
    {
      "epoch": 0.3826016260162602,
      "step": 7059,
      "training_loss": 4.1193013191223145
    },
    {
      "epoch": 0.3826558265582656,
      "grad_norm": 29.250009536743164,
      "learning_rate": 1e-05,
      "loss": 6.3571,
      "step": 7060
    },
    {
      "epoch": 0.3826558265582656,
      "step": 7060,
      "training_loss": 8.55776309967041
    },
    {
      "epoch": 0.382710027100271,
      "step": 7061,
      "training_loss": 8.104905128479004
    },
    {
      "epoch": 0.3827642276422764,
      "step": 7062,
      "training_loss": 3.6460318565368652
    },
    {
      "epoch": 0.38281842818428186,
      "step": 7063,
      "training_loss": 5.640207767486572
    },
    {
      "epoch": 0.38287262872628725,
      "grad_norm": 29.779216766357422,
      "learning_rate": 1e-05,
      "loss": 6.4872,
      "step": 7064
    },
    {
      "epoch": 0.38287262872628725,
      "step": 7064,
      "training_loss": 6.90346622467041
    },
    {
      "epoch": 0.3829268292682927,
      "step": 7065,
      "training_loss": 7.562420845031738
    },
    {
      "epoch": 0.3829810298102981,
      "step": 7066,
      "training_loss": 6.635329723358154
    },
    {
      "epoch": 0.3830352303523035,
      "step": 7067,
      "training_loss": 6.6666717529296875
    },
    {
      "epoch": 0.38308943089430897,
      "grad_norm": 20.099647521972656,
      "learning_rate": 1e-05,
      "loss": 6.942,
      "step": 7068
    },
    {
      "epoch": 0.38308943089430897,
      "step": 7068,
      "training_loss": 6.324173450469971
    },
    {
      "epoch": 0.38314363143631436,
      "step": 7069,
      "training_loss": 7.1776227951049805
    },
    {
      "epoch": 0.3831978319783198,
      "step": 7070,
      "training_loss": 6.88174295425415
    },
    {
      "epoch": 0.3832520325203252,
      "step": 7071,
      "training_loss": 6.654806613922119
    },
    {
      "epoch": 0.38330623306233064,
      "grad_norm": 25.1617488861084,
      "learning_rate": 1e-05,
      "loss": 6.7596,
      "step": 7072
    },
    {
      "epoch": 0.38330623306233064,
      "step": 7072,
      "training_loss": 7.823658466339111
    },
    {
      "epoch": 0.383360433604336,
      "step": 7073,
      "training_loss": 6.415987968444824
    },
    {
      "epoch": 0.38341463414634147,
      "step": 7074,
      "training_loss": 6.592568874359131
    },
    {
      "epoch": 0.38346883468834686,
      "step": 7075,
      "training_loss": 6.367554664611816
    },
    {
      "epoch": 0.3835230352303523,
      "grad_norm": 21.560293197631836,
      "learning_rate": 1e-05,
      "loss": 6.7999,
      "step": 7076
    },
    {
      "epoch": 0.3835230352303523,
      "step": 7076,
      "training_loss": 5.465493202209473
    },
    {
      "epoch": 0.38357723577235775,
      "step": 7077,
      "training_loss": 8.197659492492676
    },
    {
      "epoch": 0.38363143631436314,
      "step": 7078,
      "training_loss": 7.104294776916504
    },
    {
      "epoch": 0.3836856368563686,
      "step": 7079,
      "training_loss": 8.175268173217773
    },
    {
      "epoch": 0.383739837398374,
      "grad_norm": 41.79127502441406,
      "learning_rate": 1e-05,
      "loss": 7.2357,
      "step": 7080
    },
    {
      "epoch": 0.383739837398374,
      "step": 7080,
      "training_loss": 7.196562767028809
    },
    {
      "epoch": 0.3837940379403794,
      "step": 7081,
      "training_loss": 7.050652027130127
    },
    {
      "epoch": 0.3838482384823848,
      "step": 7082,
      "training_loss": 5.047696113586426
    },
    {
      "epoch": 0.38390243902439025,
      "step": 7083,
      "training_loss": 5.769705772399902
    },
    {
      "epoch": 0.38395663956639564,
      "grad_norm": 29.4522762298584,
      "learning_rate": 1e-05,
      "loss": 6.2662,
      "step": 7084
    },
    {
      "epoch": 0.38395663956639564,
      "step": 7084,
      "training_loss": 4.7792863845825195
    },
    {
      "epoch": 0.3840108401084011,
      "step": 7085,
      "training_loss": 6.5062994956970215
    },
    {
      "epoch": 0.38406504065040653,
      "step": 7086,
      "training_loss": 7.165081024169922
    },
    {
      "epoch": 0.3841192411924119,
      "step": 7087,
      "training_loss": 7.759383201599121
    },
    {
      "epoch": 0.38417344173441736,
      "grad_norm": 33.67479705810547,
      "learning_rate": 1e-05,
      "loss": 6.5525,
      "step": 7088
    },
    {
      "epoch": 0.38417344173441736,
      "step": 7088,
      "training_loss": 8.302288055419922
    },
    {
      "epoch": 0.38422764227642275,
      "step": 7089,
      "training_loss": 6.962663173675537
    },
    {
      "epoch": 0.3842818428184282,
      "step": 7090,
      "training_loss": 6.256582736968994
    },
    {
      "epoch": 0.3843360433604336,
      "step": 7091,
      "training_loss": 4.256584167480469
    },
    {
      "epoch": 0.38439024390243903,
      "grad_norm": 23.458147048950195,
      "learning_rate": 1e-05,
      "loss": 6.4445,
      "step": 7092
    },
    {
      "epoch": 0.38439024390243903,
      "step": 7092,
      "training_loss": 6.977562427520752
    },
    {
      "epoch": 0.3844444444444444,
      "step": 7093,
      "training_loss": 5.416940212249756
    },
    {
      "epoch": 0.38449864498644987,
      "step": 7094,
      "training_loss": 7.115813255310059
    },
    {
      "epoch": 0.3845528455284553,
      "step": 7095,
      "training_loss": 5.8627610206604
    },
    {
      "epoch": 0.3846070460704607,
      "grad_norm": 30.01268196105957,
      "learning_rate": 1e-05,
      "loss": 6.3433,
      "step": 7096
    },
    {
      "epoch": 0.3846070460704607,
      "step": 7096,
      "training_loss": 6.83416223526001
    },
    {
      "epoch": 0.38466124661246615,
      "step": 7097,
      "training_loss": 5.741281986236572
    },
    {
      "epoch": 0.38471544715447153,
      "step": 7098,
      "training_loss": 7.141754150390625
    },
    {
      "epoch": 0.384769647696477,
      "step": 7099,
      "training_loss": 5.847227573394775
    },
    {
      "epoch": 0.38482384823848237,
      "grad_norm": 25.33218002319336,
      "learning_rate": 1e-05,
      "loss": 6.3911,
      "step": 7100
    },
    {
      "epoch": 0.38482384823848237,
      "step": 7100,
      "training_loss": 7.708542346954346
    },
    {
      "epoch": 0.3848780487804878,
      "step": 7101,
      "training_loss": 7.526235580444336
    },
    {
      "epoch": 0.3849322493224932,
      "step": 7102,
      "training_loss": 6.5382490158081055
    },
    {
      "epoch": 0.38498644986449865,
      "step": 7103,
      "training_loss": 6.273085594177246
    },
    {
      "epoch": 0.3850406504065041,
      "grad_norm": 23.974437713623047,
      "learning_rate": 1e-05,
      "loss": 7.0115,
      "step": 7104
    },
    {
      "epoch": 0.3850406504065041,
      "step": 7104,
      "training_loss": 5.990479946136475
    },
    {
      "epoch": 0.3850948509485095,
      "step": 7105,
      "training_loss": 7.216933250427246
    },
    {
      "epoch": 0.3851490514905149,
      "step": 7106,
      "training_loss": 7.141512870788574
    },
    {
      "epoch": 0.3852032520325203,
      "step": 7107,
      "training_loss": 7.3510565757751465
    },
    {
      "epoch": 0.38525745257452576,
      "grad_norm": 35.274810791015625,
      "learning_rate": 1e-05,
      "loss": 6.925,
      "step": 7108
    },
    {
      "epoch": 0.38525745257452576,
      "step": 7108,
      "training_loss": 6.773981094360352
    },
    {
      "epoch": 0.38531165311653115,
      "step": 7109,
      "training_loss": 6.099858283996582
    },
    {
      "epoch": 0.3853658536585366,
      "step": 7110,
      "training_loss": 5.826756000518799
    },
    {
      "epoch": 0.385420054200542,
      "step": 7111,
      "training_loss": 7.819217205047607
    },
    {
      "epoch": 0.38547425474254743,
      "grad_norm": 50.94823455810547,
      "learning_rate": 1e-05,
      "loss": 6.63,
      "step": 7112
    },
    {
      "epoch": 0.38547425474254743,
      "step": 7112,
      "training_loss": 6.3717193603515625
    },
    {
      "epoch": 0.3855284552845528,
      "step": 7113,
      "training_loss": 6.3132524490356445
    },
    {
      "epoch": 0.38558265582655826,
      "step": 7114,
      "training_loss": 6.13682746887207
    },
    {
      "epoch": 0.3856368563685637,
      "step": 7115,
      "training_loss": 4.867013931274414
    },
    {
      "epoch": 0.3856910569105691,
      "grad_norm": 27.019866943359375,
      "learning_rate": 1e-05,
      "loss": 5.9222,
      "step": 7116
    },
    {
      "epoch": 0.3856910569105691,
      "step": 7116,
      "training_loss": 7.737082004547119
    },
    {
      "epoch": 0.38574525745257454,
      "step": 7117,
      "training_loss": 7.859206199645996
    },
    {
      "epoch": 0.38579945799457993,
      "step": 7118,
      "training_loss": 6.267508029937744
    },
    {
      "epoch": 0.3858536585365854,
      "step": 7119,
      "training_loss": 5.819750785827637
    },
    {
      "epoch": 0.38590785907859076,
      "grad_norm": 25.674495697021484,
      "learning_rate": 1e-05,
      "loss": 6.9209,
      "step": 7120
    },
    {
      "epoch": 0.38590785907859076,
      "step": 7120,
      "training_loss": 6.302713871002197
    },
    {
      "epoch": 0.3859620596205962,
      "step": 7121,
      "training_loss": 6.5911407470703125
    },
    {
      "epoch": 0.3860162601626016,
      "step": 7122,
      "training_loss": 6.508857250213623
    },
    {
      "epoch": 0.38607046070460704,
      "step": 7123,
      "training_loss": 7.679287433624268
    },
    {
      "epoch": 0.3861246612466125,
      "grad_norm": 37.010318756103516,
      "learning_rate": 1e-05,
      "loss": 6.7705,
      "step": 7124
    },
    {
      "epoch": 0.3861246612466125,
      "step": 7124,
      "training_loss": 6.628364086151123
    },
    {
      "epoch": 0.3861788617886179,
      "step": 7125,
      "training_loss": 7.396568775177002
    },
    {
      "epoch": 0.3862330623306233,
      "step": 7126,
      "training_loss": 3.566180944442749
    },
    {
      "epoch": 0.3862872628726287,
      "step": 7127,
      "training_loss": 6.2705817222595215
    },
    {
      "epoch": 0.38634146341463416,
      "grad_norm": 21.087549209594727,
      "learning_rate": 1e-05,
      "loss": 5.9654,
      "step": 7128
    },
    {
      "epoch": 0.38634146341463416,
      "step": 7128,
      "training_loss": 5.352570056915283
    },
    {
      "epoch": 0.38639566395663955,
      "step": 7129,
      "training_loss": 7.066267013549805
    },
    {
      "epoch": 0.386449864498645,
      "step": 7130,
      "training_loss": 7.076781272888184
    },
    {
      "epoch": 0.3865040650406504,
      "step": 7131,
      "training_loss": 5.623025417327881
    },
    {
      "epoch": 0.3865582655826558,
      "grad_norm": 24.84597396850586,
      "learning_rate": 1e-05,
      "loss": 6.2797,
      "step": 7132
    },
    {
      "epoch": 0.3865582655826558,
      "step": 7132,
      "training_loss": 6.652166843414307
    },
    {
      "epoch": 0.38661246612466127,
      "step": 7133,
      "training_loss": 7.703428268432617
    },
    {
      "epoch": 0.38666666666666666,
      "step": 7134,
      "training_loss": 4.070091724395752
    },
    {
      "epoch": 0.3867208672086721,
      "step": 7135,
      "training_loss": 7.266104221343994
    },
    {
      "epoch": 0.3867750677506775,
      "grad_norm": 30.040740966796875,
      "learning_rate": 1e-05,
      "loss": 6.4229,
      "step": 7136
    },
    {
      "epoch": 0.3867750677506775,
      "step": 7136,
      "training_loss": 6.819209575653076
    },
    {
      "epoch": 0.38682926829268294,
      "step": 7137,
      "training_loss": 7.510324954986572
    },
    {
      "epoch": 0.3868834688346883,
      "step": 7138,
      "training_loss": 6.822201251983643
    },
    {
      "epoch": 0.38693766937669377,
      "step": 7139,
      "training_loss": 6.81137752532959
    },
    {
      "epoch": 0.38699186991869916,
      "grad_norm": 20.07114028930664,
      "learning_rate": 1e-05,
      "loss": 6.9908,
      "step": 7140
    },
    {
      "epoch": 0.38699186991869916,
      "step": 7140,
      "training_loss": 6.814160346984863
    },
    {
      "epoch": 0.3870460704607046,
      "step": 7141,
      "training_loss": 6.439782619476318
    },
    {
      "epoch": 0.38710027100271005,
      "step": 7142,
      "training_loss": 7.413494110107422
    },
    {
      "epoch": 0.38715447154471544,
      "step": 7143,
      "training_loss": 7.8104448318481445
    },
    {
      "epoch": 0.3872086720867209,
      "grad_norm": 22.67823028564453,
      "learning_rate": 1e-05,
      "loss": 7.1195,
      "step": 7144
    },
    {
      "epoch": 0.3872086720867209,
      "step": 7144,
      "training_loss": 7.116803169250488
    },
    {
      "epoch": 0.3872628726287263,
      "step": 7145,
      "training_loss": 7.5484819412231445
    },
    {
      "epoch": 0.3873170731707317,
      "step": 7146,
      "training_loss": 7.248539924621582
    },
    {
      "epoch": 0.3873712737127371,
      "step": 7147,
      "training_loss": 7.524375915527344
    },
    {
      "epoch": 0.38742547425474255,
      "grad_norm": 30.859834671020508,
      "learning_rate": 1e-05,
      "loss": 7.3596,
      "step": 7148
    },
    {
      "epoch": 0.38742547425474255,
      "step": 7148,
      "training_loss": 6.697455883026123
    },
    {
      "epoch": 0.38747967479674794,
      "step": 7149,
      "training_loss": 4.546640872955322
    },
    {
      "epoch": 0.3875338753387534,
      "step": 7150,
      "training_loss": 3.83514404296875
    },
    {
      "epoch": 0.38758807588075883,
      "step": 7151,
      "training_loss": 6.55875301361084
    },
    {
      "epoch": 0.3876422764227642,
      "grad_norm": 26.476627349853516,
      "learning_rate": 1e-05,
      "loss": 5.4095,
      "step": 7152
    },
    {
      "epoch": 0.3876422764227642,
      "step": 7152,
      "training_loss": 7.069206714630127
    },
    {
      "epoch": 0.38769647696476967,
      "step": 7153,
      "training_loss": 6.985457420349121
    },
    {
      "epoch": 0.38775067750677505,
      "step": 7154,
      "training_loss": 5.840420722961426
    },
    {
      "epoch": 0.3878048780487805,
      "step": 7155,
      "training_loss": 5.005209445953369
    },
    {
      "epoch": 0.3878590785907859,
      "grad_norm": 21.148929595947266,
      "learning_rate": 1e-05,
      "loss": 6.2251,
      "step": 7156
    },
    {
      "epoch": 0.3878590785907859,
      "step": 7156,
      "training_loss": 6.490980625152588
    },
    {
      "epoch": 0.38791327913279133,
      "step": 7157,
      "training_loss": 7.995773792266846
    },
    {
      "epoch": 0.3879674796747967,
      "step": 7158,
      "training_loss": 5.3172783851623535
    },
    {
      "epoch": 0.38802168021680217,
      "step": 7159,
      "training_loss": 7.878077507019043
    },
    {
      "epoch": 0.3880758807588076,
      "grad_norm": 72.77122497558594,
      "learning_rate": 1e-05,
      "loss": 6.9205,
      "step": 7160
    },
    {
      "epoch": 0.3880758807588076,
      "step": 7160,
      "training_loss": 6.004993438720703
    },
    {
      "epoch": 0.388130081300813,
      "step": 7161,
      "training_loss": 6.582783222198486
    },
    {
      "epoch": 0.38818428184281845,
      "step": 7162,
      "training_loss": 8.035286903381348
    },
    {
      "epoch": 0.38823848238482384,
      "step": 7163,
      "training_loss": 5.9724202156066895
    },
    {
      "epoch": 0.3882926829268293,
      "grad_norm": 25.0375919342041,
      "learning_rate": 1e-05,
      "loss": 6.6489,
      "step": 7164
    },
    {
      "epoch": 0.3882926829268293,
      "step": 7164,
      "training_loss": 6.46744966506958
    },
    {
      "epoch": 0.38834688346883467,
      "step": 7165,
      "training_loss": 6.625386714935303
    },
    {
      "epoch": 0.3884010840108401,
      "step": 7166,
      "training_loss": 7.119786262512207
    },
    {
      "epoch": 0.3884552845528455,
      "step": 7167,
      "training_loss": 6.090149879455566
    },
    {
      "epoch": 0.38850948509485095,
      "grad_norm": 41.773136138916016,
      "learning_rate": 1e-05,
      "loss": 6.5757,
      "step": 7168
    },
    {
      "epoch": 0.38850948509485095,
      "step": 7168,
      "training_loss": 6.895901679992676
    },
    {
      "epoch": 0.3885636856368564,
      "step": 7169,
      "training_loss": 7.377534866333008
    },
    {
      "epoch": 0.3886178861788618,
      "step": 7170,
      "training_loss": 5.847145080566406
    },
    {
      "epoch": 0.3886720867208672,
      "step": 7171,
      "training_loss": 6.945122718811035
    },
    {
      "epoch": 0.3887262872628726,
      "grad_norm": 43.454071044921875,
      "learning_rate": 1e-05,
      "loss": 6.7664,
      "step": 7172
    },
    {
      "epoch": 0.3887262872628726,
      "step": 7172,
      "training_loss": 6.568477153778076
    },
    {
      "epoch": 0.38878048780487806,
      "step": 7173,
      "training_loss": 6.198281288146973
    },
    {
      "epoch": 0.38883468834688345,
      "step": 7174,
      "training_loss": 5.458138942718506
    },
    {
      "epoch": 0.3888888888888889,
      "step": 7175,
      "training_loss": 6.990591526031494
    },
    {
      "epoch": 0.3889430894308943,
      "grad_norm": 37.78620147705078,
      "learning_rate": 1e-05,
      "loss": 6.3039,
      "step": 7176
    },
    {
      "epoch": 0.3889430894308943,
      "step": 7176,
      "training_loss": 7.640005111694336
    },
    {
      "epoch": 0.38899728997289973,
      "step": 7177,
      "training_loss": 8.182488441467285
    },
    {
      "epoch": 0.3890514905149052,
      "step": 7178,
      "training_loss": 6.071622371673584
    },
    {
      "epoch": 0.38910569105691056,
      "step": 7179,
      "training_loss": 4.285754680633545
    },
    {
      "epoch": 0.389159891598916,
      "grad_norm": 22.143321990966797,
      "learning_rate": 1e-05,
      "loss": 6.545,
      "step": 7180
    },
    {
      "epoch": 0.389159891598916,
      "step": 7180,
      "training_loss": 6.829681873321533
    },
    {
      "epoch": 0.3892140921409214,
      "step": 7181,
      "training_loss": 3.704315423965454
    },
    {
      "epoch": 0.38926829268292684,
      "step": 7182,
      "training_loss": 7.3520426750183105
    },
    {
      "epoch": 0.38932249322493223,
      "step": 7183,
      "training_loss": 5.984243392944336
    },
    {
      "epoch": 0.3893766937669377,
      "grad_norm": 32.94315719604492,
      "learning_rate": 1e-05,
      "loss": 5.9676,
      "step": 7184
    },
    {
      "epoch": 0.3893766937669377,
      "step": 7184,
      "training_loss": 4.721563816070557
    },
    {
      "epoch": 0.38943089430894307,
      "step": 7185,
      "training_loss": 6.792606830596924
    },
    {
      "epoch": 0.3894850948509485,
      "step": 7186,
      "training_loss": 6.720722198486328
    },
    {
      "epoch": 0.38953929539295395,
      "step": 7187,
      "training_loss": 7.143580913543701
    },
    {
      "epoch": 0.38959349593495934,
      "grad_norm": 17.131990432739258,
      "learning_rate": 1e-05,
      "loss": 6.3446,
      "step": 7188
    },
    {
      "epoch": 0.38959349593495934,
      "step": 7188,
      "training_loss": 5.108522415161133
    },
    {
      "epoch": 0.3896476964769648,
      "step": 7189,
      "training_loss": 7.1016058921813965
    },
    {
      "epoch": 0.3897018970189702,
      "step": 7190,
      "training_loss": 6.99229621887207
    },
    {
      "epoch": 0.3897560975609756,
      "step": 7191,
      "training_loss": 7.112305164337158
    },
    {
      "epoch": 0.389810298102981,
      "grad_norm": 16.230833053588867,
      "learning_rate": 1e-05,
      "loss": 6.5787,
      "step": 7192
    },
    {
      "epoch": 0.389810298102981,
      "step": 7192,
      "training_loss": 6.8870930671691895
    },
    {
      "epoch": 0.38986449864498646,
      "step": 7193,
      "training_loss": 7.065417289733887
    },
    {
      "epoch": 0.38991869918699185,
      "step": 7194,
      "training_loss": 6.198197364807129
    },
    {
      "epoch": 0.3899728997289973,
      "step": 7195,
      "training_loss": 6.249063968658447
    },
    {
      "epoch": 0.39002710027100274,
      "grad_norm": 77.17227935791016,
      "learning_rate": 1e-05,
      "loss": 6.5999,
      "step": 7196
    },
    {
      "epoch": 0.39002710027100274,
      "step": 7196,
      "training_loss": 5.3426666259765625
    },
    {
      "epoch": 0.3900813008130081,
      "step": 7197,
      "training_loss": 6.1127543449401855
    },
    {
      "epoch": 0.39013550135501357,
      "step": 7198,
      "training_loss": 7.192173004150391
    },
    {
      "epoch": 0.39018970189701896,
      "step": 7199,
      "training_loss": 5.544871807098389
    },
    {
      "epoch": 0.3902439024390244,
      "grad_norm": 24.7218017578125,
      "learning_rate": 1e-05,
      "loss": 6.0481,
      "step": 7200
    },
    {
      "epoch": 0.3902439024390244,
      "step": 7200,
      "training_loss": 9.453819274902344
    },
    {
      "epoch": 0.3902981029810298,
      "step": 7201,
      "training_loss": 6.18978214263916
    },
    {
      "epoch": 0.39035230352303524,
      "step": 7202,
      "training_loss": 7.173003196716309
    },
    {
      "epoch": 0.3904065040650406,
      "step": 7203,
      "training_loss": 5.796618461608887
    },
    {
      "epoch": 0.39046070460704607,
      "grad_norm": 30.9619197845459,
      "learning_rate": 1e-05,
      "loss": 7.1533,
      "step": 7204
    },
    {
      "epoch": 0.39046070460704607,
      "step": 7204,
      "training_loss": 5.957486629486084
    },
    {
      "epoch": 0.3905149051490515,
      "step": 7205,
      "training_loss": 7.013499736785889
    },
    {
      "epoch": 0.3905691056910569,
      "step": 7206,
      "training_loss": 6.1304097175598145
    },
    {
      "epoch": 0.39062330623306235,
      "step": 7207,
      "training_loss": 6.570291042327881
    },
    {
      "epoch": 0.39067750677506774,
      "grad_norm": 26.175399780273438,
      "learning_rate": 1e-05,
      "loss": 6.4179,
      "step": 7208
    },
    {
      "epoch": 0.39067750677506774,
      "step": 7208,
      "training_loss": 7.06631326675415
    },
    {
      "epoch": 0.3907317073170732,
      "step": 7209,
      "training_loss": 6.449318885803223
    },
    {
      "epoch": 0.3907859078590786,
      "step": 7210,
      "training_loss": 3.528834104537964
    },
    {
      "epoch": 0.390840108401084,
      "step": 7211,
      "training_loss": 3.9679970741271973
    },
    {
      "epoch": 0.3908943089430894,
      "grad_norm": 27.135347366333008,
      "learning_rate": 1e-05,
      "loss": 5.2531,
      "step": 7212
    },
    {
      "epoch": 0.3908943089430894,
      "step": 7212,
      "training_loss": 6.844478607177734
    },
    {
      "epoch": 0.39094850948509485,
      "step": 7213,
      "training_loss": 4.188588619232178
    },
    {
      "epoch": 0.3910027100271003,
      "step": 7214,
      "training_loss": 7.214710235595703
    },
    {
      "epoch": 0.3910569105691057,
      "step": 7215,
      "training_loss": 6.626529216766357
    },
    {
      "epoch": 0.39111111111111113,
      "grad_norm": 25.24302864074707,
      "learning_rate": 1e-05,
      "loss": 6.2186,
      "step": 7216
    },
    {
      "epoch": 0.39111111111111113,
      "step": 7216,
      "training_loss": 5.598705291748047
    },
    {
      "epoch": 0.3911653116531165,
      "step": 7217,
      "training_loss": 6.993618488311768
    },
    {
      "epoch": 0.39121951219512197,
      "step": 7218,
      "training_loss": 6.189815998077393
    },
    {
      "epoch": 0.39127371273712735,
      "step": 7219,
      "training_loss": 5.92515754699707
    },
    {
      "epoch": 0.3913279132791328,
      "grad_norm": 27.367774963378906,
      "learning_rate": 1e-05,
      "loss": 6.1768,
      "step": 7220
    },
    {
      "epoch": 0.3913279132791328,
      "step": 7220,
      "training_loss": 7.392482280731201
    },
    {
      "epoch": 0.3913821138211382,
      "step": 7221,
      "training_loss": 8.238670349121094
    },
    {
      "epoch": 0.39143631436314363,
      "step": 7222,
      "training_loss": 7.06462287902832
    },
    {
      "epoch": 0.3914905149051491,
      "step": 7223,
      "training_loss": 7.397127628326416
    },
    {
      "epoch": 0.39154471544715447,
      "grad_norm": 17.120445251464844,
      "learning_rate": 1e-05,
      "loss": 7.5232,
      "step": 7224
    },
    {
      "epoch": 0.39154471544715447,
      "step": 7224,
      "training_loss": 6.983788967132568
    },
    {
      "epoch": 0.3915989159891599,
      "step": 7225,
      "training_loss": 6.674283981323242
    },
    {
      "epoch": 0.3916531165311653,
      "step": 7226,
      "training_loss": 7.19673490524292
    },
    {
      "epoch": 0.39170731707317075,
      "step": 7227,
      "training_loss": 6.918562889099121
    },
    {
      "epoch": 0.39176151761517614,
      "grad_norm": 36.7235107421875,
      "learning_rate": 1e-05,
      "loss": 6.9433,
      "step": 7228
    },
    {
      "epoch": 0.39176151761517614,
      "step": 7228,
      "training_loss": 4.583689212799072
    },
    {
      "epoch": 0.3918157181571816,
      "step": 7229,
      "training_loss": 5.335278034210205
    },
    {
      "epoch": 0.39186991869918697,
      "step": 7230,
      "training_loss": 6.767180442810059
    },
    {
      "epoch": 0.3919241192411924,
      "step": 7231,
      "training_loss": 4.521335601806641
    },
    {
      "epoch": 0.39197831978319786,
      "grad_norm": 33.95069122314453,
      "learning_rate": 1e-05,
      "loss": 5.3019,
      "step": 7232
    },
    {
      "epoch": 0.39197831978319786,
      "step": 7232,
      "training_loss": 5.326591968536377
    },
    {
      "epoch": 0.39203252032520325,
      "step": 7233,
      "training_loss": 8.124847412109375
    },
    {
      "epoch": 0.3920867208672087,
      "step": 7234,
      "training_loss": 7.016376495361328
    },
    {
      "epoch": 0.3921409214092141,
      "step": 7235,
      "training_loss": 5.946435928344727
    },
    {
      "epoch": 0.3921951219512195,
      "grad_norm": 23.922719955444336,
      "learning_rate": 1e-05,
      "loss": 6.6036,
      "step": 7236
    },
    {
      "epoch": 0.3921951219512195,
      "step": 7236,
      "training_loss": 4.291472911834717
    },
    {
      "epoch": 0.3922493224932249,
      "step": 7237,
      "training_loss": 6.269893646240234
    },
    {
      "epoch": 0.39230352303523036,
      "step": 7238,
      "training_loss": 6.298527717590332
    },
    {
      "epoch": 0.39235772357723575,
      "step": 7239,
      "training_loss": 5.088912487030029
    },
    {
      "epoch": 0.3924119241192412,
      "grad_norm": 26.979183197021484,
      "learning_rate": 1e-05,
      "loss": 5.4872,
      "step": 7240
    },
    {
      "epoch": 0.3924119241192412,
      "step": 7240,
      "training_loss": 7.033139228820801
    },
    {
      "epoch": 0.3924661246612466,
      "step": 7241,
      "training_loss": 7.724609375
    },
    {
      "epoch": 0.39252032520325203,
      "step": 7242,
      "training_loss": 7.436332702636719
    },
    {
      "epoch": 0.3925745257452575,
      "step": 7243,
      "training_loss": 7.669039726257324
    },
    {
      "epoch": 0.39262872628726286,
      "grad_norm": 33.29457473754883,
      "learning_rate": 1e-05,
      "loss": 7.4658,
      "step": 7244
    },
    {
      "epoch": 0.39262872628726286,
      "step": 7244,
      "training_loss": 6.7587432861328125
    },
    {
      "epoch": 0.3926829268292683,
      "step": 7245,
      "training_loss": 5.71501350402832
    },
    {
      "epoch": 0.3927371273712737,
      "step": 7246,
      "training_loss": 6.241184711456299
    },
    {
      "epoch": 0.39279132791327914,
      "step": 7247,
      "training_loss": 7.172502040863037
    },
    {
      "epoch": 0.39284552845528453,
      "grad_norm": 36.7473258972168,
      "learning_rate": 1e-05,
      "loss": 6.4719,
      "step": 7248
    },
    {
      "epoch": 0.39284552845528453,
      "step": 7248,
      "training_loss": 6.405222415924072
    },
    {
      "epoch": 0.39289972899729,
      "step": 7249,
      "training_loss": 6.823419570922852
    },
    {
      "epoch": 0.39295392953929537,
      "step": 7250,
      "training_loss": 5.7808756828308105
    },
    {
      "epoch": 0.3930081300813008,
      "step": 7251,
      "training_loss": 4.9163641929626465
    },
    {
      "epoch": 0.39306233062330626,
      "grad_norm": 26.109453201293945,
      "learning_rate": 1e-05,
      "loss": 5.9815,
      "step": 7252
    },
    {
      "epoch": 0.39306233062330626,
      "step": 7252,
      "training_loss": 3.8838093280792236
    },
    {
      "epoch": 0.39311653116531164,
      "step": 7253,
      "training_loss": 7.522944927215576
    },
    {
      "epoch": 0.3931707317073171,
      "step": 7254,
      "training_loss": 6.136791706085205
    },
    {
      "epoch": 0.3932249322493225,
      "step": 7255,
      "training_loss": 6.974949359893799
    },
    {
      "epoch": 0.3932791327913279,
      "grad_norm": 20.697620391845703,
      "learning_rate": 1e-05,
      "loss": 6.1296,
      "step": 7256
    },
    {
      "epoch": 0.3932791327913279,
      "step": 7256,
      "training_loss": 6.97199821472168
    },
    {
      "epoch": 0.3933333333333333,
      "step": 7257,
      "training_loss": 7.135804653167725
    },
    {
      "epoch": 0.39338753387533876,
      "step": 7258,
      "training_loss": 7.255858421325684
    },
    {
      "epoch": 0.39344173441734415,
      "step": 7259,
      "training_loss": 7.082683086395264
    },
    {
      "epoch": 0.3934959349593496,
      "grad_norm": 25.632648468017578,
      "learning_rate": 1e-05,
      "loss": 7.1116,
      "step": 7260
    },
    {
      "epoch": 0.3934959349593496,
      "step": 7260,
      "training_loss": 6.5511274337768555
    },
    {
      "epoch": 0.39355013550135504,
      "step": 7261,
      "training_loss": 7.262543678283691
    },
    {
      "epoch": 0.3936043360433604,
      "step": 7262,
      "training_loss": 6.653410911560059
    },
    {
      "epoch": 0.39365853658536587,
      "step": 7263,
      "training_loss": 6.457334041595459
    },
    {
      "epoch": 0.39371273712737126,
      "grad_norm": 25.54412269592285,
      "learning_rate": 1e-05,
      "loss": 6.7311,
      "step": 7264
    },
    {
      "epoch": 0.39371273712737126,
      "step": 7264,
      "training_loss": 6.483891010284424
    },
    {
      "epoch": 0.3937669376693767,
      "step": 7265,
      "training_loss": 6.885862350463867
    },
    {
      "epoch": 0.3938211382113821,
      "step": 7266,
      "training_loss": 6.126129627227783
    },
    {
      "epoch": 0.39387533875338754,
      "step": 7267,
      "training_loss": 7.241342544555664
    },
    {
      "epoch": 0.3939295392953929,
      "grad_norm": 30.65334129333496,
      "learning_rate": 1e-05,
      "loss": 6.6843,
      "step": 7268
    },
    {
      "epoch": 0.3939295392953929,
      "step": 7268,
      "training_loss": 5.025345325469971
    },
    {
      "epoch": 0.3939837398373984,
      "step": 7269,
      "training_loss": 6.698938846588135
    },
    {
      "epoch": 0.3940379403794038,
      "step": 7270,
      "training_loss": 6.963595867156982
    },
    {
      "epoch": 0.3940921409214092,
      "step": 7271,
      "training_loss": 5.72355318069458
    },
    {
      "epoch": 0.39414634146341465,
      "grad_norm": 39.23273468017578,
      "learning_rate": 1e-05,
      "loss": 6.1029,
      "step": 7272
    },
    {
      "epoch": 0.39414634146341465,
      "step": 7272,
      "training_loss": 6.7022504806518555
    },
    {
      "epoch": 0.39420054200542004,
      "step": 7273,
      "training_loss": 6.737138271331787
    },
    {
      "epoch": 0.3942547425474255,
      "step": 7274,
      "training_loss": 5.322238445281982
    },
    {
      "epoch": 0.3943089430894309,
      "step": 7275,
      "training_loss": 6.842942237854004
    },
    {
      "epoch": 0.3943631436314363,
      "grad_norm": 21.921091079711914,
      "learning_rate": 1e-05,
      "loss": 6.4011,
      "step": 7276
    },
    {
      "epoch": 0.3943631436314363,
      "step": 7276,
      "training_loss": 7.191795825958252
    },
    {
      "epoch": 0.3944173441734417,
      "step": 7277,
      "training_loss": 7.238072395324707
    },
    {
      "epoch": 0.39447154471544715,
      "step": 7278,
      "training_loss": 6.669567108154297
    },
    {
      "epoch": 0.3945257452574526,
      "step": 7279,
      "training_loss": 6.9191789627075195
    },
    {
      "epoch": 0.394579945799458,
      "grad_norm": 31.638837814331055,
      "learning_rate": 1e-05,
      "loss": 7.0047,
      "step": 7280
    },
    {
      "epoch": 0.394579945799458,
      "step": 7280,
      "training_loss": 6.992020130157471
    },
    {
      "epoch": 0.39463414634146343,
      "step": 7281,
      "training_loss": 7.933511734008789
    },
    {
      "epoch": 0.3946883468834688,
      "step": 7282,
      "training_loss": 6.96674108505249
    },
    {
      "epoch": 0.39474254742547427,
      "step": 7283,
      "training_loss": 6.522523403167725
    },
    {
      "epoch": 0.39479674796747966,
      "grad_norm": 61.30598831176758,
      "learning_rate": 1e-05,
      "loss": 7.1037,
      "step": 7284
    },
    {
      "epoch": 0.39479674796747966,
      "step": 7284,
      "training_loss": 7.404857635498047
    },
    {
      "epoch": 0.3948509485094851,
      "step": 7285,
      "training_loss": 7.099971771240234
    },
    {
      "epoch": 0.3949051490514905,
      "step": 7286,
      "training_loss": 6.643045425415039
    },
    {
      "epoch": 0.39495934959349593,
      "step": 7287,
      "training_loss": 6.5434980392456055
    },
    {
      "epoch": 0.3950135501355014,
      "grad_norm": 24.670265197753906,
      "learning_rate": 1e-05,
      "loss": 6.9228,
      "step": 7288
    },
    {
      "epoch": 0.3950135501355014,
      "step": 7288,
      "training_loss": 6.121450901031494
    },
    {
      "epoch": 0.39506775067750677,
      "step": 7289,
      "training_loss": 6.402304649353027
    },
    {
      "epoch": 0.3951219512195122,
      "step": 7290,
      "training_loss": 5.614499568939209
    },
    {
      "epoch": 0.3951761517615176,
      "step": 7291,
      "training_loss": 7.06988525390625
    },
    {
      "epoch": 0.39523035230352305,
      "grad_norm": 26.73685646057129,
      "learning_rate": 1e-05,
      "loss": 6.302,
      "step": 7292
    },
    {
      "epoch": 0.39523035230352305,
      "step": 7292,
      "training_loss": 7.894477367401123
    },
    {
      "epoch": 0.39528455284552844,
      "step": 7293,
      "training_loss": 5.689913272857666
    },
    {
      "epoch": 0.3953387533875339,
      "step": 7294,
      "training_loss": 6.85118293762207
    },
    {
      "epoch": 0.39539295392953927,
      "step": 7295,
      "training_loss": 8.092354774475098
    },
    {
      "epoch": 0.3954471544715447,
      "grad_norm": 18.5408992767334,
      "learning_rate": 1e-05,
      "loss": 7.132,
      "step": 7296
    },
    {
      "epoch": 0.3954471544715447,
      "step": 7296,
      "training_loss": 6.977597236633301
    },
    {
      "epoch": 0.39550135501355016,
      "step": 7297,
      "training_loss": 6.828029632568359
    },
    {
      "epoch": 0.39555555555555555,
      "step": 7298,
      "training_loss": 6.391794681549072
    },
    {
      "epoch": 0.395609756097561,
      "step": 7299,
      "training_loss": 6.6960296630859375
    },
    {
      "epoch": 0.3956639566395664,
      "grad_norm": 27.985801696777344,
      "learning_rate": 1e-05,
      "loss": 6.7234,
      "step": 7300
    },
    {
      "epoch": 0.3956639566395664,
      "step": 7300,
      "training_loss": 7.109903335571289
    },
    {
      "epoch": 0.39571815718157183,
      "step": 7301,
      "training_loss": 8.00784969329834
    },
    {
      "epoch": 0.3957723577235772,
      "step": 7302,
      "training_loss": 6.81566047668457
    },
    {
      "epoch": 0.39582655826558266,
      "step": 7303,
      "training_loss": 7.019426345825195
    },
    {
      "epoch": 0.39588075880758805,
      "grad_norm": 17.655521392822266,
      "learning_rate": 1e-05,
      "loss": 7.2382,
      "step": 7304
    },
    {
      "epoch": 0.39588075880758805,
      "step": 7304,
      "training_loss": 4.829493045806885
    },
    {
      "epoch": 0.3959349593495935,
      "step": 7305,
      "training_loss": 7.201014041900635
    },
    {
      "epoch": 0.39598915989159894,
      "step": 7306,
      "training_loss": 6.693699836730957
    },
    {
      "epoch": 0.39604336043360433,
      "step": 7307,
      "training_loss": 6.979384899139404
    },
    {
      "epoch": 0.3960975609756098,
      "grad_norm": 25.571557998657227,
      "learning_rate": 1e-05,
      "loss": 6.4259,
      "step": 7308
    },
    {
      "epoch": 0.3960975609756098,
      "step": 7308,
      "training_loss": 4.292574882507324
    },
    {
      "epoch": 0.39615176151761516,
      "step": 7309,
      "training_loss": 7.081496715545654
    },
    {
      "epoch": 0.3962059620596206,
      "step": 7310,
      "training_loss": 5.462164878845215
    },
    {
      "epoch": 0.396260162601626,
      "step": 7311,
      "training_loss": 7.985743522644043
    },
    {
      "epoch": 0.39631436314363144,
      "grad_norm": 24.79670524597168,
      "learning_rate": 1e-05,
      "loss": 6.2055,
      "step": 7312
    },
    {
      "epoch": 0.39631436314363144,
      "step": 7312,
      "training_loss": 7.359299182891846
    },
    {
      "epoch": 0.39636856368563683,
      "step": 7313,
      "training_loss": 5.131252288818359
    },
    {
      "epoch": 0.3964227642276423,
      "step": 7314,
      "training_loss": 6.9708709716796875
    },
    {
      "epoch": 0.3964769647696477,
      "step": 7315,
      "training_loss": 7.989283084869385
    },
    {
      "epoch": 0.3965311653116531,
      "grad_norm": 31.335405349731445,
      "learning_rate": 1e-05,
      "loss": 6.8627,
      "step": 7316
    },
    {
      "epoch": 0.3965311653116531,
      "step": 7316,
      "training_loss": 3.3177218437194824
    },
    {
      "epoch": 0.39658536585365856,
      "step": 7317,
      "training_loss": 6.762340545654297
    },
    {
      "epoch": 0.39663956639566395,
      "step": 7318,
      "training_loss": 7.651504039764404
    },
    {
      "epoch": 0.3966937669376694,
      "step": 7319,
      "training_loss": 6.416579246520996
    },
    {
      "epoch": 0.3967479674796748,
      "grad_norm": 26.656461715698242,
      "learning_rate": 1e-05,
      "loss": 6.037,
      "step": 7320
    },
    {
      "epoch": 0.3967479674796748,
      "step": 7320,
      "training_loss": 6.778052806854248
    },
    {
      "epoch": 0.3968021680216802,
      "step": 7321,
      "training_loss": 7.008183002471924
    },
    {
      "epoch": 0.3968563685636856,
      "step": 7322,
      "training_loss": 6.681262969970703
    },
    {
      "epoch": 0.39691056910569106,
      "step": 7323,
      "training_loss": 4.86995267868042
    },
    {
      "epoch": 0.3969647696476965,
      "grad_norm": 22.04300880432129,
      "learning_rate": 1e-05,
      "loss": 6.3344,
      "step": 7324
    },
    {
      "epoch": 0.3969647696476965,
      "step": 7324,
      "training_loss": 7.169356346130371
    },
    {
      "epoch": 0.3970189701897019,
      "step": 7325,
      "training_loss": 7.0016679763793945
    },
    {
      "epoch": 0.39707317073170734,
      "step": 7326,
      "training_loss": 5.969143867492676
    },
    {
      "epoch": 0.3971273712737127,
      "step": 7327,
      "training_loss": 6.476751327514648
    },
    {
      "epoch": 0.39718157181571817,
      "grad_norm": 32.02535629272461,
      "learning_rate": 1e-05,
      "loss": 6.6542,
      "step": 7328
    },
    {
      "epoch": 0.39718157181571817,
      "step": 7328,
      "training_loss": 3.8866219520568848
    },
    {
      "epoch": 0.39723577235772356,
      "step": 7329,
      "training_loss": 5.749519348144531
    },
    {
      "epoch": 0.397289972899729,
      "step": 7330,
      "training_loss": 6.690897464752197
    },
    {
      "epoch": 0.3973441734417344,
      "step": 7331,
      "training_loss": 5.992074489593506
    },
    {
      "epoch": 0.39739837398373984,
      "grad_norm": 37.677635192871094,
      "learning_rate": 1e-05,
      "loss": 5.5798,
      "step": 7332
    },
    {
      "epoch": 0.39739837398373984,
      "step": 7332,
      "training_loss": 6.92940616607666
    },
    {
      "epoch": 0.3974525745257453,
      "step": 7333,
      "training_loss": 7.113687038421631
    },
    {
      "epoch": 0.3975067750677507,
      "step": 7334,
      "training_loss": 6.60088586807251
    },
    {
      "epoch": 0.3975609756097561,
      "step": 7335,
      "training_loss": 3.6883890628814697
    },
    {
      "epoch": 0.3976151761517615,
      "grad_norm": 27.29520034790039,
      "learning_rate": 1e-05,
      "loss": 6.0831,
      "step": 7336
    },
    {
      "epoch": 0.3976151761517615,
      "step": 7336,
      "training_loss": 7.269008636474609
    },
    {
      "epoch": 0.39766937669376695,
      "step": 7337,
      "training_loss": 7.3705244064331055
    },
    {
      "epoch": 0.39772357723577234,
      "step": 7338,
      "training_loss": 3.540541172027588
    },
    {
      "epoch": 0.3977777777777778,
      "step": 7339,
      "training_loss": 6.76015043258667
    },
    {
      "epoch": 0.3978319783197832,
      "grad_norm": 40.09764862060547,
      "learning_rate": 1e-05,
      "loss": 6.2351,
      "step": 7340
    },
    {
      "epoch": 0.3978319783197832,
      "step": 7340,
      "training_loss": 5.959716796875
    },
    {
      "epoch": 0.3978861788617886,
      "step": 7341,
      "training_loss": 6.545450687408447
    },
    {
      "epoch": 0.39794037940379406,
      "step": 7342,
      "training_loss": 6.627559661865234
    },
    {
      "epoch": 0.39799457994579945,
      "step": 7343,
      "training_loss": 6.82268762588501
    },
    {
      "epoch": 0.3980487804878049,
      "grad_norm": 16.78015899658203,
      "learning_rate": 1e-05,
      "loss": 6.4889,
      "step": 7344
    },
    {
      "epoch": 0.3980487804878049,
      "step": 7344,
      "training_loss": 5.926809787750244
    },
    {
      "epoch": 0.3981029810298103,
      "step": 7345,
      "training_loss": 6.047359466552734
    },
    {
      "epoch": 0.39815718157181573,
      "step": 7346,
      "training_loss": 8.183247566223145
    },
    {
      "epoch": 0.3982113821138211,
      "step": 7347,
      "training_loss": 3.657886505126953
    },
    {
      "epoch": 0.39826558265582657,
      "grad_norm": 25.536479949951172,
      "learning_rate": 1e-05,
      "loss": 5.9538,
      "step": 7348
    },
    {
      "epoch": 0.39826558265582657,
      "step": 7348,
      "training_loss": 7.245607852935791
    },
    {
      "epoch": 0.39831978319783196,
      "step": 7349,
      "training_loss": 7.6331915855407715
    },
    {
      "epoch": 0.3983739837398374,
      "step": 7350,
      "training_loss": 6.968524932861328
    },
    {
      "epoch": 0.39842818428184285,
      "step": 7351,
      "training_loss": 6.999598026275635
    },
    {
      "epoch": 0.39848238482384823,
      "grad_norm": 21.028060913085938,
      "learning_rate": 1e-05,
      "loss": 7.2117,
      "step": 7352
    },
    {
      "epoch": 0.39848238482384823,
      "step": 7352,
      "training_loss": 7.225113868713379
    },
    {
      "epoch": 0.3985365853658537,
      "step": 7353,
      "training_loss": 8.877124786376953
    },
    {
      "epoch": 0.39859078590785907,
      "step": 7354,
      "training_loss": 7.192966938018799
    },
    {
      "epoch": 0.3986449864498645,
      "step": 7355,
      "training_loss": 6.830648899078369
    },
    {
      "epoch": 0.3986991869918699,
      "grad_norm": 41.05656433105469,
      "learning_rate": 1e-05,
      "loss": 7.5315,
      "step": 7356
    },
    {
      "epoch": 0.3986991869918699,
      "step": 7356,
      "training_loss": 6.001355171203613
    },
    {
      "epoch": 0.39875338753387535,
      "step": 7357,
      "training_loss": 6.262204170227051
    },
    {
      "epoch": 0.39880758807588074,
      "step": 7358,
      "training_loss": 7.808730602264404
    },
    {
      "epoch": 0.3988617886178862,
      "step": 7359,
      "training_loss": 7.192479133605957
    },
    {
      "epoch": 0.3989159891598916,
      "grad_norm": 24.263992309570312,
      "learning_rate": 1e-05,
      "loss": 6.8162,
      "step": 7360
    },
    {
      "epoch": 0.3989159891598916,
      "step": 7360,
      "training_loss": 6.0843634605407715
    },
    {
      "epoch": 0.398970189701897,
      "step": 7361,
      "training_loss": 7.028557300567627
    },
    {
      "epoch": 0.39902439024390246,
      "step": 7362,
      "training_loss": 7.362267971038818
    },
    {
      "epoch": 0.39907859078590785,
      "step": 7363,
      "training_loss": 6.474125385284424
    },
    {
      "epoch": 0.3991327913279133,
      "grad_norm": 25.149072647094727,
      "learning_rate": 1e-05,
      "loss": 6.7373,
      "step": 7364
    },
    {
      "epoch": 0.3991327913279133,
      "step": 7364,
      "training_loss": 4.477811336517334
    },
    {
      "epoch": 0.3991869918699187,
      "step": 7365,
      "training_loss": 7.7790207862854
    },
    {
      "epoch": 0.39924119241192413,
      "step": 7366,
      "training_loss": 7.475703716278076
    },
    {
      "epoch": 0.3992953929539295,
      "step": 7367,
      "training_loss": 5.6587419509887695
    },
    {
      "epoch": 0.39934959349593496,
      "grad_norm": 24.038265228271484,
      "learning_rate": 1e-05,
      "loss": 6.3478,
      "step": 7368
    },
    {
      "epoch": 0.39934959349593496,
      "step": 7368,
      "training_loss": 7.339603900909424
    },
    {
      "epoch": 0.39940379403794035,
      "step": 7369,
      "training_loss": 6.317002296447754
    },
    {
      "epoch": 0.3994579945799458,
      "step": 7370,
      "training_loss": 5.453810691833496
    },
    {
      "epoch": 0.39951219512195124,
      "step": 7371,
      "training_loss": 6.473306179046631
    },
    {
      "epoch": 0.39956639566395663,
      "grad_norm": 27.66282844543457,
      "learning_rate": 1e-05,
      "loss": 6.3959,
      "step": 7372
    },
    {
      "epoch": 0.39956639566395663,
      "step": 7372,
      "training_loss": 6.748651504516602
    },
    {
      "epoch": 0.3996205962059621,
      "step": 7373,
      "training_loss": 3.7664647102355957
    },
    {
      "epoch": 0.39967479674796746,
      "step": 7374,
      "training_loss": 7.226869106292725
    },
    {
      "epoch": 0.3997289972899729,
      "step": 7375,
      "training_loss": 6.296019554138184
    },
    {
      "epoch": 0.3997831978319783,
      "grad_norm": 16.738239288330078,
      "learning_rate": 1e-05,
      "loss": 6.0095,
      "step": 7376
    },
    {
      "epoch": 0.3997831978319783,
      "step": 7376,
      "training_loss": 4.110815525054932
    },
    {
      "epoch": 0.39983739837398374,
      "step": 7377,
      "training_loss": 10.25929069519043
    },
    {
      "epoch": 0.39989159891598913,
      "step": 7378,
      "training_loss": 5.433449745178223
    },
    {
      "epoch": 0.3999457994579946,
      "step": 7379,
      "training_loss": 7.112864971160889
    },
    {
      "epoch": 0.4,
      "grad_norm": 28.803207397460938,
      "learning_rate": 1e-05,
      "loss": 6.7291,
      "step": 7380
    },
    {
      "epoch": 0.4,
      "step": 7380,
      "training_loss": 7.437262058258057
    },
    {
      "epoch": 0.4000542005420054,
      "step": 7381,
      "training_loss": 8.013895988464355
    },
    {
      "epoch": 0.40010840108401086,
      "step": 7382,
      "training_loss": 5.8267717361450195
    },
    {
      "epoch": 0.40016260162601625,
      "step": 7383,
      "training_loss": 7.112789630889893
    },
    {
      "epoch": 0.4002168021680217,
      "grad_norm": 24.080347061157227,
      "learning_rate": 1e-05,
      "loss": 7.0977,
      "step": 7384
    },
    {
      "epoch": 0.4002168021680217,
      "step": 7384,
      "training_loss": 7.029842853546143
    },
    {
      "epoch": 0.4002710027100271,
      "step": 7385,
      "training_loss": 6.331428527832031
    },
    {
      "epoch": 0.4003252032520325,
      "step": 7386,
      "training_loss": 7.162086009979248
    },
    {
      "epoch": 0.4003794037940379,
      "step": 7387,
      "training_loss": 6.277514457702637
    },
    {
      "epoch": 0.40043360433604336,
      "grad_norm": 24.457334518432617,
      "learning_rate": 1e-05,
      "loss": 6.7002,
      "step": 7388
    },
    {
      "epoch": 0.40043360433604336,
      "step": 7388,
      "training_loss": 7.688632488250732
    },
    {
      "epoch": 0.4004878048780488,
      "step": 7389,
      "training_loss": 7.458871841430664
    },
    {
      "epoch": 0.4005420054200542,
      "step": 7390,
      "training_loss": 7.298785209655762
    },
    {
      "epoch": 0.40059620596205964,
      "step": 7391,
      "training_loss": 5.574996471405029
    },
    {
      "epoch": 0.400650406504065,
      "grad_norm": 37.31958770751953,
      "learning_rate": 1e-05,
      "loss": 7.0053,
      "step": 7392
    },
    {
      "epoch": 0.400650406504065,
      "step": 7392,
      "training_loss": 7.234678745269775
    },
    {
      "epoch": 0.40070460704607047,
      "step": 7393,
      "training_loss": 7.044847011566162
    },
    {
      "epoch": 0.40075880758807586,
      "step": 7394,
      "training_loss": 8.167634963989258
    },
    {
      "epoch": 0.4008130081300813,
      "step": 7395,
      "training_loss": 6.769653797149658
    },
    {
      "epoch": 0.4008672086720867,
      "grad_norm": 17.359811782836914,
      "learning_rate": 1e-05,
      "loss": 7.3042,
      "step": 7396
    },
    {
      "epoch": 0.4008672086720867,
      "step": 7396,
      "training_loss": 7.4705328941345215
    },
    {
      "epoch": 0.40092140921409214,
      "step": 7397,
      "training_loss": 5.2361321449279785
    },
    {
      "epoch": 0.4009756097560976,
      "step": 7398,
      "training_loss": 6.2078471183776855
    },
    {
      "epoch": 0.401029810298103,
      "step": 7399,
      "training_loss": 6.42321252822876
    },
    {
      "epoch": 0.4010840108401084,
      "grad_norm": 26.187379837036133,
      "learning_rate": 1e-05,
      "loss": 6.3344,
      "step": 7400
    },
    {
      "epoch": 0.4010840108401084,
      "step": 7400,
      "training_loss": 4.865018367767334
    },
    {
      "epoch": 0.4011382113821138,
      "step": 7401,
      "training_loss": 7.882285118103027
    },
    {
      "epoch": 0.40119241192411925,
      "step": 7402,
      "training_loss": 7.1632304191589355
    },
    {
      "epoch": 0.40124661246612464,
      "step": 7403,
      "training_loss": 6.33237361907959
    },
    {
      "epoch": 0.4013008130081301,
      "grad_norm": 44.111602783203125,
      "learning_rate": 1e-05,
      "loss": 6.5607,
      "step": 7404
    },
    {
      "epoch": 0.4013008130081301,
      "step": 7404,
      "training_loss": 5.606115818023682
    },
    {
      "epoch": 0.4013550135501355,
      "step": 7405,
      "training_loss": 6.474436283111572
    },
    {
      "epoch": 0.4014092140921409,
      "step": 7406,
      "training_loss": 6.879089832305908
    },
    {
      "epoch": 0.40146341463414636,
      "step": 7407,
      "training_loss": 5.770050525665283
    },
    {
      "epoch": 0.40151761517615175,
      "grad_norm": 32.62046813964844,
      "learning_rate": 1e-05,
      "loss": 6.1824,
      "step": 7408
    },
    {
      "epoch": 0.40151761517615175,
      "step": 7408,
      "training_loss": 6.669522285461426
    },
    {
      "epoch": 0.4015718157181572,
      "step": 7409,
      "training_loss": 6.704941272735596
    },
    {
      "epoch": 0.4016260162601626,
      "step": 7410,
      "training_loss": 5.7418131828308105
    },
    {
      "epoch": 0.40168021680216803,
      "step": 7411,
      "training_loss": 6.275989532470703
    },
    {
      "epoch": 0.4017344173441734,
      "grad_norm": 32.490882873535156,
      "learning_rate": 1e-05,
      "loss": 6.3481,
      "step": 7412
    },
    {
      "epoch": 0.4017344173441734,
      "step": 7412,
      "training_loss": 7.609532833099365
    },
    {
      "epoch": 0.40178861788617887,
      "step": 7413,
      "training_loss": 3.3349764347076416
    },
    {
      "epoch": 0.40184281842818426,
      "step": 7414,
      "training_loss": 6.838066101074219
    },
    {
      "epoch": 0.4018970189701897,
      "step": 7415,
      "training_loss": 6.61293888092041
    },
    {
      "epoch": 0.40195121951219515,
      "grad_norm": 46.75132751464844,
      "learning_rate": 1e-05,
      "loss": 6.0989,
      "step": 7416
    },
    {
      "epoch": 0.40195121951219515,
      "step": 7416,
      "training_loss": 8.08866024017334
    },
    {
      "epoch": 0.40200542005420054,
      "step": 7417,
      "training_loss": 7.122399806976318
    },
    {
      "epoch": 0.402059620596206,
      "step": 7418,
      "training_loss": 7.000319480895996
    },
    {
      "epoch": 0.40211382113821137,
      "step": 7419,
      "training_loss": 5.642205715179443
    },
    {
      "epoch": 0.4021680216802168,
      "grad_norm": 29.134618759155273,
      "learning_rate": 1e-05,
      "loss": 6.9634,
      "step": 7420
    },
    {
      "epoch": 0.4021680216802168,
      "step": 7420,
      "training_loss": 6.323511600494385
    },
    {
      "epoch": 0.4022222222222222,
      "step": 7421,
      "training_loss": 6.635869979858398
    },
    {
      "epoch": 0.40227642276422765,
      "step": 7422,
      "training_loss": 6.260538101196289
    },
    {
      "epoch": 0.40233062330623304,
      "step": 7423,
      "training_loss": 7.201754570007324
    },
    {
      "epoch": 0.4023848238482385,
      "grad_norm": 27.76097869873047,
      "learning_rate": 1e-05,
      "loss": 6.6054,
      "step": 7424
    },
    {
      "epoch": 0.4023848238482385,
      "step": 7424,
      "training_loss": 4.763306140899658
    },
    {
      "epoch": 0.4024390243902439,
      "step": 7425,
      "training_loss": 6.536773681640625
    },
    {
      "epoch": 0.4024932249322493,
      "step": 7426,
      "training_loss": 6.637755870819092
    },
    {
      "epoch": 0.40254742547425476,
      "step": 7427,
      "training_loss": 7.184900760650635
    },
    {
      "epoch": 0.40260162601626015,
      "grad_norm": 25.642045974731445,
      "learning_rate": 1e-05,
      "loss": 6.2807,
      "step": 7428
    },
    {
      "epoch": 0.40260162601626015,
      "step": 7428,
      "training_loss": 6.619565010070801
    },
    {
      "epoch": 0.4026558265582656,
      "step": 7429,
      "training_loss": 3.7323598861694336
    },
    {
      "epoch": 0.402710027100271,
      "step": 7430,
      "training_loss": 6.798029899597168
    },
    {
      "epoch": 0.40276422764227643,
      "step": 7431,
      "training_loss": 7.171594142913818
    },
    {
      "epoch": 0.4028184281842818,
      "grad_norm": 14.626795768737793,
      "learning_rate": 1e-05,
      "loss": 6.0804,
      "step": 7432
    },
    {
      "epoch": 0.4028184281842818,
      "step": 7432,
      "training_loss": 5.980007171630859
    },
    {
      "epoch": 0.40287262872628726,
      "step": 7433,
      "training_loss": 6.652684688568115
    },
    {
      "epoch": 0.4029268292682927,
      "step": 7434,
      "training_loss": 6.856192111968994
    },
    {
      "epoch": 0.4029810298102981,
      "step": 7435,
      "training_loss": 8.120460510253906
    },
    {
      "epoch": 0.40303523035230354,
      "grad_norm": 30.154891967773438,
      "learning_rate": 1e-05,
      "loss": 6.9023,
      "step": 7436
    },
    {
      "epoch": 0.40303523035230354,
      "step": 7436,
      "training_loss": 6.243505954742432
    },
    {
      "epoch": 0.40308943089430893,
      "step": 7437,
      "training_loss": 7.81321907043457
    },
    {
      "epoch": 0.4031436314363144,
      "step": 7438,
      "training_loss": 5.07675838470459
    },
    {
      "epoch": 0.40319783197831977,
      "step": 7439,
      "training_loss": 6.419912815093994
    },
    {
      "epoch": 0.4032520325203252,
      "grad_norm": 26.764118194580078,
      "learning_rate": 1e-05,
      "loss": 6.3883,
      "step": 7440
    },
    {
      "epoch": 0.4032520325203252,
      "step": 7440,
      "training_loss": 6.468867778778076
    },
    {
      "epoch": 0.4033062330623306,
      "step": 7441,
      "training_loss": 5.954531669616699
    },
    {
      "epoch": 0.40336043360433604,
      "step": 7442,
      "training_loss": 7.764697074890137
    },
    {
      "epoch": 0.4034146341463415,
      "step": 7443,
      "training_loss": 7.8901166915893555
    },
    {
      "epoch": 0.4034688346883469,
      "grad_norm": 21.989757537841797,
      "learning_rate": 1e-05,
      "loss": 7.0196,
      "step": 7444
    },
    {
      "epoch": 0.4034688346883469,
      "step": 7444,
      "training_loss": 5.704701900482178
    },
    {
      "epoch": 0.4035230352303523,
      "step": 7445,
      "training_loss": 8.285834312438965
    },
    {
      "epoch": 0.4035772357723577,
      "step": 7446,
      "training_loss": 6.945742607116699
    },
    {
      "epoch": 0.40363143631436316,
      "step": 7447,
      "training_loss": 4.747708797454834
    },
    {
      "epoch": 0.40368563685636855,
      "grad_norm": 41.07554626464844,
      "learning_rate": 1e-05,
      "loss": 6.421,
      "step": 7448
    },
    {
      "epoch": 0.40368563685636855,
      "step": 7448,
      "training_loss": 3.6572353839874268
    },
    {
      "epoch": 0.403739837398374,
      "step": 7449,
      "training_loss": 6.796742916107178
    },
    {
      "epoch": 0.4037940379403794,
      "step": 7450,
      "training_loss": 6.617387771606445
    },
    {
      "epoch": 0.4038482384823848,
      "step": 7451,
      "training_loss": 7.46129035949707
    },
    {
      "epoch": 0.40390243902439027,
      "grad_norm": 39.1595344543457,
      "learning_rate": 1e-05,
      "loss": 6.1332,
      "step": 7452
    },
    {
      "epoch": 0.40390243902439027,
      "step": 7452,
      "training_loss": 7.006686210632324
    },
    {
      "epoch": 0.40395663956639566,
      "step": 7453,
      "training_loss": 7.102314472198486
    },
    {
      "epoch": 0.4040108401084011,
      "step": 7454,
      "training_loss": 6.949713230133057
    },
    {
      "epoch": 0.4040650406504065,
      "step": 7455,
      "training_loss": 6.390828609466553
    },
    {
      "epoch": 0.40411924119241194,
      "grad_norm": 21.086828231811523,
      "learning_rate": 1e-05,
      "loss": 6.8624,
      "step": 7456
    },
    {
      "epoch": 0.40411924119241194,
      "step": 7456,
      "training_loss": 7.533724784851074
    },
    {
      "epoch": 0.4041734417344173,
      "step": 7457,
      "training_loss": 7.049105167388916
    },
    {
      "epoch": 0.40422764227642277,
      "step": 7458,
      "training_loss": 6.475299835205078
    },
    {
      "epoch": 0.40428184281842816,
      "step": 7459,
      "training_loss": 6.361251354217529
    },
    {
      "epoch": 0.4043360433604336,
      "grad_norm": 24.51497459411621,
      "learning_rate": 1e-05,
      "loss": 6.8548,
      "step": 7460
    },
    {
      "epoch": 0.4043360433604336,
      "step": 7460,
      "training_loss": 6.417996883392334
    },
    {
      "epoch": 0.40439024390243905,
      "step": 7461,
      "training_loss": 7.534511566162109
    },
    {
      "epoch": 0.40444444444444444,
      "step": 7462,
      "training_loss": 6.735986232757568
    },
    {
      "epoch": 0.4044986449864499,
      "step": 7463,
      "training_loss": 5.835306644439697
    },
    {
      "epoch": 0.4045528455284553,
      "grad_norm": 36.81172180175781,
      "learning_rate": 1e-05,
      "loss": 6.631,
      "step": 7464
    },
    {
      "epoch": 0.4045528455284553,
      "step": 7464,
      "training_loss": 7.058969497680664
    },
    {
      "epoch": 0.4046070460704607,
      "step": 7465,
      "training_loss": 7.153674125671387
    },
    {
      "epoch": 0.4046612466124661,
      "step": 7466,
      "training_loss": 7.226945877075195
    },
    {
      "epoch": 0.40471544715447155,
      "step": 7467,
      "training_loss": 5.56262731552124
    },
    {
      "epoch": 0.40476964769647694,
      "grad_norm": 26.00107192993164,
      "learning_rate": 1e-05,
      "loss": 6.7506,
      "step": 7468
    },
    {
      "epoch": 0.40476964769647694,
      "step": 7468,
      "training_loss": 7.287125587463379
    },
    {
      "epoch": 0.4048238482384824,
      "step": 7469,
      "training_loss": 7.150584697723389
    },
    {
      "epoch": 0.40487804878048783,
      "step": 7470,
      "training_loss": 6.644893169403076
    },
    {
      "epoch": 0.4049322493224932,
      "step": 7471,
      "training_loss": 7.798174858093262
    },
    {
      "epoch": 0.40498644986449867,
      "grad_norm": 46.114559173583984,
      "learning_rate": 1e-05,
      "loss": 7.2202,
      "step": 7472
    },
    {
      "epoch": 0.40498644986449867,
      "step": 7472,
      "training_loss": 7.009734630584717
    },
    {
      "epoch": 0.40504065040650405,
      "step": 7473,
      "training_loss": 7.050102233886719
    },
    {
      "epoch": 0.4050948509485095,
      "step": 7474,
      "training_loss": 5.927144527435303
    },
    {
      "epoch": 0.4051490514905149,
      "step": 7475,
      "training_loss": 6.248011589050293
    },
    {
      "epoch": 0.40520325203252033,
      "grad_norm": 24.11564064025879,
      "learning_rate": 1e-05,
      "loss": 6.5587,
      "step": 7476
    },
    {
      "epoch": 0.40520325203252033,
      "step": 7476,
      "training_loss": 7.512349605560303
    },
    {
      "epoch": 0.4052574525745257,
      "step": 7477,
      "training_loss": 8.90215015411377
    },
    {
      "epoch": 0.40531165311653117,
      "step": 7478,
      "training_loss": 6.358344554901123
    },
    {
      "epoch": 0.4053658536585366,
      "step": 7479,
      "training_loss": 7.246358871459961
    },
    {
      "epoch": 0.405420054200542,
      "grad_norm": 30.141935348510742,
      "learning_rate": 1e-05,
      "loss": 7.5048,
      "step": 7480
    },
    {
      "epoch": 0.405420054200542,
      "step": 7480,
      "training_loss": 7.628805637359619
    },
    {
      "epoch": 0.40547425474254745,
      "step": 7481,
      "training_loss": 7.281904697418213
    },
    {
      "epoch": 0.40552845528455284,
      "step": 7482,
      "training_loss": 7.667950630187988
    },
    {
      "epoch": 0.4055826558265583,
      "step": 7483,
      "training_loss": 6.001323699951172
    },
    {
      "epoch": 0.40563685636856367,
      "grad_norm": 34.845855712890625,
      "learning_rate": 1e-05,
      "loss": 7.145,
      "step": 7484
    },
    {
      "epoch": 0.40563685636856367,
      "step": 7484,
      "training_loss": 8.017752647399902
    },
    {
      "epoch": 0.4056910569105691,
      "step": 7485,
      "training_loss": 7.230794906616211
    },
    {
      "epoch": 0.4057452574525745,
      "step": 7486,
      "training_loss": 6.33327054977417
    },
    {
      "epoch": 0.40579945799457995,
      "step": 7487,
      "training_loss": 5.868180751800537
    },
    {
      "epoch": 0.4058536585365854,
      "grad_norm": 41.90886306762695,
      "learning_rate": 1e-05,
      "loss": 6.8625,
      "step": 7488
    },
    {
      "epoch": 0.4058536585365854,
      "step": 7488,
      "training_loss": 7.267489433288574
    },
    {
      "epoch": 0.4059078590785908,
      "step": 7489,
      "training_loss": 7.316134929656982
    },
    {
      "epoch": 0.4059620596205962,
      "step": 7490,
      "training_loss": 6.470063209533691
    },
    {
      "epoch": 0.4060162601626016,
      "step": 7491,
      "training_loss": 8.446951866149902
    },
    {
      "epoch": 0.40607046070460706,
      "grad_norm": 30.742128372192383,
      "learning_rate": 1e-05,
      "loss": 7.3752,
      "step": 7492
    },
    {
      "epoch": 0.40607046070460706,
      "step": 7492,
      "training_loss": 7.64586877822876
    },
    {
      "epoch": 0.40612466124661245,
      "step": 7493,
      "training_loss": 6.988495349884033
    },
    {
      "epoch": 0.4061788617886179,
      "step": 7494,
      "training_loss": 6.530984878540039
    },
    {
      "epoch": 0.4062330623306233,
      "step": 7495,
      "training_loss": 7.333242416381836
    },
    {
      "epoch": 0.40628726287262873,
      "grad_norm": 17.867395401000977,
      "learning_rate": 1e-05,
      "loss": 7.1246,
      "step": 7496
    },
    {
      "epoch": 0.40628726287262873,
      "step": 7496,
      "training_loss": 5.992990970611572
    },
    {
      "epoch": 0.4063414634146341,
      "step": 7497,
      "training_loss": 6.0768723487854
    },
    {
      "epoch": 0.40639566395663956,
      "step": 7498,
      "training_loss": 6.7906999588012695
    },
    {
      "epoch": 0.406449864498645,
      "step": 7499,
      "training_loss": 6.622633934020996
    },
    {
      "epoch": 0.4065040650406504,
      "grad_norm": 24.533267974853516,
      "learning_rate": 1e-05,
      "loss": 6.3708,
      "step": 7500
    },
    {
      "epoch": 0.4065040650406504,
      "step": 7500,
      "training_loss": 6.442449569702148
    },
    {
      "epoch": 0.40655826558265584,
      "step": 7501,
      "training_loss": 7.711370468139648
    },
    {
      "epoch": 0.40661246612466123,
      "step": 7502,
      "training_loss": 6.396764278411865
    },
    {
      "epoch": 0.4066666666666667,
      "step": 7503,
      "training_loss": 6.579486846923828
    },
    {
      "epoch": 0.40672086720867207,
      "grad_norm": 25.275550842285156,
      "learning_rate": 1e-05,
      "loss": 6.7825,
      "step": 7504
    },
    {
      "epoch": 0.40672086720867207,
      "step": 7504,
      "training_loss": 7.038330078125
    },
    {
      "epoch": 0.4067750677506775,
      "step": 7505,
      "training_loss": 6.205780029296875
    },
    {
      "epoch": 0.4068292682926829,
      "step": 7506,
      "training_loss": 6.487898349761963
    },
    {
      "epoch": 0.40688346883468834,
      "step": 7507,
      "training_loss": 6.441143035888672
    },
    {
      "epoch": 0.4069376693766938,
      "grad_norm": 30.29231071472168,
      "learning_rate": 1e-05,
      "loss": 6.5433,
      "step": 7508
    },
    {
      "epoch": 0.4069376693766938,
      "step": 7508,
      "training_loss": 6.6501593589782715
    },
    {
      "epoch": 0.4069918699186992,
      "step": 7509,
      "training_loss": 6.241630554199219
    },
    {
      "epoch": 0.4070460704607046,
      "step": 7510,
      "training_loss": 6.548931121826172
    },
    {
      "epoch": 0.40710027100271,
      "step": 7511,
      "training_loss": 6.286919593811035
    },
    {
      "epoch": 0.40715447154471546,
      "grad_norm": 45.81669235229492,
      "learning_rate": 1e-05,
      "loss": 6.4319,
      "step": 7512
    },
    {
      "epoch": 0.40715447154471546,
      "step": 7512,
      "training_loss": 6.183393955230713
    },
    {
      "epoch": 0.40720867208672085,
      "step": 7513,
      "training_loss": 5.941897869110107
    },
    {
      "epoch": 0.4072628726287263,
      "step": 7514,
      "training_loss": 7.010492324829102
    },
    {
      "epoch": 0.4073170731707317,
      "step": 7515,
      "training_loss": 6.752414226531982
    },
    {
      "epoch": 0.4073712737127371,
      "grad_norm": 19.60426902770996,
      "learning_rate": 1e-05,
      "loss": 6.472,
      "step": 7516
    },
    {
      "epoch": 0.4073712737127371,
      "step": 7516,
      "training_loss": 6.843292713165283
    },
    {
      "epoch": 0.40742547425474257,
      "step": 7517,
      "training_loss": 5.969305038452148
    },
    {
      "epoch": 0.40747967479674796,
      "step": 7518,
      "training_loss": 6.493997097015381
    },
    {
      "epoch": 0.4075338753387534,
      "step": 7519,
      "training_loss": 7.543930530548096
    },
    {
      "epoch": 0.4075880758807588,
      "grad_norm": 28.272472381591797,
      "learning_rate": 1e-05,
      "loss": 6.7126,
      "step": 7520
    },
    {
      "epoch": 0.4075880758807588,
      "step": 7520,
      "training_loss": 6.417809009552002
    },
    {
      "epoch": 0.40764227642276424,
      "step": 7521,
      "training_loss": 7.123584747314453
    },
    {
      "epoch": 0.4076964769647696,
      "step": 7522,
      "training_loss": 7.501959323883057
    },
    {
      "epoch": 0.40775067750677507,
      "step": 7523,
      "training_loss": 5.55683708190918
    },
    {
      "epoch": 0.40780487804878046,
      "grad_norm": 30.163503646850586,
      "learning_rate": 1e-05,
      "loss": 6.65,
      "step": 7524
    },
    {
      "epoch": 0.40780487804878046,
      "step": 7524,
      "training_loss": 5.506644248962402
    },
    {
      "epoch": 0.4078590785907859,
      "step": 7525,
      "training_loss": 8.393240928649902
    },
    {
      "epoch": 0.40791327913279135,
      "step": 7526,
      "training_loss": 6.59257698059082
    },
    {
      "epoch": 0.40796747967479674,
      "step": 7527,
      "training_loss": 5.559625148773193
    },
    {
      "epoch": 0.4080216802168022,
      "grad_norm": 34.25416946411133,
      "learning_rate": 1e-05,
      "loss": 6.513,
      "step": 7528
    },
    {
      "epoch": 0.4080216802168022,
      "step": 7528,
      "training_loss": 7.121633052825928
    },
    {
      "epoch": 0.4080758807588076,
      "step": 7529,
      "training_loss": 6.004499912261963
    },
    {
      "epoch": 0.408130081300813,
      "step": 7530,
      "training_loss": 6.415772438049316
    },
    {
      "epoch": 0.4081842818428184,
      "step": 7531,
      "training_loss": 6.161742687225342
    },
    {
      "epoch": 0.40823848238482385,
      "grad_norm": 31.83953857421875,
      "learning_rate": 1e-05,
      "loss": 6.4259,
      "step": 7532
    },
    {
      "epoch": 0.40823848238482385,
      "step": 7532,
      "training_loss": 8.941690444946289
    },
    {
      "epoch": 0.40829268292682924,
      "step": 7533,
      "training_loss": 5.295707702636719
    },
    {
      "epoch": 0.4083468834688347,
      "step": 7534,
      "training_loss": 7.214529514312744
    },
    {
      "epoch": 0.40840108401084013,
      "step": 7535,
      "training_loss": 6.876858711242676
    },
    {
      "epoch": 0.4084552845528455,
      "grad_norm": 35.78632354736328,
      "learning_rate": 1e-05,
      "loss": 7.0822,
      "step": 7536
    },
    {
      "epoch": 0.4084552845528455,
      "step": 7536,
      "training_loss": 6.399169445037842
    },
    {
      "epoch": 0.40850948509485097,
      "step": 7537,
      "training_loss": 4.144837856292725
    },
    {
      "epoch": 0.40856368563685636,
      "step": 7538,
      "training_loss": 7.82336950302124
    },
    {
      "epoch": 0.4086178861788618,
      "step": 7539,
      "training_loss": 3.744335889816284
    },
    {
      "epoch": 0.4086720867208672,
      "grad_norm": 23.24009895324707,
      "learning_rate": 1e-05,
      "loss": 5.5279,
      "step": 7540
    },
    {
      "epoch": 0.4086720867208672,
      "step": 7540,
      "training_loss": 6.392261981964111
    },
    {
      "epoch": 0.40872628726287263,
      "step": 7541,
      "training_loss": 7.120906829833984
    },
    {
      "epoch": 0.408780487804878,
      "step": 7542,
      "training_loss": 7.016310214996338
    },
    {
      "epoch": 0.40883468834688347,
      "step": 7543,
      "training_loss": 6.9040656089782715
    },
    {
      "epoch": 0.4088888888888889,
      "grad_norm": 29.42048454284668,
      "learning_rate": 1e-05,
      "loss": 6.8584,
      "step": 7544
    },
    {
      "epoch": 0.4088888888888889,
      "step": 7544,
      "training_loss": 3.4533839225769043
    },
    {
      "epoch": 0.4089430894308943,
      "step": 7545,
      "training_loss": 7.75993013381958
    },
    {
      "epoch": 0.40899728997289975,
      "step": 7546,
      "training_loss": 7.061952114105225
    },
    {
      "epoch": 0.40905149051490514,
      "step": 7547,
      "training_loss": 7.111529350280762
    },
    {
      "epoch": 0.4091056910569106,
      "grad_norm": 18.866273880004883,
      "learning_rate": 1e-05,
      "loss": 6.3467,
      "step": 7548
    },
    {
      "epoch": 0.4091056910569106,
      "step": 7548,
      "training_loss": 7.2341718673706055
    },
    {
      "epoch": 0.40915989159891597,
      "step": 7549,
      "training_loss": 4.71633768081665
    },
    {
      "epoch": 0.4092140921409214,
      "step": 7550,
      "training_loss": 6.947551727294922
    },
    {
      "epoch": 0.4092682926829268,
      "step": 7551,
      "training_loss": 7.85488224029541
    },
    {
      "epoch": 0.40932249322493225,
      "grad_norm": 36.7707405090332,
      "learning_rate": 1e-05,
      "loss": 6.6882,
      "step": 7552
    },
    {
      "epoch": 0.40932249322493225,
      "step": 7552,
      "training_loss": 6.872740268707275
    },
    {
      "epoch": 0.4093766937669377,
      "step": 7553,
      "training_loss": 7.306842803955078
    },
    {
      "epoch": 0.4094308943089431,
      "step": 7554,
      "training_loss": 6.848151683807373
    },
    {
      "epoch": 0.40948509485094853,
      "step": 7555,
      "training_loss": 6.6508402824401855
    },
    {
      "epoch": 0.4095392953929539,
      "grad_norm": 35.8945198059082,
      "learning_rate": 1e-05,
      "loss": 6.9196,
      "step": 7556
    },
    {
      "epoch": 0.4095392953929539,
      "step": 7556,
      "training_loss": 6.4541521072387695
    },
    {
      "epoch": 0.40959349593495936,
      "step": 7557,
      "training_loss": 2.893548011779785
    },
    {
      "epoch": 0.40964769647696475,
      "step": 7558,
      "training_loss": 6.393303394317627
    },
    {
      "epoch": 0.4097018970189702,
      "step": 7559,
      "training_loss": 4.960347652435303
    },
    {
      "epoch": 0.4097560975609756,
      "grad_norm": 32.13814163208008,
      "learning_rate": 1e-05,
      "loss": 5.1753,
      "step": 7560
    },
    {
      "epoch": 0.4097560975609756,
      "step": 7560,
      "training_loss": 6.598443984985352
    },
    {
      "epoch": 0.40981029810298103,
      "step": 7561,
      "training_loss": 7.411984443664551
    },
    {
      "epoch": 0.4098644986449865,
      "step": 7562,
      "training_loss": 6.720041275024414
    },
    {
      "epoch": 0.40991869918699186,
      "step": 7563,
      "training_loss": 7.453019618988037
    },
    {
      "epoch": 0.4099728997289973,
      "grad_norm": 42.159942626953125,
      "learning_rate": 1e-05,
      "loss": 7.0459,
      "step": 7564
    },
    {
      "epoch": 0.4099728997289973,
      "step": 7564,
      "training_loss": 7.610309600830078
    },
    {
      "epoch": 0.4100271002710027,
      "step": 7565,
      "training_loss": 7.25598669052124
    },
    {
      "epoch": 0.41008130081300814,
      "step": 7566,
      "training_loss": 6.925579071044922
    },
    {
      "epoch": 0.41013550135501353,
      "step": 7567,
      "training_loss": 7.815921783447266
    },
    {
      "epoch": 0.410189701897019,
      "grad_norm": 20.12781524658203,
      "learning_rate": 1e-05,
      "loss": 7.4019,
      "step": 7568
    },
    {
      "epoch": 0.410189701897019,
      "step": 7568,
      "training_loss": 6.31447696685791
    },
    {
      "epoch": 0.41024390243902437,
      "step": 7569,
      "training_loss": 6.952073097229004
    },
    {
      "epoch": 0.4102981029810298,
      "step": 7570,
      "training_loss": 6.979824542999268
    },
    {
      "epoch": 0.41035230352303526,
      "step": 7571,
      "training_loss": 8.275704383850098
    },
    {
      "epoch": 0.41040650406504064,
      "grad_norm": 26.623676300048828,
      "learning_rate": 1e-05,
      "loss": 7.1305,
      "step": 7572
    },
    {
      "epoch": 0.41040650406504064,
      "step": 7572,
      "training_loss": 5.033967971801758
    },
    {
      "epoch": 0.4104607046070461,
      "step": 7573,
      "training_loss": 5.7358903884887695
    },
    {
      "epoch": 0.4105149051490515,
      "step": 7574,
      "training_loss": 6.239565849304199
    },
    {
      "epoch": 0.4105691056910569,
      "step": 7575,
      "training_loss": 6.692193984985352
    },
    {
      "epoch": 0.4106233062330623,
      "grad_norm": 22.690155029296875,
      "learning_rate": 1e-05,
      "loss": 5.9254,
      "step": 7576
    },
    {
      "epoch": 0.4106233062330623,
      "step": 7576,
      "training_loss": 5.894453048706055
    },
    {
      "epoch": 0.41067750677506776,
      "step": 7577,
      "training_loss": 4.829363822937012
    },
    {
      "epoch": 0.41073170731707315,
      "step": 7578,
      "training_loss": 7.173053741455078
    },
    {
      "epoch": 0.4107859078590786,
      "step": 7579,
      "training_loss": 6.492788314819336
    },
    {
      "epoch": 0.41084010840108404,
      "grad_norm": 25.815793991088867,
      "learning_rate": 1e-05,
      "loss": 6.0974,
      "step": 7580
    },
    {
      "epoch": 0.41084010840108404,
      "step": 7580,
      "training_loss": 6.788500785827637
    },
    {
      "epoch": 0.4108943089430894,
      "step": 7581,
      "training_loss": 4.85092830657959
    },
    {
      "epoch": 0.41094850948509487,
      "step": 7582,
      "training_loss": 7.767731666564941
    },
    {
      "epoch": 0.41100271002710026,
      "step": 7583,
      "training_loss": 6.510496616363525
    },
    {
      "epoch": 0.4110569105691057,
      "grad_norm": 22.326005935668945,
      "learning_rate": 1e-05,
      "loss": 6.4794,
      "step": 7584
    },
    {
      "epoch": 0.4110569105691057,
      "step": 7584,
      "training_loss": 8.472265243530273
    },
    {
      "epoch": 0.4111111111111111,
      "step": 7585,
      "training_loss": 6.632848739624023
    },
    {
      "epoch": 0.41116531165311654,
      "step": 7586,
      "training_loss": 7.271413803100586
    },
    {
      "epoch": 0.41121951219512193,
      "step": 7587,
      "training_loss": 6.864455699920654
    },
    {
      "epoch": 0.4112737127371274,
      "grad_norm": 16.983320236206055,
      "learning_rate": 1e-05,
      "loss": 7.3102,
      "step": 7588
    },
    {
      "epoch": 0.4112737127371274,
      "step": 7588,
      "training_loss": 6.765711784362793
    },
    {
      "epoch": 0.4113279132791328,
      "step": 7589,
      "training_loss": 7.027331352233887
    },
    {
      "epoch": 0.4113821138211382,
      "step": 7590,
      "training_loss": 4.427988052368164
    },
    {
      "epoch": 0.41143631436314365,
      "step": 7591,
      "training_loss": 6.656741142272949
    },
    {
      "epoch": 0.41149051490514904,
      "grad_norm": 36.57645034790039,
      "learning_rate": 1e-05,
      "loss": 6.2194,
      "step": 7592
    },
    {
      "epoch": 0.41149051490514904,
      "step": 7592,
      "training_loss": 7.095437526702881
    },
    {
      "epoch": 0.4115447154471545,
      "step": 7593,
      "training_loss": 7.31374454498291
    },
    {
      "epoch": 0.4115989159891599,
      "step": 7594,
      "training_loss": 4.30722188949585
    },
    {
      "epoch": 0.4116531165311653,
      "step": 7595,
      "training_loss": 6.884979248046875
    },
    {
      "epoch": 0.4117073170731707,
      "grad_norm": 24.18291473388672,
      "learning_rate": 1e-05,
      "loss": 6.4003,
      "step": 7596
    },
    {
      "epoch": 0.4117073170731707,
      "step": 7596,
      "training_loss": 7.566868305206299
    },
    {
      "epoch": 0.41176151761517615,
      "step": 7597,
      "training_loss": 6.885537147521973
    },
    {
      "epoch": 0.4118157181571816,
      "step": 7598,
      "training_loss": 5.43121337890625
    },
    {
      "epoch": 0.411869918699187,
      "step": 7599,
      "training_loss": 7.013779163360596
    },
    {
      "epoch": 0.41192411924119243,
      "grad_norm": 36.04652786254883,
      "learning_rate": 1e-05,
      "loss": 6.7243,
      "step": 7600
    },
    {
      "epoch": 0.41192411924119243,
      "step": 7600,
      "training_loss": 6.875563144683838
    },
    {
      "epoch": 0.4119783197831978,
      "step": 7601,
      "training_loss": 7.806459426879883
    },
    {
      "epoch": 0.41203252032520327,
      "step": 7602,
      "training_loss": 6.4596052169799805
    },
    {
      "epoch": 0.41208672086720866,
      "step": 7603,
      "training_loss": 7.178589344024658
    },
    {
      "epoch": 0.4121409214092141,
      "grad_norm": 47.16157913208008,
      "learning_rate": 1e-05,
      "loss": 7.0801,
      "step": 7604
    },
    {
      "epoch": 0.4121409214092141,
      "step": 7604,
      "training_loss": 5.8933844566345215
    },
    {
      "epoch": 0.4121951219512195,
      "step": 7605,
      "training_loss": 7.996598720550537
    },
    {
      "epoch": 0.41224932249322493,
      "step": 7606,
      "training_loss": 6.012012958526611
    },
    {
      "epoch": 0.4123035230352304,
      "step": 7607,
      "training_loss": 6.668570041656494
    },
    {
      "epoch": 0.41235772357723577,
      "grad_norm": 42.55768966674805,
      "learning_rate": 1e-05,
      "loss": 6.6426,
      "step": 7608
    },
    {
      "epoch": 0.41235772357723577,
      "step": 7608,
      "training_loss": 6.931301116943359
    },
    {
      "epoch": 0.4124119241192412,
      "step": 7609,
      "training_loss": 4.988126277923584
    },
    {
      "epoch": 0.4124661246612466,
      "step": 7610,
      "training_loss": 6.968217372894287
    },
    {
      "epoch": 0.41252032520325205,
      "step": 7611,
      "training_loss": 7.113757610321045
    },
    {
      "epoch": 0.41257452574525744,
      "grad_norm": 33.930511474609375,
      "learning_rate": 1e-05,
      "loss": 6.5004,
      "step": 7612
    },
    {
      "epoch": 0.41257452574525744,
      "step": 7612,
      "training_loss": 6.427861213684082
    },
    {
      "epoch": 0.4126287262872629,
      "step": 7613,
      "training_loss": 6.7452778816223145
    },
    {
      "epoch": 0.41268292682926827,
      "step": 7614,
      "training_loss": 7.8344831466674805
    },
    {
      "epoch": 0.4127371273712737,
      "step": 7615,
      "training_loss": 6.758591175079346
    },
    {
      "epoch": 0.41279132791327916,
      "grad_norm": 16.198381423950195,
      "learning_rate": 1e-05,
      "loss": 6.9416,
      "step": 7616
    },
    {
      "epoch": 0.41279132791327916,
      "step": 7616,
      "training_loss": 5.910443305969238
    },
    {
      "epoch": 0.41284552845528455,
      "step": 7617,
      "training_loss": 7.357443332672119
    },
    {
      "epoch": 0.41289972899729,
      "step": 7618,
      "training_loss": 8.13483715057373
    },
    {
      "epoch": 0.4129539295392954,
      "step": 7619,
      "training_loss": 6.969118595123291
    },
    {
      "epoch": 0.41300813008130083,
      "grad_norm": 22.43235206604004,
      "learning_rate": 1e-05,
      "loss": 7.093,
      "step": 7620
    },
    {
      "epoch": 0.41300813008130083,
      "step": 7620,
      "training_loss": 6.6251020431518555
    },
    {
      "epoch": 0.4130623306233062,
      "step": 7621,
      "training_loss": 6.787398338317871
    },
    {
      "epoch": 0.41311653116531166,
      "step": 7622,
      "training_loss": 7.2068610191345215
    },
    {
      "epoch": 0.41317073170731705,
      "step": 7623,
      "training_loss": 7.060799598693848
    },
    {
      "epoch": 0.4132249322493225,
      "grad_norm": 24.364521026611328,
      "learning_rate": 1e-05,
      "loss": 6.92,
      "step": 7624
    },
    {
      "epoch": 0.4132249322493225,
      "step": 7624,
      "training_loss": 5.213955402374268
    },
    {
      "epoch": 0.4132791327913279,
      "step": 7625,
      "training_loss": 6.611571788787842
    },
    {
      "epoch": 0.41333333333333333,
      "step": 7626,
      "training_loss": 6.433046340942383
    },
    {
      "epoch": 0.4133875338753388,
      "step": 7627,
      "training_loss": 7.013207912445068
    },
    {
      "epoch": 0.41344173441734416,
      "grad_norm": 21.436769485473633,
      "learning_rate": 1e-05,
      "loss": 6.3179,
      "step": 7628
    },
    {
      "epoch": 0.41344173441734416,
      "step": 7628,
      "training_loss": 7.0864763259887695
    },
    {
      "epoch": 0.4134959349593496,
      "step": 7629,
      "training_loss": 5.753509044647217
    },
    {
      "epoch": 0.413550135501355,
      "step": 7630,
      "training_loss": 6.438249588012695
    },
    {
      "epoch": 0.41360433604336044,
      "step": 7631,
      "training_loss": 5.513089179992676
    },
    {
      "epoch": 0.41365853658536583,
      "grad_norm": 42.7889404296875,
      "learning_rate": 1e-05,
      "loss": 6.1978,
      "step": 7632
    },
    {
      "epoch": 0.41365853658536583,
      "step": 7632,
      "training_loss": 6.941665172576904
    },
    {
      "epoch": 0.4137127371273713,
      "step": 7633,
      "training_loss": 7.755363941192627
    },
    {
      "epoch": 0.41376693766937667,
      "step": 7634,
      "training_loss": 6.83052921295166
    },
    {
      "epoch": 0.4138211382113821,
      "step": 7635,
      "training_loss": 6.940684795379639
    },
    {
      "epoch": 0.41387533875338756,
      "grad_norm": 19.51777458190918,
      "learning_rate": 1e-05,
      "loss": 7.1171,
      "step": 7636
    },
    {
      "epoch": 0.41387533875338756,
      "step": 7636,
      "training_loss": 7.1064324378967285
    },
    {
      "epoch": 0.41392953929539295,
      "step": 7637,
      "training_loss": 5.916669845581055
    },
    {
      "epoch": 0.4139837398373984,
      "step": 7638,
      "training_loss": 6.750951290130615
    },
    {
      "epoch": 0.4140379403794038,
      "step": 7639,
      "training_loss": 5.846860885620117
    },
    {
      "epoch": 0.4140921409214092,
      "grad_norm": 24.934825897216797,
      "learning_rate": 1e-05,
      "loss": 6.4052,
      "step": 7640
    },
    {
      "epoch": 0.4140921409214092,
      "step": 7640,
      "training_loss": 6.219931602478027
    },
    {
      "epoch": 0.4141463414634146,
      "step": 7641,
      "training_loss": 6.644257068634033
    },
    {
      "epoch": 0.41420054200542006,
      "step": 7642,
      "training_loss": 7.870326042175293
    },
    {
      "epoch": 0.41425474254742545,
      "step": 7643,
      "training_loss": 7.550781726837158
    },
    {
      "epoch": 0.4143089430894309,
      "grad_norm": 27.732107162475586,
      "learning_rate": 1e-05,
      "loss": 7.0713,
      "step": 7644
    },
    {
      "epoch": 0.4143089430894309,
      "step": 7644,
      "training_loss": 7.81325101852417
    },
    {
      "epoch": 0.41436314363143634,
      "step": 7645,
      "training_loss": 7.6058173179626465
    },
    {
      "epoch": 0.4144173441734417,
      "step": 7646,
      "training_loss": 7.706308841705322
    },
    {
      "epoch": 0.41447154471544717,
      "step": 7647,
      "training_loss": 7.1765360832214355
    },
    {
      "epoch": 0.41452574525745256,
      "grad_norm": 32.484703063964844,
      "learning_rate": 1e-05,
      "loss": 7.5755,
      "step": 7648
    },
    {
      "epoch": 0.41452574525745256,
      "step": 7648,
      "training_loss": 7.0763678550720215
    },
    {
      "epoch": 0.414579945799458,
      "step": 7649,
      "training_loss": 6.750057697296143
    },
    {
      "epoch": 0.4146341463414634,
      "step": 7650,
      "training_loss": 6.668651103973389
    },
    {
      "epoch": 0.41468834688346884,
      "step": 7651,
      "training_loss": 8.400938034057617
    },
    {
      "epoch": 0.41474254742547423,
      "grad_norm": 36.185794830322266,
      "learning_rate": 1e-05,
      "loss": 7.224,
      "step": 7652
    },
    {
      "epoch": 0.41474254742547423,
      "step": 7652,
      "training_loss": 6.989224910736084
    },
    {
      "epoch": 0.4147967479674797,
      "step": 7653,
      "training_loss": 6.33795166015625
    },
    {
      "epoch": 0.4148509485094851,
      "step": 7654,
      "training_loss": 6.491886615753174
    },
    {
      "epoch": 0.4149051490514905,
      "step": 7655,
      "training_loss": 7.014217376708984
    },
    {
      "epoch": 0.41495934959349595,
      "grad_norm": 23.8441219329834,
      "learning_rate": 1e-05,
      "loss": 6.7083,
      "step": 7656
    },
    {
      "epoch": 0.41495934959349595,
      "step": 7656,
      "training_loss": 7.2931342124938965
    },
    {
      "epoch": 0.41501355013550134,
      "step": 7657,
      "training_loss": 5.423446178436279
    },
    {
      "epoch": 0.4150677506775068,
      "step": 7658,
      "training_loss": 6.834499835968018
    },
    {
      "epoch": 0.4151219512195122,
      "step": 7659,
      "training_loss": 6.018067359924316
    },
    {
      "epoch": 0.4151761517615176,
      "grad_norm": 69.72260284423828,
      "learning_rate": 1e-05,
      "loss": 6.3923,
      "step": 7660
    },
    {
      "epoch": 0.4151761517615176,
      "step": 7660,
      "training_loss": 7.7315287590026855
    },
    {
      "epoch": 0.415230352303523,
      "step": 7661,
      "training_loss": 5.9689483642578125
    },
    {
      "epoch": 0.41528455284552845,
      "step": 7662,
      "training_loss": 6.386828422546387
    },
    {
      "epoch": 0.4153387533875339,
      "step": 7663,
      "training_loss": 4.380887985229492
    },
    {
      "epoch": 0.4153929539295393,
      "grad_norm": 38.38799285888672,
      "learning_rate": 1e-05,
      "loss": 6.117,
      "step": 7664
    },
    {
      "epoch": 0.4153929539295393,
      "step": 7664,
      "training_loss": 7.384108543395996
    },
    {
      "epoch": 0.41544715447154473,
      "step": 7665,
      "training_loss": 6.036690711975098
    },
    {
      "epoch": 0.4155013550135501,
      "step": 7666,
      "training_loss": 7.038891792297363
    },
    {
      "epoch": 0.41555555555555557,
      "step": 7667,
      "training_loss": 6.383690357208252
    },
    {
      "epoch": 0.41560975609756096,
      "grad_norm": 22.98211669921875,
      "learning_rate": 1e-05,
      "loss": 6.7108,
      "step": 7668
    },
    {
      "epoch": 0.41560975609756096,
      "step": 7668,
      "training_loss": 7.812686920166016
    },
    {
      "epoch": 0.4156639566395664,
      "step": 7669,
      "training_loss": 4.707923889160156
    },
    {
      "epoch": 0.4157181571815718,
      "step": 7670,
      "training_loss": 7.300449848175049
    },
    {
      "epoch": 0.41577235772357723,
      "step": 7671,
      "training_loss": 7.871554374694824
    },
    {
      "epoch": 0.4158265582655827,
      "grad_norm": 20.65654754638672,
      "learning_rate": 1e-05,
      "loss": 6.9232,
      "step": 7672
    },
    {
      "epoch": 0.4158265582655827,
      "step": 7672,
      "training_loss": 7.270224094390869
    },
    {
      "epoch": 0.41588075880758807,
      "step": 7673,
      "training_loss": 4.510278224945068
    },
    {
      "epoch": 0.4159349593495935,
      "step": 7674,
      "training_loss": 7.776300430297852
    },
    {
      "epoch": 0.4159891598915989,
      "step": 7675,
      "training_loss": 6.464915752410889
    },
    {
      "epoch": 0.41604336043360435,
      "grad_norm": 39.84014892578125,
      "learning_rate": 1e-05,
      "loss": 6.5054,
      "step": 7676
    },
    {
      "epoch": 0.41604336043360435,
      "step": 7676,
      "training_loss": 7.654420375823975
    },
    {
      "epoch": 0.41609756097560974,
      "step": 7677,
      "training_loss": 8.03846263885498
    },
    {
      "epoch": 0.4161517615176152,
      "step": 7678,
      "training_loss": 7.218215465545654
    },
    {
      "epoch": 0.41620596205962057,
      "step": 7679,
      "training_loss": 6.210945129394531
    },
    {
      "epoch": 0.416260162601626,
      "grad_norm": 30.878751754760742,
      "learning_rate": 1e-05,
      "loss": 7.2805,
      "step": 7680
    },
    {
      "epoch": 0.416260162601626,
      "step": 7680,
      "training_loss": 3.4332821369171143
    },
    {
      "epoch": 0.41631436314363146,
      "step": 7681,
      "training_loss": 4.148479461669922
    },
    {
      "epoch": 0.41636856368563685,
      "step": 7682,
      "training_loss": 4.945489883422852
    },
    {
      "epoch": 0.4164227642276423,
      "step": 7683,
      "training_loss": 6.244405746459961
    },
    {
      "epoch": 0.4164769647696477,
      "grad_norm": 31.305910110473633,
      "learning_rate": 1e-05,
      "loss": 4.6929,
      "step": 7684
    },
    {
      "epoch": 0.4164769647696477,
      "step": 7684,
      "training_loss": 8.475407600402832
    },
    {
      "epoch": 0.41653116531165313,
      "step": 7685,
      "training_loss": 6.495334148406982
    },
    {
      "epoch": 0.4165853658536585,
      "step": 7686,
      "training_loss": 6.649301052093506
    },
    {
      "epoch": 0.41663956639566396,
      "step": 7687,
      "training_loss": 6.935438632965088
    },
    {
      "epoch": 0.41669376693766935,
      "grad_norm": 26.120376586914062,
      "learning_rate": 1e-05,
      "loss": 7.1389,
      "step": 7688
    },
    {
      "epoch": 0.41669376693766935,
      "step": 7688,
      "training_loss": 7.586895942687988
    },
    {
      "epoch": 0.4167479674796748,
      "step": 7689,
      "training_loss": 6.86264181137085
    },
    {
      "epoch": 0.41680216802168024,
      "step": 7690,
      "training_loss": 7.43306827545166
    },
    {
      "epoch": 0.41685636856368563,
      "step": 7691,
      "training_loss": 6.586820125579834
    },
    {
      "epoch": 0.4169105691056911,
      "grad_norm": 19.97188949584961,
      "learning_rate": 1e-05,
      "loss": 7.1174,
      "step": 7692
    },
    {
      "epoch": 0.4169105691056911,
      "step": 7692,
      "training_loss": 7.036323070526123
    },
    {
      "epoch": 0.41696476964769646,
      "step": 7693,
      "training_loss": 8.661930084228516
    },
    {
      "epoch": 0.4170189701897019,
      "step": 7694,
      "training_loss": 7.762115478515625
    },
    {
      "epoch": 0.4170731707317073,
      "step": 7695,
      "training_loss": 7.310917377471924
    },
    {
      "epoch": 0.41712737127371274,
      "grad_norm": 25.1306209564209,
      "learning_rate": 1e-05,
      "loss": 7.6928,
      "step": 7696
    },
    {
      "epoch": 0.41712737127371274,
      "step": 7696,
      "training_loss": 6.466590881347656
    },
    {
      "epoch": 0.41718157181571813,
      "step": 7697,
      "training_loss": 7.857304573059082
    },
    {
      "epoch": 0.4172357723577236,
      "step": 7698,
      "training_loss": 6.022629261016846
    },
    {
      "epoch": 0.417289972899729,
      "step": 7699,
      "training_loss": 6.896627902984619
    },
    {
      "epoch": 0.4173441734417344,
      "grad_norm": 51.49489974975586,
      "learning_rate": 1e-05,
      "loss": 6.8108,
      "step": 7700
    },
    {
      "epoch": 0.4173441734417344,
      "step": 7700,
      "training_loss": 6.398451805114746
    },
    {
      "epoch": 0.41739837398373986,
      "step": 7701,
      "training_loss": 6.712703704833984
    },
    {
      "epoch": 0.41745257452574525,
      "step": 7702,
      "training_loss": 7.223179817199707
    },
    {
      "epoch": 0.4175067750677507,
      "step": 7703,
      "training_loss": 4.347097396850586
    },
    {
      "epoch": 0.4175609756097561,
      "grad_norm": 33.080162048339844,
      "learning_rate": 1e-05,
      "loss": 6.1704,
      "step": 7704
    },
    {
      "epoch": 0.4175609756097561,
      "step": 7704,
      "training_loss": 6.631043910980225
    },
    {
      "epoch": 0.4176151761517615,
      "step": 7705,
      "training_loss": 6.889456748962402
    },
    {
      "epoch": 0.4176693766937669,
      "step": 7706,
      "training_loss": 7.3994059562683105
    },
    {
      "epoch": 0.41772357723577236,
      "step": 7707,
      "training_loss": 7.503105163574219
    },
    {
      "epoch": 0.4177777777777778,
      "grad_norm": 24.463356018066406,
      "learning_rate": 1e-05,
      "loss": 7.1058,
      "step": 7708
    },
    {
      "epoch": 0.4177777777777778,
      "step": 7708,
      "training_loss": 6.267416954040527
    },
    {
      "epoch": 0.4178319783197832,
      "step": 7709,
      "training_loss": 6.874973773956299
    },
    {
      "epoch": 0.41788617886178864,
      "step": 7710,
      "training_loss": 7.348937511444092
    },
    {
      "epoch": 0.417940379403794,
      "step": 7711,
      "training_loss": 7.14518404006958
    },
    {
      "epoch": 0.41799457994579947,
      "grad_norm": 21.063989639282227,
      "learning_rate": 1e-05,
      "loss": 6.9091,
      "step": 7712
    },
    {
      "epoch": 0.41799457994579947,
      "step": 7712,
      "training_loss": 4.143589973449707
    },
    {
      "epoch": 0.41804878048780486,
      "step": 7713,
      "training_loss": 7.567941665649414
    },
    {
      "epoch": 0.4181029810298103,
      "step": 7714,
      "training_loss": 5.759700298309326
    },
    {
      "epoch": 0.4181571815718157,
      "step": 7715,
      "training_loss": 6.612008094787598
    },
    {
      "epoch": 0.41821138211382114,
      "grad_norm": 24.390485763549805,
      "learning_rate": 1e-05,
      "loss": 6.0208,
      "step": 7716
    },
    {
      "epoch": 0.41821138211382114,
      "step": 7716,
      "training_loss": 8.00241470336914
    },
    {
      "epoch": 0.4182655826558266,
      "step": 7717,
      "training_loss": 6.343388557434082
    },
    {
      "epoch": 0.418319783197832,
      "step": 7718,
      "training_loss": 7.205050468444824
    },
    {
      "epoch": 0.4183739837398374,
      "step": 7719,
      "training_loss": 5.87721061706543
    },
    {
      "epoch": 0.4184281842818428,
      "grad_norm": 32.57801055908203,
      "learning_rate": 1e-05,
      "loss": 6.857,
      "step": 7720
    },
    {
      "epoch": 0.4184281842818428,
      "step": 7720,
      "training_loss": 8.068928718566895
    },
    {
      "epoch": 0.41848238482384825,
      "step": 7721,
      "training_loss": 7.034666538238525
    },
    {
      "epoch": 0.41853658536585364,
      "step": 7722,
      "training_loss": 8.419090270996094
    },
    {
      "epoch": 0.4185907859078591,
      "step": 7723,
      "training_loss": 8.118354797363281
    },
    {
      "epoch": 0.4186449864498645,
      "grad_norm": 18.819799423217773,
      "learning_rate": 1e-05,
      "loss": 7.9103,
      "step": 7724
    },
    {
      "epoch": 0.4186449864498645,
      "step": 7724,
      "training_loss": 6.401317596435547
    },
    {
      "epoch": 0.4186991869918699,
      "step": 7725,
      "training_loss": 5.383295059204102
    },
    {
      "epoch": 0.41875338753387537,
      "step": 7726,
      "training_loss": 7.197575569152832
    },
    {
      "epoch": 0.41880758807588075,
      "step": 7727,
      "training_loss": 7.412471771240234
    },
    {
      "epoch": 0.4188617886178862,
      "grad_norm": 24.57183074951172,
      "learning_rate": 1e-05,
      "loss": 6.5987,
      "step": 7728
    },
    {
      "epoch": 0.4188617886178862,
      "step": 7728,
      "training_loss": 5.229171276092529
    },
    {
      "epoch": 0.4189159891598916,
      "step": 7729,
      "training_loss": 5.852801322937012
    },
    {
      "epoch": 0.41897018970189703,
      "step": 7730,
      "training_loss": 6.702392578125
    },
    {
      "epoch": 0.4190243902439024,
      "step": 7731,
      "training_loss": 6.61341667175293
    },
    {
      "epoch": 0.41907859078590787,
      "grad_norm": 41.63737487792969,
      "learning_rate": 1e-05,
      "loss": 6.0994,
      "step": 7732
    },
    {
      "epoch": 0.41907859078590787,
      "step": 7732,
      "training_loss": 7.58092737197876
    },
    {
      "epoch": 0.41913279132791326,
      "step": 7733,
      "training_loss": 7.096378326416016
    },
    {
      "epoch": 0.4191869918699187,
      "step": 7734,
      "training_loss": 8.424118041992188
    },
    {
      "epoch": 0.41924119241192415,
      "step": 7735,
      "training_loss": 6.827807426452637
    },
    {
      "epoch": 0.41929539295392954,
      "grad_norm": 18.581838607788086,
      "learning_rate": 1e-05,
      "loss": 7.4823,
      "step": 7736
    },
    {
      "epoch": 0.41929539295392954,
      "step": 7736,
      "training_loss": 5.171513080596924
    },
    {
      "epoch": 0.419349593495935,
      "step": 7737,
      "training_loss": 6.37436056137085
    },
    {
      "epoch": 0.41940379403794037,
      "step": 7738,
      "training_loss": 6.864777088165283
    },
    {
      "epoch": 0.4194579945799458,
      "step": 7739,
      "training_loss": 6.690176010131836
    },
    {
      "epoch": 0.4195121951219512,
      "grad_norm": 31.065086364746094,
      "learning_rate": 1e-05,
      "loss": 6.2752,
      "step": 7740
    },
    {
      "epoch": 0.4195121951219512,
      "step": 7740,
      "training_loss": 4.859755992889404
    },
    {
      "epoch": 0.41956639566395665,
      "step": 7741,
      "training_loss": 7.427501678466797
    },
    {
      "epoch": 0.41962059620596204,
      "step": 7742,
      "training_loss": 7.524186611175537
    },
    {
      "epoch": 0.4196747967479675,
      "step": 7743,
      "training_loss": 5.945630073547363
    },
    {
      "epoch": 0.4197289972899729,
      "grad_norm": 27.332609176635742,
      "learning_rate": 1e-05,
      "loss": 6.4393,
      "step": 7744
    },
    {
      "epoch": 0.4197289972899729,
      "step": 7744,
      "training_loss": 6.4751057624816895
    },
    {
      "epoch": 0.4197831978319783,
      "step": 7745,
      "training_loss": 7.686435222625732
    },
    {
      "epoch": 0.41983739837398376,
      "step": 7746,
      "training_loss": 7.237191200256348
    },
    {
      "epoch": 0.41989159891598915,
      "step": 7747,
      "training_loss": 6.08746862411499
    },
    {
      "epoch": 0.4199457994579946,
      "grad_norm": 20.220949172973633,
      "learning_rate": 1e-05,
      "loss": 6.8716,
      "step": 7748
    },
    {
      "epoch": 0.4199457994579946,
      "step": 7748,
      "training_loss": 7.124852657318115
    },
    {
      "epoch": 0.42,
      "step": 7749,
      "training_loss": 6.569269180297852
    },
    {
      "epoch": 0.42005420054200543,
      "step": 7750,
      "training_loss": 6.674663066864014
    },
    {
      "epoch": 0.4201084010840108,
      "step": 7751,
      "training_loss": 6.976460933685303
    },
    {
      "epoch": 0.42016260162601626,
      "grad_norm": 58.44417190551758,
      "learning_rate": 1e-05,
      "loss": 6.8363,
      "step": 7752
    },
    {
      "epoch": 0.42016260162601626,
      "step": 7752,
      "training_loss": 5.834338665008545
    },
    {
      "epoch": 0.42021680216802165,
      "step": 7753,
      "training_loss": 4.24915075302124
    },
    {
      "epoch": 0.4202710027100271,
      "step": 7754,
      "training_loss": 6.2704854011535645
    },
    {
      "epoch": 0.42032520325203254,
      "step": 7755,
      "training_loss": 5.295060634613037
    },
    {
      "epoch": 0.42037940379403793,
      "grad_norm": 19.675804138183594,
      "learning_rate": 1e-05,
      "loss": 5.4123,
      "step": 7756
    },
    {
      "epoch": 0.42037940379403793,
      "step": 7756,
      "training_loss": 6.337949752807617
    },
    {
      "epoch": 0.4204336043360434,
      "step": 7757,
      "training_loss": 7.099472522735596
    },
    {
      "epoch": 0.42048780487804877,
      "step": 7758,
      "training_loss": 6.383653163909912
    },
    {
      "epoch": 0.4205420054200542,
      "step": 7759,
      "training_loss": 6.251302242279053
    },
    {
      "epoch": 0.4205962059620596,
      "grad_norm": 25.72007942199707,
      "learning_rate": 1e-05,
      "loss": 6.5181,
      "step": 7760
    },
    {
      "epoch": 0.4205962059620596,
      "step": 7760,
      "training_loss": 7.1793599128723145
    },
    {
      "epoch": 0.42065040650406504,
      "step": 7761,
      "training_loss": 6.289941310882568
    },
    {
      "epoch": 0.42070460704607043,
      "step": 7762,
      "training_loss": 7.241664886474609
    },
    {
      "epoch": 0.4207588075880759,
      "step": 7763,
      "training_loss": 5.625668525695801
    },
    {
      "epoch": 0.4208130081300813,
      "grad_norm": 25.290437698364258,
      "learning_rate": 1e-05,
      "loss": 6.5842,
      "step": 7764
    },
    {
      "epoch": 0.4208130081300813,
      "step": 7764,
      "training_loss": 6.682417869567871
    },
    {
      "epoch": 0.4208672086720867,
      "step": 7765,
      "training_loss": 7.283472061157227
    },
    {
      "epoch": 0.42092140921409216,
      "step": 7766,
      "training_loss": 7.340407371520996
    },
    {
      "epoch": 0.42097560975609755,
      "step": 7767,
      "training_loss": 6.91591739654541
    },
    {
      "epoch": 0.421029810298103,
      "grad_norm": 69.92362976074219,
      "learning_rate": 1e-05,
      "loss": 7.0556,
      "step": 7768
    },
    {
      "epoch": 0.421029810298103,
      "step": 7768,
      "training_loss": 7.431829929351807
    },
    {
      "epoch": 0.4210840108401084,
      "step": 7769,
      "training_loss": 6.807079315185547
    },
    {
      "epoch": 0.4211382113821138,
      "step": 7770,
      "training_loss": 6.779428482055664
    },
    {
      "epoch": 0.4211924119241192,
      "step": 7771,
      "training_loss": 7.829164505004883
    },
    {
      "epoch": 0.42124661246612466,
      "grad_norm": 20.84551429748535,
      "learning_rate": 1e-05,
      "loss": 7.2119,
      "step": 7772
    },
    {
      "epoch": 0.42124661246612466,
      "step": 7772,
      "training_loss": 6.96693229675293
    },
    {
      "epoch": 0.4213008130081301,
      "step": 7773,
      "training_loss": 7.318663120269775
    },
    {
      "epoch": 0.4213550135501355,
      "step": 7774,
      "training_loss": 6.508758068084717
    },
    {
      "epoch": 0.42140921409214094,
      "step": 7775,
      "training_loss": 7.212369441986084
    },
    {
      "epoch": 0.4214634146341463,
      "grad_norm": 22.10368537902832,
      "learning_rate": 1e-05,
      "loss": 7.0017,
      "step": 7776
    },
    {
      "epoch": 0.4214634146341463,
      "step": 7776,
      "training_loss": 6.729766368865967
    },
    {
      "epoch": 0.42151761517615177,
      "step": 7777,
      "training_loss": 6.919695854187012
    },
    {
      "epoch": 0.42157181571815716,
      "step": 7778,
      "training_loss": 7.701744556427002
    },
    {
      "epoch": 0.4216260162601626,
      "step": 7779,
      "training_loss": 6.534665107727051
    },
    {
      "epoch": 0.421680216802168,
      "grad_norm": 35.078487396240234,
      "learning_rate": 1e-05,
      "loss": 6.9715,
      "step": 7780
    },
    {
      "epoch": 0.421680216802168,
      "step": 7780,
      "training_loss": 5.732327938079834
    },
    {
      "epoch": 0.42173441734417344,
      "step": 7781,
      "training_loss": 7.564126014709473
    },
    {
      "epoch": 0.4217886178861789,
      "step": 7782,
      "training_loss": 6.052468776702881
    },
    {
      "epoch": 0.4218428184281843,
      "step": 7783,
      "training_loss": 6.4098310470581055
    },
    {
      "epoch": 0.4218970189701897,
      "grad_norm": 26.640438079833984,
      "learning_rate": 1e-05,
      "loss": 6.4397,
      "step": 7784
    },
    {
      "epoch": 0.4218970189701897,
      "step": 7784,
      "training_loss": 7.502034664154053
    },
    {
      "epoch": 0.4219512195121951,
      "step": 7785,
      "training_loss": 6.510094165802002
    },
    {
      "epoch": 0.42200542005420055,
      "step": 7786,
      "training_loss": 7.684813499450684
    },
    {
      "epoch": 0.42205962059620594,
      "step": 7787,
      "training_loss": 6.871053695678711
    },
    {
      "epoch": 0.4221138211382114,
      "grad_norm": 39.67859649658203,
      "learning_rate": 1e-05,
      "loss": 7.142,
      "step": 7788
    },
    {
      "epoch": 0.4221138211382114,
      "step": 7788,
      "training_loss": 6.864894390106201
    },
    {
      "epoch": 0.4221680216802168,
      "step": 7789,
      "training_loss": 6.309174537658691
    },
    {
      "epoch": 0.4222222222222222,
      "step": 7790,
      "training_loss": 7.228411674499512
    },
    {
      "epoch": 0.42227642276422767,
      "step": 7791,
      "training_loss": 6.949085712432861
    },
    {
      "epoch": 0.42233062330623306,
      "grad_norm": 24.76970100402832,
      "learning_rate": 1e-05,
      "loss": 6.8379,
      "step": 7792
    },
    {
      "epoch": 0.42233062330623306,
      "step": 7792,
      "training_loss": 7.406891345977783
    },
    {
      "epoch": 0.4223848238482385,
      "step": 7793,
      "training_loss": 7.076571941375732
    },
    {
      "epoch": 0.4224390243902439,
      "step": 7794,
      "training_loss": 9.45516300201416
    },
    {
      "epoch": 0.42249322493224933,
      "step": 7795,
      "training_loss": 4.166803359985352
    },
    {
      "epoch": 0.4225474254742547,
      "grad_norm": 31.0424747467041,
      "learning_rate": 1e-05,
      "loss": 7.0264,
      "step": 7796
    },
    {
      "epoch": 0.4225474254742547,
      "step": 7796,
      "training_loss": 7.301784515380859
    },
    {
      "epoch": 0.42260162601626017,
      "step": 7797,
      "training_loss": 7.484076976776123
    },
    {
      "epoch": 0.42265582655826556,
      "step": 7798,
      "training_loss": 6.873716354370117
    },
    {
      "epoch": 0.422710027100271,
      "step": 7799,
      "training_loss": 5.6030683517456055
    },
    {
      "epoch": 0.42276422764227645,
      "grad_norm": 24.62680435180664,
      "learning_rate": 1e-05,
      "loss": 6.8157,
      "step": 7800
    },
    {
      "epoch": 0.42276422764227645,
      "step": 7800,
      "training_loss": 7.3858819007873535
    },
    {
      "epoch": 0.42281842818428184,
      "step": 7801,
      "training_loss": 6.986471176147461
    },
    {
      "epoch": 0.4228726287262873,
      "step": 7802,
      "training_loss": 7.074446678161621
    },
    {
      "epoch": 0.42292682926829267,
      "step": 7803,
      "training_loss": 6.9121317863464355
    },
    {
      "epoch": 0.4229810298102981,
      "grad_norm": 17.55544662475586,
      "learning_rate": 1e-05,
      "loss": 7.0897,
      "step": 7804
    },
    {
      "epoch": 0.4229810298102981,
      "step": 7804,
      "training_loss": 6.8040876388549805
    },
    {
      "epoch": 0.4230352303523035,
      "step": 7805,
      "training_loss": 5.387664318084717
    },
    {
      "epoch": 0.42308943089430895,
      "step": 7806,
      "training_loss": 3.782909631729126
    },
    {
      "epoch": 0.42314363143631434,
      "step": 7807,
      "training_loss": 6.057889461517334
    },
    {
      "epoch": 0.4231978319783198,
      "grad_norm": 38.88338088989258,
      "learning_rate": 1e-05,
      "loss": 5.5081,
      "step": 7808
    },
    {
      "epoch": 0.4231978319783198,
      "step": 7808,
      "training_loss": 5.052989482879639
    },
    {
      "epoch": 0.4232520325203252,
      "step": 7809,
      "training_loss": 6.862192153930664
    },
    {
      "epoch": 0.4233062330623306,
      "step": 7810,
      "training_loss": 7.688868522644043
    },
    {
      "epoch": 0.42336043360433606,
      "step": 7811,
      "training_loss": 5.7483229637146
    },
    {
      "epoch": 0.42341463414634145,
      "grad_norm": 24.028352737426758,
      "learning_rate": 1e-05,
      "loss": 6.3381,
      "step": 7812
    },
    {
      "epoch": 0.42341463414634145,
      "step": 7812,
      "training_loss": 7.488256931304932
    },
    {
      "epoch": 0.4234688346883469,
      "step": 7813,
      "training_loss": 6.254744529724121
    },
    {
      "epoch": 0.4235230352303523,
      "step": 7814,
      "training_loss": 7.998236179351807
    },
    {
      "epoch": 0.42357723577235773,
      "step": 7815,
      "training_loss": 7.260943412780762
    },
    {
      "epoch": 0.4236314363143631,
      "grad_norm": 19.27342987060547,
      "learning_rate": 1e-05,
      "loss": 7.2505,
      "step": 7816
    },
    {
      "epoch": 0.4236314363143631,
      "step": 7816,
      "training_loss": 6.924376964569092
    },
    {
      "epoch": 0.42368563685636856,
      "step": 7817,
      "training_loss": 7.069772243499756
    },
    {
      "epoch": 0.423739837398374,
      "step": 7818,
      "training_loss": 7.2915849685668945
    },
    {
      "epoch": 0.4237940379403794,
      "step": 7819,
      "training_loss": 6.922695636749268
    },
    {
      "epoch": 0.42384823848238484,
      "grad_norm": 30.71107292175293,
      "learning_rate": 1e-05,
      "loss": 7.0521,
      "step": 7820
    },
    {
      "epoch": 0.42384823848238484,
      "step": 7820,
      "training_loss": 6.5576605796813965
    },
    {
      "epoch": 0.42390243902439023,
      "step": 7821,
      "training_loss": 3.973170280456543
    },
    {
      "epoch": 0.4239566395663957,
      "step": 7822,
      "training_loss": 5.320155620574951
    },
    {
      "epoch": 0.42401084010840107,
      "step": 7823,
      "training_loss": 7.016238212585449
    },
    {
      "epoch": 0.4240650406504065,
      "grad_norm": 53.354278564453125,
      "learning_rate": 1e-05,
      "loss": 5.7168,
      "step": 7824
    },
    {
      "epoch": 0.4240650406504065,
      "step": 7824,
      "training_loss": 7.819061756134033
    },
    {
      "epoch": 0.4241192411924119,
      "step": 7825,
      "training_loss": 4.881776332855225
    },
    {
      "epoch": 0.42417344173441734,
      "step": 7826,
      "training_loss": 6.622987747192383
    },
    {
      "epoch": 0.4242276422764228,
      "step": 7827,
      "training_loss": 7.3280110359191895
    },
    {
      "epoch": 0.4242818428184282,
      "grad_norm": 27.74319076538086,
      "learning_rate": 1e-05,
      "loss": 6.663,
      "step": 7828
    },
    {
      "epoch": 0.4242818428184282,
      "step": 7828,
      "training_loss": 6.421045780181885
    },
    {
      "epoch": 0.4243360433604336,
      "step": 7829,
      "training_loss": 6.118679523468018
    },
    {
      "epoch": 0.424390243902439,
      "step": 7830,
      "training_loss": 5.826162815093994
    },
    {
      "epoch": 0.42444444444444446,
      "step": 7831,
      "training_loss": 8.043225288391113
    },
    {
      "epoch": 0.42449864498644985,
      "grad_norm": 25.917627334594727,
      "learning_rate": 1e-05,
      "loss": 6.6023,
      "step": 7832
    },
    {
      "epoch": 0.42449864498644985,
      "step": 7832,
      "training_loss": 5.8083038330078125
    },
    {
      "epoch": 0.4245528455284553,
      "step": 7833,
      "training_loss": 6.805800914764404
    },
    {
      "epoch": 0.4246070460704607,
      "step": 7834,
      "training_loss": 4.261364936828613
    },
    {
      "epoch": 0.4246612466124661,
      "step": 7835,
      "training_loss": 6.186458110809326
    },
    {
      "epoch": 0.42471544715447157,
      "grad_norm": 33.43647003173828,
      "learning_rate": 1e-05,
      "loss": 5.7655,
      "step": 7836
    },
    {
      "epoch": 0.42471544715447157,
      "step": 7836,
      "training_loss": 3.1403656005859375
    },
    {
      "epoch": 0.42476964769647696,
      "step": 7837,
      "training_loss": 7.637777328491211
    },
    {
      "epoch": 0.4248238482384824,
      "step": 7838,
      "training_loss": 8.215968132019043
    },
    {
      "epoch": 0.4248780487804878,
      "step": 7839,
      "training_loss": 6.735254287719727
    },
    {
      "epoch": 0.42493224932249324,
      "grad_norm": 27.3978214263916,
      "learning_rate": 1e-05,
      "loss": 6.4323,
      "step": 7840
    },
    {
      "epoch": 0.42493224932249324,
      "step": 7840,
      "training_loss": 6.73409366607666
    },
    {
      "epoch": 0.4249864498644986,
      "step": 7841,
      "training_loss": 6.653261661529541
    },
    {
      "epoch": 0.4250406504065041,
      "step": 7842,
      "training_loss": 7.277663707733154
    },
    {
      "epoch": 0.42509485094850946,
      "step": 7843,
      "training_loss": 6.559392929077148
    },
    {
      "epoch": 0.4251490514905149,
      "grad_norm": 24.54143524169922,
      "learning_rate": 1e-05,
      "loss": 6.8061,
      "step": 7844
    },
    {
      "epoch": 0.4251490514905149,
      "step": 7844,
      "training_loss": 7.472454071044922
    },
    {
      "epoch": 0.42520325203252035,
      "step": 7845,
      "training_loss": 3.719482660293579
    },
    {
      "epoch": 0.42525745257452574,
      "step": 7846,
      "training_loss": 7.452087879180908
    },
    {
      "epoch": 0.4253116531165312,
      "step": 7847,
      "training_loss": 6.6780900955200195
    },
    {
      "epoch": 0.4253658536585366,
      "grad_norm": 26.08942985534668,
      "learning_rate": 1e-05,
      "loss": 6.3305,
      "step": 7848
    },
    {
      "epoch": 0.4253658536585366,
      "step": 7848,
      "training_loss": 6.078473091125488
    },
    {
      "epoch": 0.425420054200542,
      "step": 7849,
      "training_loss": 7.153534889221191
    },
    {
      "epoch": 0.4254742547425474,
      "step": 7850,
      "training_loss": 6.3733744621276855
    },
    {
      "epoch": 0.42552845528455285,
      "step": 7851,
      "training_loss": 6.332329750061035
    },
    {
      "epoch": 0.42558265582655824,
      "grad_norm": 23.064266204833984,
      "learning_rate": 1e-05,
      "loss": 6.4844,
      "step": 7852
    },
    {
      "epoch": 0.42558265582655824,
      "step": 7852,
      "training_loss": 7.513256072998047
    },
    {
      "epoch": 0.4256368563685637,
      "step": 7853,
      "training_loss": 6.436367034912109
    },
    {
      "epoch": 0.42569105691056913,
      "step": 7854,
      "training_loss": 7.7619948387146
    },
    {
      "epoch": 0.4257452574525745,
      "step": 7855,
      "training_loss": 7.905157566070557
    },
    {
      "epoch": 0.42579945799457997,
      "grad_norm": 28.574052810668945,
      "learning_rate": 1e-05,
      "loss": 7.4042,
      "step": 7856
    },
    {
      "epoch": 0.42579945799457997,
      "step": 7856,
      "training_loss": 5.589784145355225
    },
    {
      "epoch": 0.42585365853658536,
      "step": 7857,
      "training_loss": 6.623550891876221
    },
    {
      "epoch": 0.4259078590785908,
      "step": 7858,
      "training_loss": 7.031798839569092
    },
    {
      "epoch": 0.4259620596205962,
      "step": 7859,
      "training_loss": 7.843588829040527
    },
    {
      "epoch": 0.42601626016260163,
      "grad_norm": 16.350257873535156,
      "learning_rate": 1e-05,
      "loss": 6.7722,
      "step": 7860
    },
    {
      "epoch": 0.42601626016260163,
      "step": 7860,
      "training_loss": 6.851053714752197
    },
    {
      "epoch": 0.426070460704607,
      "step": 7861,
      "training_loss": 6.944736003875732
    },
    {
      "epoch": 0.42612466124661247,
      "step": 7862,
      "training_loss": 5.240780830383301
    },
    {
      "epoch": 0.4261788617886179,
      "step": 7863,
      "training_loss": 7.138175964355469
    },
    {
      "epoch": 0.4262330623306233,
      "grad_norm": 31.178518295288086,
      "learning_rate": 1e-05,
      "loss": 6.5437,
      "step": 7864
    },
    {
      "epoch": 0.4262330623306233,
      "step": 7864,
      "training_loss": 3.722895622253418
    },
    {
      "epoch": 0.42628726287262875,
      "step": 7865,
      "training_loss": 7.654791355133057
    },
    {
      "epoch": 0.42634146341463414,
      "step": 7866,
      "training_loss": 7.186412811279297
    },
    {
      "epoch": 0.4263956639566396,
      "step": 7867,
      "training_loss": 8.30303955078125
    },
    {
      "epoch": 0.42644986449864497,
      "grad_norm": 43.84754180908203,
      "learning_rate": 1e-05,
      "loss": 6.7168,
      "step": 7868
    },
    {
      "epoch": 0.42644986449864497,
      "step": 7868,
      "training_loss": 6.818877696990967
    },
    {
      "epoch": 0.4265040650406504,
      "step": 7869,
      "training_loss": 7.9246506690979
    },
    {
      "epoch": 0.4265582655826558,
      "step": 7870,
      "training_loss": 6.510461330413818
    },
    {
      "epoch": 0.42661246612466125,
      "step": 7871,
      "training_loss": 7.190768718719482
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 16.411664962768555,
      "learning_rate": 1e-05,
      "loss": 7.1112,
      "step": 7872
    },
    {
      "epoch": 0.4266666666666667,
      "step": 7872,
      "training_loss": 6.504652500152588
    },
    {
      "epoch": 0.4267208672086721,
      "step": 7873,
      "training_loss": 7.5367431640625
    },
    {
      "epoch": 0.42677506775067753,
      "step": 7874,
      "training_loss": 8.598840713500977
    },
    {
      "epoch": 0.4268292682926829,
      "step": 7875,
      "training_loss": 6.934776782989502
    },
    {
      "epoch": 0.42688346883468836,
      "grad_norm": 23.010074615478516,
      "learning_rate": 1e-05,
      "loss": 7.3938,
      "step": 7876
    },
    {
      "epoch": 0.42688346883468836,
      "step": 7876,
      "training_loss": 5.5561747550964355
    },
    {
      "epoch": 0.42693766937669375,
      "step": 7877,
      "training_loss": 6.491856098175049
    },
    {
      "epoch": 0.4269918699186992,
      "step": 7878,
      "training_loss": 7.779551029205322
    },
    {
      "epoch": 0.4270460704607046,
      "step": 7879,
      "training_loss": 7.290308475494385
    },
    {
      "epoch": 0.42710027100271003,
      "grad_norm": 38.83766555786133,
      "learning_rate": 1e-05,
      "loss": 6.7795,
      "step": 7880
    },
    {
      "epoch": 0.42710027100271003,
      "step": 7880,
      "training_loss": 7.408020973205566
    },
    {
      "epoch": 0.4271544715447154,
      "step": 7881,
      "training_loss": 6.287638187408447
    },
    {
      "epoch": 0.42720867208672086,
      "step": 7882,
      "training_loss": 6.474941730499268
    },
    {
      "epoch": 0.4272628726287263,
      "step": 7883,
      "training_loss": 6.014929294586182
    },
    {
      "epoch": 0.4273170731707317,
      "grad_norm": 23.2979736328125,
      "learning_rate": 1e-05,
      "loss": 6.5464,
      "step": 7884
    },
    {
      "epoch": 0.4273170731707317,
      "step": 7884,
      "training_loss": 7.660056114196777
    },
    {
      "epoch": 0.42737127371273714,
      "step": 7885,
      "training_loss": 7.805360317230225
    },
    {
      "epoch": 0.42742547425474253,
      "step": 7886,
      "training_loss": 6.847892761230469
    },
    {
      "epoch": 0.427479674796748,
      "step": 7887,
      "training_loss": 6.875881671905518
    },
    {
      "epoch": 0.42753387533875337,
      "grad_norm": 24.01569938659668,
      "learning_rate": 1e-05,
      "loss": 7.2973,
      "step": 7888
    },
    {
      "epoch": 0.42753387533875337,
      "step": 7888,
      "training_loss": 4.630331993103027
    },
    {
      "epoch": 0.4275880758807588,
      "step": 7889,
      "training_loss": 7.3683953285217285
    },
    {
      "epoch": 0.4276422764227642,
      "step": 7890,
      "training_loss": 7.6905293464660645
    },
    {
      "epoch": 0.42769647696476965,
      "step": 7891,
      "training_loss": 7.223028659820557
    },
    {
      "epoch": 0.4277506775067751,
      "grad_norm": 26.99675178527832,
      "learning_rate": 1e-05,
      "loss": 6.7281,
      "step": 7892
    },
    {
      "epoch": 0.4277506775067751,
      "step": 7892,
      "training_loss": 7.698811054229736
    },
    {
      "epoch": 0.4278048780487805,
      "step": 7893,
      "training_loss": 6.224588871002197
    },
    {
      "epoch": 0.4278590785907859,
      "step": 7894,
      "training_loss": 6.086231708526611
    },
    {
      "epoch": 0.4279132791327913,
      "step": 7895,
      "training_loss": 8.603888511657715
    },
    {
      "epoch": 0.42796747967479676,
      "grad_norm": 28.59384536743164,
      "learning_rate": 1e-05,
      "loss": 7.1534,
      "step": 7896
    },
    {
      "epoch": 0.42796747967479676,
      "step": 7896,
      "training_loss": 8.454004287719727
    },
    {
      "epoch": 0.42802168021680215,
      "step": 7897,
      "training_loss": 6.575347423553467
    },
    {
      "epoch": 0.4280758807588076,
      "step": 7898,
      "training_loss": 6.996267318725586
    },
    {
      "epoch": 0.428130081300813,
      "step": 7899,
      "training_loss": 6.466290473937988
    },
    {
      "epoch": 0.4281842818428184,
      "grad_norm": 40.723506927490234,
      "learning_rate": 1e-05,
      "loss": 7.123,
      "step": 7900
    },
    {
      "epoch": 0.4281842818428184,
      "step": 7900,
      "training_loss": 6.253154277801514
    },
    {
      "epoch": 0.42823848238482387,
      "step": 7901,
      "training_loss": 7.334366321563721
    },
    {
      "epoch": 0.42829268292682926,
      "step": 7902,
      "training_loss": 6.912310600280762
    },
    {
      "epoch": 0.4283468834688347,
      "step": 7903,
      "training_loss": 6.532101154327393
    },
    {
      "epoch": 0.4284010840108401,
      "grad_norm": 23.885108947753906,
      "learning_rate": 1e-05,
      "loss": 6.758,
      "step": 7904
    },
    {
      "epoch": 0.4284010840108401,
      "step": 7904,
      "training_loss": 7.468868255615234
    },
    {
      "epoch": 0.42845528455284554,
      "step": 7905,
      "training_loss": 4.04206657409668
    },
    {
      "epoch": 0.42850948509485093,
      "step": 7906,
      "training_loss": 7.502621650695801
    },
    {
      "epoch": 0.4285636856368564,
      "step": 7907,
      "training_loss": 6.590845584869385
    },
    {
      "epoch": 0.42861788617886176,
      "grad_norm": 22.217010498046875,
      "learning_rate": 1e-05,
      "loss": 6.4011,
      "step": 7908
    },
    {
      "epoch": 0.42861788617886176,
      "step": 7908,
      "training_loss": 7.216558933258057
    },
    {
      "epoch": 0.4286720867208672,
      "step": 7909,
      "training_loss": 6.062840461730957
    },
    {
      "epoch": 0.42872628726287265,
      "step": 7910,
      "training_loss": 6.886486053466797
    },
    {
      "epoch": 0.42878048780487804,
      "step": 7911,
      "training_loss": 7.08745813369751
    },
    {
      "epoch": 0.4288346883468835,
      "grad_norm": 29.40376091003418,
      "learning_rate": 1e-05,
      "loss": 6.8133,
      "step": 7912
    },
    {
      "epoch": 0.4288346883468835,
      "step": 7912,
      "training_loss": 6.243493556976318
    },
    {
      "epoch": 0.4288888888888889,
      "step": 7913,
      "training_loss": 8.489448547363281
    },
    {
      "epoch": 0.4289430894308943,
      "step": 7914,
      "training_loss": 7.203141212463379
    },
    {
      "epoch": 0.4289972899728997,
      "step": 7915,
      "training_loss": 7.3962507247924805
    },
    {
      "epoch": 0.42905149051490515,
      "grad_norm": 18.158260345458984,
      "learning_rate": 1e-05,
      "loss": 7.3331,
      "step": 7916
    },
    {
      "epoch": 0.42905149051490515,
      "step": 7916,
      "training_loss": 6.859111309051514
    },
    {
      "epoch": 0.42910569105691054,
      "step": 7917,
      "training_loss": 6.645291328430176
    },
    {
      "epoch": 0.429159891598916,
      "step": 7918,
      "training_loss": 7.990327835083008
    },
    {
      "epoch": 0.42921409214092143,
      "step": 7919,
      "training_loss": 7.8530802726745605
    },
    {
      "epoch": 0.4292682926829268,
      "grad_norm": 35.186649322509766,
      "learning_rate": 1e-05,
      "loss": 7.337,
      "step": 7920
    },
    {
      "epoch": 0.4292682926829268,
      "step": 7920,
      "training_loss": 7.138495922088623
    },
    {
      "epoch": 0.42932249322493227,
      "step": 7921,
      "training_loss": 7.996737957000732
    },
    {
      "epoch": 0.42937669376693766,
      "step": 7922,
      "training_loss": 6.548360347747803
    },
    {
      "epoch": 0.4294308943089431,
      "step": 7923,
      "training_loss": 6.094964027404785
    },
    {
      "epoch": 0.4294850948509485,
      "grad_norm": 33.6556510925293,
      "learning_rate": 1e-05,
      "loss": 6.9446,
      "step": 7924
    },
    {
      "epoch": 0.4294850948509485,
      "step": 7924,
      "training_loss": 6.421669960021973
    },
    {
      "epoch": 0.42953929539295393,
      "step": 7925,
      "training_loss": 10.902554512023926
    },
    {
      "epoch": 0.4295934959349593,
      "step": 7926,
      "training_loss": 6.148611545562744
    },
    {
      "epoch": 0.42964769647696477,
      "step": 7927,
      "training_loss": 6.972262382507324
    },
    {
      "epoch": 0.4297018970189702,
      "grad_norm": 20.820514678955078,
      "learning_rate": 1e-05,
      "loss": 7.6113,
      "step": 7928
    },
    {
      "epoch": 0.4297018970189702,
      "step": 7928,
      "training_loss": 7.231574535369873
    },
    {
      "epoch": 0.4297560975609756,
      "step": 7929,
      "training_loss": 6.92225980758667
    },
    {
      "epoch": 0.42981029810298105,
      "step": 7930,
      "training_loss": 4.361027717590332
    },
    {
      "epoch": 0.42986449864498644,
      "step": 7931,
      "training_loss": 8.107158660888672
    },
    {
      "epoch": 0.4299186991869919,
      "grad_norm": 44.94450378417969,
      "learning_rate": 1e-05,
      "loss": 6.6555,
      "step": 7932
    },
    {
      "epoch": 0.4299186991869919,
      "step": 7932,
      "training_loss": 6.405442237854004
    },
    {
      "epoch": 0.42997289972899727,
      "step": 7933,
      "training_loss": 6.563603401184082
    },
    {
      "epoch": 0.4300271002710027,
      "step": 7934,
      "training_loss": 4.243002414703369
    },
    {
      "epoch": 0.4300813008130081,
      "step": 7935,
      "training_loss": 5.330835819244385
    },
    {
      "epoch": 0.43013550135501355,
      "grad_norm": 66.8744888305664,
      "learning_rate": 1e-05,
      "loss": 5.6357,
      "step": 7936
    },
    {
      "epoch": 0.43013550135501355,
      "step": 7936,
      "training_loss": 6.758170127868652
    },
    {
      "epoch": 0.430189701897019,
      "step": 7937,
      "training_loss": 5.99799919128418
    },
    {
      "epoch": 0.4302439024390244,
      "step": 7938,
      "training_loss": 7.698978900909424
    },
    {
      "epoch": 0.43029810298102983,
      "step": 7939,
      "training_loss": 7.033547401428223
    },
    {
      "epoch": 0.4303523035230352,
      "grad_norm": 43.1019287109375,
      "learning_rate": 1e-05,
      "loss": 6.8722,
      "step": 7940
    },
    {
      "epoch": 0.4303523035230352,
      "step": 7940,
      "training_loss": 7.562690734863281
    },
    {
      "epoch": 0.43040650406504066,
      "step": 7941,
      "training_loss": 7.807316303253174
    },
    {
      "epoch": 0.43046070460704605,
      "step": 7942,
      "training_loss": 6.903937816619873
    },
    {
      "epoch": 0.4305149051490515,
      "step": 7943,
      "training_loss": 7.120881080627441
    },
    {
      "epoch": 0.4305691056910569,
      "grad_norm": 51.586753845214844,
      "learning_rate": 1e-05,
      "loss": 7.3487,
      "step": 7944
    },
    {
      "epoch": 0.4305691056910569,
      "step": 7944,
      "training_loss": 7.627832412719727
    },
    {
      "epoch": 0.43062330623306233,
      "step": 7945,
      "training_loss": 7.282935619354248
    },
    {
      "epoch": 0.4306775067750678,
      "step": 7946,
      "training_loss": 5.468019485473633
    },
    {
      "epoch": 0.43073170731707316,
      "step": 7947,
      "training_loss": 7.221094131469727
    },
    {
      "epoch": 0.4307859078590786,
      "grad_norm": 39.89152526855469,
      "learning_rate": 1e-05,
      "loss": 6.9,
      "step": 7948
    },
    {
      "epoch": 0.4307859078590786,
      "step": 7948,
      "training_loss": 4.248379707336426
    },
    {
      "epoch": 0.430840108401084,
      "step": 7949,
      "training_loss": 4.469666957855225
    },
    {
      "epoch": 0.43089430894308944,
      "step": 7950,
      "training_loss": 6.927561283111572
    },
    {
      "epoch": 0.43094850948509483,
      "step": 7951,
      "training_loss": 5.923295497894287
    },
    {
      "epoch": 0.4310027100271003,
      "grad_norm": 31.073457717895508,
      "learning_rate": 1e-05,
      "loss": 5.3922,
      "step": 7952
    },
    {
      "epoch": 0.4310027100271003,
      "step": 7952,
      "training_loss": 6.5247297286987305
    },
    {
      "epoch": 0.43105691056910567,
      "step": 7953,
      "training_loss": 8.831812858581543
    },
    {
      "epoch": 0.4311111111111111,
      "step": 7954,
      "training_loss": 6.306443214416504
    },
    {
      "epoch": 0.43116531165311656,
      "step": 7955,
      "training_loss": 7.538034439086914
    },
    {
      "epoch": 0.43121951219512195,
      "grad_norm": 20.317026138305664,
      "learning_rate": 1e-05,
      "loss": 7.3003,
      "step": 7956
    },
    {
      "epoch": 0.43121951219512195,
      "step": 7956,
      "training_loss": 6.213636875152588
    },
    {
      "epoch": 0.4312737127371274,
      "step": 7957,
      "training_loss": 4.86830997467041
    },
    {
      "epoch": 0.4313279132791328,
      "step": 7958,
      "training_loss": 6.714923858642578
    },
    {
      "epoch": 0.4313821138211382,
      "step": 7959,
      "training_loss": 6.665684700012207
    },
    {
      "epoch": 0.4314363143631436,
      "grad_norm": 28.620092391967773,
      "learning_rate": 1e-05,
      "loss": 6.1156,
      "step": 7960
    },
    {
      "epoch": 0.4314363143631436,
      "step": 7960,
      "training_loss": 7.437958717346191
    },
    {
      "epoch": 0.43149051490514906,
      "step": 7961,
      "training_loss": 5.095729351043701
    },
    {
      "epoch": 0.43154471544715445,
      "step": 7962,
      "training_loss": 7.9966206550598145
    },
    {
      "epoch": 0.4315989159891599,
      "step": 7963,
      "training_loss": 4.688431262969971
    },
    {
      "epoch": 0.43165311653116534,
      "grad_norm": 27.08428192138672,
      "learning_rate": 1e-05,
      "loss": 6.3047,
      "step": 7964
    },
    {
      "epoch": 0.43165311653116534,
      "step": 7964,
      "training_loss": 6.895338535308838
    },
    {
      "epoch": 0.4317073170731707,
      "step": 7965,
      "training_loss": 5.416995525360107
    },
    {
      "epoch": 0.43176151761517617,
      "step": 7966,
      "training_loss": 5.734230041503906
    },
    {
      "epoch": 0.43181571815718156,
      "step": 7967,
      "training_loss": 5.033278942108154
    },
    {
      "epoch": 0.431869918699187,
      "grad_norm": 20.754316329956055,
      "learning_rate": 1e-05,
      "loss": 5.77,
      "step": 7968
    },
    {
      "epoch": 0.431869918699187,
      "step": 7968,
      "training_loss": 6.681022644042969
    },
    {
      "epoch": 0.4319241192411924,
      "step": 7969,
      "training_loss": 7.865976333618164
    },
    {
      "epoch": 0.43197831978319784,
      "step": 7970,
      "training_loss": 6.387457847595215
    },
    {
      "epoch": 0.43203252032520323,
      "step": 7971,
      "training_loss": 2.791217565536499
    },
    {
      "epoch": 0.4320867208672087,
      "grad_norm": 42.58533477783203,
      "learning_rate": 1e-05,
      "loss": 5.9314,
      "step": 7972
    },
    {
      "epoch": 0.4320867208672087,
      "step": 7972,
      "training_loss": 6.121706962585449
    },
    {
      "epoch": 0.4321409214092141,
      "step": 7973,
      "training_loss": 6.674696922302246
    },
    {
      "epoch": 0.4321951219512195,
      "step": 7974,
      "training_loss": 6.191391944885254
    },
    {
      "epoch": 0.43224932249322495,
      "step": 7975,
      "training_loss": 7.826810359954834
    },
    {
      "epoch": 0.43230352303523034,
      "grad_norm": 39.68452835083008,
      "learning_rate": 1e-05,
      "loss": 6.7037,
      "step": 7976
    },
    {
      "epoch": 0.43230352303523034,
      "step": 7976,
      "training_loss": 6.7538323402404785
    },
    {
      "epoch": 0.4323577235772358,
      "step": 7977,
      "training_loss": 6.157514572143555
    },
    {
      "epoch": 0.4324119241192412,
      "step": 7978,
      "training_loss": 5.959238529205322
    },
    {
      "epoch": 0.4324661246612466,
      "step": 7979,
      "training_loss": 6.996457576751709
    },
    {
      "epoch": 0.432520325203252,
      "grad_norm": 22.432294845581055,
      "learning_rate": 1e-05,
      "loss": 6.4668,
      "step": 7980
    },
    {
      "epoch": 0.432520325203252,
      "step": 7980,
      "training_loss": 6.992592811584473
    },
    {
      "epoch": 0.43257452574525745,
      "step": 7981,
      "training_loss": 7.517174243927002
    },
    {
      "epoch": 0.4326287262872629,
      "step": 7982,
      "training_loss": 6.266642093658447
    },
    {
      "epoch": 0.4326829268292683,
      "step": 7983,
      "training_loss": 7.073917388916016
    },
    {
      "epoch": 0.43273712737127373,
      "grad_norm": 22.27263069152832,
      "learning_rate": 1e-05,
      "loss": 6.9626,
      "step": 7984
    },
    {
      "epoch": 0.43273712737127373,
      "step": 7984,
      "training_loss": 7.249133110046387
    },
    {
      "epoch": 0.4327913279132791,
      "step": 7985,
      "training_loss": 5.01076078414917
    },
    {
      "epoch": 0.43284552845528457,
      "step": 7986,
      "training_loss": 5.503969669342041
    },
    {
      "epoch": 0.43289972899728996,
      "step": 7987,
      "training_loss": 6.74653959274292
    },
    {
      "epoch": 0.4329539295392954,
      "grad_norm": 25.746185302734375,
      "learning_rate": 1e-05,
      "loss": 6.1276,
      "step": 7988
    },
    {
      "epoch": 0.4329539295392954,
      "step": 7988,
      "training_loss": 4.085809230804443
    },
    {
      "epoch": 0.4330081300813008,
      "step": 7989,
      "training_loss": 7.237697601318359
    },
    {
      "epoch": 0.43306233062330624,
      "step": 7990,
      "training_loss": 7.2290778160095215
    },
    {
      "epoch": 0.4331165311653117,
      "step": 7991,
      "training_loss": 8.123595237731934
    },
    {
      "epoch": 0.43317073170731707,
      "grad_norm": 33.10724639892578,
      "learning_rate": 1e-05,
      "loss": 6.669,
      "step": 7992
    },
    {
      "epoch": 0.43317073170731707,
      "step": 7992,
      "training_loss": 7.207988739013672
    },
    {
      "epoch": 0.4332249322493225,
      "step": 7993,
      "training_loss": 6.079499244689941
    },
    {
      "epoch": 0.4332791327913279,
      "step": 7994,
      "training_loss": 6.798069477081299
    },
    {
      "epoch": 0.43333333333333335,
      "step": 7995,
      "training_loss": 4.722879886627197
    },
    {
      "epoch": 0.43338753387533874,
      "grad_norm": 42.41131591796875,
      "learning_rate": 1e-05,
      "loss": 6.2021,
      "step": 7996
    },
    {
      "epoch": 0.43338753387533874,
      "step": 7996,
      "training_loss": 7.1649298667907715
    },
    {
      "epoch": 0.4334417344173442,
      "step": 7997,
      "training_loss": 6.499204158782959
    },
    {
      "epoch": 0.43349593495934957,
      "step": 7998,
      "training_loss": 4.678892135620117
    },
    {
      "epoch": 0.433550135501355,
      "step": 7999,
      "training_loss": 7.8891801834106445
    },
    {
      "epoch": 0.43360433604336046,
      "grad_norm": 18.75175666809082,
      "learning_rate": 1e-05,
      "loss": 6.5581,
      "step": 8000
    },
    {
      "epoch": 0.43360433604336046,
      "step": 8000,
      "training_loss": 6.491148471832275
    },
    {
      "epoch": 0.43365853658536585,
      "step": 8001,
      "training_loss": 6.857872486114502
    },
    {
      "epoch": 0.4337127371273713,
      "step": 8002,
      "training_loss": 8.746089935302734
    },
    {
      "epoch": 0.4337669376693767,
      "step": 8003,
      "training_loss": 7.354485988616943
    },
    {
      "epoch": 0.43382113821138213,
      "grad_norm": 25.85138511657715,
      "learning_rate": 1e-05,
      "loss": 7.3624,
      "step": 8004
    },
    {
      "epoch": 0.43382113821138213,
      "step": 8004,
      "training_loss": 5.944742679595947
    },
    {
      "epoch": 0.4338753387533875,
      "step": 8005,
      "training_loss": 7.313472270965576
    },
    {
      "epoch": 0.43392953929539296,
      "step": 8006,
      "training_loss": 7.8097333908081055
    },
    {
      "epoch": 0.43398373983739835,
      "step": 8007,
      "training_loss": 7.2130513191223145
    },
    {
      "epoch": 0.4340379403794038,
      "grad_norm": 31.135974884033203,
      "learning_rate": 1e-05,
      "loss": 7.0702,
      "step": 8008
    },
    {
      "epoch": 0.4340379403794038,
      "step": 8008,
      "training_loss": 6.919013977050781
    },
    {
      "epoch": 0.4340921409214092,
      "step": 8009,
      "training_loss": 6.9677734375
    },
    {
      "epoch": 0.43414634146341463,
      "step": 8010,
      "training_loss": 7.121743202209473
    },
    {
      "epoch": 0.4342005420054201,
      "step": 8011,
      "training_loss": 5.0575361251831055
    },
    {
      "epoch": 0.43425474254742547,
      "grad_norm": 36.47465515136719,
      "learning_rate": 1e-05,
      "loss": 6.5165,
      "step": 8012
    },
    {
      "epoch": 0.43425474254742547,
      "step": 8012,
      "training_loss": 6.952902793884277
    },
    {
      "epoch": 0.4343089430894309,
      "step": 8013,
      "training_loss": 6.135051250457764
    },
    {
      "epoch": 0.4343631436314363,
      "step": 8014,
      "training_loss": 6.774745464324951
    },
    {
      "epoch": 0.43441734417344174,
      "step": 8015,
      "training_loss": 6.561902046203613
    },
    {
      "epoch": 0.43447154471544713,
      "grad_norm": 21.59077262878418,
      "learning_rate": 1e-05,
      "loss": 6.6062,
      "step": 8016
    },
    {
      "epoch": 0.43447154471544713,
      "step": 8016,
      "training_loss": 5.320347309112549
    },
    {
      "epoch": 0.4345257452574526,
      "step": 8017,
      "training_loss": 7.04622220993042
    },
    {
      "epoch": 0.43457994579945797,
      "step": 8018,
      "training_loss": 6.629886627197266
    },
    {
      "epoch": 0.4346341463414634,
      "step": 8019,
      "training_loss": 7.459859848022461
    },
    {
      "epoch": 0.43468834688346886,
      "grad_norm": 29.126413345336914,
      "learning_rate": 1e-05,
      "loss": 6.6141,
      "step": 8020
    },
    {
      "epoch": 0.43468834688346886,
      "step": 8020,
      "training_loss": 5.973500728607178
    },
    {
      "epoch": 0.43474254742547425,
      "step": 8021,
      "training_loss": 7.839420318603516
    },
    {
      "epoch": 0.4347967479674797,
      "step": 8022,
      "training_loss": 5.878750324249268
    },
    {
      "epoch": 0.4348509485094851,
      "step": 8023,
      "training_loss": 6.197791576385498
    },
    {
      "epoch": 0.4349051490514905,
      "grad_norm": 22.514442443847656,
      "learning_rate": 1e-05,
      "loss": 6.4724,
      "step": 8024
    },
    {
      "epoch": 0.4349051490514905,
      "step": 8024,
      "training_loss": 6.760502815246582
    },
    {
      "epoch": 0.4349593495934959,
      "step": 8025,
      "training_loss": 5.593503952026367
    },
    {
      "epoch": 0.43501355013550136,
      "step": 8026,
      "training_loss": 6.654840469360352
    },
    {
      "epoch": 0.43506775067750675,
      "step": 8027,
      "training_loss": 6.4170989990234375
    },
    {
      "epoch": 0.4351219512195122,
      "grad_norm": 26.443241119384766,
      "learning_rate": 1e-05,
      "loss": 6.3565,
      "step": 8028
    },
    {
      "epoch": 0.4351219512195122,
      "step": 8028,
      "training_loss": 6.9142560958862305
    },
    {
      "epoch": 0.43517615176151764,
      "step": 8029,
      "training_loss": 6.963478088378906
    },
    {
      "epoch": 0.435230352303523,
      "step": 8030,
      "training_loss": 7.527954578399658
    },
    {
      "epoch": 0.43528455284552847,
      "step": 8031,
      "training_loss": 6.708647727966309
    },
    {
      "epoch": 0.43533875338753386,
      "grad_norm": 52.63113784790039,
      "learning_rate": 1e-05,
      "loss": 7.0286,
      "step": 8032
    },
    {
      "epoch": 0.43533875338753386,
      "step": 8032,
      "training_loss": 5.956773281097412
    },
    {
      "epoch": 0.4353929539295393,
      "step": 8033,
      "training_loss": 5.173198223114014
    },
    {
      "epoch": 0.4354471544715447,
      "step": 8034,
      "training_loss": 6.864476203918457
    },
    {
      "epoch": 0.43550135501355014,
      "step": 8035,
      "training_loss": 4.877102375030518
    },
    {
      "epoch": 0.43555555555555553,
      "grad_norm": 23.433223724365234,
      "learning_rate": 1e-05,
      "loss": 5.7179,
      "step": 8036
    },
    {
      "epoch": 0.43555555555555553,
      "step": 8036,
      "training_loss": 7.472510814666748
    },
    {
      "epoch": 0.435609756097561,
      "step": 8037,
      "training_loss": 7.395421504974365
    },
    {
      "epoch": 0.4356639566395664,
      "step": 8038,
      "training_loss": 5.979814529418945
    },
    {
      "epoch": 0.4357181571815718,
      "step": 8039,
      "training_loss": 5.44082498550415
    },
    {
      "epoch": 0.43577235772357725,
      "grad_norm": 22.260299682617188,
      "learning_rate": 1e-05,
      "loss": 6.5721,
      "step": 8040
    },
    {
      "epoch": 0.43577235772357725,
      "step": 8040,
      "training_loss": 6.331615924835205
    },
    {
      "epoch": 0.43582655826558264,
      "step": 8041,
      "training_loss": 6.457884311676025
    },
    {
      "epoch": 0.4358807588075881,
      "step": 8042,
      "training_loss": 7.436496257781982
    },
    {
      "epoch": 0.4359349593495935,
      "step": 8043,
      "training_loss": 4.397522926330566
    },
    {
      "epoch": 0.4359891598915989,
      "grad_norm": 28.271507263183594,
      "learning_rate": 1e-05,
      "loss": 6.1559,
      "step": 8044
    },
    {
      "epoch": 0.4359891598915989,
      "step": 8044,
      "training_loss": 6.987164497375488
    },
    {
      "epoch": 0.4360433604336043,
      "step": 8045,
      "training_loss": 6.241919040679932
    },
    {
      "epoch": 0.43609756097560975,
      "step": 8046,
      "training_loss": 6.579577445983887
    },
    {
      "epoch": 0.4361517615176152,
      "step": 8047,
      "training_loss": 6.7520036697387695
    },
    {
      "epoch": 0.4362059620596206,
      "grad_norm": 20.480525970458984,
      "learning_rate": 1e-05,
      "loss": 6.6402,
      "step": 8048
    },
    {
      "epoch": 0.4362059620596206,
      "step": 8048,
      "training_loss": 6.558703422546387
    },
    {
      "epoch": 0.43626016260162603,
      "step": 8049,
      "training_loss": 6.347782135009766
    },
    {
      "epoch": 0.4363143631436314,
      "step": 8050,
      "training_loss": 6.666445255279541
    },
    {
      "epoch": 0.43636856368563687,
      "step": 8051,
      "training_loss": 7.52255392074585
    },
    {
      "epoch": 0.43642276422764226,
      "grad_norm": 53.80347442626953,
      "learning_rate": 1e-05,
      "loss": 6.7739,
      "step": 8052
    },
    {
      "epoch": 0.43642276422764226,
      "step": 8052,
      "training_loss": 6.696251392364502
    },
    {
      "epoch": 0.4364769647696477,
      "step": 8053,
      "training_loss": 7.723170280456543
    },
    {
      "epoch": 0.4365311653116531,
      "step": 8054,
      "training_loss": 8.150884628295898
    },
    {
      "epoch": 0.43658536585365854,
      "step": 8055,
      "training_loss": 4.972743988037109
    },
    {
      "epoch": 0.436639566395664,
      "grad_norm": 40.29854965209961,
      "learning_rate": 1e-05,
      "loss": 6.8858,
      "step": 8056
    },
    {
      "epoch": 0.436639566395664,
      "step": 8056,
      "training_loss": 8.055663108825684
    },
    {
      "epoch": 0.43669376693766937,
      "step": 8057,
      "training_loss": 6.5953779220581055
    },
    {
      "epoch": 0.4367479674796748,
      "step": 8058,
      "training_loss": 6.466728687286377
    },
    {
      "epoch": 0.4368021680216802,
      "step": 8059,
      "training_loss": 8.05805778503418
    },
    {
      "epoch": 0.43685636856368565,
      "grad_norm": 56.46544647216797,
      "learning_rate": 1e-05,
      "loss": 7.294,
      "step": 8060
    },
    {
      "epoch": 0.43685636856368565,
      "step": 8060,
      "training_loss": 6.653526306152344
    },
    {
      "epoch": 0.43691056910569104,
      "step": 8061,
      "training_loss": 7.404813289642334
    },
    {
      "epoch": 0.4369647696476965,
      "step": 8062,
      "training_loss": 7.352372646331787
    },
    {
      "epoch": 0.43701897018970187,
      "step": 8063,
      "training_loss": 6.006574630737305
    },
    {
      "epoch": 0.4370731707317073,
      "grad_norm": 44.22993087768555,
      "learning_rate": 1e-05,
      "loss": 6.8543,
      "step": 8064
    },
    {
      "epoch": 0.4370731707317073,
      "step": 8064,
      "training_loss": 4.831226348876953
    },
    {
      "epoch": 0.43712737127371276,
      "step": 8065,
      "training_loss": 6.399166107177734
    },
    {
      "epoch": 0.43718157181571815,
      "step": 8066,
      "training_loss": 6.972890853881836
    },
    {
      "epoch": 0.4372357723577236,
      "step": 8067,
      "training_loss": 7.203618049621582
    },
    {
      "epoch": 0.437289972899729,
      "grad_norm": 29.057003021240234,
      "learning_rate": 1e-05,
      "loss": 6.3517,
      "step": 8068
    },
    {
      "epoch": 0.437289972899729,
      "step": 8068,
      "training_loss": 7.344972133636475
    },
    {
      "epoch": 0.43734417344173443,
      "step": 8069,
      "training_loss": 3.8141400814056396
    },
    {
      "epoch": 0.4373983739837398,
      "step": 8070,
      "training_loss": 7.113799571990967
    },
    {
      "epoch": 0.43745257452574526,
      "step": 8071,
      "training_loss": 7.195051193237305
    },
    {
      "epoch": 0.43750677506775065,
      "grad_norm": 15.21441650390625,
      "learning_rate": 1e-05,
      "loss": 6.367,
      "step": 8072
    },
    {
      "epoch": 0.43750677506775065,
      "step": 8072,
      "training_loss": 6.561731338500977
    },
    {
      "epoch": 0.4375609756097561,
      "step": 8073,
      "training_loss": 7.0827836990356445
    },
    {
      "epoch": 0.43761517615176154,
      "step": 8074,
      "training_loss": 6.698850631713867
    },
    {
      "epoch": 0.43766937669376693,
      "step": 8075,
      "training_loss": 7.628745079040527
    },
    {
      "epoch": 0.4377235772357724,
      "grad_norm": 46.076385498046875,
      "learning_rate": 1e-05,
      "loss": 6.993,
      "step": 8076
    },
    {
      "epoch": 0.4377235772357724,
      "step": 8076,
      "training_loss": 7.619403839111328
    },
    {
      "epoch": 0.43777777777777777,
      "step": 8077,
      "training_loss": 6.043525695800781
    },
    {
      "epoch": 0.4378319783197832,
      "step": 8078,
      "training_loss": 5.889252662658691
    },
    {
      "epoch": 0.4378861788617886,
      "step": 8079,
      "training_loss": 6.90635347366333
    },
    {
      "epoch": 0.43794037940379404,
      "grad_norm": 33.11886978149414,
      "learning_rate": 1e-05,
      "loss": 6.6146,
      "step": 8080
    },
    {
      "epoch": 0.43794037940379404,
      "step": 8080,
      "training_loss": 6.879099369049072
    },
    {
      "epoch": 0.43799457994579943,
      "step": 8081,
      "training_loss": 6.914022445678711
    },
    {
      "epoch": 0.4380487804878049,
      "step": 8082,
      "training_loss": 7.30277156829834
    },
    {
      "epoch": 0.4381029810298103,
      "step": 8083,
      "training_loss": 3.15483021736145
    },
    {
      "epoch": 0.4381571815718157,
      "grad_norm": 48.24003982543945,
      "learning_rate": 1e-05,
      "loss": 6.0627,
      "step": 8084
    },
    {
      "epoch": 0.4381571815718157,
      "step": 8084,
      "training_loss": 7.098682403564453
    },
    {
      "epoch": 0.43821138211382116,
      "step": 8085,
      "training_loss": 7.360342979431152
    },
    {
      "epoch": 0.43826558265582655,
      "step": 8086,
      "training_loss": 6.93558406829834
    },
    {
      "epoch": 0.438319783197832,
      "step": 8087,
      "training_loss": 7.0218048095703125
    },
    {
      "epoch": 0.4383739837398374,
      "grad_norm": 25.111650466918945,
      "learning_rate": 1e-05,
      "loss": 7.1041,
      "step": 8088
    },
    {
      "epoch": 0.4383739837398374,
      "step": 8088,
      "training_loss": 6.314109802246094
    },
    {
      "epoch": 0.4384281842818428,
      "step": 8089,
      "training_loss": 6.847649097442627
    },
    {
      "epoch": 0.4384823848238482,
      "step": 8090,
      "training_loss": 7.315694332122803
    },
    {
      "epoch": 0.43853658536585366,
      "step": 8091,
      "training_loss": 7.1537346839904785
    },
    {
      "epoch": 0.4385907859078591,
      "grad_norm": 25.711997985839844,
      "learning_rate": 1e-05,
      "loss": 6.9078,
      "step": 8092
    },
    {
      "epoch": 0.4385907859078591,
      "step": 8092,
      "training_loss": 7.03372859954834
    },
    {
      "epoch": 0.4386449864498645,
      "step": 8093,
      "training_loss": 7.475039958953857
    },
    {
      "epoch": 0.43869918699186994,
      "step": 8094,
      "training_loss": 4.425059795379639
    },
    {
      "epoch": 0.4387533875338753,
      "step": 8095,
      "training_loss": 7.222886562347412
    },
    {
      "epoch": 0.4388075880758808,
      "grad_norm": 20.799072265625,
      "learning_rate": 1e-05,
      "loss": 6.5392,
      "step": 8096
    },
    {
      "epoch": 0.4388075880758808,
      "step": 8096,
      "training_loss": 6.204232692718506
    },
    {
      "epoch": 0.43886178861788616,
      "step": 8097,
      "training_loss": 6.747182846069336
    },
    {
      "epoch": 0.4389159891598916,
      "step": 8098,
      "training_loss": 7.314514636993408
    },
    {
      "epoch": 0.438970189701897,
      "step": 8099,
      "training_loss": 7.91782808303833
    },
    {
      "epoch": 0.43902439024390244,
      "grad_norm": 49.8494758605957,
      "learning_rate": 1e-05,
      "loss": 7.0459,
      "step": 8100
    },
    {
      "epoch": 0.43902439024390244,
      "step": 8100,
      "training_loss": 6.443227767944336
    },
    {
      "epoch": 0.4390785907859079,
      "step": 8101,
      "training_loss": 6.3512959480285645
    },
    {
      "epoch": 0.4391327913279133,
      "step": 8102,
      "training_loss": 6.7849650382995605
    },
    {
      "epoch": 0.4391869918699187,
      "step": 8103,
      "training_loss": 7.086388111114502
    },
    {
      "epoch": 0.4392411924119241,
      "grad_norm": 28.797439575195312,
      "learning_rate": 1e-05,
      "loss": 6.6665,
      "step": 8104
    },
    {
      "epoch": 0.4392411924119241,
      "step": 8104,
      "training_loss": 7.197363376617432
    },
    {
      "epoch": 0.43929539295392955,
      "step": 8105,
      "training_loss": 6.200630187988281
    },
    {
      "epoch": 0.43934959349593494,
      "step": 8106,
      "training_loss": 8.825495719909668
    },
    {
      "epoch": 0.4394037940379404,
      "step": 8107,
      "training_loss": 5.867044448852539
    },
    {
      "epoch": 0.4394579945799458,
      "grad_norm": 39.245113372802734,
      "learning_rate": 1e-05,
      "loss": 7.0226,
      "step": 8108
    },
    {
      "epoch": 0.4394579945799458,
      "step": 8108,
      "training_loss": 6.907960414886475
    },
    {
      "epoch": 0.4395121951219512,
      "step": 8109,
      "training_loss": 6.875208854675293
    },
    {
      "epoch": 0.43956639566395667,
      "step": 8110,
      "training_loss": 6.869296073913574
    },
    {
      "epoch": 0.43962059620596206,
      "step": 8111,
      "training_loss": 6.540830612182617
    },
    {
      "epoch": 0.4396747967479675,
      "grad_norm": 19.95665168762207,
      "learning_rate": 1e-05,
      "loss": 6.7983,
      "step": 8112
    },
    {
      "epoch": 0.4396747967479675,
      "step": 8112,
      "training_loss": 6.472819805145264
    },
    {
      "epoch": 0.4397289972899729,
      "step": 8113,
      "training_loss": 6.584656238555908
    },
    {
      "epoch": 0.43978319783197833,
      "step": 8114,
      "training_loss": 6.223148822784424
    },
    {
      "epoch": 0.4398373983739837,
      "step": 8115,
      "training_loss": 6.822268486022949
    },
    {
      "epoch": 0.43989159891598917,
      "grad_norm": 28.752727508544922,
      "learning_rate": 1e-05,
      "loss": 6.5257,
      "step": 8116
    },
    {
      "epoch": 0.43989159891598917,
      "step": 8116,
      "training_loss": 4.676452159881592
    },
    {
      "epoch": 0.43994579945799456,
      "step": 8117,
      "training_loss": 7.168167591094971
    },
    {
      "epoch": 0.44,
      "step": 8118,
      "training_loss": 7.390233516693115
    },
    {
      "epoch": 0.44005420054200545,
      "step": 8119,
      "training_loss": 6.480104923248291
    },
    {
      "epoch": 0.44010840108401084,
      "grad_norm": 27.44974136352539,
      "learning_rate": 1e-05,
      "loss": 6.4287,
      "step": 8120
    },
    {
      "epoch": 0.44010840108401084,
      "step": 8120,
      "training_loss": 7.3272929191589355
    },
    {
      "epoch": 0.4401626016260163,
      "step": 8121,
      "training_loss": 7.769532203674316
    },
    {
      "epoch": 0.44021680216802167,
      "step": 8122,
      "training_loss": 7.237548828125
    },
    {
      "epoch": 0.4402710027100271,
      "step": 8123,
      "training_loss": 6.784505844116211
    },
    {
      "epoch": 0.4403252032520325,
      "grad_norm": 29.170581817626953,
      "learning_rate": 1e-05,
      "loss": 7.2797,
      "step": 8124
    },
    {
      "epoch": 0.4403252032520325,
      "step": 8124,
      "training_loss": 6.659965515136719
    },
    {
      "epoch": 0.44037940379403795,
      "step": 8125,
      "training_loss": 6.068385601043701
    },
    {
      "epoch": 0.44043360433604334,
      "step": 8126,
      "training_loss": 6.036505222320557
    },
    {
      "epoch": 0.4404878048780488,
      "step": 8127,
      "training_loss": 6.8506059646606445
    },
    {
      "epoch": 0.44054200542005423,
      "grad_norm": 15.526226997375488,
      "learning_rate": 1e-05,
      "loss": 6.4039,
      "step": 8128
    },
    {
      "epoch": 0.44054200542005423,
      "step": 8128,
      "training_loss": 7.306703567504883
    },
    {
      "epoch": 0.4405962059620596,
      "step": 8129,
      "training_loss": 6.8415303230285645
    },
    {
      "epoch": 0.44065040650406506,
      "step": 8130,
      "training_loss": 5.363858222961426
    },
    {
      "epoch": 0.44070460704607045,
      "step": 8131,
      "training_loss": 7.655520439147949
    },
    {
      "epoch": 0.4407588075880759,
      "grad_norm": 48.354251861572266,
      "learning_rate": 1e-05,
      "loss": 6.7919,
      "step": 8132
    },
    {
      "epoch": 0.4407588075880759,
      "step": 8132,
      "training_loss": 7.036459445953369
    },
    {
      "epoch": 0.4408130081300813,
      "step": 8133,
      "training_loss": 6.970671653747559
    },
    {
      "epoch": 0.44086720867208673,
      "step": 8134,
      "training_loss": 6.421081066131592
    },
    {
      "epoch": 0.4409214092140921,
      "step": 8135,
      "training_loss": 6.09926700592041
    },
    {
      "epoch": 0.44097560975609756,
      "grad_norm": 30.214513778686523,
      "learning_rate": 1e-05,
      "loss": 6.6319,
      "step": 8136
    },
    {
      "epoch": 0.44097560975609756,
      "step": 8136,
      "training_loss": 7.180535793304443
    },
    {
      "epoch": 0.44102981029810295,
      "step": 8137,
      "training_loss": 3.787065267562866
    },
    {
      "epoch": 0.4410840108401084,
      "step": 8138,
      "training_loss": 6.655517578125
    },
    {
      "epoch": 0.44113821138211384,
      "step": 8139,
      "training_loss": 6.9928436279296875
    },
    {
      "epoch": 0.44119241192411923,
      "grad_norm": 17.013643264770508,
      "learning_rate": 1e-05,
      "loss": 6.154,
      "step": 8140
    },
    {
      "epoch": 0.44119241192411923,
      "step": 8140,
      "training_loss": 5.6493916511535645
    },
    {
      "epoch": 0.4412466124661247,
      "step": 8141,
      "training_loss": 4.6769514083862305
    },
    {
      "epoch": 0.44130081300813007,
      "step": 8142,
      "training_loss": 7.8416290283203125
    },
    {
      "epoch": 0.4413550135501355,
      "step": 8143,
      "training_loss": 7.771835803985596
    },
    {
      "epoch": 0.4414092140921409,
      "grad_norm": 18.77234649658203,
      "learning_rate": 1e-05,
      "loss": 6.485,
      "step": 8144
    },
    {
      "epoch": 0.4414092140921409,
      "step": 8144,
      "training_loss": 7.553965091705322
    },
    {
      "epoch": 0.44146341463414634,
      "step": 8145,
      "training_loss": 6.578408241271973
    },
    {
      "epoch": 0.44151761517615173,
      "step": 8146,
      "training_loss": 5.737755298614502
    },
    {
      "epoch": 0.4415718157181572,
      "step": 8147,
      "training_loss": 5.589134693145752
    },
    {
      "epoch": 0.4416260162601626,
      "grad_norm": 54.36440658569336,
      "learning_rate": 1e-05,
      "loss": 6.3648,
      "step": 8148
    },
    {
      "epoch": 0.4416260162601626,
      "step": 8148,
      "training_loss": 5.267343997955322
    },
    {
      "epoch": 0.441680216802168,
      "step": 8149,
      "training_loss": 6.932278633117676
    },
    {
      "epoch": 0.44173441734417346,
      "step": 8150,
      "training_loss": 7.254813194274902
    },
    {
      "epoch": 0.44178861788617885,
      "step": 8151,
      "training_loss": 7.013768672943115
    },
    {
      "epoch": 0.4418428184281843,
      "grad_norm": 26.472793579101562,
      "learning_rate": 1e-05,
      "loss": 6.6171,
      "step": 8152
    },
    {
      "epoch": 0.4418428184281843,
      "step": 8152,
      "training_loss": 7.228199005126953
    },
    {
      "epoch": 0.4418970189701897,
      "step": 8153,
      "training_loss": 6.300885200500488
    },
    {
      "epoch": 0.4419512195121951,
      "step": 8154,
      "training_loss": 6.710201740264893
    },
    {
      "epoch": 0.4420054200542005,
      "step": 8155,
      "training_loss": 6.93809700012207
    },
    {
      "epoch": 0.44205962059620596,
      "grad_norm": 24.103425979614258,
      "learning_rate": 1e-05,
      "loss": 6.7943,
      "step": 8156
    },
    {
      "epoch": 0.44205962059620596,
      "step": 8156,
      "training_loss": 6.90388298034668
    },
    {
      "epoch": 0.4421138211382114,
      "step": 8157,
      "training_loss": 7.9935383796691895
    },
    {
      "epoch": 0.4421680216802168,
      "step": 8158,
      "training_loss": 7.669425964355469
    },
    {
      "epoch": 0.44222222222222224,
      "step": 8159,
      "training_loss": 7.1226983070373535
    },
    {
      "epoch": 0.44227642276422763,
      "grad_norm": 17.125736236572266,
      "learning_rate": 1e-05,
      "loss": 7.4224,
      "step": 8160
    },
    {
      "epoch": 0.44227642276422763,
      "step": 8160,
      "training_loss": 4.831562519073486
    },
    {
      "epoch": 0.4423306233062331,
      "step": 8161,
      "training_loss": 6.538742542266846
    },
    {
      "epoch": 0.44238482384823846,
      "step": 8162,
      "training_loss": 7.386579990386963
    },
    {
      "epoch": 0.4424390243902439,
      "step": 8163,
      "training_loss": 7.159160614013672
    },
    {
      "epoch": 0.4424932249322493,
      "grad_norm": 18.467973709106445,
      "learning_rate": 1e-05,
      "loss": 6.479,
      "step": 8164
    },
    {
      "epoch": 0.4424932249322493,
      "step": 8164,
      "training_loss": 6.9222588539123535
    },
    {
      "epoch": 0.44254742547425474,
      "step": 8165,
      "training_loss": 7.142856121063232
    },
    {
      "epoch": 0.4426016260162602,
      "step": 8166,
      "training_loss": 7.582573413848877
    },
    {
      "epoch": 0.4426558265582656,
      "step": 8167,
      "training_loss": 6.911417007446289
    },
    {
      "epoch": 0.442710027100271,
      "grad_norm": 54.247867584228516,
      "learning_rate": 1e-05,
      "loss": 7.1398,
      "step": 8168
    },
    {
      "epoch": 0.442710027100271,
      "step": 8168,
      "training_loss": 6.572088718414307
    },
    {
      "epoch": 0.4427642276422764,
      "step": 8169,
      "training_loss": 6.839517116546631
    },
    {
      "epoch": 0.44281842818428185,
      "step": 8170,
      "training_loss": 7.600141525268555
    },
    {
      "epoch": 0.44287262872628724,
      "step": 8171,
      "training_loss": 7.540291786193848
    },
    {
      "epoch": 0.4429268292682927,
      "grad_norm": 28.57255744934082,
      "learning_rate": 1e-05,
      "loss": 7.138,
      "step": 8172
    },
    {
      "epoch": 0.4429268292682927,
      "step": 8172,
      "training_loss": 6.309783935546875
    },
    {
      "epoch": 0.4429810298102981,
      "step": 8173,
      "training_loss": 5.4573750495910645
    },
    {
      "epoch": 0.4430352303523035,
      "step": 8174,
      "training_loss": 6.5311431884765625
    },
    {
      "epoch": 0.44308943089430897,
      "step": 8175,
      "training_loss": 5.687403202056885
    },
    {
      "epoch": 0.44314363143631436,
      "grad_norm": 24.89129638671875,
      "learning_rate": 1e-05,
      "loss": 5.9964,
      "step": 8176
    },
    {
      "epoch": 0.44314363143631436,
      "step": 8176,
      "training_loss": 7.100203037261963
    },
    {
      "epoch": 0.4431978319783198,
      "step": 8177,
      "training_loss": 8.453295707702637
    },
    {
      "epoch": 0.4432520325203252,
      "step": 8178,
      "training_loss": 7.566751956939697
    },
    {
      "epoch": 0.44330623306233063,
      "step": 8179,
      "training_loss": 7.42940616607666
    },
    {
      "epoch": 0.443360433604336,
      "grad_norm": 21.013362884521484,
      "learning_rate": 1e-05,
      "loss": 7.6374,
      "step": 8180
    },
    {
      "epoch": 0.443360433604336,
      "step": 8180,
      "training_loss": 7.902832984924316
    },
    {
      "epoch": 0.44341463414634147,
      "step": 8181,
      "training_loss": 7.723849296569824
    },
    {
      "epoch": 0.44346883468834686,
      "step": 8182,
      "training_loss": 7.25770378112793
    },
    {
      "epoch": 0.4435230352303523,
      "step": 8183,
      "training_loss": 6.292951583862305
    },
    {
      "epoch": 0.44357723577235775,
      "grad_norm": 18.623764038085938,
      "learning_rate": 1e-05,
      "loss": 7.2943,
      "step": 8184
    },
    {
      "epoch": 0.44357723577235775,
      "step": 8184,
      "training_loss": 7.029231548309326
    },
    {
      "epoch": 0.44363143631436314,
      "step": 8185,
      "training_loss": 6.862909317016602
    },
    {
      "epoch": 0.4436856368563686,
      "step": 8186,
      "training_loss": 7.210009574890137
    },
    {
      "epoch": 0.44373983739837397,
      "step": 8187,
      "training_loss": 5.853819370269775
    },
    {
      "epoch": 0.4437940379403794,
      "grad_norm": 57.440704345703125,
      "learning_rate": 1e-05,
      "loss": 6.739,
      "step": 8188
    },
    {
      "epoch": 0.4437940379403794,
      "step": 8188,
      "training_loss": 6.676497936248779
    },
    {
      "epoch": 0.4438482384823848,
      "step": 8189,
      "training_loss": 7.410459041595459
    },
    {
      "epoch": 0.44390243902439025,
      "step": 8190,
      "training_loss": 7.046413898468018
    },
    {
      "epoch": 0.44395663956639564,
      "step": 8191,
      "training_loss": 6.905586242675781
    },
    {
      "epoch": 0.4440108401084011,
      "grad_norm": 29.175430297851562,
      "learning_rate": 1e-05,
      "loss": 7.0097,
      "step": 8192
    },
    {
      "epoch": 0.4440108401084011,
      "step": 8192,
      "training_loss": 5.767580509185791
    },
    {
      "epoch": 0.44406504065040653,
      "step": 8193,
      "training_loss": 6.354806900024414
    },
    {
      "epoch": 0.4441192411924119,
      "step": 8194,
      "training_loss": 7.549503326416016
    },
    {
      "epoch": 0.44417344173441736,
      "step": 8195,
      "training_loss": 6.023799896240234
    },
    {
      "epoch": 0.44422764227642275,
      "grad_norm": 34.522457122802734,
      "learning_rate": 1e-05,
      "loss": 6.4239,
      "step": 8196
    },
    {
      "epoch": 0.44422764227642275,
      "step": 8196,
      "training_loss": 6.857411861419678
    },
    {
      "epoch": 0.4442818428184282,
      "step": 8197,
      "training_loss": 6.554429054260254
    },
    {
      "epoch": 0.4443360433604336,
      "step": 8198,
      "training_loss": 6.600257396697998
    },
    {
      "epoch": 0.44439024390243903,
      "step": 8199,
      "training_loss": 5.6609320640563965
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 36.98326110839844,
      "learning_rate": 1e-05,
      "loss": 6.4183,
      "step": 8200
    },
    {
      "epoch": 0.4444444444444444,
      "step": 8200,
      "training_loss": 7.102816581726074
    },
    {
      "epoch": 0.44449864498644986,
      "step": 8201,
      "training_loss": 3.1403660774230957
    },
    {
      "epoch": 0.4445528455284553,
      "step": 8202,
      "training_loss": 4.788891792297363
    },
    {
      "epoch": 0.4446070460704607,
      "step": 8203,
      "training_loss": 7.092482566833496
    },
    {
      "epoch": 0.44466124661246614,
      "grad_norm": 20.07781410217285,
      "learning_rate": 1e-05,
      "loss": 5.5311,
      "step": 8204
    },
    {
      "epoch": 0.44466124661246614,
      "step": 8204,
      "training_loss": 5.1446852684021
    },
    {
      "epoch": 0.44471544715447153,
      "step": 8205,
      "training_loss": 6.1360955238342285
    },
    {
      "epoch": 0.444769647696477,
      "step": 8206,
      "training_loss": 6.527866840362549
    },
    {
      "epoch": 0.44482384823848237,
      "step": 8207,
      "training_loss": 7.53373384475708
    },
    {
      "epoch": 0.4448780487804878,
      "grad_norm": 34.486106872558594,
      "learning_rate": 1e-05,
      "loss": 6.3356,
      "step": 8208
    },
    {
      "epoch": 0.4448780487804878,
      "step": 8208,
      "training_loss": 7.392029285430908
    },
    {
      "epoch": 0.4449322493224932,
      "step": 8209,
      "training_loss": 5.3082075119018555
    },
    {
      "epoch": 0.44498644986449865,
      "step": 8210,
      "training_loss": 6.7643232345581055
    },
    {
      "epoch": 0.4450406504065041,
      "step": 8211,
      "training_loss": 7.937789440155029
    },
    {
      "epoch": 0.4450948509485095,
      "grad_norm": 32.49357223510742,
      "learning_rate": 1e-05,
      "loss": 6.8506,
      "step": 8212
    },
    {
      "epoch": 0.4450948509485095,
      "step": 8212,
      "training_loss": 6.789936065673828
    },
    {
      "epoch": 0.4451490514905149,
      "step": 8213,
      "training_loss": 6.307281494140625
    },
    {
      "epoch": 0.4452032520325203,
      "step": 8214,
      "training_loss": 5.377074718475342
    },
    {
      "epoch": 0.44525745257452576,
      "step": 8215,
      "training_loss": 7.269587516784668
    },
    {
      "epoch": 0.44531165311653115,
      "grad_norm": 24.066511154174805,
      "learning_rate": 1e-05,
      "loss": 6.436,
      "step": 8216
    },
    {
      "epoch": 0.44531165311653115,
      "step": 8216,
      "training_loss": 5.982344150543213
    },
    {
      "epoch": 0.4453658536585366,
      "step": 8217,
      "training_loss": 6.782807350158691
    },
    {
      "epoch": 0.445420054200542,
      "step": 8218,
      "training_loss": 6.717530727386475
    },
    {
      "epoch": 0.4454742547425474,
      "step": 8219,
      "training_loss": 6.565794944763184
    },
    {
      "epoch": 0.44552845528455287,
      "grad_norm": 38.848304748535156,
      "learning_rate": 1e-05,
      "loss": 6.5121,
      "step": 8220
    },
    {
      "epoch": 0.44552845528455287,
      "step": 8220,
      "training_loss": 7.041417598724365
    },
    {
      "epoch": 0.44558265582655826,
      "step": 8221,
      "training_loss": 5.972350597381592
    },
    {
      "epoch": 0.4456368563685637,
      "step": 8222,
      "training_loss": 6.938588619232178
    },
    {
      "epoch": 0.4456910569105691,
      "step": 8223,
      "training_loss": 6.389080047607422
    },
    {
      "epoch": 0.44574525745257454,
      "grad_norm": 20.76525115966797,
      "learning_rate": 1e-05,
      "loss": 6.5854,
      "step": 8224
    },
    {
      "epoch": 0.44574525745257454,
      "step": 8224,
      "training_loss": 7.286797046661377
    },
    {
      "epoch": 0.44579945799457993,
      "step": 8225,
      "training_loss": 7.1662397384643555
    },
    {
      "epoch": 0.4458536585365854,
      "step": 8226,
      "training_loss": 4.744441032409668
    },
    {
      "epoch": 0.44590785907859076,
      "step": 8227,
      "training_loss": 7.361050605773926
    },
    {
      "epoch": 0.4459620596205962,
      "grad_norm": 22.57117462158203,
      "learning_rate": 1e-05,
      "loss": 6.6396,
      "step": 8228
    },
    {
      "epoch": 0.4459620596205962,
      "step": 8228,
      "training_loss": 5.827388763427734
    },
    {
      "epoch": 0.44601626016260165,
      "step": 8229,
      "training_loss": 6.5393829345703125
    },
    {
      "epoch": 0.44607046070460704,
      "step": 8230,
      "training_loss": 7.567068099975586
    },
    {
      "epoch": 0.4461246612466125,
      "step": 8231,
      "training_loss": 6.90751838684082
    },
    {
      "epoch": 0.4461788617886179,
      "grad_norm": 17.129789352416992,
      "learning_rate": 1e-05,
      "loss": 6.7103,
      "step": 8232
    },
    {
      "epoch": 0.4461788617886179,
      "step": 8232,
      "training_loss": 5.1618428230285645
    },
    {
      "epoch": 0.4462330623306233,
      "step": 8233,
      "training_loss": 5.606719493865967
    },
    {
      "epoch": 0.4462872628726287,
      "step": 8234,
      "training_loss": 6.816987037658691
    },
    {
      "epoch": 0.44634146341463415,
      "step": 8235,
      "training_loss": 6.330997943878174
    },
    {
      "epoch": 0.44639566395663954,
      "grad_norm": 27.952207565307617,
      "learning_rate": 1e-05,
      "loss": 5.9791,
      "step": 8236
    },
    {
      "epoch": 0.44639566395663954,
      "step": 8236,
      "training_loss": 5.060422897338867
    },
    {
      "epoch": 0.446449864498645,
      "step": 8237,
      "training_loss": 6.887959957122803
    },
    {
      "epoch": 0.44650406504065043,
      "step": 8238,
      "training_loss": 5.841223239898682
    },
    {
      "epoch": 0.4465582655826558,
      "step": 8239,
      "training_loss": 6.646287441253662
    },
    {
      "epoch": 0.44661246612466127,
      "grad_norm": 28.656160354614258,
      "learning_rate": 1e-05,
      "loss": 6.109,
      "step": 8240
    },
    {
      "epoch": 0.44661246612466127,
      "step": 8240,
      "training_loss": 7.319875717163086
    },
    {
      "epoch": 0.44666666666666666,
      "step": 8241,
      "training_loss": 6.489490509033203
    },
    {
      "epoch": 0.4467208672086721,
      "step": 8242,
      "training_loss": 6.87009859085083
    },
    {
      "epoch": 0.4467750677506775,
      "step": 8243,
      "training_loss": 7.0185346603393555
    },
    {
      "epoch": 0.44682926829268294,
      "grad_norm": 25.861936569213867,
      "learning_rate": 1e-05,
      "loss": 6.9245,
      "step": 8244
    },
    {
      "epoch": 0.44682926829268294,
      "step": 8244,
      "training_loss": 6.2918314933776855
    },
    {
      "epoch": 0.4468834688346883,
      "step": 8245,
      "training_loss": 7.287998199462891
    },
    {
      "epoch": 0.44693766937669377,
      "step": 8246,
      "training_loss": 7.5790510177612305
    },
    {
      "epoch": 0.4469918699186992,
      "step": 8247,
      "training_loss": 6.072106838226318
    },
    {
      "epoch": 0.4470460704607046,
      "grad_norm": 24.019615173339844,
      "learning_rate": 1e-05,
      "loss": 6.8077,
      "step": 8248
    },
    {
      "epoch": 0.4470460704607046,
      "step": 8248,
      "training_loss": 5.217564105987549
    },
    {
      "epoch": 0.44710027100271005,
      "step": 8249,
      "training_loss": 5.346512794494629
    },
    {
      "epoch": 0.44715447154471544,
      "step": 8250,
      "training_loss": 7.792701244354248
    },
    {
      "epoch": 0.4472086720867209,
      "step": 8251,
      "training_loss": 5.785630702972412
    },
    {
      "epoch": 0.44726287262872627,
      "grad_norm": 27.7298583984375,
      "learning_rate": 1e-05,
      "loss": 6.0356,
      "step": 8252
    },
    {
      "epoch": 0.44726287262872627,
      "step": 8252,
      "training_loss": 6.708921432495117
    },
    {
      "epoch": 0.4473170731707317,
      "step": 8253,
      "training_loss": 7.645541191101074
    },
    {
      "epoch": 0.4473712737127371,
      "step": 8254,
      "training_loss": 8.149529457092285
    },
    {
      "epoch": 0.44742547425474255,
      "step": 8255,
      "training_loss": 6.516108989715576
    },
    {
      "epoch": 0.447479674796748,
      "grad_norm": 27.343725204467773,
      "learning_rate": 1e-05,
      "loss": 7.255,
      "step": 8256
    },
    {
      "epoch": 0.447479674796748,
      "step": 8256,
      "training_loss": 6.945442199707031
    },
    {
      "epoch": 0.4475338753387534,
      "step": 8257,
      "training_loss": 6.6772661209106445
    },
    {
      "epoch": 0.44758807588075883,
      "step": 8258,
      "training_loss": 3.742710590362549
    },
    {
      "epoch": 0.4476422764227642,
      "step": 8259,
      "training_loss": 7.096062183380127
    },
    {
      "epoch": 0.44769647696476966,
      "grad_norm": 27.297866821289062,
      "learning_rate": 1e-05,
      "loss": 6.1154,
      "step": 8260
    },
    {
      "epoch": 0.44769647696476966,
      "step": 8260,
      "training_loss": 7.160518646240234
    },
    {
      "epoch": 0.44775067750677505,
      "step": 8261,
      "training_loss": 6.232922077178955
    },
    {
      "epoch": 0.4478048780487805,
      "step": 8262,
      "training_loss": 6.908133029937744
    },
    {
      "epoch": 0.4478590785907859,
      "step": 8263,
      "training_loss": 7.013347625732422
    },
    {
      "epoch": 0.44791327913279133,
      "grad_norm": 19.177486419677734,
      "learning_rate": 1e-05,
      "loss": 6.8287,
      "step": 8264
    },
    {
      "epoch": 0.44791327913279133,
      "step": 8264,
      "training_loss": 6.8296051025390625
    },
    {
      "epoch": 0.4479674796747967,
      "step": 8265,
      "training_loss": 7.352970123291016
    },
    {
      "epoch": 0.44802168021680217,
      "step": 8266,
      "training_loss": 5.382256984710693
    },
    {
      "epoch": 0.4480758807588076,
      "step": 8267,
      "training_loss": 7.453564643859863
    },
    {
      "epoch": 0.448130081300813,
      "grad_norm": 27.723094940185547,
      "learning_rate": 1e-05,
      "loss": 6.7546,
      "step": 8268
    },
    {
      "epoch": 0.448130081300813,
      "step": 8268,
      "training_loss": 6.012807846069336
    },
    {
      "epoch": 0.44818428184281844,
      "step": 8269,
      "training_loss": 5.982337474822998
    },
    {
      "epoch": 0.44823848238482383,
      "step": 8270,
      "training_loss": 7.248868465423584
    },
    {
      "epoch": 0.4482926829268293,
      "step": 8271,
      "training_loss": 6.603532791137695
    },
    {
      "epoch": 0.44834688346883467,
      "grad_norm": 20.641427993774414,
      "learning_rate": 1e-05,
      "loss": 6.4619,
      "step": 8272
    },
    {
      "epoch": 0.44834688346883467,
      "step": 8272,
      "training_loss": 7.770524501800537
    },
    {
      "epoch": 0.4484010840108401,
      "step": 8273,
      "training_loss": 6.855900287628174
    },
    {
      "epoch": 0.4484552845528455,
      "step": 8274,
      "training_loss": 7.0307464599609375
    },
    {
      "epoch": 0.44850948509485095,
      "step": 8275,
      "training_loss": 6.790366172790527
    },
    {
      "epoch": 0.4485636856368564,
      "grad_norm": 39.7060432434082,
      "learning_rate": 1e-05,
      "loss": 7.1119,
      "step": 8276
    },
    {
      "epoch": 0.4485636856368564,
      "step": 8276,
      "training_loss": 6.740177631378174
    },
    {
      "epoch": 0.4486178861788618,
      "step": 8277,
      "training_loss": 6.939429759979248
    },
    {
      "epoch": 0.4486720867208672,
      "step": 8278,
      "training_loss": 5.429165363311768
    },
    {
      "epoch": 0.4487262872628726,
      "step": 8279,
      "training_loss": 6.407424449920654
    },
    {
      "epoch": 0.44878048780487806,
      "grad_norm": 25.967195510864258,
      "learning_rate": 1e-05,
      "loss": 6.379,
      "step": 8280
    },
    {
      "epoch": 0.44878048780487806,
      "step": 8280,
      "training_loss": 8.744620323181152
    },
    {
      "epoch": 0.44883468834688345,
      "step": 8281,
      "training_loss": 6.08967399597168
    },
    {
      "epoch": 0.4488888888888889,
      "step": 8282,
      "training_loss": 8.598761558532715
    },
    {
      "epoch": 0.4489430894308943,
      "step": 8283,
      "training_loss": 6.3061676025390625
    },
    {
      "epoch": 0.4489972899728997,
      "grad_norm": 30.03961181640625,
      "learning_rate": 1e-05,
      "loss": 7.4348,
      "step": 8284
    },
    {
      "epoch": 0.4489972899728997,
      "step": 8284,
      "training_loss": 6.428534984588623
    },
    {
      "epoch": 0.44905149051490517,
      "step": 8285,
      "training_loss": 6.940668106079102
    },
    {
      "epoch": 0.44910569105691056,
      "step": 8286,
      "training_loss": 7.069773197174072
    },
    {
      "epoch": 0.449159891598916,
      "step": 8287,
      "training_loss": 6.705940246582031
    },
    {
      "epoch": 0.4492140921409214,
      "grad_norm": 67.25321197509766,
      "learning_rate": 1e-05,
      "loss": 6.7862,
      "step": 8288
    },
    {
      "epoch": 0.4492140921409214,
      "step": 8288,
      "training_loss": 5.420377731323242
    },
    {
      "epoch": 0.44926829268292684,
      "step": 8289,
      "training_loss": 8.372438430786133
    },
    {
      "epoch": 0.44932249322493223,
      "step": 8290,
      "training_loss": 7.55040168762207
    },
    {
      "epoch": 0.4493766937669377,
      "step": 8291,
      "training_loss": 3.9648425579071045
    },
    {
      "epoch": 0.44943089430894306,
      "grad_norm": 29.43997573852539,
      "learning_rate": 1e-05,
      "loss": 6.327,
      "step": 8292
    },
    {
      "epoch": 0.44943089430894306,
      "step": 8292,
      "training_loss": 6.286276817321777
    },
    {
      "epoch": 0.4494850948509485,
      "step": 8293,
      "training_loss": 6.195817947387695
    },
    {
      "epoch": 0.44953929539295395,
      "step": 8294,
      "training_loss": 3.7231767177581787
    },
    {
      "epoch": 0.44959349593495934,
      "step": 8295,
      "training_loss": 7.069536209106445
    },
    {
      "epoch": 0.4496476964769648,
      "grad_norm": 44.9501838684082,
      "learning_rate": 1e-05,
      "loss": 5.8187,
      "step": 8296
    },
    {
      "epoch": 0.4496476964769648,
      "step": 8296,
      "training_loss": 5.568358898162842
    },
    {
      "epoch": 0.4497018970189702,
      "step": 8297,
      "training_loss": 7.187375068664551
    },
    {
      "epoch": 0.4497560975609756,
      "step": 8298,
      "training_loss": 8.846266746520996
    },
    {
      "epoch": 0.449810298102981,
      "step": 8299,
      "training_loss": 6.468688488006592
    },
    {
      "epoch": 0.44986449864498645,
      "grad_norm": 28.8133602142334,
      "learning_rate": 1e-05,
      "loss": 7.0177,
      "step": 8300
    },
    {
      "epoch": 0.44986449864498645,
      "step": 8300,
      "training_loss": 6.543020725250244
    },
    {
      "epoch": 0.44991869918699184,
      "step": 8301,
      "training_loss": 6.140039443969727
    },
    {
      "epoch": 0.4499728997289973,
      "step": 8302,
      "training_loss": 7.160146236419678
    },
    {
      "epoch": 0.45002710027100273,
      "step": 8303,
      "training_loss": 7.515230655670166
    },
    {
      "epoch": 0.4500813008130081,
      "grad_norm": 61.47321701049805,
      "learning_rate": 1e-05,
      "loss": 6.8396,
      "step": 8304
    },
    {
      "epoch": 0.4500813008130081,
      "step": 8304,
      "training_loss": 7.048649787902832
    },
    {
      "epoch": 0.45013550135501357,
      "step": 8305,
      "training_loss": 6.6897053718566895
    },
    {
      "epoch": 0.45018970189701896,
      "step": 8306,
      "training_loss": 6.140384674072266
    },
    {
      "epoch": 0.4502439024390244,
      "step": 8307,
      "training_loss": 6.273830890655518
    },
    {
      "epoch": 0.4502981029810298,
      "grad_norm": 20.982810974121094,
      "learning_rate": 1e-05,
      "loss": 6.5381,
      "step": 8308
    },
    {
      "epoch": 0.4502981029810298,
      "step": 8308,
      "training_loss": 7.121150970458984
    },
    {
      "epoch": 0.45035230352303524,
      "step": 8309,
      "training_loss": 6.366192817687988
    },
    {
      "epoch": 0.4504065040650406,
      "step": 8310,
      "training_loss": 4.83152437210083
    },
    {
      "epoch": 0.45046070460704607,
      "step": 8311,
      "training_loss": 7.691376209259033
    },
    {
      "epoch": 0.4505149051490515,
      "grad_norm": 19.27788543701172,
      "learning_rate": 1e-05,
      "loss": 6.5026,
      "step": 8312
    },
    {
      "epoch": 0.4505149051490515,
      "step": 8312,
      "training_loss": 5.241520881652832
    },
    {
      "epoch": 0.4505691056910569,
      "step": 8313,
      "training_loss": 8.549886703491211
    },
    {
      "epoch": 0.45062330623306235,
      "step": 8314,
      "training_loss": 6.146019458770752
    },
    {
      "epoch": 0.45067750677506774,
      "step": 8315,
      "training_loss": 6.334733009338379
    },
    {
      "epoch": 0.4507317073170732,
      "grad_norm": 34.375980377197266,
      "learning_rate": 1e-05,
      "loss": 6.568,
      "step": 8316
    },
    {
      "epoch": 0.4507317073170732,
      "step": 8316,
      "training_loss": 7.217872619628906
    },
    {
      "epoch": 0.45078590785907857,
      "step": 8317,
      "training_loss": 6.3045525550842285
    },
    {
      "epoch": 0.450840108401084,
      "step": 8318,
      "training_loss": 6.822809219360352
    },
    {
      "epoch": 0.4508943089430894,
      "step": 8319,
      "training_loss": 6.828903675079346
    },
    {
      "epoch": 0.45094850948509485,
      "grad_norm": 23.80942726135254,
      "learning_rate": 1e-05,
      "loss": 6.7935,
      "step": 8320
    },
    {
      "epoch": 0.45094850948509485,
      "step": 8320,
      "training_loss": 6.860592365264893
    },
    {
      "epoch": 0.4510027100271003,
      "step": 8321,
      "training_loss": 7.049927234649658
    },
    {
      "epoch": 0.4510569105691057,
      "step": 8322,
      "training_loss": 6.330461025238037
    },
    {
      "epoch": 0.45111111111111113,
      "step": 8323,
      "training_loss": 6.600176811218262
    },
    {
      "epoch": 0.4511653116531165,
      "grad_norm": 21.501041412353516,
      "learning_rate": 1e-05,
      "loss": 6.7103,
      "step": 8324
    },
    {
      "epoch": 0.4511653116531165,
      "step": 8324,
      "training_loss": 6.9441022872924805
    },
    {
      "epoch": 0.45121951219512196,
      "step": 8325,
      "training_loss": 8.680023193359375
    },
    {
      "epoch": 0.45127371273712735,
      "step": 8326,
      "training_loss": 9.475342750549316
    },
    {
      "epoch": 0.4513279132791328,
      "step": 8327,
      "training_loss": 4.175700664520264
    },
    {
      "epoch": 0.4513821138211382,
      "grad_norm": 41.92763900756836,
      "learning_rate": 1e-05,
      "loss": 7.3188,
      "step": 8328
    },
    {
      "epoch": 0.4513821138211382,
      "step": 8328,
      "training_loss": 7.985508441925049
    },
    {
      "epoch": 0.45143631436314363,
      "step": 8329,
      "training_loss": 5.986815929412842
    },
    {
      "epoch": 0.4514905149051491,
      "step": 8330,
      "training_loss": 8.043913841247559
    },
    {
      "epoch": 0.45154471544715447,
      "step": 8331,
      "training_loss": 7.422543048858643
    },
    {
      "epoch": 0.4515989159891599,
      "grad_norm": 18.03217124938965,
      "learning_rate": 1e-05,
      "loss": 7.3597,
      "step": 8332
    },
    {
      "epoch": 0.4515989159891599,
      "step": 8332,
      "training_loss": 7.7730278968811035
    },
    {
      "epoch": 0.4516531165311653,
      "step": 8333,
      "training_loss": 5.8201680183410645
    },
    {
      "epoch": 0.45170731707317074,
      "step": 8334,
      "training_loss": 5.934267997741699
    },
    {
      "epoch": 0.45176151761517613,
      "step": 8335,
      "training_loss": 6.675990581512451
    },
    {
      "epoch": 0.4518157181571816,
      "grad_norm": 18.760656356811523,
      "learning_rate": 1e-05,
      "loss": 6.5509,
      "step": 8336
    },
    {
      "epoch": 0.4518157181571816,
      "step": 8336,
      "training_loss": 7.863875389099121
    },
    {
      "epoch": 0.45186991869918697,
      "step": 8337,
      "training_loss": 4.683748245239258
    },
    {
      "epoch": 0.4519241192411924,
      "step": 8338,
      "training_loss": 5.736080646514893
    },
    {
      "epoch": 0.45197831978319786,
      "step": 8339,
      "training_loss": 7.63052225112915
    },
    {
      "epoch": 0.45203252032520325,
      "grad_norm": 27.134845733642578,
      "learning_rate": 1e-05,
      "loss": 6.4786,
      "step": 8340
    },
    {
      "epoch": 0.45203252032520325,
      "step": 8340,
      "training_loss": 5.960819721221924
    },
    {
      "epoch": 0.4520867208672087,
      "step": 8341,
      "training_loss": 6.404706001281738
    },
    {
      "epoch": 0.4521409214092141,
      "step": 8342,
      "training_loss": 6.767082214355469
    },
    {
      "epoch": 0.4521951219512195,
      "step": 8343,
      "training_loss": 6.387109279632568
    },
    {
      "epoch": 0.4522493224932249,
      "grad_norm": 27.968244552612305,
      "learning_rate": 1e-05,
      "loss": 6.3799,
      "step": 8344
    },
    {
      "epoch": 0.4522493224932249,
      "step": 8344,
      "training_loss": 7.056804656982422
    },
    {
      "epoch": 0.45230352303523036,
      "step": 8345,
      "training_loss": 6.9597697257995605
    },
    {
      "epoch": 0.45235772357723575,
      "step": 8346,
      "training_loss": 6.462277412414551
    },
    {
      "epoch": 0.4524119241192412,
      "step": 8347,
      "training_loss": 7.472935199737549
    },
    {
      "epoch": 0.45246612466124664,
      "grad_norm": 27.11907386779785,
      "learning_rate": 1e-05,
      "loss": 6.9879,
      "step": 8348
    },
    {
      "epoch": 0.45246612466124664,
      "step": 8348,
      "training_loss": 3.6941616535186768
    },
    {
      "epoch": 0.452520325203252,
      "step": 8349,
      "training_loss": 7.044703006744385
    },
    {
      "epoch": 0.45257452574525747,
      "step": 8350,
      "training_loss": 7.067488193511963
    },
    {
      "epoch": 0.45262872628726286,
      "step": 8351,
      "training_loss": 6.657807350158691
    },
    {
      "epoch": 0.4526829268292683,
      "grad_norm": 27.897432327270508,
      "learning_rate": 1e-05,
      "loss": 6.116,
      "step": 8352
    },
    {
      "epoch": 0.4526829268292683,
      "step": 8352,
      "training_loss": 7.950738906860352
    },
    {
      "epoch": 0.4527371273712737,
      "step": 8353,
      "training_loss": 6.811282157897949
    },
    {
      "epoch": 0.45279132791327914,
      "step": 8354,
      "training_loss": 7.459352016448975
    },
    {
      "epoch": 0.45284552845528453,
      "step": 8355,
      "training_loss": 7.488129615783691
    },
    {
      "epoch": 0.45289972899729,
      "grad_norm": 21.766653060913086,
      "learning_rate": 1e-05,
      "loss": 7.4274,
      "step": 8356
    },
    {
      "epoch": 0.45289972899729,
      "step": 8356,
      "training_loss": 6.761117458343506
    },
    {
      "epoch": 0.4529539295392954,
      "step": 8357,
      "training_loss": 5.52523946762085
    },
    {
      "epoch": 0.4530081300813008,
      "step": 8358,
      "training_loss": 6.109141826629639
    },
    {
      "epoch": 0.45306233062330625,
      "step": 8359,
      "training_loss": 7.901735782623291
    },
    {
      "epoch": 0.45311653116531164,
      "grad_norm": 20.852689743041992,
      "learning_rate": 1e-05,
      "loss": 6.5743,
      "step": 8360
    },
    {
      "epoch": 0.45311653116531164,
      "step": 8360,
      "training_loss": 6.6889119148254395
    },
    {
      "epoch": 0.4531707317073171,
      "step": 8361,
      "training_loss": 6.573680877685547
    },
    {
      "epoch": 0.4532249322493225,
      "step": 8362,
      "training_loss": 7.065075874328613
    },
    {
      "epoch": 0.4532791327913279,
      "step": 8363,
      "training_loss": 4.416660308837891
    },
    {
      "epoch": 0.4533333333333333,
      "grad_norm": 26.6219539642334,
      "learning_rate": 1e-05,
      "loss": 6.1861,
      "step": 8364
    },
    {
      "epoch": 0.4533333333333333,
      "step": 8364,
      "training_loss": 5.010110855102539
    },
    {
      "epoch": 0.45338753387533876,
      "step": 8365,
      "training_loss": 8.336398124694824
    },
    {
      "epoch": 0.4534417344173442,
      "step": 8366,
      "training_loss": 6.635228157043457
    },
    {
      "epoch": 0.4534959349593496,
      "step": 8367,
      "training_loss": 7.008676528930664
    },
    {
      "epoch": 0.45355013550135503,
      "grad_norm": 18.3210506439209,
      "learning_rate": 1e-05,
      "loss": 6.7476,
      "step": 8368
    },
    {
      "epoch": 0.45355013550135503,
      "step": 8368,
      "training_loss": 7.671557903289795
    },
    {
      "epoch": 0.4536043360433604,
      "step": 8369,
      "training_loss": 6.8732686042785645
    },
    {
      "epoch": 0.45365853658536587,
      "step": 8370,
      "training_loss": 6.054126739501953
    },
    {
      "epoch": 0.45371273712737126,
      "step": 8371,
      "training_loss": 5.921885013580322
    },
    {
      "epoch": 0.4537669376693767,
      "grad_norm": 26.038331985473633,
      "learning_rate": 1e-05,
      "loss": 6.6302,
      "step": 8372
    },
    {
      "epoch": 0.4537669376693767,
      "step": 8372,
      "training_loss": 6.3074564933776855
    },
    {
      "epoch": 0.4538211382113821,
      "step": 8373,
      "training_loss": 6.657670974731445
    },
    {
      "epoch": 0.45387533875338754,
      "step": 8374,
      "training_loss": 8.092293739318848
    },
    {
      "epoch": 0.453929539295393,
      "step": 8375,
      "training_loss": 7.491418361663818
    },
    {
      "epoch": 0.45398373983739837,
      "grad_norm": 18.062284469604492,
      "learning_rate": 1e-05,
      "loss": 7.1372,
      "step": 8376
    },
    {
      "epoch": 0.45398373983739837,
      "step": 8376,
      "training_loss": 7.581141948699951
    },
    {
      "epoch": 0.4540379403794038,
      "step": 8377,
      "training_loss": 5.510718822479248
    },
    {
      "epoch": 0.4540921409214092,
      "step": 8378,
      "training_loss": 6.8154754638671875
    },
    {
      "epoch": 0.45414634146341465,
      "step": 8379,
      "training_loss": 6.733699321746826
    },
    {
      "epoch": 0.45420054200542004,
      "grad_norm": 16.075634002685547,
      "learning_rate": 1e-05,
      "loss": 6.6603,
      "step": 8380
    },
    {
      "epoch": 0.45420054200542004,
      "step": 8380,
      "training_loss": 7.565511703491211
    },
    {
      "epoch": 0.4542547425474255,
      "step": 8381,
      "training_loss": 6.113533020019531
    },
    {
      "epoch": 0.45430894308943087,
      "step": 8382,
      "training_loss": 5.811766624450684
    },
    {
      "epoch": 0.4543631436314363,
      "step": 8383,
      "training_loss": 6.566676616668701
    },
    {
      "epoch": 0.45441734417344176,
      "grad_norm": 20.505207061767578,
      "learning_rate": 1e-05,
      "loss": 6.5144,
      "step": 8384
    },
    {
      "epoch": 0.45441734417344176,
      "step": 8384,
      "training_loss": 7.3166375160217285
    },
    {
      "epoch": 0.45447154471544715,
      "step": 8385,
      "training_loss": 5.826606273651123
    },
    {
      "epoch": 0.4545257452574526,
      "step": 8386,
      "training_loss": 6.133037567138672
    },
    {
      "epoch": 0.454579945799458,
      "step": 8387,
      "training_loss": 5.661354064941406
    },
    {
      "epoch": 0.45463414634146343,
      "grad_norm": 32.76298522949219,
      "learning_rate": 1e-05,
      "loss": 6.2344,
      "step": 8388
    },
    {
      "epoch": 0.45463414634146343,
      "step": 8388,
      "training_loss": 6.4790191650390625
    },
    {
      "epoch": 0.4546883468834688,
      "step": 8389,
      "training_loss": 7.182042121887207
    },
    {
      "epoch": 0.45474254742547426,
      "step": 8390,
      "training_loss": 7.720861911773682
    },
    {
      "epoch": 0.45479674796747965,
      "step": 8391,
      "training_loss": 7.728996276855469
    },
    {
      "epoch": 0.4548509485094851,
      "grad_norm": 37.652835845947266,
      "learning_rate": 1e-05,
      "loss": 7.2777,
      "step": 8392
    },
    {
      "epoch": 0.4548509485094851,
      "step": 8392,
      "training_loss": 7.454465389251709
    },
    {
      "epoch": 0.4549051490514905,
      "step": 8393,
      "training_loss": 6.644428730010986
    },
    {
      "epoch": 0.45495934959349593,
      "step": 8394,
      "training_loss": 7.561052322387695
    },
    {
      "epoch": 0.4550135501355014,
      "step": 8395,
      "training_loss": 7.230408668518066
    },
    {
      "epoch": 0.45506775067750677,
      "grad_norm": 24.340932846069336,
      "learning_rate": 1e-05,
      "loss": 7.2226,
      "step": 8396
    },
    {
      "epoch": 0.45506775067750677,
      "step": 8396,
      "training_loss": 5.292126655578613
    },
    {
      "epoch": 0.4551219512195122,
      "step": 8397,
      "training_loss": 6.304010391235352
    },
    {
      "epoch": 0.4551761517615176,
      "step": 8398,
      "training_loss": 7.611096382141113
    },
    {
      "epoch": 0.45523035230352304,
      "step": 8399,
      "training_loss": 6.934770107269287
    },
    {
      "epoch": 0.45528455284552843,
      "grad_norm": 72.18305969238281,
      "learning_rate": 1e-05,
      "loss": 6.5355,
      "step": 8400
    },
    {
      "epoch": 0.45528455284552843,
      "step": 8400,
      "training_loss": 6.478612899780273
    },
    {
      "epoch": 0.4553387533875339,
      "step": 8401,
      "training_loss": 8.07593822479248
    },
    {
      "epoch": 0.45539295392953927,
      "step": 8402,
      "training_loss": 6.929280757904053
    },
    {
      "epoch": 0.4554471544715447,
      "step": 8403,
      "training_loss": 8.559234619140625
    },
    {
      "epoch": 0.45550135501355016,
      "grad_norm": 37.78125762939453,
      "learning_rate": 1e-05,
      "loss": 7.5108,
      "step": 8404
    },
    {
      "epoch": 0.45550135501355016,
      "step": 8404,
      "training_loss": 7.740141868591309
    },
    {
      "epoch": 0.45555555555555555,
      "step": 8405,
      "training_loss": 7.249494552612305
    },
    {
      "epoch": 0.455609756097561,
      "step": 8406,
      "training_loss": 4.718895435333252
    },
    {
      "epoch": 0.4556639566395664,
      "step": 8407,
      "training_loss": 7.00748872756958
    },
    {
      "epoch": 0.4557181571815718,
      "grad_norm": 14.744338989257812,
      "learning_rate": 1e-05,
      "loss": 6.679,
      "step": 8408
    },
    {
      "epoch": 0.4557181571815718,
      "step": 8408,
      "training_loss": 6.185537338256836
    },
    {
      "epoch": 0.4557723577235772,
      "step": 8409,
      "training_loss": 5.822697639465332
    },
    {
      "epoch": 0.45582655826558266,
      "step": 8410,
      "training_loss": 6.313354015350342
    },
    {
      "epoch": 0.45588075880758805,
      "step": 8411,
      "training_loss": 7.830173492431641
    },
    {
      "epoch": 0.4559349593495935,
      "grad_norm": 25.54146957397461,
      "learning_rate": 1e-05,
      "loss": 6.5379,
      "step": 8412
    },
    {
      "epoch": 0.4559349593495935,
      "step": 8412,
      "training_loss": 7.008570194244385
    },
    {
      "epoch": 0.45598915989159894,
      "step": 8413,
      "training_loss": 8.162601470947266
    },
    {
      "epoch": 0.45604336043360433,
      "step": 8414,
      "training_loss": 6.653845310211182
    },
    {
      "epoch": 0.4560975609756098,
      "step": 8415,
      "training_loss": 7.085592269897461
    },
    {
      "epoch": 0.45615176151761516,
      "grad_norm": 35.83754348754883,
      "learning_rate": 1e-05,
      "loss": 7.2277,
      "step": 8416
    },
    {
      "epoch": 0.45615176151761516,
      "step": 8416,
      "training_loss": 7.176225185394287
    },
    {
      "epoch": 0.4562059620596206,
      "step": 8417,
      "training_loss": 4.808078765869141
    },
    {
      "epoch": 0.456260162601626,
      "step": 8418,
      "training_loss": 7.416661262512207
    },
    {
      "epoch": 0.45631436314363144,
      "step": 8419,
      "training_loss": 7.084159851074219
    },
    {
      "epoch": 0.45636856368563683,
      "grad_norm": 18.835622787475586,
      "learning_rate": 1e-05,
      "loss": 6.6213,
      "step": 8420
    },
    {
      "epoch": 0.45636856368563683,
      "step": 8420,
      "training_loss": 6.252806186676025
    },
    {
      "epoch": 0.4564227642276423,
      "step": 8421,
      "training_loss": 7.361830711364746
    },
    {
      "epoch": 0.4564769647696477,
      "step": 8422,
      "training_loss": 5.932641506195068
    },
    {
      "epoch": 0.4565311653116531,
      "step": 8423,
      "training_loss": 6.086440563201904
    },
    {
      "epoch": 0.45658536585365855,
      "grad_norm": 49.76703643798828,
      "learning_rate": 1e-05,
      "loss": 6.4084,
      "step": 8424
    },
    {
      "epoch": 0.45658536585365855,
      "step": 8424,
      "training_loss": 6.124665260314941
    },
    {
      "epoch": 0.45663956639566394,
      "step": 8425,
      "training_loss": 5.835714817047119
    },
    {
      "epoch": 0.4566937669376694,
      "step": 8426,
      "training_loss": 6.481194496154785
    },
    {
      "epoch": 0.4567479674796748,
      "step": 8427,
      "training_loss": 7.2050461769104
    },
    {
      "epoch": 0.4568021680216802,
      "grad_norm": 23.946561813354492,
      "learning_rate": 1e-05,
      "loss": 6.4117,
      "step": 8428
    },
    {
      "epoch": 0.4568021680216802,
      "step": 8428,
      "training_loss": 5.769561767578125
    },
    {
      "epoch": 0.4568563685636856,
      "step": 8429,
      "training_loss": 4.779145240783691
    },
    {
      "epoch": 0.45691056910569106,
      "step": 8430,
      "training_loss": 6.554823875427246
    },
    {
      "epoch": 0.4569647696476965,
      "step": 8431,
      "training_loss": 4.604783535003662
    },
    {
      "epoch": 0.4570189701897019,
      "grad_norm": 37.633358001708984,
      "learning_rate": 1e-05,
      "loss": 5.4271,
      "step": 8432
    },
    {
      "epoch": 0.4570189701897019,
      "step": 8432,
      "training_loss": 7.92233419418335
    },
    {
      "epoch": 0.45707317073170733,
      "step": 8433,
      "training_loss": 6.397341251373291
    },
    {
      "epoch": 0.4571273712737127,
      "step": 8434,
      "training_loss": 5.66489315032959
    },
    {
      "epoch": 0.45718157181571817,
      "step": 8435,
      "training_loss": 6.529414176940918
    },
    {
      "epoch": 0.45723577235772356,
      "grad_norm": 24.724143981933594,
      "learning_rate": 1e-05,
      "loss": 6.6285,
      "step": 8436
    },
    {
      "epoch": 0.45723577235772356,
      "step": 8436,
      "training_loss": 6.183351516723633
    },
    {
      "epoch": 0.457289972899729,
      "step": 8437,
      "training_loss": 3.657766103744507
    },
    {
      "epoch": 0.4573441734417344,
      "step": 8438,
      "training_loss": 5.95081901550293
    },
    {
      "epoch": 0.45739837398373984,
      "step": 8439,
      "training_loss": 6.172309398651123
    },
    {
      "epoch": 0.4574525745257453,
      "grad_norm": 24.52756690979004,
      "learning_rate": 1e-05,
      "loss": 5.4911,
      "step": 8440
    },
    {
      "epoch": 0.4574525745257453,
      "step": 8440,
      "training_loss": 5.014325141906738
    },
    {
      "epoch": 0.45750677506775067,
      "step": 8441,
      "training_loss": 8.20913028717041
    },
    {
      "epoch": 0.4575609756097561,
      "step": 8442,
      "training_loss": 6.353869438171387
    },
    {
      "epoch": 0.4576151761517615,
      "step": 8443,
      "training_loss": 4.612145900726318
    },
    {
      "epoch": 0.45766937669376695,
      "grad_norm": 24.708412170410156,
      "learning_rate": 1e-05,
      "loss": 6.0474,
      "step": 8444
    },
    {
      "epoch": 0.45766937669376695,
      "step": 8444,
      "training_loss": 7.192948341369629
    },
    {
      "epoch": 0.45772357723577234,
      "step": 8445,
      "training_loss": 5.17155122756958
    },
    {
      "epoch": 0.4577777777777778,
      "step": 8446,
      "training_loss": 6.558570384979248
    },
    {
      "epoch": 0.4578319783197832,
      "step": 8447,
      "training_loss": 6.437488555908203
    },
    {
      "epoch": 0.4578861788617886,
      "grad_norm": 33.15867233276367,
      "learning_rate": 1e-05,
      "loss": 6.3401,
      "step": 8448
    },
    {
      "epoch": 0.4578861788617886,
      "step": 8448,
      "training_loss": 6.6680588722229
    },
    {
      "epoch": 0.45794037940379406,
      "step": 8449,
      "training_loss": 4.302699565887451
    },
    {
      "epoch": 0.45799457994579945,
      "step": 8450,
      "training_loss": 5.915868759155273
    },
    {
      "epoch": 0.4580487804878049,
      "step": 8451,
      "training_loss": 6.142035961151123
    },
    {
      "epoch": 0.4581029810298103,
      "grad_norm": 30.78451156616211,
      "learning_rate": 1e-05,
      "loss": 5.7572,
      "step": 8452
    },
    {
      "epoch": 0.4581029810298103,
      "step": 8452,
      "training_loss": 7.569204807281494
    },
    {
      "epoch": 0.45815718157181573,
      "step": 8453,
      "training_loss": 5.888049602508545
    },
    {
      "epoch": 0.4582113821138211,
      "step": 8454,
      "training_loss": 7.243458271026611
    },
    {
      "epoch": 0.45826558265582656,
      "step": 8455,
      "training_loss": 6.3824462890625
    },
    {
      "epoch": 0.45831978319783195,
      "grad_norm": 39.07431411743164,
      "learning_rate": 1e-05,
      "loss": 6.7708,
      "step": 8456
    },
    {
      "epoch": 0.45831978319783195,
      "step": 8456,
      "training_loss": 5.219546794891357
    },
    {
      "epoch": 0.4583739837398374,
      "step": 8457,
      "training_loss": 6.777557849884033
    },
    {
      "epoch": 0.45842818428184284,
      "step": 8458,
      "training_loss": 6.7231574058532715
    },
    {
      "epoch": 0.45848238482384823,
      "step": 8459,
      "training_loss": 3.4781954288482666
    },
    {
      "epoch": 0.4585365853658537,
      "grad_norm": 31.654659271240234,
      "learning_rate": 1e-05,
      "loss": 5.5496,
      "step": 8460
    },
    {
      "epoch": 0.4585365853658537,
      "step": 8460,
      "training_loss": 5.372468948364258
    },
    {
      "epoch": 0.45859078590785907,
      "step": 8461,
      "training_loss": 6.765272617340088
    },
    {
      "epoch": 0.4586449864498645,
      "step": 8462,
      "training_loss": 7.336417198181152
    },
    {
      "epoch": 0.4586991869918699,
      "step": 8463,
      "training_loss": 7.030848026275635
    },
    {
      "epoch": 0.45875338753387535,
      "grad_norm": 21.43985366821289,
      "learning_rate": 1e-05,
      "loss": 6.6263,
      "step": 8464
    },
    {
      "epoch": 0.45875338753387535,
      "step": 8464,
      "training_loss": 6.7674880027771
    },
    {
      "epoch": 0.45880758807588073,
      "step": 8465,
      "training_loss": 7.138282775878906
    },
    {
      "epoch": 0.4588617886178862,
      "step": 8466,
      "training_loss": 5.9202561378479
    },
    {
      "epoch": 0.4589159891598916,
      "step": 8467,
      "training_loss": 7.012590408325195
    },
    {
      "epoch": 0.458970189701897,
      "grad_norm": 32.85957717895508,
      "learning_rate": 1e-05,
      "loss": 6.7097,
      "step": 8468
    },
    {
      "epoch": 0.458970189701897,
      "step": 8468,
      "training_loss": 7.371553421020508
    },
    {
      "epoch": 0.45902439024390246,
      "step": 8469,
      "training_loss": 7.5112481117248535
    },
    {
      "epoch": 0.45907859078590785,
      "step": 8470,
      "training_loss": 6.438809394836426
    },
    {
      "epoch": 0.4591327913279133,
      "step": 8471,
      "training_loss": 7.826340675354004
    },
    {
      "epoch": 0.4591869918699187,
      "grad_norm": 22.654726028442383,
      "learning_rate": 1e-05,
      "loss": 7.287,
      "step": 8472
    },
    {
      "epoch": 0.4591869918699187,
      "step": 8472,
      "training_loss": 8.873156547546387
    },
    {
      "epoch": 0.4592411924119241,
      "step": 8473,
      "training_loss": 6.676289081573486
    },
    {
      "epoch": 0.4592953929539295,
      "step": 8474,
      "training_loss": 5.105082988739014
    },
    {
      "epoch": 0.45934959349593496,
      "step": 8475,
      "training_loss": 5.923914432525635
    },
    {
      "epoch": 0.4594037940379404,
      "grad_norm": 35.274112701416016,
      "learning_rate": 1e-05,
      "loss": 6.6446,
      "step": 8476
    },
    {
      "epoch": 0.4594037940379404,
      "step": 8476,
      "training_loss": 5.908389568328857
    },
    {
      "epoch": 0.4594579945799458,
      "step": 8477,
      "training_loss": 3.248802900314331
    },
    {
      "epoch": 0.45951219512195124,
      "step": 8478,
      "training_loss": 6.365559101104736
    },
    {
      "epoch": 0.45956639566395663,
      "step": 8479,
      "training_loss": 5.463208198547363
    },
    {
      "epoch": 0.4596205962059621,
      "grad_norm": 53.78577423095703,
      "learning_rate": 1e-05,
      "loss": 5.2465,
      "step": 8480
    },
    {
      "epoch": 0.4596205962059621,
      "step": 8480,
      "training_loss": 7.317103862762451
    },
    {
      "epoch": 0.45967479674796746,
      "step": 8481,
      "training_loss": 7.028984069824219
    },
    {
      "epoch": 0.4597289972899729,
      "step": 8482,
      "training_loss": 7.5140862464904785
    },
    {
      "epoch": 0.4597831978319783,
      "step": 8483,
      "training_loss": 6.746005535125732
    },
    {
      "epoch": 0.45983739837398374,
      "grad_norm": 21.463171005249023,
      "learning_rate": 1e-05,
      "loss": 7.1515,
      "step": 8484
    },
    {
      "epoch": 0.45983739837398374,
      "step": 8484,
      "training_loss": 7.468868255615234
    },
    {
      "epoch": 0.4598915989159892,
      "step": 8485,
      "training_loss": 7.394130229949951
    },
    {
      "epoch": 0.4599457994579946,
      "step": 8486,
      "training_loss": 4.515610694885254
    },
    {
      "epoch": 0.46,
      "step": 8487,
      "training_loss": 6.136929512023926
    },
    {
      "epoch": 0.4600542005420054,
      "grad_norm": 27.053024291992188,
      "learning_rate": 1e-05,
      "loss": 6.3789,
      "step": 8488
    },
    {
      "epoch": 0.4600542005420054,
      "step": 8488,
      "training_loss": 6.512476444244385
    },
    {
      "epoch": 0.46010840108401085,
      "step": 8489,
      "training_loss": 6.828607559204102
    },
    {
      "epoch": 0.46016260162601624,
      "step": 8490,
      "training_loss": 3.3929450511932373
    },
    {
      "epoch": 0.4602168021680217,
      "step": 8491,
      "training_loss": 7.026132106781006
    },
    {
      "epoch": 0.4602710027100271,
      "grad_norm": 35.406578063964844,
      "learning_rate": 1e-05,
      "loss": 5.94,
      "step": 8492
    },
    {
      "epoch": 0.4602710027100271,
      "step": 8492,
      "training_loss": 6.365007400512695
    },
    {
      "epoch": 0.4603252032520325,
      "step": 8493,
      "training_loss": 7.0618486404418945
    },
    {
      "epoch": 0.46037940379403797,
      "step": 8494,
      "training_loss": 7.258636951446533
    },
    {
      "epoch": 0.46043360433604336,
      "step": 8495,
      "training_loss": 6.21249532699585
    },
    {
      "epoch": 0.4604878048780488,
      "grad_norm": 38.1852912902832,
      "learning_rate": 1e-05,
      "loss": 6.7245,
      "step": 8496
    },
    {
      "epoch": 0.4604878048780488,
      "step": 8496,
      "training_loss": 5.334940433502197
    },
    {
      "epoch": 0.4605420054200542,
      "step": 8497,
      "training_loss": 5.650568962097168
    },
    {
      "epoch": 0.46059620596205963,
      "step": 8498,
      "training_loss": 4.020259857177734
    },
    {
      "epoch": 0.460650406504065,
      "step": 8499,
      "training_loss": 6.019588947296143
    },
    {
      "epoch": 0.46070460704607047,
      "grad_norm": 30.821977615356445,
      "learning_rate": 1e-05,
      "loss": 5.2563,
      "step": 8500
    },
    {
      "epoch": 0.46070460704607047,
      "step": 8500,
      "training_loss": 6.94798469543457
    },
    {
      "epoch": 0.46075880758807586,
      "step": 8501,
      "training_loss": 6.5714030265808105
    },
    {
      "epoch": 0.4608130081300813,
      "step": 8502,
      "training_loss": 6.798056602478027
    },
    {
      "epoch": 0.46086720867208675,
      "step": 8503,
      "training_loss": 7.37143087387085
    },
    {
      "epoch": 0.46092140921409214,
      "grad_norm": 19.84378433227539,
      "learning_rate": 1e-05,
      "loss": 6.9222,
      "step": 8504
    },
    {
      "epoch": 0.46092140921409214,
      "step": 8504,
      "training_loss": 7.9041643142700195
    },
    {
      "epoch": 0.4609756097560976,
      "step": 8505,
      "training_loss": 7.2931036949157715
    },
    {
      "epoch": 0.46102981029810297,
      "step": 8506,
      "training_loss": 7.065906047821045
    },
    {
      "epoch": 0.4610840108401084,
      "step": 8507,
      "training_loss": 5.112383842468262
    },
    {
      "epoch": 0.4611382113821138,
      "grad_norm": 22.71832275390625,
      "learning_rate": 1e-05,
      "loss": 6.8439,
      "step": 8508
    },
    {
      "epoch": 0.4611382113821138,
      "step": 8508,
      "training_loss": 5.570858955383301
    },
    {
      "epoch": 0.46119241192411925,
      "step": 8509,
      "training_loss": 7.087912559509277
    },
    {
      "epoch": 0.46124661246612464,
      "step": 8510,
      "training_loss": 6.872337818145752
    },
    {
      "epoch": 0.4613008130081301,
      "step": 8511,
      "training_loss": 6.001792907714844
    },
    {
      "epoch": 0.46135501355013553,
      "grad_norm": 28.49165153503418,
      "learning_rate": 1e-05,
      "loss": 6.3832,
      "step": 8512
    },
    {
      "epoch": 0.46135501355013553,
      "step": 8512,
      "training_loss": 6.102360248565674
    },
    {
      "epoch": 0.4614092140921409,
      "step": 8513,
      "training_loss": 6.1168975830078125
    },
    {
      "epoch": 0.46146341463414636,
      "step": 8514,
      "training_loss": 6.576451301574707
    },
    {
      "epoch": 0.46151761517615175,
      "step": 8515,
      "training_loss": 9.17586898803711
    },
    {
      "epoch": 0.4615718157181572,
      "grad_norm": 34.819026947021484,
      "learning_rate": 1e-05,
      "loss": 6.9929,
      "step": 8516
    },
    {
      "epoch": 0.4615718157181572,
      "step": 8516,
      "training_loss": 5.884393215179443
    },
    {
      "epoch": 0.4616260162601626,
      "step": 8517,
      "training_loss": 7.537317752838135
    },
    {
      "epoch": 0.46168021680216803,
      "step": 8518,
      "training_loss": 6.1886067390441895
    },
    {
      "epoch": 0.4617344173441734,
      "step": 8519,
      "training_loss": 5.967593193054199
    },
    {
      "epoch": 0.46178861788617886,
      "grad_norm": 26.896198272705078,
      "learning_rate": 1e-05,
      "loss": 6.3945,
      "step": 8520
    },
    {
      "epoch": 0.46178861788617886,
      "step": 8520,
      "training_loss": 6.77141809463501
    },
    {
      "epoch": 0.46184281842818425,
      "step": 8521,
      "training_loss": 6.872450828552246
    },
    {
      "epoch": 0.4618970189701897,
      "step": 8522,
      "training_loss": 5.975635528564453
    },
    {
      "epoch": 0.46195121951219514,
      "step": 8523,
      "training_loss": 7.094635009765625
    },
    {
      "epoch": 0.46200542005420053,
      "grad_norm": 40.893428802490234,
      "learning_rate": 1e-05,
      "loss": 6.6785,
      "step": 8524
    },
    {
      "epoch": 0.46200542005420053,
      "step": 8524,
      "training_loss": 7.321335792541504
    },
    {
      "epoch": 0.462059620596206,
      "step": 8525,
      "training_loss": 6.038415908813477
    },
    {
      "epoch": 0.46211382113821137,
      "step": 8526,
      "training_loss": 6.049764633178711
    },
    {
      "epoch": 0.4621680216802168,
      "step": 8527,
      "training_loss": 5.918550491333008
    },
    {
      "epoch": 0.4622222222222222,
      "grad_norm": 49.22575378417969,
      "learning_rate": 1e-05,
      "loss": 6.332,
      "step": 8528
    },
    {
      "epoch": 0.4622222222222222,
      "step": 8528,
      "training_loss": 8.072705268859863
    },
    {
      "epoch": 0.46227642276422765,
      "step": 8529,
      "training_loss": 6.833769798278809
    },
    {
      "epoch": 0.46233062330623304,
      "step": 8530,
      "training_loss": 6.88467264175415
    },
    {
      "epoch": 0.4623848238482385,
      "step": 8531,
      "training_loss": 7.5770649909973145
    },
    {
      "epoch": 0.4624390243902439,
      "grad_norm": 21.763357162475586,
      "learning_rate": 1e-05,
      "loss": 7.3421,
      "step": 8532
    },
    {
      "epoch": 0.4624390243902439,
      "step": 8532,
      "training_loss": 9.130842208862305
    },
    {
      "epoch": 0.4624932249322493,
      "step": 8533,
      "training_loss": 6.766871929168701
    },
    {
      "epoch": 0.46254742547425476,
      "step": 8534,
      "training_loss": 7.990833282470703
    },
    {
      "epoch": 0.46260162601626015,
      "step": 8535,
      "training_loss": 7.064733982086182
    },
    {
      "epoch": 0.4626558265582656,
      "grad_norm": 31.822816848754883,
      "learning_rate": 1e-05,
      "loss": 7.7383,
      "step": 8536
    },
    {
      "epoch": 0.4626558265582656,
      "step": 8536,
      "training_loss": 7.80715274810791
    },
    {
      "epoch": 0.462710027100271,
      "step": 8537,
      "training_loss": 6.525784015655518
    },
    {
      "epoch": 0.4627642276422764,
      "step": 8538,
      "training_loss": 6.7311296463012695
    },
    {
      "epoch": 0.4628184281842818,
      "step": 8539,
      "training_loss": 6.797152996063232
    },
    {
      "epoch": 0.46287262872628726,
      "grad_norm": 54.77821350097656,
      "learning_rate": 1e-05,
      "loss": 6.9653,
      "step": 8540
    },
    {
      "epoch": 0.46287262872628726,
      "step": 8540,
      "training_loss": 6.643914222717285
    },
    {
      "epoch": 0.4629268292682927,
      "step": 8541,
      "training_loss": 6.978574752807617
    },
    {
      "epoch": 0.4629810298102981,
      "step": 8542,
      "training_loss": 6.99643087387085
    },
    {
      "epoch": 0.46303523035230354,
      "step": 8543,
      "training_loss": 6.467453956604004
    },
    {
      "epoch": 0.46308943089430893,
      "grad_norm": 21.548940658569336,
      "learning_rate": 1e-05,
      "loss": 6.7716,
      "step": 8544
    },
    {
      "epoch": 0.46308943089430893,
      "step": 8544,
      "training_loss": 5.906507968902588
    },
    {
      "epoch": 0.4631436314363144,
      "step": 8545,
      "training_loss": 6.773350238800049
    },
    {
      "epoch": 0.46319783197831976,
      "step": 8546,
      "training_loss": 6.72076940536499
    },
    {
      "epoch": 0.4632520325203252,
      "step": 8547,
      "training_loss": 6.5645036697387695
    },
    {
      "epoch": 0.4633062330623306,
      "grad_norm": 30.533859252929688,
      "learning_rate": 1e-05,
      "loss": 6.4913,
      "step": 8548
    },
    {
      "epoch": 0.4633062330623306,
      "step": 8548,
      "training_loss": 8.055243492126465
    },
    {
      "epoch": 0.46336043360433604,
      "step": 8549,
      "training_loss": 7.054098606109619
    },
    {
      "epoch": 0.4634146341463415,
      "step": 8550,
      "training_loss": 6.2277069091796875
    },
    {
      "epoch": 0.4634688346883469,
      "step": 8551,
      "training_loss": 6.668924331665039
    },
    {
      "epoch": 0.4635230352303523,
      "grad_norm": 20.28780746459961,
      "learning_rate": 1e-05,
      "loss": 7.0015,
      "step": 8552
    },
    {
      "epoch": 0.4635230352303523,
      "step": 8552,
      "training_loss": 6.273198127746582
    },
    {
      "epoch": 0.4635772357723577,
      "step": 8553,
      "training_loss": 7.2938947677612305
    },
    {
      "epoch": 0.46363143631436315,
      "step": 8554,
      "training_loss": 7.115691184997559
    },
    {
      "epoch": 0.46368563685636854,
      "step": 8555,
      "training_loss": 6.6697893142700195
    },
    {
      "epoch": 0.463739837398374,
      "grad_norm": 31.922800064086914,
      "learning_rate": 1e-05,
      "loss": 6.8381,
      "step": 8556
    },
    {
      "epoch": 0.463739837398374,
      "step": 8556,
      "training_loss": 5.0353522300720215
    },
    {
      "epoch": 0.4637940379403794,
      "step": 8557,
      "training_loss": 7.86930513381958
    },
    {
      "epoch": 0.4638482384823848,
      "step": 8558,
      "training_loss": 3.6697776317596436
    },
    {
      "epoch": 0.46390243902439027,
      "step": 8559,
      "training_loss": 7.924859046936035
    },
    {
      "epoch": 0.46395663956639566,
      "grad_norm": 43.71812057495117,
      "learning_rate": 1e-05,
      "loss": 6.1248,
      "step": 8560
    },
    {
      "epoch": 0.46395663956639566,
      "step": 8560,
      "training_loss": 6.81377649307251
    },
    {
      "epoch": 0.4640108401084011,
      "step": 8561,
      "training_loss": 6.989928722381592
    },
    {
      "epoch": 0.4640650406504065,
      "step": 8562,
      "training_loss": 8.277199745178223
    },
    {
      "epoch": 0.46411924119241194,
      "step": 8563,
      "training_loss": 7.145013809204102
    },
    {
      "epoch": 0.4641734417344173,
      "grad_norm": 20.483856201171875,
      "learning_rate": 1e-05,
      "loss": 7.3065,
      "step": 8564
    },
    {
      "epoch": 0.4641734417344173,
      "step": 8564,
      "training_loss": 6.292242527008057
    },
    {
      "epoch": 0.46422764227642277,
      "step": 8565,
      "training_loss": 6.426300048828125
    },
    {
      "epoch": 0.46428184281842816,
      "step": 8566,
      "training_loss": 7.638297080993652
    },
    {
      "epoch": 0.4643360433604336,
      "step": 8567,
      "training_loss": 6.956747055053711
    },
    {
      "epoch": 0.46439024390243905,
      "grad_norm": 34.66873550415039,
      "learning_rate": 1e-05,
      "loss": 6.8284,
      "step": 8568
    },
    {
      "epoch": 0.46439024390243905,
      "step": 8568,
      "training_loss": 6.180942535400391
    },
    {
      "epoch": 0.46444444444444444,
      "step": 8569,
      "training_loss": 3.585958480834961
    },
    {
      "epoch": 0.4644986449864499,
      "step": 8570,
      "training_loss": 7.735703468322754
    },
    {
      "epoch": 0.46455284552845527,
      "step": 8571,
      "training_loss": 4.153733253479004
    },
    {
      "epoch": 0.4646070460704607,
      "grad_norm": 38.79208755493164,
      "learning_rate": 1e-05,
      "loss": 5.4141,
      "step": 8572
    },
    {
      "epoch": 0.4646070460704607,
      "step": 8572,
      "training_loss": 5.978084564208984
    },
    {
      "epoch": 0.4646612466124661,
      "step": 8573,
      "training_loss": 7.2192254066467285
    },
    {
      "epoch": 0.46471544715447155,
      "step": 8574,
      "training_loss": 7.480886459350586
    },
    {
      "epoch": 0.46476964769647694,
      "step": 8575,
      "training_loss": 6.196849346160889
    },
    {
      "epoch": 0.4648238482384824,
      "grad_norm": 35.37839126586914,
      "learning_rate": 1e-05,
      "loss": 6.7188,
      "step": 8576
    },
    {
      "epoch": 0.4648238482384824,
      "step": 8576,
      "training_loss": 7.4422831535339355
    },
    {
      "epoch": 0.46487804878048783,
      "step": 8577,
      "training_loss": 6.8868021965026855
    },
    {
      "epoch": 0.4649322493224932,
      "step": 8578,
      "training_loss": 3.6717631816864014
    },
    {
      "epoch": 0.46498644986449866,
      "step": 8579,
      "training_loss": 6.409389495849609
    },
    {
      "epoch": 0.46504065040650405,
      "grad_norm": 23.002132415771484,
      "learning_rate": 1e-05,
      "loss": 6.1026,
      "step": 8580
    },
    {
      "epoch": 0.46504065040650405,
      "step": 8580,
      "training_loss": 7.8093390464782715
    },
    {
      "epoch": 0.4650948509485095,
      "step": 8581,
      "training_loss": 7.974227428436279
    },
    {
      "epoch": 0.4651490514905149,
      "step": 8582,
      "training_loss": 8.1430025100708
    },
    {
      "epoch": 0.46520325203252033,
      "step": 8583,
      "training_loss": 5.116078853607178
    },
    {
      "epoch": 0.4652574525745257,
      "grad_norm": 43.27751541137695,
      "learning_rate": 1e-05,
      "loss": 7.2607,
      "step": 8584
    },
    {
      "epoch": 0.4652574525745257,
      "step": 8584,
      "training_loss": 5.507007122039795
    },
    {
      "epoch": 0.46531165311653117,
      "step": 8585,
      "training_loss": 5.968656539916992
    },
    {
      "epoch": 0.4653658536585366,
      "step": 8586,
      "training_loss": 5.933338165283203
    },
    {
      "epoch": 0.465420054200542,
      "step": 8587,
      "training_loss": 7.475781440734863
    },
    {
      "epoch": 0.46547425474254744,
      "grad_norm": 25.0750732421875,
      "learning_rate": 1e-05,
      "loss": 6.2212,
      "step": 8588
    },
    {
      "epoch": 0.46547425474254744,
      "step": 8588,
      "training_loss": 5.780303478240967
    },
    {
      "epoch": 0.46552845528455283,
      "step": 8589,
      "training_loss": 4.539261817932129
    },
    {
      "epoch": 0.4655826558265583,
      "step": 8590,
      "training_loss": 6.117464542388916
    },
    {
      "epoch": 0.46563685636856367,
      "step": 8591,
      "training_loss": 6.94151496887207
    },
    {
      "epoch": 0.4656910569105691,
      "grad_norm": 29.821840286254883,
      "learning_rate": 1e-05,
      "loss": 5.8446,
      "step": 8592
    },
    {
      "epoch": 0.4656910569105691,
      "step": 8592,
      "training_loss": 6.707578659057617
    },
    {
      "epoch": 0.4657452574525745,
      "step": 8593,
      "training_loss": 3.934690237045288
    },
    {
      "epoch": 0.46579945799457995,
      "step": 8594,
      "training_loss": 4.446638107299805
    },
    {
      "epoch": 0.4658536585365854,
      "step": 8595,
      "training_loss": 7.732671737670898
    },
    {
      "epoch": 0.4659078590785908,
      "grad_norm": 17.625490188598633,
      "learning_rate": 1e-05,
      "loss": 5.7054,
      "step": 8596
    },
    {
      "epoch": 0.4659078590785908,
      "step": 8596,
      "training_loss": 6.815301895141602
    },
    {
      "epoch": 0.4659620596205962,
      "step": 8597,
      "training_loss": 6.730497360229492
    },
    {
      "epoch": 0.4660162601626016,
      "step": 8598,
      "training_loss": 7.3247246742248535
    },
    {
      "epoch": 0.46607046070460706,
      "step": 8599,
      "training_loss": 6.212113380432129
    },
    {
      "epoch": 0.46612466124661245,
      "grad_norm": 37.60476303100586,
      "learning_rate": 1e-05,
      "loss": 6.7707,
      "step": 8600
    },
    {
      "epoch": 0.46612466124661245,
      "step": 8600,
      "training_loss": 5.159328937530518
    },
    {
      "epoch": 0.4661788617886179,
      "step": 8601,
      "training_loss": 6.279941082000732
    },
    {
      "epoch": 0.4662330623306233,
      "step": 8602,
      "training_loss": 6.097900867462158
    },
    {
      "epoch": 0.4662872628726287,
      "step": 8603,
      "training_loss": 7.815320014953613
    },
    {
      "epoch": 0.46634146341463417,
      "grad_norm": 31.801382064819336,
      "learning_rate": 1e-05,
      "loss": 6.3381,
      "step": 8604
    },
    {
      "epoch": 0.46634146341463417,
      "step": 8604,
      "training_loss": 7.113962650299072
    },
    {
      "epoch": 0.46639566395663956,
      "step": 8605,
      "training_loss": 6.084290981292725
    },
    {
      "epoch": 0.466449864498645,
      "step": 8606,
      "training_loss": 6.223576545715332
    },
    {
      "epoch": 0.4665040650406504,
      "step": 8607,
      "training_loss": 7.393270969390869
    },
    {
      "epoch": 0.46655826558265584,
      "grad_norm": 23.85835838317871,
      "learning_rate": 1e-05,
      "loss": 6.7038,
      "step": 8608
    },
    {
      "epoch": 0.46655826558265584,
      "step": 8608,
      "training_loss": 7.386989116668701
    },
    {
      "epoch": 0.46661246612466123,
      "step": 8609,
      "training_loss": 7.202570915222168
    },
    {
      "epoch": 0.4666666666666667,
      "step": 8610,
      "training_loss": 7.2412109375
    },
    {
      "epoch": 0.46672086720867206,
      "step": 8611,
      "training_loss": 6.610138416290283
    },
    {
      "epoch": 0.4667750677506775,
      "grad_norm": 28.416152954101562,
      "learning_rate": 1e-05,
      "loss": 7.1102,
      "step": 8612
    },
    {
      "epoch": 0.4667750677506775,
      "step": 8612,
      "training_loss": 6.7035393714904785
    },
    {
      "epoch": 0.46682926829268295,
      "step": 8613,
      "training_loss": 7.739098072052002
    },
    {
      "epoch": 0.46688346883468834,
      "step": 8614,
      "training_loss": 7.476682186126709
    },
    {
      "epoch": 0.4669376693766938,
      "step": 8615,
      "training_loss": 6.520513534545898
    },
    {
      "epoch": 0.4669918699186992,
      "grad_norm": 21.152376174926758,
      "learning_rate": 1e-05,
      "loss": 7.11,
      "step": 8616
    },
    {
      "epoch": 0.4669918699186992,
      "step": 8616,
      "training_loss": 6.327324867248535
    },
    {
      "epoch": 0.4670460704607046,
      "step": 8617,
      "training_loss": 8.215302467346191
    },
    {
      "epoch": 0.46710027100271,
      "step": 8618,
      "training_loss": 8.076729774475098
    },
    {
      "epoch": 0.46715447154471545,
      "step": 8619,
      "training_loss": 7.17300271987915
    },
    {
      "epoch": 0.46720867208672084,
      "grad_norm": 36.71858215332031,
      "learning_rate": 1e-05,
      "loss": 7.4481,
      "step": 8620
    },
    {
      "epoch": 0.46720867208672084,
      "step": 8620,
      "training_loss": 7.422115802764893
    },
    {
      "epoch": 0.4672628726287263,
      "step": 8621,
      "training_loss": 6.101724147796631
    },
    {
      "epoch": 0.46731707317073173,
      "step": 8622,
      "training_loss": 7.22337532043457
    },
    {
      "epoch": 0.4673712737127371,
      "step": 8623,
      "training_loss": 4.641124725341797
    },
    {
      "epoch": 0.46742547425474257,
      "grad_norm": 26.472517013549805,
      "learning_rate": 1e-05,
      "loss": 6.3471,
      "step": 8624
    },
    {
      "epoch": 0.46742547425474257,
      "step": 8624,
      "training_loss": 6.414724826812744
    },
    {
      "epoch": 0.46747967479674796,
      "step": 8625,
      "training_loss": 7.502845287322998
    },
    {
      "epoch": 0.4675338753387534,
      "step": 8626,
      "training_loss": 5.964104175567627
    },
    {
      "epoch": 0.4675880758807588,
      "step": 8627,
      "training_loss": 6.581466197967529
    },
    {
      "epoch": 0.46764227642276424,
      "grad_norm": 21.83155632019043,
      "learning_rate": 1e-05,
      "loss": 6.6158,
      "step": 8628
    },
    {
      "epoch": 0.46764227642276424,
      "step": 8628,
      "training_loss": 3.115542411804199
    },
    {
      "epoch": 0.4676964769647696,
      "step": 8629,
      "training_loss": 7.028499603271484
    },
    {
      "epoch": 0.46775067750677507,
      "step": 8630,
      "training_loss": 6.995840549468994
    },
    {
      "epoch": 0.4678048780487805,
      "step": 8631,
      "training_loss": 5.899604797363281
    },
    {
      "epoch": 0.4678590785907859,
      "grad_norm": 39.433345794677734,
      "learning_rate": 1e-05,
      "loss": 5.7599,
      "step": 8632
    },
    {
      "epoch": 0.4678590785907859,
      "step": 8632,
      "training_loss": 7.41007661819458
    },
    {
      "epoch": 0.46791327913279135,
      "step": 8633,
      "training_loss": 6.320960521697998
    },
    {
      "epoch": 0.46796747967479674,
      "step": 8634,
      "training_loss": 6.391262054443359
    },
    {
      "epoch": 0.4680216802168022,
      "step": 8635,
      "training_loss": 3.8357062339782715
    },
    {
      "epoch": 0.46807588075880757,
      "grad_norm": 29.33900260925293,
      "learning_rate": 1e-05,
      "loss": 5.9895,
      "step": 8636
    },
    {
      "epoch": 0.46807588075880757,
      "step": 8636,
      "training_loss": 6.930405139923096
    },
    {
      "epoch": 0.468130081300813,
      "step": 8637,
      "training_loss": 6.475010871887207
    },
    {
      "epoch": 0.4681842818428184,
      "step": 8638,
      "training_loss": 7.142306327819824
    },
    {
      "epoch": 0.46823848238482385,
      "step": 8639,
      "training_loss": 5.883336067199707
    },
    {
      "epoch": 0.4682926829268293,
      "grad_norm": 30.191373825073242,
      "learning_rate": 1e-05,
      "loss": 6.6078,
      "step": 8640
    },
    {
      "epoch": 0.4682926829268293,
      "step": 8640,
      "training_loss": 5.692075252532959
    },
    {
      "epoch": 0.4683468834688347,
      "step": 8641,
      "training_loss": 6.378225326538086
    },
    {
      "epoch": 0.46840108401084013,
      "step": 8642,
      "training_loss": 7.863348007202148
    },
    {
      "epoch": 0.4684552845528455,
      "step": 8643,
      "training_loss": 6.402125835418701
    },
    {
      "epoch": 0.46850948509485096,
      "grad_norm": 20.056215286254883,
      "learning_rate": 1e-05,
      "loss": 6.5839,
      "step": 8644
    },
    {
      "epoch": 0.46850948509485096,
      "step": 8644,
      "training_loss": 5.916999816894531
    },
    {
      "epoch": 0.46856368563685635,
      "step": 8645,
      "training_loss": 6.745124816894531
    },
    {
      "epoch": 0.4686178861788618,
      "step": 8646,
      "training_loss": 6.499805450439453
    },
    {
      "epoch": 0.4686720867208672,
      "step": 8647,
      "training_loss": 7.4791131019592285
    },
    {
      "epoch": 0.46872628726287263,
      "grad_norm": 42.851722717285156,
      "learning_rate": 1e-05,
      "loss": 6.6603,
      "step": 8648
    },
    {
      "epoch": 0.46872628726287263,
      "step": 8648,
      "training_loss": 7.381659030914307
    },
    {
      "epoch": 0.468780487804878,
      "step": 8649,
      "training_loss": 6.790375232696533
    },
    {
      "epoch": 0.46883468834688347,
      "step": 8650,
      "training_loss": 6.982457160949707
    },
    {
      "epoch": 0.4688888888888889,
      "step": 8651,
      "training_loss": 7.54249382019043
    },
    {
      "epoch": 0.4689430894308943,
      "grad_norm": 44.048187255859375,
      "learning_rate": 1e-05,
      "loss": 7.1742,
      "step": 8652
    },
    {
      "epoch": 0.4689430894308943,
      "step": 8652,
      "training_loss": 7.2123517990112305
    },
    {
      "epoch": 0.46899728997289974,
      "step": 8653,
      "training_loss": 4.911919593811035
    },
    {
      "epoch": 0.46905149051490513,
      "step": 8654,
      "training_loss": 7.718087196350098
    },
    {
      "epoch": 0.4691056910569106,
      "step": 8655,
      "training_loss": 8.062870979309082
    },
    {
      "epoch": 0.46915989159891597,
      "grad_norm": 23.22121238708496,
      "learning_rate": 1e-05,
      "loss": 6.9763,
      "step": 8656
    },
    {
      "epoch": 0.46915989159891597,
      "step": 8656,
      "training_loss": 7.439856052398682
    },
    {
      "epoch": 0.4692140921409214,
      "step": 8657,
      "training_loss": 6.0522003173828125
    },
    {
      "epoch": 0.4692682926829268,
      "step": 8658,
      "training_loss": 7.7125091552734375
    },
    {
      "epoch": 0.46932249322493225,
      "step": 8659,
      "training_loss": 7.214272499084473
    },
    {
      "epoch": 0.4693766937669377,
      "grad_norm": 19.825796127319336,
      "learning_rate": 1e-05,
      "loss": 7.1047,
      "step": 8660
    },
    {
      "epoch": 0.4693766937669377,
      "step": 8660,
      "training_loss": 5.923830986022949
    },
    {
      "epoch": 0.4694308943089431,
      "step": 8661,
      "training_loss": 6.521286487579346
    },
    {
      "epoch": 0.4694850948509485,
      "step": 8662,
      "training_loss": 6.038105010986328
    },
    {
      "epoch": 0.4695392953929539,
      "step": 8663,
      "training_loss": 6.662823677062988
    },
    {
      "epoch": 0.46959349593495936,
      "grad_norm": 22.464780807495117,
      "learning_rate": 1e-05,
      "loss": 6.2865,
      "step": 8664
    },
    {
      "epoch": 0.46959349593495936,
      "step": 8664,
      "training_loss": 6.864611625671387
    },
    {
      "epoch": 0.46964769647696475,
      "step": 8665,
      "training_loss": 6.272677898406982
    },
    {
      "epoch": 0.4697018970189702,
      "step": 8666,
      "training_loss": 6.4762725830078125
    },
    {
      "epoch": 0.4697560975609756,
      "step": 8667,
      "training_loss": 3.74747896194458
    },
    {
      "epoch": 0.469810298102981,
      "grad_norm": 29.769132614135742,
      "learning_rate": 1e-05,
      "loss": 5.8403,
      "step": 8668
    },
    {
      "epoch": 0.469810298102981,
      "step": 8668,
      "training_loss": 7.112630844116211
    },
    {
      "epoch": 0.4698644986449865,
      "step": 8669,
      "training_loss": 6.855426788330078
    },
    {
      "epoch": 0.46991869918699186,
      "step": 8670,
      "training_loss": 3.3026342391967773
    },
    {
      "epoch": 0.4699728997289973,
      "step": 8671,
      "training_loss": 5.98288106918335
    },
    {
      "epoch": 0.4700271002710027,
      "grad_norm": 39.04652786254883,
      "learning_rate": 1e-05,
      "loss": 5.8134,
      "step": 8672
    },
    {
      "epoch": 0.4700271002710027,
      "step": 8672,
      "training_loss": 6.556191921234131
    },
    {
      "epoch": 0.47008130081300814,
      "step": 8673,
      "training_loss": 6.0853495597839355
    },
    {
      "epoch": 0.47013550135501353,
      "step": 8674,
      "training_loss": 7.320001602172852
    },
    {
      "epoch": 0.470189701897019,
      "step": 8675,
      "training_loss": 5.3924174308776855
    },
    {
      "epoch": 0.47024390243902436,
      "grad_norm": 47.62234878540039,
      "learning_rate": 1e-05,
      "loss": 6.3385,
      "step": 8676
    },
    {
      "epoch": 0.47024390243902436,
      "step": 8676,
      "training_loss": 5.716021537780762
    },
    {
      "epoch": 0.4702981029810298,
      "step": 8677,
      "training_loss": 6.370461463928223
    },
    {
      "epoch": 0.47035230352303525,
      "step": 8678,
      "training_loss": 7.755607604980469
    },
    {
      "epoch": 0.47040650406504064,
      "step": 8679,
      "training_loss": 6.985742092132568
    },
    {
      "epoch": 0.4704607046070461,
      "grad_norm": 23.86858367919922,
      "learning_rate": 1e-05,
      "loss": 6.707,
      "step": 8680
    },
    {
      "epoch": 0.4704607046070461,
      "step": 8680,
      "training_loss": 7.509665489196777
    },
    {
      "epoch": 0.4705149051490515,
      "step": 8681,
      "training_loss": 6.731199741363525
    },
    {
      "epoch": 0.4705691056910569,
      "step": 8682,
      "training_loss": 7.700690746307373
    },
    {
      "epoch": 0.4706233062330623,
      "step": 8683,
      "training_loss": 7.787207126617432
    },
    {
      "epoch": 0.47067750677506776,
      "grad_norm": 30.10032844543457,
      "learning_rate": 1e-05,
      "loss": 7.4322,
      "step": 8684
    },
    {
      "epoch": 0.47067750677506776,
      "step": 8684,
      "training_loss": 7.749781131744385
    },
    {
      "epoch": 0.47073170731707314,
      "step": 8685,
      "training_loss": 6.955041885375977
    },
    {
      "epoch": 0.4707859078590786,
      "step": 8686,
      "training_loss": 5.730529308319092
    },
    {
      "epoch": 0.47084010840108403,
      "step": 8687,
      "training_loss": 5.895066738128662
    },
    {
      "epoch": 0.4708943089430894,
      "grad_norm": 57.95079803466797,
      "learning_rate": 1e-05,
      "loss": 6.5826,
      "step": 8688
    },
    {
      "epoch": 0.4708943089430894,
      "step": 8688,
      "training_loss": 6.033708572387695
    },
    {
      "epoch": 0.47094850948509487,
      "step": 8689,
      "training_loss": 6.940702438354492
    },
    {
      "epoch": 0.47100271002710026,
      "step": 8690,
      "training_loss": 7.261291980743408
    },
    {
      "epoch": 0.4710569105691057,
      "step": 8691,
      "training_loss": 6.546548843383789
    },
    {
      "epoch": 0.4711111111111111,
      "grad_norm": 21.850107192993164,
      "learning_rate": 1e-05,
      "loss": 6.6956,
      "step": 8692
    },
    {
      "epoch": 0.4711111111111111,
      "step": 8692,
      "training_loss": 5.593597888946533
    },
    {
      "epoch": 0.47116531165311654,
      "step": 8693,
      "training_loss": 6.772270679473877
    },
    {
      "epoch": 0.4712195121951219,
      "step": 8694,
      "training_loss": 7.062253475189209
    },
    {
      "epoch": 0.47127371273712737,
      "step": 8695,
      "training_loss": 7.0160040855407715
    },
    {
      "epoch": 0.4713279132791328,
      "grad_norm": 22.64872169494629,
      "learning_rate": 1e-05,
      "loss": 6.611,
      "step": 8696
    },
    {
      "epoch": 0.4713279132791328,
      "step": 8696,
      "training_loss": 5.76266622543335
    },
    {
      "epoch": 0.4713821138211382,
      "step": 8697,
      "training_loss": 6.062202453613281
    },
    {
      "epoch": 0.47143631436314365,
      "step": 8698,
      "training_loss": 7.76332950592041
    },
    {
      "epoch": 0.47149051490514904,
      "step": 8699,
      "training_loss": 8.213022232055664
    },
    {
      "epoch": 0.4715447154471545,
      "grad_norm": 30.958864212036133,
      "learning_rate": 1e-05,
      "loss": 6.9503,
      "step": 8700
    },
    {
      "epoch": 0.4715447154471545,
      "step": 8700,
      "training_loss": 6.7914886474609375
    },
    {
      "epoch": 0.4715989159891599,
      "step": 8701,
      "training_loss": 6.961594581604004
    },
    {
      "epoch": 0.4716531165311653,
      "step": 8702,
      "training_loss": 7.585513591766357
    },
    {
      "epoch": 0.4717073170731707,
      "step": 8703,
      "training_loss": 7.117781639099121
    },
    {
      "epoch": 0.47176151761517615,
      "grad_norm": 38.1373405456543,
      "learning_rate": 1e-05,
      "loss": 7.1141,
      "step": 8704
    },
    {
      "epoch": 0.47176151761517615,
      "step": 8704,
      "training_loss": 6.420679569244385
    },
    {
      "epoch": 0.4718157181571816,
      "step": 8705,
      "training_loss": 8.451550483703613
    },
    {
      "epoch": 0.471869918699187,
      "step": 8706,
      "training_loss": 6.703178405761719
    },
    {
      "epoch": 0.47192411924119243,
      "step": 8707,
      "training_loss": 3.3368821144104004
    },
    {
      "epoch": 0.4719783197831978,
      "grad_norm": 40.77399826049805,
      "learning_rate": 1e-05,
      "loss": 6.2281,
      "step": 8708
    },
    {
      "epoch": 0.4719783197831978,
      "step": 8708,
      "training_loss": 6.6677021980285645
    },
    {
      "epoch": 0.47203252032520326,
      "step": 8709,
      "training_loss": 6.954292297363281
    },
    {
      "epoch": 0.47208672086720865,
      "step": 8710,
      "training_loss": 6.652273654937744
    },
    {
      "epoch": 0.4721409214092141,
      "step": 8711,
      "training_loss": 6.608066082000732
    },
    {
      "epoch": 0.4721951219512195,
      "grad_norm": 27.399381637573242,
      "learning_rate": 1e-05,
      "loss": 6.7206,
      "step": 8712
    },
    {
      "epoch": 0.4721951219512195,
      "step": 8712,
      "training_loss": 6.775742053985596
    },
    {
      "epoch": 0.47224932249322493,
      "step": 8713,
      "training_loss": 7.231801986694336
    },
    {
      "epoch": 0.4723035230352304,
      "step": 8714,
      "training_loss": 6.355579376220703
    },
    {
      "epoch": 0.47235772357723577,
      "step": 8715,
      "training_loss": 7.822214126586914
    },
    {
      "epoch": 0.4724119241192412,
      "grad_norm": 42.243839263916016,
      "learning_rate": 1e-05,
      "loss": 7.0463,
      "step": 8716
    },
    {
      "epoch": 0.4724119241192412,
      "step": 8716,
      "training_loss": 6.531036376953125
    },
    {
      "epoch": 0.4724661246612466,
      "step": 8717,
      "training_loss": 5.136106967926025
    },
    {
      "epoch": 0.47252032520325205,
      "step": 8718,
      "training_loss": 6.449653148651123
    },
    {
      "epoch": 0.47257452574525743,
      "step": 8719,
      "training_loss": 6.7311882972717285
    },
    {
      "epoch": 0.4726287262872629,
      "grad_norm": 31.606645584106445,
      "learning_rate": 1e-05,
      "loss": 6.212,
      "step": 8720
    },
    {
      "epoch": 0.4726287262872629,
      "step": 8720,
      "training_loss": 6.576032638549805
    },
    {
      "epoch": 0.47268292682926827,
      "step": 8721,
      "training_loss": 6.013205528259277
    },
    {
      "epoch": 0.4727371273712737,
      "step": 8722,
      "training_loss": 7.869683742523193
    },
    {
      "epoch": 0.47279132791327916,
      "step": 8723,
      "training_loss": 8.748190879821777
    },
    {
      "epoch": 0.47284552845528455,
      "grad_norm": 35.611083984375,
      "learning_rate": 1e-05,
      "loss": 7.3018,
      "step": 8724
    },
    {
      "epoch": 0.47284552845528455,
      "step": 8724,
      "training_loss": 7.102457523345947
    },
    {
      "epoch": 0.47289972899729,
      "step": 8725,
      "training_loss": 6.6374640464782715
    },
    {
      "epoch": 0.4729539295392954,
      "step": 8726,
      "training_loss": 7.40946102142334
    },
    {
      "epoch": 0.4730081300813008,
      "step": 8727,
      "training_loss": 6.145172595977783
    },
    {
      "epoch": 0.4730623306233062,
      "grad_norm": 36.29380416870117,
      "learning_rate": 1e-05,
      "loss": 6.8236,
      "step": 8728
    },
    {
      "epoch": 0.4730623306233062,
      "step": 8728,
      "training_loss": 5.568206787109375
    },
    {
      "epoch": 0.47311653116531166,
      "step": 8729,
      "training_loss": 6.184704780578613
    },
    {
      "epoch": 0.47317073170731705,
      "step": 8730,
      "training_loss": 8.18642807006836
    },
    {
      "epoch": 0.4732249322493225,
      "step": 8731,
      "training_loss": 6.82732629776001
    },
    {
      "epoch": 0.47327913279132794,
      "grad_norm": 21.478429794311523,
      "learning_rate": 1e-05,
      "loss": 6.6917,
      "step": 8732
    },
    {
      "epoch": 0.47327913279132794,
      "step": 8732,
      "training_loss": 6.843959808349609
    },
    {
      "epoch": 0.47333333333333333,
      "step": 8733,
      "training_loss": 7.046823024749756
    },
    {
      "epoch": 0.4733875338753388,
      "step": 8734,
      "training_loss": 6.247045516967773
    },
    {
      "epoch": 0.47344173441734416,
      "step": 8735,
      "training_loss": 7.06622314453125
    },
    {
      "epoch": 0.4734959349593496,
      "grad_norm": 34.67776870727539,
      "learning_rate": 1e-05,
      "loss": 6.801,
      "step": 8736
    },
    {
      "epoch": 0.4734959349593496,
      "step": 8736,
      "training_loss": 6.784939765930176
    },
    {
      "epoch": 0.473550135501355,
      "step": 8737,
      "training_loss": 7.088143348693848
    },
    {
      "epoch": 0.47360433604336044,
      "step": 8738,
      "training_loss": 6.908008575439453
    },
    {
      "epoch": 0.47365853658536583,
      "step": 8739,
      "training_loss": 6.625329494476318
    },
    {
      "epoch": 0.4737127371273713,
      "grad_norm": 28.95180892944336,
      "learning_rate": 1e-05,
      "loss": 6.8516,
      "step": 8740
    },
    {
      "epoch": 0.4737127371273713,
      "step": 8740,
      "training_loss": 6.6881103515625
    },
    {
      "epoch": 0.4737669376693767,
      "step": 8741,
      "training_loss": 6.202998161315918
    },
    {
      "epoch": 0.4738211382113821,
      "step": 8742,
      "training_loss": 5.767998695373535
    },
    {
      "epoch": 0.47387533875338755,
      "step": 8743,
      "training_loss": 6.476691246032715
    },
    {
      "epoch": 0.47392953929539294,
      "grad_norm": 32.542144775390625,
      "learning_rate": 1e-05,
      "loss": 6.2839,
      "step": 8744
    },
    {
      "epoch": 0.47392953929539294,
      "step": 8744,
      "training_loss": 5.902226448059082
    },
    {
      "epoch": 0.4739837398373984,
      "step": 8745,
      "training_loss": 6.46298360824585
    },
    {
      "epoch": 0.4740379403794038,
      "step": 8746,
      "training_loss": 8.297635078430176
    },
    {
      "epoch": 0.4740921409214092,
      "step": 8747,
      "training_loss": 7.369076251983643
    },
    {
      "epoch": 0.4741463414634146,
      "grad_norm": 19.012176513671875,
      "learning_rate": 1e-05,
      "loss": 7.008,
      "step": 8748
    },
    {
      "epoch": 0.4741463414634146,
      "step": 8748,
      "training_loss": 6.769769191741943
    },
    {
      "epoch": 0.47420054200542006,
      "step": 8749,
      "training_loss": 6.675316333770752
    },
    {
      "epoch": 0.4742547425474255,
      "step": 8750,
      "training_loss": 7.558692455291748
    },
    {
      "epoch": 0.4743089430894309,
      "step": 8751,
      "training_loss": 5.63127326965332
    },
    {
      "epoch": 0.47436314363143633,
      "grad_norm": 38.909217834472656,
      "learning_rate": 1e-05,
      "loss": 6.6588,
      "step": 8752
    },
    {
      "epoch": 0.47436314363143633,
      "step": 8752,
      "training_loss": 5.299503326416016
    },
    {
      "epoch": 0.4744173441734417,
      "step": 8753,
      "training_loss": 5.239810466766357
    },
    {
      "epoch": 0.47447154471544717,
      "step": 8754,
      "training_loss": 6.506971836090088
    },
    {
      "epoch": 0.47452574525745256,
      "step": 8755,
      "training_loss": 6.450228691101074
    },
    {
      "epoch": 0.474579945799458,
      "grad_norm": 23.002342224121094,
      "learning_rate": 1e-05,
      "loss": 5.8741,
      "step": 8756
    },
    {
      "epoch": 0.474579945799458,
      "step": 8756,
      "training_loss": 7.766998767852783
    },
    {
      "epoch": 0.4746341463414634,
      "step": 8757,
      "training_loss": 7.820295333862305
    },
    {
      "epoch": 0.47468834688346884,
      "step": 8758,
      "training_loss": 7.205555438995361
    },
    {
      "epoch": 0.4747425474254743,
      "step": 8759,
      "training_loss": 8.483386993408203
    },
    {
      "epoch": 0.47479674796747967,
      "grad_norm": 47.67266082763672,
      "learning_rate": 1e-05,
      "loss": 7.8191,
      "step": 8760
    },
    {
      "epoch": 0.47479674796747967,
      "step": 8760,
      "training_loss": 3.733051300048828
    },
    {
      "epoch": 0.4748509485094851,
      "step": 8761,
      "training_loss": 7.7333807945251465
    },
    {
      "epoch": 0.4749051490514905,
      "step": 8762,
      "training_loss": 6.830978870391846
    },
    {
      "epoch": 0.47495934959349595,
      "step": 8763,
      "training_loss": 6.573931694030762
    },
    {
      "epoch": 0.47501355013550134,
      "grad_norm": 26.713071823120117,
      "learning_rate": 1e-05,
      "loss": 6.2178,
      "step": 8764
    },
    {
      "epoch": 0.47501355013550134,
      "step": 8764,
      "training_loss": 6.924048900604248
    },
    {
      "epoch": 0.4750677506775068,
      "step": 8765,
      "training_loss": 7.117854118347168
    },
    {
      "epoch": 0.4751219512195122,
      "step": 8766,
      "training_loss": 7.250119686126709
    },
    {
      "epoch": 0.4751761517615176,
      "step": 8767,
      "training_loss": 7.215385437011719
    },
    {
      "epoch": 0.47523035230352306,
      "grad_norm": 24.751157760620117,
      "learning_rate": 1e-05,
      "loss": 7.1269,
      "step": 8768
    },
    {
      "epoch": 0.47523035230352306,
      "step": 8768,
      "training_loss": 6.365550518035889
    },
    {
      "epoch": 0.47528455284552845,
      "step": 8769,
      "training_loss": 5.940555572509766
    },
    {
      "epoch": 0.4753387533875339,
      "step": 8770,
      "training_loss": 5.074552536010742
    },
    {
      "epoch": 0.4753929539295393,
      "step": 8771,
      "training_loss": 6.424692153930664
    },
    {
      "epoch": 0.47544715447154473,
      "grad_norm": 31.172080993652344,
      "learning_rate": 1e-05,
      "loss": 5.9513,
      "step": 8772
    },
    {
      "epoch": 0.47544715447154473,
      "step": 8772,
      "training_loss": 6.843995571136475
    },
    {
      "epoch": 0.4755013550135501,
      "step": 8773,
      "training_loss": 6.668908596038818
    },
    {
      "epoch": 0.47555555555555556,
      "step": 8774,
      "training_loss": 6.52249002456665
    },
    {
      "epoch": 0.47560975609756095,
      "step": 8775,
      "training_loss": 6.615206241607666
    },
    {
      "epoch": 0.4756639566395664,
      "grad_norm": 25.31827163696289,
      "learning_rate": 1e-05,
      "loss": 6.6627,
      "step": 8776
    },
    {
      "epoch": 0.4756639566395664,
      "step": 8776,
      "training_loss": 5.230291843414307
    },
    {
      "epoch": 0.4757181571815718,
      "step": 8777,
      "training_loss": 6.387392520904541
    },
    {
      "epoch": 0.47577235772357723,
      "step": 8778,
      "training_loss": 7.162108898162842
    },
    {
      "epoch": 0.4758265582655827,
      "step": 8779,
      "training_loss": 6.620734691619873
    },
    {
      "epoch": 0.47588075880758807,
      "grad_norm": 18.415634155273438,
      "learning_rate": 1e-05,
      "loss": 6.3501,
      "step": 8780
    },
    {
      "epoch": 0.47588075880758807,
      "step": 8780,
      "training_loss": 6.956798553466797
    },
    {
      "epoch": 0.4759349593495935,
      "step": 8781,
      "training_loss": 8.101387023925781
    },
    {
      "epoch": 0.4759891598915989,
      "step": 8782,
      "training_loss": 6.47579288482666
    },
    {
      "epoch": 0.47604336043360435,
      "step": 8783,
      "training_loss": 8.033929824829102
    },
    {
      "epoch": 0.47609756097560973,
      "grad_norm": 26.342304229736328,
      "learning_rate": 1e-05,
      "loss": 7.392,
      "step": 8784
    },
    {
      "epoch": 0.47609756097560973,
      "step": 8784,
      "training_loss": 3.227618455886841
    },
    {
      "epoch": 0.4761517615176152,
      "step": 8785,
      "training_loss": 4.3746185302734375
    },
    {
      "epoch": 0.47620596205962057,
      "step": 8786,
      "training_loss": 6.8831281661987305
    },
    {
      "epoch": 0.476260162601626,
      "step": 8787,
      "training_loss": 5.654993534088135
    },
    {
      "epoch": 0.47631436314363146,
      "grad_norm": 22.567249298095703,
      "learning_rate": 1e-05,
      "loss": 5.0351,
      "step": 8788
    },
    {
      "epoch": 0.47631436314363146,
      "step": 8788,
      "training_loss": 6.305398941040039
    },
    {
      "epoch": 0.47636856368563685,
      "step": 8789,
      "training_loss": 6.250289440155029
    },
    {
      "epoch": 0.4764227642276423,
      "step": 8790,
      "training_loss": 7.146369457244873
    },
    {
      "epoch": 0.4764769647696477,
      "step": 8791,
      "training_loss": 6.169522762298584
    },
    {
      "epoch": 0.4765311653116531,
      "grad_norm": 19.841699600219727,
      "learning_rate": 1e-05,
      "loss": 6.4679,
      "step": 8792
    },
    {
      "epoch": 0.4765311653116531,
      "step": 8792,
      "training_loss": 7.9433979988098145
    },
    {
      "epoch": 0.4765853658536585,
      "step": 8793,
      "training_loss": 9.458259582519531
    },
    {
      "epoch": 0.47663956639566396,
      "step": 8794,
      "training_loss": 7.722391128540039
    },
    {
      "epoch": 0.47669376693766935,
      "step": 8795,
      "training_loss": 7.397755146026611
    },
    {
      "epoch": 0.4767479674796748,
      "grad_norm": 21.563644409179688,
      "learning_rate": 1e-05,
      "loss": 8.1305,
      "step": 8796
    },
    {
      "epoch": 0.4767479674796748,
      "step": 8796,
      "training_loss": 6.64775276184082
    },
    {
      "epoch": 0.47680216802168024,
      "step": 8797,
      "training_loss": 5.638513088226318
    },
    {
      "epoch": 0.47685636856368563,
      "step": 8798,
      "training_loss": 4.850424766540527
    },
    {
      "epoch": 0.4769105691056911,
      "step": 8799,
      "training_loss": 6.442920684814453
    },
    {
      "epoch": 0.47696476964769646,
      "grad_norm": 28.643798828125,
      "learning_rate": 1e-05,
      "loss": 5.8949,
      "step": 8800
    },
    {
      "epoch": 0.47696476964769646,
      "step": 8800,
      "training_loss": 7.0121893882751465
    },
    {
      "epoch": 0.4770189701897019,
      "step": 8801,
      "training_loss": 6.859425067901611
    },
    {
      "epoch": 0.4770731707317073,
      "step": 8802,
      "training_loss": 7.197177410125732
    },
    {
      "epoch": 0.47712737127371274,
      "step": 8803,
      "training_loss": 5.899493217468262
    },
    {
      "epoch": 0.47718157181571813,
      "grad_norm": 21.07969856262207,
      "learning_rate": 1e-05,
      "loss": 6.7421,
      "step": 8804
    },
    {
      "epoch": 0.47718157181571813,
      "step": 8804,
      "training_loss": 6.766119003295898
    },
    {
      "epoch": 0.4772357723577236,
      "step": 8805,
      "training_loss": 6.813048839569092
    },
    {
      "epoch": 0.477289972899729,
      "step": 8806,
      "training_loss": 7.76570987701416
    },
    {
      "epoch": 0.4773441734417344,
      "step": 8807,
      "training_loss": 6.60589599609375
    },
    {
      "epoch": 0.47739837398373985,
      "grad_norm": 29.333566665649414,
      "learning_rate": 1e-05,
      "loss": 6.9877,
      "step": 8808
    },
    {
      "epoch": 0.47739837398373985,
      "step": 8808,
      "training_loss": 7.622342586517334
    },
    {
      "epoch": 0.47745257452574524,
      "step": 8809,
      "training_loss": 6.590959072113037
    },
    {
      "epoch": 0.4775067750677507,
      "step": 8810,
      "training_loss": 6.975590229034424
    },
    {
      "epoch": 0.4775609756097561,
      "step": 8811,
      "training_loss": 5.2090864181518555
    },
    {
      "epoch": 0.4776151761517615,
      "grad_norm": 20.340665817260742,
      "learning_rate": 1e-05,
      "loss": 6.5995,
      "step": 8812
    },
    {
      "epoch": 0.4776151761517615,
      "step": 8812,
      "training_loss": 5.7463226318359375
    },
    {
      "epoch": 0.4776693766937669,
      "step": 8813,
      "training_loss": 6.36509895324707
    },
    {
      "epoch": 0.47772357723577236,
      "step": 8814,
      "training_loss": 7.382028579711914
    },
    {
      "epoch": 0.4777777777777778,
      "step": 8815,
      "training_loss": 7.809224605560303
    },
    {
      "epoch": 0.4778319783197832,
      "grad_norm": 20.736373901367188,
      "learning_rate": 1e-05,
      "loss": 6.8257,
      "step": 8816
    },
    {
      "epoch": 0.4778319783197832,
      "step": 8816,
      "training_loss": 3.534034013748169
    },
    {
      "epoch": 0.47788617886178864,
      "step": 8817,
      "training_loss": 7.384912967681885
    },
    {
      "epoch": 0.477940379403794,
      "step": 8818,
      "training_loss": 6.5061869621276855
    },
    {
      "epoch": 0.47799457994579947,
      "step": 8819,
      "training_loss": 6.688403606414795
    },
    {
      "epoch": 0.47804878048780486,
      "grad_norm": 33.206565856933594,
      "learning_rate": 1e-05,
      "loss": 6.0284,
      "step": 8820
    },
    {
      "epoch": 0.47804878048780486,
      "step": 8820,
      "training_loss": 5.8532395362854
    },
    {
      "epoch": 0.4781029810298103,
      "step": 8821,
      "training_loss": 4.454146862030029
    },
    {
      "epoch": 0.4781571815718157,
      "step": 8822,
      "training_loss": 7.356954097747803
    },
    {
      "epoch": 0.47821138211382114,
      "step": 8823,
      "training_loss": 5.035706043243408
    },
    {
      "epoch": 0.4782655826558266,
      "grad_norm": 27.481599807739258,
      "learning_rate": 1e-05,
      "loss": 5.675,
      "step": 8824
    },
    {
      "epoch": 0.4782655826558266,
      "step": 8824,
      "training_loss": 7.229004383087158
    },
    {
      "epoch": 0.47831978319783197,
      "step": 8825,
      "training_loss": 6.932861328125
    },
    {
      "epoch": 0.4783739837398374,
      "step": 8826,
      "training_loss": 3.975011110305786
    },
    {
      "epoch": 0.4784281842818428,
      "step": 8827,
      "training_loss": 5.7658371925354
    },
    {
      "epoch": 0.47848238482384825,
      "grad_norm": 28.180753707885742,
      "learning_rate": 1e-05,
      "loss": 5.9757,
      "step": 8828
    },
    {
      "epoch": 0.47848238482384825,
      "step": 8828,
      "training_loss": 6.184800148010254
    },
    {
      "epoch": 0.47853658536585364,
      "step": 8829,
      "training_loss": 6.154801368713379
    },
    {
      "epoch": 0.4785907859078591,
      "step": 8830,
      "training_loss": 6.420965194702148
    },
    {
      "epoch": 0.4786449864498645,
      "step": 8831,
      "training_loss": 7.134716987609863
    },
    {
      "epoch": 0.4786991869918699,
      "grad_norm": 30.857315063476562,
      "learning_rate": 1e-05,
      "loss": 6.4738,
      "step": 8832
    },
    {
      "epoch": 0.4786991869918699,
      "step": 8832,
      "training_loss": 6.055235385894775
    },
    {
      "epoch": 0.47875338753387536,
      "step": 8833,
      "training_loss": 7.178245544433594
    },
    {
      "epoch": 0.47880758807588075,
      "step": 8834,
      "training_loss": 6.105220317840576
    },
    {
      "epoch": 0.4788617886178862,
      "step": 8835,
      "training_loss": 7.445420265197754
    },
    {
      "epoch": 0.4789159891598916,
      "grad_norm": 27.689594268798828,
      "learning_rate": 1e-05,
      "loss": 6.696,
      "step": 8836
    },
    {
      "epoch": 0.4789159891598916,
      "step": 8836,
      "training_loss": 5.635678291320801
    },
    {
      "epoch": 0.47897018970189703,
      "step": 8837,
      "training_loss": 6.7795820236206055
    },
    {
      "epoch": 0.4790243902439024,
      "step": 8838,
      "training_loss": 5.87316370010376
    },
    {
      "epoch": 0.47907859078590787,
      "step": 8839,
      "training_loss": 8.226849555969238
    },
    {
      "epoch": 0.47913279132791325,
      "grad_norm": 46.6641731262207,
      "learning_rate": 1e-05,
      "loss": 6.6288,
      "step": 8840
    },
    {
      "epoch": 0.47913279132791325,
      "step": 8840,
      "training_loss": 6.299983024597168
    },
    {
      "epoch": 0.4791869918699187,
      "step": 8841,
      "training_loss": 4.951182842254639
    },
    {
      "epoch": 0.47924119241192414,
      "step": 8842,
      "training_loss": 7.474847793579102
    },
    {
      "epoch": 0.47929539295392953,
      "step": 8843,
      "training_loss": 6.5591325759887695
    },
    {
      "epoch": 0.479349593495935,
      "grad_norm": 18.284961700439453,
      "learning_rate": 1e-05,
      "loss": 6.3213,
      "step": 8844
    },
    {
      "epoch": 0.479349593495935,
      "step": 8844,
      "training_loss": 5.744426250457764
    },
    {
      "epoch": 0.47940379403794037,
      "step": 8845,
      "training_loss": 6.01138162612915
    },
    {
      "epoch": 0.4794579945799458,
      "step": 8846,
      "training_loss": 7.191774845123291
    },
    {
      "epoch": 0.4795121951219512,
      "step": 8847,
      "training_loss": 6.814273357391357
    },
    {
      "epoch": 0.47956639566395665,
      "grad_norm": 16.893808364868164,
      "learning_rate": 1e-05,
      "loss": 6.4405,
      "step": 8848
    },
    {
      "epoch": 0.47956639566395665,
      "step": 8848,
      "training_loss": 7.210080146789551
    },
    {
      "epoch": 0.47962059620596204,
      "step": 8849,
      "training_loss": 6.066402912139893
    },
    {
      "epoch": 0.4796747967479675,
      "step": 8850,
      "training_loss": 6.1848063468933105
    },
    {
      "epoch": 0.4797289972899729,
      "step": 8851,
      "training_loss": 7.017498970031738
    },
    {
      "epoch": 0.4797831978319783,
      "grad_norm": 24.132061004638672,
      "learning_rate": 1e-05,
      "loss": 6.6197,
      "step": 8852
    },
    {
      "epoch": 0.4797831978319783,
      "step": 8852,
      "training_loss": 5.860453128814697
    },
    {
      "epoch": 0.47983739837398376,
      "step": 8853,
      "training_loss": 4.681447982788086
    },
    {
      "epoch": 0.47989159891598915,
      "step": 8854,
      "training_loss": 6.175044536590576
    },
    {
      "epoch": 0.4799457994579946,
      "step": 8855,
      "training_loss": 7.442633628845215
    },
    {
      "epoch": 0.48,
      "grad_norm": 27.961538314819336,
      "learning_rate": 1e-05,
      "loss": 6.0399,
      "step": 8856
    },
    {
      "epoch": 0.48,
      "step": 8856,
      "training_loss": 6.32319450378418
    },
    {
      "epoch": 0.4800542005420054,
      "step": 8857,
      "training_loss": 6.850034713745117
    },
    {
      "epoch": 0.4801084010840108,
      "step": 8858,
      "training_loss": 6.996112823486328
    },
    {
      "epoch": 0.48016260162601626,
      "step": 8859,
      "training_loss": 6.6716742515563965
    },
    {
      "epoch": 0.4802168021680217,
      "grad_norm": 35.995819091796875,
      "learning_rate": 1e-05,
      "loss": 6.7103,
      "step": 8860
    },
    {
      "epoch": 0.4802168021680217,
      "step": 8860,
      "training_loss": 7.316413402557373
    },
    {
      "epoch": 0.4802710027100271,
      "step": 8861,
      "training_loss": 4.848312854766846
    },
    {
      "epoch": 0.48032520325203254,
      "step": 8862,
      "training_loss": 6.952425956726074
    },
    {
      "epoch": 0.48037940379403793,
      "step": 8863,
      "training_loss": 6.888877868652344
    },
    {
      "epoch": 0.4804336043360434,
      "grad_norm": 24.0025577545166,
      "learning_rate": 1e-05,
      "loss": 6.5015,
      "step": 8864
    },
    {
      "epoch": 0.4804336043360434,
      "step": 8864,
      "training_loss": 6.4479169845581055
    },
    {
      "epoch": 0.48048780487804876,
      "step": 8865,
      "training_loss": 6.192218780517578
    },
    {
      "epoch": 0.4805420054200542,
      "step": 8866,
      "training_loss": 7.284388065338135
    },
    {
      "epoch": 0.4805962059620596,
      "step": 8867,
      "training_loss": 6.524163246154785
    },
    {
      "epoch": 0.48065040650406504,
      "grad_norm": 31.674741744995117,
      "learning_rate": 1e-05,
      "loss": 6.6122,
      "step": 8868
    },
    {
      "epoch": 0.48065040650406504,
      "step": 8868,
      "training_loss": 6.096820831298828
    },
    {
      "epoch": 0.4807046070460705,
      "step": 8869,
      "training_loss": 3.947031259536743
    },
    {
      "epoch": 0.4807588075880759,
      "step": 8870,
      "training_loss": 6.527018070220947
    },
    {
      "epoch": 0.4808130081300813,
      "step": 8871,
      "training_loss": 6.7894487380981445
    },
    {
      "epoch": 0.4808672086720867,
      "grad_norm": 46.60946273803711,
      "learning_rate": 1e-05,
      "loss": 5.8401,
      "step": 8872
    },
    {
      "epoch": 0.4808672086720867,
      "step": 8872,
      "training_loss": 5.6705403327941895
    },
    {
      "epoch": 0.48092140921409215,
      "step": 8873,
      "training_loss": 5.503927230834961
    },
    {
      "epoch": 0.48097560975609754,
      "step": 8874,
      "training_loss": 7.493914604187012
    },
    {
      "epoch": 0.481029810298103,
      "step": 8875,
      "training_loss": 5.308805465698242
    },
    {
      "epoch": 0.4810840108401084,
      "grad_norm": 34.570045471191406,
      "learning_rate": 1e-05,
      "loss": 5.9943,
      "step": 8876
    },
    {
      "epoch": 0.4810840108401084,
      "step": 8876,
      "training_loss": 8.09554672241211
    },
    {
      "epoch": 0.4811382113821138,
      "step": 8877,
      "training_loss": 7.276968479156494
    },
    {
      "epoch": 0.48119241192411927,
      "step": 8878,
      "training_loss": 7.028543472290039
    },
    {
      "epoch": 0.48124661246612466,
      "step": 8879,
      "training_loss": 5.171832084655762
    },
    {
      "epoch": 0.4813008130081301,
      "grad_norm": 43.98770523071289,
      "learning_rate": 1e-05,
      "loss": 6.8932,
      "step": 8880
    },
    {
      "epoch": 0.4813008130081301,
      "step": 8880,
      "training_loss": 7.378412246704102
    },
    {
      "epoch": 0.4813550135501355,
      "step": 8881,
      "training_loss": 6.444746494293213
    },
    {
      "epoch": 0.48140921409214094,
      "step": 8882,
      "training_loss": 6.8459086418151855
    },
    {
      "epoch": 0.4814634146341463,
      "step": 8883,
      "training_loss": 6.308406352996826
    },
    {
      "epoch": 0.48151761517615177,
      "grad_norm": 25.283098220825195,
      "learning_rate": 1e-05,
      "loss": 6.7444,
      "step": 8884
    },
    {
      "epoch": 0.48151761517615177,
      "step": 8884,
      "training_loss": 6.412582874298096
    },
    {
      "epoch": 0.48157181571815716,
      "step": 8885,
      "training_loss": 7.507418632507324
    },
    {
      "epoch": 0.4816260162601626,
      "step": 8886,
      "training_loss": 6.95683479309082
    },
    {
      "epoch": 0.48168021680216805,
      "step": 8887,
      "training_loss": 7.323493957519531
    },
    {
      "epoch": 0.48173441734417344,
      "grad_norm": 37.64848327636719,
      "learning_rate": 1e-05,
      "loss": 7.0501,
      "step": 8888
    },
    {
      "epoch": 0.48173441734417344,
      "step": 8888,
      "training_loss": 3.6900949478149414
    },
    {
      "epoch": 0.4817886178861789,
      "step": 8889,
      "training_loss": 6.984001159667969
    },
    {
      "epoch": 0.48184281842818427,
      "step": 8890,
      "training_loss": 7.962331295013428
    },
    {
      "epoch": 0.4818970189701897,
      "step": 8891,
      "training_loss": 5.778239727020264
    },
    {
      "epoch": 0.4819512195121951,
      "grad_norm": 29.00046157836914,
      "learning_rate": 1e-05,
      "loss": 6.1037,
      "step": 8892
    },
    {
      "epoch": 0.4819512195121951,
      "step": 8892,
      "training_loss": 5.331688404083252
    },
    {
      "epoch": 0.48200542005420055,
      "step": 8893,
      "training_loss": 6.8526482582092285
    },
    {
      "epoch": 0.48205962059620594,
      "step": 8894,
      "training_loss": 7.547780513763428
    },
    {
      "epoch": 0.4821138211382114,
      "step": 8895,
      "training_loss": 5.605044364929199
    },
    {
      "epoch": 0.48216802168021683,
      "grad_norm": 28.239479064941406,
      "learning_rate": 1e-05,
      "loss": 6.3343,
      "step": 8896
    },
    {
      "epoch": 0.48216802168021683,
      "step": 8896,
      "training_loss": 7.46440315246582
    },
    {
      "epoch": 0.4822222222222222,
      "step": 8897,
      "training_loss": 5.5891547203063965
    },
    {
      "epoch": 0.48227642276422766,
      "step": 8898,
      "training_loss": 7.387080669403076
    },
    {
      "epoch": 0.48233062330623305,
      "step": 8899,
      "training_loss": 7.2711591720581055
    },
    {
      "epoch": 0.4823848238482385,
      "grad_norm": 24.548261642456055,
      "learning_rate": 1e-05,
      "loss": 6.9279,
      "step": 8900
    },
    {
      "epoch": 0.4823848238482385,
      "step": 8900,
      "training_loss": 6.821017742156982
    },
    {
      "epoch": 0.4824390243902439,
      "step": 8901,
      "training_loss": 7.3133649826049805
    },
    {
      "epoch": 0.48249322493224933,
      "step": 8902,
      "training_loss": 5.19185209274292
    },
    {
      "epoch": 0.4825474254742547,
      "step": 8903,
      "training_loss": 6.828348159790039
    },
    {
      "epoch": 0.48260162601626017,
      "grad_norm": 25.60003089904785,
      "learning_rate": 1e-05,
      "loss": 6.5386,
      "step": 8904
    },
    {
      "epoch": 0.48260162601626017,
      "step": 8904,
      "training_loss": 8.165124893188477
    },
    {
      "epoch": 0.48265582655826555,
      "step": 8905,
      "training_loss": 6.218834400177002
    },
    {
      "epoch": 0.482710027100271,
      "step": 8906,
      "training_loss": 6.809776782989502
    },
    {
      "epoch": 0.48276422764227644,
      "step": 8907,
      "training_loss": 6.523629188537598
    },
    {
      "epoch": 0.48281842818428183,
      "grad_norm": 21.730079650878906,
      "learning_rate": 1e-05,
      "loss": 6.9293,
      "step": 8908
    },
    {
      "epoch": 0.48281842818428183,
      "step": 8908,
      "training_loss": 7.963545799255371
    },
    {
      "epoch": 0.4828726287262873,
      "step": 8909,
      "training_loss": 7.679685592651367
    },
    {
      "epoch": 0.48292682926829267,
      "step": 8910,
      "training_loss": 6.6245012283325195
    },
    {
      "epoch": 0.4829810298102981,
      "step": 8911,
      "training_loss": 6.892821788787842
    },
    {
      "epoch": 0.4830352303523035,
      "grad_norm": 21.655101776123047,
      "learning_rate": 1e-05,
      "loss": 7.2901,
      "step": 8912
    },
    {
      "epoch": 0.4830352303523035,
      "step": 8912,
      "training_loss": 6.231719970703125
    },
    {
      "epoch": 0.48308943089430895,
      "step": 8913,
      "training_loss": 6.977208137512207
    },
    {
      "epoch": 0.48314363143631434,
      "step": 8914,
      "training_loss": 7.04428243637085
    },
    {
      "epoch": 0.4831978319783198,
      "step": 8915,
      "training_loss": 5.996301651000977
    },
    {
      "epoch": 0.4832520325203252,
      "grad_norm": 24.78044891357422,
      "learning_rate": 1e-05,
      "loss": 6.5624,
      "step": 8916
    },
    {
      "epoch": 0.4832520325203252,
      "step": 8916,
      "training_loss": 5.717746734619141
    },
    {
      "epoch": 0.4833062330623306,
      "step": 8917,
      "training_loss": 6.166818618774414
    },
    {
      "epoch": 0.48336043360433606,
      "step": 8918,
      "training_loss": 6.918550968170166
    },
    {
      "epoch": 0.48341463414634145,
      "step": 8919,
      "training_loss": 6.049692153930664
    },
    {
      "epoch": 0.4834688346883469,
      "grad_norm": 29.95161247253418,
      "learning_rate": 1e-05,
      "loss": 6.2132,
      "step": 8920
    },
    {
      "epoch": 0.4834688346883469,
      "step": 8920,
      "training_loss": 7.7176079750061035
    },
    {
      "epoch": 0.4835230352303523,
      "step": 8921,
      "training_loss": 6.805065155029297
    },
    {
      "epoch": 0.4835772357723577,
      "step": 8922,
      "training_loss": 6.894712924957275
    },
    {
      "epoch": 0.4836314363143631,
      "step": 8923,
      "training_loss": 7.581709861755371
    },
    {
      "epoch": 0.48368563685636856,
      "grad_norm": 37.61424255371094,
      "learning_rate": 1e-05,
      "loss": 7.2498,
      "step": 8924
    },
    {
      "epoch": 0.48368563685636856,
      "step": 8924,
      "training_loss": 6.200612545013428
    },
    {
      "epoch": 0.483739837398374,
      "step": 8925,
      "training_loss": 4.083110332489014
    },
    {
      "epoch": 0.4837940379403794,
      "step": 8926,
      "training_loss": 5.279655933380127
    },
    {
      "epoch": 0.48384823848238484,
      "step": 8927,
      "training_loss": 5.652878761291504
    },
    {
      "epoch": 0.48390243902439023,
      "grad_norm": 23.751522064208984,
      "learning_rate": 1e-05,
      "loss": 5.3041,
      "step": 8928
    },
    {
      "epoch": 0.48390243902439023,
      "step": 8928,
      "training_loss": 6.751635551452637
    },
    {
      "epoch": 0.4839566395663957,
      "step": 8929,
      "training_loss": 7.439648151397705
    },
    {
      "epoch": 0.48401084010840106,
      "step": 8930,
      "training_loss": 6.658985137939453
    },
    {
      "epoch": 0.4840650406504065,
      "step": 8931,
      "training_loss": 9.676735877990723
    },
    {
      "epoch": 0.4841192411924119,
      "grad_norm": 40.969451904296875,
      "learning_rate": 1e-05,
      "loss": 7.6318,
      "step": 8932
    },
    {
      "epoch": 0.4841192411924119,
      "step": 8932,
      "training_loss": 7.38181734085083
    },
    {
      "epoch": 0.48417344173441734,
      "step": 8933,
      "training_loss": 7.839977264404297
    },
    {
      "epoch": 0.4842276422764228,
      "step": 8934,
      "training_loss": 7.318114280700684
    },
    {
      "epoch": 0.4842818428184282,
      "step": 8935,
      "training_loss": 6.29379415512085
    },
    {
      "epoch": 0.4843360433604336,
      "grad_norm": 29.59566879272461,
      "learning_rate": 1e-05,
      "loss": 7.2084,
      "step": 8936
    },
    {
      "epoch": 0.4843360433604336,
      "step": 8936,
      "training_loss": 6.636831283569336
    },
    {
      "epoch": 0.484390243902439,
      "step": 8937,
      "training_loss": 6.400794982910156
    },
    {
      "epoch": 0.48444444444444446,
      "step": 8938,
      "training_loss": 5.351787567138672
    },
    {
      "epoch": 0.48449864498644984,
      "step": 8939,
      "training_loss": 7.034098148345947
    },
    {
      "epoch": 0.4845528455284553,
      "grad_norm": 19.452600479125977,
      "learning_rate": 1e-05,
      "loss": 6.3559,
      "step": 8940
    },
    {
      "epoch": 0.4845528455284553,
      "step": 8940,
      "training_loss": 6.814493656158447
    },
    {
      "epoch": 0.4846070460704607,
      "step": 8941,
      "training_loss": 6.033960819244385
    },
    {
      "epoch": 0.4846612466124661,
      "step": 8942,
      "training_loss": 6.951189041137695
    },
    {
      "epoch": 0.48471544715447157,
      "step": 8943,
      "training_loss": 8.422263145446777
    },
    {
      "epoch": 0.48476964769647696,
      "grad_norm": 42.1578254699707,
      "learning_rate": 1e-05,
      "loss": 7.0555,
      "step": 8944
    },
    {
      "epoch": 0.48476964769647696,
      "step": 8944,
      "training_loss": 7.6651997566223145
    },
    {
      "epoch": 0.4848238482384824,
      "step": 8945,
      "training_loss": 7.701375484466553
    },
    {
      "epoch": 0.4848780487804878,
      "step": 8946,
      "training_loss": 6.653533935546875
    },
    {
      "epoch": 0.48493224932249324,
      "step": 8947,
      "training_loss": 6.82818078994751
    },
    {
      "epoch": 0.4849864498644986,
      "grad_norm": 14.794265747070312,
      "learning_rate": 1e-05,
      "loss": 7.2121,
      "step": 8948
    },
    {
      "epoch": 0.4849864498644986,
      "step": 8948,
      "training_loss": 6.569118976593018
    },
    {
      "epoch": 0.48504065040650407,
      "step": 8949,
      "training_loss": 5.136620044708252
    },
    {
      "epoch": 0.48509485094850946,
      "step": 8950,
      "training_loss": 6.920426368713379
    },
    {
      "epoch": 0.4851490514905149,
      "step": 8951,
      "training_loss": 5.695066452026367
    },
    {
      "epoch": 0.48520325203252035,
      "grad_norm": 25.692445755004883,
      "learning_rate": 1e-05,
      "loss": 6.0803,
      "step": 8952
    },
    {
      "epoch": 0.48520325203252035,
      "step": 8952,
      "training_loss": 7.026833534240723
    },
    {
      "epoch": 0.48525745257452574,
      "step": 8953,
      "training_loss": 5.443542003631592
    },
    {
      "epoch": 0.4853116531165312,
      "step": 8954,
      "training_loss": 6.681685447692871
    },
    {
      "epoch": 0.4853658536585366,
      "step": 8955,
      "training_loss": 6.7163004875183105
    },
    {
      "epoch": 0.485420054200542,
      "grad_norm": 36.96062088012695,
      "learning_rate": 1e-05,
      "loss": 6.4671,
      "step": 8956
    },
    {
      "epoch": 0.485420054200542,
      "step": 8956,
      "training_loss": 8.157255172729492
    },
    {
      "epoch": 0.4854742547425474,
      "step": 8957,
      "training_loss": 7.324065685272217
    },
    {
      "epoch": 0.48552845528455285,
      "step": 8958,
      "training_loss": 6.51101016998291
    },
    {
      "epoch": 0.48558265582655824,
      "step": 8959,
      "training_loss": 7.022419452667236
    },
    {
      "epoch": 0.4856368563685637,
      "grad_norm": 23.2686767578125,
      "learning_rate": 1e-05,
      "loss": 7.2537,
      "step": 8960
    },
    {
      "epoch": 0.4856368563685637,
      "step": 8960,
      "training_loss": 6.868050575256348
    },
    {
      "epoch": 0.48569105691056913,
      "step": 8961,
      "training_loss": 7.230299949645996
    },
    {
      "epoch": 0.4857452574525745,
      "step": 8962,
      "training_loss": 6.4818010330200195
    },
    {
      "epoch": 0.48579945799457996,
      "step": 8963,
      "training_loss": 7.742203235626221
    },
    {
      "epoch": 0.48585365853658535,
      "grad_norm": 28.08232307434082,
      "learning_rate": 1e-05,
      "loss": 7.0806,
      "step": 8964
    },
    {
      "epoch": 0.48585365853658535,
      "step": 8964,
      "training_loss": 6.799591541290283
    },
    {
      "epoch": 0.4859078590785908,
      "step": 8965,
      "training_loss": 6.836517333984375
    },
    {
      "epoch": 0.4859620596205962,
      "step": 8966,
      "training_loss": 6.027851104736328
    },
    {
      "epoch": 0.48601626016260163,
      "step": 8967,
      "training_loss": 6.296720504760742
    },
    {
      "epoch": 0.486070460704607,
      "grad_norm": 27.50383758544922,
      "learning_rate": 1e-05,
      "loss": 6.4902,
      "step": 8968
    },
    {
      "epoch": 0.486070460704607,
      "step": 8968,
      "training_loss": 5.985729217529297
    },
    {
      "epoch": 0.48612466124661247,
      "step": 8969,
      "training_loss": 7.120241165161133
    },
    {
      "epoch": 0.4861788617886179,
      "step": 8970,
      "training_loss": 6.883100509643555
    },
    {
      "epoch": 0.4862330623306233,
      "step": 8971,
      "training_loss": 6.217647552490234
    },
    {
      "epoch": 0.48628726287262874,
      "grad_norm": 22.85759925842285,
      "learning_rate": 1e-05,
      "loss": 6.5517,
      "step": 8972
    },
    {
      "epoch": 0.48628726287262874,
      "step": 8972,
      "training_loss": 7.659177780151367
    },
    {
      "epoch": 0.48634146341463413,
      "step": 8973,
      "training_loss": 7.330987453460693
    },
    {
      "epoch": 0.4863956639566396,
      "step": 8974,
      "training_loss": 6.453711032867432
    },
    {
      "epoch": 0.48644986449864497,
      "step": 8975,
      "training_loss": 4.380496025085449
    },
    {
      "epoch": 0.4865040650406504,
      "grad_norm": 81.31118774414062,
      "learning_rate": 1e-05,
      "loss": 6.4561,
      "step": 8976
    },
    {
      "epoch": 0.4865040650406504,
      "step": 8976,
      "training_loss": 6.417230606079102
    },
    {
      "epoch": 0.4865582655826558,
      "step": 8977,
      "training_loss": 5.965674877166748
    },
    {
      "epoch": 0.48661246612466125,
      "step": 8978,
      "training_loss": 6.823566436767578
    },
    {
      "epoch": 0.4866666666666667,
      "step": 8979,
      "training_loss": 5.553826808929443
    },
    {
      "epoch": 0.4867208672086721,
      "grad_norm": 24.152860641479492,
      "learning_rate": 1e-05,
      "loss": 6.1901,
      "step": 8980
    },
    {
      "epoch": 0.4867208672086721,
      "step": 8980,
      "training_loss": 7.024099826812744
    },
    {
      "epoch": 0.4867750677506775,
      "step": 8981,
      "training_loss": 5.752742290496826
    },
    {
      "epoch": 0.4868292682926829,
      "step": 8982,
      "training_loss": 7.351658821105957
    },
    {
      "epoch": 0.48688346883468836,
      "step": 8983,
      "training_loss": 3.898818016052246
    },
    {
      "epoch": 0.48693766937669375,
      "grad_norm": 37.06403732299805,
      "learning_rate": 1e-05,
      "loss": 6.0068,
      "step": 8984
    },
    {
      "epoch": 0.48693766937669375,
      "step": 8984,
      "training_loss": 5.605304718017578
    },
    {
      "epoch": 0.4869918699186992,
      "step": 8985,
      "training_loss": 6.29272985458374
    },
    {
      "epoch": 0.4870460704607046,
      "step": 8986,
      "training_loss": 7.5559539794921875
    },
    {
      "epoch": 0.48710027100271003,
      "step": 8987,
      "training_loss": 5.026524543762207
    },
    {
      "epoch": 0.4871544715447155,
      "grad_norm": 43.95143127441406,
      "learning_rate": 1e-05,
      "loss": 6.1201,
      "step": 8988
    },
    {
      "epoch": 0.4871544715447155,
      "step": 8988,
      "training_loss": 6.382904529571533
    },
    {
      "epoch": 0.48720867208672086,
      "step": 8989,
      "training_loss": 6.946070671081543
    },
    {
      "epoch": 0.4872628726287263,
      "step": 8990,
      "training_loss": 7.629547595977783
    },
    {
      "epoch": 0.4873170731707317,
      "step": 8991,
      "training_loss": 6.605504035949707
    },
    {
      "epoch": 0.48737127371273714,
      "grad_norm": 25.543476104736328,
      "learning_rate": 1e-05,
      "loss": 6.891,
      "step": 8992
    },
    {
      "epoch": 0.48737127371273714,
      "step": 8992,
      "training_loss": 7.312286376953125
    },
    {
      "epoch": 0.48742547425474253,
      "step": 8993,
      "training_loss": 6.2604804039001465
    },
    {
      "epoch": 0.487479674796748,
      "step": 8994,
      "training_loss": 8.216614723205566
    },
    {
      "epoch": 0.48753387533875336,
      "step": 8995,
      "training_loss": 6.713452339172363
    },
    {
      "epoch": 0.4875880758807588,
      "grad_norm": 41.05596923828125,
      "learning_rate": 1e-05,
      "loss": 7.1257,
      "step": 8996
    },
    {
      "epoch": 0.4875880758807588,
      "step": 8996,
      "training_loss": 7.573590278625488
    },
    {
      "epoch": 0.48764227642276425,
      "step": 8997,
      "training_loss": 6.263689041137695
    },
    {
      "epoch": 0.48769647696476964,
      "step": 8998,
      "training_loss": 6.014900207519531
    },
    {
      "epoch": 0.4877506775067751,
      "step": 8999,
      "training_loss": 5.735995769500732
    },
    {
      "epoch": 0.4878048780487805,
      "grad_norm": 28.354393005371094,
      "learning_rate": 1e-05,
      "loss": 6.397,
      "step": 9000
    },
    {
      "epoch": 0.4878048780487805,
      "step": 9000,
      "training_loss": 5.411006450653076
    },
    {
      "epoch": 0.4878590785907859,
      "step": 9001,
      "training_loss": 6.419164180755615
    },
    {
      "epoch": 0.4879132791327913,
      "step": 9002,
      "training_loss": 7.2653608322143555
    },
    {
      "epoch": 0.48796747967479676,
      "step": 9003,
      "training_loss": 7.829456329345703
    },
    {
      "epoch": 0.48802168021680215,
      "grad_norm": 27.69310188293457,
      "learning_rate": 1e-05,
      "loss": 6.7312,
      "step": 9004
    },
    {
      "epoch": 0.48802168021680215,
      "step": 9004,
      "training_loss": 5.48872184753418
    },
    {
      "epoch": 0.4880758807588076,
      "step": 9005,
      "training_loss": 6.120251178741455
    },
    {
      "epoch": 0.48813008130081303,
      "step": 9006,
      "training_loss": 8.017509460449219
    },
    {
      "epoch": 0.4881842818428184,
      "step": 9007,
      "training_loss": 6.924993991851807
    },
    {
      "epoch": 0.48823848238482387,
      "grad_norm": 21.918413162231445,
      "learning_rate": 1e-05,
      "loss": 6.6379,
      "step": 9008
    },
    {
      "epoch": 0.48823848238482387,
      "step": 9008,
      "training_loss": 5.7804341316223145
    },
    {
      "epoch": 0.48829268292682926,
      "step": 9009,
      "training_loss": 6.97171688079834
    },
    {
      "epoch": 0.4883468834688347,
      "step": 9010,
      "training_loss": 7.161952495574951
    },
    {
      "epoch": 0.4884010840108401,
      "step": 9011,
      "training_loss": 6.133023262023926
    },
    {
      "epoch": 0.48845528455284554,
      "grad_norm": 39.06163024902344,
      "learning_rate": 1e-05,
      "loss": 6.5118,
      "step": 9012
    },
    {
      "epoch": 0.48845528455284554,
      "step": 9012,
      "training_loss": 7.415543556213379
    },
    {
      "epoch": 0.4885094850948509,
      "step": 9013,
      "training_loss": 6.4871416091918945
    },
    {
      "epoch": 0.48856368563685637,
      "step": 9014,
      "training_loss": 6.876533031463623
    },
    {
      "epoch": 0.4886178861788618,
      "step": 9015,
      "training_loss": 5.862724304199219
    },
    {
      "epoch": 0.4886720867208672,
      "grad_norm": 27.88146209716797,
      "learning_rate": 1e-05,
      "loss": 6.6605,
      "step": 9016
    },
    {
      "epoch": 0.4886720867208672,
      "step": 9016,
      "training_loss": 4.2598795890808105
    },
    {
      "epoch": 0.48872628726287265,
      "step": 9017,
      "training_loss": 6.7691802978515625
    },
    {
      "epoch": 0.48878048780487804,
      "step": 9018,
      "training_loss": 6.4207587242126465
    },
    {
      "epoch": 0.4888346883468835,
      "step": 9019,
      "training_loss": 7.269820690155029
    },
    {
      "epoch": 0.4888888888888889,
      "grad_norm": 37.92618942260742,
      "learning_rate": 1e-05,
      "loss": 6.1799,
      "step": 9020
    },
    {
      "epoch": 0.4888888888888889,
      "step": 9020,
      "training_loss": 9.203091621398926
    },
    {
      "epoch": 0.4889430894308943,
      "step": 9021,
      "training_loss": 5.785216808319092
    },
    {
      "epoch": 0.4889972899728997,
      "step": 9022,
      "training_loss": 5.017086505889893
    },
    {
      "epoch": 0.48905149051490515,
      "step": 9023,
      "training_loss": 5.163398265838623
    },
    {
      "epoch": 0.4891056910569106,
      "grad_norm": 30.051837921142578,
      "learning_rate": 1e-05,
      "loss": 6.2922,
      "step": 9024
    },
    {
      "epoch": 0.4891056910569106,
      "step": 9024,
      "training_loss": 6.376031875610352
    },
    {
      "epoch": 0.489159891598916,
      "step": 9025,
      "training_loss": 6.919240474700928
    },
    {
      "epoch": 0.48921409214092143,
      "step": 9026,
      "training_loss": 5.74926233291626
    },
    {
      "epoch": 0.4892682926829268,
      "step": 9027,
      "training_loss": 6.136116981506348
    },
    {
      "epoch": 0.48932249322493226,
      "grad_norm": 19.560897827148438,
      "learning_rate": 1e-05,
      "loss": 6.2952,
      "step": 9028
    },
    {
      "epoch": 0.48932249322493226,
      "step": 9028,
      "training_loss": 6.721078872680664
    },
    {
      "epoch": 0.48937669376693765,
      "step": 9029,
      "training_loss": 5.249762535095215
    },
    {
      "epoch": 0.4894308943089431,
      "step": 9030,
      "training_loss": 7.096804141998291
    },
    {
      "epoch": 0.4894850948509485,
      "step": 9031,
      "training_loss": 5.672016143798828
    },
    {
      "epoch": 0.48953929539295393,
      "grad_norm": 37.36707305908203,
      "learning_rate": 1e-05,
      "loss": 6.1849,
      "step": 9032
    },
    {
      "epoch": 0.48953929539295393,
      "step": 9032,
      "training_loss": 6.955079555511475
    },
    {
      "epoch": 0.4895934959349593,
      "step": 9033,
      "training_loss": 6.838584899902344
    },
    {
      "epoch": 0.48964769647696477,
      "step": 9034,
      "training_loss": 7.567968845367432
    },
    {
      "epoch": 0.4897018970189702,
      "step": 9035,
      "training_loss": 5.391059398651123
    },
    {
      "epoch": 0.4897560975609756,
      "grad_norm": 23.337017059326172,
      "learning_rate": 1e-05,
      "loss": 6.6882,
      "step": 9036
    },
    {
      "epoch": 0.4897560975609756,
      "step": 9036,
      "training_loss": 6.905191421508789
    },
    {
      "epoch": 0.48981029810298105,
      "step": 9037,
      "training_loss": 6.5586748123168945
    },
    {
      "epoch": 0.48986449864498643,
      "step": 9038,
      "training_loss": 4.580617427825928
    },
    {
      "epoch": 0.4899186991869919,
      "step": 9039,
      "training_loss": 6.062633037567139
    },
    {
      "epoch": 0.48997289972899727,
      "grad_norm": 32.888450622558594,
      "learning_rate": 1e-05,
      "loss": 6.0268,
      "step": 9040
    },
    {
      "epoch": 0.48997289972899727,
      "step": 9040,
      "training_loss": 5.858994483947754
    },
    {
      "epoch": 0.4900271002710027,
      "step": 9041,
      "training_loss": 6.20511531829834
    },
    {
      "epoch": 0.4900813008130081,
      "step": 9042,
      "training_loss": 7.104310512542725
    },
    {
      "epoch": 0.49013550135501355,
      "step": 9043,
      "training_loss": 7.063283920288086
    },
    {
      "epoch": 0.490189701897019,
      "grad_norm": 36.94001388549805,
      "learning_rate": 1e-05,
      "loss": 6.5579,
      "step": 9044
    },
    {
      "epoch": 0.490189701897019,
      "step": 9044,
      "training_loss": 7.728265285491943
    },
    {
      "epoch": 0.4902439024390244,
      "step": 9045,
      "training_loss": 5.4694600105285645
    },
    {
      "epoch": 0.4902981029810298,
      "step": 9046,
      "training_loss": 5.401524066925049
    },
    {
      "epoch": 0.4903523035230352,
      "step": 9047,
      "training_loss": 8.578848838806152
    },
    {
      "epoch": 0.49040650406504066,
      "grad_norm": 48.0347785949707,
      "learning_rate": 1e-05,
      "loss": 6.7945,
      "step": 9048
    },
    {
      "epoch": 0.49040650406504066,
      "step": 9048,
      "training_loss": 6.557613372802734
    },
    {
      "epoch": 0.49046070460704605,
      "step": 9049,
      "training_loss": 6.09209680557251
    },
    {
      "epoch": 0.4905149051490515,
      "step": 9050,
      "training_loss": 6.269066333770752
    },
    {
      "epoch": 0.4905691056910569,
      "step": 9051,
      "training_loss": 6.624342918395996
    },
    {
      "epoch": 0.49062330623306233,
      "grad_norm": 31.532155990600586,
      "learning_rate": 1e-05,
      "loss": 6.3858,
      "step": 9052
    },
    {
      "epoch": 0.49062330623306233,
      "step": 9052,
      "training_loss": 6.4948859214782715
    },
    {
      "epoch": 0.4906775067750678,
      "step": 9053,
      "training_loss": 6.735886096954346
    },
    {
      "epoch": 0.49073170731707316,
      "step": 9054,
      "training_loss": 6.97871732711792
    },
    {
      "epoch": 0.4907859078590786,
      "step": 9055,
      "training_loss": 5.869349002838135
    },
    {
      "epoch": 0.490840108401084,
      "grad_norm": 25.419734954833984,
      "learning_rate": 1e-05,
      "loss": 6.5197,
      "step": 9056
    },
    {
      "epoch": 0.490840108401084,
      "step": 9056,
      "training_loss": 6.482371807098389
    },
    {
      "epoch": 0.49089430894308944,
      "step": 9057,
      "training_loss": 8.055088996887207
    },
    {
      "epoch": 0.49094850948509483,
      "step": 9058,
      "training_loss": 5.6500563621521
    },
    {
      "epoch": 0.4910027100271003,
      "step": 9059,
      "training_loss": 6.825674533843994
    },
    {
      "epoch": 0.49105691056910566,
      "grad_norm": 31.765199661254883,
      "learning_rate": 1e-05,
      "loss": 6.7533,
      "step": 9060
    },
    {
      "epoch": 0.49105691056910566,
      "step": 9060,
      "training_loss": 6.816110134124756
    },
    {
      "epoch": 0.4911111111111111,
      "step": 9061,
      "training_loss": 8.611560821533203
    },
    {
      "epoch": 0.49116531165311655,
      "step": 9062,
      "training_loss": 7.0665764808654785
    },
    {
      "epoch": 0.49121951219512194,
      "step": 9063,
      "training_loss": 6.8648552894592285
    },
    {
      "epoch": 0.4912737127371274,
      "grad_norm": 27.961091995239258,
      "learning_rate": 1e-05,
      "loss": 7.3398,
      "step": 9064
    },
    {
      "epoch": 0.4912737127371274,
      "step": 9064,
      "training_loss": 6.9767560958862305
    },
    {
      "epoch": 0.4913279132791328,
      "step": 9065,
      "training_loss": 5.305400371551514
    },
    {
      "epoch": 0.4913821138211382,
      "step": 9066,
      "training_loss": 9.235082626342773
    },
    {
      "epoch": 0.4914363143631436,
      "step": 9067,
      "training_loss": 6.795554161071777
    },
    {
      "epoch": 0.49149051490514906,
      "grad_norm": 19.755842208862305,
      "learning_rate": 1e-05,
      "loss": 7.0782,
      "step": 9068
    },
    {
      "epoch": 0.49149051490514906,
      "step": 9068,
      "training_loss": 6.6036295890808105
    },
    {
      "epoch": 0.49154471544715445,
      "step": 9069,
      "training_loss": 7.223876953125
    },
    {
      "epoch": 0.4915989159891599,
      "step": 9070,
      "training_loss": 7.157607555389404
    },
    {
      "epoch": 0.49165311653116534,
      "step": 9071,
      "training_loss": 7.024926662445068
    },
    {
      "epoch": 0.4917073170731707,
      "grad_norm": 18.185091018676758,
      "learning_rate": 1e-05,
      "loss": 7.0025,
      "step": 9072
    },
    {
      "epoch": 0.4917073170731707,
      "step": 9072,
      "training_loss": 6.475006580352783
    },
    {
      "epoch": 0.49176151761517617,
      "step": 9073,
      "training_loss": 6.652662754058838
    },
    {
      "epoch": 0.49181571815718156,
      "step": 9074,
      "training_loss": 6.968447208404541
    },
    {
      "epoch": 0.491869918699187,
      "step": 9075,
      "training_loss": 5.104265213012695
    },
    {
      "epoch": 0.4919241192411924,
      "grad_norm": 23.952850341796875,
      "learning_rate": 1e-05,
      "loss": 6.3001,
      "step": 9076
    },
    {
      "epoch": 0.4919241192411924,
      "step": 9076,
      "training_loss": 6.99242639541626
    },
    {
      "epoch": 0.49197831978319784,
      "step": 9077,
      "training_loss": 7.268124103546143
    },
    {
      "epoch": 0.4920325203252032,
      "step": 9078,
      "training_loss": 5.964552402496338
    },
    {
      "epoch": 0.49208672086720867,
      "step": 9079,
      "training_loss": 5.946582317352295
    },
    {
      "epoch": 0.4921409214092141,
      "grad_norm": 23.223888397216797,
      "learning_rate": 1e-05,
      "loss": 6.5429,
      "step": 9080
    },
    {
      "epoch": 0.4921409214092141,
      "step": 9080,
      "training_loss": 7.938473224639893
    },
    {
      "epoch": 0.4921951219512195,
      "step": 9081,
      "training_loss": 6.668980121612549
    },
    {
      "epoch": 0.49224932249322495,
      "step": 9082,
      "training_loss": 7.404988765716553
    },
    {
      "epoch": 0.49230352303523034,
      "step": 9083,
      "training_loss": 6.359721660614014
    },
    {
      "epoch": 0.4923577235772358,
      "grad_norm": 18.656591415405273,
      "learning_rate": 1e-05,
      "loss": 7.093,
      "step": 9084
    },
    {
      "epoch": 0.4923577235772358,
      "step": 9084,
      "training_loss": 7.618412017822266
    },
    {
      "epoch": 0.4924119241192412,
      "step": 9085,
      "training_loss": 7.394871711730957
    },
    {
      "epoch": 0.4924661246612466,
      "step": 9086,
      "training_loss": 8.060304641723633
    },
    {
      "epoch": 0.492520325203252,
      "step": 9087,
      "training_loss": 5.649713039398193
    },
    {
      "epoch": 0.49257452574525745,
      "grad_norm": 77.21041870117188,
      "learning_rate": 1e-05,
      "loss": 7.1808,
      "step": 9088
    },
    {
      "epoch": 0.49257452574525745,
      "step": 9088,
      "training_loss": 3.6114327907562256
    },
    {
      "epoch": 0.4926287262872629,
      "step": 9089,
      "training_loss": 4.510251998901367
    },
    {
      "epoch": 0.4926829268292683,
      "step": 9090,
      "training_loss": 7.016951560974121
    },
    {
      "epoch": 0.49273712737127373,
      "step": 9091,
      "training_loss": 6.459357261657715
    },
    {
      "epoch": 0.4927913279132791,
      "grad_norm": 25.942686080932617,
      "learning_rate": 1e-05,
      "loss": 5.3995,
      "step": 9092
    },
    {
      "epoch": 0.4927913279132791,
      "step": 9092,
      "training_loss": 6.570151329040527
    },
    {
      "epoch": 0.49284552845528456,
      "step": 9093,
      "training_loss": 5.466946125030518
    },
    {
      "epoch": 0.49289972899728995,
      "step": 9094,
      "training_loss": 4.5115966796875
    },
    {
      "epoch": 0.4929539295392954,
      "step": 9095,
      "training_loss": 6.3815083503723145
    },
    {
      "epoch": 0.4930081300813008,
      "grad_norm": 44.74559020996094,
      "learning_rate": 1e-05,
      "loss": 5.7326,
      "step": 9096
    },
    {
      "epoch": 0.4930081300813008,
      "step": 9096,
      "training_loss": 5.243258476257324
    },
    {
      "epoch": 0.49306233062330623,
      "step": 9097,
      "training_loss": 5.716365337371826
    },
    {
      "epoch": 0.4931165311653117,
      "step": 9098,
      "training_loss": 5.391549587249756
    },
    {
      "epoch": 0.49317073170731707,
      "step": 9099,
      "training_loss": 6.355703830718994
    },
    {
      "epoch": 0.4932249322493225,
      "grad_norm": 51.289527893066406,
      "learning_rate": 1e-05,
      "loss": 5.6767,
      "step": 9100
    },
    {
      "epoch": 0.4932249322493225,
      "step": 9100,
      "training_loss": 4.863423824310303
    },
    {
      "epoch": 0.4932791327913279,
      "step": 9101,
      "training_loss": 6.463311195373535
    },
    {
      "epoch": 0.49333333333333335,
      "step": 9102,
      "training_loss": 7.7676310539245605
    },
    {
      "epoch": 0.49338753387533874,
      "step": 9103,
      "training_loss": 6.705563068389893
    },
    {
      "epoch": 0.4934417344173442,
      "grad_norm": 18.264423370361328,
      "learning_rate": 1e-05,
      "loss": 6.45,
      "step": 9104
    },
    {
      "epoch": 0.4934417344173442,
      "step": 9104,
      "training_loss": 7.06267786026001
    },
    {
      "epoch": 0.49349593495934957,
      "step": 9105,
      "training_loss": 7.220362186431885
    },
    {
      "epoch": 0.493550135501355,
      "step": 9106,
      "training_loss": 5.640773296356201
    },
    {
      "epoch": 0.49360433604336046,
      "step": 9107,
      "training_loss": 6.4965009689331055
    },
    {
      "epoch": 0.49365853658536585,
      "grad_norm": 27.234582901000977,
      "learning_rate": 1e-05,
      "loss": 6.6051,
      "step": 9108
    },
    {
      "epoch": 0.49365853658536585,
      "step": 9108,
      "training_loss": 11.565091133117676
    },
    {
      "epoch": 0.4937127371273713,
      "step": 9109,
      "training_loss": 8.376219749450684
    },
    {
      "epoch": 0.4937669376693767,
      "step": 9110,
      "training_loss": 7.49989128112793
    },
    {
      "epoch": 0.4938211382113821,
      "step": 9111,
      "training_loss": 7.604022979736328
    },
    {
      "epoch": 0.4938753387533875,
      "grad_norm": 24.352190017700195,
      "learning_rate": 1e-05,
      "loss": 8.7613,
      "step": 9112
    },
    {
      "epoch": 0.4938753387533875,
      "step": 9112,
      "training_loss": 7.546797752380371
    },
    {
      "epoch": 0.49392953929539296,
      "step": 9113,
      "training_loss": 7.2345051765441895
    },
    {
      "epoch": 0.49398373983739835,
      "step": 9114,
      "training_loss": 6.8963236808776855
    },
    {
      "epoch": 0.4940379403794038,
      "step": 9115,
      "training_loss": 7.246772289276123
    },
    {
      "epoch": 0.49409214092140924,
      "grad_norm": 17.484376907348633,
      "learning_rate": 1e-05,
      "loss": 7.2311,
      "step": 9116
    },
    {
      "epoch": 0.49409214092140924,
      "step": 9116,
      "training_loss": 8.382390022277832
    },
    {
      "epoch": 0.49414634146341463,
      "step": 9117,
      "training_loss": 6.5643696784973145
    },
    {
      "epoch": 0.4942005420054201,
      "step": 9118,
      "training_loss": 8.056949615478516
    },
    {
      "epoch": 0.49425474254742546,
      "step": 9119,
      "training_loss": 6.044797897338867
    },
    {
      "epoch": 0.4943089430894309,
      "grad_norm": 24.046031951904297,
      "learning_rate": 1e-05,
      "loss": 7.2621,
      "step": 9120
    },
    {
      "epoch": 0.4943089430894309,
      "step": 9120,
      "training_loss": 6.492442607879639
    },
    {
      "epoch": 0.4943631436314363,
      "step": 9121,
      "training_loss": 6.567230701446533
    },
    {
      "epoch": 0.49441734417344174,
      "step": 9122,
      "training_loss": 6.695854663848877
    },
    {
      "epoch": 0.49447154471544713,
      "step": 9123,
      "training_loss": 4.919946193695068
    },
    {
      "epoch": 0.4945257452574526,
      "grad_norm": 21.186017990112305,
      "learning_rate": 1e-05,
      "loss": 6.1689,
      "step": 9124
    },
    {
      "epoch": 0.4945257452574526,
      "step": 9124,
      "training_loss": 5.519497394561768
    },
    {
      "epoch": 0.494579945799458,
      "step": 9125,
      "training_loss": 6.48455286026001
    },
    {
      "epoch": 0.4946341463414634,
      "step": 9126,
      "training_loss": 5.458519458770752
    },
    {
      "epoch": 0.49468834688346885,
      "step": 9127,
      "training_loss": 7.73160457611084
    },
    {
      "epoch": 0.49474254742547424,
      "grad_norm": 22.027450561523438,
      "learning_rate": 1e-05,
      "loss": 6.2985,
      "step": 9128
    },
    {
      "epoch": 0.49474254742547424,
      "step": 9128,
      "training_loss": 7.11393404006958
    },
    {
      "epoch": 0.4947967479674797,
      "step": 9129,
      "training_loss": 6.130556583404541
    },
    {
      "epoch": 0.4948509485094851,
      "step": 9130,
      "training_loss": 6.759035110473633
    },
    {
      "epoch": 0.4949051490514905,
      "step": 9131,
      "training_loss": 6.211678504943848
    },
    {
      "epoch": 0.4949593495934959,
      "grad_norm": 35.221168518066406,
      "learning_rate": 1e-05,
      "loss": 6.5538,
      "step": 9132
    },
    {
      "epoch": 0.4949593495934959,
      "step": 9132,
      "training_loss": 6.851840496063232
    },
    {
      "epoch": 0.49501355013550136,
      "step": 9133,
      "training_loss": 6.279191017150879
    },
    {
      "epoch": 0.4950677506775068,
      "step": 9134,
      "training_loss": 7.413212299346924
    },
    {
      "epoch": 0.4951219512195122,
      "step": 9135,
      "training_loss": 7.021441459655762
    },
    {
      "epoch": 0.49517615176151764,
      "grad_norm": 22.1071720123291,
      "learning_rate": 1e-05,
      "loss": 6.8914,
      "step": 9136
    },
    {
      "epoch": 0.49517615176151764,
      "step": 9136,
      "training_loss": 7.069622039794922
    },
    {
      "epoch": 0.495230352303523,
      "step": 9137,
      "training_loss": 5.1128315925598145
    },
    {
      "epoch": 0.49528455284552847,
      "step": 9138,
      "training_loss": 6.743166446685791
    },
    {
      "epoch": 0.49533875338753386,
      "step": 9139,
      "training_loss": 6.581573963165283
    },
    {
      "epoch": 0.4953929539295393,
      "grad_norm": 31.762407302856445,
      "learning_rate": 1e-05,
      "loss": 6.3768,
      "step": 9140
    },
    {
      "epoch": 0.4953929539295393,
      "step": 9140,
      "training_loss": 6.296514987945557
    },
    {
      "epoch": 0.4954471544715447,
      "step": 9141,
      "training_loss": 5.681942462921143
    },
    {
      "epoch": 0.49550135501355014,
      "step": 9142,
      "training_loss": 7.35014009475708
    },
    {
      "epoch": 0.4955555555555556,
      "step": 9143,
      "training_loss": 5.24214506149292
    },
    {
      "epoch": 0.49560975609756097,
      "grad_norm": 25.039804458618164,
      "learning_rate": 1e-05,
      "loss": 6.1427,
      "step": 9144
    },
    {
      "epoch": 0.49560975609756097,
      "step": 9144,
      "training_loss": 7.263769626617432
    },
    {
      "epoch": 0.4956639566395664,
      "step": 9145,
      "training_loss": 6.380624294281006
    },
    {
      "epoch": 0.4957181571815718,
      "step": 9146,
      "training_loss": 5.870656967163086
    },
    {
      "epoch": 0.49577235772357725,
      "step": 9147,
      "training_loss": 7.387397289276123
    },
    {
      "epoch": 0.49582655826558264,
      "grad_norm": 37.08298110961914,
      "learning_rate": 1e-05,
      "loss": 6.7256,
      "step": 9148
    },
    {
      "epoch": 0.49582655826558264,
      "step": 9148,
      "training_loss": 6.882152080535889
    },
    {
      "epoch": 0.4958807588075881,
      "step": 9149,
      "training_loss": 5.440623760223389
    },
    {
      "epoch": 0.4959349593495935,
      "step": 9150,
      "training_loss": 4.091825485229492
    },
    {
      "epoch": 0.4959891598915989,
      "step": 9151,
      "training_loss": 5.969109058380127
    },
    {
      "epoch": 0.49604336043360436,
      "grad_norm": 24.545886993408203,
      "learning_rate": 1e-05,
      "loss": 5.5959,
      "step": 9152
    },
    {
      "epoch": 0.49604336043360436,
      "step": 9152,
      "training_loss": 6.964231967926025
    },
    {
      "epoch": 0.49609756097560975,
      "step": 9153,
      "training_loss": 7.133962631225586
    },
    {
      "epoch": 0.4961517615176152,
      "step": 9154,
      "training_loss": 6.957623481750488
    },
    {
      "epoch": 0.4962059620596206,
      "step": 9155,
      "training_loss": 6.957652568817139
    },
    {
      "epoch": 0.49626016260162603,
      "grad_norm": 18.678415298461914,
      "learning_rate": 1e-05,
      "loss": 7.0034,
      "step": 9156
    },
    {
      "epoch": 0.49626016260162603,
      "step": 9156,
      "training_loss": 5.553287982940674
    },
    {
      "epoch": 0.4963143631436314,
      "step": 9157,
      "training_loss": 7.024933338165283
    },
    {
      "epoch": 0.49636856368563687,
      "step": 9158,
      "training_loss": 6.133265972137451
    },
    {
      "epoch": 0.49642276422764225,
      "step": 9159,
      "training_loss": 7.2564311027526855
    },
    {
      "epoch": 0.4964769647696477,
      "grad_norm": 22.57634735107422,
      "learning_rate": 1e-05,
      "loss": 6.492,
      "step": 9160
    },
    {
      "epoch": 0.4964769647696477,
      "step": 9160,
      "training_loss": 5.0311079025268555
    },
    {
      "epoch": 0.4965311653116531,
      "step": 9161,
      "training_loss": 5.824268817901611
    },
    {
      "epoch": 0.49658536585365853,
      "step": 9162,
      "training_loss": 6.4595537185668945
    },
    {
      "epoch": 0.496639566395664,
      "step": 9163,
      "training_loss": 2.998002290725708
    },
    {
      "epoch": 0.49669376693766937,
      "grad_norm": 33.8668098449707,
      "learning_rate": 1e-05,
      "loss": 5.0782,
      "step": 9164
    },
    {
      "epoch": 0.49669376693766937,
      "step": 9164,
      "training_loss": 6.875458240509033
    },
    {
      "epoch": 0.4967479674796748,
      "step": 9165,
      "training_loss": 6.460803508758545
    },
    {
      "epoch": 0.4968021680216802,
      "step": 9166,
      "training_loss": 7.472646713256836
    },
    {
      "epoch": 0.49685636856368565,
      "step": 9167,
      "training_loss": 7.019392013549805
    },
    {
      "epoch": 0.49691056910569104,
      "grad_norm": 34.137474060058594,
      "learning_rate": 1e-05,
      "loss": 6.9571,
      "step": 9168
    },
    {
      "epoch": 0.49691056910569104,
      "step": 9168,
      "training_loss": 4.379890441894531
    },
    {
      "epoch": 0.4969647696476965,
      "step": 9169,
      "training_loss": 7.300665378570557
    },
    {
      "epoch": 0.49701897018970187,
      "step": 9170,
      "training_loss": 6.906923294067383
    },
    {
      "epoch": 0.4970731707317073,
      "step": 9171,
      "training_loss": 5.049377918243408
    },
    {
      "epoch": 0.49712737127371276,
      "grad_norm": 29.656536102294922,
      "learning_rate": 1e-05,
      "loss": 5.9092,
      "step": 9172
    },
    {
      "epoch": 0.49712737127371276,
      "step": 9172,
      "training_loss": 3.6292219161987305
    },
    {
      "epoch": 0.49718157181571815,
      "step": 9173,
      "training_loss": 6.345781326293945
    },
    {
      "epoch": 0.4972357723577236,
      "step": 9174,
      "training_loss": 4.076997756958008
    },
    {
      "epoch": 0.497289972899729,
      "step": 9175,
      "training_loss": 6.911808490753174
    },
    {
      "epoch": 0.4973441734417344,
      "grad_norm": 24.494810104370117,
      "learning_rate": 1e-05,
      "loss": 5.241,
      "step": 9176
    },
    {
      "epoch": 0.4973441734417344,
      "step": 9176,
      "training_loss": 6.7435808181762695
    },
    {
      "epoch": 0.4973983739837398,
      "step": 9177,
      "training_loss": 7.651881694793701
    },
    {
      "epoch": 0.49745257452574526,
      "step": 9178,
      "training_loss": 6.893044471740723
    },
    {
      "epoch": 0.49750677506775065,
      "step": 9179,
      "training_loss": 6.833942413330078
    },
    {
      "epoch": 0.4975609756097561,
      "grad_norm": 51.26289749145508,
      "learning_rate": 1e-05,
      "loss": 7.0306,
      "step": 9180
    },
    {
      "epoch": 0.4975609756097561,
      "step": 9180,
      "training_loss": 5.616428852081299
    },
    {
      "epoch": 0.49761517615176154,
      "step": 9181,
      "training_loss": 4.57697868347168
    },
    {
      "epoch": 0.49766937669376693,
      "step": 9182,
      "training_loss": 5.932775497436523
    },
    {
      "epoch": 0.4977235772357724,
      "step": 9183,
      "training_loss": 6.989792823791504
    },
    {
      "epoch": 0.49777777777777776,
      "grad_norm": 19.8890380859375,
      "learning_rate": 1e-05,
      "loss": 5.779,
      "step": 9184
    },
    {
      "epoch": 0.49777777777777776,
      "step": 9184,
      "training_loss": 6.257388591766357
    },
    {
      "epoch": 0.4978319783197832,
      "step": 9185,
      "training_loss": 6.887912750244141
    },
    {
      "epoch": 0.4978861788617886,
      "step": 9186,
      "training_loss": 6.671661853790283
    },
    {
      "epoch": 0.49794037940379404,
      "step": 9187,
      "training_loss": 7.047162055969238
    },
    {
      "epoch": 0.49799457994579943,
      "grad_norm": 18.283021926879883,
      "learning_rate": 1e-05,
      "loss": 6.716,
      "step": 9188
    },
    {
      "epoch": 0.49799457994579943,
      "step": 9188,
      "training_loss": 6.967996597290039
    },
    {
      "epoch": 0.4980487804878049,
      "step": 9189,
      "training_loss": 6.6343607902526855
    },
    {
      "epoch": 0.4981029810298103,
      "step": 9190,
      "training_loss": 5.446115016937256
    },
    {
      "epoch": 0.4981571815718157,
      "step": 9191,
      "training_loss": 6.760519027709961
    },
    {
      "epoch": 0.49821138211382116,
      "grad_norm": 42.9332275390625,
      "learning_rate": 1e-05,
      "loss": 6.4522,
      "step": 9192
    },
    {
      "epoch": 0.49821138211382116,
      "step": 9192,
      "training_loss": 6.060924530029297
    },
    {
      "epoch": 0.49826558265582654,
      "step": 9193,
      "training_loss": 6.676138401031494
    },
    {
      "epoch": 0.498319783197832,
      "step": 9194,
      "training_loss": 6.987652778625488
    },
    {
      "epoch": 0.4983739837398374,
      "step": 9195,
      "training_loss": 6.2613115310668945
    },
    {
      "epoch": 0.4984281842818428,
      "grad_norm": 25.993776321411133,
      "learning_rate": 1e-05,
      "loss": 6.4965,
      "step": 9196
    },
    {
      "epoch": 0.4984281842818428,
      "step": 9196,
      "training_loss": 6.07361364364624
    },
    {
      "epoch": 0.4984823848238482,
      "step": 9197,
      "training_loss": 6.046566009521484
    },
    {
      "epoch": 0.49853658536585366,
      "step": 9198,
      "training_loss": 6.080285549163818
    },
    {
      "epoch": 0.4985907859078591,
      "step": 9199,
      "training_loss": 3.2620999813079834
    },
    {
      "epoch": 0.4986449864498645,
      "grad_norm": 27.76082420349121,
      "learning_rate": 1e-05,
      "loss": 5.3656,
      "step": 9200
    },
    {
      "epoch": 0.4986449864498645,
      "step": 9200,
      "training_loss": 7.097308158874512
    },
    {
      "epoch": 0.49869918699186994,
      "step": 9201,
      "training_loss": 6.8357720375061035
    },
    {
      "epoch": 0.4987533875338753,
      "step": 9202,
      "training_loss": 6.578258514404297
    },
    {
      "epoch": 0.49880758807588077,
      "step": 9203,
      "training_loss": 7.316877841949463
    },
    {
      "epoch": 0.49886178861788616,
      "grad_norm": 41.04412078857422,
      "learning_rate": 1e-05,
      "loss": 6.9571,
      "step": 9204
    },
    {
      "epoch": 0.49886178861788616,
      "step": 9204,
      "training_loss": 7.700393199920654
    },
    {
      "epoch": 0.4989159891598916,
      "step": 9205,
      "training_loss": 6.810184001922607
    },
    {
      "epoch": 0.498970189701897,
      "step": 9206,
      "training_loss": 6.656343936920166
    },
    {
      "epoch": 0.49902439024390244,
      "step": 9207,
      "training_loss": 7.13769006729126
    },
    {
      "epoch": 0.4990785907859079,
      "grad_norm": 21.182106018066406,
      "learning_rate": 1e-05,
      "loss": 7.0762,
      "step": 9208
    },
    {
      "epoch": 0.4990785907859079,
      "step": 9208,
      "training_loss": 8.261460304260254
    },
    {
      "epoch": 0.49913279132791327,
      "step": 9209,
      "training_loss": 6.81929874420166
    },
    {
      "epoch": 0.4991869918699187,
      "step": 9210,
      "training_loss": 6.219162464141846
    },
    {
      "epoch": 0.4992411924119241,
      "step": 9211,
      "training_loss": 6.756204605102539
    },
    {
      "epoch": 0.49929539295392955,
      "grad_norm": 20.97350311279297,
      "learning_rate": 1e-05,
      "loss": 7.014,
      "step": 9212
    },
    {
      "epoch": 0.49929539295392955,
      "step": 9212,
      "training_loss": 6.730951309204102
    },
    {
      "epoch": 0.49934959349593494,
      "step": 9213,
      "training_loss": 7.754195213317871
    },
    {
      "epoch": 0.4994037940379404,
      "step": 9214,
      "training_loss": 6.657818794250488
    },
    {
      "epoch": 0.4994579945799458,
      "step": 9215,
      "training_loss": 7.003376007080078
    },
    {
      "epoch": 0.4995121951219512,
      "grad_norm": 21.31661033630371,
      "learning_rate": 1e-05,
      "loss": 7.0366,
      "step": 9216
    },
    {
      "epoch": 0.4995121951219512,
      "step": 9216,
      "training_loss": 6.3646135330200195
    },
    {
      "epoch": 0.49956639566395666,
      "step": 9217,
      "training_loss": 5.2583818435668945
    },
    {
      "epoch": 0.49962059620596205,
      "step": 9218,
      "training_loss": 5.581732749938965
    },
    {
      "epoch": 0.4996747967479675,
      "step": 9219,
      "training_loss": 5.717523574829102
    },
    {
      "epoch": 0.4997289972899729,
      "grad_norm": 59.78232955932617,
      "learning_rate": 1e-05,
      "loss": 5.7306,
      "step": 9220
    },
    {
      "epoch": 0.4997289972899729,
      "step": 9220,
      "training_loss": 7.037298202514648
    },
    {
      "epoch": 0.49978319783197833,
      "step": 9221,
      "training_loss": 6.452640056610107
    },
    {
      "epoch": 0.4998373983739837,
      "step": 9222,
      "training_loss": 7.118925094604492
    },
    {
      "epoch": 0.49989159891598917,
      "step": 9223,
      "training_loss": 5.772362232208252
    },
    {
      "epoch": 0.49994579945799456,
      "grad_norm": 28.74480628967285,
      "learning_rate": 1e-05,
      "loss": 6.5953,
      "step": 9224
    },
    {
      "epoch": 0.49994579945799456,
      "step": 9224,
      "training_loss": 7.91542387008667
    },
    {
      "epoch": 0.5,
      "step": 9225,
      "training_loss": 6.796013355255127
    },
    {
      "epoch": 0.5000542005420054,
      "step": 9226,
      "training_loss": 6.085362911224365
    },
    {
      "epoch": 0.5001084010840109,
      "step": 9227,
      "training_loss": 4.1684889793396
    },
    {
      "epoch": 0.5001626016260162,
      "grad_norm": 30.2279052734375,
      "learning_rate": 1e-05,
      "loss": 6.2413,
      "step": 9228
    },
    {
      "epoch": 0.5001626016260162,
      "step": 9228,
      "training_loss": 7.405529975891113
    },
    {
      "epoch": 0.5002168021680217,
      "step": 9229,
      "training_loss": 5.722873210906982
    },
    {
      "epoch": 0.5002710027100271,
      "step": 9230,
      "training_loss": 7.19019079208374
    },
    {
      "epoch": 0.5003252032520326,
      "step": 9231,
      "training_loss": 6.17974328994751
    },
    {
      "epoch": 0.5003794037940379,
      "grad_norm": 32.24415588378906,
      "learning_rate": 1e-05,
      "loss": 6.6246,
      "step": 9232
    },
    {
      "epoch": 0.5003794037940379,
      "step": 9232,
      "training_loss": 6.88792610168457
    },
    {
      "epoch": 0.5004336043360433,
      "step": 9233,
      "training_loss": 7.298182487487793
    },
    {
      "epoch": 0.5004878048780488,
      "step": 9234,
      "training_loss": 7.61370849609375
    },
    {
      "epoch": 0.5005420054200542,
      "step": 9235,
      "training_loss": 6.225897312164307
    },
    {
      "epoch": 0.5005962059620597,
      "grad_norm": 41.76133728027344,
      "learning_rate": 1e-05,
      "loss": 7.0064,
      "step": 9236
    },
    {
      "epoch": 0.5005962059620597,
      "step": 9236,
      "training_loss": 6.672258377075195
    },
    {
      "epoch": 0.500650406504065,
      "step": 9237,
      "training_loss": 5.966427803039551
    },
    {
      "epoch": 0.5007046070460704,
      "step": 9238,
      "training_loss": 6.014954566955566
    },
    {
      "epoch": 0.5007588075880759,
      "step": 9239,
      "training_loss": 5.720435619354248
    },
    {
      "epoch": 0.5008130081300813,
      "grad_norm": 23.23637580871582,
      "learning_rate": 1e-05,
      "loss": 6.0935,
      "step": 9240
    },
    {
      "epoch": 0.5008130081300813,
      "step": 9240,
      "training_loss": 6.28398323059082
    },
    {
      "epoch": 0.5008672086720867,
      "step": 9241,
      "training_loss": 6.95640230178833
    },
    {
      "epoch": 0.5009214092140921,
      "step": 9242,
      "training_loss": 7.4750823974609375
    },
    {
      "epoch": 0.5009756097560976,
      "step": 9243,
      "training_loss": 7.1204514503479
    },
    {
      "epoch": 0.501029810298103,
      "grad_norm": 25.249927520751953,
      "learning_rate": 1e-05,
      "loss": 6.959,
      "step": 9244
    },
    {
      "epoch": 0.501029810298103,
      "step": 9244,
      "training_loss": 7.180030345916748
    },
    {
      "epoch": 0.5010840108401085,
      "step": 9245,
      "training_loss": 7.007579803466797
    },
    {
      "epoch": 0.5011382113821138,
      "step": 9246,
      "training_loss": 6.847432613372803
    },
    {
      "epoch": 0.5011924119241192,
      "step": 9247,
      "training_loss": 6.7306952476501465
    },
    {
      "epoch": 0.5012466124661247,
      "grad_norm": 23.90400505065918,
      "learning_rate": 1e-05,
      "loss": 6.9414,
      "step": 9248
    },
    {
      "epoch": 0.5012466124661247,
      "step": 9248,
      "training_loss": 7.464193344116211
    },
    {
      "epoch": 0.5013008130081301,
      "step": 9249,
      "training_loss": 2.9250478744506836
    },
    {
      "epoch": 0.5013550135501355,
      "step": 9250,
      "training_loss": 4.727477073669434
    },
    {
      "epoch": 0.5014092140921409,
      "step": 9251,
      "training_loss": 5.7468180656433105
    },
    {
      "epoch": 0.5014634146341463,
      "grad_norm": 40.029972076416016,
      "learning_rate": 1e-05,
      "loss": 5.2159,
      "step": 9252
    },
    {
      "epoch": 0.5014634146341463,
      "step": 9252,
      "training_loss": 6.878419876098633
    },
    {
      "epoch": 0.5015176151761518,
      "step": 9253,
      "training_loss": 5.944953441619873
    },
    {
      "epoch": 0.5015718157181572,
      "step": 9254,
      "training_loss": 6.680044174194336
    },
    {
      "epoch": 0.5016260162601626,
      "step": 9255,
      "training_loss": 6.949342727661133
    },
    {
      "epoch": 0.501680216802168,
      "grad_norm": 21.25746726989746,
      "learning_rate": 1e-05,
      "loss": 6.6132,
      "step": 9256
    },
    {
      "epoch": 0.501680216802168,
      "step": 9256,
      "training_loss": 6.953127384185791
    },
    {
      "epoch": 0.5017344173441735,
      "step": 9257,
      "training_loss": 6.543498992919922
    },
    {
      "epoch": 0.5017886178861789,
      "step": 9258,
      "training_loss": 6.009528636932373
    },
    {
      "epoch": 0.5018428184281842,
      "step": 9259,
      "training_loss": 6.935858249664307
    },
    {
      "epoch": 0.5018970189701897,
      "grad_norm": 23.894760131835938,
      "learning_rate": 1e-05,
      "loss": 6.6105,
      "step": 9260
    },
    {
      "epoch": 0.5018970189701897,
      "step": 9260,
      "training_loss": 7.696237564086914
    },
    {
      "epoch": 0.5019512195121951,
      "step": 9261,
      "training_loss": 7.3132710456848145
    },
    {
      "epoch": 0.5020054200542006,
      "step": 9262,
      "training_loss": 7.7938642501831055
    },
    {
      "epoch": 0.502059620596206,
      "step": 9263,
      "training_loss": 7.65399169921875
    },
    {
      "epoch": 0.5021138211382113,
      "grad_norm": 36.8054313659668,
      "learning_rate": 1e-05,
      "loss": 7.6143,
      "step": 9264
    },
    {
      "epoch": 0.5021138211382113,
      "step": 9264,
      "training_loss": 6.467344284057617
    },
    {
      "epoch": 0.5021680216802168,
      "step": 9265,
      "training_loss": 7.40678596496582
    },
    {
      "epoch": 0.5022222222222222,
      "step": 9266,
      "training_loss": 7.417160511016846
    },
    {
      "epoch": 0.5022764227642277,
      "step": 9267,
      "training_loss": 7.382201671600342
    },
    {
      "epoch": 0.502330623306233,
      "grad_norm": 54.224388122558594,
      "learning_rate": 1e-05,
      "loss": 7.1684,
      "step": 9268
    },
    {
      "epoch": 0.502330623306233,
      "step": 9268,
      "training_loss": 6.83365535736084
    },
    {
      "epoch": 0.5023848238482385,
      "step": 9269,
      "training_loss": 5.109129428863525
    },
    {
      "epoch": 0.5024390243902439,
      "step": 9270,
      "training_loss": 7.6540327072143555
    },
    {
      "epoch": 0.5024932249322493,
      "step": 9271,
      "training_loss": 5.41087007522583
    },
    {
      "epoch": 0.5025474254742548,
      "grad_norm": 26.68635368347168,
      "learning_rate": 1e-05,
      "loss": 6.2519,
      "step": 9272
    },
    {
      "epoch": 0.5025474254742548,
      "step": 9272,
      "training_loss": 5.438775062561035
    },
    {
      "epoch": 0.5026016260162601,
      "step": 9273,
      "training_loss": 5.489713668823242
    },
    {
      "epoch": 0.5026558265582656,
      "step": 9274,
      "training_loss": 5.3606343269348145
    },
    {
      "epoch": 0.502710027100271,
      "step": 9275,
      "training_loss": 6.941457748413086
    },
    {
      "epoch": 0.5027642276422765,
      "grad_norm": 33.9127311706543,
      "learning_rate": 1e-05,
      "loss": 5.8076,
      "step": 9276
    },
    {
      "epoch": 0.5027642276422765,
      "step": 9276,
      "training_loss": 6.594932556152344
    },
    {
      "epoch": 0.5028184281842818,
      "step": 9277,
      "training_loss": 6.74439811706543
    },
    {
      "epoch": 0.5028726287262872,
      "step": 9278,
      "training_loss": 6.94791841506958
    },
    {
      "epoch": 0.5029268292682927,
      "step": 9279,
      "training_loss": 7.697303295135498
    },
    {
      "epoch": 0.5029810298102981,
      "grad_norm": 40.984222412109375,
      "learning_rate": 1e-05,
      "loss": 6.9961,
      "step": 9280
    },
    {
      "epoch": 0.5029810298102981,
      "step": 9280,
      "training_loss": 6.13950777053833
    },
    {
      "epoch": 0.5030352303523036,
      "step": 9281,
      "training_loss": 8.564407348632812
    },
    {
      "epoch": 0.5030894308943089,
      "step": 9282,
      "training_loss": 7.297156810760498
    },
    {
      "epoch": 0.5031436314363144,
      "step": 9283,
      "training_loss": 6.592316627502441
    },
    {
      "epoch": 0.5031978319783198,
      "grad_norm": 24.069000244140625,
      "learning_rate": 1e-05,
      "loss": 7.1483,
      "step": 9284
    },
    {
      "epoch": 0.5031978319783198,
      "step": 9284,
      "training_loss": 7.2291579246521
    },
    {
      "epoch": 0.5032520325203252,
      "step": 9285,
      "training_loss": 7.275142192840576
    },
    {
      "epoch": 0.5033062330623306,
      "step": 9286,
      "training_loss": 6.371780872344971
    },
    {
      "epoch": 0.503360433604336,
      "step": 9287,
      "training_loss": 6.18124532699585
    },
    {
      "epoch": 0.5034146341463415,
      "grad_norm": 38.2996711730957,
      "learning_rate": 1e-05,
      "loss": 6.7643,
      "step": 9288
    },
    {
      "epoch": 0.5034146341463415,
      "step": 9288,
      "training_loss": 6.850641250610352
    },
    {
      "epoch": 0.5034688346883469,
      "step": 9289,
      "training_loss": 6.387641429901123
    },
    {
      "epoch": 0.5035230352303524,
      "step": 9290,
      "training_loss": 8.045665740966797
    },
    {
      "epoch": 0.5035772357723577,
      "step": 9291,
      "training_loss": 7.114301681518555
    },
    {
      "epoch": 0.5036314363143631,
      "grad_norm": 19.01157569885254,
      "learning_rate": 1e-05,
      "loss": 7.0996,
      "step": 9292
    },
    {
      "epoch": 0.5036314363143631,
      "step": 9292,
      "training_loss": 6.628777980804443
    },
    {
      "epoch": 0.5036856368563686,
      "step": 9293,
      "training_loss": 6.732247829437256
    },
    {
      "epoch": 0.503739837398374,
      "step": 9294,
      "training_loss": 5.480654716491699
    },
    {
      "epoch": 0.5037940379403794,
      "step": 9295,
      "training_loss": 6.671843528747559
    },
    {
      "epoch": 0.5038482384823848,
      "grad_norm": 22.01521110534668,
      "learning_rate": 1e-05,
      "loss": 6.3784,
      "step": 9296
    },
    {
      "epoch": 0.5038482384823848,
      "step": 9296,
      "training_loss": 7.700998783111572
    },
    {
      "epoch": 0.5039024390243902,
      "step": 9297,
      "training_loss": 7.851490497589111
    },
    {
      "epoch": 0.5039566395663957,
      "step": 9298,
      "training_loss": 6.895702362060547
    },
    {
      "epoch": 0.5040108401084011,
      "step": 9299,
      "training_loss": 6.650139808654785
    },
    {
      "epoch": 0.5040650406504065,
      "grad_norm": 25.017419815063477,
      "learning_rate": 1e-05,
      "loss": 7.2746,
      "step": 9300
    },
    {
      "epoch": 0.5040650406504065,
      "step": 9300,
      "training_loss": 7.591976165771484
    },
    {
      "epoch": 0.5041192411924119,
      "step": 9301,
      "training_loss": 7.49928092956543
    },
    {
      "epoch": 0.5041734417344174,
      "step": 9302,
      "training_loss": 5.035922527313232
    },
    {
      "epoch": 0.5042276422764228,
      "step": 9303,
      "training_loss": 9.189040184020996
    },
    {
      "epoch": 0.5042818428184281,
      "grad_norm": 78.20269012451172,
      "learning_rate": 1e-05,
      "loss": 7.3291,
      "step": 9304
    },
    {
      "epoch": 0.5042818428184281,
      "step": 9304,
      "training_loss": 4.524158000946045
    },
    {
      "epoch": 0.5043360433604336,
      "step": 9305,
      "training_loss": 7.402740001678467
    },
    {
      "epoch": 0.504390243902439,
      "step": 9306,
      "training_loss": 5.117555141448975
    },
    {
      "epoch": 0.5044444444444445,
      "step": 9307,
      "training_loss": 4.965198993682861
    },
    {
      "epoch": 0.5044986449864499,
      "grad_norm": 26.847505569458008,
      "learning_rate": 1e-05,
      "loss": 5.5024,
      "step": 9308
    },
    {
      "epoch": 0.5044986449864499,
      "step": 9308,
      "training_loss": 7.566585540771484
    },
    {
      "epoch": 0.5045528455284553,
      "step": 9309,
      "training_loss": 6.356234550476074
    },
    {
      "epoch": 0.5046070460704607,
      "step": 9310,
      "training_loss": 6.9276814460754395
    },
    {
      "epoch": 0.5046612466124661,
      "step": 9311,
      "training_loss": 6.948678493499756
    },
    {
      "epoch": 0.5047154471544716,
      "grad_norm": 27.49725341796875,
      "learning_rate": 1e-05,
      "loss": 6.9498,
      "step": 9312
    },
    {
      "epoch": 0.5047154471544716,
      "step": 9312,
      "training_loss": 5.2508344650268555
    },
    {
      "epoch": 0.5047696476964769,
      "step": 9313,
      "training_loss": 7.699601173400879
    },
    {
      "epoch": 0.5048238482384824,
      "step": 9314,
      "training_loss": 6.326183795928955
    },
    {
      "epoch": 0.5048780487804878,
      "step": 9315,
      "training_loss": 6.598257064819336
    },
    {
      "epoch": 0.5049322493224933,
      "grad_norm": 24.21921730041504,
      "learning_rate": 1e-05,
      "loss": 6.4687,
      "step": 9316
    },
    {
      "epoch": 0.5049322493224933,
      "step": 9316,
      "training_loss": 5.979980945587158
    },
    {
      "epoch": 0.5049864498644987,
      "step": 9317,
      "training_loss": 6.85888671875
    },
    {
      "epoch": 0.505040650406504,
      "step": 9318,
      "training_loss": 7.4621429443359375
    },
    {
      "epoch": 0.5050948509485095,
      "step": 9319,
      "training_loss": 6.135166168212891
    },
    {
      "epoch": 0.5051490514905149,
      "grad_norm": 28.18268585205078,
      "learning_rate": 1e-05,
      "loss": 6.609,
      "step": 9320
    },
    {
      "epoch": 0.5051490514905149,
      "step": 9320,
      "training_loss": 6.883684158325195
    },
    {
      "epoch": 0.5052032520325204,
      "step": 9321,
      "training_loss": 4.416332244873047
    },
    {
      "epoch": 0.5052574525745257,
      "step": 9322,
      "training_loss": 7.626893997192383
    },
    {
      "epoch": 0.5053116531165311,
      "step": 9323,
      "training_loss": 6.203800678253174
    },
    {
      "epoch": 0.5053658536585366,
      "grad_norm": 25.84231948852539,
      "learning_rate": 1e-05,
      "loss": 6.2827,
      "step": 9324
    },
    {
      "epoch": 0.5053658536585366,
      "step": 9324,
      "training_loss": 5.287230491638184
    },
    {
      "epoch": 0.505420054200542,
      "step": 9325,
      "training_loss": 7.376324653625488
    },
    {
      "epoch": 0.5054742547425475,
      "step": 9326,
      "training_loss": 4.685263633728027
    },
    {
      "epoch": 0.5055284552845528,
      "step": 9327,
      "training_loss": 7.270073890686035
    },
    {
      "epoch": 0.5055826558265583,
      "grad_norm": 36.456520080566406,
      "learning_rate": 1e-05,
      "loss": 6.1547,
      "step": 9328
    },
    {
      "epoch": 0.5055826558265583,
      "step": 9328,
      "training_loss": 6.6220808029174805
    },
    {
      "epoch": 0.5056368563685637,
      "step": 9329,
      "training_loss": 6.662378787994385
    },
    {
      "epoch": 0.5056910569105691,
      "step": 9330,
      "training_loss": 6.471219539642334
    },
    {
      "epoch": 0.5057452574525745,
      "step": 9331,
      "training_loss": 6.070008754730225
    },
    {
      "epoch": 0.5057994579945799,
      "grad_norm": 19.189910888671875,
      "learning_rate": 1e-05,
      "loss": 6.4564,
      "step": 9332
    },
    {
      "epoch": 0.5057994579945799,
      "step": 9332,
      "training_loss": 6.335573196411133
    },
    {
      "epoch": 0.5058536585365854,
      "step": 9333,
      "training_loss": 7.450869560241699
    },
    {
      "epoch": 0.5059078590785908,
      "step": 9334,
      "training_loss": 6.555703639984131
    },
    {
      "epoch": 0.5059620596205963,
      "step": 9335,
      "training_loss": 5.7164082527160645
    },
    {
      "epoch": 0.5060162601626016,
      "grad_norm": 25.747835159301758,
      "learning_rate": 1e-05,
      "loss": 6.5146,
      "step": 9336
    },
    {
      "epoch": 0.5060162601626016,
      "step": 9336,
      "training_loss": 7.931277751922607
    },
    {
      "epoch": 0.506070460704607,
      "step": 9337,
      "training_loss": 6.726268768310547
    },
    {
      "epoch": 0.5061246612466125,
      "step": 9338,
      "training_loss": 10.825084686279297
    },
    {
      "epoch": 0.5061788617886179,
      "step": 9339,
      "training_loss": 6.826751708984375
    },
    {
      "epoch": 0.5062330623306233,
      "grad_norm": 29.48639678955078,
      "learning_rate": 1e-05,
      "loss": 8.0773,
      "step": 9340
    },
    {
      "epoch": 0.5062330623306233,
      "step": 9340,
      "training_loss": 6.641903400421143
    },
    {
      "epoch": 0.5062872628726287,
      "step": 9341,
      "training_loss": 5.60133171081543
    },
    {
      "epoch": 0.5063414634146342,
      "step": 9342,
      "training_loss": 6.005939483642578
    },
    {
      "epoch": 0.5063956639566396,
      "step": 9343,
      "training_loss": 7.464890956878662
    },
    {
      "epoch": 0.506449864498645,
      "grad_norm": 24.916744232177734,
      "learning_rate": 1e-05,
      "loss": 6.4285,
      "step": 9344
    },
    {
      "epoch": 0.506449864498645,
      "step": 9344,
      "training_loss": 4.864725112915039
    },
    {
      "epoch": 0.5065040650406504,
      "step": 9345,
      "training_loss": 6.475304126739502
    },
    {
      "epoch": 0.5065582655826558,
      "step": 9346,
      "training_loss": 7.305524826049805
    },
    {
      "epoch": 0.5066124661246613,
      "step": 9347,
      "training_loss": 6.956238269805908
    },
    {
      "epoch": 0.5066666666666667,
      "grad_norm": 18.60189437866211,
      "learning_rate": 1e-05,
      "loss": 6.4004,
      "step": 9348
    },
    {
      "epoch": 0.5066666666666667,
      "step": 9348,
      "training_loss": 6.900885105133057
    },
    {
      "epoch": 0.506720867208672,
      "step": 9349,
      "training_loss": 7.510798931121826
    },
    {
      "epoch": 0.5067750677506775,
      "step": 9350,
      "training_loss": 6.7894487380981445
    },
    {
      "epoch": 0.5068292682926829,
      "step": 9351,
      "training_loss": 6.58537483215332
    },
    {
      "epoch": 0.5068834688346884,
      "grad_norm": 28.024919509887695,
      "learning_rate": 1e-05,
      "loss": 6.9466,
      "step": 9352
    },
    {
      "epoch": 0.5068834688346884,
      "step": 9352,
      "training_loss": 7.0175065994262695
    },
    {
      "epoch": 0.5069376693766938,
      "step": 9353,
      "training_loss": 5.763477802276611
    },
    {
      "epoch": 0.5069918699186992,
      "step": 9354,
      "training_loss": 4.779074192047119
    },
    {
      "epoch": 0.5070460704607046,
      "step": 9355,
      "training_loss": 6.7921247482299805
    },
    {
      "epoch": 0.50710027100271,
      "grad_norm": 57.80131149291992,
      "learning_rate": 1e-05,
      "loss": 6.088,
      "step": 9356
    },
    {
      "epoch": 0.50710027100271,
      "step": 9356,
      "training_loss": 7.164523124694824
    },
    {
      "epoch": 0.5071544715447155,
      "step": 9357,
      "training_loss": 7.945720672607422
    },
    {
      "epoch": 0.5072086720867208,
      "step": 9358,
      "training_loss": 6.462282657623291
    },
    {
      "epoch": 0.5072628726287263,
      "step": 9359,
      "training_loss": 5.828577518463135
    },
    {
      "epoch": 0.5073170731707317,
      "grad_norm": 55.56242752075195,
      "learning_rate": 1e-05,
      "loss": 6.8503,
      "step": 9360
    },
    {
      "epoch": 0.5073170731707317,
      "step": 9360,
      "training_loss": 6.005712509155273
    },
    {
      "epoch": 0.5073712737127372,
      "step": 9361,
      "training_loss": 5.586267471313477
    },
    {
      "epoch": 0.5074254742547425,
      "step": 9362,
      "training_loss": 5.9809465408325195
    },
    {
      "epoch": 0.5074796747967479,
      "step": 9363,
      "training_loss": 5.5904340744018555
    },
    {
      "epoch": 0.5075338753387534,
      "grad_norm": 42.794803619384766,
      "learning_rate": 1e-05,
      "loss": 5.7908,
      "step": 9364
    },
    {
      "epoch": 0.5075338753387534,
      "step": 9364,
      "training_loss": 6.534442901611328
    },
    {
      "epoch": 0.5075880758807588,
      "step": 9365,
      "training_loss": 5.909269332885742
    },
    {
      "epoch": 0.5076422764227643,
      "step": 9366,
      "training_loss": 7.585550308227539
    },
    {
      "epoch": 0.5076964769647696,
      "step": 9367,
      "training_loss": 7.080704689025879
    },
    {
      "epoch": 0.507750677506775,
      "grad_norm": 16.407014846801758,
      "learning_rate": 1e-05,
      "loss": 6.7775,
      "step": 9368
    },
    {
      "epoch": 0.507750677506775,
      "step": 9368,
      "training_loss": 3.4285476207733154
    },
    {
      "epoch": 0.5078048780487805,
      "step": 9369,
      "training_loss": 5.117558002471924
    },
    {
      "epoch": 0.5078590785907859,
      "step": 9370,
      "training_loss": 7.25670862197876
    },
    {
      "epoch": 0.5079132791327913,
      "step": 9371,
      "training_loss": 6.4113450050354
    },
    {
      "epoch": 0.5079674796747967,
      "grad_norm": 30.095380783081055,
      "learning_rate": 1e-05,
      "loss": 5.5535,
      "step": 9372
    },
    {
      "epoch": 0.5079674796747967,
      "step": 9372,
      "training_loss": 7.988765716552734
    },
    {
      "epoch": 0.5080216802168022,
      "step": 9373,
      "training_loss": 8.178577423095703
    },
    {
      "epoch": 0.5080758807588076,
      "step": 9374,
      "training_loss": 6.087115287780762
    },
    {
      "epoch": 0.508130081300813,
      "step": 9375,
      "training_loss": 7.213921546936035
    },
    {
      "epoch": 0.5081842818428184,
      "grad_norm": 23.360273361206055,
      "learning_rate": 1e-05,
      "loss": 7.3671,
      "step": 9376
    },
    {
      "epoch": 0.5081842818428184,
      "step": 9376,
      "training_loss": 7.728024005889893
    },
    {
      "epoch": 0.5082384823848238,
      "step": 9377,
      "training_loss": 6.800257205963135
    },
    {
      "epoch": 0.5082926829268293,
      "step": 9378,
      "training_loss": 6.109385013580322
    },
    {
      "epoch": 0.5083468834688347,
      "step": 9379,
      "training_loss": 7.501580715179443
    },
    {
      "epoch": 0.50840108401084,
      "grad_norm": 26.626985549926758,
      "learning_rate": 1e-05,
      "loss": 7.0348,
      "step": 9380
    },
    {
      "epoch": 0.50840108401084,
      "step": 9380,
      "training_loss": 7.180726528167725
    },
    {
      "epoch": 0.5084552845528455,
      "step": 9381,
      "training_loss": 7.238770008087158
    },
    {
      "epoch": 0.5085094850948509,
      "step": 9382,
      "training_loss": 7.853950500488281
    },
    {
      "epoch": 0.5085636856368564,
      "step": 9383,
      "training_loss": 6.99188232421875
    },
    {
      "epoch": 0.5086178861788618,
      "grad_norm": 34.85055160522461,
      "learning_rate": 1e-05,
      "loss": 7.3163,
      "step": 9384
    },
    {
      "epoch": 0.5086178861788618,
      "step": 9384,
      "training_loss": 6.633785247802734
    },
    {
      "epoch": 0.5086720867208672,
      "step": 9385,
      "training_loss": 7.195978164672852
    },
    {
      "epoch": 0.5087262872628726,
      "step": 9386,
      "training_loss": 7.041106224060059
    },
    {
      "epoch": 0.5087804878048781,
      "step": 9387,
      "training_loss": 8.544382095336914
    },
    {
      "epoch": 0.5088346883468835,
      "grad_norm": 48.63591003417969,
      "learning_rate": 1e-05,
      "loss": 7.3538,
      "step": 9388
    },
    {
      "epoch": 0.5088346883468835,
      "step": 9388,
      "training_loss": 6.42794942855835
    },
    {
      "epoch": 0.5088888888888888,
      "step": 9389,
      "training_loss": 7.115901470184326
    },
    {
      "epoch": 0.5089430894308943,
      "step": 9390,
      "training_loss": 6.532139778137207
    },
    {
      "epoch": 0.5089972899728997,
      "step": 9391,
      "training_loss": 4.846798896789551
    },
    {
      "epoch": 0.5090514905149052,
      "grad_norm": 47.7508659362793,
      "learning_rate": 1e-05,
      "loss": 6.2307,
      "step": 9392
    },
    {
      "epoch": 0.5090514905149052,
      "step": 9392,
      "training_loss": 7.3045759201049805
    },
    {
      "epoch": 0.5091056910569106,
      "step": 9393,
      "training_loss": 7.540050506591797
    },
    {
      "epoch": 0.509159891598916,
      "step": 9394,
      "training_loss": 6.420684337615967
    },
    {
      "epoch": 0.5092140921409214,
      "step": 9395,
      "training_loss": 5.783837795257568
    },
    {
      "epoch": 0.5092682926829268,
      "grad_norm": 39.45790100097656,
      "learning_rate": 1e-05,
      "loss": 6.7623,
      "step": 9396
    },
    {
      "epoch": 0.5092682926829268,
      "step": 9396,
      "training_loss": 6.995947360992432
    },
    {
      "epoch": 0.5093224932249323,
      "step": 9397,
      "training_loss": 6.224783420562744
    },
    {
      "epoch": 0.5093766937669376,
      "step": 9398,
      "training_loss": 7.60929536819458
    },
    {
      "epoch": 0.5094308943089431,
      "step": 9399,
      "training_loss": 6.936363697052002
    },
    {
      "epoch": 0.5094850948509485,
      "grad_norm": 32.61698532104492,
      "learning_rate": 1e-05,
      "loss": 6.9416,
      "step": 9400
    },
    {
      "epoch": 0.5094850948509485,
      "step": 9400,
      "training_loss": 6.748865127563477
    },
    {
      "epoch": 0.509539295392954,
      "step": 9401,
      "training_loss": 6.621376037597656
    },
    {
      "epoch": 0.5095934959349594,
      "step": 9402,
      "training_loss": 6.649735450744629
    },
    {
      "epoch": 0.5096476964769647,
      "step": 9403,
      "training_loss": 6.082646369934082
    },
    {
      "epoch": 0.5097018970189702,
      "grad_norm": 36.135589599609375,
      "learning_rate": 1e-05,
      "loss": 6.5257,
      "step": 9404
    },
    {
      "epoch": 0.5097018970189702,
      "step": 9404,
      "training_loss": 6.506075382232666
    },
    {
      "epoch": 0.5097560975609756,
      "step": 9405,
      "training_loss": 7.326719284057617
    },
    {
      "epoch": 0.5098102981029811,
      "step": 9406,
      "training_loss": 6.81652307510376
    },
    {
      "epoch": 0.5098644986449864,
      "step": 9407,
      "training_loss": 6.363936424255371
    },
    {
      "epoch": 0.5099186991869918,
      "grad_norm": 40.95758819580078,
      "learning_rate": 1e-05,
      "loss": 6.7533,
      "step": 9408
    },
    {
      "epoch": 0.5099186991869918,
      "step": 9408,
      "training_loss": 7.09166955947876
    },
    {
      "epoch": 0.5099728997289973,
      "step": 9409,
      "training_loss": 7.997737407684326
    },
    {
      "epoch": 0.5100271002710027,
      "step": 9410,
      "training_loss": 6.5144548416137695
    },
    {
      "epoch": 0.5100813008130082,
      "step": 9411,
      "training_loss": 6.1622514724731445
    },
    {
      "epoch": 0.5101355013550135,
      "grad_norm": 26.985763549804688,
      "learning_rate": 1e-05,
      "loss": 6.9415,
      "step": 9412
    },
    {
      "epoch": 0.5101355013550135,
      "step": 9412,
      "training_loss": 5.837831020355225
    },
    {
      "epoch": 0.510189701897019,
      "step": 9413,
      "training_loss": 7.470285415649414
    },
    {
      "epoch": 0.5102439024390244,
      "step": 9414,
      "training_loss": 6.062154293060303
    },
    {
      "epoch": 0.5102981029810298,
      "step": 9415,
      "training_loss": 6.263950824737549
    },
    {
      "epoch": 0.5103523035230352,
      "grad_norm": 23.17583465576172,
      "learning_rate": 1e-05,
      "loss": 6.4086,
      "step": 9416
    },
    {
      "epoch": 0.5103523035230352,
      "step": 9416,
      "training_loss": 7.062836647033691
    },
    {
      "epoch": 0.5104065040650406,
      "step": 9417,
      "training_loss": 8.644229888916016
    },
    {
      "epoch": 0.5104607046070461,
      "step": 9418,
      "training_loss": 5.683413982391357
    },
    {
      "epoch": 0.5105149051490515,
      "step": 9419,
      "training_loss": 8.732364654541016
    },
    {
      "epoch": 0.510569105691057,
      "grad_norm": 36.65022659301758,
      "learning_rate": 1e-05,
      "loss": 7.5307,
      "step": 9420
    },
    {
      "epoch": 0.510569105691057,
      "step": 9420,
      "training_loss": 6.777005672454834
    },
    {
      "epoch": 0.5106233062330623,
      "step": 9421,
      "training_loss": 5.165324687957764
    },
    {
      "epoch": 0.5106775067750677,
      "step": 9422,
      "training_loss": 7.176806449890137
    },
    {
      "epoch": 0.5107317073170732,
      "step": 9423,
      "training_loss": 7.422308444976807
    },
    {
      "epoch": 0.5107859078590786,
      "grad_norm": 44.1083869934082,
      "learning_rate": 1e-05,
      "loss": 6.6354,
      "step": 9424
    },
    {
      "epoch": 0.5107859078590786,
      "step": 9424,
      "training_loss": 5.607591152191162
    },
    {
      "epoch": 0.510840108401084,
      "step": 9425,
      "training_loss": 6.204714775085449
    },
    {
      "epoch": 0.5108943089430894,
      "step": 9426,
      "training_loss": 7.5787272453308105
    },
    {
      "epoch": 0.5109485094850948,
      "step": 9427,
      "training_loss": 6.63130521774292
    },
    {
      "epoch": 0.5110027100271003,
      "grad_norm": 32.82887268066406,
      "learning_rate": 1e-05,
      "loss": 6.5056,
      "step": 9428
    },
    {
      "epoch": 0.5110027100271003,
      "step": 9428,
      "training_loss": 7.114188194274902
    },
    {
      "epoch": 0.5110569105691057,
      "step": 9429,
      "training_loss": 5.148246765136719
    },
    {
      "epoch": 0.5111111111111111,
      "step": 9430,
      "training_loss": 6.857637882232666
    },
    {
      "epoch": 0.5111653116531165,
      "step": 9431,
      "training_loss": 5.485462188720703
    },
    {
      "epoch": 0.511219512195122,
      "grad_norm": 27.076702117919922,
      "learning_rate": 1e-05,
      "loss": 6.1514,
      "step": 9432
    },
    {
      "epoch": 0.511219512195122,
      "step": 9432,
      "training_loss": 6.562795162200928
    },
    {
      "epoch": 0.5112737127371274,
      "step": 9433,
      "training_loss": 6.5331711769104
    },
    {
      "epoch": 0.5113279132791327,
      "step": 9434,
      "training_loss": 7.745797634124756
    },
    {
      "epoch": 0.5113821138211382,
      "step": 9435,
      "training_loss": 5.58876371383667
    },
    {
      "epoch": 0.5114363143631436,
      "grad_norm": 49.34455108642578,
      "learning_rate": 1e-05,
      "loss": 6.6076,
      "step": 9436
    },
    {
      "epoch": 0.5114363143631436,
      "step": 9436,
      "training_loss": 6.233401775360107
    },
    {
      "epoch": 0.5114905149051491,
      "step": 9437,
      "training_loss": 6.853105068206787
    },
    {
      "epoch": 0.5115447154471545,
      "step": 9438,
      "training_loss": 6.86221170425415
    },
    {
      "epoch": 0.5115989159891599,
      "step": 9439,
      "training_loss": 6.541162014007568
    },
    {
      "epoch": 0.5116531165311653,
      "grad_norm": 31.21217918395996,
      "learning_rate": 1e-05,
      "loss": 6.6225,
      "step": 9440
    },
    {
      "epoch": 0.5116531165311653,
      "step": 9440,
      "training_loss": 5.65451717376709
    },
    {
      "epoch": 0.5117073170731707,
      "step": 9441,
      "training_loss": 6.563840389251709
    },
    {
      "epoch": 0.5117615176151762,
      "step": 9442,
      "training_loss": 3.678635835647583
    },
    {
      "epoch": 0.5118157181571815,
      "step": 9443,
      "training_loss": 6.4943156242370605
    },
    {
      "epoch": 0.511869918699187,
      "grad_norm": 32.11185073852539,
      "learning_rate": 1e-05,
      "loss": 5.5978,
      "step": 9444
    },
    {
      "epoch": 0.511869918699187,
      "step": 9444,
      "training_loss": 6.4007110595703125
    },
    {
      "epoch": 0.5119241192411924,
      "step": 9445,
      "training_loss": 6.852844715118408
    },
    {
      "epoch": 0.5119783197831979,
      "step": 9446,
      "training_loss": 6.931565284729004
    },
    {
      "epoch": 0.5120325203252033,
      "step": 9447,
      "training_loss": 8.416762351989746
    },
    {
      "epoch": 0.5120867208672086,
      "grad_norm": 49.863346099853516,
      "learning_rate": 1e-05,
      "loss": 7.1505,
      "step": 9448
    },
    {
      "epoch": 0.5120867208672086,
      "step": 9448,
      "training_loss": 6.584115505218506
    },
    {
      "epoch": 0.5121409214092141,
      "step": 9449,
      "training_loss": 7.1199822425842285
    },
    {
      "epoch": 0.5121951219512195,
      "step": 9450,
      "training_loss": 4.982180118560791
    },
    {
      "epoch": 0.512249322493225,
      "step": 9451,
      "training_loss": 7.955127716064453
    },
    {
      "epoch": 0.5123035230352303,
      "grad_norm": 43.33005905151367,
      "learning_rate": 1e-05,
      "loss": 6.6604,
      "step": 9452
    },
    {
      "epoch": 0.5123035230352303,
      "step": 9452,
      "training_loss": 6.521570205688477
    },
    {
      "epoch": 0.5123577235772357,
      "step": 9453,
      "training_loss": 6.458561897277832
    },
    {
      "epoch": 0.5124119241192412,
      "step": 9454,
      "training_loss": 6.973254680633545
    },
    {
      "epoch": 0.5124661246612466,
      "step": 9455,
      "training_loss": 6.849236488342285
    },
    {
      "epoch": 0.5125203252032521,
      "grad_norm": 26.45845603942871,
      "learning_rate": 1e-05,
      "loss": 6.7007,
      "step": 9456
    },
    {
      "epoch": 0.5125203252032521,
      "step": 9456,
      "training_loss": 5.489564895629883
    },
    {
      "epoch": 0.5125745257452574,
      "step": 9457,
      "training_loss": 7.037575721740723
    },
    {
      "epoch": 0.5126287262872629,
      "step": 9458,
      "training_loss": 6.7680158615112305
    },
    {
      "epoch": 0.5126829268292683,
      "step": 9459,
      "training_loss": 4.938263893127441
    },
    {
      "epoch": 0.5127371273712737,
      "grad_norm": 22.01645278930664,
      "learning_rate": 1e-05,
      "loss": 6.0584,
      "step": 9460
    },
    {
      "epoch": 0.5127371273712737,
      "step": 9460,
      "training_loss": 7.107806205749512
    },
    {
      "epoch": 0.5127913279132791,
      "step": 9461,
      "training_loss": 7.156266212463379
    },
    {
      "epoch": 0.5128455284552845,
      "step": 9462,
      "training_loss": 6.758171081542969
    },
    {
      "epoch": 0.51289972899729,
      "step": 9463,
      "training_loss": 7.133074760437012
    },
    {
      "epoch": 0.5129539295392954,
      "grad_norm": 23.483701705932617,
      "learning_rate": 1e-05,
      "loss": 7.0388,
      "step": 9464
    },
    {
      "epoch": 0.5129539295392954,
      "step": 9464,
      "training_loss": 6.875300407409668
    },
    {
      "epoch": 0.5130081300813009,
      "step": 9465,
      "training_loss": 6.552558898925781
    },
    {
      "epoch": 0.5130623306233062,
      "step": 9466,
      "training_loss": 6.695173263549805
    },
    {
      "epoch": 0.5131165311653116,
      "step": 9467,
      "training_loss": 7.415503025054932
    },
    {
      "epoch": 0.5131707317073171,
      "grad_norm": 39.57554244995117,
      "learning_rate": 1e-05,
      "loss": 6.8846,
      "step": 9468
    },
    {
      "epoch": 0.5131707317073171,
      "step": 9468,
      "training_loss": 5.662585735321045
    },
    {
      "epoch": 0.5132249322493225,
      "step": 9469,
      "training_loss": 3.76889967918396
    },
    {
      "epoch": 0.5132791327913279,
      "step": 9470,
      "training_loss": 6.607550144195557
    },
    {
      "epoch": 0.5133333333333333,
      "step": 9471,
      "training_loss": 7.1940412521362305
    },
    {
      "epoch": 0.5133875338753388,
      "grad_norm": 20.17822265625,
      "learning_rate": 1e-05,
      "loss": 5.8083,
      "step": 9472
    },
    {
      "epoch": 0.5133875338753388,
      "step": 9472,
      "training_loss": 4.771574020385742
    },
    {
      "epoch": 0.5134417344173442,
      "step": 9473,
      "training_loss": 6.561086654663086
    },
    {
      "epoch": 0.5134959349593496,
      "step": 9474,
      "training_loss": 8.23315715789795
    },
    {
      "epoch": 0.513550135501355,
      "step": 9475,
      "training_loss": 8.050536155700684
    },
    {
      "epoch": 0.5136043360433604,
      "grad_norm": 28.78531837463379,
      "learning_rate": 1e-05,
      "loss": 6.9041,
      "step": 9476
    },
    {
      "epoch": 0.5136043360433604,
      "step": 9476,
      "training_loss": 4.24435567855835
    },
    {
      "epoch": 0.5136585365853659,
      "step": 9477,
      "training_loss": 4.323211193084717
    },
    {
      "epoch": 0.5137127371273713,
      "step": 9478,
      "training_loss": 7.024258136749268
    },
    {
      "epoch": 0.5137669376693766,
      "step": 9479,
      "training_loss": 7.261996746063232
    },
    {
      "epoch": 0.5138211382113821,
      "grad_norm": 29.264427185058594,
      "learning_rate": 1e-05,
      "loss": 5.7135,
      "step": 9480
    },
    {
      "epoch": 0.5138211382113821,
      "step": 9480,
      "training_loss": 5.780209541320801
    },
    {
      "epoch": 0.5138753387533875,
      "step": 9481,
      "training_loss": 5.989528656005859
    },
    {
      "epoch": 0.513929539295393,
      "step": 9482,
      "training_loss": 7.4964399337768555
    },
    {
      "epoch": 0.5139837398373984,
      "step": 9483,
      "training_loss": 7.240569591522217
    },
    {
      "epoch": 0.5140379403794038,
      "grad_norm": 27.297069549560547,
      "learning_rate": 1e-05,
      "loss": 6.6267,
      "step": 9484
    },
    {
      "epoch": 0.5140379403794038,
      "step": 9484,
      "training_loss": 6.39105224609375
    },
    {
      "epoch": 0.5140921409214092,
      "step": 9485,
      "training_loss": 7.382957935333252
    },
    {
      "epoch": 0.5141463414634146,
      "step": 9486,
      "training_loss": 6.044888973236084
    },
    {
      "epoch": 0.5142005420054201,
      "step": 9487,
      "training_loss": 7.064299583435059
    },
    {
      "epoch": 0.5142547425474254,
      "grad_norm": 18.486177444458008,
      "learning_rate": 1e-05,
      "loss": 6.7208,
      "step": 9488
    },
    {
      "epoch": 0.5142547425474254,
      "step": 9488,
      "training_loss": 3.8143200874328613
    },
    {
      "epoch": 0.5143089430894309,
      "step": 9489,
      "training_loss": 7.006397724151611
    },
    {
      "epoch": 0.5143631436314363,
      "step": 9490,
      "training_loss": 6.006895542144775
    },
    {
      "epoch": 0.5144173441734418,
      "step": 9491,
      "training_loss": 6.530871391296387
    },
    {
      "epoch": 0.5144715447154472,
      "grad_norm": 29.28832244873047,
      "learning_rate": 1e-05,
      "loss": 5.8396,
      "step": 9492
    },
    {
      "epoch": 0.5144715447154472,
      "step": 9492,
      "training_loss": 6.897173881530762
    },
    {
      "epoch": 0.5145257452574525,
      "step": 9493,
      "training_loss": 6.640283107757568
    },
    {
      "epoch": 0.514579945799458,
      "step": 9494,
      "training_loss": 6.329916477203369
    },
    {
      "epoch": 0.5146341463414634,
      "step": 9495,
      "training_loss": 6.6031036376953125
    },
    {
      "epoch": 0.5146883468834689,
      "grad_norm": 35.833770751953125,
      "learning_rate": 1e-05,
      "loss": 6.6176,
      "step": 9496
    },
    {
      "epoch": 0.5146883468834689,
      "step": 9496,
      "training_loss": 9.456435203552246
    },
    {
      "epoch": 0.5147425474254742,
      "step": 9497,
      "training_loss": 3.6830384731292725
    },
    {
      "epoch": 0.5147967479674797,
      "step": 9498,
      "training_loss": 4.376260280609131
    },
    {
      "epoch": 0.5148509485094851,
      "step": 9499,
      "training_loss": 6.693746566772461
    },
    {
      "epoch": 0.5149051490514905,
      "grad_norm": 20.414159774780273,
      "learning_rate": 1e-05,
      "loss": 6.0524,
      "step": 9500
    },
    {
      "epoch": 0.5149051490514905,
      "step": 9500,
      "training_loss": 7.095373153686523
    },
    {
      "epoch": 0.514959349593496,
      "step": 9501,
      "training_loss": 6.960213661193848
    },
    {
      "epoch": 0.5150135501355013,
      "step": 9502,
      "training_loss": 6.820555210113525
    },
    {
      "epoch": 0.5150677506775068,
      "step": 9503,
      "training_loss": 7.382510662078857
    },
    {
      "epoch": 0.5151219512195122,
      "grad_norm": 27.323131561279297,
      "learning_rate": 1e-05,
      "loss": 7.0647,
      "step": 9504
    },
    {
      "epoch": 0.5151219512195122,
      "step": 9504,
      "training_loss": 5.765367031097412
    },
    {
      "epoch": 0.5151761517615177,
      "step": 9505,
      "training_loss": 6.856478214263916
    },
    {
      "epoch": 0.515230352303523,
      "step": 9506,
      "training_loss": 3.692814826965332
    },
    {
      "epoch": 0.5152845528455284,
      "step": 9507,
      "training_loss": 7.497641086578369
    },
    {
      "epoch": 0.5153387533875339,
      "grad_norm": 31.94963264465332,
      "learning_rate": 1e-05,
      "loss": 5.9531,
      "step": 9508
    },
    {
      "epoch": 0.5153387533875339,
      "step": 9508,
      "training_loss": 4.534415245056152
    },
    {
      "epoch": 0.5153929539295393,
      "step": 9509,
      "training_loss": 7.087235927581787
    },
    {
      "epoch": 0.5154471544715448,
      "step": 9510,
      "training_loss": 7.372198104858398
    },
    {
      "epoch": 0.5155013550135501,
      "step": 9511,
      "training_loss": 5.571072578430176
    },
    {
      "epoch": 0.5155555555555555,
      "grad_norm": 21.580663681030273,
      "learning_rate": 1e-05,
      "loss": 6.1412,
      "step": 9512
    },
    {
      "epoch": 0.5155555555555555,
      "step": 9512,
      "training_loss": 7.5760498046875
    },
    {
      "epoch": 0.515609756097561,
      "step": 9513,
      "training_loss": 7.972783088684082
    },
    {
      "epoch": 0.5156639566395664,
      "step": 9514,
      "training_loss": 7.472857475280762
    },
    {
      "epoch": 0.5157181571815718,
      "step": 9515,
      "training_loss": 6.352828502655029
    },
    {
      "epoch": 0.5157723577235772,
      "grad_norm": 22.601829528808594,
      "learning_rate": 1e-05,
      "loss": 7.3436,
      "step": 9516
    },
    {
      "epoch": 0.5157723577235772,
      "step": 9516,
      "training_loss": 6.949321746826172
    },
    {
      "epoch": 0.5158265582655827,
      "step": 9517,
      "training_loss": 6.393242359161377
    },
    {
      "epoch": 0.5158807588075881,
      "step": 9518,
      "training_loss": 6.211468696594238
    },
    {
      "epoch": 0.5159349593495935,
      "step": 9519,
      "training_loss": 5.505151748657227
    },
    {
      "epoch": 0.5159891598915989,
      "grad_norm": 28.875953674316406,
      "learning_rate": 1e-05,
      "loss": 6.2648,
      "step": 9520
    },
    {
      "epoch": 0.5159891598915989,
      "step": 9520,
      "training_loss": 7.29638671875
    },
    {
      "epoch": 0.5160433604336043,
      "step": 9521,
      "training_loss": 6.103770732879639
    },
    {
      "epoch": 0.5160975609756098,
      "step": 9522,
      "training_loss": 7.639422416687012
    },
    {
      "epoch": 0.5161517615176152,
      "step": 9523,
      "training_loss": 6.350490570068359
    },
    {
      "epoch": 0.5162059620596205,
      "grad_norm": 53.114994049072266,
      "learning_rate": 1e-05,
      "loss": 6.8475,
      "step": 9524
    },
    {
      "epoch": 0.5162059620596205,
      "step": 9524,
      "training_loss": 8.406533241271973
    },
    {
      "epoch": 0.516260162601626,
      "step": 9525,
      "training_loss": 6.042542457580566
    },
    {
      "epoch": 0.5163143631436314,
      "step": 9526,
      "training_loss": 5.346029758453369
    },
    {
      "epoch": 0.5163685636856369,
      "step": 9527,
      "training_loss": 5.899007320404053
    },
    {
      "epoch": 0.5164227642276423,
      "grad_norm": 43.10411071777344,
      "learning_rate": 1e-05,
      "loss": 6.4235,
      "step": 9528
    },
    {
      "epoch": 0.5164227642276423,
      "step": 9528,
      "training_loss": 4.499537944793701
    },
    {
      "epoch": 0.5164769647696477,
      "step": 9529,
      "training_loss": 7.274683475494385
    },
    {
      "epoch": 0.5165311653116531,
      "step": 9530,
      "training_loss": 7.495638847351074
    },
    {
      "epoch": 0.5165853658536586,
      "step": 9531,
      "training_loss": 6.56251859664917
    },
    {
      "epoch": 0.516639566395664,
      "grad_norm": 22.486217498779297,
      "learning_rate": 1e-05,
      "loss": 6.4581,
      "step": 9532
    },
    {
      "epoch": 0.516639566395664,
      "step": 9532,
      "training_loss": 7.549342632293701
    },
    {
      "epoch": 0.5166937669376693,
      "step": 9533,
      "training_loss": 7.533498764038086
    },
    {
      "epoch": 0.5167479674796748,
      "step": 9534,
      "training_loss": 7.145816802978516
    },
    {
      "epoch": 0.5168021680216802,
      "step": 9535,
      "training_loss": 6.983182430267334
    },
    {
      "epoch": 0.5168563685636857,
      "grad_norm": 25.577091217041016,
      "learning_rate": 1e-05,
      "loss": 7.303,
      "step": 9536
    },
    {
      "epoch": 0.5168563685636857,
      "step": 9536,
      "training_loss": 6.354353427886963
    },
    {
      "epoch": 0.5169105691056911,
      "step": 9537,
      "training_loss": 5.597435474395752
    },
    {
      "epoch": 0.5169647696476964,
      "step": 9538,
      "training_loss": 7.40962028503418
    },
    {
      "epoch": 0.5170189701897019,
      "step": 9539,
      "training_loss": 5.182443618774414
    },
    {
      "epoch": 0.5170731707317073,
      "grad_norm": 25.789791107177734,
      "learning_rate": 1e-05,
      "loss": 6.136,
      "step": 9540
    },
    {
      "epoch": 0.5170731707317073,
      "step": 9540,
      "training_loss": 7.066013813018799
    },
    {
      "epoch": 0.5171273712737128,
      "step": 9541,
      "training_loss": 5.481747627258301
    },
    {
      "epoch": 0.5171815718157181,
      "step": 9542,
      "training_loss": 7.156299114227295
    },
    {
      "epoch": 0.5172357723577236,
      "step": 9543,
      "training_loss": 6.628240585327148
    },
    {
      "epoch": 0.517289972899729,
      "grad_norm": 19.227689743041992,
      "learning_rate": 1e-05,
      "loss": 6.5831,
      "step": 9544
    },
    {
      "epoch": 0.517289972899729,
      "step": 9544,
      "training_loss": 5.819274425506592
    },
    {
      "epoch": 0.5173441734417344,
      "step": 9545,
      "training_loss": 7.508223056793213
    },
    {
      "epoch": 0.5173983739837399,
      "step": 9546,
      "training_loss": 5.6643290519714355
    },
    {
      "epoch": 0.5174525745257452,
      "step": 9547,
      "training_loss": 7.488202095031738
    },
    {
      "epoch": 0.5175067750677507,
      "grad_norm": 24.096357345581055,
      "learning_rate": 1e-05,
      "loss": 6.62,
      "step": 9548
    },
    {
      "epoch": 0.5175067750677507,
      "step": 9548,
      "training_loss": 4.595670700073242
    },
    {
      "epoch": 0.5175609756097561,
      "step": 9549,
      "training_loss": 6.47482442855835
    },
    {
      "epoch": 0.5176151761517616,
      "step": 9550,
      "training_loss": 6.4585676193237305
    },
    {
      "epoch": 0.5176693766937669,
      "step": 9551,
      "training_loss": 5.926566123962402
    },
    {
      "epoch": 0.5177235772357723,
      "grad_norm": 27.6567325592041,
      "learning_rate": 1e-05,
      "loss": 5.8639,
      "step": 9552
    },
    {
      "epoch": 0.5177235772357723,
      "step": 9552,
      "training_loss": 8.11286735534668
    },
    {
      "epoch": 0.5177777777777778,
      "step": 9553,
      "training_loss": 7.22021484375
    },
    {
      "epoch": 0.5178319783197832,
      "step": 9554,
      "training_loss": 8.530810356140137
    },
    {
      "epoch": 0.5178861788617887,
      "step": 9555,
      "training_loss": 8.01332950592041
    },
    {
      "epoch": 0.517940379403794,
      "grad_norm": 31.49946403503418,
      "learning_rate": 1e-05,
      "loss": 7.9693,
      "step": 9556
    },
    {
      "epoch": 0.517940379403794,
      "step": 9556,
      "training_loss": 6.397794723510742
    },
    {
      "epoch": 0.5179945799457994,
      "step": 9557,
      "training_loss": 6.019030570983887
    },
    {
      "epoch": 0.5180487804878049,
      "step": 9558,
      "training_loss": 5.41861629486084
    },
    {
      "epoch": 0.5181029810298103,
      "step": 9559,
      "training_loss": 4.300378322601318
    },
    {
      "epoch": 0.5181571815718157,
      "grad_norm": 30.522510528564453,
      "learning_rate": 1e-05,
      "loss": 5.534,
      "step": 9560
    },
    {
      "epoch": 0.5181571815718157,
      "step": 9560,
      "training_loss": 5.141674041748047
    },
    {
      "epoch": 0.5182113821138211,
      "step": 9561,
      "training_loss": 6.686334609985352
    },
    {
      "epoch": 0.5182655826558266,
      "step": 9562,
      "training_loss": 5.648861408233643
    },
    {
      "epoch": 0.518319783197832,
      "step": 9563,
      "training_loss": 7.442631244659424
    },
    {
      "epoch": 0.5183739837398375,
      "grad_norm": 32.27928924560547,
      "learning_rate": 1e-05,
      "loss": 6.2299,
      "step": 9564
    },
    {
      "epoch": 0.5183739837398375,
      "step": 9564,
      "training_loss": 8.064446449279785
    },
    {
      "epoch": 0.5184281842818428,
      "step": 9565,
      "training_loss": 8.043489456176758
    },
    {
      "epoch": 0.5184823848238482,
      "step": 9566,
      "training_loss": 7.73532772064209
    },
    {
      "epoch": 0.5185365853658537,
      "step": 9567,
      "training_loss": 6.7775373458862305
    },
    {
      "epoch": 0.5185907859078591,
      "grad_norm": 39.60411071777344,
      "learning_rate": 1e-05,
      "loss": 7.6552,
      "step": 9568
    },
    {
      "epoch": 0.5185907859078591,
      "step": 9568,
      "training_loss": 5.9825119972229
    },
    {
      "epoch": 0.5186449864498645,
      "step": 9569,
      "training_loss": 3.666909694671631
    },
    {
      "epoch": 0.5186991869918699,
      "step": 9570,
      "training_loss": 6.62580680847168
    },
    {
      "epoch": 0.5187533875338753,
      "step": 9571,
      "training_loss": 3.3958096504211426
    },
    {
      "epoch": 0.5188075880758808,
      "grad_norm": 31.188283920288086,
      "learning_rate": 1e-05,
      "loss": 4.9178,
      "step": 9572
    },
    {
      "epoch": 0.5188075880758808,
      "step": 9572,
      "training_loss": 6.954967498779297
    },
    {
      "epoch": 0.5188617886178862,
      "step": 9573,
      "training_loss": 7.771423816680908
    },
    {
      "epoch": 0.5189159891598916,
      "step": 9574,
      "training_loss": 6.8738250732421875
    },
    {
      "epoch": 0.518970189701897,
      "step": 9575,
      "training_loss": 6.395634174346924
    },
    {
      "epoch": 0.5190243902439025,
      "grad_norm": 27.62102699279785,
      "learning_rate": 1e-05,
      "loss": 6.999,
      "step": 9576
    },
    {
      "epoch": 0.5190243902439025,
      "step": 9576,
      "training_loss": 7.419196128845215
    },
    {
      "epoch": 0.5190785907859079,
      "step": 9577,
      "training_loss": 7.099149227142334
    },
    {
      "epoch": 0.5191327913279132,
      "step": 9578,
      "training_loss": 6.668705463409424
    },
    {
      "epoch": 0.5191869918699187,
      "step": 9579,
      "training_loss": 6.182611465454102
    },
    {
      "epoch": 0.5192411924119241,
      "grad_norm": 27.755712509155273,
      "learning_rate": 1e-05,
      "loss": 6.8424,
      "step": 9580
    },
    {
      "epoch": 0.5192411924119241,
      "step": 9580,
      "training_loss": 6.632083892822266
    },
    {
      "epoch": 0.5192953929539296,
      "step": 9581,
      "training_loss": 6.170544147491455
    },
    {
      "epoch": 0.519349593495935,
      "step": 9582,
      "training_loss": 7.04189395904541
    },
    {
      "epoch": 0.5194037940379403,
      "step": 9583,
      "training_loss": 6.3949456214904785
    },
    {
      "epoch": 0.5194579945799458,
      "grad_norm": 37.065025329589844,
      "learning_rate": 1e-05,
      "loss": 6.5599,
      "step": 9584
    },
    {
      "epoch": 0.5194579945799458,
      "step": 9584,
      "training_loss": 5.372675895690918
    },
    {
      "epoch": 0.5195121951219512,
      "step": 9585,
      "training_loss": 7.056602478027344
    },
    {
      "epoch": 0.5195663956639567,
      "step": 9586,
      "training_loss": 7.631365776062012
    },
    {
      "epoch": 0.519620596205962,
      "step": 9587,
      "training_loss": 6.429041862487793
    },
    {
      "epoch": 0.5196747967479675,
      "grad_norm": 28.535446166992188,
      "learning_rate": 1e-05,
      "loss": 6.6224,
      "step": 9588
    },
    {
      "epoch": 0.5196747967479675,
      "step": 9588,
      "training_loss": 6.265780925750732
    },
    {
      "epoch": 0.5197289972899729,
      "step": 9589,
      "training_loss": 6.8834991455078125
    },
    {
      "epoch": 0.5197831978319783,
      "step": 9590,
      "training_loss": 8.202510833740234
    },
    {
      "epoch": 0.5198373983739838,
      "step": 9591,
      "training_loss": 7.0856475830078125
    },
    {
      "epoch": 0.5198915989159891,
      "grad_norm": 16.67130470275879,
      "learning_rate": 1e-05,
      "loss": 7.1094,
      "step": 9592
    },
    {
      "epoch": 0.5198915989159891,
      "step": 9592,
      "training_loss": 6.087097644805908
    },
    {
      "epoch": 0.5199457994579946,
      "step": 9593,
      "training_loss": 7.215664386749268
    },
    {
      "epoch": 0.52,
      "step": 9594,
      "training_loss": 5.42456579208374
    },
    {
      "epoch": 0.5200542005420055,
      "step": 9595,
      "training_loss": 6.9215593338012695
    },
    {
      "epoch": 0.5201084010840108,
      "grad_norm": 33.887535095214844,
      "learning_rate": 1e-05,
      "loss": 6.4122,
      "step": 9596
    },
    {
      "epoch": 0.5201084010840108,
      "step": 9596,
      "training_loss": 8.394469261169434
    },
    {
      "epoch": 0.5201626016260162,
      "step": 9597,
      "training_loss": 4.7310919761657715
    },
    {
      "epoch": 0.5202168021680217,
      "step": 9598,
      "training_loss": 7.701591491699219
    },
    {
      "epoch": 0.5202710027100271,
      "step": 9599,
      "training_loss": 6.983083724975586
    },
    {
      "epoch": 0.5203252032520326,
      "grad_norm": 27.470186233520508,
      "learning_rate": 1e-05,
      "loss": 6.9526,
      "step": 9600
    },
    {
      "epoch": 0.5203252032520326,
      "step": 9600,
      "training_loss": 3.4848012924194336
    },
    {
      "epoch": 0.5203794037940379,
      "step": 9601,
      "training_loss": 6.586922645568848
    },
    {
      "epoch": 0.5204336043360434,
      "step": 9602,
      "training_loss": 9.147187232971191
    },
    {
      "epoch": 0.5204878048780488,
      "step": 9603,
      "training_loss": 3.03908634185791
    },
    {
      "epoch": 0.5205420054200542,
      "grad_norm": 32.49177551269531,
      "learning_rate": 1e-05,
      "loss": 5.5645,
      "step": 9604
    },
    {
      "epoch": 0.5205420054200542,
      "step": 9604,
      "training_loss": 7.006771564483643
    },
    {
      "epoch": 0.5205962059620596,
      "step": 9605,
      "training_loss": 5.991774559020996
    },
    {
      "epoch": 0.520650406504065,
      "step": 9606,
      "training_loss": 6.844423294067383
    },
    {
      "epoch": 0.5207046070460705,
      "step": 9607,
      "training_loss": 6.780031204223633
    },
    {
      "epoch": 0.5207588075880759,
      "grad_norm": 25.39093017578125,
      "learning_rate": 1e-05,
      "loss": 6.6558,
      "step": 9608
    },
    {
      "epoch": 0.5207588075880759,
      "step": 9608,
      "training_loss": 7.920829772949219
    },
    {
      "epoch": 0.5208130081300814,
      "step": 9609,
      "training_loss": 6.904693603515625
    },
    {
      "epoch": 0.5208672086720867,
      "step": 9610,
      "training_loss": 6.594028949737549
    },
    {
      "epoch": 0.5209214092140921,
      "step": 9611,
      "training_loss": 8.180171966552734
    },
    {
      "epoch": 0.5209756097560976,
      "grad_norm": 22.603078842163086,
      "learning_rate": 1e-05,
      "loss": 7.3999,
      "step": 9612
    },
    {
      "epoch": 0.5209756097560976,
      "step": 9612,
      "training_loss": 7.204494953155518
    },
    {
      "epoch": 0.521029810298103,
      "step": 9613,
      "training_loss": 4.3880462646484375
    },
    {
      "epoch": 0.5210840108401084,
      "step": 9614,
      "training_loss": 7.846563339233398
    },
    {
      "epoch": 0.5211382113821138,
      "step": 9615,
      "training_loss": 2.868795394897461
    },
    {
      "epoch": 0.5211924119241192,
      "grad_norm": 30.956119537353516,
      "learning_rate": 1e-05,
      "loss": 5.577,
      "step": 9616
    },
    {
      "epoch": 0.5211924119241192,
      "step": 9616,
      "training_loss": 7.776705265045166
    },
    {
      "epoch": 0.5212466124661247,
      "step": 9617,
      "training_loss": 7.379814624786377
    },
    {
      "epoch": 0.52130081300813,
      "step": 9618,
      "training_loss": 4.246032238006592
    },
    {
      "epoch": 0.5213550135501355,
      "step": 9619,
      "training_loss": 5.720118999481201
    },
    {
      "epoch": 0.5214092140921409,
      "grad_norm": 63.882389068603516,
      "learning_rate": 1e-05,
      "loss": 6.2807,
      "step": 9620
    },
    {
      "epoch": 0.5214092140921409,
      "step": 9620,
      "training_loss": 5.8913960456848145
    },
    {
      "epoch": 0.5214634146341464,
      "step": 9621,
      "training_loss": 6.697641372680664
    },
    {
      "epoch": 0.5215176151761518,
      "step": 9622,
      "training_loss": 7.028530597686768
    },
    {
      "epoch": 0.5215718157181571,
      "step": 9623,
      "training_loss": 6.734933376312256
    },
    {
      "epoch": 0.5216260162601626,
      "grad_norm": 24.382793426513672,
      "learning_rate": 1e-05,
      "loss": 6.5881,
      "step": 9624
    },
    {
      "epoch": 0.5216260162601626,
      "step": 9624,
      "training_loss": 6.387518882751465
    },
    {
      "epoch": 0.521680216802168,
      "step": 9625,
      "training_loss": 6.314081192016602
    },
    {
      "epoch": 0.5217344173441735,
      "step": 9626,
      "training_loss": 6.17048454284668
    },
    {
      "epoch": 0.5217886178861788,
      "step": 9627,
      "training_loss": 6.722458839416504
    },
    {
      "epoch": 0.5218428184281843,
      "grad_norm": 29.06219482421875,
      "learning_rate": 1e-05,
      "loss": 6.3986,
      "step": 9628
    },
    {
      "epoch": 0.5218428184281843,
      "step": 9628,
      "training_loss": 6.889704704284668
    },
    {
      "epoch": 0.5218970189701897,
      "step": 9629,
      "training_loss": 2.5911002159118652
    },
    {
      "epoch": 0.5219512195121951,
      "step": 9630,
      "training_loss": 4.760660648345947
    },
    {
      "epoch": 0.5220054200542006,
      "step": 9631,
      "training_loss": 5.509957790374756
    },
    {
      "epoch": 0.5220596205962059,
      "grad_norm": 31.314525604248047,
      "learning_rate": 1e-05,
      "loss": 4.9379,
      "step": 9632
    },
    {
      "epoch": 0.5220596205962059,
      "step": 9632,
      "training_loss": 6.636502742767334
    },
    {
      "epoch": 0.5221138211382114,
      "step": 9633,
      "training_loss": 6.586602210998535
    },
    {
      "epoch": 0.5221680216802168,
      "step": 9634,
      "training_loss": 7.2720561027526855
    },
    {
      "epoch": 0.5222222222222223,
      "step": 9635,
      "training_loss": 6.866654872894287
    },
    {
      "epoch": 0.5222764227642276,
      "grad_norm": 21.112987518310547,
      "learning_rate": 1e-05,
      "loss": 6.8405,
      "step": 9636
    },
    {
      "epoch": 0.5222764227642276,
      "step": 9636,
      "training_loss": 6.83845329284668
    },
    {
      "epoch": 0.522330623306233,
      "step": 9637,
      "training_loss": 7.1938796043396
    },
    {
      "epoch": 0.5223848238482385,
      "step": 9638,
      "training_loss": 5.256102085113525
    },
    {
      "epoch": 0.5224390243902439,
      "step": 9639,
      "training_loss": 6.570268154144287
    },
    {
      "epoch": 0.5224932249322494,
      "grad_norm": 30.790847778320312,
      "learning_rate": 1e-05,
      "loss": 6.4647,
      "step": 9640
    },
    {
      "epoch": 0.5224932249322494,
      "step": 9640,
      "training_loss": 5.579808235168457
    },
    {
      "epoch": 0.5225474254742547,
      "step": 9641,
      "training_loss": 4.80472993850708
    },
    {
      "epoch": 0.5226016260162601,
      "step": 9642,
      "training_loss": 7.458771228790283
    },
    {
      "epoch": 0.5226558265582656,
      "step": 9643,
      "training_loss": 3.927602767944336
    },
    {
      "epoch": 0.522710027100271,
      "grad_norm": 40.53163146972656,
      "learning_rate": 1e-05,
      "loss": 5.4427,
      "step": 9644
    },
    {
      "epoch": 0.522710027100271,
      "step": 9644,
      "training_loss": 6.3685526847839355
    },
    {
      "epoch": 0.5227642276422764,
      "step": 9645,
      "training_loss": 8.113601684570312
    },
    {
      "epoch": 0.5228184281842818,
      "step": 9646,
      "training_loss": 7.103826522827148
    },
    {
      "epoch": 0.5228726287262873,
      "step": 9647,
      "training_loss": 5.38153600692749
    },
    {
      "epoch": 0.5229268292682927,
      "grad_norm": 67.52688598632812,
      "learning_rate": 1e-05,
      "loss": 6.7419,
      "step": 9648
    },
    {
      "epoch": 0.5229268292682927,
      "step": 9648,
      "training_loss": 6.582376956939697
    },
    {
      "epoch": 0.5229810298102981,
      "step": 9649,
      "training_loss": 6.586444854736328
    },
    {
      "epoch": 0.5230352303523035,
      "step": 9650,
      "training_loss": 6.427563190460205
    },
    {
      "epoch": 0.5230894308943089,
      "step": 9651,
      "training_loss": 7.578298568725586
    },
    {
      "epoch": 0.5231436314363144,
      "grad_norm": 26.01799964904785,
      "learning_rate": 1e-05,
      "loss": 6.7937,
      "step": 9652
    },
    {
      "epoch": 0.5231436314363144,
      "step": 9652,
      "training_loss": 6.927414894104004
    },
    {
      "epoch": 0.5231978319783198,
      "step": 9653,
      "training_loss": 6.890048027038574
    },
    {
      "epoch": 0.5232520325203251,
      "step": 9654,
      "training_loss": 5.574130535125732
    },
    {
      "epoch": 0.5233062330623306,
      "step": 9655,
      "training_loss": 6.705051898956299
    },
    {
      "epoch": 0.523360433604336,
      "grad_norm": 26.894975662231445,
      "learning_rate": 1e-05,
      "loss": 6.5242,
      "step": 9656
    },
    {
      "epoch": 0.523360433604336,
      "step": 9656,
      "training_loss": 6.418520927429199
    },
    {
      "epoch": 0.5234146341463415,
      "step": 9657,
      "training_loss": 7.832317352294922
    },
    {
      "epoch": 0.5234688346883469,
      "step": 9658,
      "training_loss": 6.84884786605835
    },
    {
      "epoch": 0.5235230352303523,
      "step": 9659,
      "training_loss": 5.470003128051758
    },
    {
      "epoch": 0.5235772357723577,
      "grad_norm": 30.486373901367188,
      "learning_rate": 1e-05,
      "loss": 6.6424,
      "step": 9660
    },
    {
      "epoch": 0.5235772357723577,
      "step": 9660,
      "training_loss": 5.983697414398193
    },
    {
      "epoch": 0.5236314363143632,
      "step": 9661,
      "training_loss": 5.778126239776611
    },
    {
      "epoch": 0.5236856368563686,
      "step": 9662,
      "training_loss": 6.304009437561035
    },
    {
      "epoch": 0.5237398373983739,
      "step": 9663,
      "training_loss": 6.833927154541016
    },
    {
      "epoch": 0.5237940379403794,
      "grad_norm": 27.157337188720703,
      "learning_rate": 1e-05,
      "loss": 6.2249,
      "step": 9664
    },
    {
      "epoch": 0.5237940379403794,
      "step": 9664,
      "training_loss": 7.100186347961426
    },
    {
      "epoch": 0.5238482384823848,
      "step": 9665,
      "training_loss": 5.85746431350708
    },
    {
      "epoch": 0.5239024390243903,
      "step": 9666,
      "training_loss": 6.146460056304932
    },
    {
      "epoch": 0.5239566395663957,
      "step": 9667,
      "training_loss": 6.722309112548828
    },
    {
      "epoch": 0.524010840108401,
      "grad_norm": 21.341167449951172,
      "learning_rate": 1e-05,
      "loss": 6.4566,
      "step": 9668
    },
    {
      "epoch": 0.524010840108401,
      "step": 9668,
      "training_loss": 6.05898904800415
    },
    {
      "epoch": 0.5240650406504065,
      "step": 9669,
      "training_loss": 5.439909934997559
    },
    {
      "epoch": 0.5241192411924119,
      "step": 9670,
      "training_loss": 7.274693965911865
    },
    {
      "epoch": 0.5241734417344174,
      "step": 9671,
      "training_loss": 6.688558578491211
    },
    {
      "epoch": 0.5242276422764227,
      "grad_norm": 44.50340270996094,
      "learning_rate": 1e-05,
      "loss": 6.3655,
      "step": 9672
    },
    {
      "epoch": 0.5242276422764227,
      "step": 9672,
      "training_loss": 5.974871635437012
    },
    {
      "epoch": 0.5242818428184282,
      "step": 9673,
      "training_loss": 7.014312744140625
    },
    {
      "epoch": 0.5243360433604336,
      "step": 9674,
      "training_loss": 6.901167392730713
    },
    {
      "epoch": 0.524390243902439,
      "step": 9675,
      "training_loss": 5.453174114227295
    },
    {
      "epoch": 0.5244444444444445,
      "grad_norm": 32.079532623291016,
      "learning_rate": 1e-05,
      "loss": 6.3359,
      "step": 9676
    },
    {
      "epoch": 0.5244444444444445,
      "step": 9676,
      "training_loss": 6.05565881729126
    },
    {
      "epoch": 0.5244986449864498,
      "step": 9677,
      "training_loss": 6.635852813720703
    },
    {
      "epoch": 0.5245528455284553,
      "step": 9678,
      "training_loss": 4.932492256164551
    },
    {
      "epoch": 0.5246070460704607,
      "step": 9679,
      "training_loss": 6.617843151092529
    },
    {
      "epoch": 0.5246612466124662,
      "grad_norm": 46.430320739746094,
      "learning_rate": 1e-05,
      "loss": 6.0605,
      "step": 9680
    },
    {
      "epoch": 0.5246612466124662,
      "step": 9680,
      "training_loss": 6.877277851104736
    },
    {
      "epoch": 0.5247154471544715,
      "step": 9681,
      "training_loss": 5.003950595855713
    },
    {
      "epoch": 0.5247696476964769,
      "step": 9682,
      "training_loss": 5.407961845397949
    },
    {
      "epoch": 0.5248238482384824,
      "step": 9683,
      "training_loss": 7.6607842445373535
    },
    {
      "epoch": 0.5248780487804878,
      "grad_norm": 33.99900436401367,
      "learning_rate": 1e-05,
      "loss": 6.2375,
      "step": 9684
    },
    {
      "epoch": 0.5248780487804878,
      "step": 9684,
      "training_loss": 5.89048433303833
    },
    {
      "epoch": 0.5249322493224933,
      "step": 9685,
      "training_loss": 5.228835582733154
    },
    {
      "epoch": 0.5249864498644986,
      "step": 9686,
      "training_loss": 6.33176851272583
    },
    {
      "epoch": 0.525040650406504,
      "step": 9687,
      "training_loss": 6.454526424407959
    },
    {
      "epoch": 0.5250948509485095,
      "grad_norm": 25.195188522338867,
      "learning_rate": 1e-05,
      "loss": 5.9764,
      "step": 9688
    },
    {
      "epoch": 0.5250948509485095,
      "step": 9688,
      "training_loss": 6.679466724395752
    },
    {
      "epoch": 0.5251490514905149,
      "step": 9689,
      "training_loss": 6.349140167236328
    },
    {
      "epoch": 0.5252032520325203,
      "step": 9690,
      "training_loss": 5.725728988647461
    },
    {
      "epoch": 0.5252574525745257,
      "step": 9691,
      "training_loss": 6.940592288970947
    },
    {
      "epoch": 0.5253116531165312,
      "grad_norm": 31.50779914855957,
      "learning_rate": 1e-05,
      "loss": 6.4237,
      "step": 9692
    },
    {
      "epoch": 0.5253116531165312,
      "step": 9692,
      "training_loss": 5.845620632171631
    },
    {
      "epoch": 0.5253658536585366,
      "step": 9693,
      "training_loss": 8.164358139038086
    },
    {
      "epoch": 0.525420054200542,
      "step": 9694,
      "training_loss": 7.1963372230529785
    },
    {
      "epoch": 0.5254742547425474,
      "step": 9695,
      "training_loss": 6.760117053985596
    },
    {
      "epoch": 0.5255284552845528,
      "grad_norm": 30.486108779907227,
      "learning_rate": 1e-05,
      "loss": 6.9916,
      "step": 9696
    },
    {
      "epoch": 0.5255284552845528,
      "step": 9696,
      "training_loss": 6.008039951324463
    },
    {
      "epoch": 0.5255826558265583,
      "step": 9697,
      "training_loss": 6.178077697753906
    },
    {
      "epoch": 0.5256368563685637,
      "step": 9698,
      "training_loss": 6.614447116851807
    },
    {
      "epoch": 0.525691056910569,
      "step": 9699,
      "training_loss": 7.07179069519043
    },
    {
      "epoch": 0.5257452574525745,
      "grad_norm": 44.29544448852539,
      "learning_rate": 1e-05,
      "loss": 6.4681,
      "step": 9700
    },
    {
      "epoch": 0.5257452574525745,
      "step": 9700,
      "training_loss": 7.418595314025879
    },
    {
      "epoch": 0.5257994579945799,
      "step": 9701,
      "training_loss": 7.151765823364258
    },
    {
      "epoch": 0.5258536585365854,
      "step": 9702,
      "training_loss": 7.136828422546387
    },
    {
      "epoch": 0.5259078590785908,
      "step": 9703,
      "training_loss": 5.085784912109375
    },
    {
      "epoch": 0.5259620596205962,
      "grad_norm": 20.269027709960938,
      "learning_rate": 1e-05,
      "loss": 6.6982,
      "step": 9704
    },
    {
      "epoch": 0.5259620596205962,
      "step": 9704,
      "training_loss": 7.6161980628967285
    },
    {
      "epoch": 0.5260162601626016,
      "step": 9705,
      "training_loss": 4.164944171905518
    },
    {
      "epoch": 0.5260704607046071,
      "step": 9706,
      "training_loss": 6.020112991333008
    },
    {
      "epoch": 0.5261246612466125,
      "step": 9707,
      "training_loss": 5.833414554595947
    },
    {
      "epoch": 0.5261788617886178,
      "grad_norm": 41.543365478515625,
      "learning_rate": 1e-05,
      "loss": 5.9087,
      "step": 9708
    },
    {
      "epoch": 0.5261788617886178,
      "step": 9708,
      "training_loss": 5.291521072387695
    },
    {
      "epoch": 0.5262330623306233,
      "step": 9709,
      "training_loss": 6.4018754959106445
    },
    {
      "epoch": 0.5262872628726287,
      "step": 9710,
      "training_loss": 6.701310634613037
    },
    {
      "epoch": 0.5263414634146342,
      "step": 9711,
      "training_loss": 6.609109878540039
    },
    {
      "epoch": 0.5263956639566396,
      "grad_norm": 17.47312355041504,
      "learning_rate": 1e-05,
      "loss": 6.251,
      "step": 9712
    },
    {
      "epoch": 0.5263956639566396,
      "step": 9712,
      "training_loss": 6.7333807945251465
    },
    {
      "epoch": 0.526449864498645,
      "step": 9713,
      "training_loss": 7.494507312774658
    },
    {
      "epoch": 0.5265040650406504,
      "step": 9714,
      "training_loss": 7.139697074890137
    },
    {
      "epoch": 0.5265582655826558,
      "step": 9715,
      "training_loss": 5.9849348068237305
    },
    {
      "epoch": 0.5266124661246613,
      "grad_norm": 35.972591400146484,
      "learning_rate": 1e-05,
      "loss": 6.8381,
      "step": 9716
    },
    {
      "epoch": 0.5266124661246613,
      "step": 9716,
      "training_loss": 7.232282638549805
    },
    {
      "epoch": 0.5266666666666666,
      "step": 9717,
      "training_loss": 6.660080432891846
    },
    {
      "epoch": 0.5267208672086721,
      "step": 9718,
      "training_loss": 5.686517238616943
    },
    {
      "epoch": 0.5267750677506775,
      "step": 9719,
      "training_loss": 5.274191856384277
    },
    {
      "epoch": 0.526829268292683,
      "grad_norm": 21.49020004272461,
      "learning_rate": 1e-05,
      "loss": 6.2133,
      "step": 9720
    },
    {
      "epoch": 0.526829268292683,
      "step": 9720,
      "training_loss": 3.7950568199157715
    },
    {
      "epoch": 0.5268834688346884,
      "step": 9721,
      "training_loss": 6.681818008422852
    },
    {
      "epoch": 0.5269376693766937,
      "step": 9722,
      "training_loss": 7.523965358734131
    },
    {
      "epoch": 0.5269918699186992,
      "step": 9723,
      "training_loss": 5.245815753936768
    },
    {
      "epoch": 0.5270460704607046,
      "grad_norm": 23.577775955200195,
      "learning_rate": 1e-05,
      "loss": 5.8117,
      "step": 9724
    },
    {
      "epoch": 0.5270460704607046,
      "step": 9724,
      "training_loss": 6.516645908355713
    },
    {
      "epoch": 0.5271002710027101,
      "step": 9725,
      "training_loss": 7.303182125091553
    },
    {
      "epoch": 0.5271544715447154,
      "step": 9726,
      "training_loss": 6.558983325958252
    },
    {
      "epoch": 0.5272086720867208,
      "step": 9727,
      "training_loss": 5.911112308502197
    },
    {
      "epoch": 0.5272628726287263,
      "grad_norm": 16.086668014526367,
      "learning_rate": 1e-05,
      "loss": 6.5725,
      "step": 9728
    },
    {
      "epoch": 0.5272628726287263,
      "step": 9728,
      "training_loss": 6.034470081329346
    },
    {
      "epoch": 0.5273170731707317,
      "step": 9729,
      "training_loss": 5.053601264953613
    },
    {
      "epoch": 0.5273712737127372,
      "step": 9730,
      "training_loss": 6.832926273345947
    },
    {
      "epoch": 0.5274254742547425,
      "step": 9731,
      "training_loss": 8.81865406036377
    },
    {
      "epoch": 0.527479674796748,
      "grad_norm": 40.678855895996094,
      "learning_rate": 1e-05,
      "loss": 6.6849,
      "step": 9732
    },
    {
      "epoch": 0.527479674796748,
      "step": 9732,
      "training_loss": 4.554589748382568
    },
    {
      "epoch": 0.5275338753387534,
      "step": 9733,
      "training_loss": 6.046540260314941
    },
    {
      "epoch": 0.5275880758807588,
      "step": 9734,
      "training_loss": 8.450616836547852
    },
    {
      "epoch": 0.5276422764227642,
      "step": 9735,
      "training_loss": 7.576934337615967
    },
    {
      "epoch": 0.5276964769647696,
      "grad_norm": 37.34990692138672,
      "learning_rate": 1e-05,
      "loss": 6.6572,
      "step": 9736
    },
    {
      "epoch": 0.5276964769647696,
      "step": 9736,
      "training_loss": 5.4718241691589355
    },
    {
      "epoch": 0.5277506775067751,
      "step": 9737,
      "training_loss": 6.915360927581787
    },
    {
      "epoch": 0.5278048780487805,
      "step": 9738,
      "training_loss": 5.53808069229126
    },
    {
      "epoch": 0.527859078590786,
      "step": 9739,
      "training_loss": 4.89952278137207
    },
    {
      "epoch": 0.5279132791327913,
      "grad_norm": 37.9547233581543,
      "learning_rate": 1e-05,
      "loss": 5.7062,
      "step": 9740
    },
    {
      "epoch": 0.5279132791327913,
      "step": 9740,
      "training_loss": 7.221657752990723
    },
    {
      "epoch": 0.5279674796747967,
      "step": 9741,
      "training_loss": 6.366672992706299
    },
    {
      "epoch": 0.5280216802168022,
      "step": 9742,
      "training_loss": 7.261107921600342
    },
    {
      "epoch": 0.5280758807588076,
      "step": 9743,
      "training_loss": 4.595312118530273
    },
    {
      "epoch": 0.528130081300813,
      "grad_norm": 24.95877456665039,
      "learning_rate": 1e-05,
      "loss": 6.3612,
      "step": 9744
    },
    {
      "epoch": 0.528130081300813,
      "step": 9744,
      "training_loss": 4.557188510894775
    },
    {
      "epoch": 0.5281842818428184,
      "step": 9745,
      "training_loss": 4.928098678588867
    },
    {
      "epoch": 0.5282384823848238,
      "step": 9746,
      "training_loss": 7.5500078201293945
    },
    {
      "epoch": 0.5282926829268293,
      "step": 9747,
      "training_loss": 5.677328109741211
    },
    {
      "epoch": 0.5283468834688347,
      "grad_norm": 39.5737419128418,
      "learning_rate": 1e-05,
      "loss": 5.6782,
      "step": 9748
    },
    {
      "epoch": 0.5283468834688347,
      "step": 9748,
      "training_loss": 6.978540420532227
    },
    {
      "epoch": 0.5284010840108401,
      "step": 9749,
      "training_loss": 6.822380542755127
    },
    {
      "epoch": 0.5284552845528455,
      "step": 9750,
      "training_loss": 6.080850124359131
    },
    {
      "epoch": 0.528509485094851,
      "step": 9751,
      "training_loss": 7.650949478149414
    },
    {
      "epoch": 0.5285636856368564,
      "grad_norm": 27.771467208862305,
      "learning_rate": 1e-05,
      "loss": 6.8832,
      "step": 9752
    },
    {
      "epoch": 0.5285636856368564,
      "step": 9752,
      "training_loss": 6.827139377593994
    },
    {
      "epoch": 0.5286178861788617,
      "step": 9753,
      "training_loss": 7.04806661605835
    },
    {
      "epoch": 0.5286720867208672,
      "step": 9754,
      "training_loss": 7.032912731170654
    },
    {
      "epoch": 0.5287262872628726,
      "step": 9755,
      "training_loss": 6.9453864097595215
    },
    {
      "epoch": 0.5287804878048781,
      "grad_norm": 21.933883666992188,
      "learning_rate": 1e-05,
      "loss": 6.9634,
      "step": 9756
    },
    {
      "epoch": 0.5287804878048781,
      "step": 9756,
      "training_loss": 4.915214538574219
    },
    {
      "epoch": 0.5288346883468835,
      "step": 9757,
      "training_loss": 2.669625759124756
    },
    {
      "epoch": 0.5288888888888889,
      "step": 9758,
      "training_loss": 4.163006782531738
    },
    {
      "epoch": 0.5289430894308943,
      "step": 9759,
      "training_loss": 7.2818169593811035
    },
    {
      "epoch": 0.5289972899728997,
      "grad_norm": 41.63895797729492,
      "learning_rate": 1e-05,
      "loss": 4.7574,
      "step": 9760
    },
    {
      "epoch": 0.5289972899728997,
      "step": 9760,
      "training_loss": 6.5558881759643555
    },
    {
      "epoch": 0.5290514905149052,
      "step": 9761,
      "training_loss": 7.326479434967041
    },
    {
      "epoch": 0.5291056910569105,
      "step": 9762,
      "training_loss": 6.412082195281982
    },
    {
      "epoch": 0.529159891598916,
      "step": 9763,
      "training_loss": 7.675337314605713
    },
    {
      "epoch": 0.5292140921409214,
      "grad_norm": 24.761594772338867,
      "learning_rate": 1e-05,
      "loss": 6.9924,
      "step": 9764
    },
    {
      "epoch": 0.5292140921409214,
      "step": 9764,
      "training_loss": 7.148303985595703
    },
    {
      "epoch": 0.5292682926829269,
      "step": 9765,
      "training_loss": 6.593478202819824
    },
    {
      "epoch": 0.5293224932249323,
      "step": 9766,
      "training_loss": 6.848930358886719
    },
    {
      "epoch": 0.5293766937669376,
      "step": 9767,
      "training_loss": 5.9249491691589355
    },
    {
      "epoch": 0.5294308943089431,
      "grad_norm": 36.052799224853516,
      "learning_rate": 1e-05,
      "loss": 6.6289,
      "step": 9768
    },
    {
      "epoch": 0.5294308943089431,
      "step": 9768,
      "training_loss": 5.190903663635254
    },
    {
      "epoch": 0.5294850948509485,
      "step": 9769,
      "training_loss": 7.078761100769043
    },
    {
      "epoch": 0.529539295392954,
      "step": 9770,
      "training_loss": 7.779417514801025
    },
    {
      "epoch": 0.5295934959349593,
      "step": 9771,
      "training_loss": 6.294119358062744
    },
    {
      "epoch": 0.5296476964769647,
      "grad_norm": 25.51953125,
      "learning_rate": 1e-05,
      "loss": 6.5858,
      "step": 9772
    },
    {
      "epoch": 0.5296476964769647,
      "step": 9772,
      "training_loss": 7.372880935668945
    },
    {
      "epoch": 0.5297018970189702,
      "step": 9773,
      "training_loss": 7.0222649574279785
    },
    {
      "epoch": 0.5297560975609756,
      "step": 9774,
      "training_loss": 7.1469268798828125
    },
    {
      "epoch": 0.5298102981029811,
      "step": 9775,
      "training_loss": 5.029252529144287
    },
    {
      "epoch": 0.5298644986449864,
      "grad_norm": 37.86753845214844,
      "learning_rate": 1e-05,
      "loss": 6.6428,
      "step": 9776
    },
    {
      "epoch": 0.5298644986449864,
      "step": 9776,
      "training_loss": 7.096391201019287
    },
    {
      "epoch": 0.5299186991869919,
      "step": 9777,
      "training_loss": 4.582314968109131
    },
    {
      "epoch": 0.5299728997289973,
      "step": 9778,
      "training_loss": 6.623559951782227
    },
    {
      "epoch": 0.5300271002710027,
      "step": 9779,
      "training_loss": 6.287086009979248
    },
    {
      "epoch": 0.5300813008130081,
      "grad_norm": 45.93681716918945,
      "learning_rate": 1e-05,
      "loss": 6.1473,
      "step": 9780
    },
    {
      "epoch": 0.5300813008130081,
      "step": 9780,
      "training_loss": 6.915719509124756
    },
    {
      "epoch": 0.5301355013550135,
      "step": 9781,
      "training_loss": 6.325526714324951
    },
    {
      "epoch": 0.530189701897019,
      "step": 9782,
      "training_loss": 6.542122840881348
    },
    {
      "epoch": 0.5302439024390244,
      "step": 9783,
      "training_loss": 8.024178504943848
    },
    {
      "epoch": 0.5302981029810299,
      "grad_norm": 26.14285659790039,
      "learning_rate": 1e-05,
      "loss": 6.9519,
      "step": 9784
    },
    {
      "epoch": 0.5302981029810299,
      "step": 9784,
      "training_loss": 7.132741451263428
    },
    {
      "epoch": 0.5303523035230352,
      "step": 9785,
      "training_loss": 6.237795352935791
    },
    {
      "epoch": 0.5304065040650406,
      "step": 9786,
      "training_loss": 6.1982574462890625
    },
    {
      "epoch": 0.5304607046070461,
      "step": 9787,
      "training_loss": 7.729539394378662
    },
    {
      "epoch": 0.5305149051490515,
      "grad_norm": 28.002195358276367,
      "learning_rate": 1e-05,
      "loss": 6.8246,
      "step": 9788
    },
    {
      "epoch": 0.5305149051490515,
      "step": 9788,
      "training_loss": 6.75046968460083
    },
    {
      "epoch": 0.5305691056910569,
      "step": 9789,
      "training_loss": 5.093532085418701
    },
    {
      "epoch": 0.5306233062330623,
      "step": 9790,
      "training_loss": 6.298792362213135
    },
    {
      "epoch": 0.5306775067750678,
      "step": 9791,
      "training_loss": 7.655940532684326
    },
    {
      "epoch": 0.5307317073170732,
      "grad_norm": 40.006900787353516,
      "learning_rate": 1e-05,
      "loss": 6.4497,
      "step": 9792
    },
    {
      "epoch": 0.5307317073170732,
      "step": 9792,
      "training_loss": 7.363857746124268
    },
    {
      "epoch": 0.5307859078590786,
      "step": 9793,
      "training_loss": 5.628309726715088
    },
    {
      "epoch": 0.530840108401084,
      "step": 9794,
      "training_loss": 6.7273850440979
    },
    {
      "epoch": 0.5308943089430894,
      "step": 9795,
      "training_loss": 6.092717170715332
    },
    {
      "epoch": 0.5309485094850949,
      "grad_norm": 23.12781524658203,
      "learning_rate": 1e-05,
      "loss": 6.4531,
      "step": 9796
    },
    {
      "epoch": 0.5309485094850949,
      "step": 9796,
      "training_loss": 6.0570268630981445
    },
    {
      "epoch": 0.5310027100271003,
      "step": 9797,
      "training_loss": 7.038976669311523
    },
    {
      "epoch": 0.5310569105691056,
      "step": 9798,
      "training_loss": 6.61879301071167
    },
    {
      "epoch": 0.5311111111111111,
      "step": 9799,
      "training_loss": 6.9529266357421875
    },
    {
      "epoch": 0.5311653116531165,
      "grad_norm": 35.08141326904297,
      "learning_rate": 1e-05,
      "loss": 6.6669,
      "step": 9800
    },
    {
      "epoch": 0.5311653116531165,
      "step": 9800,
      "training_loss": 7.215754508972168
    },
    {
      "epoch": 0.531219512195122,
      "step": 9801,
      "training_loss": 6.791914939880371
    },
    {
      "epoch": 0.5312737127371274,
      "step": 9802,
      "training_loss": 5.619354248046875
    },
    {
      "epoch": 0.5313279132791328,
      "step": 9803,
      "training_loss": 6.70796012878418
    },
    {
      "epoch": 0.5313821138211382,
      "grad_norm": 21.581464767456055,
      "learning_rate": 1e-05,
      "loss": 6.5837,
      "step": 9804
    },
    {
      "epoch": 0.5313821138211382,
      "step": 9804,
      "training_loss": 5.7322163581848145
    },
    {
      "epoch": 0.5314363143631436,
      "step": 9805,
      "training_loss": 7.172445297241211
    },
    {
      "epoch": 0.5314905149051491,
      "step": 9806,
      "training_loss": 6.846028804779053
    },
    {
      "epoch": 0.5315447154471544,
      "step": 9807,
      "training_loss": 5.61526346206665
    },
    {
      "epoch": 0.5315989159891599,
      "grad_norm": 78.49889373779297,
      "learning_rate": 1e-05,
      "loss": 6.3415,
      "step": 9808
    },
    {
      "epoch": 0.5315989159891599,
      "step": 9808,
      "training_loss": 6.415689945220947
    },
    {
      "epoch": 0.5316531165311653,
      "step": 9809,
      "training_loss": 6.555326461791992
    },
    {
      "epoch": 0.5317073170731708,
      "step": 9810,
      "training_loss": 5.537311553955078
    },
    {
      "epoch": 0.5317615176151762,
      "step": 9811,
      "training_loss": 7.13571310043335
    },
    {
      "epoch": 0.5318157181571815,
      "grad_norm": 19.921419143676758,
      "learning_rate": 1e-05,
      "loss": 6.411,
      "step": 9812
    },
    {
      "epoch": 0.5318157181571815,
      "step": 9812,
      "training_loss": 5.748772144317627
    },
    {
      "epoch": 0.531869918699187,
      "step": 9813,
      "training_loss": 6.741727828979492
    },
    {
      "epoch": 0.5319241192411924,
      "step": 9814,
      "training_loss": 6.827011585235596
    },
    {
      "epoch": 0.5319783197831979,
      "step": 9815,
      "training_loss": 6.9182610511779785
    },
    {
      "epoch": 0.5320325203252032,
      "grad_norm": 28.5490779876709,
      "learning_rate": 1e-05,
      "loss": 6.5589,
      "step": 9816
    },
    {
      "epoch": 0.5320325203252032,
      "step": 9816,
      "training_loss": 6.700838565826416
    },
    {
      "epoch": 0.5320867208672087,
      "step": 9817,
      "training_loss": 6.931272506713867
    },
    {
      "epoch": 0.5321409214092141,
      "step": 9818,
      "training_loss": 7.420947551727295
    },
    {
      "epoch": 0.5321951219512195,
      "step": 9819,
      "training_loss": 7.660480499267578
    },
    {
      "epoch": 0.532249322493225,
      "grad_norm": 31.328405380249023,
      "learning_rate": 1e-05,
      "loss": 7.1784,
      "step": 9820
    },
    {
      "epoch": 0.532249322493225,
      "step": 9820,
      "training_loss": 7.276173114776611
    },
    {
      "epoch": 0.5323035230352303,
      "step": 9821,
      "training_loss": 5.660027980804443
    },
    {
      "epoch": 0.5323577235772358,
      "step": 9822,
      "training_loss": 7.155582904815674
    },
    {
      "epoch": 0.5324119241192412,
      "step": 9823,
      "training_loss": 5.3317742347717285
    },
    {
      "epoch": 0.5324661246612467,
      "grad_norm": 40.54351043701172,
      "learning_rate": 1e-05,
      "loss": 6.3559,
      "step": 9824
    },
    {
      "epoch": 0.5324661246612467,
      "step": 9824,
      "training_loss": 5.289310932159424
    },
    {
      "epoch": 0.532520325203252,
      "step": 9825,
      "training_loss": 5.733797073364258
    },
    {
      "epoch": 0.5325745257452574,
      "step": 9826,
      "training_loss": 5.900796890258789
    },
    {
      "epoch": 0.5326287262872629,
      "step": 9827,
      "training_loss": 6.175434112548828
    },
    {
      "epoch": 0.5326829268292683,
      "grad_norm": 23.005462646484375,
      "learning_rate": 1e-05,
      "loss": 5.7748,
      "step": 9828
    },
    {
      "epoch": 0.5326829268292683,
      "step": 9828,
      "training_loss": 4.084205150604248
    },
    {
      "epoch": 0.5327371273712738,
      "step": 9829,
      "training_loss": 7.967818260192871
    },
    {
      "epoch": 0.5327913279132791,
      "step": 9830,
      "training_loss": 5.70588493347168
    },
    {
      "epoch": 0.5328455284552845,
      "step": 9831,
      "training_loss": 3.4190192222595215
    },
    {
      "epoch": 0.53289972899729,
      "grad_norm": 32.79330062866211,
      "learning_rate": 1e-05,
      "loss": 5.2942,
      "step": 9832
    },
    {
      "epoch": 0.53289972899729,
      "step": 9832,
      "training_loss": 7.082696437835693
    },
    {
      "epoch": 0.5329539295392954,
      "step": 9833,
      "training_loss": 6.75284481048584
    },
    {
      "epoch": 0.5330081300813008,
      "step": 9834,
      "training_loss": 6.766963481903076
    },
    {
      "epoch": 0.5330623306233062,
      "step": 9835,
      "training_loss": 7.3140034675598145
    },
    {
      "epoch": 0.5331165311653117,
      "grad_norm": 25.66160011291504,
      "learning_rate": 1e-05,
      "loss": 6.9791,
      "step": 9836
    },
    {
      "epoch": 0.5331165311653117,
      "step": 9836,
      "training_loss": 6.443922519683838
    },
    {
      "epoch": 0.5331707317073171,
      "step": 9837,
      "training_loss": 6.9173078536987305
    },
    {
      "epoch": 0.5332249322493225,
      "step": 9838,
      "training_loss": 6.309428691864014
    },
    {
      "epoch": 0.5332791327913279,
      "step": 9839,
      "training_loss": 5.243016719818115
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 32.09250259399414,
      "learning_rate": 1e-05,
      "loss": 6.2284,
      "step": 9840
    },
    {
      "epoch": 0.5333333333333333,
      "step": 9840,
      "training_loss": 6.450216770172119
    },
    {
      "epoch": 0.5333875338753388,
      "step": 9841,
      "training_loss": 6.5905351638793945
    },
    {
      "epoch": 0.5334417344173442,
      "step": 9842,
      "training_loss": 5.704099655151367
    },
    {
      "epoch": 0.5334959349593495,
      "step": 9843,
      "training_loss": 6.667447090148926
    },
    {
      "epoch": 0.533550135501355,
      "grad_norm": 17.557647705078125,
      "learning_rate": 1e-05,
      "loss": 6.3531,
      "step": 9844
    },
    {
      "epoch": 0.533550135501355,
      "step": 9844,
      "training_loss": 6.5966315269470215
    },
    {
      "epoch": 0.5336043360433604,
      "step": 9845,
      "training_loss": 7.1627092361450195
    },
    {
      "epoch": 0.5336585365853659,
      "step": 9846,
      "training_loss": 7.255829811096191
    },
    {
      "epoch": 0.5337127371273713,
      "step": 9847,
      "training_loss": 7.394549369812012
    },
    {
      "epoch": 0.5337669376693767,
      "grad_norm": 21.572961807250977,
      "learning_rate": 1e-05,
      "loss": 7.1024,
      "step": 9848
    },
    {
      "epoch": 0.5337669376693767,
      "step": 9848,
      "training_loss": 8.073174476623535
    },
    {
      "epoch": 0.5338211382113821,
      "step": 9849,
      "training_loss": 6.3889312744140625
    },
    {
      "epoch": 0.5338753387533876,
      "step": 9850,
      "training_loss": 7.379858493804932
    },
    {
      "epoch": 0.533929539295393,
      "step": 9851,
      "training_loss": 5.32395601272583
    },
    {
      "epoch": 0.5339837398373983,
      "grad_norm": 40.63267517089844,
      "learning_rate": 1e-05,
      "loss": 6.7915,
      "step": 9852
    },
    {
      "epoch": 0.5339837398373983,
      "step": 9852,
      "training_loss": 7.22347354888916
    },
    {
      "epoch": 0.5340379403794038,
      "step": 9853,
      "training_loss": 6.0697197914123535
    },
    {
      "epoch": 0.5340921409214092,
      "step": 9854,
      "training_loss": 7.641605377197266
    },
    {
      "epoch": 0.5341463414634147,
      "step": 9855,
      "training_loss": 6.770161151885986
    },
    {
      "epoch": 0.5342005420054201,
      "grad_norm": 26.06795883178711,
      "learning_rate": 1e-05,
      "loss": 6.9262,
      "step": 9856
    },
    {
      "epoch": 0.5342005420054201,
      "step": 9856,
      "training_loss": 7.574159622192383
    },
    {
      "epoch": 0.5342547425474254,
      "step": 9857,
      "training_loss": 3.351170539855957
    },
    {
      "epoch": 0.5343089430894309,
      "step": 9858,
      "training_loss": 7.295527458190918
    },
    {
      "epoch": 0.5343631436314363,
      "step": 9859,
      "training_loss": 5.228156566619873
    },
    {
      "epoch": 0.5344173441734418,
      "grad_norm": 37.07360076904297,
      "learning_rate": 1e-05,
      "loss": 5.8623,
      "step": 9860
    },
    {
      "epoch": 0.5344173441734418,
      "step": 9860,
      "training_loss": 6.640567302703857
    },
    {
      "epoch": 0.5344715447154471,
      "step": 9861,
      "training_loss": 5.569455146789551
    },
    {
      "epoch": 0.5345257452574526,
      "step": 9862,
      "training_loss": 6.262401580810547
    },
    {
      "epoch": 0.534579945799458,
      "step": 9863,
      "training_loss": 6.860971450805664
    },
    {
      "epoch": 0.5346341463414634,
      "grad_norm": 17.40635871887207,
      "learning_rate": 1e-05,
      "loss": 6.3333,
      "step": 9864
    },
    {
      "epoch": 0.5346341463414634,
      "step": 9864,
      "training_loss": 5.793179035186768
    },
    {
      "epoch": 0.5346883468834689,
      "step": 9865,
      "training_loss": 3.697145938873291
    },
    {
      "epoch": 0.5347425474254742,
      "step": 9866,
      "training_loss": 4.356195449829102
    },
    {
      "epoch": 0.5347967479674797,
      "step": 9867,
      "training_loss": 6.0913872718811035
    },
    {
      "epoch": 0.5348509485094851,
      "grad_norm": 29.16472816467285,
      "learning_rate": 1e-05,
      "loss": 4.9845,
      "step": 9868
    },
    {
      "epoch": 0.5348509485094851,
      "step": 9868,
      "training_loss": 5.5994110107421875
    },
    {
      "epoch": 0.5349051490514906,
      "step": 9869,
      "training_loss": 5.746710777282715
    },
    {
      "epoch": 0.5349593495934959,
      "step": 9870,
      "training_loss": 8.261406898498535
    },
    {
      "epoch": 0.5350135501355013,
      "step": 9871,
      "training_loss": 5.681939601898193
    },
    {
      "epoch": 0.5350677506775068,
      "grad_norm": 23.448200225830078,
      "learning_rate": 1e-05,
      "loss": 6.3224,
      "step": 9872
    },
    {
      "epoch": 0.5350677506775068,
      "step": 9872,
      "training_loss": 6.04384183883667
    },
    {
      "epoch": 0.5351219512195122,
      "step": 9873,
      "training_loss": 4.67739725112915
    },
    {
      "epoch": 0.5351761517615176,
      "step": 9874,
      "training_loss": 8.41773509979248
    },
    {
      "epoch": 0.535230352303523,
      "step": 9875,
      "training_loss": 7.14284086227417
    },
    {
      "epoch": 0.5352845528455284,
      "grad_norm": 24.77376937866211,
      "learning_rate": 1e-05,
      "loss": 6.5705,
      "step": 9876
    },
    {
      "epoch": 0.5352845528455284,
      "step": 9876,
      "training_loss": 7.265745162963867
    },
    {
      "epoch": 0.5353387533875339,
      "step": 9877,
      "training_loss": 5.663590431213379
    },
    {
      "epoch": 0.5353929539295393,
      "step": 9878,
      "training_loss": 6.709957599639893
    },
    {
      "epoch": 0.5354471544715447,
      "step": 9879,
      "training_loss": 5.012781143188477
    },
    {
      "epoch": 0.5355013550135501,
      "grad_norm": 31.676528930664062,
      "learning_rate": 1e-05,
      "loss": 6.163,
      "step": 9880
    },
    {
      "epoch": 0.5355013550135501,
      "step": 9880,
      "training_loss": 6.339178562164307
    },
    {
      "epoch": 0.5355555555555556,
      "step": 9881,
      "training_loss": 6.283968925476074
    },
    {
      "epoch": 0.535609756097561,
      "step": 9882,
      "training_loss": 3.6937263011932373
    },
    {
      "epoch": 0.5356639566395663,
      "step": 9883,
      "training_loss": 7.289591312408447
    },
    {
      "epoch": 0.5357181571815718,
      "grad_norm": 22.73908805847168,
      "learning_rate": 1e-05,
      "loss": 5.9016,
      "step": 9884
    },
    {
      "epoch": 0.5357181571815718,
      "step": 9884,
      "training_loss": 7.069984436035156
    },
    {
      "epoch": 0.5357723577235772,
      "step": 9885,
      "training_loss": 7.635213375091553
    },
    {
      "epoch": 0.5358265582655827,
      "step": 9886,
      "training_loss": 5.379856109619141
    },
    {
      "epoch": 0.5358807588075881,
      "step": 9887,
      "training_loss": 2.8397598266601562
    },
    {
      "epoch": 0.5359349593495935,
      "grad_norm": 30.59885025024414,
      "learning_rate": 1e-05,
      "loss": 5.7312,
      "step": 9888
    },
    {
      "epoch": 0.5359349593495935,
      "step": 9888,
      "training_loss": 7.923654079437256
    },
    {
      "epoch": 0.5359891598915989,
      "step": 9889,
      "training_loss": 6.487328052520752
    },
    {
      "epoch": 0.5360433604336043,
      "step": 9890,
      "training_loss": 5.283884525299072
    },
    {
      "epoch": 0.5360975609756098,
      "step": 9891,
      "training_loss": 6.0763678550720215
    },
    {
      "epoch": 0.5361517615176151,
      "grad_norm": 34.290069580078125,
      "learning_rate": 1e-05,
      "loss": 6.4428,
      "step": 9892
    },
    {
      "epoch": 0.5361517615176151,
      "step": 9892,
      "training_loss": 6.426182746887207
    },
    {
      "epoch": 0.5362059620596206,
      "step": 9893,
      "training_loss": 6.433197021484375
    },
    {
      "epoch": 0.536260162601626,
      "step": 9894,
      "training_loss": 6.839251518249512
    },
    {
      "epoch": 0.5363143631436315,
      "step": 9895,
      "training_loss": 7.133983135223389
    },
    {
      "epoch": 0.5363685636856369,
      "grad_norm": 49.072227478027344,
      "learning_rate": 1e-05,
      "loss": 6.7082,
      "step": 9896
    },
    {
      "epoch": 0.5363685636856369,
      "step": 9896,
      "training_loss": 6.554077625274658
    },
    {
      "epoch": 0.5364227642276422,
      "step": 9897,
      "training_loss": 7.144684791564941
    },
    {
      "epoch": 0.5364769647696477,
      "step": 9898,
      "training_loss": 4.295400142669678
    },
    {
      "epoch": 0.5365311653116531,
      "step": 9899,
      "training_loss": 8.578272819519043
    },
    {
      "epoch": 0.5365853658536586,
      "grad_norm": 42.19497299194336,
      "learning_rate": 1e-05,
      "loss": 6.6431,
      "step": 9900
    },
    {
      "epoch": 0.5365853658536586,
      "step": 9900,
      "training_loss": 8.062020301818848
    },
    {
      "epoch": 0.5366395663956639,
      "step": 9901,
      "training_loss": 7.124002933502197
    },
    {
      "epoch": 0.5366937669376693,
      "step": 9902,
      "training_loss": 6.361248016357422
    },
    {
      "epoch": 0.5367479674796748,
      "step": 9903,
      "training_loss": 3.0151174068450928
    },
    {
      "epoch": 0.5368021680216802,
      "grad_norm": 34.68681716918945,
      "learning_rate": 1e-05,
      "loss": 6.1406,
      "step": 9904
    },
    {
      "epoch": 0.5368021680216802,
      "step": 9904,
      "training_loss": 6.755093097686768
    },
    {
      "epoch": 0.5368563685636857,
      "step": 9905,
      "training_loss": 6.605825901031494
    },
    {
      "epoch": 0.536910569105691,
      "step": 9906,
      "training_loss": 6.923766136169434
    },
    {
      "epoch": 0.5369647696476965,
      "step": 9907,
      "training_loss": 8.251556396484375
    },
    {
      "epoch": 0.5370189701897019,
      "grad_norm": 31.092206954956055,
      "learning_rate": 1e-05,
      "loss": 7.1341,
      "step": 9908
    },
    {
      "epoch": 0.5370189701897019,
      "step": 9908,
      "training_loss": 5.3045220375061035
    },
    {
      "epoch": 0.5370731707317074,
      "step": 9909,
      "training_loss": 6.172341823577881
    },
    {
      "epoch": 0.5371273712737127,
      "step": 9910,
      "training_loss": 6.797322750091553
    },
    {
      "epoch": 0.5371815718157181,
      "step": 9911,
      "training_loss": 7.418909072875977
    },
    {
      "epoch": 0.5372357723577236,
      "grad_norm": 21.826478958129883,
      "learning_rate": 1e-05,
      "loss": 6.4233,
      "step": 9912
    },
    {
      "epoch": 0.5372357723577236,
      "step": 9912,
      "training_loss": 6.699124813079834
    },
    {
      "epoch": 0.537289972899729,
      "step": 9913,
      "training_loss": 7.2060322761535645
    },
    {
      "epoch": 0.5373441734417345,
      "step": 9914,
      "training_loss": 6.131251335144043
    },
    {
      "epoch": 0.5373983739837398,
      "step": 9915,
      "training_loss": 4.434967517852783
    },
    {
      "epoch": 0.5374525745257452,
      "grad_norm": 35.79872131347656,
      "learning_rate": 1e-05,
      "loss": 6.1178,
      "step": 9916
    },
    {
      "epoch": 0.5374525745257452,
      "step": 9916,
      "training_loss": 5.244661331176758
    },
    {
      "epoch": 0.5375067750677507,
      "step": 9917,
      "training_loss": 3.5527005195617676
    },
    {
      "epoch": 0.5375609756097561,
      "step": 9918,
      "training_loss": 4.5836896896362305
    },
    {
      "epoch": 0.5376151761517615,
      "step": 9919,
      "training_loss": 7.14425802230835
    },
    {
      "epoch": 0.5376693766937669,
      "grad_norm": 19.326005935668945,
      "learning_rate": 1e-05,
      "loss": 5.1313,
      "step": 9920
    },
    {
      "epoch": 0.5376693766937669,
      "step": 9920,
      "training_loss": 6.681807994842529
    },
    {
      "epoch": 0.5377235772357724,
      "step": 9921,
      "training_loss": 6.2459211349487305
    },
    {
      "epoch": 0.5377777777777778,
      "step": 9922,
      "training_loss": 6.567477226257324
    },
    {
      "epoch": 0.5378319783197832,
      "step": 9923,
      "training_loss": 8.611211776733398
    },
    {
      "epoch": 0.5378861788617886,
      "grad_norm": 40.313011169433594,
      "learning_rate": 1e-05,
      "loss": 7.0266,
      "step": 9924
    },
    {
      "epoch": 0.5378861788617886,
      "step": 9924,
      "training_loss": 7.779663562774658
    },
    {
      "epoch": 0.537940379403794,
      "step": 9925,
      "training_loss": 7.336366176605225
    },
    {
      "epoch": 0.5379945799457995,
      "step": 9926,
      "training_loss": 6.8769850730896
    },
    {
      "epoch": 0.5380487804878049,
      "step": 9927,
      "training_loss": 7.05618143081665
    },
    {
      "epoch": 0.5381029810298102,
      "grad_norm": 21.146820068359375,
      "learning_rate": 1e-05,
      "loss": 7.2623,
      "step": 9928
    },
    {
      "epoch": 0.5381029810298102,
      "step": 9928,
      "training_loss": 7.275004863739014
    },
    {
      "epoch": 0.5381571815718157,
      "step": 9929,
      "training_loss": 7.4874958992004395
    },
    {
      "epoch": 0.5382113821138211,
      "step": 9930,
      "training_loss": 7.837551116943359
    },
    {
      "epoch": 0.5382655826558266,
      "step": 9931,
      "training_loss": 7.047699928283691
    },
    {
      "epoch": 0.538319783197832,
      "grad_norm": 66.56848907470703,
      "learning_rate": 1e-05,
      "loss": 7.4119,
      "step": 9932
    },
    {
      "epoch": 0.538319783197832,
      "step": 9932,
      "training_loss": 7.695091724395752
    },
    {
      "epoch": 0.5383739837398374,
      "step": 9933,
      "training_loss": 6.653684139251709
    },
    {
      "epoch": 0.5384281842818428,
      "step": 9934,
      "training_loss": 4.989891529083252
    },
    {
      "epoch": 0.5384823848238482,
      "step": 9935,
      "training_loss": 6.925408840179443
    },
    {
      "epoch": 0.5385365853658537,
      "grad_norm": 22.311817169189453,
      "learning_rate": 1e-05,
      "loss": 6.566,
      "step": 9936
    },
    {
      "epoch": 0.5385365853658537,
      "step": 9936,
      "training_loss": 6.746735095977783
    },
    {
      "epoch": 0.538590785907859,
      "step": 9937,
      "training_loss": 6.906179904937744
    },
    {
      "epoch": 0.5386449864498645,
      "step": 9938,
      "training_loss": 6.316070079803467
    },
    {
      "epoch": 0.5386991869918699,
      "step": 9939,
      "training_loss": 6.48468017578125
    },
    {
      "epoch": 0.5387533875338754,
      "grad_norm": 30.486621856689453,
      "learning_rate": 1e-05,
      "loss": 6.6134,
      "step": 9940
    },
    {
      "epoch": 0.5387533875338754,
      "step": 9940,
      "training_loss": 5.462783336639404
    },
    {
      "epoch": 0.5388075880758808,
      "step": 9941,
      "training_loss": 6.332703113555908
    },
    {
      "epoch": 0.5388617886178861,
      "step": 9942,
      "training_loss": 5.517433166503906
    },
    {
      "epoch": 0.5389159891598916,
      "step": 9943,
      "training_loss": 7.656428813934326
    },
    {
      "epoch": 0.538970189701897,
      "grad_norm": 42.87921142578125,
      "learning_rate": 1e-05,
      "loss": 6.2423,
      "step": 9944
    },
    {
      "epoch": 0.538970189701897,
      "step": 9944,
      "training_loss": 9.0454683303833
    },
    {
      "epoch": 0.5390243902439025,
      "step": 9945,
      "training_loss": 3.714951753616333
    },
    {
      "epoch": 0.5390785907859078,
      "step": 9946,
      "training_loss": 6.784860610961914
    },
    {
      "epoch": 0.5391327913279133,
      "step": 9947,
      "training_loss": 6.6773810386657715
    },
    {
      "epoch": 0.5391869918699187,
      "grad_norm": 16.471837997436523,
      "learning_rate": 1e-05,
      "loss": 6.5557,
      "step": 9948
    },
    {
      "epoch": 0.5391869918699187,
      "step": 9948,
      "training_loss": 6.895878791809082
    },
    {
      "epoch": 0.5392411924119241,
      "step": 9949,
      "training_loss": 6.438842296600342
    },
    {
      "epoch": 0.5392953929539296,
      "step": 9950,
      "training_loss": 6.361146450042725
    },
    {
      "epoch": 0.5393495934959349,
      "step": 9951,
      "training_loss": 5.073774814605713
    },
    {
      "epoch": 0.5394037940379404,
      "grad_norm": 49.93552017211914,
      "learning_rate": 1e-05,
      "loss": 6.1924,
      "step": 9952
    },
    {
      "epoch": 0.5394037940379404,
      "step": 9952,
      "training_loss": 5.8960137367248535
    },
    {
      "epoch": 0.5394579945799458,
      "step": 9953,
      "training_loss": 6.577657222747803
    },
    {
      "epoch": 0.5395121951219513,
      "step": 9954,
      "training_loss": 7.736851215362549
    },
    {
      "epoch": 0.5395663956639566,
      "step": 9955,
      "training_loss": 7.173737525939941
    },
    {
      "epoch": 0.539620596205962,
      "grad_norm": 18.2580623626709,
      "learning_rate": 1e-05,
      "loss": 6.8461,
      "step": 9956
    },
    {
      "epoch": 0.539620596205962,
      "step": 9956,
      "training_loss": 6.9529314041137695
    },
    {
      "epoch": 0.5396747967479675,
      "step": 9957,
      "training_loss": 6.1090497970581055
    },
    {
      "epoch": 0.5397289972899729,
      "step": 9958,
      "training_loss": 6.670487880706787
    },
    {
      "epoch": 0.5397831978319784,
      "step": 9959,
      "training_loss": 8.414233207702637
    },
    {
      "epoch": 0.5398373983739837,
      "grad_norm": 22.02490234375,
      "learning_rate": 1e-05,
      "loss": 7.0367,
      "step": 9960
    },
    {
      "epoch": 0.5398373983739837,
      "step": 9960,
      "training_loss": 6.04464864730835
    },
    {
      "epoch": 0.5398915989159891,
      "step": 9961,
      "training_loss": 7.906925201416016
    },
    {
      "epoch": 0.5399457994579946,
      "step": 9962,
      "training_loss": 6.6887688636779785
    },
    {
      "epoch": 0.54,
      "step": 9963,
      "training_loss": 6.653687000274658
    },
    {
      "epoch": 0.5400542005420054,
      "grad_norm": 26.270418167114258,
      "learning_rate": 1e-05,
      "loss": 6.8235,
      "step": 9964
    },
    {
      "epoch": 0.5400542005420054,
      "step": 9964,
      "training_loss": 7.271444797515869
    },
    {
      "epoch": 0.5401084010840108,
      "step": 9965,
      "training_loss": 8.174966812133789
    },
    {
      "epoch": 0.5401626016260163,
      "step": 9966,
      "training_loss": 7.290604591369629
    },
    {
      "epoch": 0.5402168021680217,
      "step": 9967,
      "training_loss": 7.553721904754639
    },
    {
      "epoch": 0.5402710027100271,
      "grad_norm": 25.669944763183594,
      "learning_rate": 1e-05,
      "loss": 7.5727,
      "step": 9968
    },
    {
      "epoch": 0.5402710027100271,
      "step": 9968,
      "training_loss": 6.498220920562744
    },
    {
      "epoch": 0.5403252032520325,
      "step": 9969,
      "training_loss": 6.59105920791626
    },
    {
      "epoch": 0.5403794037940379,
      "step": 9970,
      "training_loss": 6.357550621032715
    },
    {
      "epoch": 0.5404336043360434,
      "step": 9971,
      "training_loss": 5.698193550109863
    },
    {
      "epoch": 0.5404878048780488,
      "grad_norm": 44.089324951171875,
      "learning_rate": 1e-05,
      "loss": 6.2863,
      "step": 9972
    },
    {
      "epoch": 0.5404878048780488,
      "step": 9972,
      "training_loss": 6.107305526733398
    },
    {
      "epoch": 0.5405420054200542,
      "step": 9973,
      "training_loss": 7.288496494293213
    },
    {
      "epoch": 0.5405962059620596,
      "step": 9974,
      "training_loss": 6.204261302947998
    },
    {
      "epoch": 0.540650406504065,
      "step": 9975,
      "training_loss": 7.425544261932373
    },
    {
      "epoch": 0.5407046070460705,
      "grad_norm": 26.760719299316406,
      "learning_rate": 1e-05,
      "loss": 6.7564,
      "step": 9976
    },
    {
      "epoch": 0.5407046070460705,
      "step": 9976,
      "training_loss": 6.640613079071045
    },
    {
      "epoch": 0.5407588075880759,
      "step": 9977,
      "training_loss": 7.055013656616211
    },
    {
      "epoch": 0.5408130081300813,
      "step": 9978,
      "training_loss": 4.063788414001465
    },
    {
      "epoch": 0.5408672086720867,
      "step": 9979,
      "training_loss": 5.873685836791992
    },
    {
      "epoch": 0.5409214092140922,
      "grad_norm": 34.32584762573242,
      "learning_rate": 1e-05,
      "loss": 5.9083,
      "step": 9980
    },
    {
      "epoch": 0.5409214092140922,
      "step": 9980,
      "training_loss": 7.459442138671875
    },
    {
      "epoch": 0.5409756097560976,
      "step": 9981,
      "training_loss": 8.187084197998047
    },
    {
      "epoch": 0.5410298102981029,
      "step": 9982,
      "training_loss": 2.989003896713257
    },
    {
      "epoch": 0.5410840108401084,
      "step": 9983,
      "training_loss": 7.333758354187012
    },
    {
      "epoch": 0.5411382113821138,
      "grad_norm": 19.299537658691406,
      "learning_rate": 1e-05,
      "loss": 6.4923,
      "step": 9984
    },
    {
      "epoch": 0.5411382113821138,
      "step": 9984,
      "training_loss": 7.040680408477783
    },
    {
      "epoch": 0.5411924119241193,
      "step": 9985,
      "training_loss": 5.895277976989746
    },
    {
      "epoch": 0.5412466124661247,
      "step": 9986,
      "training_loss": 6.79386568069458
    },
    {
      "epoch": 0.54130081300813,
      "step": 9987,
      "training_loss": 5.774025917053223
    },
    {
      "epoch": 0.5413550135501355,
      "grad_norm": 78.65231323242188,
      "learning_rate": 1e-05,
      "loss": 6.376,
      "step": 9988
    },
    {
      "epoch": 0.5413550135501355,
      "step": 9988,
      "training_loss": 7.22315788269043
    },
    {
      "epoch": 0.5414092140921409,
      "step": 9989,
      "training_loss": 7.270186901092529
    },
    {
      "epoch": 0.5414634146341464,
      "step": 9990,
      "training_loss": 6.539029121398926
    },
    {
      "epoch": 0.5415176151761517,
      "step": 9991,
      "training_loss": 6.486447811126709
    },
    {
      "epoch": 0.5415718157181572,
      "grad_norm": 30.79615020751953,
      "learning_rate": 1e-05,
      "loss": 6.8797,
      "step": 9992
    },
    {
      "epoch": 0.5415718157181572,
      "step": 9992,
      "training_loss": 6.067354202270508
    },
    {
      "epoch": 0.5416260162601626,
      "step": 9993,
      "training_loss": 5.780787467956543
    },
    {
      "epoch": 0.541680216802168,
      "step": 9994,
      "training_loss": 7.488144874572754
    },
    {
      "epoch": 0.5417344173441735,
      "step": 9995,
      "training_loss": 6.879703998565674
    },
    {
      "epoch": 0.5417886178861788,
      "grad_norm": 16.613862991333008,
      "learning_rate": 1e-05,
      "loss": 6.554,
      "step": 9996
    },
    {
      "epoch": 0.5417886178861788,
      "step": 9996,
      "training_loss": 6.685351371765137
    },
    {
      "epoch": 0.5418428184281843,
      "step": 9997,
      "training_loss": 7.517923831939697
    },
    {
      "epoch": 0.5418970189701897,
      "step": 9998,
      "training_loss": 5.4897780418396
    },
    {
      "epoch": 0.5419512195121952,
      "step": 9999,
      "training_loss": 4.835322380065918
    },
    {
      "epoch": 0.5420054200542005,
      "grad_norm": 40.81881332397461,
      "learning_rate": 1e-05,
      "loss": 6.1321,
      "step": 10000
    },
    {
      "epoch": 0.5420054200542005,
      "step": 10000,
      "training_loss": 4.0884108543396
    },
    {
      "epoch": 0.5420596205962059,
      "step": 10001,
      "training_loss": 6.750410556793213
    },
    {
      "epoch": 0.5421138211382114,
      "step": 10002,
      "training_loss": 7.639892101287842
    },
    {
      "epoch": 0.5421680216802168,
      "step": 10003,
      "training_loss": 5.268014907836914
    },
    {
      "epoch": 0.5422222222222223,
      "grad_norm": 33.051025390625,
      "learning_rate": 1e-05,
      "loss": 5.9367,
      "step": 10004
    },
    {
      "epoch": 0.5422222222222223,
      "step": 10004,
      "training_loss": 6.657711982727051
    },
    {
      "epoch": 0.5422764227642276,
      "step": 10005,
      "training_loss": 7.018234729766846
    },
    {
      "epoch": 0.542330623306233,
      "step": 10006,
      "training_loss": 7.420594692230225
    },
    {
      "epoch": 0.5423848238482385,
      "step": 10007,
      "training_loss": 7.7137064933776855
    },
    {
      "epoch": 0.5424390243902439,
      "grad_norm": 37.660064697265625,
      "learning_rate": 1e-05,
      "loss": 7.2026,
      "step": 10008
    },
    {
      "epoch": 0.5424390243902439,
      "step": 10008,
      "training_loss": 7.852132797241211
    },
    {
      "epoch": 0.5424932249322493,
      "step": 10009,
      "training_loss": 7.288126468658447
    },
    {
      "epoch": 0.5425474254742547,
      "step": 10010,
      "training_loss": 7.922852516174316
    },
    {
      "epoch": 0.5426016260162602,
      "step": 10011,
      "training_loss": 7.039330959320068
    },
    {
      "epoch": 0.5426558265582656,
      "grad_norm": 27.83021354675293,
      "learning_rate": 1e-05,
      "loss": 7.5256,
      "step": 10012
    },
    {
      "epoch": 0.5426558265582656,
      "step": 10012,
      "training_loss": 7.426022052764893
    },
    {
      "epoch": 0.542710027100271,
      "step": 10013,
      "training_loss": 7.427744388580322
    },
    {
      "epoch": 0.5427642276422764,
      "step": 10014,
      "training_loss": 7.829416275024414
    },
    {
      "epoch": 0.5428184281842818,
      "step": 10015,
      "training_loss": 6.795017719268799
    },
    {
      "epoch": 0.5428726287262873,
      "grad_norm": 22.158002853393555,
      "learning_rate": 1e-05,
      "loss": 7.3695,
      "step": 10016
    },
    {
      "epoch": 0.5428726287262873,
      "step": 10016,
      "training_loss": 7.046565532684326
    },
    {
      "epoch": 0.5429268292682927,
      "step": 10017,
      "training_loss": 7.026900291442871
    },
    {
      "epoch": 0.542981029810298,
      "step": 10018,
      "training_loss": 7.159027576446533
    },
    {
      "epoch": 0.5430352303523035,
      "step": 10019,
      "training_loss": 6.293629169464111
    },
    {
      "epoch": 0.5430894308943089,
      "grad_norm": 24.717090606689453,
      "learning_rate": 1e-05,
      "loss": 6.8815,
      "step": 10020
    },
    {
      "epoch": 0.5430894308943089,
      "step": 10020,
      "training_loss": 6.986725330352783
    },
    {
      "epoch": 0.5431436314363144,
      "step": 10021,
      "training_loss": 5.778526782989502
    },
    {
      "epoch": 0.5431978319783198,
      "step": 10022,
      "training_loss": 6.682635307312012
    },
    {
      "epoch": 0.5432520325203252,
      "step": 10023,
      "training_loss": 5.512078762054443
    },
    {
      "epoch": 0.5433062330623306,
      "grad_norm": 31.04959487915039,
      "learning_rate": 1e-05,
      "loss": 6.24,
      "step": 10024
    },
    {
      "epoch": 0.5433062330623306,
      "step": 10024,
      "training_loss": 8.391281127929688
    },
    {
      "epoch": 0.5433604336043361,
      "step": 10025,
      "training_loss": 6.921921253204346
    },
    {
      "epoch": 0.5434146341463415,
      "step": 10026,
      "training_loss": 4.995527267456055
    },
    {
      "epoch": 0.5434688346883468,
      "step": 10027,
      "training_loss": 7.648502349853516
    },
    {
      "epoch": 0.5435230352303523,
      "grad_norm": 33.54485321044922,
      "learning_rate": 1e-05,
      "loss": 6.9893,
      "step": 10028
    },
    {
      "epoch": 0.5435230352303523,
      "step": 10028,
      "training_loss": 7.266848087310791
    },
    {
      "epoch": 0.5435772357723577,
      "step": 10029,
      "training_loss": 7.940197944641113
    },
    {
      "epoch": 0.5436314363143632,
      "step": 10030,
      "training_loss": 6.533794403076172
    },
    {
      "epoch": 0.5436856368563686,
      "step": 10031,
      "training_loss": 7.225090026855469
    },
    {
      "epoch": 0.543739837398374,
      "grad_norm": 31.14344596862793,
      "learning_rate": 1e-05,
      "loss": 7.2415,
      "step": 10032
    },
    {
      "epoch": 0.543739837398374,
      "step": 10032,
      "training_loss": 6.463386058807373
    },
    {
      "epoch": 0.5437940379403794,
      "step": 10033,
      "training_loss": 6.60625696182251
    },
    {
      "epoch": 0.5438482384823848,
      "step": 10034,
      "training_loss": 7.070001125335693
    },
    {
      "epoch": 0.5439024390243903,
      "step": 10035,
      "training_loss": 6.468708038330078
    },
    {
      "epoch": 0.5439566395663956,
      "grad_norm": 28.165328979492188,
      "learning_rate": 1e-05,
      "loss": 6.6521,
      "step": 10036
    },
    {
      "epoch": 0.5439566395663956,
      "step": 10036,
      "training_loss": 6.722848892211914
    },
    {
      "epoch": 0.5440108401084011,
      "step": 10037,
      "training_loss": 7.615884304046631
    },
    {
      "epoch": 0.5440650406504065,
      "step": 10038,
      "training_loss": 4.189123630523682
    },
    {
      "epoch": 0.544119241192412,
      "step": 10039,
      "training_loss": 7.20990514755249
    },
    {
      "epoch": 0.5441734417344174,
      "grad_norm": 26.16131019592285,
      "learning_rate": 1e-05,
      "loss": 6.4344,
      "step": 10040
    },
    {
      "epoch": 0.5441734417344174,
      "step": 10040,
      "training_loss": 7.9553046226501465
    },
    {
      "epoch": 0.5442276422764227,
      "step": 10041,
      "training_loss": 7.361443996429443
    },
    {
      "epoch": 0.5442818428184282,
      "step": 10042,
      "training_loss": 7.553104400634766
    },
    {
      "epoch": 0.5443360433604336,
      "step": 10043,
      "training_loss": 6.917912006378174
    },
    {
      "epoch": 0.5443902439024391,
      "grad_norm": 24.955175399780273,
      "learning_rate": 1e-05,
      "loss": 7.4469,
      "step": 10044
    },
    {
      "epoch": 0.5443902439024391,
      "step": 10044,
      "training_loss": 6.581033229827881
    },
    {
      "epoch": 0.5444444444444444,
      "step": 10045,
      "training_loss": 5.750679016113281
    },
    {
      "epoch": 0.5444986449864498,
      "step": 10046,
      "training_loss": 6.733935356140137
    },
    {
      "epoch": 0.5445528455284553,
      "step": 10047,
      "training_loss": 7.197137832641602
    },
    {
      "epoch": 0.5446070460704607,
      "grad_norm": 22.902759552001953,
      "learning_rate": 1e-05,
      "loss": 6.5657,
      "step": 10048
    },
    {
      "epoch": 0.5446070460704607,
      "step": 10048,
      "training_loss": 4.994439601898193
    },
    {
      "epoch": 0.5446612466124662,
      "step": 10049,
      "training_loss": 7.041933536529541
    },
    {
      "epoch": 0.5447154471544715,
      "step": 10050,
      "training_loss": 6.725452899932861
    },
    {
      "epoch": 0.544769647696477,
      "step": 10051,
      "training_loss": 6.142440319061279
    },
    {
      "epoch": 0.5448238482384824,
      "grad_norm": 23.10369300842285,
      "learning_rate": 1e-05,
      "loss": 6.2261,
      "step": 10052
    },
    {
      "epoch": 0.5448238482384824,
      "step": 10052,
      "training_loss": 6.819488048553467
    },
    {
      "epoch": 0.5448780487804878,
      "step": 10053,
      "training_loss": 7.86004114151001
    },
    {
      "epoch": 0.5449322493224932,
      "step": 10054,
      "training_loss": 6.900211334228516
    },
    {
      "epoch": 0.5449864498644986,
      "step": 10055,
      "training_loss": 7.859254837036133
    },
    {
      "epoch": 0.5450406504065041,
      "grad_norm": 29.47810935974121,
      "learning_rate": 1e-05,
      "loss": 7.3597,
      "step": 10056
    },
    {
      "epoch": 0.5450406504065041,
      "step": 10056,
      "training_loss": 4.89538049697876
    },
    {
      "epoch": 0.5450948509485095,
      "step": 10057,
      "training_loss": 6.578735828399658
    },
    {
      "epoch": 0.545149051490515,
      "step": 10058,
      "training_loss": 5.637573719024658
    },
    {
      "epoch": 0.5452032520325203,
      "step": 10059,
      "training_loss": 5.635173320770264
    },
    {
      "epoch": 0.5452574525745257,
      "grad_norm": 29.29116439819336,
      "learning_rate": 1e-05,
      "loss": 5.6867,
      "step": 10060
    },
    {
      "epoch": 0.5452574525745257,
      "step": 10060,
      "training_loss": 4.795928955078125
    },
    {
      "epoch": 0.5453116531165312,
      "step": 10061,
      "training_loss": 7.154339790344238
    },
    {
      "epoch": 0.5453658536585366,
      "step": 10062,
      "training_loss": 6.635884761810303
    },
    {
      "epoch": 0.545420054200542,
      "step": 10063,
      "training_loss": 7.31048059463501
    },
    {
      "epoch": 0.5454742547425474,
      "grad_norm": 25.97119903564453,
      "learning_rate": 1e-05,
      "loss": 6.4742,
      "step": 10064
    },
    {
      "epoch": 0.5454742547425474,
      "step": 10064,
      "training_loss": 7.184084415435791
    },
    {
      "epoch": 0.5455284552845528,
      "step": 10065,
      "training_loss": 4.914015769958496
    },
    {
      "epoch": 0.5455826558265583,
      "step": 10066,
      "training_loss": 6.093289852142334
    },
    {
      "epoch": 0.5456368563685637,
      "step": 10067,
      "training_loss": 7.331298828125
    },
    {
      "epoch": 0.5456910569105691,
      "grad_norm": 50.051719665527344,
      "learning_rate": 1e-05,
      "loss": 6.3807,
      "step": 10068
    },
    {
      "epoch": 0.5456910569105691,
      "step": 10068,
      "training_loss": 6.358314514160156
    },
    {
      "epoch": 0.5457452574525745,
      "step": 10069,
      "training_loss": 5.825453281402588
    },
    {
      "epoch": 0.54579945799458,
      "step": 10070,
      "training_loss": 7.387664318084717
    },
    {
      "epoch": 0.5458536585365854,
      "step": 10071,
      "training_loss": 6.281922340393066
    },
    {
      "epoch": 0.5459078590785907,
      "grad_norm": 30.047426223754883,
      "learning_rate": 1e-05,
      "loss": 6.4633,
      "step": 10072
    },
    {
      "epoch": 0.5459078590785907,
      "step": 10072,
      "training_loss": 5.997745990753174
    },
    {
      "epoch": 0.5459620596205962,
      "step": 10073,
      "training_loss": 6.074693202972412
    },
    {
      "epoch": 0.5460162601626016,
      "step": 10074,
      "training_loss": 3.476099967956543
    },
    {
      "epoch": 0.5460704607046071,
      "step": 10075,
      "training_loss": 6.434319019317627
    },
    {
      "epoch": 0.5461246612466125,
      "grad_norm": 36.030845642089844,
      "learning_rate": 1e-05,
      "loss": 5.4957,
      "step": 10076
    },
    {
      "epoch": 0.5461246612466125,
      "step": 10076,
      "training_loss": 8.21731948852539
    },
    {
      "epoch": 0.5461788617886179,
      "step": 10077,
      "training_loss": 6.536273956298828
    },
    {
      "epoch": 0.5462330623306233,
      "step": 10078,
      "training_loss": 6.892163276672363
    },
    {
      "epoch": 0.5462872628726287,
      "step": 10079,
      "training_loss": 6.673051834106445
    },
    {
      "epoch": 0.5463414634146342,
      "grad_norm": 20.899208068847656,
      "learning_rate": 1e-05,
      "loss": 7.0797,
      "step": 10080
    },
    {
      "epoch": 0.5463414634146342,
      "step": 10080,
      "training_loss": 8.329710960388184
    },
    {
      "epoch": 0.5463956639566395,
      "step": 10081,
      "training_loss": 4.647604465484619
    },
    {
      "epoch": 0.546449864498645,
      "step": 10082,
      "training_loss": 6.620713233947754
    },
    {
      "epoch": 0.5465040650406504,
      "step": 10083,
      "training_loss": 5.733615398406982
    },
    {
      "epoch": 0.5465582655826559,
      "grad_norm": 23.439481735229492,
      "learning_rate": 1e-05,
      "loss": 6.3329,
      "step": 10084
    },
    {
      "epoch": 0.5465582655826559,
      "step": 10084,
      "training_loss": 5.327526569366455
    },
    {
      "epoch": 0.5466124661246613,
      "step": 10085,
      "training_loss": 6.6201171875
    },
    {
      "epoch": 0.5466666666666666,
      "step": 10086,
      "training_loss": 7.086401462554932
    },
    {
      "epoch": 0.5467208672086721,
      "step": 10087,
      "training_loss": 3.394253969192505
    },
    {
      "epoch": 0.5467750677506775,
      "grad_norm": 33.04712677001953,
      "learning_rate": 1e-05,
      "loss": 5.6071,
      "step": 10088
    },
    {
      "epoch": 0.5467750677506775,
      "step": 10088,
      "training_loss": 5.180321216583252
    },
    {
      "epoch": 0.546829268292683,
      "step": 10089,
      "training_loss": 6.785147666931152
    },
    {
      "epoch": 0.5468834688346883,
      "step": 10090,
      "training_loss": 7.585442543029785
    },
    {
      "epoch": 0.5469376693766937,
      "step": 10091,
      "training_loss": 6.096438407897949
    },
    {
      "epoch": 0.5469918699186992,
      "grad_norm": 31.262996673583984,
      "learning_rate": 1e-05,
      "loss": 6.4118,
      "step": 10092
    },
    {
      "epoch": 0.5469918699186992,
      "step": 10092,
      "training_loss": 6.926726818084717
    },
    {
      "epoch": 0.5470460704607046,
      "step": 10093,
      "training_loss": 6.76611328125
    },
    {
      "epoch": 0.5471002710027101,
      "step": 10094,
      "training_loss": 6.626926898956299
    },
    {
      "epoch": 0.5471544715447154,
      "step": 10095,
      "training_loss": 6.363091945648193
    },
    {
      "epoch": 0.5472086720867209,
      "grad_norm": 30.304609298706055,
      "learning_rate": 1e-05,
      "loss": 6.6707,
      "step": 10096
    },
    {
      "epoch": 0.5472086720867209,
      "step": 10096,
      "training_loss": 6.668227195739746
    },
    {
      "epoch": 0.5472628726287263,
      "step": 10097,
      "training_loss": 6.8100266456604
    },
    {
      "epoch": 0.5473170731707317,
      "step": 10098,
      "training_loss": 6.807910919189453
    },
    {
      "epoch": 0.5473712737127371,
      "step": 10099,
      "training_loss": 7.265864372253418
    },
    {
      "epoch": 0.5474254742547425,
      "grad_norm": 33.08173370361328,
      "learning_rate": 1e-05,
      "loss": 6.888,
      "step": 10100
    },
    {
      "epoch": 0.5474254742547425,
      "step": 10100,
      "training_loss": 7.020654201507568
    },
    {
      "epoch": 0.547479674796748,
      "step": 10101,
      "training_loss": 6.44590950012207
    },
    {
      "epoch": 0.5475338753387534,
      "step": 10102,
      "training_loss": 7.288801193237305
    },
    {
      "epoch": 0.5475880758807589,
      "step": 10103,
      "training_loss": 3.7571403980255127
    },
    {
      "epoch": 0.5476422764227642,
      "grad_norm": 40.87198257446289,
      "learning_rate": 1e-05,
      "loss": 6.1281,
      "step": 10104
    },
    {
      "epoch": 0.5476422764227642,
      "step": 10104,
      "training_loss": 7.07740592956543
    },
    {
      "epoch": 0.5476964769647696,
      "step": 10105,
      "training_loss": 6.812315464019775
    },
    {
      "epoch": 0.5477506775067751,
      "step": 10106,
      "training_loss": 6.57818603515625
    },
    {
      "epoch": 0.5478048780487805,
      "step": 10107,
      "training_loss": 6.513089656829834
    },
    {
      "epoch": 0.5478590785907859,
      "grad_norm": 72.26758575439453,
      "learning_rate": 1e-05,
      "loss": 6.7452,
      "step": 10108
    },
    {
      "epoch": 0.5478590785907859,
      "step": 10108,
      "training_loss": 6.9896321296691895
    },
    {
      "epoch": 0.5479132791327913,
      "step": 10109,
      "training_loss": 7.000930309295654
    },
    {
      "epoch": 0.5479674796747968,
      "step": 10110,
      "training_loss": 5.165087699890137
    },
    {
      "epoch": 0.5480216802168022,
      "step": 10111,
      "training_loss": 5.7934417724609375
    },
    {
      "epoch": 0.5480758807588076,
      "grad_norm": 38.297157287597656,
      "learning_rate": 1e-05,
      "loss": 6.2373,
      "step": 10112
    },
    {
      "epoch": 0.5480758807588076,
      "step": 10112,
      "training_loss": 5.819374084472656
    },
    {
      "epoch": 0.548130081300813,
      "step": 10113,
      "training_loss": 7.454476356506348
    },
    {
      "epoch": 0.5481842818428184,
      "step": 10114,
      "training_loss": 7.066938877105713
    },
    {
      "epoch": 0.5482384823848239,
      "step": 10115,
      "training_loss": 6.4179816246032715
    },
    {
      "epoch": 0.5482926829268293,
      "grad_norm": 42.08332443237305,
      "learning_rate": 1e-05,
      "loss": 6.6897,
      "step": 10116
    },
    {
      "epoch": 0.5482926829268293,
      "step": 10116,
      "training_loss": 7.091528415679932
    },
    {
      "epoch": 0.5483468834688346,
      "step": 10117,
      "training_loss": 6.917365550994873
    },
    {
      "epoch": 0.5484010840108401,
      "step": 10118,
      "training_loss": 6.854114532470703
    },
    {
      "epoch": 0.5484552845528455,
      "step": 10119,
      "training_loss": 7.7164764404296875
    },
    {
      "epoch": 0.548509485094851,
      "grad_norm": 52.095115661621094,
      "learning_rate": 1e-05,
      "loss": 7.1449,
      "step": 10120
    },
    {
      "epoch": 0.548509485094851,
      "step": 10120,
      "training_loss": 7.402437686920166
    },
    {
      "epoch": 0.5485636856368564,
      "step": 10121,
      "training_loss": 6.829827308654785
    },
    {
      "epoch": 0.5486178861788618,
      "step": 10122,
      "training_loss": 7.291697025299072
    },
    {
      "epoch": 0.5486720867208672,
      "step": 10123,
      "training_loss": 7.239345073699951
    },
    {
      "epoch": 0.5487262872628726,
      "grad_norm": 22.859304428100586,
      "learning_rate": 1e-05,
      "loss": 7.1908,
      "step": 10124
    },
    {
      "epoch": 0.5487262872628726,
      "step": 10124,
      "training_loss": 5.323678016662598
    },
    {
      "epoch": 0.5487804878048781,
      "step": 10125,
      "training_loss": 7.209267616271973
    },
    {
      "epoch": 0.5488346883468834,
      "step": 10126,
      "training_loss": 7.533291339874268
    },
    {
      "epoch": 0.5488888888888889,
      "step": 10127,
      "training_loss": 8.132050514221191
    },
    {
      "epoch": 0.5489430894308943,
      "grad_norm": 25.332365036010742,
      "learning_rate": 1e-05,
      "loss": 7.0496,
      "step": 10128
    },
    {
      "epoch": 0.5489430894308943,
      "step": 10128,
      "training_loss": 4.716519355773926
    },
    {
      "epoch": 0.5489972899728998,
      "step": 10129,
      "training_loss": 6.123331069946289
    },
    {
      "epoch": 0.5490514905149051,
      "step": 10130,
      "training_loss": 3.5836381912231445
    },
    {
      "epoch": 0.5491056910569105,
      "step": 10131,
      "training_loss": 7.803130149841309
    },
    {
      "epoch": 0.549159891598916,
      "grad_norm": 27.697948455810547,
      "learning_rate": 1e-05,
      "loss": 5.5567,
      "step": 10132
    },
    {
      "epoch": 0.549159891598916,
      "step": 10132,
      "training_loss": 2.977940559387207
    },
    {
      "epoch": 0.5492140921409214,
      "step": 10133,
      "training_loss": 6.47655725479126
    },
    {
      "epoch": 0.5492682926829269,
      "step": 10134,
      "training_loss": 7.011092185974121
    },
    {
      "epoch": 0.5493224932249322,
      "step": 10135,
      "training_loss": 7.331273078918457
    },
    {
      "epoch": 0.5493766937669377,
      "grad_norm": 20.23733139038086,
      "learning_rate": 1e-05,
      "loss": 5.9492,
      "step": 10136
    },
    {
      "epoch": 0.5493766937669377,
      "step": 10136,
      "training_loss": 5.708622455596924
    },
    {
      "epoch": 0.5494308943089431,
      "step": 10137,
      "training_loss": 6.706066131591797
    },
    {
      "epoch": 0.5494850948509485,
      "step": 10138,
      "training_loss": 5.452096939086914
    },
    {
      "epoch": 0.5495392953929539,
      "step": 10139,
      "training_loss": 3.6669535636901855
    },
    {
      "epoch": 0.5495934959349593,
      "grad_norm": 33.21770095825195,
      "learning_rate": 1e-05,
      "loss": 5.3834,
      "step": 10140
    },
    {
      "epoch": 0.5495934959349593,
      "step": 10140,
      "training_loss": 6.714145183563232
    },
    {
      "epoch": 0.5496476964769648,
      "step": 10141,
      "training_loss": 5.791875839233398
    },
    {
      "epoch": 0.5497018970189702,
      "step": 10142,
      "training_loss": 5.705471515655518
    },
    {
      "epoch": 0.5497560975609757,
      "step": 10143,
      "training_loss": 8.106451988220215
    },
    {
      "epoch": 0.549810298102981,
      "grad_norm": 24.004688262939453,
      "learning_rate": 1e-05,
      "loss": 6.5795,
      "step": 10144
    },
    {
      "epoch": 0.549810298102981,
      "step": 10144,
      "training_loss": 6.899320602416992
    },
    {
      "epoch": 0.5498644986449864,
      "step": 10145,
      "training_loss": 6.917535305023193
    },
    {
      "epoch": 0.5499186991869919,
      "step": 10146,
      "training_loss": 7.3010969161987305
    },
    {
      "epoch": 0.5499728997289973,
      "step": 10147,
      "training_loss": 7.187376499176025
    },
    {
      "epoch": 0.5500271002710027,
      "grad_norm": 36.7945671081543,
      "learning_rate": 1e-05,
      "loss": 7.0763,
      "step": 10148
    },
    {
      "epoch": 0.5500271002710027,
      "step": 10148,
      "training_loss": 6.143848419189453
    },
    {
      "epoch": 0.5500813008130081,
      "step": 10149,
      "training_loss": 6.353063583374023
    },
    {
      "epoch": 0.5501355013550135,
      "step": 10150,
      "training_loss": 6.639336585998535
    },
    {
      "epoch": 0.550189701897019,
      "step": 10151,
      "training_loss": 7.526693344116211
    },
    {
      "epoch": 0.5502439024390244,
      "grad_norm": 30.859962463378906,
      "learning_rate": 1e-05,
      "loss": 6.6657,
      "step": 10152
    },
    {
      "epoch": 0.5502439024390244,
      "step": 10152,
      "training_loss": 7.201488494873047
    },
    {
      "epoch": 0.5502981029810298,
      "step": 10153,
      "training_loss": 6.628012657165527
    },
    {
      "epoch": 0.5503523035230352,
      "step": 10154,
      "training_loss": 5.558785438537598
    },
    {
      "epoch": 0.5504065040650407,
      "step": 10155,
      "training_loss": 6.957876682281494
    },
    {
      "epoch": 0.5504607046070461,
      "grad_norm": 31.963502883911133,
      "learning_rate": 1e-05,
      "loss": 6.5865,
      "step": 10156
    },
    {
      "epoch": 0.5504607046070461,
      "step": 10156,
      "training_loss": 5.725583076477051
    },
    {
      "epoch": 0.5505149051490514,
      "step": 10157,
      "training_loss": 6.074495792388916
    },
    {
      "epoch": 0.5505691056910569,
      "step": 10158,
      "training_loss": 7.196175575256348
    },
    {
      "epoch": 0.5506233062330623,
      "step": 10159,
      "training_loss": 7.350490570068359
    },
    {
      "epoch": 0.5506775067750678,
      "grad_norm": 48.960201263427734,
      "learning_rate": 1e-05,
      "loss": 6.5867,
      "step": 10160
    },
    {
      "epoch": 0.5506775067750678,
      "step": 10160,
      "training_loss": 6.710953712463379
    },
    {
      "epoch": 0.5507317073170732,
      "step": 10161,
      "training_loss": 8.08621597290039
    },
    {
      "epoch": 0.5507859078590785,
      "step": 10162,
      "training_loss": 6.506458759307861
    },
    {
      "epoch": 0.550840108401084,
      "step": 10163,
      "training_loss": 7.005580425262451
    },
    {
      "epoch": 0.5508943089430894,
      "grad_norm": 42.82406997680664,
      "learning_rate": 1e-05,
      "loss": 7.0773,
      "step": 10164
    },
    {
      "epoch": 0.5508943089430894,
      "step": 10164,
      "training_loss": 6.3521294593811035
    },
    {
      "epoch": 0.5509485094850949,
      "step": 10165,
      "training_loss": 6.678192138671875
    },
    {
      "epoch": 0.5510027100271002,
      "step": 10166,
      "training_loss": 3.2141847610473633
    },
    {
      "epoch": 0.5510569105691057,
      "step": 10167,
      "training_loss": 6.859452247619629
    },
    {
      "epoch": 0.5511111111111111,
      "grad_norm": 18.35820198059082,
      "learning_rate": 1e-05,
      "loss": 5.776,
      "step": 10168
    },
    {
      "epoch": 0.5511111111111111,
      "step": 10168,
      "training_loss": 7.446741104125977
    },
    {
      "epoch": 0.5511653116531166,
      "step": 10169,
      "training_loss": 7.28303337097168
    },
    {
      "epoch": 0.551219512195122,
      "step": 10170,
      "training_loss": 7.552099227905273
    },
    {
      "epoch": 0.5512737127371273,
      "step": 10171,
      "training_loss": 6.942210674285889
    },
    {
      "epoch": 0.5513279132791328,
      "grad_norm": 22.66388702392578,
      "learning_rate": 1e-05,
      "loss": 7.306,
      "step": 10172
    },
    {
      "epoch": 0.5513279132791328,
      "step": 10172,
      "training_loss": 6.207724094390869
    },
    {
      "epoch": 0.5513821138211382,
      "step": 10173,
      "training_loss": 7.081358909606934
    },
    {
      "epoch": 0.5514363143631437,
      "step": 10174,
      "training_loss": 7.242644309997559
    },
    {
      "epoch": 0.551490514905149,
      "step": 10175,
      "training_loss": 6.4917802810668945
    },
    {
      "epoch": 0.5515447154471544,
      "grad_norm": 24.03855323791504,
      "learning_rate": 1e-05,
      "loss": 6.7559,
      "step": 10176
    },
    {
      "epoch": 0.5515447154471544,
      "step": 10176,
      "training_loss": 6.4303717613220215
    },
    {
      "epoch": 0.5515989159891599,
      "step": 10177,
      "training_loss": 3.129624366760254
    },
    {
      "epoch": 0.5516531165311653,
      "step": 10178,
      "training_loss": 7.365009307861328
    },
    {
      "epoch": 0.5517073170731708,
      "step": 10179,
      "training_loss": 5.969769477844238
    },
    {
      "epoch": 0.5517615176151761,
      "grad_norm": 27.74235725402832,
      "learning_rate": 1e-05,
      "loss": 5.7237,
      "step": 10180
    },
    {
      "epoch": 0.5517615176151761,
      "step": 10180,
      "training_loss": 6.267726421356201
    },
    {
      "epoch": 0.5518157181571816,
      "step": 10181,
      "training_loss": 6.862852573394775
    },
    {
      "epoch": 0.551869918699187,
      "step": 10182,
      "training_loss": 5.598127365112305
    },
    {
      "epoch": 0.5519241192411924,
      "step": 10183,
      "training_loss": 5.738584041595459
    },
    {
      "epoch": 0.5519783197831978,
      "grad_norm": 18.586952209472656,
      "learning_rate": 1e-05,
      "loss": 6.1168,
      "step": 10184
    },
    {
      "epoch": 0.5519783197831978,
      "step": 10184,
      "training_loss": 7.6404290199279785
    },
    {
      "epoch": 0.5520325203252032,
      "step": 10185,
      "training_loss": 7.730036735534668
    },
    {
      "epoch": 0.5520867208672087,
      "step": 10186,
      "training_loss": 6.995104789733887
    },
    {
      "epoch": 0.5521409214092141,
      "step": 10187,
      "training_loss": 5.86696195602417
    },
    {
      "epoch": 0.5521951219512196,
      "grad_norm": 36.53228759765625,
      "learning_rate": 1e-05,
      "loss": 7.0581,
      "step": 10188
    },
    {
      "epoch": 0.5521951219512196,
      "step": 10188,
      "training_loss": 6.770162105560303
    },
    {
      "epoch": 0.5522493224932249,
      "step": 10189,
      "training_loss": 5.670939922332764
    },
    {
      "epoch": 0.5523035230352303,
      "step": 10190,
      "training_loss": 7.17880392074585
    },
    {
      "epoch": 0.5523577235772358,
      "step": 10191,
      "training_loss": 8.075448036193848
    },
    {
      "epoch": 0.5524119241192412,
      "grad_norm": 30.022159576416016,
      "learning_rate": 1e-05,
      "loss": 6.9238,
      "step": 10192
    },
    {
      "epoch": 0.5524119241192412,
      "step": 10192,
      "training_loss": 7.173426628112793
    },
    {
      "epoch": 0.5524661246612466,
      "step": 10193,
      "training_loss": 7.467563629150391
    },
    {
      "epoch": 0.552520325203252,
      "step": 10194,
      "training_loss": 7.242402076721191
    },
    {
      "epoch": 0.5525745257452574,
      "step": 10195,
      "training_loss": 7.853885173797607
    },
    {
      "epoch": 0.5526287262872629,
      "grad_norm": 35.13573455810547,
      "learning_rate": 1e-05,
      "loss": 7.4343,
      "step": 10196
    },
    {
      "epoch": 0.5526287262872629,
      "step": 10196,
      "training_loss": 4.591731071472168
    },
    {
      "epoch": 0.5526829268292683,
      "step": 10197,
      "training_loss": 4.850244045257568
    },
    {
      "epoch": 0.5527371273712737,
      "step": 10198,
      "training_loss": 6.763851642608643
    },
    {
      "epoch": 0.5527913279132791,
      "step": 10199,
      "training_loss": 7.114096641540527
    },
    {
      "epoch": 0.5528455284552846,
      "grad_norm": 35.83285140991211,
      "learning_rate": 1e-05,
      "loss": 5.83,
      "step": 10200
    },
    {
      "epoch": 0.5528455284552846,
      "step": 10200,
      "training_loss": 5.474304676055908
    },
    {
      "epoch": 0.55289972899729,
      "step": 10201,
      "training_loss": 7.591050148010254
    },
    {
      "epoch": 0.5529539295392953,
      "step": 10202,
      "training_loss": 5.960597991943359
    },
    {
      "epoch": 0.5530081300813008,
      "step": 10203,
      "training_loss": 8.88579273223877
    },
    {
      "epoch": 0.5530623306233062,
      "grad_norm": 60.52781677246094,
      "learning_rate": 1e-05,
      "loss": 6.9779,
      "step": 10204
    },
    {
      "epoch": 0.5530623306233062,
      "step": 10204,
      "training_loss": 6.438190460205078
    },
    {
      "epoch": 0.5531165311653117,
      "step": 10205,
      "training_loss": 5.049415111541748
    },
    {
      "epoch": 0.5531707317073171,
      "step": 10206,
      "training_loss": 7.946346759796143
    },
    {
      "epoch": 0.5532249322493225,
      "step": 10207,
      "training_loss": 6.730273246765137
    },
    {
      "epoch": 0.5532791327913279,
      "grad_norm": 20.127161026000977,
      "learning_rate": 1e-05,
      "loss": 6.5411,
      "step": 10208
    },
    {
      "epoch": 0.5532791327913279,
      "step": 10208,
      "training_loss": 6.131056785583496
    },
    {
      "epoch": 0.5533333333333333,
      "step": 10209,
      "training_loss": 7.221526622772217
    },
    {
      "epoch": 0.5533875338753388,
      "step": 10210,
      "training_loss": 6.1739301681518555
    },
    {
      "epoch": 0.5534417344173441,
      "step": 10211,
      "training_loss": 6.847011089324951
    },
    {
      "epoch": 0.5534959349593496,
      "grad_norm": 38.90782165527344,
      "learning_rate": 1e-05,
      "loss": 6.5934,
      "step": 10212
    },
    {
      "epoch": 0.5534959349593496,
      "step": 10212,
      "training_loss": 7.951044082641602
    },
    {
      "epoch": 0.553550135501355,
      "step": 10213,
      "training_loss": 7.127629280090332
    },
    {
      "epoch": 0.5536043360433605,
      "step": 10214,
      "training_loss": 6.188093185424805
    },
    {
      "epoch": 0.5536585365853659,
      "step": 10215,
      "training_loss": 7.174495697021484
    },
    {
      "epoch": 0.5537127371273712,
      "grad_norm": 26.495628356933594,
      "learning_rate": 1e-05,
      "loss": 7.1103,
      "step": 10216
    },
    {
      "epoch": 0.5537127371273712,
      "step": 10216,
      "training_loss": 6.144722938537598
    },
    {
      "epoch": 0.5537669376693767,
      "step": 10217,
      "training_loss": 5.825077056884766
    },
    {
      "epoch": 0.5538211382113821,
      "step": 10218,
      "training_loss": 7.872002601623535
    },
    {
      "epoch": 0.5538753387533876,
      "step": 10219,
      "training_loss": 7.355203628540039
    },
    {
      "epoch": 0.5539295392953929,
      "grad_norm": 22.19012451171875,
      "learning_rate": 1e-05,
      "loss": 6.7993,
      "step": 10220
    },
    {
      "epoch": 0.5539295392953929,
      "step": 10220,
      "training_loss": 6.8510236740112305
    },
    {
      "epoch": 0.5539837398373983,
      "step": 10221,
      "training_loss": 7.213933944702148
    },
    {
      "epoch": 0.5540379403794038,
      "step": 10222,
      "training_loss": 5.649324417114258
    },
    {
      "epoch": 0.5540921409214092,
      "step": 10223,
      "training_loss": 6.15371036529541
    },
    {
      "epoch": 0.5541463414634147,
      "grad_norm": 25.639068603515625,
      "learning_rate": 1e-05,
      "loss": 6.467,
      "step": 10224
    },
    {
      "epoch": 0.5541463414634147,
      "step": 10224,
      "training_loss": 8.149883270263672
    },
    {
      "epoch": 0.55420054200542,
      "step": 10225,
      "training_loss": 6.974882125854492
    },
    {
      "epoch": 0.5542547425474255,
      "step": 10226,
      "training_loss": 6.7808756828308105
    },
    {
      "epoch": 0.5543089430894309,
      "step": 10227,
      "training_loss": 6.1211371421813965
    },
    {
      "epoch": 0.5543631436314364,
      "grad_norm": 30.42864227294922,
      "learning_rate": 1e-05,
      "loss": 7.0067,
      "step": 10228
    },
    {
      "epoch": 0.5543631436314364,
      "step": 10228,
      "training_loss": 7.667081356048584
    },
    {
      "epoch": 0.5544173441734417,
      "step": 10229,
      "training_loss": 7.3695387840271
    },
    {
      "epoch": 0.5544715447154471,
      "step": 10230,
      "training_loss": 7.2501959800720215
    },
    {
      "epoch": 0.5545257452574526,
      "step": 10231,
      "training_loss": 6.532341957092285
    },
    {
      "epoch": 0.554579945799458,
      "grad_norm": 38.46168899536133,
      "learning_rate": 1e-05,
      "loss": 7.2048,
      "step": 10232
    },
    {
      "epoch": 0.554579945799458,
      "step": 10232,
      "training_loss": 5.366608142852783
    },
    {
      "epoch": 0.5546341463414635,
      "step": 10233,
      "training_loss": 6.731403827667236
    },
    {
      "epoch": 0.5546883468834688,
      "step": 10234,
      "training_loss": 5.537346839904785
    },
    {
      "epoch": 0.5547425474254742,
      "step": 10235,
      "training_loss": 7.446096897125244
    },
    {
      "epoch": 0.5547967479674797,
      "grad_norm": 24.86814308166504,
      "learning_rate": 1e-05,
      "loss": 6.2704,
      "step": 10236
    },
    {
      "epoch": 0.5547967479674797,
      "step": 10236,
      "training_loss": 7.42139196395874
    },
    {
      "epoch": 0.5548509485094851,
      "step": 10237,
      "training_loss": 6.238451957702637
    },
    {
      "epoch": 0.5549051490514905,
      "step": 10238,
      "training_loss": 5.147407531738281
    },
    {
      "epoch": 0.5549593495934959,
      "step": 10239,
      "training_loss": 6.117430210113525
    },
    {
      "epoch": 0.5550135501355014,
      "grad_norm": 25.291309356689453,
      "learning_rate": 1e-05,
      "loss": 6.2312,
      "step": 10240
    },
    {
      "epoch": 0.5550135501355014,
      "step": 10240,
      "training_loss": 6.143322944641113
    },
    {
      "epoch": 0.5550677506775068,
      "step": 10241,
      "training_loss": 6.7071943283081055
    },
    {
      "epoch": 0.5551219512195122,
      "step": 10242,
      "training_loss": 6.950464248657227
    },
    {
      "epoch": 0.5551761517615176,
      "step": 10243,
      "training_loss": 6.092950344085693
    },
    {
      "epoch": 0.555230352303523,
      "grad_norm": 27.62071418762207,
      "learning_rate": 1e-05,
      "loss": 6.4735,
      "step": 10244
    },
    {
      "epoch": 0.555230352303523,
      "step": 10244,
      "training_loss": 7.2164201736450195
    },
    {
      "epoch": 0.5552845528455285,
      "step": 10245,
      "training_loss": 7.302879333496094
    },
    {
      "epoch": 0.5553387533875339,
      "step": 10246,
      "training_loss": 7.927268028259277
    },
    {
      "epoch": 0.5553929539295392,
      "step": 10247,
      "training_loss": 5.859294891357422
    },
    {
      "epoch": 0.5554471544715447,
      "grad_norm": 21.62462043762207,
      "learning_rate": 1e-05,
      "loss": 7.0765,
      "step": 10248
    },
    {
      "epoch": 0.5554471544715447,
      "step": 10248,
      "training_loss": 6.66030216217041
    },
    {
      "epoch": 0.5555013550135501,
      "step": 10249,
      "training_loss": 7.778261661529541
    },
    {
      "epoch": 0.5555555555555556,
      "step": 10250,
      "training_loss": 5.894133567810059
    },
    {
      "epoch": 0.555609756097561,
      "step": 10251,
      "training_loss": 6.013963222503662
    },
    {
      "epoch": 0.5556639566395664,
      "grad_norm": 44.12455749511719,
      "learning_rate": 1e-05,
      "loss": 6.5867,
      "step": 10252
    },
    {
      "epoch": 0.5556639566395664,
      "step": 10252,
      "training_loss": 7.91405725479126
    },
    {
      "epoch": 0.5557181571815718,
      "step": 10253,
      "training_loss": 5.891777515411377
    },
    {
      "epoch": 0.5557723577235772,
      "step": 10254,
      "training_loss": 6.656589031219482
    },
    {
      "epoch": 0.5558265582655827,
      "step": 10255,
      "training_loss": 6.6174092292785645
    },
    {
      "epoch": 0.555880758807588,
      "grad_norm": 17.71992301940918,
      "learning_rate": 1e-05,
      "loss": 6.77,
      "step": 10256
    },
    {
      "epoch": 0.555880758807588,
      "step": 10256,
      "training_loss": 6.714356899261475
    },
    {
      "epoch": 0.5559349593495935,
      "step": 10257,
      "training_loss": 5.259883880615234
    },
    {
      "epoch": 0.5559891598915989,
      "step": 10258,
      "training_loss": 7.333856105804443
    },
    {
      "epoch": 0.5560433604336044,
      "step": 10259,
      "training_loss": 6.59744119644165
    },
    {
      "epoch": 0.5560975609756098,
      "grad_norm": 21.574060440063477,
      "learning_rate": 1e-05,
      "loss": 6.4764,
      "step": 10260
    },
    {
      "epoch": 0.5560975609756098,
      "step": 10260,
      "training_loss": 8.33283519744873
    },
    {
      "epoch": 0.5561517615176151,
      "step": 10261,
      "training_loss": 7.186168670654297
    },
    {
      "epoch": 0.5562059620596206,
      "step": 10262,
      "training_loss": 5.201061248779297
    },
    {
      "epoch": 0.556260162601626,
      "step": 10263,
      "training_loss": 5.112277984619141
    },
    {
      "epoch": 0.5563143631436315,
      "grad_norm": 20.4532527923584,
      "learning_rate": 1e-05,
      "loss": 6.4581,
      "step": 10264
    },
    {
      "epoch": 0.5563143631436315,
      "step": 10264,
      "training_loss": 6.803074359893799
    },
    {
      "epoch": 0.5563685636856368,
      "step": 10265,
      "training_loss": 7.433464527130127
    },
    {
      "epoch": 0.5564227642276423,
      "step": 10266,
      "training_loss": 6.7972025871276855
    },
    {
      "epoch": 0.5564769647696477,
      "step": 10267,
      "training_loss": 7.178928852081299
    },
    {
      "epoch": 0.5565311653116531,
      "grad_norm": 22.50616455078125,
      "learning_rate": 1e-05,
      "loss": 7.0532,
      "step": 10268
    },
    {
      "epoch": 0.5565311653116531,
      "step": 10268,
      "training_loss": 7.319755554199219
    },
    {
      "epoch": 0.5565853658536586,
      "step": 10269,
      "training_loss": 5.821589469909668
    },
    {
      "epoch": 0.5566395663956639,
      "step": 10270,
      "training_loss": 6.731960773468018
    },
    {
      "epoch": 0.5566937669376694,
      "step": 10271,
      "training_loss": 5.361306667327881
    },
    {
      "epoch": 0.5567479674796748,
      "grad_norm": 30.69222640991211,
      "learning_rate": 1e-05,
      "loss": 6.3087,
      "step": 10272
    },
    {
      "epoch": 0.5567479674796748,
      "step": 10272,
      "training_loss": 6.04269552230835
    },
    {
      "epoch": 0.5568021680216803,
      "step": 10273,
      "training_loss": 7.087437152862549
    },
    {
      "epoch": 0.5568563685636856,
      "step": 10274,
      "training_loss": 7.310868263244629
    },
    {
      "epoch": 0.556910569105691,
      "step": 10275,
      "training_loss": 5.809119701385498
    },
    {
      "epoch": 0.5569647696476965,
      "grad_norm": 26.33839225769043,
      "learning_rate": 1e-05,
      "loss": 6.5625,
      "step": 10276
    },
    {
      "epoch": 0.5569647696476965,
      "step": 10276,
      "training_loss": 6.614248275756836
    },
    {
      "epoch": 0.5570189701897019,
      "step": 10277,
      "training_loss": 6.851939678192139
    },
    {
      "epoch": 0.5570731707317074,
      "step": 10278,
      "training_loss": 7.000573635101318
    },
    {
      "epoch": 0.5571273712737127,
      "step": 10279,
      "training_loss": 6.464057445526123
    },
    {
      "epoch": 0.5571815718157181,
      "grad_norm": 19.93478012084961,
      "learning_rate": 1e-05,
      "loss": 6.7327,
      "step": 10280
    },
    {
      "epoch": 0.5571815718157181,
      "step": 10280,
      "training_loss": 7.106313705444336
    },
    {
      "epoch": 0.5572357723577236,
      "step": 10281,
      "training_loss": 6.940623760223389
    },
    {
      "epoch": 0.557289972899729,
      "step": 10282,
      "training_loss": 6.147639751434326
    },
    {
      "epoch": 0.5573441734417344,
      "step": 10283,
      "training_loss": 7.097616195678711
    },
    {
      "epoch": 0.5573983739837398,
      "grad_norm": 28.331865310668945,
      "learning_rate": 1e-05,
      "loss": 6.823,
      "step": 10284
    },
    {
      "epoch": 0.5573983739837398,
      "step": 10284,
      "training_loss": 4.437939643859863
    },
    {
      "epoch": 0.5574525745257453,
      "step": 10285,
      "training_loss": 7.572988510131836
    },
    {
      "epoch": 0.5575067750677507,
      "step": 10286,
      "training_loss": 3.9536550045013428
    },
    {
      "epoch": 0.5575609756097561,
      "step": 10287,
      "training_loss": 6.76115608215332
    },
    {
      "epoch": 0.5576151761517615,
      "grad_norm": 25.983631134033203,
      "learning_rate": 1e-05,
      "loss": 5.6814,
      "step": 10288
    },
    {
      "epoch": 0.5576151761517615,
      "step": 10288,
      "training_loss": 6.645658016204834
    },
    {
      "epoch": 0.5576693766937669,
      "step": 10289,
      "training_loss": 5.441540241241455
    },
    {
      "epoch": 0.5577235772357724,
      "step": 10290,
      "training_loss": 7.815933704376221
    },
    {
      "epoch": 0.5577777777777778,
      "step": 10291,
      "training_loss": 6.126594543457031
    },
    {
      "epoch": 0.5578319783197832,
      "grad_norm": 28.869827270507812,
      "learning_rate": 1e-05,
      "loss": 6.5074,
      "step": 10292
    },
    {
      "epoch": 0.5578319783197832,
      "step": 10292,
      "training_loss": 5.940779685974121
    },
    {
      "epoch": 0.5578861788617886,
      "step": 10293,
      "training_loss": 6.427518844604492
    },
    {
      "epoch": 0.557940379403794,
      "step": 10294,
      "training_loss": 5.394731521606445
    },
    {
      "epoch": 0.5579945799457995,
      "step": 10295,
      "training_loss": 8.041586875915527
    },
    {
      "epoch": 0.5580487804878049,
      "grad_norm": 37.89105224609375,
      "learning_rate": 1e-05,
      "loss": 6.4512,
      "step": 10296
    },
    {
      "epoch": 0.5580487804878049,
      "step": 10296,
      "training_loss": 6.161736011505127
    },
    {
      "epoch": 0.5581029810298103,
      "step": 10297,
      "training_loss": 5.8794121742248535
    },
    {
      "epoch": 0.5581571815718157,
      "step": 10298,
      "training_loss": 4.607844352722168
    },
    {
      "epoch": 0.5582113821138212,
      "step": 10299,
      "training_loss": 8.115015029907227
    },
    {
      "epoch": 0.5582655826558266,
      "grad_norm": 40.2039794921875,
      "learning_rate": 1e-05,
      "loss": 6.191,
      "step": 10300
    },
    {
      "epoch": 0.5582655826558266,
      "step": 10300,
      "training_loss": 4.77564001083374
    },
    {
      "epoch": 0.5583197831978319,
      "step": 10301,
      "training_loss": 6.473740577697754
    },
    {
      "epoch": 0.5583739837398374,
      "step": 10302,
      "training_loss": 3.1783080101013184
    },
    {
      "epoch": 0.5584281842818428,
      "step": 10303,
      "training_loss": 7.223475456237793
    },
    {
      "epoch": 0.5584823848238483,
      "grad_norm": 49.873130798339844,
      "learning_rate": 1e-05,
      "loss": 5.4128,
      "step": 10304
    },
    {
      "epoch": 0.5584823848238483,
      "step": 10304,
      "training_loss": 3.4612081050872803
    },
    {
      "epoch": 0.5585365853658537,
      "step": 10305,
      "training_loss": 6.840085506439209
    },
    {
      "epoch": 0.558590785907859,
      "step": 10306,
      "training_loss": 6.98634147644043
    },
    {
      "epoch": 0.5586449864498645,
      "step": 10307,
      "training_loss": 5.260972023010254
    },
    {
      "epoch": 0.5586991869918699,
      "grad_norm": 52.3125,
      "learning_rate": 1e-05,
      "loss": 5.6372,
      "step": 10308
    },
    {
      "epoch": 0.5586991869918699,
      "step": 10308,
      "training_loss": 6.815856456756592
    },
    {
      "epoch": 0.5587533875338754,
      "step": 10309,
      "training_loss": 7.400420188903809
    },
    {
      "epoch": 0.5588075880758807,
      "step": 10310,
      "training_loss": 6.3393731117248535
    },
    {
      "epoch": 0.5588617886178862,
      "step": 10311,
      "training_loss": 4.9536848068237305
    },
    {
      "epoch": 0.5589159891598916,
      "grad_norm": 24.86861228942871,
      "learning_rate": 1e-05,
      "loss": 6.3773,
      "step": 10312
    },
    {
      "epoch": 0.5589159891598916,
      "step": 10312,
      "training_loss": 7.243371486663818
    },
    {
      "epoch": 0.558970189701897,
      "step": 10313,
      "training_loss": 6.373915672302246
    },
    {
      "epoch": 0.5590243902439025,
      "step": 10314,
      "training_loss": 7.870254039764404
    },
    {
      "epoch": 0.5590785907859078,
      "step": 10315,
      "training_loss": 6.8140130043029785
    },
    {
      "epoch": 0.5591327913279133,
      "grad_norm": 28.60189437866211,
      "learning_rate": 1e-05,
      "loss": 7.0754,
      "step": 10316
    },
    {
      "epoch": 0.5591327913279133,
      "step": 10316,
      "training_loss": 6.8944091796875
    },
    {
      "epoch": 0.5591869918699187,
      "step": 10317,
      "training_loss": 5.962188243865967
    },
    {
      "epoch": 0.5592411924119242,
      "step": 10318,
      "training_loss": 6.62021017074585
    },
    {
      "epoch": 0.5592953929539295,
      "step": 10319,
      "training_loss": 7.207704067230225
    },
    {
      "epoch": 0.5593495934959349,
      "grad_norm": 29.086078643798828,
      "learning_rate": 1e-05,
      "loss": 6.6711,
      "step": 10320
    },
    {
      "epoch": 0.5593495934959349,
      "step": 10320,
      "training_loss": 8.283299446105957
    },
    {
      "epoch": 0.5594037940379404,
      "step": 10321,
      "training_loss": 7.898664951324463
    },
    {
      "epoch": 0.5594579945799458,
      "step": 10322,
      "training_loss": 6.7285003662109375
    },
    {
      "epoch": 0.5595121951219513,
      "step": 10323,
      "training_loss": 7.0810441970825195
    },
    {
      "epoch": 0.5595663956639566,
      "grad_norm": 16.439058303833008,
      "learning_rate": 1e-05,
      "loss": 7.4979,
      "step": 10324
    },
    {
      "epoch": 0.5595663956639566,
      "step": 10324,
      "training_loss": 7.337147235870361
    },
    {
      "epoch": 0.559620596205962,
      "step": 10325,
      "training_loss": 7.247504234313965
    },
    {
      "epoch": 0.5596747967479675,
      "step": 10326,
      "training_loss": 4.712551593780518
    },
    {
      "epoch": 0.5597289972899729,
      "step": 10327,
      "training_loss": 7.041409015655518
    },
    {
      "epoch": 0.5597831978319783,
      "grad_norm": 18.58390235900879,
      "learning_rate": 1e-05,
      "loss": 6.5847,
      "step": 10328
    },
    {
      "epoch": 0.5597831978319783,
      "step": 10328,
      "training_loss": 5.730121612548828
    },
    {
      "epoch": 0.5598373983739837,
      "step": 10329,
      "training_loss": 6.646227836608887
    },
    {
      "epoch": 0.5598915989159892,
      "step": 10330,
      "training_loss": 6.430820465087891
    },
    {
      "epoch": 0.5599457994579946,
      "step": 10331,
      "training_loss": 6.761920928955078
    },
    {
      "epoch": 0.56,
      "grad_norm": 28.716543197631836,
      "learning_rate": 1e-05,
      "loss": 6.3923,
      "step": 10332
    },
    {
      "epoch": 0.56,
      "step": 10332,
      "training_loss": 5.394380569458008
    },
    {
      "epoch": 0.5600542005420054,
      "step": 10333,
      "training_loss": 5.263134479522705
    },
    {
      "epoch": 0.5601084010840108,
      "step": 10334,
      "training_loss": 7.283895969390869
    },
    {
      "epoch": 0.5601626016260163,
      "step": 10335,
      "training_loss": 5.085348129272461
    },
    {
      "epoch": 0.5602168021680217,
      "grad_norm": 36.284420013427734,
      "learning_rate": 1e-05,
      "loss": 5.7567,
      "step": 10336
    },
    {
      "epoch": 0.5602168021680217,
      "step": 10336,
      "training_loss": 4.186120986938477
    },
    {
      "epoch": 0.560271002710027,
      "step": 10337,
      "training_loss": 6.635748863220215
    },
    {
      "epoch": 0.5603252032520325,
      "step": 10338,
      "training_loss": 8.433313369750977
    },
    {
      "epoch": 0.560379403794038,
      "step": 10339,
      "training_loss": 7.835522651672363
    },
    {
      "epoch": 0.5604336043360434,
      "grad_norm": 30.4494571685791,
      "learning_rate": 1e-05,
      "loss": 6.7727,
      "step": 10340
    },
    {
      "epoch": 0.5604336043360434,
      "step": 10340,
      "training_loss": 7.07396936416626
    },
    {
      "epoch": 0.5604878048780488,
      "step": 10341,
      "training_loss": 7.971859455108643
    },
    {
      "epoch": 0.5605420054200542,
      "step": 10342,
      "training_loss": 7.957704544067383
    },
    {
      "epoch": 0.5605962059620596,
      "step": 10343,
      "training_loss": 6.5305047035217285
    },
    {
      "epoch": 0.5606504065040651,
      "grad_norm": 31.92664337158203,
      "learning_rate": 1e-05,
      "loss": 7.3835,
      "step": 10344
    },
    {
      "epoch": 0.5606504065040651,
      "step": 10344,
      "training_loss": 7.814095973968506
    },
    {
      "epoch": 0.5607046070460705,
      "step": 10345,
      "training_loss": 8.150463104248047
    },
    {
      "epoch": 0.5607588075880758,
      "step": 10346,
      "training_loss": 6.864407062530518
    },
    {
      "epoch": 0.5608130081300813,
      "step": 10347,
      "training_loss": 6.665713310241699
    },
    {
      "epoch": 0.5608672086720867,
      "grad_norm": 37.638275146484375,
      "learning_rate": 1e-05,
      "loss": 7.3737,
      "step": 10348
    },
    {
      "epoch": 0.5608672086720867,
      "step": 10348,
      "training_loss": 6.750881195068359
    },
    {
      "epoch": 0.5609214092140922,
      "step": 10349,
      "training_loss": 7.614865779876709
    },
    {
      "epoch": 0.5609756097560976,
      "step": 10350,
      "training_loss": 7.700051784515381
    },
    {
      "epoch": 0.561029810298103,
      "step": 10351,
      "training_loss": 6.725773811340332
    },
    {
      "epoch": 0.5610840108401084,
      "grad_norm": 39.430599212646484,
      "learning_rate": 1e-05,
      "loss": 7.1979,
      "step": 10352
    },
    {
      "epoch": 0.5610840108401084,
      "step": 10352,
      "training_loss": 5.405179500579834
    },
    {
      "epoch": 0.5611382113821138,
      "step": 10353,
      "training_loss": 6.43522834777832
    },
    {
      "epoch": 0.5611924119241193,
      "step": 10354,
      "training_loss": 3.227720022201538
    },
    {
      "epoch": 0.5612466124661246,
      "step": 10355,
      "training_loss": 6.707723140716553
    },
    {
      "epoch": 0.5613008130081301,
      "grad_norm": 20.10828971862793,
      "learning_rate": 1e-05,
      "loss": 5.444,
      "step": 10356
    },
    {
      "epoch": 0.5613008130081301,
      "step": 10356,
      "training_loss": 6.630204200744629
    },
    {
      "epoch": 0.5613550135501355,
      "step": 10357,
      "training_loss": 8.013134956359863
    },
    {
      "epoch": 0.561409214092141,
      "step": 10358,
      "training_loss": 5.9237260818481445
    },
    {
      "epoch": 0.5614634146341464,
      "step": 10359,
      "training_loss": 8.197988510131836
    },
    {
      "epoch": 0.5615176151761517,
      "grad_norm": 33.13504409790039,
      "learning_rate": 1e-05,
      "loss": 7.1913,
      "step": 10360
    },
    {
      "epoch": 0.5615176151761517,
      "step": 10360,
      "training_loss": 7.879618167877197
    },
    {
      "epoch": 0.5615718157181572,
      "step": 10361,
      "training_loss": 5.70021390914917
    },
    {
      "epoch": 0.5616260162601626,
      "step": 10362,
      "training_loss": 7.540599822998047
    },
    {
      "epoch": 0.5616802168021681,
      "step": 10363,
      "training_loss": 7.180606365203857
    },
    {
      "epoch": 0.5617344173441734,
      "grad_norm": 22.75647735595703,
      "learning_rate": 1e-05,
      "loss": 7.0753,
      "step": 10364
    },
    {
      "epoch": 0.5617344173441734,
      "step": 10364,
      "training_loss": 5.69468879699707
    },
    {
      "epoch": 0.5617886178861788,
      "step": 10365,
      "training_loss": 7.299725532531738
    },
    {
      "epoch": 0.5618428184281843,
      "step": 10366,
      "training_loss": 7.982301712036133
    },
    {
      "epoch": 0.5618970189701897,
      "step": 10367,
      "training_loss": 6.618002414703369
    },
    {
      "epoch": 0.5619512195121952,
      "grad_norm": 32.529258728027344,
      "learning_rate": 1e-05,
      "loss": 6.8987,
      "step": 10368
    },
    {
      "epoch": 0.5619512195121952,
      "step": 10368,
      "training_loss": 6.781437397003174
    },
    {
      "epoch": 0.5620054200542005,
      "step": 10369,
      "training_loss": 7.068377494812012
    },
    {
      "epoch": 0.562059620596206,
      "step": 10370,
      "training_loss": 7.275594234466553
    },
    {
      "epoch": 0.5621138211382114,
      "step": 10371,
      "training_loss": 6.190382957458496
    },
    {
      "epoch": 0.5621680216802168,
      "grad_norm": 24.4677791595459,
      "learning_rate": 1e-05,
      "loss": 6.8289,
      "step": 10372
    },
    {
      "epoch": 0.5621680216802168,
      "step": 10372,
      "training_loss": 5.95583438873291
    },
    {
      "epoch": 0.5622222222222222,
      "step": 10373,
      "training_loss": 4.694339752197266
    },
    {
      "epoch": 0.5622764227642276,
      "step": 10374,
      "training_loss": 6.972382068634033
    },
    {
      "epoch": 0.5623306233062331,
      "step": 10375,
      "training_loss": 6.637528896331787
    },
    {
      "epoch": 0.5623848238482385,
      "grad_norm": 18.927595138549805,
      "learning_rate": 1e-05,
      "loss": 6.065,
      "step": 10376
    },
    {
      "epoch": 0.5623848238482385,
      "step": 10376,
      "training_loss": 6.6817426681518555
    },
    {
      "epoch": 0.562439024390244,
      "step": 10377,
      "training_loss": 6.825987339019775
    },
    {
      "epoch": 0.5624932249322493,
      "step": 10378,
      "training_loss": 7.085963726043701
    },
    {
      "epoch": 0.5625474254742547,
      "step": 10379,
      "training_loss": 6.702401161193848
    },
    {
      "epoch": 0.5626016260162602,
      "grad_norm": 23.961448669433594,
      "learning_rate": 1e-05,
      "loss": 6.824,
      "step": 10380
    },
    {
      "epoch": 0.5626016260162602,
      "step": 10380,
      "training_loss": 5.9597601890563965
    },
    {
      "epoch": 0.5626558265582656,
      "step": 10381,
      "training_loss": 5.536291599273682
    },
    {
      "epoch": 0.562710027100271,
      "step": 10382,
      "training_loss": 7.566895008087158
    },
    {
      "epoch": 0.5627642276422764,
      "step": 10383,
      "training_loss": 5.717249393463135
    },
    {
      "epoch": 0.5628184281842818,
      "grad_norm": 46.94078063964844,
      "learning_rate": 1e-05,
      "loss": 6.195,
      "step": 10384
    },
    {
      "epoch": 0.5628184281842818,
      "step": 10384,
      "training_loss": 6.556564807891846
    },
    {
      "epoch": 0.5628726287262873,
      "step": 10385,
      "training_loss": 7.057911396026611
    },
    {
      "epoch": 0.5629268292682926,
      "step": 10386,
      "training_loss": 7.658633232116699
    },
    {
      "epoch": 0.5629810298102981,
      "step": 10387,
      "training_loss": 6.767714977264404
    },
    {
      "epoch": 0.5630352303523035,
      "grad_norm": 22.365461349487305,
      "learning_rate": 1e-05,
      "loss": 7.0102,
      "step": 10388
    },
    {
      "epoch": 0.5630352303523035,
      "step": 10388,
      "training_loss": 7.538250923156738
    },
    {
      "epoch": 0.563089430894309,
      "step": 10389,
      "training_loss": 7.427778720855713
    },
    {
      "epoch": 0.5631436314363144,
      "step": 10390,
      "training_loss": 7.978275775909424
    },
    {
      "epoch": 0.5631978319783197,
      "step": 10391,
      "training_loss": 5.972560405731201
    },
    {
      "epoch": 0.5632520325203252,
      "grad_norm": 42.645843505859375,
      "learning_rate": 1e-05,
      "loss": 7.2292,
      "step": 10392
    },
    {
      "epoch": 0.5632520325203252,
      "step": 10392,
      "training_loss": 4.064203262329102
    },
    {
      "epoch": 0.5633062330623306,
      "step": 10393,
      "training_loss": 7.886270999908447
    },
    {
      "epoch": 0.5633604336043361,
      "step": 10394,
      "training_loss": 5.913489818572998
    },
    {
      "epoch": 0.5634146341463414,
      "step": 10395,
      "training_loss": 6.433365345001221
    },
    {
      "epoch": 0.5634688346883469,
      "grad_norm": 32.329742431640625,
      "learning_rate": 1e-05,
      "loss": 6.0743,
      "step": 10396
    },
    {
      "epoch": 0.5634688346883469,
      "step": 10396,
      "training_loss": 5.8368706703186035
    },
    {
      "epoch": 0.5635230352303523,
      "step": 10397,
      "training_loss": 7.36315393447876
    },
    {
      "epoch": 0.5635772357723577,
      "step": 10398,
      "training_loss": 3.6648662090301514
    },
    {
      "epoch": 0.5636314363143632,
      "step": 10399,
      "training_loss": 7.265148639678955
    },
    {
      "epoch": 0.5636856368563685,
      "grad_norm": 39.64714050292969,
      "learning_rate": 1e-05,
      "loss": 6.0325,
      "step": 10400
    },
    {
      "epoch": 0.5636856368563685,
      "step": 10400,
      "training_loss": 7.673219680786133
    },
    {
      "epoch": 0.563739837398374,
      "step": 10401,
      "training_loss": 7.2148332595825195
    },
    {
      "epoch": 0.5637940379403794,
      "step": 10402,
      "training_loss": 6.981037139892578
    },
    {
      "epoch": 0.5638482384823849,
      "step": 10403,
      "training_loss": 6.217399597167969
    },
    {
      "epoch": 0.5639024390243902,
      "grad_norm": 26.633861541748047,
      "learning_rate": 1e-05,
      "loss": 7.0216,
      "step": 10404
    },
    {
      "epoch": 0.5639024390243902,
      "step": 10404,
      "training_loss": 6.522485256195068
    },
    {
      "epoch": 0.5639566395663956,
      "step": 10405,
      "training_loss": 4.179061412811279
    },
    {
      "epoch": 0.5640108401084011,
      "step": 10406,
      "training_loss": 6.629424571990967
    },
    {
      "epoch": 0.5640650406504065,
      "step": 10407,
      "training_loss": 6.741423606872559
    },
    {
      "epoch": 0.564119241192412,
      "grad_norm": 32.168426513671875,
      "learning_rate": 1e-05,
      "loss": 6.0181,
      "step": 10408
    },
    {
      "epoch": 0.564119241192412,
      "step": 10408,
      "training_loss": 5.9414191246032715
    },
    {
      "epoch": 0.5641734417344173,
      "step": 10409,
      "training_loss": 7.573800086975098
    },
    {
      "epoch": 0.5642276422764227,
      "step": 10410,
      "training_loss": 7.1949687004089355
    },
    {
      "epoch": 0.5642818428184282,
      "step": 10411,
      "training_loss": 7.035340309143066
    },
    {
      "epoch": 0.5643360433604336,
      "grad_norm": 21.518634796142578,
      "learning_rate": 1e-05,
      "loss": 6.9364,
      "step": 10412
    },
    {
      "epoch": 0.5643360433604336,
      "step": 10412,
      "training_loss": 8.18941879272461
    },
    {
      "epoch": 0.564390243902439,
      "step": 10413,
      "training_loss": 7.229745388031006
    },
    {
      "epoch": 0.5644444444444444,
      "step": 10414,
      "training_loss": 5.3968281745910645
    },
    {
      "epoch": 0.5644986449864499,
      "step": 10415,
      "training_loss": 6.646704196929932
    },
    {
      "epoch": 0.5645528455284553,
      "grad_norm": 31.12897491455078,
      "learning_rate": 1e-05,
      "loss": 6.8657,
      "step": 10416
    },
    {
      "epoch": 0.5645528455284553,
      "step": 10416,
      "training_loss": 6.756365776062012
    },
    {
      "epoch": 0.5646070460704607,
      "step": 10417,
      "training_loss": 5.986629009246826
    },
    {
      "epoch": 0.5646612466124661,
      "step": 10418,
      "training_loss": 6.939435005187988
    },
    {
      "epoch": 0.5647154471544715,
      "step": 10419,
      "training_loss": 6.759681224822998
    },
    {
      "epoch": 0.564769647696477,
      "grad_norm": 19.461666107177734,
      "learning_rate": 1e-05,
      "loss": 6.6105,
      "step": 10420
    },
    {
      "epoch": 0.564769647696477,
      "step": 10420,
      "training_loss": 5.450059413909912
    },
    {
      "epoch": 0.5648238482384824,
      "step": 10421,
      "training_loss": 7.388752460479736
    },
    {
      "epoch": 0.5648780487804878,
      "step": 10422,
      "training_loss": 5.600449562072754
    },
    {
      "epoch": 0.5649322493224932,
      "step": 10423,
      "training_loss": 7.095954895019531
    },
    {
      "epoch": 0.5649864498644986,
      "grad_norm": 22.00181770324707,
      "learning_rate": 1e-05,
      "loss": 6.3838,
      "step": 10424
    },
    {
      "epoch": 0.5649864498644986,
      "step": 10424,
      "training_loss": 4.999387741088867
    },
    {
      "epoch": 0.5650406504065041,
      "step": 10425,
      "training_loss": 6.52104377746582
    },
    {
      "epoch": 0.5650948509485095,
      "step": 10426,
      "training_loss": 5.35963249206543
    },
    {
      "epoch": 0.5651490514905149,
      "step": 10427,
      "training_loss": 5.984370708465576
    },
    {
      "epoch": 0.5652032520325203,
      "grad_norm": 46.571388244628906,
      "learning_rate": 1e-05,
      "loss": 5.7161,
      "step": 10428
    },
    {
      "epoch": 0.5652032520325203,
      "step": 10428,
      "training_loss": 5.870128154754639
    },
    {
      "epoch": 0.5652574525745258,
      "step": 10429,
      "training_loss": 7.1796464920043945
    },
    {
      "epoch": 0.5653116531165312,
      "step": 10430,
      "training_loss": 6.232906818389893
    },
    {
      "epoch": 0.5653658536585365,
      "step": 10431,
      "training_loss": 6.696777820587158
    },
    {
      "epoch": 0.565420054200542,
      "grad_norm": 69.65573120117188,
      "learning_rate": 1e-05,
      "loss": 6.4949,
      "step": 10432
    },
    {
      "epoch": 0.565420054200542,
      "step": 10432,
      "training_loss": 6.906976699829102
    },
    {
      "epoch": 0.5654742547425474,
      "step": 10433,
      "training_loss": 8.144091606140137
    },
    {
      "epoch": 0.5655284552845529,
      "step": 10434,
      "training_loss": 6.827754020690918
    },
    {
      "epoch": 0.5655826558265583,
      "step": 10435,
      "training_loss": 6.301621437072754
    },
    {
      "epoch": 0.5656368563685636,
      "grad_norm": 24.571645736694336,
      "learning_rate": 1e-05,
      "loss": 7.0451,
      "step": 10436
    },
    {
      "epoch": 0.5656368563685636,
      "step": 10436,
      "training_loss": 4.974908351898193
    },
    {
      "epoch": 0.5656910569105691,
      "step": 10437,
      "training_loss": 5.791353225708008
    },
    {
      "epoch": 0.5657452574525745,
      "step": 10438,
      "training_loss": 6.790146350860596
    },
    {
      "epoch": 0.56579945799458,
      "step": 10439,
      "training_loss": 6.753128528594971
    },
    {
      "epoch": 0.5658536585365853,
      "grad_norm": 51.39850997924805,
      "learning_rate": 1e-05,
      "loss": 6.0774,
      "step": 10440
    },
    {
      "epoch": 0.5658536585365853,
      "step": 10440,
      "training_loss": 6.426671028137207
    },
    {
      "epoch": 0.5659078590785908,
      "step": 10441,
      "training_loss": 6.776679992675781
    },
    {
      "epoch": 0.5659620596205962,
      "step": 10442,
      "training_loss": 9.42129135131836
    },
    {
      "epoch": 0.5660162601626016,
      "step": 10443,
      "training_loss": 5.896242618560791
    },
    {
      "epoch": 0.5660704607046071,
      "grad_norm": 20.94673728942871,
      "learning_rate": 1e-05,
      "loss": 7.1302,
      "step": 10444
    },
    {
      "epoch": 0.5660704607046071,
      "step": 10444,
      "training_loss": 9.137641906738281
    },
    {
      "epoch": 0.5661246612466124,
      "step": 10445,
      "training_loss": 6.299225807189941
    },
    {
      "epoch": 0.5661788617886179,
      "step": 10446,
      "training_loss": 7.010969638824463
    },
    {
      "epoch": 0.5662330623306233,
      "step": 10447,
      "training_loss": 6.680572032928467
    },
    {
      "epoch": 0.5662872628726288,
      "grad_norm": 29.59135627746582,
      "learning_rate": 1e-05,
      "loss": 7.2821,
      "step": 10448
    },
    {
      "epoch": 0.5662872628726288,
      "step": 10448,
      "training_loss": 7.052921295166016
    },
    {
      "epoch": 0.5663414634146341,
      "step": 10449,
      "training_loss": 5.650815486907959
    },
    {
      "epoch": 0.5663956639566395,
      "step": 10450,
      "training_loss": 7.237578392028809
    },
    {
      "epoch": 0.566449864498645,
      "step": 10451,
      "training_loss": 7.322469711303711
    },
    {
      "epoch": 0.5665040650406504,
      "grad_norm": 24.010461807250977,
      "learning_rate": 1e-05,
      "loss": 6.8159,
      "step": 10452
    },
    {
      "epoch": 0.5665040650406504,
      "step": 10452,
      "training_loss": 8.60381031036377
    },
    {
      "epoch": 0.5665582655826559,
      "step": 10453,
      "training_loss": 7.898327827453613
    },
    {
      "epoch": 0.5666124661246612,
      "step": 10454,
      "training_loss": 5.680578708648682
    },
    {
      "epoch": 0.5666666666666667,
      "step": 10455,
      "training_loss": 6.40401029586792
    },
    {
      "epoch": 0.5667208672086721,
      "grad_norm": 19.194520950317383,
      "learning_rate": 1e-05,
      "loss": 7.1467,
      "step": 10456
    },
    {
      "epoch": 0.5667208672086721,
      "step": 10456,
      "training_loss": 3.1037139892578125
    },
    {
      "epoch": 0.5667750677506775,
      "step": 10457,
      "training_loss": 6.411881923675537
    },
    {
      "epoch": 0.5668292682926829,
      "step": 10458,
      "training_loss": 7.057480812072754
    },
    {
      "epoch": 0.5668834688346883,
      "step": 10459,
      "training_loss": 7.030624866485596
    },
    {
      "epoch": 0.5669376693766938,
      "grad_norm": 31.712692260742188,
      "learning_rate": 1e-05,
      "loss": 5.9009,
      "step": 10460
    },
    {
      "epoch": 0.5669376693766938,
      "step": 10460,
      "training_loss": 7.456546783447266
    },
    {
      "epoch": 0.5669918699186992,
      "step": 10461,
      "training_loss": 6.622799396514893
    },
    {
      "epoch": 0.5670460704607047,
      "step": 10462,
      "training_loss": 7.469241619110107
    },
    {
      "epoch": 0.56710027100271,
      "step": 10463,
      "training_loss": 5.684375286102295
    },
    {
      "epoch": 0.5671544715447154,
      "grad_norm": 28.455039978027344,
      "learning_rate": 1e-05,
      "loss": 6.8082,
      "step": 10464
    },
    {
      "epoch": 0.5671544715447154,
      "step": 10464,
      "training_loss": 8.160179138183594
    },
    {
      "epoch": 0.5672086720867209,
      "step": 10465,
      "training_loss": 3.3394057750701904
    },
    {
      "epoch": 0.5672628726287263,
      "step": 10466,
      "training_loss": 7.681423664093018
    },
    {
      "epoch": 0.5673170731707317,
      "step": 10467,
      "training_loss": 7.133221626281738
    },
    {
      "epoch": 0.5673712737127371,
      "grad_norm": 35.92129898071289,
      "learning_rate": 1e-05,
      "loss": 6.5786,
      "step": 10468
    },
    {
      "epoch": 0.5673712737127371,
      "step": 10468,
      "training_loss": 5.347471714019775
    },
    {
      "epoch": 0.5674254742547425,
      "step": 10469,
      "training_loss": 6.953397750854492
    },
    {
      "epoch": 0.567479674796748,
      "step": 10470,
      "training_loss": 7.2033586502075195
    },
    {
      "epoch": 0.5675338753387534,
      "step": 10471,
      "training_loss": 6.839695453643799
    },
    {
      "epoch": 0.5675880758807588,
      "grad_norm": 28.850046157836914,
      "learning_rate": 1e-05,
      "loss": 6.586,
      "step": 10472
    },
    {
      "epoch": 0.5675880758807588,
      "step": 10472,
      "training_loss": 6.2420783042907715
    },
    {
      "epoch": 0.5676422764227642,
      "step": 10473,
      "training_loss": 5.8736114501953125
    },
    {
      "epoch": 0.5676964769647697,
      "step": 10474,
      "training_loss": 7.9209303855896
    },
    {
      "epoch": 0.5677506775067751,
      "step": 10475,
      "training_loss": 5.673335075378418
    },
    {
      "epoch": 0.5678048780487804,
      "grad_norm": 25.874454498291016,
      "learning_rate": 1e-05,
      "loss": 6.4275,
      "step": 10476
    },
    {
      "epoch": 0.5678048780487804,
      "step": 10476,
      "training_loss": 5.909512519836426
    },
    {
      "epoch": 0.5678590785907859,
      "step": 10477,
      "training_loss": 5.998401165008545
    },
    {
      "epoch": 0.5679132791327913,
      "step": 10478,
      "training_loss": 6.960227966308594
    },
    {
      "epoch": 0.5679674796747968,
      "step": 10479,
      "training_loss": 6.182552337646484
    },
    {
      "epoch": 0.5680216802168022,
      "grad_norm": 25.8763370513916,
      "learning_rate": 1e-05,
      "loss": 6.2627,
      "step": 10480
    },
    {
      "epoch": 0.5680216802168022,
      "step": 10480,
      "training_loss": 7.431725025177002
    },
    {
      "epoch": 0.5680758807588075,
      "step": 10481,
      "training_loss": 5.8312554359436035
    },
    {
      "epoch": 0.568130081300813,
      "step": 10482,
      "training_loss": 7.5887041091918945
    },
    {
      "epoch": 0.5681842818428184,
      "step": 10483,
      "training_loss": 7.061915397644043
    },
    {
      "epoch": 0.5682384823848239,
      "grad_norm": 27.83323097229004,
      "learning_rate": 1e-05,
      "loss": 6.9784,
      "step": 10484
    },
    {
      "epoch": 0.5682384823848239,
      "step": 10484,
      "training_loss": 7.232338905334473
    },
    {
      "epoch": 0.5682926829268292,
      "step": 10485,
      "training_loss": 7.461278915405273
    },
    {
      "epoch": 0.5683468834688347,
      "step": 10486,
      "training_loss": 5.010290622711182
    },
    {
      "epoch": 0.5684010840108401,
      "step": 10487,
      "training_loss": 5.070282936096191
    },
    {
      "epoch": 0.5684552845528456,
      "grad_norm": 37.67283630371094,
      "learning_rate": 1e-05,
      "loss": 6.1935,
      "step": 10488
    },
    {
      "epoch": 0.5684552845528456,
      "step": 10488,
      "training_loss": 6.656632423400879
    },
    {
      "epoch": 0.568509485094851,
      "step": 10489,
      "training_loss": 7.278044700622559
    },
    {
      "epoch": 0.5685636856368563,
      "step": 10490,
      "training_loss": 6.175861835479736
    },
    {
      "epoch": 0.5686178861788618,
      "step": 10491,
      "training_loss": 5.3690571784973145
    },
    {
      "epoch": 0.5686720867208672,
      "grad_norm": 26.928462982177734,
      "learning_rate": 1e-05,
      "loss": 6.3699,
      "step": 10492
    },
    {
      "epoch": 0.5686720867208672,
      "step": 10492,
      "training_loss": 6.855340480804443
    },
    {
      "epoch": 0.5687262872628727,
      "step": 10493,
      "training_loss": 6.507912635803223
    },
    {
      "epoch": 0.568780487804878,
      "step": 10494,
      "training_loss": 7.2865471839904785
    },
    {
      "epoch": 0.5688346883468834,
      "step": 10495,
      "training_loss": 5.990206241607666
    },
    {
      "epoch": 0.5688888888888889,
      "grad_norm": 36.29056167602539,
      "learning_rate": 1e-05,
      "loss": 6.66,
      "step": 10496
    },
    {
      "epoch": 0.5688888888888889,
      "step": 10496,
      "training_loss": 7.183734893798828
    },
    {
      "epoch": 0.5689430894308943,
      "step": 10497,
      "training_loss": 7.284801006317139
    },
    {
      "epoch": 0.5689972899728998,
      "step": 10498,
      "training_loss": 5.9004435539245605
    },
    {
      "epoch": 0.5690514905149051,
      "step": 10499,
      "training_loss": 6.147570610046387
    },
    {
      "epoch": 0.5691056910569106,
      "grad_norm": 45.59368133544922,
      "learning_rate": 1e-05,
      "loss": 6.6291,
      "step": 10500
    },
    {
      "epoch": 0.5691056910569106,
      "step": 10500,
      "training_loss": 6.850982189178467
    },
    {
      "epoch": 0.569159891598916,
      "step": 10501,
      "training_loss": 6.295035362243652
    },
    {
      "epoch": 0.5692140921409214,
      "step": 10502,
      "training_loss": 7.011207103729248
    },
    {
      "epoch": 0.5692682926829268,
      "step": 10503,
      "training_loss": 7.248258590698242
    },
    {
      "epoch": 0.5693224932249322,
      "grad_norm": 38.847816467285156,
      "learning_rate": 1e-05,
      "loss": 6.8514,
      "step": 10504
    },
    {
      "epoch": 0.5693224932249322,
      "step": 10504,
      "training_loss": 7.67783260345459
    },
    {
      "epoch": 0.5693766937669377,
      "step": 10505,
      "training_loss": 6.64480447769165
    },
    {
      "epoch": 0.5694308943089431,
      "step": 10506,
      "training_loss": 7.602640151977539
    },
    {
      "epoch": 0.5694850948509486,
      "step": 10507,
      "training_loss": 5.6373138427734375
    },
    {
      "epoch": 0.5695392953929539,
      "grad_norm": 34.23382568359375,
      "learning_rate": 1e-05,
      "loss": 6.8906,
      "step": 10508
    },
    {
      "epoch": 0.5695392953929539,
      "step": 10508,
      "training_loss": 6.1576128005981445
    },
    {
      "epoch": 0.5695934959349593,
      "step": 10509,
      "training_loss": 7.0882568359375
    },
    {
      "epoch": 0.5696476964769648,
      "step": 10510,
      "training_loss": 3.475395917892456
    },
    {
      "epoch": 0.5697018970189702,
      "step": 10511,
      "training_loss": 4.5720906257629395
    },
    {
      "epoch": 0.5697560975609756,
      "grad_norm": 37.844112396240234,
      "learning_rate": 1e-05,
      "loss": 5.3233,
      "step": 10512
    },
    {
      "epoch": 0.5697560975609756,
      "step": 10512,
      "training_loss": 5.329232692718506
    },
    {
      "epoch": 0.569810298102981,
      "step": 10513,
      "training_loss": 6.35496187210083
    },
    {
      "epoch": 0.5698644986449865,
      "step": 10514,
      "training_loss": 3.429819107055664
    },
    {
      "epoch": 0.5699186991869919,
      "step": 10515,
      "training_loss": 6.715415000915527
    },
    {
      "epoch": 0.5699728997289973,
      "grad_norm": 16.603713989257812,
      "learning_rate": 1e-05,
      "loss": 5.4574,
      "step": 10516
    },
    {
      "epoch": 0.5699728997289973,
      "step": 10516,
      "training_loss": 6.758861541748047
    },
    {
      "epoch": 0.5700271002710027,
      "step": 10517,
      "training_loss": 6.5050835609436035
    },
    {
      "epoch": 0.5700813008130081,
      "step": 10518,
      "training_loss": 5.053009033203125
    },
    {
      "epoch": 0.5701355013550136,
      "step": 10519,
      "training_loss": 5.4364776611328125
    },
    {
      "epoch": 0.570189701897019,
      "grad_norm": 69.40625,
      "learning_rate": 1e-05,
      "loss": 5.9384,
      "step": 10520
    },
    {
      "epoch": 0.570189701897019,
      "step": 10520,
      "training_loss": 5.00968599319458
    },
    {
      "epoch": 0.5702439024390243,
      "step": 10521,
      "training_loss": 6.065668106079102
    },
    {
      "epoch": 0.5702981029810298,
      "step": 10522,
      "training_loss": 7.0172576904296875
    },
    {
      "epoch": 0.5703523035230352,
      "step": 10523,
      "training_loss": 3.3851706981658936
    },
    {
      "epoch": 0.5704065040650407,
      "grad_norm": 28.489748001098633,
      "learning_rate": 1e-05,
      "loss": 5.3694,
      "step": 10524
    },
    {
      "epoch": 0.5704065040650407,
      "step": 10524,
      "training_loss": 7.173994064331055
    },
    {
      "epoch": 0.5704607046070461,
      "step": 10525,
      "training_loss": 7.2607550621032715
    },
    {
      "epoch": 0.5705149051490515,
      "step": 10526,
      "training_loss": 6.684391975402832
    },
    {
      "epoch": 0.5705691056910569,
      "step": 10527,
      "training_loss": 4.758415699005127
    },
    {
      "epoch": 0.5706233062330623,
      "grad_norm": 25.086833953857422,
      "learning_rate": 1e-05,
      "loss": 6.4694,
      "step": 10528
    },
    {
      "epoch": 0.5706233062330623,
      "step": 10528,
      "training_loss": 6.459214210510254
    },
    {
      "epoch": 0.5706775067750678,
      "step": 10529,
      "training_loss": 7.970938205718994
    },
    {
      "epoch": 0.5707317073170731,
      "step": 10530,
      "training_loss": 3.895643711090088
    },
    {
      "epoch": 0.5707859078590786,
      "step": 10531,
      "training_loss": 5.3581767082214355
    },
    {
      "epoch": 0.570840108401084,
      "grad_norm": 62.46110916137695,
      "learning_rate": 1e-05,
      "loss": 5.921,
      "step": 10532
    },
    {
      "epoch": 0.570840108401084,
      "step": 10532,
      "training_loss": 6.9544172286987305
    },
    {
      "epoch": 0.5708943089430895,
      "step": 10533,
      "training_loss": 5.943472385406494
    },
    {
      "epoch": 0.5709485094850949,
      "step": 10534,
      "training_loss": 6.55913782119751
    },
    {
      "epoch": 0.5710027100271002,
      "step": 10535,
      "training_loss": 8.16065502166748
    },
    {
      "epoch": 0.5710569105691057,
      "grad_norm": 30.675100326538086,
      "learning_rate": 1e-05,
      "loss": 6.9044,
      "step": 10536
    },
    {
      "epoch": 0.5710569105691057,
      "step": 10536,
      "training_loss": 5.2714762687683105
    },
    {
      "epoch": 0.5711111111111111,
      "step": 10537,
      "training_loss": 5.983870029449463
    },
    {
      "epoch": 0.5711653116531166,
      "step": 10538,
      "training_loss": 5.520933151245117
    },
    {
      "epoch": 0.5712195121951219,
      "step": 10539,
      "training_loss": 8.593856811523438
    },
    {
      "epoch": 0.5712737127371273,
      "grad_norm": 34.95151138305664,
      "learning_rate": 1e-05,
      "loss": 6.3425,
      "step": 10540
    },
    {
      "epoch": 0.5712737127371273,
      "step": 10540,
      "training_loss": 7.236551284790039
    },
    {
      "epoch": 0.5713279132791328,
      "step": 10541,
      "training_loss": 7.9229278564453125
    },
    {
      "epoch": 0.5713821138211382,
      "step": 10542,
      "training_loss": 7.311246395111084
    },
    {
      "epoch": 0.5714363143631437,
      "step": 10543,
      "training_loss": 6.956802845001221
    },
    {
      "epoch": 0.571490514905149,
      "grad_norm": 40.0702018737793,
      "learning_rate": 1e-05,
      "loss": 7.3569,
      "step": 10544
    },
    {
      "epoch": 0.571490514905149,
      "step": 10544,
      "training_loss": 7.872537136077881
    },
    {
      "epoch": 0.5715447154471545,
      "step": 10545,
      "training_loss": 8.152029037475586
    },
    {
      "epoch": 0.5715989159891599,
      "step": 10546,
      "training_loss": 5.886329174041748
    },
    {
      "epoch": 0.5716531165311654,
      "step": 10547,
      "training_loss": 7.07614803314209
    },
    {
      "epoch": 0.5717073170731707,
      "grad_norm": 19.07423973083496,
      "learning_rate": 1e-05,
      "loss": 7.2468,
      "step": 10548
    },
    {
      "epoch": 0.5717073170731707,
      "step": 10548,
      "training_loss": 6.822338581085205
    },
    {
      "epoch": 0.5717615176151761,
      "step": 10549,
      "training_loss": 4.595409393310547
    },
    {
      "epoch": 0.5718157181571816,
      "step": 10550,
      "training_loss": 7.129879474639893
    },
    {
      "epoch": 0.571869918699187,
      "step": 10551,
      "training_loss": 5.790482044219971
    },
    {
      "epoch": 0.5719241192411925,
      "grad_norm": 35.56757354736328,
      "learning_rate": 1e-05,
      "loss": 6.0845,
      "step": 10552
    },
    {
      "epoch": 0.5719241192411925,
      "step": 10552,
      "training_loss": 5.787370204925537
    },
    {
      "epoch": 0.5719783197831978,
      "step": 10553,
      "training_loss": 6.075467109680176
    },
    {
      "epoch": 0.5720325203252032,
      "step": 10554,
      "training_loss": 5.729782581329346
    },
    {
      "epoch": 0.5720867208672087,
      "step": 10555,
      "training_loss": 8.041638374328613
    },
    {
      "epoch": 0.5721409214092141,
      "grad_norm": 72.20128631591797,
      "learning_rate": 1e-05,
      "loss": 6.4086,
      "step": 10556
    },
    {
      "epoch": 0.5721409214092141,
      "step": 10556,
      "training_loss": 3.375519275665283
    },
    {
      "epoch": 0.5721951219512195,
      "step": 10557,
      "training_loss": 7.40240478515625
    },
    {
      "epoch": 0.5722493224932249,
      "step": 10558,
      "training_loss": 7.610381126403809
    },
    {
      "epoch": 0.5723035230352304,
      "step": 10559,
      "training_loss": 6.635237693786621
    },
    {
      "epoch": 0.5723577235772358,
      "grad_norm": 19.871496200561523,
      "learning_rate": 1e-05,
      "loss": 6.2559,
      "step": 10560
    },
    {
      "epoch": 0.5723577235772358,
      "step": 10560,
      "training_loss": 6.4174957275390625
    },
    {
      "epoch": 0.5724119241192412,
      "step": 10561,
      "training_loss": 6.763468265533447
    },
    {
      "epoch": 0.5724661246612466,
      "step": 10562,
      "training_loss": 6.876895904541016
    },
    {
      "epoch": 0.572520325203252,
      "step": 10563,
      "training_loss": 7.192476272583008
    },
    {
      "epoch": 0.5725745257452575,
      "grad_norm": 31.02653694152832,
      "learning_rate": 1e-05,
      "loss": 6.8126,
      "step": 10564
    },
    {
      "epoch": 0.5725745257452575,
      "step": 10564,
      "training_loss": 7.295247554779053
    },
    {
      "epoch": 0.5726287262872629,
      "step": 10565,
      "training_loss": 8.640125274658203
    },
    {
      "epoch": 0.5726829268292682,
      "step": 10566,
      "training_loss": 6.03096342086792
    },
    {
      "epoch": 0.5727371273712737,
      "step": 10567,
      "training_loss": 6.882867813110352
    },
    {
      "epoch": 0.5727913279132791,
      "grad_norm": 24.53270721435547,
      "learning_rate": 1e-05,
      "loss": 7.2123,
      "step": 10568
    },
    {
      "epoch": 0.5727913279132791,
      "step": 10568,
      "training_loss": 7.176701068878174
    },
    {
      "epoch": 0.5728455284552846,
      "step": 10569,
      "training_loss": 7.336175441741943
    },
    {
      "epoch": 0.57289972899729,
      "step": 10570,
      "training_loss": 5.254494667053223
    },
    {
      "epoch": 0.5729539295392954,
      "step": 10571,
      "training_loss": 7.570196628570557
    },
    {
      "epoch": 0.5730081300813008,
      "grad_norm": 45.330955505371094,
      "learning_rate": 1e-05,
      "loss": 6.8344,
      "step": 10572
    },
    {
      "epoch": 0.5730081300813008,
      "step": 10572,
      "training_loss": 5.216699123382568
    },
    {
      "epoch": 0.5730623306233062,
      "step": 10573,
      "training_loss": 6.560548305511475
    },
    {
      "epoch": 0.5731165311653117,
      "step": 10574,
      "training_loss": 6.4349846839904785
    },
    {
      "epoch": 0.573170731707317,
      "step": 10575,
      "training_loss": 5.823992729187012
    },
    {
      "epoch": 0.5732249322493225,
      "grad_norm": 36.7844352722168,
      "learning_rate": 1e-05,
      "loss": 6.0091,
      "step": 10576
    },
    {
      "epoch": 0.5732249322493225,
      "step": 10576,
      "training_loss": 5.716328144073486
    },
    {
      "epoch": 0.5732791327913279,
      "step": 10577,
      "training_loss": 7.599839210510254
    },
    {
      "epoch": 0.5733333333333334,
      "step": 10578,
      "training_loss": 7.059874534606934
    },
    {
      "epoch": 0.5733875338753388,
      "step": 10579,
      "training_loss": 6.523560523986816
    },
    {
      "epoch": 0.5734417344173441,
      "grad_norm": 33.5770263671875,
      "learning_rate": 1e-05,
      "loss": 6.7249,
      "step": 10580
    },
    {
      "epoch": 0.5734417344173441,
      "step": 10580,
      "training_loss": 6.647366523742676
    },
    {
      "epoch": 0.5734959349593496,
      "step": 10581,
      "training_loss": 5.824982166290283
    },
    {
      "epoch": 0.573550135501355,
      "step": 10582,
      "training_loss": 5.792875289916992
    },
    {
      "epoch": 0.5736043360433605,
      "step": 10583,
      "training_loss": 7.551311492919922
    },
    {
      "epoch": 0.5736585365853658,
      "grad_norm": 25.30124855041504,
      "learning_rate": 1e-05,
      "loss": 6.4541,
      "step": 10584
    },
    {
      "epoch": 0.5736585365853658,
      "step": 10584,
      "training_loss": 7.424960136413574
    },
    {
      "epoch": 0.5737127371273713,
      "step": 10585,
      "training_loss": 7.693682670593262
    },
    {
      "epoch": 0.5737669376693767,
      "step": 10586,
      "training_loss": 6.725932598114014
    },
    {
      "epoch": 0.5738211382113821,
      "step": 10587,
      "training_loss": 6.767826080322266
    },
    {
      "epoch": 0.5738753387533876,
      "grad_norm": 26.18610191345215,
      "learning_rate": 1e-05,
      "loss": 7.1531,
      "step": 10588
    },
    {
      "epoch": 0.5738753387533876,
      "step": 10588,
      "training_loss": 6.939011573791504
    },
    {
      "epoch": 0.5739295392953929,
      "step": 10589,
      "training_loss": 6.360716342926025
    },
    {
      "epoch": 0.5739837398373984,
      "step": 10590,
      "training_loss": 5.485673904418945
    },
    {
      "epoch": 0.5740379403794038,
      "step": 10591,
      "training_loss": 6.834218978881836
    },
    {
      "epoch": 0.5740921409214093,
      "grad_norm": 22.520095825195312,
      "learning_rate": 1e-05,
      "loss": 6.4049,
      "step": 10592
    },
    {
      "epoch": 0.5740921409214093,
      "step": 10592,
      "training_loss": 7.0736517906188965
    },
    {
      "epoch": 0.5741463414634146,
      "step": 10593,
      "training_loss": 6.569272041320801
    },
    {
      "epoch": 0.57420054200542,
      "step": 10594,
      "training_loss": 7.111419200897217
    },
    {
      "epoch": 0.5742547425474255,
      "step": 10595,
      "training_loss": 6.391386985778809
    },
    {
      "epoch": 0.5743089430894309,
      "grad_norm": 42.96262741088867,
      "learning_rate": 1e-05,
      "loss": 6.7864,
      "step": 10596
    },
    {
      "epoch": 0.5743089430894309,
      "step": 10596,
      "training_loss": 7.030332088470459
    },
    {
      "epoch": 0.5743631436314364,
      "step": 10597,
      "training_loss": 4.78100061416626
    },
    {
      "epoch": 0.5744173441734417,
      "step": 10598,
      "training_loss": 5.533414840698242
    },
    {
      "epoch": 0.5744715447154471,
      "step": 10599,
      "training_loss": 5.995053291320801
    },
    {
      "epoch": 0.5745257452574526,
      "grad_norm": 24.81258201599121,
      "learning_rate": 1e-05,
      "loss": 5.835,
      "step": 10600
    },
    {
      "epoch": 0.5745257452574526,
      "step": 10600,
      "training_loss": 5.860249996185303
    },
    {
      "epoch": 0.574579945799458,
      "step": 10601,
      "training_loss": 7.313304901123047
    },
    {
      "epoch": 0.5746341463414634,
      "step": 10602,
      "training_loss": 6.771667957305908
    },
    {
      "epoch": 0.5746883468834688,
      "step": 10603,
      "training_loss": 7.507078170776367
    },
    {
      "epoch": 0.5747425474254743,
      "grad_norm": 23.650869369506836,
      "learning_rate": 1e-05,
      "loss": 6.8631,
      "step": 10604
    },
    {
      "epoch": 0.5747425474254743,
      "step": 10604,
      "training_loss": 6.816234111785889
    },
    {
      "epoch": 0.5747967479674797,
      "step": 10605,
      "training_loss": 6.600253582000732
    },
    {
      "epoch": 0.5748509485094851,
      "step": 10606,
      "training_loss": 5.4933953285217285
    },
    {
      "epoch": 0.5749051490514905,
      "step": 10607,
      "training_loss": 7.080843448638916
    },
    {
      "epoch": 0.5749593495934959,
      "grad_norm": 18.050216674804688,
      "learning_rate": 1e-05,
      "loss": 6.4977,
      "step": 10608
    },
    {
      "epoch": 0.5749593495934959,
      "step": 10608,
      "training_loss": 6.964140892028809
    },
    {
      "epoch": 0.5750135501355014,
      "step": 10609,
      "training_loss": 7.287193775177002
    },
    {
      "epoch": 0.5750677506775068,
      "step": 10610,
      "training_loss": 7.242587089538574
    },
    {
      "epoch": 0.5751219512195122,
      "step": 10611,
      "training_loss": 6.203110218048096
    },
    {
      "epoch": 0.5751761517615176,
      "grad_norm": 17.725860595703125,
      "learning_rate": 1e-05,
      "loss": 6.9243,
      "step": 10612
    },
    {
      "epoch": 0.5751761517615176,
      "step": 10612,
      "training_loss": 3.426819324493408
    },
    {
      "epoch": 0.575230352303523,
      "step": 10613,
      "training_loss": 7.33378267288208
    },
    {
      "epoch": 0.5752845528455285,
      "step": 10614,
      "training_loss": 7.0966033935546875
    },
    {
      "epoch": 0.5753387533875339,
      "step": 10615,
      "training_loss": 7.10952615737915
    },
    {
      "epoch": 0.5753929539295393,
      "grad_norm": 31.506847381591797,
      "learning_rate": 1e-05,
      "loss": 6.2417,
      "step": 10616
    },
    {
      "epoch": 0.5753929539295393,
      "step": 10616,
      "training_loss": 5.876652240753174
    },
    {
      "epoch": 0.5754471544715447,
      "step": 10617,
      "training_loss": 8.180251121520996
    },
    {
      "epoch": 0.5755013550135502,
      "step": 10618,
      "training_loss": 7.973162651062012
    },
    {
      "epoch": 0.5755555555555556,
      "step": 10619,
      "training_loss": 7.34768533706665
    },
    {
      "epoch": 0.5756097560975609,
      "grad_norm": 32.46738052368164,
      "learning_rate": 1e-05,
      "loss": 7.3444,
      "step": 10620
    },
    {
      "epoch": 0.5756097560975609,
      "step": 10620,
      "training_loss": 6.551015853881836
    },
    {
      "epoch": 0.5756639566395664,
      "step": 10621,
      "training_loss": 8.177851676940918
    },
    {
      "epoch": 0.5757181571815718,
      "step": 10622,
      "training_loss": 4.532387733459473
    },
    {
      "epoch": 0.5757723577235773,
      "step": 10623,
      "training_loss": 7.63257360458374
    },
    {
      "epoch": 0.5758265582655827,
      "grad_norm": 43.52940368652344,
      "learning_rate": 1e-05,
      "loss": 6.7235,
      "step": 10624
    },
    {
      "epoch": 0.5758265582655827,
      "step": 10624,
      "training_loss": 6.77771520614624
    },
    {
      "epoch": 0.575880758807588,
      "step": 10625,
      "training_loss": 6.1318840980529785
    },
    {
      "epoch": 0.5759349593495935,
      "step": 10626,
      "training_loss": 8.183489799499512
    },
    {
      "epoch": 0.5759891598915989,
      "step": 10627,
      "training_loss": 7.72284460067749
    },
    {
      "epoch": 0.5760433604336044,
      "grad_norm": 19.815536499023438,
      "learning_rate": 1e-05,
      "loss": 7.204,
      "step": 10628
    },
    {
      "epoch": 0.5760433604336044,
      "step": 10628,
      "training_loss": 6.936280727386475
    },
    {
      "epoch": 0.5760975609756097,
      "step": 10629,
      "training_loss": 7.31749963760376
    },
    {
      "epoch": 0.5761517615176152,
      "step": 10630,
      "training_loss": 6.945127010345459
    },
    {
      "epoch": 0.5762059620596206,
      "step": 10631,
      "training_loss": 7.4888529777526855
    },
    {
      "epoch": 0.576260162601626,
      "grad_norm": 49.41292190551758,
      "learning_rate": 1e-05,
      "loss": 7.1719,
      "step": 10632
    },
    {
      "epoch": 0.576260162601626,
      "step": 10632,
      "training_loss": 6.840275287628174
    },
    {
      "epoch": 0.5763143631436315,
      "step": 10633,
      "training_loss": 4.870210647583008
    },
    {
      "epoch": 0.5763685636856368,
      "step": 10634,
      "training_loss": 7.106381416320801
    },
    {
      "epoch": 0.5764227642276423,
      "step": 10635,
      "training_loss": 6.958207130432129
    },
    {
      "epoch": 0.5764769647696477,
      "grad_norm": 27.040481567382812,
      "learning_rate": 1e-05,
      "loss": 6.4438,
      "step": 10636
    },
    {
      "epoch": 0.5764769647696477,
      "step": 10636,
      "training_loss": 5.539544582366943
    },
    {
      "epoch": 0.5765311653116532,
      "step": 10637,
      "training_loss": 7.76188850402832
    },
    {
      "epoch": 0.5765853658536585,
      "step": 10638,
      "training_loss": 6.657186031341553
    },
    {
      "epoch": 0.5766395663956639,
      "step": 10639,
      "training_loss": 7.9906721115112305
    },
    {
      "epoch": 0.5766937669376694,
      "grad_norm": 22.708715438842773,
      "learning_rate": 1e-05,
      "loss": 6.9873,
      "step": 10640
    },
    {
      "epoch": 0.5766937669376694,
      "step": 10640,
      "training_loss": 6.829059600830078
    },
    {
      "epoch": 0.5767479674796748,
      "step": 10641,
      "training_loss": 7.424344539642334
    },
    {
      "epoch": 0.5768021680216802,
      "step": 10642,
      "training_loss": 4.163827896118164
    },
    {
      "epoch": 0.5768563685636856,
      "step": 10643,
      "training_loss": 6.828400135040283
    },
    {
      "epoch": 0.576910569105691,
      "grad_norm": 19.8441162109375,
      "learning_rate": 1e-05,
      "loss": 6.3114,
      "step": 10644
    },
    {
      "epoch": 0.576910569105691,
      "step": 10644,
      "training_loss": 7.311088562011719
    },
    {
      "epoch": 0.5769647696476965,
      "step": 10645,
      "training_loss": 4.874318599700928
    },
    {
      "epoch": 0.5770189701897019,
      "step": 10646,
      "training_loss": 4.507870197296143
    },
    {
      "epoch": 0.5770731707317073,
      "step": 10647,
      "training_loss": 6.579845428466797
    },
    {
      "epoch": 0.5771273712737127,
      "grad_norm": 32.448486328125,
      "learning_rate": 1e-05,
      "loss": 5.8183,
      "step": 10648
    },
    {
      "epoch": 0.5771273712737127,
      "step": 10648,
      "training_loss": 6.6008172035217285
    },
    {
      "epoch": 0.5771815718157182,
      "step": 10649,
      "training_loss": 6.8361358642578125
    },
    {
      "epoch": 0.5772357723577236,
      "step": 10650,
      "training_loss": 7.288567066192627
    },
    {
      "epoch": 0.5772899728997289,
      "step": 10651,
      "training_loss": 6.086943626403809
    },
    {
      "epoch": 0.5773441734417344,
      "grad_norm": 50.081077575683594,
      "learning_rate": 1e-05,
      "loss": 6.7031,
      "step": 10652
    },
    {
      "epoch": 0.5773441734417344,
      "step": 10652,
      "training_loss": 6.642703056335449
    },
    {
      "epoch": 0.5773983739837398,
      "step": 10653,
      "training_loss": 7.738502502441406
    },
    {
      "epoch": 0.5774525745257453,
      "step": 10654,
      "training_loss": 7.410053730010986
    },
    {
      "epoch": 0.5775067750677507,
      "step": 10655,
      "training_loss": 8.437114715576172
    },
    {
      "epoch": 0.577560975609756,
      "grad_norm": 34.47589111328125,
      "learning_rate": 1e-05,
      "loss": 7.5571,
      "step": 10656
    },
    {
      "epoch": 0.577560975609756,
      "step": 10656,
      "training_loss": 6.24958610534668
    },
    {
      "epoch": 0.5776151761517615,
      "step": 10657,
      "training_loss": 6.79618501663208
    },
    {
      "epoch": 0.577669376693767,
      "step": 10658,
      "training_loss": 8.561102867126465
    },
    {
      "epoch": 0.5777235772357724,
      "step": 10659,
      "training_loss": 3.384182929992676
    },
    {
      "epoch": 0.5777777777777777,
      "grad_norm": 38.68779754638672,
      "learning_rate": 1e-05,
      "loss": 6.2478,
      "step": 10660
    },
    {
      "epoch": 0.5777777777777777,
      "step": 10660,
      "training_loss": 6.243158340454102
    },
    {
      "epoch": 0.5778319783197832,
      "step": 10661,
      "training_loss": 6.7504377365112305
    },
    {
      "epoch": 0.5778861788617886,
      "step": 10662,
      "training_loss": 9.509748458862305
    },
    {
      "epoch": 0.5779403794037941,
      "step": 10663,
      "training_loss": 6.807366371154785
    },
    {
      "epoch": 0.5779945799457995,
      "grad_norm": 20.623502731323242,
      "learning_rate": 1e-05,
      "loss": 7.3277,
      "step": 10664
    },
    {
      "epoch": 0.5779945799457995,
      "step": 10664,
      "training_loss": 7.638394355773926
    },
    {
      "epoch": 0.5780487804878048,
      "step": 10665,
      "training_loss": 5.109130859375
    },
    {
      "epoch": 0.5781029810298103,
      "step": 10666,
      "training_loss": 7.828357696533203
    },
    {
      "epoch": 0.5781571815718157,
      "step": 10667,
      "training_loss": 7.52353572845459
    },
    {
      "epoch": 0.5782113821138212,
      "grad_norm": 22.122730255126953,
      "learning_rate": 1e-05,
      "loss": 7.0249,
      "step": 10668
    },
    {
      "epoch": 0.5782113821138212,
      "step": 10668,
      "training_loss": 7.150613307952881
    },
    {
      "epoch": 0.5782655826558265,
      "step": 10669,
      "training_loss": 6.507049083709717
    },
    {
      "epoch": 0.578319783197832,
      "step": 10670,
      "training_loss": 5.935432434082031
    },
    {
      "epoch": 0.5783739837398374,
      "step": 10671,
      "training_loss": 6.691883563995361
    },
    {
      "epoch": 0.5784281842818428,
      "grad_norm": 24.03052520751953,
      "learning_rate": 1e-05,
      "loss": 6.5712,
      "step": 10672
    },
    {
      "epoch": 0.5784281842818428,
      "step": 10672,
      "training_loss": 7.552915573120117
    },
    {
      "epoch": 0.5784823848238483,
      "step": 10673,
      "training_loss": 6.6719584465026855
    },
    {
      "epoch": 0.5785365853658536,
      "step": 10674,
      "training_loss": 6.330356121063232
    },
    {
      "epoch": 0.5785907859078591,
      "step": 10675,
      "training_loss": 7.782793998718262
    },
    {
      "epoch": 0.5786449864498645,
      "grad_norm": 57.11579895019531,
      "learning_rate": 1e-05,
      "loss": 7.0845,
      "step": 10676
    },
    {
      "epoch": 0.5786449864498645,
      "step": 10676,
      "training_loss": 5.922139644622803
    },
    {
      "epoch": 0.57869918699187,
      "step": 10677,
      "training_loss": 7.121176719665527
    },
    {
      "epoch": 0.5787533875338753,
      "step": 10678,
      "training_loss": 7.65575647354126
    },
    {
      "epoch": 0.5788075880758807,
      "step": 10679,
      "training_loss": 7.041973114013672
    },
    {
      "epoch": 0.5788617886178862,
      "grad_norm": 30.661161422729492,
      "learning_rate": 1e-05,
      "loss": 6.9353,
      "step": 10680
    },
    {
      "epoch": 0.5788617886178862,
      "step": 10680,
      "training_loss": 6.32002592086792
    },
    {
      "epoch": 0.5789159891598916,
      "step": 10681,
      "training_loss": 6.730837345123291
    },
    {
      "epoch": 0.5789701897018971,
      "step": 10682,
      "training_loss": 5.292316436767578
    },
    {
      "epoch": 0.5790243902439024,
      "step": 10683,
      "training_loss": 5.24578857421875
    },
    {
      "epoch": 0.5790785907859078,
      "grad_norm": 31.41204833984375,
      "learning_rate": 1e-05,
      "loss": 5.8972,
      "step": 10684
    },
    {
      "epoch": 0.5790785907859078,
      "step": 10684,
      "training_loss": 7.303308010101318
    },
    {
      "epoch": 0.5791327913279133,
      "step": 10685,
      "training_loss": 8.017271041870117
    },
    {
      "epoch": 0.5791869918699187,
      "step": 10686,
      "training_loss": 7.230189800262451
    },
    {
      "epoch": 0.5792411924119241,
      "step": 10687,
      "training_loss": 5.121545314788818
    },
    {
      "epoch": 0.5792953929539295,
      "grad_norm": 32.54673385620117,
      "learning_rate": 1e-05,
      "loss": 6.9181,
      "step": 10688
    },
    {
      "epoch": 0.5792953929539295,
      "step": 10688,
      "training_loss": 3.1690587997436523
    },
    {
      "epoch": 0.579349593495935,
      "step": 10689,
      "training_loss": 6.8542680740356445
    },
    {
      "epoch": 0.5794037940379404,
      "step": 10690,
      "training_loss": 7.94221305847168
    },
    {
      "epoch": 0.5794579945799458,
      "step": 10691,
      "training_loss": 6.747151851654053
    },
    {
      "epoch": 0.5795121951219512,
      "grad_norm": 24.096202850341797,
      "learning_rate": 1e-05,
      "loss": 6.1782,
      "step": 10692
    },
    {
      "epoch": 0.5795121951219512,
      "step": 10692,
      "training_loss": 6.873372554779053
    },
    {
      "epoch": 0.5795663956639566,
      "step": 10693,
      "training_loss": 7.672433376312256
    },
    {
      "epoch": 0.5796205962059621,
      "step": 10694,
      "training_loss": 5.493031024932861
    },
    {
      "epoch": 0.5796747967479675,
      "step": 10695,
      "training_loss": 6.084115505218506
    },
    {
      "epoch": 0.5797289972899728,
      "grad_norm": 22.863571166992188,
      "learning_rate": 1e-05,
      "loss": 6.5307,
      "step": 10696
    },
    {
      "epoch": 0.5797289972899728,
      "step": 10696,
      "training_loss": 7.586788654327393
    },
    {
      "epoch": 0.5797831978319783,
      "step": 10697,
      "training_loss": 6.478992462158203
    },
    {
      "epoch": 0.5798373983739837,
      "step": 10698,
      "training_loss": 7.263560771942139
    },
    {
      "epoch": 0.5798915989159892,
      "step": 10699,
      "training_loss": 7.127529621124268
    },
    {
      "epoch": 0.5799457994579946,
      "grad_norm": 39.41047668457031,
      "learning_rate": 1e-05,
      "loss": 7.1142,
      "step": 10700
    },
    {
      "epoch": 0.5799457994579946,
      "step": 10700,
      "training_loss": 6.615542888641357
    },
    {
      "epoch": 0.58,
      "step": 10701,
      "training_loss": 8.019083023071289
    },
    {
      "epoch": 0.5800542005420054,
      "step": 10702,
      "training_loss": 5.787032127380371
    },
    {
      "epoch": 0.5801084010840108,
      "step": 10703,
      "training_loss": 6.420877933502197
    },
    {
      "epoch": 0.5801626016260163,
      "grad_norm": 33.18110275268555,
      "learning_rate": 1e-05,
      "loss": 6.7106,
      "step": 10704
    },
    {
      "epoch": 0.5801626016260163,
      "step": 10704,
      "training_loss": 3.395906686782837
    },
    {
      "epoch": 0.5802168021680216,
      "step": 10705,
      "training_loss": 7.172216892242432
    },
    {
      "epoch": 0.5802710027100271,
      "step": 10706,
      "training_loss": 6.948159694671631
    },
    {
      "epoch": 0.5803252032520325,
      "step": 10707,
      "training_loss": 7.189533233642578
    },
    {
      "epoch": 0.580379403794038,
      "grad_norm": 23.817886352539062,
      "learning_rate": 1e-05,
      "loss": 6.1765,
      "step": 10708
    },
    {
      "epoch": 0.580379403794038,
      "step": 10708,
      "training_loss": 6.645956516265869
    },
    {
      "epoch": 0.5804336043360434,
      "step": 10709,
      "training_loss": 6.893399238586426
    },
    {
      "epoch": 0.5804878048780487,
      "step": 10710,
      "training_loss": 6.814138889312744
    },
    {
      "epoch": 0.5805420054200542,
      "step": 10711,
      "training_loss": 5.252736568450928
    },
    {
      "epoch": 0.5805962059620596,
      "grad_norm": 30.222328186035156,
      "learning_rate": 1e-05,
      "loss": 6.4016,
      "step": 10712
    },
    {
      "epoch": 0.5805962059620596,
      "step": 10712,
      "training_loss": 6.563167572021484
    },
    {
      "epoch": 0.5806504065040651,
      "step": 10713,
      "training_loss": 5.328951835632324
    },
    {
      "epoch": 0.5807046070460704,
      "step": 10714,
      "training_loss": 6.6717000007629395
    },
    {
      "epoch": 0.5807588075880759,
      "step": 10715,
      "training_loss": 3.4001104831695557
    },
    {
      "epoch": 0.5808130081300813,
      "grad_norm": 34.13042068481445,
      "learning_rate": 1e-05,
      "loss": 5.491,
      "step": 10716
    },
    {
      "epoch": 0.5808130081300813,
      "step": 10716,
      "training_loss": 6.79336404800415
    },
    {
      "epoch": 0.5808672086720867,
      "step": 10717,
      "training_loss": 6.952476501464844
    },
    {
      "epoch": 0.5809214092140922,
      "step": 10718,
      "training_loss": 7.824494361877441
    },
    {
      "epoch": 0.5809756097560975,
      "step": 10719,
      "training_loss": 7.143118381500244
    },
    {
      "epoch": 0.581029810298103,
      "grad_norm": 19.207599639892578,
      "learning_rate": 1e-05,
      "loss": 7.1784,
      "step": 10720
    },
    {
      "epoch": 0.581029810298103,
      "step": 10720,
      "training_loss": 6.89040470123291
    },
    {
      "epoch": 0.5810840108401084,
      "step": 10721,
      "training_loss": 8.750863075256348
    },
    {
      "epoch": 0.5811382113821139,
      "step": 10722,
      "training_loss": 3.7774901390075684
    },
    {
      "epoch": 0.5811924119241192,
      "step": 10723,
      "training_loss": 6.755903244018555
    },
    {
      "epoch": 0.5812466124661246,
      "grad_norm": 24.326583862304688,
      "learning_rate": 1e-05,
      "loss": 6.5437,
      "step": 10724
    },
    {
      "epoch": 0.5812466124661246,
      "step": 10724,
      "training_loss": 6.888076305389404
    },
    {
      "epoch": 0.5813008130081301,
      "step": 10725,
      "training_loss": 5.851171493530273
    },
    {
      "epoch": 0.5813550135501355,
      "step": 10726,
      "training_loss": 7.104116439819336
    },
    {
      "epoch": 0.581409214092141,
      "step": 10727,
      "training_loss": 9.426363945007324
    },
    {
      "epoch": 0.5814634146341463,
      "grad_norm": 66.2968521118164,
      "learning_rate": 1e-05,
      "loss": 7.3174,
      "step": 10728
    },
    {
      "epoch": 0.5814634146341463,
      "step": 10728,
      "training_loss": 7.729812145233154
    },
    {
      "epoch": 0.5815176151761517,
      "step": 10729,
      "training_loss": 3.5189573764801025
    },
    {
      "epoch": 0.5815718157181572,
      "step": 10730,
      "training_loss": 7.089015483856201
    },
    {
      "epoch": 0.5816260162601626,
      "step": 10731,
      "training_loss": 7.926569938659668
    },
    {
      "epoch": 0.581680216802168,
      "grad_norm": 38.33790588378906,
      "learning_rate": 1e-05,
      "loss": 6.5661,
      "step": 10732
    },
    {
      "epoch": 0.581680216802168,
      "step": 10732,
      "training_loss": 5.506194591522217
    },
    {
      "epoch": 0.5817344173441734,
      "step": 10733,
      "training_loss": 5.781031131744385
    },
    {
      "epoch": 0.5817886178861789,
      "step": 10734,
      "training_loss": 5.881262302398682
    },
    {
      "epoch": 0.5818428184281843,
      "step": 10735,
      "training_loss": 6.821425437927246
    },
    {
      "epoch": 0.5818970189701897,
      "grad_norm": 28.07146644592285,
      "learning_rate": 1e-05,
      "loss": 5.9975,
      "step": 10736
    },
    {
      "epoch": 0.5818970189701897,
      "step": 10736,
      "training_loss": 5.791848659515381
    },
    {
      "epoch": 0.5819512195121951,
      "step": 10737,
      "training_loss": 6.648776531219482
    },
    {
      "epoch": 0.5820054200542005,
      "step": 10738,
      "training_loss": 8.215598106384277
    },
    {
      "epoch": 0.582059620596206,
      "step": 10739,
      "training_loss": 6.210207939147949
    },
    {
      "epoch": 0.5821138211382114,
      "grad_norm": 50.478111267089844,
      "learning_rate": 1e-05,
      "loss": 6.7166,
      "step": 10740
    },
    {
      "epoch": 0.5821138211382114,
      "step": 10740,
      "training_loss": 5.432884693145752
    },
    {
      "epoch": 0.5821680216802168,
      "step": 10741,
      "training_loss": 6.6900739669799805
    },
    {
      "epoch": 0.5822222222222222,
      "step": 10742,
      "training_loss": 5.338370323181152
    },
    {
      "epoch": 0.5822764227642276,
      "step": 10743,
      "training_loss": 5.872139930725098
    },
    {
      "epoch": 0.5823306233062331,
      "grad_norm": 27.67526626586914,
      "learning_rate": 1e-05,
      "loss": 5.8334,
      "step": 10744
    },
    {
      "epoch": 0.5823306233062331,
      "step": 10744,
      "training_loss": 5.314620018005371
    },
    {
      "epoch": 0.5823848238482385,
      "step": 10745,
      "training_loss": 4.881579875946045
    },
    {
      "epoch": 0.5824390243902439,
      "step": 10746,
      "training_loss": 4.5221052169799805
    },
    {
      "epoch": 0.5824932249322493,
      "step": 10747,
      "training_loss": 7.036464214324951
    },
    {
      "epoch": 0.5825474254742548,
      "grad_norm": 27.047433853149414,
      "learning_rate": 1e-05,
      "loss": 5.4387,
      "step": 10748
    },
    {
      "epoch": 0.5825474254742548,
      "step": 10748,
      "training_loss": 7.2198381423950195
    },
    {
      "epoch": 0.5826016260162602,
      "step": 10749,
      "training_loss": 4.533426284790039
    },
    {
      "epoch": 0.5826558265582655,
      "step": 10750,
      "training_loss": 7.0246477127075195
    },
    {
      "epoch": 0.582710027100271,
      "step": 10751,
      "training_loss": 5.7280755043029785
    },
    {
      "epoch": 0.5827642276422764,
      "grad_norm": 26.241666793823242,
      "learning_rate": 1e-05,
      "loss": 6.1265,
      "step": 10752
    },
    {
      "epoch": 0.5827642276422764,
      "step": 10752,
      "training_loss": 8.094752311706543
    },
    {
      "epoch": 0.5828184281842819,
      "step": 10753,
      "training_loss": 6.278922080993652
    },
    {
      "epoch": 0.5828726287262873,
      "step": 10754,
      "training_loss": 7.595825672149658
    },
    {
      "epoch": 0.5829268292682926,
      "step": 10755,
      "training_loss": 4.088876247406006
    },
    {
      "epoch": 0.5829810298102981,
      "grad_norm": 37.85654830932617,
      "learning_rate": 1e-05,
      "loss": 6.5146,
      "step": 10756
    },
    {
      "epoch": 0.5829810298102981,
      "step": 10756,
      "training_loss": 5.554528713226318
    },
    {
      "epoch": 0.5830352303523035,
      "step": 10757,
      "training_loss": 8.47893238067627
    },
    {
      "epoch": 0.583089430894309,
      "step": 10758,
      "training_loss": 7.561848163604736
    },
    {
      "epoch": 0.5831436314363143,
      "step": 10759,
      "training_loss": 7.4334211349487305
    },
    {
      "epoch": 0.5831978319783198,
      "grad_norm": 41.7794075012207,
      "learning_rate": 1e-05,
      "loss": 7.2572,
      "step": 10760
    },
    {
      "epoch": 0.5831978319783198,
      "step": 10760,
      "training_loss": 6.936636447906494
    },
    {
      "epoch": 0.5832520325203252,
      "step": 10761,
      "training_loss": 6.9557390213012695
    },
    {
      "epoch": 0.5833062330623306,
      "step": 10762,
      "training_loss": 7.106219291687012
    },
    {
      "epoch": 0.5833604336043361,
      "step": 10763,
      "training_loss": 6.178569793701172
    },
    {
      "epoch": 0.5834146341463414,
      "grad_norm": 35.035484313964844,
      "learning_rate": 1e-05,
      "loss": 6.7943,
      "step": 10764
    },
    {
      "epoch": 0.5834146341463414,
      "step": 10764,
      "training_loss": 5.497514724731445
    },
    {
      "epoch": 0.5834688346883469,
      "step": 10765,
      "training_loss": 7.275423049926758
    },
    {
      "epoch": 0.5835230352303523,
      "step": 10766,
      "training_loss": 7.254736423492432
    },
    {
      "epoch": 0.5835772357723578,
      "step": 10767,
      "training_loss": 6.762832164764404
    },
    {
      "epoch": 0.5836314363143631,
      "grad_norm": 28.320152282714844,
      "learning_rate": 1e-05,
      "loss": 6.6976,
      "step": 10768
    },
    {
      "epoch": 0.5836314363143631,
      "step": 10768,
      "training_loss": 6.5917558670043945
    },
    {
      "epoch": 0.5836856368563685,
      "step": 10769,
      "training_loss": 5.617501258850098
    },
    {
      "epoch": 0.583739837398374,
      "step": 10770,
      "training_loss": 7.910782814025879
    },
    {
      "epoch": 0.5837940379403794,
      "step": 10771,
      "training_loss": 5.9974470138549805
    },
    {
      "epoch": 0.5838482384823849,
      "grad_norm": 21.32032585144043,
      "learning_rate": 1e-05,
      "loss": 6.5294,
      "step": 10772
    },
    {
      "epoch": 0.5838482384823849,
      "step": 10772,
      "training_loss": 6.191165447235107
    },
    {
      "epoch": 0.5839024390243902,
      "step": 10773,
      "training_loss": 7.189765453338623
    },
    {
      "epoch": 0.5839566395663957,
      "step": 10774,
      "training_loss": 5.528679370880127
    },
    {
      "epoch": 0.5840108401084011,
      "step": 10775,
      "training_loss": 7.746912956237793
    },
    {
      "epoch": 0.5840650406504065,
      "grad_norm": 38.54292678833008,
      "learning_rate": 1e-05,
      "loss": 6.6641,
      "step": 10776
    },
    {
      "epoch": 0.5840650406504065,
      "step": 10776,
      "training_loss": 6.296053409576416
    },
    {
      "epoch": 0.5841192411924119,
      "step": 10777,
      "training_loss": 6.402445316314697
    },
    {
      "epoch": 0.5841734417344173,
      "step": 10778,
      "training_loss": 6.0141801834106445
    },
    {
      "epoch": 0.5842276422764228,
      "step": 10779,
      "training_loss": 6.684527397155762
    },
    {
      "epoch": 0.5842818428184282,
      "grad_norm": 23.75824737548828,
      "learning_rate": 1e-05,
      "loss": 6.3493,
      "step": 10780
    },
    {
      "epoch": 0.5842818428184282,
      "step": 10780,
      "training_loss": 7.14809513092041
    },
    {
      "epoch": 0.5843360433604337,
      "step": 10781,
      "training_loss": 6.241199493408203
    },
    {
      "epoch": 0.584390243902439,
      "step": 10782,
      "training_loss": 6.945287227630615
    },
    {
      "epoch": 0.5844444444444444,
      "step": 10783,
      "training_loss": 5.626240253448486
    },
    {
      "epoch": 0.5844986449864499,
      "grad_norm": 45.01643753051758,
      "learning_rate": 1e-05,
      "loss": 6.4902,
      "step": 10784
    },
    {
      "epoch": 0.5844986449864499,
      "step": 10784,
      "training_loss": 6.398335933685303
    },
    {
      "epoch": 0.5845528455284553,
      "step": 10785,
      "training_loss": 7.155322551727295
    },
    {
      "epoch": 0.5846070460704607,
      "step": 10786,
      "training_loss": 7.956261157989502
    },
    {
      "epoch": 0.5846612466124661,
      "step": 10787,
      "training_loss": 7.964447975158691
    },
    {
      "epoch": 0.5847154471544715,
      "grad_norm": 32.76669692993164,
      "learning_rate": 1e-05,
      "loss": 7.3686,
      "step": 10788
    },
    {
      "epoch": 0.5847154471544715,
      "step": 10788,
      "training_loss": 5.176779747009277
    },
    {
      "epoch": 0.584769647696477,
      "step": 10789,
      "training_loss": 6.98179817199707
    },
    {
      "epoch": 0.5848238482384824,
      "step": 10790,
      "training_loss": 6.0992817878723145
    },
    {
      "epoch": 0.5848780487804878,
      "step": 10791,
      "training_loss": 8.20240306854248
    },
    {
      "epoch": 0.5849322493224932,
      "grad_norm": 18.977731704711914,
      "learning_rate": 1e-05,
      "loss": 6.6151,
      "step": 10792
    },
    {
      "epoch": 0.5849322493224932,
      "step": 10792,
      "training_loss": 8.350988388061523
    },
    {
      "epoch": 0.5849864498644987,
      "step": 10793,
      "training_loss": 5.289259910583496
    },
    {
      "epoch": 0.5850406504065041,
      "step": 10794,
      "training_loss": 6.769769191741943
    },
    {
      "epoch": 0.5850948509485094,
      "step": 10795,
      "training_loss": 4.688404560089111
    },
    {
      "epoch": 0.5851490514905149,
      "grad_norm": 38.34771728515625,
      "learning_rate": 1e-05,
      "loss": 6.2746,
      "step": 10796
    },
    {
      "epoch": 0.5851490514905149,
      "step": 10796,
      "training_loss": 6.963413715362549
    },
    {
      "epoch": 0.5852032520325203,
      "step": 10797,
      "training_loss": 5.193933010101318
    },
    {
      "epoch": 0.5852574525745258,
      "step": 10798,
      "training_loss": 5.595315456390381
    },
    {
      "epoch": 0.5853116531165312,
      "step": 10799,
      "training_loss": 6.668007850646973
    },
    {
      "epoch": 0.5853658536585366,
      "grad_norm": 29.41156005859375,
      "learning_rate": 1e-05,
      "loss": 6.1052,
      "step": 10800
    },
    {
      "epoch": 0.5853658536585366,
      "step": 10800,
      "training_loss": 3.9791009426116943
    },
    {
      "epoch": 0.585420054200542,
      "step": 10801,
      "training_loss": 6.788079738616943
    },
    {
      "epoch": 0.5854742547425474,
      "step": 10802,
      "training_loss": 7.332408428192139
    },
    {
      "epoch": 0.5855284552845529,
      "step": 10803,
      "training_loss": 7.771402359008789
    },
    {
      "epoch": 0.5855826558265582,
      "grad_norm": 35.53688049316406,
      "learning_rate": 1e-05,
      "loss": 6.4677,
      "step": 10804
    },
    {
      "epoch": 0.5855826558265582,
      "step": 10804,
      "training_loss": 4.527338981628418
    },
    {
      "epoch": 0.5856368563685637,
      "step": 10805,
      "training_loss": 7.055393695831299
    },
    {
      "epoch": 0.5856910569105691,
      "step": 10806,
      "training_loss": 5.661281585693359
    },
    {
      "epoch": 0.5857452574525746,
      "step": 10807,
      "training_loss": 6.472621440887451
    },
    {
      "epoch": 0.58579945799458,
      "grad_norm": 23.06268882751465,
      "learning_rate": 1e-05,
      "loss": 5.9292,
      "step": 10808
    },
    {
      "epoch": 0.58579945799458,
      "step": 10808,
      "training_loss": 7.377439022064209
    },
    {
      "epoch": 0.5858536585365853,
      "step": 10809,
      "training_loss": 6.953929901123047
    },
    {
      "epoch": 0.5859078590785908,
      "step": 10810,
      "training_loss": 7.758964538574219
    },
    {
      "epoch": 0.5859620596205962,
      "step": 10811,
      "training_loss": 5.9151763916015625
    },
    {
      "epoch": 0.5860162601626017,
      "grad_norm": 23.693817138671875,
      "learning_rate": 1e-05,
      "loss": 7.0014,
      "step": 10812
    },
    {
      "epoch": 0.5860162601626017,
      "step": 10812,
      "training_loss": 7.025575637817383
    },
    {
      "epoch": 0.586070460704607,
      "step": 10813,
      "training_loss": 7.075145721435547
    },
    {
      "epoch": 0.5861246612466124,
      "step": 10814,
      "training_loss": 6.905951023101807
    },
    {
      "epoch": 0.5861788617886179,
      "step": 10815,
      "training_loss": 7.1766862869262695
    },
    {
      "epoch": 0.5862330623306233,
      "grad_norm": 21.635770797729492,
      "learning_rate": 1e-05,
      "loss": 7.0458,
      "step": 10816
    },
    {
      "epoch": 0.5862330623306233,
      "step": 10816,
      "training_loss": 7.380486011505127
    },
    {
      "epoch": 0.5862872628726288,
      "step": 10817,
      "training_loss": 6.919126987457275
    },
    {
      "epoch": 0.5863414634146341,
      "step": 10818,
      "training_loss": 4.435816764831543
    },
    {
      "epoch": 0.5863956639566396,
      "step": 10819,
      "training_loss": 7.17893123626709
    },
    {
      "epoch": 0.586449864498645,
      "grad_norm": 18.837255477905273,
      "learning_rate": 1e-05,
      "loss": 6.4786,
      "step": 10820
    },
    {
      "epoch": 0.586449864498645,
      "step": 10820,
      "training_loss": 7.522952556610107
    },
    {
      "epoch": 0.5865040650406504,
      "step": 10821,
      "training_loss": 6.437590599060059
    },
    {
      "epoch": 0.5865582655826558,
      "step": 10822,
      "training_loss": 6.453891277313232
    },
    {
      "epoch": 0.5866124661246612,
      "step": 10823,
      "training_loss": 7.8945183753967285
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 25.07820701599121,
      "learning_rate": 1e-05,
      "loss": 7.0772,
      "step": 10824
    },
    {
      "epoch": 0.5866666666666667,
      "step": 10824,
      "training_loss": 6.708160877227783
    },
    {
      "epoch": 0.5867208672086721,
      "step": 10825,
      "training_loss": 6.499424457550049
    },
    {
      "epoch": 0.5867750677506776,
      "step": 10826,
      "training_loss": 6.619016647338867
    },
    {
      "epoch": 0.5868292682926829,
      "step": 10827,
      "training_loss": 6.592101573944092
    },
    {
      "epoch": 0.5868834688346883,
      "grad_norm": 25.250526428222656,
      "learning_rate": 1e-05,
      "loss": 6.6047,
      "step": 10828
    },
    {
      "epoch": 0.5868834688346883,
      "step": 10828,
      "training_loss": 5.591179847717285
    },
    {
      "epoch": 0.5869376693766938,
      "step": 10829,
      "training_loss": 7.40268611907959
    },
    {
      "epoch": 0.5869918699186992,
      "step": 10830,
      "training_loss": 6.768598556518555
    },
    {
      "epoch": 0.5870460704607046,
      "step": 10831,
      "training_loss": 6.797879695892334
    },
    {
      "epoch": 0.58710027100271,
      "grad_norm": 15.76722526550293,
      "learning_rate": 1e-05,
      "loss": 6.6401,
      "step": 10832
    },
    {
      "epoch": 0.58710027100271,
      "step": 10832,
      "training_loss": 7.415195941925049
    },
    {
      "epoch": 0.5871544715447155,
      "step": 10833,
      "training_loss": 6.868375301361084
    },
    {
      "epoch": 0.5872086720867209,
      "step": 10834,
      "training_loss": 6.6106791496276855
    },
    {
      "epoch": 0.5872628726287263,
      "step": 10835,
      "training_loss": 6.313642978668213
    },
    {
      "epoch": 0.5873170731707317,
      "grad_norm": 25.991504669189453,
      "learning_rate": 1e-05,
      "loss": 6.802,
      "step": 10836
    },
    {
      "epoch": 0.5873170731707317,
      "step": 10836,
      "training_loss": 7.378246307373047
    },
    {
      "epoch": 0.5873712737127371,
      "step": 10837,
      "training_loss": 7.908206939697266
    },
    {
      "epoch": 0.5874254742547426,
      "step": 10838,
      "training_loss": 6.948360443115234
    },
    {
      "epoch": 0.587479674796748,
      "step": 10839,
      "training_loss": 5.744277477264404
    },
    {
      "epoch": 0.5875338753387533,
      "grad_norm": 37.295936584472656,
      "learning_rate": 1e-05,
      "loss": 6.9948,
      "step": 10840
    },
    {
      "epoch": 0.5875338753387533,
      "step": 10840,
      "training_loss": 6.242033004760742
    },
    {
      "epoch": 0.5875880758807588,
      "step": 10841,
      "training_loss": 6.12424898147583
    },
    {
      "epoch": 0.5876422764227642,
      "step": 10842,
      "training_loss": 6.988702774047852
    },
    {
      "epoch": 0.5876964769647697,
      "step": 10843,
      "training_loss": 5.298373222351074
    },
    {
      "epoch": 0.5877506775067751,
      "grad_norm": 26.0887451171875,
      "learning_rate": 1e-05,
      "loss": 6.1633,
      "step": 10844
    },
    {
      "epoch": 0.5877506775067751,
      "step": 10844,
      "training_loss": 7.762077808380127
    },
    {
      "epoch": 0.5878048780487805,
      "step": 10845,
      "training_loss": 6.441713333129883
    },
    {
      "epoch": 0.5878590785907859,
      "step": 10846,
      "training_loss": 5.997550964355469
    },
    {
      "epoch": 0.5879132791327913,
      "step": 10847,
      "training_loss": 6.809454917907715
    },
    {
      "epoch": 0.5879674796747968,
      "grad_norm": 28.445541381835938,
      "learning_rate": 1e-05,
      "loss": 6.7527,
      "step": 10848
    },
    {
      "epoch": 0.5879674796747968,
      "step": 10848,
      "training_loss": 6.041167736053467
    },
    {
      "epoch": 0.5880216802168021,
      "step": 10849,
      "training_loss": 6.331148624420166
    },
    {
      "epoch": 0.5880758807588076,
      "step": 10850,
      "training_loss": 6.954871654510498
    },
    {
      "epoch": 0.588130081300813,
      "step": 10851,
      "training_loss": 7.082447052001953
    },
    {
      "epoch": 0.5881842818428185,
      "grad_norm": 20.744476318359375,
      "learning_rate": 1e-05,
      "loss": 6.6024,
      "step": 10852
    },
    {
      "epoch": 0.5881842818428185,
      "step": 10852,
      "training_loss": 3.7333059310913086
    },
    {
      "epoch": 0.5882384823848239,
      "step": 10853,
      "training_loss": 6.351018905639648
    },
    {
      "epoch": 0.5882926829268292,
      "step": 10854,
      "training_loss": 6.401708602905273
    },
    {
      "epoch": 0.5883468834688347,
      "step": 10855,
      "training_loss": 6.0004754066467285
    },
    {
      "epoch": 0.5884010840108401,
      "grad_norm": 21.86050033569336,
      "learning_rate": 1e-05,
      "loss": 5.6216,
      "step": 10856
    },
    {
      "epoch": 0.5884010840108401,
      "step": 10856,
      "training_loss": 6.679627895355225
    },
    {
      "epoch": 0.5884552845528456,
      "step": 10857,
      "training_loss": 7.575232982635498
    },
    {
      "epoch": 0.5885094850948509,
      "step": 10858,
      "training_loss": 6.357011318206787
    },
    {
      "epoch": 0.5885636856368563,
      "step": 10859,
      "training_loss": 7.600066184997559
    },
    {
      "epoch": 0.5886178861788618,
      "grad_norm": 24.37816619873047,
      "learning_rate": 1e-05,
      "loss": 7.053,
      "step": 10860
    },
    {
      "epoch": 0.5886178861788618,
      "step": 10860,
      "training_loss": 8.496696472167969
    },
    {
      "epoch": 0.5886720867208672,
      "step": 10861,
      "training_loss": 6.611633777618408
    },
    {
      "epoch": 0.5887262872628727,
      "step": 10862,
      "training_loss": 5.805522441864014
    },
    {
      "epoch": 0.588780487804878,
      "step": 10863,
      "training_loss": 8.25708293914795
    },
    {
      "epoch": 0.5888346883468835,
      "grad_norm": 39.84707260131836,
      "learning_rate": 1e-05,
      "loss": 7.2927,
      "step": 10864
    },
    {
      "epoch": 0.5888346883468835,
      "step": 10864,
      "training_loss": 6.600860118865967
    },
    {
      "epoch": 0.5888888888888889,
      "step": 10865,
      "training_loss": 6.194369792938232
    },
    {
      "epoch": 0.5889430894308944,
      "step": 10866,
      "training_loss": 5.367364883422852
    },
    {
      "epoch": 0.5889972899728997,
      "step": 10867,
      "training_loss": 8.00243091583252
    },
    {
      "epoch": 0.5890514905149051,
      "grad_norm": 24.986318588256836,
      "learning_rate": 1e-05,
      "loss": 6.5413,
      "step": 10868
    },
    {
      "epoch": 0.5890514905149051,
      "step": 10868,
      "training_loss": 7.399446964263916
    },
    {
      "epoch": 0.5891056910569106,
      "step": 10869,
      "training_loss": 6.008026123046875
    },
    {
      "epoch": 0.589159891598916,
      "step": 10870,
      "training_loss": 6.302243709564209
    },
    {
      "epoch": 0.5892140921409215,
      "step": 10871,
      "training_loss": 6.882573127746582
    },
    {
      "epoch": 0.5892682926829268,
      "grad_norm": 30.16670799255371,
      "learning_rate": 1e-05,
      "loss": 6.6481,
      "step": 10872
    },
    {
      "epoch": 0.5892682926829268,
      "step": 10872,
      "training_loss": 7.302220821380615
    },
    {
      "epoch": 0.5893224932249322,
      "step": 10873,
      "training_loss": 6.109467506408691
    },
    {
      "epoch": 0.5893766937669377,
      "step": 10874,
      "training_loss": 5.90629243850708
    },
    {
      "epoch": 0.5894308943089431,
      "step": 10875,
      "training_loss": 6.722400188446045
    },
    {
      "epoch": 0.5894850948509485,
      "grad_norm": 21.680017471313477,
      "learning_rate": 1e-05,
      "loss": 6.5101,
      "step": 10876
    },
    {
      "epoch": 0.5894850948509485,
      "step": 10876,
      "training_loss": 6.612547874450684
    },
    {
      "epoch": 0.5895392953929539,
      "step": 10877,
      "training_loss": 7.70654821395874
    },
    {
      "epoch": 0.5895934959349594,
      "step": 10878,
      "training_loss": 6.561415195465088
    },
    {
      "epoch": 0.5896476964769648,
      "step": 10879,
      "training_loss": 5.846531867980957
    },
    {
      "epoch": 0.5897018970189702,
      "grad_norm": 26.68223762512207,
      "learning_rate": 1e-05,
      "loss": 6.6818,
      "step": 10880
    },
    {
      "epoch": 0.5897018970189702,
      "step": 10880,
      "training_loss": 6.198846817016602
    },
    {
      "epoch": 0.5897560975609756,
      "step": 10881,
      "training_loss": 4.2323994636535645
    },
    {
      "epoch": 0.589810298102981,
      "step": 10882,
      "training_loss": 6.901535987854004
    },
    {
      "epoch": 0.5898644986449865,
      "step": 10883,
      "training_loss": 6.53735876083374
    },
    {
      "epoch": 0.5899186991869919,
      "grad_norm": 25.285293579101562,
      "learning_rate": 1e-05,
      "loss": 5.9675,
      "step": 10884
    },
    {
      "epoch": 0.5899186991869919,
      "step": 10884,
      "training_loss": 7.098373889923096
    },
    {
      "epoch": 0.5899728997289972,
      "step": 10885,
      "training_loss": 6.911816596984863
    },
    {
      "epoch": 0.5900271002710027,
      "step": 10886,
      "training_loss": 6.398226737976074
    },
    {
      "epoch": 0.5900813008130081,
      "step": 10887,
      "training_loss": 7.497204780578613
    },
    {
      "epoch": 0.5901355013550136,
      "grad_norm": 22.433027267456055,
      "learning_rate": 1e-05,
      "loss": 6.9764,
      "step": 10888
    },
    {
      "epoch": 0.5901355013550136,
      "step": 10888,
      "training_loss": 6.67626953125
    },
    {
      "epoch": 0.590189701897019,
      "step": 10889,
      "training_loss": 7.2457075119018555
    },
    {
      "epoch": 0.5902439024390244,
      "step": 10890,
      "training_loss": 6.280462741851807
    },
    {
      "epoch": 0.5902981029810298,
      "step": 10891,
      "training_loss": 6.938139915466309
    },
    {
      "epoch": 0.5903523035230352,
      "grad_norm": 22.010112762451172,
      "learning_rate": 1e-05,
      "loss": 6.7851,
      "step": 10892
    },
    {
      "epoch": 0.5903523035230352,
      "step": 10892,
      "training_loss": 6.0924973487854
    },
    {
      "epoch": 0.5904065040650407,
      "step": 10893,
      "training_loss": 7.857004165649414
    },
    {
      "epoch": 0.590460704607046,
      "step": 10894,
      "training_loss": 7.597013473510742
    },
    {
      "epoch": 0.5905149051490515,
      "step": 10895,
      "training_loss": 7.647669315338135
    },
    {
      "epoch": 0.5905691056910569,
      "grad_norm": 27.210851669311523,
      "learning_rate": 1e-05,
      "loss": 7.2985,
      "step": 10896
    },
    {
      "epoch": 0.5905691056910569,
      "step": 10896,
      "training_loss": 6.2586894035339355
    },
    {
      "epoch": 0.5906233062330624,
      "step": 10897,
      "training_loss": 7.443748474121094
    },
    {
      "epoch": 0.5906775067750677,
      "step": 10898,
      "training_loss": 7.3205671310424805
    },
    {
      "epoch": 0.5907317073170731,
      "step": 10899,
      "training_loss": 4.82878303527832
    },
    {
      "epoch": 0.5907859078590786,
      "grad_norm": 29.55854606628418,
      "learning_rate": 1e-05,
      "loss": 6.4629,
      "step": 10900
    },
    {
      "epoch": 0.5907859078590786,
      "step": 10900,
      "training_loss": 7.292073726654053
    },
    {
      "epoch": 0.590840108401084,
      "step": 10901,
      "training_loss": 6.155125617980957
    },
    {
      "epoch": 0.5908943089430895,
      "step": 10902,
      "training_loss": 6.722681522369385
    },
    {
      "epoch": 0.5909485094850948,
      "step": 10903,
      "training_loss": 6.93394136428833
    },
    {
      "epoch": 0.5910027100271003,
      "grad_norm": 62.040504455566406,
      "learning_rate": 1e-05,
      "loss": 6.776,
      "step": 10904
    },
    {
      "epoch": 0.5910027100271003,
      "step": 10904,
      "training_loss": 7.2113261222839355
    },
    {
      "epoch": 0.5910569105691057,
      "step": 10905,
      "training_loss": 6.015541076660156
    },
    {
      "epoch": 0.5911111111111111,
      "step": 10906,
      "training_loss": 7.038764476776123
    },
    {
      "epoch": 0.5911653116531165,
      "step": 10907,
      "training_loss": 4.677352428436279
    },
    {
      "epoch": 0.5912195121951219,
      "grad_norm": 33.5398063659668,
      "learning_rate": 1e-05,
      "loss": 6.2357,
      "step": 10908
    },
    {
      "epoch": 0.5912195121951219,
      "step": 10908,
      "training_loss": 5.128088474273682
    },
    {
      "epoch": 0.5912737127371274,
      "step": 10909,
      "training_loss": 7.327917575836182
    },
    {
      "epoch": 0.5913279132791328,
      "step": 10910,
      "training_loss": 6.537975311279297
    },
    {
      "epoch": 0.5913821138211383,
      "step": 10911,
      "training_loss": 5.84335994720459
    },
    {
      "epoch": 0.5914363143631436,
      "grad_norm": 22.84670639038086,
      "learning_rate": 1e-05,
      "loss": 6.2093,
      "step": 10912
    },
    {
      "epoch": 0.5914363143631436,
      "step": 10912,
      "training_loss": 3.219191551208496
    },
    {
      "epoch": 0.591490514905149,
      "step": 10913,
      "training_loss": 7.6177978515625
    },
    {
      "epoch": 0.5915447154471545,
      "step": 10914,
      "training_loss": 3.22597599029541
    },
    {
      "epoch": 0.5915989159891599,
      "step": 10915,
      "training_loss": 7.665921211242676
    },
    {
      "epoch": 0.5916531165311653,
      "grad_norm": 27.438894271850586,
      "learning_rate": 1e-05,
      "loss": 5.4322,
      "step": 10916
    },
    {
      "epoch": 0.5916531165311653,
      "step": 10916,
      "training_loss": 6.249122142791748
    },
    {
      "epoch": 0.5917073170731707,
      "step": 10917,
      "training_loss": 6.149415969848633
    },
    {
      "epoch": 0.5917615176151761,
      "step": 10918,
      "training_loss": 5.801778316497803
    },
    {
      "epoch": 0.5918157181571816,
      "step": 10919,
      "training_loss": 6.929904937744141
    },
    {
      "epoch": 0.591869918699187,
      "grad_norm": 21.524887084960938,
      "learning_rate": 1e-05,
      "loss": 6.2826,
      "step": 10920
    },
    {
      "epoch": 0.591869918699187,
      "step": 10920,
      "training_loss": 7.962332248687744
    },
    {
      "epoch": 0.5919241192411924,
      "step": 10921,
      "training_loss": 5.983090400695801
    },
    {
      "epoch": 0.5919783197831978,
      "step": 10922,
      "training_loss": 3.48252534866333
    },
    {
      "epoch": 0.5920325203252033,
      "step": 10923,
      "training_loss": 6.151144027709961
    },
    {
      "epoch": 0.5920867208672087,
      "grad_norm": 37.84500503540039,
      "learning_rate": 1e-05,
      "loss": 5.8948,
      "step": 10924
    },
    {
      "epoch": 0.5920867208672087,
      "step": 10924,
      "training_loss": 5.23261022567749
    },
    {
      "epoch": 0.592140921409214,
      "step": 10925,
      "training_loss": 6.688906192779541
    },
    {
      "epoch": 0.5921951219512195,
      "step": 10926,
      "training_loss": 6.244410037994385
    },
    {
      "epoch": 0.5922493224932249,
      "step": 10927,
      "training_loss": 7.374927043914795
    },
    {
      "epoch": 0.5923035230352304,
      "grad_norm": 36.99298858642578,
      "learning_rate": 1e-05,
      "loss": 6.3852,
      "step": 10928
    },
    {
      "epoch": 0.5923035230352304,
      "step": 10928,
      "training_loss": 7.135001182556152
    },
    {
      "epoch": 0.5923577235772358,
      "step": 10929,
      "training_loss": 7.6407647132873535
    },
    {
      "epoch": 0.5924119241192412,
      "step": 10930,
      "training_loss": 6.753104209899902
    },
    {
      "epoch": 0.5924661246612466,
      "step": 10931,
      "training_loss": 6.60596227645874
    },
    {
      "epoch": 0.592520325203252,
      "grad_norm": 28.54059410095215,
      "learning_rate": 1e-05,
      "loss": 7.0337,
      "step": 10932
    },
    {
      "epoch": 0.592520325203252,
      "step": 10932,
      "training_loss": 7.648454666137695
    },
    {
      "epoch": 0.5925745257452575,
      "step": 10933,
      "training_loss": 7.68911075592041
    },
    {
      "epoch": 0.5926287262872628,
      "step": 10934,
      "training_loss": 6.529126167297363
    },
    {
      "epoch": 0.5926829268292683,
      "step": 10935,
      "training_loss": 6.601788520812988
    },
    {
      "epoch": 0.5927371273712737,
      "grad_norm": 28.263595581054688,
      "learning_rate": 1e-05,
      "loss": 7.1171,
      "step": 10936
    },
    {
      "epoch": 0.5927371273712737,
      "step": 10936,
      "training_loss": 6.242354869842529
    },
    {
      "epoch": 0.5927913279132792,
      "step": 10937,
      "training_loss": 6.810308456420898
    },
    {
      "epoch": 0.5928455284552846,
      "step": 10938,
      "training_loss": 7.256829261779785
    },
    {
      "epoch": 0.5928997289972899,
      "step": 10939,
      "training_loss": 5.84932804107666
    },
    {
      "epoch": 0.5929539295392954,
      "grad_norm": 31.298978805541992,
      "learning_rate": 1e-05,
      "loss": 6.5397,
      "step": 10940
    },
    {
      "epoch": 0.5929539295392954,
      "step": 10940,
      "training_loss": 7.0377516746521
    },
    {
      "epoch": 0.5930081300813008,
      "step": 10941,
      "training_loss": 4.510262489318848
    },
    {
      "epoch": 0.5930623306233063,
      "step": 10942,
      "training_loss": 7.654019355773926
    },
    {
      "epoch": 0.5931165311653116,
      "step": 10943,
      "training_loss": 6.205503940582275
    },
    {
      "epoch": 0.593170731707317,
      "grad_norm": 49.37314224243164,
      "learning_rate": 1e-05,
      "loss": 6.3519,
      "step": 10944
    },
    {
      "epoch": 0.593170731707317,
      "step": 10944,
      "training_loss": 3.5934720039367676
    },
    {
      "epoch": 0.5932249322493225,
      "step": 10945,
      "training_loss": 6.514315128326416
    },
    {
      "epoch": 0.5932791327913279,
      "step": 10946,
      "training_loss": 5.6994428634643555
    },
    {
      "epoch": 0.5933333333333334,
      "step": 10947,
      "training_loss": 7.260229587554932
    },
    {
      "epoch": 0.5933875338753387,
      "grad_norm": 17.406232833862305,
      "learning_rate": 1e-05,
      "loss": 5.7669,
      "step": 10948
    },
    {
      "epoch": 0.5933875338753387,
      "step": 10948,
      "training_loss": 6.4792633056640625
    },
    {
      "epoch": 0.5934417344173442,
      "step": 10949,
      "training_loss": 7.935939311981201
    },
    {
      "epoch": 0.5934959349593496,
      "step": 10950,
      "training_loss": 5.978304386138916
    },
    {
      "epoch": 0.593550135501355,
      "step": 10951,
      "training_loss": 7.221642971038818
    },
    {
      "epoch": 0.5936043360433604,
      "grad_norm": 18.652280807495117,
      "learning_rate": 1e-05,
      "loss": 6.9038,
      "step": 10952
    },
    {
      "epoch": 0.5936043360433604,
      "step": 10952,
      "training_loss": 7.20927619934082
    },
    {
      "epoch": 0.5936585365853658,
      "step": 10953,
      "training_loss": 6.49690055847168
    },
    {
      "epoch": 0.5937127371273713,
      "step": 10954,
      "training_loss": 6.986031532287598
    },
    {
      "epoch": 0.5937669376693767,
      "step": 10955,
      "training_loss": 7.815383434295654
    },
    {
      "epoch": 0.5938211382113822,
      "grad_norm": 23.462730407714844,
      "learning_rate": 1e-05,
      "loss": 7.1269,
      "step": 10956
    },
    {
      "epoch": 0.5938211382113822,
      "step": 10956,
      "training_loss": 6.597806453704834
    },
    {
      "epoch": 0.5938753387533875,
      "step": 10957,
      "training_loss": 7.182574272155762
    },
    {
      "epoch": 0.5939295392953929,
      "step": 10958,
      "training_loss": 6.640136241912842
    },
    {
      "epoch": 0.5939837398373984,
      "step": 10959,
      "training_loss": 7.992475986480713
    },
    {
      "epoch": 0.5940379403794038,
      "grad_norm": 40.409481048583984,
      "learning_rate": 1e-05,
      "loss": 7.1032,
      "step": 10960
    },
    {
      "epoch": 0.5940379403794038,
      "step": 10960,
      "training_loss": 6.069347858428955
    },
    {
      "epoch": 0.5940921409214092,
      "step": 10961,
      "training_loss": 7.157155990600586
    },
    {
      "epoch": 0.5941463414634146,
      "step": 10962,
      "training_loss": 6.566418647766113
    },
    {
      "epoch": 0.59420054200542,
      "step": 10963,
      "training_loss": 6.959513187408447
    },
    {
      "epoch": 0.5942547425474255,
      "grad_norm": 33.699974060058594,
      "learning_rate": 1e-05,
      "loss": 6.6881,
      "step": 10964
    },
    {
      "epoch": 0.5942547425474255,
      "step": 10964,
      "training_loss": 7.070240497589111
    },
    {
      "epoch": 0.5943089430894309,
      "step": 10965,
      "training_loss": 7.371450424194336
    },
    {
      "epoch": 0.5943631436314363,
      "step": 10966,
      "training_loss": 6.8828277587890625
    },
    {
      "epoch": 0.5944173441734417,
      "step": 10967,
      "training_loss": 7.327083110809326
    },
    {
      "epoch": 0.5944715447154472,
      "grad_norm": 47.91764450073242,
      "learning_rate": 1e-05,
      "loss": 7.1629,
      "step": 10968
    },
    {
      "epoch": 0.5944715447154472,
      "step": 10968,
      "training_loss": 7.086299419403076
    },
    {
      "epoch": 0.5945257452574526,
      "step": 10969,
      "training_loss": 7.2729878425598145
    },
    {
      "epoch": 0.5945799457994579,
      "step": 10970,
      "training_loss": 7.045570373535156
    },
    {
      "epoch": 0.5946341463414634,
      "step": 10971,
      "training_loss": 5.8894429206848145
    },
    {
      "epoch": 0.5946883468834688,
      "grad_norm": 47.57496643066406,
      "learning_rate": 1e-05,
      "loss": 6.8236,
      "step": 10972
    },
    {
      "epoch": 0.5946883468834688,
      "step": 10972,
      "training_loss": 8.676595687866211
    },
    {
      "epoch": 0.5947425474254743,
      "step": 10973,
      "training_loss": 6.637314796447754
    },
    {
      "epoch": 0.5947967479674797,
      "step": 10974,
      "training_loss": 6.684154510498047
    },
    {
      "epoch": 0.5948509485094851,
      "step": 10975,
      "training_loss": 7.113494396209717
    },
    {
      "epoch": 0.5949051490514905,
      "grad_norm": 34.73836898803711,
      "learning_rate": 1e-05,
      "loss": 7.2779,
      "step": 10976
    },
    {
      "epoch": 0.5949051490514905,
      "step": 10976,
      "training_loss": 6.757529258728027
    },
    {
      "epoch": 0.594959349593496,
      "step": 10977,
      "training_loss": 5.902159214019775
    },
    {
      "epoch": 0.5950135501355014,
      "step": 10978,
      "training_loss": 7.659835338592529
    },
    {
      "epoch": 0.5950677506775067,
      "step": 10979,
      "training_loss": 6.680417537689209
    },
    {
      "epoch": 0.5951219512195122,
      "grad_norm": 18.70823097229004,
      "learning_rate": 1e-05,
      "loss": 6.75,
      "step": 10980
    },
    {
      "epoch": 0.5951219512195122,
      "step": 10980,
      "training_loss": 6.5368971824646
    },
    {
      "epoch": 0.5951761517615176,
      "step": 10981,
      "training_loss": 5.995997428894043
    },
    {
      "epoch": 0.5952303523035231,
      "step": 10982,
      "training_loss": 5.486804962158203
    },
    {
      "epoch": 0.5952845528455285,
      "step": 10983,
      "training_loss": 7.321106433868408
    },
    {
      "epoch": 0.5953387533875338,
      "grad_norm": 17.287126541137695,
      "learning_rate": 1e-05,
      "loss": 6.3352,
      "step": 10984
    },
    {
      "epoch": 0.5953387533875338,
      "step": 10984,
      "training_loss": 7.109471797943115
    },
    {
      "epoch": 0.5953929539295393,
      "step": 10985,
      "training_loss": 7.795657634735107
    },
    {
      "epoch": 0.5954471544715447,
      "step": 10986,
      "training_loss": 6.176992416381836
    },
    {
      "epoch": 0.5955013550135502,
      "step": 10987,
      "training_loss": 7.018503189086914
    },
    {
      "epoch": 0.5955555555555555,
      "grad_norm": 21.34609603881836,
      "learning_rate": 1e-05,
      "loss": 7.0252,
      "step": 10988
    },
    {
      "epoch": 0.5955555555555555,
      "step": 10988,
      "training_loss": 7.313769340515137
    },
    {
      "epoch": 0.595609756097561,
      "step": 10989,
      "training_loss": 5.759042739868164
    },
    {
      "epoch": 0.5956639566395664,
      "step": 10990,
      "training_loss": 7.361340045928955
    },
    {
      "epoch": 0.5957181571815718,
      "step": 10991,
      "training_loss": 7.464496612548828
    },
    {
      "epoch": 0.5957723577235773,
      "grad_norm": 21.616439819335938,
      "learning_rate": 1e-05,
      "loss": 6.9747,
      "step": 10992
    },
    {
      "epoch": 0.5957723577235773,
      "step": 10992,
      "training_loss": 7.053995132446289
    },
    {
      "epoch": 0.5958265582655826,
      "step": 10993,
      "training_loss": 6.485004425048828
    },
    {
      "epoch": 0.5958807588075881,
      "step": 10994,
      "training_loss": 6.759740352630615
    },
    {
      "epoch": 0.5959349593495935,
      "step": 10995,
      "training_loss": 6.217282772064209
    },
    {
      "epoch": 0.595989159891599,
      "grad_norm": 31.431102752685547,
      "learning_rate": 1e-05,
      "loss": 6.629,
      "step": 10996
    },
    {
      "epoch": 0.595989159891599,
      "step": 10996,
      "training_loss": 7.2114763259887695
    },
    {
      "epoch": 0.5960433604336043,
      "step": 10997,
      "training_loss": 7.711504936218262
    },
    {
      "epoch": 0.5960975609756097,
      "step": 10998,
      "training_loss": 7.071123123168945
    },
    {
      "epoch": 0.5961517615176152,
      "step": 10999,
      "training_loss": 7.637585639953613
    },
    {
      "epoch": 0.5962059620596206,
      "grad_norm": 35.33336639404297,
      "learning_rate": 1e-05,
      "loss": 7.4079,
      "step": 11000
    },
    {
      "epoch": 0.5962059620596206,
      "step": 11000,
      "training_loss": 6.624011993408203
    },
    {
      "epoch": 0.5962601626016261,
      "step": 11001,
      "training_loss": 7.715563774108887
    },
    {
      "epoch": 0.5963143631436314,
      "step": 11002,
      "training_loss": 6.0952229499816895
    },
    {
      "epoch": 0.5963685636856368,
      "step": 11003,
      "training_loss": 6.628664970397949
    },
    {
      "epoch": 0.5964227642276423,
      "grad_norm": 20.86016845703125,
      "learning_rate": 1e-05,
      "loss": 6.7659,
      "step": 11004
    },
    {
      "epoch": 0.5964227642276423,
      "step": 11004,
      "training_loss": 4.4166131019592285
    },
    {
      "epoch": 0.5964769647696477,
      "step": 11005,
      "training_loss": 6.440060138702393
    },
    {
      "epoch": 0.5965311653116531,
      "step": 11006,
      "training_loss": 6.8351969718933105
    },
    {
      "epoch": 0.5965853658536585,
      "step": 11007,
      "training_loss": 7.09214448928833
    },
    {
      "epoch": 0.596639566395664,
      "grad_norm": 28.267255783081055,
      "learning_rate": 1e-05,
      "loss": 6.196,
      "step": 11008
    },
    {
      "epoch": 0.596639566395664,
      "step": 11008,
      "training_loss": 5.976308345794678
    },
    {
      "epoch": 0.5966937669376694,
      "step": 11009,
      "training_loss": 5.982761859893799
    },
    {
      "epoch": 0.5967479674796748,
      "step": 11010,
      "training_loss": 6.092014312744141
    },
    {
      "epoch": 0.5968021680216802,
      "step": 11011,
      "training_loss": 6.877490043640137
    },
    {
      "epoch": 0.5968563685636856,
      "grad_norm": 18.726083755493164,
      "learning_rate": 1e-05,
      "loss": 6.2321,
      "step": 11012
    },
    {
      "epoch": 0.5968563685636856,
      "step": 11012,
      "training_loss": 6.883813381195068
    },
    {
      "epoch": 0.5969105691056911,
      "step": 11013,
      "training_loss": 4.491900444030762
    },
    {
      "epoch": 0.5969647696476965,
      "step": 11014,
      "training_loss": 7.4127936363220215
    },
    {
      "epoch": 0.5970189701897018,
      "step": 11015,
      "training_loss": 5.638383865356445
    },
    {
      "epoch": 0.5970731707317073,
      "grad_norm": 37.2763557434082,
      "learning_rate": 1e-05,
      "loss": 6.1067,
      "step": 11016
    },
    {
      "epoch": 0.5970731707317073,
      "step": 11016,
      "training_loss": 7.073550701141357
    },
    {
      "epoch": 0.5971273712737127,
      "step": 11017,
      "training_loss": 5.962313652038574
    },
    {
      "epoch": 0.5971815718157182,
      "step": 11018,
      "training_loss": 8.091687202453613
    },
    {
      "epoch": 0.5972357723577236,
      "step": 11019,
      "training_loss": 6.509961128234863
    },
    {
      "epoch": 0.597289972899729,
      "grad_norm": 16.785369873046875,
      "learning_rate": 1e-05,
      "loss": 6.9094,
      "step": 11020
    },
    {
      "epoch": 0.597289972899729,
      "step": 11020,
      "training_loss": 7.65781831741333
    },
    {
      "epoch": 0.5973441734417344,
      "step": 11021,
      "training_loss": 6.519495487213135
    },
    {
      "epoch": 0.5973983739837398,
      "step": 11022,
      "training_loss": 4.5997490882873535
    },
    {
      "epoch": 0.5974525745257453,
      "step": 11023,
      "training_loss": 8.772398948669434
    },
    {
      "epoch": 0.5975067750677506,
      "grad_norm": 43.56674575805664,
      "learning_rate": 1e-05,
      "loss": 6.8874,
      "step": 11024
    },
    {
      "epoch": 0.5975067750677506,
      "step": 11024,
      "training_loss": 6.5361223220825195
    },
    {
      "epoch": 0.5975609756097561,
      "step": 11025,
      "training_loss": 7.049206733703613
    },
    {
      "epoch": 0.5976151761517615,
      "step": 11026,
      "training_loss": 4.235907554626465
    },
    {
      "epoch": 0.597669376693767,
      "step": 11027,
      "training_loss": 5.437300682067871
    },
    {
      "epoch": 0.5977235772357724,
      "grad_norm": 28.03268814086914,
      "learning_rate": 1e-05,
      "loss": 5.8146,
      "step": 11028
    },
    {
      "epoch": 0.5977235772357724,
      "step": 11028,
      "training_loss": 6.493498802185059
    },
    {
      "epoch": 0.5977777777777777,
      "step": 11029,
      "training_loss": 6.8048319816589355
    },
    {
      "epoch": 0.5978319783197832,
      "step": 11030,
      "training_loss": 6.525485992431641
    },
    {
      "epoch": 0.5978861788617886,
      "step": 11031,
      "training_loss": 6.761744022369385
    },
    {
      "epoch": 0.5979403794037941,
      "grad_norm": 28.689022064208984,
      "learning_rate": 1e-05,
      "loss": 6.6464,
      "step": 11032
    },
    {
      "epoch": 0.5979403794037941,
      "step": 11032,
      "training_loss": 7.260319709777832
    },
    {
      "epoch": 0.5979945799457994,
      "step": 11033,
      "training_loss": 6.692844867706299
    },
    {
      "epoch": 0.5980487804878049,
      "step": 11034,
      "training_loss": 6.4098358154296875
    },
    {
      "epoch": 0.5981029810298103,
      "step": 11035,
      "training_loss": 7.184148788452148
    },
    {
      "epoch": 0.5981571815718157,
      "grad_norm": 34.189781188964844,
      "learning_rate": 1e-05,
      "loss": 6.8868,
      "step": 11036
    },
    {
      "epoch": 0.5981571815718157,
      "step": 11036,
      "training_loss": 7.063739776611328
    },
    {
      "epoch": 0.5982113821138212,
      "step": 11037,
      "training_loss": 6.28643798828125
    },
    {
      "epoch": 0.5982655826558265,
      "step": 11038,
      "training_loss": 4.544436454772949
    },
    {
      "epoch": 0.598319783197832,
      "step": 11039,
      "training_loss": 7.697327136993408
    },
    {
      "epoch": 0.5983739837398374,
      "grad_norm": 25.265592575073242,
      "learning_rate": 1e-05,
      "loss": 6.398,
      "step": 11040
    },
    {
      "epoch": 0.5983739837398374,
      "step": 11040,
      "training_loss": 6.535763263702393
    },
    {
      "epoch": 0.5984281842818429,
      "step": 11041,
      "training_loss": 7.774016380310059
    },
    {
      "epoch": 0.5984823848238482,
      "step": 11042,
      "training_loss": 6.844910144805908
    },
    {
      "epoch": 0.5985365853658536,
      "step": 11043,
      "training_loss": 7.001053333282471
    },
    {
      "epoch": 0.5985907859078591,
      "grad_norm": 39.059715270996094,
      "learning_rate": 1e-05,
      "loss": 7.0389,
      "step": 11044
    },
    {
      "epoch": 0.5985907859078591,
      "step": 11044,
      "training_loss": 6.449651718139648
    },
    {
      "epoch": 0.5986449864498645,
      "step": 11045,
      "training_loss": 6.382030963897705
    },
    {
      "epoch": 0.59869918699187,
      "step": 11046,
      "training_loss": 6.947986125946045
    },
    {
      "epoch": 0.5987533875338753,
      "step": 11047,
      "training_loss": 7.240787506103516
    },
    {
      "epoch": 0.5988075880758807,
      "grad_norm": 24.085723876953125,
      "learning_rate": 1e-05,
      "loss": 6.7551,
      "step": 11048
    },
    {
      "epoch": 0.5988075880758807,
      "step": 11048,
      "training_loss": 5.980581760406494
    },
    {
      "epoch": 0.5988617886178862,
      "step": 11049,
      "training_loss": 5.424380779266357
    },
    {
      "epoch": 0.5989159891598916,
      "step": 11050,
      "training_loss": 6.069179534912109
    },
    {
      "epoch": 0.598970189701897,
      "step": 11051,
      "training_loss": 6.845284461975098
    },
    {
      "epoch": 0.5990243902439024,
      "grad_norm": 26.171842575073242,
      "learning_rate": 1e-05,
      "loss": 6.0799,
      "step": 11052
    },
    {
      "epoch": 0.5990243902439024,
      "step": 11052,
      "training_loss": 6.222631931304932
    },
    {
      "epoch": 0.5990785907859079,
      "step": 11053,
      "training_loss": 7.399404525756836
    },
    {
      "epoch": 0.5991327913279133,
      "step": 11054,
      "training_loss": 7.3995466232299805
    },
    {
      "epoch": 0.5991869918699188,
      "step": 11055,
      "training_loss": 5.721418380737305
    },
    {
      "epoch": 0.5992411924119241,
      "grad_norm": 31.462398529052734,
      "learning_rate": 1e-05,
      "loss": 6.6858,
      "step": 11056
    },
    {
      "epoch": 0.5992411924119241,
      "step": 11056,
      "training_loss": 7.463067054748535
    },
    {
      "epoch": 0.5992953929539295,
      "step": 11057,
      "training_loss": 6.362881183624268
    },
    {
      "epoch": 0.599349593495935,
      "step": 11058,
      "training_loss": 7.3611555099487305
    },
    {
      "epoch": 0.5994037940379404,
      "step": 11059,
      "training_loss": 4.4443182945251465
    },
    {
      "epoch": 0.5994579945799458,
      "grad_norm": 24.091516494750977,
      "learning_rate": 1e-05,
      "loss": 6.4079,
      "step": 11060
    },
    {
      "epoch": 0.5994579945799458,
      "step": 11060,
      "training_loss": 6.266279220581055
    },
    {
      "epoch": 0.5995121951219512,
      "step": 11061,
      "training_loss": 6.749885082244873
    },
    {
      "epoch": 0.5995663956639566,
      "step": 11062,
      "training_loss": 7.556498050689697
    },
    {
      "epoch": 0.5996205962059621,
      "step": 11063,
      "training_loss": 7.574294567108154
    },
    {
      "epoch": 0.5996747967479675,
      "grad_norm": 27.47272491455078,
      "learning_rate": 1e-05,
      "loss": 7.0367,
      "step": 11064
    },
    {
      "epoch": 0.5996747967479675,
      "step": 11064,
      "training_loss": 5.9072265625
    },
    {
      "epoch": 0.5997289972899729,
      "step": 11065,
      "training_loss": 3.950883626937866
    },
    {
      "epoch": 0.5997831978319783,
      "step": 11066,
      "training_loss": 6.7181596755981445
    },
    {
      "epoch": 0.5998373983739838,
      "step": 11067,
      "training_loss": 6.299131393432617
    },
    {
      "epoch": 0.5998915989159892,
      "grad_norm": 32.96095657348633,
      "learning_rate": 1e-05,
      "loss": 5.7189,
      "step": 11068
    },
    {
      "epoch": 0.5998915989159892,
      "step": 11068,
      "training_loss": 6.8476243019104
    },
    {
      "epoch": 0.5999457994579945,
      "step": 11069,
      "training_loss": 7.038668155670166
    },
    {
      "epoch": 0.6,
      "step": 11070,
      "training_loss": 5.633530616760254
    },
    {
      "epoch": 0.6000542005420054,
      "step": 11071,
      "training_loss": 6.157073497772217
    },
    {
      "epoch": 0.6001084010840109,
      "grad_norm": 29.113750457763672,
      "learning_rate": 1e-05,
      "loss": 6.4192,
      "step": 11072
    },
    {
      "epoch": 0.6001084010840109,
      "step": 11072,
      "training_loss": 3.9853029251098633
    },
    {
      "epoch": 0.6001626016260163,
      "step": 11073,
      "training_loss": 7.395498752593994
    },
    {
      "epoch": 0.6002168021680216,
      "step": 11074,
      "training_loss": 5.8743510246276855
    },
    {
      "epoch": 0.6002710027100271,
      "step": 11075,
      "training_loss": 5.958527565002441
    },
    {
      "epoch": 0.6003252032520325,
      "grad_norm": 22.25904083251953,
      "learning_rate": 1e-05,
      "loss": 5.8034,
      "step": 11076
    },
    {
      "epoch": 0.6003252032520325,
      "step": 11076,
      "training_loss": 6.335699081420898
    },
    {
      "epoch": 0.600379403794038,
      "step": 11077,
      "training_loss": 5.265078544616699
    },
    {
      "epoch": 0.6004336043360433,
      "step": 11078,
      "training_loss": 7.262122631072998
    },
    {
      "epoch": 0.6004878048780488,
      "step": 11079,
      "training_loss": 5.960559368133545
    },
    {
      "epoch": 0.6005420054200542,
      "grad_norm": 21.0752010345459,
      "learning_rate": 1e-05,
      "loss": 6.2059,
      "step": 11080
    },
    {
      "epoch": 0.6005420054200542,
      "step": 11080,
      "training_loss": 7.588852405548096
    },
    {
      "epoch": 0.6005962059620596,
      "step": 11081,
      "training_loss": 6.321976184844971
    },
    {
      "epoch": 0.6006504065040651,
      "step": 11082,
      "training_loss": 5.871057033538818
    },
    {
      "epoch": 0.6007046070460704,
      "step": 11083,
      "training_loss": 7.118670463562012
    },
    {
      "epoch": 0.6007588075880759,
      "grad_norm": 35.98613357543945,
      "learning_rate": 1e-05,
      "loss": 6.7251,
      "step": 11084
    },
    {
      "epoch": 0.6007588075880759,
      "step": 11084,
      "training_loss": 7.7578277587890625
    },
    {
      "epoch": 0.6008130081300813,
      "step": 11085,
      "training_loss": 6.739837169647217
    },
    {
      "epoch": 0.6008672086720868,
      "step": 11086,
      "training_loss": 3.545435667037964
    },
    {
      "epoch": 0.6009214092140921,
      "step": 11087,
      "training_loss": 7.071723937988281
    },
    {
      "epoch": 0.6009756097560975,
      "grad_norm": 26.478544235229492,
      "learning_rate": 1e-05,
      "loss": 6.2787,
      "step": 11088
    },
    {
      "epoch": 0.6009756097560975,
      "step": 11088,
      "training_loss": 5.812066078186035
    },
    {
      "epoch": 0.601029810298103,
      "step": 11089,
      "training_loss": 5.181218147277832
    },
    {
      "epoch": 0.6010840108401084,
      "step": 11090,
      "training_loss": 5.86263370513916
    },
    {
      "epoch": 0.6011382113821139,
      "step": 11091,
      "training_loss": 6.072546005249023
    },
    {
      "epoch": 0.6011924119241192,
      "grad_norm": 36.03868865966797,
      "learning_rate": 1e-05,
      "loss": 5.7321,
      "step": 11092
    },
    {
      "epoch": 0.6011924119241192,
      "step": 11092,
      "training_loss": 7.373278617858887
    },
    {
      "epoch": 0.6012466124661247,
      "step": 11093,
      "training_loss": 6.928121566772461
    },
    {
      "epoch": 0.6013008130081301,
      "step": 11094,
      "training_loss": 6.857038497924805
    },
    {
      "epoch": 0.6013550135501355,
      "step": 11095,
      "training_loss": 7.491692066192627
    },
    {
      "epoch": 0.6014092140921409,
      "grad_norm": 32.940216064453125,
      "learning_rate": 1e-05,
      "loss": 7.1625,
      "step": 11096
    },
    {
      "epoch": 0.6014092140921409,
      "step": 11096,
      "training_loss": 6.818441867828369
    },
    {
      "epoch": 0.6014634146341463,
      "step": 11097,
      "training_loss": 5.059744834899902
    },
    {
      "epoch": 0.6015176151761518,
      "step": 11098,
      "training_loss": 7.71392822265625
    },
    {
      "epoch": 0.6015718157181572,
      "step": 11099,
      "training_loss": 7.2998456954956055
    },
    {
      "epoch": 0.6016260162601627,
      "grad_norm": 28.199628829956055,
      "learning_rate": 1e-05,
      "loss": 6.723,
      "step": 11100
    },
    {
      "epoch": 0.6016260162601627,
      "step": 11100,
      "training_loss": 7.26662015914917
    },
    {
      "epoch": 0.601680216802168,
      "step": 11101,
      "training_loss": 5.4526190757751465
    },
    {
      "epoch": 0.6017344173441734,
      "step": 11102,
      "training_loss": 6.835565567016602
    },
    {
      "epoch": 0.6017886178861789,
      "step": 11103,
      "training_loss": 4.026360988616943
    },
    {
      "epoch": 0.6018428184281843,
      "grad_norm": 25.205020904541016,
      "learning_rate": 1e-05,
      "loss": 5.8953,
      "step": 11104
    },
    {
      "epoch": 0.6018428184281843,
      "step": 11104,
      "training_loss": 3.431600570678711
    },
    {
      "epoch": 0.6018970189701897,
      "step": 11105,
      "training_loss": 6.6712799072265625
    },
    {
      "epoch": 0.6019512195121951,
      "step": 11106,
      "training_loss": 7.544037818908691
    },
    {
      "epoch": 0.6020054200542005,
      "step": 11107,
      "training_loss": 7.106781959533691
    },
    {
      "epoch": 0.602059620596206,
      "grad_norm": 38.598323822021484,
      "learning_rate": 1e-05,
      "loss": 6.1884,
      "step": 11108
    },
    {
      "epoch": 0.602059620596206,
      "step": 11108,
      "training_loss": 7.289297580718994
    },
    {
      "epoch": 0.6021138211382114,
      "step": 11109,
      "training_loss": 6.159316062927246
    },
    {
      "epoch": 0.6021680216802168,
      "step": 11110,
      "training_loss": 8.767621040344238
    },
    {
      "epoch": 0.6022222222222222,
      "step": 11111,
      "training_loss": 8.07129955291748
    },
    {
      "epoch": 0.6022764227642277,
      "grad_norm": 19.872697830200195,
      "learning_rate": 1e-05,
      "loss": 7.5719,
      "step": 11112
    },
    {
      "epoch": 0.6022764227642277,
      "step": 11112,
      "training_loss": 7.1027512550354
    },
    {
      "epoch": 0.6023306233062331,
      "step": 11113,
      "training_loss": 6.727883338928223
    },
    {
      "epoch": 0.6023848238482384,
      "step": 11114,
      "training_loss": 3.0002238750457764
    },
    {
      "epoch": 0.6024390243902439,
      "step": 11115,
      "training_loss": 5.1880784034729
    },
    {
      "epoch": 0.6024932249322493,
      "grad_norm": 22.234895706176758,
      "learning_rate": 1e-05,
      "loss": 5.5047,
      "step": 11116
    },
    {
      "epoch": 0.6024932249322493,
      "step": 11116,
      "training_loss": 5.815372467041016
    },
    {
      "epoch": 0.6025474254742548,
      "step": 11117,
      "training_loss": 7.154808521270752
    },
    {
      "epoch": 0.6026016260162602,
      "step": 11118,
      "training_loss": 7.020277976989746
    },
    {
      "epoch": 0.6026558265582656,
      "step": 11119,
      "training_loss": 6.808940410614014
    },
    {
      "epoch": 0.602710027100271,
      "grad_norm": 23.4995174407959,
      "learning_rate": 1e-05,
      "loss": 6.6999,
      "step": 11120
    },
    {
      "epoch": 0.602710027100271,
      "step": 11120,
      "training_loss": 6.451155185699463
    },
    {
      "epoch": 0.6027642276422764,
      "step": 11121,
      "training_loss": 7.615228176116943
    },
    {
      "epoch": 0.6028184281842819,
      "step": 11122,
      "training_loss": 7.08967399597168
    },
    {
      "epoch": 0.6028726287262872,
      "step": 11123,
      "training_loss": 5.846789360046387
    },
    {
      "epoch": 0.6029268292682927,
      "grad_norm": 27.270578384399414,
      "learning_rate": 1e-05,
      "loss": 6.7507,
      "step": 11124
    },
    {
      "epoch": 0.6029268292682927,
      "step": 11124,
      "training_loss": 7.180241107940674
    },
    {
      "epoch": 0.6029810298102981,
      "step": 11125,
      "training_loss": 7.126986026763916
    },
    {
      "epoch": 0.6030352303523036,
      "step": 11126,
      "training_loss": 6.95031213760376
    },
    {
      "epoch": 0.603089430894309,
      "step": 11127,
      "training_loss": 6.971716403961182
    },
    {
      "epoch": 0.6031436314363143,
      "grad_norm": 32.0150146484375,
      "learning_rate": 1e-05,
      "loss": 7.0573,
      "step": 11128
    },
    {
      "epoch": 0.6031436314363143,
      "step": 11128,
      "training_loss": 6.687315464019775
    },
    {
      "epoch": 0.6031978319783198,
      "step": 11129,
      "training_loss": 7.483044624328613
    },
    {
      "epoch": 0.6032520325203252,
      "step": 11130,
      "training_loss": 5.853895664215088
    },
    {
      "epoch": 0.6033062330623307,
      "step": 11131,
      "training_loss": 7.476398944854736
    },
    {
      "epoch": 0.603360433604336,
      "grad_norm": 31.813541412353516,
      "learning_rate": 1e-05,
      "loss": 6.8752,
      "step": 11132
    },
    {
      "epoch": 0.603360433604336,
      "step": 11132,
      "training_loss": 7.440887928009033
    },
    {
      "epoch": 0.6034146341463414,
      "step": 11133,
      "training_loss": 6.7304816246032715
    },
    {
      "epoch": 0.6034688346883469,
      "step": 11134,
      "training_loss": 5.310020446777344
    },
    {
      "epoch": 0.6035230352303523,
      "step": 11135,
      "training_loss": 7.876435279846191
    },
    {
      "epoch": 0.6035772357723578,
      "grad_norm": 32.63596725463867,
      "learning_rate": 1e-05,
      "loss": 6.8395,
      "step": 11136
    },
    {
      "epoch": 0.6035772357723578,
      "step": 11136,
      "training_loss": 5.4183030128479
    },
    {
      "epoch": 0.6036314363143631,
      "step": 11137,
      "training_loss": 4.978416442871094
    },
    {
      "epoch": 0.6036856368563686,
      "step": 11138,
      "training_loss": 5.870357036590576
    },
    {
      "epoch": 0.603739837398374,
      "step": 11139,
      "training_loss": 7.300804138183594
    },
    {
      "epoch": 0.6037940379403794,
      "grad_norm": 32.62084197998047,
      "learning_rate": 1e-05,
      "loss": 5.892,
      "step": 11140
    },
    {
      "epoch": 0.6037940379403794,
      "step": 11140,
      "training_loss": 7.719554901123047
    },
    {
      "epoch": 0.6038482384823848,
      "step": 11141,
      "training_loss": 5.946413040161133
    },
    {
      "epoch": 0.6039024390243902,
      "step": 11142,
      "training_loss": 6.110853672027588
    },
    {
      "epoch": 0.6039566395663957,
      "step": 11143,
      "training_loss": 6.8098249435424805
    },
    {
      "epoch": 0.6040108401084011,
      "grad_norm": 41.03069305419922,
      "learning_rate": 1e-05,
      "loss": 6.6467,
      "step": 11144
    },
    {
      "epoch": 0.6040108401084011,
      "step": 11144,
      "training_loss": 4.484838962554932
    },
    {
      "epoch": 0.6040650406504066,
      "step": 11145,
      "training_loss": 7.318984031677246
    },
    {
      "epoch": 0.6041192411924119,
      "step": 11146,
      "training_loss": 7.205708980560303
    },
    {
      "epoch": 0.6041734417344173,
      "step": 11147,
      "training_loss": 7.346485614776611
    },
    {
      "epoch": 0.6042276422764228,
      "grad_norm": 17.971315383911133,
      "learning_rate": 1e-05,
      "loss": 6.589,
      "step": 11148
    },
    {
      "epoch": 0.6042276422764228,
      "step": 11148,
      "training_loss": 6.546396732330322
    },
    {
      "epoch": 0.6042818428184282,
      "step": 11149,
      "training_loss": 5.486326217651367
    },
    {
      "epoch": 0.6043360433604336,
      "step": 11150,
      "training_loss": 6.443658828735352
    },
    {
      "epoch": 0.604390243902439,
      "step": 11151,
      "training_loss": 6.884119987487793
    },
    {
      "epoch": 0.6044444444444445,
      "grad_norm": 21.91139793395996,
      "learning_rate": 1e-05,
      "loss": 6.3401,
      "step": 11152
    },
    {
      "epoch": 0.6044444444444445,
      "step": 11152,
      "training_loss": 6.504848003387451
    },
    {
      "epoch": 0.6044986449864499,
      "step": 11153,
      "training_loss": 6.050110340118408
    },
    {
      "epoch": 0.6045528455284552,
      "step": 11154,
      "training_loss": 7.203690528869629
    },
    {
      "epoch": 0.6046070460704607,
      "step": 11155,
      "training_loss": 6.312140941619873
    },
    {
      "epoch": 0.6046612466124661,
      "grad_norm": 37.18186569213867,
      "learning_rate": 1e-05,
      "loss": 6.5177,
      "step": 11156
    },
    {
      "epoch": 0.6046612466124661,
      "step": 11156,
      "training_loss": 7.906692028045654
    },
    {
      "epoch": 0.6047154471544716,
      "step": 11157,
      "training_loss": 5.654050827026367
    },
    {
      "epoch": 0.604769647696477,
      "step": 11158,
      "training_loss": 5.984521865844727
    },
    {
      "epoch": 0.6048238482384823,
      "step": 11159,
      "training_loss": 4.27946662902832
    },
    {
      "epoch": 0.6048780487804878,
      "grad_norm": 33.48423767089844,
      "learning_rate": 1e-05,
      "loss": 5.9562,
      "step": 11160
    },
    {
      "epoch": 0.6048780487804878,
      "step": 11160,
      "training_loss": 7.672090530395508
    },
    {
      "epoch": 0.6049322493224932,
      "step": 11161,
      "training_loss": 4.441939830780029
    },
    {
      "epoch": 0.6049864498644987,
      "step": 11162,
      "training_loss": 6.9736738204956055
    },
    {
      "epoch": 0.605040650406504,
      "step": 11163,
      "training_loss": 6.742969989776611
    },
    {
      "epoch": 0.6050948509485095,
      "grad_norm": 28.160686492919922,
      "learning_rate": 1e-05,
      "loss": 6.4577,
      "step": 11164
    },
    {
      "epoch": 0.6050948509485095,
      "step": 11164,
      "training_loss": 6.33933162689209
    },
    {
      "epoch": 0.6051490514905149,
      "step": 11165,
      "training_loss": 7.207388877868652
    },
    {
      "epoch": 0.6052032520325203,
      "step": 11166,
      "training_loss": 5.688606262207031
    },
    {
      "epoch": 0.6052574525745258,
      "step": 11167,
      "training_loss": 6.219055652618408
    },
    {
      "epoch": 0.6053116531165311,
      "grad_norm": 34.580291748046875,
      "learning_rate": 1e-05,
      "loss": 6.3636,
      "step": 11168
    },
    {
      "epoch": 0.6053116531165311,
      "step": 11168,
      "training_loss": 7.385870933532715
    },
    {
      "epoch": 0.6053658536585366,
      "step": 11169,
      "training_loss": 5.501719951629639
    },
    {
      "epoch": 0.605420054200542,
      "step": 11170,
      "training_loss": 5.8818769454956055
    },
    {
      "epoch": 0.6054742547425475,
      "step": 11171,
      "training_loss": 6.710631370544434
    },
    {
      "epoch": 0.6055284552845528,
      "grad_norm": 24.408185958862305,
      "learning_rate": 1e-05,
      "loss": 6.37,
      "step": 11172
    },
    {
      "epoch": 0.6055284552845528,
      "step": 11172,
      "training_loss": 5.818498611450195
    },
    {
      "epoch": 0.6055826558265582,
      "step": 11173,
      "training_loss": 7.234785079956055
    },
    {
      "epoch": 0.6056368563685637,
      "step": 11174,
      "training_loss": 5.418095588684082
    },
    {
      "epoch": 0.6056910569105691,
      "step": 11175,
      "training_loss": 6.615765571594238
    },
    {
      "epoch": 0.6057452574525746,
      "grad_norm": 53.746063232421875,
      "learning_rate": 1e-05,
      "loss": 6.2718,
      "step": 11176
    },
    {
      "epoch": 0.6057452574525746,
      "step": 11176,
      "training_loss": 5.2076029777526855
    },
    {
      "epoch": 0.6057994579945799,
      "step": 11177,
      "training_loss": 6.290706634521484
    },
    {
      "epoch": 0.6058536585365853,
      "step": 11178,
      "training_loss": 5.801238536834717
    },
    {
      "epoch": 0.6059078590785908,
      "step": 11179,
      "training_loss": 5.6558990478515625
    },
    {
      "epoch": 0.6059620596205962,
      "grad_norm": 28.075519561767578,
      "learning_rate": 1e-05,
      "loss": 5.7389,
      "step": 11180
    },
    {
      "epoch": 0.6059620596205962,
      "step": 11180,
      "training_loss": 6.400613784790039
    },
    {
      "epoch": 0.6060162601626016,
      "step": 11181,
      "training_loss": 5.955056190490723
    },
    {
      "epoch": 0.606070460704607,
      "step": 11182,
      "training_loss": 6.921067237854004
    },
    {
      "epoch": 0.6061246612466125,
      "step": 11183,
      "training_loss": 3.492403984069824
    },
    {
      "epoch": 0.6061788617886179,
      "grad_norm": 32.03846740722656,
      "learning_rate": 1e-05,
      "loss": 5.6923,
      "step": 11184
    },
    {
      "epoch": 0.6061788617886179,
      "step": 11184,
      "training_loss": 5.043980598449707
    },
    {
      "epoch": 0.6062330623306234,
      "step": 11185,
      "training_loss": 6.554940223693848
    },
    {
      "epoch": 0.6062872628726287,
      "step": 11186,
      "training_loss": 7.4737772941589355
    },
    {
      "epoch": 0.6063414634146341,
      "step": 11187,
      "training_loss": 5.502895355224609
    },
    {
      "epoch": 0.6063956639566396,
      "grad_norm": 23.64360237121582,
      "learning_rate": 1e-05,
      "loss": 6.1439,
      "step": 11188
    },
    {
      "epoch": 0.6063956639566396,
      "step": 11188,
      "training_loss": 7.110880374908447
    },
    {
      "epoch": 0.606449864498645,
      "step": 11189,
      "training_loss": 6.012363433837891
    },
    {
      "epoch": 0.6065040650406504,
      "step": 11190,
      "training_loss": 3.9696340560913086
    },
    {
      "epoch": 0.6065582655826558,
      "step": 11191,
      "training_loss": 5.86651611328125
    },
    {
      "epoch": 0.6066124661246612,
      "grad_norm": 29.538890838623047,
      "learning_rate": 1e-05,
      "loss": 5.7398,
      "step": 11192
    },
    {
      "epoch": 0.6066124661246612,
      "step": 11192,
      "training_loss": 7.111260414123535
    },
    {
      "epoch": 0.6066666666666667,
      "step": 11193,
      "training_loss": 6.243556976318359
    },
    {
      "epoch": 0.6067208672086721,
      "step": 11194,
      "training_loss": 3.752134084701538
    },
    {
      "epoch": 0.6067750677506775,
      "step": 11195,
      "training_loss": 5.929152488708496
    },
    {
      "epoch": 0.6068292682926829,
      "grad_norm": 30.68099594116211,
      "learning_rate": 1e-05,
      "loss": 5.759,
      "step": 11196
    },
    {
      "epoch": 0.6068292682926829,
      "step": 11196,
      "training_loss": 6.936254024505615
    },
    {
      "epoch": 0.6068834688346884,
      "step": 11197,
      "training_loss": 6.843874454498291
    },
    {
      "epoch": 0.6069376693766938,
      "step": 11198,
      "training_loss": 6.964770317077637
    },
    {
      "epoch": 0.6069918699186991,
      "step": 11199,
      "training_loss": 7.376611232757568
    },
    {
      "epoch": 0.6070460704607046,
      "grad_norm": 46.50511169433594,
      "learning_rate": 1e-05,
      "loss": 7.0304,
      "step": 11200
    },
    {
      "epoch": 0.6070460704607046,
      "step": 11200,
      "training_loss": 7.183384895324707
    },
    {
      "epoch": 0.60710027100271,
      "step": 11201,
      "training_loss": 7.443504333496094
    },
    {
      "epoch": 0.6071544715447155,
      "step": 11202,
      "training_loss": 6.876457691192627
    },
    {
      "epoch": 0.6072086720867209,
      "step": 11203,
      "training_loss": 6.16873836517334
    },
    {
      "epoch": 0.6072628726287262,
      "grad_norm": 45.977745056152344,
      "learning_rate": 1e-05,
      "loss": 6.918,
      "step": 11204
    },
    {
      "epoch": 0.6072628726287262,
      "step": 11204,
      "training_loss": 6.689276218414307
    },
    {
      "epoch": 0.6073170731707317,
      "step": 11205,
      "training_loss": 4.460056304931641
    },
    {
      "epoch": 0.6073712737127371,
      "step": 11206,
      "training_loss": 6.421581268310547
    },
    {
      "epoch": 0.6074254742547426,
      "step": 11207,
      "training_loss": 7.392500877380371
    },
    {
      "epoch": 0.6074796747967479,
      "grad_norm": 30.580488204956055,
      "learning_rate": 1e-05,
      "loss": 6.2409,
      "step": 11208
    },
    {
      "epoch": 0.6074796747967479,
      "step": 11208,
      "training_loss": 5.146912574768066
    },
    {
      "epoch": 0.6075338753387534,
      "step": 11209,
      "training_loss": 5.7571306228637695
    },
    {
      "epoch": 0.6075880758807588,
      "step": 11210,
      "training_loss": 9.073530197143555
    },
    {
      "epoch": 0.6076422764227642,
      "step": 11211,
      "training_loss": 3.5741138458251953
    },
    {
      "epoch": 0.6076964769647697,
      "grad_norm": 32.482051849365234,
      "learning_rate": 1e-05,
      "loss": 5.8879,
      "step": 11212
    },
    {
      "epoch": 0.6076964769647697,
      "step": 11212,
      "training_loss": 6.732844829559326
    },
    {
      "epoch": 0.607750677506775,
      "step": 11213,
      "training_loss": 7.417181491851807
    },
    {
      "epoch": 0.6078048780487805,
      "step": 11214,
      "training_loss": 6.815167427062988
    },
    {
      "epoch": 0.6078590785907859,
      "step": 11215,
      "training_loss": 6.950540542602539
    },
    {
      "epoch": 0.6079132791327914,
      "grad_norm": 17.182924270629883,
      "learning_rate": 1e-05,
      "loss": 6.9789,
      "step": 11216
    },
    {
      "epoch": 0.6079132791327914,
      "step": 11216,
      "training_loss": 6.477718353271484
    },
    {
      "epoch": 0.6079674796747967,
      "step": 11217,
      "training_loss": 6.643585681915283
    },
    {
      "epoch": 0.6080216802168021,
      "step": 11218,
      "training_loss": 7.812399864196777
    },
    {
      "epoch": 0.6080758807588076,
      "step": 11219,
      "training_loss": 6.245311737060547
    },
    {
      "epoch": 0.608130081300813,
      "grad_norm": 28.52439308166504,
      "learning_rate": 1e-05,
      "loss": 6.7948,
      "step": 11220
    },
    {
      "epoch": 0.608130081300813,
      "step": 11220,
      "training_loss": 6.402613162994385
    },
    {
      "epoch": 0.6081842818428185,
      "step": 11221,
      "training_loss": 6.639903545379639
    },
    {
      "epoch": 0.6082384823848238,
      "step": 11222,
      "training_loss": 5.796301364898682
    },
    {
      "epoch": 0.6082926829268293,
      "step": 11223,
      "training_loss": 7.1629319190979
    },
    {
      "epoch": 0.6083468834688347,
      "grad_norm": 32.73838806152344,
      "learning_rate": 1e-05,
      "loss": 6.5004,
      "step": 11224
    },
    {
      "epoch": 0.6083468834688347,
      "step": 11224,
      "training_loss": 8.80589485168457
    },
    {
      "epoch": 0.6084010840108401,
      "step": 11225,
      "training_loss": 8.146546363830566
    },
    {
      "epoch": 0.6084552845528455,
      "step": 11226,
      "training_loss": 7.365365505218506
    },
    {
      "epoch": 0.6085094850948509,
      "step": 11227,
      "training_loss": 5.216592311859131
    },
    {
      "epoch": 0.6085636856368564,
      "grad_norm": 34.2731819152832,
      "learning_rate": 1e-05,
      "loss": 7.3836,
      "step": 11228
    },
    {
      "epoch": 0.6085636856368564,
      "step": 11228,
      "training_loss": 7.014476299285889
    },
    {
      "epoch": 0.6086178861788618,
      "step": 11229,
      "training_loss": 7.646817207336426
    },
    {
      "epoch": 0.6086720867208673,
      "step": 11230,
      "training_loss": 4.6355366706848145
    },
    {
      "epoch": 0.6087262872628726,
      "step": 11231,
      "training_loss": 5.363553047180176
    },
    {
      "epoch": 0.608780487804878,
      "grad_norm": 27.26681900024414,
      "learning_rate": 1e-05,
      "loss": 6.1651,
      "step": 11232
    },
    {
      "epoch": 0.608780487804878,
      "step": 11232,
      "training_loss": 6.6459808349609375
    },
    {
      "epoch": 0.6088346883468835,
      "step": 11233,
      "training_loss": 5.915040493011475
    },
    {
      "epoch": 0.6088888888888889,
      "step": 11234,
      "training_loss": 6.742127895355225
    },
    {
      "epoch": 0.6089430894308943,
      "step": 11235,
      "training_loss": 7.124571800231934
    },
    {
      "epoch": 0.6089972899728997,
      "grad_norm": 27.36038589477539,
      "learning_rate": 1e-05,
      "loss": 6.6069,
      "step": 11236
    },
    {
      "epoch": 0.6089972899728997,
      "step": 11236,
      "training_loss": 5.662044048309326
    },
    {
      "epoch": 0.6090514905149051,
      "step": 11237,
      "training_loss": 6.8114237785339355
    },
    {
      "epoch": 0.6091056910569106,
      "step": 11238,
      "training_loss": 8.750863075256348
    },
    {
      "epoch": 0.609159891598916,
      "step": 11239,
      "training_loss": 7.9501729011535645
    },
    {
      "epoch": 0.6092140921409214,
      "grad_norm": 28.623126983642578,
      "learning_rate": 1e-05,
      "loss": 7.2936,
      "step": 11240
    },
    {
      "epoch": 0.6092140921409214,
      "step": 11240,
      "training_loss": 5.928312301635742
    },
    {
      "epoch": 0.6092682926829268,
      "step": 11241,
      "training_loss": 7.001163959503174
    },
    {
      "epoch": 0.6093224932249323,
      "step": 11242,
      "training_loss": 6.664824485778809
    },
    {
      "epoch": 0.6093766937669377,
      "step": 11243,
      "training_loss": 6.761682987213135
    },
    {
      "epoch": 0.609430894308943,
      "grad_norm": 61.847042083740234,
      "learning_rate": 1e-05,
      "loss": 6.589,
      "step": 11244
    },
    {
      "epoch": 0.609430894308943,
      "step": 11244,
      "training_loss": 6.911077499389648
    },
    {
      "epoch": 0.6094850948509485,
      "step": 11245,
      "training_loss": 5.638232707977295
    },
    {
      "epoch": 0.6095392953929539,
      "step": 11246,
      "training_loss": 6.52617073059082
    },
    {
      "epoch": 0.6095934959349594,
      "step": 11247,
      "training_loss": 6.392491817474365
    },
    {
      "epoch": 0.6096476964769648,
      "grad_norm": 29.211414337158203,
      "learning_rate": 1e-05,
      "loss": 6.367,
      "step": 11248
    },
    {
      "epoch": 0.6096476964769648,
      "step": 11248,
      "training_loss": 5.275373458862305
    },
    {
      "epoch": 0.6097018970189702,
      "step": 11249,
      "training_loss": 6.5089921951293945
    },
    {
      "epoch": 0.6097560975609756,
      "step": 11250,
      "training_loss": 7.675992488861084
    },
    {
      "epoch": 0.609810298102981,
      "step": 11251,
      "training_loss": 6.947501182556152
    },
    {
      "epoch": 0.6098644986449865,
      "grad_norm": 17.48905372619629,
      "learning_rate": 1e-05,
      "loss": 6.602,
      "step": 11252
    },
    {
      "epoch": 0.6098644986449865,
      "step": 11252,
      "training_loss": 7.220412254333496
    },
    {
      "epoch": 0.6099186991869918,
      "step": 11253,
      "training_loss": 6.699283123016357
    },
    {
      "epoch": 0.6099728997289973,
      "step": 11254,
      "training_loss": 6.433595180511475
    },
    {
      "epoch": 0.6100271002710027,
      "step": 11255,
      "training_loss": 4.99775505065918
    },
    {
      "epoch": 0.6100813008130082,
      "grad_norm": 21.1135311126709,
      "learning_rate": 1e-05,
      "loss": 6.3378,
      "step": 11256
    },
    {
      "epoch": 0.6100813008130082,
      "step": 11256,
      "training_loss": 7.210528373718262
    },
    {
      "epoch": 0.6101355013550136,
      "step": 11257,
      "training_loss": 6.70083475112915
    },
    {
      "epoch": 0.6101897018970189,
      "step": 11258,
      "training_loss": 6.478548049926758
    },
    {
      "epoch": 0.6102439024390244,
      "step": 11259,
      "training_loss": 6.95396089553833
    },
    {
      "epoch": 0.6102981029810298,
      "grad_norm": 48.264259338378906,
      "learning_rate": 1e-05,
      "loss": 6.836,
      "step": 11260
    },
    {
      "epoch": 0.6102981029810298,
      "step": 11260,
      "training_loss": 6.467312335968018
    },
    {
      "epoch": 0.6103523035230353,
      "step": 11261,
      "training_loss": 6.568975448608398
    },
    {
      "epoch": 0.6104065040650406,
      "step": 11262,
      "training_loss": 7.7906084060668945
    },
    {
      "epoch": 0.610460704607046,
      "step": 11263,
      "training_loss": 7.278379917144775
    },
    {
      "epoch": 0.6105149051490515,
      "grad_norm": 42.62302780151367,
      "learning_rate": 1e-05,
      "loss": 7.0263,
      "step": 11264
    },
    {
      "epoch": 0.6105149051490515,
      "step": 11264,
      "training_loss": 8.276134490966797
    },
    {
      "epoch": 0.6105691056910569,
      "step": 11265,
      "training_loss": 6.663234233856201
    },
    {
      "epoch": 0.6106233062330624,
      "step": 11266,
      "training_loss": 6.4208149909973145
    },
    {
      "epoch": 0.6106775067750677,
      "step": 11267,
      "training_loss": 6.809808254241943
    },
    {
      "epoch": 0.6107317073170732,
      "grad_norm": 35.38264465332031,
      "learning_rate": 1e-05,
      "loss": 7.0425,
      "step": 11268
    },
    {
      "epoch": 0.6107317073170732,
      "step": 11268,
      "training_loss": 8.35557746887207
    },
    {
      "epoch": 0.6107859078590786,
      "step": 11269,
      "training_loss": 6.543444633483887
    },
    {
      "epoch": 0.610840108401084,
      "step": 11270,
      "training_loss": 7.7660441398620605
    },
    {
      "epoch": 0.6108943089430894,
      "step": 11271,
      "training_loss": 5.178247928619385
    },
    {
      "epoch": 0.6109485094850948,
      "grad_norm": 25.233232498168945,
      "learning_rate": 1e-05,
      "loss": 6.9608,
      "step": 11272
    },
    {
      "epoch": 0.6109485094850948,
      "step": 11272,
      "training_loss": 7.078662872314453
    },
    {
      "epoch": 0.6110027100271003,
      "step": 11273,
      "training_loss": 6.627451419830322
    },
    {
      "epoch": 0.6110569105691057,
      "step": 11274,
      "training_loss": 6.655632495880127
    },
    {
      "epoch": 0.6111111111111112,
      "step": 11275,
      "training_loss": 7.80885648727417
    },
    {
      "epoch": 0.6111653116531165,
      "grad_norm": 21.70892906188965,
      "learning_rate": 1e-05,
      "loss": 7.0427,
      "step": 11276
    },
    {
      "epoch": 0.6111653116531165,
      "step": 11276,
      "training_loss": 6.768720626831055
    },
    {
      "epoch": 0.6112195121951219,
      "step": 11277,
      "training_loss": 6.723751544952393
    },
    {
      "epoch": 0.6112737127371274,
      "step": 11278,
      "training_loss": 7.539618492126465
    },
    {
      "epoch": 0.6113279132791328,
      "step": 11279,
      "training_loss": 6.713935375213623
    },
    {
      "epoch": 0.6113821138211382,
      "grad_norm": 47.264644622802734,
      "learning_rate": 1e-05,
      "loss": 6.9365,
      "step": 11280
    },
    {
      "epoch": 0.6113821138211382,
      "step": 11280,
      "training_loss": 6.303622245788574
    },
    {
      "epoch": 0.6114363143631436,
      "step": 11281,
      "training_loss": 7.595890045166016
    },
    {
      "epoch": 0.611490514905149,
      "step": 11282,
      "training_loss": 8.824200630187988
    },
    {
      "epoch": 0.6115447154471545,
      "step": 11283,
      "training_loss": 7.023214340209961
    },
    {
      "epoch": 0.6115989159891599,
      "grad_norm": 23.584877014160156,
      "learning_rate": 1e-05,
      "loss": 7.4367,
      "step": 11284
    },
    {
      "epoch": 0.6115989159891599,
      "step": 11284,
      "training_loss": 7.041236400604248
    },
    {
      "epoch": 0.6116531165311653,
      "step": 11285,
      "training_loss": 5.738922119140625
    },
    {
      "epoch": 0.6117073170731707,
      "step": 11286,
      "training_loss": 7.04403829574585
    },
    {
      "epoch": 0.6117615176151762,
      "step": 11287,
      "training_loss": 6.475838661193848
    },
    {
      "epoch": 0.6118157181571816,
      "grad_norm": 39.53059768676758,
      "learning_rate": 1e-05,
      "loss": 6.575,
      "step": 11288
    },
    {
      "epoch": 0.6118157181571816,
      "step": 11288,
      "training_loss": 6.496584892272949
    },
    {
      "epoch": 0.6118699186991869,
      "step": 11289,
      "training_loss": 6.644057750701904
    },
    {
      "epoch": 0.6119241192411924,
      "step": 11290,
      "training_loss": 6.992630481719971
    },
    {
      "epoch": 0.6119783197831978,
      "step": 11291,
      "training_loss": 7.3552350997924805
    },
    {
      "epoch": 0.6120325203252033,
      "grad_norm": 26.70940589904785,
      "learning_rate": 1e-05,
      "loss": 6.8721,
      "step": 11292
    },
    {
      "epoch": 0.6120325203252033,
      "step": 11292,
      "training_loss": 6.8657989501953125
    },
    {
      "epoch": 0.6120867208672087,
      "step": 11293,
      "training_loss": 5.419997692108154
    },
    {
      "epoch": 0.6121409214092141,
      "step": 11294,
      "training_loss": 5.801185607910156
    },
    {
      "epoch": 0.6121951219512195,
      "step": 11295,
      "training_loss": 5.838531970977783
    },
    {
      "epoch": 0.612249322493225,
      "grad_norm": 23.09028434753418,
      "learning_rate": 1e-05,
      "loss": 5.9814,
      "step": 11296
    },
    {
      "epoch": 0.612249322493225,
      "step": 11296,
      "training_loss": 7.647808074951172
    },
    {
      "epoch": 0.6123035230352304,
      "step": 11297,
      "training_loss": 6.8108391761779785
    },
    {
      "epoch": 0.6123577235772357,
      "step": 11298,
      "training_loss": 8.02306079864502
    },
    {
      "epoch": 0.6124119241192412,
      "step": 11299,
      "training_loss": 5.723076343536377
    },
    {
      "epoch": 0.6124661246612466,
      "grad_norm": 33.0677604675293,
      "learning_rate": 1e-05,
      "loss": 7.0512,
      "step": 11300
    },
    {
      "epoch": 0.6124661246612466,
      "step": 11300,
      "training_loss": 7.636668682098389
    },
    {
      "epoch": 0.6125203252032521,
      "step": 11301,
      "training_loss": 6.755480766296387
    },
    {
      "epoch": 0.6125745257452575,
      "step": 11302,
      "training_loss": 5.89705753326416
    },
    {
      "epoch": 0.6126287262872628,
      "step": 11303,
      "training_loss": 6.818777561187744
    },
    {
      "epoch": 0.6126829268292683,
      "grad_norm": 29.24814224243164,
      "learning_rate": 1e-05,
      "loss": 6.777,
      "step": 11304
    },
    {
      "epoch": 0.6126829268292683,
      "step": 11304,
      "training_loss": 7.734655857086182
    },
    {
      "epoch": 0.6127371273712737,
      "step": 11305,
      "training_loss": 6.335875034332275
    },
    {
      "epoch": 0.6127913279132792,
      "step": 11306,
      "training_loss": 6.5225830078125
    },
    {
      "epoch": 0.6128455284552845,
      "step": 11307,
      "training_loss": 6.633366107940674
    },
    {
      "epoch": 0.61289972899729,
      "grad_norm": 20.891796112060547,
      "learning_rate": 1e-05,
      "loss": 6.8066,
      "step": 11308
    },
    {
      "epoch": 0.61289972899729,
      "step": 11308,
      "training_loss": 5.564530372619629
    },
    {
      "epoch": 0.6129539295392954,
      "step": 11309,
      "training_loss": 6.226563453674316
    },
    {
      "epoch": 0.6130081300813008,
      "step": 11310,
      "training_loss": 6.35615348815918
    },
    {
      "epoch": 0.6130623306233063,
      "step": 11311,
      "training_loss": 6.56524658203125
    },
    {
      "epoch": 0.6131165311653116,
      "grad_norm": 29.677932739257812,
      "learning_rate": 1e-05,
      "loss": 6.1781,
      "step": 11312
    },
    {
      "epoch": 0.6131165311653116,
      "step": 11312,
      "training_loss": 7.740209102630615
    },
    {
      "epoch": 0.6131707317073171,
      "step": 11313,
      "training_loss": 6.042128086090088
    },
    {
      "epoch": 0.6132249322493225,
      "step": 11314,
      "training_loss": 4.316457748413086
    },
    {
      "epoch": 0.613279132791328,
      "step": 11315,
      "training_loss": 5.491030693054199
    },
    {
      "epoch": 0.6133333333333333,
      "grad_norm": 29.576078414916992,
      "learning_rate": 1e-05,
      "loss": 5.8975,
      "step": 11316
    },
    {
      "epoch": 0.6133333333333333,
      "step": 11316,
      "training_loss": 6.699967861175537
    },
    {
      "epoch": 0.6133875338753387,
      "step": 11317,
      "training_loss": 6.452723503112793
    },
    {
      "epoch": 0.6134417344173442,
      "step": 11318,
      "training_loss": 7.387274265289307
    },
    {
      "epoch": 0.6134959349593496,
      "step": 11319,
      "training_loss": 6.888917922973633
    },
    {
      "epoch": 0.6135501355013551,
      "grad_norm": 32.25921630859375,
      "learning_rate": 1e-05,
      "loss": 6.8572,
      "step": 11320
    },
    {
      "epoch": 0.6135501355013551,
      "step": 11320,
      "training_loss": 7.7271599769592285
    },
    {
      "epoch": 0.6136043360433604,
      "step": 11321,
      "training_loss": 6.869986057281494
    },
    {
      "epoch": 0.6136585365853658,
      "step": 11322,
      "training_loss": 7.058345317840576
    },
    {
      "epoch": 0.6137127371273713,
      "step": 11323,
      "training_loss": 6.711979866027832
    },
    {
      "epoch": 0.6137669376693767,
      "grad_norm": 26.176889419555664,
      "learning_rate": 1e-05,
      "loss": 7.0919,
      "step": 11324
    },
    {
      "epoch": 0.6137669376693767,
      "step": 11324,
      "training_loss": 5.441460132598877
    },
    {
      "epoch": 0.6138211382113821,
      "step": 11325,
      "training_loss": 6.555063247680664
    },
    {
      "epoch": 0.6138753387533875,
      "step": 11326,
      "training_loss": 6.407551288604736
    },
    {
      "epoch": 0.613929539295393,
      "step": 11327,
      "training_loss": 8.479093551635742
    },
    {
      "epoch": 0.6139837398373984,
      "grad_norm": 41.8017692565918,
      "learning_rate": 1e-05,
      "loss": 6.7208,
      "step": 11328
    },
    {
      "epoch": 0.6139837398373984,
      "step": 11328,
      "training_loss": 8.638534545898438
    },
    {
      "epoch": 0.6140379403794038,
      "step": 11329,
      "training_loss": 6.704812049865723
    },
    {
      "epoch": 0.6140921409214092,
      "step": 11330,
      "training_loss": 5.16819953918457
    },
    {
      "epoch": 0.6141463414634146,
      "step": 11331,
      "training_loss": 5.4704203605651855
    },
    {
      "epoch": 0.6142005420054201,
      "grad_norm": 39.048866271972656,
      "learning_rate": 1e-05,
      "loss": 6.4955,
      "step": 11332
    },
    {
      "epoch": 0.6142005420054201,
      "step": 11332,
      "training_loss": 5.413432598114014
    },
    {
      "epoch": 0.6142547425474255,
      "step": 11333,
      "training_loss": 7.168967247009277
    },
    {
      "epoch": 0.6143089430894308,
      "step": 11334,
      "training_loss": 6.714488983154297
    },
    {
      "epoch": 0.6143631436314363,
      "step": 11335,
      "training_loss": 5.86995792388916
    },
    {
      "epoch": 0.6144173441734417,
      "grad_norm": 64.10388946533203,
      "learning_rate": 1e-05,
      "loss": 6.2917,
      "step": 11336
    },
    {
      "epoch": 0.6144173441734417,
      "step": 11336,
      "training_loss": 7.32692289352417
    },
    {
      "epoch": 0.6144715447154472,
      "step": 11337,
      "training_loss": 6.773665428161621
    },
    {
      "epoch": 0.6145257452574526,
      "step": 11338,
      "training_loss": 6.876765727996826
    },
    {
      "epoch": 0.614579945799458,
      "step": 11339,
      "training_loss": 6.3032755851745605
    },
    {
      "epoch": 0.6146341463414634,
      "grad_norm": 28.352487564086914,
      "learning_rate": 1e-05,
      "loss": 6.8202,
      "step": 11340
    },
    {
      "epoch": 0.6146341463414634,
      "step": 11340,
      "training_loss": 7.550464153289795
    },
    {
      "epoch": 0.6146883468834689,
      "step": 11341,
      "training_loss": 5.043428897857666
    },
    {
      "epoch": 0.6147425474254743,
      "step": 11342,
      "training_loss": 7.385724067687988
    },
    {
      "epoch": 0.6147967479674796,
      "step": 11343,
      "training_loss": 3.5865566730499268
    },
    {
      "epoch": 0.6148509485094851,
      "grad_norm": 34.023834228515625,
      "learning_rate": 1e-05,
      "loss": 5.8915,
      "step": 11344
    },
    {
      "epoch": 0.6148509485094851,
      "step": 11344,
      "training_loss": 5.4101409912109375
    },
    {
      "epoch": 0.6149051490514905,
      "step": 11345,
      "training_loss": 5.848629474639893
    },
    {
      "epoch": 0.614959349593496,
      "step": 11346,
      "training_loss": 6.231936931610107
    },
    {
      "epoch": 0.6150135501355014,
      "step": 11347,
      "training_loss": 6.972702980041504
    },
    {
      "epoch": 0.6150677506775067,
      "grad_norm": 23.260929107666016,
      "learning_rate": 1e-05,
      "loss": 6.1159,
      "step": 11348
    },
    {
      "epoch": 0.6150677506775067,
      "step": 11348,
      "training_loss": 5.41292667388916
    },
    {
      "epoch": 0.6151219512195122,
      "step": 11349,
      "training_loss": 7.562378406524658
    },
    {
      "epoch": 0.6151761517615176,
      "step": 11350,
      "training_loss": 6.329529285430908
    },
    {
      "epoch": 0.6152303523035231,
      "step": 11351,
      "training_loss": 7.996321201324463
    },
    {
      "epoch": 0.6152845528455284,
      "grad_norm": 38.92446517944336,
      "learning_rate": 1e-05,
      "loss": 6.8253,
      "step": 11352
    },
    {
      "epoch": 0.6152845528455284,
      "step": 11352,
      "training_loss": 7.149829387664795
    },
    {
      "epoch": 0.6153387533875339,
      "step": 11353,
      "training_loss": 5.866180896759033
    },
    {
      "epoch": 0.6153929539295393,
      "step": 11354,
      "training_loss": 6.5238447189331055
    },
    {
      "epoch": 0.6154471544715447,
      "step": 11355,
      "training_loss": 6.789317607879639
    },
    {
      "epoch": 0.6155013550135502,
      "grad_norm": 19.80809211730957,
      "learning_rate": 1e-05,
      "loss": 6.5823,
      "step": 11356
    },
    {
      "epoch": 0.6155013550135502,
      "step": 11356,
      "training_loss": 7.2432050704956055
    },
    {
      "epoch": 0.6155555555555555,
      "step": 11357,
      "training_loss": 7.723573207855225
    },
    {
      "epoch": 0.615609756097561,
      "step": 11358,
      "training_loss": 6.470171928405762
    },
    {
      "epoch": 0.6156639566395664,
      "step": 11359,
      "training_loss": 7.880538463592529
    },
    {
      "epoch": 0.6157181571815719,
      "grad_norm": 39.955596923828125,
      "learning_rate": 1e-05,
      "loss": 7.3294,
      "step": 11360
    },
    {
      "epoch": 0.6157181571815719,
      "step": 11360,
      "training_loss": 7.375846862792969
    },
    {
      "epoch": 0.6157723577235772,
      "step": 11361,
      "training_loss": 6.149341106414795
    },
    {
      "epoch": 0.6158265582655826,
      "step": 11362,
      "training_loss": 6.489469528198242
    },
    {
      "epoch": 0.6158807588075881,
      "step": 11363,
      "training_loss": 7.371108055114746
    },
    {
      "epoch": 0.6159349593495935,
      "grad_norm": 22.78110694885254,
      "learning_rate": 1e-05,
      "loss": 6.8464,
      "step": 11364
    },
    {
      "epoch": 0.6159349593495935,
      "step": 11364,
      "training_loss": 7.466135025024414
    },
    {
      "epoch": 0.615989159891599,
      "step": 11365,
      "training_loss": 6.901870250701904
    },
    {
      "epoch": 0.6160433604336043,
      "step": 11366,
      "training_loss": 7.144099712371826
    },
    {
      "epoch": 0.6160975609756097,
      "step": 11367,
      "training_loss": 6.562309741973877
    },
    {
      "epoch": 0.6161517615176152,
      "grad_norm": 26.26930046081543,
      "learning_rate": 1e-05,
      "loss": 7.0186,
      "step": 11368
    },
    {
      "epoch": 0.6161517615176152,
      "step": 11368,
      "training_loss": 6.659481048583984
    },
    {
      "epoch": 0.6162059620596206,
      "step": 11369,
      "training_loss": 6.7514190673828125
    },
    {
      "epoch": 0.616260162601626,
      "step": 11370,
      "training_loss": 6.889050483703613
    },
    {
      "epoch": 0.6163143631436314,
      "step": 11371,
      "training_loss": 7.457887649536133
    },
    {
      "epoch": 0.6163685636856369,
      "grad_norm": 32.93944549560547,
      "learning_rate": 1e-05,
      "loss": 6.9395,
      "step": 11372
    },
    {
      "epoch": 0.6163685636856369,
      "step": 11372,
      "training_loss": 6.600778102874756
    },
    {
      "epoch": 0.6164227642276423,
      "step": 11373,
      "training_loss": 6.660166263580322
    },
    {
      "epoch": 0.6164769647696478,
      "step": 11374,
      "training_loss": 5.915328502655029
    },
    {
      "epoch": 0.6165311653116531,
      "step": 11375,
      "training_loss": 6.538428783416748
    },
    {
      "epoch": 0.6165853658536585,
      "grad_norm": 19.925966262817383,
      "learning_rate": 1e-05,
      "loss": 6.4287,
      "step": 11376
    },
    {
      "epoch": 0.6165853658536585,
      "step": 11376,
      "training_loss": 6.400457859039307
    },
    {
      "epoch": 0.616639566395664,
      "step": 11377,
      "training_loss": 6.171314716339111
    },
    {
      "epoch": 0.6166937669376694,
      "step": 11378,
      "training_loss": 5.315052509307861
    },
    {
      "epoch": 0.6167479674796748,
      "step": 11379,
      "training_loss": 5.3396897315979
    },
    {
      "epoch": 0.6168021680216802,
      "grad_norm": 24.247392654418945,
      "learning_rate": 1e-05,
      "loss": 5.8066,
      "step": 11380
    },
    {
      "epoch": 0.6168021680216802,
      "step": 11380,
      "training_loss": 6.730795860290527
    },
    {
      "epoch": 0.6168563685636856,
      "step": 11381,
      "training_loss": 5.539433479309082
    },
    {
      "epoch": 0.6169105691056911,
      "step": 11382,
      "training_loss": 6.604463577270508
    },
    {
      "epoch": 0.6169647696476965,
      "step": 11383,
      "training_loss": 7.860656261444092
    },
    {
      "epoch": 0.6170189701897019,
      "grad_norm": 26.743085861206055,
      "learning_rate": 1e-05,
      "loss": 6.6838,
      "step": 11384
    },
    {
      "epoch": 0.6170189701897019,
      "step": 11384,
      "training_loss": 6.905025959014893
    },
    {
      "epoch": 0.6170731707317073,
      "step": 11385,
      "training_loss": 7.385547161102295
    },
    {
      "epoch": 0.6171273712737128,
      "step": 11386,
      "training_loss": 4.919545650482178
    },
    {
      "epoch": 0.6171815718157182,
      "step": 11387,
      "training_loss": 6.684041500091553
    },
    {
      "epoch": 0.6172357723577235,
      "grad_norm": 34.95923614501953,
      "learning_rate": 1e-05,
      "loss": 6.4735,
      "step": 11388
    },
    {
      "epoch": 0.6172357723577235,
      "step": 11388,
      "training_loss": 6.086111068725586
    },
    {
      "epoch": 0.617289972899729,
      "step": 11389,
      "training_loss": 10.608139991760254
    },
    {
      "epoch": 0.6173441734417344,
      "step": 11390,
      "training_loss": 8.819283485412598
    },
    {
      "epoch": 0.6173983739837399,
      "step": 11391,
      "training_loss": 6.525148391723633
    },
    {
      "epoch": 0.6174525745257453,
      "grad_norm": 32.84817886352539,
      "learning_rate": 1e-05,
      "loss": 8.0097,
      "step": 11392
    },
    {
      "epoch": 0.6174525745257453,
      "step": 11392,
      "training_loss": 5.800529479980469
    },
    {
      "epoch": 0.6175067750677506,
      "step": 11393,
      "training_loss": 5.222017765045166
    },
    {
      "epoch": 0.6175609756097561,
      "step": 11394,
      "training_loss": 7.031182289123535
    },
    {
      "epoch": 0.6176151761517615,
      "step": 11395,
      "training_loss": 5.246772766113281
    },
    {
      "epoch": 0.617669376693767,
      "grad_norm": 24.74188804626465,
      "learning_rate": 1e-05,
      "loss": 5.8251,
      "step": 11396
    },
    {
      "epoch": 0.617669376693767,
      "step": 11396,
      "training_loss": 7.470844268798828
    },
    {
      "epoch": 0.6177235772357723,
      "step": 11397,
      "training_loss": 7.243969917297363
    },
    {
      "epoch": 0.6177777777777778,
      "step": 11398,
      "training_loss": 5.692907810211182
    },
    {
      "epoch": 0.6178319783197832,
      "step": 11399,
      "training_loss": 6.4959235191345215
    },
    {
      "epoch": 0.6178861788617886,
      "grad_norm": 22.123445510864258,
      "learning_rate": 1e-05,
      "loss": 6.7259,
      "step": 11400
    },
    {
      "epoch": 0.6178861788617886,
      "step": 11400,
      "training_loss": 5.66618537902832
    },
    {
      "epoch": 0.6179403794037941,
      "step": 11401,
      "training_loss": 5.611722469329834
    },
    {
      "epoch": 0.6179945799457994,
      "step": 11402,
      "training_loss": 7.868722915649414
    },
    {
      "epoch": 0.6180487804878049,
      "step": 11403,
      "training_loss": 6.5907158851623535
    },
    {
      "epoch": 0.6181029810298103,
      "grad_norm": 32.301387786865234,
      "learning_rate": 1e-05,
      "loss": 6.4343,
      "step": 11404
    },
    {
      "epoch": 0.6181029810298103,
      "step": 11404,
      "training_loss": 6.536569118499756
    },
    {
      "epoch": 0.6181571815718158,
      "step": 11405,
      "training_loss": 7.067713260650635
    },
    {
      "epoch": 0.6182113821138211,
      "step": 11406,
      "training_loss": 7.110666275024414
    },
    {
      "epoch": 0.6182655826558265,
      "step": 11407,
      "training_loss": 6.465622901916504
    },
    {
      "epoch": 0.618319783197832,
      "grad_norm": 21.40811538696289,
      "learning_rate": 1e-05,
      "loss": 6.7951,
      "step": 11408
    },
    {
      "epoch": 0.618319783197832,
      "step": 11408,
      "training_loss": 6.641366958618164
    },
    {
      "epoch": 0.6183739837398374,
      "step": 11409,
      "training_loss": 7.417462348937988
    },
    {
      "epoch": 0.6184281842818428,
      "step": 11410,
      "training_loss": 4.870222091674805
    },
    {
      "epoch": 0.6184823848238482,
      "step": 11411,
      "training_loss": 6.760310173034668
    },
    {
      "epoch": 0.6185365853658537,
      "grad_norm": 31.947463989257812,
      "learning_rate": 1e-05,
      "loss": 6.4223,
      "step": 11412
    },
    {
      "epoch": 0.6185365853658537,
      "step": 11412,
      "training_loss": 6.739415645599365
    },
    {
      "epoch": 0.6185907859078591,
      "step": 11413,
      "training_loss": 6.626222133636475
    },
    {
      "epoch": 0.6186449864498645,
      "step": 11414,
      "training_loss": 6.477309226989746
    },
    {
      "epoch": 0.6186991869918699,
      "step": 11415,
      "training_loss": 6.343752861022949
    },
    {
      "epoch": 0.6187533875338753,
      "grad_norm": 23.466960906982422,
      "learning_rate": 1e-05,
      "loss": 6.5467,
      "step": 11416
    },
    {
      "epoch": 0.6187533875338753,
      "step": 11416,
      "training_loss": 7.153983116149902
    },
    {
      "epoch": 0.6188075880758808,
      "step": 11417,
      "training_loss": 6.6438307762146
    },
    {
      "epoch": 0.6188617886178862,
      "step": 11418,
      "training_loss": 5.20931339263916
    },
    {
      "epoch": 0.6189159891598915,
      "step": 11419,
      "training_loss": 5.469620227813721
    },
    {
      "epoch": 0.618970189701897,
      "grad_norm": 19.306188583374023,
      "learning_rate": 1e-05,
      "loss": 6.1192,
      "step": 11420
    },
    {
      "epoch": 0.618970189701897,
      "step": 11420,
      "training_loss": 7.655539512634277
    },
    {
      "epoch": 0.6190243902439024,
      "step": 11421,
      "training_loss": 6.905930519104004
    },
    {
      "epoch": 0.6190785907859079,
      "step": 11422,
      "training_loss": 5.962951183319092
    },
    {
      "epoch": 0.6191327913279133,
      "step": 11423,
      "training_loss": 8.2465238571167
    },
    {
      "epoch": 0.6191869918699187,
      "grad_norm": 29.833955764770508,
      "learning_rate": 1e-05,
      "loss": 7.1927,
      "step": 11424
    },
    {
      "epoch": 0.6191869918699187,
      "step": 11424,
      "training_loss": 7.494314193725586
    },
    {
      "epoch": 0.6192411924119241,
      "step": 11425,
      "training_loss": 5.940314769744873
    },
    {
      "epoch": 0.6192953929539295,
      "step": 11426,
      "training_loss": 7.606244087219238
    },
    {
      "epoch": 0.619349593495935,
      "step": 11427,
      "training_loss": 4.957748889923096
    },
    {
      "epoch": 0.6194037940379403,
      "grad_norm": 43.71128463745117,
      "learning_rate": 1e-05,
      "loss": 6.4997,
      "step": 11428
    },
    {
      "epoch": 0.6194037940379403,
      "step": 11428,
      "training_loss": 6.904810905456543
    },
    {
      "epoch": 0.6194579945799458,
      "step": 11429,
      "training_loss": 7.07820987701416
    },
    {
      "epoch": 0.6195121951219512,
      "step": 11430,
      "training_loss": 7.20499324798584
    },
    {
      "epoch": 0.6195663956639567,
      "step": 11431,
      "training_loss": 2.9351491928100586
    },
    {
      "epoch": 0.6196205962059621,
      "grad_norm": NaN,
      "learning_rate": 1e-05,
      "loss": 6.0308,
      "step": 11432
    },
    {
      "epoch": 0.6196205962059621,
      "step": 11432,
      "training_loss": 6.9413042068481445
    },
    {
      "epoch": 0.6196747967479674,
      "step": 11433,
      "training_loss": 6.2411699295043945
    },
    {
      "epoch": 0.6197289972899729,
      "step": 11434,
      "training_loss": 6.051945686340332
    },
    {
      "epoch": 0.6197831978319783,
      "step": 11435,
      "training_loss": 5.8712568283081055
    },
    {
      "epoch": 0.6198373983739838,
      "grad_norm": 31.292396545410156,
      "learning_rate": 1e-05,
      "loss": 6.2764,
      "step": 11436
    },
    {
      "epoch": 0.6198373983739838,
      "step": 11436,
      "training_loss": 7.141268730163574
    },
    {
      "epoch": 0.6198915989159891,
      "step": 11437,
      "training_loss": 6.752528667449951
    },
    {
      "epoch": 0.6199457994579946,
      "step": 11438,
      "training_loss": 6.805010795593262
    },
    {
      "epoch": 0.62,
      "step": 11439,
      "training_loss": 6.319882392883301
    },
    {
      "epoch": 0.6200542005420054,
      "grad_norm": 41.46897506713867,
      "learning_rate": 1e-05,
      "loss": 6.7547,
      "step": 11440
    },
    {
      "epoch": 0.6200542005420054,
      "step": 11440,
      "training_loss": 6.824941158294678
    },
    {
      "epoch": 0.6201084010840109,
      "step": 11441,
      "training_loss": 7.562222957611084
    },
    {
      "epoch": 0.6201626016260162,
      "step": 11442,
      "training_loss": 7.414915561676025
    },
    {
      "epoch": 0.6202168021680217,
      "step": 11443,
      "training_loss": 4.78086519241333
    },
    {
      "epoch": 0.6202710027100271,
      "grad_norm": 21.519500732421875,
      "learning_rate": 1e-05,
      "loss": 6.6457,
      "step": 11444
    },
    {
      "epoch": 0.6202710027100271,
      "step": 11444,
      "training_loss": 5.422993183135986
    },
    {
      "epoch": 0.6203252032520326,
      "step": 11445,
      "training_loss": 7.029477596282959
    },
    {
      "epoch": 0.6203794037940379,
      "step": 11446,
      "training_loss": 6.905835151672363
    },
    {
      "epoch": 0.6204336043360433,
      "step": 11447,
      "training_loss": 6.697786331176758
    },
    {
      "epoch": 0.6204878048780488,
      "grad_norm": 21.17983627319336,
      "learning_rate": 1e-05,
      "loss": 6.514,
      "step": 11448
    },
    {
      "epoch": 0.6204878048780488,
      "step": 11448,
      "training_loss": 3.2138311862945557
    },
    {
      "epoch": 0.6205420054200542,
      "step": 11449,
      "training_loss": 7.633329391479492
    },
    {
      "epoch": 0.6205962059620597,
      "step": 11450,
      "training_loss": 6.486152648925781
    },
    {
      "epoch": 0.620650406504065,
      "step": 11451,
      "training_loss": 7.725139617919922
    },
    {
      "epoch": 0.6207046070460704,
      "grad_norm": 63.305728912353516,
      "learning_rate": 1e-05,
      "loss": 6.2646,
      "step": 11452
    },
    {
      "epoch": 0.6207046070460704,
      "step": 11452,
      "training_loss": 7.839945316314697
    },
    {
      "epoch": 0.6207588075880759,
      "step": 11453,
      "training_loss": 6.783031463623047
    },
    {
      "epoch": 0.6208130081300813,
      "step": 11454,
      "training_loss": 5.130895614624023
    },
    {
      "epoch": 0.6208672086720867,
      "step": 11455,
      "training_loss": 8.278023719787598
    },
    {
      "epoch": 0.6209214092140921,
      "grad_norm": 31.493574142456055,
      "learning_rate": 1e-05,
      "loss": 7.008,
      "step": 11456
    },
    {
      "epoch": 0.6209214092140921,
      "step": 11456,
      "training_loss": 6.257554054260254
    },
    {
      "epoch": 0.6209756097560976,
      "step": 11457,
      "training_loss": 8.233624458312988
    },
    {
      "epoch": 0.621029810298103,
      "step": 11458,
      "training_loss": 8.270806312561035
    },
    {
      "epoch": 0.6210840108401084,
      "step": 11459,
      "training_loss": 5.534177303314209
    },
    {
      "epoch": 0.6211382113821138,
      "grad_norm": 28.535558700561523,
      "learning_rate": 1e-05,
      "loss": 7.074,
      "step": 11460
    },
    {
      "epoch": 0.6211382113821138,
      "step": 11460,
      "training_loss": 7.57560396194458
    },
    {
      "epoch": 0.6211924119241192,
      "step": 11461,
      "training_loss": 3.835627555847168
    },
    {
      "epoch": 0.6212466124661247,
      "step": 11462,
      "training_loss": 6.86619758605957
    },
    {
      "epoch": 0.6213008130081301,
      "step": 11463,
      "training_loss": 7.141982555389404
    },
    {
      "epoch": 0.6213550135501354,
      "grad_norm": 37.255828857421875,
      "learning_rate": 1e-05,
      "loss": 6.3549,
      "step": 11464
    },
    {
      "epoch": 0.6213550135501354,
      "step": 11464,
      "training_loss": 7.2726731300354
    },
    {
      "epoch": 0.6214092140921409,
      "step": 11465,
      "training_loss": 8.405336380004883
    },
    {
      "epoch": 0.6214634146341463,
      "step": 11466,
      "training_loss": 5.582237243652344
    },
    {
      "epoch": 0.6215176151761518,
      "step": 11467,
      "training_loss": 6.579826831817627
    },
    {
      "epoch": 0.6215718157181572,
      "grad_norm": 27.302186965942383,
      "learning_rate": 1e-05,
      "loss": 6.96,
      "step": 11468
    },
    {
      "epoch": 0.6215718157181572,
      "step": 11468,
      "training_loss": 6.643796920776367
    },
    {
      "epoch": 0.6216260162601626,
      "step": 11469,
      "training_loss": 6.271905899047852
    },
    {
      "epoch": 0.621680216802168,
      "step": 11470,
      "training_loss": 5.607721328735352
    },
    {
      "epoch": 0.6217344173441735,
      "step": 11471,
      "training_loss": 6.187300682067871
    },
    {
      "epoch": 0.6217886178861789,
      "grad_norm": 22.900409698486328,
      "learning_rate": 1e-05,
      "loss": 6.1777,
      "step": 11472
    },
    {
      "epoch": 0.6217886178861789,
      "step": 11472,
      "training_loss": 7.10373067855835
    },
    {
      "epoch": 0.6218428184281842,
      "step": 11473,
      "training_loss": 5.039839267730713
    },
    {
      "epoch": 0.6218970189701897,
      "step": 11474,
      "training_loss": 7.690942764282227
    },
    {
      "epoch": 0.6219512195121951,
      "step": 11475,
      "training_loss": 7.6645636558532715
    },
    {
      "epoch": 0.6220054200542006,
      "grad_norm": 38.05363845825195,
      "learning_rate": 1e-05,
      "loss": 6.8748,
      "step": 11476
    },
    {
      "epoch": 0.6220054200542006,
      "step": 11476,
      "training_loss": 5.859551906585693
    },
    {
      "epoch": 0.622059620596206,
      "step": 11477,
      "training_loss": 6.342041492462158
    },
    {
      "epoch": 0.6221138211382113,
      "step": 11478,
      "training_loss": 6.994059085845947
    },
    {
      "epoch": 0.6221680216802168,
      "step": 11479,
      "training_loss": 8.002752304077148
    },
    {
      "epoch": 0.6222222222222222,
      "grad_norm": 58.92805862426758,
      "learning_rate": 1e-05,
      "loss": 6.7996,
      "step": 11480
    },
    {
      "epoch": 0.6222222222222222,
      "step": 11480,
      "training_loss": 7.096755027770996
    },
    {
      "epoch": 0.6222764227642277,
      "step": 11481,
      "training_loss": 6.611555576324463
    },
    {
      "epoch": 0.622330623306233,
      "step": 11482,
      "training_loss": 7.363749027252197
    },
    {
      "epoch": 0.6223848238482385,
      "step": 11483,
      "training_loss": 6.256092071533203
    },
    {
      "epoch": 0.6224390243902439,
      "grad_norm": 26.842313766479492,
      "learning_rate": 1e-05,
      "loss": 6.832,
      "step": 11484
    },
    {
      "epoch": 0.6224390243902439,
      "step": 11484,
      "training_loss": 7.664494037628174
    },
    {
      "epoch": 0.6224932249322493,
      "step": 11485,
      "training_loss": 7.355506420135498
    },
    {
      "epoch": 0.6225474254742548,
      "step": 11486,
      "training_loss": 7.130784034729004
    },
    {
      "epoch": 0.6226016260162601,
      "step": 11487,
      "training_loss": 6.589031219482422
    },
    {
      "epoch": 0.6226558265582656,
      "grad_norm": 31.331823348999023,
      "learning_rate": 1e-05,
      "loss": 7.185,
      "step": 11488
    },
    {
      "epoch": 0.6226558265582656,
      "step": 11488,
      "training_loss": 6.542294979095459
    },
    {
      "epoch": 0.622710027100271,
      "step": 11489,
      "training_loss": 6.080667018890381
    },
    {
      "epoch": 0.6227642276422765,
      "step": 11490,
      "training_loss": 7.438118934631348
    },
    {
      "epoch": 0.6228184281842818,
      "step": 11491,
      "training_loss": 6.18211555480957
    },
    {
      "epoch": 0.6228726287262872,
      "grad_norm": 43.16499328613281,
      "learning_rate": 1e-05,
      "loss": 6.5608,
      "step": 11492
    },
    {
      "epoch": 0.6228726287262872,
      "step": 11492,
      "training_loss": 7.45780086517334
    },
    {
      "epoch": 0.6229268292682927,
      "step": 11493,
      "training_loss": 6.926546096801758
    },
    {
      "epoch": 0.6229810298102981,
      "step": 11494,
      "training_loss": 5.870646953582764
    },
    {
      "epoch": 0.6230352303523036,
      "step": 11495,
      "training_loss": 6.8589630126953125
    },
    {
      "epoch": 0.6230894308943089,
      "grad_norm": 15.570048332214355,
      "learning_rate": 1e-05,
      "loss": 6.7785,
      "step": 11496
    },
    {
      "epoch": 0.6230894308943089,
      "step": 11496,
      "training_loss": 6.724920749664307
    },
    {
      "epoch": 0.6231436314363143,
      "step": 11497,
      "training_loss": 6.9341278076171875
    },
    {
      "epoch": 0.6231978319783198,
      "step": 11498,
      "training_loss": 7.493228435516357
    },
    {
      "epoch": 0.6232520325203252,
      "step": 11499,
      "training_loss": 5.454426288604736
    },
    {
      "epoch": 0.6233062330623306,
      "grad_norm": 31.830642700195312,
      "learning_rate": 1e-05,
      "loss": 6.6517,
      "step": 11500
    },
    {
      "epoch": 0.6233062330623306,
      "step": 11500,
      "training_loss": 7.373132228851318
    },
    {
      "epoch": 0.623360433604336,
      "step": 11501,
      "training_loss": 5.351102352142334
    },
    {
      "epoch": 0.6234146341463415,
      "step": 11502,
      "training_loss": 6.826417446136475
    },
    {
      "epoch": 0.6234688346883469,
      "step": 11503,
      "training_loss": 7.135560989379883
    },
    {
      "epoch": 0.6235230352303524,
      "grad_norm": 24.611099243164062,
      "learning_rate": 1e-05,
      "loss": 6.6716,
      "step": 11504
    },
    {
      "epoch": 0.6235230352303524,
      "step": 11504,
      "training_loss": 5.470585823059082
    },
    {
      "epoch": 0.6235772357723577,
      "step": 11505,
      "training_loss": 5.930881500244141
    },
    {
      "epoch": 0.6236314363143631,
      "step": 11506,
      "training_loss": 6.559390544891357
    },
    {
      "epoch": 0.6236856368563686,
      "step": 11507,
      "training_loss": 7.606136798858643
    },
    {
      "epoch": 0.623739837398374,
      "grad_norm": 20.358566284179688,
      "learning_rate": 1e-05,
      "loss": 6.3917,
      "step": 11508
    },
    {
      "epoch": 0.623739837398374,
      "step": 11508,
      "training_loss": 7.260361671447754
    },
    {
      "epoch": 0.6237940379403794,
      "step": 11509,
      "training_loss": 7.117171287536621
    },
    {
      "epoch": 0.6238482384823848,
      "step": 11510,
      "training_loss": 6.310906410217285
    },
    {
      "epoch": 0.6239024390243902,
      "step": 11511,
      "training_loss": 6.793302059173584
    },
    {
      "epoch": 0.6239566395663957,
      "grad_norm": 18.46077537536621,
      "learning_rate": 1e-05,
      "loss": 6.8704,
      "step": 11512
    },
    {
      "epoch": 0.6239566395663957,
      "step": 11512,
      "training_loss": 6.983683109283447
    },
    {
      "epoch": 0.6240108401084011,
      "step": 11513,
      "training_loss": 7.3154120445251465
    },
    {
      "epoch": 0.6240650406504065,
      "step": 11514,
      "training_loss": 5.861965179443359
    },
    {
      "epoch": 0.6241192411924119,
      "step": 11515,
      "training_loss": 5.4448137283325195
    },
    {
      "epoch": 0.6241734417344174,
      "grad_norm": 23.00214195251465,
      "learning_rate": 1e-05,
      "loss": 6.4015,
      "step": 11516
    },
    {
      "epoch": 0.6241734417344174,
      "step": 11516,
      "training_loss": 6.342292785644531
    },
    {
      "epoch": 0.6242276422764228,
      "step": 11517,
      "training_loss": 5.050547122955322
    },
    {
      "epoch": 0.6242818428184281,
      "step": 11518,
      "training_loss": 6.624389171600342
    },
    {
      "epoch": 0.6243360433604336,
      "step": 11519,
      "training_loss": 7.28161096572876
    },
    {
      "epoch": 0.624390243902439,
      "grad_norm": 21.312402725219727,
      "learning_rate": 1e-05,
      "loss": 6.3247,
      "step": 11520
    },
    {
      "epoch": 0.624390243902439,
      "step": 11520,
      "training_loss": 6.81438684463501
    },
    {
      "epoch": 0.6244444444444445,
      "step": 11521,
      "training_loss": 6.046383857727051
    },
    {
      "epoch": 0.6244986449864499,
      "step": 11522,
      "training_loss": 7.120192527770996
    },
    {
      "epoch": 0.6245528455284552,
      "step": 11523,
      "training_loss": 6.566410064697266
    },
    {
      "epoch": 0.6246070460704607,
      "grad_norm": 27.912090301513672,
      "learning_rate": 1e-05,
      "loss": 6.6368,
      "step": 11524
    },
    {
      "epoch": 0.6246070460704607,
      "step": 11524,
      "training_loss": 3.386979818344116
    },
    {
      "epoch": 0.6246612466124661,
      "step": 11525,
      "training_loss": 5.407517433166504
    },
    {
      "epoch": 0.6247154471544716,
      "step": 11526,
      "training_loss": 6.758056640625
    },
    {
      "epoch": 0.6247696476964769,
      "step": 11527,
      "training_loss": 5.635386943817139
    },
    {
      "epoch": 0.6248238482384824,
      "grad_norm": 27.263317108154297,
      "learning_rate": 1e-05,
      "loss": 5.297,
      "step": 11528
    },
    {
      "epoch": 0.6248238482384824,
      "step": 11528,
      "training_loss": 5.52052116394043
    },
    {
      "epoch": 0.6248780487804878,
      "step": 11529,
      "training_loss": 6.978020191192627
    },
    {
      "epoch": 0.6249322493224932,
      "step": 11530,
      "training_loss": 6.656538963317871
    },
    {
      "epoch": 0.6249864498644987,
      "step": 11531,
      "training_loss": 3.4523122310638428
    },
    {
      "epoch": 0.625040650406504,
      "grad_norm": 34.59577178955078,
      "learning_rate": 1e-05,
      "loss": 5.6518,
      "step": 11532
    },
    {
      "epoch": 0.625040650406504,
      "step": 11532,
      "training_loss": 6.216303825378418
    },
    {
      "epoch": 0.6250948509485095,
      "step": 11533,
      "training_loss": 7.570906639099121
    },
    {
      "epoch": 0.6251490514905149,
      "step": 11534,
      "training_loss": 8.015138626098633
    },
    {
      "epoch": 0.6252032520325204,
      "step": 11535,
      "training_loss": 4.882587909698486
    },
    {
      "epoch": 0.6252574525745257,
      "grad_norm": 26.511127471923828,
      "learning_rate": 1e-05,
      "loss": 6.6712,
      "step": 11536
    },
    {
      "epoch": 0.6252574525745257,
      "step": 11536,
      "training_loss": 6.577565670013428
    },
    {
      "epoch": 0.6253116531165311,
      "step": 11537,
      "training_loss": 7.361900806427002
    },
    {
      "epoch": 0.6253658536585366,
      "step": 11538,
      "training_loss": 5.673123836517334
    },
    {
      "epoch": 0.625420054200542,
      "step": 11539,
      "training_loss": 6.523476600646973
    },
    {
      "epoch": 0.6254742547425475,
      "grad_norm": 29.904674530029297,
      "learning_rate": 1e-05,
      "loss": 6.534,
      "step": 11540
    },
    {
      "epoch": 0.6254742547425475,
      "step": 11540,
      "training_loss": 6.66992712020874
    },
    {
      "epoch": 0.6255284552845528,
      "step": 11541,
      "training_loss": 7.4056525230407715
    },
    {
      "epoch": 0.6255826558265583,
      "step": 11542,
      "training_loss": 6.377288341522217
    },
    {
      "epoch": 0.6256368563685637,
      "step": 11543,
      "training_loss": 6.185977935791016
    },
    {
      "epoch": 0.6256910569105691,
      "grad_norm": 33.30143356323242,
      "learning_rate": 1e-05,
      "loss": 6.6597,
      "step": 11544
    },
    {
      "epoch": 0.6256910569105691,
      "step": 11544,
      "training_loss": 6.797186374664307
    },
    {
      "epoch": 0.6257452574525745,
      "step": 11545,
      "training_loss": 7.276741981506348
    },
    {
      "epoch": 0.6257994579945799,
      "step": 11546,
      "training_loss": 7.194684028625488
    },
    {
      "epoch": 0.6258536585365854,
      "step": 11547,
      "training_loss": 5.834744930267334
    },
    {
      "epoch": 0.6259078590785908,
      "grad_norm": 21.539209365844727,
      "learning_rate": 1e-05,
      "loss": 6.7758,
      "step": 11548
    },
    {
      "epoch": 0.6259078590785908,
      "step": 11548,
      "training_loss": 5.381436824798584
    },
    {
      "epoch": 0.6259620596205963,
      "step": 11549,
      "training_loss": 5.592673301696777
    },
    {
      "epoch": 0.6260162601626016,
      "step": 11550,
      "training_loss": 6.813393592834473
    },
    {
      "epoch": 0.626070460704607,
      "step": 11551,
      "training_loss": 5.485158920288086
    },
    {
      "epoch": 0.6261246612466125,
      "grad_norm": 39.37210464477539,
      "learning_rate": 1e-05,
      "loss": 5.8182,
      "step": 11552
    },
    {
      "epoch": 0.6261246612466125,
      "step": 11552,
      "training_loss": 7.044721603393555
    },
    {
      "epoch": 0.6261788617886179,
      "step": 11553,
      "training_loss": 7.11760139465332
    },
    {
      "epoch": 0.6262330623306233,
      "step": 11554,
      "training_loss": 3.812392234802246
    },
    {
      "epoch": 0.6262872628726287,
      "step": 11555,
      "training_loss": 6.235739231109619
    },
    {
      "epoch": 0.6263414634146341,
      "grad_norm": 27.37174415588379,
      "learning_rate": 1e-05,
      "loss": 6.0526,
      "step": 11556
    },
    {
      "epoch": 0.6263414634146341,
      "step": 11556,
      "training_loss": 3.905977964401245
    },
    {
      "epoch": 0.6263956639566396,
      "step": 11557,
      "training_loss": 5.846651554107666
    },
    {
      "epoch": 0.626449864498645,
      "step": 11558,
      "training_loss": 8.077717781066895
    },
    {
      "epoch": 0.6265040650406504,
      "step": 11559,
      "training_loss": 5.686764240264893
    },
    {
      "epoch": 0.6265582655826558,
      "grad_norm": 46.82603073120117,
      "learning_rate": 1e-05,
      "loss": 5.8793,
      "step": 11560
    },
    {
      "epoch": 0.6265582655826558,
      "step": 11560,
      "training_loss": 7.305984020233154
    },
    {
      "epoch": 0.6266124661246613,
      "step": 11561,
      "training_loss": 7.545787811279297
    },
    {
      "epoch": 0.6266666666666667,
      "step": 11562,
      "training_loss": 7.195873260498047
    },
    {
      "epoch": 0.626720867208672,
      "step": 11563,
      "training_loss": 6.886990070343018
    },
    {
      "epoch": 0.6267750677506775,
      "grad_norm": 22.626081466674805,
      "learning_rate": 1e-05,
      "loss": 7.2337,
      "step": 11564
    },
    {
      "epoch": 0.6267750677506775,
      "step": 11564,
      "training_loss": 6.424182415008545
    },
    {
      "epoch": 0.6268292682926829,
      "step": 11565,
      "training_loss": 4.638602256774902
    },
    {
      "epoch": 0.6268834688346884,
      "step": 11566,
      "training_loss": 4.892691135406494
    },
    {
      "epoch": 0.6269376693766938,
      "step": 11567,
      "training_loss": 7.570157527923584
    },
    {
      "epoch": 0.6269918699186992,
      "grad_norm": 39.512123107910156,
      "learning_rate": 1e-05,
      "loss": 5.8814,
      "step": 11568
    },
    {
      "epoch": 0.6269918699186992,
      "step": 11568,
      "training_loss": 7.293168544769287
    },
    {
      "epoch": 0.6270460704607046,
      "step": 11569,
      "training_loss": 6.736209392547607
    },
    {
      "epoch": 0.62710027100271,
      "step": 11570,
      "training_loss": 7.775758266448975
    },
    {
      "epoch": 0.6271544715447155,
      "step": 11571,
      "training_loss": 6.9426045417785645
    },
    {
      "epoch": 0.6272086720867208,
      "grad_norm": 18.165447235107422,
      "learning_rate": 1e-05,
      "loss": 7.1869,
      "step": 11572
    },
    {
      "epoch": 0.6272086720867208,
      "step": 11572,
      "training_loss": 6.463873386383057
    },
    {
      "epoch": 0.6272628726287263,
      "step": 11573,
      "training_loss": 5.1314616203308105
    },
    {
      "epoch": 0.6273170731707317,
      "step": 11574,
      "training_loss": 6.907630443572998
    },
    {
      "epoch": 0.6273712737127372,
      "step": 11575,
      "training_loss": 4.854385852813721
    },
    {
      "epoch": 0.6274254742547426,
      "grad_norm": 26.0987548828125,
      "learning_rate": 1e-05,
      "loss": 5.8393,
      "step": 11576
    },
    {
      "epoch": 0.6274254742547426,
      "step": 11576,
      "training_loss": 6.606950283050537
    },
    {
      "epoch": 0.6274796747967479,
      "step": 11577,
      "training_loss": 7.254753112792969
    },
    {
      "epoch": 0.6275338753387534,
      "step": 11578,
      "training_loss": 4.366433620452881
    },
    {
      "epoch": 0.6275880758807588,
      "step": 11579,
      "training_loss": 3.7083940505981445
    },
    {
      "epoch": 0.6276422764227643,
      "grad_norm": 35.94243621826172,
      "learning_rate": 1e-05,
      "loss": 5.4841,
      "step": 11580
    },
    {
      "epoch": 0.6276422764227643,
      "step": 11580,
      "training_loss": 6.555232524871826
    },
    {
      "epoch": 0.6276964769647696,
      "step": 11581,
      "training_loss": 6.649840354919434
    },
    {
      "epoch": 0.627750677506775,
      "step": 11582,
      "training_loss": 6.795307636260986
    },
    {
      "epoch": 0.6278048780487805,
      "step": 11583,
      "training_loss": 6.3592529296875
    },
    {
      "epoch": 0.6278590785907859,
      "grad_norm": 30.199569702148438,
      "learning_rate": 1e-05,
      "loss": 6.5899,
      "step": 11584
    },
    {
      "epoch": 0.6278590785907859,
      "step": 11584,
      "training_loss": 6.327897548675537
    },
    {
      "epoch": 0.6279132791327914,
      "step": 11585,
      "training_loss": 7.387509346008301
    },
    {
      "epoch": 0.6279674796747967,
      "step": 11586,
      "training_loss": 6.040432453155518
    },
    {
      "epoch": 0.6280216802168022,
      "step": 11587,
      "training_loss": 7.09779691696167
    },
    {
      "epoch": 0.6280758807588076,
      "grad_norm": 18.437496185302734,
      "learning_rate": 1e-05,
      "loss": 6.7134,
      "step": 11588
    },
    {
      "epoch": 0.6280758807588076,
      "step": 11588,
      "training_loss": 7.080665111541748
    },
    {
      "epoch": 0.628130081300813,
      "step": 11589,
      "training_loss": 3.5524089336395264
    },
    {
      "epoch": 0.6281842818428184,
      "step": 11590,
      "training_loss": 7.7062668800354
    },
    {
      "epoch": 0.6282384823848238,
      "step": 11591,
      "training_loss": 7.710129737854004
    },
    {
      "epoch": 0.6282926829268293,
      "grad_norm": 54.00621795654297,
      "learning_rate": 1e-05,
      "loss": 6.5124,
      "step": 11592
    },
    {
      "epoch": 0.6282926829268293,
      "step": 11592,
      "training_loss": 6.0756754875183105
    },
    {
      "epoch": 0.6283468834688347,
      "step": 11593,
      "training_loss": 7.00514030456543
    },
    {
      "epoch": 0.6284010840108402,
      "step": 11594,
      "training_loss": 7.718172073364258
    },
    {
      "epoch": 0.6284552845528455,
      "step": 11595,
      "training_loss": 6.967085361480713
    },
    {
      "epoch": 0.6285094850948509,
      "grad_norm": 66.07813262939453,
      "learning_rate": 1e-05,
      "loss": 6.9415,
      "step": 11596
    },
    {
      "epoch": 0.6285094850948509,
      "step": 11596,
      "training_loss": 6.384727954864502
    },
    {
      "epoch": 0.6285636856368564,
      "step": 11597,
      "training_loss": 7.478121280670166
    },
    {
      "epoch": 0.6286178861788618,
      "step": 11598,
      "training_loss": 5.857002258300781
    },
    {
      "epoch": 0.6286720867208672,
      "step": 11599,
      "training_loss": 5.9838361740112305
    },
    {
      "epoch": 0.6287262872628726,
      "grad_norm": 23.66663932800293,
      "learning_rate": 1e-05,
      "loss": 6.4259,
      "step": 11600
    },
    {
      "epoch": 0.6287262872628726,
      "step": 11600,
      "training_loss": 6.611574172973633
    },
    {
      "epoch": 0.628780487804878,
      "step": 11601,
      "training_loss": 7.8097028732299805
    },
    {
      "epoch": 0.6288346883468835,
      "step": 11602,
      "training_loss": 6.945863723754883
    },
    {
      "epoch": 0.6288888888888889,
      "step": 11603,
      "training_loss": 7.657835483551025
    },
    {
      "epoch": 0.6289430894308943,
      "grad_norm": 27.820571899414062,
      "learning_rate": 1e-05,
      "loss": 7.2562,
      "step": 11604
    },
    {
      "epoch": 0.6289430894308943,
      "step": 11604,
      "training_loss": 3.6481354236602783
    },
    {
      "epoch": 0.6289972899728997,
      "step": 11605,
      "training_loss": 6.85271692276001
    },
    {
      "epoch": 0.6290514905149052,
      "step": 11606,
      "training_loss": 6.524293422698975
    },
    {
      "epoch": 0.6291056910569106,
      "step": 11607,
      "training_loss": 7.013737201690674
    },
    {
      "epoch": 0.6291598915989159,
      "grad_norm": 43.887699127197266,
      "learning_rate": 1e-05,
      "loss": 6.0097,
      "step": 11608
    },
    {
      "epoch": 0.6291598915989159,
      "step": 11608,
      "training_loss": 6.72916841506958
    },
    {
      "epoch": 0.6292140921409214,
      "step": 11609,
      "training_loss": 6.311586380004883
    },
    {
      "epoch": 0.6292682926829268,
      "step": 11610,
      "training_loss": 6.676720142364502
    },
    {
      "epoch": 0.6293224932249323,
      "step": 11611,
      "training_loss": 6.554199695587158
    },
    {
      "epoch": 0.6293766937669377,
      "grad_norm": 18.239688873291016,
      "learning_rate": 1e-05,
      "loss": 6.5679,
      "step": 11612
    },
    {
      "epoch": 0.6293766937669377,
      "step": 11612,
      "training_loss": 4.410006523132324
    },
    {
      "epoch": 0.6294308943089431,
      "step": 11613,
      "training_loss": 6.81241512298584
    },
    {
      "epoch": 0.6294850948509485,
      "step": 11614,
      "training_loss": 7.148622989654541
    },
    {
      "epoch": 0.629539295392954,
      "step": 11615,
      "training_loss": 6.720794677734375
    },
    {
      "epoch": 0.6295934959349594,
      "grad_norm": 36.45005416870117,
      "learning_rate": 1e-05,
      "loss": 6.273,
      "step": 11616
    },
    {
      "epoch": 0.6295934959349594,
      "step": 11616,
      "training_loss": 5.949161529541016
    },
    {
      "epoch": 0.6296476964769647,
      "step": 11617,
      "training_loss": 7.372554779052734
    },
    {
      "epoch": 0.6297018970189702,
      "step": 11618,
      "training_loss": 5.681575775146484
    },
    {
      "epoch": 0.6297560975609756,
      "step": 11619,
      "training_loss": 6.336404323577881
    },
    {
      "epoch": 0.6298102981029811,
      "grad_norm": 24.481412887573242,
      "learning_rate": 1e-05,
      "loss": 6.3349,
      "step": 11620
    },
    {
      "epoch": 0.6298102981029811,
      "step": 11620,
      "training_loss": 6.894272804260254
    },
    {
      "epoch": 0.6298644986449865,
      "step": 11621,
      "training_loss": 5.140134811401367
    },
    {
      "epoch": 0.6299186991869918,
      "step": 11622,
      "training_loss": 5.383610725402832
    },
    {
      "epoch": 0.6299728997289973,
      "step": 11623,
      "training_loss": 6.823522567749023
    },
    {
      "epoch": 0.6300271002710027,
      "grad_norm": 42.01029586791992,
      "learning_rate": 1e-05,
      "loss": 6.0604,
      "step": 11624
    },
    {
      "epoch": 0.6300271002710027,
      "step": 11624,
      "training_loss": 5.790747165679932
    },
    {
      "epoch": 0.6300813008130082,
      "step": 11625,
      "training_loss": 6.917034149169922
    },
    {
      "epoch": 0.6301355013550135,
      "step": 11626,
      "training_loss": 6.89534330368042
    },
    {
      "epoch": 0.630189701897019,
      "step": 11627,
      "training_loss": 6.376233100891113
    },
    {
      "epoch": 0.6302439024390244,
      "grad_norm": 31.143367767333984,
      "learning_rate": 1e-05,
      "loss": 6.4948,
      "step": 11628
    },
    {
      "epoch": 0.6302439024390244,
      "step": 11628,
      "training_loss": 5.658337593078613
    },
    {
      "epoch": 0.6302981029810298,
      "step": 11629,
      "training_loss": 5.785068035125732
    },
    {
      "epoch": 0.6303523035230353,
      "step": 11630,
      "training_loss": 7.965154647827148
    },
    {
      "epoch": 0.6304065040650406,
      "step": 11631,
      "training_loss": 7.410191059112549
    },
    {
      "epoch": 0.6304607046070461,
      "grad_norm": 19.80173683166504,
      "learning_rate": 1e-05,
      "loss": 6.7047,
      "step": 11632
    },
    {
      "epoch": 0.6304607046070461,
      "step": 11632,
      "training_loss": 8.085923194885254
    },
    {
      "epoch": 0.6305149051490515,
      "step": 11633,
      "training_loss": 6.220264911651611
    },
    {
      "epoch": 0.630569105691057,
      "step": 11634,
      "training_loss": 3.5195350646972656
    },
    {
      "epoch": 0.6306233062330623,
      "step": 11635,
      "training_loss": 6.221568584442139
    },
    {
      "epoch": 0.6306775067750677,
      "grad_norm": 25.804710388183594,
      "learning_rate": 1e-05,
      "loss": 6.0118,
      "step": 11636
    },
    {
      "epoch": 0.6306775067750677,
      "step": 11636,
      "training_loss": 6.255904197692871
    },
    {
      "epoch": 0.6307317073170732,
      "step": 11637,
      "training_loss": 8.479000091552734
    },
    {
      "epoch": 0.6307859078590786,
      "step": 11638,
      "training_loss": 4.102428436279297
    },
    {
      "epoch": 0.6308401084010841,
      "step": 11639,
      "training_loss": 7.9386444091796875
    },
    {
      "epoch": 0.6308943089430894,
      "grad_norm": 26.386075973510742,
      "learning_rate": 1e-05,
      "loss": 6.694,
      "step": 11640
    },
    {
      "epoch": 0.6308943089430894,
      "step": 11640,
      "training_loss": 8.030597686767578
    },
    {
      "epoch": 0.6309485094850948,
      "step": 11641,
      "training_loss": 7.7212815284729
    },
    {
      "epoch": 0.6310027100271003,
      "step": 11642,
      "training_loss": 4.727000713348389
    },
    {
      "epoch": 0.6310569105691057,
      "step": 11643,
      "training_loss": 6.0104475021362305
    },
    {
      "epoch": 0.6311111111111111,
      "grad_norm": 19.268030166625977,
      "learning_rate": 1e-05,
      "loss": 6.6223,
      "step": 11644
    },
    {
      "epoch": 0.6311111111111111,
      "step": 11644,
      "training_loss": 7.127519130706787
    },
    {
      "epoch": 0.6311653116531165,
      "step": 11645,
      "training_loss": 6.405059814453125
    },
    {
      "epoch": 0.631219512195122,
      "step": 11646,
      "training_loss": 5.94141149520874
    },
    {
      "epoch": 0.6312737127371274,
      "step": 11647,
      "training_loss": 6.937768459320068
    },
    {
      "epoch": 0.6313279132791328,
      "grad_norm": 54.654483795166016,
      "learning_rate": 1e-05,
      "loss": 6.6029,
      "step": 11648
    },
    {
      "epoch": 0.6313279132791328,
      "step": 11648,
      "training_loss": 7.256430625915527
    },
    {
      "epoch": 0.6313821138211382,
      "step": 11649,
      "training_loss": 6.822581768035889
    },
    {
      "epoch": 0.6314363143631436,
      "step": 11650,
      "training_loss": 6.714725017547607
    },
    {
      "epoch": 0.6314905149051491,
      "step": 11651,
      "training_loss": 6.1743974685668945
    },
    {
      "epoch": 0.6315447154471545,
      "grad_norm": 29.773014068603516,
      "learning_rate": 1e-05,
      "loss": 6.742,
      "step": 11652
    },
    {
      "epoch": 0.6315447154471545,
      "step": 11652,
      "training_loss": 6.724294662475586
    },
    {
      "epoch": 0.6315989159891598,
      "step": 11653,
      "training_loss": 7.506894588470459
    },
    {
      "epoch": 0.6316531165311653,
      "step": 11654,
      "training_loss": 6.914860725402832
    },
    {
      "epoch": 0.6317073170731707,
      "step": 11655,
      "training_loss": 6.707242965698242
    },
    {
      "epoch": 0.6317615176151762,
      "grad_norm": 38.323368072509766,
      "learning_rate": 1e-05,
      "loss": 6.9633,
      "step": 11656
    },
    {
      "epoch": 0.6317615176151762,
      "step": 11656,
      "training_loss": 6.667076110839844
    },
    {
      "epoch": 0.6318157181571816,
      "step": 11657,
      "training_loss": 7.746479034423828
    },
    {
      "epoch": 0.631869918699187,
      "step": 11658,
      "training_loss": 7.974804401397705
    },
    {
      "epoch": 0.6319241192411924,
      "step": 11659,
      "training_loss": 6.819611072540283
    },
    {
      "epoch": 0.6319783197831979,
      "grad_norm": 16.144044876098633,
      "learning_rate": 1e-05,
      "loss": 7.302,
      "step": 11660
    },
    {
      "epoch": 0.6319783197831979,
      "step": 11660,
      "training_loss": 6.979854583740234
    },
    {
      "epoch": 0.6320325203252033,
      "step": 11661,
      "training_loss": 7.035589694976807
    },
    {
      "epoch": 0.6320867208672086,
      "step": 11662,
      "training_loss": 7.221772193908691
    },
    {
      "epoch": 0.6321409214092141,
      "step": 11663,
      "training_loss": 7.47969913482666
    },
    {
      "epoch": 0.6321951219512195,
      "grad_norm": 32.67722702026367,
      "learning_rate": 1e-05,
      "loss": 7.1792,
      "step": 11664
    },
    {
      "epoch": 0.6321951219512195,
      "step": 11664,
      "training_loss": 7.136134624481201
    },
    {
      "epoch": 0.632249322493225,
      "step": 11665,
      "training_loss": 7.7142229080200195
    },
    {
      "epoch": 0.6323035230352303,
      "step": 11666,
      "training_loss": 5.06520938873291
    },
    {
      "epoch": 0.6323577235772357,
      "step": 11667,
      "training_loss": 6.69002103805542
    },
    {
      "epoch": 0.6324119241192412,
      "grad_norm": 22.38074493408203,
      "learning_rate": 1e-05,
      "loss": 6.6514,
      "step": 11668
    },
    {
      "epoch": 0.6324119241192412,
      "step": 11668,
      "training_loss": 7.0084123611450195
    },
    {
      "epoch": 0.6324661246612466,
      "step": 11669,
      "training_loss": 6.734184265136719
    },
    {
      "epoch": 0.6325203252032521,
      "step": 11670,
      "training_loss": 7.029689788818359
    },
    {
      "epoch": 0.6325745257452574,
      "step": 11671,
      "training_loss": 6.659478187561035
    },
    {
      "epoch": 0.6326287262872629,
      "grad_norm": 25.739595413208008,
      "learning_rate": 1e-05,
      "loss": 6.8579,
      "step": 11672
    },
    {
      "epoch": 0.6326287262872629,
      "step": 11672,
      "training_loss": 6.569239616394043
    },
    {
      "epoch": 0.6326829268292683,
      "step": 11673,
      "training_loss": 8.39320182800293
    },
    {
      "epoch": 0.6327371273712737,
      "step": 11674,
      "training_loss": 7.990665912628174
    },
    {
      "epoch": 0.6327913279132791,
      "step": 11675,
      "training_loss": 6.517584323883057
    },
    {
      "epoch": 0.6328455284552845,
      "grad_norm": 45.860252380371094,
      "learning_rate": 1e-05,
      "loss": 7.3677,
      "step": 11676
    },
    {
      "epoch": 0.6328455284552845,
      "step": 11676,
      "training_loss": 6.648385524749756
    },
    {
      "epoch": 0.63289972899729,
      "step": 11677,
      "training_loss": 4.377110481262207
    },
    {
      "epoch": 0.6329539295392954,
      "step": 11678,
      "training_loss": 7.3983354568481445
    },
    {
      "epoch": 0.6330081300813009,
      "step": 11679,
      "training_loss": 5.613166809082031
    },
    {
      "epoch": 0.6330623306233062,
      "grad_norm": 20.875978469848633,
      "learning_rate": 1e-05,
      "loss": 6.0092,
      "step": 11680
    },
    {
      "epoch": 0.6330623306233062,
      "step": 11680,
      "training_loss": 4.522249698638916
    },
    {
      "epoch": 0.6331165311653116,
      "step": 11681,
      "training_loss": 7.305893421173096
    },
    {
      "epoch": 0.6331707317073171,
      "step": 11682,
      "training_loss": 5.9431023597717285
    },
    {
      "epoch": 0.6332249322493225,
      "step": 11683,
      "training_loss": 6.128513336181641
    },
    {
      "epoch": 0.6332791327913279,
      "grad_norm": 26.87824058532715,
      "learning_rate": 1e-05,
      "loss": 5.9749,
      "step": 11684
    },
    {
      "epoch": 0.6332791327913279,
      "step": 11684,
      "training_loss": 5.659332752227783
    },
    {
      "epoch": 0.6333333333333333,
      "step": 11685,
      "training_loss": 6.884258270263672
    },
    {
      "epoch": 0.6333875338753387,
      "step": 11686,
      "training_loss": 3.4895217418670654
    },
    {
      "epoch": 0.6334417344173442,
      "step": 11687,
      "training_loss": 4.940441608428955
    },
    {
      "epoch": 0.6334959349593496,
      "grad_norm": 42.662940979003906,
      "learning_rate": 1e-05,
      "loss": 5.2434,
      "step": 11688
    },
    {
      "epoch": 0.6334959349593496,
      "step": 11688,
      "training_loss": 7.430475234985352
    },
    {
      "epoch": 0.633550135501355,
      "step": 11689,
      "training_loss": 7.532782077789307
    },
    {
      "epoch": 0.6336043360433604,
      "step": 11690,
      "training_loss": 7.517321586608887
    },
    {
      "epoch": 0.6336585365853659,
      "step": 11691,
      "training_loss": 4.066052436828613
    },
    {
      "epoch": 0.6337127371273713,
      "grad_norm": 39.604698181152344,
      "learning_rate": 1e-05,
      "loss": 6.6367,
      "step": 11692
    },
    {
      "epoch": 0.6337127371273713,
      "step": 11692,
      "training_loss": 6.0279059410095215
    },
    {
      "epoch": 0.6337669376693766,
      "step": 11693,
      "training_loss": 3.7552337646484375
    },
    {
      "epoch": 0.6338211382113821,
      "step": 11694,
      "training_loss": 7.163933277130127
    },
    {
      "epoch": 0.6338753387533875,
      "step": 11695,
      "training_loss": 6.144022464752197
    },
    {
      "epoch": 0.633929539295393,
      "grad_norm": 22.08597755432129,
      "learning_rate": 1e-05,
      "loss": 5.7728,
      "step": 11696
    },
    {
      "epoch": 0.633929539295393,
      "step": 11696,
      "training_loss": 6.475762367248535
    },
    {
      "epoch": 0.6339837398373984,
      "step": 11697,
      "training_loss": 5.780971050262451
    },
    {
      "epoch": 0.6340379403794038,
      "step": 11698,
      "training_loss": 6.725395679473877
    },
    {
      "epoch": 0.6340921409214092,
      "step": 11699,
      "training_loss": 6.117815971374512
    },
    {
      "epoch": 0.6341463414634146,
      "grad_norm": 19.625146865844727,
      "learning_rate": 1e-05,
      "loss": 6.275,
      "step": 11700
    },
    {
      "epoch": 0.6341463414634146,
      "step": 11700,
      "training_loss": 6.444201469421387
    },
    {
      "epoch": 0.6342005420054201,
      "step": 11701,
      "training_loss": 6.693032264709473
    },
    {
      "epoch": 0.6342547425474254,
      "step": 11702,
      "training_loss": 7.175809860229492
    },
    {
      "epoch": 0.6343089430894309,
      "step": 11703,
      "training_loss": 6.874169826507568
    },
    {
      "epoch": 0.6343631436314363,
      "grad_norm": 15.825380325317383,
      "learning_rate": 1e-05,
      "loss": 6.7968,
      "step": 11704
    },
    {
      "epoch": 0.6343631436314363,
      "step": 11704,
      "training_loss": 6.741199970245361
    },
    {
      "epoch": 0.6344173441734418,
      "step": 11705,
      "training_loss": 6.2052435874938965
    },
    {
      "epoch": 0.6344715447154472,
      "step": 11706,
      "training_loss": 6.079782962799072
    },
    {
      "epoch": 0.6345257452574525,
      "step": 11707,
      "training_loss": 7.934900760650635
    },
    {
      "epoch": 0.634579945799458,
      "grad_norm": 23.901031494140625,
      "learning_rate": 1e-05,
      "loss": 6.7403,
      "step": 11708
    },
    {
      "epoch": 0.634579945799458,
      "step": 11708,
      "training_loss": 5.862795829772949
    },
    {
      "epoch": 0.6346341463414634,
      "step": 11709,
      "training_loss": 5.977489948272705
    },
    {
      "epoch": 0.6346883468834689,
      "step": 11710,
      "training_loss": 6.695567607879639
    },
    {
      "epoch": 0.6347425474254742,
      "step": 11711,
      "training_loss": 7.68466329574585
    },
    {
      "epoch": 0.6347967479674796,
      "grad_norm": 26.541675567626953,
      "learning_rate": 1e-05,
      "loss": 6.5551,
      "step": 11712
    },
    {
      "epoch": 0.6347967479674796,
      "step": 11712,
      "training_loss": 7.305777549743652
    },
    {
      "epoch": 0.6348509485094851,
      "step": 11713,
      "training_loss": 6.191231727600098
    },
    {
      "epoch": 0.6349051490514905,
      "step": 11714,
      "training_loss": 6.837710380554199
    },
    {
      "epoch": 0.634959349593496,
      "step": 11715,
      "training_loss": 6.954896450042725
    },
    {
      "epoch": 0.6350135501355013,
      "grad_norm": 27.34204864501953,
      "learning_rate": 1e-05,
      "loss": 6.8224,
      "step": 11716
    },
    {
      "epoch": 0.6350135501355013,
      "step": 11716,
      "training_loss": 6.949438571929932
    },
    {
      "epoch": 0.6350677506775068,
      "step": 11717,
      "training_loss": 6.889804363250732
    },
    {
      "epoch": 0.6351219512195122,
      "step": 11718,
      "training_loss": 6.775296211242676
    },
    {
      "epoch": 0.6351761517615176,
      "step": 11719,
      "training_loss": 7.026795864105225
    },
    {
      "epoch": 0.635230352303523,
      "grad_norm": 19.746185302734375,
      "learning_rate": 1e-05,
      "loss": 6.9103,
      "step": 11720
    },
    {
      "epoch": 0.635230352303523,
      "step": 11720,
      "training_loss": 6.710719585418701
    },
    {
      "epoch": 0.6352845528455284,
      "step": 11721,
      "training_loss": 6.804909706115723
    },
    {
      "epoch": 0.6353387533875339,
      "step": 11722,
      "training_loss": 5.590270519256592
    },
    {
      "epoch": 0.6353929539295393,
      "step": 11723,
      "training_loss": 5.760349273681641
    },
    {
      "epoch": 0.6354471544715448,
      "grad_norm": 24.40620231628418,
      "learning_rate": 1e-05,
      "loss": 6.2166,
      "step": 11724
    },
    {
      "epoch": 0.6354471544715448,
      "step": 11724,
      "training_loss": 7.572494983673096
    },
    {
      "epoch": 0.6355013550135501,
      "step": 11725,
      "training_loss": 5.940310001373291
    },
    {
      "epoch": 0.6355555555555555,
      "step": 11726,
      "training_loss": 6.008162021636963
    },
    {
      "epoch": 0.635609756097561,
      "step": 11727,
      "training_loss": 6.520827770233154
    },
    {
      "epoch": 0.6356639566395664,
      "grad_norm": 24.63962745666504,
      "learning_rate": 1e-05,
      "loss": 6.5104,
      "step": 11728
    },
    {
      "epoch": 0.6356639566395664,
      "step": 11728,
      "training_loss": 7.429903507232666
    },
    {
      "epoch": 0.6357181571815718,
      "step": 11729,
      "training_loss": 5.872063159942627
    },
    {
      "epoch": 0.6357723577235772,
      "step": 11730,
      "training_loss": 4.338019847869873
    },
    {
      "epoch": 0.6358265582655827,
      "step": 11731,
      "training_loss": 3.4421839714050293
    },
    {
      "epoch": 0.6358807588075881,
      "grad_norm": 25.453720092773438,
      "learning_rate": 1e-05,
      "loss": 5.2705,
      "step": 11732
    },
    {
      "epoch": 0.6358807588075881,
      "step": 11732,
      "training_loss": 5.598824977874756
    },
    {
      "epoch": 0.6359349593495935,
      "step": 11733,
      "training_loss": 7.044096946716309
    },
    {
      "epoch": 0.6359891598915989,
      "step": 11734,
      "training_loss": 7.043619155883789
    },
    {
      "epoch": 0.6360433604336043,
      "step": 11735,
      "training_loss": 6.81726598739624
    },
    {
      "epoch": 0.6360975609756098,
      "grad_norm": 44.56647872924805,
      "learning_rate": 1e-05,
      "loss": 6.626,
      "step": 11736
    },
    {
      "epoch": 0.6360975609756098,
      "step": 11736,
      "training_loss": 7.236033916473389
    },
    {
      "epoch": 0.6361517615176152,
      "step": 11737,
      "training_loss": 7.205228328704834
    },
    {
      "epoch": 0.6362059620596205,
      "step": 11738,
      "training_loss": 4.139251232147217
    },
    {
      "epoch": 0.636260162601626,
      "step": 11739,
      "training_loss": 6.156902313232422
    },
    {
      "epoch": 0.6363143631436314,
      "grad_norm": 22.999982833862305,
      "learning_rate": 1e-05,
      "loss": 6.1844,
      "step": 11740
    },
    {
      "epoch": 0.6363143631436314,
      "step": 11740,
      "training_loss": 5.819483280181885
    },
    {
      "epoch": 0.6363685636856369,
      "step": 11741,
      "training_loss": 4.239505767822266
    },
    {
      "epoch": 0.6364227642276423,
      "step": 11742,
      "training_loss": 7.3598952293396
    },
    {
      "epoch": 0.6364769647696477,
      "step": 11743,
      "training_loss": 7.417023658752441
    },
    {
      "epoch": 0.6365311653116531,
      "grad_norm": 17.71719741821289,
      "learning_rate": 1e-05,
      "loss": 6.209,
      "step": 11744
    },
    {
      "epoch": 0.6365311653116531,
      "step": 11744,
      "training_loss": 6.419147968292236
    },
    {
      "epoch": 0.6365853658536585,
      "step": 11745,
      "training_loss": 6.946335792541504
    },
    {
      "epoch": 0.636639566395664,
      "step": 11746,
      "training_loss": 7.785714149475098
    },
    {
      "epoch": 0.6366937669376693,
      "step": 11747,
      "training_loss": 7.373483180999756
    },
    {
      "epoch": 0.6367479674796748,
      "grad_norm": 26.279903411865234,
      "learning_rate": 1e-05,
      "loss": 7.1312,
      "step": 11748
    },
    {
      "epoch": 0.6367479674796748,
      "step": 11748,
      "training_loss": 7.18766450881958
    },
    {
      "epoch": 0.6368021680216802,
      "step": 11749,
      "training_loss": 5.495625972747803
    },
    {
      "epoch": 0.6368563685636857,
      "step": 11750,
      "training_loss": 7.119259834289551
    },
    {
      "epoch": 0.6369105691056911,
      "step": 11751,
      "training_loss": 7.361983299255371
    },
    {
      "epoch": 0.6369647696476964,
      "grad_norm": 25.906429290771484,
      "learning_rate": 1e-05,
      "loss": 6.7911,
      "step": 11752
    },
    {
      "epoch": 0.6369647696476964,
      "step": 11752,
      "training_loss": 7.486239433288574
    },
    {
      "epoch": 0.6370189701897019,
      "step": 11753,
      "training_loss": 5.895930767059326
    },
    {
      "epoch": 0.6370731707317073,
      "step": 11754,
      "training_loss": 7.850953578948975
    },
    {
      "epoch": 0.6371273712737128,
      "step": 11755,
      "training_loss": 7.484862327575684
    },
    {
      "epoch": 0.6371815718157181,
      "grad_norm": 42.48828887939453,
      "learning_rate": 1e-05,
      "loss": 7.1795,
      "step": 11756
    },
    {
      "epoch": 0.6371815718157181,
      "step": 11756,
      "training_loss": 6.9748992919921875
    },
    {
      "epoch": 0.6372357723577236,
      "step": 11757,
      "training_loss": 6.988945007324219
    },
    {
      "epoch": 0.637289972899729,
      "step": 11758,
      "training_loss": 6.914846897125244
    },
    {
      "epoch": 0.6373441734417344,
      "step": 11759,
      "training_loss": 8.259796142578125
    },
    {
      "epoch": 0.6373983739837399,
      "grad_norm": 35.92946243286133,
      "learning_rate": 1e-05,
      "loss": 7.2846,
      "step": 11760
    },
    {
      "epoch": 0.6373983739837399,
      "step": 11760,
      "training_loss": 4.874603748321533
    },
    {
      "epoch": 0.6374525745257452,
      "step": 11761,
      "training_loss": 7.001924991607666
    },
    {
      "epoch": 0.6375067750677507,
      "step": 11762,
      "training_loss": 6.921316623687744
    },
    {
      "epoch": 0.6375609756097561,
      "step": 11763,
      "training_loss": 7.5758562088012695
    },
    {
      "epoch": 0.6376151761517616,
      "grad_norm": 21.15620231628418,
      "learning_rate": 1e-05,
      "loss": 6.5934,
      "step": 11764
    },
    {
      "epoch": 0.6376151761517616,
      "step": 11764,
      "training_loss": 6.060793876647949
    },
    {
      "epoch": 0.6376693766937669,
      "step": 11765,
      "training_loss": 7.04704475402832
    },
    {
      "epoch": 0.6377235772357723,
      "step": 11766,
      "training_loss": 6.634149551391602
    },
    {
      "epoch": 0.6377777777777778,
      "step": 11767,
      "training_loss": 5.65764856338501
    },
    {
      "epoch": 0.6378319783197832,
      "grad_norm": 66.58346557617188,
      "learning_rate": 1e-05,
      "loss": 6.3499,
      "step": 11768
    },
    {
      "epoch": 0.6378319783197832,
      "step": 11768,
      "training_loss": 5.884496212005615
    },
    {
      "epoch": 0.6378861788617887,
      "step": 11769,
      "training_loss": 7.3259477615356445
    },
    {
      "epoch": 0.637940379403794,
      "step": 11770,
      "training_loss": 7.619277000427246
    },
    {
      "epoch": 0.6379945799457994,
      "step": 11771,
      "training_loss": 5.663491249084473
    },
    {
      "epoch": 0.6380487804878049,
      "grad_norm": 40.1063117980957,
      "learning_rate": 1e-05,
      "loss": 6.6233,
      "step": 11772
    },
    {
      "epoch": 0.6380487804878049,
      "step": 11772,
      "training_loss": 7.4705095291137695
    },
    {
      "epoch": 0.6381029810298103,
      "step": 11773,
      "training_loss": 6.099836349487305
    },
    {
      "epoch": 0.6381571815718157,
      "step": 11774,
      "training_loss": 4.1025848388671875
    },
    {
      "epoch": 0.6382113821138211,
      "step": 11775,
      "training_loss": 8.479107856750488
    },
    {
      "epoch": 0.6382655826558266,
      "grad_norm": 46.72176742553711,
      "learning_rate": 1e-05,
      "loss": 6.538,
      "step": 11776
    },
    {
      "epoch": 0.6382655826558266,
      "step": 11776,
      "training_loss": 7.6117939949035645
    },
    {
      "epoch": 0.638319783197832,
      "step": 11777,
      "training_loss": 6.635526180267334
    },
    {
      "epoch": 0.6383739837398374,
      "step": 11778,
      "training_loss": 5.817753314971924
    },
    {
      "epoch": 0.6384281842818428,
      "step": 11779,
      "training_loss": 6.167905330657959
    },
    {
      "epoch": 0.6384823848238482,
      "grad_norm": 31.73203468322754,
      "learning_rate": 1e-05,
      "loss": 6.5582,
      "step": 11780
    },
    {
      "epoch": 0.6384823848238482,
      "step": 11780,
      "training_loss": 6.953126430511475
    },
    {
      "epoch": 0.6385365853658537,
      "step": 11781,
      "training_loss": 5.373810768127441
    },
    {
      "epoch": 0.6385907859078591,
      "step": 11782,
      "training_loss": 8.17015552520752
    },
    {
      "epoch": 0.6386449864498644,
      "step": 11783,
      "training_loss": 6.689027786254883
    },
    {
      "epoch": 0.6386991869918699,
      "grad_norm": 22.225540161132812,
      "learning_rate": 1e-05,
      "loss": 6.7965,
      "step": 11784
    },
    {
      "epoch": 0.6386991869918699,
      "step": 11784,
      "training_loss": 7.75863790512085
    },
    {
      "epoch": 0.6387533875338753,
      "step": 11785,
      "training_loss": 7.970412254333496
    },
    {
      "epoch": 0.6388075880758808,
      "step": 11786,
      "training_loss": 8.168437957763672
    },
    {
      "epoch": 0.6388617886178862,
      "step": 11787,
      "training_loss": 5.54536771774292
    },
    {
      "epoch": 0.6389159891598916,
      "grad_norm": 26.10710334777832,
      "learning_rate": 1e-05,
      "loss": 7.3607,
      "step": 11788
    },
    {
      "epoch": 0.6389159891598916,
      "step": 11788,
      "training_loss": 7.753032207489014
    },
    {
      "epoch": 0.638970189701897,
      "step": 11789,
      "training_loss": 7.159919261932373
    },
    {
      "epoch": 0.6390243902439025,
      "step": 11790,
      "training_loss": 6.742947101593018
    },
    {
      "epoch": 0.6390785907859079,
      "step": 11791,
      "training_loss": 7.732846736907959
    },
    {
      "epoch": 0.6391327913279132,
      "grad_norm": 28.897537231445312,
      "learning_rate": 1e-05,
      "loss": 7.3472,
      "step": 11792
    },
    {
      "epoch": 0.6391327913279132,
      "step": 11792,
      "training_loss": 7.201237201690674
    },
    {
      "epoch": 0.6391869918699187,
      "step": 11793,
      "training_loss": 8.196982383728027
    },
    {
      "epoch": 0.6392411924119241,
      "step": 11794,
      "training_loss": 5.97195291519165
    },
    {
      "epoch": 0.6392953929539296,
      "step": 11795,
      "training_loss": 5.831193923950195
    },
    {
      "epoch": 0.639349593495935,
      "grad_norm": 81.0438461303711,
      "learning_rate": 1e-05,
      "loss": 6.8003,
      "step": 11796
    },
    {
      "epoch": 0.639349593495935,
      "step": 11796,
      "training_loss": 5.963351249694824
    },
    {
      "epoch": 0.6394037940379403,
      "step": 11797,
      "training_loss": 6.524927616119385
    },
    {
      "epoch": 0.6394579945799458,
      "step": 11798,
      "training_loss": 7.802496910095215
    },
    {
      "epoch": 0.6395121951219512,
      "step": 11799,
      "training_loss": 6.944415092468262
    },
    {
      "epoch": 0.6395663956639567,
      "grad_norm": 16.417818069458008,
      "learning_rate": 1e-05,
      "loss": 6.8088,
      "step": 11800
    },
    {
      "epoch": 0.6395663956639567,
      "step": 11800,
      "training_loss": 6.963167667388916
    },
    {
      "epoch": 0.639620596205962,
      "step": 11801,
      "training_loss": 6.6446733474731445
    },
    {
      "epoch": 0.6396747967479675,
      "step": 11802,
      "training_loss": 5.663712978363037
    },
    {
      "epoch": 0.6397289972899729,
      "step": 11803,
      "training_loss": 5.549039363861084
    },
    {
      "epoch": 0.6397831978319783,
      "grad_norm": 33.5757942199707,
      "learning_rate": 1e-05,
      "loss": 6.2051,
      "step": 11804
    },
    {
      "epoch": 0.6397831978319783,
      "step": 11804,
      "training_loss": 6.667770862579346
    },
    {
      "epoch": 0.6398373983739838,
      "step": 11805,
      "training_loss": 7.990084648132324
    },
    {
      "epoch": 0.6398915989159891,
      "step": 11806,
      "training_loss": 6.831103324890137
    },
    {
      "epoch": 0.6399457994579946,
      "step": 11807,
      "training_loss": 7.380618572235107
    },
    {
      "epoch": 0.64,
      "grad_norm": 30.96148109436035,
      "learning_rate": 1e-05,
      "loss": 7.2174,
      "step": 11808
    },
    {
      "epoch": 0.64,
      "step": 11808,
      "training_loss": 6.660581111907959
    },
    {
      "epoch": 0.6400542005420055,
      "step": 11809,
      "training_loss": 7.011509895324707
    },
    {
      "epoch": 0.6401084010840108,
      "step": 11810,
      "training_loss": 6.695528507232666
    },
    {
      "epoch": 0.6401626016260162,
      "step": 11811,
      "training_loss": 6.533024311065674
    },
    {
      "epoch": 0.6402168021680217,
      "grad_norm": 33.58344650268555,
      "learning_rate": 1e-05,
      "loss": 6.7252,
      "step": 11812
    },
    {
      "epoch": 0.6402168021680217,
      "step": 11812,
      "training_loss": 5.5199432373046875
    },
    {
      "epoch": 0.6402710027100271,
      "step": 11813,
      "training_loss": 7.830564498901367
    },
    {
      "epoch": 0.6403252032520326,
      "step": 11814,
      "training_loss": 7.0081987380981445
    },
    {
      "epoch": 0.6403794037940379,
      "step": 11815,
      "training_loss": 6.179981708526611
    },
    {
      "epoch": 0.6404336043360433,
      "grad_norm": 22.96920394897461,
      "learning_rate": 1e-05,
      "loss": 6.6347,
      "step": 11816
    },
    {
      "epoch": 0.6404336043360433,
      "step": 11816,
      "training_loss": 6.373022556304932
    },
    {
      "epoch": 0.6404878048780488,
      "step": 11817,
      "training_loss": 7.956639766693115
    },
    {
      "epoch": 0.6405420054200542,
      "step": 11818,
      "training_loss": 6.732541561126709
    },
    {
      "epoch": 0.6405962059620596,
      "step": 11819,
      "training_loss": 8.358450889587402
    },
    {
      "epoch": 0.640650406504065,
      "grad_norm": 26.891782760620117,
      "learning_rate": 1e-05,
      "loss": 7.3552,
      "step": 11820
    },
    {
      "epoch": 0.640650406504065,
      "step": 11820,
      "training_loss": 6.82148551940918
    },
    {
      "epoch": 0.6407046070460705,
      "step": 11821,
      "training_loss": 7.12058162689209
    },
    {
      "epoch": 0.6407588075880759,
      "step": 11822,
      "training_loss": 5.983980655670166
    },
    {
      "epoch": 0.6408130081300814,
      "step": 11823,
      "training_loss": 4.4183759689331055
    },
    {
      "epoch": 0.6408672086720867,
      "grad_norm": 27.60979652404785,
      "learning_rate": 1e-05,
      "loss": 6.0861,
      "step": 11824
    },
    {
      "epoch": 0.6408672086720867,
      "step": 11824,
      "training_loss": 5.476255893707275
    },
    {
      "epoch": 0.6409214092140921,
      "step": 11825,
      "training_loss": 6.912441253662109
    },
    {
      "epoch": 0.6409756097560976,
      "step": 11826,
      "training_loss": 6.2231926918029785
    },
    {
      "epoch": 0.641029810298103,
      "step": 11827,
      "training_loss": 6.241001129150391
    },
    {
      "epoch": 0.6410840108401084,
      "grad_norm": 25.337453842163086,
      "learning_rate": 1e-05,
      "loss": 6.2132,
      "step": 11828
    },
    {
      "epoch": 0.6410840108401084,
      "step": 11828,
      "training_loss": 6.6960768699646
    },
    {
      "epoch": 0.6411382113821138,
      "step": 11829,
      "training_loss": 6.58093786239624
    },
    {
      "epoch": 0.6411924119241192,
      "step": 11830,
      "training_loss": 6.708583831787109
    },
    {
      "epoch": 0.6412466124661247,
      "step": 11831,
      "training_loss": 5.729235649108887
    },
    {
      "epoch": 0.6413008130081301,
      "grad_norm": 27.689130783081055,
      "learning_rate": 1e-05,
      "loss": 6.4287,
      "step": 11832
    },
    {
      "epoch": 0.6413008130081301,
      "step": 11832,
      "training_loss": 7.5200042724609375
    },
    {
      "epoch": 0.6413550135501355,
      "step": 11833,
      "training_loss": 6.380545139312744
    },
    {
      "epoch": 0.6414092140921409,
      "step": 11834,
      "training_loss": 5.71992301940918
    },
    {
      "epoch": 0.6414634146341464,
      "step": 11835,
      "training_loss": 6.835394382476807
    },
    {
      "epoch": 0.6415176151761518,
      "grad_norm": 22.56804084777832,
      "learning_rate": 1e-05,
      "loss": 6.614,
      "step": 11836
    },
    {
      "epoch": 0.6415176151761518,
      "step": 11836,
      "training_loss": 7.745523452758789
    },
    {
      "epoch": 0.6415718157181571,
      "step": 11837,
      "training_loss": 3.949768543243408
    },
    {
      "epoch": 0.6416260162601626,
      "step": 11838,
      "training_loss": 7.293341159820557
    },
    {
      "epoch": 0.641680216802168,
      "step": 11839,
      "training_loss": 3.02119779586792
    },
    {
      "epoch": 0.6417344173441735,
      "grad_norm": 35.28905487060547,
      "learning_rate": 1e-05,
      "loss": 5.5025,
      "step": 11840
    },
    {
      "epoch": 0.6417344173441735,
      "step": 11840,
      "training_loss": 4.934745788574219
    },
    {
      "epoch": 0.6417886178861789,
      "step": 11841,
      "training_loss": 6.059588432312012
    },
    {
      "epoch": 0.6418428184281842,
      "step": 11842,
      "training_loss": 6.838918209075928
    },
    {
      "epoch": 0.6418970189701897,
      "step": 11843,
      "training_loss": 5.856301784515381
    },
    {
      "epoch": 0.6419512195121951,
      "grad_norm": 29.937002182006836,
      "learning_rate": 1e-05,
      "loss": 5.9224,
      "step": 11844
    },
    {
      "epoch": 0.6419512195121951,
      "step": 11844,
      "training_loss": 8.127507209777832
    },
    {
      "epoch": 0.6420054200542006,
      "step": 11845,
      "training_loss": 7.262648582458496
    },
    {
      "epoch": 0.6420596205962059,
      "step": 11846,
      "training_loss": 6.526034832000732
    },
    {
      "epoch": 0.6421138211382114,
      "step": 11847,
      "training_loss": 6.471045017242432
    },
    {
      "epoch": 0.6421680216802168,
      "grad_norm": 23.569730758666992,
      "learning_rate": 1e-05,
      "loss": 7.0968,
      "step": 11848
    },
    {
      "epoch": 0.6421680216802168,
      "step": 11848,
      "training_loss": 7.302966594696045
    },
    {
      "epoch": 0.6422222222222222,
      "step": 11849,
      "training_loss": 6.8825249671936035
    },
    {
      "epoch": 0.6422764227642277,
      "step": 11850,
      "training_loss": 3.445009469985962
    },
    {
      "epoch": 0.642330623306233,
      "step": 11851,
      "training_loss": 6.747885227203369
    },
    {
      "epoch": 0.6423848238482385,
      "grad_norm": 16.361467361450195,
      "learning_rate": 1e-05,
      "loss": 6.0946,
      "step": 11852
    },
    {
      "epoch": 0.6423848238482385,
      "step": 11852,
      "training_loss": 5.405698776245117
    },
    {
      "epoch": 0.6424390243902439,
      "step": 11853,
      "training_loss": 7.781442642211914
    },
    {
      "epoch": 0.6424932249322494,
      "step": 11854,
      "training_loss": 5.882194995880127
    },
    {
      "epoch": 0.6425474254742547,
      "step": 11855,
      "training_loss": 7.070438861846924
    },
    {
      "epoch": 0.6426016260162601,
      "grad_norm": 26.05781364440918,
      "learning_rate": 1e-05,
      "loss": 6.5349,
      "step": 11856
    },
    {
      "epoch": 0.6426016260162601,
      "step": 11856,
      "training_loss": 6.473283290863037
    },
    {
      "epoch": 0.6426558265582656,
      "step": 11857,
      "training_loss": 6.262012958526611
    },
    {
      "epoch": 0.642710027100271,
      "step": 11858,
      "training_loss": 5.919254779815674
    },
    {
      "epoch": 0.6427642276422765,
      "step": 11859,
      "training_loss": 7.330028533935547
    },
    {
      "epoch": 0.6428184281842818,
      "grad_norm": 26.813093185424805,
      "learning_rate": 1e-05,
      "loss": 6.4961,
      "step": 11860
    },
    {
      "epoch": 0.6428184281842818,
      "step": 11860,
      "training_loss": 6.538963794708252
    },
    {
      "epoch": 0.6428726287262873,
      "step": 11861,
      "training_loss": 7.152466297149658
    },
    {
      "epoch": 0.6429268292682927,
      "step": 11862,
      "training_loss": 5.21635627746582
    },
    {
      "epoch": 0.6429810298102981,
      "step": 11863,
      "training_loss": 6.97518253326416
    },
    {
      "epoch": 0.6430352303523035,
      "grad_norm": 39.43296432495117,
      "learning_rate": 1e-05,
      "loss": 6.4707,
      "step": 11864
    },
    {
      "epoch": 0.6430352303523035,
      "step": 11864,
      "training_loss": 6.67764949798584
    },
    {
      "epoch": 0.6430894308943089,
      "step": 11865,
      "training_loss": 6.962160587310791
    },
    {
      "epoch": 0.6431436314363144,
      "step": 11866,
      "training_loss": 6.39415168762207
    },
    {
      "epoch": 0.6431978319783198,
      "step": 11867,
      "training_loss": 3.1356611251831055
    },
    {
      "epoch": 0.6432520325203253,
      "grad_norm": 67.6036148071289,
      "learning_rate": 1e-05,
      "loss": 5.7924,
      "step": 11868
    },
    {
      "epoch": 0.6432520325203253,
      "step": 11868,
      "training_loss": 6.069421291351318
    },
    {
      "epoch": 0.6433062330623306,
      "step": 11869,
      "training_loss": 4.696389675140381
    },
    {
      "epoch": 0.643360433604336,
      "step": 11870,
      "training_loss": 4.216884613037109
    },
    {
      "epoch": 0.6434146341463415,
      "step": 11871,
      "training_loss": 7.614371299743652
    },
    {
      "epoch": 0.6434688346883469,
      "grad_norm": 30.48450469970703,
      "learning_rate": 1e-05,
      "loss": 5.6493,
      "step": 11872
    },
    {
      "epoch": 0.6434688346883469,
      "step": 11872,
      "training_loss": 6.382742881774902
    },
    {
      "epoch": 0.6435230352303523,
      "step": 11873,
      "training_loss": 4.572136878967285
    },
    {
      "epoch": 0.6435772357723577,
      "step": 11874,
      "training_loss": 6.749756813049316
    },
    {
      "epoch": 0.6436314363143631,
      "step": 11875,
      "training_loss": 6.727668762207031
    },
    {
      "epoch": 0.6436856368563686,
      "grad_norm": 19.412260055541992,
      "learning_rate": 1e-05,
      "loss": 6.1081,
      "step": 11876
    },
    {
      "epoch": 0.6436856368563686,
      "step": 11876,
      "training_loss": 6.474536895751953
    },
    {
      "epoch": 0.643739837398374,
      "step": 11877,
      "training_loss": 8.762940406799316
    },
    {
      "epoch": 0.6437940379403794,
      "step": 11878,
      "training_loss": 4.966619968414307
    },
    {
      "epoch": 0.6438482384823848,
      "step": 11879,
      "training_loss": 5.80113410949707
    },
    {
      "epoch": 0.6439024390243903,
      "grad_norm": 50.98820877075195,
      "learning_rate": 1e-05,
      "loss": 6.5013,
      "step": 11880
    },
    {
      "epoch": 0.6439024390243903,
      "step": 11880,
      "training_loss": 2.641249179840088
    },
    {
      "epoch": 0.6439566395663957,
      "step": 11881,
      "training_loss": 6.995940208435059
    },
    {
      "epoch": 0.644010840108401,
      "step": 11882,
      "training_loss": 6.628015041351318
    },
    {
      "epoch": 0.6440650406504065,
      "step": 11883,
      "training_loss": 5.377385139465332
    },
    {
      "epoch": 0.6441192411924119,
      "grad_norm": 45.76964569091797,
      "learning_rate": 1e-05,
      "loss": 5.4106,
      "step": 11884
    },
    {
      "epoch": 0.6441192411924119,
      "step": 11884,
      "training_loss": 7.308255672454834
    },
    {
      "epoch": 0.6441734417344174,
      "step": 11885,
      "training_loss": 6.799653053283691
    },
    {
      "epoch": 0.6442276422764228,
      "step": 11886,
      "training_loss": 4.042882919311523
    },
    {
      "epoch": 0.6442818428184282,
      "step": 11887,
      "training_loss": 6.648024559020996
    },
    {
      "epoch": 0.6443360433604336,
      "grad_norm": 22.467308044433594,
      "learning_rate": 1e-05,
      "loss": 6.1997,
      "step": 11888
    },
    {
      "epoch": 0.6443360433604336,
      "step": 11888,
      "training_loss": 6.91936731338501
    },
    {
      "epoch": 0.644390243902439,
      "step": 11889,
      "training_loss": 5.929586887359619
    },
    {
      "epoch": 0.6444444444444445,
      "step": 11890,
      "training_loss": 5.611338138580322
    },
    {
      "epoch": 0.6444986449864498,
      "step": 11891,
      "training_loss": 8.371760368347168
    },
    {
      "epoch": 0.6445528455284553,
      "grad_norm": 87.06768035888672,
      "learning_rate": 1e-05,
      "loss": 6.708,
      "step": 11892
    },
    {
      "epoch": 0.6445528455284553,
      "step": 11892,
      "training_loss": 7.430912971496582
    },
    {
      "epoch": 0.6446070460704607,
      "step": 11893,
      "training_loss": 5.674572467803955
    },
    {
      "epoch": 0.6446612466124662,
      "step": 11894,
      "training_loss": 7.040356636047363
    },
    {
      "epoch": 0.6447154471544716,
      "step": 11895,
      "training_loss": 2.7456724643707275
    },
    {
      "epoch": 0.6447696476964769,
      "grad_norm": 27.834793090820312,
      "learning_rate": 1e-05,
      "loss": 5.7229,
      "step": 11896
    },
    {
      "epoch": 0.6447696476964769,
      "step": 11896,
      "training_loss": 6.912219047546387
    },
    {
      "epoch": 0.6448238482384824,
      "step": 11897,
      "training_loss": 8.134971618652344
    },
    {
      "epoch": 0.6448780487804878,
      "step": 11898,
      "training_loss": 7.639001369476318
    },
    {
      "epoch": 0.6449322493224933,
      "step": 11899,
      "training_loss": 6.857874870300293
    },
    {
      "epoch": 0.6449864498644986,
      "grad_norm": 31.655555725097656,
      "learning_rate": 1e-05,
      "loss": 7.386,
      "step": 11900
    },
    {
      "epoch": 0.6449864498644986,
      "step": 11900,
      "training_loss": 5.70486307144165
    },
    {
      "epoch": 0.645040650406504,
      "step": 11901,
      "training_loss": 7.902177333831787
    },
    {
      "epoch": 0.6450948509485095,
      "step": 11902,
      "training_loss": 7.049992561340332
    },
    {
      "epoch": 0.6451490514905149,
      "step": 11903,
      "training_loss": 6.520066738128662
    },
    {
      "epoch": 0.6452032520325204,
      "grad_norm": 28.344585418701172,
      "learning_rate": 1e-05,
      "loss": 6.7943,
      "step": 11904
    },
    {
      "epoch": 0.6452032520325204,
      "step": 11904,
      "training_loss": 7.096060752868652
    },
    {
      "epoch": 0.6452574525745257,
      "step": 11905,
      "training_loss": 6.620462417602539
    },
    {
      "epoch": 0.6453116531165312,
      "step": 11906,
      "training_loss": 6.0918965339660645
    },
    {
      "epoch": 0.6453658536585366,
      "step": 11907,
      "training_loss": 5.817670822143555
    },
    {
      "epoch": 0.645420054200542,
      "grad_norm": 33.12303161621094,
      "learning_rate": 1e-05,
      "loss": 6.4065,
      "step": 11908
    },
    {
      "epoch": 0.645420054200542,
      "step": 11908,
      "training_loss": 7.314813137054443
    },
    {
      "epoch": 0.6454742547425474,
      "step": 11909,
      "training_loss": 7.436476230621338
    },
    {
      "epoch": 0.6455284552845528,
      "step": 11910,
      "training_loss": 6.081169128417969
    },
    {
      "epoch": 0.6455826558265583,
      "step": 11911,
      "training_loss": 6.136452674865723
    },
    {
      "epoch": 0.6456368563685637,
      "grad_norm": 33.9500617980957,
      "learning_rate": 1e-05,
      "loss": 6.7422,
      "step": 11912
    },
    {
      "epoch": 0.6456368563685637,
      "step": 11912,
      "training_loss": 3.009551763534546
    },
    {
      "epoch": 0.6456910569105692,
      "step": 11913,
      "training_loss": 5.5132222175598145
    },
    {
      "epoch": 0.6457452574525745,
      "step": 11914,
      "training_loss": 7.98861026763916
    },
    {
      "epoch": 0.6457994579945799,
      "step": 11915,
      "training_loss": 4.910616397857666
    },
    {
      "epoch": 0.6458536585365854,
      "grad_norm": 37.1708869934082,
      "learning_rate": 1e-05,
      "loss": 5.3555,
      "step": 11916
    },
    {
      "epoch": 0.6458536585365854,
      "step": 11916,
      "training_loss": 8.322675704956055
    },
    {
      "epoch": 0.6459078590785908,
      "step": 11917,
      "training_loss": 5.943985939025879
    },
    {
      "epoch": 0.6459620596205962,
      "step": 11918,
      "training_loss": 6.725347995758057
    },
    {
      "epoch": 0.6460162601626016,
      "step": 11919,
      "training_loss": 7.139859199523926
    },
    {
      "epoch": 0.646070460704607,
      "grad_norm": 37.475276947021484,
      "learning_rate": 1e-05,
      "loss": 7.033,
      "step": 11920
    },
    {
      "epoch": 0.646070460704607,
      "step": 11920,
      "training_loss": 5.0640549659729
    },
    {
      "epoch": 0.6461246612466125,
      "step": 11921,
      "training_loss": 6.191675186157227
    },
    {
      "epoch": 0.6461788617886178,
      "step": 11922,
      "training_loss": 6.214646339416504
    },
    {
      "epoch": 0.6462330623306233,
      "step": 11923,
      "training_loss": 5.983818531036377
    },
    {
      "epoch": 0.6462872628726287,
      "grad_norm": 21.875823974609375,
      "learning_rate": 1e-05,
      "loss": 5.8635,
      "step": 11924
    },
    {
      "epoch": 0.6462872628726287,
      "step": 11924,
      "training_loss": 6.764224529266357
    },
    {
      "epoch": 0.6463414634146342,
      "step": 11925,
      "training_loss": 5.673083782196045
    },
    {
      "epoch": 0.6463956639566396,
      "step": 11926,
      "training_loss": 5.05077600479126
    },
    {
      "epoch": 0.6464498644986449,
      "step": 11927,
      "training_loss": 7.808804988861084
    },
    {
      "epoch": 0.6465040650406504,
      "grad_norm": 18.78481101989746,
      "learning_rate": 1e-05,
      "loss": 6.3242,
      "step": 11928
    },
    {
      "epoch": 0.6465040650406504,
      "step": 11928,
      "training_loss": 6.325571537017822
    },
    {
      "epoch": 0.6465582655826558,
      "step": 11929,
      "training_loss": 6.882275104522705
    },
    {
      "epoch": 0.6466124661246613,
      "step": 11930,
      "training_loss": 6.62170934677124
    },
    {
      "epoch": 0.6466666666666666,
      "step": 11931,
      "training_loss": 4.186068058013916
    },
    {
      "epoch": 0.6467208672086721,
      "grad_norm": 26.583127975463867,
      "learning_rate": 1e-05,
      "loss": 6.0039,
      "step": 11932
    },
    {
      "epoch": 0.6467208672086721,
      "step": 11932,
      "training_loss": 6.612649917602539
    },
    {
      "epoch": 0.6467750677506775,
      "step": 11933,
      "training_loss": 6.041675567626953
    },
    {
      "epoch": 0.646829268292683,
      "step": 11934,
      "training_loss": 7.036031246185303
    },
    {
      "epoch": 0.6468834688346884,
      "step": 11935,
      "training_loss": 6.56500768661499
    },
    {
      "epoch": 0.6469376693766937,
      "grad_norm": 25.519733428955078,
      "learning_rate": 1e-05,
      "loss": 6.5638,
      "step": 11936
    },
    {
      "epoch": 0.6469376693766937,
      "step": 11936,
      "training_loss": 7.578536033630371
    },
    {
      "epoch": 0.6469918699186992,
      "step": 11937,
      "training_loss": 7.2944207191467285
    },
    {
      "epoch": 0.6470460704607046,
      "step": 11938,
      "training_loss": 6.79395055770874
    },
    {
      "epoch": 0.6471002710027101,
      "step": 11939,
      "training_loss": 7.038455963134766
    },
    {
      "epoch": 0.6471544715447154,
      "grad_norm": 21.195419311523438,
      "learning_rate": 1e-05,
      "loss": 7.1763,
      "step": 11940
    },
    {
      "epoch": 0.6471544715447154,
      "step": 11940,
      "training_loss": 5.6197075843811035
    },
    {
      "epoch": 0.6472086720867208,
      "step": 11941,
      "training_loss": 6.055943489074707
    },
    {
      "epoch": 0.6472628726287263,
      "step": 11942,
      "training_loss": 5.2097392082214355
    },
    {
      "epoch": 0.6473170731707317,
      "step": 11943,
      "training_loss": 6.003670692443848
    },
    {
      "epoch": 0.6473712737127372,
      "grad_norm": 23.210094451904297,
      "learning_rate": 1e-05,
      "loss": 5.7223,
      "step": 11944
    },
    {
      "epoch": 0.6473712737127372,
      "step": 11944,
      "training_loss": 7.174233913421631
    },
    {
      "epoch": 0.6474254742547425,
      "step": 11945,
      "training_loss": 7.748323440551758
    },
    {
      "epoch": 0.647479674796748,
      "step": 11946,
      "training_loss": 6.7774338722229
    },
    {
      "epoch": 0.6475338753387534,
      "step": 11947,
      "training_loss": 11.180807113647461
    },
    {
      "epoch": 0.6475880758807588,
      "grad_norm": 57.24665832519531,
      "learning_rate": 1e-05,
      "loss": 8.2202,
      "step": 11948
    },
    {
      "epoch": 0.6475880758807588,
      "step": 11948,
      "training_loss": 6.879744529724121
    },
    {
      "epoch": 0.6476422764227642,
      "step": 11949,
      "training_loss": 6.050050258636475
    },
    {
      "epoch": 0.6476964769647696,
      "step": 11950,
      "training_loss": 7.754326343536377
    },
    {
      "epoch": 0.6477506775067751,
      "step": 11951,
      "training_loss": 5.277713775634766
    },
    {
      "epoch": 0.6478048780487805,
      "grad_norm": 68.2000961303711,
      "learning_rate": 1e-05,
      "loss": 6.4905,
      "step": 11952
    },
    {
      "epoch": 0.6478048780487805,
      "step": 11952,
      "training_loss": 4.172652721405029
    },
    {
      "epoch": 0.647859078590786,
      "step": 11953,
      "training_loss": 6.8420586585998535
    },
    {
      "epoch": 0.6479132791327913,
      "step": 11954,
      "training_loss": 4.916158676147461
    },
    {
      "epoch": 0.6479674796747967,
      "step": 11955,
      "training_loss": 6.646322727203369
    },
    {
      "epoch": 0.6480216802168022,
      "grad_norm": 25.062705993652344,
      "learning_rate": 1e-05,
      "loss": 5.6443,
      "step": 11956
    },
    {
      "epoch": 0.6480216802168022,
      "step": 11956,
      "training_loss": 5.263750076293945
    },
    {
      "epoch": 0.6480758807588076,
      "step": 11957,
      "training_loss": 6.946170806884766
    },
    {
      "epoch": 0.648130081300813,
      "step": 11958,
      "training_loss": 6.453798770904541
    },
    {
      "epoch": 0.6481842818428184,
      "step": 11959,
      "training_loss": 7.20506477355957
    },
    {
      "epoch": 0.6482384823848238,
      "grad_norm": 18.37923240661621,
      "learning_rate": 1e-05,
      "loss": 6.4672,
      "step": 11960
    },
    {
      "epoch": 0.6482384823848238,
      "step": 11960,
      "training_loss": 7.2243475914001465
    },
    {
      "epoch": 0.6482926829268293,
      "step": 11961,
      "training_loss": 6.264101505279541
    },
    {
      "epoch": 0.6483468834688347,
      "step": 11962,
      "training_loss": 8.126676559448242
    },
    {
      "epoch": 0.6484010840108401,
      "step": 11963,
      "training_loss": 6.253096103668213
    },
    {
      "epoch": 0.6484552845528455,
      "grad_norm": 64.3786849975586,
      "learning_rate": 1e-05,
      "loss": 6.9671,
      "step": 11964
    },
    {
      "epoch": 0.6484552845528455,
      "step": 11964,
      "training_loss": 6.523728370666504
    },
    {
      "epoch": 0.648509485094851,
      "step": 11965,
      "training_loss": 6.005191326141357
    },
    {
      "epoch": 0.6485636856368564,
      "step": 11966,
      "training_loss": 9.96554183959961
    },
    {
      "epoch": 0.6486178861788617,
      "step": 11967,
      "training_loss": 5.962465763092041
    },
    {
      "epoch": 0.6486720867208672,
      "grad_norm": 42.47715377807617,
      "learning_rate": 1e-05,
      "loss": 7.1142,
      "step": 11968
    },
    {
      "epoch": 0.6486720867208672,
      "step": 11968,
      "training_loss": 6.7910919189453125
    },
    {
      "epoch": 0.6487262872628726,
      "step": 11969,
      "training_loss": 5.62821626663208
    },
    {
      "epoch": 0.6487804878048781,
      "step": 11970,
      "training_loss": 7.380094051361084
    },
    {
      "epoch": 0.6488346883468835,
      "step": 11971,
      "training_loss": 7.019751071929932
    },
    {
      "epoch": 0.6488888888888888,
      "grad_norm": 23.649253845214844,
      "learning_rate": 1e-05,
      "loss": 6.7048,
      "step": 11972
    },
    {
      "epoch": 0.6488888888888888,
      "step": 11972,
      "training_loss": 6.740675449371338
    },
    {
      "epoch": 0.6489430894308943,
      "step": 11973,
      "training_loss": 7.918353080749512
    },
    {
      "epoch": 0.6489972899728997,
      "step": 11974,
      "training_loss": 5.506171703338623
    },
    {
      "epoch": 0.6490514905149052,
      "step": 11975,
      "training_loss": 5.549627304077148
    },
    {
      "epoch": 0.6491056910569105,
      "grad_norm": 28.639619827270508,
      "learning_rate": 1e-05,
      "loss": 6.4287,
      "step": 11976
    },
    {
      "epoch": 0.6491056910569105,
      "step": 11976,
      "training_loss": 6.812075614929199
    },
    {
      "epoch": 0.649159891598916,
      "step": 11977,
      "training_loss": 6.5759758949279785
    },
    {
      "epoch": 0.6492140921409214,
      "step": 11978,
      "training_loss": 7.969862937927246
    },
    {
      "epoch": 0.6492682926829269,
      "step": 11979,
      "training_loss": 7.252748489379883
    },
    {
      "epoch": 0.6493224932249323,
      "grad_norm": 45.380470275878906,
      "learning_rate": 1e-05,
      "loss": 7.1527,
      "step": 11980
    },
    {
      "epoch": 0.6493224932249323,
      "step": 11980,
      "training_loss": 6.215418338775635
    },
    {
      "epoch": 0.6493766937669376,
      "step": 11981,
      "training_loss": 7.428092956542969
    },
    {
      "epoch": 0.6494308943089431,
      "step": 11982,
      "training_loss": 6.44018030166626
    },
    {
      "epoch": 0.6494850948509485,
      "step": 11983,
      "training_loss": 7.412359714508057
    },
    {
      "epoch": 0.649539295392954,
      "grad_norm": 32.92805480957031,
      "learning_rate": 1e-05,
      "loss": 6.874,
      "step": 11984
    },
    {
      "epoch": 0.649539295392954,
      "step": 11984,
      "training_loss": 5.736299991607666
    },
    {
      "epoch": 0.6495934959349593,
      "step": 11985,
      "training_loss": 6.835362434387207
    },
    {
      "epoch": 0.6496476964769647,
      "step": 11986,
      "training_loss": 6.833380222320557
    },
    {
      "epoch": 0.6497018970189702,
      "step": 11987,
      "training_loss": 7.190483093261719
    },
    {
      "epoch": 0.6497560975609756,
      "grad_norm": 23.961429595947266,
      "learning_rate": 1e-05,
      "loss": 6.6489,
      "step": 11988
    },
    {
      "epoch": 0.6497560975609756,
      "step": 11988,
      "training_loss": 6.382753849029541
    },
    {
      "epoch": 0.6498102981029811,
      "step": 11989,
      "training_loss": 7.890622615814209
    },
    {
      "epoch": 0.6498644986449864,
      "step": 11990,
      "training_loss": 7.289433002471924
    },
    {
      "epoch": 0.6499186991869919,
      "step": 11991,
      "training_loss": 7.053399085998535
    },
    {
      "epoch": 0.6499728997289973,
      "grad_norm": 33.770484924316406,
      "learning_rate": 1e-05,
      "loss": 7.1541,
      "step": 11992
    },
    {
      "epoch": 0.6499728997289973,
      "step": 11992,
      "training_loss": 6.940412998199463
    },
    {
      "epoch": 0.6500271002710027,
      "step": 11993,
      "training_loss": 8.138921737670898
    },
    {
      "epoch": 0.6500813008130081,
      "step": 11994,
      "training_loss": 6.370700359344482
    },
    {
      "epoch": 0.6501355013550135,
      "step": 11995,
      "training_loss": 7.468482971191406
    },
    {
      "epoch": 0.650189701897019,
      "grad_norm": 19.394664764404297,
      "learning_rate": 1e-05,
      "loss": 7.2296,
      "step": 11996
    },
    {
      "epoch": 0.650189701897019,
      "step": 11996,
      "training_loss": 5.515185832977295
    },
    {
      "epoch": 0.6502439024390244,
      "step": 11997,
      "training_loss": 6.362451076507568
    },
    {
      "epoch": 0.6502981029810299,
      "step": 11998,
      "training_loss": 4.16831111907959
    },
    {
      "epoch": 0.6503523035230352,
      "step": 11999,
      "training_loss": 6.698981761932373
    },
    {
      "epoch": 0.6504065040650406,
      "grad_norm": 19.40988540649414,
      "learning_rate": 1e-05,
      "loss": 5.6862,
      "step": 12000
    },
    {
      "epoch": 0.6504065040650406,
      "eval_runtime": 457.4745,
      "eval_samples_per_second": 4.481,
      "eval_steps_per_second": 4.481,
      "step": 12000
    },
    {
      "epoch": 0.6504065040650406,
      "step": 12000,
      "training_loss": 7.407741069793701
    },
    {
      "epoch": 0.6504607046070461,
      "step": 12001,
      "training_loss": 5.595339775085449
    },
    {
      "epoch": 0.6505149051490515,
      "step": 12002,
      "training_loss": 7.354925155639648
    },
    {
      "epoch": 0.6505691056910569,
      "step": 12003,
      "training_loss": 6.953944683074951
    },
    {
      "epoch": 0.6506233062330623,
      "grad_norm": 36.21010208129883,
      "learning_rate": 1e-05,
      "loss": 6.828,
      "step": 12004
    },
    {
      "epoch": 0.6506233062330623,
      "step": 12004,
      "training_loss": 6.838759422302246
    },
    {
      "epoch": 0.6506775067750677,
      "step": 12005,
      "training_loss": 6.083053112030029
    },
    {
      "epoch": 0.6507317073170732,
      "step": 12006,
      "training_loss": 7.100635528564453
    },
    {
      "epoch": 0.6507859078590786,
      "step": 12007,
      "training_loss": 6.350955486297607
    },
    {
      "epoch": 0.650840108401084,
      "grad_norm": 28.61319923400879,
      "learning_rate": 1e-05,
      "loss": 6.5934,
      "step": 12008
    },
    {
      "epoch": 0.650840108401084,
      "step": 12008,
      "training_loss": 7.472559928894043
    },
    {
      "epoch": 0.6508943089430894,
      "step": 12009,
      "training_loss": 5.90909481048584
    },
    {
      "epoch": 0.6509485094850949,
      "step": 12010,
      "training_loss": 6.237717151641846
    },
    {
      "epoch": 0.6510027100271003,
      "step": 12011,
      "training_loss": 5.707554340362549
    },
    {
      "epoch": 0.6510569105691056,
      "grad_norm": 24.069347381591797,
      "learning_rate": 1e-05,
      "loss": 6.3317,
      "step": 12012
    },
    {
      "epoch": 0.6510569105691056,
      "step": 12012,
      "training_loss": 6.670871257781982
    },
    {
      "epoch": 0.6511111111111111,
      "step": 12013,
      "training_loss": 6.615389347076416
    },
    {
      "epoch": 0.6511653116531165,
      "step": 12014,
      "training_loss": 6.467655181884766
    },
    {
      "epoch": 0.651219512195122,
      "step": 12015,
      "training_loss": 6.461783409118652
    },
    {
      "epoch": 0.6512737127371274,
      "grad_norm": 62.304443359375,
      "learning_rate": 1e-05,
      "loss": 6.5539,
      "step": 12016
    },
    {
      "epoch": 0.6512737127371274,
      "step": 12016,
      "training_loss": 6.762962818145752
    },
    {
      "epoch": 0.6513279132791328,
      "step": 12017,
      "training_loss": 6.81658411026001
    },
    {
      "epoch": 0.6513821138211382,
      "step": 12018,
      "training_loss": 4.243419170379639
    },
    {
      "epoch": 0.6514363143631436,
      "step": 12019,
      "training_loss": 5.1849846839904785
    },
    {
      "epoch": 0.6514905149051491,
      "grad_norm": 51.61524963378906,
      "learning_rate": 1e-05,
      "loss": 5.752,
      "step": 12020
    },
    {
      "epoch": 0.6514905149051491,
      "step": 12020,
      "training_loss": 6.244510173797607
    },
    {
      "epoch": 0.6515447154471544,
      "step": 12021,
      "training_loss": 6.858454704284668
    },
    {
      "epoch": 0.6515989159891599,
      "step": 12022,
      "training_loss": 5.841434478759766
    },
    {
      "epoch": 0.6516531165311653,
      "step": 12023,
      "training_loss": 7.15764045715332
    },
    {
      "epoch": 0.6517073170731708,
      "grad_norm": 29.880130767822266,
      "learning_rate": 1e-05,
      "loss": 6.5255,
      "step": 12024
    },
    {
      "epoch": 0.6517073170731708,
      "step": 12024,
      "training_loss": 6.739175796508789
    },
    {
      "epoch": 0.6517615176151762,
      "step": 12025,
      "training_loss": 6.485569953918457
    },
    {
      "epoch": 0.6518157181571815,
      "step": 12026,
      "training_loss": 7.439775466918945
    },
    {
      "epoch": 0.651869918699187,
      "step": 12027,
      "training_loss": 6.9692816734313965
    },
    {
      "epoch": 0.6519241192411924,
      "grad_norm": 61.34626770019531,
      "learning_rate": 1e-05,
      "loss": 6.9085,
      "step": 12028
    },
    {
      "epoch": 0.6519241192411924,
      "step": 12028,
      "training_loss": 6.265361785888672
    },
    {
      "epoch": 0.6519783197831979,
      "step": 12029,
      "training_loss": 3.0204269886016846
    },
    {
      "epoch": 0.6520325203252032,
      "step": 12030,
      "training_loss": 6.185620307922363
    },
    {
      "epoch": 0.6520867208672086,
      "step": 12031,
      "training_loss": 7.164529800415039
    },
    {
      "epoch": 0.6521409214092141,
      "grad_norm": 16.144248962402344,
      "learning_rate": 1e-05,
      "loss": 5.659,
      "step": 12032
    },
    {
      "epoch": 0.6521409214092141,
      "step": 12032,
      "training_loss": 5.226269721984863
    },
    {
      "epoch": 0.6521951219512195,
      "step": 12033,
      "training_loss": 6.85666036605835
    },
    {
      "epoch": 0.652249322493225,
      "step": 12034,
      "training_loss": 5.239377975463867
    },
    {
      "epoch": 0.6523035230352303,
      "step": 12035,
      "training_loss": 6.50903844833374
    },
    {
      "epoch": 0.6523577235772358,
      "grad_norm": 23.622583389282227,
      "learning_rate": 1e-05,
      "loss": 5.9578,
      "step": 12036
    },
    {
      "epoch": 0.6523577235772358,
      "step": 12036,
      "training_loss": 6.261322498321533
    },
    {
      "epoch": 0.6524119241192412,
      "step": 12037,
      "training_loss": 6.10014533996582
    },
    {
      "epoch": 0.6524661246612466,
      "step": 12038,
      "training_loss": 6.071305274963379
    },
    {
      "epoch": 0.652520325203252,
      "step": 12039,
      "training_loss": 6.7089433670043945
    },
    {
      "epoch": 0.6525745257452574,
      "grad_norm": 26.148860931396484,
      "learning_rate": 1e-05,
      "loss": 6.2854,
      "step": 12040
    },
    {
      "epoch": 0.6525745257452574,
      "step": 12040,
      "training_loss": 6.160175800323486
    },
    {
      "epoch": 0.6526287262872629,
      "step": 12041,
      "training_loss": 6.545282363891602
    },
    {
      "epoch": 0.6526829268292683,
      "step": 12042,
      "training_loss": 6.3848419189453125
    },
    {
      "epoch": 0.6527371273712738,
      "step": 12043,
      "training_loss": 6.307718276977539
    },
    {
      "epoch": 0.6527913279132791,
      "grad_norm": 22.009313583374023,
      "learning_rate": 1e-05,
      "loss": 6.3495,
      "step": 12044
    },
    {
      "epoch": 0.6527913279132791,
      "step": 12044,
      "training_loss": 6.304195404052734
    },
    {
      "epoch": 0.6528455284552845,
      "step": 12045,
      "training_loss": 6.915491580963135
    },
    {
      "epoch": 0.65289972899729,
      "step": 12046,
      "training_loss": 8.017403602600098
    },
    {
      "epoch": 0.6529539295392954,
      "step": 12047,
      "training_loss": 7.160401344299316
    },
    {
      "epoch": 0.6530081300813008,
      "grad_norm": 26.871793746948242,
      "learning_rate": 1e-05,
      "loss": 7.0994,
      "step": 12048
    },
    {
      "epoch": 0.6530081300813008,
      "step": 12048,
      "training_loss": 6.592413902282715
    },
    {
      "epoch": 0.6530623306233062,
      "step": 12049,
      "training_loss": 6.727488994598389
    },
    {
      "epoch": 0.6531165311653117,
      "step": 12050,
      "training_loss": 5.870090484619141
    },
    {
      "epoch": 0.6531707317073171,
      "step": 12051,
      "training_loss": 5.644404411315918
    },
    {
      "epoch": 0.6532249322493225,
      "grad_norm": 36.75834655761719,
      "learning_rate": 1e-05,
      "loss": 6.2086,
      "step": 12052
    },
    {
      "epoch": 0.6532249322493225,
      "step": 12052,
      "training_loss": 6.449831962585449
    },
    {
      "epoch": 0.6532791327913279,
      "step": 12053,
      "training_loss": 5.736522197723389
    },
    {
      "epoch": 0.6533333333333333,
      "step": 12054,
      "training_loss": 3.175790548324585
    },
    {
      "epoch": 0.6533875338753388,
      "step": 12055,
      "training_loss": 6.287567615509033
    },
    {
      "epoch": 0.6534417344173442,
      "grad_norm": 41.76725769042969,
      "learning_rate": 1e-05,
      "loss": 5.4124,
      "step": 12056
    },
    {
      "epoch": 0.6534417344173442,
      "step": 12056,
      "training_loss": 3.9637253284454346
    },
    {
      "epoch": 0.6534959349593495,
      "step": 12057,
      "training_loss": 7.46309232711792
    },
    {
      "epoch": 0.653550135501355,
      "step": 12058,
      "training_loss": 5.74729585647583
    },
    {
      "epoch": 0.6536043360433604,
      "step": 12059,
      "training_loss": 5.979126930236816
    },
    {
      "epoch": 0.6536585365853659,
      "grad_norm": 32.292030334472656,
      "learning_rate": 1e-05,
      "loss": 5.7883,
      "step": 12060
    },
    {
      "epoch": 0.6536585365853659,
      "step": 12060,
      "training_loss": 7.424160957336426
    },
    {
      "epoch": 0.6537127371273713,
      "step": 12061,
      "training_loss": 5.863076686859131
    },
    {
      "epoch": 0.6537669376693767,
      "step": 12062,
      "training_loss": 7.824252128601074
    },
    {
      "epoch": 0.6538211382113821,
      "step": 12063,
      "training_loss": 6.229249477386475
    },
    {
      "epoch": 0.6538753387533875,
      "grad_norm": 45.45765686035156,
      "learning_rate": 1e-05,
      "loss": 6.8352,
      "step": 12064
    },
    {
      "epoch": 0.6538753387533875,
      "step": 12064,
      "training_loss": 5.486607551574707
    },
    {
      "epoch": 0.653929539295393,
      "step": 12065,
      "training_loss": 5.122102737426758
    },
    {
      "epoch": 0.6539837398373983,
      "step": 12066,
      "training_loss": 5.040192127227783
    },
    {
      "epoch": 0.6540379403794038,
      "step": 12067,
      "training_loss": 4.660269737243652
    },
    {
      "epoch": 0.6540921409214092,
      "grad_norm": 23.639440536499023,
      "learning_rate": 1e-05,
      "loss": 5.0773,
      "step": 12068
    },
    {
      "epoch": 0.6540921409214092,
      "step": 12068,
      "training_loss": 4.687718391418457
    },
    {
      "epoch": 0.6541463414634147,
      "step": 12069,
      "training_loss": 5.022632122039795
    },
    {
      "epoch": 0.6542005420054201,
      "step": 12070,
      "training_loss": 7.124213695526123
    },
    {
      "epoch": 0.6542547425474254,
      "step": 12071,
      "training_loss": 7.302761077880859
    },
    {
      "epoch": 0.6543089430894309,
      "grad_norm": 57.97181701660156,
      "learning_rate": 1e-05,
      "loss": 6.0343,
      "step": 12072
    },
    {
      "epoch": 0.6543089430894309,
      "step": 12072,
      "training_loss": 7.693667888641357
    },
    {
      "epoch": 0.6543631436314363,
      "step": 12073,
      "training_loss": 7.715142726898193
    },
    {
      "epoch": 0.6544173441734418,
      "step": 12074,
      "training_loss": 6.302353858947754
    },
    {
      "epoch": 0.6544715447154471,
      "step": 12075,
      "training_loss": 7.282565116882324
    },
    {
      "epoch": 0.6545257452574526,
      "grad_norm": 42.39373779296875,
      "learning_rate": 1e-05,
      "loss": 7.2484,
      "step": 12076
    },
    {
      "epoch": 0.6545257452574526,
      "step": 12076,
      "training_loss": 7.149327278137207
    },
    {
      "epoch": 0.654579945799458,
      "step": 12077,
      "training_loss": 7.15347957611084
    },
    {
      "epoch": 0.6546341463414634,
      "step": 12078,
      "training_loss": 6.60433292388916
    },
    {
      "epoch": 0.6546883468834689,
      "step": 12079,
      "training_loss": 7.65169620513916
    },
    {
      "epoch": 0.6547425474254742,
      "grad_norm": 26.48261833190918,
      "learning_rate": 1e-05,
      "loss": 7.1397,
      "step": 12080
    },
    {
      "epoch": 0.6547425474254742,
      "step": 12080,
      "training_loss": 6.400190830230713
    },
    {
      "epoch": 0.6547967479674797,
      "step": 12081,
      "training_loss": 6.253633975982666
    },
    {
      "epoch": 0.6548509485094851,
      "step": 12082,
      "training_loss": 6.833204746246338
    },
    {
      "epoch": 0.6549051490514906,
      "step": 12083,
      "training_loss": 7.0095744132995605
    },
    {
      "epoch": 0.6549593495934959,
      "grad_norm": 46.0104866027832,
      "learning_rate": 1e-05,
      "loss": 6.6242,
      "step": 12084
    },
    {
      "epoch": 0.6549593495934959,
      "step": 12084,
      "training_loss": 3.8478338718414307
    },
    {
      "epoch": 0.6550135501355013,
      "step": 12085,
      "training_loss": 8.6771879196167
    },
    {
      "epoch": 0.6550677506775068,
      "step": 12086,
      "training_loss": 6.336471080780029
    },
    {
      "epoch": 0.6551219512195122,
      "step": 12087,
      "training_loss": 6.171106815338135
    },
    {
      "epoch": 0.6551761517615177,
      "grad_norm": 40.199092864990234,
      "learning_rate": 1e-05,
      "loss": 6.2581,
      "step": 12088
    },
    {
      "epoch": 0.6551761517615177,
      "step": 12088,
      "training_loss": 6.230932712554932
    },
    {
      "epoch": 0.655230352303523,
      "step": 12089,
      "training_loss": 7.739612579345703
    },
    {
      "epoch": 0.6552845528455284,
      "step": 12090,
      "training_loss": 6.889235019683838
    },
    {
      "epoch": 0.6553387533875339,
      "step": 12091,
      "training_loss": 7.133376598358154
    },
    {
      "epoch": 0.6553929539295393,
      "grad_norm": 19.925249099731445,
      "learning_rate": 1e-05,
      "loss": 6.9983,
      "step": 12092
    },
    {
      "epoch": 0.6553929539295393,
      "step": 12092,
      "training_loss": 6.249538898468018
    },
    {
      "epoch": 0.6554471544715447,
      "step": 12093,
      "training_loss": 7.146623134613037
    },
    {
      "epoch": 0.6555013550135501,
      "step": 12094,
      "training_loss": 6.695196151733398
    },
    {
      "epoch": 0.6555555555555556,
      "step": 12095,
      "training_loss": 6.425985813140869
    },
    {
      "epoch": 0.655609756097561,
      "grad_norm": 37.43751525878906,
      "learning_rate": 1e-05,
      "loss": 6.6293,
      "step": 12096
    },
    {
      "epoch": 0.655609756097561,
      "step": 12096,
      "training_loss": 4.733188629150391
    },
    {
      "epoch": 0.6556639566395664,
      "step": 12097,
      "training_loss": 7.780342102050781
    },
    {
      "epoch": 0.6557181571815718,
      "step": 12098,
      "training_loss": 7.695565700531006
    },
    {
      "epoch": 0.6557723577235772,
      "step": 12099,
      "training_loss": 6.029994010925293
    },
    {
      "epoch": 0.6558265582655827,
      "grad_norm": 19.789714813232422,
      "learning_rate": 1e-05,
      "loss": 6.5598,
      "step": 12100
    },
    {
      "epoch": 0.6558265582655827,
      "step": 12100,
      "training_loss": 6.7710862159729
    },
    {
      "epoch": 0.6558807588075881,
      "step": 12101,
      "training_loss": 7.376003742218018
    },
    {
      "epoch": 0.6559349593495934,
      "step": 12102,
      "training_loss": 6.918330192565918
    },
    {
      "epoch": 0.6559891598915989,
      "step": 12103,
      "training_loss": 5.883820533752441
    },
    {
      "epoch": 0.6560433604336043,
      "grad_norm": 22.783817291259766,
      "learning_rate": 1e-05,
      "loss": 6.7373,
      "step": 12104
    },
    {
      "epoch": 0.6560433604336043,
      "step": 12104,
      "training_loss": 6.72255802154541
    },
    {
      "epoch": 0.6560975609756098,
      "step": 12105,
      "training_loss": 7.6080098152160645
    },
    {
      "epoch": 0.6561517615176152,
      "step": 12106,
      "training_loss": 5.93155574798584
    },
    {
      "epoch": 0.6562059620596206,
      "step": 12107,
      "training_loss": 6.178511619567871
    },
    {
      "epoch": 0.656260162601626,
      "grad_norm": 30.074668884277344,
      "learning_rate": 1e-05,
      "loss": 6.6102,
      "step": 12108
    },
    {
      "epoch": 0.656260162601626,
      "step": 12108,
      "training_loss": 4.418574810028076
    },
    {
      "epoch": 0.6563143631436315,
      "step": 12109,
      "training_loss": 6.626233100891113
    },
    {
      "epoch": 0.6563685636856369,
      "step": 12110,
      "training_loss": 3.760941743850708
    },
    {
      "epoch": 0.6564227642276422,
      "step": 12111,
      "training_loss": 4.590241432189941
    },
    {
      "epoch": 0.6564769647696477,
      "grad_norm": 27.124469757080078,
      "learning_rate": 1e-05,
      "loss": 4.849,
      "step": 12112
    },
    {
      "epoch": 0.6564769647696477,
      "step": 12112,
      "training_loss": 6.607418060302734
    },
    {
      "epoch": 0.6565311653116531,
      "step": 12113,
      "training_loss": 5.8407158851623535
    },
    {
      "epoch": 0.6565853658536586,
      "step": 12114,
      "training_loss": 6.756814002990723
    },
    {
      "epoch": 0.656639566395664,
      "step": 12115,
      "training_loss": 5.732221603393555
    },
    {
      "epoch": 0.6566937669376693,
      "grad_norm": 24.02347183227539,
      "learning_rate": 1e-05,
      "loss": 6.2343,
      "step": 12116
    },
    {
      "epoch": 0.6566937669376693,
      "step": 12116,
      "training_loss": 6.686768054962158
    },
    {
      "epoch": 0.6567479674796748,
      "step": 12117,
      "training_loss": 6.960207462310791
    },
    {
      "epoch": 0.6568021680216802,
      "step": 12118,
      "training_loss": 5.430945873260498
    },
    {
      "epoch": 0.6568563685636857,
      "step": 12119,
      "training_loss": 7.313176155090332
    },
    {
      "epoch": 0.656910569105691,
      "grad_norm": 27.156862258911133,
      "learning_rate": 1e-05,
      "loss": 6.5978,
      "step": 12120
    },
    {
      "epoch": 0.656910569105691,
      "step": 12120,
      "training_loss": 3.276346206665039
    },
    {
      "epoch": 0.6569647696476965,
      "step": 12121,
      "training_loss": 2.4611804485321045
    },
    {
      "epoch": 0.6570189701897019,
      "step": 12122,
      "training_loss": 9.135223388671875
    },
    {
      "epoch": 0.6570731707317073,
      "step": 12123,
      "training_loss": 6.707000255584717
    },
    {
      "epoch": 0.6571273712737128,
      "grad_norm": 24.829092025756836,
      "learning_rate": 1e-05,
      "loss": 5.3949,
      "step": 12124
    },
    {
      "epoch": 0.6571273712737128,
      "step": 12124,
      "training_loss": 6.928193092346191
    },
    {
      "epoch": 0.6571815718157181,
      "step": 12125,
      "training_loss": 6.442519664764404
    },
    {
      "epoch": 0.6572357723577236,
      "step": 12126,
      "training_loss": 6.594701766967773
    },
    {
      "epoch": 0.657289972899729,
      "step": 12127,
      "training_loss": 5.1883416175842285
    },
    {
      "epoch": 0.6573441734417345,
      "grad_norm": 42.798954010009766,
      "learning_rate": 1e-05,
      "loss": 6.2884,
      "step": 12128
    },
    {
      "epoch": 0.6573441734417345,
      "step": 12128,
      "training_loss": 7.340468406677246
    },
    {
      "epoch": 0.6573983739837398,
      "step": 12129,
      "training_loss": 6.533889293670654
    },
    {
      "epoch": 0.6574525745257452,
      "step": 12130,
      "training_loss": 6.419466495513916
    },
    {
      "epoch": 0.6575067750677507,
      "step": 12131,
      "training_loss": 7.340178489685059
    },
    {
      "epoch": 0.6575609756097561,
      "grad_norm": 19.590070724487305,
      "learning_rate": 1e-05,
      "loss": 6.9085,
      "step": 12132
    },
    {
      "epoch": 0.6575609756097561,
      "step": 12132,
      "training_loss": 6.194684028625488
    },
    {
      "epoch": 0.6576151761517616,
      "step": 12133,
      "training_loss": 7.663302421569824
    },
    {
      "epoch": 0.6576693766937669,
      "step": 12134,
      "training_loss": 7.090104103088379
    },
    {
      "epoch": 0.6577235772357723,
      "step": 12135,
      "training_loss": 3.119535446166992
    },
    {
      "epoch": 0.6577777777777778,
      "grad_norm": 29.13612174987793,
      "learning_rate": 1e-05,
      "loss": 6.0169,
      "step": 12136
    },
    {
      "epoch": 0.6577777777777778,
      "step": 12136,
      "training_loss": 7.030500411987305
    },
    {
      "epoch": 0.6578319783197832,
      "step": 12137,
      "training_loss": 7.97304630279541
    },
    {
      "epoch": 0.6578861788617886,
      "step": 12138,
      "training_loss": 7.017423152923584
    },
    {
      "epoch": 0.657940379403794,
      "step": 12139,
      "training_loss": 4.0489983558654785
    },
    {
      "epoch": 0.6579945799457995,
      "grad_norm": 28.127727508544922,
      "learning_rate": 1e-05,
      "loss": 6.5175,
      "step": 12140
    },
    {
      "epoch": 0.6579945799457995,
      "step": 12140,
      "training_loss": 5.154656887054443
    },
    {
      "epoch": 0.6580487804878049,
      "step": 12141,
      "training_loss": 6.266528129577637
    },
    {
      "epoch": 0.6581029810298104,
      "step": 12142,
      "training_loss": 7.427612781524658
    },
    {
      "epoch": 0.6581571815718157,
      "step": 12143,
      "training_loss": 6.6576056480407715
    },
    {
      "epoch": 0.6582113821138211,
      "grad_norm": 24.018688201904297,
      "learning_rate": 1e-05,
      "loss": 6.3766,
      "step": 12144
    },
    {
      "epoch": 0.6582113821138211,
      "step": 12144,
      "training_loss": 7.5427117347717285
    },
    {
      "epoch": 0.6582655826558266,
      "step": 12145,
      "training_loss": 6.646689414978027
    },
    {
      "epoch": 0.658319783197832,
      "step": 12146,
      "training_loss": 5.943925857543945
    },
    {
      "epoch": 0.6583739837398374,
      "step": 12147,
      "training_loss": 7.042434215545654
    },
    {
      "epoch": 0.6584281842818428,
      "grad_norm": 20.225309371948242,
      "learning_rate": 1e-05,
      "loss": 6.7939,
      "step": 12148
    },
    {
      "epoch": 0.6584281842818428,
      "step": 12148,
      "training_loss": 6.275106430053711
    },
    {
      "epoch": 0.6584823848238482,
      "step": 12149,
      "training_loss": 6.648125171661377
    },
    {
      "epoch": 0.6585365853658537,
      "step": 12150,
      "training_loss": 7.7488861083984375
    },
    {
      "epoch": 0.6585907859078591,
      "step": 12151,
      "training_loss": 4.714705467224121
    },
    {
      "epoch": 0.6586449864498645,
      "grad_norm": 47.92208480834961,
      "learning_rate": 1e-05,
      "loss": 6.3467,
      "step": 12152
    },
    {
      "epoch": 0.6586449864498645,
      "step": 12152,
      "training_loss": 6.840871810913086
    },
    {
      "epoch": 0.6586991869918699,
      "step": 12153,
      "training_loss": 6.27623987197876
    },
    {
      "epoch": 0.6587533875338754,
      "step": 12154,
      "training_loss": 7.403770923614502
    },
    {
      "epoch": 0.6588075880758808,
      "step": 12155,
      "training_loss": 7.023404121398926
    },
    {
      "epoch": 0.6588617886178861,
      "grad_norm": 29.225343704223633,
      "learning_rate": 1e-05,
      "loss": 6.8861,
      "step": 12156
    },
    {
      "epoch": 0.6588617886178861,
      "step": 12156,
      "training_loss": 4.6092424392700195
    },
    {
      "epoch": 0.6589159891598916,
      "step": 12157,
      "training_loss": 7.154092788696289
    },
    {
      "epoch": 0.658970189701897,
      "step": 12158,
      "training_loss": 8.687466621398926
    },
    {
      "epoch": 0.6590243902439025,
      "step": 12159,
      "training_loss": 7.531255722045898
    },
    {
      "epoch": 0.6590785907859079,
      "grad_norm": 48.356449127197266,
      "learning_rate": 1e-05,
      "loss": 6.9955,
      "step": 12160
    },
    {
      "epoch": 0.6590785907859079,
      "step": 12160,
      "training_loss": 6.267901420593262
    },
    {
      "epoch": 0.6591327913279132,
      "step": 12161,
      "training_loss": 6.752048015594482
    },
    {
      "epoch": 0.6591869918699187,
      "step": 12162,
      "training_loss": 7.712313652038574
    },
    {
      "epoch": 0.6592411924119241,
      "step": 12163,
      "training_loss": 6.2494306564331055
    },
    {
      "epoch": 0.6592953929539296,
      "grad_norm": 34.32781982421875,
      "learning_rate": 1e-05,
      "loss": 6.7454,
      "step": 12164
    },
    {
      "epoch": 0.6592953929539296,
      "step": 12164,
      "training_loss": 7.160004138946533
    },
    {
      "epoch": 0.6593495934959349,
      "step": 12165,
      "training_loss": 6.853363990783691
    },
    {
      "epoch": 0.6594037940379404,
      "step": 12166,
      "training_loss": 6.795382976531982
    },
    {
      "epoch": 0.6594579945799458,
      "step": 12167,
      "training_loss": 6.542664527893066
    },
    {
      "epoch": 0.6595121951219513,
      "grad_norm": 23.63969612121582,
      "learning_rate": 1e-05,
      "loss": 6.8379,
      "step": 12168
    },
    {
      "epoch": 0.6595121951219513,
      "step": 12168,
      "training_loss": 7.801461219787598
    },
    {
      "epoch": 0.6595663956639567,
      "step": 12169,
      "training_loss": 7.400857925415039
    },
    {
      "epoch": 0.659620596205962,
      "step": 12170,
      "training_loss": 7.183176517486572
    },
    {
      "epoch": 0.6596747967479675,
      "step": 12171,
      "training_loss": 3.401228904724121
    },
    {
      "epoch": 0.6597289972899729,
      "grad_norm": 37.997501373291016,
      "learning_rate": 1e-05,
      "loss": 6.4467,
      "step": 12172
    },
    {
      "epoch": 0.6597289972899729,
      "step": 12172,
      "training_loss": 7.126495838165283
    },
    {
      "epoch": 0.6597831978319784,
      "step": 12173,
      "training_loss": 6.709195137023926
    },
    {
      "epoch": 0.6598373983739837,
      "step": 12174,
      "training_loss": 6.208377361297607
    },
    {
      "epoch": 0.6598915989159891,
      "step": 12175,
      "training_loss": 7.19137716293335
    },
    {
      "epoch": 0.6599457994579946,
      "grad_norm": 19.549835205078125,
      "learning_rate": 1e-05,
      "loss": 6.8089,
      "step": 12176
    },
    {
      "epoch": 0.6599457994579946,
      "step": 12176,
      "training_loss": 6.551949501037598
    },
    {
      "epoch": 0.66,
      "step": 12177,
      "training_loss": 7.985438823699951
    },
    {
      "epoch": 0.6600542005420054,
      "step": 12178,
      "training_loss": 8.017254829406738
    },
    {
      "epoch": 0.6601084010840108,
      "step": 12179,
      "training_loss": 6.190225601196289
    },
    {
      "epoch": 0.6601626016260163,
      "grad_norm": 42.65625,
      "learning_rate": 1e-05,
      "loss": 7.1862,
      "step": 12180
    },
    {
      "epoch": 0.6601626016260163,
      "step": 12180,
      "training_loss": 5.962213039398193
    },
    {
      "epoch": 0.6602168021680217,
      "step": 12181,
      "training_loss": 4.313725471496582
    },
    {
      "epoch": 0.6602710027100271,
      "step": 12182,
      "training_loss": 6.55666971206665
    },
    {
      "epoch": 0.6603252032520325,
      "step": 12183,
      "training_loss": 6.848762035369873
    },
    {
      "epoch": 0.6603794037940379,
      "grad_norm": 32.6851921081543,
      "learning_rate": 1e-05,
      "loss": 5.9203,
      "step": 12184
    },
    {
      "epoch": 0.6603794037940379,
      "step": 12184,
      "training_loss": 7.729050636291504
    },
    {
      "epoch": 0.6604336043360434,
      "step": 12185,
      "training_loss": 7.309656620025635
    },
    {
      "epoch": 0.6604878048780488,
      "step": 12186,
      "training_loss": 7.906867980957031
    },
    {
      "epoch": 0.6605420054200541,
      "step": 12187,
      "training_loss": 6.333592414855957
    },
    {
      "epoch": 0.6605962059620596,
      "grad_norm": 19.710983276367188,
      "learning_rate": 1e-05,
      "loss": 7.3198,
      "step": 12188
    },
    {
      "epoch": 0.6605962059620596,
      "step": 12188,
      "training_loss": 6.562564849853516
    },
    {
      "epoch": 0.660650406504065,
      "step": 12189,
      "training_loss": 5.800821304321289
    },
    {
      "epoch": 0.6607046070460705,
      "step": 12190,
      "training_loss": 6.647815704345703
    },
    {
      "epoch": 0.6607588075880759,
      "step": 12191,
      "training_loss": 5.771796226501465
    },
    {
      "epoch": 0.6608130081300813,
      "grad_norm": 35.340797424316406,
      "learning_rate": 1e-05,
      "loss": 6.1957,
      "step": 12192
    },
    {
      "epoch": 0.6608130081300813,
      "step": 12192,
      "training_loss": 6.583535194396973
    },
    {
      "epoch": 0.6608672086720867,
      "step": 12193,
      "training_loss": 7.115364074707031
    },
    {
      "epoch": 0.6609214092140921,
      "step": 12194,
      "training_loss": 6.851268291473389
    },
    {
      "epoch": 0.6609756097560976,
      "step": 12195,
      "training_loss": 6.9751434326171875
    },
    {
      "epoch": 0.6610298102981029,
      "grad_norm": 20.538761138916016,
      "learning_rate": 1e-05,
      "loss": 6.8813,
      "step": 12196
    },
    {
      "epoch": 0.6610298102981029,
      "step": 12196,
      "training_loss": 2.9645285606384277
    },
    {
      "epoch": 0.6610840108401084,
      "step": 12197,
      "training_loss": 6.7079057693481445
    },
    {
      "epoch": 0.6611382113821138,
      "step": 12198,
      "training_loss": 7.41405725479126
    },
    {
      "epoch": 0.6611924119241193,
      "step": 12199,
      "training_loss": 6.1860456466674805
    },
    {
      "epoch": 0.6612466124661247,
      "grad_norm": 27.731430053710938,
      "learning_rate": 1e-05,
      "loss": 5.8181,
      "step": 12200
    },
    {
      "epoch": 0.6612466124661247,
      "step": 12200,
      "training_loss": 6.796196460723877
    },
    {
      "epoch": 0.66130081300813,
      "step": 12201,
      "training_loss": 6.500141620635986
    },
    {
      "epoch": 0.6613550135501355,
      "step": 12202,
      "training_loss": 4.201868057250977
    },
    {
      "epoch": 0.6614092140921409,
      "step": 12203,
      "training_loss": 7.186270236968994
    },
    {
      "epoch": 0.6614634146341464,
      "grad_norm": 34.54034423828125,
      "learning_rate": 1e-05,
      "loss": 6.1711,
      "step": 12204
    },
    {
      "epoch": 0.6614634146341464,
      "step": 12204,
      "training_loss": 3.5255134105682373
    },
    {
      "epoch": 0.6615176151761517,
      "step": 12205,
      "training_loss": 5.823419570922852
    },
    {
      "epoch": 0.6615718157181572,
      "step": 12206,
      "training_loss": 5.903985023498535
    },
    {
      "epoch": 0.6616260162601626,
      "step": 12207,
      "training_loss": 6.425442218780518
    },
    {
      "epoch": 0.661680216802168,
      "grad_norm": 23.572004318237305,
      "learning_rate": 1e-05,
      "loss": 5.4196,
      "step": 12208
    },
    {
      "epoch": 0.661680216802168,
      "step": 12208,
      "training_loss": 6.225102424621582
    },
    {
      "epoch": 0.6617344173441735,
      "step": 12209,
      "training_loss": 6.812216281890869
    },
    {
      "epoch": 0.6617886178861788,
      "step": 12210,
      "training_loss": 6.8480634689331055
    },
    {
      "epoch": 0.6618428184281843,
      "step": 12211,
      "training_loss": 7.849168300628662
    },
    {
      "epoch": 0.6618970189701897,
      "grad_norm": 24.500104904174805,
      "learning_rate": 1e-05,
      "loss": 6.9336,
      "step": 12212
    },
    {
      "epoch": 0.6618970189701897,
      "step": 12212,
      "training_loss": 6.121762752532959
    },
    {
      "epoch": 0.6619512195121952,
      "step": 12213,
      "training_loss": 7.4236931800842285
    },
    {
      "epoch": 0.6620054200542005,
      "step": 12214,
      "training_loss": 6.498804569244385
    },
    {
      "epoch": 0.6620596205962059,
      "step": 12215,
      "training_loss": 5.9422607421875
    },
    {
      "epoch": 0.6621138211382114,
      "grad_norm": 28.219514846801758,
      "learning_rate": 1e-05,
      "loss": 6.4966,
      "step": 12216
    },
    {
      "epoch": 0.6621138211382114,
      "step": 12216,
      "training_loss": 6.593353271484375
    },
    {
      "epoch": 0.6621680216802168,
      "step": 12217,
      "training_loss": 6.119481563568115
    },
    {
      "epoch": 0.6622222222222223,
      "step": 12218,
      "training_loss": 5.588456153869629
    },
    {
      "epoch": 0.6622764227642276,
      "step": 12219,
      "training_loss": 6.229157447814941
    },
    {
      "epoch": 0.662330623306233,
      "grad_norm": 37.13739013671875,
      "learning_rate": 1e-05,
      "loss": 6.1326,
      "step": 12220
    },
    {
      "epoch": 0.662330623306233,
      "step": 12220,
      "training_loss": 7.657437801361084
    },
    {
      "epoch": 0.6623848238482385,
      "step": 12221,
      "training_loss": 6.053653240203857
    },
    {
      "epoch": 0.6624390243902439,
      "step": 12222,
      "training_loss": 8.408751487731934
    },
    {
      "epoch": 0.6624932249322493,
      "step": 12223,
      "training_loss": 7.159266471862793
    },
    {
      "epoch": 0.6625474254742547,
      "grad_norm": 25.44255828857422,
      "learning_rate": 1e-05,
      "loss": 7.3198,
      "step": 12224
    },
    {
      "epoch": 0.6625474254742547,
      "step": 12224,
      "training_loss": 7.556168556213379
    },
    {
      "epoch": 0.6626016260162602,
      "step": 12225,
      "training_loss": 7.165965557098389
    },
    {
      "epoch": 0.6626558265582656,
      "step": 12226,
      "training_loss": 7.35803747177124
    },
    {
      "epoch": 0.662710027100271,
      "step": 12227,
      "training_loss": 5.065157413482666
    },
    {
      "epoch": 0.6627642276422764,
      "grad_norm": 29.736675262451172,
      "learning_rate": 1e-05,
      "loss": 6.7863,
      "step": 12228
    },
    {
      "epoch": 0.6627642276422764,
      "step": 12228,
      "training_loss": 7.349584102630615
    },
    {
      "epoch": 0.6628184281842818,
      "step": 12229,
      "training_loss": 6.884344100952148
    },
    {
      "epoch": 0.6628726287262873,
      "step": 12230,
      "training_loss": 5.268010139465332
    },
    {
      "epoch": 0.6629268292682927,
      "step": 12231,
      "training_loss": 7.100429058074951
    },
    {
      "epoch": 0.662981029810298,
      "grad_norm": 14.855835914611816,
      "learning_rate": 1e-05,
      "loss": 6.6506,
      "step": 12232
    },
    {
      "epoch": 0.662981029810298,
      "step": 12232,
      "training_loss": 6.125284671783447
    },
    {
      "epoch": 0.6630352303523035,
      "step": 12233,
      "training_loss": 7.218576908111572
    },
    {
      "epoch": 0.6630894308943089,
      "step": 12234,
      "training_loss": 4.481393337249756
    },
    {
      "epoch": 0.6631436314363144,
      "step": 12235,
      "training_loss": 7.947239398956299
    },
    {
      "epoch": 0.6631978319783198,
      "grad_norm": 50.0214729309082,
      "learning_rate": 1e-05,
      "loss": 6.4431,
      "step": 12236
    },
    {
      "epoch": 0.6631978319783198,
      "step": 12236,
      "training_loss": 6.340694904327393
    },
    {
      "epoch": 0.6632520325203252,
      "step": 12237,
      "training_loss": 6.872628688812256
    },
    {
      "epoch": 0.6633062330623306,
      "step": 12238,
      "training_loss": 6.6762471199035645
    },
    {
      "epoch": 0.663360433604336,
      "step": 12239,
      "training_loss": 3.5127289295196533
    },
    {
      "epoch": 0.6634146341463415,
      "grad_norm": 27.322473526000977,
      "learning_rate": 1e-05,
      "loss": 5.8506,
      "step": 12240
    },
    {
      "epoch": 0.6634146341463415,
      "step": 12240,
      "training_loss": 6.129575729370117
    },
    {
      "epoch": 0.6634688346883468,
      "step": 12241,
      "training_loss": 7.930845737457275
    },
    {
      "epoch": 0.6635230352303523,
      "step": 12242,
      "training_loss": 6.474164962768555
    },
    {
      "epoch": 0.6635772357723577,
      "step": 12243,
      "training_loss": 6.216066837310791
    },
    {
      "epoch": 0.6636314363143632,
      "grad_norm": 21.616472244262695,
      "learning_rate": 1e-05,
      "loss": 6.6877,
      "step": 12244
    },
    {
      "epoch": 0.6636314363143632,
      "step": 12244,
      "training_loss": 5.930357933044434
    },
    {
      "epoch": 0.6636856368563686,
      "step": 12245,
      "training_loss": 6.996831893920898
    },
    {
      "epoch": 0.6637398373983739,
      "step": 12246,
      "training_loss": 6.364192485809326
    },
    {
      "epoch": 0.6637940379403794,
      "step": 12247,
      "training_loss": 6.584524631500244
    },
    {
      "epoch": 0.6638482384823848,
      "grad_norm": 31.674842834472656,
      "learning_rate": 1e-05,
      "loss": 6.469,
      "step": 12248
    },
    {
      "epoch": 0.6638482384823848,
      "step": 12248,
      "training_loss": 6.263147830963135
    },
    {
      "epoch": 0.6639024390243903,
      "step": 12249,
      "training_loss": 7.106512546539307
    },
    {
      "epoch": 0.6639566395663956,
      "step": 12250,
      "training_loss": 7.231575965881348
    },
    {
      "epoch": 0.6640108401084011,
      "step": 12251,
      "training_loss": 7.476783752441406
    },
    {
      "epoch": 0.6640650406504065,
      "grad_norm": 24.012544631958008,
      "learning_rate": 1e-05,
      "loss": 7.0195,
      "step": 12252
    },
    {
      "epoch": 0.6640650406504065,
      "step": 12252,
      "training_loss": 5.868546485900879
    },
    {
      "epoch": 0.664119241192412,
      "step": 12253,
      "training_loss": 6.044689655303955
    },
    {
      "epoch": 0.6641734417344174,
      "step": 12254,
      "training_loss": 5.825056076049805
    },
    {
      "epoch": 0.6642276422764227,
      "step": 12255,
      "training_loss": 6.724717140197754
    },
    {
      "epoch": 0.6642818428184282,
      "grad_norm": 33.987060546875,
      "learning_rate": 1e-05,
      "loss": 6.1158,
      "step": 12256
    },
    {
      "epoch": 0.6642818428184282,
      "step": 12256,
      "training_loss": 7.145084381103516
    },
    {
      "epoch": 0.6643360433604336,
      "step": 12257,
      "training_loss": 4.341909408569336
    },
    {
      "epoch": 0.6643902439024391,
      "step": 12258,
      "training_loss": 6.25999641418457
    },
    {
      "epoch": 0.6644444444444444,
      "step": 12259,
      "training_loss": 7.220730781555176
    },
    {
      "epoch": 0.6644986449864498,
      "grad_norm": 23.445430755615234,
      "learning_rate": 1e-05,
      "loss": 6.2419,
      "step": 12260
    },
    {
      "epoch": 0.6644986449864498,
      "step": 12260,
      "training_loss": 7.427304744720459
    },
    {
      "epoch": 0.6645528455284553,
      "step": 12261,
      "training_loss": 6.373390197753906
    },
    {
      "epoch": 0.6646070460704607,
      "step": 12262,
      "training_loss": 5.820340156555176
    },
    {
      "epoch": 0.6646612466124662,
      "step": 12263,
      "training_loss": 5.788670063018799
    },
    {
      "epoch": 0.6647154471544715,
      "grad_norm": 21.14868927001953,
      "learning_rate": 1e-05,
      "loss": 6.3524,
      "step": 12264
    },
    {
      "epoch": 0.6647154471544715,
      "step": 12264,
      "training_loss": 5.318089485168457
    },
    {
      "epoch": 0.664769647696477,
      "step": 12265,
      "training_loss": 6.987186908721924
    },
    {
      "epoch": 0.6648238482384824,
      "step": 12266,
      "training_loss": 3.672956943511963
    },
    {
      "epoch": 0.6648780487804878,
      "step": 12267,
      "training_loss": 5.650146484375
    },
    {
      "epoch": 0.6649322493224932,
      "grad_norm": 38.42667007446289,
      "learning_rate": 1e-05,
      "loss": 5.4071,
      "step": 12268
    },
    {
      "epoch": 0.6649322493224932,
      "step": 12268,
      "training_loss": 6.96704626083374
    },
    {
      "epoch": 0.6649864498644986,
      "step": 12269,
      "training_loss": 6.651810646057129
    },
    {
      "epoch": 0.6650406504065041,
      "step": 12270,
      "training_loss": 6.46441125869751
    },
    {
      "epoch": 0.6650948509485095,
      "step": 12271,
      "training_loss": 5.600961685180664
    },
    {
      "epoch": 0.665149051490515,
      "grad_norm": 36.90138244628906,
      "learning_rate": 1e-05,
      "loss": 6.4211,
      "step": 12272
    },
    {
      "epoch": 0.665149051490515,
      "step": 12272,
      "training_loss": 6.214527130126953
    },
    {
      "epoch": 0.6652032520325203,
      "step": 12273,
      "training_loss": 7.667703151702881
    },
    {
      "epoch": 0.6652574525745257,
      "step": 12274,
      "training_loss": 4.734296798706055
    },
    {
      "epoch": 0.6653116531165312,
      "step": 12275,
      "training_loss": 7.445953845977783
    },
    {
      "epoch": 0.6653658536585366,
      "grad_norm": 21.766511917114258,
      "learning_rate": 1e-05,
      "loss": 6.5156,
      "step": 12276
    },
    {
      "epoch": 0.6653658536585366,
      "step": 12276,
      "training_loss": 7.535939693450928
    },
    {
      "epoch": 0.665420054200542,
      "step": 12277,
      "training_loss": 5.841805934906006
    },
    {
      "epoch": 0.6654742547425474,
      "step": 12278,
      "training_loss": 3.3703677654266357
    },
    {
      "epoch": 0.6655284552845528,
      "step": 12279,
      "training_loss": 7.085832118988037
    },
    {
      "epoch": 0.6655826558265583,
      "grad_norm": 26.329687118530273,
      "learning_rate": 1e-05,
      "loss": 5.9585,
      "step": 12280
    },
    {
      "epoch": 0.6655826558265583,
      "step": 12280,
      "training_loss": 7.209609031677246
    },
    {
      "epoch": 0.6656368563685637,
      "step": 12281,
      "training_loss": 6.183581829071045
    },
    {
      "epoch": 0.6656910569105691,
      "step": 12282,
      "training_loss": 7.1203131675720215
    },
    {
      "epoch": 0.6657452574525745,
      "step": 12283,
      "training_loss": 6.612001419067383
    },
    {
      "epoch": 0.66579945799458,
      "grad_norm": 36.60395812988281,
      "learning_rate": 1e-05,
      "loss": 6.7814,
      "step": 12284
    },
    {
      "epoch": 0.66579945799458,
      "step": 12284,
      "training_loss": 6.669509410858154
    },
    {
      "epoch": 0.6658536585365854,
      "step": 12285,
      "training_loss": 6.736408233642578
    },
    {
      "epoch": 0.6659078590785907,
      "step": 12286,
      "training_loss": 6.144340515136719
    },
    {
      "epoch": 0.6659620596205962,
      "step": 12287,
      "training_loss": 6.848186492919922
    },
    {
      "epoch": 0.6660162601626016,
      "grad_norm": 17.148136138916016,
      "learning_rate": 1e-05,
      "loss": 6.5996,
      "step": 12288
    },
    {
      "epoch": 0.6660162601626016,
      "step": 12288,
      "training_loss": 7.082602024078369
    },
    {
      "epoch": 0.6660704607046071,
      "step": 12289,
      "training_loss": 5.174952983856201
    },
    {
      "epoch": 0.6661246612466125,
      "step": 12290,
      "training_loss": 6.819510459899902
    },
    {
      "epoch": 0.6661788617886178,
      "step": 12291,
      "training_loss": 6.709265232086182
    },
    {
      "epoch": 0.6662330623306233,
      "grad_norm": 23.10531234741211,
      "learning_rate": 1e-05,
      "loss": 6.4466,
      "step": 12292
    },
    {
      "epoch": 0.6662330623306233,
      "step": 12292,
      "training_loss": 5.070347309112549
    },
    {
      "epoch": 0.6662872628726287,
      "step": 12293,
      "training_loss": 6.902750492095947
    },
    {
      "epoch": 0.6663414634146342,
      "step": 12294,
      "training_loss": 6.5697855949401855
    },
    {
      "epoch": 0.6663956639566395,
      "step": 12295,
      "training_loss": 6.902017593383789
    },
    {
      "epoch": 0.666449864498645,
      "grad_norm": 25.98089027404785,
      "learning_rate": 1e-05,
      "loss": 6.3612,
      "step": 12296
    },
    {
      "epoch": 0.666449864498645,
      "step": 12296,
      "training_loss": 6.96577262878418
    },
    {
      "epoch": 0.6665040650406504,
      "step": 12297,
      "training_loss": 5.49509334564209
    },
    {
      "epoch": 0.6665582655826559,
      "step": 12298,
      "training_loss": 6.284237384796143
    },
    {
      "epoch": 0.6666124661246613,
      "step": 12299,
      "training_loss": 5.777377128601074
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 31.570131301879883,
      "learning_rate": 1e-05,
      "loss": 6.1306,
      "step": 12300
    },
    {
      "epoch": 0.6666666666666666,
      "step": 12300,
      "training_loss": 7.03068733215332
    },
    {
      "epoch": 0.6667208672086721,
      "step": 12301,
      "training_loss": 6.914280414581299
    },
    {
      "epoch": 0.6667750677506775,
      "step": 12302,
      "training_loss": 4.719052791595459
    },
    {
      "epoch": 0.666829268292683,
      "step": 12303,
      "training_loss": 6.279512405395508
    },
    {
      "epoch": 0.6668834688346883,
      "grad_norm": 29.491107940673828,
      "learning_rate": 1e-05,
      "loss": 6.2359,
      "step": 12304
    },
    {
      "epoch": 0.6668834688346883,
      "step": 12304,
      "training_loss": 3.6911208629608154
    },
    {
      "epoch": 0.6669376693766937,
      "step": 12305,
      "training_loss": 6.0446648597717285
    },
    {
      "epoch": 0.6669918699186992,
      "step": 12306,
      "training_loss": 7.064200401306152
    },
    {
      "epoch": 0.6670460704607046,
      "step": 12307,
      "training_loss": 6.014981746673584
    },
    {
      "epoch": 0.6671002710027101,
      "grad_norm": 17.72905158996582,
      "learning_rate": 1e-05,
      "loss": 5.7037,
      "step": 12308
    },
    {
      "epoch": 0.6671002710027101,
      "step": 12308,
      "training_loss": 7.796557426452637
    },
    {
      "epoch": 0.6671544715447154,
      "step": 12309,
      "training_loss": 6.272387504577637
    },
    {
      "epoch": 0.6672086720867209,
      "step": 12310,
      "training_loss": 7.767026901245117
    },
    {
      "epoch": 0.6672628726287263,
      "step": 12311,
      "training_loss": 7.445077419281006
    },
    {
      "epoch": 0.6673170731707317,
      "grad_norm": 22.853364944458008,
      "learning_rate": 1e-05,
      "loss": 7.3203,
      "step": 12312
    },
    {
      "epoch": 0.6673170731707317,
      "step": 12312,
      "training_loss": 7.0653839111328125
    },
    {
      "epoch": 0.6673712737127371,
      "step": 12313,
      "training_loss": 6.963270664215088
    },
    {
      "epoch": 0.6674254742547425,
      "step": 12314,
      "training_loss": 3.3817124366760254
    },
    {
      "epoch": 0.667479674796748,
      "step": 12315,
      "training_loss": 5.563196182250977
    },
    {
      "epoch": 0.6675338753387534,
      "grad_norm": 20.429645538330078,
      "learning_rate": 1e-05,
      "loss": 5.7434,
      "step": 12316
    },
    {
      "epoch": 0.6675338753387534,
      "step": 12316,
      "training_loss": 6.0472540855407715
    },
    {
      "epoch": 0.6675880758807589,
      "step": 12317,
      "training_loss": 2.8334105014801025
    },
    {
      "epoch": 0.6676422764227642,
      "step": 12318,
      "training_loss": 7.328793048858643
    },
    {
      "epoch": 0.6676964769647696,
      "step": 12319,
      "training_loss": 6.971362590789795
    },
    {
      "epoch": 0.6677506775067751,
      "grad_norm": 24.42505645751953,
      "learning_rate": 1e-05,
      "loss": 5.7952,
      "step": 12320
    },
    {
      "epoch": 0.6677506775067751,
      "step": 12320,
      "training_loss": 6.090794086456299
    },
    {
      "epoch": 0.6678048780487805,
      "step": 12321,
      "training_loss": 6.944637775421143
    },
    {
      "epoch": 0.6678590785907859,
      "step": 12322,
      "training_loss": 5.829893589019775
    },
    {
      "epoch": 0.6679132791327913,
      "step": 12323,
      "training_loss": 8.101632118225098
    },
    {
      "epoch": 0.6679674796747967,
      "grad_norm": 18.42561149597168,
      "learning_rate": 1e-05,
      "loss": 6.7417,
      "step": 12324
    },
    {
      "epoch": 0.6679674796747967,
      "step": 12324,
      "training_loss": 5.318877696990967
    },
    {
      "epoch": 0.6680216802168022,
      "step": 12325,
      "training_loss": 5.952408313751221
    },
    {
      "epoch": 0.6680758807588076,
      "step": 12326,
      "training_loss": 5.6060943603515625
    },
    {
      "epoch": 0.668130081300813,
      "step": 12327,
      "training_loss": 5.285671710968018
    },
    {
      "epoch": 0.6681842818428184,
      "grad_norm": 32.771060943603516,
      "learning_rate": 1e-05,
      "loss": 5.5408,
      "step": 12328
    },
    {
      "epoch": 0.6681842818428184,
      "step": 12328,
      "training_loss": 6.462559700012207
    },
    {
      "epoch": 0.6682384823848239,
      "step": 12329,
      "training_loss": 6.993963718414307
    },
    {
      "epoch": 0.6682926829268293,
      "step": 12330,
      "training_loss": 5.174012660980225
    },
    {
      "epoch": 0.6683468834688346,
      "step": 12331,
      "training_loss": 5.807413101196289
    },
    {
      "epoch": 0.6684010840108401,
      "grad_norm": 31.659568786621094,
      "learning_rate": 1e-05,
      "loss": 6.1095,
      "step": 12332
    },
    {
      "epoch": 0.6684010840108401,
      "step": 12332,
      "training_loss": 5.706051826477051
    },
    {
      "epoch": 0.6684552845528455,
      "step": 12333,
      "training_loss": 6.975284576416016
    },
    {
      "epoch": 0.668509485094851,
      "step": 12334,
      "training_loss": 7.7983717918396
    },
    {
      "epoch": 0.6685636856368564,
      "step": 12335,
      "training_loss": 5.226318359375
    },
    {
      "epoch": 0.6686178861788618,
      "grad_norm": 28.887697219848633,
      "learning_rate": 1e-05,
      "loss": 6.4265,
      "step": 12336
    },
    {
      "epoch": 0.6686178861788618,
      "step": 12336,
      "training_loss": 5.180405616760254
    },
    {
      "epoch": 0.6686720867208672,
      "step": 12337,
      "training_loss": 6.640663146972656
    },
    {
      "epoch": 0.6687262872628726,
      "step": 12338,
      "training_loss": 4.1091179847717285
    },
    {
      "epoch": 0.6687804878048781,
      "step": 12339,
      "training_loss": 4.466776371002197
    },
    {
      "epoch": 0.6688346883468834,
      "grad_norm": 56.89594268798828,
      "learning_rate": 1e-05,
      "loss": 5.0992,
      "step": 12340
    },
    {
      "epoch": 0.6688346883468834,
      "step": 12340,
      "training_loss": 7.327725887298584
    },
    {
      "epoch": 0.6688888888888889,
      "step": 12341,
      "training_loss": 7.809878826141357
    },
    {
      "epoch": 0.6689430894308943,
      "step": 12342,
      "training_loss": 5.104142665863037
    },
    {
      "epoch": 0.6689972899728998,
      "step": 12343,
      "training_loss": 4.665056228637695
    },
    {
      "epoch": 0.6690514905149052,
      "grad_norm": 25.011634826660156,
      "learning_rate": 1e-05,
      "loss": 6.2267,
      "step": 12344
    },
    {
      "epoch": 0.6690514905149052,
      "step": 12344,
      "training_loss": 8.167989730834961
    },
    {
      "epoch": 0.6691056910569105,
      "step": 12345,
      "training_loss": 7.518285274505615
    },
    {
      "epoch": 0.669159891598916,
      "step": 12346,
      "training_loss": 4.183043003082275
    },
    {
      "epoch": 0.6692140921409214,
      "step": 12347,
      "training_loss": 8.231062889099121
    },
    {
      "epoch": 0.6692682926829269,
      "grad_norm": 28.40747833251953,
      "learning_rate": 1e-05,
      "loss": 7.0251,
      "step": 12348
    },
    {
      "epoch": 0.6692682926829269,
      "step": 12348,
      "training_loss": 6.9437031745910645
    },
    {
      "epoch": 0.6693224932249322,
      "step": 12349,
      "training_loss": 6.179830074310303
    },
    {
      "epoch": 0.6693766937669376,
      "step": 12350,
      "training_loss": 7.223763465881348
    },
    {
      "epoch": 0.6694308943089431,
      "step": 12351,
      "training_loss": 5.677229881286621
    },
    {
      "epoch": 0.6694850948509485,
      "grad_norm": 24.55327606201172,
      "learning_rate": 1e-05,
      "loss": 6.5061,
      "step": 12352
    },
    {
      "epoch": 0.6694850948509485,
      "step": 12352,
      "training_loss": 4.352897644042969
    },
    {
      "epoch": 0.669539295392954,
      "step": 12353,
      "training_loss": 7.5911054611206055
    },
    {
      "epoch": 0.6695934959349593,
      "step": 12354,
      "training_loss": 5.690586090087891
    },
    {
      "epoch": 0.6696476964769648,
      "step": 12355,
      "training_loss": 6.667389392852783
    },
    {
      "epoch": 0.6697018970189702,
      "grad_norm": 17.70138931274414,
      "learning_rate": 1e-05,
      "loss": 6.0755,
      "step": 12356
    },
    {
      "epoch": 0.6697018970189702,
      "step": 12356,
      "training_loss": 6.544467449188232
    },
    {
      "epoch": 0.6697560975609756,
      "step": 12357,
      "training_loss": 6.174143314361572
    },
    {
      "epoch": 0.669810298102981,
      "step": 12358,
      "training_loss": 7.585560321807861
    },
    {
      "epoch": 0.6698644986449864,
      "step": 12359,
      "training_loss": 7.0813307762146
    },
    {
      "epoch": 0.6699186991869919,
      "grad_norm": 20.73468780517578,
      "learning_rate": 1e-05,
      "loss": 6.8464,
      "step": 12360
    },
    {
      "epoch": 0.6699186991869919,
      "step": 12360,
      "training_loss": 7.104363918304443
    },
    {
      "epoch": 0.6699728997289973,
      "step": 12361,
      "training_loss": 5.977344512939453
    },
    {
      "epoch": 0.6700271002710028,
      "step": 12362,
      "training_loss": 6.537451267242432
    },
    {
      "epoch": 0.6700813008130081,
      "step": 12363,
      "training_loss": 8.241157531738281
    },
    {
      "epoch": 0.6701355013550135,
      "grad_norm": 34.14807891845703,
      "learning_rate": 1e-05,
      "loss": 6.9651,
      "step": 12364
    },
    {
      "epoch": 0.6701355013550135,
      "step": 12364,
      "training_loss": 3.6898562908172607
    },
    {
      "epoch": 0.670189701897019,
      "step": 12365,
      "training_loss": 6.492168426513672
    },
    {
      "epoch": 0.6702439024390244,
      "step": 12366,
      "training_loss": 6.386215686798096
    },
    {
      "epoch": 0.6702981029810298,
      "step": 12367,
      "training_loss": 6.908220291137695
    },
    {
      "epoch": 0.6703523035230352,
      "grad_norm": 30.343706130981445,
      "learning_rate": 1e-05,
      "loss": 5.8691,
      "step": 12368
    },
    {
      "epoch": 0.6703523035230352,
      "step": 12368,
      "training_loss": 6.330124378204346
    },
    {
      "epoch": 0.6704065040650407,
      "step": 12369,
      "training_loss": 7.202993392944336
    },
    {
      "epoch": 0.6704607046070461,
      "step": 12370,
      "training_loss": 7.447999477386475
    },
    {
      "epoch": 0.6705149051490515,
      "step": 12371,
      "training_loss": 5.752137184143066
    },
    {
      "epoch": 0.6705691056910569,
      "grad_norm": 28.17092514038086,
      "learning_rate": 1e-05,
      "loss": 6.6833,
      "step": 12372
    },
    {
      "epoch": 0.6705691056910569,
      "step": 12372,
      "training_loss": 6.224826812744141
    },
    {
      "epoch": 0.6706233062330623,
      "step": 12373,
      "training_loss": 4.393581867218018
    },
    {
      "epoch": 0.6706775067750678,
      "step": 12374,
      "training_loss": 6.715061664581299
    },
    {
      "epoch": 0.6707317073170732,
      "step": 12375,
      "training_loss": 5.316614627838135
    },
    {
      "epoch": 0.6707859078590785,
      "grad_norm": 46.13947296142578,
      "learning_rate": 1e-05,
      "loss": 5.6625,
      "step": 12376
    },
    {
      "epoch": 0.6707859078590785,
      "step": 12376,
      "training_loss": 7.6073832511901855
    },
    {
      "epoch": 0.670840108401084,
      "step": 12377,
      "training_loss": 2.734938621520996
    },
    {
      "epoch": 0.6708943089430894,
      "step": 12378,
      "training_loss": 7.083456039428711
    },
    {
      "epoch": 0.6709485094850949,
      "step": 12379,
      "training_loss": 6.966272830963135
    },
    {
      "epoch": 0.6710027100271003,
      "grad_norm": 25.22105598449707,
      "learning_rate": 1e-05,
      "loss": 6.098,
      "step": 12380
    },
    {
      "epoch": 0.6710027100271003,
      "step": 12380,
      "training_loss": 6.712271690368652
    },
    {
      "epoch": 0.6710569105691057,
      "step": 12381,
      "training_loss": 6.998368263244629
    },
    {
      "epoch": 0.6711111111111111,
      "step": 12382,
      "training_loss": 6.45428466796875
    },
    {
      "epoch": 0.6711653116531165,
      "step": 12383,
      "training_loss": 7.341285228729248
    },
    {
      "epoch": 0.671219512195122,
      "grad_norm": 43.11051559448242,
      "learning_rate": 1e-05,
      "loss": 6.8766,
      "step": 12384
    },
    {
      "epoch": 0.671219512195122,
      "step": 12384,
      "training_loss": 5.1250224113464355
    },
    {
      "epoch": 0.6712737127371273,
      "step": 12385,
      "training_loss": 6.322230815887451
    },
    {
      "epoch": 0.6713279132791328,
      "step": 12386,
      "training_loss": 7.920860290527344
    },
    {
      "epoch": 0.6713821138211382,
      "step": 12387,
      "training_loss": 7.379166603088379
    },
    {
      "epoch": 0.6714363143631437,
      "grad_norm": 18.963958740234375,
      "learning_rate": 1e-05,
      "loss": 6.6868,
      "step": 12388
    },
    {
      "epoch": 0.6714363143631437,
      "step": 12388,
      "training_loss": 4.9119720458984375
    },
    {
      "epoch": 0.6714905149051491,
      "step": 12389,
      "training_loss": 7.730815887451172
    },
    {
      "epoch": 0.6715447154471544,
      "step": 12390,
      "training_loss": 6.6263017654418945
    },
    {
      "epoch": 0.6715989159891599,
      "step": 12391,
      "training_loss": 5.886539459228516
    },
    {
      "epoch": 0.6716531165311653,
      "grad_norm": 56.227787017822266,
      "learning_rate": 1e-05,
      "loss": 6.2889,
      "step": 12392
    },
    {
      "epoch": 0.6716531165311653,
      "step": 12392,
      "training_loss": 5.767474174499512
    },
    {
      "epoch": 0.6717073170731708,
      "step": 12393,
      "training_loss": 7.356637477874756
    },
    {
      "epoch": 0.6717615176151761,
      "step": 12394,
      "training_loss": 7.263425827026367
    },
    {
      "epoch": 0.6718157181571816,
      "step": 12395,
      "training_loss": 3.814335823059082
    },
    {
      "epoch": 0.671869918699187,
      "grad_norm": 24.67853546142578,
      "learning_rate": 1e-05,
      "loss": 6.0505,
      "step": 12396
    },
    {
      "epoch": 0.671869918699187,
      "step": 12396,
      "training_loss": 7.231088161468506
    },
    {
      "epoch": 0.6719241192411924,
      "step": 12397,
      "training_loss": 6.879384994506836
    },
    {
      "epoch": 0.6719783197831979,
      "step": 12398,
      "training_loss": 7.35855770111084
    },
    {
      "epoch": 0.6720325203252032,
      "step": 12399,
      "training_loss": 8.094094276428223
    },
    {
      "epoch": 0.6720867208672087,
      "grad_norm": 31.73497772216797,
      "learning_rate": 1e-05,
      "loss": 7.3908,
      "step": 12400
    },
    {
      "epoch": 0.6720867208672087,
      "step": 12400,
      "training_loss": 6.852869510650635
    },
    {
      "epoch": 0.6721409214092141,
      "step": 12401,
      "training_loss": 6.894903659820557
    },
    {
      "epoch": 0.6721951219512196,
      "step": 12402,
      "training_loss": 7.337772369384766
    },
    {
      "epoch": 0.6722493224932249,
      "step": 12403,
      "training_loss": 8.496577262878418
    },
    {
      "epoch": 0.6723035230352303,
      "grad_norm": 39.34991455078125,
      "learning_rate": 1e-05,
      "loss": 7.3955,
      "step": 12404
    },
    {
      "epoch": 0.6723035230352303,
      "step": 12404,
      "training_loss": 4.92313814163208
    },
    {
      "epoch": 0.6723577235772358,
      "step": 12405,
      "training_loss": 5.7436418533325195
    },
    {
      "epoch": 0.6724119241192412,
      "step": 12406,
      "training_loss": 7.551586151123047
    },
    {
      "epoch": 0.6724661246612467,
      "step": 12407,
      "training_loss": 7.398898124694824
    },
    {
      "epoch": 0.672520325203252,
      "grad_norm": 22.320579528808594,
      "learning_rate": 1e-05,
      "loss": 6.4043,
      "step": 12408
    },
    {
      "epoch": 0.672520325203252,
      "step": 12408,
      "training_loss": 4.247600555419922
    },
    {
      "epoch": 0.6725745257452574,
      "step": 12409,
      "training_loss": 8.212348937988281
    },
    {
      "epoch": 0.6726287262872629,
      "step": 12410,
      "training_loss": 6.239215850830078
    },
    {
      "epoch": 0.6726829268292683,
      "step": 12411,
      "training_loss": 6.139486312866211
    },
    {
      "epoch": 0.6727371273712737,
      "grad_norm": 32.39600372314453,
      "learning_rate": 1e-05,
      "loss": 6.2097,
      "step": 12412
    },
    {
      "epoch": 0.6727371273712737,
      "step": 12412,
      "training_loss": 7.275448322296143
    },
    {
      "epoch": 0.6727913279132791,
      "step": 12413,
      "training_loss": 6.231884479522705
    },
    {
      "epoch": 0.6728455284552846,
      "step": 12414,
      "training_loss": 6.446459770202637
    },
    {
      "epoch": 0.67289972899729,
      "step": 12415,
      "training_loss": 8.058646202087402
    },
    {
      "epoch": 0.6729539295392954,
      "grad_norm": 34.43191146850586,
      "learning_rate": 1e-05,
      "loss": 7.0031,
      "step": 12416
    },
    {
      "epoch": 0.6729539295392954,
      "step": 12416,
      "training_loss": 7.123927593231201
    },
    {
      "epoch": 0.6730081300813008,
      "step": 12417,
      "training_loss": 7.311277389526367
    },
    {
      "epoch": 0.6730623306233062,
      "step": 12418,
      "training_loss": 7.106860160827637
    },
    {
      "epoch": 0.6731165311653117,
      "step": 12419,
      "training_loss": 6.010794162750244
    },
    {
      "epoch": 0.6731707317073171,
      "grad_norm": 26.301729202270508,
      "learning_rate": 1e-05,
      "loss": 6.8882,
      "step": 12420
    },
    {
      "epoch": 0.6731707317073171,
      "step": 12420,
      "training_loss": 7.2990522384643555
    },
    {
      "epoch": 0.6732249322493224,
      "step": 12421,
      "training_loss": 8.056241035461426
    },
    {
      "epoch": 0.6732791327913279,
      "step": 12422,
      "training_loss": 6.434703826904297
    },
    {
      "epoch": 0.6733333333333333,
      "step": 12423,
      "training_loss": 6.419241905212402
    },
    {
      "epoch": 0.6733875338753388,
      "grad_norm": 37.89824295043945,
      "learning_rate": 1e-05,
      "loss": 7.0523,
      "step": 12424
    },
    {
      "epoch": 0.6733875338753388,
      "step": 12424,
      "training_loss": 6.977042198181152
    },
    {
      "epoch": 0.6734417344173442,
      "step": 12425,
      "training_loss": 7.848556995391846
    },
    {
      "epoch": 0.6734959349593496,
      "step": 12426,
      "training_loss": 5.671424388885498
    },
    {
      "epoch": 0.673550135501355,
      "step": 12427,
      "training_loss": 6.129202365875244
    },
    {
      "epoch": 0.6736043360433605,
      "grad_norm": 27.975542068481445,
      "learning_rate": 1e-05,
      "loss": 6.6566,
      "step": 12428
    },
    {
      "epoch": 0.6736043360433605,
      "step": 12428,
      "training_loss": 6.337390899658203
    },
    {
      "epoch": 0.6736585365853659,
      "step": 12429,
      "training_loss": 3.8484232425689697
    },
    {
      "epoch": 0.6737127371273712,
      "step": 12430,
      "training_loss": 8.08024787902832
    },
    {
      "epoch": 0.6737669376693767,
      "step": 12431,
      "training_loss": 7.1912617683410645
    },
    {
      "epoch": 0.6738211382113821,
      "grad_norm": 19.988224029541016,
      "learning_rate": 1e-05,
      "loss": 6.3643,
      "step": 12432
    },
    {
      "epoch": 0.6738211382113821,
      "step": 12432,
      "training_loss": 7.395853042602539
    },
    {
      "epoch": 0.6738753387533876,
      "step": 12433,
      "training_loss": 6.6544508934021
    },
    {
      "epoch": 0.6739295392953929,
      "step": 12434,
      "training_loss": 5.411102771759033
    },
    {
      "epoch": 0.6739837398373983,
      "step": 12435,
      "training_loss": 5.989246368408203
    },
    {
      "epoch": 0.6740379403794038,
      "grad_norm": 29.812299728393555,
      "learning_rate": 1e-05,
      "loss": 6.3627,
      "step": 12436
    },
    {
      "epoch": 0.6740379403794038,
      "step": 12436,
      "training_loss": 6.518233776092529
    },
    {
      "epoch": 0.6740921409214092,
      "step": 12437,
      "training_loss": 7.334466934204102
    },
    {
      "epoch": 0.6741463414634147,
      "step": 12438,
      "training_loss": 7.752506732940674
    },
    {
      "epoch": 0.67420054200542,
      "step": 12439,
      "training_loss": 8.244555473327637
    },
    {
      "epoch": 0.6742547425474255,
      "grad_norm": 47.214351654052734,
      "learning_rate": 1e-05,
      "loss": 7.4624,
      "step": 12440
    },
    {
      "epoch": 0.6742547425474255,
      "step": 12440,
      "training_loss": 5.465368747711182
    },
    {
      "epoch": 0.6743089430894309,
      "step": 12441,
      "training_loss": 6.358125686645508
    },
    {
      "epoch": 0.6743631436314363,
      "step": 12442,
      "training_loss": 7.216175556182861
    },
    {
      "epoch": 0.6744173441734417,
      "step": 12443,
      "training_loss": 7.070817470550537
    },
    {
      "epoch": 0.6744715447154471,
      "grad_norm": 46.75324630737305,
      "learning_rate": 1e-05,
      "loss": 6.5276,
      "step": 12444
    },
    {
      "epoch": 0.6744715447154471,
      "step": 12444,
      "training_loss": 7.516727924346924
    },
    {
      "epoch": 0.6745257452574526,
      "step": 12445,
      "training_loss": 6.8405375480651855
    },
    {
      "epoch": 0.674579945799458,
      "step": 12446,
      "training_loss": 7.2951788902282715
    },
    {
      "epoch": 0.6746341463414635,
      "step": 12447,
      "training_loss": 7.306110382080078
    },
    {
      "epoch": 0.6746883468834688,
      "grad_norm": 20.23679542541504,
      "learning_rate": 1e-05,
      "loss": 7.2396,
      "step": 12448
    },
    {
      "epoch": 0.6746883468834688,
      "step": 12448,
      "training_loss": 7.100123405456543
    },
    {
      "epoch": 0.6747425474254742,
      "step": 12449,
      "training_loss": 6.640074253082275
    },
    {
      "epoch": 0.6747967479674797,
      "step": 12450,
      "training_loss": 5.642834186553955
    },
    {
      "epoch": 0.6748509485094851,
      "step": 12451,
      "training_loss": 6.570190906524658
    },
    {
      "epoch": 0.6749051490514905,
      "grad_norm": 22.477577209472656,
      "learning_rate": 1e-05,
      "loss": 6.4883,
      "step": 12452
    },
    {
      "epoch": 0.6749051490514905,
      "step": 12452,
      "training_loss": 6.608280181884766
    },
    {
      "epoch": 0.6749593495934959,
      "step": 12453,
      "training_loss": 7.386697769165039
    },
    {
      "epoch": 0.6750135501355014,
      "step": 12454,
      "training_loss": 6.118990421295166
    },
    {
      "epoch": 0.6750677506775068,
      "step": 12455,
      "training_loss": 7.261624336242676
    },
    {
      "epoch": 0.6751219512195122,
      "grad_norm": 42.249549865722656,
      "learning_rate": 1e-05,
      "loss": 6.8439,
      "step": 12456
    },
    {
      "epoch": 0.6751219512195122,
      "step": 12456,
      "training_loss": 6.857391834259033
    },
    {
      "epoch": 0.6751761517615176,
      "step": 12457,
      "training_loss": 6.725870132446289
    },
    {
      "epoch": 0.675230352303523,
      "step": 12458,
      "training_loss": 6.607303619384766
    },
    {
      "epoch": 0.6752845528455285,
      "step": 12459,
      "training_loss": 3.9575188159942627
    },
    {
      "epoch": 0.6753387533875339,
      "grad_norm": 20.87831687927246,
      "learning_rate": 1e-05,
      "loss": 6.037,
      "step": 12460
    },
    {
      "epoch": 0.6753387533875339,
      "step": 12460,
      "training_loss": 4.3965983390808105
    },
    {
      "epoch": 0.6753929539295392,
      "step": 12461,
      "training_loss": 6.863840579986572
    },
    {
      "epoch": 0.6754471544715447,
      "step": 12462,
      "training_loss": 6.492398738861084
    },
    {
      "epoch": 0.6755013550135501,
      "step": 12463,
      "training_loss": 7.508023738861084
    },
    {
      "epoch": 0.6755555555555556,
      "grad_norm": 20.67643165588379,
      "learning_rate": 1e-05,
      "loss": 6.3152,
      "step": 12464
    },
    {
      "epoch": 0.6755555555555556,
      "step": 12464,
      "training_loss": 6.052778720855713
    },
    {
      "epoch": 0.675609756097561,
      "step": 12465,
      "training_loss": 7.105200290679932
    },
    {
      "epoch": 0.6756639566395664,
      "step": 12466,
      "training_loss": 6.442368507385254
    },
    {
      "epoch": 0.6757181571815718,
      "step": 12467,
      "training_loss": 6.878592491149902
    },
    {
      "epoch": 0.6757723577235772,
      "grad_norm": 20.418092727661133,
      "learning_rate": 1e-05,
      "loss": 6.6197,
      "step": 12468
    },
    {
      "epoch": 0.6757723577235772,
      "step": 12468,
      "training_loss": 6.746859550476074
    },
    {
      "epoch": 0.6758265582655827,
      "step": 12469,
      "training_loss": 5.752893924713135
    },
    {
      "epoch": 0.675880758807588,
      "step": 12470,
      "training_loss": 6.909717559814453
    },
    {
      "epoch": 0.6759349593495935,
      "step": 12471,
      "training_loss": 6.559352397918701
    },
    {
      "epoch": 0.6759891598915989,
      "grad_norm": 16.495397567749023,
      "learning_rate": 1e-05,
      "loss": 6.4922,
      "step": 12472
    },
    {
      "epoch": 0.6759891598915989,
      "step": 12472,
      "training_loss": 5.082037448883057
    },
    {
      "epoch": 0.6760433604336044,
      "step": 12473,
      "training_loss": 6.178670883178711
    },
    {
      "epoch": 0.6760975609756098,
      "step": 12474,
      "training_loss": 6.889180660247803
    },
    {
      "epoch": 0.6761517615176151,
      "step": 12475,
      "training_loss": 6.608263969421387
    },
    {
      "epoch": 0.6762059620596206,
      "grad_norm": 19.710851669311523,
      "learning_rate": 1e-05,
      "loss": 6.1895,
      "step": 12476
    },
    {
      "epoch": 0.6762059620596206,
      "step": 12476,
      "training_loss": 7.822553634643555
    },
    {
      "epoch": 0.676260162601626,
      "step": 12477,
      "training_loss": 5.115542888641357
    },
    {
      "epoch": 0.6763143631436315,
      "step": 12478,
      "training_loss": 6.580297946929932
    },
    {
      "epoch": 0.6763685636856368,
      "step": 12479,
      "training_loss": 7.256064414978027
    },
    {
      "epoch": 0.6764227642276422,
      "grad_norm": 39.17287063598633,
      "learning_rate": 1e-05,
      "loss": 6.6936,
      "step": 12480
    },
    {
      "epoch": 0.6764227642276422,
      "step": 12480,
      "training_loss": 6.704174995422363
    },
    {
      "epoch": 0.6764769647696477,
      "step": 12481,
      "training_loss": 3.42211651802063
    },
    {
      "epoch": 0.6765311653116531,
      "step": 12482,
      "training_loss": 6.443594932556152
    },
    {
      "epoch": 0.6765853658536586,
      "step": 12483,
      "training_loss": 5.934043884277344
    },
    {
      "epoch": 0.6766395663956639,
      "grad_norm": 35.15264129638672,
      "learning_rate": 1e-05,
      "loss": 5.626,
      "step": 12484
    },
    {
      "epoch": 0.6766395663956639,
      "step": 12484,
      "training_loss": 6.961653232574463
    },
    {
      "epoch": 0.6766937669376694,
      "step": 12485,
      "training_loss": 6.783079624176025
    },
    {
      "epoch": 0.6767479674796748,
      "step": 12486,
      "training_loss": 6.0785746574401855
    },
    {
      "epoch": 0.6768021680216803,
      "step": 12487,
      "training_loss": 6.472824573516846
    },
    {
      "epoch": 0.6768563685636856,
      "grad_norm": 39.24912643432617,
      "learning_rate": 1e-05,
      "loss": 6.574,
      "step": 12488
    },
    {
      "epoch": 0.6768563685636856,
      "step": 12488,
      "training_loss": 6.774665355682373
    },
    {
      "epoch": 0.676910569105691,
      "step": 12489,
      "training_loss": 6.667572975158691
    },
    {
      "epoch": 0.6769647696476965,
      "step": 12490,
      "training_loss": 6.26828670501709
    },
    {
      "epoch": 0.6770189701897019,
      "step": 12491,
      "training_loss": 6.593247413635254
    },
    {
      "epoch": 0.6770731707317074,
      "grad_norm": 29.2764892578125,
      "learning_rate": 1e-05,
      "loss": 6.5759,
      "step": 12492
    },
    {
      "epoch": 0.6770731707317074,
      "step": 12492,
      "training_loss": 6.126916408538818
    },
    {
      "epoch": 0.6771273712737127,
      "step": 12493,
      "training_loss": 9.141022682189941
    },
    {
      "epoch": 0.6771815718157181,
      "step": 12494,
      "training_loss": 7.518472671508789
    },
    {
      "epoch": 0.6772357723577236,
      "step": 12495,
      "training_loss": 7.361902236938477
    },
    {
      "epoch": 0.677289972899729,
      "grad_norm": 47.51627731323242,
      "learning_rate": 1e-05,
      "loss": 7.5371,
      "step": 12496
    },
    {
      "epoch": 0.677289972899729,
      "step": 12496,
      "training_loss": 5.991299629211426
    },
    {
      "epoch": 0.6773441734417344,
      "step": 12497,
      "training_loss": 6.307281494140625
    },
    {
      "epoch": 0.6773983739837398,
      "step": 12498,
      "training_loss": 6.93682861328125
    },
    {
      "epoch": 0.6774525745257453,
      "step": 12499,
      "training_loss": 6.622730731964111
    },
    {
      "epoch": 0.6775067750677507,
      "grad_norm": 40.577178955078125,
      "learning_rate": 1e-05,
      "loss": 6.4645,
      "step": 12500
    },
    {
      "epoch": 0.6775067750677507,
      "step": 12500,
      "training_loss": 6.484772205352783
    },
    {
      "epoch": 0.6775609756097561,
      "step": 12501,
      "training_loss": 6.15651273727417
    },
    {
      "epoch": 0.6776151761517615,
      "step": 12502,
      "training_loss": 2.8609254360198975
    },
    {
      "epoch": 0.6776693766937669,
      "step": 12503,
      "training_loss": 5.450444221496582
    },
    {
      "epoch": 0.6777235772357724,
      "grad_norm": 35.934059143066406,
      "learning_rate": 1e-05,
      "loss": 5.2382,
      "step": 12504
    },
    {
      "epoch": 0.6777235772357724,
      "step": 12504,
      "training_loss": 6.54667854309082
    },
    {
      "epoch": 0.6777777777777778,
      "step": 12505,
      "training_loss": 5.7149763107299805
    },
    {
      "epoch": 0.6778319783197831,
      "step": 12506,
      "training_loss": 7.919042587280273
    },
    {
      "epoch": 0.6778861788617886,
      "step": 12507,
      "training_loss": 7.003326892852783
    },
    {
      "epoch": 0.677940379403794,
      "grad_norm": 39.080970764160156,
      "learning_rate": 1e-05,
      "loss": 6.796,
      "step": 12508
    },
    {
      "epoch": 0.677940379403794,
      "step": 12508,
      "training_loss": 7.341238975524902
    },
    {
      "epoch": 0.6779945799457995,
      "step": 12509,
      "training_loss": 7.62080192565918
    },
    {
      "epoch": 0.6780487804878049,
      "step": 12510,
      "training_loss": 7.307882785797119
    },
    {
      "epoch": 0.6781029810298103,
      "step": 12511,
      "training_loss": 7.026442050933838
    },
    {
      "epoch": 0.6781571815718157,
      "grad_norm": 46.92534637451172,
      "learning_rate": 1e-05,
      "loss": 7.3241,
      "step": 12512
    },
    {
      "epoch": 0.6781571815718157,
      "step": 12512,
      "training_loss": 7.782329559326172
    },
    {
      "epoch": 0.6782113821138211,
      "step": 12513,
      "training_loss": 6.548418998718262
    },
    {
      "epoch": 0.6782655826558266,
      "step": 12514,
      "training_loss": 7.685365676879883
    },
    {
      "epoch": 0.6783197831978319,
      "step": 12515,
      "training_loss": 4.023216724395752
    },
    {
      "epoch": 0.6783739837398374,
      "grad_norm": 44.495731353759766,
      "learning_rate": 1e-05,
      "loss": 6.5098,
      "step": 12516
    },
    {
      "epoch": 0.6783739837398374,
      "step": 12516,
      "training_loss": 7.371462821960449
    },
    {
      "epoch": 0.6784281842818428,
      "step": 12517,
      "training_loss": 6.454713821411133
    },
    {
      "epoch": 0.6784823848238483,
      "step": 12518,
      "training_loss": 6.660279273986816
    },
    {
      "epoch": 0.6785365853658537,
      "step": 12519,
      "training_loss": 9.119770050048828
    },
    {
      "epoch": 0.678590785907859,
      "grad_norm": 33.77470397949219,
      "learning_rate": 1e-05,
      "loss": 7.4016,
      "step": 12520
    },
    {
      "epoch": 0.678590785907859,
      "step": 12520,
      "training_loss": 7.026594161987305
    },
    {
      "epoch": 0.6786449864498645,
      "step": 12521,
      "training_loss": 7.829832077026367
    },
    {
      "epoch": 0.6786991869918699,
      "step": 12522,
      "training_loss": 7.055701732635498
    },
    {
      "epoch": 0.6787533875338754,
      "step": 12523,
      "training_loss": 7.327866554260254
    },
    {
      "epoch": 0.6788075880758807,
      "grad_norm": 17.86448860168457,
      "learning_rate": 1e-05,
      "loss": 7.31,
      "step": 12524
    },
    {
      "epoch": 0.6788075880758807,
      "step": 12524,
      "training_loss": 6.376633644104004
    },
    {
      "epoch": 0.6788617886178862,
      "step": 12525,
      "training_loss": 7.132798194885254
    },
    {
      "epoch": 0.6789159891598916,
      "step": 12526,
      "training_loss": 7.48317813873291
    },
    {
      "epoch": 0.678970189701897,
      "step": 12527,
      "training_loss": 7.141862392425537
    },
    {
      "epoch": 0.6790243902439025,
      "grad_norm": 30.309993743896484,
      "learning_rate": 1e-05,
      "loss": 7.0336,
      "step": 12528
    },
    {
      "epoch": 0.6790243902439025,
      "step": 12528,
      "training_loss": 7.303211212158203
    },
    {
      "epoch": 0.6790785907859078,
      "step": 12529,
      "training_loss": 7.361616134643555
    },
    {
      "epoch": 0.6791327913279133,
      "step": 12530,
      "training_loss": 6.173101902008057
    },
    {
      "epoch": 0.6791869918699187,
      "step": 12531,
      "training_loss": 6.133987903594971
    },
    {
      "epoch": 0.6792411924119242,
      "grad_norm": 31.551490783691406,
      "learning_rate": 1e-05,
      "loss": 6.743,
      "step": 12532
    },
    {
      "epoch": 0.6792411924119242,
      "step": 12532,
      "training_loss": 7.920530796051025
    },
    {
      "epoch": 0.6792953929539295,
      "step": 12533,
      "training_loss": 4.733178615570068
    },
    {
      "epoch": 0.6793495934959349,
      "step": 12534,
      "training_loss": 6.484689712524414
    },
    {
      "epoch": 0.6794037940379404,
      "step": 12535,
      "training_loss": 7.548410892486572
    },
    {
      "epoch": 0.6794579945799458,
      "grad_norm": 44.13583755493164,
      "learning_rate": 1e-05,
      "loss": 6.6717,
      "step": 12536
    },
    {
      "epoch": 0.6794579945799458,
      "step": 12536,
      "training_loss": 7.572288513183594
    },
    {
      "epoch": 0.6795121951219513,
      "step": 12537,
      "training_loss": 6.934198379516602
    },
    {
      "epoch": 0.6795663956639566,
      "step": 12538,
      "training_loss": 8.085867881774902
    },
    {
      "epoch": 0.679620596205962,
      "step": 12539,
      "training_loss": 6.913442611694336
    },
    {
      "epoch": 0.6796747967479675,
      "grad_norm": 21.486928939819336,
      "learning_rate": 1e-05,
      "loss": 7.3764,
      "step": 12540
    },
    {
      "epoch": 0.6796747967479675,
      "step": 12540,
      "training_loss": 7.037235260009766
    },
    {
      "epoch": 0.6797289972899729,
      "step": 12541,
      "training_loss": 6.55181884765625
    },
    {
      "epoch": 0.6797831978319783,
      "step": 12542,
      "training_loss": 6.726016998291016
    },
    {
      "epoch": 0.6798373983739837,
      "step": 12543,
      "training_loss": 7.337709426879883
    },
    {
      "epoch": 0.6798915989159892,
      "grad_norm": 21.346235275268555,
      "learning_rate": 1e-05,
      "loss": 6.9132,
      "step": 12544
    },
    {
      "epoch": 0.6798915989159892,
      "step": 12544,
      "training_loss": 6.451592922210693
    },
    {
      "epoch": 0.6799457994579946,
      "step": 12545,
      "training_loss": 6.916893482208252
    },
    {
      "epoch": 0.68,
      "step": 12546,
      "training_loss": 6.065831661224365
    },
    {
      "epoch": 0.6800542005420054,
      "step": 12547,
      "training_loss": 6.851455211639404
    },
    {
      "epoch": 0.6801084010840108,
      "grad_norm": 23.915433883666992,
      "learning_rate": 1e-05,
      "loss": 6.5714,
      "step": 12548
    },
    {
      "epoch": 0.6801084010840108,
      "step": 12548,
      "training_loss": 6.928933143615723
    },
    {
      "epoch": 0.6801626016260163,
      "step": 12549,
      "training_loss": 5.745715618133545
    },
    {
      "epoch": 0.6802168021680217,
      "step": 12550,
      "training_loss": 6.702484130859375
    },
    {
      "epoch": 0.680271002710027,
      "step": 12551,
      "training_loss": 7.423315525054932
    },
    {
      "epoch": 0.6803252032520325,
      "grad_norm": 28.228986740112305,
      "learning_rate": 1e-05,
      "loss": 6.7001,
      "step": 12552
    },
    {
      "epoch": 0.6803252032520325,
      "step": 12552,
      "training_loss": 4.821149826049805
    },
    {
      "epoch": 0.6803794037940379,
      "step": 12553,
      "training_loss": 5.899197578430176
    },
    {
      "epoch": 0.6804336043360434,
      "step": 12554,
      "training_loss": 8.238237380981445
    },
    {
      "epoch": 0.6804878048780488,
      "step": 12555,
      "training_loss": 7.636623382568359
    },
    {
      "epoch": 0.6805420054200542,
      "grad_norm": 30.446016311645508,
      "learning_rate": 1e-05,
      "loss": 6.6488,
      "step": 12556
    },
    {
      "epoch": 0.6805420054200542,
      "step": 12556,
      "training_loss": 6.434484481811523
    },
    {
      "epoch": 0.6805962059620596,
      "step": 12557,
      "training_loss": 6.359968185424805
    },
    {
      "epoch": 0.680650406504065,
      "step": 12558,
      "training_loss": 6.084774017333984
    },
    {
      "epoch": 0.6807046070460705,
      "step": 12559,
      "training_loss": 6.225277900695801
    },
    {
      "epoch": 0.6807588075880758,
      "grad_norm": 26.557138442993164,
      "learning_rate": 1e-05,
      "loss": 6.2761,
      "step": 12560
    },
    {
      "epoch": 0.6807588075880758,
      "step": 12560,
      "training_loss": 6.62100887298584
    },
    {
      "epoch": 0.6808130081300813,
      "step": 12561,
      "training_loss": 6.129751682281494
    },
    {
      "epoch": 0.6808672086720867,
      "step": 12562,
      "training_loss": 7.3778886795043945
    },
    {
      "epoch": 0.6809214092140922,
      "step": 12563,
      "training_loss": 6.282897472381592
    },
    {
      "epoch": 0.6809756097560976,
      "grad_norm": 20.756982803344727,
      "learning_rate": 1e-05,
      "loss": 6.6029,
      "step": 12564
    },
    {
      "epoch": 0.6809756097560976,
      "step": 12564,
      "training_loss": 7.523191928863525
    },
    {
      "epoch": 0.6810298102981029,
      "step": 12565,
      "training_loss": 5.75900936126709
    },
    {
      "epoch": 0.6810840108401084,
      "step": 12566,
      "training_loss": 7.207674980163574
    },
    {
      "epoch": 0.6811382113821138,
      "step": 12567,
      "training_loss": 7.702805519104004
    },
    {
      "epoch": 0.6811924119241193,
      "grad_norm": 24.834598541259766,
      "learning_rate": 1e-05,
      "loss": 7.0482,
      "step": 12568
    },
    {
      "epoch": 0.6811924119241193,
      "step": 12568,
      "training_loss": 5.871128082275391
    },
    {
      "epoch": 0.6812466124661246,
      "step": 12569,
      "training_loss": 5.233036518096924
    },
    {
      "epoch": 0.6813008130081301,
      "step": 12570,
      "training_loss": 6.640161991119385
    },
    {
      "epoch": 0.6813550135501355,
      "step": 12571,
      "training_loss": 4.5145182609558105
    },
    {
      "epoch": 0.681409214092141,
      "grad_norm": 48.56317901611328,
      "learning_rate": 1e-05,
      "loss": 5.5647,
      "step": 12572
    },
    {
      "epoch": 0.681409214092141,
      "step": 12572,
      "training_loss": 7.500434398651123
    },
    {
      "epoch": 0.6814634146341464,
      "step": 12573,
      "training_loss": 7.0367584228515625
    },
    {
      "epoch": 0.6815176151761517,
      "step": 12574,
      "training_loss": 6.845890522003174
    },
    {
      "epoch": 0.6815718157181572,
      "step": 12575,
      "training_loss": 7.096716403961182
    },
    {
      "epoch": 0.6816260162601626,
      "grad_norm": 34.54099655151367,
      "learning_rate": 1e-05,
      "loss": 7.1199,
      "step": 12576
    },
    {
      "epoch": 0.6816260162601626,
      "step": 12576,
      "training_loss": 5.989957809448242
    },
    {
      "epoch": 0.6816802168021681,
      "step": 12577,
      "training_loss": 7.367542266845703
    },
    {
      "epoch": 0.6817344173441734,
      "step": 12578,
      "training_loss": 7.019087791442871
    },
    {
      "epoch": 0.6817886178861788,
      "step": 12579,
      "training_loss": 7.249025344848633
    },
    {
      "epoch": 0.6818428184281843,
      "grad_norm": 23.250099182128906,
      "learning_rate": 1e-05,
      "loss": 6.9064,
      "step": 12580
    },
    {
      "epoch": 0.6818428184281843,
      "step": 12580,
      "training_loss": 4.495086669921875
    },
    {
      "epoch": 0.6818970189701897,
      "step": 12581,
      "training_loss": 6.907467365264893
    },
    {
      "epoch": 0.6819512195121952,
      "step": 12582,
      "training_loss": 6.921765327453613
    },
    {
      "epoch": 0.6820054200542005,
      "step": 12583,
      "training_loss": 7.3157854080200195
    },
    {
      "epoch": 0.682059620596206,
      "grad_norm": 28.04425048828125,
      "learning_rate": 1e-05,
      "loss": 6.41,
      "step": 12584
    },
    {
      "epoch": 0.682059620596206,
      "step": 12584,
      "training_loss": 6.7284369468688965
    },
    {
      "epoch": 0.6821138211382114,
      "step": 12585,
      "training_loss": 5.087554931640625
    },
    {
      "epoch": 0.6821680216802168,
      "step": 12586,
      "training_loss": 6.534377574920654
    },
    {
      "epoch": 0.6822222222222222,
      "step": 12587,
      "training_loss": 7.933455944061279
    },
    {
      "epoch": 0.6822764227642276,
      "grad_norm": 35.460243225097656,
      "learning_rate": 1e-05,
      "loss": 6.571,
      "step": 12588
    },
    {
      "epoch": 0.6822764227642276,
      "step": 12588,
      "training_loss": 6.437748432159424
    },
    {
      "epoch": 0.6823306233062331,
      "step": 12589,
      "training_loss": 6.699979782104492
    },
    {
      "epoch": 0.6823848238482385,
      "step": 12590,
      "training_loss": 6.342543601989746
    },
    {
      "epoch": 0.682439024390244,
      "step": 12591,
      "training_loss": 6.7218017578125
    },
    {
      "epoch": 0.6824932249322493,
      "grad_norm": 37.5239143371582,
      "learning_rate": 1e-05,
      "loss": 6.5505,
      "step": 12592
    },
    {
      "epoch": 0.6824932249322493,
      "step": 12592,
      "training_loss": 4.619539737701416
    },
    {
      "epoch": 0.6825474254742547,
      "step": 12593,
      "training_loss": 6.610567569732666
    },
    {
      "epoch": 0.6826016260162602,
      "step": 12594,
      "training_loss": 6.157721996307373
    },
    {
      "epoch": 0.6826558265582656,
      "step": 12595,
      "training_loss": 4.522501468658447
    },
    {
      "epoch": 0.682710027100271,
      "grad_norm": 31.358285903930664,
      "learning_rate": 1e-05,
      "loss": 5.4776,
      "step": 12596
    },
    {
      "epoch": 0.682710027100271,
      "step": 12596,
      "training_loss": 3.3080825805664062
    },
    {
      "epoch": 0.6827642276422764,
      "step": 12597,
      "training_loss": 7.239936828613281
    },
    {
      "epoch": 0.6828184281842818,
      "step": 12598,
      "training_loss": 5.266549110412598
    },
    {
      "epoch": 0.6828726287262873,
      "step": 12599,
      "training_loss": 7.404321670532227
    },
    {
      "epoch": 0.6829268292682927,
      "grad_norm": 28.990806579589844,
      "learning_rate": 1e-05,
      "loss": 5.8047,
      "step": 12600
    },
    {
      "epoch": 0.6829268292682927,
      "step": 12600,
      "training_loss": 7.351666450500488
    },
    {
      "epoch": 0.6829810298102981,
      "step": 12601,
      "training_loss": 3.008718252182007
    },
    {
      "epoch": 0.6830352303523035,
      "step": 12602,
      "training_loss": 6.425206184387207
    },
    {
      "epoch": 0.683089430894309,
      "step": 12603,
      "training_loss": 6.740747451782227
    },
    {
      "epoch": 0.6831436314363144,
      "grad_norm": 25.28802490234375,
      "learning_rate": 1e-05,
      "loss": 5.8816,
      "step": 12604
    },
    {
      "epoch": 0.6831436314363144,
      "step": 12604,
      "training_loss": 5.99962854385376
    },
    {
      "epoch": 0.6831978319783197,
      "step": 12605,
      "training_loss": 6.885694980621338
    },
    {
      "epoch": 0.6832520325203252,
      "step": 12606,
      "training_loss": 8.494466781616211
    },
    {
      "epoch": 0.6833062330623306,
      "step": 12607,
      "training_loss": 6.301234245300293
    },
    {
      "epoch": 0.6833604336043361,
      "grad_norm": 37.42616653442383,
      "learning_rate": 1e-05,
      "loss": 6.9203,
      "step": 12608
    },
    {
      "epoch": 0.6833604336043361,
      "step": 12608,
      "training_loss": 7.0764288902282715
    },
    {
      "epoch": 0.6834146341463415,
      "step": 12609,
      "training_loss": 8.700821876525879
    },
    {
      "epoch": 0.6834688346883468,
      "step": 12610,
      "training_loss": 6.213911056518555
    },
    {
      "epoch": 0.6835230352303523,
      "step": 12611,
      "training_loss": 7.463957786560059
    },
    {
      "epoch": 0.6835772357723577,
      "grad_norm": 18.770877838134766,
      "learning_rate": 1e-05,
      "loss": 7.3638,
      "step": 12612
    },
    {
      "epoch": 0.6835772357723577,
      "step": 12612,
      "training_loss": 8.102804183959961
    },
    {
      "epoch": 0.6836314363143632,
      "step": 12613,
      "training_loss": 4.141221046447754
    },
    {
      "epoch": 0.6836856368563685,
      "step": 12614,
      "training_loss": 7.705334663391113
    },
    {
      "epoch": 0.683739837398374,
      "step": 12615,
      "training_loss": 7.158436298370361
    },
    {
      "epoch": 0.6837940379403794,
      "grad_norm": 20.649812698364258,
      "learning_rate": 1e-05,
      "loss": 6.7769,
      "step": 12616
    },
    {
      "epoch": 0.6837940379403794,
      "step": 12616,
      "training_loss": 7.409890651702881
    },
    {
      "epoch": 0.6838482384823849,
      "step": 12617,
      "training_loss": 5.107034683227539
    },
    {
      "epoch": 0.6839024390243903,
      "step": 12618,
      "training_loss": 6.210723876953125
    },
    {
      "epoch": 0.6839566395663956,
      "step": 12619,
      "training_loss": 6.981339454650879
    },
    {
      "epoch": 0.6840108401084011,
      "grad_norm": 61.02737808227539,
      "learning_rate": 1e-05,
      "loss": 6.4272,
      "step": 12620
    },
    {
      "epoch": 0.6840108401084011,
      "step": 12620,
      "training_loss": 6.997228622436523
    },
    {
      "epoch": 0.6840650406504065,
      "step": 12621,
      "training_loss": 7.57397985458374
    },
    {
      "epoch": 0.684119241192412,
      "step": 12622,
      "training_loss": 6.935518741607666
    },
    {
      "epoch": 0.6841734417344173,
      "step": 12623,
      "training_loss": 7.936888694763184
    },
    {
      "epoch": 0.6842276422764227,
      "grad_norm": 53.257015228271484,
      "learning_rate": 1e-05,
      "loss": 7.3609,
      "step": 12624
    },
    {
      "epoch": 0.6842276422764227,
      "step": 12624,
      "training_loss": 6.68861722946167
    },
    {
      "epoch": 0.6842818428184282,
      "step": 12625,
      "training_loss": 5.811892032623291
    },
    {
      "epoch": 0.6843360433604336,
      "step": 12626,
      "training_loss": 6.7512006759643555
    },
    {
      "epoch": 0.6843902439024391,
      "step": 12627,
      "training_loss": 6.763140678405762
    },
    {
      "epoch": 0.6844444444444444,
      "grad_norm": 46.21691131591797,
      "learning_rate": 1e-05,
      "loss": 6.5037,
      "step": 12628
    },
    {
      "epoch": 0.6844444444444444,
      "step": 12628,
      "training_loss": 5.173440933227539
    },
    {
      "epoch": 0.6844986449864499,
      "step": 12629,
      "training_loss": 5.5447468757629395
    },
    {
      "epoch": 0.6845528455284553,
      "step": 12630,
      "training_loss": 6.1650071144104
    },
    {
      "epoch": 0.6846070460704607,
      "step": 12631,
      "training_loss": 5.966948509216309
    },
    {
      "epoch": 0.6846612466124661,
      "grad_norm": 27.966175079345703,
      "learning_rate": 1e-05,
      "loss": 5.7125,
      "step": 12632
    },
    {
      "epoch": 0.6846612466124661,
      "step": 12632,
      "training_loss": 4.975968837738037
    },
    {
      "epoch": 0.6847154471544715,
      "step": 12633,
      "training_loss": 7.4261555671691895
    },
    {
      "epoch": 0.684769647696477,
      "step": 12634,
      "training_loss": 5.969980239868164
    },
    {
      "epoch": 0.6848238482384824,
      "step": 12635,
      "training_loss": 5.127854824066162
    },
    {
      "epoch": 0.6848780487804879,
      "grad_norm": 27.95024871826172,
      "learning_rate": 1e-05,
      "loss": 5.875,
      "step": 12636
    },
    {
      "epoch": 0.6848780487804879,
      "step": 12636,
      "training_loss": 7.182829856872559
    },
    {
      "epoch": 0.6849322493224932,
      "step": 12637,
      "training_loss": 4.911135673522949
    },
    {
      "epoch": 0.6849864498644986,
      "step": 12638,
      "training_loss": 6.180042266845703
    },
    {
      "epoch": 0.6850406504065041,
      "step": 12639,
      "training_loss": 6.92695426940918
    },
    {
      "epoch": 0.6850948509485095,
      "grad_norm": 23.17049217224121,
      "learning_rate": 1e-05,
      "loss": 6.3002,
      "step": 12640
    },
    {
      "epoch": 0.6850948509485095,
      "step": 12640,
      "training_loss": 6.645145893096924
    },
    {
      "epoch": 0.6851490514905149,
      "step": 12641,
      "training_loss": 7.6290411949157715
    },
    {
      "epoch": 0.6852032520325203,
      "step": 12642,
      "training_loss": 7.3859052658081055
    },
    {
      "epoch": 0.6852574525745257,
      "step": 12643,
      "training_loss": 3.683634042739868
    },
    {
      "epoch": 0.6853116531165312,
      "grad_norm": 28.649656295776367,
      "learning_rate": 1e-05,
      "loss": 6.3359,
      "step": 12644
    },
    {
      "epoch": 0.6853116531165312,
      "step": 12644,
      "training_loss": 3.265850305557251
    },
    {
      "epoch": 0.6853658536585366,
      "step": 12645,
      "training_loss": 6.939016342163086
    },
    {
      "epoch": 0.685420054200542,
      "step": 12646,
      "training_loss": 7.416708469390869
    },
    {
      "epoch": 0.6854742547425474,
      "step": 12647,
      "training_loss": 6.234065532684326
    },
    {
      "epoch": 0.6855284552845529,
      "grad_norm": 19.36229133605957,
      "learning_rate": 1e-05,
      "loss": 5.9639,
      "step": 12648
    },
    {
      "epoch": 0.6855284552845529,
      "step": 12648,
      "training_loss": 6.638017654418945
    },
    {
      "epoch": 0.6855826558265583,
      "step": 12649,
      "training_loss": 6.004945278167725
    },
    {
      "epoch": 0.6856368563685636,
      "step": 12650,
      "training_loss": 7.5337395668029785
    },
    {
      "epoch": 0.6856910569105691,
      "step": 12651,
      "training_loss": 6.664027214050293
    },
    {
      "epoch": 0.6857452574525745,
      "grad_norm": 21.334854125976562,
      "learning_rate": 1e-05,
      "loss": 6.7102,
      "step": 12652
    },
    {
      "epoch": 0.6857452574525745,
      "step": 12652,
      "training_loss": 7.5828962326049805
    },
    {
      "epoch": 0.68579945799458,
      "step": 12653,
      "training_loss": 6.099898338317871
    },
    {
      "epoch": 0.6858536585365854,
      "step": 12654,
      "training_loss": 5.58544921875
    },
    {
      "epoch": 0.6859078590785908,
      "step": 12655,
      "training_loss": 7.595524311065674
    },
    {
      "epoch": 0.6859620596205962,
      "grad_norm": 23.793977737426758,
      "learning_rate": 1e-05,
      "loss": 6.7159,
      "step": 12656
    },
    {
      "epoch": 0.6859620596205962,
      "step": 12656,
      "training_loss": 7.681206226348877
    },
    {
      "epoch": 0.6860162601626016,
      "step": 12657,
      "training_loss": 6.644002914428711
    },
    {
      "epoch": 0.6860704607046071,
      "step": 12658,
      "training_loss": 5.98433780670166
    },
    {
      "epoch": 0.6861246612466124,
      "step": 12659,
      "training_loss": 6.644783973693848
    },
    {
      "epoch": 0.6861788617886179,
      "grad_norm": 34.26837158203125,
      "learning_rate": 1e-05,
      "loss": 6.7386,
      "step": 12660
    },
    {
      "epoch": 0.6861788617886179,
      "step": 12660,
      "training_loss": 6.668748378753662
    },
    {
      "epoch": 0.6862330623306233,
      "step": 12661,
      "training_loss": 6.140574932098389
    },
    {
      "epoch": 0.6862872628726288,
      "step": 12662,
      "training_loss": 7.151371955871582
    },
    {
      "epoch": 0.6863414634146342,
      "step": 12663,
      "training_loss": 6.714968204498291
    },
    {
      "epoch": 0.6863956639566395,
      "grad_norm": 28.306758880615234,
      "learning_rate": 1e-05,
      "loss": 6.6689,
      "step": 12664
    },
    {
      "epoch": 0.6863956639566395,
      "step": 12664,
      "training_loss": 7.559813976287842
    },
    {
      "epoch": 0.686449864498645,
      "step": 12665,
      "training_loss": 5.5721516609191895
    },
    {
      "epoch": 0.6865040650406504,
      "step": 12666,
      "training_loss": 6.543636322021484
    },
    {
      "epoch": 0.6865582655826559,
      "step": 12667,
      "training_loss": 6.996609687805176
    },
    {
      "epoch": 0.6866124661246612,
      "grad_norm": 24.87517738342285,
      "learning_rate": 1e-05,
      "loss": 6.6681,
      "step": 12668
    },
    {
      "epoch": 0.6866124661246612,
      "step": 12668,
      "training_loss": 7.577423572540283
    },
    {
      "epoch": 0.6866666666666666,
      "step": 12669,
      "training_loss": 7.892092227935791
    },
    {
      "epoch": 0.6867208672086721,
      "step": 12670,
      "training_loss": 7.131824970245361
    },
    {
      "epoch": 0.6867750677506775,
      "step": 12671,
      "training_loss": 6.972485065460205
    },
    {
      "epoch": 0.686829268292683,
      "grad_norm": 46.08615493774414,
      "learning_rate": 1e-05,
      "loss": 7.3935,
      "step": 12672
    },
    {
      "epoch": 0.686829268292683,
      "step": 12672,
      "training_loss": 7.490009307861328
    },
    {
      "epoch": 0.6868834688346883,
      "step": 12673,
      "training_loss": 7.008706092834473
    },
    {
      "epoch": 0.6869376693766938,
      "step": 12674,
      "training_loss": 7.980660438537598
    },
    {
      "epoch": 0.6869918699186992,
      "step": 12675,
      "training_loss": 6.64727783203125
    },
    {
      "epoch": 0.6870460704607046,
      "grad_norm": 19.952655792236328,
      "learning_rate": 1e-05,
      "loss": 7.2817,
      "step": 12676
    },
    {
      "epoch": 0.6870460704607046,
      "step": 12676,
      "training_loss": 5.101816654205322
    },
    {
      "epoch": 0.68710027100271,
      "step": 12677,
      "training_loss": 6.300053596496582
    },
    {
      "epoch": 0.6871544715447154,
      "step": 12678,
      "training_loss": 7.907440185546875
    },
    {
      "epoch": 0.6872086720867209,
      "step": 12679,
      "training_loss": 7.498847007751465
    },
    {
      "epoch": 0.6872628726287263,
      "grad_norm": 28.66148567199707,
      "learning_rate": 1e-05,
      "loss": 6.702,
      "step": 12680
    },
    {
      "epoch": 0.6872628726287263,
      "step": 12680,
      "training_loss": 4.6016154289245605
    },
    {
      "epoch": 0.6873170731707318,
      "step": 12681,
      "training_loss": 6.695167541503906
    },
    {
      "epoch": 0.6873712737127371,
      "step": 12682,
      "training_loss": 6.615034103393555
    },
    {
      "epoch": 0.6874254742547425,
      "step": 12683,
      "training_loss": 6.0855021476745605
    },
    {
      "epoch": 0.687479674796748,
      "grad_norm": 30.978832244873047,
      "learning_rate": 1e-05,
      "loss": 5.9993,
      "step": 12684
    },
    {
      "epoch": 0.687479674796748,
      "step": 12684,
      "training_loss": 7.02880334854126
    },
    {
      "epoch": 0.6875338753387534,
      "step": 12685,
      "training_loss": 7.063754081726074
    },
    {
      "epoch": 0.6875880758807588,
      "step": 12686,
      "training_loss": 5.867446422576904
    },
    {
      "epoch": 0.6876422764227642,
      "step": 12687,
      "training_loss": 7.068620204925537
    },
    {
      "epoch": 0.6876964769647697,
      "grad_norm": 17.948577880859375,
      "learning_rate": 1e-05,
      "loss": 6.7572,
      "step": 12688
    },
    {
      "epoch": 0.6876964769647697,
      "step": 12688,
      "training_loss": 8.276494979858398
    },
    {
      "epoch": 0.6877506775067751,
      "step": 12689,
      "training_loss": 3.981367826461792
    },
    {
      "epoch": 0.6878048780487804,
      "step": 12690,
      "training_loss": 6.307910442352295
    },
    {
      "epoch": 0.6878590785907859,
      "step": 12691,
      "training_loss": 7.121059894561768
    },
    {
      "epoch": 0.6879132791327913,
      "grad_norm": 18.677330017089844,
      "learning_rate": 1e-05,
      "loss": 6.4217,
      "step": 12692
    },
    {
      "epoch": 0.6879132791327913,
      "step": 12692,
      "training_loss": 6.985768795013428
    },
    {
      "epoch": 0.6879674796747968,
      "step": 12693,
      "training_loss": 7.3061065673828125
    },
    {
      "epoch": 0.6880216802168022,
      "step": 12694,
      "training_loss": 7.926797866821289
    },
    {
      "epoch": 0.6880758807588075,
      "step": 12695,
      "training_loss": 7.596773147583008
    },
    {
      "epoch": 0.688130081300813,
      "grad_norm": 34.85062789916992,
      "learning_rate": 1e-05,
      "loss": 7.4539,
      "step": 12696
    },
    {
      "epoch": 0.688130081300813,
      "step": 12696,
      "training_loss": 7.700655937194824
    },
    {
      "epoch": 0.6881842818428184,
      "step": 12697,
      "training_loss": 6.62899112701416
    },
    {
      "epoch": 0.6882384823848239,
      "step": 12698,
      "training_loss": 5.729883193969727
    },
    {
      "epoch": 0.6882926829268292,
      "step": 12699,
      "training_loss": 7.25213623046875
    },
    {
      "epoch": 0.6883468834688347,
      "grad_norm": 18.948089599609375,
      "learning_rate": 1e-05,
      "loss": 6.8279,
      "step": 12700
    },
    {
      "epoch": 0.6883468834688347,
      "step": 12700,
      "training_loss": 3.6783933639526367
    },
    {
      "epoch": 0.6884010840108401,
      "step": 12701,
      "training_loss": 7.5831499099731445
    },
    {
      "epoch": 0.6884552845528455,
      "step": 12702,
      "training_loss": 4.050121307373047
    },
    {
      "epoch": 0.688509485094851,
      "step": 12703,
      "training_loss": 5.811529636383057
    },
    {
      "epoch": 0.6885636856368563,
      "grad_norm": 32.85136795043945,
      "learning_rate": 1e-05,
      "loss": 5.2808,
      "step": 12704
    },
    {
      "epoch": 0.6885636856368563,
      "step": 12704,
      "training_loss": 6.250476837158203
    },
    {
      "epoch": 0.6886178861788618,
      "step": 12705,
      "training_loss": 6.598133087158203
    },
    {
      "epoch": 0.6886720867208672,
      "step": 12706,
      "training_loss": 6.89607572555542
    },
    {
      "epoch": 0.6887262872628727,
      "step": 12707,
      "training_loss": 7.754125118255615
    },
    {
      "epoch": 0.688780487804878,
      "grad_norm": 17.7585391998291,
      "learning_rate": 1e-05,
      "loss": 6.8747,
      "step": 12708
    },
    {
      "epoch": 0.688780487804878,
      "step": 12708,
      "training_loss": 5.74811315536499
    },
    {
      "epoch": 0.6888346883468834,
      "step": 12709,
      "training_loss": 4.013998031616211
    },
    {
      "epoch": 0.6888888888888889,
      "step": 12710,
      "training_loss": 4.581247329711914
    },
    {
      "epoch": 0.6889430894308943,
      "step": 12711,
      "training_loss": 6.679478645324707
    },
    {
      "epoch": 0.6889972899728998,
      "grad_norm": 35.86591720581055,
      "learning_rate": 1e-05,
      "loss": 5.2557,
      "step": 12712
    },
    {
      "epoch": 0.6889972899728998,
      "step": 12712,
      "training_loss": 6.198078632354736
    },
    {
      "epoch": 0.6890514905149051,
      "step": 12713,
      "training_loss": 6.164505958557129
    },
    {
      "epoch": 0.6891056910569106,
      "step": 12714,
      "training_loss": 7.76776647567749
    },
    {
      "epoch": 0.689159891598916,
      "step": 12715,
      "training_loss": 7.155494213104248
    },
    {
      "epoch": 0.6892140921409214,
      "grad_norm": 21.126934051513672,
      "learning_rate": 1e-05,
      "loss": 6.8215,
      "step": 12716
    },
    {
      "epoch": 0.6892140921409214,
      "step": 12716,
      "training_loss": 6.659087657928467
    },
    {
      "epoch": 0.6892682926829268,
      "step": 12717,
      "training_loss": 6.02125883102417
    },
    {
      "epoch": 0.6893224932249322,
      "step": 12718,
      "training_loss": 6.309248447418213
    },
    {
      "epoch": 0.6893766937669377,
      "step": 12719,
      "training_loss": 6.170177459716797
    },
    {
      "epoch": 0.6894308943089431,
      "grad_norm": 21.835535049438477,
      "learning_rate": 1e-05,
      "loss": 6.2899,
      "step": 12720
    },
    {
      "epoch": 0.6894308943089431,
      "step": 12720,
      "training_loss": 7.2289323806762695
    },
    {
      "epoch": 0.6894850948509486,
      "step": 12721,
      "training_loss": 4.571360111236572
    },
    {
      "epoch": 0.6895392953929539,
      "step": 12722,
      "training_loss": 7.010677814483643
    },
    {
      "epoch": 0.6895934959349593,
      "step": 12723,
      "training_loss": 7.406792163848877
    },
    {
      "epoch": 0.6896476964769648,
      "grad_norm": 38.75593948364258,
      "learning_rate": 1e-05,
      "loss": 6.5544,
      "step": 12724
    },
    {
      "epoch": 0.6896476964769648,
      "step": 12724,
      "training_loss": 5.994044303894043
    },
    {
      "epoch": 0.6897018970189702,
      "step": 12725,
      "training_loss": 7.788180828094482
    },
    {
      "epoch": 0.6897560975609756,
      "step": 12726,
      "training_loss": 6.013860702514648
    },
    {
      "epoch": 0.689810298102981,
      "step": 12727,
      "training_loss": 6.5548996925354
    },
    {
      "epoch": 0.6898644986449864,
      "grad_norm": 35.39348602294922,
      "learning_rate": 1e-05,
      "loss": 6.5877,
      "step": 12728
    },
    {
      "epoch": 0.6898644986449864,
      "step": 12728,
      "training_loss": 7.089804172515869
    },
    {
      "epoch": 0.6899186991869919,
      "step": 12729,
      "training_loss": 7.049248218536377
    },
    {
      "epoch": 0.6899728997289973,
      "step": 12730,
      "training_loss": 3.92128324508667
    },
    {
      "epoch": 0.6900271002710027,
      "step": 12731,
      "training_loss": 8.247551918029785
    },
    {
      "epoch": 0.6900813008130081,
      "grad_norm": 27.39714241027832,
      "learning_rate": 1e-05,
      "loss": 6.577,
      "step": 12732
    },
    {
      "epoch": 0.6900813008130081,
      "step": 12732,
      "training_loss": 5.948544979095459
    },
    {
      "epoch": 0.6901355013550136,
      "step": 12733,
      "training_loss": 5.3181257247924805
    },
    {
      "epoch": 0.690189701897019,
      "step": 12734,
      "training_loss": 6.744736194610596
    },
    {
      "epoch": 0.6902439024390243,
      "step": 12735,
      "training_loss": 7.192314624786377
    },
    {
      "epoch": 0.6902981029810298,
      "grad_norm": 18.341453552246094,
      "learning_rate": 1e-05,
      "loss": 6.3009,
      "step": 12736
    },
    {
      "epoch": 0.6902981029810298,
      "step": 12736,
      "training_loss": 7.300387859344482
    },
    {
      "epoch": 0.6903523035230352,
      "step": 12737,
      "training_loss": 7.183436870574951
    },
    {
      "epoch": 0.6904065040650407,
      "step": 12738,
      "training_loss": 7.912827491760254
    },
    {
      "epoch": 0.6904607046070461,
      "step": 12739,
      "training_loss": 4.998773097991943
    },
    {
      "epoch": 0.6905149051490515,
      "grad_norm": 27.04939842224121,
      "learning_rate": 1e-05,
      "loss": 6.8489,
      "step": 12740
    },
    {
      "epoch": 0.6905149051490515,
      "step": 12740,
      "training_loss": 6.468460559844971
    },
    {
      "epoch": 0.6905691056910569,
      "step": 12741,
      "training_loss": 6.1988019943237305
    },
    {
      "epoch": 0.6906233062330623,
      "step": 12742,
      "training_loss": 7.477048397064209
    },
    {
      "epoch": 0.6906775067750678,
      "step": 12743,
      "training_loss": 5.959055423736572
    },
    {
      "epoch": 0.6907317073170731,
      "grad_norm": 31.599451065063477,
      "learning_rate": 1e-05,
      "loss": 6.5258,
      "step": 12744
    },
    {
      "epoch": 0.6907317073170731,
      "step": 12744,
      "training_loss": 3.0416922569274902
    },
    {
      "epoch": 0.6907859078590786,
      "step": 12745,
      "training_loss": 7.792169570922852
    },
    {
      "epoch": 0.690840108401084,
      "step": 12746,
      "training_loss": 7.457396030426025
    },
    {
      "epoch": 0.6908943089430895,
      "step": 12747,
      "training_loss": 6.146816253662109
    },
    {
      "epoch": 0.6909485094850949,
      "grad_norm": 32.27098846435547,
      "learning_rate": 1e-05,
      "loss": 6.1095,
      "step": 12748
    },
    {
      "epoch": 0.6909485094850949,
      "step": 12748,
      "training_loss": 7.948604106903076
    },
    {
      "epoch": 0.6910027100271002,
      "step": 12749,
      "training_loss": 6.870298862457275
    },
    {
      "epoch": 0.6910569105691057,
      "step": 12750,
      "training_loss": 5.837186813354492
    },
    {
      "epoch": 0.6911111111111111,
      "step": 12751,
      "training_loss": 7.043903350830078
    },
    {
      "epoch": 0.6911653116531166,
      "grad_norm": 20.93601417541504,
      "learning_rate": 1e-05,
      "loss": 6.925,
      "step": 12752
    },
    {
      "epoch": 0.6911653116531166,
      "step": 12752,
      "training_loss": 7.072270393371582
    },
    {
      "epoch": 0.6912195121951219,
      "step": 12753,
      "training_loss": 5.99464750289917
    },
    {
      "epoch": 0.6912737127371273,
      "step": 12754,
      "training_loss": 7.313745975494385
    },
    {
      "epoch": 0.6913279132791328,
      "step": 12755,
      "training_loss": 7.236065864562988
    },
    {
      "epoch": 0.6913821138211382,
      "grad_norm": 22.53360939025879,
      "learning_rate": 1e-05,
      "loss": 6.9042,
      "step": 12756
    },
    {
      "epoch": 0.6913821138211382,
      "step": 12756,
      "training_loss": 7.464475631713867
    },
    {
      "epoch": 0.6914363143631437,
      "step": 12757,
      "training_loss": 6.7114739418029785
    },
    {
      "epoch": 0.691490514905149,
      "step": 12758,
      "training_loss": 3.6338138580322266
    },
    {
      "epoch": 0.6915447154471545,
      "step": 12759,
      "training_loss": 5.847563743591309
    },
    {
      "epoch": 0.6915989159891599,
      "grad_norm": 55.09132385253906,
      "learning_rate": 1e-05,
      "loss": 5.9143,
      "step": 12760
    },
    {
      "epoch": 0.6915989159891599,
      "step": 12760,
      "training_loss": 6.673520088195801
    },
    {
      "epoch": 0.6916531165311653,
      "step": 12761,
      "training_loss": 6.239089488983154
    },
    {
      "epoch": 0.6917073170731707,
      "step": 12762,
      "training_loss": 7.967682838439941
    },
    {
      "epoch": 0.6917615176151761,
      "step": 12763,
      "training_loss": 5.39705228805542
    },
    {
      "epoch": 0.6918157181571816,
      "grad_norm": 29.043241500854492,
      "learning_rate": 1e-05,
      "loss": 6.5693,
      "step": 12764
    },
    {
      "epoch": 0.6918157181571816,
      "step": 12764,
      "training_loss": 6.7969818115234375
    },
    {
      "epoch": 0.691869918699187,
      "step": 12765,
      "training_loss": 6.611893653869629
    },
    {
      "epoch": 0.6919241192411925,
      "step": 12766,
      "training_loss": 8.529891014099121
    },
    {
      "epoch": 0.6919783197831978,
      "step": 12767,
      "training_loss": 6.852279186248779
    },
    {
      "epoch": 0.6920325203252032,
      "grad_norm": 41.70293426513672,
      "learning_rate": 1e-05,
      "loss": 7.1978,
      "step": 12768
    },
    {
      "epoch": 0.6920325203252032,
      "step": 12768,
      "training_loss": 5.358756065368652
    },
    {
      "epoch": 0.6920867208672087,
      "step": 12769,
      "training_loss": 7.120738983154297
    },
    {
      "epoch": 0.6921409214092141,
      "step": 12770,
      "training_loss": 6.48328971862793
    },
    {
      "epoch": 0.6921951219512195,
      "step": 12771,
      "training_loss": 8.4578218460083
    },
    {
      "epoch": 0.6922493224932249,
      "grad_norm": 31.467161178588867,
      "learning_rate": 1e-05,
      "loss": 6.8552,
      "step": 12772
    },
    {
      "epoch": 0.6922493224932249,
      "step": 12772,
      "training_loss": 6.858389377593994
    },
    {
      "epoch": 0.6923035230352304,
      "step": 12773,
      "training_loss": 7.482795238494873
    },
    {
      "epoch": 0.6923577235772358,
      "step": 12774,
      "training_loss": 5.454744815826416
    },
    {
      "epoch": 0.6924119241192412,
      "step": 12775,
      "training_loss": 7.310693264007568
    },
    {
      "epoch": 0.6924661246612466,
      "grad_norm": 31.53667640686035,
      "learning_rate": 1e-05,
      "loss": 6.7767,
      "step": 12776
    },
    {
      "epoch": 0.6924661246612466,
      "step": 12776,
      "training_loss": 7.6417670249938965
    },
    {
      "epoch": 0.692520325203252,
      "step": 12777,
      "training_loss": 6.818605899810791
    },
    {
      "epoch": 0.6925745257452575,
      "step": 12778,
      "training_loss": 3.850264549255371
    },
    {
      "epoch": 0.6926287262872629,
      "step": 12779,
      "training_loss": 6.918889045715332
    },
    {
      "epoch": 0.6926829268292682,
      "grad_norm": 32.163326263427734,
      "learning_rate": 1e-05,
      "loss": 6.3074,
      "step": 12780
    },
    {
      "epoch": 0.6926829268292682,
      "step": 12780,
      "training_loss": 6.249410629272461
    },
    {
      "epoch": 0.6927371273712737,
      "step": 12781,
      "training_loss": 8.360719680786133
    },
    {
      "epoch": 0.6927913279132791,
      "step": 12782,
      "training_loss": 7.089612007141113
    },
    {
      "epoch": 0.6928455284552846,
      "step": 12783,
      "training_loss": 7.209106922149658
    },
    {
      "epoch": 0.69289972899729,
      "grad_norm": 25.30679702758789,
      "learning_rate": 1e-05,
      "loss": 7.2272,
      "step": 12784
    },
    {
      "epoch": 0.69289972899729,
      "step": 12784,
      "training_loss": 6.873950004577637
    },
    {
      "epoch": 0.6929539295392954,
      "step": 12785,
      "training_loss": 3.1955907344818115
    },
    {
      "epoch": 0.6930081300813008,
      "step": 12786,
      "training_loss": 6.688553810119629
    },
    {
      "epoch": 0.6930623306233062,
      "step": 12787,
      "training_loss": 6.08410120010376
    },
    {
      "epoch": 0.6931165311653117,
      "grad_norm": 25.346660614013672,
      "learning_rate": 1e-05,
      "loss": 5.7105,
      "step": 12788
    },
    {
      "epoch": 0.6931165311653117,
      "step": 12788,
      "training_loss": 6.730226516723633
    },
    {
      "epoch": 0.693170731707317,
      "step": 12789,
      "training_loss": 5.414027690887451
    },
    {
      "epoch": 0.6932249322493225,
      "step": 12790,
      "training_loss": 5.530641555786133
    },
    {
      "epoch": 0.6932791327913279,
      "step": 12791,
      "training_loss": 4.633126258850098
    },
    {
      "epoch": 0.6933333333333334,
      "grad_norm": 27.735095977783203,
      "learning_rate": 1e-05,
      "loss": 5.577,
      "step": 12792
    },
    {
      "epoch": 0.6933333333333334,
      "step": 12792,
      "training_loss": 5.9166340827941895
    },
    {
      "epoch": 0.6933875338753388,
      "step": 12793,
      "training_loss": 4.254891872406006
    },
    {
      "epoch": 0.6934417344173441,
      "step": 12794,
      "training_loss": 7.2957963943481445
    },
    {
      "epoch": 0.6934959349593496,
      "step": 12795,
      "training_loss": 6.946694850921631
    },
    {
      "epoch": 0.693550135501355,
      "grad_norm": 37.98586654663086,
      "learning_rate": 1e-05,
      "loss": 6.1035,
      "step": 12796
    },
    {
      "epoch": 0.693550135501355,
      "step": 12796,
      "training_loss": 7.0151238441467285
    },
    {
      "epoch": 0.6936043360433605,
      "step": 12797,
      "training_loss": 7.5292744636535645
    },
    {
      "epoch": 0.6936585365853658,
      "step": 12798,
      "training_loss": 7.02426815032959
    },
    {
      "epoch": 0.6937127371273712,
      "step": 12799,
      "training_loss": 6.948937892913818
    },
    {
      "epoch": 0.6937669376693767,
      "grad_norm": 31.29814910888672,
      "learning_rate": 1e-05,
      "loss": 7.1294,
      "step": 12800
    },
    {
      "epoch": 0.6937669376693767,
      "step": 12800,
      "training_loss": 7.242976188659668
    },
    {
      "epoch": 0.6938211382113821,
      "step": 12801,
      "training_loss": 6.552468299865723
    },
    {
      "epoch": 0.6938753387533876,
      "step": 12802,
      "training_loss": 6.735263347625732
    },
    {
      "epoch": 0.6939295392953929,
      "step": 12803,
      "training_loss": 7.5945634841918945
    },
    {
      "epoch": 0.6939837398373984,
      "grad_norm": 32.09575271606445,
      "learning_rate": 1e-05,
      "loss": 7.0313,
      "step": 12804
    },
    {
      "epoch": 0.6939837398373984,
      "step": 12804,
      "training_loss": 7.0639262199401855
    },
    {
      "epoch": 0.6940379403794038,
      "step": 12805,
      "training_loss": 7.422843933105469
    },
    {
      "epoch": 0.6940921409214093,
      "step": 12806,
      "training_loss": 7.1844682693481445
    },
    {
      "epoch": 0.6941463414634146,
      "step": 12807,
      "training_loss": 7.255492687225342
    },
    {
      "epoch": 0.69420054200542,
      "grad_norm": 34.453857421875,
      "learning_rate": 1e-05,
      "loss": 7.2317,
      "step": 12808
    },
    {
      "epoch": 0.69420054200542,
      "step": 12808,
      "training_loss": 5.997245788574219
    },
    {
      "epoch": 0.6942547425474255,
      "step": 12809,
      "training_loss": 6.523030757904053
    },
    {
      "epoch": 0.6943089430894309,
      "step": 12810,
      "training_loss": 7.019931316375732
    },
    {
      "epoch": 0.6943631436314364,
      "step": 12811,
      "training_loss": 5.995757579803467
    },
    {
      "epoch": 0.6944173441734417,
      "grad_norm": 33.55672073364258,
      "learning_rate": 1e-05,
      "loss": 6.384,
      "step": 12812
    },
    {
      "epoch": 0.6944173441734417,
      "step": 12812,
      "training_loss": 6.749087810516357
    },
    {
      "epoch": 0.6944715447154471,
      "step": 12813,
      "training_loss": 8.367228507995605
    },
    {
      "epoch": 0.6945257452574526,
      "step": 12814,
      "training_loss": 5.588432312011719
    },
    {
      "epoch": 0.694579945799458,
      "step": 12815,
      "training_loss": 2.907609462738037
    },
    {
      "epoch": 0.6946341463414634,
      "grad_norm": 32.480655670166016,
      "learning_rate": 1e-05,
      "loss": 5.9031,
      "step": 12816
    },
    {
      "epoch": 0.6946341463414634,
      "step": 12816,
      "training_loss": 6.832045078277588
    },
    {
      "epoch": 0.6946883468834688,
      "step": 12817,
      "training_loss": 6.115522861480713
    },
    {
      "epoch": 0.6947425474254743,
      "step": 12818,
      "training_loss": 6.594888687133789
    },
    {
      "epoch": 0.6947967479674797,
      "step": 12819,
      "training_loss": 6.592411994934082
    },
    {
      "epoch": 0.6948509485094851,
      "grad_norm": 26.874746322631836,
      "learning_rate": 1e-05,
      "loss": 6.5337,
      "step": 12820
    },
    {
      "epoch": 0.6948509485094851,
      "step": 12820,
      "training_loss": 5.701878070831299
    },
    {
      "epoch": 0.6949051490514905,
      "step": 12821,
      "training_loss": 6.016806602478027
    },
    {
      "epoch": 0.6949593495934959,
      "step": 12822,
      "training_loss": 6.7060041427612305
    },
    {
      "epoch": 0.6950135501355014,
      "step": 12823,
      "training_loss": 7.835256099700928
    },
    {
      "epoch": 0.6950677506775068,
      "grad_norm": 45.0984001159668,
      "learning_rate": 1e-05,
      "loss": 6.565,
      "step": 12824
    },
    {
      "epoch": 0.6950677506775068,
      "step": 12824,
      "training_loss": 4.691997051239014
    },
    {
      "epoch": 0.6951219512195121,
      "step": 12825,
      "training_loss": 7.7570576667785645
    },
    {
      "epoch": 0.6951761517615176,
      "step": 12826,
      "training_loss": 3.539883852005005
    },
    {
      "epoch": 0.695230352303523,
      "step": 12827,
      "training_loss": 6.121969223022461
    },
    {
      "epoch": 0.6952845528455285,
      "grad_norm": 32.84514236450195,
      "learning_rate": 1e-05,
      "loss": 5.5277,
      "step": 12828
    },
    {
      "epoch": 0.6952845528455285,
      "step": 12828,
      "training_loss": 5.863265037536621
    },
    {
      "epoch": 0.6953387533875339,
      "step": 12829,
      "training_loss": 5.349204063415527
    },
    {
      "epoch": 0.6953929539295393,
      "step": 12830,
      "training_loss": 4.335453033447266
    },
    {
      "epoch": 0.6954471544715447,
      "step": 12831,
      "training_loss": 5.489702224731445
    },
    {
      "epoch": 0.6955013550135501,
      "grad_norm": 26.571123123168945,
      "learning_rate": 1e-05,
      "loss": 5.2594,
      "step": 12832
    },
    {
      "epoch": 0.6955013550135501,
      "step": 12832,
      "training_loss": 5.857040882110596
    },
    {
      "epoch": 0.6955555555555556,
      "step": 12833,
      "training_loss": 7.927141189575195
    },
    {
      "epoch": 0.6956097560975609,
      "step": 12834,
      "training_loss": 5.369781970977783
    },
    {
      "epoch": 0.6956639566395664,
      "step": 12835,
      "training_loss": 6.694725513458252
    },
    {
      "epoch": 0.6957181571815718,
      "grad_norm": 27.00470542907715,
      "learning_rate": 1e-05,
      "loss": 6.4622,
      "step": 12836
    },
    {
      "epoch": 0.6957181571815718,
      "step": 12836,
      "training_loss": 6.630341053009033
    },
    {
      "epoch": 0.6957723577235773,
      "step": 12837,
      "training_loss": 5.494522571563721
    },
    {
      "epoch": 0.6958265582655827,
      "step": 12838,
      "training_loss": 5.675340175628662
    },
    {
      "epoch": 0.695880758807588,
      "step": 12839,
      "training_loss": 5.05571985244751
    },
    {
      "epoch": 0.6959349593495935,
      "grad_norm": 35.60387420654297,
      "learning_rate": 1e-05,
      "loss": 5.714,
      "step": 12840
    },
    {
      "epoch": 0.6959349593495935,
      "step": 12840,
      "training_loss": 7.626033306121826
    },
    {
      "epoch": 0.6959891598915989,
      "step": 12841,
      "training_loss": 5.760379791259766
    },
    {
      "epoch": 0.6960433604336044,
      "step": 12842,
      "training_loss": 6.636325359344482
    },
    {
      "epoch": 0.6960975609756097,
      "step": 12843,
      "training_loss": 7.359532833099365
    },
    {
      "epoch": 0.6961517615176152,
      "grad_norm": 31.11018180847168,
      "learning_rate": 1e-05,
      "loss": 6.8456,
      "step": 12844
    },
    {
      "epoch": 0.6961517615176152,
      "step": 12844,
      "training_loss": 5.3258233070373535
    },
    {
      "epoch": 0.6962059620596206,
      "step": 12845,
      "training_loss": 8.476173400878906
    },
    {
      "epoch": 0.696260162601626,
      "step": 12846,
      "training_loss": 7.84070348739624
    },
    {
      "epoch": 0.6963143631436315,
      "step": 12847,
      "training_loss": 6.2925286293029785
    },
    {
      "epoch": 0.6963685636856368,
      "grad_norm": 34.86989974975586,
      "learning_rate": 1e-05,
      "loss": 6.9838,
      "step": 12848
    },
    {
      "epoch": 0.6963685636856368,
      "step": 12848,
      "training_loss": 6.918856620788574
    },
    {
      "epoch": 0.6964227642276423,
      "step": 12849,
      "training_loss": 7.279223442077637
    },
    {
      "epoch": 0.6964769647696477,
      "step": 12850,
      "training_loss": 7.163742542266846
    },
    {
      "epoch": 0.6965311653116532,
      "step": 12851,
      "training_loss": 7.043299674987793
    },
    {
      "epoch": 0.6965853658536585,
      "grad_norm": 19.37337875366211,
      "learning_rate": 1e-05,
      "loss": 7.1013,
      "step": 12852
    },
    {
      "epoch": 0.6965853658536585,
      "step": 12852,
      "training_loss": 7.109624862670898
    },
    {
      "epoch": 0.6966395663956639,
      "step": 12853,
      "training_loss": 6.989125728607178
    },
    {
      "epoch": 0.6966937669376694,
      "step": 12854,
      "training_loss": 6.599148273468018
    },
    {
      "epoch": 0.6967479674796748,
      "step": 12855,
      "training_loss": 7.594025135040283
    },
    {
      "epoch": 0.6968021680216803,
      "grad_norm": 20.89903450012207,
      "learning_rate": 1e-05,
      "loss": 7.073,
      "step": 12856
    },
    {
      "epoch": 0.6968021680216803,
      "step": 12856,
      "training_loss": 6.405187606811523
    },
    {
      "epoch": 0.6968563685636856,
      "step": 12857,
      "training_loss": 7.310253620147705
    },
    {
      "epoch": 0.696910569105691,
      "step": 12858,
      "training_loss": 6.899380207061768
    },
    {
      "epoch": 0.6969647696476965,
      "step": 12859,
      "training_loss": 6.31535530090332
    },
    {
      "epoch": 0.6970189701897019,
      "grad_norm": 41.80224609375,
      "learning_rate": 1e-05,
      "loss": 6.7325,
      "step": 12860
    },
    {
      "epoch": 0.6970189701897019,
      "step": 12860,
      "training_loss": 5.657346725463867
    },
    {
      "epoch": 0.6970731707317073,
      "step": 12861,
      "training_loss": 7.2177863121032715
    },
    {
      "epoch": 0.6971273712737127,
      "step": 12862,
      "training_loss": 6.3875532150268555
    },
    {
      "epoch": 0.6971815718157182,
      "step": 12863,
      "training_loss": 6.755523681640625
    },
    {
      "epoch": 0.6972357723577236,
      "grad_norm": 25.13682746887207,
      "learning_rate": 1e-05,
      "loss": 6.5046,
      "step": 12864
    },
    {
      "epoch": 0.6972357723577236,
      "step": 12864,
      "training_loss": 6.681156158447266
    },
    {
      "epoch": 0.697289972899729,
      "step": 12865,
      "training_loss": 5.624794006347656
    },
    {
      "epoch": 0.6973441734417344,
      "step": 12866,
      "training_loss": 6.904082775115967
    },
    {
      "epoch": 0.6973983739837398,
      "step": 12867,
      "training_loss": 7.0488600730896
    },
    {
      "epoch": 0.6974525745257453,
      "grad_norm": 28.023094177246094,
      "learning_rate": 1e-05,
      "loss": 6.5647,
      "step": 12868
    },
    {
      "epoch": 0.6974525745257453,
      "step": 12868,
      "training_loss": 5.375814437866211
    },
    {
      "epoch": 0.6975067750677507,
      "step": 12869,
      "training_loss": 6.049170017242432
    },
    {
      "epoch": 0.697560975609756,
      "step": 12870,
      "training_loss": 4.922074317932129
    },
    {
      "epoch": 0.6976151761517615,
      "step": 12871,
      "training_loss": 8.08492374420166
    },
    {
      "epoch": 0.6976693766937669,
      "grad_norm": 29.113725662231445,
      "learning_rate": 1e-05,
      "loss": 6.108,
      "step": 12872
    },
    {
      "epoch": 0.6976693766937669,
      "step": 12872,
      "training_loss": 6.182267189025879
    },
    {
      "epoch": 0.6977235772357724,
      "step": 12873,
      "training_loss": 9.002216339111328
    },
    {
      "epoch": 0.6977777777777778,
      "step": 12874,
      "training_loss": 5.945525169372559
    },
    {
      "epoch": 0.6978319783197832,
      "step": 12875,
      "training_loss": 5.365732192993164
    },
    {
      "epoch": 0.6978861788617886,
      "grad_norm": 65.61983489990234,
      "learning_rate": 1e-05,
      "loss": 6.6239,
      "step": 12876
    },
    {
      "epoch": 0.6978861788617886,
      "step": 12876,
      "training_loss": 4.940234661102295
    },
    {
      "epoch": 0.697940379403794,
      "step": 12877,
      "training_loss": 7.659977436065674
    },
    {
      "epoch": 0.6979945799457995,
      "step": 12878,
      "training_loss": 5.918742656707764
    },
    {
      "epoch": 0.6980487804878048,
      "step": 12879,
      "training_loss": 7.095797061920166
    },
    {
      "epoch": 0.6981029810298103,
      "grad_norm": 19.80860710144043,
      "learning_rate": 1e-05,
      "loss": 6.4037,
      "step": 12880
    },
    {
      "epoch": 0.6981029810298103,
      "step": 12880,
      "training_loss": 7.249251365661621
    },
    {
      "epoch": 0.6981571815718157,
      "step": 12881,
      "training_loss": 7.370488166809082
    },
    {
      "epoch": 0.6982113821138212,
      "step": 12882,
      "training_loss": 7.082144260406494
    },
    {
      "epoch": 0.6982655826558266,
      "step": 12883,
      "training_loss": 6.183809280395508
    },
    {
      "epoch": 0.698319783197832,
      "grad_norm": 30.062057495117188,
      "learning_rate": 1e-05,
      "loss": 6.9714,
      "step": 12884
    },
    {
      "epoch": 0.698319783197832,
      "step": 12884,
      "training_loss": 6.76287841796875
    },
    {
      "epoch": 0.6983739837398374,
      "step": 12885,
      "training_loss": 6.696855545043945
    },
    {
      "epoch": 0.6984281842818428,
      "step": 12886,
      "training_loss": 7.878908157348633
    },
    {
      "epoch": 0.6984823848238483,
      "step": 12887,
      "training_loss": 6.377477645874023
    },
    {
      "epoch": 0.6985365853658536,
      "grad_norm": 37.901912689208984,
      "learning_rate": 1e-05,
      "loss": 6.929,
      "step": 12888
    },
    {
      "epoch": 0.6985365853658536,
      "step": 12888,
      "training_loss": 6.521144866943359
    },
    {
      "epoch": 0.6985907859078591,
      "step": 12889,
      "training_loss": 3.89682936668396
    },
    {
      "epoch": 0.6986449864498645,
      "step": 12890,
      "training_loss": 7.001614570617676
    },
    {
      "epoch": 0.69869918699187,
      "step": 12891,
      "training_loss": 6.867146015167236
    },
    {
      "epoch": 0.6987533875338754,
      "grad_norm": 47.400997161865234,
      "learning_rate": 1e-05,
      "loss": 6.0717,
      "step": 12892
    },
    {
      "epoch": 0.6987533875338754,
      "step": 12892,
      "training_loss": 5.9895172119140625
    },
    {
      "epoch": 0.6988075880758807,
      "step": 12893,
      "training_loss": 6.174781322479248
    },
    {
      "epoch": 0.6988617886178862,
      "step": 12894,
      "training_loss": 6.322815895080566
    },
    {
      "epoch": 0.6989159891598916,
      "step": 12895,
      "training_loss": 9.0357027053833
    },
    {
      "epoch": 0.6989701897018971,
      "grad_norm": 96.041259765625,
      "learning_rate": 1e-05,
      "loss": 6.8807,
      "step": 12896
    },
    {
      "epoch": 0.6989701897018971,
      "step": 12896,
      "training_loss": 5.655518054962158
    },
    {
      "epoch": 0.6990243902439024,
      "step": 12897,
      "training_loss": 6.209813117980957
    },
    {
      "epoch": 0.6990785907859078,
      "step": 12898,
      "training_loss": 5.504311561584473
    },
    {
      "epoch": 0.6991327913279133,
      "step": 12899,
      "training_loss": 3.966338872909546
    },
    {
      "epoch": 0.6991869918699187,
      "grad_norm": 22.177284240722656,
      "learning_rate": 1e-05,
      "loss": 5.334,
      "step": 12900
    },
    {
      "epoch": 0.6991869918699187,
      "step": 12900,
      "training_loss": 4.240540027618408
    },
    {
      "epoch": 0.6992411924119242,
      "step": 12901,
      "training_loss": 3.216392755508423
    },
    {
      "epoch": 0.6992953929539295,
      "step": 12902,
      "training_loss": 7.161667823791504
    },
    {
      "epoch": 0.699349593495935,
      "step": 12903,
      "training_loss": 6.572656631469727
    },
    {
      "epoch": 0.6994037940379404,
      "grad_norm": 30.44296646118164,
      "learning_rate": 1e-05,
      "loss": 5.2978,
      "step": 12904
    },
    {
      "epoch": 0.6994037940379404,
      "step": 12904,
      "training_loss": 6.020102024078369
    },
    {
      "epoch": 0.6994579945799458,
      "step": 12905,
      "training_loss": 5.950558185577393
    },
    {
      "epoch": 0.6995121951219512,
      "step": 12906,
      "training_loss": 6.845917701721191
    },
    {
      "epoch": 0.6995663956639566,
      "step": 12907,
      "training_loss": 5.854200839996338
    },
    {
      "epoch": 0.6996205962059621,
      "grad_norm": 49.929656982421875,
      "learning_rate": 1e-05,
      "loss": 6.1677,
      "step": 12908
    },
    {
      "epoch": 0.6996205962059621,
      "step": 12908,
      "training_loss": 7.236019134521484
    },
    {
      "epoch": 0.6996747967479675,
      "step": 12909,
      "training_loss": 6.723569869995117
    },
    {
      "epoch": 0.699728997289973,
      "step": 12910,
      "training_loss": 6.482645511627197
    },
    {
      "epoch": 0.6997831978319783,
      "step": 12911,
      "training_loss": 5.216484546661377
    },
    {
      "epoch": 0.6998373983739837,
      "grad_norm": 36.98634719848633,
      "learning_rate": 1e-05,
      "loss": 6.4147,
      "step": 12912
    },
    {
      "epoch": 0.6998373983739837,
      "step": 12912,
      "training_loss": 6.8889875411987305
    },
    {
      "epoch": 0.6998915989159892,
      "step": 12913,
      "training_loss": 6.405848979949951
    },
    {
      "epoch": 0.6999457994579946,
      "step": 12914,
      "training_loss": 5.997002124786377
    },
    {
      "epoch": 0.7,
      "step": 12915,
      "training_loss": 6.375850200653076
    },
    {
      "epoch": 0.7000542005420054,
      "grad_norm": 20.246177673339844,
      "learning_rate": 1e-05,
      "loss": 6.4169,
      "step": 12916
    },
    {
      "epoch": 0.7000542005420054,
      "step": 12916,
      "training_loss": 6.560962200164795
    },
    {
      "epoch": 0.7001084010840108,
      "step": 12917,
      "training_loss": 6.777101516723633
    },
    {
      "epoch": 0.7001626016260163,
      "step": 12918,
      "training_loss": 6.33328104019165
    },
    {
      "epoch": 0.7002168021680217,
      "step": 12919,
      "training_loss": 6.1848063468933105
    },
    {
      "epoch": 0.7002710027100271,
      "grad_norm": 33.45945739746094,
      "learning_rate": 1e-05,
      "loss": 6.464,
      "step": 12920
    },
    {
      "epoch": 0.7002710027100271,
      "step": 12920,
      "training_loss": 6.985029220581055
    },
    {
      "epoch": 0.7003252032520325,
      "step": 12921,
      "training_loss": 6.916967868804932
    },
    {
      "epoch": 0.700379403794038,
      "step": 12922,
      "training_loss": 6.955780982971191
    },
    {
      "epoch": 0.7004336043360434,
      "step": 12923,
      "training_loss": 6.037656784057617
    },
    {
      "epoch": 0.7004878048780487,
      "grad_norm": 31.351686477661133,
      "learning_rate": 1e-05,
      "loss": 6.7239,
      "step": 12924
    },
    {
      "epoch": 0.7004878048780487,
      "step": 12924,
      "training_loss": 7.549883842468262
    },
    {
      "epoch": 0.7005420054200542,
      "step": 12925,
      "training_loss": 7.229891777038574
    },
    {
      "epoch": 0.7005962059620596,
      "step": 12926,
      "training_loss": 7.483081340789795
    },
    {
      "epoch": 0.7006504065040651,
      "step": 12927,
      "training_loss": 5.629055500030518
    },
    {
      "epoch": 0.7007046070460705,
      "grad_norm": 36.633052825927734,
      "learning_rate": 1e-05,
      "loss": 6.973,
      "step": 12928
    },
    {
      "epoch": 0.7007046070460705,
      "step": 12928,
      "training_loss": 6.824625492095947
    },
    {
      "epoch": 0.7007588075880758,
      "step": 12929,
      "training_loss": 6.153520584106445
    },
    {
      "epoch": 0.7008130081300813,
      "step": 12930,
      "training_loss": 5.20399808883667
    },
    {
      "epoch": 0.7008672086720867,
      "step": 12931,
      "training_loss": 7.875776767730713
    },
    {
      "epoch": 0.7009214092140922,
      "grad_norm": 42.79338073730469,
      "learning_rate": 1e-05,
      "loss": 6.5145,
      "step": 12932
    },
    {
      "epoch": 0.7009214092140922,
      "step": 12932,
      "training_loss": 7.825559139251709
    },
    {
      "epoch": 0.7009756097560975,
      "step": 12933,
      "training_loss": 7.574625492095947
    },
    {
      "epoch": 0.701029810298103,
      "step": 12934,
      "training_loss": 5.8388471603393555
    },
    {
      "epoch": 0.7010840108401084,
      "step": 12935,
      "training_loss": 7.083128452301025
    },
    {
      "epoch": 0.7011382113821139,
      "grad_norm": 26.343753814697266,
      "learning_rate": 1e-05,
      "loss": 7.0805,
      "step": 12936
    },
    {
      "epoch": 0.7011382113821139,
      "step": 12936,
      "training_loss": 6.544785976409912
    },
    {
      "epoch": 0.7011924119241193,
      "step": 12937,
      "training_loss": 6.510990619659424
    },
    {
      "epoch": 0.7012466124661246,
      "step": 12938,
      "training_loss": 6.042413711547852
    },
    {
      "epoch": 0.7013008130081301,
      "step": 12939,
      "training_loss": 6.224261283874512
    },
    {
      "epoch": 0.7013550135501355,
      "grad_norm": 48.1908073425293,
      "learning_rate": 1e-05,
      "loss": 6.3306,
      "step": 12940
    },
    {
      "epoch": 0.7013550135501355,
      "step": 12940,
      "training_loss": 7.563220500946045
    },
    {
      "epoch": 0.701409214092141,
      "step": 12941,
      "training_loss": 6.801212310791016
    },
    {
      "epoch": 0.7014634146341463,
      "step": 12942,
      "training_loss": 6.653667449951172
    },
    {
      "epoch": 0.7015176151761517,
      "step": 12943,
      "training_loss": 6.851999282836914
    },
    {
      "epoch": 0.7015718157181572,
      "grad_norm": 30.64628791809082,
      "learning_rate": 1e-05,
      "loss": 6.9675,
      "step": 12944
    },
    {
      "epoch": 0.7015718157181572,
      "step": 12944,
      "training_loss": 6.386028289794922
    },
    {
      "epoch": 0.7016260162601626,
      "step": 12945,
      "training_loss": 6.347515106201172
    },
    {
      "epoch": 0.701680216802168,
      "step": 12946,
      "training_loss": 6.1302714347839355
    },
    {
      "epoch": 0.7017344173441734,
      "step": 12947,
      "training_loss": 6.321435451507568
    },
    {
      "epoch": 0.7017886178861789,
      "grad_norm": 19.09078598022461,
      "learning_rate": 1e-05,
      "loss": 6.2963,
      "step": 12948
    },
    {
      "epoch": 0.7017886178861789,
      "step": 12948,
      "training_loss": 6.418542861938477
    },
    {
      "epoch": 0.7018428184281843,
      "step": 12949,
      "training_loss": 6.237690448760986
    },
    {
      "epoch": 0.7018970189701897,
      "step": 12950,
      "training_loss": 7.250777244567871
    },
    {
      "epoch": 0.7019512195121951,
      "step": 12951,
      "training_loss": 6.640486240386963
    },
    {
      "epoch": 0.7020054200542005,
      "grad_norm": 34.212425231933594,
      "learning_rate": 1e-05,
      "loss": 6.6369,
      "step": 12952
    },
    {
      "epoch": 0.7020054200542005,
      "step": 12952,
      "training_loss": 4.309561252593994
    },
    {
      "epoch": 0.702059620596206,
      "step": 12953,
      "training_loss": 6.762486457824707
    },
    {
      "epoch": 0.7021138211382114,
      "step": 12954,
      "training_loss": 6.63250732421875
    },
    {
      "epoch": 0.7021680216802167,
      "step": 12955,
      "training_loss": 6.800983428955078
    },
    {
      "epoch": 0.7022222222222222,
      "grad_norm": 23.6320743560791,
      "learning_rate": 1e-05,
      "loss": 6.1264,
      "step": 12956
    },
    {
      "epoch": 0.7022222222222222,
      "step": 12956,
      "training_loss": 4.41088342666626
    },
    {
      "epoch": 0.7022764227642276,
      "step": 12957,
      "training_loss": 5.953261852264404
    },
    {
      "epoch": 0.7023306233062331,
      "step": 12958,
      "training_loss": 5.027101039886475
    },
    {
      "epoch": 0.7023848238482385,
      "step": 12959,
      "training_loss": 6.3857879638671875
    },
    {
      "epoch": 0.7024390243902439,
      "grad_norm": 28.763668060302734,
      "learning_rate": 1e-05,
      "loss": 5.4443,
      "step": 12960
    },
    {
      "epoch": 0.7024390243902439,
      "step": 12960,
      "training_loss": 7.03012752532959
    },
    {
      "epoch": 0.7024932249322493,
      "step": 12961,
      "training_loss": 7.025679588317871
    },
    {
      "epoch": 0.7025474254742547,
      "step": 12962,
      "training_loss": 6.326956748962402
    },
    {
      "epoch": 0.7026016260162602,
      "step": 12963,
      "training_loss": 5.769334316253662
    },
    {
      "epoch": 0.7026558265582655,
      "grad_norm": 20.170583724975586,
      "learning_rate": 1e-05,
      "loss": 6.538,
      "step": 12964
    },
    {
      "epoch": 0.7026558265582655,
      "step": 12964,
      "training_loss": 7.166200160980225
    },
    {
      "epoch": 0.702710027100271,
      "step": 12965,
      "training_loss": 5.696730136871338
    },
    {
      "epoch": 0.7027642276422764,
      "step": 12966,
      "training_loss": 5.925695896148682
    },
    {
      "epoch": 0.7028184281842819,
      "step": 12967,
      "training_loss": 3.47322416305542
    },
    {
      "epoch": 0.7028726287262873,
      "grad_norm": 31.129098892211914,
      "learning_rate": 1e-05,
      "loss": 5.5655,
      "step": 12968
    },
    {
      "epoch": 0.7028726287262873,
      "step": 12968,
      "training_loss": 4.943037033081055
    },
    {
      "epoch": 0.7029268292682926,
      "step": 12969,
      "training_loss": 6.552639484405518
    },
    {
      "epoch": 0.7029810298102981,
      "step": 12970,
      "training_loss": 6.707850456237793
    },
    {
      "epoch": 0.7030352303523035,
      "step": 12971,
      "training_loss": 6.483400821685791
    },
    {
      "epoch": 0.703089430894309,
      "grad_norm": 22.34430694580078,
      "learning_rate": 1e-05,
      "loss": 6.1717,
      "step": 12972
    },
    {
      "epoch": 0.703089430894309,
      "step": 12972,
      "training_loss": 6.413660049438477
    },
    {
      "epoch": 0.7031436314363143,
      "step": 12973,
      "training_loss": 5.808548927307129
    },
    {
      "epoch": 0.7031978319783198,
      "step": 12974,
      "training_loss": 6.758487701416016
    },
    {
      "epoch": 0.7032520325203252,
      "step": 12975,
      "training_loss": 5.7118239402771
    },
    {
      "epoch": 0.7033062330623306,
      "grad_norm": 32.156124114990234,
      "learning_rate": 1e-05,
      "loss": 6.1731,
      "step": 12976
    },
    {
      "epoch": 0.7033062330623306,
      "step": 12976,
      "training_loss": 7.128958225250244
    },
    {
      "epoch": 0.7033604336043361,
      "step": 12977,
      "training_loss": 6.069432258605957
    },
    {
      "epoch": 0.7034146341463414,
      "step": 12978,
      "training_loss": 5.660012722015381
    },
    {
      "epoch": 0.7034688346883469,
      "step": 12979,
      "training_loss": 6.460967063903809
    },
    {
      "epoch": 0.7035230352303523,
      "grad_norm": 41.62895965576172,
      "learning_rate": 1e-05,
      "loss": 6.3298,
      "step": 12980
    },
    {
      "epoch": 0.7035230352303523,
      "step": 12980,
      "training_loss": 6.343113422393799
    },
    {
      "epoch": 0.7035772357723578,
      "step": 12981,
      "training_loss": 7.546380043029785
    },
    {
      "epoch": 0.7036314363143631,
      "step": 12982,
      "training_loss": 5.612541675567627
    },
    {
      "epoch": 0.7036856368563685,
      "step": 12983,
      "training_loss": 6.913073539733887
    },
    {
      "epoch": 0.703739837398374,
      "grad_norm": 23.632341384887695,
      "learning_rate": 1e-05,
      "loss": 6.6038,
      "step": 12984
    },
    {
      "epoch": 0.703739837398374,
      "step": 12984,
      "training_loss": 7.746710300445557
    },
    {
      "epoch": 0.7037940379403794,
      "step": 12985,
      "training_loss": 6.838300704956055
    },
    {
      "epoch": 0.7038482384823849,
      "step": 12986,
      "training_loss": 7.266680717468262
    },
    {
      "epoch": 0.7039024390243902,
      "step": 12987,
      "training_loss": 5.8278422355651855
    },
    {
      "epoch": 0.7039566395663956,
      "grad_norm": 30.893503189086914,
      "learning_rate": 1e-05,
      "loss": 6.9199,
      "step": 12988
    },
    {
      "epoch": 0.7039566395663956,
      "step": 12988,
      "training_loss": 6.5796661376953125
    },
    {
      "epoch": 0.7040108401084011,
      "step": 12989,
      "training_loss": 7.248462200164795
    },
    {
      "epoch": 0.7040650406504065,
      "step": 12990,
      "training_loss": 6.179068088531494
    },
    {
      "epoch": 0.7041192411924119,
      "step": 12991,
      "training_loss": 7.032201766967773
    },
    {
      "epoch": 0.7041734417344173,
      "grad_norm": 29.372896194458008,
      "learning_rate": 1e-05,
      "loss": 6.7598,
      "step": 12992
    },
    {
      "epoch": 0.7041734417344173,
      "step": 12992,
      "training_loss": 3.345599889755249
    },
    {
      "epoch": 0.7042276422764228,
      "step": 12993,
      "training_loss": 7.590777397155762
    },
    {
      "epoch": 0.7042818428184282,
      "step": 12994,
      "training_loss": 6.687223434448242
    },
    {
      "epoch": 0.7043360433604337,
      "step": 12995,
      "training_loss": 6.82871150970459
    },
    {
      "epoch": 0.704390243902439,
      "grad_norm": 21.604398727416992,
      "learning_rate": 1e-05,
      "loss": 6.1131,
      "step": 12996
    },
    {
      "epoch": 0.704390243902439,
      "step": 12996,
      "training_loss": 7.203189373016357
    },
    {
      "epoch": 0.7044444444444444,
      "step": 12997,
      "training_loss": 6.321387767791748
    },
    {
      "epoch": 0.7044986449864499,
      "step": 12998,
      "training_loss": 7.080559730529785
    },
    {
      "epoch": 0.7045528455284553,
      "step": 12999,
      "training_loss": 5.994271278381348
    },
    {
      "epoch": 0.7046070460704607,
      "grad_norm": 31.679208755493164,
      "learning_rate": 1e-05,
      "loss": 6.6499,
      "step": 13000
    },
    {
      "epoch": 0.7046070460704607,
      "step": 13000,
      "training_loss": 5.589871406555176
    },
    {
      "epoch": 0.7046612466124661,
      "step": 13001,
      "training_loss": 6.230659484863281
    },
    {
      "epoch": 0.7047154471544715,
      "step": 13002,
      "training_loss": 3.36309552192688
    },
    {
      "epoch": 0.704769647696477,
      "step": 13003,
      "training_loss": 5.922533988952637
    },
    {
      "epoch": 0.7048238482384824,
      "grad_norm": 62.679100036621094,
      "learning_rate": 1e-05,
      "loss": 5.2765,
      "step": 13004
    },
    {
      "epoch": 0.7048238482384824,
      "step": 13004,
      "training_loss": 6.621652126312256
    },
    {
      "epoch": 0.7048780487804878,
      "step": 13005,
      "training_loss": 5.264007091522217
    },
    {
      "epoch": 0.7049322493224932,
      "step": 13006,
      "training_loss": 6.024084091186523
    },
    {
      "epoch": 0.7049864498644987,
      "step": 13007,
      "training_loss": 6.726444244384766
    },
    {
      "epoch": 0.7050406504065041,
      "grad_norm": 18.734994888305664,
      "learning_rate": 1e-05,
      "loss": 6.159,
      "step": 13008
    },
    {
      "epoch": 0.7050406504065041,
      "step": 13008,
      "training_loss": 5.532673358917236
    },
    {
      "epoch": 0.7050948509485094,
      "step": 13009,
      "training_loss": 5.530540943145752
    },
    {
      "epoch": 0.7051490514905149,
      "step": 13010,
      "training_loss": 5.553625106811523
    },
    {
      "epoch": 0.7052032520325203,
      "step": 13011,
      "training_loss": 6.7555766105651855
    },
    {
      "epoch": 0.7052574525745258,
      "grad_norm": 22.402996063232422,
      "learning_rate": 1e-05,
      "loss": 5.8431,
      "step": 13012
    },
    {
      "epoch": 0.7052574525745258,
      "step": 13012,
      "training_loss": 6.098788738250732
    },
    {
      "epoch": 0.7053116531165312,
      "step": 13013,
      "training_loss": 7.252566814422607
    },
    {
      "epoch": 0.7053658536585365,
      "step": 13014,
      "training_loss": 7.557331562042236
    },
    {
      "epoch": 0.705420054200542,
      "step": 13015,
      "training_loss": 5.482848167419434
    },
    {
      "epoch": 0.7054742547425474,
      "grad_norm": 30.074495315551758,
      "learning_rate": 1e-05,
      "loss": 6.5979,
      "step": 13016
    },
    {
      "epoch": 0.7054742547425474,
      "step": 13016,
      "training_loss": 6.816278457641602
    },
    {
      "epoch": 0.7055284552845529,
      "step": 13017,
      "training_loss": 6.856157302856445
    },
    {
      "epoch": 0.7055826558265582,
      "step": 13018,
      "training_loss": 7.435888767242432
    },
    {
      "epoch": 0.7056368563685637,
      "step": 13019,
      "training_loss": 6.920168399810791
    },
    {
      "epoch": 0.7056910569105691,
      "grad_norm": 20.167055130004883,
      "learning_rate": 1e-05,
      "loss": 7.0071,
      "step": 13020
    },
    {
      "epoch": 0.7056910569105691,
      "step": 13020,
      "training_loss": 7.922208309173584
    },
    {
      "epoch": 0.7057452574525745,
      "step": 13021,
      "training_loss": 6.4040961265563965
    },
    {
      "epoch": 0.70579945799458,
      "step": 13022,
      "training_loss": 7.541446685791016
    },
    {
      "epoch": 0.7058536585365853,
      "step": 13023,
      "training_loss": 5.663232803344727
    },
    {
      "epoch": 0.7059078590785908,
      "grad_norm": 37.493370056152344,
      "learning_rate": 1e-05,
      "loss": 6.8827,
      "step": 13024
    },
    {
      "epoch": 0.7059078590785908,
      "step": 13024,
      "training_loss": 5.456532001495361
    },
    {
      "epoch": 0.7059620596205962,
      "step": 13025,
      "training_loss": 8.13238525390625
    },
    {
      "epoch": 0.7060162601626017,
      "step": 13026,
      "training_loss": 6.056766033172607
    },
    {
      "epoch": 0.706070460704607,
      "step": 13027,
      "training_loss": 6.967586517333984
    },
    {
      "epoch": 0.7061246612466124,
      "grad_norm": 21.536863327026367,
      "learning_rate": 1e-05,
      "loss": 6.6533,
      "step": 13028
    },
    {
      "epoch": 0.7061246612466124,
      "step": 13028,
      "training_loss": 7.123885154724121
    },
    {
      "epoch": 0.7061788617886179,
      "step": 13029,
      "training_loss": 6.958802700042725
    },
    {
      "epoch": 0.7062330623306233,
      "step": 13030,
      "training_loss": 6.879376411437988
    },
    {
      "epoch": 0.7062872628726288,
      "step": 13031,
      "training_loss": 3.0016791820526123
    },
    {
      "epoch": 0.7063414634146341,
      "grad_norm": 24.31211280822754,
      "learning_rate": 1e-05,
      "loss": 5.9909,
      "step": 13032
    },
    {
      "epoch": 0.7063414634146341,
      "step": 13032,
      "training_loss": 7.666980743408203
    },
    {
      "epoch": 0.7063956639566396,
      "step": 13033,
      "training_loss": 7.0341596603393555
    },
    {
      "epoch": 0.706449864498645,
      "step": 13034,
      "training_loss": 6.936321258544922
    },
    {
      "epoch": 0.7065040650406504,
      "step": 13035,
      "training_loss": 7.003954887390137
    },
    {
      "epoch": 0.7065582655826558,
      "grad_norm": 19.665224075317383,
      "learning_rate": 1e-05,
      "loss": 7.1604,
      "step": 13036
    },
    {
      "epoch": 0.7065582655826558,
      "step": 13036,
      "training_loss": 7.219037055969238
    },
    {
      "epoch": 0.7066124661246612,
      "step": 13037,
      "training_loss": 5.32692289352417
    },
    {
      "epoch": 0.7066666666666667,
      "step": 13038,
      "training_loss": 5.22786808013916
    },
    {
      "epoch": 0.7067208672086721,
      "step": 13039,
      "training_loss": 5.050693035125732
    },
    {
      "epoch": 0.7067750677506776,
      "grad_norm": 84.92351531982422,
      "learning_rate": 1e-05,
      "loss": 5.7061,
      "step": 13040
    },
    {
      "epoch": 0.7067750677506776,
      "step": 13040,
      "training_loss": 4.976816654205322
    },
    {
      "epoch": 0.7068292682926829,
      "step": 13041,
      "training_loss": 5.889035701751709
    },
    {
      "epoch": 0.7068834688346883,
      "step": 13042,
      "training_loss": 7.195631980895996
    },
    {
      "epoch": 0.7069376693766938,
      "step": 13043,
      "training_loss": 5.234710693359375
    },
    {
      "epoch": 0.7069918699186992,
      "grad_norm": 35.67443084716797,
      "learning_rate": 1e-05,
      "loss": 5.824,
      "step": 13044
    },
    {
      "epoch": 0.7069918699186992,
      "step": 13044,
      "training_loss": 6.708902359008789
    },
    {
      "epoch": 0.7070460704607046,
      "step": 13045,
      "training_loss": 5.88458776473999
    },
    {
      "epoch": 0.70710027100271,
      "step": 13046,
      "training_loss": 6.956719875335693
    },
    {
      "epoch": 0.7071544715447154,
      "step": 13047,
      "training_loss": 7.155351161956787
    },
    {
      "epoch": 0.7072086720867209,
      "grad_norm": 21.881793975830078,
      "learning_rate": 1e-05,
      "loss": 6.6764,
      "step": 13048
    },
    {
      "epoch": 0.7072086720867209,
      "step": 13048,
      "training_loss": 6.819826126098633
    },
    {
      "epoch": 0.7072628726287263,
      "step": 13049,
      "training_loss": 8.105741500854492
    },
    {
      "epoch": 0.7073170731707317,
      "step": 13050,
      "training_loss": 5.914636611938477
    },
    {
      "epoch": 0.7073712737127371,
      "step": 13051,
      "training_loss": 6.554737091064453
    },
    {
      "epoch": 0.7074254742547426,
      "grad_norm": 40.749202728271484,
      "learning_rate": 1e-05,
      "loss": 6.8487,
      "step": 13052
    },
    {
      "epoch": 0.7074254742547426,
      "step": 13052,
      "training_loss": 6.362593173980713
    },
    {
      "epoch": 0.707479674796748,
      "step": 13053,
      "training_loss": 5.606822490692139
    },
    {
      "epoch": 0.7075338753387533,
      "step": 13054,
      "training_loss": 7.260591983795166
    },
    {
      "epoch": 0.7075880758807588,
      "step": 13055,
      "training_loss": 7.030378341674805
    },
    {
      "epoch": 0.7076422764227642,
      "grad_norm": 27.847593307495117,
      "learning_rate": 1e-05,
      "loss": 6.5651,
      "step": 13056
    },
    {
      "epoch": 0.7076422764227642,
      "step": 13056,
      "training_loss": 6.324221611022949
    },
    {
      "epoch": 0.7076964769647697,
      "step": 13057,
      "training_loss": 5.487421035766602
    },
    {
      "epoch": 0.7077506775067751,
      "step": 13058,
      "training_loss": 6.1750054359436035
    },
    {
      "epoch": 0.7078048780487805,
      "step": 13059,
      "training_loss": 5.817896366119385
    },
    {
      "epoch": 0.7078590785907859,
      "grad_norm": 38.01902389526367,
      "learning_rate": 1e-05,
      "loss": 5.9511,
      "step": 13060
    },
    {
      "epoch": 0.7078590785907859,
      "step": 13060,
      "training_loss": 7.032352924346924
    },
    {
      "epoch": 0.7079132791327913,
      "step": 13061,
      "training_loss": 4.783123970031738
    },
    {
      "epoch": 0.7079674796747968,
      "step": 13062,
      "training_loss": 7.30870246887207
    },
    {
      "epoch": 0.7080216802168021,
      "step": 13063,
      "training_loss": 7.353875637054443
    },
    {
      "epoch": 0.7080758807588076,
      "grad_norm": 27.339317321777344,
      "learning_rate": 1e-05,
      "loss": 6.6195,
      "step": 13064
    },
    {
      "epoch": 0.7080758807588076,
      "step": 13064,
      "training_loss": 7.222352027893066
    },
    {
      "epoch": 0.708130081300813,
      "step": 13065,
      "training_loss": 4.315075397491455
    },
    {
      "epoch": 0.7081842818428185,
      "step": 13066,
      "training_loss": 7.2202372550964355
    },
    {
      "epoch": 0.7082384823848239,
      "step": 13067,
      "training_loss": 5.275064468383789
    },
    {
      "epoch": 0.7082926829268292,
      "grad_norm": 37.234554290771484,
      "learning_rate": 1e-05,
      "loss": 6.0082,
      "step": 13068
    },
    {
      "epoch": 0.7082926829268292,
      "step": 13068,
      "training_loss": 5.852801322937012
    },
    {
      "epoch": 0.7083468834688347,
      "step": 13069,
      "training_loss": 3.7710304260253906
    },
    {
      "epoch": 0.7084010840108401,
      "step": 13070,
      "training_loss": 9.252089500427246
    },
    {
      "epoch": 0.7084552845528456,
      "step": 13071,
      "training_loss": 6.642172336578369
    },
    {
      "epoch": 0.7085094850948509,
      "grad_norm": 29.248985290527344,
      "learning_rate": 1e-05,
      "loss": 6.3795,
      "step": 13072
    },
    {
      "epoch": 0.7085094850948509,
      "step": 13072,
      "training_loss": 6.706705570220947
    },
    {
      "epoch": 0.7085636856368563,
      "step": 13073,
      "training_loss": 6.207284927368164
    },
    {
      "epoch": 0.7086178861788618,
      "step": 13074,
      "training_loss": 2.9272797107696533
    },
    {
      "epoch": 0.7086720867208672,
      "step": 13075,
      "training_loss": 6.011868476867676
    },
    {
      "epoch": 0.7087262872628727,
      "grad_norm": 39.86565399169922,
      "learning_rate": 1e-05,
      "loss": 5.4633,
      "step": 13076
    },
    {
      "epoch": 0.7087262872628727,
      "step": 13076,
      "training_loss": 5.658303737640381
    },
    {
      "epoch": 0.708780487804878,
      "step": 13077,
      "training_loss": 6.201160907745361
    },
    {
      "epoch": 0.7088346883468835,
      "step": 13078,
      "training_loss": 7.25700044631958
    },
    {
      "epoch": 0.7088888888888889,
      "step": 13079,
      "training_loss": 5.7303924560546875
    },
    {
      "epoch": 0.7089430894308943,
      "grad_norm": 36.56793212890625,
      "learning_rate": 1e-05,
      "loss": 6.2117,
      "step": 13080
    },
    {
      "epoch": 0.7089430894308943,
      "step": 13080,
      "training_loss": 6.0590434074401855
    },
    {
      "epoch": 0.7089972899728997,
      "step": 13081,
      "training_loss": 6.586721897125244
    },
    {
      "epoch": 0.7090514905149051,
      "step": 13082,
      "training_loss": 6.294543743133545
    },
    {
      "epoch": 0.7091056910569106,
      "step": 13083,
      "training_loss": 6.716396808624268
    },
    {
      "epoch": 0.709159891598916,
      "grad_norm": 18.317951202392578,
      "learning_rate": 1e-05,
      "loss": 6.4142,
      "step": 13084
    },
    {
      "epoch": 0.709159891598916,
      "step": 13084,
      "training_loss": 8.033443450927734
    },
    {
      "epoch": 0.7092140921409215,
      "step": 13085,
      "training_loss": 5.397462844848633
    },
    {
      "epoch": 0.7092682926829268,
      "step": 13086,
      "training_loss": 6.988453388214111
    },
    {
      "epoch": 0.7093224932249322,
      "step": 13087,
      "training_loss": 6.259932041168213
    },
    {
      "epoch": 0.7093766937669377,
      "grad_norm": 22.584117889404297,
      "learning_rate": 1e-05,
      "loss": 6.6698,
      "step": 13088
    },
    {
      "epoch": 0.7093766937669377,
      "step": 13088,
      "training_loss": 6.310359001159668
    },
    {
      "epoch": 0.7094308943089431,
      "step": 13089,
      "training_loss": 7.34999942779541
    },
    {
      "epoch": 0.7094850948509485,
      "step": 13090,
      "training_loss": 7.793971538543701
    },
    {
      "epoch": 0.7095392953929539,
      "step": 13091,
      "training_loss": 6.657289505004883
    },
    {
      "epoch": 0.7095934959349594,
      "grad_norm": 24.46436882019043,
      "learning_rate": 1e-05,
      "loss": 7.0279,
      "step": 13092
    },
    {
      "epoch": 0.7095934959349594,
      "step": 13092,
      "training_loss": 7.52318000793457
    },
    {
      "epoch": 0.7096476964769648,
      "step": 13093,
      "training_loss": 7.85189962387085
    },
    {
      "epoch": 0.7097018970189702,
      "step": 13094,
      "training_loss": 6.676557540893555
    },
    {
      "epoch": 0.7097560975609756,
      "step": 13095,
      "training_loss": 2.8705480098724365
    },
    {
      "epoch": 0.709810298102981,
      "grad_norm": 46.949737548828125,
      "learning_rate": 1e-05,
      "loss": 6.2305,
      "step": 13096
    },
    {
      "epoch": 0.709810298102981,
      "step": 13096,
      "training_loss": 6.839209079742432
    },
    {
      "epoch": 0.7098644986449865,
      "step": 13097,
      "training_loss": 7.9663405418396
    },
    {
      "epoch": 0.7099186991869919,
      "step": 13098,
      "training_loss": 6.312259197235107
    },
    {
      "epoch": 0.7099728997289972,
      "step": 13099,
      "training_loss": 6.156785488128662
    },
    {
      "epoch": 0.7100271002710027,
      "grad_norm": 22.683609008789062,
      "learning_rate": 1e-05,
      "loss": 6.8186,
      "step": 13100
    },
    {
      "epoch": 0.7100271002710027,
      "step": 13100,
      "training_loss": 7.039384365081787
    },
    {
      "epoch": 0.7100813008130081,
      "step": 13101,
      "training_loss": 7.595011234283447
    },
    {
      "epoch": 0.7101355013550136,
      "step": 13102,
      "training_loss": 5.889058589935303
    },
    {
      "epoch": 0.710189701897019,
      "step": 13103,
      "training_loss": 6.761116027832031
    },
    {
      "epoch": 0.7102439024390244,
      "grad_norm": 41.53693389892578,
      "learning_rate": 1e-05,
      "loss": 6.8211,
      "step": 13104
    },
    {
      "epoch": 0.7102439024390244,
      "step": 13104,
      "training_loss": 6.870667934417725
    },
    {
      "epoch": 0.7102981029810298,
      "step": 13105,
      "training_loss": 6.322932720184326
    },
    {
      "epoch": 0.7103523035230352,
      "step": 13106,
      "training_loss": 6.7568817138671875
    },
    {
      "epoch": 0.7104065040650407,
      "step": 13107,
      "training_loss": 6.469986915588379
    },
    {
      "epoch": 0.710460704607046,
      "grad_norm": 22.995534896850586,
      "learning_rate": 1e-05,
      "loss": 6.6051,
      "step": 13108
    },
    {
      "epoch": 0.710460704607046,
      "step": 13108,
      "training_loss": 3.3375589847564697
    },
    {
      "epoch": 0.7105149051490515,
      "step": 13109,
      "training_loss": 6.949686527252197
    },
    {
      "epoch": 0.7105691056910569,
      "step": 13110,
      "training_loss": 6.779930114746094
    },
    {
      "epoch": 0.7106233062330624,
      "step": 13111,
      "training_loss": 7.364660739898682
    },
    {
      "epoch": 0.7106775067750678,
      "grad_norm": 22.7788143157959,
      "learning_rate": 1e-05,
      "loss": 6.108,
      "step": 13112
    },
    {
      "epoch": 0.7106775067750678,
      "step": 13112,
      "training_loss": 5.170264720916748
    },
    {
      "epoch": 0.7107317073170731,
      "step": 13113,
      "training_loss": 6.056809425354004
    },
    {
      "epoch": 0.7107859078590786,
      "step": 13114,
      "training_loss": 5.085531711578369
    },
    {
      "epoch": 0.710840108401084,
      "step": 13115,
      "training_loss": 5.96295690536499
    },
    {
      "epoch": 0.7108943089430895,
      "grad_norm": 42.12388610839844,
      "learning_rate": 1e-05,
      "loss": 5.5689,
      "step": 13116
    },
    {
      "epoch": 0.7108943089430895,
      "step": 13116,
      "training_loss": 6.749891757965088
    },
    {
      "epoch": 0.7109485094850948,
      "step": 13117,
      "training_loss": 5.988825798034668
    },
    {
      "epoch": 0.7110027100271002,
      "step": 13118,
      "training_loss": 6.3639349937438965
    },
    {
      "epoch": 0.7110569105691057,
      "step": 13119,
      "training_loss": 3.260101556777954
    },
    {
      "epoch": 0.7111111111111111,
      "grad_norm": 22.50049591064453,
      "learning_rate": 1e-05,
      "loss": 5.5907,
      "step": 13120
    },
    {
      "epoch": 0.7111111111111111,
      "step": 13120,
      "training_loss": 6.954301357269287
    },
    {
      "epoch": 0.7111653116531166,
      "step": 13121,
      "training_loss": 7.291177749633789
    },
    {
      "epoch": 0.7112195121951219,
      "step": 13122,
      "training_loss": 8.161195755004883
    },
    {
      "epoch": 0.7112737127371274,
      "step": 13123,
      "training_loss": 7.274464130401611
    },
    {
      "epoch": 0.7113279132791328,
      "grad_norm": 33.47489547729492,
      "learning_rate": 1e-05,
      "loss": 7.4203,
      "step": 13124
    },
    {
      "epoch": 0.7113279132791328,
      "step": 13124,
      "training_loss": 7.499861717224121
    },
    {
      "epoch": 0.7113821138211383,
      "step": 13125,
      "training_loss": 4.12190055847168
    },
    {
      "epoch": 0.7114363143631436,
      "step": 13126,
      "training_loss": 6.842418670654297
    },
    {
      "epoch": 0.711490514905149,
      "step": 13127,
      "training_loss": 5.562767505645752
    },
    {
      "epoch": 0.7115447154471545,
      "grad_norm": 25.241472244262695,
      "learning_rate": 1e-05,
      "loss": 6.0067,
      "step": 13128
    },
    {
      "epoch": 0.7115447154471545,
      "step": 13128,
      "training_loss": 6.787140846252441
    },
    {
      "epoch": 0.7115989159891599,
      "step": 13129,
      "training_loss": 5.252135753631592
    },
    {
      "epoch": 0.7116531165311654,
      "step": 13130,
      "training_loss": 7.118078231811523
    },
    {
      "epoch": 0.7117073170731707,
      "step": 13131,
      "training_loss": 6.574334144592285
    },
    {
      "epoch": 0.7117615176151761,
      "grad_norm": 20.922632217407227,
      "learning_rate": 1e-05,
      "loss": 6.4329,
      "step": 13132
    },
    {
      "epoch": 0.7117615176151761,
      "step": 13132,
      "training_loss": 6.841639518737793
    },
    {
      "epoch": 0.7118157181571816,
      "step": 13133,
      "training_loss": 6.788896083831787
    },
    {
      "epoch": 0.711869918699187,
      "step": 13134,
      "training_loss": 6.245314598083496
    },
    {
      "epoch": 0.7119241192411924,
      "step": 13135,
      "training_loss": 7.507006645202637
    },
    {
      "epoch": 0.7119783197831978,
      "grad_norm": 22.895294189453125,
      "learning_rate": 1e-05,
      "loss": 6.8457,
      "step": 13136
    },
    {
      "epoch": 0.7119783197831978,
      "step": 13136,
      "training_loss": 4.1897292137146
    },
    {
      "epoch": 0.7120325203252033,
      "step": 13137,
      "training_loss": 7.800657272338867
    },
    {
      "epoch": 0.7120867208672087,
      "step": 13138,
      "training_loss": 6.116925239562988
    },
    {
      "epoch": 0.7121409214092141,
      "step": 13139,
      "training_loss": 7.543084144592285
    },
    {
      "epoch": 0.7121951219512195,
      "grad_norm": 53.512054443359375,
      "learning_rate": 1e-05,
      "loss": 6.4126,
      "step": 13140
    },
    {
      "epoch": 0.7121951219512195,
      "step": 13140,
      "training_loss": 5.472414970397949
    },
    {
      "epoch": 0.7122493224932249,
      "step": 13141,
      "training_loss": 5.023608684539795
    },
    {
      "epoch": 0.7123035230352304,
      "step": 13142,
      "training_loss": 6.84321403503418
    },
    {
      "epoch": 0.7123577235772358,
      "step": 13143,
      "training_loss": 6.667499542236328
    },
    {
      "epoch": 0.7124119241192411,
      "grad_norm": 23.480266571044922,
      "learning_rate": 1e-05,
      "loss": 6.0017,
      "step": 13144
    },
    {
      "epoch": 0.7124119241192411,
      "step": 13144,
      "training_loss": 7.0103759765625
    },
    {
      "epoch": 0.7124661246612466,
      "step": 13145,
      "training_loss": 7.601242542266846
    },
    {
      "epoch": 0.712520325203252,
      "step": 13146,
      "training_loss": 6.614320755004883
    },
    {
      "epoch": 0.7125745257452575,
      "step": 13147,
      "training_loss": 6.269593715667725
    },
    {
      "epoch": 0.7126287262872629,
      "grad_norm": 27.012561798095703,
      "learning_rate": 1e-05,
      "loss": 6.8739,
      "step": 13148
    },
    {
      "epoch": 0.7126287262872629,
      "step": 13148,
      "training_loss": 7.18259859085083
    },
    {
      "epoch": 0.7126829268292683,
      "step": 13149,
      "training_loss": 5.9862847328186035
    },
    {
      "epoch": 0.7127371273712737,
      "step": 13150,
      "training_loss": 6.023532390594482
    },
    {
      "epoch": 0.7127913279132791,
      "step": 13151,
      "training_loss": 6.716403484344482
    },
    {
      "epoch": 0.7128455284552846,
      "grad_norm": 22.754703521728516,
      "learning_rate": 1e-05,
      "loss": 6.4772,
      "step": 13152
    },
    {
      "epoch": 0.7128455284552846,
      "step": 13152,
      "training_loss": 6.955022811889648
    },
    {
      "epoch": 0.7128997289972899,
      "step": 13153,
      "training_loss": 3.051626205444336
    },
    {
      "epoch": 0.7129539295392954,
      "step": 13154,
      "training_loss": 4.5518927574157715
    },
    {
      "epoch": 0.7130081300813008,
      "step": 13155,
      "training_loss": 4.586121082305908
    },
    {
      "epoch": 0.7130623306233063,
      "grad_norm": 28.492332458496094,
      "learning_rate": 1e-05,
      "loss": 4.7862,
      "step": 13156
    },
    {
      "epoch": 0.7130623306233063,
      "step": 13156,
      "training_loss": 6.707155227661133
    },
    {
      "epoch": 0.7131165311653117,
      "step": 13157,
      "training_loss": 5.4715471267700195
    },
    {
      "epoch": 0.713170731707317,
      "step": 13158,
      "training_loss": 7.112586975097656
    },
    {
      "epoch": 0.7132249322493225,
      "step": 13159,
      "training_loss": 8.325483322143555
    },
    {
      "epoch": 0.7132791327913279,
      "grad_norm": 45.862213134765625,
      "learning_rate": 1e-05,
      "loss": 6.9042,
      "step": 13160
    },
    {
      "epoch": 0.7132791327913279,
      "step": 13160,
      "training_loss": 5.739996433258057
    },
    {
      "epoch": 0.7133333333333334,
      "step": 13161,
      "training_loss": 6.8025898933410645
    },
    {
      "epoch": 0.7133875338753387,
      "step": 13162,
      "training_loss": 6.324141502380371
    },
    {
      "epoch": 0.7134417344173442,
      "step": 13163,
      "training_loss": 5.341248035430908
    },
    {
      "epoch": 0.7134959349593496,
      "grad_norm": 26.19947052001953,
      "learning_rate": 1e-05,
      "loss": 6.052,
      "step": 13164
    },
    {
      "epoch": 0.7134959349593496,
      "step": 13164,
      "training_loss": 4.57912540435791
    },
    {
      "epoch": 0.713550135501355,
      "step": 13165,
      "training_loss": 6.823021411895752
    },
    {
      "epoch": 0.7136043360433605,
      "step": 13166,
      "training_loss": 6.039866924285889
    },
    {
      "epoch": 0.7136585365853658,
      "step": 13167,
      "training_loss": 4.435206413269043
    },
    {
      "epoch": 0.7137127371273713,
      "grad_norm": 45.487178802490234,
      "learning_rate": 1e-05,
      "loss": 5.4693,
      "step": 13168
    },
    {
      "epoch": 0.7137127371273713,
      "step": 13168,
      "training_loss": 6.514225006103516
    },
    {
      "epoch": 0.7137669376693767,
      "step": 13169,
      "training_loss": 7.068198204040527
    },
    {
      "epoch": 0.7138211382113822,
      "step": 13170,
      "training_loss": 6.9357428550720215
    },
    {
      "epoch": 0.7138753387533875,
      "step": 13171,
      "training_loss": 6.262918949127197
    },
    {
      "epoch": 0.7139295392953929,
      "grad_norm": 19.82747459411621,
      "learning_rate": 1e-05,
      "loss": 6.6953,
      "step": 13172
    },
    {
      "epoch": 0.7139295392953929,
      "step": 13172,
      "training_loss": 6.392280101776123
    },
    {
      "epoch": 0.7139837398373984,
      "step": 13173,
      "training_loss": 4.8991827964782715
    },
    {
      "epoch": 0.7140379403794038,
      "step": 13174,
      "training_loss": 5.677058219909668
    },
    {
      "epoch": 0.7140921409214093,
      "step": 13175,
      "training_loss": 7.383221626281738
    },
    {
      "epoch": 0.7141463414634146,
      "grad_norm": 25.374431610107422,
      "learning_rate": 1e-05,
      "loss": 6.0879,
      "step": 13176
    },
    {
      "epoch": 0.7141463414634146,
      "step": 13176,
      "training_loss": 7.203149795532227
    },
    {
      "epoch": 0.71420054200542,
      "step": 13177,
      "training_loss": 8.482892036437988
    },
    {
      "epoch": 0.7142547425474255,
      "step": 13178,
      "training_loss": 5.793209075927734
    },
    {
      "epoch": 0.7143089430894309,
      "step": 13179,
      "training_loss": 7.265188694000244
    },
    {
      "epoch": 0.7143631436314363,
      "grad_norm": 27.797807693481445,
      "learning_rate": 1e-05,
      "loss": 7.1861,
      "step": 13180
    },
    {
      "epoch": 0.7143631436314363,
      "step": 13180,
      "training_loss": 5.007872104644775
    },
    {
      "epoch": 0.7144173441734417,
      "step": 13181,
      "training_loss": 7.054932117462158
    },
    {
      "epoch": 0.7144715447154472,
      "step": 13182,
      "training_loss": 7.300564765930176
    },
    {
      "epoch": 0.7145257452574526,
      "step": 13183,
      "training_loss": 7.419626235961914
    },
    {
      "epoch": 0.714579945799458,
      "grad_norm": 26.09785270690918,
      "learning_rate": 1e-05,
      "loss": 6.6957,
      "step": 13184
    },
    {
      "epoch": 0.714579945799458,
      "step": 13184,
      "training_loss": 7.116580009460449
    },
    {
      "epoch": 0.7146341463414634,
      "step": 13185,
      "training_loss": 7.425554275512695
    },
    {
      "epoch": 0.7146883468834688,
      "step": 13186,
      "training_loss": 6.657524585723877
    },
    {
      "epoch": 0.7147425474254743,
      "step": 13187,
      "training_loss": 7.472699165344238
    },
    {
      "epoch": 0.7147967479674797,
      "grad_norm": 33.909908294677734,
      "learning_rate": 1e-05,
      "loss": 7.1681,
      "step": 13188
    },
    {
      "epoch": 0.7147967479674797,
      "step": 13188,
      "training_loss": 6.49721097946167
    },
    {
      "epoch": 0.714850948509485,
      "step": 13189,
      "training_loss": 7.633490085601807
    },
    {
      "epoch": 0.7149051490514905,
      "step": 13190,
      "training_loss": 6.9112420082092285
    },
    {
      "epoch": 0.7149593495934959,
      "step": 13191,
      "training_loss": 4.219686985015869
    },
    {
      "epoch": 0.7150135501355014,
      "grad_norm": 36.57786560058594,
      "learning_rate": 1e-05,
      "loss": 6.3154,
      "step": 13192
    },
    {
      "epoch": 0.7150135501355014,
      "step": 13192,
      "training_loss": 8.050610542297363
    },
    {
      "epoch": 0.7150677506775068,
      "step": 13193,
      "training_loss": 6.678335189819336
    },
    {
      "epoch": 0.7151219512195122,
      "step": 13194,
      "training_loss": 5.968000888824463
    },
    {
      "epoch": 0.7151761517615176,
      "step": 13195,
      "training_loss": 6.826639175415039
    },
    {
      "epoch": 0.715230352303523,
      "grad_norm": 19.646547317504883,
      "learning_rate": 1e-05,
      "loss": 6.8809,
      "step": 13196
    },
    {
      "epoch": 0.715230352303523,
      "step": 13196,
      "training_loss": 5.739468097686768
    },
    {
      "epoch": 0.7152845528455285,
      "step": 13197,
      "training_loss": 6.536761283874512
    },
    {
      "epoch": 0.7153387533875338,
      "step": 13198,
      "training_loss": 7.146003723144531
    },
    {
      "epoch": 0.7153929539295393,
      "step": 13199,
      "training_loss": 7.552235126495361
    },
    {
      "epoch": 0.7154471544715447,
      "grad_norm": 19.49837875366211,
      "learning_rate": 1e-05,
      "loss": 6.7436,
      "step": 13200
    },
    {
      "epoch": 0.7154471544715447,
      "step": 13200,
      "training_loss": 4.6031718254089355
    },
    {
      "epoch": 0.7155013550135502,
      "step": 13201,
      "training_loss": 7.3114447593688965
    },
    {
      "epoch": 0.7155555555555555,
      "step": 13202,
      "training_loss": 5.6519269943237305
    },
    {
      "epoch": 0.715609756097561,
      "step": 13203,
      "training_loss": 3.8925955295562744
    },
    {
      "epoch": 0.7156639566395664,
      "grad_norm": 53.991065979003906,
      "learning_rate": 1e-05,
      "loss": 5.3648,
      "step": 13204
    },
    {
      "epoch": 0.7156639566395664,
      "step": 13204,
      "training_loss": 6.7928690910339355
    },
    {
      "epoch": 0.7157181571815718,
      "step": 13205,
      "training_loss": 6.646694183349609
    },
    {
      "epoch": 0.7157723577235773,
      "step": 13206,
      "training_loss": 6.373787879943848
    },
    {
      "epoch": 0.7158265582655826,
      "step": 13207,
      "training_loss": 5.939112663269043
    },
    {
      "epoch": 0.7158807588075881,
      "grad_norm": 23.712467193603516,
      "learning_rate": 1e-05,
      "loss": 6.4381,
      "step": 13208
    },
    {
      "epoch": 0.7158807588075881,
      "step": 13208,
      "training_loss": 6.800149917602539
    },
    {
      "epoch": 0.7159349593495935,
      "step": 13209,
      "training_loss": 6.57784366607666
    },
    {
      "epoch": 0.715989159891599,
      "step": 13210,
      "training_loss": 3.9210383892059326
    },
    {
      "epoch": 0.7160433604336043,
      "step": 13211,
      "training_loss": 6.574557304382324
    },
    {
      "epoch": 0.7160975609756097,
      "grad_norm": 31.659955978393555,
      "learning_rate": 1e-05,
      "loss": 5.9684,
      "step": 13212
    },
    {
      "epoch": 0.7160975609756097,
      "step": 13212,
      "training_loss": 7.098886489868164
    },
    {
      "epoch": 0.7161517615176152,
      "step": 13213,
      "training_loss": 5.716943264007568
    },
    {
      "epoch": 0.7162059620596206,
      "step": 13214,
      "training_loss": 8.928184509277344
    },
    {
      "epoch": 0.7162601626016261,
      "step": 13215,
      "training_loss": 6.60182523727417
    },
    {
      "epoch": 0.7163143631436314,
      "grad_norm": 46.98234176635742,
      "learning_rate": 1e-05,
      "loss": 7.0865,
      "step": 13216
    },
    {
      "epoch": 0.7163143631436314,
      "step": 13216,
      "training_loss": 6.474893093109131
    },
    {
      "epoch": 0.7163685636856368,
      "step": 13217,
      "training_loss": 7.70739221572876
    },
    {
      "epoch": 0.7164227642276423,
      "step": 13218,
      "training_loss": 5.250882148742676
    },
    {
      "epoch": 0.7164769647696477,
      "step": 13219,
      "training_loss": 5.586735725402832
    },
    {
      "epoch": 0.7165311653116531,
      "grad_norm": 20.571271896362305,
      "learning_rate": 1e-05,
      "loss": 6.255,
      "step": 13220
    },
    {
      "epoch": 0.7165311653116531,
      "step": 13220,
      "training_loss": 6.704385757446289
    },
    {
      "epoch": 0.7165853658536585,
      "step": 13221,
      "training_loss": 5.96112060546875
    },
    {
      "epoch": 0.716639566395664,
      "step": 13222,
      "training_loss": 6.726015567779541
    },
    {
      "epoch": 0.7166937669376694,
      "step": 13223,
      "training_loss": 5.457848072052002
    },
    {
      "epoch": 0.7167479674796748,
      "grad_norm": 30.589590072631836,
      "learning_rate": 1e-05,
      "loss": 6.2123,
      "step": 13224
    },
    {
      "epoch": 0.7167479674796748,
      "step": 13224,
      "training_loss": 7.065558433532715
    },
    {
      "epoch": 0.7168021680216802,
      "step": 13225,
      "training_loss": 6.952506065368652
    },
    {
      "epoch": 0.7168563685636856,
      "step": 13226,
      "training_loss": 5.861491680145264
    },
    {
      "epoch": 0.7169105691056911,
      "step": 13227,
      "training_loss": 7.165417671203613
    },
    {
      "epoch": 0.7169647696476965,
      "grad_norm": 25.77341079711914,
      "learning_rate": 1e-05,
      "loss": 6.7612,
      "step": 13228
    },
    {
      "epoch": 0.7169647696476965,
      "step": 13228,
      "training_loss": 7.244140148162842
    },
    {
      "epoch": 0.7170189701897018,
      "step": 13229,
      "training_loss": 6.763965129852295
    },
    {
      "epoch": 0.7170731707317073,
      "step": 13230,
      "training_loss": 6.459204196929932
    },
    {
      "epoch": 0.7171273712737127,
      "step": 13231,
      "training_loss": 4.975596904754639
    },
    {
      "epoch": 0.7171815718157182,
      "grad_norm": 25.85598373413086,
      "learning_rate": 1e-05,
      "loss": 6.3607,
      "step": 13232
    },
    {
      "epoch": 0.7171815718157182,
      "step": 13232,
      "training_loss": 6.123290061950684
    },
    {
      "epoch": 0.7172357723577236,
      "step": 13233,
      "training_loss": 6.063517093658447
    },
    {
      "epoch": 0.717289972899729,
      "step": 13234,
      "training_loss": 8.53376579284668
    },
    {
      "epoch": 0.7173441734417344,
      "step": 13235,
      "training_loss": 6.931404113769531
    },
    {
      "epoch": 0.7173983739837398,
      "grad_norm": 23.246850967407227,
      "learning_rate": 1e-05,
      "loss": 6.913,
      "step": 13236
    },
    {
      "epoch": 0.7173983739837398,
      "step": 13236,
      "training_loss": 3.069284439086914
    },
    {
      "epoch": 0.7174525745257453,
      "step": 13237,
      "training_loss": 8.33031940460205
    },
    {
      "epoch": 0.7175067750677506,
      "step": 13238,
      "training_loss": 6.851667404174805
    },
    {
      "epoch": 0.7175609756097561,
      "step": 13239,
      "training_loss": 4.34522819519043
    },
    {
      "epoch": 0.7176151761517615,
      "grad_norm": 26.979787826538086,
      "learning_rate": 1e-05,
      "loss": 5.6491,
      "step": 13240
    },
    {
      "epoch": 0.7176151761517615,
      "step": 13240,
      "training_loss": 4.069032192230225
    },
    {
      "epoch": 0.717669376693767,
      "step": 13241,
      "training_loss": 7.913066864013672
    },
    {
      "epoch": 0.7177235772357724,
      "step": 13242,
      "training_loss": 6.06104850769043
    },
    {
      "epoch": 0.7177777777777777,
      "step": 13243,
      "training_loss": 7.282604217529297
    },
    {
      "epoch": 0.7178319783197832,
      "grad_norm": 23.299854278564453,
      "learning_rate": 1e-05,
      "loss": 6.3314,
      "step": 13244
    },
    {
      "epoch": 0.7178319783197832,
      "step": 13244,
      "training_loss": 7.1470866203308105
    },
    {
      "epoch": 0.7178861788617886,
      "step": 13245,
      "training_loss": 6.508866310119629
    },
    {
      "epoch": 0.7179403794037941,
      "step": 13246,
      "training_loss": 6.533463954925537
    },
    {
      "epoch": 0.7179945799457994,
      "step": 13247,
      "training_loss": 6.767521381378174
    },
    {
      "epoch": 0.7180487804878048,
      "grad_norm": 27.532272338867188,
      "learning_rate": 1e-05,
      "loss": 6.7392,
      "step": 13248
    },
    {
      "epoch": 0.7180487804878048,
      "step": 13248,
      "training_loss": 6.844742298126221
    },
    {
      "epoch": 0.7181029810298103,
      "step": 13249,
      "training_loss": 6.1819844245910645
    },
    {
      "epoch": 0.7181571815718157,
      "step": 13250,
      "training_loss": 6.335289478302002
    },
    {
      "epoch": 0.7182113821138212,
      "step": 13251,
      "training_loss": 6.598358631134033
    },
    {
      "epoch": 0.7182655826558265,
      "grad_norm": 38.40376663208008,
      "learning_rate": 1e-05,
      "loss": 6.4901,
      "step": 13252
    },
    {
      "epoch": 0.7182655826558265,
      "step": 13252,
      "training_loss": 6.379661560058594
    },
    {
      "epoch": 0.718319783197832,
      "step": 13253,
      "training_loss": 7.601834297180176
    },
    {
      "epoch": 0.7183739837398374,
      "step": 13254,
      "training_loss": 6.039095401763916
    },
    {
      "epoch": 0.7184281842818429,
      "step": 13255,
      "training_loss": 7.345287322998047
    },
    {
      "epoch": 0.7184823848238482,
      "grad_norm": 34.20930862426758,
      "learning_rate": 1e-05,
      "loss": 6.8415,
      "step": 13256
    },
    {
      "epoch": 0.7184823848238482,
      "step": 13256,
      "training_loss": 5.156648635864258
    },
    {
      "epoch": 0.7185365853658536,
      "step": 13257,
      "training_loss": 6.745880603790283
    },
    {
      "epoch": 0.7185907859078591,
      "step": 13258,
      "training_loss": 5.836207389831543
    },
    {
      "epoch": 0.7186449864498645,
      "step": 13259,
      "training_loss": 7.456315994262695
    },
    {
      "epoch": 0.71869918699187,
      "grad_norm": 18.514307022094727,
      "learning_rate": 1e-05,
      "loss": 6.2988,
      "step": 13260
    },
    {
      "epoch": 0.71869918699187,
      "step": 13260,
      "training_loss": 6.733096599578857
    },
    {
      "epoch": 0.7187533875338753,
      "step": 13261,
      "training_loss": 5.419743537902832
    },
    {
      "epoch": 0.7188075880758807,
      "step": 13262,
      "training_loss": 3.6077334880828857
    },
    {
      "epoch": 0.7188617886178862,
      "step": 13263,
      "training_loss": 6.3420844078063965
    },
    {
      "epoch": 0.7189159891598916,
      "grad_norm": 51.311771392822266,
      "learning_rate": 1e-05,
      "loss": 5.5257,
      "step": 13264
    },
    {
      "epoch": 0.7189159891598916,
      "step": 13264,
      "training_loss": 6.2845916748046875
    },
    {
      "epoch": 0.718970189701897,
      "step": 13265,
      "training_loss": 7.3543243408203125
    },
    {
      "epoch": 0.7190243902439024,
      "step": 13266,
      "training_loss": 7.804582118988037
    },
    {
      "epoch": 0.7190785907859079,
      "step": 13267,
      "training_loss": 4.621379852294922
    },
    {
      "epoch": 0.7191327913279133,
      "grad_norm": 28.910234451293945,
      "learning_rate": 1e-05,
      "loss": 6.5162,
      "step": 13268
    },
    {
      "epoch": 0.7191327913279133,
      "step": 13268,
      "training_loss": 6.683515548706055
    },
    {
      "epoch": 0.7191869918699187,
      "step": 13269,
      "training_loss": 4.458347320556641
    },
    {
      "epoch": 0.7192411924119241,
      "step": 13270,
      "training_loss": 6.777113914489746
    },
    {
      "epoch": 0.7192953929539295,
      "step": 13271,
      "training_loss": 7.721919536590576
    },
    {
      "epoch": 0.719349593495935,
      "grad_norm": 27.59107780456543,
      "learning_rate": 1e-05,
      "loss": 6.4102,
      "step": 13272
    },
    {
      "epoch": 0.719349593495935,
      "step": 13272,
      "training_loss": 6.147812366485596
    },
    {
      "epoch": 0.7194037940379404,
      "step": 13273,
      "training_loss": 6.5867791175842285
    },
    {
      "epoch": 0.7194579945799457,
      "step": 13274,
      "training_loss": 6.902504920959473
    },
    {
      "epoch": 0.7195121951219512,
      "step": 13275,
      "training_loss": 5.916768550872803
    },
    {
      "epoch": 0.7195663956639566,
      "grad_norm": 21.7803897857666,
      "learning_rate": 1e-05,
      "loss": 6.3885,
      "step": 13276
    },
    {
      "epoch": 0.7195663956639566,
      "step": 13276,
      "training_loss": 3.017545461654663
    },
    {
      "epoch": 0.7196205962059621,
      "step": 13277,
      "training_loss": 6.929685115814209
    },
    {
      "epoch": 0.7196747967479675,
      "step": 13278,
      "training_loss": 7.014784336090088
    },
    {
      "epoch": 0.7197289972899729,
      "step": 13279,
      "training_loss": 7.811357498168945
    },
    {
      "epoch": 0.7197831978319783,
      "grad_norm": 32.34319305419922,
      "learning_rate": 1e-05,
      "loss": 6.1933,
      "step": 13280
    },
    {
      "epoch": 0.7197831978319783,
      "step": 13280,
      "training_loss": 6.336000919342041
    },
    {
      "epoch": 0.7198373983739838,
      "step": 13281,
      "training_loss": 5.894397735595703
    },
    {
      "epoch": 0.7198915989159892,
      "step": 13282,
      "training_loss": 5.275509357452393
    },
    {
      "epoch": 0.7199457994579945,
      "step": 13283,
      "training_loss": 3.953361749649048
    },
    {
      "epoch": 0.72,
      "grad_norm": 31.092220306396484,
      "learning_rate": 1e-05,
      "loss": 5.3648,
      "step": 13284
    },
    {
      "epoch": 0.72,
      "step": 13284,
      "training_loss": 6.779118537902832
    },
    {
      "epoch": 0.7200542005420054,
      "step": 13285,
      "training_loss": 5.362677097320557
    },
    {
      "epoch": 0.7201084010840109,
      "step": 13286,
      "training_loss": 5.0770111083984375
    },
    {
      "epoch": 0.7201626016260163,
      "step": 13287,
      "training_loss": 2.993985891342163
    },
    {
      "epoch": 0.7202168021680216,
      "grad_norm": 35.99941635131836,
      "learning_rate": 1e-05,
      "loss": 5.0532,
      "step": 13288
    },
    {
      "epoch": 0.7202168021680216,
      "step": 13288,
      "training_loss": 7.165435314178467
    },
    {
      "epoch": 0.7202710027100271,
      "step": 13289,
      "training_loss": 7.994208812713623
    },
    {
      "epoch": 0.7203252032520325,
      "step": 13290,
      "training_loss": 7.17852258682251
    },
    {
      "epoch": 0.720379403794038,
      "step": 13291,
      "training_loss": 6.4525980949401855
    },
    {
      "epoch": 0.7204336043360433,
      "grad_norm": 29.044330596923828,
      "learning_rate": 1e-05,
      "loss": 7.1977,
      "step": 13292
    },
    {
      "epoch": 0.7204336043360433,
      "step": 13292,
      "training_loss": 6.4101362228393555
    },
    {
      "epoch": 0.7204878048780488,
      "step": 13293,
      "training_loss": 6.48541259765625
    },
    {
      "epoch": 0.7205420054200542,
      "step": 13294,
      "training_loss": 7.32874059677124
    },
    {
      "epoch": 0.7205962059620596,
      "step": 13295,
      "training_loss": 4.054500102996826
    },
    {
      "epoch": 0.7206504065040651,
      "grad_norm": 72.89259338378906,
      "learning_rate": 1e-05,
      "loss": 6.0697,
      "step": 13296
    },
    {
      "epoch": 0.7206504065040651,
      "step": 13296,
      "training_loss": 6.429282188415527
    },
    {
      "epoch": 0.7207046070460704,
      "step": 13297,
      "training_loss": 5.923741340637207
    },
    {
      "epoch": 0.7207588075880759,
      "step": 13298,
      "training_loss": 7.322052478790283
    },
    {
      "epoch": 0.7208130081300813,
      "step": 13299,
      "training_loss": 6.25271463394165
    },
    {
      "epoch": 0.7208672086720868,
      "grad_norm": 20.209802627563477,
      "learning_rate": 1e-05,
      "loss": 6.4819,
      "step": 13300
    },
    {
      "epoch": 0.7208672086720868,
      "step": 13300,
      "training_loss": 6.335618019104004
    },
    {
      "epoch": 0.7209214092140921,
      "step": 13301,
      "training_loss": 6.700979709625244
    },
    {
      "epoch": 0.7209756097560975,
      "step": 13302,
      "training_loss": 6.196776390075684
    },
    {
      "epoch": 0.721029810298103,
      "step": 13303,
      "training_loss": 6.569411277770996
    },
    {
      "epoch": 0.7210840108401084,
      "grad_norm": 33.055999755859375,
      "learning_rate": 1e-05,
      "loss": 6.4507,
      "step": 13304
    },
    {
      "epoch": 0.7210840108401084,
      "step": 13304,
      "training_loss": 7.200559139251709
    },
    {
      "epoch": 0.7211382113821139,
      "step": 13305,
      "training_loss": 6.4992876052856445
    },
    {
      "epoch": 0.7211924119241192,
      "step": 13306,
      "training_loss": 7.661912441253662
    },
    {
      "epoch": 0.7212466124661246,
      "step": 13307,
      "training_loss": 6.7639641761779785
    },
    {
      "epoch": 0.7213008130081301,
      "grad_norm": 27.359981536865234,
      "learning_rate": 1e-05,
      "loss": 7.0314,
      "step": 13308
    },
    {
      "epoch": 0.7213008130081301,
      "step": 13308,
      "training_loss": 6.043438911437988
    },
    {
      "epoch": 0.7213550135501355,
      "step": 13309,
      "training_loss": 5.552982330322266
    },
    {
      "epoch": 0.7214092140921409,
      "step": 13310,
      "training_loss": 7.445681095123291
    },
    {
      "epoch": 0.7214634146341463,
      "step": 13311,
      "training_loss": 6.086198329925537
    },
    {
      "epoch": 0.7215176151761518,
      "grad_norm": 29.88343620300293,
      "learning_rate": 1e-05,
      "loss": 6.2821,
      "step": 13312
    },
    {
      "epoch": 0.7215176151761518,
      "step": 13312,
      "training_loss": 7.920130729675293
    },
    {
      "epoch": 0.7215718157181572,
      "step": 13313,
      "training_loss": 5.746047019958496
    },
    {
      "epoch": 0.7216260162601627,
      "step": 13314,
      "training_loss": 6.753015041351318
    },
    {
      "epoch": 0.721680216802168,
      "step": 13315,
      "training_loss": 5.420428276062012
    },
    {
      "epoch": 0.7217344173441734,
      "grad_norm": 24.839906692504883,
      "learning_rate": 1e-05,
      "loss": 6.4599,
      "step": 13316
    },
    {
      "epoch": 0.7217344173441734,
      "step": 13316,
      "training_loss": 7.109580993652344
    },
    {
      "epoch": 0.7217886178861789,
      "step": 13317,
      "training_loss": 6.950700759887695
    },
    {
      "epoch": 0.7218428184281843,
      "step": 13318,
      "training_loss": 7.453989505767822
    },
    {
      "epoch": 0.7218970189701897,
      "step": 13319,
      "training_loss": 7.505390644073486
    },
    {
      "epoch": 0.7219512195121951,
      "grad_norm": 26.779624938964844,
      "learning_rate": 1e-05,
      "loss": 7.2549,
      "step": 13320
    },
    {
      "epoch": 0.7219512195121951,
      "step": 13320,
      "training_loss": 6.58603572845459
    },
    {
      "epoch": 0.7220054200542005,
      "step": 13321,
      "training_loss": 4.865367412567139
    },
    {
      "epoch": 0.722059620596206,
      "step": 13322,
      "training_loss": 6.504351615905762
    },
    {
      "epoch": 0.7221138211382114,
      "step": 13323,
      "training_loss": 7.340261936187744
    },
    {
      "epoch": 0.7221680216802168,
      "grad_norm": 55.51728057861328,
      "learning_rate": 1e-05,
      "loss": 6.324,
      "step": 13324
    },
    {
      "epoch": 0.7221680216802168,
      "step": 13324,
      "training_loss": 6.290299415588379
    },
    {
      "epoch": 0.7222222222222222,
      "step": 13325,
      "training_loss": 5.697800159454346
    },
    {
      "epoch": 0.7222764227642277,
      "step": 13326,
      "training_loss": 6.763470649719238
    },
    {
      "epoch": 0.7223306233062331,
      "step": 13327,
      "training_loss": 7.115145206451416
    },
    {
      "epoch": 0.7223848238482384,
      "grad_norm": 63.06572723388672,
      "learning_rate": 1e-05,
      "loss": 6.4667,
      "step": 13328
    },
    {
      "epoch": 0.7223848238482384,
      "step": 13328,
      "training_loss": 6.640764236450195
    },
    {
      "epoch": 0.7224390243902439,
      "step": 13329,
      "training_loss": 7.315502166748047
    },
    {
      "epoch": 0.7224932249322493,
      "step": 13330,
      "training_loss": 5.894305229187012
    },
    {
      "epoch": 0.7225474254742548,
      "step": 13331,
      "training_loss": 7.174777507781982
    },
    {
      "epoch": 0.7226016260162602,
      "grad_norm": 16.91326141357422,
      "learning_rate": 1e-05,
      "loss": 6.7563,
      "step": 13332
    },
    {
      "epoch": 0.7226016260162602,
      "step": 13332,
      "training_loss": 8.56973648071289
    },
    {
      "epoch": 0.7226558265582655,
      "step": 13333,
      "training_loss": 6.391237735748291
    },
    {
      "epoch": 0.722710027100271,
      "step": 13334,
      "training_loss": 6.4929118156433105
    },
    {
      "epoch": 0.7227642276422764,
      "step": 13335,
      "training_loss": 6.35720682144165
    },
    {
      "epoch": 0.7228184281842819,
      "grad_norm": 38.61366653442383,
      "learning_rate": 1e-05,
      "loss": 6.9528,
      "step": 13336
    },
    {
      "epoch": 0.7228184281842819,
      "step": 13336,
      "training_loss": 3.4226086139678955
    },
    {
      "epoch": 0.7228726287262872,
      "step": 13337,
      "training_loss": 4.624757289886475
    },
    {
      "epoch": 0.7229268292682927,
      "step": 13338,
      "training_loss": 7.82020902633667
    },
    {
      "epoch": 0.7229810298102981,
      "step": 13339,
      "training_loss": 6.681956768035889
    },
    {
      "epoch": 0.7230352303523035,
      "grad_norm": 29.461572647094727,
      "learning_rate": 1e-05,
      "loss": 5.6374,
      "step": 13340
    },
    {
      "epoch": 0.7230352303523035,
      "step": 13340,
      "training_loss": 7.061039447784424
    },
    {
      "epoch": 0.723089430894309,
      "step": 13341,
      "training_loss": 5.6132402420043945
    },
    {
      "epoch": 0.7231436314363143,
      "step": 13342,
      "training_loss": 4.223852634429932
    },
    {
      "epoch": 0.7231978319783198,
      "step": 13343,
      "training_loss": 6.721418857574463
    },
    {
      "epoch": 0.7232520325203252,
      "grad_norm": 23.51790428161621,
      "learning_rate": 1e-05,
      "loss": 5.9049,
      "step": 13344
    },
    {
      "epoch": 0.7232520325203252,
      "step": 13344,
      "training_loss": 4.881425380706787
    },
    {
      "epoch": 0.7233062330623307,
      "step": 13345,
      "training_loss": 7.865169525146484
    },
    {
      "epoch": 0.723360433604336,
      "step": 13346,
      "training_loss": 6.99532413482666
    },
    {
      "epoch": 0.7234146341463414,
      "step": 13347,
      "training_loss": 6.671933174133301
    },
    {
      "epoch": 0.7234688346883469,
      "grad_norm": 20.337421417236328,
      "learning_rate": 1e-05,
      "loss": 6.6035,
      "step": 13348
    },
    {
      "epoch": 0.7234688346883469,
      "step": 13348,
      "training_loss": 7.107320785522461
    },
    {
      "epoch": 0.7235230352303523,
      "step": 13349,
      "training_loss": 7.570347309112549
    },
    {
      "epoch": 0.7235772357723578,
      "step": 13350,
      "training_loss": 5.913631439208984
    },
    {
      "epoch": 0.7236314363143631,
      "step": 13351,
      "training_loss": 7.189245700836182
    },
    {
      "epoch": 0.7236856368563686,
      "grad_norm": 22.227516174316406,
      "learning_rate": 1e-05,
      "loss": 6.9451,
      "step": 13352
    },
    {
      "epoch": 0.7236856368563686,
      "step": 13352,
      "training_loss": 8.030665397644043
    },
    {
      "epoch": 0.723739837398374,
      "step": 13353,
      "training_loss": 7.560267925262451
    },
    {
      "epoch": 0.7237940379403794,
      "step": 13354,
      "training_loss": 6.621577262878418
    },
    {
      "epoch": 0.7238482384823848,
      "step": 13355,
      "training_loss": 6.821101188659668
    },
    {
      "epoch": 0.7239024390243902,
      "grad_norm": 29.935970306396484,
      "learning_rate": 1e-05,
      "loss": 7.2584,
      "step": 13356
    },
    {
      "epoch": 0.7239024390243902,
      "step": 13356,
      "training_loss": 6.075212001800537
    },
    {
      "epoch": 0.7239566395663957,
      "step": 13357,
      "training_loss": 4.023158550262451
    },
    {
      "epoch": 0.7240108401084011,
      "step": 13358,
      "training_loss": 7.1503448486328125
    },
    {
      "epoch": 0.7240650406504066,
      "step": 13359,
      "training_loss": 5.718767166137695
    },
    {
      "epoch": 0.7241192411924119,
      "grad_norm": 22.76907730102539,
      "learning_rate": 1e-05,
      "loss": 5.7419,
      "step": 13360
    },
    {
      "epoch": 0.7241192411924119,
      "step": 13360,
      "training_loss": 6.750537395477295
    },
    {
      "epoch": 0.7241734417344173,
      "step": 13361,
      "training_loss": 9.054659843444824
    },
    {
      "epoch": 0.7242276422764228,
      "step": 13362,
      "training_loss": 7.063345909118652
    },
    {
      "epoch": 0.7242818428184282,
      "step": 13363,
      "training_loss": 7.206717014312744
    },
    {
      "epoch": 0.7243360433604336,
      "grad_norm": 24.293720245361328,
      "learning_rate": 1e-05,
      "loss": 7.5188,
      "step": 13364
    },
    {
      "epoch": 0.7243360433604336,
      "step": 13364,
      "training_loss": 6.9697136878967285
    },
    {
      "epoch": 0.724390243902439,
      "step": 13365,
      "training_loss": 7.130126953125
    },
    {
      "epoch": 0.7244444444444444,
      "step": 13366,
      "training_loss": 8.000802993774414
    },
    {
      "epoch": 0.7244986449864499,
      "step": 13367,
      "training_loss": 6.5290656089782715
    },
    {
      "epoch": 0.7245528455284553,
      "grad_norm": 18.228740692138672,
      "learning_rate": 1e-05,
      "loss": 7.1574,
      "step": 13368
    },
    {
      "epoch": 0.7245528455284553,
      "step": 13368,
      "training_loss": 6.358972072601318
    },
    {
      "epoch": 0.7246070460704607,
      "step": 13369,
      "training_loss": 5.807332992553711
    },
    {
      "epoch": 0.7246612466124661,
      "step": 13370,
      "training_loss": 7.003327369689941
    },
    {
      "epoch": 0.7247154471544716,
      "step": 13371,
      "training_loss": 5.490664958953857
    },
    {
      "epoch": 0.724769647696477,
      "grad_norm": 30.35714340209961,
      "learning_rate": 1e-05,
      "loss": 6.1651,
      "step": 13372
    },
    {
      "epoch": 0.724769647696477,
      "step": 13372,
      "training_loss": 6.058359146118164
    },
    {
      "epoch": 0.7248238482384823,
      "step": 13373,
      "training_loss": 7.393620014190674
    },
    {
      "epoch": 0.7248780487804878,
      "step": 13374,
      "training_loss": 7.753908634185791
    },
    {
      "epoch": 0.7249322493224932,
      "step": 13375,
      "training_loss": 4.141869068145752
    },
    {
      "epoch": 0.7249864498644987,
      "grad_norm": 23.61443519592285,
      "learning_rate": 1e-05,
      "loss": 6.3369,
      "step": 13376
    },
    {
      "epoch": 0.7249864498644987,
      "step": 13376,
      "training_loss": 6.822795391082764
    },
    {
      "epoch": 0.7250406504065041,
      "step": 13377,
      "training_loss": 6.224702835083008
    },
    {
      "epoch": 0.7250948509485095,
      "step": 13378,
      "training_loss": 7.325944423675537
    },
    {
      "epoch": 0.7251490514905149,
      "step": 13379,
      "training_loss": 5.8769145011901855
    },
    {
      "epoch": 0.7252032520325203,
      "grad_norm": 24.305227279663086,
      "learning_rate": 1e-05,
      "loss": 6.5626,
      "step": 13380
    },
    {
      "epoch": 0.7252032520325203,
      "step": 13380,
      "training_loss": 5.535529136657715
    },
    {
      "epoch": 0.7252574525745258,
      "step": 13381,
      "training_loss": 7.14152193069458
    },
    {
      "epoch": 0.7253116531165311,
      "step": 13382,
      "training_loss": 7.6034135818481445
    },
    {
      "epoch": 0.7253658536585366,
      "step": 13383,
      "training_loss": 6.4716057777404785
    },
    {
      "epoch": 0.725420054200542,
      "grad_norm": 22.955135345458984,
      "learning_rate": 1e-05,
      "loss": 6.688,
      "step": 13384
    },
    {
      "epoch": 0.725420054200542,
      "step": 13384,
      "training_loss": 5.910870552062988
    },
    {
      "epoch": 0.7254742547425475,
      "step": 13385,
      "training_loss": 6.531782150268555
    },
    {
      "epoch": 0.7255284552845529,
      "step": 13386,
      "training_loss": 5.997900485992432
    },
    {
      "epoch": 0.7255826558265582,
      "step": 13387,
      "training_loss": 6.344372272491455
    },
    {
      "epoch": 0.7256368563685637,
      "grad_norm": 50.454856872558594,
      "learning_rate": 1e-05,
      "loss": 6.1962,
      "step": 13388
    },
    {
      "epoch": 0.7256368563685637,
      "step": 13388,
      "training_loss": 6.025367736816406
    },
    {
      "epoch": 0.7256910569105691,
      "step": 13389,
      "training_loss": 5.338465213775635
    },
    {
      "epoch": 0.7257452574525746,
      "step": 13390,
      "training_loss": 6.299246311187744
    },
    {
      "epoch": 0.7257994579945799,
      "step": 13391,
      "training_loss": 7.477888584136963
    },
    {
      "epoch": 0.7258536585365853,
      "grad_norm": 26.787691116333008,
      "learning_rate": 1e-05,
      "loss": 6.2852,
      "step": 13392
    },
    {
      "epoch": 0.7258536585365853,
      "step": 13392,
      "training_loss": 6.511817932128906
    },
    {
      "epoch": 0.7259078590785908,
      "step": 13393,
      "training_loss": 4.803808689117432
    },
    {
      "epoch": 0.7259620596205962,
      "step": 13394,
      "training_loss": 7.020132541656494
    },
    {
      "epoch": 0.7260162601626017,
      "step": 13395,
      "training_loss": 7.336922645568848
    },
    {
      "epoch": 0.726070460704607,
      "grad_norm": 23.825885772705078,
      "learning_rate": 1e-05,
      "loss": 6.4182,
      "step": 13396
    },
    {
      "epoch": 0.726070460704607,
      "step": 13396,
      "training_loss": 6.721776485443115
    },
    {
      "epoch": 0.7261246612466125,
      "step": 13397,
      "training_loss": 7.069721698760986
    },
    {
      "epoch": 0.7261788617886179,
      "step": 13398,
      "training_loss": 4.119576930999756
    },
    {
      "epoch": 0.7262330623306233,
      "step": 13399,
      "training_loss": 5.0231451988220215
    },
    {
      "epoch": 0.7262872628726287,
      "grad_norm": 30.400407791137695,
      "learning_rate": 1e-05,
      "loss": 5.7336,
      "step": 13400
    },
    {
      "epoch": 0.7262872628726287,
      "step": 13400,
      "training_loss": 6.87411642074585
    },
    {
      "epoch": 0.7263414634146341,
      "step": 13401,
      "training_loss": 5.816944122314453
    },
    {
      "epoch": 0.7263956639566396,
      "step": 13402,
      "training_loss": 4.759997367858887
    },
    {
      "epoch": 0.726449864498645,
      "step": 13403,
      "training_loss": 7.333827018737793
    },
    {
      "epoch": 0.7265040650406505,
      "grad_norm": 24.076601028442383,
      "learning_rate": 1e-05,
      "loss": 6.1962,
      "step": 13404
    },
    {
      "epoch": 0.7265040650406505,
      "step": 13404,
      "training_loss": 7.167774677276611
    },
    {
      "epoch": 0.7265582655826558,
      "step": 13405,
      "training_loss": 5.223159313201904
    },
    {
      "epoch": 0.7266124661246612,
      "step": 13406,
      "training_loss": 6.522403240203857
    },
    {
      "epoch": 0.7266666666666667,
      "step": 13407,
      "training_loss": 6.677496433258057
    },
    {
      "epoch": 0.7267208672086721,
      "grad_norm": 22.96038055419922,
      "learning_rate": 1e-05,
      "loss": 6.3977,
      "step": 13408
    },
    {
      "epoch": 0.7267208672086721,
      "step": 13408,
      "training_loss": 7.013991832733154
    },
    {
      "epoch": 0.7267750677506775,
      "step": 13409,
      "training_loss": 6.370481491088867
    },
    {
      "epoch": 0.7268292682926829,
      "step": 13410,
      "training_loss": 4.879504680633545
    },
    {
      "epoch": 0.7268834688346884,
      "step": 13411,
      "training_loss": 8.038801193237305
    },
    {
      "epoch": 0.7269376693766938,
      "grad_norm": 54.9650764465332,
      "learning_rate": 1e-05,
      "loss": 6.5757,
      "step": 13412
    },
    {
      "epoch": 0.7269376693766938,
      "step": 13412,
      "training_loss": 4.545993328094482
    },
    {
      "epoch": 0.7269918699186992,
      "step": 13413,
      "training_loss": 5.122084617614746
    },
    {
      "epoch": 0.7270460704607046,
      "step": 13414,
      "training_loss": 4.196743965148926
    },
    {
      "epoch": 0.72710027100271,
      "step": 13415,
      "training_loss": 6.994694232940674
    },
    {
      "epoch": 0.7271544715447155,
      "grad_norm": 38.566890716552734,
      "learning_rate": 1e-05,
      "loss": 5.2149,
      "step": 13416
    },
    {
      "epoch": 0.7271544715447155,
      "step": 13416,
      "training_loss": 6.905760288238525
    },
    {
      "epoch": 0.7272086720867209,
      "step": 13417,
      "training_loss": 6.548398971557617
    },
    {
      "epoch": 0.7272628726287262,
      "step": 13418,
      "training_loss": 6.89329195022583
    },
    {
      "epoch": 0.7273170731707317,
      "step": 13419,
      "training_loss": 7.077906131744385
    },
    {
      "epoch": 0.7273712737127371,
      "grad_norm": 20.22336196899414,
      "learning_rate": 1e-05,
      "loss": 6.8563,
      "step": 13420
    },
    {
      "epoch": 0.7273712737127371,
      "step": 13420,
      "training_loss": 6.352447032928467
    },
    {
      "epoch": 0.7274254742547426,
      "step": 13421,
      "training_loss": 4.640440940856934
    },
    {
      "epoch": 0.727479674796748,
      "step": 13422,
      "training_loss": 5.982509613037109
    },
    {
      "epoch": 0.7275338753387534,
      "step": 13423,
      "training_loss": 5.990671157836914
    },
    {
      "epoch": 0.7275880758807588,
      "grad_norm": 24.239904403686523,
      "learning_rate": 1e-05,
      "loss": 5.7415,
      "step": 13424
    },
    {
      "epoch": 0.7275880758807588,
      "step": 13424,
      "training_loss": 7.6788482666015625
    },
    {
      "epoch": 0.7276422764227642,
      "step": 13425,
      "training_loss": 5.849077224731445
    },
    {
      "epoch": 0.7276964769647697,
      "step": 13426,
      "training_loss": 7.427559852600098
    },
    {
      "epoch": 0.727750677506775,
      "step": 13427,
      "training_loss": 6.323968887329102
    },
    {
      "epoch": 0.7278048780487805,
      "grad_norm": 27.30931282043457,
      "learning_rate": 1e-05,
      "loss": 6.8199,
      "step": 13428
    },
    {
      "epoch": 0.7278048780487805,
      "step": 13428,
      "training_loss": 4.963994979858398
    },
    {
      "epoch": 0.7278590785907859,
      "step": 13429,
      "training_loss": 7.491960048675537
    },
    {
      "epoch": 0.7279132791327914,
      "step": 13430,
      "training_loss": 5.198741436004639
    },
    {
      "epoch": 0.7279674796747968,
      "step": 13431,
      "training_loss": 7.215926647186279
    },
    {
      "epoch": 0.7280216802168021,
      "grad_norm": 27.084550857543945,
      "learning_rate": 1e-05,
      "loss": 6.2177,
      "step": 13432
    },
    {
      "epoch": 0.7280216802168021,
      "step": 13432,
      "training_loss": 6.815515518188477
    },
    {
      "epoch": 0.7280758807588076,
      "step": 13433,
      "training_loss": 6.710438251495361
    },
    {
      "epoch": 0.728130081300813,
      "step": 13434,
      "training_loss": 2.9377601146698
    },
    {
      "epoch": 0.7281842818428185,
      "step": 13435,
      "training_loss": 6.509721279144287
    },
    {
      "epoch": 0.7282384823848238,
      "grad_norm": 42.67702865600586,
      "learning_rate": 1e-05,
      "loss": 5.7434,
      "step": 13436
    },
    {
      "epoch": 0.7282384823848238,
      "step": 13436,
      "training_loss": 6.383319854736328
    },
    {
      "epoch": 0.7282926829268292,
      "step": 13437,
      "training_loss": 7.118750095367432
    },
    {
      "epoch": 0.7283468834688347,
      "step": 13438,
      "training_loss": 6.28026819229126
    },
    {
      "epoch": 0.7284010840108401,
      "step": 13439,
      "training_loss": 6.041662693023682
    },
    {
      "epoch": 0.7284552845528456,
      "grad_norm": 41.178993225097656,
      "learning_rate": 1e-05,
      "loss": 6.456,
      "step": 13440
    },
    {
      "epoch": 0.7284552845528456,
      "step": 13440,
      "training_loss": 7.7730712890625
    },
    {
      "epoch": 0.7285094850948509,
      "step": 13441,
      "training_loss": 5.834887981414795
    },
    {
      "epoch": 0.7285636856368564,
      "step": 13442,
      "training_loss": 3.209634780883789
    },
    {
      "epoch": 0.7286178861788618,
      "step": 13443,
      "training_loss": 5.7978949546813965
    },
    {
      "epoch": 0.7286720867208673,
      "grad_norm": 23.33574867248535,
      "learning_rate": 1e-05,
      "loss": 5.6539,
      "step": 13444
    },
    {
      "epoch": 0.7286720867208673,
      "step": 13444,
      "training_loss": 7.32689905166626
    },
    {
      "epoch": 0.7287262872628726,
      "step": 13445,
      "training_loss": 7.446115493774414
    },
    {
      "epoch": 0.728780487804878,
      "step": 13446,
      "training_loss": 7.211875915527344
    },
    {
      "epoch": 0.7288346883468835,
      "step": 13447,
      "training_loss": 7.498802661895752
    },
    {
      "epoch": 0.7288888888888889,
      "grad_norm": 42.21697998046875,
      "learning_rate": 1e-05,
      "loss": 7.3709,
      "step": 13448
    },
    {
      "epoch": 0.7288888888888889,
      "step": 13448,
      "training_loss": 6.510431289672852
    },
    {
      "epoch": 0.7289430894308944,
      "step": 13449,
      "training_loss": 8.105310440063477
    },
    {
      "epoch": 0.7289972899728997,
      "step": 13450,
      "training_loss": 5.128692150115967
    },
    {
      "epoch": 0.7290514905149051,
      "step": 13451,
      "training_loss": 6.808637619018555
    },
    {
      "epoch": 0.7291056910569106,
      "grad_norm": 21.144580841064453,
      "learning_rate": 1e-05,
      "loss": 6.6383,
      "step": 13452
    },
    {
      "epoch": 0.7291056910569106,
      "step": 13452,
      "training_loss": 8.251049041748047
    },
    {
      "epoch": 0.729159891598916,
      "step": 13453,
      "training_loss": 6.882605075836182
    },
    {
      "epoch": 0.7292140921409214,
      "step": 13454,
      "training_loss": 8.889238357543945
    },
    {
      "epoch": 0.7292682926829268,
      "step": 13455,
      "training_loss": 7.003369331359863
    },
    {
      "epoch": 0.7293224932249323,
      "grad_norm": 29.478683471679688,
      "learning_rate": 1e-05,
      "loss": 7.7566,
      "step": 13456
    },
    {
      "epoch": 0.7293224932249323,
      "step": 13456,
      "training_loss": 6.060839653015137
    },
    {
      "epoch": 0.7293766937669377,
      "step": 13457,
      "training_loss": 7.01409912109375
    },
    {
      "epoch": 0.729430894308943,
      "step": 13458,
      "training_loss": 7.344442367553711
    },
    {
      "epoch": 0.7294850948509485,
      "step": 13459,
      "training_loss": 6.189558029174805
    },
    {
      "epoch": 0.7295392953929539,
      "grad_norm": 20.897193908691406,
      "learning_rate": 1e-05,
      "loss": 6.6522,
      "step": 13460
    },
    {
      "epoch": 0.7295392953929539,
      "step": 13460,
      "training_loss": 8.062010765075684
    },
    {
      "epoch": 0.7295934959349594,
      "step": 13461,
      "training_loss": 3.8508715629577637
    },
    {
      "epoch": 0.7296476964769648,
      "step": 13462,
      "training_loss": 6.286698818206787
    },
    {
      "epoch": 0.7297018970189701,
      "step": 13463,
      "training_loss": 5.245763301849365
    },
    {
      "epoch": 0.7297560975609756,
      "grad_norm": 37.3796272277832,
      "learning_rate": 1e-05,
      "loss": 5.8613,
      "step": 13464
    },
    {
      "epoch": 0.7297560975609756,
      "step": 13464,
      "training_loss": 4.944803714752197
    },
    {
      "epoch": 0.729810298102981,
      "step": 13465,
      "training_loss": 5.616625785827637
    },
    {
      "epoch": 0.7298644986449865,
      "step": 13466,
      "training_loss": 7.1992926597595215
    },
    {
      "epoch": 0.7299186991869918,
      "step": 13467,
      "training_loss": 6.100574970245361
    },
    {
      "epoch": 0.7299728997289973,
      "grad_norm": 23.064895629882812,
      "learning_rate": 1e-05,
      "loss": 5.9653,
      "step": 13468
    },
    {
      "epoch": 0.7299728997289973,
      "step": 13468,
      "training_loss": 7.149848937988281
    },
    {
      "epoch": 0.7300271002710027,
      "step": 13469,
      "training_loss": 7.522017478942871
    },
    {
      "epoch": 0.7300813008130081,
      "step": 13470,
      "training_loss": 6.835087299346924
    },
    {
      "epoch": 0.7301355013550136,
      "step": 13471,
      "training_loss": 6.329822540283203
    },
    {
      "epoch": 0.7301897018970189,
      "grad_norm": 24.567066192626953,
      "learning_rate": 1e-05,
      "loss": 6.9592,
      "step": 13472
    },
    {
      "epoch": 0.7301897018970189,
      "step": 13472,
      "training_loss": 4.123019695281982
    },
    {
      "epoch": 0.7302439024390244,
      "step": 13473,
      "training_loss": 5.919365882873535
    },
    {
      "epoch": 0.7302981029810298,
      "step": 13474,
      "training_loss": 6.4649577140808105
    },
    {
      "epoch": 0.7303523035230353,
      "step": 13475,
      "training_loss": 7.122325420379639
    },
    {
      "epoch": 0.7304065040650406,
      "grad_norm": 20.553972244262695,
      "learning_rate": 1e-05,
      "loss": 5.9074,
      "step": 13476
    },
    {
      "epoch": 0.7304065040650406,
      "step": 13476,
      "training_loss": 6.342443466186523
    },
    {
      "epoch": 0.730460704607046,
      "step": 13477,
      "training_loss": 6.8882060050964355
    },
    {
      "epoch": 0.7305149051490515,
      "step": 13478,
      "training_loss": 5.746592044830322
    },
    {
      "epoch": 0.7305691056910569,
      "step": 13479,
      "training_loss": 5.877409934997559
    },
    {
      "epoch": 0.7306233062330624,
      "grad_norm": 28.034446716308594,
      "learning_rate": 1e-05,
      "loss": 6.2137,
      "step": 13480
    },
    {
      "epoch": 0.7306233062330624,
      "step": 13480,
      "training_loss": 6.73954963684082
    },
    {
      "epoch": 0.7306775067750677,
      "step": 13481,
      "training_loss": 7.621883869171143
    },
    {
      "epoch": 0.7307317073170732,
      "step": 13482,
      "training_loss": 8.661452293395996
    },
    {
      "epoch": 0.7307859078590786,
      "step": 13483,
      "training_loss": 6.252171516418457
    },
    {
      "epoch": 0.730840108401084,
      "grad_norm": 53.64408874511719,
      "learning_rate": 1e-05,
      "loss": 7.3188,
      "step": 13484
    },
    {
      "epoch": 0.730840108401084,
      "step": 13484,
      "training_loss": 6.638716697692871
    },
    {
      "epoch": 0.7308943089430894,
      "step": 13485,
      "training_loss": 7.146195411682129
    },
    {
      "epoch": 0.7309485094850948,
      "step": 13486,
      "training_loss": 5.481610298156738
    },
    {
      "epoch": 0.7310027100271003,
      "step": 13487,
      "training_loss": 4.194460391998291
    },
    {
      "epoch": 0.7310569105691057,
      "grad_norm": 35.501380920410156,
      "learning_rate": 1e-05,
      "loss": 5.8652,
      "step": 13488
    },
    {
      "epoch": 0.7310569105691057,
      "step": 13488,
      "training_loss": 7.955050468444824
    },
    {
      "epoch": 0.7311111111111112,
      "step": 13489,
      "training_loss": 6.061462879180908
    },
    {
      "epoch": 0.7311653116531165,
      "step": 13490,
      "training_loss": 6.844863414764404
    },
    {
      "epoch": 0.7312195121951219,
      "step": 13491,
      "training_loss": 7.370242118835449
    },
    {
      "epoch": 0.7312737127371274,
      "grad_norm": 34.1123161315918,
      "learning_rate": 1e-05,
      "loss": 7.0579,
      "step": 13492
    },
    {
      "epoch": 0.7312737127371274,
      "step": 13492,
      "training_loss": 5.779714584350586
    },
    {
      "epoch": 0.7313279132791328,
      "step": 13493,
      "training_loss": 6.976110458374023
    },
    {
      "epoch": 0.7313821138211382,
      "step": 13494,
      "training_loss": 6.599745750427246
    },
    {
      "epoch": 0.7314363143631436,
      "step": 13495,
      "training_loss": 4.3173298835754395
    },
    {
      "epoch": 0.731490514905149,
      "grad_norm": 34.80900192260742,
      "learning_rate": 1e-05,
      "loss": 5.9182,
      "step": 13496
    },
    {
      "epoch": 0.731490514905149,
      "step": 13496,
      "training_loss": 6.00352668762207
    },
    {
      "epoch": 0.7315447154471545,
      "step": 13497,
      "training_loss": 7.365073204040527
    },
    {
      "epoch": 0.7315989159891599,
      "step": 13498,
      "training_loss": 6.4337382316589355
    },
    {
      "epoch": 0.7316531165311653,
      "step": 13499,
      "training_loss": 6.9695658683776855
    },
    {
      "epoch": 0.7317073170731707,
      "grad_norm": 26.82649803161621,
      "learning_rate": 1e-05,
      "loss": 6.693,
      "step": 13500
    },
    {
      "epoch": 0.7317073170731707,
      "step": 13500,
      "training_loss": 7.673190116882324
    },
    {
      "epoch": 0.7317615176151762,
      "step": 13501,
      "training_loss": 5.515758514404297
    },
    {
      "epoch": 0.7318157181571816,
      "step": 13502,
      "training_loss": 6.911350727081299
    },
    {
      "epoch": 0.7318699186991869,
      "step": 13503,
      "training_loss": 6.880151748657227
    },
    {
      "epoch": 0.7319241192411924,
      "grad_norm": 25.991971969604492,
      "learning_rate": 1e-05,
      "loss": 6.7451,
      "step": 13504
    },
    {
      "epoch": 0.7319241192411924,
      "step": 13504,
      "training_loss": 7.4884843826293945
    },
    {
      "epoch": 0.7319783197831978,
      "step": 13505,
      "training_loss": 7.039445400238037
    },
    {
      "epoch": 0.7320325203252033,
      "step": 13506,
      "training_loss": 7.364758491516113
    },
    {
      "epoch": 0.7320867208672087,
      "step": 13507,
      "training_loss": 5.004289150238037
    },
    {
      "epoch": 0.732140921409214,
      "grad_norm": 45.020599365234375,
      "learning_rate": 1e-05,
      "loss": 6.7242,
      "step": 13508
    },
    {
      "epoch": 0.732140921409214,
      "step": 13508,
      "training_loss": 4.659255504608154
    },
    {
      "epoch": 0.7321951219512195,
      "step": 13509,
      "training_loss": 7.382473468780518
    },
    {
      "epoch": 0.7322493224932249,
      "step": 13510,
      "training_loss": 7.0441670417785645
    },
    {
      "epoch": 0.7323035230352304,
      "step": 13511,
      "training_loss": 6.4998779296875
    },
    {
      "epoch": 0.7323577235772357,
      "grad_norm": 35.399166107177734,
      "learning_rate": 1e-05,
      "loss": 6.3964,
      "step": 13512
    },
    {
      "epoch": 0.7323577235772357,
      "step": 13512,
      "training_loss": 7.289936542510986
    },
    {
      "epoch": 0.7324119241192412,
      "step": 13513,
      "training_loss": 5.717230319976807
    },
    {
      "epoch": 0.7324661246612466,
      "step": 13514,
      "training_loss": 6.044439315795898
    },
    {
      "epoch": 0.732520325203252,
      "step": 13515,
      "training_loss": 6.427276611328125
    },
    {
      "epoch": 0.7325745257452575,
      "grad_norm": 21.830562591552734,
      "learning_rate": 1e-05,
      "loss": 6.3697,
      "step": 13516
    },
    {
      "epoch": 0.7325745257452575,
      "step": 13516,
      "training_loss": 6.572383403778076
    },
    {
      "epoch": 0.7326287262872628,
      "step": 13517,
      "training_loss": 6.056115627288818
    },
    {
      "epoch": 0.7326829268292683,
      "step": 13518,
      "training_loss": 6.1296234130859375
    },
    {
      "epoch": 0.7327371273712737,
      "step": 13519,
      "training_loss": 6.821695804595947
    },
    {
      "epoch": 0.7327913279132792,
      "grad_norm": 25.201026916503906,
      "learning_rate": 1e-05,
      "loss": 6.395,
      "step": 13520
    },
    {
      "epoch": 0.7327913279132792,
      "step": 13520,
      "training_loss": 4.421840190887451
    },
    {
      "epoch": 0.7328455284552845,
      "step": 13521,
      "training_loss": 6.955445766448975
    },
    {
      "epoch": 0.73289972899729,
      "step": 13522,
      "training_loss": 6.457800388336182
    },
    {
      "epoch": 0.7329539295392954,
      "step": 13523,
      "training_loss": 6.446506977081299
    },
    {
      "epoch": 0.7330081300813008,
      "grad_norm": 50.529930114746094,
      "learning_rate": 1e-05,
      "loss": 6.0704,
      "step": 13524
    },
    {
      "epoch": 0.7330081300813008,
      "step": 13524,
      "training_loss": 7.290574073791504
    },
    {
      "epoch": 0.7330623306233063,
      "step": 13525,
      "training_loss": 7.874392032623291
    },
    {
      "epoch": 0.7331165311653116,
      "step": 13526,
      "training_loss": 6.217451572418213
    },
    {
      "epoch": 0.7331707317073171,
      "step": 13527,
      "training_loss": 4.6635026931762695
    },
    {
      "epoch": 0.7332249322493225,
      "grad_norm": 33.81072235107422,
      "learning_rate": 1e-05,
      "loss": 6.5115,
      "step": 13528
    },
    {
      "epoch": 0.7332249322493225,
      "step": 13528,
      "training_loss": 7.471724033355713
    },
    {
      "epoch": 0.733279132791328,
      "step": 13529,
      "training_loss": 7.283961296081543
    },
    {
      "epoch": 0.7333333333333333,
      "step": 13530,
      "training_loss": 6.734764575958252
    },
    {
      "epoch": 0.7333875338753387,
      "step": 13531,
      "training_loss": 6.479147911071777
    },
    {
      "epoch": 0.7334417344173442,
      "grad_norm": 16.70832633972168,
      "learning_rate": 1e-05,
      "loss": 6.9924,
      "step": 13532
    },
    {
      "epoch": 0.7334417344173442,
      "step": 13532,
      "training_loss": 6.094205379486084
    },
    {
      "epoch": 0.7334959349593496,
      "step": 13533,
      "training_loss": 6.399421691894531
    },
    {
      "epoch": 0.7335501355013551,
      "step": 13534,
      "training_loss": 7.312978744506836
    },
    {
      "epoch": 0.7336043360433604,
      "step": 13535,
      "training_loss": 6.308662414550781
    },
    {
      "epoch": 0.7336585365853658,
      "grad_norm": 33.82527542114258,
      "learning_rate": 1e-05,
      "loss": 6.5288,
      "step": 13536
    },
    {
      "epoch": 0.7336585365853658,
      "step": 13536,
      "training_loss": 7.048283100128174
    },
    {
      "epoch": 0.7337127371273713,
      "step": 13537,
      "training_loss": 7.863757133483887
    },
    {
      "epoch": 0.7337669376693767,
      "step": 13538,
      "training_loss": 7.0880513191223145
    },
    {
      "epoch": 0.7338211382113821,
      "step": 13539,
      "training_loss": 6.738039016723633
    },
    {
      "epoch": 0.7338753387533875,
      "grad_norm": 23.24596405029297,
      "learning_rate": 1e-05,
      "loss": 7.1845,
      "step": 13540
    },
    {
      "epoch": 0.7338753387533875,
      "step": 13540,
      "training_loss": 6.846658706665039
    },
    {
      "epoch": 0.733929539295393,
      "step": 13541,
      "training_loss": 6.703215599060059
    },
    {
      "epoch": 0.7339837398373984,
      "step": 13542,
      "training_loss": 7.23228645324707
    },
    {
      "epoch": 0.7340379403794038,
      "step": 13543,
      "training_loss": 5.378044128417969
    },
    {
      "epoch": 0.7340921409214092,
      "grad_norm": 52.83269119262695,
      "learning_rate": 1e-05,
      "loss": 6.5401,
      "step": 13544
    },
    {
      "epoch": 0.7340921409214092,
      "step": 13544,
      "training_loss": 6.337935924530029
    },
    {
      "epoch": 0.7341463414634146,
      "step": 13545,
      "training_loss": 6.904352188110352
    },
    {
      "epoch": 0.7342005420054201,
      "step": 13546,
      "training_loss": 6.80911111831665
    },
    {
      "epoch": 0.7342547425474255,
      "step": 13547,
      "training_loss": 6.631934642791748
    },
    {
      "epoch": 0.7343089430894308,
      "grad_norm": 15.621102333068848,
      "learning_rate": 1e-05,
      "loss": 6.6708,
      "step": 13548
    },
    {
      "epoch": 0.7343089430894308,
      "step": 13548,
      "training_loss": 7.341382026672363
    },
    {
      "epoch": 0.7343631436314363,
      "step": 13549,
      "training_loss": 4.694584369659424
    },
    {
      "epoch": 0.7344173441734417,
      "step": 13550,
      "training_loss": 7.880439281463623
    },
    {
      "epoch": 0.7344715447154472,
      "step": 13551,
      "training_loss": 7.030364036560059
    },
    {
      "epoch": 0.7345257452574526,
      "grad_norm": 46.828407287597656,
      "learning_rate": 1e-05,
      "loss": 6.7367,
      "step": 13552
    },
    {
      "epoch": 0.7345257452574526,
      "step": 13552,
      "training_loss": 9.077922821044922
    },
    {
      "epoch": 0.734579945799458,
      "step": 13553,
      "training_loss": 8.407547950744629
    },
    {
      "epoch": 0.7346341463414634,
      "step": 13554,
      "training_loss": 7.101877212524414
    },
    {
      "epoch": 0.7346883468834688,
      "step": 13555,
      "training_loss": 8.197139739990234
    },
    {
      "epoch": 0.7347425474254743,
      "grad_norm": 41.44532012939453,
      "learning_rate": 1e-05,
      "loss": 8.1961,
      "step": 13556
    },
    {
      "epoch": 0.7347425474254743,
      "step": 13556,
      "training_loss": 7.049607276916504
    },
    {
      "epoch": 0.7347967479674796,
      "step": 13557,
      "training_loss": 7.738118648529053
    },
    {
      "epoch": 0.7348509485094851,
      "step": 13558,
      "training_loss": 4.480973720550537
    },
    {
      "epoch": 0.7349051490514905,
      "step": 13559,
      "training_loss": 6.626821041107178
    },
    {
      "epoch": 0.734959349593496,
      "grad_norm": 20.815731048583984,
      "learning_rate": 1e-05,
      "loss": 6.4739,
      "step": 13560
    },
    {
      "epoch": 0.734959349593496,
      "step": 13560,
      "training_loss": 5.199594974517822
    },
    {
      "epoch": 0.7350135501355014,
      "step": 13561,
      "training_loss": 7.816770553588867
    },
    {
      "epoch": 0.7350677506775067,
      "step": 13562,
      "training_loss": 6.246901035308838
    },
    {
      "epoch": 0.7351219512195122,
      "step": 13563,
      "training_loss": 7.213093280792236
    },
    {
      "epoch": 0.7351761517615176,
      "grad_norm": 41.70491027832031,
      "learning_rate": 1e-05,
      "loss": 6.6191,
      "step": 13564
    },
    {
      "epoch": 0.7351761517615176,
      "step": 13564,
      "training_loss": 7.212426662445068
    },
    {
      "epoch": 0.7352303523035231,
      "step": 13565,
      "training_loss": 5.215544700622559
    },
    {
      "epoch": 0.7352845528455284,
      "step": 13566,
      "training_loss": 7.390787601470947
    },
    {
      "epoch": 0.7353387533875338,
      "step": 13567,
      "training_loss": 6.283109188079834
    },
    {
      "epoch": 0.7353929539295393,
      "grad_norm": 27.34943389892578,
      "learning_rate": 1e-05,
      "loss": 6.5255,
      "step": 13568
    },
    {
      "epoch": 0.7353929539295393,
      "step": 13568,
      "training_loss": 6.472076892852783
    },
    {
      "epoch": 0.7354471544715447,
      "step": 13569,
      "training_loss": 6.897776126861572
    },
    {
      "epoch": 0.7355013550135502,
      "step": 13570,
      "training_loss": 8.496024131774902
    },
    {
      "epoch": 0.7355555555555555,
      "step": 13571,
      "training_loss": 6.669282913208008
    },
    {
      "epoch": 0.735609756097561,
      "grad_norm": 25.132720947265625,
      "learning_rate": 1e-05,
      "loss": 7.1338,
      "step": 13572
    },
    {
      "epoch": 0.735609756097561,
      "step": 13572,
      "training_loss": 4.040434837341309
    },
    {
      "epoch": 0.7356639566395664,
      "step": 13573,
      "training_loss": 7.079654216766357
    },
    {
      "epoch": 0.7357181571815719,
      "step": 13574,
      "training_loss": 4.46312141418457
    },
    {
      "epoch": 0.7357723577235772,
      "step": 13575,
      "training_loss": 6.633159637451172
    },
    {
      "epoch": 0.7358265582655826,
      "grad_norm": 24.594493865966797,
      "learning_rate": 1e-05,
      "loss": 5.5541,
      "step": 13576
    },
    {
      "epoch": 0.7358265582655826,
      "step": 13576,
      "training_loss": 7.857854843139648
    },
    {
      "epoch": 0.7358807588075881,
      "step": 13577,
      "training_loss": 6.607881546020508
    },
    {
      "epoch": 0.7359349593495935,
      "step": 13578,
      "training_loss": 6.7005295753479
    },
    {
      "epoch": 0.735989159891599,
      "step": 13579,
      "training_loss": 5.871949672698975
    },
    {
      "epoch": 0.7360433604336043,
      "grad_norm": 23.739046096801758,
      "learning_rate": 1e-05,
      "loss": 6.7596,
      "step": 13580
    },
    {
      "epoch": 0.7360433604336043,
      "step": 13580,
      "training_loss": 8.204289436340332
    },
    {
      "epoch": 0.7360975609756097,
      "step": 13581,
      "training_loss": 8.242982864379883
    },
    {
      "epoch": 0.7361517615176152,
      "step": 13582,
      "training_loss": 7.7290239334106445
    },
    {
      "epoch": 0.7362059620596206,
      "step": 13583,
      "training_loss": 7.38016414642334
    },
    {
      "epoch": 0.736260162601626,
      "grad_norm": 30.24229621887207,
      "learning_rate": 1e-05,
      "loss": 7.8891,
      "step": 13584
    },
    {
      "epoch": 0.736260162601626,
      "step": 13584,
      "training_loss": 3.766767978668213
    },
    {
      "epoch": 0.7363143631436314,
      "step": 13585,
      "training_loss": 6.8592529296875
    },
    {
      "epoch": 0.7363685636856369,
      "step": 13586,
      "training_loss": 7.625763893127441
    },
    {
      "epoch": 0.7364227642276423,
      "step": 13587,
      "training_loss": 7.482059001922607
    },
    {
      "epoch": 0.7364769647696477,
      "grad_norm": 28.884445190429688,
      "learning_rate": 1e-05,
      "loss": 6.4335,
      "step": 13588
    },
    {
      "epoch": 0.7364769647696477,
      "step": 13588,
      "training_loss": 6.741839408874512
    },
    {
      "epoch": 0.7365311653116531,
      "step": 13589,
      "training_loss": 7.364348888397217
    },
    {
      "epoch": 0.7365853658536585,
      "step": 13590,
      "training_loss": 7.146645545959473
    },
    {
      "epoch": 0.736639566395664,
      "step": 13591,
      "training_loss": 5.792909145355225
    },
    {
      "epoch": 0.7366937669376694,
      "grad_norm": 30.850234985351562,
      "learning_rate": 1e-05,
      "loss": 6.7614,
      "step": 13592
    },
    {
      "epoch": 0.7366937669376694,
      "step": 13592,
      "training_loss": 7.522247791290283
    },
    {
      "epoch": 0.7367479674796747,
      "step": 13593,
      "training_loss": 7.168369293212891
    },
    {
      "epoch": 0.7368021680216802,
      "step": 13594,
      "training_loss": 7.283690929412842
    },
    {
      "epoch": 0.7368563685636856,
      "step": 13595,
      "training_loss": 6.373589992523193
    },
    {
      "epoch": 0.7369105691056911,
      "grad_norm": 23.898052215576172,
      "learning_rate": 1e-05,
      "loss": 7.087,
      "step": 13596
    },
    {
      "epoch": 0.7369105691056911,
      "step": 13596,
      "training_loss": 7.412954807281494
    },
    {
      "epoch": 0.7369647696476965,
      "step": 13597,
      "training_loss": 6.187946319580078
    },
    {
      "epoch": 0.7370189701897019,
      "step": 13598,
      "training_loss": 6.55739164352417
    },
    {
      "epoch": 0.7370731707317073,
      "step": 13599,
      "training_loss": 4.832186698913574
    },
    {
      "epoch": 0.7371273712737128,
      "grad_norm": 25.65574073791504,
      "learning_rate": 1e-05,
      "loss": 6.2476,
      "step": 13600
    },
    {
      "epoch": 0.7371273712737128,
      "step": 13600,
      "training_loss": 6.8150739669799805
    },
    {
      "epoch": 0.7371815718157182,
      "step": 13601,
      "training_loss": 6.368252277374268
    },
    {
      "epoch": 0.7372357723577235,
      "step": 13602,
      "training_loss": 6.872357368469238
    },
    {
      "epoch": 0.737289972899729,
      "step": 13603,
      "training_loss": 6.01154899597168
    },
    {
      "epoch": 0.7373441734417344,
      "grad_norm": 21.996915817260742,
      "learning_rate": 1e-05,
      "loss": 6.5168,
      "step": 13604
    },
    {
      "epoch": 0.7373441734417344,
      "step": 13604,
      "training_loss": 6.035085678100586
    },
    {
      "epoch": 0.7373983739837399,
      "step": 13605,
      "training_loss": 7.7000226974487305
    },
    {
      "epoch": 0.7374525745257453,
      "step": 13606,
      "training_loss": 4.578517436981201
    },
    {
      "epoch": 0.7375067750677506,
      "step": 13607,
      "training_loss": 8.28150463104248
    },
    {
      "epoch": 0.7375609756097561,
      "grad_norm": 56.12956237792969,
      "learning_rate": 1e-05,
      "loss": 6.6488,
      "step": 13608
    },
    {
      "epoch": 0.7375609756097561,
      "step": 13608,
      "training_loss": 7.289565563201904
    },
    {
      "epoch": 0.7376151761517615,
      "step": 13609,
      "training_loss": 7.948473930358887
    },
    {
      "epoch": 0.737669376693767,
      "step": 13610,
      "training_loss": 7.003947734832764
    },
    {
      "epoch": 0.7377235772357723,
      "step": 13611,
      "training_loss": 6.340363502502441
    },
    {
      "epoch": 0.7377777777777778,
      "grad_norm": 29.132034301757812,
      "learning_rate": 1e-05,
      "loss": 7.1456,
      "step": 13612
    },
    {
      "epoch": 0.7377777777777778,
      "step": 13612,
      "training_loss": 6.0081400871276855
    },
    {
      "epoch": 0.7378319783197832,
      "step": 13613,
      "training_loss": 7.041943550109863
    },
    {
      "epoch": 0.7378861788617886,
      "step": 13614,
      "training_loss": 6.385501384735107
    },
    {
      "epoch": 0.7379403794037941,
      "step": 13615,
      "training_loss": 7.1570725440979
    },
    {
      "epoch": 0.7379945799457994,
      "grad_norm": 21.784080505371094,
      "learning_rate": 1e-05,
      "loss": 6.6482,
      "step": 13616
    },
    {
      "epoch": 0.7379945799457994,
      "step": 13616,
      "training_loss": 6.611385822296143
    },
    {
      "epoch": 0.7380487804878049,
      "step": 13617,
      "training_loss": 8.805485725402832
    },
    {
      "epoch": 0.7381029810298103,
      "step": 13618,
      "training_loss": 6.57595682144165
    },
    {
      "epoch": 0.7381571815718158,
      "step": 13619,
      "training_loss": 6.219465732574463
    },
    {
      "epoch": 0.7382113821138211,
      "grad_norm": 73.17607116699219,
      "learning_rate": 1e-05,
      "loss": 7.0531,
      "step": 13620
    },
    {
      "epoch": 0.7382113821138211,
      "step": 13620,
      "training_loss": 8.464357376098633
    },
    {
      "epoch": 0.7382655826558265,
      "step": 13621,
      "training_loss": 6.333310127258301
    },
    {
      "epoch": 0.738319783197832,
      "step": 13622,
      "training_loss": 3.757598400115967
    },
    {
      "epoch": 0.7383739837398374,
      "step": 13623,
      "training_loss": 4.78687858581543
    },
    {
      "epoch": 0.7384281842818429,
      "grad_norm": 21.97244644165039,
      "learning_rate": 1e-05,
      "loss": 5.8355,
      "step": 13624
    },
    {
      "epoch": 0.7384281842818429,
      "step": 13624,
      "training_loss": 7.019418716430664
    },
    {
      "epoch": 0.7384823848238482,
      "step": 13625,
      "training_loss": 6.367100715637207
    },
    {
      "epoch": 0.7385365853658536,
      "step": 13626,
      "training_loss": 6.848544597625732
    },
    {
      "epoch": 0.7385907859078591,
      "step": 13627,
      "training_loss": 5.695326328277588
    },
    {
      "epoch": 0.7386449864498645,
      "grad_norm": 24.188322067260742,
      "learning_rate": 1e-05,
      "loss": 6.4826,
      "step": 13628
    },
    {
      "epoch": 0.7386449864498645,
      "step": 13628,
      "training_loss": 7.160830020904541
    },
    {
      "epoch": 0.7386991869918699,
      "step": 13629,
      "training_loss": 6.906386375427246
    },
    {
      "epoch": 0.7387533875338753,
      "step": 13630,
      "training_loss": 6.039449214935303
    },
    {
      "epoch": 0.7388075880758808,
      "step": 13631,
      "training_loss": 5.738108158111572
    },
    {
      "epoch": 0.7388617886178862,
      "grad_norm": 27.495582580566406,
      "learning_rate": 1e-05,
      "loss": 6.4612,
      "step": 13632
    },
    {
      "epoch": 0.7388617886178862,
      "step": 13632,
      "training_loss": 6.062409400939941
    },
    {
      "epoch": 0.7389159891598917,
      "step": 13633,
      "training_loss": 6.692380428314209
    },
    {
      "epoch": 0.738970189701897,
      "step": 13634,
      "training_loss": 7.2774786949157715
    },
    {
      "epoch": 0.7390243902439024,
      "step": 13635,
      "training_loss": 6.275054931640625
    },
    {
      "epoch": 0.7390785907859079,
      "grad_norm": 26.98786735534668,
      "learning_rate": 1e-05,
      "loss": 6.5768,
      "step": 13636
    },
    {
      "epoch": 0.7390785907859079,
      "step": 13636,
      "training_loss": 6.3348517417907715
    },
    {
      "epoch": 0.7391327913279133,
      "step": 13637,
      "training_loss": 5.8186845779418945
    },
    {
      "epoch": 0.7391869918699187,
      "step": 13638,
      "training_loss": 6.554673194885254
    },
    {
      "epoch": 0.7392411924119241,
      "step": 13639,
      "training_loss": 7.620046615600586
    },
    {
      "epoch": 0.7392953929539295,
      "grad_norm": 20.20295524597168,
      "learning_rate": 1e-05,
      "loss": 6.5821,
      "step": 13640
    },
    {
      "epoch": 0.7392953929539295,
      "step": 13640,
      "training_loss": 7.276154041290283
    },
    {
      "epoch": 0.739349593495935,
      "step": 13641,
      "training_loss": 7.265933990478516
    },
    {
      "epoch": 0.7394037940379404,
      "step": 13642,
      "training_loss": 7.34829044342041
    },
    {
      "epoch": 0.7394579945799458,
      "step": 13643,
      "training_loss": 6.0514421463012695
    },
    {
      "epoch": 0.7395121951219512,
      "grad_norm": 16.474674224853516,
      "learning_rate": 1e-05,
      "loss": 6.9855,
      "step": 13644
    },
    {
      "epoch": 0.7395121951219512,
      "step": 13644,
      "training_loss": 8.239389419555664
    },
    {
      "epoch": 0.7395663956639567,
      "step": 13645,
      "training_loss": 7.011364459991455
    },
    {
      "epoch": 0.7396205962059621,
      "step": 13646,
      "training_loss": 5.968033790588379
    },
    {
      "epoch": 0.7396747967479674,
      "step": 13647,
      "training_loss": 6.012930393218994
    },
    {
      "epoch": 0.7397289972899729,
      "grad_norm": 18.861408233642578,
      "learning_rate": 1e-05,
      "loss": 6.8079,
      "step": 13648
    },
    {
      "epoch": 0.7397289972899729,
      "step": 13648,
      "training_loss": 5.586796760559082
    },
    {
      "epoch": 0.7397831978319783,
      "step": 13649,
      "training_loss": 7.623208999633789
    },
    {
      "epoch": 0.7398373983739838,
      "step": 13650,
      "training_loss": 7.215850353240967
    },
    {
      "epoch": 0.7398915989159892,
      "step": 13651,
      "training_loss": 5.329446315765381
    },
    {
      "epoch": 0.7399457994579945,
      "grad_norm": 45.28082275390625,
      "learning_rate": 1e-05,
      "loss": 6.4388,
      "step": 13652
    },
    {
      "epoch": 0.7399457994579945,
      "step": 13652,
      "training_loss": 5.494047164916992
    },
    {
      "epoch": 0.74,
      "step": 13653,
      "training_loss": 6.490927696228027
    },
    {
      "epoch": 0.7400542005420054,
      "step": 13654,
      "training_loss": 6.774528980255127
    },
    {
      "epoch": 0.7401084010840109,
      "step": 13655,
      "training_loss": 6.687478542327881
    },
    {
      "epoch": 0.7401626016260162,
      "grad_norm": 19.677518844604492,
      "learning_rate": 1e-05,
      "loss": 6.3617,
      "step": 13656
    },
    {
      "epoch": 0.7401626016260162,
      "step": 13656,
      "training_loss": 6.0389485359191895
    },
    {
      "epoch": 0.7402168021680217,
      "step": 13657,
      "training_loss": 4.98792028427124
    },
    {
      "epoch": 0.7402710027100271,
      "step": 13658,
      "training_loss": 6.538956642150879
    },
    {
      "epoch": 0.7403252032520325,
      "step": 13659,
      "training_loss": 6.762579917907715
    },
    {
      "epoch": 0.740379403794038,
      "grad_norm": 24.77914810180664,
      "learning_rate": 1e-05,
      "loss": 6.0821,
      "step": 13660
    },
    {
      "epoch": 0.740379403794038,
      "step": 13660,
      "training_loss": 7.256861209869385
    },
    {
      "epoch": 0.7404336043360433,
      "step": 13661,
      "training_loss": 6.171485900878906
    },
    {
      "epoch": 0.7404878048780488,
      "step": 13662,
      "training_loss": 7.98990535736084
    },
    {
      "epoch": 0.7405420054200542,
      "step": 13663,
      "training_loss": 5.290150165557861
    },
    {
      "epoch": 0.7405962059620597,
      "grad_norm": 41.222999572753906,
      "learning_rate": 1e-05,
      "loss": 6.6771,
      "step": 13664
    },
    {
      "epoch": 0.7405962059620597,
      "step": 13664,
      "training_loss": 7.923733711242676
    },
    {
      "epoch": 0.740650406504065,
      "step": 13665,
      "training_loss": 5.534920692443848
    },
    {
      "epoch": 0.7407046070460704,
      "step": 13666,
      "training_loss": 7.253479480743408
    },
    {
      "epoch": 0.7407588075880759,
      "step": 13667,
      "training_loss": 5.191288471221924
    },
    {
      "epoch": 0.7408130081300813,
      "grad_norm": 32.35132598876953,
      "learning_rate": 1e-05,
      "loss": 6.4759,
      "step": 13668
    },
    {
      "epoch": 0.7408130081300813,
      "step": 13668,
      "training_loss": 5.821228981018066
    },
    {
      "epoch": 0.7408672086720868,
      "step": 13669,
      "training_loss": 7.429830551147461
    },
    {
      "epoch": 0.7409214092140921,
      "step": 13670,
      "training_loss": 6.5975189208984375
    },
    {
      "epoch": 0.7409756097560976,
      "step": 13671,
      "training_loss": 6.68432092666626
    },
    {
      "epoch": 0.741029810298103,
      "grad_norm": 16.618663787841797,
      "learning_rate": 1e-05,
      "loss": 6.6332,
      "step": 13672
    },
    {
      "epoch": 0.741029810298103,
      "step": 13672,
      "training_loss": 4.315766334533691
    },
    {
      "epoch": 0.7410840108401084,
      "step": 13673,
      "training_loss": 6.107759952545166
    },
    {
      "epoch": 0.7411382113821138,
      "step": 13674,
      "training_loss": 7.185537815093994
    },
    {
      "epoch": 0.7411924119241192,
      "step": 13675,
      "training_loss": 6.9085493087768555
    },
    {
      "epoch": 0.7412466124661247,
      "grad_norm": 23.792768478393555,
      "learning_rate": 1e-05,
      "loss": 6.1294,
      "step": 13676
    },
    {
      "epoch": 0.7412466124661247,
      "step": 13676,
      "training_loss": 9.518789291381836
    },
    {
      "epoch": 0.7413008130081301,
      "step": 13677,
      "training_loss": 7.417720794677734
    },
    {
      "epoch": 0.7413550135501356,
      "step": 13678,
      "training_loss": 6.010306358337402
    },
    {
      "epoch": 0.7414092140921409,
      "step": 13679,
      "training_loss": 3.5311543941497803
    },
    {
      "epoch": 0.7414634146341463,
      "grad_norm": 39.16510772705078,
      "learning_rate": 1e-05,
      "loss": 6.6195,
      "step": 13680
    },
    {
      "epoch": 0.7414634146341463,
      "step": 13680,
      "training_loss": 9.429553031921387
    },
    {
      "epoch": 0.7415176151761518,
      "step": 13681,
      "training_loss": 8.757111549377441
    },
    {
      "epoch": 0.7415718157181572,
      "step": 13682,
      "training_loss": 7.195784091949463
    },
    {
      "epoch": 0.7416260162601626,
      "step": 13683,
      "training_loss": 5.943607330322266
    },
    {
      "epoch": 0.741680216802168,
      "grad_norm": 40.2976188659668,
      "learning_rate": 1e-05,
      "loss": 7.8315,
      "step": 13684
    },
    {
      "epoch": 0.741680216802168,
      "step": 13684,
      "training_loss": 6.0165228843688965
    },
    {
      "epoch": 0.7417344173441734,
      "step": 13685,
      "training_loss": 6.1914825439453125
    },
    {
      "epoch": 0.7417886178861789,
      "step": 13686,
      "training_loss": 5.016848087310791
    },
    {
      "epoch": 0.7418428184281843,
      "step": 13687,
      "training_loss": 3.408565044403076
    },
    {
      "epoch": 0.7418970189701897,
      "grad_norm": 24.030324935913086,
      "learning_rate": 1e-05,
      "loss": 5.1584,
      "step": 13688
    },
    {
      "epoch": 0.7418970189701897,
      "step": 13688,
      "training_loss": 7.005225658416748
    },
    {
      "epoch": 0.7419512195121951,
      "step": 13689,
      "training_loss": 6.736125946044922
    },
    {
      "epoch": 0.7420054200542006,
      "step": 13690,
      "training_loss": 6.883571147918701
    },
    {
      "epoch": 0.742059620596206,
      "step": 13691,
      "training_loss": 6.93651819229126
    },
    {
      "epoch": 0.7421138211382113,
      "grad_norm": 26.133970260620117,
      "learning_rate": 1e-05,
      "loss": 6.8904,
      "step": 13692
    },
    {
      "epoch": 0.7421138211382113,
      "step": 13692,
      "training_loss": 7.422123908996582
    },
    {
      "epoch": 0.7421680216802168,
      "step": 13693,
      "training_loss": 7.480413913726807
    },
    {
      "epoch": 0.7422222222222222,
      "step": 13694,
      "training_loss": 8.25173282623291
    },
    {
      "epoch": 0.7422764227642277,
      "step": 13695,
      "training_loss": 6.8235578536987305
    },
    {
      "epoch": 0.7423306233062331,
      "grad_norm": 16.70649528503418,
      "learning_rate": 1e-05,
      "loss": 7.4945,
      "step": 13696
    },
    {
      "epoch": 0.7423306233062331,
      "step": 13696,
      "training_loss": 6.674561977386475
    },
    {
      "epoch": 0.7423848238482385,
      "step": 13697,
      "training_loss": 7.9453511238098145
    },
    {
      "epoch": 0.7424390243902439,
      "step": 13698,
      "training_loss": 5.25852108001709
    },
    {
      "epoch": 0.7424932249322493,
      "step": 13699,
      "training_loss": 4.736788272857666
    },
    {
      "epoch": 0.7425474254742548,
      "grad_norm": 29.97390365600586,
      "learning_rate": 1e-05,
      "loss": 6.1538,
      "step": 13700
    },
    {
      "epoch": 0.7425474254742548,
      "step": 13700,
      "training_loss": 7.413890838623047
    },
    {
      "epoch": 0.7426016260162601,
      "step": 13701,
      "training_loss": 7.274087905883789
    },
    {
      "epoch": 0.7426558265582656,
      "step": 13702,
      "training_loss": 6.2288126945495605
    },
    {
      "epoch": 0.742710027100271,
      "step": 13703,
      "training_loss": 4.868020057678223
    },
    {
      "epoch": 0.7427642276422765,
      "grad_norm": 28.737234115600586,
      "learning_rate": 1e-05,
      "loss": 6.4462,
      "step": 13704
    },
    {
      "epoch": 0.7427642276422765,
      "step": 13704,
      "training_loss": 4.555366516113281
    },
    {
      "epoch": 0.7428184281842819,
      "step": 13705,
      "training_loss": 7.520101070404053
    },
    {
      "epoch": 0.7428726287262872,
      "step": 13706,
      "training_loss": 5.9624433517456055
    },
    {
      "epoch": 0.7429268292682927,
      "step": 13707,
      "training_loss": 6.839066982269287
    },
    {
      "epoch": 0.7429810298102981,
      "grad_norm": 25.356033325195312,
      "learning_rate": 1e-05,
      "loss": 6.2192,
      "step": 13708
    },
    {
      "epoch": 0.7429810298102981,
      "step": 13708,
      "training_loss": 6.615044116973877
    },
    {
      "epoch": 0.7430352303523036,
      "step": 13709,
      "training_loss": 7.269059181213379
    },
    {
      "epoch": 0.7430894308943089,
      "step": 13710,
      "training_loss": 8.092944145202637
    },
    {
      "epoch": 0.7431436314363143,
      "step": 13711,
      "training_loss": 6.706965923309326
    },
    {
      "epoch": 0.7431978319783198,
      "grad_norm": 22.417268753051758,
      "learning_rate": 1e-05,
      "loss": 7.171,
      "step": 13712
    },
    {
      "epoch": 0.7431978319783198,
      "step": 13712,
      "training_loss": 6.984487533569336
    },
    {
      "epoch": 0.7432520325203252,
      "step": 13713,
      "training_loss": 6.641529560089111
    },
    {
      "epoch": 0.7433062330623306,
      "step": 13714,
      "training_loss": 6.04461669921875
    },
    {
      "epoch": 0.743360433604336,
      "step": 13715,
      "training_loss": 5.291585445404053
    },
    {
      "epoch": 0.7434146341463415,
      "grad_norm": 46.31157302856445,
      "learning_rate": 1e-05,
      "loss": 6.2406,
      "step": 13716
    },
    {
      "epoch": 0.7434146341463415,
      "step": 13716,
      "training_loss": 7.409788131713867
    },
    {
      "epoch": 0.7434688346883469,
      "step": 13717,
      "training_loss": 5.560514450073242
    },
    {
      "epoch": 0.7435230352303523,
      "step": 13718,
      "training_loss": 3.988773822784424
    },
    {
      "epoch": 0.7435772357723577,
      "step": 13719,
      "training_loss": 7.652867794036865
    },
    {
      "epoch": 0.7436314363143631,
      "grad_norm": 26.79758644104004,
      "learning_rate": 1e-05,
      "loss": 6.153,
      "step": 13720
    },
    {
      "epoch": 0.7436314363143631,
      "step": 13720,
      "training_loss": 6.656843185424805
    },
    {
      "epoch": 0.7436856368563686,
      "step": 13721,
      "training_loss": 7.171507358551025
    },
    {
      "epoch": 0.743739837398374,
      "step": 13722,
      "training_loss": 7.620159149169922
    },
    {
      "epoch": 0.7437940379403793,
      "step": 13723,
      "training_loss": 6.615199089050293
    },
    {
      "epoch": 0.7438482384823848,
      "grad_norm": 37.24421310424805,
      "learning_rate": 1e-05,
      "loss": 7.0159,
      "step": 13724
    },
    {
      "epoch": 0.7438482384823848,
      "step": 13724,
      "training_loss": 3.398531913757324
    },
    {
      "epoch": 0.7439024390243902,
      "step": 13725,
      "training_loss": 5.526121616363525
    },
    {
      "epoch": 0.7439566395663957,
      "step": 13726,
      "training_loss": 7.8126325607299805
    },
    {
      "epoch": 0.7440108401084011,
      "step": 13727,
      "training_loss": 7.224792003631592
    },
    {
      "epoch": 0.7440650406504065,
      "grad_norm": 41.54314422607422,
      "learning_rate": 1e-05,
      "loss": 5.9905,
      "step": 13728
    },
    {
      "epoch": 0.7440650406504065,
      "step": 13728,
      "training_loss": 6.509204387664795
    },
    {
      "epoch": 0.7441192411924119,
      "step": 13729,
      "training_loss": 7.7087202072143555
    },
    {
      "epoch": 0.7441734417344174,
      "step": 13730,
      "training_loss": 4.915914535522461
    },
    {
      "epoch": 0.7442276422764228,
      "step": 13731,
      "training_loss": 8.016566276550293
    },
    {
      "epoch": 0.7442818428184281,
      "grad_norm": 27.8045654296875,
      "learning_rate": 1e-05,
      "loss": 6.7876,
      "step": 13732
    },
    {
      "epoch": 0.7442818428184281,
      "step": 13732,
      "training_loss": 5.882357120513916
    },
    {
      "epoch": 0.7443360433604336,
      "step": 13733,
      "training_loss": 6.336704730987549
    },
    {
      "epoch": 0.744390243902439,
      "step": 13734,
      "training_loss": 6.66601037979126
    },
    {
      "epoch": 0.7444444444444445,
      "step": 13735,
      "training_loss": 6.709481716156006
    },
    {
      "epoch": 0.7444986449864499,
      "grad_norm": 20.847606658935547,
      "learning_rate": 1e-05,
      "loss": 6.3986,
      "step": 13736
    },
    {
      "epoch": 0.7444986449864499,
      "step": 13736,
      "training_loss": 7.045767307281494
    },
    {
      "epoch": 0.7445528455284552,
      "step": 13737,
      "training_loss": 7.073735237121582
    },
    {
      "epoch": 0.7446070460704607,
      "step": 13738,
      "training_loss": 7.251561641693115
    },
    {
      "epoch": 0.7446612466124661,
      "step": 13739,
      "training_loss": 6.880259037017822
    },
    {
      "epoch": 0.7447154471544716,
      "grad_norm": 32.46747589111328,
      "learning_rate": 1e-05,
      "loss": 7.0628,
      "step": 13740
    },
    {
      "epoch": 0.7447154471544716,
      "step": 13740,
      "training_loss": 6.429050445556641
    },
    {
      "epoch": 0.7447696476964769,
      "step": 13741,
      "training_loss": 7.118768215179443
    },
    {
      "epoch": 0.7448238482384824,
      "step": 13742,
      "training_loss": 5.569270133972168
    },
    {
      "epoch": 0.7448780487804878,
      "step": 13743,
      "training_loss": 6.628505706787109
    },
    {
      "epoch": 0.7449322493224932,
      "grad_norm": 26.82903480529785,
      "learning_rate": 1e-05,
      "loss": 6.4364,
      "step": 13744
    },
    {
      "epoch": 0.7449322493224932,
      "step": 13744,
      "training_loss": 7.710482597351074
    },
    {
      "epoch": 0.7449864498644987,
      "step": 13745,
      "training_loss": 6.599737167358398
    },
    {
      "epoch": 0.745040650406504,
      "step": 13746,
      "training_loss": 6.404882907867432
    },
    {
      "epoch": 0.7450948509485095,
      "step": 13747,
      "training_loss": 5.985862731933594
    },
    {
      "epoch": 0.7451490514905149,
      "grad_norm": 23.64093780517578,
      "learning_rate": 1e-05,
      "loss": 6.6752,
      "step": 13748
    },
    {
      "epoch": 0.7451490514905149,
      "step": 13748,
      "training_loss": 6.703527450561523
    },
    {
      "epoch": 0.7452032520325204,
      "step": 13749,
      "training_loss": 8.431230545043945
    },
    {
      "epoch": 0.7452574525745257,
      "step": 13750,
      "training_loss": 6.980820655822754
    },
    {
      "epoch": 0.7453116531165311,
      "step": 13751,
      "training_loss": 7.444657802581787
    },
    {
      "epoch": 0.7453658536585366,
      "grad_norm": 28.3916072845459,
      "learning_rate": 1e-05,
      "loss": 7.3901,
      "step": 13752
    },
    {
      "epoch": 0.7453658536585366,
      "step": 13752,
      "training_loss": 6.352730751037598
    },
    {
      "epoch": 0.745420054200542,
      "step": 13753,
      "training_loss": 7.028714656829834
    },
    {
      "epoch": 0.7454742547425475,
      "step": 13754,
      "training_loss": 4.588709354400635
    },
    {
      "epoch": 0.7455284552845528,
      "step": 13755,
      "training_loss": 3.235642910003662
    },
    {
      "epoch": 0.7455826558265582,
      "grad_norm": 22.862112045288086,
      "learning_rate": 1e-05,
      "loss": 5.3014,
      "step": 13756
    },
    {
      "epoch": 0.7455826558265582,
      "step": 13756,
      "training_loss": 3.5777812004089355
    },
    {
      "epoch": 0.7456368563685637,
      "step": 13757,
      "training_loss": 6.511293411254883
    },
    {
      "epoch": 0.7456910569105691,
      "step": 13758,
      "training_loss": 5.307321071624756
    },
    {
      "epoch": 0.7457452574525745,
      "step": 13759,
      "training_loss": 7.4180908203125
    },
    {
      "epoch": 0.7457994579945799,
      "grad_norm": 42.766910552978516,
      "learning_rate": 1e-05,
      "loss": 5.7036,
      "step": 13760
    },
    {
      "epoch": 0.7457994579945799,
      "step": 13760,
      "training_loss": 7.818173408508301
    },
    {
      "epoch": 0.7458536585365854,
      "step": 13761,
      "training_loss": 6.294735908508301
    },
    {
      "epoch": 0.7459078590785908,
      "step": 13762,
      "training_loss": 7.077661991119385
    },
    {
      "epoch": 0.7459620596205963,
      "step": 13763,
      "training_loss": 8.11465072631836
    },
    {
      "epoch": 0.7460162601626016,
      "grad_norm": 21.50737953186035,
      "learning_rate": 1e-05,
      "loss": 7.3263,
      "step": 13764
    },
    {
      "epoch": 0.7460162601626016,
      "step": 13764,
      "training_loss": 6.669593334197998
    },
    {
      "epoch": 0.746070460704607,
      "step": 13765,
      "training_loss": 7.782975673675537
    },
    {
      "epoch": 0.7461246612466125,
      "step": 13766,
      "training_loss": 6.780622482299805
    },
    {
      "epoch": 0.7461788617886179,
      "step": 13767,
      "training_loss": 5.486447811126709
    },
    {
      "epoch": 0.7462330623306233,
      "grad_norm": 29.690820693969727,
      "learning_rate": 1e-05,
      "loss": 6.6799,
      "step": 13768
    },
    {
      "epoch": 0.7462330623306233,
      "step": 13768,
      "training_loss": 5.437593460083008
    },
    {
      "epoch": 0.7462872628726287,
      "step": 13769,
      "training_loss": 7.235321044921875
    },
    {
      "epoch": 0.7463414634146341,
      "step": 13770,
      "training_loss": 8.803006172180176
    },
    {
      "epoch": 0.7463956639566396,
      "step": 13771,
      "training_loss": 7.452149868011475
    },
    {
      "epoch": 0.746449864498645,
      "grad_norm": 25.23426055908203,
      "learning_rate": 1e-05,
      "loss": 7.232,
      "step": 13772
    },
    {
      "epoch": 0.746449864498645,
      "step": 13772,
      "training_loss": 7.265988349914551
    },
    {
      "epoch": 0.7465040650406504,
      "step": 13773,
      "training_loss": 5.691644668579102
    },
    {
      "epoch": 0.7465582655826558,
      "step": 13774,
      "training_loss": 8.163187026977539
    },
    {
      "epoch": 0.7466124661246613,
      "step": 13775,
      "training_loss": 7.494926929473877
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 21.234533309936523,
      "learning_rate": 1e-05,
      "loss": 7.1539,
      "step": 13776
    },
    {
      "epoch": 0.7466666666666667,
      "step": 13776,
      "training_loss": 6.579658508300781
    },
    {
      "epoch": 0.746720867208672,
      "step": 13777,
      "training_loss": 6.412326812744141
    },
    {
      "epoch": 0.7467750677506775,
      "step": 13778,
      "training_loss": 6.2065815925598145
    },
    {
      "epoch": 0.7468292682926829,
      "step": 13779,
      "training_loss": 8.029571533203125
    },
    {
      "epoch": 0.7468834688346884,
      "grad_norm": 23.63409423828125,
      "learning_rate": 1e-05,
      "loss": 6.807,
      "step": 13780
    },
    {
      "epoch": 0.7468834688346884,
      "step": 13780,
      "training_loss": 6.967666149139404
    },
    {
      "epoch": 0.7469376693766938,
      "step": 13781,
      "training_loss": 8.564995765686035
    },
    {
      "epoch": 0.7469918699186991,
      "step": 13782,
      "training_loss": 6.948886394500732
    },
    {
      "epoch": 0.7470460704607046,
      "step": 13783,
      "training_loss": 5.407748222351074
    },
    {
      "epoch": 0.74710027100271,
      "grad_norm": 24.105396270751953,
      "learning_rate": 1e-05,
      "loss": 6.9723,
      "step": 13784
    },
    {
      "epoch": 0.74710027100271,
      "step": 13784,
      "training_loss": 4.816728115081787
    },
    {
      "epoch": 0.7471544715447155,
      "step": 13785,
      "training_loss": 7.769477367401123
    },
    {
      "epoch": 0.7472086720867208,
      "step": 13786,
      "training_loss": 5.889986991882324
    },
    {
      "epoch": 0.7472628726287263,
      "step": 13787,
      "training_loss": 6.610674858093262
    },
    {
      "epoch": 0.7473170731707317,
      "grad_norm": 23.503807067871094,
      "learning_rate": 1e-05,
      "loss": 6.2717,
      "step": 13788
    },
    {
      "epoch": 0.7473170731707317,
      "step": 13788,
      "training_loss": 5.374161720275879
    },
    {
      "epoch": 0.7473712737127371,
      "step": 13789,
      "training_loss": 7.351326942443848
    },
    {
      "epoch": 0.7474254742547426,
      "step": 13790,
      "training_loss": 5.77266788482666
    },
    {
      "epoch": 0.7474796747967479,
      "step": 13791,
      "training_loss": 6.696076393127441
    },
    {
      "epoch": 0.7475338753387534,
      "grad_norm": 25.931556701660156,
      "learning_rate": 1e-05,
      "loss": 6.2986,
      "step": 13792
    },
    {
      "epoch": 0.7475338753387534,
      "step": 13792,
      "training_loss": 5.539678573608398
    },
    {
      "epoch": 0.7475880758807588,
      "step": 13793,
      "training_loss": 7.00808572769165
    },
    {
      "epoch": 0.7476422764227643,
      "step": 13794,
      "training_loss": 3.7961251735687256
    },
    {
      "epoch": 0.7476964769647696,
      "step": 13795,
      "training_loss": 6.95560359954834
    },
    {
      "epoch": 0.747750677506775,
      "grad_norm": 50.8254508972168,
      "learning_rate": 1e-05,
      "loss": 5.8249,
      "step": 13796
    },
    {
      "epoch": 0.747750677506775,
      "step": 13796,
      "training_loss": 7.396573066711426
    },
    {
      "epoch": 0.7478048780487805,
      "step": 13797,
      "training_loss": 7.236538410186768
    },
    {
      "epoch": 0.7478590785907859,
      "step": 13798,
      "training_loss": 5.703887462615967
    },
    {
      "epoch": 0.7479132791327914,
      "step": 13799,
      "training_loss": 7.09751033782959
    },
    {
      "epoch": 0.7479674796747967,
      "grad_norm": 34.90242385864258,
      "learning_rate": 1e-05,
      "loss": 6.8586,
      "step": 13800
    },
    {
      "epoch": 0.7479674796747967,
      "step": 13800,
      "training_loss": 5.886356353759766
    },
    {
      "epoch": 0.7480216802168022,
      "step": 13801,
      "training_loss": 7.0376715660095215
    },
    {
      "epoch": 0.7480758807588076,
      "step": 13802,
      "training_loss": 6.980281352996826
    },
    {
      "epoch": 0.748130081300813,
      "step": 13803,
      "training_loss": 7.411733627319336
    },
    {
      "epoch": 0.7481842818428184,
      "grad_norm": 30.53052520751953,
      "learning_rate": 1e-05,
      "loss": 6.829,
      "step": 13804
    },
    {
      "epoch": 0.7481842818428184,
      "step": 13804,
      "training_loss": 7.110684871673584
    },
    {
      "epoch": 0.7482384823848238,
      "step": 13805,
      "training_loss": 6.80885124206543
    },
    {
      "epoch": 0.7482926829268293,
      "step": 13806,
      "training_loss": 6.359182834625244
    },
    {
      "epoch": 0.7483468834688347,
      "step": 13807,
      "training_loss": 7.097990989685059
    },
    {
      "epoch": 0.7484010840108402,
      "grad_norm": 19.153995513916016,
      "learning_rate": 1e-05,
      "loss": 6.8442,
      "step": 13808
    },
    {
      "epoch": 0.7484010840108402,
      "step": 13808,
      "training_loss": 6.018373012542725
    },
    {
      "epoch": 0.7484552845528455,
      "step": 13809,
      "training_loss": 6.509079933166504
    },
    {
      "epoch": 0.7485094850948509,
      "step": 13810,
      "training_loss": 7.934745788574219
    },
    {
      "epoch": 0.7485636856368564,
      "step": 13811,
      "training_loss": 3.658266305923462
    },
    {
      "epoch": 0.7486178861788618,
      "grad_norm": 28.52021026611328,
      "learning_rate": 1e-05,
      "loss": 6.0301,
      "step": 13812
    },
    {
      "epoch": 0.7486178861788618,
      "step": 13812,
      "training_loss": 7.718282699584961
    },
    {
      "epoch": 0.7486720867208672,
      "step": 13813,
      "training_loss": 6.713961601257324
    },
    {
      "epoch": 0.7487262872628726,
      "step": 13814,
      "training_loss": 6.166252136230469
    },
    {
      "epoch": 0.748780487804878,
      "step": 13815,
      "training_loss": 6.087656021118164
    },
    {
      "epoch": 0.7488346883468835,
      "grad_norm": 30.05861473083496,
      "learning_rate": 1e-05,
      "loss": 6.6715,
      "step": 13816
    },
    {
      "epoch": 0.7488346883468835,
      "step": 13816,
      "training_loss": 3.6517271995544434
    },
    {
      "epoch": 0.7488888888888889,
      "step": 13817,
      "training_loss": 3.469534397125244
    },
    {
      "epoch": 0.7489430894308943,
      "step": 13818,
      "training_loss": 7.41249942779541
    },
    {
      "epoch": 0.7489972899728997,
      "step": 13819,
      "training_loss": 3.8493669033050537
    },
    {
      "epoch": 0.7490514905149052,
      "grad_norm": 24.169857025146484,
      "learning_rate": 1e-05,
      "loss": 4.5958,
      "step": 13820
    },
    {
      "epoch": 0.7490514905149052,
      "step": 13820,
      "training_loss": 3.2393271923065186
    },
    {
      "epoch": 0.7491056910569106,
      "step": 13821,
      "training_loss": 6.669750213623047
    },
    {
      "epoch": 0.7491598915989159,
      "step": 13822,
      "training_loss": 3.2436039447784424
    },
    {
      "epoch": 0.7492140921409214,
      "step": 13823,
      "training_loss": 4.7863545417785645
    },
    {
      "epoch": 0.7492682926829268,
      "grad_norm": 20.521326065063477,
      "learning_rate": 1e-05,
      "loss": 4.4848,
      "step": 13824
    },
    {
      "epoch": 0.7492682926829268,
      "step": 13824,
      "training_loss": 6.168854236602783
    },
    {
      "epoch": 0.7493224932249323,
      "step": 13825,
      "training_loss": 7.643805980682373
    },
    {
      "epoch": 0.7493766937669377,
      "step": 13826,
      "training_loss": 6.492942810058594
    },
    {
      "epoch": 0.749430894308943,
      "step": 13827,
      "training_loss": 6.651761054992676
    },
    {
      "epoch": 0.7494850948509485,
      "grad_norm": 18.6758975982666,
      "learning_rate": 1e-05,
      "loss": 6.7393,
      "step": 13828
    },
    {
      "epoch": 0.7494850948509485,
      "step": 13828,
      "training_loss": 6.540180683135986
    },
    {
      "epoch": 0.7495392953929539,
      "step": 13829,
      "training_loss": 8.61709976196289
    },
    {
      "epoch": 0.7495934959349594,
      "step": 13830,
      "training_loss": 7.136322975158691
    },
    {
      "epoch": 0.7496476964769647,
      "step": 13831,
      "training_loss": 7.045341968536377
    },
    {
      "epoch": 0.7497018970189702,
      "grad_norm": 24.229995727539062,
      "learning_rate": 1e-05,
      "loss": 7.3347,
      "step": 13832
    },
    {
      "epoch": 0.7497018970189702,
      "step": 13832,
      "training_loss": 7.503987789154053
    },
    {
      "epoch": 0.7497560975609756,
      "step": 13833,
      "training_loss": 7.225442886352539
    },
    {
      "epoch": 0.749810298102981,
      "step": 13834,
      "training_loss": 5.861214637756348
    },
    {
      "epoch": 0.7498644986449865,
      "step": 13835,
      "training_loss": 7.635282039642334
    },
    {
      "epoch": 0.7499186991869918,
      "grad_norm": 41.300846099853516,
      "learning_rate": 1e-05,
      "loss": 7.0565,
      "step": 13836
    },
    {
      "epoch": 0.7499186991869918,
      "step": 13836,
      "training_loss": 6.155548572540283
    },
    {
      "epoch": 0.7499728997289973,
      "step": 13837,
      "training_loss": 3.239967107772827
    },
    {
      "epoch": 0.7500271002710027,
      "step": 13838,
      "training_loss": 5.63776159286499
    },
    {
      "epoch": 0.7500813008130082,
      "step": 13839,
      "training_loss": 6.658267974853516
    },
    {
      "epoch": 0.7501355013550135,
      "grad_norm": 15.14762020111084,
      "learning_rate": 1e-05,
      "loss": 5.4229,
      "step": 13840
    },
    {
      "epoch": 0.7501355013550135,
      "step": 13840,
      "training_loss": 6.042187690734863
    },
    {
      "epoch": 0.750189701897019,
      "step": 13841,
      "training_loss": 4.411520957946777
    },
    {
      "epoch": 0.7502439024390244,
      "step": 13842,
      "training_loss": 4.937506675720215
    },
    {
      "epoch": 0.7502981029810298,
      "step": 13843,
      "training_loss": 6.618661403656006
    },
    {
      "epoch": 0.7503523035230353,
      "grad_norm": 26.13531494140625,
      "learning_rate": 1e-05,
      "loss": 5.5025,
      "step": 13844
    },
    {
      "epoch": 0.7503523035230353,
      "step": 13844,
      "training_loss": 3.0768144130706787
    },
    {
      "epoch": 0.7504065040650406,
      "step": 13845,
      "training_loss": 6.805917263031006
    },
    {
      "epoch": 0.7504607046070461,
      "step": 13846,
      "training_loss": 6.713167190551758
    },
    {
      "epoch": 0.7505149051490515,
      "step": 13847,
      "training_loss": 6.177852153778076
    },
    {
      "epoch": 0.750569105691057,
      "grad_norm": 35.928749084472656,
      "learning_rate": 1e-05,
      "loss": 5.6934,
      "step": 13848
    },
    {
      "epoch": 0.750569105691057,
      "step": 13848,
      "training_loss": 6.916708469390869
    },
    {
      "epoch": 0.7506233062330623,
      "step": 13849,
      "training_loss": 6.0028486251831055
    },
    {
      "epoch": 0.7506775067750677,
      "step": 13850,
      "training_loss": 5.912073612213135
    },
    {
      "epoch": 0.7507317073170732,
      "step": 13851,
      "training_loss": 6.458138465881348
    },
    {
      "epoch": 0.7507859078590786,
      "grad_norm": 27.805675506591797,
      "learning_rate": 1e-05,
      "loss": 6.3224,
      "step": 13852
    },
    {
      "epoch": 0.7507859078590786,
      "step": 13852,
      "training_loss": 7.951990127563477
    },
    {
      "epoch": 0.7508401084010841,
      "step": 13853,
      "training_loss": 6.484318733215332
    },
    {
      "epoch": 0.7508943089430894,
      "step": 13854,
      "training_loss": 6.585291862487793
    },
    {
      "epoch": 0.7509485094850948,
      "step": 13855,
      "training_loss": 7.081479549407959
    },
    {
      "epoch": 0.7510027100271003,
      "grad_norm": 41.05202865600586,
      "learning_rate": 1e-05,
      "loss": 7.0258,
      "step": 13856
    },
    {
      "epoch": 0.7510027100271003,
      "step": 13856,
      "training_loss": 7.992639064788818
    },
    {
      "epoch": 0.7510569105691057,
      "step": 13857,
      "training_loss": 6.691879749298096
    },
    {
      "epoch": 0.7511111111111111,
      "step": 13858,
      "training_loss": 5.893585681915283
    },
    {
      "epoch": 0.7511653116531165,
      "step": 13859,
      "training_loss": 7.185546398162842
    },
    {
      "epoch": 0.751219512195122,
      "grad_norm": 20.31263542175293,
      "learning_rate": 1e-05,
      "loss": 6.9409,
      "step": 13860
    },
    {
      "epoch": 0.751219512195122,
      "step": 13860,
      "training_loss": 6.965552806854248
    },
    {
      "epoch": 0.7512737127371274,
      "step": 13861,
      "training_loss": 4.742197036743164
    },
    {
      "epoch": 0.7513279132791328,
      "step": 13862,
      "training_loss": 6.381465911865234
    },
    {
      "epoch": 0.7513821138211382,
      "step": 13863,
      "training_loss": 6.402439117431641
    },
    {
      "epoch": 0.7514363143631436,
      "grad_norm": 24.64015007019043,
      "learning_rate": 1e-05,
      "loss": 6.1229,
      "step": 13864
    },
    {
      "epoch": 0.7514363143631436,
      "step": 13864,
      "training_loss": 5.134937763214111
    },
    {
      "epoch": 0.7514905149051491,
      "step": 13865,
      "training_loss": 7.16607666015625
    },
    {
      "epoch": 0.7515447154471545,
      "step": 13866,
      "training_loss": 5.500832557678223
    },
    {
      "epoch": 0.7515989159891598,
      "step": 13867,
      "training_loss": 7.23390007019043
    },
    {
      "epoch": 0.7516531165311653,
      "grad_norm": 38.525882720947266,
      "learning_rate": 1e-05,
      "loss": 6.2589,
      "step": 13868
    },
    {
      "epoch": 0.7516531165311653,
      "step": 13868,
      "training_loss": 6.8117899894714355
    },
    {
      "epoch": 0.7517073170731707,
      "step": 13869,
      "training_loss": 6.780588626861572
    },
    {
      "epoch": 0.7517615176151762,
      "step": 13870,
      "training_loss": 6.314352989196777
    },
    {
      "epoch": 0.7518157181571816,
      "step": 13871,
      "training_loss": 6.730205535888672
    },
    {
      "epoch": 0.751869918699187,
      "grad_norm": 16.605445861816406,
      "learning_rate": 1e-05,
      "loss": 6.6592,
      "step": 13872
    },
    {
      "epoch": 0.751869918699187,
      "step": 13872,
      "training_loss": 6.556007385253906
    },
    {
      "epoch": 0.7519241192411924,
      "step": 13873,
      "training_loss": 7.0427656173706055
    },
    {
      "epoch": 0.7519783197831978,
      "step": 13874,
      "training_loss": 7.51978063583374
    },
    {
      "epoch": 0.7520325203252033,
      "step": 13875,
      "training_loss": 5.800380706787109
    },
    {
      "epoch": 0.7520867208672086,
      "grad_norm": 46.96023941040039,
      "learning_rate": 1e-05,
      "loss": 6.7297,
      "step": 13876
    },
    {
      "epoch": 0.7520867208672086,
      "step": 13876,
      "training_loss": 6.555636405944824
    },
    {
      "epoch": 0.7521409214092141,
      "step": 13877,
      "training_loss": 6.251493453979492
    },
    {
      "epoch": 0.7521951219512195,
      "step": 13878,
      "training_loss": 4.778482913970947
    },
    {
      "epoch": 0.752249322493225,
      "step": 13879,
      "training_loss": 8.13788890838623
    },
    {
      "epoch": 0.7523035230352304,
      "grad_norm": 24.310279846191406,
      "learning_rate": 1e-05,
      "loss": 6.4309,
      "step": 13880
    },
    {
      "epoch": 0.7523035230352304,
      "step": 13880,
      "training_loss": 6.862000942230225
    },
    {
      "epoch": 0.7523577235772357,
      "step": 13881,
      "training_loss": 6.714364051818848
    },
    {
      "epoch": 0.7524119241192412,
      "step": 13882,
      "training_loss": 7.476295471191406
    },
    {
      "epoch": 0.7524661246612466,
      "step": 13883,
      "training_loss": 7.337977409362793
    },
    {
      "epoch": 0.7525203252032521,
      "grad_norm": 29.91333770751953,
      "learning_rate": 1e-05,
      "loss": 7.0977,
      "step": 13884
    },
    {
      "epoch": 0.7525203252032521,
      "step": 13884,
      "training_loss": 6.286677360534668
    },
    {
      "epoch": 0.7525745257452574,
      "step": 13885,
      "training_loss": 8.00150203704834
    },
    {
      "epoch": 0.7526287262872629,
      "step": 13886,
      "training_loss": 7.398382186889648
    },
    {
      "epoch": 0.7526829268292683,
      "step": 13887,
      "training_loss": 6.35186243057251
    },
    {
      "epoch": 0.7527371273712737,
      "grad_norm": 24.585079193115234,
      "learning_rate": 1e-05,
      "loss": 7.0096,
      "step": 13888
    },
    {
      "epoch": 0.7527371273712737,
      "step": 13888,
      "training_loss": 5.724531173706055
    },
    {
      "epoch": 0.7527913279132792,
      "step": 13889,
      "training_loss": 6.763989448547363
    },
    {
      "epoch": 0.7528455284552845,
      "step": 13890,
      "training_loss": 7.203251838684082
    },
    {
      "epoch": 0.75289972899729,
      "step": 13891,
      "training_loss": 5.8949480056762695
    },
    {
      "epoch": 0.7529539295392954,
      "grad_norm": 26.722028732299805,
      "learning_rate": 1e-05,
      "loss": 6.3967,
      "step": 13892
    },
    {
      "epoch": 0.7529539295392954,
      "step": 13892,
      "training_loss": 5.849264144897461
    },
    {
      "epoch": 0.7530081300813009,
      "step": 13893,
      "training_loss": 6.375054836273193
    },
    {
      "epoch": 0.7530623306233062,
      "step": 13894,
      "training_loss": 7.035491943359375
    },
    {
      "epoch": 0.7531165311653116,
      "step": 13895,
      "training_loss": 7.000594615936279
    },
    {
      "epoch": 0.7531707317073171,
      "grad_norm": 37.09959411621094,
      "learning_rate": 1e-05,
      "loss": 6.5651,
      "step": 13896
    },
    {
      "epoch": 0.7531707317073171,
      "step": 13896,
      "training_loss": 7.044629096984863
    },
    {
      "epoch": 0.7532249322493225,
      "step": 13897,
      "training_loss": 6.50998067855835
    },
    {
      "epoch": 0.753279132791328,
      "step": 13898,
      "training_loss": 6.395306587219238
    },
    {
      "epoch": 0.7533333333333333,
      "step": 13899,
      "training_loss": 6.742794513702393
    },
    {
      "epoch": 0.7533875338753387,
      "grad_norm": 29.407968521118164,
      "learning_rate": 1e-05,
      "loss": 6.6732,
      "step": 13900
    },
    {
      "epoch": 0.7533875338753387,
      "step": 13900,
      "training_loss": 3.5160703659057617
    },
    {
      "epoch": 0.7534417344173442,
      "step": 13901,
      "training_loss": 2.9295780658721924
    },
    {
      "epoch": 0.7534959349593496,
      "step": 13902,
      "training_loss": 6.812315464019775
    },
    {
      "epoch": 0.753550135501355,
      "step": 13903,
      "training_loss": 7.9042768478393555
    },
    {
      "epoch": 0.7536043360433604,
      "grad_norm": 38.86406707763672,
      "learning_rate": 1e-05,
      "loss": 5.2906,
      "step": 13904
    },
    {
      "epoch": 0.7536043360433604,
      "step": 13904,
      "training_loss": 6.423591613769531
    },
    {
      "epoch": 0.7536585365853659,
      "step": 13905,
      "training_loss": 7.110963821411133
    },
    {
      "epoch": 0.7537127371273713,
      "step": 13906,
      "training_loss": 6.773786544799805
    },
    {
      "epoch": 0.7537669376693767,
      "step": 13907,
      "training_loss": 6.90014123916626
    },
    {
      "epoch": 0.7538211382113821,
      "grad_norm": 26.360883712768555,
      "learning_rate": 1e-05,
      "loss": 6.8021,
      "step": 13908
    },
    {
      "epoch": 0.7538211382113821,
      "step": 13908,
      "training_loss": 7.751235485076904
    },
    {
      "epoch": 0.7538753387533875,
      "step": 13909,
      "training_loss": 7.041935443878174
    },
    {
      "epoch": 0.753929539295393,
      "step": 13910,
      "training_loss": 8.18715763092041
    },
    {
      "epoch": 0.7539837398373984,
      "step": 13911,
      "training_loss": 7.145560264587402
    },
    {
      "epoch": 0.7540379403794037,
      "grad_norm": 16.103307723999023,
      "learning_rate": 1e-05,
      "loss": 7.5315,
      "step": 13912
    },
    {
      "epoch": 0.7540379403794037,
      "step": 13912,
      "training_loss": 7.9673752784729
    },
    {
      "epoch": 0.7540921409214092,
      "step": 13913,
      "training_loss": 5.689972877502441
    },
    {
      "epoch": 0.7541463414634146,
      "step": 13914,
      "training_loss": 6.0242695808410645
    },
    {
      "epoch": 0.7542005420054201,
      "step": 13915,
      "training_loss": 6.068766117095947
    },
    {
      "epoch": 0.7542547425474255,
      "grad_norm": 31.65650177001953,
      "learning_rate": 1e-05,
      "loss": 6.4376,
      "step": 13916
    },
    {
      "epoch": 0.7542547425474255,
      "step": 13916,
      "training_loss": 7.337924003601074
    },
    {
      "epoch": 0.7543089430894309,
      "step": 13917,
      "training_loss": 5.350304126739502
    },
    {
      "epoch": 0.7543631436314363,
      "step": 13918,
      "training_loss": 4.122307777404785
    },
    {
      "epoch": 0.7544173441734418,
      "step": 13919,
      "training_loss": 6.79575777053833
    },
    {
      "epoch": 0.7544715447154472,
      "grad_norm": 21.701950073242188,
      "learning_rate": 1e-05,
      "loss": 5.9016,
      "step": 13920
    },
    {
      "epoch": 0.7544715447154472,
      "step": 13920,
      "training_loss": 7.352749347686768
    },
    {
      "epoch": 0.7545257452574525,
      "step": 13921,
      "training_loss": 7.220333099365234
    },
    {
      "epoch": 0.754579945799458,
      "step": 13922,
      "training_loss": 6.618748188018799
    },
    {
      "epoch": 0.7546341463414634,
      "step": 13923,
      "training_loss": 6.252191066741943
    },
    {
      "epoch": 0.7546883468834689,
      "grad_norm": 20.727554321289062,
      "learning_rate": 1e-05,
      "loss": 6.861,
      "step": 13924
    },
    {
      "epoch": 0.7546883468834689,
      "step": 13924,
      "training_loss": 3.0738954544067383
    },
    {
      "epoch": 0.7547425474254743,
      "step": 13925,
      "training_loss": 6.186399459838867
    },
    {
      "epoch": 0.7547967479674796,
      "step": 13926,
      "training_loss": 6.773885250091553
    },
    {
      "epoch": 0.7548509485094851,
      "step": 13927,
      "training_loss": 5.822902679443359
    },
    {
      "epoch": 0.7549051490514905,
      "grad_norm": 36.44413375854492,
      "learning_rate": 1e-05,
      "loss": 5.4643,
      "step": 13928
    },
    {
      "epoch": 0.7549051490514905,
      "step": 13928,
      "training_loss": 5.624622344970703
    },
    {
      "epoch": 0.754959349593496,
      "step": 13929,
      "training_loss": 6.742612838745117
    },
    {
      "epoch": 0.7550135501355013,
      "step": 13930,
      "training_loss": 8.916444778442383
    },
    {
      "epoch": 0.7550677506775068,
      "step": 13931,
      "training_loss": 6.796977996826172
    },
    {
      "epoch": 0.7551219512195122,
      "grad_norm": 20.84229278564453,
      "learning_rate": 1e-05,
      "loss": 7.0202,
      "step": 13932
    },
    {
      "epoch": 0.7551219512195122,
      "step": 13932,
      "training_loss": 6.287049770355225
    },
    {
      "epoch": 0.7551761517615176,
      "step": 13933,
      "training_loss": 6.823342800140381
    },
    {
      "epoch": 0.7552303523035231,
      "step": 13934,
      "training_loss": 6.696009159088135
    },
    {
      "epoch": 0.7552845528455284,
      "step": 13935,
      "training_loss": 6.984631061553955
    },
    {
      "epoch": 0.7553387533875339,
      "grad_norm": 27.066635131835938,
      "learning_rate": 1e-05,
      "loss": 6.6978,
      "step": 13936
    },
    {
      "epoch": 0.7553387533875339,
      "step": 13936,
      "training_loss": 6.940470218658447
    },
    {
      "epoch": 0.7553929539295393,
      "step": 13937,
      "training_loss": 6.86260461807251
    },
    {
      "epoch": 0.7554471544715448,
      "step": 13938,
      "training_loss": 6.817060947418213
    },
    {
      "epoch": 0.7555013550135501,
      "step": 13939,
      "training_loss": 6.2667012214660645
    },
    {
      "epoch": 0.7555555555555555,
      "grad_norm": 38.7918701171875,
      "learning_rate": 1e-05,
      "loss": 6.7217,
      "step": 13940
    },
    {
      "epoch": 0.7555555555555555,
      "step": 13940,
      "training_loss": 7.134743690490723
    },
    {
      "epoch": 0.755609756097561,
      "step": 13941,
      "training_loss": 7.856747150421143
    },
    {
      "epoch": 0.7556639566395664,
      "step": 13942,
      "training_loss": 4.470821380615234
    },
    {
      "epoch": 0.7557181571815719,
      "step": 13943,
      "training_loss": 5.3998236656188965
    },
    {
      "epoch": 0.7557723577235772,
      "grad_norm": 68.41704559326172,
      "learning_rate": 1e-05,
      "loss": 6.2155,
      "step": 13944
    },
    {
      "epoch": 0.7557723577235772,
      "step": 13944,
      "training_loss": 5.040103912353516
    },
    {
      "epoch": 0.7558265582655826,
      "step": 13945,
      "training_loss": 4.498140811920166
    },
    {
      "epoch": 0.7558807588075881,
      "step": 13946,
      "training_loss": 8.292967796325684
    },
    {
      "epoch": 0.7559349593495935,
      "step": 13947,
      "training_loss": 3.2286410331726074
    },
    {
      "epoch": 0.7559891598915989,
      "grad_norm": 24.92400550842285,
      "learning_rate": 1e-05,
      "loss": 5.265,
      "step": 13948
    },
    {
      "epoch": 0.7559891598915989,
      "step": 13948,
      "training_loss": 7.518735408782959
    },
    {
      "epoch": 0.7560433604336043,
      "step": 13949,
      "training_loss": 4.782576084136963
    },
    {
      "epoch": 0.7560975609756098,
      "step": 13950,
      "training_loss": 6.451953887939453
    },
    {
      "epoch": 0.7561517615176152,
      "step": 13951,
      "training_loss": 6.0758748054504395
    },
    {
      "epoch": 0.7562059620596207,
      "grad_norm": 41.59050369262695,
      "learning_rate": 1e-05,
      "loss": 6.2073,
      "step": 13952
    },
    {
      "epoch": 0.7562059620596207,
      "step": 13952,
      "training_loss": 4.6209492683410645
    },
    {
      "epoch": 0.756260162601626,
      "step": 13953,
      "training_loss": 6.876738548278809
    },
    {
      "epoch": 0.7563143631436314,
      "step": 13954,
      "training_loss": 7.717751502990723
    },
    {
      "epoch": 0.7563685636856369,
      "step": 13955,
      "training_loss": 6.403417587280273
    },
    {
      "epoch": 0.7564227642276423,
      "grad_norm": 25.376981735229492,
      "learning_rate": 1e-05,
      "loss": 6.4047,
      "step": 13956
    },
    {
      "epoch": 0.7564227642276423,
      "step": 13956,
      "training_loss": 6.835827827453613
    },
    {
      "epoch": 0.7564769647696477,
      "step": 13957,
      "training_loss": 7.701315402984619
    },
    {
      "epoch": 0.7565311653116531,
      "step": 13958,
      "training_loss": 7.571823596954346
    },
    {
      "epoch": 0.7565853658536585,
      "step": 13959,
      "training_loss": 7.869134902954102
    },
    {
      "epoch": 0.756639566395664,
      "grad_norm": 24.548036575317383,
      "learning_rate": 1e-05,
      "loss": 7.4945,
      "step": 13960
    },
    {
      "epoch": 0.756639566395664,
      "step": 13960,
      "training_loss": 7.371088981628418
    },
    {
      "epoch": 0.7566937669376694,
      "step": 13961,
      "training_loss": 7.606346130371094
    },
    {
      "epoch": 0.7567479674796748,
      "step": 13962,
      "training_loss": 6.090420722961426
    },
    {
      "epoch": 0.7568021680216802,
      "step": 13963,
      "training_loss": 6.6808085441589355
    },
    {
      "epoch": 0.7568563685636857,
      "grad_norm": 29.595556259155273,
      "learning_rate": 1e-05,
      "loss": 6.9372,
      "step": 13964
    },
    {
      "epoch": 0.7568563685636857,
      "step": 13964,
      "training_loss": 5.481461524963379
    },
    {
      "epoch": 0.7569105691056911,
      "step": 13965,
      "training_loss": 6.751152515411377
    },
    {
      "epoch": 0.7569647696476964,
      "step": 13966,
      "training_loss": 6.4875946044921875
    },
    {
      "epoch": 0.7570189701897019,
      "step": 13967,
      "training_loss": 6.682313442230225
    },
    {
      "epoch": 0.7570731707317073,
      "grad_norm": 19.775300979614258,
      "learning_rate": 1e-05,
      "loss": 6.3506,
      "step": 13968
    },
    {
      "epoch": 0.7570731707317073,
      "step": 13968,
      "training_loss": 6.6224846839904785
    },
    {
      "epoch": 0.7571273712737128,
      "step": 13969,
      "training_loss": 5.270442485809326
    },
    {
      "epoch": 0.7571815718157181,
      "step": 13970,
      "training_loss": 6.633214473724365
    },
    {
      "epoch": 0.7572357723577235,
      "step": 13971,
      "training_loss": 6.794211387634277
    },
    {
      "epoch": 0.757289972899729,
      "grad_norm": 35.11789321899414,
      "learning_rate": 1e-05,
      "loss": 6.3301,
      "step": 13972
    },
    {
      "epoch": 0.757289972899729,
      "step": 13972,
      "training_loss": 6.603647232055664
    },
    {
      "epoch": 0.7573441734417344,
      "step": 13973,
      "training_loss": 6.732786178588867
    },
    {
      "epoch": 0.7573983739837399,
      "step": 13974,
      "training_loss": 6.191057205200195
    },
    {
      "epoch": 0.7574525745257452,
      "step": 13975,
      "training_loss": 7.045285224914551
    },
    {
      "epoch": 0.7575067750677507,
      "grad_norm": 30.126955032348633,
      "learning_rate": 1e-05,
      "loss": 6.6432,
      "step": 13976
    },
    {
      "epoch": 0.7575067750677507,
      "step": 13976,
      "training_loss": 6.434067249298096
    },
    {
      "epoch": 0.7575609756097561,
      "step": 13977,
      "training_loss": 5.585591793060303
    },
    {
      "epoch": 0.7576151761517615,
      "step": 13978,
      "training_loss": 5.472135066986084
    },
    {
      "epoch": 0.7576693766937669,
      "step": 13979,
      "training_loss": 6.5399885177612305
    },
    {
      "epoch": 0.7577235772357723,
      "grad_norm": 33.229522705078125,
      "learning_rate": 1e-05,
      "loss": 6.0079,
      "step": 13980
    },
    {
      "epoch": 0.7577235772357723,
      "step": 13980,
      "training_loss": 5.409369945526123
    },
    {
      "epoch": 0.7577777777777778,
      "step": 13981,
      "training_loss": 7.11825704574585
    },
    {
      "epoch": 0.7578319783197832,
      "step": 13982,
      "training_loss": 6.688251972198486
    },
    {
      "epoch": 0.7578861788617887,
      "step": 13983,
      "training_loss": 3.9394726753234863
    },
    {
      "epoch": 0.757940379403794,
      "grad_norm": 34.80803680419922,
      "learning_rate": 1e-05,
      "loss": 5.7888,
      "step": 13984
    },
    {
      "epoch": 0.757940379403794,
      "step": 13984,
      "training_loss": 8.032156944274902
    },
    {
      "epoch": 0.7579945799457994,
      "step": 13985,
      "training_loss": 6.601797580718994
    },
    {
      "epoch": 0.7580487804878049,
      "step": 13986,
      "training_loss": 7.262112617492676
    },
    {
      "epoch": 0.7581029810298103,
      "step": 13987,
      "training_loss": 6.997250556945801
    },
    {
      "epoch": 0.7581571815718157,
      "grad_norm": 32.5871696472168,
      "learning_rate": 1e-05,
      "loss": 7.2233,
      "step": 13988
    },
    {
      "epoch": 0.7581571815718157,
      "step": 13988,
      "training_loss": 7.521765232086182
    },
    {
      "epoch": 0.7582113821138211,
      "step": 13989,
      "training_loss": 6.517967224121094
    },
    {
      "epoch": 0.7582655826558266,
      "step": 13990,
      "training_loss": 7.831247806549072
    },
    {
      "epoch": 0.758319783197832,
      "step": 13991,
      "training_loss": 3.46662974357605
    },
    {
      "epoch": 0.7583739837398374,
      "grad_norm": 37.735782623291016,
      "learning_rate": 1e-05,
      "loss": 6.3344,
      "step": 13992
    },
    {
      "epoch": 0.7583739837398374,
      "step": 13992,
      "training_loss": 6.097633361816406
    },
    {
      "epoch": 0.7584281842818428,
      "step": 13993,
      "training_loss": 5.806560516357422
    },
    {
      "epoch": 0.7584823848238482,
      "step": 13994,
      "training_loss": 6.822576999664307
    },
    {
      "epoch": 0.7585365853658537,
      "step": 13995,
      "training_loss": 6.820066452026367
    },
    {
      "epoch": 0.7585907859078591,
      "grad_norm": 34.77791976928711,
      "learning_rate": 1e-05,
      "loss": 6.3867,
      "step": 13996
    },
    {
      "epoch": 0.7585907859078591,
      "step": 13996,
      "training_loss": 6.545499324798584
    },
    {
      "epoch": 0.7586449864498644,
      "step": 13997,
      "training_loss": 6.46958589553833
    },
    {
      "epoch": 0.7586991869918699,
      "step": 13998,
      "training_loss": 7.13545560836792
    },
    {
      "epoch": 0.7587533875338753,
      "step": 13999,
      "training_loss": 6.657192707061768
    },
    {
      "epoch": 0.7588075880758808,
      "grad_norm": 51.0285530090332,
      "learning_rate": 1e-05,
      "loss": 6.7019,
      "step": 14000
    },
    {
      "epoch": 0.7588075880758808,
      "step": 14000,
      "training_loss": 7.332564830780029
    },
    {
      "epoch": 0.7588617886178862,
      "step": 14001,
      "training_loss": 6.057534694671631
    },
    {
      "epoch": 0.7589159891598916,
      "step": 14002,
      "training_loss": 6.4186482429504395
    },
    {
      "epoch": 0.758970189701897,
      "step": 14003,
      "training_loss": 7.452563285827637
    },
    {
      "epoch": 0.7590243902439024,
      "grad_norm": 33.80168914794922,
      "learning_rate": 1e-05,
      "loss": 6.8153,
      "step": 14004
    },
    {
      "epoch": 0.7590243902439024,
      "step": 14004,
      "training_loss": 7.525345802307129
    },
    {
      "epoch": 0.7590785907859079,
      "step": 14005,
      "training_loss": 5.541624546051025
    },
    {
      "epoch": 0.7591327913279132,
      "step": 14006,
      "training_loss": 6.036025524139404
    },
    {
      "epoch": 0.7591869918699187,
      "step": 14007,
      "training_loss": 6.318363189697266
    },
    {
      "epoch": 0.7592411924119241,
      "grad_norm": 23.388933181762695,
      "learning_rate": 1e-05,
      "loss": 6.3553,
      "step": 14008
    },
    {
      "epoch": 0.7592411924119241,
      "step": 14008,
      "training_loss": 5.908214569091797
    },
    {
      "epoch": 0.7592953929539296,
      "step": 14009,
      "training_loss": 7.459897518157959
    },
    {
      "epoch": 0.759349593495935,
      "step": 14010,
      "training_loss": 6.286645412445068
    },
    {
      "epoch": 0.7594037940379403,
      "step": 14011,
      "training_loss": 6.367324352264404
    },
    {
      "epoch": 0.7594579945799458,
      "grad_norm": 25.999433517456055,
      "learning_rate": 1e-05,
      "loss": 6.5055,
      "step": 14012
    },
    {
      "epoch": 0.7594579945799458,
      "step": 14012,
      "training_loss": 6.035159111022949
    },
    {
      "epoch": 0.7595121951219512,
      "step": 14013,
      "training_loss": 6.610824108123779
    },
    {
      "epoch": 0.7595663956639567,
      "step": 14014,
      "training_loss": 7.394717693328857
    },
    {
      "epoch": 0.759620596205962,
      "step": 14015,
      "training_loss": 6.351720809936523
    },
    {
      "epoch": 0.7596747967479675,
      "grad_norm": 22.617969512939453,
      "learning_rate": 1e-05,
      "loss": 6.5981,
      "step": 14016
    },
    {
      "epoch": 0.7596747967479675,
      "step": 14016,
      "training_loss": 6.858919620513916
    },
    {
      "epoch": 0.7597289972899729,
      "step": 14017,
      "training_loss": 6.997955799102783
    },
    {
      "epoch": 0.7597831978319783,
      "step": 14018,
      "training_loss": 6.76205587387085
    },
    {
      "epoch": 0.7598373983739838,
      "step": 14019,
      "training_loss": 6.754695892333984
    },
    {
      "epoch": 0.7598915989159891,
      "grad_norm": 18.059301376342773,
      "learning_rate": 1e-05,
      "loss": 6.8434,
      "step": 14020
    },
    {
      "epoch": 0.7598915989159891,
      "step": 14020,
      "training_loss": 4.105504035949707
    },
    {
      "epoch": 0.7599457994579946,
      "step": 14021,
      "training_loss": 6.374274730682373
    },
    {
      "epoch": 0.76,
      "step": 14022,
      "training_loss": 6.260574817657471
    },
    {
      "epoch": 0.7600542005420055,
      "step": 14023,
      "training_loss": 3.9758126735687256
    },
    {
      "epoch": 0.7601084010840108,
      "grad_norm": 33.473541259765625,
      "learning_rate": 1e-05,
      "loss": 5.179,
      "step": 14024
    },
    {
      "epoch": 0.7601084010840108,
      "step": 14024,
      "training_loss": 6.128659725189209
    },
    {
      "epoch": 0.7601626016260162,
      "step": 14025,
      "training_loss": 6.629839897155762
    },
    {
      "epoch": 0.7602168021680217,
      "step": 14026,
      "training_loss": 6.348839282989502
    },
    {
      "epoch": 0.7602710027100271,
      "step": 14027,
      "training_loss": 6.565992832183838
    },
    {
      "epoch": 0.7603252032520326,
      "grad_norm": 28.984825134277344,
      "learning_rate": 1e-05,
      "loss": 6.4183,
      "step": 14028
    },
    {
      "epoch": 0.7603252032520326,
      "step": 14028,
      "training_loss": 6.6346755027771
    },
    {
      "epoch": 0.7603794037940379,
      "step": 14029,
      "training_loss": 7.343516826629639
    },
    {
      "epoch": 0.7604336043360433,
      "step": 14030,
      "training_loss": 6.412454128265381
    },
    {
      "epoch": 0.7604878048780488,
      "step": 14031,
      "training_loss": 6.274974822998047
    },
    {
      "epoch": 0.7605420054200542,
      "grad_norm": 30.915605545043945,
      "learning_rate": 1e-05,
      "loss": 6.6664,
      "step": 14032
    },
    {
      "epoch": 0.7605420054200542,
      "step": 14032,
      "training_loss": 5.898844242095947
    },
    {
      "epoch": 0.7605962059620596,
      "step": 14033,
      "training_loss": 7.375756740570068
    },
    {
      "epoch": 0.760650406504065,
      "step": 14034,
      "training_loss": 4.684595584869385
    },
    {
      "epoch": 0.7607046070460705,
      "step": 14035,
      "training_loss": 6.26068639755249
    },
    {
      "epoch": 0.7607588075880759,
      "grad_norm": 23.0069580078125,
      "learning_rate": 1e-05,
      "loss": 6.055,
      "step": 14036
    },
    {
      "epoch": 0.7607588075880759,
      "step": 14036,
      "training_loss": 5.697808265686035
    },
    {
      "epoch": 0.7608130081300813,
      "step": 14037,
      "training_loss": 6.097753047943115
    },
    {
      "epoch": 0.7608672086720867,
      "step": 14038,
      "training_loss": 7.4113664627075195
    },
    {
      "epoch": 0.7609214092140921,
      "step": 14039,
      "training_loss": 5.889183044433594
    },
    {
      "epoch": 0.7609756097560976,
      "grad_norm": 38.75428771972656,
      "learning_rate": 1e-05,
      "loss": 6.274,
      "step": 14040
    },
    {
      "epoch": 0.7609756097560976,
      "step": 14040,
      "training_loss": 6.7060017585754395
    },
    {
      "epoch": 0.761029810298103,
      "step": 14041,
      "training_loss": 5.862371921539307
    },
    {
      "epoch": 0.7610840108401083,
      "step": 14042,
      "training_loss": 7.008450984954834
    },
    {
      "epoch": 0.7611382113821138,
      "step": 14043,
      "training_loss": 5.885298728942871
    },
    {
      "epoch": 0.7611924119241192,
      "grad_norm": 29.390666961669922,
      "learning_rate": 1e-05,
      "loss": 6.3655,
      "step": 14044
    },
    {
      "epoch": 0.7611924119241192,
      "step": 14044,
      "training_loss": 3.9806151390075684
    },
    {
      "epoch": 0.7612466124661247,
      "step": 14045,
      "training_loss": 6.648742198944092
    },
    {
      "epoch": 0.7613008130081301,
      "step": 14046,
      "training_loss": 7.272248268127441
    },
    {
      "epoch": 0.7613550135501355,
      "step": 14047,
      "training_loss": 6.2198076248168945
    },
    {
      "epoch": 0.7614092140921409,
      "grad_norm": 26.951440811157227,
      "learning_rate": 1e-05,
      "loss": 6.0304,
      "step": 14048
    },
    {
      "epoch": 0.7614092140921409,
      "step": 14048,
      "training_loss": 6.2836456298828125
    },
    {
      "epoch": 0.7614634146341464,
      "step": 14049,
      "training_loss": 7.130674362182617
    },
    {
      "epoch": 0.7615176151761518,
      "step": 14050,
      "training_loss": 5.92164421081543
    },
    {
      "epoch": 0.7615718157181571,
      "step": 14051,
      "training_loss": 7.585004806518555
    },
    {
      "epoch": 0.7616260162601626,
      "grad_norm": 35.00444793701172,
      "learning_rate": 1e-05,
      "loss": 6.7302,
      "step": 14052
    },
    {
      "epoch": 0.7616260162601626,
      "step": 14052,
      "training_loss": 5.028720855712891
    },
    {
      "epoch": 0.761680216802168,
      "step": 14053,
      "training_loss": 3.3772776126861572
    },
    {
      "epoch": 0.7617344173441735,
      "step": 14054,
      "training_loss": 5.992038726806641
    },
    {
      "epoch": 0.7617886178861789,
      "step": 14055,
      "training_loss": 5.849551677703857
    },
    {
      "epoch": 0.7618428184281842,
      "grad_norm": 19.03765106201172,
      "learning_rate": 1e-05,
      "loss": 5.0619,
      "step": 14056
    },
    {
      "epoch": 0.7618428184281842,
      "step": 14056,
      "training_loss": 6.797171592712402
    },
    {
      "epoch": 0.7618970189701897,
      "step": 14057,
      "training_loss": 7.13530969619751
    },
    {
      "epoch": 0.7619512195121951,
      "step": 14058,
      "training_loss": 7.365872383117676
    },
    {
      "epoch": 0.7620054200542006,
      "step": 14059,
      "training_loss": 5.056301116943359
    },
    {
      "epoch": 0.7620596205962059,
      "grad_norm": 29.10121726989746,
      "learning_rate": 1e-05,
      "loss": 6.5887,
      "step": 14060
    },
    {
      "epoch": 0.7620596205962059,
      "step": 14060,
      "training_loss": 5.376150608062744
    },
    {
      "epoch": 0.7621138211382114,
      "step": 14061,
      "training_loss": 6.624121189117432
    },
    {
      "epoch": 0.7621680216802168,
      "step": 14062,
      "training_loss": 6.225417613983154
    },
    {
      "epoch": 0.7622222222222222,
      "step": 14063,
      "training_loss": 6.53591775894165
    },
    {
      "epoch": 0.7622764227642277,
      "grad_norm": 28.842313766479492,
      "learning_rate": 1e-05,
      "loss": 6.1904,
      "step": 14064
    },
    {
      "epoch": 0.7622764227642277,
      "step": 14064,
      "training_loss": 6.9017534255981445
    },
    {
      "epoch": 0.762330623306233,
      "step": 14065,
      "training_loss": 2.927560567855835
    },
    {
      "epoch": 0.7623848238482385,
      "step": 14066,
      "training_loss": 7.484742641448975
    },
    {
      "epoch": 0.7624390243902439,
      "step": 14067,
      "training_loss": 5.528310298919678
    },
    {
      "epoch": 0.7624932249322494,
      "grad_norm": 41.868160247802734,
      "learning_rate": 1e-05,
      "loss": 5.7106,
      "step": 14068
    },
    {
      "epoch": 0.7624932249322494,
      "step": 14068,
      "training_loss": 7.791211128234863
    },
    {
      "epoch": 0.7625474254742547,
      "step": 14069,
      "training_loss": 5.082217693328857
    },
    {
      "epoch": 0.7626016260162601,
      "step": 14070,
      "training_loss": 6.35721492767334
    },
    {
      "epoch": 0.7626558265582656,
      "step": 14071,
      "training_loss": 7.622959136962891
    },
    {
      "epoch": 0.762710027100271,
      "grad_norm": 69.65029907226562,
      "learning_rate": 1e-05,
      "loss": 6.7134,
      "step": 14072
    },
    {
      "epoch": 0.762710027100271,
      "step": 14072,
      "training_loss": 6.803024768829346
    },
    {
      "epoch": 0.7627642276422765,
      "step": 14073,
      "training_loss": 7.314977169036865
    },
    {
      "epoch": 0.7628184281842818,
      "step": 14074,
      "training_loss": 5.785933971405029
    },
    {
      "epoch": 0.7628726287262872,
      "step": 14075,
      "training_loss": 5.167011737823486
    },
    {
      "epoch": 0.7629268292682927,
      "grad_norm": 50.60600662231445,
      "learning_rate": 1e-05,
      "loss": 6.2677,
      "step": 14076
    },
    {
      "epoch": 0.7629268292682927,
      "step": 14076,
      "training_loss": 6.650280952453613
    },
    {
      "epoch": 0.7629810298102981,
      "step": 14077,
      "training_loss": 6.343581199645996
    },
    {
      "epoch": 0.7630352303523035,
      "step": 14078,
      "training_loss": 7.624325275421143
    },
    {
      "epoch": 0.7630894308943089,
      "step": 14079,
      "training_loss": 4.580428123474121
    },
    {
      "epoch": 0.7631436314363144,
      "grad_norm": 24.76299476623535,
      "learning_rate": 1e-05,
      "loss": 6.2997,
      "step": 14080
    },
    {
      "epoch": 0.7631436314363144,
      "step": 14080,
      "training_loss": 5.967223644256592
    },
    {
      "epoch": 0.7631978319783198,
      "step": 14081,
      "training_loss": 6.805870532989502
    },
    {
      "epoch": 0.7632520325203253,
      "step": 14082,
      "training_loss": 7.083889961242676
    },
    {
      "epoch": 0.7633062330623306,
      "step": 14083,
      "training_loss": 6.287846565246582
    },
    {
      "epoch": 0.763360433604336,
      "grad_norm": 45.827598571777344,
      "learning_rate": 1e-05,
      "loss": 6.5362,
      "step": 14084
    },
    {
      "epoch": 0.763360433604336,
      "step": 14084,
      "training_loss": 6.682529449462891
    },
    {
      "epoch": 0.7634146341463415,
      "step": 14085,
      "training_loss": 7.152365684509277
    },
    {
      "epoch": 0.7634688346883469,
      "step": 14086,
      "training_loss": 7.952178478240967
    },
    {
      "epoch": 0.7635230352303523,
      "step": 14087,
      "training_loss": 5.555695533752441
    },
    {
      "epoch": 0.7635772357723577,
      "grad_norm": 30.876007080078125,
      "learning_rate": 1e-05,
      "loss": 6.8357,
      "step": 14088
    },
    {
      "epoch": 0.7635772357723577,
      "step": 14088,
      "training_loss": 6.853410720825195
    },
    {
      "epoch": 0.7636314363143631,
      "step": 14089,
      "training_loss": 6.291284084320068
    },
    {
      "epoch": 0.7636856368563686,
      "step": 14090,
      "training_loss": 5.570656776428223
    },
    {
      "epoch": 0.763739837398374,
      "step": 14091,
      "training_loss": 7.814794063568115
    },
    {
      "epoch": 0.7637940379403794,
      "grad_norm": 37.419647216796875,
      "learning_rate": 1e-05,
      "loss": 6.6325,
      "step": 14092
    },
    {
      "epoch": 0.7637940379403794,
      "step": 14092,
      "training_loss": 7.1973490715026855
    },
    {
      "epoch": 0.7638482384823848,
      "step": 14093,
      "training_loss": 7.163740158081055
    },
    {
      "epoch": 0.7639024390243903,
      "step": 14094,
      "training_loss": 7.051567077636719
    },
    {
      "epoch": 0.7639566395663957,
      "step": 14095,
      "training_loss": 6.045437812805176
    },
    {
      "epoch": 0.764010840108401,
      "grad_norm": 22.77745819091797,
      "learning_rate": 1e-05,
      "loss": 6.8645,
      "step": 14096
    },
    {
      "epoch": 0.764010840108401,
      "step": 14096,
      "training_loss": 5.385773658752441
    },
    {
      "epoch": 0.7640650406504065,
      "step": 14097,
      "training_loss": 7.3709845542907715
    },
    {
      "epoch": 0.7641192411924119,
      "step": 14098,
      "training_loss": 7.940972328186035
    },
    {
      "epoch": 0.7641734417344174,
      "step": 14099,
      "training_loss": 5.772054195404053
    },
    {
      "epoch": 0.7642276422764228,
      "grad_norm": 44.481937408447266,
      "learning_rate": 1e-05,
      "loss": 6.6174,
      "step": 14100
    },
    {
      "epoch": 0.7642276422764228,
      "step": 14100,
      "training_loss": 7.655354976654053
    },
    {
      "epoch": 0.7642818428184281,
      "step": 14101,
      "training_loss": 5.590702056884766
    },
    {
      "epoch": 0.7643360433604336,
      "step": 14102,
      "training_loss": 6.634785175323486
    },
    {
      "epoch": 0.764390243902439,
      "step": 14103,
      "training_loss": 5.409379005432129
    },
    {
      "epoch": 0.7644444444444445,
      "grad_norm": 35.79724884033203,
      "learning_rate": 1e-05,
      "loss": 6.3226,
      "step": 14104
    },
    {
      "epoch": 0.7644444444444445,
      "step": 14104,
      "training_loss": 6.320159912109375
    },
    {
      "epoch": 0.7644986449864498,
      "step": 14105,
      "training_loss": 7.743143558502197
    },
    {
      "epoch": 0.7645528455284553,
      "step": 14106,
      "training_loss": 7.618618965148926
    },
    {
      "epoch": 0.7646070460704607,
      "step": 14107,
      "training_loss": 6.5948920249938965
    },
    {
      "epoch": 0.7646612466124662,
      "grad_norm": 32.99565505981445,
      "learning_rate": 1e-05,
      "loss": 7.0692,
      "step": 14108
    },
    {
      "epoch": 0.7646612466124662,
      "step": 14108,
      "training_loss": 6.564640998840332
    },
    {
      "epoch": 0.7647154471544716,
      "step": 14109,
      "training_loss": 6.744570255279541
    },
    {
      "epoch": 0.7647696476964769,
      "step": 14110,
      "training_loss": 5.8963093757629395
    },
    {
      "epoch": 0.7648238482384824,
      "step": 14111,
      "training_loss": 4.159516334533691
    },
    {
      "epoch": 0.7648780487804878,
      "grad_norm": 29.830795288085938,
      "learning_rate": 1e-05,
      "loss": 5.8413,
      "step": 14112
    },
    {
      "epoch": 0.7648780487804878,
      "step": 14112,
      "training_loss": 5.656691074371338
    },
    {
      "epoch": 0.7649322493224933,
      "step": 14113,
      "training_loss": 6.531785488128662
    },
    {
      "epoch": 0.7649864498644986,
      "step": 14114,
      "training_loss": 7.770365238189697
    },
    {
      "epoch": 0.765040650406504,
      "step": 14115,
      "training_loss": 7.120790481567383
    },
    {
      "epoch": 0.7650948509485095,
      "grad_norm": 32.85218811035156,
      "learning_rate": 1e-05,
      "loss": 6.7699,
      "step": 14116
    },
    {
      "epoch": 0.7650948509485095,
      "step": 14116,
      "training_loss": 6.414761543273926
    },
    {
      "epoch": 0.7651490514905149,
      "step": 14117,
      "training_loss": 6.74216365814209
    },
    {
      "epoch": 0.7652032520325204,
      "step": 14118,
      "training_loss": 7.399284362792969
    },
    {
      "epoch": 0.7652574525745257,
      "step": 14119,
      "training_loss": 7.775216102600098
    },
    {
      "epoch": 0.7653116531165312,
      "grad_norm": 19.32293128967285,
      "learning_rate": 1e-05,
      "loss": 7.0829,
      "step": 14120
    },
    {
      "epoch": 0.7653116531165312,
      "step": 14120,
      "training_loss": 6.675522327423096
    },
    {
      "epoch": 0.7653658536585366,
      "step": 14121,
      "training_loss": 6.603292942047119
    },
    {
      "epoch": 0.765420054200542,
      "step": 14122,
      "training_loss": 6.967245101928711
    },
    {
      "epoch": 0.7654742547425474,
      "step": 14123,
      "training_loss": 6.8164238929748535
    },
    {
      "epoch": 0.7655284552845528,
      "grad_norm": 31.837974548339844,
      "learning_rate": 1e-05,
      "loss": 6.7656,
      "step": 14124
    },
    {
      "epoch": 0.7655284552845528,
      "step": 14124,
      "training_loss": 7.311849117279053
    },
    {
      "epoch": 0.7655826558265583,
      "step": 14125,
      "training_loss": 6.731072902679443
    },
    {
      "epoch": 0.7656368563685637,
      "step": 14126,
      "training_loss": 5.147167682647705
    },
    {
      "epoch": 0.7656910569105692,
      "step": 14127,
      "training_loss": 4.838115215301514
    },
    {
      "epoch": 0.7657452574525745,
      "grad_norm": 29.005062103271484,
      "learning_rate": 1e-05,
      "loss": 6.0071,
      "step": 14128
    },
    {
      "epoch": 0.7657452574525745,
      "step": 14128,
      "training_loss": 6.9154767990112305
    },
    {
      "epoch": 0.7657994579945799,
      "step": 14129,
      "training_loss": 8.011382102966309
    },
    {
      "epoch": 0.7658536585365854,
      "step": 14130,
      "training_loss": 4.4961256980896
    },
    {
      "epoch": 0.7659078590785908,
      "step": 14131,
      "training_loss": 6.037909030914307
    },
    {
      "epoch": 0.7659620596205962,
      "grad_norm": 21.721553802490234,
      "learning_rate": 1e-05,
      "loss": 6.3652,
      "step": 14132
    },
    {
      "epoch": 0.7659620596205962,
      "step": 14132,
      "training_loss": 7.484293460845947
    },
    {
      "epoch": 0.7660162601626016,
      "step": 14133,
      "training_loss": 7.213776588439941
    },
    {
      "epoch": 0.766070460704607,
      "step": 14134,
      "training_loss": 7.497116565704346
    },
    {
      "epoch": 0.7661246612466125,
      "step": 14135,
      "training_loss": 7.0083746910095215
    },
    {
      "epoch": 0.7661788617886179,
      "grad_norm": 31.22498321533203,
      "learning_rate": 1e-05,
      "loss": 7.3009,
      "step": 14136
    },
    {
      "epoch": 0.7661788617886179,
      "step": 14136,
      "training_loss": 8.736834526062012
    },
    {
      "epoch": 0.7662330623306233,
      "step": 14137,
      "training_loss": 6.533949375152588
    },
    {
      "epoch": 0.7662872628726287,
      "step": 14138,
      "training_loss": 6.358627796173096
    },
    {
      "epoch": 0.7663414634146342,
      "step": 14139,
      "training_loss": 6.869455337524414
    },
    {
      "epoch": 0.7663956639566396,
      "grad_norm": 16.89816665649414,
      "learning_rate": 1e-05,
      "loss": 7.1247,
      "step": 14140
    },
    {
      "epoch": 0.7663956639566396,
      "step": 14140,
      "training_loss": 5.276460647583008
    },
    {
      "epoch": 0.7664498644986449,
      "step": 14141,
      "training_loss": 7.36467981338501
    },
    {
      "epoch": 0.7665040650406504,
      "step": 14142,
      "training_loss": 7.875936985015869
    },
    {
      "epoch": 0.7665582655826558,
      "step": 14143,
      "training_loss": 6.331814765930176
    },
    {
      "epoch": 0.7666124661246613,
      "grad_norm": 22.737133026123047,
      "learning_rate": 1e-05,
      "loss": 6.7122,
      "step": 14144
    },
    {
      "epoch": 0.7666124661246613,
      "step": 14144,
      "training_loss": 6.047938823699951
    },
    {
      "epoch": 0.7666666666666667,
      "step": 14145,
      "training_loss": 7.80175256729126
    },
    {
      "epoch": 0.766720867208672,
      "step": 14146,
      "training_loss": 6.707430362701416
    },
    {
      "epoch": 0.7667750677506775,
      "step": 14147,
      "training_loss": 8.476816177368164
    },
    {
      "epoch": 0.7668292682926829,
      "grad_norm": 39.820926666259766,
      "learning_rate": 1e-05,
      "loss": 7.2585,
      "step": 14148
    },
    {
      "epoch": 0.7668292682926829,
      "step": 14148,
      "training_loss": 6.287956714630127
    },
    {
      "epoch": 0.7668834688346884,
      "step": 14149,
      "training_loss": 7.093891143798828
    },
    {
      "epoch": 0.7669376693766937,
      "step": 14150,
      "training_loss": 8.346822738647461
    },
    {
      "epoch": 0.7669918699186992,
      "step": 14151,
      "training_loss": 7.34268856048584
    },
    {
      "epoch": 0.7670460704607046,
      "grad_norm": 28.424856185913086,
      "learning_rate": 1e-05,
      "loss": 7.2678,
      "step": 14152
    },
    {
      "epoch": 0.7670460704607046,
      "step": 14152,
      "training_loss": 9.17145824432373
    },
    {
      "epoch": 0.76710027100271,
      "step": 14153,
      "training_loss": 7.726661205291748
    },
    {
      "epoch": 0.7671544715447155,
      "step": 14154,
      "training_loss": 7.288576126098633
    },
    {
      "epoch": 0.7672086720867208,
      "step": 14155,
      "training_loss": 7.378851413726807
    },
    {
      "epoch": 0.7672628726287263,
      "grad_norm": 22.926572799682617,
      "learning_rate": 1e-05,
      "loss": 7.8914,
      "step": 14156
    },
    {
      "epoch": 0.7672628726287263,
      "step": 14156,
      "training_loss": 6.4560546875
    },
    {
      "epoch": 0.7673170731707317,
      "step": 14157,
      "training_loss": 6.440276622772217
    },
    {
      "epoch": 0.7673712737127372,
      "step": 14158,
      "training_loss": 6.201153755187988
    },
    {
      "epoch": 0.7674254742547425,
      "step": 14159,
      "training_loss": 8.09875774383545
    },
    {
      "epoch": 0.767479674796748,
      "grad_norm": 18.05921173095703,
      "learning_rate": 1e-05,
      "loss": 6.7991,
      "step": 14160
    },
    {
      "epoch": 0.767479674796748,
      "step": 14160,
      "training_loss": 7.523416519165039
    },
    {
      "epoch": 0.7675338753387534,
      "step": 14161,
      "training_loss": 6.169261455535889
    },
    {
      "epoch": 0.7675880758807588,
      "step": 14162,
      "training_loss": 7.2701802253723145
    },
    {
      "epoch": 0.7676422764227643,
      "step": 14163,
      "training_loss": 6.276468276977539
    },
    {
      "epoch": 0.7676964769647696,
      "grad_norm": 26.738679885864258,
      "learning_rate": 1e-05,
      "loss": 6.8098,
      "step": 14164
    },
    {
      "epoch": 0.7676964769647696,
      "step": 14164,
      "training_loss": 6.812814712524414
    },
    {
      "epoch": 0.7677506775067751,
      "step": 14165,
      "training_loss": 5.4051384925842285
    },
    {
      "epoch": 0.7678048780487805,
      "step": 14166,
      "training_loss": 4.330983638763428
    },
    {
      "epoch": 0.767859078590786,
      "step": 14167,
      "training_loss": 4.909007549285889
    },
    {
      "epoch": 0.7679132791327913,
      "grad_norm": 25.658748626708984,
      "learning_rate": 1e-05,
      "loss": 5.3645,
      "step": 14168
    },
    {
      "epoch": 0.7679132791327913,
      "step": 14168,
      "training_loss": 6.842144966125488
    },
    {
      "epoch": 0.7679674796747967,
      "step": 14169,
      "training_loss": 5.59308385848999
    },
    {
      "epoch": 0.7680216802168022,
      "step": 14170,
      "training_loss": 6.207043170928955
    },
    {
      "epoch": 0.7680758807588076,
      "step": 14171,
      "training_loss": 7.141831874847412
    },
    {
      "epoch": 0.7681300813008131,
      "grad_norm": 42.75202178955078,
      "learning_rate": 1e-05,
      "loss": 6.446,
      "step": 14172
    },
    {
      "epoch": 0.7681300813008131,
      "step": 14172,
      "training_loss": 5.738685131072998
    },
    {
      "epoch": 0.7681842818428184,
      "step": 14173,
      "training_loss": 6.068263530731201
    },
    {
      "epoch": 0.7682384823848238,
      "step": 14174,
      "training_loss": 6.526552677154541
    },
    {
      "epoch": 0.7682926829268293,
      "step": 14175,
      "training_loss": 7.141958713531494
    },
    {
      "epoch": 0.7683468834688347,
      "grad_norm": 24.985824584960938,
      "learning_rate": 1e-05,
      "loss": 6.3689,
      "step": 14176
    },
    {
      "epoch": 0.7683468834688347,
      "step": 14176,
      "training_loss": 6.455960750579834
    },
    {
      "epoch": 0.7684010840108401,
      "step": 14177,
      "training_loss": 6.06718111038208
    },
    {
      "epoch": 0.7684552845528455,
      "step": 14178,
      "training_loss": 6.732807159423828
    },
    {
      "epoch": 0.768509485094851,
      "step": 14179,
      "training_loss": 5.708596706390381
    },
    {
      "epoch": 0.7685636856368564,
      "grad_norm": 40.97197341918945,
      "learning_rate": 1e-05,
      "loss": 6.2411,
      "step": 14180
    },
    {
      "epoch": 0.7685636856368564,
      "step": 14180,
      "training_loss": 7.367128849029541
    },
    {
      "epoch": 0.7686178861788618,
      "step": 14181,
      "training_loss": 6.267090320587158
    },
    {
      "epoch": 0.7686720867208672,
      "step": 14182,
      "training_loss": 6.670530796051025
    },
    {
      "epoch": 0.7687262872628726,
      "step": 14183,
      "training_loss": 6.631434917449951
    },
    {
      "epoch": 0.7687804878048781,
      "grad_norm": 17.987102508544922,
      "learning_rate": 1e-05,
      "loss": 6.734,
      "step": 14184
    },
    {
      "epoch": 0.7687804878048781,
      "step": 14184,
      "training_loss": 6.3948469161987305
    },
    {
      "epoch": 0.7688346883468835,
      "step": 14185,
      "training_loss": 6.940296649932861
    },
    {
      "epoch": 0.7688888888888888,
      "step": 14186,
      "training_loss": 6.550582408905029
    },
    {
      "epoch": 0.7689430894308943,
      "step": 14187,
      "training_loss": 5.09177827835083
    },
    {
      "epoch": 0.7689972899728997,
      "grad_norm": 34.36506271362305,
      "learning_rate": 1e-05,
      "loss": 6.2444,
      "step": 14188
    },
    {
      "epoch": 0.7689972899728997,
      "step": 14188,
      "training_loss": 7.2905073165893555
    },
    {
      "epoch": 0.7690514905149052,
      "step": 14189,
      "training_loss": 6.99187707901001
    },
    {
      "epoch": 0.7691056910569106,
      "step": 14190,
      "training_loss": 6.508481979370117
    },
    {
      "epoch": 0.769159891598916,
      "step": 14191,
      "training_loss": 7.9786152839660645
    },
    {
      "epoch": 0.7692140921409214,
      "grad_norm": 20.884807586669922,
      "learning_rate": 1e-05,
      "loss": 7.1924,
      "step": 14192
    },
    {
      "epoch": 0.7692140921409214,
      "step": 14192,
      "training_loss": 7.608321189880371
    },
    {
      "epoch": 0.7692682926829268,
      "step": 14193,
      "training_loss": 7.125832557678223
    },
    {
      "epoch": 0.7693224932249323,
      "step": 14194,
      "training_loss": 6.294050693511963
    },
    {
      "epoch": 0.7693766937669376,
      "step": 14195,
      "training_loss": 6.880215167999268
    },
    {
      "epoch": 0.7694308943089431,
      "grad_norm": 20.60654067993164,
      "learning_rate": 1e-05,
      "loss": 6.9771,
      "step": 14196
    },
    {
      "epoch": 0.7694308943089431,
      "step": 14196,
      "training_loss": 7.480037689208984
    },
    {
      "epoch": 0.7694850948509485,
      "step": 14197,
      "training_loss": 4.212759971618652
    },
    {
      "epoch": 0.769539295392954,
      "step": 14198,
      "training_loss": 6.585116386413574
    },
    {
      "epoch": 0.7695934959349594,
      "step": 14199,
      "training_loss": 6.0248637199401855
    },
    {
      "epoch": 0.7696476964769647,
      "grad_norm": 25.650163650512695,
      "learning_rate": 1e-05,
      "loss": 6.0757,
      "step": 14200
    },
    {
      "epoch": 0.7696476964769647,
      "step": 14200,
      "training_loss": 7.3346757888793945
    },
    {
      "epoch": 0.7697018970189702,
      "step": 14201,
      "training_loss": 6.450001239776611
    },
    {
      "epoch": 0.7697560975609756,
      "step": 14202,
      "training_loss": 7.740932464599609
    },
    {
      "epoch": 0.7698102981029811,
      "step": 14203,
      "training_loss": 5.984316349029541
    },
    {
      "epoch": 0.7698644986449864,
      "grad_norm": 26.967529296875,
      "learning_rate": 1e-05,
      "loss": 6.8775,
      "step": 14204
    },
    {
      "epoch": 0.7698644986449864,
      "step": 14204,
      "training_loss": 7.130738258361816
    },
    {
      "epoch": 0.7699186991869919,
      "step": 14205,
      "training_loss": 7.143777847290039
    },
    {
      "epoch": 0.7699728997289973,
      "step": 14206,
      "training_loss": 8.418580055236816
    },
    {
      "epoch": 0.7700271002710027,
      "step": 14207,
      "training_loss": 6.694815635681152
    },
    {
      "epoch": 0.7700813008130082,
      "grad_norm": 16.242626190185547,
      "learning_rate": 1e-05,
      "loss": 7.347,
      "step": 14208
    },
    {
      "epoch": 0.7700813008130082,
      "step": 14208,
      "training_loss": 7.804826736450195
    },
    {
      "epoch": 0.7701355013550135,
      "step": 14209,
      "training_loss": 7.150113582611084
    },
    {
      "epoch": 0.770189701897019,
      "step": 14210,
      "training_loss": 6.404576778411865
    },
    {
      "epoch": 0.7702439024390244,
      "step": 14211,
      "training_loss": 7.508147239685059
    },
    {
      "epoch": 0.7702981029810299,
      "grad_norm": 25.08606719970703,
      "learning_rate": 1e-05,
      "loss": 7.2169,
      "step": 14212
    },
    {
      "epoch": 0.7702981029810299,
      "step": 14212,
      "training_loss": 3.45770263671875
    },
    {
      "epoch": 0.7703523035230352,
      "step": 14213,
      "training_loss": 7.05586051940918
    },
    {
      "epoch": 0.7704065040650406,
      "step": 14214,
      "training_loss": 7.795965194702148
    },
    {
      "epoch": 0.7704607046070461,
      "step": 14215,
      "training_loss": 7.055278301239014
    },
    {
      "epoch": 0.7705149051490515,
      "grad_norm": 29.67222785949707,
      "learning_rate": 1e-05,
      "loss": 6.3412,
      "step": 14216
    },
    {
      "epoch": 0.7705149051490515,
      "step": 14216,
      "training_loss": 6.895639419555664
    },
    {
      "epoch": 0.770569105691057,
      "step": 14217,
      "training_loss": 6.721312046051025
    },
    {
      "epoch": 0.7706233062330623,
      "step": 14218,
      "training_loss": 6.9223504066467285
    },
    {
      "epoch": 0.7706775067750677,
      "step": 14219,
      "training_loss": 6.480055332183838
    },
    {
      "epoch": 0.7707317073170732,
      "grad_norm": 36.93439483642578,
      "learning_rate": 1e-05,
      "loss": 6.7548,
      "step": 14220
    },
    {
      "epoch": 0.7707317073170732,
      "step": 14220,
      "training_loss": 7.3866753578186035
    },
    {
      "epoch": 0.7707859078590786,
      "step": 14221,
      "training_loss": 7.088028430938721
    },
    {
      "epoch": 0.770840108401084,
      "step": 14222,
      "training_loss": 6.262505531311035
    },
    {
      "epoch": 0.7708943089430894,
      "step": 14223,
      "training_loss": 6.7658209800720215
    },
    {
      "epoch": 0.7709485094850949,
      "grad_norm": 20.843482971191406,
      "learning_rate": 1e-05,
      "loss": 6.8758,
      "step": 14224
    },
    {
      "epoch": 0.7709485094850949,
      "step": 14224,
      "training_loss": 7.288217544555664
    },
    {
      "epoch": 0.7710027100271003,
      "step": 14225,
      "training_loss": 6.708393096923828
    },
    {
      "epoch": 0.7710569105691056,
      "step": 14226,
      "training_loss": 7.124953746795654
    },
    {
      "epoch": 0.7711111111111111,
      "step": 14227,
      "training_loss": 6.153824329376221
    },
    {
      "epoch": 0.7711653116531165,
      "grad_norm": 61.824886322021484,
      "learning_rate": 1e-05,
      "loss": 6.8188,
      "step": 14228
    },
    {
      "epoch": 0.7711653116531165,
      "step": 14228,
      "training_loss": 7.210172653198242
    },
    {
      "epoch": 0.771219512195122,
      "step": 14229,
      "training_loss": 6.011572360992432
    },
    {
      "epoch": 0.7712737127371274,
      "step": 14230,
      "training_loss": 7.3266448974609375
    },
    {
      "epoch": 0.7713279132791327,
      "step": 14231,
      "training_loss": 6.507256031036377
    },
    {
      "epoch": 0.7713821138211382,
      "grad_norm": 16.319992065429688,
      "learning_rate": 1e-05,
      "loss": 6.7639,
      "step": 14232
    },
    {
      "epoch": 0.7713821138211382,
      "step": 14232,
      "training_loss": 7.592041015625
    },
    {
      "epoch": 0.7714363143631436,
      "step": 14233,
      "training_loss": 6.355304718017578
    },
    {
      "epoch": 0.7714905149051491,
      "step": 14234,
      "training_loss": 5.409910678863525
    },
    {
      "epoch": 0.7715447154471544,
      "step": 14235,
      "training_loss": 5.833123683929443
    },
    {
      "epoch": 0.7715989159891599,
      "grad_norm": 24.538339614868164,
      "learning_rate": 1e-05,
      "loss": 6.2976,
      "step": 14236
    },
    {
      "epoch": 0.7715989159891599,
      "step": 14236,
      "training_loss": 8.169486045837402
    },
    {
      "epoch": 0.7716531165311653,
      "step": 14237,
      "training_loss": 5.738251209259033
    },
    {
      "epoch": 0.7717073170731708,
      "step": 14238,
      "training_loss": 5.996790409088135
    },
    {
      "epoch": 0.7717615176151762,
      "step": 14239,
      "training_loss": 7.338494777679443
    },
    {
      "epoch": 0.7718157181571815,
      "grad_norm": 15.967521667480469,
      "learning_rate": 1e-05,
      "loss": 6.8108,
      "step": 14240
    },
    {
      "epoch": 0.7718157181571815,
      "step": 14240,
      "training_loss": 7.145751953125
    },
    {
      "epoch": 0.771869918699187,
      "step": 14241,
      "training_loss": 6.793009281158447
    },
    {
      "epoch": 0.7719241192411924,
      "step": 14242,
      "training_loss": 5.428171157836914
    },
    {
      "epoch": 0.7719783197831979,
      "step": 14243,
      "training_loss": 6.753776550292969
    },
    {
      "epoch": 0.7720325203252032,
      "grad_norm": 21.672565460205078,
      "learning_rate": 1e-05,
      "loss": 6.5302,
      "step": 14244
    },
    {
      "epoch": 0.7720325203252032,
      "step": 14244,
      "training_loss": 6.33998966217041
    },
    {
      "epoch": 0.7720867208672086,
      "step": 14245,
      "training_loss": 7.126984119415283
    },
    {
      "epoch": 0.7721409214092141,
      "step": 14246,
      "training_loss": 6.8658528327941895
    },
    {
      "epoch": 0.7721951219512195,
      "step": 14247,
      "training_loss": 5.5856499671936035
    },
    {
      "epoch": 0.772249322493225,
      "grad_norm": 25.39769744873047,
      "learning_rate": 1e-05,
      "loss": 6.4796,
      "step": 14248
    },
    {
      "epoch": 0.772249322493225,
      "step": 14248,
      "training_loss": 6.390636444091797
    },
    {
      "epoch": 0.7723035230352303,
      "step": 14249,
      "training_loss": 7.339292049407959
    },
    {
      "epoch": 0.7723577235772358,
      "step": 14250,
      "training_loss": 6.911353588104248
    },
    {
      "epoch": 0.7724119241192412,
      "step": 14251,
      "training_loss": 3.4040451049804688
    },
    {
      "epoch": 0.7724661246612466,
      "grad_norm": 35.41010284423828,
      "learning_rate": 1e-05,
      "loss": 6.0113,
      "step": 14252
    },
    {
      "epoch": 0.7724661246612466,
      "step": 14252,
      "training_loss": 6.5261945724487305
    },
    {
      "epoch": 0.772520325203252,
      "step": 14253,
      "training_loss": 7.424849987030029
    },
    {
      "epoch": 0.7725745257452574,
      "step": 14254,
      "training_loss": 5.824714183807373
    },
    {
      "epoch": 0.7726287262872629,
      "step": 14255,
      "training_loss": 7.083462715148926
    },
    {
      "epoch": 0.7726829268292683,
      "grad_norm": 16.066957473754883,
      "learning_rate": 1e-05,
      "loss": 6.7148,
      "step": 14256
    },
    {
      "epoch": 0.7726829268292683,
      "step": 14256,
      "training_loss": 5.77817440032959
    },
    {
      "epoch": 0.7727371273712738,
      "step": 14257,
      "training_loss": 7.277918815612793
    },
    {
      "epoch": 0.7727913279132791,
      "step": 14258,
      "training_loss": 6.959524154663086
    },
    {
      "epoch": 0.7728455284552845,
      "step": 14259,
      "training_loss": 7.687702178955078
    },
    {
      "epoch": 0.77289972899729,
      "grad_norm": 20.60348129272461,
      "learning_rate": 1e-05,
      "loss": 6.9258,
      "step": 14260
    },
    {
      "epoch": 0.77289972899729,
      "step": 14260,
      "training_loss": 7.684123992919922
    },
    {
      "epoch": 0.7729539295392954,
      "step": 14261,
      "training_loss": 4.763486385345459
    },
    {
      "epoch": 0.7730081300813008,
      "step": 14262,
      "training_loss": 6.986439228057861
    },
    {
      "epoch": 0.7730623306233062,
      "step": 14263,
      "training_loss": 6.189765930175781
    },
    {
      "epoch": 0.7731165311653116,
      "grad_norm": 28.04161262512207,
      "learning_rate": 1e-05,
      "loss": 6.406,
      "step": 14264
    },
    {
      "epoch": 0.7731165311653116,
      "step": 14264,
      "training_loss": 6.853952407836914
    },
    {
      "epoch": 0.7731707317073171,
      "step": 14265,
      "training_loss": 7.458179950714111
    },
    {
      "epoch": 0.7732249322493225,
      "step": 14266,
      "training_loss": 6.8016357421875
    },
    {
      "epoch": 0.7732791327913279,
      "step": 14267,
      "training_loss": 6.91265869140625
    },
    {
      "epoch": 0.7733333333333333,
      "grad_norm": 23.782329559326172,
      "learning_rate": 1e-05,
      "loss": 7.0066,
      "step": 14268
    },
    {
      "epoch": 0.7733333333333333,
      "step": 14268,
      "training_loss": 6.889935493469238
    },
    {
      "epoch": 0.7733875338753388,
      "step": 14269,
      "training_loss": 6.544236660003662
    },
    {
      "epoch": 0.7734417344173442,
      "step": 14270,
      "training_loss": 5.459221839904785
    },
    {
      "epoch": 0.7734959349593495,
      "step": 14271,
      "training_loss": 5.870614051818848
    },
    {
      "epoch": 0.773550135501355,
      "grad_norm": 35.27230453491211,
      "learning_rate": 1e-05,
      "loss": 6.191,
      "step": 14272
    },
    {
      "epoch": 0.773550135501355,
      "step": 14272,
      "training_loss": 7.610333442687988
    },
    {
      "epoch": 0.7736043360433604,
      "step": 14273,
      "training_loss": 6.60797119140625
    },
    {
      "epoch": 0.7736585365853659,
      "step": 14274,
      "training_loss": 6.9937896728515625
    },
    {
      "epoch": 0.7737127371273713,
      "step": 14275,
      "training_loss": 6.321829795837402
    },
    {
      "epoch": 0.7737669376693767,
      "grad_norm": 25.59886932373047,
      "learning_rate": 1e-05,
      "loss": 6.8835,
      "step": 14276
    },
    {
      "epoch": 0.7737669376693767,
      "step": 14276,
      "training_loss": 6.562289237976074
    },
    {
      "epoch": 0.7738211382113821,
      "step": 14277,
      "training_loss": 6.812058448791504
    },
    {
      "epoch": 0.7738753387533875,
      "step": 14278,
      "training_loss": 6.855379581451416
    },
    {
      "epoch": 0.773929539295393,
      "step": 14279,
      "training_loss": 6.93883752822876
    },
    {
      "epoch": 0.7739837398373983,
      "grad_norm": 45.77511215209961,
      "learning_rate": 1e-05,
      "loss": 6.7921,
      "step": 14280
    },
    {
      "epoch": 0.7739837398373983,
      "step": 14280,
      "training_loss": 6.170535087585449
    },
    {
      "epoch": 0.7740379403794038,
      "step": 14281,
      "training_loss": 4.917331218719482
    },
    {
      "epoch": 0.7740921409214092,
      "step": 14282,
      "training_loss": 6.841392993927002
    },
    {
      "epoch": 0.7741463414634147,
      "step": 14283,
      "training_loss": 6.728277683258057
    },
    {
      "epoch": 0.7742005420054201,
      "grad_norm": 21.653705596923828,
      "learning_rate": 1e-05,
      "loss": 6.1644,
      "step": 14284
    },
    {
      "epoch": 0.7742005420054201,
      "step": 14284,
      "training_loss": 6.89402961730957
    },
    {
      "epoch": 0.7742547425474254,
      "step": 14285,
      "training_loss": 6.909842014312744
    },
    {
      "epoch": 0.7743089430894309,
      "step": 14286,
      "training_loss": 7.0318708419799805
    },
    {
      "epoch": 0.7743631436314363,
      "step": 14287,
      "training_loss": 6.930917263031006
    },
    {
      "epoch": 0.7744173441734418,
      "grad_norm": 23.57418441772461,
      "learning_rate": 1e-05,
      "loss": 6.9417,
      "step": 14288
    },
    {
      "epoch": 0.7744173441734418,
      "step": 14288,
      "training_loss": 7.268758296966553
    },
    {
      "epoch": 0.7744715447154471,
      "step": 14289,
      "training_loss": 4.511993885040283
    },
    {
      "epoch": 0.7745257452574525,
      "step": 14290,
      "training_loss": 5.297642230987549
    },
    {
      "epoch": 0.774579945799458,
      "step": 14291,
      "training_loss": 5.320237636566162
    },
    {
      "epoch": 0.7746341463414634,
      "grad_norm": 28.246278762817383,
      "learning_rate": 1e-05,
      "loss": 5.5997,
      "step": 14292
    },
    {
      "epoch": 0.7746341463414634,
      "step": 14292,
      "training_loss": 6.56976318359375
    },
    {
      "epoch": 0.7746883468834689,
      "step": 14293,
      "training_loss": 7.298480987548828
    },
    {
      "epoch": 0.7747425474254742,
      "step": 14294,
      "training_loss": 3.3701205253601074
    },
    {
      "epoch": 0.7747967479674797,
      "step": 14295,
      "training_loss": 5.167491912841797
    },
    {
      "epoch": 0.7748509485094851,
      "grad_norm": 34.69474792480469,
      "learning_rate": 1e-05,
      "loss": 5.6015,
      "step": 14296
    },
    {
      "epoch": 0.7748509485094851,
      "step": 14296,
      "training_loss": 6.995337009429932
    },
    {
      "epoch": 0.7749051490514905,
      "step": 14297,
      "training_loss": 6.857529640197754
    },
    {
      "epoch": 0.7749593495934959,
      "step": 14298,
      "training_loss": 6.91099214553833
    },
    {
      "epoch": 0.7750135501355013,
      "step": 14299,
      "training_loss": 6.665431976318359
    },
    {
      "epoch": 0.7750677506775068,
      "grad_norm": 21.225709915161133,
      "learning_rate": 1e-05,
      "loss": 6.8573,
      "step": 14300
    },
    {
      "epoch": 0.7750677506775068,
      "step": 14300,
      "training_loss": 5.834742069244385
    },
    {
      "epoch": 0.7751219512195122,
      "step": 14301,
      "training_loss": 4.882172107696533
    },
    {
      "epoch": 0.7751761517615177,
      "step": 14302,
      "training_loss": 7.448410511016846
    },
    {
      "epoch": 0.775230352303523,
      "step": 14303,
      "training_loss": 6.289321422576904
    },
    {
      "epoch": 0.7752845528455284,
      "grad_norm": 24.34332847595215,
      "learning_rate": 1e-05,
      "loss": 6.1137,
      "step": 14304
    },
    {
      "epoch": 0.7752845528455284,
      "step": 14304,
      "training_loss": 7.1199493408203125
    },
    {
      "epoch": 0.7753387533875339,
      "step": 14305,
      "training_loss": 6.333043575286865
    },
    {
      "epoch": 0.7753929539295393,
      "step": 14306,
      "training_loss": 5.794047832489014
    },
    {
      "epoch": 0.7754471544715447,
      "step": 14307,
      "training_loss": 6.050507545471191
    },
    {
      "epoch": 0.7755013550135501,
      "grad_norm": 39.387081146240234,
      "learning_rate": 1e-05,
      "loss": 6.3244,
      "step": 14308
    },
    {
      "epoch": 0.7755013550135501,
      "step": 14308,
      "training_loss": 6.490266799926758
    },
    {
      "epoch": 0.7755555555555556,
      "step": 14309,
      "training_loss": 7.776483058929443
    },
    {
      "epoch": 0.775609756097561,
      "step": 14310,
      "training_loss": 4.100586891174316
    },
    {
      "epoch": 0.7756639566395664,
      "step": 14311,
      "training_loss": 7.40596342086792
    },
    {
      "epoch": 0.7757181571815718,
      "grad_norm": 25.014039993286133,
      "learning_rate": 1e-05,
      "loss": 6.4433,
      "step": 14312
    },
    {
      "epoch": 0.7757181571815718,
      "step": 14312,
      "training_loss": 5.602335453033447
    },
    {
      "epoch": 0.7757723577235772,
      "step": 14313,
      "training_loss": 5.750405311584473
    },
    {
      "epoch": 0.7758265582655827,
      "step": 14314,
      "training_loss": 6.341545104980469
    },
    {
      "epoch": 0.7758807588075881,
      "step": 14315,
      "training_loss": 6.993791580200195
    },
    {
      "epoch": 0.7759349593495934,
      "grad_norm": 37.749481201171875,
      "learning_rate": 1e-05,
      "loss": 6.172,
      "step": 14316
    },
    {
      "epoch": 0.7759349593495934,
      "step": 14316,
      "training_loss": 6.141877174377441
    },
    {
      "epoch": 0.7759891598915989,
      "step": 14317,
      "training_loss": 7.232775688171387
    },
    {
      "epoch": 0.7760433604336043,
      "step": 14318,
      "training_loss": 8.159667015075684
    },
    {
      "epoch": 0.7760975609756098,
      "step": 14319,
      "training_loss": 6.710525035858154
    },
    {
      "epoch": 0.7761517615176152,
      "grad_norm": 15.46052360534668,
      "learning_rate": 1e-05,
      "loss": 7.0612,
      "step": 14320
    },
    {
      "epoch": 0.7761517615176152,
      "step": 14320,
      "training_loss": 7.1117424964904785
    },
    {
      "epoch": 0.7762059620596206,
      "step": 14321,
      "training_loss": 4.351526737213135
    },
    {
      "epoch": 0.776260162601626,
      "step": 14322,
      "training_loss": 5.631028652191162
    },
    {
      "epoch": 0.7763143631436314,
      "step": 14323,
      "training_loss": 6.824237823486328
    },
    {
      "epoch": 0.7763685636856369,
      "grad_norm": 20.35711669921875,
      "learning_rate": 1e-05,
      "loss": 5.9796,
      "step": 14324
    },
    {
      "epoch": 0.7763685636856369,
      "step": 14324,
      "training_loss": 6.340699672698975
    },
    {
      "epoch": 0.7764227642276422,
      "step": 14325,
      "training_loss": 6.567979335784912
    },
    {
      "epoch": 0.7764769647696477,
      "step": 14326,
      "training_loss": 6.050267219543457
    },
    {
      "epoch": 0.7765311653116531,
      "step": 14327,
      "training_loss": 2.987489938735962
    },
    {
      "epoch": 0.7765853658536586,
      "grad_norm": 38.573421478271484,
      "learning_rate": 1e-05,
      "loss": 5.4866,
      "step": 14328
    },
    {
      "epoch": 0.7765853658536586,
      "step": 14328,
      "training_loss": 6.561289310455322
    },
    {
      "epoch": 0.776639566395664,
      "step": 14329,
      "training_loss": 6.2256269454956055
    },
    {
      "epoch": 0.7766937669376693,
      "step": 14330,
      "training_loss": 6.815627574920654
    },
    {
      "epoch": 0.7767479674796748,
      "step": 14331,
      "training_loss": 7.112911701202393
    },
    {
      "epoch": 0.7768021680216802,
      "grad_norm": 26.107189178466797,
      "learning_rate": 1e-05,
      "loss": 6.6789,
      "step": 14332
    },
    {
      "epoch": 0.7768021680216802,
      "step": 14332,
      "training_loss": 7.617738723754883
    },
    {
      "epoch": 0.7768563685636857,
      "step": 14333,
      "training_loss": 6.0637431144714355
    },
    {
      "epoch": 0.776910569105691,
      "step": 14334,
      "training_loss": 9.029729843139648
    },
    {
      "epoch": 0.7769647696476965,
      "step": 14335,
      "training_loss": 2.903973340988159
    },
    {
      "epoch": 0.7770189701897019,
      "grad_norm": 26.4421329498291,
      "learning_rate": 1e-05,
      "loss": 6.4038,
      "step": 14336
    },
    {
      "epoch": 0.7770189701897019,
      "step": 14336,
      "training_loss": 8.071447372436523
    },
    {
      "epoch": 0.7770731707317073,
      "step": 14337,
      "training_loss": 6.788902759552002
    },
    {
      "epoch": 0.7771273712737128,
      "step": 14338,
      "training_loss": 5.258823871612549
    },
    {
      "epoch": 0.7771815718157181,
      "step": 14339,
      "training_loss": 6.764766216278076
    },
    {
      "epoch": 0.7772357723577236,
      "grad_norm": 26.09610939025879,
      "learning_rate": 1e-05,
      "loss": 6.721,
      "step": 14340
    },
    {
      "epoch": 0.7772357723577236,
      "step": 14340,
      "training_loss": 8.05148696899414
    },
    {
      "epoch": 0.777289972899729,
      "step": 14341,
      "training_loss": 6.869882106781006
    },
    {
      "epoch": 0.7773441734417345,
      "step": 14342,
      "training_loss": 8.020652770996094
    },
    {
      "epoch": 0.7773983739837398,
      "step": 14343,
      "training_loss": 3.962958574295044
    },
    {
      "epoch": 0.7774525745257452,
      "grad_norm": 30.295425415039062,
      "learning_rate": 1e-05,
      "loss": 6.7262,
      "step": 14344
    },
    {
      "epoch": 0.7774525745257452,
      "step": 14344,
      "training_loss": 6.681334495544434
    },
    {
      "epoch": 0.7775067750677507,
      "step": 14345,
      "training_loss": 5.1034088134765625
    },
    {
      "epoch": 0.7775609756097561,
      "step": 14346,
      "training_loss": 6.873392105102539
    },
    {
      "epoch": 0.7776151761517616,
      "step": 14347,
      "training_loss": 6.945207118988037
    },
    {
      "epoch": 0.7776693766937669,
      "grad_norm": 27.013086318969727,
      "learning_rate": 1e-05,
      "loss": 6.4008,
      "step": 14348
    },
    {
      "epoch": 0.7776693766937669,
      "step": 14348,
      "training_loss": 7.37762975692749
    },
    {
      "epoch": 0.7777235772357723,
      "step": 14349,
      "training_loss": 7.24460506439209
    },
    {
      "epoch": 0.7777777777777778,
      "step": 14350,
      "training_loss": 6.516571044921875
    },
    {
      "epoch": 0.7778319783197832,
      "step": 14351,
      "training_loss": 7.405247688293457
    },
    {
      "epoch": 0.7778861788617886,
      "grad_norm": 28.43817138671875,
      "learning_rate": 1e-05,
      "loss": 7.136,
      "step": 14352
    },
    {
      "epoch": 0.7778861788617886,
      "step": 14352,
      "training_loss": 6.405068397521973
    },
    {
      "epoch": 0.777940379403794,
      "step": 14353,
      "training_loss": 6.968531608581543
    },
    {
      "epoch": 0.7779945799457995,
      "step": 14354,
      "training_loss": 6.608640193939209
    },
    {
      "epoch": 0.7780487804878049,
      "step": 14355,
      "training_loss": 5.838225841522217
    },
    {
      "epoch": 0.7781029810298103,
      "grad_norm": 30.513566970825195,
      "learning_rate": 1e-05,
      "loss": 6.4551,
      "step": 14356
    },
    {
      "epoch": 0.7781029810298103,
      "step": 14356,
      "training_loss": 7.649158000946045
    },
    {
      "epoch": 0.7781571815718157,
      "step": 14357,
      "training_loss": 5.9946088790893555
    },
    {
      "epoch": 0.7782113821138211,
      "step": 14358,
      "training_loss": 7.0822014808654785
    },
    {
      "epoch": 0.7782655826558266,
      "step": 14359,
      "training_loss": 5.329890727996826
    },
    {
      "epoch": 0.778319783197832,
      "grad_norm": 27.807960510253906,
      "learning_rate": 1e-05,
      "loss": 6.514,
      "step": 14360
    },
    {
      "epoch": 0.778319783197832,
      "step": 14360,
      "training_loss": 6.658551216125488
    },
    {
      "epoch": 0.7783739837398373,
      "step": 14361,
      "training_loss": 7.006776809692383
    },
    {
      "epoch": 0.7784281842818428,
      "step": 14362,
      "training_loss": 7.386507034301758
    },
    {
      "epoch": 0.7784823848238482,
      "step": 14363,
      "training_loss": 7.838078022003174
    },
    {
      "epoch": 0.7785365853658537,
      "grad_norm": 36.414527893066406,
      "learning_rate": 1e-05,
      "loss": 7.2225,
      "step": 14364
    },
    {
      "epoch": 0.7785365853658537,
      "step": 14364,
      "training_loss": 6.981536865234375
    },
    {
      "epoch": 0.7785907859078591,
      "step": 14365,
      "training_loss": 7.84129524230957
    },
    {
      "epoch": 0.7786449864498645,
      "step": 14366,
      "training_loss": 6.546269416809082
    },
    {
      "epoch": 0.7786991869918699,
      "step": 14367,
      "training_loss": 5.960237979888916
    },
    {
      "epoch": 0.7787533875338754,
      "grad_norm": 33.10465621948242,
      "learning_rate": 1e-05,
      "loss": 6.8323,
      "step": 14368
    },
    {
      "epoch": 0.7787533875338754,
      "step": 14368,
      "training_loss": 6.999881744384766
    },
    {
      "epoch": 0.7788075880758808,
      "step": 14369,
      "training_loss": 6.0970964431762695
    },
    {
      "epoch": 0.7788617886178861,
      "step": 14370,
      "training_loss": 6.60691499710083
    },
    {
      "epoch": 0.7789159891598916,
      "step": 14371,
      "training_loss": 2.763672113418579
    },
    {
      "epoch": 0.778970189701897,
      "grad_norm": 26.758193969726562,
      "learning_rate": 1e-05,
      "loss": 5.6169,
      "step": 14372
    },
    {
      "epoch": 0.778970189701897,
      "step": 14372,
      "training_loss": 6.888599395751953
    },
    {
      "epoch": 0.7790243902439025,
      "step": 14373,
      "training_loss": 5.956934452056885
    },
    {
      "epoch": 0.7790785907859079,
      "step": 14374,
      "training_loss": 7.433021068572998
    },
    {
      "epoch": 0.7791327913279132,
      "step": 14375,
      "training_loss": 6.008169651031494
    },
    {
      "epoch": 0.7791869918699187,
      "grad_norm": 42.40437316894531,
      "learning_rate": 1e-05,
      "loss": 6.5717,
      "step": 14376
    },
    {
      "epoch": 0.7791869918699187,
      "step": 14376,
      "training_loss": 7.523103713989258
    },
    {
      "epoch": 0.7792411924119241,
      "step": 14377,
      "training_loss": 6.312591075897217
    },
    {
      "epoch": 0.7792953929539296,
      "step": 14378,
      "training_loss": 7.345149993896484
    },
    {
      "epoch": 0.7793495934959349,
      "step": 14379,
      "training_loss": 6.371896266937256
    },
    {
      "epoch": 0.7794037940379404,
      "grad_norm": 31.962635040283203,
      "learning_rate": 1e-05,
      "loss": 6.8882,
      "step": 14380
    },
    {
      "epoch": 0.7794037940379404,
      "step": 14380,
      "training_loss": 6.489706993103027
    },
    {
      "epoch": 0.7794579945799458,
      "step": 14381,
      "training_loss": 7.32070779800415
    },
    {
      "epoch": 0.7795121951219512,
      "step": 14382,
      "training_loss": 7.412545680999756
    },
    {
      "epoch": 0.7795663956639567,
      "step": 14383,
      "training_loss": 6.243939399719238
    },
    {
      "epoch": 0.779620596205962,
      "grad_norm": 34.756507873535156,
      "learning_rate": 1e-05,
      "loss": 6.8667,
      "step": 14384
    },
    {
      "epoch": 0.779620596205962,
      "step": 14384,
      "training_loss": 6.373797416687012
    },
    {
      "epoch": 0.7796747967479675,
      "step": 14385,
      "training_loss": 5.506657123565674
    },
    {
      "epoch": 0.7797289972899729,
      "step": 14386,
      "training_loss": 7.693390846252441
    },
    {
      "epoch": 0.7797831978319784,
      "step": 14387,
      "training_loss": 6.831424713134766
    },
    {
      "epoch": 0.7798373983739837,
      "grad_norm": 19.075626373291016,
      "learning_rate": 1e-05,
      "loss": 6.6013,
      "step": 14388
    },
    {
      "epoch": 0.7798373983739837,
      "step": 14388,
      "training_loss": 8.276220321655273
    },
    {
      "epoch": 0.7798915989159891,
      "step": 14389,
      "training_loss": 4.614217758178711
    },
    {
      "epoch": 0.7799457994579946,
      "step": 14390,
      "training_loss": 7.217648983001709
    },
    {
      "epoch": 0.78,
      "step": 14391,
      "training_loss": 6.259721755981445
    },
    {
      "epoch": 0.7800542005420055,
      "grad_norm": 39.80931091308594,
      "learning_rate": 1e-05,
      "loss": 6.592,
      "step": 14392
    },
    {
      "epoch": 0.7800542005420055,
      "step": 14392,
      "training_loss": 6.3338518142700195
    },
    {
      "epoch": 0.7801084010840108,
      "step": 14393,
      "training_loss": 6.356760501861572
    },
    {
      "epoch": 0.7801626016260162,
      "step": 14394,
      "training_loss": 7.73838996887207
    },
    {
      "epoch": 0.7802168021680217,
      "step": 14395,
      "training_loss": 6.561626434326172
    },
    {
      "epoch": 0.7802710027100271,
      "grad_norm": 29.19744873046875,
      "learning_rate": 1e-05,
      "loss": 6.7477,
      "step": 14396
    },
    {
      "epoch": 0.7802710027100271,
      "step": 14396,
      "training_loss": 5.8661346435546875
    },
    {
      "epoch": 0.7803252032520325,
      "step": 14397,
      "training_loss": 6.307051658630371
    },
    {
      "epoch": 0.7803794037940379,
      "step": 14398,
      "training_loss": 6.846360206604004
    },
    {
      "epoch": 0.7804336043360434,
      "step": 14399,
      "training_loss": 6.900991439819336
    },
    {
      "epoch": 0.7804878048780488,
      "grad_norm": 18.61302375793457,
      "learning_rate": 1e-05,
      "loss": 6.4801,
      "step": 14400
    },
    {
      "epoch": 0.7804878048780488,
      "step": 14400,
      "training_loss": 7.3661370277404785
    },
    {
      "epoch": 0.7805420054200543,
      "step": 14401,
      "training_loss": 5.997134208679199
    },
    {
      "epoch": 0.7805962059620596,
      "step": 14402,
      "training_loss": 7.460421085357666
    },
    {
      "epoch": 0.780650406504065,
      "step": 14403,
      "training_loss": 8.528512001037598
    },
    {
      "epoch": 0.7807046070460705,
      "grad_norm": 49.68987274169922,
      "learning_rate": 1e-05,
      "loss": 7.3381,
      "step": 14404
    },
    {
      "epoch": 0.7807046070460705,
      "step": 14404,
      "training_loss": 7.158822536468506
    },
    {
      "epoch": 0.7807588075880759,
      "step": 14405,
      "training_loss": 6.521795749664307
    },
    {
      "epoch": 0.7808130081300813,
      "step": 14406,
      "training_loss": 7.018181324005127
    },
    {
      "epoch": 0.7808672086720867,
      "step": 14407,
      "training_loss": 7.420596599578857
    },
    {
      "epoch": 0.7809214092140921,
      "grad_norm": 21.365184783935547,
      "learning_rate": 1e-05,
      "loss": 7.0298,
      "step": 14408
    },
    {
      "epoch": 0.7809214092140921,
      "step": 14408,
      "training_loss": 5.710709571838379
    },
    {
      "epoch": 0.7809756097560976,
      "step": 14409,
      "training_loss": 6.91843843460083
    },
    {
      "epoch": 0.781029810298103,
      "step": 14410,
      "training_loss": 5.7956156730651855
    },
    {
      "epoch": 0.7810840108401084,
      "step": 14411,
      "training_loss": 6.867886066436768
    },
    {
      "epoch": 0.7811382113821138,
      "grad_norm": 17.207530975341797,
      "learning_rate": 1e-05,
      "loss": 6.3232,
      "step": 14412
    },
    {
      "epoch": 0.7811382113821138,
      "step": 14412,
      "training_loss": 7.055874824523926
    },
    {
      "epoch": 0.7811924119241193,
      "step": 14413,
      "training_loss": 4.503360748291016
    },
    {
      "epoch": 0.7812466124661247,
      "step": 14414,
      "training_loss": 6.719634532928467
    },
    {
      "epoch": 0.78130081300813,
      "step": 14415,
      "training_loss": 6.365655899047852
    },
    {
      "epoch": 0.7813550135501355,
      "grad_norm": 34.30807876586914,
      "learning_rate": 1e-05,
      "loss": 6.1611,
      "step": 14416
    },
    {
      "epoch": 0.7813550135501355,
      "step": 14416,
      "training_loss": 5.50300931930542
    },
    {
      "epoch": 0.7814092140921409,
      "step": 14417,
      "training_loss": 7.503997325897217
    },
    {
      "epoch": 0.7814634146341464,
      "step": 14418,
      "training_loss": 7.397432804107666
    },
    {
      "epoch": 0.7815176151761518,
      "step": 14419,
      "training_loss": 8.60901927947998
    },
    {
      "epoch": 0.7815718157181571,
      "grad_norm": 41.01483917236328,
      "learning_rate": 1e-05,
      "loss": 7.2534,
      "step": 14420
    },
    {
      "epoch": 0.7815718157181571,
      "step": 14420,
      "training_loss": 7.763779163360596
    },
    {
      "epoch": 0.7816260162601626,
      "step": 14421,
      "training_loss": 7.835779666900635
    },
    {
      "epoch": 0.781680216802168,
      "step": 14422,
      "training_loss": 6.275638580322266
    },
    {
      "epoch": 0.7817344173441735,
      "step": 14423,
      "training_loss": 7.5642924308776855
    },
    {
      "epoch": 0.7817886178861788,
      "grad_norm": 20.93152618408203,
      "learning_rate": 1e-05,
      "loss": 7.3599,
      "step": 14424
    },
    {
      "epoch": 0.7817886178861788,
      "step": 14424,
      "training_loss": 9.444464683532715
    },
    {
      "epoch": 0.7818428184281843,
      "step": 14425,
      "training_loss": 6.91551399230957
    },
    {
      "epoch": 0.7818970189701897,
      "step": 14426,
      "training_loss": 7.1579179763793945
    },
    {
      "epoch": 0.7819512195121952,
      "step": 14427,
      "training_loss": 6.949189186096191
    },
    {
      "epoch": 0.7820054200542006,
      "grad_norm": 22.53297996520996,
      "learning_rate": 1e-05,
      "loss": 7.6168,
      "step": 14428
    },
    {
      "epoch": 0.7820054200542006,
      "step": 14428,
      "training_loss": 5.929715633392334
    },
    {
      "epoch": 0.7820596205962059,
      "step": 14429,
      "training_loss": 7.2019500732421875
    },
    {
      "epoch": 0.7821138211382114,
      "step": 14430,
      "training_loss": 4.754943370819092
    },
    {
      "epoch": 0.7821680216802168,
      "step": 14431,
      "training_loss": 5.605157852172852
    },
    {
      "epoch": 0.7822222222222223,
      "grad_norm": 24.429983139038086,
      "learning_rate": 1e-05,
      "loss": 5.8729,
      "step": 14432
    },
    {
      "epoch": 0.7822222222222223,
      "step": 14432,
      "training_loss": 5.692389011383057
    },
    {
      "epoch": 0.7822764227642276,
      "step": 14433,
      "training_loss": 8.072822570800781
    },
    {
      "epoch": 0.782330623306233,
      "step": 14434,
      "training_loss": 5.334385871887207
    },
    {
      "epoch": 0.7823848238482385,
      "step": 14435,
      "training_loss": 8.663579940795898
    },
    {
      "epoch": 0.7824390243902439,
      "grad_norm": 40.535160064697266,
      "learning_rate": 1e-05,
      "loss": 6.9408,
      "step": 14436
    },
    {
      "epoch": 0.7824390243902439,
      "step": 14436,
      "training_loss": 6.398413181304932
    },
    {
      "epoch": 0.7824932249322494,
      "step": 14437,
      "training_loss": 5.679758548736572
    },
    {
      "epoch": 0.7825474254742547,
      "step": 14438,
      "training_loss": 6.152939319610596
    },
    {
      "epoch": 0.7826016260162602,
      "step": 14439,
      "training_loss": 5.9967041015625
    },
    {
      "epoch": 0.7826558265582656,
      "grad_norm": 20.5042667388916,
      "learning_rate": 1e-05,
      "loss": 6.057,
      "step": 14440
    },
    {
      "epoch": 0.7826558265582656,
      "step": 14440,
      "training_loss": 6.395290374755859
    },
    {
      "epoch": 0.782710027100271,
      "step": 14441,
      "training_loss": 6.744246482849121
    },
    {
      "epoch": 0.7827642276422764,
      "step": 14442,
      "training_loss": 7.007846355438232
    },
    {
      "epoch": 0.7828184281842818,
      "step": 14443,
      "training_loss": 6.167369365692139
    },
    {
      "epoch": 0.7828726287262873,
      "grad_norm": 26.518625259399414,
      "learning_rate": 1e-05,
      "loss": 6.5787,
      "step": 14444
    },
    {
      "epoch": 0.7828726287262873,
      "step": 14444,
      "training_loss": 7.6557536125183105
    },
    {
      "epoch": 0.7829268292682927,
      "step": 14445,
      "training_loss": 7.438044548034668
    },
    {
      "epoch": 0.7829810298102982,
      "step": 14446,
      "training_loss": 4.97676944732666
    },
    {
      "epoch": 0.7830352303523035,
      "step": 14447,
      "training_loss": 6.607637405395508
    },
    {
      "epoch": 0.7830894308943089,
      "grad_norm": 36.49319076538086,
      "learning_rate": 1e-05,
      "loss": 6.6696,
      "step": 14448
    },
    {
      "epoch": 0.7830894308943089,
      "step": 14448,
      "training_loss": 6.533691883087158
    },
    {
      "epoch": 0.7831436314363144,
      "step": 14449,
      "training_loss": 3.2076220512390137
    },
    {
      "epoch": 0.7831978319783198,
      "step": 14450,
      "training_loss": 4.255812168121338
    },
    {
      "epoch": 0.7832520325203252,
      "step": 14451,
      "training_loss": 6.682360649108887
    },
    {
      "epoch": 0.7833062330623306,
      "grad_norm": 17.437564849853516,
      "learning_rate": 1e-05,
      "loss": 5.1699,
      "step": 14452
    },
    {
      "epoch": 0.7833062330623306,
      "step": 14452,
      "training_loss": 5.548084735870361
    },
    {
      "epoch": 0.783360433604336,
      "step": 14453,
      "training_loss": 7.621517181396484
    },
    {
      "epoch": 0.7834146341463415,
      "step": 14454,
      "training_loss": 6.340348243713379
    },
    {
      "epoch": 0.7834688346883469,
      "step": 14455,
      "training_loss": 3.751662015914917
    },
    {
      "epoch": 0.7835230352303523,
      "grad_norm": 25.76944923400879,
      "learning_rate": 1e-05,
      "loss": 5.8154,
      "step": 14456
    },
    {
      "epoch": 0.7835230352303523,
      "step": 14456,
      "training_loss": 7.357730865478516
    },
    {
      "epoch": 0.7835772357723577,
      "step": 14457,
      "training_loss": 7.18194580078125
    },
    {
      "epoch": 0.7836314363143632,
      "step": 14458,
      "training_loss": 6.0895819664001465
    },
    {
      "epoch": 0.7836856368563686,
      "step": 14459,
      "training_loss": 3.1964409351348877
    },
    {
      "epoch": 0.7837398373983739,
      "grad_norm": 28.818382263183594,
      "learning_rate": 1e-05,
      "loss": 5.9564,
      "step": 14460
    },
    {
      "epoch": 0.7837398373983739,
      "step": 14460,
      "training_loss": 6.376500129699707
    },
    {
      "epoch": 0.7837940379403794,
      "step": 14461,
      "training_loss": 7.247367858886719
    },
    {
      "epoch": 0.7838482384823848,
      "step": 14462,
      "training_loss": 6.760957717895508
    },
    {
      "epoch": 0.7839024390243903,
      "step": 14463,
      "training_loss": 6.195766925811768
    },
    {
      "epoch": 0.7839566395663957,
      "grad_norm": 23.967084884643555,
      "learning_rate": 1e-05,
      "loss": 6.6451,
      "step": 14464
    },
    {
      "epoch": 0.7839566395663957,
      "step": 14464,
      "training_loss": 6.59794282913208
    },
    {
      "epoch": 0.784010840108401,
      "step": 14465,
      "training_loss": 6.104202747344971
    },
    {
      "epoch": 0.7840650406504065,
      "step": 14466,
      "training_loss": 7.97300910949707
    },
    {
      "epoch": 0.7841192411924119,
      "step": 14467,
      "training_loss": 6.712643146514893
    },
    {
      "epoch": 0.7841734417344174,
      "grad_norm": 17.390806198120117,
      "learning_rate": 1e-05,
      "loss": 6.8469,
      "step": 14468
    },
    {
      "epoch": 0.7841734417344174,
      "step": 14468,
      "training_loss": 7.370716094970703
    },
    {
      "epoch": 0.7842276422764227,
      "step": 14469,
      "training_loss": 8.39757251739502
    },
    {
      "epoch": 0.7842818428184282,
      "step": 14470,
      "training_loss": 6.504076957702637
    },
    {
      "epoch": 0.7843360433604336,
      "step": 14471,
      "training_loss": 6.369429588317871
    },
    {
      "epoch": 0.784390243902439,
      "grad_norm": 26.05730438232422,
      "learning_rate": 1e-05,
      "loss": 7.1604,
      "step": 14472
    },
    {
      "epoch": 0.784390243902439,
      "step": 14472,
      "training_loss": 6.307021617889404
    },
    {
      "epoch": 0.7844444444444445,
      "step": 14473,
      "training_loss": 7.577832221984863
    },
    {
      "epoch": 0.7844986449864498,
      "step": 14474,
      "training_loss": 6.936716556549072
    },
    {
      "epoch": 0.7845528455284553,
      "step": 14475,
      "training_loss": 5.313413143157959
    },
    {
      "epoch": 0.7846070460704607,
      "grad_norm": 32.064762115478516,
      "learning_rate": 1e-05,
      "loss": 6.5337,
      "step": 14476
    },
    {
      "epoch": 0.7846070460704607,
      "step": 14476,
      "training_loss": 7.225763320922852
    },
    {
      "epoch": 0.7846612466124662,
      "step": 14477,
      "training_loss": 7.235192775726318
    },
    {
      "epoch": 0.7847154471544715,
      "step": 14478,
      "training_loss": 8.01972770690918
    },
    {
      "epoch": 0.784769647696477,
      "step": 14479,
      "training_loss": 8.07790470123291
    },
    {
      "epoch": 0.7848238482384824,
      "grad_norm": 43.19834518432617,
      "learning_rate": 1e-05,
      "loss": 7.6396,
      "step": 14480
    },
    {
      "epoch": 0.7848238482384824,
      "step": 14480,
      "training_loss": 6.6071953773498535
    },
    {
      "epoch": 0.7848780487804878,
      "step": 14481,
      "training_loss": 5.893642425537109
    },
    {
      "epoch": 0.7849322493224932,
      "step": 14482,
      "training_loss": 6.554479122161865
    },
    {
      "epoch": 0.7849864498644986,
      "step": 14483,
      "training_loss": 6.499736309051514
    },
    {
      "epoch": 0.7850406504065041,
      "grad_norm": 46.551265716552734,
      "learning_rate": 1e-05,
      "loss": 6.3888,
      "step": 14484
    },
    {
      "epoch": 0.7850406504065041,
      "step": 14484,
      "training_loss": 7.238399982452393
    },
    {
      "epoch": 0.7850948509485095,
      "step": 14485,
      "training_loss": 7.746628761291504
    },
    {
      "epoch": 0.785149051490515,
      "step": 14486,
      "training_loss": 2.953827142715454
    },
    {
      "epoch": 0.7852032520325203,
      "step": 14487,
      "training_loss": 5.901301860809326
    },
    {
      "epoch": 0.7852574525745257,
      "grad_norm": 42.529788970947266,
      "learning_rate": 1e-05,
      "loss": 5.96,
      "step": 14488
    },
    {
      "epoch": 0.7852574525745257,
      "step": 14488,
      "training_loss": 6.229114532470703
    },
    {
      "epoch": 0.7853116531165312,
      "step": 14489,
      "training_loss": 6.9227423667907715
    },
    {
      "epoch": 0.7853658536585366,
      "step": 14490,
      "training_loss": 4.7746357917785645
    },
    {
      "epoch": 0.785420054200542,
      "step": 14491,
      "training_loss": 6.897033214569092
    },
    {
      "epoch": 0.7854742547425474,
      "grad_norm": 48.04598617553711,
      "learning_rate": 1e-05,
      "loss": 6.2059,
      "step": 14492
    },
    {
      "epoch": 0.7854742547425474,
      "step": 14492,
      "training_loss": 3.1357476711273193
    },
    {
      "epoch": 0.7855284552845528,
      "step": 14493,
      "training_loss": 5.758394718170166
    },
    {
      "epoch": 0.7855826558265583,
      "step": 14494,
      "training_loss": 6.825298309326172
    },
    {
      "epoch": 0.7856368563685637,
      "step": 14495,
      "training_loss": 5.654251575469971
    },
    {
      "epoch": 0.7856910569105691,
      "grad_norm": 24.905376434326172,
      "learning_rate": 1e-05,
      "loss": 5.3434,
      "step": 14496
    },
    {
      "epoch": 0.7856910569105691,
      "step": 14496,
      "training_loss": 7.553620338439941
    },
    {
      "epoch": 0.7857452574525745,
      "step": 14497,
      "training_loss": 4.624107837677002
    },
    {
      "epoch": 0.78579945799458,
      "step": 14498,
      "training_loss": 8.733394622802734
    },
    {
      "epoch": 0.7858536585365854,
      "step": 14499,
      "training_loss": 7.253145694732666
    },
    {
      "epoch": 0.7859078590785907,
      "grad_norm": 18.292644500732422,
      "learning_rate": 1e-05,
      "loss": 7.0411,
      "step": 14500
    },
    {
      "epoch": 0.7859078590785907,
      "step": 14500,
      "training_loss": 8.948019981384277
    },
    {
      "epoch": 0.7859620596205962,
      "step": 14501,
      "training_loss": 8.091054916381836
    },
    {
      "epoch": 0.7860162601626016,
      "step": 14502,
      "training_loss": 5.796552658081055
    },
    {
      "epoch": 0.7860704607046071,
      "step": 14503,
      "training_loss": 5.160411834716797
    },
    {
      "epoch": 0.7861246612466125,
      "grad_norm": 33.46632766723633,
      "learning_rate": 1e-05,
      "loss": 6.999,
      "step": 14504
    },
    {
      "epoch": 0.7861246612466125,
      "step": 14504,
      "training_loss": 6.97645378112793
    },
    {
      "epoch": 0.7861788617886178,
      "step": 14505,
      "training_loss": 6.394221782684326
    },
    {
      "epoch": 0.7862330623306233,
      "step": 14506,
      "training_loss": 3.220845937728882
    },
    {
      "epoch": 0.7862872628726287,
      "step": 14507,
      "training_loss": 6.478662014007568
    },
    {
      "epoch": 0.7863414634146342,
      "grad_norm": 21.23566246032715,
      "learning_rate": 1e-05,
      "loss": 5.7675,
      "step": 14508
    },
    {
      "epoch": 0.7863414634146342,
      "step": 14508,
      "training_loss": 5.98748779296875
    },
    {
      "epoch": 0.7863956639566395,
      "step": 14509,
      "training_loss": 6.209656715393066
    },
    {
      "epoch": 0.786449864498645,
      "step": 14510,
      "training_loss": 5.576611042022705
    },
    {
      "epoch": 0.7865040650406504,
      "step": 14511,
      "training_loss": 6.149647235870361
    },
    {
      "epoch": 0.7865582655826558,
      "grad_norm": 27.27703285217285,
      "learning_rate": 1e-05,
      "loss": 5.9809,
      "step": 14512
    },
    {
      "epoch": 0.7865582655826558,
      "step": 14512,
      "training_loss": 7.395842552185059
    },
    {
      "epoch": 0.7866124661246613,
      "step": 14513,
      "training_loss": 5.676008701324463
    },
    {
      "epoch": 0.7866666666666666,
      "step": 14514,
      "training_loss": 5.060646057128906
    },
    {
      "epoch": 0.7867208672086721,
      "step": 14515,
      "training_loss": 7.3726630210876465
    },
    {
      "epoch": 0.7867750677506775,
      "grad_norm": 39.636199951171875,
      "learning_rate": 1e-05,
      "loss": 6.3763,
      "step": 14516
    },
    {
      "epoch": 0.7867750677506775,
      "step": 14516,
      "training_loss": 7.617568492889404
    },
    {
      "epoch": 0.786829268292683,
      "step": 14517,
      "training_loss": 7.164665699005127
    },
    {
      "epoch": 0.7868834688346883,
      "step": 14518,
      "training_loss": 5.207503795623779
    },
    {
      "epoch": 0.7869376693766937,
      "step": 14519,
      "training_loss": 7.464993953704834
    },
    {
      "epoch": 0.7869918699186992,
      "grad_norm": 23.401647567749023,
      "learning_rate": 1e-05,
      "loss": 6.8637,
      "step": 14520
    },
    {
      "epoch": 0.7869918699186992,
      "step": 14520,
      "training_loss": 7.432423114776611
    },
    {
      "epoch": 0.7870460704607046,
      "step": 14521,
      "training_loss": 7.9629950523376465
    },
    {
      "epoch": 0.7871002710027101,
      "step": 14522,
      "training_loss": 4.121337890625
    },
    {
      "epoch": 0.7871544715447154,
      "step": 14523,
      "training_loss": 8.457740783691406
    },
    {
      "epoch": 0.7872086720867209,
      "grad_norm": 60.176753997802734,
      "learning_rate": 1e-05,
      "loss": 6.9936,
      "step": 14524
    },
    {
      "epoch": 0.7872086720867209,
      "step": 14524,
      "training_loss": 7.070671081542969
    },
    {
      "epoch": 0.7872628726287263,
      "step": 14525,
      "training_loss": 5.675931930541992
    },
    {
      "epoch": 0.7873170731707317,
      "step": 14526,
      "training_loss": 6.702131748199463
    },
    {
      "epoch": 0.7873712737127371,
      "step": 14527,
      "training_loss": 7.113663196563721
    },
    {
      "epoch": 0.7874254742547425,
      "grad_norm": 32.56112289428711,
      "learning_rate": 1e-05,
      "loss": 6.6406,
      "step": 14528
    },
    {
      "epoch": 0.7874254742547425,
      "step": 14528,
      "training_loss": 5.798037052154541
    },
    {
      "epoch": 0.787479674796748,
      "step": 14529,
      "training_loss": 6.836206912994385
    },
    {
      "epoch": 0.7875338753387534,
      "step": 14530,
      "training_loss": 5.367105960845947
    },
    {
      "epoch": 0.7875880758807589,
      "step": 14531,
      "training_loss": 7.126129627227783
    },
    {
      "epoch": 0.7876422764227642,
      "grad_norm": 19.459651947021484,
      "learning_rate": 1e-05,
      "loss": 6.2819,
      "step": 14532
    },
    {
      "epoch": 0.7876422764227642,
      "step": 14532,
      "training_loss": 3.300469398498535
    },
    {
      "epoch": 0.7876964769647696,
      "step": 14533,
      "training_loss": 6.33294153213501
    },
    {
      "epoch": 0.7877506775067751,
      "step": 14534,
      "training_loss": 6.933597564697266
    },
    {
      "epoch": 0.7878048780487805,
      "step": 14535,
      "training_loss": 6.601999759674072
    },
    {
      "epoch": 0.7878590785907859,
      "grad_norm": 29.078310012817383,
      "learning_rate": 1e-05,
      "loss": 5.7923,
      "step": 14536
    },
    {
      "epoch": 0.7878590785907859,
      "step": 14536,
      "training_loss": 5.967945098876953
    },
    {
      "epoch": 0.7879132791327913,
      "step": 14537,
      "training_loss": 6.8226094245910645
    },
    {
      "epoch": 0.7879674796747967,
      "step": 14538,
      "training_loss": 6.991813659667969
    },
    {
      "epoch": 0.7880216802168022,
      "step": 14539,
      "training_loss": 7.35308313369751
    },
    {
      "epoch": 0.7880758807588076,
      "grad_norm": 67.85003662109375,
      "learning_rate": 1e-05,
      "loss": 6.7839,
      "step": 14540
    },
    {
      "epoch": 0.7880758807588076,
      "step": 14540,
      "training_loss": 6.803962707519531
    },
    {
      "epoch": 0.788130081300813,
      "step": 14541,
      "training_loss": 6.941981315612793
    },
    {
      "epoch": 0.7881842818428184,
      "step": 14542,
      "training_loss": 6.5099616050720215
    },
    {
      "epoch": 0.7882384823848239,
      "step": 14543,
      "training_loss": 5.859531402587891
    },
    {
      "epoch": 0.7882926829268293,
      "grad_norm": 30.20913314819336,
      "learning_rate": 1e-05,
      "loss": 6.5289,
      "step": 14544
    },
    {
      "epoch": 0.7882926829268293,
      "step": 14544,
      "training_loss": 5.560628890991211
    },
    {
      "epoch": 0.7883468834688346,
      "step": 14545,
      "training_loss": 5.565546989440918
    },
    {
      "epoch": 0.7884010840108401,
      "step": 14546,
      "training_loss": 7.825389385223389
    },
    {
      "epoch": 0.7884552845528455,
      "step": 14547,
      "training_loss": 8.18223762512207
    },
    {
      "epoch": 0.788509485094851,
      "grad_norm": 36.428226470947266,
      "learning_rate": 1e-05,
      "loss": 6.7835,
      "step": 14548
    },
    {
      "epoch": 0.788509485094851,
      "step": 14548,
      "training_loss": 5.764391899108887
    },
    {
      "epoch": 0.7885636856368564,
      "step": 14549,
      "training_loss": 6.44101619720459
    },
    {
      "epoch": 0.7886178861788617,
      "step": 14550,
      "training_loss": 2.755058765411377
    },
    {
      "epoch": 0.7886720867208672,
      "step": 14551,
      "training_loss": 7.364807605743408
    },
    {
      "epoch": 0.7887262872628726,
      "grad_norm": 46.641014099121094,
      "learning_rate": 1e-05,
      "loss": 5.5813,
      "step": 14552
    },
    {
      "epoch": 0.7887262872628726,
      "step": 14552,
      "training_loss": 4.590293884277344
    },
    {
      "epoch": 0.7887804878048781,
      "step": 14553,
      "training_loss": 7.383373737335205
    },
    {
      "epoch": 0.7888346883468834,
      "step": 14554,
      "training_loss": 7.530219554901123
    },
    {
      "epoch": 0.7888888888888889,
      "step": 14555,
      "training_loss": 9.418628692626953
    },
    {
      "epoch": 0.7889430894308943,
      "grad_norm": 42.50641632080078,
      "learning_rate": 1e-05,
      "loss": 7.2306,
      "step": 14556
    },
    {
      "epoch": 0.7889430894308943,
      "step": 14556,
      "training_loss": 5.781424522399902
    },
    {
      "epoch": 0.7889972899728998,
      "step": 14557,
      "training_loss": 4.152071952819824
    },
    {
      "epoch": 0.7890514905149052,
      "step": 14558,
      "training_loss": 5.840338706970215
    },
    {
      "epoch": 0.7891056910569105,
      "step": 14559,
      "training_loss": 7.088809490203857
    },
    {
      "epoch": 0.789159891598916,
      "grad_norm": 27.427188873291016,
      "learning_rate": 1e-05,
      "loss": 5.7157,
      "step": 14560
    },
    {
      "epoch": 0.789159891598916,
      "step": 14560,
      "training_loss": 6.498135089874268
    },
    {
      "epoch": 0.7892140921409214,
      "step": 14561,
      "training_loss": 6.653378963470459
    },
    {
      "epoch": 0.7892682926829269,
      "step": 14562,
      "training_loss": 6.5868659019470215
    },
    {
      "epoch": 0.7893224932249322,
      "step": 14563,
      "training_loss": 7.337128162384033
    },
    {
      "epoch": 0.7893766937669376,
      "grad_norm": 26.503902435302734,
      "learning_rate": 1e-05,
      "loss": 6.7689,
      "step": 14564
    },
    {
      "epoch": 0.7893766937669376,
      "step": 14564,
      "training_loss": 6.722975254058838
    },
    {
      "epoch": 0.7894308943089431,
      "step": 14565,
      "training_loss": 6.697712421417236
    },
    {
      "epoch": 0.7894850948509485,
      "step": 14566,
      "training_loss": 7.370355129241943
    },
    {
      "epoch": 0.789539295392954,
      "step": 14567,
      "training_loss": 6.473635196685791
    },
    {
      "epoch": 0.7895934959349593,
      "grad_norm": 33.69749069213867,
      "learning_rate": 1e-05,
      "loss": 6.8162,
      "step": 14568
    },
    {
      "epoch": 0.7895934959349593,
      "step": 14568,
      "training_loss": 7.130485534667969
    },
    {
      "epoch": 0.7896476964769648,
      "step": 14569,
      "training_loss": 6.875607013702393
    },
    {
      "epoch": 0.7897018970189702,
      "step": 14570,
      "training_loss": 4.770392894744873
    },
    {
      "epoch": 0.7897560975609756,
      "step": 14571,
      "training_loss": 6.891145706176758
    },
    {
      "epoch": 0.789810298102981,
      "grad_norm": 28.161090850830078,
      "learning_rate": 1e-05,
      "loss": 6.4169,
      "step": 14572
    },
    {
      "epoch": 0.789810298102981,
      "step": 14572,
      "training_loss": 3.8723106384277344
    },
    {
      "epoch": 0.7898644986449864,
      "step": 14573,
      "training_loss": 3.6300458908081055
    },
    {
      "epoch": 0.7899186991869919,
      "step": 14574,
      "training_loss": 8.94128131866455
    },
    {
      "epoch": 0.7899728997289973,
      "step": 14575,
      "training_loss": 5.33225154876709
    },
    {
      "epoch": 0.7900271002710028,
      "grad_norm": 30.460134506225586,
      "learning_rate": 1e-05,
      "loss": 5.444,
      "step": 14576
    },
    {
      "epoch": 0.7900271002710028,
      "step": 14576,
      "training_loss": 8.176098823547363
    },
    {
      "epoch": 0.7900813008130081,
      "step": 14577,
      "training_loss": 4.517247676849365
    },
    {
      "epoch": 0.7901355013550135,
      "step": 14578,
      "training_loss": 6.7505269050598145
    },
    {
      "epoch": 0.790189701897019,
      "step": 14579,
      "training_loss": 6.810462474822998
    },
    {
      "epoch": 0.7902439024390244,
      "grad_norm": 27.8685359954834,
      "learning_rate": 1e-05,
      "loss": 6.5636,
      "step": 14580
    },
    {
      "epoch": 0.7902439024390244,
      "step": 14580,
      "training_loss": 6.4145917892456055
    },
    {
      "epoch": 0.7902981029810298,
      "step": 14581,
      "training_loss": 6.047850608825684
    },
    {
      "epoch": 0.7903523035230352,
      "step": 14582,
      "training_loss": 6.118043422698975
    },
    {
      "epoch": 0.7904065040650406,
      "step": 14583,
      "training_loss": 7.290329456329346
    },
    {
      "epoch": 0.7904607046070461,
      "grad_norm": 33.56235885620117,
      "learning_rate": 1e-05,
      "loss": 6.4677,
      "step": 14584
    },
    {
      "epoch": 0.7904607046070461,
      "step": 14584,
      "training_loss": 5.157307147979736
    },
    {
      "epoch": 0.7905149051490515,
      "step": 14585,
      "training_loss": 6.374038219451904
    },
    {
      "epoch": 0.7905691056910569,
      "step": 14586,
      "training_loss": 5.065328598022461
    },
    {
      "epoch": 0.7906233062330623,
      "step": 14587,
      "training_loss": 7.282721042633057
    },
    {
      "epoch": 0.7906775067750678,
      "grad_norm": 27.87748908996582,
      "learning_rate": 1e-05,
      "loss": 5.9698,
      "step": 14588
    },
    {
      "epoch": 0.7906775067750678,
      "step": 14588,
      "training_loss": 6.406839847564697
    },
    {
      "epoch": 0.7907317073170732,
      "step": 14589,
      "training_loss": 7.113072872161865
    },
    {
      "epoch": 0.7907859078590785,
      "step": 14590,
      "training_loss": 6.282131195068359
    },
    {
      "epoch": 0.790840108401084,
      "step": 14591,
      "training_loss": 6.8504252433776855
    },
    {
      "epoch": 0.7908943089430894,
      "grad_norm": 17.528079986572266,
      "learning_rate": 1e-05,
      "loss": 6.6631,
      "step": 14592
    },
    {
      "epoch": 0.7908943089430894,
      "step": 14592,
      "training_loss": 4.697473049163818
    },
    {
      "epoch": 0.7909485094850949,
      "step": 14593,
      "training_loss": 6.73521089553833
    },
    {
      "epoch": 0.7910027100271003,
      "step": 14594,
      "training_loss": 6.009973049163818
    },
    {
      "epoch": 0.7910569105691057,
      "step": 14595,
      "training_loss": 7.207578659057617
    },
    {
      "epoch": 0.7911111111111111,
      "grad_norm": 15.621273040771484,
      "learning_rate": 1e-05,
      "loss": 6.1626,
      "step": 14596
    },
    {
      "epoch": 0.7911111111111111,
      "step": 14596,
      "training_loss": 7.188266277313232
    },
    {
      "epoch": 0.7911653116531165,
      "step": 14597,
      "training_loss": 7.023224353790283
    },
    {
      "epoch": 0.791219512195122,
      "step": 14598,
      "training_loss": 6.349011421203613
    },
    {
      "epoch": 0.7912737127371273,
      "step": 14599,
      "training_loss": 5.72987699508667
    },
    {
      "epoch": 0.7913279132791328,
      "grad_norm": 25.9553165435791,
      "learning_rate": 1e-05,
      "loss": 6.5726,
      "step": 14600
    },
    {
      "epoch": 0.7913279132791328,
      "step": 14600,
      "training_loss": 6.434243679046631
    },
    {
      "epoch": 0.7913821138211382,
      "step": 14601,
      "training_loss": 7.50741720199585
    },
    {
      "epoch": 0.7914363143631437,
      "step": 14602,
      "training_loss": 4.549049377441406
    },
    {
      "epoch": 0.7914905149051491,
      "step": 14603,
      "training_loss": 2.8201377391815186
    },
    {
      "epoch": 0.7915447154471544,
      "grad_norm": 28.307586669921875,
      "learning_rate": 1e-05,
      "loss": 5.3277,
      "step": 14604
    },
    {
      "epoch": 0.7915447154471544,
      "step": 14604,
      "training_loss": 5.899152755737305
    },
    {
      "epoch": 0.7915989159891599,
      "step": 14605,
      "training_loss": 7.079234600067139
    },
    {
      "epoch": 0.7916531165311653,
      "step": 14606,
      "training_loss": 6.891788482666016
    },
    {
      "epoch": 0.7917073170731708,
      "step": 14607,
      "training_loss": 7.055758953094482
    },
    {
      "epoch": 0.7917615176151761,
      "grad_norm": 22.659887313842773,
      "learning_rate": 1e-05,
      "loss": 6.7315,
      "step": 14608
    },
    {
      "epoch": 0.7917615176151761,
      "step": 14608,
      "training_loss": 6.001386642456055
    },
    {
      "epoch": 0.7918157181571815,
      "step": 14609,
      "training_loss": 3.245542287826538
    },
    {
      "epoch": 0.791869918699187,
      "step": 14610,
      "training_loss": 6.937321186065674
    },
    {
      "epoch": 0.7919241192411924,
      "step": 14611,
      "training_loss": 3.954787254333496
    },
    {
      "epoch": 0.7919783197831979,
      "grad_norm": 29.629676818847656,
      "learning_rate": 1e-05,
      "loss": 5.0348,
      "step": 14612
    },
    {
      "epoch": 0.7919783197831979,
      "step": 14612,
      "training_loss": 6.447720050811768
    },
    {
      "epoch": 0.7920325203252032,
      "step": 14613,
      "training_loss": 6.261326789855957
    },
    {
      "epoch": 0.7920867208672087,
      "step": 14614,
      "training_loss": 6.380362033843994
    },
    {
      "epoch": 0.7921409214092141,
      "step": 14615,
      "training_loss": 6.475083351135254
    },
    {
      "epoch": 0.7921951219512195,
      "grad_norm": 31.518295288085938,
      "learning_rate": 1e-05,
      "loss": 6.3911,
      "step": 14616
    },
    {
      "epoch": 0.7921951219512195,
      "step": 14616,
      "training_loss": 4.725583553314209
    },
    {
      "epoch": 0.7922493224932249,
      "step": 14617,
      "training_loss": 7.46674919128418
    },
    {
      "epoch": 0.7923035230352303,
      "step": 14618,
      "training_loss": 4.802412509918213
    },
    {
      "epoch": 0.7923577235772358,
      "step": 14619,
      "training_loss": 7.517191410064697
    },
    {
      "epoch": 0.7924119241192412,
      "grad_norm": 20.969154357910156,
      "learning_rate": 1e-05,
      "loss": 6.128,
      "step": 14620
    },
    {
      "epoch": 0.7924119241192412,
      "step": 14620,
      "training_loss": 7.051703929901123
    },
    {
      "epoch": 0.7924661246612467,
      "step": 14621,
      "training_loss": 6.6412272453308105
    },
    {
      "epoch": 0.792520325203252,
      "step": 14622,
      "training_loss": 5.21378231048584
    },
    {
      "epoch": 0.7925745257452574,
      "step": 14623,
      "training_loss": 6.477335453033447
    },
    {
      "epoch": 0.7926287262872629,
      "grad_norm": 50.78277587890625,
      "learning_rate": 1e-05,
      "loss": 6.346,
      "step": 14624
    },
    {
      "epoch": 0.7926287262872629,
      "step": 14624,
      "training_loss": 6.156684398651123
    },
    {
      "epoch": 0.7926829268292683,
      "step": 14625,
      "training_loss": 6.489645004272461
    },
    {
      "epoch": 0.7927371273712737,
      "step": 14626,
      "training_loss": 6.785984039306641
    },
    {
      "epoch": 0.7927913279132791,
      "step": 14627,
      "training_loss": 6.3433837890625
    },
    {
      "epoch": 0.7928455284552846,
      "grad_norm": 24.055997848510742,
      "learning_rate": 1e-05,
      "loss": 6.4439,
      "step": 14628
    },
    {
      "epoch": 0.7928455284552846,
      "step": 14628,
      "training_loss": 6.0529375076293945
    },
    {
      "epoch": 0.79289972899729,
      "step": 14629,
      "training_loss": 6.770890712738037
    },
    {
      "epoch": 0.7929539295392954,
      "step": 14630,
      "training_loss": 5.111856460571289
    },
    {
      "epoch": 0.7930081300813008,
      "step": 14631,
      "training_loss": 6.49054479598999
    },
    {
      "epoch": 0.7930623306233062,
      "grad_norm": 20.195587158203125,
      "learning_rate": 1e-05,
      "loss": 6.1066,
      "step": 14632
    },
    {
      "epoch": 0.7930623306233062,
      "step": 14632,
      "training_loss": 2.8524627685546875
    },
    {
      "epoch": 0.7931165311653117,
      "step": 14633,
      "training_loss": 6.832960605621338
    },
    {
      "epoch": 0.7931707317073171,
      "step": 14634,
      "training_loss": 6.2884416580200195
    },
    {
      "epoch": 0.7932249322493224,
      "step": 14635,
      "training_loss": 6.793440341949463
    },
    {
      "epoch": 0.7932791327913279,
      "grad_norm": 16.75596809387207,
      "learning_rate": 1e-05,
      "loss": 5.6918,
      "step": 14636
    },
    {
      "epoch": 0.7932791327913279,
      "step": 14636,
      "training_loss": 4.3230695724487305
    },
    {
      "epoch": 0.7933333333333333,
      "step": 14637,
      "training_loss": 5.8904876708984375
    },
    {
      "epoch": 0.7933875338753388,
      "step": 14638,
      "training_loss": 3.9157192707061768
    },
    {
      "epoch": 0.7934417344173442,
      "step": 14639,
      "training_loss": 6.9040846824646
    },
    {
      "epoch": 0.7934959349593496,
      "grad_norm": 16.256032943725586,
      "learning_rate": 1e-05,
      "loss": 5.2583,
      "step": 14640
    },
    {
      "epoch": 0.7934959349593496,
      "step": 14640,
      "training_loss": 6.2659525871276855
    },
    {
      "epoch": 0.793550135501355,
      "step": 14641,
      "training_loss": 5.321394443511963
    },
    {
      "epoch": 0.7936043360433604,
      "step": 14642,
      "training_loss": 7.956019878387451
    },
    {
      "epoch": 0.7936585365853659,
      "step": 14643,
      "training_loss": 6.778175354003906
    },
    {
      "epoch": 0.7937127371273712,
      "grad_norm": 19.766733169555664,
      "learning_rate": 1e-05,
      "loss": 6.5804,
      "step": 14644
    },
    {
      "epoch": 0.7937127371273712,
      "step": 14644,
      "training_loss": 6.629815101623535
    },
    {
      "epoch": 0.7937669376693767,
      "step": 14645,
      "training_loss": 5.881085395812988
    },
    {
      "epoch": 0.7938211382113821,
      "step": 14646,
      "training_loss": 3.197012186050415
    },
    {
      "epoch": 0.7938753387533876,
      "step": 14647,
      "training_loss": 5.936554431915283
    },
    {
      "epoch": 0.793929539295393,
      "grad_norm": 24.73925018310547,
      "learning_rate": 1e-05,
      "loss": 5.4111,
      "step": 14648
    },
    {
      "epoch": 0.793929539295393,
      "step": 14648,
      "training_loss": 6.445821762084961
    },
    {
      "epoch": 0.7939837398373983,
      "step": 14649,
      "training_loss": 7.1004862785339355
    },
    {
      "epoch": 0.7940379403794038,
      "step": 14650,
      "training_loss": 5.68146276473999
    },
    {
      "epoch": 0.7940921409214092,
      "step": 14651,
      "training_loss": 6.412574291229248
    },
    {
      "epoch": 0.7941463414634147,
      "grad_norm": 31.536386489868164,
      "learning_rate": 1e-05,
      "loss": 6.4101,
      "step": 14652
    },
    {
      "epoch": 0.7941463414634147,
      "step": 14652,
      "training_loss": 3.309939384460449
    },
    {
      "epoch": 0.79420054200542,
      "step": 14653,
      "training_loss": 5.0895562171936035
    },
    {
      "epoch": 0.7942547425474255,
      "step": 14654,
      "training_loss": 6.767988681793213
    },
    {
      "epoch": 0.7943089430894309,
      "step": 14655,
      "training_loss": 7.430532932281494
    },
    {
      "epoch": 0.7943631436314363,
      "grad_norm": 35.206932067871094,
      "learning_rate": 1e-05,
      "loss": 5.6495,
      "step": 14656
    },
    {
      "epoch": 0.7943631436314363,
      "step": 14656,
      "training_loss": 8.485980033874512
    },
    {
      "epoch": 0.7944173441734418,
      "step": 14657,
      "training_loss": 3.1420421600341797
    },
    {
      "epoch": 0.7944715447154471,
      "step": 14658,
      "training_loss": 6.021310806274414
    },
    {
      "epoch": 0.7945257452574526,
      "step": 14659,
      "training_loss": 6.178571701049805
    },
    {
      "epoch": 0.794579945799458,
      "grad_norm": 29.785198211669922,
      "learning_rate": 1e-05,
      "loss": 5.957,
      "step": 14660
    },
    {
      "epoch": 0.794579945799458,
      "step": 14660,
      "training_loss": 3.2241554260253906
    },
    {
      "epoch": 0.7946341463414635,
      "step": 14661,
      "training_loss": 6.222936153411865
    },
    {
      "epoch": 0.7946883468834688,
      "step": 14662,
      "training_loss": 3.6556100845336914
    },
    {
      "epoch": 0.7947425474254742,
      "step": 14663,
      "training_loss": 6.864616394042969
    },
    {
      "epoch": 0.7947967479674797,
      "grad_norm": 22.720643997192383,
      "learning_rate": 1e-05,
      "loss": 4.9918,
      "step": 14664
    },
    {
      "epoch": 0.7947967479674797,
      "step": 14664,
      "training_loss": 4.6274824142456055
    },
    {
      "epoch": 0.7948509485094851,
      "step": 14665,
      "training_loss": 7.743954181671143
    },
    {
      "epoch": 0.7949051490514906,
      "step": 14666,
      "training_loss": 6.718451976776123
    },
    {
      "epoch": 0.7949593495934959,
      "step": 14667,
      "training_loss": 7.2502264976501465
    },
    {
      "epoch": 0.7950135501355013,
      "grad_norm": 37.37653350830078,
      "learning_rate": 1e-05,
      "loss": 6.585,
      "step": 14668
    },
    {
      "epoch": 0.7950135501355013,
      "step": 14668,
      "training_loss": 7.047300815582275
    },
    {
      "epoch": 0.7950677506775068,
      "step": 14669,
      "training_loss": 4.596418857574463
    },
    {
      "epoch": 0.7951219512195122,
      "step": 14670,
      "training_loss": 7.286598205566406
    },
    {
      "epoch": 0.7951761517615176,
      "step": 14671,
      "training_loss": 5.227567672729492
    },
    {
      "epoch": 0.795230352303523,
      "grad_norm": 30.02804183959961,
      "learning_rate": 1e-05,
      "loss": 6.0395,
      "step": 14672
    },
    {
      "epoch": 0.795230352303523,
      "step": 14672,
      "training_loss": 6.210343360900879
    },
    {
      "epoch": 0.7952845528455285,
      "step": 14673,
      "training_loss": 6.4979047775268555
    },
    {
      "epoch": 0.7953387533875339,
      "step": 14674,
      "training_loss": 6.871716499328613
    },
    {
      "epoch": 0.7953929539295393,
      "step": 14675,
      "training_loss": 6.378859043121338
    },
    {
      "epoch": 0.7954471544715447,
      "grad_norm": 20.067766189575195,
      "learning_rate": 1e-05,
      "loss": 6.4897,
      "step": 14676
    },
    {
      "epoch": 0.7954471544715447,
      "step": 14676,
      "training_loss": 7.530885219573975
    },
    {
      "epoch": 0.7955013550135501,
      "step": 14677,
      "training_loss": 4.621293067932129
    },
    {
      "epoch": 0.7955555555555556,
      "step": 14678,
      "training_loss": 6.666469097137451
    },
    {
      "epoch": 0.795609756097561,
      "step": 14679,
      "training_loss": 5.7711181640625
    },
    {
      "epoch": 0.7956639566395663,
      "grad_norm": 27.256593704223633,
      "learning_rate": 1e-05,
      "loss": 6.1474,
      "step": 14680
    },
    {
      "epoch": 0.7956639566395663,
      "step": 14680,
      "training_loss": 7.741037845611572
    },
    {
      "epoch": 0.7957181571815718,
      "step": 14681,
      "training_loss": 6.245941638946533
    },
    {
      "epoch": 0.7957723577235772,
      "step": 14682,
      "training_loss": 6.996481418609619
    },
    {
      "epoch": 0.7958265582655827,
      "step": 14683,
      "training_loss": 6.759434223175049
    },
    {
      "epoch": 0.7958807588075881,
      "grad_norm": 21.286409378051758,
      "learning_rate": 1e-05,
      "loss": 6.9357,
      "step": 14684
    },
    {
      "epoch": 0.7958807588075881,
      "step": 14684,
      "training_loss": 7.300211429595947
    },
    {
      "epoch": 0.7959349593495935,
      "step": 14685,
      "training_loss": 7.552423000335693
    },
    {
      "epoch": 0.7959891598915989,
      "step": 14686,
      "training_loss": 6.70546293258667
    },
    {
      "epoch": 0.7960433604336044,
      "step": 14687,
      "training_loss": 6.934966564178467
    },
    {
      "epoch": 0.7960975609756098,
      "grad_norm": 21.096345901489258,
      "learning_rate": 1e-05,
      "loss": 7.1233,
      "step": 14688
    },
    {
      "epoch": 0.7960975609756098,
      "step": 14688,
      "training_loss": 7.166121482849121
    },
    {
      "epoch": 0.7961517615176151,
      "step": 14689,
      "training_loss": 7.409850120544434
    },
    {
      "epoch": 0.7962059620596206,
      "step": 14690,
      "training_loss": 6.372899532318115
    },
    {
      "epoch": 0.796260162601626,
      "step": 14691,
      "training_loss": 7.398623466491699
    },
    {
      "epoch": 0.7963143631436315,
      "grad_norm": 41.25619125366211,
      "learning_rate": 1e-05,
      "loss": 7.0869,
      "step": 14692
    },
    {
      "epoch": 0.7963143631436315,
      "step": 14692,
      "training_loss": 8.799181938171387
    },
    {
      "epoch": 0.7963685636856369,
      "step": 14693,
      "training_loss": 7.088413238525391
    },
    {
      "epoch": 0.7964227642276422,
      "step": 14694,
      "training_loss": 6.689764022827148
    },
    {
      "epoch": 0.7964769647696477,
      "step": 14695,
      "training_loss": 6.699377536773682
    },
    {
      "epoch": 0.7965311653116531,
      "grad_norm": 32.6854248046875,
      "learning_rate": 1e-05,
      "loss": 7.3192,
      "step": 14696
    },
    {
      "epoch": 0.7965311653116531,
      "step": 14696,
      "training_loss": 6.640499591827393
    },
    {
      "epoch": 0.7965853658536586,
      "step": 14697,
      "training_loss": 4.7459917068481445
    },
    {
      "epoch": 0.7966395663956639,
      "step": 14698,
      "training_loss": 5.970940113067627
    },
    {
      "epoch": 0.7966937669376694,
      "step": 14699,
      "training_loss": 5.417306423187256
    },
    {
      "epoch": 0.7967479674796748,
      "grad_norm": 30.87009048461914,
      "learning_rate": 1e-05,
      "loss": 5.6937,
      "step": 14700
    },
    {
      "epoch": 0.7967479674796748,
      "step": 14700,
      "training_loss": 6.727242946624756
    },
    {
      "epoch": 0.7968021680216802,
      "step": 14701,
      "training_loss": 5.341204643249512
    },
    {
      "epoch": 0.7968563685636857,
      "step": 14702,
      "training_loss": 7.399750232696533
    },
    {
      "epoch": 0.796910569105691,
      "step": 14703,
      "training_loss": 5.4513397216796875
    },
    {
      "epoch": 0.7969647696476965,
      "grad_norm": 28.827041625976562,
      "learning_rate": 1e-05,
      "loss": 6.2299,
      "step": 14704
    },
    {
      "epoch": 0.7969647696476965,
      "step": 14704,
      "training_loss": 5.535598278045654
    },
    {
      "epoch": 0.7970189701897019,
      "step": 14705,
      "training_loss": 7.853961944580078
    },
    {
      "epoch": 0.7970731707317074,
      "step": 14706,
      "training_loss": 8.551592826843262
    },
    {
      "epoch": 0.7971273712737127,
      "step": 14707,
      "training_loss": 7.63191032409668
    },
    {
      "epoch": 0.7971815718157181,
      "grad_norm": 21.963354110717773,
      "learning_rate": 1e-05,
      "loss": 7.3933,
      "step": 14708
    },
    {
      "epoch": 0.7971815718157181,
      "step": 14708,
      "training_loss": 5.721003532409668
    },
    {
      "epoch": 0.7972357723577236,
      "step": 14709,
      "training_loss": 7.6604390144348145
    },
    {
      "epoch": 0.797289972899729,
      "step": 14710,
      "training_loss": 3.226673126220703
    },
    {
      "epoch": 0.7973441734417345,
      "step": 14711,
      "training_loss": 7.1583685874938965
    },
    {
      "epoch": 0.7973983739837398,
      "grad_norm": 36.79145812988281,
      "learning_rate": 1e-05,
      "loss": 5.9416,
      "step": 14712
    },
    {
      "epoch": 0.7973983739837398,
      "step": 14712,
      "training_loss": 6.327155590057373
    },
    {
      "epoch": 0.7974525745257453,
      "step": 14713,
      "training_loss": 7.1102776527404785
    },
    {
      "epoch": 0.7975067750677507,
      "step": 14714,
      "training_loss": 6.572747230529785
    },
    {
      "epoch": 0.7975609756097561,
      "step": 14715,
      "training_loss": 3.877855062484741
    },
    {
      "epoch": 0.7976151761517615,
      "grad_norm": 23.275819778442383,
      "learning_rate": 1e-05,
      "loss": 5.972,
      "step": 14716
    },
    {
      "epoch": 0.7976151761517615,
      "step": 14716,
      "training_loss": 7.186192035675049
    },
    {
      "epoch": 0.7976693766937669,
      "step": 14717,
      "training_loss": 6.99727725982666
    },
    {
      "epoch": 0.7977235772357724,
      "step": 14718,
      "training_loss": 6.656611442565918
    },
    {
      "epoch": 0.7977777777777778,
      "step": 14719,
      "training_loss": 7.399031639099121
    },
    {
      "epoch": 0.7978319783197833,
      "grad_norm": 19.62889289855957,
      "learning_rate": 1e-05,
      "loss": 7.0598,
      "step": 14720
    },
    {
      "epoch": 0.7978319783197833,
      "step": 14720,
      "training_loss": 6.67003870010376
    },
    {
      "epoch": 0.7978861788617886,
      "step": 14721,
      "training_loss": 7.882688522338867
    },
    {
      "epoch": 0.797940379403794,
      "step": 14722,
      "training_loss": 8.59970474243164
    },
    {
      "epoch": 0.7979945799457995,
      "step": 14723,
      "training_loss": 6.695624828338623
    },
    {
      "epoch": 0.7980487804878049,
      "grad_norm": 20.52322769165039,
      "learning_rate": 1e-05,
      "loss": 7.462,
      "step": 14724
    },
    {
      "epoch": 0.7980487804878049,
      "step": 14724,
      "training_loss": 6.803898334503174
    },
    {
      "epoch": 0.7981029810298103,
      "step": 14725,
      "training_loss": 5.810759544372559
    },
    {
      "epoch": 0.7981571815718157,
      "step": 14726,
      "training_loss": 6.749911308288574
    },
    {
      "epoch": 0.7982113821138211,
      "step": 14727,
      "training_loss": 6.393821716308594
    },
    {
      "epoch": 0.7982655826558266,
      "grad_norm": 25.36211585998535,
      "learning_rate": 1e-05,
      "loss": 6.4396,
      "step": 14728
    },
    {
      "epoch": 0.7982655826558266,
      "step": 14728,
      "training_loss": 8.373847961425781
    },
    {
      "epoch": 0.798319783197832,
      "step": 14729,
      "training_loss": 7.218086242675781
    },
    {
      "epoch": 0.7983739837398374,
      "step": 14730,
      "training_loss": 5.551024436950684
    },
    {
      "epoch": 0.7984281842818428,
      "step": 14731,
      "training_loss": 9.920997619628906
    },
    {
      "epoch": 0.7984823848238483,
      "grad_norm": 67.32083892822266,
      "learning_rate": 1e-05,
      "loss": 7.766,
      "step": 14732
    },
    {
      "epoch": 0.7984823848238483,
      "step": 14732,
      "training_loss": 7.492406368255615
    },
    {
      "epoch": 0.7985365853658537,
      "step": 14733,
      "training_loss": 6.073465347290039
    },
    {
      "epoch": 0.798590785907859,
      "step": 14734,
      "training_loss": 6.884680271148682
    },
    {
      "epoch": 0.7986449864498645,
      "step": 14735,
      "training_loss": 7.272940158843994
    },
    {
      "epoch": 0.7986991869918699,
      "grad_norm": 18.925424575805664,
      "learning_rate": 1e-05,
      "loss": 6.9309,
      "step": 14736
    },
    {
      "epoch": 0.7986991869918699,
      "step": 14736,
      "training_loss": 7.6250691413879395
    },
    {
      "epoch": 0.7987533875338754,
      "step": 14737,
      "training_loss": 4.148684024810791
    },
    {
      "epoch": 0.7988075880758807,
      "step": 14738,
      "training_loss": 6.76621150970459
    },
    {
      "epoch": 0.7988617886178861,
      "step": 14739,
      "training_loss": 6.596121788024902
    },
    {
      "epoch": 0.7989159891598916,
      "grad_norm": 24.207504272460938,
      "learning_rate": 1e-05,
      "loss": 6.284,
      "step": 14740
    },
    {
      "epoch": 0.7989159891598916,
      "step": 14740,
      "training_loss": 6.69221830368042
    },
    {
      "epoch": 0.798970189701897,
      "step": 14741,
      "training_loss": 5.722027778625488
    },
    {
      "epoch": 0.7990243902439025,
      "step": 14742,
      "training_loss": 6.996987342834473
    },
    {
      "epoch": 0.7990785907859078,
      "step": 14743,
      "training_loss": 6.447774887084961
    },
    {
      "epoch": 0.7991327913279133,
      "grad_norm": 26.49513053894043,
      "learning_rate": 1e-05,
      "loss": 6.4648,
      "step": 14744
    },
    {
      "epoch": 0.7991327913279133,
      "step": 14744,
      "training_loss": 7.5145769119262695
    },
    {
      "epoch": 0.7991869918699187,
      "step": 14745,
      "training_loss": 6.222638130187988
    },
    {
      "epoch": 0.7992411924119242,
      "step": 14746,
      "training_loss": 11.212470054626465
    },
    {
      "epoch": 0.7992953929539295,
      "step": 14747,
      "training_loss": 5.92640495300293
    },
    {
      "epoch": 0.7993495934959349,
      "grad_norm": 22.197959899902344,
      "learning_rate": 1e-05,
      "loss": 7.719,
      "step": 14748
    },
    {
      "epoch": 0.7993495934959349,
      "step": 14748,
      "training_loss": 8.393681526184082
    },
    {
      "epoch": 0.7994037940379404,
      "step": 14749,
      "training_loss": 7.29564094543457
    },
    {
      "epoch": 0.7994579945799458,
      "step": 14750,
      "training_loss": 8.832863807678223
    },
    {
      "epoch": 0.7995121951219513,
      "step": 14751,
      "training_loss": 4.0546770095825195
    },
    {
      "epoch": 0.7995663956639566,
      "grad_norm": 70.76455688476562,
      "learning_rate": 1e-05,
      "loss": 7.1442,
      "step": 14752
    },
    {
      "epoch": 0.7995663956639566,
      "step": 14752,
      "training_loss": 7.416884899139404
    },
    {
      "epoch": 0.799620596205962,
      "step": 14753,
      "training_loss": 7.529728412628174
    },
    {
      "epoch": 0.7996747967479675,
      "step": 14754,
      "training_loss": 6.717812538146973
    },
    {
      "epoch": 0.7997289972899729,
      "step": 14755,
      "training_loss": 6.402860164642334
    },
    {
      "epoch": 0.7997831978319783,
      "grad_norm": 24.59257698059082,
      "learning_rate": 1e-05,
      "loss": 7.0168,
      "step": 14756
    },
    {
      "epoch": 0.7997831978319783,
      "step": 14756,
      "training_loss": 8.290984153747559
    },
    {
      "epoch": 0.7998373983739837,
      "step": 14757,
      "training_loss": 5.33176326751709
    },
    {
      "epoch": 0.7998915989159892,
      "step": 14758,
      "training_loss": 6.485463619232178
    },
    {
      "epoch": 0.7999457994579946,
      "step": 14759,
      "training_loss": 8.203673362731934
    },
    {
      "epoch": 0.8,
      "grad_norm": 41.09227752685547,
      "learning_rate": 1e-05,
      "loss": 7.078,
      "step": 14760
    },
    {
      "epoch": 0.8,
      "step": 14760,
      "training_loss": 6.122069835662842
    },
    {
      "epoch": 0.8000542005420054,
      "step": 14761,
      "training_loss": 7.383237838745117
    },
    {
      "epoch": 0.8001084010840108,
      "step": 14762,
      "training_loss": 7.3148322105407715
    },
    {
      "epoch": 0.8001626016260163,
      "step": 14763,
      "training_loss": 9.228056907653809
    },
    {
      "epoch": 0.8002168021680217,
      "grad_norm": 39.37809753417969,
      "learning_rate": 1e-05,
      "loss": 7.512,
      "step": 14764
    },
    {
      "epoch": 0.8002168021680217,
      "step": 14764,
      "training_loss": 6.104683876037598
    },
    {
      "epoch": 0.800271002710027,
      "step": 14765,
      "training_loss": 6.926770210266113
    },
    {
      "epoch": 0.8003252032520325,
      "step": 14766,
      "training_loss": 6.6336283683776855
    },
    {
      "epoch": 0.8003794037940379,
      "step": 14767,
      "training_loss": 7.717413425445557
    },
    {
      "epoch": 0.8004336043360434,
      "grad_norm": 17.918169021606445,
      "learning_rate": 1e-05,
      "loss": 6.8456,
      "step": 14768
    },
    {
      "epoch": 0.8004336043360434,
      "step": 14768,
      "training_loss": 6.426639556884766
    },
    {
      "epoch": 0.8004878048780488,
      "step": 14769,
      "training_loss": 8.695442199707031
    },
    {
      "epoch": 0.8005420054200542,
      "step": 14770,
      "training_loss": 7.718241214752197
    },
    {
      "epoch": 0.8005962059620596,
      "step": 14771,
      "training_loss": 7.2864484786987305
    },
    {
      "epoch": 0.800650406504065,
      "grad_norm": 26.01835823059082,
      "learning_rate": 1e-05,
      "loss": 7.5317,
      "step": 14772
    },
    {
      "epoch": 0.800650406504065,
      "step": 14772,
      "training_loss": 6.837240219116211
    },
    {
      "epoch": 0.8007046070460705,
      "step": 14773,
      "training_loss": 6.855283737182617
    },
    {
      "epoch": 0.8007588075880758,
      "step": 14774,
      "training_loss": 7.551434516906738
    },
    {
      "epoch": 0.8008130081300813,
      "step": 14775,
      "training_loss": 6.680433750152588
    },
    {
      "epoch": 0.8008672086720867,
      "grad_norm": 17.768177032470703,
      "learning_rate": 1e-05,
      "loss": 6.9811,
      "step": 14776
    },
    {
      "epoch": 0.8008672086720867,
      "step": 14776,
      "training_loss": 6.762874603271484
    },
    {
      "epoch": 0.8009214092140922,
      "step": 14777,
      "training_loss": 7.236076354980469
    },
    {
      "epoch": 0.8009756097560976,
      "step": 14778,
      "training_loss": 7.912363052368164
    },
    {
      "epoch": 0.8010298102981029,
      "step": 14779,
      "training_loss": 6.605751991271973
    },
    {
      "epoch": 0.8010840108401084,
      "grad_norm": 19.226085662841797,
      "learning_rate": 1e-05,
      "loss": 7.1293,
      "step": 14780
    },
    {
      "epoch": 0.8010840108401084,
      "step": 14780,
      "training_loss": 6.929562091827393
    },
    {
      "epoch": 0.8011382113821138,
      "step": 14781,
      "training_loss": 7.10227108001709
    },
    {
      "epoch": 0.8011924119241193,
      "step": 14782,
      "training_loss": 7.985283374786377
    },
    {
      "epoch": 0.8012466124661246,
      "step": 14783,
      "training_loss": 6.6874823570251465
    },
    {
      "epoch": 0.80130081300813,
      "grad_norm": 25.08680534362793,
      "learning_rate": 1e-05,
      "loss": 7.1761,
      "step": 14784
    },
    {
      "epoch": 0.80130081300813,
      "step": 14784,
      "training_loss": 6.267055511474609
    },
    {
      "epoch": 0.8013550135501355,
      "step": 14785,
      "training_loss": 5.331332683563232
    },
    {
      "epoch": 0.8014092140921409,
      "step": 14786,
      "training_loss": 5.892547607421875
    },
    {
      "epoch": 0.8014634146341464,
      "step": 14787,
      "training_loss": 7.045347690582275
    },
    {
      "epoch": 0.8015176151761517,
      "grad_norm": 29.316505432128906,
      "learning_rate": 1e-05,
      "loss": 6.1341,
      "step": 14788
    },
    {
      "epoch": 0.8015176151761517,
      "step": 14788,
      "training_loss": 6.852262496948242
    },
    {
      "epoch": 0.8015718157181572,
      "step": 14789,
      "training_loss": 7.492518424987793
    },
    {
      "epoch": 0.8016260162601626,
      "step": 14790,
      "training_loss": 6.636054515838623
    },
    {
      "epoch": 0.801680216802168,
      "step": 14791,
      "training_loss": 8.060070991516113
    },
    {
      "epoch": 0.8017344173441734,
      "grad_norm": 22.115182876586914,
      "learning_rate": 1e-05,
      "loss": 7.2602,
      "step": 14792
    },
    {
      "epoch": 0.8017344173441734,
      "step": 14792,
      "training_loss": 5.833308219909668
    },
    {
      "epoch": 0.8017886178861788,
      "step": 14793,
      "training_loss": 7.144688129425049
    },
    {
      "epoch": 0.8018428184281843,
      "step": 14794,
      "training_loss": 3.6411402225494385
    },
    {
      "epoch": 0.8018970189701897,
      "step": 14795,
      "training_loss": 6.964362144470215
    },
    {
      "epoch": 0.8019512195121952,
      "grad_norm": 27.079599380493164,
      "learning_rate": 1e-05,
      "loss": 5.8959,
      "step": 14796
    },
    {
      "epoch": 0.8019512195121952,
      "step": 14796,
      "training_loss": 3.5028445720672607
    },
    {
      "epoch": 0.8020054200542005,
      "step": 14797,
      "training_loss": 7.028222560882568
    },
    {
      "epoch": 0.802059620596206,
      "step": 14798,
      "training_loss": 7.0518574714660645
    },
    {
      "epoch": 0.8021138211382114,
      "step": 14799,
      "training_loss": 7.183742046356201
    },
    {
      "epoch": 0.8021680216802168,
      "grad_norm": 32.49821472167969,
      "learning_rate": 1e-05,
      "loss": 6.1917,
      "step": 14800
    },
    {
      "epoch": 0.8021680216802168,
      "step": 14800,
      "training_loss": 6.946746826171875
    },
    {
      "epoch": 0.8022222222222222,
      "step": 14801,
      "training_loss": 8.116209983825684
    },
    {
      "epoch": 0.8022764227642276,
      "step": 14802,
      "training_loss": 5.773495197296143
    },
    {
      "epoch": 0.8023306233062331,
      "step": 14803,
      "training_loss": 7.600558757781982
    },
    {
      "epoch": 0.8023848238482385,
      "grad_norm": 18.14665412902832,
      "learning_rate": 1e-05,
      "loss": 7.1093,
      "step": 14804
    },
    {
      "epoch": 0.8023848238482385,
      "step": 14804,
      "training_loss": 6.76190185546875
    },
    {
      "epoch": 0.802439024390244,
      "step": 14805,
      "training_loss": 5.6249847412109375
    },
    {
      "epoch": 0.8024932249322493,
      "step": 14806,
      "training_loss": 6.842330455780029
    },
    {
      "epoch": 0.8025474254742547,
      "step": 14807,
      "training_loss": 7.4561333656311035
    },
    {
      "epoch": 0.8026016260162602,
      "grad_norm": 43.18202209472656,
      "learning_rate": 1e-05,
      "loss": 6.6713,
      "step": 14808
    },
    {
      "epoch": 0.8026016260162602,
      "step": 14808,
      "training_loss": 6.074673175811768
    },
    {
      "epoch": 0.8026558265582656,
      "step": 14809,
      "training_loss": 6.804075717926025
    },
    {
      "epoch": 0.802710027100271,
      "step": 14810,
      "training_loss": 7.715251922607422
    },
    {
      "epoch": 0.8027642276422764,
      "step": 14811,
      "training_loss": 5.986349582672119
    },
    {
      "epoch": 0.8028184281842818,
      "grad_norm": 24.641311645507812,
      "learning_rate": 1e-05,
      "loss": 6.6451,
      "step": 14812
    },
    {
      "epoch": 0.8028184281842818,
      "step": 14812,
      "training_loss": 6.694216251373291
    },
    {
      "epoch": 0.8028726287262873,
      "step": 14813,
      "training_loss": 6.795254230499268
    },
    {
      "epoch": 0.8029268292682927,
      "step": 14814,
      "training_loss": 7.048962593078613
    },
    {
      "epoch": 0.8029810298102981,
      "step": 14815,
      "training_loss": 7.0852556228637695
    },
    {
      "epoch": 0.8030352303523035,
      "grad_norm": 20.92904281616211,
      "learning_rate": 1e-05,
      "loss": 6.9059,
      "step": 14816
    },
    {
      "epoch": 0.8030352303523035,
      "step": 14816,
      "training_loss": 4.72935152053833
    },
    {
      "epoch": 0.803089430894309,
      "step": 14817,
      "training_loss": 4.691366672515869
    },
    {
      "epoch": 0.8031436314363144,
      "step": 14818,
      "training_loss": 6.312926769256592
    },
    {
      "epoch": 0.8031978319783197,
      "step": 14819,
      "training_loss": 6.279351711273193
    },
    {
      "epoch": 0.8032520325203252,
      "grad_norm": 25.905912399291992,
      "learning_rate": 1e-05,
      "loss": 5.5032,
      "step": 14820
    },
    {
      "epoch": 0.8032520325203252,
      "step": 14820,
      "training_loss": 5.4829277992248535
    },
    {
      "epoch": 0.8033062330623306,
      "step": 14821,
      "training_loss": 7.709666728973389
    },
    {
      "epoch": 0.8033604336043361,
      "step": 14822,
      "training_loss": 7.021679401397705
    },
    {
      "epoch": 0.8034146341463415,
      "step": 14823,
      "training_loss": 7.839358806610107
    },
    {
      "epoch": 0.8034688346883468,
      "grad_norm": 30.57074546813965,
      "learning_rate": 1e-05,
      "loss": 7.0134,
      "step": 14824
    },
    {
      "epoch": 0.8034688346883468,
      "step": 14824,
      "training_loss": 6.277340412139893
    },
    {
      "epoch": 0.8035230352303523,
      "step": 14825,
      "training_loss": 6.619953632354736
    },
    {
      "epoch": 0.8035772357723577,
      "step": 14826,
      "training_loss": 7.294776439666748
    },
    {
      "epoch": 0.8036314363143632,
      "step": 14827,
      "training_loss": 6.499995708465576
    },
    {
      "epoch": 0.8036856368563685,
      "grad_norm": 20.301467895507812,
      "learning_rate": 1e-05,
      "loss": 6.673,
      "step": 14828
    },
    {
      "epoch": 0.8036856368563685,
      "step": 14828,
      "training_loss": 7.616883277893066
    },
    {
      "epoch": 0.803739837398374,
      "step": 14829,
      "training_loss": 7.067187309265137
    },
    {
      "epoch": 0.8037940379403794,
      "step": 14830,
      "training_loss": 6.966151714324951
    },
    {
      "epoch": 0.8038482384823848,
      "step": 14831,
      "training_loss": 6.675729751586914
    },
    {
      "epoch": 0.8039024390243903,
      "grad_norm": 20.89808464050293,
      "learning_rate": 1e-05,
      "loss": 7.0815,
      "step": 14832
    },
    {
      "epoch": 0.8039024390243903,
      "step": 14832,
      "training_loss": 6.843689918518066
    },
    {
      "epoch": 0.8039566395663956,
      "step": 14833,
      "training_loss": 8.071767807006836
    },
    {
      "epoch": 0.8040108401084011,
      "step": 14834,
      "training_loss": 7.16923713684082
    },
    {
      "epoch": 0.8040650406504065,
      "step": 14835,
      "training_loss": 6.326572418212891
    },
    {
      "epoch": 0.804119241192412,
      "grad_norm": 28.1200008392334,
      "learning_rate": 1e-05,
      "loss": 7.1028,
      "step": 14836
    },
    {
      "epoch": 0.804119241192412,
      "step": 14836,
      "training_loss": 5.863734245300293
    },
    {
      "epoch": 0.8041734417344173,
      "step": 14837,
      "training_loss": 6.583016872406006
    },
    {
      "epoch": 0.8042276422764227,
      "step": 14838,
      "training_loss": 6.258988380432129
    },
    {
      "epoch": 0.8042818428184282,
      "step": 14839,
      "training_loss": 6.716562747955322
    },
    {
      "epoch": 0.8043360433604336,
      "grad_norm": 30.02959632873535,
      "learning_rate": 1e-05,
      "loss": 6.3556,
      "step": 14840
    },
    {
      "epoch": 0.8043360433604336,
      "step": 14840,
      "training_loss": 7.407993793487549
    },
    {
      "epoch": 0.8043902439024391,
      "step": 14841,
      "training_loss": 6.368086814880371
    },
    {
      "epoch": 0.8044444444444444,
      "step": 14842,
      "training_loss": 6.089757442474365
    },
    {
      "epoch": 0.8044986449864499,
      "step": 14843,
      "training_loss": 5.935675144195557
    },
    {
      "epoch": 0.8045528455284553,
      "grad_norm": 23.89103126525879,
      "learning_rate": 1e-05,
      "loss": 6.4504,
      "step": 14844
    },
    {
      "epoch": 0.8045528455284553,
      "step": 14844,
      "training_loss": 6.701523303985596
    },
    {
      "epoch": 0.8046070460704607,
      "step": 14845,
      "training_loss": 6.9633684158325195
    },
    {
      "epoch": 0.8046612466124661,
      "step": 14846,
      "training_loss": 5.864768028259277
    },
    {
      "epoch": 0.8047154471544715,
      "step": 14847,
      "training_loss": 4.466353416442871
    },
    {
      "epoch": 0.804769647696477,
      "grad_norm": 33.13957595825195,
      "learning_rate": 1e-05,
      "loss": 5.999,
      "step": 14848
    },
    {
      "epoch": 0.804769647696477,
      "step": 14848,
      "training_loss": 8.455284118652344
    },
    {
      "epoch": 0.8048238482384824,
      "step": 14849,
      "training_loss": 7.40910005569458
    },
    {
      "epoch": 0.8048780487804879,
      "step": 14850,
      "training_loss": 7.05654764175415
    },
    {
      "epoch": 0.8049322493224932,
      "step": 14851,
      "training_loss": 6.067055702209473
    },
    {
      "epoch": 0.8049864498644986,
      "grad_norm": 34.86958694458008,
      "learning_rate": 1e-05,
      "loss": 7.247,
      "step": 14852
    },
    {
      "epoch": 0.8049864498644986,
      "step": 14852,
      "training_loss": 7.441801071166992
    },
    {
      "epoch": 0.8050406504065041,
      "step": 14853,
      "training_loss": 5.688847541809082
    },
    {
      "epoch": 0.8050948509485095,
      "step": 14854,
      "training_loss": 6.764588356018066
    },
    {
      "epoch": 0.8051490514905149,
      "step": 14855,
      "training_loss": 8.316704750061035
    },
    {
      "epoch": 0.8052032520325203,
      "grad_norm": 28.52480697631836,
      "learning_rate": 1e-05,
      "loss": 7.053,
      "step": 14856
    },
    {
      "epoch": 0.8052032520325203,
      "step": 14856,
      "training_loss": 4.816826820373535
    },
    {
      "epoch": 0.8052574525745257,
      "step": 14857,
      "training_loss": 6.954893589019775
    },
    {
      "epoch": 0.8053116531165312,
      "step": 14858,
      "training_loss": 7.012796401977539
    },
    {
      "epoch": 0.8053658536585366,
      "step": 14859,
      "training_loss": 7.27479887008667
    },
    {
      "epoch": 0.805420054200542,
      "grad_norm": 22.861459732055664,
      "learning_rate": 1e-05,
      "loss": 6.5148,
      "step": 14860
    },
    {
      "epoch": 0.805420054200542,
      "step": 14860,
      "training_loss": 6.071791648864746
    },
    {
      "epoch": 0.8054742547425474,
      "step": 14861,
      "training_loss": 6.419587135314941
    },
    {
      "epoch": 0.8055284552845529,
      "step": 14862,
      "training_loss": 6.9530158042907715
    },
    {
      "epoch": 0.8055826558265583,
      "step": 14863,
      "training_loss": 6.7320876121521
    },
    {
      "epoch": 0.8056368563685636,
      "grad_norm": 19.126789093017578,
      "learning_rate": 1e-05,
      "loss": 6.5441,
      "step": 14864
    },
    {
      "epoch": 0.8056368563685636,
      "step": 14864,
      "training_loss": 6.932246208190918
    },
    {
      "epoch": 0.8056910569105691,
      "step": 14865,
      "training_loss": 6.819483280181885
    },
    {
      "epoch": 0.8057452574525745,
      "step": 14866,
      "training_loss": 6.880884647369385
    },
    {
      "epoch": 0.80579945799458,
      "step": 14867,
      "training_loss": 8.24247932434082
    },
    {
      "epoch": 0.8058536585365854,
      "grad_norm": 37.00139236450195,
      "learning_rate": 1e-05,
      "loss": 7.2188,
      "step": 14868
    },
    {
      "epoch": 0.8058536585365854,
      "step": 14868,
      "training_loss": 6.584714889526367
    },
    {
      "epoch": 0.8059078590785907,
      "step": 14869,
      "training_loss": 6.464762210845947
    },
    {
      "epoch": 0.8059620596205962,
      "step": 14870,
      "training_loss": 6.9152092933654785
    },
    {
      "epoch": 0.8060162601626016,
      "step": 14871,
      "training_loss": 6.038241863250732
    },
    {
      "epoch": 0.8060704607046071,
      "grad_norm": 37.18252182006836,
      "learning_rate": 1e-05,
      "loss": 6.5007,
      "step": 14872
    },
    {
      "epoch": 0.8060704607046071,
      "step": 14872,
      "training_loss": 6.647228717803955
    },
    {
      "epoch": 0.8061246612466124,
      "step": 14873,
      "training_loss": 4.783209323883057
    },
    {
      "epoch": 0.8061788617886179,
      "step": 14874,
      "training_loss": 5.7544755935668945
    },
    {
      "epoch": 0.8062330623306233,
      "step": 14875,
      "training_loss": 5.334662437438965
    },
    {
      "epoch": 0.8062872628726288,
      "grad_norm": 20.03459930419922,
      "learning_rate": 1e-05,
      "loss": 5.6299,
      "step": 14876
    },
    {
      "epoch": 0.8062872628726288,
      "step": 14876,
      "training_loss": 3.73962140083313
    },
    {
      "epoch": 0.8063414634146342,
      "step": 14877,
      "training_loss": 6.6131181716918945
    },
    {
      "epoch": 0.8063956639566395,
      "step": 14878,
      "training_loss": 7.847711563110352
    },
    {
      "epoch": 0.806449864498645,
      "step": 14879,
      "training_loss": 6.60174560546875
    },
    {
      "epoch": 0.8065040650406504,
      "grad_norm": 22.846574783325195,
      "learning_rate": 1e-05,
      "loss": 6.2005,
      "step": 14880
    },
    {
      "epoch": 0.8065040650406504,
      "step": 14880,
      "training_loss": 7.9647016525268555
    },
    {
      "epoch": 0.8065582655826559,
      "step": 14881,
      "training_loss": 7.882150173187256
    },
    {
      "epoch": 0.8066124661246612,
      "step": 14882,
      "training_loss": 7.64902400970459
    },
    {
      "epoch": 0.8066666666666666,
      "step": 14883,
      "training_loss": 6.076975345611572
    },
    {
      "epoch": 0.8067208672086721,
      "grad_norm": 44.8797607421875,
      "learning_rate": 1e-05,
      "loss": 7.3932,
      "step": 14884
    },
    {
      "epoch": 0.8067208672086721,
      "step": 14884,
      "training_loss": 5.067507266998291
    },
    {
      "epoch": 0.8067750677506775,
      "step": 14885,
      "training_loss": 6.622539520263672
    },
    {
      "epoch": 0.806829268292683,
      "step": 14886,
      "training_loss": 7.363065242767334
    },
    {
      "epoch": 0.8068834688346883,
      "step": 14887,
      "training_loss": 4.675155162811279
    },
    {
      "epoch": 0.8069376693766938,
      "grad_norm": 38.25812911987305,
      "learning_rate": 1e-05,
      "loss": 5.9321,
      "step": 14888
    },
    {
      "epoch": 0.8069376693766938,
      "step": 14888,
      "training_loss": 3.6113054752349854
    },
    {
      "epoch": 0.8069918699186992,
      "step": 14889,
      "training_loss": 6.702431678771973
    },
    {
      "epoch": 0.8070460704607046,
      "step": 14890,
      "training_loss": 6.115395545959473
    },
    {
      "epoch": 0.80710027100271,
      "step": 14891,
      "training_loss": 5.976775169372559
    },
    {
      "epoch": 0.8071544715447154,
      "grad_norm": 40.59557342529297,
      "learning_rate": 1e-05,
      "loss": 5.6015,
      "step": 14892
    },
    {
      "epoch": 0.8071544715447154,
      "step": 14892,
      "training_loss": 7.03715705871582
    },
    {
      "epoch": 0.8072086720867209,
      "step": 14893,
      "training_loss": 6.619129180908203
    },
    {
      "epoch": 0.8072628726287263,
      "step": 14894,
      "training_loss": 6.461709499359131
    },
    {
      "epoch": 0.8073170731707318,
      "step": 14895,
      "training_loss": 6.5910539627075195
    },
    {
      "epoch": 0.8073712737127371,
      "grad_norm": 43.85206985473633,
      "learning_rate": 1e-05,
      "loss": 6.6773,
      "step": 14896
    },
    {
      "epoch": 0.8073712737127371,
      "step": 14896,
      "training_loss": 6.494901657104492
    },
    {
      "epoch": 0.8074254742547425,
      "step": 14897,
      "training_loss": 6.778624057769775
    },
    {
      "epoch": 0.807479674796748,
      "step": 14898,
      "training_loss": 7.040032386779785
    },
    {
      "epoch": 0.8075338753387534,
      "step": 14899,
      "training_loss": 6.214024066925049
    },
    {
      "epoch": 0.8075880758807588,
      "grad_norm": 32.00518035888672,
      "learning_rate": 1e-05,
      "loss": 6.6319,
      "step": 14900
    },
    {
      "epoch": 0.8075880758807588,
      "step": 14900,
      "training_loss": 6.879268169403076
    },
    {
      "epoch": 0.8076422764227642,
      "step": 14901,
      "training_loss": 7.6313958168029785
    },
    {
      "epoch": 0.8076964769647696,
      "step": 14902,
      "training_loss": 7.964354991912842
    },
    {
      "epoch": 0.8077506775067751,
      "step": 14903,
      "training_loss": 6.100683689117432
    },
    {
      "epoch": 0.8078048780487805,
      "grad_norm": 29.257829666137695,
      "learning_rate": 1e-05,
      "loss": 7.1439,
      "step": 14904
    },
    {
      "epoch": 0.8078048780487805,
      "step": 14904,
      "training_loss": 7.789060592651367
    },
    {
      "epoch": 0.8078590785907859,
      "step": 14905,
      "training_loss": 6.897177696228027
    },
    {
      "epoch": 0.8079132791327913,
      "step": 14906,
      "training_loss": 7.492185115814209
    },
    {
      "epoch": 0.8079674796747968,
      "step": 14907,
      "training_loss": 6.161950588226318
    },
    {
      "epoch": 0.8080216802168022,
      "grad_norm": 23.18336296081543,
      "learning_rate": 1e-05,
      "loss": 7.0851,
      "step": 14908
    },
    {
      "epoch": 0.8080216802168022,
      "step": 14908,
      "training_loss": 7.068263053894043
    },
    {
      "epoch": 0.8080758807588075,
      "step": 14909,
      "training_loss": 7.698850631713867
    },
    {
      "epoch": 0.808130081300813,
      "step": 14910,
      "training_loss": 6.000282287597656
    },
    {
      "epoch": 0.8081842818428184,
      "step": 14911,
      "training_loss": 6.1925201416015625
    },
    {
      "epoch": 0.8082384823848239,
      "grad_norm": 26.068349838256836,
      "learning_rate": 1e-05,
      "loss": 6.74,
      "step": 14912
    },
    {
      "epoch": 0.8082384823848239,
      "step": 14912,
      "training_loss": 7.131940841674805
    },
    {
      "epoch": 0.8082926829268293,
      "step": 14913,
      "training_loss": 7.778680801391602
    },
    {
      "epoch": 0.8083468834688347,
      "step": 14914,
      "training_loss": 6.532548427581787
    },
    {
      "epoch": 0.8084010840108401,
      "step": 14915,
      "training_loss": 7.5991621017456055
    },
    {
      "epoch": 0.8084552845528455,
      "grad_norm": 36.57598114013672,
      "learning_rate": 1e-05,
      "loss": 7.2606,
      "step": 14916
    },
    {
      "epoch": 0.8084552845528455,
      "step": 14916,
      "training_loss": 5.995999336242676
    },
    {
      "epoch": 0.808509485094851,
      "step": 14917,
      "training_loss": 6.838527679443359
    },
    {
      "epoch": 0.8085636856368563,
      "step": 14918,
      "training_loss": 6.155827522277832
    },
    {
      "epoch": 0.8086178861788618,
      "step": 14919,
      "training_loss": 3.7975611686706543
    },
    {
      "epoch": 0.8086720867208672,
      "grad_norm": 29.206811904907227,
      "learning_rate": 1e-05,
      "loss": 5.697,
      "step": 14920
    },
    {
      "epoch": 0.8086720867208672,
      "step": 14920,
      "training_loss": 7.612776279449463
    },
    {
      "epoch": 0.8087262872628727,
      "step": 14921,
      "training_loss": 8.035045623779297
    },
    {
      "epoch": 0.8087804878048781,
      "step": 14922,
      "training_loss": 6.604852676391602
    },
    {
      "epoch": 0.8088346883468834,
      "step": 14923,
      "training_loss": 7.885673522949219
    },
    {
      "epoch": 0.8088888888888889,
      "grad_norm": 30.357177734375,
      "learning_rate": 1e-05,
      "loss": 7.5346,
      "step": 14924
    },
    {
      "epoch": 0.8088888888888889,
      "step": 14924,
      "training_loss": 6.614882946014404
    },
    {
      "epoch": 0.8089430894308943,
      "step": 14925,
      "training_loss": 7.271450042724609
    },
    {
      "epoch": 0.8089972899728998,
      "step": 14926,
      "training_loss": 7.311646938323975
    },
    {
      "epoch": 0.8090514905149051,
      "step": 14927,
      "training_loss": 6.945201873779297
    },
    {
      "epoch": 0.8091056910569105,
      "grad_norm": 25.54084014892578,
      "learning_rate": 1e-05,
      "loss": 7.0358,
      "step": 14928
    },
    {
      "epoch": 0.8091056910569105,
      "step": 14928,
      "training_loss": 8.440895080566406
    },
    {
      "epoch": 0.809159891598916,
      "step": 14929,
      "training_loss": 5.836987018585205
    },
    {
      "epoch": 0.8092140921409214,
      "step": 14930,
      "training_loss": 3.262847661972046
    },
    {
      "epoch": 0.8092682926829269,
      "step": 14931,
      "training_loss": 6.756624698638916
    },
    {
      "epoch": 0.8093224932249322,
      "grad_norm": 18.382099151611328,
      "learning_rate": 1e-05,
      "loss": 6.0743,
      "step": 14932
    },
    {
      "epoch": 0.8093224932249322,
      "step": 14932,
      "training_loss": 5.926405906677246
    },
    {
      "epoch": 0.8093766937669377,
      "step": 14933,
      "training_loss": 6.857247352600098
    },
    {
      "epoch": 0.8094308943089431,
      "step": 14934,
      "training_loss": 6.5650153160095215
    },
    {
      "epoch": 0.8094850948509485,
      "step": 14935,
      "training_loss": 6.720489978790283
    },
    {
      "epoch": 0.8095392953929539,
      "grad_norm": 20.14311408996582,
      "learning_rate": 1e-05,
      "loss": 6.5173,
      "step": 14936
    },
    {
      "epoch": 0.8095392953929539,
      "step": 14936,
      "training_loss": 6.360546112060547
    },
    {
      "epoch": 0.8095934959349593,
      "step": 14937,
      "training_loss": 5.956784248352051
    },
    {
      "epoch": 0.8096476964769648,
      "step": 14938,
      "training_loss": 6.80683708190918
    },
    {
      "epoch": 0.8097018970189702,
      "step": 14939,
      "training_loss": 7.150479316711426
    },
    {
      "epoch": 0.8097560975609757,
      "grad_norm": 25.171483993530273,
      "learning_rate": 1e-05,
      "loss": 6.5687,
      "step": 14940
    },
    {
      "epoch": 0.8097560975609757,
      "step": 14940,
      "training_loss": 6.5267767906188965
    },
    {
      "epoch": 0.809810298102981,
      "step": 14941,
      "training_loss": 6.237501621246338
    },
    {
      "epoch": 0.8098644986449864,
      "step": 14942,
      "training_loss": 7.013924598693848
    },
    {
      "epoch": 0.8099186991869919,
      "step": 14943,
      "training_loss": 6.8820672035217285
    },
    {
      "epoch": 0.8099728997289973,
      "grad_norm": 18.934236526489258,
      "learning_rate": 1e-05,
      "loss": 6.6651,
      "step": 14944
    },
    {
      "epoch": 0.8099728997289973,
      "step": 14944,
      "training_loss": 6.95590877532959
    },
    {
      "epoch": 0.8100271002710027,
      "step": 14945,
      "training_loss": 6.337671279907227
    },
    {
      "epoch": 0.8100813008130081,
      "step": 14946,
      "training_loss": 7.606405735015869
    },
    {
      "epoch": 0.8101355013550136,
      "step": 14947,
      "training_loss": 6.962528705596924
    },
    {
      "epoch": 0.810189701897019,
      "grad_norm": 34.30531692504883,
      "learning_rate": 1e-05,
      "loss": 6.9656,
      "step": 14948
    },
    {
      "epoch": 0.810189701897019,
      "step": 14948,
      "training_loss": 6.239223480224609
    },
    {
      "epoch": 0.8102439024390244,
      "step": 14949,
      "training_loss": 6.947543621063232
    },
    {
      "epoch": 0.8102981029810298,
      "step": 14950,
      "training_loss": 5.660008907318115
    },
    {
      "epoch": 0.8103523035230352,
      "step": 14951,
      "training_loss": 6.5226569175720215
    },
    {
      "epoch": 0.8104065040650407,
      "grad_norm": 24.37757110595703,
      "learning_rate": 1e-05,
      "loss": 6.3424,
      "step": 14952
    },
    {
      "epoch": 0.8104065040650407,
      "step": 14952,
      "training_loss": 7.641109943389893
    },
    {
      "epoch": 0.8104607046070461,
      "step": 14953,
      "training_loss": 7.6997857093811035
    },
    {
      "epoch": 0.8105149051490514,
      "step": 14954,
      "training_loss": 6.768679141998291
    },
    {
      "epoch": 0.8105691056910569,
      "step": 14955,
      "training_loss": 8.452088356018066
    },
    {
      "epoch": 0.8106233062330623,
      "grad_norm": 33.721710205078125,
      "learning_rate": 1e-05,
      "loss": 7.6404,
      "step": 14956
    },
    {
      "epoch": 0.8106233062330623,
      "step": 14956,
      "training_loss": 7.281157493591309
    },
    {
      "epoch": 0.8106775067750678,
      "step": 14957,
      "training_loss": 5.3483052253723145
    },
    {
      "epoch": 0.8107317073170732,
      "step": 14958,
      "training_loss": 6.163080215454102
    },
    {
      "epoch": 0.8107859078590786,
      "step": 14959,
      "training_loss": 5.611883640289307
    },
    {
      "epoch": 0.810840108401084,
      "grad_norm": 26.807512283325195,
      "learning_rate": 1e-05,
      "loss": 6.1011,
      "step": 14960
    },
    {
      "epoch": 0.810840108401084,
      "step": 14960,
      "training_loss": 7.244281768798828
    },
    {
      "epoch": 0.8108943089430894,
      "step": 14961,
      "training_loss": 7.100048542022705
    },
    {
      "epoch": 0.8109485094850949,
      "step": 14962,
      "training_loss": 4.665862560272217
    },
    {
      "epoch": 0.8110027100271002,
      "step": 14963,
      "training_loss": 6.5395684242248535
    },
    {
      "epoch": 0.8110569105691057,
      "grad_norm": 28.193572998046875,
      "learning_rate": 1e-05,
      "loss": 6.3874,
      "step": 14964
    },
    {
      "epoch": 0.8110569105691057,
      "step": 14964,
      "training_loss": 4.0644989013671875
    },
    {
      "epoch": 0.8111111111111111,
      "step": 14965,
      "training_loss": 5.615253448486328
    },
    {
      "epoch": 0.8111653116531166,
      "step": 14966,
      "training_loss": 6.297740936279297
    },
    {
      "epoch": 0.811219512195122,
      "step": 14967,
      "training_loss": 7.527111530303955
    },
    {
      "epoch": 0.8112737127371273,
      "grad_norm": 28.075904846191406,
      "learning_rate": 1e-05,
      "loss": 5.8762,
      "step": 14968
    },
    {
      "epoch": 0.8112737127371273,
      "step": 14968,
      "training_loss": 6.90001106262207
    },
    {
      "epoch": 0.8113279132791328,
      "step": 14969,
      "training_loss": 4.544368267059326
    },
    {
      "epoch": 0.8113821138211382,
      "step": 14970,
      "training_loss": 7.948348522186279
    },
    {
      "epoch": 0.8114363143631437,
      "step": 14971,
      "training_loss": 7.119556427001953
    },
    {
      "epoch": 0.811490514905149,
      "grad_norm": 29.487817764282227,
      "learning_rate": 1e-05,
      "loss": 6.6281,
      "step": 14972
    },
    {
      "epoch": 0.811490514905149,
      "step": 14972,
      "training_loss": 6.1142191886901855
    },
    {
      "epoch": 0.8115447154471545,
      "step": 14973,
      "training_loss": 4.465585231781006
    },
    {
      "epoch": 0.8115989159891599,
      "step": 14974,
      "training_loss": 5.680897235870361
    },
    {
      "epoch": 0.8116531165311653,
      "step": 14975,
      "training_loss": 5.420836925506592
    },
    {
      "epoch": 0.8117073170731708,
      "grad_norm": 21.27403450012207,
      "learning_rate": 1e-05,
      "loss": 5.4204,
      "step": 14976
    },
    {
      "epoch": 0.8117073170731708,
      "step": 14976,
      "training_loss": 6.200204372406006
    },
    {
      "epoch": 0.8117615176151761,
      "step": 14977,
      "training_loss": 5.950356483459473
    },
    {
      "epoch": 0.8118157181571816,
      "step": 14978,
      "training_loss": 6.861560821533203
    },
    {
      "epoch": 0.811869918699187,
      "step": 14979,
      "training_loss": 7.218624591827393
    },
    {
      "epoch": 0.8119241192411925,
      "grad_norm": 23.663814544677734,
      "learning_rate": 1e-05,
      "loss": 6.5577,
      "step": 14980
    },
    {
      "epoch": 0.8119241192411925,
      "step": 14980,
      "training_loss": 6.4446635246276855
    },
    {
      "epoch": 0.8119783197831978,
      "step": 14981,
      "training_loss": 6.005152225494385
    },
    {
      "epoch": 0.8120325203252032,
      "step": 14982,
      "training_loss": 5.087308406829834
    },
    {
      "epoch": 0.8120867208672087,
      "step": 14983,
      "training_loss": 6.152413845062256
    },
    {
      "epoch": 0.8121409214092141,
      "grad_norm": 33.56725311279297,
      "learning_rate": 1e-05,
      "loss": 5.9224,
      "step": 14984
    },
    {
      "epoch": 0.8121409214092141,
      "step": 14984,
      "training_loss": 7.087810516357422
    },
    {
      "epoch": 0.8121951219512196,
      "step": 14985,
      "training_loss": 6.336535930633545
    },
    {
      "epoch": 0.8122493224932249,
      "step": 14986,
      "training_loss": 5.799411296844482
    },
    {
      "epoch": 0.8123035230352303,
      "step": 14987,
      "training_loss": 7.308919906616211
    },
    {
      "epoch": 0.8123577235772358,
      "grad_norm": 18.97059440612793,
      "learning_rate": 1e-05,
      "loss": 6.6332,
      "step": 14988
    },
    {
      "epoch": 0.8123577235772358,
      "step": 14988,
      "training_loss": 6.437302112579346
    },
    {
      "epoch": 0.8124119241192412,
      "step": 14989,
      "training_loss": 9.717697143554688
    },
    {
      "epoch": 0.8124661246612466,
      "step": 14990,
      "training_loss": 6.713078498840332
    },
    {
      "epoch": 0.812520325203252,
      "step": 14991,
      "training_loss": 6.96733283996582
    },
    {
      "epoch": 0.8125745257452575,
      "grad_norm": 16.293100357055664,
      "learning_rate": 1e-05,
      "loss": 7.4589,
      "step": 14992
    },
    {
      "epoch": 0.8125745257452575,
      "step": 14992,
      "training_loss": 3.7908637523651123
    },
    {
      "epoch": 0.8126287262872629,
      "step": 14993,
      "training_loss": 6.790587425231934
    },
    {
      "epoch": 0.8126829268292682,
      "step": 14994,
      "training_loss": 6.56951379776001
    },
    {
      "epoch": 0.8127371273712737,
      "step": 14995,
      "training_loss": 7.168982982635498
    },
    {
      "epoch": 0.8127913279132791,
      "grad_norm": 19.95266342163086,
      "learning_rate": 1e-05,
      "loss": 6.08,
      "step": 14996
    },
    {
      "epoch": 0.8127913279132791,
      "step": 14996,
      "training_loss": 6.540103912353516
    },
    {
      "epoch": 0.8128455284552846,
      "step": 14997,
      "training_loss": 6.5953688621521
    },
    {
      "epoch": 0.81289972899729,
      "step": 14998,
      "training_loss": 5.860419273376465
    },
    {
      "epoch": 0.8129539295392954,
      "step": 14999,
      "training_loss": 7.105055809020996
    },
    {
      "epoch": 0.8130081300813008,
      "grad_norm": 23.90735626220703,
      "learning_rate": 1e-05,
      "loss": 6.5252,
      "step": 15000
    },
    {
      "epoch": 0.8130081300813008,
      "step": 15000,
      "training_loss": 6.408420085906982
    },
    {
      "epoch": 0.8130623306233062,
      "step": 15001,
      "training_loss": 5.700494766235352
    },
    {
      "epoch": 0.8131165311653117,
      "step": 15002,
      "training_loss": 5.429967880249023
    },
    {
      "epoch": 0.813170731707317,
      "step": 15003,
      "training_loss": 6.685116291046143
    },
    {
      "epoch": 0.8132249322493225,
      "grad_norm": 20.36429786682129,
      "learning_rate": 1e-05,
      "loss": 6.056,
      "step": 15004
    },
    {
      "epoch": 0.8132249322493225,
      "step": 15004,
      "training_loss": 6.322824478149414
    },
    {
      "epoch": 0.8132791327913279,
      "step": 15005,
      "training_loss": 7.787909507751465
    },
    {
      "epoch": 0.8133333333333334,
      "step": 15006,
      "training_loss": 5.944018840789795
    },
    {
      "epoch": 0.8133875338753388,
      "step": 15007,
      "training_loss": 6.306685447692871
    },
    {
      "epoch": 0.8134417344173441,
      "grad_norm": 31.46767807006836,
      "learning_rate": 1e-05,
      "loss": 6.5904,
      "step": 15008
    },
    {
      "epoch": 0.8134417344173441,
      "step": 15008,
      "training_loss": 7.9038920402526855
    },
    {
      "epoch": 0.8134959349593496,
      "step": 15009,
      "training_loss": 3.965196371078491
    },
    {
      "epoch": 0.813550135501355,
      "step": 15010,
      "training_loss": 7.3858795166015625
    },
    {
      "epoch": 0.8136043360433605,
      "step": 15011,
      "training_loss": 7.116854667663574
    },
    {
      "epoch": 0.8136585365853658,
      "grad_norm": 41.00196075439453,
      "learning_rate": 1e-05,
      "loss": 6.593,
      "step": 15012
    },
    {
      "epoch": 0.8136585365853658,
      "step": 15012,
      "training_loss": 6.807883262634277
    },
    {
      "epoch": 0.8137127371273712,
      "step": 15013,
      "training_loss": 4.552911281585693
    },
    {
      "epoch": 0.8137669376693767,
      "step": 15014,
      "training_loss": 3.198030710220337
    },
    {
      "epoch": 0.8138211382113821,
      "step": 15015,
      "training_loss": 5.838141441345215
    },
    {
      "epoch": 0.8138753387533876,
      "grad_norm": 29.707294464111328,
      "learning_rate": 1e-05,
      "loss": 5.0992,
      "step": 15016
    },
    {
      "epoch": 0.8138753387533876,
      "step": 15016,
      "training_loss": 7.355268955230713
    },
    {
      "epoch": 0.8139295392953929,
      "step": 15017,
      "training_loss": 6.858585834503174
    },
    {
      "epoch": 0.8139837398373984,
      "step": 15018,
      "training_loss": 6.899465560913086
    },
    {
      "epoch": 0.8140379403794038,
      "step": 15019,
      "training_loss": 5.321177959442139
    },
    {
      "epoch": 0.8140921409214092,
      "grad_norm": 54.375789642333984,
      "learning_rate": 1e-05,
      "loss": 6.6086,
      "step": 15020
    },
    {
      "epoch": 0.8140921409214092,
      "step": 15020,
      "training_loss": 6.081855297088623
    },
    {
      "epoch": 0.8141463414634146,
      "step": 15021,
      "training_loss": 5.901139736175537
    },
    {
      "epoch": 0.81420054200542,
      "step": 15022,
      "training_loss": 3.882174491882324
    },
    {
      "epoch": 0.8142547425474255,
      "step": 15023,
      "training_loss": 6.771279335021973
    },
    {
      "epoch": 0.8143089430894309,
      "grad_norm": 22.36758041381836,
      "learning_rate": 1e-05,
      "loss": 5.6591,
      "step": 15024
    },
    {
      "epoch": 0.8143089430894309,
      "step": 15024,
      "training_loss": 6.235650539398193
    },
    {
      "epoch": 0.8143631436314364,
      "step": 15025,
      "training_loss": 6.803570747375488
    },
    {
      "epoch": 0.8144173441734417,
      "step": 15026,
      "training_loss": 6.803022861480713
    },
    {
      "epoch": 0.8144715447154471,
      "step": 15027,
      "training_loss": 7.933150291442871
    },
    {
      "epoch": 0.8145257452574526,
      "grad_norm": 51.41715621948242,
      "learning_rate": 1e-05,
      "loss": 6.9438,
      "step": 15028
    },
    {
      "epoch": 0.8145257452574526,
      "step": 15028,
      "training_loss": 7.304266929626465
    },
    {
      "epoch": 0.814579945799458,
      "step": 15029,
      "training_loss": 6.1158671379089355
    },
    {
      "epoch": 0.8146341463414634,
      "step": 15030,
      "training_loss": 6.643702507019043
    },
    {
      "epoch": 0.8146883468834688,
      "step": 15031,
      "training_loss": 4.514025688171387
    },
    {
      "epoch": 0.8147425474254743,
      "grad_norm": 64.98109436035156,
      "learning_rate": 1e-05,
      "loss": 6.1445,
      "step": 15032
    },
    {
      "epoch": 0.8147425474254743,
      "step": 15032,
      "training_loss": 6.2606282234191895
    },
    {
      "epoch": 0.8147967479674797,
      "step": 15033,
      "training_loss": 6.456152439117432
    },
    {
      "epoch": 0.8148509485094851,
      "step": 15034,
      "training_loss": 7.098824977874756
    },
    {
      "epoch": 0.8149051490514905,
      "step": 15035,
      "training_loss": 5.009393215179443
    },
    {
      "epoch": 0.8149593495934959,
      "grad_norm": 63.500892639160156,
      "learning_rate": 1e-05,
      "loss": 6.2062,
      "step": 15036
    },
    {
      "epoch": 0.8149593495934959,
      "step": 15036,
      "training_loss": 7.592599391937256
    },
    {
      "epoch": 0.8150135501355014,
      "step": 15037,
      "training_loss": 7.42153263092041
    },
    {
      "epoch": 0.8150677506775068,
      "step": 15038,
      "training_loss": 7.829047679901123
    },
    {
      "epoch": 0.8151219512195121,
      "step": 15039,
      "training_loss": 3.996891975402832
    },
    {
      "epoch": 0.8151761517615176,
      "grad_norm": 24.200727462768555,
      "learning_rate": 1e-05,
      "loss": 6.71,
      "step": 15040
    },
    {
      "epoch": 0.8151761517615176,
      "step": 15040,
      "training_loss": 6.457491397857666
    },
    {
      "epoch": 0.815230352303523,
      "step": 15041,
      "training_loss": 5.478900909423828
    },
    {
      "epoch": 0.8152845528455285,
      "step": 15042,
      "training_loss": 6.98536491394043
    },
    {
      "epoch": 0.8153387533875339,
      "step": 15043,
      "training_loss": 9.135004997253418
    },
    {
      "epoch": 0.8153929539295393,
      "grad_norm": 77.52745056152344,
      "learning_rate": 1e-05,
      "loss": 7.0142,
      "step": 15044
    },
    {
      "epoch": 0.8153929539295393,
      "step": 15044,
      "training_loss": 4.70219087600708
    },
    {
      "epoch": 0.8154471544715447,
      "step": 15045,
      "training_loss": 7.200711250305176
    },
    {
      "epoch": 0.8155013550135501,
      "step": 15046,
      "training_loss": 6.6561431884765625
    },
    {
      "epoch": 0.8155555555555556,
      "step": 15047,
      "training_loss": 5.710845470428467
    },
    {
      "epoch": 0.8156097560975609,
      "grad_norm": 27.614871978759766,
      "learning_rate": 1e-05,
      "loss": 6.0675,
      "step": 15048
    },
    {
      "epoch": 0.8156097560975609,
      "step": 15048,
      "training_loss": 6.396889686584473
    },
    {
      "epoch": 0.8156639566395664,
      "step": 15049,
      "training_loss": 7.273136138916016
    },
    {
      "epoch": 0.8157181571815718,
      "step": 15050,
      "training_loss": 5.567731857299805
    },
    {
      "epoch": 0.8157723577235773,
      "step": 15051,
      "training_loss": 6.317819595336914
    },
    {
      "epoch": 0.8158265582655827,
      "grad_norm": 25.75861930847168,
      "learning_rate": 1e-05,
      "loss": 6.3889,
      "step": 15052
    },
    {
      "epoch": 0.8158265582655827,
      "step": 15052,
      "training_loss": 6.505542755126953
    },
    {
      "epoch": 0.815880758807588,
      "step": 15053,
      "training_loss": 6.874470233917236
    },
    {
      "epoch": 0.8159349593495935,
      "step": 15054,
      "training_loss": 5.355279922485352
    },
    {
      "epoch": 0.8159891598915989,
      "step": 15055,
      "training_loss": 7.286659240722656
    },
    {
      "epoch": 0.8160433604336044,
      "grad_norm": 23.893112182617188,
      "learning_rate": 1e-05,
      "loss": 6.5055,
      "step": 15056
    },
    {
      "epoch": 0.8160433604336044,
      "step": 15056,
      "training_loss": 6.847135066986084
    },
    {
      "epoch": 0.8160975609756097,
      "step": 15057,
      "training_loss": 6.534045696258545
    },
    {
      "epoch": 0.8161517615176151,
      "step": 15058,
      "training_loss": 3.770345449447632
    },
    {
      "epoch": 0.8162059620596206,
      "step": 15059,
      "training_loss": 7.871272087097168
    },
    {
      "epoch": 0.816260162601626,
      "grad_norm": 49.937232971191406,
      "learning_rate": 1e-05,
      "loss": 6.2557,
      "step": 15060
    },
    {
      "epoch": 0.816260162601626,
      "step": 15060,
      "training_loss": 6.2280473709106445
    },
    {
      "epoch": 0.8163143631436315,
      "step": 15061,
      "training_loss": 7.619204521179199
    },
    {
      "epoch": 0.8163685636856368,
      "step": 15062,
      "training_loss": 3.5848286151885986
    },
    {
      "epoch": 0.8164227642276423,
      "step": 15063,
      "training_loss": 6.940415859222412
    },
    {
      "epoch": 0.8164769647696477,
      "grad_norm": 39.750511169433594,
      "learning_rate": 1e-05,
      "loss": 6.0931,
      "step": 15064
    },
    {
      "epoch": 0.8164769647696477,
      "step": 15064,
      "training_loss": 7.48705530166626
    },
    {
      "epoch": 0.8165311653116532,
      "step": 15065,
      "training_loss": 6.8829755783081055
    },
    {
      "epoch": 0.8165853658536585,
      "step": 15066,
      "training_loss": 6.838427543640137
    },
    {
      "epoch": 0.8166395663956639,
      "step": 15067,
      "training_loss": 5.985434055328369
    },
    {
      "epoch": 0.8166937669376694,
      "grad_norm": 40.89513397216797,
      "learning_rate": 1e-05,
      "loss": 6.7985,
      "step": 15068
    },
    {
      "epoch": 0.8166937669376694,
      "step": 15068,
      "training_loss": 6.454473495483398
    },
    {
      "epoch": 0.8167479674796748,
      "step": 15069,
      "training_loss": 6.813220977783203
    },
    {
      "epoch": 0.8168021680216803,
      "step": 15070,
      "training_loss": 5.548939228057861
    },
    {
      "epoch": 0.8168563685636856,
      "step": 15071,
      "training_loss": 6.195763111114502
    },
    {
      "epoch": 0.816910569105691,
      "grad_norm": 32.08122634887695,
      "learning_rate": 1e-05,
      "loss": 6.2531,
      "step": 15072
    },
    {
      "epoch": 0.816910569105691,
      "step": 15072,
      "training_loss": 5.840585708618164
    },
    {
      "epoch": 0.8169647696476965,
      "step": 15073,
      "training_loss": 5.830719947814941
    },
    {
      "epoch": 0.8170189701897019,
      "step": 15074,
      "training_loss": 6.723170280456543
    },
    {
      "epoch": 0.8170731707317073,
      "step": 15075,
      "training_loss": 6.974402904510498
    },
    {
      "epoch": 0.8171273712737127,
      "grad_norm": 28.732919692993164,
      "learning_rate": 1e-05,
      "loss": 6.3422,
      "step": 15076
    },
    {
      "epoch": 0.8171273712737127,
      "step": 15076,
      "training_loss": 6.534336566925049
    },
    {
      "epoch": 0.8171815718157182,
      "step": 15077,
      "training_loss": 5.924095153808594
    },
    {
      "epoch": 0.8172357723577236,
      "step": 15078,
      "training_loss": 6.332099437713623
    },
    {
      "epoch": 0.817289972899729,
      "step": 15079,
      "training_loss": 7.13305139541626
    },
    {
      "epoch": 0.8173441734417344,
      "grad_norm": 25.245718002319336,
      "learning_rate": 1e-05,
      "loss": 6.4809,
      "step": 15080
    },
    {
      "epoch": 0.8173441734417344,
      "step": 15080,
      "training_loss": 5.254356384277344
    },
    {
      "epoch": 0.8173983739837398,
      "step": 15081,
      "training_loss": 6.275816917419434
    },
    {
      "epoch": 0.8174525745257453,
      "step": 15082,
      "training_loss": 4.579875946044922
    },
    {
      "epoch": 0.8175067750677507,
      "step": 15083,
      "training_loss": 6.116694927215576
    },
    {
      "epoch": 0.817560975609756,
      "grad_norm": 31.674707412719727,
      "learning_rate": 1e-05,
      "loss": 5.5567,
      "step": 15084
    },
    {
      "epoch": 0.817560975609756,
      "step": 15084,
      "training_loss": 7.463564395904541
    },
    {
      "epoch": 0.8176151761517615,
      "step": 15085,
      "training_loss": 7.2740864753723145
    },
    {
      "epoch": 0.8176693766937669,
      "step": 15086,
      "training_loss": 5.718423366546631
    },
    {
      "epoch": 0.8177235772357724,
      "step": 15087,
      "training_loss": 6.638707637786865
    },
    {
      "epoch": 0.8177777777777778,
      "grad_norm": 22.05927848815918,
      "learning_rate": 1e-05,
      "loss": 6.7737,
      "step": 15088
    },
    {
      "epoch": 0.8177777777777778,
      "step": 15088,
      "training_loss": 4.372653484344482
    },
    {
      "epoch": 0.8178319783197832,
      "step": 15089,
      "training_loss": 7.736753463745117
    },
    {
      "epoch": 0.8178861788617886,
      "step": 15090,
      "training_loss": 6.745465278625488
    },
    {
      "epoch": 0.817940379403794,
      "step": 15091,
      "training_loss": 7.977668285369873
    },
    {
      "epoch": 0.8179945799457995,
      "grad_norm": 23.711214065551758,
      "learning_rate": 1e-05,
      "loss": 6.7081,
      "step": 15092
    },
    {
      "epoch": 0.8179945799457995,
      "step": 15092,
      "training_loss": 6.6223907470703125
    },
    {
      "epoch": 0.8180487804878048,
      "step": 15093,
      "training_loss": 5.582380771636963
    },
    {
      "epoch": 0.8181029810298103,
      "step": 15094,
      "training_loss": 5.367928981781006
    },
    {
      "epoch": 0.8181571815718157,
      "step": 15095,
      "training_loss": 6.3769121170043945
    },
    {
      "epoch": 0.8182113821138212,
      "grad_norm": 41.3347282409668,
      "learning_rate": 1e-05,
      "loss": 5.9874,
      "step": 15096
    },
    {
      "epoch": 0.8182113821138212,
      "step": 15096,
      "training_loss": 6.72174596786499
    },
    {
      "epoch": 0.8182655826558266,
      "step": 15097,
      "training_loss": 5.441403388977051
    },
    {
      "epoch": 0.8183197831978319,
      "step": 15098,
      "training_loss": 7.920594692230225
    },
    {
      "epoch": 0.8183739837398374,
      "step": 15099,
      "training_loss": 4.900649070739746
    },
    {
      "epoch": 0.8184281842818428,
      "grad_norm": 39.65675354003906,
      "learning_rate": 1e-05,
      "loss": 6.2461,
      "step": 15100
    },
    {
      "epoch": 0.8184281842818428,
      "step": 15100,
      "training_loss": 6.542311668395996
    },
    {
      "epoch": 0.8184823848238483,
      "step": 15101,
      "training_loss": 8.036849975585938
    },
    {
      "epoch": 0.8185365853658536,
      "step": 15102,
      "training_loss": 6.056587219238281
    },
    {
      "epoch": 0.818590785907859,
      "step": 15103,
      "training_loss": 7.442596435546875
    },
    {
      "epoch": 0.8186449864498645,
      "grad_norm": 26.175601959228516,
      "learning_rate": 1e-05,
      "loss": 7.0196,
      "step": 15104
    },
    {
      "epoch": 0.8186449864498645,
      "step": 15104,
      "training_loss": 6.311328887939453
    },
    {
      "epoch": 0.8186991869918699,
      "step": 15105,
      "training_loss": 4.410853385925293
    },
    {
      "epoch": 0.8187533875338754,
      "step": 15106,
      "training_loss": 5.8461151123046875
    },
    {
      "epoch": 0.8188075880758807,
      "step": 15107,
      "training_loss": 5.298717498779297
    },
    {
      "epoch": 0.8188617886178862,
      "grad_norm": 40.119319915771484,
      "learning_rate": 1e-05,
      "loss": 5.4668,
      "step": 15108
    },
    {
      "epoch": 0.8188617886178862,
      "step": 15108,
      "training_loss": 6.842068195343018
    },
    {
      "epoch": 0.8189159891598916,
      "step": 15109,
      "training_loss": 5.081421375274658
    },
    {
      "epoch": 0.8189701897018971,
      "step": 15110,
      "training_loss": 4.861854076385498
    },
    {
      "epoch": 0.8190243902439024,
      "step": 15111,
      "training_loss": 5.7510528564453125
    },
    {
      "epoch": 0.8190785907859078,
      "grad_norm": 27.030717849731445,
      "learning_rate": 1e-05,
      "loss": 5.6341,
      "step": 15112
    },
    {
      "epoch": 0.8190785907859078,
      "step": 15112,
      "training_loss": 5.068080425262451
    },
    {
      "epoch": 0.8191327913279133,
      "step": 15113,
      "training_loss": 6.377302169799805
    },
    {
      "epoch": 0.8191869918699187,
      "step": 15114,
      "training_loss": 7.365746021270752
    },
    {
      "epoch": 0.8192411924119242,
      "step": 15115,
      "training_loss": 7.1236772537231445
    },
    {
      "epoch": 0.8192953929539295,
      "grad_norm": 46.868873596191406,
      "learning_rate": 1e-05,
      "loss": 6.4837,
      "step": 15116
    },
    {
      "epoch": 0.8192953929539295,
      "step": 15116,
      "training_loss": 6.856049537658691
    },
    {
      "epoch": 0.819349593495935,
      "step": 15117,
      "training_loss": 6.10876989364624
    },
    {
      "epoch": 0.8194037940379404,
      "step": 15118,
      "training_loss": 6.300164699554443
    },
    {
      "epoch": 0.8194579945799458,
      "step": 15119,
      "training_loss": 6.625673770904541
    },
    {
      "epoch": 0.8195121951219512,
      "grad_norm": 19.043886184692383,
      "learning_rate": 1e-05,
      "loss": 6.4727,
      "step": 15120
    },
    {
      "epoch": 0.8195121951219512,
      "step": 15120,
      "training_loss": 6.592048645019531
    },
    {
      "epoch": 0.8195663956639566,
      "step": 15121,
      "training_loss": 7.0551300048828125
    },
    {
      "epoch": 0.8196205962059621,
      "step": 15122,
      "training_loss": 10.817181587219238
    },
    {
      "epoch": 0.8196747967479675,
      "step": 15123,
      "training_loss": 6.083179950714111
    },
    {
      "epoch": 0.819728997289973,
      "grad_norm": 49.521968841552734,
      "learning_rate": 1e-05,
      "loss": 7.6369,
      "step": 15124
    },
    {
      "epoch": 0.819728997289973,
      "step": 15124,
      "training_loss": 9.140944480895996
    },
    {
      "epoch": 0.8197831978319783,
      "step": 15125,
      "training_loss": 8.25582218170166
    },
    {
      "epoch": 0.8198373983739837,
      "step": 15126,
      "training_loss": 6.548459053039551
    },
    {
      "epoch": 0.8198915989159892,
      "step": 15127,
      "training_loss": 7.326089859008789
    },
    {
      "epoch": 0.8199457994579946,
      "grad_norm": 23.554758071899414,
      "learning_rate": 1e-05,
      "loss": 7.8178,
      "step": 15128
    },
    {
      "epoch": 0.8199457994579946,
      "step": 15128,
      "training_loss": 6.2507853507995605
    },
    {
      "epoch": 0.82,
      "step": 15129,
      "training_loss": 6.429708957672119
    },
    {
      "epoch": 0.8200542005420054,
      "step": 15130,
      "training_loss": 6.049013614654541
    },
    {
      "epoch": 0.8201084010840108,
      "step": 15131,
      "training_loss": 5.560832500457764
    },
    {
      "epoch": 0.8201626016260163,
      "grad_norm": 36.67006301879883,
      "learning_rate": 1e-05,
      "loss": 6.0726,
      "step": 15132
    },
    {
      "epoch": 0.8201626016260163,
      "step": 15132,
      "training_loss": 7.15467643737793
    },
    {
      "epoch": 0.8202168021680217,
      "step": 15133,
      "training_loss": 4.792187213897705
    },
    {
      "epoch": 0.8202710027100271,
      "step": 15134,
      "training_loss": 7.030564785003662
    },
    {
      "epoch": 0.8203252032520325,
      "step": 15135,
      "training_loss": 6.513270378112793
    },
    {
      "epoch": 0.820379403794038,
      "grad_norm": 30.23285675048828,
      "learning_rate": 1e-05,
      "loss": 6.3727,
      "step": 15136
    },
    {
      "epoch": 0.820379403794038,
      "step": 15136,
      "training_loss": 6.343985080718994
    },
    {
      "epoch": 0.8204336043360434,
      "step": 15137,
      "training_loss": 8.274873733520508
    },
    {
      "epoch": 0.8204878048780487,
      "step": 15138,
      "training_loss": 5.995317459106445
    },
    {
      "epoch": 0.8205420054200542,
      "step": 15139,
      "training_loss": 7.425985336303711
    },
    {
      "epoch": 0.8205962059620596,
      "grad_norm": 33.20625686645508,
      "learning_rate": 1e-05,
      "loss": 7.01,
      "step": 15140
    },
    {
      "epoch": 0.8205962059620596,
      "step": 15140,
      "training_loss": 6.872392654418945
    },
    {
      "epoch": 0.8206504065040651,
      "step": 15141,
      "training_loss": 7.079603672027588
    },
    {
      "epoch": 0.8207046070460705,
      "step": 15142,
      "training_loss": 7.499279975891113
    },
    {
      "epoch": 0.8207588075880758,
      "step": 15143,
      "training_loss": 7.036848545074463
    },
    {
      "epoch": 0.8208130081300813,
      "grad_norm": 17.68274688720703,
      "learning_rate": 1e-05,
      "loss": 7.122,
      "step": 15144
    },
    {
      "epoch": 0.8208130081300813,
      "step": 15144,
      "training_loss": 4.764542102813721
    },
    {
      "epoch": 0.8208672086720867,
      "step": 15145,
      "training_loss": 5.212093353271484
    },
    {
      "epoch": 0.8209214092140922,
      "step": 15146,
      "training_loss": 7.134737491607666
    },
    {
      "epoch": 0.8209756097560975,
      "step": 15147,
      "training_loss": 7.874239921569824
    },
    {
      "epoch": 0.821029810298103,
      "grad_norm": 79.2315902709961,
      "learning_rate": 1e-05,
      "loss": 6.2464,
      "step": 15148
    },
    {
      "epoch": 0.821029810298103,
      "step": 15148,
      "training_loss": 5.756607532501221
    },
    {
      "epoch": 0.8210840108401084,
      "step": 15149,
      "training_loss": 6.9372124671936035
    },
    {
      "epoch": 0.8211382113821138,
      "step": 15150,
      "training_loss": 6.512606143951416
    },
    {
      "epoch": 0.8211924119241193,
      "step": 15151,
      "training_loss": 3.4306490421295166
    },
    {
      "epoch": 0.8212466124661246,
      "grad_norm": 50.734920501708984,
      "learning_rate": 1e-05,
      "loss": 5.6593,
      "step": 15152
    },
    {
      "epoch": 0.8212466124661246,
      "step": 15152,
      "training_loss": 4.190794467926025
    },
    {
      "epoch": 0.8213008130081301,
      "step": 15153,
      "training_loss": 6.047425270080566
    },
    {
      "epoch": 0.8213550135501355,
      "step": 15154,
      "training_loss": 7.61658239364624
    },
    {
      "epoch": 0.821409214092141,
      "step": 15155,
      "training_loss": 6.938338279724121
    },
    {
      "epoch": 0.8214634146341463,
      "grad_norm": 23.82923698425293,
      "learning_rate": 1e-05,
      "loss": 6.1983,
      "step": 15156
    },
    {
      "epoch": 0.8214634146341463,
      "step": 15156,
      "training_loss": 7.639659404754639
    },
    {
      "epoch": 0.8215176151761517,
      "step": 15157,
      "training_loss": 6.554764747619629
    },
    {
      "epoch": 0.8215718157181572,
      "step": 15158,
      "training_loss": 6.322889804840088
    },
    {
      "epoch": 0.8216260162601626,
      "step": 15159,
      "training_loss": 3.793731451034546
    },
    {
      "epoch": 0.8216802168021681,
      "grad_norm": 38.026981353759766,
      "learning_rate": 1e-05,
      "loss": 6.0778,
      "step": 15160
    },
    {
      "epoch": 0.8216802168021681,
      "step": 15160,
      "training_loss": 8.11868667602539
    },
    {
      "epoch": 0.8217344173441734,
      "step": 15161,
      "training_loss": 6.793417930603027
    },
    {
      "epoch": 0.8217886178861789,
      "step": 15162,
      "training_loss": 4.522231101989746
    },
    {
      "epoch": 0.8218428184281843,
      "step": 15163,
      "training_loss": 5.238875389099121
    },
    {
      "epoch": 0.8218970189701897,
      "grad_norm": 58.84799575805664,
      "learning_rate": 1e-05,
      "loss": 6.1683,
      "step": 15164
    },
    {
      "epoch": 0.8218970189701897,
      "step": 15164,
      "training_loss": 5.983391284942627
    },
    {
      "epoch": 0.8219512195121951,
      "step": 15165,
      "training_loss": 6.7290449142456055
    },
    {
      "epoch": 0.8220054200542005,
      "step": 15166,
      "training_loss": 6.262603759765625
    },
    {
      "epoch": 0.822059620596206,
      "step": 15167,
      "training_loss": 6.6404008865356445
    },
    {
      "epoch": 0.8221138211382114,
      "grad_norm": 21.70720863342285,
      "learning_rate": 1e-05,
      "loss": 6.4039,
      "step": 15168
    },
    {
      "epoch": 0.8221138211382114,
      "step": 15168,
      "training_loss": 6.1820173263549805
    },
    {
      "epoch": 0.8221680216802169,
      "step": 15169,
      "training_loss": 7.327333927154541
    },
    {
      "epoch": 0.8222222222222222,
      "step": 15170,
      "training_loss": 7.195369720458984
    },
    {
      "epoch": 0.8222764227642276,
      "step": 15171,
      "training_loss": 6.123244762420654
    },
    {
      "epoch": 0.8223306233062331,
      "grad_norm": 25.16853904724121,
      "learning_rate": 1e-05,
      "loss": 6.707,
      "step": 15172
    },
    {
      "epoch": 0.8223306233062331,
      "step": 15172,
      "training_loss": 6.2141313552856445
    },
    {
      "epoch": 0.8223848238482385,
      "step": 15173,
      "training_loss": 8.969803810119629
    },
    {
      "epoch": 0.8224390243902439,
      "step": 15174,
      "training_loss": 7.642298221588135
    },
    {
      "epoch": 0.8224932249322493,
      "step": 15175,
      "training_loss": 7.336997032165527
    },
    {
      "epoch": 0.8225474254742547,
      "grad_norm": 18.429166793823242,
      "learning_rate": 1e-05,
      "loss": 7.5408,
      "step": 15176
    },
    {
      "epoch": 0.8225474254742547,
      "step": 15176,
      "training_loss": 7.62852144241333
    },
    {
      "epoch": 0.8226016260162602,
      "step": 15177,
      "training_loss": 6.525424957275391
    },
    {
      "epoch": 0.8226558265582656,
      "step": 15178,
      "training_loss": 6.4816484451293945
    },
    {
      "epoch": 0.822710027100271,
      "step": 15179,
      "training_loss": 6.730959892272949
    },
    {
      "epoch": 0.8227642276422764,
      "grad_norm": 41.8432502746582,
      "learning_rate": 1e-05,
      "loss": 6.8416,
      "step": 15180
    },
    {
      "epoch": 0.8227642276422764,
      "step": 15180,
      "training_loss": 6.0887227058410645
    },
    {
      "epoch": 0.8228184281842819,
      "step": 15181,
      "training_loss": 6.970793724060059
    },
    {
      "epoch": 0.8228726287262873,
      "step": 15182,
      "training_loss": 6.0260186195373535
    },
    {
      "epoch": 0.8229268292682926,
      "step": 15183,
      "training_loss": 5.03379487991333
    },
    {
      "epoch": 0.8229810298102981,
      "grad_norm": 20.0217342376709,
      "learning_rate": 1e-05,
      "loss": 6.0298,
      "step": 15184
    },
    {
      "epoch": 0.8229810298102981,
      "step": 15184,
      "training_loss": 3.0601813793182373
    },
    {
      "epoch": 0.8230352303523035,
      "step": 15185,
      "training_loss": 7.210732460021973
    },
    {
      "epoch": 0.823089430894309,
      "step": 15186,
      "training_loss": 4.961409568786621
    },
    {
      "epoch": 0.8231436314363144,
      "step": 15187,
      "training_loss": 6.37224006652832
    },
    {
      "epoch": 0.8231978319783197,
      "grad_norm": 25.894563674926758,
      "learning_rate": 1e-05,
      "loss": 5.4011,
      "step": 15188
    },
    {
      "epoch": 0.8231978319783197,
      "step": 15188,
      "training_loss": 6.992031574249268
    },
    {
      "epoch": 0.8232520325203252,
      "step": 15189,
      "training_loss": 7.110368728637695
    },
    {
      "epoch": 0.8233062330623306,
      "step": 15190,
      "training_loss": 5.714568614959717
    },
    {
      "epoch": 0.8233604336043361,
      "step": 15191,
      "training_loss": 6.384654521942139
    },
    {
      "epoch": 0.8234146341463414,
      "grad_norm": 24.749279022216797,
      "learning_rate": 1e-05,
      "loss": 6.5504,
      "step": 15192
    },
    {
      "epoch": 0.8234146341463414,
      "step": 15192,
      "training_loss": 7.4145731925964355
    },
    {
      "epoch": 0.8234688346883469,
      "step": 15193,
      "training_loss": 6.1875
    },
    {
      "epoch": 0.8235230352303523,
      "step": 15194,
      "training_loss": 7.131000518798828
    },
    {
      "epoch": 0.8235772357723578,
      "step": 15195,
      "training_loss": 4.505648612976074
    },
    {
      "epoch": 0.8236314363143632,
      "grad_norm": 28.91506576538086,
      "learning_rate": 1e-05,
      "loss": 6.3097,
      "step": 15196
    },
    {
      "epoch": 0.8236314363143632,
      "step": 15196,
      "training_loss": 2.5204241275787354
    },
    {
      "epoch": 0.8236856368563685,
      "step": 15197,
      "training_loss": 5.4327802658081055
    },
    {
      "epoch": 0.823739837398374,
      "step": 15198,
      "training_loss": 7.860067844390869
    },
    {
      "epoch": 0.8237940379403794,
      "step": 15199,
      "training_loss": 6.622036933898926
    },
    {
      "epoch": 0.8238482384823849,
      "grad_norm": 19.96048927307129,
      "learning_rate": 1e-05,
      "loss": 5.6088,
      "step": 15200
    },
    {
      "epoch": 0.8238482384823849,
      "step": 15200,
      "training_loss": 5.147743225097656
    },
    {
      "epoch": 0.8239024390243902,
      "step": 15201,
      "training_loss": 7.642431735992432
    },
    {
      "epoch": 0.8239566395663956,
      "step": 15202,
      "training_loss": 3.1721887588500977
    },
    {
      "epoch": 0.8240108401084011,
      "step": 15203,
      "training_loss": 3.3420331478118896
    },
    {
      "epoch": 0.8240650406504065,
      "grad_norm": 31.408687591552734,
      "learning_rate": 1e-05,
      "loss": 4.8261,
      "step": 15204
    },
    {
      "epoch": 0.8240650406504065,
      "step": 15204,
      "training_loss": 6.135364532470703
    },
    {
      "epoch": 0.824119241192412,
      "step": 15205,
      "training_loss": 4.210056304931641
    },
    {
      "epoch": 0.8241734417344173,
      "step": 15206,
      "training_loss": 8.5924072265625
    },
    {
      "epoch": 0.8242276422764228,
      "step": 15207,
      "training_loss": 5.374790191650391
    },
    {
      "epoch": 0.8242818428184282,
      "grad_norm": 45.861209869384766,
      "learning_rate": 1e-05,
      "loss": 6.0782,
      "step": 15208
    },
    {
      "epoch": 0.8242818428184282,
      "step": 15208,
      "training_loss": 4.977984428405762
    },
    {
      "epoch": 0.8243360433604336,
      "step": 15209,
      "training_loss": 7.121632099151611
    },
    {
      "epoch": 0.824390243902439,
      "step": 15210,
      "training_loss": 7.298011302947998
    },
    {
      "epoch": 0.8244444444444444,
      "step": 15211,
      "training_loss": 6.6975932121276855
    },
    {
      "epoch": 0.8244986449864499,
      "grad_norm": 20.604536056518555,
      "learning_rate": 1e-05,
      "loss": 6.5238,
      "step": 15212
    },
    {
      "epoch": 0.8244986449864499,
      "step": 15212,
      "training_loss": 6.2883195877075195
    },
    {
      "epoch": 0.8245528455284553,
      "step": 15213,
      "training_loss": 6.9428300857543945
    },
    {
      "epoch": 0.8246070460704608,
      "step": 15214,
      "training_loss": 7.153597831726074
    },
    {
      "epoch": 0.8246612466124661,
      "step": 15215,
      "training_loss": 5.677078723907471
    },
    {
      "epoch": 0.8247154471544715,
      "grad_norm": 51.730926513671875,
      "learning_rate": 1e-05,
      "loss": 6.5155,
      "step": 15216
    },
    {
      "epoch": 0.8247154471544715,
      "step": 15216,
      "training_loss": 6.874547481536865
    },
    {
      "epoch": 0.824769647696477,
      "step": 15217,
      "training_loss": 5.865240097045898
    },
    {
      "epoch": 0.8248238482384824,
      "step": 15218,
      "training_loss": 6.797348499298096
    },
    {
      "epoch": 0.8248780487804878,
      "step": 15219,
      "training_loss": 4.965664863586426
    },
    {
      "epoch": 0.8249322493224932,
      "grad_norm": 29.2163028717041,
      "learning_rate": 1e-05,
      "loss": 6.1257,
      "step": 15220
    },
    {
      "epoch": 0.8249322493224932,
      "step": 15220,
      "training_loss": 4.269374370574951
    },
    {
      "epoch": 0.8249864498644986,
      "step": 15221,
      "training_loss": 7.065875053405762
    },
    {
      "epoch": 0.8250406504065041,
      "step": 15222,
      "training_loss": 5.363596439361572
    },
    {
      "epoch": 0.8250948509485095,
      "step": 15223,
      "training_loss": 6.539371013641357
    },
    {
      "epoch": 0.8251490514905149,
      "grad_norm": 25.444217681884766,
      "learning_rate": 1e-05,
      "loss": 5.8096,
      "step": 15224
    },
    {
      "epoch": 0.8251490514905149,
      "step": 15224,
      "training_loss": 6.430372714996338
    },
    {
      "epoch": 0.8252032520325203,
      "step": 15225,
      "training_loss": 6.535390853881836
    },
    {
      "epoch": 0.8252574525745258,
      "step": 15226,
      "training_loss": 7.250349998474121
    },
    {
      "epoch": 0.8253116531165312,
      "step": 15227,
      "training_loss": 3.095794200897217
    },
    {
      "epoch": 0.8253658536585365,
      "grad_norm": 44.98538589477539,
      "learning_rate": 1e-05,
      "loss": 5.828,
      "step": 15228
    },
    {
      "epoch": 0.8253658536585365,
      "step": 15228,
      "training_loss": 5.079056262969971
    },
    {
      "epoch": 0.825420054200542,
      "step": 15229,
      "training_loss": 8.095256805419922
    },
    {
      "epoch": 0.8254742547425474,
      "step": 15230,
      "training_loss": 7.3456830978393555
    },
    {
      "epoch": 0.8255284552845529,
      "step": 15231,
      "training_loss": 5.4538469314575195
    },
    {
      "epoch": 0.8255826558265583,
      "grad_norm": 33.123226165771484,
      "learning_rate": 1e-05,
      "loss": 6.4935,
      "step": 15232
    },
    {
      "epoch": 0.8255826558265583,
      "step": 15232,
      "training_loss": 6.885489463806152
    },
    {
      "epoch": 0.8256368563685637,
      "step": 15233,
      "training_loss": 7.367193698883057
    },
    {
      "epoch": 0.8256910569105691,
      "step": 15234,
      "training_loss": 7.236685752868652
    },
    {
      "epoch": 0.8257452574525745,
      "step": 15235,
      "training_loss": 6.780393123626709
    },
    {
      "epoch": 0.82579945799458,
      "grad_norm": 23.403453826904297,
      "learning_rate": 1e-05,
      "loss": 7.0674,
      "step": 15236
    },
    {
      "epoch": 0.82579945799458,
      "step": 15236,
      "training_loss": 7.499345779418945
    },
    {
      "epoch": 0.8258536585365853,
      "step": 15237,
      "training_loss": 3.3069746494293213
    },
    {
      "epoch": 0.8259078590785908,
      "step": 15238,
      "training_loss": 6.868440628051758
    },
    {
      "epoch": 0.8259620596205962,
      "step": 15239,
      "training_loss": 6.833546161651611
    },
    {
      "epoch": 0.8260162601626017,
      "grad_norm": 23.152957916259766,
      "learning_rate": 1e-05,
      "loss": 6.1271,
      "step": 15240
    },
    {
      "epoch": 0.8260162601626017,
      "step": 15240,
      "training_loss": 4.331010341644287
    },
    {
      "epoch": 0.8260704607046071,
      "step": 15241,
      "training_loss": 4.679677963256836
    },
    {
      "epoch": 0.8261246612466124,
      "step": 15242,
      "training_loss": 4.728426456451416
    },
    {
      "epoch": 0.8261788617886179,
      "step": 15243,
      "training_loss": 7.221513748168945
    },
    {
      "epoch": 0.8262330623306233,
      "grad_norm": 20.330915451049805,
      "learning_rate": 1e-05,
      "loss": 5.2402,
      "step": 15244
    },
    {
      "epoch": 0.8262330623306233,
      "step": 15244,
      "training_loss": 6.513905048370361
    },
    {
      "epoch": 0.8262872628726288,
      "step": 15245,
      "training_loss": 5.720166206359863
    },
    {
      "epoch": 0.8263414634146341,
      "step": 15246,
      "training_loss": 5.010788440704346
    },
    {
      "epoch": 0.8263956639566395,
      "step": 15247,
      "training_loss": 6.4714837074279785
    },
    {
      "epoch": 0.826449864498645,
      "grad_norm": 24.879844665527344,
      "learning_rate": 1e-05,
      "loss": 5.9291,
      "step": 15248
    },
    {
      "epoch": 0.826449864498645,
      "step": 15248,
      "training_loss": 7.08170223236084
    },
    {
      "epoch": 0.8265040650406504,
      "step": 15249,
      "training_loss": 6.920924186706543
    },
    {
      "epoch": 0.8265582655826558,
      "step": 15250,
      "training_loss": 6.954553604125977
    },
    {
      "epoch": 0.8266124661246612,
      "step": 15251,
      "training_loss": 7.07964563369751
    },
    {
      "epoch": 0.8266666666666667,
      "grad_norm": 21.795345306396484,
      "learning_rate": 1e-05,
      "loss": 7.0092,
      "step": 15252
    },
    {
      "epoch": 0.8266666666666667,
      "step": 15252,
      "training_loss": 6.78345251083374
    },
    {
      "epoch": 0.8267208672086721,
      "step": 15253,
      "training_loss": 6.742360591888428
    },
    {
      "epoch": 0.8267750677506776,
      "step": 15254,
      "training_loss": 6.384067535400391
    },
    {
      "epoch": 0.8268292682926829,
      "step": 15255,
      "training_loss": 6.352426052093506
    },
    {
      "epoch": 0.8268834688346883,
      "grad_norm": 32.50712203979492,
      "learning_rate": 1e-05,
      "loss": 6.5656,
      "step": 15256
    },
    {
      "epoch": 0.8268834688346883,
      "step": 15256,
      "training_loss": 5.877772331237793
    },
    {
      "epoch": 0.8269376693766938,
      "step": 15257,
      "training_loss": 6.84623384475708
    },
    {
      "epoch": 0.8269918699186992,
      "step": 15258,
      "training_loss": 6.903265476226807
    },
    {
      "epoch": 0.8270460704607046,
      "step": 15259,
      "training_loss": 5.053194046020508
    },
    {
      "epoch": 0.82710027100271,
      "grad_norm": 33.130859375,
      "learning_rate": 1e-05,
      "loss": 6.1701,
      "step": 15260
    },
    {
      "epoch": 0.82710027100271,
      "step": 15260,
      "training_loss": 6.883763313293457
    },
    {
      "epoch": 0.8271544715447154,
      "step": 15261,
      "training_loss": 6.454610347747803
    },
    {
      "epoch": 0.8272086720867209,
      "step": 15262,
      "training_loss": 6.9237284660339355
    },
    {
      "epoch": 0.8272628726287263,
      "step": 15263,
      "training_loss": 7.68959903717041
    },
    {
      "epoch": 0.8273170731707317,
      "grad_norm": 40.29119110107422,
      "learning_rate": 1e-05,
      "loss": 6.9879,
      "step": 15264
    },
    {
      "epoch": 0.8273170731707317,
      "step": 15264,
      "training_loss": 6.171900749206543
    },
    {
      "epoch": 0.8273712737127371,
      "step": 15265,
      "training_loss": 7.356645584106445
    },
    {
      "epoch": 0.8274254742547426,
      "step": 15266,
      "training_loss": 6.731120586395264
    },
    {
      "epoch": 0.827479674796748,
      "step": 15267,
      "training_loss": 6.959600925445557
    },
    {
      "epoch": 0.8275338753387533,
      "grad_norm": 16.565107345581055,
      "learning_rate": 1e-05,
      "loss": 6.8048,
      "step": 15268
    },
    {
      "epoch": 0.8275338753387533,
      "step": 15268,
      "training_loss": 7.64348030090332
    },
    {
      "epoch": 0.8275880758807588,
      "step": 15269,
      "training_loss": 3.2480742931365967
    },
    {
      "epoch": 0.8276422764227642,
      "step": 15270,
      "training_loss": 6.841836929321289
    },
    {
      "epoch": 0.8276964769647697,
      "step": 15271,
      "training_loss": 6.160327434539795
    },
    {
      "epoch": 0.8277506775067751,
      "grad_norm": 29.95722770690918,
      "learning_rate": 1e-05,
      "loss": 5.9734,
      "step": 15272
    },
    {
      "epoch": 0.8277506775067751,
      "step": 15272,
      "training_loss": 6.99332857131958
    },
    {
      "epoch": 0.8278048780487804,
      "step": 15273,
      "training_loss": 6.6449761390686035
    },
    {
      "epoch": 0.8278590785907859,
      "step": 15274,
      "training_loss": 11.136744499206543
    },
    {
      "epoch": 0.8279132791327913,
      "step": 15275,
      "training_loss": 6.769025802612305
    },
    {
      "epoch": 0.8279674796747968,
      "grad_norm": 16.013717651367188,
      "learning_rate": 1e-05,
      "loss": 7.886,
      "step": 15276
    },
    {
      "epoch": 0.8279674796747968,
      "step": 15276,
      "training_loss": 5.193706512451172
    },
    {
      "epoch": 0.8280216802168021,
      "step": 15277,
      "training_loss": 7.262597560882568
    },
    {
      "epoch": 0.8280758807588076,
      "step": 15278,
      "training_loss": 5.617754936218262
    },
    {
      "epoch": 0.828130081300813,
      "step": 15279,
      "training_loss": 7.097275257110596
    },
    {
      "epoch": 0.8281842818428184,
      "grad_norm": 21.86272621154785,
      "learning_rate": 1e-05,
      "loss": 6.2928,
      "step": 15280
    },
    {
      "epoch": 0.8281842818428184,
      "step": 15280,
      "training_loss": 8.427125930786133
    },
    {
      "epoch": 0.8282384823848239,
      "step": 15281,
      "training_loss": 7.196683406829834
    },
    {
      "epoch": 0.8282926829268292,
      "step": 15282,
      "training_loss": 5.360816955566406
    },
    {
      "epoch": 0.8283468834688347,
      "step": 15283,
      "training_loss": 7.238715648651123
    },
    {
      "epoch": 0.8284010840108401,
      "grad_norm": 39.120731353759766,
      "learning_rate": 1e-05,
      "loss": 7.0558,
      "step": 15284
    },
    {
      "epoch": 0.8284010840108401,
      "step": 15284,
      "training_loss": 5.604093551635742
    },
    {
      "epoch": 0.8284552845528456,
      "step": 15285,
      "training_loss": 6.912059307098389
    },
    {
      "epoch": 0.8285094850948509,
      "step": 15286,
      "training_loss": 6.63517427444458
    },
    {
      "epoch": 0.8285636856368563,
      "step": 15287,
      "training_loss": 6.336402893066406
    },
    {
      "epoch": 0.8286178861788618,
      "grad_norm": 21.380834579467773,
      "learning_rate": 1e-05,
      "loss": 6.3719,
      "step": 15288
    },
    {
      "epoch": 0.8286178861788618,
      "step": 15288,
      "training_loss": 7.063472270965576
    },
    {
      "epoch": 0.8286720867208672,
      "step": 15289,
      "training_loss": 6.088362693786621
    },
    {
      "epoch": 0.8287262872628727,
      "step": 15290,
      "training_loss": 6.307650089263916
    },
    {
      "epoch": 0.828780487804878,
      "step": 15291,
      "training_loss": 7.210943222045898
    },
    {
      "epoch": 0.8288346883468835,
      "grad_norm": 19.27074432373047,
      "learning_rate": 1e-05,
      "loss": 6.6676,
      "step": 15292
    },
    {
      "epoch": 0.8288346883468835,
      "step": 15292,
      "training_loss": 7.145772457122803
    },
    {
      "epoch": 0.8288888888888889,
      "step": 15293,
      "training_loss": 3.109961748123169
    },
    {
      "epoch": 0.8289430894308943,
      "step": 15294,
      "training_loss": 6.863638401031494
    },
    {
      "epoch": 0.8289972899728997,
      "step": 15295,
      "training_loss": 6.441467761993408
    },
    {
      "epoch": 0.8290514905149051,
      "grad_norm": 26.59322738647461,
      "learning_rate": 1e-05,
      "loss": 5.8902,
      "step": 15296
    },
    {
      "epoch": 0.8290514905149051,
      "step": 15296,
      "training_loss": 6.88308572769165
    },
    {
      "epoch": 0.8291056910569106,
      "step": 15297,
      "training_loss": 7.320237636566162
    },
    {
      "epoch": 0.829159891598916,
      "step": 15298,
      "training_loss": 6.622220993041992
    },
    {
      "epoch": 0.8292140921409215,
      "step": 15299,
      "training_loss": 6.898381233215332
    },
    {
      "epoch": 0.8292682926829268,
      "grad_norm": 24.554601669311523,
      "learning_rate": 1e-05,
      "loss": 6.931,
      "step": 15300
    },
    {
      "epoch": 0.8292682926829268,
      "step": 15300,
      "training_loss": 7.24385929107666
    },
    {
      "epoch": 0.8293224932249322,
      "step": 15301,
      "training_loss": 7.457657814025879
    },
    {
      "epoch": 0.8293766937669377,
      "step": 15302,
      "training_loss": 3.8135526180267334
    },
    {
      "epoch": 0.8294308943089431,
      "step": 15303,
      "training_loss": 6.418914794921875
    },
    {
      "epoch": 0.8294850948509485,
      "grad_norm": 20.088590621948242,
      "learning_rate": 1e-05,
      "loss": 6.2335,
      "step": 15304
    },
    {
      "epoch": 0.8294850948509485,
      "step": 15304,
      "training_loss": 6.755785942077637
    },
    {
      "epoch": 0.8295392953929539,
      "step": 15305,
      "training_loss": 6.848912239074707
    },
    {
      "epoch": 0.8295934959349593,
      "step": 15306,
      "training_loss": 7.3025031089782715
    },
    {
      "epoch": 0.8296476964769648,
      "step": 15307,
      "training_loss": 5.8713860511779785
    },
    {
      "epoch": 0.8297018970189702,
      "grad_norm": 28.74789810180664,
      "learning_rate": 1e-05,
      "loss": 6.6946,
      "step": 15308
    },
    {
      "epoch": 0.8297018970189702,
      "step": 15308,
      "training_loss": 6.828159809112549
    },
    {
      "epoch": 0.8297560975609756,
      "step": 15309,
      "training_loss": 5.934433937072754
    },
    {
      "epoch": 0.829810298102981,
      "step": 15310,
      "training_loss": 6.161627292633057
    },
    {
      "epoch": 0.8298644986449865,
      "step": 15311,
      "training_loss": 7.402778625488281
    },
    {
      "epoch": 0.8299186991869919,
      "grad_norm": 25.050012588500977,
      "learning_rate": 1e-05,
      "loss": 6.5817,
      "step": 15312
    },
    {
      "epoch": 0.8299186991869919,
      "step": 15312,
      "training_loss": 7.00187873840332
    },
    {
      "epoch": 0.8299728997289972,
      "step": 15313,
      "training_loss": 6.011629581451416
    },
    {
      "epoch": 0.8300271002710027,
      "step": 15314,
      "training_loss": 6.820937633514404
    },
    {
      "epoch": 0.8300813008130081,
      "step": 15315,
      "training_loss": 6.891650199890137
    },
    {
      "epoch": 0.8301355013550136,
      "grad_norm": 27.396339416503906,
      "learning_rate": 1e-05,
      "loss": 6.6815,
      "step": 15316
    },
    {
      "epoch": 0.8301355013550136,
      "step": 15316,
      "training_loss": 5.81016731262207
    },
    {
      "epoch": 0.830189701897019,
      "step": 15317,
      "training_loss": 7.683216094970703
    },
    {
      "epoch": 0.8302439024390244,
      "step": 15318,
      "training_loss": 6.086012840270996
    },
    {
      "epoch": 0.8302981029810298,
      "step": 15319,
      "training_loss": 6.779712200164795
    },
    {
      "epoch": 0.8303523035230352,
      "grad_norm": 23.447961807250977,
      "learning_rate": 1e-05,
      "loss": 6.5898,
      "step": 15320
    },
    {
      "epoch": 0.8303523035230352,
      "step": 15320,
      "training_loss": 6.3398590087890625
    },
    {
      "epoch": 0.8304065040650407,
      "step": 15321,
      "training_loss": 6.821333885192871
    },
    {
      "epoch": 0.830460704607046,
      "step": 15322,
      "training_loss": 6.625919818878174
    },
    {
      "epoch": 0.8305149051490515,
      "step": 15323,
      "training_loss": 6.654684543609619
    },
    {
      "epoch": 0.8305691056910569,
      "grad_norm": 24.257204055786133,
      "learning_rate": 1e-05,
      "loss": 6.6104,
      "step": 15324
    },
    {
      "epoch": 0.8305691056910569,
      "step": 15324,
      "training_loss": 7.010890960693359
    },
    {
      "epoch": 0.8306233062330624,
      "step": 15325,
      "training_loss": 6.993902206420898
    },
    {
      "epoch": 0.8306775067750678,
      "step": 15326,
      "training_loss": 5.582338333129883
    },
    {
      "epoch": 0.8307317073170731,
      "step": 15327,
      "training_loss": 6.898025035858154
    },
    {
      "epoch": 0.8307859078590786,
      "grad_norm": 22.765254974365234,
      "learning_rate": 1e-05,
      "loss": 6.6213,
      "step": 15328
    },
    {
      "epoch": 0.8307859078590786,
      "step": 15328,
      "training_loss": 6.045454502105713
    },
    {
      "epoch": 0.830840108401084,
      "step": 15329,
      "training_loss": 8.011190414428711
    },
    {
      "epoch": 0.8308943089430895,
      "step": 15330,
      "training_loss": 6.408624172210693
    },
    {
      "epoch": 0.8309485094850948,
      "step": 15331,
      "training_loss": 6.4401984214782715
    },
    {
      "epoch": 0.8310027100271002,
      "grad_norm": 25.237375259399414,
      "learning_rate": 1e-05,
      "loss": 6.7264,
      "step": 15332
    },
    {
      "epoch": 0.8310027100271002,
      "step": 15332,
      "training_loss": 6.494500637054443
    },
    {
      "epoch": 0.8310569105691057,
      "step": 15333,
      "training_loss": 7.363859176635742
    },
    {
      "epoch": 0.8311111111111111,
      "step": 15334,
      "training_loss": 6.416583061218262
    },
    {
      "epoch": 0.8311653116531166,
      "step": 15335,
      "training_loss": 6.925808429718018
    },
    {
      "epoch": 0.8312195121951219,
      "grad_norm": 18.030696868896484,
      "learning_rate": 1e-05,
      "loss": 6.8002,
      "step": 15336
    },
    {
      "epoch": 0.8312195121951219,
      "step": 15336,
      "training_loss": 5.972472190856934
    },
    {
      "epoch": 0.8312737127371274,
      "step": 15337,
      "training_loss": 7.193376064300537
    },
    {
      "epoch": 0.8313279132791328,
      "step": 15338,
      "training_loss": 6.680173397064209
    },
    {
      "epoch": 0.8313821138211382,
      "step": 15339,
      "training_loss": 6.54585599899292
    },
    {
      "epoch": 0.8314363143631436,
      "grad_norm": 28.51141929626465,
      "learning_rate": 1e-05,
      "loss": 6.598,
      "step": 15340
    },
    {
      "epoch": 0.8314363143631436,
      "step": 15340,
      "training_loss": 6.207021236419678
    },
    {
      "epoch": 0.831490514905149,
      "step": 15341,
      "training_loss": 7.0556111335754395
    },
    {
      "epoch": 0.8315447154471545,
      "step": 15342,
      "training_loss": 6.996867656707764
    },
    {
      "epoch": 0.8315989159891599,
      "step": 15343,
      "training_loss": 5.576016426086426
    },
    {
      "epoch": 0.8316531165311654,
      "grad_norm": 32.2570686340332,
      "learning_rate": 1e-05,
      "loss": 6.4589,
      "step": 15344
    },
    {
      "epoch": 0.8316531165311654,
      "step": 15344,
      "training_loss": 6.3808441162109375
    },
    {
      "epoch": 0.8317073170731707,
      "step": 15345,
      "training_loss": 7.735496520996094
    },
    {
      "epoch": 0.8317615176151761,
      "step": 15346,
      "training_loss": 5.488809108734131
    },
    {
      "epoch": 0.8318157181571816,
      "step": 15347,
      "training_loss": 6.914191246032715
    },
    {
      "epoch": 0.831869918699187,
      "grad_norm": 29.47740364074707,
      "learning_rate": 1e-05,
      "loss": 6.6298,
      "step": 15348
    },
    {
      "epoch": 0.831869918699187,
      "step": 15348,
      "training_loss": 5.863732814788818
    },
    {
      "epoch": 0.8319241192411924,
      "step": 15349,
      "training_loss": 6.683090686798096
    },
    {
      "epoch": 0.8319783197831978,
      "step": 15350,
      "training_loss": 7.7570672035217285
    },
    {
      "epoch": 0.8320325203252033,
      "step": 15351,
      "training_loss": 7.111701488494873
    },
    {
      "epoch": 0.8320867208672087,
      "grad_norm": 17.492156982421875,
      "learning_rate": 1e-05,
      "loss": 6.8539,
      "step": 15352
    },
    {
      "epoch": 0.8320867208672087,
      "step": 15352,
      "training_loss": 5.5513529777526855
    },
    {
      "epoch": 0.8321409214092141,
      "step": 15353,
      "training_loss": 7.322136402130127
    },
    {
      "epoch": 0.8321951219512195,
      "step": 15354,
      "training_loss": 6.651059627532959
    },
    {
      "epoch": 0.8322493224932249,
      "step": 15355,
      "training_loss": 6.578133583068848
    },
    {
      "epoch": 0.8323035230352304,
      "grad_norm": 31.922718048095703,
      "learning_rate": 1e-05,
      "loss": 6.5257,
      "step": 15356
    },
    {
      "epoch": 0.8323035230352304,
      "step": 15356,
      "training_loss": 6.489893913269043
    },
    {
      "epoch": 0.8323577235772358,
      "step": 15357,
      "training_loss": 7.250176906585693
    },
    {
      "epoch": 0.8324119241192411,
      "step": 15358,
      "training_loss": 6.802842617034912
    },
    {
      "epoch": 0.8324661246612466,
      "step": 15359,
      "training_loss": 6.422388076782227
    },
    {
      "epoch": 0.832520325203252,
      "grad_norm": 33.58701705932617,
      "learning_rate": 1e-05,
      "loss": 6.7413,
      "step": 15360
    },
    {
      "epoch": 0.832520325203252,
      "step": 15360,
      "training_loss": 6.86179780960083
    },
    {
      "epoch": 0.8325745257452575,
      "step": 15361,
      "training_loss": 6.386422157287598
    },
    {
      "epoch": 0.8326287262872629,
      "step": 15362,
      "training_loss": 4.172496795654297
    },
    {
      "epoch": 0.8326829268292683,
      "step": 15363,
      "training_loss": 5.167715072631836
    },
    {
      "epoch": 0.8327371273712737,
      "grad_norm": 65.02509307861328,
      "learning_rate": 1e-05,
      "loss": 5.6471,
      "step": 15364
    },
    {
      "epoch": 0.8327371273712737,
      "step": 15364,
      "training_loss": 6.698173999786377
    },
    {
      "epoch": 0.8327913279132791,
      "step": 15365,
      "training_loss": 7.453108787536621
    },
    {
      "epoch": 0.8328455284552846,
      "step": 15366,
      "training_loss": 5.82243013381958
    },
    {
      "epoch": 0.8328997289972899,
      "step": 15367,
      "training_loss": 7.043482780456543
    },
    {
      "epoch": 0.8329539295392954,
      "grad_norm": 20.898847579956055,
      "learning_rate": 1e-05,
      "loss": 6.7543,
      "step": 15368
    },
    {
      "epoch": 0.8329539295392954,
      "step": 15368,
      "training_loss": 6.108051776885986
    },
    {
      "epoch": 0.8330081300813008,
      "step": 15369,
      "training_loss": 6.741825103759766
    },
    {
      "epoch": 0.8330623306233063,
      "step": 15370,
      "training_loss": 8.48492431640625
    },
    {
      "epoch": 0.8331165311653117,
      "step": 15371,
      "training_loss": 6.230103015899658
    },
    {
      "epoch": 0.833170731707317,
      "grad_norm": 27.515031814575195,
      "learning_rate": 1e-05,
      "loss": 6.8912,
      "step": 15372
    },
    {
      "epoch": 0.833170731707317,
      "step": 15372,
      "training_loss": 4.061847686767578
    },
    {
      "epoch": 0.8332249322493225,
      "step": 15373,
      "training_loss": 7.340877056121826
    },
    {
      "epoch": 0.8332791327913279,
      "step": 15374,
      "training_loss": 5.976491451263428
    },
    {
      "epoch": 0.8333333333333334,
      "step": 15375,
      "training_loss": 6.3719868659973145
    },
    {
      "epoch": 0.8333875338753387,
      "grad_norm": 20.49672508239746,
      "learning_rate": 1e-05,
      "loss": 5.9378,
      "step": 15376
    },
    {
      "epoch": 0.8333875338753387,
      "step": 15376,
      "training_loss": 5.912994861602783
    },
    {
      "epoch": 0.8334417344173441,
      "step": 15377,
      "training_loss": 6.420030117034912
    },
    {
      "epoch": 0.8334959349593496,
      "step": 15378,
      "training_loss": 6.709204196929932
    },
    {
      "epoch": 0.833550135501355,
      "step": 15379,
      "training_loss": 2.33939266204834
    },
    {
      "epoch": 0.8336043360433605,
      "grad_norm": 35.37152099609375,
      "learning_rate": 1e-05,
      "loss": 5.3454,
      "step": 15380
    },
    {
      "epoch": 0.8336043360433605,
      "step": 15380,
      "training_loss": 7.045475959777832
    },
    {
      "epoch": 0.8336585365853658,
      "step": 15381,
      "training_loss": 7.320460319519043
    },
    {
      "epoch": 0.8337127371273713,
      "step": 15382,
      "training_loss": 6.8887834548950195
    },
    {
      "epoch": 0.8337669376693767,
      "step": 15383,
      "training_loss": 6.859367847442627
    },
    {
      "epoch": 0.8338211382113822,
      "grad_norm": 20.769670486450195,
      "learning_rate": 1e-05,
      "loss": 7.0285,
      "step": 15384
    },
    {
      "epoch": 0.8338211382113822,
      "step": 15384,
      "training_loss": 7.43629789352417
    },
    {
      "epoch": 0.8338753387533875,
      "step": 15385,
      "training_loss": 6.062836170196533
    },
    {
      "epoch": 0.8339295392953929,
      "step": 15386,
      "training_loss": 7.606485843658447
    },
    {
      "epoch": 0.8339837398373984,
      "step": 15387,
      "training_loss": 6.51866340637207
    },
    {
      "epoch": 0.8340379403794038,
      "grad_norm": 22.78318977355957,
      "learning_rate": 1e-05,
      "loss": 6.9061,
      "step": 15388
    },
    {
      "epoch": 0.8340379403794038,
      "step": 15388,
      "training_loss": 5.493066787719727
    },
    {
      "epoch": 0.8340921409214093,
      "step": 15389,
      "training_loss": 7.6378984451293945
    },
    {
      "epoch": 0.8341463414634146,
      "step": 15390,
      "training_loss": 6.949668884277344
    },
    {
      "epoch": 0.83420054200542,
      "step": 15391,
      "training_loss": 6.015603065490723
    },
    {
      "epoch": 0.8342547425474255,
      "grad_norm": 27.074628829956055,
      "learning_rate": 1e-05,
      "loss": 6.5241,
      "step": 15392
    },
    {
      "epoch": 0.8342547425474255,
      "step": 15392,
      "training_loss": 9.31432819366455
    },
    {
      "epoch": 0.8343089430894309,
      "step": 15393,
      "training_loss": 5.20650053024292
    },
    {
      "epoch": 0.8343631436314363,
      "step": 15394,
      "training_loss": 6.182246208190918
    },
    {
      "epoch": 0.8344173441734417,
      "step": 15395,
      "training_loss": 6.799571990966797
    },
    {
      "epoch": 0.8344715447154472,
      "grad_norm": 31.032411575317383,
      "learning_rate": 1e-05,
      "loss": 6.8757,
      "step": 15396
    },
    {
      "epoch": 0.8344715447154472,
      "step": 15396,
      "training_loss": 6.300199031829834
    },
    {
      "epoch": 0.8345257452574526,
      "step": 15397,
      "training_loss": 6.7972211837768555
    },
    {
      "epoch": 0.834579945799458,
      "step": 15398,
      "training_loss": 7.095731735229492
    },
    {
      "epoch": 0.8346341463414634,
      "step": 15399,
      "training_loss": 6.612040042877197
    },
    {
      "epoch": 0.8346883468834688,
      "grad_norm": 33.636085510253906,
      "learning_rate": 1e-05,
      "loss": 6.7013,
      "step": 15400
    },
    {
      "epoch": 0.8346883468834688,
      "step": 15400,
      "training_loss": 3.046882152557373
    },
    {
      "epoch": 0.8347425474254743,
      "step": 15401,
      "training_loss": 5.360917568206787
    },
    {
      "epoch": 0.8347967479674797,
      "step": 15402,
      "training_loss": 6.302944660186768
    },
    {
      "epoch": 0.834850948509485,
      "step": 15403,
      "training_loss": 6.495584487915039
    },
    {
      "epoch": 0.8349051490514905,
      "grad_norm": 25.315265655517578,
      "learning_rate": 1e-05,
      "loss": 5.3016,
      "step": 15404
    },
    {
      "epoch": 0.8349051490514905,
      "step": 15404,
      "training_loss": 5.738980770111084
    },
    {
      "epoch": 0.8349593495934959,
      "step": 15405,
      "training_loss": 7.478650093078613
    },
    {
      "epoch": 0.8350135501355014,
      "step": 15406,
      "training_loss": 6.461274147033691
    },
    {
      "epoch": 0.8350677506775068,
      "step": 15407,
      "training_loss": 7.9583563804626465
    },
    {
      "epoch": 0.8351219512195122,
      "grad_norm": 37.96247100830078,
      "learning_rate": 1e-05,
      "loss": 6.9093,
      "step": 15408
    },
    {
      "epoch": 0.8351219512195122,
      "step": 15408,
      "training_loss": 7.784298419952393
    },
    {
      "epoch": 0.8351761517615176,
      "step": 15409,
      "training_loss": 4.899969100952148
    },
    {
      "epoch": 0.835230352303523,
      "step": 15410,
      "training_loss": 7.078303813934326
    },
    {
      "epoch": 0.8352845528455285,
      "step": 15411,
      "training_loss": 6.253693580627441
    },
    {
      "epoch": 0.8353387533875338,
      "grad_norm": 24.521438598632812,
      "learning_rate": 1e-05,
      "loss": 6.5041,
      "step": 15412
    },
    {
      "epoch": 0.8353387533875338,
      "step": 15412,
      "training_loss": 7.46571683883667
    },
    {
      "epoch": 0.8353929539295393,
      "step": 15413,
      "training_loss": 9.932574272155762
    },
    {
      "epoch": 0.8354471544715447,
      "step": 15414,
      "training_loss": 7.936224460601807
    },
    {
      "epoch": 0.8355013550135502,
      "step": 15415,
      "training_loss": 2.303612232208252
    },
    {
      "epoch": 0.8355555555555556,
      "grad_norm": 32.17633819580078,
      "learning_rate": 1e-05,
      "loss": 6.9095,
      "step": 15416
    },
    {
      "epoch": 0.8355555555555556,
      "step": 15416,
      "training_loss": 7.040068626403809
    },
    {
      "epoch": 0.8356097560975609,
      "step": 15417,
      "training_loss": 5.899019241333008
    },
    {
      "epoch": 0.8356639566395664,
      "step": 15418,
      "training_loss": 7.323248386383057
    },
    {
      "epoch": 0.8357181571815718,
      "step": 15419,
      "training_loss": 7.255318641662598
    },
    {
      "epoch": 0.8357723577235773,
      "grad_norm": 22.294809341430664,
      "learning_rate": 1e-05,
      "loss": 6.8794,
      "step": 15420
    },
    {
      "epoch": 0.8357723577235773,
      "step": 15420,
      "training_loss": 7.400739669799805
    },
    {
      "epoch": 0.8358265582655826,
      "step": 15421,
      "training_loss": 7.266229152679443
    },
    {
      "epoch": 0.835880758807588,
      "step": 15422,
      "training_loss": 4.6869797706604
    },
    {
      "epoch": 0.8359349593495935,
      "step": 15423,
      "training_loss": 6.61459493637085
    },
    {
      "epoch": 0.8359891598915989,
      "grad_norm": 25.762619018554688,
      "learning_rate": 1e-05,
      "loss": 6.4921,
      "step": 15424
    },
    {
      "epoch": 0.8359891598915989,
      "step": 15424,
      "training_loss": 6.030631065368652
    },
    {
      "epoch": 0.8360433604336044,
      "step": 15425,
      "training_loss": 6.8250956535339355
    },
    {
      "epoch": 0.8360975609756097,
      "step": 15426,
      "training_loss": 6.043205738067627
    },
    {
      "epoch": 0.8361517615176152,
      "step": 15427,
      "training_loss": 7.128418445587158
    },
    {
      "epoch": 0.8362059620596206,
      "grad_norm": 32.81226348876953,
      "learning_rate": 1e-05,
      "loss": 6.5068,
      "step": 15428
    },
    {
      "epoch": 0.8362059620596206,
      "step": 15428,
      "training_loss": 4.1807990074157715
    },
    {
      "epoch": 0.8362601626016261,
      "step": 15429,
      "training_loss": 6.927432537078857
    },
    {
      "epoch": 0.8363143631436314,
      "step": 15430,
      "training_loss": 6.387382984161377
    },
    {
      "epoch": 0.8363685636856368,
      "step": 15431,
      "training_loss": 5.319417953491211
    },
    {
      "epoch": 0.8364227642276423,
      "grad_norm": 28.846805572509766,
      "learning_rate": 1e-05,
      "loss": 5.7038,
      "step": 15432
    },
    {
      "epoch": 0.8364227642276423,
      "step": 15432,
      "training_loss": 6.882539749145508
    },
    {
      "epoch": 0.8364769647696477,
      "step": 15433,
      "training_loss": 7.311502933502197
    },
    {
      "epoch": 0.8365311653116532,
      "step": 15434,
      "training_loss": 7.173126697540283
    },
    {
      "epoch": 0.8365853658536585,
      "step": 15435,
      "training_loss": 6.244292736053467
    },
    {
      "epoch": 0.836639566395664,
      "grad_norm": 44.0823974609375,
      "learning_rate": 1e-05,
      "loss": 6.9029,
      "step": 15436
    },
    {
      "epoch": 0.836639566395664,
      "step": 15436,
      "training_loss": 4.445542812347412
    },
    {
      "epoch": 0.8366937669376694,
      "step": 15437,
      "training_loss": 5.121498107910156
    },
    {
      "epoch": 0.8367479674796748,
      "step": 15438,
      "training_loss": 6.478323936462402
    },
    {
      "epoch": 0.8368021680216802,
      "step": 15439,
      "training_loss": 6.926534175872803
    },
    {
      "epoch": 0.8368563685636856,
      "grad_norm": 19.584386825561523,
      "learning_rate": 1e-05,
      "loss": 5.743,
      "step": 15440
    },
    {
      "epoch": 0.8368563685636856,
      "step": 15440,
      "training_loss": 5.909417629241943
    },
    {
      "epoch": 0.8369105691056911,
      "step": 15441,
      "training_loss": 7.7030415534973145
    },
    {
      "epoch": 0.8369647696476965,
      "step": 15442,
      "training_loss": 6.953037738800049
    },
    {
      "epoch": 0.837018970189702,
      "step": 15443,
      "training_loss": 5.682528972625732
    },
    {
      "epoch": 0.8370731707317073,
      "grad_norm": 35.618003845214844,
      "learning_rate": 1e-05,
      "loss": 6.562,
      "step": 15444
    },
    {
      "epoch": 0.8370731707317073,
      "step": 15444,
      "training_loss": 5.219971656799316
    },
    {
      "epoch": 0.8371273712737127,
      "step": 15445,
      "training_loss": 7.3714776039123535
    },
    {
      "epoch": 0.8371815718157182,
      "step": 15446,
      "training_loss": 6.557091236114502
    },
    {
      "epoch": 0.8372357723577236,
      "step": 15447,
      "training_loss": 8.070420265197754
    },
    {
      "epoch": 0.837289972899729,
      "grad_norm": 38.23728561401367,
      "learning_rate": 1e-05,
      "loss": 6.8047,
      "step": 15448
    },
    {
      "epoch": 0.837289972899729,
      "step": 15448,
      "training_loss": 6.663060188293457
    },
    {
      "epoch": 0.8373441734417344,
      "step": 15449,
      "training_loss": 6.522895812988281
    },
    {
      "epoch": 0.8373983739837398,
      "step": 15450,
      "training_loss": 7.341385841369629
    },
    {
      "epoch": 0.8374525745257453,
      "step": 15451,
      "training_loss": 4.421815395355225
    },
    {
      "epoch": 0.8375067750677507,
      "grad_norm": 23.713056564331055,
      "learning_rate": 1e-05,
      "loss": 6.2373,
      "step": 15452
    },
    {
      "epoch": 0.8375067750677507,
      "step": 15452,
      "training_loss": 7.6923699378967285
    },
    {
      "epoch": 0.8375609756097561,
      "step": 15453,
      "training_loss": 6.704970836639404
    },
    {
      "epoch": 0.8376151761517615,
      "step": 15454,
      "training_loss": 5.77418851852417
    },
    {
      "epoch": 0.837669376693767,
      "step": 15455,
      "training_loss": 6.466888427734375
    },
    {
      "epoch": 0.8377235772357724,
      "grad_norm": 23.011751174926758,
      "learning_rate": 1e-05,
      "loss": 6.6596,
      "step": 15456
    },
    {
      "epoch": 0.8377235772357724,
      "step": 15456,
      "training_loss": 7.304094314575195
    },
    {
      "epoch": 0.8377777777777777,
      "step": 15457,
      "training_loss": 5.33732795715332
    },
    {
      "epoch": 0.8378319783197832,
      "step": 15458,
      "training_loss": 5.741355895996094
    },
    {
      "epoch": 0.8378861788617886,
      "step": 15459,
      "training_loss": 5.951462745666504
    },
    {
      "epoch": 0.8379403794037941,
      "grad_norm": 35.037776947021484,
      "learning_rate": 1e-05,
      "loss": 6.0836,
      "step": 15460
    },
    {
      "epoch": 0.8379403794037941,
      "step": 15460,
      "training_loss": 7.95743989944458
    },
    {
      "epoch": 0.8379945799457995,
      "step": 15461,
      "training_loss": 7.6852707862854
    },
    {
      "epoch": 0.8380487804878048,
      "step": 15462,
      "training_loss": 5.746263027191162
    },
    {
      "epoch": 0.8381029810298103,
      "step": 15463,
      "training_loss": 6.821147918701172
    },
    {
      "epoch": 0.8381571815718157,
      "grad_norm": 26.347158432006836,
      "learning_rate": 1e-05,
      "loss": 7.0525,
      "step": 15464
    },
    {
      "epoch": 0.8381571815718157,
      "step": 15464,
      "training_loss": 7.038455963134766
    },
    {
      "epoch": 0.8382113821138212,
      "step": 15465,
      "training_loss": 6.74973201751709
    },
    {
      "epoch": 0.8382655826558265,
      "step": 15466,
      "training_loss": 6.8333353996276855
    },
    {
      "epoch": 0.838319783197832,
      "step": 15467,
      "training_loss": 6.143828392028809
    },
    {
      "epoch": 0.8383739837398374,
      "grad_norm": 36.016483306884766,
      "learning_rate": 1e-05,
      "loss": 6.6913,
      "step": 15468
    },
    {
      "epoch": 0.8383739837398374,
      "step": 15468,
      "training_loss": 7.294156074523926
    },
    {
      "epoch": 0.8384281842818428,
      "step": 15469,
      "training_loss": 6.28253173828125
    },
    {
      "epoch": 0.8384823848238483,
      "step": 15470,
      "training_loss": 6.660261154174805
    },
    {
      "epoch": 0.8385365853658536,
      "step": 15471,
      "training_loss": 4.228612899780273
    },
    {
      "epoch": 0.8385907859078591,
      "grad_norm": 24.134479522705078,
      "learning_rate": 1e-05,
      "loss": 6.1164,
      "step": 15472
    },
    {
      "epoch": 0.8385907859078591,
      "step": 15472,
      "training_loss": 5.532501220703125
    },
    {
      "epoch": 0.8386449864498645,
      "step": 15473,
      "training_loss": 6.3238019943237305
    },
    {
      "epoch": 0.83869918699187,
      "step": 15474,
      "training_loss": 7.589456081390381
    },
    {
      "epoch": 0.8387533875338753,
      "step": 15475,
      "training_loss": 5.136547088623047
    },
    {
      "epoch": 0.8388075880758807,
      "grad_norm": 21.637218475341797,
      "learning_rate": 1e-05,
      "loss": 6.1456,
      "step": 15476
    },
    {
      "epoch": 0.8388075880758807,
      "step": 15476,
      "training_loss": 5.73933219909668
    },
    {
      "epoch": 0.8388617886178862,
      "step": 15477,
      "training_loss": 5.976924896240234
    },
    {
      "epoch": 0.8389159891598916,
      "step": 15478,
      "training_loss": 6.790964603424072
    },
    {
      "epoch": 0.8389701897018971,
      "step": 15479,
      "training_loss": 7.507718086242676
    },
    {
      "epoch": 0.8390243902439024,
      "grad_norm": 20.164981842041016,
      "learning_rate": 1e-05,
      "loss": 6.5037,
      "step": 15480
    },
    {
      "epoch": 0.8390243902439024,
      "step": 15480,
      "training_loss": 6.213205814361572
    },
    {
      "epoch": 0.8390785907859079,
      "step": 15481,
      "training_loss": 5.473924160003662
    },
    {
      "epoch": 0.8391327913279133,
      "step": 15482,
      "training_loss": 6.389517784118652
    },
    {
      "epoch": 0.8391869918699187,
      "step": 15483,
      "training_loss": 7.631747245788574
    },
    {
      "epoch": 0.8392411924119241,
      "grad_norm": 33.10688781738281,
      "learning_rate": 1e-05,
      "loss": 6.4271,
      "step": 15484
    },
    {
      "epoch": 0.8392411924119241,
      "step": 15484,
      "training_loss": 4.237540245056152
    },
    {
      "epoch": 0.8392953929539295,
      "step": 15485,
      "training_loss": 6.682749271392822
    },
    {
      "epoch": 0.839349593495935,
      "step": 15486,
      "training_loss": 6.93458366394043
    },
    {
      "epoch": 0.8394037940379404,
      "step": 15487,
      "training_loss": 6.5555291175842285
    },
    {
      "epoch": 0.8394579945799459,
      "grad_norm": 18.797346115112305,
      "learning_rate": 1e-05,
      "loss": 6.1026,
      "step": 15488
    },
    {
      "epoch": 0.8394579945799459,
      "step": 15488,
      "training_loss": 7.2307000160217285
    },
    {
      "epoch": 0.8395121951219512,
      "step": 15489,
      "training_loss": 7.811237812042236
    },
    {
      "epoch": 0.8395663956639566,
      "step": 15490,
      "training_loss": 6.390410423278809
    },
    {
      "epoch": 0.8396205962059621,
      "step": 15491,
      "training_loss": 6.987064838409424
    },
    {
      "epoch": 0.8396747967479675,
      "grad_norm": 29.849374771118164,
      "learning_rate": 1e-05,
      "loss": 7.1049,
      "step": 15492
    },
    {
      "epoch": 0.8396747967479675,
      "step": 15492,
      "training_loss": 6.900698184967041
    },
    {
      "epoch": 0.8397289972899729,
      "step": 15493,
      "training_loss": 6.110837459564209
    },
    {
      "epoch": 0.8397831978319783,
      "step": 15494,
      "training_loss": 5.337347030639648
    },
    {
      "epoch": 0.8398373983739837,
      "step": 15495,
      "training_loss": 6.422821998596191
    },
    {
      "epoch": 0.8398915989159892,
      "grad_norm": 23.121063232421875,
      "learning_rate": 1e-05,
      "loss": 6.1929,
      "step": 15496
    },
    {
      "epoch": 0.8398915989159892,
      "step": 15496,
      "training_loss": 8.084403038024902
    },
    {
      "epoch": 0.8399457994579946,
      "step": 15497,
      "training_loss": 7.267318248748779
    },
    {
      "epoch": 0.84,
      "step": 15498,
      "training_loss": 7.580589294433594
    },
    {
      "epoch": 0.8400542005420054,
      "step": 15499,
      "training_loss": 5.657900810241699
    },
    {
      "epoch": 0.8401084010840109,
      "grad_norm": 39.96730422973633,
      "learning_rate": 1e-05,
      "loss": 7.1476,
      "step": 15500
    },
    {
      "epoch": 0.8401084010840109,
      "step": 15500,
      "training_loss": 6.1497344970703125
    },
    {
      "epoch": 0.8401626016260163,
      "step": 15501,
      "training_loss": 6.0263590812683105
    },
    {
      "epoch": 0.8402168021680216,
      "step": 15502,
      "training_loss": 4.499803066253662
    },
    {
      "epoch": 0.8402710027100271,
      "step": 15503,
      "training_loss": 6.626063346862793
    },
    {
      "epoch": 0.8403252032520325,
      "grad_norm": 29.63718605041504,
      "learning_rate": 1e-05,
      "loss": 5.8255,
      "step": 15504
    },
    {
      "epoch": 0.8403252032520325,
      "step": 15504,
      "training_loss": 6.761014938354492
    },
    {
      "epoch": 0.840379403794038,
      "step": 15505,
      "training_loss": 5.092731952667236
    },
    {
      "epoch": 0.8404336043360433,
      "step": 15506,
      "training_loss": 3.259521722793579
    },
    {
      "epoch": 0.8404878048780487,
      "step": 15507,
      "training_loss": 5.539967060089111
    },
    {
      "epoch": 0.8405420054200542,
      "grad_norm": 41.100914001464844,
      "learning_rate": 1e-05,
      "loss": 5.1633,
      "step": 15508
    },
    {
      "epoch": 0.8405420054200542,
      "step": 15508,
      "training_loss": 7.300596714019775
    },
    {
      "epoch": 0.8405962059620596,
      "step": 15509,
      "training_loss": 6.221268653869629
    },
    {
      "epoch": 0.8406504065040651,
      "step": 15510,
      "training_loss": 6.1837944984436035
    },
    {
      "epoch": 0.8407046070460704,
      "step": 15511,
      "training_loss": 6.481010913848877
    },
    {
      "epoch": 0.8407588075880759,
      "grad_norm": 31.69989013671875,
      "learning_rate": 1e-05,
      "loss": 6.5467,
      "step": 15512
    },
    {
      "epoch": 0.8407588075880759,
      "step": 15512,
      "training_loss": 4.927242755889893
    },
    {
      "epoch": 0.8408130081300813,
      "step": 15513,
      "training_loss": 7.794536590576172
    },
    {
      "epoch": 0.8408672086720868,
      "step": 15514,
      "training_loss": 5.8980021476745605
    },
    {
      "epoch": 0.8409214092140921,
      "step": 15515,
      "training_loss": 5.944029331207275
    },
    {
      "epoch": 0.8409756097560975,
      "grad_norm": 19.464876174926758,
      "learning_rate": 1e-05,
      "loss": 6.141,
      "step": 15516
    },
    {
      "epoch": 0.8409756097560975,
      "step": 15516,
      "training_loss": 7.123745441436768
    },
    {
      "epoch": 0.841029810298103,
      "step": 15517,
      "training_loss": 7.102785587310791
    },
    {
      "epoch": 0.8410840108401084,
      "step": 15518,
      "training_loss": 7.586441993713379
    },
    {
      "epoch": 0.8411382113821139,
      "step": 15519,
      "training_loss": 6.966952800750732
    },
    {
      "epoch": 0.8411924119241192,
      "grad_norm": 30.421241760253906,
      "learning_rate": 1e-05,
      "loss": 7.195,
      "step": 15520
    },
    {
      "epoch": 0.8411924119241192,
      "step": 15520,
      "training_loss": 5.986778259277344
    },
    {
      "epoch": 0.8412466124661246,
      "step": 15521,
      "training_loss": 6.679957866668701
    },
    {
      "epoch": 0.8413008130081301,
      "step": 15522,
      "training_loss": 6.768834114074707
    },
    {
      "epoch": 0.8413550135501355,
      "step": 15523,
      "training_loss": 6.761143684387207
    },
    {
      "epoch": 0.8414092140921409,
      "grad_norm": 30.438392639160156,
      "learning_rate": 1e-05,
      "loss": 6.5492,
      "step": 15524
    },
    {
      "epoch": 0.8414092140921409,
      "step": 15524,
      "training_loss": 7.172431468963623
    },
    {
      "epoch": 0.8414634146341463,
      "step": 15525,
      "training_loss": 7.309536457061768
    },
    {
      "epoch": 0.8415176151761518,
      "step": 15526,
      "training_loss": 6.687273025512695
    },
    {
      "epoch": 0.8415718157181572,
      "step": 15527,
      "training_loss": 6.773504734039307
    },
    {
      "epoch": 0.8416260162601626,
      "grad_norm": 49.071292877197266,
      "learning_rate": 1e-05,
      "loss": 6.9857,
      "step": 15528
    },
    {
      "epoch": 0.8416260162601626,
      "step": 15528,
      "training_loss": 7.119030952453613
    },
    {
      "epoch": 0.841680216802168,
      "step": 15529,
      "training_loss": 5.127007961273193
    },
    {
      "epoch": 0.8417344173441734,
      "step": 15530,
      "training_loss": 8.476022720336914
    },
    {
      "epoch": 0.8417886178861789,
      "step": 15531,
      "training_loss": 6.490704536437988
    },
    {
      "epoch": 0.8418428184281843,
      "grad_norm": 26.044206619262695,
      "learning_rate": 1e-05,
      "loss": 6.8032,
      "step": 15532
    },
    {
      "epoch": 0.8418428184281843,
      "step": 15532,
      "training_loss": 6.717691421508789
    },
    {
      "epoch": 0.8418970189701896,
      "step": 15533,
      "training_loss": 5.45507287979126
    },
    {
      "epoch": 0.8419512195121951,
      "step": 15534,
      "training_loss": 5.320486068725586
    },
    {
      "epoch": 0.8420054200542005,
      "step": 15535,
      "training_loss": 7.2817840576171875
    },
    {
      "epoch": 0.842059620596206,
      "grad_norm": 94.05307006835938,
      "learning_rate": 1e-05,
      "loss": 6.1938,
      "step": 15536
    },
    {
      "epoch": 0.842059620596206,
      "step": 15536,
      "training_loss": 3.570547342300415
    },
    {
      "epoch": 0.8421138211382114,
      "step": 15537,
      "training_loss": 6.152153015136719
    },
    {
      "epoch": 0.8421680216802168,
      "step": 15538,
      "training_loss": 6.135256290435791
    },
    {
      "epoch": 0.8422222222222222,
      "step": 15539,
      "training_loss": 7.066221237182617
    },
    {
      "epoch": 0.8422764227642277,
      "grad_norm": 26.50912857055664,
      "learning_rate": 1e-05,
      "loss": 5.731,
      "step": 15540
    },
    {
      "epoch": 0.8422764227642277,
      "step": 15540,
      "training_loss": 8.287631034851074
    },
    {
      "epoch": 0.8423306233062331,
      "step": 15541,
      "training_loss": 6.903817653656006
    },
    {
      "epoch": 0.8423848238482384,
      "step": 15542,
      "training_loss": 7.055520057678223
    },
    {
      "epoch": 0.8424390243902439,
      "step": 15543,
      "training_loss": 9.025285720825195
    },
    {
      "epoch": 0.8424932249322493,
      "grad_norm": 58.62733459472656,
      "learning_rate": 1e-05,
      "loss": 7.8181,
      "step": 15544
    },
    {
      "epoch": 0.8424932249322493,
      "step": 15544,
      "training_loss": 7.227504253387451
    },
    {
      "epoch": 0.8425474254742548,
      "step": 15545,
      "training_loss": 6.768616676330566
    },
    {
      "epoch": 0.8426016260162602,
      "step": 15546,
      "training_loss": 8.754535675048828
    },
    {
      "epoch": 0.8426558265582655,
      "step": 15547,
      "training_loss": 3.378786325454712
    },
    {
      "epoch": 0.842710027100271,
      "grad_norm": 33.10806655883789,
      "learning_rate": 1e-05,
      "loss": 6.5324,
      "step": 15548
    },
    {
      "epoch": 0.842710027100271,
      "step": 15548,
      "training_loss": 4.767847061157227
    },
    {
      "epoch": 0.8427642276422764,
      "step": 15549,
      "training_loss": 6.62877082824707
    },
    {
      "epoch": 0.8428184281842819,
      "step": 15550,
      "training_loss": 7.027066707611084
    },
    {
      "epoch": 0.8428726287262872,
      "step": 15551,
      "training_loss": 5.343297481536865
    },
    {
      "epoch": 0.8429268292682927,
      "grad_norm": 24.611783981323242,
      "learning_rate": 1e-05,
      "loss": 5.9417,
      "step": 15552
    },
    {
      "epoch": 0.8429268292682927,
      "step": 15552,
      "training_loss": 2.3867080211639404
    },
    {
      "epoch": 0.8429810298102981,
      "step": 15553,
      "training_loss": 6.488588809967041
    },
    {
      "epoch": 0.8430352303523035,
      "step": 15554,
      "training_loss": 7.206915378570557
    },
    {
      "epoch": 0.843089430894309,
      "step": 15555,
      "training_loss": 7.340549468994141
    },
    {
      "epoch": 0.8431436314363143,
      "grad_norm": 23.41118621826172,
      "learning_rate": 1e-05,
      "loss": 5.8557,
      "step": 15556
    },
    {
      "epoch": 0.8431436314363143,
      "step": 15556,
      "training_loss": 7.415441989898682
    },
    {
      "epoch": 0.8431978319783198,
      "step": 15557,
      "training_loss": 6.147406101226807
    },
    {
      "epoch": 0.8432520325203252,
      "step": 15558,
      "training_loss": 6.505178451538086
    },
    {
      "epoch": 0.8433062330623307,
      "step": 15559,
      "training_loss": 7.6865234375
    },
    {
      "epoch": 0.843360433604336,
      "grad_norm": 28.59483528137207,
      "learning_rate": 1e-05,
      "loss": 6.9386,
      "step": 15560
    },
    {
      "epoch": 0.843360433604336,
      "step": 15560,
      "training_loss": 7.4624505043029785
    },
    {
      "epoch": 0.8434146341463414,
      "step": 15561,
      "training_loss": 6.593984603881836
    },
    {
      "epoch": 0.8434688346883469,
      "step": 15562,
      "training_loss": 5.864140033721924
    },
    {
      "epoch": 0.8435230352303523,
      "step": 15563,
      "training_loss": 6.839666366577148
    },
    {
      "epoch": 0.8435772357723578,
      "grad_norm": 24.503646850585938,
      "learning_rate": 1e-05,
      "loss": 6.6901,
      "step": 15564
    },
    {
      "epoch": 0.8435772357723578,
      "step": 15564,
      "training_loss": 7.187211036682129
    },
    {
      "epoch": 0.8436314363143631,
      "step": 15565,
      "training_loss": 7.935097694396973
    },
    {
      "epoch": 0.8436856368563685,
      "step": 15566,
      "training_loss": 6.615851879119873
    },
    {
      "epoch": 0.843739837398374,
      "step": 15567,
      "training_loss": 5.673758506774902
    },
    {
      "epoch": 0.8437940379403794,
      "grad_norm": 26.984209060668945,
      "learning_rate": 1e-05,
      "loss": 6.853,
      "step": 15568
    },
    {
      "epoch": 0.8437940379403794,
      "step": 15568,
      "training_loss": 6.84679651260376
    },
    {
      "epoch": 0.8438482384823848,
      "step": 15569,
      "training_loss": 7.40402889251709
    },
    {
      "epoch": 0.8439024390243902,
      "step": 15570,
      "training_loss": 8.850621223449707
    },
    {
      "epoch": 0.8439566395663957,
      "step": 15571,
      "training_loss": 5.827710151672363
    },
    {
      "epoch": 0.8440108401084011,
      "grad_norm": 31.996036529541016,
      "learning_rate": 1e-05,
      "loss": 7.2323,
      "step": 15572
    },
    {
      "epoch": 0.8440108401084011,
      "step": 15572,
      "training_loss": 6.119202136993408
    },
    {
      "epoch": 0.8440650406504066,
      "step": 15573,
      "training_loss": 6.482970237731934
    },
    {
      "epoch": 0.8441192411924119,
      "step": 15574,
      "training_loss": 5.750619888305664
    },
    {
      "epoch": 0.8441734417344173,
      "step": 15575,
      "training_loss": 7.076390266418457
    },
    {
      "epoch": 0.8442276422764228,
      "grad_norm": 30.129724502563477,
      "learning_rate": 1e-05,
      "loss": 6.3573,
      "step": 15576
    },
    {
      "epoch": 0.8442276422764228,
      "step": 15576,
      "training_loss": 6.098762512207031
    },
    {
      "epoch": 0.8442818428184282,
      "step": 15577,
      "training_loss": 8.146336555480957
    },
    {
      "epoch": 0.8443360433604336,
      "step": 15578,
      "training_loss": 7.144702434539795
    },
    {
      "epoch": 0.844390243902439,
      "step": 15579,
      "training_loss": 4.032516002655029
    },
    {
      "epoch": 0.8444444444444444,
      "grad_norm": 27.054899215698242,
      "learning_rate": 1e-05,
      "loss": 6.3556,
      "step": 15580
    },
    {
      "epoch": 0.8444444444444444,
      "step": 15580,
      "training_loss": 7.307556629180908
    },
    {
      "epoch": 0.8444986449864499,
      "step": 15581,
      "training_loss": 6.39202880859375
    },
    {
      "epoch": 0.8445528455284553,
      "step": 15582,
      "training_loss": 4.915463924407959
    },
    {
      "epoch": 0.8446070460704607,
      "step": 15583,
      "training_loss": 6.515573501586914
    },
    {
      "epoch": 0.8446612466124661,
      "grad_norm": 33.72809982299805,
      "learning_rate": 1e-05,
      "loss": 6.2827,
      "step": 15584
    },
    {
      "epoch": 0.8446612466124661,
      "step": 15584,
      "training_loss": 6.325803756713867
    },
    {
      "epoch": 0.8447154471544716,
      "step": 15585,
      "training_loss": 6.535630226135254
    },
    {
      "epoch": 0.844769647696477,
      "step": 15586,
      "training_loss": 7.0240478515625
    },
    {
      "epoch": 0.8448238482384823,
      "step": 15587,
      "training_loss": 5.318403720855713
    },
    {
      "epoch": 0.8448780487804878,
      "grad_norm": 43.114219665527344,
      "learning_rate": 1e-05,
      "loss": 6.301,
      "step": 15588
    },
    {
      "epoch": 0.8448780487804878,
      "step": 15588,
      "training_loss": 6.799540996551514
    },
    {
      "epoch": 0.8449322493224932,
      "step": 15589,
      "training_loss": 6.763495922088623
    },
    {
      "epoch": 0.8449864498644987,
      "step": 15590,
      "training_loss": 7.170428276062012
    },
    {
      "epoch": 0.8450406504065041,
      "step": 15591,
      "training_loss": 7.260722637176514
    },
    {
      "epoch": 0.8450948509485094,
      "grad_norm": 49.294952392578125,
      "learning_rate": 1e-05,
      "loss": 6.9985,
      "step": 15592
    },
    {
      "epoch": 0.8450948509485094,
      "step": 15592,
      "training_loss": 7.473852634429932
    },
    {
      "epoch": 0.8451490514905149,
      "step": 15593,
      "training_loss": 5.46687126159668
    },
    {
      "epoch": 0.8452032520325203,
      "step": 15594,
      "training_loss": 6.363114833831787
    },
    {
      "epoch": 0.8452574525745258,
      "step": 15595,
      "training_loss": 7.945976734161377
    },
    {
      "epoch": 0.8453116531165311,
      "grad_norm": 27.534542083740234,
      "learning_rate": 1e-05,
      "loss": 6.8125,
      "step": 15596
    },
    {
      "epoch": 0.8453116531165311,
      "step": 15596,
      "training_loss": 6.089879512786865
    },
    {
      "epoch": 0.8453658536585366,
      "step": 15597,
      "training_loss": 6.715273380279541
    },
    {
      "epoch": 0.845420054200542,
      "step": 15598,
      "training_loss": 6.162702560424805
    },
    {
      "epoch": 0.8454742547425474,
      "step": 15599,
      "training_loss": 4.634095668792725
    },
    {
      "epoch": 0.8455284552845529,
      "grad_norm": 28.645700454711914,
      "learning_rate": 1e-05,
      "loss": 5.9005,
      "step": 15600
    },
    {
      "epoch": 0.8455284552845529,
      "step": 15600,
      "training_loss": 7.401095390319824
    },
    {
      "epoch": 0.8455826558265582,
      "step": 15601,
      "training_loss": 7.649815082550049
    },
    {
      "epoch": 0.8456368563685637,
      "step": 15602,
      "training_loss": 5.902219772338867
    },
    {
      "epoch": 0.8456910569105691,
      "step": 15603,
      "training_loss": 6.407344818115234
    },
    {
      "epoch": 0.8457452574525746,
      "grad_norm": 45.1354866027832,
      "learning_rate": 1e-05,
      "loss": 6.8401,
      "step": 15604
    },
    {
      "epoch": 0.8457452574525746,
      "step": 15604,
      "training_loss": 7.1167216300964355
    },
    {
      "epoch": 0.8457994579945799,
      "step": 15605,
      "training_loss": 6.603485584259033
    },
    {
      "epoch": 0.8458536585365853,
      "step": 15606,
      "training_loss": 7.207271099090576
    },
    {
      "epoch": 0.8459078590785908,
      "step": 15607,
      "training_loss": 5.228018283843994
    },
    {
      "epoch": 0.8459620596205962,
      "grad_norm": 46.737789154052734,
      "learning_rate": 1e-05,
      "loss": 6.5389,
      "step": 15608
    },
    {
      "epoch": 0.8459620596205962,
      "step": 15608,
      "training_loss": 3.4846675395965576
    },
    {
      "epoch": 0.8460162601626017,
      "step": 15609,
      "training_loss": 6.836287498474121
    },
    {
      "epoch": 0.846070460704607,
      "step": 15610,
      "training_loss": 7.036292552947998
    },
    {
      "epoch": 0.8461246612466125,
      "step": 15611,
      "training_loss": 6.5478291511535645
    },
    {
      "epoch": 0.8461788617886179,
      "grad_norm": 44.99875259399414,
      "learning_rate": 1e-05,
      "loss": 5.9763,
      "step": 15612
    },
    {
      "epoch": 0.8461788617886179,
      "step": 15612,
      "training_loss": 7.0331902503967285
    },
    {
      "epoch": 0.8462330623306233,
      "step": 15613,
      "training_loss": 8.198424339294434
    },
    {
      "epoch": 0.8462872628726287,
      "step": 15614,
      "training_loss": 5.9795145988464355
    },
    {
      "epoch": 0.8463414634146341,
      "step": 15615,
      "training_loss": 7.735840797424316
    },
    {
      "epoch": 0.8463956639566396,
      "grad_norm": 38.67906188964844,
      "learning_rate": 1e-05,
      "loss": 7.2367,
      "step": 15616
    },
    {
      "epoch": 0.8463956639566396,
      "step": 15616,
      "training_loss": 6.619875431060791
    },
    {
      "epoch": 0.846449864498645,
      "step": 15617,
      "training_loss": 7.23652458190918
    },
    {
      "epoch": 0.8465040650406505,
      "step": 15618,
      "training_loss": 7.663106918334961
    },
    {
      "epoch": 0.8465582655826558,
      "step": 15619,
      "training_loss": 6.990164756774902
    },
    {
      "epoch": 0.8466124661246612,
      "grad_norm": 18.363048553466797,
      "learning_rate": 1e-05,
      "loss": 7.1274,
      "step": 15620
    },
    {
      "epoch": 0.8466124661246612,
      "step": 15620,
      "training_loss": 6.663801193237305
    },
    {
      "epoch": 0.8466666666666667,
      "step": 15621,
      "training_loss": 6.452617645263672
    },
    {
      "epoch": 0.8467208672086721,
      "step": 15622,
      "training_loss": 6.7959136962890625
    },
    {
      "epoch": 0.8467750677506775,
      "step": 15623,
      "training_loss": 7.122981071472168
    },
    {
      "epoch": 0.8468292682926829,
      "grad_norm": 29.31314468383789,
      "learning_rate": 1e-05,
      "loss": 6.7588,
      "step": 15624
    },
    {
      "epoch": 0.8468292682926829,
      "step": 15624,
      "training_loss": 5.990854740142822
    },
    {
      "epoch": 0.8468834688346883,
      "step": 15625,
      "training_loss": 7.2172393798828125
    },
    {
      "epoch": 0.8469376693766938,
      "step": 15626,
      "training_loss": 7.118345737457275
    },
    {
      "epoch": 0.8469918699186992,
      "step": 15627,
      "training_loss": 5.152342319488525
    },
    {
      "epoch": 0.8470460704607046,
      "grad_norm": 35.66453170776367,
      "learning_rate": 1e-05,
      "loss": 6.3697,
      "step": 15628
    },
    {
      "epoch": 0.8470460704607046,
      "step": 15628,
      "training_loss": 8.329119682312012
    },
    {
      "epoch": 0.84710027100271,
      "step": 15629,
      "training_loss": 3.6629669666290283
    },
    {
      "epoch": 0.8471544715447155,
      "step": 15630,
      "training_loss": 6.843427658081055
    },
    {
      "epoch": 0.8472086720867209,
      "step": 15631,
      "training_loss": 5.999104976654053
    },
    {
      "epoch": 0.8472628726287262,
      "grad_norm": 26.868995666503906,
      "learning_rate": 1e-05,
      "loss": 6.2087,
      "step": 15632
    },
    {
      "epoch": 0.8472628726287262,
      "step": 15632,
      "training_loss": 5.085327625274658
    },
    {
      "epoch": 0.8473170731707317,
      "step": 15633,
      "training_loss": 7.445699214935303
    },
    {
      "epoch": 0.8473712737127371,
      "step": 15634,
      "training_loss": 6.122340679168701
    },
    {
      "epoch": 0.8474254742547426,
      "step": 15635,
      "training_loss": 7.440186500549316
    },
    {
      "epoch": 0.847479674796748,
      "grad_norm": 25.45085906982422,
      "learning_rate": 1e-05,
      "loss": 6.5234,
      "step": 15636
    },
    {
      "epoch": 0.847479674796748,
      "step": 15636,
      "training_loss": 7.215080261230469
    },
    {
      "epoch": 0.8475338753387534,
      "step": 15637,
      "training_loss": 7.12714958190918
    },
    {
      "epoch": 0.8475880758807588,
      "step": 15638,
      "training_loss": 8.584084510803223
    },
    {
      "epoch": 0.8476422764227642,
      "step": 15639,
      "training_loss": 8.587608337402344
    },
    {
      "epoch": 0.8476964769647697,
      "grad_norm": 43.59116744995117,
      "learning_rate": 1e-05,
      "loss": 7.8785,
      "step": 15640
    },
    {
      "epoch": 0.8476964769647697,
      "step": 15640,
      "training_loss": 5.598627090454102
    },
    {
      "epoch": 0.847750677506775,
      "step": 15641,
      "training_loss": 6.062114238739014
    },
    {
      "epoch": 0.8478048780487805,
      "step": 15642,
      "training_loss": 6.966665267944336
    },
    {
      "epoch": 0.8478590785907859,
      "step": 15643,
      "training_loss": 7.317050933837891
    },
    {
      "epoch": 0.8479132791327914,
      "grad_norm": 34.412540435791016,
      "learning_rate": 1e-05,
      "loss": 6.4861,
      "step": 15644
    },
    {
      "epoch": 0.8479132791327914,
      "step": 15644,
      "training_loss": 4.092005252838135
    },
    {
      "epoch": 0.8479674796747968,
      "step": 15645,
      "training_loss": 5.913280963897705
    },
    {
      "epoch": 0.8480216802168021,
      "step": 15646,
      "training_loss": 5.920303821563721
    },
    {
      "epoch": 0.8480758807588076,
      "step": 15647,
      "training_loss": 7.018243312835693
    },
    {
      "epoch": 0.848130081300813,
      "grad_norm": 21.11417007446289,
      "learning_rate": 1e-05,
      "loss": 5.736,
      "step": 15648
    },
    {
      "epoch": 0.848130081300813,
      "step": 15648,
      "training_loss": 7.1933770179748535
    },
    {
      "epoch": 0.8481842818428185,
      "step": 15649,
      "training_loss": 6.8024516105651855
    },
    {
      "epoch": 0.8482384823848238,
      "step": 15650,
      "training_loss": 5.960134506225586
    },
    {
      "epoch": 0.8482926829268292,
      "step": 15651,
      "training_loss": 6.495024681091309
    },
    {
      "epoch": 0.8483468834688347,
      "grad_norm": 24.00705909729004,
      "learning_rate": 1e-05,
      "loss": 6.6127,
      "step": 15652
    },
    {
      "epoch": 0.8483468834688347,
      "step": 15652,
      "training_loss": 7.409718990325928
    },
    {
      "epoch": 0.8484010840108401,
      "step": 15653,
      "training_loss": 6.332759380340576
    },
    {
      "epoch": 0.8484552845528456,
      "step": 15654,
      "training_loss": 7.401457786560059
    },
    {
      "epoch": 0.8485094850948509,
      "step": 15655,
      "training_loss": 6.6095051765441895
    },
    {
      "epoch": 0.8485636856368564,
      "grad_norm": 23.888017654418945,
      "learning_rate": 1e-05,
      "loss": 6.9384,
      "step": 15656
    },
    {
      "epoch": 0.8485636856368564,
      "step": 15656,
      "training_loss": 6.478884220123291
    },
    {
      "epoch": 0.8486178861788618,
      "step": 15657,
      "training_loss": 6.896421909332275
    },
    {
      "epoch": 0.8486720867208672,
      "step": 15658,
      "training_loss": 7.344728469848633
    },
    {
      "epoch": 0.8487262872628726,
      "step": 15659,
      "training_loss": 6.40488338470459
    },
    {
      "epoch": 0.848780487804878,
      "grad_norm": 22.51392364501953,
      "learning_rate": 1e-05,
      "loss": 6.7812,
      "step": 15660
    },
    {
      "epoch": 0.848780487804878,
      "step": 15660,
      "training_loss": 6.381862640380859
    },
    {
      "epoch": 0.8488346883468835,
      "step": 15661,
      "training_loss": 7.343751430511475
    },
    {
      "epoch": 0.8488888888888889,
      "step": 15662,
      "training_loss": 5.886399745941162
    },
    {
      "epoch": 0.8489430894308944,
      "step": 15663,
      "training_loss": 7.553589820861816
    },
    {
      "epoch": 0.8489972899728997,
      "grad_norm": 29.804035186767578,
      "learning_rate": 1e-05,
      "loss": 6.7914,
      "step": 15664
    },
    {
      "epoch": 0.8489972899728997,
      "step": 15664,
      "training_loss": 6.6912760734558105
    },
    {
      "epoch": 0.8490514905149051,
      "step": 15665,
      "training_loss": 6.791365146636963
    },
    {
      "epoch": 0.8491056910569106,
      "step": 15666,
      "training_loss": 5.915872097015381
    },
    {
      "epoch": 0.849159891598916,
      "step": 15667,
      "training_loss": 6.550736904144287
    },
    {
      "epoch": 0.8492140921409214,
      "grad_norm": 20.032032012939453,
      "learning_rate": 1e-05,
      "loss": 6.4873,
      "step": 15668
    },
    {
      "epoch": 0.8492140921409214,
      "step": 15668,
      "training_loss": 5.732667446136475
    },
    {
      "epoch": 0.8492682926829268,
      "step": 15669,
      "training_loss": 6.760161876678467
    },
    {
      "epoch": 0.8493224932249323,
      "step": 15670,
      "training_loss": 5.137715816497803
    },
    {
      "epoch": 0.8493766937669377,
      "step": 15671,
      "training_loss": 7.799076557159424
    },
    {
      "epoch": 0.8494308943089431,
      "grad_norm": 20.50324249267578,
      "learning_rate": 1e-05,
      "loss": 6.3574,
      "step": 15672
    },
    {
      "epoch": 0.8494308943089431,
      "step": 15672,
      "training_loss": 7.641541957855225
    },
    {
      "epoch": 0.8494850948509485,
      "step": 15673,
      "training_loss": 6.136043071746826
    },
    {
      "epoch": 0.8495392953929539,
      "step": 15674,
      "training_loss": 6.471138954162598
    },
    {
      "epoch": 0.8495934959349594,
      "step": 15675,
      "training_loss": 7.2259521484375
    },
    {
      "epoch": 0.8496476964769648,
      "grad_norm": 20.184024810791016,
      "learning_rate": 1e-05,
      "loss": 6.8687,
      "step": 15676
    },
    {
      "epoch": 0.8496476964769648,
      "step": 15676,
      "training_loss": 7.079265594482422
    },
    {
      "epoch": 0.8497018970189701,
      "step": 15677,
      "training_loss": 6.671935081481934
    },
    {
      "epoch": 0.8497560975609756,
      "step": 15678,
      "training_loss": 7.193958759307861
    },
    {
      "epoch": 0.849810298102981,
      "step": 15679,
      "training_loss": 6.716170787811279
    },
    {
      "epoch": 0.8498644986449865,
      "grad_norm": 31.933324813842773,
      "learning_rate": 1e-05,
      "loss": 6.9153,
      "step": 15680
    },
    {
      "epoch": 0.8498644986449865,
      "step": 15680,
      "training_loss": 7.652822494506836
    },
    {
      "epoch": 0.8499186991869919,
      "step": 15681,
      "training_loss": 7.0486297607421875
    },
    {
      "epoch": 0.8499728997289973,
      "step": 15682,
      "training_loss": 7.817652225494385
    },
    {
      "epoch": 0.8500271002710027,
      "step": 15683,
      "training_loss": 6.595215320587158
    },
    {
      "epoch": 0.8500813008130081,
      "grad_norm": 28.87801170349121,
      "learning_rate": 1e-05,
      "loss": 7.2786,
      "step": 15684
    },
    {
      "epoch": 0.8500813008130081,
      "step": 15684,
      "training_loss": 6.099503040313721
    },
    {
      "epoch": 0.8501355013550136,
      "step": 15685,
      "training_loss": 6.856410503387451
    },
    {
      "epoch": 0.8501897018970189,
      "step": 15686,
      "training_loss": 7.279886722564697
    },
    {
      "epoch": 0.8502439024390244,
      "step": 15687,
      "training_loss": 6.292049407958984
    },
    {
      "epoch": 0.8502981029810298,
      "grad_norm": 44.858985900878906,
      "learning_rate": 1e-05,
      "loss": 6.632,
      "step": 15688
    },
    {
      "epoch": 0.8502981029810298,
      "step": 15688,
      "training_loss": 7.019906997680664
    },
    {
      "epoch": 0.8503523035230353,
      "step": 15689,
      "training_loss": 5.988574981689453
    },
    {
      "epoch": 0.8504065040650407,
      "step": 15690,
      "training_loss": 6.4373579025268555
    },
    {
      "epoch": 0.850460704607046,
      "step": 15691,
      "training_loss": 6.293611526489258
    },
    {
      "epoch": 0.8505149051490515,
      "grad_norm": 25.725887298583984,
      "learning_rate": 1e-05,
      "loss": 6.4349,
      "step": 15692
    },
    {
      "epoch": 0.8505149051490515,
      "step": 15692,
      "training_loss": 4.186891555786133
    },
    {
      "epoch": 0.8505691056910569,
      "step": 15693,
      "training_loss": 6.054296016693115
    },
    {
      "epoch": 0.8506233062330624,
      "step": 15694,
      "training_loss": 7.043422222137451
    },
    {
      "epoch": 0.8506775067750677,
      "step": 15695,
      "training_loss": 6.859200477600098
    },
    {
      "epoch": 0.8507317073170731,
      "grad_norm": 26.730981826782227,
      "learning_rate": 1e-05,
      "loss": 6.036,
      "step": 15696
    },
    {
      "epoch": 0.8507317073170731,
      "step": 15696,
      "training_loss": 6.949191093444824
    },
    {
      "epoch": 0.8507859078590786,
      "step": 15697,
      "training_loss": 6.431461811065674
    },
    {
      "epoch": 0.850840108401084,
      "step": 15698,
      "training_loss": 3.613306760787964
    },
    {
      "epoch": 0.8508943089430895,
      "step": 15699,
      "training_loss": 9.340385437011719
    },
    {
      "epoch": 0.8509485094850948,
      "grad_norm": 45.525753021240234,
      "learning_rate": 1e-05,
      "loss": 6.5836,
      "step": 15700
    },
    {
      "epoch": 0.8509485094850948,
      "step": 15700,
      "training_loss": 6.664449214935303
    },
    {
      "epoch": 0.8510027100271003,
      "step": 15701,
      "training_loss": 5.78224515914917
    },
    {
      "epoch": 0.8510569105691057,
      "step": 15702,
      "training_loss": 5.434047698974609
    },
    {
      "epoch": 0.8511111111111112,
      "step": 15703,
      "training_loss": 7.118587017059326
    },
    {
      "epoch": 0.8511653116531165,
      "grad_norm": 24.060632705688477,
      "learning_rate": 1e-05,
      "loss": 6.2498,
      "step": 15704
    },
    {
      "epoch": 0.8511653116531165,
      "step": 15704,
      "training_loss": 5.927682876586914
    },
    {
      "epoch": 0.8512195121951219,
      "step": 15705,
      "training_loss": 4.435614585876465
    },
    {
      "epoch": 0.8512737127371274,
      "step": 15706,
      "training_loss": 6.979716777801514
    },
    {
      "epoch": 0.8513279132791328,
      "step": 15707,
      "training_loss": 7.754428386688232
    },
    {
      "epoch": 0.8513821138211383,
      "grad_norm": 23.348587036132812,
      "learning_rate": 1e-05,
      "loss": 6.2744,
      "step": 15708
    },
    {
      "epoch": 0.8513821138211383,
      "step": 15708,
      "training_loss": 5.3849592208862305
    },
    {
      "epoch": 0.8514363143631436,
      "step": 15709,
      "training_loss": 6.523815155029297
    },
    {
      "epoch": 0.851490514905149,
      "step": 15710,
      "training_loss": 6.871161937713623
    },
    {
      "epoch": 0.8515447154471545,
      "step": 15711,
      "training_loss": 6.13967227935791
    },
    {
      "epoch": 0.8515989159891599,
      "grad_norm": 22.301891326904297,
      "learning_rate": 1e-05,
      "loss": 6.2299,
      "step": 15712
    },
    {
      "epoch": 0.8515989159891599,
      "step": 15712,
      "training_loss": 6.361731052398682
    },
    {
      "epoch": 0.8516531165311653,
      "step": 15713,
      "training_loss": 4.984652996063232
    },
    {
      "epoch": 0.8517073170731707,
      "step": 15714,
      "training_loss": 6.530109882354736
    },
    {
      "epoch": 0.8517615176151762,
      "step": 15715,
      "training_loss": 6.939359664916992
    },
    {
      "epoch": 0.8518157181571816,
      "grad_norm": 33.16731262207031,
      "learning_rate": 1e-05,
      "loss": 6.204,
      "step": 15716
    },
    {
      "epoch": 0.8518157181571816,
      "step": 15716,
      "training_loss": 7.740975856781006
    },
    {
      "epoch": 0.851869918699187,
      "step": 15717,
      "training_loss": 4.439187049865723
    },
    {
      "epoch": 0.8519241192411924,
      "step": 15718,
      "training_loss": 6.474024295806885
    },
    {
      "epoch": 0.8519783197831978,
      "step": 15719,
      "training_loss": 7.340818405151367
    },
    {
      "epoch": 0.8520325203252033,
      "grad_norm": 22.34210968017578,
      "learning_rate": 1e-05,
      "loss": 6.4988,
      "step": 15720
    },
    {
      "epoch": 0.8520325203252033,
      "step": 15720,
      "training_loss": 5.583864688873291
    },
    {
      "epoch": 0.8520867208672087,
      "step": 15721,
      "training_loss": 4.538800239562988
    },
    {
      "epoch": 0.852140921409214,
      "step": 15722,
      "training_loss": 8.505325317382812
    },
    {
      "epoch": 0.8521951219512195,
      "step": 15723,
      "training_loss": 5.128437042236328
    },
    {
      "epoch": 0.8522493224932249,
      "grad_norm": 49.518985748291016,
      "learning_rate": 1e-05,
      "loss": 5.9391,
      "step": 15724
    },
    {
      "epoch": 0.8522493224932249,
      "step": 15724,
      "training_loss": 5.958545207977295
    },
    {
      "epoch": 0.8523035230352304,
      "step": 15725,
      "training_loss": 6.501250267028809
    },
    {
      "epoch": 0.8523577235772358,
      "step": 15726,
      "training_loss": 4.6546125411987305
    },
    {
      "epoch": 0.8524119241192412,
      "step": 15727,
      "training_loss": 5.617193222045898
    },
    {
      "epoch": 0.8524661246612466,
      "grad_norm": 33.514644622802734,
      "learning_rate": 1e-05,
      "loss": 5.6829,
      "step": 15728
    },
    {
      "epoch": 0.8524661246612466,
      "step": 15728,
      "training_loss": 6.757119178771973
    },
    {
      "epoch": 0.852520325203252,
      "step": 15729,
      "training_loss": 6.106253147125244
    },
    {
      "epoch": 0.8525745257452575,
      "step": 15730,
      "training_loss": 8.18472957611084
    },
    {
      "epoch": 0.8526287262872628,
      "step": 15731,
      "training_loss": 7.109680652618408
    },
    {
      "epoch": 0.8526829268292683,
      "grad_norm": 31.581737518310547,
      "learning_rate": 1e-05,
      "loss": 7.0394,
      "step": 15732
    },
    {
      "epoch": 0.8526829268292683,
      "step": 15732,
      "training_loss": 7.158828258514404
    },
    {
      "epoch": 0.8527371273712737,
      "step": 15733,
      "training_loss": 6.924417495727539
    },
    {
      "epoch": 0.8527913279132792,
      "step": 15734,
      "training_loss": 6.235618591308594
    },
    {
      "epoch": 0.8528455284552846,
      "step": 15735,
      "training_loss": 6.570685386657715
    },
    {
      "epoch": 0.8528997289972899,
      "grad_norm": 70.29644775390625,
      "learning_rate": 1e-05,
      "loss": 6.7224,
      "step": 15736
    },
    {
      "epoch": 0.8528997289972899,
      "step": 15736,
      "training_loss": 6.471790790557861
    },
    {
      "epoch": 0.8529539295392954,
      "step": 15737,
      "training_loss": 6.0786542892456055
    },
    {
      "epoch": 0.8530081300813008,
      "step": 15738,
      "training_loss": 6.766406536102295
    },
    {
      "epoch": 0.8530623306233063,
      "step": 15739,
      "training_loss": 4.414475440979004
    },
    {
      "epoch": 0.8531165311653116,
      "grad_norm": 30.703895568847656,
      "learning_rate": 1e-05,
      "loss": 5.9328,
      "step": 15740
    },
    {
      "epoch": 0.8531165311653116,
      "step": 15740,
      "training_loss": 7.599039077758789
    },
    {
      "epoch": 0.853170731707317,
      "step": 15741,
      "training_loss": 5.923820972442627
    },
    {
      "epoch": 0.8532249322493225,
      "step": 15742,
      "training_loss": 7.500741481781006
    },
    {
      "epoch": 0.8532791327913279,
      "step": 15743,
      "training_loss": 6.21028470993042
    },
    {
      "epoch": 0.8533333333333334,
      "grad_norm": 33.76821517944336,
      "learning_rate": 1e-05,
      "loss": 6.8085,
      "step": 15744
    },
    {
      "epoch": 0.8533333333333334,
      "step": 15744,
      "training_loss": 6.596860885620117
    },
    {
      "epoch": 0.8533875338753387,
      "step": 15745,
      "training_loss": 6.522271633148193
    },
    {
      "epoch": 0.8534417344173442,
      "step": 15746,
      "training_loss": 6.482017517089844
    },
    {
      "epoch": 0.8534959349593496,
      "step": 15747,
      "training_loss": 6.08235502243042
    },
    {
      "epoch": 0.8535501355013551,
      "grad_norm": 44.514556884765625,
      "learning_rate": 1e-05,
      "loss": 6.4209,
      "step": 15748
    },
    {
      "epoch": 0.8535501355013551,
      "step": 15748,
      "training_loss": 6.314939975738525
    },
    {
      "epoch": 0.8536043360433604,
      "step": 15749,
      "training_loss": 5.821998119354248
    },
    {
      "epoch": 0.8536585365853658,
      "step": 15750,
      "training_loss": 5.039882183074951
    },
    {
      "epoch": 0.8537127371273713,
      "step": 15751,
      "training_loss": 6.356532096862793
    },
    {
      "epoch": 0.8537669376693767,
      "grad_norm": 37.102176666259766,
      "learning_rate": 1e-05,
      "loss": 5.8833,
      "step": 15752
    },
    {
      "epoch": 0.8537669376693767,
      "step": 15752,
      "training_loss": 6.153640270233154
    },
    {
      "epoch": 0.8538211382113822,
      "step": 15753,
      "training_loss": 5.404195308685303
    },
    {
      "epoch": 0.8538753387533875,
      "step": 15754,
      "training_loss": 7.339447975158691
    },
    {
      "epoch": 0.853929539295393,
      "step": 15755,
      "training_loss": 5.429233551025391
    },
    {
      "epoch": 0.8539837398373984,
      "grad_norm": 30.702592849731445,
      "learning_rate": 1e-05,
      "loss": 6.0816,
      "step": 15756
    },
    {
      "epoch": 0.8539837398373984,
      "step": 15756,
      "training_loss": 6.569699764251709
    },
    {
      "epoch": 0.8540379403794038,
      "step": 15757,
      "training_loss": 7.03096342086792
    },
    {
      "epoch": 0.8540921409214092,
      "step": 15758,
      "training_loss": 7.236912727355957
    },
    {
      "epoch": 0.8541463414634146,
      "step": 15759,
      "training_loss": 4.155892372131348
    },
    {
      "epoch": 0.8542005420054201,
      "grad_norm": 24.4616756439209,
      "learning_rate": 1e-05,
      "loss": 6.2484,
      "step": 15760
    },
    {
      "epoch": 0.8542005420054201,
      "step": 15760,
      "training_loss": 6.725937366485596
    },
    {
      "epoch": 0.8542547425474255,
      "step": 15761,
      "training_loss": 7.054769515991211
    },
    {
      "epoch": 0.8543089430894308,
      "step": 15762,
      "training_loss": 7.462185859680176
    },
    {
      "epoch": 0.8543631436314363,
      "step": 15763,
      "training_loss": 6.907590866088867
    },
    {
      "epoch": 0.8544173441734417,
      "grad_norm": 44.49028396606445,
      "learning_rate": 1e-05,
      "loss": 7.0376,
      "step": 15764
    },
    {
      "epoch": 0.8544173441734417,
      "step": 15764,
      "training_loss": 7.808427333831787
    },
    {
      "epoch": 0.8544715447154472,
      "step": 15765,
      "training_loss": 4.54935359954834
    },
    {
      "epoch": 0.8545257452574526,
      "step": 15766,
      "training_loss": 5.146352291107178
    },
    {
      "epoch": 0.854579945799458,
      "step": 15767,
      "training_loss": 6.553454875946045
    },
    {
      "epoch": 0.8546341463414634,
      "grad_norm": 25.184967041015625,
      "learning_rate": 1e-05,
      "loss": 6.0144,
      "step": 15768
    },
    {
      "epoch": 0.8546341463414634,
      "step": 15768,
      "training_loss": 6.828233242034912
    },
    {
      "epoch": 0.8546883468834688,
      "step": 15769,
      "training_loss": 5.813889026641846
    },
    {
      "epoch": 0.8547425474254743,
      "step": 15770,
      "training_loss": 8.228904724121094
    },
    {
      "epoch": 0.8547967479674796,
      "step": 15771,
      "training_loss": 5.071536064147949
    },
    {
      "epoch": 0.8548509485094851,
      "grad_norm": 51.797977447509766,
      "learning_rate": 1e-05,
      "loss": 6.4856,
      "step": 15772
    },
    {
      "epoch": 0.8548509485094851,
      "step": 15772,
      "training_loss": 6.713597774505615
    },
    {
      "epoch": 0.8549051490514905,
      "step": 15773,
      "training_loss": 7.236448764801025
    },
    {
      "epoch": 0.854959349593496,
      "step": 15774,
      "training_loss": 5.653167247772217
    },
    {
      "epoch": 0.8550135501355014,
      "step": 15775,
      "training_loss": 6.645050048828125
    },
    {
      "epoch": 0.8550677506775067,
      "grad_norm": 24.316423416137695,
      "learning_rate": 1e-05,
      "loss": 6.5621,
      "step": 15776
    },
    {
      "epoch": 0.8550677506775067,
      "step": 15776,
      "training_loss": 6.604381084442139
    },
    {
      "epoch": 0.8551219512195122,
      "step": 15777,
      "training_loss": 6.868334770202637
    },
    {
      "epoch": 0.8551761517615176,
      "step": 15778,
      "training_loss": 6.105519771575928
    },
    {
      "epoch": 0.8552303523035231,
      "step": 15779,
      "training_loss": 7.881011962890625
    },
    {
      "epoch": 0.8552845528455284,
      "grad_norm": 34.31187057495117,
      "learning_rate": 1e-05,
      "loss": 6.8648,
      "step": 15780
    },
    {
      "epoch": 0.8552845528455284,
      "step": 15780,
      "training_loss": 3.9787492752075195
    },
    {
      "epoch": 0.8553387533875338,
      "step": 15781,
      "training_loss": 6.2441606521606445
    },
    {
      "epoch": 0.8553929539295393,
      "step": 15782,
      "training_loss": 3.0847716331481934
    },
    {
      "epoch": 0.8554471544715447,
      "step": 15783,
      "training_loss": 3.4813337326049805
    },
    {
      "epoch": 0.8555013550135502,
      "grad_norm": 29.359764099121094,
      "learning_rate": 1e-05,
      "loss": 4.1973,
      "step": 15784
    },
    {
      "epoch": 0.8555013550135502,
      "step": 15784,
      "training_loss": 6.992625713348389
    },
    {
      "epoch": 0.8555555555555555,
      "step": 15785,
      "training_loss": 5.422153472900391
    },
    {
      "epoch": 0.855609756097561,
      "step": 15786,
      "training_loss": 6.950894355773926
    },
    {
      "epoch": 0.8556639566395664,
      "step": 15787,
      "training_loss": 5.442015171051025
    },
    {
      "epoch": 0.8557181571815718,
      "grad_norm": 33.79671096801758,
      "learning_rate": 1e-05,
      "loss": 6.2019,
      "step": 15788
    },
    {
      "epoch": 0.8557181571815718,
      "step": 15788,
      "training_loss": 6.852908611297607
    },
    {
      "epoch": 0.8557723577235772,
      "step": 15789,
      "training_loss": 6.182816982269287
    },
    {
      "epoch": 0.8558265582655826,
      "step": 15790,
      "training_loss": 6.767577648162842
    },
    {
      "epoch": 0.8558807588075881,
      "step": 15791,
      "training_loss": 6.763702869415283
    },
    {
      "epoch": 0.8559349593495935,
      "grad_norm": 16.444984436035156,
      "learning_rate": 1e-05,
      "loss": 6.6418,
      "step": 15792
    },
    {
      "epoch": 0.8559349593495935,
      "step": 15792,
      "training_loss": 5.463323593139648
    },
    {
      "epoch": 0.855989159891599,
      "step": 15793,
      "training_loss": 4.375739574432373
    },
    {
      "epoch": 0.8560433604336043,
      "step": 15794,
      "training_loss": 6.685143947601318
    },
    {
      "epoch": 0.8560975609756097,
      "step": 15795,
      "training_loss": 6.8046088218688965
    },
    {
      "epoch": 0.8561517615176152,
      "grad_norm": 26.89276885986328,
      "learning_rate": 1e-05,
      "loss": 5.8322,
      "step": 15796
    },
    {
      "epoch": 0.8561517615176152,
      "step": 15796,
      "training_loss": 7.645333290100098
    },
    {
      "epoch": 0.8562059620596206,
      "step": 15797,
      "training_loss": 7.326912879943848
    },
    {
      "epoch": 0.856260162601626,
      "step": 15798,
      "training_loss": 5.7625627517700195
    },
    {
      "epoch": 0.8563143631436314,
      "step": 15799,
      "training_loss": 6.757956027984619
    },
    {
      "epoch": 0.8563685636856369,
      "grad_norm": 31.218252182006836,
      "learning_rate": 1e-05,
      "loss": 6.8732,
      "step": 15800
    },
    {
      "epoch": 0.8563685636856369,
      "step": 15800,
      "training_loss": 5.986872673034668
    },
    {
      "epoch": 0.8564227642276423,
      "step": 15801,
      "training_loss": 5.67205810546875
    },
    {
      "epoch": 0.8564769647696477,
      "step": 15802,
      "training_loss": 5.574733734130859
    },
    {
      "epoch": 0.8565311653116531,
      "step": 15803,
      "training_loss": 5.6984710693359375
    },
    {
      "epoch": 0.8565853658536585,
      "grad_norm": 31.479631423950195,
      "learning_rate": 1e-05,
      "loss": 5.733,
      "step": 15804
    },
    {
      "epoch": 0.8565853658536585,
      "step": 15804,
      "training_loss": 6.791209697723389
    },
    {
      "epoch": 0.856639566395664,
      "step": 15805,
      "training_loss": 6.60112190246582
    },
    {
      "epoch": 0.8566937669376694,
      "step": 15806,
      "training_loss": 5.254605293273926
    },
    {
      "epoch": 0.8567479674796747,
      "step": 15807,
      "training_loss": 4.956500053405762
    },
    {
      "epoch": 0.8568021680216802,
      "grad_norm": 65.7602310180664,
      "learning_rate": 1e-05,
      "loss": 5.9009,
      "step": 15808
    },
    {
      "epoch": 0.8568021680216802,
      "step": 15808,
      "training_loss": 5.325283050537109
    },
    {
      "epoch": 0.8568563685636856,
      "step": 15809,
      "training_loss": 7.516937732696533
    },
    {
      "epoch": 0.8569105691056911,
      "step": 15810,
      "training_loss": 5.981832504272461
    },
    {
      "epoch": 0.8569647696476965,
      "step": 15811,
      "training_loss": 6.653369903564453
    },
    {
      "epoch": 0.8570189701897019,
      "grad_norm": 22.775798797607422,
      "learning_rate": 1e-05,
      "loss": 6.3694,
      "step": 15812
    },
    {
      "epoch": 0.8570189701897019,
      "step": 15812,
      "training_loss": 7.179583549499512
    },
    {
      "epoch": 0.8570731707317073,
      "step": 15813,
      "training_loss": 6.033339977264404
    },
    {
      "epoch": 0.8571273712737127,
      "step": 15814,
      "training_loss": 6.692203521728516
    },
    {
      "epoch": 0.8571815718157182,
      "step": 15815,
      "training_loss": 8.326396942138672
    },
    {
      "epoch": 0.8572357723577235,
      "grad_norm": 25.14867401123047,
      "learning_rate": 1e-05,
      "loss": 7.0579,
      "step": 15816
    },
    {
      "epoch": 0.8572357723577235,
      "step": 15816,
      "training_loss": 6.472387790679932
    },
    {
      "epoch": 0.857289972899729,
      "step": 15817,
      "training_loss": 7.408709526062012
    },
    {
      "epoch": 0.8573441734417344,
      "step": 15818,
      "training_loss": 5.825234413146973
    },
    {
      "epoch": 0.8573983739837399,
      "step": 15819,
      "training_loss": 7.04392147064209
    },
    {
      "epoch": 0.8574525745257453,
      "grad_norm": 40.80459213256836,
      "learning_rate": 1e-05,
      "loss": 6.6876,
      "step": 15820
    },
    {
      "epoch": 0.8574525745257453,
      "step": 15820,
      "training_loss": 6.90974760055542
    },
    {
      "epoch": 0.8575067750677506,
      "step": 15821,
      "training_loss": 6.714648246765137
    },
    {
      "epoch": 0.8575609756097561,
      "step": 15822,
      "training_loss": 7.049258232116699
    },
    {
      "epoch": 0.8576151761517615,
      "step": 15823,
      "training_loss": 6.383600234985352
    },
    {
      "epoch": 0.857669376693767,
      "grad_norm": 32.261173248291016,
      "learning_rate": 1e-05,
      "loss": 6.7643,
      "step": 15824
    },
    {
      "epoch": 0.857669376693767,
      "step": 15824,
      "training_loss": 5.8719868659973145
    },
    {
      "epoch": 0.8577235772357723,
      "step": 15825,
      "training_loss": 3.122307777404785
    },
    {
      "epoch": 0.8577777777777778,
      "step": 15826,
      "training_loss": 5.534736633300781
    },
    {
      "epoch": 0.8578319783197832,
      "step": 15827,
      "training_loss": 6.09573221206665
    },
    {
      "epoch": 0.8578861788617886,
      "grad_norm": 29.856229782104492,
      "learning_rate": 1e-05,
      "loss": 5.1562,
      "step": 15828
    },
    {
      "epoch": 0.8578861788617886,
      "step": 15828,
      "training_loss": 7.0698466300964355
    },
    {
      "epoch": 0.8579403794037941,
      "step": 15829,
      "training_loss": 6.474331855773926
    },
    {
      "epoch": 0.8579945799457994,
      "step": 15830,
      "training_loss": 7.010588645935059
    },
    {
      "epoch": 0.8580487804878049,
      "step": 15831,
      "training_loss": 6.004958152770996
    },
    {
      "epoch": 0.8581029810298103,
      "grad_norm": 29.269540786743164,
      "learning_rate": 1e-05,
      "loss": 6.6399,
      "step": 15832
    },
    {
      "epoch": 0.8581029810298103,
      "step": 15832,
      "training_loss": 5.26887845993042
    },
    {
      "epoch": 0.8581571815718158,
      "step": 15833,
      "training_loss": 6.640378952026367
    },
    {
      "epoch": 0.8582113821138211,
      "step": 15834,
      "training_loss": 7.358988285064697
    },
    {
      "epoch": 0.8582655826558265,
      "step": 15835,
      "training_loss": 8.37492847442627
    },
    {
      "epoch": 0.858319783197832,
      "grad_norm": 38.69807052612305,
      "learning_rate": 1e-05,
      "loss": 6.9108,
      "step": 15836
    },
    {
      "epoch": 0.858319783197832,
      "step": 15836,
      "training_loss": 6.91485071182251
    },
    {
      "epoch": 0.8583739837398374,
      "step": 15837,
      "training_loss": 4.430662631988525
    },
    {
      "epoch": 0.8584281842818429,
      "step": 15838,
      "training_loss": 6.9822187423706055
    },
    {
      "epoch": 0.8584823848238482,
      "step": 15839,
      "training_loss": 6.024082660675049
    },
    {
      "epoch": 0.8585365853658536,
      "grad_norm": 26.41073989868164,
      "learning_rate": 1e-05,
      "loss": 6.088,
      "step": 15840
    },
    {
      "epoch": 0.8585365853658536,
      "step": 15840,
      "training_loss": 6.961050987243652
    },
    {
      "epoch": 0.8585907859078591,
      "step": 15841,
      "training_loss": 7.656367778778076
    },
    {
      "epoch": 0.8586449864498645,
      "step": 15842,
      "training_loss": 6.31082010269165
    },
    {
      "epoch": 0.8586991869918699,
      "step": 15843,
      "training_loss": 4.895190715789795
    },
    {
      "epoch": 0.8587533875338753,
      "grad_norm": 26.891117095947266,
      "learning_rate": 1e-05,
      "loss": 6.4559,
      "step": 15844
    },
    {
      "epoch": 0.8587533875338753,
      "step": 15844,
      "training_loss": 6.806182384490967
    },
    {
      "epoch": 0.8588075880758808,
      "step": 15845,
      "training_loss": 7.185642242431641
    },
    {
      "epoch": 0.8588617886178862,
      "step": 15846,
      "training_loss": 6.755191802978516
    },
    {
      "epoch": 0.8589159891598916,
      "step": 15847,
      "training_loss": 6.429551601409912
    },
    {
      "epoch": 0.858970189701897,
      "grad_norm": 66.67420196533203,
      "learning_rate": 1e-05,
      "loss": 6.7941,
      "step": 15848
    },
    {
      "epoch": 0.858970189701897,
      "step": 15848,
      "training_loss": 6.425529956817627
    },
    {
      "epoch": 0.8590243902439024,
      "step": 15849,
      "training_loss": 7.402030944824219
    },
    {
      "epoch": 0.8590785907859079,
      "step": 15850,
      "training_loss": 5.480368137359619
    },
    {
      "epoch": 0.8591327913279133,
      "step": 15851,
      "training_loss": 7.735970973968506
    },
    {
      "epoch": 0.8591869918699186,
      "grad_norm": 24.877431869506836,
      "learning_rate": 1e-05,
      "loss": 6.761,
      "step": 15852
    },
    {
      "epoch": 0.8591869918699186,
      "step": 15852,
      "training_loss": 6.073821544647217
    },
    {
      "epoch": 0.8592411924119241,
      "step": 15853,
      "training_loss": 6.770602226257324
    },
    {
      "epoch": 0.8592953929539295,
      "step": 15854,
      "training_loss": 4.220327377319336
    },
    {
      "epoch": 0.859349593495935,
      "step": 15855,
      "training_loss": 6.355815887451172
    },
    {
      "epoch": 0.8594037940379404,
      "grad_norm": 24.998865127563477,
      "learning_rate": 1e-05,
      "loss": 5.8551,
      "step": 15856
    },
    {
      "epoch": 0.8594037940379404,
      "step": 15856,
      "training_loss": 7.6529221534729
    },
    {
      "epoch": 0.8594579945799458,
      "step": 15857,
      "training_loss": 5.267852306365967
    },
    {
      "epoch": 0.8595121951219512,
      "step": 15858,
      "training_loss": 5.7891693115234375
    },
    {
      "epoch": 0.8595663956639567,
      "step": 15859,
      "training_loss": 6.547555923461914
    },
    {
      "epoch": 0.8596205962059621,
      "grad_norm": 42.31875228881836,
      "learning_rate": 1e-05,
      "loss": 6.3144,
      "step": 15860
    },
    {
      "epoch": 0.8596205962059621,
      "step": 15860,
      "training_loss": 7.527558326721191
    },
    {
      "epoch": 0.8596747967479674,
      "step": 15861,
      "training_loss": 6.281736373901367
    },
    {
      "epoch": 0.8597289972899729,
      "step": 15862,
      "training_loss": 6.414126873016357
    },
    {
      "epoch": 0.8597831978319783,
      "step": 15863,
      "training_loss": 7.34613037109375
    },
    {
      "epoch": 0.8598373983739838,
      "grad_norm": 20.219186782836914,
      "learning_rate": 1e-05,
      "loss": 6.8924,
      "step": 15864
    },
    {
      "epoch": 0.8598373983739838,
      "step": 15864,
      "training_loss": 8.141878128051758
    },
    {
      "epoch": 0.8598915989159892,
      "step": 15865,
      "training_loss": 6.181607723236084
    },
    {
      "epoch": 0.8599457994579945,
      "step": 15866,
      "training_loss": 6.168467044830322
    },
    {
      "epoch": 0.86,
      "step": 15867,
      "training_loss": 6.340498447418213
    },
    {
      "epoch": 0.8600542005420054,
      "grad_norm": 32.949180603027344,
      "learning_rate": 1e-05,
      "loss": 6.7081,
      "step": 15868
    },
    {
      "epoch": 0.8600542005420054,
      "step": 15868,
      "training_loss": 7.071298599243164
    },
    {
      "epoch": 0.8601084010840109,
      "step": 15869,
      "training_loss": 6.9721174240112305
    },
    {
      "epoch": 0.8601626016260162,
      "step": 15870,
      "training_loss": 6.81941032409668
    },
    {
      "epoch": 0.8602168021680217,
      "step": 15871,
      "training_loss": 5.21145486831665
    },
    {
      "epoch": 0.8602710027100271,
      "grad_norm": 18.774293899536133,
      "learning_rate": 1e-05,
      "loss": 6.5186,
      "step": 15872
    },
    {
      "epoch": 0.8602710027100271,
      "step": 15872,
      "training_loss": 7.019750595092773
    },
    {
      "epoch": 0.8603252032520325,
      "step": 15873,
      "training_loss": 4.600251197814941
    },
    {
      "epoch": 0.860379403794038,
      "step": 15874,
      "training_loss": 6.618649959564209
    },
    {
      "epoch": 0.8604336043360433,
      "step": 15875,
      "training_loss": 6.052556037902832
    },
    {
      "epoch": 0.8604878048780488,
      "grad_norm": 27.46235466003418,
      "learning_rate": 1e-05,
      "loss": 6.0728,
      "step": 15876
    },
    {
      "epoch": 0.8604878048780488,
      "step": 15876,
      "training_loss": 5.017549514770508
    },
    {
      "epoch": 0.8605420054200542,
      "step": 15877,
      "training_loss": 6.434089660644531
    },
    {
      "epoch": 0.8605962059620597,
      "step": 15878,
      "training_loss": 7.402684211730957
    },
    {
      "epoch": 0.860650406504065,
      "step": 15879,
      "training_loss": 7.366841793060303
    },
    {
      "epoch": 0.8607046070460704,
      "grad_norm": 41.7295036315918,
      "learning_rate": 1e-05,
      "loss": 6.5553,
      "step": 15880
    },
    {
      "epoch": 0.8607046070460704,
      "step": 15880,
      "training_loss": 5.404162883758545
    },
    {
      "epoch": 0.8607588075880759,
      "step": 15881,
      "training_loss": 7.217970371246338
    },
    {
      "epoch": 0.8608130081300813,
      "step": 15882,
      "training_loss": 7.330438613891602
    },
    {
      "epoch": 0.8608672086720868,
      "step": 15883,
      "training_loss": 5.999998569488525
    },
    {
      "epoch": 0.8609214092140921,
      "grad_norm": 21.45440673828125,
      "learning_rate": 1e-05,
      "loss": 6.4881,
      "step": 15884
    },
    {
      "epoch": 0.8609214092140921,
      "step": 15884,
      "training_loss": 8.332008361816406
    },
    {
      "epoch": 0.8609756097560975,
      "step": 15885,
      "training_loss": 5.705512523651123
    },
    {
      "epoch": 0.861029810298103,
      "step": 15886,
      "training_loss": 8.12780475616455
    },
    {
      "epoch": 0.8610840108401084,
      "step": 15887,
      "training_loss": 6.8252668380737305
    },
    {
      "epoch": 0.8611382113821138,
      "grad_norm": 36.35545349121094,
      "learning_rate": 1e-05,
      "loss": 7.2476,
      "step": 15888
    },
    {
      "epoch": 0.8611382113821138,
      "step": 15888,
      "training_loss": 6.995296001434326
    },
    {
      "epoch": 0.8611924119241192,
      "step": 15889,
      "training_loss": 6.7024149894714355
    },
    {
      "epoch": 0.8612466124661247,
      "step": 15890,
      "training_loss": 5.149474143981934
    },
    {
      "epoch": 0.8613008130081301,
      "step": 15891,
      "training_loss": 6.795468807220459
    },
    {
      "epoch": 0.8613550135501356,
      "grad_norm": 34.22947311401367,
      "learning_rate": 1e-05,
      "loss": 6.4107,
      "step": 15892
    },
    {
      "epoch": 0.8613550135501356,
      "step": 15892,
      "training_loss": 7.07926082611084
    },
    {
      "epoch": 0.8614092140921409,
      "step": 15893,
      "training_loss": 5.770559310913086
    },
    {
      "epoch": 0.8614634146341463,
      "step": 15894,
      "training_loss": 6.472820281982422
    },
    {
      "epoch": 0.8615176151761518,
      "step": 15895,
      "training_loss": 7.413013458251953
    },
    {
      "epoch": 0.8615718157181572,
      "grad_norm": 22.846424102783203,
      "learning_rate": 1e-05,
      "loss": 6.6839,
      "step": 15896
    },
    {
      "epoch": 0.8615718157181572,
      "step": 15896,
      "training_loss": 5.343864917755127
    },
    {
      "epoch": 0.8616260162601626,
      "step": 15897,
      "training_loss": 3.7219908237457275
    },
    {
      "epoch": 0.861680216802168,
      "step": 15898,
      "training_loss": 6.393690586090088
    },
    {
      "epoch": 0.8617344173441734,
      "step": 15899,
      "training_loss": 7.264069080352783
    },
    {
      "epoch": 0.8617886178861789,
      "grad_norm": 19.22279930114746,
      "learning_rate": 1e-05,
      "loss": 5.6809,
      "step": 15900
    },
    {
      "epoch": 0.8617886178861789,
      "step": 15900,
      "training_loss": 5.6656622886657715
    },
    {
      "epoch": 0.8618428184281843,
      "step": 15901,
      "training_loss": 7.048882484436035
    },
    {
      "epoch": 0.8618970189701897,
      "step": 15902,
      "training_loss": 6.816364288330078
    },
    {
      "epoch": 0.8619512195121951,
      "step": 15903,
      "training_loss": 7.0743727684021
    },
    {
      "epoch": 0.8620054200542006,
      "grad_norm": 43.58604431152344,
      "learning_rate": 1e-05,
      "loss": 6.6513,
      "step": 15904
    },
    {
      "epoch": 0.8620054200542006,
      "step": 15904,
      "training_loss": 6.67474889755249
    },
    {
      "epoch": 0.862059620596206,
      "step": 15905,
      "training_loss": 5.307103157043457
    },
    {
      "epoch": 0.8621138211382113,
      "step": 15906,
      "training_loss": 7.347202777862549
    },
    {
      "epoch": 0.8621680216802168,
      "step": 15907,
      "training_loss": 7.322250843048096
    },
    {
      "epoch": 0.8622222222222222,
      "grad_norm": 21.064624786376953,
      "learning_rate": 1e-05,
      "loss": 6.6628,
      "step": 15908
    },
    {
      "epoch": 0.8622222222222222,
      "step": 15908,
      "training_loss": 6.4759521484375
    },
    {
      "epoch": 0.8622764227642277,
      "step": 15909,
      "training_loss": 5.709850311279297
    },
    {
      "epoch": 0.8623306233062331,
      "step": 15910,
      "training_loss": 6.770322322845459
    },
    {
      "epoch": 0.8623848238482384,
      "step": 15911,
      "training_loss": 5.703092098236084
    },
    {
      "epoch": 0.8624390243902439,
      "grad_norm": 83.88695526123047,
      "learning_rate": 1e-05,
      "loss": 6.1648,
      "step": 15912
    },
    {
      "epoch": 0.8624390243902439,
      "step": 15912,
      "training_loss": 6.419878005981445
    },
    {
      "epoch": 0.8624932249322493,
      "step": 15913,
      "training_loss": 7.273090362548828
    },
    {
      "epoch": 0.8625474254742548,
      "step": 15914,
      "training_loss": 7.444719314575195
    },
    {
      "epoch": 0.8626016260162601,
      "step": 15915,
      "training_loss": 8.841997146606445
    },
    {
      "epoch": 0.8626558265582656,
      "grad_norm": 51.488189697265625,
      "learning_rate": 1e-05,
      "loss": 7.4949,
      "step": 15916
    },
    {
      "epoch": 0.8626558265582656,
      "step": 15916,
      "training_loss": 2.6920723915100098
    },
    {
      "epoch": 0.862710027100271,
      "step": 15917,
      "training_loss": 5.998380184173584
    },
    {
      "epoch": 0.8627642276422764,
      "step": 15918,
      "training_loss": 6.690509796142578
    },
    {
      "epoch": 0.8628184281842819,
      "step": 15919,
      "training_loss": 5.446986198425293
    },
    {
      "epoch": 0.8628726287262872,
      "grad_norm": 40.88755798339844,
      "learning_rate": 1e-05,
      "loss": 5.207,
      "step": 15920
    },
    {
      "epoch": 0.8628726287262872,
      "step": 15920,
      "training_loss": 3.25858998298645
    },
    {
      "epoch": 0.8629268292682927,
      "step": 15921,
      "training_loss": 6.694873809814453
    },
    {
      "epoch": 0.8629810298102981,
      "step": 15922,
      "training_loss": 2.6490464210510254
    },
    {
      "epoch": 0.8630352303523036,
      "step": 15923,
      "training_loss": 7.708303451538086
    },
    {
      "epoch": 0.8630894308943089,
      "grad_norm": 52.57291793823242,
      "learning_rate": 1e-05,
      "loss": 5.0777,
      "step": 15924
    },
    {
      "epoch": 0.8630894308943089,
      "step": 15924,
      "training_loss": 5.667257308959961
    },
    {
      "epoch": 0.8631436314363143,
      "step": 15925,
      "training_loss": 7.969515800476074
    },
    {
      "epoch": 0.8631978319783198,
      "step": 15926,
      "training_loss": 7.367327690124512
    },
    {
      "epoch": 0.8632520325203252,
      "step": 15927,
      "training_loss": 5.983645915985107
    },
    {
      "epoch": 0.8633062330623307,
      "grad_norm": 67.56226348876953,
      "learning_rate": 1e-05,
      "loss": 6.7469,
      "step": 15928
    },
    {
      "epoch": 0.8633062330623307,
      "step": 15928,
      "training_loss": 5.035013675689697
    },
    {
      "epoch": 0.863360433604336,
      "step": 15929,
      "training_loss": 5.492476463317871
    },
    {
      "epoch": 0.8634146341463415,
      "step": 15930,
      "training_loss": 6.698173522949219
    },
    {
      "epoch": 0.8634688346883469,
      "step": 15931,
      "training_loss": 7.168766498565674
    },
    {
      "epoch": 0.8635230352303523,
      "grad_norm": 21.230436325073242,
      "learning_rate": 1e-05,
      "loss": 6.0986,
      "step": 15932
    },
    {
      "epoch": 0.8635230352303523,
      "step": 15932,
      "training_loss": 6.054925441741943
    },
    {
      "epoch": 0.8635772357723577,
      "step": 15933,
      "training_loss": 7.0784478187561035
    },
    {
      "epoch": 0.8636314363143631,
      "step": 15934,
      "training_loss": 6.578872203826904
    },
    {
      "epoch": 0.8636856368563686,
      "step": 15935,
      "training_loss": 7.255064487457275
    },
    {
      "epoch": 0.863739837398374,
      "grad_norm": 39.35387420654297,
      "learning_rate": 1e-05,
      "loss": 6.7418,
      "step": 15936
    },
    {
      "epoch": 0.863739837398374,
      "step": 15936,
      "training_loss": 8.010611534118652
    },
    {
      "epoch": 0.8637940379403795,
      "step": 15937,
      "training_loss": 7.60454797744751
    },
    {
      "epoch": 0.8638482384823848,
      "step": 15938,
      "training_loss": 7.005908966064453
    },
    {
      "epoch": 0.8639024390243902,
      "step": 15939,
      "training_loss": 5.595242023468018
    },
    {
      "epoch": 0.8639566395663957,
      "grad_norm": 43.606117248535156,
      "learning_rate": 1e-05,
      "loss": 7.0541,
      "step": 15940
    },
    {
      "epoch": 0.8639566395663957,
      "step": 15940,
      "training_loss": 8.311017036437988
    },
    {
      "epoch": 0.8640108401084011,
      "step": 15941,
      "training_loss": 5.29608154296875
    },
    {
      "epoch": 0.8640650406504065,
      "step": 15942,
      "training_loss": 4.27159309387207
    },
    {
      "epoch": 0.8641192411924119,
      "step": 15943,
      "training_loss": 6.278393745422363
    },
    {
      "epoch": 0.8641734417344173,
      "grad_norm": 24.84879493713379,
      "learning_rate": 1e-05,
      "loss": 6.0393,
      "step": 15944
    },
    {
      "epoch": 0.8641734417344173,
      "step": 15944,
      "training_loss": 6.487741947174072
    },
    {
      "epoch": 0.8642276422764228,
      "step": 15945,
      "training_loss": 6.591466903686523
    },
    {
      "epoch": 0.8642818428184282,
      "step": 15946,
      "training_loss": 7.304347038269043
    },
    {
      "epoch": 0.8643360433604336,
      "step": 15947,
      "training_loss": 6.0179572105407715
    },
    {
      "epoch": 0.864390243902439,
      "grad_norm": 19.92055320739746,
      "learning_rate": 1e-05,
      "loss": 6.6004,
      "step": 15948
    },
    {
      "epoch": 0.864390243902439,
      "step": 15948,
      "training_loss": 4.911203861236572
    },
    {
      "epoch": 0.8644444444444445,
      "step": 15949,
      "training_loss": 5.541621685028076
    },
    {
      "epoch": 0.8644986449864499,
      "step": 15950,
      "training_loss": 6.47087287902832
    },
    {
      "epoch": 0.8645528455284552,
      "step": 15951,
      "training_loss": 6.167936325073242
    },
    {
      "epoch": 0.8646070460704607,
      "grad_norm": 38.6890754699707,
      "learning_rate": 1e-05,
      "loss": 5.7729,
      "step": 15952
    },
    {
      "epoch": 0.8646070460704607,
      "step": 15952,
      "training_loss": 6.187007427215576
    },
    {
      "epoch": 0.8646612466124661,
      "step": 15953,
      "training_loss": 6.517321586608887
    },
    {
      "epoch": 0.8647154471544716,
      "step": 15954,
      "training_loss": 6.587057113647461
    },
    {
      "epoch": 0.864769647696477,
      "step": 15955,
      "training_loss": 6.563413143157959
    },
    {
      "epoch": 0.8648238482384824,
      "grad_norm": 48.60643005371094,
      "learning_rate": 1e-05,
      "loss": 6.4637,
      "step": 15956
    },
    {
      "epoch": 0.8648238482384824,
      "step": 15956,
      "training_loss": 7.235547065734863
    },
    {
      "epoch": 0.8648780487804878,
      "step": 15957,
      "training_loss": 6.503778457641602
    },
    {
      "epoch": 0.8649322493224932,
      "step": 15958,
      "training_loss": 6.35544490814209
    },
    {
      "epoch": 0.8649864498644987,
      "step": 15959,
      "training_loss": 6.495248317718506
    },
    {
      "epoch": 0.865040650406504,
      "grad_norm": 25.71976089477539,
      "learning_rate": 1e-05,
      "loss": 6.6475,
      "step": 15960
    },
    {
      "epoch": 0.865040650406504,
      "step": 15960,
      "training_loss": 6.402416229248047
    },
    {
      "epoch": 0.8650948509485095,
      "step": 15961,
      "training_loss": 4.039399147033691
    },
    {
      "epoch": 0.8651490514905149,
      "step": 15962,
      "training_loss": 7.874966621398926
    },
    {
      "epoch": 0.8652032520325204,
      "step": 15963,
      "training_loss": 6.575196266174316
    },
    {
      "epoch": 0.8652574525745258,
      "grad_norm": 41.59730911254883,
      "learning_rate": 1e-05,
      "loss": 6.223,
      "step": 15964
    },
    {
      "epoch": 0.8652574525745258,
      "step": 15964,
      "training_loss": 2.528461456298828
    },
    {
      "epoch": 0.8653116531165311,
      "step": 15965,
      "training_loss": 7.527429580688477
    },
    {
      "epoch": 0.8653658536585366,
      "step": 15966,
      "training_loss": 6.991257190704346
    },
    {
      "epoch": 0.865420054200542,
      "step": 15967,
      "training_loss": 6.206392288208008
    },
    {
      "epoch": 0.8654742547425475,
      "grad_norm": 44.92317199707031,
      "learning_rate": 1e-05,
      "loss": 5.8134,
      "step": 15968
    },
    {
      "epoch": 0.8654742547425475,
      "step": 15968,
      "training_loss": 7.416842937469482
    },
    {
      "epoch": 0.8655284552845528,
      "step": 15969,
      "training_loss": 6.053779602050781
    },
    {
      "epoch": 0.8655826558265582,
      "step": 15970,
      "training_loss": 5.054602146148682
    },
    {
      "epoch": 0.8656368563685637,
      "step": 15971,
      "training_loss": 7.472684860229492
    },
    {
      "epoch": 0.8656910569105691,
      "grad_norm": 23.165674209594727,
      "learning_rate": 1e-05,
      "loss": 6.4995,
      "step": 15972
    },
    {
      "epoch": 0.8656910569105691,
      "step": 15972,
      "training_loss": 8.90368938446045
    },
    {
      "epoch": 0.8657452574525746,
      "step": 15973,
      "training_loss": 6.044017791748047
    },
    {
      "epoch": 0.8657994579945799,
      "step": 15974,
      "training_loss": 7.7047834396362305
    },
    {
      "epoch": 0.8658536585365854,
      "step": 15975,
      "training_loss": 5.568770885467529
    },
    {
      "epoch": 0.8659078590785908,
      "grad_norm": 20.1708984375,
      "learning_rate": 1e-05,
      "loss": 7.0553,
      "step": 15976
    },
    {
      "epoch": 0.8659078590785908,
      "step": 15976,
      "training_loss": 6.585581302642822
    },
    {
      "epoch": 0.8659620596205962,
      "step": 15977,
      "training_loss": 7.785912990570068
    },
    {
      "epoch": 0.8660162601626016,
      "step": 15978,
      "training_loss": 5.598601818084717
    },
    {
      "epoch": 0.866070460704607,
      "step": 15979,
      "training_loss": 5.540655136108398
    },
    {
      "epoch": 0.8661246612466125,
      "grad_norm": 26.820327758789062,
      "learning_rate": 1e-05,
      "loss": 6.3777,
      "step": 15980
    },
    {
      "epoch": 0.8661246612466125,
      "step": 15980,
      "training_loss": 6.766165256500244
    },
    {
      "epoch": 0.8661788617886179,
      "step": 15981,
      "training_loss": 5.893943786621094
    },
    {
      "epoch": 0.8662330623306234,
      "step": 15982,
      "training_loss": 5.609403610229492
    },
    {
      "epoch": 0.8662872628726287,
      "step": 15983,
      "training_loss": 7.972087383270264
    },
    {
      "epoch": 0.8663414634146341,
      "grad_norm": 33.61773681640625,
      "learning_rate": 1e-05,
      "loss": 6.5604,
      "step": 15984
    },
    {
      "epoch": 0.8663414634146341,
      "step": 15984,
      "training_loss": 6.2365217208862305
    },
    {
      "epoch": 0.8663956639566396,
      "step": 15985,
      "training_loss": 6.915554523468018
    },
    {
      "epoch": 0.866449864498645,
      "step": 15986,
      "training_loss": 6.837224006652832
    },
    {
      "epoch": 0.8665040650406504,
      "step": 15987,
      "training_loss": 6.869495868682861
    },
    {
      "epoch": 0.8665582655826558,
      "grad_norm": 22.828752517700195,
      "learning_rate": 1e-05,
      "loss": 6.7147,
      "step": 15988
    },
    {
      "epoch": 0.8665582655826558,
      "step": 15988,
      "training_loss": 7.189484119415283
    },
    {
      "epoch": 0.8666124661246613,
      "step": 15989,
      "training_loss": 7.830368518829346
    },
    {
      "epoch": 0.8666666666666667,
      "step": 15990,
      "training_loss": 5.869593143463135
    },
    {
      "epoch": 0.8667208672086721,
      "step": 15991,
      "training_loss": 5.8756256103515625
    },
    {
      "epoch": 0.8667750677506775,
      "grad_norm": 26.035045623779297,
      "learning_rate": 1e-05,
      "loss": 6.6913,
      "step": 15992
    },
    {
      "epoch": 0.8667750677506775,
      "step": 15992,
      "training_loss": 6.678489685058594
    },
    {
      "epoch": 0.8668292682926829,
      "step": 15993,
      "training_loss": 7.324881553649902
    },
    {
      "epoch": 0.8668834688346884,
      "step": 15994,
      "training_loss": 6.84811544418335
    },
    {
      "epoch": 0.8669376693766938,
      "step": 15995,
      "training_loss": 6.397361755371094
    },
    {
      "epoch": 0.8669918699186991,
      "grad_norm": 22.67448616027832,
      "learning_rate": 1e-05,
      "loss": 6.8122,
      "step": 15996
    },
    {
      "epoch": 0.8669918699186991,
      "step": 15996,
      "training_loss": 7.613471508026123
    },
    {
      "epoch": 0.8670460704607046,
      "step": 15997,
      "training_loss": 5.2871599197387695
    },
    {
      "epoch": 0.86710027100271,
      "step": 15998,
      "training_loss": 6.516442775726318
    },
    {
      "epoch": 0.8671544715447155,
      "step": 15999,
      "training_loss": 6.633199691772461
    },
    {
      "epoch": 0.8672086720867209,
      "grad_norm": 27.374469757080078,
      "learning_rate": 1e-05,
      "loss": 6.5126,
      "step": 16000
    },
    {
      "epoch": 0.8672086720867209,
      "step": 16000,
      "training_loss": 7.529169082641602
    },
    {
      "epoch": 0.8672628726287263,
      "step": 16001,
      "training_loss": 5.520017623901367
    },
    {
      "epoch": 0.8673170731707317,
      "step": 16002,
      "training_loss": 7.869194030761719
    },
    {
      "epoch": 0.8673712737127371,
      "step": 16003,
      "training_loss": 6.587244987487793
    },
    {
      "epoch": 0.8674254742547426,
      "grad_norm": 34.77960968017578,
      "learning_rate": 1e-05,
      "loss": 6.8764,
      "step": 16004
    },
    {
      "epoch": 0.8674254742547426,
      "step": 16004,
      "training_loss": 6.844839572906494
    },
    {
      "epoch": 0.8674796747967479,
      "step": 16005,
      "training_loss": 6.735759258270264
    },
    {
      "epoch": 0.8675338753387534,
      "step": 16006,
      "training_loss": 8.060507774353027
    },
    {
      "epoch": 0.8675880758807588,
      "step": 16007,
      "training_loss": 6.286954402923584
    },
    {
      "epoch": 0.8676422764227643,
      "grad_norm": 18.769102096557617,
      "learning_rate": 1e-05,
      "loss": 6.982,
      "step": 16008
    },
    {
      "epoch": 0.8676422764227643,
      "step": 16008,
      "training_loss": 6.691368579864502
    },
    {
      "epoch": 0.8676964769647697,
      "step": 16009,
      "training_loss": 6.697145938873291
    },
    {
      "epoch": 0.867750677506775,
      "step": 16010,
      "training_loss": 6.527512073516846
    },
    {
      "epoch": 0.8678048780487805,
      "step": 16011,
      "training_loss": 5.393218517303467
    },
    {
      "epoch": 0.8678590785907859,
      "grad_norm": 18.85376739501953,
      "learning_rate": 1e-05,
      "loss": 6.3273,
      "step": 16012
    },
    {
      "epoch": 0.8678590785907859,
      "step": 16012,
      "training_loss": 6.382229328155518
    },
    {
      "epoch": 0.8679132791327914,
      "step": 16013,
      "training_loss": 7.004613399505615
    },
    {
      "epoch": 0.8679674796747967,
      "step": 16014,
      "training_loss": 7.056063652038574
    },
    {
      "epoch": 0.8680216802168021,
      "step": 16015,
      "training_loss": 5.125393867492676
    },
    {
      "epoch": 0.8680758807588076,
      "grad_norm": 54.22833251953125,
      "learning_rate": 1e-05,
      "loss": 6.3921,
      "step": 16016
    },
    {
      "epoch": 0.8680758807588076,
      "step": 16016,
      "training_loss": 7.50260066986084
    },
    {
      "epoch": 0.868130081300813,
      "step": 16017,
      "training_loss": 7.761421203613281
    },
    {
      "epoch": 0.8681842818428184,
      "step": 16018,
      "training_loss": 6.839056968688965
    },
    {
      "epoch": 0.8682384823848238,
      "step": 16019,
      "training_loss": 8.376140594482422
    },
    {
      "epoch": 0.8682926829268293,
      "grad_norm": 31.309438705444336,
      "learning_rate": 1e-05,
      "loss": 7.6198,
      "step": 16020
    },
    {
      "epoch": 0.8682926829268293,
      "step": 16020,
      "training_loss": 6.837948322296143
    },
    {
      "epoch": 0.8683468834688347,
      "step": 16021,
      "training_loss": 6.902543067932129
    },
    {
      "epoch": 0.8684010840108402,
      "step": 16022,
      "training_loss": 4.207431793212891
    },
    {
      "epoch": 0.8684552845528455,
      "step": 16023,
      "training_loss": 5.450139999389648
    },
    {
      "epoch": 0.8685094850948509,
      "grad_norm": 22.428163528442383,
      "learning_rate": 1e-05,
      "loss": 5.8495,
      "step": 16024
    },
    {
      "epoch": 0.8685094850948509,
      "step": 16024,
      "training_loss": 6.769612789154053
    },
    {
      "epoch": 0.8685636856368564,
      "step": 16025,
      "training_loss": 5.976689338684082
    },
    {
      "epoch": 0.8686178861788618,
      "step": 16026,
      "training_loss": 7.696134090423584
    },
    {
      "epoch": 0.8686720867208672,
      "step": 16027,
      "training_loss": 6.942986488342285
    },
    {
      "epoch": 0.8687262872628726,
      "grad_norm": 51.03275680541992,
      "learning_rate": 1e-05,
      "loss": 6.8464,
      "step": 16028
    },
    {
      "epoch": 0.8687262872628726,
      "step": 16028,
      "training_loss": 7.920104026794434
    },
    {
      "epoch": 0.868780487804878,
      "step": 16029,
      "training_loss": 6.143176078796387
    },
    {
      "epoch": 0.8688346883468835,
      "step": 16030,
      "training_loss": 6.056968688964844
    },
    {
      "epoch": 0.8688888888888889,
      "step": 16031,
      "training_loss": 7.760666847229004
    },
    {
      "epoch": 0.8689430894308943,
      "grad_norm": 18.78965950012207,
      "learning_rate": 1e-05,
      "loss": 6.9702,
      "step": 16032
    },
    {
      "epoch": 0.8689430894308943,
      "step": 16032,
      "training_loss": 6.9838948249816895
    },
    {
      "epoch": 0.8689972899728997,
      "step": 16033,
      "training_loss": 5.852741718292236
    },
    {
      "epoch": 0.8690514905149052,
      "step": 16034,
      "training_loss": 6.299374580383301
    },
    {
      "epoch": 0.8691056910569106,
      "step": 16035,
      "training_loss": 7.424670696258545
    },
    {
      "epoch": 0.8691598915989159,
      "grad_norm": 26.596332550048828,
      "learning_rate": 1e-05,
      "loss": 6.6402,
      "step": 16036
    },
    {
      "epoch": 0.8691598915989159,
      "step": 16036,
      "training_loss": 3.945435047149658
    },
    {
      "epoch": 0.8692140921409214,
      "step": 16037,
      "training_loss": 5.635561943054199
    },
    {
      "epoch": 0.8692682926829268,
      "step": 16038,
      "training_loss": 6.144177436828613
    },
    {
      "epoch": 0.8693224932249323,
      "step": 16039,
      "training_loss": 6.58099889755249
    },
    {
      "epoch": 0.8693766937669377,
      "grad_norm": 43.77238845825195,
      "learning_rate": 1e-05,
      "loss": 5.5765,
      "step": 16040
    },
    {
      "epoch": 0.8693766937669377,
      "step": 16040,
      "training_loss": 7.939502239227295
    },
    {
      "epoch": 0.869430894308943,
      "step": 16041,
      "training_loss": 5.956537246704102
    },
    {
      "epoch": 0.8694850948509485,
      "step": 16042,
      "training_loss": 6.815320014953613
    },
    {
      "epoch": 0.8695392953929539,
      "step": 16043,
      "training_loss": 5.508808612823486
    },
    {
      "epoch": 0.8695934959349594,
      "grad_norm": 29.87931251525879,
      "learning_rate": 1e-05,
      "loss": 6.555,
      "step": 16044
    },
    {
      "epoch": 0.8695934959349594,
      "step": 16044,
      "training_loss": 6.406805038452148
    },
    {
      "epoch": 0.8696476964769647,
      "step": 16045,
      "training_loss": 7.563633918762207
    },
    {
      "epoch": 0.8697018970189702,
      "step": 16046,
      "training_loss": 7.089717388153076
    },
    {
      "epoch": 0.8697560975609756,
      "step": 16047,
      "training_loss": 7.415268421173096
    },
    {
      "epoch": 0.869810298102981,
      "grad_norm": 22.052236557006836,
      "learning_rate": 1e-05,
      "loss": 7.1189,
      "step": 16048
    },
    {
      "epoch": 0.869810298102981,
      "step": 16048,
      "training_loss": 7.556746006011963
    },
    {
      "epoch": 0.8698644986449865,
      "step": 16049,
      "training_loss": 6.118629455566406
    },
    {
      "epoch": 0.8699186991869918,
      "step": 16050,
      "training_loss": 7.364957332611084
    },
    {
      "epoch": 0.8699728997289973,
      "step": 16051,
      "training_loss": 7.161371231079102
    },
    {
      "epoch": 0.8700271002710027,
      "grad_norm": 23.55050277709961,
      "learning_rate": 1e-05,
      "loss": 7.0504,
      "step": 16052
    },
    {
      "epoch": 0.8700271002710027,
      "step": 16052,
      "training_loss": 7.02850866317749
    },
    {
      "epoch": 0.8700813008130082,
      "step": 16053,
      "training_loss": 6.73416805267334
    },
    {
      "epoch": 0.8701355013550135,
      "step": 16054,
      "training_loss": 6.788069248199463
    },
    {
      "epoch": 0.8701897018970189,
      "step": 16055,
      "training_loss": 7.7571492195129395
    },
    {
      "epoch": 0.8702439024390244,
      "grad_norm": 34.5611686706543,
      "learning_rate": 1e-05,
      "loss": 7.077,
      "step": 16056
    },
    {
      "epoch": 0.8702439024390244,
      "step": 16056,
      "training_loss": 6.200673580169678
    },
    {
      "epoch": 0.8702981029810298,
      "step": 16057,
      "training_loss": 6.961452960968018
    },
    {
      "epoch": 0.8703523035230353,
      "step": 16058,
      "training_loss": 6.055240631103516
    },
    {
      "epoch": 0.8704065040650406,
      "step": 16059,
      "training_loss": 4.004700660705566
    },
    {
      "epoch": 0.870460704607046,
      "grad_norm": 26.04218864440918,
      "learning_rate": 1e-05,
      "loss": 5.8055,
      "step": 16060
    },
    {
      "epoch": 0.870460704607046,
      "step": 16060,
      "training_loss": 6.348544120788574
    },
    {
      "epoch": 0.8705149051490515,
      "step": 16061,
      "training_loss": 7.222902774810791
    },
    {
      "epoch": 0.8705691056910569,
      "step": 16062,
      "training_loss": 5.953283786773682
    },
    {
      "epoch": 0.8706233062330623,
      "step": 16063,
      "training_loss": 7.815154075622559
    },
    {
      "epoch": 0.8706775067750677,
      "grad_norm": 57.39164733886719,
      "learning_rate": 1e-05,
      "loss": 6.835,
      "step": 16064
    },
    {
      "epoch": 0.8706775067750677,
      "step": 16064,
      "training_loss": 5.75724983215332
    },
    {
      "epoch": 0.8707317073170732,
      "step": 16065,
      "training_loss": 7.01593017578125
    },
    {
      "epoch": 0.8707859078590786,
      "step": 16066,
      "training_loss": 6.392524719238281
    },
    {
      "epoch": 0.8708401084010841,
      "step": 16067,
      "training_loss": 6.03001070022583
    },
    {
      "epoch": 0.8708943089430894,
      "grad_norm": 34.9144401550293,
      "learning_rate": 1e-05,
      "loss": 6.2989,
      "step": 16068
    },
    {
      "epoch": 0.8708943089430894,
      "step": 16068,
      "training_loss": 4.052464008331299
    },
    {
      "epoch": 0.8709485094850948,
      "step": 16069,
      "training_loss": 6.076970100402832
    },
    {
      "epoch": 0.8710027100271003,
      "step": 16070,
      "training_loss": 7.030601501464844
    },
    {
      "epoch": 0.8710569105691057,
      "step": 16071,
      "training_loss": 6.719836235046387
    },
    {
      "epoch": 0.8711111111111111,
      "grad_norm": 44.09303665161133,
      "learning_rate": 1e-05,
      "loss": 5.97,
      "step": 16072
    },
    {
      "epoch": 0.8711111111111111,
      "step": 16072,
      "training_loss": 6.484257698059082
    },
    {
      "epoch": 0.8711653116531165,
      "step": 16073,
      "training_loss": 5.753033638000488
    },
    {
      "epoch": 0.871219512195122,
      "step": 16074,
      "training_loss": 7.067133903503418
    },
    {
      "epoch": 0.8712737127371274,
      "step": 16075,
      "training_loss": 6.391134262084961
    },
    {
      "epoch": 0.8713279132791328,
      "grad_norm": 43.05756759643555,
      "learning_rate": 1e-05,
      "loss": 6.4239,
      "step": 16076
    },
    {
      "epoch": 0.8713279132791328,
      "step": 16076,
      "training_loss": 6.614743709564209
    },
    {
      "epoch": 0.8713821138211382,
      "step": 16077,
      "training_loss": 3.914015293121338
    },
    {
      "epoch": 0.8714363143631436,
      "step": 16078,
      "training_loss": 4.98832368850708
    },
    {
      "epoch": 0.8714905149051491,
      "step": 16079,
      "training_loss": 4.806432247161865
    },
    {
      "epoch": 0.8715447154471545,
      "grad_norm": 28.950580596923828,
      "learning_rate": 1e-05,
      "loss": 5.0809,
      "step": 16080
    },
    {
      "epoch": 0.8715447154471545,
      "step": 16080,
      "training_loss": 6.2711262702941895
    },
    {
      "epoch": 0.8715989159891598,
      "step": 16081,
      "training_loss": 6.533541202545166
    },
    {
      "epoch": 0.8716531165311653,
      "step": 16082,
      "training_loss": 5.977308750152588
    },
    {
      "epoch": 0.8717073170731707,
      "step": 16083,
      "training_loss": 6.366627216339111
    },
    {
      "epoch": 0.8717615176151762,
      "grad_norm": 36.75767517089844,
      "learning_rate": 1e-05,
      "loss": 6.2872,
      "step": 16084
    },
    {
      "epoch": 0.8717615176151762,
      "step": 16084,
      "training_loss": 6.794674873352051
    },
    {
      "epoch": 0.8718157181571816,
      "step": 16085,
      "training_loss": 5.93084716796875
    },
    {
      "epoch": 0.871869918699187,
      "step": 16086,
      "training_loss": 5.89086389541626
    },
    {
      "epoch": 0.8719241192411924,
      "step": 16087,
      "training_loss": 7.235593795776367
    },
    {
      "epoch": 0.8719783197831978,
      "grad_norm": 18.980514526367188,
      "learning_rate": 1e-05,
      "loss": 6.463,
      "step": 16088
    },
    {
      "epoch": 0.8719783197831978,
      "step": 16088,
      "training_loss": 6.435072422027588
    },
    {
      "epoch": 0.8720325203252033,
      "step": 16089,
      "training_loss": 6.50742769241333
    },
    {
      "epoch": 0.8720867208672086,
      "step": 16090,
      "training_loss": 6.680185317993164
    },
    {
      "epoch": 0.8721409214092141,
      "step": 16091,
      "training_loss": 7.614990234375
    },
    {
      "epoch": 0.8721951219512195,
      "grad_norm": 33.50123977661133,
      "learning_rate": 1e-05,
      "loss": 6.8094,
      "step": 16092
    },
    {
      "epoch": 0.8721951219512195,
      "step": 16092,
      "training_loss": 6.498256683349609
    },
    {
      "epoch": 0.872249322493225,
      "step": 16093,
      "training_loss": 6.834977149963379
    },
    {
      "epoch": 0.8723035230352304,
      "step": 16094,
      "training_loss": 9.784321784973145
    },
    {
      "epoch": 0.8723577235772357,
      "step": 16095,
      "training_loss": 7.032729148864746
    },
    {
      "epoch": 0.8724119241192412,
      "grad_norm": 22.405668258666992,
      "learning_rate": 1e-05,
      "loss": 7.5376,
      "step": 16096
    },
    {
      "epoch": 0.8724119241192412,
      "step": 16096,
      "training_loss": 5.469363212585449
    },
    {
      "epoch": 0.8724661246612466,
      "step": 16097,
      "training_loss": 4.7414116859436035
    },
    {
      "epoch": 0.8725203252032521,
      "step": 16098,
      "training_loss": 5.18211555480957
    },
    {
      "epoch": 0.8725745257452574,
      "step": 16099,
      "training_loss": 6.774861812591553
    },
    {
      "epoch": 0.8726287262872628,
      "grad_norm": 35.78432846069336,
      "learning_rate": 1e-05,
      "loss": 5.5419,
      "step": 16100
    },
    {
      "epoch": 0.8726287262872628,
      "step": 16100,
      "training_loss": 7.211136341094971
    },
    {
      "epoch": 0.8726829268292683,
      "step": 16101,
      "training_loss": 4.7649760246276855
    },
    {
      "epoch": 0.8727371273712737,
      "step": 16102,
      "training_loss": 6.527431011199951
    },
    {
      "epoch": 0.8727913279132792,
      "step": 16103,
      "training_loss": 7.011441707611084
    },
    {
      "epoch": 0.8728455284552845,
      "grad_norm": 26.606098175048828,
      "learning_rate": 1e-05,
      "loss": 6.3787,
      "step": 16104
    },
    {
      "epoch": 0.8728455284552845,
      "step": 16104,
      "training_loss": 6.850083351135254
    },
    {
      "epoch": 0.87289972899729,
      "step": 16105,
      "training_loss": 7.054358005523682
    },
    {
      "epoch": 0.8729539295392954,
      "step": 16106,
      "training_loss": 6.682497024536133
    },
    {
      "epoch": 0.8730081300813008,
      "step": 16107,
      "training_loss": 4.600057125091553
    },
    {
      "epoch": 0.8730623306233062,
      "grad_norm": 22.023387908935547,
      "learning_rate": 1e-05,
      "loss": 6.2967,
      "step": 16108
    },
    {
      "epoch": 0.8730623306233062,
      "step": 16108,
      "training_loss": 8.14948558807373
    },
    {
      "epoch": 0.8731165311653116,
      "step": 16109,
      "training_loss": 8.781169891357422
    },
    {
      "epoch": 0.8731707317073171,
      "step": 16110,
      "training_loss": 7.029891490936279
    },
    {
      "epoch": 0.8732249322493225,
      "step": 16111,
      "training_loss": 6.820895671844482
    },
    {
      "epoch": 0.873279132791328,
      "grad_norm": 25.57401466369629,
      "learning_rate": 1e-05,
      "loss": 7.6954,
      "step": 16112
    },
    {
      "epoch": 0.873279132791328,
      "step": 16112,
      "training_loss": 6.740512371063232
    },
    {
      "epoch": 0.8733333333333333,
      "step": 16113,
      "training_loss": 6.882254600524902
    },
    {
      "epoch": 0.8733875338753387,
      "step": 16114,
      "training_loss": 4.766537189483643
    },
    {
      "epoch": 0.8734417344173442,
      "step": 16115,
      "training_loss": 6.923731327056885
    },
    {
      "epoch": 0.8734959349593496,
      "grad_norm": 20.06818962097168,
      "learning_rate": 1e-05,
      "loss": 6.3283,
      "step": 16116
    },
    {
      "epoch": 0.8734959349593496,
      "step": 16116,
      "training_loss": 5.291210174560547
    },
    {
      "epoch": 0.873550135501355,
      "step": 16117,
      "training_loss": 8.071020126342773
    },
    {
      "epoch": 0.8736043360433604,
      "step": 16118,
      "training_loss": 2.826169967651367
    },
    {
      "epoch": 0.8736585365853659,
      "step": 16119,
      "training_loss": 4.166819095611572
    },
    {
      "epoch": 0.8737127371273713,
      "grad_norm": 21.717994689941406,
      "learning_rate": 1e-05,
      "loss": 5.0888,
      "step": 16120
    },
    {
      "epoch": 0.8737127371273713,
      "step": 16120,
      "training_loss": 7.608073711395264
    },
    {
      "epoch": 0.8737669376693767,
      "step": 16121,
      "training_loss": 7.1012420654296875
    },
    {
      "epoch": 0.8738211382113821,
      "step": 16122,
      "training_loss": 4.876248359680176
    },
    {
      "epoch": 0.8738753387533875,
      "step": 16123,
      "training_loss": 6.734221458435059
    },
    {
      "epoch": 0.873929539295393,
      "grad_norm": 31.29974937438965,
      "learning_rate": 1e-05,
      "loss": 6.5799,
      "step": 16124
    },
    {
      "epoch": 0.873929539295393,
      "step": 16124,
      "training_loss": 6.177709102630615
    },
    {
      "epoch": 0.8739837398373984,
      "step": 16125,
      "training_loss": 5.99403190612793
    },
    {
      "epoch": 0.8740379403794037,
      "step": 16126,
      "training_loss": 6.192572116851807
    },
    {
      "epoch": 0.8740921409214092,
      "step": 16127,
      "training_loss": 4.037611961364746
    },
    {
      "epoch": 0.8741463414634146,
      "grad_norm": 24.4570255279541,
      "learning_rate": 1e-05,
      "loss": 5.6005,
      "step": 16128
    },
    {
      "epoch": 0.8741463414634146,
      "step": 16128,
      "training_loss": 7.207559585571289
    },
    {
      "epoch": 0.8742005420054201,
      "step": 16129,
      "training_loss": 6.888204097747803
    },
    {
      "epoch": 0.8742547425474255,
      "step": 16130,
      "training_loss": 7.861764907836914
    },
    {
      "epoch": 0.8743089430894309,
      "step": 16131,
      "training_loss": 6.2960429191589355
    },
    {
      "epoch": 0.8743631436314363,
      "grad_norm": 22.328689575195312,
      "learning_rate": 1e-05,
      "loss": 7.0634,
      "step": 16132
    },
    {
      "epoch": 0.8743631436314363,
      "step": 16132,
      "training_loss": 5.8788018226623535
    },
    {
      "epoch": 0.8744173441734417,
      "step": 16133,
      "training_loss": 6.389568328857422
    },
    {
      "epoch": 0.8744715447154472,
      "step": 16134,
      "training_loss": 5.757218837738037
    },
    {
      "epoch": 0.8745257452574525,
      "step": 16135,
      "training_loss": 7.579315662384033
    },
    {
      "epoch": 0.874579945799458,
      "grad_norm": 24.189367294311523,
      "learning_rate": 1e-05,
      "loss": 6.4012,
      "step": 16136
    },
    {
      "epoch": 0.874579945799458,
      "step": 16136,
      "training_loss": 6.380067825317383
    },
    {
      "epoch": 0.8746341463414634,
      "step": 16137,
      "training_loss": 7.6899895668029785
    },
    {
      "epoch": 0.8746883468834689,
      "step": 16138,
      "training_loss": 7.380374908447266
    },
    {
      "epoch": 0.8747425474254743,
      "step": 16139,
      "training_loss": 6.798728942871094
    },
    {
      "epoch": 0.8747967479674796,
      "grad_norm": 33.289886474609375,
      "learning_rate": 1e-05,
      "loss": 7.0623,
      "step": 16140
    },
    {
      "epoch": 0.8747967479674796,
      "step": 16140,
      "training_loss": 5.610526084899902
    },
    {
      "epoch": 0.8748509485094851,
      "step": 16141,
      "training_loss": 6.198351860046387
    },
    {
      "epoch": 0.8749051490514905,
      "step": 16142,
      "training_loss": 7.656191825866699
    },
    {
      "epoch": 0.874959349593496,
      "step": 16143,
      "training_loss": 6.973742485046387
    },
    {
      "epoch": 0.8750135501355013,
      "grad_norm": 41.38330841064453,
      "learning_rate": 1e-05,
      "loss": 6.6097,
      "step": 16144
    },
    {
      "epoch": 0.8750135501355013,
      "step": 16144,
      "training_loss": 6.907358646392822
    },
    {
      "epoch": 0.8750677506775068,
      "step": 16145,
      "training_loss": 7.811386585235596
    },
    {
      "epoch": 0.8751219512195122,
      "step": 16146,
      "training_loss": 7.5993733406066895
    },
    {
      "epoch": 0.8751761517615176,
      "step": 16147,
      "training_loss": 7.099565029144287
    },
    {
      "epoch": 0.8752303523035231,
      "grad_norm": 22.7960147857666,
      "learning_rate": 1e-05,
      "loss": 7.3544,
      "step": 16148
    },
    {
      "epoch": 0.8752303523035231,
      "step": 16148,
      "training_loss": 7.949317455291748
    },
    {
      "epoch": 0.8752845528455284,
      "step": 16149,
      "training_loss": 6.55879545211792
    },
    {
      "epoch": 0.8753387533875339,
      "step": 16150,
      "training_loss": 7.353943347930908
    },
    {
      "epoch": 0.8753929539295393,
      "step": 16151,
      "training_loss": 7.14314603805542
    },
    {
      "epoch": 0.8754471544715448,
      "grad_norm": 33.81306838989258,
      "learning_rate": 1e-05,
      "loss": 7.2513,
      "step": 16152
    },
    {
      "epoch": 0.8754471544715448,
      "step": 16152,
      "training_loss": 6.988533020019531
    },
    {
      "epoch": 0.8755013550135501,
      "step": 16153,
      "training_loss": 6.779752254486084
    },
    {
      "epoch": 0.8755555555555555,
      "step": 16154,
      "training_loss": 7.198225498199463
    },
    {
      "epoch": 0.875609756097561,
      "step": 16155,
      "training_loss": 6.7237324714660645
    },
    {
      "epoch": 0.8756639566395664,
      "grad_norm": 23.897371292114258,
      "learning_rate": 1e-05,
      "loss": 6.9226,
      "step": 16156
    },
    {
      "epoch": 0.8756639566395664,
      "step": 16156,
      "training_loss": 6.913026332855225
    },
    {
      "epoch": 0.8757181571815719,
      "step": 16157,
      "training_loss": 6.884688377380371
    },
    {
      "epoch": 0.8757723577235772,
      "step": 16158,
      "training_loss": 6.702266216278076
    },
    {
      "epoch": 0.8758265582655826,
      "step": 16159,
      "training_loss": 6.241345405578613
    },
    {
      "epoch": 0.8758807588075881,
      "grad_norm": 28.016225814819336,
      "learning_rate": 1e-05,
      "loss": 6.6853,
      "step": 16160
    },
    {
      "epoch": 0.8758807588075881,
      "step": 16160,
      "training_loss": 7.38347053527832
    },
    {
      "epoch": 0.8759349593495935,
      "step": 16161,
      "training_loss": 5.691278457641602
    },
    {
      "epoch": 0.8759891598915989,
      "step": 16162,
      "training_loss": 7.344552040100098
    },
    {
      "epoch": 0.8760433604336043,
      "step": 16163,
      "training_loss": 7.057255268096924
    },
    {
      "epoch": 0.8760975609756098,
      "grad_norm": 54.16009521484375,
      "learning_rate": 1e-05,
      "loss": 6.8691,
      "step": 16164
    },
    {
      "epoch": 0.8760975609756098,
      "step": 16164,
      "training_loss": 7.935335159301758
    },
    {
      "epoch": 0.8761517615176152,
      "step": 16165,
      "training_loss": 7.658520698547363
    },
    {
      "epoch": 0.8762059620596206,
      "step": 16166,
      "training_loss": 6.912945747375488
    },
    {
      "epoch": 0.876260162601626,
      "step": 16167,
      "training_loss": 6.97098445892334
    },
    {
      "epoch": 0.8763143631436314,
      "grad_norm": 18.47676658630371,
      "learning_rate": 1e-05,
      "loss": 7.3694,
      "step": 16168
    },
    {
      "epoch": 0.8763143631436314,
      "step": 16168,
      "training_loss": 6.766881465911865
    },
    {
      "epoch": 0.8763685636856369,
      "step": 16169,
      "training_loss": 7.57950496673584
    },
    {
      "epoch": 0.8764227642276423,
      "step": 16170,
      "training_loss": 8.062102317810059
    },
    {
      "epoch": 0.8764769647696476,
      "step": 16171,
      "training_loss": 6.685797214508057
    },
    {
      "epoch": 0.8765311653116531,
      "grad_norm": 22.039464950561523,
      "learning_rate": 1e-05,
      "loss": 7.2736,
      "step": 16172
    },
    {
      "epoch": 0.8765311653116531,
      "step": 16172,
      "training_loss": 6.452042102813721
    },
    {
      "epoch": 0.8765853658536585,
      "step": 16173,
      "training_loss": 4.509647369384766
    },
    {
      "epoch": 0.876639566395664,
      "step": 16174,
      "training_loss": 7.298182487487793
    },
    {
      "epoch": 0.8766937669376694,
      "step": 16175,
      "training_loss": 6.331610202789307
    },
    {
      "epoch": 0.8767479674796748,
      "grad_norm": 26.126832962036133,
      "learning_rate": 1e-05,
      "loss": 6.1479,
      "step": 16176
    },
    {
      "epoch": 0.8767479674796748,
      "step": 16176,
      "training_loss": 6.83085298538208
    },
    {
      "epoch": 0.8768021680216802,
      "step": 16177,
      "training_loss": 3.859689474105835
    },
    {
      "epoch": 0.8768563685636857,
      "step": 16178,
      "training_loss": 7.246747016906738
    },
    {
      "epoch": 0.8769105691056911,
      "step": 16179,
      "training_loss": 6.474454879760742
    },
    {
      "epoch": 0.8769647696476964,
      "grad_norm": 33.59025955200195,
      "learning_rate": 1e-05,
      "loss": 6.1029,
      "step": 16180
    },
    {
      "epoch": 0.8769647696476964,
      "step": 16180,
      "training_loss": 6.935843467712402
    },
    {
      "epoch": 0.8770189701897019,
      "step": 16181,
      "training_loss": 6.930669784545898
    },
    {
      "epoch": 0.8770731707317073,
      "step": 16182,
      "training_loss": 3.75003981590271
    },
    {
      "epoch": 0.8771273712737128,
      "step": 16183,
      "training_loss": 6.9071574211120605
    },
    {
      "epoch": 0.8771815718157182,
      "grad_norm": 38.670257568359375,
      "learning_rate": 1e-05,
      "loss": 6.1309,
      "step": 16184
    },
    {
      "epoch": 0.8771815718157182,
      "step": 16184,
      "training_loss": 7.611640453338623
    },
    {
      "epoch": 0.8772357723577235,
      "step": 16185,
      "training_loss": 6.351141929626465
    },
    {
      "epoch": 0.877289972899729,
      "step": 16186,
      "training_loss": 4.244901657104492
    },
    {
      "epoch": 0.8773441734417344,
      "step": 16187,
      "training_loss": 6.323273181915283
    },
    {
      "epoch": 0.8773983739837399,
      "grad_norm": 27.7944393157959,
      "learning_rate": 1e-05,
      "loss": 6.1327,
      "step": 16188
    },
    {
      "epoch": 0.8773983739837399,
      "step": 16188,
      "training_loss": 6.970857620239258
    },
    {
      "epoch": 0.8774525745257452,
      "step": 16189,
      "training_loss": 6.211334228515625
    },
    {
      "epoch": 0.8775067750677507,
      "step": 16190,
      "training_loss": 7.193501949310303
    },
    {
      "epoch": 0.8775609756097561,
      "step": 16191,
      "training_loss": 5.741811752319336
    },
    {
      "epoch": 0.8776151761517615,
      "grad_norm": 28.30862808227539,
      "learning_rate": 1e-05,
      "loss": 6.5294,
      "step": 16192
    },
    {
      "epoch": 0.8776151761517615,
      "step": 16192,
      "training_loss": 3.3575122356414795
    },
    {
      "epoch": 0.877669376693767,
      "step": 16193,
      "training_loss": 7.283719539642334
    },
    {
      "epoch": 0.8777235772357723,
      "step": 16194,
      "training_loss": 6.684319019317627
    },
    {
      "epoch": 0.8777777777777778,
      "step": 16195,
      "training_loss": 6.702355861663818
    },
    {
      "epoch": 0.8778319783197832,
      "grad_norm": 28.263864517211914,
      "learning_rate": 1e-05,
      "loss": 6.007,
      "step": 16196
    },
    {
      "epoch": 0.8778319783197832,
      "step": 16196,
      "training_loss": 7.735999584197998
    },
    {
      "epoch": 0.8778861788617887,
      "step": 16197,
      "training_loss": 6.4150190353393555
    },
    {
      "epoch": 0.877940379403794,
      "step": 16198,
      "training_loss": 6.612827777862549
    },
    {
      "epoch": 0.8779945799457994,
      "step": 16199,
      "training_loss": 6.787431240081787
    },
    {
      "epoch": 0.8780487804878049,
      "grad_norm": 23.105361938476562,
      "learning_rate": 1e-05,
      "loss": 6.8878,
      "step": 16200
    },
    {
      "epoch": 0.8780487804878049,
      "step": 16200,
      "training_loss": 4.923587322235107
    },
    {
      "epoch": 0.8781029810298103,
      "step": 16201,
      "training_loss": 7.129632472991943
    },
    {
      "epoch": 0.8781571815718158,
      "step": 16202,
      "training_loss": 6.664327621459961
    },
    {
      "epoch": 0.8782113821138211,
      "step": 16203,
      "training_loss": 4.0280680656433105
    },
    {
      "epoch": 0.8782655826558265,
      "grad_norm": 39.305171966552734,
      "learning_rate": 1e-05,
      "loss": 5.6864,
      "step": 16204
    },
    {
      "epoch": 0.8782655826558265,
      "step": 16204,
      "training_loss": 7.500607013702393
    },
    {
      "epoch": 0.878319783197832,
      "step": 16205,
      "training_loss": 7.279974937438965
    },
    {
      "epoch": 0.8783739837398374,
      "step": 16206,
      "training_loss": 7.603729248046875
    },
    {
      "epoch": 0.8784281842818428,
      "step": 16207,
      "training_loss": 7.168820858001709
    },
    {
      "epoch": 0.8784823848238482,
      "grad_norm": 22.071245193481445,
      "learning_rate": 1e-05,
      "loss": 7.3883,
      "step": 16208
    },
    {
      "epoch": 0.8784823848238482,
      "step": 16208,
      "training_loss": 6.662105560302734
    },
    {
      "epoch": 0.8785365853658537,
      "step": 16209,
      "training_loss": 5.854431629180908
    },
    {
      "epoch": 0.8785907859078591,
      "step": 16210,
      "training_loss": 6.5921831130981445
    },
    {
      "epoch": 0.8786449864498646,
      "step": 16211,
      "training_loss": 7.157589435577393
    },
    {
      "epoch": 0.8786991869918699,
      "grad_norm": 33.83438491821289,
      "learning_rate": 1e-05,
      "loss": 6.5666,
      "step": 16212
    },
    {
      "epoch": 0.8786991869918699,
      "step": 16212,
      "training_loss": 6.857976913452148
    },
    {
      "epoch": 0.8787533875338753,
      "step": 16213,
      "training_loss": 5.477214813232422
    },
    {
      "epoch": 0.8788075880758808,
      "step": 16214,
      "training_loss": 7.697829723358154
    },
    {
      "epoch": 0.8788617886178862,
      "step": 16215,
      "training_loss": 7.222676753997803
    },
    {
      "epoch": 0.8789159891598916,
      "grad_norm": 22.29119300842285,
      "learning_rate": 1e-05,
      "loss": 6.8139,
      "step": 16216
    },
    {
      "epoch": 0.8789159891598916,
      "step": 16216,
      "training_loss": 5.9320149421691895
    },
    {
      "epoch": 0.878970189701897,
      "step": 16217,
      "training_loss": 6.722334861755371
    },
    {
      "epoch": 0.8790243902439024,
      "step": 16218,
      "training_loss": 7.594551086425781
    },
    {
      "epoch": 0.8790785907859079,
      "step": 16219,
      "training_loss": 6.283497333526611
    },
    {
      "epoch": 0.8791327913279133,
      "grad_norm": 36.417633056640625,
      "learning_rate": 1e-05,
      "loss": 6.6331,
      "step": 16220
    },
    {
      "epoch": 0.8791327913279133,
      "step": 16220,
      "training_loss": 5.5526018142700195
    },
    {
      "epoch": 0.8791869918699187,
      "step": 16221,
      "training_loss": 6.222177982330322
    },
    {
      "epoch": 0.8792411924119241,
      "step": 16222,
      "training_loss": 7.920938968658447
    },
    {
      "epoch": 0.8792953929539296,
      "step": 16223,
      "training_loss": 6.852234363555908
    },
    {
      "epoch": 0.879349593495935,
      "grad_norm": 16.550241470336914,
      "learning_rate": 1e-05,
      "loss": 6.637,
      "step": 16224
    },
    {
      "epoch": 0.879349593495935,
      "step": 16224,
      "training_loss": 5.488089561462402
    },
    {
      "epoch": 0.8794037940379403,
      "step": 16225,
      "training_loss": 6.549197673797607
    },
    {
      "epoch": 0.8794579945799458,
      "step": 16226,
      "training_loss": 7.105866432189941
    },
    {
      "epoch": 0.8795121951219512,
      "step": 16227,
      "training_loss": 5.4512763023376465
    },
    {
      "epoch": 0.8795663956639567,
      "grad_norm": 23.90097427368164,
      "learning_rate": 1e-05,
      "loss": 6.1486,
      "step": 16228
    },
    {
      "epoch": 0.8795663956639567,
      "step": 16228,
      "training_loss": 3.8396658897399902
    },
    {
      "epoch": 0.8796205962059621,
      "step": 16229,
      "training_loss": 6.49709415435791
    },
    {
      "epoch": 0.8796747967479674,
      "step": 16230,
      "training_loss": 6.281795024871826
    },
    {
      "epoch": 0.8797289972899729,
      "step": 16231,
      "training_loss": 5.622200965881348
    },
    {
      "epoch": 0.8797831978319783,
      "grad_norm": 24.267738342285156,
      "learning_rate": 1e-05,
      "loss": 5.5602,
      "step": 16232
    },
    {
      "epoch": 0.8797831978319783,
      "step": 16232,
      "training_loss": 7.4645676612854
    },
    {
      "epoch": 0.8798373983739838,
      "step": 16233,
      "training_loss": 6.931890487670898
    },
    {
      "epoch": 0.8798915989159891,
      "step": 16234,
      "training_loss": 7.011014461517334
    },
    {
      "epoch": 0.8799457994579946,
      "step": 16235,
      "training_loss": 6.255730152130127
    },
    {
      "epoch": 0.88,
      "grad_norm": 21.314987182617188,
      "learning_rate": 1e-05,
      "loss": 6.9158,
      "step": 16236
    },
    {
      "epoch": 0.88,
      "step": 16236,
      "training_loss": 6.941951274871826
    },
    {
      "epoch": 0.8800542005420054,
      "step": 16237,
      "training_loss": 7.2968597412109375
    },
    {
      "epoch": 0.8801084010840109,
      "step": 16238,
      "training_loss": 4.939172267913818
    },
    {
      "epoch": 0.8801626016260162,
      "step": 16239,
      "training_loss": 5.564209461212158
    },
    {
      "epoch": 0.8802168021680217,
      "grad_norm": 38.84553909301758,
      "learning_rate": 1e-05,
      "loss": 6.1855,
      "step": 16240
    },
    {
      "epoch": 0.8802168021680217,
      "step": 16240,
      "training_loss": 5.627219200134277
    },
    {
      "epoch": 0.8802710027100271,
      "step": 16241,
      "training_loss": 6.212828636169434
    },
    {
      "epoch": 0.8803252032520326,
      "step": 16242,
      "training_loss": 6.844417095184326
    },
    {
      "epoch": 0.8803794037940379,
      "step": 16243,
      "training_loss": 7.448229789733887
    },
    {
      "epoch": 0.8804336043360433,
      "grad_norm": 24.283443450927734,
      "learning_rate": 1e-05,
      "loss": 6.5332,
      "step": 16244
    },
    {
      "epoch": 0.8804336043360433,
      "step": 16244,
      "training_loss": 6.643203258514404
    },
    {
      "epoch": 0.8804878048780488,
      "step": 16245,
      "training_loss": 5.680987358093262
    },
    {
      "epoch": 0.8805420054200542,
      "step": 16246,
      "training_loss": 6.485915184020996
    },
    {
      "epoch": 0.8805962059620597,
      "step": 16247,
      "training_loss": 6.28432035446167
    },
    {
      "epoch": 0.880650406504065,
      "grad_norm": 17.286890029907227,
      "learning_rate": 1e-05,
      "loss": 6.2736,
      "step": 16248
    },
    {
      "epoch": 0.880650406504065,
      "step": 16248,
      "training_loss": 6.804131984710693
    },
    {
      "epoch": 0.8807046070460705,
      "step": 16249,
      "training_loss": 6.507284164428711
    },
    {
      "epoch": 0.8807588075880759,
      "step": 16250,
      "training_loss": 6.906529903411865
    },
    {
      "epoch": 0.8808130081300813,
      "step": 16251,
      "training_loss": 6.074514389038086
    },
    {
      "epoch": 0.8808672086720867,
      "grad_norm": 36.02346420288086,
      "learning_rate": 1e-05,
      "loss": 6.5731,
      "step": 16252
    },
    {
      "epoch": 0.8808672086720867,
      "step": 16252,
      "training_loss": 5.5910844802856445
    },
    {
      "epoch": 0.8809214092140921,
      "step": 16253,
      "training_loss": 7.271838188171387
    },
    {
      "epoch": 0.8809756097560976,
      "step": 16254,
      "training_loss": 7.035607814788818
    },
    {
      "epoch": 0.881029810298103,
      "step": 16255,
      "training_loss": 5.409318923950195
    },
    {
      "epoch": 0.8810840108401085,
      "grad_norm": 76.56698608398438,
      "learning_rate": 1e-05,
      "loss": 6.327,
      "step": 16256
    },
    {
      "epoch": 0.8810840108401085,
      "step": 16256,
      "training_loss": 7.02039909362793
    },
    {
      "epoch": 0.8811382113821138,
      "step": 16257,
      "training_loss": 6.905824184417725
    },
    {
      "epoch": 0.8811924119241192,
      "step": 16258,
      "training_loss": 6.955582618713379
    },
    {
      "epoch": 0.8812466124661247,
      "step": 16259,
      "training_loss": 5.707728862762451
    },
    {
      "epoch": 0.8813008130081301,
      "grad_norm": 38.74095916748047,
      "learning_rate": 1e-05,
      "loss": 6.6474,
      "step": 16260
    },
    {
      "epoch": 0.8813008130081301,
      "step": 16260,
      "training_loss": 5.1649556159973145
    },
    {
      "epoch": 0.8813550135501355,
      "step": 16261,
      "training_loss": 5.913054466247559
    },
    {
      "epoch": 0.8814092140921409,
      "step": 16262,
      "training_loss": 7.0345258712768555
    },
    {
      "epoch": 0.8814634146341463,
      "step": 16263,
      "training_loss": 6.705010890960693
    },
    {
      "epoch": 0.8815176151761518,
      "grad_norm": 16.574337005615234,
      "learning_rate": 1e-05,
      "loss": 6.2044,
      "step": 16264
    },
    {
      "epoch": 0.8815176151761518,
      "step": 16264,
      "training_loss": 6.376646518707275
    },
    {
      "epoch": 0.8815718157181572,
      "step": 16265,
      "training_loss": 3.5125155448913574
    },
    {
      "epoch": 0.8816260162601626,
      "step": 16266,
      "training_loss": 6.76275634765625
    },
    {
      "epoch": 0.881680216802168,
      "step": 16267,
      "training_loss": 6.814596176147461
    },
    {
      "epoch": 0.8817344173441735,
      "grad_norm": 19.06229591369629,
      "learning_rate": 1e-05,
      "loss": 5.8666,
      "step": 16268
    },
    {
      "epoch": 0.8817344173441735,
      "step": 16268,
      "training_loss": 6.687326908111572
    },
    {
      "epoch": 0.8817886178861789,
      "step": 16269,
      "training_loss": 6.149443626403809
    },
    {
      "epoch": 0.8818428184281842,
      "step": 16270,
      "training_loss": 7.664223670959473
    },
    {
      "epoch": 0.8818970189701897,
      "step": 16271,
      "training_loss": 6.734315872192383
    },
    {
      "epoch": 0.8819512195121951,
      "grad_norm": 28.991249084472656,
      "learning_rate": 1e-05,
      "loss": 6.8088,
      "step": 16272
    },
    {
      "epoch": 0.8819512195121951,
      "step": 16272,
      "training_loss": 6.019402503967285
    },
    {
      "epoch": 0.8820054200542006,
      "step": 16273,
      "training_loss": 6.491944313049316
    },
    {
      "epoch": 0.8820596205962059,
      "step": 16274,
      "training_loss": 5.691368579864502
    },
    {
      "epoch": 0.8821138211382114,
      "step": 16275,
      "training_loss": 7.448557376861572
    },
    {
      "epoch": 0.8821680216802168,
      "grad_norm": 17.219459533691406,
      "learning_rate": 1e-05,
      "loss": 6.4128,
      "step": 16276
    },
    {
      "epoch": 0.8821680216802168,
      "step": 16276,
      "training_loss": 7.7080488204956055
    },
    {
      "epoch": 0.8822222222222222,
      "step": 16277,
      "training_loss": 4.957787036895752
    },
    {
      "epoch": 0.8822764227642277,
      "step": 16278,
      "training_loss": 6.988834381103516
    },
    {
      "epoch": 0.882330623306233,
      "step": 16279,
      "training_loss": 7.739348888397217
    },
    {
      "epoch": 0.8823848238482385,
      "grad_norm": 45.60380935668945,
      "learning_rate": 1e-05,
      "loss": 6.8485,
      "step": 16280
    },
    {
      "epoch": 0.8823848238482385,
      "step": 16280,
      "training_loss": 6.7443437576293945
    },
    {
      "epoch": 0.8824390243902439,
      "step": 16281,
      "training_loss": 7.788032531738281
    },
    {
      "epoch": 0.8824932249322494,
      "step": 16282,
      "training_loss": 6.4366230964660645
    },
    {
      "epoch": 0.8825474254742547,
      "step": 16283,
      "training_loss": 7.888302803039551
    },
    {
      "epoch": 0.8826016260162601,
      "grad_norm": 36.36423873901367,
      "learning_rate": 1e-05,
      "loss": 7.2143,
      "step": 16284
    },
    {
      "epoch": 0.8826016260162601,
      "step": 16284,
      "training_loss": 7.3868088722229
    },
    {
      "epoch": 0.8826558265582656,
      "step": 16285,
      "training_loss": 7.571360111236572
    },
    {
      "epoch": 0.882710027100271,
      "step": 16286,
      "training_loss": 6.811639785766602
    },
    {
      "epoch": 0.8827642276422765,
      "step": 16287,
      "training_loss": 5.8763861656188965
    },
    {
      "epoch": 0.8828184281842818,
      "grad_norm": 24.49166488647461,
      "learning_rate": 1e-05,
      "loss": 6.9115,
      "step": 16288
    },
    {
      "epoch": 0.8828184281842818,
      "step": 16288,
      "training_loss": 3.3480186462402344
    },
    {
      "epoch": 0.8828726287262872,
      "step": 16289,
      "training_loss": 7.956160068511963
    },
    {
      "epoch": 0.8829268292682927,
      "step": 16290,
      "training_loss": 4.948752403259277
    },
    {
      "epoch": 0.8829810298102981,
      "step": 16291,
      "training_loss": 6.650151252746582
    },
    {
      "epoch": 0.8830352303523035,
      "grad_norm": 24.734344482421875,
      "learning_rate": 1e-05,
      "loss": 5.7258,
      "step": 16292
    },
    {
      "epoch": 0.8830352303523035,
      "step": 16292,
      "training_loss": 7.618250370025635
    },
    {
      "epoch": 0.8830894308943089,
      "step": 16293,
      "training_loss": 6.337301254272461
    },
    {
      "epoch": 0.8831436314363144,
      "step": 16294,
      "training_loss": 5.712741851806641
    },
    {
      "epoch": 0.8831978319783198,
      "step": 16295,
      "training_loss": 7.067160606384277
    },
    {
      "epoch": 0.8832520325203252,
      "grad_norm": 20.9918212890625,
      "learning_rate": 1e-05,
      "loss": 6.6839,
      "step": 16296
    },
    {
      "epoch": 0.8832520325203252,
      "step": 16296,
      "training_loss": 6.255098819732666
    },
    {
      "epoch": 0.8833062330623306,
      "step": 16297,
      "training_loss": 6.36945104598999
    },
    {
      "epoch": 0.883360433604336,
      "step": 16298,
      "training_loss": 7.629331111907959
    },
    {
      "epoch": 0.8834146341463415,
      "step": 16299,
      "training_loss": 5.2747039794921875
    },
    {
      "epoch": 0.8834688346883469,
      "grad_norm": 73.60588836669922,
      "learning_rate": 1e-05,
      "loss": 6.3821,
      "step": 16300
    },
    {
      "epoch": 0.8834688346883469,
      "step": 16300,
      "training_loss": 5.44353723526001
    },
    {
      "epoch": 0.8835230352303522,
      "step": 16301,
      "training_loss": 5.210937023162842
    },
    {
      "epoch": 0.8835772357723577,
      "step": 16302,
      "training_loss": 6.342074871063232
    },
    {
      "epoch": 0.8836314363143631,
      "step": 16303,
      "training_loss": 6.873593807220459
    },
    {
      "epoch": 0.8836856368563686,
      "grad_norm": 26.25973892211914,
      "learning_rate": 1e-05,
      "loss": 5.9675,
      "step": 16304
    },
    {
      "epoch": 0.8836856368563686,
      "step": 16304,
      "training_loss": 7.579299449920654
    },
    {
      "epoch": 0.883739837398374,
      "step": 16305,
      "training_loss": 4.3516716957092285
    },
    {
      "epoch": 0.8837940379403794,
      "step": 16306,
      "training_loss": 6.698874473571777
    },
    {
      "epoch": 0.8838482384823848,
      "step": 16307,
      "training_loss": 6.797945976257324
    },
    {
      "epoch": 0.8839024390243903,
      "grad_norm": 22.739341735839844,
      "learning_rate": 1e-05,
      "loss": 6.3569,
      "step": 16308
    },
    {
      "epoch": 0.8839024390243903,
      "step": 16308,
      "training_loss": 7.471350193023682
    },
    {
      "epoch": 0.8839566395663957,
      "step": 16309,
      "training_loss": 6.451164722442627
    },
    {
      "epoch": 0.884010840108401,
      "step": 16310,
      "training_loss": 6.3496599197387695
    },
    {
      "epoch": 0.8840650406504065,
      "step": 16311,
      "training_loss": 7.272218704223633
    },
    {
      "epoch": 0.8841192411924119,
      "grad_norm": 19.77685546875,
      "learning_rate": 1e-05,
      "loss": 6.8861,
      "step": 16312
    },
    {
      "epoch": 0.8841192411924119,
      "step": 16312,
      "training_loss": 5.732848167419434
    },
    {
      "epoch": 0.8841734417344174,
      "step": 16313,
      "training_loss": 5.763758659362793
    },
    {
      "epoch": 0.8842276422764228,
      "step": 16314,
      "training_loss": 6.486178874969482
    },
    {
      "epoch": 0.8842818428184281,
      "step": 16315,
      "training_loss": 7.432406902313232
    },
    {
      "epoch": 0.8843360433604336,
      "grad_norm": 22.214908599853516,
      "learning_rate": 1e-05,
      "loss": 6.3538,
      "step": 16316
    },
    {
      "epoch": 0.8843360433604336,
      "step": 16316,
      "training_loss": 5.241045951843262
    },
    {
      "epoch": 0.884390243902439,
      "step": 16317,
      "training_loss": 6.283017635345459
    },
    {
      "epoch": 0.8844444444444445,
      "step": 16318,
      "training_loss": 7.681235313415527
    },
    {
      "epoch": 0.8844986449864498,
      "step": 16319,
      "training_loss": 4.923888206481934
    },
    {
      "epoch": 0.8845528455284553,
      "grad_norm": 29.6170711517334,
      "learning_rate": 1e-05,
      "loss": 6.0323,
      "step": 16320
    },
    {
      "epoch": 0.8845528455284553,
      "step": 16320,
      "training_loss": 7.546404838562012
    },
    {
      "epoch": 0.8846070460704607,
      "step": 16321,
      "training_loss": 6.1160407066345215
    },
    {
      "epoch": 0.8846612466124661,
      "step": 16322,
      "training_loss": 6.093003273010254
    },
    {
      "epoch": 0.8847154471544716,
      "step": 16323,
      "training_loss": 6.3839111328125
    },
    {
      "epoch": 0.8847696476964769,
      "grad_norm": 23.33020782470703,
      "learning_rate": 1e-05,
      "loss": 6.5348,
      "step": 16324
    },
    {
      "epoch": 0.8847696476964769,
      "step": 16324,
      "training_loss": 7.514215469360352
    },
    {
      "epoch": 0.8848238482384824,
      "step": 16325,
      "training_loss": 8.041706085205078
    },
    {
      "epoch": 0.8848780487804878,
      "step": 16326,
      "training_loss": 4.388975620269775
    },
    {
      "epoch": 0.8849322493224933,
      "step": 16327,
      "training_loss": 7.259993076324463
    },
    {
      "epoch": 0.8849864498644986,
      "grad_norm": 30.060949325561523,
      "learning_rate": 1e-05,
      "loss": 6.8012,
      "step": 16328
    },
    {
      "epoch": 0.8849864498644986,
      "step": 16328,
      "training_loss": 6.6411871910095215
    },
    {
      "epoch": 0.885040650406504,
      "step": 16329,
      "training_loss": 6.955702781677246
    },
    {
      "epoch": 0.8850948509485095,
      "step": 16330,
      "training_loss": 7.6450958251953125
    },
    {
      "epoch": 0.8851490514905149,
      "step": 16331,
      "training_loss": 7.400286674499512
    },
    {
      "epoch": 0.8852032520325204,
      "grad_norm": 74.87821197509766,
      "learning_rate": 1e-05,
      "loss": 7.1606,
      "step": 16332
    },
    {
      "epoch": 0.8852032520325204,
      "step": 16332,
      "training_loss": 6.96476936340332
    },
    {
      "epoch": 0.8852574525745257,
      "step": 16333,
      "training_loss": 5.159557342529297
    },
    {
      "epoch": 0.8853116531165311,
      "step": 16334,
      "training_loss": 6.738874912261963
    },
    {
      "epoch": 0.8853658536585366,
      "step": 16335,
      "training_loss": 6.828142166137695
    },
    {
      "epoch": 0.885420054200542,
      "grad_norm": 24.118803024291992,
      "learning_rate": 1e-05,
      "loss": 6.4228,
      "step": 16336
    },
    {
      "epoch": 0.885420054200542,
      "step": 16336,
      "training_loss": 3.9526216983795166
    },
    {
      "epoch": 0.8854742547425474,
      "step": 16337,
      "training_loss": 7.3368120193481445
    },
    {
      "epoch": 0.8855284552845528,
      "step": 16338,
      "training_loss": 6.564462661743164
    },
    {
      "epoch": 0.8855826558265583,
      "step": 16339,
      "training_loss": 7.176948547363281
    },
    {
      "epoch": 0.8856368563685637,
      "grad_norm": 39.59274673461914,
      "learning_rate": 1e-05,
      "loss": 6.2577,
      "step": 16340
    },
    {
      "epoch": 0.8856368563685637,
      "step": 16340,
      "training_loss": 6.673501014709473
    },
    {
      "epoch": 0.8856910569105692,
      "step": 16341,
      "training_loss": 7.2184576988220215
    },
    {
      "epoch": 0.8857452574525745,
      "step": 16342,
      "training_loss": 4.960080623626709
    },
    {
      "epoch": 0.8857994579945799,
      "step": 16343,
      "training_loss": 6.59197998046875
    },
    {
      "epoch": 0.8858536585365854,
      "grad_norm": 32.55229568481445,
      "learning_rate": 1e-05,
      "loss": 6.361,
      "step": 16344
    },
    {
      "epoch": 0.8858536585365854,
      "step": 16344,
      "training_loss": 5.6409478187561035
    },
    {
      "epoch": 0.8859078590785908,
      "step": 16345,
      "training_loss": 7.105512619018555
    },
    {
      "epoch": 0.8859620596205962,
      "step": 16346,
      "training_loss": 5.482973098754883
    },
    {
      "epoch": 0.8860162601626016,
      "step": 16347,
      "training_loss": 5.963119029998779
    },
    {
      "epoch": 0.886070460704607,
      "grad_norm": 22.814388275146484,
      "learning_rate": 1e-05,
      "loss": 6.0481,
      "step": 16348
    },
    {
      "epoch": 0.886070460704607,
      "step": 16348,
      "training_loss": 6.752408027648926
    },
    {
      "epoch": 0.8861246612466125,
      "step": 16349,
      "training_loss": 5.4715657234191895
    },
    {
      "epoch": 0.8861788617886179,
      "step": 16350,
      "training_loss": 8.941732406616211
    },
    {
      "epoch": 0.8862330623306233,
      "step": 16351,
      "training_loss": 6.357893943786621
    },
    {
      "epoch": 0.8862872628726287,
      "grad_norm": 33.73069381713867,
      "learning_rate": 1e-05,
      "loss": 6.8809,
      "step": 16352
    },
    {
      "epoch": 0.8862872628726287,
      "step": 16352,
      "training_loss": 6.043553352355957
    },
    {
      "epoch": 0.8863414634146342,
      "step": 16353,
      "training_loss": 4.723349094390869
    },
    {
      "epoch": 0.8863956639566396,
      "step": 16354,
      "training_loss": 6.83899450302124
    },
    {
      "epoch": 0.8864498644986449,
      "step": 16355,
      "training_loss": 6.07919979095459
    },
    {
      "epoch": 0.8865040650406504,
      "grad_norm": 28.10577392578125,
      "learning_rate": 1e-05,
      "loss": 5.9213,
      "step": 16356
    },
    {
      "epoch": 0.8865040650406504,
      "step": 16356,
      "training_loss": 6.610316753387451
    },
    {
      "epoch": 0.8865582655826558,
      "step": 16357,
      "training_loss": 7.5817484855651855
    },
    {
      "epoch": 0.8866124661246613,
      "step": 16358,
      "training_loss": 7.45106840133667
    },
    {
      "epoch": 0.8866666666666667,
      "step": 16359,
      "training_loss": 6.788393020629883
    },
    {
      "epoch": 0.886720867208672,
      "grad_norm": 24.575284957885742,
      "learning_rate": 1e-05,
      "loss": 7.1079,
      "step": 16360
    },
    {
      "epoch": 0.886720867208672,
      "step": 16360,
      "training_loss": 7.301067352294922
    },
    {
      "epoch": 0.8867750677506775,
      "step": 16361,
      "training_loss": 6.960587024688721
    },
    {
      "epoch": 0.8868292682926829,
      "step": 16362,
      "training_loss": 7.56486701965332
    },
    {
      "epoch": 0.8868834688346884,
      "step": 16363,
      "training_loss": 8.78268814086914
    },
    {
      "epoch": 0.8869376693766937,
      "grad_norm": 60.351898193359375,
      "learning_rate": 1e-05,
      "loss": 7.6523,
      "step": 16364
    },
    {
      "epoch": 0.8869376693766937,
      "step": 16364,
      "training_loss": 6.117102146148682
    },
    {
      "epoch": 0.8869918699186992,
      "step": 16365,
      "training_loss": 6.938382148742676
    },
    {
      "epoch": 0.8870460704607046,
      "step": 16366,
      "training_loss": 7.037078380584717
    },
    {
      "epoch": 0.88710027100271,
      "step": 16367,
      "training_loss": 4.734068870544434
    },
    {
      "epoch": 0.8871544715447155,
      "grad_norm": 83.06864166259766,
      "learning_rate": 1e-05,
      "loss": 6.2067,
      "step": 16368
    },
    {
      "epoch": 0.8871544715447155,
      "step": 16368,
      "training_loss": 6.48158073425293
    },
    {
      "epoch": 0.8872086720867208,
      "step": 16369,
      "training_loss": 6.861054420471191
    },
    {
      "epoch": 0.8872628726287263,
      "step": 16370,
      "training_loss": 7.320653915405273
    },
    {
      "epoch": 0.8873170731707317,
      "step": 16371,
      "training_loss": 5.6148576736450195
    },
    {
      "epoch": 0.8873712737127372,
      "grad_norm": 23.887765884399414,
      "learning_rate": 1e-05,
      "loss": 6.5695,
      "step": 16372
    },
    {
      "epoch": 0.8873712737127372,
      "step": 16372,
      "training_loss": 6.205410003662109
    },
    {
      "epoch": 0.8874254742547425,
      "step": 16373,
      "training_loss": 6.745288848876953
    },
    {
      "epoch": 0.8874796747967479,
      "step": 16374,
      "training_loss": 6.663930416107178
    },
    {
      "epoch": 0.8875338753387534,
      "step": 16375,
      "training_loss": 8.039031028747559
    },
    {
      "epoch": 0.8875880758807588,
      "grad_norm": 42.46949768066406,
      "learning_rate": 1e-05,
      "loss": 6.9134,
      "step": 16376
    },
    {
      "epoch": 0.8875880758807588,
      "step": 16376,
      "training_loss": 7.18406867980957
    },
    {
      "epoch": 0.8876422764227643,
      "step": 16377,
      "training_loss": 4.707579135894775
    },
    {
      "epoch": 0.8876964769647696,
      "step": 16378,
      "training_loss": 5.8607306480407715
    },
    {
      "epoch": 0.887750677506775,
      "step": 16379,
      "training_loss": 7.333747863769531
    },
    {
      "epoch": 0.8878048780487805,
      "grad_norm": 27.68413543701172,
      "learning_rate": 1e-05,
      "loss": 6.2715,
      "step": 16380
    },
    {
      "epoch": 0.8878048780487805,
      "step": 16380,
      "training_loss": 4.796787738800049
    },
    {
      "epoch": 0.8878590785907859,
      "step": 16381,
      "training_loss": 7.325150966644287
    },
    {
      "epoch": 0.8879132791327913,
      "step": 16382,
      "training_loss": 6.511449337005615
    },
    {
      "epoch": 0.8879674796747967,
      "step": 16383,
      "training_loss": 7.301548957824707
    },
    {
      "epoch": 0.8880216802168022,
      "grad_norm": 17.16895866394043,
      "learning_rate": 1e-05,
      "loss": 6.4837,
      "step": 16384
    },
    {
      "epoch": 0.8880216802168022,
      "step": 16384,
      "training_loss": 5.36842679977417
    },
    {
      "epoch": 0.8880758807588076,
      "step": 16385,
      "training_loss": 7.312533378601074
    },
    {
      "epoch": 0.8881300813008131,
      "step": 16386,
      "training_loss": 6.897887706756592
    },
    {
      "epoch": 0.8881842818428184,
      "step": 16387,
      "training_loss": 4.829286098480225
    },
    {
      "epoch": 0.8882384823848238,
      "grad_norm": 25.925813674926758,
      "learning_rate": 1e-05,
      "loss": 6.102,
      "step": 16388
    },
    {
      "epoch": 0.8882384823848238,
      "step": 16388,
      "training_loss": 3.293278217315674
    },
    {
      "epoch": 0.8882926829268293,
      "step": 16389,
      "training_loss": 5.049850940704346
    },
    {
      "epoch": 0.8883468834688347,
      "step": 16390,
      "training_loss": 3.8130605220794678
    },
    {
      "epoch": 0.8884010840108401,
      "step": 16391,
      "training_loss": 7.7045416831970215
    },
    {
      "epoch": 0.8884552845528455,
      "grad_norm": 20.396785736083984,
      "learning_rate": 1e-05,
      "loss": 4.9652,
      "step": 16392
    },
    {
      "epoch": 0.8884552845528455,
      "step": 16392,
      "training_loss": 8.778133392333984
    },
    {
      "epoch": 0.888509485094851,
      "step": 16393,
      "training_loss": 7.503726482391357
    },
    {
      "epoch": 0.8885636856368564,
      "step": 16394,
      "training_loss": 6.312596797943115
    },
    {
      "epoch": 0.8886178861788618,
      "step": 16395,
      "training_loss": 6.824011325836182
    },
    {
      "epoch": 0.8886720867208672,
      "grad_norm": 20.472904205322266,
      "learning_rate": 1e-05,
      "loss": 7.3546,
      "step": 16396
    },
    {
      "epoch": 0.8886720867208672,
      "step": 16396,
      "training_loss": 7.1168212890625
    },
    {
      "epoch": 0.8887262872628726,
      "step": 16397,
      "training_loss": 5.6092000007629395
    },
    {
      "epoch": 0.8887804878048781,
      "step": 16398,
      "training_loss": 7.702116966247559
    },
    {
      "epoch": 0.8888346883468835,
      "step": 16399,
      "training_loss": 4.330404758453369
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 27.99752426147461,
      "learning_rate": 1e-05,
      "loss": 6.1896,
      "step": 16400
    },
    {
      "epoch": 0.8888888888888888,
      "step": 16400,
      "training_loss": 6.506317615509033
    },
    {
      "epoch": 0.8889430894308943,
      "step": 16401,
      "training_loss": 5.5804524421691895
    },
    {
      "epoch": 0.8889972899728997,
      "step": 16402,
      "training_loss": 3.4798200130462646
    },
    {
      "epoch": 0.8890514905149052,
      "step": 16403,
      "training_loss": 6.80084228515625
    },
    {
      "epoch": 0.8891056910569106,
      "grad_norm": 23.245237350463867,
      "learning_rate": 1e-05,
      "loss": 5.5919,
      "step": 16404
    },
    {
      "epoch": 0.8891056910569106,
      "step": 16404,
      "training_loss": 6.913923740386963
    },
    {
      "epoch": 0.889159891598916,
      "step": 16405,
      "training_loss": 7.201761245727539
    },
    {
      "epoch": 0.8892140921409214,
      "step": 16406,
      "training_loss": 6.2376556396484375
    },
    {
      "epoch": 0.8892682926829268,
      "step": 16407,
      "training_loss": 5.727364540100098
    },
    {
      "epoch": 0.8893224932249323,
      "grad_norm": 24.927005767822266,
      "learning_rate": 1e-05,
      "loss": 6.5202,
      "step": 16408
    },
    {
      "epoch": 0.8893224932249323,
      "step": 16408,
      "training_loss": 6.051374912261963
    },
    {
      "epoch": 0.8893766937669376,
      "step": 16409,
      "training_loss": 4.238945484161377
    },
    {
      "epoch": 0.8894308943089431,
      "step": 16410,
      "training_loss": 6.623310565948486
    },
    {
      "epoch": 0.8894850948509485,
      "step": 16411,
      "training_loss": 5.988734245300293
    },
    {
      "epoch": 0.889539295392954,
      "grad_norm": 28.72970199584961,
      "learning_rate": 1e-05,
      "loss": 5.7256,
      "step": 16412
    },
    {
      "epoch": 0.889539295392954,
      "step": 16412,
      "training_loss": 6.597886085510254
    },
    {
      "epoch": 0.8895934959349594,
      "step": 16413,
      "training_loss": 7.294646739959717
    },
    {
      "epoch": 0.8896476964769647,
      "step": 16414,
      "training_loss": 6.90089225769043
    },
    {
      "epoch": 0.8897018970189702,
      "step": 16415,
      "training_loss": 6.702959060668945
    },
    {
      "epoch": 0.8897560975609756,
      "grad_norm": 24.850387573242188,
      "learning_rate": 1e-05,
      "loss": 6.8741,
      "step": 16416
    },
    {
      "epoch": 0.8897560975609756,
      "step": 16416,
      "training_loss": 7.846729755401611
    },
    {
      "epoch": 0.8898102981029811,
      "step": 16417,
      "training_loss": 6.69592809677124
    },
    {
      "epoch": 0.8898644986449864,
      "step": 16418,
      "training_loss": 3.3203682899475098
    },
    {
      "epoch": 0.8899186991869918,
      "step": 16419,
      "training_loss": 4.245434761047363
    },
    {
      "epoch": 0.8899728997289973,
      "grad_norm": 20.48187828063965,
      "learning_rate": 1e-05,
      "loss": 5.5271,
      "step": 16420
    },
    {
      "epoch": 0.8899728997289973,
      "step": 16420,
      "training_loss": 3.8533122539520264
    },
    {
      "epoch": 0.8900271002710027,
      "step": 16421,
      "training_loss": 7.106569766998291
    },
    {
      "epoch": 0.8900813008130082,
      "step": 16422,
      "training_loss": 6.355059623718262
    },
    {
      "epoch": 0.8901355013550135,
      "step": 16423,
      "training_loss": 7.419143199920654
    },
    {
      "epoch": 0.890189701897019,
      "grad_norm": 41.6098518371582,
      "learning_rate": 1e-05,
      "loss": 6.1835,
      "step": 16424
    },
    {
      "epoch": 0.890189701897019,
      "step": 16424,
      "training_loss": 6.015321254730225
    },
    {
      "epoch": 0.8902439024390244,
      "step": 16425,
      "training_loss": 7.007999897003174
    },
    {
      "epoch": 0.8902981029810298,
      "step": 16426,
      "training_loss": 9.134844779968262
    },
    {
      "epoch": 0.8903523035230352,
      "step": 16427,
      "training_loss": 8.50255298614502
    },
    {
      "epoch": 0.8904065040650406,
      "grad_norm": 43.166358947753906,
      "learning_rate": 1e-05,
      "loss": 7.6652,
      "step": 16428
    },
    {
      "epoch": 0.8904065040650406,
      "step": 16428,
      "training_loss": 8.959637641906738
    },
    {
      "epoch": 0.8904607046070461,
      "step": 16429,
      "training_loss": 7.253174781799316
    },
    {
      "epoch": 0.8905149051490515,
      "step": 16430,
      "training_loss": 5.778744220733643
    },
    {
      "epoch": 0.890569105691057,
      "step": 16431,
      "training_loss": 6.408044338226318
    },
    {
      "epoch": 0.8906233062330623,
      "grad_norm": 43.14775466918945,
      "learning_rate": 1e-05,
      "loss": 7.0999,
      "step": 16432
    },
    {
      "epoch": 0.8906233062330623,
      "step": 16432,
      "training_loss": 5.7006988525390625
    },
    {
      "epoch": 0.8906775067750677,
      "step": 16433,
      "training_loss": 6.3779072761535645
    },
    {
      "epoch": 0.8907317073170732,
      "step": 16434,
      "training_loss": 7.271320343017578
    },
    {
      "epoch": 0.8907859078590786,
      "step": 16435,
      "training_loss": 4.4773149490356445
    },
    {
      "epoch": 0.890840108401084,
      "grad_norm": 33.56062316894531,
      "learning_rate": 1e-05,
      "loss": 5.9568,
      "step": 16436
    },
    {
      "epoch": 0.890840108401084,
      "step": 16436,
      "training_loss": 6.970992088317871
    },
    {
      "epoch": 0.8908943089430894,
      "step": 16437,
      "training_loss": 7.744760513305664
    },
    {
      "epoch": 0.8909485094850949,
      "step": 16438,
      "training_loss": 6.228892803192139
    },
    {
      "epoch": 0.8910027100271003,
      "step": 16439,
      "training_loss": 6.311065196990967
    },
    {
      "epoch": 0.8910569105691057,
      "grad_norm": 31.705102920532227,
      "learning_rate": 1e-05,
      "loss": 6.8139,
      "step": 16440
    },
    {
      "epoch": 0.8910569105691057,
      "step": 16440,
      "training_loss": 5.94208288192749
    },
    {
      "epoch": 0.8911111111111111,
      "step": 16441,
      "training_loss": 7.723761081695557
    },
    {
      "epoch": 0.8911653116531165,
      "step": 16442,
      "training_loss": 6.5728678703308105
    },
    {
      "epoch": 0.891219512195122,
      "step": 16443,
      "training_loss": 7.192651748657227
    },
    {
      "epoch": 0.8912737127371274,
      "grad_norm": 35.986942291259766,
      "learning_rate": 1e-05,
      "loss": 6.8578,
      "step": 16444
    },
    {
      "epoch": 0.8912737127371274,
      "step": 16444,
      "training_loss": 2.2664456367492676
    },
    {
      "epoch": 0.8913279132791327,
      "step": 16445,
      "training_loss": 5.818356990814209
    },
    {
      "epoch": 0.8913821138211382,
      "step": 16446,
      "training_loss": 5.5048298835754395
    },
    {
      "epoch": 0.8914363143631436,
      "step": 16447,
      "training_loss": 6.948551177978516
    },
    {
      "epoch": 0.8914905149051491,
      "grad_norm": 21.677705764770508,
      "learning_rate": 1e-05,
      "loss": 5.1345,
      "step": 16448
    },
    {
      "epoch": 0.8914905149051491,
      "step": 16448,
      "training_loss": 6.796381950378418
    },
    {
      "epoch": 0.8915447154471545,
      "step": 16449,
      "training_loss": 6.757991313934326
    },
    {
      "epoch": 0.8915989159891599,
      "step": 16450,
      "training_loss": 7.794685363769531
    },
    {
      "epoch": 0.8916531165311653,
      "step": 16451,
      "training_loss": 6.563109397888184
    },
    {
      "epoch": 0.8917073170731707,
      "grad_norm": 28.499271392822266,
      "learning_rate": 1e-05,
      "loss": 6.978,
      "step": 16452
    },
    {
      "epoch": 0.8917073170731707,
      "step": 16452,
      "training_loss": 6.470669746398926
    },
    {
      "epoch": 0.8917615176151762,
      "step": 16453,
      "training_loss": 8.912931442260742
    },
    {
      "epoch": 0.8918157181571815,
      "step": 16454,
      "training_loss": 5.240159511566162
    },
    {
      "epoch": 0.891869918699187,
      "step": 16455,
      "training_loss": 6.679563045501709
    },
    {
      "epoch": 0.8919241192411924,
      "grad_norm": 31.435914993286133,
      "learning_rate": 1e-05,
      "loss": 6.8258,
      "step": 16456
    },
    {
      "epoch": 0.8919241192411924,
      "step": 16456,
      "training_loss": 7.977163314819336
    },
    {
      "epoch": 0.8919783197831979,
      "step": 16457,
      "training_loss": 5.64174222946167
    },
    {
      "epoch": 0.8920325203252033,
      "step": 16458,
      "training_loss": 5.555458068847656
    },
    {
      "epoch": 0.8920867208672086,
      "step": 16459,
      "training_loss": 6.2975568771362305
    },
    {
      "epoch": 0.8921409214092141,
      "grad_norm": 23.25967025756836,
      "learning_rate": 1e-05,
      "loss": 6.368,
      "step": 16460
    },
    {
      "epoch": 0.8921409214092141,
      "step": 16460,
      "training_loss": 6.801940441131592
    },
    {
      "epoch": 0.8921951219512195,
      "step": 16461,
      "training_loss": 9.997817039489746
    },
    {
      "epoch": 0.892249322493225,
      "step": 16462,
      "training_loss": 7.709671974182129
    },
    {
      "epoch": 0.8923035230352303,
      "step": 16463,
      "training_loss": 6.808181285858154
    },
    {
      "epoch": 0.8923577235772358,
      "grad_norm": 38.995361328125,
      "learning_rate": 1e-05,
      "loss": 7.8294,
      "step": 16464
    },
    {
      "epoch": 0.8923577235772358,
      "step": 16464,
      "training_loss": 7.488511562347412
    },
    {
      "epoch": 0.8924119241192412,
      "step": 16465,
      "training_loss": 5.350327014923096
    },
    {
      "epoch": 0.8924661246612466,
      "step": 16466,
      "training_loss": 5.791987895965576
    },
    {
      "epoch": 0.8925203252032521,
      "step": 16467,
      "training_loss": 7.8361029624938965
    },
    {
      "epoch": 0.8925745257452574,
      "grad_norm": 30.479488372802734,
      "learning_rate": 1e-05,
      "loss": 6.6167,
      "step": 16468
    },
    {
      "epoch": 0.8925745257452574,
      "step": 16468,
      "training_loss": 6.417983531951904
    },
    {
      "epoch": 0.8926287262872629,
      "step": 16469,
      "training_loss": 6.502274513244629
    },
    {
      "epoch": 0.8926829268292683,
      "step": 16470,
      "training_loss": 6.00706672668457
    },
    {
      "epoch": 0.8927371273712738,
      "step": 16471,
      "training_loss": 6.647792816162109
    },
    {
      "epoch": 0.8927913279132791,
      "grad_norm": 25.560443878173828,
      "learning_rate": 1e-05,
      "loss": 6.3938,
      "step": 16472
    },
    {
      "epoch": 0.8927913279132791,
      "step": 16472,
      "training_loss": 3.8721089363098145
    },
    {
      "epoch": 0.8928455284552845,
      "step": 16473,
      "training_loss": 6.607048034667969
    },
    {
      "epoch": 0.89289972899729,
      "step": 16474,
      "training_loss": 7.220122814178467
    },
    {
      "epoch": 0.8929539295392954,
      "step": 16475,
      "training_loss": 8.04001522064209
    },
    {
      "epoch": 0.8930081300813009,
      "grad_norm": 41.462398529052734,
      "learning_rate": 1e-05,
      "loss": 6.4348,
      "step": 16476
    },
    {
      "epoch": 0.8930081300813009,
      "step": 16476,
      "training_loss": 4.6787638664245605
    },
    {
      "epoch": 0.8930623306233062,
      "step": 16477,
      "training_loss": 7.034825325012207
    },
    {
      "epoch": 0.8931165311653116,
      "step": 16478,
      "training_loss": 6.301064491271973
    },
    {
      "epoch": 0.8931707317073171,
      "step": 16479,
      "training_loss": 5.770132541656494
    },
    {
      "epoch": 0.8932249322493225,
      "grad_norm": 26.6119384765625,
      "learning_rate": 1e-05,
      "loss": 5.9462,
      "step": 16480
    },
    {
      "epoch": 0.8932249322493225,
      "step": 16480,
      "training_loss": 5.918608665466309
    },
    {
      "epoch": 0.8932791327913279,
      "step": 16481,
      "training_loss": 7.440194606781006
    },
    {
      "epoch": 0.8933333333333333,
      "step": 16482,
      "training_loss": 8.209479331970215
    },
    {
      "epoch": 0.8933875338753388,
      "step": 16483,
      "training_loss": 6.759469032287598
    },
    {
      "epoch": 0.8934417344173442,
      "grad_norm": 21.76115608215332,
      "learning_rate": 1e-05,
      "loss": 7.0819,
      "step": 16484
    },
    {
      "epoch": 0.8934417344173442,
      "step": 16484,
      "training_loss": 7.095344543457031
    },
    {
      "epoch": 0.8934959349593496,
      "step": 16485,
      "training_loss": 7.401263236999512
    },
    {
      "epoch": 0.893550135501355,
      "step": 16486,
      "training_loss": 6.052481651306152
    },
    {
      "epoch": 0.8936043360433604,
      "step": 16487,
      "training_loss": 6.787093639373779
    },
    {
      "epoch": 0.8936585365853659,
      "grad_norm": 24.615564346313477,
      "learning_rate": 1e-05,
      "loss": 6.834,
      "step": 16488
    },
    {
      "epoch": 0.8936585365853659,
      "step": 16488,
      "training_loss": 6.1668524742126465
    },
    {
      "epoch": 0.8937127371273713,
      "step": 16489,
      "training_loss": 6.977711200714111
    },
    {
      "epoch": 0.8937669376693766,
      "step": 16490,
      "training_loss": 5.368133544921875
    },
    {
      "epoch": 0.8938211382113821,
      "step": 16491,
      "training_loss": 6.913019180297852
    },
    {
      "epoch": 0.8938753387533875,
      "grad_norm": 23.991764068603516,
      "learning_rate": 1e-05,
      "loss": 6.3564,
      "step": 16492
    },
    {
      "epoch": 0.8938753387533875,
      "step": 16492,
      "training_loss": 6.390364170074463
    },
    {
      "epoch": 0.893929539295393,
      "step": 16493,
      "training_loss": 5.80591344833374
    },
    {
      "epoch": 0.8939837398373984,
      "step": 16494,
      "training_loss": 6.2973222732543945
    },
    {
      "epoch": 0.8940379403794038,
      "step": 16495,
      "training_loss": 6.977135181427002
    },
    {
      "epoch": 0.8940921409214092,
      "grad_norm": 26.151132583618164,
      "learning_rate": 1e-05,
      "loss": 6.3677,
      "step": 16496
    },
    {
      "epoch": 0.8940921409214092,
      "step": 16496,
      "training_loss": 5.716433048248291
    },
    {
      "epoch": 0.8941463414634147,
      "step": 16497,
      "training_loss": 5.2126970291137695
    },
    {
      "epoch": 0.8942005420054201,
      "step": 16498,
      "training_loss": 3.4101145267486572
    },
    {
      "epoch": 0.8942547425474254,
      "step": 16499,
      "training_loss": 7.345120906829834
    },
    {
      "epoch": 0.8943089430894309,
      "grad_norm": 42.544864654541016,
      "learning_rate": 1e-05,
      "loss": 5.4211,
      "step": 16500
    },
    {
      "epoch": 0.8943089430894309,
      "step": 16500,
      "training_loss": 7.7857666015625
    },
    {
      "epoch": 0.8943631436314363,
      "step": 16501,
      "training_loss": 5.088243007659912
    },
    {
      "epoch": 0.8944173441734418,
      "step": 16502,
      "training_loss": 7.137870788574219
    },
    {
      "epoch": 0.8944715447154472,
      "step": 16503,
      "training_loss": 7.0009002685546875
    },
    {
      "epoch": 0.8945257452574525,
      "grad_norm": 36.39982604980469,
      "learning_rate": 1e-05,
      "loss": 6.7532,
      "step": 16504
    },
    {
      "epoch": 0.8945257452574525,
      "step": 16504,
      "training_loss": 6.686227321624756
    },
    {
      "epoch": 0.894579945799458,
      "step": 16505,
      "training_loss": 5.648910045623779
    },
    {
      "epoch": 0.8946341463414634,
      "step": 16506,
      "training_loss": 3.922945499420166
    },
    {
      "epoch": 0.8946883468834689,
      "step": 16507,
      "training_loss": 5.5978169441223145
    },
    {
      "epoch": 0.8947425474254742,
      "grad_norm": 33.61893081665039,
      "learning_rate": 1e-05,
      "loss": 5.464,
      "step": 16508
    },
    {
      "epoch": 0.8947425474254742,
      "step": 16508,
      "training_loss": 6.001384258270264
    },
    {
      "epoch": 0.8947967479674797,
      "step": 16509,
      "training_loss": 4.72165584564209
    },
    {
      "epoch": 0.8948509485094851,
      "step": 16510,
      "training_loss": 6.8146467208862305
    },
    {
      "epoch": 0.8949051490514905,
      "step": 16511,
      "training_loss": 4.061631679534912
    },
    {
      "epoch": 0.894959349593496,
      "grad_norm": 40.56195831298828,
      "learning_rate": 1e-05,
      "loss": 5.3998,
      "step": 16512
    },
    {
      "epoch": 0.894959349593496,
      "step": 16512,
      "training_loss": 6.180490016937256
    },
    {
      "epoch": 0.8950135501355013,
      "step": 16513,
      "training_loss": 6.426627159118652
    },
    {
      "epoch": 0.8950677506775068,
      "step": 16514,
      "training_loss": 5.861419200897217
    },
    {
      "epoch": 0.8951219512195122,
      "step": 16515,
      "training_loss": 6.21677303314209
    },
    {
      "epoch": 0.8951761517615177,
      "grad_norm": 31.583690643310547,
      "learning_rate": 1e-05,
      "loss": 6.1713,
      "step": 16516
    },
    {
      "epoch": 0.8951761517615177,
      "step": 16516,
      "training_loss": 7.914864540100098
    },
    {
      "epoch": 0.895230352303523,
      "step": 16517,
      "training_loss": 6.46079158782959
    },
    {
      "epoch": 0.8952845528455284,
      "step": 16518,
      "training_loss": 6.000638008117676
    },
    {
      "epoch": 0.8953387533875339,
      "step": 16519,
      "training_loss": 2.6876182556152344
    },
    {
      "epoch": 0.8953929539295393,
      "grad_norm": 32.95825958251953,
      "learning_rate": 1e-05,
      "loss": 5.766,
      "step": 16520
    },
    {
      "epoch": 0.8953929539295393,
      "step": 16520,
      "training_loss": 5.774566650390625
    },
    {
      "epoch": 0.8954471544715448,
      "step": 16521,
      "training_loss": 6.377584934234619
    },
    {
      "epoch": 0.8955013550135501,
      "step": 16522,
      "training_loss": 4.309332370758057
    },
    {
      "epoch": 0.8955555555555555,
      "step": 16523,
      "training_loss": 4.252425193786621
    },
    {
      "epoch": 0.895609756097561,
      "grad_norm": 23.537548065185547,
      "learning_rate": 1e-05,
      "loss": 5.1785,
      "step": 16524
    },
    {
      "epoch": 0.895609756097561,
      "step": 16524,
      "training_loss": 7.718618392944336
    },
    {
      "epoch": 0.8956639566395664,
      "step": 16525,
      "training_loss": 4.41533899307251
    },
    {
      "epoch": 0.8957181571815718,
      "step": 16526,
      "training_loss": 5.274014472961426
    },
    {
      "epoch": 0.8957723577235772,
      "step": 16527,
      "training_loss": 5.8744215965271
    },
    {
      "epoch": 0.8958265582655827,
      "grad_norm": 35.07508850097656,
      "learning_rate": 1e-05,
      "loss": 5.8206,
      "step": 16528
    },
    {
      "epoch": 0.8958265582655827,
      "step": 16528,
      "training_loss": 6.6257476806640625
    },
    {
      "epoch": 0.8958807588075881,
      "step": 16529,
      "training_loss": 8.272043228149414
    },
    {
      "epoch": 0.8959349593495934,
      "step": 16530,
      "training_loss": 6.332050800323486
    },
    {
      "epoch": 0.8959891598915989,
      "step": 16531,
      "training_loss": 3.2292792797088623
    },
    {
      "epoch": 0.8960433604336043,
      "grad_norm": 32.87667465209961,
      "learning_rate": 1e-05,
      "loss": 6.1148,
      "step": 16532
    },
    {
      "epoch": 0.8960433604336043,
      "step": 16532,
      "training_loss": 4.680527687072754
    },
    {
      "epoch": 0.8960975609756098,
      "step": 16533,
      "training_loss": 6.72355318069458
    },
    {
      "epoch": 0.8961517615176152,
      "step": 16534,
      "training_loss": 5.825167179107666
    },
    {
      "epoch": 0.8962059620596206,
      "step": 16535,
      "training_loss": 6.769229412078857
    },
    {
      "epoch": 0.896260162601626,
      "grad_norm": 22.91006088256836,
      "learning_rate": 1e-05,
      "loss": 5.9996,
      "step": 16536
    },
    {
      "epoch": 0.896260162601626,
      "step": 16536,
      "training_loss": 7.047469139099121
    },
    {
      "epoch": 0.8963143631436314,
      "step": 16537,
      "training_loss": 6.020793437957764
    },
    {
      "epoch": 0.8963685636856369,
      "step": 16538,
      "training_loss": 6.377033710479736
    },
    {
      "epoch": 0.8964227642276422,
      "step": 16539,
      "training_loss": 6.682199001312256
    },
    {
      "epoch": 0.8964769647696477,
      "grad_norm": 39.00353240966797,
      "learning_rate": 1e-05,
      "loss": 6.5319,
      "step": 16540
    },
    {
      "epoch": 0.8964769647696477,
      "step": 16540,
      "training_loss": 3.4685089588165283
    },
    {
      "epoch": 0.8965311653116531,
      "step": 16541,
      "training_loss": 4.726424217224121
    },
    {
      "epoch": 0.8965853658536586,
      "step": 16542,
      "training_loss": 6.050688743591309
    },
    {
      "epoch": 0.896639566395664,
      "step": 16543,
      "training_loss": 6.746222496032715
    },
    {
      "epoch": 0.8966937669376693,
      "grad_norm": 20.93659782409668,
      "learning_rate": 1e-05,
      "loss": 5.248,
      "step": 16544
    },
    {
      "epoch": 0.8966937669376693,
      "step": 16544,
      "training_loss": 5.366246223449707
    },
    {
      "epoch": 0.8967479674796748,
      "step": 16545,
      "training_loss": 7.6124267578125
    },
    {
      "epoch": 0.8968021680216802,
      "step": 16546,
      "training_loss": 6.600456714630127
    },
    {
      "epoch": 0.8968563685636857,
      "step": 16547,
      "training_loss": 7.600573539733887
    },
    {
      "epoch": 0.896910569105691,
      "grad_norm": 25.440513610839844,
      "learning_rate": 1e-05,
      "loss": 6.7949,
      "step": 16548
    },
    {
      "epoch": 0.896910569105691,
      "step": 16548,
      "training_loss": 5.6483378410339355
    },
    {
      "epoch": 0.8969647696476964,
      "step": 16549,
      "training_loss": 7.20777702331543
    },
    {
      "epoch": 0.8970189701897019,
      "step": 16550,
      "training_loss": 7.9559526443481445
    },
    {
      "epoch": 0.8970731707317073,
      "step": 16551,
      "training_loss": 4.769173622131348
    },
    {
      "epoch": 0.8971273712737128,
      "grad_norm": 24.390424728393555,
      "learning_rate": 1e-05,
      "loss": 6.3953,
      "step": 16552
    },
    {
      "epoch": 0.8971273712737128,
      "step": 16552,
      "training_loss": 6.946728229522705
    },
    {
      "epoch": 0.8971815718157181,
      "step": 16553,
      "training_loss": 7.734067916870117
    },
    {
      "epoch": 0.8972357723577236,
      "step": 16554,
      "training_loss": 7.085958480834961
    },
    {
      "epoch": 0.897289972899729,
      "step": 16555,
      "training_loss": 6.999805927276611
    },
    {
      "epoch": 0.8973441734417344,
      "grad_norm": 51.79275894165039,
      "learning_rate": 1e-05,
      "loss": 7.1916,
      "step": 16556
    },
    {
      "epoch": 0.8973441734417344,
      "step": 16556,
      "training_loss": 6.328207969665527
    },
    {
      "epoch": 0.8973983739837398,
      "step": 16557,
      "training_loss": 6.505393981933594
    },
    {
      "epoch": 0.8974525745257452,
      "step": 16558,
      "training_loss": 5.549863815307617
    },
    {
      "epoch": 0.8975067750677507,
      "step": 16559,
      "training_loss": 3.222247838973999
    },
    {
      "epoch": 0.8975609756097561,
      "grad_norm": 34.59964370727539,
      "learning_rate": 1e-05,
      "loss": 5.4014,
      "step": 16560
    },
    {
      "epoch": 0.8975609756097561,
      "step": 16560,
      "training_loss": 6.1705732345581055
    },
    {
      "epoch": 0.8976151761517616,
      "step": 16561,
      "training_loss": 6.179451942443848
    },
    {
      "epoch": 0.8976693766937669,
      "step": 16562,
      "training_loss": 6.102781295776367
    },
    {
      "epoch": 0.8977235772357723,
      "step": 16563,
      "training_loss": 6.048711776733398
    },
    {
      "epoch": 0.8977777777777778,
      "grad_norm": 48.00349807739258,
      "learning_rate": 1e-05,
      "loss": 6.1254,
      "step": 16564
    },
    {
      "epoch": 0.8977777777777778,
      "step": 16564,
      "training_loss": 7.775498390197754
    },
    {
      "epoch": 0.8978319783197832,
      "step": 16565,
      "training_loss": 6.946605205535889
    },
    {
      "epoch": 0.8978861788617886,
      "step": 16566,
      "training_loss": 5.773040771484375
    },
    {
      "epoch": 0.897940379403794,
      "step": 16567,
      "training_loss": 6.548391342163086
    },
    {
      "epoch": 0.8979945799457995,
      "grad_norm": 20.762792587280273,
      "learning_rate": 1e-05,
      "loss": 6.7609,
      "step": 16568
    },
    {
      "epoch": 0.8979945799457995,
      "step": 16568,
      "training_loss": 3.0774409770965576
    },
    {
      "epoch": 0.8980487804878049,
      "step": 16569,
      "training_loss": 5.80779504776001
    },
    {
      "epoch": 0.8981029810298103,
      "step": 16570,
      "training_loss": 7.139615535736084
    },
    {
      "epoch": 0.8981571815718157,
      "step": 16571,
      "training_loss": 3.769973039627075
    },
    {
      "epoch": 0.8982113821138211,
      "grad_norm": 77.96926879882812,
      "learning_rate": 1e-05,
      "loss": 4.9487,
      "step": 16572
    },
    {
      "epoch": 0.8982113821138211,
      "step": 16572,
      "training_loss": 6.653196811676025
    },
    {
      "epoch": 0.8982655826558266,
      "step": 16573,
      "training_loss": 6.151552677154541
    },
    {
      "epoch": 0.898319783197832,
      "step": 16574,
      "training_loss": 6.1677751541137695
    },
    {
      "epoch": 0.8983739837398373,
      "step": 16575,
      "training_loss": 8.122392654418945
    },
    {
      "epoch": 0.8984281842818428,
      "grad_norm": 23.95293426513672,
      "learning_rate": 1e-05,
      "loss": 6.7737,
      "step": 16576
    },
    {
      "epoch": 0.8984281842818428,
      "step": 16576,
      "training_loss": 6.852252960205078
    },
    {
      "epoch": 0.8984823848238482,
      "step": 16577,
      "training_loss": 7.291207790374756
    },
    {
      "epoch": 0.8985365853658537,
      "step": 16578,
      "training_loss": 5.973812103271484
    },
    {
      "epoch": 0.8985907859078591,
      "step": 16579,
      "training_loss": 7.234479904174805
    },
    {
      "epoch": 0.8986449864498645,
      "grad_norm": 61.89065170288086,
      "learning_rate": 1e-05,
      "loss": 6.8379,
      "step": 16580
    },
    {
      "epoch": 0.8986449864498645,
      "step": 16580,
      "training_loss": 7.569510459899902
    },
    {
      "epoch": 0.8986991869918699,
      "step": 16581,
      "training_loss": 6.785968780517578
    },
    {
      "epoch": 0.8987533875338753,
      "step": 16582,
      "training_loss": 6.8313374519348145
    },
    {
      "epoch": 0.8988075880758808,
      "step": 16583,
      "training_loss": 6.5084547996521
    },
    {
      "epoch": 0.8988617886178861,
      "grad_norm": 48.58668899536133,
      "learning_rate": 1e-05,
      "loss": 6.9238,
      "step": 16584
    },
    {
      "epoch": 0.8988617886178861,
      "step": 16584,
      "training_loss": 6.962806701660156
    },
    {
      "epoch": 0.8989159891598916,
      "step": 16585,
      "training_loss": 6.296700477600098
    },
    {
      "epoch": 0.898970189701897,
      "step": 16586,
      "training_loss": 6.695537567138672
    },
    {
      "epoch": 0.8990243902439025,
      "step": 16587,
      "training_loss": 7.073156833648682
    },
    {
      "epoch": 0.8990785907859079,
      "grad_norm": 22.72551727294922,
      "learning_rate": 1e-05,
      "loss": 6.7571,
      "step": 16588
    },
    {
      "epoch": 0.8990785907859079,
      "step": 16588,
      "training_loss": 6.806288719177246
    },
    {
      "epoch": 0.8991327913279132,
      "step": 16589,
      "training_loss": 5.839554786682129
    },
    {
      "epoch": 0.8991869918699187,
      "step": 16590,
      "training_loss": 6.899106502532959
    },
    {
      "epoch": 0.8992411924119241,
      "step": 16591,
      "training_loss": 7.814542293548584
    },
    {
      "epoch": 0.8992953929539296,
      "grad_norm": 27.60817527770996,
      "learning_rate": 1e-05,
      "loss": 6.8399,
      "step": 16592
    },
    {
      "epoch": 0.8992953929539296,
      "step": 16592,
      "training_loss": 6.717092514038086
    },
    {
      "epoch": 0.8993495934959349,
      "step": 16593,
      "training_loss": 8.43343448638916
    },
    {
      "epoch": 0.8994037940379404,
      "step": 16594,
      "training_loss": 6.435276508331299
    },
    {
      "epoch": 0.8994579945799458,
      "step": 16595,
      "training_loss": 6.0884246826171875
    },
    {
      "epoch": 0.8995121951219512,
      "grad_norm": 25.702821731567383,
      "learning_rate": 1e-05,
      "loss": 6.9186,
      "step": 16596
    },
    {
      "epoch": 0.8995121951219512,
      "step": 16596,
      "training_loss": 6.691293239593506
    },
    {
      "epoch": 0.8995663956639567,
      "step": 16597,
      "training_loss": 6.428099155426025
    },
    {
      "epoch": 0.899620596205962,
      "step": 16598,
      "training_loss": 5.835659503936768
    },
    {
      "epoch": 0.8996747967479675,
      "step": 16599,
      "training_loss": 7.791601181030273
    },
    {
      "epoch": 0.8997289972899729,
      "grad_norm": 39.67918395996094,
      "learning_rate": 1e-05,
      "loss": 6.6867,
      "step": 16600
    },
    {
      "epoch": 0.8997289972899729,
      "step": 16600,
      "training_loss": 6.641873836517334
    },
    {
      "epoch": 0.8997831978319784,
      "step": 16601,
      "training_loss": 6.2415452003479
    },
    {
      "epoch": 0.8998373983739837,
      "step": 16602,
      "training_loss": 3.37241792678833
    },
    {
      "epoch": 0.8998915989159891,
      "step": 16603,
      "training_loss": 8.132497787475586
    },
    {
      "epoch": 0.8999457994579946,
      "grad_norm": 45.009056091308594,
      "learning_rate": 1e-05,
      "loss": 6.0971,
      "step": 16604
    },
    {
      "epoch": 0.8999457994579946,
      "step": 16604,
      "training_loss": 7.090181827545166
    },
    {
      "epoch": 0.9,
      "step": 16605,
      "training_loss": 5.510450839996338
    },
    {
      "epoch": 0.9000542005420055,
      "step": 16606,
      "training_loss": 5.092402935028076
    },
    {
      "epoch": 0.9001084010840108,
      "step": 16607,
      "training_loss": 7.549952983856201
    },
    {
      "epoch": 0.9001626016260162,
      "grad_norm": 26.17081642150879,
      "learning_rate": 1e-05,
      "loss": 6.3107,
      "step": 16608
    },
    {
      "epoch": 0.9001626016260162,
      "step": 16608,
      "training_loss": 8.082834243774414
    },
    {
      "epoch": 0.9002168021680217,
      "step": 16609,
      "training_loss": 5.883973121643066
    },
    {
      "epoch": 0.9002710027100271,
      "step": 16610,
      "training_loss": 7.730072498321533
    },
    {
      "epoch": 0.9003252032520325,
      "step": 16611,
      "training_loss": 4.659149646759033
    },
    {
      "epoch": 0.9003794037940379,
      "grad_norm": 25.383258819580078,
      "learning_rate": 1e-05,
      "loss": 6.589,
      "step": 16612
    },
    {
      "epoch": 0.9003794037940379,
      "step": 16612,
      "training_loss": 6.328218460083008
    },
    {
      "epoch": 0.9004336043360434,
      "step": 16613,
      "training_loss": 4.335010528564453
    },
    {
      "epoch": 0.9004878048780488,
      "step": 16614,
      "training_loss": 7.142117977142334
    },
    {
      "epoch": 0.9005420054200542,
      "step": 16615,
      "training_loss": 6.548407077789307
    },
    {
      "epoch": 0.9005962059620596,
      "grad_norm": 28.874420166015625,
      "learning_rate": 1e-05,
      "loss": 6.0884,
      "step": 16616
    },
    {
      "epoch": 0.9005962059620596,
      "step": 16616,
      "training_loss": 3.9934616088867188
    },
    {
      "epoch": 0.900650406504065,
      "step": 16617,
      "training_loss": 7.529146671295166
    },
    {
      "epoch": 0.9007046070460705,
      "step": 16618,
      "training_loss": 5.077587127685547
    },
    {
      "epoch": 0.9007588075880759,
      "step": 16619,
      "training_loss": 6.527362823486328
    },
    {
      "epoch": 0.9008130081300812,
      "grad_norm": 37.66897964477539,
      "learning_rate": 1e-05,
      "loss": 5.7819,
      "step": 16620
    },
    {
      "epoch": 0.9008130081300812,
      "step": 16620,
      "training_loss": 5.248110771179199
    },
    {
      "epoch": 0.9008672086720867,
      "step": 16621,
      "training_loss": 6.299346446990967
    },
    {
      "epoch": 0.9009214092140921,
      "step": 16622,
      "training_loss": 6.298257350921631
    },
    {
      "epoch": 0.9009756097560976,
      "step": 16623,
      "training_loss": 6.853605270385742
    },
    {
      "epoch": 0.901029810298103,
      "grad_norm": 25.386728286743164,
      "learning_rate": 1e-05,
      "loss": 6.1748,
      "step": 16624
    },
    {
      "epoch": 0.901029810298103,
      "step": 16624,
      "training_loss": 5.320818901062012
    },
    {
      "epoch": 0.9010840108401084,
      "step": 16625,
      "training_loss": 6.262387752532959
    },
    {
      "epoch": 0.9011382113821138,
      "step": 16626,
      "training_loss": 6.97772216796875
    },
    {
      "epoch": 0.9011924119241193,
      "step": 16627,
      "training_loss": 7.747575283050537
    },
    {
      "epoch": 0.9012466124661247,
      "grad_norm": 22.239418029785156,
      "learning_rate": 1e-05,
      "loss": 6.5771,
      "step": 16628
    },
    {
      "epoch": 0.9012466124661247,
      "step": 16628,
      "training_loss": 6.280867576599121
    },
    {
      "epoch": 0.90130081300813,
      "step": 16629,
      "training_loss": 8.026619911193848
    },
    {
      "epoch": 0.9013550135501355,
      "step": 16630,
      "training_loss": 3.870264768600464
    },
    {
      "epoch": 0.9014092140921409,
      "step": 16631,
      "training_loss": 7.652233123779297
    },
    {
      "epoch": 0.9014634146341464,
      "grad_norm": 28.77397918701172,
      "learning_rate": 1e-05,
      "loss": 6.4575,
      "step": 16632
    },
    {
      "epoch": 0.9014634146341464,
      "step": 16632,
      "training_loss": 7.072696685791016
    },
    {
      "epoch": 0.9015176151761518,
      "step": 16633,
      "training_loss": 7.156833171844482
    },
    {
      "epoch": 0.9015718157181571,
      "step": 16634,
      "training_loss": 6.681851387023926
    },
    {
      "epoch": 0.9016260162601626,
      "step": 16635,
      "training_loss": 6.586161136627197
    },
    {
      "epoch": 0.901680216802168,
      "grad_norm": 28.90230941772461,
      "learning_rate": 1e-05,
      "loss": 6.8744,
      "step": 16636
    },
    {
      "epoch": 0.901680216802168,
      "step": 16636,
      "training_loss": 7.502561092376709
    },
    {
      "epoch": 0.9017344173441735,
      "step": 16637,
      "training_loss": 7.543614864349365
    },
    {
      "epoch": 0.9017886178861788,
      "step": 16638,
      "training_loss": 7.359135627746582
    },
    {
      "epoch": 0.9018428184281843,
      "step": 16639,
      "training_loss": 5.841695785522461
    },
    {
      "epoch": 0.9018970189701897,
      "grad_norm": 46.73316955566406,
      "learning_rate": 1e-05,
      "loss": 7.0618,
      "step": 16640
    },
    {
      "epoch": 0.9018970189701897,
      "step": 16640,
      "training_loss": 7.101765155792236
    },
    {
      "epoch": 0.9019512195121951,
      "step": 16641,
      "training_loss": 3.7295281887054443
    },
    {
      "epoch": 0.9020054200542006,
      "step": 16642,
      "training_loss": 3.8247666358947754
    },
    {
      "epoch": 0.9020596205962059,
      "step": 16643,
      "training_loss": 6.414010524749756
    },
    {
      "epoch": 0.9021138211382114,
      "grad_norm": 23.727781295776367,
      "learning_rate": 1e-05,
      "loss": 5.2675,
      "step": 16644
    },
    {
      "epoch": 0.9021138211382114,
      "step": 16644,
      "training_loss": 6.892303466796875
    },
    {
      "epoch": 0.9021680216802168,
      "step": 16645,
      "training_loss": 6.73618745803833
    },
    {
      "epoch": 0.9022222222222223,
      "step": 16646,
      "training_loss": 8.00174331665039
    },
    {
      "epoch": 0.9022764227642276,
      "step": 16647,
      "training_loss": 7.784207344055176
    },
    {
      "epoch": 0.902330623306233,
      "grad_norm": 34.9200325012207,
      "learning_rate": 1e-05,
      "loss": 7.3536,
      "step": 16648
    },
    {
      "epoch": 0.902330623306233,
      "step": 16648,
      "training_loss": 5.601866245269775
    },
    {
      "epoch": 0.9023848238482385,
      "step": 16649,
      "training_loss": 5.911288261413574
    },
    {
      "epoch": 0.9024390243902439,
      "step": 16650,
      "training_loss": 7.643125057220459
    },
    {
      "epoch": 0.9024932249322494,
      "step": 16651,
      "training_loss": 5.459246635437012
    },
    {
      "epoch": 0.9025474254742547,
      "grad_norm": 28.31970977783203,
      "learning_rate": 1e-05,
      "loss": 6.1539,
      "step": 16652
    },
    {
      "epoch": 0.9025474254742547,
      "step": 16652,
      "training_loss": 6.746822357177734
    },
    {
      "epoch": 0.9026016260162602,
      "step": 16653,
      "training_loss": 9.077452659606934
    },
    {
      "epoch": 0.9026558265582656,
      "step": 16654,
      "training_loss": 6.560684680938721
    },
    {
      "epoch": 0.902710027100271,
      "step": 16655,
      "training_loss": 6.695988655090332
    },
    {
      "epoch": 0.9027642276422764,
      "grad_norm": 31.51762580871582,
      "learning_rate": 1e-05,
      "loss": 7.2702,
      "step": 16656
    },
    {
      "epoch": 0.9027642276422764,
      "step": 16656,
      "training_loss": 7.430135250091553
    },
    {
      "epoch": 0.9028184281842818,
      "step": 16657,
      "training_loss": 5.93319845199585
    },
    {
      "epoch": 0.9028726287262873,
      "step": 16658,
      "training_loss": 7.522727012634277
    },
    {
      "epoch": 0.9029268292682927,
      "step": 16659,
      "training_loss": 8.09091567993164
    },
    {
      "epoch": 0.9029810298102982,
      "grad_norm": 26.233449935913086,
      "learning_rate": 1e-05,
      "loss": 7.2442,
      "step": 16660
    },
    {
      "epoch": 0.9029810298102982,
      "step": 16660,
      "training_loss": 6.0961527824401855
    },
    {
      "epoch": 0.9030352303523035,
      "step": 16661,
      "training_loss": 5.34128475189209
    },
    {
      "epoch": 0.9030894308943089,
      "step": 16662,
      "training_loss": 4.885653495788574
    },
    {
      "epoch": 0.9031436314363144,
      "step": 16663,
      "training_loss": 5.666031837463379
    },
    {
      "epoch": 0.9031978319783198,
      "grad_norm": 32.1547737121582,
      "learning_rate": 1e-05,
      "loss": 5.4973,
      "step": 16664
    },
    {
      "epoch": 0.9031978319783198,
      "step": 16664,
      "training_loss": 6.507637023925781
    },
    {
      "epoch": 0.9032520325203252,
      "step": 16665,
      "training_loss": 7.272566795349121
    },
    {
      "epoch": 0.9033062330623306,
      "step": 16666,
      "training_loss": 6.099066257476807
    },
    {
      "epoch": 0.903360433604336,
      "step": 16667,
      "training_loss": 6.890387058258057
    },
    {
      "epoch": 0.9034146341463415,
      "grad_norm": 32.87541198730469,
      "learning_rate": 1e-05,
      "loss": 6.6924,
      "step": 16668
    },
    {
      "epoch": 0.9034146341463415,
      "step": 16668,
      "training_loss": 7.432788848876953
    },
    {
      "epoch": 0.9034688346883469,
      "step": 16669,
      "training_loss": 6.781774520874023
    },
    {
      "epoch": 0.9035230352303523,
      "step": 16670,
      "training_loss": 5.9643874168396
    },
    {
      "epoch": 0.9035772357723577,
      "step": 16671,
      "training_loss": 6.006577014923096
    },
    {
      "epoch": 0.9036314363143632,
      "grad_norm": 34.23926544189453,
      "learning_rate": 1e-05,
      "loss": 6.5464,
      "step": 16672
    },
    {
      "epoch": 0.9036314363143632,
      "step": 16672,
      "training_loss": 7.874899864196777
    },
    {
      "epoch": 0.9036856368563686,
      "step": 16673,
      "training_loss": 7.369104862213135
    },
    {
      "epoch": 0.9037398373983739,
      "step": 16674,
      "training_loss": 5.505077362060547
    },
    {
      "epoch": 0.9037940379403794,
      "step": 16675,
      "training_loss": 5.737734794616699
    },
    {
      "epoch": 0.9038482384823848,
      "grad_norm": 26.408662796020508,
      "learning_rate": 1e-05,
      "loss": 6.6217,
      "step": 16676
    },
    {
      "epoch": 0.9038482384823848,
      "step": 16676,
      "training_loss": 5.939069747924805
    },
    {
      "epoch": 0.9039024390243903,
      "step": 16677,
      "training_loss": 6.0455756187438965
    },
    {
      "epoch": 0.9039566395663957,
      "step": 16678,
      "training_loss": 6.605981349945068
    },
    {
      "epoch": 0.904010840108401,
      "step": 16679,
      "training_loss": 7.130182266235352
    },
    {
      "epoch": 0.9040650406504065,
      "grad_norm": 16.225704193115234,
      "learning_rate": 1e-05,
      "loss": 6.4302,
      "step": 16680
    },
    {
      "epoch": 0.9040650406504065,
      "step": 16680,
      "training_loss": 7.276819705963135
    },
    {
      "epoch": 0.9041192411924119,
      "step": 16681,
      "training_loss": 7.478801727294922
    },
    {
      "epoch": 0.9041734417344174,
      "step": 16682,
      "training_loss": 2.828320026397705
    },
    {
      "epoch": 0.9042276422764227,
      "step": 16683,
      "training_loss": 7.220242977142334
    },
    {
      "epoch": 0.9042818428184282,
      "grad_norm": 25.88383674621582,
      "learning_rate": 1e-05,
      "loss": 6.201,
      "step": 16684
    },
    {
      "epoch": 0.9042818428184282,
      "step": 16684,
      "training_loss": 5.99154806137085
    },
    {
      "epoch": 0.9043360433604336,
      "step": 16685,
      "training_loss": 7.140707015991211
    },
    {
      "epoch": 0.904390243902439,
      "step": 16686,
      "training_loss": 5.933841705322266
    },
    {
      "epoch": 0.9044444444444445,
      "step": 16687,
      "training_loss": 5.859370708465576
    },
    {
      "epoch": 0.9044986449864498,
      "grad_norm": 42.367950439453125,
      "learning_rate": 1e-05,
      "loss": 6.2314,
      "step": 16688
    },
    {
      "epoch": 0.9044986449864498,
      "step": 16688,
      "training_loss": 8.75389289855957
    },
    {
      "epoch": 0.9045528455284553,
      "step": 16689,
      "training_loss": 7.33616828918457
    },
    {
      "epoch": 0.9046070460704607,
      "step": 16690,
      "training_loss": 5.243429660797119
    },
    {
      "epoch": 0.9046612466124662,
      "step": 16691,
      "training_loss": 6.231579303741455
    },
    {
      "epoch": 0.9047154471544715,
      "grad_norm": 19.45160484313965,
      "learning_rate": 1e-05,
      "loss": 6.8913,
      "step": 16692
    },
    {
      "epoch": 0.9047154471544715,
      "step": 16692,
      "training_loss": 9.067831039428711
    },
    {
      "epoch": 0.9047696476964769,
      "step": 16693,
      "training_loss": 6.316452503204346
    },
    {
      "epoch": 0.9048238482384824,
      "step": 16694,
      "training_loss": 6.6158552169799805
    },
    {
      "epoch": 0.9048780487804878,
      "step": 16695,
      "training_loss": 6.491904258728027
    },
    {
      "epoch": 0.9049322493224933,
      "grad_norm": 28.866376876831055,
      "learning_rate": 1e-05,
      "loss": 7.123,
      "step": 16696
    },
    {
      "epoch": 0.9049322493224933,
      "step": 16696,
      "training_loss": 6.187151908874512
    },
    {
      "epoch": 0.9049864498644986,
      "step": 16697,
      "training_loss": 7.288007736206055
    },
    {
      "epoch": 0.905040650406504,
      "step": 16698,
      "training_loss": 7.202718734741211
    },
    {
      "epoch": 0.9050948509485095,
      "step": 16699,
      "training_loss": 7.189072608947754
    },
    {
      "epoch": 0.9051490514905149,
      "grad_norm": 34.81140899658203,
      "learning_rate": 1e-05,
      "loss": 6.9667,
      "step": 16700
    },
    {
      "epoch": 0.9051490514905149,
      "step": 16700,
      "training_loss": 7.179917812347412
    },
    {
      "epoch": 0.9052032520325203,
      "step": 16701,
      "training_loss": 5.902700901031494
    },
    {
      "epoch": 0.9052574525745257,
      "step": 16702,
      "training_loss": 6.877492904663086
    },
    {
      "epoch": 0.9053116531165312,
      "step": 16703,
      "training_loss": 7.391958236694336
    },
    {
      "epoch": 0.9053658536585366,
      "grad_norm": 20.711185455322266,
      "learning_rate": 1e-05,
      "loss": 6.838,
      "step": 16704
    },
    {
      "epoch": 0.9053658536585366,
      "step": 16704,
      "training_loss": 3.8251054286956787
    },
    {
      "epoch": 0.9054200542005421,
      "step": 16705,
      "training_loss": 5.283656120300293
    },
    {
      "epoch": 0.9054742547425474,
      "step": 16706,
      "training_loss": 7.776988983154297
    },
    {
      "epoch": 0.9055284552845528,
      "step": 16707,
      "training_loss": 5.136696815490723
    },
    {
      "epoch": 0.9055826558265583,
      "grad_norm": 20.933029174804688,
      "learning_rate": 1e-05,
      "loss": 5.5056,
      "step": 16708
    },
    {
      "epoch": 0.9055826558265583,
      "step": 16708,
      "training_loss": 6.570008754730225
    },
    {
      "epoch": 0.9056368563685637,
      "step": 16709,
      "training_loss": 6.107767581939697
    },
    {
      "epoch": 0.9056910569105691,
      "step": 16710,
      "training_loss": 6.235376358032227
    },
    {
      "epoch": 0.9057452574525745,
      "step": 16711,
      "training_loss": 6.557056427001953
    },
    {
      "epoch": 0.90579945799458,
      "grad_norm": 22.335561752319336,
      "learning_rate": 1e-05,
      "loss": 6.3676,
      "step": 16712
    },
    {
      "epoch": 0.90579945799458,
      "step": 16712,
      "training_loss": 6.609952926635742
    },
    {
      "epoch": 0.9058536585365854,
      "step": 16713,
      "training_loss": 6.878112316131592
    },
    {
      "epoch": 0.9059078590785908,
      "step": 16714,
      "training_loss": 6.527894496917725
    },
    {
      "epoch": 0.9059620596205962,
      "step": 16715,
      "training_loss": 6.194387435913086
    },
    {
      "epoch": 0.9060162601626016,
      "grad_norm": 21.57105827331543,
      "learning_rate": 1e-05,
      "loss": 6.5526,
      "step": 16716
    },
    {
      "epoch": 0.9060162601626016,
      "step": 16716,
      "training_loss": 5.5861406326293945
    },
    {
      "epoch": 0.9060704607046071,
      "step": 16717,
      "training_loss": 7.311350345611572
    },
    {
      "epoch": 0.9061246612466125,
      "step": 16718,
      "training_loss": 4.764297962188721
    },
    {
      "epoch": 0.9061788617886178,
      "step": 16719,
      "training_loss": 4.248229026794434
    },
    {
      "epoch": 0.9062330623306233,
      "grad_norm": 31.703832626342773,
      "learning_rate": 1e-05,
      "loss": 5.4775,
      "step": 16720
    },
    {
      "epoch": 0.9062330623306233,
      "step": 16720,
      "training_loss": 6.688888072967529
    },
    {
      "epoch": 0.9062872628726287,
      "step": 16721,
      "training_loss": 5.709938049316406
    },
    {
      "epoch": 0.9063414634146342,
      "step": 16722,
      "training_loss": 7.607216835021973
    },
    {
      "epoch": 0.9063956639566396,
      "step": 16723,
      "training_loss": 7.94998025894165
    },
    {
      "epoch": 0.906449864498645,
      "grad_norm": 40.12266540527344,
      "learning_rate": 1e-05,
      "loss": 6.989,
      "step": 16724
    },
    {
      "epoch": 0.906449864498645,
      "step": 16724,
      "training_loss": 8.040271759033203
    },
    {
      "epoch": 0.9065040650406504,
      "step": 16725,
      "training_loss": 6.956982135772705
    },
    {
      "epoch": 0.9065582655826558,
      "step": 16726,
      "training_loss": 6.841942310333252
    },
    {
      "epoch": 0.9066124661246613,
      "step": 16727,
      "training_loss": 6.67357063293457
    },
    {
      "epoch": 0.9066666666666666,
      "grad_norm": 20.47290802001953,
      "learning_rate": 1e-05,
      "loss": 7.1282,
      "step": 16728
    },
    {
      "epoch": 0.9066666666666666,
      "step": 16728,
      "training_loss": 6.415393829345703
    },
    {
      "epoch": 0.9067208672086721,
      "step": 16729,
      "training_loss": 3.0819754600524902
    },
    {
      "epoch": 0.9067750677506775,
      "step": 16730,
      "training_loss": 6.300848960876465
    },
    {
      "epoch": 0.906829268292683,
      "step": 16731,
      "training_loss": 5.585728168487549
    },
    {
      "epoch": 0.9068834688346884,
      "grad_norm": 50.40637969970703,
      "learning_rate": 1e-05,
      "loss": 5.346,
      "step": 16732
    },
    {
      "epoch": 0.9068834688346884,
      "step": 16732,
      "training_loss": 6.5243048667907715
    },
    {
      "epoch": 0.9069376693766937,
      "step": 16733,
      "training_loss": 5.786112308502197
    },
    {
      "epoch": 0.9069918699186992,
      "step": 16734,
      "training_loss": 6.315769195556641
    },
    {
      "epoch": 0.9070460704607046,
      "step": 16735,
      "training_loss": 6.1713643074035645
    },
    {
      "epoch": 0.9071002710027101,
      "grad_norm": 17.314956665039062,
      "learning_rate": 1e-05,
      "loss": 6.1994,
      "step": 16736
    },
    {
      "epoch": 0.9071002710027101,
      "step": 16736,
      "training_loss": 6.771953582763672
    },
    {
      "epoch": 0.9071544715447154,
      "step": 16737,
      "training_loss": 6.613911151885986
    },
    {
      "epoch": 0.9072086720867208,
      "step": 16738,
      "training_loss": 6.836765766143799
    },
    {
      "epoch": 0.9072628726287263,
      "step": 16739,
      "training_loss": 8.013347625732422
    },
    {
      "epoch": 0.9073170731707317,
      "grad_norm": 37.414222717285156,
      "learning_rate": 1e-05,
      "loss": 7.059,
      "step": 16740
    },
    {
      "epoch": 0.9073170731707317,
      "step": 16740,
      "training_loss": 6.530753135681152
    },
    {
      "epoch": 0.9073712737127372,
      "step": 16741,
      "training_loss": 6.635166168212891
    },
    {
      "epoch": 0.9074254742547425,
      "step": 16742,
      "training_loss": 6.9618706703186035
    },
    {
      "epoch": 0.907479674796748,
      "step": 16743,
      "training_loss": 7.33627462387085
    },
    {
      "epoch": 0.9075338753387534,
      "grad_norm": 24.292564392089844,
      "learning_rate": 1e-05,
      "loss": 6.866,
      "step": 16744
    },
    {
      "epoch": 0.9075338753387534,
      "step": 16744,
      "training_loss": 3.5542120933532715
    },
    {
      "epoch": 0.9075880758807588,
      "step": 16745,
      "training_loss": 6.679228782653809
    },
    {
      "epoch": 0.9076422764227642,
      "step": 16746,
      "training_loss": 7.34606409072876
    },
    {
      "epoch": 0.9076964769647696,
      "step": 16747,
      "training_loss": 6.590019226074219
    },
    {
      "epoch": 0.9077506775067751,
      "grad_norm": 35.441654205322266,
      "learning_rate": 1e-05,
      "loss": 6.0424,
      "step": 16748
    },
    {
      "epoch": 0.9077506775067751,
      "step": 16748,
      "training_loss": 7.226239204406738
    },
    {
      "epoch": 0.9078048780487805,
      "step": 16749,
      "training_loss": 6.18141508102417
    },
    {
      "epoch": 0.907859078590786,
      "step": 16750,
      "training_loss": 7.7769012451171875
    },
    {
      "epoch": 0.9079132791327913,
      "step": 16751,
      "training_loss": 5.390125274658203
    },
    {
      "epoch": 0.9079674796747967,
      "grad_norm": 42.24040222167969,
      "learning_rate": 1e-05,
      "loss": 6.6437,
      "step": 16752
    },
    {
      "epoch": 0.9079674796747967,
      "step": 16752,
      "training_loss": 5.49361515045166
    },
    {
      "epoch": 0.9080216802168022,
      "step": 16753,
      "training_loss": 7.88268518447876
    },
    {
      "epoch": 0.9080758807588076,
      "step": 16754,
      "training_loss": 5.853958606719971
    },
    {
      "epoch": 0.908130081300813,
      "step": 16755,
      "training_loss": 6.894908428192139
    },
    {
      "epoch": 0.9081842818428184,
      "grad_norm": 17.055835723876953,
      "learning_rate": 1e-05,
      "loss": 6.5313,
      "step": 16756
    },
    {
      "epoch": 0.9081842818428184,
      "step": 16756,
      "training_loss": 6.775146961212158
    },
    {
      "epoch": 0.9082384823848239,
      "step": 16757,
      "training_loss": 6.688885688781738
    },
    {
      "epoch": 0.9082926829268293,
      "step": 16758,
      "training_loss": 6.477871894836426
    },
    {
      "epoch": 0.9083468834688347,
      "step": 16759,
      "training_loss": 6.482822895050049
    },
    {
      "epoch": 0.9084010840108401,
      "grad_norm": 18.603527069091797,
      "learning_rate": 1e-05,
      "loss": 6.6062,
      "step": 16760
    },
    {
      "epoch": 0.9084010840108401,
      "step": 16760,
      "training_loss": 6.662633419036865
    },
    {
      "epoch": 0.9084552845528455,
      "step": 16761,
      "training_loss": 6.460174083709717
    },
    {
      "epoch": 0.908509485094851,
      "step": 16762,
      "training_loss": 3.5232367515563965
    },
    {
      "epoch": 0.9085636856368564,
      "step": 16763,
      "training_loss": 6.164920806884766
    },
    {
      "epoch": 0.9086178861788617,
      "grad_norm": 32.20292663574219,
      "learning_rate": 1e-05,
      "loss": 5.7027,
      "step": 16764
    },
    {
      "epoch": 0.9086178861788617,
      "step": 16764,
      "training_loss": 6.78637170791626
    },
    {
      "epoch": 0.9086720867208672,
      "step": 16765,
      "training_loss": 7.086691379547119
    },
    {
      "epoch": 0.9087262872628726,
      "step": 16766,
      "training_loss": 6.4643378257751465
    },
    {
      "epoch": 0.9087804878048781,
      "step": 16767,
      "training_loss": 6.702589511871338
    },
    {
      "epoch": 0.9088346883468835,
      "grad_norm": 25.741262435913086,
      "learning_rate": 1e-05,
      "loss": 6.76,
      "step": 16768
    },
    {
      "epoch": 0.9088346883468835,
      "step": 16768,
      "training_loss": 5.30582332611084
    },
    {
      "epoch": 0.9088888888888889,
      "step": 16769,
      "training_loss": 6.739721298217773
    },
    {
      "epoch": 0.9089430894308943,
      "step": 16770,
      "training_loss": 6.283294677734375
    },
    {
      "epoch": 0.9089972899728997,
      "step": 16771,
      "training_loss": 6.257465839385986
    },
    {
      "epoch": 0.9090514905149052,
      "grad_norm": 25.175600051879883,
      "learning_rate": 1e-05,
      "loss": 6.1466,
      "step": 16772
    },
    {
      "epoch": 0.9090514905149052,
      "step": 16772,
      "training_loss": 7.31358003616333
    },
    {
      "epoch": 0.9091056910569105,
      "step": 16773,
      "training_loss": 6.982306957244873
    },
    {
      "epoch": 0.909159891598916,
      "step": 16774,
      "training_loss": 5.437374591827393
    },
    {
      "epoch": 0.9092140921409214,
      "step": 16775,
      "training_loss": 7.020493984222412
    },
    {
      "epoch": 0.9092682926829269,
      "grad_norm": 29.418384552001953,
      "learning_rate": 1e-05,
      "loss": 6.6884,
      "step": 16776
    },
    {
      "epoch": 0.9092682926829269,
      "step": 16776,
      "training_loss": 5.241882801055908
    },
    {
      "epoch": 0.9093224932249323,
      "step": 16777,
      "training_loss": 6.2129435539245605
    },
    {
      "epoch": 0.9093766937669376,
      "step": 16778,
      "training_loss": 6.616865158081055
    },
    {
      "epoch": 0.9094308943089431,
      "step": 16779,
      "training_loss": 4.847043991088867
    },
    {
      "epoch": 0.9094850948509485,
      "grad_norm": 29.048307418823242,
      "learning_rate": 1e-05,
      "loss": 5.7297,
      "step": 16780
    },
    {
      "epoch": 0.9094850948509485,
      "step": 16780,
      "training_loss": 4.591211318969727
    },
    {
      "epoch": 0.909539295392954,
      "step": 16781,
      "training_loss": 5.891852855682373
    },
    {
      "epoch": 0.9095934959349593,
      "step": 16782,
      "training_loss": 7.714519500732422
    },
    {
      "epoch": 0.9096476964769648,
      "step": 16783,
      "training_loss": 5.627232551574707
    },
    {
      "epoch": 0.9097018970189702,
      "grad_norm": 30.45087242126465,
      "learning_rate": 1e-05,
      "loss": 5.9562,
      "step": 16784
    },
    {
      "epoch": 0.9097018970189702,
      "step": 16784,
      "training_loss": 4.131388187408447
    },
    {
      "epoch": 0.9097560975609756,
      "step": 16785,
      "training_loss": 7.282581329345703
    },
    {
      "epoch": 0.909810298102981,
      "step": 16786,
      "training_loss": 7.145346641540527
    },
    {
      "epoch": 0.9098644986449864,
      "step": 16787,
      "training_loss": 6.337430000305176
    },
    {
      "epoch": 0.9099186991869919,
      "grad_norm": 32.60314178466797,
      "learning_rate": 1e-05,
      "loss": 6.2242,
      "step": 16788
    },
    {
      "epoch": 0.9099186991869919,
      "step": 16788,
      "training_loss": 7.755008697509766
    },
    {
      "epoch": 0.9099728997289973,
      "step": 16789,
      "training_loss": 5.315670967102051
    },
    {
      "epoch": 0.9100271002710028,
      "step": 16790,
      "training_loss": 5.74611759185791
    },
    {
      "epoch": 0.9100813008130081,
      "step": 16791,
      "training_loss": 6.184192657470703
    },
    {
      "epoch": 0.9101355013550135,
      "grad_norm": 29.546098709106445,
      "learning_rate": 1e-05,
      "loss": 6.2502,
      "step": 16792
    },
    {
      "epoch": 0.9101355013550135,
      "step": 16792,
      "training_loss": 6.7076592445373535
    },
    {
      "epoch": 0.910189701897019,
      "step": 16793,
      "training_loss": 7.263617992401123
    },
    {
      "epoch": 0.9102439024390244,
      "step": 16794,
      "training_loss": 6.0225653648376465
    },
    {
      "epoch": 0.9102981029810298,
      "step": 16795,
      "training_loss": 5.441070079803467
    },
    {
      "epoch": 0.9103523035230352,
      "grad_norm": 42.607635498046875,
      "learning_rate": 1e-05,
      "loss": 6.3587,
      "step": 16796
    },
    {
      "epoch": 0.9103523035230352,
      "step": 16796,
      "training_loss": 7.957615852355957
    },
    {
      "epoch": 0.9104065040650406,
      "step": 16797,
      "training_loss": 7.45850133895874
    },
    {
      "epoch": 0.9104607046070461,
      "step": 16798,
      "training_loss": 6.261051177978516
    },
    {
      "epoch": 0.9105149051490515,
      "step": 16799,
      "training_loss": 6.690618991851807
    },
    {
      "epoch": 0.9105691056910569,
      "grad_norm": 38.903141021728516,
      "learning_rate": 1e-05,
      "loss": 7.0919,
      "step": 16800
    },
    {
      "epoch": 0.9105691056910569,
      "step": 16800,
      "training_loss": 8.795114517211914
    },
    {
      "epoch": 0.9106233062330623,
      "step": 16801,
      "training_loss": 6.2548651695251465
    },
    {
      "epoch": 0.9106775067750678,
      "step": 16802,
      "training_loss": 4.912687301635742
    },
    {
      "epoch": 0.9107317073170732,
      "step": 16803,
      "training_loss": 7.784730434417725
    },
    {
      "epoch": 0.9107859078590785,
      "grad_norm": 34.312828063964844,
      "learning_rate": 1e-05,
      "loss": 6.9368,
      "step": 16804
    },
    {
      "epoch": 0.9107859078590785,
      "step": 16804,
      "training_loss": 6.095121383666992
    },
    {
      "epoch": 0.910840108401084,
      "step": 16805,
      "training_loss": 7.484464645385742
    },
    {
      "epoch": 0.9108943089430894,
      "step": 16806,
      "training_loss": 6.545603275299072
    },
    {
      "epoch": 0.9109485094850949,
      "step": 16807,
      "training_loss": 7.649420261383057
    },
    {
      "epoch": 0.9110027100271003,
      "grad_norm": 38.65251922607422,
      "learning_rate": 1e-05,
      "loss": 6.9437,
      "step": 16808
    },
    {
      "epoch": 0.9110027100271003,
      "step": 16808,
      "training_loss": 5.855302333831787
    },
    {
      "epoch": 0.9110569105691056,
      "step": 16809,
      "training_loss": 7.236356735229492
    },
    {
      "epoch": 0.9111111111111111,
      "step": 16810,
      "training_loss": 6.355457305908203
    },
    {
      "epoch": 0.9111653116531165,
      "step": 16811,
      "training_loss": 5.944356441497803
    },
    {
      "epoch": 0.911219512195122,
      "grad_norm": 23.8554630279541,
      "learning_rate": 1e-05,
      "loss": 6.3479,
      "step": 16812
    },
    {
      "epoch": 0.911219512195122,
      "step": 16812,
      "training_loss": 8.293457984924316
    },
    {
      "epoch": 0.9112737127371273,
      "step": 16813,
      "training_loss": 7.592185974121094
    },
    {
      "epoch": 0.9113279132791328,
      "step": 16814,
      "training_loss": 6.788720607757568
    },
    {
      "epoch": 0.9113821138211382,
      "step": 16815,
      "training_loss": 7.138548374176025
    },
    {
      "epoch": 0.9114363143631437,
      "grad_norm": 29.584178924560547,
      "learning_rate": 1e-05,
      "loss": 7.4532,
      "step": 16816
    },
    {
      "epoch": 0.9114363143631437,
      "step": 16816,
      "training_loss": 5.986957550048828
    },
    {
      "epoch": 0.9114905149051491,
      "step": 16817,
      "training_loss": 7.053474426269531
    },
    {
      "epoch": 0.9115447154471544,
      "step": 16818,
      "training_loss": 5.055306911468506
    },
    {
      "epoch": 0.9115989159891599,
      "step": 16819,
      "training_loss": 7.624694347381592
    },
    {
      "epoch": 0.9116531165311653,
      "grad_norm": 21.230504989624023,
      "learning_rate": 1e-05,
      "loss": 6.4301,
      "step": 16820
    },
    {
      "epoch": 0.9116531165311653,
      "step": 16820,
      "training_loss": 6.199483871459961
    },
    {
      "epoch": 0.9117073170731708,
      "step": 16821,
      "training_loss": 5.768121242523193
    },
    {
      "epoch": 0.9117615176151761,
      "step": 16822,
      "training_loss": 6.549615859985352
    },
    {
      "epoch": 0.9118157181571815,
      "step": 16823,
      "training_loss": 4.481748104095459
    },
    {
      "epoch": 0.911869918699187,
      "grad_norm": 21.52135467529297,
      "learning_rate": 1e-05,
      "loss": 5.7497,
      "step": 16824
    },
    {
      "epoch": 0.911869918699187,
      "step": 16824,
      "training_loss": 6.917524337768555
    },
    {
      "epoch": 0.9119241192411924,
      "step": 16825,
      "training_loss": 6.068938732147217
    },
    {
      "epoch": 0.9119783197831979,
      "step": 16826,
      "training_loss": 7.808708190917969
    },
    {
      "epoch": 0.9120325203252032,
      "step": 16827,
      "training_loss": 7.094460487365723
    },
    {
      "epoch": 0.9120867208672087,
      "grad_norm": 20.72989273071289,
      "learning_rate": 1e-05,
      "loss": 6.9724,
      "step": 16828
    },
    {
      "epoch": 0.9120867208672087,
      "step": 16828,
      "training_loss": 7.858661651611328
    },
    {
      "epoch": 0.9121409214092141,
      "step": 16829,
      "training_loss": 5.528810501098633
    },
    {
      "epoch": 0.9121951219512195,
      "step": 16830,
      "training_loss": 7.204341411590576
    },
    {
      "epoch": 0.9122493224932249,
      "step": 16831,
      "training_loss": 7.326541900634766
    },
    {
      "epoch": 0.9123035230352303,
      "grad_norm": 24.917890548706055,
      "learning_rate": 1e-05,
      "loss": 6.9796,
      "step": 16832
    },
    {
      "epoch": 0.9123035230352303,
      "step": 16832,
      "training_loss": 7.989813327789307
    },
    {
      "epoch": 0.9123577235772358,
      "step": 16833,
      "training_loss": 6.715090274810791
    },
    {
      "epoch": 0.9124119241192412,
      "step": 16834,
      "training_loss": 6.836978435516357
    },
    {
      "epoch": 0.9124661246612467,
      "step": 16835,
      "training_loss": 7.15551233291626
    },
    {
      "epoch": 0.912520325203252,
      "grad_norm": 23.530303955078125,
      "learning_rate": 1e-05,
      "loss": 7.1743,
      "step": 16836
    },
    {
      "epoch": 0.912520325203252,
      "step": 16836,
      "training_loss": 6.265374183654785
    },
    {
      "epoch": 0.9125745257452574,
      "step": 16837,
      "training_loss": 6.587026596069336
    },
    {
      "epoch": 0.9126287262872629,
      "step": 16838,
      "training_loss": 5.471649646759033
    },
    {
      "epoch": 0.9126829268292683,
      "step": 16839,
      "training_loss": 6.250788688659668
    },
    {
      "epoch": 0.9127371273712737,
      "grad_norm": 41.483062744140625,
      "learning_rate": 1e-05,
      "loss": 6.1437,
      "step": 16840
    },
    {
      "epoch": 0.9127371273712737,
      "step": 16840,
      "training_loss": 6.315256595611572
    },
    {
      "epoch": 0.9127913279132791,
      "step": 16841,
      "training_loss": 5.345874786376953
    },
    {
      "epoch": 0.9128455284552845,
      "step": 16842,
      "training_loss": 5.289865970611572
    },
    {
      "epoch": 0.91289972899729,
      "step": 16843,
      "training_loss": 6.900543689727783
    },
    {
      "epoch": 0.9129539295392954,
      "grad_norm": 21.214649200439453,
      "learning_rate": 1e-05,
      "loss": 5.9629,
      "step": 16844
    },
    {
      "epoch": 0.9129539295392954,
      "step": 16844,
      "training_loss": 5.901381492614746
    },
    {
      "epoch": 0.9130081300813008,
      "step": 16845,
      "training_loss": 6.886768817901611
    },
    {
      "epoch": 0.9130623306233062,
      "step": 16846,
      "training_loss": 6.900185585021973
    },
    {
      "epoch": 0.9131165311653117,
      "step": 16847,
      "training_loss": 7.449020862579346
    },
    {
      "epoch": 0.9131707317073171,
      "grad_norm": 29.17604637145996,
      "learning_rate": 1e-05,
      "loss": 6.7843,
      "step": 16848
    },
    {
      "epoch": 0.9131707317073171,
      "step": 16848,
      "training_loss": 7.462212562561035
    },
    {
      "epoch": 0.9132249322493224,
      "step": 16849,
      "training_loss": 5.588354587554932
    },
    {
      "epoch": 0.9132791327913279,
      "step": 16850,
      "training_loss": 6.097896099090576
    },
    {
      "epoch": 0.9133333333333333,
      "step": 16851,
      "training_loss": 7.027945518493652
    },
    {
      "epoch": 0.9133875338753388,
      "grad_norm": 22.02553939819336,
      "learning_rate": 1e-05,
      "loss": 6.5441,
      "step": 16852
    },
    {
      "epoch": 0.9133875338753388,
      "step": 16852,
      "training_loss": 7.700755596160889
    },
    {
      "epoch": 0.9134417344173442,
      "step": 16853,
      "training_loss": 7.656533718109131
    },
    {
      "epoch": 0.9134959349593496,
      "step": 16854,
      "training_loss": 5.288459777832031
    },
    {
      "epoch": 0.913550135501355,
      "step": 16855,
      "training_loss": 7.102248191833496
    },
    {
      "epoch": 0.9136043360433604,
      "grad_norm": 20.43976593017578,
      "learning_rate": 1e-05,
      "loss": 6.937,
      "step": 16856
    },
    {
      "epoch": 0.9136043360433604,
      "step": 16856,
      "training_loss": 6.903355598449707
    },
    {
      "epoch": 0.9136585365853659,
      "step": 16857,
      "training_loss": 7.118232727050781
    },
    {
      "epoch": 0.9137127371273712,
      "step": 16858,
      "training_loss": 6.443395137786865
    },
    {
      "epoch": 0.9137669376693767,
      "step": 16859,
      "training_loss": 6.89707088470459
    },
    {
      "epoch": 0.9138211382113821,
      "grad_norm": 25.713369369506836,
      "learning_rate": 1e-05,
      "loss": 6.8405,
      "step": 16860
    },
    {
      "epoch": 0.9138211382113821,
      "step": 16860,
      "training_loss": 6.739502429962158
    },
    {
      "epoch": 0.9138753387533876,
      "step": 16861,
      "training_loss": 7.60279655456543
    },
    {
      "epoch": 0.913929539295393,
      "step": 16862,
      "training_loss": 6.675322532653809
    },
    {
      "epoch": 0.9139837398373983,
      "step": 16863,
      "training_loss": 6.255932807922363
    },
    {
      "epoch": 0.9140379403794038,
      "grad_norm": 24.34580421447754,
      "learning_rate": 1e-05,
      "loss": 6.8184,
      "step": 16864
    },
    {
      "epoch": 0.9140379403794038,
      "step": 16864,
      "training_loss": 6.988887786865234
    },
    {
      "epoch": 0.9140921409214092,
      "step": 16865,
      "training_loss": 7.106143951416016
    },
    {
      "epoch": 0.9141463414634147,
      "step": 16866,
      "training_loss": 6.628423690795898
    },
    {
      "epoch": 0.91420054200542,
      "step": 16867,
      "training_loss": 6.245092868804932
    },
    {
      "epoch": 0.9142547425474254,
      "grad_norm": 19.523700714111328,
      "learning_rate": 1e-05,
      "loss": 6.7421,
      "step": 16868
    },
    {
      "epoch": 0.9142547425474254,
      "step": 16868,
      "training_loss": 4.241322994232178
    },
    {
      "epoch": 0.9143089430894309,
      "step": 16869,
      "training_loss": 5.727212429046631
    },
    {
      "epoch": 0.9143631436314363,
      "step": 16870,
      "training_loss": 6.854129791259766
    },
    {
      "epoch": 0.9144173441734418,
      "step": 16871,
      "training_loss": 6.802787780761719
    },
    {
      "epoch": 0.9144715447154471,
      "grad_norm": 21.253475189208984,
      "learning_rate": 1e-05,
      "loss": 5.9064,
      "step": 16872
    },
    {
      "epoch": 0.9144715447154471,
      "step": 16872,
      "training_loss": 6.511745452880859
    },
    {
      "epoch": 0.9145257452574526,
      "step": 16873,
      "training_loss": 4.255340576171875
    },
    {
      "epoch": 0.914579945799458,
      "step": 16874,
      "training_loss": 6.898792266845703
    },
    {
      "epoch": 0.9146341463414634,
      "step": 16875,
      "training_loss": 6.765801429748535
    },
    {
      "epoch": 0.9146883468834688,
      "grad_norm": 31.02202033996582,
      "learning_rate": 1e-05,
      "loss": 6.1079,
      "step": 16876
    },
    {
      "epoch": 0.9146883468834688,
      "step": 16876,
      "training_loss": 4.243108749389648
    },
    {
      "epoch": 0.9147425474254742,
      "step": 16877,
      "training_loss": 6.661023139953613
    },
    {
      "epoch": 0.9147967479674797,
      "step": 16878,
      "training_loss": 6.9103102684021
    },
    {
      "epoch": 0.9148509485094851,
      "step": 16879,
      "training_loss": 5.783597469329834
    },
    {
      "epoch": 0.9149051490514906,
      "grad_norm": 22.63644790649414,
      "learning_rate": 1e-05,
      "loss": 5.8995,
      "step": 16880
    },
    {
      "epoch": 0.9149051490514906,
      "step": 16880,
      "training_loss": 6.0961127281188965
    },
    {
      "epoch": 0.9149593495934959,
      "step": 16881,
      "training_loss": 8.355203628540039
    },
    {
      "epoch": 0.9150135501355013,
      "step": 16882,
      "training_loss": 6.714111804962158
    },
    {
      "epoch": 0.9150677506775068,
      "step": 16883,
      "training_loss": 5.515687942504883
    },
    {
      "epoch": 0.9151219512195122,
      "grad_norm": 32.018394470214844,
      "learning_rate": 1e-05,
      "loss": 6.6703,
      "step": 16884
    },
    {
      "epoch": 0.9151219512195122,
      "step": 16884,
      "training_loss": 3.0264387130737305
    },
    {
      "epoch": 0.9151761517615176,
      "step": 16885,
      "training_loss": 5.84441614151001
    },
    {
      "epoch": 0.915230352303523,
      "step": 16886,
      "training_loss": 7.695356369018555
    },
    {
      "epoch": 0.9152845528455285,
      "step": 16887,
      "training_loss": 8.117951393127441
    },
    {
      "epoch": 0.9153387533875339,
      "grad_norm": 25.671422958374023,
      "learning_rate": 1e-05,
      "loss": 6.171,
      "step": 16888
    },
    {
      "epoch": 0.9153387533875339,
      "step": 16888,
      "training_loss": 7.399318218231201
    },
    {
      "epoch": 0.9153929539295393,
      "step": 16889,
      "training_loss": 7.0817131996154785
    },
    {
      "epoch": 0.9154471544715447,
      "step": 16890,
      "training_loss": 6.398962497711182
    },
    {
      "epoch": 0.9155013550135501,
      "step": 16891,
      "training_loss": 6.876463890075684
    },
    {
      "epoch": 0.9155555555555556,
      "grad_norm": 25.343416213989258,
      "learning_rate": 1e-05,
      "loss": 6.9391,
      "step": 16892
    },
    {
      "epoch": 0.9155555555555556,
      "step": 16892,
      "training_loss": 7.767491340637207
    },
    {
      "epoch": 0.915609756097561,
      "step": 16893,
      "training_loss": 7.008846282958984
    },
    {
      "epoch": 0.9156639566395663,
      "step": 16894,
      "training_loss": 5.847997665405273
    },
    {
      "epoch": 0.9157181571815718,
      "step": 16895,
      "training_loss": 5.401648044586182
    },
    {
      "epoch": 0.9157723577235772,
      "grad_norm": 27.346038818359375,
      "learning_rate": 1e-05,
      "loss": 6.5065,
      "step": 16896
    },
    {
      "epoch": 0.9157723577235772,
      "step": 16896,
      "training_loss": 6.788907527923584
    },
    {
      "epoch": 0.9158265582655827,
      "step": 16897,
      "training_loss": 6.107202053070068
    },
    {
      "epoch": 0.9158807588075881,
      "step": 16898,
      "training_loss": 7.185860633850098
    },
    {
      "epoch": 0.9159349593495935,
      "step": 16899,
      "training_loss": 7.749871730804443
    },
    {
      "epoch": 0.9159891598915989,
      "grad_norm": 31.269981384277344,
      "learning_rate": 1e-05,
      "loss": 6.958,
      "step": 16900
    },
    {
      "epoch": 0.9159891598915989,
      "step": 16900,
      "training_loss": 8.183201789855957
    },
    {
      "epoch": 0.9160433604336043,
      "step": 16901,
      "training_loss": 6.551032543182373
    },
    {
      "epoch": 0.9160975609756098,
      "step": 16902,
      "training_loss": 6.295754432678223
    },
    {
      "epoch": 0.9161517615176151,
      "step": 16903,
      "training_loss": 6.545159816741943
    },
    {
      "epoch": 0.9162059620596206,
      "grad_norm": 28.28114891052246,
      "learning_rate": 1e-05,
      "loss": 6.8938,
      "step": 16904
    },
    {
      "epoch": 0.9162059620596206,
      "step": 16904,
      "training_loss": 5.65763521194458
    },
    {
      "epoch": 0.916260162601626,
      "step": 16905,
      "training_loss": 7.133111476898193
    },
    {
      "epoch": 0.9163143631436315,
      "step": 16906,
      "training_loss": 7.448417663574219
    },
    {
      "epoch": 0.9163685636856369,
      "step": 16907,
      "training_loss": 7.285274982452393
    },
    {
      "epoch": 0.9164227642276422,
      "grad_norm": 21.430870056152344,
      "learning_rate": 1e-05,
      "loss": 6.8811,
      "step": 16908
    },
    {
      "epoch": 0.9164227642276422,
      "step": 16908,
      "training_loss": 8.048852920532227
    },
    {
      "epoch": 0.9164769647696477,
      "step": 16909,
      "training_loss": 6.597940444946289
    },
    {
      "epoch": 0.9165311653116531,
      "step": 16910,
      "training_loss": 6.300061225891113
    },
    {
      "epoch": 0.9165853658536586,
      "step": 16911,
      "training_loss": 7.190533638000488
    },
    {
      "epoch": 0.9166395663956639,
      "grad_norm": 19.831844329833984,
      "learning_rate": 1e-05,
      "loss": 7.0343,
      "step": 16912
    },
    {
      "epoch": 0.9166395663956639,
      "step": 16912,
      "training_loss": 6.757300853729248
    },
    {
      "epoch": 0.9166937669376694,
      "step": 16913,
      "training_loss": 8.022396087646484
    },
    {
      "epoch": 0.9167479674796748,
      "step": 16914,
      "training_loss": 6.373237133026123
    },
    {
      "epoch": 0.9168021680216802,
      "step": 16915,
      "training_loss": 6.794622421264648
    },
    {
      "epoch": 0.9168563685636857,
      "grad_norm": 31.618045806884766,
      "learning_rate": 1e-05,
      "loss": 6.9869,
      "step": 16916
    },
    {
      "epoch": 0.9168563685636857,
      "step": 16916,
      "training_loss": 6.656548023223877
    },
    {
      "epoch": 0.916910569105691,
      "step": 16917,
      "training_loss": 7.938146114349365
    },
    {
      "epoch": 0.9169647696476965,
      "step": 16918,
      "training_loss": 6.10016393661499
    },
    {
      "epoch": 0.9170189701897019,
      "step": 16919,
      "training_loss": 4.109135627746582
    },
    {
      "epoch": 0.9170731707317074,
      "grad_norm": 24.06929588317871,
      "learning_rate": 1e-05,
      "loss": 6.201,
      "step": 16920
    },
    {
      "epoch": 0.9170731707317074,
      "step": 16920,
      "training_loss": 6.572301387786865
    },
    {
      "epoch": 0.9171273712737127,
      "step": 16921,
      "training_loss": 6.942062854766846
    },
    {
      "epoch": 0.9171815718157181,
      "step": 16922,
      "training_loss": 6.77084493637085
    },
    {
      "epoch": 0.9172357723577236,
      "step": 16923,
      "training_loss": 8.255949020385742
    },
    {
      "epoch": 0.917289972899729,
      "grad_norm": 31.358728408813477,
      "learning_rate": 1e-05,
      "loss": 7.1353,
      "step": 16924
    },
    {
      "epoch": 0.917289972899729,
      "step": 16924,
      "training_loss": 6.2965779304504395
    },
    {
      "epoch": 0.9173441734417345,
      "step": 16925,
      "training_loss": 5.237968921661377
    },
    {
      "epoch": 0.9173983739837398,
      "step": 16926,
      "training_loss": 7.072226524353027
    },
    {
      "epoch": 0.9174525745257452,
      "step": 16927,
      "training_loss": 7.8950700759887695
    },
    {
      "epoch": 0.9175067750677507,
      "grad_norm": 29.57445526123047,
      "learning_rate": 1e-05,
      "loss": 6.6255,
      "step": 16928
    },
    {
      "epoch": 0.9175067750677507,
      "step": 16928,
      "training_loss": 6.699148178100586
    },
    {
      "epoch": 0.9175609756097561,
      "step": 16929,
      "training_loss": 6.726745128631592
    },
    {
      "epoch": 0.9176151761517615,
      "step": 16930,
      "training_loss": 5.167660236358643
    },
    {
      "epoch": 0.9176693766937669,
      "step": 16931,
      "training_loss": 7.557263374328613
    },
    {
      "epoch": 0.9177235772357724,
      "grad_norm": 18.808544158935547,
      "learning_rate": 1e-05,
      "loss": 6.5377,
      "step": 16932
    },
    {
      "epoch": 0.9177235772357724,
      "step": 16932,
      "training_loss": 6.590561389923096
    },
    {
      "epoch": 0.9177777777777778,
      "step": 16933,
      "training_loss": 7.459108829498291
    },
    {
      "epoch": 0.9178319783197832,
      "step": 16934,
      "training_loss": 5.832923412322998
    },
    {
      "epoch": 0.9178861788617886,
      "step": 16935,
      "training_loss": 6.726844310760498
    },
    {
      "epoch": 0.917940379403794,
      "grad_norm": 35.73267364501953,
      "learning_rate": 1e-05,
      "loss": 6.6524,
      "step": 16936
    },
    {
      "epoch": 0.917940379403794,
      "step": 16936,
      "training_loss": 3.887237787246704
    },
    {
      "epoch": 0.9179945799457995,
      "step": 16937,
      "training_loss": 6.876771926879883
    },
    {
      "epoch": 0.9180487804878049,
      "step": 16938,
      "training_loss": 6.984155178070068
    },
    {
      "epoch": 0.9181029810298103,
      "step": 16939,
      "training_loss": 7.334811210632324
    },
    {
      "epoch": 0.9181571815718157,
      "grad_norm": 40.465518951416016,
      "learning_rate": 1e-05,
      "loss": 6.2707,
      "step": 16940
    },
    {
      "epoch": 0.9181571815718157,
      "step": 16940,
      "training_loss": 7.448970317840576
    },
    {
      "epoch": 0.9182113821138211,
      "step": 16941,
      "training_loss": 4.502318859100342
    },
    {
      "epoch": 0.9182655826558266,
      "step": 16942,
      "training_loss": 6.278566360473633
    },
    {
      "epoch": 0.918319783197832,
      "step": 16943,
      "training_loss": 5.40544319152832
    },
    {
      "epoch": 0.9183739837398374,
      "grad_norm": 24.276975631713867,
      "learning_rate": 1e-05,
      "loss": 5.9088,
      "step": 16944
    },
    {
      "epoch": 0.9183739837398374,
      "step": 16944,
      "training_loss": 8.198683738708496
    },
    {
      "epoch": 0.9184281842818428,
      "step": 16945,
      "training_loss": 7.197356224060059
    },
    {
      "epoch": 0.9184823848238483,
      "step": 16946,
      "training_loss": 4.782855033874512
    },
    {
      "epoch": 0.9185365853658537,
      "step": 16947,
      "training_loss": 6.232069492340088
    },
    {
      "epoch": 0.918590785907859,
      "grad_norm": 33.13105392456055,
      "learning_rate": 1e-05,
      "loss": 6.6027,
      "step": 16948
    },
    {
      "epoch": 0.918590785907859,
      "step": 16948,
      "training_loss": 7.907398700714111
    },
    {
      "epoch": 0.9186449864498645,
      "step": 16949,
      "training_loss": 7.369970798492432
    },
    {
      "epoch": 0.9186991869918699,
      "step": 16950,
      "training_loss": 7.486407279968262
    },
    {
      "epoch": 0.9187533875338754,
      "step": 16951,
      "training_loss": 6.081439971923828
    },
    {
      "epoch": 0.9188075880758808,
      "grad_norm": 79.18473815917969,
      "learning_rate": 1e-05,
      "loss": 7.2113,
      "step": 16952
    },
    {
      "epoch": 0.9188075880758808,
      "step": 16952,
      "training_loss": 7.897318363189697
    },
    {
      "epoch": 0.9188617886178861,
      "step": 16953,
      "training_loss": 6.5835442543029785
    },
    {
      "epoch": 0.9189159891598916,
      "step": 16954,
      "training_loss": 6.986634254455566
    },
    {
      "epoch": 0.918970189701897,
      "step": 16955,
      "training_loss": 5.768171310424805
    },
    {
      "epoch": 0.9190243902439025,
      "grad_norm": 25.678192138671875,
      "learning_rate": 1e-05,
      "loss": 6.8089,
      "step": 16956
    },
    {
      "epoch": 0.9190243902439025,
      "step": 16956,
      "training_loss": 5.825279712677002
    },
    {
      "epoch": 0.9190785907859078,
      "step": 16957,
      "training_loss": 5.907923221588135
    },
    {
      "epoch": 0.9191327913279133,
      "step": 16958,
      "training_loss": 5.553078651428223
    },
    {
      "epoch": 0.9191869918699187,
      "step": 16959,
      "training_loss": 7.6214919090271
    },
    {
      "epoch": 0.9192411924119241,
      "grad_norm": 26.059640884399414,
      "learning_rate": 1e-05,
      "loss": 6.2269,
      "step": 16960
    },
    {
      "epoch": 0.9192411924119241,
      "step": 16960,
      "training_loss": 7.870455741882324
    },
    {
      "epoch": 0.9192953929539296,
      "step": 16961,
      "training_loss": 6.082414150238037
    },
    {
      "epoch": 0.9193495934959349,
      "step": 16962,
      "training_loss": 7.666182041168213
    },
    {
      "epoch": 0.9194037940379404,
      "step": 16963,
      "training_loss": 6.577683925628662
    },
    {
      "epoch": 0.9194579945799458,
      "grad_norm": 33.65401077270508,
      "learning_rate": 1e-05,
      "loss": 7.0492,
      "step": 16964
    },
    {
      "epoch": 0.9194579945799458,
      "step": 16964,
      "training_loss": 5.579562187194824
    },
    {
      "epoch": 0.9195121951219513,
      "step": 16965,
      "training_loss": 7.148910999298096
    },
    {
      "epoch": 0.9195663956639566,
      "step": 16966,
      "training_loss": 7.016161918640137
    },
    {
      "epoch": 0.919620596205962,
      "step": 16967,
      "training_loss": 5.220643043518066
    },
    {
      "epoch": 0.9196747967479675,
      "grad_norm": 31.585092544555664,
      "learning_rate": 1e-05,
      "loss": 6.2413,
      "step": 16968
    },
    {
      "epoch": 0.9196747967479675,
      "step": 16968,
      "training_loss": 6.887500286102295
    },
    {
      "epoch": 0.9197289972899729,
      "step": 16969,
      "training_loss": 5.1911540031433105
    },
    {
      "epoch": 0.9197831978319784,
      "step": 16970,
      "training_loss": 7.070016384124756
    },
    {
      "epoch": 0.9198373983739837,
      "step": 16971,
      "training_loss": 6.79914665222168
    },
    {
      "epoch": 0.9198915989159892,
      "grad_norm": 23.504634857177734,
      "learning_rate": 1e-05,
      "loss": 6.487,
      "step": 16972
    },
    {
      "epoch": 0.9198915989159892,
      "step": 16972,
      "training_loss": 7.298830032348633
    },
    {
      "epoch": 0.9199457994579946,
      "step": 16973,
      "training_loss": 6.562260627746582
    },
    {
      "epoch": 0.92,
      "step": 16974,
      "training_loss": 5.988722324371338
    },
    {
      "epoch": 0.9200542005420054,
      "step": 16975,
      "training_loss": 6.654113292694092
    },
    {
      "epoch": 0.9201084010840108,
      "grad_norm": 20.505687713623047,
      "learning_rate": 1e-05,
      "loss": 6.626,
      "step": 16976
    },
    {
      "epoch": 0.9201084010840108,
      "step": 16976,
      "training_loss": 6.641808986663818
    },
    {
      "epoch": 0.9201626016260163,
      "step": 16977,
      "training_loss": 7.253491401672363
    },
    {
      "epoch": 0.9202168021680217,
      "step": 16978,
      "training_loss": 6.562947750091553
    },
    {
      "epoch": 0.9202710027100272,
      "step": 16979,
      "training_loss": 6.066809177398682
    },
    {
      "epoch": 0.9203252032520325,
      "grad_norm": 39.43755340576172,
      "learning_rate": 1e-05,
      "loss": 6.6313,
      "step": 16980
    },
    {
      "epoch": 0.9203252032520325,
      "step": 16980,
      "training_loss": 6.671034336090088
    },
    {
      "epoch": 0.9203794037940379,
      "step": 16981,
      "training_loss": 6.072954177856445
    },
    {
      "epoch": 0.9204336043360434,
      "step": 16982,
      "training_loss": 6.338023662567139
    },
    {
      "epoch": 0.9204878048780488,
      "step": 16983,
      "training_loss": 6.324666500091553
    },
    {
      "epoch": 0.9205420054200542,
      "grad_norm": 20.85081672668457,
      "learning_rate": 1e-05,
      "loss": 6.3517,
      "step": 16984
    },
    {
      "epoch": 0.9205420054200542,
      "step": 16984,
      "training_loss": 6.2765092849731445
    },
    {
      "epoch": 0.9205962059620596,
      "step": 16985,
      "training_loss": 5.586455345153809
    },
    {
      "epoch": 0.920650406504065,
      "step": 16986,
      "training_loss": 6.094070911407471
    },
    {
      "epoch": 0.9207046070460705,
      "step": 16987,
      "training_loss": 7.251253128051758
    },
    {
      "epoch": 0.9207588075880759,
      "grad_norm": 25.455631256103516,
      "learning_rate": 1e-05,
      "loss": 6.3021,
      "step": 16988
    },
    {
      "epoch": 0.9207588075880759,
      "step": 16988,
      "training_loss": 5.902388095855713
    },
    {
      "epoch": 0.9208130081300813,
      "step": 16989,
      "training_loss": 7.757004261016846
    },
    {
      "epoch": 0.9208672086720867,
      "step": 16990,
      "training_loss": 4.314871311187744
    },
    {
      "epoch": 0.9209214092140922,
      "step": 16991,
      "training_loss": 6.310635566711426
    },
    {
      "epoch": 0.9209756097560976,
      "grad_norm": 18.753612518310547,
      "learning_rate": 1e-05,
      "loss": 6.0712,
      "step": 16992
    },
    {
      "epoch": 0.9209756097560976,
      "step": 16992,
      "training_loss": 7.12850284576416
    },
    {
      "epoch": 0.9210298102981029,
      "step": 16993,
      "training_loss": 8.542922019958496
    },
    {
      "epoch": 0.9210840108401084,
      "step": 16994,
      "training_loss": 6.692837715148926
    },
    {
      "epoch": 0.9211382113821138,
      "step": 16995,
      "training_loss": 7.790744781494141
    },
    {
      "epoch": 0.9211924119241193,
      "grad_norm": 31.39075469970703,
      "learning_rate": 1e-05,
      "loss": 7.5388,
      "step": 16996
    },
    {
      "epoch": 0.9211924119241193,
      "step": 16996,
      "training_loss": 6.250264644622803
    },
    {
      "epoch": 0.9212466124661247,
      "step": 16997,
      "training_loss": 4.110515117645264
    },
    {
      "epoch": 0.92130081300813,
      "step": 16998,
      "training_loss": 7.270269393920898
    },
    {
      "epoch": 0.9213550135501355,
      "step": 16999,
      "training_loss": 4.693112850189209
    },
    {
      "epoch": 0.9214092140921409,
      "grad_norm": 26.503934860229492,
      "learning_rate": 1e-05,
      "loss": 5.581,
      "step": 17000
    },
    {
      "epoch": 0.9214092140921409,
      "step": 17000,
      "training_loss": 7.167410373687744
    },
    {
      "epoch": 0.9214634146341464,
      "step": 17001,
      "training_loss": 6.0503950119018555
    },
    {
      "epoch": 0.9215176151761517,
      "step": 17002,
      "training_loss": 6.880168914794922
    },
    {
      "epoch": 0.9215718157181572,
      "step": 17003,
      "training_loss": 5.721323013305664
    },
    {
      "epoch": 0.9216260162601626,
      "grad_norm": 20.843128204345703,
      "learning_rate": 1e-05,
      "loss": 6.4548,
      "step": 17004
    },
    {
      "epoch": 0.9216260162601626,
      "step": 17004,
      "training_loss": 5.968238353729248
    },
    {
      "epoch": 0.921680216802168,
      "step": 17005,
      "training_loss": 7.289370536804199
    },
    {
      "epoch": 0.9217344173441735,
      "step": 17006,
      "training_loss": 6.959553241729736
    },
    {
      "epoch": 0.9217886178861788,
      "step": 17007,
      "training_loss": 5.699664115905762
    },
    {
      "epoch": 0.9218428184281843,
      "grad_norm": 26.5638427734375,
      "learning_rate": 1e-05,
      "loss": 6.4792,
      "step": 17008
    },
    {
      "epoch": 0.9218428184281843,
      "step": 17008,
      "training_loss": 7.266307353973389
    },
    {
      "epoch": 0.9218970189701897,
      "step": 17009,
      "training_loss": 7.0588698387146
    },
    {
      "epoch": 0.9219512195121952,
      "step": 17010,
      "training_loss": 5.745240211486816
    },
    {
      "epoch": 0.9220054200542005,
      "step": 17011,
      "training_loss": 2.959808349609375
    },
    {
      "epoch": 0.9220596205962059,
      "grad_norm": 23.520477294921875,
      "learning_rate": 1e-05,
      "loss": 5.7576,
      "step": 17012
    },
    {
      "epoch": 0.9220596205962059,
      "step": 17012,
      "training_loss": 8.666097640991211
    },
    {
      "epoch": 0.9221138211382114,
      "step": 17013,
      "training_loss": 6.544844150543213
    },
    {
      "epoch": 0.9221680216802168,
      "step": 17014,
      "training_loss": 6.701381206512451
    },
    {
      "epoch": 0.9222222222222223,
      "step": 17015,
      "training_loss": 5.561339855194092
    },
    {
      "epoch": 0.9222764227642276,
      "grad_norm": 28.718923568725586,
      "learning_rate": 1e-05,
      "loss": 6.8684,
      "step": 17016
    },
    {
      "epoch": 0.9222764227642276,
      "step": 17016,
      "training_loss": 7.136988639831543
    },
    {
      "epoch": 0.922330623306233,
      "step": 17017,
      "training_loss": 7.297632694244385
    },
    {
      "epoch": 0.9223848238482385,
      "step": 17018,
      "training_loss": 5.5176544189453125
    },
    {
      "epoch": 0.922439024390244,
      "step": 17019,
      "training_loss": 5.441620349884033
    },
    {
      "epoch": 0.9224932249322493,
      "grad_norm": 40.88813400268555,
      "learning_rate": 1e-05,
      "loss": 6.3485,
      "step": 17020
    },
    {
      "epoch": 0.9224932249322493,
      "step": 17020,
      "training_loss": 3.9179983139038086
    },
    {
      "epoch": 0.9225474254742547,
      "step": 17021,
      "training_loss": 7.114180564880371
    },
    {
      "epoch": 0.9226016260162602,
      "step": 17022,
      "training_loss": 6.413222789764404
    },
    {
      "epoch": 0.9226558265582656,
      "step": 17023,
      "training_loss": 6.451552867889404
    },
    {
      "epoch": 0.9227100271002711,
      "grad_norm": 26.0628719329834,
      "learning_rate": 1e-05,
      "loss": 5.9742,
      "step": 17024
    },
    {
      "epoch": 0.9227100271002711,
      "step": 17024,
      "training_loss": 7.255105972290039
    },
    {
      "epoch": 0.9227642276422764,
      "step": 17025,
      "training_loss": 7.71649694442749
    },
    {
      "epoch": 0.9228184281842818,
      "step": 17026,
      "training_loss": 7.375272750854492
    },
    {
      "epoch": 0.9228726287262873,
      "step": 17027,
      "training_loss": 5.464664936065674
    },
    {
      "epoch": 0.9229268292682927,
      "grad_norm": 33.15983581542969,
      "learning_rate": 1e-05,
      "loss": 6.9529,
      "step": 17028
    },
    {
      "epoch": 0.9229268292682927,
      "step": 17028,
      "training_loss": 5.600558280944824
    },
    {
      "epoch": 0.9229810298102981,
      "step": 17029,
      "training_loss": 7.519784450531006
    },
    {
      "epoch": 0.9230352303523035,
      "step": 17030,
      "training_loss": 6.021025657653809
    },
    {
      "epoch": 0.923089430894309,
      "step": 17031,
      "training_loss": 6.3732686042785645
    },
    {
      "epoch": 0.9231436314363144,
      "grad_norm": 33.91265106201172,
      "learning_rate": 1e-05,
      "loss": 6.3787,
      "step": 17032
    },
    {
      "epoch": 0.9231436314363144,
      "step": 17032,
      "training_loss": 7.477108955383301
    },
    {
      "epoch": 0.9231978319783198,
      "step": 17033,
      "training_loss": 7.701135158538818
    },
    {
      "epoch": 0.9232520325203252,
      "step": 17034,
      "training_loss": 5.036226749420166
    },
    {
      "epoch": 0.9233062330623306,
      "step": 17035,
      "training_loss": 6.812793731689453
    },
    {
      "epoch": 0.9233604336043361,
      "grad_norm": 34.06938934326172,
      "learning_rate": 1e-05,
      "loss": 6.7568,
      "step": 17036
    },
    {
      "epoch": 0.9233604336043361,
      "step": 17036,
      "training_loss": 7.141264915466309
    },
    {
      "epoch": 0.9234146341463415,
      "step": 17037,
      "training_loss": 7.068947792053223
    },
    {
      "epoch": 0.9234688346883468,
      "step": 17038,
      "training_loss": 8.026873588562012
    },
    {
      "epoch": 0.9235230352303523,
      "step": 17039,
      "training_loss": 5.022714138031006
    },
    {
      "epoch": 0.9235772357723577,
      "grad_norm": 25.362747192382812,
      "learning_rate": 1e-05,
      "loss": 6.815,
      "step": 17040
    },
    {
      "epoch": 0.9235772357723577,
      "step": 17040,
      "training_loss": 6.033472061157227
    },
    {
      "epoch": 0.9236314363143632,
      "step": 17041,
      "training_loss": 6.617269515991211
    },
    {
      "epoch": 0.9236856368563685,
      "step": 17042,
      "training_loss": 6.17209529876709
    },
    {
      "epoch": 0.923739837398374,
      "step": 17043,
      "training_loss": 5.162541389465332
    },
    {
      "epoch": 0.9237940379403794,
      "grad_norm": 35.71198654174805,
      "learning_rate": 1e-05,
      "loss": 5.9963,
      "step": 17044
    },
    {
      "epoch": 0.9237940379403794,
      "step": 17044,
      "training_loss": 7.097049713134766
    },
    {
      "epoch": 0.9238482384823848,
      "step": 17045,
      "training_loss": 6.170593738555908
    },
    {
      "epoch": 0.9239024390243903,
      "step": 17046,
      "training_loss": 8.893062591552734
    },
    {
      "epoch": 0.9239566395663956,
      "step": 17047,
      "training_loss": 4.945649147033691
    },
    {
      "epoch": 0.9240108401084011,
      "grad_norm": 29.299964904785156,
      "learning_rate": 1e-05,
      "loss": 6.7766,
      "step": 17048
    },
    {
      "epoch": 0.9240108401084011,
      "step": 17048,
      "training_loss": 5.747480869293213
    },
    {
      "epoch": 0.9240650406504065,
      "step": 17049,
      "training_loss": 6.9881439208984375
    },
    {
      "epoch": 0.924119241192412,
      "step": 17050,
      "training_loss": 8.284131050109863
    },
    {
      "epoch": 0.9241734417344173,
      "step": 17051,
      "training_loss": 6.774250030517578
    },
    {
      "epoch": 0.9242276422764227,
      "grad_norm": 16.284202575683594,
      "learning_rate": 1e-05,
      "loss": 6.9485,
      "step": 17052
    },
    {
      "epoch": 0.9242276422764227,
      "step": 17052,
      "training_loss": 5.666721820831299
    },
    {
      "epoch": 0.9242818428184282,
      "step": 17053,
      "training_loss": 7.19946813583374
    },
    {
      "epoch": 0.9243360433604336,
      "step": 17054,
      "training_loss": 7.133631229400635
    },
    {
      "epoch": 0.9243902439024391,
      "step": 17055,
      "training_loss": 5.544482231140137
    },
    {
      "epoch": 0.9244444444444444,
      "grad_norm": 22.991125106811523,
      "learning_rate": 1e-05,
      "loss": 6.3861,
      "step": 17056
    },
    {
      "epoch": 0.9244444444444444,
      "step": 17056,
      "training_loss": 7.6808247566223145
    },
    {
      "epoch": 0.9244986449864498,
      "step": 17057,
      "training_loss": 6.696815490722656
    },
    {
      "epoch": 0.9245528455284553,
      "step": 17058,
      "training_loss": 6.1980767250061035
    },
    {
      "epoch": 0.9246070460704607,
      "step": 17059,
      "training_loss": 6.21226692199707
    },
    {
      "epoch": 0.9246612466124661,
      "grad_norm": 27.258516311645508,
      "learning_rate": 1e-05,
      "loss": 6.697,
      "step": 17060
    },
    {
      "epoch": 0.9246612466124661,
      "step": 17060,
      "training_loss": 6.726874351501465
    },
    {
      "epoch": 0.9247154471544715,
      "step": 17061,
      "training_loss": 7.126491546630859
    },
    {
      "epoch": 0.924769647696477,
      "step": 17062,
      "training_loss": 5.765495777130127
    },
    {
      "epoch": 0.9248238482384824,
      "step": 17063,
      "training_loss": 4.608433723449707
    },
    {
      "epoch": 0.9248780487804878,
      "grad_norm": 25.798635482788086,
      "learning_rate": 1e-05,
      "loss": 6.0568,
      "step": 17064
    },
    {
      "epoch": 0.9248780487804878,
      "step": 17064,
      "training_loss": 6.587343692779541
    },
    {
      "epoch": 0.9249322493224932,
      "step": 17065,
      "training_loss": 6.457099437713623
    },
    {
      "epoch": 0.9249864498644986,
      "step": 17066,
      "training_loss": 6.747448444366455
    },
    {
      "epoch": 0.9250406504065041,
      "step": 17067,
      "training_loss": 6.119275093078613
    },
    {
      "epoch": 0.9250948509485095,
      "grad_norm": 34.66602325439453,
      "learning_rate": 1e-05,
      "loss": 6.4778,
      "step": 17068
    },
    {
      "epoch": 0.9250948509485095,
      "step": 17068,
      "training_loss": 7.232485294342041
    },
    {
      "epoch": 0.9251490514905149,
      "step": 17069,
      "training_loss": 7.908736228942871
    },
    {
      "epoch": 0.9252032520325203,
      "step": 17070,
      "training_loss": 6.720000267028809
    },
    {
      "epoch": 0.9252574525745257,
      "step": 17071,
      "training_loss": 6.3764166831970215
    },
    {
      "epoch": 0.9253116531165312,
      "grad_norm": 27.235750198364258,
      "learning_rate": 1e-05,
      "loss": 7.0594,
      "step": 17072
    },
    {
      "epoch": 0.9253116531165312,
      "step": 17072,
      "training_loss": 4.922589302062988
    },
    {
      "epoch": 0.9253658536585366,
      "step": 17073,
      "training_loss": 5.6454644203186035
    },
    {
      "epoch": 0.925420054200542,
      "step": 17074,
      "training_loss": 6.474371433258057
    },
    {
      "epoch": 0.9254742547425474,
      "step": 17075,
      "training_loss": 6.08784294128418
    },
    {
      "epoch": 0.9255284552845529,
      "grad_norm": 24.04773712158203,
      "learning_rate": 1e-05,
      "loss": 5.7826,
      "step": 17076
    },
    {
      "epoch": 0.9255284552845529,
      "step": 17076,
      "training_loss": 6.23760986328125
    },
    {
      "epoch": 0.9255826558265583,
      "step": 17077,
      "training_loss": 5.098259449005127
    },
    {
      "epoch": 0.9256368563685636,
      "step": 17078,
      "training_loss": 5.965357780456543
    },
    {
      "epoch": 0.9256910569105691,
      "step": 17079,
      "training_loss": 5.788844108581543
    },
    {
      "epoch": 0.9257452574525745,
      "grad_norm": 23.209674835205078,
      "learning_rate": 1e-05,
      "loss": 5.7725,
      "step": 17080
    },
    {
      "epoch": 0.9257452574525745,
      "step": 17080,
      "training_loss": 6.937255859375
    },
    {
      "epoch": 0.92579945799458,
      "step": 17081,
      "training_loss": 6.518953800201416
    },
    {
      "epoch": 0.9258536585365854,
      "step": 17082,
      "training_loss": 5.9255218505859375
    },
    {
      "epoch": 0.9259078590785907,
      "step": 17083,
      "training_loss": 6.215267658233643
    },
    {
      "epoch": 0.9259620596205962,
      "grad_norm": 22.429561614990234,
      "learning_rate": 1e-05,
      "loss": 6.3992,
      "step": 17084
    },
    {
      "epoch": 0.9259620596205962,
      "step": 17084,
      "training_loss": 6.775914669036865
    },
    {
      "epoch": 0.9260162601626016,
      "step": 17085,
      "training_loss": 4.2370476722717285
    },
    {
      "epoch": 0.9260704607046071,
      "step": 17086,
      "training_loss": 4.393772602081299
    },
    {
      "epoch": 0.9261246612466124,
      "step": 17087,
      "training_loss": 5.733956813812256
    },
    {
      "epoch": 0.9261788617886179,
      "grad_norm": 24.865123748779297,
      "learning_rate": 1e-05,
      "loss": 5.2852,
      "step": 17088
    },
    {
      "epoch": 0.9261788617886179,
      "step": 17088,
      "training_loss": 7.22750997543335
    },
    {
      "epoch": 0.9262330623306233,
      "step": 17089,
      "training_loss": 6.047274589538574
    },
    {
      "epoch": 0.9262872628726287,
      "step": 17090,
      "training_loss": 7.301305294036865
    },
    {
      "epoch": 0.9263414634146342,
      "step": 17091,
      "training_loss": 7.43468713760376
    },
    {
      "epoch": 0.9263956639566395,
      "grad_norm": 32.47119140625,
      "learning_rate": 1e-05,
      "loss": 7.0027,
      "step": 17092
    },
    {
      "epoch": 0.9263956639566395,
      "step": 17092,
      "training_loss": 3.089353322982788
    },
    {
      "epoch": 0.926449864498645,
      "step": 17093,
      "training_loss": 5.716818332672119
    },
    {
      "epoch": 0.9265040650406504,
      "step": 17094,
      "training_loss": 4.516225337982178
    },
    {
      "epoch": 0.9265582655826559,
      "step": 17095,
      "training_loss": 5.543503761291504
    },
    {
      "epoch": 0.9266124661246612,
      "grad_norm": 26.09312629699707,
      "learning_rate": 1e-05,
      "loss": 4.7165,
      "step": 17096
    },
    {
      "epoch": 0.9266124661246612,
      "step": 17096,
      "training_loss": 8.383014678955078
    },
    {
      "epoch": 0.9266666666666666,
      "step": 17097,
      "training_loss": 7.15984582901001
    },
    {
      "epoch": 0.9267208672086721,
      "step": 17098,
      "training_loss": 9.078857421875
    },
    {
      "epoch": 0.9267750677506775,
      "step": 17099,
      "training_loss": 6.965020656585693
    },
    {
      "epoch": 0.926829268292683,
      "grad_norm": 53.491512298583984,
      "learning_rate": 1e-05,
      "loss": 7.8967,
      "step": 17100
    },
    {
      "epoch": 0.926829268292683,
      "step": 17100,
      "training_loss": 6.480101585388184
    },
    {
      "epoch": 0.9268834688346883,
      "step": 17101,
      "training_loss": 6.178399562835693
    },
    {
      "epoch": 0.9269376693766938,
      "step": 17102,
      "training_loss": 6.594605922698975
    },
    {
      "epoch": 0.9269918699186992,
      "step": 17103,
      "training_loss": 5.612673282623291
    },
    {
      "epoch": 0.9270460704607046,
      "grad_norm": 40.941734313964844,
      "learning_rate": 1e-05,
      "loss": 6.2164,
      "step": 17104
    },
    {
      "epoch": 0.9270460704607046,
      "step": 17104,
      "training_loss": 6.4491400718688965
    },
    {
      "epoch": 0.92710027100271,
      "step": 17105,
      "training_loss": 6.079949378967285
    },
    {
      "epoch": 0.9271544715447154,
      "step": 17106,
      "training_loss": 6.637732982635498
    },
    {
      "epoch": 0.9272086720867209,
      "step": 17107,
      "training_loss": 8.10090160369873
    },
    {
      "epoch": 0.9272628726287263,
      "grad_norm": 30.28754425048828,
      "learning_rate": 1e-05,
      "loss": 6.8169,
      "step": 17108
    },
    {
      "epoch": 0.9272628726287263,
      "step": 17108,
      "training_loss": 6.9321441650390625
    },
    {
      "epoch": 0.9273170731707318,
      "step": 17109,
      "training_loss": 5.177787780761719
    },
    {
      "epoch": 0.9273712737127371,
      "step": 17110,
      "training_loss": 3.3034744262695312
    },
    {
      "epoch": 0.9274254742547425,
      "step": 17111,
      "training_loss": 4.896852493286133
    },
    {
      "epoch": 0.927479674796748,
      "grad_norm": 23.096843719482422,
      "learning_rate": 1e-05,
      "loss": 5.0776,
      "step": 17112
    },
    {
      "epoch": 0.927479674796748,
      "step": 17112,
      "training_loss": 6.684747219085693
    },
    {
      "epoch": 0.9275338753387534,
      "step": 17113,
      "training_loss": 6.5131988525390625
    },
    {
      "epoch": 0.9275880758807588,
      "step": 17114,
      "training_loss": 4.425731658935547
    },
    {
      "epoch": 0.9276422764227642,
      "step": 17115,
      "training_loss": 6.23261833190918
    },
    {
      "epoch": 0.9276964769647696,
      "grad_norm": 33.80192184448242,
      "learning_rate": 1e-05,
      "loss": 5.9641,
      "step": 17116
    },
    {
      "epoch": 0.9276964769647696,
      "step": 17116,
      "training_loss": 7.303499698638916
    },
    {
      "epoch": 0.9277506775067751,
      "step": 17117,
      "training_loss": 3.4938600063323975
    },
    {
      "epoch": 0.9278048780487805,
      "step": 17118,
      "training_loss": 6.208927631378174
    },
    {
      "epoch": 0.9278590785907859,
      "step": 17119,
      "training_loss": 7.233126640319824
    },
    {
      "epoch": 0.9279132791327913,
      "grad_norm": 19.024242401123047,
      "learning_rate": 1e-05,
      "loss": 6.0599,
      "step": 17120
    },
    {
      "epoch": 0.9279132791327913,
      "step": 17120,
      "training_loss": 5.238083839416504
    },
    {
      "epoch": 0.9279674796747968,
      "step": 17121,
      "training_loss": 6.66251277923584
    },
    {
      "epoch": 0.9280216802168022,
      "step": 17122,
      "training_loss": 7.490536212921143
    },
    {
      "epoch": 0.9280758807588075,
      "step": 17123,
      "training_loss": 6.549671173095703
    },
    {
      "epoch": 0.928130081300813,
      "grad_norm": 23.44610595703125,
      "learning_rate": 1e-05,
      "loss": 6.4852,
      "step": 17124
    },
    {
      "epoch": 0.928130081300813,
      "step": 17124,
      "training_loss": 6.765102386474609
    },
    {
      "epoch": 0.9281842818428184,
      "step": 17125,
      "training_loss": 6.634074687957764
    },
    {
      "epoch": 0.9282384823848239,
      "step": 17126,
      "training_loss": 4.744614601135254
    },
    {
      "epoch": 0.9282926829268293,
      "step": 17127,
      "training_loss": 8.106422424316406
    },
    {
      "epoch": 0.9283468834688346,
      "grad_norm": 46.281272888183594,
      "learning_rate": 1e-05,
      "loss": 6.5626,
      "step": 17128
    },
    {
      "epoch": 0.9283468834688346,
      "step": 17128,
      "training_loss": 7.710996627807617
    },
    {
      "epoch": 0.9284010840108401,
      "step": 17129,
      "training_loss": 7.265102863311768
    },
    {
      "epoch": 0.9284552845528455,
      "step": 17130,
      "training_loss": 7.0913166999816895
    },
    {
      "epoch": 0.928509485094851,
      "step": 17131,
      "training_loss": 6.53835391998291
    },
    {
      "epoch": 0.9285636856368563,
      "grad_norm": 26.695749282836914,
      "learning_rate": 1e-05,
      "loss": 7.1514,
      "step": 17132
    },
    {
      "epoch": 0.9285636856368563,
      "step": 17132,
      "training_loss": 7.740138053894043
    },
    {
      "epoch": 0.9286178861788618,
      "step": 17133,
      "training_loss": 7.030226230621338
    },
    {
      "epoch": 0.9286720867208672,
      "step": 17134,
      "training_loss": 7.97372579574585
    },
    {
      "epoch": 0.9287262872628727,
      "step": 17135,
      "training_loss": 7.316976547241211
    },
    {
      "epoch": 0.9287804878048781,
      "grad_norm": 29.181106567382812,
      "learning_rate": 1e-05,
      "loss": 7.5153,
      "step": 17136
    },
    {
      "epoch": 0.9287804878048781,
      "step": 17136,
      "training_loss": 6.587606430053711
    },
    {
      "epoch": 0.9288346883468834,
      "step": 17137,
      "training_loss": 6.51210880279541
    },
    {
      "epoch": 0.9288888888888889,
      "step": 17138,
      "training_loss": 8.008317947387695
    },
    {
      "epoch": 0.9289430894308943,
      "step": 17139,
      "training_loss": 6.267529487609863
    },
    {
      "epoch": 0.9289972899728998,
      "grad_norm": 26.113224029541016,
      "learning_rate": 1e-05,
      "loss": 6.8439,
      "step": 17140
    },
    {
      "epoch": 0.9289972899728998,
      "step": 17140,
      "training_loss": 7.363399982452393
    },
    {
      "epoch": 0.9290514905149051,
      "step": 17141,
      "training_loss": 7.0212883949279785
    },
    {
      "epoch": 0.9291056910569105,
      "step": 17142,
      "training_loss": 6.386547088623047
    },
    {
      "epoch": 0.929159891598916,
      "step": 17143,
      "training_loss": 6.207871913909912
    },
    {
      "epoch": 0.9292140921409214,
      "grad_norm": 24.889446258544922,
      "learning_rate": 1e-05,
      "loss": 6.7448,
      "step": 17144
    },
    {
      "epoch": 0.9292140921409214,
      "step": 17144,
      "training_loss": 7.022035598754883
    },
    {
      "epoch": 0.9292682926829269,
      "step": 17145,
      "training_loss": 6.451864242553711
    },
    {
      "epoch": 0.9293224932249322,
      "step": 17146,
      "training_loss": 7.336313247680664
    },
    {
      "epoch": 0.9293766937669377,
      "step": 17147,
      "training_loss": 6.734050750732422
    },
    {
      "epoch": 0.9294308943089431,
      "grad_norm": 30.63601303100586,
      "learning_rate": 1e-05,
      "loss": 6.8861,
      "step": 17148
    },
    {
      "epoch": 0.9294308943089431,
      "step": 17148,
      "training_loss": 7.614776134490967
    },
    {
      "epoch": 0.9294850948509485,
      "step": 17149,
      "training_loss": 6.889511585235596
    },
    {
      "epoch": 0.9295392953929539,
      "step": 17150,
      "training_loss": 5.2259955406188965
    },
    {
      "epoch": 0.9295934959349593,
      "step": 17151,
      "training_loss": 6.45946741104126
    },
    {
      "epoch": 0.9296476964769648,
      "grad_norm": 27.65842628479004,
      "learning_rate": 1e-05,
      "loss": 6.5474,
      "step": 17152
    },
    {
      "epoch": 0.9296476964769648,
      "step": 17152,
      "training_loss": 6.453814506530762
    },
    {
      "epoch": 0.9297018970189702,
      "step": 17153,
      "training_loss": 6.091771125793457
    },
    {
      "epoch": 0.9297560975609757,
      "step": 17154,
      "training_loss": 6.2289628982543945
    },
    {
      "epoch": 0.929810298102981,
      "step": 17155,
      "training_loss": 4.456632614135742
    },
    {
      "epoch": 0.9298644986449864,
      "grad_norm": 25.63997459411621,
      "learning_rate": 1e-05,
      "loss": 5.8078,
      "step": 17156
    },
    {
      "epoch": 0.9298644986449864,
      "step": 17156,
      "training_loss": 6.484966278076172
    },
    {
      "epoch": 0.9299186991869919,
      "step": 17157,
      "training_loss": 5.734207630157471
    },
    {
      "epoch": 0.9299728997289973,
      "step": 17158,
      "training_loss": 6.129838466644287
    },
    {
      "epoch": 0.9300271002710027,
      "step": 17159,
      "training_loss": 6.607493877410889
    },
    {
      "epoch": 0.9300813008130081,
      "grad_norm": 20.828807830810547,
      "learning_rate": 1e-05,
      "loss": 6.2391,
      "step": 17160
    },
    {
      "epoch": 0.9300813008130081,
      "step": 17160,
      "training_loss": 6.156055927276611
    },
    {
      "epoch": 0.9301355013550135,
      "step": 17161,
      "training_loss": 3.8761491775512695
    },
    {
      "epoch": 0.930189701897019,
      "step": 17162,
      "training_loss": 6.334834098815918
    },
    {
      "epoch": 0.9302439024390244,
      "step": 17163,
      "training_loss": 7.005268096923828
    },
    {
      "epoch": 0.9302981029810298,
      "grad_norm": 38.503318786621094,
      "learning_rate": 1e-05,
      "loss": 5.8431,
      "step": 17164
    },
    {
      "epoch": 0.9302981029810298,
      "step": 17164,
      "training_loss": 6.626719951629639
    },
    {
      "epoch": 0.9303523035230352,
      "step": 17165,
      "training_loss": 6.488567352294922
    },
    {
      "epoch": 0.9304065040650407,
      "step": 17166,
      "training_loss": 5.48137903213501
    },
    {
      "epoch": 0.9304607046070461,
      "step": 17167,
      "training_loss": 7.217296600341797
    },
    {
      "epoch": 0.9305149051490514,
      "grad_norm": 18.06103515625,
      "learning_rate": 1e-05,
      "loss": 6.4535,
      "step": 17168
    },
    {
      "epoch": 0.9305149051490514,
      "step": 17168,
      "training_loss": 3.5786352157592773
    },
    {
      "epoch": 0.9305691056910569,
      "step": 17169,
      "training_loss": 6.675901889801025
    },
    {
      "epoch": 0.9306233062330623,
      "step": 17170,
      "training_loss": 5.863844394683838
    },
    {
      "epoch": 0.9306775067750678,
      "step": 17171,
      "training_loss": 4.888606071472168
    },
    {
      "epoch": 0.9307317073170732,
      "grad_norm": 28.344751358032227,
      "learning_rate": 1e-05,
      "loss": 5.2517,
      "step": 17172
    },
    {
      "epoch": 0.9307317073170732,
      "step": 17172,
      "training_loss": 6.287392616271973
    },
    {
      "epoch": 0.9307859078590786,
      "step": 17173,
      "training_loss": 7.498988628387451
    },
    {
      "epoch": 0.930840108401084,
      "step": 17174,
      "training_loss": 5.234260082244873
    },
    {
      "epoch": 0.9308943089430894,
      "step": 17175,
      "training_loss": 6.963212966918945
    },
    {
      "epoch": 0.9309485094850949,
      "grad_norm": 18.748401641845703,
      "learning_rate": 1e-05,
      "loss": 6.496,
      "step": 17176
    },
    {
      "epoch": 0.9309485094850949,
      "step": 17176,
      "training_loss": 3.8569819927215576
    },
    {
      "epoch": 0.9310027100271002,
      "step": 17177,
      "training_loss": 6.540687084197998
    },
    {
      "epoch": 0.9310569105691057,
      "step": 17178,
      "training_loss": 5.606660842895508
    },
    {
      "epoch": 0.9311111111111111,
      "step": 17179,
      "training_loss": 6.785520076751709
    },
    {
      "epoch": 0.9311653116531166,
      "grad_norm": 22.375598907470703,
      "learning_rate": 1e-05,
      "loss": 5.6975,
      "step": 17180
    },
    {
      "epoch": 0.9311653116531166,
      "step": 17180,
      "training_loss": 6.429509162902832
    },
    {
      "epoch": 0.931219512195122,
      "step": 17181,
      "training_loss": 6.002950668334961
    },
    {
      "epoch": 0.9312737127371273,
      "step": 17182,
      "training_loss": 6.483997821807861
    },
    {
      "epoch": 0.9313279132791328,
      "step": 17183,
      "training_loss": 6.943429470062256
    },
    {
      "epoch": 0.9313821138211382,
      "grad_norm": 30.356056213378906,
      "learning_rate": 1e-05,
      "loss": 6.465,
      "step": 17184
    },
    {
      "epoch": 0.9313821138211382,
      "step": 17184,
      "training_loss": 9.927770614624023
    },
    {
      "epoch": 0.9314363143631437,
      "step": 17185,
      "training_loss": 6.898075103759766
    },
    {
      "epoch": 0.931490514905149,
      "step": 17186,
      "training_loss": 6.281208515167236
    },
    {
      "epoch": 0.9315447154471544,
      "step": 17187,
      "training_loss": 6.540027618408203
    },
    {
      "epoch": 0.9315989159891599,
      "grad_norm": 33.20908737182617,
      "learning_rate": 1e-05,
      "loss": 7.4118,
      "step": 17188
    },
    {
      "epoch": 0.9315989159891599,
      "step": 17188,
      "training_loss": 5.107466697692871
    },
    {
      "epoch": 0.9316531165311653,
      "step": 17189,
      "training_loss": 7.558997631072998
    },
    {
      "epoch": 0.9317073170731708,
      "step": 17190,
      "training_loss": 6.751563549041748
    },
    {
      "epoch": 0.9317615176151761,
      "step": 17191,
      "training_loss": 4.952174186706543
    },
    {
      "epoch": 0.9318157181571816,
      "grad_norm": 44.34288787841797,
      "learning_rate": 1e-05,
      "loss": 6.0926,
      "step": 17192
    },
    {
      "epoch": 0.9318157181571816,
      "step": 17192,
      "training_loss": 5.292703151702881
    },
    {
      "epoch": 0.931869918699187,
      "step": 17193,
      "training_loss": 6.999423503875732
    },
    {
      "epoch": 0.9319241192411925,
      "step": 17194,
      "training_loss": 7.088374614715576
    },
    {
      "epoch": 0.9319783197831978,
      "step": 17195,
      "training_loss": 6.3723907470703125
    },
    {
      "epoch": 0.9320325203252032,
      "grad_norm": 26.03778076171875,
      "learning_rate": 1e-05,
      "loss": 6.4382,
      "step": 17196
    },
    {
      "epoch": 0.9320325203252032,
      "step": 17196,
      "training_loss": 7.526247024536133
    },
    {
      "epoch": 0.9320867208672087,
      "step": 17197,
      "training_loss": 6.321271896362305
    },
    {
      "epoch": 0.9321409214092141,
      "step": 17198,
      "training_loss": 6.266533374786377
    },
    {
      "epoch": 0.9321951219512196,
      "step": 17199,
      "training_loss": 7.073549270629883
    },
    {
      "epoch": 0.9322493224932249,
      "grad_norm": 18.69791603088379,
      "learning_rate": 1e-05,
      "loss": 6.7969,
      "step": 17200
    },
    {
      "epoch": 0.9322493224932249,
      "step": 17200,
      "training_loss": 5.1416192054748535
    },
    {
      "epoch": 0.9323035230352303,
      "step": 17201,
      "training_loss": 9.001883506774902
    },
    {
      "epoch": 0.9323577235772358,
      "step": 17202,
      "training_loss": 7.999480724334717
    },
    {
      "epoch": 0.9324119241192412,
      "step": 17203,
      "training_loss": 6.10837459564209
    },
    {
      "epoch": 0.9324661246612466,
      "grad_norm": 33.489837646484375,
      "learning_rate": 1e-05,
      "loss": 7.0628,
      "step": 17204
    },
    {
      "epoch": 0.9324661246612466,
      "step": 17204,
      "training_loss": 4.378343105316162
    },
    {
      "epoch": 0.932520325203252,
      "step": 17205,
      "training_loss": 4.352629661560059
    },
    {
      "epoch": 0.9325745257452575,
      "step": 17206,
      "training_loss": 6.335569858551025
    },
    {
      "epoch": 0.9326287262872629,
      "step": 17207,
      "training_loss": 5.799545764923096
    },
    {
      "epoch": 0.9326829268292683,
      "grad_norm": 33.985984802246094,
      "learning_rate": 1e-05,
      "loss": 5.2165,
      "step": 17208
    },
    {
      "epoch": 0.9326829268292683,
      "step": 17208,
      "training_loss": 6.469760417938232
    },
    {
      "epoch": 0.9327371273712737,
      "step": 17209,
      "training_loss": 8.206951141357422
    },
    {
      "epoch": 0.9327913279132791,
      "step": 17210,
      "training_loss": 6.505788326263428
    },
    {
      "epoch": 0.9328455284552846,
      "step": 17211,
      "training_loss": 7.236220359802246
    },
    {
      "epoch": 0.93289972899729,
      "grad_norm": 25.906116485595703,
      "learning_rate": 1e-05,
      "loss": 7.1047,
      "step": 17212
    },
    {
      "epoch": 0.93289972899729,
      "step": 17212,
      "training_loss": 6.414228439331055
    },
    {
      "epoch": 0.9329539295392953,
      "step": 17213,
      "training_loss": 7.665899753570557
    },
    {
      "epoch": 0.9330081300813008,
      "step": 17214,
      "training_loss": 4.161070346832275
    },
    {
      "epoch": 0.9330623306233062,
      "step": 17215,
      "training_loss": 6.0120344161987305
    },
    {
      "epoch": 0.9331165311653117,
      "grad_norm": 27.765512466430664,
      "learning_rate": 1e-05,
      "loss": 6.0633,
      "step": 17216
    },
    {
      "epoch": 0.9331165311653117,
      "step": 17216,
      "training_loss": 6.052180290222168
    },
    {
      "epoch": 0.9331707317073171,
      "step": 17217,
      "training_loss": 6.439336776733398
    },
    {
      "epoch": 0.9332249322493225,
      "step": 17218,
      "training_loss": 6.910276889801025
    },
    {
      "epoch": 0.9332791327913279,
      "step": 17219,
      "training_loss": 9.687997817993164
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 72.76776885986328,
      "learning_rate": 1e-05,
      "loss": 7.2724,
      "step": 17220
    },
    {
      "epoch": 0.9333333333333333,
      "step": 17220,
      "training_loss": 5.332808017730713
    },
    {
      "epoch": 0.9333875338753388,
      "step": 17221,
      "training_loss": 7.299666404724121
    },
    {
      "epoch": 0.9334417344173441,
      "step": 17222,
      "training_loss": 5.510103225708008
    },
    {
      "epoch": 0.9334959349593496,
      "step": 17223,
      "training_loss": 6.056246757507324
    },
    {
      "epoch": 0.933550135501355,
      "grad_norm": 21.980947494506836,
      "learning_rate": 1e-05,
      "loss": 6.0497,
      "step": 17224
    },
    {
      "epoch": 0.933550135501355,
      "step": 17224,
      "training_loss": 5.679665565490723
    },
    {
      "epoch": 0.9336043360433605,
      "step": 17225,
      "training_loss": 7.335574626922607
    },
    {
      "epoch": 0.9336585365853659,
      "step": 17226,
      "training_loss": 3.4597597122192383
    },
    {
      "epoch": 0.9337127371273712,
      "step": 17227,
      "training_loss": 7.151068210601807
    },
    {
      "epoch": 0.9337669376693767,
      "grad_norm": 36.50696563720703,
      "learning_rate": 1e-05,
      "loss": 5.9065,
      "step": 17228
    },
    {
      "epoch": 0.9337669376693767,
      "step": 17228,
      "training_loss": 6.37044095993042
    },
    {
      "epoch": 0.9338211382113821,
      "step": 17229,
      "training_loss": 7.080155372619629
    },
    {
      "epoch": 0.9338753387533876,
      "step": 17230,
      "training_loss": 5.539237976074219
    },
    {
      "epoch": 0.9339295392953929,
      "step": 17231,
      "training_loss": 6.648936748504639
    },
    {
      "epoch": 0.9339837398373984,
      "grad_norm": 23.65875244140625,
      "learning_rate": 1e-05,
      "loss": 6.4097,
      "step": 17232
    },
    {
      "epoch": 0.9339837398373984,
      "step": 17232,
      "training_loss": 6.305069446563721
    },
    {
      "epoch": 0.9340379403794038,
      "step": 17233,
      "training_loss": 6.062551021575928
    },
    {
      "epoch": 0.9340921409214092,
      "step": 17234,
      "training_loss": 4.257623195648193
    },
    {
      "epoch": 0.9341463414634147,
      "step": 17235,
      "training_loss": 7.2061052322387695
    },
    {
      "epoch": 0.93420054200542,
      "grad_norm": 20.12755012512207,
      "learning_rate": 1e-05,
      "loss": 5.9578,
      "step": 17236
    },
    {
      "epoch": 0.93420054200542,
      "step": 17236,
      "training_loss": 6.644364833831787
    },
    {
      "epoch": 0.9342547425474255,
      "step": 17237,
      "training_loss": 7.815066814422607
    },
    {
      "epoch": 0.9343089430894309,
      "step": 17238,
      "training_loss": 8.607983589172363
    },
    {
      "epoch": 0.9343631436314364,
      "step": 17239,
      "training_loss": 7.135603904724121
    },
    {
      "epoch": 0.9344173441734417,
      "grad_norm": 24.83885383605957,
      "learning_rate": 1e-05,
      "loss": 7.5508,
      "step": 17240
    },
    {
      "epoch": 0.9344173441734417,
      "step": 17240,
      "training_loss": 7.229218482971191
    },
    {
      "epoch": 0.9344715447154471,
      "step": 17241,
      "training_loss": 5.64168119430542
    },
    {
      "epoch": 0.9345257452574526,
      "step": 17242,
      "training_loss": 6.963518142700195
    },
    {
      "epoch": 0.934579945799458,
      "step": 17243,
      "training_loss": 6.225335121154785
    },
    {
      "epoch": 0.9346341463414635,
      "grad_norm": 33.99433135986328,
      "learning_rate": 1e-05,
      "loss": 6.5149,
      "step": 17244
    },
    {
      "epoch": 0.9346341463414635,
      "step": 17244,
      "training_loss": 6.328574180603027
    },
    {
      "epoch": 0.9346883468834688,
      "step": 17245,
      "training_loss": 7.322980880737305
    },
    {
      "epoch": 0.9347425474254742,
      "step": 17246,
      "training_loss": 3.733792304992676
    },
    {
      "epoch": 0.9347967479674797,
      "step": 17247,
      "training_loss": 7.2300333976745605
    },
    {
      "epoch": 0.9348509485094851,
      "grad_norm": 26.160120010375977,
      "learning_rate": 1e-05,
      "loss": 6.1538,
      "step": 17248
    },
    {
      "epoch": 0.9348509485094851,
      "step": 17248,
      "training_loss": 6.062936305999756
    },
    {
      "epoch": 0.9349051490514905,
      "step": 17249,
      "training_loss": 7.191648960113525
    },
    {
      "epoch": 0.9349593495934959,
      "step": 17250,
      "training_loss": 6.282596588134766
    },
    {
      "epoch": 0.9350135501355014,
      "step": 17251,
      "training_loss": 5.309631824493408
    },
    {
      "epoch": 0.9350677506775068,
      "grad_norm": 26.370460510253906,
      "learning_rate": 1e-05,
      "loss": 6.2117,
      "step": 17252
    },
    {
      "epoch": 0.9350677506775068,
      "step": 17252,
      "training_loss": 7.464895248413086
    },
    {
      "epoch": 0.9351219512195122,
      "step": 17253,
      "training_loss": 5.7556071281433105
    },
    {
      "epoch": 0.9351761517615176,
      "step": 17254,
      "training_loss": 5.19411039352417
    },
    {
      "epoch": 0.935230352303523,
      "step": 17255,
      "training_loss": 6.898317813873291
    },
    {
      "epoch": 0.9352845528455285,
      "grad_norm": 17.325986862182617,
      "learning_rate": 1e-05,
      "loss": 6.3282,
      "step": 17256
    },
    {
      "epoch": 0.9352845528455285,
      "step": 17256,
      "training_loss": 7.113824844360352
    },
    {
      "epoch": 0.9353387533875339,
      "step": 17257,
      "training_loss": 6.145708084106445
    },
    {
      "epoch": 0.9353929539295393,
      "step": 17258,
      "training_loss": 6.997662544250488
    },
    {
      "epoch": 0.9354471544715447,
      "step": 17259,
      "training_loss": 6.854528427124023
    },
    {
      "epoch": 0.9355013550135501,
      "grad_norm": 22.01521873474121,
      "learning_rate": 1e-05,
      "loss": 6.7779,
      "step": 17260
    },
    {
      "epoch": 0.9355013550135501,
      "step": 17260,
      "training_loss": 7.114531517028809
    },
    {
      "epoch": 0.9355555555555556,
      "step": 17261,
      "training_loss": 6.955248832702637
    },
    {
      "epoch": 0.935609756097561,
      "step": 17262,
      "training_loss": 7.643112659454346
    },
    {
      "epoch": 0.9356639566395664,
      "step": 17263,
      "training_loss": 7.251165390014648
    },
    {
      "epoch": 0.9357181571815718,
      "grad_norm": 31.688701629638672,
      "learning_rate": 1e-05,
      "loss": 7.241,
      "step": 17264
    },
    {
      "epoch": 0.9357181571815718,
      "step": 17264,
      "training_loss": 6.364945411682129
    },
    {
      "epoch": 0.9357723577235773,
      "step": 17265,
      "training_loss": 5.98460054397583
    },
    {
      "epoch": 0.9358265582655827,
      "step": 17266,
      "training_loss": 6.924371242523193
    },
    {
      "epoch": 0.935880758807588,
      "step": 17267,
      "training_loss": 7.108466625213623
    },
    {
      "epoch": 0.9359349593495935,
      "grad_norm": 24.585140228271484,
      "learning_rate": 1e-05,
      "loss": 6.5956,
      "step": 17268
    },
    {
      "epoch": 0.9359349593495935,
      "step": 17268,
      "training_loss": 6.819095134735107
    },
    {
      "epoch": 0.9359891598915989,
      "step": 17269,
      "training_loss": 6.768045902252197
    },
    {
      "epoch": 0.9360433604336044,
      "step": 17270,
      "training_loss": 9.044407844543457
    },
    {
      "epoch": 0.9360975609756098,
      "step": 17271,
      "training_loss": 4.18526554107666
    },
    {
      "epoch": 0.9361517615176151,
      "grad_norm": 23.741424560546875,
      "learning_rate": 1e-05,
      "loss": 6.7042,
      "step": 17272
    },
    {
      "epoch": 0.9361517615176151,
      "step": 17272,
      "training_loss": 5.189750671386719
    },
    {
      "epoch": 0.9362059620596206,
      "step": 17273,
      "training_loss": 5.383898735046387
    },
    {
      "epoch": 0.936260162601626,
      "step": 17274,
      "training_loss": 7.295900821685791
    },
    {
      "epoch": 0.9363143631436315,
      "step": 17275,
      "training_loss": 7.210197925567627
    },
    {
      "epoch": 0.9363685636856368,
      "grad_norm": 22.076326370239258,
      "learning_rate": 1e-05,
      "loss": 6.2699,
      "step": 17276
    },
    {
      "epoch": 0.9363685636856368,
      "step": 17276,
      "training_loss": 5.90986442565918
    },
    {
      "epoch": 0.9364227642276423,
      "step": 17277,
      "training_loss": 6.641894817352295
    },
    {
      "epoch": 0.9364769647696477,
      "step": 17278,
      "training_loss": 6.890562057495117
    },
    {
      "epoch": 0.9365311653116531,
      "step": 17279,
      "training_loss": 6.317035675048828
    },
    {
      "epoch": 0.9365853658536586,
      "grad_norm": 20.989038467407227,
      "learning_rate": 1e-05,
      "loss": 6.4398,
      "step": 17280
    },
    {
      "epoch": 0.9365853658536586,
      "step": 17280,
      "training_loss": 5.339662551879883
    },
    {
      "epoch": 0.9366395663956639,
      "step": 17281,
      "training_loss": 6.495211124420166
    },
    {
      "epoch": 0.9366937669376694,
      "step": 17282,
      "training_loss": 7.224908351898193
    },
    {
      "epoch": 0.9367479674796748,
      "step": 17283,
      "training_loss": 7.1020002365112305
    },
    {
      "epoch": 0.9368021680216803,
      "grad_norm": 25.5203914642334,
      "learning_rate": 1e-05,
      "loss": 6.5404,
      "step": 17284
    },
    {
      "epoch": 0.9368021680216803,
      "step": 17284,
      "training_loss": 6.706918239593506
    },
    {
      "epoch": 0.9368563685636856,
      "step": 17285,
      "training_loss": 5.6838908195495605
    },
    {
      "epoch": 0.936910569105691,
      "step": 17286,
      "training_loss": 5.8492279052734375
    },
    {
      "epoch": 0.9369647696476965,
      "step": 17287,
      "training_loss": 5.837047100067139
    },
    {
      "epoch": 0.9370189701897019,
      "grad_norm": 23.91474151611328,
      "learning_rate": 1e-05,
      "loss": 6.0193,
      "step": 17288
    },
    {
      "epoch": 0.9370189701897019,
      "step": 17288,
      "training_loss": 8.28884506225586
    },
    {
      "epoch": 0.9370731707317074,
      "step": 17289,
      "training_loss": 6.9604363441467285
    },
    {
      "epoch": 0.9371273712737127,
      "step": 17290,
      "training_loss": 7.6021528244018555
    },
    {
      "epoch": 0.9371815718157182,
      "step": 17291,
      "training_loss": 6.67276668548584
    },
    {
      "epoch": 0.9372357723577236,
      "grad_norm": 17.067842483520508,
      "learning_rate": 1e-05,
      "loss": 7.3811,
      "step": 17292
    },
    {
      "epoch": 0.9372357723577236,
      "step": 17292,
      "training_loss": 7.046677112579346
    },
    {
      "epoch": 0.937289972899729,
      "step": 17293,
      "training_loss": 5.710755348205566
    },
    {
      "epoch": 0.9373441734417344,
      "step": 17294,
      "training_loss": 6.785633563995361
    },
    {
      "epoch": 0.9373983739837398,
      "step": 17295,
      "training_loss": 6.720684051513672
    },
    {
      "epoch": 0.9374525745257453,
      "grad_norm": 31.87647819519043,
      "learning_rate": 1e-05,
      "loss": 6.5659,
      "step": 17296
    },
    {
      "epoch": 0.9374525745257453,
      "step": 17296,
      "training_loss": 7.097342014312744
    },
    {
      "epoch": 0.9375067750677507,
      "step": 17297,
      "training_loss": 7.199523448944092
    },
    {
      "epoch": 0.937560975609756,
      "step": 17298,
      "training_loss": 6.525923728942871
    },
    {
      "epoch": 0.9376151761517615,
      "step": 17299,
      "training_loss": 6.507479190826416
    },
    {
      "epoch": 0.9376693766937669,
      "grad_norm": 24.216650009155273,
      "learning_rate": 1e-05,
      "loss": 6.8326,
      "step": 17300
    },
    {
      "epoch": 0.9376693766937669,
      "step": 17300,
      "training_loss": 6.338804244995117
    },
    {
      "epoch": 0.9377235772357724,
      "step": 17301,
      "training_loss": 5.312039375305176
    },
    {
      "epoch": 0.9377777777777778,
      "step": 17302,
      "training_loss": 3.1907851696014404
    },
    {
      "epoch": 0.9378319783197832,
      "step": 17303,
      "training_loss": 4.171834945678711
    },
    {
      "epoch": 0.9378861788617886,
      "grad_norm": 23.99452781677246,
      "learning_rate": 1e-05,
      "loss": 4.7534,
      "step": 17304
    },
    {
      "epoch": 0.9378861788617886,
      "step": 17304,
      "training_loss": 5.44613790512085
    },
    {
      "epoch": 0.937940379403794,
      "step": 17305,
      "training_loss": 6.388174057006836
    },
    {
      "epoch": 0.9379945799457995,
      "step": 17306,
      "training_loss": 5.8189377784729
    },
    {
      "epoch": 0.9380487804878048,
      "step": 17307,
      "training_loss": 6.444884777069092
    },
    {
      "epoch": 0.9381029810298103,
      "grad_norm": 16.88873863220215,
      "learning_rate": 1e-05,
      "loss": 6.0245,
      "step": 17308
    },
    {
      "epoch": 0.9381029810298103,
      "step": 17308,
      "training_loss": 7.2076029777526855
    },
    {
      "epoch": 0.9381571815718157,
      "step": 17309,
      "training_loss": 7.612176418304443
    },
    {
      "epoch": 0.9382113821138212,
      "step": 17310,
      "training_loss": 6.946488857269287
    },
    {
      "epoch": 0.9382655826558266,
      "step": 17311,
      "training_loss": 7.733100891113281
    },
    {
      "epoch": 0.9383197831978319,
      "grad_norm": 14.930325508117676,
      "learning_rate": 1e-05,
      "loss": 7.3748,
      "step": 17312
    },
    {
      "epoch": 0.9383197831978319,
      "step": 17312,
      "training_loss": 6.686645030975342
    },
    {
      "epoch": 0.9383739837398374,
      "step": 17313,
      "training_loss": 5.8348493576049805
    },
    {
      "epoch": 0.9384281842818428,
      "step": 17314,
      "training_loss": 7.946089744567871
    },
    {
      "epoch": 0.9384823848238483,
      "step": 17315,
      "training_loss": 7.322295188903809
    },
    {
      "epoch": 0.9385365853658536,
      "grad_norm": 70.7969741821289,
      "learning_rate": 1e-05,
      "loss": 6.9475,
      "step": 17316
    },
    {
      "epoch": 0.9385365853658536,
      "step": 17316,
      "training_loss": 6.327905178070068
    },
    {
      "epoch": 0.938590785907859,
      "step": 17317,
      "training_loss": 6.023690223693848
    },
    {
      "epoch": 0.9386449864498645,
      "step": 17318,
      "training_loss": 5.352842330932617
    },
    {
      "epoch": 0.9386991869918699,
      "step": 17319,
      "training_loss": 5.989815711975098
    },
    {
      "epoch": 0.9387533875338754,
      "grad_norm": 29.06317138671875,
      "learning_rate": 1e-05,
      "loss": 5.9236,
      "step": 17320
    },
    {
      "epoch": 0.9387533875338754,
      "step": 17320,
      "training_loss": 6.306764125823975
    },
    {
      "epoch": 0.9388075880758807,
      "step": 17321,
      "training_loss": 6.428492546081543
    },
    {
      "epoch": 0.9388617886178862,
      "step": 17322,
      "training_loss": 6.529792785644531
    },
    {
      "epoch": 0.9389159891598916,
      "step": 17323,
      "training_loss": 5.958105564117432
    },
    {
      "epoch": 0.938970189701897,
      "grad_norm": 24.80375862121582,
      "learning_rate": 1e-05,
      "loss": 6.3058,
      "step": 17324
    },
    {
      "epoch": 0.938970189701897,
      "step": 17324,
      "training_loss": 7.495465278625488
    },
    {
      "epoch": 0.9390243902439024,
      "step": 17325,
      "training_loss": 6.27658224105835
    },
    {
      "epoch": 0.9390785907859078,
      "step": 17326,
      "training_loss": 5.603785514831543
    },
    {
      "epoch": 0.9391327913279133,
      "step": 17327,
      "training_loss": 6.457394123077393
    },
    {
      "epoch": 0.9391869918699187,
      "grad_norm": 19.704721450805664,
      "learning_rate": 1e-05,
      "loss": 6.4583,
      "step": 17328
    },
    {
      "epoch": 0.9391869918699187,
      "step": 17328,
      "training_loss": 8.807909965515137
    },
    {
      "epoch": 0.9392411924119242,
      "step": 17329,
      "training_loss": 5.4037346839904785
    },
    {
      "epoch": 0.9392953929539295,
      "step": 17330,
      "training_loss": 5.84697151184082
    },
    {
      "epoch": 0.9393495934959349,
      "step": 17331,
      "training_loss": 7.893060207366943
    },
    {
      "epoch": 0.9394037940379404,
      "grad_norm": 25.232633590698242,
      "learning_rate": 1e-05,
      "loss": 6.9879,
      "step": 17332
    },
    {
      "epoch": 0.9394037940379404,
      "step": 17332,
      "training_loss": 6.924659252166748
    },
    {
      "epoch": 0.9394579945799458,
      "step": 17333,
      "training_loss": 4.3064165115356445
    },
    {
      "epoch": 0.9395121951219512,
      "step": 17334,
      "training_loss": 5.481969833374023
    },
    {
      "epoch": 0.9395663956639566,
      "step": 17335,
      "training_loss": 6.354055881500244
    },
    {
      "epoch": 0.939620596205962,
      "grad_norm": 36.54642868041992,
      "learning_rate": 1e-05,
      "loss": 5.7668,
      "step": 17336
    },
    {
      "epoch": 0.939620596205962,
      "step": 17336,
      "training_loss": 6.747903347015381
    },
    {
      "epoch": 0.9396747967479675,
      "step": 17337,
      "training_loss": 6.528625965118408
    },
    {
      "epoch": 0.939728997289973,
      "step": 17338,
      "training_loss": 6.065795421600342
    },
    {
      "epoch": 0.9397831978319783,
      "step": 17339,
      "training_loss": 7.3375396728515625
    },
    {
      "epoch": 0.9398373983739837,
      "grad_norm": 28.264314651489258,
      "learning_rate": 1e-05,
      "loss": 6.67,
      "step": 17340
    },
    {
      "epoch": 0.9398373983739837,
      "step": 17340,
      "training_loss": 4.402796268463135
    },
    {
      "epoch": 0.9398915989159892,
      "step": 17341,
      "training_loss": 7.31801700592041
    },
    {
      "epoch": 0.9399457994579946,
      "step": 17342,
      "training_loss": 6.362685680389404
    },
    {
      "epoch": 0.94,
      "step": 17343,
      "training_loss": 5.790699005126953
    },
    {
      "epoch": 0.9400542005420054,
      "grad_norm": 22.769216537475586,
      "learning_rate": 1e-05,
      "loss": 5.9685,
      "step": 17344
    },
    {
      "epoch": 0.9400542005420054,
      "step": 17344,
      "training_loss": 7.040874481201172
    },
    {
      "epoch": 0.9401084010840108,
      "step": 17345,
      "training_loss": 7.267248630523682
    },
    {
      "epoch": 0.9401626016260163,
      "step": 17346,
      "training_loss": 6.277000904083252
    },
    {
      "epoch": 0.9402168021680217,
      "step": 17347,
      "training_loss": 6.9124274253845215
    },
    {
      "epoch": 0.9402710027100271,
      "grad_norm": 36.69921875,
      "learning_rate": 1e-05,
      "loss": 6.8744,
      "step": 17348
    },
    {
      "epoch": 0.9402710027100271,
      "step": 17348,
      "training_loss": 6.6585822105407715
    },
    {
      "epoch": 0.9403252032520325,
      "step": 17349,
      "training_loss": 2.677229642868042
    },
    {
      "epoch": 0.940379403794038,
      "step": 17350,
      "training_loss": 7.660171031951904
    },
    {
      "epoch": 0.9404336043360434,
      "step": 17351,
      "training_loss": 6.859852313995361
    },
    {
      "epoch": 0.9404878048780487,
      "grad_norm": 22.84429168701172,
      "learning_rate": 1e-05,
      "loss": 5.964,
      "step": 17352
    },
    {
      "epoch": 0.9404878048780487,
      "step": 17352,
      "training_loss": 6.269724369049072
    },
    {
      "epoch": 0.9405420054200542,
      "step": 17353,
      "training_loss": 7.261903762817383
    },
    {
      "epoch": 0.9405962059620596,
      "step": 17354,
      "training_loss": 6.631809234619141
    },
    {
      "epoch": 0.9406504065040651,
      "step": 17355,
      "training_loss": 6.856350421905518
    },
    {
      "epoch": 0.9407046070460705,
      "grad_norm": 31.222841262817383,
      "learning_rate": 1e-05,
      "loss": 6.7549,
      "step": 17356
    },
    {
      "epoch": 0.9407046070460705,
      "step": 17356,
      "training_loss": 7.848769664764404
    },
    {
      "epoch": 0.9407588075880758,
      "step": 17357,
      "training_loss": 5.643946170806885
    },
    {
      "epoch": 0.9408130081300813,
      "step": 17358,
      "training_loss": 5.534313678741455
    },
    {
      "epoch": 0.9408672086720867,
      "step": 17359,
      "training_loss": 6.385719299316406
    },
    {
      "epoch": 0.9409214092140922,
      "grad_norm": 30.729318618774414,
      "learning_rate": 1e-05,
      "loss": 6.3532,
      "step": 17360
    },
    {
      "epoch": 0.9409214092140922,
      "step": 17360,
      "training_loss": 6.424167156219482
    },
    {
      "epoch": 0.9409756097560975,
      "step": 17361,
      "training_loss": 4.1766180992126465
    },
    {
      "epoch": 0.941029810298103,
      "step": 17362,
      "training_loss": 6.601290225982666
    },
    {
      "epoch": 0.9410840108401084,
      "step": 17363,
      "training_loss": 6.402226448059082
    },
    {
      "epoch": 0.9411382113821138,
      "grad_norm": 18.321746826171875,
      "learning_rate": 1e-05,
      "loss": 5.9011,
      "step": 17364
    },
    {
      "epoch": 0.9411382113821138,
      "step": 17364,
      "training_loss": 6.402707576751709
    },
    {
      "epoch": 0.9411924119241193,
      "step": 17365,
      "training_loss": 7.930643558502197
    },
    {
      "epoch": 0.9412466124661246,
      "step": 17366,
      "training_loss": 7.335303783416748
    },
    {
      "epoch": 0.9413008130081301,
      "step": 17367,
      "training_loss": 6.509946346282959
    },
    {
      "epoch": 0.9413550135501355,
      "grad_norm": 25.765352249145508,
      "learning_rate": 1e-05,
      "loss": 7.0447,
      "step": 17368
    },
    {
      "epoch": 0.9413550135501355,
      "step": 17368,
      "training_loss": 6.518472671508789
    },
    {
      "epoch": 0.941409214092141,
      "step": 17369,
      "training_loss": 7.4833784103393555
    },
    {
      "epoch": 0.9414634146341463,
      "step": 17370,
      "training_loss": 7.700902462005615
    },
    {
      "epoch": 0.9415176151761517,
      "step": 17371,
      "training_loss": 5.763189315795898
    },
    {
      "epoch": 0.9415718157181572,
      "grad_norm": 29.638084411621094,
      "learning_rate": 1e-05,
      "loss": 6.8665,
      "step": 17372
    },
    {
      "epoch": 0.9415718157181572,
      "step": 17372,
      "training_loss": 7.946856498718262
    },
    {
      "epoch": 0.9416260162601626,
      "step": 17373,
      "training_loss": 5.103333950042725
    },
    {
      "epoch": 0.9416802168021681,
      "step": 17374,
      "training_loss": 6.404636383056641
    },
    {
      "epoch": 0.9417344173441734,
      "step": 17375,
      "training_loss": 7.493978500366211
    },
    {
      "epoch": 0.9417886178861788,
      "grad_norm": 21.66266632080078,
      "learning_rate": 1e-05,
      "loss": 6.7372,
      "step": 17376
    },
    {
      "epoch": 0.9417886178861788,
      "step": 17376,
      "training_loss": 5.47446346282959
    },
    {
      "epoch": 0.9418428184281843,
      "step": 17377,
      "training_loss": 7.008041858673096
    },
    {
      "epoch": 0.9418970189701897,
      "step": 17378,
      "training_loss": 6.987682342529297
    },
    {
      "epoch": 0.9419512195121951,
      "step": 17379,
      "training_loss": 5.096679210662842
    },
    {
      "epoch": 0.9420054200542005,
      "grad_norm": 26.8146915435791,
      "learning_rate": 1e-05,
      "loss": 6.1417,
      "step": 17380
    },
    {
      "epoch": 0.9420054200542005,
      "step": 17380,
      "training_loss": 6.985410213470459
    },
    {
      "epoch": 0.942059620596206,
      "step": 17381,
      "training_loss": 6.869719982147217
    },
    {
      "epoch": 0.9421138211382114,
      "step": 17382,
      "training_loss": 5.746366500854492
    },
    {
      "epoch": 0.9421680216802168,
      "step": 17383,
      "training_loss": 6.716081142425537
    },
    {
      "epoch": 0.9422222222222222,
      "grad_norm": 22.746715545654297,
      "learning_rate": 1e-05,
      "loss": 6.5794,
      "step": 17384
    },
    {
      "epoch": 0.9422222222222222,
      "step": 17384,
      "training_loss": 7.243890762329102
    },
    {
      "epoch": 0.9422764227642276,
      "step": 17385,
      "training_loss": 6.69848108291626
    },
    {
      "epoch": 0.9423306233062331,
      "step": 17386,
      "training_loss": 6.998265266418457
    },
    {
      "epoch": 0.9423848238482385,
      "step": 17387,
      "training_loss": 6.273098945617676
    },
    {
      "epoch": 0.9424390243902439,
      "grad_norm": 18.6175594329834,
      "learning_rate": 1e-05,
      "loss": 6.8034,
      "step": 17388
    },
    {
      "epoch": 0.9424390243902439,
      "step": 17388,
      "training_loss": 6.611409664154053
    },
    {
      "epoch": 0.9424932249322493,
      "step": 17389,
      "training_loss": 7.437123775482178
    },
    {
      "epoch": 0.9425474254742547,
      "step": 17390,
      "training_loss": 6.117801189422607
    },
    {
      "epoch": 0.9426016260162602,
      "step": 17391,
      "training_loss": 6.51128625869751
    },
    {
      "epoch": 0.9426558265582656,
      "grad_norm": 46.391475677490234,
      "learning_rate": 1e-05,
      "loss": 6.6694,
      "step": 17392
    },
    {
      "epoch": 0.9426558265582656,
      "step": 17392,
      "training_loss": 7.812600135803223
    },
    {
      "epoch": 0.942710027100271,
      "step": 17393,
      "training_loss": 5.587010860443115
    },
    {
      "epoch": 0.9427642276422764,
      "step": 17394,
      "training_loss": 6.223522186279297
    },
    {
      "epoch": 0.9428184281842819,
      "step": 17395,
      "training_loss": 7.1380934715271
    },
    {
      "epoch": 0.9428726287262873,
      "grad_norm": 19.28438377380371,
      "learning_rate": 1e-05,
      "loss": 6.6903,
      "step": 17396
    },
    {
      "epoch": 0.9428726287262873,
      "step": 17396,
      "training_loss": 6.9192914962768555
    },
    {
      "epoch": 0.9429268292682926,
      "step": 17397,
      "training_loss": 7.884478569030762
    },
    {
      "epoch": 0.9429810298102981,
      "step": 17398,
      "training_loss": 6.441137790679932
    },
    {
      "epoch": 0.9430352303523035,
      "step": 17399,
      "training_loss": 5.326022624969482
    },
    {
      "epoch": 0.943089430894309,
      "grad_norm": 55.68275451660156,
      "learning_rate": 1e-05,
      "loss": 6.6427,
      "step": 17400
    },
    {
      "epoch": 0.943089430894309,
      "step": 17400,
      "training_loss": 3.2000951766967773
    },
    {
      "epoch": 0.9431436314363144,
      "step": 17401,
      "training_loss": 5.814675331115723
    },
    {
      "epoch": 0.9431978319783197,
      "step": 17402,
      "training_loss": 7.679358959197998
    },
    {
      "epoch": 0.9432520325203252,
      "step": 17403,
      "training_loss": 4.1404314041137695
    },
    {
      "epoch": 0.9433062330623306,
      "grad_norm": 23.964447021484375,
      "learning_rate": 1e-05,
      "loss": 5.2086,
      "step": 17404
    },
    {
      "epoch": 0.9433062330623306,
      "step": 17404,
      "training_loss": 5.688193321228027
    },
    {
      "epoch": 0.9433604336043361,
      "step": 17405,
      "training_loss": 5.9849629402160645
    },
    {
      "epoch": 0.9434146341463414,
      "step": 17406,
      "training_loss": 7.365144729614258
    },
    {
      "epoch": 0.9434688346883469,
      "step": 17407,
      "training_loss": 6.192385196685791
    },
    {
      "epoch": 0.9435230352303523,
      "grad_norm": 29.632104873657227,
      "learning_rate": 1e-05,
      "loss": 6.3077,
      "step": 17408
    },
    {
      "epoch": 0.9435230352303523,
      "step": 17408,
      "training_loss": 6.893157005310059
    },
    {
      "epoch": 0.9435772357723577,
      "step": 17409,
      "training_loss": 3.6161954402923584
    },
    {
      "epoch": 0.9436314363143632,
      "step": 17410,
      "training_loss": 4.903752326965332
    },
    {
      "epoch": 0.9436856368563685,
      "step": 17411,
      "training_loss": 4.208837985992432
    },
    {
      "epoch": 0.943739837398374,
      "grad_norm": 53.9763298034668,
      "learning_rate": 1e-05,
      "loss": 4.9055,
      "step": 17412
    },
    {
      "epoch": 0.943739837398374,
      "step": 17412,
      "training_loss": 6.404522895812988
    },
    {
      "epoch": 0.9437940379403794,
      "step": 17413,
      "training_loss": 5.330899715423584
    },
    {
      "epoch": 0.9438482384823849,
      "step": 17414,
      "training_loss": 6.602198123931885
    },
    {
      "epoch": 0.9439024390243902,
      "step": 17415,
      "training_loss": 6.547428131103516
    },
    {
      "epoch": 0.9439566395663956,
      "grad_norm": 25.48054313659668,
      "learning_rate": 1e-05,
      "loss": 6.2213,
      "step": 17416
    },
    {
      "epoch": 0.9439566395663956,
      "step": 17416,
      "training_loss": 6.443385601043701
    },
    {
      "epoch": 0.9440108401084011,
      "step": 17417,
      "training_loss": 7.7796173095703125
    },
    {
      "epoch": 0.9440650406504065,
      "step": 17418,
      "training_loss": 6.336180210113525
    },
    {
      "epoch": 0.944119241192412,
      "step": 17419,
      "training_loss": 7.350391864776611
    },
    {
      "epoch": 0.9441734417344173,
      "grad_norm": 16.02443504333496,
      "learning_rate": 1e-05,
      "loss": 6.9774,
      "step": 17420
    },
    {
      "epoch": 0.9441734417344173,
      "step": 17420,
      "training_loss": 7.249523639678955
    },
    {
      "epoch": 0.9442276422764228,
      "step": 17421,
      "training_loss": 5.97153902053833
    },
    {
      "epoch": 0.9442818428184282,
      "step": 17422,
      "training_loss": 7.367810249328613
    },
    {
      "epoch": 0.9443360433604336,
      "step": 17423,
      "training_loss": 6.752458572387695
    },
    {
      "epoch": 0.944390243902439,
      "grad_norm": 25.69053077697754,
      "learning_rate": 1e-05,
      "loss": 6.8353,
      "step": 17424
    },
    {
      "epoch": 0.944390243902439,
      "step": 17424,
      "training_loss": 6.466093063354492
    },
    {
      "epoch": 0.9444444444444444,
      "step": 17425,
      "training_loss": 6.975950717926025
    },
    {
      "epoch": 0.9444986449864499,
      "step": 17426,
      "training_loss": 6.272905349731445
    },
    {
      "epoch": 0.9445528455284553,
      "step": 17427,
      "training_loss": 4.598898410797119
    },
    {
      "epoch": 0.9446070460704608,
      "grad_norm": 36.32880783081055,
      "learning_rate": 1e-05,
      "loss": 6.0785,
      "step": 17428
    },
    {
      "epoch": 0.9446070460704608,
      "step": 17428,
      "training_loss": 7.166340351104736
    },
    {
      "epoch": 0.9446612466124661,
      "step": 17429,
      "training_loss": 4.923799991607666
    },
    {
      "epoch": 0.9447154471544715,
      "step": 17430,
      "training_loss": 7.593258380889893
    },
    {
      "epoch": 0.944769647696477,
      "step": 17431,
      "training_loss": 6.2912278175354
    },
    {
      "epoch": 0.9448238482384824,
      "grad_norm": 23.913419723510742,
      "learning_rate": 1e-05,
      "loss": 6.4937,
      "step": 17432
    },
    {
      "epoch": 0.9448238482384824,
      "step": 17432,
      "training_loss": 7.2849273681640625
    },
    {
      "epoch": 0.9448780487804878,
      "step": 17433,
      "training_loss": 6.870813369750977
    },
    {
      "epoch": 0.9449322493224932,
      "step": 17434,
      "training_loss": 5.728733539581299
    },
    {
      "epoch": 0.9449864498644986,
      "step": 17435,
      "training_loss": 7.73411750793457
    },
    {
      "epoch": 0.9450406504065041,
      "grad_norm": 22.79566764831543,
      "learning_rate": 1e-05,
      "loss": 6.9046,
      "step": 17436
    },
    {
      "epoch": 0.9450406504065041,
      "step": 17436,
      "training_loss": 5.9284563064575195
    },
    {
      "epoch": 0.9450948509485095,
      "step": 17437,
      "training_loss": 6.685798168182373
    },
    {
      "epoch": 0.9451490514905149,
      "step": 17438,
      "training_loss": 6.302500247955322
    },
    {
      "epoch": 0.9452032520325203,
      "step": 17439,
      "training_loss": 7.000115871429443
    },
    {
      "epoch": 0.9452574525745258,
      "grad_norm": 43.70185470581055,
      "learning_rate": 1e-05,
      "loss": 6.4792,
      "step": 17440
    },
    {
      "epoch": 0.9452574525745258,
      "step": 17440,
      "training_loss": 6.924924850463867
    },
    {
      "epoch": 0.9453116531165312,
      "step": 17441,
      "training_loss": 8.097832679748535
    },
    {
      "epoch": 0.9453658536585365,
      "step": 17442,
      "training_loss": 7.005626678466797
    },
    {
      "epoch": 0.945420054200542,
      "step": 17443,
      "training_loss": 7.212348937988281
    },
    {
      "epoch": 0.9454742547425474,
      "grad_norm": 22.94857406616211,
      "learning_rate": 1e-05,
      "loss": 7.3102,
      "step": 17444
    },
    {
      "epoch": 0.9454742547425474,
      "step": 17444,
      "training_loss": 6.328178882598877
    },
    {
      "epoch": 0.9455284552845529,
      "step": 17445,
      "training_loss": 7.424506187438965
    },
    {
      "epoch": 0.9455826558265583,
      "step": 17446,
      "training_loss": 6.925844669342041
    },
    {
      "epoch": 0.9456368563685636,
      "step": 17447,
      "training_loss": 7.418848037719727
    },
    {
      "epoch": 0.9456910569105691,
      "grad_norm": 34.311363220214844,
      "learning_rate": 1e-05,
      "loss": 7.0243,
      "step": 17448
    },
    {
      "epoch": 0.9456910569105691,
      "step": 17448,
      "training_loss": 6.166919708251953
    },
    {
      "epoch": 0.9457452574525745,
      "step": 17449,
      "training_loss": 6.9728288650512695
    },
    {
      "epoch": 0.94579945799458,
      "step": 17450,
      "training_loss": 6.540329456329346
    },
    {
      "epoch": 0.9458536585365853,
      "step": 17451,
      "training_loss": 6.7962212562561035
    },
    {
      "epoch": 0.9459078590785908,
      "grad_norm": 25.0382137298584,
      "learning_rate": 1e-05,
      "loss": 6.6191,
      "step": 17452
    },
    {
      "epoch": 0.9459078590785908,
      "step": 17452,
      "training_loss": 5.926795482635498
    },
    {
      "epoch": 0.9459620596205962,
      "step": 17453,
      "training_loss": 5.3261518478393555
    },
    {
      "epoch": 0.9460162601626017,
      "step": 17454,
      "training_loss": 5.162550926208496
    },
    {
      "epoch": 0.9460704607046071,
      "step": 17455,
      "training_loss": 6.3420281410217285
    },
    {
      "epoch": 0.9461246612466124,
      "grad_norm": 27.554080963134766,
      "learning_rate": 1e-05,
      "loss": 5.6894,
      "step": 17456
    },
    {
      "epoch": 0.9461246612466124,
      "step": 17456,
      "training_loss": 6.098634243011475
    },
    {
      "epoch": 0.9461788617886179,
      "step": 17457,
      "training_loss": 4.988584518432617
    },
    {
      "epoch": 0.9462330623306233,
      "step": 17458,
      "training_loss": 7.577514171600342
    },
    {
      "epoch": 0.9462872628726288,
      "step": 17459,
      "training_loss": 9.684486389160156
    },
    {
      "epoch": 0.9463414634146341,
      "grad_norm": 65.11131286621094,
      "learning_rate": 1e-05,
      "loss": 7.0873,
      "step": 17460
    },
    {
      "epoch": 0.9463414634146341,
      "step": 17460,
      "training_loss": 6.730255603790283
    },
    {
      "epoch": 0.9463956639566395,
      "step": 17461,
      "training_loss": 7.1178975105285645
    },
    {
      "epoch": 0.946449864498645,
      "step": 17462,
      "training_loss": 5.932468891143799
    },
    {
      "epoch": 0.9465040650406504,
      "step": 17463,
      "training_loss": 6.571873188018799
    },
    {
      "epoch": 0.9465582655826559,
      "grad_norm": 23.440221786499023,
      "learning_rate": 1e-05,
      "loss": 6.5881,
      "step": 17464
    },
    {
      "epoch": 0.9465582655826559,
      "step": 17464,
      "training_loss": 7.829785346984863
    },
    {
      "epoch": 0.9466124661246612,
      "step": 17465,
      "training_loss": 5.805186748504639
    },
    {
      "epoch": 0.9466666666666667,
      "step": 17466,
      "training_loss": 6.150819778442383
    },
    {
      "epoch": 0.9467208672086721,
      "step": 17467,
      "training_loss": 7.694072723388672
    },
    {
      "epoch": 0.9467750677506775,
      "grad_norm": 35.312435150146484,
      "learning_rate": 1e-05,
      "loss": 6.87,
      "step": 17468
    },
    {
      "epoch": 0.9467750677506775,
      "step": 17468,
      "training_loss": 6.8198676109313965
    },
    {
      "epoch": 0.9468292682926829,
      "step": 17469,
      "training_loss": 7.8334879875183105
    },
    {
      "epoch": 0.9468834688346883,
      "step": 17470,
      "training_loss": 3.8294131755828857
    },
    {
      "epoch": 0.9469376693766938,
      "step": 17471,
      "training_loss": 7.6653828620910645
    },
    {
      "epoch": 0.9469918699186992,
      "grad_norm": 43.76546859741211,
      "learning_rate": 1e-05,
      "loss": 6.537,
      "step": 17472
    },
    {
      "epoch": 0.9469918699186992,
      "step": 17472,
      "training_loss": 5.722537040710449
    },
    {
      "epoch": 0.9470460704607047,
      "step": 17473,
      "training_loss": 7.043227195739746
    },
    {
      "epoch": 0.94710027100271,
      "step": 17474,
      "training_loss": 7.0002970695495605
    },
    {
      "epoch": 0.9471544715447154,
      "step": 17475,
      "training_loss": 5.93166971206665
    },
    {
      "epoch": 0.9472086720867209,
      "grad_norm": 62.78177261352539,
      "learning_rate": 1e-05,
      "loss": 6.4244,
      "step": 17476
    },
    {
      "epoch": 0.9472086720867209,
      "step": 17476,
      "training_loss": 7.133071422576904
    },
    {
      "epoch": 0.9472628726287263,
      "step": 17477,
      "training_loss": 6.755654335021973
    },
    {
      "epoch": 0.9473170731707317,
      "step": 17478,
      "training_loss": 6.258896350860596
    },
    {
      "epoch": 0.9473712737127371,
      "step": 17479,
      "training_loss": 6.16718864440918
    },
    {
      "epoch": 0.9474254742547426,
      "grad_norm": 23.06072235107422,
      "learning_rate": 1e-05,
      "loss": 6.5787,
      "step": 17480
    },
    {
      "epoch": 0.9474254742547426,
      "step": 17480,
      "training_loss": 6.820865154266357
    },
    {
      "epoch": 0.947479674796748,
      "step": 17481,
      "training_loss": 4.746706008911133
    },
    {
      "epoch": 0.9475338753387534,
      "step": 17482,
      "training_loss": 6.494455814361572
    },
    {
      "epoch": 0.9475880758807588,
      "step": 17483,
      "training_loss": 7.198578357696533
    },
    {
      "epoch": 0.9476422764227642,
      "grad_norm": 16.917484283447266,
      "learning_rate": 1e-05,
      "loss": 6.3152,
      "step": 17484
    },
    {
      "epoch": 0.9476422764227642,
      "step": 17484,
      "training_loss": 8.21493148803711
    },
    {
      "epoch": 0.9476964769647697,
      "step": 17485,
      "training_loss": 5.789470672607422
    },
    {
      "epoch": 0.9477506775067751,
      "step": 17486,
      "training_loss": 6.530734539031982
    },
    {
      "epoch": 0.9478048780487804,
      "step": 17487,
      "training_loss": 4.339235782623291
    },
    {
      "epoch": 0.9478590785907859,
      "grad_norm": 29.084943771362305,
      "learning_rate": 1e-05,
      "loss": 6.2186,
      "step": 17488
    },
    {
      "epoch": 0.9478590785907859,
      "step": 17488,
      "training_loss": 6.40536642074585
    },
    {
      "epoch": 0.9479132791327913,
      "step": 17489,
      "training_loss": 6.381688594818115
    },
    {
      "epoch": 0.9479674796747968,
      "step": 17490,
      "training_loss": 6.744743824005127
    },
    {
      "epoch": 0.9480216802168022,
      "step": 17491,
      "training_loss": 7.457237720489502
    },
    {
      "epoch": 0.9480758807588076,
      "grad_norm": 29.8011417388916,
      "learning_rate": 1e-05,
      "loss": 6.7473,
      "step": 17492
    },
    {
      "epoch": 0.9480758807588076,
      "step": 17492,
      "training_loss": 6.575930118560791
    },
    {
      "epoch": 0.948130081300813,
      "step": 17493,
      "training_loss": 7.485442161560059
    },
    {
      "epoch": 0.9481842818428184,
      "step": 17494,
      "training_loss": 6.356297969818115
    },
    {
      "epoch": 0.9482384823848239,
      "step": 17495,
      "training_loss": 7.252878189086914
    },
    {
      "epoch": 0.9482926829268292,
      "grad_norm": 18.93907356262207,
      "learning_rate": 1e-05,
      "loss": 6.9176,
      "step": 17496
    },
    {
      "epoch": 0.9482926829268292,
      "step": 17496,
      "training_loss": 5.489582061767578
    },
    {
      "epoch": 0.9483468834688347,
      "step": 17497,
      "training_loss": 6.895717144012451
    },
    {
      "epoch": 0.9484010840108401,
      "step": 17498,
      "training_loss": 6.685844898223877
    },
    {
      "epoch": 0.9484552845528456,
      "step": 17499,
      "training_loss": 4.753807067871094
    },
    {
      "epoch": 0.948509485094851,
      "grad_norm": 25.328920364379883,
      "learning_rate": 1e-05,
      "loss": 5.9562,
      "step": 17500
    },
    {
      "epoch": 0.948509485094851,
      "step": 17500,
      "training_loss": 5.2401299476623535
    },
    {
      "epoch": 0.9485636856368563,
      "step": 17501,
      "training_loss": 4.516141891479492
    },
    {
      "epoch": 0.9486178861788618,
      "step": 17502,
      "training_loss": 5.356496810913086
    },
    {
      "epoch": 0.9486720867208672,
      "step": 17503,
      "training_loss": 5.51460599899292
    },
    {
      "epoch": 0.9487262872628727,
      "grad_norm": 56.03617477416992,
      "learning_rate": 1e-05,
      "loss": 5.1568,
      "step": 17504
    },
    {
      "epoch": 0.9487262872628727,
      "step": 17504,
      "training_loss": 5.371634483337402
    },
    {
      "epoch": 0.948780487804878,
      "step": 17505,
      "training_loss": 3.653618812561035
    },
    {
      "epoch": 0.9488346883468834,
      "step": 17506,
      "training_loss": 5.512812614440918
    },
    {
      "epoch": 0.9488888888888889,
      "step": 17507,
      "training_loss": 3.4149744510650635
    },
    {
      "epoch": 0.9489430894308943,
      "grad_norm": 25.09244155883789,
      "learning_rate": 1e-05,
      "loss": 4.4883,
      "step": 17508
    },
    {
      "epoch": 0.9489430894308943,
      "step": 17508,
      "training_loss": 5.279371738433838
    },
    {
      "epoch": 0.9489972899728998,
      "step": 17509,
      "training_loss": 2.731024980545044
    },
    {
      "epoch": 0.9490514905149051,
      "step": 17510,
      "training_loss": 4.498807907104492
    },
    {
      "epoch": 0.9491056910569106,
      "step": 17511,
      "training_loss": 6.290599346160889
    },
    {
      "epoch": 0.949159891598916,
      "grad_norm": 32.66504669189453,
      "learning_rate": 1e-05,
      "loss": 4.7,
      "step": 17512
    },
    {
      "epoch": 0.949159891598916,
      "step": 17512,
      "training_loss": 7.466775894165039
    },
    {
      "epoch": 0.9492140921409215,
      "step": 17513,
      "training_loss": 3.282374382019043
    },
    {
      "epoch": 0.9492682926829268,
      "step": 17514,
      "training_loss": 6.7292070388793945
    },
    {
      "epoch": 0.9493224932249322,
      "step": 17515,
      "training_loss": 5.1274189949035645
    },
    {
      "epoch": 0.9493766937669377,
      "grad_norm": 38.026790618896484,
      "learning_rate": 1e-05,
      "loss": 5.6514,
      "step": 17516
    },
    {
      "epoch": 0.9493766937669377,
      "step": 17516,
      "training_loss": 3.198716163635254
    },
    {
      "epoch": 0.9494308943089431,
      "step": 17517,
      "training_loss": 6.662600517272949
    },
    {
      "epoch": 0.9494850948509486,
      "step": 17518,
      "training_loss": 5.43733549118042
    },
    {
      "epoch": 0.9495392953929539,
      "step": 17519,
      "training_loss": 6.761172771453857
    },
    {
      "epoch": 0.9495934959349593,
      "grad_norm": 19.409290313720703,
      "learning_rate": 1e-05,
      "loss": 5.515,
      "step": 17520
    },
    {
      "epoch": 0.9495934959349593,
      "step": 17520,
      "training_loss": 6.072505950927734
    },
    {
      "epoch": 0.9496476964769648,
      "step": 17521,
      "training_loss": 6.478883743286133
    },
    {
      "epoch": 0.9497018970189702,
      "step": 17522,
      "training_loss": 7.534989833831787
    },
    {
      "epoch": 0.9497560975609756,
      "step": 17523,
      "training_loss": 6.408773899078369
    },
    {
      "epoch": 0.949810298102981,
      "grad_norm": 31.492778778076172,
      "learning_rate": 1e-05,
      "loss": 6.6238,
      "step": 17524
    },
    {
      "epoch": 0.949810298102981,
      "step": 17524,
      "training_loss": 5.296749114990234
    },
    {
      "epoch": 0.9498644986449865,
      "step": 17525,
      "training_loss": 5.9172892570495605
    },
    {
      "epoch": 0.9499186991869919,
      "step": 17526,
      "training_loss": 6.310299396514893
    },
    {
      "epoch": 0.9499728997289973,
      "step": 17527,
      "training_loss": 7.445979595184326
    },
    {
      "epoch": 0.9500271002710027,
      "grad_norm": 44.51712417602539,
      "learning_rate": 1e-05,
      "loss": 6.2426,
      "step": 17528
    },
    {
      "epoch": 0.9500271002710027,
      "step": 17528,
      "training_loss": 2.8517842292785645
    },
    {
      "epoch": 0.9500813008130081,
      "step": 17529,
      "training_loss": 5.4158735275268555
    },
    {
      "epoch": 0.9501355013550136,
      "step": 17530,
      "training_loss": 6.505314826965332
    },
    {
      "epoch": 0.950189701897019,
      "step": 17531,
      "training_loss": 5.495737552642822
    },
    {
      "epoch": 0.9502439024390243,
      "grad_norm": 34.78562545776367,
      "learning_rate": 1e-05,
      "loss": 5.0672,
      "step": 17532
    },
    {
      "epoch": 0.9502439024390243,
      "step": 17532,
      "training_loss": 6.828556537628174
    },
    {
      "epoch": 0.9502981029810298,
      "step": 17533,
      "training_loss": 5.785889148712158
    },
    {
      "epoch": 0.9503523035230352,
      "step": 17534,
      "training_loss": 6.457787990570068
    },
    {
      "epoch": 0.9504065040650407,
      "step": 17535,
      "training_loss": 6.184162139892578
    },
    {
      "epoch": 0.9504607046070461,
      "grad_norm": 31.56157112121582,
      "learning_rate": 1e-05,
      "loss": 6.3141,
      "step": 17536
    },
    {
      "epoch": 0.9504607046070461,
      "step": 17536,
      "training_loss": 6.8115620613098145
    },
    {
      "epoch": 0.9505149051490515,
      "step": 17537,
      "training_loss": 5.501992702484131
    },
    {
      "epoch": 0.9505691056910569,
      "step": 17538,
      "training_loss": 5.997304916381836
    },
    {
      "epoch": 0.9506233062330623,
      "step": 17539,
      "training_loss": 6.9300923347473145
    },
    {
      "epoch": 0.9506775067750678,
      "grad_norm": 17.457277297973633,
      "learning_rate": 1e-05,
      "loss": 6.3102,
      "step": 17540
    },
    {
      "epoch": 0.9506775067750678,
      "step": 17540,
      "training_loss": 7.088762283325195
    },
    {
      "epoch": 0.9507317073170731,
      "step": 17541,
      "training_loss": 4.758543491363525
    },
    {
      "epoch": 0.9507859078590786,
      "step": 17542,
      "training_loss": 7.565730094909668
    },
    {
      "epoch": 0.950840108401084,
      "step": 17543,
      "training_loss": 7.060309410095215
    },
    {
      "epoch": 0.9508943089430895,
      "grad_norm": 62.90354919433594,
      "learning_rate": 1e-05,
      "loss": 6.6183,
      "step": 17544
    },
    {
      "epoch": 0.9508943089430895,
      "step": 17544,
      "training_loss": 5.133127689361572
    },
    {
      "epoch": 0.9509485094850949,
      "step": 17545,
      "training_loss": 5.103358268737793
    },
    {
      "epoch": 0.9510027100271002,
      "step": 17546,
      "training_loss": 6.5818915367126465
    },
    {
      "epoch": 0.9510569105691057,
      "step": 17547,
      "training_loss": 6.597532749176025
    },
    {
      "epoch": 0.9511111111111111,
      "grad_norm": 45.24448776245117,
      "learning_rate": 1e-05,
      "loss": 5.854,
      "step": 17548
    },
    {
      "epoch": 0.9511111111111111,
      "step": 17548,
      "training_loss": 3.7281196117401123
    },
    {
      "epoch": 0.9511653116531166,
      "step": 17549,
      "training_loss": 7.99740743637085
    },
    {
      "epoch": 0.9512195121951219,
      "step": 17550,
      "training_loss": 6.670844078063965
    },
    {
      "epoch": 0.9512737127371274,
      "step": 17551,
      "training_loss": 6.797140121459961
    },
    {
      "epoch": 0.9513279132791328,
      "grad_norm": 26.244901657104492,
      "learning_rate": 1e-05,
      "loss": 6.2984,
      "step": 17552
    },
    {
      "epoch": 0.9513279132791328,
      "step": 17552,
      "training_loss": 6.517581462860107
    },
    {
      "epoch": 0.9513821138211382,
      "step": 17553,
      "training_loss": 7.649979591369629
    },
    {
      "epoch": 0.9514363143631436,
      "step": 17554,
      "training_loss": 6.365146636962891
    },
    {
      "epoch": 0.951490514905149,
      "step": 17555,
      "training_loss": 7.084537029266357
    },
    {
      "epoch": 0.9515447154471545,
      "grad_norm": 30.53980827331543,
      "learning_rate": 1e-05,
      "loss": 6.9043,
      "step": 17556
    },
    {
      "epoch": 0.9515447154471545,
      "step": 17556,
      "training_loss": 6.971850395202637
    },
    {
      "epoch": 0.9515989159891599,
      "step": 17557,
      "training_loss": 7.2493510246276855
    },
    {
      "epoch": 0.9516531165311654,
      "step": 17558,
      "training_loss": 6.628815174102783
    },
    {
      "epoch": 0.9517073170731707,
      "step": 17559,
      "training_loss": 5.270083427429199
    },
    {
      "epoch": 0.9517615176151761,
      "grad_norm": 18.649757385253906,
      "learning_rate": 1e-05,
      "loss": 6.53,
      "step": 17560
    },
    {
      "epoch": 0.9517615176151761,
      "step": 17560,
      "training_loss": 6.122105121612549
    },
    {
      "epoch": 0.9518157181571816,
      "step": 17561,
      "training_loss": 6.552371501922607
    },
    {
      "epoch": 0.951869918699187,
      "step": 17562,
      "training_loss": 7.117702007293701
    },
    {
      "epoch": 0.9519241192411924,
      "step": 17563,
      "training_loss": 7.440861701965332
    },
    {
      "epoch": 0.9519783197831978,
      "grad_norm": 23.95174789428711,
      "learning_rate": 1e-05,
      "loss": 6.8083,
      "step": 17564
    },
    {
      "epoch": 0.9519783197831978,
      "step": 17564,
      "training_loss": 6.298675537109375
    },
    {
      "epoch": 0.9520325203252032,
      "step": 17565,
      "training_loss": 5.890688896179199
    },
    {
      "epoch": 0.9520867208672087,
      "step": 17566,
      "training_loss": 8.062024116516113
    },
    {
      "epoch": 0.9521409214092141,
      "step": 17567,
      "training_loss": 6.05716609954834
    },
    {
      "epoch": 0.9521951219512195,
      "grad_norm": 25.795175552368164,
      "learning_rate": 1e-05,
      "loss": 6.5771,
      "step": 17568
    },
    {
      "epoch": 0.9521951219512195,
      "step": 17568,
      "training_loss": 5.1555891036987305
    },
    {
      "epoch": 0.9522493224932249,
      "step": 17569,
      "training_loss": 5.703198432922363
    },
    {
      "epoch": 0.9523035230352304,
      "step": 17570,
      "training_loss": 6.8443684577941895
    },
    {
      "epoch": 0.9523577235772358,
      "step": 17571,
      "training_loss": 7.3604326248168945
    },
    {
      "epoch": 0.9524119241192411,
      "grad_norm": 23.45591163635254,
      "learning_rate": 1e-05,
      "loss": 6.2659,
      "step": 17572
    },
    {
      "epoch": 0.9524119241192411,
      "step": 17572,
      "training_loss": 5.8915252685546875
    },
    {
      "epoch": 0.9524661246612466,
      "step": 17573,
      "training_loss": 6.480540752410889
    },
    {
      "epoch": 0.952520325203252,
      "step": 17574,
      "training_loss": 4.433302879333496
    },
    {
      "epoch": 0.9525745257452575,
      "step": 17575,
      "training_loss": 6.253377437591553
    },
    {
      "epoch": 0.9526287262872629,
      "grad_norm": 21.620840072631836,
      "learning_rate": 1e-05,
      "loss": 5.7647,
      "step": 17576
    },
    {
      "epoch": 0.9526287262872629,
      "step": 17576,
      "training_loss": 3.344270944595337
    },
    {
      "epoch": 0.9526829268292683,
      "step": 17577,
      "training_loss": 7.1074299812316895
    },
    {
      "epoch": 0.9527371273712737,
      "step": 17578,
      "training_loss": 6.947517395019531
    },
    {
      "epoch": 0.9527913279132791,
      "step": 17579,
      "training_loss": 7.110979080200195
    },
    {
      "epoch": 0.9528455284552846,
      "grad_norm": 21.666719436645508,
      "learning_rate": 1e-05,
      "loss": 6.1275,
      "step": 17580
    },
    {
      "epoch": 0.9528455284552846,
      "step": 17580,
      "training_loss": 7.653092384338379
    },
    {
      "epoch": 0.9528997289972899,
      "step": 17581,
      "training_loss": 7.572805404663086
    },
    {
      "epoch": 0.9529539295392954,
      "step": 17582,
      "training_loss": 6.798088550567627
    },
    {
      "epoch": 0.9530081300813008,
      "step": 17583,
      "training_loss": 5.383947372436523
    },
    {
      "epoch": 0.9530623306233063,
      "grad_norm": 28.219694137573242,
      "learning_rate": 1e-05,
      "loss": 6.852,
      "step": 17584
    },
    {
      "epoch": 0.9530623306233063,
      "step": 17584,
      "training_loss": 5.0362324714660645
    },
    {
      "epoch": 0.9531165311653117,
      "step": 17585,
      "training_loss": 5.865192890167236
    },
    {
      "epoch": 0.953170731707317,
      "step": 17586,
      "training_loss": 7.1121344566345215
    },
    {
      "epoch": 0.9532249322493225,
      "step": 17587,
      "training_loss": 6.4548540115356445
    },
    {
      "epoch": 0.9532791327913279,
      "grad_norm": 25.1005916595459,
      "learning_rate": 1e-05,
      "loss": 6.1171,
      "step": 17588
    },
    {
      "epoch": 0.9532791327913279,
      "step": 17588,
      "training_loss": 7.091489315032959
    },
    {
      "epoch": 0.9533333333333334,
      "step": 17589,
      "training_loss": 6.05478572845459
    },
    {
      "epoch": 0.9533875338753387,
      "step": 17590,
      "training_loss": 6.776444911956787
    },
    {
      "epoch": 0.9534417344173441,
      "step": 17591,
      "training_loss": 8.599246978759766
    },
    {
      "epoch": 0.9534959349593496,
      "grad_norm": 38.593074798583984,
      "learning_rate": 1e-05,
      "loss": 7.1305,
      "step": 17592
    },
    {
      "epoch": 0.9534959349593496,
      "step": 17592,
      "training_loss": 7.854668140411377
    },
    {
      "epoch": 0.953550135501355,
      "step": 17593,
      "training_loss": 5.849607944488525
    },
    {
      "epoch": 0.9536043360433605,
      "step": 17594,
      "training_loss": 6.97620153427124
    },
    {
      "epoch": 0.9536585365853658,
      "step": 17595,
      "training_loss": 6.984825611114502
    },
    {
      "epoch": 0.9537127371273713,
      "grad_norm": 27.05088996887207,
      "learning_rate": 1e-05,
      "loss": 6.9163,
      "step": 17596
    },
    {
      "epoch": 0.9537127371273713,
      "step": 17596,
      "training_loss": 5.214233875274658
    },
    {
      "epoch": 0.9537669376693767,
      "step": 17597,
      "training_loss": 7.181741714477539
    },
    {
      "epoch": 0.9538211382113821,
      "step": 17598,
      "training_loss": 6.42527437210083
    },
    {
      "epoch": 0.9538753387533875,
      "step": 17599,
      "training_loss": 8.766921997070312
    },
    {
      "epoch": 0.9539295392953929,
      "grad_norm": 42.30924987792969,
      "learning_rate": 1e-05,
      "loss": 6.897,
      "step": 17600
    },
    {
      "epoch": 0.9539295392953929,
      "step": 17600,
      "training_loss": 6.738406658172607
    },
    {
      "epoch": 0.9539837398373984,
      "step": 17601,
      "training_loss": 6.918692111968994
    },
    {
      "epoch": 0.9540379403794038,
      "step": 17602,
      "training_loss": 6.06476354598999
    },
    {
      "epoch": 0.9540921409214093,
      "step": 17603,
      "training_loss": 4.8202385902404785
    },
    {
      "epoch": 0.9541463414634146,
      "grad_norm": 41.63243865966797,
      "learning_rate": 1e-05,
      "loss": 6.1355,
      "step": 17604
    },
    {
      "epoch": 0.9541463414634146,
      "step": 17604,
      "training_loss": 6.8960795402526855
    },
    {
      "epoch": 0.95420054200542,
      "step": 17605,
      "training_loss": 6.816368579864502
    },
    {
      "epoch": 0.9542547425474255,
      "step": 17606,
      "training_loss": 7.36412239074707
    },
    {
      "epoch": 0.9543089430894309,
      "step": 17607,
      "training_loss": 4.444403648376465
    },
    {
      "epoch": 0.9543631436314363,
      "grad_norm": 26.516603469848633,
      "learning_rate": 1e-05,
      "loss": 6.3802,
      "step": 17608
    },
    {
      "epoch": 0.9543631436314363,
      "step": 17608,
      "training_loss": 7.740779399871826
    },
    {
      "epoch": 0.9544173441734417,
      "step": 17609,
      "training_loss": 5.742513656616211
    },
    {
      "epoch": 0.9544715447154472,
      "step": 17610,
      "training_loss": 5.661862373352051
    },
    {
      "epoch": 0.9545257452574526,
      "step": 17611,
      "training_loss": 6.420766830444336
    },
    {
      "epoch": 0.954579945799458,
      "grad_norm": 18.725662231445312,
      "learning_rate": 1e-05,
      "loss": 6.3915,
      "step": 17612
    },
    {
      "epoch": 0.954579945799458,
      "step": 17612,
      "training_loss": 6.883190631866455
    },
    {
      "epoch": 0.9546341463414634,
      "step": 17613,
      "training_loss": 4.357088088989258
    },
    {
      "epoch": 0.9546883468834688,
      "step": 17614,
      "training_loss": 4.686140537261963
    },
    {
      "epoch": 0.9547425474254743,
      "step": 17615,
      "training_loss": 7.1696367263793945
    },
    {
      "epoch": 0.9547967479674797,
      "grad_norm": 43.43191909790039,
      "learning_rate": 1e-05,
      "loss": 5.774,
      "step": 17616
    },
    {
      "epoch": 0.9547967479674797,
      "step": 17616,
      "training_loss": 5.66749906539917
    },
    {
      "epoch": 0.954850948509485,
      "step": 17617,
      "training_loss": 8.025762557983398
    },
    {
      "epoch": 0.9549051490514905,
      "step": 17618,
      "training_loss": 7.982508659362793
    },
    {
      "epoch": 0.9549593495934959,
      "step": 17619,
      "training_loss": 5.537086486816406
    },
    {
      "epoch": 0.9550135501355014,
      "grad_norm": 38.958030700683594,
      "learning_rate": 1e-05,
      "loss": 6.8032,
      "step": 17620
    },
    {
      "epoch": 0.9550135501355014,
      "step": 17620,
      "training_loss": 5.248265266418457
    },
    {
      "epoch": 0.9550677506775068,
      "step": 17621,
      "training_loss": 7.175272464752197
    },
    {
      "epoch": 0.9551219512195122,
      "step": 17622,
      "training_loss": 4.573538780212402
    },
    {
      "epoch": 0.9551761517615176,
      "step": 17623,
      "training_loss": 5.721347808837891
    },
    {
      "epoch": 0.955230352303523,
      "grad_norm": 31.634403228759766,
      "learning_rate": 1e-05,
      "loss": 5.6796,
      "step": 17624
    },
    {
      "epoch": 0.955230352303523,
      "step": 17624,
      "training_loss": 6.391454696655273
    },
    {
      "epoch": 0.9552845528455285,
      "step": 17625,
      "training_loss": 6.565596580505371
    },
    {
      "epoch": 0.9553387533875338,
      "step": 17626,
      "training_loss": 3.1243247985839844
    },
    {
      "epoch": 0.9553929539295393,
      "step": 17627,
      "training_loss": 5.851476192474365
    },
    {
      "epoch": 0.9554471544715447,
      "grad_norm": 28.416072845458984,
      "learning_rate": 1e-05,
      "loss": 5.4832,
      "step": 17628
    },
    {
      "epoch": 0.9554471544715447,
      "step": 17628,
      "training_loss": 7.358409881591797
    },
    {
      "epoch": 0.9555013550135502,
      "step": 17629,
      "training_loss": 3.9304122924804688
    },
    {
      "epoch": 0.9555555555555556,
      "step": 17630,
      "training_loss": 6.6143798828125
    },
    {
      "epoch": 0.9556097560975609,
      "step": 17631,
      "training_loss": 5.7290730476379395
    },
    {
      "epoch": 0.9556639566395664,
      "grad_norm": 39.873565673828125,
      "learning_rate": 1e-05,
      "loss": 5.9081,
      "step": 17632
    },
    {
      "epoch": 0.9556639566395664,
      "step": 17632,
      "training_loss": 5.170854091644287
    },
    {
      "epoch": 0.9557181571815718,
      "step": 17633,
      "training_loss": 7.786475658416748
    },
    {
      "epoch": 0.9557723577235773,
      "step": 17634,
      "training_loss": 5.552675724029541
    },
    {
      "epoch": 0.9558265582655826,
      "step": 17635,
      "training_loss": 6.866765975952148
    },
    {
      "epoch": 0.955880758807588,
      "grad_norm": 19.97492790222168,
      "learning_rate": 1e-05,
      "loss": 6.3442,
      "step": 17636
    },
    {
      "epoch": 0.955880758807588,
      "step": 17636,
      "training_loss": 5.91563081741333
    },
    {
      "epoch": 0.9559349593495935,
      "step": 17637,
      "training_loss": 7.37917947769165
    },
    {
      "epoch": 0.9559891598915989,
      "step": 17638,
      "training_loss": 7.194326877593994
    },
    {
      "epoch": 0.9560433604336044,
      "step": 17639,
      "training_loss": 5.62330436706543
    },
    {
      "epoch": 0.9560975609756097,
      "grad_norm": 36.6619758605957,
      "learning_rate": 1e-05,
      "loss": 6.5281,
      "step": 17640
    },
    {
      "epoch": 0.9560975609756097,
      "step": 17640,
      "training_loss": 6.611698150634766
    },
    {
      "epoch": 0.9561517615176152,
      "step": 17641,
      "training_loss": 6.0269856452941895
    },
    {
      "epoch": 0.9562059620596206,
      "step": 17642,
      "training_loss": 7.540007591247559
    },
    {
      "epoch": 0.956260162601626,
      "step": 17643,
      "training_loss": 6.309382915496826
    },
    {
      "epoch": 0.9563143631436314,
      "grad_norm": 48.13462829589844,
      "learning_rate": 1e-05,
      "loss": 6.622,
      "step": 17644
    },
    {
      "epoch": 0.9563143631436314,
      "step": 17644,
      "training_loss": 5.7724761962890625
    },
    {
      "epoch": 0.9563685636856368,
      "step": 17645,
      "training_loss": 7.503382205963135
    },
    {
      "epoch": 0.9564227642276423,
      "step": 17646,
      "training_loss": 6.752157688140869
    },
    {
      "epoch": 0.9564769647696477,
      "step": 17647,
      "training_loss": 6.429225444793701
    },
    {
      "epoch": 0.9565311653116532,
      "grad_norm": 25.263710021972656,
      "learning_rate": 1e-05,
      "loss": 6.6143,
      "step": 17648
    },
    {
      "epoch": 0.9565311653116532,
      "step": 17648,
      "training_loss": 6.89420223236084
    },
    {
      "epoch": 0.9565853658536585,
      "step": 17649,
      "training_loss": 6.819145202636719
    },
    {
      "epoch": 0.9566395663956639,
      "step": 17650,
      "training_loss": 7.227690696716309
    },
    {
      "epoch": 0.9566937669376694,
      "step": 17651,
      "training_loss": 5.443247318267822
    },
    {
      "epoch": 0.9567479674796748,
      "grad_norm": 30.466569900512695,
      "learning_rate": 1e-05,
      "loss": 6.5961,
      "step": 17652
    },
    {
      "epoch": 0.9567479674796748,
      "step": 17652,
      "training_loss": 7.683399200439453
    },
    {
      "epoch": 0.9568021680216802,
      "step": 17653,
      "training_loss": 7.30752420425415
    },
    {
      "epoch": 0.9568563685636856,
      "step": 17654,
      "training_loss": 6.7706146240234375
    },
    {
      "epoch": 0.9569105691056911,
      "step": 17655,
      "training_loss": 6.770098686218262
    },
    {
      "epoch": 0.9569647696476965,
      "grad_norm": 22.7834529876709,
      "learning_rate": 1e-05,
      "loss": 7.1329,
      "step": 17656
    },
    {
      "epoch": 0.9569647696476965,
      "step": 17656,
      "training_loss": 7.488004207611084
    },
    {
      "epoch": 0.957018970189702,
      "step": 17657,
      "training_loss": 8.079809188842773
    },
    {
      "epoch": 0.9570731707317073,
      "step": 17658,
      "training_loss": 5.962674140930176
    },
    {
      "epoch": 0.9571273712737127,
      "step": 17659,
      "training_loss": 7.093574047088623
    },
    {
      "epoch": 0.9571815718157182,
      "grad_norm": 22.578414916992188,
      "learning_rate": 1e-05,
      "loss": 7.156,
      "step": 17660
    },
    {
      "epoch": 0.9571815718157182,
      "step": 17660,
      "training_loss": 5.898336410522461
    },
    {
      "epoch": 0.9572357723577236,
      "step": 17661,
      "training_loss": 6.877732276916504
    },
    {
      "epoch": 0.957289972899729,
      "step": 17662,
      "training_loss": 7.502670764923096
    },
    {
      "epoch": 0.9573441734417344,
      "step": 17663,
      "training_loss": 6.948700904846191
    },
    {
      "epoch": 0.9573983739837398,
      "grad_norm": 28.70919418334961,
      "learning_rate": 1e-05,
      "loss": 6.8069,
      "step": 17664
    },
    {
      "epoch": 0.9573983739837398,
      "step": 17664,
      "training_loss": 7.1390156745910645
    },
    {
      "epoch": 0.9574525745257453,
      "step": 17665,
      "training_loss": 6.8474016189575195
    },
    {
      "epoch": 0.9575067750677507,
      "step": 17666,
      "training_loss": 6.809551239013672
    },
    {
      "epoch": 0.9575609756097561,
      "step": 17667,
      "training_loss": 5.1332688331604
    },
    {
      "epoch": 0.9576151761517615,
      "grad_norm": 47.65705871582031,
      "learning_rate": 1e-05,
      "loss": 6.4823,
      "step": 17668
    },
    {
      "epoch": 0.9576151761517615,
      "step": 17668,
      "training_loss": 4.994492530822754
    },
    {
      "epoch": 0.957669376693767,
      "step": 17669,
      "training_loss": 5.3669962882995605
    },
    {
      "epoch": 0.9577235772357724,
      "step": 17670,
      "training_loss": 6.876142501831055
    },
    {
      "epoch": 0.9577777777777777,
      "step": 17671,
      "training_loss": 4.577003002166748
    },
    {
      "epoch": 0.9578319783197832,
      "grad_norm": 32.06019592285156,
      "learning_rate": 1e-05,
      "loss": 5.4537,
      "step": 17672
    },
    {
      "epoch": 0.9578319783197832,
      "step": 17672,
      "training_loss": 4.03405237197876
    },
    {
      "epoch": 0.9578861788617886,
      "step": 17673,
      "training_loss": 7.0441789627075195
    },
    {
      "epoch": 0.9579403794037941,
      "step": 17674,
      "training_loss": 6.615761756896973
    },
    {
      "epoch": 0.9579945799457995,
      "step": 17675,
      "training_loss": 7.333737850189209
    },
    {
      "epoch": 0.9580487804878048,
      "grad_norm": 28.82943344116211,
      "learning_rate": 1e-05,
      "loss": 6.2569,
      "step": 17676
    },
    {
      "epoch": 0.9580487804878048,
      "step": 17676,
      "training_loss": 4.446025371551514
    },
    {
      "epoch": 0.9581029810298103,
      "step": 17677,
      "training_loss": 6.947115898132324
    },
    {
      "epoch": 0.9581571815718157,
      "step": 17678,
      "training_loss": 6.654085159301758
    },
    {
      "epoch": 0.9582113821138212,
      "step": 17679,
      "training_loss": 6.834490776062012
    },
    {
      "epoch": 0.9582655826558265,
      "grad_norm": 21.555360794067383,
      "learning_rate": 1e-05,
      "loss": 6.2204,
      "step": 17680
    },
    {
      "epoch": 0.9582655826558265,
      "step": 17680,
      "training_loss": 7.938088893890381
    },
    {
      "epoch": 0.958319783197832,
      "step": 17681,
      "training_loss": 5.8946027755737305
    },
    {
      "epoch": 0.9583739837398374,
      "step": 17682,
      "training_loss": 6.8777947425842285
    },
    {
      "epoch": 0.9584281842818428,
      "step": 17683,
      "training_loss": 3.0462324619293213
    },
    {
      "epoch": 0.9584823848238483,
      "grad_norm": 28.240224838256836,
      "learning_rate": 1e-05,
      "loss": 5.9392,
      "step": 17684
    },
    {
      "epoch": 0.9584823848238483,
      "step": 17684,
      "training_loss": 5.108145713806152
    },
    {
      "epoch": 0.9585365853658536,
      "step": 17685,
      "training_loss": 4.259246826171875
    },
    {
      "epoch": 0.9585907859078591,
      "step": 17686,
      "training_loss": 5.8391008377075195
    },
    {
      "epoch": 0.9586449864498645,
      "step": 17687,
      "training_loss": 7.321547508239746
    },
    {
      "epoch": 0.95869918699187,
      "grad_norm": 33.614315032958984,
      "learning_rate": 1e-05,
      "loss": 5.632,
      "step": 17688
    },
    {
      "epoch": 0.95869918699187,
      "step": 17688,
      "training_loss": 6.848731517791748
    },
    {
      "epoch": 0.9587533875338753,
      "step": 17689,
      "training_loss": 6.5260186195373535
    },
    {
      "epoch": 0.9588075880758807,
      "step": 17690,
      "training_loss": 6.965877532958984
    },
    {
      "epoch": 0.9588617886178862,
      "step": 17691,
      "training_loss": 6.090777397155762
    },
    {
      "epoch": 0.9589159891598916,
      "grad_norm": 33.736236572265625,
      "learning_rate": 1e-05,
      "loss": 6.6079,
      "step": 17692
    },
    {
      "epoch": 0.9589159891598916,
      "step": 17692,
      "training_loss": 7.809835433959961
    },
    {
      "epoch": 0.9589701897018971,
      "step": 17693,
      "training_loss": 6.240448474884033
    },
    {
      "epoch": 0.9590243902439024,
      "step": 17694,
      "training_loss": 5.708807468414307
    },
    {
      "epoch": 0.9590785907859078,
      "step": 17695,
      "training_loss": 7.7238640785217285
    },
    {
      "epoch": 0.9591327913279133,
      "grad_norm": 16.883089065551758,
      "learning_rate": 1e-05,
      "loss": 6.8707,
      "step": 17696
    },
    {
      "epoch": 0.9591327913279133,
      "step": 17696,
      "training_loss": 6.664735794067383
    },
    {
      "epoch": 0.9591869918699187,
      "step": 17697,
      "training_loss": 7.541457176208496
    },
    {
      "epoch": 0.9592411924119241,
      "step": 17698,
      "training_loss": 4.5651535987854
    },
    {
      "epoch": 0.9592953929539295,
      "step": 17699,
      "training_loss": 7.581668853759766
    },
    {
      "epoch": 0.959349593495935,
      "grad_norm": 35.038700103759766,
      "learning_rate": 1e-05,
      "loss": 6.5883,
      "step": 17700
    },
    {
      "epoch": 0.959349593495935,
      "step": 17700,
      "training_loss": 7.1854248046875
    },
    {
      "epoch": 0.9594037940379404,
      "step": 17701,
      "training_loss": 8.73975658416748
    },
    {
      "epoch": 0.9594579945799458,
      "step": 17702,
      "training_loss": 4.573944568634033
    },
    {
      "epoch": 0.9595121951219512,
      "step": 17703,
      "training_loss": 5.593268394470215
    },
    {
      "epoch": 0.9595663956639566,
      "grad_norm": 22.832447052001953,
      "learning_rate": 1e-05,
      "loss": 6.5231,
      "step": 17704
    },
    {
      "epoch": 0.9595663956639566,
      "step": 17704,
      "training_loss": 7.058586597442627
    },
    {
      "epoch": 0.9596205962059621,
      "step": 17705,
      "training_loss": 6.976917266845703
    },
    {
      "epoch": 0.9596747967479675,
      "step": 17706,
      "training_loss": 5.66713809967041
    },
    {
      "epoch": 0.9597289972899729,
      "step": 17707,
      "training_loss": 6.80458402633667
    },
    {
      "epoch": 0.9597831978319783,
      "grad_norm": 36.23591995239258,
      "learning_rate": 1e-05,
      "loss": 6.6268,
      "step": 17708
    },
    {
      "epoch": 0.9597831978319783,
      "step": 17708,
      "training_loss": 7.588144302368164
    },
    {
      "epoch": 0.9598373983739837,
      "step": 17709,
      "training_loss": 6.818447589874268
    },
    {
      "epoch": 0.9598915989159892,
      "step": 17710,
      "training_loss": 6.868584156036377
    },
    {
      "epoch": 0.9599457994579946,
      "step": 17711,
      "training_loss": 7.2158098220825195
    },
    {
      "epoch": 0.96,
      "grad_norm": 23.53434181213379,
      "learning_rate": 1e-05,
      "loss": 7.1227,
      "step": 17712
    },
    {
      "epoch": 0.96,
      "step": 17712,
      "training_loss": 5.503746032714844
    },
    {
      "epoch": 0.9600542005420054,
      "step": 17713,
      "training_loss": 7.465636253356934
    },
    {
      "epoch": 0.9601084010840109,
      "step": 17714,
      "training_loss": 6.0049262046813965
    },
    {
      "epoch": 0.9601626016260163,
      "step": 17715,
      "training_loss": 7.605191230773926
    },
    {
      "epoch": 0.9602168021680216,
      "grad_norm": 25.74603843688965,
      "learning_rate": 1e-05,
      "loss": 6.6449,
      "step": 17716
    },
    {
      "epoch": 0.9602168021680216,
      "step": 17716,
      "training_loss": 6.4143266677856445
    },
    {
      "epoch": 0.9602710027100271,
      "step": 17717,
      "training_loss": 4.720227241516113
    },
    {
      "epoch": 0.9603252032520325,
      "step": 17718,
      "training_loss": 6.624330997467041
    },
    {
      "epoch": 0.960379403794038,
      "step": 17719,
      "training_loss": 7.8120527267456055
    },
    {
      "epoch": 0.9604336043360434,
      "grad_norm": 27.10587501525879,
      "learning_rate": 1e-05,
      "loss": 6.3927,
      "step": 17720
    },
    {
      "epoch": 0.9604336043360434,
      "step": 17720,
      "training_loss": 7.4779253005981445
    },
    {
      "epoch": 0.9604878048780487,
      "step": 17721,
      "training_loss": 7.36885404586792
    },
    {
      "epoch": 0.9605420054200542,
      "step": 17722,
      "training_loss": 6.870696544647217
    },
    {
      "epoch": 0.9605962059620596,
      "step": 17723,
      "training_loss": 4.770395278930664
    },
    {
      "epoch": 0.9606504065040651,
      "grad_norm": 23.51506996154785,
      "learning_rate": 1e-05,
      "loss": 6.622,
      "step": 17724
    },
    {
      "epoch": 0.9606504065040651,
      "step": 17724,
      "training_loss": 6.945767402648926
    },
    {
      "epoch": 0.9607046070460704,
      "step": 17725,
      "training_loss": 6.633310794830322
    },
    {
      "epoch": 0.9607588075880759,
      "step": 17726,
      "training_loss": 6.911441802978516
    },
    {
      "epoch": 0.9608130081300813,
      "step": 17727,
      "training_loss": 7.095839500427246
    },
    {
      "epoch": 0.9608672086720867,
      "grad_norm": 46.15949249267578,
      "learning_rate": 1e-05,
      "loss": 6.8966,
      "step": 17728
    },
    {
      "epoch": 0.9608672086720867,
      "step": 17728,
      "training_loss": 5.965494155883789
    },
    {
      "epoch": 0.9609214092140922,
      "step": 17729,
      "training_loss": 6.763205528259277
    },
    {
      "epoch": 0.9609756097560975,
      "step": 17730,
      "training_loss": 6.787193775177002
    },
    {
      "epoch": 0.961029810298103,
      "step": 17731,
      "training_loss": 6.754261016845703
    },
    {
      "epoch": 0.9610840108401084,
      "grad_norm": 15.879301071166992,
      "learning_rate": 1e-05,
      "loss": 6.5675,
      "step": 17732
    },
    {
      "epoch": 0.9610840108401084,
      "step": 17732,
      "training_loss": 4.497224807739258
    },
    {
      "epoch": 0.9611382113821139,
      "step": 17733,
      "training_loss": 6.763156414031982
    },
    {
      "epoch": 0.9611924119241192,
      "step": 17734,
      "training_loss": 7.247450828552246
    },
    {
      "epoch": 0.9612466124661246,
      "step": 17735,
      "training_loss": 6.347710609436035
    },
    {
      "epoch": 0.9613008130081301,
      "grad_norm": 26.403200149536133,
      "learning_rate": 1e-05,
      "loss": 6.2139,
      "step": 17736
    },
    {
      "epoch": 0.9613008130081301,
      "step": 17736,
      "training_loss": 7.1952314376831055
    },
    {
      "epoch": 0.9613550135501355,
      "step": 17737,
      "training_loss": 6.753790378570557
    },
    {
      "epoch": 0.961409214092141,
      "step": 17738,
      "training_loss": 6.24290657043457
    },
    {
      "epoch": 0.9614634146341463,
      "step": 17739,
      "training_loss": 6.780904293060303
    },
    {
      "epoch": 0.9615176151761518,
      "grad_norm": 49.58386993408203,
      "learning_rate": 1e-05,
      "loss": 6.7432,
      "step": 17740
    },
    {
      "epoch": 0.9615176151761518,
      "step": 17740,
      "training_loss": 6.402077674865723
    },
    {
      "epoch": 0.9615718157181572,
      "step": 17741,
      "training_loss": 6.494441509246826
    },
    {
      "epoch": 0.9616260162601626,
      "step": 17742,
      "training_loss": 5.621449947357178
    },
    {
      "epoch": 0.961680216802168,
      "step": 17743,
      "training_loss": 6.734694004058838
    },
    {
      "epoch": 0.9617344173441734,
      "grad_norm": 35.049598693847656,
      "learning_rate": 1e-05,
      "loss": 6.3132,
      "step": 17744
    },
    {
      "epoch": 0.9617344173441734,
      "step": 17744,
      "training_loss": 5.897336959838867
    },
    {
      "epoch": 0.9617886178861789,
      "step": 17745,
      "training_loss": 6.662973403930664
    },
    {
      "epoch": 0.9618428184281843,
      "step": 17746,
      "training_loss": 7.4952850341796875
    },
    {
      "epoch": 0.9618970189701898,
      "step": 17747,
      "training_loss": 6.532153129577637
    },
    {
      "epoch": 0.9619512195121951,
      "grad_norm": 24.085264205932617,
      "learning_rate": 1e-05,
      "loss": 6.6469,
      "step": 17748
    },
    {
      "epoch": 0.9619512195121951,
      "step": 17748,
      "training_loss": 6.411820888519287
    },
    {
      "epoch": 0.9620054200542005,
      "step": 17749,
      "training_loss": 6.742638111114502
    },
    {
      "epoch": 0.962059620596206,
      "step": 17750,
      "training_loss": 7.486847877502441
    },
    {
      "epoch": 0.9621138211382114,
      "step": 17751,
      "training_loss": 4.7331013679504395
    },
    {
      "epoch": 0.9621680216802168,
      "grad_norm": 36.316829681396484,
      "learning_rate": 1e-05,
      "loss": 6.3436,
      "step": 17752
    },
    {
      "epoch": 0.9621680216802168,
      "step": 17752,
      "training_loss": 6.518854141235352
    },
    {
      "epoch": 0.9622222222222222,
      "step": 17753,
      "training_loss": 7.826994895935059
    },
    {
      "epoch": 0.9622764227642276,
      "step": 17754,
      "training_loss": 6.895060062408447
    },
    {
      "epoch": 0.9623306233062331,
      "step": 17755,
      "training_loss": 4.268230438232422
    },
    {
      "epoch": 0.9623848238482385,
      "grad_norm": 24.073549270629883,
      "learning_rate": 1e-05,
      "loss": 6.3773,
      "step": 17756
    },
    {
      "epoch": 0.9623848238482385,
      "step": 17756,
      "training_loss": 7.099031925201416
    },
    {
      "epoch": 0.9624390243902439,
      "step": 17757,
      "training_loss": 5.566598892211914
    },
    {
      "epoch": 0.9624932249322493,
      "step": 17758,
      "training_loss": 4.654128551483154
    },
    {
      "epoch": 0.9625474254742548,
      "step": 17759,
      "training_loss": 8.067748069763184
    },
    {
      "epoch": 0.9626016260162602,
      "grad_norm": 25.987096786499023,
      "learning_rate": 1e-05,
      "loss": 6.3469,
      "step": 17760
    },
    {
      "epoch": 0.9626016260162602,
      "step": 17760,
      "training_loss": 7.093634605407715
    },
    {
      "epoch": 0.9626558265582655,
      "step": 17761,
      "training_loss": 6.613285064697266
    },
    {
      "epoch": 0.962710027100271,
      "step": 17762,
      "training_loss": 6.77976131439209
    },
    {
      "epoch": 0.9627642276422764,
      "step": 17763,
      "training_loss": 6.812681198120117
    },
    {
      "epoch": 0.9628184281842819,
      "grad_norm": 26.0819034576416,
      "learning_rate": 1e-05,
      "loss": 6.8248,
      "step": 17764
    },
    {
      "epoch": 0.9628184281842819,
      "step": 17764,
      "training_loss": 6.958795547485352
    },
    {
      "epoch": 0.9628726287262873,
      "step": 17765,
      "training_loss": 6.2309417724609375
    },
    {
      "epoch": 0.9629268292682926,
      "step": 17766,
      "training_loss": 7.0393967628479
    },
    {
      "epoch": 0.9629810298102981,
      "step": 17767,
      "training_loss": 7.374701976776123
    },
    {
      "epoch": 0.9630352303523035,
      "grad_norm": 25.455894470214844,
      "learning_rate": 1e-05,
      "loss": 6.901,
      "step": 17768
    },
    {
      "epoch": 0.9630352303523035,
      "step": 17768,
      "training_loss": 8.34717082977295
    },
    {
      "epoch": 0.963089430894309,
      "step": 17769,
      "training_loss": 6.276134490966797
    },
    {
      "epoch": 0.9631436314363143,
      "step": 17770,
      "training_loss": 7.216944217681885
    },
    {
      "epoch": 0.9631978319783198,
      "step": 17771,
      "training_loss": 6.839197635650635
    },
    {
      "epoch": 0.9632520325203252,
      "grad_norm": 31.34419822692871,
      "learning_rate": 1e-05,
      "loss": 7.1699,
      "step": 17772
    },
    {
      "epoch": 0.9632520325203252,
      "step": 17772,
      "training_loss": 4.993653774261475
    },
    {
      "epoch": 0.9633062330623307,
      "step": 17773,
      "training_loss": 5.810678005218506
    },
    {
      "epoch": 0.9633604336043361,
      "step": 17774,
      "training_loss": 5.889869689941406
    },
    {
      "epoch": 0.9634146341463414,
      "step": 17775,
      "training_loss": 5.708986759185791
    },
    {
      "epoch": 0.9634688346883469,
      "grad_norm": 37.037967681884766,
      "learning_rate": 1e-05,
      "loss": 5.6008,
      "step": 17776
    },
    {
      "epoch": 0.9634688346883469,
      "step": 17776,
      "training_loss": 6.80230712890625
    },
    {
      "epoch": 0.9635230352303523,
      "step": 17777,
      "training_loss": 7.619190692901611
    },
    {
      "epoch": 0.9635772357723578,
      "step": 17778,
      "training_loss": 7.319461345672607
    },
    {
      "epoch": 0.9636314363143631,
      "step": 17779,
      "training_loss": 7.139650344848633
    },
    {
      "epoch": 0.9636856368563685,
      "grad_norm": 24.42715835571289,
      "learning_rate": 1e-05,
      "loss": 7.2202,
      "step": 17780
    },
    {
      "epoch": 0.9636856368563685,
      "step": 17780,
      "training_loss": 6.8578643798828125
    },
    {
      "epoch": 0.963739837398374,
      "step": 17781,
      "training_loss": 5.745924949645996
    },
    {
      "epoch": 0.9637940379403794,
      "step": 17782,
      "training_loss": 5.3653059005737305
    },
    {
      "epoch": 0.9638482384823849,
      "step": 17783,
      "training_loss": 6.15538215637207
    },
    {
      "epoch": 0.9639024390243902,
      "grad_norm": 20.663803100585938,
      "learning_rate": 1e-05,
      "loss": 6.0311,
      "step": 17784
    },
    {
      "epoch": 0.9639024390243902,
      "step": 17784,
      "training_loss": 5.63694429397583
    },
    {
      "epoch": 0.9639566395663957,
      "step": 17785,
      "training_loss": 4.380839824676514
    },
    {
      "epoch": 0.9640108401084011,
      "step": 17786,
      "training_loss": 6.78994607925415
    },
    {
      "epoch": 0.9640650406504065,
      "step": 17787,
      "training_loss": 7.367546081542969
    },
    {
      "epoch": 0.9641192411924119,
      "grad_norm": 22.6099796295166,
      "learning_rate": 1e-05,
      "loss": 6.0438,
      "step": 17788
    },
    {
      "epoch": 0.9641192411924119,
      "step": 17788,
      "training_loss": 6.602391242980957
    },
    {
      "epoch": 0.9641734417344173,
      "step": 17789,
      "training_loss": 7.025666236877441
    },
    {
      "epoch": 0.9642276422764228,
      "step": 17790,
      "training_loss": 7.717718124389648
    },
    {
      "epoch": 0.9642818428184282,
      "step": 17791,
      "training_loss": 5.977388381958008
    },
    {
      "epoch": 0.9643360433604337,
      "grad_norm": 23.087141036987305,
      "learning_rate": 1e-05,
      "loss": 6.8308,
      "step": 17792
    },
    {
      "epoch": 0.9643360433604337,
      "step": 17792,
      "training_loss": 7.478307247161865
    },
    {
      "epoch": 0.964390243902439,
      "step": 17793,
      "training_loss": 6.303988933563232
    },
    {
      "epoch": 0.9644444444444444,
      "step": 17794,
      "training_loss": 6.880039215087891
    },
    {
      "epoch": 0.9644986449864499,
      "step": 17795,
      "training_loss": 6.359608173370361
    },
    {
      "epoch": 0.9645528455284553,
      "grad_norm": 45.90850067138672,
      "learning_rate": 1e-05,
      "loss": 6.7555,
      "step": 17796
    },
    {
      "epoch": 0.9645528455284553,
      "step": 17796,
      "training_loss": 5.986851215362549
    },
    {
      "epoch": 0.9646070460704607,
      "step": 17797,
      "training_loss": 7.825918197631836
    },
    {
      "epoch": 0.9646612466124661,
      "step": 17798,
      "training_loss": 6.679316997528076
    },
    {
      "epoch": 0.9647154471544716,
      "step": 17799,
      "training_loss": 4.943567752838135
    },
    {
      "epoch": 0.964769647696477,
      "grad_norm": 30.589977264404297,
      "learning_rate": 1e-05,
      "loss": 6.3589,
      "step": 17800
    },
    {
      "epoch": 0.964769647696477,
      "step": 17800,
      "training_loss": 6.016224384307861
    },
    {
      "epoch": 0.9648238482384824,
      "step": 17801,
      "training_loss": 8.566637992858887
    },
    {
      "epoch": 0.9648780487804878,
      "step": 17802,
      "training_loss": 5.666004180908203
    },
    {
      "epoch": 0.9649322493224932,
      "step": 17803,
      "training_loss": 7.34274959564209
    },
    {
      "epoch": 0.9649864498644987,
      "grad_norm": 34.73080825805664,
      "learning_rate": 1e-05,
      "loss": 6.8979,
      "step": 17804
    },
    {
      "epoch": 0.9649864498644987,
      "step": 17804,
      "training_loss": 6.527811527252197
    },
    {
      "epoch": 0.9650406504065041,
      "step": 17805,
      "training_loss": 5.867110729217529
    },
    {
      "epoch": 0.9650948509485094,
      "step": 17806,
      "training_loss": 6.8476786613464355
    },
    {
      "epoch": 0.9651490514905149,
      "step": 17807,
      "training_loss": 5.840713024139404
    },
    {
      "epoch": 0.9652032520325203,
      "grad_norm": 32.86546325683594,
      "learning_rate": 1e-05,
      "loss": 6.2708,
      "step": 17808
    },
    {
      "epoch": 0.9652032520325203,
      "step": 17808,
      "training_loss": 7.244141101837158
    },
    {
      "epoch": 0.9652574525745258,
      "step": 17809,
      "training_loss": 7.221002101898193
    },
    {
      "epoch": 0.9653116531165311,
      "step": 17810,
      "training_loss": 7.028244495391846
    },
    {
      "epoch": 0.9653658536585366,
      "step": 17811,
      "training_loss": 6.158170700073242
    },
    {
      "epoch": 0.965420054200542,
      "grad_norm": 23.241477966308594,
      "learning_rate": 1e-05,
      "loss": 6.9129,
      "step": 17812
    },
    {
      "epoch": 0.965420054200542,
      "step": 17812,
      "training_loss": 4.980262279510498
    },
    {
      "epoch": 0.9654742547425474,
      "step": 17813,
      "training_loss": 7.06610631942749
    },
    {
      "epoch": 0.9655284552845529,
      "step": 17814,
      "training_loss": 5.7352190017700195
    },
    {
      "epoch": 0.9655826558265582,
      "step": 17815,
      "training_loss": 7.104954242706299
    },
    {
      "epoch": 0.9656368563685637,
      "grad_norm": 19.612192153930664,
      "learning_rate": 1e-05,
      "loss": 6.2216,
      "step": 17816
    },
    {
      "epoch": 0.9656368563685637,
      "step": 17816,
      "training_loss": 5.9143877029418945
    },
    {
      "epoch": 0.9656910569105691,
      "step": 17817,
      "training_loss": 7.194389820098877
    },
    {
      "epoch": 0.9657452574525746,
      "step": 17818,
      "training_loss": 6.739902496337891
    },
    {
      "epoch": 0.9657994579945799,
      "step": 17819,
      "training_loss": 7.446169376373291
    },
    {
      "epoch": 0.9658536585365853,
      "grad_norm": 22.14478302001953,
      "learning_rate": 1e-05,
      "loss": 6.8237,
      "step": 17820
    },
    {
      "epoch": 0.9658536585365853,
      "step": 17820,
      "training_loss": 5.309605598449707
    },
    {
      "epoch": 0.9659078590785908,
      "step": 17821,
      "training_loss": 6.9517903327941895
    },
    {
      "epoch": 0.9659620596205962,
      "step": 17822,
      "training_loss": 5.286206245422363
    },
    {
      "epoch": 0.9660162601626017,
      "step": 17823,
      "training_loss": 5.841795921325684
    },
    {
      "epoch": 0.966070460704607,
      "grad_norm": 30.95966911315918,
      "learning_rate": 1e-05,
      "loss": 5.8474,
      "step": 17824
    },
    {
      "epoch": 0.966070460704607,
      "step": 17824,
      "training_loss": 6.387972354888916
    },
    {
      "epoch": 0.9661246612466124,
      "step": 17825,
      "training_loss": 7.053915977478027
    },
    {
      "epoch": 0.9661788617886179,
      "step": 17826,
      "training_loss": 5.353731155395508
    },
    {
      "epoch": 0.9662330623306233,
      "step": 17827,
      "training_loss": 6.151287078857422
    },
    {
      "epoch": 0.9662872628726287,
      "grad_norm": 18.442672729492188,
      "learning_rate": 1e-05,
      "loss": 6.2367,
      "step": 17828
    },
    {
      "epoch": 0.9662872628726287,
      "step": 17828,
      "training_loss": 5.784414291381836
    },
    {
      "epoch": 0.9663414634146341,
      "step": 17829,
      "training_loss": 4.763593673706055
    },
    {
      "epoch": 0.9663956639566396,
      "step": 17830,
      "training_loss": 7.556508541107178
    },
    {
      "epoch": 0.966449864498645,
      "step": 17831,
      "training_loss": 3.6409666538238525
    },
    {
      "epoch": 0.9665040650406505,
      "grad_norm": 55.4093017578125,
      "learning_rate": 1e-05,
      "loss": 5.4364,
      "step": 17832
    },
    {
      "epoch": 0.9665040650406505,
      "step": 17832,
      "training_loss": 3.820383071899414
    },
    {
      "epoch": 0.9665582655826558,
      "step": 17833,
      "training_loss": 5.22438907623291
    },
    {
      "epoch": 0.9666124661246612,
      "step": 17834,
      "training_loss": 6.836510181427002
    },
    {
      "epoch": 0.9666666666666667,
      "step": 17835,
      "training_loss": 7.430418491363525
    },
    {
      "epoch": 0.9667208672086721,
      "grad_norm": 24.312252044677734,
      "learning_rate": 1e-05,
      "loss": 5.8279,
      "step": 17836
    },
    {
      "epoch": 0.9667208672086721,
      "step": 17836,
      "training_loss": 7.0710906982421875
    },
    {
      "epoch": 0.9667750677506775,
      "step": 17837,
      "training_loss": 6.405618190765381
    },
    {
      "epoch": 0.9668292682926829,
      "step": 17838,
      "training_loss": 7.142819881439209
    },
    {
      "epoch": 0.9668834688346883,
      "step": 17839,
      "training_loss": 5.236006259918213
    },
    {
      "epoch": 0.9669376693766938,
      "grad_norm": 20.581180572509766,
      "learning_rate": 1e-05,
      "loss": 6.4639,
      "step": 17840
    },
    {
      "epoch": 0.9669376693766938,
      "step": 17840,
      "training_loss": 6.1640777587890625
    },
    {
      "epoch": 0.9669918699186992,
      "step": 17841,
      "training_loss": 7.582441329956055
    },
    {
      "epoch": 0.9670460704607046,
      "step": 17842,
      "training_loss": 6.210155010223389
    },
    {
      "epoch": 0.96710027100271,
      "step": 17843,
      "training_loss": 8.389328002929688
    },
    {
      "epoch": 0.9671544715447155,
      "grad_norm": 29.227968215942383,
      "learning_rate": 1e-05,
      "loss": 7.0865,
      "step": 17844
    },
    {
      "epoch": 0.9671544715447155,
      "step": 17844,
      "training_loss": 4.910876274108887
    },
    {
      "epoch": 0.9672086720867209,
      "step": 17845,
      "training_loss": 7.357545852661133
    },
    {
      "epoch": 0.9672628726287262,
      "step": 17846,
      "training_loss": 7.222135543823242
    },
    {
      "epoch": 0.9673170731707317,
      "step": 17847,
      "training_loss": 8.223175048828125
    },
    {
      "epoch": 0.9673712737127371,
      "grad_norm": 26.9217529296875,
      "learning_rate": 1e-05,
      "loss": 6.9284,
      "step": 17848
    },
    {
      "epoch": 0.9673712737127371,
      "step": 17848,
      "training_loss": 6.335530757904053
    },
    {
      "epoch": 0.9674254742547426,
      "step": 17849,
      "training_loss": 6.796692371368408
    },
    {
      "epoch": 0.967479674796748,
      "step": 17850,
      "training_loss": 4.448875427246094
    },
    {
      "epoch": 0.9675338753387533,
      "step": 17851,
      "training_loss": 5.705031871795654
    },
    {
      "epoch": 0.9675880758807588,
      "grad_norm": 42.242393493652344,
      "learning_rate": 1e-05,
      "loss": 5.8215,
      "step": 17852
    },
    {
      "epoch": 0.9675880758807588,
      "step": 17852,
      "training_loss": 6.443734645843506
    },
    {
      "epoch": 0.9676422764227642,
      "step": 17853,
      "training_loss": 6.068588733673096
    },
    {
      "epoch": 0.9676964769647697,
      "step": 17854,
      "training_loss": 5.769317150115967
    },
    {
      "epoch": 0.967750677506775,
      "step": 17855,
      "training_loss": 7.820434093475342
    },
    {
      "epoch": 0.9678048780487805,
      "grad_norm": 27.055753707885742,
      "learning_rate": 1e-05,
      "loss": 6.5255,
      "step": 17856
    },
    {
      "epoch": 0.9678048780487805,
      "step": 17856,
      "training_loss": 5.968188285827637
    },
    {
      "epoch": 0.9678590785907859,
      "step": 17857,
      "training_loss": 6.504002094268799
    },
    {
      "epoch": 0.9679132791327913,
      "step": 17858,
      "training_loss": 6.197669982910156
    },
    {
      "epoch": 0.9679674796747968,
      "step": 17859,
      "training_loss": 6.921571731567383
    },
    {
      "epoch": 0.9680216802168021,
      "grad_norm": 20.99863624572754,
      "learning_rate": 1e-05,
      "loss": 6.3979,
      "step": 17860
    },
    {
      "epoch": 0.9680216802168021,
      "step": 17860,
      "training_loss": 6.599571228027344
    },
    {
      "epoch": 0.9680758807588076,
      "step": 17861,
      "training_loss": 5.390021324157715
    },
    {
      "epoch": 0.968130081300813,
      "step": 17862,
      "training_loss": 3.5234453678131104
    },
    {
      "epoch": 0.9681842818428185,
      "step": 17863,
      "training_loss": 7.3487138748168945
    },
    {
      "epoch": 0.9682384823848238,
      "grad_norm": 30.979307174682617,
      "learning_rate": 1e-05,
      "loss": 5.7154,
      "step": 17864
    },
    {
      "epoch": 0.9682384823848238,
      "step": 17864,
      "training_loss": 7.316684722900391
    },
    {
      "epoch": 0.9682926829268292,
      "step": 17865,
      "training_loss": 5.980770587921143
    },
    {
      "epoch": 0.9683468834688347,
      "step": 17866,
      "training_loss": 7.934782981872559
    },
    {
      "epoch": 0.9684010840108401,
      "step": 17867,
      "training_loss": 7.637340545654297
    },
    {
      "epoch": 0.9684552845528456,
      "grad_norm": 24.239578247070312,
      "learning_rate": 1e-05,
      "loss": 7.2174,
      "step": 17868
    },
    {
      "epoch": 0.9684552845528456,
      "step": 17868,
      "training_loss": 6.705742835998535
    },
    {
      "epoch": 0.9685094850948509,
      "step": 17869,
      "training_loss": 4.768326282501221
    },
    {
      "epoch": 0.9685636856368564,
      "step": 17870,
      "training_loss": 6.554046154022217
    },
    {
      "epoch": 0.9686178861788618,
      "step": 17871,
      "training_loss": 6.8665642738342285
    },
    {
      "epoch": 0.9686720867208672,
      "grad_norm": 28.567359924316406,
      "learning_rate": 1e-05,
      "loss": 6.2237,
      "step": 17872
    },
    {
      "epoch": 0.9686720867208672,
      "step": 17872,
      "training_loss": 6.696885585784912
    },
    {
      "epoch": 0.9687262872628726,
      "step": 17873,
      "training_loss": 3.0481603145599365
    },
    {
      "epoch": 0.968780487804878,
      "step": 17874,
      "training_loss": 6.892460346221924
    },
    {
      "epoch": 0.9688346883468835,
      "step": 17875,
      "training_loss": 7.1331987380981445
    },
    {
      "epoch": 0.9688888888888889,
      "grad_norm": 17.898561477661133,
      "learning_rate": 1e-05,
      "loss": 5.9427,
      "step": 17876
    },
    {
      "epoch": 0.9688888888888889,
      "step": 17876,
      "training_loss": 7.03005838394165
    },
    {
      "epoch": 0.9689430894308944,
      "step": 17877,
      "training_loss": 6.608516693115234
    },
    {
      "epoch": 0.9689972899728997,
      "step": 17878,
      "training_loss": 5.380336761474609
    },
    {
      "epoch": 0.9690514905149051,
      "step": 17879,
      "training_loss": 7.353803634643555
    },
    {
      "epoch": 0.9691056910569106,
      "grad_norm": 20.94368553161621,
      "learning_rate": 1e-05,
      "loss": 6.5932,
      "step": 17880
    },
    {
      "epoch": 0.9691056910569106,
      "step": 17880,
      "training_loss": 6.249177932739258
    },
    {
      "epoch": 0.969159891598916,
      "step": 17881,
      "training_loss": 5.809084415435791
    },
    {
      "epoch": 0.9692140921409214,
      "step": 17882,
      "training_loss": 6.614796161651611
    },
    {
      "epoch": 0.9692682926829268,
      "step": 17883,
      "training_loss": 6.425238609313965
    },
    {
      "epoch": 0.9693224932249322,
      "grad_norm": 28.025785446166992,
      "learning_rate": 1e-05,
      "loss": 6.2746,
      "step": 17884
    },
    {
      "epoch": 0.9693224932249322,
      "step": 17884,
      "training_loss": 7.354322910308838
    },
    {
      "epoch": 0.9693766937669377,
      "step": 17885,
      "training_loss": 6.514432430267334
    },
    {
      "epoch": 0.9694308943089431,
      "step": 17886,
      "training_loss": 6.128574848175049
    },
    {
      "epoch": 0.9694850948509485,
      "step": 17887,
      "training_loss": 7.890405654907227
    },
    {
      "epoch": 0.9695392953929539,
      "grad_norm": 37.8133430480957,
      "learning_rate": 1e-05,
      "loss": 6.9719,
      "step": 17888
    },
    {
      "epoch": 0.9695392953929539,
      "step": 17888,
      "training_loss": 6.676238536834717
    },
    {
      "epoch": 0.9695934959349594,
      "step": 17889,
      "training_loss": 5.9319562911987305
    },
    {
      "epoch": 0.9696476964769648,
      "step": 17890,
      "training_loss": 6.0157694816589355
    },
    {
      "epoch": 0.9697018970189701,
      "step": 17891,
      "training_loss": 7.103900909423828
    },
    {
      "epoch": 0.9697560975609756,
      "grad_norm": 20.307390213012695,
      "learning_rate": 1e-05,
      "loss": 6.432,
      "step": 17892
    },
    {
      "epoch": 0.9697560975609756,
      "step": 17892,
      "training_loss": 6.92307710647583
    },
    {
      "epoch": 0.969810298102981,
      "step": 17893,
      "training_loss": 5.58407735824585
    },
    {
      "epoch": 0.9698644986449865,
      "step": 17894,
      "training_loss": 7.134535789489746
    },
    {
      "epoch": 0.9699186991869919,
      "step": 17895,
      "training_loss": 7.61102294921875
    },
    {
      "epoch": 0.9699728997289973,
      "grad_norm": 27.0516357421875,
      "learning_rate": 1e-05,
      "loss": 6.8132,
      "step": 17896
    },
    {
      "epoch": 0.9699728997289973,
      "step": 17896,
      "training_loss": 7.118065357208252
    },
    {
      "epoch": 0.9700271002710027,
      "step": 17897,
      "training_loss": 6.381786346435547
    },
    {
      "epoch": 0.9700813008130081,
      "step": 17898,
      "training_loss": 3.386256694793701
    },
    {
      "epoch": 0.9701355013550136,
      "step": 17899,
      "training_loss": 5.817796230316162
    },
    {
      "epoch": 0.9701897018970189,
      "grad_norm": 40.61204147338867,
      "learning_rate": 1e-05,
      "loss": 5.676,
      "step": 17900
    },
    {
      "epoch": 0.9701897018970189,
      "step": 17900,
      "training_loss": 7.339701175689697
    },
    {
      "epoch": 0.9702439024390244,
      "step": 17901,
      "training_loss": 7.546457290649414
    },
    {
      "epoch": 0.9702981029810298,
      "step": 17902,
      "training_loss": 6.800195217132568
    },
    {
      "epoch": 0.9703523035230353,
      "step": 17903,
      "training_loss": 8.12271785736084
    },
    {
      "epoch": 0.9704065040650407,
      "grad_norm": 21.38185691833496,
      "learning_rate": 1e-05,
      "loss": 7.4523,
      "step": 17904
    },
    {
      "epoch": 0.9704065040650407,
      "step": 17904,
      "training_loss": 7.1899614334106445
    },
    {
      "epoch": 0.970460704607046,
      "step": 17905,
      "training_loss": 6.604359149932861
    },
    {
      "epoch": 0.9705149051490515,
      "step": 17906,
      "training_loss": 5.934507369995117
    },
    {
      "epoch": 0.9705691056910569,
      "step": 17907,
      "training_loss": 6.9377241134643555
    },
    {
      "epoch": 0.9706233062330624,
      "grad_norm": 24.830379486083984,
      "learning_rate": 1e-05,
      "loss": 6.6666,
      "step": 17908
    },
    {
      "epoch": 0.9706233062330624,
      "step": 17908,
      "training_loss": 5.529821872711182
    },
    {
      "epoch": 0.9706775067750677,
      "step": 17909,
      "training_loss": 6.3726091384887695
    },
    {
      "epoch": 0.9707317073170731,
      "step": 17910,
      "training_loss": 7.155263900756836
    },
    {
      "epoch": 0.9707859078590786,
      "step": 17911,
      "training_loss": 8.49068546295166
    },
    {
      "epoch": 0.970840108401084,
      "grad_norm": 54.238426208496094,
      "learning_rate": 1e-05,
      "loss": 6.8871,
      "step": 17912
    },
    {
      "epoch": 0.970840108401084,
      "step": 17912,
      "training_loss": 7.2574005126953125
    },
    {
      "epoch": 0.9708943089430895,
      "step": 17913,
      "training_loss": 4.904709815979004
    },
    {
      "epoch": 0.9709485094850948,
      "step": 17914,
      "training_loss": 7.149785041809082
    },
    {
      "epoch": 0.9710027100271003,
      "step": 17915,
      "training_loss": 7.127742290496826
    },
    {
      "epoch": 0.9710569105691057,
      "grad_norm": 48.258323669433594,
      "learning_rate": 1e-05,
      "loss": 6.6099,
      "step": 17916
    },
    {
      "epoch": 0.9710569105691057,
      "step": 17916,
      "training_loss": 6.453567028045654
    },
    {
      "epoch": 0.9711111111111111,
      "step": 17917,
      "training_loss": 5.664980411529541
    },
    {
      "epoch": 0.9711653116531165,
      "step": 17918,
      "training_loss": 6.989452362060547
    },
    {
      "epoch": 0.9712195121951219,
      "step": 17919,
      "training_loss": 5.400267601013184
    },
    {
      "epoch": 0.9712737127371274,
      "grad_norm": 22.032238006591797,
      "learning_rate": 1e-05,
      "loss": 6.1271,
      "step": 17920
    },
    {
      "epoch": 0.9712737127371274,
      "step": 17920,
      "training_loss": 8.118544578552246
    },
    {
      "epoch": 0.9713279132791328,
      "step": 17921,
      "training_loss": 6.386972427368164
    },
    {
      "epoch": 0.9713821138211383,
      "step": 17922,
      "training_loss": 6.636639595031738
    },
    {
      "epoch": 0.9714363143631436,
      "step": 17923,
      "training_loss": 5.846674919128418
    },
    {
      "epoch": 0.971490514905149,
      "grad_norm": 41.1201286315918,
      "learning_rate": 1e-05,
      "loss": 6.7472,
      "step": 17924
    },
    {
      "epoch": 0.971490514905149,
      "step": 17924,
      "training_loss": 6.646268844604492
    },
    {
      "epoch": 0.9715447154471545,
      "step": 17925,
      "training_loss": 7.2367844581604
    },
    {
      "epoch": 0.9715989159891599,
      "step": 17926,
      "training_loss": 6.320105075836182
    },
    {
      "epoch": 0.9716531165311653,
      "step": 17927,
      "training_loss": 7.311168670654297
    },
    {
      "epoch": 0.9717073170731707,
      "grad_norm": 21.167808532714844,
      "learning_rate": 1e-05,
      "loss": 6.8786,
      "step": 17928
    },
    {
      "epoch": 0.9717073170731707,
      "step": 17928,
      "training_loss": 7.635982036590576
    },
    {
      "epoch": 0.9717615176151762,
      "step": 17929,
      "training_loss": 5.83378791809082
    },
    {
      "epoch": 0.9718157181571816,
      "step": 17930,
      "training_loss": 7.195252895355225
    },
    {
      "epoch": 0.971869918699187,
      "step": 17931,
      "training_loss": 6.3358612060546875
    },
    {
      "epoch": 0.9719241192411924,
      "grad_norm": 28.10455322265625,
      "learning_rate": 1e-05,
      "loss": 6.7502,
      "step": 17932
    },
    {
      "epoch": 0.9719241192411924,
      "step": 17932,
      "training_loss": 6.577620029449463
    },
    {
      "epoch": 0.9719783197831978,
      "step": 17933,
      "training_loss": 6.597155570983887
    },
    {
      "epoch": 0.9720325203252033,
      "step": 17934,
      "training_loss": 8.209281921386719
    },
    {
      "epoch": 0.9720867208672087,
      "step": 17935,
      "training_loss": 5.7862372398376465
    },
    {
      "epoch": 0.972140921409214,
      "grad_norm": 25.54834747314453,
      "learning_rate": 1e-05,
      "loss": 6.7926,
      "step": 17936
    },
    {
      "epoch": 0.972140921409214,
      "step": 17936,
      "training_loss": 5.578769207000732
    },
    {
      "epoch": 0.9721951219512195,
      "step": 17937,
      "training_loss": 4.490639686584473
    },
    {
      "epoch": 0.9722493224932249,
      "step": 17938,
      "training_loss": 7.185382843017578
    },
    {
      "epoch": 0.9723035230352304,
      "step": 17939,
      "training_loss": 7.518350124359131
    },
    {
      "epoch": 0.9723577235772358,
      "grad_norm": 29.8557186126709,
      "learning_rate": 1e-05,
      "loss": 6.1933,
      "step": 17940
    },
    {
      "epoch": 0.9723577235772358,
      "step": 17940,
      "training_loss": 4.7001729011535645
    },
    {
      "epoch": 0.9724119241192412,
      "step": 17941,
      "training_loss": 8.188547134399414
    },
    {
      "epoch": 0.9724661246612466,
      "step": 17942,
      "training_loss": 7.081676483154297
    },
    {
      "epoch": 0.972520325203252,
      "step": 17943,
      "training_loss": 7.154128551483154
    },
    {
      "epoch": 0.9725745257452575,
      "grad_norm": 20.695466995239258,
      "learning_rate": 1e-05,
      "loss": 6.7811,
      "step": 17944
    },
    {
      "epoch": 0.9725745257452575,
      "step": 17944,
      "training_loss": 2.9472198486328125
    },
    {
      "epoch": 0.9726287262872628,
      "step": 17945,
      "training_loss": 5.480737209320068
    },
    {
      "epoch": 0.9726829268292683,
      "step": 17946,
      "training_loss": 7.268416404724121
    },
    {
      "epoch": 0.9727371273712737,
      "step": 17947,
      "training_loss": 6.75064754486084
    },
    {
      "epoch": 0.9727913279132792,
      "grad_norm": 21.073917388916016,
      "learning_rate": 1e-05,
      "loss": 5.6118,
      "step": 17948
    },
    {
      "epoch": 0.9727913279132792,
      "step": 17948,
      "training_loss": 6.253859996795654
    },
    {
      "epoch": 0.9728455284552846,
      "step": 17949,
      "training_loss": 7.700929641723633
    },
    {
      "epoch": 0.9728997289972899,
      "step": 17950,
      "training_loss": 7.243316650390625
    },
    {
      "epoch": 0.9729539295392954,
      "step": 17951,
      "training_loss": 6.5316386222839355
    },
    {
      "epoch": 0.9730081300813008,
      "grad_norm": 16.372541427612305,
      "learning_rate": 1e-05,
      "loss": 6.9324,
      "step": 17952
    },
    {
      "epoch": 0.9730081300813008,
      "step": 17952,
      "training_loss": 7.644891262054443
    },
    {
      "epoch": 0.9730623306233063,
      "step": 17953,
      "training_loss": 5.870972633361816
    },
    {
      "epoch": 0.9731165311653116,
      "step": 17954,
      "training_loss": 7.672605991363525
    },
    {
      "epoch": 0.973170731707317,
      "step": 17955,
      "training_loss": 6.770830154418945
    },
    {
      "epoch": 0.9732249322493225,
      "grad_norm": 34.94174575805664,
      "learning_rate": 1e-05,
      "loss": 6.9898,
      "step": 17956
    },
    {
      "epoch": 0.9732249322493225,
      "step": 17956,
      "training_loss": 6.5621747970581055
    },
    {
      "epoch": 0.9732791327913279,
      "step": 17957,
      "training_loss": 5.568011283874512
    },
    {
      "epoch": 0.9733333333333334,
      "step": 17958,
      "training_loss": 6.609124660491943
    },
    {
      "epoch": 0.9733875338753387,
      "step": 17959,
      "training_loss": 5.6115617752075195
    },
    {
      "epoch": 0.9734417344173442,
      "grad_norm": 20.045190811157227,
      "learning_rate": 1e-05,
      "loss": 6.0877,
      "step": 17960
    },
    {
      "epoch": 0.9734417344173442,
      "step": 17960,
      "training_loss": 6.542125225067139
    },
    {
      "epoch": 0.9734959349593496,
      "step": 17961,
      "training_loss": 6.952816486358643
    },
    {
      "epoch": 0.973550135501355,
      "step": 17962,
      "training_loss": 6.404338836669922
    },
    {
      "epoch": 0.9736043360433604,
      "step": 17963,
      "training_loss": 7.202922821044922
    },
    {
      "epoch": 0.9736585365853658,
      "grad_norm": 50.87599182128906,
      "learning_rate": 1e-05,
      "loss": 6.7756,
      "step": 17964
    },
    {
      "epoch": 0.9736585365853658,
      "step": 17964,
      "training_loss": 6.699772357940674
    },
    {
      "epoch": 0.9737127371273713,
      "step": 17965,
      "training_loss": 5.451394557952881
    },
    {
      "epoch": 0.9737669376693767,
      "step": 17966,
      "training_loss": 6.156422138214111
    },
    {
      "epoch": 0.9738211382113822,
      "step": 17967,
      "training_loss": 7.3307976722717285
    },
    {
      "epoch": 0.9738753387533875,
      "grad_norm": 16.9200382232666,
      "learning_rate": 1e-05,
      "loss": 6.4096,
      "step": 17968
    },
    {
      "epoch": 0.9738753387533875,
      "step": 17968,
      "training_loss": 7.891706943511963
    },
    {
      "epoch": 0.9739295392953929,
      "step": 17969,
      "training_loss": 8.238505363464355
    },
    {
      "epoch": 0.9739837398373984,
      "step": 17970,
      "training_loss": 6.5615668296813965
    },
    {
      "epoch": 0.9740379403794038,
      "step": 17971,
      "training_loss": 5.296351909637451
    },
    {
      "epoch": 0.9740921409214092,
      "grad_norm": 26.320362091064453,
      "learning_rate": 1e-05,
      "loss": 6.997,
      "step": 17972
    },
    {
      "epoch": 0.9740921409214092,
      "step": 17972,
      "training_loss": 7.6499152183532715
    },
    {
      "epoch": 0.9741463414634146,
      "step": 17973,
      "training_loss": 7.032869338989258
    },
    {
      "epoch": 0.9742005420054201,
      "step": 17974,
      "training_loss": 7.563191890716553
    },
    {
      "epoch": 0.9742547425474255,
      "step": 17975,
      "training_loss": 7.456010341644287
    },
    {
      "epoch": 0.974308943089431,
      "grad_norm": 16.77284049987793,
      "learning_rate": 1e-05,
      "loss": 7.4255,
      "step": 17976
    },
    {
      "epoch": 0.974308943089431,
      "step": 17976,
      "training_loss": 6.694386959075928
    },
    {
      "epoch": 0.9743631436314363,
      "step": 17977,
      "training_loss": 7.702730655670166
    },
    {
      "epoch": 0.9744173441734417,
      "step": 17978,
      "training_loss": 3.9644100666046143
    },
    {
      "epoch": 0.9744715447154472,
      "step": 17979,
      "training_loss": 5.889127254486084
    },
    {
      "epoch": 0.9745257452574526,
      "grad_norm": 23.72762680053711,
      "learning_rate": 1e-05,
      "loss": 6.0627,
      "step": 17980
    },
    {
      "epoch": 0.9745257452574526,
      "step": 17980,
      "training_loss": 5.786532402038574
    },
    {
      "epoch": 0.974579945799458,
      "step": 17981,
      "training_loss": 7.243149757385254
    },
    {
      "epoch": 0.9746341463414634,
      "step": 17982,
      "training_loss": 6.043354034423828
    },
    {
      "epoch": 0.9746883468834688,
      "step": 17983,
      "training_loss": 6.887657642364502
    },
    {
      "epoch": 0.9747425474254743,
      "grad_norm": 30.325645446777344,
      "learning_rate": 1e-05,
      "loss": 6.4902,
      "step": 17984
    },
    {
      "epoch": 0.9747425474254743,
      "step": 17984,
      "training_loss": 5.216816425323486
    },
    {
      "epoch": 0.9747967479674797,
      "step": 17985,
      "training_loss": 6.53110933303833
    },
    {
      "epoch": 0.9748509485094851,
      "step": 17986,
      "training_loss": 3.4945523738861084
    },
    {
      "epoch": 0.9749051490514905,
      "step": 17987,
      "training_loss": 6.479580402374268
    },
    {
      "epoch": 0.974959349593496,
      "grad_norm": 27.069950103759766,
      "learning_rate": 1e-05,
      "loss": 5.4305,
      "step": 17988
    },
    {
      "epoch": 0.974959349593496,
      "step": 17988,
      "training_loss": 7.144223213195801
    },
    {
      "epoch": 0.9750135501355014,
      "step": 17989,
      "training_loss": 6.726395606994629
    },
    {
      "epoch": 0.9750677506775067,
      "step": 17990,
      "training_loss": 6.695329666137695
    },
    {
      "epoch": 0.9751219512195122,
      "step": 17991,
      "training_loss": 7.105203151702881
    },
    {
      "epoch": 0.9751761517615176,
      "grad_norm": 43.977317810058594,
      "learning_rate": 1e-05,
      "loss": 6.9178,
      "step": 17992
    },
    {
      "epoch": 0.9751761517615176,
      "step": 17992,
      "training_loss": 6.872227668762207
    },
    {
      "epoch": 0.9752303523035231,
      "step": 17993,
      "training_loss": 4.2532830238342285
    },
    {
      "epoch": 0.9752845528455285,
      "step": 17994,
      "training_loss": 7.581008434295654
    },
    {
      "epoch": 0.9753387533875338,
      "step": 17995,
      "training_loss": 6.876699924468994
    },
    {
      "epoch": 0.9753929539295393,
      "grad_norm": 17.80541229248047,
      "learning_rate": 1e-05,
      "loss": 6.3958,
      "step": 17996
    },
    {
      "epoch": 0.9753929539295393,
      "step": 17996,
      "training_loss": 4.550118923187256
    },
    {
      "epoch": 0.9754471544715447,
      "step": 17997,
      "training_loss": 7.082501411437988
    },
    {
      "epoch": 0.9755013550135502,
      "step": 17998,
      "training_loss": 7.480291366577148
    },
    {
      "epoch": 0.9755555555555555,
      "step": 17999,
      "training_loss": 8.067337036132812
    },
    {
      "epoch": 0.975609756097561,
      "grad_norm": 52.74172592163086,
      "learning_rate": 1e-05,
      "loss": 6.7951,
      "step": 18000
    },
    {
      "epoch": 0.975609756097561,
      "eval_runtime": 466.3648,
      "eval_samples_per_second": 4.396,
      "eval_steps_per_second": 4.396,
      "step": 18000
    },
    {
      "epoch": 0.975609756097561,
      "step": 18000,
      "training_loss": 7.381755828857422
    },
    {
      "epoch": 0.9756639566395664,
      "step": 18001,
      "training_loss": 5.88077974319458
    },
    {
      "epoch": 0.9757181571815718,
      "step": 18002,
      "training_loss": 5.210422992706299
    },
    {
      "epoch": 0.9757723577235773,
      "step": 18003,
      "training_loss": 6.303434371948242
    },
    {
      "epoch": 0.9758265582655826,
      "grad_norm": 23.699831008911133,
      "learning_rate": 1e-05,
      "loss": 6.1941,
      "step": 18004
    },
    {
      "epoch": 0.9758265582655826,
      "step": 18004,
      "training_loss": 6.549517631530762
    },
    {
      "epoch": 0.9758807588075881,
      "step": 18005,
      "training_loss": 7.033631801605225
    },
    {
      "epoch": 0.9759349593495935,
      "step": 18006,
      "training_loss": 7.797751426696777
    },
    {
      "epoch": 0.975989159891599,
      "step": 18007,
      "training_loss": 6.628506660461426
    },
    {
      "epoch": 0.9760433604336043,
      "grad_norm": 40.79899978637695,
      "learning_rate": 1e-05,
      "loss": 7.0024,
      "step": 18008
    },
    {
      "epoch": 0.9760433604336043,
      "step": 18008,
      "training_loss": 5.905080795288086
    },
    {
      "epoch": 0.9760975609756097,
      "step": 18009,
      "training_loss": 6.623448848724365
    },
    {
      "epoch": 0.9761517615176152,
      "step": 18010,
      "training_loss": 7.536134243011475
    },
    {
      "epoch": 0.9762059620596206,
      "step": 18011,
      "training_loss": 7.110294342041016
    },
    {
      "epoch": 0.9762601626016261,
      "grad_norm": 47.430049896240234,
      "learning_rate": 1e-05,
      "loss": 6.7937,
      "step": 18012
    },
    {
      "epoch": 0.9762601626016261,
      "step": 18012,
      "training_loss": 6.629335403442383
    },
    {
      "epoch": 0.9763143631436314,
      "step": 18013,
      "training_loss": 7.6827239990234375
    },
    {
      "epoch": 0.9763685636856368,
      "step": 18014,
      "training_loss": 7.316059112548828
    },
    {
      "epoch": 0.9764227642276423,
      "step": 18015,
      "training_loss": 7.6642913818359375
    },
    {
      "epoch": 0.9764769647696477,
      "grad_norm": 27.597440719604492,
      "learning_rate": 1e-05,
      "loss": 7.3231,
      "step": 18016
    },
    {
      "epoch": 0.9764769647696477,
      "step": 18016,
      "training_loss": 6.317391395568848
    },
    {
      "epoch": 0.9765311653116531,
      "step": 18017,
      "training_loss": 6.743782043457031
    },
    {
      "epoch": 0.9765853658536585,
      "step": 18018,
      "training_loss": 7.450860500335693
    },
    {
      "epoch": 0.976639566395664,
      "step": 18019,
      "training_loss": 6.800153732299805
    },
    {
      "epoch": 0.9766937669376694,
      "grad_norm": 22.227792739868164,
      "learning_rate": 1e-05,
      "loss": 6.828,
      "step": 18020
    },
    {
      "epoch": 0.9766937669376694,
      "step": 18020,
      "training_loss": 7.417730808258057
    },
    {
      "epoch": 0.9767479674796749,
      "step": 18021,
      "training_loss": 9.157959938049316
    },
    {
      "epoch": 0.9768021680216802,
      "step": 18022,
      "training_loss": 7.029854774475098
    },
    {
      "epoch": 0.9768563685636856,
      "step": 18023,
      "training_loss": 6.413259506225586
    },
    {
      "epoch": 0.9769105691056911,
      "grad_norm": 23.964643478393555,
      "learning_rate": 1e-05,
      "loss": 7.5047,
      "step": 18024
    },
    {
      "epoch": 0.9769105691056911,
      "step": 18024,
      "training_loss": 6.156361103057861
    },
    {
      "epoch": 0.9769647696476965,
      "step": 18025,
      "training_loss": 6.132875919342041
    },
    {
      "epoch": 0.9770189701897019,
      "step": 18026,
      "training_loss": 6.203532695770264
    },
    {
      "epoch": 0.9770731707317073,
      "step": 18027,
      "training_loss": 4.646199703216553
    },
    {
      "epoch": 0.9771273712737127,
      "grad_norm": 52.29353713989258,
      "learning_rate": 1e-05,
      "loss": 5.7847,
      "step": 18028
    },
    {
      "epoch": 0.9771273712737127,
      "step": 18028,
      "training_loss": 5.532069206237793
    },
    {
      "epoch": 0.9771815718157182,
      "step": 18029,
      "training_loss": 5.818079948425293
    },
    {
      "epoch": 0.9772357723577236,
      "step": 18030,
      "training_loss": 4.188046455383301
    },
    {
      "epoch": 0.977289972899729,
      "step": 18031,
      "training_loss": 6.97816801071167
    },
    {
      "epoch": 0.9773441734417344,
      "grad_norm": 22.768573760986328,
      "learning_rate": 1e-05,
      "loss": 5.6291,
      "step": 18032
    },
    {
      "epoch": 0.9773441734417344,
      "step": 18032,
      "training_loss": 5.776589870452881
    },
    {
      "epoch": 0.9773983739837399,
      "step": 18033,
      "training_loss": 6.2557692527771
    },
    {
      "epoch": 0.9774525745257453,
      "step": 18034,
      "training_loss": 6.819111347198486
    },
    {
      "epoch": 0.9775067750677506,
      "step": 18035,
      "training_loss": 7.031744003295898
    },
    {
      "epoch": 0.9775609756097561,
      "grad_norm": 38.184017181396484,
      "learning_rate": 1e-05,
      "loss": 6.4708,
      "step": 18036
    },
    {
      "epoch": 0.9775609756097561,
      "step": 18036,
      "training_loss": 6.867807388305664
    },
    {
      "epoch": 0.9776151761517615,
      "step": 18037,
      "training_loss": 4.327617168426514
    },
    {
      "epoch": 0.977669376693767,
      "step": 18038,
      "training_loss": 7.356688499450684
    },
    {
      "epoch": 0.9777235772357724,
      "step": 18039,
      "training_loss": 7.614027976989746
    },
    {
      "epoch": 0.9777777777777777,
      "grad_norm": 33.001625061035156,
      "learning_rate": 1e-05,
      "loss": 6.5415,
      "step": 18040
    },
    {
      "epoch": 0.9777777777777777,
      "step": 18040,
      "training_loss": 5.230513095855713
    },
    {
      "epoch": 0.9778319783197832,
      "step": 18041,
      "training_loss": 6.6154093742370605
    },
    {
      "epoch": 0.9778861788617886,
      "step": 18042,
      "training_loss": 6.800808429718018
    },
    {
      "epoch": 0.9779403794037941,
      "step": 18043,
      "training_loss": 5.115242004394531
    },
    {
      "epoch": 0.9779945799457994,
      "grad_norm": 37.14978790283203,
      "learning_rate": 1e-05,
      "loss": 5.9405,
      "step": 18044
    },
    {
      "epoch": 0.9779945799457994,
      "step": 18044,
      "training_loss": 6.782038688659668
    },
    {
      "epoch": 0.9780487804878049,
      "step": 18045,
      "training_loss": 6.7802557945251465
    },
    {
      "epoch": 0.9781029810298103,
      "step": 18046,
      "training_loss": 5.919986248016357
    },
    {
      "epoch": 0.9781571815718157,
      "step": 18047,
      "training_loss": 6.9490966796875
    },
    {
      "epoch": 0.9782113821138212,
      "grad_norm": 23.81862449645996,
      "learning_rate": 1e-05,
      "loss": 6.6078,
      "step": 18048
    },
    {
      "epoch": 0.9782113821138212,
      "step": 18048,
      "training_loss": 7.111525058746338
    },
    {
      "epoch": 0.9782655826558265,
      "step": 18049,
      "training_loss": 7.250848770141602
    },
    {
      "epoch": 0.978319783197832,
      "step": 18050,
      "training_loss": 7.335170269012451
    },
    {
      "epoch": 0.9783739837398374,
      "step": 18051,
      "training_loss": 6.1147637367248535
    },
    {
      "epoch": 0.9784281842818429,
      "grad_norm": 40.34854507446289,
      "learning_rate": 1e-05,
      "loss": 6.9531,
      "step": 18052
    },
    {
      "epoch": 0.9784281842818429,
      "step": 18052,
      "training_loss": 6.147735118865967
    },
    {
      "epoch": 0.9784823848238482,
      "step": 18053,
      "training_loss": 6.111837387084961
    },
    {
      "epoch": 0.9785365853658536,
      "step": 18054,
      "training_loss": 7.374765396118164
    },
    {
      "epoch": 0.9785907859078591,
      "step": 18055,
      "training_loss": 6.091123580932617
    },
    {
      "epoch": 0.9786449864498645,
      "grad_norm": 20.83499526977539,
      "learning_rate": 1e-05,
      "loss": 6.4314,
      "step": 18056
    },
    {
      "epoch": 0.9786449864498645,
      "step": 18056,
      "training_loss": 5.9749250411987305
    },
    {
      "epoch": 0.97869918699187,
      "step": 18057,
      "training_loss": 7.501789093017578
    },
    {
      "epoch": 0.9787533875338753,
      "step": 18058,
      "training_loss": 5.001307010650635
    },
    {
      "epoch": 0.9788075880758808,
      "step": 18059,
      "training_loss": 7.193220615386963
    },
    {
      "epoch": 0.9788617886178862,
      "grad_norm": 19.97594451904297,
      "learning_rate": 1e-05,
      "loss": 6.4178,
      "step": 18060
    },
    {
      "epoch": 0.9788617886178862,
      "step": 18060,
      "training_loss": 3.641115665435791
    },
    {
      "epoch": 0.9789159891598916,
      "step": 18061,
      "training_loss": 6.873680591583252
    },
    {
      "epoch": 0.978970189701897,
      "step": 18062,
      "training_loss": 5.265079975128174
    },
    {
      "epoch": 0.9790243902439024,
      "step": 18063,
      "training_loss": 6.898571968078613
    },
    {
      "epoch": 0.9790785907859079,
      "grad_norm": 19.142505645751953,
      "learning_rate": 1e-05,
      "loss": 5.6696,
      "step": 18064
    },
    {
      "epoch": 0.9790785907859079,
      "step": 18064,
      "training_loss": 6.790560245513916
    },
    {
      "epoch": 0.9791327913279133,
      "step": 18065,
      "training_loss": 7.624965190887451
    },
    {
      "epoch": 0.9791869918699186,
      "step": 18066,
      "training_loss": 4.642735481262207
    },
    {
      "epoch": 0.9792411924119241,
      "step": 18067,
      "training_loss": 6.637338161468506
    },
    {
      "epoch": 0.9792953929539295,
      "grad_norm": 19.622812271118164,
      "learning_rate": 1e-05,
      "loss": 6.4239,
      "step": 18068
    },
    {
      "epoch": 0.9792953929539295,
      "step": 18068,
      "training_loss": 7.692684650421143
    },
    {
      "epoch": 0.979349593495935,
      "step": 18069,
      "training_loss": 3.654845952987671
    },
    {
      "epoch": 0.9794037940379404,
      "step": 18070,
      "training_loss": 7.4355621337890625
    },
    {
      "epoch": 0.9794579945799458,
      "step": 18071,
      "training_loss": 8.730585098266602
    },
    {
      "epoch": 0.9795121951219512,
      "grad_norm": 33.766448974609375,
      "learning_rate": 1e-05,
      "loss": 6.8784,
      "step": 18072
    },
    {
      "epoch": 0.9795121951219512,
      "step": 18072,
      "training_loss": 6.418618202209473
    },
    {
      "epoch": 0.9795663956639566,
      "step": 18073,
      "training_loss": 6.221102237701416
    },
    {
      "epoch": 0.9796205962059621,
      "step": 18074,
      "training_loss": 6.463526725769043
    },
    {
      "epoch": 0.9796747967479674,
      "step": 18075,
      "training_loss": 8.364843368530273
    },
    {
      "epoch": 0.9797289972899729,
      "grad_norm": 56.71981430053711,
      "learning_rate": 1e-05,
      "loss": 6.867,
      "step": 18076
    },
    {
      "epoch": 0.9797289972899729,
      "step": 18076,
      "training_loss": 6.471710681915283
    },
    {
      "epoch": 0.9797831978319783,
      "step": 18077,
      "training_loss": 7.395788669586182
    },
    {
      "epoch": 0.9798373983739838,
      "step": 18078,
      "training_loss": 7.241029262542725
    },
    {
      "epoch": 0.9798915989159892,
      "step": 18079,
      "training_loss": 7.249617099761963
    },
    {
      "epoch": 0.9799457994579945,
      "grad_norm": 18.675987243652344,
      "learning_rate": 1e-05,
      "loss": 7.0895,
      "step": 18080
    },
    {
      "epoch": 0.9799457994579945,
      "step": 18080,
      "training_loss": 4.563072681427002
    },
    {
      "epoch": 0.98,
      "step": 18081,
      "training_loss": 2.8788201808929443
    },
    {
      "epoch": 0.9800542005420054,
      "step": 18082,
      "training_loss": 4.636317253112793
    },
    {
      "epoch": 0.9801084010840109,
      "step": 18083,
      "training_loss": 6.476785659790039
    },
    {
      "epoch": 0.9801626016260162,
      "grad_norm": 22.70265769958496,
      "learning_rate": 1e-05,
      "loss": 4.6387,
      "step": 18084
    },
    {
      "epoch": 0.9801626016260162,
      "step": 18084,
      "training_loss": 7.57996129989624
    },
    {
      "epoch": 0.9802168021680217,
      "step": 18085,
      "training_loss": 7.2832746505737305
    },
    {
      "epoch": 0.9802710027100271,
      "step": 18086,
      "training_loss": 6.830804824829102
    },
    {
      "epoch": 0.9803252032520325,
      "step": 18087,
      "training_loss": 6.814295768737793
    },
    {
      "epoch": 0.980379403794038,
      "grad_norm": 22.520357131958008,
      "learning_rate": 1e-05,
      "loss": 7.1271,
      "step": 18088
    },
    {
      "epoch": 0.980379403794038,
      "step": 18088,
      "training_loss": 7.237858295440674
    },
    {
      "epoch": 0.9804336043360433,
      "step": 18089,
      "training_loss": 6.248501300811768
    },
    {
      "epoch": 0.9804878048780488,
      "step": 18090,
      "training_loss": 7.775968074798584
    },
    {
      "epoch": 0.9805420054200542,
      "step": 18091,
      "training_loss": 8.020895004272461
    },
    {
      "epoch": 0.9805962059620597,
      "grad_norm": 42.734981536865234,
      "learning_rate": 1e-05,
      "loss": 7.3208,
      "step": 18092
    },
    {
      "epoch": 0.9805962059620597,
      "step": 18092,
      "training_loss": 6.45600700378418
    },
    {
      "epoch": 0.980650406504065,
      "step": 18093,
      "training_loss": 6.529827117919922
    },
    {
      "epoch": 0.9807046070460704,
      "step": 18094,
      "training_loss": 6.129210472106934
    },
    {
      "epoch": 0.9807588075880759,
      "step": 18095,
      "training_loss": 6.504322528839111
    },
    {
      "epoch": 0.9808130081300813,
      "grad_norm": 37.761844635009766,
      "learning_rate": 1e-05,
      "loss": 6.4048,
      "step": 18096
    },
    {
      "epoch": 0.9808130081300813,
      "step": 18096,
      "training_loss": 5.447867393493652
    },
    {
      "epoch": 0.9808672086720868,
      "step": 18097,
      "training_loss": 7.384051322937012
    },
    {
      "epoch": 0.9809214092140921,
      "step": 18098,
      "training_loss": 6.305700778961182
    },
    {
      "epoch": 0.9809756097560975,
      "step": 18099,
      "training_loss": 6.264237403869629
    },
    {
      "epoch": 0.981029810298103,
      "grad_norm": 21.38011360168457,
      "learning_rate": 1e-05,
      "loss": 6.3505,
      "step": 18100
    },
    {
      "epoch": 0.981029810298103,
      "step": 18100,
      "training_loss": 5.746036529541016
    },
    {
      "epoch": 0.9810840108401084,
      "step": 18101,
      "training_loss": 6.124135971069336
    },
    {
      "epoch": 0.9811382113821138,
      "step": 18102,
      "training_loss": 6.166012763977051
    },
    {
      "epoch": 0.9811924119241192,
      "step": 18103,
      "training_loss": 5.465888023376465
    },
    {
      "epoch": 0.9812466124661247,
      "grad_norm": 22.012277603149414,
      "learning_rate": 1e-05,
      "loss": 5.8755,
      "step": 18104
    },
    {
      "epoch": 0.9812466124661247,
      "step": 18104,
      "training_loss": 7.84353494644165
    },
    {
      "epoch": 0.9813008130081301,
      "step": 18105,
      "training_loss": 5.773528575897217
    },
    {
      "epoch": 0.9813550135501355,
      "step": 18106,
      "training_loss": 6.66151237487793
    },
    {
      "epoch": 0.9814092140921409,
      "step": 18107,
      "training_loss": 7.80822229385376
    },
    {
      "epoch": 0.9814634146341463,
      "grad_norm": 43.48711395263672,
      "learning_rate": 1e-05,
      "loss": 7.0217,
      "step": 18108
    },
    {
      "epoch": 0.9814634146341463,
      "step": 18108,
      "training_loss": 5.223689556121826
    },
    {
      "epoch": 0.9815176151761518,
      "step": 18109,
      "training_loss": 6.644377708435059
    },
    {
      "epoch": 0.9815718157181572,
      "step": 18110,
      "training_loss": 6.974461555480957
    },
    {
      "epoch": 0.9816260162601625,
      "step": 18111,
      "training_loss": 7.283863067626953
    },
    {
      "epoch": 0.981680216802168,
      "grad_norm": 24.494434356689453,
      "learning_rate": 1e-05,
      "loss": 6.5316,
      "step": 18112
    },
    {
      "epoch": 0.981680216802168,
      "step": 18112,
      "training_loss": 6.9276251792907715
    },
    {
      "epoch": 0.9817344173441734,
      "step": 18113,
      "training_loss": 6.135997772216797
    },
    {
      "epoch": 0.9817886178861789,
      "step": 18114,
      "training_loss": 7.415765762329102
    },
    {
      "epoch": 0.9818428184281843,
      "step": 18115,
      "training_loss": 6.373998165130615
    },
    {
      "epoch": 0.9818970189701897,
      "grad_norm": 19.42597007751465,
      "learning_rate": 1e-05,
      "loss": 6.7133,
      "step": 18116
    },
    {
      "epoch": 0.9818970189701897,
      "step": 18116,
      "training_loss": 7.280285358428955
    },
    {
      "epoch": 0.9819512195121951,
      "step": 18117,
      "training_loss": 6.852837085723877
    },
    {
      "epoch": 0.9820054200542006,
      "step": 18118,
      "training_loss": 3.1442885398864746
    },
    {
      "epoch": 0.982059620596206,
      "step": 18119,
      "training_loss": 7.587215900421143
    },
    {
      "epoch": 0.9821138211382113,
      "grad_norm": 20.515718460083008,
      "learning_rate": 1e-05,
      "loss": 6.2162,
      "step": 18120
    },
    {
      "epoch": 0.9821138211382113,
      "step": 18120,
      "training_loss": 6.477599143981934
    },
    {
      "epoch": 0.9821680216802168,
      "step": 18121,
      "training_loss": 6.414031982421875
    },
    {
      "epoch": 0.9822222222222222,
      "step": 18122,
      "training_loss": 6.577809810638428
    },
    {
      "epoch": 0.9822764227642277,
      "step": 18123,
      "training_loss": 6.0742597579956055
    },
    {
      "epoch": 0.9823306233062331,
      "grad_norm": 37.8873291015625,
      "learning_rate": 1e-05,
      "loss": 6.3859,
      "step": 18124
    },
    {
      "epoch": 0.9823306233062331,
      "step": 18124,
      "training_loss": 6.742999076843262
    },
    {
      "epoch": 0.9823848238482384,
      "step": 18125,
      "training_loss": 5.445432186126709
    },
    {
      "epoch": 0.9824390243902439,
      "step": 18126,
      "training_loss": 5.988912582397461
    },
    {
      "epoch": 0.9824932249322493,
      "step": 18127,
      "training_loss": 6.2218217849731445
    },
    {
      "epoch": 0.9825474254742548,
      "grad_norm": 32.442562103271484,
      "learning_rate": 1e-05,
      "loss": 6.0998,
      "step": 18128
    },
    {
      "epoch": 0.9825474254742548,
      "step": 18128,
      "training_loss": 5.665441513061523
    },
    {
      "epoch": 0.9826016260162601,
      "step": 18129,
      "training_loss": 6.55924654006958
    },
    {
      "epoch": 0.9826558265582656,
      "step": 18130,
      "training_loss": 6.217994213104248
    },
    {
      "epoch": 0.982710027100271,
      "step": 18131,
      "training_loss": 7.147294044494629
    },
    {
      "epoch": 0.9827642276422764,
      "grad_norm": 42.932064056396484,
      "learning_rate": 1e-05,
      "loss": 6.3975,
      "step": 18132
    },
    {
      "epoch": 0.9827642276422764,
      "step": 18132,
      "training_loss": 7.790714263916016
    },
    {
      "epoch": 0.9828184281842819,
      "step": 18133,
      "training_loss": 6.319421291351318
    },
    {
      "epoch": 0.9828726287262872,
      "step": 18134,
      "training_loss": 7.754025936126709
    },
    {
      "epoch": 0.9829268292682927,
      "step": 18135,
      "training_loss": 7.403501033782959
    },
    {
      "epoch": 0.9829810298102981,
      "grad_norm": 44.92253112792969,
      "learning_rate": 1e-05,
      "loss": 7.3169,
      "step": 18136
    },
    {
      "epoch": 0.9829810298102981,
      "step": 18136,
      "training_loss": 6.939364910125732
    },
    {
      "epoch": 0.9830352303523036,
      "step": 18137,
      "training_loss": 3.681910753250122
    },
    {
      "epoch": 0.9830894308943089,
      "step": 18138,
      "training_loss": 5.550745487213135
    },
    {
      "epoch": 0.9831436314363143,
      "step": 18139,
      "training_loss": 4.979991912841797
    },
    {
      "epoch": 0.9831978319783198,
      "grad_norm": 26.58431053161621,
      "learning_rate": 1e-05,
      "loss": 5.288,
      "step": 18140
    },
    {
      "epoch": 0.9831978319783198,
      "step": 18140,
      "training_loss": 8.528077125549316
    },
    {
      "epoch": 0.9832520325203252,
      "step": 18141,
      "training_loss": 5.8826141357421875
    },
    {
      "epoch": 0.9833062330623307,
      "step": 18142,
      "training_loss": 5.122344017028809
    },
    {
      "epoch": 0.983360433604336,
      "step": 18143,
      "training_loss": 5.949782848358154
    },
    {
      "epoch": 0.9834146341463414,
      "grad_norm": 43.281471252441406,
      "learning_rate": 1e-05,
      "loss": 6.3707,
      "step": 18144
    },
    {
      "epoch": 0.9834146341463414,
      "step": 18144,
      "training_loss": 6.145774841308594
    },
    {
      "epoch": 0.9834688346883469,
      "step": 18145,
      "training_loss": 6.798088073730469
    },
    {
      "epoch": 0.9835230352303523,
      "step": 18146,
      "training_loss": 6.321804046630859
    },
    {
      "epoch": 0.9835772357723577,
      "step": 18147,
      "training_loss": 6.501136779785156
    },
    {
      "epoch": 0.9836314363143631,
      "grad_norm": 41.031341552734375,
      "learning_rate": 1e-05,
      "loss": 6.4417,
      "step": 18148
    },
    {
      "epoch": 0.9836314363143631,
      "step": 18148,
      "training_loss": 4.984603404998779
    },
    {
      "epoch": 0.9836856368563686,
      "step": 18149,
      "training_loss": 8.161151885986328
    },
    {
      "epoch": 0.983739837398374,
      "step": 18150,
      "training_loss": 5.9008708000183105
    },
    {
      "epoch": 0.9837940379403795,
      "step": 18151,
      "training_loss": 5.758307456970215
    },
    {
      "epoch": 0.9838482384823848,
      "grad_norm": 28.959875106811523,
      "learning_rate": 1e-05,
      "loss": 6.2012,
      "step": 18152
    },
    {
      "epoch": 0.9838482384823848,
      "step": 18152,
      "training_loss": 7.426486492156982
    },
    {
      "epoch": 0.9839024390243902,
      "step": 18153,
      "training_loss": 7.6407880783081055
    },
    {
      "epoch": 0.9839566395663957,
      "step": 18154,
      "training_loss": 7.552921295166016
    },
    {
      "epoch": 0.9840108401084011,
      "step": 18155,
      "training_loss": 7.801660060882568
    },
    {
      "epoch": 0.9840650406504065,
      "grad_norm": 44.532127380371094,
      "learning_rate": 1e-05,
      "loss": 7.6055,
      "step": 18156
    },
    {
      "epoch": 0.9840650406504065,
      "step": 18156,
      "training_loss": 6.320589542388916
    },
    {
      "epoch": 0.9841192411924119,
      "step": 18157,
      "training_loss": 5.692004203796387
    },
    {
      "epoch": 0.9841734417344173,
      "step": 18158,
      "training_loss": 5.899074077606201
    },
    {
      "epoch": 0.9842276422764228,
      "step": 18159,
      "training_loss": 7.717435359954834
    },
    {
      "epoch": 0.9842818428184282,
      "grad_norm": 22.008766174316406,
      "learning_rate": 1e-05,
      "loss": 6.4073,
      "step": 18160
    },
    {
      "epoch": 0.9842818428184282,
      "step": 18160,
      "training_loss": 5.664306640625
    },
    {
      "epoch": 0.9843360433604336,
      "step": 18161,
      "training_loss": 7.046267986297607
    },
    {
      "epoch": 0.984390243902439,
      "step": 18162,
      "training_loss": 7.6467766761779785
    },
    {
      "epoch": 0.9844444444444445,
      "step": 18163,
      "training_loss": 5.859374046325684
    },
    {
      "epoch": 0.9844986449864499,
      "grad_norm": 40.024940490722656,
      "learning_rate": 1e-05,
      "loss": 6.5542,
      "step": 18164
    },
    {
      "epoch": 0.9844986449864499,
      "step": 18164,
      "training_loss": 2.854290246963501
    },
    {
      "epoch": 0.9845528455284552,
      "step": 18165,
      "training_loss": 7.2047905921936035
    },
    {
      "epoch": 0.9846070460704607,
      "step": 18166,
      "training_loss": 7.8380889892578125
    },
    {
      "epoch": 0.9846612466124661,
      "step": 18167,
      "training_loss": 6.33546781539917
    },
    {
      "epoch": 0.9847154471544716,
      "grad_norm": 35.792118072509766,
      "learning_rate": 1e-05,
      "loss": 6.0582,
      "step": 18168
    },
    {
      "epoch": 0.9847154471544716,
      "step": 18168,
      "training_loss": 5.806665420532227
    },
    {
      "epoch": 0.984769647696477,
      "step": 18169,
      "training_loss": 5.344751358032227
    },
    {
      "epoch": 0.9848238482384823,
      "step": 18170,
      "training_loss": 6.474518299102783
    },
    {
      "epoch": 0.9848780487804878,
      "step": 18171,
      "training_loss": 7.880478382110596
    },
    {
      "epoch": 0.9849322493224932,
      "grad_norm": 50.546871185302734,
      "learning_rate": 1e-05,
      "loss": 6.3766,
      "step": 18172
    },
    {
      "epoch": 0.9849322493224932,
      "step": 18172,
      "training_loss": 6.5066142082214355
    },
    {
      "epoch": 0.9849864498644987,
      "step": 18173,
      "training_loss": 7.503281593322754
    },
    {
      "epoch": 0.985040650406504,
      "step": 18174,
      "training_loss": 7.029974937438965
    },
    {
      "epoch": 0.9850948509485095,
      "step": 18175,
      "training_loss": 6.515500545501709
    },
    {
      "epoch": 0.9851490514905149,
      "grad_norm": 28.725940704345703,
      "learning_rate": 1e-05,
      "loss": 6.8888,
      "step": 18176
    },
    {
      "epoch": 0.9851490514905149,
      "step": 18176,
      "training_loss": 6.392836570739746
    },
    {
      "epoch": 0.9852032520325203,
      "step": 18177,
      "training_loss": 7.5784993171691895
    },
    {
      "epoch": 0.9852574525745258,
      "step": 18178,
      "training_loss": 6.327884197235107
    },
    {
      "epoch": 0.9853116531165311,
      "step": 18179,
      "training_loss": 4.364593029022217
    },
    {
      "epoch": 0.9853658536585366,
      "grad_norm": 21.292766571044922,
      "learning_rate": 1e-05,
      "loss": 6.166,
      "step": 18180
    },
    {
      "epoch": 0.9853658536585366,
      "step": 18180,
      "training_loss": 7.900360584259033
    },
    {
      "epoch": 0.985420054200542,
      "step": 18181,
      "training_loss": 7.114080429077148
    },
    {
      "epoch": 0.9854742547425475,
      "step": 18182,
      "training_loss": 7.163670539855957
    },
    {
      "epoch": 0.9855284552845528,
      "step": 18183,
      "training_loss": 6.710049629211426
    },
    {
      "epoch": 0.9855826558265582,
      "grad_norm": 30.890018463134766,
      "learning_rate": 1e-05,
      "loss": 7.222,
      "step": 18184
    },
    {
      "epoch": 0.9855826558265582,
      "step": 18184,
      "training_loss": 6.3595380783081055
    },
    {
      "epoch": 0.9856368563685637,
      "step": 18185,
      "training_loss": 7.186072826385498
    },
    {
      "epoch": 0.9856910569105691,
      "step": 18186,
      "training_loss": 8.13509464263916
    },
    {
      "epoch": 0.9857452574525746,
      "step": 18187,
      "training_loss": 5.306526184082031
    },
    {
      "epoch": 0.9857994579945799,
      "grad_norm": 32.40738296508789,
      "learning_rate": 1e-05,
      "loss": 6.7468,
      "step": 18188
    },
    {
      "epoch": 0.9857994579945799,
      "step": 18188,
      "training_loss": 6.430249214172363
    },
    {
      "epoch": 0.9858536585365854,
      "step": 18189,
      "training_loss": 7.186382293701172
    },
    {
      "epoch": 0.9859078590785908,
      "step": 18190,
      "training_loss": 4.99169921875
    },
    {
      "epoch": 0.9859620596205962,
      "step": 18191,
      "training_loss": 7.043741703033447
    },
    {
      "epoch": 0.9860162601626016,
      "grad_norm": 28.033191680908203,
      "learning_rate": 1e-05,
      "loss": 6.413,
      "step": 18192
    },
    {
      "epoch": 0.9860162601626016,
      "step": 18192,
      "training_loss": 5.5772013664245605
    },
    {
      "epoch": 0.986070460704607,
      "step": 18193,
      "training_loss": 7.203960418701172
    },
    {
      "epoch": 0.9861246612466125,
      "step": 18194,
      "training_loss": 7.201385974884033
    },
    {
      "epoch": 0.9861788617886179,
      "step": 18195,
      "training_loss": 8.094039916992188
    },
    {
      "epoch": 0.9862330623306234,
      "grad_norm": 30.072303771972656,
      "learning_rate": 1e-05,
      "loss": 7.0191,
      "step": 18196
    },
    {
      "epoch": 0.9862330623306234,
      "step": 18196,
      "training_loss": 6.529215335845947
    },
    {
      "epoch": 0.9862872628726287,
      "step": 18197,
      "training_loss": 6.03888463973999
    },
    {
      "epoch": 0.9863414634146341,
      "step": 18198,
      "training_loss": 6.997767925262451
    },
    {
      "epoch": 0.9863956639566396,
      "step": 18199,
      "training_loss": 6.859123706817627
    },
    {
      "epoch": 0.986449864498645,
      "grad_norm": 20.498394012451172,
      "learning_rate": 1e-05,
      "loss": 6.6062,
      "step": 18200
    },
    {
      "epoch": 0.986449864498645,
      "step": 18200,
      "training_loss": 7.293768405914307
    },
    {
      "epoch": 0.9865040650406504,
      "step": 18201,
      "training_loss": 6.263139247894287
    },
    {
      "epoch": 0.9865582655826558,
      "step": 18202,
      "training_loss": 6.083904266357422
    },
    {
      "epoch": 0.9866124661246612,
      "step": 18203,
      "training_loss": 5.3612847328186035
    },
    {
      "epoch": 0.9866666666666667,
      "grad_norm": 19.573820114135742,
      "learning_rate": 1e-05,
      "loss": 6.2505,
      "step": 18204
    },
    {
      "epoch": 0.9866666666666667,
      "step": 18204,
      "training_loss": 6.735308647155762
    },
    {
      "epoch": 0.9867208672086721,
      "step": 18205,
      "training_loss": 6.112903118133545
    },
    {
      "epoch": 0.9867750677506775,
      "step": 18206,
      "training_loss": 6.824870586395264
    },
    {
      "epoch": 0.9868292682926829,
      "step": 18207,
      "training_loss": 6.5225419998168945
    },
    {
      "epoch": 0.9868834688346884,
      "grad_norm": 38.58503341674805,
      "learning_rate": 1e-05,
      "loss": 6.5489,
      "step": 18208
    },
    {
      "epoch": 0.9868834688346884,
      "step": 18208,
      "training_loss": 7.885220527648926
    },
    {
      "epoch": 0.9869376693766938,
      "step": 18209,
      "training_loss": 7.238310813903809
    },
    {
      "epoch": 0.9869918699186991,
      "step": 18210,
      "training_loss": 7.609361171722412
    },
    {
      "epoch": 0.9870460704607046,
      "step": 18211,
      "training_loss": 7.157658100128174
    },
    {
      "epoch": 0.98710027100271,
      "grad_norm": 13.93069076538086,
      "learning_rate": 1e-05,
      "loss": 7.4726,
      "step": 18212
    },
    {
      "epoch": 0.98710027100271,
      "step": 18212,
      "training_loss": 7.162997722625732
    },
    {
      "epoch": 0.9871544715447155,
      "step": 18213,
      "training_loss": 7.418365955352783
    },
    {
      "epoch": 0.9872086720867209,
      "step": 18214,
      "training_loss": 5.594499111175537
    },
    {
      "epoch": 0.9872628726287263,
      "step": 18215,
      "training_loss": 6.555241584777832
    },
    {
      "epoch": 0.9873170731707317,
      "grad_norm": 27.177396774291992,
      "learning_rate": 1e-05,
      "loss": 6.6828,
      "step": 18216
    },
    {
      "epoch": 0.9873170731707317,
      "step": 18216,
      "training_loss": 6.631262302398682
    },
    {
      "epoch": 0.9873712737127371,
      "step": 18217,
      "training_loss": 4.136319160461426
    },
    {
      "epoch": 0.9874254742547426,
      "step": 18218,
      "training_loss": 7.241536617279053
    },
    {
      "epoch": 0.9874796747967479,
      "step": 18219,
      "training_loss": 5.717357635498047
    },
    {
      "epoch": 0.9875338753387534,
      "grad_norm": 43.99566650390625,
      "learning_rate": 1e-05,
      "loss": 5.9316,
      "step": 18220
    },
    {
      "epoch": 0.9875338753387534,
      "step": 18220,
      "training_loss": 6.3062567710876465
    },
    {
      "epoch": 0.9875880758807588,
      "step": 18221,
      "training_loss": 6.836838722229004
    },
    {
      "epoch": 0.9876422764227643,
      "step": 18222,
      "training_loss": 6.398355484008789
    },
    {
      "epoch": 0.9876964769647697,
      "step": 18223,
      "training_loss": 7.2186598777771
    },
    {
      "epoch": 0.987750677506775,
      "grad_norm": 32.38071823120117,
      "learning_rate": 1e-05,
      "loss": 6.69,
      "step": 18224
    },
    {
      "epoch": 0.987750677506775,
      "step": 18224,
      "training_loss": 4.582332611083984
    },
    {
      "epoch": 0.9878048780487805,
      "step": 18225,
      "training_loss": 5.9376726150512695
    },
    {
      "epoch": 0.9878590785907859,
      "step": 18226,
      "training_loss": 5.466579437255859
    },
    {
      "epoch": 0.9879132791327914,
      "step": 18227,
      "training_loss": 6.815408229827881
    },
    {
      "epoch": 0.9879674796747967,
      "grad_norm": 35.072349548339844,
      "learning_rate": 1e-05,
      "loss": 5.7005,
      "step": 18228
    },
    {
      "epoch": 0.9879674796747967,
      "step": 18228,
      "training_loss": 5.763455390930176
    },
    {
      "epoch": 0.9880216802168021,
      "step": 18229,
      "training_loss": 5.48003625869751
    },
    {
      "epoch": 0.9880758807588076,
      "step": 18230,
      "training_loss": 7.0500264167785645
    },
    {
      "epoch": 0.988130081300813,
      "step": 18231,
      "training_loss": 3.0325355529785156
    },
    {
      "epoch": 0.9881842818428185,
      "grad_norm": 33.20647430419922,
      "learning_rate": 1e-05,
      "loss": 5.3315,
      "step": 18232
    },
    {
      "epoch": 0.9881842818428185,
      "step": 18232,
      "training_loss": 6.948583126068115
    },
    {
      "epoch": 0.9882384823848238,
      "step": 18233,
      "training_loss": 6.671688079833984
    },
    {
      "epoch": 0.9882926829268293,
      "step": 18234,
      "training_loss": 6.6910481452941895
    },
    {
      "epoch": 0.9883468834688347,
      "step": 18235,
      "training_loss": 3.399252414703369
    },
    {
      "epoch": 0.9884010840108401,
      "grad_norm": 31.750303268432617,
      "learning_rate": 1e-05,
      "loss": 5.9276,
      "step": 18236
    },
    {
      "epoch": 0.9884010840108401,
      "step": 18236,
      "training_loss": 7.323452949523926
    },
    {
      "epoch": 0.9884552845528455,
      "step": 18237,
      "training_loss": 6.722628593444824
    },
    {
      "epoch": 0.9885094850948509,
      "step": 18238,
      "training_loss": 7.765341758728027
    },
    {
      "epoch": 0.9885636856368564,
      "step": 18239,
      "training_loss": 6.809994697570801
    },
    {
      "epoch": 0.9886178861788618,
      "grad_norm": 33.162899017333984,
      "learning_rate": 1e-05,
      "loss": 7.1554,
      "step": 18240
    },
    {
      "epoch": 0.9886178861788618,
      "step": 18240,
      "training_loss": 6.263932228088379
    },
    {
      "epoch": 0.9886720867208673,
      "step": 18241,
      "training_loss": 7.496626853942871
    },
    {
      "epoch": 0.9887262872628726,
      "step": 18242,
      "training_loss": 3.547006368637085
    },
    {
      "epoch": 0.988780487804878,
      "step": 18243,
      "training_loss": 4.8056111335754395
    },
    {
      "epoch": 0.9888346883468835,
      "grad_norm": 20.37884521484375,
      "learning_rate": 1e-05,
      "loss": 5.5283,
      "step": 18244
    },
    {
      "epoch": 0.9888346883468835,
      "step": 18244,
      "training_loss": 7.399076461791992
    },
    {
      "epoch": 0.9888888888888889,
      "step": 18245,
      "training_loss": 6.757614612579346
    },
    {
      "epoch": 0.9889430894308943,
      "step": 18246,
      "training_loss": 6.400229454040527
    },
    {
      "epoch": 0.9889972899728997,
      "step": 18247,
      "training_loss": 6.123354434967041
    },
    {
      "epoch": 0.9890514905149052,
      "grad_norm": 30.611221313476562,
      "learning_rate": 1e-05,
      "loss": 6.6701,
      "step": 18248
    },
    {
      "epoch": 0.9890514905149052,
      "step": 18248,
      "training_loss": 5.642604351043701
    },
    {
      "epoch": 0.9891056910569106,
      "step": 18249,
      "training_loss": 5.694250583648682
    },
    {
      "epoch": 0.989159891598916,
      "step": 18250,
      "training_loss": 6.323740005493164
    },
    {
      "epoch": 0.9892140921409214,
      "step": 18251,
      "training_loss": 5.027178764343262
    },
    {
      "epoch": 0.9892682926829268,
      "grad_norm": 28.218002319335938,
      "learning_rate": 1e-05,
      "loss": 5.6719,
      "step": 18252
    },
    {
      "epoch": 0.9892682926829268,
      "step": 18252,
      "training_loss": 6.938632965087891
    },
    {
      "epoch": 0.9893224932249323,
      "step": 18253,
      "training_loss": 8.556772232055664
    },
    {
      "epoch": 0.9893766937669377,
      "step": 18254,
      "training_loss": 4.483565807342529
    },
    {
      "epoch": 0.989430894308943,
      "step": 18255,
      "training_loss": 5.587284564971924
    },
    {
      "epoch": 0.9894850948509485,
      "grad_norm": 27.037004470825195,
      "learning_rate": 1e-05,
      "loss": 6.3916,
      "step": 18256
    },
    {
      "epoch": 0.9894850948509485,
      "step": 18256,
      "training_loss": 8.73674201965332
    },
    {
      "epoch": 0.9895392953929539,
      "step": 18257,
      "training_loss": 6.222282886505127
    },
    {
      "epoch": 0.9895934959349594,
      "step": 18258,
      "training_loss": 7.589062690734863
    },
    {
      "epoch": 0.9896476964769648,
      "step": 18259,
      "training_loss": 8.046031951904297
    },
    {
      "epoch": 0.9897018970189702,
      "grad_norm": 28.911914825439453,
      "learning_rate": 1e-05,
      "loss": 7.6485,
      "step": 18260
    },
    {
      "epoch": 0.9897018970189702,
      "step": 18260,
      "training_loss": 7.749255657196045
    },
    {
      "epoch": 0.9897560975609756,
      "step": 18261,
      "training_loss": 6.895077228546143
    },
    {
      "epoch": 0.989810298102981,
      "step": 18262,
      "training_loss": 6.911719799041748
    },
    {
      "epoch": 0.9898644986449865,
      "step": 18263,
      "training_loss": 6.943435192108154
    },
    {
      "epoch": 0.9899186991869918,
      "grad_norm": 25.341325759887695,
      "learning_rate": 1e-05,
      "loss": 7.1249,
      "step": 18264
    },
    {
      "epoch": 0.9899186991869918,
      "step": 18264,
      "training_loss": 6.2686357498168945
    },
    {
      "epoch": 0.9899728997289973,
      "step": 18265,
      "training_loss": 5.768682956695557
    },
    {
      "epoch": 0.9900271002710027,
      "step": 18266,
      "training_loss": 7.204646110534668
    },
    {
      "epoch": 0.9900813008130082,
      "step": 18267,
      "training_loss": 6.826138019561768
    },
    {
      "epoch": 0.9901355013550136,
      "grad_norm": 19.704309463500977,
      "learning_rate": 1e-05,
      "loss": 6.517,
      "step": 18268
    },
    {
      "epoch": 0.9901355013550136,
      "step": 18268,
      "training_loss": 7.379114151000977
    },
    {
      "epoch": 0.9901897018970189,
      "step": 18269,
      "training_loss": 7.040218830108643
    },
    {
      "epoch": 0.9902439024390244,
      "step": 18270,
      "training_loss": 7.380478858947754
    },
    {
      "epoch": 0.9902981029810298,
      "step": 18271,
      "training_loss": 7.408631801605225
    },
    {
      "epoch": 0.9903523035230353,
      "grad_norm": 21.862985610961914,
      "learning_rate": 1e-05,
      "loss": 7.3021,
      "step": 18272
    },
    {
      "epoch": 0.9903523035230353,
      "step": 18272,
      "training_loss": 6.326562881469727
    },
    {
      "epoch": 0.9904065040650406,
      "step": 18273,
      "training_loss": 6.837579727172852
    },
    {
      "epoch": 0.990460704607046,
      "step": 18274,
      "training_loss": 6.3440022468566895
    },
    {
      "epoch": 0.9905149051490515,
      "step": 18275,
      "training_loss": 7.852084159851074
    },
    {
      "epoch": 0.9905691056910569,
      "grad_norm": 35.254276275634766,
      "learning_rate": 1e-05,
      "loss": 6.8401,
      "step": 18276
    },
    {
      "epoch": 0.9905691056910569,
      "step": 18276,
      "training_loss": 7.003269672393799
    },
    {
      "epoch": 0.9906233062330624,
      "step": 18277,
      "training_loss": 4.13967227935791
    },
    {
      "epoch": 0.9906775067750677,
      "step": 18278,
      "training_loss": 6.778923511505127
    },
    {
      "epoch": 0.9907317073170732,
      "step": 18279,
      "training_loss": 6.988128662109375
    },
    {
      "epoch": 0.9907859078590786,
      "grad_norm": 35.409706115722656,
      "learning_rate": 1e-05,
      "loss": 6.2275,
      "step": 18280
    },
    {
      "epoch": 0.9907859078590786,
      "step": 18280,
      "training_loss": 6.246025085449219
    },
    {
      "epoch": 0.990840108401084,
      "step": 18281,
      "training_loss": 6.669661045074463
    },
    {
      "epoch": 0.9908943089430894,
      "step": 18282,
      "training_loss": 6.879344463348389
    },
    {
      "epoch": 0.9909485094850948,
      "step": 18283,
      "training_loss": 8.591107368469238
    },
    {
      "epoch": 0.9910027100271003,
      "grad_norm": 48.283599853515625,
      "learning_rate": 1e-05,
      "loss": 7.0965,
      "step": 18284
    },
    {
      "epoch": 0.9910027100271003,
      "step": 18284,
      "training_loss": 6.2102885246276855
    },
    {
      "epoch": 0.9910569105691057,
      "step": 18285,
      "training_loss": 7.27458381652832
    },
    {
      "epoch": 0.9911111111111112,
      "step": 18286,
      "training_loss": 7.187037467956543
    },
    {
      "epoch": 0.9911653116531165,
      "step": 18287,
      "training_loss": 6.2672953605651855
    },
    {
      "epoch": 0.9912195121951219,
      "grad_norm": 28.48366928100586,
      "learning_rate": 1e-05,
      "loss": 6.7348,
      "step": 18288
    },
    {
      "epoch": 0.9912195121951219,
      "step": 18288,
      "training_loss": 6.090367317199707
    },
    {
      "epoch": 0.9912737127371274,
      "step": 18289,
      "training_loss": 7.416592121124268
    },
    {
      "epoch": 0.9913279132791328,
      "step": 18290,
      "training_loss": 7.429004669189453
    },
    {
      "epoch": 0.9913821138211382,
      "step": 18291,
      "training_loss": 6.577320575714111
    },
    {
      "epoch": 0.9914363143631436,
      "grad_norm": 31.560340881347656,
      "learning_rate": 1e-05,
      "loss": 6.8783,
      "step": 18292
    },
    {
      "epoch": 0.9914363143631436,
      "step": 18292,
      "training_loss": 8.187623977661133
    },
    {
      "epoch": 0.9914905149051491,
      "step": 18293,
      "training_loss": 7.07763671875
    },
    {
      "epoch": 0.9915447154471545,
      "step": 18294,
      "training_loss": 5.543091297149658
    },
    {
      "epoch": 0.99159891598916,
      "step": 18295,
      "training_loss": 6.970612525939941
    },
    {
      "epoch": 0.9916531165311653,
      "grad_norm": 54.47494125366211,
      "learning_rate": 1e-05,
      "loss": 6.9447,
      "step": 18296
    },
    {
      "epoch": 0.9916531165311653,
      "step": 18296,
      "training_loss": 3.330552101135254
    },
    {
      "epoch": 0.9917073170731707,
      "step": 18297,
      "training_loss": 5.566018104553223
    },
    {
      "epoch": 0.9917615176151762,
      "step": 18298,
      "training_loss": 6.936773300170898
    },
    {
      "epoch": 0.9918157181571816,
      "step": 18299,
      "training_loss": 8.628167152404785
    },
    {
      "epoch": 0.991869918699187,
      "grad_norm": 33.874961853027344,
      "learning_rate": 1e-05,
      "loss": 6.1154,
      "step": 18300
    },
    {
      "epoch": 0.991869918699187,
      "step": 18300,
      "training_loss": 6.341502666473389
    },
    {
      "epoch": 0.9919241192411924,
      "step": 18301,
      "training_loss": 7.447516918182373
    },
    {
      "epoch": 0.9919783197831978,
      "step": 18302,
      "training_loss": 5.4748053550720215
    },
    {
      "epoch": 0.9920325203252033,
      "step": 18303,
      "training_loss": 7.17805814743042
    },
    {
      "epoch": 0.9920867208672087,
      "grad_norm": 20.0711669921875,
      "learning_rate": 1e-05,
      "loss": 6.6105,
      "step": 18304
    },
    {
      "epoch": 0.9920867208672087,
      "step": 18304,
      "training_loss": 5.843175888061523
    },
    {
      "epoch": 0.9921409214092141,
      "step": 18305,
      "training_loss": 6.237451553344727
    },
    {
      "epoch": 0.9921951219512195,
      "step": 18306,
      "training_loss": 5.877425670623779
    },
    {
      "epoch": 0.992249322493225,
      "step": 18307,
      "training_loss": 6.29387092590332
    },
    {
      "epoch": 0.9923035230352304,
      "grad_norm": 23.121593475341797,
      "learning_rate": 1e-05,
      "loss": 6.063,
      "step": 18308
    },
    {
      "epoch": 0.9923035230352304,
      "step": 18308,
      "training_loss": 5.531890392303467
    },
    {
      "epoch": 0.9923577235772357,
      "step": 18309,
      "training_loss": 7.417318344116211
    },
    {
      "epoch": 0.9924119241192412,
      "step": 18310,
      "training_loss": 5.888968467712402
    },
    {
      "epoch": 0.9924661246612466,
      "step": 18311,
      "training_loss": 7.60532283782959
    },
    {
      "epoch": 0.9925203252032521,
      "grad_norm": 39.974632263183594,
      "learning_rate": 1e-05,
      "loss": 6.6109,
      "step": 18312
    },
    {
      "epoch": 0.9925203252032521,
      "step": 18312,
      "training_loss": 7.334068298339844
    },
    {
      "epoch": 0.9925745257452575,
      "step": 18313,
      "training_loss": 6.201536655426025
    },
    {
      "epoch": 0.9926287262872628,
      "step": 18314,
      "training_loss": 5.341745376586914
    },
    {
      "epoch": 0.9926829268292683,
      "step": 18315,
      "training_loss": 6.55202054977417
    },
    {
      "epoch": 0.9927371273712737,
      "grad_norm": 26.791156768798828,
      "learning_rate": 1e-05,
      "loss": 6.3573,
      "step": 18316
    },
    {
      "epoch": 0.9927371273712737,
      "step": 18316,
      "training_loss": 6.734784126281738
    },
    {
      "epoch": 0.9927913279132792,
      "step": 18317,
      "training_loss": 6.180556774139404
    },
    {
      "epoch": 0.9928455284552845,
      "step": 18318,
      "training_loss": 5.813499450683594
    },
    {
      "epoch": 0.99289972899729,
      "step": 18319,
      "training_loss": 3.7585983276367188
    },
    {
      "epoch": 0.9929539295392954,
      "grad_norm": 32.7746696472168,
      "learning_rate": 1e-05,
      "loss": 5.6219,
      "step": 18320
    },
    {
      "epoch": 0.9929539295392954,
      "step": 18320,
      "training_loss": 6.5755767822265625
    },
    {
      "epoch": 0.9930081300813008,
      "step": 18321,
      "training_loss": 6.168818950653076
    },
    {
      "epoch": 0.9930623306233062,
      "step": 18322,
      "training_loss": 6.273530006408691
    },
    {
      "epoch": 0.9931165311653116,
      "step": 18323,
      "training_loss": 7.0977582931518555
    },
    {
      "epoch": 0.9931707317073171,
      "grad_norm": 16.475482940673828,
      "learning_rate": 1e-05,
      "loss": 6.5289,
      "step": 18324
    },
    {
      "epoch": 0.9931707317073171,
      "step": 18324,
      "training_loss": 7.108664035797119
    },
    {
      "epoch": 0.9932249322493225,
      "step": 18325,
      "training_loss": 6.859626770019531
    },
    {
      "epoch": 0.993279132791328,
      "step": 18326,
      "training_loss": 6.320417404174805
    },
    {
      "epoch": 0.9933333333333333,
      "step": 18327,
      "training_loss": 6.372952938079834
    },
    {
      "epoch": 0.9933875338753387,
      "grad_norm": 26.32244873046875,
      "learning_rate": 1e-05,
      "loss": 6.6654,
      "step": 18328
    },
    {
      "epoch": 0.9933875338753387,
      "step": 18328,
      "training_loss": 6.635621070861816
    },
    {
      "epoch": 0.9934417344173442,
      "step": 18329,
      "training_loss": 6.835999965667725
    },
    {
      "epoch": 0.9934959349593496,
      "step": 18330,
      "training_loss": 6.772094249725342
    },
    {
      "epoch": 0.993550135501355,
      "step": 18331,
      "training_loss": 6.378615856170654
    },
    {
      "epoch": 0.9936043360433604,
      "grad_norm": 28.943159103393555,
      "learning_rate": 1e-05,
      "loss": 6.6556,
      "step": 18332
    },
    {
      "epoch": 0.9936043360433604,
      "step": 18332,
      "training_loss": 6.78654670715332
    },
    {
      "epoch": 0.9936585365853658,
      "step": 18333,
      "training_loss": 5.22908878326416
    },
    {
      "epoch": 0.9937127371273713,
      "step": 18334,
      "training_loss": 6.29437255859375
    },
    {
      "epoch": 0.9937669376693767,
      "step": 18335,
      "training_loss": 5.82656717300415
    },
    {
      "epoch": 0.9938211382113821,
      "grad_norm": 31.524330139160156,
      "learning_rate": 1e-05,
      "loss": 6.0341,
      "step": 18336
    },
    {
      "epoch": 0.9938211382113821,
      "step": 18336,
      "training_loss": 6.690455913543701
    },
    {
      "epoch": 0.9938753387533875,
      "step": 18337,
      "training_loss": 6.168380260467529
    },
    {
      "epoch": 0.993929539295393,
      "step": 18338,
      "training_loss": 7.46333122253418
    },
    {
      "epoch": 0.9939837398373984,
      "step": 18339,
      "training_loss": 6.740591049194336
    },
    {
      "epoch": 0.9940379403794037,
      "grad_norm": 24.639917373657227,
      "learning_rate": 1e-05,
      "loss": 6.7657,
      "step": 18340
    },
    {
      "epoch": 0.9940379403794037,
      "step": 18340,
      "training_loss": 6.527111053466797
    },
    {
      "epoch": 0.9940921409214092,
      "step": 18341,
      "training_loss": 7.482625484466553
    },
    {
      "epoch": 0.9941463414634146,
      "step": 18342,
      "training_loss": 7.238159656524658
    },
    {
      "epoch": 0.9942005420054201,
      "step": 18343,
      "training_loss": 7.585014343261719
    },
    {
      "epoch": 0.9942547425474255,
      "grad_norm": 23.60405731201172,
      "learning_rate": 1e-05,
      "loss": 7.2082,
      "step": 18344
    },
    {
      "epoch": 0.9942547425474255,
      "step": 18344,
      "training_loss": 6.8388214111328125
    },
    {
      "epoch": 0.9943089430894309,
      "step": 18345,
      "training_loss": 4.9993205070495605
    },
    {
      "epoch": 0.9943631436314363,
      "step": 18346,
      "training_loss": 6.554224967956543
    },
    {
      "epoch": 0.9944173441734417,
      "step": 18347,
      "training_loss": 7.955802917480469
    },
    {
      "epoch": 0.9944715447154472,
      "grad_norm": 33.94646453857422,
      "learning_rate": 1e-05,
      "loss": 6.587,
      "step": 18348
    },
    {
      "epoch": 0.9944715447154472,
      "step": 18348,
      "training_loss": 6.1312713623046875
    },
    {
      "epoch": 0.9945257452574525,
      "step": 18349,
      "training_loss": 8.181276321411133
    },
    {
      "epoch": 0.994579945799458,
      "step": 18350,
      "training_loss": 6.262284755706787
    },
    {
      "epoch": 0.9946341463414634,
      "step": 18351,
      "training_loss": 5.510382652282715
    },
    {
      "epoch": 0.9946883468834689,
      "grad_norm": 75.25205993652344,
      "learning_rate": 1e-05,
      "loss": 6.5213,
      "step": 18352
    },
    {
      "epoch": 0.9946883468834689,
      "step": 18352,
      "training_loss": 5.310559272766113
    },
    {
      "epoch": 0.9947425474254743,
      "step": 18353,
      "training_loss": 6.8370585441589355
    },
    {
      "epoch": 0.9947967479674796,
      "step": 18354,
      "training_loss": 7.008017063140869
    },
    {
      "epoch": 0.9948509485094851,
      "step": 18355,
      "training_loss": 6.618661403656006
    },
    {
      "epoch": 0.9949051490514905,
      "grad_norm": 30.899173736572266,
      "learning_rate": 1e-05,
      "loss": 6.4436,
      "step": 18356
    },
    {
      "epoch": 0.9949051490514905,
      "step": 18356,
      "training_loss": 5.88762092590332
    },
    {
      "epoch": 0.994959349593496,
      "step": 18357,
      "training_loss": 6.697902202606201
    },
    {
      "epoch": 0.9950135501355013,
      "step": 18358,
      "training_loss": 6.218449115753174
    },
    {
      "epoch": 0.9950677506775067,
      "step": 18359,
      "training_loss": 7.321391582489014
    },
    {
      "epoch": 0.9951219512195122,
      "grad_norm": 23.07063102722168,
      "learning_rate": 1e-05,
      "loss": 6.5313,
      "step": 18360
    },
    {
      "epoch": 0.9951219512195122,
      "step": 18360,
      "training_loss": 6.469702243804932
    },
    {
      "epoch": 0.9951761517615176,
      "step": 18361,
      "training_loss": 6.741983413696289
    },
    {
      "epoch": 0.9952303523035231,
      "step": 18362,
      "training_loss": 5.7033867835998535
    },
    {
      "epoch": 0.9952845528455284,
      "step": 18363,
      "training_loss": 5.222264766693115
    },
    {
      "epoch": 0.9953387533875339,
      "grad_norm": 25.153059005737305,
      "learning_rate": 1e-05,
      "loss": 6.0343,
      "step": 18364
    },
    {
      "epoch": 0.9953387533875339,
      "step": 18364,
      "training_loss": 5.785714626312256
    },
    {
      "epoch": 0.9953929539295393,
      "step": 18365,
      "training_loss": 6.552289009094238
    },
    {
      "epoch": 0.9954471544715447,
      "step": 18366,
      "training_loss": 6.282066822052002
    },
    {
      "epoch": 0.9955013550135501,
      "step": 18367,
      "training_loss": 6.738900184631348
    },
    {
      "epoch": 0.9955555555555555,
      "grad_norm": 17.926231384277344,
      "learning_rate": 1e-05,
      "loss": 6.3397,
      "step": 18368
    },
    {
      "epoch": 0.9955555555555555,
      "step": 18368,
      "training_loss": 4.367718696594238
    },
    {
      "epoch": 0.995609756097561,
      "step": 18369,
      "training_loss": 6.528543472290039
    },
    {
      "epoch": 0.9956639566395664,
      "step": 18370,
      "training_loss": 6.177882194519043
    },
    {
      "epoch": 0.9957181571815719,
      "step": 18371,
      "training_loss": 6.368006229400635
    },
    {
      "epoch": 0.9957723577235772,
      "grad_norm": 31.291173934936523,
      "learning_rate": 1e-05,
      "loss": 5.8605,
      "step": 18372
    },
    {
      "epoch": 0.9957723577235772,
      "step": 18372,
      "training_loss": 5.3947434425354
    },
    {
      "epoch": 0.9958265582655826,
      "step": 18373,
      "training_loss": 2.713629961013794
    },
    {
      "epoch": 0.9958807588075881,
      "step": 18374,
      "training_loss": 6.250790119171143
    },
    {
      "epoch": 0.9959349593495935,
      "step": 18375,
      "training_loss": 6.671384334564209
    },
    {
      "epoch": 0.9959891598915989,
      "grad_norm": 25.74505043029785,
      "learning_rate": 1e-05,
      "loss": 5.2576,
      "step": 18376
    },
    {
      "epoch": 0.9959891598915989,
      "step": 18376,
      "training_loss": 8.122693061828613
    },
    {
      "epoch": 0.9960433604336043,
      "step": 18377,
      "training_loss": 3.6313257217407227
    },
    {
      "epoch": 0.9960975609756098,
      "step": 18378,
      "training_loss": 6.5613813400268555
    },
    {
      "epoch": 0.9961517615176152,
      "step": 18379,
      "training_loss": 7.51607608795166
    },
    {
      "epoch": 0.9962059620596206,
      "grad_norm": 20.821184158325195,
      "learning_rate": 1e-05,
      "loss": 6.4579,
      "step": 18380
    },
    {
      "epoch": 0.9962059620596206,
      "step": 18380,
      "training_loss": 6.9385576248168945
    },
    {
      "epoch": 0.996260162601626,
      "step": 18381,
      "training_loss": 6.496454238891602
    },
    {
      "epoch": 0.9963143631436314,
      "step": 18382,
      "training_loss": 6.5707831382751465
    },
    {
      "epoch": 0.9963685636856369,
      "step": 18383,
      "training_loss": 7.381438255310059
    },
    {
      "epoch": 0.9964227642276423,
      "grad_norm": 23.060466766357422,
      "learning_rate": 1e-05,
      "loss": 6.8468,
      "step": 18384
    },
    {
      "epoch": 0.9964227642276423,
      "step": 18384,
      "training_loss": 7.4396138191223145
    },
    {
      "epoch": 0.9964769647696476,
      "step": 18385,
      "training_loss": 6.739831447601318
    },
    {
      "epoch": 0.9965311653116531,
      "step": 18386,
      "training_loss": 7.506170749664307
    },
    {
      "epoch": 0.9965853658536585,
      "step": 18387,
      "training_loss": 6.954015254974365
    },
    {
      "epoch": 0.996639566395664,
      "grad_norm": 34.75117111206055,
      "learning_rate": 1e-05,
      "loss": 7.1599,
      "step": 18388
    },
    {
      "epoch": 0.996639566395664,
      "step": 18388,
      "training_loss": 6.877797603607178
    },
    {
      "epoch": 0.9966937669376694,
      "step": 18389,
      "training_loss": 6.59373664855957
    },
    {
      "epoch": 0.9967479674796748,
      "step": 18390,
      "training_loss": 6.758548259735107
    },
    {
      "epoch": 0.9968021680216802,
      "step": 18391,
      "training_loss": 6.050397872924805
    },
    {
      "epoch": 0.9968563685636856,
      "grad_norm": 32.79024887084961,
      "learning_rate": 1e-05,
      "loss": 6.5701,
      "step": 18392
    },
    {
      "epoch": 0.9968563685636856,
      "step": 18392,
      "training_loss": 7.175841331481934
    },
    {
      "epoch": 0.9969105691056911,
      "step": 18393,
      "training_loss": 6.542922496795654
    },
    {
      "epoch": 0.9969647696476964,
      "step": 18394,
      "training_loss": 6.453057765960693
    },
    {
      "epoch": 0.9970189701897019,
      "step": 18395,
      "training_loss": 7.507397651672363
    },
    {
      "epoch": 0.9970731707317073,
      "grad_norm": 21.840160369873047,
      "learning_rate": 1e-05,
      "loss": 6.9198,
      "step": 18396
    },
    {
      "epoch": 0.9970731707317073,
      "step": 18396,
      "training_loss": 5.456941604614258
    },
    {
      "epoch": 0.9971273712737128,
      "step": 18397,
      "training_loss": 7.621942520141602
    },
    {
      "epoch": 0.9971815718157182,
      "step": 18398,
      "training_loss": 6.3489484786987305
    },
    {
      "epoch": 0.9972357723577235,
      "step": 18399,
      "training_loss": 5.996794700622559
    },
    {
      "epoch": 0.997289972899729,
      "grad_norm": 19.381406784057617,
      "learning_rate": 1e-05,
      "loss": 6.3562,
      "step": 18400
    },
    {
      "epoch": 0.997289972899729,
      "step": 18400,
      "training_loss": 6.407830238342285
    },
    {
      "epoch": 0.9973441734417344,
      "step": 18401,
      "training_loss": 5.880231857299805
    },
    {
      "epoch": 0.9973983739837399,
      "step": 18402,
      "training_loss": 6.691018104553223
    },
    {
      "epoch": 0.9974525745257452,
      "step": 18403,
      "training_loss": 4.669883728027344
    },
    {
      "epoch": 0.9975067750677507,
      "grad_norm": 27.778806686401367,
      "learning_rate": 1e-05,
      "loss": 5.9122,
      "step": 18404
    },
    {
      "epoch": 0.9975067750677507,
      "step": 18404,
      "training_loss": 4.352049827575684
    },
    {
      "epoch": 0.9975609756097561,
      "step": 18405,
      "training_loss": 7.932835102081299
    },
    {
      "epoch": 0.9976151761517615,
      "step": 18406,
      "training_loss": 6.460053443908691
    },
    {
      "epoch": 0.997669376693767,
      "step": 18407,
      "training_loss": 6.787561416625977
    },
    {
      "epoch": 0.9977235772357723,
      "grad_norm": 30.399229049682617,
      "learning_rate": 1e-05,
      "loss": 6.3831,
      "step": 18408
    },
    {
      "epoch": 0.9977235772357723,
      "step": 18408,
      "training_loss": 7.37664794921875
    },
    {
      "epoch": 0.9977777777777778,
      "step": 18409,
      "training_loss": 7.287808418273926
    },
    {
      "epoch": 0.9978319783197832,
      "step": 18410,
      "training_loss": 4.172555446624756
    },
    {
      "epoch": 0.9978861788617887,
      "step": 18411,
      "training_loss": 7.677814483642578
    },
    {
      "epoch": 0.997940379403794,
      "grad_norm": 21.694135665893555,
      "learning_rate": 1e-05,
      "loss": 6.6287,
      "step": 18412
    },
    {
      "epoch": 0.997940379403794,
      "step": 18412,
      "training_loss": 6.445023536682129
    },
    {
      "epoch": 0.9979945799457994,
      "step": 18413,
      "training_loss": 6.582195281982422
    },
    {
      "epoch": 0.9980487804878049,
      "step": 18414,
      "training_loss": 5.844589710235596
    },
    {
      "epoch": 0.9981029810298103,
      "step": 18415,
      "training_loss": 7.707230567932129
    },
    {
      "epoch": 0.9981571815718158,
      "grad_norm": 25.984251022338867,
      "learning_rate": 1e-05,
      "loss": 6.6448,
      "step": 18416
    },
    {
      "epoch": 0.9981571815718158,
      "step": 18416,
      "training_loss": 6.042418956756592
    },
    {
      "epoch": 0.9982113821138211,
      "step": 18417,
      "training_loss": 7.136342525482178
    },
    {
      "epoch": 0.9982655826558265,
      "step": 18418,
      "training_loss": 8.09211540222168
    },
    {
      "epoch": 0.998319783197832,
      "step": 18419,
      "training_loss": 6.630349159240723
    },
    {
      "epoch": 0.9983739837398374,
      "grad_norm": 24.4525089263916,
      "learning_rate": 1e-05,
      "loss": 6.9753,
      "step": 18420
    },
    {
      "epoch": 0.9983739837398374,
      "step": 18420,
      "training_loss": 6.466827392578125
    },
    {
      "epoch": 0.9984281842818428,
      "step": 18421,
      "training_loss": 6.715820789337158
    },
    {
      "epoch": 0.9984823848238482,
      "step": 18422,
      "training_loss": 5.7952775955200195
    },
    {
      "epoch": 0.9985365853658537,
      "step": 18423,
      "training_loss": 5.500731945037842
    },
    {
      "epoch": 0.9985907859078591,
      "grad_norm": 26.631267547607422,
      "learning_rate": 1e-05,
      "loss": 6.1197,
      "step": 18424
    },
    {
      "epoch": 0.9985907859078591,
      "step": 18424,
      "training_loss": 6.636647701263428
    },
    {
      "epoch": 0.9986449864498645,
      "step": 18425,
      "training_loss": 7.038519382476807
    },
    {
      "epoch": 0.9986991869918699,
      "step": 18426,
      "training_loss": 5.631545543670654
    },
    {
      "epoch": 0.9987533875338753,
      "step": 18427,
      "training_loss": 7.3615546226501465
    },
    {
      "epoch": 0.9988075880758808,
      "grad_norm": 23.83097267150879,
      "learning_rate": 1e-05,
      "loss": 6.6671,
      "step": 18428
    },
    {
      "epoch": 0.9988075880758808,
      "step": 18428,
      "training_loss": 7.860846996307373
    },
    {
      "epoch": 0.9988617886178862,
      "step": 18429,
      "training_loss": 6.728977680206299
    },
    {
      "epoch": 0.9989159891598915,
      "step": 18430,
      "training_loss": 6.427685260772705
    },
    {
      "epoch": 0.998970189701897,
      "step": 18431,
      "training_loss": 6.565779685974121
    },
    {
      "epoch": 0.9990243902439024,
      "grad_norm": 20.1542911529541,
      "learning_rate": 1e-05,
      "loss": 6.8958,
      "step": 18432
    },
    {
      "epoch": 0.9990243902439024,
      "step": 18432,
      "training_loss": 7.148247241973877
    },
    {
      "epoch": 0.9990785907859079,
      "step": 18433,
      "training_loss": 7.4303364753723145
    },
    {
      "epoch": 0.9991327913279133,
      "step": 18434,
      "training_loss": 7.7055230140686035
    },
    {
      "epoch": 0.9991869918699187,
      "step": 18435,
      "training_loss": 5.611475944519043
    },
    {
      "epoch": 0.9992411924119241,
      "grad_norm": 20.98822021484375,
      "learning_rate": 1e-05,
      "loss": 6.9739,
      "step": 18436
    },
    {
      "epoch": 0.9992411924119241,
      "step": 18436,
      "training_loss": 6.109339237213135
    },
    {
      "epoch": 0.9992953929539296,
      "step": 18437,
      "training_loss": 6.030284881591797
    },
    {
      "epoch": 0.999349593495935,
      "step": 18438,
      "training_loss": 3.164600372314453
    },
    {
      "epoch": 0.9994037940379403,
      "step": 18439,
      "training_loss": 6.068990707397461
    },
    {
      "epoch": 0.9994579945799458,
      "grad_norm": 66.48893737792969,
      "learning_rate": 1e-05,
      "loss": 5.3433,
      "step": 18440
    },
    {
      "epoch": 0.9994579945799458,
      "step": 18440,
      "training_loss": 6.335142135620117
    },
    {
      "epoch": 0.9995121951219512,
      "step": 18441,
      "training_loss": 6.089239597320557
    },
    {
      "epoch": 0.9995663956639567,
      "step": 18442,
      "training_loss": 8.372125625610352
    },
    {
      "epoch": 0.9996205962059621,
      "step": 18443,
      "training_loss": 7.04242467880249
    },
    {
      "epoch": 0.9996747967479674,
      "grad_norm": 19.17066764831543,
      "learning_rate": 1e-05,
      "loss": 6.9597,
      "step": 18444
    },
    {
      "epoch": 0.9996747967479674,
      "step": 18444,
      "training_loss": 4.600052833557129
    },
    {
      "epoch": 0.9997289972899729,
      "step": 18445,
      "training_loss": 7.032505035400391
    },
    {
      "epoch": 0.9997831978319783,
      "step": 18446,
      "training_loss": 5.749290466308594
    },
    {
      "epoch": 0.9998373983739838,
      "step": 18447,
      "training_loss": 7.046295642852783
    },
    {
      "epoch": 0.9998915989159891,
      "grad_norm": 31.031736373901367,
      "learning_rate": 1e-05,
      "loss": 6.107,
      "step": 18448
    },
    {
      "epoch": 0.9998915989159891,
      "step": 18448,
      "training_loss": 6.36853551864624
    },
    {
      "epoch": 0.9999457994579946,
      "step": 18449,
      "training_loss": 4.984358787536621
    }
  ],
  "logging_steps": 4,
  "max_steps": 18450,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 6000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 9.35088153804288e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
