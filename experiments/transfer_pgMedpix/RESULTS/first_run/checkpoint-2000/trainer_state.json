{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.43360433604336046,
  "eval_steps": 1000,
  "global_step": 2000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0,
      "step": 0,
      "training_loss": 14.129926681518555
    },
    {
      "epoch": 0,
      "step": 0,
      "training_loss": 10.987876892089844
    },
    {
      "epoch": 0,
      "step": 0,
      "training_loss": 12.557652473449707
    },
    {
      "epoch": 0,
      "step": 0,
      "training_loss": 12.1863431930542
    },
    {
      "epoch": 0.00021680216802168022,
      "step": 1,
      "training_loss": 14.943990707397461
    },
    {
      "epoch": 0.00021680216802168022,
      "step": 1,
      "training_loss": 11.802595138549805
    },
    {
      "epoch": 0.00021680216802168022,
      "step": 1,
      "training_loss": 11.903421401977539
    },
    {
      "epoch": 0.00021680216802168022,
      "step": 1,
      "training_loss": 11.384498596191406
    },
    {
      "epoch": 0.00043360433604336043,
      "step": 2,
      "training_loss": 11.09670352935791
    },
    {
      "epoch": 0.00043360433604336043,
      "step": 2,
      "training_loss": 12.613395690917969
    },
    {
      "epoch": 0.00043360433604336043,
      "step": 2,
      "training_loss": 12.124246597290039
    },
    {
      "epoch": 0.00043360433604336043,
      "step": 2,
      "training_loss": 11.94039249420166
    },
    {
      "epoch": 0.0006504065040650406,
      "step": 3,
      "training_loss": 12.062873840332031
    },
    {
      "epoch": 0.0006504065040650406,
      "step": 3,
      "training_loss": 12.933205604553223
    },
    {
      "epoch": 0.0006504065040650406,
      "step": 3,
      "training_loss": 12.185049057006836
    },
    {
      "epoch": 0.0006504065040650406,
      "step": 3,
      "training_loss": 12.294814109802246
    },
    {
      "epoch": 0.0008672086720867209,
      "grad_norm": NaN,
      "learning_rate": 1e-05,
      "loss": 12.3217,
      "step": 4
    },
    {
      "epoch": 0.0008672086720867209,
      "step": 4,
      "training_loss": 12.577984809875488
    },
    {
      "epoch": 0.0008672086720867209,
      "step": 4,
      "training_loss": 12.268057823181152
    },
    {
      "epoch": 0.0008672086720867209,
      "step": 4,
      "training_loss": 11.713841438293457
    },
    {
      "epoch": 0.0008672086720867209,
      "step": 4,
      "training_loss": 11.191978454589844
    },
    {
      "epoch": 0.001084010840108401,
      "step": 5,
      "training_loss": 12.615577697753906
    },
    {
      "epoch": 0.001084010840108401,
      "step": 5,
      "training_loss": 11.070530891418457
    },
    {
      "epoch": 0.001084010840108401,
      "step": 5,
      "training_loss": 11.047524452209473
    },
    {
      "epoch": 0.001084010840108401,
      "step": 5,
      "training_loss": 10.725958824157715
    },
    {
      "epoch": 0.0013008130081300813,
      "step": 6,
      "training_loss": 13.691665649414062
    },
    {
      "epoch": 0.0013008130081300813,
      "step": 6,
      "training_loss": 11.939536094665527
    },
    {
      "epoch": 0.0013008130081300813,
      "step": 6,
      "training_loss": 10.67147445678711
    },
    {
      "epoch": 0.0013008130081300813,
      "step": 6,
      "training_loss": 12.520310401916504
    },
    {
      "epoch": 0.0015176151761517615,
      "step": 7,
      "training_loss": 12.12801742553711
    },
    {
      "epoch": 0.0015176151761517615,
      "step": 7,
      "training_loss": 12.262654304504395
    },
    {
      "epoch": 0.0015176151761517615,
      "step": 7,
      "training_loss": 12.351304054260254
    },
    {
      "epoch": 0.0015176151761517615,
      "step": 7,
      "training_loss": 11.123879432678223
    },
    {
      "epoch": 0.0017344173441734417,
      "grad_norm": 40.368019104003906,
      "learning_rate": 1e-05,
      "loss": 11.8688,
      "step": 8
    },
    {
      "epoch": 0.0017344173441734417,
      "step": 8,
      "training_loss": 11.890803337097168
    },
    {
      "epoch": 0.0017344173441734417,
      "step": 8,
      "training_loss": 11.987373352050781
    },
    {
      "epoch": 0.0017344173441734417,
      "step": 8,
      "training_loss": 10.584735870361328
    },
    {
      "epoch": 0.0017344173441734417,
      "step": 8,
      "training_loss": 11.384757041931152
    },
    {
      "epoch": 0.001951219512195122,
      "step": 9,
      "training_loss": 12.245636940002441
    },
    {
      "epoch": 0.001951219512195122,
      "step": 9,
      "training_loss": 13.458088874816895
    },
    {
      "epoch": 0.001951219512195122,
      "step": 9,
      "training_loss": 12.077686309814453
    },
    {
      "epoch": 0.001951219512195122,
      "step": 9,
      "training_loss": 20.728862762451172
    },
    {
      "epoch": 0.002168021680216802,
      "step": 10,
      "training_loss": 36.670955657958984
    },
    {
      "epoch": 0.002168021680216802,
      "step": 10,
      "training_loss": 11.576823234558105
    },
    {
      "epoch": 0.002168021680216802,
      "step": 10,
      "training_loss": 9.953940391540527
    },
    {
      "epoch": 0.002168021680216802,
      "step": 10,
      "training_loss": 12.280098915100098
    },
    {
      "epoch": 0.0023848238482384824,
      "step": 11,
      "training_loss": 13.311910629272461
    },
    {
      "epoch": 0.0023848238482384824,
      "step": 11,
      "training_loss": 11.046745300292969
    },
    {
      "epoch": 0.0023848238482384824,
      "step": 11,
      "training_loss": 11.21998405456543
    },
    {
      "epoch": 0.0023848238482384824,
      "step": 11,
      "training_loss": 13.478476524353027
    },
    {
      "epoch": 0.0026016260162601626,
      "grad_norm": 57.13459777832031,
      "learning_rate": 1e-05,
      "loss": 13.9936,
      "step": 12
    },
    {
      "epoch": 0.0026016260162601626,
      "step": 12,
      "training_loss": 11.741504669189453
    },
    {
      "epoch": 0.0026016260162601626,
      "step": 12,
      "training_loss": 11.608926773071289
    },
    {
      "epoch": 0.0026016260162601626,
      "step": 12,
      "training_loss": 12.865472793579102
    },
    {
      "epoch": 0.0026016260162601626,
      "step": 12,
      "training_loss": 11.580991744995117
    },
    {
      "epoch": 0.002818428184281843,
      "step": 13,
      "training_loss": 12.551597595214844
    },
    {
      "epoch": 0.002818428184281843,
      "step": 13,
      "training_loss": 12.194153785705566
    },
    {
      "epoch": 0.002818428184281843,
      "step": 13,
      "training_loss": 12.380539894104004
    },
    {
      "epoch": 0.002818428184281843,
      "step": 13,
      "training_loss": 12.721258163452148
    },
    {
      "epoch": 0.003035230352303523,
      "step": 14,
      "training_loss": 12.327311515808105
    },
    {
      "epoch": 0.003035230352303523,
      "step": 14,
      "training_loss": 14.72461223602295
    },
    {
      "epoch": 0.003035230352303523,
      "step": 14,
      "training_loss": 11.198002815246582
    },
    {
      "epoch": 0.003035230352303523,
      "step": 14,
      "training_loss": 11.78889274597168
    },
    {
      "epoch": 0.0032520325203252032,
      "step": 15,
      "training_loss": 11.589438438415527
    },
    {
      "epoch": 0.0032520325203252032,
      "step": 15,
      "training_loss": 12.854010581970215
    },
    {
      "epoch": 0.0032520325203252032,
      "step": 15,
      "training_loss": 12.525895118713379
    },
    {
      "epoch": 0.0032520325203252032,
      "step": 15,
      "training_loss": 11.336115837097168
    },
    {
      "epoch": 0.0034688346883468835,
      "grad_norm": 124.1433334350586,
      "learning_rate": 1e-05,
      "loss": 12.2493,
      "step": 16
    },
    {
      "epoch": 0.0034688346883468835,
      "step": 16,
      "training_loss": 11.673766136169434
    },
    {
      "epoch": 0.0034688346883468835,
      "step": 16,
      "training_loss": 12.111603736877441
    },
    {
      "epoch": 0.0034688346883468835,
      "step": 16,
      "training_loss": 11.936027526855469
    },
    {
      "epoch": 0.0034688346883468835,
      "step": 16,
      "training_loss": 11.786352157592773
    },
    {
      "epoch": 0.0036856368563685637,
      "step": 17,
      "training_loss": 13.175630569458008
    },
    {
      "epoch": 0.0036856368563685637,
      "step": 17,
      "training_loss": 10.511706352233887
    },
    {
      "epoch": 0.0036856368563685637,
      "step": 17,
      "training_loss": 12.884683609008789
    },
    {
      "epoch": 0.0036856368563685637,
      "step": 17,
      "training_loss": 12.899235725402832
    },
    {
      "epoch": 0.003902439024390244,
      "step": 18,
      "training_loss": 11.828648567199707
    },
    {
      "epoch": 0.003902439024390244,
      "step": 18,
      "training_loss": 12.130088806152344
    },
    {
      "epoch": 0.003902439024390244,
      "step": 18,
      "training_loss": 12.403667449951172
    },
    {
      "epoch": 0.003902439024390244,
      "step": 18,
      "training_loss": 13.090483665466309
    },
    {
      "epoch": 0.004119241192411924,
      "step": 19,
      "training_loss": 10.568077087402344
    },
    {
      "epoch": 0.004119241192411924,
      "step": 19,
      "training_loss": 12.776058197021484
    },
    {
      "epoch": 0.004119241192411924,
      "step": 19,
      "training_loss": 11.860496520996094
    },
    {
      "epoch": 0.004119241192411924,
      "step": 19,
      "training_loss": 13.441543579101562
    },
    {
      "epoch": 0.004336043360433604,
      "grad_norm": 38.48869323730469,
      "learning_rate": 1e-05,
      "loss": 12.1924,
      "step": 20
    },
    {
      "epoch": 0.004336043360433604,
      "step": 20,
      "training_loss": 11.59726619720459
    },
    {
      "epoch": 0.004336043360433604,
      "step": 20,
      "training_loss": 12.38797664642334
    },
    {
      "epoch": 0.004336043360433604,
      "step": 20,
      "training_loss": 11.997509956359863
    },
    {
      "epoch": 0.004336043360433604,
      "step": 20,
      "training_loss": 12.171975135803223
    },
    {
      "epoch": 0.0045528455284552845,
      "step": 21,
      "training_loss": 13.252991676330566
    },
    {
      "epoch": 0.0045528455284552845,
      "step": 21,
      "training_loss": 11.797636985778809
    },
    {
      "epoch": 0.0045528455284552845,
      "step": 21,
      "training_loss": 12.351250648498535
    },
    {
      "epoch": 0.0045528455284552845,
      "step": 21,
      "training_loss": 12.240026473999023
    },
    {
      "epoch": 0.004769647696476965,
      "step": 22,
      "training_loss": 12.585379600524902
    },
    {
      "epoch": 0.004769647696476965,
      "step": 22,
      "training_loss": 19.705284118652344
    },
    {
      "epoch": 0.004769647696476965,
      "step": 22,
      "training_loss": 12.349409103393555
    },
    {
      "epoch": 0.004769647696476965,
      "step": 22,
      "training_loss": 11.908636093139648
    },
    {
      "epoch": 0.004986449864498645,
      "step": 23,
      "training_loss": 24.068218231201172
    },
    {
      "epoch": 0.004986449864498645,
      "step": 23,
      "training_loss": 12.227001190185547
    },
    {
      "epoch": 0.004986449864498645,
      "step": 23,
      "training_loss": 12.088069915771484
    },
    {
      "epoch": 0.004986449864498645,
      "step": 23,
      "training_loss": 13.330714225769043
    },
    {
      "epoch": 0.005203252032520325,
      "grad_norm": 411.0171813964844,
      "learning_rate": 1e-05,
      "loss": 13.5037,
      "step": 24
    },
    {
      "epoch": 0.005203252032520325,
      "step": 24,
      "training_loss": 11.038077354431152
    },
    {
      "epoch": 0.005203252032520325,
      "step": 24,
      "training_loss": 10.62121295928955
    },
    {
      "epoch": 0.005203252032520325,
      "step": 24,
      "training_loss": 12.416570663452148
    },
    {
      "epoch": 0.005203252032520325,
      "step": 24,
      "training_loss": 12.03183364868164
    },
    {
      "epoch": 0.005420054200542005,
      "step": 25,
      "training_loss": 13.300182342529297
    },
    {
      "epoch": 0.005420054200542005,
      "step": 25,
      "training_loss": 12.596121788024902
    },
    {
      "epoch": 0.005420054200542005,
      "step": 25,
      "training_loss": 11.770462036132812
    },
    {
      "epoch": 0.005420054200542005,
      "step": 25,
      "training_loss": 10.030396461486816
    },
    {
      "epoch": 0.005636856368563686,
      "step": 26,
      "training_loss": 11.13257884979248
    },
    {
      "epoch": 0.005636856368563686,
      "step": 26,
      "training_loss": 11.982141494750977
    },
    {
      "epoch": 0.005636856368563686,
      "step": 26,
      "training_loss": 12.298714637756348
    },
    {
      "epoch": 0.005636856368563686,
      "step": 26,
      "training_loss": 12.26333999633789
    },
    {
      "epoch": 0.005853658536585366,
      "step": 27,
      "training_loss": 11.968616485595703
    },
    {
      "epoch": 0.005853658536585366,
      "step": 27,
      "training_loss": 11.122617721557617
    },
    {
      "epoch": 0.005853658536585366,
      "step": 27,
      "training_loss": 12.440252304077148
    },
    {
      "epoch": 0.005853658536585366,
      "step": 27,
      "training_loss": 12.12020492553711
    },
    {
      "epoch": 0.006070460704607046,
      "grad_norm": NaN,
      "learning_rate": 1e-05,
      "loss": 11.8208,
      "step": 28
    },
    {
      "epoch": 0.006070460704607046,
      "step": 28,
      "training_loss": 12.061192512512207
    },
    {
      "epoch": 0.006070460704607046,
      "step": 28,
      "training_loss": 10.90265941619873
    },
    {
      "epoch": 0.006070460704607046,
      "step": 28,
      "training_loss": 12.013381004333496
    },
    {
      "epoch": 0.006070460704607046,
      "step": 28,
      "training_loss": 11.932342529296875
    },
    {
      "epoch": 0.006287262872628726,
      "step": 29,
      "training_loss": 13.995896339416504
    },
    {
      "epoch": 0.006287262872628726,
      "step": 29,
      "training_loss": 13.08926010131836
    },
    {
      "epoch": 0.006287262872628726,
      "step": 29,
      "training_loss": 10.756449699401855
    },
    {
      "epoch": 0.006287262872628726,
      "step": 29,
      "training_loss": 12.076013565063477
    },
    {
      "epoch": 0.0065040650406504065,
      "step": 30,
      "training_loss": 12.211387634277344
    },
    {
      "epoch": 0.0065040650406504065,
      "step": 30,
      "training_loss": 13.422768592834473
    },
    {
      "epoch": 0.0065040650406504065,
      "step": 30,
      "training_loss": 12.660667419433594
    },
    {
      "epoch": 0.0065040650406504065,
      "step": 30,
      "training_loss": 10.969892501831055
    },
    {
      "epoch": 0.006720867208672087,
      "step": 31,
      "training_loss": 12.335991859436035
    },
    {
      "epoch": 0.006720867208672087,
      "step": 31,
      "training_loss": 11.659415245056152
    },
    {
      "epoch": 0.006720867208672087,
      "step": 31,
      "training_loss": 11.375340461730957
    },
    {
      "epoch": 0.006720867208672087,
      "step": 31,
      "training_loss": 11.980965614318848
    },
    {
      "epoch": 0.006937669376693767,
      "grad_norm": 27.553009033203125,
      "learning_rate": 1e-05,
      "loss": 12.0902,
      "step": 32
    },
    {
      "epoch": 0.006937669376693767,
      "step": 32,
      "training_loss": 14.025689125061035
    },
    {
      "epoch": 0.006937669376693767,
      "step": 32,
      "training_loss": 12.932597160339355
    },
    {
      "epoch": 0.006937669376693767,
      "step": 32,
      "training_loss": 10.210565567016602
    },
    {
      "epoch": 0.006937669376693767,
      "step": 32,
      "training_loss": 13.066004753112793
    },
    {
      "epoch": 0.007154471544715447,
      "step": 33,
      "training_loss": 13.513357162475586
    },
    {
      "epoch": 0.007154471544715447,
      "step": 33,
      "training_loss": 13.421650886535645
    },
    {
      "epoch": 0.007154471544715447,
      "step": 33,
      "training_loss": 14.564518928527832
    },
    {
      "epoch": 0.007154471544715447,
      "step": 33,
      "training_loss": 14.454317092895508
    },
    {
      "epoch": 0.007371273712737127,
      "step": 34,
      "training_loss": 12.03699016571045
    },
    {
      "epoch": 0.007371273712737127,
      "step": 34,
      "training_loss": 13.2438383102417
    },
    {
      "epoch": 0.007371273712737127,
      "step": 34,
      "training_loss": 12.322173118591309
    },
    {
      "epoch": 0.007371273712737127,
      "step": 34,
      "training_loss": 10.199790954589844
    },
    {
      "epoch": 0.007588075880758808,
      "step": 35,
      "training_loss": 12.012755393981934
    },
    {
      "epoch": 0.007588075880758808,
      "step": 35,
      "training_loss": 10.50222396850586
    },
    {
      "epoch": 0.007588075880758808,
      "step": 35,
      "training_loss": 12.61445140838623
    },
    {
      "epoch": 0.007588075880758808,
      "step": 35,
      "training_loss": 11.925227165222168
    },
    {
      "epoch": 0.007804878048780488,
      "grad_norm": 32.004520416259766,
      "learning_rate": 1e-05,
      "loss": 12.5654,
      "step": 36
    },
    {
      "epoch": 0.007804878048780488,
      "step": 36,
      "training_loss": 12.147042274475098
    },
    {
      "epoch": 0.007804878048780488,
      "step": 36,
      "training_loss": 11.876177787780762
    },
    {
      "epoch": 0.007804878048780488,
      "step": 36,
      "training_loss": 13.665154457092285
    },
    {
      "epoch": 0.007804878048780488,
      "step": 36,
      "training_loss": 13.972586631774902
    },
    {
      "epoch": 0.008021680216802168,
      "step": 37,
      "training_loss": 12.473161697387695
    },
    {
      "epoch": 0.008021680216802168,
      "step": 37,
      "training_loss": 10.58101749420166
    },
    {
      "epoch": 0.008021680216802168,
      "step": 37,
      "training_loss": 12.095193862915039
    },
    {
      "epoch": 0.008021680216802168,
      "step": 37,
      "training_loss": 9.499665260314941
    },
    {
      "epoch": 0.008238482384823848,
      "step": 38,
      "training_loss": 12.277303695678711
    },
    {
      "epoch": 0.008238482384823848,
      "step": 38,
      "training_loss": 10.623516082763672
    },
    {
      "epoch": 0.008238482384823848,
      "step": 38,
      "training_loss": 12.467352867126465
    },
    {
      "epoch": 0.008238482384823848,
      "step": 38,
      "training_loss": 11.0867280960083
    },
    {
      "epoch": 0.008455284552845528,
      "step": 39,
      "training_loss": 12.746847152709961
    },
    {
      "epoch": 0.008455284552845528,
      "step": 39,
      "training_loss": 13.503342628479004
    },
    {
      "epoch": 0.008455284552845528,
      "step": 39,
      "training_loss": 11.63931655883789
    },
    {
      "epoch": 0.008455284552845528,
      "step": 39,
      "training_loss": 12.60031795501709
    },
    {
      "epoch": 0.008672086720867209,
      "grad_norm": 14.570959091186523,
      "learning_rate": 1e-05,
      "loss": 12.0784,
      "step": 40
    },
    {
      "epoch": 0.008672086720867209,
      "step": 40,
      "training_loss": 14.338016510009766
    },
    {
      "epoch": 0.008672086720867209,
      "step": 40,
      "training_loss": 18.55862808227539
    },
    {
      "epoch": 0.008672086720867209,
      "step": 40,
      "training_loss": 12.906817436218262
    },
    {
      "epoch": 0.008672086720867209,
      "step": 40,
      "training_loss": 12.881967544555664
    },
    {
      "epoch": 0.008888888888888889,
      "step": 41,
      "training_loss": 12.30904483795166
    },
    {
      "epoch": 0.008888888888888889,
      "step": 41,
      "training_loss": 11.220399856567383
    },
    {
      "epoch": 0.008888888888888889,
      "step": 41,
      "training_loss": 10.912490844726562
    },
    {
      "epoch": 0.008888888888888889,
      "step": 41,
      "training_loss": 11.895825386047363
    },
    {
      "epoch": 0.009105691056910569,
      "step": 42,
      "training_loss": 12.333757400512695
    },
    {
      "epoch": 0.009105691056910569,
      "step": 42,
      "training_loss": 11.236323356628418
    },
    {
      "epoch": 0.009105691056910569,
      "step": 42,
      "training_loss": 11.940875053405762
    },
    {
      "epoch": 0.009105691056910569,
      "step": 42,
      "training_loss": 12.968191146850586
    },
    {
      "epoch": 0.00932249322493225,
      "step": 43,
      "training_loss": 11.587560653686523
    },
    {
      "epoch": 0.00932249322493225,
      "step": 43,
      "training_loss": 12.269242286682129
    },
    {
      "epoch": 0.00932249322493225,
      "step": 43,
      "training_loss": 11.196037292480469
    },
    {
      "epoch": 0.00932249322493225,
      "step": 43,
      "training_loss": 11.465539932250977
    },
    {
      "epoch": 0.00953929539295393,
      "grad_norm": 36.722023010253906,
      "learning_rate": 1e-05,
      "loss": 12.5013,
      "step": 44
    },
    {
      "epoch": 0.00953929539295393,
      "step": 44,
      "training_loss": 11.402819633483887
    },
    {
      "epoch": 0.00953929539295393,
      "step": 44,
      "training_loss": 12.936206817626953
    },
    {
      "epoch": 0.00953929539295393,
      "step": 44,
      "training_loss": 12.485295295715332
    },
    {
      "epoch": 0.00953929539295393,
      "step": 44,
      "training_loss": 10.630491256713867
    },
    {
      "epoch": 0.00975609756097561,
      "step": 45,
      "training_loss": 11.937567710876465
    },
    {
      "epoch": 0.00975609756097561,
      "step": 45,
      "training_loss": 11.756980895996094
    },
    {
      "epoch": 0.00975609756097561,
      "step": 45,
      "training_loss": 12.865609169006348
    },
    {
      "epoch": 0.00975609756097561,
      "step": 45,
      "training_loss": 12.892926216125488
    },
    {
      "epoch": 0.00997289972899729,
      "step": 46,
      "training_loss": 12.017492294311523
    },
    {
      "epoch": 0.00997289972899729,
      "step": 46,
      "training_loss": 10.977046012878418
    },
    {
      "epoch": 0.00997289972899729,
      "step": 46,
      "training_loss": 11.284907341003418
    },
    {
      "epoch": 0.00997289972899729,
      "step": 46,
      "training_loss": 10.70380973815918
    },
    {
      "epoch": 0.01018970189701897,
      "step": 47,
      "training_loss": 11.038915634155273
    },
    {
      "epoch": 0.01018970189701897,
      "step": 47,
      "training_loss": 11.353190422058105
    },
    {
      "epoch": 0.01018970189701897,
      "step": 47,
      "training_loss": 11.42149543762207
    },
    {
      "epoch": 0.01018970189701897,
      "step": 47,
      "training_loss": 11.40738582611084
    },
    {
      "epoch": 0.01040650406504065,
      "grad_norm": 78.1668930053711,
      "learning_rate": 1e-05,
      "loss": 11.6945,
      "step": 48
    },
    {
      "epoch": 0.01040650406504065,
      "step": 48,
      "training_loss": 11.541104316711426
    },
    {
      "epoch": 0.01040650406504065,
      "step": 48,
      "training_loss": 13.541463851928711
    },
    {
      "epoch": 0.01040650406504065,
      "step": 48,
      "training_loss": 10.414023399353027
    },
    {
      "epoch": 0.01040650406504065,
      "step": 48,
      "training_loss": 11.290352821350098
    },
    {
      "epoch": 0.01062330623306233,
      "step": 49,
      "training_loss": 11.51103401184082
    },
    {
      "epoch": 0.01062330623306233,
      "step": 49,
      "training_loss": 11.443431854248047
    },
    {
      "epoch": 0.01062330623306233,
      "step": 49,
      "training_loss": 11.95838737487793
    },
    {
      "epoch": 0.01062330623306233,
      "step": 49,
      "training_loss": 9.770739555358887
    },
    {
      "epoch": 0.01084010840108401,
      "step": 50,
      "training_loss": 10.50637435913086
    },
    {
      "epoch": 0.01084010840108401,
      "step": 50,
      "training_loss": 11.624364852905273
    },
    {
      "epoch": 0.01084010840108401,
      "step": 50,
      "training_loss": 12.952534675598145
    },
    {
      "epoch": 0.01084010840108401,
      "step": 50,
      "training_loss": 10.989760398864746
    },
    {
      "epoch": 0.011056910569105691,
      "step": 51,
      "training_loss": 12.793395042419434
    },
    {
      "epoch": 0.011056910569105691,
      "step": 51,
      "training_loss": 11.67139720916748
    },
    {
      "epoch": 0.011056910569105691,
      "step": 51,
      "training_loss": 11.2592191696167
    },
    {
      "epoch": 0.011056910569105691,
      "step": 51,
      "training_loss": 9.760356903076172
    },
    {
      "epoch": 0.011273712737127371,
      "grad_norm": 84.78911590576172,
      "learning_rate": 1e-05,
      "loss": 11.4392,
      "step": 52
    },
    {
      "epoch": 0.011273712737127371,
      "step": 52,
      "training_loss": 12.081472396850586
    },
    {
      "epoch": 0.011273712737127371,
      "step": 52,
      "training_loss": 11.038619995117188
    },
    {
      "epoch": 0.011273712737127371,
      "step": 52,
      "training_loss": 12.72945499420166
    },
    {
      "epoch": 0.011273712737127371,
      "step": 52,
      "training_loss": 11.966651916503906
    },
    {
      "epoch": 0.011490514905149051,
      "step": 53,
      "training_loss": 12.189189910888672
    },
    {
      "epoch": 0.011490514905149051,
      "step": 53,
      "training_loss": 12.904059410095215
    },
    {
      "epoch": 0.011490514905149051,
      "step": 53,
      "training_loss": 11.13443374633789
    },
    {
      "epoch": 0.011490514905149051,
      "step": 53,
      "training_loss": 11.541630744934082
    },
    {
      "epoch": 0.011707317073170732,
      "step": 54,
      "training_loss": 12.216711044311523
    },
    {
      "epoch": 0.011707317073170732,
      "step": 54,
      "training_loss": 12.9473295211792
    },
    {
      "epoch": 0.011707317073170732,
      "step": 54,
      "training_loss": 12.398697853088379
    },
    {
      "epoch": 0.011707317073170732,
      "step": 54,
      "training_loss": 11.98063850402832
    },
    {
      "epoch": 0.011924119241192412,
      "step": 55,
      "training_loss": 12.213766098022461
    },
    {
      "epoch": 0.011924119241192412,
      "step": 55,
      "training_loss": 12.971565246582031
    },
    {
      "epoch": 0.011924119241192412,
      "step": 55,
      "training_loss": 12.111949920654297
    },
    {
      "epoch": 0.011924119241192412,
      "step": 55,
      "training_loss": 13.111401557922363
    },
    {
      "epoch": 0.012140921409214092,
      "grad_norm": 41.4530029296875,
      "learning_rate": 1e-05,
      "loss": 12.2211,
      "step": 56
    },
    {
      "epoch": 0.012140921409214092,
      "step": 56,
      "training_loss": 11.622699737548828
    },
    {
      "epoch": 0.012140921409214092,
      "step": 56,
      "training_loss": 12.156288146972656
    },
    {
      "epoch": 0.012140921409214092,
      "step": 56,
      "training_loss": 11.434711456298828
    },
    {
      "epoch": 0.012140921409214092,
      "step": 56,
      "training_loss": 13.113154411315918
    },
    {
      "epoch": 0.012357723577235772,
      "step": 57,
      "training_loss": 12.211792945861816
    },
    {
      "epoch": 0.012357723577235772,
      "step": 57,
      "training_loss": 11.503585815429688
    },
    {
      "epoch": 0.012357723577235772,
      "step": 57,
      "training_loss": 10.892461776733398
    },
    {
      "epoch": 0.012357723577235772,
      "step": 57,
      "training_loss": 13.641779899597168
    },
    {
      "epoch": 0.012574525745257453,
      "step": 58,
      "training_loss": 12.064449310302734
    },
    {
      "epoch": 0.012574525745257453,
      "step": 58,
      "training_loss": 14.21044635772705
    },
    {
      "epoch": 0.012574525745257453,
      "step": 58,
      "training_loss": 12.425569534301758
    },
    {
      "epoch": 0.012574525745257453,
      "step": 58,
      "training_loss": 11.969542503356934
    },
    {
      "epoch": 0.012791327913279133,
      "step": 59,
      "training_loss": 12.789684295654297
    },
    {
      "epoch": 0.012791327913279133,
      "step": 59,
      "training_loss": 12.793673515319824
    },
    {
      "epoch": 0.012791327913279133,
      "step": 59,
      "training_loss": 11.771599769592285
    },
    {
      "epoch": 0.012791327913279133,
      "step": 59,
      "training_loss": 36.234397888183594
    },
    {
      "epoch": 0.013008130081300813,
      "grad_norm": 1420.3411865234375,
      "learning_rate": 1e-05,
      "loss": 13.8022,
      "step": 60
    },
    {
      "epoch": 0.013008130081300813,
      "step": 60,
      "training_loss": 11.49308967590332
    },
    {
      "epoch": 0.013008130081300813,
      "step": 60,
      "training_loss": 11.771706581115723
    },
    {
      "epoch": 0.013008130081300813,
      "step": 60,
      "training_loss": 10.686327934265137
    },
    {
      "epoch": 0.013008130081300813,
      "step": 60,
      "training_loss": 11.416329383850098
    },
    {
      "epoch": 0.013224932249322493,
      "step": 61,
      "training_loss": 12.271880149841309
    },
    {
      "epoch": 0.013224932249322493,
      "step": 61,
      "training_loss": 11.650886535644531
    },
    {
      "epoch": 0.013224932249322493,
      "step": 61,
      "training_loss": 12.369511604309082
    },
    {
      "epoch": 0.013224932249322493,
      "step": 61,
      "training_loss": 9.913251876831055
    },
    {
      "epoch": 0.013441734417344173,
      "step": 62,
      "training_loss": 12.663847923278809
    },
    {
      "epoch": 0.013441734417344173,
      "step": 62,
      "training_loss": 12.112873077392578
    },
    {
      "epoch": 0.013441734417344173,
      "step": 62,
      "training_loss": 10.092921257019043
    },
    {
      "epoch": 0.013441734417344173,
      "step": 62,
      "training_loss": 11.312943458557129
    },
    {
      "epoch": 0.013658536585365854,
      "step": 63,
      "training_loss": 12.857704162597656
    },
    {
      "epoch": 0.013658536585365854,
      "step": 63,
      "training_loss": 10.574960708618164
    },
    {
      "epoch": 0.013658536585365854,
      "step": 63,
      "training_loss": 12.046663284301758
    },
    {
      "epoch": 0.013658536585365854,
      "step": 63,
      "training_loss": 10.921147346496582
    },
    {
      "epoch": 0.013875338753387534,
      "grad_norm": 75.99407196044922,
      "learning_rate": 1e-05,
      "loss": 11.5098,
      "step": 64
    },
    {
      "epoch": 0.013875338753387534,
      "step": 64,
      "training_loss": 11.450579643249512
    },
    {
      "epoch": 0.013875338753387534,
      "step": 64,
      "training_loss": 11.541406631469727
    },
    {
      "epoch": 0.013875338753387534,
      "step": 64,
      "training_loss": 12.942134857177734
    },
    {
      "epoch": 0.013875338753387534,
      "step": 64,
      "training_loss": 12.004615783691406
    },
    {
      "epoch": 0.014092140921409214,
      "step": 65,
      "training_loss": 12.192880630493164
    },
    {
      "epoch": 0.014092140921409214,
      "step": 65,
      "training_loss": 12.413687705993652
    },
    {
      "epoch": 0.014092140921409214,
      "step": 65,
      "training_loss": 12.061151504516602
    },
    {
      "epoch": 0.014092140921409214,
      "step": 65,
      "training_loss": 12.230608940124512
    },
    {
      "epoch": 0.014308943089430894,
      "step": 66,
      "training_loss": 11.248644828796387
    },
    {
      "epoch": 0.014308943089430894,
      "step": 66,
      "training_loss": 11.5568265914917
    },
    {
      "epoch": 0.014308943089430894,
      "step": 66,
      "training_loss": 11.653212547302246
    },
    {
      "epoch": 0.014308943089430894,
      "step": 66,
      "training_loss": 10.51690673828125
    },
    {
      "epoch": 0.014525745257452575,
      "step": 67,
      "training_loss": 11.898366928100586
    },
    {
      "epoch": 0.014525745257452575,
      "step": 67,
      "training_loss": 11.121514320373535
    },
    {
      "epoch": 0.014525745257452575,
      "step": 67,
      "training_loss": 10.499296188354492
    },
    {
      "epoch": 0.014525745257452575,
      "step": 67,
      "training_loss": 11.558634757995605
    },
    {
      "epoch": 0.014742547425474255,
      "grad_norm": 139.97105407714844,
      "learning_rate": 1e-05,
      "loss": 11.6807,
      "step": 68
    },
    {
      "epoch": 0.014742547425474255,
      "step": 68,
      "training_loss": 9.84030818939209
    },
    {
      "epoch": 0.014742547425474255,
      "step": 68,
      "training_loss": 12.201244354248047
    },
    {
      "epoch": 0.014742547425474255,
      "step": 68,
      "training_loss": 11.820385932922363
    },
    {
      "epoch": 0.014742547425474255,
      "step": 68,
      "training_loss": 11.725385665893555
    },
    {
      "epoch": 0.014959349593495935,
      "step": 69,
      "training_loss": 12.904593467712402
    },
    {
      "epoch": 0.014959349593495935,
      "step": 69,
      "training_loss": 9.284343719482422
    },
    {
      "epoch": 0.014959349593495935,
      "step": 69,
      "training_loss": 10.819629669189453
    },
    {
      "epoch": 0.014959349593495935,
      "step": 69,
      "training_loss": 10.63996696472168
    },
    {
      "epoch": 0.015176151761517615,
      "step": 70,
      "training_loss": 11.77999496459961
    },
    {
      "epoch": 0.015176151761517615,
      "step": 70,
      "training_loss": 10.990220069885254
    },
    {
      "epoch": 0.015176151761517615,
      "step": 70,
      "training_loss": 9.649593353271484
    },
    {
      "epoch": 0.015176151761517615,
      "step": 70,
      "training_loss": 10.993184089660645
    },
    {
      "epoch": 0.015392953929539295,
      "step": 71,
      "training_loss": 12.101469039916992
    },
    {
      "epoch": 0.015392953929539295,
      "step": 71,
      "training_loss": 12.055069923400879
    },
    {
      "epoch": 0.015392953929539295,
      "step": 71,
      "training_loss": 13.806293487548828
    },
    {
      "epoch": 0.015392953929539295,
      "step": 71,
      "training_loss": 11.601532936096191
    },
    {
      "epoch": 0.015609756097560976,
      "grad_norm": 53.66135025024414,
      "learning_rate": 1e-05,
      "loss": 11.3883,
      "step": 72
    },
    {
      "epoch": 0.015609756097560976,
      "step": 72,
      "training_loss": 12.781106948852539
    },
    {
      "epoch": 0.015609756097560976,
      "step": 72,
      "training_loss": 12.000506401062012
    },
    {
      "epoch": 0.015609756097560976,
      "step": 72,
      "training_loss": 11.054526329040527
    },
    {
      "epoch": 0.015609756097560976,
      "step": 72,
      "training_loss": 11.048627853393555
    },
    {
      "epoch": 0.015826558265582658,
      "step": 73,
      "training_loss": 11.667784690856934
    },
    {
      "epoch": 0.015826558265582658,
      "step": 73,
      "training_loss": 12.172250747680664
    },
    {
      "epoch": 0.015826558265582658,
      "step": 73,
      "training_loss": 12.238003730773926
    },
    {
      "epoch": 0.015826558265582658,
      "step": 73,
      "training_loss": 11.078591346740723
    },
    {
      "epoch": 0.016043360433604336,
      "step": 74,
      "training_loss": 11.856114387512207
    },
    {
      "epoch": 0.016043360433604336,
      "step": 74,
      "training_loss": 10.336078643798828
    },
    {
      "epoch": 0.016043360433604336,
      "step": 74,
      "training_loss": 12.70907974243164
    },
    {
      "epoch": 0.016043360433604336,
      "step": 74,
      "training_loss": 11.49555778503418
    },
    {
      "epoch": 0.016260162601626018,
      "step": 75,
      "training_loss": 11.698912620544434
    },
    {
      "epoch": 0.016260162601626018,
      "step": 75,
      "training_loss": 11.984797477722168
    },
    {
      "epoch": 0.016260162601626018,
      "step": 75,
      "training_loss": 11.84056568145752
    },
    {
      "epoch": 0.016260162601626018,
      "step": 75,
      "training_loss": 11.607672691345215
    },
    {
      "epoch": 0.016476964769647696,
      "grad_norm": 14.534847259521484,
      "learning_rate": 1e-05,
      "loss": 11.7231,
      "step": 76
    },
    {
      "epoch": 0.016476964769647696,
      "step": 76,
      "training_loss": 13.105883598327637
    },
    {
      "epoch": 0.016476964769647696,
      "step": 76,
      "training_loss": 11.540433883666992
    },
    {
      "epoch": 0.016476964769647696,
      "step": 76,
      "training_loss": 10.462786674499512
    },
    {
      "epoch": 0.016476964769647696,
      "step": 76,
      "training_loss": 11.852398872375488
    },
    {
      "epoch": 0.01669376693766938,
      "step": 77,
      "training_loss": 13.063292503356934
    },
    {
      "epoch": 0.01669376693766938,
      "step": 77,
      "training_loss": 10.431804656982422
    },
    {
      "epoch": 0.01669376693766938,
      "step": 77,
      "training_loss": 11.545136451721191
    },
    {
      "epoch": 0.01669376693766938,
      "step": 77,
      "training_loss": 12.49875545501709
    },
    {
      "epoch": 0.016910569105691057,
      "step": 78,
      "training_loss": 10.920513153076172
    },
    {
      "epoch": 0.016910569105691057,
      "step": 78,
      "training_loss": 11.873916625976562
    },
    {
      "epoch": 0.016910569105691057,
      "step": 78,
      "training_loss": 11.328883171081543
    },
    {
      "epoch": 0.016910569105691057,
      "step": 78,
      "training_loss": 11.472853660583496
    },
    {
      "epoch": 0.01712737127371274,
      "step": 79,
      "training_loss": 10.716094017028809
    },
    {
      "epoch": 0.01712737127371274,
      "step": 79,
      "training_loss": 9.91590404510498
    },
    {
      "epoch": 0.01712737127371274,
      "step": 79,
      "training_loss": 11.970865249633789
    },
    {
      "epoch": 0.01712737127371274,
      "step": 79,
      "training_loss": 10.922883033752441
    },
    {
      "epoch": 0.017344173441734417,
      "grad_norm": 17.095088958740234,
      "learning_rate": 1e-05,
      "loss": 11.4764,
      "step": 80
    },
    {
      "epoch": 0.017344173441734417,
      "step": 80,
      "training_loss": 12.132744789123535
    },
    {
      "epoch": 0.017344173441734417,
      "step": 80,
      "training_loss": 10.863932609558105
    },
    {
      "epoch": 0.017344173441734417,
      "step": 80,
      "training_loss": 11.483118057250977
    },
    {
      "epoch": 0.017344173441734417,
      "step": 80,
      "training_loss": 12.577951431274414
    },
    {
      "epoch": 0.0175609756097561,
      "step": 81,
      "training_loss": 11.609021186828613
    },
    {
      "epoch": 0.0175609756097561,
      "step": 81,
      "training_loss": 10.501195907592773
    },
    {
      "epoch": 0.0175609756097561,
      "step": 81,
      "training_loss": 12.02330493927002
    },
    {
      "epoch": 0.0175609756097561,
      "step": 81,
      "training_loss": 11.25774097442627
    },
    {
      "epoch": 0.017777777777777778,
      "step": 82,
      "training_loss": 12.602705955505371
    },
    {
      "epoch": 0.017777777777777778,
      "step": 82,
      "training_loss": 12.730243682861328
    },
    {
      "epoch": 0.017777777777777778,
      "step": 82,
      "training_loss": 10.242645263671875
    },
    {
      "epoch": 0.017777777777777778,
      "step": 82,
      "training_loss": 10.385655403137207
    },
    {
      "epoch": 0.01799457994579946,
      "step": 83,
      "training_loss": 11.50455379486084
    },
    {
      "epoch": 0.01799457994579946,
      "step": 83,
      "training_loss": 11.97238826751709
    },
    {
      "epoch": 0.01799457994579946,
      "step": 83,
      "training_loss": 12.154504776000977
    },
    {
      "epoch": 0.01799457994579946,
      "step": 83,
      "training_loss": 10.986393928527832
    },
    {
      "epoch": 0.018211382113821138,
      "grad_norm": 33.50981521606445,
      "learning_rate": 1e-05,
      "loss": 11.5643,
      "step": 84
    },
    {
      "epoch": 0.018211382113821138,
      "step": 84,
      "training_loss": 12.280348777770996
    },
    {
      "epoch": 0.018211382113821138,
      "step": 84,
      "training_loss": 11.74482536315918
    },
    {
      "epoch": 0.018211382113821138,
      "step": 84,
      "training_loss": 11.623929023742676
    },
    {
      "epoch": 0.018211382113821138,
      "step": 84,
      "training_loss": 11.238550186157227
    },
    {
      "epoch": 0.01842818428184282,
      "step": 85,
      "training_loss": 11.81783676147461
    },
    {
      "epoch": 0.01842818428184282,
      "step": 85,
      "training_loss": 10.899602890014648
    },
    {
      "epoch": 0.01842818428184282,
      "step": 85,
      "training_loss": 10.141265869140625
    },
    {
      "epoch": 0.01842818428184282,
      "step": 85,
      "training_loss": 12.631092071533203
    },
    {
      "epoch": 0.0186449864498645,
      "step": 86,
      "training_loss": 10.414007186889648
    },
    {
      "epoch": 0.0186449864498645,
      "step": 86,
      "training_loss": 11.661026000976562
    },
    {
      "epoch": 0.0186449864498645,
      "step": 86,
      "training_loss": 10.221563339233398
    },
    {
      "epoch": 0.0186449864498645,
      "step": 86,
      "training_loss": 10.34593677520752
    },
    {
      "epoch": 0.01886178861788618,
      "step": 87,
      "training_loss": 11.143449783325195
    },
    {
      "epoch": 0.01886178861788618,
      "step": 87,
      "training_loss": 11.69706916809082
    },
    {
      "epoch": 0.01886178861788618,
      "step": 87,
      "training_loss": 9.322813987731934
    },
    {
      "epoch": 0.01886178861788618,
      "step": 87,
      "training_loss": 12.576661109924316
    },
    {
      "epoch": 0.01907859078590786,
      "grad_norm": 130.18362426757812,
      "learning_rate": 1e-05,
      "loss": 11.235,
      "step": 88
    },
    {
      "epoch": 0.01907859078590786,
      "step": 88,
      "training_loss": 11.708223342895508
    },
    {
      "epoch": 0.01907859078590786,
      "step": 88,
      "training_loss": 12.316985130310059
    },
    {
      "epoch": 0.01907859078590786,
      "step": 88,
      "training_loss": 10.009742736816406
    },
    {
      "epoch": 0.01907859078590786,
      "step": 88,
      "training_loss": 12.043660163879395
    },
    {
      "epoch": 0.01929539295392954,
      "step": 89,
      "training_loss": 12.527496337890625
    },
    {
      "epoch": 0.01929539295392954,
      "step": 89,
      "training_loss": 11.610227584838867
    },
    {
      "epoch": 0.01929539295392954,
      "step": 89,
      "training_loss": 12.932027816772461
    },
    {
      "epoch": 0.01929539295392954,
      "step": 89,
      "training_loss": 10.844712257385254
    },
    {
      "epoch": 0.01951219512195122,
      "step": 90,
      "training_loss": 11.860650062561035
    },
    {
      "epoch": 0.01951219512195122,
      "step": 90,
      "training_loss": 10.88185977935791
    },
    {
      "epoch": 0.01951219512195122,
      "step": 90,
      "training_loss": 11.904728889465332
    },
    {
      "epoch": 0.01951219512195122,
      "step": 90,
      "training_loss": 12.208636283874512
    },
    {
      "epoch": 0.0197289972899729,
      "step": 91,
      "training_loss": 11.958331108093262
    },
    {
      "epoch": 0.0197289972899729,
      "step": 91,
      "training_loss": 9.263877868652344
    },
    {
      "epoch": 0.0197289972899729,
      "step": 91,
      "training_loss": 11.752960205078125
    },
    {
      "epoch": 0.0197289972899729,
      "step": 91,
      "training_loss": 8.998052597045898
    },
    {
      "epoch": 0.01994579945799458,
      "grad_norm": 22.492517471313477,
      "learning_rate": 1e-05,
      "loss": 11.4264,
      "step": 92
    },
    {
      "epoch": 0.01994579945799458,
      "step": 92,
      "training_loss": 11.136180877685547
    },
    {
      "epoch": 0.01994579945799458,
      "step": 92,
      "training_loss": 10.843345642089844
    },
    {
      "epoch": 0.01994579945799458,
      "step": 92,
      "training_loss": 11.757469177246094
    },
    {
      "epoch": 0.01994579945799458,
      "step": 92,
      "training_loss": 12.19736099243164
    },
    {
      "epoch": 0.020162601626016262,
      "step": 93,
      "training_loss": 10.040278434753418
    },
    {
      "epoch": 0.020162601626016262,
      "step": 93,
      "training_loss": 10.467357635498047
    },
    {
      "epoch": 0.020162601626016262,
      "step": 93,
      "training_loss": 11.77796745300293
    },
    {
      "epoch": 0.020162601626016262,
      "step": 93,
      "training_loss": 10.463929176330566
    },
    {
      "epoch": 0.02037940379403794,
      "step": 94,
      "training_loss": 10.93805980682373
    },
    {
      "epoch": 0.02037940379403794,
      "step": 94,
      "training_loss": 11.678519248962402
    },
    {
      "epoch": 0.02037940379403794,
      "step": 94,
      "training_loss": 11.139307975769043
    },
    {
      "epoch": 0.02037940379403794,
      "step": 94,
      "training_loss": 10.885810852050781
    },
    {
      "epoch": 0.020596205962059622,
      "step": 95,
      "training_loss": 11.857054710388184
    },
    {
      "epoch": 0.020596205962059622,
      "step": 95,
      "training_loss": 8.778817176818848
    },
    {
      "epoch": 0.020596205962059622,
      "step": 95,
      "training_loss": 11.219146728515625
    },
    {
      "epoch": 0.020596205962059622,
      "step": 95,
      "training_loss": 11.679966926574707
    },
    {
      "epoch": 0.0208130081300813,
      "grad_norm": 63.60947799682617,
      "learning_rate": 1e-05,
      "loss": 11.0538,
      "step": 96
    },
    {
      "epoch": 0.0208130081300813,
      "step": 96,
      "training_loss": 10.916865348815918
    },
    {
      "epoch": 0.0208130081300813,
      "step": 96,
      "training_loss": 11.880400657653809
    },
    {
      "epoch": 0.0208130081300813,
      "step": 96,
      "training_loss": 9.864355087280273
    },
    {
      "epoch": 0.0208130081300813,
      "step": 96,
      "training_loss": 10.800646781921387
    },
    {
      "epoch": 0.021029810298102983,
      "step": 97,
      "training_loss": 10.504168510437012
    },
    {
      "epoch": 0.021029810298102983,
      "step": 97,
      "training_loss": 11.277281761169434
    },
    {
      "epoch": 0.021029810298102983,
      "step": 97,
      "training_loss": 11.678836822509766
    },
    {
      "epoch": 0.021029810298102983,
      "step": 97,
      "training_loss": 11.577343940734863
    },
    {
      "epoch": 0.02124661246612466,
      "step": 98,
      "training_loss": 11.622729301452637
    },
    {
      "epoch": 0.02124661246612466,
      "step": 98,
      "training_loss": 8.916601181030273
    },
    {
      "epoch": 0.02124661246612466,
      "step": 98,
      "training_loss": 11.45644760131836
    },
    {
      "epoch": 0.02124661246612466,
      "step": 98,
      "training_loss": 10.906023979187012
    },
    {
      "epoch": 0.021463414634146343,
      "step": 99,
      "training_loss": 12.007176399230957
    },
    {
      "epoch": 0.021463414634146343,
      "step": 99,
      "training_loss": 11.46556568145752
    },
    {
      "epoch": 0.021463414634146343,
      "step": 99,
      "training_loss": 10.766195297241211
    },
    {
      "epoch": 0.021463414634146343,
      "step": 99,
      "training_loss": 11.40876579284668
    },
    {
      "epoch": 0.02168021680216802,
      "grad_norm": 28.538280487060547,
      "learning_rate": 1e-05,
      "loss": 11.0656,
      "step": 100
    },
    {
      "epoch": 0.02168021680216802,
      "step": 100,
      "training_loss": 10.849106788635254
    },
    {
      "epoch": 0.02168021680216802,
      "step": 100,
      "training_loss": 11.25267219543457
    },
    {
      "epoch": 0.02168021680216802,
      "step": 100,
      "training_loss": 10.875493049621582
    },
    {
      "epoch": 0.02168021680216802,
      "step": 100,
      "training_loss": 10.81655216217041
    },
    {
      "epoch": 0.021897018970189704,
      "step": 101,
      "training_loss": 12.866523742675781
    },
    {
      "epoch": 0.021897018970189704,
      "step": 101,
      "training_loss": 9.47890853881836
    },
    {
      "epoch": 0.021897018970189704,
      "step": 101,
      "training_loss": 9.623369216918945
    },
    {
      "epoch": 0.021897018970189704,
      "step": 101,
      "training_loss": 9.938318252563477
    },
    {
      "epoch": 0.022113821138211382,
      "step": 102,
      "training_loss": 8.597893714904785
    },
    {
      "epoch": 0.022113821138211382,
      "step": 102,
      "training_loss": 10.821208000183105
    },
    {
      "epoch": 0.022113821138211382,
      "step": 102,
      "training_loss": 11.524106979370117
    },
    {
      "epoch": 0.022113821138211382,
      "step": 102,
      "training_loss": 10.418317794799805
    },
    {
      "epoch": 0.022330623306233064,
      "step": 103,
      "training_loss": 11.360268592834473
    },
    {
      "epoch": 0.022330623306233064,
      "step": 103,
      "training_loss": 11.503006935119629
    },
    {
      "epoch": 0.022330623306233064,
      "step": 103,
      "training_loss": 10.96911334991455
    },
    {
      "epoch": 0.022330623306233064,
      "step": 103,
      "training_loss": 10.988500595092773
    },
    {
      "epoch": 0.022547425474254743,
      "grad_norm": 41.238765716552734,
      "learning_rate": 1e-05,
      "loss": 10.7427,
      "step": 104
    },
    {
      "epoch": 0.022547425474254743,
      "step": 104,
      "training_loss": 10.327693939208984
    },
    {
      "epoch": 0.022547425474254743,
      "step": 104,
      "training_loss": 10.58430004119873
    },
    {
      "epoch": 0.022547425474254743,
      "step": 104,
      "training_loss": 10.050786018371582
    },
    {
      "epoch": 0.022547425474254743,
      "step": 104,
      "training_loss": 9.802963256835938
    },
    {
      "epoch": 0.022764227642276424,
      "step": 105,
      "training_loss": 11.016083717346191
    },
    {
      "epoch": 0.022764227642276424,
      "step": 105,
      "training_loss": 11.338485717773438
    },
    {
      "epoch": 0.022764227642276424,
      "step": 105,
      "training_loss": 10.731383323669434
    },
    {
      "epoch": 0.022764227642276424,
      "step": 105,
      "training_loss": 10.093250274658203
    },
    {
      "epoch": 0.022981029810298103,
      "step": 106,
      "training_loss": 12.697294235229492
    },
    {
      "epoch": 0.022981029810298103,
      "step": 106,
      "training_loss": 10.235753059387207
    },
    {
      "epoch": 0.022981029810298103,
      "step": 106,
      "training_loss": 11.49268627166748
    },
    {
      "epoch": 0.022981029810298103,
      "step": 106,
      "training_loss": 8.621170997619629
    },
    {
      "epoch": 0.023197831978319785,
      "step": 107,
      "training_loss": 10.976311683654785
    },
    {
      "epoch": 0.023197831978319785,
      "step": 107,
      "training_loss": 11.477538108825684
    },
    {
      "epoch": 0.023197831978319785,
      "step": 107,
      "training_loss": 10.320906639099121
    },
    {
      "epoch": 0.023197831978319785,
      "step": 107,
      "training_loss": 9.944619178771973
    },
    {
      "epoch": 0.023414634146341463,
      "grad_norm": 18.820316314697266,
      "learning_rate": 1e-05,
      "loss": 10.607,
      "step": 108
    },
    {
      "epoch": 0.023414634146341463,
      "step": 108,
      "training_loss": 10.80138874053955
    },
    {
      "epoch": 0.023414634146341463,
      "step": 108,
      "training_loss": 10.661816596984863
    },
    {
      "epoch": 0.023414634146341463,
      "step": 108,
      "training_loss": 11.557169914245605
    },
    {
      "epoch": 0.023414634146341463,
      "step": 108,
      "training_loss": 11.075803756713867
    },
    {
      "epoch": 0.023631436314363145,
      "step": 109,
      "training_loss": 11.290813446044922
    },
    {
      "epoch": 0.023631436314363145,
      "step": 109,
      "training_loss": 10.858915328979492
    },
    {
      "epoch": 0.023631436314363145,
      "step": 109,
      "training_loss": 10.129401206970215
    },
    {
      "epoch": 0.023631436314363145,
      "step": 109,
      "training_loss": 9.323092460632324
    },
    {
      "epoch": 0.023848238482384824,
      "step": 110,
      "training_loss": 10.842066764831543
    },
    {
      "epoch": 0.023848238482384824,
      "step": 110,
      "training_loss": 12.861568450927734
    },
    {
      "epoch": 0.023848238482384824,
      "step": 110,
      "training_loss": 10.413304328918457
    },
    {
      "epoch": 0.023848238482384824,
      "step": 110,
      "training_loss": 11.340656280517578
    },
    {
      "epoch": 0.024065040650406506,
      "step": 111,
      "training_loss": 11.190388679504395
    },
    {
      "epoch": 0.024065040650406506,
      "step": 111,
      "training_loss": 11.424149513244629
    },
    {
      "epoch": 0.024065040650406506,
      "step": 111,
      "training_loss": 11.568138122558594
    },
    {
      "epoch": 0.024065040650406506,
      "step": 111,
      "training_loss": 11.215180397033691
    },
    {
      "epoch": 0.024281842818428184,
      "grad_norm": 43.66325759887695,
      "learning_rate": 1e-05,
      "loss": 11.0346,
      "step": 112
    },
    {
      "epoch": 0.024281842818428184,
      "step": 112,
      "training_loss": 11.245038032531738
    },
    {
      "epoch": 0.024281842818428184,
      "step": 112,
      "training_loss": 11.173795700073242
    },
    {
      "epoch": 0.024281842818428184,
      "step": 112,
      "training_loss": 9.451376914978027
    },
    {
      "epoch": 0.024281842818428184,
      "step": 112,
      "training_loss": 12.407135009765625
    },
    {
      "epoch": 0.024498644986449866,
      "step": 113,
      "training_loss": 11.287910461425781
    },
    {
      "epoch": 0.024498644986449866,
      "step": 113,
      "training_loss": 10.341022491455078
    },
    {
      "epoch": 0.024498644986449866,
      "step": 113,
      "training_loss": 9.15455150604248
    },
    {
      "epoch": 0.024498644986449866,
      "step": 113,
      "training_loss": 10.13912296295166
    },
    {
      "epoch": 0.024715447154471545,
      "step": 114,
      "training_loss": 10.365714073181152
    },
    {
      "epoch": 0.024715447154471545,
      "step": 114,
      "training_loss": 10.81946849822998
    },
    {
      "epoch": 0.024715447154471545,
      "step": 114,
      "training_loss": 10.130449295043945
    },
    {
      "epoch": 0.024715447154471545,
      "step": 114,
      "training_loss": 10.584336280822754
    },
    {
      "epoch": 0.024932249322493227,
      "step": 115,
      "training_loss": 10.04595947265625
    },
    {
      "epoch": 0.024932249322493227,
      "step": 115,
      "training_loss": 12.092448234558105
    },
    {
      "epoch": 0.024932249322493227,
      "step": 115,
      "training_loss": 10.615715980529785
    },
    {
      "epoch": 0.024932249322493227,
      "step": 115,
      "training_loss": 10.749505996704102
    },
    {
      "epoch": 0.025149051490514905,
      "grad_norm": 20.692522048950195,
      "learning_rate": 1e-05,
      "loss": 10.6627,
      "step": 116
    },
    {
      "epoch": 0.025149051490514905,
      "step": 116,
      "training_loss": 10.940417289733887
    },
    {
      "epoch": 0.025149051490514905,
      "step": 116,
      "training_loss": 10.547669410705566
    },
    {
      "epoch": 0.025149051490514905,
      "step": 116,
      "training_loss": 11.975298881530762
    },
    {
      "epoch": 0.025149051490514905,
      "step": 116,
      "training_loss": 11.289423942565918
    },
    {
      "epoch": 0.025365853658536587,
      "step": 117,
      "training_loss": 10.086949348449707
    },
    {
      "epoch": 0.025365853658536587,
      "step": 117,
      "training_loss": 9.290812492370605
    },
    {
      "epoch": 0.025365853658536587,
      "step": 117,
      "training_loss": 10.357450485229492
    },
    {
      "epoch": 0.025365853658536587,
      "step": 117,
      "training_loss": 10.817875862121582
    },
    {
      "epoch": 0.025582655826558266,
      "step": 118,
      "training_loss": 8.44282341003418
    },
    {
      "epoch": 0.025582655826558266,
      "step": 118,
      "training_loss": 10.180169105529785
    },
    {
      "epoch": 0.025582655826558266,
      "step": 118,
      "training_loss": 11.879542350769043
    },
    {
      "epoch": 0.025582655826558266,
      "step": 118,
      "training_loss": 11.476774215698242
    },
    {
      "epoch": 0.025799457994579948,
      "step": 119,
      "training_loss": 10.876595497131348
    },
    {
      "epoch": 0.025799457994579948,
      "step": 119,
      "training_loss": 11.399162292480469
    },
    {
      "epoch": 0.025799457994579948,
      "step": 119,
      "training_loss": 11.497428894042969
    },
    {
      "epoch": 0.025799457994579948,
      "step": 119,
      "training_loss": 10.160965919494629
    },
    {
      "epoch": 0.026016260162601626,
      "grad_norm": 55.83879852294922,
      "learning_rate": 1e-05,
      "loss": 10.7012,
      "step": 120
    },
    {
      "epoch": 0.026016260162601626,
      "step": 120,
      "training_loss": 10.489282608032227
    },
    {
      "epoch": 0.026016260162601626,
      "step": 120,
      "training_loss": 11.236295700073242
    },
    {
      "epoch": 0.026016260162601626,
      "step": 120,
      "training_loss": 9.995172500610352
    },
    {
      "epoch": 0.026016260162601626,
      "step": 120,
      "training_loss": 9.580608367919922
    },
    {
      "epoch": 0.026233062330623308,
      "step": 121,
      "training_loss": 12.118667602539062
    },
    {
      "epoch": 0.026233062330623308,
      "step": 121,
      "training_loss": 10.24130630493164
    },
    {
      "epoch": 0.026233062330623308,
      "step": 121,
      "training_loss": 10.9619779586792
    },
    {
      "epoch": 0.026233062330623308,
      "step": 121,
      "training_loss": 12.208261489868164
    },
    {
      "epoch": 0.026449864498644986,
      "step": 122,
      "training_loss": 10.963373184204102
    },
    {
      "epoch": 0.026449864498644986,
      "step": 122,
      "training_loss": 11.035115242004395
    },
    {
      "epoch": 0.026449864498644986,
      "step": 122,
      "training_loss": 11.566999435424805
    },
    {
      "epoch": 0.026449864498644986,
      "step": 122,
      "training_loss": 11.98878288269043
    },
    {
      "epoch": 0.02666666666666667,
      "step": 123,
      "training_loss": 10.727729797363281
    },
    {
      "epoch": 0.02666666666666667,
      "step": 123,
      "training_loss": 10.703031539916992
    },
    {
      "epoch": 0.02666666666666667,
      "step": 123,
      "training_loss": 11.225591659545898
    },
    {
      "epoch": 0.02666666666666667,
      "step": 123,
      "training_loss": 9.971311569213867
    },
    {
      "epoch": 0.026883468834688347,
      "grad_norm": 36.33486557006836,
      "learning_rate": 1e-05,
      "loss": 10.9383,
      "step": 124
    },
    {
      "epoch": 0.026883468834688347,
      "step": 124,
      "training_loss": 10.620291709899902
    },
    {
      "epoch": 0.026883468834688347,
      "step": 124,
      "training_loss": 9.465699195861816
    },
    {
      "epoch": 0.026883468834688347,
      "step": 124,
      "training_loss": 9.768173217773438
    },
    {
      "epoch": 0.026883468834688347,
      "step": 124,
      "training_loss": 10.609957695007324
    },
    {
      "epoch": 0.02710027100271003,
      "step": 125,
      "training_loss": 9.368616104125977
    },
    {
      "epoch": 0.02710027100271003,
      "step": 125,
      "training_loss": 10.51439380645752
    },
    {
      "epoch": 0.02710027100271003,
      "step": 125,
      "training_loss": 11.479109764099121
    },
    {
      "epoch": 0.02710027100271003,
      "step": 125,
      "training_loss": 11.775002479553223
    },
    {
      "epoch": 0.027317073170731707,
      "step": 126,
      "training_loss": 10.5986909866333
    },
    {
      "epoch": 0.027317073170731707,
      "step": 126,
      "training_loss": 9.40007209777832
    },
    {
      "epoch": 0.027317073170731707,
      "step": 126,
      "training_loss": 11.024740219116211
    },
    {
      "epoch": 0.027317073170731707,
      "step": 126,
      "training_loss": 10.911218643188477
    },
    {
      "epoch": 0.02753387533875339,
      "step": 127,
      "training_loss": 10.561436653137207
    },
    {
      "epoch": 0.02753387533875339,
      "step": 127,
      "training_loss": 8.797490119934082
    },
    {
      "epoch": 0.02753387533875339,
      "step": 127,
      "training_loss": 10.928735733032227
    },
    {
      "epoch": 0.02753387533875339,
      "step": 127,
      "training_loss": 11.29634952545166
    },
    {
      "epoch": 0.027750677506775068,
      "grad_norm": 66.634033203125,
      "learning_rate": 1e-05,
      "loss": 10.445,
      "step": 128
    },
    {
      "epoch": 0.027750677506775068,
      "step": 128,
      "training_loss": 10.015891075134277
    },
    {
      "epoch": 0.027750677506775068,
      "step": 128,
      "training_loss": 8.866271018981934
    },
    {
      "epoch": 0.027750677506775068,
      "step": 128,
      "training_loss": 10.25137710571289
    },
    {
      "epoch": 0.027750677506775068,
      "step": 128,
      "training_loss": 11.149786949157715
    },
    {
      "epoch": 0.02796747967479675,
      "step": 129,
      "training_loss": 10.193763732910156
    },
    {
      "epoch": 0.02796747967479675,
      "step": 129,
      "training_loss": 9.491982460021973
    },
    {
      "epoch": 0.02796747967479675,
      "step": 129,
      "training_loss": 10.5200777053833
    },
    {
      "epoch": 0.02796747967479675,
      "step": 129,
      "training_loss": 11.410456657409668
    },
    {
      "epoch": 0.028184281842818428,
      "step": 130,
      "training_loss": 9.918971061706543
    },
    {
      "epoch": 0.028184281842818428,
      "step": 130,
      "training_loss": 9.610514640808105
    },
    {
      "epoch": 0.028184281842818428,
      "step": 130,
      "training_loss": 9.881634712219238
    },
    {
      "epoch": 0.028184281842818428,
      "step": 130,
      "training_loss": 11.019242286682129
    },
    {
      "epoch": 0.02840108401084011,
      "step": 131,
      "training_loss": 11.056554794311523
    },
    {
      "epoch": 0.02840108401084011,
      "step": 131,
      "training_loss": 11.070612907409668
    },
    {
      "epoch": 0.02840108401084011,
      "step": 131,
      "training_loss": 12.080760955810547
    },
    {
      "epoch": 0.02840108401084011,
      "step": 131,
      "training_loss": 11.679936408996582
    },
    {
      "epoch": 0.02861788617886179,
      "grad_norm": 19.20637321472168,
      "learning_rate": 1e-05,
      "loss": 10.5136,
      "step": 132
    },
    {
      "epoch": 0.02861788617886179,
      "step": 132,
      "training_loss": 9.217036247253418
    },
    {
      "epoch": 0.02861788617886179,
      "step": 132,
      "training_loss": 10.997283935546875
    },
    {
      "epoch": 0.02861788617886179,
      "step": 132,
      "training_loss": 10.526914596557617
    },
    {
      "epoch": 0.02861788617886179,
      "step": 132,
      "training_loss": 10.38538932800293
    },
    {
      "epoch": 0.02883468834688347,
      "step": 133,
      "training_loss": 11.136190414428711
    },
    {
      "epoch": 0.02883468834688347,
      "step": 133,
      "training_loss": 10.831467628479004
    },
    {
      "epoch": 0.02883468834688347,
      "step": 133,
      "training_loss": 11.011555671691895
    },
    {
      "epoch": 0.02883468834688347,
      "step": 133,
      "training_loss": 9.72938060760498
    },
    {
      "epoch": 0.02905149051490515,
      "step": 134,
      "training_loss": 9.910465240478516
    },
    {
      "epoch": 0.02905149051490515,
      "step": 134,
      "training_loss": 10.075799942016602
    },
    {
      "epoch": 0.02905149051490515,
      "step": 134,
      "training_loss": 11.616181373596191
    },
    {
      "epoch": 0.02905149051490515,
      "step": 134,
      "training_loss": 11.327160835266113
    },
    {
      "epoch": 0.02926829268292683,
      "step": 135,
      "training_loss": 11.247900009155273
    },
    {
      "epoch": 0.02926829268292683,
      "step": 135,
      "training_loss": 10.320304870605469
    },
    {
      "epoch": 0.02926829268292683,
      "step": 135,
      "training_loss": 10.27843189239502
    },
    {
      "epoch": 0.02926829268292683,
      "step": 135,
      "training_loss": 11.556682586669922
    },
    {
      "epoch": 0.02948509485094851,
      "grad_norm": 21.617870330810547,
      "learning_rate": 1e-05,
      "loss": 10.6355,
      "step": 136
    },
    {
      "epoch": 0.02948509485094851,
      "step": 136,
      "training_loss": 9.068586349487305
    },
    {
      "epoch": 0.02948509485094851,
      "step": 136,
      "training_loss": 10.574161529541016
    },
    {
      "epoch": 0.02948509485094851,
      "step": 136,
      "training_loss": 11.008432388305664
    },
    {
      "epoch": 0.02948509485094851,
      "step": 136,
      "training_loss": 9.473938941955566
    },
    {
      "epoch": 0.02970189701897019,
      "step": 137,
      "training_loss": 9.296638488769531
    },
    {
      "epoch": 0.02970189701897019,
      "step": 137,
      "training_loss": 11.094266891479492
    },
    {
      "epoch": 0.02970189701897019,
      "step": 137,
      "training_loss": 11.563307762145996
    },
    {
      "epoch": 0.02970189701897019,
      "step": 137,
      "training_loss": 10.037761688232422
    },
    {
      "epoch": 0.02991869918699187,
      "step": 138,
      "training_loss": 10.979998588562012
    },
    {
      "epoch": 0.02991869918699187,
      "step": 138,
      "training_loss": 10.117148399353027
    },
    {
      "epoch": 0.02991869918699187,
      "step": 138,
      "training_loss": 10.430035591125488
    },
    {
      "epoch": 0.02991869918699187,
      "step": 138,
      "training_loss": 9.893535614013672
    },
    {
      "epoch": 0.030135501355013552,
      "step": 139,
      "training_loss": 9.175020217895508
    },
    {
      "epoch": 0.030135501355013552,
      "step": 139,
      "training_loss": 9.050354957580566
    },
    {
      "epoch": 0.030135501355013552,
      "step": 139,
      "training_loss": 11.28423023223877
    },
    {
      "epoch": 0.030135501355013552,
      "step": 139,
      "training_loss": 8.82651138305664
    },
    {
      "epoch": 0.03035230352303523,
      "grad_norm": 105.61105346679688,
      "learning_rate": 1e-05,
      "loss": 10.1171,
      "step": 140
    },
    {
      "epoch": 0.03035230352303523,
      "step": 140,
      "training_loss": 10.816479682922363
    },
    {
      "epoch": 0.03035230352303523,
      "step": 140,
      "training_loss": 9.861920356750488
    },
    {
      "epoch": 0.03035230352303523,
      "step": 140,
      "training_loss": 9.993489265441895
    },
    {
      "epoch": 0.03035230352303523,
      "step": 140,
      "training_loss": 9.455035209655762
    },
    {
      "epoch": 0.030569105691056912,
      "step": 141,
      "training_loss": 9.409661293029785
    },
    {
      "epoch": 0.030569105691056912,
      "step": 141,
      "training_loss": 10.60765266418457
    },
    {
      "epoch": 0.030569105691056912,
      "step": 141,
      "training_loss": 9.025524139404297
    },
    {
      "epoch": 0.030569105691056912,
      "step": 141,
      "training_loss": 11.302472114562988
    },
    {
      "epoch": 0.03078590785907859,
      "step": 142,
      "training_loss": 10.13127613067627
    },
    {
      "epoch": 0.03078590785907859,
      "step": 142,
      "training_loss": 11.79173469543457
    },
    {
      "epoch": 0.03078590785907859,
      "step": 142,
      "training_loss": 9.625326156616211
    },
    {
      "epoch": 0.03078590785907859,
      "step": 142,
      "training_loss": 9.825491905212402
    },
    {
      "epoch": 0.031002710027100273,
      "step": 143,
      "training_loss": 11.007796287536621
    },
    {
      "epoch": 0.031002710027100273,
      "step": 143,
      "training_loss": 8.808599472045898
    },
    {
      "epoch": 0.031002710027100273,
      "step": 143,
      "training_loss": 10.434618949890137
    },
    {
      "epoch": 0.031002710027100273,
      "step": 143,
      "training_loss": 9.07772445678711
    },
    {
      "epoch": 0.03121951219512195,
      "grad_norm": 18.18276023864746,
      "learning_rate": 1e-05,
      "loss": 10.0734,
      "step": 144
    },
    {
      "epoch": 0.03121951219512195,
      "step": 144,
      "training_loss": 11.359310150146484
    },
    {
      "epoch": 0.03121951219512195,
      "step": 144,
      "training_loss": 9.533360481262207
    },
    {
      "epoch": 0.03121951219512195,
      "step": 144,
      "training_loss": 10.016037940979004
    },
    {
      "epoch": 0.03121951219512195,
      "step": 144,
      "training_loss": 8.480786323547363
    },
    {
      "epoch": 0.03143631436314363,
      "step": 145,
      "training_loss": 11.618303298950195
    },
    {
      "epoch": 0.03143631436314363,
      "step": 145,
      "training_loss": 9.270350456237793
    },
    {
      "epoch": 0.03143631436314363,
      "step": 145,
      "training_loss": 10.656017303466797
    },
    {
      "epoch": 0.03143631436314363,
      "step": 145,
      "training_loss": 10.925705909729004
    },
    {
      "epoch": 0.031653116531165315,
      "step": 146,
      "training_loss": 10.453651428222656
    },
    {
      "epoch": 0.031653116531165315,
      "step": 146,
      "training_loss": 9.224896430969238
    },
    {
      "epoch": 0.031653116531165315,
      "step": 146,
      "training_loss": 9.673699378967285
    },
    {
      "epoch": 0.031653116531165315,
      "step": 146,
      "training_loss": 11.773744583129883
    },
    {
      "epoch": 0.03186991869918699,
      "step": 147,
      "training_loss": 9.927433013916016
    },
    {
      "epoch": 0.03186991869918699,
      "step": 147,
      "training_loss": 9.232353210449219
    },
    {
      "epoch": 0.03186991869918699,
      "step": 147,
      "training_loss": 11.061736106872559
    },
    {
      "epoch": 0.03186991869918699,
      "step": 147,
      "training_loss": 10.58759593963623
    },
    {
      "epoch": 0.03208672086720867,
      "grad_norm": 42.537139892578125,
      "learning_rate": 1e-05,
      "loss": 10.2372,
      "step": 148
    },
    {
      "epoch": 0.03208672086720867,
      "step": 148,
      "training_loss": 10.731697082519531
    },
    {
      "epoch": 0.03208672086720867,
      "step": 148,
      "training_loss": 9.828227996826172
    },
    {
      "epoch": 0.03208672086720867,
      "step": 148,
      "training_loss": 9.809300422668457
    },
    {
      "epoch": 0.03208672086720867,
      "step": 148,
      "training_loss": 8.474058151245117
    },
    {
      "epoch": 0.032303523035230354,
      "step": 149,
      "training_loss": 9.802331924438477
    },
    {
      "epoch": 0.032303523035230354,
      "step": 149,
      "training_loss": 8.84597396850586
    },
    {
      "epoch": 0.032303523035230354,
      "step": 149,
      "training_loss": 9.180809020996094
    },
    {
      "epoch": 0.032303523035230354,
      "step": 149,
      "training_loss": 10.063895225524902
    },
    {
      "epoch": 0.032520325203252036,
      "step": 150,
      "training_loss": 9.967555046081543
    },
    {
      "epoch": 0.032520325203252036,
      "step": 150,
      "training_loss": 9.998435974121094
    },
    {
      "epoch": 0.032520325203252036,
      "step": 150,
      "training_loss": 9.245038032531738
    },
    {
      "epoch": 0.032520325203252036,
      "step": 150,
      "training_loss": 10.064384460449219
    },
    {
      "epoch": 0.03273712737127371,
      "step": 151,
      "training_loss": 9.381134986877441
    },
    {
      "epoch": 0.03273712737127371,
      "step": 151,
      "training_loss": 8.764579772949219
    },
    {
      "epoch": 0.03273712737127371,
      "step": 151,
      "training_loss": 9.321680068969727
    },
    {
      "epoch": 0.03273712737127371,
      "step": 151,
      "training_loss": 10.341085433959961
    },
    {
      "epoch": 0.03295392953929539,
      "grad_norm": 13.916402816772461,
      "learning_rate": 1e-05,
      "loss": 9.6138,
      "step": 152
    },
    {
      "epoch": 0.03295392953929539,
      "step": 152,
      "training_loss": 9.398460388183594
    },
    {
      "epoch": 0.03295392953929539,
      "step": 152,
      "training_loss": 11.251470565795898
    },
    {
      "epoch": 0.03295392953929539,
      "step": 152,
      "training_loss": 8.56664752960205
    },
    {
      "epoch": 0.03295392953929539,
      "step": 152,
      "training_loss": 11.114553451538086
    },
    {
      "epoch": 0.033170731707317075,
      "step": 153,
      "training_loss": 11.300224304199219
    },
    {
      "epoch": 0.033170731707317075,
      "step": 153,
      "training_loss": 11.244832992553711
    },
    {
      "epoch": 0.033170731707317075,
      "step": 153,
      "training_loss": 8.961382865905762
    },
    {
      "epoch": 0.033170731707317075,
      "step": 153,
      "training_loss": 11.608078956604004
    },
    {
      "epoch": 0.03338753387533876,
      "step": 154,
      "training_loss": 8.850805282592773
    },
    {
      "epoch": 0.03338753387533876,
      "step": 154,
      "training_loss": 9.36047077178955
    },
    {
      "epoch": 0.03338753387533876,
      "step": 154,
      "training_loss": 11.810346603393555
    },
    {
      "epoch": 0.03338753387533876,
      "step": 154,
      "training_loss": 9.960837364196777
    },
    {
      "epoch": 0.03360433604336043,
      "step": 155,
      "training_loss": 10.651169776916504
    },
    {
      "epoch": 0.03360433604336043,
      "step": 155,
      "training_loss": 9.161591529846191
    },
    {
      "epoch": 0.03360433604336043,
      "step": 155,
      "training_loss": 8.582335472106934
    },
    {
      "epoch": 0.03360433604336043,
      "step": 155,
      "training_loss": 8.877738952636719
    },
    {
      "epoch": 0.033821138211382114,
      "grad_norm": 37.48689651489258,
      "learning_rate": 1e-05,
      "loss": 10.0438,
      "step": 156
    },
    {
      "epoch": 0.033821138211382114,
      "step": 156,
      "training_loss": 8.870003700256348
    },
    {
      "epoch": 0.033821138211382114,
      "step": 156,
      "training_loss": 9.932001113891602
    },
    {
      "epoch": 0.033821138211382114,
      "step": 156,
      "training_loss": 10.61023235321045
    },
    {
      "epoch": 0.033821138211382114,
      "step": 156,
      "training_loss": 10.246857643127441
    },
    {
      "epoch": 0.034037940379403796,
      "step": 157,
      "training_loss": 9.649466514587402
    },
    {
      "epoch": 0.034037940379403796,
      "step": 157,
      "training_loss": 10.469298362731934
    },
    {
      "epoch": 0.034037940379403796,
      "step": 157,
      "training_loss": 11.165931701660156
    },
    {
      "epoch": 0.034037940379403796,
      "step": 157,
      "training_loss": 8.927701950073242
    },
    {
      "epoch": 0.03425474254742548,
      "step": 158,
      "training_loss": 9.783921241760254
    },
    {
      "epoch": 0.03425474254742548,
      "step": 158,
      "training_loss": 10.484967231750488
    },
    {
      "epoch": 0.03425474254742548,
      "step": 158,
      "training_loss": 10.43018913269043
    },
    {
      "epoch": 0.03425474254742548,
      "step": 158,
      "training_loss": 9.46003246307373
    },
    {
      "epoch": 0.03447154471544715,
      "step": 159,
      "training_loss": 11.492266654968262
    },
    {
      "epoch": 0.03447154471544715,
      "step": 159,
      "training_loss": 8.669312477111816
    },
    {
      "epoch": 0.03447154471544715,
      "step": 159,
      "training_loss": 12.573001861572266
    },
    {
      "epoch": 0.03447154471544715,
      "step": 159,
      "training_loss": 8.726821899414062
    },
    {
      "epoch": 0.034688346883468835,
      "grad_norm": 16.240442276000977,
      "learning_rate": 1e-05,
      "loss": 10.0933,
      "step": 160
    },
    {
      "epoch": 0.034688346883468835,
      "step": 160,
      "training_loss": 10.525421142578125
    },
    {
      "epoch": 0.034688346883468835,
      "step": 160,
      "training_loss": 10.460885047912598
    },
    {
      "epoch": 0.034688346883468835,
      "step": 160,
      "training_loss": 8.369852066040039
    },
    {
      "epoch": 0.034688346883468835,
      "step": 160,
      "training_loss": 9.092550277709961
    },
    {
      "epoch": 0.03490514905149052,
      "step": 161,
      "training_loss": 9.987252235412598
    },
    {
      "epoch": 0.03490514905149052,
      "step": 161,
      "training_loss": 9.822426795959473
    },
    {
      "epoch": 0.03490514905149052,
      "step": 161,
      "training_loss": 8.282438278198242
    },
    {
      "epoch": 0.03490514905149052,
      "step": 161,
      "training_loss": 9.070656776428223
    },
    {
      "epoch": 0.0351219512195122,
      "step": 162,
      "training_loss": 9.763993263244629
    },
    {
      "epoch": 0.0351219512195122,
      "step": 162,
      "training_loss": 10.434200286865234
    },
    {
      "epoch": 0.0351219512195122,
      "step": 162,
      "training_loss": 8.992134094238281
    },
    {
      "epoch": 0.0351219512195122,
      "step": 162,
      "training_loss": 7.936783790588379
    },
    {
      "epoch": 0.035338753387533874,
      "step": 163,
      "training_loss": 8.92808723449707
    },
    {
      "epoch": 0.035338753387533874,
      "step": 163,
      "training_loss": 10.293033599853516
    },
    {
      "epoch": 0.035338753387533874,
      "step": 163,
      "training_loss": 9.982207298278809
    },
    {
      "epoch": 0.035338753387533874,
      "step": 163,
      "training_loss": 8.977181434631348
    },
    {
      "epoch": 0.035555555555555556,
      "grad_norm": 28.31463050842285,
      "learning_rate": 1e-05,
      "loss": 9.4324,
      "step": 164
    },
    {
      "epoch": 0.035555555555555556,
      "step": 164,
      "training_loss": 10.087586402893066
    },
    {
      "epoch": 0.035555555555555556,
      "step": 164,
      "training_loss": 9.590227127075195
    },
    {
      "epoch": 0.035555555555555556,
      "step": 164,
      "training_loss": 8.858109474182129
    },
    {
      "epoch": 0.035555555555555556,
      "step": 164,
      "training_loss": 9.333183288574219
    },
    {
      "epoch": 0.03577235772357724,
      "step": 165,
      "training_loss": 8.479607582092285
    },
    {
      "epoch": 0.03577235772357724,
      "step": 165,
      "training_loss": 9.125449180603027
    },
    {
      "epoch": 0.03577235772357724,
      "step": 165,
      "training_loss": 8.458152770996094
    },
    {
      "epoch": 0.03577235772357724,
      "step": 165,
      "training_loss": 8.512654304504395
    },
    {
      "epoch": 0.03598915989159892,
      "step": 166,
      "training_loss": 10.945270538330078
    },
    {
      "epoch": 0.03598915989159892,
      "step": 166,
      "training_loss": 9.392878532409668
    },
    {
      "epoch": 0.03598915989159892,
      "step": 166,
      "training_loss": 10.031336784362793
    },
    {
      "epoch": 0.03598915989159892,
      "step": 166,
      "training_loss": 8.512808799743652
    },
    {
      "epoch": 0.036205962059620594,
      "step": 167,
      "training_loss": 10.64546012878418
    },
    {
      "epoch": 0.036205962059620594,
      "step": 167,
      "training_loss": 9.919219017028809
    },
    {
      "epoch": 0.036205962059620594,
      "step": 167,
      "training_loss": 10.021720886230469
    },
    {
      "epoch": 0.036205962059620594,
      "step": 167,
      "training_loss": 10.226263046264648
    },
    {
      "epoch": 0.036422764227642276,
      "grad_norm": 32.81133270263672,
      "learning_rate": 1e-05,
      "loss": 9.5087,
      "step": 168
    },
    {
      "epoch": 0.036422764227642276,
      "step": 168,
      "training_loss": 10.195094108581543
    },
    {
      "epoch": 0.036422764227642276,
      "step": 168,
      "training_loss": 10.200845718383789
    },
    {
      "epoch": 0.036422764227642276,
      "step": 168,
      "training_loss": 9.468170166015625
    },
    {
      "epoch": 0.036422764227642276,
      "step": 168,
      "training_loss": 9.669925689697266
    },
    {
      "epoch": 0.03663956639566396,
      "step": 169,
      "training_loss": 11.13676643371582
    },
    {
      "epoch": 0.03663956639566396,
      "step": 169,
      "training_loss": 9.729119300842285
    },
    {
      "epoch": 0.03663956639566396,
      "step": 169,
      "training_loss": 10.373879432678223
    },
    {
      "epoch": 0.03663956639566396,
      "step": 169,
      "training_loss": 8.572173118591309
    },
    {
      "epoch": 0.03685636856368564,
      "step": 170,
      "training_loss": 11.096277236938477
    },
    {
      "epoch": 0.03685636856368564,
      "step": 170,
      "training_loss": 8.103405952453613
    },
    {
      "epoch": 0.03685636856368564,
      "step": 170,
      "training_loss": 8.225003242492676
    },
    {
      "epoch": 0.03685636856368564,
      "step": 170,
      "training_loss": 9.464632987976074
    },
    {
      "epoch": 0.037073170731707315,
      "step": 171,
      "training_loss": 12.195710182189941
    },
    {
      "epoch": 0.037073170731707315,
      "step": 171,
      "training_loss": 11.018428802490234
    },
    {
      "epoch": 0.037073170731707315,
      "step": 171,
      "training_loss": 9.171996116638184
    },
    {
      "epoch": 0.037073170731707315,
      "step": 171,
      "training_loss": 8.251835823059082
    },
    {
      "epoch": 0.037289972899729,
      "grad_norm": 110.4805679321289,
      "learning_rate": 1e-05,
      "loss": 9.8046,
      "step": 172
    },
    {
      "epoch": 0.037289972899729,
      "step": 172,
      "training_loss": 8.797334671020508
    },
    {
      "epoch": 0.037289972899729,
      "step": 172,
      "training_loss": 9.258408546447754
    },
    {
      "epoch": 0.037289972899729,
      "step": 172,
      "training_loss": 8.657556533813477
    },
    {
      "epoch": 0.037289972899729,
      "step": 172,
      "training_loss": 11.32192611694336
    },
    {
      "epoch": 0.03750677506775068,
      "step": 173,
      "training_loss": 8.920894622802734
    },
    {
      "epoch": 0.03750677506775068,
      "step": 173,
      "training_loss": 9.37264347076416
    },
    {
      "epoch": 0.03750677506775068,
      "step": 173,
      "training_loss": 10.00223159790039
    },
    {
      "epoch": 0.03750677506775068,
      "step": 173,
      "training_loss": 8.93328857421875
    },
    {
      "epoch": 0.03772357723577236,
      "step": 174,
      "training_loss": 11.250020980834961
    },
    {
      "epoch": 0.03772357723577236,
      "step": 174,
      "training_loss": 11.073384284973145
    },
    {
      "epoch": 0.03772357723577236,
      "step": 174,
      "training_loss": 9.475135803222656
    },
    {
      "epoch": 0.03772357723577236,
      "step": 174,
      "training_loss": 11.286503791809082
    },
    {
      "epoch": 0.037940379403794036,
      "step": 175,
      "training_loss": 8.962199211120605
    },
    {
      "epoch": 0.037940379403794036,
      "step": 175,
      "training_loss": 8.872245788574219
    },
    {
      "epoch": 0.037940379403794036,
      "step": 175,
      "training_loss": 9.304991722106934
    },
    {
      "epoch": 0.037940379403794036,
      "step": 175,
      "training_loss": 10.175762176513672
    },
    {
      "epoch": 0.03815718157181572,
      "grad_norm": 21.306596755981445,
      "learning_rate": 1e-05,
      "loss": 9.729,
      "step": 176
    },
    {
      "epoch": 0.03815718157181572,
      "step": 176,
      "training_loss": 8.190937995910645
    },
    {
      "epoch": 0.03815718157181572,
      "step": 176,
      "training_loss": 10.442930221557617
    },
    {
      "epoch": 0.03815718157181572,
      "step": 176,
      "training_loss": 13.4029541015625
    },
    {
      "epoch": 0.03815718157181572,
      "step": 176,
      "training_loss": 8.148096084594727
    },
    {
      "epoch": 0.0383739837398374,
      "step": 177,
      "training_loss": 10.842682838439941
    },
    {
      "epoch": 0.0383739837398374,
      "step": 177,
      "training_loss": 9.170015335083008
    },
    {
      "epoch": 0.0383739837398374,
      "step": 177,
      "training_loss": 10.535585403442383
    },
    {
      "epoch": 0.0383739837398374,
      "step": 177,
      "training_loss": 7.880204200744629
    },
    {
      "epoch": 0.03859078590785908,
      "step": 178,
      "training_loss": 6.856189250946045
    },
    {
      "epoch": 0.03859078590785908,
      "step": 178,
      "training_loss": 8.266321182250977
    },
    {
      "epoch": 0.03859078590785908,
      "step": 178,
      "training_loss": 8.605414390563965
    },
    {
      "epoch": 0.03859078590785908,
      "step": 178,
      "training_loss": 9.208160400390625
    },
    {
      "epoch": 0.03880758807588076,
      "step": 179,
      "training_loss": 8.858125686645508
    },
    {
      "epoch": 0.03880758807588076,
      "step": 179,
      "training_loss": 8.418610572814941
    },
    {
      "epoch": 0.03880758807588076,
      "step": 179,
      "training_loss": 8.684006690979004
    },
    {
      "epoch": 0.03880758807588076,
      "step": 179,
      "training_loss": 9.0328369140625
    },
    {
      "epoch": 0.03902439024390244,
      "grad_norm": 29.667078018188477,
      "learning_rate": 1e-05,
      "loss": 9.1589,
      "step": 180
    },
    {
      "epoch": 0.03902439024390244,
      "step": 180,
      "training_loss": 9.18176555633545
    },
    {
      "epoch": 0.03902439024390244,
      "step": 180,
      "training_loss": 10.007966041564941
    },
    {
      "epoch": 0.03902439024390244,
      "step": 180,
      "training_loss": 9.42605972290039
    },
    {
      "epoch": 0.03902439024390244,
      "step": 180,
      "training_loss": 10.04361343383789
    },
    {
      "epoch": 0.03924119241192412,
      "step": 181,
      "training_loss": 7.873898029327393
    },
    {
      "epoch": 0.03924119241192412,
      "step": 181,
      "training_loss": 10.032234191894531
    },
    {
      "epoch": 0.03924119241192412,
      "step": 181,
      "training_loss": 9.170682907104492
    },
    {
      "epoch": 0.03924119241192412,
      "step": 181,
      "training_loss": 9.39611530303955
    },
    {
      "epoch": 0.0394579945799458,
      "step": 182,
      "training_loss": 8.157442092895508
    },
    {
      "epoch": 0.0394579945799458,
      "step": 182,
      "training_loss": 8.899999618530273
    },
    {
      "epoch": 0.0394579945799458,
      "step": 182,
      "training_loss": 9.822864532470703
    },
    {
      "epoch": 0.0394579945799458,
      "step": 182,
      "training_loss": 9.374216079711914
    },
    {
      "epoch": 0.03967479674796748,
      "step": 183,
      "training_loss": 9.258973121643066
    },
    {
      "epoch": 0.03967479674796748,
      "step": 183,
      "training_loss": 9.333534240722656
    },
    {
      "epoch": 0.03967479674796748,
      "step": 183,
      "training_loss": 9.597258567810059
    },
    {
      "epoch": 0.03967479674796748,
      "step": 183,
      "training_loss": 8.082286834716797
    },
    {
      "epoch": 0.03989159891598916,
      "grad_norm": 27.6683349609375,
      "learning_rate": 1e-05,
      "loss": 9.2287,
      "step": 184
    },
    {
      "epoch": 0.03989159891598916,
      "step": 184,
      "training_loss": 10.179617881774902
    },
    {
      "epoch": 0.03989159891598916,
      "step": 184,
      "training_loss": 8.531780242919922
    },
    {
      "epoch": 0.03989159891598916,
      "step": 184,
      "training_loss": 8.322684288024902
    },
    {
      "epoch": 0.03989159891598916,
      "step": 184,
      "training_loss": 9.610331535339355
    },
    {
      "epoch": 0.04010840108401084,
      "step": 185,
      "training_loss": 10.3333101272583
    },
    {
      "epoch": 0.04010840108401084,
      "step": 185,
      "training_loss": 8.047547340393066
    },
    {
      "epoch": 0.04010840108401084,
      "step": 185,
      "training_loss": 9.811383247375488
    },
    {
      "epoch": 0.04010840108401084,
      "step": 185,
      "training_loss": 9.923904418945312
    },
    {
      "epoch": 0.040325203252032524,
      "step": 186,
      "training_loss": 10.12647819519043
    },
    {
      "epoch": 0.040325203252032524,
      "step": 186,
      "training_loss": 9.886507034301758
    },
    {
      "epoch": 0.040325203252032524,
      "step": 186,
      "training_loss": 8.847984313964844
    },
    {
      "epoch": 0.040325203252032524,
      "step": 186,
      "training_loss": 9.029501914978027
    },
    {
      "epoch": 0.0405420054200542,
      "step": 187,
      "training_loss": 8.510956764221191
    },
    {
      "epoch": 0.0405420054200542,
      "step": 187,
      "training_loss": 10.015551567077637
    },
    {
      "epoch": 0.0405420054200542,
      "step": 187,
      "training_loss": 9.48879337310791
    },
    {
      "epoch": 0.0405420054200542,
      "step": 187,
      "training_loss": 9.027856826782227
    },
    {
      "epoch": 0.04075880758807588,
      "grad_norm": 29.682771682739258,
      "learning_rate": 1e-05,
      "loss": 9.3559,
      "step": 188
    },
    {
      "epoch": 0.04075880758807588,
      "step": 188,
      "training_loss": 9.501133918762207
    },
    {
      "epoch": 0.04075880758807588,
      "step": 188,
      "training_loss": 9.207417488098145
    },
    {
      "epoch": 0.04075880758807588,
      "step": 188,
      "training_loss": 8.88254165649414
    },
    {
      "epoch": 0.04075880758807588,
      "step": 188,
      "training_loss": 9.142385482788086
    },
    {
      "epoch": 0.04097560975609756,
      "step": 189,
      "training_loss": 8.77894115447998
    },
    {
      "epoch": 0.04097560975609756,
      "step": 189,
      "training_loss": 9.200428009033203
    },
    {
      "epoch": 0.04097560975609756,
      "step": 189,
      "training_loss": 9.45388126373291
    },
    {
      "epoch": 0.04097560975609756,
      "step": 189,
      "training_loss": 8.236109733581543
    },
    {
      "epoch": 0.041192411924119245,
      "step": 190,
      "training_loss": 8.099496841430664
    },
    {
      "epoch": 0.041192411924119245,
      "step": 190,
      "training_loss": 8.502636909484863
    },
    {
      "epoch": 0.041192411924119245,
      "step": 190,
      "training_loss": 8.73820972442627
    },
    {
      "epoch": 0.041192411924119245,
      "step": 190,
      "training_loss": 10.241384506225586
    },
    {
      "epoch": 0.04140921409214092,
      "step": 191,
      "training_loss": 8.114360809326172
    },
    {
      "epoch": 0.04140921409214092,
      "step": 191,
      "training_loss": 8.252923965454102
    },
    {
      "epoch": 0.04140921409214092,
      "step": 191,
      "training_loss": 9.643531799316406
    },
    {
      "epoch": 0.04140921409214092,
      "step": 191,
      "training_loss": 7.751065254211426
    },
    {
      "epoch": 0.0416260162601626,
      "grad_norm": 10.694164276123047,
      "learning_rate": 1e-05,
      "loss": 8.8592,
      "step": 192
    },
    {
      "epoch": 0.0416260162601626,
      "step": 192,
      "training_loss": 8.676260948181152
    },
    {
      "epoch": 0.0416260162601626,
      "step": 192,
      "training_loss": 9.274267196655273
    },
    {
      "epoch": 0.0416260162601626,
      "step": 192,
      "training_loss": 8.67809772491455
    },
    {
      "epoch": 0.0416260162601626,
      "step": 192,
      "training_loss": 9.480671882629395
    },
    {
      "epoch": 0.041842818428184284,
      "step": 193,
      "training_loss": 9.182045936584473
    },
    {
      "epoch": 0.041842818428184284,
      "step": 193,
      "training_loss": 10.04173469543457
    },
    {
      "epoch": 0.041842818428184284,
      "step": 193,
      "training_loss": 7.960485458374023
    },
    {
      "epoch": 0.041842818428184284,
      "step": 193,
      "training_loss": 7.857387065887451
    },
    {
      "epoch": 0.042059620596205965,
      "step": 194,
      "training_loss": 10.860186576843262
    },
    {
      "epoch": 0.042059620596205965,
      "step": 194,
      "training_loss": 9.000139236450195
    },
    {
      "epoch": 0.042059620596205965,
      "step": 194,
      "training_loss": 8.353124618530273
    },
    {
      "epoch": 0.042059620596205965,
      "step": 194,
      "training_loss": 8.539379119873047
    },
    {
      "epoch": 0.04227642276422764,
      "step": 195,
      "training_loss": 9.980469703674316
    },
    {
      "epoch": 0.04227642276422764,
      "step": 195,
      "training_loss": 9.813726425170898
    },
    {
      "epoch": 0.04227642276422764,
      "step": 195,
      "training_loss": 9.700019836425781
    },
    {
      "epoch": 0.04227642276422764,
      "step": 195,
      "training_loss": 10.60743236541748
    },
    {
      "epoch": 0.04249322493224932,
      "grad_norm": 11.520845413208008,
      "learning_rate": 1e-05,
      "loss": 9.2503,
      "step": 196
    },
    {
      "epoch": 0.04249322493224932,
      "step": 196,
      "training_loss": 9.195685386657715
    },
    {
      "epoch": 0.04249322493224932,
      "step": 196,
      "training_loss": 10.527647018432617
    },
    {
      "epoch": 0.04249322493224932,
      "step": 196,
      "training_loss": 9.044052124023438
    },
    {
      "epoch": 0.04249322493224932,
      "step": 196,
      "training_loss": 8.94534969329834
    },
    {
      "epoch": 0.042710027100271004,
      "step": 197,
      "training_loss": 8.66502571105957
    },
    {
      "epoch": 0.042710027100271004,
      "step": 197,
      "training_loss": 10.28990650177002
    },
    {
      "epoch": 0.042710027100271004,
      "step": 197,
      "training_loss": 8.749714851379395
    },
    {
      "epoch": 0.042710027100271004,
      "step": 197,
      "training_loss": 8.974568367004395
    },
    {
      "epoch": 0.042926829268292686,
      "step": 198,
      "training_loss": 8.593314170837402
    },
    {
      "epoch": 0.042926829268292686,
      "step": 198,
      "training_loss": 8.412419319152832
    },
    {
      "epoch": 0.042926829268292686,
      "step": 198,
      "training_loss": 8.98813533782959
    },
    {
      "epoch": 0.042926829268292686,
      "step": 198,
      "training_loss": 10.883451461791992
    },
    {
      "epoch": 0.04314363143631436,
      "step": 199,
      "training_loss": 9.441195487976074
    },
    {
      "epoch": 0.04314363143631436,
      "step": 199,
      "training_loss": 9.741485595703125
    },
    {
      "epoch": 0.04314363143631436,
      "step": 199,
      "training_loss": 8.38135814666748
    },
    {
      "epoch": 0.04314363143631436,
      "step": 199,
      "training_loss": 9.792855262756348
    },
    {
      "epoch": 0.04336043360433604,
      "grad_norm": 16.712202072143555,
      "learning_rate": 1e-05,
      "loss": 9.2891,
      "step": 200
    },
    {
      "epoch": 0.04336043360433604,
      "step": 200,
      "training_loss": 10.11628532409668
    },
    {
      "epoch": 0.04336043360433604,
      "step": 200,
      "training_loss": 9.353677749633789
    },
    {
      "epoch": 0.04336043360433604,
      "step": 200,
      "training_loss": 9.348518371582031
    },
    {
      "epoch": 0.04336043360433604,
      "step": 200,
      "training_loss": 8.571377754211426
    },
    {
      "epoch": 0.043577235772357725,
      "step": 201,
      "training_loss": 8.632287979125977
    },
    {
      "epoch": 0.043577235772357725,
      "step": 201,
      "training_loss": 9.704370498657227
    },
    {
      "epoch": 0.043577235772357725,
      "step": 201,
      "training_loss": 9.187885284423828
    },
    {
      "epoch": 0.043577235772357725,
      "step": 201,
      "training_loss": 9.775745391845703
    },
    {
      "epoch": 0.04379403794037941,
      "step": 202,
      "training_loss": 8.586414337158203
    },
    {
      "epoch": 0.04379403794037941,
      "step": 202,
      "training_loss": 9.574522018432617
    },
    {
      "epoch": 0.04379403794037941,
      "step": 202,
      "training_loss": 9.013510704040527
    },
    {
      "epoch": 0.04379403794037941,
      "step": 202,
      "training_loss": 8.558547973632812
    },
    {
      "epoch": 0.04401084010840108,
      "step": 203,
      "training_loss": 8.430643081665039
    },
    {
      "epoch": 0.04401084010840108,
      "step": 203,
      "training_loss": 8.895831108093262
    },
    {
      "epoch": 0.04401084010840108,
      "step": 203,
      "training_loss": 9.262554168701172
    },
    {
      "epoch": 0.04401084010840108,
      "step": 203,
      "training_loss": 8.210737228393555
    },
    {
      "epoch": 0.044227642276422764,
      "grad_norm": 10.223176002502441,
      "learning_rate": 1e-05,
      "loss": 9.0764,
      "step": 204
    },
    {
      "epoch": 0.044227642276422764,
      "step": 204,
      "training_loss": 9.912593841552734
    },
    {
      "epoch": 0.044227642276422764,
      "step": 204,
      "training_loss": 7.235473155975342
    },
    {
      "epoch": 0.044227642276422764,
      "step": 204,
      "training_loss": 9.521230697631836
    },
    {
      "epoch": 0.044227642276422764,
      "step": 204,
      "training_loss": 7.95389986038208
    },
    {
      "epoch": 0.044444444444444446,
      "step": 205,
      "training_loss": 9.561641693115234
    },
    {
      "epoch": 0.044444444444444446,
      "step": 205,
      "training_loss": 8.780721664428711
    },
    {
      "epoch": 0.044444444444444446,
      "step": 205,
      "training_loss": 8.284393310546875
    },
    {
      "epoch": 0.044444444444444446,
      "step": 205,
      "training_loss": 8.517133712768555
    },
    {
      "epoch": 0.04466124661246613,
      "step": 206,
      "training_loss": 7.965731143951416
    },
    {
      "epoch": 0.04466124661246613,
      "step": 206,
      "training_loss": 10.25855541229248
    },
    {
      "epoch": 0.04466124661246613,
      "step": 206,
      "training_loss": 9.10841178894043
    },
    {
      "epoch": 0.04466124661246613,
      "step": 206,
      "training_loss": 7.545067310333252
    },
    {
      "epoch": 0.0448780487804878,
      "step": 207,
      "training_loss": 9.758606910705566
    },
    {
      "epoch": 0.0448780487804878,
      "step": 207,
      "training_loss": 7.658039093017578
    },
    {
      "epoch": 0.0448780487804878,
      "step": 207,
      "training_loss": 9.027703285217285
    },
    {
      "epoch": 0.0448780487804878,
      "step": 207,
      "training_loss": 8.982710838317871
    },
    {
      "epoch": 0.045094850948509485,
      "grad_norm": 26.47637939453125,
      "learning_rate": 1e-05,
      "loss": 8.7545,
      "step": 208
    },
    {
      "epoch": 0.045094850948509485,
      "step": 208,
      "training_loss": 8.354823112487793
    },
    {
      "epoch": 0.045094850948509485,
      "step": 208,
      "training_loss": 8.848957061767578
    },
    {
      "epoch": 0.045094850948509485,
      "step": 208,
      "training_loss": 8.053898811340332
    },
    {
      "epoch": 0.045094850948509485,
      "step": 208,
      "training_loss": 8.293992042541504
    },
    {
      "epoch": 0.04531165311653117,
      "step": 209,
      "training_loss": 8.449030876159668
    },
    {
      "epoch": 0.04531165311653117,
      "step": 209,
      "training_loss": 9.48731803894043
    },
    {
      "epoch": 0.04531165311653117,
      "step": 209,
      "training_loss": 9.99634075164795
    },
    {
      "epoch": 0.04531165311653117,
      "step": 209,
      "training_loss": 8.824938774108887
    },
    {
      "epoch": 0.04552845528455285,
      "step": 210,
      "training_loss": 7.6552534103393555
    },
    {
      "epoch": 0.04552845528455285,
      "step": 210,
      "training_loss": 8.244559288024902
    },
    {
      "epoch": 0.04552845528455285,
      "step": 210,
      "training_loss": 10.653692245483398
    },
    {
      "epoch": 0.04552845528455285,
      "step": 210,
      "training_loss": 7.64069128036499
    },
    {
      "epoch": 0.045745257452574524,
      "step": 211,
      "training_loss": 9.28791332244873
    },
    {
      "epoch": 0.045745257452574524,
      "step": 211,
      "training_loss": 7.922374725341797
    },
    {
      "epoch": 0.045745257452574524,
      "step": 211,
      "training_loss": 8.96840763092041
    },
    {
      "epoch": 0.045745257452574524,
      "step": 211,
      "training_loss": 7.673058032989502
    },
    {
      "epoch": 0.045962059620596206,
      "grad_norm": 9.75320053100586,
      "learning_rate": 1e-05,
      "loss": 8.6472,
      "step": 212
    },
    {
      "epoch": 0.045962059620596206,
      "step": 212,
      "training_loss": 9.946268081665039
    },
    {
      "epoch": 0.045962059620596206,
      "step": 212,
      "training_loss": 8.856410026550293
    },
    {
      "epoch": 0.045962059620596206,
      "step": 212,
      "training_loss": 9.484140396118164
    },
    {
      "epoch": 0.045962059620596206,
      "step": 212,
      "training_loss": 7.797473907470703
    },
    {
      "epoch": 0.04617886178861789,
      "step": 213,
      "training_loss": 10.392521858215332
    },
    {
      "epoch": 0.04617886178861789,
      "step": 213,
      "training_loss": 8.208340644836426
    },
    {
      "epoch": 0.04617886178861789,
      "step": 213,
      "training_loss": 9.295947074890137
    },
    {
      "epoch": 0.04617886178861789,
      "step": 213,
      "training_loss": 10.306452751159668
    },
    {
      "epoch": 0.04639566395663957,
      "step": 214,
      "training_loss": 8.782776832580566
    },
    {
      "epoch": 0.04639566395663957,
      "step": 214,
      "training_loss": 7.714079856872559
    },
    {
      "epoch": 0.04639566395663957,
      "step": 214,
      "training_loss": 7.591038227081299
    },
    {
      "epoch": 0.04639566395663957,
      "step": 214,
      "training_loss": 10.375707626342773
    },
    {
      "epoch": 0.046612466124661245,
      "step": 215,
      "training_loss": 9.854559898376465
    },
    {
      "epoch": 0.046612466124661245,
      "step": 215,
      "training_loss": 9.23068904876709
    },
    {
      "epoch": 0.046612466124661245,
      "step": 215,
      "training_loss": 7.687963962554932
    },
    {
      "epoch": 0.046612466124661245,
      "step": 215,
      "training_loss": 8.474515914916992
    },
    {
      "epoch": 0.04682926829268293,
      "grad_norm": 28.891124725341797,
      "learning_rate": 1e-05,
      "loss": 8.9999,
      "step": 216
    },
    {
      "epoch": 0.04682926829268293,
      "step": 216,
      "training_loss": 8.45561695098877
    },
    {
      "epoch": 0.04682926829268293,
      "step": 216,
      "training_loss": 10.410689353942871
    },
    {
      "epoch": 0.04682926829268293,
      "step": 216,
      "training_loss": 10.676591873168945
    },
    {
      "epoch": 0.04682926829268293,
      "step": 216,
      "training_loss": 9.136011123657227
    },
    {
      "epoch": 0.04704607046070461,
      "step": 217,
      "training_loss": 8.221179008483887
    },
    {
      "epoch": 0.04704607046070461,
      "step": 217,
      "training_loss": 7.840110778808594
    },
    {
      "epoch": 0.04704607046070461,
      "step": 217,
      "training_loss": 7.715977668762207
    },
    {
      "epoch": 0.04704607046070461,
      "step": 217,
      "training_loss": 8.515694618225098
    },
    {
      "epoch": 0.04726287262872629,
      "step": 218,
      "training_loss": 8.378270149230957
    },
    {
      "epoch": 0.04726287262872629,
      "step": 218,
      "training_loss": 8.848543167114258
    },
    {
      "epoch": 0.04726287262872629,
      "step": 218,
      "training_loss": 8.912487983703613
    },
    {
      "epoch": 0.04726287262872629,
      "step": 218,
      "training_loss": 7.844642639160156
    },
    {
      "epoch": 0.047479674796747966,
      "step": 219,
      "training_loss": 8.270233154296875
    },
    {
      "epoch": 0.047479674796747966,
      "step": 219,
      "training_loss": 8.082622528076172
    },
    {
      "epoch": 0.047479674796747966,
      "step": 219,
      "training_loss": 8.622319221496582
    },
    {
      "epoch": 0.047479674796747966,
      "step": 219,
      "training_loss": 9.690159797668457
    },
    {
      "epoch": 0.04769647696476965,
      "grad_norm": 13.985469818115234,
      "learning_rate": 1e-05,
      "loss": 8.7263,
      "step": 220
    },
    {
      "epoch": 0.04769647696476965,
      "step": 220,
      "training_loss": 8.384882926940918
    },
    {
      "epoch": 0.04769647696476965,
      "step": 220,
      "training_loss": 8.646834373474121
    },
    {
      "epoch": 0.04769647696476965,
      "step": 220,
      "training_loss": 10.081990242004395
    },
    {
      "epoch": 0.04769647696476965,
      "step": 220,
      "training_loss": 7.943273544311523
    },
    {
      "epoch": 0.04791327913279133,
      "step": 221,
      "training_loss": 8.3005952835083
    },
    {
      "epoch": 0.04791327913279133,
      "step": 221,
      "training_loss": 9.915096282958984
    },
    {
      "epoch": 0.04791327913279133,
      "step": 221,
      "training_loss": 8.07335090637207
    },
    {
      "epoch": 0.04791327913279133,
      "step": 221,
      "training_loss": 8.158790588378906
    },
    {
      "epoch": 0.04813008130081301,
      "step": 222,
      "training_loss": 7.622870445251465
    },
    {
      "epoch": 0.04813008130081301,
      "step": 222,
      "training_loss": 9.200005531311035
    },
    {
      "epoch": 0.04813008130081301,
      "step": 222,
      "training_loss": 8.182999610900879
    },
    {
      "epoch": 0.04813008130081301,
      "step": 222,
      "training_loss": 10.061273574829102
    },
    {
      "epoch": 0.04834688346883469,
      "step": 223,
      "training_loss": 10.184999465942383
    },
    {
      "epoch": 0.04834688346883469,
      "step": 223,
      "training_loss": 9.318503379821777
    },
    {
      "epoch": 0.04834688346883469,
      "step": 223,
      "training_loss": 8.631250381469727
    },
    {
      "epoch": 0.04834688346883469,
      "step": 223,
      "training_loss": 9.146003723144531
    },
    {
      "epoch": 0.04856368563685637,
      "grad_norm": 30.91859245300293,
      "learning_rate": 1e-05,
      "loss": 8.8658,
      "step": 224
    },
    {
      "epoch": 0.04856368563685637,
      "step": 224,
      "training_loss": 9.274283409118652
    },
    {
      "epoch": 0.04856368563685637,
      "step": 224,
      "training_loss": 8.896712303161621
    },
    {
      "epoch": 0.04856368563685637,
      "step": 224,
      "training_loss": 7.998490810394287
    },
    {
      "epoch": 0.04856368563685637,
      "step": 224,
      "training_loss": 9.344998359680176
    },
    {
      "epoch": 0.04878048780487805,
      "step": 225,
      "training_loss": 8.142383575439453
    },
    {
      "epoch": 0.04878048780487805,
      "step": 225,
      "training_loss": 8.975244522094727
    },
    {
      "epoch": 0.04878048780487805,
      "step": 225,
      "training_loss": 8.391912460327148
    },
    {
      "epoch": 0.04878048780487805,
      "step": 225,
      "training_loss": 8.618861198425293
    },
    {
      "epoch": 0.04899728997289973,
      "step": 226,
      "training_loss": 8.598305702209473
    },
    {
      "epoch": 0.04899728997289973,
      "step": 226,
      "training_loss": 9.665105819702148
    },
    {
      "epoch": 0.04899728997289973,
      "step": 226,
      "training_loss": 8.434403419494629
    },
    {
      "epoch": 0.04899728997289973,
      "step": 226,
      "training_loss": 9.732810974121094
    },
    {
      "epoch": 0.04921409214092141,
      "step": 227,
      "training_loss": 8.558923721313477
    },
    {
      "epoch": 0.04921409214092141,
      "step": 227,
      "training_loss": 7.901880741119385
    },
    {
      "epoch": 0.04921409214092141,
      "step": 227,
      "training_loss": 9.830090522766113
    },
    {
      "epoch": 0.04921409214092141,
      "step": 227,
      "training_loss": 9.125487327575684
    },
    {
      "epoch": 0.04943089430894309,
      "grad_norm": 8.107732772827148,
      "learning_rate": 1e-05,
      "loss": 8.8431,
      "step": 228
    },
    {
      "epoch": 0.04943089430894309,
      "step": 228,
      "training_loss": 7.939123630523682
    },
    {
      "epoch": 0.04943089430894309,
      "step": 228,
      "training_loss": 8.543916702270508
    },
    {
      "epoch": 0.04943089430894309,
      "step": 228,
      "training_loss": 9.789125442504883
    },
    {
      "epoch": 0.04943089430894309,
      "step": 228,
      "training_loss": 9.612396240234375
    },
    {
      "epoch": 0.04964769647696477,
      "step": 229,
      "training_loss": 8.476707458496094
    },
    {
      "epoch": 0.04964769647696477,
      "step": 229,
      "training_loss": 7.921596050262451
    },
    {
      "epoch": 0.04964769647696477,
      "step": 229,
      "training_loss": 9.539976119995117
    },
    {
      "epoch": 0.04964769647696477,
      "step": 229,
      "training_loss": 8.591914176940918
    },
    {
      "epoch": 0.04986449864498645,
      "step": 230,
      "training_loss": 7.50105619430542
    },
    {
      "epoch": 0.04986449864498645,
      "step": 230,
      "training_loss": 8.96015739440918
    },
    {
      "epoch": 0.04986449864498645,
      "step": 230,
      "training_loss": 9.374116897583008
    },
    {
      "epoch": 0.04986449864498645,
      "step": 230,
      "training_loss": 8.813368797302246
    },
    {
      "epoch": 0.05008130081300813,
      "step": 231,
      "training_loss": 9.482199668884277
    },
    {
      "epoch": 0.05008130081300813,
      "step": 231,
      "training_loss": 9.239583969116211
    },
    {
      "epoch": 0.05008130081300813,
      "step": 231,
      "training_loss": 8.053783416748047
    },
    {
      "epoch": 0.05008130081300813,
      "step": 231,
      "training_loss": 10.249286651611328
    },
    {
      "epoch": 0.05029810298102981,
      "grad_norm": 11.678272247314453,
      "learning_rate": 1e-05,
      "loss": 8.8805,
      "step": 232
    },
    {
      "epoch": 0.05029810298102981,
      "step": 232,
      "training_loss": 7.882024765014648
    },
    {
      "epoch": 0.05029810298102981,
      "step": 232,
      "training_loss": 8.221413612365723
    },
    {
      "epoch": 0.05029810298102981,
      "step": 232,
      "training_loss": 7.750812530517578
    },
    {
      "epoch": 0.05029810298102981,
      "step": 232,
      "training_loss": 8.95352840423584
    },
    {
      "epoch": 0.05051490514905149,
      "step": 233,
      "training_loss": 8.54931926727295
    },
    {
      "epoch": 0.05051490514905149,
      "step": 233,
      "training_loss": 7.656803607940674
    },
    {
      "epoch": 0.05051490514905149,
      "step": 233,
      "training_loss": 8.269474029541016
    },
    {
      "epoch": 0.05051490514905149,
      "step": 233,
      "training_loss": 8.393102645874023
    },
    {
      "epoch": 0.050731707317073174,
      "step": 234,
      "training_loss": 7.427395343780518
    },
    {
      "epoch": 0.050731707317073174,
      "step": 234,
      "training_loss": 7.818050384521484
    },
    {
      "epoch": 0.050731707317073174,
      "step": 234,
      "training_loss": 8.138447761535645
    },
    {
      "epoch": 0.050731707317073174,
      "step": 234,
      "training_loss": 8.382891654968262
    },
    {
      "epoch": 0.05094850948509485,
      "step": 235,
      "training_loss": 7.860915184020996
    },
    {
      "epoch": 0.05094850948509485,
      "step": 235,
      "training_loss": 8.054132461547852
    },
    {
      "epoch": 0.05094850948509485,
      "step": 235,
      "training_loss": 8.726097106933594
    },
    {
      "epoch": 0.05094850948509485,
      "step": 235,
      "training_loss": 8.315268516540527
    },
    {
      "epoch": 0.05116531165311653,
      "grad_norm": 11.049824714660645,
      "learning_rate": 1e-05,
      "loss": 8.15,
      "step": 236
    },
    {
      "epoch": 0.05116531165311653,
      "step": 236,
      "training_loss": 10.31949234008789
    },
    {
      "epoch": 0.05116531165311653,
      "step": 236,
      "training_loss": 9.61026382446289
    },
    {
      "epoch": 0.05116531165311653,
      "step": 236,
      "training_loss": 7.917852878570557
    },
    {
      "epoch": 0.05116531165311653,
      "step": 236,
      "training_loss": 8.416048049926758
    },
    {
      "epoch": 0.05138211382113821,
      "step": 237,
      "training_loss": 9.7183198928833
    },
    {
      "epoch": 0.05138211382113821,
      "step": 237,
      "training_loss": 10.184354782104492
    },
    {
      "epoch": 0.05138211382113821,
      "step": 237,
      "training_loss": 7.743063449859619
    },
    {
      "epoch": 0.05138211382113821,
      "step": 237,
      "training_loss": 9.641520500183105
    },
    {
      "epoch": 0.051598915989159895,
      "step": 238,
      "training_loss": 8.718070983886719
    },
    {
      "epoch": 0.051598915989159895,
      "step": 238,
      "training_loss": 7.9094977378845215
    },
    {
      "epoch": 0.051598915989159895,
      "step": 238,
      "training_loss": 8.947247505187988
    },
    {
      "epoch": 0.051598915989159895,
      "step": 238,
      "training_loss": 8.86545467376709
    },
    {
      "epoch": 0.05181571815718157,
      "step": 239,
      "training_loss": 7.044713973999023
    },
    {
      "epoch": 0.05181571815718157,
      "step": 239,
      "training_loss": 9.160497665405273
    },
    {
      "epoch": 0.05181571815718157,
      "step": 239,
      "training_loss": 8.449448585510254
    },
    {
      "epoch": 0.05181571815718157,
      "step": 239,
      "training_loss": 8.300745010375977
    },
    {
      "epoch": 0.05203252032520325,
      "grad_norm": 43.70433044433594,
      "learning_rate": 1e-05,
      "loss": 8.8092,
      "step": 240
    },
    {
      "epoch": 0.05203252032520325,
      "step": 240,
      "training_loss": 8.358410835266113
    },
    {
      "epoch": 0.05203252032520325,
      "step": 240,
      "training_loss": 10.699419021606445
    },
    {
      "epoch": 0.05203252032520325,
      "step": 240,
      "training_loss": 9.10665225982666
    },
    {
      "epoch": 0.05203252032520325,
      "step": 240,
      "training_loss": 8.443190574645996
    },
    {
      "epoch": 0.052249322493224934,
      "step": 241,
      "training_loss": 7.338542938232422
    },
    {
      "epoch": 0.052249322493224934,
      "step": 241,
      "training_loss": 9.145149230957031
    },
    {
      "epoch": 0.052249322493224934,
      "step": 241,
      "training_loss": 8.403533935546875
    },
    {
      "epoch": 0.052249322493224934,
      "step": 241,
      "training_loss": 8.446634292602539
    },
    {
      "epoch": 0.052466124661246616,
      "step": 242,
      "training_loss": 9.66707706451416
    },
    {
      "epoch": 0.052466124661246616,
      "step": 242,
      "training_loss": 8.744996070861816
    },
    {
      "epoch": 0.052466124661246616,
      "step": 242,
      "training_loss": 7.067841053009033
    },
    {
      "epoch": 0.052466124661246616,
      "step": 242,
      "training_loss": 8.54128646850586
    },
    {
      "epoch": 0.05268292682926829,
      "step": 243,
      "training_loss": 9.081012725830078
    },
    {
      "epoch": 0.05268292682926829,
      "step": 243,
      "training_loss": 9.884233474731445
    },
    {
      "epoch": 0.05268292682926829,
      "step": 243,
      "training_loss": 10.015748977661133
    },
    {
      "epoch": 0.05268292682926829,
      "step": 243,
      "training_loss": 10.199177742004395
    },
    {
      "epoch": 0.05289972899728997,
      "grad_norm": 33.50074768066406,
      "learning_rate": 1e-05,
      "loss": 8.9464,
      "step": 244
    },
    {
      "epoch": 0.05289972899728997,
      "step": 244,
      "training_loss": 10.996451377868652
    },
    {
      "epoch": 0.05289972899728997,
      "step": 244,
      "training_loss": 9.463934898376465
    },
    {
      "epoch": 0.05289972899728997,
      "step": 244,
      "training_loss": 8.961787223815918
    },
    {
      "epoch": 0.05289972899728997,
      "step": 244,
      "training_loss": 10.656190872192383
    },
    {
      "epoch": 0.053116531165311655,
      "step": 245,
      "training_loss": 8.267074584960938
    },
    {
      "epoch": 0.053116531165311655,
      "step": 245,
      "training_loss": 7.363171100616455
    },
    {
      "epoch": 0.053116531165311655,
      "step": 245,
      "training_loss": 8.088752746582031
    },
    {
      "epoch": 0.053116531165311655,
      "step": 245,
      "training_loss": 9.666877746582031
    },
    {
      "epoch": 0.05333333333333334,
      "step": 246,
      "training_loss": 8.65302562713623
    },
    {
      "epoch": 0.05333333333333334,
      "step": 246,
      "training_loss": 8.880585670471191
    },
    {
      "epoch": 0.05333333333333334,
      "step": 246,
      "training_loss": 9.727646827697754
    },
    {
      "epoch": 0.05333333333333334,
      "step": 246,
      "training_loss": 8.384279251098633
    },
    {
      "epoch": 0.05355013550135501,
      "step": 247,
      "training_loss": 9.773380279541016
    },
    {
      "epoch": 0.05355013550135501,
      "step": 247,
      "training_loss": 8.494054794311523
    },
    {
      "epoch": 0.05355013550135501,
      "step": 247,
      "training_loss": 8.441964149475098
    },
    {
      "epoch": 0.05355013550135501,
      "step": 247,
      "training_loss": 6.763637542724609
    },
    {
      "epoch": 0.053766937669376694,
      "grad_norm": 11.865252494812012,
      "learning_rate": 1e-05,
      "loss": 8.9114,
      "step": 248
    },
    {
      "epoch": 0.053766937669376694,
      "step": 248,
      "training_loss": 10.10632038116455
    },
    {
      "epoch": 0.053766937669376694,
      "step": 248,
      "training_loss": 9.045766830444336
    },
    {
      "epoch": 0.053766937669376694,
      "step": 248,
      "training_loss": 7.85205078125
    },
    {
      "epoch": 0.053766937669376694,
      "step": 248,
      "training_loss": 7.961782455444336
    },
    {
      "epoch": 0.053983739837398376,
      "step": 249,
      "training_loss": 8.286725044250488
    },
    {
      "epoch": 0.053983739837398376,
      "step": 249,
      "training_loss": 7.879993915557861
    },
    {
      "epoch": 0.053983739837398376,
      "step": 249,
      "training_loss": 8.477680206298828
    },
    {
      "epoch": 0.053983739837398376,
      "step": 249,
      "training_loss": 8.633960723876953
    },
    {
      "epoch": 0.05420054200542006,
      "step": 250,
      "training_loss": 8.268553733825684
    },
    {
      "epoch": 0.05420054200542006,
      "step": 250,
      "training_loss": 9.9968900680542
    },
    {
      "epoch": 0.05420054200542006,
      "step": 250,
      "training_loss": 8.4429349899292
    },
    {
      "epoch": 0.05420054200542006,
      "step": 250,
      "training_loss": 7.772231101989746
    },
    {
      "epoch": 0.05441734417344173,
      "step": 251,
      "training_loss": 8.894099235534668
    },
    {
      "epoch": 0.05441734417344173,
      "step": 251,
      "training_loss": 9.401931762695312
    },
    {
      "epoch": 0.05441734417344173,
      "step": 251,
      "training_loss": 8.762146949768066
    },
    {
      "epoch": 0.05441734417344173,
      "step": 251,
      "training_loss": 8.861817359924316
    },
    {
      "epoch": 0.054634146341463415,
      "grad_norm": 9.076627731323242,
      "learning_rate": 1e-05,
      "loss": 8.6653,
      "step": 252
    },
    {
      "epoch": 0.054634146341463415,
      "step": 252,
      "training_loss": 8.517682075500488
    },
    {
      "epoch": 0.054634146341463415,
      "step": 252,
      "training_loss": 8.686667442321777
    },
    {
      "epoch": 0.054634146341463415,
      "step": 252,
      "training_loss": 8.751819610595703
    },
    {
      "epoch": 0.054634146341463415,
      "step": 252,
      "training_loss": 8.28868579864502
    },
    {
      "epoch": 0.0548509485094851,
      "step": 253,
      "training_loss": 7.862929821014404
    },
    {
      "epoch": 0.0548509485094851,
      "step": 253,
      "training_loss": 8.378758430480957
    },
    {
      "epoch": 0.0548509485094851,
      "step": 253,
      "training_loss": 8.25780200958252
    },
    {
      "epoch": 0.0548509485094851,
      "step": 253,
      "training_loss": 7.967566967010498
    },
    {
      "epoch": 0.05506775067750678,
      "step": 254,
      "training_loss": 8.222722053527832
    },
    {
      "epoch": 0.05506775067750678,
      "step": 254,
      "training_loss": 7.03478479385376
    },
    {
      "epoch": 0.05506775067750678,
      "step": 254,
      "training_loss": 7.757225513458252
    },
    {
      "epoch": 0.05506775067750678,
      "step": 254,
      "training_loss": 9.479537010192871
    },
    {
      "epoch": 0.055284552845528454,
      "step": 255,
      "training_loss": 8.215774536132812
    },
    {
      "epoch": 0.055284552845528454,
      "step": 255,
      "training_loss": 8.08840560913086
    },
    {
      "epoch": 0.055284552845528454,
      "step": 255,
      "training_loss": 10.228283882141113
    },
    {
      "epoch": 0.055284552845528454,
      "step": 255,
      "training_loss": 7.293333053588867
    },
    {
      "epoch": 0.055501355013550135,
      "grad_norm": 8.850709915161133,
      "learning_rate": 1e-05,
      "loss": 8.3145,
      "step": 256
    },
    {
      "epoch": 0.055501355013550135,
      "step": 256,
      "training_loss": 7.041707992553711
    },
    {
      "epoch": 0.055501355013550135,
      "step": 256,
      "training_loss": 7.700944423675537
    },
    {
      "epoch": 0.055501355013550135,
      "step": 256,
      "training_loss": 8.701765060424805
    },
    {
      "epoch": 0.055501355013550135,
      "step": 256,
      "training_loss": 6.91071081161499
    },
    {
      "epoch": 0.05571815718157182,
      "step": 257,
      "training_loss": 10.052162170410156
    },
    {
      "epoch": 0.05571815718157182,
      "step": 257,
      "training_loss": 9.622241973876953
    },
    {
      "epoch": 0.05571815718157182,
      "step": 257,
      "training_loss": 7.940246105194092
    },
    {
      "epoch": 0.05571815718157182,
      "step": 257,
      "training_loss": 7.633832931518555
    },
    {
      "epoch": 0.0559349593495935,
      "step": 258,
      "training_loss": 8.178584098815918
    },
    {
      "epoch": 0.0559349593495935,
      "step": 258,
      "training_loss": 7.704583168029785
    },
    {
      "epoch": 0.0559349593495935,
      "step": 258,
      "training_loss": 10.439787864685059
    },
    {
      "epoch": 0.0559349593495935,
      "step": 258,
      "training_loss": 9.34423828125
    },
    {
      "epoch": 0.056151761517615174,
      "step": 259,
      "training_loss": 10.4271821975708
    },
    {
      "epoch": 0.056151761517615174,
      "step": 259,
      "training_loss": 8.957082748413086
    },
    {
      "epoch": 0.056151761517615174,
      "step": 259,
      "training_loss": 7.539772987365723
    },
    {
      "epoch": 0.056151761517615174,
      "step": 259,
      "training_loss": 9.143678665161133
    },
    {
      "epoch": 0.056368563685636856,
      "grad_norm": 38.594356536865234,
      "learning_rate": 1e-05,
      "loss": 8.5837,
      "step": 260
    },
    {
      "epoch": 0.056368563685636856,
      "step": 260,
      "training_loss": 8.149690628051758
    },
    {
      "epoch": 0.056368563685636856,
      "step": 260,
      "training_loss": 7.366560459136963
    },
    {
      "epoch": 0.056368563685636856,
      "step": 260,
      "training_loss": 7.213303089141846
    },
    {
      "epoch": 0.056368563685636856,
      "step": 260,
      "training_loss": 8.973657608032227
    },
    {
      "epoch": 0.05658536585365854,
      "step": 261,
      "training_loss": 8.569131851196289
    },
    {
      "epoch": 0.05658536585365854,
      "step": 261,
      "training_loss": 7.117707252502441
    },
    {
      "epoch": 0.05658536585365854,
      "step": 261,
      "training_loss": 7.85753059387207
    },
    {
      "epoch": 0.05658536585365854,
      "step": 261,
      "training_loss": 7.938337802886963
    },
    {
      "epoch": 0.05680216802168022,
      "step": 262,
      "training_loss": 7.262900352478027
    },
    {
      "epoch": 0.05680216802168022,
      "step": 262,
      "training_loss": 8.245474815368652
    },
    {
      "epoch": 0.05680216802168022,
      "step": 262,
      "training_loss": 8.04386043548584
    },
    {
      "epoch": 0.05680216802168022,
      "step": 262,
      "training_loss": 8.338813781738281
    },
    {
      "epoch": 0.057018970189701895,
      "step": 263,
      "training_loss": 9.358623504638672
    },
    {
      "epoch": 0.057018970189701895,
      "step": 263,
      "training_loss": 8.891729354858398
    },
    {
      "epoch": 0.057018970189701895,
      "step": 263,
      "training_loss": 7.804759502410889
    },
    {
      "epoch": 0.057018970189701895,
      "step": 263,
      "training_loss": 7.24848747253418
    },
    {
      "epoch": 0.05723577235772358,
      "grad_norm": 11.721390724182129,
      "learning_rate": 1e-05,
      "loss": 8.0238,
      "step": 264
    },
    {
      "epoch": 0.05723577235772358,
      "step": 264,
      "training_loss": 6.571340084075928
    },
    {
      "epoch": 0.05723577235772358,
      "step": 264,
      "training_loss": 9.408628463745117
    },
    {
      "epoch": 0.05723577235772358,
      "step": 264,
      "training_loss": 7.950092792510986
    },
    {
      "epoch": 0.05723577235772358,
      "step": 264,
      "training_loss": 8.225046157836914
    },
    {
      "epoch": 0.05745257452574526,
      "step": 265,
      "training_loss": 8.839768409729004
    },
    {
      "epoch": 0.05745257452574526,
      "step": 265,
      "training_loss": 9.584136962890625
    },
    {
      "epoch": 0.05745257452574526,
      "step": 265,
      "training_loss": 8.074234008789062
    },
    {
      "epoch": 0.05745257452574526,
      "step": 265,
      "training_loss": 9.556145668029785
    },
    {
      "epoch": 0.05766937669376694,
      "step": 266,
      "training_loss": 9.041784286499023
    },
    {
      "epoch": 0.05766937669376694,
      "step": 266,
      "training_loss": 6.968522071838379
    },
    {
      "epoch": 0.05766937669376694,
      "step": 266,
      "training_loss": 10.353960990905762
    },
    {
      "epoch": 0.05766937669376694,
      "step": 266,
      "training_loss": 6.97034215927124
    },
    {
      "epoch": 0.057886178861788616,
      "step": 267,
      "training_loss": 9.393511772155762
    },
    {
      "epoch": 0.057886178861788616,
      "step": 267,
      "training_loss": 8.863360404968262
    },
    {
      "epoch": 0.057886178861788616,
      "step": 267,
      "training_loss": 9.166291236877441
    },
    {
      "epoch": 0.057886178861788616,
      "step": 267,
      "training_loss": 7.412930011749268
    },
    {
      "epoch": 0.0581029810298103,
      "grad_norm": 11.307465553283691,
      "learning_rate": 1e-05,
      "loss": 8.5238,
      "step": 268
    },
    {
      "epoch": 0.0581029810298103,
      "step": 268,
      "training_loss": 7.6980109214782715
    },
    {
      "epoch": 0.0581029810298103,
      "step": 268,
      "training_loss": 9.004973411560059
    },
    {
      "epoch": 0.0581029810298103,
      "step": 268,
      "training_loss": 8.233198165893555
    },
    {
      "epoch": 0.0581029810298103,
      "step": 268,
      "training_loss": 10.546204566955566
    },
    {
      "epoch": 0.05831978319783198,
      "step": 269,
      "training_loss": 9.321991920471191
    },
    {
      "epoch": 0.05831978319783198,
      "step": 269,
      "training_loss": 8.262923240661621
    },
    {
      "epoch": 0.05831978319783198,
      "step": 269,
      "training_loss": 8.829191207885742
    },
    {
      "epoch": 0.05831978319783198,
      "step": 269,
      "training_loss": 8.149827003479004
    },
    {
      "epoch": 0.05853658536585366,
      "step": 270,
      "training_loss": 8.96813678741455
    },
    {
      "epoch": 0.05853658536585366,
      "step": 270,
      "training_loss": 10.8367280960083
    },
    {
      "epoch": 0.05853658536585366,
      "step": 270,
      "training_loss": 8.708678245544434
    },
    {
      "epoch": 0.05853658536585366,
      "step": 270,
      "training_loss": 7.612382411956787
    },
    {
      "epoch": 0.05875338753387534,
      "step": 271,
      "training_loss": 7.97789192199707
    },
    {
      "epoch": 0.05875338753387534,
      "step": 271,
      "training_loss": 8.204960823059082
    },
    {
      "epoch": 0.05875338753387534,
      "step": 271,
      "training_loss": 9.16907787322998
    },
    {
      "epoch": 0.05875338753387534,
      "step": 271,
      "training_loss": 7.610901832580566
    },
    {
      "epoch": 0.05897018970189702,
      "grad_norm": 7.192559719085693,
      "learning_rate": 1e-05,
      "loss": 8.6959,
      "step": 272
    },
    {
      "epoch": 0.05897018970189702,
      "step": 272,
      "training_loss": 8.211956024169922
    },
    {
      "epoch": 0.05897018970189702,
      "step": 272,
      "training_loss": 7.143931865692139
    },
    {
      "epoch": 0.05897018970189702,
      "step": 272,
      "training_loss": 8.156781196594238
    },
    {
      "epoch": 0.05897018970189702,
      "step": 272,
      "training_loss": 8.081438064575195
    },
    {
      "epoch": 0.0591869918699187,
      "step": 273,
      "training_loss": 7.708268642425537
    },
    {
      "epoch": 0.0591869918699187,
      "step": 273,
      "training_loss": 7.833703517913818
    },
    {
      "epoch": 0.0591869918699187,
      "step": 273,
      "training_loss": 6.895991325378418
    },
    {
      "epoch": 0.0591869918699187,
      "step": 273,
      "training_loss": 8.319313049316406
    },
    {
      "epoch": 0.05940379403794038,
      "step": 274,
      "training_loss": 7.098052978515625
    },
    {
      "epoch": 0.05940379403794038,
      "step": 274,
      "training_loss": 8.255372047424316
    },
    {
      "epoch": 0.05940379403794038,
      "step": 274,
      "training_loss": 9.610295295715332
    },
    {
      "epoch": 0.05940379403794038,
      "step": 274,
      "training_loss": 10.054465293884277
    },
    {
      "epoch": 0.05962059620596206,
      "step": 275,
      "training_loss": 7.725369453430176
    },
    {
      "epoch": 0.05962059620596206,
      "step": 275,
      "training_loss": 9.0997896194458
    },
    {
      "epoch": 0.05962059620596206,
      "step": 275,
      "training_loss": 9.698041915893555
    },
    {
      "epoch": 0.05962059620596206,
      "step": 275,
      "training_loss": 7.835655689239502
    },
    {
      "epoch": 0.05983739837398374,
      "grad_norm": 19.41514778137207,
      "learning_rate": 1e-05,
      "loss": 8.233,
      "step": 276
    },
    {
      "epoch": 0.05983739837398374,
      "step": 276,
      "training_loss": 7.349086761474609
    },
    {
      "epoch": 0.05983739837398374,
      "step": 276,
      "training_loss": 6.936474800109863
    },
    {
      "epoch": 0.05983739837398374,
      "step": 276,
      "training_loss": 8.334657669067383
    },
    {
      "epoch": 0.05983739837398374,
      "step": 276,
      "training_loss": 9.017963409423828
    },
    {
      "epoch": 0.06005420054200542,
      "step": 277,
      "training_loss": 7.852164268493652
    },
    {
      "epoch": 0.06005420054200542,
      "step": 277,
      "training_loss": 9.126914978027344
    },
    {
      "epoch": 0.06005420054200542,
      "step": 277,
      "training_loss": 8.692806243896484
    },
    {
      "epoch": 0.06005420054200542,
      "step": 277,
      "training_loss": 8.533787727355957
    },
    {
      "epoch": 0.060271002710027104,
      "step": 278,
      "training_loss": 8.115591049194336
    },
    {
      "epoch": 0.060271002710027104,
      "step": 278,
      "training_loss": 8.073673248291016
    },
    {
      "epoch": 0.060271002710027104,
      "step": 278,
      "training_loss": 8.061239242553711
    },
    {
      "epoch": 0.060271002710027104,
      "step": 278,
      "training_loss": 8.20637035369873
    },
    {
      "epoch": 0.06048780487804878,
      "step": 279,
      "training_loss": 7.9252729415893555
    },
    {
      "epoch": 0.06048780487804878,
      "step": 279,
      "training_loss": 7.285467147827148
    },
    {
      "epoch": 0.06048780487804878,
      "step": 279,
      "training_loss": 9.436643600463867
    },
    {
      "epoch": 0.06048780487804878,
      "step": 279,
      "training_loss": 8.289361953735352
    },
    {
      "epoch": 0.06070460704607046,
      "grad_norm": 9.630855560302734,
      "learning_rate": 1e-05,
      "loss": 8.2023,
      "step": 280
    },
    {
      "epoch": 0.06070460704607046,
      "step": 280,
      "training_loss": 10.012991905212402
    },
    {
      "epoch": 0.06070460704607046,
      "step": 280,
      "training_loss": 9.455740928649902
    },
    {
      "epoch": 0.06070460704607046,
      "step": 280,
      "training_loss": 10.257798194885254
    },
    {
      "epoch": 0.06070460704607046,
      "step": 280,
      "training_loss": 7.811270713806152
    },
    {
      "epoch": 0.06092140921409214,
      "step": 281,
      "training_loss": 7.397870063781738
    },
    {
      "epoch": 0.06092140921409214,
      "step": 281,
      "training_loss": 6.418354034423828
    },
    {
      "epoch": 0.06092140921409214,
      "step": 281,
      "training_loss": 7.796180248260498
    },
    {
      "epoch": 0.06092140921409214,
      "step": 281,
      "training_loss": 8.585938453674316
    },
    {
      "epoch": 0.061138211382113825,
      "step": 282,
      "training_loss": 7.678459644317627
    },
    {
      "epoch": 0.061138211382113825,
      "step": 282,
      "training_loss": 7.226465225219727
    },
    {
      "epoch": 0.061138211382113825,
      "step": 282,
      "training_loss": 8.83869457244873
    },
    {
      "epoch": 0.061138211382113825,
      "step": 282,
      "training_loss": 7.971148490905762
    },
    {
      "epoch": 0.0613550135501355,
      "step": 283,
      "training_loss": 8.321772575378418
    },
    {
      "epoch": 0.0613550135501355,
      "step": 283,
      "training_loss": 7.98628568649292
    },
    {
      "epoch": 0.0613550135501355,
      "step": 283,
      "training_loss": 8.684231758117676
    },
    {
      "epoch": 0.0613550135501355,
      "step": 283,
      "training_loss": 7.382626056671143
    },
    {
      "epoch": 0.06157181571815718,
      "grad_norm": 6.444254398345947,
      "learning_rate": 1e-05,
      "loss": 8.2391,
      "step": 284
    },
    {
      "epoch": 0.06157181571815718,
      "step": 284,
      "training_loss": 8.660303115844727
    },
    {
      "epoch": 0.06157181571815718,
      "step": 284,
      "training_loss": 8.256440162658691
    },
    {
      "epoch": 0.06157181571815718,
      "step": 284,
      "training_loss": 7.905787467956543
    },
    {
      "epoch": 0.06157181571815718,
      "step": 284,
      "training_loss": 7.719996452331543
    },
    {
      "epoch": 0.061788617886178863,
      "step": 285,
      "training_loss": 7.996413707733154
    },
    {
      "epoch": 0.061788617886178863,
      "step": 285,
      "training_loss": 7.702603340148926
    },
    {
      "epoch": 0.061788617886178863,
      "step": 285,
      "training_loss": 8.574524879455566
    },
    {
      "epoch": 0.061788617886178863,
      "step": 285,
      "training_loss": 7.450150966644287
    },
    {
      "epoch": 0.062005420054200545,
      "step": 286,
      "training_loss": 8.744946479797363
    },
    {
      "epoch": 0.062005420054200545,
      "step": 286,
      "training_loss": 8.993914604187012
    },
    {
      "epoch": 0.062005420054200545,
      "step": 286,
      "training_loss": 8.456974983215332
    },
    {
      "epoch": 0.062005420054200545,
      "step": 286,
      "training_loss": 8.539258003234863
    },
    {
      "epoch": 0.06222222222222222,
      "step": 287,
      "training_loss": 9.539827346801758
    },
    {
      "epoch": 0.06222222222222222,
      "step": 287,
      "training_loss": 8.30477237701416
    },
    {
      "epoch": 0.06222222222222222,
      "step": 287,
      "training_loss": 8.330719947814941
    },
    {
      "epoch": 0.06222222222222222,
      "step": 287,
      "training_loss": 9.51285171508789
    },
    {
      "epoch": 0.0624390243902439,
      "grad_norm": 10.49563980102539,
      "learning_rate": 1e-05,
      "loss": 8.4181,
      "step": 288
    },
    {
      "epoch": 0.0624390243902439,
      "step": 288,
      "training_loss": 6.713016033172607
    },
    {
      "epoch": 0.0624390243902439,
      "step": 288,
      "training_loss": 7.813157081604004
    },
    {
      "epoch": 0.0624390243902439,
      "step": 288,
      "training_loss": 8.828655242919922
    },
    {
      "epoch": 0.0624390243902439,
      "step": 288,
      "training_loss": 8.889424324035645
    },
    {
      "epoch": 0.06265582655826558,
      "step": 289,
      "training_loss": 8.16108226776123
    },
    {
      "epoch": 0.06265582655826558,
      "step": 289,
      "training_loss": 8.891965866088867
    },
    {
      "epoch": 0.06265582655826558,
      "step": 289,
      "training_loss": 8.412369728088379
    },
    {
      "epoch": 0.06265582655826558,
      "step": 289,
      "training_loss": 9.242706298828125
    },
    {
      "epoch": 0.06287262872628727,
      "step": 290,
      "training_loss": 7.443171977996826
    },
    {
      "epoch": 0.06287262872628727,
      "step": 290,
      "training_loss": 5.8830485343933105
    },
    {
      "epoch": 0.06287262872628727,
      "step": 290,
      "training_loss": 7.293688774108887
    },
    {
      "epoch": 0.06287262872628727,
      "step": 290,
      "training_loss": 8.267038345336914
    },
    {
      "epoch": 0.06308943089430895,
      "step": 291,
      "training_loss": 9.025017738342285
    },
    {
      "epoch": 0.06308943089430895,
      "step": 291,
      "training_loss": 7.4852495193481445
    },
    {
      "epoch": 0.06308943089430895,
      "step": 291,
      "training_loss": 6.731475830078125
    },
    {
      "epoch": 0.06308943089430895,
      "step": 291,
      "training_loss": 9.166031837463379
    },
    {
      "epoch": 0.06330623306233063,
      "grad_norm": 26.53466796875,
      "learning_rate": 1e-05,
      "loss": 8.0154,
      "step": 292
    },
    {
      "epoch": 0.06330623306233063,
      "step": 292,
      "training_loss": 8.389669418334961
    },
    {
      "epoch": 0.06330623306233063,
      "step": 292,
      "training_loss": 7.2123308181762695
    },
    {
      "epoch": 0.06330623306233063,
      "step": 292,
      "training_loss": 9.341156005859375
    },
    {
      "epoch": 0.06330623306233063,
      "step": 292,
      "training_loss": 6.903926849365234
    },
    {
      "epoch": 0.0635230352303523,
      "step": 293,
      "training_loss": 8.49953842163086
    },
    {
      "epoch": 0.0635230352303523,
      "step": 293,
      "training_loss": 8.782790184020996
    },
    {
      "epoch": 0.0635230352303523,
      "step": 293,
      "training_loss": 7.926547527313232
    },
    {
      "epoch": 0.0635230352303523,
      "step": 293,
      "training_loss": 8.60677433013916
    },
    {
      "epoch": 0.06373983739837398,
      "step": 294,
      "training_loss": 7.980974197387695
    },
    {
      "epoch": 0.06373983739837398,
      "step": 294,
      "training_loss": 9.126267433166504
    },
    {
      "epoch": 0.06373983739837398,
      "step": 294,
      "training_loss": 7.926476001739502
    },
    {
      "epoch": 0.06373983739837398,
      "step": 294,
      "training_loss": 7.389068603515625
    },
    {
      "epoch": 0.06395663956639566,
      "step": 295,
      "training_loss": 7.144017219543457
    },
    {
      "epoch": 0.06395663956639566,
      "step": 295,
      "training_loss": 8.55170726776123
    },
    {
      "epoch": 0.06395663956639566,
      "step": 295,
      "training_loss": 8.619813919067383
    },
    {
      "epoch": 0.06395663956639566,
      "step": 295,
      "training_loss": 7.3454813957214355
    },
    {
      "epoch": 0.06417344173441734,
      "grad_norm": 6.517373085021973,
      "learning_rate": 1e-05,
      "loss": 8.1092,
      "step": 296
    },
    {
      "epoch": 0.06417344173441734,
      "step": 296,
      "training_loss": 8.87939453125
    },
    {
      "epoch": 0.06417344173441734,
      "step": 296,
      "training_loss": 7.502101421356201
    },
    {
      "epoch": 0.06417344173441734,
      "step": 296,
      "training_loss": 8.395060539245605
    },
    {
      "epoch": 0.06417344173441734,
      "step": 296,
      "training_loss": 8.441081047058105
    },
    {
      "epoch": 0.06439024390243903,
      "step": 297,
      "training_loss": 7.889405727386475
    },
    {
      "epoch": 0.06439024390243903,
      "step": 297,
      "training_loss": 7.735417366027832
    },
    {
      "epoch": 0.06439024390243903,
      "step": 297,
      "training_loss": 7.449061870574951
    },
    {
      "epoch": 0.06439024390243903,
      "step": 297,
      "training_loss": 8.367488861083984
    },
    {
      "epoch": 0.06460704607046071,
      "step": 298,
      "training_loss": 7.981578350067139
    },
    {
      "epoch": 0.06460704607046071,
      "step": 298,
      "training_loss": 6.923366069793701
    },
    {
      "epoch": 0.06460704607046071,
      "step": 298,
      "training_loss": 8.064813613891602
    },
    {
      "epoch": 0.06460704607046071,
      "step": 298,
      "training_loss": 7.18273401260376
    },
    {
      "epoch": 0.06482384823848239,
      "step": 299,
      "training_loss": 8.516828536987305
    },
    {
      "epoch": 0.06482384823848239,
      "step": 299,
      "training_loss": 7.837547302246094
    },
    {
      "epoch": 0.06482384823848239,
      "step": 299,
      "training_loss": 7.324316024780273
    },
    {
      "epoch": 0.06482384823848239,
      "step": 299,
      "training_loss": 7.213824272155762
    },
    {
      "epoch": 0.06504065040650407,
      "grad_norm": 14.532815933227539,
      "learning_rate": 1e-05,
      "loss": 7.8565,
      "step": 300
    },
    {
      "epoch": 0.06504065040650407,
      "step": 300,
      "training_loss": 8.252074241638184
    },
    {
      "epoch": 0.06504065040650407,
      "step": 300,
      "training_loss": 7.6087260246276855
    },
    {
      "epoch": 0.06504065040650407,
      "step": 300,
      "training_loss": 6.487219333648682
    },
    {
      "epoch": 0.06504065040650407,
      "step": 300,
      "training_loss": 6.677879810333252
    },
    {
      "epoch": 0.06525745257452574,
      "step": 301,
      "training_loss": 7.815026760101318
    },
    {
      "epoch": 0.06525745257452574,
      "step": 301,
      "training_loss": 9.154719352722168
    },
    {
      "epoch": 0.06525745257452574,
      "step": 301,
      "training_loss": 7.915988445281982
    },
    {
      "epoch": 0.06525745257452574,
      "step": 301,
      "training_loss": 8.312129020690918
    },
    {
      "epoch": 0.06547425474254742,
      "step": 302,
      "training_loss": 10.032352447509766
    },
    {
      "epoch": 0.06547425474254742,
      "step": 302,
      "training_loss": 7.317715167999268
    },
    {
      "epoch": 0.06547425474254742,
      "step": 302,
      "training_loss": 8.003211975097656
    },
    {
      "epoch": 0.06547425474254742,
      "step": 302,
      "training_loss": 8.646702766418457
    },
    {
      "epoch": 0.0656910569105691,
      "step": 303,
      "training_loss": 7.280012607574463
    },
    {
      "epoch": 0.0656910569105691,
      "step": 303,
      "training_loss": 7.074512481689453
    },
    {
      "epoch": 0.0656910569105691,
      "step": 303,
      "training_loss": 8.770028114318848
    },
    {
      "epoch": 0.0656910569105691,
      "step": 303,
      "training_loss": 8.169517517089844
    },
    {
      "epoch": 0.06590785907859079,
      "grad_norm": 6.991789817810059,
      "learning_rate": 1e-05,
      "loss": 7.9699,
      "step": 304
    },
    {
      "epoch": 0.06590785907859079,
      "step": 304,
      "training_loss": 7.910707950592041
    },
    {
      "epoch": 0.06590785907859079,
      "step": 304,
      "training_loss": 8.851441383361816
    },
    {
      "epoch": 0.06590785907859079,
      "step": 304,
      "training_loss": 8.706559181213379
    },
    {
      "epoch": 0.06590785907859079,
      "step": 304,
      "training_loss": 6.3202643394470215
    },
    {
      "epoch": 0.06612466124661247,
      "step": 305,
      "training_loss": 7.877843379974365
    },
    {
      "epoch": 0.06612466124661247,
      "step": 305,
      "training_loss": 9.840760231018066
    },
    {
      "epoch": 0.06612466124661247,
      "step": 305,
      "training_loss": 8.974129676818848
    },
    {
      "epoch": 0.06612466124661247,
      "step": 305,
      "training_loss": 7.76145076751709
    },
    {
      "epoch": 0.06634146341463415,
      "step": 306,
      "training_loss": 7.7866973876953125
    },
    {
      "epoch": 0.06634146341463415,
      "step": 306,
      "training_loss": 8.774632453918457
    },
    {
      "epoch": 0.06634146341463415,
      "step": 306,
      "training_loss": 6.816519260406494
    },
    {
      "epoch": 0.06634146341463415,
      "step": 306,
      "training_loss": 8.052779197692871
    },
    {
      "epoch": 0.06655826558265583,
      "step": 307,
      "training_loss": 8.340723991394043
    },
    {
      "epoch": 0.06655826558265583,
      "step": 307,
      "training_loss": 10.074390411376953
    },
    {
      "epoch": 0.06655826558265583,
      "step": 307,
      "training_loss": 8.010220527648926
    },
    {
      "epoch": 0.06655826558265583,
      "step": 307,
      "training_loss": 7.8430352210998535
    },
    {
      "epoch": 0.06677506775067751,
      "grad_norm": 14.068233489990234,
      "learning_rate": 1e-05,
      "loss": 8.2464,
      "step": 308
    },
    {
      "epoch": 0.06677506775067751,
      "step": 308,
      "training_loss": 6.388254642486572
    },
    {
      "epoch": 0.06677506775067751,
      "step": 308,
      "training_loss": 7.852964401245117
    },
    {
      "epoch": 0.06677506775067751,
      "step": 308,
      "training_loss": 7.737032413482666
    },
    {
      "epoch": 0.06677506775067751,
      "step": 308,
      "training_loss": 7.142114639282227
    },
    {
      "epoch": 0.06699186991869918,
      "step": 309,
      "training_loss": 7.610208511352539
    },
    {
      "epoch": 0.06699186991869918,
      "step": 309,
      "training_loss": 7.9658942222595215
    },
    {
      "epoch": 0.06699186991869918,
      "step": 309,
      "training_loss": 6.273921489715576
    },
    {
      "epoch": 0.06699186991869918,
      "step": 309,
      "training_loss": 8.789116859436035
    },
    {
      "epoch": 0.06720867208672086,
      "step": 310,
      "training_loss": 8.110419273376465
    },
    {
      "epoch": 0.06720867208672086,
      "step": 310,
      "training_loss": 7.327016830444336
    },
    {
      "epoch": 0.06720867208672086,
      "step": 310,
      "training_loss": 7.846200942993164
    },
    {
      "epoch": 0.06720867208672086,
      "step": 310,
      "training_loss": 7.474301338195801
    },
    {
      "epoch": 0.06742547425474255,
      "step": 311,
      "training_loss": 8.066723823547363
    },
    {
      "epoch": 0.06742547425474255,
      "step": 311,
      "training_loss": 7.077022075653076
    },
    {
      "epoch": 0.06742547425474255,
      "step": 311,
      "training_loss": 7.682167053222656
    },
    {
      "epoch": 0.06742547425474255,
      "step": 311,
      "training_loss": 7.381542205810547
    },
    {
      "epoch": 0.06764227642276423,
      "grad_norm": 9.920146942138672,
      "learning_rate": 1e-05,
      "loss": 7.5453,
      "step": 312
    },
    {
      "epoch": 0.06764227642276423,
      "step": 312,
      "training_loss": 8.33939266204834
    },
    {
      "epoch": 0.06764227642276423,
      "step": 312,
      "training_loss": 6.895317554473877
    },
    {
      "epoch": 0.06764227642276423,
      "step": 312,
      "training_loss": 8.396190643310547
    },
    {
      "epoch": 0.06764227642276423,
      "step": 312,
      "training_loss": 7.400044918060303
    },
    {
      "epoch": 0.06785907859078591,
      "step": 313,
      "training_loss": 8.516083717346191
    },
    {
      "epoch": 0.06785907859078591,
      "step": 313,
      "training_loss": 8.807485580444336
    },
    {
      "epoch": 0.06785907859078591,
      "step": 313,
      "training_loss": 8.003668785095215
    },
    {
      "epoch": 0.06785907859078591,
      "step": 313,
      "training_loss": 7.768014430999756
    },
    {
      "epoch": 0.06807588075880759,
      "step": 314,
      "training_loss": 7.698553085327148
    },
    {
      "epoch": 0.06807588075880759,
      "step": 314,
      "training_loss": 8.067449569702148
    },
    {
      "epoch": 0.06807588075880759,
      "step": 314,
      "training_loss": 7.569189071655273
    },
    {
      "epoch": 0.06807588075880759,
      "step": 314,
      "training_loss": 6.929469108581543
    },
    {
      "epoch": 0.06829268292682927,
      "step": 315,
      "training_loss": 7.595730304718018
    },
    {
      "epoch": 0.06829268292682927,
      "step": 315,
      "training_loss": 8.766096115112305
    },
    {
      "epoch": 0.06829268292682927,
      "step": 315,
      "training_loss": 8.365006446838379
    },
    {
      "epoch": 0.06829268292682927,
      "step": 315,
      "training_loss": 8.457813262939453
    },
    {
      "epoch": 0.06850948509485096,
      "grad_norm": 13.564172744750977,
      "learning_rate": 1e-05,
      "loss": 7.9735,
      "step": 316
    },
    {
      "epoch": 0.06850948509485096,
      "step": 316,
      "training_loss": 7.869708061218262
    },
    {
      "epoch": 0.06850948509485096,
      "step": 316,
      "training_loss": 8.249859809875488
    },
    {
      "epoch": 0.06850948509485096,
      "step": 316,
      "training_loss": 7.330406188964844
    },
    {
      "epoch": 0.06850948509485096,
      "step": 316,
      "training_loss": 8.050220489501953
    },
    {
      "epoch": 0.06872628726287262,
      "step": 317,
      "training_loss": 9.197529792785645
    },
    {
      "epoch": 0.06872628726287262,
      "step": 317,
      "training_loss": 7.8260393142700195
    },
    {
      "epoch": 0.06872628726287262,
      "step": 317,
      "training_loss": 6.945930004119873
    },
    {
      "epoch": 0.06872628726287262,
      "step": 317,
      "training_loss": 6.705056190490723
    },
    {
      "epoch": 0.0689430894308943,
      "step": 318,
      "training_loss": 6.9135870933532715
    },
    {
      "epoch": 0.0689430894308943,
      "step": 318,
      "training_loss": 8.502147674560547
    },
    {
      "epoch": 0.0689430894308943,
      "step": 318,
      "training_loss": 7.929758071899414
    },
    {
      "epoch": 0.0689430894308943,
      "step": 318,
      "training_loss": 6.8894548416137695
    },
    {
      "epoch": 0.06915989159891599,
      "step": 319,
      "training_loss": 7.837393283843994
    },
    {
      "epoch": 0.06915989159891599,
      "step": 319,
      "training_loss": 6.301910400390625
    },
    {
      "epoch": 0.06915989159891599,
      "step": 319,
      "training_loss": 8.650349617004395
    },
    {
      "epoch": 0.06915989159891599,
      "step": 319,
      "training_loss": 8.492217063903809
    },
    {
      "epoch": 0.06937669376693767,
      "grad_norm": 6.831161975860596,
      "learning_rate": 1e-05,
      "loss": 7.7307,
      "step": 320
    },
    {
      "epoch": 0.06937669376693767,
      "step": 320,
      "training_loss": 7.002568244934082
    },
    {
      "epoch": 0.06937669376693767,
      "step": 320,
      "training_loss": 7.825201988220215
    },
    {
      "epoch": 0.06937669376693767,
      "step": 320,
      "training_loss": 7.719254493713379
    },
    {
      "epoch": 0.06937669376693767,
      "step": 320,
      "training_loss": 8.011031150817871
    },
    {
      "epoch": 0.06959349593495935,
      "step": 321,
      "training_loss": 9.093493461608887
    },
    {
      "epoch": 0.06959349593495935,
      "step": 321,
      "training_loss": 6.611315727233887
    },
    {
      "epoch": 0.06959349593495935,
      "step": 321,
      "training_loss": 7.274296760559082
    },
    {
      "epoch": 0.06959349593495935,
      "step": 321,
      "training_loss": 7.970090866088867
    },
    {
      "epoch": 0.06981029810298103,
      "step": 322,
      "training_loss": 7.148209571838379
    },
    {
      "epoch": 0.06981029810298103,
      "step": 322,
      "training_loss": 7.954981327056885
    },
    {
      "epoch": 0.06981029810298103,
      "step": 322,
      "training_loss": 8.5009126663208
    },
    {
      "epoch": 0.06981029810298103,
      "step": 322,
      "training_loss": 7.443122386932373
    },
    {
      "epoch": 0.07002710027100272,
      "step": 323,
      "training_loss": 7.160701274871826
    },
    {
      "epoch": 0.07002710027100272,
      "step": 323,
      "training_loss": 7.467447757720947
    },
    {
      "epoch": 0.07002710027100272,
      "step": 323,
      "training_loss": 8.878803253173828
    },
    {
      "epoch": 0.07002710027100272,
      "step": 323,
      "training_loss": 7.319275856018066
    },
    {
      "epoch": 0.0702439024390244,
      "grad_norm": 12.464693069458008,
      "learning_rate": 1e-05,
      "loss": 7.7113,
      "step": 324
    },
    {
      "epoch": 0.0702439024390244,
      "step": 324,
      "training_loss": 8.202373504638672
    },
    {
      "epoch": 0.0702439024390244,
      "step": 324,
      "training_loss": 8.009256362915039
    },
    {
      "epoch": 0.0702439024390244,
      "step": 324,
      "training_loss": 7.268817901611328
    },
    {
      "epoch": 0.0702439024390244,
      "step": 324,
      "training_loss": 6.240869998931885
    },
    {
      "epoch": 0.07046070460704607,
      "step": 325,
      "training_loss": 8.876509666442871
    },
    {
      "epoch": 0.07046070460704607,
      "step": 325,
      "training_loss": 8.650528907775879
    },
    {
      "epoch": 0.07046070460704607,
      "step": 325,
      "training_loss": 7.716962814331055
    },
    {
      "epoch": 0.07046070460704607,
      "step": 325,
      "training_loss": 8.397171974182129
    },
    {
      "epoch": 0.07067750677506775,
      "step": 326,
      "training_loss": 7.445191383361816
    },
    {
      "epoch": 0.07067750677506775,
      "step": 326,
      "training_loss": 7.716911792755127
    },
    {
      "epoch": 0.07067750677506775,
      "step": 326,
      "training_loss": 8.72461223602295
    },
    {
      "epoch": 0.07067750677506775,
      "step": 326,
      "training_loss": 5.966598033905029
    },
    {
      "epoch": 0.07089430894308943,
      "step": 327,
      "training_loss": 6.920076847076416
    },
    {
      "epoch": 0.07089430894308943,
      "step": 327,
      "training_loss": 6.894033908843994
    },
    {
      "epoch": 0.07089430894308943,
      "step": 327,
      "training_loss": 7.85788106918335
    },
    {
      "epoch": 0.07089430894308943,
      "step": 327,
      "training_loss": 7.540774822235107
    },
    {
      "epoch": 0.07111111111111111,
      "grad_norm": 7.278809070587158,
      "learning_rate": 1e-05,
      "loss": 7.6518,
      "step": 328
    },
    {
      "epoch": 0.07111111111111111,
      "step": 328,
      "training_loss": 7.3588995933532715
    },
    {
      "epoch": 0.07111111111111111,
      "step": 328,
      "training_loss": 7.230800151824951
    },
    {
      "epoch": 0.07111111111111111,
      "step": 328,
      "training_loss": 9.12582015991211
    },
    {
      "epoch": 0.07111111111111111,
      "step": 328,
      "training_loss": 6.536040306091309
    },
    {
      "epoch": 0.07132791327913279,
      "step": 329,
      "training_loss": 7.049560070037842
    },
    {
      "epoch": 0.07132791327913279,
      "step": 329,
      "training_loss": 7.585072040557861
    },
    {
      "epoch": 0.07132791327913279,
      "step": 329,
      "training_loss": 6.499294757843018
    },
    {
      "epoch": 0.07132791327913279,
      "step": 329,
      "training_loss": 7.185570240020752
    },
    {
      "epoch": 0.07154471544715447,
      "step": 330,
      "training_loss": 8.560257911682129
    },
    {
      "epoch": 0.07154471544715447,
      "step": 330,
      "training_loss": 7.9662089347839355
    },
    {
      "epoch": 0.07154471544715447,
      "step": 330,
      "training_loss": 7.9150896072387695
    },
    {
      "epoch": 0.07154471544715447,
      "step": 330,
      "training_loss": 7.782442092895508
    },
    {
      "epoch": 0.07176151761517616,
      "step": 331,
      "training_loss": 6.808966636657715
    },
    {
      "epoch": 0.07176151761517616,
      "step": 331,
      "training_loss": 7.0312652587890625
    },
    {
      "epoch": 0.07176151761517616,
      "step": 331,
      "training_loss": 8.256035804748535
    },
    {
      "epoch": 0.07176151761517616,
      "step": 331,
      "training_loss": 5.9824957847595215
    },
    {
      "epoch": 0.07197831978319784,
      "grad_norm": 27.485139846801758,
      "learning_rate": 1e-05,
      "loss": 7.4296,
      "step": 332
    },
    {
      "epoch": 0.07197831978319784,
      "step": 332,
      "training_loss": 7.996812343597412
    },
    {
      "epoch": 0.07197831978319784,
      "step": 332,
      "training_loss": 8.52172565460205
    },
    {
      "epoch": 0.07197831978319784,
      "step": 332,
      "training_loss": 6.970703601837158
    },
    {
      "epoch": 0.07197831978319784,
      "step": 332,
      "training_loss": 8.100028991699219
    },
    {
      "epoch": 0.0721951219512195,
      "step": 333,
      "training_loss": 6.653751850128174
    },
    {
      "epoch": 0.0721951219512195,
      "step": 333,
      "training_loss": 6.863412857055664
    },
    {
      "epoch": 0.0721951219512195,
      "step": 333,
      "training_loss": 8.512393951416016
    },
    {
      "epoch": 0.0721951219512195,
      "step": 333,
      "training_loss": 8.101210594177246
    },
    {
      "epoch": 0.07241192411924119,
      "step": 334,
      "training_loss": 8.219964981079102
    },
    {
      "epoch": 0.07241192411924119,
      "step": 334,
      "training_loss": 8.555787086486816
    },
    {
      "epoch": 0.07241192411924119,
      "step": 334,
      "training_loss": 6.899551868438721
    },
    {
      "epoch": 0.07241192411924119,
      "step": 334,
      "training_loss": 8.902730941772461
    },
    {
      "epoch": 0.07262872628726287,
      "step": 335,
      "training_loss": 8.157779693603516
    },
    {
      "epoch": 0.07262872628726287,
      "step": 335,
      "training_loss": 7.2505292892456055
    },
    {
      "epoch": 0.07262872628726287,
      "step": 335,
      "training_loss": 8.7435941696167
    },
    {
      "epoch": 0.07262872628726287,
      "step": 335,
      "training_loss": 8.325323104858398
    },
    {
      "epoch": 0.07284552845528455,
      "grad_norm": 9.657209396362305,
      "learning_rate": 1e-05,
      "loss": 7.9235,
      "step": 336
    },
    {
      "epoch": 0.07284552845528455,
      "step": 336,
      "training_loss": 10.248441696166992
    },
    {
      "epoch": 0.07284552845528455,
      "step": 336,
      "training_loss": 9.168166160583496
    },
    {
      "epoch": 0.07284552845528455,
      "step": 336,
      "training_loss": 6.4800028800964355
    },
    {
      "epoch": 0.07284552845528455,
      "step": 336,
      "training_loss": 7.3843536376953125
    },
    {
      "epoch": 0.07306233062330623,
      "step": 337,
      "training_loss": 7.292364597320557
    },
    {
      "epoch": 0.07306233062330623,
      "step": 337,
      "training_loss": 7.466684341430664
    },
    {
      "epoch": 0.07306233062330623,
      "step": 337,
      "training_loss": 8.085734367370605
    },
    {
      "epoch": 0.07306233062330623,
      "step": 337,
      "training_loss": 6.877345085144043
    },
    {
      "epoch": 0.07327913279132792,
      "step": 338,
      "training_loss": 7.6778740882873535
    },
    {
      "epoch": 0.07327913279132792,
      "step": 338,
      "training_loss": 8.524276733398438
    },
    {
      "epoch": 0.07327913279132792,
      "step": 338,
      "training_loss": 5.903565883636475
    },
    {
      "epoch": 0.07327913279132792,
      "step": 338,
      "training_loss": 8.051684379577637
    },
    {
      "epoch": 0.0734959349593496,
      "step": 339,
      "training_loss": 8.170751571655273
    },
    {
      "epoch": 0.0734959349593496,
      "step": 339,
      "training_loss": 8.803227424621582
    },
    {
      "epoch": 0.0734959349593496,
      "step": 339,
      "training_loss": 6.672534465789795
    },
    {
      "epoch": 0.0734959349593496,
      "step": 339,
      "training_loss": 6.787819862365723
    },
    {
      "epoch": 0.07371273712737128,
      "grad_norm": 11.158797264099121,
      "learning_rate": 1e-05,
      "loss": 7.7247,
      "step": 340
    },
    {
      "epoch": 0.07371273712737128,
      "step": 340,
      "training_loss": 8.05754566192627
    },
    {
      "epoch": 0.07371273712737128,
      "step": 340,
      "training_loss": 7.328925609588623
    },
    {
      "epoch": 0.07371273712737128,
      "step": 340,
      "training_loss": 7.325913429260254
    },
    {
      "epoch": 0.07371273712737128,
      "step": 340,
      "training_loss": 10.541116714477539
    },
    {
      "epoch": 0.07392953929539295,
      "step": 341,
      "training_loss": 7.610118865966797
    },
    {
      "epoch": 0.07392953929539295,
      "step": 341,
      "training_loss": 7.334056854248047
    },
    {
      "epoch": 0.07392953929539295,
      "step": 341,
      "training_loss": 7.565546035766602
    },
    {
      "epoch": 0.07392953929539295,
      "step": 341,
      "training_loss": 8.530682563781738
    },
    {
      "epoch": 0.07414634146341463,
      "step": 342,
      "training_loss": 7.741385459899902
    },
    {
      "epoch": 0.07414634146341463,
      "step": 342,
      "training_loss": 8.623273849487305
    },
    {
      "epoch": 0.07414634146341463,
      "step": 342,
      "training_loss": 8.464166641235352
    },
    {
      "epoch": 0.07414634146341463,
      "step": 342,
      "training_loss": 7.518078327178955
    },
    {
      "epoch": 0.07436314363143631,
      "step": 343,
      "training_loss": 8.212327003479004
    },
    {
      "epoch": 0.07436314363143631,
      "step": 343,
      "training_loss": 8.203678131103516
    },
    {
      "epoch": 0.07436314363143631,
      "step": 343,
      "training_loss": 6.548776149749756
    },
    {
      "epoch": 0.07436314363143631,
      "step": 343,
      "training_loss": 8.018510818481445
    },
    {
      "epoch": 0.074579945799458,
      "grad_norm": 12.790303230285645,
      "learning_rate": 1e-05,
      "loss": 7.9765,
      "step": 344
    },
    {
      "epoch": 0.074579945799458,
      "step": 344,
      "training_loss": 6.895763397216797
    },
    {
      "epoch": 0.074579945799458,
      "step": 344,
      "training_loss": 6.724978923797607
    },
    {
      "epoch": 0.074579945799458,
      "step": 344,
      "training_loss": 7.96275520324707
    },
    {
      "epoch": 0.074579945799458,
      "step": 344,
      "training_loss": 7.274966239929199
    },
    {
      "epoch": 0.07479674796747968,
      "step": 345,
      "training_loss": 10.541850090026855
    },
    {
      "epoch": 0.07479674796747968,
      "step": 345,
      "training_loss": 8.429983139038086
    },
    {
      "epoch": 0.07479674796747968,
      "step": 345,
      "training_loss": 6.944403171539307
    },
    {
      "epoch": 0.07479674796747968,
      "step": 345,
      "training_loss": 7.4602251052856445
    },
    {
      "epoch": 0.07501355013550136,
      "step": 346,
      "training_loss": 7.646166801452637
    },
    {
      "epoch": 0.07501355013550136,
      "step": 346,
      "training_loss": 8.556116104125977
    },
    {
      "epoch": 0.07501355013550136,
      "step": 346,
      "training_loss": 7.953361988067627
    },
    {
      "epoch": 0.07501355013550136,
      "step": 346,
      "training_loss": 10.278975486755371
    },
    {
      "epoch": 0.07523035230352304,
      "step": 347,
      "training_loss": 7.50942325592041
    },
    {
      "epoch": 0.07523035230352304,
      "step": 347,
      "training_loss": 8.2050199508667
    },
    {
      "epoch": 0.07523035230352304,
      "step": 347,
      "training_loss": 8.435586929321289
    },
    {
      "epoch": 0.07523035230352304,
      "step": 347,
      "training_loss": 7.581918239593506
    },
    {
      "epoch": 0.07544715447154472,
      "grad_norm": 8.359407424926758,
      "learning_rate": 1e-05,
      "loss": 8.0251,
      "step": 348
    },
    {
      "epoch": 0.07544715447154472,
      "step": 348,
      "training_loss": 7.187657356262207
    },
    {
      "epoch": 0.07544715447154472,
      "step": 348,
      "training_loss": 8.220602989196777
    },
    {
      "epoch": 0.07544715447154472,
      "step": 348,
      "training_loss": 7.714519023895264
    },
    {
      "epoch": 0.07544715447154472,
      "step": 348,
      "training_loss": 6.87969446182251
    },
    {
      "epoch": 0.07566395663956639,
      "step": 349,
      "training_loss": 5.847953796386719
    },
    {
      "epoch": 0.07566395663956639,
      "step": 349,
      "training_loss": 7.259443283081055
    },
    {
      "epoch": 0.07566395663956639,
      "step": 349,
      "training_loss": 7.641909599304199
    },
    {
      "epoch": 0.07566395663956639,
      "step": 349,
      "training_loss": 7.88498067855835
    },
    {
      "epoch": 0.07588075880758807,
      "step": 350,
      "training_loss": 7.256746768951416
    },
    {
      "epoch": 0.07588075880758807,
      "step": 350,
      "training_loss": 8.373141288757324
    },
    {
      "epoch": 0.07588075880758807,
      "step": 350,
      "training_loss": 7.68518590927124
    },
    {
      "epoch": 0.07588075880758807,
      "step": 350,
      "training_loss": 6.997099876403809
    },
    {
      "epoch": 0.07609756097560975,
      "step": 351,
      "training_loss": 8.21064281463623
    },
    {
      "epoch": 0.07609756097560975,
      "step": 351,
      "training_loss": 6.817462921142578
    },
    {
      "epoch": 0.07609756097560975,
      "step": 351,
      "training_loss": 8.52826976776123
    },
    {
      "epoch": 0.07609756097560975,
      "step": 351,
      "training_loss": 7.30765962600708
    },
    {
      "epoch": 0.07631436314363144,
      "grad_norm": 8.219947814941406,
      "learning_rate": 1e-05,
      "loss": 7.4883,
      "step": 352
    },
    {
      "epoch": 0.07631436314363144,
      "step": 352,
      "training_loss": 7.689026355743408
    },
    {
      "epoch": 0.07631436314363144,
      "step": 352,
      "training_loss": 7.617109775543213
    },
    {
      "epoch": 0.07631436314363144,
      "step": 352,
      "training_loss": 7.739031791687012
    },
    {
      "epoch": 0.07631436314363144,
      "step": 352,
      "training_loss": 7.603020668029785
    },
    {
      "epoch": 0.07653116531165312,
      "step": 353,
      "training_loss": 6.514982223510742
    },
    {
      "epoch": 0.07653116531165312,
      "step": 353,
      "training_loss": 7.027559280395508
    },
    {
      "epoch": 0.07653116531165312,
      "step": 353,
      "training_loss": 7.732449054718018
    },
    {
      "epoch": 0.07653116531165312,
      "step": 353,
      "training_loss": 8.451412200927734
    },
    {
      "epoch": 0.0767479674796748,
      "step": 354,
      "training_loss": 7.511340141296387
    },
    {
      "epoch": 0.0767479674796748,
      "step": 354,
      "training_loss": 8.37960147857666
    },
    {
      "epoch": 0.0767479674796748,
      "step": 354,
      "training_loss": 8.750621795654297
    },
    {
      "epoch": 0.0767479674796748,
      "step": 354,
      "training_loss": 9.927215576171875
    },
    {
      "epoch": 0.07696476964769648,
      "step": 355,
      "training_loss": 6.567143440246582
    },
    {
      "epoch": 0.07696476964769648,
      "step": 355,
      "training_loss": 8.477529525756836
    },
    {
      "epoch": 0.07696476964769648,
      "step": 355,
      "training_loss": 7.915719032287598
    },
    {
      "epoch": 0.07696476964769648,
      "step": 355,
      "training_loss": 6.968222141265869
    },
    {
      "epoch": 0.07718157181571816,
      "grad_norm": 38.21114730834961,
      "learning_rate": 1e-05,
      "loss": 7.8045,
      "step": 356
    },
    {
      "epoch": 0.07718157181571816,
      "step": 356,
      "training_loss": 6.177818775177002
    },
    {
      "epoch": 0.07718157181571816,
      "step": 356,
      "training_loss": 7.300991058349609
    },
    {
      "epoch": 0.07718157181571816,
      "step": 356,
      "training_loss": 6.489441871643066
    },
    {
      "epoch": 0.07718157181571816,
      "step": 356,
      "training_loss": 6.3603081703186035
    },
    {
      "epoch": 0.07739837398373983,
      "step": 357,
      "training_loss": 8.500944137573242
    },
    {
      "epoch": 0.07739837398373983,
      "step": 357,
      "training_loss": 8.282540321350098
    },
    {
      "epoch": 0.07739837398373983,
      "step": 357,
      "training_loss": 6.954254150390625
    },
    {
      "epoch": 0.07739837398373983,
      "step": 357,
      "training_loss": 6.281638145446777
    },
    {
      "epoch": 0.07761517615176151,
      "step": 358,
      "training_loss": 8.158886909484863
    },
    {
      "epoch": 0.07761517615176151,
      "step": 358,
      "training_loss": 8.06462287902832
    },
    {
      "epoch": 0.07761517615176151,
      "step": 358,
      "training_loss": 7.806986331939697
    },
    {
      "epoch": 0.07761517615176151,
      "step": 358,
      "training_loss": 7.653802394866943
    },
    {
      "epoch": 0.0778319783197832,
      "step": 359,
      "training_loss": 7.382436275482178
    },
    {
      "epoch": 0.0778319783197832,
      "step": 359,
      "training_loss": 7.428923606872559
    },
    {
      "epoch": 0.0778319783197832,
      "step": 359,
      "training_loss": 7.367988109588623
    },
    {
      "epoch": 0.0778319783197832,
      "step": 359,
      "training_loss": 7.514551639556885
    },
    {
      "epoch": 0.07804878048780488,
      "grad_norm": 8.67624568939209,
      "learning_rate": 1e-05,
      "loss": 7.3579,
      "step": 360
    },
    {
      "epoch": 0.07804878048780488,
      "step": 360,
      "training_loss": 7.569649696350098
    },
    {
      "epoch": 0.07804878048780488,
      "step": 360,
      "training_loss": 7.19333028793335
    },
    {
      "epoch": 0.07804878048780488,
      "step": 360,
      "training_loss": 7.599008083343506
    },
    {
      "epoch": 0.07804878048780488,
      "step": 360,
      "training_loss": 8.926871299743652
    },
    {
      "epoch": 0.07826558265582656,
      "step": 361,
      "training_loss": 8.505086898803711
    },
    {
      "epoch": 0.07826558265582656,
      "step": 361,
      "training_loss": 8.26209545135498
    },
    {
      "epoch": 0.07826558265582656,
      "step": 361,
      "training_loss": 7.934647560119629
    },
    {
      "epoch": 0.07826558265582656,
      "step": 361,
      "training_loss": 6.3065505027771
    },
    {
      "epoch": 0.07848238482384824,
      "step": 362,
      "training_loss": 7.9667558670043945
    },
    {
      "epoch": 0.07848238482384824,
      "step": 362,
      "training_loss": 7.28446102142334
    },
    {
      "epoch": 0.07848238482384824,
      "step": 362,
      "training_loss": 8.471291542053223
    },
    {
      "epoch": 0.07848238482384824,
      "step": 362,
      "training_loss": 6.672237396240234
    },
    {
      "epoch": 0.07869918699186992,
      "step": 363,
      "training_loss": 8.080378532409668
    },
    {
      "epoch": 0.07869918699186992,
      "step": 363,
      "training_loss": 8.164806365966797
    },
    {
      "epoch": 0.07869918699186992,
      "step": 363,
      "training_loss": 7.377711772918701
    },
    {
      "epoch": 0.07869918699186992,
      "step": 363,
      "training_loss": 9.048792839050293
    },
    {
      "epoch": 0.0789159891598916,
      "grad_norm": 8.28503704071045,
      "learning_rate": 1e-05,
      "loss": 7.8352,
      "step": 364
    },
    {
      "epoch": 0.0789159891598916,
      "step": 364,
      "training_loss": 7.037072658538818
    },
    {
      "epoch": 0.0789159891598916,
      "step": 364,
      "training_loss": 8.051529884338379
    },
    {
      "epoch": 0.0789159891598916,
      "step": 364,
      "training_loss": 7.193315505981445
    },
    {
      "epoch": 0.0789159891598916,
      "step": 364,
      "training_loss": 8.116616249084473
    },
    {
      "epoch": 0.07913279132791327,
      "step": 365,
      "training_loss": 8.625510215759277
    },
    {
      "epoch": 0.07913279132791327,
      "step": 365,
      "training_loss": 7.75101375579834
    },
    {
      "epoch": 0.07913279132791327,
      "step": 365,
      "training_loss": 6.594483852386475
    },
    {
      "epoch": 0.07913279132791327,
      "step": 365,
      "training_loss": 7.566758155822754
    },
    {
      "epoch": 0.07934959349593496,
      "step": 366,
      "training_loss": 6.676406383514404
    },
    {
      "epoch": 0.07934959349593496,
      "step": 366,
      "training_loss": 5.961306095123291
    },
    {
      "epoch": 0.07934959349593496,
      "step": 366,
      "training_loss": 6.858781814575195
    },
    {
      "epoch": 0.07934959349593496,
      "step": 366,
      "training_loss": 7.72995662689209
    },
    {
      "epoch": 0.07956639566395664,
      "step": 367,
      "training_loss": 7.304133892059326
    },
    {
      "epoch": 0.07956639566395664,
      "step": 367,
      "training_loss": 8.491721153259277
    },
    {
      "epoch": 0.07956639566395664,
      "step": 367,
      "training_loss": 7.835065841674805
    },
    {
      "epoch": 0.07956639566395664,
      "step": 367,
      "training_loss": 8.217796325683594
    },
    {
      "epoch": 0.07978319783197832,
      "grad_norm": 6.4822468757629395,
      "learning_rate": 1e-05,
      "loss": 7.5007,
      "step": 368
    },
    {
      "epoch": 0.07978319783197832,
      "step": 368,
      "training_loss": 8.482872009277344
    },
    {
      "epoch": 0.07978319783197832,
      "step": 368,
      "training_loss": 7.724673271179199
    },
    {
      "epoch": 0.07978319783197832,
      "step": 368,
      "training_loss": 7.986774921417236
    },
    {
      "epoch": 0.07978319783197832,
      "step": 368,
      "training_loss": 8.917150497436523
    },
    {
      "epoch": 0.08,
      "step": 369,
      "training_loss": 8.199212074279785
    },
    {
      "epoch": 0.08,
      "step": 369,
      "training_loss": 8.338844299316406
    },
    {
      "epoch": 0.08,
      "step": 369,
      "training_loss": 7.4475178718566895
    },
    {
      "epoch": 0.08,
      "step": 369,
      "training_loss": 7.746762752532959
    },
    {
      "epoch": 0.08021680216802168,
      "step": 370,
      "training_loss": 8.658126831054688
    },
    {
      "epoch": 0.08021680216802168,
      "step": 370,
      "training_loss": 7.789925575256348
    },
    {
      "epoch": 0.08021680216802168,
      "step": 370,
      "training_loss": 7.565216541290283
    },
    {
      "epoch": 0.08021680216802168,
      "step": 370,
      "training_loss": 7.844878673553467
    },
    {
      "epoch": 0.08043360433604337,
      "step": 371,
      "training_loss": 7.409068584442139
    },
    {
      "epoch": 0.08043360433604337,
      "step": 371,
      "training_loss": 7.5737810134887695
    },
    {
      "epoch": 0.08043360433604337,
      "step": 371,
      "training_loss": 9.6215181350708
    },
    {
      "epoch": 0.08043360433604337,
      "step": 371,
      "training_loss": 7.220926284790039
    },
    {
      "epoch": 0.08065040650406505,
      "grad_norm": 7.346324920654297,
      "learning_rate": 1e-05,
      "loss": 8.033,
      "step": 372
    },
    {
      "epoch": 0.08065040650406505,
      "step": 372,
      "training_loss": 6.772524833679199
    },
    {
      "epoch": 0.08065040650406505,
      "step": 372,
      "training_loss": 8.599050521850586
    },
    {
      "epoch": 0.08065040650406505,
      "step": 372,
      "training_loss": 7.914992332458496
    },
    {
      "epoch": 0.08065040650406505,
      "step": 372,
      "training_loss": 8.557478904724121
    },
    {
      "epoch": 0.08086720867208672,
      "step": 373,
      "training_loss": 7.507643699645996
    },
    {
      "epoch": 0.08086720867208672,
      "step": 373,
      "training_loss": 8.550390243530273
    },
    {
      "epoch": 0.08086720867208672,
      "step": 373,
      "training_loss": 6.837709903717041
    },
    {
      "epoch": 0.08086720867208672,
      "step": 373,
      "training_loss": 7.908990383148193
    },
    {
      "epoch": 0.0810840108401084,
      "step": 374,
      "training_loss": 6.894425868988037
    },
    {
      "epoch": 0.0810840108401084,
      "step": 374,
      "training_loss": 7.860786437988281
    },
    {
      "epoch": 0.0810840108401084,
      "step": 374,
      "training_loss": 10.20931625366211
    },
    {
      "epoch": 0.0810840108401084,
      "step": 374,
      "training_loss": 7.11106538772583
    },
    {
      "epoch": 0.08130081300813008,
      "step": 375,
      "training_loss": 6.464744567871094
    },
    {
      "epoch": 0.08130081300813008,
      "step": 375,
      "training_loss": 7.5650482177734375
    },
    {
      "epoch": 0.08130081300813008,
      "step": 375,
      "training_loss": 6.485140323638916
    },
    {
      "epoch": 0.08130081300813008,
      "step": 375,
      "training_loss": 8.6277494430542
    },
    {
      "epoch": 0.08151761517615176,
      "grad_norm": 6.913656234741211,
      "learning_rate": 1e-05,
      "loss": 7.7417,
      "step": 376
    },
    {
      "epoch": 0.08151761517615176,
      "step": 376,
      "training_loss": 7.382986068725586
    },
    {
      "epoch": 0.08151761517615176,
      "step": 376,
      "training_loss": 7.160738468170166
    },
    {
      "epoch": 0.08151761517615176,
      "step": 376,
      "training_loss": 5.765810966491699
    },
    {
      "epoch": 0.08151761517615176,
      "step": 376,
      "training_loss": 8.376479148864746
    },
    {
      "epoch": 0.08173441734417344,
      "step": 377,
      "training_loss": 7.711132049560547
    },
    {
      "epoch": 0.08173441734417344,
      "step": 377,
      "training_loss": 6.837111473083496
    },
    {
      "epoch": 0.08173441734417344,
      "step": 377,
      "training_loss": 7.531510353088379
    },
    {
      "epoch": 0.08173441734417344,
      "step": 377,
      "training_loss": 6.058841228485107
    },
    {
      "epoch": 0.08195121951219513,
      "step": 378,
      "training_loss": 7.356544017791748
    },
    {
      "epoch": 0.08195121951219513,
      "step": 378,
      "training_loss": 7.759592056274414
    },
    {
      "epoch": 0.08195121951219513,
      "step": 378,
      "training_loss": 6.572195529937744
    },
    {
      "epoch": 0.08195121951219513,
      "step": 378,
      "training_loss": 7.247291088104248
    },
    {
      "epoch": 0.08216802168021681,
      "step": 379,
      "training_loss": 8.073227882385254
    },
    {
      "epoch": 0.08216802168021681,
      "step": 379,
      "training_loss": 7.549444675445557
    },
    {
      "epoch": 0.08216802168021681,
      "step": 379,
      "training_loss": 6.55302619934082
    },
    {
      "epoch": 0.08216802168021681,
      "step": 379,
      "training_loss": 8.655436515808105
    },
    {
      "epoch": 0.08238482384823849,
      "grad_norm": 7.607263565063477,
      "learning_rate": 1e-05,
      "loss": 7.287,
      "step": 380
    },
    {
      "epoch": 0.08238482384823849,
      "step": 380,
      "training_loss": 7.948608875274658
    },
    {
      "epoch": 0.08238482384823849,
      "step": 380,
      "training_loss": 6.852585315704346
    },
    {
      "epoch": 0.08238482384823849,
      "step": 380,
      "training_loss": 7.866298675537109
    },
    {
      "epoch": 0.08238482384823849,
      "step": 380,
      "training_loss": 7.278505325317383
    },
    {
      "epoch": 0.08260162601626016,
      "step": 381,
      "training_loss": 6.752599239349365
    },
    {
      "epoch": 0.08260162601626016,
      "step": 381,
      "training_loss": 7.490969657897949
    },
    {
      "epoch": 0.08260162601626016,
      "step": 381,
      "training_loss": 6.140195369720459
    },
    {
      "epoch": 0.08260162601626016,
      "step": 381,
      "training_loss": 6.640631198883057
    },
    {
      "epoch": 0.08281842818428184,
      "step": 382,
      "training_loss": 6.846495628356934
    },
    {
      "epoch": 0.08281842818428184,
      "step": 382,
      "training_loss": 7.44287633895874
    },
    {
      "epoch": 0.08281842818428184,
      "step": 382,
      "training_loss": 7.46273136138916
    },
    {
      "epoch": 0.08281842818428184,
      "step": 382,
      "training_loss": 7.740181922912598
    },
    {
      "epoch": 0.08303523035230352,
      "step": 383,
      "training_loss": 7.432460308074951
    },
    {
      "epoch": 0.08303523035230352,
      "step": 383,
      "training_loss": 6.514998435974121
    },
    {
      "epoch": 0.08303523035230352,
      "step": 383,
      "training_loss": 7.7037811279296875
    },
    {
      "epoch": 0.08303523035230352,
      "step": 383,
      "training_loss": 8.572640419006348
    },
    {
      "epoch": 0.0832520325203252,
      "grad_norm": 34.64687728881836,
      "learning_rate": 1e-05,
      "loss": 7.2929,
      "step": 384
    },
    {
      "epoch": 0.0832520325203252,
      "step": 384,
      "training_loss": 7.393444061279297
    },
    {
      "epoch": 0.0832520325203252,
      "step": 384,
      "training_loss": 6.809739589691162
    },
    {
      "epoch": 0.0832520325203252,
      "step": 384,
      "training_loss": 8.354009628295898
    },
    {
      "epoch": 0.0832520325203252,
      "step": 384,
      "training_loss": 7.224018573760986
    },
    {
      "epoch": 0.08346883468834689,
      "step": 385,
      "training_loss": 7.06647253036499
    },
    {
      "epoch": 0.08346883468834689,
      "step": 385,
      "training_loss": 8.077034950256348
    },
    {
      "epoch": 0.08346883468834689,
      "step": 385,
      "training_loss": 6.546486854553223
    },
    {
      "epoch": 0.08346883468834689,
      "step": 385,
      "training_loss": 6.608115196228027
    },
    {
      "epoch": 0.08368563685636857,
      "step": 386,
      "training_loss": 8.464731216430664
    },
    {
      "epoch": 0.08368563685636857,
      "step": 386,
      "training_loss": 7.66046667098999
    },
    {
      "epoch": 0.08368563685636857,
      "step": 386,
      "training_loss": 7.584188938140869
    },
    {
      "epoch": 0.08368563685636857,
      "step": 386,
      "training_loss": 7.831490993499756
    },
    {
      "epoch": 0.08390243902439025,
      "step": 387,
      "training_loss": 7.013270854949951
    },
    {
      "epoch": 0.08390243902439025,
      "step": 387,
      "training_loss": 6.784848690032959
    },
    {
      "epoch": 0.08390243902439025,
      "step": 387,
      "training_loss": 6.726080417633057
    },
    {
      "epoch": 0.08390243902439025,
      "step": 387,
      "training_loss": 7.718171119689941
    },
    {
      "epoch": 0.08411924119241193,
      "grad_norm": 7.433840751647949,
      "learning_rate": 1e-05,
      "loss": 7.3664,
      "step": 388
    },
    {
      "epoch": 0.08411924119241193,
      "step": 388,
      "training_loss": 6.544736862182617
    },
    {
      "epoch": 0.08411924119241193,
      "step": 388,
      "training_loss": 8.203983306884766
    },
    {
      "epoch": 0.08411924119241193,
      "step": 388,
      "training_loss": 7.465991020202637
    },
    {
      "epoch": 0.08411924119241193,
      "step": 388,
      "training_loss": 8.106051445007324
    },
    {
      "epoch": 0.0843360433604336,
      "step": 389,
      "training_loss": 8.131522178649902
    },
    {
      "epoch": 0.0843360433604336,
      "step": 389,
      "training_loss": 8.356942176818848
    },
    {
      "epoch": 0.0843360433604336,
      "step": 389,
      "training_loss": 8.284195899963379
    },
    {
      "epoch": 0.0843360433604336,
      "step": 389,
      "training_loss": 8.43376636505127
    },
    {
      "epoch": 0.08455284552845528,
      "step": 390,
      "training_loss": 6.587597370147705
    },
    {
      "epoch": 0.08455284552845528,
      "step": 390,
      "training_loss": 7.629831790924072
    },
    {
      "epoch": 0.08455284552845528,
      "step": 390,
      "training_loss": 7.891706943511963
    },
    {
      "epoch": 0.08455284552845528,
      "step": 390,
      "training_loss": 8.281379699707031
    },
    {
      "epoch": 0.08476964769647696,
      "step": 391,
      "training_loss": 7.473642349243164
    },
    {
      "epoch": 0.08476964769647696,
      "step": 391,
      "training_loss": 7.356862545013428
    },
    {
      "epoch": 0.08476964769647696,
      "step": 391,
      "training_loss": 7.434154510498047
    },
    {
      "epoch": 0.08476964769647696,
      "step": 391,
      "training_loss": 7.545454025268555
    },
    {
      "epoch": 0.08498644986449864,
      "grad_norm": 8.564620971679688,
      "learning_rate": 1e-05,
      "loss": 7.733,
      "step": 392
    },
    {
      "epoch": 0.08498644986449864,
      "step": 392,
      "training_loss": 7.493045806884766
    },
    {
      "epoch": 0.08498644986449864,
      "step": 392,
      "training_loss": 7.530567646026611
    },
    {
      "epoch": 0.08498644986449864,
      "step": 392,
      "training_loss": 6.9867377281188965
    },
    {
      "epoch": 0.08498644986449864,
      "step": 392,
      "training_loss": 7.497313022613525
    },
    {
      "epoch": 0.08520325203252033,
      "step": 393,
      "training_loss": 7.54642915725708
    },
    {
      "epoch": 0.08520325203252033,
      "step": 393,
      "training_loss": 7.227397918701172
    },
    {
      "epoch": 0.08520325203252033,
      "step": 393,
      "training_loss": 6.083568572998047
    },
    {
      "epoch": 0.08520325203252033,
      "step": 393,
      "training_loss": 7.5797858238220215
    },
    {
      "epoch": 0.08542005420054201,
      "step": 394,
      "training_loss": 7.190927505493164
    },
    {
      "epoch": 0.08542005420054201,
      "step": 394,
      "training_loss": 7.804437637329102
    },
    {
      "epoch": 0.08542005420054201,
      "step": 394,
      "training_loss": 6.25315523147583
    },
    {
      "epoch": 0.08542005420054201,
      "step": 394,
      "training_loss": 8.14346981048584
    },
    {
      "epoch": 0.08563685636856369,
      "step": 395,
      "training_loss": 8.10193920135498
    },
    {
      "epoch": 0.08563685636856369,
      "step": 395,
      "training_loss": 8.656279563903809
    },
    {
      "epoch": 0.08563685636856369,
      "step": 395,
      "training_loss": 7.551058769226074
    },
    {
      "epoch": 0.08563685636856369,
      "step": 395,
      "training_loss": 7.372847557067871
    },
    {
      "epoch": 0.08585365853658537,
      "grad_norm": 8.214591026306152,
      "learning_rate": 1e-05,
      "loss": 7.4387,
      "step": 396
    },
    {
      "epoch": 0.08585365853658537,
      "step": 396,
      "training_loss": 7.052064895629883
    },
    {
      "epoch": 0.08585365853658537,
      "step": 396,
      "training_loss": 7.301991939544678
    },
    {
      "epoch": 0.08585365853658537,
      "step": 396,
      "training_loss": 7.68660306930542
    },
    {
      "epoch": 0.08585365853658537,
      "step": 396,
      "training_loss": 7.6993088722229
    },
    {
      "epoch": 0.08607046070460704,
      "step": 397,
      "training_loss": 7.585814476013184
    },
    {
      "epoch": 0.08607046070460704,
      "step": 397,
      "training_loss": 5.526637554168701
    },
    {
      "epoch": 0.08607046070460704,
      "step": 397,
      "training_loss": 8.026785850524902
    },
    {
      "epoch": 0.08607046070460704,
      "step": 397,
      "training_loss": 7.8537092208862305
    },
    {
      "epoch": 0.08628726287262872,
      "step": 398,
      "training_loss": 7.7518157958984375
    },
    {
      "epoch": 0.08628726287262872,
      "step": 398,
      "training_loss": 8.07005500793457
    },
    {
      "epoch": 0.08628726287262872,
      "step": 398,
      "training_loss": 8.92111873626709
    },
    {
      "epoch": 0.08628726287262872,
      "step": 398,
      "training_loss": 6.033825874328613
    },
    {
      "epoch": 0.0865040650406504,
      "step": 399,
      "training_loss": 6.670497417449951
    },
    {
      "epoch": 0.0865040650406504,
      "step": 399,
      "training_loss": 7.326803207397461
    },
    {
      "epoch": 0.0865040650406504,
      "step": 399,
      "training_loss": 5.786319255828857
    },
    {
      "epoch": 0.0865040650406504,
      "step": 399,
      "training_loss": 4.986851692199707
    },
    {
      "epoch": 0.08672086720867209,
      "grad_norm": 8.211631774902344,
      "learning_rate": 1e-05,
      "loss": 7.1425,
      "step": 400
    },
    {
      "epoch": 0.08672086720867209,
      "step": 400,
      "training_loss": 8.16402530670166
    },
    {
      "epoch": 0.08672086720867209,
      "step": 400,
      "training_loss": 6.480024814605713
    },
    {
      "epoch": 0.08672086720867209,
      "step": 400,
      "training_loss": 7.280276775360107
    },
    {
      "epoch": 0.08672086720867209,
      "step": 400,
      "training_loss": 8.154717445373535
    },
    {
      "epoch": 0.08693766937669377,
      "step": 401,
      "training_loss": 7.655904293060303
    },
    {
      "epoch": 0.08693766937669377,
      "step": 401,
      "training_loss": 7.352047443389893
    },
    {
      "epoch": 0.08693766937669377,
      "step": 401,
      "training_loss": 5.682362079620361
    },
    {
      "epoch": 0.08693766937669377,
      "step": 401,
      "training_loss": 8.11989974975586
    },
    {
      "epoch": 0.08715447154471545,
      "step": 402,
      "training_loss": 8.387131690979004
    },
    {
      "epoch": 0.08715447154471545,
      "step": 402,
      "training_loss": 9.53703784942627
    },
    {
      "epoch": 0.08715447154471545,
      "step": 402,
      "training_loss": 6.655045509338379
    },
    {
      "epoch": 0.08715447154471545,
      "step": 402,
      "training_loss": 7.705557823181152
    },
    {
      "epoch": 0.08737127371273713,
      "step": 403,
      "training_loss": 7.438412189483643
    },
    {
      "epoch": 0.08737127371273713,
      "step": 403,
      "training_loss": 8.17999267578125
    },
    {
      "epoch": 0.08737127371273713,
      "step": 403,
      "training_loss": 7.171300888061523
    },
    {
      "epoch": 0.08737127371273713,
      "step": 403,
      "training_loss": 6.299173831939697
    },
    {
      "epoch": 0.08758807588075881,
      "grad_norm": 7.1993303298950195,
      "learning_rate": 1e-05,
      "loss": 7.5164,
      "step": 404
    },
    {
      "epoch": 0.08758807588075881,
      "step": 404,
      "training_loss": 7.641836166381836
    },
    {
      "epoch": 0.08758807588075881,
      "step": 404,
      "training_loss": 7.117701053619385
    },
    {
      "epoch": 0.08758807588075881,
      "step": 404,
      "training_loss": 8.085641860961914
    },
    {
      "epoch": 0.08758807588075881,
      "step": 404,
      "training_loss": 6.889163970947266
    },
    {
      "epoch": 0.08780487804878048,
      "step": 405,
      "training_loss": 6.277770519256592
    },
    {
      "epoch": 0.08780487804878048,
      "step": 405,
      "training_loss": 7.646060943603516
    },
    {
      "epoch": 0.08780487804878048,
      "step": 405,
      "training_loss": 7.830228328704834
    },
    {
      "epoch": 0.08780487804878048,
      "step": 405,
      "training_loss": 8.776656150817871
    },
    {
      "epoch": 0.08802168021680216,
      "step": 406,
      "training_loss": 8.884283065795898
    },
    {
      "epoch": 0.08802168021680216,
      "step": 406,
      "training_loss": 7.368470668792725
    },
    {
      "epoch": 0.08802168021680216,
      "step": 406,
      "training_loss": 8.663987159729004
    },
    {
      "epoch": 0.08802168021680216,
      "step": 406,
      "training_loss": 7.82201623916626
    },
    {
      "epoch": 0.08823848238482385,
      "step": 407,
      "training_loss": 6.9849371910095215
    },
    {
      "epoch": 0.08823848238482385,
      "step": 407,
      "training_loss": 7.868908882141113
    },
    {
      "epoch": 0.08823848238482385,
      "step": 407,
      "training_loss": 7.82205867767334
    },
    {
      "epoch": 0.08823848238482385,
      "step": 407,
      "training_loss": 6.70831823348999
    },
    {
      "epoch": 0.08845528455284553,
      "grad_norm": 8.574630737304688,
      "learning_rate": 1e-05,
      "loss": 7.6493,
      "step": 408
    },
    {
      "epoch": 0.08845528455284553,
      "step": 408,
      "training_loss": 7.717168807983398
    },
    {
      "epoch": 0.08845528455284553,
      "step": 408,
      "training_loss": 7.876805305480957
    },
    {
      "epoch": 0.08845528455284553,
      "step": 408,
      "training_loss": 7.386096000671387
    },
    {
      "epoch": 0.08845528455284553,
      "step": 408,
      "training_loss": 6.769972324371338
    },
    {
      "epoch": 0.08867208672086721,
      "step": 409,
      "training_loss": 6.8474907875061035
    },
    {
      "epoch": 0.08867208672086721,
      "step": 409,
      "training_loss": 7.430103302001953
    },
    {
      "epoch": 0.08867208672086721,
      "step": 409,
      "training_loss": 8.60749340057373
    },
    {
      "epoch": 0.08867208672086721,
      "step": 409,
      "training_loss": 6.915833473205566
    },
    {
      "epoch": 0.08888888888888889,
      "step": 410,
      "training_loss": 6.886184215545654
    },
    {
      "epoch": 0.08888888888888889,
      "step": 410,
      "training_loss": 7.472280502319336
    },
    {
      "epoch": 0.08888888888888889,
      "step": 410,
      "training_loss": 8.016729354858398
    },
    {
      "epoch": 0.08888888888888889,
      "step": 410,
      "training_loss": 6.445272445678711
    },
    {
      "epoch": 0.08910569105691057,
      "step": 411,
      "training_loss": 7.496355056762695
    },
    {
      "epoch": 0.08910569105691057,
      "step": 411,
      "training_loss": 6.299161434173584
    },
    {
      "epoch": 0.08910569105691057,
      "step": 411,
      "training_loss": 8.038983345031738
    },
    {
      "epoch": 0.08910569105691057,
      "step": 411,
      "training_loss": 7.582717418670654
    },
    {
      "epoch": 0.08932249322493226,
      "grad_norm": 7.722811222076416,
      "learning_rate": 1e-05,
      "loss": 7.3618,
      "step": 412
    },
    {
      "epoch": 0.08932249322493226,
      "step": 412,
      "training_loss": 6.286516189575195
    },
    {
      "epoch": 0.08932249322493226,
      "step": 412,
      "training_loss": 7.193554401397705
    },
    {
      "epoch": 0.08932249322493226,
      "step": 412,
      "training_loss": 6.946856498718262
    },
    {
      "epoch": 0.08932249322493226,
      "step": 412,
      "training_loss": 7.37235689163208
    },
    {
      "epoch": 0.08953929539295392,
      "step": 413,
      "training_loss": 7.574662685394287
    },
    {
      "epoch": 0.08953929539295392,
      "step": 413,
      "training_loss": 8.490656852722168
    },
    {
      "epoch": 0.08953929539295392,
      "step": 413,
      "training_loss": 7.368077754974365
    },
    {
      "epoch": 0.08953929539295392,
      "step": 413,
      "training_loss": 7.071966171264648
    },
    {
      "epoch": 0.0897560975609756,
      "step": 414,
      "training_loss": 8.683417320251465
    },
    {
      "epoch": 0.0897560975609756,
      "step": 414,
      "training_loss": 7.295628547668457
    },
    {
      "epoch": 0.0897560975609756,
      "step": 414,
      "training_loss": 7.127044677734375
    },
    {
      "epoch": 0.0897560975609756,
      "step": 414,
      "training_loss": 7.379115104675293
    },
    {
      "epoch": 0.08997289972899729,
      "step": 415,
      "training_loss": 6.642946720123291
    },
    {
      "epoch": 0.08997289972899729,
      "step": 415,
      "training_loss": 5.8854169845581055
    },
    {
      "epoch": 0.08997289972899729,
      "step": 415,
      "training_loss": 7.564972400665283
    },
    {
      "epoch": 0.08997289972899729,
      "step": 415,
      "training_loss": 8.172124862670898
    },
    {
      "epoch": 0.09018970189701897,
      "grad_norm": 9.949014663696289,
      "learning_rate": 1e-05,
      "loss": 7.316,
      "step": 416
    },
    {
      "epoch": 0.09018970189701897,
      "step": 416,
      "training_loss": 7.642681121826172
    },
    {
      "epoch": 0.09018970189701897,
      "step": 416,
      "training_loss": 7.551724433898926
    },
    {
      "epoch": 0.09018970189701897,
      "step": 416,
      "training_loss": 8.745495796203613
    },
    {
      "epoch": 0.09018970189701897,
      "step": 416,
      "training_loss": 7.428471565246582
    },
    {
      "epoch": 0.09040650406504065,
      "step": 417,
      "training_loss": 7.171889781951904
    },
    {
      "epoch": 0.09040650406504065,
      "step": 417,
      "training_loss": 5.769639492034912
    },
    {
      "epoch": 0.09040650406504065,
      "step": 417,
      "training_loss": 7.432443618774414
    },
    {
      "epoch": 0.09040650406504065,
      "step": 417,
      "training_loss": 8.923596382141113
    },
    {
      "epoch": 0.09062330623306233,
      "step": 418,
      "training_loss": 6.7726359367370605
    },
    {
      "epoch": 0.09062330623306233,
      "step": 418,
      "training_loss": 8.836204528808594
    },
    {
      "epoch": 0.09062330623306233,
      "step": 418,
      "training_loss": 6.804697036743164
    },
    {
      "epoch": 0.09062330623306233,
      "step": 418,
      "training_loss": 7.142188549041748
    },
    {
      "epoch": 0.09084010840108402,
      "step": 419,
      "training_loss": 9.05344295501709
    },
    {
      "epoch": 0.09084010840108402,
      "step": 419,
      "training_loss": 5.8570780754089355
    },
    {
      "epoch": 0.09084010840108402,
      "step": 419,
      "training_loss": 8.59079360961914
    },
    {
      "epoch": 0.09084010840108402,
      "step": 419,
      "training_loss": 7.467078685760498
    },
    {
      "epoch": 0.0910569105691057,
      "grad_norm": 10.086739540100098,
      "learning_rate": 1e-05,
      "loss": 7.5744,
      "step": 420
    },
    {
      "epoch": 0.0910569105691057,
      "step": 420,
      "training_loss": 7.523290634155273
    },
    {
      "epoch": 0.0910569105691057,
      "step": 420,
      "training_loss": 9.388203620910645
    },
    {
      "epoch": 0.0910569105691057,
      "step": 420,
      "training_loss": 7.851738452911377
    },
    {
      "epoch": 0.0910569105691057,
      "step": 420,
      "training_loss": 8.036615371704102
    },
    {
      "epoch": 0.09127371273712737,
      "step": 421,
      "training_loss": 8.924742698669434
    },
    {
      "epoch": 0.09127371273712737,
      "step": 421,
      "training_loss": 8.338311195373535
    },
    {
      "epoch": 0.09127371273712737,
      "step": 421,
      "training_loss": 7.435419082641602
    },
    {
      "epoch": 0.09127371273712737,
      "step": 421,
      "training_loss": 9.776920318603516
    },
    {
      "epoch": 0.09149051490514905,
      "step": 422,
      "training_loss": 7.931642055511475
    },
    {
      "epoch": 0.09149051490514905,
      "step": 422,
      "training_loss": 8.37321662902832
    },
    {
      "epoch": 0.09149051490514905,
      "step": 422,
      "training_loss": 6.431545734405518
    },
    {
      "epoch": 0.09149051490514905,
      "step": 422,
      "training_loss": 7.484476089477539
    },
    {
      "epoch": 0.09170731707317073,
      "step": 423,
      "training_loss": 7.720913410186768
    },
    {
      "epoch": 0.09170731707317073,
      "step": 423,
      "training_loss": 7.32297945022583
    },
    {
      "epoch": 0.09170731707317073,
      "step": 423,
      "training_loss": 9.89228630065918
    },
    {
      "epoch": 0.09170731707317073,
      "step": 423,
      "training_loss": 7.917448043823242
    },
    {
      "epoch": 0.09192411924119241,
      "grad_norm": 13.031113624572754,
      "learning_rate": 1e-05,
      "loss": 8.1469,
      "step": 424
    },
    {
      "epoch": 0.09192411924119241,
      "step": 424,
      "training_loss": 7.06943416595459
    },
    {
      "epoch": 0.09192411924119241,
      "step": 424,
      "training_loss": 7.262650966644287
    },
    {
      "epoch": 0.09192411924119241,
      "step": 424,
      "training_loss": 8.45206069946289
    },
    {
      "epoch": 0.09192411924119241,
      "step": 424,
      "training_loss": 6.875007629394531
    },
    {
      "epoch": 0.0921409214092141,
      "step": 425,
      "training_loss": 7.519100189208984
    },
    {
      "epoch": 0.0921409214092141,
      "step": 425,
      "training_loss": 7.1032538414001465
    },
    {
      "epoch": 0.0921409214092141,
      "step": 425,
      "training_loss": 5.48206901550293
    },
    {
      "epoch": 0.0921409214092141,
      "step": 425,
      "training_loss": 6.675551891326904
    },
    {
      "epoch": 0.09235772357723578,
      "step": 426,
      "training_loss": 8.335246086120605
    },
    {
      "epoch": 0.09235772357723578,
      "step": 426,
      "training_loss": 6.578075408935547
    },
    {
      "epoch": 0.09235772357723578,
      "step": 426,
      "training_loss": 8.709754943847656
    },
    {
      "epoch": 0.09235772357723578,
      "step": 426,
      "training_loss": 7.48773717880249
    },
    {
      "epoch": 0.09257452574525746,
      "step": 427,
      "training_loss": 8.329510688781738
    },
    {
      "epoch": 0.09257452574525746,
      "step": 427,
      "training_loss": 7.336324691772461
    },
    {
      "epoch": 0.09257452574525746,
      "step": 427,
      "training_loss": 7.861161708831787
    },
    {
      "epoch": 0.09257452574525746,
      "step": 427,
      "training_loss": 7.82258939743042
    },
    {
      "epoch": 0.09279132791327914,
      "grad_norm": 6.152872562408447,
      "learning_rate": 1e-05,
      "loss": 7.4312,
      "step": 428
    },
    {
      "epoch": 0.09279132791327914,
      "step": 428,
      "training_loss": 7.716245651245117
    },
    {
      "epoch": 0.09279132791327914,
      "step": 428,
      "training_loss": 6.747432708740234
    },
    {
      "epoch": 0.09279132791327914,
      "step": 428,
      "training_loss": 7.629819869995117
    },
    {
      "epoch": 0.09279132791327914,
      "step": 428,
      "training_loss": 8.541764259338379
    },
    {
      "epoch": 0.09300813008130081,
      "step": 429,
      "training_loss": 7.039007186889648
    },
    {
      "epoch": 0.09300813008130081,
      "step": 429,
      "training_loss": 8.476766586303711
    },
    {
      "epoch": 0.09300813008130081,
      "step": 429,
      "training_loss": 7.546884536743164
    },
    {
      "epoch": 0.09300813008130081,
      "step": 429,
      "training_loss": 7.051605224609375
    },
    {
      "epoch": 0.09322493224932249,
      "step": 430,
      "training_loss": 7.005039215087891
    },
    {
      "epoch": 0.09322493224932249,
      "step": 430,
      "training_loss": 8.787872314453125
    },
    {
      "epoch": 0.09322493224932249,
      "step": 430,
      "training_loss": 6.768316745758057
    },
    {
      "epoch": 0.09322493224932249,
      "step": 430,
      "training_loss": 7.395030498504639
    },
    {
      "epoch": 0.09344173441734417,
      "step": 431,
      "training_loss": 6.614141941070557
    },
    {
      "epoch": 0.09344173441734417,
      "step": 431,
      "training_loss": 7.476695537567139
    },
    {
      "epoch": 0.09344173441734417,
      "step": 431,
      "training_loss": 8.085238456726074
    },
    {
      "epoch": 0.09344173441734417,
      "step": 431,
      "training_loss": 6.956167697906494
    },
    {
      "epoch": 0.09365853658536585,
      "grad_norm": 7.150528430938721,
      "learning_rate": 1e-05,
      "loss": 7.4899,
      "step": 432
    },
    {
      "epoch": 0.09365853658536585,
      "step": 432,
      "training_loss": 6.2686638832092285
    },
    {
      "epoch": 0.09365853658536585,
      "step": 432,
      "training_loss": 6.817713737487793
    },
    {
      "epoch": 0.09365853658536585,
      "step": 432,
      "training_loss": 7.953629016876221
    },
    {
      "epoch": 0.09365853658536585,
      "step": 432,
      "training_loss": 7.653858661651611
    },
    {
      "epoch": 0.09387533875338754,
      "step": 433,
      "training_loss": 7.211630821228027
    },
    {
      "epoch": 0.09387533875338754,
      "step": 433,
      "training_loss": 7.571349620819092
    },
    {
      "epoch": 0.09387533875338754,
      "step": 433,
      "training_loss": 7.674591541290283
    },
    {
      "epoch": 0.09387533875338754,
      "step": 433,
      "training_loss": 8.07883071899414
    },
    {
      "epoch": 0.09409214092140922,
      "step": 434,
      "training_loss": 7.360065937042236
    },
    {
      "epoch": 0.09409214092140922,
      "step": 434,
      "training_loss": 8.174786567687988
    },
    {
      "epoch": 0.09409214092140922,
      "step": 434,
      "training_loss": 8.172672271728516
    },
    {
      "epoch": 0.09409214092140922,
      "step": 434,
      "training_loss": 7.628303050994873
    },
    {
      "epoch": 0.0943089430894309,
      "step": 435,
      "training_loss": 7.551377296447754
    },
    {
      "epoch": 0.0943089430894309,
      "step": 435,
      "training_loss": 7.402233600616455
    },
    {
      "epoch": 0.0943089430894309,
      "step": 435,
      "training_loss": 7.185695171356201
    },
    {
      "epoch": 0.0943089430894309,
      "step": 435,
      "training_loss": 7.558728218078613
    },
    {
      "epoch": 0.09452574525745258,
      "grad_norm": 7.155255317687988,
      "learning_rate": 1e-05,
      "loss": 7.5165,
      "step": 436
    },
    {
      "epoch": 0.09452574525745258,
      "step": 436,
      "training_loss": 7.7870097160339355
    },
    {
      "epoch": 0.09452574525745258,
      "step": 436,
      "training_loss": 6.7949018478393555
    },
    {
      "epoch": 0.09452574525745258,
      "step": 436,
      "training_loss": 7.516891956329346
    },
    {
      "epoch": 0.09452574525745258,
      "step": 436,
      "training_loss": 7.011209487915039
    },
    {
      "epoch": 0.09474254742547425,
      "step": 437,
      "training_loss": 7.368472099304199
    },
    {
      "epoch": 0.09474254742547425,
      "step": 437,
      "training_loss": 8.414665222167969
    },
    {
      "epoch": 0.09474254742547425,
      "step": 437,
      "training_loss": 8.271146774291992
    },
    {
      "epoch": 0.09474254742547425,
      "step": 437,
      "training_loss": 7.160569667816162
    },
    {
      "epoch": 0.09495934959349593,
      "step": 438,
      "training_loss": 8.261310577392578
    },
    {
      "epoch": 0.09495934959349593,
      "step": 438,
      "training_loss": 7.390544414520264
    },
    {
      "epoch": 0.09495934959349593,
      "step": 438,
      "training_loss": 8.799598693847656
    },
    {
      "epoch": 0.09495934959349593,
      "step": 438,
      "training_loss": 8.128928184509277
    },
    {
      "epoch": 0.09517615176151761,
      "step": 439,
      "training_loss": 7.6385111808776855
    },
    {
      "epoch": 0.09517615176151761,
      "step": 439,
      "training_loss": 7.949865341186523
    },
    {
      "epoch": 0.09517615176151761,
      "step": 439,
      "training_loss": 6.722685813903809
    },
    {
      "epoch": 0.09517615176151761,
      "step": 439,
      "training_loss": 6.048281669616699
    },
    {
      "epoch": 0.0953929539295393,
      "grad_norm": 9.056074142456055,
      "learning_rate": 1e-05,
      "loss": 7.579,
      "step": 440
    },
    {
      "epoch": 0.0953929539295393,
      "step": 440,
      "training_loss": 7.325395584106445
    },
    {
      "epoch": 0.0953929539295393,
      "step": 440,
      "training_loss": 6.809396743774414
    },
    {
      "epoch": 0.0953929539295393,
      "step": 440,
      "training_loss": 7.93270206451416
    },
    {
      "epoch": 0.0953929539295393,
      "step": 440,
      "training_loss": 5.876395225524902
    },
    {
      "epoch": 0.09560975609756098,
      "step": 441,
      "training_loss": 8.715688705444336
    },
    {
      "epoch": 0.09560975609756098,
      "step": 441,
      "training_loss": 8.271340370178223
    },
    {
      "epoch": 0.09560975609756098,
      "step": 441,
      "training_loss": 7.611556053161621
    },
    {
      "epoch": 0.09560975609756098,
      "step": 441,
      "training_loss": 7.576800346374512
    },
    {
      "epoch": 0.09582655826558266,
      "step": 442,
      "training_loss": 7.465314865112305
    },
    {
      "epoch": 0.09582655826558266,
      "step": 442,
      "training_loss": 6.9765095710754395
    },
    {
      "epoch": 0.09582655826558266,
      "step": 442,
      "training_loss": 7.722802639007568
    },
    {
      "epoch": 0.09582655826558266,
      "step": 442,
      "training_loss": 5.882603645324707
    },
    {
      "epoch": 0.09604336043360434,
      "step": 443,
      "training_loss": 7.558326721191406
    },
    {
      "epoch": 0.09604336043360434,
      "step": 443,
      "training_loss": 8.456668853759766
    },
    {
      "epoch": 0.09604336043360434,
      "step": 443,
      "training_loss": 7.297536373138428
    },
    {
      "epoch": 0.09604336043360434,
      "step": 443,
      "training_loss": 8.358458518981934
    },
    {
      "epoch": 0.09626016260162602,
      "grad_norm": 8.361883163452148,
      "learning_rate": 1e-05,
      "loss": 7.4898,
      "step": 444
    },
    {
      "epoch": 0.09626016260162602,
      "step": 444,
      "training_loss": 7.301019668579102
    },
    {
      "epoch": 0.09626016260162602,
      "step": 444,
      "training_loss": 7.562089920043945
    },
    {
      "epoch": 0.09626016260162602,
      "step": 444,
      "training_loss": 7.377427577972412
    },
    {
      "epoch": 0.09626016260162602,
      "step": 444,
      "training_loss": 7.837671756744385
    },
    {
      "epoch": 0.09647696476964769,
      "step": 445,
      "training_loss": 8.031326293945312
    },
    {
      "epoch": 0.09647696476964769,
      "step": 445,
      "training_loss": 7.766227722167969
    },
    {
      "epoch": 0.09647696476964769,
      "step": 445,
      "training_loss": 8.1034574508667
    },
    {
      "epoch": 0.09647696476964769,
      "step": 445,
      "training_loss": 6.900975704193115
    },
    {
      "epoch": 0.09669376693766937,
      "step": 446,
      "training_loss": 6.8105316162109375
    },
    {
      "epoch": 0.09669376693766937,
      "step": 446,
      "training_loss": 7.705560207366943
    },
    {
      "epoch": 0.09669376693766937,
      "step": 446,
      "training_loss": 7.321799278259277
    },
    {
      "epoch": 0.09669376693766937,
      "step": 446,
      "training_loss": 7.985131740570068
    },
    {
      "epoch": 0.09691056910569106,
      "step": 447,
      "training_loss": 8.134916305541992
    },
    {
      "epoch": 0.09691056910569106,
      "step": 447,
      "training_loss": 7.9979987144470215
    },
    {
      "epoch": 0.09691056910569106,
      "step": 447,
      "training_loss": 8.813652038574219
    },
    {
      "epoch": 0.09691056910569106,
      "step": 447,
      "training_loss": 7.7494001388549805
    },
    {
      "epoch": 0.09712737127371274,
      "grad_norm": 7.949248790740967,
      "learning_rate": 1e-05,
      "loss": 7.7124,
      "step": 448
    },
    {
      "epoch": 0.09712737127371274,
      "step": 448,
      "training_loss": 6.6748366355896
    },
    {
      "epoch": 0.09712737127371274,
      "step": 448,
      "training_loss": 9.430197715759277
    },
    {
      "epoch": 0.09712737127371274,
      "step": 448,
      "training_loss": 6.950160503387451
    },
    {
      "epoch": 0.09712737127371274,
      "step": 448,
      "training_loss": 6.608154296875
    },
    {
      "epoch": 0.09734417344173442,
      "step": 449,
      "training_loss": 7.827464580535889
    },
    {
      "epoch": 0.09734417344173442,
      "step": 449,
      "training_loss": 7.335095405578613
    },
    {
      "epoch": 0.09734417344173442,
      "step": 449,
      "training_loss": 6.692455291748047
    },
    {
      "epoch": 0.09734417344173442,
      "step": 449,
      "training_loss": 7.753864288330078
    },
    {
      "epoch": 0.0975609756097561,
      "step": 450,
      "training_loss": 6.910593032836914
    },
    {
      "epoch": 0.0975609756097561,
      "step": 450,
      "training_loss": 7.026668548583984
    },
    {
      "epoch": 0.0975609756097561,
      "step": 450,
      "training_loss": 6.699347972869873
    },
    {
      "epoch": 0.0975609756097561,
      "step": 450,
      "training_loss": 8.73515510559082
    },
    {
      "epoch": 0.09777777777777778,
      "step": 451,
      "training_loss": 5.924276351928711
    },
    {
      "epoch": 0.09777777777777778,
      "step": 451,
      "training_loss": 7.194492816925049
    },
    {
      "epoch": 0.09777777777777778,
      "step": 451,
      "training_loss": 6.815627574920654
    },
    {
      "epoch": 0.09777777777777778,
      "step": 451,
      "training_loss": 8.569952011108398
    },
    {
      "epoch": 0.09799457994579946,
      "grad_norm": 9.376100540161133,
      "learning_rate": 1e-05,
      "loss": 7.3218,
      "step": 452
    },
    {
      "epoch": 0.09799457994579946,
      "step": 452,
      "training_loss": 7.071786403656006
    },
    {
      "epoch": 0.09799457994579946,
      "step": 452,
      "training_loss": 8.500837326049805
    },
    {
      "epoch": 0.09799457994579946,
      "step": 452,
      "training_loss": 7.616519927978516
    },
    {
      "epoch": 0.09799457994579946,
      "step": 452,
      "training_loss": 7.41066312789917
    },
    {
      "epoch": 0.09821138211382113,
      "step": 453,
      "training_loss": 7.498581409454346
    },
    {
      "epoch": 0.09821138211382113,
      "step": 453,
      "training_loss": 7.662198066711426
    },
    {
      "epoch": 0.09821138211382113,
      "step": 453,
      "training_loss": 8.533330917358398
    },
    {
      "epoch": 0.09821138211382113,
      "step": 453,
      "training_loss": 7.826202869415283
    },
    {
      "epoch": 0.09842818428184281,
      "step": 454,
      "training_loss": 5.781799793243408
    },
    {
      "epoch": 0.09842818428184281,
      "step": 454,
      "training_loss": 7.796501636505127
    },
    {
      "epoch": 0.09842818428184281,
      "step": 454,
      "training_loss": 7.433226585388184
    },
    {
      "epoch": 0.09842818428184281,
      "step": 454,
      "training_loss": 8.162298202514648
    },
    {
      "epoch": 0.0986449864498645,
      "step": 455,
      "training_loss": 6.141223907470703
    },
    {
      "epoch": 0.0986449864498645,
      "step": 455,
      "training_loss": 8.053763389587402
    },
    {
      "epoch": 0.0986449864498645,
      "step": 455,
      "training_loss": 6.651839256286621
    },
    {
      "epoch": 0.0986449864498645,
      "step": 455,
      "training_loss": 7.9693827629089355
    },
    {
      "epoch": 0.09886178861788618,
      "grad_norm": 7.522998332977295,
      "learning_rate": 1e-05,
      "loss": 7.5069,
      "step": 456
    },
    {
      "epoch": 0.09886178861788618,
      "step": 456,
      "training_loss": 7.886975288391113
    },
    {
      "epoch": 0.09886178861788618,
      "step": 456,
      "training_loss": 7.430150032043457
    },
    {
      "epoch": 0.09886178861788618,
      "step": 456,
      "training_loss": 7.33018159866333
    },
    {
      "epoch": 0.09886178861788618,
      "step": 456,
      "training_loss": 6.763383388519287
    },
    {
      "epoch": 0.09907859078590786,
      "step": 457,
      "training_loss": 7.151362895965576
    },
    {
      "epoch": 0.09907859078590786,
      "step": 457,
      "training_loss": 8.488028526306152
    },
    {
      "epoch": 0.09907859078590786,
      "step": 457,
      "training_loss": 7.582300662994385
    },
    {
      "epoch": 0.09907859078590786,
      "step": 457,
      "training_loss": 7.7537641525268555
    },
    {
      "epoch": 0.09929539295392954,
      "step": 458,
      "training_loss": 7.809164047241211
    },
    {
      "epoch": 0.09929539295392954,
      "step": 458,
      "training_loss": 7.224438190460205
    },
    {
      "epoch": 0.09929539295392954,
      "step": 458,
      "training_loss": 7.036888599395752
    },
    {
      "epoch": 0.09929539295392954,
      "step": 458,
      "training_loss": 6.6881103515625
    },
    {
      "epoch": 0.09951219512195122,
      "step": 459,
      "training_loss": 7.0610270500183105
    },
    {
      "epoch": 0.09951219512195122,
      "step": 459,
      "training_loss": 6.495622634887695
    },
    {
      "epoch": 0.09951219512195122,
      "step": 459,
      "training_loss": 6.982619285583496
    },
    {
      "epoch": 0.09951219512195122,
      "step": 459,
      "training_loss": 8.471846580505371
    },
    {
      "epoch": 0.0997289972899729,
      "grad_norm": 7.548433780670166,
      "learning_rate": 1e-05,
      "loss": 7.3847,
      "step": 460
    },
    {
      "epoch": 0.0997289972899729,
      "step": 460,
      "training_loss": 7.492752552032471
    },
    {
      "epoch": 0.0997289972899729,
      "step": 460,
      "training_loss": 7.414783954620361
    },
    {
      "epoch": 0.0997289972899729,
      "step": 460,
      "training_loss": 6.958139419555664
    },
    {
      "epoch": 0.0997289972899729,
      "step": 460,
      "training_loss": 7.523969650268555
    },
    {
      "epoch": 0.09994579945799457,
      "step": 461,
      "training_loss": 7.313895225524902
    },
    {
      "epoch": 0.09994579945799457,
      "step": 461,
      "training_loss": 9.326557159423828
    },
    {
      "epoch": 0.09994579945799457,
      "step": 461,
      "training_loss": 6.414394855499268
    },
    {
      "epoch": 0.09994579945799457,
      "step": 461,
      "training_loss": 8.158541679382324
    },
    {
      "epoch": 0.10016260162601626,
      "step": 462,
      "training_loss": 8.524713516235352
    },
    {
      "epoch": 0.10016260162601626,
      "step": 462,
      "training_loss": 7.452775955200195
    },
    {
      "epoch": 0.10016260162601626,
      "step": 462,
      "training_loss": 6.711016654968262
    },
    {
      "epoch": 0.10016260162601626,
      "step": 462,
      "training_loss": 6.0021891593933105
    },
    {
      "epoch": 0.10037940379403794,
      "step": 463,
      "training_loss": 7.351870059967041
    },
    {
      "epoch": 0.10037940379403794,
      "step": 463,
      "training_loss": 8.341731071472168
    },
    {
      "epoch": 0.10037940379403794,
      "step": 463,
      "training_loss": 6.927850246429443
    },
    {
      "epoch": 0.10037940379403794,
      "step": 463,
      "training_loss": 8.103446960449219
    },
    {
      "epoch": 0.10059620596205962,
      "grad_norm": 8.838705062866211,
      "learning_rate": 1e-05,
      "loss": 7.5012,
      "step": 464
    },
    {
      "epoch": 0.10059620596205962,
      "step": 464,
      "training_loss": 7.5359787940979
    },
    {
      "epoch": 0.10059620596205962,
      "step": 464,
      "training_loss": 7.917109489440918
    },
    {
      "epoch": 0.10059620596205962,
      "step": 464,
      "training_loss": 8.253795623779297
    },
    {
      "epoch": 0.10059620596205962,
      "step": 464,
      "training_loss": 5.830416679382324
    },
    {
      "epoch": 0.1008130081300813,
      "step": 465,
      "training_loss": 7.173283100128174
    },
    {
      "epoch": 0.1008130081300813,
      "step": 465,
      "training_loss": 6.86555814743042
    },
    {
      "epoch": 0.1008130081300813,
      "step": 465,
      "training_loss": 8.2816743850708
    },
    {
      "epoch": 0.1008130081300813,
      "step": 465,
      "training_loss": 7.030264377593994
    },
    {
      "epoch": 0.10102981029810298,
      "step": 466,
      "training_loss": 7.218795299530029
    },
    {
      "epoch": 0.10102981029810298,
      "step": 466,
      "training_loss": 8.179518699645996
    },
    {
      "epoch": 0.10102981029810298,
      "step": 466,
      "training_loss": 6.069608211517334
    },
    {
      "epoch": 0.10102981029810298,
      "step": 466,
      "training_loss": 7.452979564666748
    },
    {
      "epoch": 0.10124661246612467,
      "step": 467,
      "training_loss": 7.952457427978516
    },
    {
      "epoch": 0.10124661246612467,
      "step": 467,
      "training_loss": 7.3262481689453125
    },
    {
      "epoch": 0.10124661246612467,
      "step": 467,
      "training_loss": 7.294139862060547
    },
    {
      "epoch": 0.10124661246612467,
      "step": 467,
      "training_loss": 8.272405624389648
    },
    {
      "epoch": 0.10146341463414635,
      "grad_norm": 8.8307466506958,
      "learning_rate": 1e-05,
      "loss": 7.4159,
      "step": 468
    },
    {
      "epoch": 0.10146341463414635,
      "step": 468,
      "training_loss": 7.995546817779541
    },
    {
      "epoch": 0.10146341463414635,
      "step": 468,
      "training_loss": 7.348943710327148
    },
    {
      "epoch": 0.10146341463414635,
      "step": 468,
      "training_loss": 7.752555847167969
    },
    {
      "epoch": 0.10146341463414635,
      "step": 468,
      "training_loss": 7.205842018127441
    },
    {
      "epoch": 0.10168021680216802,
      "step": 469,
      "training_loss": 5.3938069343566895
    },
    {
      "epoch": 0.10168021680216802,
      "step": 469,
      "training_loss": 8.541333198547363
    },
    {
      "epoch": 0.10168021680216802,
      "step": 469,
      "training_loss": 6.5666656494140625
    },
    {
      "epoch": 0.10168021680216802,
      "step": 469,
      "training_loss": 8.063101768493652
    },
    {
      "epoch": 0.1018970189701897,
      "step": 470,
      "training_loss": 7.871635913848877
    },
    {
      "epoch": 0.1018970189701897,
      "step": 470,
      "training_loss": 6.627650260925293
    },
    {
      "epoch": 0.1018970189701897,
      "step": 470,
      "training_loss": 7.3707194328308105
    },
    {
      "epoch": 0.1018970189701897,
      "step": 470,
      "training_loss": 8.60824203491211
    },
    {
      "epoch": 0.10211382113821138,
      "step": 471,
      "training_loss": 7.370887756347656
    },
    {
      "epoch": 0.10211382113821138,
      "step": 471,
      "training_loss": 8.135141372680664
    },
    {
      "epoch": 0.10211382113821138,
      "step": 471,
      "training_loss": 6.571417808532715
    },
    {
      "epoch": 0.10211382113821138,
      "step": 471,
      "training_loss": 8.569228172302246
    },
    {
      "epoch": 0.10233062330623306,
      "grad_norm": 5.495779514312744,
      "learning_rate": 1e-05,
      "loss": 7.4995,
      "step": 472
    },
    {
      "epoch": 0.10233062330623306,
      "step": 472,
      "training_loss": 7.3555755615234375
    },
    {
      "epoch": 0.10233062330623306,
      "step": 472,
      "training_loss": 7.581902027130127
    },
    {
      "epoch": 0.10233062330623306,
      "step": 472,
      "training_loss": 7.661810398101807
    },
    {
      "epoch": 0.10233062330623306,
      "step": 472,
      "training_loss": 8.451849937438965
    },
    {
      "epoch": 0.10254742547425474,
      "step": 473,
      "training_loss": 7.544458389282227
    },
    {
      "epoch": 0.10254742547425474,
      "step": 473,
      "training_loss": 7.733592510223389
    },
    {
      "epoch": 0.10254742547425474,
      "step": 473,
      "training_loss": 8.055648803710938
    },
    {
      "epoch": 0.10254742547425474,
      "step": 473,
      "training_loss": 6.366407871246338
    },
    {
      "epoch": 0.10276422764227643,
      "step": 474,
      "training_loss": 8.399275779724121
    },
    {
      "epoch": 0.10276422764227643,
      "step": 474,
      "training_loss": 7.604538440704346
    },
    {
      "epoch": 0.10276422764227643,
      "step": 474,
      "training_loss": 7.138541221618652
    },
    {
      "epoch": 0.10276422764227643,
      "step": 474,
      "training_loss": 7.221652030944824
    },
    {
      "epoch": 0.10298102981029811,
      "step": 475,
      "training_loss": 6.910350322723389
    },
    {
      "epoch": 0.10298102981029811,
      "step": 475,
      "training_loss": 7.670030117034912
    },
    {
      "epoch": 0.10298102981029811,
      "step": 475,
      "training_loss": 6.891989707946777
    },
    {
      "epoch": 0.10298102981029811,
      "step": 475,
      "training_loss": 7.760045051574707
    },
    {
      "epoch": 0.10319783197831979,
      "grad_norm": 7.496448040008545,
      "learning_rate": 1e-05,
      "loss": 7.5217,
      "step": 476
    },
    {
      "epoch": 0.10319783197831979,
      "step": 476,
      "training_loss": 5.98838472366333
    },
    {
      "epoch": 0.10319783197831979,
      "step": 476,
      "training_loss": 6.972364902496338
    },
    {
      "epoch": 0.10319783197831979,
      "step": 476,
      "training_loss": 8.017916679382324
    },
    {
      "epoch": 0.10319783197831979,
      "step": 476,
      "training_loss": 6.72564697265625
    },
    {
      "epoch": 0.10341463414634146,
      "step": 477,
      "training_loss": 7.107974529266357
    },
    {
      "epoch": 0.10341463414634146,
      "step": 477,
      "training_loss": 7.51851749420166
    },
    {
      "epoch": 0.10341463414634146,
      "step": 477,
      "training_loss": 8.02220630645752
    },
    {
      "epoch": 0.10341463414634146,
      "step": 477,
      "training_loss": 6.731963157653809
    },
    {
      "epoch": 0.10363143631436314,
      "step": 478,
      "training_loss": 7.734429836273193
    },
    {
      "epoch": 0.10363143631436314,
      "step": 478,
      "training_loss": 7.6751532554626465
    },
    {
      "epoch": 0.10363143631436314,
      "step": 478,
      "training_loss": 8.270689964294434
    },
    {
      "epoch": 0.10363143631436314,
      "step": 478,
      "training_loss": 8.36730670928955
    },
    {
      "epoch": 0.10384823848238482,
      "step": 479,
      "training_loss": 8.998497009277344
    },
    {
      "epoch": 0.10384823848238482,
      "step": 479,
      "training_loss": 8.339462280273438
    },
    {
      "epoch": 0.10384823848238482,
      "step": 479,
      "training_loss": 8.456426620483398
    },
    {
      "epoch": 0.10384823848238482,
      "step": 479,
      "training_loss": 6.2014360427856445
    },
    {
      "epoch": 0.1040650406504065,
      "grad_norm": 25.53122901916504,
      "learning_rate": 1e-05,
      "loss": 7.5705,
      "step": 480
    },
    {
      "epoch": 0.1040650406504065,
      "step": 480,
      "training_loss": 6.7765889167785645
    },
    {
      "epoch": 0.1040650406504065,
      "step": 480,
      "training_loss": 5.939384937286377
    },
    {
      "epoch": 0.1040650406504065,
      "step": 480,
      "training_loss": 7.431704044342041
    },
    {
      "epoch": 0.1040650406504065,
      "step": 480,
      "training_loss": 7.0311503410339355
    },
    {
      "epoch": 0.10428184281842819,
      "step": 481,
      "training_loss": 7.85010290145874
    },
    {
      "epoch": 0.10428184281842819,
      "step": 481,
      "training_loss": 8.657299995422363
    },
    {
      "epoch": 0.10428184281842819,
      "step": 481,
      "training_loss": 7.776172637939453
    },
    {
      "epoch": 0.10428184281842819,
      "step": 481,
      "training_loss": 8.400893211364746
    },
    {
      "epoch": 0.10449864498644987,
      "step": 482,
      "training_loss": 7.41817045211792
    },
    {
      "epoch": 0.10449864498644987,
      "step": 482,
      "training_loss": 5.291060447692871
    },
    {
      "epoch": 0.10449864498644987,
      "step": 482,
      "training_loss": 6.7957353591918945
    },
    {
      "epoch": 0.10449864498644987,
      "step": 482,
      "training_loss": 7.213968753814697
    },
    {
      "epoch": 0.10471544715447155,
      "step": 483,
      "training_loss": 7.194128513336182
    },
    {
      "epoch": 0.10471544715447155,
      "step": 483,
      "training_loss": 8.623279571533203
    },
    {
      "epoch": 0.10471544715447155,
      "step": 483,
      "training_loss": 7.415239334106445
    },
    {
      "epoch": 0.10471544715447155,
      "step": 483,
      "training_loss": 7.158830642700195
    },
    {
      "epoch": 0.10493224932249323,
      "grad_norm": 8.475302696228027,
      "learning_rate": 1e-05,
      "loss": 7.3109,
      "step": 484
    },
    {
      "epoch": 0.10493224932249323,
      "step": 484,
      "training_loss": 6.010516166687012
    },
    {
      "epoch": 0.10493224932249323,
      "step": 484,
      "training_loss": 6.4728193283081055
    },
    {
      "epoch": 0.10493224932249323,
      "step": 484,
      "training_loss": 7.426761150360107
    },
    {
      "epoch": 0.10493224932249323,
      "step": 484,
      "training_loss": 7.637828350067139
    },
    {
      "epoch": 0.1051490514905149,
      "step": 485,
      "training_loss": 7.0229811668396
    },
    {
      "epoch": 0.1051490514905149,
      "step": 485,
      "training_loss": 7.485072612762451
    },
    {
      "epoch": 0.1051490514905149,
      "step": 485,
      "training_loss": 7.580550193786621
    },
    {
      "epoch": 0.1051490514905149,
      "step": 485,
      "training_loss": 8.183653831481934
    },
    {
      "epoch": 0.10536585365853658,
      "step": 486,
      "training_loss": 8.61182975769043
    },
    {
      "epoch": 0.10536585365853658,
      "step": 486,
      "training_loss": 7.392693519592285
    },
    {
      "epoch": 0.10536585365853658,
      "step": 486,
      "training_loss": 7.6934428215026855
    },
    {
      "epoch": 0.10536585365853658,
      "step": 486,
      "training_loss": 7.78805685043335
    },
    {
      "epoch": 0.10558265582655826,
      "step": 487,
      "training_loss": 6.823986530303955
    },
    {
      "epoch": 0.10558265582655826,
      "step": 487,
      "training_loss": 5.321688652038574
    },
    {
      "epoch": 0.10558265582655826,
      "step": 487,
      "training_loss": 8.120028495788574
    },
    {
      "epoch": 0.10558265582655826,
      "step": 487,
      "training_loss": 6.619423866271973
    },
    {
      "epoch": 0.10579945799457995,
      "grad_norm": 9.314112663269043,
      "learning_rate": 1e-05,
      "loss": 7.262,
      "step": 488
    },
    {
      "epoch": 0.10579945799457995,
      "step": 488,
      "training_loss": 6.464946746826172
    },
    {
      "epoch": 0.10579945799457995,
      "step": 488,
      "training_loss": 7.431856632232666
    },
    {
      "epoch": 0.10579945799457995,
      "step": 488,
      "training_loss": 6.49228572845459
    },
    {
      "epoch": 0.10579945799457995,
      "step": 488,
      "training_loss": 7.5033674240112305
    },
    {
      "epoch": 0.10601626016260163,
      "step": 489,
      "training_loss": 5.852793216705322
    },
    {
      "epoch": 0.10601626016260163,
      "step": 489,
      "training_loss": 7.260061740875244
    },
    {
      "epoch": 0.10601626016260163,
      "step": 489,
      "training_loss": 7.412514686584473
    },
    {
      "epoch": 0.10601626016260163,
      "step": 489,
      "training_loss": 6.1333746910095215
    },
    {
      "epoch": 0.10623306233062331,
      "step": 490,
      "training_loss": 7.116766452789307
    },
    {
      "epoch": 0.10623306233062331,
      "step": 490,
      "training_loss": 6.125434398651123
    },
    {
      "epoch": 0.10623306233062331,
      "step": 490,
      "training_loss": 6.8508477210998535
    },
    {
      "epoch": 0.10623306233062331,
      "step": 490,
      "training_loss": 7.747311115264893
    },
    {
      "epoch": 0.10644986449864499,
      "step": 491,
      "training_loss": 8.02690601348877
    },
    {
      "epoch": 0.10644986449864499,
      "step": 491,
      "training_loss": 7.734346866607666
    },
    {
      "epoch": 0.10644986449864499,
      "step": 491,
      "training_loss": 7.265091896057129
    },
    {
      "epoch": 0.10644986449864499,
      "step": 491,
      "training_loss": 7.37477445602417
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 6.400746822357178,
      "learning_rate": 1e-05,
      "loss": 7.0495,
      "step": 492
    },
    {
      "epoch": 0.10666666666666667,
      "step": 492,
      "training_loss": 7.530344009399414
    },
    {
      "epoch": 0.10666666666666667,
      "step": 492,
      "training_loss": 6.486377716064453
    },
    {
      "epoch": 0.10666666666666667,
      "step": 492,
      "training_loss": 6.711554527282715
    },
    {
      "epoch": 0.10666666666666667,
      "step": 492,
      "training_loss": 8.266678810119629
    },
    {
      "epoch": 0.10688346883468834,
      "step": 493,
      "training_loss": 7.165747165679932
    },
    {
      "epoch": 0.10688346883468834,
      "step": 493,
      "training_loss": 6.487946033477783
    },
    {
      "epoch": 0.10688346883468834,
      "step": 493,
      "training_loss": 7.897132396697998
    },
    {
      "epoch": 0.10688346883468834,
      "step": 493,
      "training_loss": 7.752471923828125
    },
    {
      "epoch": 0.10710027100271002,
      "step": 494,
      "training_loss": 8.042768478393555
    },
    {
      "epoch": 0.10710027100271002,
      "step": 494,
      "training_loss": 7.82958984375
    },
    {
      "epoch": 0.10710027100271002,
      "step": 494,
      "training_loss": 8.048827171325684
    },
    {
      "epoch": 0.10710027100271002,
      "step": 494,
      "training_loss": 7.088045120239258
    },
    {
      "epoch": 0.1073170731707317,
      "step": 495,
      "training_loss": 7.428301811218262
    },
    {
      "epoch": 0.1073170731707317,
      "step": 495,
      "training_loss": 7.58991003036499
    },
    {
      "epoch": 0.1073170731707317,
      "step": 495,
      "training_loss": 7.183356285095215
    },
    {
      "epoch": 0.1073170731707317,
      "step": 495,
      "training_loss": 7.424529075622559
    },
    {
      "epoch": 0.10753387533875339,
      "grad_norm": 5.952877044677734,
      "learning_rate": 1e-05,
      "loss": 7.4333,
      "step": 496
    },
    {
      "epoch": 0.10753387533875339,
      "step": 496,
      "training_loss": 7.634365558624268
    },
    {
      "epoch": 0.10753387533875339,
      "step": 496,
      "training_loss": 7.395888328552246
    },
    {
      "epoch": 0.10753387533875339,
      "step": 496,
      "training_loss": 6.1642680168151855
    },
    {
      "epoch": 0.10753387533875339,
      "step": 496,
      "training_loss": 11.371374130249023
    },
    {
      "epoch": 0.10775067750677507,
      "step": 497,
      "training_loss": 7.7658867835998535
    },
    {
      "epoch": 0.10775067750677507,
      "step": 497,
      "training_loss": 7.91407585144043
    },
    {
      "epoch": 0.10775067750677507,
      "step": 497,
      "training_loss": 7.287159442901611
    },
    {
      "epoch": 0.10775067750677507,
      "step": 497,
      "training_loss": 7.620785713195801
    },
    {
      "epoch": 0.10796747967479675,
      "step": 498,
      "training_loss": 6.069972515106201
    },
    {
      "epoch": 0.10796747967479675,
      "step": 498,
      "training_loss": 6.981333255767822
    },
    {
      "epoch": 0.10796747967479675,
      "step": 498,
      "training_loss": 8.453526496887207
    },
    {
      "epoch": 0.10796747967479675,
      "step": 498,
      "training_loss": 7.676165580749512
    },
    {
      "epoch": 0.10818428184281843,
      "step": 499,
      "training_loss": 7.617778778076172
    },
    {
      "epoch": 0.10818428184281843,
      "step": 499,
      "training_loss": 7.226446628570557
    },
    {
      "epoch": 0.10818428184281843,
      "step": 499,
      "training_loss": 6.792215347290039
    },
    {
      "epoch": 0.10818428184281843,
      "step": 499,
      "training_loss": 7.699793338775635
    },
    {
      "epoch": 0.10840108401084012,
      "grad_norm": 6.784480571746826,
      "learning_rate": 1e-05,
      "loss": 7.6044,
      "step": 500
    },
    {
      "epoch": 0.10840108401084012,
      "step": 500,
      "training_loss": 6.936556816101074
    },
    {
      "epoch": 0.10840108401084012,
      "step": 500,
      "training_loss": 7.314278602600098
    },
    {
      "epoch": 0.10840108401084012,
      "step": 500,
      "training_loss": 6.333626747131348
    },
    {
      "epoch": 0.10840108401084012,
      "step": 500,
      "training_loss": 7.541306018829346
    },
    {
      "epoch": 0.10861788617886178,
      "step": 501,
      "training_loss": 7.859274864196777
    },
    {
      "epoch": 0.10861788617886178,
      "step": 501,
      "training_loss": 7.090661525726318
    },
    {
      "epoch": 0.10861788617886178,
      "step": 501,
      "training_loss": 7.179004192352295
    },
    {
      "epoch": 0.10861788617886178,
      "step": 501,
      "training_loss": 7.451000690460205
    },
    {
      "epoch": 0.10883468834688347,
      "step": 502,
      "training_loss": 8.14928150177002
    },
    {
      "epoch": 0.10883468834688347,
      "step": 502,
      "training_loss": 6.634354114532471
    },
    {
      "epoch": 0.10883468834688347,
      "step": 502,
      "training_loss": 8.010232925415039
    },
    {
      "epoch": 0.10883468834688347,
      "step": 502,
      "training_loss": 6.803582668304443
    },
    {
      "epoch": 0.10905149051490515,
      "step": 503,
      "training_loss": 6.186071395874023
    },
    {
      "epoch": 0.10905149051490515,
      "step": 503,
      "training_loss": 6.031855583190918
    },
    {
      "epoch": 0.10905149051490515,
      "step": 503,
      "training_loss": 9.090021133422852
    },
    {
      "epoch": 0.10905149051490515,
      "step": 503,
      "training_loss": 6.947539329528809
    },
    {
      "epoch": 0.10926829268292683,
      "grad_norm": 10.279557228088379,
      "learning_rate": 1e-05,
      "loss": 7.2224,
      "step": 504
    },
    {
      "epoch": 0.10926829268292683,
      "step": 504,
      "training_loss": 7.197842121124268
    },
    {
      "epoch": 0.10926829268292683,
      "step": 504,
      "training_loss": 5.550083637237549
    },
    {
      "epoch": 0.10926829268292683,
      "step": 504,
      "training_loss": 7.927826881408691
    },
    {
      "epoch": 0.10926829268292683,
      "step": 504,
      "training_loss": 8.098876953125
    },
    {
      "epoch": 0.10948509485094851,
      "step": 505,
      "training_loss": 7.544885158538818
    },
    {
      "epoch": 0.10948509485094851,
      "step": 505,
      "training_loss": 5.2315802574157715
    },
    {
      "epoch": 0.10948509485094851,
      "step": 505,
      "training_loss": 6.434506416320801
    },
    {
      "epoch": 0.10948509485094851,
      "step": 505,
      "training_loss": 7.8484296798706055
    },
    {
      "epoch": 0.1097018970189702,
      "step": 506,
      "training_loss": 7.890492916107178
    },
    {
      "epoch": 0.1097018970189702,
      "step": 506,
      "training_loss": 6.7303290367126465
    },
    {
      "epoch": 0.1097018970189702,
      "step": 506,
      "training_loss": 8.096400260925293
    },
    {
      "epoch": 0.1097018970189702,
      "step": 506,
      "training_loss": 8.270633697509766
    },
    {
      "epoch": 0.10991869918699188,
      "step": 507,
      "training_loss": 6.146450519561768
    },
    {
      "epoch": 0.10991869918699188,
      "step": 507,
      "training_loss": 6.086196422576904
    },
    {
      "epoch": 0.10991869918699188,
      "step": 507,
      "training_loss": 7.130472183227539
    },
    {
      "epoch": 0.10991869918699188,
      "step": 507,
      "training_loss": 9.620548248291016
    },
    {
      "epoch": 0.11013550135501356,
      "grad_norm": 11.04758358001709,
      "learning_rate": 1e-05,
      "loss": 7.2378,
      "step": 508
    },
    {
      "epoch": 0.11013550135501356,
      "step": 508,
      "training_loss": 7.292830467224121
    },
    {
      "epoch": 0.11013550135501356,
      "step": 508,
      "training_loss": 7.308938503265381
    },
    {
      "epoch": 0.11013550135501356,
      "step": 508,
      "training_loss": 8.264904975891113
    },
    {
      "epoch": 0.11013550135501356,
      "step": 508,
      "training_loss": 7.9970855712890625
    },
    {
      "epoch": 0.11035230352303523,
      "step": 509,
      "training_loss": 7.1894073486328125
    },
    {
      "epoch": 0.11035230352303523,
      "step": 509,
      "training_loss": 7.078858852386475
    },
    {
      "epoch": 0.11035230352303523,
      "step": 509,
      "training_loss": 7.143139362335205
    },
    {
      "epoch": 0.11035230352303523,
      "step": 509,
      "training_loss": 7.0466766357421875
    },
    {
      "epoch": 0.11056910569105691,
      "step": 510,
      "training_loss": 7.1531219482421875
    },
    {
      "epoch": 0.11056910569105691,
      "step": 510,
      "training_loss": 7.736265659332275
    },
    {
      "epoch": 0.11056910569105691,
      "step": 510,
      "training_loss": 7.479001522064209
    },
    {
      "epoch": 0.11056910569105691,
      "step": 510,
      "training_loss": 7.206132411956787
    },
    {
      "epoch": 0.11078590785907859,
      "step": 511,
      "training_loss": 6.567325115203857
    },
    {
      "epoch": 0.11078590785907859,
      "step": 511,
      "training_loss": 9.430864334106445
    },
    {
      "epoch": 0.11078590785907859,
      "step": 511,
      "training_loss": 7.865579605102539
    },
    {
      "epoch": 0.11078590785907859,
      "step": 511,
      "training_loss": 7.944159984588623
    },
    {
      "epoch": 0.11100271002710027,
      "grad_norm": 10.99915599822998,
      "learning_rate": 1e-05,
      "loss": 7.544,
      "step": 512
    },
    {
      "epoch": 0.11100271002710027,
      "step": 512,
      "training_loss": 7.875278472900391
    },
    {
      "epoch": 0.11100271002710027,
      "step": 512,
      "training_loss": 7.499728679656982
    },
    {
      "epoch": 0.11100271002710027,
      "step": 512,
      "training_loss": 7.56744384765625
    },
    {
      "epoch": 0.11100271002710027,
      "step": 512,
      "training_loss": 6.49501895904541
    },
    {
      "epoch": 0.11121951219512195,
      "step": 513,
      "training_loss": 7.927114486694336
    },
    {
      "epoch": 0.11121951219512195,
      "step": 513,
      "training_loss": 7.889367580413818
    },
    {
      "epoch": 0.11121951219512195,
      "step": 513,
      "training_loss": 7.124241352081299
    },
    {
      "epoch": 0.11121951219512195,
      "step": 513,
      "training_loss": 7.091158390045166
    },
    {
      "epoch": 0.11143631436314363,
      "step": 514,
      "training_loss": 7.612726211547852
    },
    {
      "epoch": 0.11143631436314363,
      "step": 514,
      "training_loss": 7.023258686065674
    },
    {
      "epoch": 0.11143631436314363,
      "step": 514,
      "training_loss": 7.678445339202881
    },
    {
      "epoch": 0.11143631436314363,
      "step": 514,
      "training_loss": 6.276734352111816
    },
    {
      "epoch": 0.11165311653116532,
      "step": 515,
      "training_loss": 7.79238748550415
    },
    {
      "epoch": 0.11165311653116532,
      "step": 515,
      "training_loss": 7.798552989959717
    },
    {
      "epoch": 0.11165311653116532,
      "step": 515,
      "training_loss": 8.04211711883545
    },
    {
      "epoch": 0.11165311653116532,
      "step": 515,
      "training_loss": 7.001851558685303
    },
    {
      "epoch": 0.111869918699187,
      "grad_norm": 6.438345432281494,
      "learning_rate": 1e-05,
      "loss": 7.4185,
      "step": 516
    },
    {
      "epoch": 0.111869918699187,
      "step": 516,
      "training_loss": 7.143211841583252
    },
    {
      "epoch": 0.111869918699187,
      "step": 516,
      "training_loss": 9.173112869262695
    },
    {
      "epoch": 0.111869918699187,
      "step": 516,
      "training_loss": 7.394917011260986
    },
    {
      "epoch": 0.111869918699187,
      "step": 516,
      "training_loss": 7.45618200302124
    },
    {
      "epoch": 0.11208672086720867,
      "step": 517,
      "training_loss": 7.865028381347656
    },
    {
      "epoch": 0.11208672086720867,
      "step": 517,
      "training_loss": 8.293980598449707
    },
    {
      "epoch": 0.11208672086720867,
      "step": 517,
      "training_loss": 6.5899176597595215
    },
    {
      "epoch": 0.11208672086720867,
      "step": 517,
      "training_loss": 6.7772955894470215
    },
    {
      "epoch": 0.11230352303523035,
      "step": 518,
      "training_loss": 7.769937992095947
    },
    {
      "epoch": 0.11230352303523035,
      "step": 518,
      "training_loss": 7.875511169433594
    },
    {
      "epoch": 0.11230352303523035,
      "step": 518,
      "training_loss": 6.373845100402832
    },
    {
      "epoch": 0.11230352303523035,
      "step": 518,
      "training_loss": 6.6106672286987305
    },
    {
      "epoch": 0.11252032520325203,
      "step": 519,
      "training_loss": 8.324528694152832
    },
    {
      "epoch": 0.11252032520325203,
      "step": 519,
      "training_loss": 7.267722129821777
    },
    {
      "epoch": 0.11252032520325203,
      "step": 519,
      "training_loss": 6.403731822967529
    },
    {
      "epoch": 0.11252032520325203,
      "step": 519,
      "training_loss": 7.540292739868164
    },
    {
      "epoch": 0.11273712737127371,
      "grad_norm": 9.208507537841797,
      "learning_rate": 1e-05,
      "loss": 7.4287,
      "step": 520
    },
    {
      "epoch": 0.11273712737127371,
      "step": 520,
      "training_loss": 6.188186168670654
    },
    {
      "epoch": 0.11273712737127371,
      "step": 520,
      "training_loss": 7.074416637420654
    },
    {
      "epoch": 0.11273712737127371,
      "step": 520,
      "training_loss": 6.974092483520508
    },
    {
      "epoch": 0.11273712737127371,
      "step": 520,
      "training_loss": 7.678687572479248
    },
    {
      "epoch": 0.1129539295392954,
      "step": 521,
      "training_loss": 6.949998378753662
    },
    {
      "epoch": 0.1129539295392954,
      "step": 521,
      "training_loss": 7.662763595581055
    },
    {
      "epoch": 0.1129539295392954,
      "step": 521,
      "training_loss": 7.934101581573486
    },
    {
      "epoch": 0.1129539295392954,
      "step": 521,
      "training_loss": 7.031049728393555
    },
    {
      "epoch": 0.11317073170731708,
      "step": 522,
      "training_loss": 8.691899299621582
    },
    {
      "epoch": 0.11317073170731708,
      "step": 522,
      "training_loss": 7.6412129402160645
    },
    {
      "epoch": 0.11317073170731708,
      "step": 522,
      "training_loss": 7.41140079498291
    },
    {
      "epoch": 0.11317073170731708,
      "step": 522,
      "training_loss": 7.246796131134033
    },
    {
      "epoch": 0.11338753387533876,
      "step": 523,
      "training_loss": 7.197312831878662
    },
    {
      "epoch": 0.11338753387533876,
      "step": 523,
      "training_loss": 7.531804084777832
    },
    {
      "epoch": 0.11338753387533876,
      "step": 523,
      "training_loss": 8.101283073425293
    },
    {
      "epoch": 0.11338753387533876,
      "step": 523,
      "training_loss": 6.982879638671875
    },
    {
      "epoch": 0.11360433604336044,
      "grad_norm": 7.2206597328186035,
      "learning_rate": 1e-05,
      "loss": 7.3936,
      "step": 524
    },
    {
      "epoch": 0.11360433604336044,
      "step": 524,
      "training_loss": 5.994927883148193
    },
    {
      "epoch": 0.11360433604336044,
      "step": 524,
      "training_loss": 7.510817050933838
    },
    {
      "epoch": 0.11360433604336044,
      "step": 524,
      "training_loss": 7.491668224334717
    },
    {
      "epoch": 0.11360433604336044,
      "step": 524,
      "training_loss": 8.631285667419434
    },
    {
      "epoch": 0.11382113821138211,
      "step": 525,
      "training_loss": 7.820464134216309
    },
    {
      "epoch": 0.11382113821138211,
      "step": 525,
      "training_loss": 8.025981903076172
    },
    {
      "epoch": 0.11382113821138211,
      "step": 525,
      "training_loss": 8.016201972961426
    },
    {
      "epoch": 0.11382113821138211,
      "step": 525,
      "training_loss": 7.091838836669922
    },
    {
      "epoch": 0.11403794037940379,
      "step": 526,
      "training_loss": 7.195727348327637
    },
    {
      "epoch": 0.11403794037940379,
      "step": 526,
      "training_loss": 6.70258903503418
    },
    {
      "epoch": 0.11403794037940379,
      "step": 526,
      "training_loss": 7.134093761444092
    },
    {
      "epoch": 0.11403794037940379,
      "step": 526,
      "training_loss": 7.5409836769104
    },
    {
      "epoch": 0.11425474254742547,
      "step": 527,
      "training_loss": 7.384543418884277
    },
    {
      "epoch": 0.11425474254742547,
      "step": 527,
      "training_loss": 8.114914894104004
    },
    {
      "epoch": 0.11425474254742547,
      "step": 527,
      "training_loss": 7.395365238189697
    },
    {
      "epoch": 0.11425474254742547,
      "step": 527,
      "training_loss": 7.335023403167725
    },
    {
      "epoch": 0.11447154471544715,
      "grad_norm": 8.254216194152832,
      "learning_rate": 1e-05,
      "loss": 7.4617,
      "step": 528
    },
    {
      "epoch": 0.11447154471544715,
      "step": 528,
      "training_loss": 7.49200439453125
    },
    {
      "epoch": 0.11447154471544715,
      "step": 528,
      "training_loss": 7.4064531326293945
    },
    {
      "epoch": 0.11447154471544715,
      "step": 528,
      "training_loss": 7.799133777618408
    },
    {
      "epoch": 0.11447154471544715,
      "step": 528,
      "training_loss": 7.238255500793457
    },
    {
      "epoch": 0.11468834688346884,
      "step": 529,
      "training_loss": 7.0656890869140625
    },
    {
      "epoch": 0.11468834688346884,
      "step": 529,
      "training_loss": 6.918032169342041
    },
    {
      "epoch": 0.11468834688346884,
      "step": 529,
      "training_loss": 7.360342979431152
    },
    {
      "epoch": 0.11468834688346884,
      "step": 529,
      "training_loss": 7.714888095855713
    },
    {
      "epoch": 0.11490514905149052,
      "step": 530,
      "training_loss": 7.256121635437012
    },
    {
      "epoch": 0.11490514905149052,
      "step": 530,
      "training_loss": 7.048753261566162
    },
    {
      "epoch": 0.11490514905149052,
      "step": 530,
      "training_loss": 7.488709449768066
    },
    {
      "epoch": 0.11490514905149052,
      "step": 530,
      "training_loss": 8.110106468200684
    },
    {
      "epoch": 0.1151219512195122,
      "step": 531,
      "training_loss": 6.803780555725098
    },
    {
      "epoch": 0.1151219512195122,
      "step": 531,
      "training_loss": 7.6590895652771
    },
    {
      "epoch": 0.1151219512195122,
      "step": 531,
      "training_loss": 8.994193077087402
    },
    {
      "epoch": 0.1151219512195122,
      "step": 531,
      "training_loss": 7.455229759216309
    },
    {
      "epoch": 0.11533875338753388,
      "grad_norm": 8.605949401855469,
      "learning_rate": 1e-05,
      "loss": 7.4882,
      "step": 532
    },
    {
      "epoch": 0.11533875338753388,
      "step": 532,
      "training_loss": 6.872522830963135
    },
    {
      "epoch": 0.11533875338753388,
      "step": 532,
      "training_loss": 6.711930751800537
    },
    {
      "epoch": 0.11533875338753388,
      "step": 532,
      "training_loss": 8.425943374633789
    },
    {
      "epoch": 0.11533875338753388,
      "step": 532,
      "training_loss": 6.626667499542236
    },
    {
      "epoch": 0.11555555555555555,
      "step": 533,
      "training_loss": 7.283633232116699
    },
    {
      "epoch": 0.11555555555555555,
      "step": 533,
      "training_loss": 7.237564563751221
    },
    {
      "epoch": 0.11555555555555555,
      "step": 533,
      "training_loss": 7.342020511627197
    },
    {
      "epoch": 0.11555555555555555,
      "step": 533,
      "training_loss": 7.193355560302734
    },
    {
      "epoch": 0.11577235772357723,
      "step": 534,
      "training_loss": 7.633035659790039
    },
    {
      "epoch": 0.11577235772357723,
      "step": 534,
      "training_loss": 8.704571723937988
    },
    {
      "epoch": 0.11577235772357723,
      "step": 534,
      "training_loss": 7.387469291687012
    },
    {
      "epoch": 0.11577235772357723,
      "step": 534,
      "training_loss": 6.8588104248046875
    },
    {
      "epoch": 0.11598915989159891,
      "step": 535,
      "training_loss": 7.158219814300537
    },
    {
      "epoch": 0.11598915989159891,
      "step": 535,
      "training_loss": 5.944448471069336
    },
    {
      "epoch": 0.11598915989159891,
      "step": 535,
      "training_loss": 7.678129196166992
    },
    {
      "epoch": 0.11598915989159891,
      "step": 535,
      "training_loss": 6.531997203826904
    },
    {
      "epoch": 0.1162059620596206,
      "grad_norm": 12.98642635345459,
      "learning_rate": 1e-05,
      "loss": 7.2244,
      "step": 536
    },
    {
      "epoch": 0.1162059620596206,
      "step": 536,
      "training_loss": 6.5984039306640625
    },
    {
      "epoch": 0.1162059620596206,
      "step": 536,
      "training_loss": 7.342191219329834
    },
    {
      "epoch": 0.1162059620596206,
      "step": 536,
      "training_loss": 6.002650737762451
    },
    {
      "epoch": 0.1162059620596206,
      "step": 536,
      "training_loss": 7.219787120819092
    },
    {
      "epoch": 0.11642276422764228,
      "step": 537,
      "training_loss": 7.468268394470215
    },
    {
      "epoch": 0.11642276422764228,
      "step": 537,
      "training_loss": 8.031564712524414
    },
    {
      "epoch": 0.11642276422764228,
      "step": 537,
      "training_loss": 6.4438252449035645
    },
    {
      "epoch": 0.11642276422764228,
      "step": 537,
      "training_loss": 7.23739767074585
    },
    {
      "epoch": 0.11663956639566396,
      "step": 538,
      "training_loss": 6.673415184020996
    },
    {
      "epoch": 0.11663956639566396,
      "step": 538,
      "training_loss": 5.952277183532715
    },
    {
      "epoch": 0.11663956639566396,
      "step": 538,
      "training_loss": 6.126577377319336
    },
    {
      "epoch": 0.11663956639566396,
      "step": 538,
      "training_loss": 6.435783386230469
    },
    {
      "epoch": 0.11685636856368564,
      "step": 539,
      "training_loss": 5.865754127502441
    },
    {
      "epoch": 0.11685636856368564,
      "step": 539,
      "training_loss": 5.166076183319092
    },
    {
      "epoch": 0.11685636856368564,
      "step": 539,
      "training_loss": 7.044633865356445
    },
    {
      "epoch": 0.11685636856368564,
      "step": 539,
      "training_loss": 7.728236675262451
    },
    {
      "epoch": 0.11707317073170732,
      "grad_norm": 13.224288940429688,
      "learning_rate": 1e-05,
      "loss": 6.7086,
      "step": 540
    },
    {
      "epoch": 0.11707317073170732,
      "step": 540,
      "training_loss": 6.528472900390625
    },
    {
      "epoch": 0.11707317073170732,
      "step": 540,
      "training_loss": 7.1472487449646
    },
    {
      "epoch": 0.11707317073170732,
      "step": 540,
      "training_loss": 7.960062503814697
    },
    {
      "epoch": 0.11707317073170732,
      "step": 540,
      "training_loss": 8.45230484008789
    },
    {
      "epoch": 0.11728997289972899,
      "step": 541,
      "training_loss": 5.881997108459473
    },
    {
      "epoch": 0.11728997289972899,
      "step": 541,
      "training_loss": 7.3338236808776855
    },
    {
      "epoch": 0.11728997289972899,
      "step": 541,
      "training_loss": 5.855220317840576
    },
    {
      "epoch": 0.11728997289972899,
      "step": 541,
      "training_loss": 7.768827438354492
    },
    {
      "epoch": 0.11750677506775067,
      "step": 542,
      "training_loss": 6.52283239364624
    },
    {
      "epoch": 0.11750677506775067,
      "step": 542,
      "training_loss": 7.111107349395752
    },
    {
      "epoch": 0.11750677506775067,
      "step": 542,
      "training_loss": 6.901499271392822
    },
    {
      "epoch": 0.11750677506775067,
      "step": 542,
      "training_loss": 7.300691604614258
    },
    {
      "epoch": 0.11772357723577236,
      "step": 543,
      "training_loss": 8.309020042419434
    },
    {
      "epoch": 0.11772357723577236,
      "step": 543,
      "training_loss": 8.151947975158691
    },
    {
      "epoch": 0.11772357723577236,
      "step": 543,
      "training_loss": 5.975563049316406
    },
    {
      "epoch": 0.11772357723577236,
      "step": 543,
      "training_loss": 6.330888271331787
    },
    {
      "epoch": 0.11794037940379404,
      "grad_norm": 12.605628967285156,
      "learning_rate": 1e-05,
      "loss": 7.0957,
      "step": 544
    },
    {
      "epoch": 0.11794037940379404,
      "step": 544,
      "training_loss": 7.967072010040283
    },
    {
      "epoch": 0.11794037940379404,
      "step": 544,
      "training_loss": 7.832306861877441
    },
    {
      "epoch": 0.11794037940379404,
      "step": 544,
      "training_loss": 8.231558799743652
    },
    {
      "epoch": 0.11794037940379404,
      "step": 544,
      "training_loss": 7.048768520355225
    },
    {
      "epoch": 0.11815718157181572,
      "step": 545,
      "training_loss": 7.960262298583984
    },
    {
      "epoch": 0.11815718157181572,
      "step": 545,
      "training_loss": 7.344545364379883
    },
    {
      "epoch": 0.11815718157181572,
      "step": 545,
      "training_loss": 7.109943389892578
    },
    {
      "epoch": 0.11815718157181572,
      "step": 545,
      "training_loss": 4.733938694000244
    },
    {
      "epoch": 0.1183739837398374,
      "step": 546,
      "training_loss": 7.238724708557129
    },
    {
      "epoch": 0.1183739837398374,
      "step": 546,
      "training_loss": 8.260741233825684
    },
    {
      "epoch": 0.1183739837398374,
      "step": 546,
      "training_loss": 7.376689910888672
    },
    {
      "epoch": 0.1183739837398374,
      "step": 546,
      "training_loss": 7.577080249786377
    },
    {
      "epoch": 0.11859078590785908,
      "step": 547,
      "training_loss": 7.346119403839111
    },
    {
      "epoch": 0.11859078590785908,
      "step": 547,
      "training_loss": 7.154162883758545
    },
    {
      "epoch": 0.11859078590785908,
      "step": 547,
      "training_loss": 7.60212516784668
    },
    {
      "epoch": 0.11859078590785908,
      "step": 547,
      "training_loss": 7.365157127380371
    },
    {
      "epoch": 0.11880758807588077,
      "grad_norm": 9.049332618713379,
      "learning_rate": 1e-05,
      "loss": 7.3843,
      "step": 548
    },
    {
      "epoch": 0.11880758807588077,
      "step": 548,
      "training_loss": 7.4785919189453125
    },
    {
      "epoch": 0.11880758807588077,
      "step": 548,
      "training_loss": 7.48644495010376
    },
    {
      "epoch": 0.11880758807588077,
      "step": 548,
      "training_loss": 7.324497222900391
    },
    {
      "epoch": 0.11880758807588077,
      "step": 548,
      "training_loss": 6.539449214935303
    },
    {
      "epoch": 0.11902439024390243,
      "step": 549,
      "training_loss": 7.7453484535217285
    },
    {
      "epoch": 0.11902439024390243,
      "step": 549,
      "training_loss": 8.373583793640137
    },
    {
      "epoch": 0.11902439024390243,
      "step": 549,
      "training_loss": 5.793534278869629
    },
    {
      "epoch": 0.11902439024390243,
      "step": 549,
      "training_loss": 7.276280879974365
    },
    {
      "epoch": 0.11924119241192412,
      "step": 550,
      "training_loss": 7.802945137023926
    },
    {
      "epoch": 0.11924119241192412,
      "step": 550,
      "training_loss": 8.766831398010254
    },
    {
      "epoch": 0.11924119241192412,
      "step": 550,
      "training_loss": 7.4768781661987305
    },
    {
      "epoch": 0.11924119241192412,
      "step": 550,
      "training_loss": 9.5725736618042
    },
    {
      "epoch": 0.1194579945799458,
      "step": 551,
      "training_loss": 8.261630058288574
    },
    {
      "epoch": 0.1194579945799458,
      "step": 551,
      "training_loss": 7.059144496917725
    },
    {
      "epoch": 0.1194579945799458,
      "step": 551,
      "training_loss": 7.958282470703125
    },
    {
      "epoch": 0.1194579945799458,
      "step": 551,
      "training_loss": 6.153409481048584
    },
    {
      "epoch": 0.11967479674796748,
      "grad_norm": 11.152636528015137,
      "learning_rate": 1e-05,
      "loss": 7.5668,
      "step": 552
    },
    {
      "epoch": 0.11967479674796748,
      "step": 552,
      "training_loss": 7.548931121826172
    },
    {
      "epoch": 0.11967479674796748,
      "step": 552,
      "training_loss": 7.065240859985352
    },
    {
      "epoch": 0.11967479674796748,
      "step": 552,
      "training_loss": 8.215862274169922
    },
    {
      "epoch": 0.11967479674796748,
      "step": 552,
      "training_loss": 7.424259662628174
    },
    {
      "epoch": 0.11989159891598916,
      "step": 553,
      "training_loss": 6.2460222244262695
    },
    {
      "epoch": 0.11989159891598916,
      "step": 553,
      "training_loss": 5.549643039703369
    },
    {
      "epoch": 0.11989159891598916,
      "step": 553,
      "training_loss": 7.507060527801514
    },
    {
      "epoch": 0.11989159891598916,
      "step": 553,
      "training_loss": 7.211172103881836
    },
    {
      "epoch": 0.12010840108401084,
      "step": 554,
      "training_loss": 6.626739025115967
    },
    {
      "epoch": 0.12010840108401084,
      "step": 554,
      "training_loss": 7.4139251708984375
    },
    {
      "epoch": 0.12010840108401084,
      "step": 554,
      "training_loss": 6.043968677520752
    },
    {
      "epoch": 0.12010840108401084,
      "step": 554,
      "training_loss": 9.454302787780762
    },
    {
      "epoch": 0.12032520325203253,
      "step": 555,
      "training_loss": 5.712962627410889
    },
    {
      "epoch": 0.12032520325203253,
      "step": 555,
      "training_loss": 6.568973541259766
    },
    {
      "epoch": 0.12032520325203253,
      "step": 555,
      "training_loss": 8.186714172363281
    },
    {
      "epoch": 0.12032520325203253,
      "step": 555,
      "training_loss": 6.4905500411987305
    },
    {
      "epoch": 0.12054200542005421,
      "grad_norm": 8.562798500061035,
      "learning_rate": 1e-05,
      "loss": 7.0791,
      "step": 556
    },
    {
      "epoch": 0.12054200542005421,
      "step": 556,
      "training_loss": 7.399631023406982
    },
    {
      "epoch": 0.12054200542005421,
      "step": 556,
      "training_loss": 6.462893962860107
    },
    {
      "epoch": 0.12054200542005421,
      "step": 556,
      "training_loss": 7.647140979766846
    },
    {
      "epoch": 0.12054200542005421,
      "step": 556,
      "training_loss": 8.959585189819336
    },
    {
      "epoch": 0.12075880758807588,
      "step": 557,
      "training_loss": 7.3586039543151855
    },
    {
      "epoch": 0.12075880758807588,
      "step": 557,
      "training_loss": 6.308656215667725
    },
    {
      "epoch": 0.12075880758807588,
      "step": 557,
      "training_loss": 7.304111003875732
    },
    {
      "epoch": 0.12075880758807588,
      "step": 557,
      "training_loss": 7.803801536560059
    },
    {
      "epoch": 0.12097560975609756,
      "step": 558,
      "training_loss": 6.349695205688477
    },
    {
      "epoch": 0.12097560975609756,
      "step": 558,
      "training_loss": 7.3514580726623535
    },
    {
      "epoch": 0.12097560975609756,
      "step": 558,
      "training_loss": 7.856917381286621
    },
    {
      "epoch": 0.12097560975609756,
      "step": 558,
      "training_loss": 6.344872951507568
    },
    {
      "epoch": 0.12119241192411924,
      "step": 559,
      "training_loss": 10.208879470825195
    },
    {
      "epoch": 0.12119241192411924,
      "step": 559,
      "training_loss": 8.334113121032715
    },
    {
      "epoch": 0.12119241192411924,
      "step": 559,
      "training_loss": 8.131044387817383
    },
    {
      "epoch": 0.12119241192411924,
      "step": 559,
      "training_loss": 6.98945951461792
    },
    {
      "epoch": 0.12140921409214092,
      "grad_norm": 13.086893081665039,
      "learning_rate": 1e-05,
      "loss": 7.5507,
      "step": 560
    },
    {
      "epoch": 0.12140921409214092,
      "step": 560,
      "training_loss": 6.109250068664551
    },
    {
      "epoch": 0.12140921409214092,
      "step": 560,
      "training_loss": 6.0021820068359375
    },
    {
      "epoch": 0.12140921409214092,
      "step": 560,
      "training_loss": 7.3011908531188965
    },
    {
      "epoch": 0.12140921409214092,
      "step": 560,
      "training_loss": 6.9080095291137695
    },
    {
      "epoch": 0.1216260162601626,
      "step": 561,
      "training_loss": 5.985989570617676
    },
    {
      "epoch": 0.1216260162601626,
      "step": 561,
      "training_loss": 7.494702339172363
    },
    {
      "epoch": 0.1216260162601626,
      "step": 561,
      "training_loss": 7.356332302093506
    },
    {
      "epoch": 0.1216260162601626,
      "step": 561,
      "training_loss": 6.303308486938477
    },
    {
      "epoch": 0.12184281842818429,
      "step": 562,
      "training_loss": 6.847715854644775
    },
    {
      "epoch": 0.12184281842818429,
      "step": 562,
      "training_loss": 7.621277332305908
    },
    {
      "epoch": 0.12184281842818429,
      "step": 562,
      "training_loss": 7.958037853240967
    },
    {
      "epoch": 0.12184281842818429,
      "step": 562,
      "training_loss": 6.312685966491699
    },
    {
      "epoch": 0.12205962059620597,
      "step": 563,
      "training_loss": 7.9227399826049805
    },
    {
      "epoch": 0.12205962059620597,
      "step": 563,
      "training_loss": 6.91876745223999
    },
    {
      "epoch": 0.12205962059620597,
      "step": 563,
      "training_loss": 6.980694770812988
    },
    {
      "epoch": 0.12205962059620597,
      "step": 563,
      "training_loss": 6.127100944519043
    },
    {
      "epoch": 0.12227642276422765,
      "grad_norm": 9.478686332702637,
      "learning_rate": 1e-05,
      "loss": 6.8844,
      "step": 564
    },
    {
      "epoch": 0.12227642276422765,
      "step": 564,
      "training_loss": 7.785765171051025
    },
    {
      "epoch": 0.12227642276422765,
      "step": 564,
      "training_loss": 7.069398403167725
    },
    {
      "epoch": 0.12227642276422765,
      "step": 564,
      "training_loss": 7.624000549316406
    },
    {
      "epoch": 0.12227642276422765,
      "step": 564,
      "training_loss": 7.589402198791504
    },
    {
      "epoch": 0.12249322493224932,
      "step": 565,
      "training_loss": 6.71028470993042
    },
    {
      "epoch": 0.12249322493224932,
      "step": 565,
      "training_loss": 6.8040995597839355
    },
    {
      "epoch": 0.12249322493224932,
      "step": 565,
      "training_loss": 8.297085762023926
    },
    {
      "epoch": 0.12249322493224932,
      "step": 565,
      "training_loss": 6.994999885559082
    },
    {
      "epoch": 0.122710027100271,
      "step": 566,
      "training_loss": 6.133907318115234
    },
    {
      "epoch": 0.122710027100271,
      "step": 566,
      "training_loss": 6.354856491088867
    },
    {
      "epoch": 0.122710027100271,
      "step": 566,
      "training_loss": 6.07767391204834
    },
    {
      "epoch": 0.122710027100271,
      "step": 566,
      "training_loss": 7.313139915466309
    },
    {
      "epoch": 0.12292682926829268,
      "step": 567,
      "training_loss": 5.891556739807129
    },
    {
      "epoch": 0.12292682926829268,
      "step": 567,
      "training_loss": 7.023702621459961
    },
    {
      "epoch": 0.12292682926829268,
      "step": 567,
      "training_loss": 6.9758782386779785
    },
    {
      "epoch": 0.12292682926829268,
      "step": 567,
      "training_loss": 7.132079124450684
    },
    {
      "epoch": 0.12314363143631436,
      "grad_norm": 7.249563217163086,
      "learning_rate": 1e-05,
      "loss": 6.9861,
      "step": 568
    },
    {
      "epoch": 0.12314363143631436,
      "step": 568,
      "training_loss": 5.720625877380371
    },
    {
      "epoch": 0.12314363143631436,
      "step": 568,
      "training_loss": 7.588978290557861
    },
    {
      "epoch": 0.12314363143631436,
      "step": 568,
      "training_loss": 7.318280220031738
    },
    {
      "epoch": 0.12314363143631436,
      "step": 568,
      "training_loss": 7.8056640625
    },
    {
      "epoch": 0.12336043360433604,
      "step": 569,
      "training_loss": 6.798443794250488
    },
    {
      "epoch": 0.12336043360433604,
      "step": 569,
      "training_loss": 7.070276737213135
    },
    {
      "epoch": 0.12336043360433604,
      "step": 569,
      "training_loss": 7.421245098114014
    },
    {
      "epoch": 0.12336043360433604,
      "step": 569,
      "training_loss": 8.829109191894531
    },
    {
      "epoch": 0.12357723577235773,
      "step": 570,
      "training_loss": 7.762360572814941
    },
    {
      "epoch": 0.12357723577235773,
      "step": 570,
      "training_loss": 6.913220405578613
    },
    {
      "epoch": 0.12357723577235773,
      "step": 570,
      "training_loss": 7.664241790771484
    },
    {
      "epoch": 0.12357723577235773,
      "step": 570,
      "training_loss": 7.506215572357178
    },
    {
      "epoch": 0.12379403794037941,
      "step": 571,
      "training_loss": 7.45806884765625
    },
    {
      "epoch": 0.12379403794037941,
      "step": 571,
      "training_loss": 6.911649703979492
    },
    {
      "epoch": 0.12379403794037941,
      "step": 571,
      "training_loss": 7.029008865356445
    },
    {
      "epoch": 0.12379403794037941,
      "step": 571,
      "training_loss": 9.186979293823242
    },
    {
      "epoch": 0.12401084010840109,
      "grad_norm": 7.404519081115723,
      "learning_rate": 1e-05,
      "loss": 7.4365,
      "step": 572
    },
    {
      "epoch": 0.12401084010840109,
      "step": 572,
      "training_loss": 7.69561767578125
    },
    {
      "epoch": 0.12401084010840109,
      "step": 572,
      "training_loss": 7.253920078277588
    },
    {
      "epoch": 0.12401084010840109,
      "step": 572,
      "training_loss": 7.224653244018555
    },
    {
      "epoch": 0.12401084010840109,
      "step": 572,
      "training_loss": 7.014443874359131
    },
    {
      "epoch": 0.12422764227642276,
      "step": 573,
      "training_loss": 6.856091499328613
    },
    {
      "epoch": 0.12422764227642276,
      "step": 573,
      "training_loss": 7.475865840911865
    },
    {
      "epoch": 0.12422764227642276,
      "step": 573,
      "training_loss": 8.294323921203613
    },
    {
      "epoch": 0.12422764227642276,
      "step": 573,
      "training_loss": 6.822495937347412
    },
    {
      "epoch": 0.12444444444444444,
      "step": 574,
      "training_loss": 8.247729301452637
    },
    {
      "epoch": 0.12444444444444444,
      "step": 574,
      "training_loss": 7.257664680480957
    },
    {
      "epoch": 0.12444444444444444,
      "step": 574,
      "training_loss": 6.6761603355407715
    },
    {
      "epoch": 0.12444444444444444,
      "step": 574,
      "training_loss": 7.511137008666992
    },
    {
      "epoch": 0.12466124661246612,
      "step": 575,
      "training_loss": 7.891904354095459
    },
    {
      "epoch": 0.12466124661246612,
      "step": 575,
      "training_loss": 6.459043025970459
    },
    {
      "epoch": 0.12466124661246612,
      "step": 575,
      "training_loss": 6.4446587562561035
    },
    {
      "epoch": 0.12466124661246612,
      "step": 575,
      "training_loss": 6.46220064163208
    },
    {
      "epoch": 0.1248780487804878,
      "grad_norm": 13.324430465698242,
      "learning_rate": 1e-05,
      "loss": 7.2242,
      "step": 576
    },
    {
      "epoch": 0.1248780487804878,
      "step": 576,
      "training_loss": 7.240333080291748
    },
    {
      "epoch": 0.1248780487804878,
      "step": 576,
      "training_loss": 5.891350269317627
    },
    {
      "epoch": 0.1248780487804878,
      "step": 576,
      "training_loss": 8.440022468566895
    },
    {
      "epoch": 0.1248780487804878,
      "step": 576,
      "training_loss": 7.336248874664307
    },
    {
      "epoch": 0.12509485094850947,
      "step": 577,
      "training_loss": 8.08369255065918
    },
    {
      "epoch": 0.12509485094850947,
      "step": 577,
      "training_loss": 6.678980350494385
    },
    {
      "epoch": 0.12509485094850947,
      "step": 577,
      "training_loss": 7.388444423675537
    },
    {
      "epoch": 0.12509485094850947,
      "step": 577,
      "training_loss": 8.116167068481445
    },
    {
      "epoch": 0.12531165311653117,
      "step": 578,
      "training_loss": 7.985495090484619
    },
    {
      "epoch": 0.12531165311653117,
      "step": 578,
      "training_loss": 7.1554694175720215
    },
    {
      "epoch": 0.12531165311653117,
      "step": 578,
      "training_loss": 5.734152793884277
    },
    {
      "epoch": 0.12531165311653117,
      "step": 578,
      "training_loss": 7.836318492889404
    },
    {
      "epoch": 0.12552845528455284,
      "step": 579,
      "training_loss": 8.015624046325684
    },
    {
      "epoch": 0.12552845528455284,
      "step": 579,
      "training_loss": 7.644883155822754
    },
    {
      "epoch": 0.12552845528455284,
      "step": 579,
      "training_loss": 6.862589359283447
    },
    {
      "epoch": 0.12552845528455284,
      "step": 579,
      "training_loss": 7.435445308685303
    },
    {
      "epoch": 0.12574525745257453,
      "grad_norm": 7.489919185638428,
      "learning_rate": 1e-05,
      "loss": 7.3653,
      "step": 580
    },
    {
      "epoch": 0.12574525745257453,
      "step": 580,
      "training_loss": 6.606504440307617
    },
    {
      "epoch": 0.12574525745257453,
      "step": 580,
      "training_loss": 7.32516622543335
    },
    {
      "epoch": 0.12574525745257453,
      "step": 580,
      "training_loss": 5.255186080932617
    },
    {
      "epoch": 0.12574525745257453,
      "step": 580,
      "training_loss": 6.040641784667969
    },
    {
      "epoch": 0.1259620596205962,
      "step": 581,
      "training_loss": 7.831211566925049
    },
    {
      "epoch": 0.1259620596205962,
      "step": 581,
      "training_loss": 5.917975902557373
    },
    {
      "epoch": 0.1259620596205962,
      "step": 581,
      "training_loss": 6.239455699920654
    },
    {
      "epoch": 0.1259620596205962,
      "step": 581,
      "training_loss": 7.3138813972473145
    },
    {
      "epoch": 0.1261788617886179,
      "step": 582,
      "training_loss": 8.090262413024902
    },
    {
      "epoch": 0.1261788617886179,
      "step": 582,
      "training_loss": 8.045931816101074
    },
    {
      "epoch": 0.1261788617886179,
      "step": 582,
      "training_loss": 8.861750602722168
    },
    {
      "epoch": 0.1261788617886179,
      "step": 582,
      "training_loss": 8.141983032226562
    },
    {
      "epoch": 0.12639566395663956,
      "step": 583,
      "training_loss": 8.104612350463867
    },
    {
      "epoch": 0.12639566395663956,
      "step": 583,
      "training_loss": 6.6913981437683105
    },
    {
      "epoch": 0.12639566395663956,
      "step": 583,
      "training_loss": 8.990403175354004
    },
    {
      "epoch": 0.12639566395663956,
      "step": 583,
      "training_loss": 9.890838623046875
    },
    {
      "epoch": 0.12661246612466126,
      "grad_norm": 10.313264846801758,
      "learning_rate": 1e-05,
      "loss": 7.4592,
      "step": 584
    },
    {
      "epoch": 0.12661246612466126,
      "step": 584,
      "training_loss": 7.043955326080322
    },
    {
      "epoch": 0.12661246612466126,
      "step": 584,
      "training_loss": 7.812482833862305
    },
    {
      "epoch": 0.12661246612466126,
      "step": 584,
      "training_loss": 7.216663837432861
    },
    {
      "epoch": 0.12661246612466126,
      "step": 584,
      "training_loss": 7.2877197265625
    },
    {
      "epoch": 0.12682926829268293,
      "step": 585,
      "training_loss": 7.716070652008057
    },
    {
      "epoch": 0.12682926829268293,
      "step": 585,
      "training_loss": 7.508369445800781
    },
    {
      "epoch": 0.12682926829268293,
      "step": 585,
      "training_loss": 7.194730758666992
    },
    {
      "epoch": 0.12682926829268293,
      "step": 585,
      "training_loss": 7.643498420715332
    },
    {
      "epoch": 0.1270460704607046,
      "step": 586,
      "training_loss": 6.741654872894287
    },
    {
      "epoch": 0.1270460704607046,
      "step": 586,
      "training_loss": 5.544003009796143
    },
    {
      "epoch": 0.1270460704607046,
      "step": 586,
      "training_loss": 5.179015636444092
    },
    {
      "epoch": 0.1270460704607046,
      "step": 586,
      "training_loss": 8.52990436553955
    },
    {
      "epoch": 0.1272628726287263,
      "step": 587,
      "training_loss": 6.550567150115967
    },
    {
      "epoch": 0.1272628726287263,
      "step": 587,
      "training_loss": 7.098201274871826
    },
    {
      "epoch": 0.1272628726287263,
      "step": 587,
      "training_loss": 6.233389854431152
    },
    {
      "epoch": 0.1272628726287263,
      "step": 587,
      "training_loss": 7.521161079406738
    },
    {
      "epoch": 0.12747967479674796,
      "grad_norm": 8.909944534301758,
      "learning_rate": 1e-05,
      "loss": 7.0513,
      "step": 588
    },
    {
      "epoch": 0.12747967479674796,
      "step": 588,
      "training_loss": 7.2927165031433105
    },
    {
      "epoch": 0.12747967479674796,
      "step": 588,
      "training_loss": 7.610311031341553
    },
    {
      "epoch": 0.12747967479674796,
      "step": 588,
      "training_loss": 7.310665130615234
    },
    {
      "epoch": 0.12747967479674796,
      "step": 588,
      "training_loss": 8.091915130615234
    },
    {
      "epoch": 0.12769647696476966,
      "step": 589,
      "training_loss": 7.787126541137695
    },
    {
      "epoch": 0.12769647696476966,
      "step": 589,
      "training_loss": 7.051408767700195
    },
    {
      "epoch": 0.12769647696476966,
      "step": 589,
      "training_loss": 7.773773193359375
    },
    {
      "epoch": 0.12769647696476966,
      "step": 589,
      "training_loss": 7.184485912322998
    },
    {
      "epoch": 0.12791327913279132,
      "step": 590,
      "training_loss": 7.855920791625977
    },
    {
      "epoch": 0.12791327913279132,
      "step": 590,
      "training_loss": 7.103989601135254
    },
    {
      "epoch": 0.12791327913279132,
      "step": 590,
      "training_loss": 7.841363430023193
    },
    {
      "epoch": 0.12791327913279132,
      "step": 590,
      "training_loss": 9.239950180053711
    },
    {
      "epoch": 0.12813008130081302,
      "step": 591,
      "training_loss": 6.608368873596191
    },
    {
      "epoch": 0.12813008130081302,
      "step": 591,
      "training_loss": 7.081403732299805
    },
    {
      "epoch": 0.12813008130081302,
      "step": 591,
      "training_loss": 7.458740711212158
    },
    {
      "epoch": 0.12813008130081302,
      "step": 591,
      "training_loss": 6.932859897613525
    },
    {
      "epoch": 0.1283468834688347,
      "grad_norm": 7.619635105133057,
      "learning_rate": 1e-05,
      "loss": 7.5141,
      "step": 592
    },
    {
      "epoch": 0.1283468834688347,
      "step": 592,
      "training_loss": 7.711199760437012
    },
    {
      "epoch": 0.1283468834688347,
      "step": 592,
      "training_loss": 6.179352283477783
    },
    {
      "epoch": 0.1283468834688347,
      "step": 592,
      "training_loss": 7.0197038650512695
    },
    {
      "epoch": 0.1283468834688347,
      "step": 592,
      "training_loss": 7.366167068481445
    },
    {
      "epoch": 0.12856368563685636,
      "step": 593,
      "training_loss": 7.081969738006592
    },
    {
      "epoch": 0.12856368563685636,
      "step": 593,
      "training_loss": 7.563540935516357
    },
    {
      "epoch": 0.12856368563685636,
      "step": 593,
      "training_loss": 7.957453727722168
    },
    {
      "epoch": 0.12856368563685636,
      "step": 593,
      "training_loss": 6.4261298179626465
    },
    {
      "epoch": 0.12878048780487805,
      "step": 594,
      "training_loss": 7.284909248352051
    },
    {
      "epoch": 0.12878048780487805,
      "step": 594,
      "training_loss": 7.347063064575195
    },
    {
      "epoch": 0.12878048780487805,
      "step": 594,
      "training_loss": 7.23387336730957
    },
    {
      "epoch": 0.12878048780487805,
      "step": 594,
      "training_loss": 7.124420642852783
    },
    {
      "epoch": 0.12899728997289972,
      "step": 595,
      "training_loss": 8.148276329040527
    },
    {
      "epoch": 0.12899728997289972,
      "step": 595,
      "training_loss": 7.832248687744141
    },
    {
      "epoch": 0.12899728997289972,
      "step": 595,
      "training_loss": 7.663163185119629
    },
    {
      "epoch": 0.12899728997289972,
      "step": 595,
      "training_loss": 7.190361022949219
    },
    {
      "epoch": 0.12921409214092142,
      "grad_norm": 10.322369575500488,
      "learning_rate": 1e-05,
      "loss": 7.3206,
      "step": 596
    },
    {
      "epoch": 0.12921409214092142,
      "step": 596,
      "training_loss": 6.5823822021484375
    },
    {
      "epoch": 0.12921409214092142,
      "step": 596,
      "training_loss": 7.621179103851318
    },
    {
      "epoch": 0.12921409214092142,
      "step": 596,
      "training_loss": 6.843857765197754
    },
    {
      "epoch": 0.12921409214092142,
      "step": 596,
      "training_loss": 7.259695529937744
    },
    {
      "epoch": 0.12943089430894308,
      "step": 597,
      "training_loss": 6.15716552734375
    },
    {
      "epoch": 0.12943089430894308,
      "step": 597,
      "training_loss": 6.123781204223633
    },
    {
      "epoch": 0.12943089430894308,
      "step": 597,
      "training_loss": 7.600485801696777
    },
    {
      "epoch": 0.12943089430894308,
      "step": 597,
      "training_loss": 7.457756519317627
    },
    {
      "epoch": 0.12964769647696478,
      "step": 598,
      "training_loss": 5.8051838874816895
    },
    {
      "epoch": 0.12964769647696478,
      "step": 598,
      "training_loss": 8.342947959899902
    },
    {
      "epoch": 0.12964769647696478,
      "step": 598,
      "training_loss": 6.8732991218566895
    },
    {
      "epoch": 0.12964769647696478,
      "step": 598,
      "training_loss": 9.609371185302734
    },
    {
      "epoch": 0.12986449864498645,
      "step": 599,
      "training_loss": 7.363448143005371
    },
    {
      "epoch": 0.12986449864498645,
      "step": 599,
      "training_loss": 7.682497978210449
    },
    {
      "epoch": 0.12986449864498645,
      "step": 599,
      "training_loss": 6.98959493637085
    },
    {
      "epoch": 0.12986449864498645,
      "step": 599,
      "training_loss": 5.413798809051514
    },
    {
      "epoch": 0.13008130081300814,
      "grad_norm": 7.93290901184082,
      "learning_rate": 1e-05,
      "loss": 7.1079,
      "step": 600
    },
    {
      "epoch": 0.13008130081300814,
      "step": 600,
      "training_loss": 5.863027095794678
    },
    {
      "epoch": 0.13008130081300814,
      "step": 600,
      "training_loss": 7.1672043800354
    },
    {
      "epoch": 0.13008130081300814,
      "step": 600,
      "training_loss": 7.288692951202393
    },
    {
      "epoch": 0.13008130081300814,
      "step": 600,
      "training_loss": 7.979000091552734
    },
    {
      "epoch": 0.1302981029810298,
      "step": 601,
      "training_loss": 7.516534328460693
    },
    {
      "epoch": 0.1302981029810298,
      "step": 601,
      "training_loss": 7.801942825317383
    },
    {
      "epoch": 0.1302981029810298,
      "step": 601,
      "training_loss": 6.100656032562256
    },
    {
      "epoch": 0.1302981029810298,
      "step": 601,
      "training_loss": 6.618739128112793
    },
    {
      "epoch": 0.13051490514905148,
      "step": 602,
      "training_loss": 7.000290870666504
    },
    {
      "epoch": 0.13051490514905148,
      "step": 602,
      "training_loss": 6.902126789093018
    },
    {
      "epoch": 0.13051490514905148,
      "step": 602,
      "training_loss": 6.2357048988342285
    },
    {
      "epoch": 0.13051490514905148,
      "step": 602,
      "training_loss": 6.861132621765137
    },
    {
      "epoch": 0.13073170731707318,
      "step": 603,
      "training_loss": 7.270315647125244
    },
    {
      "epoch": 0.13073170731707318,
      "step": 603,
      "training_loss": 8.226339340209961
    },
    {
      "epoch": 0.13073170731707318,
      "step": 603,
      "training_loss": 7.3134918212890625
    },
    {
      "epoch": 0.13073170731707318,
      "step": 603,
      "training_loss": 6.79039192199707
    },
    {
      "epoch": 0.13094850948509484,
      "grad_norm": 9.5869140625,
      "learning_rate": 1e-05,
      "loss": 7.0585,
      "step": 604
    },
    {
      "epoch": 0.13094850948509484,
      "step": 604,
      "training_loss": 7.229596138000488
    },
    {
      "epoch": 0.13094850948509484,
      "step": 604,
      "training_loss": 7.62630033493042
    },
    {
      "epoch": 0.13094850948509484,
      "step": 604,
      "training_loss": 7.256913185119629
    },
    {
      "epoch": 0.13094850948509484,
      "step": 604,
      "training_loss": 5.7639288902282715
    },
    {
      "epoch": 0.13116531165311654,
      "step": 605,
      "training_loss": 8.023152351379395
    },
    {
      "epoch": 0.13116531165311654,
      "step": 605,
      "training_loss": 5.484198093414307
    },
    {
      "epoch": 0.13116531165311654,
      "step": 605,
      "training_loss": 6.825581073760986
    },
    {
      "epoch": 0.13116531165311654,
      "step": 605,
      "training_loss": 7.332122802734375
    },
    {
      "epoch": 0.1313821138211382,
      "step": 606,
      "training_loss": 7.500748634338379
    },
    {
      "epoch": 0.1313821138211382,
      "step": 606,
      "training_loss": 7.523744106292725
    },
    {
      "epoch": 0.1313821138211382,
      "step": 606,
      "training_loss": 6.091108798980713
    },
    {
      "epoch": 0.1313821138211382,
      "step": 606,
      "training_loss": 7.950115203857422
    },
    {
      "epoch": 0.1315989159891599,
      "step": 607,
      "training_loss": 7.617984294891357
    },
    {
      "epoch": 0.1315989159891599,
      "step": 607,
      "training_loss": 7.866483688354492
    },
    {
      "epoch": 0.1315989159891599,
      "step": 607,
      "training_loss": 6.162181854248047
    },
    {
      "epoch": 0.1315989159891599,
      "step": 607,
      "training_loss": 8.873619079589844
    },
    {
      "epoch": 0.13181571815718157,
      "grad_norm": 8.048406600952148,
      "learning_rate": 1e-05,
      "loss": 7.1955,
      "step": 608
    },
    {
      "epoch": 0.13181571815718157,
      "step": 608,
      "training_loss": 7.955214023590088
    },
    {
      "epoch": 0.13181571815718157,
      "step": 608,
      "training_loss": 7.716982841491699
    },
    {
      "epoch": 0.13181571815718157,
      "step": 608,
      "training_loss": 7.862886905670166
    },
    {
      "epoch": 0.13181571815718157,
      "step": 608,
      "training_loss": 7.912551403045654
    },
    {
      "epoch": 0.13203252032520324,
      "step": 609,
      "training_loss": 7.8794050216674805
    },
    {
      "epoch": 0.13203252032520324,
      "step": 609,
      "training_loss": 6.291535377502441
    },
    {
      "epoch": 0.13203252032520324,
      "step": 609,
      "training_loss": 7.558544158935547
    },
    {
      "epoch": 0.13203252032520324,
      "step": 609,
      "training_loss": 7.4672532081604
    },
    {
      "epoch": 0.13224932249322494,
      "step": 610,
      "training_loss": 7.938068866729736
    },
    {
      "epoch": 0.13224932249322494,
      "step": 610,
      "training_loss": 7.853997707366943
    },
    {
      "epoch": 0.13224932249322494,
      "step": 610,
      "training_loss": 5.898573398590088
    },
    {
      "epoch": 0.13224932249322494,
      "step": 610,
      "training_loss": 7.487909317016602
    },
    {
      "epoch": 0.1324661246612466,
      "step": 611,
      "training_loss": 7.8547682762146
    },
    {
      "epoch": 0.1324661246612466,
      "step": 611,
      "training_loss": 7.927160263061523
    },
    {
      "epoch": 0.1324661246612466,
      "step": 611,
      "training_loss": 7.053619384765625
    },
    {
      "epoch": 0.1324661246612466,
      "step": 611,
      "training_loss": 7.528899192810059
    },
    {
      "epoch": 0.1326829268292683,
      "grad_norm": 11.865911483764648,
      "learning_rate": 1e-05,
      "loss": 7.5117,
      "step": 612
    },
    {
      "epoch": 0.1326829268292683,
      "step": 612,
      "training_loss": 6.785801410675049
    },
    {
      "epoch": 0.1326829268292683,
      "step": 612,
      "training_loss": 7.2341461181640625
    },
    {
      "epoch": 0.1326829268292683,
      "step": 612,
      "training_loss": 6.397313117980957
    },
    {
      "epoch": 0.1326829268292683,
      "step": 612,
      "training_loss": 8.108588218688965
    },
    {
      "epoch": 0.13289972899728997,
      "step": 613,
      "training_loss": 6.625888824462891
    },
    {
      "epoch": 0.13289972899728997,
      "step": 613,
      "training_loss": 8.769024848937988
    },
    {
      "epoch": 0.13289972899728997,
      "step": 613,
      "training_loss": 7.340517520904541
    },
    {
      "epoch": 0.13289972899728997,
      "step": 613,
      "training_loss": 8.80515193939209
    },
    {
      "epoch": 0.13311653116531166,
      "step": 614,
      "training_loss": 8.321527481079102
    },
    {
      "epoch": 0.13311653116531166,
      "step": 614,
      "training_loss": 7.630622863769531
    },
    {
      "epoch": 0.13311653116531166,
      "step": 614,
      "training_loss": 7.603785037994385
    },
    {
      "epoch": 0.13311653116531166,
      "step": 614,
      "training_loss": 7.64886474609375
    },
    {
      "epoch": 0.13333333333333333,
      "step": 615,
      "training_loss": 7.109368801116943
    },
    {
      "epoch": 0.13333333333333333,
      "step": 615,
      "training_loss": 10.849748611450195
    },
    {
      "epoch": 0.13333333333333333,
      "step": 615,
      "training_loss": 6.718138217926025
    },
    {
      "epoch": 0.13333333333333333,
      "step": 615,
      "training_loss": 7.589798927307129
    },
    {
      "epoch": 0.13355013550135503,
      "grad_norm": 11.95450210571289,
      "learning_rate": 1e-05,
      "loss": 7.7211,
      "step": 616
    },
    {
      "epoch": 0.13355013550135503,
      "step": 616,
      "training_loss": 7.257184982299805
    },
    {
      "epoch": 0.13355013550135503,
      "step": 616,
      "training_loss": 6.896993637084961
    },
    {
      "epoch": 0.13355013550135503,
      "step": 616,
      "training_loss": 7.137396812438965
    },
    {
      "epoch": 0.13355013550135503,
      "step": 616,
      "training_loss": 7.252776145935059
    },
    {
      "epoch": 0.1337669376693767,
      "step": 617,
      "training_loss": 7.8186421394348145
    },
    {
      "epoch": 0.1337669376693767,
      "step": 617,
      "training_loss": 7.251648902893066
    },
    {
      "epoch": 0.1337669376693767,
      "step": 617,
      "training_loss": 6.9130096435546875
    },
    {
      "epoch": 0.1337669376693767,
      "step": 617,
      "training_loss": 6.634657859802246
    },
    {
      "epoch": 0.13398373983739836,
      "step": 618,
      "training_loss": 6.703446865081787
    },
    {
      "epoch": 0.13398373983739836,
      "step": 618,
      "training_loss": 7.668133735656738
    },
    {
      "epoch": 0.13398373983739836,
      "step": 618,
      "training_loss": 5.454965591430664
    },
    {
      "epoch": 0.13398373983739836,
      "step": 618,
      "training_loss": 6.864248752593994
    },
    {
      "epoch": 0.13420054200542006,
      "step": 619,
      "training_loss": 7.6096296310424805
    },
    {
      "epoch": 0.13420054200542006,
      "step": 619,
      "training_loss": 8.163223266601562
    },
    {
      "epoch": 0.13420054200542006,
      "step": 619,
      "training_loss": 7.514528751373291
    },
    {
      "epoch": 0.13420054200542006,
      "step": 619,
      "training_loss": 7.8950581550598145
    },
    {
      "epoch": 0.13441734417344173,
      "grad_norm": 9.179863929748535,
      "learning_rate": 1e-05,
      "loss": 7.1897,
      "step": 620
    },
    {
      "epoch": 0.13441734417344173,
      "step": 620,
      "training_loss": 7.0427398681640625
    },
    {
      "epoch": 0.13441734417344173,
      "step": 620,
      "training_loss": 8.806157112121582
    },
    {
      "epoch": 0.13441734417344173,
      "step": 620,
      "training_loss": 7.0806779861450195
    },
    {
      "epoch": 0.13441734417344173,
      "step": 620,
      "training_loss": 7.725435256958008
    },
    {
      "epoch": 0.13463414634146342,
      "step": 621,
      "training_loss": 6.998117446899414
    },
    {
      "epoch": 0.13463414634146342,
      "step": 621,
      "training_loss": 8.644993782043457
    },
    {
      "epoch": 0.13463414634146342,
      "step": 621,
      "training_loss": 7.055511474609375
    },
    {
      "epoch": 0.13463414634146342,
      "step": 621,
      "training_loss": 6.919378280639648
    },
    {
      "epoch": 0.1348509485094851,
      "step": 622,
      "training_loss": 7.5139384269714355
    },
    {
      "epoch": 0.1348509485094851,
      "step": 622,
      "training_loss": 6.738946437835693
    },
    {
      "epoch": 0.1348509485094851,
      "step": 622,
      "training_loss": 7.804373264312744
    },
    {
      "epoch": 0.1348509485094851,
      "step": 622,
      "training_loss": 7.803759574890137
    },
    {
      "epoch": 0.1350677506775068,
      "step": 623,
      "training_loss": 6.615255832672119
    },
    {
      "epoch": 0.1350677506775068,
      "step": 623,
      "training_loss": 7.634895324707031
    },
    {
      "epoch": 0.1350677506775068,
      "step": 623,
      "training_loss": 7.8634233474731445
    },
    {
      "epoch": 0.1350677506775068,
      "step": 623,
      "training_loss": 8.031856536865234
    },
    {
      "epoch": 0.13528455284552846,
      "grad_norm": 10.611557006835938,
      "learning_rate": 1e-05,
      "loss": 7.5175,
      "step": 624
    },
    {
      "epoch": 0.13528455284552846,
      "step": 624,
      "training_loss": 7.690569877624512
    },
    {
      "epoch": 0.13528455284552846,
      "step": 624,
      "training_loss": 6.589330196380615
    },
    {
      "epoch": 0.13528455284552846,
      "step": 624,
      "training_loss": 6.584785461425781
    },
    {
      "epoch": 0.13528455284552846,
      "step": 624,
      "training_loss": 6.5766706466674805
    },
    {
      "epoch": 0.13550135501355012,
      "step": 625,
      "training_loss": 6.742151260375977
    },
    {
      "epoch": 0.13550135501355012,
      "step": 625,
      "training_loss": 6.255079746246338
    },
    {
      "epoch": 0.13550135501355012,
      "step": 625,
      "training_loss": 7.709362983703613
    },
    {
      "epoch": 0.13550135501355012,
      "step": 625,
      "training_loss": 8.217350006103516
    },
    {
      "epoch": 0.13571815718157182,
      "step": 626,
      "training_loss": 7.06015682220459
    },
    {
      "epoch": 0.13571815718157182,
      "step": 626,
      "training_loss": 6.313155174255371
    },
    {
      "epoch": 0.13571815718157182,
      "step": 626,
      "training_loss": 7.056081295013428
    },
    {
      "epoch": 0.13571815718157182,
      "step": 626,
      "training_loss": 7.849941730499268
    },
    {
      "epoch": 0.1359349593495935,
      "step": 627,
      "training_loss": 6.5791144371032715
    },
    {
      "epoch": 0.1359349593495935,
      "step": 627,
      "training_loss": 8.137368202209473
    },
    {
      "epoch": 0.1359349593495935,
      "step": 627,
      "training_loss": 7.19508171081543
    },
    {
      "epoch": 0.1359349593495935,
      "step": 627,
      "training_loss": 6.661494731903076
    },
    {
      "epoch": 0.13615176151761518,
      "grad_norm": 7.693917274475098,
      "learning_rate": 1e-05,
      "loss": 7.0761,
      "step": 628
    },
    {
      "epoch": 0.13615176151761518,
      "step": 628,
      "training_loss": 8.119651794433594
    },
    {
      "epoch": 0.13615176151761518,
      "step": 628,
      "training_loss": 7.5288567543029785
    },
    {
      "epoch": 0.13615176151761518,
      "step": 628,
      "training_loss": 7.959064960479736
    },
    {
      "epoch": 0.13615176151761518,
      "step": 628,
      "training_loss": 9.150483131408691
    },
    {
      "epoch": 0.13636856368563685,
      "step": 629,
      "training_loss": 7.391473770141602
    },
    {
      "epoch": 0.13636856368563685,
      "step": 629,
      "training_loss": 7.91225528717041
    },
    {
      "epoch": 0.13636856368563685,
      "step": 629,
      "training_loss": 4.802752494812012
    },
    {
      "epoch": 0.13636856368563685,
      "step": 629,
      "training_loss": 7.874536037445068
    },
    {
      "epoch": 0.13658536585365855,
      "step": 630,
      "training_loss": 7.54563045501709
    },
    {
      "epoch": 0.13658536585365855,
      "step": 630,
      "training_loss": 8.387598991394043
    },
    {
      "epoch": 0.13658536585365855,
      "step": 630,
      "training_loss": 6.7841949462890625
    },
    {
      "epoch": 0.13658536585365855,
      "step": 630,
      "training_loss": 7.481412410736084
    },
    {
      "epoch": 0.13680216802168021,
      "step": 631,
      "training_loss": 6.446166038513184
    },
    {
      "epoch": 0.13680216802168021,
      "step": 631,
      "training_loss": 6.585354804992676
    },
    {
      "epoch": 0.13680216802168021,
      "step": 631,
      "training_loss": 7.915628433227539
    },
    {
      "epoch": 0.13680216802168021,
      "step": 631,
      "training_loss": 8.058070182800293
    },
    {
      "epoch": 0.1370189701897019,
      "grad_norm": 13.476020812988281,
      "learning_rate": 1e-05,
      "loss": 7.4964,
      "step": 632
    },
    {
      "epoch": 0.1370189701897019,
      "step": 632,
      "training_loss": 5.982921123504639
    },
    {
      "epoch": 0.1370189701897019,
      "step": 632,
      "training_loss": 7.5262346267700195
    },
    {
      "epoch": 0.1370189701897019,
      "step": 632,
      "training_loss": 7.167222499847412
    },
    {
      "epoch": 0.1370189701897019,
      "step": 632,
      "training_loss": 7.830445289611816
    },
    {
      "epoch": 0.13723577235772358,
      "step": 633,
      "training_loss": 7.163572788238525
    },
    {
      "epoch": 0.13723577235772358,
      "step": 633,
      "training_loss": 7.750284194946289
    },
    {
      "epoch": 0.13723577235772358,
      "step": 633,
      "training_loss": 5.81481409072876
    },
    {
      "epoch": 0.13723577235772358,
      "step": 633,
      "training_loss": 5.630077838897705
    },
    {
      "epoch": 0.13745257452574525,
      "step": 634,
      "training_loss": 7.124134540557861
    },
    {
      "epoch": 0.13745257452574525,
      "step": 634,
      "training_loss": 7.522799015045166
    },
    {
      "epoch": 0.13745257452574525,
      "step": 634,
      "training_loss": 7.225833415985107
    },
    {
      "epoch": 0.13745257452574525,
      "step": 634,
      "training_loss": 8.06759262084961
    },
    {
      "epoch": 0.13766937669376694,
      "step": 635,
      "training_loss": 5.810555458068848
    },
    {
      "epoch": 0.13766937669376694,
      "step": 635,
      "training_loss": 6.746077060699463
    },
    {
      "epoch": 0.13766937669376694,
      "step": 635,
      "training_loss": 7.4301300048828125
    },
    {
      "epoch": 0.13766937669376694,
      "step": 635,
      "training_loss": 6.782974720001221
    },
    {
      "epoch": 0.1378861788617886,
      "grad_norm": 10.01047420501709,
      "learning_rate": 1e-05,
      "loss": 6.9735,
      "step": 636
    },
    {
      "epoch": 0.1378861788617886,
      "step": 636,
      "training_loss": 7.033430099487305
    },
    {
      "epoch": 0.1378861788617886,
      "step": 636,
      "training_loss": 7.119344234466553
    },
    {
      "epoch": 0.1378861788617886,
      "step": 636,
      "training_loss": 9.009607315063477
    },
    {
      "epoch": 0.1378861788617886,
      "step": 636,
      "training_loss": 6.190314292907715
    },
    {
      "epoch": 0.1381029810298103,
      "step": 637,
      "training_loss": 6.2979512214660645
    },
    {
      "epoch": 0.1381029810298103,
      "step": 637,
      "training_loss": 6.006887912750244
    },
    {
      "epoch": 0.1381029810298103,
      "step": 637,
      "training_loss": 5.69260835647583
    },
    {
      "epoch": 0.1381029810298103,
      "step": 637,
      "training_loss": 7.93494176864624
    },
    {
      "epoch": 0.13831978319783197,
      "step": 638,
      "training_loss": 8.463129997253418
    },
    {
      "epoch": 0.13831978319783197,
      "step": 638,
      "training_loss": 8.12779712677002
    },
    {
      "epoch": 0.13831978319783197,
      "step": 638,
      "training_loss": 6.717981338500977
    },
    {
      "epoch": 0.13831978319783197,
      "step": 638,
      "training_loss": 6.579882621765137
    },
    {
      "epoch": 0.13853658536585367,
      "step": 639,
      "training_loss": 7.652796745300293
    },
    {
      "epoch": 0.13853658536585367,
      "step": 639,
      "training_loss": 6.9944000244140625
    },
    {
      "epoch": 0.13853658536585367,
      "step": 639,
      "training_loss": 7.34970760345459
    },
    {
      "epoch": 0.13853658536585367,
      "step": 639,
      "training_loss": 7.760429859161377
    },
    {
      "epoch": 0.13875338753387534,
      "grad_norm": 8.153667449951172,
      "learning_rate": 1e-05,
      "loss": 7.1832,
      "step": 640
    },
    {
      "epoch": 0.13875338753387534,
      "step": 640,
      "training_loss": 6.8320088386535645
    },
    {
      "epoch": 0.13875338753387534,
      "step": 640,
      "training_loss": 7.609013557434082
    },
    {
      "epoch": 0.13875338753387534,
      "step": 640,
      "training_loss": 7.697287082672119
    },
    {
      "epoch": 0.13875338753387534,
      "step": 640,
      "training_loss": 7.936979293823242
    },
    {
      "epoch": 0.138970189701897,
      "step": 641,
      "training_loss": 6.716914176940918
    },
    {
      "epoch": 0.138970189701897,
      "step": 641,
      "training_loss": 7.154397964477539
    },
    {
      "epoch": 0.138970189701897,
      "step": 641,
      "training_loss": 7.838075637817383
    },
    {
      "epoch": 0.138970189701897,
      "step": 641,
      "training_loss": 7.491228103637695
    },
    {
      "epoch": 0.1391869918699187,
      "step": 642,
      "training_loss": 7.0453877449035645
    },
    {
      "epoch": 0.1391869918699187,
      "step": 642,
      "training_loss": 7.26906156539917
    },
    {
      "epoch": 0.1391869918699187,
      "step": 642,
      "training_loss": 6.503844738006592
    },
    {
      "epoch": 0.1391869918699187,
      "step": 642,
      "training_loss": 6.591574668884277
    },
    {
      "epoch": 0.13940379403794037,
      "step": 643,
      "training_loss": 7.147425174713135
    },
    {
      "epoch": 0.13940379403794037,
      "step": 643,
      "training_loss": 7.488796710968018
    },
    {
      "epoch": 0.13940379403794037,
      "step": 643,
      "training_loss": 7.209327220916748
    },
    {
      "epoch": 0.13940379403794037,
      "step": 643,
      "training_loss": 5.4793829917907715
    },
    {
      "epoch": 0.13962059620596207,
      "grad_norm": 9.911272048950195,
      "learning_rate": 1e-05,
      "loss": 7.1257,
      "step": 644
    },
    {
      "epoch": 0.13962059620596207,
      "step": 644,
      "training_loss": 6.690517902374268
    },
    {
      "epoch": 0.13962059620596207,
      "step": 644,
      "training_loss": 7.453930854797363
    },
    {
      "epoch": 0.13962059620596207,
      "step": 644,
      "training_loss": 7.430661678314209
    },
    {
      "epoch": 0.13962059620596207,
      "step": 644,
      "training_loss": 7.411694526672363
    },
    {
      "epoch": 0.13983739837398373,
      "step": 645,
      "training_loss": 7.168932914733887
    },
    {
      "epoch": 0.13983739837398373,
      "step": 645,
      "training_loss": 8.349129676818848
    },
    {
      "epoch": 0.13983739837398373,
      "step": 645,
      "training_loss": 7.541749954223633
    },
    {
      "epoch": 0.13983739837398373,
      "step": 645,
      "training_loss": 8.19633674621582
    },
    {
      "epoch": 0.14005420054200543,
      "step": 646,
      "training_loss": 7.347944736480713
    },
    {
      "epoch": 0.14005420054200543,
      "step": 646,
      "training_loss": 8.2139253616333
    },
    {
      "epoch": 0.14005420054200543,
      "step": 646,
      "training_loss": 7.517119407653809
    },
    {
      "epoch": 0.14005420054200543,
      "step": 646,
      "training_loss": 7.3139801025390625
    },
    {
      "epoch": 0.1402710027100271,
      "step": 647,
      "training_loss": 7.087775707244873
    },
    {
      "epoch": 0.1402710027100271,
      "step": 647,
      "training_loss": 6.694464683532715
    },
    {
      "epoch": 0.1402710027100271,
      "step": 647,
      "training_loss": 7.009782314300537
    },
    {
      "epoch": 0.1402710027100271,
      "step": 647,
      "training_loss": 5.610466003417969
    },
    {
      "epoch": 0.1404878048780488,
      "grad_norm": 9.804287910461426,
      "learning_rate": 1e-05,
      "loss": 7.3149,
      "step": 648
    },
    {
      "epoch": 0.1404878048780488,
      "step": 648,
      "training_loss": 6.101987361907959
    },
    {
      "epoch": 0.1404878048780488,
      "step": 648,
      "training_loss": 6.421648025512695
    },
    {
      "epoch": 0.1404878048780488,
      "step": 648,
      "training_loss": 8.165241241455078
    },
    {
      "epoch": 0.1404878048780488,
      "step": 648,
      "training_loss": 6.517752647399902
    },
    {
      "epoch": 0.14070460704607046,
      "step": 649,
      "training_loss": 7.045173645019531
    },
    {
      "epoch": 0.14070460704607046,
      "step": 649,
      "training_loss": 6.186534404754639
    },
    {
      "epoch": 0.14070460704607046,
      "step": 649,
      "training_loss": 6.8046159744262695
    },
    {
      "epoch": 0.14070460704607046,
      "step": 649,
      "training_loss": 7.126046180725098
    },
    {
      "epoch": 0.14092140921409213,
      "step": 650,
      "training_loss": 7.928966522216797
    },
    {
      "epoch": 0.14092140921409213,
      "step": 650,
      "training_loss": 7.348383903503418
    },
    {
      "epoch": 0.14092140921409213,
      "step": 650,
      "training_loss": 6.833086967468262
    },
    {
      "epoch": 0.14092140921409213,
      "step": 650,
      "training_loss": 6.14984130859375
    },
    {
      "epoch": 0.14113821138211383,
      "step": 651,
      "training_loss": 7.28413200378418
    },
    {
      "epoch": 0.14113821138211383,
      "step": 651,
      "training_loss": 8.91409683227539
    },
    {
      "epoch": 0.14113821138211383,
      "step": 651,
      "training_loss": 6.1575727462768555
    },
    {
      "epoch": 0.14113821138211383,
      "step": 651,
      "training_loss": 7.604197978973389
    },
    {
      "epoch": 0.1413550135501355,
      "grad_norm": 8.240758895874023,
      "learning_rate": 1e-05,
      "loss": 7.0368,
      "step": 652
    },
    {
      "epoch": 0.1413550135501355,
      "step": 652,
      "training_loss": 7.483431816101074
    },
    {
      "epoch": 0.1413550135501355,
      "step": 652,
      "training_loss": 6.619241237640381
    },
    {
      "epoch": 0.1413550135501355,
      "step": 652,
      "training_loss": 6.7724504470825195
    },
    {
      "epoch": 0.1413550135501355,
      "step": 652,
      "training_loss": 7.779647350311279
    },
    {
      "epoch": 0.1415718157181572,
      "step": 653,
      "training_loss": 8.029610633850098
    },
    {
      "epoch": 0.1415718157181572,
      "step": 653,
      "training_loss": 7.847306251525879
    },
    {
      "epoch": 0.1415718157181572,
      "step": 653,
      "training_loss": 8.167267799377441
    },
    {
      "epoch": 0.1415718157181572,
      "step": 653,
      "training_loss": 5.850249290466309
    },
    {
      "epoch": 0.14178861788617886,
      "step": 654,
      "training_loss": 8.200510025024414
    },
    {
      "epoch": 0.14178861788617886,
      "step": 654,
      "training_loss": 7.976884841918945
    },
    {
      "epoch": 0.14178861788617886,
      "step": 654,
      "training_loss": 7.122704029083252
    },
    {
      "epoch": 0.14178861788617886,
      "step": 654,
      "training_loss": 7.784440994262695
    },
    {
      "epoch": 0.14200542005420055,
      "step": 655,
      "training_loss": 7.086678981781006
    },
    {
      "epoch": 0.14200542005420055,
      "step": 655,
      "training_loss": 7.878862380981445
    },
    {
      "epoch": 0.14200542005420055,
      "step": 655,
      "training_loss": 6.911147594451904
    },
    {
      "epoch": 0.14200542005420055,
      "step": 655,
      "training_loss": 8.810918807983398
    },
    {
      "epoch": 0.14222222222222222,
      "grad_norm": 8.949639320373535,
      "learning_rate": 1e-05,
      "loss": 7.5201,
      "step": 656
    },
    {
      "epoch": 0.14222222222222222,
      "step": 656,
      "training_loss": 8.174928665161133
    },
    {
      "epoch": 0.14222222222222222,
      "step": 656,
      "training_loss": 8.46475601196289
    },
    {
      "epoch": 0.14222222222222222,
      "step": 656,
      "training_loss": 7.743515968322754
    },
    {
      "epoch": 0.14222222222222222,
      "step": 656,
      "training_loss": 6.3208537101745605
    },
    {
      "epoch": 0.1424390243902439,
      "step": 657,
      "training_loss": 9.431600570678711
    },
    {
      "epoch": 0.1424390243902439,
      "step": 657,
      "training_loss": 7.73713493347168
    },
    {
      "epoch": 0.1424390243902439,
      "step": 657,
      "training_loss": 7.477538585662842
    },
    {
      "epoch": 0.1424390243902439,
      "step": 657,
      "training_loss": 7.694230556488037
    },
    {
      "epoch": 0.14265582655826559,
      "step": 658,
      "training_loss": 6.77695369720459
    },
    {
      "epoch": 0.14265582655826559,
      "step": 658,
      "training_loss": 7.611932277679443
    },
    {
      "epoch": 0.14265582655826559,
      "step": 658,
      "training_loss": 8.679218292236328
    },
    {
      "epoch": 0.14265582655826559,
      "step": 658,
      "training_loss": 7.213522434234619
    },
    {
      "epoch": 0.14287262872628725,
      "step": 659,
      "training_loss": 7.131553649902344
    },
    {
      "epoch": 0.14287262872628725,
      "step": 659,
      "training_loss": 7.900057315826416
    },
    {
      "epoch": 0.14287262872628725,
      "step": 659,
      "training_loss": 7.052402496337891
    },
    {
      "epoch": 0.14287262872628725,
      "step": 659,
      "training_loss": 7.303713321685791
    },
    {
      "epoch": 0.14308943089430895,
      "grad_norm": 9.742547035217285,
      "learning_rate": 1e-05,
      "loss": 7.6696,
      "step": 660
    },
    {
      "epoch": 0.14308943089430895,
      "step": 660,
      "training_loss": 8.109513282775879
    },
    {
      "epoch": 0.14308943089430895,
      "step": 660,
      "training_loss": 8.700970649719238
    },
    {
      "epoch": 0.14308943089430895,
      "step": 660,
      "training_loss": 6.375213623046875
    },
    {
      "epoch": 0.14308943089430895,
      "step": 660,
      "training_loss": 6.323480606079102
    },
    {
      "epoch": 0.14330623306233062,
      "step": 661,
      "training_loss": 6.410595893859863
    },
    {
      "epoch": 0.14330623306233062,
      "step": 661,
      "training_loss": 8.223325729370117
    },
    {
      "epoch": 0.14330623306233062,
      "step": 661,
      "training_loss": 7.4896416664123535
    },
    {
      "epoch": 0.14330623306233062,
      "step": 661,
      "training_loss": 7.41136360168457
    },
    {
      "epoch": 0.1435230352303523,
      "step": 662,
      "training_loss": 7.093486785888672
    },
    {
      "epoch": 0.1435230352303523,
      "step": 662,
      "training_loss": 7.962798595428467
    },
    {
      "epoch": 0.1435230352303523,
      "step": 662,
      "training_loss": 6.783671855926514
    },
    {
      "epoch": 0.1435230352303523,
      "step": 662,
      "training_loss": 7.9288811683654785
    },
    {
      "epoch": 0.14373983739837398,
      "step": 663,
      "training_loss": 7.214912414550781
    },
    {
      "epoch": 0.14373983739837398,
      "step": 663,
      "training_loss": 7.583855152130127
    },
    {
      "epoch": 0.14373983739837398,
      "step": 663,
      "training_loss": 8.02242374420166
    },
    {
      "epoch": 0.14373983739837398,
      "step": 663,
      "training_loss": 8.287652969360352
    },
    {
      "epoch": 0.14395663956639568,
      "grad_norm": 8.3956298828125,
      "learning_rate": 1e-05,
      "loss": 7.4951,
      "step": 664
    },
    {
      "epoch": 0.14395663956639568,
      "step": 664,
      "training_loss": 6.655033111572266
    },
    {
      "epoch": 0.14395663956639568,
      "step": 664,
      "training_loss": 6.859521865844727
    },
    {
      "epoch": 0.14395663956639568,
      "step": 664,
      "training_loss": 7.403736114501953
    },
    {
      "epoch": 0.14395663956639568,
      "step": 664,
      "training_loss": 7.7692694664001465
    },
    {
      "epoch": 0.14417344173441735,
      "step": 665,
      "training_loss": 7.195956230163574
    },
    {
      "epoch": 0.14417344173441735,
      "step": 665,
      "training_loss": 7.0312323570251465
    },
    {
      "epoch": 0.14417344173441735,
      "step": 665,
      "training_loss": 6.801809787750244
    },
    {
      "epoch": 0.14417344173441735,
      "step": 665,
      "training_loss": 5.832334518432617
    },
    {
      "epoch": 0.144390243902439,
      "step": 666,
      "training_loss": 7.629120826721191
    },
    {
      "epoch": 0.144390243902439,
      "step": 666,
      "training_loss": 7.128756046295166
    },
    {
      "epoch": 0.144390243902439,
      "step": 666,
      "training_loss": 6.115785598754883
    },
    {
      "epoch": 0.144390243902439,
      "step": 666,
      "training_loss": 6.762349605560303
    },
    {
      "epoch": 0.1446070460704607,
      "step": 667,
      "training_loss": 6.528202533721924
    },
    {
      "epoch": 0.1446070460704607,
      "step": 667,
      "training_loss": 7.133510589599609
    },
    {
      "epoch": 0.1446070460704607,
      "step": 667,
      "training_loss": 7.177834510803223
    },
    {
      "epoch": 0.1446070460704607,
      "step": 667,
      "training_loss": 7.630080699920654
    },
    {
      "epoch": 0.14482384823848238,
      "grad_norm": 7.629683494567871,
      "learning_rate": 1e-05,
      "loss": 6.9784,
      "step": 668
    },
    {
      "epoch": 0.14482384823848238,
      "step": 668,
      "training_loss": 7.206544399261475
    },
    {
      "epoch": 0.14482384823848238,
      "step": 668,
      "training_loss": 7.687783241271973
    },
    {
      "epoch": 0.14482384823848238,
      "step": 668,
      "training_loss": 8.02108097076416
    },
    {
      "epoch": 0.14482384823848238,
      "step": 668,
      "training_loss": 7.218125820159912
    },
    {
      "epoch": 0.14504065040650407,
      "step": 669,
      "training_loss": 6.214282512664795
    },
    {
      "epoch": 0.14504065040650407,
      "step": 669,
      "training_loss": 6.846371173858643
    },
    {
      "epoch": 0.14504065040650407,
      "step": 669,
      "training_loss": 7.5454864501953125
    },
    {
      "epoch": 0.14504065040650407,
      "step": 669,
      "training_loss": 7.466509819030762
    },
    {
      "epoch": 0.14525745257452574,
      "step": 670,
      "training_loss": 7.416057586669922
    },
    {
      "epoch": 0.14525745257452574,
      "step": 670,
      "training_loss": 7.269529819488525
    },
    {
      "epoch": 0.14525745257452574,
      "step": 670,
      "training_loss": 8.064579963684082
    },
    {
      "epoch": 0.14525745257452574,
      "step": 670,
      "training_loss": 7.235340118408203
    },
    {
      "epoch": 0.14547425474254744,
      "step": 671,
      "training_loss": 8.030264854431152
    },
    {
      "epoch": 0.14547425474254744,
      "step": 671,
      "training_loss": 6.418236255645752
    },
    {
      "epoch": 0.14547425474254744,
      "step": 671,
      "training_loss": 6.922667503356934
    },
    {
      "epoch": 0.14547425474254744,
      "step": 671,
      "training_loss": 6.7122650146484375
    },
    {
      "epoch": 0.1456910569105691,
      "grad_norm": 11.072211265563965,
      "learning_rate": 1e-05,
      "loss": 7.2672,
      "step": 672
    },
    {
      "epoch": 0.1456910569105691,
      "step": 672,
      "training_loss": 8.938671112060547
    },
    {
      "epoch": 0.1456910569105691,
      "step": 672,
      "training_loss": 7.6342668533325195
    },
    {
      "epoch": 0.1456910569105691,
      "step": 672,
      "training_loss": 6.6158905029296875
    },
    {
      "epoch": 0.1456910569105691,
      "step": 672,
      "training_loss": 8.289597511291504
    },
    {
      "epoch": 0.14590785907859077,
      "step": 673,
      "training_loss": 7.555471897125244
    },
    {
      "epoch": 0.14590785907859077,
      "step": 673,
      "training_loss": 7.190801620483398
    },
    {
      "epoch": 0.14590785907859077,
      "step": 673,
      "training_loss": 6.536323070526123
    },
    {
      "epoch": 0.14590785907859077,
      "step": 673,
      "training_loss": 9.616280555725098
    },
    {
      "epoch": 0.14612466124661247,
      "step": 674,
      "training_loss": 8.075754165649414
    },
    {
      "epoch": 0.14612466124661247,
      "step": 674,
      "training_loss": 5.615295886993408
    },
    {
      "epoch": 0.14612466124661247,
      "step": 674,
      "training_loss": 7.979087829589844
    },
    {
      "epoch": 0.14612466124661247,
      "step": 674,
      "training_loss": 8.457880020141602
    },
    {
      "epoch": 0.14634146341463414,
      "step": 675,
      "training_loss": 7.58794641494751
    },
    {
      "epoch": 0.14634146341463414,
      "step": 675,
      "training_loss": 7.5620036125183105
    },
    {
      "epoch": 0.14634146341463414,
      "step": 675,
      "training_loss": 6.1433515548706055
    },
    {
      "epoch": 0.14634146341463414,
      "step": 675,
      "training_loss": 6.961207866668701
    },
    {
      "epoch": 0.14655826558265583,
      "grad_norm": 8.521005630493164,
      "learning_rate": 1e-05,
      "loss": 7.5475,
      "step": 676
    },
    {
      "epoch": 0.14655826558265583,
      "step": 676,
      "training_loss": 7.261760234832764
    },
    {
      "epoch": 0.14655826558265583,
      "step": 676,
      "training_loss": 6.554717540740967
    },
    {
      "epoch": 0.14655826558265583,
      "step": 676,
      "training_loss": 7.289490699768066
    },
    {
      "epoch": 0.14655826558265583,
      "step": 676,
      "training_loss": 6.898194789886475
    },
    {
      "epoch": 0.1467750677506775,
      "step": 677,
      "training_loss": 7.643047332763672
    },
    {
      "epoch": 0.1467750677506775,
      "step": 677,
      "training_loss": 6.528102874755859
    },
    {
      "epoch": 0.1467750677506775,
      "step": 677,
      "training_loss": 6.874150276184082
    },
    {
      "epoch": 0.1467750677506775,
      "step": 677,
      "training_loss": 5.520148277282715
    },
    {
      "epoch": 0.1469918699186992,
      "step": 678,
      "training_loss": 8.791017532348633
    },
    {
      "epoch": 0.1469918699186992,
      "step": 678,
      "training_loss": 7.693221092224121
    },
    {
      "epoch": 0.1469918699186992,
      "step": 678,
      "training_loss": 6.908621311187744
    },
    {
      "epoch": 0.1469918699186992,
      "step": 678,
      "training_loss": 7.768990993499756
    },
    {
      "epoch": 0.14720867208672087,
      "step": 679,
      "training_loss": 7.435976028442383
    },
    {
      "epoch": 0.14720867208672087,
      "step": 679,
      "training_loss": 7.174536228179932
    },
    {
      "epoch": 0.14720867208672087,
      "step": 679,
      "training_loss": 7.372771739959717
    },
    {
      "epoch": 0.14720867208672087,
      "step": 679,
      "training_loss": 6.212368011474609
    },
    {
      "epoch": 0.14742547425474256,
      "grad_norm": 8.009778022766113,
      "learning_rate": 1e-05,
      "loss": 7.1204,
      "step": 680
    },
    {
      "epoch": 0.14742547425474256,
      "step": 680,
      "training_loss": 8.101927757263184
    },
    {
      "epoch": 0.14742547425474256,
      "step": 680,
      "training_loss": 8.021136283874512
    },
    {
      "epoch": 0.14742547425474256,
      "step": 680,
      "training_loss": 6.8517165184021
    },
    {
      "epoch": 0.14742547425474256,
      "step": 680,
      "training_loss": 6.456894397735596
    },
    {
      "epoch": 0.14764227642276423,
      "step": 681,
      "training_loss": 8.120262145996094
    },
    {
      "epoch": 0.14764227642276423,
      "step": 681,
      "training_loss": 5.69822359085083
    },
    {
      "epoch": 0.14764227642276423,
      "step": 681,
      "training_loss": 8.33031177520752
    },
    {
      "epoch": 0.14764227642276423,
      "step": 681,
      "training_loss": 8.184324264526367
    },
    {
      "epoch": 0.1478590785907859,
      "step": 682,
      "training_loss": 7.660877227783203
    },
    {
      "epoch": 0.1478590785907859,
      "step": 682,
      "training_loss": 6.408108234405518
    },
    {
      "epoch": 0.1478590785907859,
      "step": 682,
      "training_loss": 7.114947319030762
    },
    {
      "epoch": 0.1478590785907859,
      "step": 682,
      "training_loss": 6.465716361999512
    },
    {
      "epoch": 0.1480758807588076,
      "step": 683,
      "training_loss": 6.893744945526123
    },
    {
      "epoch": 0.1480758807588076,
      "step": 683,
      "training_loss": 7.693607807159424
    },
    {
      "epoch": 0.1480758807588076,
      "step": 683,
      "training_loss": 8.601404190063477
    },
    {
      "epoch": 0.1480758807588076,
      "step": 683,
      "training_loss": 7.1552958488464355
    },
    {
      "epoch": 0.14829268292682926,
      "grad_norm": 8.637632369995117,
      "learning_rate": 1e-05,
      "loss": 7.3599,
      "step": 684
    },
    {
      "epoch": 0.14829268292682926,
      "step": 684,
      "training_loss": 8.449953079223633
    },
    {
      "epoch": 0.14829268292682926,
      "step": 684,
      "training_loss": 6.092698574066162
    },
    {
      "epoch": 0.14829268292682926,
      "step": 684,
      "training_loss": 7.23623514175415
    },
    {
      "epoch": 0.14829268292682926,
      "step": 684,
      "training_loss": 6.218639373779297
    },
    {
      "epoch": 0.14850948509485096,
      "step": 685,
      "training_loss": 7.042680740356445
    },
    {
      "epoch": 0.14850948509485096,
      "step": 685,
      "training_loss": 6.683070182800293
    },
    {
      "epoch": 0.14850948509485096,
      "step": 685,
      "training_loss": 7.148660182952881
    },
    {
      "epoch": 0.14850948509485096,
      "step": 685,
      "training_loss": 6.5183491706848145
    },
    {
      "epoch": 0.14872628726287263,
      "step": 686,
      "training_loss": 7.062304496765137
    },
    {
      "epoch": 0.14872628726287263,
      "step": 686,
      "training_loss": 8.05009937286377
    },
    {
      "epoch": 0.14872628726287263,
      "step": 686,
      "training_loss": 6.545003890991211
    },
    {
      "epoch": 0.14872628726287263,
      "step": 686,
      "training_loss": 6.777060031890869
    },
    {
      "epoch": 0.14894308943089432,
      "step": 687,
      "training_loss": 7.287845134735107
    },
    {
      "epoch": 0.14894308943089432,
      "step": 687,
      "training_loss": 7.333927154541016
    },
    {
      "epoch": 0.14894308943089432,
      "step": 687,
      "training_loss": 6.781002998352051
    },
    {
      "epoch": 0.14894308943089432,
      "step": 687,
      "training_loss": 6.593295097351074
    },
    {
      "epoch": 0.149159891598916,
      "grad_norm": 8.430625915527344,
      "learning_rate": 1e-05,
      "loss": 6.9888,
      "step": 688
    },
    {
      "epoch": 0.149159891598916,
      "step": 688,
      "training_loss": 7.843874931335449
    },
    {
      "epoch": 0.149159891598916,
      "step": 688,
      "training_loss": 8.43603801727295
    },
    {
      "epoch": 0.149159891598916,
      "step": 688,
      "training_loss": 7.0038862228393555
    },
    {
      "epoch": 0.149159891598916,
      "step": 688,
      "training_loss": 7.914568901062012
    },
    {
      "epoch": 0.14937669376693766,
      "step": 689,
      "training_loss": 7.252508163452148
    },
    {
      "epoch": 0.14937669376693766,
      "step": 689,
      "training_loss": 7.488450050354004
    },
    {
      "epoch": 0.14937669376693766,
      "step": 689,
      "training_loss": 8.247780799865723
    },
    {
      "epoch": 0.14937669376693766,
      "step": 689,
      "training_loss": 5.2898736000061035
    },
    {
      "epoch": 0.14959349593495935,
      "step": 690,
      "training_loss": 7.816662788391113
    },
    {
      "epoch": 0.14959349593495935,
      "step": 690,
      "training_loss": 6.292922496795654
    },
    {
      "epoch": 0.14959349593495935,
      "step": 690,
      "training_loss": 7.026313304901123
    },
    {
      "epoch": 0.14959349593495935,
      "step": 690,
      "training_loss": 6.4099931716918945
    },
    {
      "epoch": 0.14981029810298102,
      "step": 691,
      "training_loss": 6.112621784210205
    },
    {
      "epoch": 0.14981029810298102,
      "step": 691,
      "training_loss": 6.309559345245361
    },
    {
      "epoch": 0.14981029810298102,
      "step": 691,
      "training_loss": 8.057075500488281
    },
    {
      "epoch": 0.14981029810298102,
      "step": 691,
      "training_loss": 5.60748291015625
    },
    {
      "epoch": 0.15002710027100272,
      "grad_norm": 9.589264869689941,
      "learning_rate": 1e-05,
      "loss": 7.0694,
      "step": 692
    },
    {
      "epoch": 0.15002710027100272,
      "step": 692,
      "training_loss": 6.944591999053955
    },
    {
      "epoch": 0.15002710027100272,
      "step": 692,
      "training_loss": 7.298734188079834
    },
    {
      "epoch": 0.15002710027100272,
      "step": 692,
      "training_loss": 7.7446064949035645
    },
    {
      "epoch": 0.15002710027100272,
      "step": 692,
      "training_loss": 8.947562217712402
    },
    {
      "epoch": 0.15024390243902438,
      "step": 693,
      "training_loss": 8.578198432922363
    },
    {
      "epoch": 0.15024390243902438,
      "step": 693,
      "training_loss": 6.776474952697754
    },
    {
      "epoch": 0.15024390243902438,
      "step": 693,
      "training_loss": 7.689108848571777
    },
    {
      "epoch": 0.15024390243902438,
      "step": 693,
      "training_loss": 7.707582950592041
    },
    {
      "epoch": 0.15046070460704608,
      "step": 694,
      "training_loss": 6.9742536544799805
    },
    {
      "epoch": 0.15046070460704608,
      "step": 694,
      "training_loss": 7.655471324920654
    },
    {
      "epoch": 0.15046070460704608,
      "step": 694,
      "training_loss": 6.792566776275635
    },
    {
      "epoch": 0.15046070460704608,
      "step": 694,
      "training_loss": 7.6118035316467285
    },
    {
      "epoch": 0.15067750677506775,
      "step": 695,
      "training_loss": 6.983582019805908
    },
    {
      "epoch": 0.15067750677506775,
      "step": 695,
      "training_loss": 7.595003604888916
    },
    {
      "epoch": 0.15067750677506775,
      "step": 695,
      "training_loss": 7.40387487411499
    },
    {
      "epoch": 0.15067750677506775,
      "step": 695,
      "training_loss": 6.533393859863281
    },
    {
      "epoch": 0.15089430894308944,
      "grad_norm": 8.779928207397461,
      "learning_rate": 1e-05,
      "loss": 7.4523,
      "step": 696
    },
    {
      "epoch": 0.15089430894308944,
      "step": 696,
      "training_loss": 6.148072242736816
    },
    {
      "epoch": 0.15089430894308944,
      "step": 696,
      "training_loss": 7.678252696990967
    },
    {
      "epoch": 0.15089430894308944,
      "step": 696,
      "training_loss": 7.911110877990723
    },
    {
      "epoch": 0.15089430894308944,
      "step": 696,
      "training_loss": 6.86262845993042
    },
    {
      "epoch": 0.1511111111111111,
      "step": 697,
      "training_loss": 8.367439270019531
    },
    {
      "epoch": 0.1511111111111111,
      "step": 697,
      "training_loss": 7.396990776062012
    },
    {
      "epoch": 0.1511111111111111,
      "step": 697,
      "training_loss": 6.561540126800537
    },
    {
      "epoch": 0.1511111111111111,
      "step": 697,
      "training_loss": 7.649150848388672
    },
    {
      "epoch": 0.15132791327913278,
      "step": 698,
      "training_loss": 6.69673490524292
    },
    {
      "epoch": 0.15132791327913278,
      "step": 698,
      "training_loss": 7.375219821929932
    },
    {
      "epoch": 0.15132791327913278,
      "step": 698,
      "training_loss": 7.70353889465332
    },
    {
      "epoch": 0.15132791327913278,
      "step": 698,
      "training_loss": 6.853096961975098
    },
    {
      "epoch": 0.15154471544715448,
      "step": 699,
      "training_loss": 8.121430397033691
    },
    {
      "epoch": 0.15154471544715448,
      "step": 699,
      "training_loss": 7.880465030670166
    },
    {
      "epoch": 0.15154471544715448,
      "step": 699,
      "training_loss": 6.479558944702148
    },
    {
      "epoch": 0.15154471544715448,
      "step": 699,
      "training_loss": 7.869632720947266
    },
    {
      "epoch": 0.15176151761517614,
      "grad_norm": 9.258282661437988,
      "learning_rate": 1e-05,
      "loss": 7.3472,
      "step": 700
    },
    {
      "epoch": 0.15176151761517614,
      "step": 700,
      "training_loss": 7.030467987060547
    },
    {
      "epoch": 0.15176151761517614,
      "step": 700,
      "training_loss": 5.8309831619262695
    },
    {
      "epoch": 0.15176151761517614,
      "step": 700,
      "training_loss": 5.556358814239502
    },
    {
      "epoch": 0.15176151761517614,
      "step": 700,
      "training_loss": 7.555022239685059
    },
    {
      "epoch": 0.15197831978319784,
      "step": 701,
      "training_loss": 8.32147216796875
    },
    {
      "epoch": 0.15197831978319784,
      "step": 701,
      "training_loss": 6.492121696472168
    },
    {
      "epoch": 0.15197831978319784,
      "step": 701,
      "training_loss": 9.32041072845459
    },
    {
      "epoch": 0.15197831978319784,
      "step": 701,
      "training_loss": 6.08317756652832
    },
    {
      "epoch": 0.1521951219512195,
      "step": 702,
      "training_loss": 8.02802848815918
    },
    {
      "epoch": 0.1521951219512195,
      "step": 702,
      "training_loss": 5.466261386871338
    },
    {
      "epoch": 0.1521951219512195,
      "step": 702,
      "training_loss": 7.287408351898193
    },
    {
      "epoch": 0.1521951219512195,
      "step": 702,
      "training_loss": 7.372259616851807
    },
    {
      "epoch": 0.1524119241192412,
      "step": 703,
      "training_loss": 7.903319835662842
    },
    {
      "epoch": 0.1524119241192412,
      "step": 703,
      "training_loss": 7.544984817504883
    },
    {
      "epoch": 0.1524119241192412,
      "step": 703,
      "training_loss": 7.343652725219727
    },
    {
      "epoch": 0.1524119241192412,
      "step": 703,
      "training_loss": 7.654857158660889
    },
    {
      "epoch": 0.15262872628726287,
      "grad_norm": 8.622983932495117,
      "learning_rate": 1e-05,
      "loss": 7.1744,
      "step": 704
    },
    {
      "epoch": 0.15262872628726287,
      "step": 704,
      "training_loss": 7.812174320220947
    },
    {
      "epoch": 0.15262872628726287,
      "step": 704,
      "training_loss": 5.909177780151367
    },
    {
      "epoch": 0.15262872628726287,
      "step": 704,
      "training_loss": 5.8095245361328125
    },
    {
      "epoch": 0.15262872628726287,
      "step": 704,
      "training_loss": 7.052878379821777
    },
    {
      "epoch": 0.15284552845528454,
      "step": 705,
      "training_loss": 7.985577583312988
    },
    {
      "epoch": 0.15284552845528454,
      "step": 705,
      "training_loss": 6.716677665710449
    },
    {
      "epoch": 0.15284552845528454,
      "step": 705,
      "training_loss": 5.420307636260986
    },
    {
      "epoch": 0.15284552845528454,
      "step": 705,
      "training_loss": 8.052311897277832
    },
    {
      "epoch": 0.15306233062330624,
      "step": 706,
      "training_loss": 7.1189422607421875
    },
    {
      "epoch": 0.15306233062330624,
      "step": 706,
      "training_loss": 6.5761003494262695
    },
    {
      "epoch": 0.15306233062330624,
      "step": 706,
      "training_loss": 6.242554187774658
    },
    {
      "epoch": 0.15306233062330624,
      "step": 706,
      "training_loss": 7.71758508682251
    },
    {
      "epoch": 0.1532791327913279,
      "step": 707,
      "training_loss": 7.018841743469238
    },
    {
      "epoch": 0.1532791327913279,
      "step": 707,
      "training_loss": 6.8804097175598145
    },
    {
      "epoch": 0.1532791327913279,
      "step": 707,
      "training_loss": 8.242166519165039
    },
    {
      "epoch": 0.1532791327913279,
      "step": 707,
      "training_loss": 6.773303508758545
    },
    {
      "epoch": 0.1534959349593496,
      "grad_norm": 8.638007164001465,
      "learning_rate": 1e-05,
      "loss": 6.958,
      "step": 708
    },
    {
      "epoch": 0.1534959349593496,
      "step": 708,
      "training_loss": 5.959477424621582
    },
    {
      "epoch": 0.1534959349593496,
      "step": 708,
      "training_loss": 7.246318340301514
    },
    {
      "epoch": 0.1534959349593496,
      "step": 708,
      "training_loss": 6.894919395446777
    },
    {
      "epoch": 0.1534959349593496,
      "step": 708,
      "training_loss": 6.691508769989014
    },
    {
      "epoch": 0.15371273712737127,
      "step": 709,
      "training_loss": 7.414511203765869
    },
    {
      "epoch": 0.15371273712737127,
      "step": 709,
      "training_loss": 7.229193687438965
    },
    {
      "epoch": 0.15371273712737127,
      "step": 709,
      "training_loss": 6.448711395263672
    },
    {
      "epoch": 0.15371273712737127,
      "step": 709,
      "training_loss": 7.0033111572265625
    },
    {
      "epoch": 0.15392953929539296,
      "step": 710,
      "training_loss": 7.093571186065674
    },
    {
      "epoch": 0.15392953929539296,
      "step": 710,
      "training_loss": 5.801601886749268
    },
    {
      "epoch": 0.15392953929539296,
      "step": 710,
      "training_loss": 8.135292053222656
    },
    {
      "epoch": 0.15392953929539296,
      "step": 710,
      "training_loss": 5.782474040985107
    },
    {
      "epoch": 0.15414634146341463,
      "step": 711,
      "training_loss": 6.3200788497924805
    },
    {
      "epoch": 0.15414634146341463,
      "step": 711,
      "training_loss": 5.94486141204834
    },
    {
      "epoch": 0.15414634146341463,
      "step": 711,
      "training_loss": 7.074196815490723
    },
    {
      "epoch": 0.15414634146341463,
      "step": 711,
      "training_loss": 7.28468656539917
    },
    {
      "epoch": 0.15436314363143633,
      "grad_norm": 11.58680534362793,
      "learning_rate": 1e-05,
      "loss": 6.7703,
      "step": 712
    },
    {
      "epoch": 0.15436314363143633,
      "step": 712,
      "training_loss": 6.849686145782471
    },
    {
      "epoch": 0.15436314363143633,
      "step": 712,
      "training_loss": 7.030930519104004
    },
    {
      "epoch": 0.15436314363143633,
      "step": 712,
      "training_loss": 6.761579513549805
    },
    {
      "epoch": 0.15436314363143633,
      "step": 712,
      "training_loss": 7.286937713623047
    },
    {
      "epoch": 0.154579945799458,
      "step": 713,
      "training_loss": 9.033803939819336
    },
    {
      "epoch": 0.154579945799458,
      "step": 713,
      "training_loss": 7.709596633911133
    },
    {
      "epoch": 0.154579945799458,
      "step": 713,
      "training_loss": 7.591322898864746
    },
    {
      "epoch": 0.154579945799458,
      "step": 713,
      "training_loss": 7.10847282409668
    },
    {
      "epoch": 0.15479674796747966,
      "step": 714,
      "training_loss": 5.561008453369141
    },
    {
      "epoch": 0.15479674796747966,
      "step": 714,
      "training_loss": 7.242018699645996
    },
    {
      "epoch": 0.15479674796747966,
      "step": 714,
      "training_loss": 5.796981334686279
    },
    {
      "epoch": 0.15479674796747966,
      "step": 714,
      "training_loss": 6.962856292724609
    },
    {
      "epoch": 0.15501355013550136,
      "step": 715,
      "training_loss": 6.757320404052734
    },
    {
      "epoch": 0.15501355013550136,
      "step": 715,
      "training_loss": 6.800143718719482
    },
    {
      "epoch": 0.15501355013550136,
      "step": 715,
      "training_loss": 7.972363471984863
    },
    {
      "epoch": 0.15501355013550136,
      "step": 715,
      "training_loss": 5.255859851837158
    },
    {
      "epoch": 0.15523035230352303,
      "grad_norm": 8.860085487365723,
      "learning_rate": 1e-05,
      "loss": 6.9826,
      "step": 716
    },
    {
      "epoch": 0.15523035230352303,
      "step": 716,
      "training_loss": 7.4591965675354
    },
    {
      "epoch": 0.15523035230352303,
      "step": 716,
      "training_loss": 6.7074995040893555
    },
    {
      "epoch": 0.15523035230352303,
      "step": 716,
      "training_loss": 6.161533355712891
    },
    {
      "epoch": 0.15523035230352303,
      "step": 716,
      "training_loss": 6.999730587005615
    },
    {
      "epoch": 0.15544715447154472,
      "step": 717,
      "training_loss": 5.71613883972168
    },
    {
      "epoch": 0.15544715447154472,
      "step": 717,
      "training_loss": 6.86704683303833
    },
    {
      "epoch": 0.15544715447154472,
      "step": 717,
      "training_loss": 6.0736894607543945
    },
    {
      "epoch": 0.15544715447154472,
      "step": 717,
      "training_loss": 7.124951362609863
    },
    {
      "epoch": 0.1556639566395664,
      "step": 718,
      "training_loss": 6.697651386260986
    },
    {
      "epoch": 0.1556639566395664,
      "step": 718,
      "training_loss": 8.009465217590332
    },
    {
      "epoch": 0.1556639566395664,
      "step": 718,
      "training_loss": 7.249051094055176
    },
    {
      "epoch": 0.1556639566395664,
      "step": 718,
      "training_loss": 6.208799839019775
    },
    {
      "epoch": 0.1558807588075881,
      "step": 719,
      "training_loss": 7.2842583656311035
    },
    {
      "epoch": 0.1558807588075881,
      "step": 719,
      "training_loss": 7.352067470550537
    },
    {
      "epoch": 0.1558807588075881,
      "step": 719,
      "training_loss": 7.21425199508667
    },
    {
      "epoch": 0.1558807588075881,
      "step": 719,
      "training_loss": 7.694626808166504
    },
    {
      "epoch": 0.15609756097560976,
      "grad_norm": 8.041057586669922,
      "learning_rate": 1e-05,
      "loss": 6.9262,
      "step": 720
    },
    {
      "epoch": 0.15609756097560976,
      "step": 720,
      "training_loss": 7.710392475128174
    },
    {
      "epoch": 0.15609756097560976,
      "step": 720,
      "training_loss": 5.673832416534424
    },
    {
      "epoch": 0.15609756097560976,
      "step": 720,
      "training_loss": 7.350873947143555
    },
    {
      "epoch": 0.15609756097560976,
      "step": 720,
      "training_loss": 7.452645301818848
    },
    {
      "epoch": 0.15631436314363142,
      "step": 721,
      "training_loss": 7.4787726402282715
    },
    {
      "epoch": 0.15631436314363142,
      "step": 721,
      "training_loss": 7.880707740783691
    },
    {
      "epoch": 0.15631436314363142,
      "step": 721,
      "training_loss": 7.578438758850098
    },
    {
      "epoch": 0.15631436314363142,
      "step": 721,
      "training_loss": 8.347559928894043
    },
    {
      "epoch": 0.15653116531165312,
      "step": 722,
      "training_loss": 6.601321220397949
    },
    {
      "epoch": 0.15653116531165312,
      "step": 722,
      "training_loss": 8.1145658493042
    },
    {
      "epoch": 0.15653116531165312,
      "step": 722,
      "training_loss": 7.230293273925781
    },
    {
      "epoch": 0.15653116531165312,
      "step": 722,
      "training_loss": 7.785109996795654
    },
    {
      "epoch": 0.1567479674796748,
      "step": 723,
      "training_loss": 7.164449214935303
    },
    {
      "epoch": 0.1567479674796748,
      "step": 723,
      "training_loss": 10.3302640914917
    },
    {
      "epoch": 0.1567479674796748,
      "step": 723,
      "training_loss": 6.7654948234558105
    },
    {
      "epoch": 0.1567479674796748,
      "step": 723,
      "training_loss": 6.416160583496094
    },
    {
      "epoch": 0.15696476964769648,
      "grad_norm": 17.19939422607422,
      "learning_rate": 1e-05,
      "loss": 7.4926,
      "step": 724
    },
    {
      "epoch": 0.15696476964769648,
      "step": 724,
      "training_loss": 7.297775745391846
    },
    {
      "epoch": 0.15696476964769648,
      "step": 724,
      "training_loss": 7.254092216491699
    },
    {
      "epoch": 0.15696476964769648,
      "step": 724,
      "training_loss": 6.75316858291626
    },
    {
      "epoch": 0.15696476964769648,
      "step": 724,
      "training_loss": 7.239148139953613
    },
    {
      "epoch": 0.15718157181571815,
      "step": 725,
      "training_loss": 7.915564060211182
    },
    {
      "epoch": 0.15718157181571815,
      "step": 725,
      "training_loss": 6.573504447937012
    },
    {
      "epoch": 0.15718157181571815,
      "step": 725,
      "training_loss": 7.444130897521973
    },
    {
      "epoch": 0.15718157181571815,
      "step": 725,
      "training_loss": 6.038114070892334
    },
    {
      "epoch": 0.15739837398373985,
      "step": 726,
      "training_loss": 7.86410665512085
    },
    {
      "epoch": 0.15739837398373985,
      "step": 726,
      "training_loss": 7.0940260887146
    },
    {
      "epoch": 0.15739837398373985,
      "step": 726,
      "training_loss": 7.964905738830566
    },
    {
      "epoch": 0.15739837398373985,
      "step": 726,
      "training_loss": 5.967769622802734
    },
    {
      "epoch": 0.15761517615176152,
      "step": 727,
      "training_loss": 7.103727340698242
    },
    {
      "epoch": 0.15761517615176152,
      "step": 727,
      "training_loss": 7.943150997161865
    },
    {
      "epoch": 0.15761517615176152,
      "step": 727,
      "training_loss": 6.697291374206543
    },
    {
      "epoch": 0.15761517615176152,
      "step": 727,
      "training_loss": 8.190081596374512
    },
    {
      "epoch": 0.1578319783197832,
      "grad_norm": 9.935938835144043,
      "learning_rate": 1e-05,
      "loss": 7.2088,
      "step": 728
    },
    {
      "epoch": 0.1578319783197832,
      "step": 728,
      "training_loss": 7.934054851531982
    },
    {
      "epoch": 0.1578319783197832,
      "step": 728,
      "training_loss": 7.397598743438721
    },
    {
      "epoch": 0.1578319783197832,
      "step": 728,
      "training_loss": 6.877037048339844
    },
    {
      "epoch": 0.1578319783197832,
      "step": 728,
      "training_loss": 7.371365547180176
    },
    {
      "epoch": 0.15804878048780488,
      "step": 729,
      "training_loss": 6.468707084655762
    },
    {
      "epoch": 0.15804878048780488,
      "step": 729,
      "training_loss": 7.584269046783447
    },
    {
      "epoch": 0.15804878048780488,
      "step": 729,
      "training_loss": 7.97596549987793
    },
    {
      "epoch": 0.15804878048780488,
      "step": 729,
      "training_loss": 5.887796401977539
    },
    {
      "epoch": 0.15826558265582655,
      "step": 730,
      "training_loss": 6.6856255531311035
    },
    {
      "epoch": 0.15826558265582655,
      "step": 730,
      "training_loss": 7.671935558319092
    },
    {
      "epoch": 0.15826558265582655,
      "step": 730,
      "training_loss": 7.419550895690918
    },
    {
      "epoch": 0.15826558265582655,
      "step": 730,
      "training_loss": 7.113692283630371
    },
    {
      "epoch": 0.15848238482384824,
      "step": 731,
      "training_loss": 6.560783386230469
    },
    {
      "epoch": 0.15848238482384824,
      "step": 731,
      "training_loss": 6.018510341644287
    },
    {
      "epoch": 0.15848238482384824,
      "step": 731,
      "training_loss": 6.281029224395752
    },
    {
      "epoch": 0.15848238482384824,
      "step": 731,
      "training_loss": 7.373250961303711
    },
    {
      "epoch": 0.1586991869918699,
      "grad_norm": 9.6453218460083,
      "learning_rate": 1e-05,
      "loss": 7.0388,
      "step": 732
    },
    {
      "epoch": 0.1586991869918699,
      "step": 732,
      "training_loss": 6.59787130355835
    },
    {
      "epoch": 0.1586991869918699,
      "step": 732,
      "training_loss": 6.1958088874816895
    },
    {
      "epoch": 0.1586991869918699,
      "step": 732,
      "training_loss": 5.841987609863281
    },
    {
      "epoch": 0.1586991869918699,
      "step": 732,
      "training_loss": 7.673733234405518
    },
    {
      "epoch": 0.1589159891598916,
      "step": 733,
      "training_loss": 7.844405174255371
    },
    {
      "epoch": 0.1589159891598916,
      "step": 733,
      "training_loss": 7.504095554351807
    },
    {
      "epoch": 0.1589159891598916,
      "step": 733,
      "training_loss": 7.97158670425415
    },
    {
      "epoch": 0.1589159891598916,
      "step": 733,
      "training_loss": 7.09932279586792
    },
    {
      "epoch": 0.15913279132791328,
      "step": 734,
      "training_loss": 7.428367614746094
    },
    {
      "epoch": 0.15913279132791328,
      "step": 734,
      "training_loss": 5.931997299194336
    },
    {
      "epoch": 0.15913279132791328,
      "step": 734,
      "training_loss": 5.997110366821289
    },
    {
      "epoch": 0.15913279132791328,
      "step": 734,
      "training_loss": 7.226871490478516
    },
    {
      "epoch": 0.15934959349593497,
      "step": 735,
      "training_loss": 6.523054599761963
    },
    {
      "epoch": 0.15934959349593497,
      "step": 735,
      "training_loss": 5.714462757110596
    },
    {
      "epoch": 0.15934959349593497,
      "step": 735,
      "training_loss": 7.630645751953125
    },
    {
      "epoch": 0.15934959349593497,
      "step": 735,
      "training_loss": 5.443638324737549
    },
    {
      "epoch": 0.15956639566395664,
      "grad_norm": 11.290186882019043,
      "learning_rate": 1e-05,
      "loss": 6.7891,
      "step": 736
    },
    {
      "epoch": 0.15956639566395664,
      "step": 736,
      "training_loss": 7.464349269866943
    },
    {
      "epoch": 0.15956639566395664,
      "step": 736,
      "training_loss": 5.307511329650879
    },
    {
      "epoch": 0.15956639566395664,
      "step": 736,
      "training_loss": 6.255017280578613
    },
    {
      "epoch": 0.15956639566395664,
      "step": 736,
      "training_loss": 6.451249599456787
    },
    {
      "epoch": 0.1597831978319783,
      "step": 737,
      "training_loss": 6.7138566970825195
    },
    {
      "epoch": 0.1597831978319783,
      "step": 737,
      "training_loss": 7.377171039581299
    },
    {
      "epoch": 0.1597831978319783,
      "step": 737,
      "training_loss": 7.201420307159424
    },
    {
      "epoch": 0.1597831978319783,
      "step": 737,
      "training_loss": 6.3959059715271
    },
    {
      "epoch": 0.16,
      "step": 738,
      "training_loss": 6.23306131362915
    },
    {
      "epoch": 0.16,
      "step": 738,
      "training_loss": 5.694160461425781
    },
    {
      "epoch": 0.16,
      "step": 738,
      "training_loss": 7.187521457672119
    },
    {
      "epoch": 0.16,
      "step": 738,
      "training_loss": 8.979655265808105
    },
    {
      "epoch": 0.16021680216802167,
      "step": 739,
      "training_loss": 6.956150531768799
    },
    {
      "epoch": 0.16021680216802167,
      "step": 739,
      "training_loss": 5.67344856262207
    },
    {
      "epoch": 0.16021680216802167,
      "step": 739,
      "training_loss": 7.267752647399902
    },
    {
      "epoch": 0.16021680216802167,
      "step": 739,
      "training_loss": 6.5684309005737305
    },
    {
      "epoch": 0.16043360433604337,
      "grad_norm": 10.0569429397583,
      "learning_rate": 1e-05,
      "loss": 6.7329,
      "step": 740
    },
    {
      "epoch": 0.16043360433604337,
      "step": 740,
      "training_loss": 6.244104385375977
    },
    {
      "epoch": 0.16043360433604337,
      "step": 740,
      "training_loss": 7.919495105743408
    },
    {
      "epoch": 0.16043360433604337,
      "step": 740,
      "training_loss": 8.121923446655273
    },
    {
      "epoch": 0.16043360433604337,
      "step": 740,
      "training_loss": 6.918493270874023
    },
    {
      "epoch": 0.16065040650406504,
      "step": 741,
      "training_loss": 7.184842586517334
    },
    {
      "epoch": 0.16065040650406504,
      "step": 741,
      "training_loss": 7.298986911773682
    },
    {
      "epoch": 0.16065040650406504,
      "step": 741,
      "training_loss": 6.329951763153076
    },
    {
      "epoch": 0.16065040650406504,
      "step": 741,
      "training_loss": 9.115906715393066
    },
    {
      "epoch": 0.16086720867208673,
      "step": 742,
      "training_loss": 7.0547261238098145
    },
    {
      "epoch": 0.16086720867208673,
      "step": 742,
      "training_loss": 8.008286476135254
    },
    {
      "epoch": 0.16086720867208673,
      "step": 742,
      "training_loss": 5.8476457595825195
    },
    {
      "epoch": 0.16086720867208673,
      "step": 742,
      "training_loss": 7.836241245269775
    },
    {
      "epoch": 0.1610840108401084,
      "step": 743,
      "training_loss": 6.950997829437256
    },
    {
      "epoch": 0.1610840108401084,
      "step": 743,
      "training_loss": 5.062143325805664
    },
    {
      "epoch": 0.1610840108401084,
      "step": 743,
      "training_loss": 7.1118950843811035
    },
    {
      "epoch": 0.1610840108401084,
      "step": 743,
      "training_loss": 7.521003246307373
    },
    {
      "epoch": 0.1613008130081301,
      "grad_norm": 12.965163230895996,
      "learning_rate": 1e-05,
      "loss": 7.1579,
      "step": 744
    },
    {
      "epoch": 0.1613008130081301,
      "step": 744,
      "training_loss": 5.460243225097656
    },
    {
      "epoch": 0.1613008130081301,
      "step": 744,
      "training_loss": 8.338834762573242
    },
    {
      "epoch": 0.1613008130081301,
      "step": 744,
      "training_loss": 8.477792739868164
    },
    {
      "epoch": 0.1613008130081301,
      "step": 744,
      "training_loss": 8.510186195373535
    },
    {
      "epoch": 0.16151761517615176,
      "step": 745,
      "training_loss": 7.366612434387207
    },
    {
      "epoch": 0.16151761517615176,
      "step": 745,
      "training_loss": 5.444656848907471
    },
    {
      "epoch": 0.16151761517615176,
      "step": 745,
      "training_loss": 6.648079872131348
    },
    {
      "epoch": 0.16151761517615176,
      "step": 745,
      "training_loss": 6.5677080154418945
    },
    {
      "epoch": 0.16173441734417343,
      "step": 746,
      "training_loss": 6.497233867645264
    },
    {
      "epoch": 0.16173441734417343,
      "step": 746,
      "training_loss": 7.339270114898682
    },
    {
      "epoch": 0.16173441734417343,
      "step": 746,
      "training_loss": 6.455827236175537
    },
    {
      "epoch": 0.16173441734417343,
      "step": 746,
      "training_loss": 6.312677383422852
    },
    {
      "epoch": 0.16195121951219513,
      "step": 747,
      "training_loss": 7.146262168884277
    },
    {
      "epoch": 0.16195121951219513,
      "step": 747,
      "training_loss": 9.07483959197998
    },
    {
      "epoch": 0.16195121951219513,
      "step": 747,
      "training_loss": 6.557300090789795
    },
    {
      "epoch": 0.16195121951219513,
      "step": 747,
      "training_loss": 5.9276041984558105
    },
    {
      "epoch": 0.1621680216802168,
      "grad_norm": 14.075063705444336,
      "learning_rate": 1e-05,
      "loss": 7.0078,
      "step": 748
    },
    {
      "epoch": 0.1621680216802168,
      "step": 748,
      "training_loss": 8.006896018981934
    },
    {
      "epoch": 0.1621680216802168,
      "step": 748,
      "training_loss": 7.5673909187316895
    },
    {
      "epoch": 0.1621680216802168,
      "step": 748,
      "training_loss": 5.856039524078369
    },
    {
      "epoch": 0.1621680216802168,
      "step": 748,
      "training_loss": 5.972984790802002
    },
    {
      "epoch": 0.1623848238482385,
      "step": 749,
      "training_loss": 7.561403274536133
    },
    {
      "epoch": 0.1623848238482385,
      "step": 749,
      "training_loss": 7.06856632232666
    },
    {
      "epoch": 0.1623848238482385,
      "step": 749,
      "training_loss": 7.3454108238220215
    },
    {
      "epoch": 0.1623848238482385,
      "step": 749,
      "training_loss": 7.717676639556885
    },
    {
      "epoch": 0.16260162601626016,
      "step": 750,
      "training_loss": 7.056063652038574
    },
    {
      "epoch": 0.16260162601626016,
      "step": 750,
      "training_loss": 6.363266468048096
    },
    {
      "epoch": 0.16260162601626016,
      "step": 750,
      "training_loss": 7.54683256149292
    },
    {
      "epoch": 0.16260162601626016,
      "step": 750,
      "training_loss": 6.342522144317627
    },
    {
      "epoch": 0.16281842818428185,
      "step": 751,
      "training_loss": 8.633951187133789
    },
    {
      "epoch": 0.16281842818428185,
      "step": 751,
      "training_loss": 8.731681823730469
    },
    {
      "epoch": 0.16281842818428185,
      "step": 751,
      "training_loss": 7.0735764503479
    },
    {
      "epoch": 0.16281842818428185,
      "step": 751,
      "training_loss": 5.569624423980713
    },
    {
      "epoch": 0.16303523035230352,
      "grad_norm": 13.974790573120117,
      "learning_rate": 1e-05,
      "loss": 7.1509,
      "step": 752
    },
    {
      "epoch": 0.16303523035230352,
      "step": 752,
      "training_loss": 7.840026378631592
    },
    {
      "epoch": 0.16303523035230352,
      "step": 752,
      "training_loss": 7.616765975952148
    },
    {
      "epoch": 0.16303523035230352,
      "step": 752,
      "training_loss": 5.288039207458496
    },
    {
      "epoch": 0.16303523035230352,
      "step": 752,
      "training_loss": 7.358343601226807
    },
    {
      "epoch": 0.1632520325203252,
      "step": 753,
      "training_loss": 7.27634859085083
    },
    {
      "epoch": 0.1632520325203252,
      "step": 753,
      "training_loss": 7.6418304443359375
    },
    {
      "epoch": 0.1632520325203252,
      "step": 753,
      "training_loss": 7.2174506187438965
    },
    {
      "epoch": 0.1632520325203252,
      "step": 753,
      "training_loss": 6.411586284637451
    },
    {
      "epoch": 0.1634688346883469,
      "step": 754,
      "training_loss": 6.900961875915527
    },
    {
      "epoch": 0.1634688346883469,
      "step": 754,
      "training_loss": 6.968139171600342
    },
    {
      "epoch": 0.1634688346883469,
      "step": 754,
      "training_loss": 5.8199896812438965
    },
    {
      "epoch": 0.1634688346883469,
      "step": 754,
      "training_loss": 6.450069904327393
    },
    {
      "epoch": 0.16368563685636855,
      "step": 755,
      "training_loss": 6.636783599853516
    },
    {
      "epoch": 0.16368563685636855,
      "step": 755,
      "training_loss": 6.720037937164307
    },
    {
      "epoch": 0.16368563685636855,
      "step": 755,
      "training_loss": 6.309404373168945
    },
    {
      "epoch": 0.16368563685636855,
      "step": 755,
      "training_loss": 5.681249618530273
    },
    {
      "epoch": 0.16390243902439025,
      "grad_norm": 12.393040657043457,
      "learning_rate": 1e-05,
      "loss": 6.7586,
      "step": 756
    },
    {
      "epoch": 0.16390243902439025,
      "step": 756,
      "training_loss": 6.951699733734131
    },
    {
      "epoch": 0.16390243902439025,
      "step": 756,
      "training_loss": 7.504156589508057
    },
    {
      "epoch": 0.16390243902439025,
      "step": 756,
      "training_loss": 7.197661399841309
    },
    {
      "epoch": 0.16390243902439025,
      "step": 756,
      "training_loss": 7.398370265960693
    },
    {
      "epoch": 0.16411924119241192,
      "step": 757,
      "training_loss": 7.896425247192383
    },
    {
      "epoch": 0.16411924119241192,
      "step": 757,
      "training_loss": 8.971856117248535
    },
    {
      "epoch": 0.16411924119241192,
      "step": 757,
      "training_loss": 6.46296501159668
    },
    {
      "epoch": 0.16411924119241192,
      "step": 757,
      "training_loss": 5.605593204498291
    },
    {
      "epoch": 0.16433604336043361,
      "step": 758,
      "training_loss": 6.434176445007324
    },
    {
      "epoch": 0.16433604336043361,
      "step": 758,
      "training_loss": 7.443437099456787
    },
    {
      "epoch": 0.16433604336043361,
      "step": 758,
      "training_loss": 5.2601799964904785
    },
    {
      "epoch": 0.16433604336043361,
      "step": 758,
      "training_loss": 7.065011024475098
    },
    {
      "epoch": 0.16455284552845528,
      "step": 759,
      "training_loss": 6.256528854370117
    },
    {
      "epoch": 0.16455284552845528,
      "step": 759,
      "training_loss": 6.231419086456299
    },
    {
      "epoch": 0.16455284552845528,
      "step": 759,
      "training_loss": 8.050885200500488
    },
    {
      "epoch": 0.16455284552845528,
      "step": 759,
      "training_loss": 7.220293998718262
    },
    {
      "epoch": 0.16476964769647698,
      "grad_norm": 9.28491497039795,
      "learning_rate": 1e-05,
      "loss": 6.9969,
      "step": 760
    },
    {
      "epoch": 0.16476964769647698,
      "step": 760,
      "training_loss": 6.95959997177124
    },
    {
      "epoch": 0.16476964769647698,
      "step": 760,
      "training_loss": 8.705526351928711
    },
    {
      "epoch": 0.16476964769647698,
      "step": 760,
      "training_loss": 8.727036476135254
    },
    {
      "epoch": 0.16476964769647698,
      "step": 760,
      "training_loss": 5.517380237579346
    },
    {
      "epoch": 0.16498644986449865,
      "step": 761,
      "training_loss": 7.507992267608643
    },
    {
      "epoch": 0.16498644986449865,
      "step": 761,
      "training_loss": 6.41192626953125
    },
    {
      "epoch": 0.16498644986449865,
      "step": 761,
      "training_loss": 8.236661911010742
    },
    {
      "epoch": 0.16498644986449865,
      "step": 761,
      "training_loss": 8.467326164245605
    },
    {
      "epoch": 0.16520325203252031,
      "step": 762,
      "training_loss": 7.4355292320251465
    },
    {
      "epoch": 0.16520325203252031,
      "step": 762,
      "training_loss": 7.225676536560059
    },
    {
      "epoch": 0.16520325203252031,
      "step": 762,
      "training_loss": 6.937514305114746
    },
    {
      "epoch": 0.16520325203252031,
      "step": 762,
      "training_loss": 7.482503414154053
    },
    {
      "epoch": 0.165420054200542,
      "step": 763,
      "training_loss": 5.641543865203857
    },
    {
      "epoch": 0.165420054200542,
      "step": 763,
      "training_loss": 6.729276180267334
    },
    {
      "epoch": 0.165420054200542,
      "step": 763,
      "training_loss": 7.264945983886719
    },
    {
      "epoch": 0.165420054200542,
      "step": 763,
      "training_loss": 8.20069408416748
    },
    {
      "epoch": 0.16563685636856368,
      "grad_norm": 8.33326530456543,
      "learning_rate": 1e-05,
      "loss": 7.3407,
      "step": 764
    },
    {
      "epoch": 0.16563685636856368,
      "step": 764,
      "training_loss": 7.086641788482666
    },
    {
      "epoch": 0.16563685636856368,
      "step": 764,
      "training_loss": 7.6157732009887695
    },
    {
      "epoch": 0.16563685636856368,
      "step": 764,
      "training_loss": 7.25689172744751
    },
    {
      "epoch": 0.16563685636856368,
      "step": 764,
      "training_loss": 7.3634867668151855
    },
    {
      "epoch": 0.16585365853658537,
      "step": 765,
      "training_loss": 7.484757900238037
    },
    {
      "epoch": 0.16585365853658537,
      "step": 765,
      "training_loss": 6.550750732421875
    },
    {
      "epoch": 0.16585365853658537,
      "step": 765,
      "training_loss": 5.9314374923706055
    },
    {
      "epoch": 0.16585365853658537,
      "step": 765,
      "training_loss": 7.975395679473877
    },
    {
      "epoch": 0.16607046070460704,
      "step": 766,
      "training_loss": 7.568387508392334
    },
    {
      "epoch": 0.16607046070460704,
      "step": 766,
      "training_loss": 7.632331371307373
    },
    {
      "epoch": 0.16607046070460704,
      "step": 766,
      "training_loss": 8.138890266418457
    },
    {
      "epoch": 0.16607046070460704,
      "step": 766,
      "training_loss": 7.667007923126221
    },
    {
      "epoch": 0.16628726287262874,
      "step": 767,
      "training_loss": 7.61824893951416
    },
    {
      "epoch": 0.16628726287262874,
      "step": 767,
      "training_loss": 7.937856674194336
    },
    {
      "epoch": 0.16628726287262874,
      "step": 767,
      "training_loss": 8.544729232788086
    },
    {
      "epoch": 0.16628726287262874,
      "step": 767,
      "training_loss": 6.40806245803833
    },
    {
      "epoch": 0.1665040650406504,
      "grad_norm": 8.572357177734375,
      "learning_rate": 1e-05,
      "loss": 7.4238,
      "step": 768
    },
    {
      "epoch": 0.1665040650406504,
      "step": 768,
      "training_loss": 7.4458208084106445
    },
    {
      "epoch": 0.1665040650406504,
      "step": 768,
      "training_loss": 7.594648838043213
    },
    {
      "epoch": 0.1665040650406504,
      "step": 768,
      "training_loss": 7.095149517059326
    },
    {
      "epoch": 0.1665040650406504,
      "step": 768,
      "training_loss": 5.969532489776611
    },
    {
      "epoch": 0.16672086720867207,
      "step": 769,
      "training_loss": 6.8944926261901855
    },
    {
      "epoch": 0.16672086720867207,
      "step": 769,
      "training_loss": 7.890063285827637
    },
    {
      "epoch": 0.16672086720867207,
      "step": 769,
      "training_loss": 5.807255268096924
    },
    {
      "epoch": 0.16672086720867207,
      "step": 769,
      "training_loss": 6.778004169464111
    },
    {
      "epoch": 0.16693766937669377,
      "step": 770,
      "training_loss": 7.68062686920166
    },
    {
      "epoch": 0.16693766937669377,
      "step": 770,
      "training_loss": 6.7813615798950195
    },
    {
      "epoch": 0.16693766937669377,
      "step": 770,
      "training_loss": 7.005338668823242
    },
    {
      "epoch": 0.16693766937669377,
      "step": 770,
      "training_loss": 7.582813262939453
    },
    {
      "epoch": 0.16715447154471544,
      "step": 771,
      "training_loss": 7.20383882522583
    },
    {
      "epoch": 0.16715447154471544,
      "step": 771,
      "training_loss": 6.341447353363037
    },
    {
      "epoch": 0.16715447154471544,
      "step": 771,
      "training_loss": 7.3400654792785645
    },
    {
      "epoch": 0.16715447154471544,
      "step": 771,
      "training_loss": 7.805850982666016
    },
    {
      "epoch": 0.16737127371273713,
      "grad_norm": 7.7894463539123535,
      "learning_rate": 1e-05,
      "loss": 7.076,
      "step": 772
    },
    {
      "epoch": 0.16737127371273713,
      "step": 772,
      "training_loss": 7.550410747528076
    },
    {
      "epoch": 0.16737127371273713,
      "step": 772,
      "training_loss": 9.259661674499512
    },
    {
      "epoch": 0.16737127371273713,
      "step": 772,
      "training_loss": 6.415372371673584
    },
    {
      "epoch": 0.16737127371273713,
      "step": 772,
      "training_loss": 7.1147541999816895
    },
    {
      "epoch": 0.1675880758807588,
      "step": 773,
      "training_loss": 8.33450698852539
    },
    {
      "epoch": 0.1675880758807588,
      "step": 773,
      "training_loss": 6.94950008392334
    },
    {
      "epoch": 0.1675880758807588,
      "step": 773,
      "training_loss": 7.3309125900268555
    },
    {
      "epoch": 0.1675880758807588,
      "step": 773,
      "training_loss": 6.3152642250061035
    },
    {
      "epoch": 0.1678048780487805,
      "step": 774,
      "training_loss": 6.100374221801758
    },
    {
      "epoch": 0.1678048780487805,
      "step": 774,
      "training_loss": 7.920892715454102
    },
    {
      "epoch": 0.1678048780487805,
      "step": 774,
      "training_loss": 7.095296382904053
    },
    {
      "epoch": 0.1678048780487805,
      "step": 774,
      "training_loss": 7.4029741287231445
    },
    {
      "epoch": 0.16802168021680217,
      "step": 775,
      "training_loss": 6.7282586097717285
    },
    {
      "epoch": 0.16802168021680217,
      "step": 775,
      "training_loss": 6.756066799163818
    },
    {
      "epoch": 0.16802168021680217,
      "step": 775,
      "training_loss": 6.877449989318848
    },
    {
      "epoch": 0.16802168021680217,
      "step": 775,
      "training_loss": 8.843703269958496
    },
    {
      "epoch": 0.16823848238482386,
      "grad_norm": 14.068558692932129,
      "learning_rate": 1e-05,
      "loss": 7.3122,
      "step": 776
    },
    {
      "epoch": 0.16823848238482386,
      "step": 776,
      "training_loss": 7.154531002044678
    },
    {
      "epoch": 0.16823848238482386,
      "step": 776,
      "training_loss": 6.661750316619873
    },
    {
      "epoch": 0.16823848238482386,
      "step": 776,
      "training_loss": 6.3830156326293945
    },
    {
      "epoch": 0.16823848238482386,
      "step": 776,
      "training_loss": 7.446633815765381
    },
    {
      "epoch": 0.16845528455284553,
      "step": 777,
      "training_loss": 7.467200756072998
    },
    {
      "epoch": 0.16845528455284553,
      "step": 777,
      "training_loss": 7.7809929847717285
    },
    {
      "epoch": 0.16845528455284553,
      "step": 777,
      "training_loss": 7.532763481140137
    },
    {
      "epoch": 0.16845528455284553,
      "step": 777,
      "training_loss": 7.069321632385254
    },
    {
      "epoch": 0.1686720867208672,
      "step": 778,
      "training_loss": 6.666829586029053
    },
    {
      "epoch": 0.1686720867208672,
      "step": 778,
      "training_loss": 6.729734897613525
    },
    {
      "epoch": 0.1686720867208672,
      "step": 778,
      "training_loss": 8.047247886657715
    },
    {
      "epoch": 0.1686720867208672,
      "step": 778,
      "training_loss": 7.445354461669922
    },
    {
      "epoch": 0.1688888888888889,
      "step": 779,
      "training_loss": 8.056983947753906
    },
    {
      "epoch": 0.1688888888888889,
      "step": 779,
      "training_loss": 7.447768211364746
    },
    {
      "epoch": 0.1688888888888889,
      "step": 779,
      "training_loss": 7.610297679901123
    },
    {
      "epoch": 0.1688888888888889,
      "step": 779,
      "training_loss": 7.414412975311279
    },
    {
      "epoch": 0.16910569105691056,
      "grad_norm": 8.444063186645508,
      "learning_rate": 1e-05,
      "loss": 7.3072,
      "step": 780
    },
    {
      "epoch": 0.16910569105691056,
      "step": 780,
      "training_loss": 6.923367023468018
    },
    {
      "epoch": 0.16910569105691056,
      "step": 780,
      "training_loss": 8.491064071655273
    },
    {
      "epoch": 0.16910569105691056,
      "step": 780,
      "training_loss": 7.770700454711914
    },
    {
      "epoch": 0.16910569105691056,
      "step": 780,
      "training_loss": 6.343165874481201
    },
    {
      "epoch": 0.16932249322493226,
      "step": 781,
      "training_loss": 8.539068222045898
    },
    {
      "epoch": 0.16932249322493226,
      "step": 781,
      "training_loss": 8.747261047363281
    },
    {
      "epoch": 0.16932249322493226,
      "step": 781,
      "training_loss": 6.936436653137207
    },
    {
      "epoch": 0.16932249322493226,
      "step": 781,
      "training_loss": 7.332629203796387
    },
    {
      "epoch": 0.16953929539295393,
      "step": 782,
      "training_loss": 6.83334493637085
    },
    {
      "epoch": 0.16953929539295393,
      "step": 782,
      "training_loss": 6.433723449707031
    },
    {
      "epoch": 0.16953929539295393,
      "step": 782,
      "training_loss": 7.838510036468506
    },
    {
      "epoch": 0.16953929539295393,
      "step": 782,
      "training_loss": 6.165923595428467
    },
    {
      "epoch": 0.16975609756097562,
      "step": 783,
      "training_loss": 6.647995948791504
    },
    {
      "epoch": 0.16975609756097562,
      "step": 783,
      "training_loss": 6.556159496307373
    },
    {
      "epoch": 0.16975609756097562,
      "step": 783,
      "training_loss": 6.321399211883545
    },
    {
      "epoch": 0.16975609756097562,
      "step": 783,
      "training_loss": 7.451937198638916
    },
    {
      "epoch": 0.1699728997289973,
      "grad_norm": 7.768092632293701,
      "learning_rate": 1e-05,
      "loss": 7.2083,
      "step": 784
    },
    {
      "epoch": 0.1699728997289973,
      "step": 784,
      "training_loss": 7.1944146156311035
    },
    {
      "epoch": 0.1699728997289973,
      "step": 784,
      "training_loss": 8.14492130279541
    },
    {
      "epoch": 0.1699728997289973,
      "step": 784,
      "training_loss": 6.813396453857422
    },
    {
      "epoch": 0.1699728997289973,
      "step": 784,
      "training_loss": 7.070866107940674
    },
    {
      "epoch": 0.17018970189701896,
      "step": 785,
      "training_loss": 7.5733256340026855
    },
    {
      "epoch": 0.17018970189701896,
      "step": 785,
      "training_loss": 7.310108661651611
    },
    {
      "epoch": 0.17018970189701896,
      "step": 785,
      "training_loss": 7.646653175354004
    },
    {
      "epoch": 0.17018970189701896,
      "step": 785,
      "training_loss": 7.8057990074157715
    },
    {
      "epoch": 0.17040650406504065,
      "step": 786,
      "training_loss": 7.227962970733643
    },
    {
      "epoch": 0.17040650406504065,
      "step": 786,
      "training_loss": 7.460399627685547
    },
    {
      "epoch": 0.17040650406504065,
      "step": 786,
      "training_loss": 10.749659538269043
    },
    {
      "epoch": 0.17040650406504065,
      "step": 786,
      "training_loss": 7.301640033721924
    },
    {
      "epoch": 0.17062330623306232,
      "step": 787,
      "training_loss": 6.835189342498779
    },
    {
      "epoch": 0.17062330623306232,
      "step": 787,
      "training_loss": 6.297918319702148
    },
    {
      "epoch": 0.17062330623306232,
      "step": 787,
      "training_loss": 7.33172607421875
    },
    {
      "epoch": 0.17062330623306232,
      "step": 787,
      "training_loss": 8.268495559692383
    },
    {
      "epoch": 0.17084010840108402,
      "grad_norm": 12.736845970153809,
      "learning_rate": 1e-05,
      "loss": 7.5645,
      "step": 788
    },
    {
      "epoch": 0.17084010840108402,
      "step": 788,
      "training_loss": 7.2564849853515625
    },
    {
      "epoch": 0.17084010840108402,
      "step": 788,
      "training_loss": 6.693613052368164
    },
    {
      "epoch": 0.17084010840108402,
      "step": 788,
      "training_loss": 8.08535099029541
    },
    {
      "epoch": 0.17084010840108402,
      "step": 788,
      "training_loss": 7.0054850578308105
    },
    {
      "epoch": 0.17105691056910569,
      "step": 789,
      "training_loss": 6.573246479034424
    },
    {
      "epoch": 0.17105691056910569,
      "step": 789,
      "training_loss": 7.443109512329102
    },
    {
      "epoch": 0.17105691056910569,
      "step": 789,
      "training_loss": 6.466366767883301
    },
    {
      "epoch": 0.17105691056910569,
      "step": 789,
      "training_loss": 7.204606056213379
    },
    {
      "epoch": 0.17127371273712738,
      "step": 790,
      "training_loss": 6.855348110198975
    },
    {
      "epoch": 0.17127371273712738,
      "step": 790,
      "training_loss": 4.441232681274414
    },
    {
      "epoch": 0.17127371273712738,
      "step": 790,
      "training_loss": 7.399599075317383
    },
    {
      "epoch": 0.17127371273712738,
      "step": 790,
      "training_loss": 8.294011116027832
    },
    {
      "epoch": 0.17149051490514905,
      "step": 791,
      "training_loss": 6.7782511711120605
    },
    {
      "epoch": 0.17149051490514905,
      "step": 791,
      "training_loss": 6.616808891296387
    },
    {
      "epoch": 0.17149051490514905,
      "step": 791,
      "training_loss": 8.379448890686035
    },
    {
      "epoch": 0.17149051490514905,
      "step": 791,
      "training_loss": 8.07263469696045
    },
    {
      "epoch": 0.17170731707317075,
      "grad_norm": 9.135390281677246,
      "learning_rate": 1e-05,
      "loss": 7.0978,
      "step": 792
    },
    {
      "epoch": 0.17170731707317075,
      "step": 792,
      "training_loss": 7.490533828735352
    },
    {
      "epoch": 0.17170731707317075,
      "step": 792,
      "training_loss": 7.667505741119385
    },
    {
      "epoch": 0.17170731707317075,
      "step": 792,
      "training_loss": 6.516371726989746
    },
    {
      "epoch": 0.17170731707317075,
      "step": 792,
      "training_loss": 6.871809005737305
    },
    {
      "epoch": 0.1719241192411924,
      "step": 793,
      "training_loss": 7.892791271209717
    },
    {
      "epoch": 0.1719241192411924,
      "step": 793,
      "training_loss": 7.648448467254639
    },
    {
      "epoch": 0.1719241192411924,
      "step": 793,
      "training_loss": 7.065356731414795
    },
    {
      "epoch": 0.1719241192411924,
      "step": 793,
      "training_loss": 5.8899312019348145
    },
    {
      "epoch": 0.17214092140921408,
      "step": 794,
      "training_loss": 6.102813243865967
    },
    {
      "epoch": 0.17214092140921408,
      "step": 794,
      "training_loss": 7.722466945648193
    },
    {
      "epoch": 0.17214092140921408,
      "step": 794,
      "training_loss": 7.7273454666137695
    },
    {
      "epoch": 0.17214092140921408,
      "step": 794,
      "training_loss": 6.9797468185424805
    },
    {
      "epoch": 0.17235772357723578,
      "step": 795,
      "training_loss": 7.617501258850098
    },
    {
      "epoch": 0.17235772357723578,
      "step": 795,
      "training_loss": 5.828641891479492
    },
    {
      "epoch": 0.17235772357723578,
      "step": 795,
      "training_loss": 7.586007595062256
    },
    {
      "epoch": 0.17235772357723578,
      "step": 795,
      "training_loss": 7.656508922576904
    },
    {
      "epoch": 0.17257452574525745,
      "grad_norm": 8.31425952911377,
      "learning_rate": 1e-05,
      "loss": 7.1415,
      "step": 796
    },
    {
      "epoch": 0.17257452574525745,
      "step": 796,
      "training_loss": 7.667788505554199
    },
    {
      "epoch": 0.17257452574525745,
      "step": 796,
      "training_loss": 7.039983749389648
    },
    {
      "epoch": 0.17257452574525745,
      "step": 796,
      "training_loss": 8.142295837402344
    },
    {
      "epoch": 0.17257452574525745,
      "step": 796,
      "training_loss": 7.74735689163208
    },
    {
      "epoch": 0.17279132791327914,
      "step": 797,
      "training_loss": 7.005985260009766
    },
    {
      "epoch": 0.17279132791327914,
      "step": 797,
      "training_loss": 5.8914337158203125
    },
    {
      "epoch": 0.17279132791327914,
      "step": 797,
      "training_loss": 6.9917707443237305
    },
    {
      "epoch": 0.17279132791327914,
      "step": 797,
      "training_loss": 7.970510482788086
    },
    {
      "epoch": 0.1730081300813008,
      "step": 798,
      "training_loss": 7.457269668579102
    },
    {
      "epoch": 0.1730081300813008,
      "step": 798,
      "training_loss": 6.707930088043213
    },
    {
      "epoch": 0.1730081300813008,
      "step": 798,
      "training_loss": 7.463604927062988
    },
    {
      "epoch": 0.1730081300813008,
      "step": 798,
      "training_loss": 7.081424236297607
    },
    {
      "epoch": 0.1732249322493225,
      "step": 799,
      "training_loss": 6.173180103302002
    },
    {
      "epoch": 0.1732249322493225,
      "step": 799,
      "training_loss": 8.315511703491211
    },
    {
      "epoch": 0.1732249322493225,
      "step": 799,
      "training_loss": 6.111395359039307
    },
    {
      "epoch": 0.1732249322493225,
      "step": 799,
      "training_loss": 7.675925254821777
    },
    {
      "epoch": 0.17344173441734417,
      "grad_norm": 13.885756492614746,
      "learning_rate": 1e-05,
      "loss": 7.2152,
      "step": 800
    },
    {
      "epoch": 0.17344173441734417,
      "step": 800,
      "training_loss": 7.944310665130615
    },
    {
      "epoch": 0.17344173441734417,
      "step": 800,
      "training_loss": 7.619216442108154
    },
    {
      "epoch": 0.17344173441734417,
      "step": 800,
      "training_loss": 6.1233744621276855
    },
    {
      "epoch": 0.17344173441734417,
      "step": 800,
      "training_loss": 7.6428070068359375
    },
    {
      "epoch": 0.17365853658536584,
      "step": 801,
      "training_loss": 7.94877815246582
    },
    {
      "epoch": 0.17365853658536584,
      "step": 801,
      "training_loss": 7.0549116134643555
    },
    {
      "epoch": 0.17365853658536584,
      "step": 801,
      "training_loss": 6.682488918304443
    },
    {
      "epoch": 0.17365853658536584,
      "step": 801,
      "training_loss": 6.743875026702881
    },
    {
      "epoch": 0.17387533875338754,
      "step": 802,
      "training_loss": 6.513547420501709
    },
    {
      "epoch": 0.17387533875338754,
      "step": 802,
      "training_loss": 7.396149158477783
    },
    {
      "epoch": 0.17387533875338754,
      "step": 802,
      "training_loss": 8.035909652709961
    },
    {
      "epoch": 0.17387533875338754,
      "step": 802,
      "training_loss": 6.492295265197754
    },
    {
      "epoch": 0.1740921409214092,
      "step": 803,
      "training_loss": 8.638956069946289
    },
    {
      "epoch": 0.1740921409214092,
      "step": 803,
      "training_loss": 7.584041595458984
    },
    {
      "epoch": 0.1740921409214092,
      "step": 803,
      "training_loss": 7.426419734954834
    },
    {
      "epoch": 0.1740921409214092,
      "step": 803,
      "training_loss": 7.097825527191162
    },
    {
      "epoch": 0.1743089430894309,
      "grad_norm": 7.657343864440918,
      "learning_rate": 1e-05,
      "loss": 7.3091,
      "step": 804
    },
    {
      "epoch": 0.1743089430894309,
      "step": 804,
      "training_loss": 6.421038627624512
    },
    {
      "epoch": 0.1743089430894309,
      "step": 804,
      "training_loss": 7.746757984161377
    },
    {
      "epoch": 0.1743089430894309,
      "step": 804,
      "training_loss": 7.267167091369629
    },
    {
      "epoch": 0.1743089430894309,
      "step": 804,
      "training_loss": 6.538160800933838
    },
    {
      "epoch": 0.17452574525745257,
      "step": 805,
      "training_loss": 6.926204681396484
    },
    {
      "epoch": 0.17452574525745257,
      "step": 805,
      "training_loss": 7.096925258636475
    },
    {
      "epoch": 0.17452574525745257,
      "step": 805,
      "training_loss": 7.609364986419678
    },
    {
      "epoch": 0.17452574525745257,
      "step": 805,
      "training_loss": 6.561868190765381
    },
    {
      "epoch": 0.17474254742547426,
      "step": 806,
      "training_loss": 7.904791355133057
    },
    {
      "epoch": 0.17474254742547426,
      "step": 806,
      "training_loss": 6.172348499298096
    },
    {
      "epoch": 0.17474254742547426,
      "step": 806,
      "training_loss": 6.510406970977783
    },
    {
      "epoch": 0.17474254742547426,
      "step": 806,
      "training_loss": 3.943934679031372
    },
    {
      "epoch": 0.17495934959349593,
      "step": 807,
      "training_loss": 7.78411865234375
    },
    {
      "epoch": 0.17495934959349593,
      "step": 807,
      "training_loss": 5.865633964538574
    },
    {
      "epoch": 0.17495934959349593,
      "step": 807,
      "training_loss": 7.366123676300049
    },
    {
      "epoch": 0.17495934959349593,
      "step": 807,
      "training_loss": 7.765300750732422
    },
    {
      "epoch": 0.17517615176151763,
      "grad_norm": 7.649136066436768,
      "learning_rate": 1e-05,
      "loss": 6.8425,
      "step": 808
    },
    {
      "epoch": 0.17517615176151763,
      "step": 808,
      "training_loss": 6.673496246337891
    },
    {
      "epoch": 0.17517615176151763,
      "step": 808,
      "training_loss": 6.912662982940674
    },
    {
      "epoch": 0.17517615176151763,
      "step": 808,
      "training_loss": 7.4880218505859375
    },
    {
      "epoch": 0.17517615176151763,
      "step": 808,
      "training_loss": 7.5267252922058105
    },
    {
      "epoch": 0.1753929539295393,
      "step": 809,
      "training_loss": 7.249334335327148
    },
    {
      "epoch": 0.1753929539295393,
      "step": 809,
      "training_loss": 7.383227825164795
    },
    {
      "epoch": 0.1753929539295393,
      "step": 809,
      "training_loss": 6.732342720031738
    },
    {
      "epoch": 0.1753929539295393,
      "step": 809,
      "training_loss": 6.841943264007568
    },
    {
      "epoch": 0.17560975609756097,
      "step": 810,
      "training_loss": 8.066043853759766
    },
    {
      "epoch": 0.17560975609756097,
      "step": 810,
      "training_loss": 7.152496337890625
    },
    {
      "epoch": 0.17560975609756097,
      "step": 810,
      "training_loss": 7.930498123168945
    },
    {
      "epoch": 0.17560975609756097,
      "step": 810,
      "training_loss": 7.643558979034424
    },
    {
      "epoch": 0.17582655826558266,
      "step": 811,
      "training_loss": 6.635989189147949
    },
    {
      "epoch": 0.17582655826558266,
      "step": 811,
      "training_loss": 6.804720878601074
    },
    {
      "epoch": 0.17582655826558266,
      "step": 811,
      "training_loss": 6.8450469970703125
    },
    {
      "epoch": 0.17582655826558266,
      "step": 811,
      "training_loss": 7.259118556976318
    },
    {
      "epoch": 0.17604336043360433,
      "grad_norm": 11.723189353942871,
      "learning_rate": 1e-05,
      "loss": 7.1966,
      "step": 812
    },
    {
      "epoch": 0.17604336043360433,
      "step": 812,
      "training_loss": 8.049393653869629
    },
    {
      "epoch": 0.17604336043360433,
      "step": 812,
      "training_loss": 7.0914154052734375
    },
    {
      "epoch": 0.17604336043360433,
      "step": 812,
      "training_loss": 6.122917175292969
    },
    {
      "epoch": 0.17604336043360433,
      "step": 812,
      "training_loss": 7.678223133087158
    },
    {
      "epoch": 0.17626016260162602,
      "step": 813,
      "training_loss": 8.251885414123535
    },
    {
      "epoch": 0.17626016260162602,
      "step": 813,
      "training_loss": 7.544054985046387
    },
    {
      "epoch": 0.17626016260162602,
      "step": 813,
      "training_loss": 5.424549579620361
    },
    {
      "epoch": 0.17626016260162602,
      "step": 813,
      "training_loss": 7.093045234680176
    },
    {
      "epoch": 0.1764769647696477,
      "step": 814,
      "training_loss": 6.6615800857543945
    },
    {
      "epoch": 0.1764769647696477,
      "step": 814,
      "training_loss": 6.85531759262085
    },
    {
      "epoch": 0.1764769647696477,
      "step": 814,
      "training_loss": 7.89941930770874
    },
    {
      "epoch": 0.1764769647696477,
      "step": 814,
      "training_loss": 8.123095512390137
    },
    {
      "epoch": 0.1766937669376694,
      "step": 815,
      "training_loss": 7.062263011932373
    },
    {
      "epoch": 0.1766937669376694,
      "step": 815,
      "training_loss": 5.183375358581543
    },
    {
      "epoch": 0.1766937669376694,
      "step": 815,
      "training_loss": 7.723530292510986
    },
    {
      "epoch": 0.1766937669376694,
      "step": 815,
      "training_loss": 7.928625106811523
    },
    {
      "epoch": 0.17691056910569106,
      "grad_norm": 10.438928604125977,
      "learning_rate": 1e-05,
      "loss": 7.1683,
      "step": 816
    },
    {
      "epoch": 0.17691056910569106,
      "step": 816,
      "training_loss": 6.329004287719727
    },
    {
      "epoch": 0.17691056910569106,
      "step": 816,
      "training_loss": 7.395474433898926
    },
    {
      "epoch": 0.17691056910569106,
      "step": 816,
      "training_loss": 7.169943809509277
    },
    {
      "epoch": 0.17691056910569106,
      "step": 816,
      "training_loss": 7.302660942077637
    },
    {
      "epoch": 0.17712737127371272,
      "step": 817,
      "training_loss": 4.311930179595947
    },
    {
      "epoch": 0.17712737127371272,
      "step": 817,
      "training_loss": 6.7956223487854
    },
    {
      "epoch": 0.17712737127371272,
      "step": 817,
      "training_loss": 8.561283111572266
    },
    {
      "epoch": 0.17712737127371272,
      "step": 817,
      "training_loss": 7.382592678070068
    },
    {
      "epoch": 0.17734417344173442,
      "step": 818,
      "training_loss": 7.079009056091309
    },
    {
      "epoch": 0.17734417344173442,
      "step": 818,
      "training_loss": 7.315610885620117
    },
    {
      "epoch": 0.17734417344173442,
      "step": 818,
      "training_loss": 6.858888626098633
    },
    {
      "epoch": 0.17734417344173442,
      "step": 818,
      "training_loss": 7.796277046203613
    },
    {
      "epoch": 0.1775609756097561,
      "step": 819,
      "training_loss": 7.829658508300781
    },
    {
      "epoch": 0.1775609756097561,
      "step": 819,
      "training_loss": 8.531761169433594
    },
    {
      "epoch": 0.1775609756097561,
      "step": 819,
      "training_loss": 6.132144451141357
    },
    {
      "epoch": 0.1775609756097561,
      "step": 819,
      "training_loss": 6.622104644775391
    },
    {
      "epoch": 0.17777777777777778,
      "grad_norm": 10.736778259277344,
      "learning_rate": 1e-05,
      "loss": 7.0884,
      "step": 820
    },
    {
      "epoch": 0.17777777777777778,
      "step": 820,
      "training_loss": 7.063544750213623
    },
    {
      "epoch": 0.17777777777777778,
      "step": 820,
      "training_loss": 6.15837287902832
    },
    {
      "epoch": 0.17777777777777778,
      "step": 820,
      "training_loss": 7.392588138580322
    },
    {
      "epoch": 0.17777777777777778,
      "step": 820,
      "training_loss": 7.8159050941467285
    },
    {
      "epoch": 0.17799457994579945,
      "step": 821,
      "training_loss": 7.323587417602539
    },
    {
      "epoch": 0.17799457994579945,
      "step": 821,
      "training_loss": 6.974554061889648
    },
    {
      "epoch": 0.17799457994579945,
      "step": 821,
      "training_loss": 9.954418182373047
    },
    {
      "epoch": 0.17799457994579945,
      "step": 821,
      "training_loss": 6.285775184631348
    },
    {
      "epoch": 0.17821138211382115,
      "step": 822,
      "training_loss": 6.7255473136901855
    },
    {
      "epoch": 0.17821138211382115,
      "step": 822,
      "training_loss": 5.315337657928467
    },
    {
      "epoch": 0.17821138211382115,
      "step": 822,
      "training_loss": 5.433378219604492
    },
    {
      "epoch": 0.17821138211382115,
      "step": 822,
      "training_loss": 9.515216827392578
    },
    {
      "epoch": 0.17842818428184282,
      "step": 823,
      "training_loss": 8.122336387634277
    },
    {
      "epoch": 0.17842818428184282,
      "step": 823,
      "training_loss": 7.4263410568237305
    },
    {
      "epoch": 0.17842818428184282,
      "step": 823,
      "training_loss": 6.806458950042725
    },
    {
      "epoch": 0.17842818428184282,
      "step": 823,
      "training_loss": 7.625648498535156
    },
    {
      "epoch": 0.1786449864498645,
      "grad_norm": 10.938361167907715,
      "learning_rate": 1e-05,
      "loss": 7.2462,
      "step": 824
    },
    {
      "epoch": 0.1786449864498645,
      "step": 824,
      "training_loss": 7.0844244956970215
    },
    {
      "epoch": 0.1786449864498645,
      "step": 824,
      "training_loss": 7.456233978271484
    },
    {
      "epoch": 0.1786449864498645,
      "step": 824,
      "training_loss": 6.636801242828369
    },
    {
      "epoch": 0.1786449864498645,
      "step": 824,
      "training_loss": 7.5240960121154785
    },
    {
      "epoch": 0.17886178861788618,
      "step": 825,
      "training_loss": 8.554115295410156
    },
    {
      "epoch": 0.17886178861788618,
      "step": 825,
      "training_loss": 7.113595485687256
    },
    {
      "epoch": 0.17886178861788618,
      "step": 825,
      "training_loss": 7.418328762054443
    },
    {
      "epoch": 0.17886178861788618,
      "step": 825,
      "training_loss": 7.812263488769531
    },
    {
      "epoch": 0.17907859078590785,
      "step": 826,
      "training_loss": 7.303267478942871
    },
    {
      "epoch": 0.17907859078590785,
      "step": 826,
      "training_loss": 6.643023490905762
    },
    {
      "epoch": 0.17907859078590785,
      "step": 826,
      "training_loss": 7.4047346115112305
    },
    {
      "epoch": 0.17907859078590785,
      "step": 826,
      "training_loss": 6.257386684417725
    },
    {
      "epoch": 0.17929539295392954,
      "step": 827,
      "training_loss": 6.618614196777344
    },
    {
      "epoch": 0.17929539295392954,
      "step": 827,
      "training_loss": 7.383209228515625
    },
    {
      "epoch": 0.17929539295392954,
      "step": 827,
      "training_loss": 7.28087043762207
    },
    {
      "epoch": 0.17929539295392954,
      "step": 827,
      "training_loss": 6.783229827880859
    },
    {
      "epoch": 0.1795121951219512,
      "grad_norm": 10.570295333862305,
      "learning_rate": 1e-05,
      "loss": 7.2046,
      "step": 828
    },
    {
      "epoch": 0.1795121951219512,
      "step": 828,
      "training_loss": 7.066954135894775
    },
    {
      "epoch": 0.1795121951219512,
      "step": 828,
      "training_loss": 6.017994403839111
    },
    {
      "epoch": 0.1795121951219512,
      "step": 828,
      "training_loss": 5.889477252960205
    },
    {
      "epoch": 0.1795121951219512,
      "step": 828,
      "training_loss": 7.422532558441162
    },
    {
      "epoch": 0.1797289972899729,
      "step": 829,
      "training_loss": 6.714961528778076
    },
    {
      "epoch": 0.1797289972899729,
      "step": 829,
      "training_loss": 6.765348434448242
    },
    {
      "epoch": 0.1797289972899729,
      "step": 829,
      "training_loss": 8.227232933044434
    },
    {
      "epoch": 0.1797289972899729,
      "step": 829,
      "training_loss": 6.908946990966797
    },
    {
      "epoch": 0.17994579945799458,
      "step": 830,
      "training_loss": 6.873594284057617
    },
    {
      "epoch": 0.17994579945799458,
      "step": 830,
      "training_loss": 6.139333248138428
    },
    {
      "epoch": 0.17994579945799458,
      "step": 830,
      "training_loss": 7.538086414337158
    },
    {
      "epoch": 0.17994579945799458,
      "step": 830,
      "training_loss": 7.394338607788086
    },
    {
      "epoch": 0.18016260162601627,
      "step": 831,
      "training_loss": 5.5286993980407715
    },
    {
      "epoch": 0.18016260162601627,
      "step": 831,
      "training_loss": 7.41973352432251
    },
    {
      "epoch": 0.18016260162601627,
      "step": 831,
      "training_loss": 7.282325267791748
    },
    {
      "epoch": 0.18016260162601627,
      "step": 831,
      "training_loss": 6.877228260040283
    },
    {
      "epoch": 0.18037940379403794,
      "grad_norm": 7.164116382598877,
      "learning_rate": 1e-05,
      "loss": 6.8792,
      "step": 832
    },
    {
      "epoch": 0.18037940379403794,
      "step": 832,
      "training_loss": 6.014946937561035
    },
    {
      "epoch": 0.18037940379403794,
      "step": 832,
      "training_loss": 6.261376857757568
    },
    {
      "epoch": 0.18037940379403794,
      "step": 832,
      "training_loss": 7.785135746002197
    },
    {
      "epoch": 0.18037940379403794,
      "step": 832,
      "training_loss": 6.2608208656311035
    },
    {
      "epoch": 0.1805962059620596,
      "step": 833,
      "training_loss": 7.561666965484619
    },
    {
      "epoch": 0.1805962059620596,
      "step": 833,
      "training_loss": 7.101681709289551
    },
    {
      "epoch": 0.1805962059620596,
      "step": 833,
      "training_loss": 7.488606929779053
    },
    {
      "epoch": 0.1805962059620596,
      "step": 833,
      "training_loss": 7.884815692901611
    },
    {
      "epoch": 0.1808130081300813,
      "step": 834,
      "training_loss": 7.263153076171875
    },
    {
      "epoch": 0.1808130081300813,
      "step": 834,
      "training_loss": 7.207045555114746
    },
    {
      "epoch": 0.1808130081300813,
      "step": 834,
      "training_loss": 6.977006912231445
    },
    {
      "epoch": 0.1808130081300813,
      "step": 834,
      "training_loss": 9.40546703338623
    },
    {
      "epoch": 0.18102981029810297,
      "step": 835,
      "training_loss": 7.275892734527588
    },
    {
      "epoch": 0.18102981029810297,
      "step": 835,
      "training_loss": 7.392833232879639
    },
    {
      "epoch": 0.18102981029810297,
      "step": 835,
      "training_loss": 7.277686595916748
    },
    {
      "epoch": 0.18102981029810297,
      "step": 835,
      "training_loss": 7.715944290161133
    },
    {
      "epoch": 0.18124661246612467,
      "grad_norm": 11.026599884033203,
      "learning_rate": 1e-05,
      "loss": 7.3046,
      "step": 836
    },
    {
      "epoch": 0.18124661246612467,
      "step": 836,
      "training_loss": 7.373568058013916
    },
    {
      "epoch": 0.18124661246612467,
      "step": 836,
      "training_loss": 7.282296180725098
    },
    {
      "epoch": 0.18124661246612467,
      "step": 836,
      "training_loss": 8.989021301269531
    },
    {
      "epoch": 0.18124661246612467,
      "step": 836,
      "training_loss": 8.298437118530273
    },
    {
      "epoch": 0.18146341463414634,
      "step": 837,
      "training_loss": 7.262816429138184
    },
    {
      "epoch": 0.18146341463414634,
      "step": 837,
      "training_loss": 7.362588882446289
    },
    {
      "epoch": 0.18146341463414634,
      "step": 837,
      "training_loss": 6.636457920074463
    },
    {
      "epoch": 0.18146341463414634,
      "step": 837,
      "training_loss": 7.431212902069092
    },
    {
      "epoch": 0.18168021680216803,
      "step": 838,
      "training_loss": 7.652865886688232
    },
    {
      "epoch": 0.18168021680216803,
      "step": 838,
      "training_loss": 5.918848991394043
    },
    {
      "epoch": 0.18168021680216803,
      "step": 838,
      "training_loss": 7.0359721183776855
    },
    {
      "epoch": 0.18168021680216803,
      "step": 838,
      "training_loss": 6.489068031311035
    },
    {
      "epoch": 0.1818970189701897,
      "step": 839,
      "training_loss": 7.97586727142334
    },
    {
      "epoch": 0.1818970189701897,
      "step": 839,
      "training_loss": 6.597050189971924
    },
    {
      "epoch": 0.1818970189701897,
      "step": 839,
      "training_loss": 7.466129779815674
    },
    {
      "epoch": 0.1818970189701897,
      "step": 839,
      "training_loss": 7.548438549041748
    },
    {
      "epoch": 0.1821138211382114,
      "grad_norm": 10.666437149047852,
      "learning_rate": 1e-05,
      "loss": 7.3325,
      "step": 840
    },
    {
      "epoch": 0.1821138211382114,
      "step": 840,
      "training_loss": 7.647886753082275
    },
    {
      "epoch": 0.1821138211382114,
      "step": 840,
      "training_loss": 7.946865081787109
    },
    {
      "epoch": 0.1821138211382114,
      "step": 840,
      "training_loss": 7.2192254066467285
    },
    {
      "epoch": 0.1821138211382114,
      "step": 840,
      "training_loss": 7.136842727661133
    },
    {
      "epoch": 0.18233062330623306,
      "step": 841,
      "training_loss": 7.086144924163818
    },
    {
      "epoch": 0.18233062330623306,
      "step": 841,
      "training_loss": 5.14063835144043
    },
    {
      "epoch": 0.18233062330623306,
      "step": 841,
      "training_loss": 7.505218982696533
    },
    {
      "epoch": 0.18233062330623306,
      "step": 841,
      "training_loss": 7.121418476104736
    },
    {
      "epoch": 0.18254742547425473,
      "step": 842,
      "training_loss": 8.363995552062988
    },
    {
      "epoch": 0.18254742547425473,
      "step": 842,
      "training_loss": 6.321098327636719
    },
    {
      "epoch": 0.18254742547425473,
      "step": 842,
      "training_loss": 7.077032566070557
    },
    {
      "epoch": 0.18254742547425473,
      "step": 842,
      "training_loss": 6.553358554840088
    },
    {
      "epoch": 0.18276422764227643,
      "step": 843,
      "training_loss": 7.954373359680176
    },
    {
      "epoch": 0.18276422764227643,
      "step": 843,
      "training_loss": 7.888192176818848
    },
    {
      "epoch": 0.18276422764227643,
      "step": 843,
      "training_loss": 6.50044584274292
    },
    {
      "epoch": 0.18276422764227643,
      "step": 843,
      "training_loss": 6.498714447021484
    },
    {
      "epoch": 0.1829810298102981,
      "grad_norm": 10.264307975769043,
      "learning_rate": 1e-05,
      "loss": 7.1226,
      "step": 844
    },
    {
      "epoch": 0.1829810298102981,
      "step": 844,
      "training_loss": 7.122339248657227
    },
    {
      "epoch": 0.1829810298102981,
      "step": 844,
      "training_loss": 10.349291801452637
    },
    {
      "epoch": 0.1829810298102981,
      "step": 844,
      "training_loss": 6.962682247161865
    },
    {
      "epoch": 0.1829810298102981,
      "step": 844,
      "training_loss": 6.65241003036499
    },
    {
      "epoch": 0.1831978319783198,
      "step": 845,
      "training_loss": 6.716928482055664
    },
    {
      "epoch": 0.1831978319783198,
      "step": 845,
      "training_loss": 6.524501323699951
    },
    {
      "epoch": 0.1831978319783198,
      "step": 845,
      "training_loss": 6.878113746643066
    },
    {
      "epoch": 0.1831978319783198,
      "step": 845,
      "training_loss": 6.864073753356934
    },
    {
      "epoch": 0.18341463414634146,
      "step": 846,
      "training_loss": 7.688050270080566
    },
    {
      "epoch": 0.18341463414634146,
      "step": 846,
      "training_loss": 6.648819923400879
    },
    {
      "epoch": 0.18341463414634146,
      "step": 846,
      "training_loss": 5.486545562744141
    },
    {
      "epoch": 0.18341463414634146,
      "step": 846,
      "training_loss": 6.570918083190918
    },
    {
      "epoch": 0.18363143631436316,
      "step": 847,
      "training_loss": 5.990664005279541
    },
    {
      "epoch": 0.18363143631436316,
      "step": 847,
      "training_loss": 8.49235725402832
    },
    {
      "epoch": 0.18363143631436316,
      "step": 847,
      "training_loss": 7.460155010223389
    },
    {
      "epoch": 0.18363143631436316,
      "step": 847,
      "training_loss": 6.888789176940918
    },
    {
      "epoch": 0.18384823848238482,
      "grad_norm": 12.738500595092773,
      "learning_rate": 1e-05,
      "loss": 7.081,
      "step": 848
    },
    {
      "epoch": 0.18384823848238482,
      "step": 848,
      "training_loss": 5.939150810241699
    },
    {
      "epoch": 0.18384823848238482,
      "step": 848,
      "training_loss": 7.700911045074463
    },
    {
      "epoch": 0.18384823848238482,
      "step": 848,
      "training_loss": 6.2215189933776855
    },
    {
      "epoch": 0.18384823848238482,
      "step": 848,
      "training_loss": 6.312709331512451
    },
    {
      "epoch": 0.1840650406504065,
      "step": 849,
      "training_loss": 6.869744300842285
    },
    {
      "epoch": 0.1840650406504065,
      "step": 849,
      "training_loss": 6.386552810668945
    },
    {
      "epoch": 0.1840650406504065,
      "step": 849,
      "training_loss": 7.309284210205078
    },
    {
      "epoch": 0.1840650406504065,
      "step": 849,
      "training_loss": 6.659444808959961
    },
    {
      "epoch": 0.1842818428184282,
      "step": 850,
      "training_loss": 7.308578968048096
    },
    {
      "epoch": 0.1842818428184282,
      "step": 850,
      "training_loss": 6.570850849151611
    },
    {
      "epoch": 0.1842818428184282,
      "step": 850,
      "training_loss": 7.458028316497803
    },
    {
      "epoch": 0.1842818428184282,
      "step": 850,
      "training_loss": 8.026230812072754
    },
    {
      "epoch": 0.18449864498644986,
      "step": 851,
      "training_loss": 7.743609428405762
    },
    {
      "epoch": 0.18449864498644986,
      "step": 851,
      "training_loss": 7.304820537567139
    },
    {
      "epoch": 0.18449864498644986,
      "step": 851,
      "training_loss": 6.992911338806152
    },
    {
      "epoch": 0.18449864498644986,
      "step": 851,
      "training_loss": 7.8466620445251465
    },
    {
      "epoch": 0.18471544715447155,
      "grad_norm": 7.237311363220215,
      "learning_rate": 1e-05,
      "loss": 7.0407,
      "step": 852
    },
    {
      "epoch": 0.18471544715447155,
      "step": 852,
      "training_loss": 6.739663600921631
    },
    {
      "epoch": 0.18471544715447155,
      "step": 852,
      "training_loss": 6.594604969024658
    },
    {
      "epoch": 0.18471544715447155,
      "step": 852,
      "training_loss": 7.4628400802612305
    },
    {
      "epoch": 0.18471544715447155,
      "step": 852,
      "training_loss": 6.074709892272949
    },
    {
      "epoch": 0.18493224932249322,
      "step": 853,
      "training_loss": 5.920360088348389
    },
    {
      "epoch": 0.18493224932249322,
      "step": 853,
      "training_loss": 7.624337196350098
    },
    {
      "epoch": 0.18493224932249322,
      "step": 853,
      "training_loss": 7.200811386108398
    },
    {
      "epoch": 0.18493224932249322,
      "step": 853,
      "training_loss": 5.6813764572143555
    },
    {
      "epoch": 0.18514905149051492,
      "step": 854,
      "training_loss": 6.261751174926758
    },
    {
      "epoch": 0.18514905149051492,
      "step": 854,
      "training_loss": 7.322147846221924
    },
    {
      "epoch": 0.18514905149051492,
      "step": 854,
      "training_loss": 6.742356300354004
    },
    {
      "epoch": 0.18514905149051492,
      "step": 854,
      "training_loss": 6.786734580993652
    },
    {
      "epoch": 0.18536585365853658,
      "step": 855,
      "training_loss": 6.4852986335754395
    },
    {
      "epoch": 0.18536585365853658,
      "step": 855,
      "training_loss": 7.360162734985352
    },
    {
      "epoch": 0.18536585365853658,
      "step": 855,
      "training_loss": 7.615264892578125
    },
    {
      "epoch": 0.18536585365853658,
      "step": 855,
      "training_loss": 6.292037010192871
    },
    {
      "epoch": 0.18558265582655828,
      "grad_norm": 10.1194486618042,
      "learning_rate": 1e-05,
      "loss": 6.7603,
      "step": 856
    },
    {
      "epoch": 0.18558265582655828,
      "step": 856,
      "training_loss": 6.3731489181518555
    },
    {
      "epoch": 0.18558265582655828,
      "step": 856,
      "training_loss": 7.642509937286377
    },
    {
      "epoch": 0.18558265582655828,
      "step": 856,
      "training_loss": 6.940797328948975
    },
    {
      "epoch": 0.18558265582655828,
      "step": 856,
      "training_loss": 6.354060649871826
    },
    {
      "epoch": 0.18579945799457995,
      "step": 857,
      "training_loss": 6.625466346740723
    },
    {
      "epoch": 0.18579945799457995,
      "step": 857,
      "training_loss": 7.004022598266602
    },
    {
      "epoch": 0.18579945799457995,
      "step": 857,
      "training_loss": 6.887704849243164
    },
    {
      "epoch": 0.18579945799457995,
      "step": 857,
      "training_loss": 6.381670951843262
    },
    {
      "epoch": 0.18601626016260162,
      "step": 858,
      "training_loss": 7.742708206176758
    },
    {
      "epoch": 0.18601626016260162,
      "step": 858,
      "training_loss": 5.806938171386719
    },
    {
      "epoch": 0.18601626016260162,
      "step": 858,
      "training_loss": 9.25085163116455
    },
    {
      "epoch": 0.18601626016260162,
      "step": 858,
      "training_loss": 8.006744384765625
    },
    {
      "epoch": 0.1862330623306233,
      "step": 859,
      "training_loss": 7.139254093170166
    },
    {
      "epoch": 0.1862330623306233,
      "step": 859,
      "training_loss": 7.4344868659973145
    },
    {
      "epoch": 0.1862330623306233,
      "step": 859,
      "training_loss": 6.517035484313965
    },
    {
      "epoch": 0.1862330623306233,
      "step": 859,
      "training_loss": 7.687093257904053
    },
    {
      "epoch": 0.18644986449864498,
      "grad_norm": 13.363895416259766,
      "learning_rate": 1e-05,
      "loss": 7.1122,
      "step": 860
    },
    {
      "epoch": 0.18644986449864498,
      "step": 860,
      "training_loss": 7.885530948638916
    },
    {
      "epoch": 0.18644986449864498,
      "step": 860,
      "training_loss": 5.295608997344971
    },
    {
      "epoch": 0.18644986449864498,
      "step": 860,
      "training_loss": 7.261105537414551
    },
    {
      "epoch": 0.18644986449864498,
      "step": 860,
      "training_loss": 7.543768882751465
    },
    {
      "epoch": 0.18666666666666668,
      "step": 861,
      "training_loss": 6.631839752197266
    },
    {
      "epoch": 0.18666666666666668,
      "step": 861,
      "training_loss": 8.036160469055176
    },
    {
      "epoch": 0.18666666666666668,
      "step": 861,
      "training_loss": 7.843410015106201
    },
    {
      "epoch": 0.18666666666666668,
      "step": 861,
      "training_loss": 7.620923042297363
    },
    {
      "epoch": 0.18688346883468834,
      "step": 862,
      "training_loss": 6.896880149841309
    },
    {
      "epoch": 0.18688346883468834,
      "step": 862,
      "training_loss": 7.265169620513916
    },
    {
      "epoch": 0.18688346883468834,
      "step": 862,
      "training_loss": 6.748443126678467
    },
    {
      "epoch": 0.18688346883468834,
      "step": 862,
      "training_loss": 7.148123741149902
    },
    {
      "epoch": 0.18710027100271004,
      "step": 863,
      "training_loss": 7.159070014953613
    },
    {
      "epoch": 0.18710027100271004,
      "step": 863,
      "training_loss": 7.454427242279053
    },
    {
      "epoch": 0.18710027100271004,
      "step": 863,
      "training_loss": 7.603905200958252
    },
    {
      "epoch": 0.18710027100271004,
      "step": 863,
      "training_loss": 6.6103315353393555
    },
    {
      "epoch": 0.1873170731707317,
      "grad_norm": 14.529662132263184,
      "learning_rate": 1e-05,
      "loss": 7.1878,
      "step": 864
    },
    {
      "epoch": 0.1873170731707317,
      "step": 864,
      "training_loss": 6.908429145812988
    },
    {
      "epoch": 0.1873170731707317,
      "step": 864,
      "training_loss": 4.97613000869751
    },
    {
      "epoch": 0.1873170731707317,
      "step": 864,
      "training_loss": 8.182188034057617
    },
    {
      "epoch": 0.1873170731707317,
      "step": 864,
      "training_loss": 6.001644134521484
    },
    {
      "epoch": 0.18753387533875338,
      "step": 865,
      "training_loss": 7.8494768142700195
    },
    {
      "epoch": 0.18753387533875338,
      "step": 865,
      "training_loss": 7.309213161468506
    },
    {
      "epoch": 0.18753387533875338,
      "step": 865,
      "training_loss": 6.899145126342773
    },
    {
      "epoch": 0.18753387533875338,
      "step": 865,
      "training_loss": 7.575263500213623
    },
    {
      "epoch": 0.18775067750677507,
      "step": 866,
      "training_loss": 5.268899440765381
    },
    {
      "epoch": 0.18775067750677507,
      "step": 866,
      "training_loss": 7.0977020263671875
    },
    {
      "epoch": 0.18775067750677507,
      "step": 866,
      "training_loss": 8.327857971191406
    },
    {
      "epoch": 0.18775067750677507,
      "step": 866,
      "training_loss": 7.346909046173096
    },
    {
      "epoch": 0.18796747967479674,
      "step": 867,
      "training_loss": 7.880013942718506
    },
    {
      "epoch": 0.18796747967479674,
      "step": 867,
      "training_loss": 6.058511257171631
    },
    {
      "epoch": 0.18796747967479674,
      "step": 867,
      "training_loss": 7.517721652984619
    },
    {
      "epoch": 0.18796747967479674,
      "step": 867,
      "training_loss": 7.46694278717041
    },
    {
      "epoch": 0.18818428184281843,
      "grad_norm": 7.738489627838135,
      "learning_rate": 1e-05,
      "loss": 7.0416,
      "step": 868
    },
    {
      "epoch": 0.18818428184281843,
      "step": 868,
      "training_loss": 7.537184238433838
    },
    {
      "epoch": 0.18818428184281843,
      "step": 868,
      "training_loss": 6.965476989746094
    },
    {
      "epoch": 0.18818428184281843,
      "step": 868,
      "training_loss": 6.977582931518555
    },
    {
      "epoch": 0.18818428184281843,
      "step": 868,
      "training_loss": 6.999542713165283
    },
    {
      "epoch": 0.1884010840108401,
      "step": 869,
      "training_loss": 7.124688148498535
    },
    {
      "epoch": 0.1884010840108401,
      "step": 869,
      "training_loss": 7.587009906768799
    },
    {
      "epoch": 0.1884010840108401,
      "step": 869,
      "training_loss": 7.983232021331787
    },
    {
      "epoch": 0.1884010840108401,
      "step": 869,
      "training_loss": 7.21464729309082
    },
    {
      "epoch": 0.1886178861788618,
      "step": 870,
      "training_loss": 7.629908561706543
    },
    {
      "epoch": 0.1886178861788618,
      "step": 870,
      "training_loss": 6.601301193237305
    },
    {
      "epoch": 0.1886178861788618,
      "step": 870,
      "training_loss": 5.424865245819092
    },
    {
      "epoch": 0.1886178861788618,
      "step": 870,
      "training_loss": 7.169931411743164
    },
    {
      "epoch": 0.18883468834688347,
      "step": 871,
      "training_loss": 7.0615057945251465
    },
    {
      "epoch": 0.18883468834688347,
      "step": 871,
      "training_loss": 7.941755771636963
    },
    {
      "epoch": 0.18883468834688347,
      "step": 871,
      "training_loss": 7.539806842803955
    },
    {
      "epoch": 0.18883468834688347,
      "step": 871,
      "training_loss": 7.41072940826416
    },
    {
      "epoch": 0.18905149051490516,
      "grad_norm": 12.689265251159668,
      "learning_rate": 1e-05,
      "loss": 7.1981,
      "step": 872
    },
    {
      "epoch": 0.18905149051490516,
      "step": 872,
      "training_loss": 5.062407970428467
    },
    {
      "epoch": 0.18905149051490516,
      "step": 872,
      "training_loss": 5.46240758895874
    },
    {
      "epoch": 0.18905149051490516,
      "step": 872,
      "training_loss": 7.425811767578125
    },
    {
      "epoch": 0.18905149051490516,
      "step": 872,
      "training_loss": 6.492099761962891
    },
    {
      "epoch": 0.18926829268292683,
      "step": 873,
      "training_loss": 7.750023365020752
    },
    {
      "epoch": 0.18926829268292683,
      "step": 873,
      "training_loss": 6.900044918060303
    },
    {
      "epoch": 0.18926829268292683,
      "step": 873,
      "training_loss": 6.329041004180908
    },
    {
      "epoch": 0.18926829268292683,
      "step": 873,
      "training_loss": 7.1648101806640625
    },
    {
      "epoch": 0.1894850948509485,
      "step": 874,
      "training_loss": 8.222869873046875
    },
    {
      "epoch": 0.1894850948509485,
      "step": 874,
      "training_loss": 7.404592990875244
    },
    {
      "epoch": 0.1894850948509485,
      "step": 874,
      "training_loss": 6.908819675445557
    },
    {
      "epoch": 0.1894850948509485,
      "step": 874,
      "training_loss": 6.736179351806641
    },
    {
      "epoch": 0.1897018970189702,
      "step": 875,
      "training_loss": 7.546988487243652
    },
    {
      "epoch": 0.1897018970189702,
      "step": 875,
      "training_loss": 6.58929967880249
    },
    {
      "epoch": 0.1897018970189702,
      "step": 875,
      "training_loss": 6.964486598968506
    },
    {
      "epoch": 0.1897018970189702,
      "step": 875,
      "training_loss": 8.306330680847168
    },
    {
      "epoch": 0.18991869918699186,
      "grad_norm": 10.150222778320312,
      "learning_rate": 1e-05,
      "loss": 6.9541,
      "step": 876
    },
    {
      "epoch": 0.18991869918699186,
      "step": 876,
      "training_loss": 4.916464805603027
    },
    {
      "epoch": 0.18991869918699186,
      "step": 876,
      "training_loss": 7.70362663269043
    },
    {
      "epoch": 0.18991869918699186,
      "step": 876,
      "training_loss": 7.413809776306152
    },
    {
      "epoch": 0.18991869918699186,
      "step": 876,
      "training_loss": 6.724100112915039
    },
    {
      "epoch": 0.19013550135501356,
      "step": 877,
      "training_loss": 6.129174709320068
    },
    {
      "epoch": 0.19013550135501356,
      "step": 877,
      "training_loss": 6.72393798828125
    },
    {
      "epoch": 0.19013550135501356,
      "step": 877,
      "training_loss": 6.359467029571533
    },
    {
      "epoch": 0.19013550135501356,
      "step": 877,
      "training_loss": 7.650168418884277
    },
    {
      "epoch": 0.19035230352303523,
      "step": 878,
      "training_loss": 7.47109842300415
    },
    {
      "epoch": 0.19035230352303523,
      "step": 878,
      "training_loss": 5.767757892608643
    },
    {
      "epoch": 0.19035230352303523,
      "step": 878,
      "training_loss": 6.9738969802856445
    },
    {
      "epoch": 0.19035230352303523,
      "step": 878,
      "training_loss": 7.568243026733398
    },
    {
      "epoch": 0.19056910569105692,
      "step": 879,
      "training_loss": 7.924320697784424
    },
    {
      "epoch": 0.19056910569105692,
      "step": 879,
      "training_loss": 8.216288566589355
    },
    {
      "epoch": 0.19056910569105692,
      "step": 879,
      "training_loss": 7.06545877456665
    },
    {
      "epoch": 0.19056910569105692,
      "step": 879,
      "training_loss": 7.254753112792969
    },
    {
      "epoch": 0.1907859078590786,
      "grad_norm": 10.832039833068848,
      "learning_rate": 1e-05,
      "loss": 6.9914,
      "step": 880
    },
    {
      "epoch": 0.1907859078590786,
      "step": 880,
      "training_loss": 6.783562183380127
    },
    {
      "epoch": 0.1907859078590786,
      "step": 880,
      "training_loss": 7.076141834259033
    },
    {
      "epoch": 0.1907859078590786,
      "step": 880,
      "training_loss": 7.6703200340271
    },
    {
      "epoch": 0.1907859078590786,
      "step": 880,
      "training_loss": 7.284185409545898
    },
    {
      "epoch": 0.19100271002710026,
      "step": 881,
      "training_loss": 7.897521495819092
    },
    {
      "epoch": 0.19100271002710026,
      "step": 881,
      "training_loss": 8.662328720092773
    },
    {
      "epoch": 0.19100271002710026,
      "step": 881,
      "training_loss": 7.0982346534729
    },
    {
      "epoch": 0.19100271002710026,
      "step": 881,
      "training_loss": 7.200083255767822
    },
    {
      "epoch": 0.19121951219512195,
      "step": 882,
      "training_loss": 7.154634475708008
    },
    {
      "epoch": 0.19121951219512195,
      "step": 882,
      "training_loss": 5.406583786010742
    },
    {
      "epoch": 0.19121951219512195,
      "step": 882,
      "training_loss": 6.681096076965332
    },
    {
      "epoch": 0.19121951219512195,
      "step": 882,
      "training_loss": 6.295332431793213
    },
    {
      "epoch": 0.19143631436314362,
      "step": 883,
      "training_loss": 6.527408599853516
    },
    {
      "epoch": 0.19143631436314362,
      "step": 883,
      "training_loss": 6.087545394897461
    },
    {
      "epoch": 0.19143631436314362,
      "step": 883,
      "training_loss": 6.703707695007324
    },
    {
      "epoch": 0.19143631436314362,
      "step": 883,
      "training_loss": 6.524468898773193
    },
    {
      "epoch": 0.19165311653116532,
      "grad_norm": 10.1476469039917,
      "learning_rate": 1e-05,
      "loss": 6.9408,
      "step": 884
    },
    {
      "epoch": 0.19165311653116532,
      "step": 884,
      "training_loss": 7.715442657470703
    },
    {
      "epoch": 0.19165311653116532,
      "step": 884,
      "training_loss": 7.3051934242248535
    },
    {
      "epoch": 0.19165311653116532,
      "step": 884,
      "training_loss": 6.390124320983887
    },
    {
      "epoch": 0.19165311653116532,
      "step": 884,
      "training_loss": 7.0039825439453125
    },
    {
      "epoch": 0.191869918699187,
      "step": 885,
      "training_loss": 6.599710941314697
    },
    {
      "epoch": 0.191869918699187,
      "step": 885,
      "training_loss": 7.457118034362793
    },
    {
      "epoch": 0.191869918699187,
      "step": 885,
      "training_loss": 7.729769229888916
    },
    {
      "epoch": 0.191869918699187,
      "step": 885,
      "training_loss": 5.300081253051758
    },
    {
      "epoch": 0.19208672086720868,
      "step": 886,
      "training_loss": 7.241535186767578
    },
    {
      "epoch": 0.19208672086720868,
      "step": 886,
      "training_loss": 6.716594696044922
    },
    {
      "epoch": 0.19208672086720868,
      "step": 886,
      "training_loss": 10.763550758361816
    },
    {
      "epoch": 0.19208672086720868,
      "step": 886,
      "training_loss": 7.097225189208984
    },
    {
      "epoch": 0.19230352303523035,
      "step": 887,
      "training_loss": 6.860349178314209
    },
    {
      "epoch": 0.19230352303523035,
      "step": 887,
      "training_loss": 5.056646347045898
    },
    {
      "epoch": 0.19230352303523035,
      "step": 887,
      "training_loss": 6.769491195678711
    },
    {
      "epoch": 0.19230352303523035,
      "step": 887,
      "training_loss": 7.8604888916015625
    },
    {
      "epoch": 0.19252032520325205,
      "grad_norm": 10.353286743164062,
      "learning_rate": 1e-05,
      "loss": 7.1167,
      "step": 888
    },
    {
      "epoch": 0.19252032520325205,
      "step": 888,
      "training_loss": 4.667062282562256
    },
    {
      "epoch": 0.19252032520325205,
      "step": 888,
      "training_loss": 6.999044418334961
    },
    {
      "epoch": 0.19252032520325205,
      "step": 888,
      "training_loss": 6.802943229675293
    },
    {
      "epoch": 0.19252032520325205,
      "step": 888,
      "training_loss": 5.5111613273620605
    },
    {
      "epoch": 0.19273712737127371,
      "step": 889,
      "training_loss": 7.988242149353027
    },
    {
      "epoch": 0.19273712737127371,
      "step": 889,
      "training_loss": 6.311324596405029
    },
    {
      "epoch": 0.19273712737127371,
      "step": 889,
      "training_loss": 6.752025127410889
    },
    {
      "epoch": 0.19273712737127371,
      "step": 889,
      "training_loss": 8.992109298706055
    },
    {
      "epoch": 0.19295392953929538,
      "step": 890,
      "training_loss": 6.959993839263916
    },
    {
      "epoch": 0.19295392953929538,
      "step": 890,
      "training_loss": 7.000390529632568
    },
    {
      "epoch": 0.19295392953929538,
      "step": 890,
      "training_loss": 6.956244945526123
    },
    {
      "epoch": 0.19295392953929538,
      "step": 890,
      "training_loss": 6.791262149810791
    },
    {
      "epoch": 0.19317073170731708,
      "step": 891,
      "training_loss": 6.730249881744385
    },
    {
      "epoch": 0.19317073170731708,
      "step": 891,
      "training_loss": 8.142066955566406
    },
    {
      "epoch": 0.19317073170731708,
      "step": 891,
      "training_loss": 6.706309795379639
    },
    {
      "epoch": 0.19317073170731708,
      "step": 891,
      "training_loss": 7.471441268920898
    },
    {
      "epoch": 0.19338753387533875,
      "grad_norm": 8.586185455322266,
      "learning_rate": 1e-05,
      "loss": 6.9239,
      "step": 892
    },
    {
      "epoch": 0.19338753387533875,
      "step": 892,
      "training_loss": 7.10991096496582
    },
    {
      "epoch": 0.19338753387533875,
      "step": 892,
      "training_loss": 9.815572738647461
    },
    {
      "epoch": 0.19338753387533875,
      "step": 892,
      "training_loss": 6.91194486618042
    },
    {
      "epoch": 0.19338753387533875,
      "step": 892,
      "training_loss": 5.406633377075195
    },
    {
      "epoch": 0.19360433604336044,
      "step": 893,
      "training_loss": 7.5275492668151855
    },
    {
      "epoch": 0.19360433604336044,
      "step": 893,
      "training_loss": 6.079318046569824
    },
    {
      "epoch": 0.19360433604336044,
      "step": 893,
      "training_loss": 7.262707710266113
    },
    {
      "epoch": 0.19360433604336044,
      "step": 893,
      "training_loss": 7.2677483558654785
    },
    {
      "epoch": 0.1938211382113821,
      "step": 894,
      "training_loss": 8.208398818969727
    },
    {
      "epoch": 0.1938211382113821,
      "step": 894,
      "training_loss": 6.665585994720459
    },
    {
      "epoch": 0.1938211382113821,
      "step": 894,
      "training_loss": 5.156864166259766
    },
    {
      "epoch": 0.1938211382113821,
      "step": 894,
      "training_loss": 6.864404201507568
    },
    {
      "epoch": 0.1940379403794038,
      "step": 895,
      "training_loss": 7.2122602462768555
    },
    {
      "epoch": 0.1940379403794038,
      "step": 895,
      "training_loss": 7.06465482711792
    },
    {
      "epoch": 0.1940379403794038,
      "step": 895,
      "training_loss": 7.250428676605225
    },
    {
      "epoch": 0.1940379403794038,
      "step": 895,
      "training_loss": 7.6371612548828125
    },
    {
      "epoch": 0.19425474254742547,
      "grad_norm": 7.234125137329102,
      "learning_rate": 1e-05,
      "loss": 7.0901,
      "step": 896
    },
    {
      "epoch": 0.19425474254742547,
      "step": 896,
      "training_loss": 7.440708637237549
    },
    {
      "epoch": 0.19425474254742547,
      "step": 896,
      "training_loss": 7.934587001800537
    },
    {
      "epoch": 0.19425474254742547,
      "step": 896,
      "training_loss": 7.739702224731445
    },
    {
      "epoch": 0.19425474254742547,
      "step": 896,
      "training_loss": 7.604779243469238
    },
    {
      "epoch": 0.19447154471544714,
      "step": 897,
      "training_loss": 6.641813278198242
    },
    {
      "epoch": 0.19447154471544714,
      "step": 897,
      "training_loss": 6.754274845123291
    },
    {
      "epoch": 0.19447154471544714,
      "step": 897,
      "training_loss": 7.286022186279297
    },
    {
      "epoch": 0.19447154471544714,
      "step": 897,
      "training_loss": 8.066521644592285
    },
    {
      "epoch": 0.19468834688346884,
      "step": 898,
      "training_loss": 7.359044075012207
    },
    {
      "epoch": 0.19468834688346884,
      "step": 898,
      "training_loss": 8.003812789916992
    },
    {
      "epoch": 0.19468834688346884,
      "step": 898,
      "training_loss": 5.381690502166748
    },
    {
      "epoch": 0.19468834688346884,
      "step": 898,
      "training_loss": 7.545904159545898
    },
    {
      "epoch": 0.1949051490514905,
      "step": 899,
      "training_loss": 6.94319486618042
    },
    {
      "epoch": 0.1949051490514905,
      "step": 899,
      "training_loss": 6.592953205108643
    },
    {
      "epoch": 0.1949051490514905,
      "step": 899,
      "training_loss": 7.475757122039795
    },
    {
      "epoch": 0.1949051490514905,
      "step": 899,
      "training_loss": 5.290265083312988
    },
    {
      "epoch": 0.1951219512195122,
      "grad_norm": 12.212139129638672,
      "learning_rate": 1e-05,
      "loss": 7.1288,
      "step": 900
    },
    {
      "epoch": 0.1951219512195122,
      "step": 900,
      "training_loss": 6.401431083679199
    },
    {
      "epoch": 0.1951219512195122,
      "step": 900,
      "training_loss": 7.175251007080078
    },
    {
      "epoch": 0.1951219512195122,
      "step": 900,
      "training_loss": 6.627004623413086
    },
    {
      "epoch": 0.1951219512195122,
      "step": 900,
      "training_loss": 6.592196464538574
    },
    {
      "epoch": 0.19533875338753387,
      "step": 901,
      "training_loss": 8.014532089233398
    },
    {
      "epoch": 0.19533875338753387,
      "step": 901,
      "training_loss": 8.453191757202148
    },
    {
      "epoch": 0.19533875338753387,
      "step": 901,
      "training_loss": 6.473550319671631
    },
    {
      "epoch": 0.19533875338753387,
      "step": 901,
      "training_loss": 5.163286209106445
    },
    {
      "epoch": 0.19555555555555557,
      "step": 902,
      "training_loss": 6.889769077301025
    },
    {
      "epoch": 0.19555555555555557,
      "step": 902,
      "training_loss": 7.036489009857178
    },
    {
      "epoch": 0.19555555555555557,
      "step": 902,
      "training_loss": 6.779224395751953
    },
    {
      "epoch": 0.19555555555555557,
      "step": 902,
      "training_loss": 6.895600318908691
    },
    {
      "epoch": 0.19577235772357723,
      "step": 903,
      "training_loss": 6.134620189666748
    },
    {
      "epoch": 0.19577235772357723,
      "step": 903,
      "training_loss": 8.013327598571777
    },
    {
      "epoch": 0.19577235772357723,
      "step": 903,
      "training_loss": 6.4609761238098145
    },
    {
      "epoch": 0.19577235772357723,
      "step": 903,
      "training_loss": 7.004875659942627
    },
    {
      "epoch": 0.19598915989159893,
      "grad_norm": 8.652647972106934,
      "learning_rate": 1e-05,
      "loss": 6.8822,
      "step": 904
    },
    {
      "epoch": 0.19598915989159893,
      "step": 904,
      "training_loss": 5.638509750366211
    },
    {
      "epoch": 0.19598915989159893,
      "step": 904,
      "training_loss": 7.867103576660156
    },
    {
      "epoch": 0.19598915989159893,
      "step": 904,
      "training_loss": 7.349686145782471
    },
    {
      "epoch": 0.19598915989159893,
      "step": 904,
      "training_loss": 7.487237453460693
    },
    {
      "epoch": 0.1962059620596206,
      "step": 905,
      "training_loss": 6.321836948394775
    },
    {
      "epoch": 0.1962059620596206,
      "step": 905,
      "training_loss": 6.190227508544922
    },
    {
      "epoch": 0.1962059620596206,
      "step": 905,
      "training_loss": 7.496026992797852
    },
    {
      "epoch": 0.1962059620596206,
      "step": 905,
      "training_loss": 7.660998344421387
    },
    {
      "epoch": 0.19642276422764227,
      "step": 906,
      "training_loss": 8.9119291305542
    },
    {
      "epoch": 0.19642276422764227,
      "step": 906,
      "training_loss": 8.554183006286621
    },
    {
      "epoch": 0.19642276422764227,
      "step": 906,
      "training_loss": 6.828163146972656
    },
    {
      "epoch": 0.19642276422764227,
      "step": 906,
      "training_loss": 8.23051929473877
    },
    {
      "epoch": 0.19663956639566396,
      "step": 907,
      "training_loss": 6.412253379821777
    },
    {
      "epoch": 0.19663956639566396,
      "step": 907,
      "training_loss": 8.449088096618652
    },
    {
      "epoch": 0.19663956639566396,
      "step": 907,
      "training_loss": 7.166393280029297
    },
    {
      "epoch": 0.19663956639566396,
      "step": 907,
      "training_loss": 5.820252895355225
    },
    {
      "epoch": 0.19685636856368563,
      "grad_norm": 15.45479965209961,
      "learning_rate": 1e-05,
      "loss": 7.274,
      "step": 908
    },
    {
      "epoch": 0.19685636856368563,
      "step": 908,
      "training_loss": 8.545425415039062
    },
    {
      "epoch": 0.19685636856368563,
      "step": 908,
      "training_loss": 6.9689106941223145
    },
    {
      "epoch": 0.19685636856368563,
      "step": 908,
      "training_loss": 7.681533336639404
    },
    {
      "epoch": 0.19685636856368563,
      "step": 908,
      "training_loss": 8.247234344482422
    },
    {
      "epoch": 0.19707317073170733,
      "step": 909,
      "training_loss": 5.67457389831543
    },
    {
      "epoch": 0.19707317073170733,
      "step": 909,
      "training_loss": 5.293242454528809
    },
    {
      "epoch": 0.19707317073170733,
      "step": 909,
      "training_loss": 7.21744966506958
    },
    {
      "epoch": 0.19707317073170733,
      "step": 909,
      "training_loss": 6.8160552978515625
    },
    {
      "epoch": 0.197289972899729,
      "step": 910,
      "training_loss": 8.147981643676758
    },
    {
      "epoch": 0.197289972899729,
      "step": 910,
      "training_loss": 5.1454997062683105
    },
    {
      "epoch": 0.197289972899729,
      "step": 910,
      "training_loss": 6.737396240234375
    },
    {
      "epoch": 0.197289972899729,
      "step": 910,
      "training_loss": 6.068857192993164
    },
    {
      "epoch": 0.1975067750677507,
      "step": 911,
      "training_loss": 6.522346496582031
    },
    {
      "epoch": 0.1975067750677507,
      "step": 911,
      "training_loss": 7.239411354064941
    },
    {
      "epoch": 0.1975067750677507,
      "step": 911,
      "training_loss": 6.320777893066406
    },
    {
      "epoch": 0.1975067750677507,
      "step": 911,
      "training_loss": 7.624478816986084
    },
    {
      "epoch": 0.19772357723577236,
      "grad_norm": 10.170125007629395,
      "learning_rate": 1e-05,
      "loss": 6.8907,
      "step": 912
    },
    {
      "epoch": 0.19772357723577236,
      "step": 912,
      "training_loss": 8.306585311889648
    },
    {
      "epoch": 0.19772357723577236,
      "step": 912,
      "training_loss": 7.89127254486084
    },
    {
      "epoch": 0.19772357723577236,
      "step": 912,
      "training_loss": 6.6195783615112305
    },
    {
      "epoch": 0.19772357723577236,
      "step": 912,
      "training_loss": 7.293664455413818
    },
    {
      "epoch": 0.19794037940379403,
      "step": 913,
      "training_loss": 6.025571823120117
    },
    {
      "epoch": 0.19794037940379403,
      "step": 913,
      "training_loss": 7.590465068817139
    },
    {
      "epoch": 0.19794037940379403,
      "step": 913,
      "training_loss": 6.431428909301758
    },
    {
      "epoch": 0.19794037940379403,
      "step": 913,
      "training_loss": 7.9945173263549805
    },
    {
      "epoch": 0.19815718157181572,
      "step": 914,
      "training_loss": 7.4139084815979
    },
    {
      "epoch": 0.19815718157181572,
      "step": 914,
      "training_loss": 6.611980438232422
    },
    {
      "epoch": 0.19815718157181572,
      "step": 914,
      "training_loss": 6.585293769836426
    },
    {
      "epoch": 0.19815718157181572,
      "step": 914,
      "training_loss": 7.0576372146606445
    },
    {
      "epoch": 0.1983739837398374,
      "step": 915,
      "training_loss": 7.9786152839660645
    },
    {
      "epoch": 0.1983739837398374,
      "step": 915,
      "training_loss": 8.03677749633789
    },
    {
      "epoch": 0.1983739837398374,
      "step": 915,
      "training_loss": 7.17863130569458
    },
    {
      "epoch": 0.1983739837398374,
      "step": 915,
      "training_loss": 6.563412666320801
    },
    {
      "epoch": 0.19859078590785909,
      "grad_norm": 9.507015228271484,
      "learning_rate": 1e-05,
      "loss": 7.2237,
      "step": 916
    },
    {
      "epoch": 0.19859078590785909,
      "step": 916,
      "training_loss": 7.506018161773682
    },
    {
      "epoch": 0.19859078590785909,
      "step": 916,
      "training_loss": 6.961234092712402
    },
    {
      "epoch": 0.19859078590785909,
      "step": 916,
      "training_loss": 7.495973587036133
    },
    {
      "epoch": 0.19859078590785909,
      "step": 916,
      "training_loss": 10.008259773254395
    },
    {
      "epoch": 0.19880758807588075,
      "step": 917,
      "training_loss": 5.648875713348389
    },
    {
      "epoch": 0.19880758807588075,
      "step": 917,
      "training_loss": 7.365983963012695
    },
    {
      "epoch": 0.19880758807588075,
      "step": 917,
      "training_loss": 7.316587924957275
    },
    {
      "epoch": 0.19880758807588075,
      "step": 917,
      "training_loss": 10.8967924118042
    },
    {
      "epoch": 0.19902439024390245,
      "step": 918,
      "training_loss": 7.952255725860596
    },
    {
      "epoch": 0.19902439024390245,
      "step": 918,
      "training_loss": 7.4896135330200195
    },
    {
      "epoch": 0.19902439024390245,
      "step": 918,
      "training_loss": 6.710374355316162
    },
    {
      "epoch": 0.19902439024390245,
      "step": 918,
      "training_loss": 8.04035472869873
    },
    {
      "epoch": 0.19924119241192412,
      "step": 919,
      "training_loss": 7.045206546783447
    },
    {
      "epoch": 0.19924119241192412,
      "step": 919,
      "training_loss": 7.0320916175842285
    },
    {
      "epoch": 0.19924119241192412,
      "step": 919,
      "training_loss": 5.548027992248535
    },
    {
      "epoch": 0.19924119241192412,
      "step": 919,
      "training_loss": 5.064159870147705
    },
    {
      "epoch": 0.1994579945799458,
      "grad_norm": 10.154857635498047,
      "learning_rate": 1e-05,
      "loss": 7.3801,
      "step": 920
    },
    {
      "epoch": 0.1994579945799458,
      "step": 920,
      "training_loss": 8.215863227844238
    },
    {
      "epoch": 0.1994579945799458,
      "step": 920,
      "training_loss": 6.244930744171143
    },
    {
      "epoch": 0.1994579945799458,
      "step": 920,
      "training_loss": 7.521228790283203
    },
    {
      "epoch": 0.1994579945799458,
      "step": 920,
      "training_loss": 6.889376640319824
    },
    {
      "epoch": 0.19967479674796748,
      "step": 921,
      "training_loss": 6.395261287689209
    },
    {
      "epoch": 0.19967479674796748,
      "step": 921,
      "training_loss": 8.156518936157227
    },
    {
      "epoch": 0.19967479674796748,
      "step": 921,
      "training_loss": 6.503940582275391
    },
    {
      "epoch": 0.19967479674796748,
      "step": 921,
      "training_loss": 6.132629871368408
    },
    {
      "epoch": 0.19989159891598915,
      "step": 922,
      "training_loss": 6.810621738433838
    },
    {
      "epoch": 0.19989159891598915,
      "step": 922,
      "training_loss": 6.033893585205078
    },
    {
      "epoch": 0.19989159891598915,
      "step": 922,
      "training_loss": 6.136981964111328
    },
    {
      "epoch": 0.19989159891598915,
      "step": 922,
      "training_loss": 7.134891033172607
    },
    {
      "epoch": 0.20010840108401085,
      "step": 923,
      "training_loss": 5.800745964050293
    },
    {
      "epoch": 0.20010840108401085,
      "step": 923,
      "training_loss": 7.12330436706543
    },
    {
      "epoch": 0.20010840108401085,
      "step": 923,
      "training_loss": 5.94764518737793
    },
    {
      "epoch": 0.20010840108401085,
      "step": 923,
      "training_loss": 7.088926315307617
    },
    {
      "epoch": 0.2003252032520325,
      "grad_norm": 15.699043273925781,
      "learning_rate": 1e-05,
      "loss": 6.7585,
      "step": 924
    },
    {
      "epoch": 0.2003252032520325,
      "step": 924,
      "training_loss": 6.7463860511779785
    },
    {
      "epoch": 0.2003252032520325,
      "step": 924,
      "training_loss": 7.379523277282715
    },
    {
      "epoch": 0.2003252032520325,
      "step": 924,
      "training_loss": 7.026024341583252
    },
    {
      "epoch": 0.2003252032520325,
      "step": 924,
      "training_loss": 6.146527290344238
    },
    {
      "epoch": 0.2005420054200542,
      "step": 925,
      "training_loss": 6.631680488586426
    },
    {
      "epoch": 0.2005420054200542,
      "step": 925,
      "training_loss": 5.587727069854736
    },
    {
      "epoch": 0.2005420054200542,
      "step": 925,
      "training_loss": 7.874544143676758
    },
    {
      "epoch": 0.2005420054200542,
      "step": 925,
      "training_loss": 7.26963996887207
    },
    {
      "epoch": 0.20075880758807588,
      "step": 926,
      "training_loss": 6.96865177154541
    },
    {
      "epoch": 0.20075880758807588,
      "step": 926,
      "training_loss": 7.387615203857422
    },
    {
      "epoch": 0.20075880758807588,
      "step": 926,
      "training_loss": 7.577230930328369
    },
    {
      "epoch": 0.20075880758807588,
      "step": 926,
      "training_loss": 7.907308578491211
    },
    {
      "epoch": 0.20097560975609757,
      "step": 927,
      "training_loss": 7.070199012756348
    },
    {
      "epoch": 0.20097560975609757,
      "step": 927,
      "training_loss": 5.672652721405029
    },
    {
      "epoch": 0.20097560975609757,
      "step": 927,
      "training_loss": 5.533665657043457
    },
    {
      "epoch": 0.20097560975609757,
      "step": 927,
      "training_loss": 8.239709854125977
    },
    {
      "epoch": 0.20119241192411924,
      "grad_norm": 10.684964179992676,
      "learning_rate": 1e-05,
      "loss": 6.9387,
      "step": 928
    },
    {
      "epoch": 0.20119241192411924,
      "step": 928,
      "training_loss": 5.306830883026123
    },
    {
      "epoch": 0.20119241192411924,
      "step": 928,
      "training_loss": 7.0807294845581055
    },
    {
      "epoch": 0.20119241192411924,
      "step": 928,
      "training_loss": 7.104422092437744
    },
    {
      "epoch": 0.20119241192411924,
      "step": 928,
      "training_loss": 7.069748878479004
    },
    {
      "epoch": 0.2014092140921409,
      "step": 929,
      "training_loss": 7.462544918060303
    },
    {
      "epoch": 0.2014092140921409,
      "step": 929,
      "training_loss": 5.234628677368164
    },
    {
      "epoch": 0.2014092140921409,
      "step": 929,
      "training_loss": 6.4365339279174805
    },
    {
      "epoch": 0.2014092140921409,
      "step": 929,
      "training_loss": 6.622644901275635
    },
    {
      "epoch": 0.2016260162601626,
      "step": 930,
      "training_loss": 8.1332368850708
    },
    {
      "epoch": 0.2016260162601626,
      "step": 930,
      "training_loss": 6.042628288269043
    },
    {
      "epoch": 0.2016260162601626,
      "step": 930,
      "training_loss": 7.300492763519287
    },
    {
      "epoch": 0.2016260162601626,
      "step": 930,
      "training_loss": 6.684426784515381
    },
    {
      "epoch": 0.20184281842818427,
      "step": 931,
      "training_loss": 8.131669044494629
    },
    {
      "epoch": 0.20184281842818427,
      "step": 931,
      "training_loss": 7.155475616455078
    },
    {
      "epoch": 0.20184281842818427,
      "step": 931,
      "training_loss": 7.361519813537598
    },
    {
      "epoch": 0.20184281842818427,
      "step": 931,
      "training_loss": 6.139490604400635
    },
    {
      "epoch": 0.20205962059620597,
      "grad_norm": 11.229714393615723,
      "learning_rate": 1e-05,
      "loss": 6.8292,
      "step": 932
    },
    {
      "epoch": 0.20205962059620597,
      "step": 932,
      "training_loss": 8.513408660888672
    },
    {
      "epoch": 0.20205962059620597,
      "step": 932,
      "training_loss": 7.47006368637085
    },
    {
      "epoch": 0.20205962059620597,
      "step": 932,
      "training_loss": 8.397581100463867
    },
    {
      "epoch": 0.20205962059620597,
      "step": 932,
      "training_loss": 6.862460613250732
    },
    {
      "epoch": 0.20227642276422764,
      "step": 933,
      "training_loss": 7.429778575897217
    },
    {
      "epoch": 0.20227642276422764,
      "step": 933,
      "training_loss": 7.159305572509766
    },
    {
      "epoch": 0.20227642276422764,
      "step": 933,
      "training_loss": 7.055986404418945
    },
    {
      "epoch": 0.20227642276422764,
      "step": 933,
      "training_loss": 6.316487789154053
    },
    {
      "epoch": 0.20249322493224933,
      "step": 934,
      "training_loss": 7.5689377784729
    },
    {
      "epoch": 0.20249322493224933,
      "step": 934,
      "training_loss": 7.640231132507324
    },
    {
      "epoch": 0.20249322493224933,
      "step": 934,
      "training_loss": 7.580584526062012
    },
    {
      "epoch": 0.20249322493224933,
      "step": 934,
      "training_loss": 7.678936004638672
    },
    {
      "epoch": 0.202710027100271,
      "step": 935,
      "training_loss": 7.801337242126465
    },
    {
      "epoch": 0.202710027100271,
      "step": 935,
      "training_loss": 5.979376316070557
    },
    {
      "epoch": 0.202710027100271,
      "step": 935,
      "training_loss": 6.328587532043457
    },
    {
      "epoch": 0.202710027100271,
      "step": 935,
      "training_loss": 7.152510643005371
    },
    {
      "epoch": 0.2029268292682927,
      "grad_norm": 8.050337791442871,
      "learning_rate": 1e-05,
      "loss": 7.3085,
      "step": 936
    },
    {
      "epoch": 0.2029268292682927,
      "step": 936,
      "training_loss": 7.412740230560303
    },
    {
      "epoch": 0.2029268292682927,
      "step": 936,
      "training_loss": 7.542388916015625
    },
    {
      "epoch": 0.2029268292682927,
      "step": 936,
      "training_loss": 7.299499988555908
    },
    {
      "epoch": 0.2029268292682927,
      "step": 936,
      "training_loss": 6.4343390464782715
    },
    {
      "epoch": 0.20314363143631436,
      "step": 937,
      "training_loss": 5.112453937530518
    },
    {
      "epoch": 0.20314363143631436,
      "step": 937,
      "training_loss": 5.56180477142334
    },
    {
      "epoch": 0.20314363143631436,
      "step": 937,
      "training_loss": 6.815736770629883
    },
    {
      "epoch": 0.20314363143631436,
      "step": 937,
      "training_loss": 6.620245456695557
    },
    {
      "epoch": 0.20336043360433603,
      "step": 938,
      "training_loss": 7.05021333694458
    },
    {
      "epoch": 0.20336043360433603,
      "step": 938,
      "training_loss": 7.488569736480713
    },
    {
      "epoch": 0.20336043360433603,
      "step": 938,
      "training_loss": 8.58554458618164
    },
    {
      "epoch": 0.20336043360433603,
      "step": 938,
      "training_loss": 7.182085990905762
    },
    {
      "epoch": 0.20357723577235773,
      "step": 939,
      "training_loss": 7.535633087158203
    },
    {
      "epoch": 0.20357723577235773,
      "step": 939,
      "training_loss": 6.3729329109191895
    },
    {
      "epoch": 0.20357723577235773,
      "step": 939,
      "training_loss": 6.7463603019714355
    },
    {
      "epoch": 0.20357723577235773,
      "step": 939,
      "training_loss": 7.912971019744873
    },
    {
      "epoch": 0.2037940379403794,
      "grad_norm": 10.25662899017334,
      "learning_rate": 1e-05,
      "loss": 6.9796,
      "step": 940
    },
    {
      "epoch": 0.2037940379403794,
      "step": 940,
      "training_loss": 7.69617223739624
    },
    {
      "epoch": 0.2037940379403794,
      "step": 940,
      "training_loss": 7.972501754760742
    },
    {
      "epoch": 0.2037940379403794,
      "step": 940,
      "training_loss": 6.604824066162109
    },
    {
      "epoch": 0.2037940379403794,
      "step": 940,
      "training_loss": 6.748988151550293
    },
    {
      "epoch": 0.2040108401084011,
      "step": 941,
      "training_loss": 7.5925397872924805
    },
    {
      "epoch": 0.2040108401084011,
      "step": 941,
      "training_loss": 6.980581283569336
    },
    {
      "epoch": 0.2040108401084011,
      "step": 941,
      "training_loss": 5.221174240112305
    },
    {
      "epoch": 0.2040108401084011,
      "step": 941,
      "training_loss": 8.271592140197754
    },
    {
      "epoch": 0.20422764227642276,
      "step": 942,
      "training_loss": 8.285240173339844
    },
    {
      "epoch": 0.20422764227642276,
      "step": 942,
      "training_loss": 7.198753356933594
    },
    {
      "epoch": 0.20422764227642276,
      "step": 942,
      "training_loss": 8.651240348815918
    },
    {
      "epoch": 0.20422764227642276,
      "step": 942,
      "training_loss": 7.786141872406006
    },
    {
      "epoch": 0.20444444444444446,
      "step": 943,
      "training_loss": 7.848208904266357
    },
    {
      "epoch": 0.20444444444444446,
      "step": 943,
      "training_loss": 6.1206889152526855
    },
    {
      "epoch": 0.20444444444444446,
      "step": 943,
      "training_loss": 5.202738285064697
    },
    {
      "epoch": 0.20444444444444446,
      "step": 943,
      "training_loss": 6.917944431304932
    },
    {
      "epoch": 0.20466124661246612,
      "grad_norm": 20.709840774536133,
      "learning_rate": 1e-05,
      "loss": 7.1937,
      "step": 944
    },
    {
      "epoch": 0.20466124661246612,
      "step": 944,
      "training_loss": 6.641531467437744
    },
    {
      "epoch": 0.20466124661246612,
      "step": 944,
      "training_loss": 6.21943998336792
    },
    {
      "epoch": 0.20466124661246612,
      "step": 944,
      "training_loss": 6.054539680480957
    },
    {
      "epoch": 0.20466124661246612,
      "step": 944,
      "training_loss": 7.983400821685791
    },
    {
      "epoch": 0.2048780487804878,
      "step": 945,
      "training_loss": 7.18771505355835
    },
    {
      "epoch": 0.2048780487804878,
      "step": 945,
      "training_loss": 7.0154500007629395
    },
    {
      "epoch": 0.2048780487804878,
      "step": 945,
      "training_loss": 7.3548264503479
    },
    {
      "epoch": 0.2048780487804878,
      "step": 945,
      "training_loss": 8.938995361328125
    },
    {
      "epoch": 0.2050948509485095,
      "step": 946,
      "training_loss": 7.730776786804199
    },
    {
      "epoch": 0.2050948509485095,
      "step": 946,
      "training_loss": 7.442458152770996
    },
    {
      "epoch": 0.2050948509485095,
      "step": 946,
      "training_loss": 6.281740188598633
    },
    {
      "epoch": 0.2050948509485095,
      "step": 946,
      "training_loss": 7.285667419433594
    },
    {
      "epoch": 0.20531165311653116,
      "step": 947,
      "training_loss": 7.419149875640869
    },
    {
      "epoch": 0.20531165311653116,
      "step": 947,
      "training_loss": 7.9907026290893555
    },
    {
      "epoch": 0.20531165311653116,
      "step": 947,
      "training_loss": 7.841375827789307
    },
    {
      "epoch": 0.20531165311653116,
      "step": 947,
      "training_loss": 6.770568370819092
    },
    {
      "epoch": 0.20552845528455285,
      "grad_norm": 11.56143569946289,
      "learning_rate": 1e-05,
      "loss": 7.2599,
      "step": 948
    },
    {
      "epoch": 0.20552845528455285,
      "step": 948,
      "training_loss": 7.147912502288818
    },
    {
      "epoch": 0.20552845528455285,
      "step": 948,
      "training_loss": 6.523369312286377
    },
    {
      "epoch": 0.20552845528455285,
      "step": 948,
      "training_loss": 7.673999309539795
    },
    {
      "epoch": 0.20552845528455285,
      "step": 948,
      "training_loss": 6.8589253425598145
    },
    {
      "epoch": 0.20574525745257452,
      "step": 949,
      "training_loss": 6.6316914558410645
    },
    {
      "epoch": 0.20574525745257452,
      "step": 949,
      "training_loss": 6.505194187164307
    },
    {
      "epoch": 0.20574525745257452,
      "step": 949,
      "training_loss": 6.881287097930908
    },
    {
      "epoch": 0.20574525745257452,
      "step": 949,
      "training_loss": 6.716148376464844
    },
    {
      "epoch": 0.20596205962059622,
      "step": 950,
      "training_loss": 7.705357551574707
    },
    {
      "epoch": 0.20596205962059622,
      "step": 950,
      "training_loss": 7.234653949737549
    },
    {
      "epoch": 0.20596205962059622,
      "step": 950,
      "training_loss": 4.728617191314697
    },
    {
      "epoch": 0.20596205962059622,
      "step": 950,
      "training_loss": 8.132536888122559
    },
    {
      "epoch": 0.20617886178861788,
      "step": 951,
      "training_loss": 6.226592540740967
    },
    {
      "epoch": 0.20617886178861788,
      "step": 951,
      "training_loss": 6.024975776672363
    },
    {
      "epoch": 0.20617886178861788,
      "step": 951,
      "training_loss": 6.391762733459473
    },
    {
      "epoch": 0.20617886178861788,
      "step": 951,
      "training_loss": 6.8909783363342285
    },
    {
      "epoch": 0.20639566395663958,
      "grad_norm": 7.193974018096924,
      "learning_rate": 1e-05,
      "loss": 6.7671,
      "step": 952
    },
    {
      "epoch": 0.20639566395663958,
      "step": 952,
      "training_loss": 7.503868579864502
    },
    {
      "epoch": 0.20639566395663958,
      "step": 952,
      "training_loss": 6.246789932250977
    },
    {
      "epoch": 0.20639566395663958,
      "step": 952,
      "training_loss": 7.054797649383545
    },
    {
      "epoch": 0.20639566395663958,
      "step": 952,
      "training_loss": 6.230038642883301
    },
    {
      "epoch": 0.20661246612466125,
      "step": 953,
      "training_loss": 7.662936687469482
    },
    {
      "epoch": 0.20661246612466125,
      "step": 953,
      "training_loss": 5.528838634490967
    },
    {
      "epoch": 0.20661246612466125,
      "step": 953,
      "training_loss": 8.346099853515625
    },
    {
      "epoch": 0.20661246612466125,
      "step": 953,
      "training_loss": 7.101447582244873
    },
    {
      "epoch": 0.20682926829268292,
      "step": 954,
      "training_loss": 7.253212928771973
    },
    {
      "epoch": 0.20682926829268292,
      "step": 954,
      "training_loss": 6.623206615447998
    },
    {
      "epoch": 0.20682926829268292,
      "step": 954,
      "training_loss": 6.733837604522705
    },
    {
      "epoch": 0.20682926829268292,
      "step": 954,
      "training_loss": 7.201745986938477
    },
    {
      "epoch": 0.2070460704607046,
      "step": 955,
      "training_loss": 5.60967493057251
    },
    {
      "epoch": 0.2070460704607046,
      "step": 955,
      "training_loss": 6.806154727935791
    },
    {
      "epoch": 0.2070460704607046,
      "step": 955,
      "training_loss": 6.711158275604248
    },
    {
      "epoch": 0.2070460704607046,
      "step": 955,
      "training_loss": 7.49789571762085
    },
    {
      "epoch": 0.20726287262872628,
      "grad_norm": 10.065765380859375,
      "learning_rate": 1e-05,
      "loss": 6.882,
      "step": 956
    },
    {
      "epoch": 0.20726287262872628,
      "step": 956,
      "training_loss": 7.21502161026001
    },
    {
      "epoch": 0.20726287262872628,
      "step": 956,
      "training_loss": 7.57265043258667
    },
    {
      "epoch": 0.20726287262872628,
      "step": 956,
      "training_loss": 8.609515190124512
    },
    {
      "epoch": 0.20726287262872628,
      "step": 956,
      "training_loss": 7.69774866104126
    },
    {
      "epoch": 0.20747967479674798,
      "step": 957,
      "training_loss": 7.754255771636963
    },
    {
      "epoch": 0.20747967479674798,
      "step": 957,
      "training_loss": 7.47467041015625
    },
    {
      "epoch": 0.20747967479674798,
      "step": 957,
      "training_loss": 6.135778903961182
    },
    {
      "epoch": 0.20747967479674798,
      "step": 957,
      "training_loss": 6.833698272705078
    },
    {
      "epoch": 0.20769647696476964,
      "step": 958,
      "training_loss": 8.076727867126465
    },
    {
      "epoch": 0.20769647696476964,
      "step": 958,
      "training_loss": 6.18071985244751
    },
    {
      "epoch": 0.20769647696476964,
      "step": 958,
      "training_loss": 7.544920921325684
    },
    {
      "epoch": 0.20769647696476964,
      "step": 958,
      "training_loss": 6.746448516845703
    },
    {
      "epoch": 0.20791327913279134,
      "step": 959,
      "training_loss": 5.129591941833496
    },
    {
      "epoch": 0.20791327913279134,
      "step": 959,
      "training_loss": 7.287917137145996
    },
    {
      "epoch": 0.20791327913279134,
      "step": 959,
      "training_loss": 8.023297309875488
    },
    {
      "epoch": 0.20791327913279134,
      "step": 959,
      "training_loss": 7.695043087005615
    },
    {
      "epoch": 0.208130081300813,
      "grad_norm": 12.473396301269531,
      "learning_rate": 1e-05,
      "loss": 7.2486,
      "step": 960
    },
    {
      "epoch": 0.208130081300813,
      "step": 960,
      "training_loss": 6.8862624168396
    },
    {
      "epoch": 0.208130081300813,
      "step": 960,
      "training_loss": 8.018321990966797
    },
    {
      "epoch": 0.208130081300813,
      "step": 960,
      "training_loss": 5.963377952575684
    },
    {
      "epoch": 0.208130081300813,
      "step": 960,
      "training_loss": 7.080880165100098
    },
    {
      "epoch": 0.20834688346883468,
      "step": 961,
      "training_loss": 6.9259724617004395
    },
    {
      "epoch": 0.20834688346883468,
      "step": 961,
      "training_loss": 6.3260040283203125
    },
    {
      "epoch": 0.20834688346883468,
      "step": 961,
      "training_loss": 7.495688438415527
    },
    {
      "epoch": 0.20834688346883468,
      "step": 961,
      "training_loss": 7.327057361602783
    },
    {
      "epoch": 0.20856368563685637,
      "step": 962,
      "training_loss": 7.417838096618652
    },
    {
      "epoch": 0.20856368563685637,
      "step": 962,
      "training_loss": 6.969508171081543
    },
    {
      "epoch": 0.20856368563685637,
      "step": 962,
      "training_loss": 6.5072021484375
    },
    {
      "epoch": 0.20856368563685637,
      "step": 962,
      "training_loss": 5.406200408935547
    },
    {
      "epoch": 0.20878048780487804,
      "step": 963,
      "training_loss": 8.404203414916992
    },
    {
      "epoch": 0.20878048780487804,
      "step": 963,
      "training_loss": 6.724545001983643
    },
    {
      "epoch": 0.20878048780487804,
      "step": 963,
      "training_loss": 8.785811424255371
    },
    {
      "epoch": 0.20878048780487804,
      "step": 963,
      "training_loss": 7.580672264099121
    },
    {
      "epoch": 0.20899728997289974,
      "grad_norm": 14.45427131652832,
      "learning_rate": 1e-05,
      "loss": 7.1137,
      "step": 964
    },
    {
      "epoch": 0.20899728997289974,
      "step": 964,
      "training_loss": 7.370247840881348
    },
    {
      "epoch": 0.20899728997289974,
      "step": 964,
      "training_loss": 6.299409866333008
    },
    {
      "epoch": 0.20899728997289974,
      "step": 964,
      "training_loss": 6.963869094848633
    },
    {
      "epoch": 0.20899728997289974,
      "step": 964,
      "training_loss": 7.019730091094971
    },
    {
      "epoch": 0.2092140921409214,
      "step": 965,
      "training_loss": 7.072245121002197
    },
    {
      "epoch": 0.2092140921409214,
      "step": 965,
      "training_loss": 7.386733531951904
    },
    {
      "epoch": 0.2092140921409214,
      "step": 965,
      "training_loss": 6.684301376342773
    },
    {
      "epoch": 0.2092140921409214,
      "step": 965,
      "training_loss": 4.898955345153809
    },
    {
      "epoch": 0.2094308943089431,
      "step": 966,
      "training_loss": 7.214037895202637
    },
    {
      "epoch": 0.2094308943089431,
      "step": 966,
      "training_loss": 6.378685474395752
    },
    {
      "epoch": 0.2094308943089431,
      "step": 966,
      "training_loss": 7.377138137817383
    },
    {
      "epoch": 0.2094308943089431,
      "step": 966,
      "training_loss": 6.4468841552734375
    },
    {
      "epoch": 0.20964769647696477,
      "step": 967,
      "training_loss": 7.723830223083496
    },
    {
      "epoch": 0.20964769647696477,
      "step": 967,
      "training_loss": 7.185025215148926
    },
    {
      "epoch": 0.20964769647696477,
      "step": 967,
      "training_loss": 6.125648498535156
    },
    {
      "epoch": 0.20964769647696477,
      "step": 967,
      "training_loss": 5.005349636077881
    },
    {
      "epoch": 0.20986449864498646,
      "grad_norm": 9.891280174255371,
      "learning_rate": 1e-05,
      "loss": 6.697,
      "step": 968
    },
    {
      "epoch": 0.20986449864498646,
      "step": 968,
      "training_loss": 5.908299446105957
    },
    {
      "epoch": 0.20986449864498646,
      "step": 968,
      "training_loss": 7.348592281341553
    },
    {
      "epoch": 0.20986449864498646,
      "step": 968,
      "training_loss": 7.623134136199951
    },
    {
      "epoch": 0.20986449864498646,
      "step": 968,
      "training_loss": 5.8047685623168945
    },
    {
      "epoch": 0.21008130081300813,
      "step": 969,
      "training_loss": 8.166605949401855
    },
    {
      "epoch": 0.21008130081300813,
      "step": 969,
      "training_loss": 6.214479923248291
    },
    {
      "epoch": 0.21008130081300813,
      "step": 969,
      "training_loss": 7.763429164886475
    },
    {
      "epoch": 0.21008130081300813,
      "step": 969,
      "training_loss": 7.246802806854248
    },
    {
      "epoch": 0.2102981029810298,
      "step": 970,
      "training_loss": 6.360289573669434
    },
    {
      "epoch": 0.2102981029810298,
      "step": 970,
      "training_loss": 7.215107440948486
    },
    {
      "epoch": 0.2102981029810298,
      "step": 970,
      "training_loss": 6.240716457366943
    },
    {
      "epoch": 0.2102981029810298,
      "step": 970,
      "training_loss": 7.251758098602295
    },
    {
      "epoch": 0.2105149051490515,
      "step": 971,
      "training_loss": 6.222929000854492
    },
    {
      "epoch": 0.2105149051490515,
      "step": 971,
      "training_loss": 7.0632829666137695
    },
    {
      "epoch": 0.2105149051490515,
      "step": 971,
      "training_loss": 7.167920112609863
    },
    {
      "epoch": 0.2105149051490515,
      "step": 971,
      "training_loss": 7.49587869644165
    },
    {
      "epoch": 0.21073170731707316,
      "grad_norm": 16.57097816467285,
      "learning_rate": 1e-05,
      "loss": 6.9434,
      "step": 972
    },
    {
      "epoch": 0.21073170731707316,
      "step": 972,
      "training_loss": 6.962765216827393
    },
    {
      "epoch": 0.21073170731707316,
      "step": 972,
      "training_loss": 7.189886569976807
    },
    {
      "epoch": 0.21073170731707316,
      "step": 972,
      "training_loss": 6.9198713302612305
    },
    {
      "epoch": 0.21073170731707316,
      "step": 972,
      "training_loss": 7.37045431137085
    },
    {
      "epoch": 0.21094850948509486,
      "step": 973,
      "training_loss": 7.1267571449279785
    },
    {
      "epoch": 0.21094850948509486,
      "step": 973,
      "training_loss": 6.627594470977783
    },
    {
      "epoch": 0.21094850948509486,
      "step": 973,
      "training_loss": 7.7014641761779785
    },
    {
      "epoch": 0.21094850948509486,
      "step": 973,
      "training_loss": 8.065690994262695
    },
    {
      "epoch": 0.21116531165311653,
      "step": 974,
      "training_loss": 7.901725769042969
    },
    {
      "epoch": 0.21116531165311653,
      "step": 974,
      "training_loss": 7.292377471923828
    },
    {
      "epoch": 0.21116531165311653,
      "step": 974,
      "training_loss": 7.605903148651123
    },
    {
      "epoch": 0.21116531165311653,
      "step": 974,
      "training_loss": 7.1569037437438965
    },
    {
      "epoch": 0.21138211382113822,
      "step": 975,
      "training_loss": 7.275448322296143
    },
    {
      "epoch": 0.21138211382113822,
      "step": 975,
      "training_loss": 6.153111934661865
    },
    {
      "epoch": 0.21138211382113822,
      "step": 975,
      "training_loss": 7.113094329833984
    },
    {
      "epoch": 0.21138211382113822,
      "step": 975,
      "training_loss": 7.552021503448486
    },
    {
      "epoch": 0.2115989159891599,
      "grad_norm": 12.221220016479492,
      "learning_rate": 1e-05,
      "loss": 7.2509,
      "step": 976
    },
    {
      "epoch": 0.2115989159891599,
      "step": 976,
      "training_loss": 5.582419395446777
    },
    {
      "epoch": 0.2115989159891599,
      "step": 976,
      "training_loss": 6.608456611633301
    },
    {
      "epoch": 0.2115989159891599,
      "step": 976,
      "training_loss": 6.077754497528076
    },
    {
      "epoch": 0.2115989159891599,
      "step": 976,
      "training_loss": 6.377397537231445
    },
    {
      "epoch": 0.21181571815718156,
      "step": 977,
      "training_loss": 8.005874633789062
    },
    {
      "epoch": 0.21181571815718156,
      "step": 977,
      "training_loss": 6.943305492401123
    },
    {
      "epoch": 0.21181571815718156,
      "step": 977,
      "training_loss": 5.039041996002197
    },
    {
      "epoch": 0.21181571815718156,
      "step": 977,
      "training_loss": 5.3582987785339355
    },
    {
      "epoch": 0.21203252032520326,
      "step": 978,
      "training_loss": 6.967587471008301
    },
    {
      "epoch": 0.21203252032520326,
      "step": 978,
      "training_loss": 7.118664741516113
    },
    {
      "epoch": 0.21203252032520326,
      "step": 978,
      "training_loss": 7.86191463470459
    },
    {
      "epoch": 0.21203252032520326,
      "step": 978,
      "training_loss": 7.3010711669921875
    },
    {
      "epoch": 0.21224932249322492,
      "step": 979,
      "training_loss": 7.054254531860352
    },
    {
      "epoch": 0.21224932249322492,
      "step": 979,
      "training_loss": 6.0650224685668945
    },
    {
      "epoch": 0.21224932249322492,
      "step": 979,
      "training_loss": 6.174571514129639
    },
    {
      "epoch": 0.21224932249322492,
      "step": 979,
      "training_loss": 7.094186305999756
    },
    {
      "epoch": 0.21246612466124662,
      "grad_norm": 11.45328426361084,
      "learning_rate": 1e-05,
      "loss": 6.6019,
      "step": 980
    },
    {
      "epoch": 0.21246612466124662,
      "step": 980,
      "training_loss": 7.299440383911133
    },
    {
      "epoch": 0.21246612466124662,
      "step": 980,
      "training_loss": 7.498958587646484
    },
    {
      "epoch": 0.21246612466124662,
      "step": 980,
      "training_loss": 5.643187522888184
    },
    {
      "epoch": 0.21246612466124662,
      "step": 980,
      "training_loss": 6.710718631744385
    },
    {
      "epoch": 0.2126829268292683,
      "step": 981,
      "training_loss": 4.989567279815674
    },
    {
      "epoch": 0.2126829268292683,
      "step": 981,
      "training_loss": 7.3485188484191895
    },
    {
      "epoch": 0.2126829268292683,
      "step": 981,
      "training_loss": 6.408294200897217
    },
    {
      "epoch": 0.2126829268292683,
      "step": 981,
      "training_loss": 6.234899520874023
    },
    {
      "epoch": 0.21289972899728998,
      "step": 982,
      "training_loss": 7.222224235534668
    },
    {
      "epoch": 0.21289972899728998,
      "step": 982,
      "training_loss": 5.323256492614746
    },
    {
      "epoch": 0.21289972899728998,
      "step": 982,
      "training_loss": 7.286306381225586
    },
    {
      "epoch": 0.21289972899728998,
      "step": 982,
      "training_loss": 7.765387058258057
    },
    {
      "epoch": 0.21311653116531165,
      "step": 983,
      "training_loss": 6.803764820098877
    },
    {
      "epoch": 0.21311653116531165,
      "step": 983,
      "training_loss": 6.057846546173096
    },
    {
      "epoch": 0.21311653116531165,
      "step": 983,
      "training_loss": 7.369040489196777
    },
    {
      "epoch": 0.21311653116531165,
      "step": 983,
      "training_loss": 4.631045818328857
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 13.128499984741211,
      "learning_rate": 1e-05,
      "loss": 6.537,
      "step": 984
    },
    {
      "epoch": 0.21333333333333335,
      "step": 984,
      "training_loss": 7.0165300369262695
    },
    {
      "epoch": 0.21333333333333335,
      "step": 984,
      "training_loss": 9.636990547180176
    },
    {
      "epoch": 0.21333333333333335,
      "step": 984,
      "training_loss": 7.92141056060791
    },
    {
      "epoch": 0.21333333333333335,
      "step": 984,
      "training_loss": 8.328207969665527
    },
    {
      "epoch": 0.21355013550135502,
      "step": 985,
      "training_loss": 8.655137062072754
    },
    {
      "epoch": 0.21355013550135502,
      "step": 985,
      "training_loss": 5.962042331695557
    },
    {
      "epoch": 0.21355013550135502,
      "step": 985,
      "training_loss": 7.139155387878418
    },
    {
      "epoch": 0.21355013550135502,
      "step": 985,
      "training_loss": 5.951781272888184
    },
    {
      "epoch": 0.21376693766937668,
      "step": 986,
      "training_loss": 7.242577075958252
    },
    {
      "epoch": 0.21376693766937668,
      "step": 986,
      "training_loss": 6.192465305328369
    },
    {
      "epoch": 0.21376693766937668,
      "step": 986,
      "training_loss": 7.066593170166016
    },
    {
      "epoch": 0.21376693766937668,
      "step": 986,
      "training_loss": 5.052526473999023
    },
    {
      "epoch": 0.21398373983739838,
      "step": 987,
      "training_loss": 6.169510841369629
    },
    {
      "epoch": 0.21398373983739838,
      "step": 987,
      "training_loss": 6.320633411407471
    },
    {
      "epoch": 0.21398373983739838,
      "step": 987,
      "training_loss": 7.460927963256836
    },
    {
      "epoch": 0.21398373983739838,
      "step": 987,
      "training_loss": 6.767777919769287
    },
    {
      "epoch": 0.21420054200542005,
      "grad_norm": 13.682676315307617,
      "learning_rate": 1e-05,
      "loss": 7.0553,
      "step": 988
    },
    {
      "epoch": 0.21420054200542005,
      "step": 988,
      "training_loss": 7.26161527633667
    },
    {
      "epoch": 0.21420054200542005,
      "step": 988,
      "training_loss": 7.195328712463379
    },
    {
      "epoch": 0.21420054200542005,
      "step": 988,
      "training_loss": 6.575880527496338
    },
    {
      "epoch": 0.21420054200542005,
      "step": 988,
      "training_loss": 7.457463264465332
    },
    {
      "epoch": 0.21441734417344174,
      "step": 989,
      "training_loss": 7.717080116271973
    },
    {
      "epoch": 0.21441734417344174,
      "step": 989,
      "training_loss": 7.568975448608398
    },
    {
      "epoch": 0.21441734417344174,
      "step": 989,
      "training_loss": 7.150315761566162
    },
    {
      "epoch": 0.21441734417344174,
      "step": 989,
      "training_loss": 6.121634006500244
    },
    {
      "epoch": 0.2146341463414634,
      "step": 990,
      "training_loss": 7.137089252471924
    },
    {
      "epoch": 0.2146341463414634,
      "step": 990,
      "training_loss": 6.512327194213867
    },
    {
      "epoch": 0.2146341463414634,
      "step": 990,
      "training_loss": 5.7186503410339355
    },
    {
      "epoch": 0.2146341463414634,
      "step": 990,
      "training_loss": 7.857564449310303
    },
    {
      "epoch": 0.2148509485094851,
      "step": 991,
      "training_loss": 6.544626235961914
    },
    {
      "epoch": 0.2148509485094851,
      "step": 991,
      "training_loss": 5.669384956359863
    },
    {
      "epoch": 0.2148509485094851,
      "step": 991,
      "training_loss": 8.197263717651367
    },
    {
      "epoch": 0.2148509485094851,
      "step": 991,
      "training_loss": 7.903849124908447
    },
    {
      "epoch": 0.21506775067750677,
      "grad_norm": 9.18397045135498,
      "learning_rate": 1e-05,
      "loss": 7.0368,
      "step": 992
    },
    {
      "epoch": 0.21506775067750677,
      "step": 992,
      "training_loss": 8.318384170532227
    },
    {
      "epoch": 0.21506775067750677,
      "step": 992,
      "training_loss": 6.759368896484375
    },
    {
      "epoch": 0.21506775067750677,
      "step": 992,
      "training_loss": 7.203463077545166
    },
    {
      "epoch": 0.21506775067750677,
      "step": 992,
      "training_loss": 6.139601230621338
    },
    {
      "epoch": 0.21528455284552844,
      "step": 993,
      "training_loss": 7.125643730163574
    },
    {
      "epoch": 0.21528455284552844,
      "step": 993,
      "training_loss": 7.2910990715026855
    },
    {
      "epoch": 0.21528455284552844,
      "step": 993,
      "training_loss": 7.852033615112305
    },
    {
      "epoch": 0.21528455284552844,
      "step": 993,
      "training_loss": 7.999146938323975
    },
    {
      "epoch": 0.21550135501355014,
      "step": 994,
      "training_loss": 6.983765602111816
    },
    {
      "epoch": 0.21550135501355014,
      "step": 994,
      "training_loss": 7.62445068359375
    },
    {
      "epoch": 0.21550135501355014,
      "step": 994,
      "training_loss": 6.96693229675293
    },
    {
      "epoch": 0.21550135501355014,
      "step": 994,
      "training_loss": 7.5112762451171875
    },
    {
      "epoch": 0.2157181571815718,
      "step": 995,
      "training_loss": 6.913211822509766
    },
    {
      "epoch": 0.2157181571815718,
      "step": 995,
      "training_loss": 7.895196437835693
    },
    {
      "epoch": 0.2157181571815718,
      "step": 995,
      "training_loss": 7.714592933654785
    },
    {
      "epoch": 0.2157181571815718,
      "step": 995,
      "training_loss": 6.136826515197754
    },
    {
      "epoch": 0.2159349593495935,
      "grad_norm": 11.356926918029785,
      "learning_rate": 1e-05,
      "loss": 7.2772,
      "step": 996
    },
    {
      "epoch": 0.2159349593495935,
      "step": 996,
      "training_loss": 6.39324951171875
    },
    {
      "epoch": 0.2159349593495935,
      "step": 996,
      "training_loss": 6.965189456939697
    },
    {
      "epoch": 0.2159349593495935,
      "step": 996,
      "training_loss": 4.856173038482666
    },
    {
      "epoch": 0.2159349593495935,
      "step": 996,
      "training_loss": 6.796143531799316
    },
    {
      "epoch": 0.21615176151761517,
      "step": 997,
      "training_loss": 7.012423038482666
    },
    {
      "epoch": 0.21615176151761517,
      "step": 997,
      "training_loss": 7.791145324707031
    },
    {
      "epoch": 0.21615176151761517,
      "step": 997,
      "training_loss": 6.089158535003662
    },
    {
      "epoch": 0.21615176151761517,
      "step": 997,
      "training_loss": 7.546772003173828
    },
    {
      "epoch": 0.21636856368563687,
      "step": 998,
      "training_loss": 7.113350868225098
    },
    {
      "epoch": 0.21636856368563687,
      "step": 998,
      "training_loss": 6.909821033477783
    },
    {
      "epoch": 0.21636856368563687,
      "step": 998,
      "training_loss": 5.086513996124268
    },
    {
      "epoch": 0.21636856368563687,
      "step": 998,
      "training_loss": 5.281689167022705
    },
    {
      "epoch": 0.21658536585365853,
      "step": 999,
      "training_loss": 7.155663013458252
    },
    {
      "epoch": 0.21658536585365853,
      "step": 999,
      "training_loss": 7.97465181350708
    },
    {
      "epoch": 0.21658536585365853,
      "step": 999,
      "training_loss": 6.3320536613464355
    },
    {
      "epoch": 0.21658536585365853,
      "step": 999,
      "training_loss": 7.37673807144165
    },
    {
      "epoch": 0.21680216802168023,
      "grad_norm": 9.61492919921875,
      "learning_rate": 1e-05,
      "loss": 6.6675,
      "step": 1000
    },
    {
      "epoch": 0.21680216802168023,
      "eval_runtime": 475.941,
      "eval_samples_per_second": 4.307,
      "eval_steps_per_second": 4.307,
      "step": 1000
    },
    {
      "epoch": 0.21680216802168023,
      "step": 1000,
      "training_loss": 7.200301170349121
    },
    {
      "epoch": 0.21680216802168023,
      "step": 1000,
      "training_loss": 7.18200159072876
    },
    {
      "epoch": 0.21680216802168023,
      "step": 1000,
      "training_loss": 6.448754787445068
    },
    {
      "epoch": 0.21680216802168023,
      "step": 1000,
      "training_loss": 7.443253517150879
    },
    {
      "epoch": 0.2170189701897019,
      "step": 1001,
      "training_loss": 6.204944133758545
    },
    {
      "epoch": 0.2170189701897019,
      "step": 1001,
      "training_loss": 6.956484317779541
    },
    {
      "epoch": 0.2170189701897019,
      "step": 1001,
      "training_loss": 8.278193473815918
    },
    {
      "epoch": 0.2170189701897019,
      "step": 1001,
      "training_loss": 6.354549407958984
    },
    {
      "epoch": 0.21723577235772357,
      "step": 1002,
      "training_loss": 7.706345558166504
    },
    {
      "epoch": 0.21723577235772357,
      "step": 1002,
      "training_loss": 6.88260555267334
    },
    {
      "epoch": 0.21723577235772357,
      "step": 1002,
      "training_loss": 7.415295600891113
    },
    {
      "epoch": 0.21723577235772357,
      "step": 1002,
      "training_loss": 9.46681022644043
    },
    {
      "epoch": 0.21745257452574526,
      "step": 1003,
      "training_loss": 6.28498649597168
    },
    {
      "epoch": 0.21745257452574526,
      "step": 1003,
      "training_loss": 6.753265380859375
    },
    {
      "epoch": 0.21745257452574526,
      "step": 1003,
      "training_loss": 7.21872615814209
    },
    {
      "epoch": 0.21745257452574526,
      "step": 1003,
      "training_loss": 6.794654846191406
    },
    {
      "epoch": 0.21766937669376693,
      "grad_norm": 11.087937355041504,
      "learning_rate": 1e-05,
      "loss": 7.1619,
      "step": 1004
    },
    {
      "epoch": 0.21766937669376693,
      "step": 1004,
      "training_loss": 8.103793144226074
    },
    {
      "epoch": 0.21766937669376693,
      "step": 1004,
      "training_loss": 6.696789264678955
    },
    {
      "epoch": 0.21766937669376693,
      "step": 1004,
      "training_loss": 7.202451229095459
    },
    {
      "epoch": 0.21766937669376693,
      "step": 1004,
      "training_loss": 6.795474052429199
    },
    {
      "epoch": 0.21788617886178863,
      "step": 1005,
      "training_loss": 7.07622766494751
    },
    {
      "epoch": 0.21788617886178863,
      "step": 1005,
      "training_loss": 5.795262813568115
    },
    {
      "epoch": 0.21788617886178863,
      "step": 1005,
      "training_loss": 5.790053367614746
    },
    {
      "epoch": 0.21788617886178863,
      "step": 1005,
      "training_loss": 7.690417766571045
    },
    {
      "epoch": 0.2181029810298103,
      "step": 1006,
      "training_loss": 7.886086940765381
    },
    {
      "epoch": 0.2181029810298103,
      "step": 1006,
      "training_loss": 7.402839660644531
    },
    {
      "epoch": 0.2181029810298103,
      "step": 1006,
      "training_loss": 7.679451942443848
    },
    {
      "epoch": 0.2181029810298103,
      "step": 1006,
      "training_loss": 8.481178283691406
    },
    {
      "epoch": 0.218319783197832,
      "step": 1007,
      "training_loss": 7.219592094421387
    },
    {
      "epoch": 0.218319783197832,
      "step": 1007,
      "training_loss": 7.535593032836914
    },
    {
      "epoch": 0.218319783197832,
      "step": 1007,
      "training_loss": 7.372814655303955
    },
    {
      "epoch": 0.218319783197832,
      "step": 1007,
      "training_loss": 7.237918853759766
    },
    {
      "epoch": 0.21853658536585366,
      "grad_norm": 10.041038513183594,
      "learning_rate": 1e-05,
      "loss": 7.2479,
      "step": 1008
    },
    {
      "epoch": 0.21853658536585366,
      "step": 1008,
      "training_loss": 6.08104944229126
    },
    {
      "epoch": 0.21853658536585366,
      "step": 1008,
      "training_loss": 7.404486179351807
    },
    {
      "epoch": 0.21853658536585366,
      "step": 1008,
      "training_loss": 6.244776248931885
    },
    {
      "epoch": 0.21853658536585366,
      "step": 1008,
      "training_loss": 6.723374366760254
    },
    {
      "epoch": 0.21875338753387533,
      "step": 1009,
      "training_loss": 7.370257377624512
    },
    {
      "epoch": 0.21875338753387533,
      "step": 1009,
      "training_loss": 6.3641252517700195
    },
    {
      "epoch": 0.21875338753387533,
      "step": 1009,
      "training_loss": 6.315952777862549
    },
    {
      "epoch": 0.21875338753387533,
      "step": 1009,
      "training_loss": 6.128642559051514
    },
    {
      "epoch": 0.21897018970189702,
      "step": 1010,
      "training_loss": 7.036430358886719
    },
    {
      "epoch": 0.21897018970189702,
      "step": 1010,
      "training_loss": 5.0449652671813965
    },
    {
      "epoch": 0.21897018970189702,
      "step": 1010,
      "training_loss": 5.234369277954102
    },
    {
      "epoch": 0.21897018970189702,
      "step": 1010,
      "training_loss": 7.228369235992432
    },
    {
      "epoch": 0.2191869918699187,
      "step": 1011,
      "training_loss": 6.7594709396362305
    },
    {
      "epoch": 0.2191869918699187,
      "step": 1011,
      "training_loss": 7.44128942489624
    },
    {
      "epoch": 0.2191869918699187,
      "step": 1011,
      "training_loss": 6.200995445251465
    },
    {
      "epoch": 0.2191869918699187,
      "step": 1011,
      "training_loss": 5.882501602172852
    },
    {
      "epoch": 0.2194037940379404,
      "grad_norm": 9.559895515441895,
      "learning_rate": 1e-05,
      "loss": 6.4663,
      "step": 1012
    },
    {
      "epoch": 0.2194037940379404,
      "step": 1012,
      "training_loss": 4.724367618560791
    },
    {
      "epoch": 0.2194037940379404,
      "step": 1012,
      "training_loss": 5.559926986694336
    },
    {
      "epoch": 0.2194037940379404,
      "step": 1012,
      "training_loss": 7.216229438781738
    },
    {
      "epoch": 0.2194037940379404,
      "step": 1012,
      "training_loss": 7.121089458465576
    },
    {
      "epoch": 0.21962059620596205,
      "step": 1013,
      "training_loss": 7.787789344787598
    },
    {
      "epoch": 0.21962059620596205,
      "step": 1013,
      "training_loss": 7.201678276062012
    },
    {
      "epoch": 0.21962059620596205,
      "step": 1013,
      "training_loss": 7.663787364959717
    },
    {
      "epoch": 0.21962059620596205,
      "step": 1013,
      "training_loss": 6.99213171005249
    },
    {
      "epoch": 0.21983739837398375,
      "step": 1014,
      "training_loss": 7.7690510749816895
    },
    {
      "epoch": 0.21983739837398375,
      "step": 1014,
      "training_loss": 6.907878398895264
    },
    {
      "epoch": 0.21983739837398375,
      "step": 1014,
      "training_loss": 6.482990741729736
    },
    {
      "epoch": 0.21983739837398375,
      "step": 1014,
      "training_loss": 7.241072654724121
    },
    {
      "epoch": 0.22005420054200542,
      "step": 1015,
      "training_loss": 6.2811713218688965
    },
    {
      "epoch": 0.22005420054200542,
      "step": 1015,
      "training_loss": 6.746082782745361
    },
    {
      "epoch": 0.22005420054200542,
      "step": 1015,
      "training_loss": 6.04698371887207
    },
    {
      "epoch": 0.22005420054200542,
      "step": 1015,
      "training_loss": 7.200124740600586
    },
    {
      "epoch": 0.22027100271002711,
      "grad_norm": 10.433239936828613,
      "learning_rate": 1e-05,
      "loss": 6.8089,
      "step": 1016
    },
    {
      "epoch": 0.22027100271002711,
      "step": 1016,
      "training_loss": 8.50014877319336
    },
    {
      "epoch": 0.22027100271002711,
      "step": 1016,
      "training_loss": 5.461256980895996
    },
    {
      "epoch": 0.22027100271002711,
      "step": 1016,
      "training_loss": 6.957305908203125
    },
    {
      "epoch": 0.22027100271002711,
      "step": 1016,
      "training_loss": 7.50396203994751
    },
    {
      "epoch": 0.22048780487804878,
      "step": 1017,
      "training_loss": 6.355972766876221
    },
    {
      "epoch": 0.22048780487804878,
      "step": 1017,
      "training_loss": 7.653649806976318
    },
    {
      "epoch": 0.22048780487804878,
      "step": 1017,
      "training_loss": 7.5602216720581055
    },
    {
      "epoch": 0.22048780487804878,
      "step": 1017,
      "training_loss": 8.097250938415527
    },
    {
      "epoch": 0.22070460704607045,
      "step": 1018,
      "training_loss": 6.60841703414917
    },
    {
      "epoch": 0.22070460704607045,
      "step": 1018,
      "training_loss": 5.600517272949219
    },
    {
      "epoch": 0.22070460704607045,
      "step": 1018,
      "training_loss": 7.712099075317383
    },
    {
      "epoch": 0.22070460704607045,
      "step": 1018,
      "training_loss": 7.394993782043457
    },
    {
      "epoch": 0.22092140921409215,
      "step": 1019,
      "training_loss": 4.237303256988525
    },
    {
      "epoch": 0.22092140921409215,
      "step": 1019,
      "training_loss": 5.942808151245117
    },
    {
      "epoch": 0.22092140921409215,
      "step": 1019,
      "training_loss": 7.009834289550781
    },
    {
      "epoch": 0.22092140921409215,
      "step": 1019,
      "training_loss": 7.011541843414307
    },
    {
      "epoch": 0.22113821138211381,
      "grad_norm": 13.654816627502441,
      "learning_rate": 1e-05,
      "loss": 6.8505,
      "step": 1020
    },
    {
      "epoch": 0.22113821138211381,
      "step": 1020,
      "training_loss": 5.7924089431762695
    },
    {
      "epoch": 0.22113821138211381,
      "step": 1020,
      "training_loss": 7.501516819000244
    },
    {
      "epoch": 0.22113821138211381,
      "step": 1020,
      "training_loss": 7.5411529541015625
    },
    {
      "epoch": 0.22113821138211381,
      "step": 1020,
      "training_loss": 8.314125061035156
    },
    {
      "epoch": 0.2213550135501355,
      "step": 1021,
      "training_loss": 6.455413818359375
    },
    {
      "epoch": 0.2213550135501355,
      "step": 1021,
      "training_loss": 7.414417743682861
    },
    {
      "epoch": 0.2213550135501355,
      "step": 1021,
      "training_loss": 7.0845866203308105
    },
    {
      "epoch": 0.2213550135501355,
      "step": 1021,
      "training_loss": 7.044702529907227
    },
    {
      "epoch": 0.22157181571815718,
      "step": 1022,
      "training_loss": 6.698920249938965
    },
    {
      "epoch": 0.22157181571815718,
      "step": 1022,
      "training_loss": 6.773724555969238
    },
    {
      "epoch": 0.22157181571815718,
      "step": 1022,
      "training_loss": 6.1285786628723145
    },
    {
      "epoch": 0.22157181571815718,
      "step": 1022,
      "training_loss": 7.847387313842773
    },
    {
      "epoch": 0.22178861788617887,
      "step": 1023,
      "training_loss": 7.4636735916137695
    },
    {
      "epoch": 0.22178861788617887,
      "step": 1023,
      "training_loss": 7.389291286468506
    },
    {
      "epoch": 0.22178861788617887,
      "step": 1023,
      "training_loss": 5.85585355758667
    },
    {
      "epoch": 0.22178861788617887,
      "step": 1023,
      "training_loss": 7.383887767791748
    },
    {
      "epoch": 0.22200542005420054,
      "grad_norm": 8.285881042480469,
      "learning_rate": 1e-05,
      "loss": 7.0431,
      "step": 1024
    },
    {
      "epoch": 0.22200542005420054,
      "step": 1024,
      "training_loss": 5.822943687438965
    },
    {
      "epoch": 0.22200542005420054,
      "step": 1024,
      "training_loss": 7.0572829246521
    },
    {
      "epoch": 0.22200542005420054,
      "step": 1024,
      "training_loss": 5.449740409851074
    },
    {
      "epoch": 0.22200542005420054,
      "step": 1024,
      "training_loss": 7.2519330978393555
    },
    {
      "epoch": 0.2222222222222222,
      "step": 1025,
      "training_loss": 6.51719331741333
    },
    {
      "epoch": 0.2222222222222222,
      "step": 1025,
      "training_loss": 7.338062763214111
    },
    {
      "epoch": 0.2222222222222222,
      "step": 1025,
      "training_loss": 7.16584587097168
    },
    {
      "epoch": 0.2222222222222222,
      "step": 1025,
      "training_loss": 7.173580646514893
    },
    {
      "epoch": 0.2224390243902439,
      "step": 1026,
      "training_loss": 7.56890344619751
    },
    {
      "epoch": 0.2224390243902439,
      "step": 1026,
      "training_loss": 8.209773063659668
    },
    {
      "epoch": 0.2224390243902439,
      "step": 1026,
      "training_loss": 7.2873430252075195
    },
    {
      "epoch": 0.2224390243902439,
      "step": 1026,
      "training_loss": 7.560556888580322
    },
    {
      "epoch": 0.22265582655826557,
      "step": 1027,
      "training_loss": 6.707864284515381
    },
    {
      "epoch": 0.22265582655826557,
      "step": 1027,
      "training_loss": 5.823641777038574
    },
    {
      "epoch": 0.22265582655826557,
      "step": 1027,
      "training_loss": 6.833600044250488
    },
    {
      "epoch": 0.22265582655826557,
      "step": 1027,
      "training_loss": 8.218318939208984
    },
    {
      "epoch": 0.22287262872628727,
      "grad_norm": 12.471418380737305,
      "learning_rate": 1e-05,
      "loss": 6.9992,
      "step": 1028
    },
    {
      "epoch": 0.22287262872628727,
      "step": 1028,
      "training_loss": 7.208240985870361
    },
    {
      "epoch": 0.22287262872628727,
      "step": 1028,
      "training_loss": 7.266573905944824
    },
    {
      "epoch": 0.22287262872628727,
      "step": 1028,
      "training_loss": 4.696186065673828
    },
    {
      "epoch": 0.22287262872628727,
      "step": 1028,
      "training_loss": 4.908297538757324
    },
    {
      "epoch": 0.22308943089430894,
      "step": 1029,
      "training_loss": 7.106673240661621
    },
    {
      "epoch": 0.22308943089430894,
      "step": 1029,
      "training_loss": 8.08116626739502
    },
    {
      "epoch": 0.22308943089430894,
      "step": 1029,
      "training_loss": 8.204023361206055
    },
    {
      "epoch": 0.22308943089430894,
      "step": 1029,
      "training_loss": 8.50902271270752
    },
    {
      "epoch": 0.22330623306233063,
      "step": 1030,
      "training_loss": 7.294286251068115
    },
    {
      "epoch": 0.22330623306233063,
      "step": 1030,
      "training_loss": 7.718829154968262
    },
    {
      "epoch": 0.22330623306233063,
      "step": 1030,
      "training_loss": 6.613008499145508
    },
    {
      "epoch": 0.22330623306233063,
      "step": 1030,
      "training_loss": 6.452101707458496
    },
    {
      "epoch": 0.2235230352303523,
      "step": 1031,
      "training_loss": 6.291262149810791
    },
    {
      "epoch": 0.2235230352303523,
      "step": 1031,
      "training_loss": 5.840047359466553
    },
    {
      "epoch": 0.2235230352303523,
      "step": 1031,
      "training_loss": 6.497626781463623
    },
    {
      "epoch": 0.2235230352303523,
      "step": 1031,
      "training_loss": 7.500667572021484
    },
    {
      "epoch": 0.223739837398374,
      "grad_norm": 12.266073226928711,
      "learning_rate": 1e-05,
      "loss": 6.8868,
      "step": 1032
    },
    {
      "epoch": 0.223739837398374,
      "step": 1032,
      "training_loss": 6.990556716918945
    },
    {
      "epoch": 0.223739837398374,
      "step": 1032,
      "training_loss": 5.555000305175781
    },
    {
      "epoch": 0.223739837398374,
      "step": 1032,
      "training_loss": 5.675119876861572
    },
    {
      "epoch": 0.223739837398374,
      "step": 1032,
      "training_loss": 7.266265392303467
    },
    {
      "epoch": 0.22395663956639567,
      "step": 1033,
      "training_loss": 7.478727340698242
    },
    {
      "epoch": 0.22395663956639567,
      "step": 1033,
      "training_loss": 6.761635780334473
    },
    {
      "epoch": 0.22395663956639567,
      "step": 1033,
      "training_loss": 7.305683612823486
    },
    {
      "epoch": 0.22395663956639567,
      "step": 1033,
      "training_loss": 7.112029552459717
    },
    {
      "epoch": 0.22417344173441733,
      "step": 1034,
      "training_loss": 6.085116386413574
    },
    {
      "epoch": 0.22417344173441733,
      "step": 1034,
      "training_loss": 7.093148231506348
    },
    {
      "epoch": 0.22417344173441733,
      "step": 1034,
      "training_loss": 6.1049699783325195
    },
    {
      "epoch": 0.22417344173441733,
      "step": 1034,
      "training_loss": 7.65914249420166
    },
    {
      "epoch": 0.22439024390243903,
      "step": 1035,
      "training_loss": 6.601381778717041
    },
    {
      "epoch": 0.22439024390243903,
      "step": 1035,
      "training_loss": 7.438216686248779
    },
    {
      "epoch": 0.22439024390243903,
      "step": 1035,
      "training_loss": 7.827940940856934
    },
    {
      "epoch": 0.22439024390243903,
      "step": 1035,
      "training_loss": 7.400212287902832
    },
    {
      "epoch": 0.2246070460704607,
      "grad_norm": 11.598660469055176,
      "learning_rate": 1e-05,
      "loss": 6.8972,
      "step": 1036
    },
    {
      "epoch": 0.2246070460704607,
      "step": 1036,
      "training_loss": 6.105923175811768
    },
    {
      "epoch": 0.2246070460704607,
      "step": 1036,
      "training_loss": 7.433703422546387
    },
    {
      "epoch": 0.2246070460704607,
      "step": 1036,
      "training_loss": 6.329004287719727
    },
    {
      "epoch": 0.2246070460704607,
      "step": 1036,
      "training_loss": 7.754707336425781
    },
    {
      "epoch": 0.2248238482384824,
      "step": 1037,
      "training_loss": 5.259593486785889
    },
    {
      "epoch": 0.2248238482384824,
      "step": 1037,
      "training_loss": 5.191527843475342
    },
    {
      "epoch": 0.2248238482384824,
      "step": 1037,
      "training_loss": 7.280404567718506
    },
    {
      "epoch": 0.2248238482384824,
      "step": 1037,
      "training_loss": 6.775933742523193
    },
    {
      "epoch": 0.22504065040650406,
      "step": 1038,
      "training_loss": 6.614051342010498
    },
    {
      "epoch": 0.22504065040650406,
      "step": 1038,
      "training_loss": 6.991091251373291
    },
    {
      "epoch": 0.22504065040650406,
      "step": 1038,
      "training_loss": 6.857244491577148
    },
    {
      "epoch": 0.22504065040650406,
      "step": 1038,
      "training_loss": 8.396681785583496
    },
    {
      "epoch": 0.22525745257452576,
      "step": 1039,
      "training_loss": 6.568772315979004
    },
    {
      "epoch": 0.22525745257452576,
      "step": 1039,
      "training_loss": 8.086036682128906
    },
    {
      "epoch": 0.22525745257452576,
      "step": 1039,
      "training_loss": 7.344949245452881
    },
    {
      "epoch": 0.22525745257452576,
      "step": 1039,
      "training_loss": 7.0377888679504395
    },
    {
      "epoch": 0.22547425474254743,
      "grad_norm": 10.380792617797852,
      "learning_rate": 1e-05,
      "loss": 6.8767,
      "step": 1040
    },
    {
      "epoch": 0.22547425474254743,
      "step": 1040,
      "training_loss": 7.178520679473877
    },
    {
      "epoch": 0.22547425474254743,
      "step": 1040,
      "training_loss": 6.566020488739014
    },
    {
      "epoch": 0.22547425474254743,
      "step": 1040,
      "training_loss": 6.640204429626465
    },
    {
      "epoch": 0.22547425474254743,
      "step": 1040,
      "training_loss": 7.8094000816345215
    },
    {
      "epoch": 0.2256910569105691,
      "step": 1041,
      "training_loss": 5.617241382598877
    },
    {
      "epoch": 0.2256910569105691,
      "step": 1041,
      "training_loss": 6.784266471862793
    },
    {
      "epoch": 0.2256910569105691,
      "step": 1041,
      "training_loss": 7.283026218414307
    },
    {
      "epoch": 0.2256910569105691,
      "step": 1041,
      "training_loss": 7.685701847076416
    },
    {
      "epoch": 0.2259078590785908,
      "step": 1042,
      "training_loss": 7.1293230056762695
    },
    {
      "epoch": 0.2259078590785908,
      "step": 1042,
      "training_loss": 7.440011978149414
    },
    {
      "epoch": 0.2259078590785908,
      "step": 1042,
      "training_loss": 7.385231971740723
    },
    {
      "epoch": 0.2259078590785908,
      "step": 1042,
      "training_loss": 7.485172271728516
    },
    {
      "epoch": 0.22612466124661246,
      "step": 1043,
      "training_loss": 7.451851844787598
    },
    {
      "epoch": 0.22612466124661246,
      "step": 1043,
      "training_loss": 7.494245529174805
    },
    {
      "epoch": 0.22612466124661246,
      "step": 1043,
      "training_loss": 7.109187602996826
    },
    {
      "epoch": 0.22612466124661246,
      "step": 1043,
      "training_loss": 7.664866924285889
    },
    {
      "epoch": 0.22634146341463415,
      "grad_norm": 9.93511962890625,
      "learning_rate": 1e-05,
      "loss": 7.1703,
      "step": 1044
    },
    {
      "epoch": 0.22634146341463415,
      "step": 1044,
      "training_loss": 6.319580554962158
    },
    {
      "epoch": 0.22634146341463415,
      "step": 1044,
      "training_loss": 6.672450065612793
    },
    {
      "epoch": 0.22634146341463415,
      "step": 1044,
      "training_loss": 6.607071876525879
    },
    {
      "epoch": 0.22634146341463415,
      "step": 1044,
      "training_loss": 7.855536460876465
    },
    {
      "epoch": 0.22655826558265582,
      "step": 1045,
      "training_loss": 7.627927780151367
    },
    {
      "epoch": 0.22655826558265582,
      "step": 1045,
      "training_loss": 6.867918491363525
    },
    {
      "epoch": 0.22655826558265582,
      "step": 1045,
      "training_loss": 7.7895965576171875
    },
    {
      "epoch": 0.22655826558265582,
      "step": 1045,
      "training_loss": 7.668528079986572
    },
    {
      "epoch": 0.22677506775067752,
      "step": 1046,
      "training_loss": 7.69627571105957
    },
    {
      "epoch": 0.22677506775067752,
      "step": 1046,
      "training_loss": 6.788503170013428
    },
    {
      "epoch": 0.22677506775067752,
      "step": 1046,
      "training_loss": 5.816336154937744
    },
    {
      "epoch": 0.22677506775067752,
      "step": 1046,
      "training_loss": 7.832911491394043
    },
    {
      "epoch": 0.22699186991869919,
      "step": 1047,
      "training_loss": 7.302175998687744
    },
    {
      "epoch": 0.22699186991869919,
      "step": 1047,
      "training_loss": 6.334800720214844
    },
    {
      "epoch": 0.22699186991869919,
      "step": 1047,
      "training_loss": 7.404547214508057
    },
    {
      "epoch": 0.22699186991869919,
      "step": 1047,
      "training_loss": 7.6564531326293945
    },
    {
      "epoch": 0.22720867208672088,
      "grad_norm": 12.097260475158691,
      "learning_rate": 1e-05,
      "loss": 7.14,
      "step": 1048
    },
    {
      "epoch": 0.22720867208672088,
      "step": 1048,
      "training_loss": 6.3673624992370605
    },
    {
      "epoch": 0.22720867208672088,
      "step": 1048,
      "training_loss": 7.345746994018555
    },
    {
      "epoch": 0.22720867208672088,
      "step": 1048,
      "training_loss": 6.399808406829834
    },
    {
      "epoch": 0.22720867208672088,
      "step": 1048,
      "training_loss": 6.893328666687012
    },
    {
      "epoch": 0.22742547425474255,
      "step": 1049,
      "training_loss": 7.641597747802734
    },
    {
      "epoch": 0.22742547425474255,
      "step": 1049,
      "training_loss": 7.329206466674805
    },
    {
      "epoch": 0.22742547425474255,
      "step": 1049,
      "training_loss": 7.3025922775268555
    },
    {
      "epoch": 0.22742547425474255,
      "step": 1049,
      "training_loss": 7.347461700439453
    },
    {
      "epoch": 0.22764227642276422,
      "step": 1050,
      "training_loss": 6.975097179412842
    },
    {
      "epoch": 0.22764227642276422,
      "step": 1050,
      "training_loss": 7.418933868408203
    },
    {
      "epoch": 0.22764227642276422,
      "step": 1050,
      "training_loss": 8.513043403625488
    },
    {
      "epoch": 0.22764227642276422,
      "step": 1050,
      "training_loss": 7.2217535972595215
    },
    {
      "epoch": 0.2278590785907859,
      "step": 1051,
      "training_loss": 5.857076644897461
    },
    {
      "epoch": 0.2278590785907859,
      "step": 1051,
      "training_loss": 6.3144211769104
    },
    {
      "epoch": 0.2278590785907859,
      "step": 1051,
      "training_loss": 6.755739688873291
    },
    {
      "epoch": 0.2278590785907859,
      "step": 1051,
      "training_loss": 7.035030841827393
    },
    {
      "epoch": 0.22807588075880758,
      "grad_norm": 9.242719650268555,
      "learning_rate": 1e-05,
      "loss": 7.0449,
      "step": 1052
    },
    {
      "epoch": 0.22807588075880758,
      "step": 1052,
      "training_loss": 6.895745277404785
    },
    {
      "epoch": 0.22807588075880758,
      "step": 1052,
      "training_loss": 8.109973907470703
    },
    {
      "epoch": 0.22807588075880758,
      "step": 1052,
      "training_loss": 5.583008289337158
    },
    {
      "epoch": 0.22807588075880758,
      "step": 1052,
      "training_loss": 8.251558303833008
    },
    {
      "epoch": 0.22829268292682928,
      "step": 1053,
      "training_loss": 7.153377532958984
    },
    {
      "epoch": 0.22829268292682928,
      "step": 1053,
      "training_loss": 5.056821346282959
    },
    {
      "epoch": 0.22829268292682928,
      "step": 1053,
      "training_loss": 6.79836893081665
    },
    {
      "epoch": 0.22829268292682928,
      "step": 1053,
      "training_loss": 8.359689712524414
    },
    {
      "epoch": 0.22850948509485094,
      "step": 1054,
      "training_loss": 7.371549129486084
    },
    {
      "epoch": 0.22850948509485094,
      "step": 1054,
      "training_loss": 8.227720260620117
    },
    {
      "epoch": 0.22850948509485094,
      "step": 1054,
      "training_loss": 7.735574722290039
    },
    {
      "epoch": 0.22850948509485094,
      "step": 1054,
      "training_loss": 8.125937461853027
    },
    {
      "epoch": 0.22872628726287264,
      "step": 1055,
      "training_loss": 5.980463027954102
    },
    {
      "epoch": 0.22872628726287264,
      "step": 1055,
      "training_loss": 7.033767223358154
    },
    {
      "epoch": 0.22872628726287264,
      "step": 1055,
      "training_loss": 6.632011890411377
    },
    {
      "epoch": 0.22872628726287264,
      "step": 1055,
      "training_loss": 7.17477560043335
    },
    {
      "epoch": 0.2289430894308943,
      "grad_norm": 10.914666175842285,
      "learning_rate": 1e-05,
      "loss": 7.1556,
      "step": 1056
    },
    {
      "epoch": 0.2289430894308943,
      "step": 1056,
      "training_loss": 6.08601713180542
    },
    {
      "epoch": 0.2289430894308943,
      "step": 1056,
      "training_loss": 7.758199691772461
    },
    {
      "epoch": 0.2289430894308943,
      "step": 1056,
      "training_loss": 8.062204360961914
    },
    {
      "epoch": 0.2289430894308943,
      "step": 1056,
      "training_loss": 7.48990535736084
    },
    {
      "epoch": 0.22915989159891598,
      "step": 1057,
      "training_loss": 5.015381336212158
    },
    {
      "epoch": 0.22915989159891598,
      "step": 1057,
      "training_loss": 6.642941951751709
    },
    {
      "epoch": 0.22915989159891598,
      "step": 1057,
      "training_loss": 5.399835586547852
    },
    {
      "epoch": 0.22915989159891598,
      "step": 1057,
      "training_loss": 5.2063493728637695
    },
    {
      "epoch": 0.22937669376693767,
      "step": 1058,
      "training_loss": 7.959075450897217
    },
    {
      "epoch": 0.22937669376693767,
      "step": 1058,
      "training_loss": 7.305070400238037
    },
    {
      "epoch": 0.22937669376693767,
      "step": 1058,
      "training_loss": 7.004550933837891
    },
    {
      "epoch": 0.22937669376693767,
      "step": 1058,
      "training_loss": 5.697859764099121
    },
    {
      "epoch": 0.22959349593495934,
      "step": 1059,
      "training_loss": 8.242985725402832
    },
    {
      "epoch": 0.22959349593495934,
      "step": 1059,
      "training_loss": 5.253163814544678
    },
    {
      "epoch": 0.22959349593495934,
      "step": 1059,
      "training_loss": 6.840559005737305
    },
    {
      "epoch": 0.22959349593495934,
      "step": 1059,
      "training_loss": 6.0575852394104
    },
    {
      "epoch": 0.22981029810298104,
      "grad_norm": 10.77590560913086,
      "learning_rate": 1e-05,
      "loss": 6.6264,
      "step": 1060
    },
    {
      "epoch": 0.22981029810298104,
      "step": 1060,
      "training_loss": 7.509031772613525
    },
    {
      "epoch": 0.22981029810298104,
      "step": 1060,
      "training_loss": 8.184059143066406
    },
    {
      "epoch": 0.22981029810298104,
      "step": 1060,
      "training_loss": 6.0021891593933105
    },
    {
      "epoch": 0.22981029810298104,
      "step": 1060,
      "training_loss": 7.411652088165283
    },
    {
      "epoch": 0.2300271002710027,
      "step": 1061,
      "training_loss": 6.2971110343933105
    },
    {
      "epoch": 0.2300271002710027,
      "step": 1061,
      "training_loss": 6.750975131988525
    },
    {
      "epoch": 0.2300271002710027,
      "step": 1061,
      "training_loss": 6.984745979309082
    },
    {
      "epoch": 0.2300271002710027,
      "step": 1061,
      "training_loss": 5.132136821746826
    },
    {
      "epoch": 0.2302439024390244,
      "step": 1062,
      "training_loss": 7.457449436187744
    },
    {
      "epoch": 0.2302439024390244,
      "step": 1062,
      "training_loss": 7.116127967834473
    },
    {
      "epoch": 0.2302439024390244,
      "step": 1062,
      "training_loss": 7.153439521789551
    },
    {
      "epoch": 0.2302439024390244,
      "step": 1062,
      "training_loss": 7.290377140045166
    },
    {
      "epoch": 0.23046070460704607,
      "step": 1063,
      "training_loss": 7.057905673980713
    },
    {
      "epoch": 0.23046070460704607,
      "step": 1063,
      "training_loss": 6.842280387878418
    },
    {
      "epoch": 0.23046070460704607,
      "step": 1063,
      "training_loss": 7.3945770263671875
    },
    {
      "epoch": 0.23046070460704607,
      "step": 1063,
      "training_loss": 6.831722259521484
    },
    {
      "epoch": 0.23067750677506776,
      "grad_norm": 14.613655090332031,
      "learning_rate": 1e-05,
      "loss": 6.9635,
      "step": 1064
    },
    {
      "epoch": 0.23067750677506776,
      "step": 1064,
      "training_loss": 6.2669901847839355
    },
    {
      "epoch": 0.23067750677506776,
      "step": 1064,
      "training_loss": 6.934587001800537
    },
    {
      "epoch": 0.23067750677506776,
      "step": 1064,
      "training_loss": 7.670973777770996
    },
    {
      "epoch": 0.23067750677506776,
      "step": 1064,
      "training_loss": 7.986721515655518
    },
    {
      "epoch": 0.23089430894308943,
      "step": 1065,
      "training_loss": 7.8918046951293945
    },
    {
      "epoch": 0.23089430894308943,
      "step": 1065,
      "training_loss": 7.162020683288574
    },
    {
      "epoch": 0.23089430894308943,
      "step": 1065,
      "training_loss": 6.458258628845215
    },
    {
      "epoch": 0.23089430894308943,
      "step": 1065,
      "training_loss": 7.468177318572998
    },
    {
      "epoch": 0.2311111111111111,
      "step": 1066,
      "training_loss": 6.8539509773254395
    },
    {
      "epoch": 0.2311111111111111,
      "step": 1066,
      "training_loss": 7.622634410858154
    },
    {
      "epoch": 0.2311111111111111,
      "step": 1066,
      "training_loss": 7.5419206619262695
    },
    {
      "epoch": 0.2311111111111111,
      "step": 1066,
      "training_loss": 6.760746002197266
    },
    {
      "epoch": 0.2313279132791328,
      "step": 1067,
      "training_loss": 7.96550989151001
    },
    {
      "epoch": 0.2313279132791328,
      "step": 1067,
      "training_loss": 7.1270623207092285
    },
    {
      "epoch": 0.2313279132791328,
      "step": 1067,
      "training_loss": 5.981945037841797
    },
    {
      "epoch": 0.2313279132791328,
      "step": 1067,
      "training_loss": 6.8015594482421875
    },
    {
      "epoch": 0.23154471544715446,
      "grad_norm": 13.775136947631836,
      "learning_rate": 1e-05,
      "loss": 7.1559,
      "step": 1068
    },
    {
      "epoch": 0.23154471544715446,
      "step": 1068,
      "training_loss": 6.845040798187256
    },
    {
      "epoch": 0.23154471544715446,
      "step": 1068,
      "training_loss": 7.283699989318848
    },
    {
      "epoch": 0.23154471544715446,
      "step": 1068,
      "training_loss": 9.003326416015625
    },
    {
      "epoch": 0.23154471544715446,
      "step": 1068,
      "training_loss": 6.6761555671691895
    },
    {
      "epoch": 0.23176151761517616,
      "step": 1069,
      "training_loss": 6.57176399230957
    },
    {
      "epoch": 0.23176151761517616,
      "step": 1069,
      "training_loss": 7.327366352081299
    },
    {
      "epoch": 0.23176151761517616,
      "step": 1069,
      "training_loss": 7.818884372711182
    },
    {
      "epoch": 0.23176151761517616,
      "step": 1069,
      "training_loss": 8.759390830993652
    },
    {
      "epoch": 0.23197831978319783,
      "step": 1070,
      "training_loss": 6.509615898132324
    },
    {
      "epoch": 0.23197831978319783,
      "step": 1070,
      "training_loss": 6.089116096496582
    },
    {
      "epoch": 0.23197831978319783,
      "step": 1070,
      "training_loss": 6.983151912689209
    },
    {
      "epoch": 0.23197831978319783,
      "step": 1070,
      "training_loss": 4.655735492706299
    },
    {
      "epoch": 0.23219512195121952,
      "step": 1071,
      "training_loss": 6.780440330505371
    },
    {
      "epoch": 0.23219512195121952,
      "step": 1071,
      "training_loss": 7.716338157653809
    },
    {
      "epoch": 0.23219512195121952,
      "step": 1071,
      "training_loss": 5.1714396476745605
    },
    {
      "epoch": 0.23219512195121952,
      "step": 1071,
      "training_loss": 6.8689470291137695
    },
    {
      "epoch": 0.2324119241192412,
      "grad_norm": 8.392318725585938,
      "learning_rate": 1e-05,
      "loss": 6.9413,
      "step": 1072
    },
    {
      "epoch": 0.2324119241192412,
      "step": 1072,
      "training_loss": 5.517389297485352
    },
    {
      "epoch": 0.2324119241192412,
      "step": 1072,
      "training_loss": 8.977272987365723
    },
    {
      "epoch": 0.2324119241192412,
      "step": 1072,
      "training_loss": 6.809225559234619
    },
    {
      "epoch": 0.2324119241192412,
      "step": 1072,
      "training_loss": 7.463841915130615
    },
    {
      "epoch": 0.23262872628726286,
      "step": 1073,
      "training_loss": 6.447659015655518
    },
    {
      "epoch": 0.23262872628726286,
      "step": 1073,
      "training_loss": 7.50148344039917
    },
    {
      "epoch": 0.23262872628726286,
      "step": 1073,
      "training_loss": 5.934373378753662
    },
    {
      "epoch": 0.23262872628726286,
      "step": 1073,
      "training_loss": 6.661991596221924
    },
    {
      "epoch": 0.23284552845528456,
      "step": 1074,
      "training_loss": 7.052711009979248
    },
    {
      "epoch": 0.23284552845528456,
      "step": 1074,
      "training_loss": 7.569423198699951
    },
    {
      "epoch": 0.23284552845528456,
      "step": 1074,
      "training_loss": 7.451755046844482
    },
    {
      "epoch": 0.23284552845528456,
      "step": 1074,
      "training_loss": 7.64124870300293
    },
    {
      "epoch": 0.23306233062330622,
      "step": 1075,
      "training_loss": 4.964218616485596
    },
    {
      "epoch": 0.23306233062330622,
      "step": 1075,
      "training_loss": 6.876412391662598
    },
    {
      "epoch": 0.23306233062330622,
      "step": 1075,
      "training_loss": 5.77583646774292
    },
    {
      "epoch": 0.23306233062330622,
      "step": 1075,
      "training_loss": 7.141819477081299
    },
    {
      "epoch": 0.23327913279132792,
      "grad_norm": 9.439066886901855,
      "learning_rate": 1e-05,
      "loss": 6.8617,
      "step": 1076
    },
    {
      "epoch": 0.23327913279132792,
      "step": 1076,
      "training_loss": 4.808403968811035
    },
    {
      "epoch": 0.23327913279132792,
      "step": 1076,
      "training_loss": 7.526983261108398
    },
    {
      "epoch": 0.23327913279132792,
      "step": 1076,
      "training_loss": 9.496932029724121
    },
    {
      "epoch": 0.23327913279132792,
      "step": 1076,
      "training_loss": 8.357320785522461
    },
    {
      "epoch": 0.2334959349593496,
      "step": 1077,
      "training_loss": 7.576632022857666
    },
    {
      "epoch": 0.2334959349593496,
      "step": 1077,
      "training_loss": 7.371687889099121
    },
    {
      "epoch": 0.2334959349593496,
      "step": 1077,
      "training_loss": 7.517840385437012
    },
    {
      "epoch": 0.2334959349593496,
      "step": 1077,
      "training_loss": 6.106588840484619
    },
    {
      "epoch": 0.23371273712737128,
      "step": 1078,
      "training_loss": 6.547543048858643
    },
    {
      "epoch": 0.23371273712737128,
      "step": 1078,
      "training_loss": 6.974145889282227
    },
    {
      "epoch": 0.23371273712737128,
      "step": 1078,
      "training_loss": 6.242876052856445
    },
    {
      "epoch": 0.23371273712737128,
      "step": 1078,
      "training_loss": 6.707537651062012
    },
    {
      "epoch": 0.23392953929539295,
      "step": 1079,
      "training_loss": 7.446581840515137
    },
    {
      "epoch": 0.23392953929539295,
      "step": 1079,
      "training_loss": 6.468570709228516
    },
    {
      "epoch": 0.23392953929539295,
      "step": 1079,
      "training_loss": 7.283873081207275
    },
    {
      "epoch": 0.23392953929539295,
      "step": 1079,
      "training_loss": 7.450695991516113
    },
    {
      "epoch": 0.23414634146341465,
      "grad_norm": 9.613053321838379,
      "learning_rate": 1e-05,
      "loss": 7.1178,
      "step": 1080
    },
    {
      "epoch": 0.23414634146341465,
      "step": 1080,
      "training_loss": 7.3586225509643555
    },
    {
      "epoch": 0.23414634146341465,
      "step": 1080,
      "training_loss": 7.023218154907227
    },
    {
      "epoch": 0.23414634146341465,
      "step": 1080,
      "training_loss": 6.840522766113281
    },
    {
      "epoch": 0.23414634146341465,
      "step": 1080,
      "training_loss": 7.577470779418945
    },
    {
      "epoch": 0.23436314363143632,
      "step": 1081,
      "training_loss": 7.662054538726807
    },
    {
      "epoch": 0.23436314363143632,
      "step": 1081,
      "training_loss": 7.614591121673584
    },
    {
      "epoch": 0.23436314363143632,
      "step": 1081,
      "training_loss": 7.271722316741943
    },
    {
      "epoch": 0.23436314363143632,
      "step": 1081,
      "training_loss": 7.777010917663574
    },
    {
      "epoch": 0.23457994579945798,
      "step": 1082,
      "training_loss": 6.943894386291504
    },
    {
      "epoch": 0.23457994579945798,
      "step": 1082,
      "training_loss": 8.010992050170898
    },
    {
      "epoch": 0.23457994579945798,
      "step": 1082,
      "training_loss": 8.623173713684082
    },
    {
      "epoch": 0.23457994579945798,
      "step": 1082,
      "training_loss": 6.850984573364258
    },
    {
      "epoch": 0.23479674796747968,
      "step": 1083,
      "training_loss": 7.643616199493408
    },
    {
      "epoch": 0.23479674796747968,
      "step": 1083,
      "training_loss": 7.491645336151123
    },
    {
      "epoch": 0.23479674796747968,
      "step": 1083,
      "training_loss": 7.125601768493652
    },
    {
      "epoch": 0.23479674796747968,
      "step": 1083,
      "training_loss": 4.913458347320557
    },
    {
      "epoch": 0.23501355013550135,
      "grad_norm": 8.79106330871582,
      "learning_rate": 1e-05,
      "loss": 7.2955,
      "step": 1084
    },
    {
      "epoch": 0.23501355013550135,
      "step": 1084,
      "training_loss": 7.173964023590088
    },
    {
      "epoch": 0.23501355013550135,
      "step": 1084,
      "training_loss": 7.297435283660889
    },
    {
      "epoch": 0.23501355013550135,
      "step": 1084,
      "training_loss": 7.333782196044922
    },
    {
      "epoch": 0.23501355013550135,
      "step": 1084,
      "training_loss": 8.542112350463867
    },
    {
      "epoch": 0.23523035230352304,
      "step": 1085,
      "training_loss": 6.460133075714111
    },
    {
      "epoch": 0.23523035230352304,
      "step": 1085,
      "training_loss": 5.85720682144165
    },
    {
      "epoch": 0.23523035230352304,
      "step": 1085,
      "training_loss": 6.7959418296813965
    },
    {
      "epoch": 0.23523035230352304,
      "step": 1085,
      "training_loss": 6.539281368255615
    },
    {
      "epoch": 0.2354471544715447,
      "step": 1086,
      "training_loss": 7.20054817199707
    },
    {
      "epoch": 0.2354471544715447,
      "step": 1086,
      "training_loss": 7.184398651123047
    },
    {
      "epoch": 0.2354471544715447,
      "step": 1086,
      "training_loss": 5.156643390655518
    },
    {
      "epoch": 0.2354471544715447,
      "step": 1086,
      "training_loss": 7.109844207763672
    },
    {
      "epoch": 0.2356639566395664,
      "step": 1087,
      "training_loss": 6.975315570831299
    },
    {
      "epoch": 0.2356639566395664,
      "step": 1087,
      "training_loss": 7.874215126037598
    },
    {
      "epoch": 0.2356639566395664,
      "step": 1087,
      "training_loss": 7.840130805969238
    },
    {
      "epoch": 0.2356639566395664,
      "step": 1087,
      "training_loss": 5.93142557144165
    },
    {
      "epoch": 0.23588075880758808,
      "grad_norm": 7.796830177307129,
      "learning_rate": 1e-05,
      "loss": 6.9545,
      "step": 1088
    },
    {
      "epoch": 0.23588075880758808,
      "step": 1088,
      "training_loss": 6.182673454284668
    },
    {
      "epoch": 0.23588075880758808,
      "step": 1088,
      "training_loss": 6.938268661499023
    },
    {
      "epoch": 0.23588075880758808,
      "step": 1088,
      "training_loss": 7.1496076583862305
    },
    {
      "epoch": 0.23588075880758808,
      "step": 1088,
      "training_loss": 7.490072250366211
    },
    {
      "epoch": 0.23609756097560974,
      "step": 1089,
      "training_loss": 7.772456169128418
    },
    {
      "epoch": 0.23609756097560974,
      "step": 1089,
      "training_loss": 8.363109588623047
    },
    {
      "epoch": 0.23609756097560974,
      "step": 1089,
      "training_loss": 6.738237380981445
    },
    {
      "epoch": 0.23609756097560974,
      "step": 1089,
      "training_loss": 7.197934627532959
    },
    {
      "epoch": 0.23631436314363144,
      "step": 1090,
      "training_loss": 6.413341045379639
    },
    {
      "epoch": 0.23631436314363144,
      "step": 1090,
      "training_loss": 5.644072532653809
    },
    {
      "epoch": 0.23631436314363144,
      "step": 1090,
      "training_loss": 6.7704925537109375
    },
    {
      "epoch": 0.23631436314363144,
      "step": 1090,
      "training_loss": 6.999139785766602
    },
    {
      "epoch": 0.2365311653116531,
      "step": 1091,
      "training_loss": 5.939326763153076
    },
    {
      "epoch": 0.2365311653116531,
      "step": 1091,
      "training_loss": 7.018746376037598
    },
    {
      "epoch": 0.2365311653116531,
      "step": 1091,
      "training_loss": 6.948537349700928
    },
    {
      "epoch": 0.2365311653116531,
      "step": 1091,
      "training_loss": 7.390830039978027
    },
    {
      "epoch": 0.2367479674796748,
      "grad_norm": 10.31556224822998,
      "learning_rate": 1e-05,
      "loss": 6.9348,
      "step": 1092
    },
    {
      "epoch": 0.2367479674796748,
      "step": 1092,
      "training_loss": 6.706887245178223
    },
    {
      "epoch": 0.2367479674796748,
      "step": 1092,
      "training_loss": 6.8857269287109375
    },
    {
      "epoch": 0.2367479674796748,
      "step": 1092,
      "training_loss": 7.2255754470825195
    },
    {
      "epoch": 0.2367479674796748,
      "step": 1092,
      "training_loss": 6.717510223388672
    },
    {
      "epoch": 0.23696476964769647,
      "step": 1093,
      "training_loss": 8.182604789733887
    },
    {
      "epoch": 0.23696476964769647,
      "step": 1093,
      "training_loss": 7.68071174621582
    },
    {
      "epoch": 0.23696476964769647,
      "step": 1093,
      "training_loss": 7.467784404754639
    },
    {
      "epoch": 0.23696476964769647,
      "step": 1093,
      "training_loss": 7.960620880126953
    },
    {
      "epoch": 0.23718157181571817,
      "step": 1094,
      "training_loss": 6.501740455627441
    },
    {
      "epoch": 0.23718157181571817,
      "step": 1094,
      "training_loss": 7.283770561218262
    },
    {
      "epoch": 0.23718157181571817,
      "step": 1094,
      "training_loss": 7.546706199645996
    },
    {
      "epoch": 0.23718157181571817,
      "step": 1094,
      "training_loss": 6.932991981506348
    },
    {
      "epoch": 0.23739837398373984,
      "step": 1095,
      "training_loss": 6.975515365600586
    },
    {
      "epoch": 0.23739837398373984,
      "step": 1095,
      "training_loss": 6.783822059631348
    },
    {
      "epoch": 0.23739837398373984,
      "step": 1095,
      "training_loss": 5.917794227600098
    },
    {
      "epoch": 0.23739837398373984,
      "step": 1095,
      "training_loss": 8.070178031921387
    },
    {
      "epoch": 0.23761517615176153,
      "grad_norm": 8.253833770751953,
      "learning_rate": 1e-05,
      "loss": 7.1775,
      "step": 1096
    },
    {
      "epoch": 0.23761517615176153,
      "step": 1096,
      "training_loss": 6.7036356925964355
    },
    {
      "epoch": 0.23761517615176153,
      "step": 1096,
      "training_loss": 9.509157180786133
    },
    {
      "epoch": 0.23761517615176153,
      "step": 1096,
      "training_loss": 8.868484497070312
    },
    {
      "epoch": 0.23761517615176153,
      "step": 1096,
      "training_loss": 8.851401329040527
    },
    {
      "epoch": 0.2378319783197832,
      "step": 1097,
      "training_loss": 6.902122974395752
    },
    {
      "epoch": 0.2378319783197832,
      "step": 1097,
      "training_loss": 7.179004192352295
    },
    {
      "epoch": 0.2378319783197832,
      "step": 1097,
      "training_loss": 7.203049182891846
    },
    {
      "epoch": 0.2378319783197832,
      "step": 1097,
      "training_loss": 6.357303142547607
    },
    {
      "epoch": 0.23804878048780487,
      "step": 1098,
      "training_loss": 7.695252418518066
    },
    {
      "epoch": 0.23804878048780487,
      "step": 1098,
      "training_loss": 7.777548313140869
    },
    {
      "epoch": 0.23804878048780487,
      "step": 1098,
      "training_loss": 7.571182727813721
    },
    {
      "epoch": 0.23804878048780487,
      "step": 1098,
      "training_loss": 6.445879936218262
    },
    {
      "epoch": 0.23826558265582656,
      "step": 1099,
      "training_loss": 7.613519668579102
    },
    {
      "epoch": 0.23826558265582656,
      "step": 1099,
      "training_loss": 6.962215900421143
    },
    {
      "epoch": 0.23826558265582656,
      "step": 1099,
      "training_loss": 6.133123874664307
    },
    {
      "epoch": 0.23826558265582656,
      "step": 1099,
      "training_loss": 8.359917640686035
    },
    {
      "epoch": 0.23848238482384823,
      "grad_norm": 10.273412704467773,
      "learning_rate": 1e-05,
      "loss": 7.5083,
      "step": 1100
    },
    {
      "epoch": 0.23848238482384823,
      "step": 1100,
      "training_loss": 7.837104797363281
    },
    {
      "epoch": 0.23848238482384823,
      "step": 1100,
      "training_loss": 7.864569187164307
    },
    {
      "epoch": 0.23848238482384823,
      "step": 1100,
      "training_loss": 6.128785610198975
    },
    {
      "epoch": 0.23848238482384823,
      "step": 1100,
      "training_loss": 7.110869884490967
    },
    {
      "epoch": 0.23869918699186993,
      "step": 1101,
      "training_loss": 6.118786334991455
    },
    {
      "epoch": 0.23869918699186993,
      "step": 1101,
      "training_loss": 7.5314130783081055
    },
    {
      "epoch": 0.23869918699186993,
      "step": 1101,
      "training_loss": 6.923222541809082
    },
    {
      "epoch": 0.23869918699186993,
      "step": 1101,
      "training_loss": 6.041040897369385
    },
    {
      "epoch": 0.2389159891598916,
      "step": 1102,
      "training_loss": 7.838395118713379
    },
    {
      "epoch": 0.2389159891598916,
      "step": 1102,
      "training_loss": 7.815962791442871
    },
    {
      "epoch": 0.2389159891598916,
      "step": 1102,
      "training_loss": 7.22232723236084
    },
    {
      "epoch": 0.2389159891598916,
      "step": 1102,
      "training_loss": 6.440571308135986
    },
    {
      "epoch": 0.2391327913279133,
      "step": 1103,
      "training_loss": 6.829898834228516
    },
    {
      "epoch": 0.2391327913279133,
      "step": 1103,
      "training_loss": 7.788761138916016
    },
    {
      "epoch": 0.2391327913279133,
      "step": 1103,
      "training_loss": 7.312337398529053
    },
    {
      "epoch": 0.2391327913279133,
      "step": 1103,
      "training_loss": 7.062758445739746
    },
    {
      "epoch": 0.23934959349593496,
      "grad_norm": 10.478727340698242,
      "learning_rate": 1e-05,
      "loss": 7.1167,
      "step": 1104
    },
    {
      "epoch": 0.23934959349593496,
      "step": 1104,
      "training_loss": 7.387634754180908
    },
    {
      "epoch": 0.23934959349593496,
      "step": 1104,
      "training_loss": 7.007828235626221
    },
    {
      "epoch": 0.23934959349593496,
      "step": 1104,
      "training_loss": 7.022501468658447
    },
    {
      "epoch": 0.23934959349593496,
      "step": 1104,
      "training_loss": 8.141789436340332
    },
    {
      "epoch": 0.23956639566395663,
      "step": 1105,
      "training_loss": 8.145479202270508
    },
    {
      "epoch": 0.23956639566395663,
      "step": 1105,
      "training_loss": 7.367302417755127
    },
    {
      "epoch": 0.23956639566395663,
      "step": 1105,
      "training_loss": 6.961704730987549
    },
    {
      "epoch": 0.23956639566395663,
      "step": 1105,
      "training_loss": 7.268761157989502
    },
    {
      "epoch": 0.23978319783197832,
      "step": 1106,
      "training_loss": 7.133292198181152
    },
    {
      "epoch": 0.23978319783197832,
      "step": 1106,
      "training_loss": 8.854631423950195
    },
    {
      "epoch": 0.23978319783197832,
      "step": 1106,
      "training_loss": 5.868349552154541
    },
    {
      "epoch": 0.23978319783197832,
      "step": 1106,
      "training_loss": 6.791266918182373
    },
    {
      "epoch": 0.24,
      "step": 1107,
      "training_loss": 7.084886074066162
    },
    {
      "epoch": 0.24,
      "step": 1107,
      "training_loss": 8.723010063171387
    },
    {
      "epoch": 0.24,
      "step": 1107,
      "training_loss": 6.407963275909424
    },
    {
      "epoch": 0.24,
      "step": 1107,
      "training_loss": 6.553590297698975
    },
    {
      "epoch": 0.2402168021680217,
      "grad_norm": 17.42902946472168,
      "learning_rate": 1e-05,
      "loss": 7.295,
      "step": 1108
    },
    {
      "epoch": 0.2402168021680217,
      "step": 1108,
      "training_loss": 8.86971378326416
    },
    {
      "epoch": 0.2402168021680217,
      "step": 1108,
      "training_loss": 9.03045654296875
    },
    {
      "epoch": 0.2402168021680217,
      "step": 1108,
      "training_loss": 6.139096736907959
    },
    {
      "epoch": 0.2402168021680217,
      "step": 1108,
      "training_loss": 11.144896507263184
    },
    {
      "epoch": 0.24043360433604336,
      "step": 1109,
      "training_loss": 7.756535053253174
    },
    {
      "epoch": 0.24043360433604336,
      "step": 1109,
      "training_loss": 6.81001091003418
    },
    {
      "epoch": 0.24043360433604336,
      "step": 1109,
      "training_loss": 6.646246910095215
    },
    {
      "epoch": 0.24043360433604336,
      "step": 1109,
      "training_loss": 7.881695747375488
    },
    {
      "epoch": 0.24065040650406505,
      "step": 1110,
      "training_loss": 7.300922393798828
    },
    {
      "epoch": 0.24065040650406505,
      "step": 1110,
      "training_loss": 6.151089668273926
    },
    {
      "epoch": 0.24065040650406505,
      "step": 1110,
      "training_loss": 6.902618885040283
    },
    {
      "epoch": 0.24065040650406505,
      "step": 1110,
      "training_loss": 7.260714054107666
    },
    {
      "epoch": 0.24086720867208672,
      "step": 1111,
      "training_loss": 6.074488639831543
    },
    {
      "epoch": 0.24086720867208672,
      "step": 1111,
      "training_loss": 8.0690336227417
    },
    {
      "epoch": 0.24086720867208672,
      "step": 1111,
      "training_loss": 6.1732378005981445
    },
    {
      "epoch": 0.24086720867208672,
      "step": 1111,
      "training_loss": 5.205733299255371
    },
    {
      "epoch": 0.24108401084010841,
      "grad_norm": 12.11021900177002,
      "learning_rate": 1e-05,
      "loss": 7.3385,
      "step": 1112
    },
    {
      "epoch": 0.24108401084010841,
      "step": 1112,
      "training_loss": 6.859509468078613
    },
    {
      "epoch": 0.24108401084010841,
      "step": 1112,
      "training_loss": 7.189981460571289
    },
    {
      "epoch": 0.24108401084010841,
      "step": 1112,
      "training_loss": 5.786984443664551
    },
    {
      "epoch": 0.24108401084010841,
      "step": 1112,
      "training_loss": 7.397222995758057
    },
    {
      "epoch": 0.24130081300813008,
      "step": 1113,
      "training_loss": 7.497747898101807
    },
    {
      "epoch": 0.24130081300813008,
      "step": 1113,
      "training_loss": 8.272007942199707
    },
    {
      "epoch": 0.24130081300813008,
      "step": 1113,
      "training_loss": 7.611568927764893
    },
    {
      "epoch": 0.24130081300813008,
      "step": 1113,
      "training_loss": 7.451854228973389
    },
    {
      "epoch": 0.24151761517615175,
      "step": 1114,
      "training_loss": 7.055583477020264
    },
    {
      "epoch": 0.24151761517615175,
      "step": 1114,
      "training_loss": 7.863482475280762
    },
    {
      "epoch": 0.24151761517615175,
      "step": 1114,
      "training_loss": 7.29779577255249
    },
    {
      "epoch": 0.24151761517615175,
      "step": 1114,
      "training_loss": 6.202818870544434
    },
    {
      "epoch": 0.24173441734417345,
      "step": 1115,
      "training_loss": 7.964652061462402
    },
    {
      "epoch": 0.24173441734417345,
      "step": 1115,
      "training_loss": 7.111798286437988
    },
    {
      "epoch": 0.24173441734417345,
      "step": 1115,
      "training_loss": 7.046978950500488
    },
    {
      "epoch": 0.24173441734417345,
      "step": 1115,
      "training_loss": 7.631354808807373
    },
    {
      "epoch": 0.24195121951219511,
      "grad_norm": 9.776307106018066,
      "learning_rate": 1e-05,
      "loss": 7.2651,
      "step": 1116
    },
    {
      "epoch": 0.24195121951219511,
      "step": 1116,
      "training_loss": 7.081651210784912
    },
    {
      "epoch": 0.24195121951219511,
      "step": 1116,
      "training_loss": 6.910873889923096
    },
    {
      "epoch": 0.24195121951219511,
      "step": 1116,
      "training_loss": 8.817336082458496
    },
    {
      "epoch": 0.24195121951219511,
      "step": 1116,
      "training_loss": 6.902690410614014
    },
    {
      "epoch": 0.2421680216802168,
      "step": 1117,
      "training_loss": 6.952691078186035
    },
    {
      "epoch": 0.2421680216802168,
      "step": 1117,
      "training_loss": 7.0165510177612305
    },
    {
      "epoch": 0.2421680216802168,
      "step": 1117,
      "training_loss": 7.657864093780518
    },
    {
      "epoch": 0.2421680216802168,
      "step": 1117,
      "training_loss": 8.258048057556152
    },
    {
      "epoch": 0.24238482384823848,
      "step": 1118,
      "training_loss": 5.2499847412109375
    },
    {
      "epoch": 0.24238482384823848,
      "step": 1118,
      "training_loss": 8.84226131439209
    },
    {
      "epoch": 0.24238482384823848,
      "step": 1118,
      "training_loss": 6.827990531921387
    },
    {
      "epoch": 0.24238482384823848,
      "step": 1118,
      "training_loss": 6.969838619232178
    },
    {
      "epoch": 0.24260162601626017,
      "step": 1119,
      "training_loss": 7.397452354431152
    },
    {
      "epoch": 0.24260162601626017,
      "step": 1119,
      "training_loss": 6.739284992218018
    },
    {
      "epoch": 0.24260162601626017,
      "step": 1119,
      "training_loss": 6.4374518394470215
    },
    {
      "epoch": 0.24260162601626017,
      "step": 1119,
      "training_loss": 7.235106945037842
    },
    {
      "epoch": 0.24281842818428184,
      "grad_norm": 9.97748851776123,
      "learning_rate": 1e-05,
      "loss": 7.2061,
      "step": 1120
    },
    {
      "epoch": 0.24281842818428184,
      "step": 1120,
      "training_loss": 7.747211456298828
    },
    {
      "epoch": 0.24281842818428184,
      "step": 1120,
      "training_loss": 7.463890075683594
    },
    {
      "epoch": 0.24281842818428184,
      "step": 1120,
      "training_loss": 7.142908096313477
    },
    {
      "epoch": 0.24281842818428184,
      "step": 1120,
      "training_loss": 7.472138404846191
    },
    {
      "epoch": 0.2430352303523035,
      "step": 1121,
      "training_loss": 7.741088390350342
    },
    {
      "epoch": 0.2430352303523035,
      "step": 1121,
      "training_loss": 8.071581840515137
    },
    {
      "epoch": 0.2430352303523035,
      "step": 1121,
      "training_loss": 6.970482349395752
    },
    {
      "epoch": 0.2430352303523035,
      "step": 1121,
      "training_loss": 7.932971000671387
    },
    {
      "epoch": 0.2432520325203252,
      "step": 1122,
      "training_loss": 7.981753826141357
    },
    {
      "epoch": 0.2432520325203252,
      "step": 1122,
      "training_loss": 7.395490646362305
    },
    {
      "epoch": 0.2432520325203252,
      "step": 1122,
      "training_loss": 6.723923206329346
    },
    {
      "epoch": 0.2432520325203252,
      "step": 1122,
      "training_loss": 7.2320332527160645
    },
    {
      "epoch": 0.24346883468834687,
      "step": 1123,
      "training_loss": 6.999042510986328
    },
    {
      "epoch": 0.24346883468834687,
      "step": 1123,
      "training_loss": 7.415954113006592
    },
    {
      "epoch": 0.24346883468834687,
      "step": 1123,
      "training_loss": 6.993943691253662
    },
    {
      "epoch": 0.24346883468834687,
      "step": 1123,
      "training_loss": 7.559228897094727
    },
    {
      "epoch": 0.24368563685636857,
      "grad_norm": 11.001664161682129,
      "learning_rate": 1e-05,
      "loss": 7.4277,
      "step": 1124
    },
    {
      "epoch": 0.24368563685636857,
      "step": 1124,
      "training_loss": 7.23488712310791
    },
    {
      "epoch": 0.24368563685636857,
      "step": 1124,
      "training_loss": 7.027560234069824
    },
    {
      "epoch": 0.24368563685636857,
      "step": 1124,
      "training_loss": 7.145557880401611
    },
    {
      "epoch": 0.24368563685636857,
      "step": 1124,
      "training_loss": 7.09623384475708
    },
    {
      "epoch": 0.24390243902439024,
      "step": 1125,
      "training_loss": 7.6892170906066895
    },
    {
      "epoch": 0.24390243902439024,
      "step": 1125,
      "training_loss": 6.903112888336182
    },
    {
      "epoch": 0.24390243902439024,
      "step": 1125,
      "training_loss": 8.179741859436035
    },
    {
      "epoch": 0.24390243902439024,
      "step": 1125,
      "training_loss": 7.517232894897461
    },
    {
      "epoch": 0.24411924119241193,
      "step": 1126,
      "training_loss": 7.217609405517578
    },
    {
      "epoch": 0.24411924119241193,
      "step": 1126,
      "training_loss": 5.93771505355835
    },
    {
      "epoch": 0.24411924119241193,
      "step": 1126,
      "training_loss": 7.674633979797363
    },
    {
      "epoch": 0.24411924119241193,
      "step": 1126,
      "training_loss": 5.596259593963623
    },
    {
      "epoch": 0.2443360433604336,
      "step": 1127,
      "training_loss": 5.923689365386963
    },
    {
      "epoch": 0.2443360433604336,
      "step": 1127,
      "training_loss": 7.915530681610107
    },
    {
      "epoch": 0.2443360433604336,
      "step": 1127,
      "training_loss": 6.266429901123047
    },
    {
      "epoch": 0.2443360433604336,
      "step": 1127,
      "training_loss": 5.050105571746826
    },
    {
      "epoch": 0.2445528455284553,
      "grad_norm": 15.972588539123535,
      "learning_rate": 1e-05,
      "loss": 6.8985,
      "step": 1128
    },
    {
      "epoch": 0.2445528455284553,
      "step": 1128,
      "training_loss": 6.348333835601807
    },
    {
      "epoch": 0.2445528455284553,
      "step": 1128,
      "training_loss": 7.78695821762085
    },
    {
      "epoch": 0.2445528455284553,
      "step": 1128,
      "training_loss": 7.588552951812744
    },
    {
      "epoch": 0.2445528455284553,
      "step": 1128,
      "training_loss": 5.387616157531738
    },
    {
      "epoch": 0.24476964769647697,
      "step": 1129,
      "training_loss": 7.091767311096191
    },
    {
      "epoch": 0.24476964769647697,
      "step": 1129,
      "training_loss": 6.773248672485352
    },
    {
      "epoch": 0.24476964769647697,
      "step": 1129,
      "training_loss": 6.850849151611328
    },
    {
      "epoch": 0.24476964769647697,
      "step": 1129,
      "training_loss": 7.6522321701049805
    },
    {
      "epoch": 0.24498644986449863,
      "step": 1130,
      "training_loss": 5.444101333618164
    },
    {
      "epoch": 0.24498644986449863,
      "step": 1130,
      "training_loss": 6.200252056121826
    },
    {
      "epoch": 0.24498644986449863,
      "step": 1130,
      "training_loss": 7.116288185119629
    },
    {
      "epoch": 0.24498644986449863,
      "step": 1130,
      "training_loss": 6.438629627227783
    },
    {
      "epoch": 0.24520325203252033,
      "step": 1131,
      "training_loss": 7.565104961395264
    },
    {
      "epoch": 0.24520325203252033,
      "step": 1131,
      "training_loss": 7.404239177703857
    },
    {
      "epoch": 0.24520325203252033,
      "step": 1131,
      "training_loss": 7.34057092666626
    },
    {
      "epoch": 0.24520325203252033,
      "step": 1131,
      "training_loss": 6.69857931137085
    },
    {
      "epoch": 0.245420054200542,
      "grad_norm": 9.367098808288574,
      "learning_rate": 1e-05,
      "loss": 6.8555,
      "step": 1132
    },
    {
      "epoch": 0.245420054200542,
      "step": 1132,
      "training_loss": 7.002165794372559
    },
    {
      "epoch": 0.245420054200542,
      "step": 1132,
      "training_loss": 6.947790145874023
    },
    {
      "epoch": 0.245420054200542,
      "step": 1132,
      "training_loss": 5.140801429748535
    },
    {
      "epoch": 0.245420054200542,
      "step": 1132,
      "training_loss": 6.022055625915527
    },
    {
      "epoch": 0.2456368563685637,
      "step": 1133,
      "training_loss": 7.279783725738525
    },
    {
      "epoch": 0.2456368563685637,
      "step": 1133,
      "training_loss": 6.91513729095459
    },
    {
      "epoch": 0.2456368563685637,
      "step": 1133,
      "training_loss": 7.324364185333252
    },
    {
      "epoch": 0.2456368563685637,
      "step": 1133,
      "training_loss": 6.624607086181641
    },
    {
      "epoch": 0.24585365853658536,
      "step": 1134,
      "training_loss": 7.415003776550293
    },
    {
      "epoch": 0.24585365853658536,
      "step": 1134,
      "training_loss": 7.767943859100342
    },
    {
      "epoch": 0.24585365853658536,
      "step": 1134,
      "training_loss": 7.033409595489502
    },
    {
      "epoch": 0.24585365853658536,
      "step": 1134,
      "training_loss": 7.40108585357666
    },
    {
      "epoch": 0.24607046070460706,
      "step": 1135,
      "training_loss": 7.511155128479004
    },
    {
      "epoch": 0.24607046070460706,
      "step": 1135,
      "training_loss": 5.5691304206848145
    },
    {
      "epoch": 0.24607046070460706,
      "step": 1135,
      "training_loss": 6.379209041595459
    },
    {
      "epoch": 0.24607046070460706,
      "step": 1135,
      "training_loss": 7.140615940093994
    },
    {
      "epoch": 0.24628726287262873,
      "grad_norm": 14.256927490234375,
      "learning_rate": 1e-05,
      "loss": 6.8421,
      "step": 1136
    },
    {
      "epoch": 0.24628726287262873,
      "step": 1136,
      "training_loss": 4.5922627449035645
    },
    {
      "epoch": 0.24628726287262873,
      "step": 1136,
      "training_loss": 8.785099029541016
    },
    {
      "epoch": 0.24628726287262873,
      "step": 1136,
      "training_loss": 5.730871200561523
    },
    {
      "epoch": 0.24628726287262873,
      "step": 1136,
      "training_loss": 7.395756244659424
    },
    {
      "epoch": 0.2465040650406504,
      "step": 1137,
      "training_loss": 7.6411614418029785
    },
    {
      "epoch": 0.2465040650406504,
      "step": 1137,
      "training_loss": 8.765597343444824
    },
    {
      "epoch": 0.2465040650406504,
      "step": 1137,
      "training_loss": 7.964103698730469
    },
    {
      "epoch": 0.2465040650406504,
      "step": 1137,
      "training_loss": 7.796679973602295
    },
    {
      "epoch": 0.2467208672086721,
      "step": 1138,
      "training_loss": 6.668231964111328
    },
    {
      "epoch": 0.2467208672086721,
      "step": 1138,
      "training_loss": 6.6248955726623535
    },
    {
      "epoch": 0.2467208672086721,
      "step": 1138,
      "training_loss": 7.353137493133545
    },
    {
      "epoch": 0.2467208672086721,
      "step": 1138,
      "training_loss": 7.2922043800354
    },
    {
      "epoch": 0.24693766937669376,
      "step": 1139,
      "training_loss": 7.626474380493164
    },
    {
      "epoch": 0.24693766937669376,
      "step": 1139,
      "training_loss": 7.110000133514404
    },
    {
      "epoch": 0.24693766937669376,
      "step": 1139,
      "training_loss": 7.288928031921387
    },
    {
      "epoch": 0.24693766937669376,
      "step": 1139,
      "training_loss": 7.043907165527344
    },
    {
      "epoch": 0.24715447154471545,
      "grad_norm": 8.873779296875,
      "learning_rate": 1e-05,
      "loss": 7.23,
      "step": 1140
    },
    {
      "epoch": 0.24715447154471545,
      "step": 1140,
      "training_loss": 7.286092758178711
    },
    {
      "epoch": 0.24715447154471545,
      "step": 1140,
      "training_loss": 6.954174995422363
    },
    {
      "epoch": 0.24715447154471545,
      "step": 1140,
      "training_loss": 7.6156229972839355
    },
    {
      "epoch": 0.24715447154471545,
      "step": 1140,
      "training_loss": 7.50462532043457
    },
    {
      "epoch": 0.24737127371273712,
      "step": 1141,
      "training_loss": 6.891541004180908
    },
    {
      "epoch": 0.24737127371273712,
      "step": 1141,
      "training_loss": 7.732840538024902
    },
    {
      "epoch": 0.24737127371273712,
      "step": 1141,
      "training_loss": 5.010836124420166
    },
    {
      "epoch": 0.24737127371273712,
      "step": 1141,
      "training_loss": 6.993682384490967
    },
    {
      "epoch": 0.24758807588075882,
      "step": 1142,
      "training_loss": 6.211600303649902
    },
    {
      "epoch": 0.24758807588075882,
      "step": 1142,
      "training_loss": 6.3390302658081055
    },
    {
      "epoch": 0.24758807588075882,
      "step": 1142,
      "training_loss": 6.563206195831299
    },
    {
      "epoch": 0.24758807588075882,
      "step": 1142,
      "training_loss": 6.9002885818481445
    },
    {
      "epoch": 0.24780487804878049,
      "step": 1143,
      "training_loss": 6.558388710021973
    },
    {
      "epoch": 0.24780487804878049,
      "step": 1143,
      "training_loss": 6.788573741912842
    },
    {
      "epoch": 0.24780487804878049,
      "step": 1143,
      "training_loss": 5.072666168212891
    },
    {
      "epoch": 0.24780487804878049,
      "step": 1143,
      "training_loss": 6.560475826263428
    },
    {
      "epoch": 0.24802168021680218,
      "grad_norm": 13.835308074951172,
      "learning_rate": 1e-05,
      "loss": 6.6865,
      "step": 1144
    },
    {
      "epoch": 0.24802168021680218,
      "step": 1144,
      "training_loss": 6.744873046875
    },
    {
      "epoch": 0.24802168021680218,
      "step": 1144,
      "training_loss": 8.118261337280273
    },
    {
      "epoch": 0.24802168021680218,
      "step": 1144,
      "training_loss": 6.340214252471924
    },
    {
      "epoch": 0.24802168021680218,
      "step": 1144,
      "training_loss": 7.968196868896484
    },
    {
      "epoch": 0.24823848238482385,
      "step": 1145,
      "training_loss": 6.053826808929443
    },
    {
      "epoch": 0.24823848238482385,
      "step": 1145,
      "training_loss": 7.1634135246276855
    },
    {
      "epoch": 0.24823848238482385,
      "step": 1145,
      "training_loss": 5.703420162200928
    },
    {
      "epoch": 0.24823848238482385,
      "step": 1145,
      "training_loss": 7.777592182159424
    },
    {
      "epoch": 0.24845528455284552,
      "step": 1146,
      "training_loss": 9.033265113830566
    },
    {
      "epoch": 0.24845528455284552,
      "step": 1146,
      "training_loss": 7.376434803009033
    },
    {
      "epoch": 0.24845528455284552,
      "step": 1146,
      "training_loss": 6.554478168487549
    },
    {
      "epoch": 0.24845528455284552,
      "step": 1146,
      "training_loss": 6.756682395935059
    },
    {
      "epoch": 0.2486720867208672,
      "step": 1147,
      "training_loss": 7.179178237915039
    },
    {
      "epoch": 0.2486720867208672,
      "step": 1147,
      "training_loss": 7.439974308013916
    },
    {
      "epoch": 0.2486720867208672,
      "step": 1147,
      "training_loss": 5.618058204650879
    },
    {
      "epoch": 0.2486720867208672,
      "step": 1147,
      "training_loss": 6.74072790145874
    },
    {
      "epoch": 0.24888888888888888,
      "grad_norm": 10.661195755004883,
      "learning_rate": 1e-05,
      "loss": 7.0355,
      "step": 1148
    },
    {
      "epoch": 0.24888888888888888,
      "step": 1148,
      "training_loss": 7.415238380432129
    },
    {
      "epoch": 0.24888888888888888,
      "step": 1148,
      "training_loss": 6.607497692108154
    },
    {
      "epoch": 0.24888888888888888,
      "step": 1148,
      "training_loss": 7.08932638168335
    },
    {
      "epoch": 0.24888888888888888,
      "step": 1148,
      "training_loss": 6.9643635749816895
    },
    {
      "epoch": 0.24910569105691058,
      "step": 1149,
      "training_loss": 7.01400089263916
    },
    {
      "epoch": 0.24910569105691058,
      "step": 1149,
      "training_loss": 6.17967414855957
    },
    {
      "epoch": 0.24910569105691058,
      "step": 1149,
      "training_loss": 8.01211929321289
    },
    {
      "epoch": 0.24910569105691058,
      "step": 1149,
      "training_loss": 5.8196539878845215
    },
    {
      "epoch": 0.24932249322493225,
      "step": 1150,
      "training_loss": 6.727422714233398
    },
    {
      "epoch": 0.24932249322493225,
      "step": 1150,
      "training_loss": 7.999715328216553
    },
    {
      "epoch": 0.24932249322493225,
      "step": 1150,
      "training_loss": 7.391567707061768
    },
    {
      "epoch": 0.24932249322493225,
      "step": 1150,
      "training_loss": 6.365152359008789
    },
    {
      "epoch": 0.24953929539295394,
      "step": 1151,
      "training_loss": 6.841097354888916
    },
    {
      "epoch": 0.24953929539295394,
      "step": 1151,
      "training_loss": 6.818741321563721
    },
    {
      "epoch": 0.24953929539295394,
      "step": 1151,
      "training_loss": 6.938418865203857
    },
    {
      "epoch": 0.24953929539295394,
      "step": 1151,
      "training_loss": 6.963674545288086
    },
    {
      "epoch": 0.2497560975609756,
      "grad_norm": 11.21865177154541,
      "learning_rate": 1e-05,
      "loss": 6.9467,
      "step": 1152
    },
    {
      "epoch": 0.2497560975609756,
      "step": 1152,
      "training_loss": 7.4520583152771
    },
    {
      "epoch": 0.2497560975609756,
      "step": 1152,
      "training_loss": 7.823613166809082
    },
    {
      "epoch": 0.2497560975609756,
      "step": 1152,
      "training_loss": 5.404146194458008
    },
    {
      "epoch": 0.2497560975609756,
      "step": 1152,
      "training_loss": 7.487601280212402
    },
    {
      "epoch": 0.24997289972899728,
      "step": 1153,
      "training_loss": 7.365325927734375
    },
    {
      "epoch": 0.24997289972899728,
      "step": 1153,
      "training_loss": 7.028332710266113
    },
    {
      "epoch": 0.24997289972899728,
      "step": 1153,
      "training_loss": 5.7311601638793945
    },
    {
      "epoch": 0.24997289972899728,
      "step": 1153,
      "training_loss": 5.662105083465576
    },
    {
      "epoch": 0.25018970189701895,
      "step": 1154,
      "training_loss": 6.568940162658691
    },
    {
      "epoch": 0.25018970189701895,
      "step": 1154,
      "training_loss": 6.9440765380859375
    },
    {
      "epoch": 0.25018970189701895,
      "step": 1154,
      "training_loss": 5.710484027862549
    },
    {
      "epoch": 0.25018970189701895,
      "step": 1154,
      "training_loss": 7.435183525085449
    },
    {
      "epoch": 0.25040650406504067,
      "step": 1155,
      "training_loss": 6.2931599617004395
    },
    {
      "epoch": 0.25040650406504067,
      "step": 1155,
      "training_loss": 7.414081573486328
    },
    {
      "epoch": 0.25040650406504067,
      "step": 1155,
      "training_loss": 9.005419731140137
    },
    {
      "epoch": 0.25040650406504067,
      "step": 1155,
      "training_loss": 5.952235698699951
    },
    {
      "epoch": 0.25062330623306234,
      "grad_norm": 14.655587196350098,
      "learning_rate": 1e-05,
      "loss": 6.8299,
      "step": 1156
    },
    {
      "epoch": 0.25062330623306234,
      "step": 1156,
      "training_loss": 7.423302173614502
    },
    {
      "epoch": 0.25062330623306234,
      "step": 1156,
      "training_loss": 7.1964263916015625
    },
    {
      "epoch": 0.25062330623306234,
      "step": 1156,
      "training_loss": 7.204663276672363
    },
    {
      "epoch": 0.25062330623306234,
      "step": 1156,
      "training_loss": 7.223381519317627
    },
    {
      "epoch": 0.250840108401084,
      "step": 1157,
      "training_loss": 6.98484468460083
    },
    {
      "epoch": 0.250840108401084,
      "step": 1157,
      "training_loss": 7.008689880371094
    },
    {
      "epoch": 0.250840108401084,
      "step": 1157,
      "training_loss": 5.015990257263184
    },
    {
      "epoch": 0.250840108401084,
      "step": 1157,
      "training_loss": 6.821047782897949
    },
    {
      "epoch": 0.2510569105691057,
      "step": 1158,
      "training_loss": 7.089737415313721
    },
    {
      "epoch": 0.2510569105691057,
      "step": 1158,
      "training_loss": 6.383350372314453
    },
    {
      "epoch": 0.2510569105691057,
      "step": 1158,
      "training_loss": 7.270363807678223
    },
    {
      "epoch": 0.2510569105691057,
      "step": 1158,
      "training_loss": 7.178927421569824
    },
    {
      "epoch": 0.2512737127371274,
      "step": 1159,
      "training_loss": 7.80311393737793
    },
    {
      "epoch": 0.2512737127371274,
      "step": 1159,
      "training_loss": 6.509444236755371
    },
    {
      "epoch": 0.2512737127371274,
      "step": 1159,
      "training_loss": 6.180529594421387
    },
    {
      "epoch": 0.2512737127371274,
      "step": 1159,
      "training_loss": 7.599409580230713
    },
    {
      "epoch": 0.25149051490514907,
      "grad_norm": 11.047155380249023,
      "learning_rate": 1e-05,
      "loss": 6.9308,
      "step": 1160
    },
    {
      "epoch": 0.25149051490514907,
      "step": 1160,
      "training_loss": 7.117722511291504
    },
    {
      "epoch": 0.25149051490514907,
      "step": 1160,
      "training_loss": 7.501136302947998
    },
    {
      "epoch": 0.25149051490514907,
      "step": 1160,
      "training_loss": 6.803651809692383
    },
    {
      "epoch": 0.25149051490514907,
      "step": 1160,
      "training_loss": 7.442080974578857
    },
    {
      "epoch": 0.25170731707317073,
      "step": 1161,
      "training_loss": 8.038607597351074
    },
    {
      "epoch": 0.25170731707317073,
      "step": 1161,
      "training_loss": 7.1581010818481445
    },
    {
      "epoch": 0.25170731707317073,
      "step": 1161,
      "training_loss": 6.982692241668701
    },
    {
      "epoch": 0.25170731707317073,
      "step": 1161,
      "training_loss": 7.1746087074279785
    },
    {
      "epoch": 0.2519241192411924,
      "step": 1162,
      "training_loss": 7.324269771575928
    },
    {
      "epoch": 0.2519241192411924,
      "step": 1162,
      "training_loss": 7.806074619293213
    },
    {
      "epoch": 0.2519241192411924,
      "step": 1162,
      "training_loss": 7.207949161529541
    },
    {
      "epoch": 0.2519241192411924,
      "step": 1162,
      "training_loss": 6.2532782554626465
    },
    {
      "epoch": 0.25214092140921407,
      "step": 1163,
      "training_loss": 7.651294231414795
    },
    {
      "epoch": 0.25214092140921407,
      "step": 1163,
      "training_loss": 5.190150737762451
    },
    {
      "epoch": 0.25214092140921407,
      "step": 1163,
      "training_loss": 5.7953619956970215
    },
    {
      "epoch": 0.25214092140921407,
      "step": 1163,
      "training_loss": 7.803294658660889
    },
    {
      "epoch": 0.2523577235772358,
      "grad_norm": 12.522839546203613,
      "learning_rate": 1e-05,
      "loss": 7.0781,
      "step": 1164
    },
    {
      "epoch": 0.2523577235772358,
      "step": 1164,
      "training_loss": 7.4406914710998535
    },
    {
      "epoch": 0.2523577235772358,
      "step": 1164,
      "training_loss": 4.573291778564453
    },
    {
      "epoch": 0.2523577235772358,
      "step": 1164,
      "training_loss": 6.900152683258057
    },
    {
      "epoch": 0.2523577235772358,
      "step": 1164,
      "training_loss": 6.560080051422119
    },
    {
      "epoch": 0.25257452574525746,
      "step": 1165,
      "training_loss": 5.820485591888428
    },
    {
      "epoch": 0.25257452574525746,
      "step": 1165,
      "training_loss": 6.599278926849365
    },
    {
      "epoch": 0.25257452574525746,
      "step": 1165,
      "training_loss": 6.974061489105225
    },
    {
      "epoch": 0.25257452574525746,
      "step": 1165,
      "training_loss": 7.717818260192871
    },
    {
      "epoch": 0.25279132791327913,
      "step": 1166,
      "training_loss": 7.473382472991943
    },
    {
      "epoch": 0.25279132791327913,
      "step": 1166,
      "training_loss": 7.549659252166748
    },
    {
      "epoch": 0.25279132791327913,
      "step": 1166,
      "training_loss": 5.05867338180542
    },
    {
      "epoch": 0.25279132791327913,
      "step": 1166,
      "training_loss": 6.784838676452637
    },
    {
      "epoch": 0.2530081300813008,
      "step": 1167,
      "training_loss": 7.417977333068848
    },
    {
      "epoch": 0.2530081300813008,
      "step": 1167,
      "training_loss": 6.3143839836120605
    },
    {
      "epoch": 0.2530081300813008,
      "step": 1167,
      "training_loss": 7.330992698669434
    },
    {
      "epoch": 0.2530081300813008,
      "step": 1167,
      "training_loss": 7.574884414672852
    },
    {
      "epoch": 0.2532249322493225,
      "grad_norm": 9.691932678222656,
      "learning_rate": 1e-05,
      "loss": 6.7557,
      "step": 1168
    },
    {
      "epoch": 0.2532249322493225,
      "step": 1168,
      "training_loss": 6.77662992477417
    },
    {
      "epoch": 0.2532249322493225,
      "step": 1168,
      "training_loss": 7.4967803955078125
    },
    {
      "epoch": 0.2532249322493225,
      "step": 1168,
      "training_loss": 6.533064365386963
    },
    {
      "epoch": 0.2532249322493225,
      "step": 1168,
      "training_loss": 7.251468181610107
    },
    {
      "epoch": 0.2534417344173442,
      "step": 1169,
      "training_loss": 6.018828392028809
    },
    {
      "epoch": 0.2534417344173442,
      "step": 1169,
      "training_loss": 7.261981010437012
    },
    {
      "epoch": 0.2534417344173442,
      "step": 1169,
      "training_loss": 7.590882778167725
    },
    {
      "epoch": 0.2534417344173442,
      "step": 1169,
      "training_loss": 7.463891983032227
    },
    {
      "epoch": 0.25365853658536586,
      "step": 1170,
      "training_loss": 6.906278610229492
    },
    {
      "epoch": 0.25365853658536586,
      "step": 1170,
      "training_loss": 6.782890796661377
    },
    {
      "epoch": 0.25365853658536586,
      "step": 1170,
      "training_loss": 5.343433380126953
    },
    {
      "epoch": 0.25365853658536586,
      "step": 1170,
      "training_loss": 7.055598735809326
    },
    {
      "epoch": 0.2538753387533875,
      "step": 1171,
      "training_loss": 6.450456619262695
    },
    {
      "epoch": 0.2538753387533875,
      "step": 1171,
      "training_loss": 7.461159706115723
    },
    {
      "epoch": 0.2538753387533875,
      "step": 1171,
      "training_loss": 7.000478744506836
    },
    {
      "epoch": 0.2538753387533875,
      "step": 1171,
      "training_loss": 7.56417989730835
    },
    {
      "epoch": 0.2540921409214092,
      "grad_norm": 12.962100982666016,
      "learning_rate": 1e-05,
      "loss": 6.9349,
      "step": 1172
    },
    {
      "epoch": 0.2540921409214092,
      "step": 1172,
      "training_loss": 6.13246488571167
    },
    {
      "epoch": 0.2540921409214092,
      "step": 1172,
      "training_loss": 7.09539270401001
    },
    {
      "epoch": 0.2540921409214092,
      "step": 1172,
      "training_loss": 6.872598648071289
    },
    {
      "epoch": 0.2540921409214092,
      "step": 1172,
      "training_loss": 6.749472141265869
    },
    {
      "epoch": 0.2543089430894309,
      "step": 1173,
      "training_loss": 5.364550590515137
    },
    {
      "epoch": 0.2543089430894309,
      "step": 1173,
      "training_loss": 5.177147388458252
    },
    {
      "epoch": 0.2543089430894309,
      "step": 1173,
      "training_loss": 6.134965896606445
    },
    {
      "epoch": 0.2543089430894309,
      "step": 1173,
      "training_loss": 6.518931865692139
    },
    {
      "epoch": 0.2545257452574526,
      "step": 1174,
      "training_loss": 5.903183937072754
    },
    {
      "epoch": 0.2545257452574526,
      "step": 1174,
      "training_loss": 7.078738689422607
    },
    {
      "epoch": 0.2545257452574526,
      "step": 1174,
      "training_loss": 7.381341934204102
    },
    {
      "epoch": 0.2545257452574526,
      "step": 1174,
      "training_loss": 5.422458648681641
    },
    {
      "epoch": 0.25474254742547425,
      "step": 1175,
      "training_loss": 7.336745738983154
    },
    {
      "epoch": 0.25474254742547425,
      "step": 1175,
      "training_loss": 6.6642961502075195
    },
    {
      "epoch": 0.25474254742547425,
      "step": 1175,
      "training_loss": 7.424095630645752
    },
    {
      "epoch": 0.25474254742547425,
      "step": 1175,
      "training_loss": 6.793268203735352
    },
    {
      "epoch": 0.2549593495934959,
      "grad_norm": 10.94908618927002,
      "learning_rate": 1e-05,
      "loss": 6.5031,
      "step": 1176
    },
    {
      "epoch": 0.2549593495934959,
      "step": 1176,
      "training_loss": 6.43558931350708
    },
    {
      "epoch": 0.2549593495934959,
      "step": 1176,
      "training_loss": 6.163074970245361
    },
    {
      "epoch": 0.2549593495934959,
      "step": 1176,
      "training_loss": 6.519376754760742
    },
    {
      "epoch": 0.2549593495934959,
      "step": 1176,
      "training_loss": 5.832204818725586
    },
    {
      "epoch": 0.2551761517615176,
      "step": 1177,
      "training_loss": 6.510299205780029
    },
    {
      "epoch": 0.2551761517615176,
      "step": 1177,
      "training_loss": 4.278517246246338
    },
    {
      "epoch": 0.2551761517615176,
      "step": 1177,
      "training_loss": 7.555248260498047
    },
    {
      "epoch": 0.2551761517615176,
      "step": 1177,
      "training_loss": 8.092315673828125
    },
    {
      "epoch": 0.2553929539295393,
      "step": 1178,
      "training_loss": 8.844780921936035
    },
    {
      "epoch": 0.2553929539295393,
      "step": 1178,
      "training_loss": 7.8191633224487305
    },
    {
      "epoch": 0.2553929539295393,
      "step": 1178,
      "training_loss": 8.341814994812012
    },
    {
      "epoch": 0.2553929539295393,
      "step": 1178,
      "training_loss": 7.23590612411499
    },
    {
      "epoch": 0.255609756097561,
      "step": 1179,
      "training_loss": 7.56386137008667
    },
    {
      "epoch": 0.255609756097561,
      "step": 1179,
      "training_loss": 7.186968803405762
    },
    {
      "epoch": 0.255609756097561,
      "step": 1179,
      "training_loss": 7.001454830169678
    },
    {
      "epoch": 0.255609756097561,
      "step": 1179,
      "training_loss": 6.793973445892334
    },
    {
      "epoch": 0.25582655826558265,
      "grad_norm": 9.312244415283203,
      "learning_rate": 1e-05,
      "loss": 7.0109,
      "step": 1180
    },
    {
      "epoch": 0.25582655826558265,
      "step": 1180,
      "training_loss": 5.4906158447265625
    },
    {
      "epoch": 0.25582655826558265,
      "step": 1180,
      "training_loss": 7.540551662445068
    },
    {
      "epoch": 0.25582655826558265,
      "step": 1180,
      "training_loss": 7.7183074951171875
    },
    {
      "epoch": 0.25582655826558265,
      "step": 1180,
      "training_loss": 7.339768409729004
    },
    {
      "epoch": 0.2560433604336043,
      "step": 1181,
      "training_loss": 6.320673942565918
    },
    {
      "epoch": 0.2560433604336043,
      "step": 1181,
      "training_loss": 8.197400093078613
    },
    {
      "epoch": 0.2560433604336043,
      "step": 1181,
      "training_loss": 5.233614921569824
    },
    {
      "epoch": 0.2560433604336043,
      "step": 1181,
      "training_loss": 7.218170166015625
    },
    {
      "epoch": 0.25626016260162604,
      "step": 1182,
      "training_loss": 4.234715461730957
    },
    {
      "epoch": 0.25626016260162604,
      "step": 1182,
      "training_loss": 7.325976371765137
    },
    {
      "epoch": 0.25626016260162604,
      "step": 1182,
      "training_loss": 5.98531436920166
    },
    {
      "epoch": 0.25626016260162604,
      "step": 1182,
      "training_loss": 5.788468837738037
    },
    {
      "epoch": 0.2564769647696477,
      "step": 1183,
      "training_loss": 7.40206241607666
    },
    {
      "epoch": 0.2564769647696477,
      "step": 1183,
      "training_loss": 7.339653015136719
    },
    {
      "epoch": 0.2564769647696477,
      "step": 1183,
      "training_loss": 6.313889026641846
    },
    {
      "epoch": 0.2564769647696477,
      "step": 1183,
      "training_loss": 6.6097846031188965
    },
    {
      "epoch": 0.2566937669376694,
      "grad_norm": 8.467920303344727,
      "learning_rate": 1e-05,
      "loss": 6.6287,
      "step": 1184
    },
    {
      "epoch": 0.2566937669376694,
      "step": 1184,
      "training_loss": 5.63134765625
    },
    {
      "epoch": 0.2566937669376694,
      "step": 1184,
      "training_loss": 5.8272318840026855
    },
    {
      "epoch": 0.2566937669376694,
      "step": 1184,
      "training_loss": 5.162258148193359
    },
    {
      "epoch": 0.2566937669376694,
      "step": 1184,
      "training_loss": 8.674830436706543
    },
    {
      "epoch": 0.25691056910569104,
      "step": 1185,
      "training_loss": 6.109567165374756
    },
    {
      "epoch": 0.25691056910569104,
      "step": 1185,
      "training_loss": 6.421520709991455
    },
    {
      "epoch": 0.25691056910569104,
      "step": 1185,
      "training_loss": 8.160548210144043
    },
    {
      "epoch": 0.25691056910569104,
      "step": 1185,
      "training_loss": 7.455776691436768
    },
    {
      "epoch": 0.2571273712737127,
      "step": 1186,
      "training_loss": 6.420004367828369
    },
    {
      "epoch": 0.2571273712737127,
      "step": 1186,
      "training_loss": 7.29403829574585
    },
    {
      "epoch": 0.2571273712737127,
      "step": 1186,
      "training_loss": 5.6046013832092285
    },
    {
      "epoch": 0.2571273712737127,
      "step": 1186,
      "training_loss": 7.489034175872803
    },
    {
      "epoch": 0.25734417344173444,
      "step": 1187,
      "training_loss": 7.136376857757568
    },
    {
      "epoch": 0.25734417344173444,
      "step": 1187,
      "training_loss": 6.963831424713135
    },
    {
      "epoch": 0.25734417344173444,
      "step": 1187,
      "training_loss": 7.355506420135498
    },
    {
      "epoch": 0.25734417344173444,
      "step": 1187,
      "training_loss": 7.515232563018799
    },
    {
      "epoch": 0.2575609756097561,
      "grad_norm": 11.724323272705078,
      "learning_rate": 1e-05,
      "loss": 6.8264,
      "step": 1188
    },
    {
      "epoch": 0.2575609756097561,
      "step": 1188,
      "training_loss": 6.033239841461182
    },
    {
      "epoch": 0.2575609756097561,
      "step": 1188,
      "training_loss": 4.48406457901001
    },
    {
      "epoch": 0.2575609756097561,
      "step": 1188,
      "training_loss": 7.264068126678467
    },
    {
      "epoch": 0.2575609756097561,
      "step": 1188,
      "training_loss": 8.6771879196167
    },
    {
      "epoch": 0.2577777777777778,
      "step": 1189,
      "training_loss": 7.208704471588135
    },
    {
      "epoch": 0.2577777777777778,
      "step": 1189,
      "training_loss": 8.120426177978516
    },
    {
      "epoch": 0.2577777777777778,
      "step": 1189,
      "training_loss": 6.957635879516602
    },
    {
      "epoch": 0.2577777777777778,
      "step": 1189,
      "training_loss": 6.636363983154297
    },
    {
      "epoch": 0.25799457994579944,
      "step": 1190,
      "training_loss": 7.148530960083008
    },
    {
      "epoch": 0.25799457994579944,
      "step": 1190,
      "training_loss": 5.273324012756348
    },
    {
      "epoch": 0.25799457994579944,
      "step": 1190,
      "training_loss": 6.88399600982666
    },
    {
      "epoch": 0.25799457994579944,
      "step": 1190,
      "training_loss": 7.364534854888916
    },
    {
      "epoch": 0.25821138211382116,
      "step": 1191,
      "training_loss": 7.548931121826172
    },
    {
      "epoch": 0.25821138211382116,
      "step": 1191,
      "training_loss": 6.850070476531982
    },
    {
      "epoch": 0.25821138211382116,
      "step": 1191,
      "training_loss": 6.5268683433532715
    },
    {
      "epoch": 0.25821138211382116,
      "step": 1191,
      "training_loss": 6.222479820251465
    },
    {
      "epoch": 0.25842818428184283,
      "grad_norm": 9.511556625366211,
      "learning_rate": 1e-05,
      "loss": 6.825,
      "step": 1192
    },
    {
      "epoch": 0.25842818428184283,
      "step": 1192,
      "training_loss": 6.714018821716309
    },
    {
      "epoch": 0.25842818428184283,
      "step": 1192,
      "training_loss": 7.807712078094482
    },
    {
      "epoch": 0.25842818428184283,
      "step": 1192,
      "training_loss": 7.41910982131958
    },
    {
      "epoch": 0.25842818428184283,
      "step": 1192,
      "training_loss": 5.798588275909424
    },
    {
      "epoch": 0.2586449864498645,
      "step": 1193,
      "training_loss": 7.615363597869873
    },
    {
      "epoch": 0.2586449864498645,
      "step": 1193,
      "training_loss": 7.158195972442627
    },
    {
      "epoch": 0.2586449864498645,
      "step": 1193,
      "training_loss": 7.69288444519043
    },
    {
      "epoch": 0.2586449864498645,
      "step": 1193,
      "training_loss": 5.74700927734375
    },
    {
      "epoch": 0.25886178861788617,
      "step": 1194,
      "training_loss": 6.784201622009277
    },
    {
      "epoch": 0.25886178861788617,
      "step": 1194,
      "training_loss": 6.223033428192139
    },
    {
      "epoch": 0.25886178861788617,
      "step": 1194,
      "training_loss": 6.455259799957275
    },
    {
      "epoch": 0.25886178861788617,
      "step": 1194,
      "training_loss": 7.301753997802734
    },
    {
      "epoch": 0.25907859078590784,
      "step": 1195,
      "training_loss": 8.444475173950195
    },
    {
      "epoch": 0.25907859078590784,
      "step": 1195,
      "training_loss": 7.213710784912109
    },
    {
      "epoch": 0.25907859078590784,
      "step": 1195,
      "training_loss": 7.037543773651123
    },
    {
      "epoch": 0.25907859078590784,
      "step": 1195,
      "training_loss": 7.432907581329346
    },
    {
      "epoch": 0.25929539295392956,
      "grad_norm": 12.161343574523926,
      "learning_rate": 1e-05,
      "loss": 7.0529,
      "step": 1196
    },
    {
      "epoch": 0.25929539295392956,
      "step": 1196,
      "training_loss": 7.6507368087768555
    },
    {
      "epoch": 0.25929539295392956,
      "step": 1196,
      "training_loss": 6.043193817138672
    },
    {
      "epoch": 0.25929539295392956,
      "step": 1196,
      "training_loss": 7.209499359130859
    },
    {
      "epoch": 0.25929539295392956,
      "step": 1196,
      "training_loss": 6.354310989379883
    },
    {
      "epoch": 0.25951219512195123,
      "step": 1197,
      "training_loss": 6.846682071685791
    },
    {
      "epoch": 0.25951219512195123,
      "step": 1197,
      "training_loss": 7.209164142608643
    },
    {
      "epoch": 0.25951219512195123,
      "step": 1197,
      "training_loss": 5.90344762802124
    },
    {
      "epoch": 0.25951219512195123,
      "step": 1197,
      "training_loss": 7.13301420211792
    },
    {
      "epoch": 0.2597289972899729,
      "step": 1198,
      "training_loss": 7.744858264923096
    },
    {
      "epoch": 0.2597289972899729,
      "step": 1198,
      "training_loss": 6.225622653961182
    },
    {
      "epoch": 0.2597289972899729,
      "step": 1198,
      "training_loss": 7.490937232971191
    },
    {
      "epoch": 0.2597289972899729,
      "step": 1198,
      "training_loss": 6.8980393409729
    },
    {
      "epoch": 0.25994579945799456,
      "step": 1199,
      "training_loss": 5.608983993530273
    },
    {
      "epoch": 0.25994579945799456,
      "step": 1199,
      "training_loss": 6.820324420928955
    },
    {
      "epoch": 0.25994579945799456,
      "step": 1199,
      "training_loss": 6.090577125549316
    },
    {
      "epoch": 0.25994579945799456,
      "step": 1199,
      "training_loss": 6.783963680267334
    },
    {
      "epoch": 0.2601626016260163,
      "grad_norm": 21.024232864379883,
      "learning_rate": 1e-05,
      "loss": 6.7508,
      "step": 1200
    },
    {
      "epoch": 0.2601626016260163,
      "step": 1200,
      "training_loss": 7.310865879058838
    },
    {
      "epoch": 0.2601626016260163,
      "step": 1200,
      "training_loss": 7.290266513824463
    },
    {
      "epoch": 0.2601626016260163,
      "step": 1200,
      "training_loss": 6.54668664932251
    },
    {
      "epoch": 0.2601626016260163,
      "step": 1200,
      "training_loss": 6.961453437805176
    },
    {
      "epoch": 0.26037940379403796,
      "step": 1201,
      "training_loss": 6.960027694702148
    },
    {
      "epoch": 0.26037940379403796,
      "step": 1201,
      "training_loss": 5.793966293334961
    },
    {
      "epoch": 0.26037940379403796,
      "step": 1201,
      "training_loss": 6.493354320526123
    },
    {
      "epoch": 0.26037940379403796,
      "step": 1201,
      "training_loss": 6.544284820556641
    },
    {
      "epoch": 0.2605962059620596,
      "step": 1202,
      "training_loss": 4.545827865600586
    },
    {
      "epoch": 0.2605962059620596,
      "step": 1202,
      "training_loss": 8.087959289550781
    },
    {
      "epoch": 0.2605962059620596,
      "step": 1202,
      "training_loss": 6.832186222076416
    },
    {
      "epoch": 0.2605962059620596,
      "step": 1202,
      "training_loss": 7.036766529083252
    },
    {
      "epoch": 0.2608130081300813,
      "step": 1203,
      "training_loss": 5.860195159912109
    },
    {
      "epoch": 0.2608130081300813,
      "step": 1203,
      "training_loss": 8.513921737670898
    },
    {
      "epoch": 0.2608130081300813,
      "step": 1203,
      "training_loss": 7.195688724517822
    },
    {
      "epoch": 0.2608130081300813,
      "step": 1203,
      "training_loss": 7.112873077392578
    },
    {
      "epoch": 0.26102981029810296,
      "grad_norm": 12.02924633026123,
      "learning_rate": 1e-05,
      "loss": 6.8179,
      "step": 1204
    },
    {
      "epoch": 0.26102981029810296,
      "step": 1204,
      "training_loss": 7.0836896896362305
    },
    {
      "epoch": 0.26102981029810296,
      "step": 1204,
      "training_loss": 6.9020094871521
    },
    {
      "epoch": 0.26102981029810296,
      "step": 1204,
      "training_loss": 7.006012439727783
    },
    {
      "epoch": 0.26102981029810296,
      "step": 1204,
      "training_loss": 6.341948986053467
    },
    {
      "epoch": 0.2612466124661247,
      "step": 1205,
      "training_loss": 6.273036956787109
    },
    {
      "epoch": 0.2612466124661247,
      "step": 1205,
      "training_loss": 6.720661163330078
    },
    {
      "epoch": 0.2612466124661247,
      "step": 1205,
      "training_loss": 7.189017295837402
    },
    {
      "epoch": 0.2612466124661247,
      "step": 1205,
      "training_loss": 7.507137298583984
    },
    {
      "epoch": 0.26146341463414635,
      "step": 1206,
      "training_loss": 6.583150386810303
    },
    {
      "epoch": 0.26146341463414635,
      "step": 1206,
      "training_loss": 6.950738906860352
    },
    {
      "epoch": 0.26146341463414635,
      "step": 1206,
      "training_loss": 6.556240081787109
    },
    {
      "epoch": 0.26146341463414635,
      "step": 1206,
      "training_loss": 7.301921844482422
    },
    {
      "epoch": 0.261680216802168,
      "step": 1207,
      "training_loss": 6.896803379058838
    },
    {
      "epoch": 0.261680216802168,
      "step": 1207,
      "training_loss": 7.059595584869385
    },
    {
      "epoch": 0.261680216802168,
      "step": 1207,
      "training_loss": 6.461666584014893
    },
    {
      "epoch": 0.261680216802168,
      "step": 1207,
      "training_loss": 7.826074123382568
    },
    {
      "epoch": 0.2618970189701897,
      "grad_norm": 11.116612434387207,
      "learning_rate": 1e-05,
      "loss": 6.9162,
      "step": 1208
    },
    {
      "epoch": 0.2618970189701897,
      "step": 1208,
      "training_loss": 7.442145347595215
    },
    {
      "epoch": 0.2618970189701897,
      "step": 1208,
      "training_loss": 7.006429672241211
    },
    {
      "epoch": 0.2618970189701897,
      "step": 1208,
      "training_loss": 7.307609558105469
    },
    {
      "epoch": 0.2618970189701897,
      "step": 1208,
      "training_loss": 7.538306713104248
    },
    {
      "epoch": 0.26211382113821136,
      "step": 1209,
      "training_loss": 7.71394157409668
    },
    {
      "epoch": 0.26211382113821136,
      "step": 1209,
      "training_loss": 5.6306843757629395
    },
    {
      "epoch": 0.26211382113821136,
      "step": 1209,
      "training_loss": 7.643363952636719
    },
    {
      "epoch": 0.26211382113821136,
      "step": 1209,
      "training_loss": 7.408754825592041
    },
    {
      "epoch": 0.2623306233062331,
      "step": 1210,
      "training_loss": 6.284713268280029
    },
    {
      "epoch": 0.2623306233062331,
      "step": 1210,
      "training_loss": 8.127771377563477
    },
    {
      "epoch": 0.2623306233062331,
      "step": 1210,
      "training_loss": 8.222599983215332
    },
    {
      "epoch": 0.2623306233062331,
      "step": 1210,
      "training_loss": 7.12717342376709
    },
    {
      "epoch": 0.26254742547425475,
      "step": 1211,
      "training_loss": 7.229034423828125
    },
    {
      "epoch": 0.26254742547425475,
      "step": 1211,
      "training_loss": 7.639761924743652
    },
    {
      "epoch": 0.26254742547425475,
      "step": 1211,
      "training_loss": 6.236316204071045
    },
    {
      "epoch": 0.26254742547425475,
      "step": 1211,
      "training_loss": 6.9902472496032715
    },
    {
      "epoch": 0.2627642276422764,
      "grad_norm": 12.58489990234375,
      "learning_rate": 1e-05,
      "loss": 7.2218,
      "step": 1212
    },
    {
      "epoch": 0.2627642276422764,
      "step": 1212,
      "training_loss": 7.4343109130859375
    },
    {
      "epoch": 0.2627642276422764,
      "step": 1212,
      "training_loss": 6.683711528778076
    },
    {
      "epoch": 0.2627642276422764,
      "step": 1212,
      "training_loss": 7.106671333312988
    },
    {
      "epoch": 0.2627642276422764,
      "step": 1212,
      "training_loss": 6.827040195465088
    },
    {
      "epoch": 0.2629810298102981,
      "step": 1213,
      "training_loss": 7.5085554122924805
    },
    {
      "epoch": 0.2629810298102981,
      "step": 1213,
      "training_loss": 6.699880599975586
    },
    {
      "epoch": 0.2629810298102981,
      "step": 1213,
      "training_loss": 6.732553482055664
    },
    {
      "epoch": 0.2629810298102981,
      "step": 1213,
      "training_loss": 5.975132465362549
    },
    {
      "epoch": 0.2631978319783198,
      "step": 1214,
      "training_loss": 7.537517547607422
    },
    {
      "epoch": 0.2631978319783198,
      "step": 1214,
      "training_loss": 6.29100227355957
    },
    {
      "epoch": 0.2631978319783198,
      "step": 1214,
      "training_loss": 6.629748344421387
    },
    {
      "epoch": 0.2631978319783198,
      "step": 1214,
      "training_loss": 7.53555965423584
    },
    {
      "epoch": 0.2634146341463415,
      "step": 1215,
      "training_loss": 7.030120372772217
    },
    {
      "epoch": 0.2634146341463415,
      "step": 1215,
      "training_loss": 7.901857852935791
    },
    {
      "epoch": 0.2634146341463415,
      "step": 1215,
      "training_loss": 5.911977291107178
    },
    {
      "epoch": 0.2634146341463415,
      "step": 1215,
      "training_loss": 7.630941390991211
    },
    {
      "epoch": 0.26363143631436314,
      "grad_norm": 10.77598762512207,
      "learning_rate": 1e-05,
      "loss": 6.9648,
      "step": 1216
    },
    {
      "epoch": 0.26363143631436314,
      "step": 1216,
      "training_loss": 7.552185535430908
    },
    {
      "epoch": 0.26363143631436314,
      "step": 1216,
      "training_loss": 7.420846939086914
    },
    {
      "epoch": 0.26363143631436314,
      "step": 1216,
      "training_loss": 8.370104789733887
    },
    {
      "epoch": 0.26363143631436314,
      "step": 1216,
      "training_loss": 6.470491409301758
    },
    {
      "epoch": 0.2638482384823848,
      "step": 1217,
      "training_loss": 6.05180549621582
    },
    {
      "epoch": 0.2638482384823848,
      "step": 1217,
      "training_loss": 5.329020023345947
    },
    {
      "epoch": 0.2638482384823848,
      "step": 1217,
      "training_loss": 7.885063648223877
    },
    {
      "epoch": 0.2638482384823848,
      "step": 1217,
      "training_loss": 7.591660976409912
    },
    {
      "epoch": 0.2640650406504065,
      "step": 1218,
      "training_loss": 6.759666919708252
    },
    {
      "epoch": 0.2640650406504065,
      "step": 1218,
      "training_loss": 4.971434116363525
    },
    {
      "epoch": 0.2640650406504065,
      "step": 1218,
      "training_loss": 7.4828901290893555
    },
    {
      "epoch": 0.2640650406504065,
      "step": 1218,
      "training_loss": 6.880161285400391
    },
    {
      "epoch": 0.2642818428184282,
      "step": 1219,
      "training_loss": 6.736654758453369
    },
    {
      "epoch": 0.2642818428184282,
      "step": 1219,
      "training_loss": 6.319893836975098
    },
    {
      "epoch": 0.2642818428184282,
      "step": 1219,
      "training_loss": 6.946145534515381
    },
    {
      "epoch": 0.2642818428184282,
      "step": 1219,
      "training_loss": 7.134067058563232
    },
    {
      "epoch": 0.26449864498644987,
      "grad_norm": 9.612577438354492,
      "learning_rate": 1e-05,
      "loss": 6.8689,
      "step": 1220
    },
    {
      "epoch": 0.26449864498644987,
      "step": 1220,
      "training_loss": 6.826146602630615
    },
    {
      "epoch": 0.26449864498644987,
      "step": 1220,
      "training_loss": 7.397317886352539
    },
    {
      "epoch": 0.26449864498644987,
      "step": 1220,
      "training_loss": 7.595570087432861
    },
    {
      "epoch": 0.26449864498644987,
      "step": 1220,
      "training_loss": 6.639533519744873
    },
    {
      "epoch": 0.26471544715447154,
      "step": 1221,
      "training_loss": 6.947284698486328
    },
    {
      "epoch": 0.26471544715447154,
      "step": 1221,
      "training_loss": 8.150077819824219
    },
    {
      "epoch": 0.26471544715447154,
      "step": 1221,
      "training_loss": 6.278438091278076
    },
    {
      "epoch": 0.26471544715447154,
      "step": 1221,
      "training_loss": 5.025537014007568
    },
    {
      "epoch": 0.2649322493224932,
      "step": 1222,
      "training_loss": 5.157489776611328
    },
    {
      "epoch": 0.2649322493224932,
      "step": 1222,
      "training_loss": 7.755422592163086
    },
    {
      "epoch": 0.2649322493224932,
      "step": 1222,
      "training_loss": 5.737362384796143
    },
    {
      "epoch": 0.2649322493224932,
      "step": 1222,
      "training_loss": 6.784856796264648
    },
    {
      "epoch": 0.26514905149051493,
      "step": 1223,
      "training_loss": 6.959603786468506
    },
    {
      "epoch": 0.26514905149051493,
      "step": 1223,
      "training_loss": 7.3351898193359375
    },
    {
      "epoch": 0.26514905149051493,
      "step": 1223,
      "training_loss": 7.8290181159973145
    },
    {
      "epoch": 0.26514905149051493,
      "step": 1223,
      "training_loss": 5.880040645599365
    },
    {
      "epoch": 0.2653658536585366,
      "grad_norm": 10.695844650268555,
      "learning_rate": 1e-05,
      "loss": 6.7687,
      "step": 1224
    },
    {
      "epoch": 0.2653658536585366,
      "step": 1224,
      "training_loss": 4.799084663391113
    },
    {
      "epoch": 0.2653658536585366,
      "step": 1224,
      "training_loss": 6.671602725982666
    },
    {
      "epoch": 0.2653658536585366,
      "step": 1224,
      "training_loss": 7.413687705993652
    },
    {
      "epoch": 0.2653658536585366,
      "step": 1224,
      "training_loss": 6.597569942474365
    },
    {
      "epoch": 0.26558265582655827,
      "step": 1225,
      "training_loss": 6.580340385437012
    },
    {
      "epoch": 0.26558265582655827,
      "step": 1225,
      "training_loss": 6.912883281707764
    },
    {
      "epoch": 0.26558265582655827,
      "step": 1225,
      "training_loss": 7.479572296142578
    },
    {
      "epoch": 0.26558265582655827,
      "step": 1225,
      "training_loss": 8.901373863220215
    },
    {
      "epoch": 0.26579945799457994,
      "step": 1226,
      "training_loss": 6.327216625213623
    },
    {
      "epoch": 0.26579945799457994,
      "step": 1226,
      "training_loss": 7.238004207611084
    },
    {
      "epoch": 0.26579945799457994,
      "step": 1226,
      "training_loss": 7.1274895668029785
    },
    {
      "epoch": 0.26579945799457994,
      "step": 1226,
      "training_loss": 6.632993698120117
    },
    {
      "epoch": 0.2660162601626016,
      "step": 1227,
      "training_loss": 6.511383056640625
    },
    {
      "epoch": 0.2660162601626016,
      "step": 1227,
      "training_loss": 7.0075812339782715
    },
    {
      "epoch": 0.2660162601626016,
      "step": 1227,
      "training_loss": 7.63429594039917
    },
    {
      "epoch": 0.2660162601626016,
      "step": 1227,
      "training_loss": 8.311016082763672
    },
    {
      "epoch": 0.2662330623306233,
      "grad_norm": 10.96148681640625,
      "learning_rate": 1e-05,
      "loss": 7.0091,
      "step": 1228
    },
    {
      "epoch": 0.2662330623306233,
      "step": 1228,
      "training_loss": 6.845617294311523
    },
    {
      "epoch": 0.2662330623306233,
      "step": 1228,
      "training_loss": 7.200986862182617
    },
    {
      "epoch": 0.2662330623306233,
      "step": 1228,
      "training_loss": 6.3055100440979
    },
    {
      "epoch": 0.2662330623306233,
      "step": 1228,
      "training_loss": 7.109217166900635
    },
    {
      "epoch": 0.266449864498645,
      "step": 1229,
      "training_loss": 7.220516681671143
    },
    {
      "epoch": 0.266449864498645,
      "step": 1229,
      "training_loss": 7.481105804443359
    },
    {
      "epoch": 0.266449864498645,
      "step": 1229,
      "training_loss": 4.863149642944336
    },
    {
      "epoch": 0.266449864498645,
      "step": 1229,
      "training_loss": 8.252840995788574
    },
    {
      "epoch": 0.26666666666666666,
      "step": 1230,
      "training_loss": 7.092360973358154
    },
    {
      "epoch": 0.26666666666666666,
      "step": 1230,
      "training_loss": 6.8711628913879395
    },
    {
      "epoch": 0.26666666666666666,
      "step": 1230,
      "training_loss": 7.700523853302002
    },
    {
      "epoch": 0.26666666666666666,
      "step": 1230,
      "training_loss": 7.846122741699219
    },
    {
      "epoch": 0.26688346883468833,
      "step": 1231,
      "training_loss": 8.060955047607422
    },
    {
      "epoch": 0.26688346883468833,
      "step": 1231,
      "training_loss": 6.129741668701172
    },
    {
      "epoch": 0.26688346883468833,
      "step": 1231,
      "training_loss": 7.361046314239502
    },
    {
      "epoch": 0.26688346883468833,
      "step": 1231,
      "training_loss": 6.358890056610107
    },
    {
      "epoch": 0.26710027100271005,
      "grad_norm": 11.890700340270996,
      "learning_rate": 1e-05,
      "loss": 7.0437,
      "step": 1232
    },
    {
      "epoch": 0.26710027100271005,
      "step": 1232,
      "training_loss": 6.636552333831787
    },
    {
      "epoch": 0.26710027100271005,
      "step": 1232,
      "training_loss": 7.303225040435791
    },
    {
      "epoch": 0.26710027100271005,
      "step": 1232,
      "training_loss": 7.946299076080322
    },
    {
      "epoch": 0.26710027100271005,
      "step": 1232,
      "training_loss": 6.062041759490967
    },
    {
      "epoch": 0.2673170731707317,
      "step": 1233,
      "training_loss": 6.9236602783203125
    },
    {
      "epoch": 0.2673170731707317,
      "step": 1233,
      "training_loss": 7.498474597930908
    },
    {
      "epoch": 0.2673170731707317,
      "step": 1233,
      "training_loss": 7.441540718078613
    },
    {
      "epoch": 0.2673170731707317,
      "step": 1233,
      "training_loss": 6.918821334838867
    },
    {
      "epoch": 0.2675338753387534,
      "step": 1234,
      "training_loss": 7.993755340576172
    },
    {
      "epoch": 0.2675338753387534,
      "step": 1234,
      "training_loss": 6.9297566413879395
    },
    {
      "epoch": 0.2675338753387534,
      "step": 1234,
      "training_loss": 8.009839057922363
    },
    {
      "epoch": 0.2675338753387534,
      "step": 1234,
      "training_loss": 6.498166561126709
    },
    {
      "epoch": 0.26775067750677506,
      "step": 1235,
      "training_loss": 7.161844730377197
    },
    {
      "epoch": 0.26775067750677506,
      "step": 1235,
      "training_loss": 7.106975078582764
    },
    {
      "epoch": 0.26775067750677506,
      "step": 1235,
      "training_loss": 6.207282066345215
    },
    {
      "epoch": 0.26775067750677506,
      "step": 1235,
      "training_loss": 6.765815734863281
    },
    {
      "epoch": 0.2679674796747967,
      "grad_norm": 9.107978820800781,
      "learning_rate": 1e-05,
      "loss": 7.0878,
      "step": 1236
    },
    {
      "epoch": 0.2679674796747967,
      "step": 1236,
      "training_loss": 5.849630355834961
    },
    {
      "epoch": 0.2679674796747967,
      "step": 1236,
      "training_loss": 6.371317386627197
    },
    {
      "epoch": 0.2679674796747967,
      "step": 1236,
      "training_loss": 6.191779136657715
    },
    {
      "epoch": 0.2679674796747967,
      "step": 1236,
      "training_loss": 6.019562721252441
    },
    {
      "epoch": 0.26818428184281845,
      "step": 1237,
      "training_loss": 5.905892372131348
    },
    {
      "epoch": 0.26818428184281845,
      "step": 1237,
      "training_loss": 6.11245059967041
    },
    {
      "epoch": 0.26818428184281845,
      "step": 1237,
      "training_loss": 7.026834964752197
    },
    {
      "epoch": 0.26818428184281845,
      "step": 1237,
      "training_loss": 7.0406646728515625
    },
    {
      "epoch": 0.2684010840108401,
      "step": 1238,
      "training_loss": 6.710328578948975
    },
    {
      "epoch": 0.2684010840108401,
      "step": 1238,
      "training_loss": 6.472901344299316
    },
    {
      "epoch": 0.2684010840108401,
      "step": 1238,
      "training_loss": 6.818559646606445
    },
    {
      "epoch": 0.2684010840108401,
      "step": 1238,
      "training_loss": 5.996326923370361
    },
    {
      "epoch": 0.2686178861788618,
      "step": 1239,
      "training_loss": 7.8045759201049805
    },
    {
      "epoch": 0.2686178861788618,
      "step": 1239,
      "training_loss": 3.9682514667510986
    },
    {
      "epoch": 0.2686178861788618,
      "step": 1239,
      "training_loss": 8.040393829345703
    },
    {
      "epoch": 0.2686178861788618,
      "step": 1239,
      "training_loss": 4.91656494140625
    },
    {
      "epoch": 0.26883468834688345,
      "grad_norm": 11.770962715148926,
      "learning_rate": 1e-05,
      "loss": 6.3279,
      "step": 1240
    },
    {
      "epoch": 0.26883468834688345,
      "step": 1240,
      "training_loss": 8.293050765991211
    },
    {
      "epoch": 0.26883468834688345,
      "step": 1240,
      "training_loss": 7.436187744140625
    },
    {
      "epoch": 0.26883468834688345,
      "step": 1240,
      "training_loss": 7.488138198852539
    },
    {
      "epoch": 0.26883468834688345,
      "step": 1240,
      "training_loss": 7.439058303833008
    },
    {
      "epoch": 0.2690514905149051,
      "step": 1241,
      "training_loss": 8.052603721618652
    },
    {
      "epoch": 0.2690514905149051,
      "step": 1241,
      "training_loss": 6.454054355621338
    },
    {
      "epoch": 0.2690514905149051,
      "step": 1241,
      "training_loss": 7.150751113891602
    },
    {
      "epoch": 0.2690514905149051,
      "step": 1241,
      "training_loss": 6.504859447479248
    },
    {
      "epoch": 0.26926829268292685,
      "step": 1242,
      "training_loss": 5.96629524230957
    },
    {
      "epoch": 0.26926829268292685,
      "step": 1242,
      "training_loss": 7.078763008117676
    },
    {
      "epoch": 0.26926829268292685,
      "step": 1242,
      "training_loss": 6.796637535095215
    },
    {
      "epoch": 0.26926829268292685,
      "step": 1242,
      "training_loss": 7.0615692138671875
    },
    {
      "epoch": 0.2694850948509485,
      "step": 1243,
      "training_loss": 6.777421474456787
    },
    {
      "epoch": 0.2694850948509485,
      "step": 1243,
      "training_loss": 6.386005878448486
    },
    {
      "epoch": 0.2694850948509485,
      "step": 1243,
      "training_loss": 7.266984462738037
    },
    {
      "epoch": 0.2694850948509485,
      "step": 1243,
      "training_loss": 5.806630611419678
    },
    {
      "epoch": 0.2697018970189702,
      "grad_norm": 12.314183235168457,
      "learning_rate": 1e-05,
      "loss": 6.9974,
      "step": 1244
    },
    {
      "epoch": 0.2697018970189702,
      "step": 1244,
      "training_loss": 8.345503807067871
    },
    {
      "epoch": 0.2697018970189702,
      "step": 1244,
      "training_loss": 6.465434551239014
    },
    {
      "epoch": 0.2697018970189702,
      "step": 1244,
      "training_loss": 6.599984645843506
    },
    {
      "epoch": 0.2697018970189702,
      "step": 1244,
      "training_loss": 7.862142086029053
    },
    {
      "epoch": 0.26991869918699185,
      "step": 1245,
      "training_loss": 7.071755409240723
    },
    {
      "epoch": 0.26991869918699185,
      "step": 1245,
      "training_loss": 7.470027446746826
    },
    {
      "epoch": 0.26991869918699185,
      "step": 1245,
      "training_loss": 6.132706165313721
    },
    {
      "epoch": 0.26991869918699185,
      "step": 1245,
      "training_loss": 7.22521448135376
    },
    {
      "epoch": 0.2701355013550136,
      "step": 1246,
      "training_loss": 7.572267055511475
    },
    {
      "epoch": 0.2701355013550136,
      "step": 1246,
      "training_loss": 6.919271469116211
    },
    {
      "epoch": 0.2701355013550136,
      "step": 1246,
      "training_loss": 7.696727275848389
    },
    {
      "epoch": 0.2701355013550136,
      "step": 1246,
      "training_loss": 7.447562217712402
    },
    {
      "epoch": 0.27035230352303524,
      "step": 1247,
      "training_loss": 7.081358432769775
    },
    {
      "epoch": 0.27035230352303524,
      "step": 1247,
      "training_loss": 7.83201265335083
    },
    {
      "epoch": 0.27035230352303524,
      "step": 1247,
      "training_loss": 7.250486373901367
    },
    {
      "epoch": 0.27035230352303524,
      "step": 1247,
      "training_loss": 7.757256031036377
    },
    {
      "epoch": 0.2705691056910569,
      "grad_norm": 10.899338722229004,
      "learning_rate": 1e-05,
      "loss": 7.2956,
      "step": 1248
    },
    {
      "epoch": 0.2705691056910569,
      "step": 1248,
      "training_loss": 6.986612319946289
    },
    {
      "epoch": 0.2705691056910569,
      "step": 1248,
      "training_loss": 7.579284191131592
    },
    {
      "epoch": 0.2705691056910569,
      "step": 1248,
      "training_loss": 7.088249206542969
    },
    {
      "epoch": 0.2705691056910569,
      "step": 1248,
      "training_loss": 7.446885585784912
    },
    {
      "epoch": 0.2707859078590786,
      "step": 1249,
      "training_loss": 6.8246283531188965
    },
    {
      "epoch": 0.2707859078590786,
      "step": 1249,
      "training_loss": 5.281497955322266
    },
    {
      "epoch": 0.2707859078590786,
      "step": 1249,
      "training_loss": 6.333003520965576
    },
    {
      "epoch": 0.2707859078590786,
      "step": 1249,
      "training_loss": 7.709160327911377
    },
    {
      "epoch": 0.27100271002710025,
      "step": 1250,
      "training_loss": 7.3298821449279785
    },
    {
      "epoch": 0.27100271002710025,
      "step": 1250,
      "training_loss": 7.8211469650268555
    },
    {
      "epoch": 0.27100271002710025,
      "step": 1250,
      "training_loss": 6.6198906898498535
    },
    {
      "epoch": 0.27100271002710025,
      "step": 1250,
      "training_loss": 6.729305267333984
    },
    {
      "epoch": 0.27121951219512197,
      "step": 1251,
      "training_loss": 6.483108997344971
    },
    {
      "epoch": 0.27121951219512197,
      "step": 1251,
      "training_loss": 7.198068141937256
    },
    {
      "epoch": 0.27121951219512197,
      "step": 1251,
      "training_loss": 8.65249252319336
    },
    {
      "epoch": 0.27121951219512197,
      "step": 1251,
      "training_loss": 7.09812068939209
    },
    {
      "epoch": 0.27143631436314364,
      "grad_norm": 11.298846244812012,
      "learning_rate": 1e-05,
      "loss": 7.0738,
      "step": 1252
    },
    {
      "epoch": 0.27143631436314364,
      "step": 1252,
      "training_loss": 7.12604284286499
    },
    {
      "epoch": 0.27143631436314364,
      "step": 1252,
      "training_loss": 7.181591510772705
    },
    {
      "epoch": 0.27143631436314364,
      "step": 1252,
      "training_loss": 6.7035064697265625
    },
    {
      "epoch": 0.27143631436314364,
      "step": 1252,
      "training_loss": 5.914373397827148
    },
    {
      "epoch": 0.2716531165311653,
      "step": 1253,
      "training_loss": 7.364342212677002
    },
    {
      "epoch": 0.2716531165311653,
      "step": 1253,
      "training_loss": 7.50762939453125
    },
    {
      "epoch": 0.2716531165311653,
      "step": 1253,
      "training_loss": 6.099970817565918
    },
    {
      "epoch": 0.2716531165311653,
      "step": 1253,
      "training_loss": 7.483819484710693
    },
    {
      "epoch": 0.271869918699187,
      "step": 1254,
      "training_loss": 5.518998622894287
    },
    {
      "epoch": 0.271869918699187,
      "step": 1254,
      "training_loss": 7.171819686889648
    },
    {
      "epoch": 0.271869918699187,
      "step": 1254,
      "training_loss": 8.104472160339355
    },
    {
      "epoch": 0.271869918699187,
      "step": 1254,
      "training_loss": 7.613284587860107
    },
    {
      "epoch": 0.2720867208672087,
      "step": 1255,
      "training_loss": 7.07728910446167
    },
    {
      "epoch": 0.2720867208672087,
      "step": 1255,
      "training_loss": 8.95007038116455
    },
    {
      "epoch": 0.2720867208672087,
      "step": 1255,
      "training_loss": 8.343103408813477
    },
    {
      "epoch": 0.2720867208672087,
      "step": 1255,
      "training_loss": 7.3635406494140625
    },
    {
      "epoch": 0.27230352303523037,
      "grad_norm": 12.329615592956543,
      "learning_rate": 1e-05,
      "loss": 7.2202,
      "step": 1256
    },
    {
      "epoch": 0.27230352303523037,
      "step": 1256,
      "training_loss": 7.354058742523193
    },
    {
      "epoch": 0.27230352303523037,
      "step": 1256,
      "training_loss": 7.0200276374816895
    },
    {
      "epoch": 0.27230352303523037,
      "step": 1256,
      "training_loss": 5.931285858154297
    },
    {
      "epoch": 0.27230352303523037,
      "step": 1256,
      "training_loss": 6.522705078125
    },
    {
      "epoch": 0.27252032520325203,
      "step": 1257,
      "training_loss": 7.487317085266113
    },
    {
      "epoch": 0.27252032520325203,
      "step": 1257,
      "training_loss": 6.218979358673096
    },
    {
      "epoch": 0.27252032520325203,
      "step": 1257,
      "training_loss": 7.860516548156738
    },
    {
      "epoch": 0.27252032520325203,
      "step": 1257,
      "training_loss": 7.272420883178711
    },
    {
      "epoch": 0.2727371273712737,
      "step": 1258,
      "training_loss": 7.958540439605713
    },
    {
      "epoch": 0.2727371273712737,
      "step": 1258,
      "training_loss": 6.471065044403076
    },
    {
      "epoch": 0.2727371273712737,
      "step": 1258,
      "training_loss": 5.800843238830566
    },
    {
      "epoch": 0.2727371273712737,
      "step": 1258,
      "training_loss": 7.129497528076172
    },
    {
      "epoch": 0.27295392953929537,
      "step": 1259,
      "training_loss": 6.205568313598633
    },
    {
      "epoch": 0.27295392953929537,
      "step": 1259,
      "training_loss": 7.711884498596191
    },
    {
      "epoch": 0.27295392953929537,
      "step": 1259,
      "training_loss": 7.568289279937744
    },
    {
      "epoch": 0.27295392953929537,
      "step": 1259,
      "training_loss": 7.252121448516846
    },
    {
      "epoch": 0.2731707317073171,
      "grad_norm": 9.190211296081543,
      "learning_rate": 1e-05,
      "loss": 6.9853,
      "step": 1260
    },
    {
      "epoch": 0.2731707317073171,
      "step": 1260,
      "training_loss": 6.832106590270996
    },
    {
      "epoch": 0.2731707317073171,
      "step": 1260,
      "training_loss": 7.264849662780762
    },
    {
      "epoch": 0.2731707317073171,
      "step": 1260,
      "training_loss": 7.250072479248047
    },
    {
      "epoch": 0.2731707317073171,
      "step": 1260,
      "training_loss": 6.596536636352539
    },
    {
      "epoch": 0.27338753387533876,
      "step": 1261,
      "training_loss": 7.22640323638916
    },
    {
      "epoch": 0.27338753387533876,
      "step": 1261,
      "training_loss": 6.633586406707764
    },
    {
      "epoch": 0.27338753387533876,
      "step": 1261,
      "training_loss": 8.099926948547363
    },
    {
      "epoch": 0.27338753387533876,
      "step": 1261,
      "training_loss": 7.192752838134766
    },
    {
      "epoch": 0.27360433604336043,
      "step": 1262,
      "training_loss": 7.716578006744385
    },
    {
      "epoch": 0.27360433604336043,
      "step": 1262,
      "training_loss": 7.886001110076904
    },
    {
      "epoch": 0.27360433604336043,
      "step": 1262,
      "training_loss": 8.187666893005371
    },
    {
      "epoch": 0.27360433604336043,
      "step": 1262,
      "training_loss": 7.324018955230713
    },
    {
      "epoch": 0.2738211382113821,
      "step": 1263,
      "training_loss": 9.200560569763184
    },
    {
      "epoch": 0.2738211382113821,
      "step": 1263,
      "training_loss": 6.916619777679443
    },
    {
      "epoch": 0.2738211382113821,
      "step": 1263,
      "training_loss": 7.176090240478516
    },
    {
      "epoch": 0.2738211382113821,
      "step": 1263,
      "training_loss": 6.026546478271484
    },
    {
      "epoch": 0.2740379403794038,
      "grad_norm": 12.51988697052002,
      "learning_rate": 1e-05,
      "loss": 7.3456,
      "step": 1264
    },
    {
      "epoch": 0.2740379403794038,
      "step": 1264,
      "training_loss": 7.486380100250244
    },
    {
      "epoch": 0.2740379403794038,
      "step": 1264,
      "training_loss": 5.772532939910889
    },
    {
      "epoch": 0.2740379403794038,
      "step": 1264,
      "training_loss": 6.485970497131348
    },
    {
      "epoch": 0.2740379403794038,
      "step": 1264,
      "training_loss": 7.2397780418396
    },
    {
      "epoch": 0.2742547425474255,
      "step": 1265,
      "training_loss": 5.14519739151001
    },
    {
      "epoch": 0.2742547425474255,
      "step": 1265,
      "training_loss": 7.872018814086914
    },
    {
      "epoch": 0.2742547425474255,
      "step": 1265,
      "training_loss": 6.7055792808532715
    },
    {
      "epoch": 0.2742547425474255,
      "step": 1265,
      "training_loss": 6.634101390838623
    },
    {
      "epoch": 0.27447154471544716,
      "step": 1266,
      "training_loss": 6.431583404541016
    },
    {
      "epoch": 0.27447154471544716,
      "step": 1266,
      "training_loss": 7.227764129638672
    },
    {
      "epoch": 0.27447154471544716,
      "step": 1266,
      "training_loss": 6.4657368659973145
    },
    {
      "epoch": 0.27447154471544716,
      "step": 1266,
      "training_loss": 7.430370330810547
    },
    {
      "epoch": 0.2746883468834688,
      "step": 1267,
      "training_loss": 7.364115238189697
    },
    {
      "epoch": 0.2746883468834688,
      "step": 1267,
      "training_loss": 7.20124626159668
    },
    {
      "epoch": 0.2746883468834688,
      "step": 1267,
      "training_loss": 7.181028842926025
    },
    {
      "epoch": 0.2746883468834688,
      "step": 1267,
      "training_loss": 6.1012349128723145
    },
    {
      "epoch": 0.2749051490514905,
      "grad_norm": 10.627791404724121,
      "learning_rate": 1e-05,
      "loss": 6.7965,
      "step": 1268
    },
    {
      "epoch": 0.2749051490514905,
      "step": 1268,
      "training_loss": 6.191298961639404
    },
    {
      "epoch": 0.2749051490514905,
      "step": 1268,
      "training_loss": 6.381189823150635
    },
    {
      "epoch": 0.2749051490514905,
      "step": 1268,
      "training_loss": 7.455287456512451
    },
    {
      "epoch": 0.2749051490514905,
      "step": 1268,
      "training_loss": 6.20367431640625
    },
    {
      "epoch": 0.2751219512195122,
      "step": 1269,
      "training_loss": 7.4449357986450195
    },
    {
      "epoch": 0.2751219512195122,
      "step": 1269,
      "training_loss": 6.958320140838623
    },
    {
      "epoch": 0.2751219512195122,
      "step": 1269,
      "training_loss": 7.434619903564453
    },
    {
      "epoch": 0.2751219512195122,
      "step": 1269,
      "training_loss": 7.020669460296631
    },
    {
      "epoch": 0.2753387533875339,
      "step": 1270,
      "training_loss": 7.991962432861328
    },
    {
      "epoch": 0.2753387533875339,
      "step": 1270,
      "training_loss": 6.396665573120117
    },
    {
      "epoch": 0.2753387533875339,
      "step": 1270,
      "training_loss": 7.261954307556152
    },
    {
      "epoch": 0.2753387533875339,
      "step": 1270,
      "training_loss": 5.437427043914795
    },
    {
      "epoch": 0.27555555555555555,
      "step": 1271,
      "training_loss": 6.299627780914307
    },
    {
      "epoch": 0.27555555555555555,
      "step": 1271,
      "training_loss": 6.944193363189697
    },
    {
      "epoch": 0.27555555555555555,
      "step": 1271,
      "training_loss": 6.804725646972656
    },
    {
      "epoch": 0.27555555555555555,
      "step": 1271,
      "training_loss": 7.047982215881348
    },
    {
      "epoch": 0.2757723577235772,
      "grad_norm": 8.057882308959961,
      "learning_rate": 1e-05,
      "loss": 6.8297,
      "step": 1272
    },
    {
      "epoch": 0.2757723577235772,
      "step": 1272,
      "training_loss": 7.779261112213135
    },
    {
      "epoch": 0.2757723577235772,
      "step": 1272,
      "training_loss": 7.536351680755615
    },
    {
      "epoch": 0.2757723577235772,
      "step": 1272,
      "training_loss": 6.552501678466797
    },
    {
      "epoch": 0.2757723577235772,
      "step": 1272,
      "training_loss": 5.787855625152588
    },
    {
      "epoch": 0.2759891598915989,
      "step": 1273,
      "training_loss": 7.11043643951416
    },
    {
      "epoch": 0.2759891598915989,
      "step": 1273,
      "training_loss": 7.0965895652771
    },
    {
      "epoch": 0.2759891598915989,
      "step": 1273,
      "training_loss": 5.974819183349609
    },
    {
      "epoch": 0.2759891598915989,
      "step": 1273,
      "training_loss": 5.270234107971191
    },
    {
      "epoch": 0.2762059620596206,
      "step": 1274,
      "training_loss": 7.228073596954346
    },
    {
      "epoch": 0.2762059620596206,
      "step": 1274,
      "training_loss": 7.212876319885254
    },
    {
      "epoch": 0.2762059620596206,
      "step": 1274,
      "training_loss": 6.7968645095825195
    },
    {
      "epoch": 0.2762059620596206,
      "step": 1274,
      "training_loss": 6.230345726013184
    },
    {
      "epoch": 0.2764227642276423,
      "step": 1275,
      "training_loss": 4.732089042663574
    },
    {
      "epoch": 0.2764227642276423,
      "step": 1275,
      "training_loss": 6.295330047607422
    },
    {
      "epoch": 0.2764227642276423,
      "step": 1275,
      "training_loss": 6.884389400482178
    },
    {
      "epoch": 0.2764227642276423,
      "step": 1275,
      "training_loss": 5.155648708343506
    },
    {
      "epoch": 0.27663956639566395,
      "grad_norm": 12.547107696533203,
      "learning_rate": 1e-05,
      "loss": 6.4777,
      "step": 1276
    },
    {
      "epoch": 0.27663956639566395,
      "step": 1276,
      "training_loss": 7.297157287597656
    },
    {
      "epoch": 0.27663956639566395,
      "step": 1276,
      "training_loss": 7.182052135467529
    },
    {
      "epoch": 0.27663956639566395,
      "step": 1276,
      "training_loss": 7.6082000732421875
    },
    {
      "epoch": 0.27663956639566395,
      "step": 1276,
      "training_loss": 7.382030010223389
    },
    {
      "epoch": 0.2768563685636856,
      "step": 1277,
      "training_loss": 6.652834415435791
    },
    {
      "epoch": 0.2768563685636856,
      "step": 1277,
      "training_loss": 6.359307765960693
    },
    {
      "epoch": 0.2768563685636856,
      "step": 1277,
      "training_loss": 6.649117469787598
    },
    {
      "epoch": 0.2768563685636856,
      "step": 1277,
      "training_loss": 5.324779510498047
    },
    {
      "epoch": 0.27707317073170734,
      "step": 1278,
      "training_loss": 6.535033702850342
    },
    {
      "epoch": 0.27707317073170734,
      "step": 1278,
      "training_loss": 7.647547245025635
    },
    {
      "epoch": 0.27707317073170734,
      "step": 1278,
      "training_loss": 6.056833744049072
    },
    {
      "epoch": 0.27707317073170734,
      "step": 1278,
      "training_loss": 7.196736812591553
    },
    {
      "epoch": 0.277289972899729,
      "step": 1279,
      "training_loss": 7.4795966148376465
    },
    {
      "epoch": 0.277289972899729,
      "step": 1279,
      "training_loss": 6.748595714569092
    },
    {
      "epoch": 0.277289972899729,
      "step": 1279,
      "training_loss": 8.200261116027832
    },
    {
      "epoch": 0.277289972899729,
      "step": 1279,
      "training_loss": 6.433755874633789
    },
    {
      "epoch": 0.2775067750677507,
      "grad_norm": 10.212786674499512,
      "learning_rate": 1e-05,
      "loss": 6.9221,
      "step": 1280
    },
    {
      "epoch": 0.2775067750677507,
      "step": 1280,
      "training_loss": 8.760422706604004
    },
    {
      "epoch": 0.2775067750677507,
      "step": 1280,
      "training_loss": 7.414706707000732
    },
    {
      "epoch": 0.2775067750677507,
      "step": 1280,
      "training_loss": 7.854870796203613
    },
    {
      "epoch": 0.2775067750677507,
      "step": 1280,
      "training_loss": 7.5017991065979
    },
    {
      "epoch": 0.27772357723577235,
      "step": 1281,
      "training_loss": 7.2580766677856445
    },
    {
      "epoch": 0.27772357723577235,
      "step": 1281,
      "training_loss": 7.865551948547363
    },
    {
      "epoch": 0.27772357723577235,
      "step": 1281,
      "training_loss": 6.386936187744141
    },
    {
      "epoch": 0.27772357723577235,
      "step": 1281,
      "training_loss": 6.07961368560791
    },
    {
      "epoch": 0.277940379403794,
      "step": 1282,
      "training_loss": 8.077245712280273
    },
    {
      "epoch": 0.277940379403794,
      "step": 1282,
      "training_loss": 7.739903926849365
    },
    {
      "epoch": 0.277940379403794,
      "step": 1282,
      "training_loss": 7.058558464050293
    },
    {
      "epoch": 0.277940379403794,
      "step": 1282,
      "training_loss": 8.166058540344238
    },
    {
      "epoch": 0.27815718157181574,
      "step": 1283,
      "training_loss": 7.019698143005371
    },
    {
      "epoch": 0.27815718157181574,
      "step": 1283,
      "training_loss": 7.248083114624023
    },
    {
      "epoch": 0.27815718157181574,
      "step": 1283,
      "training_loss": 6.623615741729736
    },
    {
      "epoch": 0.27815718157181574,
      "step": 1283,
      "training_loss": 7.137217998504639
    },
    {
      "epoch": 0.2783739837398374,
      "grad_norm": 11.437820434570312,
      "learning_rate": 1e-05,
      "loss": 7.387,
      "step": 1284
    },
    {
      "epoch": 0.2783739837398374,
      "step": 1284,
      "training_loss": 6.342959880828857
    },
    {
      "epoch": 0.2783739837398374,
      "step": 1284,
      "training_loss": 7.3350830078125
    },
    {
      "epoch": 0.2783739837398374,
      "step": 1284,
      "training_loss": 7.493411064147949
    },
    {
      "epoch": 0.2783739837398374,
      "step": 1284,
      "training_loss": 7.449929237365723
    },
    {
      "epoch": 0.2785907859078591,
      "step": 1285,
      "training_loss": 6.68914794921875
    },
    {
      "epoch": 0.2785907859078591,
      "step": 1285,
      "training_loss": 6.920162677764893
    },
    {
      "epoch": 0.2785907859078591,
      "step": 1285,
      "training_loss": 6.010358810424805
    },
    {
      "epoch": 0.2785907859078591,
      "step": 1285,
      "training_loss": 4.9592437744140625
    },
    {
      "epoch": 0.27880758807588074,
      "step": 1286,
      "training_loss": 6.716350555419922
    },
    {
      "epoch": 0.27880758807588074,
      "step": 1286,
      "training_loss": 7.166469097137451
    },
    {
      "epoch": 0.27880758807588074,
      "step": 1286,
      "training_loss": 6.167840957641602
    },
    {
      "epoch": 0.27880758807588074,
      "step": 1286,
      "training_loss": 7.282151699066162
    },
    {
      "epoch": 0.27902439024390246,
      "step": 1287,
      "training_loss": 4.951041221618652
    },
    {
      "epoch": 0.27902439024390246,
      "step": 1287,
      "training_loss": 4.914383888244629
    },
    {
      "epoch": 0.27902439024390246,
      "step": 1287,
      "training_loss": 5.4579596519470215
    },
    {
      "epoch": 0.27902439024390246,
      "step": 1287,
      "training_loss": 7.177332878112793
    },
    {
      "epoch": 0.27924119241192413,
      "grad_norm": 11.151546478271484,
      "learning_rate": 1e-05,
      "loss": 6.4396,
      "step": 1288
    },
    {
      "epoch": 0.27924119241192413,
      "step": 1288,
      "training_loss": 8.484631538391113
    },
    {
      "epoch": 0.27924119241192413,
      "step": 1288,
      "training_loss": 7.470307350158691
    },
    {
      "epoch": 0.27924119241192413,
      "step": 1288,
      "training_loss": 7.935141563415527
    },
    {
      "epoch": 0.27924119241192413,
      "step": 1288,
      "training_loss": 6.906927108764648
    },
    {
      "epoch": 0.2794579945799458,
      "step": 1289,
      "training_loss": 5.911740779876709
    },
    {
      "epoch": 0.2794579945799458,
      "step": 1289,
      "training_loss": 5.860156059265137
    },
    {
      "epoch": 0.2794579945799458,
      "step": 1289,
      "training_loss": 6.713443756103516
    },
    {
      "epoch": 0.2794579945799458,
      "step": 1289,
      "training_loss": 5.7943501472473145
    },
    {
      "epoch": 0.27967479674796747,
      "step": 1290,
      "training_loss": 4.562552452087402
    },
    {
      "epoch": 0.27967479674796747,
      "step": 1290,
      "training_loss": 7.292029857635498
    },
    {
      "epoch": 0.27967479674796747,
      "step": 1290,
      "training_loss": 6.060975551605225
    },
    {
      "epoch": 0.27967479674796747,
      "step": 1290,
      "training_loss": 7.09102725982666
    },
    {
      "epoch": 0.27989159891598914,
      "step": 1291,
      "training_loss": 6.987026214599609
    },
    {
      "epoch": 0.27989159891598914,
      "step": 1291,
      "training_loss": 7.813706874847412
    },
    {
      "epoch": 0.27989159891598914,
      "step": 1291,
      "training_loss": 7.327681541442871
    },
    {
      "epoch": 0.27989159891598914,
      "step": 1291,
      "training_loss": 6.3886823654174805
    },
    {
      "epoch": 0.28010840108401086,
      "grad_norm": 9.337859153747559,
      "learning_rate": 1e-05,
      "loss": 6.7875,
      "step": 1292
    },
    {
      "epoch": 0.28010840108401086,
      "step": 1292,
      "training_loss": 6.765022277832031
    },
    {
      "epoch": 0.28010840108401086,
      "step": 1292,
      "training_loss": 7.07673978805542
    },
    {
      "epoch": 0.28010840108401086,
      "step": 1292,
      "training_loss": 6.924882888793945
    },
    {
      "epoch": 0.28010840108401086,
      "step": 1292,
      "training_loss": 6.3975510597229
    },
    {
      "epoch": 0.28032520325203253,
      "step": 1293,
      "training_loss": 7.109369277954102
    },
    {
      "epoch": 0.28032520325203253,
      "step": 1293,
      "training_loss": 7.5706257820129395
    },
    {
      "epoch": 0.28032520325203253,
      "step": 1293,
      "training_loss": 6.070343017578125
    },
    {
      "epoch": 0.28032520325203253,
      "step": 1293,
      "training_loss": 7.163394451141357
    },
    {
      "epoch": 0.2805420054200542,
      "step": 1294,
      "training_loss": 7.371772289276123
    },
    {
      "epoch": 0.2805420054200542,
      "step": 1294,
      "training_loss": 6.317259311676025
    },
    {
      "epoch": 0.2805420054200542,
      "step": 1294,
      "training_loss": 7.800706386566162
    },
    {
      "epoch": 0.2805420054200542,
      "step": 1294,
      "training_loss": 7.093240261077881
    },
    {
      "epoch": 0.28075880758807586,
      "step": 1295,
      "training_loss": 7.069164276123047
    },
    {
      "epoch": 0.28075880758807586,
      "step": 1295,
      "training_loss": 6.665639400482178
    },
    {
      "epoch": 0.28075880758807586,
      "step": 1295,
      "training_loss": 7.378109931945801
    },
    {
      "epoch": 0.28075880758807586,
      "step": 1295,
      "training_loss": 4.296513557434082
    },
    {
      "epoch": 0.2809756097560976,
      "grad_norm": 10.117953300476074,
      "learning_rate": 1e-05,
      "loss": 6.8169,
      "step": 1296
    },
    {
      "epoch": 0.2809756097560976,
      "step": 1296,
      "training_loss": 6.673667907714844
    },
    {
      "epoch": 0.2809756097560976,
      "step": 1296,
      "training_loss": 6.871021747589111
    },
    {
      "epoch": 0.2809756097560976,
      "step": 1296,
      "training_loss": 7.358849048614502
    },
    {
      "epoch": 0.2809756097560976,
      "step": 1296,
      "training_loss": 6.731753826141357
    },
    {
      "epoch": 0.28119241192411926,
      "step": 1297,
      "training_loss": 6.8480377197265625
    },
    {
      "epoch": 0.28119241192411926,
      "step": 1297,
      "training_loss": 7.191940784454346
    },
    {
      "epoch": 0.28119241192411926,
      "step": 1297,
      "training_loss": 6.602221488952637
    },
    {
      "epoch": 0.28119241192411926,
      "step": 1297,
      "training_loss": 7.805811405181885
    },
    {
      "epoch": 0.2814092140921409,
      "step": 1298,
      "training_loss": 7.199617862701416
    },
    {
      "epoch": 0.2814092140921409,
      "step": 1298,
      "training_loss": 7.7522993087768555
    },
    {
      "epoch": 0.2814092140921409,
      "step": 1298,
      "training_loss": 8.09804630279541
    },
    {
      "epoch": 0.2814092140921409,
      "step": 1298,
      "training_loss": 7.324092864990234
    },
    {
      "epoch": 0.2816260162601626,
      "step": 1299,
      "training_loss": 7.170595169067383
    },
    {
      "epoch": 0.2816260162601626,
      "step": 1299,
      "training_loss": 6.018655776977539
    },
    {
      "epoch": 0.2816260162601626,
      "step": 1299,
      "training_loss": 7.029481887817383
    },
    {
      "epoch": 0.2816260162601626,
      "step": 1299,
      "training_loss": 8.119815826416016
    },
    {
      "epoch": 0.28184281842818426,
      "grad_norm": 11.177828788757324,
      "learning_rate": 1e-05,
      "loss": 7.1747,
      "step": 1300
    },
    {
      "epoch": 0.28184281842818426,
      "step": 1300,
      "training_loss": 5.457675457000732
    },
    {
      "epoch": 0.28184281842818426,
      "step": 1300,
      "training_loss": 7.0447916984558105
    },
    {
      "epoch": 0.28184281842818426,
      "step": 1300,
      "training_loss": 4.75991153717041
    },
    {
      "epoch": 0.28184281842818426,
      "step": 1300,
      "training_loss": 7.154180526733398
    },
    {
      "epoch": 0.282059620596206,
      "step": 1301,
      "training_loss": 6.460989475250244
    },
    {
      "epoch": 0.282059620596206,
      "step": 1301,
      "training_loss": 6.905231475830078
    },
    {
      "epoch": 0.282059620596206,
      "step": 1301,
      "training_loss": 7.632200717926025
    },
    {
      "epoch": 0.282059620596206,
      "step": 1301,
      "training_loss": 7.022422790527344
    },
    {
      "epoch": 0.28227642276422765,
      "step": 1302,
      "training_loss": 7.462324142456055
    },
    {
      "epoch": 0.28227642276422765,
      "step": 1302,
      "training_loss": 7.509594917297363
    },
    {
      "epoch": 0.28227642276422765,
      "step": 1302,
      "training_loss": 7.52032470703125
    },
    {
      "epoch": 0.28227642276422765,
      "step": 1302,
      "training_loss": 7.169502258300781
    },
    {
      "epoch": 0.2824932249322493,
      "step": 1303,
      "training_loss": 7.371226787567139
    },
    {
      "epoch": 0.2824932249322493,
      "step": 1303,
      "training_loss": 6.745395660400391
    },
    {
      "epoch": 0.2824932249322493,
      "step": 1303,
      "training_loss": 7.349982738494873
    },
    {
      "epoch": 0.2824932249322493,
      "step": 1303,
      "training_loss": 6.67792272567749
    },
    {
      "epoch": 0.282710027100271,
      "grad_norm": 10.226861000061035,
      "learning_rate": 1e-05,
      "loss": 6.8902,
      "step": 1304
    },
    {
      "epoch": 0.282710027100271,
      "step": 1304,
      "training_loss": 7.015510559082031
    },
    {
      "epoch": 0.282710027100271,
      "step": 1304,
      "training_loss": 7.3602118492126465
    },
    {
      "epoch": 0.282710027100271,
      "step": 1304,
      "training_loss": 6.850759506225586
    },
    {
      "epoch": 0.282710027100271,
      "step": 1304,
      "training_loss": 6.6882734298706055
    },
    {
      "epoch": 0.28292682926829266,
      "step": 1305,
      "training_loss": 6.742886543273926
    },
    {
      "epoch": 0.28292682926829266,
      "step": 1305,
      "training_loss": 6.813981533050537
    },
    {
      "epoch": 0.28292682926829266,
      "step": 1305,
      "training_loss": 6.784328937530518
    },
    {
      "epoch": 0.28292682926829266,
      "step": 1305,
      "training_loss": 6.833166599273682
    },
    {
      "epoch": 0.2831436314363144,
      "step": 1306,
      "training_loss": 5.909376621246338
    },
    {
      "epoch": 0.2831436314363144,
      "step": 1306,
      "training_loss": 6.2184247970581055
    },
    {
      "epoch": 0.2831436314363144,
      "step": 1306,
      "training_loss": 7.927612781524658
    },
    {
      "epoch": 0.2831436314363144,
      "step": 1306,
      "training_loss": 8.04352855682373
    },
    {
      "epoch": 0.28336043360433605,
      "step": 1307,
      "training_loss": 7.735752105712891
    },
    {
      "epoch": 0.28336043360433605,
      "step": 1307,
      "training_loss": 5.815115451812744
    },
    {
      "epoch": 0.28336043360433605,
      "step": 1307,
      "training_loss": 6.124726295471191
    },
    {
      "epoch": 0.28336043360433605,
      "step": 1307,
      "training_loss": 5.417520999908447
    },
    {
      "epoch": 0.2835772357723577,
      "grad_norm": 13.038675308227539,
      "learning_rate": 1e-05,
      "loss": 6.7676,
      "step": 1308
    },
    {
      "epoch": 0.2835772357723577,
      "step": 1308,
      "training_loss": 7.480415344238281
    },
    {
      "epoch": 0.2835772357723577,
      "step": 1308,
      "training_loss": 6.557668685913086
    },
    {
      "epoch": 0.2835772357723577,
      "step": 1308,
      "training_loss": 7.134233474731445
    },
    {
      "epoch": 0.2835772357723577,
      "step": 1308,
      "training_loss": 6.072049617767334
    },
    {
      "epoch": 0.2837940379403794,
      "step": 1309,
      "training_loss": 7.505715847015381
    },
    {
      "epoch": 0.2837940379403794,
      "step": 1309,
      "training_loss": 7.311806678771973
    },
    {
      "epoch": 0.2837940379403794,
      "step": 1309,
      "training_loss": 5.496044635772705
    },
    {
      "epoch": 0.2837940379403794,
      "step": 1309,
      "training_loss": 7.561929702758789
    },
    {
      "epoch": 0.2840108401084011,
      "step": 1310,
      "training_loss": 7.156541347503662
    },
    {
      "epoch": 0.2840108401084011,
      "step": 1310,
      "training_loss": 5.872918128967285
    },
    {
      "epoch": 0.2840108401084011,
      "step": 1310,
      "training_loss": 6.320230484008789
    },
    {
      "epoch": 0.2840108401084011,
      "step": 1310,
      "training_loss": 6.041848659515381
    },
    {
      "epoch": 0.2842276422764228,
      "step": 1311,
      "training_loss": 6.231814861297607
    },
    {
      "epoch": 0.2842276422764228,
      "step": 1311,
      "training_loss": 4.949749946594238
    },
    {
      "epoch": 0.2842276422764228,
      "step": 1311,
      "training_loss": 6.612771987915039
    },
    {
      "epoch": 0.2842276422764228,
      "step": 1311,
      "training_loss": 6.236177921295166
    },
    {
      "epoch": 0.28444444444444444,
      "grad_norm": 11.853654861450195,
      "learning_rate": 1e-05,
      "loss": 6.5339,
      "step": 1312
    },
    {
      "epoch": 0.28444444444444444,
      "step": 1312,
      "training_loss": 6.232537746429443
    },
    {
      "epoch": 0.28444444444444444,
      "step": 1312,
      "training_loss": 6.563932418823242
    },
    {
      "epoch": 0.28444444444444444,
      "step": 1312,
      "training_loss": 6.863009929656982
    },
    {
      "epoch": 0.28444444444444444,
      "step": 1312,
      "training_loss": 7.182210445404053
    },
    {
      "epoch": 0.2846612466124661,
      "step": 1313,
      "training_loss": 6.934208869934082
    },
    {
      "epoch": 0.2846612466124661,
      "step": 1313,
      "training_loss": 7.005838871002197
    },
    {
      "epoch": 0.2846612466124661,
      "step": 1313,
      "training_loss": 6.401938438415527
    },
    {
      "epoch": 0.2846612466124661,
      "step": 1313,
      "training_loss": 7.347143650054932
    },
    {
      "epoch": 0.2848780487804878,
      "step": 1314,
      "training_loss": 4.7606892585754395
    },
    {
      "epoch": 0.2848780487804878,
      "step": 1314,
      "training_loss": 7.073276042938232
    },
    {
      "epoch": 0.2848780487804878,
      "step": 1314,
      "training_loss": 9.667080879211426
    },
    {
      "epoch": 0.2848780487804878,
      "step": 1314,
      "training_loss": 5.672601222991943
    },
    {
      "epoch": 0.2850948509485095,
      "step": 1315,
      "training_loss": 7.4445343017578125
    },
    {
      "epoch": 0.2850948509485095,
      "step": 1315,
      "training_loss": 6.6006550788879395
    },
    {
      "epoch": 0.2850948509485095,
      "step": 1315,
      "training_loss": 5.335942268371582
    },
    {
      "epoch": 0.2850948509485095,
      "step": 1315,
      "training_loss": 6.92181921005249
    },
    {
      "epoch": 0.28531165311653117,
      "grad_norm": 11.346056938171387,
      "learning_rate": 1e-05,
      "loss": 6.7505,
      "step": 1316
    },
    {
      "epoch": 0.28531165311653117,
      "step": 1316,
      "training_loss": 6.487114429473877
    },
    {
      "epoch": 0.28531165311653117,
      "step": 1316,
      "training_loss": 7.314789295196533
    },
    {
      "epoch": 0.28531165311653117,
      "step": 1316,
      "training_loss": 3.908294677734375
    },
    {
      "epoch": 0.28531165311653117,
      "step": 1316,
      "training_loss": 7.076794147491455
    },
    {
      "epoch": 0.28552845528455284,
      "step": 1317,
      "training_loss": 7.004128932952881
    },
    {
      "epoch": 0.28552845528455284,
      "step": 1317,
      "training_loss": 7.6794657707214355
    },
    {
      "epoch": 0.28552845528455284,
      "step": 1317,
      "training_loss": 8.069730758666992
    },
    {
      "epoch": 0.28552845528455284,
      "step": 1317,
      "training_loss": 6.201910495758057
    },
    {
      "epoch": 0.2857452574525745,
      "step": 1318,
      "training_loss": 7.755342960357666
    },
    {
      "epoch": 0.2857452574525745,
      "step": 1318,
      "training_loss": 7.571633815765381
    },
    {
      "epoch": 0.2857452574525745,
      "step": 1318,
      "training_loss": 5.47178840637207
    },
    {
      "epoch": 0.2857452574525745,
      "step": 1318,
      "training_loss": 7.042029857635498
    },
    {
      "epoch": 0.28596205962059623,
      "step": 1319,
      "training_loss": 7.956708908081055
    },
    {
      "epoch": 0.28596205962059623,
      "step": 1319,
      "training_loss": 7.140070915222168
    },
    {
      "epoch": 0.28596205962059623,
      "step": 1319,
      "training_loss": 6.5484795570373535
    },
    {
      "epoch": 0.28596205962059623,
      "step": 1319,
      "training_loss": 6.887307167053223
    },
    {
      "epoch": 0.2861788617886179,
      "grad_norm": 11.570833206176758,
      "learning_rate": 1e-05,
      "loss": 6.8822,
      "step": 1320
    },
    {
      "epoch": 0.2861788617886179,
      "step": 1320,
      "training_loss": 6.982356548309326
    },
    {
      "epoch": 0.2861788617886179,
      "step": 1320,
      "training_loss": 6.907086372375488
    },
    {
      "epoch": 0.2861788617886179,
      "step": 1320,
      "training_loss": 7.332438945770264
    },
    {
      "epoch": 0.2861788617886179,
      "step": 1320,
      "training_loss": 7.667771339416504
    },
    {
      "epoch": 0.28639566395663957,
      "step": 1321,
      "training_loss": 6.753884315490723
    },
    {
      "epoch": 0.28639566395663957,
      "step": 1321,
      "training_loss": 7.441008567810059
    },
    {
      "epoch": 0.28639566395663957,
      "step": 1321,
      "training_loss": 6.996374130249023
    },
    {
      "epoch": 0.28639566395663957,
      "step": 1321,
      "training_loss": 5.1016106605529785
    },
    {
      "epoch": 0.28661246612466124,
      "step": 1322,
      "training_loss": 7.035507678985596
    },
    {
      "epoch": 0.28661246612466124,
      "step": 1322,
      "training_loss": 7.753342151641846
    },
    {
      "epoch": 0.28661246612466124,
      "step": 1322,
      "training_loss": 6.813234329223633
    },
    {
      "epoch": 0.28661246612466124,
      "step": 1322,
      "training_loss": 7.146568298339844
    },
    {
      "epoch": 0.2868292682926829,
      "step": 1323,
      "training_loss": 7.718034267425537
    },
    {
      "epoch": 0.2868292682926829,
      "step": 1323,
      "training_loss": 7.454529285430908
    },
    {
      "epoch": 0.2868292682926829,
      "step": 1323,
      "training_loss": 6.284377098083496
    },
    {
      "epoch": 0.2868292682926829,
      "step": 1323,
      "training_loss": 6.2020769119262695
    },
    {
      "epoch": 0.2870460704607046,
      "grad_norm": 12.721735954284668,
      "learning_rate": 1e-05,
      "loss": 6.9744,
      "step": 1324
    },
    {
      "epoch": 0.2870460704607046,
      "step": 1324,
      "training_loss": 7.726288795471191
    },
    {
      "epoch": 0.2870460704607046,
      "step": 1324,
      "training_loss": 6.038079738616943
    },
    {
      "epoch": 0.2870460704607046,
      "step": 1324,
      "training_loss": 7.662810325622559
    },
    {
      "epoch": 0.2870460704607046,
      "step": 1324,
      "training_loss": 6.4698309898376465
    },
    {
      "epoch": 0.2872628726287263,
      "step": 1325,
      "training_loss": 7.829518795013428
    },
    {
      "epoch": 0.2872628726287263,
      "step": 1325,
      "training_loss": 4.62178897857666
    },
    {
      "epoch": 0.2872628726287263,
      "step": 1325,
      "training_loss": 4.727189064025879
    },
    {
      "epoch": 0.2872628726287263,
      "step": 1325,
      "training_loss": 5.835144996643066
    },
    {
      "epoch": 0.28747967479674796,
      "step": 1326,
      "training_loss": 7.082463264465332
    },
    {
      "epoch": 0.28747967479674796,
      "step": 1326,
      "training_loss": 7.051929473876953
    },
    {
      "epoch": 0.28747967479674796,
      "step": 1326,
      "training_loss": 6.762205123901367
    },
    {
      "epoch": 0.28747967479674796,
      "step": 1326,
      "training_loss": 7.034854412078857
    },
    {
      "epoch": 0.28769647696476963,
      "step": 1327,
      "training_loss": 7.242672443389893
    },
    {
      "epoch": 0.28769647696476963,
      "step": 1327,
      "training_loss": 6.980411529541016
    },
    {
      "epoch": 0.28769647696476963,
      "step": 1327,
      "training_loss": 6.1357855796813965
    },
    {
      "epoch": 0.28769647696476963,
      "step": 1327,
      "training_loss": 7.259984970092773
    },
    {
      "epoch": 0.28791327913279136,
      "grad_norm": 10.993103981018066,
      "learning_rate": 1e-05,
      "loss": 6.6538,
      "step": 1328
    },
    {
      "epoch": 0.28791327913279136,
      "step": 1328,
      "training_loss": 6.100094318389893
    },
    {
      "epoch": 0.28791327913279136,
      "step": 1328,
      "training_loss": 6.4005632400512695
    },
    {
      "epoch": 0.28791327913279136,
      "step": 1328,
      "training_loss": 6.480111598968506
    },
    {
      "epoch": 0.28791327913279136,
      "step": 1328,
      "training_loss": 7.13843297958374
    },
    {
      "epoch": 0.288130081300813,
      "step": 1329,
      "training_loss": 6.69028377532959
    },
    {
      "epoch": 0.288130081300813,
      "step": 1329,
      "training_loss": 4.571645736694336
    },
    {
      "epoch": 0.288130081300813,
      "step": 1329,
      "training_loss": 7.5363450050354
    },
    {
      "epoch": 0.288130081300813,
      "step": 1329,
      "training_loss": 6.548077583312988
    },
    {
      "epoch": 0.2883468834688347,
      "step": 1330,
      "training_loss": 6.861446857452393
    },
    {
      "epoch": 0.2883468834688347,
      "step": 1330,
      "training_loss": 6.1627197265625
    },
    {
      "epoch": 0.2883468834688347,
      "step": 1330,
      "training_loss": 6.439582347869873
    },
    {
      "epoch": 0.2883468834688347,
      "step": 1330,
      "training_loss": 7.292685508728027
    },
    {
      "epoch": 0.28856368563685636,
      "step": 1331,
      "training_loss": 7.195518970489502
    },
    {
      "epoch": 0.28856368563685636,
      "step": 1331,
      "training_loss": 5.6012282371521
    },
    {
      "epoch": 0.28856368563685636,
      "step": 1331,
      "training_loss": 6.921881198883057
    },
    {
      "epoch": 0.28856368563685636,
      "step": 1331,
      "training_loss": 7.73827600479126
    },
    {
      "epoch": 0.288780487804878,
      "grad_norm": 10.853577613830566,
      "learning_rate": 1e-05,
      "loss": 6.6049,
      "step": 1332
    },
    {
      "epoch": 0.288780487804878,
      "step": 1332,
      "training_loss": 6.523479461669922
    },
    {
      "epoch": 0.288780487804878,
      "step": 1332,
      "training_loss": 6.7955217361450195
    },
    {
      "epoch": 0.288780487804878,
      "step": 1332,
      "training_loss": 7.789525508880615
    },
    {
      "epoch": 0.288780487804878,
      "step": 1332,
      "training_loss": 5.979030132293701
    },
    {
      "epoch": 0.28899728997289975,
      "step": 1333,
      "training_loss": 7.096251010894775
    },
    {
      "epoch": 0.28899728997289975,
      "step": 1333,
      "training_loss": 6.96211576461792
    },
    {
      "epoch": 0.28899728997289975,
      "step": 1333,
      "training_loss": 7.657576084136963
    },
    {
      "epoch": 0.28899728997289975,
      "step": 1333,
      "training_loss": 6.919751167297363
    },
    {
      "epoch": 0.2892140921409214,
      "step": 1334,
      "training_loss": 8.351832389831543
    },
    {
      "epoch": 0.2892140921409214,
      "step": 1334,
      "training_loss": 6.949306964874268
    },
    {
      "epoch": 0.2892140921409214,
      "step": 1334,
      "training_loss": 6.74993896484375
    },
    {
      "epoch": 0.2892140921409214,
      "step": 1334,
      "training_loss": 7.515335559844971
    },
    {
      "epoch": 0.2894308943089431,
      "step": 1335,
      "training_loss": 5.78221321105957
    },
    {
      "epoch": 0.2894308943089431,
      "step": 1335,
      "training_loss": 7.265688419342041
    },
    {
      "epoch": 0.2894308943089431,
      "step": 1335,
      "training_loss": 7.246512413024902
    },
    {
      "epoch": 0.2894308943089431,
      "step": 1335,
      "training_loss": 7.708591938018799
    },
    {
      "epoch": 0.28964769647696476,
      "grad_norm": 11.682160377502441,
      "learning_rate": 1e-05,
      "loss": 7.0808,
      "step": 1336
    },
    {
      "epoch": 0.28964769647696476,
      "step": 1336,
      "training_loss": 6.824801445007324
    },
    {
      "epoch": 0.28964769647696476,
      "step": 1336,
      "training_loss": 7.007654666900635
    },
    {
      "epoch": 0.28964769647696476,
      "step": 1336,
      "training_loss": 7.415992736816406
    },
    {
      "epoch": 0.28964769647696476,
      "step": 1336,
      "training_loss": 8.032255172729492
    },
    {
      "epoch": 0.2898644986449864,
      "step": 1337,
      "training_loss": 5.8042778968811035
    },
    {
      "epoch": 0.2898644986449864,
      "step": 1337,
      "training_loss": 6.809566974639893
    },
    {
      "epoch": 0.2898644986449864,
      "step": 1337,
      "training_loss": 5.978530406951904
    },
    {
      "epoch": 0.2898644986449864,
      "step": 1337,
      "training_loss": 6.352853298187256
    },
    {
      "epoch": 0.29008130081300815,
      "step": 1338,
      "training_loss": 6.228477954864502
    },
    {
      "epoch": 0.29008130081300815,
      "step": 1338,
      "training_loss": 5.611088752746582
    },
    {
      "epoch": 0.29008130081300815,
      "step": 1338,
      "training_loss": 5.435359477996826
    },
    {
      "epoch": 0.29008130081300815,
      "step": 1338,
      "training_loss": 6.7648138999938965
    },
    {
      "epoch": 0.2902981029810298,
      "step": 1339,
      "training_loss": 6.945111274719238
    },
    {
      "epoch": 0.2902981029810298,
      "step": 1339,
      "training_loss": 7.901163101196289
    },
    {
      "epoch": 0.2902981029810298,
      "step": 1339,
      "training_loss": 7.842253684997559
    },
    {
      "epoch": 0.2902981029810298,
      "step": 1339,
      "training_loss": 7.210491180419922
    },
    {
      "epoch": 0.2905149051490515,
      "grad_norm": 10.606245994567871,
      "learning_rate": 1e-05,
      "loss": 6.7603,
      "step": 1340
    },
    {
      "epoch": 0.2905149051490515,
      "step": 1340,
      "training_loss": 6.311218738555908
    },
    {
      "epoch": 0.2905149051490515,
      "step": 1340,
      "training_loss": 5.217872142791748
    },
    {
      "epoch": 0.2905149051490515,
      "step": 1340,
      "training_loss": 8.194448471069336
    },
    {
      "epoch": 0.2905149051490515,
      "step": 1340,
      "training_loss": 7.878105163574219
    },
    {
      "epoch": 0.29073170731707315,
      "step": 1341,
      "training_loss": 11.6558837890625
    },
    {
      "epoch": 0.29073170731707315,
      "step": 1341,
      "training_loss": 6.836665153503418
    },
    {
      "epoch": 0.29073170731707315,
      "step": 1341,
      "training_loss": 6.588724613189697
    },
    {
      "epoch": 0.29073170731707315,
      "step": 1341,
      "training_loss": 5.229272365570068
    },
    {
      "epoch": 0.2909485094850949,
      "step": 1342,
      "training_loss": 5.667141914367676
    },
    {
      "epoch": 0.2909485094850949,
      "step": 1342,
      "training_loss": 7.593451976776123
    },
    {
      "epoch": 0.2909485094850949,
      "step": 1342,
      "training_loss": 6.007673740386963
    },
    {
      "epoch": 0.2909485094850949,
      "step": 1342,
      "training_loss": 6.426514148712158
    },
    {
      "epoch": 0.29116531165311654,
      "step": 1343,
      "training_loss": 7.989998817443848
    },
    {
      "epoch": 0.29116531165311654,
      "step": 1343,
      "training_loss": 6.5093231201171875
    },
    {
      "epoch": 0.29116531165311654,
      "step": 1343,
      "training_loss": 5.574617385864258
    },
    {
      "epoch": 0.29116531165311654,
      "step": 1343,
      "training_loss": 6.440223217010498
    },
    {
      "epoch": 0.2913821138211382,
      "grad_norm": 13.454280853271484,
      "learning_rate": 1e-05,
      "loss": 6.8826,
      "step": 1344
    },
    {
      "epoch": 0.2913821138211382,
      "step": 1344,
      "training_loss": 8.032011032104492
    },
    {
      "epoch": 0.2913821138211382,
      "step": 1344,
      "training_loss": 9.014286994934082
    },
    {
      "epoch": 0.2913821138211382,
      "step": 1344,
      "training_loss": 7.4970221519470215
    },
    {
      "epoch": 0.2913821138211382,
      "step": 1344,
      "training_loss": 6.96103572845459
    },
    {
      "epoch": 0.2915989159891599,
      "step": 1345,
      "training_loss": 6.952782154083252
    },
    {
      "epoch": 0.2915989159891599,
      "step": 1345,
      "training_loss": 7.255221366882324
    },
    {
      "epoch": 0.2915989159891599,
      "step": 1345,
      "training_loss": 7.723636150360107
    },
    {
      "epoch": 0.2915989159891599,
      "step": 1345,
      "training_loss": 6.618013858795166
    },
    {
      "epoch": 0.29181571815718155,
      "step": 1346,
      "training_loss": 5.940160751342773
    },
    {
      "epoch": 0.29181571815718155,
      "step": 1346,
      "training_loss": 6.145793914794922
    },
    {
      "epoch": 0.29181571815718155,
      "step": 1346,
      "training_loss": 7.313621997833252
    },
    {
      "epoch": 0.29181571815718155,
      "step": 1346,
      "training_loss": 7.170774459838867
    },
    {
      "epoch": 0.29203252032520327,
      "step": 1347,
      "training_loss": 7.1098151206970215
    },
    {
      "epoch": 0.29203252032520327,
      "step": 1347,
      "training_loss": 6.732095241546631
    },
    {
      "epoch": 0.29203252032520327,
      "step": 1347,
      "training_loss": 6.519405841827393
    },
    {
      "epoch": 0.29203252032520327,
      "step": 1347,
      "training_loss": 8.106035232543945
    },
    {
      "epoch": 0.29224932249322494,
      "grad_norm": 13.475750923156738,
      "learning_rate": 1e-05,
      "loss": 7.1932,
      "step": 1348
    },
    {
      "epoch": 0.29224932249322494,
      "step": 1348,
      "training_loss": 6.935546875
    },
    {
      "epoch": 0.29224932249322494,
      "step": 1348,
      "training_loss": 6.606312274932861
    },
    {
      "epoch": 0.29224932249322494,
      "step": 1348,
      "training_loss": 6.7403883934021
    },
    {
      "epoch": 0.29224932249322494,
      "step": 1348,
      "training_loss": 7.593976974487305
    },
    {
      "epoch": 0.2924661246612466,
      "step": 1349,
      "training_loss": 7.560909748077393
    },
    {
      "epoch": 0.2924661246612466,
      "step": 1349,
      "training_loss": 8.948156356811523
    },
    {
      "epoch": 0.2924661246612466,
      "step": 1349,
      "training_loss": 6.484177112579346
    },
    {
      "epoch": 0.2924661246612466,
      "step": 1349,
      "training_loss": 7.438445091247559
    },
    {
      "epoch": 0.2926829268292683,
      "step": 1350,
      "training_loss": 7.189422607421875
    },
    {
      "epoch": 0.2926829268292683,
      "step": 1350,
      "training_loss": 7.638350963592529
    },
    {
      "epoch": 0.2926829268292683,
      "step": 1350,
      "training_loss": 7.381503105163574
    },
    {
      "epoch": 0.2926829268292683,
      "step": 1350,
      "training_loss": 6.686741828918457
    },
    {
      "epoch": 0.29289972899729,
      "step": 1351,
      "training_loss": 7.805331707000732
    },
    {
      "epoch": 0.29289972899729,
      "step": 1351,
      "training_loss": 5.934804439544678
    },
    {
      "epoch": 0.29289972899729,
      "step": 1351,
      "training_loss": 5.950994491577148
    },
    {
      "epoch": 0.29289972899729,
      "step": 1351,
      "training_loss": 6.139565467834473
    },
    {
      "epoch": 0.29311653116531167,
      "grad_norm": 14.636817932128906,
      "learning_rate": 1e-05,
      "loss": 7.0647,
      "step": 1352
    },
    {
      "epoch": 0.29311653116531167,
      "step": 1352,
      "training_loss": 6.852400779724121
    },
    {
      "epoch": 0.29311653116531167,
      "step": 1352,
      "training_loss": 6.8775177001953125
    },
    {
      "epoch": 0.29311653116531167,
      "step": 1352,
      "training_loss": 6.238414764404297
    },
    {
      "epoch": 0.29311653116531167,
      "step": 1352,
      "training_loss": 4.532907485961914
    },
    {
      "epoch": 0.29333333333333333,
      "step": 1353,
      "training_loss": 7.531967639923096
    },
    {
      "epoch": 0.29333333333333333,
      "step": 1353,
      "training_loss": 7.753028392791748
    },
    {
      "epoch": 0.29333333333333333,
      "step": 1353,
      "training_loss": 7.0324273109436035
    },
    {
      "epoch": 0.29333333333333333,
      "step": 1353,
      "training_loss": 7.654078483581543
    },
    {
      "epoch": 0.293550135501355,
      "step": 1354,
      "training_loss": 8.870500564575195
    },
    {
      "epoch": 0.293550135501355,
      "step": 1354,
      "training_loss": 7.951671123504639
    },
    {
      "epoch": 0.293550135501355,
      "step": 1354,
      "training_loss": 5.697400093078613
    },
    {
      "epoch": 0.293550135501355,
      "step": 1354,
      "training_loss": 6.8311967849731445
    },
    {
      "epoch": 0.29376693766937667,
      "step": 1355,
      "training_loss": 7.4813551902771
    },
    {
      "epoch": 0.29376693766937667,
      "step": 1355,
      "training_loss": 6.989306449890137
    },
    {
      "epoch": 0.29376693766937667,
      "step": 1355,
      "training_loss": 6.480403900146484
    },
    {
      "epoch": 0.29376693766937667,
      "step": 1355,
      "training_loss": 5.853555202484131
    },
    {
      "epoch": 0.2939837398373984,
      "grad_norm": 9.848360061645508,
      "learning_rate": 1e-05,
      "loss": 6.9143,
      "step": 1356
    },
    {
      "epoch": 0.2939837398373984,
      "step": 1356,
      "training_loss": 7.160450458526611
    },
    {
      "epoch": 0.2939837398373984,
      "step": 1356,
      "training_loss": 9.265316009521484
    },
    {
      "epoch": 0.2939837398373984,
      "step": 1356,
      "training_loss": 5.6797590255737305
    },
    {
      "epoch": 0.2939837398373984,
      "step": 1356,
      "training_loss": 7.725698471069336
    },
    {
      "epoch": 0.29420054200542006,
      "step": 1357,
      "training_loss": 6.271605014801025
    },
    {
      "epoch": 0.29420054200542006,
      "step": 1357,
      "training_loss": 6.655755043029785
    },
    {
      "epoch": 0.29420054200542006,
      "step": 1357,
      "training_loss": 7.683396339416504
    },
    {
      "epoch": 0.29420054200542006,
      "step": 1357,
      "training_loss": 7.1551361083984375
    },
    {
      "epoch": 0.29441734417344173,
      "step": 1358,
      "training_loss": 5.465001583099365
    },
    {
      "epoch": 0.29441734417344173,
      "step": 1358,
      "training_loss": 6.998783588409424
    },
    {
      "epoch": 0.29441734417344173,
      "step": 1358,
      "training_loss": 5.759510040283203
    },
    {
      "epoch": 0.29441734417344173,
      "step": 1358,
      "training_loss": 6.5449676513671875
    },
    {
      "epoch": 0.2946341463414634,
      "step": 1359,
      "training_loss": 6.773847579956055
    },
    {
      "epoch": 0.2946341463414634,
      "step": 1359,
      "training_loss": 7.480836868286133
    },
    {
      "epoch": 0.2946341463414634,
      "step": 1359,
      "training_loss": 6.200096130371094
    },
    {
      "epoch": 0.2946341463414634,
      "step": 1359,
      "training_loss": 6.834667205810547
    },
    {
      "epoch": 0.2948509485094851,
      "grad_norm": 7.9464874267578125,
      "learning_rate": 1e-05,
      "loss": 6.8534,
      "step": 1360
    },
    {
      "epoch": 0.2948509485094851,
      "step": 1360,
      "training_loss": 6.602974891662598
    },
    {
      "epoch": 0.2948509485094851,
      "step": 1360,
      "training_loss": 6.837883472442627
    },
    {
      "epoch": 0.2948509485094851,
      "step": 1360,
      "training_loss": 6.169592380523682
    },
    {
      "epoch": 0.2948509485094851,
      "step": 1360,
      "training_loss": 6.7832794189453125
    },
    {
      "epoch": 0.2950677506775068,
      "step": 1361,
      "training_loss": 5.615916728973389
    },
    {
      "epoch": 0.2950677506775068,
      "step": 1361,
      "training_loss": 6.291788101196289
    },
    {
      "epoch": 0.2950677506775068,
      "step": 1361,
      "training_loss": 7.007497787475586
    },
    {
      "epoch": 0.2950677506775068,
      "step": 1361,
      "training_loss": 7.599608898162842
    },
    {
      "epoch": 0.29528455284552846,
      "step": 1362,
      "training_loss": 6.297563552856445
    },
    {
      "epoch": 0.29528455284552846,
      "step": 1362,
      "training_loss": 6.689769744873047
    },
    {
      "epoch": 0.29528455284552846,
      "step": 1362,
      "training_loss": 7.952274322509766
    },
    {
      "epoch": 0.29528455284552846,
      "step": 1362,
      "training_loss": 6.103635787963867
    },
    {
      "epoch": 0.2955013550135501,
      "step": 1363,
      "training_loss": 6.959481716156006
    },
    {
      "epoch": 0.2955013550135501,
      "step": 1363,
      "training_loss": 7.166902542114258
    },
    {
      "epoch": 0.2955013550135501,
      "step": 1363,
      "training_loss": 8.091177940368652
    },
    {
      "epoch": 0.2955013550135501,
      "step": 1363,
      "training_loss": 4.3638129234313965
    },
    {
      "epoch": 0.2957181571815718,
      "grad_norm": 12.043890953063965,
      "learning_rate": 1e-05,
      "loss": 6.6583,
      "step": 1364
    },
    {
      "epoch": 0.2957181571815718,
      "step": 1364,
      "training_loss": 6.939330101013184
    },
    {
      "epoch": 0.2957181571815718,
      "step": 1364,
      "training_loss": 6.968878269195557
    },
    {
      "epoch": 0.2957181571815718,
      "step": 1364,
      "training_loss": 6.055292129516602
    },
    {
      "epoch": 0.2957181571815718,
      "step": 1364,
      "training_loss": 5.49023962020874
    },
    {
      "epoch": 0.2959349593495935,
      "step": 1365,
      "training_loss": 6.542580604553223
    },
    {
      "epoch": 0.2959349593495935,
      "step": 1365,
      "training_loss": 6.122120380401611
    },
    {
      "epoch": 0.2959349593495935,
      "step": 1365,
      "training_loss": 5.168903350830078
    },
    {
      "epoch": 0.2959349593495935,
      "step": 1365,
      "training_loss": 4.701756477355957
    },
    {
      "epoch": 0.2961517615176152,
      "step": 1366,
      "training_loss": 7.169476509094238
    },
    {
      "epoch": 0.2961517615176152,
      "step": 1366,
      "training_loss": 7.45853853225708
    },
    {
      "epoch": 0.2961517615176152,
      "step": 1366,
      "training_loss": 7.703368186950684
    },
    {
      "epoch": 0.2961517615176152,
      "step": 1366,
      "training_loss": 7.924389839172363
    },
    {
      "epoch": 0.29636856368563685,
      "step": 1367,
      "training_loss": 7.750249862670898
    },
    {
      "epoch": 0.29636856368563685,
      "step": 1367,
      "training_loss": 8.013692855834961
    },
    {
      "epoch": 0.29636856368563685,
      "step": 1367,
      "training_loss": 8.545732498168945
    },
    {
      "epoch": 0.29636856368563685,
      "step": 1367,
      "training_loss": 8.221172332763672
    },
    {
      "epoch": 0.2965853658536585,
      "grad_norm": 14.070898056030273,
      "learning_rate": 1e-05,
      "loss": 6.9235,
      "step": 1368
    },
    {
      "epoch": 0.2965853658536585,
      "step": 1368,
      "training_loss": 7.032261848449707
    },
    {
      "epoch": 0.2965853658536585,
      "step": 1368,
      "training_loss": 9.290417671203613
    },
    {
      "epoch": 0.2965853658536585,
      "step": 1368,
      "training_loss": 6.489999294281006
    },
    {
      "epoch": 0.2965853658536585,
      "step": 1368,
      "training_loss": 5.302497863769531
    },
    {
      "epoch": 0.2968021680216802,
      "step": 1369,
      "training_loss": 8.262619972229004
    },
    {
      "epoch": 0.2968021680216802,
      "step": 1369,
      "training_loss": 5.9532790184021
    },
    {
      "epoch": 0.2968021680216802,
      "step": 1369,
      "training_loss": 7.153698921203613
    },
    {
      "epoch": 0.2968021680216802,
      "step": 1369,
      "training_loss": 7.275404453277588
    },
    {
      "epoch": 0.2970189701897019,
      "step": 1370,
      "training_loss": 5.103616714477539
    },
    {
      "epoch": 0.2970189701897019,
      "step": 1370,
      "training_loss": 7.458254337310791
    },
    {
      "epoch": 0.2970189701897019,
      "step": 1370,
      "training_loss": 6.560132026672363
    },
    {
      "epoch": 0.2970189701897019,
      "step": 1370,
      "training_loss": 7.353178977966309
    },
    {
      "epoch": 0.2972357723577236,
      "step": 1371,
      "training_loss": 4.871937274932861
    },
    {
      "epoch": 0.2972357723577236,
      "step": 1371,
      "training_loss": 5.661942958831787
    },
    {
      "epoch": 0.2972357723577236,
      "step": 1371,
      "training_loss": 7.106959819793701
    },
    {
      "epoch": 0.2972357723577236,
      "step": 1371,
      "training_loss": 5.760831832885742
    },
    {
      "epoch": 0.29745257452574525,
      "grad_norm": 10.26778793334961,
      "learning_rate": 1e-05,
      "loss": 6.6648,
      "step": 1372
    },
    {
      "epoch": 0.29745257452574525,
      "step": 1372,
      "training_loss": 7.379616737365723
    },
    {
      "epoch": 0.29745257452574525,
      "step": 1372,
      "training_loss": 6.360511302947998
    },
    {
      "epoch": 0.29745257452574525,
      "step": 1372,
      "training_loss": 5.747204780578613
    },
    {
      "epoch": 0.29745257452574525,
      "step": 1372,
      "training_loss": 4.835597515106201
    },
    {
      "epoch": 0.2976693766937669,
      "step": 1373,
      "training_loss": 5.931272506713867
    },
    {
      "epoch": 0.2976693766937669,
      "step": 1373,
      "training_loss": 6.717137336730957
    },
    {
      "epoch": 0.2976693766937669,
      "step": 1373,
      "training_loss": 7.324906826019287
    },
    {
      "epoch": 0.2976693766937669,
      "step": 1373,
      "training_loss": 6.816966533660889
    },
    {
      "epoch": 0.29788617886178864,
      "step": 1374,
      "training_loss": 6.761072635650635
    },
    {
      "epoch": 0.29788617886178864,
      "step": 1374,
      "training_loss": 5.644361972808838
    },
    {
      "epoch": 0.29788617886178864,
      "step": 1374,
      "training_loss": 7.094188690185547
    },
    {
      "epoch": 0.29788617886178864,
      "step": 1374,
      "training_loss": 6.382448673248291
    },
    {
      "epoch": 0.2981029810298103,
      "step": 1375,
      "training_loss": 7.319972038269043
    },
    {
      "epoch": 0.2981029810298103,
      "step": 1375,
      "training_loss": 7.047584056854248
    },
    {
      "epoch": 0.2981029810298103,
      "step": 1375,
      "training_loss": 6.7177228927612305
    },
    {
      "epoch": 0.2981029810298103,
      "step": 1375,
      "training_loss": 7.689339637756348
    },
    {
      "epoch": 0.298319783197832,
      "grad_norm": 10.375510215759277,
      "learning_rate": 1e-05,
      "loss": 6.6106,
      "step": 1376
    },
    {
      "epoch": 0.298319783197832,
      "step": 1376,
      "training_loss": 7.2303900718688965
    },
    {
      "epoch": 0.298319783197832,
      "step": 1376,
      "training_loss": 6.533024787902832
    },
    {
      "epoch": 0.298319783197832,
      "step": 1376,
      "training_loss": 6.985009670257568
    },
    {
      "epoch": 0.298319783197832,
      "step": 1376,
      "training_loss": 7.577392101287842
    },
    {
      "epoch": 0.29853658536585365,
      "step": 1377,
      "training_loss": 7.68538761138916
    },
    {
      "epoch": 0.29853658536585365,
      "step": 1377,
      "training_loss": 7.098452568054199
    },
    {
      "epoch": 0.29853658536585365,
      "step": 1377,
      "training_loss": 5.7065534591674805
    },
    {
      "epoch": 0.29853658536585365,
      "step": 1377,
      "training_loss": 6.18903923034668
    },
    {
      "epoch": 0.2987533875338753,
      "step": 1378,
      "training_loss": 7.139894008636475
    },
    {
      "epoch": 0.2987533875338753,
      "step": 1378,
      "training_loss": 4.7996015548706055
    },
    {
      "epoch": 0.2987533875338753,
      "step": 1378,
      "training_loss": 6.853250503540039
    },
    {
      "epoch": 0.2987533875338753,
      "step": 1378,
      "training_loss": 6.596813678741455
    },
    {
      "epoch": 0.29897018970189704,
      "step": 1379,
      "training_loss": 8.015609741210938
    },
    {
      "epoch": 0.29897018970189704,
      "step": 1379,
      "training_loss": 6.114710330963135
    },
    {
      "epoch": 0.29897018970189704,
      "step": 1379,
      "training_loss": 6.386952877044678
    },
    {
      "epoch": 0.29897018970189704,
      "step": 1379,
      "training_loss": 7.221292495727539
    },
    {
      "epoch": 0.2991869918699187,
      "grad_norm": 13.035935401916504,
      "learning_rate": 1e-05,
      "loss": 6.7583,
      "step": 1380
    },
    {
      "epoch": 0.2991869918699187,
      "step": 1380,
      "training_loss": 7.888249397277832
    },
    {
      "epoch": 0.2991869918699187,
      "step": 1380,
      "training_loss": 7.865950107574463
    },
    {
      "epoch": 0.2991869918699187,
      "step": 1380,
      "training_loss": 7.622276306152344
    },
    {
      "epoch": 0.2991869918699187,
      "step": 1380,
      "training_loss": 5.8083086013793945
    },
    {
      "epoch": 0.2994037940379404,
      "step": 1381,
      "training_loss": 7.886032581329346
    },
    {
      "epoch": 0.2994037940379404,
      "step": 1381,
      "training_loss": 5.771447658538818
    },
    {
      "epoch": 0.2994037940379404,
      "step": 1381,
      "training_loss": 7.9803466796875
    },
    {
      "epoch": 0.2994037940379404,
      "step": 1381,
      "training_loss": 6.093290328979492
    },
    {
      "epoch": 0.29962059620596204,
      "step": 1382,
      "training_loss": 6.862654685974121
    },
    {
      "epoch": 0.29962059620596204,
      "step": 1382,
      "training_loss": 7.529066562652588
    },
    {
      "epoch": 0.29962059620596204,
      "step": 1382,
      "training_loss": 7.836895942687988
    },
    {
      "epoch": 0.29962059620596204,
      "step": 1382,
      "training_loss": 6.23337459564209
    },
    {
      "epoch": 0.29983739837398377,
      "step": 1383,
      "training_loss": 6.84740686416626
    },
    {
      "epoch": 0.29983739837398377,
      "step": 1383,
      "training_loss": 5.606608867645264
    },
    {
      "epoch": 0.29983739837398377,
      "step": 1383,
      "training_loss": 8.038259506225586
    },
    {
      "epoch": 0.29983739837398377,
      "step": 1383,
      "training_loss": 5.8188982009887695
    },
    {
      "epoch": 0.30005420054200543,
      "grad_norm": 9.299691200256348,
      "learning_rate": 1e-05,
      "loss": 6.9806,
      "step": 1384
    },
    {
      "epoch": 0.30005420054200543,
      "step": 1384,
      "training_loss": 7.616774559020996
    },
    {
      "epoch": 0.30005420054200543,
      "step": 1384,
      "training_loss": 6.244619369506836
    },
    {
      "epoch": 0.30005420054200543,
      "step": 1384,
      "training_loss": 5.866208553314209
    },
    {
      "epoch": 0.30005420054200543,
      "step": 1384,
      "training_loss": 7.60125207901001
    },
    {
      "epoch": 0.3002710027100271,
      "step": 1385,
      "training_loss": 6.241443157196045
    },
    {
      "epoch": 0.3002710027100271,
      "step": 1385,
      "training_loss": 7.052032947540283
    },
    {
      "epoch": 0.3002710027100271,
      "step": 1385,
      "training_loss": 7.680506706237793
    },
    {
      "epoch": 0.3002710027100271,
      "step": 1385,
      "training_loss": 6.904367446899414
    },
    {
      "epoch": 0.30048780487804877,
      "step": 1386,
      "training_loss": 6.455206394195557
    },
    {
      "epoch": 0.30048780487804877,
      "step": 1386,
      "training_loss": 7.218884468078613
    },
    {
      "epoch": 0.30048780487804877,
      "step": 1386,
      "training_loss": 8.393888473510742
    },
    {
      "epoch": 0.30048780487804877,
      "step": 1386,
      "training_loss": 6.222735404968262
    },
    {
      "epoch": 0.30070460704607044,
      "step": 1387,
      "training_loss": 8.01293659210205
    },
    {
      "epoch": 0.30070460704607044,
      "step": 1387,
      "training_loss": 6.5095977783203125
    },
    {
      "epoch": 0.30070460704607044,
      "step": 1387,
      "training_loss": 8.192197799682617
    },
    {
      "epoch": 0.30070460704607044,
      "step": 1387,
      "training_loss": 7.696662902832031
    },
    {
      "epoch": 0.30092140921409216,
      "grad_norm": 12.596505165100098,
      "learning_rate": 1e-05,
      "loss": 7.1193,
      "step": 1388
    },
    {
      "epoch": 0.30092140921409216,
      "step": 1388,
      "training_loss": 6.2796735763549805
    },
    {
      "epoch": 0.30092140921409216,
      "step": 1388,
      "training_loss": 5.602993965148926
    },
    {
      "epoch": 0.30092140921409216,
      "step": 1388,
      "training_loss": 7.855165958404541
    },
    {
      "epoch": 0.30092140921409216,
      "step": 1388,
      "training_loss": 6.567735195159912
    },
    {
      "epoch": 0.30113821138211383,
      "step": 1389,
      "training_loss": 6.039257526397705
    },
    {
      "epoch": 0.30113821138211383,
      "step": 1389,
      "training_loss": 6.491916656494141
    },
    {
      "epoch": 0.30113821138211383,
      "step": 1389,
      "training_loss": 7.013677597045898
    },
    {
      "epoch": 0.30113821138211383,
      "step": 1389,
      "training_loss": 7.5685296058654785
    },
    {
      "epoch": 0.3013550135501355,
      "step": 1390,
      "training_loss": 5.985456943511963
    },
    {
      "epoch": 0.3013550135501355,
      "step": 1390,
      "training_loss": 7.040940284729004
    },
    {
      "epoch": 0.3013550135501355,
      "step": 1390,
      "training_loss": 6.3586835861206055
    },
    {
      "epoch": 0.3013550135501355,
      "step": 1390,
      "training_loss": 7.661607265472412
    },
    {
      "epoch": 0.30157181571815717,
      "step": 1391,
      "training_loss": 7.847589492797852
    },
    {
      "epoch": 0.30157181571815717,
      "step": 1391,
      "training_loss": 7.510908126831055
    },
    {
      "epoch": 0.30157181571815717,
      "step": 1391,
      "training_loss": 7.205118656158447
    },
    {
      "epoch": 0.30157181571815717,
      "step": 1391,
      "training_loss": 6.293356895446777
    },
    {
      "epoch": 0.3017886178861789,
      "grad_norm": 11.011350631713867,
      "learning_rate": 1e-05,
      "loss": 6.8327,
      "step": 1392
    },
    {
      "epoch": 0.3017886178861789,
      "step": 1392,
      "training_loss": 7.618500232696533
    },
    {
      "epoch": 0.3017886178861789,
      "step": 1392,
      "training_loss": 8.685961723327637
    },
    {
      "epoch": 0.3017886178861789,
      "step": 1392,
      "training_loss": 6.603429317474365
    },
    {
      "epoch": 0.3017886178861789,
      "step": 1392,
      "training_loss": 6.9169511795043945
    },
    {
      "epoch": 0.30200542005420056,
      "step": 1393,
      "training_loss": 8.606983184814453
    },
    {
      "epoch": 0.30200542005420056,
      "step": 1393,
      "training_loss": 6.6667890548706055
    },
    {
      "epoch": 0.30200542005420056,
      "step": 1393,
      "training_loss": 5.856312274932861
    },
    {
      "epoch": 0.30200542005420056,
      "step": 1393,
      "training_loss": 6.630408763885498
    },
    {
      "epoch": 0.3022222222222222,
      "step": 1394,
      "training_loss": 7.250106334686279
    },
    {
      "epoch": 0.3022222222222222,
      "step": 1394,
      "training_loss": 7.8623833656311035
    },
    {
      "epoch": 0.3022222222222222,
      "step": 1394,
      "training_loss": 6.544172286987305
    },
    {
      "epoch": 0.3022222222222222,
      "step": 1394,
      "training_loss": 7.752744674682617
    },
    {
      "epoch": 0.3024390243902439,
      "step": 1395,
      "training_loss": 5.4434003829956055
    },
    {
      "epoch": 0.3024390243902439,
      "step": 1395,
      "training_loss": 7.390340328216553
    },
    {
      "epoch": 0.3024390243902439,
      "step": 1395,
      "training_loss": 5.347696304321289
    },
    {
      "epoch": 0.3024390243902439,
      "step": 1395,
      "training_loss": 7.026673316955566
    },
    {
      "epoch": 0.30265582655826556,
      "grad_norm": 15.36958122253418,
      "learning_rate": 1e-05,
      "loss": 7.0127,
      "step": 1396
    },
    {
      "epoch": 0.30265582655826556,
      "step": 1396,
      "training_loss": 6.992153644561768
    },
    {
      "epoch": 0.30265582655826556,
      "step": 1396,
      "training_loss": 5.949040412902832
    },
    {
      "epoch": 0.30265582655826556,
      "step": 1396,
      "training_loss": 6.59953498840332
    },
    {
      "epoch": 0.30265582655826556,
      "step": 1396,
      "training_loss": 7.455568790435791
    },
    {
      "epoch": 0.3028726287262873,
      "step": 1397,
      "training_loss": 7.74334716796875
    },
    {
      "epoch": 0.3028726287262873,
      "step": 1397,
      "training_loss": 6.146509647369385
    },
    {
      "epoch": 0.3028726287262873,
      "step": 1397,
      "training_loss": 8.783475875854492
    },
    {
      "epoch": 0.3028726287262873,
      "step": 1397,
      "training_loss": 6.798627853393555
    },
    {
      "epoch": 0.30308943089430895,
      "step": 1398,
      "training_loss": 8.551275253295898
    },
    {
      "epoch": 0.30308943089430895,
      "step": 1398,
      "training_loss": 7.110095977783203
    },
    {
      "epoch": 0.30308943089430895,
      "step": 1398,
      "training_loss": 6.742674827575684
    },
    {
      "epoch": 0.30308943089430895,
      "step": 1398,
      "training_loss": 6.710419654846191
    },
    {
      "epoch": 0.3033062330623306,
      "step": 1399,
      "training_loss": 5.862302780151367
    },
    {
      "epoch": 0.3033062330623306,
      "step": 1399,
      "training_loss": 6.996175289154053
    },
    {
      "epoch": 0.3033062330623306,
      "step": 1399,
      "training_loss": 7.88018274307251
    },
    {
      "epoch": 0.3033062330623306,
      "step": 1399,
      "training_loss": 7.524196624755859
    },
    {
      "epoch": 0.3035230352303523,
      "grad_norm": 14.112421035766602,
      "learning_rate": 1e-05,
      "loss": 7.1153,
      "step": 1400
    },
    {
      "epoch": 0.3035230352303523,
      "step": 1400,
      "training_loss": 5.992388725280762
    },
    {
      "epoch": 0.3035230352303523,
      "step": 1400,
      "training_loss": 7.210493564605713
    },
    {
      "epoch": 0.3035230352303523,
      "step": 1400,
      "training_loss": 6.33189058303833
    },
    {
      "epoch": 0.3035230352303523,
      "step": 1400,
      "training_loss": 6.761303424835205
    },
    {
      "epoch": 0.30373983739837396,
      "step": 1401,
      "training_loss": 7.101006031036377
    },
    {
      "epoch": 0.30373983739837396,
      "step": 1401,
      "training_loss": 6.568661689758301
    },
    {
      "epoch": 0.30373983739837396,
      "step": 1401,
      "training_loss": 7.2070512771606445
    },
    {
      "epoch": 0.30373983739837396,
      "step": 1401,
      "training_loss": 7.6570587158203125
    },
    {
      "epoch": 0.3039566395663957,
      "step": 1402,
      "training_loss": 6.533921718597412
    },
    {
      "epoch": 0.3039566395663957,
      "step": 1402,
      "training_loss": 6.505926609039307
    },
    {
      "epoch": 0.3039566395663957,
      "step": 1402,
      "training_loss": 7.503663539886475
    },
    {
      "epoch": 0.3039566395663957,
      "step": 1402,
      "training_loss": 6.131793022155762
    },
    {
      "epoch": 0.30417344173441735,
      "step": 1403,
      "training_loss": 6.460870742797852
    },
    {
      "epoch": 0.30417344173441735,
      "step": 1403,
      "training_loss": 5.904085159301758
    },
    {
      "epoch": 0.30417344173441735,
      "step": 1403,
      "training_loss": 6.846635341644287
    },
    {
      "epoch": 0.30417344173441735,
      "step": 1403,
      "training_loss": 6.712334632873535
    },
    {
      "epoch": 0.304390243902439,
      "grad_norm": 12.288518905639648,
      "learning_rate": 1e-05,
      "loss": 6.7143,
      "step": 1404
    },
    {
      "epoch": 0.304390243902439,
      "step": 1404,
      "training_loss": 6.981381416320801
    },
    {
      "epoch": 0.304390243902439,
      "step": 1404,
      "training_loss": 7.826851844787598
    },
    {
      "epoch": 0.304390243902439,
      "step": 1404,
      "training_loss": 6.63201379776001
    },
    {
      "epoch": 0.304390243902439,
      "step": 1404,
      "training_loss": 7.499067306518555
    },
    {
      "epoch": 0.3046070460704607,
      "step": 1405,
      "training_loss": 6.203002452850342
    },
    {
      "epoch": 0.3046070460704607,
      "step": 1405,
      "training_loss": 7.393346309661865
    },
    {
      "epoch": 0.3046070460704607,
      "step": 1405,
      "training_loss": 6.631730556488037
    },
    {
      "epoch": 0.3046070460704607,
      "step": 1405,
      "training_loss": 6.927936553955078
    },
    {
      "epoch": 0.3048238482384824,
      "step": 1406,
      "training_loss": 7.355736255645752
    },
    {
      "epoch": 0.3048238482384824,
      "step": 1406,
      "training_loss": 6.747978687286377
    },
    {
      "epoch": 0.3048238482384824,
      "step": 1406,
      "training_loss": 6.135331153869629
    },
    {
      "epoch": 0.3048238482384824,
      "step": 1406,
      "training_loss": 7.348727226257324
    },
    {
      "epoch": 0.3050406504065041,
      "step": 1407,
      "training_loss": 7.295470714569092
    },
    {
      "epoch": 0.3050406504065041,
      "step": 1407,
      "training_loss": 6.6481032371521
    },
    {
      "epoch": 0.3050406504065041,
      "step": 1407,
      "training_loss": 7.4384765625
    },
    {
      "epoch": 0.3050406504065041,
      "step": 1407,
      "training_loss": 6.247075080871582
    },
    {
      "epoch": 0.30525745257452574,
      "grad_norm": 12.232086181640625,
      "learning_rate": 1e-05,
      "loss": 6.957,
      "step": 1408
    },
    {
      "epoch": 0.30525745257452574,
      "step": 1408,
      "training_loss": 7.263253688812256
    },
    {
      "epoch": 0.30525745257452574,
      "step": 1408,
      "training_loss": 6.4109787940979
    },
    {
      "epoch": 0.30525745257452574,
      "step": 1408,
      "training_loss": 8.182090759277344
    },
    {
      "epoch": 0.30525745257452574,
      "step": 1408,
      "training_loss": 6.4318084716796875
    },
    {
      "epoch": 0.3054742547425474,
      "step": 1409,
      "training_loss": 5.708280086517334
    },
    {
      "epoch": 0.3054742547425474,
      "step": 1409,
      "training_loss": 7.526701927185059
    },
    {
      "epoch": 0.3054742547425474,
      "step": 1409,
      "training_loss": 7.864998817443848
    },
    {
      "epoch": 0.3054742547425474,
      "step": 1409,
      "training_loss": 5.122880935668945
    },
    {
      "epoch": 0.3056910569105691,
      "step": 1410,
      "training_loss": 6.540657043457031
    },
    {
      "epoch": 0.3056910569105691,
      "step": 1410,
      "training_loss": 6.9968180656433105
    },
    {
      "epoch": 0.3056910569105691,
      "step": 1410,
      "training_loss": 8.707228660583496
    },
    {
      "epoch": 0.3056910569105691,
      "step": 1410,
      "training_loss": 7.4557061195373535
    },
    {
      "epoch": 0.3059078590785908,
      "step": 1411,
      "training_loss": 7.013806343078613
    },
    {
      "epoch": 0.3059078590785908,
      "step": 1411,
      "training_loss": 7.476376056671143
    },
    {
      "epoch": 0.3059078590785908,
      "step": 1411,
      "training_loss": 6.754803657531738
    },
    {
      "epoch": 0.3059078590785908,
      "step": 1411,
      "training_loss": 7.189484119415283
    },
    {
      "epoch": 0.3061246612466125,
      "grad_norm": 15.738781929016113,
      "learning_rate": 1e-05,
      "loss": 7.0404,
      "step": 1412
    },
    {
      "epoch": 0.3061246612466125,
      "step": 1412,
      "training_loss": 8.292672157287598
    },
    {
      "epoch": 0.3061246612466125,
      "step": 1412,
      "training_loss": 6.799072265625
    },
    {
      "epoch": 0.3061246612466125,
      "step": 1412,
      "training_loss": 7.051100730895996
    },
    {
      "epoch": 0.3061246612466125,
      "step": 1412,
      "training_loss": 7.666577339172363
    },
    {
      "epoch": 0.30634146341463414,
      "step": 1413,
      "training_loss": 6.065462112426758
    },
    {
      "epoch": 0.30634146341463414,
      "step": 1413,
      "training_loss": 7.1452813148498535
    },
    {
      "epoch": 0.30634146341463414,
      "step": 1413,
      "training_loss": 7.011553764343262
    },
    {
      "epoch": 0.30634146341463414,
      "step": 1413,
      "training_loss": 6.85711145401001
    },
    {
      "epoch": 0.3065582655826558,
      "step": 1414,
      "training_loss": 7.57594633102417
    },
    {
      "epoch": 0.3065582655826558,
      "step": 1414,
      "training_loss": 8.051820755004883
    },
    {
      "epoch": 0.3065582655826558,
      "step": 1414,
      "training_loss": 6.132729530334473
    },
    {
      "epoch": 0.3065582655826558,
      "step": 1414,
      "training_loss": 6.795983791351318
    },
    {
      "epoch": 0.30677506775067753,
      "step": 1415,
      "training_loss": 5.390820026397705
    },
    {
      "epoch": 0.30677506775067753,
      "step": 1415,
      "training_loss": 7.186334609985352
    },
    {
      "epoch": 0.30677506775067753,
      "step": 1415,
      "training_loss": 7.031548023223877
    },
    {
      "epoch": 0.30677506775067753,
      "step": 1415,
      "training_loss": 6.626773834228516
    },
    {
      "epoch": 0.3069918699186992,
      "grad_norm": 8.29749584197998,
      "learning_rate": 1e-05,
      "loss": 6.98,
      "step": 1416
    },
    {
      "epoch": 0.3069918699186992,
      "step": 1416,
      "training_loss": 6.3575873374938965
    },
    {
      "epoch": 0.3069918699186992,
      "step": 1416,
      "training_loss": 6.601925373077393
    },
    {
      "epoch": 0.3069918699186992,
      "step": 1416,
      "training_loss": 7.452220439910889
    },
    {
      "epoch": 0.3069918699186992,
      "step": 1416,
      "training_loss": 6.345230579376221
    },
    {
      "epoch": 0.30720867208672087,
      "step": 1417,
      "training_loss": 7.616097927093506
    },
    {
      "epoch": 0.30720867208672087,
      "step": 1417,
      "training_loss": 5.663667678833008
    },
    {
      "epoch": 0.30720867208672087,
      "step": 1417,
      "training_loss": 7.896946907043457
    },
    {
      "epoch": 0.30720867208672087,
      "step": 1417,
      "training_loss": 7.977719783782959
    },
    {
      "epoch": 0.30742547425474254,
      "step": 1418,
      "training_loss": 4.830687999725342
    },
    {
      "epoch": 0.30742547425474254,
      "step": 1418,
      "training_loss": 7.663933277130127
    },
    {
      "epoch": 0.30742547425474254,
      "step": 1418,
      "training_loss": 7.446840763092041
    },
    {
      "epoch": 0.30742547425474254,
      "step": 1418,
      "training_loss": 8.051241874694824
    },
    {
      "epoch": 0.3076422764227642,
      "step": 1419,
      "training_loss": 6.585836887359619
    },
    {
      "epoch": 0.3076422764227642,
      "step": 1419,
      "training_loss": 5.4851155281066895
    },
    {
      "epoch": 0.3076422764227642,
      "step": 1419,
      "training_loss": 6.524850368499756
    },
    {
      "epoch": 0.3076422764227642,
      "step": 1419,
      "training_loss": 7.347733974456787
    },
    {
      "epoch": 0.30785907859078593,
      "grad_norm": 8.4509916305542,
      "learning_rate": 1e-05,
      "loss": 6.8655,
      "step": 1420
    },
    {
      "epoch": 0.30785907859078593,
      "step": 1420,
      "training_loss": 7.645861625671387
    },
    {
      "epoch": 0.30785907859078593,
      "step": 1420,
      "training_loss": 6.119181156158447
    },
    {
      "epoch": 0.30785907859078593,
      "step": 1420,
      "training_loss": 7.079891681671143
    },
    {
      "epoch": 0.30785907859078593,
      "step": 1420,
      "training_loss": 6.737837791442871
    },
    {
      "epoch": 0.3080758807588076,
      "step": 1421,
      "training_loss": 6.554779052734375
    },
    {
      "epoch": 0.3080758807588076,
      "step": 1421,
      "training_loss": 6.771641254425049
    },
    {
      "epoch": 0.3080758807588076,
      "step": 1421,
      "training_loss": 6.011497974395752
    },
    {
      "epoch": 0.3080758807588076,
      "step": 1421,
      "training_loss": 7.1961445808410645
    },
    {
      "epoch": 0.30829268292682926,
      "step": 1422,
      "training_loss": 7.413820266723633
    },
    {
      "epoch": 0.30829268292682926,
      "step": 1422,
      "training_loss": 6.713561058044434
    },
    {
      "epoch": 0.30829268292682926,
      "step": 1422,
      "training_loss": 8.936905860900879
    },
    {
      "epoch": 0.30829268292682926,
      "step": 1422,
      "training_loss": 8.005341529846191
    },
    {
      "epoch": 0.30850948509485093,
      "step": 1423,
      "training_loss": 6.128911018371582
    },
    {
      "epoch": 0.30850948509485093,
      "step": 1423,
      "training_loss": 5.007757663726807
    },
    {
      "epoch": 0.30850948509485093,
      "step": 1423,
      "training_loss": 6.544111251831055
    },
    {
      "epoch": 0.30850948509485093,
      "step": 1423,
      "training_loss": 7.572416305541992
    },
    {
      "epoch": 0.30872628726287266,
      "grad_norm": 16.256132125854492,
      "learning_rate": 1e-05,
      "loss": 6.9025,
      "step": 1424
    },
    {
      "epoch": 0.30872628726287266,
      "step": 1424,
      "training_loss": 7.195235729217529
    },
    {
      "epoch": 0.30872628726287266,
      "step": 1424,
      "training_loss": 7.7447123527526855
    },
    {
      "epoch": 0.30872628726287266,
      "step": 1424,
      "training_loss": 7.836194038391113
    },
    {
      "epoch": 0.30872628726287266,
      "step": 1424,
      "training_loss": 6.735487937927246
    },
    {
      "epoch": 0.3089430894308943,
      "step": 1425,
      "training_loss": 7.170560359954834
    },
    {
      "epoch": 0.3089430894308943,
      "step": 1425,
      "training_loss": 6.743103981018066
    },
    {
      "epoch": 0.3089430894308943,
      "step": 1425,
      "training_loss": 5.835610389709473
    },
    {
      "epoch": 0.3089430894308943,
      "step": 1425,
      "training_loss": 6.476447105407715
    },
    {
      "epoch": 0.309159891598916,
      "step": 1426,
      "training_loss": 5.512821197509766
    },
    {
      "epoch": 0.309159891598916,
      "step": 1426,
      "training_loss": 7.240321636199951
    },
    {
      "epoch": 0.309159891598916,
      "step": 1426,
      "training_loss": 5.621574878692627
    },
    {
      "epoch": 0.309159891598916,
      "step": 1426,
      "training_loss": 7.67365837097168
    },
    {
      "epoch": 0.30937669376693766,
      "step": 1427,
      "training_loss": 7.055909633636475
    },
    {
      "epoch": 0.30937669376693766,
      "step": 1427,
      "training_loss": 7.57295036315918
    },
    {
      "epoch": 0.30937669376693766,
      "step": 1427,
      "training_loss": 7.571426868438721
    },
    {
      "epoch": 0.30937669376693766,
      "step": 1427,
      "training_loss": 6.748170375823975
    },
    {
      "epoch": 0.30959349593495933,
      "grad_norm": 9.082473754882812,
      "learning_rate": 1e-05,
      "loss": 6.9209,
      "step": 1428
    },
    {
      "epoch": 0.30959349593495933,
      "step": 1428,
      "training_loss": 8.947654724121094
    },
    {
      "epoch": 0.30959349593495933,
      "step": 1428,
      "training_loss": 7.081883430480957
    },
    {
      "epoch": 0.30959349593495933,
      "step": 1428,
      "training_loss": 7.192691802978516
    },
    {
      "epoch": 0.30959349593495933,
      "step": 1428,
      "training_loss": 7.025510787963867
    },
    {
      "epoch": 0.30981029810298105,
      "step": 1429,
      "training_loss": 7.269570350646973
    },
    {
      "epoch": 0.30981029810298105,
      "step": 1429,
      "training_loss": 6.722842693328857
    },
    {
      "epoch": 0.30981029810298105,
      "step": 1429,
      "training_loss": 7.818140506744385
    },
    {
      "epoch": 0.30981029810298105,
      "step": 1429,
      "training_loss": 7.122247219085693
    },
    {
      "epoch": 0.3100271002710027,
      "step": 1430,
      "training_loss": 6.903950214385986
    },
    {
      "epoch": 0.3100271002710027,
      "step": 1430,
      "training_loss": 8.25213623046875
    },
    {
      "epoch": 0.3100271002710027,
      "step": 1430,
      "training_loss": 5.994275093078613
    },
    {
      "epoch": 0.3100271002710027,
      "step": 1430,
      "training_loss": 7.0283613204956055
    },
    {
      "epoch": 0.3102439024390244,
      "step": 1431,
      "training_loss": 5.533544063568115
    },
    {
      "epoch": 0.3102439024390244,
      "step": 1431,
      "training_loss": 6.837000846862793
    },
    {
      "epoch": 0.3102439024390244,
      "step": 1431,
      "training_loss": 5.196258068084717
    },
    {
      "epoch": 0.3102439024390244,
      "step": 1431,
      "training_loss": 6.8776631355285645
    },
    {
      "epoch": 0.31046070460704606,
      "grad_norm": 9.4660062789917,
      "learning_rate": 1e-05,
      "loss": 6.9877,
      "step": 1432
    },
    {
      "epoch": 0.31046070460704606,
      "step": 1432,
      "training_loss": 6.038643836975098
    },
    {
      "epoch": 0.31046070460704606,
      "step": 1432,
      "training_loss": 7.3765058517456055
    },
    {
      "epoch": 0.31046070460704606,
      "step": 1432,
      "training_loss": 6.340682506561279
    },
    {
      "epoch": 0.31046070460704606,
      "step": 1432,
      "training_loss": 6.101675987243652
    },
    {
      "epoch": 0.3106775067750677,
      "step": 1433,
      "training_loss": 4.499532699584961
    },
    {
      "epoch": 0.3106775067750677,
      "step": 1433,
      "training_loss": 7.082478046417236
    },
    {
      "epoch": 0.3106775067750677,
      "step": 1433,
      "training_loss": 8.09986400604248
    },
    {
      "epoch": 0.3106775067750677,
      "step": 1433,
      "training_loss": 6.814929485321045
    },
    {
      "epoch": 0.31089430894308945,
      "step": 1434,
      "training_loss": 7.543132781982422
    },
    {
      "epoch": 0.31089430894308945,
      "step": 1434,
      "training_loss": 6.280941009521484
    },
    {
      "epoch": 0.31089430894308945,
      "step": 1434,
      "training_loss": 6.535815715789795
    },
    {
      "epoch": 0.31089430894308945,
      "step": 1434,
      "training_loss": 7.031418323516846
    },
    {
      "epoch": 0.3111111111111111,
      "step": 1435,
      "training_loss": 6.725866794586182
    },
    {
      "epoch": 0.3111111111111111,
      "step": 1435,
      "training_loss": 7.475240230560303
    },
    {
      "epoch": 0.3111111111111111,
      "step": 1435,
      "training_loss": 7.3349504470825195
    },
    {
      "epoch": 0.3111111111111111,
      "step": 1435,
      "training_loss": 5.909976482391357
    },
    {
      "epoch": 0.3113279132791328,
      "grad_norm": 10.810956001281738,
      "learning_rate": 1e-05,
      "loss": 6.6995,
      "step": 1436
    },
    {
      "epoch": 0.3113279132791328,
      "step": 1436,
      "training_loss": 6.327454090118408
    },
    {
      "epoch": 0.3113279132791328,
      "step": 1436,
      "training_loss": 6.766556262969971
    },
    {
      "epoch": 0.3113279132791328,
      "step": 1436,
      "training_loss": 6.3729658126831055
    },
    {
      "epoch": 0.3113279132791328,
      "step": 1436,
      "training_loss": 7.490363597869873
    },
    {
      "epoch": 0.31154471544715445,
      "step": 1437,
      "training_loss": 7.372931957244873
    },
    {
      "epoch": 0.31154471544715445,
      "step": 1437,
      "training_loss": 7.885336875915527
    },
    {
      "epoch": 0.31154471544715445,
      "step": 1437,
      "training_loss": 5.889808177947998
    },
    {
      "epoch": 0.31154471544715445,
      "step": 1437,
      "training_loss": 7.572299480438232
    },
    {
      "epoch": 0.3117615176151762,
      "step": 1438,
      "training_loss": 5.253669738769531
    },
    {
      "epoch": 0.3117615176151762,
      "step": 1438,
      "training_loss": 4.77220344543457
    },
    {
      "epoch": 0.3117615176151762,
      "step": 1438,
      "training_loss": 7.3003249168396
    },
    {
      "epoch": 0.3117615176151762,
      "step": 1438,
      "training_loss": 7.25508451461792
    },
    {
      "epoch": 0.31197831978319784,
      "step": 1439,
      "training_loss": 7.625039100646973
    },
    {
      "epoch": 0.31197831978319784,
      "step": 1439,
      "training_loss": 5.542277812957764
    },
    {
      "epoch": 0.31197831978319784,
      "step": 1439,
      "training_loss": 6.593131065368652
    },
    {
      "epoch": 0.31197831978319784,
      "step": 1439,
      "training_loss": 7.420450210571289
    },
    {
      "epoch": 0.3121951219512195,
      "grad_norm": 11.003679275512695,
      "learning_rate": 1e-05,
      "loss": 6.715,
      "step": 1440
    },
    {
      "epoch": 0.3121951219512195,
      "step": 1440,
      "training_loss": 7.196735858917236
    },
    {
      "epoch": 0.3121951219512195,
      "step": 1440,
      "training_loss": 7.127843379974365
    },
    {
      "epoch": 0.3121951219512195,
      "step": 1440,
      "training_loss": 5.491322040557861
    },
    {
      "epoch": 0.3121951219512195,
      "step": 1440,
      "training_loss": 6.404238224029541
    },
    {
      "epoch": 0.3124119241192412,
      "step": 1441,
      "training_loss": 7.79624080657959
    },
    {
      "epoch": 0.3124119241192412,
      "step": 1441,
      "training_loss": 6.137145519256592
    },
    {
      "epoch": 0.3124119241192412,
      "step": 1441,
      "training_loss": 7.42323112487793
    },
    {
      "epoch": 0.3124119241192412,
      "step": 1441,
      "training_loss": 6.737654685974121
    },
    {
      "epoch": 0.31262872628726285,
      "step": 1442,
      "training_loss": 5.563323497772217
    },
    {
      "epoch": 0.31262872628726285,
      "step": 1442,
      "training_loss": 7.361433506011963
    },
    {
      "epoch": 0.31262872628726285,
      "step": 1442,
      "training_loss": 7.376361846923828
    },
    {
      "epoch": 0.31262872628726285,
      "step": 1442,
      "training_loss": 7.301097393035889
    },
    {
      "epoch": 0.31284552845528457,
      "step": 1443,
      "training_loss": 6.715987682342529
    },
    {
      "epoch": 0.31284552845528457,
      "step": 1443,
      "training_loss": 7.577033519744873
    },
    {
      "epoch": 0.31284552845528457,
      "step": 1443,
      "training_loss": 7.312084197998047
    },
    {
      "epoch": 0.31284552845528457,
      "step": 1443,
      "training_loss": 7.304893493652344
    },
    {
      "epoch": 0.31306233062330624,
      "grad_norm": 13.495100021362305,
      "learning_rate": 1e-05,
      "loss": 6.9267,
      "step": 1444
    },
    {
      "epoch": 0.31306233062330624,
      "step": 1444,
      "training_loss": 7.1951680183410645
    },
    {
      "epoch": 0.31306233062330624,
      "step": 1444,
      "training_loss": 7.3375115394592285
    },
    {
      "epoch": 0.31306233062330624,
      "step": 1444,
      "training_loss": 7.264612197875977
    },
    {
      "epoch": 0.31306233062330624,
      "step": 1444,
      "training_loss": 7.721154689788818
    },
    {
      "epoch": 0.3132791327913279,
      "step": 1445,
      "training_loss": 7.054561138153076
    },
    {
      "epoch": 0.3132791327913279,
      "step": 1445,
      "training_loss": 8.185332298278809
    },
    {
      "epoch": 0.3132791327913279,
      "step": 1445,
      "training_loss": 7.015085220336914
    },
    {
      "epoch": 0.3132791327913279,
      "step": 1445,
      "training_loss": 7.509518146514893
    },
    {
      "epoch": 0.3134959349593496,
      "step": 1446,
      "training_loss": 7.873014450073242
    },
    {
      "epoch": 0.3134959349593496,
      "step": 1446,
      "training_loss": 6.107041835784912
    },
    {
      "epoch": 0.3134959349593496,
      "step": 1446,
      "training_loss": 7.505728721618652
    },
    {
      "epoch": 0.3134959349593496,
      "step": 1446,
      "training_loss": 7.5776686668396
    },
    {
      "epoch": 0.3137127371273713,
      "step": 1447,
      "training_loss": 7.2978949546813965
    },
    {
      "epoch": 0.3137127371273713,
      "step": 1447,
      "training_loss": 7.468022346496582
    },
    {
      "epoch": 0.3137127371273713,
      "step": 1447,
      "training_loss": 8.27284049987793
    },
    {
      "epoch": 0.3137127371273713,
      "step": 1447,
      "training_loss": 7.04074239730835
    },
    {
      "epoch": 0.31392953929539297,
      "grad_norm": 12.019756317138672,
      "learning_rate": 1e-05,
      "loss": 7.4016,
      "step": 1448
    },
    {
      "epoch": 0.31392953929539297,
      "step": 1448,
      "training_loss": 7.734931468963623
    },
    {
      "epoch": 0.31392953929539297,
      "step": 1448,
      "training_loss": 5.778878688812256
    },
    {
      "epoch": 0.31392953929539297,
      "step": 1448,
      "training_loss": 7.201860427856445
    },
    {
      "epoch": 0.31392953929539297,
      "step": 1448,
      "training_loss": 7.017196178436279
    },
    {
      "epoch": 0.31414634146341464,
      "step": 1449,
      "training_loss": 7.406534671783447
    },
    {
      "epoch": 0.31414634146341464,
      "step": 1449,
      "training_loss": 7.5368170738220215
    },
    {
      "epoch": 0.31414634146341464,
      "step": 1449,
      "training_loss": 7.59942626953125
    },
    {
      "epoch": 0.31414634146341464,
      "step": 1449,
      "training_loss": 8.965322494506836
    },
    {
      "epoch": 0.3143631436314363,
      "step": 1450,
      "training_loss": 7.203126430511475
    },
    {
      "epoch": 0.3143631436314363,
      "step": 1450,
      "training_loss": 6.716803073883057
    },
    {
      "epoch": 0.3143631436314363,
      "step": 1450,
      "training_loss": 7.687141418457031
    },
    {
      "epoch": 0.3143631436314363,
      "step": 1450,
      "training_loss": 7.343740463256836
    },
    {
      "epoch": 0.31457994579945797,
      "step": 1451,
      "training_loss": 5.106528282165527
    },
    {
      "epoch": 0.31457994579945797,
      "step": 1451,
      "training_loss": 6.790108680725098
    },
    {
      "epoch": 0.31457994579945797,
      "step": 1451,
      "training_loss": 7.62070894241333
    },
    {
      "epoch": 0.31457994579945797,
      "step": 1451,
      "training_loss": 4.73736047744751
    },
    {
      "epoch": 0.3147967479674797,
      "grad_norm": 10.74943733215332,
      "learning_rate": 1e-05,
      "loss": 7.0279,
      "step": 1452
    },
    {
      "epoch": 0.3147967479674797,
      "step": 1452,
      "training_loss": 6.954719543457031
    },
    {
      "epoch": 0.3147967479674797,
      "step": 1452,
      "training_loss": 7.396217346191406
    },
    {
      "epoch": 0.3147967479674797,
      "step": 1452,
      "training_loss": 5.288550853729248
    },
    {
      "epoch": 0.3147967479674797,
      "step": 1452,
      "training_loss": 6.314439296722412
    },
    {
      "epoch": 0.31501355013550136,
      "step": 1453,
      "training_loss": 3.955944061279297
    },
    {
      "epoch": 0.31501355013550136,
      "step": 1453,
      "training_loss": 7.0616679191589355
    },
    {
      "epoch": 0.31501355013550136,
      "step": 1453,
      "training_loss": 6.139200687408447
    },
    {
      "epoch": 0.31501355013550136,
      "step": 1453,
      "training_loss": 6.722194194793701
    },
    {
      "epoch": 0.31523035230352303,
      "step": 1454,
      "training_loss": 7.280148029327393
    },
    {
      "epoch": 0.31523035230352303,
      "step": 1454,
      "training_loss": 7.2019243240356445
    },
    {
      "epoch": 0.31523035230352303,
      "step": 1454,
      "training_loss": 6.922427654266357
    },
    {
      "epoch": 0.31523035230352303,
      "step": 1454,
      "training_loss": 8.802499771118164
    },
    {
      "epoch": 0.3154471544715447,
      "step": 1455,
      "training_loss": 6.642228603363037
    },
    {
      "epoch": 0.3154471544715447,
      "step": 1455,
      "training_loss": 8.066892623901367
    },
    {
      "epoch": 0.3154471544715447,
      "step": 1455,
      "training_loss": 7.02841854095459
    },
    {
      "epoch": 0.3154471544715447,
      "step": 1455,
      "training_loss": 7.496020317077637
    },
    {
      "epoch": 0.3156639566395664,
      "grad_norm": 12.797309875488281,
      "learning_rate": 1e-05,
      "loss": 6.8296,
      "step": 1456
    },
    {
      "epoch": 0.3156639566395664,
      "step": 1456,
      "training_loss": 7.205528736114502
    },
    {
      "epoch": 0.3156639566395664,
      "step": 1456,
      "training_loss": 7.247345924377441
    },
    {
      "epoch": 0.3156639566395664,
      "step": 1456,
      "training_loss": 6.55270528793335
    },
    {
      "epoch": 0.3156639566395664,
      "step": 1456,
      "training_loss": 8.708107948303223
    },
    {
      "epoch": 0.3158807588075881,
      "step": 1457,
      "training_loss": 6.092589378356934
    },
    {
      "epoch": 0.3158807588075881,
      "step": 1457,
      "training_loss": 7.735528469085693
    },
    {
      "epoch": 0.3158807588075881,
      "step": 1457,
      "training_loss": 5.23781681060791
    },
    {
      "epoch": 0.3158807588075881,
      "step": 1457,
      "training_loss": 8.45285701751709
    },
    {
      "epoch": 0.31609756097560976,
      "step": 1458,
      "training_loss": 7.191458702087402
    },
    {
      "epoch": 0.31609756097560976,
      "step": 1458,
      "training_loss": 7.621027946472168
    },
    {
      "epoch": 0.31609756097560976,
      "step": 1458,
      "training_loss": 7.36692476272583
    },
    {
      "epoch": 0.31609756097560976,
      "step": 1458,
      "training_loss": 6.306450843811035
    },
    {
      "epoch": 0.3163143631436314,
      "step": 1459,
      "training_loss": 7.202468395233154
    },
    {
      "epoch": 0.3163143631436314,
      "step": 1459,
      "training_loss": 7.312747478485107
    },
    {
      "epoch": 0.3163143631436314,
      "step": 1459,
      "training_loss": 6.302276611328125
    },
    {
      "epoch": 0.3163143631436314,
      "step": 1459,
      "training_loss": 5.934462070465088
    },
    {
      "epoch": 0.3165311653116531,
      "grad_norm": 11.152758598327637,
      "learning_rate": 1e-05,
      "loss": 7.0294,
      "step": 1460
    },
    {
      "epoch": 0.3165311653116531,
      "step": 1460,
      "training_loss": 5.056506156921387
    },
    {
      "epoch": 0.3165311653116531,
      "step": 1460,
      "training_loss": 7.494815826416016
    },
    {
      "epoch": 0.3165311653116531,
      "step": 1460,
      "training_loss": 8.361584663391113
    },
    {
      "epoch": 0.3165311653116531,
      "step": 1460,
      "training_loss": 6.1710333824157715
    },
    {
      "epoch": 0.3167479674796748,
      "step": 1461,
      "training_loss": 7.0336408615112305
    },
    {
      "epoch": 0.3167479674796748,
      "step": 1461,
      "training_loss": 7.116451740264893
    },
    {
      "epoch": 0.3167479674796748,
      "step": 1461,
      "training_loss": 7.633138179779053
    },
    {
      "epoch": 0.3167479674796748,
      "step": 1461,
      "training_loss": 7.489717483520508
    },
    {
      "epoch": 0.3169647696476965,
      "step": 1462,
      "training_loss": 7.376957893371582
    },
    {
      "epoch": 0.3169647696476965,
      "step": 1462,
      "training_loss": 8.62699031829834
    },
    {
      "epoch": 0.3169647696476965,
      "step": 1462,
      "training_loss": 6.946261405944824
    },
    {
      "epoch": 0.3169647696476965,
      "step": 1462,
      "training_loss": 8.145672798156738
    },
    {
      "epoch": 0.31718157181571816,
      "step": 1463,
      "training_loss": 6.755641937255859
    },
    {
      "epoch": 0.31718157181571816,
      "step": 1463,
      "training_loss": 6.857307434082031
    },
    {
      "epoch": 0.31718157181571816,
      "step": 1463,
      "training_loss": 7.743814945220947
    },
    {
      "epoch": 0.31718157181571816,
      "step": 1463,
      "training_loss": 5.142797946929932
    },
    {
      "epoch": 0.3173983739837398,
      "grad_norm": 10.66592788696289,
      "learning_rate": 1e-05,
      "loss": 7.122,
      "step": 1464
    },
    {
      "epoch": 0.3173983739837398,
      "step": 1464,
      "training_loss": 7.058120250701904
    },
    {
      "epoch": 0.3173983739837398,
      "step": 1464,
      "training_loss": 4.814089298248291
    },
    {
      "epoch": 0.3173983739837398,
      "step": 1464,
      "training_loss": 7.53451681137085
    },
    {
      "epoch": 0.3173983739837398,
      "step": 1464,
      "training_loss": 8.563786506652832
    },
    {
      "epoch": 0.3176151761517615,
      "step": 1465,
      "training_loss": 6.0748772621154785
    },
    {
      "epoch": 0.3176151761517615,
      "step": 1465,
      "training_loss": 8.209442138671875
    },
    {
      "epoch": 0.3176151761517615,
      "step": 1465,
      "training_loss": 7.295759201049805
    },
    {
      "epoch": 0.3176151761517615,
      "step": 1465,
      "training_loss": 6.6278977394104
    },
    {
      "epoch": 0.3178319783197832,
      "step": 1466,
      "training_loss": 8.239376068115234
    },
    {
      "epoch": 0.3178319783197832,
      "step": 1466,
      "training_loss": 6.1779327392578125
    },
    {
      "epoch": 0.3178319783197832,
      "step": 1466,
      "training_loss": 5.073973655700684
    },
    {
      "epoch": 0.3178319783197832,
      "step": 1466,
      "training_loss": 6.142087936401367
    },
    {
      "epoch": 0.3180487804878049,
      "step": 1467,
      "training_loss": 6.691007137298584
    },
    {
      "epoch": 0.3180487804878049,
      "step": 1467,
      "training_loss": 7.284801006317139
    },
    {
      "epoch": 0.3180487804878049,
      "step": 1467,
      "training_loss": 4.924217700958252
    },
    {
      "epoch": 0.3180487804878049,
      "step": 1467,
      "training_loss": 7.509249210357666
    },
    {
      "epoch": 0.31826558265582655,
      "grad_norm": 12.7317533493042,
      "learning_rate": 1e-05,
      "loss": 6.7638,
      "step": 1468
    },
    {
      "epoch": 0.31826558265582655,
      "step": 1468,
      "training_loss": 7.121735572814941
    },
    {
      "epoch": 0.31826558265582655,
      "step": 1468,
      "training_loss": 6.5940728187561035
    },
    {
      "epoch": 0.31826558265582655,
      "step": 1468,
      "training_loss": 6.694380760192871
    },
    {
      "epoch": 0.31826558265582655,
      "step": 1468,
      "training_loss": 7.148159980773926
    },
    {
      "epoch": 0.3184823848238482,
      "step": 1469,
      "training_loss": 6.489004135131836
    },
    {
      "epoch": 0.3184823848238482,
      "step": 1469,
      "training_loss": 8.725561141967773
    },
    {
      "epoch": 0.3184823848238482,
      "step": 1469,
      "training_loss": 6.553973197937012
    },
    {
      "epoch": 0.3184823848238482,
      "step": 1469,
      "training_loss": 6.739065170288086
    },
    {
      "epoch": 0.31869918699186994,
      "step": 1470,
      "training_loss": 8.40593433380127
    },
    {
      "epoch": 0.31869918699186994,
      "step": 1470,
      "training_loss": 5.018245697021484
    },
    {
      "epoch": 0.31869918699186994,
      "step": 1470,
      "training_loss": 6.7906904220581055
    },
    {
      "epoch": 0.31869918699186994,
      "step": 1470,
      "training_loss": 5.705933094024658
    },
    {
      "epoch": 0.3189159891598916,
      "step": 1471,
      "training_loss": 7.093111991882324
    },
    {
      "epoch": 0.3189159891598916,
      "step": 1471,
      "training_loss": 6.2558112144470215
    },
    {
      "epoch": 0.3189159891598916,
      "step": 1471,
      "training_loss": 7.459235668182373
    },
    {
      "epoch": 0.3189159891598916,
      "step": 1471,
      "training_loss": 7.077858924865723
    },
    {
      "epoch": 0.3191327913279133,
      "grad_norm": 8.393448829650879,
      "learning_rate": 1e-05,
      "loss": 6.867,
      "step": 1472
    },
    {
      "epoch": 0.3191327913279133,
      "step": 1472,
      "training_loss": 6.8415846824646
    },
    {
      "epoch": 0.3191327913279133,
      "step": 1472,
      "training_loss": 7.41372537612915
    },
    {
      "epoch": 0.3191327913279133,
      "step": 1472,
      "training_loss": 7.768524646759033
    },
    {
      "epoch": 0.3191327913279133,
      "step": 1472,
      "training_loss": 8.140840530395508
    },
    {
      "epoch": 0.31934959349593495,
      "step": 1473,
      "training_loss": 7.01556396484375
    },
    {
      "epoch": 0.31934959349593495,
      "step": 1473,
      "training_loss": 7.273509979248047
    },
    {
      "epoch": 0.31934959349593495,
      "step": 1473,
      "training_loss": 7.372488021850586
    },
    {
      "epoch": 0.31934959349593495,
      "step": 1473,
      "training_loss": 7.0927019119262695
    },
    {
      "epoch": 0.3195663956639566,
      "step": 1474,
      "training_loss": 7.0975117683410645
    },
    {
      "epoch": 0.3195663956639566,
      "step": 1474,
      "training_loss": 6.357959747314453
    },
    {
      "epoch": 0.3195663956639566,
      "step": 1474,
      "training_loss": 6.45302677154541
    },
    {
      "epoch": 0.3195663956639566,
      "step": 1474,
      "training_loss": 5.716552257537842
    },
    {
      "epoch": 0.31978319783197834,
      "step": 1475,
      "training_loss": 5.994971752166748
    },
    {
      "epoch": 0.31978319783197834,
      "step": 1475,
      "training_loss": 6.062013626098633
    },
    {
      "epoch": 0.31978319783197834,
      "step": 1475,
      "training_loss": 6.632230281829834
    },
    {
      "epoch": 0.31978319783197834,
      "step": 1475,
      "training_loss": 6.090371608734131
    },
    {
      "epoch": 0.32,
      "grad_norm": 12.0968599319458,
      "learning_rate": 1e-05,
      "loss": 6.8327,
      "step": 1476
    },
    {
      "epoch": 0.32,
      "step": 1476,
      "training_loss": 7.816427707672119
    },
    {
      "epoch": 0.32,
      "step": 1476,
      "training_loss": 6.530907154083252
    },
    {
      "epoch": 0.32,
      "step": 1476,
      "training_loss": 7.095999240875244
    },
    {
      "epoch": 0.32,
      "step": 1476,
      "training_loss": 7.457586288452148
    },
    {
      "epoch": 0.3202168021680217,
      "step": 1477,
      "training_loss": 7.580542087554932
    },
    {
      "epoch": 0.3202168021680217,
      "step": 1477,
      "training_loss": 7.009202003479004
    },
    {
      "epoch": 0.3202168021680217,
      "step": 1477,
      "training_loss": 6.824170112609863
    },
    {
      "epoch": 0.3202168021680217,
      "step": 1477,
      "training_loss": 7.42884635925293
    },
    {
      "epoch": 0.32043360433604334,
      "step": 1478,
      "training_loss": 6.802205562591553
    },
    {
      "epoch": 0.32043360433604334,
      "step": 1478,
      "training_loss": 7.146924018859863
    },
    {
      "epoch": 0.32043360433604334,
      "step": 1478,
      "training_loss": 6.786345481872559
    },
    {
      "epoch": 0.32043360433604334,
      "step": 1478,
      "training_loss": 6.708792686462402
    },
    {
      "epoch": 0.32065040650406507,
      "step": 1479,
      "training_loss": 4.88918924331665
    },
    {
      "epoch": 0.32065040650406507,
      "step": 1479,
      "training_loss": 7.047660827636719
    },
    {
      "epoch": 0.32065040650406507,
      "step": 1479,
      "training_loss": 7.239055633544922
    },
    {
      "epoch": 0.32065040650406507,
      "step": 1479,
      "training_loss": 6.109212875366211
    },
    {
      "epoch": 0.32086720867208673,
      "grad_norm": 10.484597206115723,
      "learning_rate": 1e-05,
      "loss": 6.9046,
      "step": 1480
    },
    {
      "epoch": 0.32086720867208673,
      "step": 1480,
      "training_loss": 7.297184467315674
    },
    {
      "epoch": 0.32086720867208673,
      "step": 1480,
      "training_loss": 5.630864143371582
    },
    {
      "epoch": 0.32086720867208673,
      "step": 1480,
      "training_loss": 8.033440589904785
    },
    {
      "epoch": 0.32086720867208673,
      "step": 1480,
      "training_loss": 7.6650238037109375
    },
    {
      "epoch": 0.3210840108401084,
      "step": 1481,
      "training_loss": 6.907752990722656
    },
    {
      "epoch": 0.3210840108401084,
      "step": 1481,
      "training_loss": 6.331697463989258
    },
    {
      "epoch": 0.3210840108401084,
      "step": 1481,
      "training_loss": 5.866756439208984
    },
    {
      "epoch": 0.3210840108401084,
      "step": 1481,
      "training_loss": 6.63816499710083
    },
    {
      "epoch": 0.32130081300813007,
      "step": 1482,
      "training_loss": 7.371243476867676
    },
    {
      "epoch": 0.32130081300813007,
      "step": 1482,
      "training_loss": 6.892439842224121
    },
    {
      "epoch": 0.32130081300813007,
      "step": 1482,
      "training_loss": 7.002565860748291
    },
    {
      "epoch": 0.32130081300813007,
      "step": 1482,
      "training_loss": 5.080848693847656
    },
    {
      "epoch": 0.32151761517615174,
      "step": 1483,
      "training_loss": 7.895777702331543
    },
    {
      "epoch": 0.32151761517615174,
      "step": 1483,
      "training_loss": 7.072266101837158
    },
    {
      "epoch": 0.32151761517615174,
      "step": 1483,
      "training_loss": 8.462347030639648
    },
    {
      "epoch": 0.32151761517615174,
      "step": 1483,
      "training_loss": 6.593554973602295
    },
    {
      "epoch": 0.32173441734417346,
      "grad_norm": 14.166592597961426,
      "learning_rate": 1e-05,
      "loss": 6.9214,
      "step": 1484
    },
    {
      "epoch": 0.32173441734417346,
      "step": 1484,
      "training_loss": 8.11735725402832
    },
    {
      "epoch": 0.32173441734417346,
      "step": 1484,
      "training_loss": 6.722397804260254
    },
    {
      "epoch": 0.32173441734417346,
      "step": 1484,
      "training_loss": 7.57069206237793
    },
    {
      "epoch": 0.32173441734417346,
      "step": 1484,
      "training_loss": 7.467612266540527
    },
    {
      "epoch": 0.32195121951219513,
      "step": 1485,
      "training_loss": 5.948267459869385
    },
    {
      "epoch": 0.32195121951219513,
      "step": 1485,
      "training_loss": 8.04246997833252
    },
    {
      "epoch": 0.32195121951219513,
      "step": 1485,
      "training_loss": 7.620236873626709
    },
    {
      "epoch": 0.32195121951219513,
      "step": 1485,
      "training_loss": 6.42311429977417
    },
    {
      "epoch": 0.3221680216802168,
      "step": 1486,
      "training_loss": 7.935181140899658
    },
    {
      "epoch": 0.3221680216802168,
      "step": 1486,
      "training_loss": 6.865161895751953
    },
    {
      "epoch": 0.3221680216802168,
      "step": 1486,
      "training_loss": 7.7514495849609375
    },
    {
      "epoch": 0.3221680216802168,
      "step": 1486,
      "training_loss": 7.156529426574707
    },
    {
      "epoch": 0.32238482384823847,
      "step": 1487,
      "training_loss": 5.679855823516846
    },
    {
      "epoch": 0.32238482384823847,
      "step": 1487,
      "training_loss": 7.2400898933410645
    },
    {
      "epoch": 0.32238482384823847,
      "step": 1487,
      "training_loss": 6.466557502746582
    },
    {
      "epoch": 0.32238482384823847,
      "step": 1487,
      "training_loss": 9.490569114685059
    },
    {
      "epoch": 0.3226016260162602,
      "grad_norm": 13.826894760131836,
      "learning_rate": 1e-05,
      "loss": 7.2811,
      "step": 1488
    },
    {
      "epoch": 0.3226016260162602,
      "step": 1488,
      "training_loss": 5.125198841094971
    },
    {
      "epoch": 0.3226016260162602,
      "step": 1488,
      "training_loss": 6.309676647186279
    },
    {
      "epoch": 0.3226016260162602,
      "step": 1488,
      "training_loss": 6.505801677703857
    },
    {
      "epoch": 0.3226016260162602,
      "step": 1488,
      "training_loss": 6.370552062988281
    },
    {
      "epoch": 0.32281842818428186,
      "step": 1489,
      "training_loss": 6.993795394897461
    },
    {
      "epoch": 0.32281842818428186,
      "step": 1489,
      "training_loss": 7.037813663482666
    },
    {
      "epoch": 0.32281842818428186,
      "step": 1489,
      "training_loss": 6.305518627166748
    },
    {
      "epoch": 0.32281842818428186,
      "step": 1489,
      "training_loss": 7.9220967292785645
    },
    {
      "epoch": 0.3230352303523035,
      "step": 1490,
      "training_loss": 6.260763645172119
    },
    {
      "epoch": 0.3230352303523035,
      "step": 1490,
      "training_loss": 6.439047813415527
    },
    {
      "epoch": 0.3230352303523035,
      "step": 1490,
      "training_loss": 7.565482139587402
    },
    {
      "epoch": 0.3230352303523035,
      "step": 1490,
      "training_loss": 7.141110897064209
    },
    {
      "epoch": 0.3232520325203252,
      "step": 1491,
      "training_loss": 6.279150009155273
    },
    {
      "epoch": 0.3232520325203252,
      "step": 1491,
      "training_loss": 6.92315149307251
    },
    {
      "epoch": 0.3232520325203252,
      "step": 1491,
      "training_loss": 5.8375935554504395
    },
    {
      "epoch": 0.3232520325203252,
      "step": 1491,
      "training_loss": 6.845547676086426
    },
    {
      "epoch": 0.32346883468834686,
      "grad_norm": 10.373464584350586,
      "learning_rate": 1e-05,
      "loss": 6.6164,
      "step": 1492
    },
    {
      "epoch": 0.32346883468834686,
      "step": 1492,
      "training_loss": 8.311989784240723
    },
    {
      "epoch": 0.32346883468834686,
      "step": 1492,
      "training_loss": 6.872740268707275
    },
    {
      "epoch": 0.32346883468834686,
      "step": 1492,
      "training_loss": 7.22636079788208
    },
    {
      "epoch": 0.32346883468834686,
      "step": 1492,
      "training_loss": 7.666040897369385
    },
    {
      "epoch": 0.3236856368563686,
      "step": 1493,
      "training_loss": 7.332926273345947
    },
    {
      "epoch": 0.3236856368563686,
      "step": 1493,
      "training_loss": 6.362229347229004
    },
    {
      "epoch": 0.3236856368563686,
      "step": 1493,
      "training_loss": 5.685070991516113
    },
    {
      "epoch": 0.3236856368563686,
      "step": 1493,
      "training_loss": 7.823200225830078
    },
    {
      "epoch": 0.32390243902439025,
      "step": 1494,
      "training_loss": 6.93397855758667
    },
    {
      "epoch": 0.32390243902439025,
      "step": 1494,
      "training_loss": 6.794132709503174
    },
    {
      "epoch": 0.32390243902439025,
      "step": 1494,
      "training_loss": 6.21920919418335
    },
    {
      "epoch": 0.32390243902439025,
      "step": 1494,
      "training_loss": 7.527624607086182
    },
    {
      "epoch": 0.3241192411924119,
      "step": 1495,
      "training_loss": 8.047450065612793
    },
    {
      "epoch": 0.3241192411924119,
      "step": 1495,
      "training_loss": 6.52551794052124
    },
    {
      "epoch": 0.3241192411924119,
      "step": 1495,
      "training_loss": 4.773271083831787
    },
    {
      "epoch": 0.3241192411924119,
      "step": 1495,
      "training_loss": 7.087333679199219
    },
    {
      "epoch": 0.3243360433604336,
      "grad_norm": 15.861369132995605,
      "learning_rate": 1e-05,
      "loss": 6.9493,
      "step": 1496
    },
    {
      "epoch": 0.3243360433604336,
      "step": 1496,
      "training_loss": 7.071812152862549
    },
    {
      "epoch": 0.3243360433604336,
      "step": 1496,
      "training_loss": 7.896045207977295
    },
    {
      "epoch": 0.3243360433604336,
      "step": 1496,
      "training_loss": 5.830113410949707
    },
    {
      "epoch": 0.3243360433604336,
      "step": 1496,
      "training_loss": 5.0348124504089355
    },
    {
      "epoch": 0.32455284552845526,
      "step": 1497,
      "training_loss": 7.281698703765869
    },
    {
      "epoch": 0.32455284552845526,
      "step": 1497,
      "training_loss": 7.696913242340088
    },
    {
      "epoch": 0.32455284552845526,
      "step": 1497,
      "training_loss": 7.104727268218994
    },
    {
      "epoch": 0.32455284552845526,
      "step": 1497,
      "training_loss": 7.897377967834473
    },
    {
      "epoch": 0.324769647696477,
      "step": 1498,
      "training_loss": 8.172799110412598
    },
    {
      "epoch": 0.324769647696477,
      "step": 1498,
      "training_loss": 7.417323589324951
    },
    {
      "epoch": 0.324769647696477,
      "step": 1498,
      "training_loss": 6.35821533203125
    },
    {
      "epoch": 0.324769647696477,
      "step": 1498,
      "training_loss": 6.7873101234436035
    },
    {
      "epoch": 0.32498644986449865,
      "step": 1499,
      "training_loss": 7.561492919921875
    },
    {
      "epoch": 0.32498644986449865,
      "step": 1499,
      "training_loss": 6.3956618309021
    },
    {
      "epoch": 0.32498644986449865,
      "step": 1499,
      "training_loss": 7.135068416595459
    },
    {
      "epoch": 0.32498644986449865,
      "step": 1499,
      "training_loss": 6.371339797973633
    },
    {
      "epoch": 0.3252032520325203,
      "grad_norm": 9.242025375366211,
      "learning_rate": 1e-05,
      "loss": 7.0008,
      "step": 1500
    },
    {
      "epoch": 0.3252032520325203,
      "step": 1500,
      "training_loss": 6.9870405197143555
    },
    {
      "epoch": 0.3252032520325203,
      "step": 1500,
      "training_loss": 7.008581161499023
    },
    {
      "epoch": 0.3252032520325203,
      "step": 1500,
      "training_loss": 5.829616546630859
    },
    {
      "epoch": 0.3252032520325203,
      "step": 1500,
      "training_loss": 7.8617844581604
    },
    {
      "epoch": 0.325420054200542,
      "step": 1501,
      "training_loss": 5.29081392288208
    },
    {
      "epoch": 0.325420054200542,
      "step": 1501,
      "training_loss": 7.914795875549316
    },
    {
      "epoch": 0.325420054200542,
      "step": 1501,
      "training_loss": 7.074616432189941
    },
    {
      "epoch": 0.325420054200542,
      "step": 1501,
      "training_loss": 6.704603672027588
    },
    {
      "epoch": 0.3256368563685637,
      "step": 1502,
      "training_loss": 6.480629920959473
    },
    {
      "epoch": 0.3256368563685637,
      "step": 1502,
      "training_loss": 7.285736083984375
    },
    {
      "epoch": 0.3256368563685637,
      "step": 1502,
      "training_loss": 7.554870128631592
    },
    {
      "epoch": 0.3256368563685637,
      "step": 1502,
      "training_loss": 7.025040626525879
    },
    {
      "epoch": 0.3258536585365854,
      "step": 1503,
      "training_loss": 7.271465301513672
    },
    {
      "epoch": 0.3258536585365854,
      "step": 1503,
      "training_loss": 7.406000137329102
    },
    {
      "epoch": 0.3258536585365854,
      "step": 1503,
      "training_loss": 7.4595537185668945
    },
    {
      "epoch": 0.3258536585365854,
      "step": 1503,
      "training_loss": 7.405961513519287
    },
    {
      "epoch": 0.32607046070460705,
      "grad_norm": 18.385435104370117,
      "learning_rate": 1e-05,
      "loss": 7.0351,
      "step": 1504
    },
    {
      "epoch": 0.32607046070460705,
      "step": 1504,
      "training_loss": 6.858046531677246
    },
    {
      "epoch": 0.32607046070460705,
      "step": 1504,
      "training_loss": 6.147611141204834
    },
    {
      "epoch": 0.32607046070460705,
      "step": 1504,
      "training_loss": 5.9822468757629395
    },
    {
      "epoch": 0.32607046070460705,
      "step": 1504,
      "training_loss": 5.231870174407959
    },
    {
      "epoch": 0.3262872628726287,
      "step": 1505,
      "training_loss": 7.503199577331543
    },
    {
      "epoch": 0.3262872628726287,
      "step": 1505,
      "training_loss": 7.242766380310059
    },
    {
      "epoch": 0.3262872628726287,
      "step": 1505,
      "training_loss": 5.9149603843688965
    },
    {
      "epoch": 0.3262872628726287,
      "step": 1505,
      "training_loss": 7.793839931488037
    },
    {
      "epoch": 0.3265040650406504,
      "step": 1506,
      "training_loss": 6.993989944458008
    },
    {
      "epoch": 0.3265040650406504,
      "step": 1506,
      "training_loss": 6.539967060089111
    },
    {
      "epoch": 0.3265040650406504,
      "step": 1506,
      "training_loss": 7.328751087188721
    },
    {
      "epoch": 0.3265040650406504,
      "step": 1506,
      "training_loss": 7.197627067565918
    },
    {
      "epoch": 0.3267208672086721,
      "step": 1507,
      "training_loss": 7.157642841339111
    },
    {
      "epoch": 0.3267208672086721,
      "step": 1507,
      "training_loss": 6.266626834869385
    },
    {
      "epoch": 0.3267208672086721,
      "step": 1507,
      "training_loss": 8.21352481842041
    },
    {
      "epoch": 0.3267208672086721,
      "step": 1507,
      "training_loss": 5.5008015632629395
    },
    {
      "epoch": 0.3269376693766938,
      "grad_norm": 17.788345336914062,
      "learning_rate": 1e-05,
      "loss": 6.7421,
      "step": 1508
    },
    {
      "epoch": 0.3269376693766938,
      "step": 1508,
      "training_loss": 7.062148094177246
    },
    {
      "epoch": 0.3269376693766938,
      "step": 1508,
      "training_loss": 6.850600242614746
    },
    {
      "epoch": 0.3269376693766938,
      "step": 1508,
      "training_loss": 6.880724906921387
    },
    {
      "epoch": 0.3269376693766938,
      "step": 1508,
      "training_loss": 6.792598247528076
    },
    {
      "epoch": 0.32715447154471544,
      "step": 1509,
      "training_loss": 7.780336380004883
    },
    {
      "epoch": 0.32715447154471544,
      "step": 1509,
      "training_loss": 7.148305892944336
    },
    {
      "epoch": 0.32715447154471544,
      "step": 1509,
      "training_loss": 5.5271711349487305
    },
    {
      "epoch": 0.32715447154471544,
      "step": 1509,
      "training_loss": 6.399451732635498
    },
    {
      "epoch": 0.3273712737127371,
      "step": 1510,
      "training_loss": 8.282221794128418
    },
    {
      "epoch": 0.3273712737127371,
      "step": 1510,
      "training_loss": 8.109012603759766
    },
    {
      "epoch": 0.3273712737127371,
      "step": 1510,
      "training_loss": 8.672455787658691
    },
    {
      "epoch": 0.3273712737127371,
      "step": 1510,
      "training_loss": 8.301733016967773
    },
    {
      "epoch": 0.32758807588075883,
      "step": 1511,
      "training_loss": 7.009026050567627
    },
    {
      "epoch": 0.32758807588075883,
      "step": 1511,
      "training_loss": 7.372262001037598
    },
    {
      "epoch": 0.32758807588075883,
      "step": 1511,
      "training_loss": 5.762768268585205
    },
    {
      "epoch": 0.32758807588075883,
      "step": 1511,
      "training_loss": 7.1008195877075195
    },
    {
      "epoch": 0.3278048780487805,
      "grad_norm": 11.720280647277832,
      "learning_rate": 1e-05,
      "loss": 7.1907,
      "step": 1512
    },
    {
      "epoch": 0.3278048780487805,
      "step": 1512,
      "training_loss": 6.699037075042725
    },
    {
      "epoch": 0.3278048780487805,
      "step": 1512,
      "training_loss": 7.970765590667725
    },
    {
      "epoch": 0.3278048780487805,
      "step": 1512,
      "training_loss": 7.061394691467285
    },
    {
      "epoch": 0.3278048780487805,
      "step": 1512,
      "training_loss": 7.119976043701172
    },
    {
      "epoch": 0.32802168021680217,
      "step": 1513,
      "training_loss": 5.770598411560059
    },
    {
      "epoch": 0.32802168021680217,
      "step": 1513,
      "training_loss": 5.281170845031738
    },
    {
      "epoch": 0.32802168021680217,
      "step": 1513,
      "training_loss": 5.9546003341674805
    },
    {
      "epoch": 0.32802168021680217,
      "step": 1513,
      "training_loss": 6.812594413757324
    },
    {
      "epoch": 0.32823848238482384,
      "step": 1514,
      "training_loss": 7.05815315246582
    },
    {
      "epoch": 0.32823848238482384,
      "step": 1514,
      "training_loss": 6.810143947601318
    },
    {
      "epoch": 0.32823848238482384,
      "step": 1514,
      "training_loss": 7.385339736938477
    },
    {
      "epoch": 0.32823848238482384,
      "step": 1514,
      "training_loss": 6.910335063934326
    },
    {
      "epoch": 0.3284552845528455,
      "step": 1515,
      "training_loss": 4.030887126922607
    },
    {
      "epoch": 0.3284552845528455,
      "step": 1515,
      "training_loss": 7.9736456871032715
    },
    {
      "epoch": 0.3284552845528455,
      "step": 1515,
      "training_loss": 7.5106706619262695
    },
    {
      "epoch": 0.3284552845528455,
      "step": 1515,
      "training_loss": 5.8004961013793945
    },
    {
      "epoch": 0.32867208672086723,
      "grad_norm": 11.75008487701416,
      "learning_rate": 1e-05,
      "loss": 6.6344,
      "step": 1516
    },
    {
      "epoch": 0.32867208672086723,
      "step": 1516,
      "training_loss": 5.847898960113525
    },
    {
      "epoch": 0.32867208672086723,
      "step": 1516,
      "training_loss": 6.596672534942627
    },
    {
      "epoch": 0.32867208672086723,
      "step": 1516,
      "training_loss": 7.069430351257324
    },
    {
      "epoch": 0.32867208672086723,
      "step": 1516,
      "training_loss": 7.311767101287842
    },
    {
      "epoch": 0.3288888888888889,
      "step": 1517,
      "training_loss": 7.209158420562744
    },
    {
      "epoch": 0.3288888888888889,
      "step": 1517,
      "training_loss": 7.241708755493164
    },
    {
      "epoch": 0.3288888888888889,
      "step": 1517,
      "training_loss": 6.202678680419922
    },
    {
      "epoch": 0.3288888888888889,
      "step": 1517,
      "training_loss": 6.324984550476074
    },
    {
      "epoch": 0.32910569105691057,
      "step": 1518,
      "training_loss": 6.4890313148498535
    },
    {
      "epoch": 0.32910569105691057,
      "step": 1518,
      "training_loss": 7.615678310394287
    },
    {
      "epoch": 0.32910569105691057,
      "step": 1518,
      "training_loss": 7.173210144042969
    },
    {
      "epoch": 0.32910569105691057,
      "step": 1518,
      "training_loss": 7.010451793670654
    },
    {
      "epoch": 0.32932249322493223,
      "step": 1519,
      "training_loss": 7.767827987670898
    },
    {
      "epoch": 0.32932249322493223,
      "step": 1519,
      "training_loss": 6.500355243682861
    },
    {
      "epoch": 0.32932249322493223,
      "step": 1519,
      "training_loss": 7.51600456237793
    },
    {
      "epoch": 0.32932249322493223,
      "step": 1519,
      "training_loss": 7.432053089141846
    },
    {
      "epoch": 0.32953929539295396,
      "grad_norm": 10.331283569335938,
      "learning_rate": 1e-05,
      "loss": 6.9568,
      "step": 1520
    },
    {
      "epoch": 0.32953929539295396,
      "step": 1520,
      "training_loss": 6.078956127166748
    },
    {
      "epoch": 0.32953929539295396,
      "step": 1520,
      "training_loss": 7.132768630981445
    },
    {
      "epoch": 0.32953929539295396,
      "step": 1520,
      "training_loss": 7.675037384033203
    },
    {
      "epoch": 0.32953929539295396,
      "step": 1520,
      "training_loss": 7.107254981994629
    },
    {
      "epoch": 0.3297560975609756,
      "step": 1521,
      "training_loss": 6.258859634399414
    },
    {
      "epoch": 0.3297560975609756,
      "step": 1521,
      "training_loss": 7.496452331542969
    },
    {
      "epoch": 0.3297560975609756,
      "step": 1521,
      "training_loss": 8.5132474899292
    },
    {
      "epoch": 0.3297560975609756,
      "step": 1521,
      "training_loss": 6.895040512084961
    },
    {
      "epoch": 0.3299728997289973,
      "step": 1522,
      "training_loss": 4.6646342277526855
    },
    {
      "epoch": 0.3299728997289973,
      "step": 1522,
      "training_loss": 7.30001974105835
    },
    {
      "epoch": 0.3299728997289973,
      "step": 1522,
      "training_loss": 7.714179039001465
    },
    {
      "epoch": 0.3299728997289973,
      "step": 1522,
      "training_loss": 6.715773582458496
    },
    {
      "epoch": 0.33018970189701896,
      "step": 1523,
      "training_loss": 6.371140003204346
    },
    {
      "epoch": 0.33018970189701896,
      "step": 1523,
      "training_loss": 6.728188514709473
    },
    {
      "epoch": 0.33018970189701896,
      "step": 1523,
      "training_loss": 7.476552486419678
    },
    {
      "epoch": 0.33018970189701896,
      "step": 1523,
      "training_loss": 5.8777995109558105
    },
    {
      "epoch": 0.33040650406504063,
      "grad_norm": 10.392309188842773,
      "learning_rate": 1e-05,
      "loss": 6.8754,
      "step": 1524
    },
    {
      "epoch": 0.33040650406504063,
      "step": 1524,
      "training_loss": 6.9671311378479
    },
    {
      "epoch": 0.33040650406504063,
      "step": 1524,
      "training_loss": 7.414606094360352
    },
    {
      "epoch": 0.33040650406504063,
      "step": 1524,
      "training_loss": 6.438645362854004
    },
    {
      "epoch": 0.33040650406504063,
      "step": 1524,
      "training_loss": 6.493244647979736
    },
    {
      "epoch": 0.33062330623306235,
      "step": 1525,
      "training_loss": 7.343078136444092
    },
    {
      "epoch": 0.33062330623306235,
      "step": 1525,
      "training_loss": 7.191317081451416
    },
    {
      "epoch": 0.33062330623306235,
      "step": 1525,
      "training_loss": 5.954232215881348
    },
    {
      "epoch": 0.33062330623306235,
      "step": 1525,
      "training_loss": 7.810855865478516
    },
    {
      "epoch": 0.330840108401084,
      "step": 1526,
      "training_loss": 6.043963432312012
    },
    {
      "epoch": 0.330840108401084,
      "step": 1526,
      "training_loss": 7.046950340270996
    },
    {
      "epoch": 0.330840108401084,
      "step": 1526,
      "training_loss": 7.3906025886535645
    },
    {
      "epoch": 0.330840108401084,
      "step": 1526,
      "training_loss": 6.786735534667969
    },
    {
      "epoch": 0.3310569105691057,
      "step": 1527,
      "training_loss": 9.355414390563965
    },
    {
      "epoch": 0.3310569105691057,
      "step": 1527,
      "training_loss": 7.250055313110352
    },
    {
      "epoch": 0.3310569105691057,
      "step": 1527,
      "training_loss": 7.848613262176514
    },
    {
      "epoch": 0.3310569105691057,
      "step": 1527,
      "training_loss": 6.576263904571533
    },
    {
      "epoch": 0.33127371273712736,
      "grad_norm": 13.070449829101562,
      "learning_rate": 1e-05,
      "loss": 7.1195,
      "step": 1528
    },
    {
      "epoch": 0.33127371273712736,
      "step": 1528,
      "training_loss": 6.76240873336792
    },
    {
      "epoch": 0.33127371273712736,
      "step": 1528,
      "training_loss": 7.7436323165893555
    },
    {
      "epoch": 0.33127371273712736,
      "step": 1528,
      "training_loss": 7.4753570556640625
    },
    {
      "epoch": 0.33127371273712736,
      "step": 1528,
      "training_loss": 6.6861772537231445
    },
    {
      "epoch": 0.331490514905149,
      "step": 1529,
      "training_loss": 7.829360008239746
    },
    {
      "epoch": 0.331490514905149,
      "step": 1529,
      "training_loss": 6.9325761795043945
    },
    {
      "epoch": 0.331490514905149,
      "step": 1529,
      "training_loss": 7.954930782318115
    },
    {
      "epoch": 0.331490514905149,
      "step": 1529,
      "training_loss": 7.40125036239624
    },
    {
      "epoch": 0.33170731707317075,
      "step": 1530,
      "training_loss": 6.129059791564941
    },
    {
      "epoch": 0.33170731707317075,
      "step": 1530,
      "training_loss": 7.280336380004883
    },
    {
      "epoch": 0.33170731707317075,
      "step": 1530,
      "training_loss": 7.927753448486328
    },
    {
      "epoch": 0.33170731707317075,
      "step": 1530,
      "training_loss": 7.320268154144287
    },
    {
      "epoch": 0.3319241192411924,
      "step": 1531,
      "training_loss": 7.363841533660889
    },
    {
      "epoch": 0.3319241192411924,
      "step": 1531,
      "training_loss": 5.218011379241943
    },
    {
      "epoch": 0.3319241192411924,
      "step": 1531,
      "training_loss": 6.860517501831055
    },
    {
      "epoch": 0.3319241192411924,
      "step": 1531,
      "training_loss": 6.838335037231445
    },
    {
      "epoch": 0.3321409214092141,
      "grad_norm": 9.68878173828125,
      "learning_rate": 1e-05,
      "loss": 7.1077,
      "step": 1532
    },
    {
      "epoch": 0.3321409214092141,
      "step": 1532,
      "training_loss": 8.250575065612793
    },
    {
      "epoch": 0.3321409214092141,
      "step": 1532,
      "training_loss": 6.8633809089660645
    },
    {
      "epoch": 0.3321409214092141,
      "step": 1532,
      "training_loss": 6.274353504180908
    },
    {
      "epoch": 0.3321409214092141,
      "step": 1532,
      "training_loss": 5.089649677276611
    },
    {
      "epoch": 0.33235772357723575,
      "step": 1533,
      "training_loss": 6.992873191833496
    },
    {
      "epoch": 0.33235772357723575,
      "step": 1533,
      "training_loss": 5.908535480499268
    },
    {
      "epoch": 0.33235772357723575,
      "step": 1533,
      "training_loss": 7.387166500091553
    },
    {
      "epoch": 0.33235772357723575,
      "step": 1533,
      "training_loss": 6.716615676879883
    },
    {
      "epoch": 0.3325745257452575,
      "step": 1534,
      "training_loss": 7.7142462730407715
    },
    {
      "epoch": 0.3325745257452575,
      "step": 1534,
      "training_loss": 7.9289469718933105
    },
    {
      "epoch": 0.3325745257452575,
      "step": 1534,
      "training_loss": 7.108893394470215
    },
    {
      "epoch": 0.3325745257452575,
      "step": 1534,
      "training_loss": 8.054829597473145
    },
    {
      "epoch": 0.33279132791327914,
      "step": 1535,
      "training_loss": 6.932031631469727
    },
    {
      "epoch": 0.33279132791327914,
      "step": 1535,
      "training_loss": 4.591732025146484
    },
    {
      "epoch": 0.33279132791327914,
      "step": 1535,
      "training_loss": 5.89019250869751
    },
    {
      "epoch": 0.33279132791327914,
      "step": 1535,
      "training_loss": 7.435198783874512
    },
    {
      "epoch": 0.3330081300813008,
      "grad_norm": 12.044297218322754,
      "learning_rate": 1e-05,
      "loss": 6.8212,
      "step": 1536
    },
    {
      "epoch": 0.3330081300813008,
      "step": 1536,
      "training_loss": 6.8325347900390625
    },
    {
      "epoch": 0.3330081300813008,
      "step": 1536,
      "training_loss": 6.242559909820557
    },
    {
      "epoch": 0.3330081300813008,
      "step": 1536,
      "training_loss": 5.244423866271973
    },
    {
      "epoch": 0.3330081300813008,
      "step": 1536,
      "training_loss": 6.540383815765381
    },
    {
      "epoch": 0.3332249322493225,
      "step": 1537,
      "training_loss": 6.3226094245910645
    },
    {
      "epoch": 0.3332249322493225,
      "step": 1537,
      "training_loss": 6.693676471710205
    },
    {
      "epoch": 0.3332249322493225,
      "step": 1537,
      "training_loss": 6.9794816970825195
    },
    {
      "epoch": 0.3332249322493225,
      "step": 1537,
      "training_loss": 7.2742533683776855
    },
    {
      "epoch": 0.33344173441734415,
      "step": 1538,
      "training_loss": 6.821539402008057
    },
    {
      "epoch": 0.33344173441734415,
      "step": 1538,
      "training_loss": 6.504179000854492
    },
    {
      "epoch": 0.33344173441734415,
      "step": 1538,
      "training_loss": 6.894129276275635
    },
    {
      "epoch": 0.33344173441734415,
      "step": 1538,
      "training_loss": 6.614343166351318
    },
    {
      "epoch": 0.3336585365853659,
      "step": 1539,
      "training_loss": 7.146452903747559
    },
    {
      "epoch": 0.3336585365853659,
      "step": 1539,
      "training_loss": 7.386776447296143
    },
    {
      "epoch": 0.3336585365853659,
      "step": 1539,
      "training_loss": 6.861644744873047
    },
    {
      "epoch": 0.3336585365853659,
      "step": 1539,
      "training_loss": 7.391119480133057
    },
    {
      "epoch": 0.33387533875338754,
      "grad_norm": 12.59222412109375,
      "learning_rate": 1e-05,
      "loss": 6.7344,
      "step": 1540
    },
    {
      "epoch": 0.33387533875338754,
      "step": 1540,
      "training_loss": 7.398303031921387
    },
    {
      "epoch": 0.33387533875338754,
      "step": 1540,
      "training_loss": 5.799543857574463
    },
    {
      "epoch": 0.33387533875338754,
      "step": 1540,
      "training_loss": 7.848348140716553
    },
    {
      "epoch": 0.33387533875338754,
      "step": 1540,
      "training_loss": 7.192043304443359
    },
    {
      "epoch": 0.3340921409214092,
      "step": 1541,
      "training_loss": 5.643686294555664
    },
    {
      "epoch": 0.3340921409214092,
      "step": 1541,
      "training_loss": 5.376526355743408
    },
    {
      "epoch": 0.3340921409214092,
      "step": 1541,
      "training_loss": 7.166069984436035
    },
    {
      "epoch": 0.3340921409214092,
      "step": 1541,
      "training_loss": 7.020016670227051
    },
    {
      "epoch": 0.3343089430894309,
      "step": 1542,
      "training_loss": 6.507603168487549
    },
    {
      "epoch": 0.3343089430894309,
      "step": 1542,
      "training_loss": 7.805343151092529
    },
    {
      "epoch": 0.3343089430894309,
      "step": 1542,
      "training_loss": 9.50865364074707
    },
    {
      "epoch": 0.3343089430894309,
      "step": 1542,
      "training_loss": 7.089278221130371
    },
    {
      "epoch": 0.3345257452574526,
      "step": 1543,
      "training_loss": 7.386650562286377
    },
    {
      "epoch": 0.3345257452574526,
      "step": 1543,
      "training_loss": 6.522400379180908
    },
    {
      "epoch": 0.3345257452574526,
      "step": 1543,
      "training_loss": 7.703763008117676
    },
    {
      "epoch": 0.3345257452574526,
      "step": 1543,
      "training_loss": 4.697107315063477
    },
    {
      "epoch": 0.33474254742547427,
      "grad_norm": 10.08260726928711,
      "learning_rate": 1e-05,
      "loss": 6.9166,
      "step": 1544
    },
    {
      "epoch": 0.33474254742547427,
      "step": 1544,
      "training_loss": 5.851428031921387
    },
    {
      "epoch": 0.33474254742547427,
      "step": 1544,
      "training_loss": 6.99343204498291
    },
    {
      "epoch": 0.33474254742547427,
      "step": 1544,
      "training_loss": 6.854610443115234
    },
    {
      "epoch": 0.33474254742547427,
      "step": 1544,
      "training_loss": 7.744635105133057
    },
    {
      "epoch": 0.33495934959349594,
      "step": 1545,
      "training_loss": 7.019931316375732
    },
    {
      "epoch": 0.33495934959349594,
      "step": 1545,
      "training_loss": 8.106963157653809
    },
    {
      "epoch": 0.33495934959349594,
      "step": 1545,
      "training_loss": 7.513197898864746
    },
    {
      "epoch": 0.33495934959349594,
      "step": 1545,
      "training_loss": 7.5076375007629395
    },
    {
      "epoch": 0.3351761517615176,
      "step": 1546,
      "training_loss": 6.798973560333252
    },
    {
      "epoch": 0.3351761517615176,
      "step": 1546,
      "training_loss": 7.572307109832764
    },
    {
      "epoch": 0.3351761517615176,
      "step": 1546,
      "training_loss": 6.490939617156982
    },
    {
      "epoch": 0.3351761517615176,
      "step": 1546,
      "training_loss": 4.500717639923096
    },
    {
      "epoch": 0.3353929539295393,
      "step": 1547,
      "training_loss": 6.069121360778809
    },
    {
      "epoch": 0.3353929539295393,
      "step": 1547,
      "training_loss": 6.903626441955566
    },
    {
      "epoch": 0.3353929539295393,
      "step": 1547,
      "training_loss": 7.16799259185791
    },
    {
      "epoch": 0.3353929539295393,
      "step": 1547,
      "training_loss": 6.62802267074585
    },
    {
      "epoch": 0.335609756097561,
      "grad_norm": 12.796228408813477,
      "learning_rate": 1e-05,
      "loss": 6.8577,
      "step": 1548
    },
    {
      "epoch": 0.335609756097561,
      "step": 1548,
      "training_loss": 7.72029447555542
    },
    {
      "epoch": 0.335609756097561,
      "step": 1548,
      "training_loss": 7.408884525299072
    },
    {
      "epoch": 0.335609756097561,
      "step": 1548,
      "training_loss": 7.488094329833984
    },
    {
      "epoch": 0.335609756097561,
      "step": 1548,
      "training_loss": 8.130730628967285
    },
    {
      "epoch": 0.33582655826558266,
      "step": 1549,
      "training_loss": 6.633065700531006
    },
    {
      "epoch": 0.33582655826558266,
      "step": 1549,
      "training_loss": 7.972618579864502
    },
    {
      "epoch": 0.33582655826558266,
      "step": 1549,
      "training_loss": 7.070233345031738
    },
    {
      "epoch": 0.33582655826558266,
      "step": 1549,
      "training_loss": 6.4056396484375
    },
    {
      "epoch": 0.33604336043360433,
      "step": 1550,
      "training_loss": 8.094992637634277
    },
    {
      "epoch": 0.33604336043360433,
      "step": 1550,
      "training_loss": 7.548053741455078
    },
    {
      "epoch": 0.33604336043360433,
      "step": 1550,
      "training_loss": 6.680094242095947
    },
    {
      "epoch": 0.33604336043360433,
      "step": 1550,
      "training_loss": 7.6496195793151855
    },
    {
      "epoch": 0.336260162601626,
      "step": 1551,
      "training_loss": 7.0262651443481445
    },
    {
      "epoch": 0.336260162601626,
      "step": 1551,
      "training_loss": 5.00564432144165
    },
    {
      "epoch": 0.336260162601626,
      "step": 1551,
      "training_loss": 6.87119722366333
    },
    {
      "epoch": 0.336260162601626,
      "step": 1551,
      "training_loss": 7.358814716339111
    },
    {
      "epoch": 0.3364769647696477,
      "grad_norm": 13.501379013061523,
      "learning_rate": 1e-05,
      "loss": 7.1915,
      "step": 1552
    },
    {
      "epoch": 0.3364769647696477,
      "step": 1552,
      "training_loss": 6.104362964630127
    },
    {
      "epoch": 0.3364769647696477,
      "step": 1552,
      "training_loss": 7.489148139953613
    },
    {
      "epoch": 0.3364769647696477,
      "step": 1552,
      "training_loss": 6.853282928466797
    },
    {
      "epoch": 0.3364769647696477,
      "step": 1552,
      "training_loss": 6.563688278198242
    },
    {
      "epoch": 0.3366937669376694,
      "step": 1553,
      "training_loss": 7.3281402587890625
    },
    {
      "epoch": 0.3366937669376694,
      "step": 1553,
      "training_loss": 7.469242095947266
    },
    {
      "epoch": 0.3366937669376694,
      "step": 1553,
      "training_loss": 5.026288032531738
    },
    {
      "epoch": 0.3366937669376694,
      "step": 1553,
      "training_loss": 7.45184850692749
    },
    {
      "epoch": 0.33691056910569106,
      "step": 1554,
      "training_loss": 7.425402641296387
    },
    {
      "epoch": 0.33691056910569106,
      "step": 1554,
      "training_loss": 6.7374267578125
    },
    {
      "epoch": 0.33691056910569106,
      "step": 1554,
      "training_loss": 5.835734844207764
    },
    {
      "epoch": 0.33691056910569106,
      "step": 1554,
      "training_loss": 4.527021408081055
    },
    {
      "epoch": 0.33712737127371273,
      "step": 1555,
      "training_loss": 7.1122636795043945
    },
    {
      "epoch": 0.33712737127371273,
      "step": 1555,
      "training_loss": 7.2383246421813965
    },
    {
      "epoch": 0.33712737127371273,
      "step": 1555,
      "training_loss": 7.6551289558410645
    },
    {
      "epoch": 0.33712737127371273,
      "step": 1555,
      "training_loss": 6.775114059448242
    },
    {
      "epoch": 0.3373441734417344,
      "grad_norm": 12.31808853149414,
      "learning_rate": 1e-05,
      "loss": 6.7245,
      "step": 1556
    },
    {
      "epoch": 0.3373441734417344,
      "step": 1556,
      "training_loss": 6.701052188873291
    },
    {
      "epoch": 0.3373441734417344,
      "step": 1556,
      "training_loss": 6.389093399047852
    },
    {
      "epoch": 0.3373441734417344,
      "step": 1556,
      "training_loss": 6.539053440093994
    },
    {
      "epoch": 0.3373441734417344,
      "step": 1556,
      "training_loss": 7.020531177520752
    },
    {
      "epoch": 0.3375609756097561,
      "step": 1557,
      "training_loss": 6.138461589813232
    },
    {
      "epoch": 0.3375609756097561,
      "step": 1557,
      "training_loss": 6.635265350341797
    },
    {
      "epoch": 0.3375609756097561,
      "step": 1557,
      "training_loss": 7.7618021965026855
    },
    {
      "epoch": 0.3375609756097561,
      "step": 1557,
      "training_loss": 5.263593673706055
    },
    {
      "epoch": 0.3377777777777778,
      "step": 1558,
      "training_loss": 6.0194573402404785
    },
    {
      "epoch": 0.3377777777777778,
      "step": 1558,
      "training_loss": 5.306835174560547
    },
    {
      "epoch": 0.3377777777777778,
      "step": 1558,
      "training_loss": 6.172009468078613
    },
    {
      "epoch": 0.3377777777777778,
      "step": 1558,
      "training_loss": 8.10030746459961
    },
    {
      "epoch": 0.33799457994579946,
      "step": 1559,
      "training_loss": 7.301654815673828
    },
    {
      "epoch": 0.33799457994579946,
      "step": 1559,
      "training_loss": 7.814505100250244
    },
    {
      "epoch": 0.33799457994579946,
      "step": 1559,
      "training_loss": 6.61742639541626
    },
    {
      "epoch": 0.33799457994579946,
      "step": 1559,
      "training_loss": 8.071049690246582
    },
    {
      "epoch": 0.3382113821138211,
      "grad_norm": 12.947366714477539,
      "learning_rate": 1e-05,
      "loss": 6.7408,
      "step": 1560
    },
    {
      "epoch": 0.3382113821138211,
      "step": 1560,
      "training_loss": 6.612431526184082
    },
    {
      "epoch": 0.3382113821138211,
      "step": 1560,
      "training_loss": 7.2637457847595215
    },
    {
      "epoch": 0.3382113821138211,
      "step": 1560,
      "training_loss": 7.1085076332092285
    },
    {
      "epoch": 0.3382113821138211,
      "step": 1560,
      "training_loss": 6.9591383934021
    },
    {
      "epoch": 0.3384281842818428,
      "step": 1561,
      "training_loss": 7.798898696899414
    },
    {
      "epoch": 0.3384281842818428,
      "step": 1561,
      "training_loss": 7.053314685821533
    },
    {
      "epoch": 0.3384281842818428,
      "step": 1561,
      "training_loss": 7.361542224884033
    },
    {
      "epoch": 0.3384281842818428,
      "step": 1561,
      "training_loss": 6.123429775238037
    },
    {
      "epoch": 0.3386449864498645,
      "step": 1562,
      "training_loss": 6.044321060180664
    },
    {
      "epoch": 0.3386449864498645,
      "step": 1562,
      "training_loss": 6.545816421508789
    },
    {
      "epoch": 0.3386449864498645,
      "step": 1562,
      "training_loss": 7.247438907623291
    },
    {
      "epoch": 0.3386449864498645,
      "step": 1562,
      "training_loss": 7.304589748382568
    },
    {
      "epoch": 0.3388617886178862,
      "step": 1563,
      "training_loss": 6.16622257232666
    },
    {
      "epoch": 0.3388617886178862,
      "step": 1563,
      "training_loss": 6.697808742523193
    },
    {
      "epoch": 0.3388617886178862,
      "step": 1563,
      "training_loss": 8.21203327178955
    },
    {
      "epoch": 0.3388617886178862,
      "step": 1563,
      "training_loss": 6.143420696258545
    },
    {
      "epoch": 0.33907859078590785,
      "grad_norm": 13.075034141540527,
      "learning_rate": 1e-05,
      "loss": 6.9152,
      "step": 1564
    },
    {
      "epoch": 0.33907859078590785,
      "step": 1564,
      "training_loss": 6.412783622741699
    },
    {
      "epoch": 0.33907859078590785,
      "step": 1564,
      "training_loss": 7.630626678466797
    },
    {
      "epoch": 0.33907859078590785,
      "step": 1564,
      "training_loss": 6.651858806610107
    },
    {
      "epoch": 0.33907859078590785,
      "step": 1564,
      "training_loss": 7.3929643630981445
    },
    {
      "epoch": 0.3392953929539295,
      "step": 1565,
      "training_loss": 8.311470031738281
    },
    {
      "epoch": 0.3392953929539295,
      "step": 1565,
      "training_loss": 6.354746341705322
    },
    {
      "epoch": 0.3392953929539295,
      "step": 1565,
      "training_loss": 5.63199520111084
    },
    {
      "epoch": 0.3392953929539295,
      "step": 1565,
      "training_loss": 5.585002899169922
    },
    {
      "epoch": 0.33951219512195124,
      "step": 1566,
      "training_loss": 6.487421035766602
    },
    {
      "epoch": 0.33951219512195124,
      "step": 1566,
      "training_loss": 6.605375289916992
    },
    {
      "epoch": 0.33951219512195124,
      "step": 1566,
      "training_loss": 6.794691562652588
    },
    {
      "epoch": 0.33951219512195124,
      "step": 1566,
      "training_loss": 7.170222282409668
    },
    {
      "epoch": 0.3397289972899729,
      "step": 1567,
      "training_loss": 5.710525989532471
    },
    {
      "epoch": 0.3397289972899729,
      "step": 1567,
      "training_loss": 5.8284149169921875
    },
    {
      "epoch": 0.3397289972899729,
      "step": 1567,
      "training_loss": 7.7029547691345215
    },
    {
      "epoch": 0.3397289972899729,
      "step": 1567,
      "training_loss": 6.093373775482178
    },
    {
      "epoch": 0.3399457994579946,
      "grad_norm": 10.254435539245605,
      "learning_rate": 1e-05,
      "loss": 6.6478,
      "step": 1568
    },
    {
      "epoch": 0.3399457994579946,
      "step": 1568,
      "training_loss": 4.610118389129639
    },
    {
      "epoch": 0.3399457994579946,
      "step": 1568,
      "training_loss": 6.554502487182617
    },
    {
      "epoch": 0.3399457994579946,
      "step": 1568,
      "training_loss": 7.037563800811768
    },
    {
      "epoch": 0.3399457994579946,
      "step": 1568,
      "training_loss": 6.858814716339111
    },
    {
      "epoch": 0.34016260162601625,
      "step": 1569,
      "training_loss": 5.282252311706543
    },
    {
      "epoch": 0.34016260162601625,
      "step": 1569,
      "training_loss": 6.8920722007751465
    },
    {
      "epoch": 0.34016260162601625,
      "step": 1569,
      "training_loss": 7.908417224884033
    },
    {
      "epoch": 0.34016260162601625,
      "step": 1569,
      "training_loss": 7.07145881652832
    },
    {
      "epoch": 0.3403794037940379,
      "step": 1570,
      "training_loss": 5.308371067047119
    },
    {
      "epoch": 0.3403794037940379,
      "step": 1570,
      "training_loss": 6.9295654296875
    },
    {
      "epoch": 0.3403794037940379,
      "step": 1570,
      "training_loss": 6.477464199066162
    },
    {
      "epoch": 0.3403794037940379,
      "step": 1570,
      "training_loss": 7.070078372955322
    },
    {
      "epoch": 0.34059620596205964,
      "step": 1571,
      "training_loss": 6.079158306121826
    },
    {
      "epoch": 0.34059620596205964,
      "step": 1571,
      "training_loss": 6.655366897583008
    },
    {
      "epoch": 0.34059620596205964,
      "step": 1571,
      "training_loss": 6.749014854431152
    },
    {
      "epoch": 0.34059620596205964,
      "step": 1571,
      "training_loss": 7.0263872146606445
    },
    {
      "epoch": 0.3408130081300813,
      "grad_norm": 10.380596160888672,
      "learning_rate": 1e-05,
      "loss": 6.5319,
      "step": 1572
    },
    {
      "epoch": 0.3408130081300813,
      "step": 1572,
      "training_loss": 8.197425842285156
    },
    {
      "epoch": 0.3408130081300813,
      "step": 1572,
      "training_loss": 7.594279766082764
    },
    {
      "epoch": 0.3408130081300813,
      "step": 1572,
      "training_loss": 7.081794261932373
    },
    {
      "epoch": 0.3408130081300813,
      "step": 1572,
      "training_loss": 6.936027526855469
    },
    {
      "epoch": 0.341029810298103,
      "step": 1573,
      "training_loss": 6.78994607925415
    },
    {
      "epoch": 0.341029810298103,
      "step": 1573,
      "training_loss": 8.01774787902832
    },
    {
      "epoch": 0.341029810298103,
      "step": 1573,
      "training_loss": 6.670179843902588
    },
    {
      "epoch": 0.341029810298103,
      "step": 1573,
      "training_loss": 6.506396770477295
    },
    {
      "epoch": 0.34124661246612464,
      "step": 1574,
      "training_loss": 7.705448150634766
    },
    {
      "epoch": 0.34124661246612464,
      "step": 1574,
      "training_loss": 6.891811370849609
    },
    {
      "epoch": 0.34124661246612464,
      "step": 1574,
      "training_loss": 6.872387886047363
    },
    {
      "epoch": 0.34124661246612464,
      "step": 1574,
      "training_loss": 6.741433143615723
    },
    {
      "epoch": 0.34146341463414637,
      "step": 1575,
      "training_loss": 7.06976318359375
    },
    {
      "epoch": 0.34146341463414637,
      "step": 1575,
      "training_loss": 7.059040069580078
    },
    {
      "epoch": 0.34146341463414637,
      "step": 1575,
      "training_loss": 4.955383777618408
    },
    {
      "epoch": 0.34146341463414637,
      "step": 1575,
      "training_loss": 6.56139612197876
    },
    {
      "epoch": 0.34168021680216804,
      "grad_norm": 9.031493186950684,
      "learning_rate": 1e-05,
      "loss": 6.9782,
      "step": 1576
    },
    {
      "epoch": 0.34168021680216804,
      "step": 1576,
      "training_loss": 6.774226665496826
    },
    {
      "epoch": 0.34168021680216804,
      "step": 1576,
      "training_loss": 6.293191909790039
    },
    {
      "epoch": 0.34168021680216804,
      "step": 1576,
      "training_loss": 7.434230327606201
    },
    {
      "epoch": 0.34168021680216804,
      "step": 1576,
      "training_loss": 5.874380111694336
    },
    {
      "epoch": 0.3418970189701897,
      "step": 1577,
      "training_loss": 8.362016677856445
    },
    {
      "epoch": 0.3418970189701897,
      "step": 1577,
      "training_loss": 7.577723503112793
    },
    {
      "epoch": 0.3418970189701897,
      "step": 1577,
      "training_loss": 9.221121788024902
    },
    {
      "epoch": 0.3418970189701897,
      "step": 1577,
      "training_loss": 7.639347553253174
    },
    {
      "epoch": 0.34211382113821137,
      "step": 1578,
      "training_loss": 7.460571765899658
    },
    {
      "epoch": 0.34211382113821137,
      "step": 1578,
      "training_loss": 6.282898902893066
    },
    {
      "epoch": 0.34211382113821137,
      "step": 1578,
      "training_loss": 7.098132133483887
    },
    {
      "epoch": 0.34211382113821137,
      "step": 1578,
      "training_loss": 6.683923721313477
    },
    {
      "epoch": 0.34233062330623304,
      "step": 1579,
      "training_loss": 7.946771621704102
    },
    {
      "epoch": 0.34233062330623304,
      "step": 1579,
      "training_loss": 6.706377029418945
    },
    {
      "epoch": 0.34233062330623304,
      "step": 1579,
      "training_loss": 7.130889892578125
    },
    {
      "epoch": 0.34233062330623304,
      "step": 1579,
      "training_loss": 6.769467830657959
    },
    {
      "epoch": 0.34254742547425476,
      "grad_norm": 12.698612213134766,
      "learning_rate": 1e-05,
      "loss": 7.2035,
      "step": 1580
    },
    {
      "epoch": 0.34254742547425476,
      "step": 1580,
      "training_loss": 7.129439353942871
    },
    {
      "epoch": 0.34254742547425476,
      "step": 1580,
      "training_loss": 7.097682476043701
    },
    {
      "epoch": 0.34254742547425476,
      "step": 1580,
      "training_loss": 6.86043119430542
    },
    {
      "epoch": 0.34254742547425476,
      "step": 1580,
      "training_loss": 6.893394947052002
    },
    {
      "epoch": 0.34276422764227643,
      "step": 1581,
      "training_loss": 8.221287727355957
    },
    {
      "epoch": 0.34276422764227643,
      "step": 1581,
      "training_loss": 8.8204927444458
    },
    {
      "epoch": 0.34276422764227643,
      "step": 1581,
      "training_loss": 7.209373474121094
    },
    {
      "epoch": 0.34276422764227643,
      "step": 1581,
      "training_loss": 7.988851070404053
    },
    {
      "epoch": 0.3429810298102981,
      "step": 1582,
      "training_loss": 5.507349491119385
    },
    {
      "epoch": 0.3429810298102981,
      "step": 1582,
      "training_loss": 8.085285186767578
    },
    {
      "epoch": 0.3429810298102981,
      "step": 1582,
      "training_loss": 8.149514198303223
    },
    {
      "epoch": 0.3429810298102981,
      "step": 1582,
      "training_loss": 6.638140678405762
    },
    {
      "epoch": 0.34319783197831977,
      "step": 1583,
      "training_loss": 6.0998334884643555
    },
    {
      "epoch": 0.34319783197831977,
      "step": 1583,
      "training_loss": 5.75364875793457
    },
    {
      "epoch": 0.34319783197831977,
      "step": 1583,
      "training_loss": 7.1528706550598145
    },
    {
      "epoch": 0.34319783197831977,
      "step": 1583,
      "training_loss": 3.833508014678955
    },
    {
      "epoch": 0.3434146341463415,
      "grad_norm": 12.17536735534668,
      "learning_rate": 1e-05,
      "loss": 6.9651,
      "step": 1584
    },
    {
      "epoch": 0.3434146341463415,
      "step": 1584,
      "training_loss": 6.993448257446289
    },
    {
      "epoch": 0.3434146341463415,
      "step": 1584,
      "training_loss": 7.0467071533203125
    },
    {
      "epoch": 0.3434146341463415,
      "step": 1584,
      "training_loss": 7.8126115798950195
    },
    {
      "epoch": 0.3434146341463415,
      "step": 1584,
      "training_loss": 6.853119850158691
    },
    {
      "epoch": 0.34363143631436316,
      "step": 1585,
      "training_loss": 6.062319278717041
    },
    {
      "epoch": 0.34363143631436316,
      "step": 1585,
      "training_loss": 7.264142990112305
    },
    {
      "epoch": 0.34363143631436316,
      "step": 1585,
      "training_loss": 7.4881205558776855
    },
    {
      "epoch": 0.34363143631436316,
      "step": 1585,
      "training_loss": 6.536728382110596
    },
    {
      "epoch": 0.3438482384823848,
      "step": 1586,
      "training_loss": 6.311653137207031
    },
    {
      "epoch": 0.3438482384823848,
      "step": 1586,
      "training_loss": 6.404109477996826
    },
    {
      "epoch": 0.3438482384823848,
      "step": 1586,
      "training_loss": 7.878824234008789
    },
    {
      "epoch": 0.3438482384823848,
      "step": 1586,
      "training_loss": 6.816279888153076
    },
    {
      "epoch": 0.3440650406504065,
      "step": 1587,
      "training_loss": 8.290351867675781
    },
    {
      "epoch": 0.3440650406504065,
      "step": 1587,
      "training_loss": 5.647396564483643
    },
    {
      "epoch": 0.3440650406504065,
      "step": 1587,
      "training_loss": 7.8939714431762695
    },
    {
      "epoch": 0.3440650406504065,
      "step": 1587,
      "training_loss": 6.794314384460449
    },
    {
      "epoch": 0.34428184281842816,
      "grad_norm": 8.81733512878418,
      "learning_rate": 1e-05,
      "loss": 7.0059,
      "step": 1588
    },
    {
      "epoch": 0.34428184281842816,
      "step": 1588,
      "training_loss": 7.613370895385742
    },
    {
      "epoch": 0.34428184281842816,
      "step": 1588,
      "training_loss": 6.314619064331055
    },
    {
      "epoch": 0.34428184281842816,
      "step": 1588,
      "training_loss": 7.996423721313477
    },
    {
      "epoch": 0.34428184281842816,
      "step": 1588,
      "training_loss": 5.794291973114014
    },
    {
      "epoch": 0.3444986449864499,
      "step": 1589,
      "training_loss": 6.913082599639893
    },
    {
      "epoch": 0.3444986449864499,
      "step": 1589,
      "training_loss": 8.110983848571777
    },
    {
      "epoch": 0.3444986449864499,
      "step": 1589,
      "training_loss": 5.436304092407227
    },
    {
      "epoch": 0.3444986449864499,
      "step": 1589,
      "training_loss": 5.136867046356201
    },
    {
      "epoch": 0.34471544715447155,
      "step": 1590,
      "training_loss": 6.487758636474609
    },
    {
      "epoch": 0.34471544715447155,
      "step": 1590,
      "training_loss": 5.761600971221924
    },
    {
      "epoch": 0.34471544715447155,
      "step": 1590,
      "training_loss": 6.882430076599121
    },
    {
      "epoch": 0.34471544715447155,
      "step": 1590,
      "training_loss": 7.352480411529541
    },
    {
      "epoch": 0.3449322493224932,
      "step": 1591,
      "training_loss": 7.196094989776611
    },
    {
      "epoch": 0.3449322493224932,
      "step": 1591,
      "training_loss": 5.742993354797363
    },
    {
      "epoch": 0.3449322493224932,
      "step": 1591,
      "training_loss": 6.06826114654541
    },
    {
      "epoch": 0.3449322493224932,
      "step": 1591,
      "training_loss": 6.634592056274414
    },
    {
      "epoch": 0.3451490514905149,
      "grad_norm": 8.9036865234375,
      "learning_rate": 1e-05,
      "loss": 6.5901,
      "step": 1592
    },
    {
      "epoch": 0.3451490514905149,
      "step": 1592,
      "training_loss": 7.34094762802124
    },
    {
      "epoch": 0.3451490514905149,
      "step": 1592,
      "training_loss": 7.263111114501953
    },
    {
      "epoch": 0.3451490514905149,
      "step": 1592,
      "training_loss": 6.544160842895508
    },
    {
      "epoch": 0.3451490514905149,
      "step": 1592,
      "training_loss": 4.843978404998779
    },
    {
      "epoch": 0.34536585365853656,
      "step": 1593,
      "training_loss": 7.418364524841309
    },
    {
      "epoch": 0.34536585365853656,
      "step": 1593,
      "training_loss": 6.770902156829834
    },
    {
      "epoch": 0.34536585365853656,
      "step": 1593,
      "training_loss": 7.350747108459473
    },
    {
      "epoch": 0.34536585365853656,
      "step": 1593,
      "training_loss": 7.3594512939453125
    },
    {
      "epoch": 0.3455826558265583,
      "step": 1594,
      "training_loss": 6.075109958648682
    },
    {
      "epoch": 0.3455826558265583,
      "step": 1594,
      "training_loss": 7.966270923614502
    },
    {
      "epoch": 0.3455826558265583,
      "step": 1594,
      "training_loss": 7.481382369995117
    },
    {
      "epoch": 0.3455826558265583,
      "step": 1594,
      "training_loss": 6.650601387023926
    },
    {
      "epoch": 0.34579945799457995,
      "step": 1595,
      "training_loss": 6.341209888458252
    },
    {
      "epoch": 0.34579945799457995,
      "step": 1595,
      "training_loss": 4.554572105407715
    },
    {
      "epoch": 0.34579945799457995,
      "step": 1595,
      "training_loss": 7.636709690093994
    },
    {
      "epoch": 0.34579945799457995,
      "step": 1595,
      "training_loss": 6.9654221534729
    },
    {
      "epoch": 0.3460162601626016,
      "grad_norm": 11.944253921508789,
      "learning_rate": 1e-05,
      "loss": 6.7852,
      "step": 1596
    },
    {
      "epoch": 0.3460162601626016,
      "step": 1596,
      "training_loss": 7.499706268310547
    },
    {
      "epoch": 0.3460162601626016,
      "step": 1596,
      "training_loss": 5.19542932510376
    },
    {
      "epoch": 0.3460162601626016,
      "step": 1596,
      "training_loss": 8.227527618408203
    },
    {
      "epoch": 0.3460162601626016,
      "step": 1596,
      "training_loss": 7.394549369812012
    },
    {
      "epoch": 0.3462330623306233,
      "step": 1597,
      "training_loss": 7.531883716583252
    },
    {
      "epoch": 0.3462330623306233,
      "step": 1597,
      "training_loss": 6.767837047576904
    },
    {
      "epoch": 0.3462330623306233,
      "step": 1597,
      "training_loss": 6.702320575714111
    },
    {
      "epoch": 0.3462330623306233,
      "step": 1597,
      "training_loss": 7.746737480163574
    },
    {
      "epoch": 0.346449864498645,
      "step": 1598,
      "training_loss": 7.0282135009765625
    },
    {
      "epoch": 0.346449864498645,
      "step": 1598,
      "training_loss": 4.102729797363281
    },
    {
      "epoch": 0.346449864498645,
      "step": 1598,
      "training_loss": 7.401953220367432
    },
    {
      "epoch": 0.346449864498645,
      "step": 1598,
      "training_loss": 7.156118869781494
    },
    {
      "epoch": 0.3466666666666667,
      "step": 1599,
      "training_loss": 6.717785358428955
    },
    {
      "epoch": 0.3466666666666667,
      "step": 1599,
      "training_loss": 5.777705669403076
    },
    {
      "epoch": 0.3466666666666667,
      "step": 1599,
      "training_loss": 6.043062686920166
    },
    {
      "epoch": 0.3466666666666667,
      "step": 1599,
      "training_loss": 7.474100589752197
    },
    {
      "epoch": 0.34688346883468835,
      "grad_norm": 13.530964851379395,
      "learning_rate": 1e-05,
      "loss": 6.798,
      "step": 1600
    },
    {
      "epoch": 0.34688346883468835,
      "step": 1600,
      "training_loss": 6.9119873046875
    },
    {
      "epoch": 0.34688346883468835,
      "step": 1600,
      "training_loss": 6.736239433288574
    },
    {
      "epoch": 0.34688346883468835,
      "step": 1600,
      "training_loss": 5.269401550292969
    },
    {
      "epoch": 0.34688346883468835,
      "step": 1600,
      "training_loss": 7.128897190093994
    },
    {
      "epoch": 0.34710027100271,
      "step": 1601,
      "training_loss": 6.029704570770264
    },
    {
      "epoch": 0.34710027100271,
      "step": 1601,
      "training_loss": 6.243692398071289
    },
    {
      "epoch": 0.34710027100271,
      "step": 1601,
      "training_loss": 6.906351089477539
    },
    {
      "epoch": 0.34710027100271,
      "step": 1601,
      "training_loss": 5.828474521636963
    },
    {
      "epoch": 0.3473170731707317,
      "step": 1602,
      "training_loss": 6.633449077606201
    },
    {
      "epoch": 0.3473170731707317,
      "step": 1602,
      "training_loss": 7.098293304443359
    },
    {
      "epoch": 0.3473170731707317,
      "step": 1602,
      "training_loss": 5.785239219665527
    },
    {
      "epoch": 0.3473170731707317,
      "step": 1602,
      "training_loss": 8.005455017089844
    },
    {
      "epoch": 0.3475338753387534,
      "step": 1603,
      "training_loss": 7.607629776000977
    },
    {
      "epoch": 0.3475338753387534,
      "step": 1603,
      "training_loss": 7.02903938293457
    },
    {
      "epoch": 0.3475338753387534,
      "step": 1603,
      "training_loss": 6.93367338180542
    },
    {
      "epoch": 0.3475338753387534,
      "step": 1603,
      "training_loss": 7.358798503875732
    },
    {
      "epoch": 0.3477506775067751,
      "grad_norm": 16.171613693237305,
      "learning_rate": 1e-05,
      "loss": 6.7191,
      "step": 1604
    },
    {
      "epoch": 0.3477506775067751,
      "step": 1604,
      "training_loss": 7.532411575317383
    },
    {
      "epoch": 0.3477506775067751,
      "step": 1604,
      "training_loss": 7.609661102294922
    },
    {
      "epoch": 0.3477506775067751,
      "step": 1604,
      "training_loss": 6.7671074867248535
    },
    {
      "epoch": 0.3477506775067751,
      "step": 1604,
      "training_loss": 7.63854455947876
    },
    {
      "epoch": 0.34796747967479674,
      "step": 1605,
      "training_loss": 7.057258129119873
    },
    {
      "epoch": 0.34796747967479674,
      "step": 1605,
      "training_loss": 5.449775218963623
    },
    {
      "epoch": 0.34796747967479674,
      "step": 1605,
      "training_loss": 5.281497001647949
    },
    {
      "epoch": 0.34796747967479674,
      "step": 1605,
      "training_loss": 7.420844554901123
    },
    {
      "epoch": 0.3481842818428184,
      "step": 1606,
      "training_loss": 7.128045558929443
    },
    {
      "epoch": 0.3481842818428184,
      "step": 1606,
      "training_loss": 5.868773937225342
    },
    {
      "epoch": 0.3481842818428184,
      "step": 1606,
      "training_loss": 6.903678894042969
    },
    {
      "epoch": 0.3481842818428184,
      "step": 1606,
      "training_loss": 7.197463512420654
    },
    {
      "epoch": 0.34840108401084013,
      "step": 1607,
      "training_loss": 4.6735920906066895
    },
    {
      "epoch": 0.34840108401084013,
      "step": 1607,
      "training_loss": 7.2936110496521
    },
    {
      "epoch": 0.34840108401084013,
      "step": 1607,
      "training_loss": 6.832991600036621
    },
    {
      "epoch": 0.34840108401084013,
      "step": 1607,
      "training_loss": 7.010087013244629
    },
    {
      "epoch": 0.3486178861788618,
      "grad_norm": 9.512726783752441,
      "learning_rate": 1e-05,
      "loss": 6.7291,
      "step": 1608
    },
    {
      "epoch": 0.3486178861788618,
      "step": 1608,
      "training_loss": 7.335170269012451
    },
    {
      "epoch": 0.3486178861788618,
      "step": 1608,
      "training_loss": 6.929201126098633
    },
    {
      "epoch": 0.3486178861788618,
      "step": 1608,
      "training_loss": 6.1051154136657715
    },
    {
      "epoch": 0.3486178861788618,
      "step": 1608,
      "training_loss": 6.739389896392822
    },
    {
      "epoch": 0.34883468834688347,
      "step": 1609,
      "training_loss": 4.331240653991699
    },
    {
      "epoch": 0.34883468834688347,
      "step": 1609,
      "training_loss": 5.600114822387695
    },
    {
      "epoch": 0.34883468834688347,
      "step": 1609,
      "training_loss": 6.337505340576172
    },
    {
      "epoch": 0.34883468834688347,
      "step": 1609,
      "training_loss": 6.787952899932861
    },
    {
      "epoch": 0.34905149051490514,
      "step": 1610,
      "training_loss": 6.362746715545654
    },
    {
      "epoch": 0.34905149051490514,
      "step": 1610,
      "training_loss": 6.412031650543213
    },
    {
      "epoch": 0.34905149051490514,
      "step": 1610,
      "training_loss": 7.123015403747559
    },
    {
      "epoch": 0.34905149051490514,
      "step": 1610,
      "training_loss": 6.0087504386901855
    },
    {
      "epoch": 0.3492682926829268,
      "step": 1611,
      "training_loss": 7.055446147918701
    },
    {
      "epoch": 0.3492682926829268,
      "step": 1611,
      "training_loss": 7.996683120727539
    },
    {
      "epoch": 0.3492682926829268,
      "step": 1611,
      "training_loss": 6.548735618591309
    },
    {
      "epoch": 0.3492682926829268,
      "step": 1611,
      "training_loss": 7.080217361450195
    },
    {
      "epoch": 0.34948509485094853,
      "grad_norm": 13.095880508422852,
      "learning_rate": 1e-05,
      "loss": 6.5471,
      "step": 1612
    },
    {
      "epoch": 0.34948509485094853,
      "step": 1612,
      "training_loss": 4.842983722686768
    },
    {
      "epoch": 0.34948509485094853,
      "step": 1612,
      "training_loss": 8.068650245666504
    },
    {
      "epoch": 0.34948509485094853,
      "step": 1612,
      "training_loss": 7.146355152130127
    },
    {
      "epoch": 0.34948509485094853,
      "step": 1612,
      "training_loss": 6.971560478210449
    },
    {
      "epoch": 0.3497018970189702,
      "step": 1613,
      "training_loss": 6.439585208892822
    },
    {
      "epoch": 0.3497018970189702,
      "step": 1613,
      "training_loss": 4.611536502838135
    },
    {
      "epoch": 0.3497018970189702,
      "step": 1613,
      "training_loss": 7.543732643127441
    },
    {
      "epoch": 0.3497018970189702,
      "step": 1613,
      "training_loss": 6.968500137329102
    },
    {
      "epoch": 0.34991869918699187,
      "step": 1614,
      "training_loss": 7.3407301902771
    },
    {
      "epoch": 0.34991869918699187,
      "step": 1614,
      "training_loss": 6.693871021270752
    },
    {
      "epoch": 0.34991869918699187,
      "step": 1614,
      "training_loss": 7.525429725646973
    },
    {
      "epoch": 0.34991869918699187,
      "step": 1614,
      "training_loss": 7.0880279541015625
    },
    {
      "epoch": 0.35013550135501353,
      "step": 1615,
      "training_loss": 7.441201686859131
    },
    {
      "epoch": 0.35013550135501353,
      "step": 1615,
      "training_loss": 6.301263809204102
    },
    {
      "epoch": 0.35013550135501353,
      "step": 1615,
      "training_loss": 6.671921253204346
    },
    {
      "epoch": 0.35013550135501353,
      "step": 1615,
      "training_loss": 6.9919538497924805
    },
    {
      "epoch": 0.35035230352303526,
      "grad_norm": 12.578733444213867,
      "learning_rate": 1e-05,
      "loss": 6.7905,
      "step": 1616
    },
    {
      "epoch": 0.35035230352303526,
      "step": 1616,
      "training_loss": 7.268984317779541
    },
    {
      "epoch": 0.35035230352303526,
      "step": 1616,
      "training_loss": 7.391542434692383
    },
    {
      "epoch": 0.35035230352303526,
      "step": 1616,
      "training_loss": 6.698020935058594
    },
    {
      "epoch": 0.35035230352303526,
      "step": 1616,
      "training_loss": 5.779595851898193
    },
    {
      "epoch": 0.3505691056910569,
      "step": 1617,
      "training_loss": 6.253486633300781
    },
    {
      "epoch": 0.3505691056910569,
      "step": 1617,
      "training_loss": 7.6002607345581055
    },
    {
      "epoch": 0.3505691056910569,
      "step": 1617,
      "training_loss": 6.269921779632568
    },
    {
      "epoch": 0.3505691056910569,
      "step": 1617,
      "training_loss": 7.523846626281738
    },
    {
      "epoch": 0.3507859078590786,
      "step": 1618,
      "training_loss": 5.42051887512207
    },
    {
      "epoch": 0.3507859078590786,
      "step": 1618,
      "training_loss": 6.881711959838867
    },
    {
      "epoch": 0.3507859078590786,
      "step": 1618,
      "training_loss": 7.1694746017456055
    },
    {
      "epoch": 0.3507859078590786,
      "step": 1618,
      "training_loss": 6.344405651092529
    },
    {
      "epoch": 0.35100271002710026,
      "step": 1619,
      "training_loss": 6.747634410858154
    },
    {
      "epoch": 0.35100271002710026,
      "step": 1619,
      "training_loss": 6.9038848876953125
    },
    {
      "epoch": 0.35100271002710026,
      "step": 1619,
      "training_loss": 6.870635509490967
    },
    {
      "epoch": 0.35100271002710026,
      "step": 1619,
      "training_loss": 6.426238536834717
    },
    {
      "epoch": 0.35121951219512193,
      "grad_norm": 17.87577247619629,
      "learning_rate": 1e-05,
      "loss": 6.7219,
      "step": 1620
    },
    {
      "epoch": 0.35121951219512193,
      "step": 1620,
      "training_loss": 7.248898506164551
    },
    {
      "epoch": 0.35121951219512193,
      "step": 1620,
      "training_loss": 7.631362438201904
    },
    {
      "epoch": 0.35121951219512193,
      "step": 1620,
      "training_loss": 7.502667427062988
    },
    {
      "epoch": 0.35121951219512193,
      "step": 1620,
      "training_loss": 4.632380962371826
    },
    {
      "epoch": 0.35143631436314365,
      "step": 1621,
      "training_loss": 7.547854900360107
    },
    {
      "epoch": 0.35143631436314365,
      "step": 1621,
      "training_loss": 6.281871318817139
    },
    {
      "epoch": 0.35143631436314365,
      "step": 1621,
      "training_loss": 7.384843349456787
    },
    {
      "epoch": 0.35143631436314365,
      "step": 1621,
      "training_loss": 6.8378705978393555
    },
    {
      "epoch": 0.3516531165311653,
      "step": 1622,
      "training_loss": 7.186596870422363
    },
    {
      "epoch": 0.3516531165311653,
      "step": 1622,
      "training_loss": 7.537238121032715
    },
    {
      "epoch": 0.3516531165311653,
      "step": 1622,
      "training_loss": 6.94361686706543
    },
    {
      "epoch": 0.3516531165311653,
      "step": 1622,
      "training_loss": 7.4248857498168945
    },
    {
      "epoch": 0.351869918699187,
      "step": 1623,
      "training_loss": 6.282595634460449
    },
    {
      "epoch": 0.351869918699187,
      "step": 1623,
      "training_loss": 6.450291156768799
    },
    {
      "epoch": 0.351869918699187,
      "step": 1623,
      "training_loss": 6.845824241638184
    },
    {
      "epoch": 0.351869918699187,
      "step": 1623,
      "training_loss": 7.138287544250488
    },
    {
      "epoch": 0.35208672086720866,
      "grad_norm": 15.487622261047363,
      "learning_rate": 1e-05,
      "loss": 6.9298,
      "step": 1624
    },
    {
      "epoch": 0.35208672086720866,
      "step": 1624,
      "training_loss": 6.491145133972168
    },
    {
      "epoch": 0.35208672086720866,
      "step": 1624,
      "training_loss": 8.138792991638184
    },
    {
      "epoch": 0.35208672086720866,
      "step": 1624,
      "training_loss": 7.0929036140441895
    },
    {
      "epoch": 0.35208672086720866,
      "step": 1624,
      "training_loss": 6.250836372375488
    },
    {
      "epoch": 0.3523035230352303,
      "step": 1625,
      "training_loss": 7.1605634689331055
    },
    {
      "epoch": 0.3523035230352303,
      "step": 1625,
      "training_loss": 5.519092082977295
    },
    {
      "epoch": 0.3523035230352303,
      "step": 1625,
      "training_loss": 6.260827541351318
    },
    {
      "epoch": 0.3523035230352303,
      "step": 1625,
      "training_loss": 8.101604461669922
    },
    {
      "epoch": 0.35252032520325205,
      "step": 1626,
      "training_loss": 4.602090358734131
    },
    {
      "epoch": 0.35252032520325205,
      "step": 1626,
      "training_loss": 7.5394158363342285
    },
    {
      "epoch": 0.35252032520325205,
      "step": 1626,
      "training_loss": 4.895697116851807
    },
    {
      "epoch": 0.35252032520325205,
      "step": 1626,
      "training_loss": 7.775718688964844
    },
    {
      "epoch": 0.3527371273712737,
      "step": 1627,
      "training_loss": 7.580375671386719
    },
    {
      "epoch": 0.3527371273712737,
      "step": 1627,
      "training_loss": 5.8121137619018555
    },
    {
      "epoch": 0.3527371273712737,
      "step": 1627,
      "training_loss": 7.494195938110352
    },
    {
      "epoch": 0.3527371273712737,
      "step": 1627,
      "training_loss": 8.090353965759277
    },
    {
      "epoch": 0.3529539295392954,
      "grad_norm": 13.086752891540527,
      "learning_rate": 1e-05,
      "loss": 6.8004,
      "step": 1628
    },
    {
      "epoch": 0.3529539295392954,
      "step": 1628,
      "training_loss": 7.035274028778076
    },
    {
      "epoch": 0.3529539295392954,
      "step": 1628,
      "training_loss": 7.388011932373047
    },
    {
      "epoch": 0.3529539295392954,
      "step": 1628,
      "training_loss": 6.629570484161377
    },
    {
      "epoch": 0.3529539295392954,
      "step": 1628,
      "training_loss": 6.784739017486572
    },
    {
      "epoch": 0.35317073170731705,
      "step": 1629,
      "training_loss": 6.432830810546875
    },
    {
      "epoch": 0.35317073170731705,
      "step": 1629,
      "training_loss": 7.864521026611328
    },
    {
      "epoch": 0.35317073170731705,
      "step": 1629,
      "training_loss": 7.372235298156738
    },
    {
      "epoch": 0.35317073170731705,
      "step": 1629,
      "training_loss": 6.416447639465332
    },
    {
      "epoch": 0.3533875338753388,
      "step": 1630,
      "training_loss": 7.228590488433838
    },
    {
      "epoch": 0.3533875338753388,
      "step": 1630,
      "training_loss": 7.656662464141846
    },
    {
      "epoch": 0.3533875338753388,
      "step": 1630,
      "training_loss": 6.448760032653809
    },
    {
      "epoch": 0.3533875338753388,
      "step": 1630,
      "training_loss": 5.6397318840026855
    },
    {
      "epoch": 0.35360433604336045,
      "step": 1631,
      "training_loss": 7.292810916900635
    },
    {
      "epoch": 0.35360433604336045,
      "step": 1631,
      "training_loss": 7.2669677734375
    },
    {
      "epoch": 0.35360433604336045,
      "step": 1631,
      "training_loss": 4.271528720855713
    },
    {
      "epoch": 0.35360433604336045,
      "step": 1631,
      "training_loss": 7.430130481719971
    },
    {
      "epoch": 0.3538211382113821,
      "grad_norm": 11.75705623626709,
      "learning_rate": 1e-05,
      "loss": 6.8224,
      "step": 1632
    },
    {
      "epoch": 0.3538211382113821,
      "step": 1632,
      "training_loss": 6.547144889831543
    },
    {
      "epoch": 0.3538211382113821,
      "step": 1632,
      "training_loss": 7.017606258392334
    },
    {
      "epoch": 0.3538211382113821,
      "step": 1632,
      "training_loss": 7.144464492797852
    },
    {
      "epoch": 0.3538211382113821,
      "step": 1632,
      "training_loss": 6.67789888381958
    },
    {
      "epoch": 0.3540379403794038,
      "step": 1633,
      "training_loss": 7.026703357696533
    },
    {
      "epoch": 0.3540379403794038,
      "step": 1633,
      "training_loss": 7.343909740447998
    },
    {
      "epoch": 0.3540379403794038,
      "step": 1633,
      "training_loss": 7.966641426086426
    },
    {
      "epoch": 0.3540379403794038,
      "step": 1633,
      "training_loss": 6.86928129196167
    },
    {
      "epoch": 0.35425474254742545,
      "step": 1634,
      "training_loss": 7.571976184844971
    },
    {
      "epoch": 0.35425474254742545,
      "step": 1634,
      "training_loss": 7.141619682312012
    },
    {
      "epoch": 0.35425474254742545,
      "step": 1634,
      "training_loss": 6.123843669891357
    },
    {
      "epoch": 0.35425474254742545,
      "step": 1634,
      "training_loss": 7.867520332336426
    },
    {
      "epoch": 0.3544715447154472,
      "step": 1635,
      "training_loss": 7.1538543701171875
    },
    {
      "epoch": 0.3544715447154472,
      "step": 1635,
      "training_loss": 8.19681167602539
    },
    {
      "epoch": 0.3544715447154472,
      "step": 1635,
      "training_loss": 6.10245418548584
    },
    {
      "epoch": 0.3544715447154472,
      "step": 1635,
      "training_loss": 7.75191593170166
    },
    {
      "epoch": 0.35468834688346884,
      "grad_norm": 14.453248977661133,
      "learning_rate": 1e-05,
      "loss": 7.1565,
      "step": 1636
    },
    {
      "epoch": 0.35468834688346884,
      "step": 1636,
      "training_loss": 5.722966194152832
    },
    {
      "epoch": 0.35468834688346884,
      "step": 1636,
      "training_loss": 6.962075233459473
    },
    {
      "epoch": 0.35468834688346884,
      "step": 1636,
      "training_loss": 8.005974769592285
    },
    {
      "epoch": 0.35468834688346884,
      "step": 1636,
      "training_loss": 6.193337440490723
    },
    {
      "epoch": 0.3549051490514905,
      "step": 1637,
      "training_loss": 5.213945388793945
    },
    {
      "epoch": 0.3549051490514905,
      "step": 1637,
      "training_loss": 7.278277397155762
    },
    {
      "epoch": 0.3549051490514905,
      "step": 1637,
      "training_loss": 6.423695087432861
    },
    {
      "epoch": 0.3549051490514905,
      "step": 1637,
      "training_loss": 6.756588935852051
    },
    {
      "epoch": 0.3551219512195122,
      "step": 1638,
      "training_loss": 6.581984043121338
    },
    {
      "epoch": 0.3551219512195122,
      "step": 1638,
      "training_loss": 7.6014084815979
    },
    {
      "epoch": 0.3551219512195122,
      "step": 1638,
      "training_loss": 5.850255966186523
    },
    {
      "epoch": 0.3551219512195122,
      "step": 1638,
      "training_loss": 7.2173004150390625
    },
    {
      "epoch": 0.3553387533875339,
      "step": 1639,
      "training_loss": 5.480970859527588
    },
    {
      "epoch": 0.3553387533875339,
      "step": 1639,
      "training_loss": 6.006164073944092
    },
    {
      "epoch": 0.3553387533875339,
      "step": 1639,
      "training_loss": 7.573928356170654
    },
    {
      "epoch": 0.3553387533875339,
      "step": 1639,
      "training_loss": 7.344253063201904
    },
    {
      "epoch": 0.35555555555555557,
      "grad_norm": 11.045234680175781,
      "learning_rate": 1e-05,
      "loss": 6.6383,
      "step": 1640
    },
    {
      "epoch": 0.35555555555555557,
      "step": 1640,
      "training_loss": 7.222402095794678
    },
    {
      "epoch": 0.35555555555555557,
      "step": 1640,
      "training_loss": 7.513901233673096
    },
    {
      "epoch": 0.35555555555555557,
      "step": 1640,
      "training_loss": 6.329756259918213
    },
    {
      "epoch": 0.35555555555555557,
      "step": 1640,
      "training_loss": 7.079931735992432
    },
    {
      "epoch": 0.35577235772357724,
      "step": 1641,
      "training_loss": 7.507558822631836
    },
    {
      "epoch": 0.35577235772357724,
      "step": 1641,
      "training_loss": 6.7917704582214355
    },
    {
      "epoch": 0.35577235772357724,
      "step": 1641,
      "training_loss": 6.300641059875488
    },
    {
      "epoch": 0.35577235772357724,
      "step": 1641,
      "training_loss": 5.931935787200928
    },
    {
      "epoch": 0.3559891598915989,
      "step": 1642,
      "training_loss": 7.298342227935791
    },
    {
      "epoch": 0.3559891598915989,
      "step": 1642,
      "training_loss": 5.78041934967041
    },
    {
      "epoch": 0.3559891598915989,
      "step": 1642,
      "training_loss": 7.451823711395264
    },
    {
      "epoch": 0.3559891598915989,
      "step": 1642,
      "training_loss": 6.83962869644165
    },
    {
      "epoch": 0.3562059620596206,
      "step": 1643,
      "training_loss": 7.786454677581787
    },
    {
      "epoch": 0.3562059620596206,
      "step": 1643,
      "training_loss": 7.9477643966674805
    },
    {
      "epoch": 0.3562059620596206,
      "step": 1643,
      "training_loss": 7.290524959564209
    },
    {
      "epoch": 0.3562059620596206,
      "step": 1643,
      "training_loss": 7.1234540939331055
    },
    {
      "epoch": 0.3564227642276423,
      "grad_norm": 12.099395751953125,
      "learning_rate": 1e-05,
      "loss": 7.0123,
      "step": 1644
    },
    {
      "epoch": 0.3564227642276423,
      "step": 1644,
      "training_loss": 8.634918212890625
    },
    {
      "epoch": 0.3564227642276423,
      "step": 1644,
      "training_loss": 7.078726768493652
    },
    {
      "epoch": 0.3564227642276423,
      "step": 1644,
      "training_loss": 7.770812511444092
    },
    {
      "epoch": 0.3564227642276423,
      "step": 1644,
      "training_loss": 6.364267349243164
    },
    {
      "epoch": 0.35663956639566397,
      "step": 1645,
      "training_loss": 6.038773536682129
    },
    {
      "epoch": 0.35663956639566397,
      "step": 1645,
      "training_loss": 7.167923927307129
    },
    {
      "epoch": 0.35663956639566397,
      "step": 1645,
      "training_loss": 6.789864540100098
    },
    {
      "epoch": 0.35663956639566397,
      "step": 1645,
      "training_loss": 7.056997776031494
    },
    {
      "epoch": 0.35685636856368563,
      "step": 1646,
      "training_loss": 7.289417266845703
    },
    {
      "epoch": 0.35685636856368563,
      "step": 1646,
      "training_loss": 5.742889881134033
    },
    {
      "epoch": 0.35685636856368563,
      "step": 1646,
      "training_loss": 6.813155651092529
    },
    {
      "epoch": 0.35685636856368563,
      "step": 1646,
      "training_loss": 6.993440628051758
    },
    {
      "epoch": 0.3570731707317073,
      "step": 1647,
      "training_loss": 5.620208740234375
    },
    {
      "epoch": 0.3570731707317073,
      "step": 1647,
      "training_loss": 7.8773274421691895
    },
    {
      "epoch": 0.3570731707317073,
      "step": 1647,
      "training_loss": 6.8481059074401855
    },
    {
      "epoch": 0.3570731707317073,
      "step": 1647,
      "training_loss": 5.884833812713623
    },
    {
      "epoch": 0.357289972899729,
      "grad_norm": 19.162572860717773,
      "learning_rate": 1e-05,
      "loss": 6.8732,
      "step": 1648
    },
    {
      "epoch": 0.357289972899729,
      "step": 1648,
      "training_loss": 8.394376754760742
    },
    {
      "epoch": 0.357289972899729,
      "step": 1648,
      "training_loss": 6.077556133270264
    },
    {
      "epoch": 0.357289972899729,
      "step": 1648,
      "training_loss": 6.646731376647949
    },
    {
      "epoch": 0.357289972899729,
      "step": 1648,
      "training_loss": 6.986873149871826
    },
    {
      "epoch": 0.3575067750677507,
      "step": 1649,
      "training_loss": 8.119180679321289
    },
    {
      "epoch": 0.3575067750677507,
      "step": 1649,
      "training_loss": 7.600971221923828
    },
    {
      "epoch": 0.3575067750677507,
      "step": 1649,
      "training_loss": 8.020804405212402
    },
    {
      "epoch": 0.3575067750677507,
      "step": 1649,
      "training_loss": 7.9661478996276855
    },
    {
      "epoch": 0.35772357723577236,
      "step": 1650,
      "training_loss": 6.9196062088012695
    },
    {
      "epoch": 0.35772357723577236,
      "step": 1650,
      "training_loss": 7.59690523147583
    },
    {
      "epoch": 0.35772357723577236,
      "step": 1650,
      "training_loss": 6.639510154724121
    },
    {
      "epoch": 0.35772357723577236,
      "step": 1650,
      "training_loss": 4.5969462394714355
    },
    {
      "epoch": 0.35794037940379403,
      "step": 1651,
      "training_loss": 7.346288204193115
    },
    {
      "epoch": 0.35794037940379403,
      "step": 1651,
      "training_loss": 6.954309463500977
    },
    {
      "epoch": 0.35794037940379403,
      "step": 1651,
      "training_loss": 7.221027374267578
    },
    {
      "epoch": 0.35794037940379403,
      "step": 1651,
      "training_loss": 7.677178859710693
    },
    {
      "epoch": 0.3581571815718157,
      "grad_norm": 11.13240909576416,
      "learning_rate": 1e-05,
      "loss": 7.1728,
      "step": 1652
    },
    {
      "epoch": 0.3581571815718157,
      "step": 1652,
      "training_loss": 7.175774097442627
    },
    {
      "epoch": 0.3581571815718157,
      "step": 1652,
      "training_loss": 6.139369010925293
    },
    {
      "epoch": 0.3581571815718157,
      "step": 1652,
      "training_loss": 7.230066776275635
    },
    {
      "epoch": 0.3581571815718157,
      "step": 1652,
      "training_loss": 7.309122562408447
    },
    {
      "epoch": 0.3583739837398374,
      "step": 1653,
      "training_loss": 7.990145683288574
    },
    {
      "epoch": 0.3583739837398374,
      "step": 1653,
      "training_loss": 6.96368932723999
    },
    {
      "epoch": 0.3583739837398374,
      "step": 1653,
      "training_loss": 7.15973424911499
    },
    {
      "epoch": 0.3583739837398374,
      "step": 1653,
      "training_loss": 6.784533977508545
    },
    {
      "epoch": 0.3585907859078591,
      "step": 1654,
      "training_loss": 6.5659027099609375
    },
    {
      "epoch": 0.3585907859078591,
      "step": 1654,
      "training_loss": 7.981420040130615
    },
    {
      "epoch": 0.3585907859078591,
      "step": 1654,
      "training_loss": 7.315033912658691
    },
    {
      "epoch": 0.3585907859078591,
      "step": 1654,
      "training_loss": 6.883950233459473
    },
    {
      "epoch": 0.35880758807588076,
      "step": 1655,
      "training_loss": 4.630131244659424
    },
    {
      "epoch": 0.35880758807588076,
      "step": 1655,
      "training_loss": 7.492422103881836
    },
    {
      "epoch": 0.35880758807588076,
      "step": 1655,
      "training_loss": 4.306910037994385
    },
    {
      "epoch": 0.35880758807588076,
      "step": 1655,
      "training_loss": 7.1662983894348145
    },
    {
      "epoch": 0.3590243902439024,
      "grad_norm": 11.326271057128906,
      "learning_rate": 1e-05,
      "loss": 6.8184,
      "step": 1656
    },
    {
      "epoch": 0.3590243902439024,
      "step": 1656,
      "training_loss": 8.891188621520996
    },
    {
      "epoch": 0.3590243902439024,
      "step": 1656,
      "training_loss": 6.691408634185791
    },
    {
      "epoch": 0.3590243902439024,
      "step": 1656,
      "training_loss": 7.112861633300781
    },
    {
      "epoch": 0.3590243902439024,
      "step": 1656,
      "training_loss": 7.149021625518799
    },
    {
      "epoch": 0.3592411924119241,
      "step": 1657,
      "training_loss": 5.639978885650635
    },
    {
      "epoch": 0.3592411924119241,
      "step": 1657,
      "training_loss": 6.218925952911377
    },
    {
      "epoch": 0.3592411924119241,
      "step": 1657,
      "training_loss": 7.559835433959961
    },
    {
      "epoch": 0.3592411924119241,
      "step": 1657,
      "training_loss": 7.812148094177246
    },
    {
      "epoch": 0.3594579945799458,
      "step": 1658,
      "training_loss": 6.570540428161621
    },
    {
      "epoch": 0.3594579945799458,
      "step": 1658,
      "training_loss": 7.25718879699707
    },
    {
      "epoch": 0.3594579945799458,
      "step": 1658,
      "training_loss": 7.025303840637207
    },
    {
      "epoch": 0.3594579945799458,
      "step": 1658,
      "training_loss": 7.104306221008301
    },
    {
      "epoch": 0.3596747967479675,
      "step": 1659,
      "training_loss": 7.15133810043335
    },
    {
      "epoch": 0.3596747967479675,
      "step": 1659,
      "training_loss": 7.43947696685791
    },
    {
      "epoch": 0.3596747967479675,
      "step": 1659,
      "training_loss": 6.746843338012695
    },
    {
      "epoch": 0.3596747967479675,
      "step": 1659,
      "training_loss": 7.8758440017700195
    },
    {
      "epoch": 0.35989159891598915,
      "grad_norm": 17.59665870666504,
      "learning_rate": 1e-05,
      "loss": 7.1404,
      "step": 1660
    },
    {
      "epoch": 0.35989159891598915,
      "step": 1660,
      "training_loss": 5.940486431121826
    },
    {
      "epoch": 0.35989159891598915,
      "step": 1660,
      "training_loss": 7.498424053192139
    },
    {
      "epoch": 0.35989159891598915,
      "step": 1660,
      "training_loss": 7.15302038192749
    },
    {
      "epoch": 0.35989159891598915,
      "step": 1660,
      "training_loss": 8.146894454956055
    },
    {
      "epoch": 0.3601084010840108,
      "step": 1661,
      "training_loss": 6.749403476715088
    },
    {
      "epoch": 0.3601084010840108,
      "step": 1661,
      "training_loss": 6.5777106285095215
    },
    {
      "epoch": 0.3601084010840108,
      "step": 1661,
      "training_loss": 6.142855167388916
    },
    {
      "epoch": 0.3601084010840108,
      "step": 1661,
      "training_loss": 6.335160732269287
    },
    {
      "epoch": 0.36032520325203254,
      "step": 1662,
      "training_loss": 7.061380386352539
    },
    {
      "epoch": 0.36032520325203254,
      "step": 1662,
      "training_loss": 6.468817710876465
    },
    {
      "epoch": 0.36032520325203254,
      "step": 1662,
      "training_loss": 7.43763542175293
    },
    {
      "epoch": 0.36032520325203254,
      "step": 1662,
      "training_loss": 7.822184085845947
    },
    {
      "epoch": 0.3605420054200542,
      "step": 1663,
      "training_loss": 6.984842777252197
    },
    {
      "epoch": 0.3605420054200542,
      "step": 1663,
      "training_loss": 7.837283134460449
    },
    {
      "epoch": 0.3605420054200542,
      "step": 1663,
      "training_loss": 7.584733009338379
    },
    {
      "epoch": 0.3605420054200542,
      "step": 1663,
      "training_loss": 7.959263801574707
    },
    {
      "epoch": 0.3607588075880759,
      "grad_norm": 13.841094970703125,
      "learning_rate": 1e-05,
      "loss": 7.1063,
      "step": 1664
    },
    {
      "epoch": 0.3607588075880759,
      "step": 1664,
      "training_loss": 7.0689005851745605
    },
    {
      "epoch": 0.3607588075880759,
      "step": 1664,
      "training_loss": 7.562760353088379
    },
    {
      "epoch": 0.3607588075880759,
      "step": 1664,
      "training_loss": 7.28458833694458
    },
    {
      "epoch": 0.3607588075880759,
      "step": 1664,
      "training_loss": 7.129278182983398
    },
    {
      "epoch": 0.36097560975609755,
      "step": 1665,
      "training_loss": 6.687686443328857
    },
    {
      "epoch": 0.36097560975609755,
      "step": 1665,
      "training_loss": 7.302489757537842
    },
    {
      "epoch": 0.36097560975609755,
      "step": 1665,
      "training_loss": 7.127472877502441
    },
    {
      "epoch": 0.36097560975609755,
      "step": 1665,
      "training_loss": 7.249658584594727
    },
    {
      "epoch": 0.3611924119241192,
      "step": 1666,
      "training_loss": 6.987952709197998
    },
    {
      "epoch": 0.3611924119241192,
      "step": 1666,
      "training_loss": 7.6695556640625
    },
    {
      "epoch": 0.3611924119241192,
      "step": 1666,
      "training_loss": 6.395518779754639
    },
    {
      "epoch": 0.3611924119241192,
      "step": 1666,
      "training_loss": 7.275481700897217
    },
    {
      "epoch": 0.36140921409214094,
      "step": 1667,
      "training_loss": 5.9360246658325195
    },
    {
      "epoch": 0.36140921409214094,
      "step": 1667,
      "training_loss": 6.6552839279174805
    },
    {
      "epoch": 0.36140921409214094,
      "step": 1667,
      "training_loss": 6.443770408630371
    },
    {
      "epoch": 0.36140921409214094,
      "step": 1667,
      "training_loss": 6.88043737411499
    },
    {
      "epoch": 0.3616260162601626,
      "grad_norm": 14.938139915466309,
      "learning_rate": 1e-05,
      "loss": 6.9786,
      "step": 1668
    },
    {
      "epoch": 0.3616260162601626,
      "step": 1668,
      "training_loss": 6.14177131652832
    },
    {
      "epoch": 0.3616260162601626,
      "step": 1668,
      "training_loss": 4.512699604034424
    },
    {
      "epoch": 0.3616260162601626,
      "step": 1668,
      "training_loss": 6.601744174957275
    },
    {
      "epoch": 0.3616260162601626,
      "step": 1668,
      "training_loss": 6.55983829498291
    },
    {
      "epoch": 0.3618428184281843,
      "step": 1669,
      "training_loss": 7.8302321434021
    },
    {
      "epoch": 0.3618428184281843,
      "step": 1669,
      "training_loss": 7.6451497077941895
    },
    {
      "epoch": 0.3618428184281843,
      "step": 1669,
      "training_loss": 6.1439008712768555
    },
    {
      "epoch": 0.3618428184281843,
      "step": 1669,
      "training_loss": 6.526185989379883
    },
    {
      "epoch": 0.36205962059620594,
      "step": 1670,
      "training_loss": 5.2877116203308105
    },
    {
      "epoch": 0.36205962059620594,
      "step": 1670,
      "training_loss": 4.654654502868652
    },
    {
      "epoch": 0.36205962059620594,
      "step": 1670,
      "training_loss": 7.419631004333496
    },
    {
      "epoch": 0.36205962059620594,
      "step": 1670,
      "training_loss": 7.440070152282715
    },
    {
      "epoch": 0.36227642276422767,
      "step": 1671,
      "training_loss": 5.846433162689209
    },
    {
      "epoch": 0.36227642276422767,
      "step": 1671,
      "training_loss": 6.788661003112793
    },
    {
      "epoch": 0.36227642276422767,
      "step": 1671,
      "training_loss": 6.497644424438477
    },
    {
      "epoch": 0.36227642276422767,
      "step": 1671,
      "training_loss": 7.351506233215332
    },
    {
      "epoch": 0.36249322493224934,
      "grad_norm": 14.49915885925293,
      "learning_rate": 1e-05,
      "loss": 6.453,
      "step": 1672
    },
    {
      "epoch": 0.36249322493224934,
      "step": 1672,
      "training_loss": 8.317204475402832
    },
    {
      "epoch": 0.36249322493224934,
      "step": 1672,
      "training_loss": 7.511528968811035
    },
    {
      "epoch": 0.36249322493224934,
      "step": 1672,
      "training_loss": 6.9377288818359375
    },
    {
      "epoch": 0.36249322493224934,
      "step": 1672,
      "training_loss": 6.518052577972412
    },
    {
      "epoch": 0.362710027100271,
      "step": 1673,
      "training_loss": 7.148252964019775
    },
    {
      "epoch": 0.362710027100271,
      "step": 1673,
      "training_loss": 6.573976516723633
    },
    {
      "epoch": 0.362710027100271,
      "step": 1673,
      "training_loss": 5.07976770401001
    },
    {
      "epoch": 0.362710027100271,
      "step": 1673,
      "training_loss": 7.899874210357666
    },
    {
      "epoch": 0.36292682926829267,
      "step": 1674,
      "training_loss": 6.115024566650391
    },
    {
      "epoch": 0.36292682926829267,
      "step": 1674,
      "training_loss": 6.924779891967773
    },
    {
      "epoch": 0.36292682926829267,
      "step": 1674,
      "training_loss": 6.100345611572266
    },
    {
      "epoch": 0.36292682926829267,
      "step": 1674,
      "training_loss": 6.68431282043457
    },
    {
      "epoch": 0.36314363143631434,
      "step": 1675,
      "training_loss": 6.6276021003723145
    },
    {
      "epoch": 0.36314363143631434,
      "step": 1675,
      "training_loss": 6.781975746154785
    },
    {
      "epoch": 0.36314363143631434,
      "step": 1675,
      "training_loss": 6.838397979736328
    },
    {
      "epoch": 0.36314363143631434,
      "step": 1675,
      "training_loss": 6.928897857666016
    },
    {
      "epoch": 0.36336043360433606,
      "grad_norm": 14.006051063537598,
      "learning_rate": 1e-05,
      "loss": 6.8117,
      "step": 1676
    },
    {
      "epoch": 0.36336043360433606,
      "step": 1676,
      "training_loss": 7.027317523956299
    },
    {
      "epoch": 0.36336043360433606,
      "step": 1676,
      "training_loss": 7.256129264831543
    },
    {
      "epoch": 0.36336043360433606,
      "step": 1676,
      "training_loss": 7.039803504943848
    },
    {
      "epoch": 0.36336043360433606,
      "step": 1676,
      "training_loss": 6.903931617736816
    },
    {
      "epoch": 0.36357723577235773,
      "step": 1677,
      "training_loss": 7.4183149337768555
    },
    {
      "epoch": 0.36357723577235773,
      "step": 1677,
      "training_loss": 5.382026195526123
    },
    {
      "epoch": 0.36357723577235773,
      "step": 1677,
      "training_loss": 5.5647358894348145
    },
    {
      "epoch": 0.36357723577235773,
      "step": 1677,
      "training_loss": 4.679791450500488
    },
    {
      "epoch": 0.3637940379403794,
      "step": 1678,
      "training_loss": 7.907369613647461
    },
    {
      "epoch": 0.3637940379403794,
      "step": 1678,
      "training_loss": 5.923351287841797
    },
    {
      "epoch": 0.3637940379403794,
      "step": 1678,
      "training_loss": 5.99993371963501
    },
    {
      "epoch": 0.3637940379403794,
      "step": 1678,
      "training_loss": 5.4318928718566895
    },
    {
      "epoch": 0.36401084010840107,
      "step": 1679,
      "training_loss": 7.665497303009033
    },
    {
      "epoch": 0.36401084010840107,
      "step": 1679,
      "training_loss": 8.006265640258789
    },
    {
      "epoch": 0.36401084010840107,
      "step": 1679,
      "training_loss": 8.021444320678711
    },
    {
      "epoch": 0.36401084010840107,
      "step": 1679,
      "training_loss": 7.540765762329102
    },
    {
      "epoch": 0.3642276422764228,
      "grad_norm": 11.739519119262695,
      "learning_rate": 1e-05,
      "loss": 6.7355,
      "step": 1680
    },
    {
      "epoch": 0.3642276422764228,
      "step": 1680,
      "training_loss": 5.89285945892334
    },
    {
      "epoch": 0.3642276422764228,
      "step": 1680,
      "training_loss": 7.71382474899292
    },
    {
      "epoch": 0.3642276422764228,
      "step": 1680,
      "training_loss": 6.816633224487305
    },
    {
      "epoch": 0.3642276422764228,
      "step": 1680,
      "training_loss": 6.009468078613281
    },
    {
      "epoch": 0.36444444444444446,
      "step": 1681,
      "training_loss": 6.325674057006836
    },
    {
      "epoch": 0.36444444444444446,
      "step": 1681,
      "training_loss": 6.932051181793213
    },
    {
      "epoch": 0.36444444444444446,
      "step": 1681,
      "training_loss": 7.128702163696289
    },
    {
      "epoch": 0.36444444444444446,
      "step": 1681,
      "training_loss": 7.036040782928467
    },
    {
      "epoch": 0.36466124661246613,
      "step": 1682,
      "training_loss": 6.925948143005371
    },
    {
      "epoch": 0.36466124661246613,
      "step": 1682,
      "training_loss": 7.4384918212890625
    },
    {
      "epoch": 0.36466124661246613,
      "step": 1682,
      "training_loss": 7.715723514556885
    },
    {
      "epoch": 0.36466124661246613,
      "step": 1682,
      "training_loss": 9.318169593811035
    },
    {
      "epoch": 0.3648780487804878,
      "step": 1683,
      "training_loss": 5.673376560211182
    },
    {
      "epoch": 0.3648780487804878,
      "step": 1683,
      "training_loss": 6.8608598709106445
    },
    {
      "epoch": 0.3648780487804878,
      "step": 1683,
      "training_loss": 7.029080867767334
    },
    {
      "epoch": 0.3648780487804878,
      "step": 1683,
      "training_loss": 6.8649187088012695
    },
    {
      "epoch": 0.36509485094850946,
      "grad_norm": 13.151961326599121,
      "learning_rate": 1e-05,
      "loss": 6.9801,
      "step": 1684
    },
    {
      "epoch": 0.36509485094850946,
      "step": 1684,
      "training_loss": 7.157257080078125
    },
    {
      "epoch": 0.36509485094850946,
      "step": 1684,
      "training_loss": 8.566368103027344
    },
    {
      "epoch": 0.36509485094850946,
      "step": 1684,
      "training_loss": 7.524257183074951
    },
    {
      "epoch": 0.36509485094850946,
      "step": 1684,
      "training_loss": 8.08071517944336
    },
    {
      "epoch": 0.3653116531165312,
      "step": 1685,
      "training_loss": 7.033411502838135
    },
    {
      "epoch": 0.3653116531165312,
      "step": 1685,
      "training_loss": 6.791049957275391
    },
    {
      "epoch": 0.3653116531165312,
      "step": 1685,
      "training_loss": 7.074739933013916
    },
    {
      "epoch": 0.3653116531165312,
      "step": 1685,
      "training_loss": 5.667596817016602
    },
    {
      "epoch": 0.36552845528455286,
      "step": 1686,
      "training_loss": 7.234703063964844
    },
    {
      "epoch": 0.36552845528455286,
      "step": 1686,
      "training_loss": 7.219968318939209
    },
    {
      "epoch": 0.36552845528455286,
      "step": 1686,
      "training_loss": 3.918245792388916
    },
    {
      "epoch": 0.36552845528455286,
      "step": 1686,
      "training_loss": 6.888444423675537
    },
    {
      "epoch": 0.3657452574525745,
      "step": 1687,
      "training_loss": 5.995876312255859
    },
    {
      "epoch": 0.3657452574525745,
      "step": 1687,
      "training_loss": 6.779206275939941
    },
    {
      "epoch": 0.3657452574525745,
      "step": 1687,
      "training_loss": 5.1513237953186035
    },
    {
      "epoch": 0.3657452574525745,
      "step": 1687,
      "training_loss": 6.223159313201904
    },
    {
      "epoch": 0.3659620596205962,
      "grad_norm": 8.749671936035156,
      "learning_rate": 1e-05,
      "loss": 6.7066,
      "step": 1688
    },
    {
      "epoch": 0.3659620596205962,
      "step": 1688,
      "training_loss": 8.856369972229004
    },
    {
      "epoch": 0.3659620596205962,
      "step": 1688,
      "training_loss": 6.505768299102783
    },
    {
      "epoch": 0.3659620596205962,
      "step": 1688,
      "training_loss": 7.277553081512451
    },
    {
      "epoch": 0.3659620596205962,
      "step": 1688,
      "training_loss": 7.324300765991211
    },
    {
      "epoch": 0.36617886178861786,
      "step": 1689,
      "training_loss": 7.546770095825195
    },
    {
      "epoch": 0.36617886178861786,
      "step": 1689,
      "training_loss": 7.09352970123291
    },
    {
      "epoch": 0.36617886178861786,
      "step": 1689,
      "training_loss": 8.01525592803955
    },
    {
      "epoch": 0.36617886178861786,
      "step": 1689,
      "training_loss": 6.872211456298828
    },
    {
      "epoch": 0.3663956639566396,
      "step": 1690,
      "training_loss": 6.442684173583984
    },
    {
      "epoch": 0.3663956639566396,
      "step": 1690,
      "training_loss": 6.18049955368042
    },
    {
      "epoch": 0.3663956639566396,
      "step": 1690,
      "training_loss": 6.748684883117676
    },
    {
      "epoch": 0.3663956639566396,
      "step": 1690,
      "training_loss": 6.794894695281982
    },
    {
      "epoch": 0.36661246612466125,
      "step": 1691,
      "training_loss": 7.881214141845703
    },
    {
      "epoch": 0.36661246612466125,
      "step": 1691,
      "training_loss": 6.8969316482543945
    },
    {
      "epoch": 0.36661246612466125,
      "step": 1691,
      "training_loss": 6.557491302490234
    },
    {
      "epoch": 0.36661246612466125,
      "step": 1691,
      "training_loss": 7.487244606018066
    },
    {
      "epoch": 0.3668292682926829,
      "grad_norm": 8.011662483215332,
      "learning_rate": 1e-05,
      "loss": 7.1551,
      "step": 1692
    },
    {
      "epoch": 0.3668292682926829,
      "step": 1692,
      "training_loss": 6.588006496429443
    },
    {
      "epoch": 0.3668292682926829,
      "step": 1692,
      "training_loss": 6.732091903686523
    },
    {
      "epoch": 0.3668292682926829,
      "step": 1692,
      "training_loss": 7.349502086639404
    },
    {
      "epoch": 0.3668292682926829,
      "step": 1692,
      "training_loss": 9.306407928466797
    },
    {
      "epoch": 0.3670460704607046,
      "step": 1693,
      "training_loss": 7.503824710845947
    },
    {
      "epoch": 0.3670460704607046,
      "step": 1693,
      "training_loss": 6.0495429039001465
    },
    {
      "epoch": 0.3670460704607046,
      "step": 1693,
      "training_loss": 7.346329689025879
    },
    {
      "epoch": 0.3670460704607046,
      "step": 1693,
      "training_loss": 6.9621100425720215
    },
    {
      "epoch": 0.3672628726287263,
      "step": 1694,
      "training_loss": 8.094799995422363
    },
    {
      "epoch": 0.3672628726287263,
      "step": 1694,
      "training_loss": 5.726823806762695
    },
    {
      "epoch": 0.3672628726287263,
      "step": 1694,
      "training_loss": 7.4876627922058105
    },
    {
      "epoch": 0.3672628726287263,
      "step": 1694,
      "training_loss": 7.250857830047607
    },
    {
      "epoch": 0.367479674796748,
      "step": 1695,
      "training_loss": 6.597093105316162
    },
    {
      "epoch": 0.367479674796748,
      "step": 1695,
      "training_loss": 6.966362953186035
    },
    {
      "epoch": 0.367479674796748,
      "step": 1695,
      "training_loss": 6.746964454650879
    },
    {
      "epoch": 0.367479674796748,
      "step": 1695,
      "training_loss": 7.432823657989502
    },
    {
      "epoch": 0.36769647696476965,
      "grad_norm": 10.307558059692383,
      "learning_rate": 1e-05,
      "loss": 7.1338,
      "step": 1696
    },
    {
      "epoch": 0.36769647696476965,
      "step": 1696,
      "training_loss": 6.5038299560546875
    },
    {
      "epoch": 0.36769647696476965,
      "step": 1696,
      "training_loss": 8.636213302612305
    },
    {
      "epoch": 0.36769647696476965,
      "step": 1696,
      "training_loss": 7.070670127868652
    },
    {
      "epoch": 0.36769647696476965,
      "step": 1696,
      "training_loss": 6.8507890701293945
    },
    {
      "epoch": 0.3679132791327913,
      "step": 1697,
      "training_loss": 7.37852144241333
    },
    {
      "epoch": 0.3679132791327913,
      "step": 1697,
      "training_loss": 6.4799580574035645
    },
    {
      "epoch": 0.3679132791327913,
      "step": 1697,
      "training_loss": 6.959658145904541
    },
    {
      "epoch": 0.3679132791327913,
      "step": 1697,
      "training_loss": 4.461940288543701
    },
    {
      "epoch": 0.368130081300813,
      "step": 1698,
      "training_loss": 5.715892791748047
    },
    {
      "epoch": 0.368130081300813,
      "step": 1698,
      "training_loss": 6.834873676300049
    },
    {
      "epoch": 0.368130081300813,
      "step": 1698,
      "training_loss": 5.191016674041748
    },
    {
      "epoch": 0.368130081300813,
      "step": 1698,
      "training_loss": 7.281731128692627
    },
    {
      "epoch": 0.3683468834688347,
      "step": 1699,
      "training_loss": 6.169281005859375
    },
    {
      "epoch": 0.3683468834688347,
      "step": 1699,
      "training_loss": 7.587100982666016
    },
    {
      "epoch": 0.3683468834688347,
      "step": 1699,
      "training_loss": 6.744732856750488
    },
    {
      "epoch": 0.3683468834688347,
      "step": 1699,
      "training_loss": 6.832852363586426
    },
    {
      "epoch": 0.3685636856368564,
      "grad_norm": 12.867781639099121,
      "learning_rate": 1e-05,
      "loss": 6.6687,
      "step": 1700
    },
    {
      "epoch": 0.3685636856368564,
      "step": 1700,
      "training_loss": 6.531707763671875
    },
    {
      "epoch": 0.3685636856368564,
      "step": 1700,
      "training_loss": 6.8858323097229
    },
    {
      "epoch": 0.3685636856368564,
      "step": 1700,
      "training_loss": 5.185482025146484
    },
    {
      "epoch": 0.3685636856368564,
      "step": 1700,
      "training_loss": 6.865591526031494
    },
    {
      "epoch": 0.36878048780487804,
      "step": 1701,
      "training_loss": 7.259845733642578
    },
    {
      "epoch": 0.36878048780487804,
      "step": 1701,
      "training_loss": 6.449408531188965
    },
    {
      "epoch": 0.36878048780487804,
      "step": 1701,
      "training_loss": 7.5872344970703125
    },
    {
      "epoch": 0.36878048780487804,
      "step": 1701,
      "training_loss": 7.218433856964111
    },
    {
      "epoch": 0.3689972899728997,
      "step": 1702,
      "training_loss": 9.245667457580566
    },
    {
      "epoch": 0.3689972899728997,
      "step": 1702,
      "training_loss": 6.874835014343262
    },
    {
      "epoch": 0.3689972899728997,
      "step": 1702,
      "training_loss": 8.685964584350586
    },
    {
      "epoch": 0.3689972899728997,
      "step": 1702,
      "training_loss": 7.121236324310303
    },
    {
      "epoch": 0.36921409214092143,
      "step": 1703,
      "training_loss": 7.378653049468994
    },
    {
      "epoch": 0.36921409214092143,
      "step": 1703,
      "training_loss": 7.127613067626953
    },
    {
      "epoch": 0.36921409214092143,
      "step": 1703,
      "training_loss": 6.230998992919922
    },
    {
      "epoch": 0.36921409214092143,
      "step": 1703,
      "training_loss": 7.13712215423584
    },
    {
      "epoch": 0.3694308943089431,
      "grad_norm": 11.50800609588623,
      "learning_rate": 1e-05,
      "loss": 7.1116,
      "step": 1704
    },
    {
      "epoch": 0.3694308943089431,
      "step": 1704,
      "training_loss": 5.903327941894531
    },
    {
      "epoch": 0.3694308943089431,
      "step": 1704,
      "training_loss": 6.428694725036621
    },
    {
      "epoch": 0.3694308943089431,
      "step": 1704,
      "training_loss": 6.799359321594238
    },
    {
      "epoch": 0.3694308943089431,
      "step": 1704,
      "training_loss": 7.20259952545166
    },
    {
      "epoch": 0.36964769647696477,
      "step": 1705,
      "training_loss": 7.261512279510498
    },
    {
      "epoch": 0.36964769647696477,
      "step": 1705,
      "training_loss": 6.09173059463501
    },
    {
      "epoch": 0.36964769647696477,
      "step": 1705,
      "training_loss": 7.01450252532959
    },
    {
      "epoch": 0.36964769647696477,
      "step": 1705,
      "training_loss": 7.6902666091918945
    },
    {
      "epoch": 0.36986449864498644,
      "step": 1706,
      "training_loss": 6.994780540466309
    },
    {
      "epoch": 0.36986449864498644,
      "step": 1706,
      "training_loss": 7.652253150939941
    },
    {
      "epoch": 0.36986449864498644,
      "step": 1706,
      "training_loss": 7.710801124572754
    },
    {
      "epoch": 0.36986449864498644,
      "step": 1706,
      "training_loss": 7.156329154968262
    },
    {
      "epoch": 0.3700813008130081,
      "step": 1707,
      "training_loss": 7.452376842498779
    },
    {
      "epoch": 0.3700813008130081,
      "step": 1707,
      "training_loss": 6.810173511505127
    },
    {
      "epoch": 0.3700813008130081,
      "step": 1707,
      "training_loss": 4.978171348571777
    },
    {
      "epoch": 0.3700813008130081,
      "step": 1707,
      "training_loss": 7.969327449798584
    },
    {
      "epoch": 0.37029810298102983,
      "grad_norm": 13.130169868469238,
      "learning_rate": 1e-05,
      "loss": 6.9448,
      "step": 1708
    },
    {
      "epoch": 0.37029810298102983,
      "step": 1708,
      "training_loss": 7.230527877807617
    },
    {
      "epoch": 0.37029810298102983,
      "step": 1708,
      "training_loss": 7.0443806648254395
    },
    {
      "epoch": 0.37029810298102983,
      "step": 1708,
      "training_loss": 7.306171417236328
    },
    {
      "epoch": 0.37029810298102983,
      "step": 1708,
      "training_loss": 5.743917465209961
    },
    {
      "epoch": 0.3705149051490515,
      "step": 1709,
      "training_loss": 6.201673984527588
    },
    {
      "epoch": 0.3705149051490515,
      "step": 1709,
      "training_loss": 7.694666862487793
    },
    {
      "epoch": 0.3705149051490515,
      "step": 1709,
      "training_loss": 6.663734436035156
    },
    {
      "epoch": 0.3705149051490515,
      "step": 1709,
      "training_loss": 5.878359794616699
    },
    {
      "epoch": 0.37073170731707317,
      "step": 1710,
      "training_loss": 7.137277126312256
    },
    {
      "epoch": 0.37073170731707317,
      "step": 1710,
      "training_loss": 7.021060466766357
    },
    {
      "epoch": 0.37073170731707317,
      "step": 1710,
      "training_loss": 6.786210536956787
    },
    {
      "epoch": 0.37073170731707317,
      "step": 1710,
      "training_loss": 5.751482963562012
    },
    {
      "epoch": 0.37094850948509484,
      "step": 1711,
      "training_loss": 7.497482776641846
    },
    {
      "epoch": 0.37094850948509484,
      "step": 1711,
      "training_loss": 6.6252665519714355
    },
    {
      "epoch": 0.37094850948509484,
      "step": 1711,
      "training_loss": 6.6860246658325195
    },
    {
      "epoch": 0.37094850948509484,
      "step": 1711,
      "training_loss": 6.861536502838135
    },
    {
      "epoch": 0.37116531165311656,
      "grad_norm": 30.83160400390625,
      "learning_rate": 1e-05,
      "loss": 6.7581,
      "step": 1712
    },
    {
      "epoch": 0.37116531165311656,
      "step": 1712,
      "training_loss": 6.0784454345703125
    },
    {
      "epoch": 0.37116531165311656,
      "step": 1712,
      "training_loss": 7.03397274017334
    },
    {
      "epoch": 0.37116531165311656,
      "step": 1712,
      "training_loss": 7.045401573181152
    },
    {
      "epoch": 0.37116531165311656,
      "step": 1712,
      "training_loss": 6.993930339813232
    },
    {
      "epoch": 0.3713821138211382,
      "step": 1713,
      "training_loss": 5.765843391418457
    },
    {
      "epoch": 0.3713821138211382,
      "step": 1713,
      "training_loss": 6.54266357421875
    },
    {
      "epoch": 0.3713821138211382,
      "step": 1713,
      "training_loss": 7.390023231506348
    },
    {
      "epoch": 0.3713821138211382,
      "step": 1713,
      "training_loss": 7.791756629943848
    },
    {
      "epoch": 0.3715989159891599,
      "step": 1714,
      "training_loss": 7.1250691413879395
    },
    {
      "epoch": 0.3715989159891599,
      "step": 1714,
      "training_loss": 7.620538234710693
    },
    {
      "epoch": 0.3715989159891599,
      "step": 1714,
      "training_loss": 6.48237943649292
    },
    {
      "epoch": 0.3715989159891599,
      "step": 1714,
      "training_loss": 7.7695631980896
    },
    {
      "epoch": 0.37181571815718156,
      "step": 1715,
      "training_loss": 6.921669006347656
    },
    {
      "epoch": 0.37181571815718156,
      "step": 1715,
      "training_loss": 7.8871870040893555
    },
    {
      "epoch": 0.37181571815718156,
      "step": 1715,
      "training_loss": 7.199451446533203
    },
    {
      "epoch": 0.37181571815718156,
      "step": 1715,
      "training_loss": 7.728370666503906
    },
    {
      "epoch": 0.37203252032520323,
      "grad_norm": 18.980958938598633,
      "learning_rate": 1e-05,
      "loss": 7.086,
      "step": 1716
    },
    {
      "epoch": 0.37203252032520323,
      "step": 1716,
      "training_loss": 7.409213542938232
    },
    {
      "epoch": 0.37203252032520323,
      "step": 1716,
      "training_loss": 8.447017669677734
    },
    {
      "epoch": 0.37203252032520323,
      "step": 1716,
      "training_loss": 4.826411724090576
    },
    {
      "epoch": 0.37203252032520323,
      "step": 1716,
      "training_loss": 5.995752334594727
    },
    {
      "epoch": 0.37224932249322495,
      "step": 1717,
      "training_loss": 6.494748115539551
    },
    {
      "epoch": 0.37224932249322495,
      "step": 1717,
      "training_loss": 6.784407615661621
    },
    {
      "epoch": 0.37224932249322495,
      "step": 1717,
      "training_loss": 7.1160359382629395
    },
    {
      "epoch": 0.37224932249322495,
      "step": 1717,
      "training_loss": 6.276813983917236
    },
    {
      "epoch": 0.3724661246612466,
      "step": 1718,
      "training_loss": 7.82333517074585
    },
    {
      "epoch": 0.3724661246612466,
      "step": 1718,
      "training_loss": 7.617834568023682
    },
    {
      "epoch": 0.3724661246612466,
      "step": 1718,
      "training_loss": 6.923198223114014
    },
    {
      "epoch": 0.3724661246612466,
      "step": 1718,
      "training_loss": 6.599327087402344
    },
    {
      "epoch": 0.3726829268292683,
      "step": 1719,
      "training_loss": 6.4899821281433105
    },
    {
      "epoch": 0.3726829268292683,
      "step": 1719,
      "training_loss": 7.964627265930176
    },
    {
      "epoch": 0.3726829268292683,
      "step": 1719,
      "training_loss": 6.857070446014404
    },
    {
      "epoch": 0.3726829268292683,
      "step": 1719,
      "training_loss": 5.7394537925720215
    },
    {
      "epoch": 0.37289972899728996,
      "grad_norm": 10.738057136535645,
      "learning_rate": 1e-05,
      "loss": 6.8353,
      "step": 1720
    },
    {
      "epoch": 0.37289972899728996,
      "step": 1720,
      "training_loss": 8.47487735748291
    },
    {
      "epoch": 0.37289972899728996,
      "step": 1720,
      "training_loss": 7.118866920471191
    },
    {
      "epoch": 0.37289972899728996,
      "step": 1720,
      "training_loss": 5.380270481109619
    },
    {
      "epoch": 0.37289972899728996,
      "step": 1720,
      "training_loss": 7.277040481567383
    },
    {
      "epoch": 0.3731165311653116,
      "step": 1721,
      "training_loss": 6.398674011230469
    },
    {
      "epoch": 0.3731165311653116,
      "step": 1721,
      "training_loss": 9.168145179748535
    },
    {
      "epoch": 0.3731165311653116,
      "step": 1721,
      "training_loss": 7.083852767944336
    },
    {
      "epoch": 0.3731165311653116,
      "step": 1721,
      "training_loss": 5.4466657638549805
    },
    {
      "epoch": 0.37333333333333335,
      "step": 1722,
      "training_loss": 6.245082855224609
    },
    {
      "epoch": 0.37333333333333335,
      "step": 1722,
      "training_loss": 6.567190170288086
    },
    {
      "epoch": 0.37333333333333335,
      "step": 1722,
      "training_loss": 5.432550430297852
    },
    {
      "epoch": 0.37333333333333335,
      "step": 1722,
      "training_loss": 5.65767765045166
    },
    {
      "epoch": 0.373550135501355,
      "step": 1723,
      "training_loss": 8.891239166259766
    },
    {
      "epoch": 0.373550135501355,
      "step": 1723,
      "training_loss": 6.689113140106201
    },
    {
      "epoch": 0.373550135501355,
      "step": 1723,
      "training_loss": 7.853936195373535
    },
    {
      "epoch": 0.373550135501355,
      "step": 1723,
      "training_loss": 7.461939334869385
    },
    {
      "epoch": 0.3737669376693767,
      "grad_norm": 20.250947952270508,
      "learning_rate": 1e-05,
      "loss": 6.9467,
      "step": 1724
    },
    {
      "epoch": 0.3737669376693767,
      "step": 1724,
      "training_loss": 6.925532817840576
    },
    {
      "epoch": 0.3737669376693767,
      "step": 1724,
      "training_loss": 4.236240863800049
    },
    {
      "epoch": 0.3737669376693767,
      "step": 1724,
      "training_loss": 6.791351795196533
    },
    {
      "epoch": 0.3737669376693767,
      "step": 1724,
      "training_loss": 6.691317558288574
    },
    {
      "epoch": 0.37398373983739835,
      "step": 1725,
      "training_loss": 7.389069557189941
    },
    {
      "epoch": 0.37398373983739835,
      "step": 1725,
      "training_loss": 5.629187107086182
    },
    {
      "epoch": 0.37398373983739835,
      "step": 1725,
      "training_loss": 6.685420513153076
    },
    {
      "epoch": 0.37398373983739835,
      "step": 1725,
      "training_loss": 8.544066429138184
    },
    {
      "epoch": 0.3742005420054201,
      "step": 1726,
      "training_loss": 6.203817844390869
    },
    {
      "epoch": 0.3742005420054201,
      "step": 1726,
      "training_loss": 9.076380729675293
    },
    {
      "epoch": 0.3742005420054201,
      "step": 1726,
      "training_loss": 8.063587188720703
    },
    {
      "epoch": 0.3742005420054201,
      "step": 1726,
      "training_loss": 7.295113563537598
    },
    {
      "epoch": 0.37441734417344175,
      "step": 1727,
      "training_loss": 9.71883773803711
    },
    {
      "epoch": 0.37441734417344175,
      "step": 1727,
      "training_loss": 6.988846778869629
    },
    {
      "epoch": 0.37441734417344175,
      "step": 1727,
      "training_loss": 6.228342056274414
    },
    {
      "epoch": 0.37441734417344175,
      "step": 1727,
      "training_loss": 5.842641353607178
    },
    {
      "epoch": 0.3746341463414634,
      "grad_norm": 20.20690155029297,
      "learning_rate": 1e-05,
      "loss": 7.0194,
      "step": 1728
    },
    {
      "epoch": 0.3746341463414634,
      "step": 1728,
      "training_loss": 7.396588325500488
    },
    {
      "epoch": 0.3746341463414634,
      "step": 1728,
      "training_loss": 4.955908298492432
    },
    {
      "epoch": 0.3746341463414634,
      "step": 1728,
      "training_loss": 6.851317882537842
    },
    {
      "epoch": 0.3746341463414634,
      "step": 1728,
      "training_loss": 5.459029197692871
    },
    {
      "epoch": 0.3748509485094851,
      "step": 1729,
      "training_loss": 7.495425224304199
    },
    {
      "epoch": 0.3748509485094851,
      "step": 1729,
      "training_loss": 7.071164608001709
    },
    {
      "epoch": 0.3748509485094851,
      "step": 1729,
      "training_loss": 5.212894439697266
    },
    {
      "epoch": 0.3748509485094851,
      "step": 1729,
      "training_loss": 6.320476055145264
    },
    {
      "epoch": 0.37506775067750675,
      "step": 1730,
      "training_loss": 7.938924312591553
    },
    {
      "epoch": 0.37506775067750675,
      "step": 1730,
      "training_loss": 7.6005988121032715
    },
    {
      "epoch": 0.37506775067750675,
      "step": 1730,
      "training_loss": 5.764976978302002
    },
    {
      "epoch": 0.37506775067750675,
      "step": 1730,
      "training_loss": 6.761519432067871
    },
    {
      "epoch": 0.3752845528455285,
      "step": 1731,
      "training_loss": 6.120822429656982
    },
    {
      "epoch": 0.3752845528455285,
      "step": 1731,
      "training_loss": 7.032553672790527
    },
    {
      "epoch": 0.3752845528455285,
      "step": 1731,
      "training_loss": 5.128564357757568
    },
    {
      "epoch": 0.3752845528455285,
      "step": 1731,
      "training_loss": 7.02775239944458
    },
    {
      "epoch": 0.37550135501355014,
      "grad_norm": 14.133788108825684,
      "learning_rate": 1e-05,
      "loss": 6.5087,
      "step": 1732
    },
    {
      "epoch": 0.37550135501355014,
      "step": 1732,
      "training_loss": 4.583703517913818
    },
    {
      "epoch": 0.37550135501355014,
      "step": 1732,
      "training_loss": 6.081716537475586
    },
    {
      "epoch": 0.37550135501355014,
      "step": 1732,
      "training_loss": 5.408565044403076
    },
    {
      "epoch": 0.37550135501355014,
      "step": 1732,
      "training_loss": 7.2709431648254395
    },
    {
      "epoch": 0.3757181571815718,
      "step": 1733,
      "training_loss": 5.697758197784424
    },
    {
      "epoch": 0.3757181571815718,
      "step": 1733,
      "training_loss": 7.498837947845459
    },
    {
      "epoch": 0.3757181571815718,
      "step": 1733,
      "training_loss": 5.823853969573975
    },
    {
      "epoch": 0.3757181571815718,
      "step": 1733,
      "training_loss": 7.879730224609375
    },
    {
      "epoch": 0.3759349593495935,
      "step": 1734,
      "training_loss": 6.914913177490234
    },
    {
      "epoch": 0.3759349593495935,
      "step": 1734,
      "training_loss": 7.645260810852051
    },
    {
      "epoch": 0.3759349593495935,
      "step": 1734,
      "training_loss": 7.082399368286133
    },
    {
      "epoch": 0.3759349593495935,
      "step": 1734,
      "training_loss": 6.838405132293701
    },
    {
      "epoch": 0.3761517615176152,
      "step": 1735,
      "training_loss": 5.909290790557861
    },
    {
      "epoch": 0.3761517615176152,
      "step": 1735,
      "training_loss": 7.015606880187988
    },
    {
      "epoch": 0.3761517615176152,
      "step": 1735,
      "training_loss": 7.289491176605225
    },
    {
      "epoch": 0.3761517615176152,
      "step": 1735,
      "training_loss": 6.892923355102539
    },
    {
      "epoch": 0.37636856368563687,
      "grad_norm": 10.175223350524902,
      "learning_rate": 1e-05,
      "loss": 6.6146,
      "step": 1736
    },
    {
      "epoch": 0.37636856368563687,
      "step": 1736,
      "training_loss": 6.8888750076293945
    },
    {
      "epoch": 0.37636856368563687,
      "step": 1736,
      "training_loss": 4.6204423904418945
    },
    {
      "epoch": 0.37636856368563687,
      "step": 1736,
      "training_loss": 6.216033935546875
    },
    {
      "epoch": 0.37636856368563687,
      "step": 1736,
      "training_loss": 7.054903507232666
    },
    {
      "epoch": 0.37658536585365854,
      "step": 1737,
      "training_loss": 7.818185329437256
    },
    {
      "epoch": 0.37658536585365854,
      "step": 1737,
      "training_loss": 5.811028480529785
    },
    {
      "epoch": 0.37658536585365854,
      "step": 1737,
      "training_loss": 6.911950588226318
    },
    {
      "epoch": 0.37658536585365854,
      "step": 1737,
      "training_loss": 7.307302474975586
    },
    {
      "epoch": 0.3768021680216802,
      "step": 1738,
      "training_loss": 7.138708114624023
    },
    {
      "epoch": 0.3768021680216802,
      "step": 1738,
      "training_loss": 7.2269368171691895
    },
    {
      "epoch": 0.3768021680216802,
      "step": 1738,
      "training_loss": 7.485801696777344
    },
    {
      "epoch": 0.3768021680216802,
      "step": 1738,
      "training_loss": 5.7592573165893555
    },
    {
      "epoch": 0.3770189701897019,
      "step": 1739,
      "training_loss": 7.8889875411987305
    },
    {
      "epoch": 0.3770189701897019,
      "step": 1739,
      "training_loss": 5.8979105949401855
    },
    {
      "epoch": 0.3770189701897019,
      "step": 1739,
      "training_loss": 6.945556640625
    },
    {
      "epoch": 0.3770189701897019,
      "step": 1739,
      "training_loss": 8.737360000610352
    },
    {
      "epoch": 0.3772357723577236,
      "grad_norm": 12.42385482788086,
      "learning_rate": 1e-05,
      "loss": 6.8568,
      "step": 1740
    },
    {
      "epoch": 0.3772357723577236,
      "step": 1740,
      "training_loss": 8.335494995117188
    },
    {
      "epoch": 0.3772357723577236,
      "step": 1740,
      "training_loss": 7.493769645690918
    },
    {
      "epoch": 0.3772357723577236,
      "step": 1740,
      "training_loss": 6.338626384735107
    },
    {
      "epoch": 0.3772357723577236,
      "step": 1740,
      "training_loss": 7.3475847244262695
    },
    {
      "epoch": 0.37745257452574527,
      "step": 1741,
      "training_loss": 6.042696952819824
    },
    {
      "epoch": 0.37745257452574527,
      "step": 1741,
      "training_loss": 6.067044258117676
    },
    {
      "epoch": 0.37745257452574527,
      "step": 1741,
      "training_loss": 6.672798156738281
    },
    {
      "epoch": 0.37745257452574527,
      "step": 1741,
      "training_loss": 6.403214454650879
    },
    {
      "epoch": 0.37766937669376693,
      "step": 1742,
      "training_loss": 8.011043548583984
    },
    {
      "epoch": 0.37766937669376693,
      "step": 1742,
      "training_loss": 6.2248454093933105
    },
    {
      "epoch": 0.37766937669376693,
      "step": 1742,
      "training_loss": 7.13377046585083
    },
    {
      "epoch": 0.37766937669376693,
      "step": 1742,
      "training_loss": 6.758477210998535
    },
    {
      "epoch": 0.3778861788617886,
      "step": 1743,
      "training_loss": 7.724386692047119
    },
    {
      "epoch": 0.3778861788617886,
      "step": 1743,
      "training_loss": 6.823332786560059
    },
    {
      "epoch": 0.3778861788617886,
      "step": 1743,
      "training_loss": 6.935319423675537
    },
    {
      "epoch": 0.3778861788617886,
      "step": 1743,
      "training_loss": 6.815923690795898
    },
    {
      "epoch": 0.3781029810298103,
      "grad_norm": 16.19879722595215,
      "learning_rate": 1e-05,
      "loss": 6.9455,
      "step": 1744
    },
    {
      "epoch": 0.3781029810298103,
      "step": 1744,
      "training_loss": 6.7710676193237305
    },
    {
      "epoch": 0.3781029810298103,
      "step": 1744,
      "training_loss": 7.72130012512207
    },
    {
      "epoch": 0.3781029810298103,
      "step": 1744,
      "training_loss": 6.907829761505127
    },
    {
      "epoch": 0.3781029810298103,
      "step": 1744,
      "training_loss": 7.989322185516357
    },
    {
      "epoch": 0.378319783197832,
      "step": 1745,
      "training_loss": 6.965546131134033
    },
    {
      "epoch": 0.378319783197832,
      "step": 1745,
      "training_loss": 6.932740688323975
    },
    {
      "epoch": 0.378319783197832,
      "step": 1745,
      "training_loss": 7.500763416290283
    },
    {
      "epoch": 0.378319783197832,
      "step": 1745,
      "training_loss": 7.771521091461182
    },
    {
      "epoch": 0.37853658536585366,
      "step": 1746,
      "training_loss": 8.103300094604492
    },
    {
      "epoch": 0.37853658536585366,
      "step": 1746,
      "training_loss": 8.415884971618652
    },
    {
      "epoch": 0.37853658536585366,
      "step": 1746,
      "training_loss": 5.694822788238525
    },
    {
      "epoch": 0.37853658536585366,
      "step": 1746,
      "training_loss": 5.554482460021973
    },
    {
      "epoch": 0.37875338753387533,
      "step": 1747,
      "training_loss": 6.951190948486328
    },
    {
      "epoch": 0.37875338753387533,
      "step": 1747,
      "training_loss": 6.283847332000732
    },
    {
      "epoch": 0.37875338753387533,
      "step": 1747,
      "training_loss": 5.114089012145996
    },
    {
      "epoch": 0.37875338753387533,
      "step": 1747,
      "training_loss": 7.377551078796387
    },
    {
      "epoch": 0.378970189701897,
      "grad_norm": 12.03246021270752,
      "learning_rate": 1e-05,
      "loss": 7.0035,
      "step": 1748
    },
    {
      "epoch": 0.378970189701897,
      "step": 1748,
      "training_loss": 6.602658271789551
    },
    {
      "epoch": 0.378970189701897,
      "step": 1748,
      "training_loss": 9.55819320678711
    },
    {
      "epoch": 0.378970189701897,
      "step": 1748,
      "training_loss": 7.183732032775879
    },
    {
      "epoch": 0.378970189701897,
      "step": 1748,
      "training_loss": 7.6955885887146
    },
    {
      "epoch": 0.3791869918699187,
      "step": 1749,
      "training_loss": 6.5928449630737305
    },
    {
      "epoch": 0.3791869918699187,
      "step": 1749,
      "training_loss": 8.012932777404785
    },
    {
      "epoch": 0.3791869918699187,
      "step": 1749,
      "training_loss": 6.7899394035339355
    },
    {
      "epoch": 0.3791869918699187,
      "step": 1749,
      "training_loss": 7.5177130699157715
    },
    {
      "epoch": 0.3794037940379404,
      "step": 1750,
      "training_loss": 6.105759620666504
    },
    {
      "epoch": 0.3794037940379404,
      "step": 1750,
      "training_loss": 6.436723709106445
    },
    {
      "epoch": 0.3794037940379404,
      "step": 1750,
      "training_loss": 8.856595039367676
    },
    {
      "epoch": 0.3794037940379404,
      "step": 1750,
      "training_loss": 7.466708183288574
    },
    {
      "epoch": 0.37962059620596206,
      "step": 1751,
      "training_loss": 7.971372127532959
    },
    {
      "epoch": 0.37962059620596206,
      "step": 1751,
      "training_loss": 5.152716636657715
    },
    {
      "epoch": 0.37962059620596206,
      "step": 1751,
      "training_loss": 7.785099506378174
    },
    {
      "epoch": 0.37962059620596206,
      "step": 1751,
      "training_loss": 4.552044868469238
    },
    {
      "epoch": 0.3798373983739837,
      "grad_norm": 12.0724458694458,
      "learning_rate": 1e-05,
      "loss": 7.1425,
      "step": 1752
    },
    {
      "epoch": 0.3798373983739837,
      "step": 1752,
      "training_loss": 6.573974609375
    },
    {
      "epoch": 0.3798373983739837,
      "step": 1752,
      "training_loss": 6.946788311004639
    },
    {
      "epoch": 0.3798373983739837,
      "step": 1752,
      "training_loss": 7.351075172424316
    },
    {
      "epoch": 0.3798373983739837,
      "step": 1752,
      "training_loss": 6.7664899826049805
    },
    {
      "epoch": 0.3800542005420054,
      "step": 1753,
      "training_loss": 6.829586505889893
    },
    {
      "epoch": 0.3800542005420054,
      "step": 1753,
      "training_loss": 8.20547103881836
    },
    {
      "epoch": 0.3800542005420054,
      "step": 1753,
      "training_loss": 7.633475303649902
    },
    {
      "epoch": 0.3800542005420054,
      "step": 1753,
      "training_loss": 6.138539791107178
    },
    {
      "epoch": 0.3802710027100271,
      "step": 1754,
      "training_loss": 7.245657444000244
    },
    {
      "epoch": 0.3802710027100271,
      "step": 1754,
      "training_loss": 6.929986476898193
    },
    {
      "epoch": 0.3802710027100271,
      "step": 1754,
      "training_loss": 7.314255237579346
    },
    {
      "epoch": 0.3802710027100271,
      "step": 1754,
      "training_loss": 6.410684108734131
    },
    {
      "epoch": 0.3804878048780488,
      "step": 1755,
      "training_loss": 7.023324012756348
    },
    {
      "epoch": 0.3804878048780488,
      "step": 1755,
      "training_loss": 7.60191535949707
    },
    {
      "epoch": 0.3804878048780488,
      "step": 1755,
      "training_loss": 6.761828899383545
    },
    {
      "epoch": 0.3804878048780488,
      "step": 1755,
      "training_loss": 7.751462459564209
    },
    {
      "epoch": 0.38070460704607045,
      "grad_norm": 12.534482955932617,
      "learning_rate": 1e-05,
      "loss": 7.0928,
      "step": 1756
    },
    {
      "epoch": 0.38070460704607045,
      "step": 1756,
      "training_loss": 6.145917892456055
    },
    {
      "epoch": 0.38070460704607045,
      "step": 1756,
      "training_loss": 5.998371601104736
    },
    {
      "epoch": 0.38070460704607045,
      "step": 1756,
      "training_loss": 6.989431858062744
    },
    {
      "epoch": 0.38070460704607045,
      "step": 1756,
      "training_loss": 6.9270710945129395
    },
    {
      "epoch": 0.3809214092140921,
      "step": 1757,
      "training_loss": 7.830364227294922
    },
    {
      "epoch": 0.3809214092140921,
      "step": 1757,
      "training_loss": 5.4314284324646
    },
    {
      "epoch": 0.3809214092140921,
      "step": 1757,
      "training_loss": 6.539542198181152
    },
    {
      "epoch": 0.3809214092140921,
      "step": 1757,
      "training_loss": 6.499624729156494
    },
    {
      "epoch": 0.38113821138211385,
      "step": 1758,
      "training_loss": 7.559450149536133
    },
    {
      "epoch": 0.38113821138211385,
      "step": 1758,
      "training_loss": 7.231332778930664
    },
    {
      "epoch": 0.38113821138211385,
      "step": 1758,
      "training_loss": 7.739800930023193
    },
    {
      "epoch": 0.38113821138211385,
      "step": 1758,
      "training_loss": 7.1511030197143555
    },
    {
      "epoch": 0.3813550135501355,
      "step": 1759,
      "training_loss": 7.642301082611084
    },
    {
      "epoch": 0.3813550135501355,
      "step": 1759,
      "training_loss": 6.3157806396484375
    },
    {
      "epoch": 0.3813550135501355,
      "step": 1759,
      "training_loss": 6.448112964630127
    },
    {
      "epoch": 0.3813550135501355,
      "step": 1759,
      "training_loss": 6.239654064178467
    },
    {
      "epoch": 0.3815718157181572,
      "grad_norm": 15.675093650817871,
      "learning_rate": 1e-05,
      "loss": 6.7931,
      "step": 1760
    },
    {
      "epoch": 0.3815718157181572,
      "step": 1760,
      "training_loss": 6.62177038192749
    },
    {
      "epoch": 0.3815718157181572,
      "step": 1760,
      "training_loss": 6.588141441345215
    },
    {
      "epoch": 0.3815718157181572,
      "step": 1760,
      "training_loss": 7.193245887756348
    },
    {
      "epoch": 0.3815718157181572,
      "step": 1760,
      "training_loss": 7.248629570007324
    },
    {
      "epoch": 0.38178861788617885,
      "step": 1761,
      "training_loss": 9.107278823852539
    },
    {
      "epoch": 0.38178861788617885,
      "step": 1761,
      "training_loss": 8.830804824829102
    },
    {
      "epoch": 0.38178861788617885,
      "step": 1761,
      "training_loss": 6.539987564086914
    },
    {
      "epoch": 0.38178861788617885,
      "step": 1761,
      "training_loss": 6.340684413909912
    },
    {
      "epoch": 0.3820054200542005,
      "step": 1762,
      "training_loss": 6.339652061462402
    },
    {
      "epoch": 0.3820054200542005,
      "step": 1762,
      "training_loss": 5.128067493438721
    },
    {
      "epoch": 0.3820054200542005,
      "step": 1762,
      "training_loss": 8.175581932067871
    },
    {
      "epoch": 0.3820054200542005,
      "step": 1762,
      "training_loss": 7.30057954788208
    },
    {
      "epoch": 0.38222222222222224,
      "step": 1763,
      "training_loss": 8.215672492980957
    },
    {
      "epoch": 0.38222222222222224,
      "step": 1763,
      "training_loss": 4.427018165588379
    },
    {
      "epoch": 0.38222222222222224,
      "step": 1763,
      "training_loss": 7.434797286987305
    },
    {
      "epoch": 0.38222222222222224,
      "step": 1763,
      "training_loss": 7.475817680358887
    },
    {
      "epoch": 0.3824390243902439,
      "grad_norm": 18.552806854248047,
      "learning_rate": 1e-05,
      "loss": 7.0605,
      "step": 1764
    },
    {
      "epoch": 0.3824390243902439,
      "step": 1764,
      "training_loss": 6.897288799285889
    },
    {
      "epoch": 0.3824390243902439,
      "step": 1764,
      "training_loss": 7.231258869171143
    },
    {
      "epoch": 0.3824390243902439,
      "step": 1764,
      "training_loss": 7.927798748016357
    },
    {
      "epoch": 0.3824390243902439,
      "step": 1764,
      "training_loss": 7.259566783905029
    },
    {
      "epoch": 0.3826558265582656,
      "step": 1765,
      "training_loss": 7.394286155700684
    },
    {
      "epoch": 0.3826558265582656,
      "step": 1765,
      "training_loss": 7.078211784362793
    },
    {
      "epoch": 0.3826558265582656,
      "step": 1765,
      "training_loss": 7.496518611907959
    },
    {
      "epoch": 0.3826558265582656,
      "step": 1765,
      "training_loss": 7.188920021057129
    },
    {
      "epoch": 0.38287262872628725,
      "step": 1766,
      "training_loss": 5.646343231201172
    },
    {
      "epoch": 0.38287262872628725,
      "step": 1766,
      "training_loss": 7.739194393157959
    },
    {
      "epoch": 0.38287262872628725,
      "step": 1766,
      "training_loss": 7.171651363372803
    },
    {
      "epoch": 0.38287262872628725,
      "step": 1766,
      "training_loss": 7.099861145019531
    },
    {
      "epoch": 0.38308943089430897,
      "step": 1767,
      "training_loss": 7.913536071777344
    },
    {
      "epoch": 0.38308943089430897,
      "step": 1767,
      "training_loss": 5.36080265045166
    },
    {
      "epoch": 0.38308943089430897,
      "step": 1767,
      "training_loss": 6.705816268920898
    },
    {
      "epoch": 0.38308943089430897,
      "step": 1767,
      "training_loss": 6.440118312835693
    },
    {
      "epoch": 0.38330623306233064,
      "grad_norm": 13.6639986038208,
      "learning_rate": 1e-05,
      "loss": 7.0344,
      "step": 1768
    },
    {
      "epoch": 0.38330623306233064,
      "step": 1768,
      "training_loss": 4.6651835441589355
    },
    {
      "epoch": 0.38330623306233064,
      "step": 1768,
      "training_loss": 6.7520527839660645
    },
    {
      "epoch": 0.38330623306233064,
      "step": 1768,
      "training_loss": 6.5689005851745605
    },
    {
      "epoch": 0.38330623306233064,
      "step": 1768,
      "training_loss": 7.218936443328857
    },
    {
      "epoch": 0.3835230352303523,
      "step": 1769,
      "training_loss": 4.742941856384277
    },
    {
      "epoch": 0.3835230352303523,
      "step": 1769,
      "training_loss": 8.224800109863281
    },
    {
      "epoch": 0.3835230352303523,
      "step": 1769,
      "training_loss": 7.452648639678955
    },
    {
      "epoch": 0.3835230352303523,
      "step": 1769,
      "training_loss": 5.753363609313965
    },
    {
      "epoch": 0.383739837398374,
      "step": 1770,
      "training_loss": 7.110918045043945
    },
    {
      "epoch": 0.383739837398374,
      "step": 1770,
      "training_loss": 7.76862907409668
    },
    {
      "epoch": 0.383739837398374,
      "step": 1770,
      "training_loss": 7.710855484008789
    },
    {
      "epoch": 0.383739837398374,
      "step": 1770,
      "training_loss": 6.52162504196167
    },
    {
      "epoch": 0.38395663956639564,
      "step": 1771,
      "training_loss": 6.728224277496338
    },
    {
      "epoch": 0.38395663956639564,
      "step": 1771,
      "training_loss": 4.925248146057129
    },
    {
      "epoch": 0.38395663956639564,
      "step": 1771,
      "training_loss": 8.26810073852539
    },
    {
      "epoch": 0.38395663956639564,
      "step": 1771,
      "training_loss": 4.982512474060059
    },
    {
      "epoch": 0.38417344173441736,
      "grad_norm": 15.092267990112305,
      "learning_rate": 1e-05,
      "loss": 6.5872,
      "step": 1772
    },
    {
      "epoch": 0.38417344173441736,
      "step": 1772,
      "training_loss": 5.838324069976807
    },
    {
      "epoch": 0.38417344173441736,
      "step": 1772,
      "training_loss": 6.333170413970947
    },
    {
      "epoch": 0.38417344173441736,
      "step": 1772,
      "training_loss": 6.447743892669678
    },
    {
      "epoch": 0.38417344173441736,
      "step": 1772,
      "training_loss": 6.126394748687744
    },
    {
      "epoch": 0.38439024390243903,
      "step": 1773,
      "training_loss": 7.409499645233154
    },
    {
      "epoch": 0.38439024390243903,
      "step": 1773,
      "training_loss": 7.463222026824951
    },
    {
      "epoch": 0.38439024390243903,
      "step": 1773,
      "training_loss": 6.415781497955322
    },
    {
      "epoch": 0.38439024390243903,
      "step": 1773,
      "training_loss": 7.41391658782959
    },
    {
      "epoch": 0.3846070460704607,
      "step": 1774,
      "training_loss": 8.288098335266113
    },
    {
      "epoch": 0.3846070460704607,
      "step": 1774,
      "training_loss": 7.360701084136963
    },
    {
      "epoch": 0.3846070460704607,
      "step": 1774,
      "training_loss": 5.284470558166504
    },
    {
      "epoch": 0.3846070460704607,
      "step": 1774,
      "training_loss": 5.657751083374023
    },
    {
      "epoch": 0.38482384823848237,
      "step": 1775,
      "training_loss": 7.265291690826416
    },
    {
      "epoch": 0.38482384823848237,
      "step": 1775,
      "training_loss": 6.196761608123779
    },
    {
      "epoch": 0.38482384823848237,
      "step": 1775,
      "training_loss": 6.319493293762207
    },
    {
      "epoch": 0.38482384823848237,
      "step": 1775,
      "training_loss": 6.509529113769531
    },
    {
      "epoch": 0.3850406504065041,
      "grad_norm": 12.000161170959473,
      "learning_rate": 1e-05,
      "loss": 6.6456,
      "step": 1776
    },
    {
      "epoch": 0.3850406504065041,
      "step": 1776,
      "training_loss": 6.015764236450195
    },
    {
      "epoch": 0.3850406504065041,
      "step": 1776,
      "training_loss": 6.987449645996094
    },
    {
      "epoch": 0.3850406504065041,
      "step": 1776,
      "training_loss": 7.262732982635498
    },
    {
      "epoch": 0.3850406504065041,
      "step": 1776,
      "training_loss": 7.196621417999268
    },
    {
      "epoch": 0.38525745257452576,
      "step": 1777,
      "training_loss": 6.9996490478515625
    },
    {
      "epoch": 0.38525745257452576,
      "step": 1777,
      "training_loss": 8.538321495056152
    },
    {
      "epoch": 0.38525745257452576,
      "step": 1777,
      "training_loss": 6.310145854949951
    },
    {
      "epoch": 0.38525745257452576,
      "step": 1777,
      "training_loss": 6.864439964294434
    },
    {
      "epoch": 0.38547425474254743,
      "step": 1778,
      "training_loss": 6.299463748931885
    },
    {
      "epoch": 0.38547425474254743,
      "step": 1778,
      "training_loss": 8.066932678222656
    },
    {
      "epoch": 0.38547425474254743,
      "step": 1778,
      "training_loss": 5.502048492431641
    },
    {
      "epoch": 0.38547425474254743,
      "step": 1778,
      "training_loss": 6.683802127838135
    },
    {
      "epoch": 0.3856910569105691,
      "step": 1779,
      "training_loss": 5.685250282287598
    },
    {
      "epoch": 0.3856910569105691,
      "step": 1779,
      "training_loss": 5.941386699676514
    },
    {
      "epoch": 0.3856910569105691,
      "step": 1779,
      "training_loss": 7.373898506164551
    },
    {
      "epoch": 0.3856910569105691,
      "step": 1779,
      "training_loss": 8.137752532958984
    },
    {
      "epoch": 0.38590785907859076,
      "grad_norm": 11.688458442687988,
      "learning_rate": 1e-05,
      "loss": 6.8666,
      "step": 1780
    },
    {
      "epoch": 0.38590785907859076,
      "step": 1780,
      "training_loss": 7.976484298706055
    },
    {
      "epoch": 0.38590785907859076,
      "step": 1780,
      "training_loss": 6.572079181671143
    },
    {
      "epoch": 0.38590785907859076,
      "step": 1780,
      "training_loss": 9.011366844177246
    },
    {
      "epoch": 0.38590785907859076,
      "step": 1780,
      "training_loss": 8.122790336608887
    },
    {
      "epoch": 0.3861246612466125,
      "step": 1781,
      "training_loss": 7.47820520401001
    },
    {
      "epoch": 0.3861246612466125,
      "step": 1781,
      "training_loss": 6.5671916007995605
    },
    {
      "epoch": 0.3861246612466125,
      "step": 1781,
      "training_loss": 6.57891845703125
    },
    {
      "epoch": 0.3861246612466125,
      "step": 1781,
      "training_loss": 6.627481460571289
    },
    {
      "epoch": 0.38634146341463416,
      "step": 1782,
      "training_loss": 6.8312668800354
    },
    {
      "epoch": 0.38634146341463416,
      "step": 1782,
      "training_loss": 7.671876907348633
    },
    {
      "epoch": 0.38634146341463416,
      "step": 1782,
      "training_loss": 7.659461975097656
    },
    {
      "epoch": 0.38634146341463416,
      "step": 1782,
      "training_loss": 6.4945549964904785
    },
    {
      "epoch": 0.3865582655826558,
      "step": 1783,
      "training_loss": 7.397139072418213
    },
    {
      "epoch": 0.3865582655826558,
      "step": 1783,
      "training_loss": 6.855741024017334
    },
    {
      "epoch": 0.3865582655826558,
      "step": 1783,
      "training_loss": 6.953240394592285
    },
    {
      "epoch": 0.3865582655826558,
      "step": 1783,
      "training_loss": 4.584287643432617
    },
    {
      "epoch": 0.3867750677506775,
      "grad_norm": 11.714080810546875,
      "learning_rate": 1e-05,
      "loss": 7.0864,
      "step": 1784
    },
    {
      "epoch": 0.3867750677506775,
      "step": 1784,
      "training_loss": 7.111969470977783
    },
    {
      "epoch": 0.3867750677506775,
      "step": 1784,
      "training_loss": 6.158313751220703
    },
    {
      "epoch": 0.3867750677506775,
      "step": 1784,
      "training_loss": 8.213050842285156
    },
    {
      "epoch": 0.3867750677506775,
      "step": 1784,
      "training_loss": 7.230630874633789
    },
    {
      "epoch": 0.38699186991869916,
      "step": 1785,
      "training_loss": 7.266404151916504
    },
    {
      "epoch": 0.38699186991869916,
      "step": 1785,
      "training_loss": 5.203005313873291
    },
    {
      "epoch": 0.38699186991869916,
      "step": 1785,
      "training_loss": 6.351623058319092
    },
    {
      "epoch": 0.38699186991869916,
      "step": 1785,
      "training_loss": 6.268675327301025
    },
    {
      "epoch": 0.3872086720867209,
      "step": 1786,
      "training_loss": 8.393134117126465
    },
    {
      "epoch": 0.3872086720867209,
      "step": 1786,
      "training_loss": 6.740728378295898
    },
    {
      "epoch": 0.3872086720867209,
      "step": 1786,
      "training_loss": 6.854189872741699
    },
    {
      "epoch": 0.3872086720867209,
      "step": 1786,
      "training_loss": 7.071226596832275
    },
    {
      "epoch": 0.38742547425474255,
      "step": 1787,
      "training_loss": 7.8813276290893555
    },
    {
      "epoch": 0.38742547425474255,
      "step": 1787,
      "training_loss": 8.3281888961792
    },
    {
      "epoch": 0.38742547425474255,
      "step": 1787,
      "training_loss": 6.0343756675720215
    },
    {
      "epoch": 0.38742547425474255,
      "step": 1787,
      "training_loss": 6.307174205780029
    },
    {
      "epoch": 0.3876422764227642,
      "grad_norm": 14.50039291381836,
      "learning_rate": 1e-05,
      "loss": 6.9634,
      "step": 1788
    },
    {
      "epoch": 0.3876422764227642,
      "step": 1788,
      "training_loss": 7.976319313049316
    },
    {
      "epoch": 0.3876422764227642,
      "step": 1788,
      "training_loss": 6.645056247711182
    },
    {
      "epoch": 0.3876422764227642,
      "step": 1788,
      "training_loss": 7.511394500732422
    },
    {
      "epoch": 0.3876422764227642,
      "step": 1788,
      "training_loss": 6.5417985916137695
    },
    {
      "epoch": 0.3878590785907859,
      "step": 1789,
      "training_loss": 7.215287685394287
    },
    {
      "epoch": 0.3878590785907859,
      "step": 1789,
      "training_loss": 5.7668232917785645
    },
    {
      "epoch": 0.3878590785907859,
      "step": 1789,
      "training_loss": 7.287937641143799
    },
    {
      "epoch": 0.3878590785907859,
      "step": 1789,
      "training_loss": 7.101224422454834
    },
    {
      "epoch": 0.3880758807588076,
      "step": 1790,
      "training_loss": 5.846227169036865
    },
    {
      "epoch": 0.3880758807588076,
      "step": 1790,
      "training_loss": 6.35775899887085
    },
    {
      "epoch": 0.3880758807588076,
      "step": 1790,
      "training_loss": 7.474123954772949
    },
    {
      "epoch": 0.3880758807588076,
      "step": 1790,
      "training_loss": 6.220396041870117
    },
    {
      "epoch": 0.3882926829268293,
      "step": 1791,
      "training_loss": 7.388857364654541
    },
    {
      "epoch": 0.3882926829268293,
      "step": 1791,
      "training_loss": 7.5723652839660645
    },
    {
      "epoch": 0.3882926829268293,
      "step": 1791,
      "training_loss": 6.1657209396362305
    },
    {
      "epoch": 0.3882926829268293,
      "step": 1791,
      "training_loss": 6.761062145233154
    },
    {
      "epoch": 0.38850948509485095,
      "grad_norm": 12.748668670654297,
      "learning_rate": 1e-05,
      "loss": 6.8645,
      "step": 1792
    },
    {
      "epoch": 0.38850948509485095,
      "step": 1792,
      "training_loss": 7.272033214569092
    },
    {
      "epoch": 0.38850948509485095,
      "step": 1792,
      "training_loss": 7.4339599609375
    },
    {
      "epoch": 0.38850948509485095,
      "step": 1792,
      "training_loss": 4.9245991706848145
    },
    {
      "epoch": 0.38850948509485095,
      "step": 1792,
      "training_loss": 7.352509021759033
    },
    {
      "epoch": 0.3887262872628726,
      "step": 1793,
      "training_loss": 6.316900730133057
    },
    {
      "epoch": 0.3887262872628726,
      "step": 1793,
      "training_loss": 5.453239917755127
    },
    {
      "epoch": 0.3887262872628726,
      "step": 1793,
      "training_loss": 7.300760269165039
    },
    {
      "epoch": 0.3887262872628726,
      "step": 1793,
      "training_loss": 5.927396774291992
    },
    {
      "epoch": 0.3889430894308943,
      "step": 1794,
      "training_loss": 6.758857250213623
    },
    {
      "epoch": 0.3889430894308943,
      "step": 1794,
      "training_loss": 7.318098545074463
    },
    {
      "epoch": 0.3889430894308943,
      "step": 1794,
      "training_loss": 6.375629425048828
    },
    {
      "epoch": 0.3889430894308943,
      "step": 1794,
      "training_loss": 6.910512447357178
    },
    {
      "epoch": 0.389159891598916,
      "step": 1795,
      "training_loss": 6.533486843109131
    },
    {
      "epoch": 0.389159891598916,
      "step": 1795,
      "training_loss": 6.960281848907471
    },
    {
      "epoch": 0.389159891598916,
      "step": 1795,
      "training_loss": 7.139842510223389
    },
    {
      "epoch": 0.389159891598916,
      "step": 1795,
      "training_loss": 7.464033126831055
    },
    {
      "epoch": 0.3893766937669377,
      "grad_norm": 13.828165054321289,
      "learning_rate": 1e-05,
      "loss": 6.7151,
      "step": 1796
    },
    {
      "epoch": 0.3893766937669377,
      "step": 1796,
      "training_loss": 6.684892177581787
    },
    {
      "epoch": 0.3893766937669377,
      "step": 1796,
      "training_loss": 5.739161491394043
    },
    {
      "epoch": 0.3893766937669377,
      "step": 1796,
      "training_loss": 7.0712056159973145
    },
    {
      "epoch": 0.3893766937669377,
      "step": 1796,
      "training_loss": 7.457199573516846
    },
    {
      "epoch": 0.38959349593495934,
      "step": 1797,
      "training_loss": 6.514532566070557
    },
    {
      "epoch": 0.38959349593495934,
      "step": 1797,
      "training_loss": 6.857333660125732
    },
    {
      "epoch": 0.38959349593495934,
      "step": 1797,
      "training_loss": 7.476129531860352
    },
    {
      "epoch": 0.38959349593495934,
      "step": 1797,
      "training_loss": 6.354600429534912
    },
    {
      "epoch": 0.389810298102981,
      "step": 1798,
      "training_loss": 6.936346530914307
    },
    {
      "epoch": 0.389810298102981,
      "step": 1798,
      "training_loss": 5.738579750061035
    },
    {
      "epoch": 0.389810298102981,
      "step": 1798,
      "training_loss": 6.630368232727051
    },
    {
      "epoch": 0.389810298102981,
      "step": 1798,
      "training_loss": 6.724833965301514
    },
    {
      "epoch": 0.39002710027100274,
      "step": 1799,
      "training_loss": 6.581324100494385
    },
    {
      "epoch": 0.39002710027100274,
      "step": 1799,
      "training_loss": 7.498419284820557
    },
    {
      "epoch": 0.39002710027100274,
      "step": 1799,
      "training_loss": 5.45848274230957
    },
    {
      "epoch": 0.39002710027100274,
      "step": 1799,
      "training_loss": 7.225796222686768
    },
    {
      "epoch": 0.3902439024390244,
      "grad_norm": 12.12498950958252,
      "learning_rate": 1e-05,
      "loss": 6.6843,
      "step": 1800
    },
    {
      "epoch": 0.3902439024390244,
      "step": 1800,
      "training_loss": 7.729139804840088
    },
    {
      "epoch": 0.3902439024390244,
      "step": 1800,
      "training_loss": 5.80200719833374
    },
    {
      "epoch": 0.3902439024390244,
      "step": 1800,
      "training_loss": 7.160938739776611
    },
    {
      "epoch": 0.3902439024390244,
      "step": 1800,
      "training_loss": 7.987941741943359
    },
    {
      "epoch": 0.39046070460704607,
      "step": 1801,
      "training_loss": 7.82224178314209
    },
    {
      "epoch": 0.39046070460704607,
      "step": 1801,
      "training_loss": 6.845699787139893
    },
    {
      "epoch": 0.39046070460704607,
      "step": 1801,
      "training_loss": 5.907830715179443
    },
    {
      "epoch": 0.39046070460704607,
      "step": 1801,
      "training_loss": 6.2790398597717285
    },
    {
      "epoch": 0.39067750677506774,
      "step": 1802,
      "training_loss": 7.749516487121582
    },
    {
      "epoch": 0.39067750677506774,
      "step": 1802,
      "training_loss": 5.8204779624938965
    },
    {
      "epoch": 0.39067750677506774,
      "step": 1802,
      "training_loss": 6.808022499084473
    },
    {
      "epoch": 0.39067750677506774,
      "step": 1802,
      "training_loss": 7.222045421600342
    },
    {
      "epoch": 0.3908943089430894,
      "step": 1803,
      "training_loss": 5.502439975738525
    },
    {
      "epoch": 0.3908943089430894,
      "step": 1803,
      "training_loss": 4.578292369842529
    },
    {
      "epoch": 0.3908943089430894,
      "step": 1803,
      "training_loss": 6.527950286865234
    },
    {
      "epoch": 0.3908943089430894,
      "step": 1803,
      "training_loss": 6.489729404449463
    },
    {
      "epoch": 0.39111111111111113,
      "grad_norm": 11.94982624053955,
      "learning_rate": 1e-05,
      "loss": 6.6396,
      "step": 1804
    },
    {
      "epoch": 0.39111111111111113,
      "step": 1804,
      "training_loss": 6.790987968444824
    },
    {
      "epoch": 0.39111111111111113,
      "step": 1804,
      "training_loss": 7.022675037384033
    },
    {
      "epoch": 0.39111111111111113,
      "step": 1804,
      "training_loss": 6.575833797454834
    },
    {
      "epoch": 0.39111111111111113,
      "step": 1804,
      "training_loss": 6.598699569702148
    },
    {
      "epoch": 0.3913279132791328,
      "step": 1805,
      "training_loss": 4.583224296569824
    },
    {
      "epoch": 0.3913279132791328,
      "step": 1805,
      "training_loss": 7.92532205581665
    },
    {
      "epoch": 0.3913279132791328,
      "step": 1805,
      "training_loss": 6.0115814208984375
    },
    {
      "epoch": 0.3913279132791328,
      "step": 1805,
      "training_loss": 8.834277153015137
    },
    {
      "epoch": 0.39154471544715447,
      "step": 1806,
      "training_loss": 6.460102558135986
    },
    {
      "epoch": 0.39154471544715447,
      "step": 1806,
      "training_loss": 5.4692583084106445
    },
    {
      "epoch": 0.39154471544715447,
      "step": 1806,
      "training_loss": 6.232648849487305
    },
    {
      "epoch": 0.39154471544715447,
      "step": 1806,
      "training_loss": 5.362189769744873
    },
    {
      "epoch": 0.39176151761517614,
      "step": 1807,
      "training_loss": 6.48718786239624
    },
    {
      "epoch": 0.39176151761517614,
      "step": 1807,
      "training_loss": 8.168778419494629
    },
    {
      "epoch": 0.39176151761517614,
      "step": 1807,
      "training_loss": 7.380178928375244
    },
    {
      "epoch": 0.39176151761517614,
      "step": 1807,
      "training_loss": 8.485404968261719
    },
    {
      "epoch": 0.39197831978319786,
      "grad_norm": 11.802701950073242,
      "learning_rate": 1e-05,
      "loss": 6.7743,
      "step": 1808
    },
    {
      "epoch": 0.39197831978319786,
      "step": 1808,
      "training_loss": 6.934202194213867
    },
    {
      "epoch": 0.39197831978319786,
      "step": 1808,
      "training_loss": 6.858664035797119
    },
    {
      "epoch": 0.39197831978319786,
      "step": 1808,
      "training_loss": 8.014054298400879
    },
    {
      "epoch": 0.39197831978319786,
      "step": 1808,
      "training_loss": 5.790808200836182
    },
    {
      "epoch": 0.3921951219512195,
      "step": 1809,
      "training_loss": 7.110710144042969
    },
    {
      "epoch": 0.3921951219512195,
      "step": 1809,
      "training_loss": 7.479976177215576
    },
    {
      "epoch": 0.3921951219512195,
      "step": 1809,
      "training_loss": 5.903993129730225
    },
    {
      "epoch": 0.3921951219512195,
      "step": 1809,
      "training_loss": 7.71138858795166
    },
    {
      "epoch": 0.3924119241192412,
      "step": 1810,
      "training_loss": 7.130838394165039
    },
    {
      "epoch": 0.3924119241192412,
      "step": 1810,
      "training_loss": 5.652756690979004
    },
    {
      "epoch": 0.3924119241192412,
      "step": 1810,
      "training_loss": 6.402455806732178
    },
    {
      "epoch": 0.3924119241192412,
      "step": 1810,
      "training_loss": 7.345299243927002
    },
    {
      "epoch": 0.39262872628726286,
      "step": 1811,
      "training_loss": 5.420858383178711
    },
    {
      "epoch": 0.39262872628726286,
      "step": 1811,
      "training_loss": 8.52608585357666
    },
    {
      "epoch": 0.39262872628726286,
      "step": 1811,
      "training_loss": 7.941612720489502
    },
    {
      "epoch": 0.39262872628726286,
      "step": 1811,
      "training_loss": 6.912479400634766
    },
    {
      "epoch": 0.39284552845528453,
      "grad_norm": 13.284111976623535,
      "learning_rate": 1e-05,
      "loss": 6.946,
      "step": 1812
    },
    {
      "epoch": 0.39284552845528453,
      "step": 1812,
      "training_loss": 7.185894966125488
    },
    {
      "epoch": 0.39284552845528453,
      "step": 1812,
      "training_loss": 7.12203311920166
    },
    {
      "epoch": 0.39284552845528453,
      "step": 1812,
      "training_loss": 5.610137939453125
    },
    {
      "epoch": 0.39284552845528453,
      "step": 1812,
      "training_loss": 6.8565673828125
    },
    {
      "epoch": 0.39306233062330626,
      "step": 1813,
      "training_loss": 8.63602066040039
    },
    {
      "epoch": 0.39306233062330626,
      "step": 1813,
      "training_loss": 6.764125347137451
    },
    {
      "epoch": 0.39306233062330626,
      "step": 1813,
      "training_loss": 5.794399261474609
    },
    {
      "epoch": 0.39306233062330626,
      "step": 1813,
      "training_loss": 7.864560127258301
    },
    {
      "epoch": 0.3932791327913279,
      "step": 1814,
      "training_loss": 8.38438606262207
    },
    {
      "epoch": 0.3932791327913279,
      "step": 1814,
      "training_loss": 5.550642490386963
    },
    {
      "epoch": 0.3932791327913279,
      "step": 1814,
      "training_loss": 6.8981099128723145
    },
    {
      "epoch": 0.3932791327913279,
      "step": 1814,
      "training_loss": 6.4067277908325195
    },
    {
      "epoch": 0.3934959349593496,
      "step": 1815,
      "training_loss": 5.053300857543945
    },
    {
      "epoch": 0.3934959349593496,
      "step": 1815,
      "training_loss": 5.773242950439453
    },
    {
      "epoch": 0.3934959349593496,
      "step": 1815,
      "training_loss": 6.958186626434326
    },
    {
      "epoch": 0.3934959349593496,
      "step": 1815,
      "training_loss": 7.147613525390625
    },
    {
      "epoch": 0.39371273712737126,
      "grad_norm": 12.64732551574707,
      "learning_rate": 1e-05,
      "loss": 6.7504,
      "step": 1816
    },
    {
      "epoch": 0.39371273712737126,
      "step": 1816,
      "training_loss": 5.4965009689331055
    },
    {
      "epoch": 0.39371273712737126,
      "step": 1816,
      "training_loss": 5.825068473815918
    },
    {
      "epoch": 0.39371273712737126,
      "step": 1816,
      "training_loss": 6.1767754554748535
    },
    {
      "epoch": 0.39371273712737126,
      "step": 1816,
      "training_loss": 5.576098918914795
    },
    {
      "epoch": 0.3939295392953929,
      "step": 1817,
      "training_loss": 7.222804546356201
    },
    {
      "epoch": 0.3939295392953929,
      "step": 1817,
      "training_loss": 7.080185890197754
    },
    {
      "epoch": 0.3939295392953929,
      "step": 1817,
      "training_loss": 8.037071228027344
    },
    {
      "epoch": 0.3939295392953929,
      "step": 1817,
      "training_loss": 7.127996921539307
    },
    {
      "epoch": 0.39414634146341465,
      "step": 1818,
      "training_loss": 5.19455099105835
    },
    {
      "epoch": 0.39414634146341465,
      "step": 1818,
      "training_loss": 6.606066703796387
    },
    {
      "epoch": 0.39414634146341465,
      "step": 1818,
      "training_loss": 7.438028335571289
    },
    {
      "epoch": 0.39414634146341465,
      "step": 1818,
      "training_loss": 6.336576461791992
    },
    {
      "epoch": 0.3943631436314363,
      "step": 1819,
      "training_loss": 6.924776554107666
    },
    {
      "epoch": 0.3943631436314363,
      "step": 1819,
      "training_loss": 6.158502101898193
    },
    {
      "epoch": 0.3943631436314363,
      "step": 1819,
      "training_loss": 6.826911449432373
    },
    {
      "epoch": 0.3943631436314363,
      "step": 1819,
      "training_loss": 4.567472457885742
    },
    {
      "epoch": 0.394579945799458,
      "grad_norm": 11.792019844055176,
      "learning_rate": 1e-05,
      "loss": 6.4122,
      "step": 1820
    },
    {
      "epoch": 0.394579945799458,
      "step": 1820,
      "training_loss": 7.956368446350098
    },
    {
      "epoch": 0.394579945799458,
      "step": 1820,
      "training_loss": 8.184361457824707
    },
    {
      "epoch": 0.394579945799458,
      "step": 1820,
      "training_loss": 7.5145416259765625
    },
    {
      "epoch": 0.394579945799458,
      "step": 1820,
      "training_loss": 6.627597808837891
    },
    {
      "epoch": 0.39479674796747966,
      "step": 1821,
      "training_loss": 6.757842063903809
    },
    {
      "epoch": 0.39479674796747966,
      "step": 1821,
      "training_loss": 6.67906379699707
    },
    {
      "epoch": 0.39479674796747966,
      "step": 1821,
      "training_loss": 7.164046287536621
    },
    {
      "epoch": 0.39479674796747966,
      "step": 1821,
      "training_loss": 4.533711910247803
    },
    {
      "epoch": 0.3950135501355014,
      "step": 1822,
      "training_loss": 6.661974906921387
    },
    {
      "epoch": 0.3950135501355014,
      "step": 1822,
      "training_loss": 7.044585704803467
    },
    {
      "epoch": 0.3950135501355014,
      "step": 1822,
      "training_loss": 7.552823543548584
    },
    {
      "epoch": 0.3950135501355014,
      "step": 1822,
      "training_loss": 7.3324079513549805
    },
    {
      "epoch": 0.39523035230352305,
      "step": 1823,
      "training_loss": 8.255638122558594
    },
    {
      "epoch": 0.39523035230352305,
      "step": 1823,
      "training_loss": 7.3350043296813965
    },
    {
      "epoch": 0.39523035230352305,
      "step": 1823,
      "training_loss": 6.9027276039123535
    },
    {
      "epoch": 0.39523035230352305,
      "step": 1823,
      "training_loss": 6.271425724029541
    },
    {
      "epoch": 0.3954471544715447,
      "grad_norm": 10.723921775817871,
      "learning_rate": 1e-05,
      "loss": 7.0484,
      "step": 1824
    },
    {
      "epoch": 0.3954471544715447,
      "step": 1824,
      "training_loss": 4.590123176574707
    },
    {
      "epoch": 0.3954471544715447,
      "step": 1824,
      "training_loss": 7.510931491851807
    },
    {
      "epoch": 0.3954471544715447,
      "step": 1824,
      "training_loss": 7.372846603393555
    },
    {
      "epoch": 0.3954471544715447,
      "step": 1824,
      "training_loss": 7.702863693237305
    },
    {
      "epoch": 0.3956639566395664,
      "step": 1825,
      "training_loss": 7.108877182006836
    },
    {
      "epoch": 0.3956639566395664,
      "step": 1825,
      "training_loss": 8.013314247131348
    },
    {
      "epoch": 0.3956639566395664,
      "step": 1825,
      "training_loss": 7.461329936981201
    },
    {
      "epoch": 0.3956639566395664,
      "step": 1825,
      "training_loss": 6.355632781982422
    },
    {
      "epoch": 0.39588075880758805,
      "step": 1826,
      "training_loss": 5.421542167663574
    },
    {
      "epoch": 0.39588075880758805,
      "step": 1826,
      "training_loss": 7.7091827392578125
    },
    {
      "epoch": 0.39588075880758805,
      "step": 1826,
      "training_loss": 6.456087112426758
    },
    {
      "epoch": 0.39588075880758805,
      "step": 1826,
      "training_loss": 7.254866600036621
    },
    {
      "epoch": 0.3960975609756098,
      "step": 1827,
      "training_loss": 6.996821880340576
    },
    {
      "epoch": 0.3960975609756098,
      "step": 1827,
      "training_loss": 7.180839538574219
    },
    {
      "epoch": 0.3960975609756098,
      "step": 1827,
      "training_loss": 6.939302444458008
    },
    {
      "epoch": 0.3960975609756098,
      "step": 1827,
      "training_loss": 9.969782829284668
    },
    {
      "epoch": 0.39631436314363144,
      "grad_norm": 15.115302085876465,
      "learning_rate": 1e-05,
      "loss": 7.1278,
      "step": 1828
    },
    {
      "epoch": 0.39631436314363144,
      "step": 1828,
      "training_loss": 7.237140655517578
    },
    {
      "epoch": 0.39631436314363144,
      "step": 1828,
      "training_loss": 6.605502128601074
    },
    {
      "epoch": 0.39631436314363144,
      "step": 1828,
      "training_loss": 5.959279537200928
    },
    {
      "epoch": 0.39631436314363144,
      "step": 1828,
      "training_loss": 7.661234378814697
    },
    {
      "epoch": 0.3965311653116531,
      "step": 1829,
      "training_loss": 6.672307968139648
    },
    {
      "epoch": 0.3965311653116531,
      "step": 1829,
      "training_loss": 6.581669330596924
    },
    {
      "epoch": 0.3965311653116531,
      "step": 1829,
      "training_loss": 6.970946788787842
    },
    {
      "epoch": 0.3965311653116531,
      "step": 1829,
      "training_loss": 8.141255378723145
    },
    {
      "epoch": 0.3967479674796748,
      "step": 1830,
      "training_loss": 6.3864617347717285
    },
    {
      "epoch": 0.3967479674796748,
      "step": 1830,
      "training_loss": 7.101136684417725
    },
    {
      "epoch": 0.3967479674796748,
      "step": 1830,
      "training_loss": 6.99885892868042
    },
    {
      "epoch": 0.3967479674796748,
      "step": 1830,
      "training_loss": 7.488223075866699
    },
    {
      "epoch": 0.3969647696476965,
      "step": 1831,
      "training_loss": 10.41711139678955
    },
    {
      "epoch": 0.3969647696476965,
      "step": 1831,
      "training_loss": 6.75139045715332
    },
    {
      "epoch": 0.3969647696476965,
      "step": 1831,
      "training_loss": 7.723464012145996
    },
    {
      "epoch": 0.3969647696476965,
      "step": 1831,
      "training_loss": 7.378602504730225
    },
    {
      "epoch": 0.39718157181571817,
      "grad_norm": 18.38558578491211,
      "learning_rate": 1e-05,
      "loss": 7.2547,
      "step": 1832
    },
    {
      "epoch": 0.39718157181571817,
      "step": 1832,
      "training_loss": 7.640625476837158
    },
    {
      "epoch": 0.39718157181571817,
      "step": 1832,
      "training_loss": 7.065558910369873
    },
    {
      "epoch": 0.39718157181571817,
      "step": 1832,
      "training_loss": 7.850683689117432
    },
    {
      "epoch": 0.39718157181571817,
      "step": 1832,
      "training_loss": 6.737874984741211
    },
    {
      "epoch": 0.39739837398373984,
      "step": 1833,
      "training_loss": 6.505273342132568
    },
    {
      "epoch": 0.39739837398373984,
      "step": 1833,
      "training_loss": 4.914711952209473
    },
    {
      "epoch": 0.39739837398373984,
      "step": 1833,
      "training_loss": 6.023563385009766
    },
    {
      "epoch": 0.39739837398373984,
      "step": 1833,
      "training_loss": 7.170594692230225
    },
    {
      "epoch": 0.3976151761517615,
      "step": 1834,
      "training_loss": 5.847465515136719
    },
    {
      "epoch": 0.3976151761517615,
      "step": 1834,
      "training_loss": 6.268424034118652
    },
    {
      "epoch": 0.3976151761517615,
      "step": 1834,
      "training_loss": 6.996109962463379
    },
    {
      "epoch": 0.3976151761517615,
      "step": 1834,
      "training_loss": 6.152270317077637
    },
    {
      "epoch": 0.3978319783197832,
      "step": 1835,
      "training_loss": 5.5727972984313965
    },
    {
      "epoch": 0.3978319783197832,
      "step": 1835,
      "training_loss": 7.93475341796875
    },
    {
      "epoch": 0.3978319783197832,
      "step": 1835,
      "training_loss": 6.5782976150512695
    },
    {
      "epoch": 0.3978319783197832,
      "step": 1835,
      "training_loss": 6.864645957946777
    },
    {
      "epoch": 0.3980487804878049,
      "grad_norm": 18.051366806030273,
      "learning_rate": 1e-05,
      "loss": 6.6327,
      "step": 1836
    },
    {
      "epoch": 0.3980487804878049,
      "step": 1836,
      "training_loss": 8.365279197692871
    },
    {
      "epoch": 0.3980487804878049,
      "step": 1836,
      "training_loss": 6.800463676452637
    },
    {
      "epoch": 0.3980487804878049,
      "step": 1836,
      "training_loss": 6.46727991104126
    },
    {
      "epoch": 0.3980487804878049,
      "step": 1836,
      "training_loss": 6.490615367889404
    },
    {
      "epoch": 0.39826558265582657,
      "step": 1837,
      "training_loss": 6.1424336433410645
    },
    {
      "epoch": 0.39826558265582657,
      "step": 1837,
      "training_loss": 7.517714500427246
    },
    {
      "epoch": 0.39826558265582657,
      "step": 1837,
      "training_loss": 4.674238204956055
    },
    {
      "epoch": 0.39826558265582657,
      "step": 1837,
      "training_loss": 7.077371597290039
    },
    {
      "epoch": 0.39848238482384823,
      "step": 1838,
      "training_loss": 6.610633850097656
    },
    {
      "epoch": 0.39848238482384823,
      "step": 1838,
      "training_loss": 5.685612678527832
    },
    {
      "epoch": 0.39848238482384823,
      "step": 1838,
      "training_loss": 6.372512340545654
    },
    {
      "epoch": 0.39848238482384823,
      "step": 1838,
      "training_loss": 7.052853107452393
    },
    {
      "epoch": 0.3986991869918699,
      "step": 1839,
      "training_loss": 7.129977703094482
    },
    {
      "epoch": 0.3986991869918699,
      "step": 1839,
      "training_loss": 6.839815616607666
    },
    {
      "epoch": 0.3986991869918699,
      "step": 1839,
      "training_loss": 7.09061336517334
    },
    {
      "epoch": 0.3986991869918699,
      "step": 1839,
      "training_loss": 7.087224006652832
    },
    {
      "epoch": 0.3989159891598916,
      "grad_norm": 11.506441116333008,
      "learning_rate": 1e-05,
      "loss": 6.7128,
      "step": 1840
    },
    {
      "epoch": 0.3989159891598916,
      "step": 1840,
      "training_loss": 7.550500392913818
    },
    {
      "epoch": 0.3989159891598916,
      "step": 1840,
      "training_loss": 7.226983547210693
    },
    {
      "epoch": 0.3989159891598916,
      "step": 1840,
      "training_loss": 6.876450538635254
    },
    {
      "epoch": 0.3989159891598916,
      "step": 1840,
      "training_loss": 7.145572185516357
    },
    {
      "epoch": 0.3991327913279133,
      "step": 1841,
      "training_loss": 5.253406047821045
    },
    {
      "epoch": 0.3991327913279133,
      "step": 1841,
      "training_loss": 5.942151069641113
    },
    {
      "epoch": 0.3991327913279133,
      "step": 1841,
      "training_loss": 6.523910999298096
    },
    {
      "epoch": 0.3991327913279133,
      "step": 1841,
      "training_loss": 7.143002986907959
    },
    {
      "epoch": 0.39934959349593496,
      "step": 1842,
      "training_loss": 7.204346179962158
    },
    {
      "epoch": 0.39934959349593496,
      "step": 1842,
      "training_loss": 4.099836349487305
    },
    {
      "epoch": 0.39934959349593496,
      "step": 1842,
      "training_loss": 6.578270435333252
    },
    {
      "epoch": 0.39934959349593496,
      "step": 1842,
      "training_loss": 7.070333480834961
    },
    {
      "epoch": 0.39956639566395663,
      "step": 1843,
      "training_loss": 7.033196926116943
    },
    {
      "epoch": 0.39956639566395663,
      "step": 1843,
      "training_loss": 4.5437164306640625
    },
    {
      "epoch": 0.39956639566395663,
      "step": 1843,
      "training_loss": 6.935102939605713
    },
    {
      "epoch": 0.39956639566395663,
      "step": 1843,
      "training_loss": 6.723697185516357
    },
    {
      "epoch": 0.3997831978319783,
      "grad_norm": 11.042301177978516,
      "learning_rate": 1e-05,
      "loss": 6.4907,
      "step": 1844
    },
    {
      "epoch": 0.3997831978319783,
      "step": 1844,
      "training_loss": 7.165626049041748
    },
    {
      "epoch": 0.3997831978319783,
      "step": 1844,
      "training_loss": 6.035970211029053
    },
    {
      "epoch": 0.3997831978319783,
      "step": 1844,
      "training_loss": 6.42431640625
    },
    {
      "epoch": 0.3997831978319783,
      "step": 1844,
      "training_loss": 8.1741943359375
    },
    {
      "epoch": 0.4,
      "step": 1845,
      "training_loss": 6.911184310913086
    },
    {
      "epoch": 0.4,
      "step": 1845,
      "training_loss": 6.595042705535889
    },
    {
      "epoch": 0.4,
      "step": 1845,
      "training_loss": 7.025390148162842
    },
    {
      "epoch": 0.4,
      "step": 1845,
      "training_loss": 7.393616199493408
    },
    {
      "epoch": 0.4002168021680217,
      "step": 1846,
      "training_loss": 6.1543450355529785
    },
    {
      "epoch": 0.4002168021680217,
      "step": 1846,
      "training_loss": 6.194869041442871
    },
    {
      "epoch": 0.4002168021680217,
      "step": 1846,
      "training_loss": 8.316924095153809
    },
    {
      "epoch": 0.4002168021680217,
      "step": 1846,
      "training_loss": 7.058084964752197
    },
    {
      "epoch": 0.40043360433604336,
      "step": 1847,
      "training_loss": 8.285669326782227
    },
    {
      "epoch": 0.40043360433604336,
      "step": 1847,
      "training_loss": 6.717796802520752
    },
    {
      "epoch": 0.40043360433604336,
      "step": 1847,
      "training_loss": 7.5489912033081055
    },
    {
      "epoch": 0.40043360433604336,
      "step": 1847,
      "training_loss": 7.303476333618164
    },
    {
      "epoch": 0.400650406504065,
      "grad_norm": 11.109623908996582,
      "learning_rate": 1e-05,
      "loss": 7.0816,
      "step": 1848
    },
    {
      "epoch": 0.400650406504065,
      "step": 1848,
      "training_loss": 6.893130779266357
    },
    {
      "epoch": 0.400650406504065,
      "step": 1848,
      "training_loss": 6.010936260223389
    },
    {
      "epoch": 0.400650406504065,
      "step": 1848,
      "training_loss": 6.923093795776367
    },
    {
      "epoch": 0.400650406504065,
      "step": 1848,
      "training_loss": 7.647600173950195
    },
    {
      "epoch": 0.4008672086720867,
      "step": 1849,
      "training_loss": 7.308612823486328
    },
    {
      "epoch": 0.4008672086720867,
      "step": 1849,
      "training_loss": 6.429826259613037
    },
    {
      "epoch": 0.4008672086720867,
      "step": 1849,
      "training_loss": 6.371376037597656
    },
    {
      "epoch": 0.4008672086720867,
      "step": 1849,
      "training_loss": 7.301163673400879
    },
    {
      "epoch": 0.4010840108401084,
      "step": 1850,
      "training_loss": 6.977085590362549
    },
    {
      "epoch": 0.4010840108401084,
      "step": 1850,
      "training_loss": 8.22973346710205
    },
    {
      "epoch": 0.4010840108401084,
      "step": 1850,
      "training_loss": 6.007702827453613
    },
    {
      "epoch": 0.4010840108401084,
      "step": 1850,
      "training_loss": 6.800291538238525
    },
    {
      "epoch": 0.4013008130081301,
      "step": 1851,
      "training_loss": 5.537547588348389
    },
    {
      "epoch": 0.4013008130081301,
      "step": 1851,
      "training_loss": 8.26563549041748
    },
    {
      "epoch": 0.4013008130081301,
      "step": 1851,
      "training_loss": 7.36803674697876
    },
    {
      "epoch": 0.4013008130081301,
      "step": 1851,
      "training_loss": 8.525911331176758
    },
    {
      "epoch": 0.40151761517615175,
      "grad_norm": 16.693214416503906,
      "learning_rate": 1e-05,
      "loss": 7.0374,
      "step": 1852
    },
    {
      "epoch": 0.40151761517615175,
      "step": 1852,
      "training_loss": 5.719332695007324
    },
    {
      "epoch": 0.40151761517615175,
      "step": 1852,
      "training_loss": 6.488830089569092
    },
    {
      "epoch": 0.40151761517615175,
      "step": 1852,
      "training_loss": 6.7235426902771
    },
    {
      "epoch": 0.40151761517615175,
      "step": 1852,
      "training_loss": 7.136974811553955
    },
    {
      "epoch": 0.4017344173441734,
      "step": 1853,
      "training_loss": 6.248893737792969
    },
    {
      "epoch": 0.4017344173441734,
      "step": 1853,
      "training_loss": 6.060917854309082
    },
    {
      "epoch": 0.4017344173441734,
      "step": 1853,
      "training_loss": 6.51327657699585
    },
    {
      "epoch": 0.4017344173441734,
      "step": 1853,
      "training_loss": 7.96453332901001
    },
    {
      "epoch": 0.40195121951219515,
      "step": 1854,
      "training_loss": 7.333781719207764
    },
    {
      "epoch": 0.40195121951219515,
      "step": 1854,
      "training_loss": 5.533329963684082
    },
    {
      "epoch": 0.40195121951219515,
      "step": 1854,
      "training_loss": 6.328481197357178
    },
    {
      "epoch": 0.40195121951219515,
      "step": 1854,
      "training_loss": 5.761972427368164
    },
    {
      "epoch": 0.4021680216802168,
      "step": 1855,
      "training_loss": 6.813888072967529
    },
    {
      "epoch": 0.4021680216802168,
      "step": 1855,
      "training_loss": 8.245430946350098
    },
    {
      "epoch": 0.4021680216802168,
      "step": 1855,
      "training_loss": 7.029170036315918
    },
    {
      "epoch": 0.4021680216802168,
      "step": 1855,
      "training_loss": 6.464137554168701
    },
    {
      "epoch": 0.4023848238482385,
      "grad_norm": 11.961882591247559,
      "learning_rate": 1e-05,
      "loss": 6.6479,
      "step": 1856
    },
    {
      "epoch": 0.4023848238482385,
      "step": 1856,
      "training_loss": 6.189012050628662
    },
    {
      "epoch": 0.4023848238482385,
      "step": 1856,
      "training_loss": 5.820988655090332
    },
    {
      "epoch": 0.4023848238482385,
      "step": 1856,
      "training_loss": 6.997926235198975
    },
    {
      "epoch": 0.4023848238482385,
      "step": 1856,
      "training_loss": 6.629144668579102
    },
    {
      "epoch": 0.40260162601626015,
      "step": 1857,
      "training_loss": 5.956190586090088
    },
    {
      "epoch": 0.40260162601626015,
      "step": 1857,
      "training_loss": 6.071488380432129
    },
    {
      "epoch": 0.40260162601626015,
      "step": 1857,
      "training_loss": 6.158321857452393
    },
    {
      "epoch": 0.40260162601626015,
      "step": 1857,
      "training_loss": 7.008922576904297
    },
    {
      "epoch": 0.4028184281842818,
      "step": 1858,
      "training_loss": 7.972340106964111
    },
    {
      "epoch": 0.4028184281842818,
      "step": 1858,
      "training_loss": 5.066133975982666
    },
    {
      "epoch": 0.4028184281842818,
      "step": 1858,
      "training_loss": 5.830930709838867
    },
    {
      "epoch": 0.4028184281842818,
      "step": 1858,
      "training_loss": 7.319993019104004
    },
    {
      "epoch": 0.40303523035230354,
      "step": 1859,
      "training_loss": 5.784542560577393
    },
    {
      "epoch": 0.40303523035230354,
      "step": 1859,
      "training_loss": 6.3943963050842285
    },
    {
      "epoch": 0.40303523035230354,
      "step": 1859,
      "training_loss": 6.41094970703125
    },
    {
      "epoch": 0.40303523035230354,
      "step": 1859,
      "training_loss": 8.207623481750488
    },
    {
      "epoch": 0.4032520325203252,
      "grad_norm": 12.640978813171387,
      "learning_rate": 1e-05,
      "loss": 6.4887,
      "step": 1860
    },
    {
      "epoch": 0.4032520325203252,
      "step": 1860,
      "training_loss": 6.982181549072266
    },
    {
      "epoch": 0.4032520325203252,
      "step": 1860,
      "training_loss": 8.009668350219727
    },
    {
      "epoch": 0.4032520325203252,
      "step": 1860,
      "training_loss": 5.8114447593688965
    },
    {
      "epoch": 0.4032520325203252,
      "step": 1860,
      "training_loss": 5.490384578704834
    },
    {
      "epoch": 0.4034688346883469,
      "step": 1861,
      "training_loss": 6.747119426727295
    },
    {
      "epoch": 0.4034688346883469,
      "step": 1861,
      "training_loss": 6.426513195037842
    },
    {
      "epoch": 0.4034688346883469,
      "step": 1861,
      "training_loss": 6.782283306121826
    },
    {
      "epoch": 0.4034688346883469,
      "step": 1861,
      "training_loss": 6.98311710357666
    },
    {
      "epoch": 0.40368563685636855,
      "step": 1862,
      "training_loss": 7.813360691070557
    },
    {
      "epoch": 0.40368563685636855,
      "step": 1862,
      "training_loss": 7.366974830627441
    },
    {
      "epoch": 0.40368563685636855,
      "step": 1862,
      "training_loss": 6.2428131103515625
    },
    {
      "epoch": 0.40368563685636855,
      "step": 1862,
      "training_loss": 6.895580291748047
    },
    {
      "epoch": 0.40390243902439027,
      "step": 1863,
      "training_loss": 6.624913692474365
    },
    {
      "epoch": 0.40390243902439027,
      "step": 1863,
      "training_loss": 7.30287504196167
    },
    {
      "epoch": 0.40390243902439027,
      "step": 1863,
      "training_loss": 6.859025001525879
    },
    {
      "epoch": 0.40390243902439027,
      "step": 1863,
      "training_loss": 6.830589294433594
    },
    {
      "epoch": 0.40411924119241194,
      "grad_norm": 12.75561237335205,
      "learning_rate": 1e-05,
      "loss": 6.8231,
      "step": 1864
    },
    {
      "epoch": 0.40411924119241194,
      "step": 1864,
      "training_loss": 6.19089412689209
    },
    {
      "epoch": 0.40411924119241194,
      "step": 1864,
      "training_loss": 7.30990743637085
    },
    {
      "epoch": 0.40411924119241194,
      "step": 1864,
      "training_loss": 7.393222332000732
    },
    {
      "epoch": 0.40411924119241194,
      "step": 1864,
      "training_loss": 7.623100757598877
    },
    {
      "epoch": 0.4043360433604336,
      "step": 1865,
      "training_loss": 6.828075885772705
    },
    {
      "epoch": 0.4043360433604336,
      "step": 1865,
      "training_loss": 7.946479320526123
    },
    {
      "epoch": 0.4043360433604336,
      "step": 1865,
      "training_loss": 4.728020668029785
    },
    {
      "epoch": 0.4043360433604336,
      "step": 1865,
      "training_loss": 7.688373565673828
    },
    {
      "epoch": 0.4045528455284553,
      "step": 1866,
      "training_loss": 5.266350269317627
    },
    {
      "epoch": 0.4045528455284553,
      "step": 1866,
      "training_loss": 7.405632495880127
    },
    {
      "epoch": 0.4045528455284553,
      "step": 1866,
      "training_loss": 6.200445652008057
    },
    {
      "epoch": 0.4045528455284553,
      "step": 1866,
      "training_loss": 7.005291938781738
    },
    {
      "epoch": 0.40476964769647694,
      "step": 1867,
      "training_loss": 6.319213390350342
    },
    {
      "epoch": 0.40476964769647694,
      "step": 1867,
      "training_loss": 6.807861328125
    },
    {
      "epoch": 0.40476964769647694,
      "step": 1867,
      "training_loss": 7.005979061126709
    },
    {
      "epoch": 0.40476964769647694,
      "step": 1867,
      "training_loss": 7.42253303527832
    },
    {
      "epoch": 0.40498644986449867,
      "grad_norm": 10.527029991149902,
      "learning_rate": 1e-05,
      "loss": 6.8213,
      "step": 1868
    },
    {
      "epoch": 0.40498644986449867,
      "step": 1868,
      "training_loss": 6.297486782073975
    },
    {
      "epoch": 0.40498644986449867,
      "step": 1868,
      "training_loss": 6.818125247955322
    },
    {
      "epoch": 0.40498644986449867,
      "step": 1868,
      "training_loss": 6.02869176864624
    },
    {
      "epoch": 0.40498644986449867,
      "step": 1868,
      "training_loss": 6.753157138824463
    },
    {
      "epoch": 0.40520325203252033,
      "step": 1869,
      "training_loss": 7.932427883148193
    },
    {
      "epoch": 0.40520325203252033,
      "step": 1869,
      "training_loss": 4.807745933532715
    },
    {
      "epoch": 0.40520325203252033,
      "step": 1869,
      "training_loss": 6.337674140930176
    },
    {
      "epoch": 0.40520325203252033,
      "step": 1869,
      "training_loss": 6.303576469421387
    },
    {
      "epoch": 0.405420054200542,
      "step": 1870,
      "training_loss": 7.1000447273254395
    },
    {
      "epoch": 0.405420054200542,
      "step": 1870,
      "training_loss": 7.04230260848999
    },
    {
      "epoch": 0.405420054200542,
      "step": 1870,
      "training_loss": 7.383580684661865
    },
    {
      "epoch": 0.405420054200542,
      "step": 1870,
      "training_loss": 8.577484130859375
    },
    {
      "epoch": 0.40563685636856367,
      "step": 1871,
      "training_loss": 6.535900592803955
    },
    {
      "epoch": 0.40563685636856367,
      "step": 1871,
      "training_loss": 6.714801788330078
    },
    {
      "epoch": 0.40563685636856367,
      "step": 1871,
      "training_loss": 6.746295928955078
    },
    {
      "epoch": 0.40563685636856367,
      "step": 1871,
      "training_loss": 7.28424072265625
    },
    {
      "epoch": 0.4058536585365854,
      "grad_norm": 10.588761329650879,
      "learning_rate": 1e-05,
      "loss": 6.7915,
      "step": 1872
    },
    {
      "epoch": 0.4058536585365854,
      "step": 1872,
      "training_loss": 5.052041053771973
    },
    {
      "epoch": 0.4058536585365854,
      "step": 1872,
      "training_loss": 6.7975053787231445
    },
    {
      "epoch": 0.4058536585365854,
      "step": 1872,
      "training_loss": 8.095544815063477
    },
    {
      "epoch": 0.4058536585365854,
      "step": 1872,
      "training_loss": 7.827065467834473
    },
    {
      "epoch": 0.40607046070460706,
      "step": 1873,
      "training_loss": 7.503422260284424
    },
    {
      "epoch": 0.40607046070460706,
      "step": 1873,
      "training_loss": 7.768783092498779
    },
    {
      "epoch": 0.40607046070460706,
      "step": 1873,
      "training_loss": 4.6531291007995605
    },
    {
      "epoch": 0.40607046070460706,
      "step": 1873,
      "training_loss": 6.810931205749512
    },
    {
      "epoch": 0.40628726287262873,
      "step": 1874,
      "training_loss": 6.688874244689941
    },
    {
      "epoch": 0.40628726287262873,
      "step": 1874,
      "training_loss": 6.94431209564209
    },
    {
      "epoch": 0.40628726287262873,
      "step": 1874,
      "training_loss": 7.779850959777832
    },
    {
      "epoch": 0.40628726287262873,
      "step": 1874,
      "training_loss": 7.747437000274658
    },
    {
      "epoch": 0.4065040650406504,
      "step": 1875,
      "training_loss": 5.9736647605896
    },
    {
      "epoch": 0.4065040650406504,
      "step": 1875,
      "training_loss": 6.549195289611816
    },
    {
      "epoch": 0.4065040650406504,
      "step": 1875,
      "training_loss": 6.829215049743652
    },
    {
      "epoch": 0.4065040650406504,
      "step": 1875,
      "training_loss": 7.014966011047363
    },
    {
      "epoch": 0.40672086720867207,
      "grad_norm": 13.86168098449707,
      "learning_rate": 1e-05,
      "loss": 6.8772,
      "step": 1876
    },
    {
      "epoch": 0.40672086720867207,
      "step": 1876,
      "training_loss": 7.500443935394287
    },
    {
      "epoch": 0.40672086720867207,
      "step": 1876,
      "training_loss": 7.329555988311768
    },
    {
      "epoch": 0.40672086720867207,
      "step": 1876,
      "training_loss": 6.283778667449951
    },
    {
      "epoch": 0.40672086720867207,
      "step": 1876,
      "training_loss": 5.295026779174805
    },
    {
      "epoch": 0.4069376693766938,
      "step": 1877,
      "training_loss": 7.256815433502197
    },
    {
      "epoch": 0.4069376693766938,
      "step": 1877,
      "training_loss": 7.584316730499268
    },
    {
      "epoch": 0.4069376693766938,
      "step": 1877,
      "training_loss": 7.102174282073975
    },
    {
      "epoch": 0.4069376693766938,
      "step": 1877,
      "training_loss": 6.085043907165527
    },
    {
      "epoch": 0.40715447154471546,
      "step": 1878,
      "training_loss": 4.777405738830566
    },
    {
      "epoch": 0.40715447154471546,
      "step": 1878,
      "training_loss": 7.250609874725342
    },
    {
      "epoch": 0.40715447154471546,
      "step": 1878,
      "training_loss": 6.975409984588623
    },
    {
      "epoch": 0.40715447154471546,
      "step": 1878,
      "training_loss": 7.075806140899658
    },
    {
      "epoch": 0.4073712737127371,
      "step": 1879,
      "training_loss": 8.218644142150879
    },
    {
      "epoch": 0.4073712737127371,
      "step": 1879,
      "training_loss": 6.866625785827637
    },
    {
      "epoch": 0.4073712737127371,
      "step": 1879,
      "training_loss": 6.410890579223633
    },
    {
      "epoch": 0.4073712737127371,
      "step": 1879,
      "training_loss": 7.301613807678223
    },
    {
      "epoch": 0.4075880758807588,
      "grad_norm": 12.762906074523926,
      "learning_rate": 1e-05,
      "loss": 6.8321,
      "step": 1880
    },
    {
      "epoch": 0.4075880758807588,
      "step": 1880,
      "training_loss": 6.353157043457031
    },
    {
      "epoch": 0.4075880758807588,
      "step": 1880,
      "training_loss": 7.417308807373047
    },
    {
      "epoch": 0.4075880758807588,
      "step": 1880,
      "training_loss": 6.474914073944092
    },
    {
      "epoch": 0.4075880758807588,
      "step": 1880,
      "training_loss": 7.285009860992432
    },
    {
      "epoch": 0.40780487804878046,
      "step": 1881,
      "training_loss": 7.104901313781738
    },
    {
      "epoch": 0.40780487804878046,
      "step": 1881,
      "training_loss": 7.058778285980225
    },
    {
      "epoch": 0.40780487804878046,
      "step": 1881,
      "training_loss": 7.14928674697876
    },
    {
      "epoch": 0.40780487804878046,
      "step": 1881,
      "training_loss": 6.646563529968262
    },
    {
      "epoch": 0.4080216802168022,
      "step": 1882,
      "training_loss": 5.495670795440674
    },
    {
      "epoch": 0.4080216802168022,
      "step": 1882,
      "training_loss": 6.833701133728027
    },
    {
      "epoch": 0.4080216802168022,
      "step": 1882,
      "training_loss": 6.344050884246826
    },
    {
      "epoch": 0.4080216802168022,
      "step": 1882,
      "training_loss": 7.233257293701172
    },
    {
      "epoch": 0.40823848238482385,
      "step": 1883,
      "training_loss": 7.738831520080566
    },
    {
      "epoch": 0.40823848238482385,
      "step": 1883,
      "training_loss": 7.391574382781982
    },
    {
      "epoch": 0.40823848238482385,
      "step": 1883,
      "training_loss": 6.449222087860107
    },
    {
      "epoch": 0.40823848238482385,
      "step": 1883,
      "training_loss": 7.641183853149414
    },
    {
      "epoch": 0.4084552845528455,
      "grad_norm": 18.488468170166016,
      "learning_rate": 1e-05,
      "loss": 6.9136,
      "step": 1884
    },
    {
      "epoch": 0.4084552845528455,
      "step": 1884,
      "training_loss": 7.845779895782471
    },
    {
      "epoch": 0.4084552845528455,
      "step": 1884,
      "training_loss": 7.720578670501709
    },
    {
      "epoch": 0.4084552845528455,
      "step": 1884,
      "training_loss": 6.838652610778809
    },
    {
      "epoch": 0.4084552845528455,
      "step": 1884,
      "training_loss": 6.251973628997803
    },
    {
      "epoch": 0.4086720867208672,
      "step": 1885,
      "training_loss": 5.5964179039001465
    },
    {
      "epoch": 0.4086720867208672,
      "step": 1885,
      "training_loss": 7.382522106170654
    },
    {
      "epoch": 0.4086720867208672,
      "step": 1885,
      "training_loss": 7.339742183685303
    },
    {
      "epoch": 0.4086720867208672,
      "step": 1885,
      "training_loss": 7.457535266876221
    },
    {
      "epoch": 0.4088888888888889,
      "step": 1886,
      "training_loss": 6.303336143493652
    },
    {
      "epoch": 0.4088888888888889,
      "step": 1886,
      "training_loss": 6.992598533630371
    },
    {
      "epoch": 0.4088888888888889,
      "step": 1886,
      "training_loss": 7.0442681312561035
    },
    {
      "epoch": 0.4088888888888889,
      "step": 1886,
      "training_loss": 8.125143051147461
    },
    {
      "epoch": 0.4091056910569106,
      "step": 1887,
      "training_loss": 7.815236568450928
    },
    {
      "epoch": 0.4091056910569106,
      "step": 1887,
      "training_loss": 7.896109104156494
    },
    {
      "epoch": 0.4091056910569106,
      "step": 1887,
      "training_loss": 6.741309642791748
    },
    {
      "epoch": 0.4091056910569106,
      "step": 1887,
      "training_loss": 8.633638381958008
    },
    {
      "epoch": 0.40932249322493225,
      "grad_norm": 13.929642677307129,
      "learning_rate": 1e-05,
      "loss": 7.2491,
      "step": 1888
    },
    {
      "epoch": 0.40932249322493225,
      "step": 1888,
      "training_loss": 7.418387413024902
    },
    {
      "epoch": 0.40932249322493225,
      "step": 1888,
      "training_loss": 6.003640174865723
    },
    {
      "epoch": 0.40932249322493225,
      "step": 1888,
      "training_loss": 5.897780418395996
    },
    {
      "epoch": 0.40932249322493225,
      "step": 1888,
      "training_loss": 8.545174598693848
    },
    {
      "epoch": 0.4095392953929539,
      "step": 1889,
      "training_loss": 8.546612739562988
    },
    {
      "epoch": 0.4095392953929539,
      "step": 1889,
      "training_loss": 6.9504241943359375
    },
    {
      "epoch": 0.4095392953929539,
      "step": 1889,
      "training_loss": 6.89384126663208
    },
    {
      "epoch": 0.4095392953929539,
      "step": 1889,
      "training_loss": 5.641729831695557
    },
    {
      "epoch": 0.4097560975609756,
      "step": 1890,
      "training_loss": 7.996703624725342
    },
    {
      "epoch": 0.4097560975609756,
      "step": 1890,
      "training_loss": 7.687174320220947
    },
    {
      "epoch": 0.4097560975609756,
      "step": 1890,
      "training_loss": 7.430656433105469
    },
    {
      "epoch": 0.4097560975609756,
      "step": 1890,
      "training_loss": 7.940340518951416
    },
    {
      "epoch": 0.4099728997289973,
      "step": 1891,
      "training_loss": 6.971424102783203
    },
    {
      "epoch": 0.4099728997289973,
      "step": 1891,
      "training_loss": 6.718386173248291
    },
    {
      "epoch": 0.4099728997289973,
      "step": 1891,
      "training_loss": 5.6631245613098145
    },
    {
      "epoch": 0.4099728997289973,
      "step": 1891,
      "training_loss": 7.376100063323975
    },
    {
      "epoch": 0.410189701897019,
      "grad_norm": 11.093749046325684,
      "learning_rate": 1e-05,
      "loss": 7.1051,
      "step": 1892
    },
    {
      "epoch": 0.410189701897019,
      "step": 1892,
      "training_loss": 7.893421173095703
    },
    {
      "epoch": 0.410189701897019,
      "step": 1892,
      "training_loss": 7.197078704833984
    },
    {
      "epoch": 0.410189701897019,
      "step": 1892,
      "training_loss": 7.295474529266357
    },
    {
      "epoch": 0.410189701897019,
      "step": 1892,
      "training_loss": 6.739606857299805
    },
    {
      "epoch": 0.41040650406504064,
      "step": 1893,
      "training_loss": 5.868967533111572
    },
    {
      "epoch": 0.41040650406504064,
      "step": 1893,
      "training_loss": 7.32244348526001
    },
    {
      "epoch": 0.41040650406504064,
      "step": 1893,
      "training_loss": 4.48317813873291
    },
    {
      "epoch": 0.41040650406504064,
      "step": 1893,
      "training_loss": 6.545616626739502
    },
    {
      "epoch": 0.4106233062330623,
      "step": 1894,
      "training_loss": 7.727317810058594
    },
    {
      "epoch": 0.4106233062330623,
      "step": 1894,
      "training_loss": 7.61076545715332
    },
    {
      "epoch": 0.4106233062330623,
      "step": 1894,
      "training_loss": 6.215098857879639
    },
    {
      "epoch": 0.4106233062330623,
      "step": 1894,
      "training_loss": 7.444573879241943
    },
    {
      "epoch": 0.41084010840108404,
      "step": 1895,
      "training_loss": 6.0021843910217285
    },
    {
      "epoch": 0.41084010840108404,
      "step": 1895,
      "training_loss": 7.451939582824707
    },
    {
      "epoch": 0.41084010840108404,
      "step": 1895,
      "training_loss": 6.4332170486450195
    },
    {
      "epoch": 0.41084010840108404,
      "step": 1895,
      "training_loss": 6.838639259338379
    },
    {
      "epoch": 0.4110569105691057,
      "grad_norm": 13.854036331176758,
      "learning_rate": 1e-05,
      "loss": 6.8168,
      "step": 1896
    },
    {
      "epoch": 0.4110569105691057,
      "step": 1896,
      "training_loss": 6.013025760650635
    },
    {
      "epoch": 0.4110569105691057,
      "step": 1896,
      "training_loss": 7.325876712799072
    },
    {
      "epoch": 0.4110569105691057,
      "step": 1896,
      "training_loss": 6.640594005584717
    },
    {
      "epoch": 0.4110569105691057,
      "step": 1896,
      "training_loss": 5.888084888458252
    },
    {
      "epoch": 0.4112737127371274,
      "step": 1897,
      "training_loss": 8.735339164733887
    },
    {
      "epoch": 0.4112737127371274,
      "step": 1897,
      "training_loss": 7.662420272827148
    },
    {
      "epoch": 0.4112737127371274,
      "step": 1897,
      "training_loss": 6.850462436676025
    },
    {
      "epoch": 0.4112737127371274,
      "step": 1897,
      "training_loss": 7.181193828582764
    },
    {
      "epoch": 0.41149051490514904,
      "step": 1898,
      "training_loss": 3.688311815261841
    },
    {
      "epoch": 0.41149051490514904,
      "step": 1898,
      "training_loss": 7.0135064125061035
    },
    {
      "epoch": 0.41149051490514904,
      "step": 1898,
      "training_loss": 8.031797409057617
    },
    {
      "epoch": 0.41149051490514904,
      "step": 1898,
      "training_loss": 7.285133361816406
    },
    {
      "epoch": 0.4117073170731707,
      "step": 1899,
      "training_loss": 7.989424228668213
    },
    {
      "epoch": 0.4117073170731707,
      "step": 1899,
      "training_loss": 7.258533477783203
    },
    {
      "epoch": 0.4117073170731707,
      "step": 1899,
      "training_loss": 7.7671098709106445
    },
    {
      "epoch": 0.4117073170731707,
      "step": 1899,
      "training_loss": 5.597353935241699
    },
    {
      "epoch": 0.41192411924119243,
      "grad_norm": 11.021623611450195,
      "learning_rate": 1e-05,
      "loss": 6.933,
      "step": 1900
    },
    {
      "epoch": 0.41192411924119243,
      "step": 1900,
      "training_loss": 7.121721267700195
    },
    {
      "epoch": 0.41192411924119243,
      "step": 1900,
      "training_loss": 6.383524417877197
    },
    {
      "epoch": 0.41192411924119243,
      "step": 1900,
      "training_loss": 6.434305667877197
    },
    {
      "epoch": 0.41192411924119243,
      "step": 1900,
      "training_loss": 6.960063934326172
    },
    {
      "epoch": 0.4121409214092141,
      "step": 1901,
      "training_loss": 6.615703105926514
    },
    {
      "epoch": 0.4121409214092141,
      "step": 1901,
      "training_loss": 7.8913702964782715
    },
    {
      "epoch": 0.4121409214092141,
      "step": 1901,
      "training_loss": 8.506864547729492
    },
    {
      "epoch": 0.4121409214092141,
      "step": 1901,
      "training_loss": 6.381888389587402
    },
    {
      "epoch": 0.41235772357723577,
      "step": 1902,
      "training_loss": 6.889255046844482
    },
    {
      "epoch": 0.41235772357723577,
      "step": 1902,
      "training_loss": 7.8379621505737305
    },
    {
      "epoch": 0.41235772357723577,
      "step": 1902,
      "training_loss": 7.407572269439697
    },
    {
      "epoch": 0.41235772357723577,
      "step": 1902,
      "training_loss": 5.817511081695557
    },
    {
      "epoch": 0.41257452574525744,
      "step": 1903,
      "training_loss": 7.4382710456848145
    },
    {
      "epoch": 0.41257452574525744,
      "step": 1903,
      "training_loss": 6.669683933258057
    },
    {
      "epoch": 0.41257452574525744,
      "step": 1903,
      "training_loss": 8.769148826599121
    },
    {
      "epoch": 0.41257452574525744,
      "step": 1903,
      "training_loss": 5.655536651611328
    },
    {
      "epoch": 0.41279132791327916,
      "grad_norm": 11.375889778137207,
      "learning_rate": 1e-05,
      "loss": 7.0488,
      "step": 1904
    },
    {
      "epoch": 0.41279132791327916,
      "step": 1904,
      "training_loss": 8.88162612915039
    },
    {
      "epoch": 0.41279132791327916,
      "step": 1904,
      "training_loss": 7.511412620544434
    },
    {
      "epoch": 0.41279132791327916,
      "step": 1904,
      "training_loss": 7.900771617889404
    },
    {
      "epoch": 0.41279132791327916,
      "step": 1904,
      "training_loss": 8.277113914489746
    },
    {
      "epoch": 0.41300813008130083,
      "step": 1905,
      "training_loss": 5.520416259765625
    },
    {
      "epoch": 0.41300813008130083,
      "step": 1905,
      "training_loss": 6.221579074859619
    },
    {
      "epoch": 0.41300813008130083,
      "step": 1905,
      "training_loss": 7.139410495758057
    },
    {
      "epoch": 0.41300813008130083,
      "step": 1905,
      "training_loss": 7.423389911651611
    },
    {
      "epoch": 0.4132249322493225,
      "step": 1906,
      "training_loss": 7.270284175872803
    },
    {
      "epoch": 0.4132249322493225,
      "step": 1906,
      "training_loss": 7.472412109375
    },
    {
      "epoch": 0.4132249322493225,
      "step": 1906,
      "training_loss": 7.826138496398926
    },
    {
      "epoch": 0.4132249322493225,
      "step": 1906,
      "training_loss": 6.1350250244140625
    },
    {
      "epoch": 0.41344173441734416,
      "step": 1907,
      "training_loss": 6.319952964782715
    },
    {
      "epoch": 0.41344173441734416,
      "step": 1907,
      "training_loss": 6.837907314300537
    },
    {
      "epoch": 0.41344173441734416,
      "step": 1907,
      "training_loss": 6.1874566078186035
    },
    {
      "epoch": 0.41344173441734416,
      "step": 1907,
      "training_loss": 6.805717468261719
    },
    {
      "epoch": 0.41365853658536583,
      "grad_norm": 13.564123153686523,
      "learning_rate": 1e-05,
      "loss": 7.1082,
      "step": 1908
    },
    {
      "epoch": 0.41365853658536583,
      "step": 1908,
      "training_loss": 8.735082626342773
    },
    {
      "epoch": 0.41365853658536583,
      "step": 1908,
      "training_loss": 7.322756767272949
    },
    {
      "epoch": 0.41365853658536583,
      "step": 1908,
      "training_loss": 6.731095314025879
    },
    {
      "epoch": 0.41365853658536583,
      "step": 1908,
      "training_loss": 6.985803127288818
    },
    {
      "epoch": 0.41387533875338756,
      "step": 1909,
      "training_loss": 6.697235107421875
    },
    {
      "epoch": 0.41387533875338756,
      "step": 1909,
      "training_loss": 5.441770076751709
    },
    {
      "epoch": 0.41387533875338756,
      "step": 1909,
      "training_loss": 7.8661322593688965
    },
    {
      "epoch": 0.41387533875338756,
      "step": 1909,
      "training_loss": 7.5420756340026855
    },
    {
      "epoch": 0.4140921409214092,
      "step": 1910,
      "training_loss": 7.726109027862549
    },
    {
      "epoch": 0.4140921409214092,
      "step": 1910,
      "training_loss": 6.84493350982666
    },
    {
      "epoch": 0.4140921409214092,
      "step": 1910,
      "training_loss": 6.883676528930664
    },
    {
      "epoch": 0.4140921409214092,
      "step": 1910,
      "training_loss": 6.091180324554443
    },
    {
      "epoch": 0.4143089430894309,
      "step": 1911,
      "training_loss": 6.519358158111572
    },
    {
      "epoch": 0.4143089430894309,
      "step": 1911,
      "training_loss": 6.739306926727295
    },
    {
      "epoch": 0.4143089430894309,
      "step": 1911,
      "training_loss": 7.859296798706055
    },
    {
      "epoch": 0.4143089430894309,
      "step": 1911,
      "training_loss": 7.813914775848389
    },
    {
      "epoch": 0.41452574525745256,
      "grad_norm": 16.095975875854492,
      "learning_rate": 1e-05,
      "loss": 7.1125,
      "step": 1912
    },
    {
      "epoch": 0.41452574525745256,
      "step": 1912,
      "training_loss": 8.065293312072754
    },
    {
      "epoch": 0.41452574525745256,
      "step": 1912,
      "training_loss": 6.563775539398193
    },
    {
      "epoch": 0.41452574525745256,
      "step": 1912,
      "training_loss": 7.439987659454346
    },
    {
      "epoch": 0.41452574525745256,
      "step": 1912,
      "training_loss": 5.613483905792236
    },
    {
      "epoch": 0.41474254742547423,
      "step": 1913,
      "training_loss": 6.86297607421875
    },
    {
      "epoch": 0.41474254742547423,
      "step": 1913,
      "training_loss": 7.875993728637695
    },
    {
      "epoch": 0.41474254742547423,
      "step": 1913,
      "training_loss": 7.460069179534912
    },
    {
      "epoch": 0.41474254742547423,
      "step": 1913,
      "training_loss": 6.443350791931152
    },
    {
      "epoch": 0.41495934959349595,
      "step": 1914,
      "training_loss": 5.9850687980651855
    },
    {
      "epoch": 0.41495934959349595,
      "step": 1914,
      "training_loss": 7.7412590980529785
    },
    {
      "epoch": 0.41495934959349595,
      "step": 1914,
      "training_loss": 6.945632457733154
    },
    {
      "epoch": 0.41495934959349595,
      "step": 1914,
      "training_loss": 6.994552135467529
    },
    {
      "epoch": 0.4151761517615176,
      "step": 1915,
      "training_loss": 6.17425012588501
    },
    {
      "epoch": 0.4151761517615176,
      "step": 1915,
      "training_loss": 6.968750476837158
    },
    {
      "epoch": 0.4151761517615176,
      "step": 1915,
      "training_loss": 8.462943077087402
    },
    {
      "epoch": 0.4151761517615176,
      "step": 1915,
      "training_loss": 7.948525428771973
    },
    {
      "epoch": 0.4153929539295393,
      "grad_norm": 10.507119178771973,
      "learning_rate": 1e-05,
      "loss": 7.0966,
      "step": 1916
    },
    {
      "epoch": 0.4153929539295393,
      "step": 1916,
      "training_loss": 7.413222312927246
    },
    {
      "epoch": 0.4153929539295393,
      "step": 1916,
      "training_loss": 4.715931415557861
    },
    {
      "epoch": 0.4153929539295393,
      "step": 1916,
      "training_loss": 7.629273891448975
    },
    {
      "epoch": 0.4153929539295393,
      "step": 1916,
      "training_loss": 6.479766845703125
    },
    {
      "epoch": 0.41560975609756096,
      "step": 1917,
      "training_loss": 5.805432319641113
    },
    {
      "epoch": 0.41560975609756096,
      "step": 1917,
      "training_loss": 5.097076892852783
    },
    {
      "epoch": 0.41560975609756096,
      "step": 1917,
      "training_loss": 5.917294025421143
    },
    {
      "epoch": 0.41560975609756096,
      "step": 1917,
      "training_loss": 6.919555187225342
    },
    {
      "epoch": 0.4158265582655827,
      "step": 1918,
      "training_loss": 7.312130928039551
    },
    {
      "epoch": 0.4158265582655827,
      "step": 1918,
      "training_loss": 5.845226287841797
    },
    {
      "epoch": 0.4158265582655827,
      "step": 1918,
      "training_loss": 7.298004150390625
    },
    {
      "epoch": 0.4158265582655827,
      "step": 1918,
      "training_loss": 6.82334566116333
    },
    {
      "epoch": 0.41604336043360435,
      "step": 1919,
      "training_loss": 6.255473613739014
    },
    {
      "epoch": 0.41604336043360435,
      "step": 1919,
      "training_loss": 6.163650989532471
    },
    {
      "epoch": 0.41604336043360435,
      "step": 1919,
      "training_loss": 7.125606536865234
    },
    {
      "epoch": 0.41604336043360435,
      "step": 1919,
      "training_loss": 7.612839698791504
    },
    {
      "epoch": 0.416260162601626,
      "grad_norm": 16.654434204101562,
      "learning_rate": 1e-05,
      "loss": 6.5259,
      "step": 1920
    },
    {
      "epoch": 0.416260162601626,
      "step": 1920,
      "training_loss": 5.9556145668029785
    },
    {
      "epoch": 0.416260162601626,
      "step": 1920,
      "training_loss": 6.470804214477539
    },
    {
      "epoch": 0.416260162601626,
      "step": 1920,
      "training_loss": 7.27482271194458
    },
    {
      "epoch": 0.416260162601626,
      "step": 1920,
      "training_loss": 6.32291316986084
    },
    {
      "epoch": 0.4164769647696477,
      "step": 1921,
      "training_loss": 6.321402549743652
    },
    {
      "epoch": 0.4164769647696477,
      "step": 1921,
      "training_loss": 4.991641998291016
    },
    {
      "epoch": 0.4164769647696477,
      "step": 1921,
      "training_loss": 7.973714828491211
    },
    {
      "epoch": 0.4164769647696477,
      "step": 1921,
      "training_loss": 7.014674186706543
    },
    {
      "epoch": 0.41669376693766935,
      "step": 1922,
      "training_loss": 7.398632049560547
    },
    {
      "epoch": 0.41669376693766935,
      "step": 1922,
      "training_loss": 7.174102783203125
    },
    {
      "epoch": 0.41669376693766935,
      "step": 1922,
      "training_loss": 7.3336181640625
    },
    {
      "epoch": 0.41669376693766935,
      "step": 1922,
      "training_loss": 6.5655646324157715
    },
    {
      "epoch": 0.4169105691056911,
      "step": 1923,
      "training_loss": 6.168823719024658
    },
    {
      "epoch": 0.4169105691056911,
      "step": 1923,
      "training_loss": 5.889039993286133
    },
    {
      "epoch": 0.4169105691056911,
      "step": 1923,
      "training_loss": 7.5314531326293945
    },
    {
      "epoch": 0.4169105691056911,
      "step": 1923,
      "training_loss": 6.874566555023193
    },
    {
      "epoch": 0.41712737127371274,
      "grad_norm": 8.861069679260254,
      "learning_rate": 1e-05,
      "loss": 6.7038,
      "step": 1924
    },
    {
      "epoch": 0.41712737127371274,
      "step": 1924,
      "training_loss": 6.946407318115234
    },
    {
      "epoch": 0.41712737127371274,
      "step": 1924,
      "training_loss": 7.376439094543457
    },
    {
      "epoch": 0.41712737127371274,
      "step": 1924,
      "training_loss": 7.543226718902588
    },
    {
      "epoch": 0.41712737127371274,
      "step": 1924,
      "training_loss": 6.751337051391602
    },
    {
      "epoch": 0.4173441734417344,
      "step": 1925,
      "training_loss": 7.005956172943115
    },
    {
      "epoch": 0.4173441734417344,
      "step": 1925,
      "training_loss": 6.83931303024292
    },
    {
      "epoch": 0.4173441734417344,
      "step": 1925,
      "training_loss": 6.895480155944824
    },
    {
      "epoch": 0.4173441734417344,
      "step": 1925,
      "training_loss": 7.595380783081055
    },
    {
      "epoch": 0.4175609756097561,
      "step": 1926,
      "training_loss": 7.995596408843994
    },
    {
      "epoch": 0.4175609756097561,
      "step": 1926,
      "training_loss": 7.204065799713135
    },
    {
      "epoch": 0.4175609756097561,
      "step": 1926,
      "training_loss": 5.238842964172363
    },
    {
      "epoch": 0.4175609756097561,
      "step": 1926,
      "training_loss": 7.124108791351318
    },
    {
      "epoch": 0.4177777777777778,
      "step": 1927,
      "training_loss": 8.171958923339844
    },
    {
      "epoch": 0.4177777777777778,
      "step": 1927,
      "training_loss": 7.071220397949219
    },
    {
      "epoch": 0.4177777777777778,
      "step": 1927,
      "training_loss": 4.9354095458984375
    },
    {
      "epoch": 0.4177777777777778,
      "step": 1927,
      "training_loss": 7.535356521606445
    },
    {
      "epoch": 0.41799457994579947,
      "grad_norm": 16.097341537475586,
      "learning_rate": 1e-05,
      "loss": 7.0144,
      "step": 1928
    },
    {
      "epoch": 0.41799457994579947,
      "step": 1928,
      "training_loss": 4.874967575073242
    },
    {
      "epoch": 0.41799457994579947,
      "step": 1928,
      "training_loss": 7.6744160652160645
    },
    {
      "epoch": 0.41799457994579947,
      "step": 1928,
      "training_loss": 7.308837890625
    },
    {
      "epoch": 0.41799457994579947,
      "step": 1928,
      "training_loss": 7.987208366394043
    },
    {
      "epoch": 0.41821138211382114,
      "step": 1929,
      "training_loss": 7.639956951141357
    },
    {
      "epoch": 0.41821138211382114,
      "step": 1929,
      "training_loss": 6.864324569702148
    },
    {
      "epoch": 0.41821138211382114,
      "step": 1929,
      "training_loss": 7.360225200653076
    },
    {
      "epoch": 0.41821138211382114,
      "step": 1929,
      "training_loss": 7.0563836097717285
    },
    {
      "epoch": 0.4184281842818428,
      "step": 1930,
      "training_loss": 8.198180198669434
    },
    {
      "epoch": 0.4184281842818428,
      "step": 1930,
      "training_loss": 6.972628593444824
    },
    {
      "epoch": 0.4184281842818428,
      "step": 1930,
      "training_loss": 4.953065395355225
    },
    {
      "epoch": 0.4184281842818428,
      "step": 1930,
      "training_loss": 6.735562801361084
    },
    {
      "epoch": 0.4186449864498645,
      "step": 1931,
      "training_loss": 7.097900867462158
    },
    {
      "epoch": 0.4186449864498645,
      "step": 1931,
      "training_loss": 6.8009867668151855
    },
    {
      "epoch": 0.4186449864498645,
      "step": 1931,
      "training_loss": 5.873628616333008
    },
    {
      "epoch": 0.4186449864498645,
      "step": 1931,
      "training_loss": 6.565676212310791
    },
    {
      "epoch": 0.4188617886178862,
      "grad_norm": 11.986502647399902,
      "learning_rate": 1e-05,
      "loss": 6.8727,
      "step": 1932
    },
    {
      "epoch": 0.4188617886178862,
      "step": 1932,
      "training_loss": 5.998910427093506
    },
    {
      "epoch": 0.4188617886178862,
      "step": 1932,
      "training_loss": 7.419452667236328
    },
    {
      "epoch": 0.4188617886178862,
      "step": 1932,
      "training_loss": 8.14021110534668
    },
    {
      "epoch": 0.4188617886178862,
      "step": 1932,
      "training_loss": 7.406551361083984
    },
    {
      "epoch": 0.41907859078590787,
      "step": 1933,
      "training_loss": 6.400725364685059
    },
    {
      "epoch": 0.41907859078590787,
      "step": 1933,
      "training_loss": 5.920090198516846
    },
    {
      "epoch": 0.41907859078590787,
      "step": 1933,
      "training_loss": 7.444692611694336
    },
    {
      "epoch": 0.41907859078590787,
      "step": 1933,
      "training_loss": 8.126631736755371
    },
    {
      "epoch": 0.41929539295392954,
      "step": 1934,
      "training_loss": 7.241993427276611
    },
    {
      "epoch": 0.41929539295392954,
      "step": 1934,
      "training_loss": 6.441658020019531
    },
    {
      "epoch": 0.41929539295392954,
      "step": 1934,
      "training_loss": 7.149231433868408
    },
    {
      "epoch": 0.41929539295392954,
      "step": 1934,
      "training_loss": 7.095348834991455
    },
    {
      "epoch": 0.4195121951219512,
      "step": 1935,
      "training_loss": 9.788049697875977
    },
    {
      "epoch": 0.4195121951219512,
      "step": 1935,
      "training_loss": 7.61066198348999
    },
    {
      "epoch": 0.4195121951219512,
      "step": 1935,
      "training_loss": 7.145210266113281
    },
    {
      "epoch": 0.4195121951219512,
      "step": 1935,
      "training_loss": 7.3247785568237305
    },
    {
      "epoch": 0.4197289972899729,
      "grad_norm": 13.691765785217285,
      "learning_rate": 1e-05,
      "loss": 7.2909,
      "step": 1936
    },
    {
      "epoch": 0.4197289972899729,
      "step": 1936,
      "training_loss": 6.853979110717773
    },
    {
      "epoch": 0.4197289972899729,
      "step": 1936,
      "training_loss": 4.185957908630371
    },
    {
      "epoch": 0.4197289972899729,
      "step": 1936,
      "training_loss": 8.564746856689453
    },
    {
      "epoch": 0.4197289972899729,
      "step": 1936,
      "training_loss": 8.607500076293945
    },
    {
      "epoch": 0.4199457994579946,
      "step": 1937,
      "training_loss": 8.194698333740234
    },
    {
      "epoch": 0.4199457994579946,
      "step": 1937,
      "training_loss": 8.338704109191895
    },
    {
      "epoch": 0.4199457994579946,
      "step": 1937,
      "training_loss": 7.567476749420166
    },
    {
      "epoch": 0.4199457994579946,
      "step": 1937,
      "training_loss": 8.680437088012695
    },
    {
      "epoch": 0.42016260162601626,
      "step": 1938,
      "training_loss": 6.902261257171631
    },
    {
      "epoch": 0.42016260162601626,
      "step": 1938,
      "training_loss": 8.195412635803223
    },
    {
      "epoch": 0.42016260162601626,
      "step": 1938,
      "training_loss": 7.306352138519287
    },
    {
      "epoch": 0.42016260162601626,
      "step": 1938,
      "training_loss": 7.043200969696045
    },
    {
      "epoch": 0.42037940379403793,
      "step": 1939,
      "training_loss": 5.1203532218933105
    },
    {
      "epoch": 0.42037940379403793,
      "step": 1939,
      "training_loss": 6.700787544250488
    },
    {
      "epoch": 0.42037940379403793,
      "step": 1939,
      "training_loss": 7.085653305053711
    },
    {
      "epoch": 0.42037940379403793,
      "step": 1939,
      "training_loss": 7.807476043701172
    },
    {
      "epoch": 0.4205962059620596,
      "grad_norm": 10.451992988586426,
      "learning_rate": 1e-05,
      "loss": 7.3222,
      "step": 1940
    },
    {
      "epoch": 0.4205962059620596,
      "step": 1940,
      "training_loss": 7.867525577545166
    },
    {
      "epoch": 0.4205962059620596,
      "step": 1940,
      "training_loss": 7.829272270202637
    },
    {
      "epoch": 0.4205962059620596,
      "step": 1940,
      "training_loss": 6.678114414215088
    },
    {
      "epoch": 0.4205962059620596,
      "step": 1940,
      "training_loss": 6.996206283569336
    },
    {
      "epoch": 0.4208130081300813,
      "step": 1941,
      "training_loss": 7.625606536865234
    },
    {
      "epoch": 0.4208130081300813,
      "step": 1941,
      "training_loss": 6.65707540512085
    },
    {
      "epoch": 0.4208130081300813,
      "step": 1941,
      "training_loss": 6.985065460205078
    },
    {
      "epoch": 0.4208130081300813,
      "step": 1941,
      "training_loss": 6.782149791717529
    },
    {
      "epoch": 0.421029810298103,
      "step": 1942,
      "training_loss": 7.075211524963379
    },
    {
      "epoch": 0.421029810298103,
      "step": 1942,
      "training_loss": 6.900926113128662
    },
    {
      "epoch": 0.421029810298103,
      "step": 1942,
      "training_loss": 7.269252300262451
    },
    {
      "epoch": 0.421029810298103,
      "step": 1942,
      "training_loss": 7.79475736618042
    },
    {
      "epoch": 0.42124661246612466,
      "step": 1943,
      "training_loss": 6.642531394958496
    },
    {
      "epoch": 0.42124661246612466,
      "step": 1943,
      "training_loss": 5.0019683837890625
    },
    {
      "epoch": 0.42124661246612466,
      "step": 1943,
      "training_loss": 5.917761325836182
    },
    {
      "epoch": 0.42124661246612466,
      "step": 1943,
      "training_loss": 6.730395317077637
    },
    {
      "epoch": 0.4214634146341463,
      "grad_norm": 15.386128425598145,
      "learning_rate": 1e-05,
      "loss": 6.9221,
      "step": 1944
    },
    {
      "epoch": 0.4214634146341463,
      "step": 1944,
      "training_loss": 6.443520545959473
    },
    {
      "epoch": 0.4214634146341463,
      "step": 1944,
      "training_loss": 7.32811975479126
    },
    {
      "epoch": 0.4214634146341463,
      "step": 1944,
      "training_loss": 6.523280620574951
    },
    {
      "epoch": 0.4214634146341463,
      "step": 1944,
      "training_loss": 6.642639636993408
    },
    {
      "epoch": 0.421680216802168,
      "step": 1945,
      "training_loss": 6.290378093719482
    },
    {
      "epoch": 0.421680216802168,
      "step": 1945,
      "training_loss": 4.975742816925049
    },
    {
      "epoch": 0.421680216802168,
      "step": 1945,
      "training_loss": 6.881274223327637
    },
    {
      "epoch": 0.421680216802168,
      "step": 1945,
      "training_loss": 6.151573657989502
    },
    {
      "epoch": 0.4218970189701897,
      "step": 1946,
      "training_loss": 6.532840251922607
    },
    {
      "epoch": 0.4218970189701897,
      "step": 1946,
      "training_loss": 6.237017631530762
    },
    {
      "epoch": 0.4218970189701897,
      "step": 1946,
      "training_loss": 7.07658576965332
    },
    {
      "epoch": 0.4218970189701897,
      "step": 1946,
      "training_loss": 7.761266231536865
    },
    {
      "epoch": 0.4221138211382114,
      "step": 1947,
      "training_loss": 7.303860664367676
    },
    {
      "epoch": 0.4221138211382114,
      "step": 1947,
      "training_loss": 7.252689361572266
    },
    {
      "epoch": 0.4221138211382114,
      "step": 1947,
      "training_loss": 8.320611000061035
    },
    {
      "epoch": 0.4221138211382114,
      "step": 1947,
      "training_loss": 6.927450656890869
    },
    {
      "epoch": 0.42233062330623306,
      "grad_norm": 17.67192268371582,
      "learning_rate": 1e-05,
      "loss": 6.7906,
      "step": 1948
    },
    {
      "epoch": 0.42233062330623306,
      "step": 1948,
      "training_loss": 7.624355792999268
    },
    {
      "epoch": 0.42233062330623306,
      "step": 1948,
      "training_loss": 6.422626972198486
    },
    {
      "epoch": 0.42233062330623306,
      "step": 1948,
      "training_loss": 6.142302989959717
    },
    {
      "epoch": 0.42233062330623306,
      "step": 1948,
      "training_loss": 7.038931369781494
    },
    {
      "epoch": 0.4225474254742547,
      "step": 1949,
      "training_loss": 7.559001445770264
    },
    {
      "epoch": 0.4225474254742547,
      "step": 1949,
      "training_loss": 6.604286193847656
    },
    {
      "epoch": 0.4225474254742547,
      "step": 1949,
      "training_loss": 6.675896167755127
    },
    {
      "epoch": 0.4225474254742547,
      "step": 1949,
      "training_loss": 6.52841854095459
    },
    {
      "epoch": 0.42276422764227645,
      "step": 1950,
      "training_loss": 6.397129535675049
    },
    {
      "epoch": 0.42276422764227645,
      "step": 1950,
      "training_loss": 6.168642997741699
    },
    {
      "epoch": 0.42276422764227645,
      "step": 1950,
      "training_loss": 7.577730178833008
    },
    {
      "epoch": 0.42276422764227645,
      "step": 1950,
      "training_loss": 6.841401100158691
    },
    {
      "epoch": 0.4229810298102981,
      "step": 1951,
      "training_loss": 7.105597019195557
    },
    {
      "epoch": 0.4229810298102981,
      "step": 1951,
      "training_loss": 7.413871765136719
    },
    {
      "epoch": 0.4229810298102981,
      "step": 1951,
      "training_loss": 6.278578281402588
    },
    {
      "epoch": 0.4229810298102981,
      "step": 1951,
      "training_loss": 7.306751728057861
    },
    {
      "epoch": 0.4231978319783198,
      "grad_norm": 13.590761184692383,
      "learning_rate": 1e-05,
      "loss": 6.8553,
      "step": 1952
    },
    {
      "epoch": 0.4231978319783198,
      "step": 1952,
      "training_loss": 7.374983310699463
    },
    {
      "epoch": 0.4231978319783198,
      "step": 1952,
      "training_loss": 7.561798572540283
    },
    {
      "epoch": 0.4231978319783198,
      "step": 1952,
      "training_loss": 7.379948616027832
    },
    {
      "epoch": 0.4231978319783198,
      "step": 1952,
      "training_loss": 6.370915412902832
    },
    {
      "epoch": 0.42341463414634145,
      "step": 1953,
      "training_loss": 6.595465183258057
    },
    {
      "epoch": 0.42341463414634145,
      "step": 1953,
      "training_loss": 7.41229772567749
    },
    {
      "epoch": 0.42341463414634145,
      "step": 1953,
      "training_loss": 7.02300500869751
    },
    {
      "epoch": 0.42341463414634145,
      "step": 1953,
      "training_loss": 7.013139247894287
    },
    {
      "epoch": 0.4236314363143631,
      "step": 1954,
      "training_loss": 7.351168632507324
    },
    {
      "epoch": 0.4236314363143631,
      "step": 1954,
      "training_loss": 6.783641815185547
    },
    {
      "epoch": 0.4236314363143631,
      "step": 1954,
      "training_loss": 6.59029483795166
    },
    {
      "epoch": 0.4236314363143631,
      "step": 1954,
      "training_loss": 6.31036376953125
    },
    {
      "epoch": 0.42384823848238484,
      "step": 1955,
      "training_loss": 7.154048442840576
    },
    {
      "epoch": 0.42384823848238484,
      "step": 1955,
      "training_loss": 4.541780948638916
    },
    {
      "epoch": 0.42384823848238484,
      "step": 1955,
      "training_loss": 6.256210803985596
    },
    {
      "epoch": 0.42384823848238484,
      "step": 1955,
      "training_loss": 5.983292102813721
    },
    {
      "epoch": 0.4240650406504065,
      "grad_norm": 17.856430053710938,
      "learning_rate": 1e-05,
      "loss": 6.7314,
      "step": 1956
    },
    {
      "epoch": 0.4240650406504065,
      "step": 1956,
      "training_loss": 6.971498012542725
    },
    {
      "epoch": 0.4240650406504065,
      "step": 1956,
      "training_loss": 7.170068264007568
    },
    {
      "epoch": 0.4240650406504065,
      "step": 1956,
      "training_loss": 6.919621467590332
    },
    {
      "epoch": 0.4240650406504065,
      "step": 1956,
      "training_loss": 8.217477798461914
    },
    {
      "epoch": 0.4242818428184282,
      "step": 1957,
      "training_loss": 7.411377429962158
    },
    {
      "epoch": 0.4242818428184282,
      "step": 1957,
      "training_loss": 6.993001461029053
    },
    {
      "epoch": 0.4242818428184282,
      "step": 1957,
      "training_loss": 6.57179594039917
    },
    {
      "epoch": 0.4242818428184282,
      "step": 1957,
      "training_loss": 7.422266960144043
    },
    {
      "epoch": 0.42449864498644985,
      "step": 1958,
      "training_loss": 7.746294021606445
    },
    {
      "epoch": 0.42449864498644985,
      "step": 1958,
      "training_loss": 6.105112075805664
    },
    {
      "epoch": 0.42449864498644985,
      "step": 1958,
      "training_loss": 6.962817192077637
    },
    {
      "epoch": 0.42449864498644985,
      "step": 1958,
      "training_loss": 5.1677069664001465
    },
    {
      "epoch": 0.42471544715447157,
      "step": 1959,
      "training_loss": 8.667131423950195
    },
    {
      "epoch": 0.42471544715447157,
      "step": 1959,
      "training_loss": 6.767185688018799
    },
    {
      "epoch": 0.42471544715447157,
      "step": 1959,
      "training_loss": 7.4697113037109375
    },
    {
      "epoch": 0.42471544715447157,
      "step": 1959,
      "training_loss": 7.979172229766846
    },
    {
      "epoch": 0.42493224932249324,
      "grad_norm": 15.37397289276123,
      "learning_rate": 1e-05,
      "loss": 7.1589,
      "step": 1960
    },
    {
      "epoch": 0.42493224932249324,
      "step": 1960,
      "training_loss": 6.983154773712158
    },
    {
      "epoch": 0.42493224932249324,
      "step": 1960,
      "training_loss": 5.882586479187012
    },
    {
      "epoch": 0.42493224932249324,
      "step": 1960,
      "training_loss": 6.503383159637451
    },
    {
      "epoch": 0.42493224932249324,
      "step": 1960,
      "training_loss": 7.089922904968262
    },
    {
      "epoch": 0.4251490514905149,
      "step": 1961,
      "training_loss": 7.263847351074219
    },
    {
      "epoch": 0.4251490514905149,
      "step": 1961,
      "training_loss": 6.3130598068237305
    },
    {
      "epoch": 0.4251490514905149,
      "step": 1961,
      "training_loss": 6.571686744689941
    },
    {
      "epoch": 0.4251490514905149,
      "step": 1961,
      "training_loss": 6.898287773132324
    },
    {
      "epoch": 0.4253658536585366,
      "step": 1962,
      "training_loss": 6.403120040893555
    },
    {
      "epoch": 0.4253658536585366,
      "step": 1962,
      "training_loss": 6.763805389404297
    },
    {
      "epoch": 0.4253658536585366,
      "step": 1962,
      "training_loss": 6.71752405166626
    },
    {
      "epoch": 0.4253658536585366,
      "step": 1962,
      "training_loss": 7.504560947418213
    },
    {
      "epoch": 0.42558265582655824,
      "step": 1963,
      "training_loss": 7.0814385414123535
    },
    {
      "epoch": 0.42558265582655824,
      "step": 1963,
      "training_loss": 6.498963832855225
    },
    {
      "epoch": 0.42558265582655824,
      "step": 1963,
      "training_loss": 7.172513484954834
    },
    {
      "epoch": 0.42558265582655824,
      "step": 1963,
      "training_loss": 6.693256378173828
    },
    {
      "epoch": 0.42579945799457997,
      "grad_norm": 15.651253700256348,
      "learning_rate": 1e-05,
      "loss": 6.7713,
      "step": 1964
    },
    {
      "epoch": 0.42579945799457997,
      "step": 1964,
      "training_loss": 7.026828289031982
    },
    {
      "epoch": 0.42579945799457997,
      "step": 1964,
      "training_loss": 8.342308044433594
    },
    {
      "epoch": 0.42579945799457997,
      "step": 1964,
      "training_loss": 8.198749542236328
    },
    {
      "epoch": 0.42579945799457997,
      "step": 1964,
      "training_loss": 6.7507734298706055
    },
    {
      "epoch": 0.42601626016260163,
      "step": 1965,
      "training_loss": 7.750661849975586
    },
    {
      "epoch": 0.42601626016260163,
      "step": 1965,
      "training_loss": 7.066956996917725
    },
    {
      "epoch": 0.42601626016260163,
      "step": 1965,
      "training_loss": 7.043732643127441
    },
    {
      "epoch": 0.42601626016260163,
      "step": 1965,
      "training_loss": 8.29996109008789
    },
    {
      "epoch": 0.4262330623306233,
      "step": 1966,
      "training_loss": 6.9204230308532715
    },
    {
      "epoch": 0.4262330623306233,
      "step": 1966,
      "training_loss": 8.187813758850098
    },
    {
      "epoch": 0.4262330623306233,
      "step": 1966,
      "training_loss": 4.72829532623291
    },
    {
      "epoch": 0.4262330623306233,
      "step": 1966,
      "training_loss": 6.3525238037109375
    },
    {
      "epoch": 0.42644986449864497,
      "step": 1967,
      "training_loss": 7.213558673858643
    },
    {
      "epoch": 0.42644986449864497,
      "step": 1967,
      "training_loss": 4.701779842376709
    },
    {
      "epoch": 0.42644986449864497,
      "step": 1967,
      "training_loss": 7.079111576080322
    },
    {
      "epoch": 0.42644986449864497,
      "step": 1967,
      "training_loss": 8.880973815917969
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 10.618593215942383,
      "learning_rate": 1e-05,
      "loss": 7.159,
      "step": 1968
    },
    {
      "epoch": 0.4266666666666667,
      "step": 1968,
      "training_loss": 6.579076766967773
    },
    {
      "epoch": 0.4266666666666667,
      "step": 1968,
      "training_loss": 7.4573469161987305
    },
    {
      "epoch": 0.4266666666666667,
      "step": 1968,
      "training_loss": 8.305821418762207
    },
    {
      "epoch": 0.4266666666666667,
      "step": 1968,
      "training_loss": 7.151198387145996
    },
    {
      "epoch": 0.42688346883468836,
      "step": 1969,
      "training_loss": 6.881896495819092
    },
    {
      "epoch": 0.42688346883468836,
      "step": 1969,
      "training_loss": 6.3196282386779785
    },
    {
      "epoch": 0.42688346883468836,
      "step": 1969,
      "training_loss": 8.036487579345703
    },
    {
      "epoch": 0.42688346883468836,
      "step": 1969,
      "training_loss": 5.304418087005615
    },
    {
      "epoch": 0.42710027100271003,
      "step": 1970,
      "training_loss": 4.559196472167969
    },
    {
      "epoch": 0.42710027100271003,
      "step": 1970,
      "training_loss": 8.081337928771973
    },
    {
      "epoch": 0.42710027100271003,
      "step": 1970,
      "training_loss": 7.021636486053467
    },
    {
      "epoch": 0.42710027100271003,
      "step": 1970,
      "training_loss": 7.767545223236084
    },
    {
      "epoch": 0.4273170731707317,
      "step": 1971,
      "training_loss": 7.316423416137695
    },
    {
      "epoch": 0.4273170731707317,
      "step": 1971,
      "training_loss": 7.6828932762146
    },
    {
      "epoch": 0.4273170731707317,
      "step": 1971,
      "training_loss": 7.745388507843018
    },
    {
      "epoch": 0.4273170731707317,
      "step": 1971,
      "training_loss": 6.191034317016602
    },
    {
      "epoch": 0.42753387533875337,
      "grad_norm": 11.665891647338867,
      "learning_rate": 1e-05,
      "loss": 7.0251,
      "step": 1972
    },
    {
      "epoch": 0.42753387533875337,
      "step": 1972,
      "training_loss": 7.936312675476074
    },
    {
      "epoch": 0.42753387533875337,
      "step": 1972,
      "training_loss": 6.955715656280518
    },
    {
      "epoch": 0.42753387533875337,
      "step": 1972,
      "training_loss": 8.38556957244873
    },
    {
      "epoch": 0.42753387533875337,
      "step": 1972,
      "training_loss": 5.923854827880859
    },
    {
      "epoch": 0.4277506775067751,
      "step": 1973,
      "training_loss": 7.645789623260498
    },
    {
      "epoch": 0.4277506775067751,
      "step": 1973,
      "training_loss": 7.23798131942749
    },
    {
      "epoch": 0.4277506775067751,
      "step": 1973,
      "training_loss": 5.679558753967285
    },
    {
      "epoch": 0.4277506775067751,
      "step": 1973,
      "training_loss": 6.594541549682617
    },
    {
      "epoch": 0.42796747967479676,
      "step": 1974,
      "training_loss": 7.224583148956299
    },
    {
      "epoch": 0.42796747967479676,
      "step": 1974,
      "training_loss": 7.335134029388428
    },
    {
      "epoch": 0.42796747967479676,
      "step": 1974,
      "training_loss": 9.031699180603027
    },
    {
      "epoch": 0.42796747967479676,
      "step": 1974,
      "training_loss": 7.1317949295043945
    },
    {
      "epoch": 0.4281842818428184,
      "step": 1975,
      "training_loss": 6.165164947509766
    },
    {
      "epoch": 0.4281842818428184,
      "step": 1975,
      "training_loss": 5.137516021728516
    },
    {
      "epoch": 0.4281842818428184,
      "step": 1975,
      "training_loss": 6.757380485534668
    },
    {
      "epoch": 0.4281842818428184,
      "step": 1975,
      "training_loss": 6.9374847412109375
    },
    {
      "epoch": 0.4284010840108401,
      "grad_norm": 10.788152694702148,
      "learning_rate": 1e-05,
      "loss": 7.005,
      "step": 1976
    },
    {
      "epoch": 0.4284010840108401,
      "step": 1976,
      "training_loss": 5.954747676849365
    },
    {
      "epoch": 0.4284010840108401,
      "step": 1976,
      "training_loss": 7.710104465484619
    },
    {
      "epoch": 0.4284010840108401,
      "step": 1976,
      "training_loss": 8.561812400817871
    },
    {
      "epoch": 0.4284010840108401,
      "step": 1976,
      "training_loss": 7.082159996032715
    },
    {
      "epoch": 0.42861788617886176,
      "step": 1977,
      "training_loss": 6.62467622756958
    },
    {
      "epoch": 0.42861788617886176,
      "step": 1977,
      "training_loss": 6.250877857208252
    },
    {
      "epoch": 0.42861788617886176,
      "step": 1977,
      "training_loss": 6.678908824920654
    },
    {
      "epoch": 0.42861788617886176,
      "step": 1977,
      "training_loss": 7.935847759246826
    },
    {
      "epoch": 0.4288346883468835,
      "step": 1978,
      "training_loss": 8.234628677368164
    },
    {
      "epoch": 0.4288346883468835,
      "step": 1978,
      "training_loss": 6.941432476043701
    },
    {
      "epoch": 0.4288346883468835,
      "step": 1978,
      "training_loss": 6.2896504402160645
    },
    {
      "epoch": 0.4288346883468835,
      "step": 1978,
      "training_loss": 5.495303153991699
    },
    {
      "epoch": 0.42905149051490515,
      "step": 1979,
      "training_loss": 6.510032653808594
    },
    {
      "epoch": 0.42905149051490515,
      "step": 1979,
      "training_loss": 4.867443084716797
    },
    {
      "epoch": 0.42905149051490515,
      "step": 1979,
      "training_loss": 7.465303897857666
    },
    {
      "epoch": 0.42905149051490515,
      "step": 1979,
      "training_loss": 7.058575630187988
    },
    {
      "epoch": 0.4292682926829268,
      "grad_norm": 9.85473346710205,
      "learning_rate": 1e-05,
      "loss": 6.8538,
      "step": 1980
    },
    {
      "epoch": 0.4292682926829268,
      "step": 1980,
      "training_loss": 7.7476701736450195
    },
    {
      "epoch": 0.4292682926829268,
      "step": 1980,
      "training_loss": 7.509133815765381
    },
    {
      "epoch": 0.4292682926829268,
      "step": 1980,
      "training_loss": 4.123628616333008
    },
    {
      "epoch": 0.4292682926829268,
      "step": 1980,
      "training_loss": 6.655900001525879
    },
    {
      "epoch": 0.4294850948509485,
      "step": 1981,
      "training_loss": 6.669153690338135
    },
    {
      "epoch": 0.4294850948509485,
      "step": 1981,
      "training_loss": 8.09969711303711
    },
    {
      "epoch": 0.4294850948509485,
      "step": 1981,
      "training_loss": 6.3873467445373535
    },
    {
      "epoch": 0.4294850948509485,
      "step": 1981,
      "training_loss": 6.78691291809082
    },
    {
      "epoch": 0.4297018970189702,
      "step": 1982,
      "training_loss": 6.474372386932373
    },
    {
      "epoch": 0.4297018970189702,
      "step": 1982,
      "training_loss": 6.360513210296631
    },
    {
      "epoch": 0.4297018970189702,
      "step": 1982,
      "training_loss": 7.359066009521484
    },
    {
      "epoch": 0.4297018970189702,
      "step": 1982,
      "training_loss": 8.183147430419922
    },
    {
      "epoch": 0.4299186991869919,
      "step": 1983,
      "training_loss": 7.824506759643555
    },
    {
      "epoch": 0.4299186991869919,
      "step": 1983,
      "training_loss": 4.4300217628479
    },
    {
      "epoch": 0.4299186991869919,
      "step": 1983,
      "training_loss": 7.524045467376709
    },
    {
      "epoch": 0.4299186991869919,
      "step": 1983,
      "training_loss": 6.31549072265625
    },
    {
      "epoch": 0.43013550135501355,
      "grad_norm": 15.7307710647583,
      "learning_rate": 1e-05,
      "loss": 6.7782,
      "step": 1984
    },
    {
      "epoch": 0.43013550135501355,
      "step": 1984,
      "training_loss": 6.6879072189331055
    },
    {
      "epoch": 0.43013550135501355,
      "step": 1984,
      "training_loss": 7.03000020980835
    },
    {
      "epoch": 0.43013550135501355,
      "step": 1984,
      "training_loss": 7.4502763748168945
    },
    {
      "epoch": 0.43013550135501355,
      "step": 1984,
      "training_loss": 7.038091659545898
    },
    {
      "epoch": 0.4303523035230352,
      "step": 1985,
      "training_loss": 7.109073638916016
    },
    {
      "epoch": 0.4303523035230352,
      "step": 1985,
      "training_loss": 8.37456226348877
    },
    {
      "epoch": 0.4303523035230352,
      "step": 1985,
      "training_loss": 7.430029392242432
    },
    {
      "epoch": 0.4303523035230352,
      "step": 1985,
      "training_loss": 6.720231533050537
    },
    {
      "epoch": 0.4305691056910569,
      "step": 1986,
      "training_loss": 6.25650691986084
    },
    {
      "epoch": 0.4305691056910569,
      "step": 1986,
      "training_loss": 6.41398286819458
    },
    {
      "epoch": 0.4305691056910569,
      "step": 1986,
      "training_loss": 6.511202335357666
    },
    {
      "epoch": 0.4305691056910569,
      "step": 1986,
      "training_loss": 7.144651889801025
    },
    {
      "epoch": 0.4307859078590786,
      "step": 1987,
      "training_loss": 6.860373020172119
    },
    {
      "epoch": 0.4307859078590786,
      "step": 1987,
      "training_loss": 4.5792155265808105
    },
    {
      "epoch": 0.4307859078590786,
      "step": 1987,
      "training_loss": 6.831054210662842
    },
    {
      "epoch": 0.4307859078590786,
      "step": 1987,
      "training_loss": 5.506758689880371
    },
    {
      "epoch": 0.4310027100271003,
      "grad_norm": 11.347823143005371,
      "learning_rate": 1e-05,
      "loss": 6.7465,
      "step": 1988
    },
    {
      "epoch": 0.4310027100271003,
      "step": 1988,
      "training_loss": 6.926590442657471
    },
    {
      "epoch": 0.4310027100271003,
      "step": 1988,
      "training_loss": 6.852100372314453
    },
    {
      "epoch": 0.4310027100271003,
      "step": 1988,
      "training_loss": 8.31456470489502
    },
    {
      "epoch": 0.4310027100271003,
      "step": 1988,
      "training_loss": 6.915981769561768
    },
    {
      "epoch": 0.43121951219512195,
      "step": 1989,
      "training_loss": 4.174050807952881
    },
    {
      "epoch": 0.43121951219512195,
      "step": 1989,
      "training_loss": 7.759764671325684
    },
    {
      "epoch": 0.43121951219512195,
      "step": 1989,
      "training_loss": 6.598691463470459
    },
    {
      "epoch": 0.43121951219512195,
      "step": 1989,
      "training_loss": 6.864677906036377
    },
    {
      "epoch": 0.4314363143631436,
      "step": 1990,
      "training_loss": 6.910934925079346
    },
    {
      "epoch": 0.4314363143631436,
      "step": 1990,
      "training_loss": 7.448502063751221
    },
    {
      "epoch": 0.4314363143631436,
      "step": 1990,
      "training_loss": 7.696954727172852
    },
    {
      "epoch": 0.4314363143631436,
      "step": 1990,
      "training_loss": 6.857715606689453
    },
    {
      "epoch": 0.43165311653116534,
      "step": 1991,
      "training_loss": 6.754056453704834
    },
    {
      "epoch": 0.43165311653116534,
      "step": 1991,
      "training_loss": 6.856870174407959
    },
    {
      "epoch": 0.43165311653116534,
      "step": 1991,
      "training_loss": 6.503749847412109
    },
    {
      "epoch": 0.43165311653116534,
      "step": 1991,
      "training_loss": 6.55587100982666
    },
    {
      "epoch": 0.431869918699187,
      "grad_norm": 12.2257719039917,
      "learning_rate": 1e-05,
      "loss": 6.8744,
      "step": 1992
    },
    {
      "epoch": 0.431869918699187,
      "step": 1992,
      "training_loss": 5.4187092781066895
    },
    {
      "epoch": 0.431869918699187,
      "step": 1992,
      "training_loss": 8.052759170532227
    },
    {
      "epoch": 0.431869918699187,
      "step": 1992,
      "training_loss": 6.97600793838501
    },
    {
      "epoch": 0.431869918699187,
      "step": 1992,
      "training_loss": 6.721277713775635
    },
    {
      "epoch": 0.4320867208672087,
      "step": 1993,
      "training_loss": 7.309533596038818
    },
    {
      "epoch": 0.4320867208672087,
      "step": 1993,
      "training_loss": 6.469273090362549
    },
    {
      "epoch": 0.4320867208672087,
      "step": 1993,
      "training_loss": 7.42769193649292
    },
    {
      "epoch": 0.4320867208672087,
      "step": 1993,
      "training_loss": 6.989121437072754
    },
    {
      "epoch": 0.43230352303523034,
      "step": 1994,
      "training_loss": 6.927725791931152
    },
    {
      "epoch": 0.43230352303523034,
      "step": 1994,
      "training_loss": 7.3562912940979
    },
    {
      "epoch": 0.43230352303523034,
      "step": 1994,
      "training_loss": 6.560801982879639
    },
    {
      "epoch": 0.43230352303523034,
      "step": 1994,
      "training_loss": 6.854404926300049
    },
    {
      "epoch": 0.432520325203252,
      "step": 1995,
      "training_loss": 6.963420867919922
    },
    {
      "epoch": 0.432520325203252,
      "step": 1995,
      "training_loss": 6.9125518798828125
    },
    {
      "epoch": 0.432520325203252,
      "step": 1995,
      "training_loss": 6.3697075843811035
    },
    {
      "epoch": 0.432520325203252,
      "step": 1995,
      "training_loss": 7.212264060974121
    },
    {
      "epoch": 0.43273712737127373,
      "grad_norm": 11.187137603759766,
      "learning_rate": 1e-05,
      "loss": 6.9076,
      "step": 1996
    },
    {
      "epoch": 0.43273712737127373,
      "step": 1996,
      "training_loss": 7.0475053787231445
    },
    {
      "epoch": 0.43273712737127373,
      "step": 1996,
      "training_loss": 6.562124252319336
    },
    {
      "epoch": 0.43273712737127373,
      "step": 1996,
      "training_loss": 5.170841217041016
    },
    {
      "epoch": 0.43273712737127373,
      "step": 1996,
      "training_loss": 7.37847900390625
    },
    {
      "epoch": 0.4329539295392954,
      "step": 1997,
      "training_loss": 6.926302433013916
    },
    {
      "epoch": 0.4329539295392954,
      "step": 1997,
      "training_loss": 8.096616744995117
    },
    {
      "epoch": 0.4329539295392954,
      "step": 1997,
      "training_loss": 7.417169570922852
    },
    {
      "epoch": 0.4329539295392954,
      "step": 1997,
      "training_loss": 6.49241828918457
    },
    {
      "epoch": 0.43317073170731707,
      "step": 1998,
      "training_loss": 6.823380947113037
    },
    {
      "epoch": 0.43317073170731707,
      "step": 1998,
      "training_loss": 7.091014385223389
    },
    {
      "epoch": 0.43317073170731707,
      "step": 1998,
      "training_loss": 7.034282207489014
    },
    {
      "epoch": 0.43317073170731707,
      "step": 1998,
      "training_loss": 7.916765213012695
    },
    {
      "epoch": 0.43338753387533874,
      "step": 1999,
      "training_loss": 7.817077159881592
    },
    {
      "epoch": 0.43338753387533874,
      "step": 1999,
      "training_loss": 6.454538822174072
    },
    {
      "epoch": 0.43338753387533874,
      "step": 1999,
      "training_loss": 7.3396897315979
    },
    {
      "epoch": 0.43338753387533874,
      "step": 1999,
      "training_loss": 7.636676788330078
    },
    {
      "epoch": 0.43360433604336046,
      "grad_norm": 10.755325317382812,
      "learning_rate": 1e-05,
      "loss": 7.0753,
      "step": 2000
    },
    {
      "epoch": 0.43360433604336046,
      "eval_runtime": 474.0073,
      "eval_samples_per_second": 4.325,
      "eval_steps_per_second": 4.325,
      "step": 2000
    }
  ],
  "logging_steps": 4,
  "max_steps": 4612,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 2000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 4.0545827807232e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
